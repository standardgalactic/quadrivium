
Fundamentals of Statistical  
Experimental Design  
and Analysis


Fundamentals of Statistical  
Experimental Design  
and Analysis
Robert G. Easterling
Cedar Crest, New Mexico, USA

This edition first published 2015
© 2015 John Wiley & Sons, Ltd
Registered Office
John Wiley & Sons, Ltd, The Atrium, Southern Gate, Chichester, West Sussex, PO19 8SQ,  
United Kingdom 
For details of our global editorial offices, for customer services and for information about how to apply 
for permission to reuse the copyright material in this book please see our website at  
www.wiley.com. 
The right of the author to be identified as the author of this work has been asserted in accordance 
with the Copyright, Designs and Patents Act 1988. 
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or 
transmitted, in any form or by any means, electronic, mechanical, photocopying, recording or 
otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior 
permission of the publisher.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print 
may not be available in electronic books.
Designations used by companies to distinguish their products are often claimed as trademarks. All 
brand names and product names used in this book are trade names, service marks, trademarks or 
registered trademarks of their respective owners. The publisher is not associated with any product or 
vendor mentioned in this book. 
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts in 
preparing this book, they make no representations or warranties with respect to the accuracy or 
completeness of the contents of this book and specifically disclaim any implied warranties of 
merchantability or fitness for a particular purpose. It is sold on the understanding that the publisher is 
not engaged in rendering professional services and neither the publisher nor the author shall be liable 
for damages arising herefrom. If professional advice or other expert assistance is required, the services 
of a competent professional should be sought
Library of Congress Cataloging-in-Publication Data
Easterling, Robert G.
  Fundamentals of statistical experimental design and analysis / Robert G. Easterling, Cedar Crest, 
New Mexico, USA.
    pages  cm
  Includes bibliographical references and index.
  ISBN 978-1-118-95463-8 (cloth)
1.  Mathematical statistics–Study and teaching.  2.  Mathematical statistics–Anecdotes.   
3.  Experimental design.  I.  Title.
  QA276.18.E27 2015
  519.5′7–dc23
	
2015015481
A catalogue record for this book is available from the British Library.
Set in 10/13pt Avenir by SPi Global, Pondicherry, India
1  2015

Dedications
Experimental Design Mentors, Oklahoma State University
David Weeks
Bob Morrison
Statistical Consulting Mentor, Sandia National Laboratories
Dick Prairie


Preface
xiii
Acknowledgments
xix
Credits	
xxi
1  Introduction
1
Motivation: Why Experiment?
1
Steps in an Experimental Program
2
Planning and analysis
2
Communication
3
Subject‐Matter Passion
4
Case Study
5
Overview of Text
9
Assignment
10
References
10
2  Fundamentals of Experimental Design
11
Introduction
11
Experimental Structure
13
Experimental units
13
Blocks and block structures
15
Treatments and treatment structures
17
Response measurement
19
Principles of Experimental Design
20
Replication
21
Randomization
22
Blocking
24
Control
26
Assignment
27
References
27
Contents

Contents
viii
3  Fundamentals of Statistical Data Analysis
29
Introduction
29
Boys’ Shoes Experiment
30
Experimental design
30
Graphical displays
31
Significance testing
34
Probability and probability distributions
34
Sign test
36
Misinterpretation of P‐values
38
Randomization test
39
Normal distribution theory t‐test
40
Summary and discussion: Significance tests
46
Economic analysis: The bigger picture
48
Statistical confidence intervals
50
Discussion
53
Why calculate statistical confidence limits?
54
Sample size determination
54
Tomato Fertilizer Experiment
56
Experimental design
56
Analysis 1: Plot the data
56
The value of randomization
58
The importance of ancillary data
59
A New Tomato Experiment
59
Analysis 1: Plot the data
59
Significance tests
62
Rank sum test
63
Randomization test
64
Normal theory t‐test
66
Confidence intervals
69
Determining the size of an experiment
71
Comparing Standard Deviations
77
Discussion
79
Appendix 3.A The Binomial Distribution
79
Appendix 3.B Sampling from a Normal Distribution
81
Appendix 3.C Statistical Underpinnings
85
Single sample
86
Two samples
87
Assignment
89
References
89
4  Completely Randomized Design
91
Introduction
91
Design Issues
92
CRD: Single Qualitative Factor
92

ix
Contents
Example: Market research
92
Analysis of Variance
95
Within‐group variation
96
Among‐groups variation
97
The F‐test
98
Analysis of variance
99
Discussion
100
Results
101
Testing the Assumptions of Equal Variances and Normality
103
Confidence Intervals
103
Inference
105
Statistical Prediction Interval
105
Example: Tomato Fertilizer Experiment Revisited
106
Sizing a Completely Randomized Experiment
107
CRD: Single Quantitative Factor
107
Example: Growth rate of rats
108
Graphical display
109
Curve fit
109
Analysis of variance
111
Design Issues
113
Enhanced Case Study: Power Window Gear Teeth
114
Graphical display
117
ANOVA
119
Discussion
120
Assignment
120
References
121
5  Completely Randomized Design with Multiple  
Treatment Factors
123
Introduction
123
Design Issues
124
Example 1 (Two qualitative factors): Poisons and antidotes
124
Analysis 1: Plot the data
126
Eyeball analysis
126
Interaction
128
ANOVA
130
Generalizing the ANOVA for a CRD with two factors
131
Antidote B versus Antidote D
132
Estimation of effects
133
Prediction intervals
135
Probability estimation and tolerance intervals
136
Further experiments
138
Example 2 (Two quantitative factors): Ethanol blends  
and CO emissions
139

Contents
x
Data displays
142
Discussion
144
Regression analysis and ANOVA
145
Discussion
148
Response Surface Designs
149
Extensions: More than two treatment factors
150
Example 3: Poison/antidote experiment extended
151
Example 4: Ethanol experiment extended
154
Special Case: Two‐Level Factorial Experiments
155
Example 5: Pot production
156
Analysis 1: Look at the data
158
Analysis 2: Regression analysis
159
Analysis 2: Stepwise regression
162
Analysis 3: “Effect sparsity” and graphical analysis
162
Fractional Two‐Level Factorials
167
Example 6: E‐mail marketing
167
One‐factor‐at‐a‐time designs
168
Results: E‐mail experiment
170
Example 7: Flower pot experiment revisited
171
Extensions
175
Assignment
175
References
175
6  Randomized Complete Block Design
177
Introduction
177
Design Issues
178
RBD with replication: Example 1—battery experiment
179
Analysis 1: Plot the data
180
Analysis of variance
181
Reliability analysis
183
Further analysis
184
Bringing subject‐matter knowledge to bear
185
Example 2: More tomato fertilizer experiments
187
Example 3: More gear teeth experiments
188
RBD with Single Replication
188
Example 4: Penicillin production
189
Components of variation
191
Sizing a Randomized Block Experiment
194
True Replication
195
Example 5: Cookies
195
Example 6: Battery experiment revisited
196
Example 7: Boys’ shoes revisited
197
Extensions of the RBD
199
Multifactor treatments and blocks—example:  
Penicillin experiment extended
199

xi
Contents
Example 8: A blocks‐only “experiment”—textile production
201
Analysis 1: Plot the data
201
Discussion
202
Balanced Incomplete Block Designs
203
Example: Boys’ shoes revisited again
203
Summary
205
Assignment
205
References
205
7  Other Experimental Designs
207
Introduction
207
Latin Square Design
208
Example: Gasoline additives and car emissions
208
Analysis 1: Plot the data
212
ANOVA
214
Discussion
215
Follow‐on experiments
216
Exercise
216
Extensions
217
Split‐Unit Designs
218
Example: Corrosion Resistance
220
Analysis 1: Plot the data
222
ANOVA
225
Discussion
228
Repeated Measures Designs
230
Example: Effects of drugs on heart rate
231
Analysis 1: Plot the data
232
Discussion
234
Extensions
235
Robust Designs
235
Introduction
235
Variance transmission
235
Mathematical model: Robustness
238
Concluding comments
239
Optimal Designs
240
Introduction
240
Finding “optimal experimental designs”
240
Design augmentation
242
Assignment
243
References
243
Index
245


I have a dream: that professionals in all areas—business; government; the 
physical, life, and social sciences; engineering; medicine; and others—will 
increasingly use statistical experimental design to better understand their 
worlds and to use that understanding to improve the products, processes, and 
programs they are responsible for. To this end, these professionals need to be 
inspired and taught, early, to conduct well‐conceived and well‐executed exper-
iments and then properly extract, communicate, and act on information gen-
erated by the experiment. This learning can and should happen at the 
undergraduate level—in a way that carries over into a student’s eventual 
career. This text is aimed at fulfilling that goal.
Many excellent statistical texts on experimental design and analysis have 
been written by statisticians, primarily for students in statistics. These texts are 
generally more technical and more comprehensive than is appropriate for a 
mixed‐discipline undergraduate audience and a one‐semester course, the audi-
ence and scope this text addresses. Such texts tend to focus heavily on statistical 
analysis, for a catalog of designs. In practice, however, finding and implement-
ing an experimental design capable of answering questions of importance are 
often where the battle is won. The data from a well‐designed experiment may 
almost analyze themselves—often graphically. Rising generations of statisti-
cians and the professionals with whom they will collaborate need more training 
on the design process than may be provided in graduate‐level statistical texts.
Additionally, there are many experimental design texts, typically used in 
research methods courses in individual disciplines, that focus on one area of 
application. This book is aimed at a more heterogeneous collection of stu-
dents who may not yet have chosen a particular career path. The examples 
have been chosen to be understandable without any specialized knowledge, 
while the basic ideas are transferable to particular situations and applications 
a student will subsequently encounter.
Successful experiments require subject‐matter knowledge and passion and 
the statistical tools to translate that knowledge and passion into useful 
Preface

Preface
xiv
information. Archie Bunker, in the TV series, All in the Family, once told his 
son‐in‐law (approximately and with typical inadvertent profundity), “Don’t give 
me no stastistics (sic), Meathead. I want facts!” Statistical texts naturally focus 
on “stastistics”: here’s how to calculate a regression line, a confidence interval, 
an analysis of variance table, etc. For the professional in fields other than 
statistics, those methods are only a means to an end: revealing and under-
standing new facts pertinent to his or her area of interest. This text strives to 
make the connection between facts and statistics. Students should see from 
the beginning the connection between the statistics and the wider business or 
scientific context served by those statistics.
To achieve this goal, I tell stories about experiments, and bring in appro-
priate analyses, graphical and mathematical, as needed to move the stories 
along. I try to describe the situation that led to the experiment, what was 
learned, and what might happen after the experiment: “Fire the quality man-
ager! Give the worthy statistician a bonus!” Experimental results need to be 
communicated in clear and convincing ways, so I emphasize graphical displays 
more than is often done in experimental design texts.
My stories are built around examples in statistical texts on experimental 
design, especially examples found in the classic text, Statistics for Experimenters, 
by Box, Hunter, and Hunter (1978, 2005). This “BHH” text has been on my 
desk since the first edition came out. I have taught several university classes 
based on it and have incorporated some of its material into my introductory 
statistics classes. Most of the examples are simple at first glance, but I have 
found it useful to (shamelessly) expand the stories in ways that address more 
of the design issues and more of the what‐do‐we‐do‐next issues. I try to make 
the stories provocative and entertaining because real‐life experimentation is 
provocative and entertaining. I want the issues and concepts to be discussable 
by an interdisciplinary group of students and the lessons to be transferable to 
a student’s particular interests, with enough staying power to affect the stu-
dent’s subsequent career. An underlying theme is that it is subject‐matter 
enthusiasms that give rise to experiments, shape their design, and guide 
actions based on the findings. Statistical experimental design and data analysis 
methods facilitate and enhance the whole process. In short, statistics is a team 
sport. This text tries to demonstrate that.
In 1974, I taught at the University of Wisconsin and had the opportunity to 
attend the renowned Monday night “beer seminars” in the basement of the 
late Professor George Box’s home. He would invite researchers in to discuss 
their work, and the evening would turn into a grand consulting session among 
George, the researcher, and the students and faculty in attendance. The late 
Bill Hunter, also a professor in the Statistics Department and an innovative 
teacher of experimental design, was often a participant. I learned a lot in those 
sessions and hope that the atmosphere of those Monday night consulting 
sessions is reflected in the stories I have created here. The other H in BHH is 
J. Stuart Hunter, also an innovator in the teaching of experimental design; his 

xv
Preface
presentations and articles have influenced me greatly, and his support for this 
book is especially valued. He puts humor into statistics that nobody would 
believe exists. I attended several Gordon Research Conferences at which B, H, 
and H all generated a lot of fun. Statistics can be fun. I have fun being a statis-
tician and I have tried to spice this book with a sense of fun. (Please note that 
this book’s title begins with fun.)
In this book, mathematical detail takes a backseat to the stories and the pic-
tures. Experimental design is not just for the mathematically inclined. I rely on 
software to do the analyses, and I focus on the story, not formulas. Once you 
understand the structure of a basic analysis of variance, I believe you can rely 
on software (and maybe a friendly, local statistician) to calculate an ANOVA 
table of the sort considered in this text. Thus, I do not give formulas for sums 
of squares for every design considered. Ample references are just a quick 
Google or Wikipedia search away for the mathematically intrigued students or 
instructors so inclined. I give formulas for standard errors and confidence inter-
vals where needed. I would be pleased if class discussions and questions, and 
alternate stories, led to displays and analyses not covered in my stories.
To offset my expanded stories, I limit the scope of this text’s topics to what 
I think is appropriate for an introductory course. I indicate and reference pos-
sible extensions beyond the text’s coverage. Individual instructors can tailor 
their excursions into such areas in ways that fit their students. This text can 
best be used by instructors with experience in designing experiments, ana-
lyzing the resulting data, and working with collaborators or clients to develop 
next steps. They can usefully supplement my stories with theirs.
Chapter‐end assignments emphasize the experimental design process, not 
computational exercises. I want students to pursue their passions and design 
experiments that could illuminate issues of interest to them. I want them to 
think about the displays and analyses they would use more than I want them to 
practice turning the crank on somebody else’s data. Ideally, I would like for 
these exercises to be worked by two‐ or three‐person teams, as in the real‐
world environment a student will encounter after college. (My ideal class 
makeup would be half statistics‐leaning majors and half majors from a variety 
of other fields, and I would pair a stat major with a nonstat major to do assign-
ments and projects.)
Existing texts contain an ample supply of analysis exercises that an instructor 
can choose from and assign, if desired. Some are listed at the end of this Preface. 
Individual instructors may or should have their own favorite texts and exercises. 
I would suggest only that each selected analysis exercise should be augmented 
by Analysis 1: Plot the data. These exercise resources are also useful windows 
on aspects of experimental design and analysis beyond the scope of this book 
that a student might want to pursue later in his studies or her career.
Software packages such as Minitab® also provide exercises. Teaching anal-
ysis methods in conjunction with software is also left to the individual instructor 
and campus resources. I use Minitab in most of my graphical displays and 

Preface
xvi
quantitative analyses, just because it suits my needs. Microsoft Excel® can also 
be used for many of the analyses and displays in this book. JMP® software 
covers basic analyses and provides more advanced capabilities that could be 
used and taught. Individual instructors should choose the software appro-
priate for their classrooms and campuses.
Projects provide another opportunity to experience and develop the ability to 
conceive, design, conduct, analyze, and communicate the results of experiments 
that students care about. I still recall my long‐ago experiment to evaluate the 
effect of salt and sugar on water’s time to boil (not that boiling water was a 
youthful passion of mine, but getting an assignment done on time was). A four‐
burner kitchen stove was integral to the design. I cannot tell you the effects of 
salt and sugar on time to boil, but I was able to reject with certainty the hypo-
thesis that “a watched pot never boils.” Again, I would encourage these projects 
to be done by small teams, rather than individually. Supplementary online 
material for the widely used text by Montgomery (2013) contains a large number 
of examples of student projects. I encourage students to seek inspiration from 
such examples. Much real‐world research is motivated by a desire to extend or 
improve upon prior work in a particular field, so if students want to find better 
ways to design and test paper airplanes, more power to them. I also recommend 
oral and written reports by students to develop the communication skills that are 
so important in their subsequent careers. This is time well spent.
In‐class experiments are another valuable learning tool. George Box, Bill 
Hunter, Stuart Hunter, and the Wisconsin program are innovators in this area. 
The second edition of BHH (Box, Hunter, and Hunter 2005) contains a case 
study of their popular paper‐helicopter design problem. In my classes, I sim-
plify the problem to a two‐ or three‐factor design space to simplify the task 
and shorten the time required by this exercise.
This text provides in Chapter  3 enough of basic statistical concepts 
(estimation, significance tests, and confidence intervals), within the context of 
designed experiments, that a previous course in statistics should not be 
required. Again, I think that once concepts are understood, a student or 
working professional can understand and appreciate the application of those 
concepts to other situations. My hope is that this text will make it more likely 
that universities will offer an undergraduate (and beginning graduate)‐level 
course in experimental design. This could be taught as a stand‐alone course, 
or, as was the case when I taught at the University of Auckland, one course 
could have two parallel tracks: experimental design and survey sampling, 
taught by different instructors. This text should also be useful for short courses 
in business, industry, and government.
I am convinced that personal and organizational progress, and even national 
and global progress, depends on how well we, the people, individually and 
collectively, deal with data. The statistical design of experiments and analysis 
of the resulting data can greatly enhance our ability to learn from data. 
In  George Box’s engagingly illustrated formulation (Box 2006), scientific 

xvii
Preface
progress occurs when intelligent, interested people intervene, experimentally, 
in processes to bring about potentially interesting events and then use their 
intelligence and the experimental results to better understand and improve 
those processes. My sincere hope is that this text will advance that cause.
References
Box, G., (2006) Improving Almost Anything: Ideas and Essays, revised ed., John Wiley & 
Sons, Inc., New York.
Box, G., Hunter, J., and Hunter, W. (1978, 2005) Statistics for Experimenters, 1st and 
2nd eds., John Wiley & Sons, New York.
Montgomery, D. (2009, 2013) Design and Analysis of Experiments, 7th and 8th eds., 
John Wiley & Sons, Inc., New York.
Statistical Software
JMP Statistical Discovery Software. jmp.com
Microsoft Excel. microsoftstore.com
Minitab Statistical Software. minitab.com
Sources for Student Exercises  
(in addition to the above references)
Cobb, G. (1997) Design and Analysis of Experiments, Springer‐Verlag, New York.
Cochran, W. G., and Cox, G. M. (1957) Experimental Designs, John Wiley & Sons, Inc., 
New York.
Ledolter, J., and Swersey, A. J. (2007) Testing 1‐2‐3: Experimental Design with 
Applications in Marketing and Service Operations. Stanford University Press, 
Stanford, CA.
Morris, M. (2011) Design of Experiments: An Introduction Based on Linear Models, 
Chapman and Hall/CRC Press, New York.
NIST/SEMATECH (2012) e‐Handbook of Statistical Methods, http://www.itl.nist.gov/
div898/handbook/
Oehlert, G. W. (2000) A First Course in Design and Analysis of Experiments, Freeman & 
Company, New York.
Wu, C.F., and Hamada, M. (2000). Experiments: Planning, Analysis, and Parameter 
Design Optimization, John Wiley & Sons, Inc., New York.


After my retirement from Sandia National Laboratories Vijay Nair, University of 
Michigan, invited me to teach an introductory course on experimental design 
at that campus. That experience and subsequent teaching opportunities at the 
University of Auckland, McMurry University, and the Naval Postgraduate 
School led to the development of class notes that evolved into this book. 
My wife, Susie, and I thoroughly enjoyed life as an itinerant Visiting Professor 
and greatly benefited from the stimulating environments provided by these 
universities.
I particularly want to thank the following authors for granting me permission 
to use examples from their texts: George Box and J. Stuart Hunter (Statistics for 
Experimenters); Johannes Ledolter, Arthur Swersey, and Gordon Bell (Testing 
1‐2‐3: Experimental Design with Applications in Marketing and Service 
Operations); Douglas Montgomery (Design and Analysis of Experiments); Chris 
Triggs (Sample Surveys and Experimental Designs); and George Milliken and 
Dallas Johnson (Analysis of Messy Data, vol. I., Designed Experiments). Wiley’s 
reviewers provided insightful and helpful comments on the draft manuscript, 
and a review of the manuscript from a student’s perspective by Naveen 
Narisetty, University of Michigan, was especially valuable. Max Morris, Iowa 
State University, provided a very helpful sounding board throughout. I also am 
thankful to Prachi Sinha Sahay, Jo Taylor, and Kathryn Sharples of the editorial 
staff at John Wiley & Sons, and to Umamaheshwari Chelladurai and Prasanna 
Venkatakrishnan who shepherded this project through to publication.
Robert G. Easterling
Cedar Crest, New Mexico
Acknowledgments


Credits
John Wiley & Sons Ltd for permission to use material from the following 
books:
●
●
Statistics for Experimenters (Box, Hunter, and Hunter, 1978, 2005)
●
●
Design and Analysis of Experiments, 5th ed. (Montgomery 2001)
●
●
Chance Encounters (Wild and Seber, 2000)
Stanford University Press for permission to use material from:
●
●
Testing 1‐2‐3 (Ledolter and Swersey, 2007)
Chapman and Hall/CRC Press for permission to use material from:
●
●
Analysis of Messy Data Volume I: Designed Experiments, 2nd ed., (Milliken and 
Johnson, 2009)
Department of Statistics, University of Auckland, for permission to use 
material from:
●
●
Sample Surveys and Experimental Designs (Scott and Triggs 2003)


Fundamentals of Statistical Experimental Design and Analysis, First Edition. Robert G. Easterling. 
© 2015 John Wiley & Sons, Ltd. Published 2015 by John Wiley & Sons, Ltd.
Motivation: Why Experiment?
Statistics is “learning from data.” We do statistics when we compare prices 
and specifications and perhaps Consumer Reports data in choosing a new cell 
phone, and we do it when we conduct large‐scale experiments pertaining to 
medications and treatments for debilitating diseases.
Much of the way we learn from data is observational. We collect data on 
people, products, and processes to learn how they work. We look for relation-
ships between variables that may provide clues on how to affect and improve 
those processes. Early studies on the association between smoking and ­various 
health problems are examples of the observational process—well organized 
and well executed.
The late Professor George Box (Box, Leonard, and Wu 1983; Box 2006; and 
in various conference presentations in the 1980s) depicted history as a series 
of events, some interesting, most mundane. Progress happens when there is 
an intelligent observer present who sees the interesting event and reacts—
who capitalizes on what has been learned. Box cited the ­second fermentation 
of grapes, which produces champagne, as an especially ­serendipitous obser-
vation. (Legend has it that a French monk, Dom Pérignon, made the discovery: 
“Come quickly, I’m drinking the stars!” (Wikipedia 2015).)
Now clearly, as Professor Box argued, progress is speeded when interesting 
events happen more frequently and when there are more intelligent observers 
present at the event—“more” in the senses of both a greater number of intel-
ligent observers and observers who are more intelligent. Experimentation—
active, controlled intervention in a process, changing inputs and features of 
the process to see what happens to the outcome (rather than waiting for 
Introduction
1

Fundamentals of Statistical Experimental Design and Analysis
2
nature to act and be caught in the act)—by people 
with insight and knowledge offers the opportunity 
and means to learn from data with greater quickness 
and depth than would otherwise be the case. For 
example, by observation our ancestors learned that 
friction between certain materials could cause fire. By 
experimentation, and engineer­ing, their descendants 
learned to make fire starting fast, reliable, and cheap—
a mere flick of the  Bic®. Active experimentation is 
now very much  a  part of business, science, engi-
neering, ­education, government, and medicine. That 
role should grow.
For experimentation to be successful, experimental plans (“designs”) 
must be well conceived and faithfully executed. They must be capable of 
answering the questions that drive the research. Experimental designs 
need to be effective and efficient. Next, the experiment’s data need to 
be summarized and interpreted in a straightforward, informative way. The 
implications of the experiment’s results need to be clearly communicated. 
At the same time, limitations of what is learned need to be honestly 
acknowledged and clearly explained. Experiments yield limited, not infi-
nite, data, and so conclusions need to be tempered by this fact. That’s what 
statistics is all about. This chapter provides an overview of the experimental 
design and statistical data analysis process, and the subsequent chapters 
do the details.
Steps in an Experimental Program
Planning and analysis
Learning from data: To do this successfully, data must first contain information. 
The purpose of experimental design is to maximize, for a given amount of 
resources, the chance that information‐laden data will be generated and struc-
tured in such a way as to be conducive to extracting and communicating that 
information. More simply, we need data with a message, and we need that 
message to be apparent.
Figure 1.1 is a cartoon view of this process. There is a data cloud, from which 
information is precipitating. But this information may be fuzzy, indistinct, disor-
ganized, and incomplete. The purpose of statistical analysis is to collect that 
information and distill it into clear, well‐organized INFORMATION. But this 
process does not work on its own. Intervention is needed. First, if we do some 
cloud seeding at the start—planning studies and designing experiments—we 
should increase the amount and quality of precipitating information, and we 
should facilitate the distillation process. That is, with good planning, it should 
take less work to extract information from the data. Further, the distillation 

3
Introduction
process needs catalysts—subject‐matter knowledge, models, assumptions, 
and statistical methods. The aim of this text is to provide plans and analysis 
methods for turning ideas into experiments which yield data that yield 
information that translates into knowledge and actions based on our improved 
state of knowledge.
Good experimentation starts with subject‐matter knowledge and passion—
a strong desire to better understand natural and created phenomena. From 
this passion flow questions to be answered, questions that can best be posed 
from a foundation of subject‐matter understanding. Statistics provides the 
tools and framework (i) for translating these questions into experiments and 
(ii) for interpreting the resulting data. We need to run experiments that are 
efficient and that are capable of answering questions; we need statistical 
methods to discover and characterize relationships in the experimental data 
and to answer whether apparent relationships are real or could easily be 
random. We need subject‐matter knowledge and context to interpret and act 
on the relationships that are found in the experiments. Subject‐matter 
knowledge and statistical methods need to be intertwined to be most effec-
tive in conducting experiments and learning 
from the resulting data.
Communication
Learning has to be communicated. As men-
tioned in the Preface, Archie Bunker, of the All 
in the Family TV show (check your cable TV 
­listings for reruns), once told his son‐in‐law 
(approximately, and with typical inadvertent 
­profundity), “Don’t give me no stastistics (sic), 
Meathead! I  want facts!” What he meant was: 
talk to me in terms of the subject‐matter, not in 
statistical jargon.
Statistics: the big picture
Planning
Information
Data cloud
Statistical analysis
Assumptions, models,
methods, subject-
matter knowledge
Figure 1.1  Statistics Schematic.

Fundamentals of Statistical Experimental Design and Analysis
4
Statistics is inherently collaborative—a team sport. Successful experiments 
require subject‐matter knowledge and passion and the statistical tools to trans-
late that knowledge and passion into useful information. Statisticians tend to be 
passionate about the methods they can use to extract information from data. 
That’s what they want to talk about. For the collaborative professional in another 
field, those methods are only a means to an end: revealing and understanding 
new facts pertinent to his or her area of interest/passion. The experiment and 
resulting data advance understanding in that field, so it is essential, as Archie 
said, that statistical results be communicated in this context, not as “statistics” 
per se.
Subject‐Matter Passion
An example that shows the importance of bringing subject‐matter passion to 
the appreciation and interpretation of data is a case study I call “Charlie Clark 
and the Car Charts.” The statistics group I managed at Sandia National 
Laboratories in Albuquerque had a small library, and when we got a new 
addition, I would route it around to the group so they would be aware of it. 
One new book I routed dealt with graphical methods. Charlie Clark was both 
thorough and a car nut. He did more than skim the table of contents—he read 
the book. One chart he came across was a scatter plot of automobile engine 
displacement versus body weight. This plot (approximately reproduced in 
Fig. 1.2) showed a slightly curved positive association—heavier cars have larger 
engines—and a couple of outlying points. The authors made the statistical 
points that you could not “see” the relationship between body size and engine 
size, or the outliers in a table of the data, whereas a plot shows these clearly. 
Then they commented that the outliers might be unusual cars or mistakes in 
the data and went on to other topics.
Charlie Clark and the Car Charts
Engine size
Body weight
Chevette
Opel
Figure 1.2  Car Data: Engine Size versus Body Weight.

5
Introduction
Well, the two outlying cars are more than just unusual to a car nut. They 
would be special: the outlying points are two cars with unusually large engines 
for their body weights. They would thus be high‐performance autos, so Charlie 
not only noticed the outliers, he got excited. He wanted one of those cars, 
so he looked up the source data (provided in the book’s appendices). Alas, 
they were the Opel and Chevette, which he knew were performance dogs—
“econoboxes.” He then went to the original Consumer Reports data source 
and found that transcription errors had been made between the source data 
and the text. Sorry, Charlie.
The moral of this story is that Charlie found the true “message” in the data 
(albeit only a transcription error), which is what statistical analysis is all about, 
not because he was a better statistician than the authors, but because he had 
a passionate interest in the subject matter. For more on this theme, see 
Easterling (2004, 2010). See also Box (1984).
Case Study
Integrated circuits (ICs), the guts of computing 
and communication technology, are circuits 
imprinted on tiny silicon chips. In a piece of 
electronic equipment, these ICs are attached to 
a board by teensy wires, soldered at each end. 
Those solder joints have to be strong enough to 
assure that the connection will not be broken 
if the equipment is jostled or abused to some 
extent in use. In other words, the wire bonds 
have to be reliable.
To assure reliability, producers periodically sample from production and do 
pull‐tests on a chip’s bonds. (These tests are usually done on chips that have 
failed for other reasons—it’s not good business practice to destroy good prod-
uct.) The test consists of placing a hook under the wire and then pulling 
the hook until the wire or wire bond breaks. This test is instrumented so that 
the force required to break the bond is recorded. A manufacturer or the cus-
tomer will specify a lower limit on acceptable strength. If too many bonds 
break below this breaking‐strength limit, then that is a sign that the bonding 
process is not working as designed and adjustments are needed.
Well, a process engineer showed up at Ed Thomas’s office one day with a file 
of thousands of test results collected over some period of time. (Ed is a statis-
tician at Sandia National Laboratories, Albuquerque, NM.) The engineer 
wanted Ed to use the data to estimate wire‐bond reliability. This reliability 
would be the probability that a bond strength exceeds its acceptable lower 
limit. (Although we haven’t discussed “probability” yet, just think in terms of a 
more familiar situation, such as the SAT scores of high school seniors in 2014. 
These scores vary and a “probability distribution”—sometimes a “bell‐shaped 

Fundamentals of Statistical Experimental Design and Analysis
6
curve”—portrays this variability.) The initial plan was to use the data to estimate 
a “probability distribution” of bond strength and, from this distribution, 
estimate the percent of the distribution that exceeded the lower limit 
(see Fig. 1.3).
The bars are the percentages of bond‐strength measurements in specified, 
adjacent intervals. The blue curve is a “Normal Distribution” fitted to the 
bond‐strength data. The estimated reliability is the percent of the distribution 
above the lower limit.
But Ed was inquisitive—snoopy (and bright). He noticed that the data file did 
not just have bond‐strength data and chip identification data such as date and 
lot number. The file also had variables such as “bond technician” and “test 
operator” associated with each test result. He sorted and plotted the bond‐
strength data for different bond and test operators and found differences. 
Bond strength seemed to depend on who did the bonding operation and who 
did the test! This latter dependence is not a good characteristic of an industrial 
measurement process. You want measurement process components, both 
equipment and personnel, to be consistent no matter who is doing the work. If 
not, wrong decisions can be made that have a substantial economic impact. 
You also want a manufacturing process to be consistent across all the personnel 
involved. A problem with the available data, though, was that the person who 
did the bonding operation was often the same person who did the test opera-
tion. From these data, one could not tell what the source of the inconsistency 
was. It would not make sense to try to estimate reliability at this point: you 
would have to say (apparent) reliability depends on who did the test. That 
doesn’t make sense. What was needed was further investigation and process 
improvement to find the source of the inconsistencies in the data and to 
improve the production and test processes to eliminate these inconsistencies.
After a series of discussions, the process engineer and Ed came up with  
the following experiment. They would have three operators each use three 
different machines to make wire bonds. That is, chips would be bonded 
Bond strength
Lower limit
Figure 1.3  Bond‐Strength Distribution.

7
Introduction
to packages using all nine possible 
combinations 
of 
operator 
and 
machine. Then the boards for each of 
these combinations would be ran-
domly divided into three groups, 
each group then pull‐tested by a dif-
ferent test operator. This makes 27 
combinations 
of 
bond 
operator, 
machine, and test operator in the 
experiment. For each of these combi-
nations, there would be two chips, 
each with 48 bonds. Thus, the grand 
total of bond‐test results would be 
27 × 96 = 2592. This is a large experiment, but the time and cost involved were 
reasonable. These are the sorts of issues faced and resolved in a collaborative 
design of an experiment.
Statistical analysis of the data, by methods presented in later chapters, led 
to these findings:
●
●
There were no appreciable differences among bonding machines.
●
●
There were substantial differences among both bonding operators and test 
operators.
A couple of points before we look at the data: (i) It is not surprising to find 
that machines are more consistent than people. Look around. There’s a lot 
more variation among your fellow students than there is in the laptops or tab-
lets they use. (ii) Because the experiment was “balanced,” meaning that all 
combinations of bonding and test operators produced the same number of 
bond tests, it is now possible to separate the effects of bond operator and test 
operator in the experiment’s data.
Figure 1.4 shows the average pull strengths for each combination of bond 
and test operators. These averages are averages across machines, chips, and 
bonds—total of 288 test results in each average.
The results in Figure 1.4 have very consistent patterns:
●
●
Bond operator B produces consistently stronger bonds.
●
●
There are consistent differences among pull‐test operators—operator 
A consistently had the highest average pull strengths; operator B ­consistently 
had the lowest.
(Statistical number crunching showed that these patterns could not be 
­attributed to the inherent variability of the production and testing processes; 
they were “real” differences, not random variation.)
Overall, in Figure 1.4, there is nearly a factor of two between the average pull 
strength for the best combination of operators and for the worst (9.0 vs. 5.1 g). 

Fundamentals of Statistical Experimental Design and Analysis
8
You do not want your production and measurement systems, machines and 
people, to be this inconsistent.
With this data‐based information in hand, the process engineer has a license 
to examine the production and testing procedures carefully, along with the 
technicians involved, and find ways to eliminate these inconsistencies.
(A friend of mine tells his audiences: “Without data you’re just another loud-
mouth with an opinion!” Another often‐used statistical funny: “In God we 
trust. All others bring data.”)
The focus for process improvement has to be on procedures—not peo-
ple. We’re not going to fire bond operator C because he produced the 
weakest bonds. We’re going to find out what these operators are doing 
differently to cause this disparity. It could be that they are interpreting or 
remembering possibly unclear process instructions in different ways. That 
can be fixed.
One specific subsequent finding was that it made a difference in pull‐testing 
if the hook was centered or offset toward one end of the wire or the other. 
Making the instructions and operation consistent on that part of the process 
greatly reduced the differences among test operators. (Knowing where to 
place the hook to best characterize a bond’s strength requires subject‐matter 
knowledge—physics, in this case.) Additional iterations of experimenting and 
process improvement led to much better consistency in the production and 
testing procedures.
Summary: The process engineer came to Ed wanting a number—a “reli-
ability.” Ed, ever alert, found evidence that the (observational) data would not 
support a credible reliability number. Well‐designed and well‐executed exper-
iments found evidence of production and testing problems, and the process 
engineer and technicians used these findings and their understanding of the 
processes to greatly improve those processes. Labor and management were 
both happy and heaped lavish praise on Ed.
Average pull strengths by bond operator and pull test
operator (averages of 288 pull tests)
10.0
9.0
Grams
8.0
7.0
6.0
5.0
4.0
3.0
2.0
1.0
0.0
A
B
Pull-test operator
C
A
B
Bonding operator
C
Figure 1.4  Average Bond Strengths by Bonding and Pull‐Test Operators.

9
Introduction
This picture is not Ed, but it could have 
been. The voice‐over of this celebratory scene 
in an old Microsoft Office commercial told us 
that “With time running out, he took an 
impossibly large amount of data and made 
something incredibly beautiful.” May every 
person who studies this book become a data 
hero such as this!
Overview of Text
Chapter 2 describes the basic elements of experimental design: experimental 
units, treatments, and blocks. (Briefly, “treatments” is statistical terminology 
for the interventions in a process.) Three principles that determine the preci-
sion with which treatment “effects” can be estimated—replication, randomiza-
tion, and blocking—are defined and discussed.
Chapter 3 addresses the fundamentals of statistical data analysis, starting 
with my recommended Analysis 1: Plot the Data. In particular, plot the data 
in a way that illuminates possible relations among the variables in the 
experiment.
Next come quantitative analyses—number crunching. In my view, the 
fundamental concept of statistical analysis is a comparison of “the data we 
got” to a probability distribution of “data we might have gotten” under spec-
ified “hypotheses” (generally assumptions about treatment effects). 
Significance tests and confidence intervals are statistical findings that emerge 
from these comparisons and help sort out and communicate the facts and the 
statistics, in Archie Bunker’s formulation. Two two‐treatment examples from 
Box, Hunter, and Hunter (1978, 2005) are the launching pads for a wide‐ranging 
discussion of statistical methods and issues in Chapter 3.
Chapter 4 introduces the family of completely randomized designs for the 
case of one treatment factor, either quantitative or qualitative. Chapter 5 is 
about completely randomized designs when the treatments are comprised of 
combinations of two or more treatment factors.
Chapter 6 introduces the family of randomized block designs and considers 
various treatment configurations. Chapter  7, titled Other Experimental 
Designs, addresses designs that are hybrids of completely randomized and 
randomized block designs or that require extending the principles of experi-
mental design beyond the scope of these design families.
And that’s it. This book is meant to be introductory, not comprehensive. At 
various points, I point to extensions and special cases of the basic experi-
mental designs and provide references. Formulas are minimized. They can be 
found in the references or online, if needed. I rely on software, primarily 
Minitab®, to produce data plots and to crunch the numbers. Other statistical 

Fundamentals of Statistical Experimental Design and Analysis
10
software is available. Microsoft Excel® can be coerced into most of the analyses 
in this text. I think choice of software now is equivalent to choice of desk cal-
culator 50 years ago: at this point in time, it does not matter that much. 
My focus is on the experimental design and data analysis processes, including 
the interplay between statistics and the application, between “friendly, local 
statisticians” and subject‐matter professionals. I try to illustrate data‐enhanced 
collaboration as a way to encourage such approaches to the large and small 
issues students will face when they leave the university and embark upon  
a career.
Assignment
Choose one of your areas of passionate interest. Find an article on that topic 
that illustrates the statistics schematic in Figure 1.1. To the extent possible, 
identify and discuss what that article tells you about the different elements in 
that process: data, assumptions, models, methods, subject‐matter knowledge, 
statistical analysis, and information generated and communicated. Evaluate 
how well you think the article succeeds in producing and communicating ­useful 
information. Suggest improvements.
References
Box, G. (1984) The Importance of Practice in the Development of Statistics, 
Technometrics, 26, 1–8.
Box, G., (2006) Improving Almost Anything: Ideas and Essays, John Wiley & Sons, Inc., 
New York.
Box, G., Hunter, W., and Hunter, J. (1978, 2005) Statistics for Experimenters, John Wiley 
& Sons, Inc., New York.
Box, G., Leonard, T., and Wu, C‐F. (eds.) (1983) Scientific Inference, Data Analysis, and 
Robustness, pp. 51–84, Academic Press, New York.
Easterling, R. (2004) Teaching Experimental Design, The American Statistician, 58, 
244–252.
Easterling, R. (2010) Passion‐Driven Statistics, The American Statistician, 64, 1–5.
Wikipedia (2015) Dom Pérignon (monk), http://en.wikipedia.org/wiki/Dom_Pérignon_ 
(monk).

Fundamentals of Statistical Experimental Design and Analysis, First Edition. Robert G. Easterling. 
© 2015 John Wiley & Sons, Ltd. Published 2015 by John Wiley & Sons, Ltd.
Introduction
The experiments dealt within this book are comparative: the purpose of doing the 
experiments is to compare two or more ways of doing something. In this 
context, an experimental design defines a suite, or set, of experiments. In this suite 
of experiments, different experimental units are subjected to different treat-
ments. Responses of the experimental units to the different treatments are 
measured and compared (statistically analyzed) to assess the extent to which 
different treatments lead to different responses and to characterize the rela-
tionship of responses to treatments. This process will be illustrated numerous 
ways throughout this book.
Agricultural experimentation, which gave rise 
to much of the early research on statistical exper-
imental design (see, e.g., Fisher 1947), provides 
a simple conceptual example. An experimenter 
wants to compare the crop yield and environ-
mental effects for two different fertilizers. The 
experimental units are separate plots of land. 
Some of these plots will be treated with Fertilizer 
A, and some with Fertilizer B. For example, 
Fertilizer A may be the currently used fertilizer; 
Fertilizer B is a newly developed alternative, per-
haps one designed to have the same or better crop growth yields but with 
reduced environmental side effects. Better food production with reduced envi-
ronmental impact is clearly something a research scientist and the public could 
be passionate, or at least enthusiastic, about.
Fundamentals of 
Experimental Design
2

Fundamentals of Statistical Experimental Design and Analysis
12
In this conceptual experiment, the selection of the plots and the experimental 
protocol will assure that the fertilizer used on one plot does not bleed onto 
another. Schedules for the amount and timing of fertilizer application will  
be set up. Crops will be raised and harvested on each plot, and the crop 
­production and residual soil chemicals will be measured and compared to see 
if the new fertilizer is performing as designed and is an improvement over the 
current fertilizer.
This example can be readily translated into other contexts:
●
●
Medical experiments in which the experimental units are patients and the 
treatments evaluated might be a new medication, perhaps at different 
­dosage levels, and a placebo
●
●
Industrial experiments in which different product designs or manufacturing 
processes are to be compared
●
●
Market research experiments in which the experimental units are ­consumers 
and the treatments are different advertising presentations
●
●
Education experiments in which the experimental units are groups 
of  ­children and the treatments are different teaching materials or 
methods
The possibilities are endless, which is why experimental design is so important 
to scientific and societal progress on all fronts.
Note the importance of running comparative experiments. If we applied 
Fertilizer B to all of our plants in this year’s test, we might get what appear 
to be very satisfactory yields, perhaps even better than Fertilizer A got in 
previous years. But we would not know whether Fertilizer A would have 
gotten comparable yields this year due, say, to especially favorable growing 
conditions or experimental care compared to previous years. To know 
whether B is better than A, you have to run experiments in which some 
experimental units get A, some get B, and all other conditions are as similar 
as possible.
Moreover, you have to assign A and B to experimental units in a way that 
does not bias the comparison. And you need to run the experiment with 
enough experimental units to have an adequate capability to detect a 
difference between fertilizers, relative to the natural variability of crop yields. 
For a wide variety of reasons, crop yields on identically sized, similar plots of 
land, all receiving the same fertilizer treatment, will vary; they won’t be 
­identical. (As car commercials warn about gas mileage: actual results may vary.) 
The potential average crop‐yield differences between plots with Fertilizer 
A and plots with Fertilizer B have to be evaluated relative to the inherent vari-
ability of plots that receive the same fertilizer. In experimental design termi-
nology, to do a fair and effective comparison of Fertilizers A and B, you have 
to ­randomize and replicate. These are two principles of experimental design, 
discussed later in this chapter.

13
Fundamentals of Experimental Design
Experimental Structure
The common features of all the preceding examples, the building blocks of a 
comparative experiment, are:
●
●
Experimental units (eus)—the entities that receive an independent 
­application of one of the experiment’s treatments
●
●
Treatments—the set of conditions under study
●
●
Responses—the measured characteristics used to evaluate the effect of 
treatments on experimental units
Basically, in conducting an experiment, we apply treatments to experimental 
units and measure the responses. Then we compare and relate the responses 
to the treatments. The goal of experimental design is to do this informatively 
and efficiently. The following sections discuss the above aspects of experi-
mental structures.
Experimental units
The choice of experimental units can be critical 
to the success of an experiment. In an example 
given by Box, Hunter, and Hunter (1978, 2005) 
(which will be abbreviated BHH throughout this 
book), the purpose of the experiment is to com-
pare two shoe‐sole materials for boys’ shoes. 
The experimental unit could be a boy, and each 
boy in the experiment would wear a pair of 
shoes of one material or the other. Or the exper-
imental unit could be a foot, and each boy 
would wear one shoe of each material. As we 
shall see, the latter experiment dramatically 
improves the precision with which the wear 
quality of the two materials can be compared. Where one foot goes, the 
other goes also, so the wear conditions experienced (the experimental 
­protocol is for the boys to wear the shoes in their everyday lives for a specific 
period of time) are very much the same (skateboarding and other one‐foot 
dominant activities not allowed). Such is not the case for different boys with 
different activities. Some boys are just naturally harder on shoes than other 
boys. As will be seen in Chapter 3, the data from this experiment show much 
more variability of shoe wear among different boys than there is between the 
two feet of one boy. This difference translates into a much more precise, 
­efficient, comparison of shoe materials when the experimental unit is a foot 
than when it is a boy.

Fundamentals of Statistical Experimental Design and Analysis
14
The selection of the boys needs to be discussed. The experiment described 
by BHH included 10 boys. It is possible, but unlikely (and not stated), that 
these 10 boys were randomly selected from some well‐defined “population,” 
such as all the fifth‐grade boys enrolled in Madison, Wisconsin, schools on 
October 1, 1975. Random sampling from identified populations is key to the 
validity and reliability of opinion polls and industrial quality control and can be 
used to select experimental units (or pairs of eus in this case) for an experiment. 
However, as will be discussed in Chapter 3, it is the random assignment of 
treatments to experimental units that establishes the validity of the experiment, 
not the random selection of the experimental units, themselves.
It is more likely that the boys are a “judgment sample” (Deming 1975), 
selected perhaps for convenience (such as children of the shoe company’s 
employees or all the boys in one particular classroom in a nearby school). This 
nonrandom selection would be based on an informed judgment by shoe 
company scientists that the boys selected are likely to subject the shoes to 
wear conditions that are “representative” of what the general population of 
boys would inflict on the company’s shoes. In fact, the boys may have been 
selected deliberately to span the plausible range of wear conditions. For any 
such judgment sample selections, any extension of the results of this experiment 
to the general shoe‐wearing population of boys will rest on such subject‐
matter knowledge. The conclusions drawn will be a subject‐matter inference, 
not a statistical inference. The statistical inference (Chapter 3) will be whether 
or not observed differences in wear for the two shoe material are “real  
or random.”
Why 10 boys, not 5, 50, or 100? Statistical analyses addressed in later 
­chapters can shed some light on this issue, but these analyses must be built on 
subject‐matter knowledge about the cost of experimentation and the 
­anticipated variability in shoe‐sole wear and its measurement. For the sake of 
completing the story line of the BHH experiment, let’s suppose that this is a 
pilot experiment aimed at providing a preliminary evaluation of the “wear-
ability” of the cheap substitute material B and of the variability of shoe wear 
among boys and between a boy’s two feet. Ten boys just happened to be 
available. (Available time and resources often drive experimental designs and 
other statistical studies. Afterward, we find out if we collected enough data or 
more than were needed. Also, textbook examples tend to be small for reasons 
of space and clarity.) If shoe‐sole material B is promising, more extensive 
experimentation, sized based on what we learn in this pilot experiment and on 
the degree of precision required to support a final decision, will follow.
The experimental unit issues in this shoe experiment are indicative of issues 
that arise in other contexts. In the agricultural experiment discussed ­previously, 
the location, size, and layout of the plots of land can be key to an experi-
ment’s success, both in its findings and in its credibility. In medical laboratory 
experiments, the choice of animal experimental units—mice, rats, gerbils, or 
something else—can be important.

15
Fundamentals of Experimental Design
In all of these situations, subject‐matter expertise is the primary determinant 
of the experimental unit. The statistician can evaluate the ramifications of dif-
ferent choices, for example, a boy vs. a foot, an acre vs. a hectare, etc., but 
subject‐matter knowledge and experimental objectives will define the choices.
Blocks and block structures
There are many ways that the experimental units selected for an experiment 
can be organized or structured. The simplest situation is to have one group 
of homogeneous experimental units (meaning similar, not identical), such as 
individual plots of land in a field or a garden in which the soil quality is 
­presumed or known to be essentially the same throughout. In a medical 
experiment, the experimental units could be patients in a specified demo-
graphic or ­medical category.
Alternatively, an experiment can have multiple groups of experimental 
units. The groups might differ in some recognizable way, but within each 
group, the experimental units would be relatively homogeneous. An example 
is plots of land in different fields or gardens or regions. The different gardens 
or regions would be the blocks of eus. In the boys’ shoes experiment, the 
experimental units—the boys’ feet—were paired (grouped or “blocked”) by 
boy. There were two experimental units (feet) per boy, and each eu got a dif-
ferent “treatment.” Clearly, a boy’s two feet are more similar to each other, 
with respect to the wear conditions they experienced, than are the feet of two 
different boys, whose physiques and activity levels could differ substantially. 
Hence, there is more homogeneity of experimental units within a group than 
between groups. Conventional terminology is to refer to groups of experi-
mental units as “blocks.”
An alternative, but strange, way to do the shoe‐sole experiment would be to 
ignore the pairing of a boy’s feet and randomly assign 10 of the 20 ft in the 
experiment to material A and the others to B. This assignment would likely 
have some boys with two shoes of one material and other boys with one shoe 
of each material. This “experimental design” would clearly not be very ­efficient 
or effective for comparing the two materials.
Blocks of experimental units can be defined by combinations of more than 
one variable. For example, patients in a medical experiment might be grouped 
by sex and age categories, particularly if it was known or suspected that differ-
ent age groups and the two sexes might respond differently to the treatments 
in the study. With three age groups, for example, there would then be six 
groups defined by the combinations of age and sex categories. The different 
treatments would be applied to subsets of the patients in each group or block 
of experimental units.
Expressing this grouping in conventional experimental design terminology, 
we would say that there are two blocking factors, sex and age, and that these 

Fundamentals of Statistical Experimental Design and Analysis
16
two factors are crossed—the same age categories are used for both men and 
women in the study.
Another experimental unit structure has nested factors. For example, in a 
manufacturing situation, the items produced, which, in a subsequent 
experiment, would be subjected to various treatments, could be grouped by 
the factory that produced them and then by assembly line within factories.
The blocking factors would be factory and assembly line, and the assembly 
line would be nested in factories. These factors are nested, not crossed, 
because assembly lines 1 and 2 in one factory are not the same as assembly 
lines 1 and 2 in another. The two assembly lines labeled line 1 are physically 
distinct. Figure 2.1 illustrates this structure: four (color‐coded) groups of exper-
imental units with the indicated structure. Subsequent experiments might be 
to evaluate properties of the tires, in this example, perhaps by testing them on 
a machine that simulates impacts and measures the blowout resistance of the 
tires as a function of impact velocity and inflation pressure. Comparisons of the 
test data between factories and among assembly lines could indicate manufac-
turing problems.
Chapters 4–7 in this book generally start with an experimental unit structure 
that defines a class of experimental designs. Those structures and the data 
analyses they lead to will be discussed in detail in those chapters. Blocking, as 
one of the principles of experimental design, is discussed later in this chapter.
Experimental unit structure can become quite complex, especially when 
experimental units of one size are split into smaller units. Agricultural and 
industrial experiments offer the most intuitive examples of such situations. 
Suppose 20 plots of reasonably homogeneous land are available, and one of 
four varieties of wheat will be planted on five plots each. Then, after planting 
and germination, each plot is divided into four quadrants so that different fer-
tilizers (randomly assigned) can be applied to each quadrant. Then, after the 
plants have grown for a predetermined period, each quadrant is divided into 
Factory
A
1
1
2
2
Assembly line
Experimental units
B
Figure 2.1  Nested Experimental Unit Structure.

17
Fundamentals of Experimental Design
two halves (now octants) for the application of two different insecticides. Thus, 
there are three levels, or sizes, of experimental units in this experiment, linked 
to the treatments that are applied: for wheat varieties, the pertinent experi-
mental unit is a plot; for fertilizer, the experimental unit is a quadrant; and for 
insecticides, the experimental unit is an octant. The statistical analysis of the 
resulting data (to be seen in Chapter 7) will have to account for these different 
experimental units in the same experiment.
Treatments and treatment structures
Experimental objectives also are the starting 
point for defining treatments. For example, 
a researcher may want to find the optimum 
time and ­temperature settings in a process 
such as baking bread or in producing com-
puter microchips. Previous work or theory—
subject‐matter knowledge—may suggest a 
starting point for the two “treatment 
factors,” time and temperature, as well as a 
range of these two factors to be consid-
ered. The experimental conditions to be evaluated and compared, that is, the 
“treatments,” or “treatment combinations,” will be a selected set of time and 
temperature combinations. Statistical considerations help select efficient, 
potentially informative combinations, subject to constraints dictated by the 
subject matter and built on the foreknowledge of the possible or, likely, the 
nature of the dependence of an experimental unit’s response on time and 
­temperature. In both complex and deceptively simple situations, there may be 
a large number of factors that potentially influence the response of an experi-
mental unit, and there is generally a trade‐off to be determined between the 
size of the experiment and the number of treatment factors and treatment 
combinations to be included in the experiment. Subject‐matter understanding 
is essential to making those trade‐offs sensibly.
As just indicated, treatments can be structured according to multiple 
“factors.” Treatment factors can be either categorical, such as types of shoe‐
sole material, or quantitative, such as time, temperature, and other physical 
variables.
In an experiment, each factor has a selected “levels.” For example, for a 
categorical variable, such as type of fertilizer, the levels are the names or iden-
tifiers of the different fertilizers in the experiment. The levels of a quantitative 
factor, such the amount of fertilizer applied, would be selected amounts, 
expressed, for example, in pounds per acre. The treatments, or “treatment 
combinations,” then can consist of all combinations of factors and factor levels 
or just a subset of the possible combinations. For example, if time and temper-
ature each have three levels, say, low, medium, and high, then the full set of 

Fundamentals of Statistical Experimental Design and Analysis
18
treatments that can be constructed from these two factors are the nine ­possible 
combinations of time and temperature levels. We say treatments created this 
way have a “factorial treatment structure.” In some contexts, when the 
treatment combinations are tabulated or entered into a spreadsheet, the suite 
of experiments is called an experimental matrix. Experiments with this struc-
ture provide an efficient way to evaluate the separate and the combined 
effects of the factors on responses of interest.
The simplest treatment structure is that of a single factor, such as fertilizer 
type or shoe material in examples already discussed. The experiment will be 
done to compare the responses as a function of the different levels of this 
factor. Next, treatment structures (as was the case for block structures) can be 
based on two factors, three factors, etc. It can be the case that the resulting 
number of multifactor treatment combinations becomes too large to be prac-
tical. “Fractional factorial” treatment structure (to be discussed in Chapter 5) 
is the specification of a well‐selected subset of the possible treatment combi-
nations. The determination of a fractional factorial set of treatments has to be 
done with care in order to maximize the amount of information the experiment 
provides pertaining to the separate and combined effects of the treatments. 
Clearly, some potential information about treatment effects has to be sacri-
ficed when using an experiment with a fractional factorial set of treatments, 
and, again, subject‐matter understanding, combined with statistical analysis, is 
necessary for determining where that sacrifice is to be made.
Another aspect of treatment structure is that the treatment factors can either 
be crossed or nested, just as is the case for blocking factors. Two factors are 
crossed when the levels of one factor are the same when used in combination 
with each level of the other factor. For example, in a crossed‐factor treatment 
structure, the temperature levels in the baking experiment would be the same 
for each level of the time factor.
The treatment combinations resulting from crossed factors, however, may 
include some combinations that the experimenter knows are not feasible—will 
not produce edible cookies. The knowledgeable experimenter knows that the 
higher the temperature, the shorter the baking time should be. Thus, the selected 
levels of baking time (say, low, medium, and high) considered in the experiment 
would be different at high temperature than at low temperature. When 
the levels of one factor are not the same at each level of the other factor, the 
factors are “nested.” Once again, subject‐matter understanding is essential to 
recognizing this relationship and designing an experiment appropriate to it.
Figure 2.2 shows the treatment combinations for two crossed and nested 
factors, each with three levels.
The plots in Figure 2.2 are appropriate for two quantitative factors. Factors 
can also be crossed or nested when one or both factors in a two‐factor 
experiment are qualitative. For example, in a medical experiment, one factor 
might be medication source (a qualitative factor with levels that are different 
manufacturers of pain‐relief medications), and a second factor might be dose 

19
Fundamentals of Experimental Design
(quantitative), with different levels for each manufacturer based on manufacturer 
recommendations.
(Note—The focus in this discussion on experimental unit and treatment “struc-
tures” follows that of Scott and Triggs (2003). This approach, which I encountered 
when I taught for a semester at the University of Auckland, is somewhat unusual 
and, I think, a very useful way to present basic features of an experimental design).
The discussion thus far, illustrated by Figure 2.2, has defined factor levels 
generically, as low, medium, and high. Choosing the numerical levels of those 
factors, though, is critical to the meaningfulness and success of the experiment. 
You don’t want to run the experiment over such a tight range of the factors that 
only minor differences in the response result and inference outside of that tight 
experimental region cannot be supported. On the other hand, you don’t want 
to run the experiment over such a wide range of factor levels that, for example, 
at the selected low temperature the cookies don’t get cooked and at high tem-
perature they’re burned to a crisp. You don’t want to starve your laboratory 
mice or incapacitate them by feeding them too little or too much of a dietary 
supplement. General guidance is to “be bold” in selecting factor ranges—give 
the factor a good opportunity to affect the response—but not foolhardy. 
Subject‐matter knowledge is essential to knowing the physical or biological or 
business limits of the treatment factors in a practical experiment. Pilot experi-
ments are useful in defining appropriate ranges for the experimental factors.
Response measurement
For an experiment to be effective, the effect of treatments on experimental 
units must be reflected in the measured responses of the experimental units. 
Sometimes, the choice of measurements that reflect the effect of treatments 
on experimental units is obvious; other times, it is not. Characteristics such as 
health, quality of life, mechanical fit, customer satisfaction, environmental 
impact, and learning can be difficult to capture quantitatively.
Crossed factor treatment combinations
A
B
Nested factor treatment combinations:
B is nested in A
A
B
Figure 2.2  Crossed and Nested Combinations of Treatment Factors. The three levels 
of B are different at the three levels of A; B is nested in A.

Fundamentals of Statistical Experimental Design and Analysis
20
Once it has been decided what to measure, measurement devices and 
­techniques can differ. For example, in the shoe example, one could measure 
“wear” as the percent decrease in sole thickness at one central point on the 
sole or as the average wear, or maximum wear, across several points at which 
the decrease in sole thickness is measured. Sole thickness might be measured 
with a foot ruler, a caliper, or a laser. The duration of the experiment and pos-
sible intermediate times at which measurements are made must also be 
decided as part of the measurement process. Obviously, the number and loca-
tions of the points at which to measure sole wear and the choice of measuring 
device  could influence our ability to detect differences in wear of the two 
shoe‐sole materials. In many fields, measurement protocols have been devel-
oped and improved over a long period of time. Ever more modern technology, 
though, can enhance response measurement.
Though we will focus in this book on single responses, it is generally appro-
priate to measure multiple characteristics—to record more than one response 
variable on each experimental unit. For the shoe experiment, we might try to 
measure comfort as well as wear. Consider an education experiment in which 
a class is the experimental unit. Tests could be given to the students at the end 
of the experiment that measure class “learning” in various subject areas. One 
could also measure class performance in these areas by average test score, the 
median test score, or some other variable such as the percentage of the class 
who exceed some threshold score, or all of these and more.
In addition to measuring responses that can reliably show treatment effects, 
other possibly influential variables (sometimes called “ancillary variables”) 
should also be measured. For example, in the boys’ shoes experiment, it might 
be pertinent to know certain characteristics of the boys, such as age, weight, 
and activity level (e.g., did a boy walk to school or not?).
Principles of Experimental Design
An experimental design consists of overlaying a treatment structure on an 
experimental unit structure and then measuring pertinent responses of the 
experimental units. The total measurement process includes experimental 
­protocol—the care and feeding of the experimental units throughout the 
course of the experiment and the controls in place to assure the experiment 
are ­carried out as designed. The way in which this overlay is done will determine 
what questions can be answered by the experiment, and it will determine the 
precision with which relationships can be characterized. Three experimental 
design principles underlie this overlay:
●
●
Replication (assignment of individual treatments to multiple experimental units)
●
●
Randomization (assignment of treatments to experimental units by using a 
chance mechanism)

21
Fundamentals of Experimental Design
●
●
Blocking (assignment of treatments within multiple groups of experimental 
units)
Application of these principles determines the validity and utility of the 
experiment.
Replication
Things vary. That’s a fundamental characteristic 
of the real world. No two plots of land, even side 
by side, and the crop‐growing environments 
they experience will be the same. Patients taking 
the same medication will respond differently. 
Neither cookies nor integrated circuits, prepared 
by the same recipe, will be identical from cookie 
to cookie or from batch to batch. The raw input 
material and the processing will vary, perhaps 
in  small ways, perhaps in large. This variability 
means that to compare three cookie recipes, for 
example, it is not enough to cook only one 
batch by each recipe. To determine whether an apparent difference is real or 
could just be “random” (meaning only due to the inherent variability of the 
phenomenon being studied), we need to know whether any apparent differ-
ences between recipes are greater than the inherent variability of multiple 
batches for cookies produced by following the same recipe. To measure that 
inherent variability, we need to “replicate”—the experiment needs to have 
multiple experimental units that receive independent applications of the same 
treatment. The variability of responses over these replications, which has 
the statistical term, experimental error, provides a yardstick against which we 
measure differences among treatments.
Replication—multiple experimental units receiving independent applica-
tions of a treatment—is different from subsampling within an experimental 
unit or repeated measurements on the same experimental unit. For 
example, suppose an experiment consisted of growing variety A tomato 
plants on one plot of land and variety B tomato plants on another plot of 
land. If we then measure tomato yield on each plant, the single plant yields 
are subsamples within a single experimental unit; they are not replicate 
experimental units.
Chemical processes are often characterized by taking measurements on a 
specimen drawn from a well‐mixed vat of material produced by a single ­process 
run. Drawing and measuring multiple specimens from one run is a case of 
­multiple measurements of one experimental unit. It does not constitute true 
replication of the process. True replication would be multiple vat loads, 
­produced independently by the same protocol.

Fundamentals of Statistical Experimental Design and Analysis
22
One more example: teaching single classes by each of four teaching 
methods, then testing the students, does not constitute true replication. A 
­student is a subsample of the experimental unit, not an experimental unit. The 
class is the experimental unit—the entity to which a treatment is applied. 
Students within a class all experience the same teaching method, taught by a 
single teacher, over the same time period, and there are all sorts of within‐class 
dynamics and personalities that introduce associations among the students 
and their performance. As mentioned earlier, a measurement on the experi-
mental unit of a class could be the class average, the class median, or some 
other measure of class performance, such as the percentage exceeding a test 
score that defines the minimally satisfactory achievement. It does not matter 
whether there are 10 students per class or 50; there is still only one experi-
mental unit per treatment. You would have to have multiple classes taught by 
each method to have true replication—to be able to make a valid comparison 
of methods. (The number of students per class is a measurement issue: the 
class average of a class of 50 students is a more precise measurement than the 
average of a class of 10 students, but the effectiveness of a teaching method 
could depend on class size; class size might a factor to be incorporated into 
the experimental design.)
Replication contributes to statistical analysis of the experimental data in two 
ways:
1.  It provides the data from which to estimate the inherent variability of 
experimental units.
2.  It influences the precision with which the treatments can be compared.
The larger the number of replications, the better one can estimate inherent 
variability and the more precisely one can estimate, for example, average 
­differences in patients’ response for different treatments. Choosing the number 
of replications is a critical part of experimental design. The choice generally 
requires a trade‐off between subject‐matter considerations—primarily cost, 
time, and the availability of people, materials, and facilities—and the statistical 
considerations of estimation precision. Guidance and methodology for deter-
mining the extent of replication are given in later chapters.
Randomization
Suppose that we have a set of experimental units and have defined the 
­treatments that will be applied. How should treatments be assigned to exper-
imental units? The answer is randomly. This means literally drawing numbers 
from a hat or simulating that drawing via a computer or other means. For 
example, if one has 20 experimental units and four treatments to assign, one 
could number the experimental units from 1 to 20; then list the numbers, 1, 2, 
3, …, 19, 20, in a computer‐generated random order; and then assign treatment 

23
Fundamentals of Experimental Design
A to the experimental units corresponding to the first five numbers, treatment 
B to the next five, etc.
One might suppose an experimenter should 
make assignments in as fair and balanced manner 
as possible by considering the characteristics of 
each experimental unit and trying to make sure 
those characteristics are balanced across the 
groups of experimental units assigned to each 
treatment. But, in general, this is very hard to do. 
Not all pertinent characteristics of the experi-
mental units are apparent—important characteris-
tics may not be realized or measurable—and there may be too many 
characteristics to attempt to balance. And, no matter how fair the experi-
menter tries to be, it is hard to prevent subtle and unintentional biases from 
influencing the assignment of treatments. For example, if, in a medical 
experiment, individual doctors are permitted to assign treatments to patients 
by their own judgment, subjective considerations are bound to intrude. One 
then will not know whether apparent treatment differences are due to biases 
in the assignment of treatments to patients or are actually due to the treat-
ments. By using a randomization device to make the treatment assignments, 
we remove the possibility of bias or the appearance of bias in the assignment, 
and we enhance the credibility of the experiment. Medical experiments are 
often done with a “double‐blind” protocol for the assignment of treatments. 
Neither the doctor nor the patient knows what medication is assigned.
A major reason for random assignment of treatments to experimental units is 
that, as will be seen in the next chapter, statistical analysis involves a comparison 
of the “data we got” to a “probability distribution of data we might have 
gotten.” Randomization gives us a means of creating that distribution of data 
we might have gotten and in so doing assures us that valid comparisons of 
treatments are obtained (Fisher 1947; Box, Hunter, and Hunter 2005).
There are situations in which randomized assignment of treatments to exper-
imental units cannot be done, ethically or legally. For example, you cannot 
recruit a large group of teenagers for a study on tobacco effects and then ran-
domly assign half of them to smoke 10 cigarettes a day and the other half not 
to smoke at all, say, for the next 10 years in both cases. In the social sciences, 
it is difficult to run a randomized experiment in which, for example, one group 
of randomly selected preschoolers is enrolled in Head Start and another ran-
domly selected group is not allowed to enroll. (There are exceptions in which 
participants are selected by lottery from a pool of applicants, although eligible 
participants whose parents did not apply would be left out, so the assessment 
of the Head Start effect would pertain to families who signed up for the Head 
Start enrollment lottery. Thus, any inference about the effect of Head Start 
would apply among children whose families applied to participate in Head 
Start.) Researchers in such situations have proposed nonexperimental 

Fundamentals of Statistical Experimental Design and Analysis
24
­alternatives for evaluating social programs, such as by matching, to the extent 
possible, voluntary participants and nonparticipants. Characteristics such as 
age, race, sex, ethnicity, zip code, and family income are candidate matching 
variables. It is not clear that these approaches are successful (Peikes, Moreno, 
and Orzol 2008). Of course, though, well‐designed experiments can occasion-
ally be misleading, also. Randomization and replication, though, limit the risk 
to a known level. That cannot be said of nonexperimental studies.
Blocking
The previous subsection on “experimental unit structure” identified one struc-
ture as groups, or blocks, of experimental units. Such groups can be based on 
inherent characteristics of the experimental units, for example, male and 
female subjects in a medical experiment or groups of experimental plots of 
land in different locations and climates for an agricultural experiment.
Blocks can also be constructed or assembled by an experimenter as a means 
of enhancing the precision with which treatments can be compared. As an 
example, consider a chemical process in which it is planned to compare four 
treatments over a total of 20 runs (experimental units), scheduled as four per 
day for one 5‐day work week. One way to run the experiment is to randomly 
assign the treatments to the 20 experimental units (five runs per treatment), 
without regard to which day any given treatment will be run. That is, the exper-
imenter could regard the 20 available time periods as a homogeneous set of 
experimental units (a single block) and randomly assign the treatments to 
them. Thus, on any day, some treatments may be run more than once, some 
not at all. Figure 2.3a shows one random assignment of the four treatments to 
the 20 experimental units (periods).
Suppose that there are day‐to‐day differences in the process that are related, 
perhaps, to day‐to‐day environmental changes, personnel assignments, or 
equipment setup activities. These day‐to‐day differences will inflate the vari-
ability of the multiple experimental units that receive the same treatment, 
relative to a situation in which there is no extraneous day‐to‐day variation. Note 
that in Figure 2.3a T1 is applied on days 1–4, while T3 is applied on days 1, 2, 
and 5. Thus, day‐to‐day variability affects the responses in a systematic way, 
and the variability of eus that receive the same treatment would be inflated by 
day‐to‐day variability. The only “clean” comparisons of T1 and T3 are in the 
tests done on days 1 and 2: both treatments were done on these two days, so 
the day‐to‐day differences cancel out of the difference between treatments.
Alternatively, an experimenter can cancel out day‐to‐day variation by block-
ing the experimental units and assignment of treatments as follows: on each 
day, each of the four treatments would be run once, randomly assigned to the 
four time periods. Figure  2.3b illustrates one such block‐by‐block random 
assignment. With this design, the treatments can be compared within each 
block (day), and then the within‐block treatment differences can be averaged 

25
Fundamentals of Experimental Design
across days. The inherent variability yardstick against which the treatment 
comparisons will be gauged (in certain situations to be discussed in the chapter 
on randomized block designs) is only within‐day variation; among‐days varia-
tion is eliminated from the comparison.
On the other hand, there is a convenient design, a simplification that a clever 
lab technician might decide would work just as well and save time: run 
treatment 1 five times on day 1, treatment 2 five times on day 2, etc. (some 
overtime might be required, but setup time between runs could be reduced, 
so maybe not), and then take Friday off. This bit of improvisation turns a day 
into the experimental unit, with five samples within each eu, but only one rep-
lication of each treatment, hence no valid comparison of treatments. You can’t 
separate treatment effects from day‐to‐day differences with this “design.” 
Variation within a day does not provide a yardstick against which to measure 
differences between different days. Shortcuts happen, often with noble 
motives. This is why successful experiments require well‐defined experimental 
protocols and oversight—control of the experimental process.
Subject‐matter knowledge is essential to an intelligent specification of 
blocks. Only someone who understands the process can identify potential 
sources of variation that can be controlled by blocking. A good statistical con-
sultant, however, will ask probing questions about possible extraneous sources 
of variability that might be controlled by blocking.
The boys’ shoes experiment discussed previously is an example of a blocked 
set of experiments in which each block (boy) consisted of two experimental 
units (feet). However, the alternative to this design is not to randomly assign 
materials to feet, ignoring the boys they are attached to. This would not be a 
sensible design. The issue in the shoe experiment is not to block or not to 
block. Rather, the issue is choice of experimental unit—boy or foot.
Some authors (e.g., Wikipedia 2014) regard blocks as “nuisance” factors: 
they are not of particular interest. An example is the blocking in Figure 2.3b, 
by day, in order to eliminate the day‐to‐day differences from the comparison 
of treatments. We’re not interested in these particular days; we’ll never see 
Day 1
(a)
(b)
Day 2
Day 3
Day 4
Day 5
T3
T1
T4
T4
T4
T3
T1
T3
T4
T2
T2
T1
T1
T2
T2
T1
T2
T3
T4
T3
Day 1
Day 2
Day 3
Day 4
Day 5
T4
T3
T2
T1
T1
T2
T4
T3
T2
T3
T4
T1
T3
T4
T2
T1
T1
T2
T3
T4
Figure  2.3  (a) Completely Randomized Assignment of Treatments. (b) Random 
Assignment of Treatments within Each Block (Day).

Fundamentals of Statistical Experimental Design and Analysis
26
them again. Of course, in the general interest of quality, one should want to 
know about and take steps to reduce day‐to‐day variation, but in experiments 
of this type, the primary motivation for the experiment is to learn about the 
treatment effects. For that purpose, day‐to‐day variation is called a “nuisance,” 
to be dealt with by blocking. But even nuisance factors can be interesting. For 
example, car‐nut lore says do not buy a car that is assembled on Monday or 
Friday. A well‐designed experiment, following the pattern in Figure 2.3b, could 
substantiate or debunk that notion. As with the case study in Chapter 1, we 
could learn how to eliminate “nuisance” sources of variability.
In many situations, the factors that define blocks are of direct interest. If we 
want to compare the effects of different diets, or medical treatments, or TV 
commercials on people, we may block our subjects by sex because we want to 
know if such treatments have different effects on men and women. Sex is not 
a nuisance. Sex is of direct interest.
Blocking also defines the scope of the conclusions that can be drawn from 
an experiment. In agriculture, a developer of a new variety of corn wants to 
know whether that variety outproduces other varieties in a (reasonably) wide 
variety of soils and growing conditions. Thus, the experiment will be blocked 
by location, with the locations selected to span the desired range of condi-
tions. A new variety that excels only in a very limited set of conditions is not as 
marketable as one that excels in a broad set of conditions. It takes blocked 
experiments to fully evaluate the new corn variety versus its competitors and 
convince farmers, many of whom have learned about experimental design at 
their university or from friendly local county agents, to buy it. The blocking 
factor—location—is not a nuisance; it’s key to the experiment.
Bottom Line: Don’t let anybody tell you that blocking is just for nuisance factors.
Control
In a textbook, it is easy to say, “Suppose an experiment was run according to 
such and such a design and resulted in the data in Table 1.” In real life, though, 
as already alluded to, there can be many a slip twixt design and data. People 
and machines are involved. One study participant can decide, “Why should I 
run the tests in that cockamamie order? It’s easier to do all of the low temper-
ature tests first, then all the high temperature tests next. I can also test many 
more items simultaneously than they apparently think I can. Whoever planned 
those tests doesn’t really understand my lab’s capabilities. I’ll simplify this plan, 
knock these tests off quickly, and go home early.” There go your careful block-
ing, replication, and randomization plans. There goes your ability to separate 
real effects and random variation, signal and noise. Consequently, there goes 
your chance to learn something useful. But don’t blame the innovative team 
member. Blame the project leaders who didn’t assure that all involved knew 
the protocol and its importance (scientific and economic—flawed experiments 
cost money, too).

27
Fundamentals of Experimental Design
Experimental protocols need to be established and implemented that 
­prevent such freelancing (and much more subtle modifications) and protect 
the integrity of the design and subsequent data analysis. Doing this may mean 
the friendly, local statistician, or research scientist will have to visit the lab, 
hospital, stores, or classrooms to see where the treatments meet the experi-
mental units. That’s a good thing.
The basic experimental designs to be discussed in Chapters 4–7 will be 
defined by their blocking, replication, and randomization, and these design 
characteristics will be emphasized. Though it will be generally assumed that 
measurements and protocols will be properly handled, the reader should 
not  lose sight of the importance of these aspects of experimental design. 
A great design on paper can fail if not diligently implemented.
The goal of an experiment is information‐laden data, precipitating from a 
well‐seeded cloud and captured for statistical analysis. Methods for distilling 
and communicating information from experimental data are the topic of the 
next chapter.
Assignment
Choose a topic of interest to you. Identify an issue that you would like to inves-
tigate with a designed experiment. Identify and discuss the experimental units, 
treatments, response measurement(s), and possible ancillary variables. 
Describe the experimental protocol for applying treatments and collecting 
data. Discuss your plans with a fellow student, a teaching assistant, or your 
instructor. Revise and repeat.
References
Box, G. E. P., Hunter, W. G., and Hunter, J. S. (1978, 2005) Statistics for Experimenters, 
1st and 2nd eds., John Wiley and Sons, New York.
Deming, W. E. (1975) On Probability as a Basis for Action, The American Statistician, 29, 
146–152.
Fisher, R. (1947) The Design of Experiments, Oliver and Boyd, London and Edinburgh.
Peikes, D., Moreno, L., and Orzol, S. (2008) Propensity Score Matching: A Note of 
Caution for Evaluators of Social Programs, The American Statistician, 76, 222–231.
Scott, A., and Triggs, C. (2003) Lecture Notes for Paper STATS 340, Department of 
Statistics, University of Auckland, Auckland.
Wikipedia (2014) Randomized Block Designs, http://en.wikipedia.org/wiki/Complete_ 
block_designs.


Fundamentals of Statistical Experimental Design and Analysis, First Edition. Robert G. Easterling. 
© 2015 John Wiley & Sons, Ltd. Published 2015 by John Wiley & Sons, Ltd.
Introduction
Once an experiment has been conducted and the data collected, the next 
task is to extract and communicate the information contained in the data 
(as depicted in the cloud cartoon; Fig. 1.1). The structure of an experiment 
­dictates, to a large extent, the nature of the statistical data analysis to be car-
ried out. (Indeed, careful planning of an experiment includes the anticipated 
analyses and even anticipated results.) In the remaining chapters in this book, 
detailed statistical data analyses will be discussed and illustrated in conjunction 
with the different experimental designs addressed. Some general principles 
and basic analyses, though, are set forth in this chapter and illustrated with 
simple two‐treatment experiments. Two types of intertwined analysis are 
­discussed—graphical and quantitative. My general approach is as follows:
Analysis 1. Plot the data! An analysis will often 
cycle between plots and calculations related to 
the plots as the message in the data is extracted 
and communicated.
Analysis 2. Do appropriate number crunching 
to characterize patterns seen in data plots, to sep-
arate and measure what is real from what could 
just be random variation, and to point the way to 
further data displays and analyses.
The two experiments addressed in some detail 
in this chapter are from the classic experimental 
design text, Box, Hunter, and Hunter (1978, 2005). The first, an experiment on 
the wear of boys’ shoes, was introduced in the preceding chapter. In this 
Fundamentals 
of Statistical Data Analysis
3

Fundamentals of Statistical Experimental Design and Analysis
30
chapter, we continue that story through several layers of analyses and issues. 
The story gets a little lengthy, but it illustrates that there can be several 
­legitimate ways to extract and communicate information from that data cloud.
Next, we take up the story of a gardener’s experiment that compares two 
tomato fertilizers. We also address what happens after the analysis: business 
decisions and consequences. Both of these examples are very simple experi-
ments, but they illuminate fundamental issues and concepts that come into 
play in all experimental contexts.
Boys’ Shoes Experiment
Experimental design
Consider again the boys’ shoes experiment introduced in Chapter 2. The data 
from that BHH experiment are given in Table 3.1. Recall that each of 10 boys 
in the experiment wore one shoe of each sole material, A or B, randomly 
assigned to the left and right feet. They wore the shoes for some period of 
time after which the percent wear was measured. Thus, and this is important, 
each measured response, the percent wear on a shoe sole, is associated with 
a boy, a foot, and a material. This association is shown in Table 3.1.
The reason for pointing out the association is that any data plot (Analysis 
1) should initially reflect this association—in all its dimensions, if possible. If, 
however, the data show no evidence of a particular association, then 
subsequent displays need not maintain the linkage. In the shoe experiment, 
the assignment of materials to feet was done by flipping a coin, with the 
result that seven boys wore material A on the left foot and three wore B, as 
is also shown in Table 3.1.
Table  3.1  Boys’ Shoes Example: Percent Wear for Soles 
of Materials A and B; Material A on the Indicated Foot.
Boy
A‐Foot
A
B
1
L
13.2
14.0
2
L
8.2
8.8
3
R
10.9
11.2
4
L
14.3
14.2
5
R
10.7
11.8
6
L
6.6
6.4
7
L
9.5
9.8
8
L
10.8
11.3
9
R
8.8
9.3
10
L
13.3
13.6
Source: Box, Hunter, and Hunter (2005, p. 81); reproduced with 
permission from John Wiley & Sons.

31
Fundamentals of Statistical Data Analysis
Here’s a design issue right off: an alternative experimental design would have 
balanced the left/right (L/R) assignments—five randomly selected boys would 
have been assigned material A to their left feet, and the other five would have B 
on their left feet. If the experimenters had thought the L/R choice might have an 
appreciable effect, they might have incorporated such a balancing constraint in 
the design (it can be shown mathematically that equal replication maximizes the 
precision with which any L/R difference can be estimated). The experimenters 
(relying on subject‐matter expertise—knowledge that both feet must experi-
ence very similar conditions) may not have expected a bias toward one foot or 
the other and so did not balance the L/R assignments, but because they took 
the precaution of recording the assignments, we can check that possibility.
(Though not given in the example, for a carefully carried out experiment, 
one might expect or hope that other, “ancillary,” data pertaining to the boys 
would have been obtained, such as age, weight, and the number of days the 
shoes were worn. The analysis would also look for relationships between these 
variables and shoe wear.)
Graphical displays
There are several ways to display the shoe‐wear data. Because there is a pair 
of data for each boy, one appropriate plot is a scatter plot (an “XY (Scatter)” 
plot in Excel terminology, “Scatterplot” in Minitab) of the data pairs. Figure 3.1 
shows a scatter plot of the B‐material wear (Y axis) versus the A‐material wear 
A (%)
B (%)
15
14
13
12
11
10
9
8
7
6
15
14
13
12
11
10
9
8
7
6
A-foot
L
R
Scatter plot shoe wear: material B vs. material A
Figure 3.1  Scatter Plot of B‐Material Wear versus A‐Material Wear; separate plotting 
symbols for the left and right foot assignments of material A.

Fundamentals of Statistical Experimental Design and Analysis
32
(X axis), for the 10 boys, with separate plotting symbols used for the foot on 
which the material A shoe was worn. If it was important to know which point 
corresponded with which boy, the points could have been labeled with the boy 
number. Figure 3.1 also shows an overlay of the equal‐wear (45°) line. This line 
facilitates the visual comparison of shoe sole materials. Points above the line 
are cases for which there was more wear on the B‐material sole than on the 
A‐material sole and vice versa for points below the line.
From Figure 3.1, it is fairly clear that material A generally wore better (less 
thickness loss) than B in this experiment: in eight of the 10 cases, there was less 
wear with A than with B (the points above the diagonal line). In the two situa-
tions in which B wore less than A, the difference was comparatively small—
these two points being quite close to the diagonal line. Furthermore, there is 
no evident separation of the three “A‐right” points and the seven “A‐left” 
points, so the expectation that there would be no foot bias is supported by the 
data. Subsequent plots will therefore not maintain the L/R distinction.
Although the data favor material A, the differences appear to be small, 
especially in comparison with the variation among boys. The wear percentages 
range from about 6 to 14% across the 10 boys, but the A–B differences, as can 
be seen from Table 3.1, are generally less than a percentage point. The impor-
tant questions of whether the wear differences are statistically or practically or 
economically significant will be addressed later in this analysis.
Two other plots make it easier to see the differences between A and B for 
the 10 boys (compared to reading the distances of the points in Fig. 3.1 
from the diagonal line). One is a scatter plot of the A and B wear percents 
versus boy number, shown in Figure 3.2. Now, there is no intrinsic ordering 
Boy no.
Wear %
10
9
8
7
6
5
4
3
2
1
15
14
13
12
11
10
9
8
7
6
Material
A
B
Scatter plot of wear % vs. boy no.
Figure 3.2  Wear % by Material and Boy.

33
Fundamentals of Statistical Data Analysis
of boys—it could have been alphabetical ordering, or by the date they 
turned in their shoes, or completely haphazard. So, the purpose of the plot 
is not to look for a relationship between shoe wear and boy number. The 
purpose is to facilitate the comparison of materials A and B across the  
10 boys. (Note that if the boys had been characterized by variables such as 
age, weight, or number of days the shoes were worn, then it would have 
been meaningful, and maybe ­informative, to have plotted the A and B wear 
data vs. such variables.)
Figure  3.2 shows clearly that A “won” eight of the 10 comparisons (less 
wear! more often!) and that in the two cases in which B won (boys 4 and 6), 
the difference was quite small in comparison with A’s winning margins. Details 
are still to come, but the thoughtful reader may have some intuition that the 
probability, say, of getting as many as eight heads (wins for A) in 10 fair tosses 
(boys) of a fair coin is fairly small, so, by comparison, the fact that A won eight 
of 10 comparisons is at least a hint that the material difference is “real,” not 
just random.
One other plot that can be used to show these data is a line plot. This plot 
(Fig. 3.3) is simply a collection of lines connecting the A and B wear data for 
each boy separately. We see that eight of 10 of the lines slope upward, again 
indicating more (worse) wear for B than A in eight of the 10 cases, while two 
lines have slightly negative slopes, reflecting the two cases in which B had less 
wear than A. The line plot is useful for a small number of comparisons, but if 
we had many more than 10 cases, the plot would become unreadable.
B
A
15
14
13
12
11
10
9
8
7
6
Wear %
10
1
2
3
4
5
6
7
8
9
Boy
Line plot of wear %, A vs. B, 10 boys
Figure 3.3  Line Plot of Wear Data for Materials A and B.

Fundamentals of Statistical Experimental Design and Analysis
34
Figure 3.3 also shows quite markedly again the substantial differences 
among boys. The amount of sole wear ranged roughly from 6 to 14%. A 
shoe manufacturer is probably more interested in extremes than in average 
wear. If the company conducting this experiment could identify the factors 
leading to relatively high sole wear, say, physical characteristics of the boys 
or their activities, they might be able to design a more robust shoe and 
expand their market. Or they could print a warning on the box: Your Momma 
says, “Pick your feet up!” Figure 3.3 also shows the close association bet-
ween shoe sole wear on a boy’s two feet. The differences between feet are 
small relative to the differences among boys. Everywhere one foot goes the 
other goes, too.
Significance testing
Displays of the data from the shoe experiment (Figs. 3.1–3.3) showed that sole 
material A, the currently used material, tended to wear better than the cheap 
potential substitute, material B. The differences varied, though, among the 
10 boys, which is not surprising: shoe sole manufacturing and, especially, shoe 
wear are not perfectly repeatable or perfectly measured processes. There is 
bound to be some inherent variability of wear, even if the two shoes worn by a 
boy both had the same sole material and even if the two shoes traveled the 
same terrain. The question to be addressed is: Even in light of this inherent 
process and product variability, is there evidence that one material is better 
than the other? And if so, how much better?
Statistical methods address these questions by making comparisons:
We compare the “data we got” to a probability distribution of “data we 
might have gotten” (under specific assumptions).
This comparison is the basic idea of statistical “significance testing.” 
To develop this technique, the concept of a “probability distribution of data 
we might have gotten” needs to be explained. This requires a discussion of 
probability. Probability provides the framework against which an experiment’s 
data can be evaluated.
Probability and probability distributions
It is natural to think of probability in terms of games of chance. In a single 
fair toss of a fair coin, there are a probability of .5 that the result is a head 
and, consequently, a probability of .5 that the result is a tail (a 50–50 chance 
in common lingo). In lotteries, assuming a fair physical or computerized 
method of generating random numbers, the probability of a winning 
combination of numbers can be calculated (by the number of combinations 
that are winners divided by the total number of possible combinations). 

35
Fundamentals of Statistical Data Analysis
What the term probability means is that if, say, coin tossing was repeated 
an infinite number of times, a head would occur in half of the trials. 
Further,  the sequence of heads and tails would be “random”; each out-
come is independent of all the others. In the other example, the lottery 
would be won in the calculated fraction of a conceptual infinite replay of 
the lottery.
What, though, can we expect in a limited number of trials, say, 10? If a fair 
coin is fairly tossed 10 times, the 11 possible outcomes range from 10 heads 
and zero tails to zero heads and 10 tails. Intuitively, we know that some out-
comes, such as 5 heads and 5 tails, are more likely (or probable) than other 
outcomes in that they would occur more often in repeated sets of 10 tosses 
than the extreme results of 10 heads or 10 tails. Probability theory supports 
and quantifies this intuition. Numerous science fair projects have tested the 
underlying theory.
Probability theory tells us the following: under the assumption of n 
“independent trials,” each with probability p of a particular outcome, the 
probability of observing exactly x of these outcomes is given by what is 
called the binomial probability distribution (Wikipedia 2014a and numerous 
statistical texts). (Independent trials mean that the outcome of one trial 
does not affect the outcome of other trials.) This probability distribution is 
a mathematical function that gives the probability of all of the possible out-
comes, x = 0, 1, 2, …, n − 1, n. The mathematical expression for the binomial 
probability distribution is given in many statistical texts, and the distribu-
tion is available in spreadsheet software such as Excel and in statistical 
packages such as Minitab. Appendix 3.A to this chapter gives the formula 
for the binomial distribution and discusses statistical aspects of the distri-
bution. For our present purposes, we will rely on software and forego the 
mathematics. Trust us.
For the fair coin‐tossing case of n = 10 and p = .5, the binomial probability 
distribution of the number of heads is tabulated and plotted in Figure 3.4. 
Figure  3.4 shows that the probability of all heads (or all tails) is .001 
­(actually .510 = 1/1024 = .00098). The most likely outcome, 5 heads and 5 tails, 
has a probability of almost 25%. This means that if a fair coin was fairly flipped 
10 times, over and over, the proportion of cases in which five heads and five 
tails would result is .246. Other outcomes have lower probabilities that 
decrease as possible outcomes differ more and more from the most likely out-
come of five heads and five tails. The probabilities of the 11 possible outcomes 
sum to 1.0. (It is a property of probability distributions that the sum of the 
probabilities for all possible outcomes must equal 1.0.) Note also that this dis-
tribution is symmetric: the probability of, say, 3 heads and 7 tails is the same as 
the probability of 7 heads and 3 tails, namely, .117. For a biased coin, in which 
case the probability of a head on a single toss is not .5, the distribution would 
not be symmetric.

Fundamentals of Statistical Experimental Design and Analysis
36
Sign test
Why do we care about this particular binomial distribution? Our interest is com-
paring shoe sole materials, not flipping coins. Well, if there really is NO difference 
between materials, the outcome, “A wears less than B,” would be expected to 
occur half the time, like the heads outcome for a fair coin toss. In this case, the 
wear test results for 10 boys would then be analogous to, or comparable to,  
10 fair tosses of a fair coin. To evaluate the viability of the “hypothesis” of no 
difference between materials, it is thus appropriate to compare the experimental 
outcome (“the data we got”), namely, that eight of 10 cases had the result, 
A  wears less than B, to the “probability distribution of data we might have 
gotten under the assumption of no real difference in materials.” This distribution 
is the binomial probability distribution with p = .5 portrayed in Figure 3.4.
Figure  3.5 shows the comparison. In statistical terminology, the binomial 
­distribution to which the experimental outcome is compared is called the 
­“reference distribution.” We “refer” the data (we got) to this probability distri-
bution (of data we might have gotten) to evaluate the degree of agreement of 
the data with the situation of no real difference between materials.
The message from the comparison in Figure 3.5 is that the outcome, A wins 
eight times in 10 trials, is fairly unlikely, just by chance, if there was no under-
lying difference between A and B. In particular, the probability of that particular 
outcome is only .044.
So, have we proved that A is better than B, beyond a reasonable doubt, as 
is required in jury trials? No, not at all. The evidence supporting that claim is 
Binomial distribution: n = 10, p = .5
.001 .010
.044
.117
.205
.246
.205
.117
.044
.010 .001
.00
.05
.10
.15
.20
.25
.30
0
1
2
3
4
5
6
7
8
9
10
x = number of heads
Prob.
x
B(x:10, .5)
0
.001
1
.010
2
.044
3
.117
4
.205
5
.246
6
.205
7
.117
8
.044
9
.010
10
.001
Figure  3.4  Binomial Distribution. B(x:10, .5) denotes the probability of x heads in 
10 fair tosses of a fair coin.

37
Fundamentals of Statistical Data Analysis
strong, but not absolute. If A had won nine of the comparisons, the evidence 
would be stronger; if A had won all 10 comparisons, we would still not be 
absolutely certain that A was better—just by chance there is still a .001 proba-
bility of that extreme result just by chance. This is the sort of uncertainty we 
have to cope with in interpreting experimental data and making decisions 
based on the data and our subject‐matter knowledge (statistics means never 
having to say you’re certain). In spite of this uncertainty, we are obviously more 
informed having done the experiment than if we had not.
Figure 3.5 shows the comparison of data we got to the distribution of 
data we might have gotten. The picture tells the story. The picture is a little 
indefinite, with respect to an unequivocal decision about the equality of the 
materials, but that’s because of the limited amount of data available to test 
the hypothesis of equality. As shown in the following subsections, by other 
analyses, we can sharpen the comparison substantially, but not eliminate  
all uncertainty.
Graphical comparisons of the data we got to a reference distribution of 
data we might have gotten can become a little unwieldy and take up too 
much space in a report or text. Statistical convention is to summarize this 
picture by a number called the “P‐value.” The P‐value tells the reader how 
far out on one tail or the other of the reference distribution that the data 
we got fall. (Most distributions we deal with, such as the upcoming “bell‐
shaped curve” and the Normal distribution, are shaped so that the occurrence 
Binomial distribution: n = 10, p = .5
.001
.010
.044
.117
.205
.246
.205
.117
.044
.010
.001
.00
.05
.10
.15
.20
.25
.30
0
1
2
3
4
5
6
7
8
9
10
x = number of heads
Prob.
Shoe exp. 
outcome
Figure 3.5  Comparison of Shoe Experiment Results to the Binomial Distribution for 
n = 10, p = .5.

Fundamentals of Statistical Experimental Design and Analysis
38
probability of possible outcomes decreases in the tails of the distribution.) 
More technically:
●
●
The P‐value equals the probability of an outcome that disagrees with the 
hypothesis, or assumption, used to construct the reference distribution, by 
as much as or more than the observed data do.
In our case, the outcomes that define the P‐value are the cases when A wins 8, 9, 
or 10 of the comparisons. The outcome of 8 wins for A is what was observed; 9 and 
10 are the other outcomes that define the upper tail: more decisive wins for A. 
Thus, the probability of these outcomes, by Figure 3.8, is P = .044 + .010 + .001 = .055. 
Because we considered only those cases for eight or more wins by A, which 
corresponds to the upper tail of the probability ­distribution in Figure 3.8, this would 
be called an upper one‐tail P‐value. The P‐value tells us that the data we got 
correspond to the outcome that defines the upper .055 (or 5.5%) tail of its ref-
erence distribution. That is, reporting that the upper‐tail P‐value is .055, in this 
case, is numerical shorthand for the picture showing that the data we got fell 
at the .055 point of the upper tail of its reference distribution of possible data, 
calculated under the assumption of no real difference between the two materials.
In this situation, subject‐matter knowledge (presumably) tells us that 
A should wear better than the cheaper material, B. That’s why B is cheaper. 
So, it is appropriate to focus our interest and analysis on the cases in which 
A won eight cases or more—the upper tail of the reference distribution. (If A 
had won only two of the 10 comparisons, or fewer, the message would be that 
B is cheaper and wears better. Changing to B is then a win–win situation.)
The process we have just gone through is called a “significance test” in 
statistical literature. Because this analysis only considered the direction, or sign 
(positive or negative) of the A versus B comparisons, the particular test used 
here is called the “sign test” (Wikipedia 2014b). The reference distribution for 
the sign test, in this example, is the binomial distribution of positive or nega-
tive outcomes, or heads and tails by analogy, for the case of n = 10 and p = .5.
Misinterpretation of P‐values
There is a tendency to misinterpret a P‐value as the probability, in this example, 
that the A and B shoe sole materials wear the same. However, “A and B shoe 
sole materials wear the same” is not a random variable, like the outcome of  
10 tosses of a fair coin. This proposition doesn’t have a probability distribution, 
so you cannot make probability statements about it.
The P‐value is simply a numerical summary of the comparison of the data we 
got (8 wins out of 10 trials for A) to the binomial distribution of data we might have 
gotten, if the proposition of no difference was true, in which case that distribution 
is the binomial distribution with p = .5. The P‐value summarizes that comparison by 
telling us how far out on the tail of that distribution that the experiment’s outcome 

39
Fundamentals of Statistical Data Analysis
fell. The smaller the P‐value, the sharper the level of disagreement between the 
data we got and the distribution of data we might have gotten.
As will be seen in the following sections, there can be more than one way to 
summarize the data we got and make the comparison to data we might have 
gotten, under the situation in which A and B wear the same.
Also, a P‐value does not tell one anything about the magnitude of the effect 
that is being estimated from the data. For example, the small P‐value for the 
boys’ shoes sign test does not indicate how much difference there is between 
the underlying probability that A wears better than B and the hypothesized 
value of p = .5. Statistically significant and practically meaningful are not the 
same thing. P‐values have been a subject of much discussion in the scientific 
literature. For a good summary of the issues, see Nuzzo (2014).
Randomization test
The sign test we just carried out was based on considering only the sign of the 
B–A differences, case by case. In eight of the 10 cases, that difference was 
positive; B wore more than A. Summarizing the data in this way ignores the 
sizes of the differences in wear percentages. A large difference is not distin-
guished from a small difference with the same sign. The magnitudes of the 
differences tell us more about the A versus B difference. As we saw in the data 
plots, material A generally won by a larger margin in its eight wins than B did 
in its two wins. We can make a more sensitive comparison of the two materials 
if we consider the sizes of the differences. Size matters.
Think again about the hypothesis (assumption) of no real difference bet-
ween the two materials. If that assumption is true, then the observed differ-
ences just reflect the random outcomes of assigning A and B to left and right 
feet. Boy 1 had A on his left foot and recorded 13.2% wear for A and 14.0% for 
B on his right foot. If the randomization had put B on his left foot, then, 
assuming no difference between materials, boy 1’s data would have been 
14.0% for A and 13.2% for B. That is, his observed B–A difference in wear 
could have been either .8 or −.8%, each with probability .5. Similarly, for the 
rest of the 10 boys, each of their B–A differences would have been changed in 
sign if the foot assignment had been reversed. There are thus 210 = 1024 pos-
sible (and equally likely) outcomes for the signed differences between B and A 
under the assumption of no real difference between materials.
To compare the data we got to the distribution of data we might have 
gotten, if there was no difference between materials A and B, we need a sum-
mary statistic that reflects the size of the difference. A natural statistic is the 
average difference. For the observed data, the average difference between 
the B and A wear percentages (taking B–A) is .41%. We will call this average 
difference dbar. Now, for each of the 1024 possible A/B foot assignments, we 
can calculate the resulting d‐values and their average, dbar. For example, if all 
10 assignments were the opposite of the assignments in this experiment, dbar 

Fundamentals of Statistical Experimental Design and Analysis
40
would equal −.41%. BHH (2005) did the full set of calculations to create the 
probability distribution of possible dbar’s, compared the “dbar we got” to this 
reference distribution, and found that only three of the 1024 possible average 
differences were greater than .41%; four of them were exactly .41%. With a 
“continuity correction,” they counted half of the latter four outcomes to obtain 
a (one‐tail) P‐value of 5/1024 = .005. The picture that this P‐value summarizes is 
a histogram of all 1024 possible dbars, under the assumption of no difference 
between A and B materials, with the outcome we got, dbar = .41%, corresponding 
to an upper‐tail probability of .005. This P‐value is substantially stronger evi-
dence of a real difference than the P‐value of .055 for the sign test. This smaller 
P‐value tells us that the data we got are more extreme with respect to the ran-
domization test than they were with respect to the sign test. This smaller P‐value 
means that the evidence against the assumption of no real difference between 
materials is stronger using the randomization test than it is using the sign test.
This calculation of the randomization test (Wikipedia 2014c) is particularly 
appropriate when in fact the experiment was conducted by randomly assign-
ing materials to feet, as was the case here. Random assignment of treatments 
to experimental units establishes the validity of the randomization test. That’s 
important. It justifies comparing “the data we got” to a reference distribution 
of “data we might have gotten” based on the assumption of random treatment 
assignments. Similarly, randomization established the validity of the sign test. 
The two tests gave different result because two different summary statistics 
and corresponding reference distributions were used in the analyses.
The small P‐value of .005 means that the observed outcome is in the extreme 
upper tail of the distribution of average differences generated under the assump-
tion (hypothesis) that there is no difference between materials A and B. So, it is 
quite unusual (though still not impossible) to get a result, just by chance, in which 
the average B–A difference is as large as or larger than the experiment’s result 
of dbar = .41%. We have quite strong evidence that there is a real difference, on 
average, between the sole materials. Whether that average difference is impor-
tant in selling shoes, the question that motivated this experiment, remains to be 
determined. We’ll get to it. But first, let’s consider one other approach to choos-
ing the distribution of “data we might have gotten.”
Normal distribution theory t‐test
The Normal distribution is a mathematical function that defines a “bell‐shaped 
curve.” This curve, shown in Figure 3.6, is an example of a probability density 
function. The vertical axis is probability density. The density function has the 
property that the total area under the curve is 1.0, just as the sum of the prob-
abilities of the eleven discrete outcomes of the binomial distribution in 
Figure 3.4 is 1.0. (The vertical axis for the other probability density functions 
illustrated in this text will not be labeled because it is not of intrinsic interest.) 
The Normal distribution, however, pertains to the distribution of a continuous 

41
Fundamentals of Statistical Data Analysis
variable, x. If you draw, or generate, a random value of x from a Normal distribution, 
the probability that x falls in a particular interval, say, from a to b, is given 
by the area under the curve between a and b. Software or the use of widely 
available tables of the Normal distribution can be used to calculate these 
probabilities.
The Normal distribution is a mathematical ideal, but real‐world populations 
may be adequately approximated by it. The more important characteristic, 
though, for analyzing real‐world data is that random samples from a Normal 
distribution (e.g., computer generated) often look like real data, whether small 
numbers of observations or large. There are gaps, clusters, apparent outliers, 
longer tails in one direction or the other, etc. That is, real data we get from 
experiments and other sources can often look like a “random sample” from a 
Normal distribution (meaning independent observations generated, e.g., by a 
computer programmed to do so; see Appendix 3.B for a demonstration of 
random sampling from a Normal distribution). This is quite fortunate because 
an awful lot of statistical theory has been built on the model (assumption) of 
data obtained by random sampling from a Normal distribution. So, random 
samples from a Normal distribution can serve as a frame of reference and 
source of reference distributions for the “data we got.”
The Normal distribution, as a mathematical function, is characterized by two 
parameters—two quantities in the mathematical expression for the distribution. 
These parameters determine where the curve is centered and how spread out it 
is. Conventional symbols used for these parameters are μ (mu) for the distribution 
z
4
3
2
1
0
–1
–2
–3
–4
.4
.3
.2
.1
.0
Normal density function; mean = 0, standard deviation = 1.0
Figure 3.6  The Standard Normal Distribution. Statistical convention is to denote a 
variable that has the standard normal distribution by z.

Fundamentals of Statistical Experimental Design and Analysis
42
mean (center) and σ (sigma) for the distribution standard deviation (spread). The 
standard Normal distribution in Figure 3.6 corresponds to μ = 0 and σ = 1.0. (By 
way of comparison, the two parameters of the binomial distribution are n and 
p, and they define a particular binomial distribution and together determine 
the center and spread and shape of a particular binomial distribution.)
The Normal distribution is symmetric about its center, which is the distribu-
tion mean, μ. The Normal distribution also has the properties that 95% of the 
distribution falls in the interval, μ ± 1.96σ (typically rounded to μ ± 2σ), and 68% 
of the distribution falls in the interval, μ ± 1.0σ. Thus, the larger σ is, the more 
spread out the distribution is. As mentioned, software can calculate any prob-
abilities of interest for a Normal distribution, given input values of μ and σ. 
Textbooks generally have a table of standard Normal distribution probabilities 
and percentiles. Much more extensive discussions of the Normal distribution 
can be found in many statistical texts. My focus in this chapter is on how the 
Normal distribution can help us evaluate the difference between shoe sole 
materials and, still to come, tomato fertilizers.
Figure 3.7 shows an individual value plot (from Minitab) of the 10 differ-
ences—yet another way to display this experiment’s data. As we have noted 
before, two of the differences are negative; the other eight are positive. The 
pattern of variability among these 10 data points is not at all unusual when 
sampling from a Normal distribution (see Appendix 3.A), so to develop a 
­reference distribution based on the Normal distribution model is a reasonable 
thing to do. (Ties, which are unusual for a continuous variable, result from 
the resolution of the measurements—rounded to one‐tenth of a percent.)
1.2
.9
.6
.3
.0
Diff. (B–A)
Individual value plot of diff. (B – A)
Figure 3.7  Individual Value Plot of Shoe‐Wear Differences (%). The average difference 
is indicated by the blue symbol.

43
Fundamentals of Statistical Data Analysis
Now, if there is no real difference between materials A and B, the appropriate 
Normal distribution model for measured differences would have a mean of 
zero. Thus, to address the question whether it is real or random, we will compare 
the data in Figure 3.7 to data that could have come from a Normal distribution 
with a mean of zero (but with unspecified standard deviation).
Eyeball‐wise, is it easy to imagine a Normal distribution centered at zero 
yielding a random sample as off‐centered from zero as the data in Figure 3.7? 
I don’t think so, but that’s a subjective impression based on my vast experience. 
The following analysis calibrates this visual impression.
As in the previous two analyses (the sign test and the randomization test), 
the comparison of the data we got to the distribution of data we might have 
gotten from a Normal distribution will be done using a summary statistic. In 
this case (theory tells us—Trust me!), the appropriate summary statistic is what 
statisticians call the t‐statistic. This statistic is a function of the sample size, n, 
the data average, dbar, and s, the standard deviation of the observed differ-
ences. The sample standard deviation is equal to
s
d
dbar
n
i –
,
2
1
where di represents the ith wear difference, B–A, for the ith boy. For a random 
sample of data from a Normal distribution, s is an estimate of σ.
In particular, Normal distribution theory tells us that in random sampling 
from a Normal distribution with mean μ, the statistic
t
dbar
s
n
/
has a known probability distribution, known as the “Student’s t‐distribution.” 
This relationship is what links the data we got to the distribution of data we 
might have gotten for a particular value of μ. The t‐distribution depends only 
on a parameter called the degrees of freedom (generally abbreviated df) asso-
ciated with the standard deviation, s, namely, n − 1. That is, the distribution 
does not depend on the Normal distribution’s unknown mean, μ, or standard 
deviation, σ. For moderately large n (say, n > 30), the t‐distribution is closely 
approximated by the standard Normal distribution.
The term “degrees of freedom” needs some explanation. The deviation of 
the ith difference from dbar is di − dbar. The above formula for s involves the 
sum of the squares of these deviations. A mathematical property of the 
unsquared deviations is that they sum to zero. This means that if you arbitrarily 
specified n − 1 of these deviations, the remaining deviation would be deter-
mined by subtraction (because the sum of all the deviations has to be zero). 
Hence, in engineering terminology applied to statistics, there are n − 1 degrees 
of freedom associated with the standard deviation, s.

Fundamentals of Statistical Experimental Design and Analysis
44
If there is no real, underlying, difference between the two materials, then 
μ = 0. Substituting 0 for μ in the above expression for t leads to the test sta-
tistic, t
dbar
s
n
/
/
. The distribution of t‐values we might have gotten 
when μ = 0 is the t‐distribution with n − 1 degrees of freedom. Thus, calculating 
the t‐statistic based on the data we got and comparing calculated t to the  
t‐distribution with n − 1 df provide another significance test for the comparison 
of the wear qualities of the two materials.
For the shoe data,
n
dbar
s
d
dbar
n
i
10
41
1
39
2
;
.
%;
.
%;
t
.
.
/
. .
41
39
10
3 4
The t‐distribution, more appropriately the family of t‐distributions, is widely 
tabulated and available in software. Figure 3.8 displays the t‐distribution with 
9 df and shows where our observed t‐value of 3.4 falls on this distribution 
and  the corresponding tail probability, the P‐value. Under this distribution, 
the probability of a t‐value greater than or equal to 3.4 is .004. This one‐tail 
P‐value summarizes the graphical comparison in Figure 3.8 and indicates that 
the t‐value we got is rather unusual if there is no difference between shoe 
t
3.4
.004
0
t-distribution with 9 degrees of freedom
Figure 3.8  Comparison of the Observed t‐Value (3.4) to the t‐Distribution with 9 df.

45
Fundamentals of Statistical Data Analysis
materials. This t‐test P‐value is quite close to the .005 obtained under the 
­randomization test, which hints at another reason that an analysis based on the 
Normal distribution can often be used: the t‐test based on Normal distribution 
theory often provides a good approximation to the “exact” randomization 
test, a test which depended only on the assumption of random assignment of 
treatments to experimental units. Thus, we can often use the extensive Normal 
distribution‐based analysis methods in place of the less available and some-
times complex randomization analyses.
Figure 3.8 and the P‐value summarizing that comparison tell us that the evi-
dence is strongly against concluding that the observed (average) difference 
between materials is purely random. The evidence strongly indicates that the 
difference is real because it is very rare that a random sample from a Normal 
distribution with a mean of zero could yield data as far offset from zero as our 
observed shoe material differences (see Fig. 3.7).
This analysis is known as the “paired t‐test analysis,” and it can be carried 
out by various statistical software packages. Table  3.2 shows the Minitab 
output for this analysis.
Table 3.2 introduces some new terminology: the column labeled SE Mean, 
which denotes the standard error of the mean, discussed in more detail below 
and in Appendix 3.B. The mean of interest in this analysis is the average wear 
difference, dbar = .41. The standard error associated with dbar is simply the 
denominator of the above t‐statistic: SE stdev/
.
.
n
387 10
122.
The ratio of dbar to its standard error is t = .41/.122 = 3.4. The t‐value of  
3.4 means that the difference between the observed average wear difference 
of .41% and zero is equal to 3.4 standard errors.
(For reasons to be discussed later, Table 3.2 pertains to the case of a two‐
tailed significance test: the P‐value is the tail above t = 3.4 and below t = −3.4. 
Thus, the P‐value in Table 3.2 is twice the upper‐tail P‐value, rounded. Minitab’s 
two‐tail analysis also includes a 95% confidence interval on the underlying 
average difference which will be discussed in the following and used in 
subsequent analyses.)
Excel’s® analysis of the shoe data is shown in Table 3.3.
Table 3.2  Minitab Output for Paired t‐Test: Boys’ Shoes.
Paired T for B–A
N
Mean
StDev
SE Mean
B
10
11.04
2.52
.796
A
10
10.63
2.45
.775
Difference
10
.41
.387
.122
95% CI for mean difference: (.13, .69).
t‐test of mean difference = 0 (vs. not = 0):  
t‐value = 3.35, P‐value = .009.

Fundamentals of Statistical Experimental Design and Analysis
46
In Table 3.3, Pearson’s correlation is a summary statistic that measures the 
linear association of the A and B results. Graphically, it measures the linearity of 
the A–B data scatter plot in Figure 3.1. If the data points fell exactly on a straight 
line with a positive slope, the correlation coefficient would be 1.0. Perfect 
­linearity with a negative slope would have a correlation coefficient of −1.0.
Summary and discussion: Significance tests
For the boys’ shoes experiment, we have illustrated the process of comparing 
“the data we got” to the distribution of “data we might have gotten” in three 
ways—three summary statistics giving rise to three reference distributions.
Sign test
The summary statistic used was the number of boys (eight), out of 10, for 
which B had more wear than A. The reference distribution was the binomial 
distribution based on the assumption (hypothesis) that the underlying proba-
bility of A winning the wear comparison was p = .5. This comparison is shown 
in Figure 3.5 and the test’s P‐value was .055.
Randomization test
The summary statistic was the average wear difference, B–A, of .41%. The ref-
erence distribution was the collection of all 1024 possible average differences 
corresponding to all possible random assignments of plus or minus signs to 
the observed 10 differences between left and right shoe wear. BHH generated 
that distribution and this test’s one‐tail P‐value was .005.
t‐Test
The summary statistic was the t‐statistic, calculated under the hypothesis that 
the underlying mean difference in wear was μ = 0. The reference distribution 
Table 3.3  Excel Paired t‐Test Analysis of the Boys’ Shoe Data.
t‐Test: Paired Two Samples for Means
B
A
Mean
11.04
10.63
Variance
6.34
6.01
Observations
10
10
Pearson correlation
.99
Hypothesized mean diff.
0
df
9
t Stat
3.35
P(T ≤ t) one tail
.004
P(T ≤ t) two tail
.009

47
Fundamentals of Statistical Data Analysis
was the t‐distribution with nine degrees of freedom, generated from the 
assumption of an underlying Normal distribution of wear differences, centered 
on μ = 0, and the resulting one‐tail P‐value was .004.
Now, it should not be surprising or a concern that three ways of summarizing 
the data and creating corresponding reference distributions yield different 
answers. The messages, though, are all complementary: the experimental data 
all point to the conclusion that, for different ways of looking at the data, the 
apparent differences between materials are not just due to chance.
Nor is there any reason to expect or insist that only one answer is “right.” 
Theory would dictate a best answer only under specific assumptions. If it is 
assumed that the boys were a random sample from an assumed population 
and that the wear differences for that population have a Normal distribution, 
then the t‐test is optimum. But, as has been discussed, the boys were likely a 
“judgment sample,” not a random sample from a defined population. Also, 
the assumption of Normality, while plausible, is at best an approximation to 
the ill‐defined population’s actual distribution. If it is assumed that the shoe 
wear on the boys’ left and right feet would have been the same as they were 
in this experiment, even if the shoe was on the other foot, so to speak, then 
the randomization test is valid. The sign test rests on the weakest assumptions: 
nothing is assumed about the magnitude of the shoe‐wear percentages; only 
the A or B winner on each boy is considered. So, this test ignores pertinent 
information, which is not an optimum thing to do. Nevertheless, the sign test 
is an easy, readily communicated, first way to look at the data and that is a 
valuable asset.
One further point is that in all three analyses, the conclusions apply just to 
the boys in the experiment: the shoe‐wear differences among these 10 boys 
are unusual just due to chance; there must be a real underlying difference and 
that difference is large enough to stand out from the inherent variability of the 
experimental units. Any inference that these results apply to the general 
population of shoe‐wearing boys depends on knowledge about these boys 
relative to that population. That knowledge is subject‐matter knowledge, not 
statistical inference or magic. It is based on the way the boys were selected 
and what a shoe manufacturing company knows about their shoe‐wearing 
habits relative to those of the general population. This dependence of the 
experiment’s meaningfulness and utility on subject‐matter knowledge puts 
pressure on the experimental design to assure that the experiment involves 
meaningful experimental units—both in number and nature—as we discussed 
in Chapter  2. Good statistics relies on good subject‐matter involvement. 
Understanding this interaction creates buy‐in from all involved in planning 
and conducting experiments, interpreting the results, and acting on the 
informa­tion obtained.
The reader should not despair. This example and its analyses do not support 
the old saw that “Statisticians can get any answer you want.” Statistics is about 
dealing with uncertainty. It must be recognized, as just illustrated, that there 

Fundamentals of Statistical Experimental Design and Analysis
48
can generally be more than one way to measure uncertainty. But, nevertheless, 
we learn from data; we advance the state of knowledge. We have learned that 
it would be unusual for the observed differences in this experiment to occur 
“just by chance.” We need to examine the implications of that difference. 
Should we switch to the cheaper material? Will the customer notice the 
difference and stop buying our shoes? Is it right to sacrifice quality for profits? 
These questions, which are much more important than whether we should use 
a randomization test or a t‐test, are addressed in the following.
The data analysis process illustrated here, at some length, is generally called 
“significance testing” and sometimes “hypothesis testing.” The formal approach 
to hypothesis testing (well covered in many statistical texts) is to express the 
problem in terms of decision‐making. A “null hypothesis” is stated (such as μ = 0) 
and, in essence, the decision rule is that if the resulting test statistic falls in a 
particular region of the reference distribution (generally either one selected tail 
or two tails) having a specified occurrence probability (often .05), the hypothesis 
will be rejected. Some regulatory or quality control applications, in which the 
“false‐positive” probability must be controlled, call for this formality. Information‐
gathering, knowledge‐generating experiments have a less prescribed objective. 
It’s what we learn from the comparisons of data we got to the distribution of 
data we might have gotten that is the objective in this context. Decisions, such 
as what shoe sole material to use, require (in Archie Bunker’s terminology) 
“facts,” not just “statistics.”
Economic analysis: The bigger picture
Material B is cheaper than A and it doesn’t wear as well. If the shoe manufac-
turer switches to B, the company will save money but may lose customers if it 
becomes apparent that the shoes do not wear as well as what customers have 
come to expect. Let’s examine that trade‐off.
In the experiment, the average percent wear over the duration of the 
experiment was about 10%, and the B–A average difference was about .4%. 
Suppose that shoe wear‐out is operationally defined as 50% wear. That amount 
of wear, according to shoe lore, let us say, is the approximate condition that 
would prompt a boy’s parents to buy new shoes. Let’s project forward and sup-
pose that the B–A average difference would at that level of wear would also be 
a factor of five larger, namely, 5 × (.4%) = 2%. Thus, if material A would provide 
1 year of wear, material B would wear out 2% sooner, that is, by .02(365) = 7 days 
sooner. Surely, no one would notice that difference (“Don’t call me Shirley,” 
Airplane 1980). Let’s tell the boss to go with the cheaper material and expect 
a nice bonus from the cost savings achieved.
But don’t be hasty. There are other characteristics of a shoe that are 
­important to customers. What if material B soles don’t sound as good (I once 
had a pair of sneakers that were so squeaky I donated them to charity), look as 
good, or feel as good as material A soles? (Were the boys asked to score these 

49
Fundamentals of Statistical Data Analysis
attributes? Careful planning before the experiment would have seen that they 
were.) If the new sole has any of these characteristics, we may lose customers 
for these reasons.
One other consideration is that the difference in wear‐out times for the 
two materials varies. For some boys, the difference in shoe lifetimes (50% 
wear‐out times) would be larger than the average value of 2%. Suppose we’re 
willing to make the working assumption that the differences would vary among 
our customer population with approximately the standard deviation observed 
among the 10 boys in the experiment. That standard deviation was about 
.4%. Thus, for the “plus two‐sigma” person (only about 2.5% of the distribu-
tion exceeds the mean plus two‐sigma point on the Normal distribution), the 
wear difference at a nominal 10% wear would be .4 + 2(.4) = 1.2%. Projecting 
forward to 50% wear‐out by multiplying by five means that wear‐out time in 
such instances would be about 6% less for B than for A. For a 1‐year life, this 
means material B would wear out about 3 weeks sooner. That may be notice-
able and cost us some customers. The bottom line is that manufacturing cost 
savings and increased profits could be wiped out by the loss of customers. 
One can envision further cost/benefit analysis that would address this 
possibility.
Note further that this sort of back‐of‐the‐envelope economic analysis is 
based on the observed mean and standard deviation of wear differences for 
only 10 boys. Even without dealing with this additional uncertainty technically 
(by methods discussed later), it’s apparent that this limited amount of data 
raises the risk of making a wrong decision.
Then there’s ethics. Suppose the shoe company’s slogan is “Always the 
best,” meaning they pride themselves on using the best available materials 
and methods to produce shoes. If they cut corners on shoe sole material, 
what’s next? More cheap substitutes for other parts of the shoe, nearly as 
good as the original material? The product could gradually lose its quality, lose 
its reputation, lose business, go bankrupt!! (Schlitz beer experienced just this 
sort of decline in the early 1970s: “The reformulated product resulted in a beer 
that not only lost much of the flavor and consistency of the traditional formula 
but spoiled more quickly, rapidly losing public appeal” Wikipedia 2014d). 
Does the shoe design team want to risk starting the company down this 
slippery slope? Maybe the prudent thing is a larger, more informative 
experiment that is capable of resolving some of the questions arising from this 
experiment. Maybe the boss wants a decision right now, though. What’s a 
body to do?
This may be an overly dramatic turn in my story, but it makes this point: 
there are often more than mere technical issues involved in designing, con-
ducting, and analyzing the data from an experiment. Bosses, customers, 
thesis advisors, regulators, and others can all have a stake in the outcome 
and can all have agendas. For example, if the suppliers of sole materials 
A  and B knew about the experiment being planned to compare their 

Fundamentals of Statistical Experimental Design and Analysis
50
materials, they would want to assure that the experiment was not in some 
subtle or inadvertent way biased against their material. When the Department 
of Defense tests a proposed new multimillion dollar system to determine its 
combat readiness, there will be many interested parties with a stake in how 
the experiment is designed, conducted, and analyzed. Such interest is not 
sinister, only realistic. In fact, you want people to be interested in the 
experiment. That’s the best way to assure that subject‐matter knowledge is 
fully engaged in the project. The humble statistician working this project has 
to know how to work ethically, intelligently, and effectively in this environ-
ment, not just crunch numbers.
Statistical confidence intervals
A significance test can tell you whether an observed difference, for 
example, between means, is real or could easily be random, but it doesn’t 
tell you how large or small an actual underlying difference could be. For 
example, for the 10 boys in the shoe sole experiment, the average wear 
difference was .4%. The significance test told us that an underlying average 
difference (this underlying difference being the parameter, μ, in the “dis-
tribution of data we might have gotten”) of zero would not be consistent 
with the data. But how large or how small might that underlying difference 
be, consistent with the data? There is uncertainty in drawing conclusions 
based on data. We need to look at that uncertainty before deciding which 
sole material to use.
The degree of agreement of the data with any hypothesized or conjectured 
value of μ, not just zero, can be evaluated using the t‐statistic:
t
dbar
s
n
/
.
For example, if the supplier of material B claimed, “We think our material will 
wear more than material A, on average, by only .5%,” then we would evaluate 
the data against that claim by the statistic
t
.
.
.
/
.
.
41
5
39
10
73
By comparing this t‐value to the t‐distribution with 9 df (shown in Fig. 3.8), we 
can see that this t‐value is not far from the middle of the distribution; 
the ­software for the t‐distribution shows that it falls at the lower .24 point, not 
particularly unusual. There is no evidence in the data to contradict the suppli-
er’s claim.
Someone representing the shoe manufacturer might say, “If material B wears 
more than 1% more than material A, though, we wouldn’t like that.” (Perhaps this 
person has already done the cost/benefit study mentioned earlier.) Is it possible 

51
Fundamentals of Statistical Data Analysis
to get the data we got if indeed the underlying value of μ was 1.0? The t‐statistic 
for evaluating this conjecture is
t
.
.
.
/
. .
41 1 0
39
10
4 8
This value is far out on the lower tail of the t(9) distribution (P‐value <.001), so the 
data put to rest the shoe rep’s worry about a 1% increase in wear using material B.
These calculations show that values of μ of 0 or 1% are not at all consistent 
with the data. But μ = .5% is. In general, and intuitively, values of μ close to .41, 
the data average, are more consistent with the data than values of μ further 
away from .41. This notion of closeness, or consistency, is characterized in 
statistics by the calculation of statistical confidence ­intervals. The objective of 
confidence intervals is to define a “ballpark” of values of μ that are consistent 
(in agreement) with the observed data to a specified degree.
To derive a confidence interval, we start with the test statistic for character-
izing the agreement of the data with any possible value of μ:
t
dbar
s
n
/
.
Values of μ that are consistent with the data are those that lead to a t‐value in 
the middle of the t‐distribution with n − 1 df. To be specific about “middle,” 
let’s consider the question: What values of μ lead to a t‐value in the middle 
95% of the t‐distribution? The answer, algebraically, is given by this inequality:
t
dbar
s
n
t
.
.
(
)
/
,
025
025
where t.025 is the upper .025 point on the t‐distribution with n − 1 df. Rearranging 
this inequality leads to the following inequality for μ:
dbar
t
s
n
dbar
t
s
n
.
.
/
/
.
025
025
This inequality on μ defines what is called a 95% statistical confidence interval 
on μ, the underlying mean of the Normal distribution used as a reference 
model for our experimental data. The end points of this interval are 
the “confidence limits.” By changing the percentile of the distribution used to 
characterize “middle” and thus changing the t‐value in the inequality, we can 
obtain 90%, 75%, etc. confidence intervals.
For the shoe data, with n = 10, there are 9 df, so from tables or software, we find 
that t.025 = 2.26. Thus, the 95% confidence interval on μ for the shoe data is given by
.
.
.
,
.
.
.
41 2 262
39
10
41 2 262
39
10
.
.
, .
.
.
%, .
% .
41
28
41
28
13
69

Fundamentals of Statistical Experimental Design and Analysis
52
In round numbers, we can summarize this calculation by saying that the 
data indicate that the average wear difference between the materials is 
about .4%, but it could be as low as .1% or as high as .7%, at the 95% 
confidence level.
This confidence interval is shown graphically in Figure 3.9. The t‐statistic, as 
a function of μ, is given by
t
dbar
s
n
/
,
.
.
/
,
41
39
10
so t is a linear function of μ.
The solid line in Figure 3.9 plots t versus μ. At the right side of the figure is the 
t‐distribution with 9 df. Its center 95% is the interval from −2.26 to 2.26. The two 
horizontal arrows in the figure correspond to these end points. These arrows 
intersect the line at μ = .13 and μ = .69. Thus, as defined earlier in this section, for 
mu between these two values, the resulting t‐statistic is in the middle 95% of the 
t(9) distribution. The interval (.13%, .69%) is the 95% confidence interval on μ. 
Values of μ in this interval are consistent with the data to the extent that 
corresponding t‐values are in the middle 95% of the t(9) distribution.
t vs. µ: boys' shoes experiment
–3.000
–2.000
–1.000
.000
1.000
2.000
3.000
0
.2
.4
.6
.8
µ
t
3
2
1
0
–1
–2
–3
.69
.13
Figure 3.9  Illustration of Confidence Interval: Boys’ Shoes Experiment.

53
Fundamentals of Statistical Data Analysis
Discussion
The denominator of the t‐statistic is s
n
/
, which is equal to .12% in this case. 
This quantity, as discussed earlier with respect to Table  3.2, is called the 
­“standard error of the difference” and might be denoted by SE(dbar). 
(Technically, a standard error is the estimate of the standard deviation of the 
distribution of the estimate. That is, the distribution of an average difference, 
dbar, has a standard deviation of 
/
n, where σ is standard deviation of the 
distribution of differences. Replacing the unknown σ by its estimate, s, ­provides 
the standard error.) The standard error pretty well determines the width of the 
confidence interval on the underlying difference, μ, because the t‐value multi-
plier, for a given confidence level, for at least moderate degrees of ­freedom is 
fairly constant. For example, for 95% confidence and a sample size of 30 or 
more, the half‐width of the confidence interval is essentially 2.0 times the 
­standard error of the difference.
Algebraically and graphically, we have found that an underlying average 
wear difference, μ, between .13 and .69% is consistent with the data to the 
degree that a calculated t‐value for μ in this interval will fall in the middle 95% 
of the t(9) distribution. In statistical terminology, the 95% confidence interval 
for μ, based on this experiment’s data, is the interval (.13%, .69%). Now, what 
do we do with this information?
Earlier (p. 48), to analyze the effect of the findings of this experiment, we 
multiplied the average wear difference in this experiment (.4%) by a factor of 
5.0 to estimate the average wear difference in a shoe’s lifetime. To get a con-
servative upper bound on the average lifetime wear difference, we multiply 
the upper end of the confidence interval, .69%, by 5.0 to get 3.5%. Thus, the 
average wear difference could be as high as 3.5% at the 97.5% confidence 
level (the upper end of a two‐sided 95% confidence interval is an upper 97.5% 
confidence  limit). Applied to a year’s time, this means that the average 
difference could be .035 × 365 = 13 days, essentially two weeks. One might 
suppose this average wear difference would still not be too noticeable by 
customers.
Earlier (p. 49), we also considered an upper percentile on the wear difference 
distribution as another way to interpret this experiment’s results. It is possible, 
by the method known as statistical tolerance limits, to obtain a confidence 
interval on a percentile of interest, but doing so is beyond the scope of this 
chapter. The essential point is that statistical confidence limits can be used to 
characterize the uncertainty with which this experiment can estimate charac-
teristics of the underlying wear difference distribution.
The selected confidence level in this example was the 95% level for the 
two‐sided interval. Other choices are, of course, possible. It is sometimes 
useful to calculate confidence intervals at a variety of confidence levels. 
Choice of confidence level corresponds to how conservative one wants to 
be in defining the range of plausible parameter values to consider. There are 
no definitive rules for choosing a confidence level, but subject‐matter 

Fundamentals of Statistical Experimental Design and Analysis
54
­considerations, like the cost of adopting a seriously unsatisfactory material, 
should help make the determination. Conventional confidence levels are 
90, 95, and 99%.
Why calculate statistical confidence limits?
Confidence limits are not just for perfunctory reporting—the “statistically 
correct” thing to do. They are to be used to help guide subsequent 
decisions. In particular, they provide a ballpark for economic or other 
analyses. Here, we found that, nominally, B would wear out about a week 
sooner than A. Using the upper end of a 95% confidence interval on the 
underlying average wear difference led to the conclusion that, conserva-
tively, B would wear out 2 weeks sooner than A. If management’s view is 
that neither of these differences is likely to affect sales, the nominal and the 
conservative analysis would lead to the same conclusion and action: switch 
to the cheaper material.
In general, confidence intervals provide limits for subsequent parametric 
analyses pertaining to subsequent actions or decisions. If the same decision 
would be reached for any value of μ in its confidence interval, then this decision 
is robust to the uncertainty inherent in the limited amount of data from the 
experiment. If the same decision would not be made at both ends of 
the confidence interval, then management is either faced with a risky decision 
they may not want to make or a decision that more data are needed in order 
to reduce the risk to a tolerable level.
Sample size determination
Because of the small number of subjects (boys) in this experiment and the 
resulting uncertainty about what the underlying average wear difference bet-
ween materials B and A might be, one possible course of action would be to 
run a follow‐up experiment designed to provide more definitive information. 
More precision will also require a larger number of participants. How many 
subjects do we need and how do we decide?
Consider the 95% confidence limits on the underlying mean difference, μ:
dbar
t
s
n
dbar
t
s
n
–
/
/
.
.
.
025
025
and
For our 10 boys, that confidence interval was (.13%, .69%). Suppose a cost/
benefit analysis by the company’s green‐eyeshade analysts and lawyers led 
to the conclusion that if the underlying mean difference in wear (B–A) was 
no  more than .5%, then the company would be comfortable replacing 
material A by B: the risk of the reduced wear leading to loss of customers 
would be minimal.

55
Fundamentals of Statistical Data Analysis
Let’s translate that objective into this statistical criterion: if the data from this 
follow‐up experiment result in the upper end of the 95% confidence interval 
on μ being .5% or less, we will conclude the case for changing materials is 
­adequately made. That is, we want to choose n so that
dbar
t
s
n
.
/
. %.
025
5
Of course, we don’t know what values of dbar and s will result in the follow‐
up experiment. For planning purposes, though, let’s use the results from 
the first experiment (I’m assuming that the second experiment follows basi-
cally the same protocol as the first): dbar = .41% and s = .39%. How large 
would n have to be so that the upper 97.5% confidence limit on μ would be 
equal to .5%?
To answer this question, we need to solve for n in the equation
.
.
.
.
41
39
5
025
t
n
For a first cut, set the t‐value, which is a function of n, equal to 2.0. Then, the 
equation to solve is
2
39
09
.
.
,
n
which leads to n = 75. For 74 df, t.025 = 1.99, close enough to 2.0 that there is no 
need to refine the analysis.
Thus, by the objective we set, our follow‐up experiment would require  
75 boys. Of course, there is no guarantee that the same dbar and s would be 
obtained, so we might want to repeat this sample size analysis with somewhat 
conservative working values of these summary statistics. We might also want 
to consider other confidence levels. Out of a suite of such analyses, we could 
arrive at the number of boys to recruit. Also, while we’re at it, we ought to fix 
the shortcomings in the original experiment—collect data on boy characteris-
tics and evaluate other characteristics of the shoe materials. We also want to 
be careful on how we select the boys to assure that they will fairly represent 
the customer population.
Of course, some bright engineer might say, “Why go to all the trouble 
to recruit a bunch of unruly, unreliable boys whose shoe‐wearing habits 
we can’t really control. Give me your budget and I’ll build a shoe‐wear 
testing machine and we’ll control the variables that affect shoe sole wear.” 
Never discount technology. Never underestimate, though, the value of 
realistic testing.
There are more detailed ways to address sample size determination, but 
I will defer those to the next experiment.

Fundamentals of Statistical Experimental Design and Analysis
56
Tomato Fertilizer Experiment
Experimental design
In another example from BHH (1978, 2005), 
an  experiment to compare two fertilizers was con-
ducted as follows: the experimental units were 11 
tomato plants (presumably all of the same variety, 
approximate size, and health), planted in one row of 
a garden. The experimenter randomly assigned five 
of the plants to get Fertilizer A and six to get a pos-
sibly improved Fertilizer B. Experimental protocol 
and plant spacing, let us further assume, assured that 
the fertilizer used on one plant would not bleed into 
the adjacent plant sites. The tomatoes were harvested 
when ripe and weighed resulting in the total weight 
of tomatoes, by plant, given in Table  3.4. The 
question of interest is whether these data suggest 
choosing one fertilizer over the other. Analysis 1: 
Let’s plot the data.
Analysis 1: Plot the data
How should these data be plotted? We cannot plot A yields versus B yields, a 
la the boys’ shoes, because the experimental units are not paired and they’re 
randomly distributed along the row. In this case, each yield is associated with 
a fertilizer and, thanks to BHH(!), a row position. (I applaud that inclusion 
Table 3.4  Results of Tomato Fertilizer Experiment.
Position
Fertilizer
Yield (lbs.)
1
A
29.9
2
A
11.4
3
B
26.6
4
B
23.7
5
A
25.3
6
B
28.5
7
B
14.2
8
B
17.9
9
A
16.5
10
A
21.1
11
B
24.3
Box, Hunter, and Hunter (2005, p. 78); reproduced with 
permission from John Wiley & Sons.

57
Fundamentals of Statistical Data Analysis
because some experimenters do not keep track of such ancillary information. 
Recall in Chapter 1 that ancillary data pertaining to wire bonding led to Ed 
Thomas finding and then helping to correct problems in the integrated circuit 
bonding and testing processes.) As we will soon see, having this ancillary vari-
able recorded is a key to interpreting the data and to extracting (kicking and 
screaming) the message in these data. There is a tendency in textbooks not to 
record such information. Never ignore ancillary data.
A straightforward way to display (all) the data in a way that captures all the 
dimensions in the data is shown in Figure 3.10, which is a scatter plot of yield 
versus position, with different plotting symbols for the two fertilizers. This plot 
shows all three dimensions of the data: yield, fertilizer, and row position. Right 
away, as should be the case with a good display, we “see” important information 
about the relationship of yield to fertilizer and row position: there appears to 
be a distinct trend in soil quality or fertility, resulting in tomato yield that gen-
erally drops off from left to right along the row. If we had not kept track of the 
tomato yields by position as well as fertilizer, we would not have learned about 
the fertility trend. Further, the plant in position 2 had unusually low yield, 
relative to its neighbors. Perhaps tomato worms or disease infected that plant; 
perhaps there was a recording error—the actual yield might have been 
21.4 lbs., not 11.4. If we had not been able to associate the yields with ­position, 
this “outlier” would have not been detected: the yield on that plant would 
11
10
9
8
7
6
5
4
3
2
1
0
30
25
20
15
10
Position
Yield (lbs.)
A
B
Fert.
Scatter plot of yield vs. position by fertilizer
Figure  3.10  Data from Tomato Fertilizer Experiment. Yield, in pounds, is plotted 
versus row position, by fertilizer.

Fundamentals of Statistical Experimental Design and Analysis
58
look consistent with the variability among all the plants that got Fertilizer A, 
similar to the case of the Opel and Chevette in the car data in Chapter 1.
There is a principle involved here:
●
●
Outliers need to be explained, if possible.
Statistical techniques such as plotting the data and statistical significance tests 
for outliers (see, e.g., Barnett and Lewis 1994) can identify apparent outliers; 
subject‐matter insight is needed to explain them. Note that in this case, plot-
ting the yields versus position allowed the possible outlier to be identified. 
Plots of the A and B data that ignore position would not (could not) identify 
position 2 as a possible outlier. If an outlier cannot be explained, my practice 
is to analyze the data with and without the outlier(s) to see if it makes a 
difference in the conclusions and subsequent actions.
Now, with respect to the reason the experiment was run—Fertilizer A versus 
B—the yields from the two fertilizers are pretty well intermingled in Figure 3.10 
(setting aside position 2 from this assessment). There’s no evidence of an 
advantage of one fertilizer over the other. The message from this experiment 
in this garden is that:
●
●
It is more important where you plant your tomatoes than what fertilizer 
you use!
And note that we arrived at this finding from just the picture, no number 
crunching required. The experiment is not a failure, though, unless you are the 
developer of Fertilizer B who expected it to do better than the competition, 
Fertilizer A. The gardener has learned that she could increase yield by improving 
and equalizing the soil quality in her garden and that she can choose fertilizer 
next year based on other considerations, such as cost or environmental impact.
The value of randomization
One further important point to make is the value of randomization. If the 
gardener had run a convenient experiment, such as Fertilizer A on the left half 
of the row and Fertilizer B on the right half, misleading results would have 
been obtained. The soil‐quality trend would have been wrongly interpreted as 
a fertilizer difference. In statistical terminology, row position would be con-
founded with fertilizer type. Randomization mixed up the fertilizer assignments 
so that we had an essentially fair comparison of the two fertilizers. (Note 
though that randomization could have resulted in the convenient allocation, 
but with small probability. A prudent experimenter would have rerandomized 
the assignments just to provide protection against possible soil‐quality trends.)
If the experimenter had suspected a fertility trend beforehand, the 
experiment could have been designed differently to minimize the effect of 
such a trend. For example, as Box, Hunter, and Hunter (2005) suggest, fertilizer 

59
Fundamentals of Statistical Data Analysis
assignment could have been done randomly within each adjacent pair of plants 
(which would require an even number of plants). Such a design approach 
would be based on subject‐matter knowledge that fertility generally varies 
gradually over an area such as a garden, not erratically from plot to plot. Thus, 
if there is a trend, adjacent plots would be more similar than nonadjacent 
plots. The resulting experiment would then have been structured like the boys’ 
shoes example. There would be five or six pairs (blocks) of experimental units 
in the experiment, based on proximity.
The importance of ancillary data
There is a lesson in both the boys’ shoes experiment and the tomato fertilizer 
experiment—the need to think about and record ancillary data (also called 
concomitant data) pertaining to the experimental units and experimental con-
ditions, in addition to the treatments and responses of primary interest. Row 
position and L/R foot assignment are two such ancillary variables in these two 
examples. Then, such data need to be used in investigating possible, perhaps 
unanticipated, relationships—relationships that could either enhance under-
standing or invalidate findings. In the shoe example, we were left wondering if 
boy variables, such as age and weight, influenced shoe wear. Better to have 
thought of these possibilities before the experiment was run, recorded the 
data, and then did an analysis to see if such factors contribute to shoe wear.
A New Tomato Experiment
Back to the tomato story: suppose that the 
gardener rototills nutrients into her garden, 
works the soil, and then has soil testing done 
that confirms that she has achieved improved 
and more uniform soil quality in her garden. 
Suppose she also expands her tomato patch 
so that the following year she can plant 
16 plants and then, being a good scientist, 
always looking for ways to improve things, 
she conducts an experiment to compare 
Fertilizer A to a new candidate, Fertilizer C.
Analysis 1: Plot the data
Data (randomly generated by the author’s computer) from the gardener’s year 
2 experiment are given in Table 3.5 and plotted in Figure 3.11. Note the larger 
and more consistent yields than were obtained in Experiment 1. She learned 
from her experience—the previous year’s data. Her designed experiment was 
the basis for that learning.

Fundamentals of Statistical Experimental Design and Analysis
60
Table 3.5  Results of Year 2 Tomato Experiment.
Position
Fertilizer
Yield
1
A
30.5
2
A
28.8
3
C
32.0
4
A
29.0
5
A
27.1
6
A
30.1
7
C
26.6
8
C
34.2
9
C
28.7
10
C
32.8
11
C
30.6
12
A
30.8
13
A
26.9
14
C
32.8
15
C
29.4
16
A
28.8
Yield in Pounds.
Position
Yield
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
35
34
33
32
31
30
29
28
27
26
Fertilizer
A
C
Tomato experiment 2
Yield by fertilizer and position
Figure 3.11  Tomato Yield by Position and Fertilizer: Experiment 2.

61
Fundamentals of Statistical Data Analysis
Figure 3.11 (in contrast to Fig. 3.10) shows no evidence of a fertility trend 
along the row of tomato plants (but it’s good that we could check this). There 
is some (visual) evidence that Fertilizer C produces higher yield than A: the top 
four yields are all from Fertilizer C (but so was the lowest single yield). There is 
substantial overlap among plant yields for the two fertilizers, though, so it 
remains to be seen whether these results indicate a real difference in fertilizers 
or could easily be due only to the variability of tomato yields in this garden. 
Stay tuned.
Figure 3.11 shows that tomato yield is not associated with position, so 
the data display can be simplified by ignoring position. Figure 3.12 displays 
a “side‐by‐side dot plot” of the tomato yields—the yields are simply plot-
ted along a single axis, with separate plots for the two fertilizers. The 
average yields for the two fertilizers are also indicated and connected. 
Figure 3.12 tells us that Fertilizer C apparently leads to higher yields, on 
average by about two pounds, but with more variability. The A data are 
covered by the C data. Are these apparent differences between A and C 
real or could they just be random? It is now time to carry out some 
quantitative analyses—to calibrate our eyeball impression by evaluating 
the extent to which the observed difference between fertilizers could be 
“real or random.”
C
A
35
34
33
32
31
30
29
28
27
26
Fertilizer
Yield (lbs.)
Dot plot of yield vs. fertilizer
Figure 3.12  Dot Plot of Yields by Fertilizer: Experiment 2.

Fundamentals of Statistical Experimental Design and Analysis
62
Significance tests
As with the boys’ shoes experiment, there are a variety of significance 
tests that are appropriate for the second tomato fertilizer experiment. In 
Experiment 2, the experimental units were more homogeneous: yield was 
not a function of row position as it was in Experiment 1. The analysis 
methods we will illustrate are based on models that assume we have homo-
geneous data, as would occur in independent random samples from each 
of two distributions. The Experiment 1 data were not consistent with this 
model. Instead, tomato yield depended on location as well as, possibly, 
fertilizer. Thus, in a model for this situation, there would be a different dis-
tribution of possible yields at each position in the row. The yields would be 
position dependent, not random across positions. It is possible to construct 
a model and analysis for this situation, called the “analysis of covariance,” 
but that analysis is beyond the scope of this text. It should be noted, how-
ever, that the randomization test is valid even in the presence of a fertility 
trend and the t‐test is still valid as a useful approximation to the randomi-
zation test, even though the assumptions on which it is based are not a 
particularly good model for these data.
One’s intent in a statistical analysis should go beyond conducting a signifi-
cance test and finding a P‐value. (Statistical) life does not end with a P‐value. 
The analysis goal, as with the boys’ shoes, is a more general description of the 
relationships found in the data and the implications of those relationship for 
further actions. Our graphical analysis of Experiment 1 showed that yield was 
dependent on position, an important finding that led to further spadework 
and experimentation.
The above plot of the Experiment 2 data (Fig. 3.12) shows some evidence 
of a real difference between fertilizers. Similar to the boys’ shoes experiment, 
the analysis objective is to compare the data we got with a reference 
­distribution of data we might have gotten, if there was no difference in 
fertilizers.
If there is no difference between fertilizers, then the 16 yields obtained 
would have been obtained regardless of what fertilizer was used—the 
observed yields reflect only the intrinsic quality of the soil and plant at each 
site. Any additional effect of fertilizer (it is assumed) would be the same for 
the two fertilizers, so a different random assignment would have resulted in 
the same 16 yields, but with different labels corresponding to the two fertil-
izers. Thus, we can choose a summary statistic that measures the difference 
between fertilizers and then generate the reference distribution of that sta-
tistic by calculating it for all the 12 870 (= the number of combinations of 16 
objects selected 8 at a time = 16!/(8!8!)) possible random assignments of 
­fertilizers to experimental units. As with the boys’ shoes experiment, we will 
consider three summary statistics: one that reflects just the ordering of the 
yields and two that consider the magnitudes of the yields for the two 
fertilizers.

63
Fundamentals of Statistical Data Analysis
Rank sum test
The rank sum significance test, like the sign test for paired data, is based 
only on the data ordering, not the yields themselves. To do this test, first 
rank the combined data (the 16 tomato yields in Table 3.5) from low to high, 
with the smallest observation having rank 1 and the largest observation hav-
ing rank 16. The summary statistic is then the sum of the ranks of the obser-
vations in one of the groups. (The summed ranks of one group determine 
the summed ranks of the other group, so there is no need to sum the ranks 
of the second group; it can be determined by subtraction.) If, for example, 
the group being counted generally has the smallest yields, the rank sum will 
be small relative to the sum of the ranks of the other group. For the tomato 
fertilizer data, the ranks of the eight Fertilizer A results are 2, 3, 4, 6, 7, 9, 
10, and 12, which sum to 53. Is this unusually small or large? Don’t know.  
We need a reference (probability) distribution against which to compare  
this result.
As a reference distribution for the rank sum statistic for Experiment 2, 
­consider a random selection of eight integers, without replacement, from the 
numbers 1, 2, …, 16. For example, one could shuffle 16 cards numbered one 
through 16, then deal the top eight cards (physically or via computer simula-
tion), and calculate their sum. Carrying out this randomization repeatedly 
would generate a reference distribution for the rank sum statistic. If there is no 
difference between fertilizers, then “the data we got,” as summarized by the 
“rank sum” statistic for those data, should be like data we could get from this 
random selection and summing of eight ranks.
Let nA denote the number of observations in the selected group and nC 
denote the number of observations in the other group. Theory can tell us the 
probability of each possible rank sum result, if this randomization was done 
repeatedly (analogous to the way theory provides the binomial distribution for 
the sign test). For our purposes here, though, it suffices to note that this rank 
sum distribution would have:
●
●
Mean = nA(nA + nC + 1)/2
●
●
Standard deviation = √[(nAnC(nA + nC + 1)/12]
(See, e.g., Hollander and Wolfe 1999.) For our case of nA = 8 and nC = 8, the 
mean rank sum is
8 8
8
1
2
68
and the standard deviation is
8 8
17
12
9 5
. .

Fundamentals of Statistical Experimental Design and Analysis
64
(Some intuition about these results: the average of the ranks, 1 to 16, is 8.5. 
Thus, the sum of eight randomly selected ranks in this case would be expected 
to have an average of 8 × 8.5 = 68. The standard deviation takes a little more 
theory to derive.)
Theory or computer simulation could be used to obtain or estimate the exact 
reference distribution of rank sums in this situation, under the assumption of 
no  difference between groups. The Normal distribution, though, ­provides an 
­adequate approximation for our case. Figure  3.13 shows this distribution and 
­compares the observed rank sum of 53 to it. The (one‐tail) P‐value that summarizes 
this comparison is about .06.
The significance test we have just done is called the Mann–Whitney test 
(Wikipedia 2014e). Software can carry out this analysis, so don’t worry about the 
above formulas. Minitab’s calculation of a P‐value applies a “continuity correction” 
(because the rank sum statistic is limited to integers) by calculating the area 
under the reference distribution to the left of 53.5 and thus obtains P = .064 (not 
importantly different from the uncorrected calculation). There is appreciable evi-
dence that Fertilizer C increases tomato yield relative to Fertilizer A. Whether the 
data justify switching to Fertilizer C will be addressed later.
Randomization test
A natural summary statistic for comparing the two fertilizers, based on the 
observed yields, is the difference between average yields. For Experiment 2, 
Rank sum
53
.057
68
Normal, mean = 68,St Dev = 9.5
Graphical rank sum test: tomato experiment 2
Figure 3.13  Reference Distribution for the Rank Sum Statistic: Tomato Experiment 2.

65
Fundamentals of Statistical Data Analysis
the  average yield for A was 29 lbs. and for C was 31 lbs., for an average 
difference, for C–A, of 2.0 lbs.
As mentioned earlier, there are 12 870 ways the eight A and C labels can 
be mixed up on the 16 yields. For each of these assignments, the resulting 
average difference could be calculated. Rather than attempt to (ask a com-
puter to) enumerate all the possibilities, I used the Stat101 software (Simon 
1997) to randomly select eight of the 16 yields and assign them to A and 
the other eight to C and then calculated the average difference. This can 
be done quickly so I ran 100 000 cases. The resulting randomization distri-
bution of the average yield difference is shown in Figure 3.14. The observed 
average difference of 2.0 lbs. falls fairly far out on the upper tail. The upper‐
tail P‐value is .04, which is slightly stronger evidence of a real difference 
than the rank sum test indicated (P = .06). More information, the actual 
yields, as opposed to their ranks, provides a bit more precision in our find-
ings. (Note: We know the randomization distribution of the C–A averages 
is symmetric, so the histogram in Figure 3.14 could have been refined by 
averaging mirror‐image frequencies (e.g., averaging the proportions in the 
intervals (−1.2, −.8) and (.8, 1.2)), but we don’t really require more ­precision, 
so that was not done).
Diff
.6
–3.2
–2.4
–1.6
–.8
0
.8
1.6
2.4
3.2
–2.8
–2
–1.2
–.4
.4
1.2
2
2.8
3.6
Figure  3.14  Randomization Distribution of Average Difference Between Fertilizers 
in Tomato Experiment 2: C–A. Plot produced by Stat101 software (Simon 1997).

Fundamentals of Statistical Experimental Design and Analysis
66
Normal theory t‐test
For the boys’ shoes data, which were paired (by boy), the Normal distribution theory 
significance test was based on a t‐distribution derived from the model of a Normal 
distribution of differences between materials A and B. In the tomato experiment, 
the experimental units are unpaired—they are not linked, for example, by location. 
In this case, it may not surprise you that a reference distribution for the situation of 
“no real difference” between fertilizers will be based on the statistical model of two 
Normal distributions that have the same mean. Two cases will be considered:
(a)  Assume the two distributions have the same (but unknown) standard 
deviations.
(b)  Assume the two distributions have unequal (unknown) standard deviations.
The two analyses are available in software and often provide similar results. 
In general, the more you assume, the more precise the conclusions you can 
draw. In this case, assuming equal variances provides more precision. Context 
and experience can sometimes justify one or the other assumption. Also, as we 
shall see, the choice of assumption does not have to be made blindly. A signif-
icance test can be done to evaluate whether an apparent difference between 
the data standard deviations is “real or random.”
Analysis a: Assume equal underlying standard deviations
Consider the situation of two independent random samples from the same 
Normal distribution. Denote the common unknown standard deviation by σ. 
Let nA and nC denote the number of samples from the two distributions. 
Further, let the mean and standard deviation of each data set be denoted by 
ybarA and sA and ybarC and sC. Because the two standard deviations are assumed 
to be independent estimates of the common unknown standard deviation, σ, 
they can be combined to obtain a “pooled” estimate of σ. That estimate is
	
s
n
s
n
s
n
n
p
A
A
C
C
A
C
–
–
.
1
1
2
2
2

(3.1)
The subscript p denotes “pooled.” This formula says that the pooled variance, 
sp
2, is equal to a weighted average of the variances in the two sets of data and 
that the weights are proportional to the respective degrees of freedom.
Theory then tells us that the following statistic, for the case of two 
independent random samples from the same Normal distribution, has a known 
probability distribution:
	
t
ybar
ybar
s
n
n
C
A
p
A
C
–
/
/
.
1
1

(3.2)
That distribution is again a t‐distribution, this time with (nA + nC − 2) degrees of 
freedom. Comparing the value of t calculated from the data to this distribution 
provides a graphical and quantitative indication of the extent to which the 

67
Fundamentals of Statistical Data Analysis
data are consistent with the assumption of equal underlying means. The 
statistical theory underlying this analysis is discussed in Appendix 3.B.
I used Minitab to calculate and summarize the t‐test for the Experiment 2 data. 
The results are given in Table 3.6. In the table of results, the data means and stan-
dard deviations are given for each fertilizer, then the estimated underlying 
difference in average yield, namely, 2.0 lbs. (rounded), which is simply the difference 
between the two data means (C–A), then a lower 95% confidence limit on the 
underlying average difference (to be discussed later), and then the t‐test results. 
The last line in Table 3.6 is the pooled standard deviation calculated via (3.2) above.
The graphical comparison summarized by this output is given in Figure 3.15. 
We see that the difference between average yields for the two fertilizers, as 
Table  3.6  Minitab Output: Tomato Fertilizer Experiment 2: Two‐Sample t‐Test 
Assuming Equal Variances.
Two‐Sample t for C versus A
N
Mean
StDev
SE Mean
C
8
31.0
2.66
.94
A
8
29.0
1.46
.52
Difference = μ (C) − μ (A)
Estimate for difference: 2.0
95% lower bound for difference: .15
t‐test of difference = 0 (vs. >): t‐value = 1.90 P‐value = .039 df = 14
Pooled StDev = 2.15
.4
.3
.2
.1
.0
t (14 df)
Density
1.9
.039
0
t Distribution with 14 df
Figure  3.15  Tomato Experiment 2. Comparison of the observed t‐statistic to the 
t(14)‐distribution.

Fundamentals of Statistical Experimental Design and Analysis
68
gauged via the t‐statistic, is fairly unlikely compared to the probability distribution 
of t‐values that results when there is no difference between the underlying 
means. The (one‐tail) P‐value of .039 indicates that there is only about a 4% 
probability of a t‐value as large as or larger than the observed 2.0. Thus, there 
is fairly strong, though not conclusive, evidence—statistics means never hav-
ing to say you’re certain—of a real difference between fertilizers: Fertilizer C 
has about a two pounds per plant better yield than does Fertilizer A. In a 
subsequent section, we will consider the uncertainty of this estimated 
difference and the subsequent actions a tomato grower might take based on 
this result.
Analysis b: Assume unequal standard deviations
When it is assumed that the underlying standard deviations are (possibly) 
unequal, the test statistic and its reference distribution change. The t‐statistic 
is calculated as in (3.2) but with a different denominator:
	
t
ybar
ybar
s
n
s
n
C
A
C
C
A
A
2
2
/
/
.
(3.3)
In this statistic, because there is no assumption that the underlying variances 
(the σ2s) are equal, the variances of the two groups are estimated separately; 
they are not pooled. The squared denominator of (3.3) is an estimate of the 
variance of the numerator.
There is one potential problem for the t‐statistic in (3.3): its probability 
­distribution, theory tells us, depends on the ratio of the unknown standard 
deviations and is not a t‐distribution. That being the case, there are no 
(exact) reference distribution and no exact yardstick against which to eval-
uate the calculated t‐value. But that doesn’t stop us. Theory and empirical 
investigations lead to the operating assumption that the reference distribu-
tion is approximately a t‐distribution with degrees of freedom calculated 
from the data—by a somewhat messy function of the standard deviations 
and sample sizes of the two sets of data (Satterthwaite’s method; Wikipedia 
2014f). Minitab, Excel, and other software calculate an “effective degrees of 
freedom” to be used to specify the reference distribution for our observed 
t‐value. For the Experiment 2 data, the Minitab output in Table 3.7 shows 
that calculated df to be 10, in contrast to the 14 df for the case of assumed 
equal variances (Table 3.6).
For the case of equal sample sizes, the denominators in the two t‐
statistics, (3.2) and (3.3), are identical, so the calculated t‐values are the 
same, namely, t = 1.90. The reference t‐distributions differ, though, in their 
degrees of freedom. The calculated df for the unequal variance case will 
always be less than or equal to the pooled df in the equal variance case. 
This inequality means the significance test based on unequal variances 

69
Fundamentals of Statistical Data Analysis
is  conservative—it will lead to a larger P‐value, and it will lead to wider 
confidence intervals. In this example, the P‐values are .039 and .044—not 
appreciably or importantly different. The difference in df and P‐values 
increases as the ratio of the data standard deviations increasingly exceeds 
or is less than 1.0.
For the case of unequal sample sizes, the t‐statistics will be different for 
the two analyses, and the analysis based on the assumption of equal 
­variances will generally lead to a smaller P‐value because it will be based 
on more df than is the case when unequal variances are assumed. The 
intuitive reason for this conservatism is that the assumption of equal 
underlying variances injects ­additional information into the analysis—only 
one sigma has to be estimated, rather than two. More information, or 
stronger assumptions, means more ­precision in evaluating the difference 
between means.
Confidence intervals
All we have learned so far is that C may produce a greater yield than A. The 
observed difference in Experiment 2 of two pounds per plant is an imprecise 
estimate of what the long‐run average yield difference would be. The cost‐
effectiveness of changing fertilizers will depend on the unknown underlying 
difference, (μC − μA), in tomato yields for the two fertilizers. If Fertilizer C is 
more expensive than A, the additional cost may not be offset by the increased 
yield. To do trade‐off calculations, one needs to conduct the economic anal-
ysis over a plausible range of possible long‐run yield differences—say, a 
best‐case and a worst‐case analysis. Statistical confidence limits provide the 
bounds for such an analysis.
For our gardener, who is only raising tomatoes for herself and a few friends, 
there is no real trade‐off to consider. She’s not selling tomatoes, so it’s just a 
Table  3.7  Minitab Analysis of Tomato Experiment 2: Two‐Sample t‐Test Assuming 
Unequal Variances.
Two‐Sample t for C versus A
N
Mean
StDev
SE Mean
C
8
31.01
2.66
.94
A
8
28.98
1.46
.52
Difference = μ (C) − μ (A)
Estimate for difference: 2.04
95% lower bound for difference: .09
t‐test of difference = 0 (vs. >): t‐value = 1.90; P‐value = .044; df = 10

Fundamentals of Statistical Experimental Design and Analysis
70
matter of how much she is willing to spend for fertilizer, not whether she will 
cover the cost by selling more tomatoes. For a commercial grower, however, 
the cost and income trade‐off is vitally important.
The experimental results were that plants with Fertilizer C averaged about 
two pounds higher yield, per plant, than plants with Fertilizer A. At this 
average difference, suppose that it would be profitable for the grower to 
switch to Fertilizer C. However, there is uncertainty in the finding of roughly a 
two‐pound average difference; there were only eight plants in each fertilizer 
group, and there was substantial variability of yields among these eight plants. 
A confidence interval for the underlying average difference, (μC − μA), will convey 
this uncertainty.
Following the same logic as in the boys’ shoes example, the 95% 
confidence interval for (μC − μA), for the case in which it is assumed that the 
underlying variances of the two groups are unequal, is the range of (μC − μA) 
values such that
t
ybar
ybar
s
n
s
n
C
A
C
A
C
C
A
A
2
2
/
/
is in the center 95% of the t‐distribution with specified degrees of freedom. 
This requirement leads to the result that the 95% confidence interval on 
(μC − μA) is given by
ybar
ybar
t
df
s
n
s
n
C
A
C
C
A
A
–
/
/
.
.025
2
2
For the Tomato Experiment 2 data, the df was found to be 10. The value of 
t.025, which defines the middle 95% of the t‐distribution with 10 df is 2.23. The 
square‐root quantity that is multiplied by this t‐value to obtain the confidence 
interval, called the “standard error of the difference,” is equal to 1.07 lbs. 
Thus, the 95% confidence interval on the underlying mean difference between 
­fertilizer yields, namely, (μC − μA), is
31 0
29 0
2 23 1 07
2 0
2 4
4
4 4
.
.
.
.
.
.
.
.
., .
. .
lbs
lbs
lbs
Thus, all we can claim (with 95% confidence) is that, based on this small 
experiment and its attendant variability of yields, Fertilizer C could increase 
average yield by as much as 4.4 pounds per plant, or Fertilizer A could actually 
provide as much as .4 pounds per plant higher yield than Fertilizer C, or 
anything in between these extremes. The data do not provide a definitive 
conclusion about the difference between fertilizers A and C favoring one 
­fertilizer or the other. When conclusions and actions would change as a param-
eter ranges over its confidence interval, this is a message from the data telling 
us that more data are needed. If more data, though, are out of the question, 
then the message is that a risky decision must be made. Here, the confidence 
interval is telling us on the one hand that switching to C could actually decrease 

71
Fundamentals of Statistical Data Analysis
yield in the long run. On the other hand (once again, statistics means never 
having to say you’re certain), it’s plausible that the average yield could be in 
the neighborhood of four pounds greater per plant.
Note: If the preceding analysis is conducted under the assumption of equal 
underlying variances, the standard error of the difference is still 1.07 lbs., but the 
associated degrees of freedom, in this case, is 14. The t.025(14) value is 2.14, so the 
95% confidence interval becomes (−.3 lbs., 4.3 lbs.), which is negligibly different 
from the more conservative analysis based on unequal underlying variances.
To continue the story, let’s suppose we’re a commercial grower not yet wil-
ling to gamble on switching to Fertilizer C. “I need more data!” the grower 
says. How much? Next section.
Determining the size of an experiment
The structure of the tomato experiments is different from that of the boys’ shoes 
experiment, so the analyses required to answer questions about the size of an 
experiment are different in the details, though similar in concept. We first consider 
sizing the experiment based on confidence interval width and then introduce and 
illustrate the concept of a power curve and its role in sizing an experiment.
Confidence interval width
Suppose the commercial tomato grower’s financial advisors do some calcula-
tions and tell the grower: “If you can get at least 1.5 lbs. additional yield, per 
plant, it will be cost‐effective to switch to Fertilizer C.” Clearly, the current 
garden‐size experiment says we can’t claim that minimum difference has been 
established. A further experiment is required. How many tomato plants should 
be in this experiment?
Consider, for example, the lower 97.5% confidence limit on the underlying 
difference, for the case in which equal variances are assumed:
ybar
ybar
t
n
n
s
n
n
C
A
A
C
p
A
C
.
/
/
.
025
2
1
1
Suppose that for planning purposes, it is assumed that the follow‐up experiment 
will result in the same average yield difference of 2.0 lbs. and pooled standard 
deviation of 2.15 lbs. that resulted in Experiment 2. Then, in order for the lower 
confidence limit to be at least 1.5 lbs., the following inequality must hold:
ybar
ybar
t
n
n
s
n
n
C
A
A
C
p
A
C
lbs
.
( /
/
)
.
.
025
2
1
1
1 5
Substituting 2.0 lbs. for ybarC − ybarA, 2.15 for sp, and 2.0 for t.025 (because the 
sample size is apt to be large) leads to the requirement to choose nA and nC 
such that
2
2 15
1
1
5
.
/
/
.
.
n
n
A
C
lbs

Fundamentals of Statistical Experimental Design and Analysis
72
(The solutions to sample size problems are generally “rough” numbers so 
exactitude is not called for.)
The total sample size is minimized by having equal sample sizes in the 
two groups (proving this is an exercise left for the reader), so let n denote 
the number of plants in each fertilizer group. The equation to be solved 
for n is
2
2 15
2
5
.
. .
n
The solution is that at least n = 148 plants should be included in each fertilizer 
group in the follow‐up experiment. We might do some further calculations 
with different assumed average differences and pooled standard deviations 
and confidence levels to provide some additional margin.
Power curve sample size analysis
A decision made on either the basis of the small garden‐scale experiment or 
a commercial grower‐scale experiment is risky. Consider the commercial 
grower’s situation. Even in a large follow‐up experiment, it is possible, just 
due to random variation, that the experiment’s data might lead to a decision 
(i) to switch to Fertilizer C when you shouldn’t (because the underlying average 
difference in yields is not high enough to offset the higher cost of Fertilizer C) 
or (ii) to the decision not to switch to Fertilizer C when you should (the 
­underlying average yield difference more than offsets its higher cost). These 
erroneous decisions cannot be totally avoided, but it is possible to specify a 
sample size and decision criterion such that the probabilities of erroneous 
decisions are limited. The decision is akin to deciding how much automobile 
or home insurance to buy.
Let’s set up the decision problem more formally. The experiment will be 
done with n plants in each fertilizer group. Summary statistics from the 
resulting data will be calculated, namely, ybarC, ybarA, and sp (for planning 
purposes, and based on our previous data, we work the problem by assuming 
the ­underlying standard deviations for the two fertilizers are equal). To sim-
plify the notation, I will denote the observed average difference in yields as 
d (=ybarC − ybarA) and let the standard error of d be denoted by s
s
n
d
p
2 /
. 
The degrees of freedom associated with this standard error is df = 2n − 2.
If d is “big enough,” we will decide to switch to Fertilizer C. How big? 
The  economic analysts said the break‐even mean difference in yields is 
1.5 lbs. per plant. We wouldn’t want to switch fertilizers if the underlying 
average difference, call it δ (delta), was that small or smaller. As a starting 
point, let’s say we will choose C only if the lower 90% confidence limit (one 
sided) exceeds 1.5 lbs. We want to have 90% confidence that δ exceeds 
1.5 lbs. If δ happened to be 1.5 lbs., right at the crossover point, we would 
only have a 10% chance on deciding to switch to C. This choice of decision 

73
Fundamentals of Statistical Data Analysis
Table 3.8  Minitab Output: Power and Sample Size.
Power and Sample Size
2‐sample t‐test
Testing mean 1 = mean 2 (vs. >)
Calculating power for mean 1 = mean 2 + difference
Alpha = .1 Assumed standard deviation = 2.15
Sample target
Difference
Size
Power
Actual power
.5
482
.99
.99
The sample size is for each group
criterion controls the probability of choosing C when we shouldn’t (δ < 1.5 lbs.) 
to 10% or less.
On the other hand, suppose the economic analysts say the grower will have 
a handsome profit if δ is 2.0 lbs. or more. Suppose we translate this economic 
context into numbers and say that we want our sample size and decision rule 
to have a .99 probability of deciding to switch to C when the underlying δ is 
2.0 lbs. (we really want to reap a handsome profit if C is that much better 
than  A). These two criteria—specified decision probabilities at δ‐values of 
1.5 and 2.0 lbs.—will determine the sample size, n, by math we don’t need to 
get into. Minitab and other software can solve for the sample size and decision 
threshold that satisfy these criteria. Catalogs of test plans also exist that can be 
used if you find yourself without the necessary software.
Some terminology
The error of deciding to switch to C when you shouldn’t, that is, when δ ≤ 1.5, 
is called a type I error, and its specified probability is usually denoted by α 
(alpha). Type I error is also called the “significance level” of the decision rule 
(or test of hypothesis). The probability of not switching to C when you should 
(i.e., when δ ≥ 2.0) is called a type II error, and its specified probability is denoted 
by β (beta). Power, which is the probability of switching to Fertilizer C when 
δ = 2.0, is equal to 1 − β.
The Minitab output for solving this sample size problem is given in Table 3.8.
The result in Table 3.8 is that we need nearly 500 plants in each fertilizer 
group to control the error probabilities as specified.
The (Minitab‐generated) power curve in Figure  3.16 displays the error 
­probabilities and other characteristics of this test plan graphically.
The horizontal axis in Figure 3.16 is the difference between the actual δ and 
the threshold δ‐value of 1.5 lbs. Thus, Difference = 0 corresponds to the 
threshold average yield difference, δ = 1.5 lbs. We see, as one of the design 
criteria specified, that at Difference = 0, the power, the probability of 
deci­ding to choose Fertilizer C, is equal to the specified alpha value of .10. 

Fundamentals of Statistical Experimental Design and Analysis
74
At Difference = .5, corresponding to δ = 2.0 lbs., the power is .99, so the type II 
error probability is beta = .01, as the other design criterion specified. 
Furthermore, the power curve tells us the probability of switching to C as a 
function of the underlying δ. For example, if the actual underlying average 
difference in yields was 1.7 lbs. per plant (a difference in Fig. 3.16 of .2 lbs.), 
there is about a 55% chance that the experiment would lead to the conclusion 
that C is economically preferable to A.
Now, an experiment with 1000 plants may be prohibitive even for a 
commercial grower. What if we run a smaller experiment? How much power 
will we sacrifice? Let’s consider the cases of n = 200, 300, 400, and 500. The 
power curves for these sample sizes are given in Figure 3.17. The power values 
at Difference = .5 lbs. are shown in the figure. Conceptually, larger sample sizes 
result in greater power, or greater sensitivity or resolution, in being able to use 
the data to discriminate between a 1.5 lbs. and a 2.0 lbs. increased yield of  
C versus A. The power curves quantify this relationship.
(Note: A future boys’ shoes paired experiment could have been sized via 
power considerations as well. The appropriate menu selection from Minitab’s 
Power and Sample Size menu would be the “one‐sample t” analysis. The input 
planning value of sigma would be the assumed standard deviation of the dis-
tribution of shoe‐wear differences.)
Let’s say the tomato grower ponders the costs and risks and settles on an 
experiment with 400 plants in each fertilizer group. How should the experiment 
be designed? The analysis we have just done to derive and evaluate the 
.6
.5
.4
.3
.2
.1
.0
1.0
.8
.6
.4
.2
.0
Difference ( = Δ–1.5)
Power
Alpha
.1
St Dev
2.15
Alternative
>
Assumptions
482
size
Sample
Power curve for 2-sample t test
Figure 3.16  Power Curve for the Plan Determined in Table 3.8.

75
Fundamentals of Statistical Data Analysis
.8
.7
.6
.5
.4
.3
.2
.1
.0
1.0
.8
.6
.4
.2
.0
Difference
Power
Alpha
.1
St Dev
2.15
Alternative
>
Assumptions
200
300
400
500
size
Sample
Power curve for 2-sample t test
.5
500
.99
.5
400
.98
.5
300
.94
.5
200
.85
difference
Size Power
Sample
Figure 3.17  Power Curves for Different Sample Sizes.
grower’s potential experiments is based on the statistical model of 
independent random samples from two Normal distributions. The experi-
mental design that matches the concept of independent random samples 
from two distributions is the completely randomized design, the subject of 
the next chapter. In this design, we would first set aside and plant 800 tomato 
plants for the experiment—perhaps in one corner of a large field. The plants 
would be separated enough so that the fertilizer applied to one plant would 
not affect its neighbors. Then, we would randomly assign 400 plants to be 
fertilized with Fertilizer A and the other 400 plants with Fertilizer C. This 
could become a logistical and time‐consuming nightmare. What are some 
alternative designs?
Plan A. A logistically tempting design would be to set out the plants in a 
40 by 20 grid and then divide the grid in half: one randomly selected half to 
get Fertilizer A and the other half to get Fertilizer C. A conceptually similar 
design would be to have two rows of 400 plants each and then randomly 
assign one row to A and one to C. But these designs change the experi-
mental unit from a single plant to a group of 400 plants. The problem with 
this is that we would only have one experimental unit for each fertilizer—that 
is, no replication and no way to legitimately test whether an apparent 
difference in yields for the two fertilizers is real or random or fertilizer caused 
or location related.
Now, the grower might argue (express his subject‐matter knowledge by 
saying) that the field in which this experiment will be run is so uniform in soil 

Fundamentals of Statistical Experimental Design and Analysis
76
quality that randomly assigning a fertilizer to individual plants really won’t be 
different from either of these logistically simpler designs. Plant‐to‐plant yield 
differences, he would claim, will be the same among 400 plants in a single row, 
say, as they would be among 400 randomly selected individual plants out of a 
field of 800 plants. Well, maybe, but the prudent experimenter is leery of such 
arguments. As the gardener found a surprise fertility trend in her little garden 
in Experiment 1, so might the commercial grower in his large field. Rather than 
insist on the purity of a completely randomized design, though, I would pro-
pose a compromise.
Plan B. Set out the tomato plants in 32 groups of 25 plants, each group per-
haps in a 5 × 5 grid, perhaps in a single row. Then randomly assign 16 of the 
groups to get Fertilizer A and 16 to get C. The groups would be ­separated 
enough so that the fertilizer of one group would not bleed onto its neigh-
boring groups. Now, the experimental unit is a group of 25 plants, and with 
16 replications, we would have the ability to test for a statistical difference 
between fertilizers. I would still measure the yield on individual plants, though, 
in order to test the grower’s conjecture of a homogeneous field.
There are other obvious alternative designs: 20 groups of 40 plants, 40 
groups of 20 plants, etc. Deciding among these is apt to be more a matter of 
logistical convenience than of statistical properties.
One further point is that if there is group‐to‐group variability that exceeds 
what would be expected from the plant‐to‐plant variability within a group, 
this means that the standard error for comparing the average yields for the 
two fertilizers will be larger than what was assumed in this planning analysis. 
One might want to consider a more conservative assumed sigma in the 
analysis.
Now, let’s look at the perspective of the manufacturer of Fertilizer C. He, 
of course, will be gratified if the grower’s experiment shows a substantially 
better yield for Fertilizer C than for A. He can advertise this result. But the 
grower’s results just apply to that grower’s field. A broader inference, based 
on agronomy, not statistics, is that the grower’s findings would apply to 
other fields with similar soil and growing conditions. But will a grower in a 
different county, or a different state, with substantially different soil and 
growing conditions also find Fertilizer C to be a winner? The manufacturer 
needs an experiment that spans some appropriate spectrum of growing 
conditions to answer this question. This broader inference would require 
doing experiments something like the grower’s at multiple locations. (A 
­scientifically/statistically astute fertilizer manufacturer would already have 
done this broader experiment and used its results to advertise the breadth 
of conditions in which its new Fertilizer C outyields old Fertilizer A.) When 
“treatments” are compared via experiments on multiple groups, or blocks, 
of experimental units (see Fig.  2.1), the design is a “randomized block 
design.” This design is the topic of Chapter 6.

77
Fundamentals of Statistical Data Analysis
Comparing Standard Deviations
One choice in an analysis comparing the means of two sets of data is whether 
to make the assumption of equal variances in the statistical model of 
independent Normal distributions for “data we might have gotten.” 
Sometimes, previous data or subject‐matter context help us make the choice. 
For example, we might know that adjusting one knob in a production process 
can be expected to move a process characteristic up or down, but not affect 
its variability. In other situations, we may suspect that changing a product or 
process may also affect its variability, so we will be just as interested in 
­comparing standard deviations as in comparing means. In other situations, we 
will just want to compare standard deviations, statistically, as a preliminary to 
a comparison of means.
The two sets of tomato yields in Experiment 2 gave us two standard devia-
tions—Fertilizer A: sA = 1.46 lbs. and Fertilizer C: sC = 2.66 lbs., each based on 
eight tomato‐plant yields. Is this apparent difference of nearly a factor of two 
“real” or could it just be due to the random variation inherent in sampling 
eight observations from two Normal distributions that have the same under-
lying standard deviations? Theory to the rescue, once again.
What theory tells us is that the optimum way to compare two standard devi-
ations is via their ratio. Further, the ratio of two independent estimates of the 
same variance (the standard deviation squared) has a probability distribution 
that has been derived, named, tabulated, and captured in software. This distri-
bution is called the F‐distribution, and this family of distributions has two 
parameters: the degrees of freedom associated with the two estimated stan-
dard deviations. By comparing the F‐statistic calculated from “the data we 
got” to its appropriate F‐distribution, we can evaluate the extent to which the 
data conform to the assumption of equal underlying standard deviations.
For the Experiment 2 data, the F‐ratio is (2.66/1.46)2 = 3.33. (It is conven-
tional to take the ratio of the larger standard deviation to the smaller.) Is this 
unusually large? Figure 3.18 provides the graphical comparison.
For samples of eight observations for each fertilizer group, there are seven 
degrees of freedom associated with each data standard deviation. Thus, the 
calculated F‐ratio is compared to the F‐distribution with 7 df in both numerator 
and denominator. The appropriate F‐ratio is denoted by F(7, 7) in Figure 3.18. 
The one‐tail P‐value is .07, so a ratio this large is fairly unusual, though not 
“statistically significant” at a conventional threshold of .05. Moreover, because 
we had no prior reason to suppose that Fertilizer C would lead to increased 
variability, it is appropriate to double the P‐value and thus report P = .14. (The 
F‐distribution is not “ratio independent” in the case of unequal numerator and 
denominator degrees of freedom, but this doubling of the upper‐tail P‐value 
is generally an adequate summary of the comparison.) A two‐tail plot is shown 
in Figure 3.19. The lower tail is defined by F = 1/3.33 = .30.

Fundamentals of Statistical Experimental Design and Analysis
78
The conclusion is that there is some, but not overwhelming, evidence of 
greater variability in yield from tomato plants fertilized with C. The future 
planned experiments could address this question with much more precision. 
For the case of Experiment 2, we saw a negligible difference between the 
F(7, 7)
3.33
P-value =.07
0
Graphical F-test for equal variances: experiment 2
Figure 3.18  Comparison of F‐Statistic for Experiment 2 to the F(7, 7) Distribution.
F(7, 7)
.30
.07
3.33
.07
Two-tail F-test: experiment 2
Figure 3.19  Two‐Tail F‐Test for Comparing Variances for Fertilizers A and C.

79
Fundamentals of Statistical Data Analysis
conclusions drawn about the yield difference from the two analyses: one 
assuming equal standard deviations and the other not. So, for this purpose, it’s 
a nonissue. Minitab and other software can perform the F‐test for comparing 
two variance estimates.
Economic considerations, however, could lead to a further consideration of 
the consequences of unequal variances. Suppose tomatoes are sorted by weight 
and the price per pound of heavier tomatoes is greater than that of lighter 
tomatoes. In this case, the difference in return for the two fertilizers would be a 
function of both the difference in the average weights and the standard devia-
tions of weights. The analysis could get complicated. We also might need to 
reconsider our data collection, and instead of measuring just the total weight of 
tomatoes from each plant, we might need to weigh individual tomatoes.
Discussion
Some statistical textbooks dispose of the paired t‐test and the two‐sample  
t‐test in a few pages. I have tried to show that there is (much) more to the 
story of experiments, data analysis, conclusions, and consequences than those 
­bare‐bones analyses. Thank you for your patience. Subsequent chapters will 
deal with more involved experimental designs, but less‐developed stories and 
alternative analyses. The reader should be aware, though, that real‐world 
experiments are apt to raise numerous issues, experimental, economic, 
statistical, scientific, ethical, cosmic, and contextual, as these two simple two‐
treatment experiments have illustrated.
Conventional texts also relegate “nonparametric tests,” tests not based on 
an assumed distribution such as the Normal distribution, to a separate chapter 
from the conventional Normal distribution‐based analyses, or to separate 
books. The implication is that the experimenter should do only the one anal-
ysis that is somehow best for the data. I have tried to show that there is more 
than one way to evaluate data and that these analyses can be complementary 
and collectively informative, not conflicting.
The following appendices provide some of the theoretical background to 
the analyses illustrated in this chapter.
Appendix 3.A The Binomial Distribution
The binomial probability distribution was used in the sign test comparison 
of the two boys’ shoe sole materials. In that application, the particular 
binomial distribution considered was the case of 10 “trials” (boys wearing 
one shoe of each material), each of which was scored a win for material A if 
material A wore less than did material B. The assumption being tested by 
the sign test was that there was no real difference between materials, in 
which case the probability of a win for A on each trial was .5. Thus, the 

Fundamentals of Statistical Experimental Design and Analysis
80
analogy was that of coin tossing. If there was no real difference between 
materials, then the experimental ­outcome should be comparable to the 
result of 10 fair tosses of a fair coin. It was found that the outcome of eight 
wins for A fell at the .055 point on the upper tail of the pertinent binomial 
distribution.
Let’s now generalize the situation to n independent trials in which the 
binary  outcome of interest, let’s call it a failure, has a constant probability, 
p, of ­occurring on each trial. Theory leads to the following result: the proba-
bility of x failures in n trials, denoted by B(x: n, p), is given by
B x n p
p
p
x
n
n
x
n
x
n x
: ,
,
, ,
.,
, ,
C
1
0 1
1
where Cx
n
n
x
n
x
!/
!
!  is the number of ways to order x failures and n − x 
successes.
This is the binomial probability distribution. Figure 3.5, earlier in the chapter, 
shows this distribution for the case of n = 10, p = .5.
As noted in the chapter, software, such as Excel and Minitab, can calculate 
binomial distribution probabilities and tail areas.
In some experimental situations, such as reliability, n items are tested and 
f failures are observed. Statistical analyses for significance tests for hypothe-
sized values of p can be carried out and statistical confidence intervals on 
p determined. See, for example, Meeker and Escobar (1998).
As an example, in the boys’ shoe experiment, material A lost 2 out of  
10 trials (paired comparisons). Thus, the natural and statistically preferred 
point estimate of the underlying probability of A failing to win such trials is 
p^ = 2/10 = .20. For a binomial distribution with n = 10 and p = .20, this is the 
most likely outcome. This is shown in Figure 3.A.1 which plots the binomial 
distribution with n = 10 and p = .20.
Of course, with this small amount of data, we cannot pin down p exactly. 
Other values of p are consistent with the data to a lesser extent than is 
p = .20. We can adapt the approach used earlier to define confidence limits 
on the underlying average wear difference to the binomial distribution and 
confidence limits on p. For example, we can ask: For what values of p is the 
observed outcome (the data we got) of two failures in 10 tests not in either 
of the .05 tails of the corresponding binomial distribution? The resulting 
confidence limits can be calculated but also illustrated graphically. We can 
move up and down the p scale and plot the corresponding binomial distribu-
tion. By trial and error, we can find the interval on p that answers the question. 
Figure  3.A.2a and b shows that at p = .037, the observed outcome, x = 2, 
defines the upper .05 tail of the distribution and at p = .51 the observed out-
come defines the lower .05 tail of the distribution. Thus, the interval (.037, 
.51) is the 90% confidence interval on the failure probability, p, based on 
binomial data of two failures in 10 trials. This limited amount of data does not 
pin p down very precisely—the interval spans slightly more than a factor of 
10, upper limit versus lower limit, for p.

81
Fundamentals of Statistical Data Analysis
Appendix 3.B Sampling from a Normal Distribution
In the Normal distribution‐based analysis of the boys’ shoes data, the “data we 
got,” namely, the wear differences for the two sole materials for the 10 boys in 
the experiment, were compared to “data we might have gotten” by sampling 
from a Normal distribution. The Normal distribution, the bell‐shaped curve, is 
an idealized probability distribution. Real data, particularly for small to 
moderate sample sizes, cannot and will not look like the ideal. But, less strin-
gently, real data might look like, or be comparable to, random samples from a 
Normal distribution. Let’s examine that notion.
Figure 3.B.1 is a side‐by‐side dot plot of 10 random samples of size 10 from 
a standard Normal distribution (generated by Minitab). Note how different 
samples from the same Normal distribution can be. There are asymmetries, 
clusters, gaps, and outlying points. Variability happens—in nature and in 
­computer generation.
By way of comparison, Figure 3.B.2 gives a dot plot of the boys’ shoe‐wear 
differences. The patterns in the data we got are not unlike the patterns seen in 
Figure 3.B.1. (The shoe data are rounded to two significant figures, but the 
computer‐generated data are not.)
It is conventional in statistical textbook analyses to start out by assuming the 
data we got are a random sample from a Normal distribution. That’s a very 
strong assumption. My view is that is more justifiable just to say that the “data 
10
9
8
7
6
5
4
3
2
1
0
.30
.25
.20
.15
.10
.05
.00
X
Probability
Distribution plot
Binomial, n = 10, p = 0.2
Figure 3.A.1  Binomial Distribution for n = 10, p = .2. The observed outcome, x = 2, is 
the most likely value in this distribution.

Fundamentals of Statistical Experimental Design and Analysis
82
Figure 3.A.2  (a) Binomial Distribution for n = 10, p = .037. (b) Binomial Distribution for 
n = 10, p = .051.
10
9
8
7
6
5
4
3
2
1
0
.25
.20
.15
.10
.05
.00
X
Probability
.05
Distribution plot
Binomial, n = 10, p = .51
(b)
10
9
8
7
6
5
4
3
2
1
0
.7
(a)
.6
.5
.4
.3
.2
.1
.0
X
Probability
.05
Distribution plot
Binomial, n= 10, p = 0.037

83
Fundamentals of Statistical Data Analysis
we got” are comparable to (or can be modeled as) data from a Normal distri-
bution. Then, I can use Normal distribution theory to gauge the extent, in the 
shoe experiment, to which the data support or contradict the hypothesis of no 
real underlying difference between the two shoe sole materials. I can use 
Normal distribution methods to set the bounds on this underlying difference 
for economic trade‐off studies. The support for making this comparison can 
be evaluated by looking at the data. In addition to visual comparisons, as 
­previously mentioned, there are quantitative significance tests by which one 
can gauge the extent to which the data look like a sample from a Normal dis-
tribution. Additionally, empirical studies have shown that Normal distribution‐
based methods often work reasonably well when the underlying distribution is 
not a Normal distribution. The Normal distribution provides analysis tools; it is 
not a binding assumption about nature or populations.
Choosing to evaluate the “data we got” by comparing them to data we 
might have gotten from one or two Normal distributions does not have to be 
an arbitrary or blind choice. Plots of the data help justify such analyses. Here, 
seeing that our data plot in Figure 3.A.4 is similar to data plots of computer‐
generated samples from a Normal distribution (Fig.  3.A.3) tells us that the 
­significance tests and confidence intervals we obtained via Normal distribu-
tion  theory are plausible—they’re useful. Additionally, we can quantify the 
“goodness of fit” of a Normal distribution to a set of data by a statistical 
­significance test.
A special data plot for assessing goodness of fit of a Normal distribution is 
called the Normal probability plot. This plot plots the ordered data versus 
cumulative probability (the proportion of the probability distribution less than 
a given value) on a special scale. The scale is chosen so that the cumulative 
3.6
2.4
1.2
.0
–1.2
–2.4
–3.6
C1
C2
C3
C4
C5
C6
C7
C8
C9
C10
Z
Dot plot of 10 samples of size10 from a Std. Normal dist.
Figure 3.B.1  Ten Random Samples of 10 Observations from a Normal Distribution.

Fundamentals of Statistical Experimental Design and Analysis
84
Normal distribution plotted on this scale is a straight line. The extent to which 
the “data we got” are grouped about a straight line provides a visual 
assessment of goodness of fit. Additionally, a goodness‐of‐fit statistic can be 
calculated that quantifies the degree to which the data fit a straight line and a 
significance test can be carried out.
Figure 3.B.2 gives the Minitab output for this graphical and quantitative 
analysis for the shoe‐wear differences. The straight line in the plot is the 
Normal distribution with μ equal to the data mean (.41) and σ = to the data 
standard deviation (.39). This line is the estimated cumulative probability dis-
tribution. For example, at a value of diff‐shoes = 0, the corresponding 
cumulative probability is about .15 (15%). At a value of diff‐shoes = .5, the 
corresponding cumulative probability is about 60%. The red diamonds in the 
plot are value of what is termed the empirical cumulative probability distribu-
tion plotted versus the ordered data values. There is some technical tweak-
ing in all of this that we don’t need to go into. An eyeball assessment is that 
the red diamonds are fairly well grouped around the straight line, so graph-
ically, there is no reason to rule out the Normal distribution as a model under-
lying these data.
The curved lines in Figure 3.B.3 are point‐wise 95% confidence intervals on 
the cumulative probability at each value of diff‐shoes. They illustrate the impre-
cision with which cumulative probabilities can be estimated based on this small 
sample of 10 observed differences. For example, at a diff‐shoes value of .0, for 
which the estimated cumulative probability was 15%, the 95% confidence 
1.0
.8
.6
.4
.2
.0
–.2
Diff-shoes
Dot plot of shoe-wear differences (B – A)
Figure 3.B.2  Dot Plot of Boys’ Shoe Data.

85
Fundamentals of Statistical Data Analysis
interval on the cumulative probability ranges from about 1 to 35%. Estimating 
tail probabilities with small samples is very imprecise.
Various goodness‐of‐fit statistics can be used to measure the agreement of 
data with the Normal distribution. Minitab uses the Anderson–Darling statistic, 
denoted by AD in Figure 3.B.3. The formula for this statistic (Wikipedia 2014g 
and references therein) is unimportant, but it can be thought of as a weighted 
sum of squares of the vertical difference between the red dots in Figure 3.B.3 
and the straight line; the poorer the fit the larger the AD value. Here, AD = .26. 
We don’t know whether this value is large or small until we compare it to its 
­reference distribution. That distribution has been derived, or approximated, 
and tabulated, and the Minitab software makes the comparison. The P‐value in 
Figure 3.A.5 of .62 tells us that the AD of .26 corresponds to an upper‐tail 
probability of .62. This means that, for our data, the AD value is fairly near the 
middle of the distribution—no evidence against the use of the Normal distri-
bution in analyzing these data. Whew!
Appendix 3.C Statistical Underpinnings
The analyses in this chapter, based on the Normal distribution as a model for 
“data we might have gotten,” went straight to t‐test statistics and t‐distribu-
tion‐based confidence intervals on underlying average differences. This 
appendix describes the statistical basis for those results. Many statistical texts 
2.0
1.5
1.0
.5
.0
–.5
–1.0
99
95
90
80
70
60
50
40
30
20
10
5
1
Diff-shoes
Cumulative probability (%)
Mean
.41
St Dev
.3872
N
10
AD
.261
P-value
.622
Probability plot of diff-shoes
Normal–95% CI
Figure 3.B.3  Probability Plot of Boys’ Shoes Data.

Fundamentals of Statistical Experimental Design and Analysis
86
and online resources provide more extensive derivations for those readers 
who are interested.
Single sample
As a starting point, consider a random sample of n observations from any 
probability distribution. The fundamental problem in mathematical statistics is 
that we don’t know the underlying probability distribution, but we know that 
the data provide information from which we can infer something about that 
probability distribution. Let’s consider the situation in which we’re interested 
in the mean of the underlying distribution, call it μ. (The symbol, μ, is often 
used, as here, to denote a probability distribution mean in general, not just the 
mean of a Normal distribution.)
A natural estimate for μ, and one that has desirable statistical properties, is 
the data mean, ybar. Suppose, conceptually, that we could repeatedly take 
samples of size n from the unknown distribution of interest and for each of 
these samples calculate ybar. The data mean, ybar, would vary from sample to 
sample—it has a probability distribution. Theory shows, not surprisingly, that 
the mean of that distribution is μ. This means that ybar has the property that it 
is an unbiased estimate of μ. That’s a good property, but it does not tell us how 
close ybar might be to μ.
Next, theory tells us that the standard deviation of the distribution of ybar is 
/
n, where σ is the standard deviation of the distribution underlying the data 
(there are unusual distributions in which the standard deviation does not exist, 
but we won’t worry about those pathological cases). The standard deviation of 
a distribution reflects its spread. Thus, the larger n is, the smaller 
/
n is, 
which means that the larger the sample size is, the more precise ybar becomes 
as an estimate of μ: the larger n is, the closer ybar is likely to be to μ. That’s an 
intuitive result; the square root of n divisor expresses it mathematically.
Another important property of ybar is what is called the central limit 
­theorem effect. Theory tells us that for almost any underlying data distri-
bution, the distribution of ybar approaches the Normal distribution as 
n gets very large. Empirical studies show us, though, that even for small 
n  from fairly unusual distributions, ybar has a distribution that is well 
approximated by a Normal distribution. Thus, to answer questions such as 
to what extent is the observed ybar consistent with a conjectured, or 
hypothesized distribution mean, say, μ0, we can compare the observed 
ybar to the Normal distribution with mean μ0 and standard deviation 
/
n, 
if we happened to “know” σ. We could calculate a P‐value that character-
izes that comparison. With a known σ, we could also calculate statistical 
confidence limits on μ.
The more general situation is unknown σ. For any underlying probability 
­distribution (not just the Normal distribution), the sample standard deviation, s, 
is an estimate of σ. It has the property that s2 is an unbiased estimate of σ2. 

87
Fundamentals of Statistical Data Analysis
For large n, we can generally make adequate inferences (significance tests and 
confidence intervals) about the distribution mean, μ, by assuming that the prob-
ability distribution of t = (ybar − μ)/ s
n
/
 is approximately the standard Normal 
distribution. In one special case, though, we can do better. In the case of sam-
pling from a Normal distribution, we know that the exact probability distribution 
of t for any n, large or small, is the Student’s t‐distribution with n − 1 degrees of 
freedom. This result enables us to characterize what ybar tells us about μ without 
having to know, or pretend to know, the standard deviation, σ, underlying the 
data. The case of the boys’ shoes, in which the wear differences were treated as 
a random sample from a Normal distribution, illustrates these analyses.
Two samples
Consider random samples from two Normal distributions: n1 observations from 
a Normal distribution with mean μ1 and standard deviation σ1 and n2 observa-
tions from a Normal distribution with mean μ2 and standard deviation σ2. 
Denote the data means and standard deviations by ybar1 and ybar2 and s1 and 
s2. What do these statistics tell us about the underlying distributions?
As noted earlier, theory tells us that the distribution of ybari is a Normal dis-
tribution with mean μi and standard deviation 
i
in
/
, i = 1 and 2. Let’s say 
we’re interested in the difference μ1 − μ2. The conventional, intuitive, theory‐
blessed, and data‐based estimate of that difference is ybar1 − ybar2. What are 
the properties of this estimate?
Conceptually, if we took repeated samples from the two underlying Normal 
distributions and calculated this difference of data means, these differences 
would vary; they would have a probability distribution. This distribution is a 
Normal distribution with mean μ1 − μ2. Thus, the difference between data 
means is an unbiased estimate of the difference between the underlying distri-
bution means. The standard deviation of the distribution of mean differences 
is based on the following result from theory.
The variance of the sum or difference of two independent random variables 
is the sum of the variances of the two random variables.
Thus,
Variance
ybar
ybar
n
n
1
2
1
2
1
2
2
2
.
The standard deviation of ybar1 − ybar2 is the square root of this variance. If these 
two underlying sigmas were known, then we could use these properties of the 
distribution of ybar1 − ybar2 to obtain confidence intervals on μ1 − μ2 and to do a 
significance test gauging the agreement of the data to a hypothesized value, 
typically zero, of the difference between underlying means. In general, though, 
not knowing the two sigmas, we’ll do what we did in the case of a single sample: 
replace the unknown sigmas by suitable estimates obtained from the data.

Fundamentals of Statistical Experimental Design and Analysis
88
Suppose first that it is reasonable to assume that σ1 = σ2 = σ, say. Then, theory 
says that the best way to estimate this common sigma is by
s
n
s
n
s
n
n
p
1
1
2
2
2
2
1
2
1
1
2
–
–
,
where the subscript p denotes “pooled.” This formula says to first calculate a 
weighted average of the sample variances with the weights being proportional 
to the degrees of freedom associated with the sample variances. The degrees of 
freedom associated with this pooled estimate is the sum of the degrees of free-
dom associated with the two sample variances, namely, (n1 − 1) + (n2 − 1) = n1 + n2 
− 2. The square root of the pooled variance is the pooled standard deviation.
Next, theory tells us that the quantity
t
ybar
ybar
s
n
n
1
2
1
2
1
2
1
1
p
/
/
has a t‐distribution with n1 + n2 − 2 degrees of freedom. The mathematical 
­formula for this distribution is known and is incorporated in statistical software 
so the t‐distribution can be used, as in our analysis of the tomato yield data, to 
obtain significance tests for hypothesized values of (μ1 − μ2) and to obtain 
confidence intervals on the underlying (μ1 − μ2).
The assumption of equal underlying standard deviations can oftentimes be 
unwarranted. If that assumption is not made, then it is natural to estimate 
the variance of (ybar1 − ybar2) by replacing the two sigmas in the above vari-
ance equation by the sample standard deviations. This leads to an estimated 
standard deviation for (ybar1 − ybar2) of
s
s
n
s
n
u
1
2
1
2
2
2
,
where the subscript u denotes the assumption of unequal variances. This leads 
to the “t‐like” quantity
t
ybar
ybar
s
n
s
n
1
2
1
2
1
2
1
2
2
2
/
/
,
from which to obtain significance tests and confidence intervals for (μ1 − μ2). 
This t does not have a t‐distribution, and in fact, its exact distribution depends 
on the ratio of the two unknown underlying standard deviations. Nevertheless, 
in what is generally an adequate approximation, the t‐distribution with degrees 
of freedom calculated from a function of the ni’s and si’s (see Wikipedia 2014f) 

89
Fundamentals of Statistical Data Analysis
can be used for significance tests and confidence intervals. This approximation 
is used in Excel, Minitab, and other software.
Assignment
To address a topic of interest to you, design two experiments to compare two 
treatments. In one case, make the design a paired experiment, and in the 
other case, a two‐sample (unpaired) experiment.
For each experiment, describe the design: the experimental units, the 
random assignment of treatments, the response measurements, other mea-
surements, and experimental protocol.
Discuss potential analyses: data displays, significance tests, confidence 
intervals, and the use of these statistical results.
Discuss the pros and cons of the two designs.
References
Barnett, V., and Lewis, T. (1994) Outliers in Statistical Data, 3rd ed., John Wiley & Sons, 
Inc., New York.
Box, G. E. P., Hunter, W. G., and Hunter, J. S. (1978, 2005) Statistics for Experimenters, 
1st and 2nd eds., John Wiley & Sons, New York.
Hollander, M., and Wolfe, D. (1999) Nonparametric Statistical Methods, 2nd ed., John 
Wiley & Sons, Inc., New York.
Meeker, W., and Escobar, L. (1998) Statistical Methods for Reliability Data, John Wiley 
& Sons, Inc., New York.
Nuzzo, R. (2014) Scientific method: Statistical Errors, http://www.nature.com/news/
scientific‐method‐statistical‐errors‐1.14700.
Simon, J. (1997) Resampling: The New Statistics, http://www.statistics101.net/index.
htm.
Wikipedia (2014a) Binomial Distribution, http://en.wikipedia.org/wiki/Binomial_ 
distribution.
Wikipedia (2014b) Sign Test, http://en.wikipedia.org/wiki/Sign_test.
Wikipedia (2014c) Randomization Test, http://en.wikipedia.org/wiki/Randomization_ 
test.
Wikipedia (2014d) Joseph Schlitz Brewing Company, http://en.wikipedia.org/wiki/
Joseph_Schlitz_Brewing_Company.
Wikipedia (2014e) Mann–Whitney U, http://en.wikipedia.org/wiki/Mann‐Whitney_U.
Wikipedia (2014f) Welch–Satterthwaite Equation, http://en.wikipedia.org/wiki/Welch% 
E2%80%93Satterthwaite_equation.
Wikipedia (2014g) Anderson–Darling Statistic, http://en.wikipedia.org/wiki/Anderson% 
E2%80%93Darling_test.


Fundamentals of Statistical Experimental Design and Analysis, First Edition. Robert G. Easterling. 
© 2015 John Wiley & Sons, Ltd. Published 2015 by John Wiley & Sons, Ltd.
Introduction
In both of the tomato fertilizer experiments in Chapter 3, the gardener set out 
several tomato plants in her garden and randomly assigned one of two 
­fertilizers to each of them. Her experiment is the simplest example of the 
experimental design that is called the “completely randomized design (CRD),” 
which is the topic of this and the following chapter. The tomato experiment 
had a single qualitative treatment factor (fertilizer), and this factor had two 
­discrete, qualitative levels (brands A and B in the first experiment, brands A 
and C in the second).
In the general case of a CRD, there are k (= two or more) treatments. In this 
chapter, these k treatments are the k selected levels of a single treatment factor. 
Chapter 5 addresses the situation in which the treatments are combinations of 
various levels of two or more factors. The experimental units (eus) to which the 
treatments are applied are a single group of essentially homogeneous entities, 
like laboratory mice, plots of land, or people in a certain demographic or 
­medical group. There are no distinguishing characteristics on which one might 
group, or block, the experimental units. Blocked designs are addressed in 
Chapters 6 and 7.
In a CRD, the assignment of treatments to experimental units is, as the 
label implies, completely randomized: ni eus are selected at random to 
receive level i of the treatment factor, i = 1, …, k. The numbers of eus that 
receive each treatment are often equal, but need not be. The purpose of 
the experiment is to evaluate differences among the different treatment 
levels in their effect on the responses of interest. There may be differences 
or trends in the data for the k treatments that have important implications, 
Completely Randomized 
Design
4

Fundamentals of Statistical Experimental Design and Analysis
92
such as finding the most productive fertilizer, for the process, product, or 
phenomenon under study.
In this chapter, we consider two situations:
1.  The treatment factor is a qualitative variable—discrete types, brands, 
materials, methods, etc.—that we want to evaluate and compare, as in the 
tomato experiments comparing fertilizers.
2.  The treatment factor is a quantitative variable, generally measured on a 
continuous scale, such as concentrations, temperature, dose amounts, or 
dollars. In this case, the design and analysis objective is generally to find a 
functional relationship between a response and the x‐variable that is the 
factor of interest. That function can then be used to address issues such as 
what dose level of a medication is required to achieve a desired response.
Design Issues
The key design decisions the experimenter must make in designing a ­completely 
randomized experiment to evaluate the effect of a single factor are the choice 
of experimental units, how many factor levels to include in the experiment, and 
the number of eus to be assigned to each factor level and in total. The choices 
of response(s) and measurement method are always design issues for this and all 
other designs. Experimental controls and protocols are context specific.
Subject‐matter considerations dominate the selection of factor levels. For 
example, if the objective is to compare the ability of five different paper towel 
brands to absorb spills (a common student project), and these brands have been 
purposely chosen—say, the largest selling—then those five brands will be the 
levels included in the experiment. Cost, schedule, and statistical‐­precision con-
siderations, addressed later in this chapter, determine the number of experi-
mental units and the allocation of experimental units among the factor levels.
CRD: Single Qualitative Factor
Example: Market research
Selling consumer products is highly com-
petitive, as a stroll down any Wal‐Mart or 
Walgreen’s aisle tells you. You’ve got to 
have visibility in order to generate sales. You 
have to know what catches the consumer’s 
eye and what doesn’t. Market research has 
long had the objective of finding out 
what sells and what beats the competition. 
The business community has found that 
designed experiments not only have a 

93
Completely Randomized Design
role in product research but also in market research. Controlled experimentation 
­generates directly usable, information‐laden data more quickly and cleanly 
than observational data. I will use the following marketing example and story, 
inspired by, but different from, a marketing example in Testing 1‐2‐3. 
Experimental Design with Applications in Marketing and Service Operations 
(Ledolter and Swersey 2007), for our first illustration of a CRD (with more than 
two factor levels). (See Ledolter and Swersey 2007 for many examples of 
business applications of designed experiments.)
The marketing department for a company that makes shampoo has come up 
with four candidate store displays for their product. Rather than just ask upper 
management to choose the display they like (think Dilbert and his pointy‐haired 
boss), the department decided to run an experiment in actual stores to see how 
much of an effect the displays have on sales—decisions to be made on customer 
data, not management opinion! After considering cost and schedule and 
convenience, they select 20 stores, such as CVS, in the surrounding counties in 
which to do the study. Someone in the marketing department has heard that to 
make it a fair comparison, the displays should be randomly assigned to stores. 
Five stores are randomly assigned to each of the displays. (Recall that it’s the 
random assignment that validates the analysis by which the “real or random?” 
question can be answered. The stores do not have to be a random sample of 
stores from the “population” of stores that carry the ­company’s product.)
The company installs the displays for a week and records the shampoo sales 
for that week (automatically, via bar codes and scanners). Different stores have 
different levels of activity and sales, so to normalize the sales figures and 
make them more comparable across stores, the measurement used to assess 
the effect of a display on the store’s sales is the percent increase (or decrease) in 
a store’s shampoo sales (try saying that phrase quickly) during the week relative 
to the store’s “base sales.” The base chosen is the previous month’s average 
weekly sales. The resulting data are given in Table 4.1, along with the means and 
standard deviations for each display. What is the message in these data?
Table  4.1  Sales Data: %Increase in Sales for Four Displays. 
Each display was installed in five different stores for 1 week.
Sales Increase (%)
Display
D1
D2
D3
D4
4.2
8.4
3.0
4.9
2.7
4.5
3.8
2.8
3.1
4.9
2.0
6.1
4.6
7.3
2.1
4.2
1.2
5.7
3.2
3.7
Average
3.2
6.2
2.8
4.3
Std. dev.
1.3
1.7
.74
1.2

Fundamentals of Statistical Experimental Design and Analysis
94
If someone walked into my office with these data, here are some questions 
I would ask:
●
●
Hey, where’d you get that coat?
●
●
Why 20 stores? (I realize I can’t change this after 
the fact, but this and other questions can pro-
vide subject‐matter information that could be 
pertinent later.)
●
●
How were they selected?
●
●
How were the displays assigned to stores? 
(Haphazard assignment is not the same as 
random assignment. Purposeful, biased assignment, as in, “I think Display 
2 will play well in Peoria,” will invalidate the results.)
●
●
Are there variables of potential interest that characterize individual stores, 
such as their location: city, town, or village?
●
●
When was the test conducted? (This could be important for a seasonal 
product, which shampoo isn’t.)
●
●
Was it the same calendar week at every store? (One might imagine a ­scenario 
in which one of each display was constructed, then trucked around to five 
different stores in some metropolitan region over a 5 (or more) week period.)
●
●
Why was 1 week selected as the time period for measuring sales, rather 
than, say, 2 weeks, a month, or a quarter?
●
●
What was the base period and was it the same for every store?
●
●
Do you have the sales data for the base and test periods? (In working with 
ratios, it’s always good to have both numerator and denominator. Sometimes 
you can be surprised. For example, if a store has a small base level of sales, 
a small increase in sales, dollar‐wise, might result in a large percentage 
increase.)
●
●
Was the display removed after its 1‐week exposure?
●
●
Do you have sales data for some time period after the display was removed? 
(With product bar codes and networked computerized recording of sales 
details, retailers have an astounding data‐based ability to react to their 
sales environment. I like to tell a Wal‐Mart story. One year Thanksgiving 
weekend sales were weaker than expected. Wal‐Mart’s data system told 
them the bad news almost immediately and also identified the problem 
areas. The company was able to make pricing and advertising changes, and 
maybe product display changes, aimed at fixing these problems, quickly, 
and recovered to have a successful Christmas season. That’s like turning an 
ocean liner around on a dime.)
●
●
On the other hand, if the display was left up, do you have data for 
subsequent weeks’ sales to see if the effect on sales persists? (Even if the 
display was taken down, it would be interesting to have data from 
subsequent weeks to see if, or how long, the increased sales persisted after 
the experiment.)

95
Completely Randomized Design
I would have liked to ask and discuss these questions when the experiment 
was being planned. The success of an experiment and its ability to generate 
data that have an important message generally depend more on these planning 
decisions than on any of the statistical‐analysis gymnastics the data are subse-
quently put through.
Time for Analysis 1: Plot the data. Figure 4.1 shows the sales‐growth data for 
each of the four displays.
If there is a winner, eyeball analysis of Figure 4.1 says it is Display 2—the 
largest increases. Of course, all four displays are winners in the sense that they 
all resulted in increased sales at every store. On the other hand, if the goal had 
been to achieve a 10% increase, all four displays are losers. Context matters. 
But, before we draw any conclusions, we need to ask: could the apparent sales 
differences among displays happen just by chance? We need a statistical test.
Analysis of Variance
One way to compare the display means would be to compare all or various 
selected pairs of displays by applying the two‐sample t‐test of Chapter 3 to 
the selected pairs. But, this is tedious (for large k) and subject to bias: if, for 
example, one picked out the display with the highest average sales increase 
and compared it to the display with the lowest average sales increase by way 
of a two‐sample t‐test, then (as theory can show), just by chance, there is more 
D4
D3
D2
D1
9
8
7
6
5
4
3
2
1
Display
SalesIncr %
Individual value plot of sales increase (%)
Figure 4.1  Pct. Increase of Shampoo Sales by Display.

Fundamentals of Statistical Experimental Design and Analysis
96
than a 5% chance, say, of getting a t‐test P‐value less than .05, when in fact 
there is no difference among all of the underlying means. Selecting the highest 
and lowest means from the group of k means biases the t‐test analysis toward 
showing a significant difference.
Our analysis goal is to do a fair comparison of the “data we got” to the 
­distribution of “data we might have gotten” if there were no underlying differ-
ences among the displays, that is, if the apparent differences were just due to 
chance. More to the point, we need make this comparison via a statistic 
(­analogous to the t‐statistic for two independent sets of data) that is sensitive 
to the differences among the four data sets. Once that statistic is selected, 
the probability distribution of that statistic when there is no difference among 
the underlying distributions will be the reference distribution of the test statistic 
against which we compare the test statistic calculated from the data we got. In 
short, we need to apply the same sort of analysis we did to compare two tomato 
fertilizers to this extended situation that involves comparisons of four displays.
The data in Figure  4.1 exhibit variation of two types: (i) variation within 
­displays, that’s the vertical variability of sales results among the stores that had 
the same display, and (ii) variation among displays, the horizontal variability, in 
this case primarily the upward shift of Display 2 data relative to the data for 
Displays 1, 3, and 4. The statistical tool called the analysis of variance (­generally 
abbreviated ANOVA) quantifies and compares these two sources of variation 
via a particular statistic, an F‐ratio, previously discussed in Chapter 3 for the 
comparison of two variance estimates. The ANOVA addresses the question: is 
the variation among displays larger than what would be expected, just by 
chance, due to the variation within displays? It answers that question by mod-
eling the “data we might have gotten” by chance as k random samples of size 
n from the same Normal distribution.
One can also think in terms of a randomization analysis: if one took the 
20 data points in Table 4.1 and randomly distributed them into four groups of 
five, how likely is it that we would get variability among the four constructed 
groups comparable to that shown in Figure  4.1? That question could be 
addressed via randomization‐analysis software (Simon 1997).
Table 4.1 shows the summary statistics, ybari and si (the mean and standard 
deviation), for each display’s data. These statistics will be used to develop the 
ANOVA’s comparison of the variability among groups to the variability within 
groups. The following subsections give some formulas underlying an ANOVA. 
The formula‐averse reader can skip to the discussion of the ANOVA table itself.
Within‐group variation
Under the assumption of equal underlying variances for all treatments, each 
data set’s variance, si
2, estimates this common variance, call it σ2. This variance 
is the within‐group variance. It is also called the experimental‐error variance, 

97
Completely Randomized Design
or just the error variance, because it is the variance among experimental 
units that receive the same treatment. (Recall that in Chapter  2, experi-
mental error was defined as the variation among experimental units (eus) 
that receive the same treatment; error does not mean mistake.) From the 
data, we can estimate this common variance by combining these k esti-
mates into an overall, or pooled, estimate of σ2 by calculating the average 
of the si
2, s
s
s
s
s
s
k
k
i
p
average
2
1
2
2
2
2
2
,
,
,
,
where the subscript p stands for pooled and the summation (denoted by 
) 
is across the k treatments.
The degrees of freedom associated with this pooled estimate is k(n − 1) 
because there are n − 1 df in each treatment, pooled across k treatments.
For the case of unequal sample sizes, sp
2, the pooled variance estimate, is 
obtained by calculating a weighted average of the si
2, s, the weights being 
equal to the degrees of freedom (ni − 1) associated with each si
2. Thus, in the 
general case, the combined estimate of σ2 is
s
n
s
n
n
s
n
k
i
i
i
i
i
2
2
2
1
1
1
.
,
where n
ni
.
the total number of observations.
This pooled estimate of variance is the same as that used in the two‐sample 
t‐test when the equal‐sigma assumption is made. The associated df in the case 
of unequal sample sizes is 
n
n
k
i
1
.
.
Among‐groups variation
A statistic that measures variation among the k treatments is the variance of 
the ybar’s:
	
s
ybar
ybar
k
ybar
i
2
2
1
–
.
, 	
where ybar. is the overall average (the “dot” subscript denotes that an 
average has been taken). This formula says just calculate the sample vari-
ance of the k treatment means. Under the assumption of k samples of n 
observations from the same distribution, this variance of the ybars estimates 
σ2/n, which is the variance of a mean based on n observations. This means 
that nsybar
2
 estimates σ2.

Fundamentals of Statistical Experimental Design and Analysis
98
For the case of unequal treatment‐group sizes, say n1, n2, …, nk, the among 
groups variance is given by
	
s
n
ybar
ybar
n k
ybar
i
i
2
2
1
.
.
, 	
where n
ni
.
.
The F‐test
The previous two subsections have provided two estimates of the underlying 
within‐treatment variance, σ2, under the assumption of no real difference 
among treatments. The next important thing that theory tells us is that the 
within‐group variance estimate of σ2, namely, sp
2, is statistically independent of 
the among‐group estimate, namely, nsybar
2
. Intuitively, one could move any of 
the k different data sets up or down, en masse, and not affect the within‐group 
variance, but the among‐group variance would change. The greater the differ-
ences among the ybars, the larger sybar
2
 would be.
The theory has one more step to take: compare the two variance estimates 
via their ratio, call it F:
	
F
ns
s
ybar
2
2
. 	
As discussed in Chapter  3, the F‐statistic, as the ratio of two independent 
­estimates of the same variance, has a known probability distribution. In this 
situation, the particular F‐distribution has k − 1 degrees of freedom (df ) in the 
numerator and k(n − 1) df in the denominator. Comparing the F‐statistic we got 
to the appropriate F‐distribution provides us a means of evaluating the “real 
or random?” question about the differences among treatment means.
The F‐statistic is sort of a generalized t2 statistic. That is, the F‐statistic is 
roughly (can be thought of as) an average of squared t‐statistics among all 
pairs of the k treatments. The important fact is that the probability distribution 
function of F has been derived, calculated, tabulated, plotted, and programmed. 
The F‐distribution family (or at least selected percentiles) is tabulated in many 
texts and is available in software such as Minitab, JMP, and Excel.
Conceptually, the comparison of calculated F to its reference distribution is 
shown in Figure 4.2. The scale of F is not shown, because it changes for differ-
ent members of the family of F‐distributions, but the center of the distribution 
is around 1.0 because the numerator and denominator variances are ­estimating 
the same underlying σ2 so the nominal value of this ratio is 1.0.
The more variability there is among treatment means, relative to the within‐
treatment variability, the larger the F‐ratio will be. Thus, larger F‐values mean 
stronger evidence against the hypothesis of no real difference among treatments. 
This means that the appropriate P‐value to calculate to summarize this picture of 

99
Completely Randomized Design
the comparison of the F‐value we got to the distribution of F‐values we might 
have gotten, just randomly, is, as shown in Figure 4.2, the upper tail—the area 
under the curve to the right of the observed F‐value.
Analysis of variance
For many years, dating back before computers, the preceding calculations have 
been organized into an “ANOVA” table. Even with computers doing the tedious 
calculations for us, software still presents the calculations leading up to and 
including the F‐test statistic and its P‐value in this ANOVA table format. Table 4.2 
gives the ANOVA table for the present case of a completely randomized 
experiment with one treatment factor with k levels. In subsequent chapters, the 
ANOVA table will be extended to more complicated experimental designs.
The column entries in Table 4.2 are the following:
Source means source of variation. The table entries, reading from the bottom, 
are Total, Within Treatments, and Among Treatments. Shorthand terminology 
is Total, Error, and Treatments. Error means experimental error—the variation 
among experimental units that receive the same treatment, that is, the ­variation 
within Treatments.
df means degrees of freedom. Just as a variance calculated from n observa-
tions has n − 1 degrees of freedom, a variance calculated from k means has 
k − 1 df.
F-test, conceptually
F
Observed F 
P-value
Figure 4.2  Graphical F‐Test for Testing the Hypothesis of No Difference Among the 
Underlying Means for k Treatment Groups.

Fundamentals of Statistical Experimental Design and Analysis
100
SS means sum of squares. The entries in this column are various algebraic 
expressions involving sums of squared quantities. The double summations in 
these mathematical expressions mean that the sum is taken over both 
­subscripts, i and j.
MS means mean squares. It is calculated by dividing SS by df.
TMS stands for treatment mean square and is in fact the numerator of the  
F‐statistic developed previously.
EMS stands for error mean square and is the denominator of the F‐statistic 
developed previously.
F is the F‐statistic, namely, the quotient, TMS/EMS.
P‐value is the probability of exceeding the observed F‐value, denoted by 
Prob(>F), based on the F‐distribution with, for this situation, k − 1 numerator 
degrees of freedom and k(n − 1) denominator degrees of freedom. The P‐value 
tells you how far out on the upper tail of the distribution the F‐statistic falls. 
The smaller the P‐value, the stronger the evidence of real differences among 
the treatment means.
Discussion
In Table 4.2, the df for Treatments and for Error add up to equal the Total df. The 
Total SS is not used directly in the F‐test calculation, and its inclusion in the ANOVA 
table traces back to hand or mechanical calculator ANOVA calculations.
Now, algebraically, it can be shown that the sums of squares add up like the 
df: Total SS = Treatment SS + Error SS. This relationship would be used in 
the calculations by calculating Total SS and TSS using the appropriate variance 
formulas and then calculating ESS by subtraction. Or, if all three SS’s were 
­calculated, they could be checked to see if they added up as they should. No 
more do we need to do all this, but we who date back to that era get some 
comfort from seeing the full ANOVA table and seeing that the entries that are 
supposed to add up really and truly do.
These relationships among the SS and df help us understand conceptually 
what the ANOVA is doing with the data—it is partitioning total variability 
into separate parts: (i) variation among and (ii) variation within Treatments 
Table 4.2  Analysis of Variance (ANOVA) Table for the Completely Randomized Design: 
One Treatment Factor with k Levels and n Observations per Treatment.
Source
df
SS
MS = SS/df
F
P‐Value
Treatments
k − 1
n
ybar
ybar
i
.
2
TMS
TMS/EMS
Prob(>F)
Error
k(n − 1)
y
ybar
ij
i
2
EMS
Total
nk − 1
y
ybar
ij
.
2
ybar. is the overall mean; ybari is the mean for treatment i.

101
Completely Randomized Design
(displays in this example). This is similar to doing a chemical analysis of some 
compound to determine the relative concentrations of its constituent ele-
ments. We are doing this separation in order to judge whether the variability 
among treatment means (the possible signal of a real difference among dis-
plays) is appreciably larger than the experimental‐error variability (the noise). 
The F‐distribution provides the frame of reference for evaluating the signal‐
to‐noise ratio.
Results
Table 4.3 gives the ANOVA results from Minitab for the display experiment. In 
addition to the ANOVA table, Minitab provides a graphical display that 
­compares the means for the four displays.
The P‐value of .004 for the test of no difference among Displays is quite 
strong evidence of a real difference in the sales boosts provided by the ­different 
displays. Display 2 had increased sales of about 6.2%, while the other displays 
had increases of roughly 3–4%. That sales difference may be practically 
significant, in dollars, as well as statistically significant.
Figure  4.3 gives the picture behind the P‐value in the previous ANOVA 
table. The observed F‐ratio for comparing displays is 6.81. The appropriate 
reference distribution under the hypothesis of no difference among displays is 
the F‐distribution with 3 and 16 numerator and denominator df, respectively. 
Software, tables, and Figure  4.3 show that the F‐ratio calculated from the 
experimental data we got falls at about the upper .004 point on the F(3,16) 
distribution. The probability associated with the right tail area beyond F = 6.81 
is so small that the distribution curve and the horizontal axis cannot be 
Table 4.3  ANOVA of SalesIncr% Data.
Source
DF
SS
MS
F
P
Displays
3
33.91
11.30
6.81
.004
Error
16
26.57
1.66
Total
19
60.47
S = 1.29
Individual 95% CIs for mean based on pooled StDev
Level
N
Mean
StDev   ‐‐‐‐‐‐+‐‐‐‐‐‐‐‐‐+‐‐‐‐‐‐‐‐‐
D1
5
3.2
1.34     (‐‐‐‐‐‐‐*‐‐‐‐‐‐‐)
D2
5
6.2
1.65                              (‐‐‐‐‐‐‐*‐‐‐‐‐‐‐)
D3
5
2.8
.74  (‐‐‐‐‐‐‐*‐‐‐‐‐‐‐)
D4
5
4.3
1.25            (‐‐‐‐‐‐‐*‐‐‐‐‐‐‐)
       ‐‐‐‐‐‐‐‐‐‐-‐+--------+--------+--------+
                3.0          4.5       6.0     7.5

Fundamentals of Statistical Experimental Design and Analysis
102
distinguished at the scale of the figure. The P‐value of .004 is fairly substantial 
­evidence of a real difference in sales among the three displays. Only four times 
in 1000 repetitions of sampling from four identical Normal distributions (sample 
sizes of five) could you expect to get an F‐ratio this large or larger (F > 6.81).
Before the shampoo company makes the decision to install Display 2 nation-
wide, costs would need to be figured in and the return on investment evaluated. 
Floor or shelf space for the displays may be expensive. Also, as indicated by 
the questions previously, the useful life of a new display needs to be consid-
ered. If people get bored by the display and sales fall back quickly, or if the 
competition comes up with something hotter, it could all go for naught. The 
market research department could be embarrassed. This may not be rocket 
science, but it is still complicated—and serious business.
Often in experiments like this, a Control group will be included. In this 
experiment, each store’s previous “base” period of sales served as its control. 
However, to be sure that the increases seen for the stores in the study are 
not just the result of something fortuitous, such as good weather, it would 
have been useful to select five additional stores where no display was 
added and collect their base and pseudo‐test‐week sales and the percentage 
change in sales to provide a baseline against which to compare the 20 stores 
with displays.
In physics, there is an “observer effect:” the act of measuring something 
affects the thing being measured. That happens in human affairs, too. Store 
personnel who see the new displays going up might conclude, “Hmm, looks 
like the home office is trying to boost the sales of Shampoo X. Let’s talk it up 
8
7
6
5
4
3
2
1
0
.8
.7
.6
.5
.4
.3
.2
.1
.0
F(3,16)
Density
6.81
.004
F-test: shampoo sales experiment
F, df1 = 3, df2 = 16
Figure 4.3  Graphical Depiction of F‐Test for Shampoo Sales Experiment.

103
Completely Randomized Design
with our customers.” The experiment’s protocol would need to prevent this 
sort of bias from infecting the experiment.
Testing the Assumptions of Equal  
Variances and Normality
The assumption underlying the ANOVA is that the underlying standard devia-
tions for the three displays are identical. Empirical studies have been done 
evaluating the effect of deviations from this assumption. The general conclusion 
is that it depends—on how seriously the assumption is violated and on the 
sample sizes. In general, though, the ANOVA F‐test is fairly robust to violations 
of the assumption of equal underlying variances.
There are formal statistical significance tests of the equal‐variances 
­hypothesis for more than two groups of data in the statistical literature and in 
software. Minitab does the Bartlett’s and Levene’s tests (NIST/Sematech 2010). 
For the Display data, those tests confirm what the eye easily sees in Figure 4.1: 
the variability of the five data points in each display is quite consistent across 
the four displays. Similarly, with these small sample sizes, there is no reason to 
discredit the Normal distribution assumption underlying the analysis.
Confidence Intervals
So, Display 2 increased sales by about 6.2%. By being based on only five 
stores, that’s a pretty imprecise estimate. To make the decision that it will be 
profitable to install this display nationwide, let’s evaluate that imprecision and 
see if that decision is adequately supported.
We could use only the Display 2 data and calculate a confidence interval on 
an underlying mean just as we did for the underlying mean difference in wear 
for the boys’ shoes data. But, for data from a CRD, we can do better. The 
underlying statistical assumption, supported by the data, is that the underlying 
variability in each display group is the same for all three groups. That means 
we can use the data from all four displays to estimate the common sigma. The 
ANOVA does that for us.
A term used in statistics for this use of more than just the Display 2 data is “bor-
rowing strength.” Our test is strengthened, made more powerful, by ­borrowing 
information in the other three displays to estimate the common σ. The additional 
strength is reflected in the degrees of freedom: k(n − 1) for S versus n − 1 for s2.
In the ANOVA table, the EMS is the pooled estimate of the common 
­variance, σ2. Its square root, 1 66
1 29
.
.
 (denoted by S in Table 4.3), based on 
16 df, is the standard deviation that we will now use in the confidence interval 
for the underlying average sales% for Display 2, call it μ2. The formula is
	
95
2
16
5
2
025
%
..
:
/
.
.
C I on
ybar
t
S
	

Fundamentals of Statistical Experimental Design and Analysis
104
The 5 divisor in this formula comes from the fact that ybar2 is based on five 
observations. Plugging in the following values:
	
ybar
S
t
2
6 2
1 29
16
2 12
025
. ,
.
,
.
.
	
leads to
	
95
6 2
1 2
5 0
7 4
3
% ..
:
. %
. %
. %, . % .
C I on
	
(This is the confidence interval roughly displayed in the Minitab output in 
Table 4.3.)
In round numbers, the finding is that our data are consistent with an under-
lying average 1‐week increase in sales of between 5 and 7.4% (at the 95% level 
of confidence). Shall we go national with Display 2?
Suppose our accountants have told us that if we can get a 4% or more 
increase, the new display will be profitable. Then, the data, via the confidence 
interval, say there’s negligible risk in installing it: the return on investment is 
substantially greater than 4%. If I am the shampoo company’s chief financial 
officer (CFO), though, I would be concerned about whether a 1‐week spike in 
sales is enough evidence. It’s annual profit that gets the attention of stock-
holders and determines my bonus. If the market research staff says that in our 
experience and that of others in similar situations, we have found that the 
staying power of a new display is such that over a year we will show one‐half 
the gain as the initial week, then I would not support installing Display 2 
because one‐half of the lower bound is 2.5%, substantially less than 4%. 
Otherwise, I’d like to see the sales data for at least a few months of Display 2 
exposure before adopting it.
Based on the results, we have pretty well dropped Displays 1, 3, and 4 
from consideration. Suppose the shampoo company’s CFO’s niece, who 
is enrolled in an MBA program, but is working at the company as a 
summer intern, had developed Display 4. She says, “My display did pretty 
well, too. My two best stores outsold the lowest two stores with Display 
2. Display 4 may be close enough to Display 2 to warrant further 
consideration.” The marketing department, recognizing the sensitivity of 
this issue, hires a statistical consultant from the business school at the 
state university.
The consultant calculates a 95% confidence interval on the underlying mean 
difference, μ2 − μ2, as follows:
	
ybar
ybar
t
S
2
4
16
2
5
025
–
.
.
	
The factor 2 shows up in this equation because we are dealing with the 
difference of two means of five observations. (The theory tells us that if a mean 
has a standard deviation of 
1/ n , then the difference of two means with 

105
Completely Randomized Design
these same standard deviations has a standard deviation of /
2 / n .) Doing 
the calculation leads to
95% confidence interval on μ2 − μ4:
	
6 2
4 3
3 2
1 9
1 7
2
2 9
.
.
. %
.
. %
. %, . % .	
The CFO’s niece says, “See. The difference could be pretty small (less than 
a percentage point) and my display is less expensive. Let’s do some more 
testing before we make a final decision.” The consultant says, “Yeah, and I 
can design you a really neat experiment.” And, so it goes …. Science 
marches on.
Inference
The inference about the underlying difference between μ2 and μ4 is a state-
ment about the size of the difference between Normal distribution means 
that could result in the difference observed in the experiment’s data—the 
20 participating stores. Broader inference relies on how the selected stores 
relate to the population of stores nationwide. If the 20 stores were selected 
at random nationally, then the statistical inference could be extended to 
that population. Otherwise, broader inference is a matter of subject‐matter 
knowledge: characteristics of the 20 stores relative to the nationwide col-
lection of stores. For example, if, for convenience, the experiment was run 
in Ohio, then inference beyond Ohio has to be based on knowledge about 
how Ohio stores and ­customers and shampoo preferences relate to those 
elsewhere. Sometimes, “just‐pretend” inference is engaged in. Let’s pre-
tend these 20 stores are a random sample from all stores that carry 
Shampoo X. Under that “pretension,” it is then claimed that the inference 
about the underlying mean difference is a statement about that hypothet-
ical population. I prefer to interpret patterns in the data we have and rely 
on subject‐matter knowledge to carry broader inferences beyond the 
experimental framework.
Statistical Prediction Interval
The statistical confidence interval for an underlying mean is a statement of 
uncertainty about nationwide sales, that is, sales averaged across a very large 
number of stores. All individual store managers, of course, imagine themselves 
as being above average, but they still might wonder, what might the sales% 
increase be in my store? Statistical prediction intervals (see, e.g., Meeker and 
Hahn 1991) answer this question.

Fundamentals of Statistical Experimental Design and Analysis
106
The formula for a 95% prediction interval for a single future sales increase 
when Display 2 is used is
	
ybar
t
df S
n
2
1
1
025
.
.	
In this expression, df is the Error degrees of freedom, S is the square root of 
the Error MS, and n is the number of observations on which ybar2 is based, 
namely, five.
Evaluating this expression leads to
	
95
6 2
4 9
1 3
11 1
%
% : .
. %
. %,
. % .
prediction interval for sales
	
This could make the store manager feel reasonably comfortable, particularly if he 
has reasons to expect the sales boost to persist at a reasonable level. The store 
manager, though, has a different perspective than the shampoo ­manufacturer 
does. There’s only so much shampoo you can sell, so increased sales of Shampoo 
X in this experiment may be offset by decreased sales of other shampoos.
Note that the common (estimated) “two‐sigma” limit in this situation would 
be ybar2 ± 2S = 6.2% ± 2.6% = (3.6%, 8.8%). This can be regarded as a crude, 
approximate 95% prediction interval, but it is somewhat more optimistically 
precise than the statistical prediction interval. The statistical prediction interval 
accounts for both the imprecision of S as an estimate of sigma and the 
­imprecision of ybar2 as an estimate of the underlying μ2, the way any good 
statistician should do.
Example: Tomato Fertilizer Experiment Revisited
As noted in the introduction, the tomato fertilizer experiments in Chapter 3 
were examples of a CRD in the special case of only two treatments. The 
­significance testing portion of the analysis there was a two‐sample t‐test. The 
data can also be analyzed via an ANOVA. The assumptions underlying both 
analyses are the same: two independent random samples from the same 
Normal distribution, so you might hope and should expect that the results will 
be the same. And you would not be disappointed. Table 4.4 presents that anal-
ysis for Experiment 2.
The F‐statistic for no difference between Fertilizers A and C is 3.60, which 
when compared to the F(1, 14) distribution gives a P‐value of .08. By way of 
comparison, consider one of the analyses in Chapter 3 which was a two‐sample 
t‐test under the assumption of equal underlying standard deviations. The 
result was t = 1.90, which when compared to the t(14) distribution, had a one‐
tail P‐value of .04. The two‐tail P‐value, then, is .08. This matches the ANOVA result. 
The F‐test ignores the sign of the difference between Fertilizers C and A, 
as does the two‐tail t‐test. The two analyses do result in the same result. 

107
Completely Randomized Design
Further, note that the square of the t‐test statistic equals the F‐ratio: (1.9)2 = 3.6. 
In general, the square of a t‐value based on f degrees of freedom is equivalent 
to F with 1 and f degrees of freedom. What is lost in the F‐statistic is the 
direction of the difference between the two treatment means. Of course, if 
we have plotted the data before doing the ANOVA (Analysis 1: Plot the Data), 
we already know the direction of the difference. Also, it is the case that the 
pooled standard deviation in the t‐test is equal to the square root of the Error 
MS in the ANOVA table.
Sizing a Completely Randomized Experiment
Because a completely randomized experiment generally ends up with an 
­analysis involving the precision of the estimate of one particular treatment’s 
mean or of the difference between two treatment means, the methods dis-
cussed in Chapter 3, based on confidence interval widths or power curves, can 
be applied to the issue of sizing future experiments. The only difference is that 
for three or more treatments in the experiment, there will be more degrees of 
freedom associated with the estimated standard deviation than in the case of 
two treatments. Thus, applying the two‐sample sample size analyses to k > 2 
situations will be conservative, though not greatly so. Statistical literature has 
other power‐curve analyses for sizing a CRD.
CRD: Single Quantitative Factor
In many experimental situations, some of the factors of interest are quantitative 
variables, often continuous. This is particularly true in experiments that address 
physical or chemical processes that involve factors such as temperature, 
pressure, concentration, voltage, electrical current, and the like. Dosage levels 
in medical experiments, fertilizer amounts in agricultural experiments, and 
advertising dollars spent in a business experiment are other examples of a 
quantitative treatment factor.
For the case of a single quantitative factor, designing an appropriate CRD 
experiment involves the same issues as in Chapter 4—choice of factor levels 
and the nature, number, and allocation of the experimental units to the 
Table 4.4  ANOVA for Tomato Fertilizer Experiment 2.
Source
DF
SS
MS
F
P
Fert.
1
16.61
16.61
3.60
.08
Error
14
64.60
4.61
Total
15
81.21
S = 2.15

Fundamentals of Statistical Experimental Design and Analysis
108
selected treatment levels. Allocation of treatment levels to experimental units 
is completely at random, per the CRD protocol. However, the choice of levels 
and replication can be driven by other considerations. Instead of choosing the 
number of replications, say, to provide a given level of precision in estimating 
the difference between two treatment means, the objective can be designing 
the experiment with enough data to estimate the slope of a line with a desired 
level of precision. Replication is still a consideration in order to provide an 
estimate of experimental error variation with adequate precision.
The analysis of the resulting data can and should differ considerably. The 
objective of the data analysis for a quantitative factor often is to fit a 
mathematical function, a curve or a multidimensional function to the data, 
rather than determining if the observed average response differences, say, 
between two levels of a quantitative variable, were real or random, picking 
winning treatments, and estimating selected treatment differences.
Curve fitting (more formally regression analysis) is a much‐used statistical 
tool in contexts other than designed experiments. For example, the manufac-
turer of a product made by injection molding may keep data on various 
processing variables, such as injection volume and mold temperature, and 
then do strength tests on a sample of produced items. After accumulating 
such observational data for some period, a regression analysis could be done 
seeking a mathematical model for the relationship of strength to volume and 
temperature. As many authors, including BHH (1978, 2005), have pointed out, 
such analyses of “happenstance,” or observational data, can be inadequate or 
misleading, primarily due to the lack of control of the predictor variables 
(volume and temperature in this example). If you really want to know how 
changing the injection volume changes product strength, you have to run a 
controlled, randomized experiment.
Example: Growth rate of rats
Box, Hunter, and Hunter (1978, 2005) give an example pertaining to the growth 
rate of rats in which the factor of interest is the amount of a particular dietary 
supplement (denoted by x, measured in grams) in a rat’s diet. Ten rats were in 
the experiment, with one to three rats being assigned to each of the six levels 
of x selected for the experiment. BHH note that for the sake of clarity, ­textbook 
examples are generally smaller than real‐world experiments would be, but 
that’s OK for our purposes, too. This is the sort of experiment that might be 
done for a science fair—which reminds me of a story. I once judged a junior 
high school science fair. When I came to the end of one student’s report, I 
read: “Unfortunately, I was unable to come to any conclusions due to the 
untimely death of my control rat.” The honesty was good. Sadly, though, the 
experimental design lacked replication.
The protocol for feeding the rats and measuring their growth rates are 
left to the reader’s imagination. The growth rates are given in a coded 

109
Completely Randomized Design
measurement: weight gain (perhaps ounces) per unit of time, such as a day. 
Also, in terms of context, the reader should assume that the experimenters 
were not just interested in rats, but rather in what they could learn from these 
experiments that would possibly be applicable to humans.
Graphical display
The natural data plot for these data is a scatter plot of the response, y  
(= growth rate) versus x (= supplement amount), as shown in Figure 4.4. This 
plot shows clearly that growth rate first increases as a function of x, reaches a 
maximum growth rate in the neighborhood of 20–25 g of supplement, and 
then decreases for higher amounts of supplement. Even a rat can get too 
much of a good thing! But seriously, this plot calls out for a curve to be 
smoothed through the data and that is the direction the analysis takes.
Curve fit
What kind of curve should we fit? Well, it’s possible that biological–nutritional 
theory could suggest a function to use. In the absence of theory, we will just 
consider simple mathematical functions. The simplest is a straight line: 
y = a + bx. Clearly, from the data plot, this experiment’s data would not be well 
fitted by a straight line. There’s a definite concave shape to the relationship. 
A quadratic function, y = a + bx + cx2, is a possibility. For negative c, this curve 
35
30
25
20
15
10
90
85
80
75
70
65
x-supp
y-gr-rate
Scatter plot of y-gr-rate vs x-supp
Figure  4.4  Data from Growth Rate Experiment. Source: Box, Hunter, and Hunter 
(2005, p. 381), used here by permission of John Wiley & Sons.

Fundamentals of Statistical Experimental Design and Analysis
110
will have a concave downward shape. Statistical curve fitting is done by a 
method called least squares regression. The regression analysis, in this case, 
finds the values of a, b, and c that give a curve that fits closest to the data in 
the sense of minimizing the sum of squared differences between the data 
points and the fitted function. Details can be found in many textbooks and 
internet sources. Statistical software does the fitting. Minitab does the honors 
here; the results are in Table 4.5.
Before evaluating the fitted equation in Table 4.5, let’s discuss the other 
entries in the table. There are some t‐test statistics and P‐values. Where did 
they come from? To answer this, we need to introduce the statistical model 
explicitly.
The statistical model underlying the Table 4.5 analysis is the equation
	
y
x
x
e
2
,	
where e denotes random error which is assumed to be a random observation 
from a Normal distribution with a mean of zero and an unknown standard 
deviation, σ. (You can tell that we’re into serious modeling here because Greek 
letters are used for the model coefficients.) In words, this model (for data we 
might have gotten) says that there is a quadratic function that gives the mean of 
the distribution of rat growth rates at any particular value of x and individual 
data points vary around this mean curve according to a Normal distribution with 
a standard deviation, σ, that is constant across all x. Under that model, the impre-
cision of a, b, and c as estimates of α, β, and δ can be evaluated by ­calculating 
the standard errors (SEs) of the coefficients. The t‐values in the table come from 
comparing the estimates to hypothesized values of zero. Thus, t = COEF/SE. If 
growth rate was not a function of the amount of supplement given the rats, the 
underlying coefficients of x and x2 would be zero; the “curve” would be a 
horizontal line. The large t‐values and small P‐values for the coefficient estimates 
confirm the visual impression in Figure 4.4 and show that the relationship bet-
ween growth rate and amount of supplement is ­definitely not a horizontal line.
The question of model fit can be answered graphically. Figure 4.5 overlays 
the fitted curve on the data plot and shows that the model fits the data quite 
nicely. This impression will be substantiated by the following data analysis.
Table 4.5  Regression Analysis Results.
The Regression Equation Is
y‐gr‐rate = 35.7 + 5.26 x‐supp − .128 x‐sq
Predictor
Coef
SE Coef
t
P
Constant
35.7
5.6
6.35
.000
x‐supp
5.3
.56
9.43
.000
x‐sq
−.13
.013
−9.97
.000
S = 2.54

111
Completely Randomized Design
Analysis of variance
The quadratic model leads to its own ANOVA table: Table 4.6. Let’s discuss the 
entries in this ANOVA table.
The first line in the ANOVA, Regression, represents the variation accounted 
for by the two variables, x and x2, in the regression model. That is why there 
are two df associated with the Regression SS.
The Total SS is the sum of squared deviations of the 10 observations from 
the overall mean (the numerator of s2 calculated from all the data). It has 9 df 
(= n − 1) and the third coefficient in the model, the constant, α, is accounted for 
in this SS, as is reflected in the df.
Residual Error is the remaining variation: the difference between the Total SS 
and the Regression SS. The Residual Error SS is the sum of the squared differ-
ences between the observed growth rates and the fitted growth rates based 
on the model. This is the quantity that was minimized by the least squares 
fitting method. The Residual Error has seven df. The intuition for this is that we 
started with 10 data points. We fitted a model with three constants in it that 
were estimated from the data. Thus, the unexplained or residual variation has 
7 df associated with it.
The ANOVA in Table  4.6 goes further. In the two indented lines of the 
ANOVA following the Residual SS, the Residual SS is partitioned into Lack of 
Fit and Pure Error. We can do this separation because of the replication in the 
experiment. As can be seen in Figure  4.4, this experiment had multiple 
­experimental units (rats) at some of the x‐values. At two x’s, there were two 
35
30
25
20
15
10
90
85
80
75
70
65
60
x-supp
y-gr-rate
Scatter plot of y-gr-rate vs x-supp
Figure 4.5  Data Plot and Fitted Model.

Fundamentals of Statistical Experimental Design and Analysis
112
replications; at one, there were three. The variability within these three small 
groups, pooled together, provide the Pure Error SS, which has 4 df associated 
with it: 1 df from each group of two eus and 2 df from the group of three eus 
pooled together, just as the within‐groups variability was pooled together in 
the CRD ANOVA for a qualitative factor. What’s left, by subtraction, is called 
Lack of Fit. If the quadratic model was not adequate, the Lack of Fit MS would 
be large relative to the Pure Error MS. If the fit is adequate, these two MS’s are 
independent estimates of the residual variance, σ2. The F‐ratio of these two 
MS’s thus tells us how well the selected model fits the data. In the case of the 
growth rate data, F = .90, on 3 and 4 df, and the P‐value of .52 is large; it shows 
that the F we got is right in the middle of the distribution of Fs we might have 
gotten if there is no lack of fit relative to the fitted quadratic model. The two 
MS’s are nearly equal. There is no evidence against the quadratic relationship 
for these data, as we could see in Figure 4.4. If that had not been the case, we 
would have had to try another model.
One benefit of having a mathematical function that relates growth rate to 
amount of supplement is that we can use the fitted function to predict growth 
rates at values of x where we have no data. Also, we can work the following 
problem.
Suppose you’re the producer of the rat diet supplement (hey, somebody’s 
got to do it) and need to tell customers what amount of supplement is needed 
to achieve certain growth rates. For example, if the target is a growth rate of 
80 or more, you can draw a horizontal line at y = 80 and then read down from 
the fitted model to the corresponding x interval. Doing this determination in 
Figure 4.4 leads to the finding that a supplement level between, roughly, 12 
and 30 g will meet this goal. This analysis could be refined to include the impre-
cision with which the curve is estimated. Statistical software makes it possible 
to obtain confidence intervals and prediction intervals at any x‐value. Figure 4.6 
shows what are called “point‐wise” 95% confidence intervals on the under-
lying average growth rate as a function of x. For example, at x = 20, the 
conclusion is that with 95% confidence, the underlying average growth rate of 
rats fed this amount of supplement is between 87 and 92 g.
If you read down from where the lower confidence limit intersects the line at 
a growth rate of 80, the approximate x interval that will achieve this is roughly 
Table 4.6  ANOVA for Quadratic Model.
Source
DF
SS
MS
F
P
Regression
2
665.7
332.9
51.6
.000
Residual error
7
45.2
6.46
Lack of fit
3
18.2
6.06
.90
.52
Pure error
4
27.0
6.75
Total
9
710.9

113
Completely Randomized Design
from 14 to 28 g. So, being conservative, one might recommend supplement 
amounts in this tighter interval (relative to that obtained from the fitted model) 
to achieve a growth rate of at least 80.
Design Issues
You may have noticed that I did not recount or make up a story about how this 
rat growth rate experiment came to be designed as it was. I succumbed to the 
statistician’s innate enthusiasm for data analysis and plunged right in. Let us 
now take a belated look at the design considerations. The situation is to design 
a CRD experiment for the case of one quantitative treatment factor.
The design issues, mentioned previously (p. 107), are:
●
●
What levels of x to choose?
●
●
How many replicates should be assigned to each level?
The starting point would be to determine the range of x. Here, the experi-
menter determined that the amount of supplement fed to rats, per feeding, 
I presume, would be from 10 to 35 g (.35 to 1.2 ounces). BHH (2005) state that 
“From similar investigations it was believed that over the range tested the 
response would be approximately linear.” Boy, were they (the investigators) in 
for a surprise!
35
30
25
20
15
10
95
90
85
80
75
70
65
60
x-supp
y-gr-rate
Fitted line plot
y-gr-rate = 35.66 + 5.263 x-supp
–0.1277 x-supp**2
S
2.54092
R-Sq
93.6%
R-Sq(adj) 91.8%
Regression
95%CI
Figure 4.6  Fitted Model with 95% Confidence Intervals on the Underlying Expected 
Growth Rate as a Function of Amount of Supplement.

Fundamentals of Statistical Experimental Design and Analysis
114
If it was assumed that the response was a linear function of x, theory tells 
us that the statistically “optimum” design would have been to feed half of 
the rats 10 g, the other half 35 g. That is, under the assumption of a straight‐
line relationship, over the x interval considered, this design provides the 
most precise estimates of the slope and intercept. (Intuitively, under the 
straight‐line assumption, you would devote your data to pinning down 
the two end points and then drawing the connecting line.) With a two‐level 
design, though, there is no way to verify or test the linearity assumption. We 
can see from the previous plots of the data if we experimented at only x = 10 
and 35 g, we would get a very different curve fit: a straight line with a down-
ward slope. We would completely miss the peak in between. We would 
wrongly conclude that maximum growth is attained at 10 g and more supple-
ment would actually slow growth.
Fortunately, the investigators were working with an applied statistician, 
rather than a theoretical one, in designing the experiment. He or she cau-
tioned, “You really should run some intermediate points just to be able to 
detect a departure from the linear relationship you expect. In my experience, 
nature can sometimes throw you a curve (heh).”
After some further discussion, it was decided to run the experiment at 5 g 
increments from 10 to 35 g and to devote 10 rats to the experiment. Fortunately, 
that allowed for some replication in the experiment from which experimental 
error variation could be estimated and used to assess the goodness of fit for the 
final model. The pattern of replications is a little unusual—due to the untimely 
death of some experimental units?—but the experiment still provided a useful 
model. One can see in Figure 4.6 that the confidence limits are wider at the right 
end of the curve than at the left end, because of the differences in replication. A 
follow‐up experiment over the range of 15–25 g might be useful in estimating the 
level of x that maximizes rat growth with more precision than this experiment 
provided. For a real experiment pertaining to rat diets, see Vento et al. (2008).
Enhanced Case Study: Power Window  
Gear Teeth
The data in this example are from an article by 
Gunter (1988) and later appeared in the text by 
Wild and Seber (2000). The following story is my 
dramatization of the episode. Any resemblance 
to real events and people is purely accidental.
Once upon a time, way back in the 1980s, an 
American car manufacturer encountered a 
problem with a plastic gear in the power window 
mechanism: teeth were breaking and jamming 

115
Completely Randomized Design
the window. Warranty claims and angry letters rolled in to Detroit. Either the 
gear teeth did not have the strength they were required to have or the stresses 
to which the gears were subjected were greater than had been designed for. 
A crisis team was assembled to find and fix the problem. “We need data!” 
someone cried. Data were needed on gear tooth strength in order to identify 
and then resolve the problem. The plan was to grab a bunch of gears, then test 
them to determine the stress at which they would break. Those data could 
identify whether the spec was wrong or if the gears were not meeting the 
spec. Gears can be tested by putting a gear in a fixture with one tooth held in 
place, then torqueing the gear until the tooth breaks. The stress at which the 
tooth breaks is the recorded response. Because the gear is damaged, or effec-
tively destroyed, only one tooth on a gear could be tested to failure.
At first glance, at least to the layperson, a gear, as in the above clip art, looks 
to be symmetric, front and back and around its circumference—there are no 
identifying features that distinguish the gear teeth, 12 in this case, on a gear. 
Those who know the manufacturing process, which is injection molding, know, 
however, that there is a small dimple at the point at which powder, say, is 
injected into a mold. This “injection port” is on the end of one of the teeth, so 
this dimpled tooth provides a reference point on the gear. It was important to 
have this reference point because someone on the team with knowledge of 
the manufacturing process must have said, “Breaking strength might be 
related to tooth position. We need to keep track of the position of each tooth 
we test and we need to be sure that we test an adequate number of teeth at 
each position in order to have enough data to see if tooth strength is related 
to position.” This subject‐matter knowledge was the key ingredient in the 
experiment and led to resolution of the problem.
The project team adopted the following tooth numbering scheme: the 
gear is oriented with the injection dimple at the bottom. Then the teeth are 
numbered clockwise from the top tooth, which is designated position 1. 
Thus, the tooth at the injection point is in position 7. These gears are 
symmetric, front to back, so you cannot tell, for example, whether a tooth 
immediately adjacent to position 1 is position 2 or position 12. Thus, strength 
measurements of a tooth adjacent to position 1 can only be identified as 
position 2 or 12. Similarly, the other teeth can be paired according to their 
position relative to position 1.
Details of this experiment are not available, but we will suppose it was car-
ried out as a CRD: each of the available gears was randomly assigned to be 
tested (broken) at one of the seven positions. That is, treatment t in this 
experiment was to destructively test the tooth in position t, for t running from 
1 to 7 (recognizing that position pairs 2 and 12, 3 and 11, etc., cannot be dis-
tinguished). As will be seen in Table 4.7, the number of teeth tested varies 
considerably among positions—from 9 to 33. Whether that was planned or just 
happened is not known. If a statistician had been on the team to begin with, 
he or she might have insisted on better balance. The best precision for 

Table 4.7  Gear Teeth Data and Schematic.a
Position 1
Positions  
2 and 12
Positions  
3 and 11
Positions  
4 and 10
Positions  
5 and 9
Positions  
6 and 8
Position 7
1976
2425
2228
2186
2228
2431
2287
1916
2000
2347
2521
2180
2250
2275
2090
2251
2251
2156
2114
2311
1946
2000
2096
2222
2216
2365
2210
2150
2323
2132
1940
2593
2299
2329
2228
1904
1964
1904
2204
2072
2263
1695
2048
1750
1820
2228
2323
2353
2000
2222
2018
2012
2198
2449
2251
2006
2048
1766
2204
2150
2300
2275
1945
2174
2144
2311
2078
1958
2006
1976
2305
2102
2150
2185
2209
2138
2042
2138
2377
2216
2455
2120
1982
2108
1934
1886
2419
2042
2257
1904
2246
2162
2030
2383
1958
2287
2251
2216
2323
1964
2030
2222
2305
2246
2066
2210
2204
2251
2222
2084
2198
2156
2066
2383
2204
2419
1964
2132
2162
2329
2150
2210
2120
2198
2114
2222
2108
2269
2125
1766
2030
2287
2210
2078
2180
2330
1588
1994
2251
2329
2234
2198
2210
2228
2210
2162
2216
2156
1874
2168
2204
2132
2210
1641
2108
2341
2263
1892
2000
2120
1671
2132
2156
1
A
Impact strength
1976
P 1
P 1
P 1
P 7
P 7
P 2 & 12
P 2 & 12
1916
2090
•••
•••
•••
•••
•••
•••
•••
•••
•••
•••
•••
•••
2425
2000
2120
2156
Position
B
2
3
4
35
36
163
164
1
2
3
4
5
6
7
8
9
10
11
12
a Source: reproduced with the permission of John Wiley & Sons from Wild and Seber (2000, p. 119).

117
Completely Randomized Design
­comparing tooth positions is obtained by having an equal number of tests at 
each position. Balance, though, is not required for the analysis of data from 
the CRD.
Table 4.7 provides the data. The tooth breaking strength is called “impact 
strength” and is measured in foot‐pounds of torque. The table shows that the 
measurements were identified by position or position pair: position 1, 2–12, 
3–11, etc. To start to identify and understand any impact strength patterns 
among tooth positions, let us do Analysis 1: Plot the data!
Graphical display
There are several ways to plot the data in order to compare the impact strength 
distributions across positions, including the individual value plots used earlier. 
Figure 4.7 is a Minitab “box and whisker plot” of the data. The boxes cover the 
middle 50% of the data at each position, and the whiskers are calculated to 
cover approximately 95% of the data in each group. Data points beyond the 
whiskers are shown as asterisks. Also shown, to help provide focus, are the 
average impact strengths at each position and a line connecting these 
­averages. The most notable pattern in the data is that it appears that average 
impact strength increases in going from position 2–12 to position 6–8. That is, 
as you can see in Figure 4.7, for these circumferential positions, impact strength 
decreases as the distance from the injection point increases. The impact 
Data
P7
P6–8
P5–9
P4–10
P3–11
P2–12
P1
2500
2250
2000
1750
1500
Box plot of P1, P2–12, P3–11, P4–10, P5–9, P6–8, P7
Figure 4.7  Box and Whisker Plot of Impact Strength Data.

Fundamentals of Statistical Experimental Design and Analysis
118
strengths at the injection point and directly across, positions 7 and 1, both 
have relatively lower impact strengths and look similar to each other.
To show the tooth‐strength pattern even more clearly relative to the gear, the 
average impact strengths were plotted on a schematic of the gear: Figure 4.8.
After seeing these data, my imagined scenario 
is that the injection‐molding specialist on the team 
clapped himself on the forehead and exclaimed 
something like, “I know what’s going on. Our sup-
plier is shorting us! They’re not injecting enough 
powder so we’re not getting the material density 
we need consistently throughout the mold.” (If the 
team had not thought of identifying the data by 
tooth position, they would have missed the whole 
story!) At this point, the ­statistician who has been 
brought in to analyze (or autopsy) the data, says, “I see the pattern you’re 
talking about, but there’s quite a bit of strength variability within tooth‐positions 
as well as across different positions. Let me do a bit of analysis to see whether 
the apparent differences could be just due to random variation among gears.” 
She soon reports back that the pattern is real (if pressed, she can even present 
the ANOVA and explain a P‐value). The team reports to the vice president in 
charge and recommends coming down hard on the supplier.
Sandia Stats-5 (numer.descr.)
2093
Geographic display of averages 2
2044
2045
2256
2250
2153
Inject. port
2210
Figure 4.8  Average Impact Strengths by Position.

119
Completely Randomized Design
At this point, it gets ugly. The VP says, “How come you and the supplier 
haven’t been monitoring this process? How come you didn’t catch this 
problem before it became a major field problem? That’s your job. You didn’t 
do it. In the words of Donald Trump, ‘You’re Fired’—all except the statistician, 
Mary—Is that your name? Mary, I want you to talk to the Executive Committee 
about how we could better monitor and improve the processes we’re 
responsible for and how many statisticians we should hire to help us do it 
right,” (I am making this up.)
ANOVA
Mary did an ANOVA, the results are shown in Table 4.8. The F‐ratio for evalu-
ating the variation among positions is 6.86, on 6 and 156 df, with a resulting 
P‐value less than .001. It is quite unlikely to get the sort of differences among 
tooth positions seen here, just by chance.
Our previous plots of the data tell us about the trend around the perimeter 
of the gear. Minitab shows us 95% confidence intervals for each underlying 
mean by position and shows us the separation: the confidence intervals for 
positions 4–10, 5–9, and 6–8 are all above and not overlapping the confidence 
intervals for positions 1 and 7, while the means for positions 2–12 and 3–11 are 
intermediate between these two groups.
Table 4.8  ANOVA of Gear Teeth Impact Strength Data.
Source
DF
SS
MS
F
P
Factor
6
975 056
162 509
6.86
.000
Error
156
369 4221
23 681
Total
162
4 669 278
S = 153.9
Individual 95% CIs for mean based  
on pooled StDev
Level
N
Mean
StDev
--------+---------+---------+---------+-
P1
33
2085.9
172.6
       (----*---)
P2‐12
9
2044.7
215.4
 (-------*--------)
P3‐11
17
2152.5
162.0
             (-----*------)
P4‐10
33
2191.3
126.7
                     (----*---)
P5‐9
27
2261.0
102.9
                            (---*----)
P6‐8
11
2256.0
120.3
                      (-------*-------)
P7
33
2067.0
178.2
         (---*----)
 -------+---------+---------+---------+-
       2040    2160      2280     2400
Pooled StDev = 153.9

Fundamentals of Statistical Experimental Design and Analysis
120
Discussion
The ANOVA told us that the tooth strengths observed in our test data differ 
among tooth positions by more than could be due just to chance. We don’t 
know enough about the gears in our test—their pedigrees—to know that they 
adequately represent the gears that have been installed in cars and that have 
subsequently broken. It’s possible that the injection problem has been present 
through all of the production or it might just have been associated with a few 
production runs. There is a need for further investigations to pin down the 
extent of the problem. This is why you see recall notices for certain cars and car 
models and production periods.
The preceding analysis treated the tooth positions as qualitative levels of 
the treatment: position. From the schematic, though, we see that we could 
represent these positions quantitatively. For example, x could be the ­distance 
from the injection point to the base of each tooth. Or x could be the angle 
formed between the radii for each tooth versus tooth 7. There might be physics 
flow models that suggest particular mathematical functions. That’s an area for 
further research. If indeed the problem is one of powder‐volume control—
tooth strengths around the gear depend on how much powder is injected into 
the mold—then we might want to run some other experiments with injection 
volume as a quantitative factor of interest. We might want to include other 
process variables, such as the time and temperature settings for the molding 
process, in those experiments. More work for Mary and the company’s statisti-
cians. Rather than a blame game, one would hope that the manufacturer’s 
personnel would work ­harmoniously with the supplier’s personnel to under-
stand the production process and establish appropriate controls to assure that 
reliable product is produced—and live happily ever after.
Assignment
Choose a topic of interest to you and identify issues to investigate with two 
experiments.
(a)  Design a completely randomized experiment to compare at least three 
levels of a qualitative treatment factor. Describe the design in detail: 
experimental units, treatments, response measurement, experimental 
protocol, and issues to be addressed. Describe your anticipated data 
plots and analyses.
(b)  Design a completely randomized experiment to investigate the effect of 
a single quantitative treatment factor. Describe the design in detail: 
experimental units, treatments, response measurement, experimental 
protocol, and issues to be addressed. Describe your anticipated data 
plots and analyses.

121
Completely Randomized Design
References
Box, G., Hunter, W. G., and Hunter, J. S. (1978, 2005) Statistics for Experimenters,  
1st and 2nd eds., John Wiley & Sons, New York.
Gunter, B. (1988) Subversive Data Analysis, Part II: More Graphics, Including My Favorite 
Example, Quality Progress, 21, 77–78.
Ledolter, J., and Swersey, A. (2007) Testing 1‐2‐3, Stanford University Press, Stanford, CA.
Meeker, W., and Hahn, G. (1991) Statistical Intervals: A Guide for Practitioners, John 
Wiley & Sons, Inc., New York.
NIST/SEMATECH (2010) e‐Handbook of Statistical Methods, http://www.itl.nist.gov/
div898/handbook/2010.
Simon, J. (1997) Resampling: The New Statistics, http://www.statistics101.net/index.htm.
Vento, P., Swartz, M. E., Martin, L. B., and Daniels, D. (2008) Food Intake in Laboratory 
Rats Provided Standard and Fenbendazole‐supplemented Diets, Journal of the 
American Association for Laboratory Animal Science, 47(6), 46–50.
Wild, C., and Seber, G. (2000) Chance Encounters, John Wiley & Sons, Inc., New York.


Fundamentals of Statistical Experimental Design and Analysis, First Edition. Robert G. Easterling. 
© 2015 John Wiley & Sons, Ltd. Published 2015 by John Wiley & Sons, Ltd.
Introduction
Processes and natural phenomena generally 
involve multiple variables or “factors,” for example, 
cookies. Producing cookies from a box of mix 
requires adding certain amounts of other ingredi-
ents to the mix, mixing the dough by hand or mixer 
for a specified time, preheating an oven, and then 
baking cookies for a specified time at a specified 
temperature. A cake‐mix manufacturer like Betty 
Crocker or Duncan Hines needs to know how the 
various factors (steps and ingredients in the recipe) 
affect cookie quality, perhaps as measured by a panel of tasters. The manufac-
turer can then use this information to determine the preparation and baking 
instructions on the box. In addition to maximizing quality, the manufacturer 
would like the process settings (recipe) to be “robust,” meaning that if you and 
I don’t get the ingredients or settings quite right or if our ovens don’t quite 
achieve the desired temperature, we will still get good cookies. Similar “multi-
factor” problems need to be solved in many other contexts. It is not a coinci-
dence that the silicon chips that so much of our technology relies on are called 
“cookies” in their production.
In Chapter 2, we described how treatments (or blocks) may be defined by 
two or more factors and those factors could be either crossed or nested. They 
can also be either qualitative or quantitative. In this chapter, we continue the 
discussion of experiments that have a completely randomized design (CRD) 
and extend applications of this design to situations in which the treatments in 
Completely Randomized 
Design with Multiple 
Treatment Factors
5

Fundamentals of Statistical Experimental Design and Analysis
124
the experiment are structured according to two or more factors. The conduct 
of the experiment—random assignment of treatments to individual units in a 
single group of homogeneous eus—is the same as in the previous chapters, 
but the factorial structure of the treatments calls for different graphical ­displays 
and different quantitative analyses (primarily analysis of variance (ANOVA)) 
than we have covered so far.
Some authors use the term, “factorial experimental design,” but I do not. 
Experimental design pertains to the organization of experimental units and 
the assignment of treatments to those experimental units. If, as in this chapter, 
the treatments are combinations of two or more factors, that treatment struc-
ture doesn’t change the design. It is still a CRD. We could say that we have a 
“factorial treatment design” in our CRD experiment. In the next chapter, we 
will see factorial treatment designs in the context of another family of experi-
mental designs, the randomized block design.
Design Issues
In addition to the issues faced in determining a particular CRD, the issues in a 
multifactor experiment are first the choice of factors and then the choice of levels 
for each factor in the experiment. We suppose initially that a full factorial set of 
treatment combinations will be included (meaning, e.g., that with one four‐level 
factor and one five‐level factor, the experiment would include all 20 possible 
treatment combinations). You can quickly see that if many factors with many 
levels are included in an experiment, the number of treatment combinations can 
become unwieldy. But if you have too few levels, you may have inadequate 
­coverage of the factor space. In choosing a design, there can be trade‐offs bet-
ween the numbers of factors, factor levels, and replications to resolve. Designing 
an experiment that uses only a fraction of the full set of factorial treatment com-
binations is another option that will be discussed later. Methods for finding 
“optimal designs” in unwieldy circumstances are discussed in Chapter 7.
The two primary examples in this chapter are CRDs with two‐factor treatments. 
In the first example, both factors are qualitative, and in the second, both factors 
are quantitative. This distinction is reflected in both the design and the analysis.
Example 1 (Two qualitative factors):  
Poisons and antidotes
In an experiment first reported in Box and Cox (1964) and then included in the 
Box, Hunter, and Hunter (BHH) (2005, p. 318) text, laboratory animals were 
exposed to a poison and then treated with an antidote. The response measured 
is survival time. The goal was to identify the antidote or antidotes that were most 
effective in combating the poisons. No other information was given about the 
experiment, which means I can make up a story and set it in modern times.

125
Completely Randomized Design with Multiple Treatment Factors
This experiment may cause some uneasiness, so let’s set it in a national security 
context. If our soldiers are ever in a chemical or biological warfare environment 
and are exposed to poison, we want them to be carrying antidotes that they can 
take that will delay the effect of the poison long enough for them to reach an aid 
station or field hospital and receive life‐saving treatment, if at all possible. There 
are many potential poisons and possible antidotes. We obviously cannot test 
them on people, so if we’re going to learn anything ­useful about antidotes, we’ll 
need to learn it from laboratory animals (my ­apologies to PETA). It’s up to biology 
experts then to transfer that information to the human scale, if possible.
The experiment (continuing my invented story) was conducted by a univer-
sity laboratory under a government contract. The government agency chose 
the three poisons to span a range of threats, knowing that they had different 
levels of lethality and perhaps attacked different organs. The four antidotes, 
let us suppose, were provided by four pharmaceutical companies competing 
for a contract. A desirable outcome would be to find one or more antidotes 
that are effective against all of the poisons in the experiment.
Running the experiment with all the combinations of these two factors 
means that the experiment had 12 treatments (3 poisons × 4 antidotes). Based 
perhaps on preliminary experimentation, it was decided that there would be 
four replicates (let’s say rats, again) of each of the 12 treatments, randomly 
assigned. The investigators established a detailed protocol for exposing an 
animal to a poison and then administering an antidote. Pre‐1964, someone 
would then sit in the lab with a stopwatch (graduate students did this sort of 
work) and record the animal’s survival time. Today, we will suppose each rat 
had an electronic chip implanted that would record the time of death and send 
the data to a monitoring computer. The experiment’s data are in Table 5.1
Table 5.1  Animal Survival Times (in tenths of hour).
Poison
Antidote
A
B
C
D
I
31
82
43
45
45
110
45
71
46
88
63
66
43
72
76
62
II
36
92
44
56
29
61
35
102
40
49
31
71
23
124
40
38
III
22
30
23
30
21
37
25
36
18
38
24
31
23
29
22
33
Source: Box, Hunter, and Hunter (2005, p. 318), used here by 
permission of John Wiley & Sons.

Fundamentals of Statistical Experimental Design and Analysis
126
Analysis 1: Plot the data
The experiment’s data are plotted in Figure 5.1, which shows the rat survival 
times (in tenths of an hour) versus the antidote for the three poisons sepa-
rately. As has been stated before, it is important that initial data plots include 
all the dimensions in the data, as is done here.
Eyeball analysis
We see in Figure 5.1 a very similar pattern across all three poisons: antidotes 
B and D look to be consistent winners (they result in the longest lifetimes) 
for all three poisons. The poisons also differ in their lethality—in particular, 
Poison III results in considerably shorter lifetimes than the other two poi-
sons. We also see that the variability within each group of four animals dif-
fers appreciably among the 12 treatment combinations. Intuitively, the 
pattern makes sense: if a poison is slow acting, there is apt to be more var-
iability in the longer survival times than there is for poisons that act quickly 
and survival times are short.
BHH decide to use a transformation of the data to make the data standard 
deviations more homogeneous. (Aside. The article by Box and Cox in 1964 is 
a very influential, oft‐cited paper on transformations.) The reason for making a 
transformation is that we want to be able to use the statistical model of 12 
independent random samples of four observations from Normal distributions 
with the same variances as our starting point—our model for “data we might 
D
C
B
A
120
90
60
30
D
C
B
A
120
90
60
30
I
Antidote
Life
I I
I I I
Plot of lifetime (tenths of hours) vs. antidotes, by poison
Panel variable: Poison
Figure 5.1  Data Plot: Poison—Antidote Experiment.

127
Completely Randomized Design with Multiple Treatment Factors
have gotten”—for evaluating whether the apparent poison and antidote 
­differences we see in Figure 5.1 are “real or random.” To enhance the appro-
priateness of this evaluation, we would like the 12 sets of data to have more 
homogeneous variances.
BHH’s analysis led them to chose to analyze the reciprocal, 1/(lifetime), of 
the data. I will call this transformed response the death rate. In reliability appli-
cations, the term sometimes used is “force of mortality.” Low death rates 
are advantageous—they correspond to longer lifetimes. Figure 5.2 is a plot 
of  these death rates, expressed in units of hour−1, and shows that this 
­transformation did make the within‐group variances much more similar.
As with the original data, we see some definite patterns among both poi-
sons and antidotes in Figure 5.2. Antidotes B and D consistently result in lower 
death rates. Could this pattern just be random or are there some real poison 
and antidote effects underlying these patterns? As you might expect, ANOVA 
will be the tool by which we address this question.
Before getting into ANOVA intricacies, let’s stop for a moment and admire 
the value of the factorial structure of poisons and antidotes. All four anti-
dotes are tested against all three poisons. This balance and symmetry help 
us see patterns. We can make side‐by‐side antidote comparisons poison by 
poison. We can find consistent differences and inconsistent differences. 
Other treatment structures (combinations of poisons and antidotes) without 
this factorial balance and symmetry would not be so accommodating and 
informative.
D
C
B
A
.5
.4
.3
.2
.1
D
C
B
A
.5
.4
.3
.2
.1
I
Antidote
Death-rate
II
III
Panel variable: Poison
Plot of death-rate vs. antidote, by poison
Figure 5.2  Plot of Death Rates versus Antidotes, by Poisons.

Fundamentals of Statistical Experimental Design and Analysis
128
For example, if we asked each poison manufacturer, “What antidote(s) 
should we use to treat your poison?” one might say A and D, another might 
say B, C, and D, and the third might say A and C. If we designed our experiment 
accordingly, overall patterns would be very difficult to see with this sort of 
mishmash. And overall patterns are very important here. If we find one anti-
dote that is effective against all three poisons, then that means the field kits 
we provide soldiers need only contain that antidote. Otherwise, the kit might 
require two or more antidotes, and the soldier would have to identify the 
poison and then take the right antidote. When time is of the essence, this is 
not good.
Interaction
As a starting point, we could analyze these data as we would for a CRD with 
12 qualitative treatments. The ANOVA would have 11 df for treatments, and 
the F‐test would tell us whether the variation among average death rates for 
those 12 treatments was “real or random.”
The factorial treatment structure, though, permits us, indeed requires us, to 
go further: we can separate out the variability among antidotes, averaged over 
poisons (and it is the balanced, factorial structure that makes the antidote 
averages comparable). The corresponding ANOVA entry would be an SS for 
antidotes based on 3 df, in this case, because it pertains to the variability 
among four antidotes. Similarly, the variation among the three poisons, aver-
aged over antidotes, would have 3 − 1 = 2 df. That leaves 6 df, from the treat-
ments’ 11 df (11 − 2 − 3 = 6), left over. What sort of variability is that reflecting?
The answer is interaction—the interaction between poisons and antidotes. 
To explain interaction, let’s start with a picture—what Minitab calls an “interac-
tion plot.” The average death rates for the four rats in each poison/antidote 
group are tabulated in Table 5.2. The interaction plot, Figure 5.3, is a plot of 
these means versus the levels of one of the factors, for each level of the other 
factor, overlaid.
Figure 5.3 shows that Antidote B is consistently the best: it has the lowest 
death rate (longest survival time) for all three poisons. Antidote D is close to 
B in effectiveness, particularly against Poison III. Antidotes A and C are 
Table 5.2  Average Death Rates (units of hour−1) by Poisons and Antidotes.
Average Death Rate
Antidote
Poison
A
B
C
D
I
.25
.12
.19
.17
II
.33
.14
.27
.17
III
.48
.30
.43
.31
Average
.35
.19
.30
.22

129
Completely Randomized Design with Multiple Treatment Factors
consistently the least effective. Moreover, the poison‐to‐poison patterns for all 
four antidotes are fairly consistent: the death rates are somewhat higher for 
Poison II than for Poison I and are substantially higher for Poison III than for 
Poison II. The four lines are nearly parallel.
Perfectly parallel lines would be a case of no interaction. Of course, with 
only four rats per poison/antidote combination, experimental error (variation 
among the lifetimes of these rats) means that this plot of the treatment means 
is not apt to be exactly parallel, even if the (unknown) underlying means exhib-
ited no interaction (plotted as parallel lines). That’s why we need the ANOVA’s 
F‐test to tell us whether apparent interaction (nonparallel lines) is “real or 
random” (more pronounced than could be due just to random variation).
By way of contrast, if the lines in Figure 5.3 crisscrossed substantially, this 
would depict high interaction. In that case, the best antidote might be differ-
ent for each poison; there would be no consistent winners. Soldiers would 
have to carry several poison‐specific antidotes.
An interaction plot of the means in Table 5.1 can also be done reversing the 
roles of the two factors: Figure 5.4 is the result.
This plot focuses on the poison differences, and the near‐parallel lines in this ori-
entation indicate that the poison differences are quite consistent across antidotes. 
Poison I is consistently the least lethal (low death rate, high lifetime), Poison II is 
intermediate, and Poison III is the most lethal. The choice of plot depends on 
which factor is of primary interest—antidote in this example. Minitab includes an 
option to plot both interaction plots in separate panels on the same plot.
III
II
I
.5
.4
.3
.2
.1
Poison
Mean
A
B
C
D
Antidote
Interaction plot for death-rate
Data means
Figure 5.3  Interaction Plot. Average death rate versus poison, by antidote.

Fundamentals of Statistical Experimental Design and Analysis
130
ANOVA
Formulas for the ANOVA entries in a multifactor experiment are given in many 
texts and will not be repeated here. We will trust the software to do the correct 
calculations and go directly to the ANOVA for this experiment (Table 5.3). 
What we have to know in order to use the ANOVA properly is which mean 
squares (MS) to compare in order to make the appropriate “real or random” 
evaluations. By the fact that this experiment was carried out as a CRD with 12 
treatments arranged in a 3 × 4 structure, with four replications per treatment 
combination, we can calculate separate MS for interaction, antidotes, and poi-
sons and compare them to the error MS, which is “pure error,” calculated from 
the variability among the four experimental units in each treatment, pooled 
across all 12 treatment combinations.
Table 5.3  ANOVA for Death Rate.
Source
DF
SS
MS
F
P
Poison
2
.349
.174
72.6
.000
Antidote
3
.204
.068
28.3
.000
Interaction
6
.016
.0026
1.1
.39
Error
36
.086
.0024
Total
47
.655
D
C
B
A
.05
.04
.03
.02
.01
Antidote
Mean
I
II
III
Poison
Interaction plot for death-rate
Data means
Figure 5.4  Interaction Plot. Average death rate versus antidote, by poison.

131
Completely Randomized Design with Multiple Treatment Factors
Interpretation of the ANOVA for a multifactor experiment starts at the 
­bottom. That is, we (should) consider first the F‐statistic for interaction. The 
observed F‐statistic of 1.1 falls near the middle of the F‐distribution based on 
6 and 36 df, as the P‐value of about .4 indicates. Thus, the interaction MS is 
indistinguishable from the error MS. There is no evidence of underlying 
­interaction. Our eyeball‐analysis conclusion of consistent antidote and poison 
differences is quantitatively supported by the ANOVA.
Moving up a line, the F‐test for comparing the antidotes compares the four 
antidote means, averaged across poisons. The denominator is again the error 
MS. The F‐ratio of 28.3, on 3 and 36 df, with a resulting P‐value of essentially 
zero, is off the chart: there are definite differences among antidotes, as our 
eyeball analysis told us. The fact that poison/antidote interaction was not 
significant means that it is meaningful to compare antidotes averaged across 
poisons. If there had been significant interaction, this would have been an 
­indication that comparing antidotes averaged across poisons might not be 
appropriate. Significant interaction means that the differences among ­antidotes 
are not consistent across poisons. Averaging across poisons could wrongly 
suggest a consistent winning antidote.
Lastly, moving up one more line, because the poisons were selected to be 
different, the highly significant (off the chart) F‐test for poisons, averaged over 
antidotes, is confirmation that the poisons were different, as planned, not a 
surprise or a finding on which to act.
The Minitab graphical output in Figure 5.5 shows graphically the source of 
the significant difference among antidotes. There is substantial separation bet-
ween Antidotes A and C (high death rates, short lifetimes) versus B and D, 
which are statistically tied (substantially overlapping confidence intervals).
Generalizing the ANOVA for a CRD with two factors
As noted previously, if we had done the ANOVA for a CRD with 12 treatments, 
four experimental units per treatment, the treatment line in the ANOVA would 
have had 11 df. In Table 5.2 ANOVA, those 11 df have been partitioned into 
Individual 95% CIs for mean based on pooled StDev
Antidote mean 
+---------+---------+---------+----
A      .35
(----*---)
B      .19
(----*----)
C      .29
(----*----)
D      .22
(----*----)
----+---------+---------+---------+----
.180      .240      .300      .360
Figure 5.5  Antidote Average Death Rates.

Fundamentals of Statistical Experimental Design and Analysis
132
poisons (2 df  ), antidotes (3 df ), and poison by antidote interaction (6 df ). In 
general, if the first factor, A, has a levels and the second factor, B, has b levels, 
then there will be a − 1 df for factor A, b − 1 df for B, and the product (a − 1)
(b − 1) df for the A–B interaction. The df for these three lines in the ANOVA 
add up to ab − 1. The corresponding SSs add up to equal the treatment SS for 
the 12 treatments. In the general case of r replicates in each treatment, the 
error SS and MS are based on ab(r − 1) df: r − 1 df for pure error in each 
treatment, pooled across ab treatments.
The partitioning of treatment SS into main effect and interaction SS is 
due to the crossed factorial structure of the two factors. The term some-
times used is to say the two factors are “orthogonal.” Just any old 12 com-
binations of poisons and antidotes would not have this property. They 
would not provide a clean separation of what are called the “main effects” 
(average effects) of A and B and the interaction effect of A and B. One of 
the most important contributions of statistical experimental design is the 
development of efficient and informative multifactor designs that provide 
this clean separation. This is not to say that multifactor treatment designs 
that do not have a crossed factorial structure cannot be analyzed to obtain 
and evaluate estimated main effects and interactions. Methods based on 
general linear models can be used. Morris (2011) and Milliken and Johnson 
(2009) are experimental design texts that illustrate these analyses in a wide 
variety of contexts.
Antidote B versus Antidote D
The graphical display in Figure  5.4 indicates that there may not be a real 
difference between Antidotes B and D. Thus, if we want to pick a winning anti-
dote, we may not have enough evidence to justify picking the narrow winner, 
B, over D. We can quantify this evidence by a significance test of the difference 
in mean death rates and by a confidence interval on the underlying mean 
difference.
The average death rates from Table 5.1 are ybarB = .19 and ybarD = .22. 
Each of these means is based on 12 observations (four rats for each of 
three ­poisons). Therefore, the variance of the difference between these 
two means is 2σ2/12 = σ2/6, where σ is the experimental error standard 
deviation—the ­variability among experimental units that receive the same 
treatment. This standard deviation is estimated by the square root of 
the error MS in the ANOVA table (Table 5.2), namely, s
(
)
.
.
0024
049. 
The standard error (SE) of the difference between two antidote means is 
given by substituting s for σ in the variance of the difference and then taking 
the square root:
SE
diff
.
.
.
0024
6
02

133
Completely Randomized Design with Multiple Treatment Factors
This is the yardstick against which we evaluate the observed difference in 
means via the t‐statistic:
t
ybar
ybar
B
D
SE(diff)
.
.
.
. .
19
22
02
1 5
The degree of freedom associated with this t‐statistic is the error df, which is 
36. The resulting P‐value is .07 (one tail).
The 95% confidence interval for the underlying mean difference in death 
rates is given by
ybar
ybar
t
B
D
SE
.
(
) *
.
.
(.
)
.
.
.
, .
025 36
03
2 03 02
03
04
07
01 .
If the government agency wants stronger evidence, say, a one‐tail P‐value less 
than .025 (which would correspond to a two‐sided 95% confidence interval 
that did not cover a difference of zero) in order to make a contract award, then 
there is not enough evidence here to choose Antidote B over D. A runoff 
experiment would be required. The sorts of power and sample size analyses 
done in Chapter 3 could be used to size the experiment.
Estimation of effects
Let’s suppose we want to estimate the survival time distribution (for lab rats) 
for a particular poison/antidote combination. In particular, let’s look at the 
strongest poison, Poison III, and the best antidote, B.
One straightforward way to approach this problem is to consider only the 
four survival times for that case. This would yield a fairly imprecise estimate 
of survival time for that poison/antidote combination. Our analysis findings, 
so far, in particular the finding of no interaction between poisons and anti-
dotes, enable us to do a more precise analysis. This analysis requires some 
preliminaries.
Common terminology in the analysis of two‐ or more‐factor experiments is 
to consider the effects of the factors. Let ybarall denote the overall mean of the 
data, that is, the mean of all 48 death rates. That mean is .26 hour−1. The anti-
dote effects are defined as the differences between the antidote means and 
the overall mean:
A effect: .35 − .26 = .09
B effect: .19 − .26 = −.07
C effect: .29 − .26 = .03
D effect: .22 − .26 = −.04
(Note: If I had carried more digits in these means, the four effects would have 
added to zero.)

Fundamentals of Statistical Experimental Design and Analysis
134
Thus, B and D, which have negative effects, decrease the death rates; A and 
C increase it, relative to the average antidote effect.
The poison effects are:
Poison I effect: .18 − .26 = −.08
Poison II effect: .23 − .26 = −.03
Poison III effect: .38 − .26 = .12
Because we found that there was no interaction in the data, the antidote 
effects apply to any of the three poisons in the experiment. Thus, the esti-
mated mean death rate for Antidote B applied to Poison III is the Poison III 
mean of .38 plus the effect of Antidote B (which is negative).
Est. mean death rate for Poison III, Antidote B = μ^B,III
^ = .38 − .07 = .31.
Note that this estimate is based on all 48 data points in the experiment, not 
just the four data points pertaining to that particular poison/antidote 
combination. When the expressions for the various means are expanded alge-
braically, one can see that this estimate is a weighted sum of the mean death 
rates in the 12 poison/antidote treatment combinations. Those weights are 
given in Table 5.4.
Note that the largest positive weight (1/2) is for the III/B combination. All 
the Antidote B cells have positive weights as do all of the Poison III cells. All 
other poison/antidote combinations have negative weights. The weights 
sum to 1.0.
Now, what is the SE of this estimate? Each cell mean has a variance of σ2/4. 
The cell means are all independent; they’re based on separate data. Theory 
tells us that the variance of a weighted sum of quantities that all have the same 
variance is equal to the sum of the coefficients squared times that common 
variance. In Table 5.3, the squared coefficients sum to 1/2. Thus, the variance 
of the estimated mean is σ2/8. If we had estimated the III/B mean from the III/B 
data only, that estimate would have a variance of σ2/4. Thus, by using all of the 
data, we have reduced the variance by a factor of two. This additional preci-
sion, thank you very much, was brought to you by the finding of no interaction. 
Instead of having an antidote effect depend on the poison, we found that 
the antidote effect was independent of poison, and thus, we estimated the 
­antidote effect averaged over poisons. Ditto for the poison effect.
Table 5.4  Weights in Estimated Mean Death Rate: Poison III, Antidote B.
Poison
Antidote
A
B
C
D
I
−1/12
1/4
−1/12
−1/12
II
−1/12
1/4
−1/12
−1/12
III
1/6
1/2
1/6
1/6

135
Completely Randomized Design with Multiple Treatment Factors
Substituting the error MS for σ2 provides the SE of this estimated mean and 
also confidence intervals on the underlying mean death rate for the III/B 
combination of poison and antidote:
SE
B III
,
^
.
.
.
0024
8
017
95
31 2 03 017
31
035
%
: .
.
.
.
.
.
,
confidence interval for
B III
Note that we are using the error MS based on all the data; thus, the SE is 
based on 36 df. If we had used only the III/B data, the standard deviation 
of those data would have been based on only 3 df. Thus, by being able to 
use all the data to estimate the III/B mean and the error variance, we 
improve the ­variance of the estimate and improve the precision with which 
we estimate that variance, compared to using only the III/B four observa-
tions. As mentioned earlier, the term used to describe this aspect of a 
statistical analysis is “borrowing strength.” The clever device of analyzing 
the reciprocals (the death rates), rather than the lifetimes, homogenized 
the variances and resulted in a no‐interaction pattern among the cell 
means.
The preceding analysis has been mostly illustrative. It shows how all the data 
are used in estimating death rates for particular situations and how such 
­“borrowing strength” is reflected in SEs for estimates of interest. The software, 
based on the underlying mathematical models, will calculate the effects and 
estimated means and their SEs, directly.
Now, when dealing with a variable such as lifetime, we’re probably not 
­interested so much in average or median lifetimes as we are in distribution 
extremes, such as: What survival time will be exceeded by 99%, say, of lab rats 
(and ultimately, troops) exposed to Poison III and then treated by Antidote B? 
Answering that question is the topic of the next subsection.
Prediction intervals
In Chapter 4, we calculated prediction intervals for sales increases associated 
with a particular shampoo display. By following the same approach, we can 
use the results of this experiment to predict the lifetime of a rat exposed to 
one of the experiment’s poisons and then injected with one of the experiment’s 
antidotes.
Let the symbol, yfuture, denote a future individual death rate (reciprocal 
­lifetime) for a rat exposed to the III/B combination. A point prediction of yfuture 
would be the previous (p. 134) estimated mean death rate for that combination, 
namely, μ^
B,III = .31. We need to account for two sources of uncertainty in using 
this estimated mean to predict a future outcome: (i) individual death rates will 
vary, with a standard deviation, σ, and (ii) the estimated mean is imprecise: its 
standard deviation is / 8.

Fundamentals of Statistical Experimental Design and Analysis
136
Consider the difference yfuture − μ^
B,III. The variance of this difference, obtained 
by summing the variances of the two terms in this difference, is σ2(1 + 1/8). If we 
replace σ2 by the error MS and then take the square root, we obtain the SE of 
the difference:
SE
future
B III
y
–
.
.
.
^
,
0024 9
8
052
The difference divided by its SE has a t‐distribution, in this case based on 36 
df. Thus, we can write, for example,
.
.
.
.
(
(
) /
)
^
,
98
2 43
2 43
Prob
SE
future
B III
y
That is, the middle 98% of a t(36 df ) distribution is given by the interval ±2.43. 
(I just wanted to use a confidence level other than 95% to show it can be 
done.) If we plug in the estimated mean and the SE of the difference and 
then rearrange this inequality so that yfuture is in the middle, we get what is in 
essence a 98% confidence interval on the future death rate ( = the reciprocal 
lifetime) of a single rat. The result is
98
31 2 43 052
31
13
1
%
: .
.
.
.
.
(.
confidence interval for
future
y
8 44
, .
).
The statistical terminology is to call a confidence interval on a future ­observation 
a statistical prediction interval.
Taking the reciprocals of the end points of this prediction interval for death 
rates yields the following 98% prediction interval for survival time: (2.3, 5.6) h. 
We’re worried about short survival times, so the conclusion here would be that 
with 99% confidence, a future survival time (for a random rat subjected to the 
III/B combo) would be at least 2.3 h.
Probability estimation and tolerance intervals
The statistical model for “data we might have gotten” in this experiment is that 
the death rates for rats given a particular poison/antidote combination have a 
Normal distribution. We don’t know the mean and standard deviation of that dis-
tribution, but the experimental data provide us estimates of the mean and stan-
dard deviation of the distribution. For the Poison III/Antidote B combination, the 
estimated mean is .31 and the estimated standard deviation is 
.
.
0024
049. 
We can use these estimates to estimate survival probabilities.
Consider a particular survival time of interest, call it L*. (The choice of this 
limit would be based on a biological analysis pertaining to what constitutes an 
adequate survival time.) We want to estimate the probability that a rat will sur-
vive at least L* hours. Denote this probability by Prob(L > L*), where L denotes 

137
Completely Randomized Design with Multiple Treatment Factors
survival time. We can relate this probability to the probability distribution of 
death rate by
Prob
Prob
L L
y
L
*
* ,
1
where y = 1/L. The statistical model under consideration is that y has a Normal 
distribution. For the Poison III/Antidote B case, we can estimate this proba-
bility by using the estimated mean and standard deviation given in the pre-
ceding paragraph. Thus, for example, the estimated probability of exceeding 
a lifetime of 2.3 h is equal to the probability that y < 1/(2.3) = .435, where y has 
a Normal distribution with mean .31 and standard deviation .049. That proba-
bility, using software or tables of the Normal distribution, is .995.
This estimated survival probability, of course, is imprecise: it is based on 
data‐based estimates, not known parameters. We need a confidence interval 
on the underlying survival probability. Consider the previous calculated 95% 
confidence interval on the III/B mean: .31 ± .035 = (.275, .345). If we calculate 
the probability that y < .435 for Normal distributions with means at either end 
of this interval and a standard deviation of .049, we will obtain an approximate 
95% confidence interval on the underlying survival probability.
(This calculation is an approximation because we haven’t accounted for 
the uncertainty in the estimated standard deviation. However, because the 
t‐distribution with 36 df is negligibly different from the standard Normal 
distribution, the approximation is not bad. An exact calculation is possible 
but beyond the scope of this text. Search online for “statistical tolerance 
intervals.”)
Setting the mean equal to .275 yields a survival probability of .9995. Setting 
the mean equal to .345 yields .967. Thus, with 95% confidence, the probability 
of surviving 2.3 h is between .967 and .9995.
This analysis can be repeated for other values of L*. Doing so leads to the 
survival curves in Figure 5.6. Reading the three curves at L* = 2.3 h gives the 
point estimate and (approximate) 95% confidence interval for the survival 
probability calculated in the preceding paragraphs.
We can also read horizontally in Figure 5.6 and determine confidence inter-
vals on particular percentiles of the survival time distribution. For example, 
drawing a horizontal line at a survival probability of .5 and then reading the L* 
values at which this line intersects the curves tell you that the estimated median 
lifetime is 3.23 h ( = 1/.31) and that with 95% confidence, the median lifetime is 
between 2.90 and 3.64 h. Statistical confidence limits on percentiles of a distri-
bution are called statistical tolerance intervals.
At this point, this analysis is just illustrative and somewhat academic. If the 
government agency had done the rat–human calibration and decreed that to be 
viable, an antidote, when applied to a rat that has been exposed to Poison III, 
should demonstrate at least a 95% survival probability at 2.5 h, and then we could 
focus the analysis on this criterion. The nominal survival probability at 2.5 h is 

Fundamentals of Statistical Experimental Design and Analysis
138
.97, but the lower end of the corresponding 95% confidence interval is .87 and 
thus does not meet the agency’s criterion. More data, or a better ­antidote, 
would be needed.
This and the previous subsection provide two sorts of statistical inferences:
1.  With 99% confidence, the lifetime of a future random rat subjected to the 
III/B combination will exceed 2.3 h.
2.  With 97.5% confidence, at least 96.7% of rats of the type used in the 
experiment would survive at least 2.3 h.
These are complementary, not contradictory. The first statement pertains to 
a single future individual; the second pertains to the proportion of a 
“population.”
Further experiments
The experiment considered here is apt to lead to further experiments, rather 
than to a contract. One such possibility mentioned previously (p. 133) is an 
experiment that amounts to a runoff between Antidotes B and D.
Alternatively, even if the contract is given to the maker of Antidote B based 
on this initial experiment, there is much to be learned about Antidote B before 
it goes into the field to be used by troops. For example, what might be the 
effects of different dose levels of poison and antidote on survival time, and 
5.0
4.5
4.0
3.5
3.0
2.5
2.0
1.0
.8
.6
.4
.2
.0
L*
Probability
P(>L*)
P-lower
P-upper
Estimated survival probabilities and confidence intervals
Figure 5.6  Estimated Survival Curve and 95% Statistical Confidence Intervals.

139
Completely Randomized Design with Multiple Treatment Factors
what might be the effects of different delay times between the exposure to a 
poison and the administration of the antidote? Follow‐on experiments would 
be needed to answer these questions. Presumably, prior to this experiment, 
the poison and antidote dosages and the antidote/delay times were discussed 
and the decision made to hold them fixed in these experiments. Now that the 
number of antidotes has been whittled down to one or two, the next step 
could be to design and run experiments to address these additional factors. 
It is typical in a design process to consider many factors of potential interest 
and then, in order to keep the experiment manageable, to decide to hold 
some factors fixed at specific levels and vary others as specified by the exper-
imental design. At the end of this chapter, we will discuss experiments with 
large numbers of factors.
Another direction for follow‐on experimentation is to do experiments 
on  another variety of laboratory animals in order to help build a basis for 
extrapolating the findings to humans. We might also want to consider other 
poisons. Subject‐matter knowledge is essential to sorting out and prioritizing 
all of this.
Example 2 (Two quantitative factors): Ethanol blends  
and CO emissions
The development of clean and efficient fuel sources is one of society’s major 
objectives. This example experiment tackles that problem.
Blending ethanol with gasoline reduces carbon dioxide and carbon ­monoxide 
emissions from automobile engines, but does not eliminate these emissions. 
The emissions generated by ethanol–gasoline mixtures depend on several 
factors pertaining to the fuel mixture and engine characteristics. BHH (2005, 
Chapters 10 and 11) give an example (used here with the permission of John 
Wiley & Sons) of a designed experiment for investigating the effects of two 
such factors on an automobile engine’s carbon monoxide (CO) emissions.
The two factors studied are x1 = ethanol concentration and x2 = air–fuel 
ratio. These are both quantitative factors. (Indeed, labeling the factors by x’s, 
as opposed to A and B, suggests that our analysis is going to be aimed at 
fitting a mathematical function to the data.) The ethanol factor is a fuel‐mix 
variable; the air–fuel factor is an engine/carburetor design variable. In the 
experiment, each factor had three equally spaced levels, and all nine combina-
tions (crossed factors) constituted the treatments in the experiment. The 
factors and their levels (Fig. 11.17 in BHH 2005) are:
Ethanol concentration x1
1 2
3
: . , . , .
Air
fuel ratio x2
14 15 16
:
,
,
(The units for these variables are not given. However, as discussed later, these 
are dimensionless ratios, generally expressed as percentages.) The BHH 

Fundamentals of Statistical Experimental Design and Analysis
140
analysis is based on coding the levels of x1 and x2 as −1, 0, 1, but we will use 
the original variables.
A little Wikipedia (2011) research indicates that these are pertinent variables 
and realistic and meaningful levels for these variables in ethanol–gasoline‐
mixed fuels are in use at present. In particular, x1 is the proportion by volume 
of ethanol in the ethanol–gasoline‐mixed fuel. E10, for example, which corre-
sponds to x1 = .1 (=10%), is a conventional level required in many states and 
countries. E25 was used in Brazil in 2008. Thus, x1 ranging from .1 to .3 pretty 
well matches actual ethanol‐mixed fuel.
The air–fuel ratio, x2, is also known as the “percent excess combustion air” 
relative to the amount of free oxygen that is required to combine with all of the 
fuel. An x2 value of 14.7% is the nominal ratio for gasoline. A higher ratio is a 
lean fuel mix; a lower ratio is a rich fuel mix. This experiment covered the range 
on x2 of 14 − 16%.
It was decided to run the 
experiment by making two 
runs on a test engine for each 
of the nine combinations of 
x1 and x2 levels. Experimental 
protocol would dictate details 
of a “run” (e.g., duration and 
throttle level) and the method by which the engine is purged and set  
up between runs. Protocol would also specify how emissions during the 
run would be captured and measured for CO (carbon monoxide). The CO 
measurements obtained were in units of micrograms per cubic meter  
(μg/m3). To guard against any time trends, the 18 runs were conducted in a 
random order.
What we do not know about the experiment’s design is why it was decided 
to run the experiment with each treatment factor at three levels or why it 
was decided to do two runs at each of the nine treatment combinations. 
Presumably (I’ll conjecture), prior testing and perhaps theory indicated that 
the relationships between these two variables and CO emissions were not 
linear. Three levels are the minimum required in order to detect and fit a 
nonlinear relationship. Moreover, just because of the physics involved, there 
was apt to be an interaction between x1 and x2. Thus, more than two levels 
would be required to detect and characterize these relationships. Both the 
cost of experimentation and subject‐matter knowledge may have supported 
the decision not to run the tests at four or five levels of the two treatment 
factors. The number of replications could have been cost driven. It’s pos-
sible that this was an exploratory experiment that would be followed by 
further experimentation aimed at zeroing in on, with good precision, the 
(x1, x2) region that minimizes CO emissions, without unduly sacrificing 
performance.

141
Completely Randomized Design with Multiple Treatment Factors
There are other ways to run an experiment that would result in two 
measurements of CO at nine combinations of x1 and x2, so it is important 
to detail the experiment’s design. The analysis could be different or even 
impossible, depending on how the experiment is conducted. Some design 
options are:
1.  The 18 runs could be blocked: one “block” of single runs of the nine 
treatment combinations, in a random order, could have been done first, 
followed by a second block of nine runs in a separately randomized order. 
The two blocks might be done on different days. Or they might have been 
done on two different test engines. These experiments would be 
randomized block designs, discussed in the next chapter. The analysis, as 
we shall see, would necessarily reflect this block structure.
2.  Another conceivable way the experiment might have been conducted 
would be to have had 18 test engines available and then randomly assign 
the nine treatment combinations to two engines each. This would still be 
a CRD, but the inferences would be different. In the experiment that was 
actually run, experimental error would be the variability of repeated runs 
on the same engine. If this 18‐engine experiment was run, experimental 
error would be the variability among different engines run under the same 
treatment combination. One would expect the latter to have the larger 
experimental error variation. However, the inference would be broader 
because 18 engines were involved, not just one.
3.  One other possibility is that the people running the tests might have 
looked at the list of test conditions listed in a random order provided by 
the project’s friendly, local statistician (FLS) and said to themselves, “This 
crazy mixed‐up order of testing that the boss specified will be too time‐
consuming. It will be easier and quicker if we set the test engine’s carbu-
retor for one fuel–air ratio and then make all six of the runs called for at 
that ratio consecutively. Furthermore, those six runs could be more conve-
niently ordered, say, by doing two consecutive runs with the low ethanol 
level, then two with the middle ethanol, and then two at the high ethanol. 
Then we will adjust the carburetor to change to the next fuel–air ratio and 
repeat those six runs and then do the same at the third fuel–air ratio.” 
Such “convenient” experiments destroy the key design features of ran-
domization and replication and make analysis and interpretation iffy, at 
best. If the experiment does not provide valid estimates of experimental 
error, you cannot expect valid conclusions about factor effects and 
interactions.
This discussion shows how important the procedural details of an experiment 
are. If a colleague, or client, walked into your office and said, “I’ve got 18 data 
points, two each at nine combinations of x1 and x2. Please analyze these 
data,” you can’t do the analysis until you know the data’s pedigree, how they 

Fundamentals of Statistical Experimental Design and Analysis
142
were obtained. The whole process works best if the FLS, or you, after reading 
this book, are involved in the design of the experiment, not just the number 
plotting and crunching at the end.
Data displays
We start with the same sort of data plot as we used for the poison/antidote 
experiment. Figure 5.7 plots CO versus x1, with separate panels for the three 
x2 values. We can see that increasing the amount of ethanol (x1) in the mixed 
fuel has quite different effects at the three levels of x2. At x2 = 14, the x1 effect 
is positive; at x2 = 16, the effect is negative. At the middle value, x2 = 15, the 
relationship is not monotonic. In other words, the eyeball impression from 
these data is that there is quite substantial interaction between x1 and x2.
An alternative way to plot the data is to plot CO versus x2, with separate 
panels for the three levels of x1 (Fig. 5.8). Again, a different plot of the same 
data shows that the interaction is quite apparent—the relationship of CO to x2 
is quite different for the three levels of x1. Both plots show that CO emissions 
are minimized at high ethanol content (x1 = .3) and high air–fuel ratio (x2 = 16). 
Both plots also show that the variability between the two replications at each 
of the nine treatment combinations is reasonably homogeneous across the 
nine treatment combinations.
Given that the variability within the 18 pairs of runs is quite consistent, we 
can also do an interaction plot of the data means at the nine (x1, x2) treatment 
combinations (Fig. 5.9).
.3
.2
.1
90
80
70
60
.3
.2
.1
90
80
70
60
14
x1
CO
15
16
Panel variable: x2
Plot of CO vs. x1, by values of x2 
Figure 5.7  Plots of CO versus x1, by Values of x2.

143
Completely Randomized Design with Multiple Treatment Factors
It is clear that there is interaction—the effect of each of the factors differs 
greatly across the levels of the other factor. But the nature of the synergistic, 
combined effects of x1 and x2 are not easy to see because these plots treat 
the panel variable (the variable labeling each of the three panels) as a qualitative 
16
15
14
90
80
70
60
.3
.2
.1
90
80
70
60
x1
x2
.1
2
3
x1
14
15
16
x2
Interaction plot for CO
Data means
Figure 5.9  Interaction Plots for Ethanol Experiment.
16
15
14
90
80
70
60
16
15
14
90
80
70
60
.1
x2
CO
.2
.3
Panel variable: x1
Plot of CO vs. x2, by values of x1
Figure 5.8  Plots of CO versus x2, by Values of x1.

Fundamentals of Statistical Experimental Design and Analysis
144
factor, which it isn’t. There are better ways to show the data that make use of 
the fact that both x1 and x2 are continuous quantitative variables.
Figure 5.10 shows the nine design (x1, x2) points, labeled by the average 
CO emissions at those points. As you look at this plot, you can see a ridge: 
maximum CO occurs at the lower right‐hand corner and declines as you move 
diagonally to the upper left‐hand corner. CO falls off on either side of this 
ridge. The lowest CO is at the upper right‐hand corner (which is the combination 
of a high air–fuel ratio and high ethanol content, which makes sense: the more 
air and ethanol in the mix, the less CO will be emitted). The next lowest CO is 
diagonally opposite (which is more difficult to intuit). The fact that these are 
quantitative factors makes it meaningful to interpolate between the design 
points, to visualize peaks, valleys, and ridges.
Rather than just imagine contours sketched through the points in Figure 5.10, 
we can have software create a contour plot (Fig. 5.11). The ridge is quite apparent 
in this picture of the data. This display provides us a much clearer picture of the 
relationship of CO to x1 and x2 than does the interaction plot (Fig. 5.9.)
Message: Do not do interaction plots for two (or more) quantitative 
factors.
Discussion
Our goal is to find x1 and x2 settings that would minimize CO emissions. 
The data thus far suggest either the lower left corner or the upper right 
corner of the (x1, x2) experimental region. The shape of the contours, 
.30
.25
.20
.15
.10
16.0
15.5
15.0
14.5
14.0
E-conc
AF-ratio
59.0
67.5
67.3
76.3
80.8
69.7
91.8
79.5
63.8
Treatment combinations: Labels are average CO
Figure 5.10  Average CO Emissions by x1, x2 Combinations.

145
Completely Randomized Design with Multiple Treatment Factors
though, suggests that if we move beyond these two corners, we might 
reduce CO even further. Whether or not this is feasible requires subject‐
matter knowledge. An automobile engine might not even run for (x1, x2) 
­combinations outside of the experimental region in this experiment. An 
engine that doesn’t run is one way to minimize CO emissions, but is not 
what you (or Charlie Clark, Chapter 1) want in an automobile. If feasible, 
though, we would want to run further tests in the regions indicated by the 
contour plot, rather than settle on the best (x1, x2) combination or combi-
nations found in this experiment.
Before making final recommendations, though, we would need to consider 
other characteristics of engine performance, such as power and engine tem-
perature. We don’t want combinations of fuels and carburetor settings that 
don’t provide adequate power or run so hot they damage the engine.
Regression analysis and ANOVA
In Chapter  4, which dealt with CRD experiments with a single quantitative 
factor, the analysis approach was curve fitting, also known as regression anal-
ysis: fitting a mathematical function, or model, to describe the relationship 
between a single factor, x, and the response, y. With two quantitative factors 
in the present example, we extend the regression analysis in Chapter 4 of y as 
a function of single x‐variable to a multiple regression analysis of y as a function 
of both x1 and x2.
x1
x2
.30
.25
.20
.15
.10
16.0
15.5
15.0
14.5
14.0
>
< 60
60 – 65
65 – 70
70 – 75
75 – 80
80 – 85
85 – 90
90
CO
Contour plot of CO vs. x2, x1
Figure 5.11  Contour Plot of CO versus x1 and x2.

Fundamentals of Statistical Experimental Design and Analysis
146
The simplest mathematical model would be a linear model: CO = b0 + b1x1 + b2x2. 
A contour plot of this function would have parallel straight lines. Such a model 
would not describe the ridge seen in our contour plot (Fig. 5.9) of these data.
A model that can have ridge‐like contours is the second‐order function:
y
b
b x
b x
b x
b x
b x x
0
1
2
3
2
4
2
5
1
2
1
2
1 2.
Table 5.5 gives the regression analysis results for fitting this model. Let’s 
­discuss what all this is telling us.
The first line of Table  5.5 is the least‐squares fitted equation. With this 
equation, we can predict what the CO emissions would be for any (x1, x2) 
combination in the experimental region. The residual standard deviation for 
this fit is s = 2.53 μg/m3. This represents experimental error, which is the vari-
ability of measured emissions between the two replicates run at each (x1, x2) 
combination, and possible lack of fit of the second‐order model to the data. 
The ANOVA in Table 5.6 evaluates lack of fit.
The table of predictors in Table  5.5 gives each coefficient and its SE. 
(Conceptually, an SE of a coefficient estimate is analogous to the SE of a 
treatment mean or of a difference between treatment means, which we have 
considered in the previous chapters. The experimental design affects these 
SEs through the spread of the (x1, x2) points in the design and the number of 
Table 5.5  Regression Analysis of CO Emissions Data.
The regression equation is
CO = − 1046 + 1586 x1 + 135 x2 − 457 x1‐sq − 4.13 x2‐sq − 90.6 x1x2
S = 2.53; DF = 12
Predictor
Coef
SE coef
T
P (2‐tail)
Constant
−1046
284.8
−3.67
.003
x1
1586
143.3
11.07
.000
x2
135
37.9
3.56
.004
x1‐sq
−457
126.3
−3.62
.003
x2‐sq
−4.13
1.26
−3.27
.007
x1x2
−90.6
8.93
−10.15
.000
Table 5.6  ANOVA for CO Quadratic Model.
Source
DF
SS
MS
F
P
Regression
5
1604.7
320.9
50.3
.000
Residual error
12
76.5
6.38
Lack of fit
3
31.7
10.58
2.13
.17
Pure error
9
44.8
4.98
Total
17
1681.2

147
Completely Randomized Design with Multiple Treatment Factors
replications at the design points.) Each coefficient is then evaluated for its 
­significance relative to a coefficient of zero. We’re asking: Is the difference 
­between a coefficient and zero (in which case the coefficient estimate could 
just be due to the inherent variability of the data) “real or random?” Here, the 
large t‐values, in absolute value, and the corresponding small P‐values tell us 
that all five terms in the model (not counting the intercept) are making real 
contributions to the fit.
But the significance of all terms does not necessarily mean that we have a 
good fitting model. We can evaluate goodness of fit graphically and by an 
ANOVA.
Figure  5.12 shows the observed values of CO emissions plotted versus 
the “fitted values.” Also shown is the 45° line as a reference. For a perfect fit, 
the observed values would fall on this line. A poorly fitting model would have 
systematic departures from a straight line. Visually, because in most cases the 
fitted value falls between replicate observations at the experimental points, 
Figure 5.12 does not indicate a poorly fitting model.
We can substantiate the visual impression of a satisfactory fit by an ANOVA 
of the data. That ANOVA separates the residual variability between “pure 
error” (the variability among the pairs of replications) and lack of fit (the addi-
tional error because the second‐degree function doesn’t adequately describe 
the underlying relationship between CO and x1 and x2). Table 5.6 gives this 
ANOVA.
The F‐test statistic comparing lack of fit to pure error in Table 5.4 is equal to 
2.13, based on three numerator and nine denominator df. The resulting 
90
85
80
75
70
65
60
95
90
85
80
75
70
65
60
Fitted values
CO
Scatter plot of CO vs. fitted values
Figure 5.12  Scatter Plot of Observed CO Values versus Fitted Values.

Fundamentals of Statistical Experimental Design and Analysis
148
P‐value of .17 indicates that there is not strong evidence of lack of fit. The 
ANOVA confirms the visual impression in Figure  5.10. Also, the highly 
significant F‐statistic (F = 50.3, P ~ 10−7) for the overall model assures us, no sur-
prise, that our fitted model is much better than nothing.
Discussion
This analysis had a very nice (crisp, clear, readily communicated) outcome—
due in large part to the experimental design, especially the balance and sym-
metry of the 3 × 3 factorial structure of the treatments and the replication in 
the experiment. The result is a simple, well‐fitting model, validatable in the 
sense that we could do a goodness of fit statistical test of the model. The only 
downside is that we learned that low CO was only achieved in two corners of 
the experimental region.
In contrast, the situation in an ethanol research lab might be that over some 
period of time, various researchers might have done various tests at some col-
lection of (x1, x2) points, but not in a coordinated way. Suppose you have 18 
such runs. They are not likely to have covered the (x1, x2) space in a way that 
provides such a clear picture of the combined effects of these factors. 
Experimental protocol would likely not have been consistent over these runs, 
either. There may have been other process variables and protocols that were 
not controlled in the historical set of runs, but were in the designed experiment. 
(BHH 2005 and other authors discuss the pitfalls of drawing conclusions from 
haphazardly collected data.)
Anecdote: Bill Hunter once recounted that when he worked for a chemical 
company, he did an analysis of some collected process data and came to the 
conclusion that water content had little effect on the process. The process engi-
neers guffawed: “If you don’t run the process at precisely the right water content, 
you get an explosion! Therefore, we control water content very tightly.” The 
collected data thus had water content controlled very tightly and too tightly for 
the process output to vary, explosively, over that range. Data mining of the hap-
hazardly collected data might lead to a second‐degree model, maybe even very 
similar to the one yielded by the experimental data, but all of these concerns 
about the way the data were assembled would limit our confidence in the model. 
We might have to run a designed, controlled experiment to validate it.
Bottom Line: Designed experiments provide a stronger basis for learning 
and actions than do data obtained without the discipline that an experimental 
design imposes.
This model also gives us a much better understanding of the effects of eth-
anol concentration and air–fuel ratio on carbon monoxide emissions than we 
would have gotten if we had analyzed this experiment ignoring the quantitative 
nature of these two factors in the experiment.
At this point, it is appropriate to share one of Professor Box’s most widely 
quoted remarks: “All models are wrong; some models are useful.” (BHH 2005, 

149
Completely Randomized Design with Multiple Treatment Factors
p. 440). This means that all mathematical models are approximations to real‐
world relationships. Even science‐based models make simplifying assumptions 
about nature, such as uniform material properties, so they are approximations 
(wrong but often still useful). In the ethanol case, we are not assuming that the 
“true” relationship between CO and x1 and x2 is a second‐order function. It 
just does a good job of usefully approximating what may be a very complex, 
higher‐dimensional relationship. There are other variables, say, x3, x4, …, that 
may have an effect, but not enough to preclude using the selected and fitted 
model to draw some conclusions about how to minimize CO emissions and to 
point the way to further experimentation. For more on this topic, see Wikiquote 
(2014) and its references.
Response Surface Designs
Because of the utility of quadratic mathematical functions relating quantitative 
factors to experimental responses, statisticians have developed experimental 
designs that facilitate fitting these models. These designs are called “response 
surface designs.” For two x‐variables, these designs take the form of 2 × 2 fac-
torial plus a “star.” The (x1, x2) coordinates at which experiments are run are 
given in Table 5.7 and shown graphically in Figure 5.13.
The “star” portion of the design consists of the center point and two axial points 
on each axis. Defining the axial points by a
2 has some desirable model‐
fitting properties, but context constraints are also important considerations. 
The preceding ethanol experiment, which had a 3 × 3 set of factorial treatment 
combinations, was the special case of a = 1.
When fitting mathematical functions to data, it is important to consider trans-
formations of the x‐variables or the response variable. Theoretical consider-
ations or empirical evidence can suggest using logarithms, reciprocals, geometric 
functions, etc. rather than the values directly measured. If transformations are 
Table  5.7  Response Surface 
Design for Two x‐Variables.
x1
x2
−1
−1
−1
1
1
−1
1
1
0
0
−a
0
a
0
0
−a
0
a

Fundamentals of Statistical Experimental Design and Analysis
150
known ahead of time, the experiment should be designed in terms of the 
transformed x‐variables.
Depending on context and cost considerations, the whole response surface 
design can be replicated r times. When experimental units and tests are expen-
sive, it is generally recommended that only the center point be replicated. 
Replication provides an estimate of “pure error” variation and is necessary for 
evaluating lack of fit of a candidate model. Note also that having treatment 
combinations that include five levels of each x‐variable also provides a way 
to evaluate the fit of second‐degree polynomial models. For much more 
information about response surface designs and examples, see, for example, 
Myers, Montgomery, and Anderson‐Cook (2009).
Extensions: More than two treatment factors
Two‐factor treatments are the easiest to illustrate and the place to start in 
thinking about multifactor experiments, interactions among factors, and main 
effects of factors. Many issues and processes of interest in science, industry, 
and government, though, involve more than two factors, maybe many more 
than two. In most of the examples in this book, the discussion of potential 
follow‐on experiments pointed to the investigation of the effects of additional 
factors. Let’s consider the two examples of this chapter.
1.5
1.0
.5
.0
–.5
–1.0
–1.5
1.5
1.0
.5
.0
–.5
–1.0
–1.5
x1
x2
Response surface design: Two x-variables
Figure  5.13  Response Surface Design for Two x‐Variables in Coded Units. 2 × 2 
­factorial points connected by black lines; “star” points connected by blue lines; axial 
points at ±1.4.

151
Completely Randomized Design with Multiple Treatment Factors
Example 3: Poison/antidote experiment extended
In the poison/antidote experiment, the antidotes were administered at a 
­specified time after the laboratory animal was exposed to a poison. If these 
antidotes were to be used in a battlefield situation, conditions could preclude 
prompt administration of an antidote. Thus, one additional factor that might 
be addressed in subsequent experimentation is delay time. Let’s suppose the 
government agency sponsoring the testing of antidotes has decided to focus 
follow‐up experiments on antidotes B and D and test these antidotes against 
the same three poisons used in the first experiment. The agency reps thus 
decide to consider the quantitative variable, delay time, as a factor (variable) 
in their follow‐up experiment.
Another factor held fixed in the first experiment was dose level. Presumably, 
for that experiment, the four antidote manufacturers specified particular 
dose levels that their analyses indicated would be optimal in some sense. 
However, from the agency’s point of view, there are issues of both cost and 
effectiveness that could depend on dose. Thus, the government reps ask the 
manufacturers to specify a low, nominal, and high dose level. The nominal 
level would be the level tested in the first experiment. Low and high would 
be spaced widely enough to have a good chance of detecting a dose effect, 
if one exists. The dose levels would be quantitative variables, in units such as 
milligrams, but because the antidotes are different chemicals, their dosage 
levels are not in the same dimension. Thus, dosage is a nested factor: it is 
nested within antidotes, not crossed with them. Though we may label them 
low, nominal, and high, they are not necessarily the same dosages for the 
two antidotes.
After discussions among the sponsoring agency and the antidote manufac-
turers, the experiment is designed. The experiment will have the following 
four factors and factor levels:
1.  Poisons (3 levels): I, II, and III
2.  Antidotes (2 levels): B and D
3.  Delay times (4 levels): t1, t2, t3, and t4
4.  Doses (3 levels for each ant.): B, dB1, dB2, and dB3, and D, dD1, dD2, and dD3
The treatment combinations will be all 3 × 2 × 4 × 3 = 72 combinations of 
these four treatment factors. To bring more precision to the experiment and 
provide more sensitivity in comparing Antidotes B and D, there will be five lab-
oratory animals (experimental units) for each treatment. (The first experiment 
provided an estimate of the experimental error standard deviation, σ, and a 
sample size analysis could have been done to determine the number of repli-
cations.) Thus, there will be a total of 360 experimental units in the experiment, 
each treatment combination being randomly assigned to five experimental units. 
Note that this experiment has 180 eus for each antidote. The original 
experiment had 12 eus for each antidote.

Fundamentals of Statistical Experimental Design and Analysis
152
The response measured will be lifetime, after an animal has been exposed 
to a poison and been treated with an antidote, at a specified dose, after a 
specified delay time. The variable analyzed will be death rate, the reciprocal of 
lifetime. What will the analysis look like?
Analysis 1: Plot the data
The appropriate starting point would be scatter plots of individual death rates 
versus dose level for each of the 24 poison/antidote/delay time combinations. 
The main value of these plots is to see whether the variability within each 
treatment combination is reasonably consistent across the experiment. One 
could also see whether increasing the dose decreases the death rate (increases 
survival time) consistently, or more so for one antidote than the other, or differ-
ently for different delay times, etc. Additionally, one could plot death rates 
versus delay times for each of the 18 poison/antidote/dose combinations and 
look for consistencies and differences in the delay time effect.
Subsequent displays would be various presentations of the 72 treatment 
means. Because two of the factors are quantitative, contour plots of average 
death rate versus t and d (dropping the subscripts for ease of presentation), 
as in Figure 5.11, for each of the six poison/antidote combinations, would 
be appropriate. This would provide a comparison of the two antidotes’ 
effectiveness for each poison, as a function of delay time and dose. One 
could follow these plots with contour plots in time and dose for each poison 
and antidote, being careful to define the color bands to be the same for all 
six contour plots.
Analysis 2: ANOVA
Because the dose levels are different for the two antidotes, I would do sep-
arate ANOVAs for the two antidotes, as shown in Table 5.8. Each of these 
ANOVAs has entries for poison, delay, and dose; their two‐way interac-
tions; and their three‐way interactions. Statistical software such as Minitab 
can do these ANOVAs. Interested readers can find the formulas for the 
sums of squares for each line in the ANOVA in references such as 
Montgomery (2012). Minitab and other software can take input that spec-
ifies the sources of ­variation and generate the proper ANOVA table. Or see 
your FLS. For our purposes, let’s focus on the structure of this table and 
how to interpret it.
The degrees of freedom follow the pattern we’ve seen before. The df associ-
ated with each factor’s main effect is one less than the number of levels. For 
example, three dosage levels mean 2 df for dose; four delay times mean 3 df 
for delay time. For each two‐way interaction, the associated df is the product of 
the two factors’ dfs—for example, the poison × dose interaction has 2 × 2 = 4 df. 
Then, for the big one, the three‐way P × T × D interaction, it has 2 × 3 × 2 = 12 df.
Note that if you add the df for all of these treatment‐related sources of 
­variation, the sum is 2 + 3 + 2 + 6 + 4 + 6 + 12 = 35 df. There were 36 treatment 

153
Completely Randomized Design with Multiple Treatment Factors
combinations within each antidote. If we had just done an ANOVA for a one‐
way classification with 36 treatments, there would be 35 df for treatments in that 
ANOVA. What we have been able to do because of the three‐factor ­factorial 
treatment structure is to separate the various interactions and main effects. 
We don’t just have 36 assorted treatments. We’ve got a 3 × 4 × 3 factorial 
arrangement of three treatment factors. Our ANOVA can and should 
reflect and provide for an evaluation of the variation associated with that 
structure.
The error lines in the ANOVA each have 144 df. Each of the 36 treat-
ments was applied to five experimental units. Thus, there are 4 df in each 
of these 36 groups of eus. Thus, when the corresponding sums of squares 
are pooled, the pooled error has 36 × 4 = 144 df. This pooling is done under 
the assumption that the underlying variances among eus in each treatment 
group are equal. Data plots and formal tests of significance can evaluate 
this assumption.
F‐ratios for each effect and interaction in the table are obtained by dividing 
each MS by the error MS. To interpret the ANOVA, we start at the bottom. 
The F‐ratio for the P × T × D interaction is asking: Does the effect of any one 
factor depend on the levels of both of the other factors? A large F‐ratio and 
corresponding small P‐value say there is no consistency of factor effects. We 
can’t generalize at all. To be specific for this experiment, if we had a significant 
Table 5.8  ANOVA for Poison/Antidote/Delay/Dosage Experiment.
Source of Variation
df
Antidote B
Poison (P)
2
Delay time (T)
3
Dose (D)
2
P × T
6
P × D
4
T × D
6
P × T × D
12
Error (B)
144
Total (B)
179
Antidote D
Poison (P)
2
Delay time (T)
3
Dose (D)
2
P × T
6
P × D
4
T × D
6
P × T × D
12
Error (D)
144
Total (D)
179

Fundamentals of Statistical Experimental Design and Analysis
154
three‐way interaction and plotted average death rate versus dose level, for 
all 12 poison/delay time combinations for a given antidote, those plots would 
not be near‐parallel lines. It’s still possible, though, to have a winning anti-
dote, delay time, and dose level. The lines might not be parallel, but the 
lowest death rate might be, say, Antidote B, minimum delay time, high dose 
for all three poisons. All is not lost when you have a significant three‐way 
interaction.
The two‐way interactions correspond to two‐way tables of mean responses 
for a pair of factors, averaged over the third factor. A significant three‐way 
interaction tells you that this collapsing across one factor may not be appro-
priate, for the purpose of evaluating the effects of any pair of factors. The 
pattern of two‐way and three‐way interactions reflects the lack of consistency 
seen in data plots. The ANOVA is quantifying patterns seen in corresponding 
data plots, and it is gauging these patterns against the inherent variability 
among eus in order to determine real or random.
Analyzing the data for each antidote separately enables us to estimate and 
compare average death rates but also the variability of death rates. As noted 
earlier, the best antidote would have low average death rates and low vari-
ability about those averages.
Analysis 3: Further analyses
Estimates of average death rates for factor‐level combinations of interest, 
confidence and prediction intervals, survival curves, and other analyses 
would be issue and context driven in this follow‐on experiment as they were 
previously for the initial poison/antidote experiment. The sponsoring agency 
would be looking for antidotes and dosage levels that lead to the best 
survival times across all poisons. They would be looking at how sensitive 
antidote effectiveness is as a function of delay time. If different combina-
tions led to comparable survival times, other antidote characteristics such as 
cost and side effects could be “factored in” (pardon the expression) to the 
decision process.
Example 4: Ethanol experiment extended
The two quantitative factors in the ethanol experiment were the ethanol 
concentration (x1) and the air–fuel ratio (x2). A third factor of interest (I sug-
gest without much knowledge of the variables affecting CO emissions from 
engines that burn ethanol fuel) might be the octane level (x3) of the gasoline 
with which the ethanol is mixed. We learned in the first experiment that x1 and 
x2 both near the upper ends of the ranges considered led to reduced CO. 
Let’s suppose that in a follow‐up experiment, we decide to consider three 
levels of both x1 and x2, perhaps centered on the upper right corner of the 
original experiment, without wandering into the region where zero CO is emit-
ted by an engine that won’t run. Suppose the investigators’ decision is to 

155
Completely Randomized Design with Multiple Treatment Factors
incorporate octane, x3, at three levels corresponding to conventional gasoline 
into the treatment structure. Thus, there will be 3 × 3 × 3 = 27 treatments. 
Suppose also, as before, that the experiment will have two replications at each 
treatment combination for a total of 54 runs.
Analysis 1: Plot the data
Because it is difficult to display four dimensions on two‐dimensional paper, a 
good starting point for plotting the data from this extended experiment is to 
produce contour plots of emissions versus x1 and x2, for each level of x3 
(octane). The plots would need to have the same scales and colors in order to 
facilitate comparisons across octane levels.
Analysis 2: Model fitting
A full second‐degree model for three x‐variables has a constant, three linear 
terms, three squared terms, and three two‐variable products, for a total of 
10 parameters in the model. The purpose of curve fitting and other analyses 
would be to find and characterize the relationship of CO emissions to these 
three process variables.
As will be discussed in Chapter 7, Optimal Experimental Design methods, 
also known as computer‐aided designs, can be especially useful in finding 
designs that are appropriate for complex situations.
Special Case: Two‐Level Factorial Experiments
When your eu is a laboratory rat and your measurement system is a grad-
uate student with a stopwatch, you can afford to run large experiments with 
many factors at moderate to many levels and many replications. If your eu 
is a missile, a school system, or an expensive pharmaceutical process, you 
cannot. These systems have a large number of factors that may affect their 
performance, but it is not economically possible to run experiments that 
explore their effects and interactions extensively. Are designed experi-
ments out of the picture? (I have encountered consulting clients who said, 
“I can’t afford to run a designed experiment. I can do only a small number 
of tests.”) Au contraire! When data are expensive and when you can’t over-
whelm an issue with data, that’s when you most need efficient experimental 
design in order to maximize the amount of usable information obtained for 
the money spent.
The place to start whittling the problem down to a manageable size is to use 
subject‐matter knowledge, theory, and experience to reduce the number of 
factors to include in the experiment. This effort is apt to require subject‐matter 
expertise in more than one area.
Of course, one reason to experiment is to find out if hitherto unevaluated or 
unappreciated factors may have unexpected effects, so you don’t want to 

Fundamentals of Statistical Experimental Design and Analysis
156
reduce the experiment to confirming what is already known. Former U.S. 
Secretary of Defense, Donald Rumsfeld, expressed the issue poetically:
The Unknown
As we know,
There are known knowns.
There are things we know we know.
We also know
There are known unknowns.
That is to say
We know there are some things
We do not know.
But there are also unknown unknowns,
The ones we don’t know we don’t know.
February 12, 2002, Department of  
Defense news briefing (source: Hart  
Seely, SLATE online, April 2, 2003)
Experiments are for finding unknown effects of known factors, but as with 
the tomato fertilizer experiment, we also can find effects of factors we did not 
know or suspect—e.g., the fertility trend in the experimenter’s garden—if we 
are alert and curious and have the data.
Once a set of factors has been identified, the next way to reduce the size of the 
experiment is to limit the number of levels of each of the factors. For this reason, 
a large amount of statistical experimental design research has been devoted to 
the case of factors with only two or three levels. Sometimes, these experiments 
are called “industrial experiments,” but they have applicability in any multifactor, 
high‐cost situation. They are also termed “screening experiments” because they 
may be conducted to identify the factors with the largest effects; subsequent 
experiments would then explore these factors more extensively. Many texts such 
as Box, Hunter, and Hunter (2005), Ledolter and Swersey (2007), and Montgomery 
(2012) deal extensively with treatment design for two‐level factors. Here, I will 
just present a description of the main ideas of experiments with two‐level factors 
and illustrate these ideas with two examples.
Example 5: Pot production
This example (with some enhancement on my 
part) is from Ledolter and Swersey (2007). It 
seems that a company that manufactures clay 
flower pots (you were expecting a different pot 
story?) was having problems with high rates of 
cracked pots. On the advice of their FLS, to find 

157
Completely Randomized Design with Multiple Treatment Factors
the cause of the problem and to identify possible fixes to the production 
­process, they ran a series of experiments including one that included the fol-
lowing four treatment factors, each at the indicated two levels:
Cooling rate (R): slow and fast
Temperature (T): 2000 and 2060°F
Clay coefficient of expansion (C): low and high
Conveyor belt carrier (D): metal and rubberized
The low/high and slow/fast levels for factors R and C were appropriately 
defined and specified in appropriate units.
One can envision a conveyor belt that takes soft‐clay molded pots into an 
oven where they are baked and then through a cooling chamber to complete 
the process. The pots in the experiment will be made of one of two clays, car-
ried along on one of two types of carriers, baked at one of two temperatures, 
and cooled at one of two rates.
The eu was a batch of 100 pots processed in one run of the baking and 
cooling process. The response measured was the percent of cracked pots in  
a batch.
With four two‐level factors, there are a total of 16 treatment combinations, 
and in this experiment, a total of 1600 pots produced for a single replicate of 
each treatment combination. The production manager (who knew statisticians 
like to replicate) was concerned about having to run several replications of 
each of these treatments; the lost production time and (potentially) many 
cracked pots would be costly. The FLS, though, said that it might be possible 
to run only one replication of each treatment combination, so only 16 batches 
would need to be produced and processed, but said, “First, let’s talk about 
interactions.”
“Interaction means that the effect of one process variable on pot cracking 
depends on one or more of the other three process variables. For example, 
would you expect the difference in crack% between the clays with low and 
high expansion coefficients to be similar for slow and fast cooling, or is the 
difference apt to depend on the cooling rate? If the latter, that’s an example of 
interaction. Would you also expect the difference between the two clays to 
depend on the baking temperature? Would the effects of any of these vari-
ables be expected to depend on which carrier the pots rode on? If we can 
assume that the four possible three‐factor interactions and the four‐factor 
interaction are negligible, relative to the inherent variability of the process, 
then we can just run one replication and have a few degrees of freedom left 
over for estimating that inherent variability and evaluating the fitted model 
relative to that inherent “error” variability.”
The process and ceramic engineers ponder these questions and decide that 
the only potential interaction they anticipate is between R and C. For pots that 
are slow cooled, the coefficient of expansion should not have as much of an 

Fundamentals of Statistical Experimental Design and Analysis
158
effect as when they are fast cooled. Other than that, no other interactions, 
particularly three way and four way, seem plausible, they say.
So the experiment is conducted with one run of each of the 16 treatment 
combinations. The order is randomized and independent batches of clay pots 
are produced for each run. The design matrix and response data are given in 
Table 5.9. The two levels of each treatment factor are indicated by −1 and 1. 
For quantitative factors (R, T, and C in this experiment), these coded levels 
denote the low and high levels; for qualitative factors (D in this experiment), 
the coded levels denote the two different categories. Here, D = −1 denotes 
the metal carrier, and D = 1 denotes the rubberized carrier.
Analysis 1: Look at the data
Note the layout of Table 5.9. The 16 treatment combinations of the four two‐
level factors are organized in what is often called the “standard order.” In 
column R, the levels alternate −1, 1, eight times. Then column T has a pair of 
−1s followed by a pair of 1s, repeated four times. Next, in column C, the levels 
of C are in groups of four, and then in Column D, the first eight treatment com-
binations all at the −1 level, followed by the next eight all at the 1 level. Thus, 
all 16 combinations are created.
Analysis 1 is usually plot the data. With five‐dimensional data, though, that 
is hard to do, particularly without some analysis to pick out the important 
Table 5.9  Pot Production Experimental Results.
StdOrder
R
T
C
D
%Cracked
  1
−1
−1
−1
−1
14
  2
1
−1
−1
−1
16
  3
−1
1
−1
−1
8
  4
1
1
−1
−1
22
  5
−1
−1
1
−1
19
  6
1
−1
1
−1
37
  7
−1
1
1
−1
20
  8
1
1
1
−1
38
  9
−1
−1
−1
1
1
10
1
−1
−1
1
8
11
−1
1
−1
1
4
12
1
1
−1
1
10
13
−1
−1
1
1
12
14
1
−1
1
1
30
15
−1
1
1
1
13
16
1
1
1
1
30
Source: Reproduced with the permission of Stanford University Press, repro-
duced Table 4.7, p. 80, Testing 1‐2‐3, by Ledolter and Swersey (2007).

159
Completely Randomized Design with Multiple Treatment Factors
dimensions and patterns to examine. The organization of Table 5.8, though, 
makes it easy to look at the data—in a table in this case, rather than a plot. First, 
note that the top eight runs all have higher %crackeds than the bottom eight. 
These runs differ by factor D, the carrier. The R, T, and C combinations are the 
same in the top half of the table as in the lower half. We can see that the rub-
berized carrier appreciably decreased the percentage of cracked pots.
Next, in column C, we can compare successive groups of four runs. They 
differ only in the level of C. We can see that in the top half of the table that 
C = 1, which corresponds to the higher expansion coefficient, results in higher 
%crackeds than C = −1, and the same sort of difference holds in the bottom 
half of the table.
The T (temperature) effect can be seen by comparing successive pairs of two 
runs. Runs 1 and 2, at T = −1, averaged 15% cracked pots, and runs 3 and 4, 
which differ from runs 1 and 2 only in that they were done at T = 1, also aver-
aged 15%. Thus, no evidence of a temperature effect there. Look at the next 
two pairs of two runs. Again, not much difference between the two T levels. 
Similar patterns pertain on down the table. It looks like the difference between 
temperatures of 2000 and 2060°F does not have much of an effect on pot 
cracking.
To see the effect of R (cooling rate), we compare successive pairs of runs: 
one is at R = −1, and the next is at R = 1. Generally, the R = 1 runs result in 
higher cracked pot percentages than the runs at R = −1, but the differences 
vary somewhat. This is an evidence that fast cooling causes cracked pots, but 
the extent of this effect may depend on one or more of the other factors. That 
is, R may interact with other factors.
Thus, just by looking at the table of data, properly organized, we can see 
patterns and arrive at recommended process variables: use the low expansion 
coefficient clay, cool at the lower rate, give the pots a rubberized carrier to ride 
in, and keep the temperature between 2000 and 2060°F. This sort of transpar-
ency—the data almost analyze themselves!—is one of the great advantages of 
two‐level factorial treatment structures. Of course, we can’t quit here. We 
need to do some number crunching and data plotting to confirm and quantify 
what we see in the data table. After all, this is a statistics book, so you deserve 
the full treatment.
Analysis 2: Regression analysis
As noted, the two levels of each factor are coded −1 and 1. A unique property 
of two‐level treatment designs is that the analysis can treat both quantitative 
and qualitative variables as quantitative variables. A regression fit in these var-
iables will estimate the average difference in responses between the low and 
high levels of each variable. Cross‐product terms will correspond to two‐ and 
more‐factor interactions. Of course, with only two levels of each factor, 
quadratic or higher‐order effects cannot be evaluated.

Fundamentals of Statistical Experimental Design and Analysis
160
The assumption of no three‐factor or four‐factor interactions means that our 
candidate regression model for fitting these data will contain the four main 
effects and six two‐factor interactions. Table 5.10 shows these 10 variables; the 
indicated products are obtained by multiplying the factor levels of the two var-
iables in each product.
The coefficients in the fitted model are given in Table 5.11.
Table 5.11 shows that only R (cooling rate), C (clay), and their interaction, RC, 
along with D have significant effects (small P‐values, all <.01), relative to the 
Table 5.10  Main Effect and Two‐Factor Interaction Variables in Regression Analysis.
R
T
C
D
RT
RC
RD
TC
TD
CD
%Cracked
−1
−1
−1
−1
1
1
1
1
1
1
14
1
−1
−1
−1
−1
−1
−1
1
1
1
16
−1
1
−1
−1
−1
1
1
−1
−1
1
8
1
1
−1
−1
1
−1
−1
−1
−1
1
22
−1
−1
1
−1
1
−1
1
−1
1
−1
19
1
−1
1
−1
−1
1
−1
−1
1
−1
37
−1
1
1
−1
−1
−1
1
1
−1
−1
20
1
1
1
−1
1
1
−1
1
−1
−1
38
−1
−1
−1
1
1
1
−1
1
−1
−1
1
1
−1
−1
1
−1
−1
1
1
−1
−1
8
−1
1
−1
1
−1
1
−1
−1
1
−1
4
1
1
−1
1
1
−1
1
−1
1
−1
10
−1
−1
1
1
1
−1
−1
−1
−1
1
12
1
−1
1
1
−1
1
1
−1
−1
1
30
−1
1
1
1
−1
−1
−1
1
1
1
13
1
1
1
1
1
1
1
1
1
1
30
Table 5.11  Fitted Coefficients for a Full Four-Factor Model.
Predictor
Coef
SE Coef
T
P
Constant
17.625
.64
27.6
.000
R
6.250
.64
9.8
.000
T
.500
.64
.78
.47
C
7.250
.64
11.3
.000
D
−4.125
.64
−6.5
.001
RT
.625
.64
.98
.37
RC
2.625
.64
4.1
.009
RD
−.250
.64
−.39
.71
TC
−.125
.64
−.20
.85
TD
.250
.64
.39
.71
CD
.500
.64
.78
.47

161
Completely Randomized Design with Multiple Treatment Factors
residual error, which is estimated from the three‐ and four‐factor interactions. 
Factor T (temperature) has no significant effect by itself or in interactions 
involving the other factors. Our initial eyeball analysis of the data table is thus 
substantiated by this regression analysis. Therefore, we can ignore T (which 
seemed to my unscientific eye to have been varied by a very small amount, 
from 2000 to only 2060°F, in the experiment, which may be why no significant 
effect was found; maybe, the process engineers had already determined the 
process needed to run in this tight temperature interval in order to produce 
good pots). This means that the experiment reduces to a 23 set of treatments 
with two replications of each. The standard sort of analysis for this structure 
could be carried out. Because of the balanced, symmetric factorial structure of 
our 16 treatments, all of these single‐ and two‐variable estimated coefficients 
are mutually independent. We could further reduce the model to the four 
significant terms and collapse the other terms into the residual error.
The R by C interaction is illustrated in Figure 5.14. This plot indicates that 
the process yields fewer cracked pots using the clay with the lower coefficient 
of expansion and that the effect of the cooling rate (the higher rate leads to 
higher cracking rates) is less pronounced for this clay than for the clay with the 
higher expansion coefficient. These results make physical sense because cracks 
result from too‐rapid expansion and contraction as temperature is ramped up 
then down.
Additionally, the negative effect of D, the carrier, means that the rubberized 
carrier results in a lower rate of cracking than the metal plate, so that’s good 
1
–1
35
30
25
20
15
10
R
Mean
–1
1
C
Interaction plot for %cracked
Data means
Figure 5.14  Interaction Plot for Factors R and C.

Fundamentals of Statistical Experimental Design and Analysis
162
to know. The coefficient of −4.2 for D is half of the overall average difference 
between the mean %cracked for the two carriers. This means that the rubber-
ized carrier results in an average of about 8.4% fewer cracked pots than the 
metal carrier, consistently across the other process variables. Handle with care 
is good advice for manufacturing clay flower pots.
Analysis 2: Stepwise regression
This flower pot experiment was designed under the expectation that higher‐
order interactions would be negligible. That assumption justified running 
only one replication of the 16 treatments. It left us five degrees of freedom 
for estimating experimental error (a model with 11 terms was fitted to 16 
data points, thus leaving 5 df for residual variation). However, once the data 
are obtained, we don’t have to do the analysis under the design planning 
assumption. In fact, further simplifications of the regression model were sug-
gested in the previous subsection. Once we have the data, we can evaluate 
whether or not the data will support that assumption. We can use other ways 
to identify which effects and interactions are “significant” and which ones are 
in the noise.
“Stepwise regression” is one analysis method that can be used. In stepwise 
regression (see Wikipedia 2014 and references therein) one starts with a list of 
candidate independent variables and then uses various rules for selecting the 
most important explanatory variable, then the next most important variable, 
etc. and then stops when none of the remaining variables have enough of an 
effect to stand out above the residual variation. The analyst has considerable 
leeway in specifying selection and stopping rules, so “actual results may vary.” 
(Statistics means never having to say you’re certain.) For this experiment’s data, 
reasonable selection rules are apt to pick out the R, C, and D main effects and 
the RC interaction found in the previous analysis. The factorial treatment design 
avoids some of the difficulties that stepwise regression can encounter when 
there are correlations among all the candidate independent variables. With this 
24 treatment design, all of the candidate variables are orthogonal, meaning that 
their estimated coefficients are all independent of one another, thus making the 
separation of winner and loser independent variables more trustworthy.
Analysis 3: “Effect sparsity” and graphical analysis
A third analysis method for this experiment is built around a concept of “effect 
sparsity” (Box and Meyer 1986). In this experiment, which has a 24 treatment 
structure, we can actually fit the full model which would consist of four single‐
variable terms, six two‐variable products, four three‐variable products, and one 
four‐variable product: a total of 15 variables in the regression model which, with 
only one replication, means that there are no residual degrees of freedom for 

163
Completely Randomized Design with Multiple Treatment Factors
estimating the error variability. The notion of effect sparsity means that nature 
and good scientific and other systems are not apt to be so complex that all 15 
of these terms in the model will have strong effects on the response. They will 
sort themselves out into the vital few and the trivial many. We can use graphical 
methods to facilitate this sorting. So rather than make subject‐matter decisions 
guided by an ad hoc assumption, for example, that three‐ and four‐factor inter-
actions are negligible, we will give every effect and interaction a chance to stand 
up and be counted, comfortable in the expectation that only a few of them 
will—usually. Though the approach is different, the end points of this analysis 
and stepwise regression are similar: a simplified regression model that describes 
the data and provides a characterization of experimental error variation.
Table 5.11 (p. 160) gives the coefficients for the full 15‐term regression model. 
Let’s now look at these coefficients (other than the constant or intercept term 
in the model) graphically. One way to compare the coefficients is by a dot plot; 
Figure 5.15 shows that plot.
Here, we see a fairly clear pattern: there are 11 coefficients clustered around 
zero and four coefficients clearly separated from this cluster. These four are the 
coefficients we earlier found to be statistically significant (very small P‐values): 
R, C, D, and RC. It is fairly intuitive to call the cluster “noise” (random) and the 
separated points “signal” (real).
None of the four “significant” coefficients involve three or four factors, but in 
another experiment, they could have. Suppose there was a large three‐variable 
6.4
4.8
3.2
1.6
.0
–1.6
–3.2
Coef
Dot plot of coefficients: full four-variable model-pot exp't.
Figure 5.15  Dot Plot of Coefficients.

Fundamentals of Statistical Experimental Design and Analysis
164
interaction. If we had blindly fitted a model with only single variables and  
two‐variable products, the residual error would have been inflated by this three‐
variable interaction, and we might not have found truly significant effects, and 
we would not have discovered the significant three‐variable interaction. Thus, a 
safer analysis philosophy is to give every potential predictor a chance to be dis-
covered, rather than rule some of them out by assumption.
Another way to display the 15 coefficients in a way that helps to sort them 
out is to plot them in a Normal probability plot (Fig. 5.16).
Figure 5.16 shows a clear separation: the 11‐point middle cluster looks like 
a random sample from a Normal distribution centered near zero, in other 
words like random (experimental) error, but there are four outliers, relative to 
this error distribution. A goodness of fit test tells you that these data just 
couldn’t arise by random sampling from a single Normal distribution and we 
can see why. The four outlying coefficients are “real” effects. The rest are 
noise. If none of the predictor variables (including all the products) had an 
effect, that is, if the %cracked pots were unaffected by any of the four experi-
mental variables, singly or jointly, the estimated coefficients would look like 
noise—15 observations from a Normal distribution with a mean of zero.
The analysis can be carried further by using the error data to estimate 
“sigma” and then by testing the outlying coefficients against this sigma to get 
an approximate t‐statistic and P‐value. Statistical software such as Minitab can 
carry out this enhanced graphical analysis.
Much experimental design literature analyzes two‐level multifactor experi-
ments in terms of estimating main effects and interaction effects rather than in 
7.5
5.0
2.5
.0
–2.5
–5.0
99
95
90
80
70
60
50
40
30
20
10
5
1
Coef
Percent
Probability plot of Coef
Normal
Figure 5.16  Normal Probability Plot of Estimated Coefficients.

165
Completely Randomized Design with Multiple Treatment Factors
terms of regression analysis. In the present experiment, the estimated main 
effect of factor R is the difference between the average %cracked for the 
eight runs at which R = 1 and the average for the eight runs at which R = −1. 
So the effect of R is the average change in the response variable as R goes 
from −1 to 1. In a regression analysis, the coefficient of R is the slope, which 
is the change in the average response per unit change in x. The change in x 
is from −1 to 1 = 2 coded units. Thus, the regression coefficient is one‐half 
of the effect of R.
Now, consider interactions and cross‐product terms in a regression model. 
The interaction is given by the difference between the average response when 
the product of coded variables = 1 and the average response when the pro­
duct = −1. For example, see the RT column in Table  5.9. The estimated RT 
interaction effect is the difference between the average response for the eight 
runs at which RT = 1 and the average response for the eight runs at which 
RT = −1. The regression coefficient of RT is one‐half of that difference.
Let us now illustrate this correspondence between an “effects” analysis 
and the regression analysis carried out previously. Table 5.12, which is the 
Minitab output for the cracked pot experiment, gives both the effects and 
the regression coefficients. As discussed, the latter are one‐half times the 
former.
Minitab output also includes the Normal probability plot of the effects 
(rather than the coefficients as in Fig. 5.16). This plot, Figure 5.17, is the same 
as Figure 5.16 but with the scale changed by a factor of two.
Table 5.12  Effects and Regression Coefficients for Flower Pot Experiment.
Term
Effect
Coef
Constant
17.625
R
12.50
6.250
T
1.00
.500
C
14.50
7.250
D
−8.25
−4.125
R*T
1.25
.625
R*C
5.25
2.625
R*D
−.50
−.250
T*C
−.25
−.125
T*D
.50
.250
C*D
1.00
.500
R*T*C
−1.50
−.750
R*T*D
−1.75
−.875
R*C*D
.25
.125
T*C*D
−.75
−.375
R*T*C*D
1.50
.750

Fundamentals of Statistical Experimental Design and Analysis
166
Figure 5.17 includes Lenth’s pseudostandard error (PSE) (Lenth 1989). The 
blue line overlay in Figure 5.13 is the Normal distribution with mean zero and 
standard deviation equal to the PSE. The software does the work of separating 
signal and noise and calculating an SE against which to evaluate the outlying 
points. The color coding indicates the significance of the resulting pseudo‐t‐
tests. The significant effects and interactions (or coefficients) are labeled so 
you don’t have to look them up. The result is qualitatively the same as that 
obtained by reducing the model by stepwise regression or ad hoc model 
­simplification to just the four terms highlighted and consigning everything else 
to residual variability. All of these analyses confirm what our initial eyeball 
­analysis of the table of experimental results suggested. The flower pot manu-
facturer has learned how to run the process, within the scope of this experiment, 
to minimize the percentage of cracked pots, as follows: use clay with a low 
expansion coefficient, a slow cooling rate, and rubberized carriers. The experi-
mental results were 2.5% cracked pots for that condition, averaged over the runs 
at the different temperatures. (Oh, yes: make sure the temperature is held 
­between 2000 and 2060°F. We don’t know what happens outside of that 
range.) The fitted model gives a predicted rate of 2.6% for that set of design 
parameters. By way of comparison, the worst condition in the experiment had 
nearly 40% cracked pots.
Is the best process found in this experiment good enough or is more improve-
ment needed and feasible? That requires more information about the context. 
15
10
5
0
–5
–10
99
95
90
80
70
60
50
40
30
20
10
5
1
Effect
Percent
A
R
B
T
C
C
D
D
Factor
Name
Not significant
Significant
Effect type
AC
D
C
A
Normal plot of the effects
(response is %cracked, α = .05)
Lenth's PSE =1.5
Figure 5.17  Normal Probability Plot of Effects and Interactions.

167
Completely Randomized Design with Multiple Treatment Factors
Running the process with the slow cooling rate slows the process, reducing the 
throughput rate. Does the increased yield offset the longer production time? 
Can we increase the cooling rate somewhat without decreasing the yield by too 
much? The current experiment with only two cooling rates means we can only 
do a straight‐line interpolation between those two rates. More experimentation 
at intermediate rates could tell us if there is a bend in the curve. Can we run the 
process at an even slower cooling rate and get even closer to 100% yield? 
Further experimentation could address this question. Economic analyses are 
required to tell us about the costs and benefits incurred by trading process time 
for process yield.
Fractional Two‐Level Factorials
Example 6: E‐mail marketing
The preceding pot production example shows how “effect sparsity” makes 
it possible to run multifactor two‐level experiments with only one replica-
tion and still be able to separate real effects from “noise,” or experimental 
error variability. However, as the number of two‐level factors in an experiment, 
say, k, increases, the required number of runs, 2k, can be too large to be 
­feasible. For example, Ledolter and Swersey (2007, p. 228), present a case 
study (Case 5—summarized here with the permission of author Gordon H. 
Bell, president of LucidView) involving an experiment with 13 two‐level 
­variables in a potential e‐mail marketing campaign by an office supplies 
retailer. The variables included the e‐mail design (simple or “stronger”) and 
color (white or blue), the offer of a free gift (or not) with a purchase, product 
pictures (few or many), and the subject line (two versions). In addition, 
two other two‐level variables were used to identify customer segments—­
effectively three blocks of experimental units (customers). The objective of 
the experiment was to identify an e‐mail design that would maximize sales. 
Details of the experiment and its context (story) are given in Ledolter and 
Swersey (2007).
Running all 215 = 32,768 block–treatment combinations was out of the 
question. (A “run” consists of sending e‐mail to a large number of people and 
measuring the response. Different sets of people would be used for each run. 
A response rate of around 1% was expected.) Something less than 50 runs was 
feasible. What to do?
One approach is to eliminate some of the variables—hold them constant in 
the experiment. If we could whittle the experimental variables down to five (!), 
then we could run one replicate: 32 runs. We could even replicate half of them 
for a total of 48 runs. But the marketer wanted to learn something about all 15 
variables and was not willing to arbitrarily rule eight of them out of contention. 
That would defeat the purpose of the experiment.

Fundamentals of Statistical Experimental Design and Analysis
168
One‐factor‐at‐a‐time designs
Somebody may have come up with the following idea (I’m making this part up): 
our current e‐mail design is essentially a control condition. Let’s run that 
combination in the experiment, and then let’s do an additional 13 runs where we 
change one factor at a time to an alternative version, or level, while holding the 
other 12 variables at their control setting. Thus, we would learn something about 
all 13 variables in only 14 runs! From those 14 runs, we could pick the winning level 
of each of the 13 variables and then run one more run at those selected settings 
to confirm (or contradict) the anticipated economic benefit of the selected design. 
We could replicate the whole experiment three times.
At this point, a consultant is hired and is asked about this design. He says that it 
is a very inefficient design. It’s called an OFAT (one-factor-at-a-time) design. He 
illustrates this inefficiency in the simpler case of three two‐level variables, as follows.
Figure 5.18 depicts the OFAT treatment design. The vertices of the cube 
represent the full set of eight treatment combinations in a 23 experiment. The 
control condition, with coordinates (−1, −1, −1), is the lower left‐hand point. 
Then there are three excursions from this point, one along each of the three 
axes. You can see in Figure 5.18 that the cube is not very well covered by the 
four OFAT points. Furthermore, the effect of each factor is estimated by com-
paring only two of the points—the pair along each axis—and this effect esti-
mated is a local effect, relative to the control condition. There may be 
synergisms (interactions) among the variables that cannot be detected.
1
–1
1
–1
1
–1
C
B
A
One-factor-at-a-time experiment: Three variables
Figure 5.18  Schematic of One‐Factor‐at‐a‐Time Experiment: Three Two‐Level Factors.

169
Completely Randomized Design with Multiple Treatment Factors
Now, consider what is called a fractional factorial set of treatments. There 
are many ways you can pick four points from the eight cube vertices, all of 
which could be considered a one‐half fraction of a 23 full factorial, but some 
are better than others, for reasons that will be discussed. Figure 5.19 shows 
one of the good fractions.
Figure 5.19 includes the control condition in the lower left corner, but the 
other three points are at the other end of diagonals across each of the 
three faces. These four points cover the cube better than the OFAT points, 
and they provide more precision in estimating the effects of the three 
variables.
Consider estimating the effect of variable A: the difference in average 
response between the A = 1 and A = −1 conditions. The two points in 
Figure 5.19 at A = −1 are the two points on the left face of the cube. In those 
two points, B occurs once at B = −1 and once at B = 1. Similarly, both C = −1 
and C = 1 occur in these two points. At A = 1, the right face of the cube, the 
same balance and symmetry occur; B and C take on both levels, 1 and −1. 
Now, to estimate the effect of A, we can compare the average of the two 
points on the right face to the average of the two points on the left face. Under 
the assumption of no interactions among the three factors, the effects of B and 
C cancel out of this comparison, so we get a clean estimate of the A effect 
(or coefficient) based on all four data points. Similarly, the effects of B and C 
are estimated using all four data points. In contrast, the OFAT design esti-
mated the A effect from the difference of only two points.
1
–1
1
–1
1
–1
C
B
A
Fractional factorial experiment: Three variables
Figure 5.19  Schematic of Fractional Factorial Experiment: Three Two‐Level Factors.

Fundamentals of Statistical Experimental Design and Analysis
170
The key to this increased precision is the assumption of no, or at least 
­negligible, interaction. With this set of four points, you can estimate coeffi-
cients for only three terms in the model. Under the sparsity assumption, main 
effects are apt to be (but not certain to be) larger than any of the interactions. 
With a fractional factorial, though, there is always the chance that an interac-
tion is masquerading as a main effect. It takes further experimentation to sort 
out such conundrums.
Now, suppose variable A turns out to have no (appreciable) effect. Then 
the left and right faces of the cube can be collapsed, thus leaving a 22 set 
of treatments in variables B and C. Now, the interaction of B and C can be 
estimated (but we still would not have a data‐based error against which to 
judge the estimated effects and interaction). This projection property is 
what makes the particular fractional factorial shown in Figure 5.19 useful. 
By way of contrast, the OFAT set of treatments just collapses into a two‐
factor OFAT.
Many experimental design texts get into the nitty‐gritty of choosing and 
characterizing fractional factorials, but these details are beyond the scope of 
this text. Experimental design software can generate fractional factorials and 
tell you what can and cannot be estimated with them. So, too can good 
consultants.
Results: E‐mail experiment
In this case, the consultant proposed a fractional factorial of 32 runs. With this 
fraction, in the case of no three‐factor or higher‐order interactions, the 15 
main effects can be estimated and some two‐factor interactions can also be 
estimated. However, each two‐factor interaction is generally unseparable from 
(or confounded with, in the usual statistical terminology) six other two‐factor 
interactions. We will later demonstrate that characteristic using the flower pot 
example. Subject‐matter knowledge or further experimentation would be 
required to separate the interactions.
The number of customers who were sent the e‐mail defined by one of the 32 
run settings (block and treatment factors) ranged from 641 to 1515. A total of 
34,060 customers received e‐mail; no customer received multiple e‐mails. The 
responses measured were the number and size of orders that were placed by 
the e‐mail recipients within 1 week of the mailing. This case study pertains to 
“response rate,” which is the percent of e‐mail recipients who placed an order 
within 1 week.
The results of this experiment are interesting. The control condition actu-
ally had the fourth highest return rate. The best return rate was 1.75%; the 
control condition return rate was 1.39%. Thus, most of the hoped‐for improve-
ments that would be achieved by jazzing up the e‐mail design turned out not 
to be (the factor levels had been chosen to represent “bold” departures from 
the current e‐mail design). That must have been discouraging. The only 

171
Completely Randomized Design with Multiple Treatment Factors
significant variables that had a positive effect were the offer of a free gift with 
each order and the offer of a 15% discount (the discount was already offered 
and the experimental alternative was no discount). The promise of free stuff 
trumped e‐mail glitz! (Are you surprised?) That’s a worthy finding, though 
perhaps disappointing to e‐mail‐campaign designers. The study’s sponsor 
became a big fan of fractional factorial experiments, particularly in contrast 
with OFAT experiments: “He learned in 1 week what would take 6 months 
using standard techniques of testing one variable at a time” (Ledolter and 
Swersey 2007, p. 239).
Example 7: Flower pot experiment revisited
Let us suppose that instead of the full 24 set of treatment combinations used 
in the flower pot experiment, the consulting FLS had suggested running only 
eight of those 16 combinations, still with only one run of each treatment. The 
production manager is interested, because of the lesser time and cost, but 
wonders what information will be lost. “Well,” the FLS says, “Under the 
assumption that three‐way and four‐way interactions are negligible, we will still 
be able to estimate the main effects cleanly. With eight observations, after we 
estimate the four main effects, that leaves only three degrees of freedom for 
estimating interaction. There are six possible interactions among four vari-
ables, so we cannot estimate them all. In fact, pairs of interactions will be ‘con-
founded,’ that is, unseparable. However, based on your understanding of the 
process, that subject‐matter knowledge may help you identify the active inter-
action if one of the interaction pairs turns out to be significant.”
We know now that the production manager opted to run the full 24 
experiment, but let’s consider how this experiment might have come out if the 
fractional factorial set of eight treatments had been run.
First, how would the eight combinations have been selected? Table 5.13a 
shows all of the 16 treatment combinations of the four variables along with the 
two‐, three‐, and four‐variable products. These are the 15 predictor variables 
used in the model fit summarized in Table  5.12a. Note that each column 
includes eight 1s and eight −1s.
Now, let’s sort the rows of this table by RTCD. The result is Table 5.13b.
Now, let’s suppose we choose the last eight rows of this table as our half 
fraction of the set of 16 variable combinations. For these eight runs, RTCD 
equals 1 in all cases. This means that we could not estimate the four‐variable 
interaction, even if we wanted to, from this fraction. But that doesn’t matter 
because we’re going to assume that the four‐variable interaction is negligible. 
Ditto for the four three‐variable interactions. That leaves us with the single‐­
variable and two‐variable products in Table 5.14.
Examination of the two‐variable products shows that the RT column is the 
same as the CD column. Thus, we cannot separate the effects of these two 
predictors: they are confounded. Similarly, RC = TD and RD = TC. Note the 

Fundamentals of Statistical Experimental Design and Analysis
172
pattern: each two‐variable product is confounded with the product of the 
other two variables. That is a direct consequence of using the set of eight runs 
for which RTCD = 1.
Let’s suppose that this fractional factorial had been run and that the same 
results as before for these eight runs were obtained. Under the assumption that 
Table 5.13  Regression Variables in 24 Factorial Treatment Design.
Std Order
R
T
C
D
RT RC RD TC
TD
CD RTC RTD RCD TCD RTCD
(a) Standard order
1
−1 −1 −1 −1
1
1
1
1
1
1
−1
−1
−1
−1
1
2
1 −1 −1 −1 −1 −1
−1
1
1
1
1
1
1
−1
−1
3
−1
1 −1 −1 −1
1
1 −1
−1
1
1
1
−1
1
−1
4
1
1 −1 −1
1 −1
−1 −1
−1
1
−1
−1
1
1
1
5
−1 −1
1 −1
1 −1
1 −1
1
−1
1
−1
1
1
−1
6
1 −1
1 −1 −1
1
−1 −1
1
−1
−1
1
−1
1
1
7
−1
1
1 −1 −1 −1
1
1
−1
−1
−1
1
1
−1
1
  8
1
1
1 −1
1
1
−1
1
−1
−1
1
−1
−1
−1
−1
  9
−1 −1 −1
1
1
1
−1
1
−1
−1
−1
1
1
1
−1
10
1 −1 −1
1 −1 −1
1
1
−1
−1
1
−1
−1
1
1
11
−1
1 −1
1 −1
1
−1 −1
1
−1
1
−1
1
−1
1
12
1
1 −1
1
1 −1
1 −1
1
−1
−1
1
−1
−1
−1
13
−1 −1
1
1
1 −1
−1 −1
−1
1
1
1
−1
−1
1
14
1 −1
1
1 −1
1
1 −1
−1
1
−1
−1
1
−1
−1
15
−1
1
1
1 −1 −1
−1
1
1
1
−1
−1
−1
1
−1
16
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
(b) Sorted by RTCD
2
1 −1 −1 −1 −1 −1
−1
1
1
1
1
1
1
−1
−1
3
−1
1 −1 −1 −1
1
1 −1
−1
1
1
1
−1
1
−1
5
−1 −1
1 −1
1 −1
1 −1
1
−1
1
−1
1
1
−1
8
1
1
1 −1
1
1
−1
1
−1
−1
1
−1
−1
−1
−1
9
−1 −1 −1
1
1
1
−1
1
−1
−1
−1
1
1
1
−1
12
1
1 −1
1
1 −1
1 −1
1
−1
−1
1
−1
−1
−1
14
1 −1
1
1 −1
1
1 −1
−1
1
−1
−1
1
−1
−1
15
−1
1
1
1 −1 −1
−1
1
1
1
−1
−1
−1
1
−1
1
−1 −1 −1 −1
1
1
1
1
1
1
−1
−1
−1
−1
1
4
1
1 −1 −1
1 −1
−1 −1
−1
1
−1
−1
1
1
1
6
1 −1
1 −1 −1
1
−1 −1
1
−1
−1
1
−1
1
1
7
−1
1
1 −1 −1 −1
1
1
−1
−1
−1
1
1
−1
1
10
1 −1 −1
1 −1 −1
1
1
−1
−1
1
−1
−1
1
1
11
−1
1 −1
1 −1
1
−1 −1
1
−1
1
−1
1
−1
1
13
−1 −1
1
1
1 −1
−1 −1
−1
1
1
1
−1
−1
1
16
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

173
Completely Randomized Design with Multiple Treatment Factors
interactions involving T are negligible, the set of predictors to be ­considered is 
R, T, C, D, RC, RD, and CD. The resulting coefficients are as follows.
Predictor
Coef
Constant
18.375
R
5.875
T
.625
C
6.375
D
−4.875
RC
2.875
RD
−.375
CD
1.125
The results are similar to the analysis of the full set of 16 runs in that R, C, D, 
and RC are the largest coefficients. Of course, this would mean that we have 
four signals and three noise coefficients and that is not quite what is meant by 
effect sparsity. The graphical analysis of effects yields Figure 5.20 (recall that 
an “effect” is equal to two times the coefficient). By the standard threshold for 
significance, none of the effects or interactions stands out from the noise.
We chose the fractional factorial for which RTCD = 1 in the preceding analysis. 
Alternatively, the fraction for which RTCD = −1 could have been chosen. When 
the same analysis is carried out for this fraction, the result is summarized in 
Figure 5.21. Again, none of the effects or interactions stands out as “significant.”
If the production manager had to make a decision on how to produce flower 
pots based on either of these two fractional factorial experiments, by picking 
the level of the factors associated with the lower cracked pot rates, he probably 
would have made the same decision that arose from the full factorial experiment, 
although the lack of statistical significance could have made him reluctant to 
make any decisions. He might have rationally said, “I’m concerned about the 
uncertainty level in the fractional experiment. Let’s run the other half fraction 
and see if that substantiates our tentative conclusions from the first half fraction.”
Table 5.14  Regression Variables in 1/2 of a 24 Factorial Treatment Design.
Std Order
R
T
C
D
RT
RC
RD
TC
TD
CD
%Cracked
1
−1
−1
−1
−1
1
1
1
1
1
1
14
4
1
1
−1
−1
1
−1
−1
−1
−1
1
22
6
1
−1
1
−1
−1
1
−1
−1
1
−1
37
7
−1
1
1
−1
−1
−1
1
1
−1
−1
20
10
1
−1
−1
1
−1
−1
1
1
−1
−1
8
11
−1
1
−1
1
−1
1
−1
−1
1
−1
4
13
−1
−1
1
1
1
−1
−1
−1
−1
1
12
16
1
1
1
1
1
1
1
1
1
1
30

Fundamentals of Statistical Experimental Design and Analysis
174
Please take this example as a cautionary note: fractional, unreplicated treatment 
designs can be unable to find important effects and interactions. Either more 
replication or additional experimentation may be required to support high‐
consequence decisions.
20
10
0
–10
–20
Effect
Normal plot of the effects: Fraction RTCD = 1
(response is % cracked, α = .05)
Lenth's PSE =8.625
R
C
RC
D
A
R
B
T
C
C
D
D
Factor
Name
Not significant
Significant
Effect type
99
95
90
80
70
60
50
40
30
20
10
5
1
Percent
Figure 5.20  Graphical Analysis of 1/2 Fraction: RTCD = 1.
R
C
RC
D
Normal plot of the effects: Fraction RTCD = –1
(response is % cracked, α = .05)
A
R
B
T
C
C
D
D
Factor
Name
Not significant
Significant
Effect type
99
95
90
80
70
60
50
40
30
20
10
5
1
Percent
20
10
0
–10
–20
Effect
Lenth's PSE =7.125
Figure 5.21  Graphical Analysis of 1/2 Fraction: RTCD = −1.

175
Completely Randomized Design with Multiple Treatment Factors
Extensions
For much more on two‐level full and fractional factorial experiments, see Box, 
Hunter, and Hunter (1978, 2005), Montgomery (2012), or Ledolter and Swersey 
(2007) and references in those texts.
Assignment
1.  Choose a topic of interest to you and issues to investigate with an 
experiment that has a CRD and has three treatment factors to consider. 
Describe the design in detail: experimental units, treatments, response 
measurement, experimental protocol, and issues to be addressed. 
Describe your anticipated data plots and analyses.
2.  Consider the extended poison/antidote/dose/delay time experiment 
­discussed in this chapter and its ANOVA given in Table 5.7. Suppose that 
the ANOVA for Antidote B showed that both dose and delay time were 
significant and that the only significant interaction was poison by dose. 
Draw hypothetical plots of the treatment means that could correspond to 
these ANOVA results.
References
Box, G., and Cox, D. (1964) An Analysis of Transformations, Journal of the Royal 
Statistical Society, Series B, 26, 211–252.
Box, G., and Meyer, D. (1986) An Analysis for Unreplicated Fractional Factorials, 
Technometrics, 28, 11–18.
Box, G. E. P., Hunter, W. G., and Hunter, J. S. (1978, 2005) Statistics for Experimenters, 
1st and 2nd eds., John Wiley & Sons, New York.
Ledolter, J., and Swersey, A. (2007) Testing 1‐2‐3, Stanford University Press, Stanford, CA.
Lenth, R. (1989) Quick and Easy Analysis of Unreplicated Factorials, Technometrics, 31, 
469–473.
Milliken, G., and Johnson, D. (2009) Analysis of Messy Data Volume I: Designed 
Experiments, 2nd ed., Chapman and Hall/CRC Press, Danvers, MA.
Montgomery, D. (2012) Design and Analysis of Experiments, 8th ed., John Wiley & 
Sons, Inc., New York.
Morris, M. D. (2011) Design of Experiments: An Introduction Based on Linear Models, 
Chapman and Hall/CRC Press, Boca Raton, FL.
Myers, R., Montgomery, D., and Anderson‐Cook, C. (2009) Response Surface 
Methodology: Process and Product Optimization Using Designed Experiments, 
3rd ed., John Wiley & Sons, Inc., New York.
Wikipedia (2011) Ethanol, http://en.wikipedia.org/wiki/Ethanol.
Wikipedia (2014) Stepwise Regression, http://en.wikipedia.org/wiki/Stepwise_regression.
Wikiquote (2014) George E. P. Box, http://en.wikiquote.org/wiki/George_E._P._Box.


Fundamentals of Statistical Experimental Design and Analysis, First Edition. Robert G. Easterling. 
© 2015 John Wiley & Sons, Ltd. Published 2015 by John Wiley & Sons, Ltd.
Introduction
As discussed in Chapter 2, blocking of the experimental units in an experiment 
can be crucial to an experiment’s success and value. The choice of blocks 
defines the scope of the experiment and the conclusions it supports. Blocking 
can also increase the precision with which treatments are compared, relative to 
an unblocked experiment spanning the same set of experimental units. We will 
see later in this chapter a dramatic demonstration of the efficiency of the 
blocked design in the context of the boys’ shoes experiment in Chapter 3.
The randomized block design (RBD) is constructed like a collection of b 
­separate completely randomized designs (CRD) for comparing k treatments, 
where b is the number of blocks. In each block, each of k treatments is ran-
domly assigned to one or more experimental units, as in a CRD. In the simplest 
situation, the number of treatments equals the number of experimental units 
(eus) in a block, so each treatment is assigned to only one eu in each block. 
(As discussed later (p. 203), it is possible for there to be fewer than k eus in a 
block, so that not all treatments can be run in each block.) We have already 
seen an example of this type of a randomized block experiment: the afore-
mentioned boys’ shoes material experiment in Chapter 3. In that experiment, 
there were 10 blocks (boys), each of which (whom) contained two experimental 
units (feet). The two treatments, shoe sole materials A and B, were assigned 
­randomly to the two feet in each block. Of course, no single block could be 
analyzed as a CRD because there was no replication—there was only one 
­replicate of each treatment in a block. However, the collection of wear differ-
ences, (B–A), across boys, was a set of replicate differences and thus could be 
statistically analyzed, as in Chapter 3.
Randomized Complete 
Block Design
6

Fundamentals of Statistical Experimental Design and Analysis
178
In other situations, one can have blocks that have enough eus so that each 
treatment can be randomly assigned to two or more eus in each block. For 
example, in an experiment to evaluate the effects of medication on cholesterol 
levels, the blocks may be groups of people categorized by sex and three age 
groupings. One might have 60 participants in each of these six blocks and 
­randomly assign 20 (eus) in each block to one of three medications, say lipitor, 
zocor, and the ever‐popular placebo. Cholesterol measurements would be 
made at the beginning of the trial and periodically during the trial. Other par-
ticipant characteristics such as weight and body mass index would also be 
measured initially and periodically. The response that might be measured and 
analyzed could be the change in cholesterol levels between final and initial 
measurements. Other analyses could evaluate how quickly a medication led to 
a change in cholesterol.
In this sort of experiment, we can evaluate treatment differences in each 
block, and moreover, we can evaluate whether the treatment differences are 
consistent across participant groups, that is, from block to block. In more 
technical terms, we can test for “block–treatment interaction.”
The RBD can have unequal replication (of treatment assignments within 
blocks), but for convenience, we will restrict our attention here to equal 
replication, say, r replicates of each treatment in each block. We start with 
the general case of two or more replicates of each treatment in each block 
and then address the special case of single replicates for each treatment in 
a block.
Design Issues
The first issue to deal with is whether the experiment should be blocked or 
not. Blocks serve several purposes in a design, as discussed earlier. Are there 
natural blocks of experimental units that we are interested in? For example, 
men and women subjects in a clinical trial. Do we want to use blocks to deter-
mine the range of applicability of the experiment’s results (e.g., different 
growing conditions in an agricultural experiment, different sources of raw 
materials used in an industrial process, different regions in a marketing 
experiment)? Is there a source of variation that we can control by blocking 
(e.g., time of day in a production process)? Are there experimental objectives 
that call for the inclusion of particular blocks (e.g., multiple suppliers of an 
automobile part)?
Once the decision has been made to structure or group the experimental units 
into blocks, the next design issues pertain to the number and nature of the blocks 
and the number of experimental units in each. If we have freedom to ­control 
the number of eus in a block, a control we didn’t have with the boys’ feet, but 
did with the cholesterol medication experiment in the previous section, then the 
choice of block size will depend on the number of treatments to consider in 

179
Randomized Complete Block Design
the experiment and the number of replicates in each block. Power and sample 
size calculations can be used to help size the experiment.
RBD with replication: Example 1—battery experiment
Montgomery (2001, Chapter  5) provides data from an experiment that 
addresses the effect of temperature on battery life. The following story 
wrapped around the data is mine.
The government, for a special (perhaps secret) 
­application, needs to power an electronic device with 
a battery that will provide power for a specified 
­duration and do so reliably over a wide range of tem-
peratures. Three contract bidders have each produced 
12 prototype batteries for testing by the government 
agency (these are expensive, government‐procured 
batteries, not household batteries, and that is the 
reason for the small size of this experiment). These batteries differ primarily in 
their plate material: denoted here, for the sake of security, as materials 1, 2, 
and 3. Thus, there are a total of 36 experimental units, structured as three 
blocks of 12 eus each.
The treatments to which these batteries will be subjected are defined by 
operating temperature. The government’s requirement is that the batteries 
have “reliable operation” over temperatures ranging from 15°F (degrees 
Fahrenheit) to 125°F. The agency has decided to test at those extremes and 
the midrange; thus, the selected temperature levels are 15, 70, and 125°F. 
(Note: My former employer, Sandia National Laboratories, operates a Battery 
Abuse Testing Laboratory, known coolly as the BATLab, and they could do this 
testing and more. Note the many other possibilities: a camera abuse testing 
laboratory would be a CATLab, etc.)
Four batteries in each block are randomly assigned to each temperature. 
The batteries are connected to a specified load while operating at their 
assigned temperature. We’re not told, but will assume that storage conditions 
are applied independently to each battery in the experiment. We will also 
assume that the temperatures are tightly and accurately controlled. During the 
test, the batteries are continually monitored and their lifetime, in hours (or 
possibly a coded response, you can’t be sure), is the recorded response. 
Lifetime is operationally defined as the time at which battery output falls below 
a specified level.
As noted in Chapter  2, some authors, for example, the author of the 
Wikipedia (2014a) entry on RBD, describe blocking factors as “nuisances”—
extraneous sources of variation that the design controls in order to improve 
the precision with which treatments can be compared. That is decidedly not 
the case of the blocks in this experiment. The blocks are the three manufac-
turers, and the government is most interested in choosing among these three 

Fundamentals of Statistical Experimental Design and Analysis
180
battery makers. This experiment’s design is an RBD because of the way in 
which the eus (batteries) are grouped and the treatments (temperatures) are 
assigned to eus. Whether the blocking factor (or factors) are nuisances or of 
keen interest doesn’t change the experimental design; it would, however, 
affect the focus of the analysis.
The results of this experiment are shown in Table 6.1.
Analysis 1: Plot the data
Because the treatment factor (temperature) is a quantitative variable and the 
blocking variable is a qualitative variable (plate material), the appropriate 
display of the data is a plot of lifetime versus temperature for each material. 
This plot is shown in Figure 6.1.
Figure 6.1 shows that temperature, especially high temperature (125°F), 
substantially shortens battery life, relative to low‐temperature (15°F) 
storage. The lifetimes versus temperature trends, though, differ consider-
ably by material. Material 1 is degraded substantially at 70°F, while material 
3 is not affected at all at 70°F (relative to life at 15°F). Material 2 is 
intermediate—some degradation from 15 to 70°F and additional degrada-
tion at 125°F.
It is clear that if we have to pick a material based on this experiment, material 
3 is the winner: batteries with this material have the longest lifetimes at both 
70 and 125°F and lifetimes at 15°F that are comparable to those for the other 
two materials.
Table 6.1  Battery Lifetimes by Material and Temperature.
Material
Temperature (°F)
15
70
125
1
130
34
20
74
80
82
155
40
70
180
75
58
2
150
136
25
159
106
58
188
122
70
126
115
45
3
138
174
96
168
150
82
110
120
104
160
139
60
Table 5.1, p. 176 of Montgomery (2001), reproduced by permission 
of John Wiley & Sons.

181
Randomized Complete Block Design
Figure 6.1 shows (eyeball analysis—never underestimate the value of eye-
ball analysis) that the variability among batteries, of each material, that are 
exposed to the same temperature is pretty consistent across temperatures 
and materials. Thus, we collapse the data in each material/temperature 
combination and display the averages in an interaction plot: Figure 6.2. Note, 
though, that in contrast to the previous chapter which dealt with interaction 
among treatment factors, here, we are dealing with block by treatment inter-
action. Just to ­reiterate: material, or manufacturer, is a blocking factor; it 
defines groups of experimental units. It is not a treatment applied to an 
experimental unit.
As discussed in Chapter 5, (statistical) interaction between two factors, in 
this case plate material and temperature, occurs when the effect of one 
factor is different for the different levels of the other factor. That is distinctly 
the case here, at least graphically, because 70°F apparently affects material 
1 batteries quite differently (average lifetime is cut in half) than it does for the 
other two materials.
Analysis of variance
To address the question of whether the apparent interaction is “real or 
random,” ANOVA is again the tool. For the randomized complete block design 
with replication, the ANOVA has the structure shown in Table 6.2.
120
90
60
30
0
200
150
100
50
0
120
90
60
30
0
200
150
100
50
0
1
Temp.
y-hours
2
3
1
2
3
Material
Scatter plot of y-hours vs temp.
Panel variable: Material
Figure 6.1  Scatter Plot of Battery Lifetimes versus Temperature, by Material.

Fundamentals of Statistical Experimental Design and Analysis
182
This is the very same ANOVA structure, a “two‐way ANOVA” with replica-
tion, as in the case of a CRD with a crossed two‐factor treatment structure and 
r eus for each treatment combination (e.g., the poison/antidote experiment of 
Chapter 5). Though the ANOVAs are the same mathematically and data tables 
would look the same, this should not confuse you into thinking the experi-
ments are the same. In Chapter 5, we had one group of homogeneous eus and 
did one randomization of the assignment of the (two‐factor) treatment combi-
nations to eus. The design was a CRD. Here in Chapter 6, we have b different 
types/blocks of experimental units, and we do separate randomization of 
treatments to eus in each block—this is an RBD.
Note that the Error term in Table 6.2 is the variability of the r eus in each 
block–treatment combination, pooled across the bt block–treatment combina-
tions. There are bt block–treatment combinations and r − 1 df for experimental 
error in each set of r replicates; hence, the Error df is bt(r − 1).
Table 6.2  ANOVA Structure for Randomized Complete Block Design with Replication.
Source
df
SS
MS
F
P
Blocks (B)
b − 1
Treatments (T)
t − 1
B × T interaction
(b − 1)(t − 1)
Error
bt(r − 1)
Total
btr − 1
125
70
15
150
125
100
75
50
Temp.
Mean
1
2
3
Material
Interaction plot for y-hours
Data means
Figure 6.2  Interaction Plot of Material/Temperature Means.

183
Randomized Complete Block Design
The ANOVA for the battery lifetime data is shown in Table 6.3.
The pertinent result to consider first in Table 6.3 is the test for interaction 
because if interaction is significant, the message of that finding is that you 
should not compare materials averaged across temperatures; the tempera-
ture effects are not the same across materials. The interaction test result for 
these data is F = 3.44 on 4 and 27 df, and a P‐value of .02. This is fairly 
strong evidence that the apparent interaction is real: the temperature 
effects are not the same for all three materials. The ANOVA confirms the 
eyeball impression.
But so what? Can we choose a winning battery from this result? Well, it 
depends. (Statistical) Life doesn’t end with a significance test. We don’t know 
the requirements a battery is supposed to meet. If the requirement was that 
the battery should last 1 h, reliably, over the entire temperature range, then all 
three battery types meet this requirement, comfortably (with perhaps a little 
concern about materials 1 and 2 at 125°F—see Fig. 6.1). We could choose the 
contractor based on cost or Congressional district—whatever.
On the other hand, if the requirement was that the battery should last 50 h 
reliably (meaning with high probability), over the entire temperature range, 
then none of the battery types looks adequate.
Reliability analysis
The best battery at 125°F is material 3, which had an average lifetime (over 
the four batteries tested) of 85.5 h. The variability among batteries of the 
same type at a given temperature is estimated by the square root of the 
Error MS in Table 6.1: s
675
26 h. For the sake of estimating battery 
reliability (for this example, reliability is defined as the probability that bat-
tery life exceeds 50 h), let’s use the statistical model that lifetime has a 
Normal distribution with a mean of 85.5 h and a standard deviation of 26 h. 
Then, the estimated reliability is Prob(z > (50 − 85.5)/26) = Prob(z > −1.36) = 
.91, where z has a standard Normal distribution (mean = 0, stdev = 1.0). 
Thus, as a point estimate, about 9% of material 3 batteries would fail to 
provide 50 h of life at 125°F. This may not be good enough for the 
government. Furthermore, because the mean lifetime is only based on four 
Table 6.3  ANOVA for Battery Experiment.
Source
DF
SS
MS
F
P
Material
2
10 684
5342
7.91
.002
Temp
2
39 119
19 559
29.0
.000
Interaction
4
9614
2403
3.56
.019
Error
27
18 235
675
Total
35
77 647

Fundamentals of Statistical Experimental Design and Analysis
184
observations, this is a pretty imprecise estimate. We need further analysis 
to account for this imprecision, such as follows.
The standard error of the estimated mean is SE 26
4
13
/
, based on 27 
df. The t‐value for the .025 point on the t(27) distribution is −2.05. Thus, the 
lower 97.5% confidence limit on mean lifetime for material 3 at 125°F is given 
by 85.5 − 2.05(13) = 58.9. By using this conservative value of 58.9 h for the 
mean and 26 h as the standard deviation, the estimated probability of 
exceeding 50 h life is Prob(z > (50 − 58.9)/26) = Prob(z > −.34) = .63. Thus, 
­conservatively (with ~97.5% confidence in this illustration), as many as 37% of 
material 3 batteries could fail to meet the requirement. (This calculation is a 
little bit optimistic because we have not accounted for the imprecision in the 
estimated standard deviation, but, even with a bit of optimism, the basic 
conclusion is that we definitely have a reliability problem.)
This (low reliability) is bad news. It’s back to the drawing board for the 
battery manufacturers. Or, hmm. Maybe it’s worth reexamining the require-
ments. This happens in practice. Do we really need 50 h of reliable life? And 
just how high a reliability do we really need? And how likely is it that the 
device will really be required to operate at 125°F? Government (and other) 
agencies have been known to set “stretch requirements” for systems and 
devices in order to build in some margin (after all, field operation will be 
different from battery testing in a controlled environment). Contract bid-
ders have been known to ask that the requirements be “scrubbed” a bit. 
“Let’s sharpen our pencils,” they say. The discussion could get contentious, 
but at least all parties have data to work from. And the discussions could 
lead to additional testing; the data obtained in this experiment could be 
used to help decide how much testing should be done. As we discussed in 
the case of the boys’ shoes (Chapter  3), integrity is needed as well as 
statistical and battery design and manufacturing skills to assure that the 
government gets the battery it needs at a price that doesn’t gouge the 
taxpayer.
Further analysis
Let’s suppose the government decides to drop the battery made with material 1, 
with its poor performance at 70°, from consideration. How do materials 2 and 
3 compare? Graphically, they have a similar pattern, but material 3 batteries 
have longer lifetimes at the two higher temperatures. Is the difference real, or 
could it be random?
Table 6.4 gives the ANOVA for the battery life data, excluding material 1.
Table 6.4 shows some evidence of interaction and some evidence of a real 
difference between materials, averaged over temperatures (which is appro-
priate if you assume interaction is negligible). A cautious government agency 
would call for further experimentation to better resolve the difference ­between 
materials 2 and 3.

185
Randomized Complete Block Design
Bringing subject‐matter knowledge to bear
The alert reader will have noticed that temperature is a quantitative treatment 
factor and our analysis thus far has not made use of that characteristic. The 
analysis has helped the sponsoring agency decide to eliminate material 1 from 
further consideration and to conduct further experiments to sharpen the 
difference between materials 2 and 3, and that may be enough. Further anal-
ysis aimed, for example, at linking battery lifetime to temperature via a 
mathematical function is not really needed. For designing further experiments, 
though, considering such a relationship may be helpful. This experiment also 
provides an opportunity to bring in some subject‐matter theory to the analysis, 
which is always a good idea.
Chemical reactions, such as occur in the discharge of a battery, are often 
accelerated by temperature. That is why the battery lifetimes in this 
experiment decrease with increasing temperature. The theory underlying 
this phenomenon has led to what is called an Arrhenius equation (Wikipedia 
2011). Under this theoretical relationship, the logarithm of lifetime is a linear 
function of the reciprocal of “absolute temperature.” Absolute temperature 
is measured on what is called the Kelvin temperature scale. Its “zero point” 
is absolute zero, where no thermal activity occurs. By way of contrast, the 
Celsius (or Centigrade) temperature scale has its zero point at the freezing 
temperature of water. The relationship between these two temperature 
scales is that absolute temperature = 273 + Celsius temperature. For 
example, 20°C is equal to 293°K.
The temperatures in the battery experiment are expressed on the Fahrenheit 
scale (which has its zero point at 32° below the freezing point of water). 
Converting the Fahrenheit temperatures to Celsius and then adding 273 reex-
presses the experiment’s temperatures on the Kelvin scale. The Arrhenius 
equation uses the reciprocal of absolute temperature. Table  6.5 shows the 
conversion from Fahrenheit temperatures to Kelvin temperatures and then the 
reciprocal of Kelvin temperatures. The latter is labeled TKinv.
The Arrhenius relationship refers to the rate of a chemical reaction. The 
­battery characteristic measured is lifetime, not reaction rate. Lifetime and reac-
tion rate are related, at least conceptually, because if the reaction rate doubles, 
Table 6.4  ANOVA for Battery Experiment: Materials 2 and 3.
Source
DF
SS
MS
F
P
Material
1
1683
1683
3.68
.07
Temp
2
30 231
15 115
33.1
.00
Interaction
2
2537
1268
2.78
.09
Error
18
8226
457
Total
23
42 677

Fundamentals of Statistical Experimental Design and Analysis
186
the lifetime, that is, the time for the battery to discharge all of its stored energy, 
would be reduced by a factor of two. Thus, if the Arrhenius equation applies 
to these batteries, a plot of the logarithm of lifetime versus inverse absolute 
temperature would be approximately linear. However, we don’t know how life-
time was measured in this experiment. It could have been the time at which 
output voltage dropped below a particular threshold, not when it went to 
zero. Output voltage is not a linear function of remaining charge (as I learned 
from Wikipedia), so this measure of lifetime might not follow the Arrhenius 
relationship. We need to look at the data and see what the relationship is.
Figure 6.3 shows a plot of the average log‐lifetimes versus inverse absolute 
temperature, by material. (This plot is essentially the mirror image of the ­earlier 
interaction plot—Figure  6.2—because, as can be seen in Table 6.5, TKinv 
is  nearly a linear function of the experimental temperatures, in degrees 
Fahrenheit. In Figure 6.3, temperatures decrease from left to right.)
0.0038
0.0034
0.0031
5.00
4.75
4.50
4.25
4.00
TKinv
Mean
1
2
3
Material
Plot of average log-lifetimes vs. inverse absolute temp., by material
Figure 6.3  Plot of Average Log‐Lifetimes versus Inverse Absolute Temperature.
Table 6.5  Fahrenheit Temperatures Converted to Centigrade, 
Kelvin, and the Reciprocal of Kelvin Temperature.
Temp (°F)
Temp (°C)
Temp (K)
TKinv
15
−9.4
263.6
.038
70
21.1
294.1
.034
125
51.7
324.7
.031

187
Randomized Complete Block Design
If the Arrhenius relationship applied, we would expect to see roughly three 
straight lines, perhaps differing in slope. The slopes would differ if the different 
plate materials had different “activation energies,” in Arrhenius terminology. 
(In more general terms, the materials would have different temperature 
­sensitivities.) No such pattern is exhibited. When separate straight‐line regres-
sion models are fitted to each material’s data separately, only material 3 is 
statistically consistent with a straight‐line model.
In this case, some subject‐matter knowledge did not help us interpret the 
data. Clearly, we don’t know enough about this experiment and its data to 
push this analysis further. Perhaps the follow‐on experiments can be built to a 
greater extent on appropriate battery chemistry theory.
Example 2: More tomato fertilizer experiments
Garden experiments
The experimenter in Chapter  3 soon realizes that getting a definitive 
comparison of tomato fertilizers out of her garden alone is difficult. She doesn’t 
have room to plant many tomato plants, and there is enough plant‐to‐plant 
variability among tomato yields that it is difficult to detect important differ-
ences in yield among the fertilizers she has in her experiment. One way to get 
more replication is to repeat her experiment year after year, but who has time 
for that? She has an inspiration: I’ll get other gardeners to join me in the 
experiment. She goes to a tomato‐grower online chat room and soon finds 10 
other people scattered around the country who are interested. They work out 
a protocol and eventually decide that they will each plant 10 tomato plants of 
one variety (not necessarily the same for all 11 experimenters—a possible 
expansion of the experiment) and randomly assign five plants each to fertil-
izers A and C. Thus, taken together, the experimental design is an RBD with 11 
blocks, two treatments, and five replications of each treatment in each block. 
Plotting and analyzing the combined data (crowdsourcing?) would tell the 
tomato‐growing group (and readers of their online report) whether there was 
a consistent difference between fertilizers across locations and whether that 
difference was real. It might also show interaction, which could mean that the 
winning fertilizer and the margin of victory were location dependent, which 
could lead to more experimentation as science marches on.
Product research experiments
A research organization charged with developing new and better fertilizers 
(perhaps more environmentally friendly) will want to run experiments that 
compare a new fertilizer to fertilizers on the market. The new fertilizer will have 
a broader, more profitable market if it can be shown that the new fertilizer con-
sistently produces better yields and less harmful environmental effects than 
the competition in a wide variety of soil types and growing conditions. Thus, a 
blocked experiment is called for: choose locations (blocks) that span a suitable 

Fundamentals of Statistical Experimental Design and Analysis
188
range of soil quality and growing conditions and at each location randomly 
assign fertilizers to plants, follow a good and consistent experimental pro-
tocol, and analyze the resulting yields. Because small differences in yield can 
have large economic effects, conduct a large enough experiment so that these 
differences can be estimated with good precision.
Example 3: More gear teeth experiments
Problems found in automobiles often result in recalls: owners are notified of the 
problem and advised to bring in their cars for corrective service, paid for by the 
manufacturer (at the time this is written, General Motors has had a massive 
recall of ignition switches). The story (my story) in Chapter 4 was that broken 
teeth are caused by low injection volumes of gear material into the gear mold. 
We don’t know if that problem had existed from the start of production, or if it 
is more localized. The set of tests in Chapter 4 did not identify the gears by 
production date. Now, ideally, the auto company could visit their supplier, say, 
“Let’s work together on this,” and ask for their production records: “You do 
keep records on how much powder was used in each ­production run, don’t 
you?” Of course, even if you know when underweight gears were produced, 
you don’t necessarily know in what cars they were installed. Major parts have 
serial numbers that can be tracked, small plastic gears don’t. So, the auto man-
ufacturer may not be able to target their recall. If they could, though, we could 
do a blocked experiment: choose gears from b different production runs, ran-
domly assign the seven tooth positions for strength testing in each block of 
gears, and then do plots and analyses to ­compare the production runs and to 
see if the patterns of tooth strengths by position differed among the different 
production runs. This experiment and analysis could isolate the problem gears.
RBD with Single Replication
The basic RBD described in most texts has only a single replication of each 
treatment in each block: there are b blocks of k eus each and k treatments to 
compare. Each treatment is randomly assigned to one eu in each block. Thus, 
there is no true replication—we do not have multiple homogeneous eus 
receiving the same treatment, so we don’t have a direct estimate of the 
­experimental error variance (the variability among eus that independently 
receive the same treatment). You can see in the RBD ANOVA in Table 6.1 that 
with r = 1 there are zero df for Error. You can’t get something (an estimate of 
experimental error) from nothing (zero df).
The answer (theory tells us) is that IF (this is a big if) there is no underlying 
block–treatment interaction for the eus in the experiment (which means that 
the difference between treatment means is consistent across blocks), then the 
calculated block–treatment interaction MS is an estimate of the experimental 

189
Randomized Complete Block Design
error variance. (This is the very situation we were testing for when we tested 
for interaction in the battery RBD with replication.) If there is underlying block–
treatment interaction, then when we use the block–treatment interaction MS 
as our estimate of experimental error, which is the denominator for the F‐test 
for block or treatment effects, we are using an inflated estimate; hence, we 
lose sensitivity and validity in evaluating possible differences among blocks or 
treatments. We would tend to overestimate true experimental error and fail to 
detect real treatment differences. That may be an acceptable cost for the 
efficiency of a single‐replicate RBD. You can find treatment differences even 
with overestimated experimental error variability.
The assumption of no block–treatment interaction should not be made 
blindly. Subject‐matter knowledge is essential: is it plausible to assume that 
the underlying differences among treatments are consistent across blocks 
(meaning no block–treatment interaction)?
Example 4: Penicillin production
A Wikipedia website (July 2008) stated, “The produc-
tion of penicillin is an area that requires ­scientists and 
engineers to work together to achieve the most effi-
cient way of producing large amounts of penicillin.” 
The penicillin production process starts with a batch 
of a particular fungus that is then fed ­various nutrients 
that stress the fungus and cause it to ­synthesize peni-
cillin. (Similarly, the stress of an exam can sometimes 
cause students to produce something they didn’t 
know they were capable of.)
An experiment from Box, Hunter, and Hunter (2005, Chapter 4) is in the 
spirit of scientists and engineers (and collaborating friendly local statisticians, 
I hope) working together to find efficient ways of producing penicillin. The 
experiment is aimed at comparing the process yield for four process variations 
(yield, expressed as a percentage, is the amount of penicillin produced per 
unit of process input; 100% yield would mean no waste). One ingredient in all 
four processes is the nutrient, “corn steep liquor.” It is known that important 
penicillin‐producing properties of corn steep liquor vary appreciably from 
batch to batch. (BHH use the term “blend,” but I think batch is more intuitive.) 
We could run a CRD in this situation with a batch of corn steep liquor being the 
experimental unit that is then processed (treated) by one of the four processes. 
Some number of replicates (batches) would be run through each process. The 
resulting experimental Error MS will reflect this batch‐to‐batch variability of 
corn steep liquor as well as the variability of the processes themselves (they 
aren’t perfectly repeatable).
As BHH note, though, it was found that a batch was large enough that it 
could be divided into fourths and each of these subbatches of corn steep 

Fundamentals of Statistical Experimental Design and Analysis
190
liquor used to produce penicillin by one of the four processes, randomly 
assigned within each batch. If this experiment was run with several batches 
(blocks), say, then we would block out batch effects from the experimental 
error that we’re going to use to evaluate the difference among processes. That 
is, we could evaluate process (treatment) differences against the variability of 
subbatches within a batch (block). That should provide better precision in 
comparing processes than if we ran the CRD with, say, 20 total runs. And thus, 
it came to pass that this was the experiment that was run: randomized complete 
block design with five blocks (corn steep liquor batches) of four experimental 
units (subbatches), four treatments (processes), and single replications of each 
treatment in each block.
But wait! What about the assumption of no batch‐process interaction? Is it 
plausible? Well, corn steep liquor batches are produced to have the same 
ingredients and penicillin‐growing properties, so common sense (subject‐
matter knowledge) tells us that, even though these properties do vary, the 
underlying yield differences among processes should be (reasonably) consis-
tent from batch to batch of the same liquor. There is no physical/chemical 
reason to suppose that some processes favor particular batch properties over 
others. Further, batch‐to‐batch differences can probably be treated as just 
random variation (some of that nuisance variability that some authors write 
about); there are no systematic factors affecting this variability. The assump-
tion of no batch‐process interaction seems justified.
The data from this experiment are plotted in Figure 6.4. We see that yield 
ranges from 77 to 94%. There are no consistent winners or losers among the 
four processes, though. For example, Process B (red symbol and line) has the 
lowest yields for Batches 1 and 2, the highest for Batches 3 and 4, and again 
not so good for Batch 5. There is some indication of consistent batch differ-
ences. In particular, Batches 2 and 5 had generally the lowest yields, while 
Batches 1 and 4 fairly consistently had the highest yields.
Table 6.6 gives the ANOVA for these data. These results confirm what our 
eyeball analysis told us: there are no consistent differences among processes 
that stand out above the inherent variability of these processes (note 
the small F, large P). The marginally significant batch differences show that 
by blocking on batches, some reduction of experimental error was 
accomplished.
Where we might go from here depends on what is important. Because no 
real yield differences among processes were found, this means that the choice 
of process could be based on considerations other than yield. Cost, processing 
time, and environmental impact (say of process waste disposal) would be three 
such considerations.
Probably, the finding of most concern here is the finding of (what seems 
to me to be) substantial yield variability from batch to batch and among sub-
batches within a batch. This is akin to the finding in the first tomato fertilizer 
experience that there was a soil quality trend in the garden that contributed 

191
Randomized Complete Block Design
to tomato yield variability. In the next subsection, we will analyze these 
“components” of variability.
Components of variation
The finding that there was no evidence of real differences among processes 
means we can plot the data without showing the association of each data 
point with the process (treatment) it pertained to. Figure 6.5 shows the data by 
Batch. (Fig. 6.5 is Fig. 6.4 without linking the data points by process.)
The visual impression from Figure 6.5 is that the variability among the sub-
batch yields in each batch is consistent across batches and that the separa-
tion of the batches indicates that there is more variability among batches 
that one might expect from the amount of variability observed within 
5
4
3
2
1
100
95
90
85
80
Batch
Yield
A
B
C
D
Process
Scatter plot of yield vs batch, by process
Figure 6.4  Data Plot for Penicillin Yield Experiment. Table 4.4, p. 146, BHH (2005), 
used with permission of John Wiley & Sons.
Table 6.6  Two‐Way ANOVA: Yield versus Batch, Process.
Source
DF
SS
MS
F
P
Batch
4
264
66.0
3.50
.04
Process
3
70
23.3
1.24
.34
Error
12
226
18.8
Total
19
560

Fundamentals of Statistical Experimental Design and Analysis
192
batches. The impression is similar to that for the gear teeth data discussed 
in Chapter 4. This impression is quantified by carrying out a one‐way ANOVA 
on the data: the only factor to consider is Batch. That ANOVA has the same 
structure as the ANOVA for a CRD with one Treatment factor. Table  6.7 
gives the ANOVA.
The Error df and SS in this ANOVA are equal to the sum of the Process and Error 
df and SS in Table 6.4. The P‐value for Batches is essentially the same as before.
(Note that I have done something that many texts never do: I have run two 
ANOVAs on the same data. Not arbitrarily, though. After the first ANOVA indi-
cated one factor, process, was not a contributor to yields, I dropped that factor 
from consideration and did the appropriate resulting one‐way ANOVA. In the 
same way, I transitioned from a plot that reflected both factors to a plot that 
just illustrated the more important factor—batch.)
Now, for the one‐factor CRD, the follow‐up analysis (Chapter  4) was to 
examine the treatment differences to identify the differences that stand out. 
That is not appropriate here. For example, we will never see Batches 1 and 3 
again, so it makes no sense to ask whether the mean yields for Batches 1 and 
3 are significantly different. Instead, our analysis is aimed at characterizing 
the  two sources of variation in the data: (i) batch‐to‐batch variability and 
(ii) subbatch variability within batches. To provide a basis for this analysis, I will 
introduce a statistical model.
5
4
3
2
1
100
95
90
85
80
75
Batch
Yield (%)
Plot of penicillin yields by batch
Figure 6.5  Penicillin Yields by Batch.

193
Randomized Complete Block Design
Let yij denote the yield for batch i, subbatch j. A simple, additive statistical 
model for yij (for yield data we might have gotten) is
y
b
e
ij
i
ij,
where:
μ = overall average yield, bi = the (random) “effect” (on yield) of batch i; bi is 
assumed to have a Normal distribution with mean zero and standard deviation, 
σb, eij = (random) effect of subbatch i,j; eij is assumed to have a Normal distribu-
tion with mean zero and standard deviation, σe, which is constant across all 
batches, and bi and eij are independent random variables.
This model is an example of what is called the “random effects model,” 
also a “variance components model,” on which there are extensive refer-
ences in the statistical literature. In words, the model says that there is a 
random batch effect (modeled as a deviation from the overall average yield) 
that affects every subbatch in the batch and there is additional random vari-
ation among subbatches in a batch. The objective of the analysis is to 
estimate the two standard deviations in the model. Of course, we don’t 
observe μ or the batch and subbatch effects; all we observe is the subbatch 
yields. Nevertheless, ANOVA provides the quantities from which the two 
sigmas can be estimated.
First, the square root of the Error MS estimates σe, the standard deviation 
among subbatches in a batch. From Table 6.4, that estimate is 19 7
4 44
.
.
. 
The df associated with this estimate is 15.
Second, theory shows us that the Batch MS in the ANOVA estimates 
e
b
2
2
4
. 
(In more technical terms, the “expected value” of the Batch MS is equal to 
e
b
2
2
4
. This means that if we repeatedly generated data according to the above 
variance component model and calculated the ANOVA table repeatedly, then 
the long‐run average of the Batch MS would be this function of the two sigmas.) 
The factor of four is due to the fact that we had four subbatches in each batch. 
Note that if there was no batch‐to‐batch variability, σb would be equal to zero, so 
both the Error MS and the Batch MS would be estimating the same variance. 
That’s why the F‐test is appropriate for detecting this additional source of varia-
tion. Because the Error MS estimates 
e
2, we can estimate 
b
2 by
Estimated
BatchMS
ErrorMS
b
2
4
66 0
19 7
4
11 6
–
. –
.
. .
Table 6.7  One‐Way ANOVA: Penicillin Yield versus Batch.
Source
DF
SS
MS
F
P
Batch
  4
264
66.0
3.34
.04
Error
15
296
19.7
Total
19
560

Fundamentals of Statistical Experimental Design and Analysis
194
For the penicillin yield data: estimated 
b
11 6
3 40
.
.
. (Note: The difference 
between BatchMS and ErrorMS can be negative, even though the quantity 
being estimated, 
b
2, must be positive. Convention, in such a case, is to use 
zero as the estimated batch variance.)
The approximated df associated with this estimated variance is generally 
obtained by a method called Satterthwaite’s approximation (Wikipedia 
2014b). Applying this method in this case results in the conclusion that the 
­approximate df associated with this estimate is only two. This means that 
this is a very imprecise estimate. The 95% confidence interval on the 
­underlying batch yield standard deviation is the interval: (1.8, 21%). With 
only five batches in the experiment, the maximum df we could have had for 
the estimate of batch ­variation is four. The within‐batch variation, though, 
obscures what we can learn about batch‐to‐batch variation, so we only get 
2 df worth of information, not 4 df. We need considerably more batches in 
the experiment to obtain a meaningful estimate of the batch standard 
deviation.
Sizing a Randomized Block Experiment
Consider an RBD with b blocks, k treatments, and r replicates of each treatment 
in each block (r > 1). The analysis of data from this design often comes down to 
the comparison of two treatment averages. The standard error of the difference 
between two treatment averages, say, treatments A and B, is
SE
EMS
A
B
ybat
ybsr
br
–
,
2
where EMS is the Error MS from the ANOVA, based on bt(r − 1) df. The denom-
inator, br, reflects the fact that each average is an average of br observations; 
the factor of two reflects the fact that we are taking the difference between 
two independent averages.
This standard error is similar to that of a difference between two treatment 
means in an unpaired, two‐treatment experiment with br experimental units 
in each treatment. The difference is that here the EMS is based on all the 
data, not just the data for the two treatment means being compared. Thus, 
if we use power curve criteria, plus a planning value for the error standard 
deviation, σ, for determining the number of observations in each treatment, 
as in Chapter  3, our analysis will be somewhat conservative because the 
actual error term will be based on more df than this two‐sample analysis 
assumes. The n that results from that analysis can be equated to br, and the 
experiment planners can trade off the number of blocks, b, and the number 
of replicates, r.

195
Randomized Complete Block Design
In the case of an RBD with one replicate of each treatment in each block, the 
standard error of the difference between two treatment averages is
SE
EMS
A
B
ybar
ybar
b
–
,
2
where now the EMS is based on (b − 1)(t − 1) degrees of freedom. A power 
curve‐based analysis for a paired experiment could be used, somewhat 
­conservatively, to determine the number of blocks required to satisfy specified 
power criteria.
True Replication
Example 5: Cookies
Let’s suppose we wanted to do an experiment 
to evaluate the effect of baking temperature on 
cookies made by three cookie‐mix manufac-
turers. Here’s one way the experiment could be 
designed:
●
●
Prepare one batch of dough for each manu-
facturer: A, B, and C.
●
●
Split each batch into three bowls of dough. 
Assign each bowl, randomly, to one of three 
temperatures selected for the experiment.
●
●
Make four cookies from the dough in each bowl.
●
●
Bake the four cookies simultaneously at their assigned temperature.
●
●
Measure, by some means, the quality of each cookie.
This experiment looks a lot like the battery experiment; there are three 
­manufacturers, three temperatures, and four cookies for each of the nine 
­manufacturer–temperature combinations. A data table would look the same. 
But is it the same experimental design?
First, what is the experimental unit—the material to which the temperature 
treatment is independently applied? The answer is the eu is a group of four 
cookies formed from one bowl of dough. The response measured on the eu 
would be the average quality score of the four cookies that make up a single 
experimental unit.
Was the experiment a block design? Yes. A block is the three bowls of dough 
made from one batch of dough made with a single manufacturer’s mix. Thus, 
we have three blocks of three eus; each eu in a block is randomly assigned one 
of three temperatures. The design is an RBD with a single replicate of each 
treatment in each block.

Fundamentals of Statistical Experimental Design and Analysis
196
What analyses can we do? We can do a two‐way ANOVA, without ­replication. 
The F‐test for temperature effects would have to use the block by temperature 
interaction as the denominator, which is valid under the assumption that the 
temperature effect is the same for all three manufacturers, that is, there is no 
block by treatment interaction. Presumably, though, a major reason for running 
the experiment is to find out, as in the battery experiment, if there is any 
difference in temperature sensitivity among the three mix manufacturers. This 
experiment makes it impossible to answer the question that motivated the 
experiment! I have seen this happen in real‐world experiments with much more 
at stake than cookies.
Under the assumption of no block by temperature interaction, we can also 
do a test for block differences. However, because we created only one batch 
of dough for each manufacturer, we don’t know whether any apparent block 
differences are due to manufacturer or just reflect inherent batch‐to‐batch var-
iability of batches independently prepared with the same mix. The experiment 
would need to have multiple batches from the same mix in order to separate 
manufacturer differences from batch variability.
If the ANOVA was done on the individual cookie scores and interpreted as 
though the design was an RBD with replication, the variability among the 
four cookies in the nine different mix‐temperature combinations would be 
used to test for interaction as well as for temperature and mix effects. This 
assumed error term, though, is measuring the variability within an experi-
mental unit and is therefore not appropriate for evaluating effects and inter-
actions that are measured at the bowl‐of‐dough level. Because within‐eu 
variability is apt to be smaller than among‐eu variability, the tests may be 
unduly sensitive and lead to an unwarranted conclusion of real effects and 
interaction where none exist.
Bottom Line: Experimental design matters. You can’t tell a design by the 
layout of a spreadsheet—or a cookie sheet.
Example 6: Battery experiment revisited
In the battery experiment, the protocol was that individual batteries were 
stored at assigned temperatures and their lifetime was recorded. Suppose, 
though, that someone had noted that the temperature chamber and instru-
mentation would permit, say, all four batteries with a given plate material to be 
stored, attached to a load, and monitored simultaneously. That would save 
time. It would also sacrifice replication at the battery level. It makes this 
experiment equivalent to the (very flawed) cookie experiment. Now, the exper-
imental unit is a group of four batteries with the same plate material. It’s the 
group of four that gets a single application of the temperature treatment. 
Individual batteries are samples from this experimental unit.
The natural response for the group (the eu) is the average lifetime of the four 
batteries in the group. This collapsing of the data results in an RBD with three 

197
Randomized Complete Block Design
blocks and three treatments and only single replicates in each block. Table 6.8 
gives the ANOVA for these data.
(The first three lines of this ANOVA are the first three lines of the ANOVA in 
Table 6.3 divided by four. The factor of four is due to the fact that we are ana-
lyzing the means of four observations.) Had we done the experiment this way 
and gotten the same numerical results, we cannot tell from this ANOVA that 
there is significant interaction between materials and temperatures (we can’t 
test for it as we could when we processed the batteries singly and indepen-
dently), and consequently, when we compare the differences among materials 
to our Error MS, based on the assumption of no block–treatment interaction, 
the conclusion is that there is no (statistically significant) difference in materials. 
That’s the wrong conclusion. What we know from the original experiment is that 
the different materials respond quite differently to increased temperature. This 
example should serve as a cautionary note about running single‐replicate RBD. 
If resources permit, multiple‐replicate RBD should be used.
Note: There is a way to test for a particular type of interaction in the case of 
only one replication in a two‐way classification of data, such as a block–
treatment cross‐classification. This method is due to Tukey (1949), and it is 
illustrated in Box, Hunter, and Hunter (2005, p. 222) on data from the penicillin 
yield experiment.
Example 7: Boys’ shoes revisited
As discussed in the Introduction to this chapter, the boys’ shoes experiment in 
Chapter 3 was a randomized block experiment: there were 10 blocks (boys), 
each with two eus (feet), and two treatments (Materials A and B) assigned ran-
domly in each block. Because each material could only be assigned to one 
foot, this is an RBD with no replication.
In Chapter 3, the boys’ shoes data were ultimately analyzed via the paired 
t‐test. We obtained a t‐value of 3.35, based on 9 df, and found the two‐tailed 
P‐value to be .009. Now that we’ve progressed to Chapter 6 and see that 
this experiment was a randomized block experiment, we can also address the 
question of a “real or random?” difference between the two sole materials by 
an ANOVA of the data. Table 6.9 gives this ANOVA.
The F‐test statistic for comparing the materials is 11.2, based on 1 and 9 
df (numerator and denominator), for which the P‐value is, lo and behold, 
Table 6.8  ANOVA for Modified Battery Experiment.
Source
DF
SS
MS
F
P
Material
2
2626
1313
2.24
.22
Temp
2
9784
4895
8.34
.04
Error
4
2348
587
Total
8
5953

Fundamentals of Statistical Experimental Design and Analysis
198
P = .009. In fact, the F‐value is equal to the t‐value squared (3.352 = 11.21, 
with a little difference due to round‐off). Theory shows that an F‐statistic 
based on (1, f ) numerator and denominator df’s is equivalent to the square of a 
t‐statistic based on f df. The difference, though, is the ANOVA doesn’t tell 
us the sign of the difference; the t‐test does. Of course, the graphical dis-
plays one should or would do before either of these quantitative analyses 
would have told us the direction of the difference between materials: B had 
more wear than A.
As discussed, one purpose of blocking an experiment is to remove a 
source of variation from the comparison of treatments. The boys’ shoes 
experiment provides an example of the power of blocking. For the blocked 
experiment, the standard error of the average difference was SE(dbar) = .12%. 
This ­standard error was determined by dividing the standard deviation of 
the 10 differences by the square root of 10. That is exactly the same as the 
square root of the quantity: two times the Error MS in the Table 6.7 ANOVA 
divided by 10.
Now, suppose we had run a CRD in which we randomly assigned n boys to 
wear a pair of shoes of material A and another n boys to wear a material B pair. 
The standard error of the difference in this case (see the Chapter 3 discussion 
of the tomato fertilizer experiment) is s
n
2 /
, where s is the pooled boy‐to‐boy 
standard deviation within materials. The (boy‐to‐boy) standard deviations of 
both the A‐data and the B‐data are about 2.5%. Thus, for example, if the CRD 
experiment had been done with n = 10 boys in each material group, the stan-
dard error of the difference would have been 2 5
2 10
1 12
.
/
.
. This is 
approximately 10 times the SE(dbar) obtained from the blocked experiment. 
Because the SE is inversely proportional to the square root of n, this means it 
would have taken 102 or 100 times as many boys to get the same precision as 
the blocked experiment with 10 boys got. In other words, we would have to 
have had 1000 boys wearing A and 1000 boys wearing B to get the same pre-
cision as the blocked experiment got with 10 boys wearing one shoe of each 
material! Amazing. When the FLS who came up with the idea to block the 
experiment showed her boss this comparison, she was immediately promoted 
to Chief Statistical Officer and given a sizable bonus as well.
Nothing was said explicitly in Chapter 3 about the assumption of no block–
treatment interaction. The issue was discussed, though, in other terms. In 
Table 6.9  Two‐Way ANOVA: Wear % versus Boy No., Material.
Source
DF
SS
MS
F
P
Boy
9
110.5
12.3
163.8
.000
Material
1
.84
.84
11.2
.009
Error
9
.68
.075
Total
19
112.00

199
Randomized Complete Block Design
particular, we wondered whether the wear differences might depend on variables 
(such as age, weight, or activity levels of the boys in the experiment) that might 
have been useful in understanding the wear differences if they had been 
recorded. In the absence of such concomitant variables, we relied first on a 
randomization analysis and then a paired t‐test and related confidence inter-
vals, which treats the variation of differences as random variation, rather being 
due in part to variables such as boy’s weight, to evaluate the wear differences 
between B and A.
For the randomization analysis, the statistical model for data we might have 
gotten, under the assumption of no difference between A and B, was to ran-
domly rearrange the A and B labels on the 10 pairs of wear data. The random-
ization test is validated simply by the act of randomly assigning treatments, so 
the no‐interaction assumption does not come into play.
For the t‐based analysis, the statistical model for data we might have gotten 
is a random sample of 10 differences from a single Normal distribution. The 
mean of that distribution, call it δ, is a constant. That is, the expected wear 
difference between B and A is the same for all boys. We didn’t have the data 
that would enable us to challenge that assumption. For example, suppose the 
underlying difference between sole wear of the two materials was related to a 
boy (block) characteristic such as number of hours the shoes were worn. Then, 
we would have single samples from ten different distributions, not 10 random 
samples from one distribution. This problem is the same as when in the first 
tomato fertilizer experiment it was found that yield depended on row position, 
thus contradicting the model of random samples from two Normal distributions. 
A key message you should take from this discussion is that randomization is the 
key to proper interpretation of experimental results, not ad hoc assumptions 
about Normal distributions.
Extensions of the RBD
Multifactor treatments and blocks—example: Penicillin 
experiment extended
The preceding examples in this chapter were all RBD experiments involving 
one blocking factor and one treatment factor. Just as in Chapter 5 we extended 
the CRD introduced in Chapter  4 to multifactor treatments, it is natural to 
extend the RBD to multifactor blocks, treatments, or both. This means that the 
experiment and subsequent statistical data analysis can address not just gen-
eral differences among blocks or treatments but also the separate effects of 
block or treatment factors and the interaction among block factors, treatment 
factors, and between block and treatment factors.
Suppose you are the statistician who has been brought in to analyze the 
penicillin yield data and you ask the process engineers, “Tell me more 
about these four processes you want to compare. How are they defined; 

Fundamentals of Statistical Experimental Design and Analysis
200
how are they different?” (Good consulting statisticians always ask lots of 
questions.) Eventually, you find out that two of the key processing variables 
are the time and temperature conditions under which fungi are fed nutri-
ents. You further draw out the fact that the four processes of interest are 
defined by the four combinations of low and high temperature and short 
and long time. “Wow,” you say. “That’s a 2 × 2 factorial set of processes. 
We’ll be able to evaluate the separate effects of time and temperature and 
we’ll see whether time and temperature interact.” (More than once in real 
life I have found out that vague conditions, or processes, in an experiment 
were actually multifactor combinations of environmental or processing var-
iables.) As a starting point in identifying the effects of the treatment factors, 
this treatment structure means that the ANOVA in Table 6.10 will have the 
following form.
(The entries for Time, Temp., and Time × Temp are indented to denote that 
these three lines in the ANOVA represent the decomposition of the three df 
for Process.)
If we could define the four processes in terms of time and temperature, we 
could separate the Process SS accordingly and find out what effects these two 
variables, singly and jointly—possible interaction—have on yield.)
Now, let’s look at the overall picture. The primary finding in the earlier 
variance component analysis (p. 191–194) is that there is appreciable batch‐
to‐batch variation in yields but we need more data to get a better handle 
on this variation. We need to run more batches. Running more batches will 
also improve the sensitivity with which we can evaluate the process differ-
ences and the time and temperature effects. We might even consider 
obtaining multiple batches of corn steep liquor from multiple suppliers. 
Other suppliers might be more ­consistent than the one we are now using. 
Our analysis of these further experiments would address differences among 
suppliers with respect to average yield and with respect to the batch‐to‐
batch variability of yield. The main thing we’ve learned about penicillin 
yield is that we need more data to understand it better. Such is the nature 
of science—and manufacturing.
Table 6.10  ANOVA for Penicillin Experiment, Extended.
Source
DF
SS
MS
F
P
Batch
4
264.0
66.0
3.50
.04
Process
3
70.0
23.3
1.24
.34
Time
1
Temp.
1
Time × Temp.
1
Error
12
226.0
18.8
Total
19
560.0

201
Randomized Complete Block Design
Example 8: A blocks‐only “experiment”—textile  
production
In Chapter 1, I defined an experiment as a controlled intervention in a process. 
In such an intervention, treatments are applied to experimental units and the 
resulting responses provide information about the treatment effects. There is 
a class of structured observation studies, though, in which multiple factors in, 
typically, a production process or a measurement process are studied with an 
aim of identifying dominant sources of variability in these processes. The find-
ings of this study might be followed up with experiments aimed at finding 
processes changes that would reduce this variability. This is a fundamental 
method by which organizations can improve the quality of processes and prod-
ucts—reduced variability. For example, in the penicillin production example, 
the producer of corn steep liquor batches could look for ways to change the 
production process to produce more uniform (and high‐yield) batches. In the 
bond‐strength case study in Chapter 1, process engineers, aided by their FLS, 
found that the bonding and testing processes both were major sources of var-
iability in the measured strength of bonds and subsequent experimentation 
and process changes led to reducing this variability. The following example, 
which deals with textile production, is from Scott and Triggs (2003).
In a textile factory, cloth is produced on a loom operated by one technician. 
Plant management (my story) is concerned about variation in the strength of 
the cloth produced in its factory. A study is undertaken in which cloth samples 
are produced by each of five technicians using each of seven looms. The two 
factors are thus crossed, and the five technicians and seven looms are ran-
domly selected from large populations of each. (Visualize a large factory with 
at least dozens of looms and technicians.) Why the study was limited to five 
technicians and seven looms is not explained. I will speculate that it was to 
control the disruption of cloth production by the factory.
In the study, each technician produces one cloth sample on each loom so the 
result is 35 cloth samples (eus) cross‐classified by loom and technician. No 
treatments, at this point, are applied to these 35 eus. Thus, we have 35 blocks, 
defined by the two blocking factors, loom and technician, with one eu in each 
block. The response measurement of interest is cloth strength, and Triggs’ 
random effects analysis leads to estimates of the loom and operator variance 
components. One might regard this as a uniformity study, similar in concept to 
the gauge studies discussed earlier.
Analysis 1: Plot the data
Figure 6.6 shows the data.
First, note that the looms are arbitrarily numbered, so there is no horizontal 
trend to look for across looms. The fact that the lines are fairly flat for all five 
technicians indicates that the looms are fairly consistent. We see that 

Fundamentals of Statistical Experimental Design and Analysis
202
machines are less variable than people because the main feature that stands 
out is the difference between technician 2 and her (I’ll assume, pardon the ste-
reotype) peers. Her cloth samples had considerably higher strength than the 
other four technicians, by roughly 50%. (On second thought, maybe technician 
2 is a guy, the other four are women, and, as we all know, men don’t read 
instructions.) What’s going on? Is one technician doing the right thing or are 
four? This pattern is not just a reflection of Normally distributed random varia-
tion, which one would assume in doing a variance components analysis of these 
data. Management needs to talk to these technicians to understand the causes 
of the variability among them and then take steps to reduce the variability.
Discussion
This example also reinforces the importance of plotting the data. One could 
feed these 35 cloth‐strength measurements into statistical software and arrive 
at estimates of three variance components: loom to loom, technician to tech-
nician, and repeatability, which is the variation in repeated runs by the same 
technician on the same loom. Note that there was no replication in this 
experiment, so the repeatability variance component has to be estimated by 
the loom by technician interaction mean square in the ANOVA. The result of 
this analysis, as indicated by the data plot, is that the technician variance is the 
L7
L6
L5
L4
L3
L2
L1
600
500
400
300
200
100
0
Strength
T1
T2
T3
T4
T5
Tech
Line plot of cloth strength: technicians by looms
Figure 6.6  Cloth Strength by Technicians and Looms. Source: Scott and Triggs (2003, 
p. 91), used here by permission of Department of Statistics, University of Auckland.

203
Randomized Complete Block Design
largest source of variation. This might/should lead the data analyst to look at 
the data and discover the patterns of variability noted earlier. We wouldn’t 
want to conclude from the quantitative results that, “Oh, people are variable, 
just random. Not much we can do about that.” This is not like the penicillin 
production example where we will never see the batches that were in the 
experiment again. These technicians and looms are still part of our production 
process, so we need to understand the causes of the differences we saw in the 
data. There may be cost implications of the study’s findings. Handled correctly, 
this study could lead management and labor to work together in a never‐
ending, data‐driven, empowering, motivated quest for better quality, consis-
tency, and cost‐effectiveness!
Balanced Incomplete Block Designs
Example: Boys’ shoes revisited again
Suppose the head of shoe research comes to the statistician who had earned 
great respect with the paired experiment (randomized block) design to 
­compare two shoe sole materials and says, “Now, we need to compare three 
different tread designs. How are you going to put three shoes on one boy? 
Huh, huh?”
Never fear. Statisticians have developed a class of designs called balanced 
incomplete block designs that can be used when the number of experimental 
units in a block is less than the number of treatments. In general, suppose there 
are b blocks available (or we can create them) with k experimental units in each 
and we want to evaluate t treatments (t > k). Catalogs exist (e.g., Cochran and 
Cox 1957, Chapter 11) that provide designs for feasible combinations of b, k, 
and t. For the requested shoe experiment, the appropriate incomplete block 
design would be as follows:
●
●
Recruit n boys and randomly divide them into three groups (of the same 
size, if possible, though that’s not a requirement).
●
●
In Group 1, randomly assign tread designs A and B to each boy’s two feet. 
In Group 2, randomly assign tread designs A and C to each boy’s two feet. In 
Group 3, randomly assign tread designs B and C to each boy’s two feet.
●
●
Have the boys wear the shoes for some period of time and then measure 
appropriate characteristics of the soles, such as wear at key locations.
Thus, we run three paired designs. The analysis is a little tricky and beyond 
the scope of this text, but appropriate software can handle it. Just to indicate 
the issues, note that, say, the difference between designs A and B can be esti-
mated from the Group 1 boys. Let dbar(A − B) denote the mean difference of 
the selected measurement for Group 1. This average difference estimates the 

Fundamentals of Statistical Experimental Design and Analysis
204
A − B difference. Similarly, from Group 2 we get dbar(A − C) and from Group 3 
we get dbar(B − C). Now, if we take the difference between the dbars for 
Groups 2 and 3, we get an estimate of the A − B difference based on the 
Groups 2 and 3 data. Thus, we have two estimates of the same difference: one 
from Group 1 and one from the difference between Groups 2 and 3. We need 
to combine them. The appropriate combination is a weighted average of 
the two A − B estimates.
Let’s suppose that the underlying variance from boy to boy of wear differ-
ences is the same for all three groups. Denote this common variance by σ2. This 
assumption can be checked with the data. Suppose also that the number of 
boys in each group is the same for the three groups, say, r. Denote the two 
estimates of the A − B difference that the experiment provides as
Group
A
B
A
B
1
1
:
^
dbar
Groups
and
A
B
A
C
B
C
2
3
2 3
:
^
.
,
dbar
dbar
The “hats” (carats) in these expressions indicate that we’re estimating 
the underlying average difference between A and B. The subscripts denote 
the Groups on which the estimates are based.
The variances of these two estimates are
var A
B
[(
)^ ]
1
2
r
var
A
B ^ ,
2 3
2
2
r
Theory tells us that the optimum way to take a weighted average of two esti-
mates of the same quantity is to weight the estimates inversely proportional to 
their variances. Thus, we weight (A − B)^1 by a weight of 2/3 and (A − B)^2,3 
by 1/3 (the weights need to sum to 1.0). In terms of the average differences, 
this combined estimate of the underlying difference between tread designs A 
and B is
A
B
A
B
A
C
B
C
^
2
3
1
3
1
3
dbar
dbar
dbar
.
The variance of this estimate, by using the property that the variance of a linear 
combination of random variables is equal to the sum of the squares of the 
coefficients times the variance of each term, is equal to
var
A
B
/
/
/
^
/
/
/
/
/
.
4 9 1 9 1 9
2 3
3
2
2
2
2
r
r
r
Note that if we used only the Group 1 differences between A and B, the 
average difference would have a variance of σ2/r. By including the other two 
Groups to get a combined estimate of the A − B difference, the denominator 

205
Randomized Complete Block Design
is increased by one‐half. Thus, for example, if r = 10, the combined estimate is 
as precise as if we had run a paired experiment with only tread designs A and B 
with 15 boys. We have “borrowed information” about the A − B difference from 
Groups 2 and 3 and improved the precision with which the A − B difference can 
be estimated. In statistical literature, this information borrowing is called the 
recovery of interblock information. The 20 boys in Groups 2 and 3 in essence 
provide five boys worth of information about the A − B difference. This is yet 
another amazing accomplishment of good statistics.
This same sort of analysis can be carried out in general for incomplete block 
designs involving b blocks of k eus and t (>k) treatments.
Summary
This chapter has dealt primarily with the basic RBD: treatments are randomly 
assigned within blocks of experimental units. In addition to balanced incom-
plete block designs, statisticians have developed other designs that feature 
constraints on the definition of blocks and assignment of treatments. Examples 
of these designs are discussed in the next chapter.
Assignment
Choose a situation and issue(s) of interest to you. Design a randomized block 
experiment appropriate for investigating your issue or issues. Include at least 
one blocking factor and at least two treatment factors in your experiment:
●
●
Define the experimental units, blocks, treatments, replication, and response 
measurement.
●
●
Describe the protocol for applying treatments to experimental units and 
measuring the response.
●
●
Describe how you would plot the resulting data.
●
●
Lay out the ANOVA table for your experiment. Give the sources of variation 
and corresponding df.
●
●
Discuss potential follow‐on experiments.
References
Box, G., Hunter, W. G., and Hunter, J. S. (1978, 2005) Statistics for Experimenters, John 
Wiley & Sons, Inc., New York.
Cochran, W., and Cox, G. (1957) Experimental Designs, John Wiley & Sons, Inc., 
New York.
Montgomery, D. (2001) Design and Analysis of Experiments, John Wiley & Sons, New York.

Fundamentals of Statistical Experimental Design and Analysis
206
Scott, A., and Triggs, C. (2003) Lecture Notes for Paper STATS 340, Department of 
Statistics, University of Auckland, Auckland.
Tukey, J. (1949) One Degree of Freedom for Non‐additivity, Biometrics, 5, 232–242.
Wikipedia (2008) Penicillin, http://en.wikipedia.org/wiki/Penicillin#Mass_production.
Wikipedia 
(2011) 
Arrhenius 
Equation, 
http://en.wikipedia.org/wiki/Arrhenius_ 
equation.
Wikipedia (2014a) Randomized Block Design, http://en.wikipedia.org/wiki/Randomized_ 
block_design.
Wikipedia (2014b) Welch–Satterthwaite Equation, http://en.wikipedia.org/wiki/Welch% 
E2%80%93Satterthwaite_equation.

Fundamentals of Statistical Experimental Design and Analysis, First Edition. Robert G. Easterling. 
© 2015 John Wiley & Sons, Ltd. Published 2015 by John Wiley & Sons, Ltd.
Introduction
Over the years, the need to develop experimental designs that efficiently 
address important issues in the face of various constraints on experimental 
materials, protocols, and costs has led to the creation of new experimental 
designs. We have seen some examples:
1.  The shoe research manager needed to compare three tread designs in a 
situation where it was advantageous (for reasons of statistical precision) to 
use blocks (boys) that had only two experimental units (feet) in each block. 
Balanced incomplete block designs, developed for agricultural needs, 
provided a clever and efficient way to do that.
2.  Industrial processes posed new problems that agriculturally motivated 
designs did not address adequately. Industrial processes often involve a 
large number of process variables, in which case running an experiment 
with a full factorial set of treatment combinations is often prohibitive 
because of cost or time requirements. This constraint led to experiments 
with all factors at only two or three levels. Then, when there are enough 
factors that full factorial treatment combinations still become excessive, 
research led to the selection of cleverly selected fractions of the set of all 
possible factor combinations. There is no free lunch, though. We usually 
pay a cost for fractionalization in that the design then cannot detect inter-
actions that might be important. If subject‐matter context supports 
assuming certain interactions are negligible, this cost is often acceptable. 
If the experiment is a “screening” experiment, aimed at finding the most 
important factor effects as a preliminary to deeper experimentation with a 
Other Experimental Designs
7

Fundamentals of Statistical Experimental Design and Analysis
208
subset of potential factors, this sort of staging can be an effective strategy. 
These sorts of trade‐offs and decisions are made based on subject‐matter 
knowledge and theory and helped by theory and analyses pertaining 
to statistical precision and the efficiency of candidate designs. It is not 
easy to make these decisions. They are not readily reduced to an algorithm.
Thus far in this text, we have considered two basic families of experimental 
designs: the completely randomized design and the randomized block design. 
In these designs, treatments are either assigned to a single set of experimental 
units completely at random, or they are randomly assigned to experimental 
units within each of multiple blocks of experimental units. Variations within 
these two families of designs have had to do with treatment or block structure. 
Both blocks and treatments can have multifactor structures, and both block 
and treatment factors can be either quantitative or qualitative. These aspects 
of the design affect the analysis, but do not change the basic structure of the 
design—the way treatments are assigned to experimental units. As broad as 
these two families are, extensions or modifications are still called for in many 
contexts. In this chapter, we consider some additional designs that modify or 
extend the basic CRD and RCB designs in various ways. Designs that depart 
from these basic structures are also discussed.
Latin Square Design
Example: Gasoline additives and car emissions
Reducing the emissions from automobiles is an area of ongoing widespread 
research. Federal and state governments set progressively lower limits, and 
regulators evaluate automobiles and fuels to determine whether limits are met 
or adequate progress is being made. Researchers therefore search for effec-
tive and cost‐effective methods to reduce emissions. In Chapter 5 we saw an 
experiment pertaining to the effects of the ethanol/gasoline/air mix on CO 
emissions from automobiles. We now consider another emissions reduction 
example from Box, Hunter, and Hunter (2005). The following story is mine.
Suppose that a chemical manufacturer has developed three candidate 
gasoline additives. The director of research wants to do some testing to com-
pare the additives. Preliminary tests on the company’s test engine have shown 
some apparent differences among the additives, but the director now wants a 
more stringent, realistic test. He wants to test these additives under the sorts 
of conditions that real drivers driving real cars in real traffic impose. He hires a 
testing lab to design and conduct an experiment.
After some fruitful discussion, the test lab rep says, “How about this: You 
provide us with four cars and we will instrument them”. You choose the cars to 
cover a reasonable variety of makes and sizes. They will be instrumented to 

209
Other Experimental Designs
capture and measure the emissions produced by a 
drive of whatever specified duration and driving condi-
tions we decide on. I will employ just one driver in order 
to eliminate driver‐to‐driver variability and hold down 
costs. Driving each of the four cars, he will make one 
run using gasoline treated with each of the three addi-
tives plus a control run with no additive, in a random 
order with suitable purging of the fuel tank, fuel lines, 
carburetor, and exhaust pipes and chambers between 
runs. Thus, a total of 16 test drives. Showing off, he 
adds: “This is a randomized block experimental design with four blocks (the 
cars) and four treatments (the additives, counting the control condition of ‘no 
additive’ as one of the additives).”
The director replies, “I think drivers can make a difference. Two drivers 
driving the same car are apt to produce different amounts of emissions (from 
the engines, mind you) just because of driving style. I think we need more 
drivers in the experiment.”
The lab rep says, “OK, I’ll hire four different drivers. I’ll (randomly) assign 
each driver to one of the cars for the duration of the test. Each of the four car/
driver combinations will make single runs using each of the four additives, in a 
random order, as before. Still a total of 16 test drives.”
Director:	 “The problem with that, I think, is that if we see an apparent 
difference among car/driver pairs, we won’t know whether 
it’s the car or the driver. I don’t want one carmaker looking 
bad in this test just because its driver happened to be lead‐
footed.”
Lab rep:	
“OK, then. Suppose we run tests with all 16 combinations of 
four drivers and four cars. For each of those combinations we’ll 
make the four runs with the four additives, as before. Now, our 
experiment is a randomized block with 16 blocks, structured 
as a 4 × 4 factorial combination of cars and drivers, and four 
runs in each block, one with each additive. (He sketches a 
table showing this design.) This makes for a total of 64 test 
runs. That’s going to cost you.” And he gives a figure.
Director:	 “Wow! That’s too much.” He then one‐ups the lab rep by 
asking, “Say, have you ever heard of a Latin square design?” 
(He heard about that design from his neighbor who happens 
to be a FLS.) “That might be appropriate here.”
Lab rep:	
“I’ll look into it.”
A week later, the lab rep reports back. “That Latin thingy looks like a possible 
solution. Here’s a table (Table 7.1) that shows the combinations of cars, drivers, 
and additives we will run.” The experiment still has a total of 16 runs.

Fundamentals of Statistical Experimental Design and Analysis
210
“This table has a special characteristic: Each additive appears once in each row 
and each column. That is, each additive is run once with each car and each driver. 
Thus, when you compare the Additive means, each mean is calculated over runs 
that include all four cars and all four drivers. This balance means that we will get 
fairly clean comparisons of the effects of the four additives: the car and driver 
effects will cancel out. My own FLS, who happens to be my sister‐in‐law, says that 
the assumption we’re making is that the differences (in emissions) among 
additives should be consistent across cars and drivers and car/driver combina-
tions. Similarly, we’re assuming that the difference between drivers should be 
consistent across cars; lead‐footed in one car, lead‐footed in the others.”
Director:	 “Seems reasonable to me. Let’s roll.”
They then work out the protocol. Each test drive is to follow a specified 
course of 150 miles, with a mix of urban and rural driving, at the posted speed 
limits, conditions permitting. Each driver’s four runs will be done in a random 
order. The engines, fuel tanks, and exhaust systems will be carefully purged 
between runs. At the end of each run, the cumulative emissions data will be 
collected, and perhaps, other variables such as elapsed time, fuel consump-
tion, and ambient temperature will be collected and entered into an Excel 
spreadsheet. The test managers will also check the odometers to be sure the 
drive covered 150 miles.
Incidentally, as is often the case, there is a measurement issue in this 
experiment. You could measure emissions per mile driven or emissions per gal-
lon of fuel consumed (or both). The former measures the combined effect of 
engine design and additive. The latter measures additive effect more directly.
The experiment was conducted without a hitch, and the emissions data 
(in coded units) are given in Table 7.2.
Details
Randomization in a Latin square design is done by starting with a basic Latin 
square and then randomly assigning the factor levels to the symbols. In this 
experiment, when the additives were randomly assigned to symbols, the 
Table 7.1  4 × 4 Latin Square Design for Additive Experiment.a
Driver
Car
A
B
C
D
I
A1
A2
A4
A3
II
A4
A3
A1
A2
III
A2
A4
A3
A1
IV
A3
A1
A2
A4
a Cell entries denote the additive assigned to a given car/driver 
combination.

211
Other Experimental Designs
treatment of no additives was labeled A2 in Table 7.1. This treatment is 
denoted by CL, for control, in Table 7.2. Also, the random assignment of the 
three additives to symbols resulted in A4 being assigned the A2 additive; A1 
and A3 actually were assigned the A1 and A3 symbols. The additive labels in 
each cell of Table 7.2 are the chemical company’s additive names, not the 
generic names in Table 7.1. I point all this out in case any alert reader wonders 
why the A2s in Table 7.1 are not in the same boxes as they are in Table 7.2.
(All this detail is part of my story creation. The original example in BHH did 
not identify one of the additive levels as the control condition (no additive), 
but that seems to me the right thing to do in an experiment like this. While we 
want to know whether any additive enables car manufacturers to meet 
regulatory limits, we also want to know how much reduction it provides com-
pared to adding nothing. That knowledge could help define further research.)
Let’s look at the Latin square design in the contexts we have seen in previous 
chapters. First, note that the Latin square design could be called a very incom-
plete block design. In this example, we have 16 blocks, the car/driver combi-
nations, with only one experimental unit (a 150 mile prescribed drive) per 
block. It’s going to be difficult (impossible) to measure the variability among 
experimental units or variability among treatment differences within a block in 
this situation (as with the boys’ shoes). All of our information about additive 
differences comes from interblock comparisons; there is no intrablock 
information.
In the Latin square, each additive is assigned to four blocks but, systemati-
cally, not at random. That is why the Latin square design is not a special case 
of the randomized block design. The assignment is constrained to achieve the 
balance discussed earlier. Randomization enters the design as described 
­previously in this section by randomly assigning the A, B, C, and D labels to the 
Table 7.2  Results of Car Emission Latin Square Experiment.a
Driver
Car
A
B
C
D
I
A1
CL
A2
A3
19
24
23
26
II
A2
A3
A1
CL
23
24
19
30
III
CL
A2
A3
A1
15
14
16
16
IV
A3
A1
CL
A2
19
18
19
16
a Emissions as a function of car, driver, and additive.
Source: Reproduced from BHH (2005, Table 4.8, p. 157), with permission 
of John Wiley & Sons.

Fundamentals of Statistical Experimental Design and Analysis
212
four cars in the experiment and by similarly randomly assigning the labels for 
drivers and additives. Also, when order might be a concern, as here, the 16 
runs should be done in a random order. If they don’t have a schedule laid out, 
and a test manager to assure the schedule is followed, test personnel might be 
tempted to run a more convenient order. For example, all four drivers might 
do their A1‐additive runs on Monday, their A2‐additive runs on Tuesday, etc. 
That reduces the chance of putting the wrong additive in a car on any given 
day. But, it’s possible that there could be a learning curve or a boredom 
trend at work in this test, so a finding of apparent differences among additives 
could be really the effect of learning or boredom in repeatedly driving the  
150 mile course.
Second, note that the Latin square is a fractional factorial arrangement of 
blocks and treatments. The emissions experiment has three four‐level factors 
(two block factors, one treatment factor). One replication of the full set of fac-
torial combinations (the alternative design that was rejected as too expensive) 
would have 4 × 4 × 4 = 64 runs. The 4 × 4 Latin square design specifies a 
particular 1/4 fraction of those 64 runs.
Analysis 1: Plot the data
I have repeatedly said that initial data plots should show all the dimensions of 
the data, if possible. The fractional nature of this experiment makes it impos-
sible to produce a meaningful data display showing all four dimensions of the 
Latin square design: cars, drivers, additives, and emissions. For example, if you 
plot emissions versus additive, by car, the four points for a given car differ not 
only by additive but also by driver, so you cannot graphically isolate the effects 
of car, driver, and additive. This is the graphical manifestation of the fact, as will 
be seen, that by the fractional nature of this design, one cannot evaluate inter-
actions in the ANOVA.
Under the assumption of no interaction, we can (and in this case, have to) 
go straight to main‐effect plots, which are given in Figure 7.1. Each point in 
Figure 7.1 is the average for a given factor level, averaged over the four runs 
in the experiment that were done at that factor level. For example, the four 
runs, for additive A1, included all four cars and all four drivers as specified in 
the Latin square design table. All three plots have the same vertical axis for 
ease of comparison.
Figure 7.1 shows, by the greater spread among the four drivers, that drivers 
apparently have more of an effect on car emissions than do cars (which might be 
an indication that emissions were measured on a per‐gallon‐consumed basis, 
not a per‐mile‐driven basis) or additives. Additive was a qualitative treatment 
factor, as far as we know, so the apparent linear trend for additives in the figure 
is not meaningful. It is just happenstance in the random assignment of labels to 
additives. Now, if we found out that A1–A3 were three decreasing concentration 
levels of one additive (and CL was zero additive), then additive is in fact a 

213
Other Experimental Designs
quantitative factor, and we would want to plot the emission averages versus 
concentration to see if increasing concentrations resulted in reduced emissions. 
What the additive main‐plot display shows is that all three additives resulted in 
lower emissions than the control treatment of no additive. That’s an encouraging 
sign. We will soon see if that apparent difference is “real or random.”
First, though, if there are no, or only minor, differences among cars, then it 
makes sense to plot the data in a way that ignores cars. One of the nice fea-
tures of the Latin square (and other well‐chosen fractional factorials) is that if 
one of the three factors in the experiment turns out to have a negligible effect, 
the design collapses to a balanced two‐factor design. By ignoring cars, we’re 
left with a 4 × 4 arrangement of drivers (blocks) and additives (treatments) in 
what is a randomized block design with one replication of each treatment in 
each block. Thus, we display the data in an interaction plot. Figure 7.2 pro-
vides that interaction plot of emissions versus driver.
Figure 7.2 shows an intriguing pattern. The lower‐left panel shows that for 
drivers I and II (the red and black plotting symbols), there was a substantial 
difference in emissions for the four additives, while for drivers III and IV (green 
and blue), there was not. This looks like a classic case of interaction, but with 
no replication, we have to treat these inconsistencies as random variation. 
Further experimentation is necessary to resolve the issue of real versus random 
interaction. Based on these data, I would like to have an off‐the‐record chat 
with these drivers to see if they deviated from the assigned drive in any way. If 
D
C
B
A
25.0
22.5
20.0
17.5
15.0
IV
III
II
I
CL
A3
A2
A1
25.0
22.5
20.0
17.5
15.0
Car
Mean
Driver
Additive
Main-effects plot for emissions data
Figure 7.1  Average Emissions by Car, Driver, and Additive.

Fundamentals of Statistical Experimental Design and Analysis
214
a driver drove over the speed limit and then took a half‐hour break midway 
through the run for lunch or other refreshment, that might affect his car’s 
­emissions. Not accusin,’ just sayin.’
From the graphical evidence, we might choose A1 as the winning additive: 
it had the lowest emissions for two drivers and not so bad for the other two 
drivers. On the other hand, if we could get everybody to drive like drivers III 
and IV (the green and blue symbols), we might be able to get away with using 
no additive. Obviously, though, we need a lot more data before making any 
such decisions that have nationwide ramifications. Some possible follow‐on 
experiments are discussed in a later section.
ANOVA
The ANOVA for this Latin square experiment can separate out the variation 
associated with the main effects of car, driver, and additive. No interactions 
can be evaluated. The MSs for all three ANOVA entries are calculated from 
the variances of the four means in each of the panels in Figure 7.1. The 
ANOVA for the emissions data (Table 7.3) shows that only the differences 
among drivers stand out appreciably above the residual error variability. 
This result is consistent with the visual impression in Figure 7.1. The afore-
mentioned data plots have shown us the inconsistencies that contribute to 
this error variability.
IV
III
II
I
30
25
20
15
CL
A3
A2
A1
30
25
20
15
Additive
Driver
A1
A2
A3
CL
Additive
I
II
III
IV
Driver
Interaction plot for emissions vs. additive and driver
Figure 7.2  Interaction Plot of Emissions Data by Additive and Driver.

215
Other Experimental Designs
Just as we simplified plots of the data by ignoring cars, we can simplify the 
ANOVA by dropping the car source of variability (which means merging these 
three df and corresponding SS with the error line in the ANOVA). (Mathematically, 
we’re taking the car effect out of the statistical model underlying the analysis.) 
The result, in Table 7.4, doesn’t change our conclusions: substantial differences 
among drivers, some evidence of differences on average among additives, 
but no way to test for interaction. We need a bigger and better experiment 
to  decide if we need better additives or better drivers in order to reduce 
­automobile emissions.
Discussion
This example illustrates a larger truth: findings in a tightly controlled labo-
ratory experiment may not carry over to a much noisier environment, espe-
cially one involving people who have a myriad of unpredictable ways to use 
and abuse the scientist’s or engineer’s creations. Variability happens! 
Anybody who makes or sells consumer products knows this. We saw this 
phenomenon manifested in the case study in Chapter 1 and in the textile 
production example in Chapter 6. Machines were consistent; human oper-
ators were not (because of human nature, not deliberate actions). One 
thing though, the results of this experiment validated the research direc-
tor’s aversion to running the experiment with one driver only. It also vali-
dated his recognition that the additives needed to be evaluated in realistic 
driving situations, not just in the lab. His company would like eventually to 
Table 7.3  ANOVA for Emissions Experiment.
Source
DF
SS
MS
F
P
Car
3
  24
8.0
1.5
.31
Driver
3
216
72.0
13.5
.004
Additive
3
  40
13.3
2.5
.16
Error
6
  32
5.3
Total
15
312
Table 7.4  Reduced ANOVA of Emissions Data.
Source
DF
SS
MS
F
P
Driver
3
216
72.0
11.6
.002
Additive
3
40
13.3
2.14
.17
Error
9
56
6.2
Total
15
312

Fundamentals of Statistical Experimental Design and Analysis
216
market their additive to millions of drivers. Betting the company’s bottom line 
on only lab data, or on one driver’s data, is not an acceptable risk. Inadequate 
market research has torpedoed more than one product. Remember New 
Coke? Remember the Edsel? (Well, probably not.)
Follow‐on experiments
There are various ways to extend this emissions experiment. These include:
1.  Repeat the exact same Latin square experiment: same drivers, cars, and 
additives, same test and measurement protocols. This would provide a 
direct measure of repeatability—variability.
2.  Run the same Latin square again but with four different drivers.
3.  Run a different Latin square, by rerandomizing the assignments of factor 
levels, with the same drivers, cars, and additives. Choose the second Latin 
square so that the combined set of 32 runs will help one separate out 
some of the interactions. (The combinatorial problems that can be worked 
in this situation are beyond the scope of this text.)
4.  Run a different Latin square with four new drivers, same four cars and additives.
5.  Run the 48 runs needed to complete the full 43 combinations of drivers, cars, 
and additives. The design can be analyzed as a complete three‐way 
classification. Note, though, that this two‐stage design (and randomizations) 
is not the same as a RBD with 16 blocks, four treatments assigned randomly 
to four experimental units in each block. The analysis would need to reflect 
the staging of the experiment which, in effect, is an additional blocking factor.
Here, the additive manufacturer is considering augmenting the first Latin 
square with another Latin square to help resolve some of the questions raised 
by the first experiment. The lapse in time between the two experiments could 
introduce some new sources of variation. The drivers might say, “You mean 
you want me to take four more drives over the same course? Boring.” (The test 
director might want an observer to ride with each driver to assure protocol 
is  followed and to collect ancillary data—for example, traffic conditions.) 
In hindsight, it might have been good to consider the alternative of a replicated 
Latin square design (total of 32 runs) which would fall between the 16‐run Latin 
square and the 64‐run randomized block.
Some of these replicated Latin square designs are used in the context of 
“repeated measures” designs discussed later in this chapter. In these designs, 
individual experimental units are measured repeatedly. They may also have 
different treatments applied to them sequentially.
Exercise
Write out the ANOVA tables, Source, and df, for alternative designs 1, 2, and 
5 in the above list.

217
Other Experimental Designs
Extensions
The emission example had two blocking factors and one treatment factor. 
Other experimental situations may have one blocking factor and two treatment 
factors. This turns the design into an incomplete block design. For the 4 × 4 Latin 
square, we would have four blocks of four experimental units and 16 treat-
ments. The Latin square layout would define which four treatment combina-
tions are assigned to each block, and those four combinations would be 
randomly assigned to the four experimental units (runs, in this case) in a block.
Latin squares of any size can be constructed. Cochran and Cox (1957) cat-
alog some designs up to 12 × 12. Not all physical situations lend themselves to 
having three factors with the same number of levels. However, there are some 
tricks to pull. For example, in the emissions situation, another candidate design 
could have been a 5 × 5 Latin square with five cars and five drivers. The treat-
ments could have been the three additives and two control runs.
In the basic Latin square design, the three factors are generically called rows, 
columns, and treatments. It is possible that any of these three generic factors 
could be factorial combinations of other factors. For example, a 6 × 6 Latin 
square might be run in which the six treatments were the six combinations of 
a three‐level and a two‐level factor. Then, the 5 df for treatments in the ANOVA 
could be separated into, say, factor F1 with 2 df, factor F2 with 1 df, and 
F1 × F2 interaction with 2 df.
Another extension is to add a fourth factor. If the Latin square is of at least 
dimension 3 × 3, this can be done in a balanced way. This four‐factor design is 
called—are you ready?—a Graeco‐Latin square. Table 7.5 gives a 4 × 4 Graeco‐
Latin square design.
The alphanumeric characters in the table indicate the combinations of 
treatment factors to be run in each row/column combination. For example, the 
upper left entry of A1 means that, say, when driver 1 uses car 1, the treatment 
combination will be factor1 at the A level and factor2 at the “1” level. Note 
that each number occurs once in each row and column and once with each 
letter. This balance means that in the ANOVA, we can separate the SS into 
rows, columns, Factor1, and Factor2, each with three df. This leaves 3 df for 
error. Under the (strong) assumption of no interactions among the blocking 
and treatment factors, this experiment provides clean estimates of the effects 
of all four factors.
Table 7.5  4 × 4 Graeco‐Latin Design.
Row
Column
A1
B3
C4
D2
B2
A4
D3
C1
C3
D1
A2
B4
D4
C2
B1
A3

Fundamentals of Statistical Experimental Design and Analysis
218
In fractional factorial terminology, the 4 × 4 Graeco‐Latin square is a 1/16th 
fraction of a 44 set of factor combinations.
In the Table 7.5 design, the second treatment factor could be the order of 
the runs. In the preceding, the Latin square experiment was done by random-
izing the order each driver did his four runs. Alternatively, we could balance 
the run orders using this Graeco‐Latin square. Thus, driver 1’s first run would 
be car (column) 1 using additive A. Her second run would be car 4 using 
additive D, then car 2 with B, and then car 3 with C.
Latin square and Graeco‐Latin square designs offer a chance to evaluate the 
effects of three or four factors with a minimum of runs. They yield clean estimates 
of the effects of these factors only if there are no interactions among any of them, 
as is the case with fractional factorial arrangements in any situation. However, as 
is stated in BHH (2005, p. 160), to use a Latin square design to study process 
factors known to interact is an “abuse” of the design. Subject‐matter knowledge 
is required to avoid this abuse and the resulting misleading conclusions.
Split‐Unit Designs
Consider again the commercial‐scale tomato fertilizer experiments discussed 
in Chapter 3. Suppose it was decided to run the experiment with 300 plants 
treated with Fertilizer A and 300 plants treated with Fertilizer C. Rather than 
randomly assign fertilizers to individual plants, suppose it was decided that the 
experimental unit would be a plot with 30 plants (perhaps a 6 × 5 grid, perhaps 
a row). Fertilizers would be applied simultaneously to these groups of contig-
uous plants, which is much more convenient than applying fertilizer one plant 
at a time. Now, the experiment would have a total of 20 experimental units 
(consisting of 30 contiguous plants), and each fertilizer would be randomly 
assigned to 10 of these eus.
Suppose that the experimenter decides that the amount of fertilizer is 
another important factor. If the experiment is done with just one level of fertil-
izer and we see a difference between Fertilizers A and C, could it be that the 
same difference would occur if we had used either a higher or lower amount 
of fertilizer? Also, will applying more fertilizer lead to more tomatoes? If so, will 
increased yield offset the increased expense? I don’t want to wait until the next 
crop and run experiments at a different level of fertilizer. Can we vary the 
amount of fertilizer in the current design? In particular, could we consider three 
levels of fertilizer, say, low, medium, and high? Inquiring (scientific) minds want 
to know. The tomato mogul asks his FLS, “How can we include fertilizer amount 
in the experiment?”
The experiment now has six treatments: the six combinations of two fertil-
izers each at three levels. The FLS comes up with two experimental designs.
Let’s suppose we start over and redefine the experimental unit structure as 
groups of 10 plants. Our layout would then have 60 eus. With each fertilizer at 

219
Other Experimental Designs
three amounts, this makes a total of six treatments. We would then randomly 
assign each of these six fertilizer/amount combinations to 10 eus each. This 
would be a completely randomized experiment with six treatments in a 2 × 3 
structure with 10 replicates (of groups of 10 plants) of each.
On the other hand, suppose we split each eu of 30 plants that have already 
been randomly assigned a fertilizer into three subunits of 10 contiguous plants 
each. Within each experimental unit, then, we would randomly assign the 
three fertilizer levels to one subunit each. This way, we could measure the 
effect of fertilizer level within each experimental unit (contiguous group of 30 
plants). The variability among the three subunits within an experimental unit 
should be less than the variability among the experimental units over the 
whole field. Thus, we ought to be able to estimate the effect of fertilizer level 
more precisely than with the completely randomized experiment.
In this second experimental design, the design at the subunit level is a 
randomized block design. Each “main‐plot” unit (30 plants) is a block of three 
“subplot” units, randomly assigned the three levels of the fertilizer assigned to 
the main plot. We started with a completely randomized design for the fertil-
izer factor and then, in essence, embedded a randomized block design for the 
level factor. This gives us an experiment with two sizes of experimental units 
and with two levels of randomization. Figure 7.3 illustrates the experimental 
structure and randomization. It shows a subset of the main units and their fer-
tilizer assignment; then within each main unit, the three subunits are shown 
with their randomly assigned levels of fertilizer.
This hybrid design is called a split‐unit or split‐plot design. The latter term 
reflects the design’s origins in agricultural experiments such as the tomato 
fertilizer experiment, where plots of land are split into subplots. Split unit is a 
more general term: one experimental unit is split into subunits to which 
subsequent treatments are applied. Manufacturing processes often involve a 
number of sequential steps, and such situations make split‐unit experimental 
designs feasible and practical. At selected steps, the material being pro-
cessed can be split into subbatches for the application of factors involved in 
the next step.
Split‐unit experiments can be difficult to recognize. A spreadsheet of data 
can look like any of a variety of multifactor experiments. It can take a lot of 
detective work to find out what factors, if any, are blocking factors and which 
are treatment factors and, critically, what were the experimental units for the 
application of the treatment factors. You have to understand the experimental 
protocol before you can make sense of the data and extract any message 
hidden in that cloud (see front cover). The following example is a case of 
aggregated units, rather than split units. One treatment factor is applied to 
individual units; then subgroups of units are aggregated for the application of 
the second treatment factor. So, the experiment has treatment factors that are 
applied to different sizes of experimental units via aggregation, rather than 
splitting. Recognizing this is key to running the correct ANOVA.

Fundamentals of Statistical Experimental Design and Analysis
220
Example: Corrosion Resistance
I again turn to the classic Box, Hunter, and Hunter 
(2005) text for an example. As always, the following 
story is my own embellishment.
A team of chemists in a chemical products 
company is charged with developing coating mate-
rials that will prevent corrosion on structures such as 
buildings and bridges made of steel. Protection 
from corrosion is important to assure structural 
integrity and to minimize the amount of inspection 
and maintenance that is required.
Coating is applied to steel parts by spraying the coating on a part and then 
curing (baking) the part at a specified temperature for a specified time. At this 
stage of their investigation, the chemists have selected four coating materials to 
compare. They have settled on what the curing time should be, but they want 
their experiment to include curing temperature as an experimental variable to 
be investigated. The reason for this objective is that an ideal coating material 
should provide good corrosion prevention over a range of curing temperatures. 
This is an important characteristic because steel companies and other customers 
for their coatings will not necessarily be able to control curing temperature very 
precisely and consistently. A coating that is effective when cured at 370° (F), say, 
but ineffective if cured at 360° or 380° is not a good product, particularly if the 
C
L
H
M
C
H
M
L
A
L
H
M
……..……..
A
M
H
L
Total of 20 main units each 
divided into three subunits
Figure 7.3  Schematic Illustrating Split‐Unit Design for Tomato Fertilizer Experiment. 
The design has 20 main‐plot units, with each fertilizer randomly assigned to 10 units. 
Each main‐plot unit is divided into three subplots, and the three fertilizer levels are ran-
domly assigned to one subplot unit each.

221
Other Experimental Designs
user’s furnace cannot control temperature that precisely. The terminology 
­sometimes used is to say that such a coating is not “robust.” Robust products 
mean better structures, happier customers, and more sales. That’s the ultimate 
goal. More discussion of robust designs is given later in this chapter.
Developing and improving chemical products is a statistics‐rich environ-
ment. Efficient experimentation is essential to success. Thus, large, successful 
chemical companies have in‐house departments of friendly, local statisticians. 
Smaller companies often hire university or private statistical consultants, which 
is the case here.
Two chemists meet with a statistician from the local university. They describe 
the coating and curing processes and tell the statistician that they have 24 steel 
bars to use in the experiment. They say that after a bar is coated and cured, it 
is subjected to a corrosive environment, and then corrosion resistance is mea-
sured by standard techniques.
After some discussion, it is decided to experiment at three temperatures, 
360, 370, and 380°F, for reasons discussed earlier. The statistician suggests 
that they run all 12 treatment combinations, four materials at three tempera-
tures, on two bars each. The treatment combinations will be randomly assigned 
to bars and run in a random order. Thus, the design would be a completely 
randomized experiment with 12 treatments applied independently to two ran-
domly selected experimental units each. The FLS even goes so far as to lay out 
the random treatment assignment and a random order of experimentation. He 
also stresses the importance of true replication: between each run, the coating 
and curing processes must be shut down and restarted in order to really have 
multiple independent applications of treatments to experimental units.
The chemists frown. “It takes a lot of time to shut down the temperature 
chamber and restart it. Besides, our chamber will hold more than a single bar. 
If we cure several bars at a time, we’ll save a lot of time and expense.” They 
suggest putting two bars with each coating in the chamber for a given temper-
ature setting. Thus, they could do eight bars at 360°F, say, as one “heat,” and 
another eight at 370°F and the remaining eight at 380 F. Coatings would be 
randomly assigned to bars within each heat group. That way, the experiment 
could be done in three heats, rather than 24. “Neat, huh?”
“I’m sorry,” says the FLS (searching for a polite way to say that this is a bad 
idea). “With only one heat at each of the three temperatures, your experiment 
won’t have any replication of the curing‐temperature treatment. We won’t be 
able to estimate the inherent variability of the process and won’t be able to 
tell whether any apparent differences among temperatures are real or 
random.”
“I have an idea, though,” he says. “How about if we do heats of four bars—
one with each coating material. This will mean a total of six heats, two at each 
temperature, thus (minimally) replicating the temperature treatment. We 
should randomly assign temperatures to groups of four bars and randomly 
order these six heats. We also should randomize the oven positions for the 
four bars in each heat. How about this as a workable compromise?”

Fundamentals of Statistical Experimental Design and Analysis
222
“Well, OK,” say the chemists. “But could we run the heats in this order: 360, 370, 
380, 380, 370, 360? Ramping up and then ramping back down will save us time.”
The FLS is not too happy with this plan, but he doesn’t want to push too 
hard. He asks, “In your experience is there any carry‐over effect, or hysteresis, 
when you do this?” “No, don’t think so,” respond the chemists. So, that is the 
way the experiment is run.
Note that this experiment has treatments applied to two different experi-
mental units. It is not a completely randomized design. Coating materials are 
assigned to single bars with six bars being randomly assigned to each of the 
four coatings. Thus, for the coating treatment, the design is a CRD, and a 
single bar is the experimental unit that gets the randomly assigned coating 
material. However, the curing‐temperature treatment is applied to groups of 
four bars; thus, the group of four bars is the experimental unit for the temper-
ature treatment. We therefore have a split‐unit design, in reverse. We got this 
experimental structure by aggregating subunit experimental units (the coated 
bars), rather than splitting main units, as in the aforementioned agricultural 
example. The result is the same, though: different experimental units for differ-
ent treatment factors in the same experiment. Our analysis will have to reflect 
that experimental structure.
Analysis 1: Plot the data
Figure 7.4 gives a plot of the corrosion‐resistance measurements versus the order 
of the heats, by coating material. The temperatures are also shown, ordered in 
the sequence in which they were done, as discussed. All 24 data points are shown 
in Figure 7.4, and each point is identified by its coating, curing temperature, and 
heat order. Thus, all dimensions of the data are captured in the plot.
Figure 7.4 exhibits some erratic behavior. The two 360°F heats, heats 1 and 6, 
differ substantially, as do the pairs of heats at 370 and 380°F. There is little 
difference between heats 1 and 2, run at 360 and 370°, respectively, while on the 
ramp down, there is a large difference between these two temperatures in heats 
Table 7.6  Corrosion‐Resistance Experiment Data.
Temp. (°F)
Heat
Coating
1
2
3
4
360
1
  67
  73
  83
  89
6
  33
    8
  46
  54
370
2
  65
  91
  87
  86
5
140
142
121
150
380
3
155
127
147
212
4
108
100
  90
153
Reproduced from BHH (2005, Table 9.1, p. 336), with permission of John Wiley and Sons.

223
Other Experimental Designs
5 and 6. The data look like the chemists might not have been able to control the 
heat chamber temperature closely enough to be sure that they were getting 
reliable results. For example, consider heats 4 and 5. The lower temperature 
(370) yielded better resistance than the higher temperature (380) (subject‐matter 
knowledge says that this should not happen). For heats 2 and 3, at the same pair 
of treatments, the order is reversed. If there is a temperature control problem in 
the experiment, this is embarrassing because the chemists picked the tempera-
tures because of their concern that their customers may not be able to control 
temperature closely enough to get consistently good corrosion resistance. 
Or, maybe some data were mislabeled. The pattern would make more sense 
if heats 4 and 5 data were interchanged. It’s all a little disturbing, but, unfortu-
nately, we cannot go to the source and pursue these questions.
The picture with respect to coatings looks more informative. Coating 4 is 
fairly consistently the best—the highest or near‐highest corrosion resistance in 
each heat, especially the two 380°F heats.
To show the relationship of corrosion resistance to temperature, Figure 7.5 
gives scatter plots of those two variables for each coating separately. The data 
points are labeled by the heat number to show how the bars are grouped in heats.
Figure 7.5 shows that there is some improvement of corrosion resistance as 
temperature is increased, but the pattern is different for the four coatings. This 
evidence of coating by temperature interaction is more clearly seen in an inter-
action plot (Fig. 7.6).
6
5
4
3
2
1
200
150
100
50
0
Heat
Mean
C1
C2
C3
C4
Coating
Interaction plot for corr-res
Data means
360
370
380
380
370
360
Figure 7.4  Corrosion Resistance Plotted as a Function of Heat, Curing Temperature, 
and Coating.

Fundamentals of Statistical Experimental Design and Analysis
224
380
370
360
200
175
150
125
100
75
50
Temp.
Mean
C1
C2
C3
C4
Coating
Interaction plot for corr-res
Data means
Figure 7.6  Interaction Plot of Corrosion Resistance Averages versus Temperature by 
Coating.
380
375
370
365
360
200
150
100
50
0
380
375
370
365
360
200
150
100
50
0
C1
Temp.
Corr-res
C2
C3
C4
C1
C2
C3
C4
Coating
6
5
4
3
2
1
6
5
4
3
2
1
6
5
4
3
2
1
6
5
4
3
2
1
Scatter plot of corr-res vs. temp.
Panel variable: Coating
Figure 7.5  Plot of Corrosion Resistance versus Curing Temperature by Coating.

225
Other Experimental Designs
In Figure 7.6, each plotting point is the average corrosion resistance over 
the two heats run at the selected temperature. This figure shows that Coating 
4 has markedly better corrosion resistance when cured at 380°F than do the 
other three coatings which have corrosion resistances that level off between 
370 and 380°F.
ANOVA
Now, let’s construct the ANOVA for this experiment. At the main‐unit level, the 
experimental unit is a “heat,” which is a group of four bars, all simultaneously 
cured at one temperature. There are six such main units; two for each of the 
three temperatures. Thus, at this level, the experiment is a completely 
randomized design with three treatments and two replicates for each. This 
part of the ANOVA therefore has the following structure.
Main‐Unit ANOVA.
Source
df
Temperature
2
Error1
3
The ANOVA entry labeled Error1 is pure main‐unit experimental error: it is 
the variability between the two heats at each temperature, pooled across the 
three temperatures. I use the label, Error1, to distinguish this level of experi-
mental variability from the subunit variability: next.
The subunit in this experiment is a single bar, and there are four in each 
main‐unit group of bars (a heat). Each of the bars in each heat was randomly 
assigned one of the four coating materials. Thus, for the subunit part of the 
experiment, the design is a randomized block design with six blocks (the heats) 
of four experimental units (individual bars), and as is the case for a randomized 
block design, each of these subunits is randomly assigned one of the four coat-
ing materials. Each coating is applied to one bar in each “block.” Thus, the 
subunit ANOVA has the structure of a randomized block design with six blocks, 
four treatments, and only one replicate of each treatment in a block, as follows.
Subunit ANOVA.
Source
df
Heats
  5
Coatings
  3
Error2
15
There is overlap between these two ANOVAs. The 5 df for heats in the subunit 
ANOVA are the same 5 df in the main‐unit ANOVA, separated there into 

Fundamentals of Statistical Experimental Design and Analysis
226
temperature (2 df) and heats within temperatures (3 df), which is Error1. Error in 
the subunit ANOVA is the heat by coating interaction—the variability of coating 
differences from heat to heat. That error needs to be further separated.
The error in the subunit ANOVA can be further resolved because of the 
structure of the heats: three temperatures, two main units for each. That is, 
the six heats were not nominally identical runs as was the case in Chapter 6 
for the batches of material used in the penicillin production experiment. Part 
of the heat by coating interaction is actually the temperature by coating inter-
action, with 6 df (of error’s 15 df ). The remaining 9 df consists of heat by coat-
ing interaction (3 df ) within each temperature. (In each temperature, there are 
two heats crossed with four coatings. Thus, there are (2 − 1) × (4 − 1) = 3 df for 
interaction.) Pooling these interactions across the three temperatures makes 
up the 9 df. This entry is labeled Error2, the subunit residual error. The full 
ANOVA has the structure shown in Table 7.7.
The full ANOVA for the corrosion‐resistance data is given in Table 7.8. Some 
statistical software is capable of separating the main unit and subunit ANOVAs, 
particularly the two error terms. Note the difference in the two error mean 
squares. Alternatively, one can run a one‐way ANOVA for the temperature and 
heats within temperature and a two‐way ANOVA on temperature and coating 
to get the entries in the Table 7.8 ANOVA. Error1 is the variability among heats 
(groups of four bars). Error2 is the variability among bars within the same heat.
Table 7.7  ANOVA Structure for Corrosion‐Resistance Experiment.
Source
df
Main unit
Temp
  2
Error1
  3
‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐
‐‐‐‐‐‐
‐‐‐‐‐‐‐‐‐‐‐‐
Coating
  3
Subunit
Temp × Coating
  6
Error2
  9
Total
23
Table 7.8  ANOVA for Corrosion‐Resistance Split‐Unit Experiment.
Source
DF
SS
MS
F
P
Temp
2
26 519
13 260
  2.8
.21
Error1
3
14 440
4813
38.7
.00
‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐------------------------------------------------------------------------------------------------
Coating
3
4289
1430
11.5
.002
Temp*coating
6
3270
545
  4.4
.024
Error2
9
1121
125
Total
23
49 639

227
Other Experimental Designs
Often, it is the case that main‐unit variability is larger than subunit variability. 
The two error MSs for these data have this relationship: Error1 MS is about 40 
times the Error2 MS. This reflects the patterns seen in Figure 7.7. The connected 
lines for each coating are roughly parallel (small interaction), while the two 
heats within each temperature differ substantially.
Interpretation of the ANOVA starts with the F‐ratio and P‐value for the 
­temperature by coating interaction. The small P‐value reflects the pattern 
in Figure 7.6 in which the Coating 4 increase in corrosion resistance at 380°F 
differs from the other three coatings. The ANOVA confirms (as it should) the 
visual impression in Figure 7.6. If maximizing corrosion resistance is the 
objective, subsequent analyses such as confidence and prediction intervals 
would focus on Coating 4 and its temperature dependence. The producer of 
Coating 4 would instruct users to cure the coating at 380°F. However, this 
experiment started with the objective of finding a coating that delivered ade-
quate corrosion resistance across the whole temperature range. Such “robust-
ness” would enable a customer to produce corrosion‐resistant steel even if 
they were not able to control temperature accurately within the 360–380°F 
range considered in the experiment. To address this issue, we need a defini-
tion of what minimum corrosion resistance is considered necessary. I won’t 
make up that part of the story.
My data‐snooping habits would lead me to look deeper into why there is so 
much variability between the two heats run at each temperature. Could the 
assigned temperatures have been missed or the data mislabeled? For example, 
if heats 4 and 5 were reversed, the pattern in Figure 7.4 would be more like 
what I would expect, at least for the 370–380–380–370 middle four heats. The 
dismal performance of heat 6 at 360°F for all four coatings makes me wonder 
if the actual temperature was 360°F or if the curing time was not run as pre-
scribed. Sometimes experiments raise more questions than they resolve. That’s 
why the sainted Sir Ronald A. Fisher (1938) stated,
To consult the statistician after an experiment is finished is often merely to ask 
him to conduct a post mortem examination. He can perhaps say what the 
experiment died of.
The post mortem continues—there are two other aspects of this experiment 
that might be at least partially responsible for the unusual variability seen in 
the experiment:
1.  The four coatings are actually defined by two other factors, B, the base 
treatment, and F, the finish, each at two levels.
2.  The four bars were randomly assigned to four positions in the furnace (see 
Table 9.1 in BHH) (2005, p. 336).
The two‐factor structure of the coatings means that the three df for coating in 
the ANOVA can be separated into the ANOVA entries of B, F, and B × F, each 

Fundamentals of Statistical Experimental Design and Analysis
228
with 1 df. When this is done (it is shown in BHH), the interaction is quite 
significant, meaning that the four coatings can be treated as four distinct coat-
ings, as I have done. There are no consistent B or F effects to explain the coat-
ing differences.
Additionally, the T × C interaction in the ANOVA can be separated into B × C, 
F × C, and B × F × C, each with 2 df. In this case, only the B × F × C interaction is 
significant. High‐order interactions are a sign of erratic patterns in the data.
The random, and hence unbalanced, assignment of bars to positions could also 
be a contributor to the variation in the data. I will leave it to the intrepid instructor 
or student to investigate this aspect of the data. The position and coating assign-
ments are in Table 9.1a on p. 336 of BHH (2005). Another potential assignment is 
to redesign the experiment so that a possible position effect could be evaluated 
and balanced so that the coating effects could be more cleanly estimated.
Discussion
As stated prior to this example, it is often difficult to recognize split‐unit 
designs after the fact. You certainly cannot detect splitting and grouping from 
a data table. Those who plan and conduct the experiment and know where 
experimental units were split or merged along the way may not appreciate the 
effect these actions have on the subsequent statistical analysis. When the data 
are dropped on the FLS’s desk, the full story may not be told. I have spent a lot 
of time asking experimenters polite questions, trying to reconstruct the scene 
of the crime, so to speak. Presented with the data organized as in Table 7.6, 
I would ask what this heat variable is and perhaps soon find out that the four 
bars in each heat were cured simultaneously, while coatings were applied to 
individual bars. If heat had not been shown in the table, I would probably note 
that there were consistent differences between the groups of four bars in each 
temperature and wonder if the experimenters had just arranged the data that 
way or if these differences reflected something about how the experiment was 
conducted.
Now, let us suppose (erroneously!) that the FLS asked to analyze the data 
looked at the table of data and said, “That looks like a completely randomized 
design with 12 treatments, consisting of combinations of four coatings and 
three temperatures and two experimental units in each treatment combination” 
(kind of like the poison/antidote experiment in Chapter  5). Therefore, the 
ANOVA is that of a two‐way classification with two replications. Then the 
ANOVA would be that as shown in Table 7.9.
This ANOVA merges the whole‐unit error and subunit error that were prop-
erly separated in the split‐unit ANOVA in Table 7.8. The whole‐unit oranges 
are mixed with the sub-unit grapes into a bad‐tasting fruit salad. Taking the 
analysis at face value, one would conclude that there is no temperature 
by  coating interaction (i.e., the coating differences are consistent across 
­temperatures—relative to the experimental error—and no overall differences 

229
Other Experimental Designs
among coatings). wrong! Temperature, as we have seen in our careful and 
correct analysis, has a strong effect on corrosion resistance, particularly for 
Coating Material 4. The conclusions drawn from the incorrect analysis are 
just the opposite (!) of the conclusions drawn from the correct split‐unit 
ANOVA: no difference among temperatures and coating differences that are 
not consistent across temperature. (Jones and Nachtsheim (2009) also con-
trast the correct and incorrect analyses of this experiment and provide much 
more information on split‐plot or split‐unit designs.)
The morals of this story:
1.  Design matters.
2.  I’ll say it again: you can’t tell how an experiment was conducted just by 
looking at a data table.
Split‐unit designs can be used in settings other than agricultural or industrial. 
For example, education researchers might evaluate teaching materials and 
testing methods as follows: first, some number of classes (defined, e.g., by 
location, grade level, and teacher) would be selected for the experiment. These 
might be chosen to be reasonably homogeneous, or they might be selected to 
span some range of demographic characteristics, depending on context and 
issues that the experimenters want to consider. The different teaching materials 
under consideration would be assigned to classes, either completely at random 
or randomly within demographic or geographic blocks. At the end of the 
instructional period, each class would be randomly divided into subgroups of 
students, and then the testing methods would be randomly assigned to these 
subgroups. (This assignment should be controlled by the experimenters. If test‐
method assignment is left to individual teachers, some bias could creep in—
well‐meaning teacher: “I think Susie will do best if she is tested by Method B.”)
Thus, in this experiment, for the treatment factor of teaching material, the 
experimental unit is a class. For the treatment factor of testing method, the 
experimental unit is a subgroup within a class. Conceptually, this is the same as 
the tomato grower’s experiment with fertilizer types and amounts discussed 
earlier in this section. (“The nice thing about statistics is that the nouns may 
Table 7.9  Incorrect ANOVA of Corrosion‐Resistance Experiment: Split‐Unit Design 
Structure Not Recognized.
Source
DF
SS
MS
F
P
Temp
2
6519
3260
10.2
.003
Coating
3
4289
1430
1.10
.39
Interaction
6
3270
545
.42
.85
Error
12
15 561
1297
Total
23
49 639

Fundamentals of Statistical Experimental Design and Analysis
230
change, but the verbs stay the same,” said Prof. Carl Marshall ( Oklahoma 
State University, ca. 1965).)
Additionally, in this experiment, the researchers are apt to record many 
demographic variables related to schools, teachers, and students, and such 
data may help explain whatever differences are found in the effect of teaching 
materials and testing methods. We may find out that Susie would do best 
under Method C.
Repeated Measures Designs
There are a variety of experimental situations in which the experimental units 
in an experiment are measured repeatedly over the course of the experiment. 
Some examples are as follows.
In a marketing experiment that compares three product displays and their 
effects on sales, suppose that 12 stores have been selected for the experiment. 
The experimental protocol would be to put up one display for a 2‐month 
period, and monthly sales would be recorded. Then another display would be 
put up for another 2‐month test period, and then the third display would put 
up for its 2‐month test period. Thus, each experimental unit (store) is sub-
jected sequentially to each of three treatments (product displays). It is entirely 
possible that the order of presentation could be important, so the display 
orders could be balanced. There are six ways to order the three displays. Two 
stores each would be randomly assigned to each order.
As another example, consider a missile component that is required to 
operate successfully under a variety of environmental conditions, such as low 
temperature, high temperature, vibration, and mechanical shock. Each com-
ponent in the experiment would be exposed to all of these environments in 
an assigned order, and performance characteristics would be measured dur-
ing or after each environment. Subject‐matter context could specify a 
particular order or the experiment could be designed, as in the marketing 
experiment, so that all possible orders are tested. Another possible extension 
is that each component would be exposed to multiple sequences of 
environments.
In medical experiments, subjects are often treated and measured repeatedly 
over time to evaluate and compare different medical interventions and also to 
see if there was a time trend in the responses.
Experimental designs for situations in which experimental units are mea-
sured repeatedly under various protocols for assigning treatments before 
or during the experimental period are called repeated measures designs. 
As we will see, the data layout and analysis can resemble randomized block 
or split‐unit designs. However, they differ in that “time” is not a treatment 
that can be randomly assigned in a repeated measures design. If a subject’s 
­cholesterol is measured monthly for 12 months, “month” is not a treatment 
factor that can  be randomly assigned. Time marches on in order, not 

231
Other Experimental Designs
randomly. Because randomization protocols define an experimental design, 
not just data table layout, repeated measures experiments are not “the 
same as” the randomized block or split‐unit families of designs. The exper-
imental units in a repeated measures experiment could either be a single 
group of eus or groups of eus in various blocks. Rather than a single response 
measurement on each eu, a suite of response measurements are made on 
each eu.
Example: Effects of drugs on heart rate
Authors George Milliken and Dallas Johnson (2009) in their book with the 
­conflicted title, Analysis of Messy Data: 1. Designed Experiments (I say “con-
flicted” because in my view experiments are designed so as not to lead to 
“messy data”; however, missing data or botched protocols can result in data 
that do not have the balance and structure of the intended design—hence 
messy data), ­provide an example of a repeated measures experiment that is 
not messy.
Twenty‐four female human subjects (not another experiment on rats, you 
may be relieved to know) have been selected for an experiment aimed at 
evaluating the effect of two prescription drugs on heart rate. After a drug is 
administered, heart rate is measured four 
times at 5 min intervals. The purpose, I 
conjecture, could be twofold. An elevated 
heart rate could be a potential undesirable 
side effect that the experimenters want to 
find and evaluate. On the other hand, if the 
objective of the drugs is to increase a sub-
ject’s heart rate, then questions of interest 
would be the rapidity with which the heart 
rate becomes elevated and the length of 
time that it stays at an elevated level.
The treatment assignment was done by randomly dividing the group of sub-
jects into three groups of eight people: one to receive Drug A, one to receive 
Drug B, and the third group to receive a placebo (labeled Drug C, for control).
If I had been involved in planning this experiment, I would have recom-
mended that each subject’s heart rate be measured perhaps four times at 
5 min intervals before the drug is administered and then afterward, as planned. 
This way, each subject would be its own control so that drug effects could be 
measured within subjects and evaluated against within‐subject experimental 
error (à la the boys’ shoes experiment). This experiment, though, was done 
with a control group of subjects, so in the experiment under consideration, 
the drug effects will have to be evaluated against among‐subject variability.
The data for this experiment are given in Table 7.10.

Fundamentals of Statistical Experimental Design and Analysis
232
Analysis 1: Plot the data
The data are shown in Figure 7.7, which is a connected scatter plot of pulse 
rate versus time, by drug and subject. (The data points are linked by subject. 
Subject is included in the plots only because if there were any outliers in the 
data, this would help identify the particular subject.)
Figure 7.7 indicates that Drug A subjects had a rising and falling pulse over 
the 20 min test period, while Drug B is elevated (compared to the control 
group, C) over the whole period. The mostly consistent rise and fall of pulse 
rate for all eight subjects over the four tests, though, seems a little unusual. 
The placebo, Drug C, as would be expected of a proper placebo, shows a gen-
erally flat pattern and at a lower average pulse rate than Drug B. These differ-
ences can be better seen in the interaction plot for drugs and time (averaged 
over subjects) in Figure 7.8
Table 7.10  Heart Rate Data.a
ç
Drug
t1
t2
t3
t4
  1
A
72
86
81
77
  2
A
78
83
88
81
  3
A
71
82
81
75
  4
A
72
83
83
69
  5
A
66
79
77
66
  6
A
74
83
84
77
  7
A
62
73
78
70
  8
A
69
75
76
70
  9
B
85
86
83
80
10
B
82
86
80
84
11
B
71
78
70
75
12
B
83
88
79
81
13
B
86
85
76
76
14
B
85
82
83
80
15
B
79
83
80
81
16
B
83
84
78
81
17
C
69
73
72
74
18
C
66
62
67
73
19
C
84
90
88
87
20
C
80
81
77
72
21
C
72
72
69
70
22
C
65
62
65
61
23
C
75
69
69
68
24
C
71
70
65
63
a Table entries are the heart rates at 5 min intervals for each subject after taking the indicated drug.
Source: Reproduced from Milliken and Johnson (2009, Table 26.4, p. 506), with permission of 
Chapman and Hall/CRC Press.

233
Other Experimental Designs
Figure 7.8 shows considerable interaction: the average pulse rate versus 
time patterns are markedly different for the three drugs.
The ANOVA structure for this experiment is the same as for the corrosion‐
resistance split‐unit experiment (under certain simplifying assumptions 
20
15
10
5
84
82
80
78
76
74
72
70
Time
Mean
A
B
C
Drug
Plot of average pulse-rate vs. time, by drug
Figure 7.8  Average Pulse Rate versus Time by Drug.
20
15
10
5
90
80
70
60
20
15
10
5
90
80
70
60
A
Time
Pulse-rate
B
C
10
11
12
13
14
15
16
17
18
19
1
20
21
22
23
24
2
3
4
5
6
7
8
9
Subject
Panel variable: Drug
Plot of pulse-rate vs. time, by subject and drug
Figure 7.7  Plot of Pulse Rate versus Time (min) by Subject and Drug.

Fundamentals of Statistical Experimental Design and Analysis
234
about the variation across time periods within subjects). At the subject 
level, the design of this experiment is a completely randomized design with 
three treatments, each applied to eight experimental units. Within each 
drug, there is a two‐way classification of the responses, subjects (8) by times 
(4). The ANOVA will have two error terms: among subjects (Error1) and 
within subjects (Error2). Table 7.11 gives the ANOVA and confirms what our 
eye sees. The significant drug by time interaction confirms the Figure 7.8 
visual impression that the drug effects over time are not at all consistent 
over the three drugs; they are not just manifestations of the random 
­variability among subjects.
If the purpose of the drugs was to elevate pulse rate, the interaction plot 
shows that both drugs were successful but Drug B achieves that increase ear-
lier and longer than does Drug A.
Discussion
Some of the experimental situations in the previous chapters could have been 
addressed with a repeated measures design. For example, consider the market 
research objective in Chapter 4. The objective was to compare the effects of 
three store displays on shampoo sales. A completely randomized design was 
used in which 15 stores were selected for the experiment, and each display 
was randomly assigned to five stores. An alternative way to design the 
experiment would be to have each of the 15 stores run the three displays suc-
cessively with a protocol that specifies an appropriate time lag between dis-
plays. Because order could have an effect, the six different orders could be 
assigned in a balanced way: three of the orders could be assigned to two 
stores each, and three of the orders could be assigned to three stores each. 
Or, the experiment might have been done with either 12 or 18 total stores to 
keep things balanced.
In the present chapter, the Latin square experiment involving cars, drivers, 
and additives can be thought of as a repeated measures design. Each driver 
was subjected to a sequence of car/additive combinations. Learning or 
boredom effects were a concern in this experiment.
Table 7.11  ANOVA for Drug/Pulse Rate Experiment.
Source
df
SS
MS
F
P
Drug
2
1333
666.5
6.0
.009
Error1
21
2337
111.3
15.0
.000
Time
3
290
96.5
13.0
.000
Drug × time
6
527
87.9
11.8
.000
Error2
63
469
7.5
Total
95
4957

235
Other Experimental Designs
Extensions
An extension of the drug/heart rate experiment would be to repeat the 
experiment with the same subjects but assign each group to another drug. 
This could be done again so that every subject is tested against all three drugs. 
The time between each test (and its four pulse rate measurements) would be 
long enough to dissipate the effects of the previous drug. This is a crossover 
design (Cochran and Cox 1957; Ohlert 2000). This class of designs is often 
used in agricultural experiments. For example, plots of land in a tomato field 
that got Fertilizer A this year would get Fertilizer C next year, and vice versa.
Robust Designs
Introduction
The concept of robustness and the role of designed experiments were men-
tioned in the coating material experiment earlier in this chapter: the goal was to 
implement a curing process that is insensitive to curing‐temperature variations 
within a range that a coating purchaser is able to control to in its use of the 
material. The coating would have good corrosion resistance as long as the cur-
ing temperature is kept within the specified limits. Demonstrating or achieving 
robustness is also an objective in several examples in the preceding chapters. 
Producers of crop seed or fertilizers want to be able to demonstrate that their 
product is effective in a variety of growing conditions, so a randomized block 
experiment, where blocks span anticipated growing conditions, provides a 
means of evaluating product robustness with respect to growing conditions. 
Producers of a fuel additive would like the benefit of that additive (reduced 
emissions) to be robust to driving conditions, so tests, defined by a Latin square 
design, were conducted using multiple cars and drivers to evaluate different 
additives and their robustness of emission control to variation of cars and 
drivers. As I have tried to stress, experiments don’t end with an ANOVA and its 
P‐values. It is what is learned and how that learning can be used to improve 
products, processes, and programs that is the real payoff. Experiments that are 
aimed at achieving robustness are an important category of experiments. This 
section deals with a particular approach to designing experiments aimed at 
demonstrating robustness or finding sources of nonrobustness.
Variance transmission
Robustness can be expressed in terms of designing a system or process in 
order to minimize the variability of its output. Figure 7.9 illustrates this graph-
ically for the design of a pendulum to be used, say, in a grandfather’s clock. 
There is a mathematical, physics‐based relationship between the period of a 

Fundamentals of Statistical Experimental Design and Analysis
236
pendulum (the time for one swing) and the pendulum’s length. The curved line 
in Figure 7.9 depicts that relationship; note that the curve is plotted on a semi-
log scale. Pendulums can vary in length, just due to manufacturing variability, 
as represented by the probability distributions along the horizontal axis. The 
variability of lengths is transmitted, mathematically into variability of the pen-
dulum’s period, as shown along the vertical axis. Gears in the clock translate 
the pendulum’s motion to movement of the clock hands that tell us what time 
it is (I assume that readers have seen clocks with hands). A manufacturer will 
have spec limits on clock accuracy, so high variability of the pendulum’s period 
among clocks, due to variation in the pendulum lengths, would mean that 
more clocks would fail the specs and have to have their pendulums replaced.
To minimize variability of the pendulum’s period, the message from Figure 7.9 
is that the pendulum should be as long as possible. Size matters. (Can I say 
that?) Note that the transmitted variation at x = 10 cm is considerably less than 
it is at x = 2 cm. That’s (in part) why tall grandfather clocks cost more than short 
ones or table clocks.
In this example, the mathematical relationship between system performance 
(y) and one particular design variable (x) is known from physics. In more gen-
eral and more complex situations, the relationship between system performance 
and system design is not known. As an example, in the case study in Chapter 1, 
the relationship between the locations at which a wire bond was pulled to 
measure pull strength was not known. Designed experiments, though, were 
conducted, in essence, to estimate that relationship and then use the findings 
to better control the testing process.
In the 1980s, the concept of robust design received a lot of attention, thanks 
to the work of Genichi Taguchi (1991). He recommended a variety of designed 
experiments and data analyses that would lead to the design and manufacture 
.6
.4
.2
2
Period, y (s)
(log scale)
4
6
Length, X (cm)
8
10
Figure 7.9  The Transmission of Variation in the Length of a Pendulum to Variability of 
the Pendulum’s Period. 
Source: Reproduced from BHH (2005, p. 551), with the permission of John Wiley & Sons.

237
Other Experimental Designs
of robust systems. Taguchi’s methods would find the design parameters that 
would minimize the variability of system output in a variety of situations. 
Conferences were held by statisticians to understand and critique these 
statistical methods.
One example I remember hearing at the time is the following: a manufac-
turer of ceramic pots was experiencing a high rate of breakage (sound 
familiar?). Investigation led to the finding that the oven in which the pots were 
baked had a very uneven temperature distribution (sound familiar?). The solu-
tion proposed was to buy a new and better oven that would provide a more 
uniform temperature throughout. This would be a very expensive solution. 
However, process engineers, through experimentation with the ceramic design 
parameters (perhaps guided by Professor Taguchi, I don’t remember), found 
that by increasing the amount of ash in the clay, the pots became less temper-
ature sensitive (more robust to temperature differences), so less breakage 
occurred. Ash is cheap, so this was a much less expensive solution. A celebra-
tion ensued and bonuses were paid (I’m making that part up).
Taguchi experimental designs aimed at determining robust processes or 
products have the following structure. First, known or potential factors (or var-
iables) that affect a product’s performance need to be identified. These factors 
fall in two categories:
1.  Control factors. These are characteristics of the product, such as dimen-
sions and materials, that the designer has control of and that will ultimately 
define the product.
2.  Noise factors. These are possible influences on product performance 
outside of the designer’s control, such as environmental conditions or 
uncontrolled and unavoidable deviations in product characteristics such as 
the variation of pendulum lengths in Figure 7.9.
Then, a Taguchi experimental design is as follows: first, choose an “inner 
array” of design factors. These arrays are generally factorial or fractional facto-
rial combinations of two‐ or three‐level design factors.
Next, an “outer array” of noise factors is determined—again, usually two‐ or 
three‐level factorial or fractional factorial combinations of the noise factors.
Then, the complete set of experimental conditions is obtained by running 
the outer array of noise factors at each combination of design factors in the 
inner array.
Figure 7.10 illustrates this compounding of inner and outer arrays graphically.
In Figure 7.10, there are a total of 9 × 8 (=72) treatment combinations in the 
experiment. Depending on how the experiment might be conducted, there 
might be blocking, replication, or unit splitting in the conduct of the experiment. 
Statistical analysis would need to reflect these aspects of the experiment.
Now, how should the data be analyzed to determine the most robust set of 
design factors? This is where Taguchi methodology departs from convention 
by calculating “signal‐to‐noise” summary statistics at each design combination 

Fundamentals of Statistical Experimental Design and Analysis
238
and then picking the combination of control factors that maximizes signal to 
noise. Further experimentation might be done to refine the choice.
For the experimental array in Figure 7.10, the signal‐to‐noise analysis might 
be as follows:
1.  Calculate ybar and s, the mean and standard deviation of the eight 
responses at each of the nine combinations of design factors.
2.  Calculate the relative standard deviation, s/ybar, at each of the nine set-
tings of design factors. This is a particular noise‐to‐signal ratio.
3.  Choose the combination with the smallest relative standard deviation. 
Possibly conduct further experimentation and analysis to refine and con-
firm the choice of design factor levels.
To illustrate an alternative to the Taguchi signal‐to‐noise approach, I need to 
commit a little bit of mathematics.
Mathematical model: Robustness
Process output, y, is a function of design factors and noise factors, expressed 
as follows:
y
f x w
,
,
where x is a set of control factors and w is a set of noise factors; w can be con-
trolled in an experiment, not in use.
The nature of f(x,w) determines how variability of w will be transmitted into 
variability of y, which means variability of product performance, which means 
quality and customer satisfaction, good or bad. Thus, the experimental design 
and subsequent data analysis will be aimed at estimating this relationship. 
That estimate will then provide a means of finding the x settings that are most 
robust to variation in w. Here is a simple illustrative example.
Inner array
Outer array
C
f
Figure 7.10  An Inner 3 × 3 Array of Two Design Factors and an Outer 23 Array of 
Three Noise Factors. 
Source: Reproduced from BHH (2005, p. 553), with the permission of John Wiley & Sons.

239
Other Experimental Designs
Consider a situation in which there are two control factors, x1 and x2, and 
one noise factor, w. Suppose the statistical relationship between a response 
variable, y, and these three factors is:
y
a
bx
cw
dx w
e
a
bx
c
dx w
e
1
2
1
2
(
)
,
where e is the effect of additional noise factors, not controlled in the experiment. 
In use, both e and w are random variables.
Note that the cross‐product term in this model, dx2w, means that the effect 
of x2 on y is amplified by w. There is interaction between these two factors, in 
the statistical sense: the effect of x2 on y is different for different values of w.
By using the properties of sums of random variables and the product of a 
constant and a random variable, under the aforementioned model, the vari-
ance of y is
var
var
var
( )
(
)
(
)
( ).
y
c
dx
w
e
2
2
Thus, to minimize the variance of y, x2 needs to be chosen to minimize 
(c + dx2)2. If feasible, the optimum choice, as a little algebra shows, is x2 = −c/d, 
in which case w has no effect on the variation of the process output. If this 
solution for x2 is not feasible, the optimal choice of x2 would be the feasible 
value of x2 that minimizes |c + dx2|. The experiment and data analysis would 
provide estimates of c and d from which the most robust x2 setting would be 
determined.
What about x1? Its effect on y is not affected by w, so x1 can be dialed up or 
down so that the performance characteristic will meet its target value and not 
transmit additional variation. In our pendulum example, once the pendulum 
length has been specified, gear ratios can be adjusted appropriately so that 
the clock keeps accurate time.
Robust design is sometimes said to “capitalize on interaction.” This simple 
example illustrates how that can be done.
Concluding comments
There are many contexts in which robustness is an important design goal. 
Questionnaires needed to be worded clearly, so that confusion and misinterpre-
tation do not cloud the results. Products need to operate reliably in a range of 
operating conditions. Online systems for signing up for health insurance need to 
be capable of coping with variations in demand and user needs and capabilities, 
etc. Designed experiments and appropriate data analysis and communication of 
results help achieve robust products and processes. Taguchi and subsequent 
researchers and practitioners in the United States (see, e.g., Phadke 1989; Wu 
and Hamada 2000; Wikipedia (2014a)) encouraged a heightened focus on prod-
uct and process robustness that needs to be widely recognized.

Fundamentals of Statistical Experimental Design and Analysis
240
Optimal Designs
Introduction
For the most part, the experiments used to illustrate the design families in this 
book have come to us fully formed with respect to choices of treatment and 
blocking factors, the number and levels of these factors, the combinations of 
factors that are included in the experiment, the number of replications, and the 
responses and ancillary variables that were measured. I have indicated that 
­subject‐matter knowledge and issues, along with statistical and economic con­
siderations and even personalities, must interact to make these key design 
decisions in order to have some assurance before the experiment that the 
experiment has the capability of answering pertinent questions. For example, in 
the ethanol experiment in Chapter 5, we took it on faith that the experimenters 
had identified two important and pertinent x‐variables that might influence CO 
emissions from automobiles. We did have some knowledge from the literature 
that the ranges of the x‐variables were reasonable. We do not know, however, 
why the experimenters chose to run two replications of a 3 × 3 set of treatment 
(x‐variable) combinations. It could have been cost; it could have been that 
previous, related work had found that a second‐order polynomial at least 
roughly described the relationship between the x‐variables and CO emissions. 
It could be that they saw this design in another textbook.
There are of course many other suites of treatment designs that could have been 
run. For example, a response surface design with a 2 × 2 factorial set of points, plus 
four radial points, plus a center point would again make for a total of nine treatment 
combinations. The levels that define the 2 × 2 points and the star points would be 
other design options. The nine selected treatment combinations could all have 
been replicated twice for 18 runs. Or, a 4 × 4 set of points, with only one replication, 
plus two center points, would have been another way to distribute 18 treatment 
design points around the selected x1 − x2 region. And, of course, there is probably 
no strong reason to make exactly 18 runs, so the possibilities are endless.
For the most part, in this book, we have considered experimental designs 
with rectangular experimental regions—multifactorial structures. Nature and 
theory are not always this neat and tidy. There will be constraints that preclude 
the designs, both treatment designs and experimental designs, that we have 
discussed and illustrated. Active research in this area is aimed at finding optimal 
designs that incorporate specific constraints and objectives in determining an 
experimental design. This approach is better than trying to force a basic design 
into a constrained situation. Once again, statisticians have risen to the occasion 
of finding designs that better fit real‐world situations.
Finding “optimal experimental designs”
Statistical and computational tools exist for evaluating candidate designs. These 
start with various measures of the precision of estimates and predictions that 
would be obtained from the experiment. Estimates and predictions generally 

241
Other Experimental Designs
depend on the nature of the function ultimately fitted to the experiment’s data—
the model. But we don’t necessarily know that function, you say. That’s why 
we’re running the experiment, for goodness’ sakes! Relax. For planning pur-
poses, we always make some assumptions. This is directly analogous to the 
sample size analyses we did in earlier chapters in which we made assumptions, 
for example, about the error variance and the Normal distribution in order to 
determine how many observations we needed to achieve a desired confidence 
interval width or to meet power curve targets.
In the rat food example in Chapter 4, the scientists expected the relationship 
between growth rate and supplement amount to be linear. Under that assump-
tion, the optimum design would be half of the runs at each end of the selected 
range of the x‐variable. Prudence, though (she was the FLS involved in the study), 
led to a design with several intermediate x‐values. We can, and should, in gen-
eral, vary the assumptions and see how the design changes in response. Also, it 
should be pointed out that basic designs, such as a randomized block design 
with multifactor treatments, are also linked to particular statistical models.
Suppose, for planning purposes, that we assume that a second‐order polyno-
mial is a good approximation to the relationship in nature, or the laboratory 
experiment, between CO and x1 and x2. (“All models are wrong; some are use-
ful,” said George Box.) Under this assumption, we know the mathematical 
function of the data by which the coefficients will be estimated and predictions 
will be calculated. We know the formulas for the precision of those estimates 
and predictions, up to the value of sigma. These formulas are all a function of 
the experimental design—the location of the design points and the number and 
locations of replications. We and our computers can do those calculations for 
every candidate design and compare the results and try to balance cost and 
precision. We (or your friendly, local statistician) could repeat the whole exercise 
for another candidate family, such as a bivariate log‐linear model. Subject‐
matter knowledge and theory can identify candidate models. We might conjec-
ture, also, that for planning purposes, if we choose a design that does a good 
job of fitting the second‐order model, it might still be OK (robust enough) for 
fitting another model that we ultimately determine when we analyze the data. 
Methods for finding model‐robust designs (as opposed to model‐dependent 
designs) are an area of active research (see, e.g., Smucker, del Castillo, and 
Rosenburger 2011 and references).
This comparison of designs would be an unwieldy analysis, so statisticians 
have developed summary measures of a design’s “goodness.” These include:
1.  The average of the variances of the estimated coefficients
2.  The maximum of the variances of the estimated coefficients
3.  The maximum variance of predictions in a specified region of explanatory 
variables
These variances are all multiples of the experimental error variance; the 
­multiplier is a function of the array of design points in the design.

Fundamentals of Statistical Experimental Design and Analysis
242
Instead of evaluating a possibly large number of candidate designs, we can 
(let the computer) work the problem in reverse. We can give the computer a 
large number of possible design points, such as a fine grid of points in the 
x1 − x2 region of the ethanol experiment and then tell the computer, “I want to 
fit a quadratic model using only 18 points in this region. Find me the 18 points 
that minimize the maximum variance of predictions in the x1 − x2 region (crite-
rion 3).” And, the computer will do it (see, e.g., JMP, jmp.com). We can repeat 
the process for n = 24 and 36, say, to see how the design changes and preci-
sion improves with these larger experiments and then consider cost–benefit 
trade‐offs.
Experimental designs obtained in this way are sometimes called “computer‐
aided designs.” For a concise discussion of these designs, see the online NIST 
Engineering Statistics Handbook, Section  5.5.2 (NIST 2014). In addition to 
quantitative criteria for deciding on a model, graphical methods are also useful 
(Anderson‐Cook, Montgomery, and Myers 2009).
Design augmentation
In several examples in this book, I discussed possible further experimentation 
that might be done to follow up on the results of the experiment under discussion. 
For example, a follow‐up multifactor experiment might be conducted in an adja-
cent factor region. The present experiment would give an idea of the nature of 
the relationship between the response and the treatment factors, so one could 
use that information to specify a candidate set of additional treatment combina-
tions and find the optimal design for augmenting the current data to best enhance 
the fit of the model fitted to the initial experiment.
Some other situations in which optimal, computer‐aided experimental 
designs are valuable are:
1.  Constraints on the experimental region eliminate some points in a full or 
fractional array of block or treatment combinations.
2.  Block structures and sizes for conventional experiments may not be 
appropriate.
Computational resources, of course not available when the CRD and RCB 
design families and other designs in the preceding chapters were developed, 
provide a great opportunity for expanding the actual use of experimental 
design, which happens also to be the purpose of this book. For a case study‐
based introduction to optimal design and illustrations of the wide range of 
situations in which optimal designs can provide context‐specific experimental 
designs, see Goos and Jones (2011). This book is written as a dialogue bet-
ween the two authors, asking and answering questions that a reader might 
have. Another pertinent reference is Atkinson, Donev, and Tobias (2007). 
Wikipedia (2014b) provides an overview and additional references.

243
Other Experimental Designs
Assignment
Choose a topic of interest to you and an issue to investigate with an experiment. 
Develop three experimental designs for investigating this issue: (i) Latin square, 
(ii) split unit, and (iii) repeated measures. Discuss and compare your three alterna-
tives. Show how you would plot the data and lay out the ANOVA tables for each.
References
Anderson‐Cook, C., Montgomery, D., and Myers, R. (2009) Response Surface Methodology: 
Process and Product Optimization Using Designed Experiments, 3rd ed., John Wiley 
& Sons, New York.
Atkinson, A. C., Donev, A. N., and Tobias, R. D. (2007) Optimum Experimental Designs, 
with SAS, Oxford University Press, Oxford.
Box, G., Hunter, W. G., and Hunter, J. S. (1978, 2005) Statistics for Experimenters, John 
Wiley & Sons, Inc., New York.
Cochran, W., and Cox, G. (1957) Experimental Design, John Wiley & Sons, Inc., 
New York.
Fisher, R. A. (1938) Presidential Address to the First Indian Statistical Congress, http://
www.economics.soton.ac.uk/staff/aldrich/fisherguide/quotations.htm.
Goos, P., and Jones, B. (2011) Optimal Design of Experiments: A Case Study Approach, 
John Wiley & Sons, Ltd, Chichester.
Jones, B., and Nachtsheim, C. (2009) Split‐Plot Designs: What, Why, and How, Journal 
of Quality Technology, 41(4): 340–361.
Milliken, G., and Johnson, D. (2009) Analysis of Messy Data Volume I: Designed 
Experiments, 2nd ed., Chapman and Hall/CRC Press, Danvers, MA.
NIST‐SEMATECH (2014) Engineering Statistics Online Handbook, Section 5.5.2. http://
itl.nist.gov/div898/handbook/.
Ohlert, G. (2000) A First Course in Design and Analysis of Experiments, W. H. Freeman, 
New York.
Phadke, M. (1989) Quality Engineering Using Robust Design, Prentice‐Hall, Englewood 
Cliffs, NJ.
Smucker, B., del Castillo, E., and Rosenburger, J. (2011) Exchange Algorithms for 
Constructing Model‐Robust Designs, Journal of Quality Technology, 43, 1–15.
Taguchi, G. (1991) System of Experimental Design: Engineering Methods to Optimize 
Quality and Minimize Cost, Quality Resources, White Plains, NY.
Wikipedia (2014a) Robust Parameter Design, http://en.wikipedia.org/wiki/Robust_ 
parameter_design.
Wikipedia (2014b) Optimal Design, http://en.wikipedia.org/wiki/Optimal_design.
Wu, C. F., and Hamada, M. (2000) Experiments: Planning, Analysis, and Parameter 
Design Optimization. John Wiley & Sons, Inc., New York.


Fundamentals of Statistical Experimental Design and Analysis, First Edition. Robert G. Easterling. 
© 2015 John Wiley & Sons, Ltd. Published 2015 by John Wiley & Sons, Ltd.
analysis of variance (ANOVA), 95–102, 
111–2, 181–3
degrees of freedom (df), 43, 99–100
F‐ratios, 98–9
mean squares (MS), 100
sources of variation, 96, 99
sum of squares (SS), 100
ancillary variables, 20, 57, 59
block structure, 15–17
borrowing strength, 103, 135, 205
central limit theorem effect
confidence interval
binomial distribution, p, 80–82
difference between means of two 
Normal distributions, 69–71, 87–9
mean of Normal distribution, 50–54
regression line, 112–13
survival probability, 136–8
data displays
box and whisker plot, 117
contour plot, 144–5
dot plot, 61, 81–4, 163–4
individual value plot, 42, 95
interaction plot, 128–30, 142–4, 
161–2, 181–2, 213–14, 223–4, 233
line plot, 33, 202
main effect plot, 212
probability plot, 83–5, 164–6
scatter plot, 4, 31–2, 57, 109, 147, 
181, 191, 223–4
effect sparcity, 162–7
examples
battery life, 179–87, 196–7
boys’ shoe soles, 13–14, 25, 30–46, 
197–9, 203–5
corrosion resistance, 220–230
e‐mail marketing, 167–71
ethanol fuel mixes, 139–49, 154–5
gasoline additives, 208–16
gear teeth strength, 114–20, 188
growth rate of rats, 108–14, 241
heart rate medications, 231–4
penicillin production, 189–94, 
199–200
poisons and antidotes, 124–39,  
151–4
pot production, 156–67, 171–4
shampoo marketing, 92–106, 234
textile production, 201–3
tomato fertilizer, 56–79, 106–7, 187–8, 
218–20
wire bond strength, 5–9, 236
Index

Index
246
experimental designs
balanced incomplete block, 203–5
completely randomized, 91–120
crossover, 235
Graeco–Latin square, 217–18
Latin square, 208–16
one‐factor‐at‐a‐time, 168–70
optimal design, 240–242
randomized complete block, 177–95
repeated measures, 230–234
response surface, 149–150
robust design, 235–9
split‐unit, 218–30
experimental units, 11–15
interaction, block and/or treatment 
factors, 128–30, 144, 157,  
188–9, 239
nuisance factors, 25–6
prediction intervals, 105–6, 135–6
principles, experimental design
blocking, 24–6
control, 26–7
randomization, 22–4
replication, 21–2
probability distributions
binomial, 35–7, 79–81
F, 77–8, 98–100
Normal, 40–42, 81–5
Student’s t, 43–4
regression, curve‐fitting, 109–13, 145–8, 
159–63
reliability analysis, 183–4
response measurement, 19–20
Satterthwaite’s approximation, 68, 194
significance testing, 34–8, 46–8, 62
P‐value, 37–9
reference distribution, 36, 46–8
significance tests
paired sample t, 44–6
randomization, 39–40, 64–5
rank sum (Mann–Whitney), 63–4
sign, 36–8
two‐sample t, 66–9
sizing an experiment
based on confidence‐interval width, 71–2
based on power curve, 72–5
standard error
of a difference between means,  
70–71, 132, 194–5
of a mean, 45
of a regression coefficient, 110
tolerance intervals, 53, 136–8
treatment structures, 17–19
two‐level factors
fractional factorial, 167–75
full factorial, 155–67
variance components, 193–4, 201–3
variance transmission, 235–6


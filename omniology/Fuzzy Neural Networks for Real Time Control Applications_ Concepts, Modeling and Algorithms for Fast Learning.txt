FUZZY NEURAL
NETWORKS FOR
REAL TIME
CONTROL
APPLICATIONS

FUZZY NEURAL
NETWORKS FOR
REAL TIME
CONTROL
APPLICATIONS
Concepts, Modeling and
Algorithms for Fast
Learning
ERDAL KAYACAN
MOJTABA AHMADIEH KHANESAR
AMSTERDAM â€¢ BOSTON â€¢ HEIDELBERG â€¢ LONDON
NEW YORK â€¢ OXFORD â€¢ PARIS â€¢ SAN DIEGO
SAN FRANCISCO â€¢ SINGAPORE â€¢ SYDNEY â€¢ TOKYO
Butterworth-Heinemann is an imprint of Elsevier

Butterworth Heinemann is an imprint of Elsevier
The Boulevard, Langford Lane, Kidlington, Oxford OX5 1GB, UK
225 Wyman Street, Waltham, MA 02451, USA
Copyright Â© 2016 Elsevier Inc. All rights reserved.
No part of this publication may be reproduced or transmitted in any form or by any means, electronic
or mechanical, including photocopying, recording, or any information storage and retrieval system,
without permission in writing from the publisher. Details on how to seek permission, further
information about the Publisherâ€™s permissions policies and our arrangements with organizations such as
the Copyright Clearance Center and the Copyright Licensing Agency, can be found at our website:
www.elsevier.com/permissions
This book and the individual contributions contained in it are protected under copyright by the
Publisher (other than as may be noted herein).
Notices
Knowledge and best practice in this field are constantly changing. As new research and experience
broaden our understanding, changes in research methods, professional practices, or medical treatment
may become necessary.
Practitioners and researchers must always rely on their own experience and knowledge in evaluating
and using any information, methods, compounds, or experiments described herein. In using such
information or methods they should be mindful of their own safety and the safety of others, including
parties for whom they have a professional responsibility.
To the fullest extent of the law, neither the Publisher nor the authors, contributors, or editors, assume
any liability for any injury and/or damage to persons or property as a matter of products liability,
negligence or otherwise, or from any use or operation of any methods, products, instructions, or ideas
contained in the material herein.
British Library Cataloguing in Publication Data
A catalogue record for this book is available from the British Library
Library of Congress Cataloging-in-Publication Data
A catalog record for this book is available from the Library of Congress
For information on all Butterworth Heinemann publications
visit our website at http://store.elsevier.com/
ISBN: 978-0-12-802687-8
Publisher: Joe Hayton
Acquisition Editor: Sonnini Yura
Editorial Project Manager: Mariana KÃ¼hl Leme
Editorial Project Manager Intern: Ana Claudia A. Garcia
Production Manager: Kiruthika Govindaraju
Marketing Manager: Louise Springthorpe
Cover Designer: Mark Rogers

DEDICATION
To Omar KhayyÃ¡m (1048â€“1131)
who played with mathematics
not only in science but also in his poems.
v

FOREWORD
When Nilesh Karnik and I developed type-2 fuzzy logic systems (T2FLSs)
in the late 1990s we required the following fundamental design requirement [1]:
When all sources of (membership function) uncertainty disappear a T2FLS
must reduce to a type-1 (T1)FLS. The biggest problem we faced was how to
go from a type-2 fuzzy set (T2FS) to a numberâ€”the defuzzified output of
the T2FLS. Our approach was to do this in two steps [2]: (1) type reduction
(TR), in which a T2FS is projected into a T1FS, and (2) defuzzification
of that T1FS. Unfortunately, type reduction cannot be performed using
closed-form formulas; it is done using iterative algorithms, e.g., enhanced
KM Algorithms [3]. Iterative TR may not be a good thing to do in a
real-time T2FLS, especially for fuzzy logic control, because of its inherent
time delays, and not having closed-form formulas means no mathematical
analyses of the T2FLS can be performed, something that is abhorred by
those who like to do analyses.
Beginning in 2001 some courageous researchers proposed T2FLSs that
bypassed TR and went directly to a defuzzified output for the T2FLS. All
of their T2FLSs satisfied the above fundamental design requirement. I call
these researchers â€œcourageousâ€ because they had the courage to challenge
what we had done, and this is what research should be about.
Hongwei Wu and Mendel [4] were the first ones to do this when they
developed minimax uncertainty bounds (WM UBs) for the end points of the
type-reduced set. These results became the starting architecture for some of
the applications of Hani Hagras and his students [5]. Next came Nie and Tan
[6] (NT) who took the union of the fired-rule output sets and computed
the output of the resulting T2FLS as the center of gravity of the average
of its lower and upper membership functions (MFs). Biglarbegian et al. [7]
(BMM) proposed three simplified architectures each of which combined the
left and right ends of the firing intervals in different ways. In one of these
architectures the output of the T2FLS is a weighted combination of two
very simple T1FLSs, one associated with the left-end values of the firing
intervals and the other with the right-end values of the firing intervals (this
is sometimes called an m-n or BMM architecture) and, in another of these
architectures the T2FLS is assumed to be a weighted average of the average
of the lower and upper firing intervals, where the weights are the respective
consequents of TSK rules. The latter is analogous to the NT architecture
xi

xii
Foreword
when it is applied directly to the firing intervals and could be called the
BMM-NT architecture.
All of these direct defuzzification approaches bypassed type reduction;
however, the WM UB architecture was too complicated to be used in
analyses of T2FLS, whereas the NT, BMM, and BMM-NT architectures
are simple enough so that they can be used in analyses. Biglarbegian et al.
already did this in [7â€“9] for the BMM architecture. The authors of the
present book have done it for the BMM-NT architecture, and are to be
congratulated for demonstrating many kinds of rigorous analyses that can
be performed for it.
Optimizing T2MF parameters by using some training data is very
important. When Qilian Liang and I were doing this for IT2FLSs that
included type reduction, around 2000 [10], we focused on gradient-
based optimization algorithms (e.g., steepest descent). Computing partial
derivatives for such T2FLSs is fraught with difficulties [11] because the two
switch points that are associated with the type-reduced set change from
one time point to the next and the EKM Algorithms that are used to
compute those switch points require a reordering of a set of numbers in an
increasing order, after which the original ordering must be restored so that
correct derivatives are computed. During the past decade (or longer) T2
researchers have focused on all kinds of alternatives to using derivative-
based optimization algorithms, many of which are biologically inspired [12].
ACO, PSO, QPSO, and GA are some examples. Another benefit to using
such algorithms is that (in theory) they will not get trapped in local extrema,
whereas a gradient-based algorithm will.
When type reduction is not in the architecture of a T2FLS computing
partial derivatives is quite simple because there no longer are switch points
that change from one time to the next and there is no reordering of any set of
numbers. The authors of this book have recognized this and have provided
both some derivative-based and derivative-free optimization algorithms.
They are able to perform some very serious convergence/stability analysis
for all of them, for some parameters in their BMM-NT architecture. They
are to be congratulated for demonstrating that serious analyses can indeed
be performed for a T2FLS.
In summary, this book will be of great value to those who believe it is
important to simplify T2FLSs and to use modern optimization algorithms
to tune the MF parameters of such systems.
Jerry M. Mendel
University of Southern California, Los Angeles, CA
July 24, 2015

Foreword
xiii
REFERENCES
[1] J.M. Mendel, Uncertain Rule-Based Fuzzy Logic Systems: Introduction and New
Directions, Prentice-Hall, Upper Saddle River, NJ, 2001.
[2] N.N. Karnik, J.M. Mendel, Q. Liang, Type-2 fuzzy logic systems, IEEE Trans. Fuzzy
Syst. 7 (1999) 643-658.
[3] D. Wu, J.M. Mendel, Enhanced Karnik-Mendel algorithms, IEEE Trans. Fuzzy Syst.
17 (2009) 923-934.
[4] H. Wu, J.M. Mendel, Uncertainty bounds and their use in the design of interval type-2
fuzzy logic systems, IEEE Trans. Fuzzy Syst. 10 (2002) 622-639.
[5] C. Lynch, H. Hagras, V. Callaghan, Using uncertainty bounds in the design of an
embedded real-time type-2 neuro-fuzzy speed controller for marine diesel engines,
in: Proc. FUZZ-IEEE 2006, Vancouver, Canada, 2006, pp. 7217-224.
[6] M. Nie, W.W. Tan, Towards an efficient type-reduction method for interval type-2
fuzzy logic systems?, in: Proc. IEEE FUZZ Conference, Paper # FS0339, Hong Kong,
China, June 2008.
[7] M. Biglarbegian, W.W. Melek, J.M. Mendel, Stability analysis of type-2 fuzzy systems,
in: Proc. of FUZZ-IEEE 2008, Hong Kong, China, June 2008, pp. 947-953.
[8] M. Biglarbegian, W.W. Melek, J.M. Mendel, On the stability of interval type-2 TSK
fuzzy logic control systems, IEEE Trans. Syst. Man Cybernet. B Cybernet. 40 (2010)
798-818.
[9] M. Biglarbegian, W.W. Melek, J.M. Mendel, On the robustness of type-1 and interval
type-2 fuzzy logic systems in modeling, Informat. Sci. 181 (2011) 1325-1347.
[10] Q. Liang, J.M. Mendel, Interval type-2 fuzzy logic systems: theory and design, IEEE
Trans. Fuzzy Syst. 8 (2000) 535-550.
[11] J.M. Mendel, Computing derivatives in interval type-2 fuzzy logic systems, IEEE Trans.
Fuzzy Syst. 12 (2004) 84-98.
[12] O. Castillo, P. Melin, Optimization of type-2 fuzzy systems based on bio-inspired
methods: a concise review, Informat. Sci. 205 (2012) 1-19.

PREFACE
This book presents the basics of FNNs, in particular T2FNNs, for the
identification and learning control of real-time systems. In addition to
conventional parameter tuning methods, e.g., GD, SMC theory-based
learning algorithms, which are simple and have closed forms, their stability
analysis are also introduced. This book has been prepared in a way that can
be easily understood by those who are both experienced and inexperienced
in this field. Readers can benefit from the computer source codes for both
identification and control purposes that are given at the end of the book.
There are number of books in the area of FLSs and FNNs. However,
this book is more specific in several aspects. First of all, whereas so many
books focus on the theory of type-1 and type-2 FLCs, we give more details
on the parameter update algorithms of FNNs and their stability analysis.
Second, the emphasis here is on the SMC theory-based learning algorithms
for the training of FNNs, because we think these algorithms are the simplest
and most efficient methods when compared to other algorithms, e.g., the
GD algorithm. Last but not least, this book is prepared from the view of
the identification and control of real-time systems, which makes it more
practical.
The fuzzy logic principles were used to control a steam engine by
Ebraham Mamdani of University of London in 1974. It was the first
milestone for the fuzzy logic theory. The first industrial application was
a cement kiln built in Denmark in 1975. In the 1980s, Fuji Electric
applied fuzzy logic theory to the control a water purification process. As
a challenging engineering project, in 1987, the Sendai Railway system that
had automatic train operation FLCs since from 1987, not many books are
available in the market as a reference for real-time systems. This book aims
at filling this gap.
In Chapter 1, we summarize the basic mathematical preliminaries for
a better understanding of the consecutive chapters. The given materials
include the notations, definitions and related equations.
In Chapter 2, we introduce the concepts of type-1 fuzzy sets and
T1FLCs. While Boolean logic results are restricted to 0 and 1, fuzzy
logic results are between 0 and 1. In other words, fuzzy logic defines
some intermediate values between sharp evaluations like absolute true and
absolute false. That means fuzzy sets can handle concepts we commonly
xv

xvi
Preface
meet in daily life, like very old, old, young, and very young. Fuzzy logic is
more like human thinking because it is based on degrees of truth and uses
linguistic variables.
In Chapter 3, we introduce the basics of type-2 fuzzy sets, type-2 fuzzy
MFs, and T2FLCs. There are two different approaches to FLSs design:
T1FLSs and T2FLSs. The latter is proposed as an extension of the former
with the intention of being able to model the uncertainties that invariably
exist in the rule base of the system. In type-1 fuzzy sets, MFs are totally
certain, whereas in type-2 fuzzy sets MFs are themselves fuzzy. The latter
case results in the fact that the antecedents and consequents of the rules are
uncertain.
In Chapter 4, type-1 and type-2 TSK fuzzy logic models are introduced.
The two most common artificial intelligence techniques, fuzzy logic and
ANNs, can be used in the same structure simultaneously, namely FNNs.
The advantages of ANNs such as learning capability from input-output data,
generalization capability and robustness and the advantages of fuzzy logic
theory such as using expert knowledge are harmonized in FNNs. Instead of
using fuzzy sets in the consequent part (like in Mamdani models), the TSK
model uses a function of the input variables. The order of the function
determines the order of the model, e.g., zeroth-order TSK model, first-
order TSK model, etc.
In Chapter 5, we briefly discuss a multivariate optimization technique,
namely the GD algorithm, to optimize a nonlinear unconstrained problem.
In particular, the referred optimization problem is a cost function of a
FNN, either type-1 or type-2. The main features, drawbacks and stability
conditions of these algorithms are elaborated. Given an initial point, if
an algorithm tries to follow the negative of the gradient of the function
at the current point to be able to reach a local minimum, we face the
most common iterative method to optimize a nonlinear function: the GD
method.
In Chapter 6, the EKF algorithm is introduced to optimize the parame-
ters of T2FNNs. The basic version of KF is an optimal linear estimator when
the system is linear and is subject to white uncorrelated noise. However, it
is possible to use Taylor expansion to extend its applications to nonlinear
cases. Finally, the decoupled version of the EKF is also discussed, which
is computationally more efficient than EKF to tune the parameters of
T2FNNs.
In Chapter 7, in order to deal with nonlinearities, lack of modeling,
several uncertainties and noise in both identification and control problems,

Preface
xvii
SMC theory-based learning algorithms are designed to tune both the
premise and consequent parts of T2FNNs. Furthermore, the stability of
the learning algorithms for control and identification purposes are proved by
using appropriate Lyapunov functions. In addition to its well-known feature
of being robust, the most significant advantage of the proposed learning
algorithm for the identification case is that the algorithm has a closed form,
and thus it is easier to implement in real-time when compared to the other
existing methods.
In Chapter 8, a novel hybrid training method based on continuous
version of PSO and SMC theory-based training method for T2FNNs is
proposed. The approach uses PSO for the training of the antecedent part of
T2FNNs, which appear nonlinearly in the output of T2FNNs, and SMC
theory-based training method for the training of the parameters of their
consequent part. The use of PSO makes it possible to lessen the probability
of entrapment of the parameters in a local minima while proposing simple
adaptation laws for the parameters of the antecedent part of T2FNNs when
compared to the most popular approaches like GD, LM and EKF. The
stability of the proposed hybrid training method is proved by using an
appropriate Lyapunov function.
In Chapter 9, an attempt is made to show the effect of input noise in
the rule base numerically in a general way. There exist number of papers in
literature claiming that the performance of T2FLSs is better than their type-
1 counterparts under noisy conditions. They attempt to justify this claim by
simulation studies only for some specific systems. However, in this chapter,
such an analysis is done independent of the system to be controlled. For
such an analysis, a novel type-2 fuzzy MF (elliptic MF) is proposed. This
type-2 MF has certain values on both ends of the support and the kernel and
some uncertain values for the other values of the support. The findings of
the general analysis in this chapter and the aforementioned studies published
in literature are coherent.
In Chapter 10, the learning algorithms proposed in the previous chapters
(GD-based, SMC theory-based, EKF and hybrid PSO-based learning
algorithms) are used to identify and predict two nonlinear systems, namely
Mackey-Glass and a second-order nonlinear time-varying plant. Several
comparisons are done, and it has been shown that the proposed SMC
theory-based algorithm has faster convergence speed than the existing
methods such as the GD-based and swarm intelligence-based methods.
Moreover, the proposed learning algorithm has an explicit form, and it
is easier to implement than other existing methods. However, for offline

xviii
Preface
algorithms for which computation time is not an issue, the hybrid training
method based on PSO and SMC theory may be a preferable choice.
In Chapter 11, three real-world control problems, namely anesthesia,
magnetic rigid spacecraft and tractor-implement system, are studied by using
SMC theory-based learning algorithms for T2FNNs. For all the systems,
the FEL scheme is preferred in which a conventional controller (PD, etc.)
works in parallel with an intelligent structure (T1FNNs, T2FNN, etc.). The
proposed learning algorithms have been shown to be able to control these
real-world example problems with a satisfactory performance. Note that the
proposed control algorithms do not need a priori knowledge of the system
to be controlled.
Potential readers of this book are expected to be undergraduate and
graduate students, engineers, mathematicians and computer scientists. Not
only can this book be used as a reference source for a scientist who is
interested in FNNs and their real-time implementation but also as a course
book on FNNs or artificial intelligence in master or doctorate university
studies. We hope this book will serve its main purpose successfully.
Erdal Kayacan
(Nanyang Technological University, Singapore)
Mojtaba Ahmadieh Khanesar
(Semnan University, Iran)
June 2015

ACKNOWLEDGMENTS
We would like to acknowledge our families for their support and patience,
without whom this book would have been incomplete.
xix

LIST OF ACRONYMS/ABBREVIATIONS
ANN
artificial neural network
BP
back propagation
BIBO
bounded input bounded output
BIS
bispectral index
CPSO
continous-time particle swarm optimization
DEKF
decoupled extended Kalman filter
DCN
distortion caused by noise
EKF
extended Kalman filter
EPSC
extended prediction self-adaptive controller
FEL
feedback error learning
FOU
footprint of uncertainty
FLC
fuzzy logic controller
FLS
fuzzy logic system
FNN
fuzzy neural network
FPGA
field-programmable gate array
GD
gradient descent
HR
heart rate
HCPSO
higher-order continous-time particle swarm optimization
KF
Kalman filter
LM
Levenberg-Marquardt
LBM
lean body mass
MPC
model predictive control
MSE
mean-squared error
MF
membership function
MIMO
multiple input multiple output
MISO
multiple input single output
ODE
ordinary differential equation
PD
proportional-derivative
PID
proportional-derivative-integral
PSO
particle swarm optimization
RMSE
root mean-squared error
SISO
single input single output
xxi

xxii
List of Acronyms/Abbreviations
SNR
signal-to-noise ratio
SMC
sliding mode control
SSE
squared of the error
TSK
Takagi-Sugeno-Kang
T1FLC
type-1 fuzzy logic controller
T1FLS
type-1 fuzzy logic system
T1FNN
type-1 fuzzy neural network
T2FLC
type-2 fuzzy logic controller
T2FLS
type-2 fuzzy logic system
T2FNN
type-2 fuzzy neural network
VLSI
very-large-scale integration

CHAPTER 1
Mathematical Preliminaries
Contents
1.1
Introduction
1
1.2
Linear Matrix Algebra
1
1.3
Function
4
1.4
Stability Analysis
5
1.5
Sliding Mode Control Theory
7
1.6
Conclusion
11
References
11
Abstract
This chapter summarizes the basic mathematical preliminaries for a better understand-
ing of the consecutive chapters. The given materials include the notations, definitions
and related equations.
Keywords
Matrix, Matrix inversion, Functions, Taylor expansion, Gradient, Hessian matrix, Stability
analysis, Lyapunov function
1.1 INTRODUCTION
Design, optimization and parameter tuning of FNNs require fundamental
knowledge about matrix theory, linear algebra, function approximation,
partial derivatives, nonlinear programming, state estimation and nonlinear
stability analysis. Although this chapter summarizes a few fundamental
mathematical preliminaries that will allow the reader to follow the consec-
utive chapters easier, they are selective, and serve only as a reference for the
notations and theories used in this book. For a more detailed explanation
of the theories, the reader is encouraged to refer to other textbooks.
1.2 LINEAR MATRIX ALGEBRA
A matrix is a rectangular array of elements that are usually numbers or
functions arranged in rows and columns. Let A be a m Ã— n matrix given as
follows [1]:
Fuzzy Neural Networks for Real Time Control Applications
Copyright Â© 2016 Elsevier Inc.
http://dx.doi.org/10.1016/B978-0-12-802687-8.00001-3
All rights reserved.
1

2
Fuzzy Neural Networks for Real Time Control Applications
A =
âŽ¡
âŽ¢âŽ¢âŽ¢âŽ£
a11
a12
. . .
a1n
a21
a22
. . .
a2n
...
...
...
am1
am2
. . .
amn
âŽ¤
âŽ¥âŽ¥âŽ¥âŽ¦
(1.1)
which has m rows and n columns, and aij is the element of the matrix A in
ith row and jth column. If m = n, the matrix A is called a square matrix. If ai
is a mÃ—1 matrix, it is called a column vector. Similarly, if it is a 1Ã—n matrix,
it is called a row vector. If the sizes of the matrices A and B are n Ã— m and
m Ã— p, respectively, then their matrix multiplication is defined as follows:
AB =
 m
	
k=1
aikbkj

(1.2)
To be able to multiply two matrices, the number of the columns of the
first matrix must be equal to the number of the rows of the second matrix.
The transpose of the matrix A is represented by AT and is obtained by
rewriting all rows of the matrix as its columns and all its columns as its rows.
Among number of different properties of the transpose operator, the most
two frequently used ones are as follows:
(AT)T = A
(1.3)
(AB)T = BTAT
(1.4)
An n Ã— n matrix A is called symmetric if AT = A, and it is called skew-
symmetric if AT = âˆ’A. Any matrix can be rewritten as a summation of its
symmetric and skew-symmetric matrices as follows:
A = 1
2(A + AT) + 1
2(A âˆ’AT)
(1.5)
where 1
2(A + AT) is the symmetric and 1
2(A âˆ’AT) is the skew-symmetric
part.
If the elements aij = 0, âˆ€i Ì¸= j for a n Ã— n matrix A, then the matrix A
is called a diagonal matrix and can be represented by:
A = diag(a11, a22, . . . , ann)
(1.6)

Mathematical Preliminaries
3
Furthermore, if aii = 1, i = 1, . . . , n, the matrix is called an identity matrix
and is represented by In. The identity matrix has the following property:
AAâˆ’1 = I,
âˆ€A âˆˆRn
(1.7)
where Aâˆ’1 is called the inverse of the matrix A. However, if the matrix A
is not a square matrix, then a unique matrix Aâ€  is called the pseudo-inverse
of the matrix A provided that it satisfies the following conditions:
1. AAâ€ A = A
2. Aâ€ AAâ€  = Aâ€ 
3. (AAâ€ )T = AAâ€ 
4. (Aâ€ A)T = Aâ€ A
If the matrix A is square and non-singular, then the pseudo-inverse of
A is equal to its inverse, i.e., Aâ€  = Aâˆ’1.
There exist different lemmas for the inversion of a matrix, one of which
is as follows:
Lemma 1.1 (Matrix Inversion Lemma [2]). Let A, C, and Câˆ’1 +
DAâˆ’1B be nonsingular square matrices. Then A + BCD is invertible, and
(A + BCD)âˆ’1 = Aâˆ’1 âˆ’Aâˆ’1B(Câˆ’1 + DAâˆ’1B)âˆ’1DAâˆ’1
Proof. The following can be obtained by using direct multiplication:
(A + BCD) Ã—

Aâˆ’1 âˆ’Aâˆ’1B(Câˆ’1 + DAâˆ’1B)âˆ’1DAâˆ’1
= I + BCDAâˆ’1 âˆ’B(Câˆ’1 + DAâˆ’1B)âˆ’1DAâˆ’1
âˆ’BCDAâˆ’1B(Câˆ’1 + DAâˆ’1B)âˆ’1DAâˆ’1
= I + BCDAâˆ’1âˆ’BC(Câˆ’1+DAâˆ’1B)(Câˆ’1+DAâˆ’1B)âˆ’1DAâˆ’1
= I
Let A âˆˆRnÃ—n be a square matrix. The scalar Î»i is called the eigenvalue
of the matrix A if it satisfies the following equation [3]:
Avi = Î»ivi
(1.8)
and vi is called its corresponding eigenvectors.

4
Fuzzy Neural Networks for Real Time Control Applications
The condition number of a matrix A is defined as:
cond(A) =

Î»max(A)
Î»min(A)

(1.9)
where Î»max(A) and Î»min(A) represent the largest and the smallest eigen-
values of the matrix A, respectively. Apparently, the condition number
is greater than 1. A large value for the condition number indicates that
the matrix A is ill-conditioned and may suffer from numerical instability
specially when it comes to computing its inverse.
1.3 FUNCTION
A real-valued function F(x) may be defined in a vector form as follows:
F(x) = [F1, F2, . . . , Fm], F : Rn â†’Rm
(1.10)
where Fi, i = 1, . . . , m the elements of the matrix F are real-valued
functions of real numbers. If F(x) is a scalar matrix, its gradient is defined
as follows:
âˆ‡xF(x) = âˆ‚F
âˆ‚x =
 âˆ‚F
âˆ‚x1
, âˆ‚F
âˆ‚x2
, . . . , âˆ‚F
âˆ‚xn
T
(1.11)
The gradient of the function F(x) is a column vector.
The Hessian matrix of the function F(x) is defined as follows [3]:
H(x) = âˆ‡2F(x) = âˆ‚
âˆ‚x
âˆ‚F(x)
âˆ‚x
T
(1.12)
and is calculated as:
H(x) =
âŽ¡
âŽ¢âŽ¢âŽ¢âŽ¢âŽ¢âŽ£
âˆ‚2F
âˆ‚x2
1
âˆ‚2F
âˆ‚x1âˆ‚x2
. . .
âˆ‚2F
âˆ‚x1âˆ‚xn
âˆ‚2F
âˆ‚x2âˆ‚x1
âˆ‚2F
âˆ‚x2
2
. . .
âˆ‚2F
âˆ‚x2âˆ‚xn
...
...
...
...
âˆ‚2F
âˆ‚xnâˆ‚x1
âˆ‚2F
âˆ‚xnâˆ‚x2
. . .
âˆ‚2F
âˆ‚x2n
âŽ¤
âŽ¥âŽ¥âŽ¥âŽ¥âŽ¥âŽ¦
(1.13)

Mathematical Preliminaries
5
If F(x) is a vector function as:
F(x) = [F1(x), F2(x), . . . , Fm(x)] , Rn â†’Rm
(1.14)
then its Jacobian matrix is defined as follows:
J(x) = âˆ‚F(x)
âˆ‚x
=
âŽ¡
âŽ¢âŽ¢âŽ¢âŽ¢âŽ¢âŽ£
âˆ‚F1
âˆ‚x1
âˆ‚F1
âˆ‚x2
. . .
âˆ‚F1
âˆ‚xn
âˆ‚F2
âˆ‚x1
âˆ‚F2
âˆ‚x2
. . .
âˆ‚F2
âˆ‚xn
...
...
...
...
âˆ‚Fm
âˆ‚x1
âˆ‚Fm
âˆ‚x2
. . .
âˆ‚Fm
âˆ‚xn
âŽ¤
âŽ¥âŽ¥âŽ¥âŽ¥âŽ¥âŽ¦
(1.15)
Let g(.) : Rr â†’Rm and h(.) : Rn â†’Rr be two functions and F(.) is
defined as follows:
F(x) = g(h(x))
(1.16)
then we have:
âˆ‡F(x) = âˆ‡hg(h(x))âˆ‡xh(x)
(1.17)
Let F(x) be a real-valued and differentiable scalar function of the input
vector x âˆˆRn. The Taylor expansion of F(x) about the point x0 is:
F(x) = F(x0) +
n
	
i=1
âˆ‚F(x)
âˆ‚xi
xi + 1
2
n
	
i=1
n
	
j=1
âˆ‚2F(x)
âˆ‚xiâˆ‚xj
xixj + H.O.T.
(1.18)
where xi = xi âˆ’x0i is the deviation of the variable x from x0 in its ith
dimension and H.O.T. represents the higher-order terms.
1.4 STABILITY ANALYSIS
In order to prove the stability of a nonlinear ordinary differential equation,
the Lyapunov stability theory is the most powerful and applicable method.
In order to investigate the stability of a system using this method, a positive
energy-like function of the states on the trajectory of the differential
equation is taken into account, which is called the Lyapunov function. A
Lyapunov function decreases along the trajectory of the ODE.
Let the ordinary differential equation of the system be of the following
form:
Ë™x = F(x)
(1.19)

6
Fuzzy Neural Networks for Real Time Control Applications
where F(x) : Rn â†’Rn and x âˆˆRn be the state vector of the system. The
following theorem holds:
Theorem 1.1 (Stability of Continuous Time Systems [4]). Let x = 0 be
an equilibrium point and D âˆˆRn be a domain containing x = 0. Let V : D â†’R
be a continually differentiable function such that:
V(0) = 0, and V(x) > 0 in D âˆ’{0}
(1.20)
and
Ë™V â‰¤0 in D
(1.21)
then x = 0 is stable. Moreover, if:
Ë™V < 0 in D âˆ’{0}
(1.22)
then x = 0 is asymptotically stable.
It is also possible to investigate the stability of a discrete time difference
equation using Lyapunov theory. Let the discrete time difference equation
of a system be of the following form:
xk+1 = F(xk)
(1.23)
where F(x) : Rn â†’Rn and x âˆˆRn be the state vector of the system. The
following theorem holds:
Theorem 1.2 (Stability of Discrete Time Systems [5]). Let x = 0 be an
equilibrium point and D âˆˆRn be a domain containing x = 0. Let V : D â†’R
be a continually differentiable function such that
V(0) = 0, and V(x) > 0 in D âˆ’{0}
(1.24)
then x = 0 is stable. Moreover, if:
V = V(xk+1) âˆ’V(xk) < 0 in D âˆ’{0}
(1.25)
then x = 0 is asymptotically stable.

Mathematical Preliminaries
7
1.5 SLIDING MODE CONTROL THEORY
SMC is a robust control design technique that can successfully be applied to
linear and nonlinear dynamic systems in which the desired behavior of the
system is defined as a sliding manifold. This control method has two modes:
reaching (hitting) mode and sliding mode [4]. Reaching mode occurs when
the states of the system are not on the sliding manifold. In this mode, the
controller must be designed such that it drives the states of the system to the
sliding manifold. When the states of the system reach the sliding manifold,
the controller must maintain the states of the system on it. This mode is
called sliding mode.
For a better understanding of SMC, the following example is given:
Consider a second-order nonlinear dynamic system as follows:
Ë™x1 = x2
Ë™x2 = f (x) + u
(1.26)
where f (x) : R2 â†’R is a nonlinear function of the states of the system. It
is assumed that f (x) is partially known and has the following form:
f (x) = fn(x) + f (x)
(1.27)
where fn(x) is the nominal part of f (x) that is considered to be known,
and f (x) is the unknown part of f (x), which satisfies |f (x)| < F. The
duty of the sliding mode controller is to design a control law to constrain
the motion of the system to the manifold (in this case, a sliding line as in
Fig. 1.1) s = Î»x1 + x2 = 0. On this manifold, the motion of the system
is governed by Ë™x1 = âˆ’Î»x1, which is the desired behavior of the system.
The choice of Î» > 0 guarantees that x(t) tends to zero as t tends to infinity.
The rate of convergence is controlled by the choice of Î». When the states
of the system are on the manifold, the controlled trajectories of the system
are independent of the dynamic of the system f (x). For the time derivative
of s, the following equation is obtained:
Ë™s = Î»Ë™x1 + Ë™x2 = Î»x2 + f (x) + u
(1.28)
A Lyapunov function can be defined as follows:
V = 1
2s2
(1.29)

8
Fuzzy Neural Networks for Real Time Control Applications
s=0
x2
x1
Figure 1.1 Typical phase portrait under SMC.
The time derivative of this function is as follows:
Ë™V = sË™s
(1.30)
Considering (1.28), the following equation is obtained:
Ë™V = s(Î»x2 + f (x) + u)
(1.31)
and further:
Ë™V = s(Î»x2 + fn(x) + f (x) + u)
(1.32)
The control signal u is assumed to have two parts as follows:
u = un + ur
(1.33)
where un represents the part of the control signal, which cancels the nominal
part of the nonlinear function and ur is a term to guarantee the robustness
of the controller in the presence of f (x). The nominal part of the control
signal, un, is taken as follows:

Mathematical Preliminaries
9
Ë™V = s(f (x) + ur)
(1.34)
In order to have a finite time convergence of the manifold to zero, it is
assumed that:
Ë™V â‰¤âˆ’Î·|s|
(1.35)
and hence:
s(f (x) + ur) < âˆ’Î·|s|
(1.36)
ur is taken as follows:
ur = âˆ’Ksgn(s)
(1.37)
where sgn(s) is defined as follows:
sgn(s) =
âŽ§
âŽ¨
âŽ©
1,
s > 0
âˆ’1
s < 0
0
s = 0
(1.38)
Therefore:
|s||f (x)| âˆ’K|s| < âˆ’Î·|s|
(1.39)
and further:
âˆ’K|s| < âˆ’Î·|s| âˆ’F|s|
(1.40)
or equivalently:
Î· + F < K
(1.41)
Hence, the SMC, which guarantees the stability of the system, is as follows:
u = âˆ’Î»x2 âˆ’fn(x) âˆ’Ksgn(s)
(1.42)
A typical phase portrait of a second-order nonlinear dynamic system and
its sliding manifold is depicted in Fig. 1.1.

10
Fuzzy Neural Networks for Real Time Control Applications
As already mentioned earlier, SMC is a robust control algorithm that
can control nonlinear dynamic systems in the presence of uncertainties.
However, this controller suffers from several drawbacks:
1. The presence of sgn function makes the controller sensitive to noise. For
instance, suppose that the value of s is equal to 10âˆ’6 and a small value
of noise with amplitude of 10âˆ’5 with a different sign is added to s. This
small value can vary the sign of ur. Since K has a large value, it may
greatly affect the control signal and mislead the system.
2. In theory, when the states of the system hit the manifold, the ideal SMC
maintains them on it. However, in practice, there are some uncertainties
in the system and there also exists a delay between the time that the
sign of s changes and the controller switches. These reasons result in
the states crossing the manifold. This event is repetitive and produces a
zig-zag motion in the phase plane with a high frequency and is called
chattering (see Fig. 1.2)
Sliding manifold
S>0
S<0
Figure 1.2 Chattering due to delay and uncertainty in control signal switching.

Mathematical Preliminaries
11
3. In the design of an SMC, it is required that the nominal value of
the nonlinear functions of the system (e.g., fn in our example) is
known. Furthermore, the upper bound of the uncertainties in the system
(e.g., f (x) in our example) is also known.
In order to cope with these challenges, different methods have been pro-
posed: classical approaches and intelligent methods. The classical approaches
replace the sign function that usually exists in SMC with a smooth function.
Furthermore, as was mentioned earlier, a priori knowledge about the
nominal functions of the system is required in order to design an SMC. The
adaptive approaches may be used to approximate these functions and hence
lessen the need of SMC to priori knowledge about the system. However,
the intelligent methods use neural networks, fuzzy logic and FNNs to cope
with the challenging problems of SMC. Because of the proven general
function approximation property, flexibility and capability of using human
knowledge of FNNs, this structure is one of the most important structures
to overcome the drawbacks of SMC [6â€“8]. The intelligent methods are
known to cause less chattering in control signal and eliminate the knowledge
needed of the dynamic model of the system.
1.6 CONCLUSION
The aim of this chapter is to make the next chapters easier to follow and
understand. In order to accomplish this, a summary of basic mathematical
relationships and equations that will be used in consecutive chapters is given.
The reader is encouraged to refer to other textbooks for more detailed
explanations and discussions.
REFERENCES
[1] A. Cochocki, R. Unbehauen, Neural Networks for Optimization and Signal Processing,
John Wiley & Sons, Inc., New York, 1993.
[2] K.J. Ã…strÃ¶m, B. Wittenmark, Adaptive Control, Courier Corporation, Mineola, NY,
2013.
[3] X.-S. Yang, Engineering Optimization: An Introduction with Metaheuristic Applica-
tions, John Wiley & Sons, , 2010.
[4] H.K. Khalil, J. Grizzle, Nonlinear Systems, vol. 3, Prentice Hall, Upper Saddle River,
NJ, 1996.
[5] M. Gupta, L. Jin, N. Homma, Static and Dynamic Neural Networks: From Fundamen-
tals to Advanced Theory, John Wiley & Sons, New York, 2004.
[6] H.-Y.C. Lon-Chen Hung, Decoupled sliding-mode with fuzzy-neural network con-
troller for nonlinear systems, Int. J. Approx. Reason. 46 (2007) 74â€“97.

12
Fuzzy Neural Networks for Real Time Control Applications
[7] E. Kayacan, O. Cigdem, O. Kaynak, Sliding mode control approach for online learning
as applied to Type-2 fuzzy neural networks and its experimental evaluation, IEEE Trans.
Indust. Elect. 59 (9) (2012) 3510-3520.
[8] M. Khanesar, E. Kayacan, M. Reyhanoglu, O. Kaynak, Feedback error learning control
of magnetic satellites using Type-2 fuzzy neural networks with elliptic membership
functions, IEEE Trans. Cybernet. 45 (4) (2015) 858-868.

CHAPTER 2
Fundamentals of Type-1 Fuzzy
Logic Theory
Contents
2.1
Introduction
13
2.2
Type-1 Fuzzy Sets
16
2.3
Basics of Fuzzy Logic Control
18
2.3.1 FLC Block Diagram
18
2.3.1.1 Fuzzification
18
2.3.1.2 Rule Base
19
2.3.1.3 Inference
19
2.3.1.4 Defuzzification
20
2.4
Pros and Cons of Fuzzy Logic Control
20
2.5
Western and Eastern Perspectives on Fuzzy Logic
21
2.6
Conclusion
24
References
24
Abstract
While Boolean logic results are restricted to 0 and 1, fuzzy logic results are between 0
and 1. In other words, fuzzy logic, as a super set of conventional Boolean logic, defines
some intermediate values between sharp evaluations like absolute true and absolute
false. That means fuzzy sets can handle concepts we commonly face in daily life, like
very old, old, young and very young. In this chapter, we introduce the concepts of type-1
fuzzy sets and T1FLCs.
Keywords
Fuzzy logic theory, Fuzzy logic control, Crisp sets, Fuzzy sets, Boolean logic
2.1 INTRODUCTION
Fuzzy theory was first proposed in 1965 by Professor Lotfi A. Zadeh
at the University of California at Berkeley who introduced a set theory
that operates over the range [0, 1]. The core of his theory was outlined
in his seminal work entitled â€œFuzzy Setsâ€ in the journal Information and
Control [1].
Fuzzy Neural Networks for Real Time Control Applications
Copyright Â© 2016 Elsevier Inc.
http://dx.doi.org/10.1016/B978-0-12-802687-8.00002-5
All rights reserved.
13

14
Fuzzy Neural Networks for Real Time Control Applications
While Boolean logic results are restricted to 0 and 1, fuzzy logic results
are between 0 and 1. In other words, fuzzy logic defines some intermediate
values between sharp evaluations like absolute true and absolute false. This
means fuzzy sets can handle concepts we commonly face in daily life, like
very old, old, young and very young. Fuzzy logic is more like human thinking
because it is based on degrees of truth and uses linguistic variables.
Fuzzy logic deals with fuzzy sets whose elements have degrees of
memberships. In other words, an element can be a member of more than
one set associated with different membership values. For instance, in most
Western countries, weekdays are from Monday to Friday; the weekend
includes Saturday and Sunday. As an alternative to this Boolean logic, one
can think that people start feeling the positive effect of a weekend on Friday.
Thus, we may think that while Friday belongs to the set of â€œweekdaysâ€
with a membership value of 0.9, it belongs to the set of â€œweekendâ€ with a
membership value of 0.1. A similar logic can be constructed for Sunday too.
This example illustrates that while the conventional logic deals with â€œtruth
of any statement,â€ fuzzy logic concerns with â€œdegree of truth.â€
Zadeh makes the following statement for the fuzzy logic:
â€œFuzzy logic is a precise conceptual system of reasoning, deduction, and compu-
tation in which the objects of discourse and analysis are, or are allowed to be,
associated with imperfect information. Imperfect information is information which
in one or more respects is imprecise, uncertain, incomplete, unreliable, vague or
partially trueâ€ [2].
Fuzzy logic was probably not an acceptable concept for the scientists in
the early 1960s, because it contained vagueness in the engineering field. In
particular, for the Western countries, the concept of â€œfuzzyâ€ had a negative
connotation both in science and engineering. Science exists to get rid of
â€œvaguenessâ€ in our daily life and make everything clear. So, why would we
bring a concept such as â€œfuzzyâ€ into science?
Even if there were many scientists who had considered fuzzy logic to be
unscientific, since the 1970s, this approach to set theory has been widely
applied to control systems. As a significant milestone, the principles of
fuzzy logic were used to control a steam engine by Ebraham Mamdani
of University of London in 1974 [3]. The first industrial application was
a cement kiln built in Denmark in 1975. In the 1980s, Fuji Electric applied
fuzzy logic theory to the control of a water purification process. As a
challenging engineering project, in 1987, Sendai Railway system that had
automatic train operation control was built with fuzzy logic principles in

Fundamentals of Type-1 Fuzzy Logic Theory
15
Japan. Fuzzy control techniques were used in all the critical operations in the
control of the train, such as accelerating, breaking, and stopping operations.
In 1987, Takeshi Yamakawa used fuzzy control in an inverted pendulum
experiment, which is a classical benchmark control problem. After these
successful applications, not only the engineers but also the social scientists
applied fuzzy logic to different areas. In todayâ€™s technology, many companies
use fuzzy logic in their engineering projects for air conditioners, video
cameras, televisions, washing machines, bus time tables, decision-making
systems, medical diagnoses, etc.
The classical model-based control theory, typically a PID controller,
uses a mathematical model to define the relationship between the input
and output. The most serious disadvantage of these controllers is that they
usually assume the system to be linear or at least that it behaves as a linear
system in some range. If an accurate mathematical model of a system is
available, a conventional PID controller can make the performance of the
system quite acceptable. However, in real-time industrial systems, it is often
the case that there exist considerable difficulties in obtaining an accurate
model. Even when the model is sufficiently accurate, there are many other
uncertainties, e.g., due to the precision of the sensors, noise produced
by the sensors, environmental conditions of the sensors and nonlinear
characteristics of the actuators. Then, not only does the performance of the
model-based approaches drastically decrease, but also the complexity of the
controller design increases. In real-time applications, one solution is to tune
the controller coefficients conservatively. Then, robustness comes at a price
of a limited control performance beyond question. In such cases, a model-
free approach is preferable. Fortunately, FLCs have the ability to control
a system using some limited expert knowledge. In most cases, the design
procedure of a FLC tries to imitate an expert or a skilled human operator.
Besides, FLCs are low-cost implementations based on cheap sensors.
Although the concept of fuzzy logic and the concept of probability
seem to be similar, they are quite different. While probability makes guesses
about a certain reality, fuzzy logic does not make probability statements
but represents membership in vaguely defined sets. For instance, if 0.5 is
defined as a probability value for a person to be old, it can be said that
there is a chance that he/she may be old. It is not known whether he/she
is old or young. However, in fuzzy logic, if 0.5 is defined as the degree of
membership in the set of young and old people, we have some knowledge
about his/him and he/she is positioned in the middle of young and old
people.

16
Fuzzy Neural Networks for Real Time Control Applications
2.2 TYPE-1 FUZZY SETS
Fuzzy logic is considered much closer in spirit to human thinking and
natural language. The way of human thinking is realized with MFs, which
define how every point in the input space is mapped to a membership values
space. The membership values in fuzzy sets are in the range of [0;1]. If an
axiom is absolutely true, the membership value in fuzzy sets is 1. Similarly,
if it is absolutely false, the membership value in fuzzy sets is 0. The output
of a MF is called an antecedent (Âµ). While the input values for a MF are
crisp, they are changed into fuzzy variables by these MFs.
Figures 2.1 and 2.2 show the difference between classical Boolean (or
binary) logic and fuzzy logic. In Fig. 2.1, the MF is sharp-edged, which
means a small change in an input value might cause a big change in the
output values. According to this type of logic, any person shorter than
170 cm is considered to be short. However, in daily life, our way of thinking
is completely different, but similar to the one in Fig. 2.2. In the latter
approach, the MF is continuous and smooth and is more like human
thinking. According to the green expert in Fig. 2.2, any person shorter than
150 cm is considered to be short, whereas any person taller than 190 cm is
tall. Meanwhile, people who are taller than 150 cm but shorter than 190 cm
belong to both the â€œshortâ€ and â€œtallâ€ sets with specific membership values.
Height (cm)
(Short) 0.0
(Tall) 1.0
Degree of membership (Âµ)
170
Crisp
membership
function
Figure 2.1 Possible description of the vague concept â€œtallâ€by a crisp set.

Fundamentals of Type-1 Fuzzy Logic Theory
17
150
160
180
Height (cm)
Two different
fuzzy
membership
functions
190
(Short) 0.0
(Tall) 1.0
Degree of membership (Âµ)
Figure 2.2 Possible description of the vague concept â€œtallâ€by a fuzzy set.
On the other hand, the red expert thinks in a slightly different way. His/her
thresholds for being â€œshortâ€ and â€œtallâ€ are 160 and 180 cm, respectively.
The difference between these two experts brings the phenomena of â€œexpert
knowledgeâ€ to fuzzy logic theory. This feature is, of course, an additional
degree of freedom.
A type-1 fuzzy set, A, which is in terms of a single variable, x âˆˆX, may
be represented as:
A = {(x, ÂµA(x))|
âˆ€x âˆˆX}
(2.1)
A can also be defined as:
A =

xâˆˆX
ÂµA(x)/x
(2.2)
where

denotes union over all admissible x.
As can be seen from Fig. 2.3, a type-1 Gaussian MF, ÂµA(x), is constrained
to be between 0 and 1 for all x âˆˆX, and is a two-dimensional function.
This type of MF does not contain any uncertainty. In other words, there
exists a clear membership value for every input data point.

18
Fuzzy Neural Networks for Real Time Control Applications
0
1
2
3
4
5
6
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
x(input)
m(x)
Figure 2.3 Gaussian type-1 fuzzy membership function.
2.3 BASICS OF FUZZY LOGIC CONTROL
The main idea of FLC is very well explained by Kickert and Mamdani as:
â€œThe basic idea behind this approach was to incorporate the â€œexperienceâ€ of a hu-
manprocessoperatorinthedesignofcontroller.Fromthesetoflinguisticruleswhich
describetheoperatorâ€™scontrolstrategy a controlalgorithm isconstructed wherethe
words are defined as fuzzy sets. The main advantage of this approach seem to be
the possibility of implementing rule of thumb experience, intuition, heuristics and
the fact that it does not need a model of the processâ€ [4].
2.3.1 FLC Block Diagram
The input of a FLC is always crisp that is fuzzified in a fuzzification process
based on the rules in the rule base. After the fuzzy decisions are made by the
inference, the output of the FLC is converted into a crisp value. This is called
as defuzzification process.
A FLC can be divided into four main sub-groups: fuzzification, infer-
ence, rule base, and defuzzification (as shown in Fig. 2.4).
2.3.1.1 Fuzzification
Fuzzification is the process of converting a crisp input value to a fuzzy
value that is performed by the use of the information in the knowledge
base. Although various types of curves can be seen in literature, Gaussian,

Fundamentals of Type-1 Fuzzy Logic Theory
19
Rules
Inference
Defuzzifier
Fuzzifier
Crisp
input
Crisp
output
Fuzzy input
sets
Fuzzy output
sets
Figure 2.4 Fuzzy controller block diagram.
triangular, and trapezoidal MFs are the most commonly used in the
fuzzification process. These types of MFs can easily be implemented by
embedded controllers.
The MFs are defined mathematically with several parameters. In order
to fine-tune the performance of a FLC, these parameters, or the shape of
the MFs, can be adapted.
2.3.1.2 Rule Base
In this step, the expert knowledge is formulated as a finite number of rules.
The rule base contains the rules that are to be used in making decisions.
These rules are generally based on personal experience and intuition.
However, in some cases, the rules can be obtained by using neural networks,
genetic algorithms, or some empirical approaches.
A rule is composed of two main parts: an antecedent block (between the
If and Then) and a consequent block (following Then).
If (antecedent) Then (consequent)
Although the antecedent and the consequent parts have single arguments
in the above, a rule can be written with multiple arguments. While single
arguments are used in SISO systems, multiple arguments are used to deal
with MIMO or MISO systems.
2.3.1.3 Inference
Fuzzy decisions are produced in this process using the rules in the rule base.
During this process, each rule is evaluated separately and then a decision
is made for each individual rule. The result is a set of fuzzy decisions.
Logical operators, such as â€œAND,â€ â€œOR,â€ and â€œNOTâ€ define how the
fuzzy variables are combined.

20
Fuzzy Neural Networks for Real Time Control Applications
2.3.1.4 Defuzzification
The final step is a defuzzification process where the fuzzy output is translated
into a single crisp value, like the fuzzification process, by the degree of
membership values. Defuzzification is an inverse transformation compared
with the fuzzification process, because in this process, the fuzzy output is
converted into crisp values to be applied to the system.
There are several heuristic defuzzification methods. For instance, some
methods produce an integral output considering all the elements of the
resulting fuzzy set with the corresponding weights. One of the widely used
methods is the center-of-area method that takes the center of gravity of the
fuzzy set.
2.4 PROS AND CONS OF FUZZY LOGIC CONTROL
The classical control theory uses a mathematical model to define the
relationships between the input and output of a system. The most common
type of these controllers are PID controllers. After they take the output
of the system and compare it with the desired input, they generate an
appropriate control signal based on the calculated error value. The most
serious disadvantage of these controllers is that PID controllers usually
assume the system to be linear or at least it behaves as a linear system in some
range. If an accurate mathematical model of a control system is available, a
conventional PID controller can make the performance of the system quite
acceptable. Scientists have been working on classical control theory for a
long time. Today, the design of a PID type controller is very well-known
subject, and its implementation is simple and cheap.
There are, of course, reasons why fuzzy logic has been famous in the last
several decades. In real life, an accurate mathematical model of a control
process is generally not available, and it may not even exist. The real
world is nonlinear, uncertain, and always contains incomplete data. If the
mathematical model is not known by the designer, there is no way to come
up with a good PID controller design. The trial-and-error method may be
time consuming in a complex application, especially when there exist many
subsystems interacting each other. Assuming that the mathematical model
is known and relatively accurate, the parameters of the system are likely to
change by some outside factors, like heat, pressure, etc. In such cases, FLCs
have the ability to control a system with just some limited expert knowledge.
Further, FLCs are low-cost implementations based on cheap sensors.
Although fuzzy control fills an important gap in controller design
methodologies that require a full mathematical clarity about a system, it has

Fundamentals of Type-1 Fuzzy Logic Theory
21
also some serious drawbacks. First of all, because fuzzy control is a method
of nonlinear variable structure control, deriving its analytical structure is
the first step for analytical study. However, this step is very difficult and
sometimes impossible.
The second disadvantage of using a FLC is the number of design
parameters. Although a classical PID controller has only three design
parameters, the number of parameters for a FLC can be very large. The
number and shape of input and output fuzzy sets, scaling factors, and fuzzy
AND and OR operator characteristics must be determined by the designer.
Moreover, there are no clear relationships between these parameters and the
controllerâ€™s performance.
The last disadvantage of using a FLC is that even if there exist some stud-
ies for showing the stability of the overall system [5â€“7], there does not exist
a simple and systematic way of analyzing the stability of the system. On the
other hand, this work is a very straightforward task with linear controllers.
2.5 WESTERN AND EASTERN PERSPECTIVES ON
FUZZY LOGIC
It is a well-known fact that fuzzy logic has always been more popular in
Eastern countries when compared to western countries. Why?
According to Buddha, almost everything contains some of its opposite.
In other words, even if one belongs to one set significantly, it also belongs
to its opposite set with another membership value. For instance, there is not
a fully good or bad person in this world. Instead, everyone belongs to the set
of good and bad with some membership values. This idea can be regarded
as the basics of fuzzy logic theory where every input belongs to a set with
some specific membership values.
Another philosopher, Aristotle, saw things differently. According to
his logic, something could be either true or false, which is the principle
foundation of mathematics and Boolean logic. This approach claims that
something belongs to one set or its opposite. In such logic, the idea of
being true and false simultaneously cannot exist.
The Town Hall (Dutch: Stadhuis) of Leuven in Fig. 2.5(a) is a glamorous
masterpiece in Europe and a landmark building on that cityâ€™s hearth.
This monument was built in a Brabantine Late Gothic style between 1448
and 1469. The most significant details on this building are pointed Gothic
windows, octagonal turrets on the roof, and eye-catching sculptures. On the
other hand, the building in Fig. 2.5(b) is Kofukuji Buddhist temple in the
city of Nara, Japan. Kofukuji was established in 669 by Kagami-no-Okimi,

22
Fuzzy Neural Networks for Real Time Control Applications
(a)
(b)
Figure 2.5 Town Hall of Leuven, Belgium (a); Kofukuji Temple, Nara, Japan (b).

Fundamentals of Type-1 Fuzzy Logic Theory
23
the wife of Fujiwara no Kamatari, wishing for her husbandâ€™s recovery from
illness. In Fig. 2.5(b), instead of using sharp lines, smooth and round lines
were preferred, especially on the roofs.
When we consider the discussion above, it is not a big surprise that fuzzy
logic theory was proposed for the first time by an Asian professor, Lotfali
Askar Zadeh. Using Web of Science Core Collection, another search is
made. In the first search (see Fig. 2.6(a)), the topic of â€œfuzzy logic controlâ€
is performed and the results are analyzed w.r.t. countries and territories.
China
USA
Taiwan
India
England Canada
Turkey
S. Korea
Japan
Iran
0
100
200
300
400
500
600
Publication number
Fuzzy logic control
(a)
USA
China
England Germany Canada
Italy
Spain
Switzer
land
Nether
lands
Australia
0
500
1000
1500
2000
2500
Publication number
Model predictive control
(b)
Figure 2.6 Web of Science search for â€œfuzzy logic controlâ€(a); Web of Science search for
â€œmodel predictive controlâ€(b). (Accessed in June 2015).

24
Fuzzy Neural Networks for Real Time Control Applications
A similar search is performed for the second time (see Fig. 2.6(b)), but
for the topic of â€œmodel predictive control.â€ Whereas the former is a
model-free and heuristic approach, the latter is a model-based one. Con-
sidering the fact that there are many Asians in western countries (and vice
versa), there is still a clear pattern in Fig. 2.6 shows that fuzzy logic control,
as a model-free method, is more popular in Asian countries.
2.6 CONCLUSION
Real-time industrial systems require considerable man power/effort for
obtaining their accurate mathematical models. Even when the model is
sufficiently accurate, there are many other uncertainties, e.g., due to the
precision of the sensors, noise produced by the sensors, environmental con-
ditions of the sensors, and nonlinear characteristics of the actuators. Then,
not only does the performance of the model-based approaches drastically
decrease, but also the complexity of the controller design increases. In such
cases, model-free approaches are generally preferred both for modeling
and control purposes. The most common model-free approach is the
use of fuzzy logic theory.
REFERENCES
[1] L. Zadeh, Fuzzy sets, Informat. Control 8 (1965) 338-353.
[2] L. Zadeh, Toward extended fuzzy logicâ€”a first step, Fuzzy Sets Syst. 160 (2009)
3175-3181.
[3] E.H. Mamdani, Applications of fuzzy algorithms for control of a simple dynamic plant,
Proc. IEEE 121 (1974) 1585-1588.
[4] W. Kickert, E. Mamdani, Analysis of a fuzzy logic controller, Fuzzy Sets Syst. 1 (1978)
29-44.
[5] K. Tanaka, M. Sugeno, Stability analysis and design of fuzzy control systems, Fuzzy Sets
Syst. 45 (2) (1992) 135-156.
[6] Y.-Y. Cao, P. Frank, Stability analysis and synthesis of nonlinear time-delay systems via
linear TakagiSugeno fuzzy models, Fuzzy Sets Syst. 124 (2) (2001) 213-229.
[7] S.G. Cao, N.W. Rees, G. Feng, Stability analysis and design for a class of continuous-time
fuzzy control systems, Int. J. Control 64 (6) (1996) 1069-1087.

CHAPTER 3
Fundamentals of Type-2 Fuzzy
Logic Theory
Contents
3.1
Introduction
25
3.2
Type-2 Fuzzy Sets
27
3.2.1 Interval Type-2 Fuzzy Sets
28
3.2.2 T2FLS Block Diagram
29
3.2.2.1 Fuzzifier
30
3.2.2.2 Rule Base
30
3.2.2.3 Inference
30
3.2.2.4 Type Reduction
30
3.2.2.5 Defuzzification
30
3.3
Existing Type-2 Membership Functions
30
3.3.1 A Novel Type-2 MF: Elliptic MF
33
3.4
Conclusion
35
References
35
Abstract
There are two different approaches to FLS design: T1FLSs and T2FLSs. The latter is
proposed as an extension of the former with the intention of being able to model the
uncertainties that invariably exist in the rule base of the system. In type-1 fuzzy sets, MFs
are totally certain, whereas in type-2 fuzzy sets, MFs are themselves fuzzy. The latter re-
sults in a case where the antecedents and consequents of the rules are uncertain. In this
chapter, we will introduce the basics of type-2 fuzzy sets, type-2 fuzzy MFs and T2FLCs.
Keywords
Type-2 fuzzy logic theory, Type-2 fuzzy logic control, Footprint of uncertainty, Elliptic
membership function, Type-2 fuzzy neural networks
3.1 INTRODUCTION
Type-2 fuzzy sets were introduced by Lotfi A. Zadeh in 1975 as an
extension of type-1 fuzzy sets. Mendel and Karnik developed the theory of
type-2 fuzzy sets further in their 1999 IEEE Trans. Fuzzy Syst. article [1].
Fuzzy Neural Networks for Real Time Control Applications
Copyright Â© 2016 Elsevier Inc.
http://dx.doi.org/10.1016/B978-0-12-802687-8.00003-7
All rights reserved.
25

26
Fuzzy Neural Networks for Real Time Control Applications
The basics of the interval type-2 fuzzy systems and their design principles
are described in Ref. [2].
T2FLSs appear to be a more promising method than their type-1
counterparts for handling uncertainties such as noisy data and changing
environments [3, 4]. In Refs. [5, 6] the effects of the measurement noise in
T1FLCs and T2FLCs and identifiers are simulated to perform a comparative
analysis. It was concluded that the use of T2FLCs in real-world applications
[7], which exhibit measurement noise and modeling uncertainties, can be a
better option than T1FLCs.
There are (at least) eight sources of uncertainties in FLSs:
1. Precision of the measurement devices
2. Noise on the measurement devices
3. Environmental conditions of the measurement devices
4. Unknown nonlinear characteristics of the actuators
5. Lack of modeling
6. The meanings of the words that are used in the antecedents and
consequents of rules can be uncertain (words mean different things to
different people) [8]
7. Consequents may have a histogram of values associated with them,
especially when knowledge is extracted from a group of experts who
do not all agree [8]
8. Uncertainty caused by some unvisited data that the fuzzy system does
not have any predefined rules for
When a system has large amount of uncertainties, T1FLSs may not be
able to achieve the desired level of performance with reasonable complexity
of the structure [9]. In such cases, the use of T2FLSs is suggested as the
preferable approach in literature in many areas, such as forecasting of time-
series [10], controlling of mobile robots [11], and the truck backing-up
control problem [12]. The VLSI and FPGA implementations of T2FLSs are
also discussed in Refs. [13, 14]. In Ref. [10], it is shown that when the
parameters are tuned appropriately, T2FLSs can result in a better prediction
ability as compared to T1FLSs. In Ref. [11], T2FLSs are applied to real-
time mobile robots for indoor and outdoor environments. The real-time
implementation studies show that a traditional T1FLC cannot handle high
levels of uncertainties in the system effectively and a T2FLC using type-2
fuzzy sets results in better performance. Moreover, with the latter approach,
the number of rules to be determined may be reduced (it should be
noted that this may not mean a corresponding decrease in the parameters
to be updated). In Ref. [12], the authors construct an interval T2FNN

Fundamentals of Type-2 Fuzzy Logic Theory
27
structure, and show that a better performance can be obtained as compared
to a conventional T1FNN. The VLSI implementation of T2FLSs is also
discussed in literature and it has been shown that the inference speed can
be sufficiently high for real-time applications. In Ref. [14], a type-2 self-
organizing neural fuzzy system and its hardware implementation is proposed.
It is reported that using interval type-2 fuzzy sets in that structure enables
the overall system to be more robust than the one with type-1 fuzzy systems.
There exist two main approaches to the design of a FLC:
1. Type-1 fuzzy sets: MFs are totally certain.
2. Type-2 fuzzy sets: MFs that are themselves fuzzy. This results in an-
tecedents and consequents of the rules being uncertain.
3.2 TYPE-2 FUZZY SETS
A type-2 fuzzy set, ËœA, may be represented as [8]:
ËœA = {((x, u), Î¼ ËœA(x, u))|
âˆ€x âˆˆX
âˆ€u âˆˆJx âŠ†[0, 1]}
(3.1)
where Î¼ ËœA(x, u) is the type-2 fuzzy MF in which 0 â‰¤Î¼ ËœA(x, u)) â‰¤1. ËœA can
also be defined as [8]:
ËœA =

xâˆˆX

uâˆˆJx
Î¼ ËœA(x, u)/(x, u)
Jx âŠ†[0, 1]
(3.2)
where   denotes the union over all admissible x and u [8].
Jx is called primary membership of x [8]. Additionally, there is a secondary
membership value corresponding to each primary membership value that
defines the possibility for primary memberships [15]. While the secondary
MFs can take values in the interval of [0,1] in generalized T2FLSs, they are
uniform functions that only take on values of 1 in interval T2FLSs. Since
the general T2FLSs are computationally demanding, the use of interval
T2FLSs is more commonly seen in the literature, due to the fact that the
computations are more manageable.
If the circumstances are so fuzzy, the places of the MFs may not be
determined precisely. In such cases, the membership grade cannot be
determined as a crisp number in [0,1], and the use of type-2 fuzzy sets
might be a preferable option.
If the standard deviation of a type-1 fuzzy Gaussian MF is blurred,
Fig. 3.1 can be obtained. In Fig. 3.1, the MF does not have a single

28
Fuzzy Neural Networks for Real Time Control Applications
c (center)
m(x )
0
0.2
0.4
0.6
0.8
1
Lower MF
Upper MF
s lower
s upper
Figure 3.1 Gaussian type-2 fuzzy MF (FOU).
value for a specific value of an input. The values for a specific input, the
vertical line intersecting the MFs, do not need to all be weighted the same.
Moreover, an amplitude distribution can be assigned to all of those points.
Hence, a three-dimensional MF-a type-2 MF- that characterizes a type-2
fuzzy set is created if the amplitude distribution operation is done for all
inputs [9].
The FOU, the union of all primary memberships, is said to be the
bounded region that represents the uncertainty in the primary memberships
of a type-2 fuzzy set. In Fig. 3.1, an upper MF (red) and a lower MF (blue)
are two type-1 MFs that are the bounds for the FOU of a type-2 fuzzy set
[9]. It is assumed to have infinite type-1 MFs between the lower and upper
MFs in a type-2 fuzzy set.
3.2.1 Interval Type-2 Fuzzy Sets
When all Î¼ ËœA(x, u) are equal to 1, then ËœA is an interval type-2 fuzzy set. The
special case of (3.2) might be defined for the interval type-2 fuzzy sets:
ËœA =

xâˆˆX

uâˆˆJx
1/(x, u)
Jx âŠ†[0, 1]
(3.3)
Researchers are familiar with the computational burden of general
T2FLS. Hence, interval T2FLSs are commonly used in literature. Both the
general and interval type-2 fuzzy MFs are three-dimensional. As can be
seen from Fig. 3.2, the only difference between them is that the secondary

Fundamentals of Type-2 Fuzzy Logic Theory
29
x (input)
0
0.5
m(x)
0
0.2
0.4
0.6
0.8
1
1
m(x,u)
Figure 3.2 Three-dimensional representation of interval type-2 fuzzy MFs.
membership value of the interval type-2 MF is equal to 1. In this book, it
is focused on the interval T2FLSs.
3.2.2 T2FLS Block Diagram
The T2FLS block diagram is shown in Fig. 3.3. The reader should have
basic knowledge of T1FLSs, since only the similarities and the differences
between T1FLSs and T2FLSs will be given in this book. As can be seen
from Fig. 3.3, an additional block to Fig. 2.4 (type reduction) is needed in
T2FLS design. Although the structure in Fig. 3.3 brings some advantages
when dealing with uncertainties, it also increases the computational burden.
Crisp input
Rules
Inference
Defuzzifier
Type-reducer
Fuzzifier
Fuzzy input
sets
Crisp output
Output processing
Type-reduced
set (type-1)
Fuzzy output
sets
Figure 3.3 T2FLS block diagram.

30
Fuzzy Neural Networks for Real Time Control Applications
The following are the basic blocks of a T2FLS:
3.2.2.1 Fuzzifier
The fuzzifier maps crisp inputs into type-2 fuzzy sets, which activates the
inference engine.
3.2.2.2 Rule Base
The rules in a T2FLS remain the same as in a T1FLS, but antecedents and
consequents are represented by interval type-2 fuzzy sets.
3.2.2.3 Inference
The inference block assigns fuzzy inputs to fuzzy outputs using the rules in
the rule base and operators such as union and intersection. In type-2 fuzzy
sets, join (âŠ”) and meet operators (âŠ“), which are new concepts in fuzzy logic
theory, are used instead of union and intersection operators. These two new
operators are used in secondary MFs, and they are defined and explained in
detail in Ref. [16].
3.2.2.4 Type Reduction
The type-2 fuzzy outputs of the inference engine are transformed into type-
1 fuzzy sets that are called the type-reduced sets. There are two common
methods for the type-reduction operation in the interval T2FLSs: One is
the Karnik-Mendel iteration algorithm, and the other is Wu-Mendel un-
certainty bounds method. These two methods are based on the calculation
of the centroid.
3.2.2.5 Defuzzification
The outputs of the type reduction block are given to the defuzzificaton
block. The type-reduced sets are determined by their left end point and
right end point, and the defuzzified value is calculated by the average of
these points.
3.3 EXISTING TYPE-2 MEMBERSHIP FUNCTIONS
There exists number of type-2 fuzzy MFs in literature, i.e., triangular,
Gaussian, trapezoidal, sigmoidal, pi-shaped, etc. Gaussian type MFs are
widely used in which uncertainties can be associated with mean and standard
deviation of the Gaussian function.

Fundamentals of Type-2 Fuzzy Logic Theory
31
c
m(x )
0
0.2
0.4
0.6
0.8
1
s lower
s upper
0
(a)
(b)
0.2
0.4
0.6
0.8
1
m (x)
c lower
c upper
s
s
Figure 3.4 Type-2 MFs with uncertain standard deviation (a) and uncertain
mean (b).
In Fig. 3.4(a) and (b), Gaussian type-2 fuzzy MFs with uncertain standard
deviation and uncertain mean are shown. The MF is expressed as:
ËœÎ¼(x) = exp

âˆ’1
2
(x âˆ’c)2
Ïƒ 2

(3.4)

32
Fuzzy Neural Networks for Real Time Control Applications
m (x)
0
(a)
(b)
0.2
0.4
0.6
0.8
1
c
câ€“ d1
c â€“ d2
c+ d1
c+ d2
m (x )
0
0.2
0.4
0.6
0.8
1
c 1
c 2
c1â€“d
c2â€“d
c1+ d
c2+ d
Figure 3.5 Type-2 MFs with uncertain width (a) and center (b).
where c and Ïƒ are the center and the width of the MF and x is the input
vector.
In Fig. 3.5(a) and (b), triangular type-2 fuzzy sets with uncertain width
and uncertain center are shown. The MF is expressed as:

Fundamentals of Type-2 Fuzzy Logic Theory
33
ËœÎ¼(x) =

1 âˆ’|xâˆ’c|
d
if c âˆ’d < x < c + d
0
else
(3.5)
where c and d are the center and the width of the MF and x is the input
vector.
3.3.1 A Novel Type-2 MF: Elliptic MF
A novel type-2 fuzzy MF is introduced in this section. It has certain
values on both ends of the support and the kernel, and some uncertain
values on the other values of the support. The need for proposing such
a novel MF will be explained in detail in Chapter 9. The lower (Î¼) and
the upper ( Â¯Î¼) MFs with the parameters c, d, a1, and a2 are defined as
follows:
Â¯Î¼(x) =
 1 âˆ’|xâˆ’c
d |a11/a1
if c âˆ’d < x < c + d
0
else
(3.6)
Î¼(x) =
 
1 âˆ’|xâˆ’c
d |a21/a2
if c âˆ’d < x < c + d
0
else
(3.7)
where c and d are the center and the width of the MF and x is the input
vector. The parameters a1 and a2 determine the width of the uncertainty of
the proposed MF, and these parameters should be selected as follows:
a1 > 1
(3.8)
0 < a2 < 1
Figure 3.6(a), (b), and (c) show the shapes of the proposed MF for a1 =
a2 = 1, a1 = 1.2, a2 = 0.8, and a1 = 1.4, a2 = 0.6, respectively. As can
be seen from Fig. 3.6a, the shape of the proposed type-2 MF is changed to
a type-1 triangular MF when its parameters are selected as a1 = a2 = 1.
These parameters can be selected as some constants or they can be tuned
adaptively.

m (x )
0
(a)
0.2
0.4
0.6
0.8
1
c
c â€“ d
c + d
(b)
m (x )
0
0.2
0.4
0.6
0.8
1
c
c â€“ d
c + d
(c)
m (x)
0
0.2
0.4
0.6
0.8
1
c
c â€“ d
c + d
Figure 3.6 Shapes of the proposed type-2 MF with different values for a1 and a2.

Fundamentals of Type-2 Fuzzy Logic Theory
35
3.4 CONCLUSION
If the circumstances are so fuzzy, the places of the type-1 MFs may not
be determined precisely. In cases where the membership grade cannot be
determined as a crisp number in [0,1], the use of type-2 fuzzy sets might
be a preferable option. T2FLSs appear to be a more promising method than
their type-1 counterparts for handling uncertainties such as noisy data and
changing working environments.
REFERENCES
[1]
N.N. Karnik, J.M. Mendel, Q. Liang, Type-2 fuzzy logic systems, IEEE Trans. Fuzzy
Syst. 7 (1999) 643-658.
[2]
J.M. Mendel, R. John, F. Liu, Interval Type-2 fuzzy logic systems made simple, IEEE
Trans. Fuzzy Syst. 14 (2006) 808-821.
[3]
C.-F. Juang, C.-H. Hsu, Reinforcement interval Type-2 fuzzy controller design by
online rule generation and Q-value-aided ant colony optimization, IEEE Trans. Syst.
Man Cybernet. B Cybernet. 39 (6) (2009) 1528-1542.
[4]
C. Juang, Y. Tsao, A self-evolving interval Type-2 fuzzy neural network with online
structure and parameter learning, IEEE Trans. Fuzzy Syst. 16 (2008) 1411-1424.
[5] R. Sepulveda, P. Melin, A. Rodriguez, A. Mancilla, O. Montiel, Analyzing the effects
of the footprint of uncertainty in type-2 fuzzy logic controllers, Eng. Lett. 13 (2006)
138-147.
[6]
M.A. Khanesar, M. Teshnehlab, E. Kayacan, O. Kaynak, A novel Type-2 fuzzy
membership function: application to the prediction of noisy data, in: Proceedings of
the IEEE International Conference on Computational Intelligence for Measurement
System and Applications, 2010, pp. 128-133.
[7]
M. Biglarbegian, W. Melek, J. Mendel, Design of novel interval Type-2 fuzzy
controllers for modular and reconfigurable robots: theory and experiments, IEEE Trans.
Indust. Electron. 58 (4) (2011) 1371-1384.
[8]
J.M. Mendel, R.I.B. John, Type-2 fuzzy sets made simple, IEEE Trans. Fuzzy Syst. 10
(2002) 117-127.
[9]
J.M. Mendel, Uncertain Rule-Based Fuzzy Logic System: Introduction and New
Directions, Prentice Hall, Upper Saddle River, NJ, 2001.
[10]
N. Karnik, J. Mendel, Applications of type-2 fuzzy logic systems to forecasting of
time-series, Informat. Sci. 120 (1999) 89-111.
[11]
H.A. Hagras, A hierarchical type-2 fuzzy logic control architecture for autonomous
mobile robots, IEEE Trans. Fuzzy Syst. 12 (4) (2004) 524-539.
[12]
C. Wang, C. Cheng, T. Lee, Dynamical optimal training for interval type-2 fuzzy
neural network (T2FNN), IEEE Trans. Syst. Man Cybernet. B Cybernet. 34 (2004)
1462-1477.
[13]
S. Huang, Y. Chen, VLSI implementation of type-2 fuzzy inference processor, in:
Proceedings of IEEE ISCAS 2005, 2005, pp. 3307-3310.
[14]
C. Juang, Y. Tsao, A type-2 self-organizing neural fuzzy system and its FPGA imple-
mentation, IEEE Trans. Syst. Man Cybernet. B Cybernet. 38 (6) (2008) 1537-1548.
[15]
Q. Liang, J.M.Mendel, Interval type-2 fuzzy logic systems: theory and design, IEEE
Trans. Fuzzy Syst. 8 (2000) 535-550.
[16]
N.N. Karnik, J.M. Mendel, Operations on type-2 fuzzy sets, Fuzzy Sets Syst. 122 (2)
(2001) 327-348.

CHAPTER 4
Type-2 Fuzzy Neural Networks
Contents
4.1
Type-1 Takagi-Sugeno-Kang Model
37
4.2
Other Takagi-Sugeno-Kang Models
38
4.2.1 Model I
38
4.2.2 Model II
39
4.2.2.1 Interval Type-2 TSK FLS
40
4.2.2.2 Numerical Example of the Interval Type-2 TSK FLS
41
4.2.3 Model III
42
4.3
Conclusion
43
References
43
Abstract
The two most common artificial intelligence techniques, FLSs and ANNs, can be used in
the same structure simultaneously, namely as â€œfuzzy neural networks.â€The advantages of
ANNs such as learning capability from input-output data, generalization capability, and
robustness and the advantages of fuzzy logic theory such as using expert knowledge
are harmonized in FNNs. In this chapter, type-1 and type-2 TSK fuzzy logic models are in-
troduced. Instead of using fuzzy sets in the consequent part (as in Mamdani models), the
TSK model uses a function of the input variables. The order of the function determines
the order of the model, e.g., zeroth-order TSK model, first-order TSK model, etc.
Keywords
Type-1 fuzzy neural networks, Type-2 fuzzy neural networks, TSK models, Artificial
intelligence, Fuzzy logic, Neural networks
4.1 TYPE-1 TAKAGI-SUGENO-KANG MODEL
A type-1 TSK model can be described by fuzzy If-Then rules. For instance,
in a first-order type-1 TSK model, the rule base is as follows:
IF x1 is Aj1 and x2 is Aj2 and â€¦and xn is Ajn
(4.1)
THEN uj =
n

i=1
wijxi + bj
Fuzzy Neural Networks for Real Time Control Applications
Copyright Â© 2016 Elsevier Inc.
http://dx.doi.org/10.1016/B978-0-12-802687-8.00004-9
All rights reserved.
37

38
Fuzzy Neural Networks for Real Time Control Applications
where x1, x2, . . . ,xn are the input variables, ujâ€™s are the output variables, and
Ajiâ€™s are type-1 fuzzy sets for the jth rule and the ith input. The parameters in
the consequent part of the rules are wij and bj (i = 1, . . . , n, j = 1, . . . , M).
The final output of the system can be written as:
u =
M
j=1 fjuj
M
j=1 fj
(4.2)
where fj is given by:
fj(x) = Î¼Aj1(x1) âˆ—Â· Â· Â· âˆ—Î¼Ajn(xn)
(4.3)
in which âˆ—represents the t-norm, which is the prod operator in this
book.
4.2 OTHER TAKAGI-SUGENO-KANG MODELS
Other TSK models (shown in Table 4.1) can be classified into three
groups [1]:
1. Model I: Antecedents are type-2 fuzzy sets, and consequents are type-1
fuzzy sets (A2-C1)
2. Model II: Antecedents are type-2 fuzzy sets, and consequents are crisp
numbers (A2-C0)
3. Model III: Antecedents are type-1 fuzzy sets, and consequents are type-1
fuzzy sets (A1-C1)
4.2.1 Model I
Type-2 Model I can be described by fuzzy If-Then rules in which the
antecedent part is type-2 fuzzy sets. In the consequent part, the structure is
similar to that of a type-1 TSK fuzzy system, however, the parameters are
type-1 fuzzy sets rather than crisp numbers. They are therefore named as
â€œType-2 TSK Model Iâ€ systems. In Model I, the rule base is as follows:
Table 4.1 Classification of other TSK models
Other TSK FLSs Model I
Model II
Model III
Antecedent
Type-2 fuzzy sets Type-2 fuzzy sets Type-1 fuzzy sets
Consequent
Type-1 fuzzy sets Crisp numbers
Type-1 fuzzy sets

Type-2 Fuzzy Neural Networks
39
IF x1 is ËœAj1 and x2 is ËœAj2 and â€¦and xn is ËœAjn
(4.4)
THEN Uj =
n

i=1
Wijxi + Bj
where x1, x2, . . . ,xn are the input variables, Ujâ€™s are the output variables, and
ËœAjiâ€™s are type-2 fuzzy sets for the jth rule and the ith input. The parameters in
the consequent part of the rules are Wij and Bj (i = 1, . . . , n, j = 1, . . . , M),
which are type-1 fuzzy sets. The final output of the first-order type-2 TSK
Model I is as follows [1]:
U(U1, . . . , UM, F1, . . . , FM) =

u1
Â· Â· Â·

uM

f1
Â· Â· Â·

fM
Ï„ M
j=1Î¼Uj(uj)â‹†
Ï„ M
j=1Î¼Fj(fj)
M
j=1 fjuj
M
j=1 fj
(4.5)
where M is the number of rules fired, uj âˆˆUj, fj âˆˆFj, and Ï„ and â‹†indicate
the t-norm. Fj is the firing strength which is defined as:
Fj = Î¼ ËœAj1(x1) âŠ“Î¼ ËœAj2(x2) âŠ“Â· Â· Â· âŠ“Î¼ ËœAjn(xn)
(4.6)
where âŠ“shows the meet operation.
Although the calculation of (4.5) is difficult, some general concepts are
explained in Ref. [2]. When interval type-2 sets are used in the antecedent
part and type-1 sets are used in the consequent part, it is shown in Ref. [1]
that the output of an interval T2FLS is:
f (x) = ul + ur
2
(4.7)
where ur and ul are the maximum and minimum values of u, respectively.
The reader is encouraged to refer [1] and [2] for further information
about the calculation process of ur and ul. A broad survey exists in literature
in which number of different type reducers for Model I are compared [4].
4.2.2 Model II
Model II can be regarded as a special case of Model I where the antecedents
are type-2 fuzzy sets and the consequents are polynomials. A type-2 TSK

40
Fuzzy Neural Networks for Real Time Control Applications
Model II can be described by fuzzy If-Then rules. For instance, in a first-
order type-2 TSK Model II, the rule base is as follows [1]:
IF x1 is ËœAj1 and x2 is ËœAj2 and â€¦and xn is ËœAjn
(4.8)
THEN uj =
n

i=1
wijxi + bj
where x1, x2, . . . ,xn are the input variables, ujâ€™s are the output variables,
ËœAjiâ€™s are type-2 fuzzy sets for the jth rule and the ith input. The parameters in
the consequent part of the rules are wij and bj (i = 1, . . . , n, j = 1, . . . , M).
The final output of the model is as follows [1]:
U(F1, . . . , FM) =

f1
Â· Â· Â·

fM
Ï„ M
j=1Î¼Fj(fj)
M
j=1 fjuj
M
j=1 fj
(4.9)
where M is the number of rules fired, fj âˆˆFj, and Ï„ indicates the t-norm.
Note that (4.9) is a special case of (4.5), because each Uj in (4.5) is
converted into a crisp value here. The firing strength is the same as (4.6).
4.2.2.1 Interval Type-2 TSK FLS
In the structure of the interval type-2 TSK FLS, (4.9) is given as follows [3]:
YTSK/A2âˆ’C0 =

f 1âˆˆ[ f 1,f 1]
Â· Â· Â·

f Mâˆˆ[f M,f M]
1
M
j=1 fjuj
M
j=1 fj
(4.10)
where f j and f j are given by:
f j(x) = Î¼Aj1(x1) âˆ—Â· Â· Â· âˆ—Î¼Ajn(xn)
(4.11)
f j(x) = Î¼Aj1(x1) âˆ—Â· Â· Â· âˆ—Î¼Ajn(xn)
in which âˆ—represents the t-norm, which is the prod operator in this book.
The output of the fuzzy system in closed form is approximated by [3]:
YTSK/A2âˆ’C0 =
M
j=1 f juj
M
j=1 f j + M
j=1 f j
+
M
j=1 f juj
M
j=1 f j + M
j=1 f j
(4.12)

Type-2 Fuzzy Neural Networks
41
4.2.2.2 Numerical Example of the Interval Type-2 TSK FLS
In order to be able to give a clear explanation about the inference of this
type FLS, a numerical example is given:
Letâ€™s assume a T2FLS (A2-CO) with two inputs and two type-2 fuzzy
MFs for each. While the antecedent type-2 fuzzy MFs are given in Fig. 4.1,
the consequent part of the rules are given as follows: u1 = 4x1 + x2 and
u2 = 2x1 + 3x2.
0
2
4
6
8
10
12
0
0.2
0.4
0.6
0.8
1
x1
m(x1)
A11
~
A12
~
x1= 5.25
*
0
2
4
6
8
10
12
0
0.2
0.4
0.6
0.8
1
x2
m(x2)
A21
~
A22
~
x2=6.5
*
Figure 4.1 Two rules each having two type-2 triangular fuzzy MFs.

42
Fuzzy Neural Networks for Real Time Control Applications
The mathematical form of triangular MFs are as follows:
ËœÎ¼(x) =

1 âˆ’|xâˆ’c|
d
if c âˆ’d < x < c + d
0
else
(4.13)
where c11 = c21 = c11 = c21 = 4, c12 = c22 = c12 = c22 = 8, and
d11 = d21 = d12 = d22 = 3, d11 = d21 = d12 = d22 = 4.
The input 1 (xâˆ—
1) and the input 2 (xâˆ—
2) are selected as 5.25 and 6.5,
respectively. The firing strengths are as follows:
f 1 = 0.6875 âˆ—0.6250 = 0.4297
(4.14)
f 1 = 0.5833 âˆ—0.5000 = 0.2917
f 2 = 0.3125 âˆ—0.3750 = 0.1172
f 2 = 0.0833 âˆ—0.1667 = 0.0139
u1 = 4x1 + x2 = 4 âˆ—5.25 + 6.5 = 27.50
(4.15)
u2 = 2x1 + 3x2 = 2 âˆ—5.25 + 3 âˆ—6.5 = 30.00
ul = 0.4297 âˆ—27.50 + 0.0139 âˆ—30.00
0.4297 + 0.0139
= 27.5783
(4.16)
ur = 0.2917 âˆ—27.50 + 0.1172 âˆ—30.00
0.2917 + 0.1172
= 28.2166
The final output is calculated as follows:
uâˆ—= ul + ur
2
= 27.8974
(4.17)
4.2.3 Model III
The TSK Model III can be described by fuzzy If-Then rules. For instance,
in a first-order type-2 TSK Model III the rule base is as follows [1]:
IF x1 is Aj1 and x2 is Aj2 and â€¦and xn is Ajn
(4.18)
THEN Uj =
n

i=1
Wijxi + Bj

Type-2 Fuzzy Neural Networks
43
where x1, x2, . . . ,xn are the input variables, Ujâ€™s are the output variables,
and Ajiâ€™s are type-1 fuzzy sets for jth rule and the ith input. The parameters in
the consequent part of the rules are Wij and Bj (i = 1, . . . , n, j = 1, . . . , M).
Note that both the consequent parameters and the outputs of the rules above
are type-1 fuzzy sets. Also, Ajkâ€™s are type-1 fuzzy sets (k = 1, . . . , n).
The final output of the model is as follows:
U(U1, . . . , UM) =

u1
Â· Â· Â·

uM
Ï„ M
j=1Î¼Uj(uj)
M
j=1 fjuj
M
j=1 fj
(4.19)
where M is the number of rules fired, uj âˆˆUj.
fj is the firing strength, which is defined as:
fj = Î¼Aj1(x1) âˆ—Î¼Aj2(x2) âˆ—Â· Â· Â· âˆ—Î¼Ajn(xn)
(4.20)
4.3 CONCLUSION
The advantages of ANNs such as learning capability from input-output data,
generalization capability, and robustness and the advantages of fuzzy logic
theory such as using expert knowledge are harmonized in FNNs. Inspired
by the conventional FNNs, T2FNNs have been designed in which the MFs
are type-2. These systems are stronger to deal with uncertainties in the rule
base of the system compared to their type-1 counterparts.
REFERENCES
[1] Q. Liang, Fading channel equalization and video traffic classification using nonlinear
signal processing techniques, University of Southern California, USA, 2000.
[2] N. Karnik, J. Mendel, An introduction to type-2 fuzzy logic systems 1998, October
1998, URL http://sipi.usc.edu/mendel/report.
[3] M. Begian, W. Melek, J. Mendel, Parametric design of stable type-2 TSK fuzzy systems,
in: Fuzzy Information Processing Society, 2008. NAFIPS 2008. Annual Meeting of the
North American, 2008, pp. 1-6.
[4] Wu. Dongrui, Approaches for reducing the computational cost of interval type-2 fuzzy
logic systems: overview and comparisons. Fuzzy Systems, IEEE Transactions on 21, no. 1
(2013) 80-99.

CHAPTER 5
Gradient Descent Methods
for Type-2 Fuzzy Neural Networks
Contents
5.1
Introduction
46
5.2
Overview of Iterative Gradient Descent Methods
46
5.2.1 Basic Gradient-Descent Optimization Algorithm
47
5.2.2 Newton and Gauss-Newton Optimization Algorithms
52
5.2.3 LM Algorithm
55
5.2.4 Gradient Descent Algorithm with an Adaptive Learning Rate
55
5.2.5 GD Algorithm with a Momentum Term
56
5.3
Gradient Descent Based Learning Algorithms for Type-2 Fuzzy Neural Networks
57
5.3.1 Consequent Part Parameters
57
5.3.2 Premise Part Parameters
58
5.3.3 Variants of the Back-Propagation Algorithm for Training the T2FNNs
61
5.4
Stability Analysis
61
5.4.1 Stability Analysis of GD for Training of T2FNN
61
5.4.2 Stability Analysis of the LM for Training of T2FNN
63
5.5
Further Reading
69
5.6
Conclusion
69
References
69
Abstract
Given an initial point, if an algorithm tries to follow the negative of the gradient of the
function at the current point to reach a local minimum, we face the most common
iterative method to optimize a nonlinear function: the GD method. The main goal of
this chapter is to briefly discuss a multivariate optimization technique, namely the GD
algorithm, to optimize a nonlinear unconstrained problem. The referred optimization
problem is a cost function of a FNN, either type-1 or type-2, in this chapter. The main
features, drawbacks and stability conditions of these algorithms are discussed.
Keywords
Gradient descent for FNN, Levenberg-Marquardt for FNN, Momentum-term gradient
descent, Adaptive learning rate, Gradient descent, Stability analysis
Fuzzy Neural Networks for Real Time Control Applications
Copyright Â© 2016 Elsevier Inc.
http://dx.doi.org/10.1016/B978-0-12-802687-8.00005-0
All rights reserved.
45

46
Fuzzy Neural Networks for Real Time Control Applications
5.1 INTRODUCTION
Optimization methods are ubiquitous when it comes to the estimation of
the parameters of a FNN, either type-1 or type-2. In the case of using
a FNN as a controller, identifier or classifier, the design process of the
corresponding FNN is nonlinear, and in most cases, an unconstrained
optimization problem. The first step is to define an objective function that
may also be called a cost function or performance index. In controller
design, the objective function is defined based on the difference between the
output of the plant and the reference signal. In the design of an identifier,
the cost function is defined on the basis of the difference between the output
of the FNN and the measured output of the system. When one wants to
design a classifier, the difference between the class number of an input with
respect to its real class number may be used as the cost function.
The purpose of this chapter is to briefly discuss the iterative numerical
methods based on gradient descent algorithm to optimize a nonlinear
unconstraint problem in order to design a FNN. The main features,
drawbacks and stability conditions of these algorithms are also considered.
5.2 OVERVIEW OF ITERATIVE GRADIENT DESCENT METHODS
Consider the following nonlinear optimization problem:
Minimize the scalar cost function F(x)
subject to x âˆˆRn
There exist two kinds of minimums for F(x): local minimum(see Fig. 5.1)
and global minimum (see Fig. 5.1). The solution xâˆ—is said to be a local
minimum of the function if there exists no better point in its neighbor-
hood or:
F(xâˆ—) â‰¤F(x)
âˆ€x
s.t.
âˆ¥x âˆ’xâˆ—âˆ¥< Ïµ, 0 < Ïµ
(5.1)
on the other hand, xâˆ—is defined as a global minimum if there exists no
better vector in all other possible vectors, i.e.,
F(xâˆ—) â‰¤F(x),
âˆ€x âˆˆRn
(5.2)
Although the main goal of an optimization technique is always to
find the global minimum of the cost function, the local minimum exists

Gradient Descent Methods for Type-2 Fuzzy Neural Networks
47
x
Local minimum
Local minimum
Global minimum
F(x)
Figure 5.1 Local minimum and global minimum in one-dimensional space.
inevitably in the parameter estimation space of the FNN. Thus, it is possible
that the iterative optimization algorithms may trap a local minimum.
5.2.1 Basic Gradient-Descent Optimization Algorithm
Since a cost function is a scalar function having several dimensions, the basic
derivative must be generalized to cover higher dimensional functions. Such a
generalization of the simple derivative to higher dimensions results in a new
operator, namely the gradient. The symbol â€œâˆ‡â€ represents this operator.
Given a vector x âˆˆRn with âˆ‡F(x) Ì¸= 0, the GD update rule to find the
minimum is given as:
x(k + 1) = x(k) âˆ’Î±âˆ‡F(x),
âˆ€Î± > 0
(5.3)
The first-order Taylor expansion of F (x(k + 1)) around x(k) is given by:
F (x(k + 1)) = F(x(k)) + (âˆ‡F(x(k)))T (x(k + 1) âˆ’x(k))
(5.4)
+ o(âˆ¥x(k + 1) âˆ’x(k)âˆ¥)
Considering the update rule as in (5.3), we have:
F(x(k + 1)) = F(x(k)) âˆ’Î±(âˆ‡F(x(k)))Tâˆ‡F(x(k)) + o(Î±âˆ¥âˆ‡F(x(k))âˆ¥)
(5.5)

48
Fuzzy Neural Networks for Real Time Control Applications
The term Î±(âˆ‡F(x(k)))Tâˆ‡F(x(k)) dominates the higher-order terms
o(Î±âˆ¥âˆ‡F(x(k))âˆ¥) near zero [1]. Hence, for small variations, F(x(k + 1)) <
F(x(k)). Consequently, (5.3) guides the input variable x toward a minima
for F(x).
Ex 1: As an example, consider the following nonlinear unconstrained
optimization problem:
F(x1, x2) = 25
30x4
1 âˆ’1
6x3
1 âˆ’7
3x2
1 + 2
3x1 + 25
30x4
2 âˆ’1
6x3
2 âˆ’7
3x2
2 + 2
3x2
(5.6)
The three-dimensional plot of this function is depicted in Fig. 5.2(a).
The contour plot of this function is also illustrated in Fig. 5.2(b). As can be
seen from these figures, this problem has three local minimums and a global
minimum, which are marked in the figures. The gradient of this function
is derived as follows:
âˆ‡F(x1, x2) =
 10
3 x3
1 âˆ’1
2x2
1 âˆ’14
3 x1 + 2
3
10
3 x3
2 âˆ’1
2x2
2 âˆ’14
3 x2 + 2
3

(5.7)
In order to optimize this function, the following iterative equation can
be used:
 x1(k + 1)
x2(k + 1)

=
 x1(k)
x2(k)

âˆ’Î±
 10
3 x3
1(k) âˆ’1
2x2
1(k) âˆ’14
3 x1(k) + 2
3(k)
10
3 x3
2(k) âˆ’1
2x2
2(k) âˆ’14
3 x2(k) + 2
3(k)

(5.8)
Figure 5.3 shows the evolution of the input vector when the initial
conditions are selected as x(0) = [0, âˆ’0.2]T and the learning rate Î± is
selected as 0.1. As can be seen from this figure, the algorithm converges to
the global minimum of the function. This is a successful implementation
of the GD on a complex nonlinear multi-variable function. However,
in order to show the sensitivity of GD to initial conditions, another set
of experiments are carried out. In the new experiments, three different
initial conditions are selected as being equal to [0.5, 0.2]T, [2, âˆ’2]T, and
[âˆ’0.5, 0.5]T and the learning rate Î± is selected as being equal to 0.1.
The results of the simulations are depicted in Fig. 5.4. As can be seen

Gradient Descent Methods for Type-2 Fuzzy Neural Networks
49
10
(a)
5
0
F(x1,x2)
â€“5
2
0
â€“2
â€“2
0
x2
x1
2
Global minimum
Local minimum
Local minimum
Local minimum
âˆ’2
(b)
âˆ’1
0
1
2
âˆ’2
âˆ’1.5
âˆ’1
âˆ’0.5
0
0.5
1
1.5
2
Figure 5.2 (a) Three-dimensional plot of the function in Ex. 1, (b) contour plot of the
function given in Ex. 1.
from Fig. 5.4, the GD algorithm gets stuck in local minimums for all the
aforementioned initial conditions. Thus, the conclusion of these simulations
is that GD algorithm is quite sensitive to the initial conditions.
Ex 2: In order to show the effect of the selection of the learning rate,
another experiment is conducted. Consider the following one-dimensional
unconstraint function to be optimized:
F(x) = x2
(5.9)

50
Fuzzy Neural Networks for Real Time Control Applications
Global minimum
Local minimum
Local minimum
Local minimum
Appropriate initial conditions
âˆ’2
âˆ’1
0
1
2
âˆ’2
âˆ’1.5
âˆ’1
âˆ’0.5
0
0.5
1
1.5
2
Figure 5.3 Result ofapplying GD toF(x)whenappropriateinitial conditions areselected
in Ex 1.
Global minimum
Local minimum
Local minimum
Local minimum
âˆ’2
âˆ’1
0
1
2
âˆ’2
âˆ’1.5
âˆ’1
âˆ’0.5
0
0.5
1
1.5
2
Figure 5.4 Result of applying GD to F(x) when inappropriate initial conditions are
selected in Ex 1.

Gradient Descent Methods for Type-2 Fuzzy Neural Networks
51
The gradient of this function is achieved as follows:
âˆ‡F(x) = 2x
(5.10)
Hence, the GD algorithm update rule can be obtained as follows:
x(k + 1) = x(k) âˆ’2Î±x(k)
(5.11)
In order to investigate the effect of the learning rate selection on the
optimization process, two experiments are carried out with two different
learning rates. In the first case, the initial conditions are selected as being
equal to 2.5 and the learning rate is selected as Î± = 0.3. It is observed
that the minimum of the function can be found using the GD algorithm.
On the other hand, in the second experiment, the same initial condition
is used with the learning rate of 1.1. Figure 5.5 shows the evolution of
the solutions when the learning rate is equal to 1.1. As can be seen from
the figure, even if the GD algorithm gives the correct moving direction to
optimize the function, the algorithm diverges and fails to find the optimum
of the function since the step-size is chosen too large. In this chapter, we
do not provide another experiment showing the disadvantage of choosing
âˆ’6
âˆ’4
âˆ’2
0
2
4
6
8
0
10
20
30
40
50
60
70
Initial condition
Point #2
Point #3
Point #4
Point #5
Figure 5.5 Result of applying GD to F(x) = x2 when inappropriate value is selected for
the learning rate in Ex. 2.

52
Fuzzy Neural Networks for Real Time Control Applications
a very small learning rate. However, it is a well-known fact that such a very
small selection of the learning rate increases the probability of entrapment
in a local minimum. The overall finding is that neither a too small nor a too
large learning rate is appropriate for the GD algorithm.
5.2.2 Newton and Gauss-Newton Optimization Algorithms
Different from the GD algorithm where we use only the first derivatives,
Newtonâ€™s optimization method is a second-order optimization method that
also uses the second derivative, Hessian, of the function. Since the Hessian
is taken into the account in this algorithm, it is expected that it gives
better results when compared to an algorithm that only uses first derivatives.
Moreover, since no learning rate exists in its original version, it encounters
fewer instability issues. The second-order Taylor expansion of F(x) around
x(k) is given as:
F(x) â‰ˆG(x) = F(x(k)) + (âˆ‡F(x(k)))T(x âˆ’x(k))
+ 1
2(x âˆ’x(k))TH(x(k))(x âˆ’x(k))
(5.12)
where H(x) is the Hessian matrix of F(x). Not only does (5.12) give the
first order approximate of F(x) but its second-order term is also taken into
account. Thus, the approximation is more exact when it is compared to
the first-order approximation. The second-order Taylor expansion of F(x),
which is represented by (5.12), is a quadratic function and it is possible to
find its minima by solving âˆ‡G(x) = 0. The gradient of G(x) is obtained as
follows:
âˆ‡G(x) = âˆ‡F(x(k)) + H(x(k))(x âˆ’x(k))
(5.13)
The optimum step size for minimizing G(x) is obtained as the solution
of the following equation:
âˆ‡G(x) = âˆ‡F(x(k)) + H(x(k))(x âˆ’x(k)) = 0
(5.14)
which results in:
x(k + 1) = âˆ’Hâˆ’1(x(k))âˆ‡F(x(k))
(5.15)

Gradient Descent Methods for Type-2 Fuzzy Neural Networks
53
As can be seen from (5.15), there exists no learning rate in the formula
resulting in this algorithm having a suggestion for this parameter. In other
words, if we compare (5.15) with (5.3), it can be observed that the Hessian
matrix acts as a learning rate, which is a matrix rather than a scalar value.
However, the calculation of the Hessian matrix is a tedious work, especially
when it comes to the estimation of the parameters of the premise part of
a FNN. Moreover, the calculation of the inverse of the Hessian matrix
is needed, which may have a high dimension. The complexity of matrix
inversion may be as high as O(n3), in which n is the number of the rows of
the matrix to be inverted.
Since higher-order terms are neglected in the Newton optimization
algorithm, it may be modified as follows to improve its robustness:
x(k + 1) = âˆ’Î³ (H(x(k)))âˆ’1âˆ‡F(x(k)), 0 < Î³ < 1
(5.16)
Since the calculation of the Hessian matrix is complex as mentioned
earlier, this matrix is approximated in order to reduce the complexity of the
Newton optimization method.
Consider the cost function F(x) to be in the following form:
F(x) = eT(x)e(x)
(5.17)
where e(x) âˆˆRmÃ—1 is a vectorial function of x that represents the difference
between the desired value of the FNN and its output. The gradient of F(x)
is derived as follows:
âˆ‡F(x) = JT(x)e(x)
(5.18)
in which J is the Jacobian matrix and is defined as follows:
J(x) =
âŽ¡
âŽ¢âŽ¢âŽ¢âŽ¢âŽ£
âˆ‚e1(x)
âˆ‚x1
âˆ‚e1(x)
âˆ‚x2
Â· Â· Â·
âˆ‚e1(x)
âˆ‚xn
âˆ‚e2(x)
âˆ‚x1
âˆ‚e2(x)
âˆ‚x2
Â· Â· Â·
âˆ‚e2(x)
âˆ‚xn
...
...
...
...
âˆ‚em(x)
âˆ‚x1
âˆ‚em(x)
âˆ‚x2
Â· Â· Â·
âˆ‚em(x)
âˆ‚xn
âŽ¤
âŽ¥âŽ¥âŽ¥âŽ¥âŽ¦
Furthermore, the Hessian matrix is obtained as follows:
âˆ‡2F(x) = JT(x)J(x) + S(x)
(5.19)

54
Fuzzy Neural Networks for Real Time Control Applications
where:
S(x) =
m

i=1
ei(x)âˆ‡2ei(x)
and ei(x) is the ith element of the vector e(x). In this way, the terms that
include the first-order gradient are separated from the terms that include the
second-order gradient. In other words, JT(x)J(x) includes the first-order
gradient while S(x) capsulates the second-order gradient terms. Since it
is difficult to calculate the second-order gradient term S(x), this term
may be completely neglected. Hence, the update rule of (5.16) can be
approximated as:
x(k + 1) = x(k) âˆ’[JT(x(k))J(x(k))]âˆ’1JT(x(k))e(x(k))
(5.20)
This iterative method to find the optimal value of F(x) is called the
Gauss-Newton optimization method. It is further possible to derive the
formulation of the Gauss-Newton optimization method using a different
approach. Consider the same cost function as for the previous approach as
follows:
F(x) = eT(x)e(x)
(5.21)
It is possible to use the first-order Taylor expansion for e(x) around x(k) as
follows:
e(x) â‰ˆe(x(k)) + (J(x(k)))T(x âˆ’x(k))
(5.22)
where J(x) is Jacobian matrix. The term x(k +1) is the solution to the cost
function defined by (5.21). Hence, we have the following equation:
x(x + 1) = arg min
xâˆˆRneT(x)e(x)
â‰ˆmin
xâˆˆRn

e(x(k)) + (J(x(k)))T(x âˆ’x(k))
T
Ã—

e(x(k)) + (J(x(k)))T(x âˆ’x(k))

= min
xâˆˆRn{eT(x(k))e(x(k))
+ 2(x âˆ’x(k))TJ(x(k))e(x(k))
+ (x âˆ’x(k))TJ(x(k))(J(x(k)))T(x âˆ’x(k))
(5.23)

Gradient Descent Methods for Type-2 Fuzzy Neural Networks
55
The above equation is a quadratic equation and its optimal point can be
easily found by making its gradient equal to zero. Hence, its minimum point
is obtained by the following equation:
x(k + 1) = x(k) âˆ’[J(x(k))(J(x(k)))T]âˆ’1J(x(k))e(x(k))
(5.24)
5.2.3 LM Algorithm
As can be seen from (5.24), it is possible that J(x(k))JT(x(k)) may become
singular or close to singular. In order to solve this problem, the original
update rule for the Gauss-Newton algorithm is modified as follows:
x(k + 1) = x(k) âˆ’[J(x(k))JT(x(k)) + Î¼(k)I]âˆ’1J(x(k))e(x(k))
(5.25)
where I is a identity matrix having an appropriate size and Î¼(k) has a positive
value, which avoids the inversion of a singular matrix. As can be seen from
(5.25), if Î¼(k) is chosen as equal to zero, this equation is quite similar to
the one in the Gauss-Newton case. Moreover, if a large value for Î¼(k)
is chosen, J(x(k))JT(x(k)) can be ignored and (5.25) changes to a simple
GD algorithm. Therefore, it is suggested to begin with a small Î¼(k) to use
Gauss-Newton and speed-up convergence. In this way, the algorithm will
be less sensitive to initial values considered for x(k). After some iterations,
it is suggested to increase Î¼(k) to avoid singularity. The pseudo code for
the LM algorithm, which includes an appropriate selection for Î¼, can be
summarized as in Fig. 5.6 [2].
5.2.4 Gradient Descent Algorithm with an Adaptive
Learning Rate
It is possible to consider having an adaptive learning rate in the GD
algorithm. Possible guidelines for increasing or decreasing the learning rate
are as follows:
1. If a sign change in the solution does not occur in multiple consecutive
iterations, the learning rate is possibly chosen too small, resulting in the
algorithm converging too slow. In this case, it is wise to increase the
value of the learning rate to increase the convergence speed.
2. If the sign of âˆ‚F/âˆ‚xj changes in multiple consecutive iterations, the
algorithm is possibility oscillating around a local solution. In this case, it
is wise to decrease the learning rate.
The adaptive learning rate is mostly used in batch training algorithms [3].

56
Fuzzy Neural Networks for Real Time Control Applications
Initialize the weights and the
parameter m(k) (m=0.01 is
appropriate)
Compute the sum of squared
errors over all inputs F(x(k))
Compute the Jacobian
matrix J(x(k))
Solve (5.25) to obtain the new
solutions x(k+1)
x(k+1) is accepted and
m(k+1) =bm(k)
0 < b< 1
m(k)=m(k)/b
0<b<1
IF F(x(k+1))<F(x(k)
Yes
No
Figure 5.6 Flowchart of the LM algorithm.
5.2.5 GD Algorithm with a Momentum Term
Another possible solution to decreasing the risk of instability is using a
momentum term. The GD algorithm with a momentum term is defined as
follows:
x(k) = x(k + 1) âˆ’x(k) = âˆ’Î±âˆ‚F(x(k))
âˆ‚x(k)
+ Î²x(k âˆ’1)
(5.26)
In order to see how the momentum term is affecting the training
algorithm, an analysis is done here. If we consider that x is a one-dimensional
variable, we can take the Z-transform of x(k), so that the following
equation is derived:

Gradient Descent Methods for Type-2 Fuzzy Neural Networks
57
X(z) = âˆ’Î±âˆ‚F(x(k))
âˆ‚x(k)
+ zâˆ’1Î²X(z)
(5.27)
Therefore:
X(z) =
âˆ’Î± âˆ‚F(x(k))
âˆ‚x(k)
1 âˆ’Î²zâˆ’1
(5.28)
in which
1
1âˆ’Î²zâˆ’1 acts as a low-pass filter that filters âˆ’Î± âˆ‚F(x(k))
âˆ‚x(k)
and reduces
oscillations in x(k) considerably. Furthermore, in order to have a stable filter,
the value of Î² must be selected as 0 < Î² < 1.
5.3 GRADIENT DESCENT BASED LEARNING ALGORITHMS
FOR TYPE-2 FUZZY NEURAL NETWORKS
5.3.1 Consequent Part Parameters
As mentioned earlier, the output of an interval T2FNN is computed as
follows:
yN = q
N

r=1
fr Ëœw + (1 âˆ’q)
N

r=1
fr Ëœw
(5.29)
where:
fr = arixi + br,
âˆ€r = 1, 2, . . . , N,
and
i = 1, 2, . . . , n
As can be seen from the equation above, the output of a FNN, either
type-1 or type-2, is linear with respect to the parameters of the consequent
part of the FNN. Hence, it is quite easy to derive the GD-based adaptation
laws for the parameters of the consequent part. The cost function is
rewritten for ease of reference:
E = 1
2e2 = 1
2(y âˆ’yN)2
(5.30)
The GD-based parameter tuning rules for the consequent part are
derived as follows:
ari(k + 1) = ari(k) + Î±

qËœw + (1 âˆ’q) Ëœw

xi(k)e(k), 0 < Î±
(5.31)

58
Fuzzy Neural Networks for Real Time Control Applications
br(k + 1) = br(k) + Î±

qËœw + (1 âˆ’q) Ëœw

e(k), 0 < Î±
(5.32)
q(k + 1) = q(k) + Î±
 N

r=1
fr( Ëœw âˆ’Ëœw)

e(k), 0 < Î±
(5.33)
where Î± is the learning rate for the consequent part parameters. The
parameter q has a bounded value by definition, and it should be kept in the
interval [0, 1]. As can be seen from these equations, GD-based adaptation
rules for the parameters of the consequent part are straightforward and
have closed form. Note that the learning rates of the parameters of the
consequent part do not need to be equal and thus may differ from one
parameter to another. It should also be noted that in some studies, the GD-
based training method is used to tune only the parameters of the consequent
part, and the parameters of the the antecedent part are kept constant. Since
the sensitivity of the output of a FNN to the parameters of the consequent
part are more than that of antecedent part, such approaches may result in
fairly accurate performance, but with fast training benefiting from simple
adaptation laws. However, in order to design a more precise FNN, the
parameters of the antecedent part should also be tuned. Unfortunately, this is
a more complex task when compared to the tuning of the consequent part.
5.3.2 Premise Part Parameters
In the previous section, it was already stated that the calculation of the
partial derivative of the T2FNN output with respect to the parameters of the
antecedent part is more difficult than that of the consequent part. Since there
exists a normalization layer in the structure of a T2FNN, the parameters of
the premise part of each MF, no matter if it has participated in a rule or
not, exists at least in the denominator of the output of the normalization
layer. Hence, there are no closed forms for computing the partial derivative
of the T2FNN output with respect to its premise part. In this chapter, the
adaptation laws for a T2FNN, which has only two inputs, is considered.
Consider a T2FNN with two inputs and I number of MFs for the first
input and J number of MFs for the second input. The upper MFs for the
first input x1 are considered to be Î¼11(x1), Î¼12(x1), . . ., and Î¼1I(x1), and
its corresponding lower MFs are considered to be Î¼11(x1), Î¼12(x1), . . . and
Î¼1I(x1). The upper MFs for the second input (x2) are considered to be
Î¼21(x2), Î¼22(x2), . . ., and Î¼2I(x2), and its corresponding lower MFs are
considered to be Î¼21(x2), Î¼22(x2), . . ., and Î¼2J(x2). The terms Î¼11(x2),
Î¼11(x1) are considered to be defined as follows:

Gradient Descent Methods for Type-2 Fuzzy Neural Networks
59
Î¼11(x1) = exp

âˆ’
x1 âˆ’c11
Ïƒ 11
2
(5.34)
Î¼11(x1) = exp

âˆ’
x1 âˆ’c11
Ïƒ 11
2
(5.35)
The upper and the lower rules of the system are derived as follows:
w1 = Î¼11(x1)Î¼21(x2), . . . , w1Ã—J = Î¼11(x1)Î¼2J(x2)
w1Ã—J+1 = Î¼12(x1)Î¼21(x2), . . . , w2Ã—J = Î¼12(x1)Î¼2J(x2)
...
w(Iâˆ’1)Ã—J+1 = Î¼1I(x1)Î¼21(x2), . . . , wIÃ—J = Î¼1I(x1)Î¼2J(x2)
(5.36)
and
w1 = Î¼11(x1)Î¼21(x2), . . . , w1Ã—J = Î¼11(x1)Î¼2J(x2)
w1Ã—J+1 = Î¼12(x1)Î¼21(x2), . . . , w2Ã—J = Î¼12(x1)Î¼2J(x2)
...
w(Iâˆ’1)Ã—J+1 = Î¼1I(x1)Î¼21(x2), . . . , wIÃ—J = Î¼1I(x1)Î¼2J(x2)
(5.37)
The output of the T2FNN is obtained as follows:
yN = q
N=IÃ—J

r=1
fr Ëœwr + (1 âˆ’q)
N=IÃ—J

r=1
fr Ëœwr
(5.38)
where:
Ëœwr =
wr
N
l=1 wl
, Ëœwr =
wr
N
l=1 wl
(5.39)
The chain rule to obtain the partial derivative of F = 1
2e2 with respect
to a sample parameter c1 is given in (5.40). F is the cost function to be
optimized, which is itself a function of e = y âˆ’yN, in which yN is the
output of T2FNN and y is its corresponding target value. This partial
derivative is obtained as follows:

60
Fuzzy Neural Networks for Real Time Control Applications
âˆ‚F
âˆ‚c11
= âˆ‚F
âˆ‚e . âˆ‚e
âˆ‚yN
.âˆ‚yN
âˆ‚c11
(5.40)
and further:
âˆ‚yN
âˆ‚c11
=
N

r=1
J
j=1

âˆ‚yN
âˆ‚Ëœwr
âˆ‚Ëœwr
âˆ‚wj
âˆ‚wj
âˆ‚Î¼11
âˆ‚Î¼11
âˆ‚c11

+
N

r=1
J
j=1

âˆ‚yN
âˆ‚Ëœwr
âˆ‚Ëœwr
âˆ‚wj
âˆ‚wj
âˆ‚Î¼11
âˆ‚Î¼11
âˆ‚c11

(5.41)
where:
âˆ‚yN
âˆ‚Ëœwr
= fr, r = 1, . . . , N
âˆ‚wi
âˆ‚Î¼11
= Î¼2j, j = 1, . . . , J
âˆ‚Î¼11
âˆ‚c11
= âˆ’(exp(âˆ’(x1 âˆ’c11)2/(2Ïƒ 2
11))(2c11 âˆ’2x1))/(2Ïƒ 2
11)
(5.42)
and similarly, we have the following equations:
âˆ‚yN
âˆ‚Ëœwr
= fr, r = 1, . . . , N
âˆ‚wi
âˆ‚Î¼11
= Î¼2j, j = 1, . . . , J
âˆ‚Î¼11
âˆ‚c11
= âˆ’(exp(âˆ’(x1 âˆ’c11)2/(2Ïƒ 2
11))(2c11 âˆ’2x1))/(2Ïƒ 2
11)
(5.43)
However, the partial derivatives of Ëœwr, r
=
1, . . . , N w.r.t. wj,
j = 1, . . . , J and the partial derivatives of Ëœwr, r = 1, . . . , N w.r.t. wj,
j = 1, . . . , J are multi-rule. If wj appears only in the denominator of Ëœwr,
the partial derivatives of Ëœwrâ€™s w.r.t. wjâ€™s are derived as follows:
âˆ‚Ëœwr
âˆ‚wj
= âˆ’
Ëœwr
N
l=1 wl
(5.44)
However, in some cases wj appears both in the numerator and denominator
of Ëœwr. In these cases, the partial derivatives of Ëœwrâ€™s w.r.t. wjâ€™s are calculated as
follows:

Gradient Descent Methods for Type-2 Fuzzy Neural Networks
61
âˆ‚Ëœwr
âˆ‚wj
= âˆ‚Ëœwr
âˆ‚wr
= 1 âˆ’Ëœwr
N
i=l wl
(5.45)
Similar calculations apply for the partial derivatives of Ëœwr w.r.t. wj. This
ends the calculation of the partial derivative of the T2FNN output w.r.t. c11,
which is one of the parameters of the antecedent part of the T2FNN. As can
be seen from these calculations, the partial derivative of all the normalized
rules w.r.t. c11 exists in the calculation of the partial derivative of yN w.r.t.
c11, which is generally non-zero. This is the main reason the computation
of the partial derivative with respect to the parameters of the consequent
part is hard to derive. Interested readers are encouraged to calculate the rest
of the partial derivatives of yN w.r.t. the parameters of the antecedent part
and complete the GD learning rules.
5.3.3 Variants of the Back-Propagation Algorithm for Training
the T2FNNs
Having completed the calculation of the partial derivative of yN w.r.t. the
parameters of T2FNN, any algorithm such as basic GD, momentum term
GD and LM algorithms may be applied to the parameter update rules
of a T2FNN. However, because of the high complexity of the Newtonâ€™s
optimization algorithm, it has never been applied to the training of T2FNN.
5.4 STABILITY ANALYSIS
GD-based training methods suffer from lack of stability analysis. In this
section, a stability analysis of the GD algorithm is considered. However,
note that the Taylor expansion with only the first-order term is considered,
which makes the analysis local and valid for only small changes in the
solution. Similar stability analysis for the GD training of FNNs can be found
in Refs. [4, 5].
5.4.1 Stability Analysis of GD for Training of T2FNN
In order to analyze the stability of the GD-based training algorithm, the
following discrete time Lyapunov function is considered:
V(k) = e2(k)
(5.46)

62
Fuzzy Neural Networks for Real Time Control Applications
By using the Lyapunov theory, in order to prove the stability of a discrete
time system, it is required that its time difference is negative. Thus, in the fol-
lowing equation, the time difference of the Lyapunov function is considered:
V(k) = V(k + 1) âˆ’V(k) = e2(k + 1) âˆ’e2(k)
(5.47)
By using the first-order Taylor expansion of e(k), we have the following
equation:
V(k) = e(k)

e(k) + 1
2e(k)

=
âˆ‚e(k)
âˆ‚Ïƒ
T
Ïƒ +
âˆ‚e(k)
âˆ‚Ïƒ
T
Ïƒ +
âˆ‚e(k)
âˆ‚c
T
c
+
âˆ‚e(k)
âˆ‚f
T
f

Ã—

e(k) + 1
2
âˆ‚e(k)
âˆ‚Ïƒ
T
Ïƒ
+1
2
âˆ‚e(k)
âˆ‚Ïƒ
T
Ïƒ + 1
2
âˆ‚e(k)
âˆ‚c
T
c + 1
2
âˆ‚e(k)
âˆ‚f
T
f

(5.48)
Therefore:
V(k) = âˆ’1
2e2(k)
âˆ‚yNN(k)
âˆ‚Ïƒ
T
Î·Ïƒ
âˆ‚yNN(k)
âˆ‚Ïƒ
+
âˆ‚yNN(k)
âˆ‚Ïƒ
T
Î·Ïƒ
âˆ‚yNN(k)
âˆ‚Ïƒ
+
âˆ‚yNN(k)
âˆ‚c
T
Î·c
âˆ‚yNN(k)
âˆ‚c
+
âˆ‚yNN(k)
âˆ‚f
T
Î·f
âˆ‚yNN(k)
âˆ‚f

Ã—

2âˆ’
âˆ‚yNN(k)
âˆ‚Ïƒ
T
Î·Ïƒ
âˆ‚yNN(k)
âˆ‚Ïƒ
âˆ’
âˆ‚yNN(k)
âˆ‚Ïƒ
T
Î·Ïƒ
âˆ‚yNN(k)
âˆ‚Ïƒ
âˆ’
âˆ‚yNN(k)
âˆ‚c
T
Î·c
âˆ‚yNN(k)
âˆ‚c
âˆ’
âˆ‚yNN(k)
âˆ‚f
T
Î·f
âˆ‚yNN(k)
âˆ‚f

(5.49)

Gradient Descent Methods for Type-2 Fuzzy Neural Networks
63
and further:
V(k) = âˆ’1
2e2(k)

Î·Ïƒ

âˆ‚yNN(k)
âˆ‚Ïƒ

2
+ Î·Ïƒ

âˆ‚yNN(k)
âˆ‚Ïƒ

2
+ Î·c

âˆ‚yNN(k)
âˆ‚c

2
+ Î·f

âˆ‚yNN(k)
âˆ‚f

2
Ã—

2 âˆ’Î·Ïƒ

âˆ‚yNN(k)
âˆ‚Ïƒ

2
âˆ’Î·Ïƒ

âˆ‚yNN(k)
âˆ‚Ïƒ

2
âˆ’Î·c

âˆ‚yNN(k)
âˆ‚c

2
âˆ’Î·f

âˆ‚yNN(k)
âˆ‚f

2
(5.50)
which implies that in order to have V(k) < 0, we must have the following
inequalities:
0 < Î·Ïƒ <
1
2
âˆ‚yNN
âˆ‚Ïƒ

2 , 0 < Î·Ïƒ <
1
2
âˆ‚yNN
âˆ‚Ïƒ

2
0 < Î·c <
1
2
âˆ‚yNN
âˆ‚c

2 , 0 < Î·f <
1
2
âˆ‚yNN
âˆ‚f

2
(5.51)
The above equations give an adaptive selection for the adaptive learning
rates that stabilizes the GD-based learning algorithm.
5.4.2 Stability Analysis of the LM for Training of T2FNN
The stability analysis of LM method for the training of FNN has been
previously considered in Ref. [6]. However, the proof of the stability in
Ref. [6] requires the eigenvalues of J(x(t))JT(x(t)) to be computed, which
is a very tedious work. The complexity of the eigenvalues computation
of the matrices is at least 2n3/3 + O(n2). Alternatively, the benefit of the
stability analysis in this chapter is that it does not require any eigenvalue
to be computed, and hence it is much simpler. As a beginning point, the
following Lyapunov function is considered:
V(k) = e2(k)
(5.52)

64
Fuzzy Neural Networks for Real Time Control Applications
In order to have a stable training algorithm, V(k) must be less than
zero. The following equation is achieved for V(k):
V(k) = V(k + 1) âˆ’V(k) = e2(k + 1) âˆ’e2(k)
(5.53)
By using the first-order Taylor expansion of e(k), we have the following
equation:
V(k) = e(k)

e(k) + 1
2e(k)

â‰ˆ
âˆ‚e(k)
âˆ‚Ïƒ
T
Ïƒ +
âˆ‚e(k)
âˆ‚Ïƒ
T
Ïƒ +
âˆ‚e(k)
âˆ‚c
T
c
+
âˆ‚e(k)
âˆ‚f
T
f

Ã—

e(k) + 1
2
âˆ‚e(k)
âˆ‚Ïƒ
T
Ïƒ
+1
2
âˆ‚e(k)
âˆ‚Ïƒ
T
Ïƒ + 1
2
âˆ‚e(k)
âˆ‚c
T
c + 1
2
âˆ‚e(k)
âˆ‚f
T
f

(5.54)
so that:
âˆ‚e(k)
âˆ‚Ïƒ
T
Ïƒ =
âˆ‚e(k)
âˆ‚Ïƒ
T 
âˆ‚e(k)
âˆ‚Ïƒ
âˆ‚e(k)
âˆ‚Ïƒ
T
+ Î¼ÏƒI
âˆ’1  âˆ‚e
âˆ‚Ïƒ

e(k)
=
âˆ‚yNN(k)
âˆ‚Ïƒ
T 
âˆ‚yNN(k)
âˆ‚Ïƒ
âˆ‚yNN(k)
âˆ‚Ïƒ
T
+Î¼Ïƒ I
âˆ’1 âˆ‚yNN
âˆ‚Ïƒ

e(k)
(5.55)
It is possible to define an auxiliary variable 
 as:

 =

âˆ‚yNN(k)
âˆ‚Ïƒ
âˆ‚yNN(k)
âˆ‚Ïƒ
T
+ Î¼ÏƒI
âˆ’1
(5.56)

Gradient Descent Methods for Type-2 Fuzzy Neural Networks
65
Lemma 5.1 (Matrix Inversion Lemma). Let A, C, and Câˆ’1 + DAâˆ’1B
be non-singular square matrices. Then A + BCD is invertible, and [7]:
(A + B C D)âˆ’1 = Aâˆ’1 âˆ’Aâˆ’1B(Câˆ’1 + DAâˆ’1B)âˆ’1D Aâˆ’1
(5.57)
Using Lemma 5.1. it is possible to write 
 as follows:

 = Î¼âˆ’1
Ïƒ I âˆ’Î¼âˆ’1
Ïƒ
âˆ‚yNN
âˆ‚Ïƒ

I + Î¼Ïƒ
âˆ‚yNN
âˆ‚Ïƒ
T âˆ‚yNN
âˆ‚Ïƒ
âˆ’1 âˆ‚yNN
âˆ‚Ïƒ
T
Î¼âˆ’1
Ïƒ
= Î¼âˆ’1
Ïƒ I âˆ’âˆ‚yNN
âˆ‚Ïƒ

Î¼Ïƒ I +
âˆ‚yNN
âˆ‚Ïƒ
T âˆ‚yNN
âˆ‚Ïƒ
âˆ’1 âˆ‚yNN
âˆ‚Ïƒ
T
Î¼âˆ’1
Ïƒ
(5.58)
and
âˆ‚e(k)
âˆ‚Ïƒ
T
Ïƒ =
âˆ‚yNN
âˆ‚Ïƒ
T

âˆ‚yNN
âˆ‚Ïƒ e(k)
= Î¼âˆ’1
Ïƒ

âˆ‚yNN
âˆ‚Ïƒ

2
e(k) âˆ’Î¼âˆ’1
Ïƒ

âˆ‚yNN
âˆ‚Ïƒ

4

Î¼Ïƒ +

âˆ‚yNN
âˆ‚Ïƒ

2âˆ’1
e(k)
(5.59)

âˆ‚e(k)
âˆ‚Ïƒ
T
Ïƒ is obtained as follows:
âˆ‚e(k)
âˆ‚Ïƒ
T
Ïƒ = Î¼âˆ’1
Ïƒ

âˆ‚yNN
âˆ‚Ïƒ

2
e(k) âˆ’Î¼âˆ’1
Ïƒ

âˆ‚yNN
âˆ‚Ïƒ

4

Î¼Ïƒ +

âˆ‚yNN
âˆ‚Ïƒ

2âˆ’1
e(k)
(5.60)
Similarly we have:
âˆ‚e(k)
âˆ‚c
T
c = Î¼âˆ’1
c

âˆ‚yNN
âˆ‚c

2
e(k) âˆ’Î¼âˆ’1
c

âˆ‚yNN
âˆ‚c

4

Î¼c +

âˆ‚yNN
âˆ‚c

2âˆ’1
e(k)
(5.61)

66
Fuzzy Neural Networks for Real Time Control Applications
and:
âˆ‚e(k)
âˆ‚f
T
f = Î¼âˆ’1
f

âˆ‚yNN
âˆ‚f

2
e(k) âˆ’Î¼âˆ’1
f

âˆ‚yNN
âˆ‚f

4

Î¼f +

âˆ‚yNN
âˆ‚f

2âˆ’1
e(k)
(5.62)
Therefore, the following equation is obtained for the time difference of the
Lyapunov function V(k):
V(k)=âˆ’1
2e2(k)

Î¼âˆ’1
Ïƒ

âˆ‚yNN
âˆ‚Ïƒ

2
âˆ’Î¼âˆ’1
Ïƒ

âˆ‚yNN
âˆ‚Ïƒ

4 
Î¼Ïƒ +

âˆ‚yNN
âˆ‚Ïƒ

2âˆ’1
+ Î¼âˆ’1
Ïƒ

âˆ‚yNN
âˆ‚Ïƒ

2
âˆ’Î¼âˆ’1
Ïƒ

âˆ‚yNN
âˆ‚Ïƒ

4 
Î¼Ïƒ +

âˆ‚yNN
âˆ‚Ïƒ

2âˆ’1
+ Î¼âˆ’1
c

âˆ‚yNN
âˆ‚c

2
âˆ’Î¼âˆ’1
c

âˆ‚yNN
âˆ‚c

4 
Î¼c +

âˆ‚yNN
âˆ‚c

2âˆ’1
+Î¼âˆ’1
f

âˆ‚yNN
âˆ‚f

2
âˆ’Î¼âˆ’1
f

âˆ‚yNN
âˆ‚f

4 
Î¼f +

âˆ‚yNN
âˆ‚f

2âˆ’1âŽž
âŽ 
Ã—
âŽ›
âŽ2 âˆ’Î¼âˆ’1
Ïƒ

âˆ‚yNN
âˆ‚Ïƒ

2
+ Î¼âˆ’1
Ïƒ

âˆ‚yNN
âˆ‚Ïƒ

4 
Î¼Ïƒ +

âˆ‚yNN
âˆ‚Ïƒ

2âˆ’1
âˆ’Î¼âˆ’1
Ïƒ

âˆ‚yNN
âˆ‚Ïƒ

2
+ Î¼âˆ’1
Ïƒ

âˆ‚yNN
âˆ‚Ïƒ

4 
Î¼Ïƒ +

âˆ‚yNN
âˆ‚Ïƒ

2âˆ’1
âˆ’Î¼âˆ’1
c

âˆ‚yNN
âˆ‚c

2
+ Î¼âˆ’1
c

âˆ‚yNN
âˆ‚c

4 
Î¼c +

âˆ‚yNN
âˆ‚c

2âˆ’1
âˆ’Î¼âˆ’1
f

âˆ‚yNN
âˆ‚f

2
+ Î¼âˆ’1
f

âˆ‚yNN
âˆ‚f

4 
Î¼f +

âˆ‚yNN
âˆ‚f

2âˆ’1âŽž
âŽ 
(5.63)

Gradient Descent Methods for Type-2 Fuzzy Neural Networks
67
Since:
0 â‰¤

âˆ‚yNN
âˆ‚Ïƒ

2
(5.64)
we have:
Î¼âˆ’1
Ïƒ

âˆ‚yNN
âˆ‚Ïƒ

4
â‰¤

âˆ‚yNN
âˆ‚Ïƒ

2
+ Î¼âˆ’1
Ïƒ

âˆ‚yNN
âˆ‚Ïƒ

4
(5.65)
It is possible to multiply both sides by

Î¼Ïƒ + âˆ¥âˆ‚yNN
âˆ‚Ïƒ âˆ¥2âˆ’1
and we have:
Î¼âˆ’1
Ïƒ

âˆ‚yNN
âˆ‚Ïƒ

4 
Î¼Ïƒ +

âˆ‚yNN
âˆ‚Ïƒ

2âˆ’1
â‰¤Î¼âˆ’1
Ïƒ

âˆ‚yNN
âˆ‚Ïƒ

2
(5.66)
so that:
0 â‰¤Î¼âˆ’1
Ïƒ

âˆ‚yNN
âˆ‚Ïƒ

2
âˆ’Î¼âˆ’1
Ïƒ

âˆ‚yNN
âˆ‚Ïƒ

4 
Î¼Ïƒ +

âˆ‚yNN
âˆ‚Ïƒ

2âˆ’1
(5.67)
and using a similar analysis we get:
0 â‰¤Î¼âˆ’1
Ïƒ

âˆ‚yNN
âˆ‚Ïƒ

2
âˆ’Î¼âˆ’1
Ïƒ

âˆ‚yNN
âˆ‚Ïƒ

4 
Î¼Ïƒ +

âˆ‚yNN
âˆ‚Ïƒ

2âˆ’1
(5.68)
and:
0 â‰¤Î¼âˆ’1
c

âˆ‚yNN
âˆ‚c

2
âˆ’Î¼âˆ’1
c

âˆ‚yNN
âˆ‚c

4 
Î¼c +

âˆ‚yNN
âˆ‚c

2âˆ’1
(5.69)
and:
0 â‰¤Î¼âˆ’1
f

âˆ‚yNN
âˆ‚f

2
âˆ’Î¼âˆ’1
f

âˆ‚yNN
âˆ‚f

4 
Î¼f +

âˆ‚yNN
âˆ‚f

2âˆ’1
(5.70)

68
Fuzzy Neural Networks for Real Time Control Applications
Hence, in order to have a negative time difference for V(k), we must
have:
0 â‰¤1
2 âˆ’Î¼âˆ’1
Ïƒ

âˆ‚yNN
âˆ‚Ïƒ

2
+ Î¼âˆ’1
Ïƒ

âˆ‚yNN
âˆ‚Ïƒ

4 
Î¼Ïƒ +

âˆ‚yNN
âˆ‚Ïƒ

2âˆ’1
(5.71)
and:
0 â‰¤1
2 âˆ’Î¼âˆ’1
Ïƒ

âˆ‚yNN
âˆ‚Ïƒ

2
+ Î¼âˆ’1
Ïƒ

âˆ‚yNN
âˆ‚Ïƒ

4 
Î¼Ïƒ +

âˆ‚yNN
âˆ‚Ïƒ

2âˆ’1
(5.72)
and:
0 â‰¤1
2 âˆ’Î¼âˆ’1
c

âˆ‚yNN
âˆ‚c

2
+ Î¼âˆ’1
c

âˆ‚yNN
âˆ‚c

4 
Î¼c +

âˆ‚yNN
âˆ‚c

2âˆ’1
(5.73)
and:
0 â‰¤1
2 âˆ’Î¼âˆ’1
f

âˆ‚yNN
âˆ‚f

2
+ Î¼âˆ’1
f

âˆ‚yNN
âˆ‚f

4 
Î¼f +

âˆ‚yNN
âˆ‚f

2âˆ’1
(5.74)
Equation (5.71) can be further simplified as:
0 â‰¤

1
2 âˆ’Î¼âˆ’1
Ïƒ

âˆ‚yNN
âˆ‚Ïƒ

2 
Î¼Ïƒ +

âˆ‚yNN
âˆ‚Ïƒ

2
+ Î¼âˆ’1
Ïƒ

âˆ‚yNN
âˆ‚Ïƒ

4
(5.75)
Thus,
0 â‰¤1
2Î¼Ïƒ âˆ’1
2

âˆ‚yNN
âˆ‚Ïƒ

2
(5.76)
and finally the following constraint is obtained for Î¼Ïƒ:

âˆ‚yNN
âˆ‚Ïƒ

2
â‰¤Î¼Ïƒ
(5.77)

Gradient Descent Methods for Type-2 Fuzzy Neural Networks
69
Using a similar analysis the following constraints are obtained for Î¼Ïƒ , Î¼c,
and Î¼f :

âˆ‚yNN
âˆ‚Ïƒ

2
â‰¤Î¼Ïƒ,

âˆ‚yNN
âˆ‚c

2
â‰¤Î¼c,

âˆ‚yNN
âˆ‚f

2
â‰¤Î¼f
(5.78)
5.5 FURTHER READING
Interested readers may refer to [1, 8] for more information and a detailed
discussion of existing nonlinear programming techniques. Different variants
of GD as applied to ANNs may be found in Ref. [3].
5.6 CONCLUSION
This chapter introduces the basic notion and mathematics of GD-based
training methods including its benefits and challenges. The stability analysis
of these methods are also considered using appropriate Lyapunov functions.
GD-based training methods are one of the most commonly used optimiza-
tion algorithms to estimate the parameters of T2FNNs. However, since
these methods are based on the first-order gradient, there is always the
possibility of entrapment in a local minima. The second-order gradient
methods, such as LM, may lessen the probability of entrapment in local
minima. However, they never fully solve the aforementioned problem.
The calculation of a gradient with respect to the parameters of T2FNNs,
especially the parameters of the antecedent part, is also difficult and does
not have a closed form. In addition, since these rules need some matrix
manipulations and partial derivatives, they are not easy to implement in
real-time systems.
REFERENCES
[1] D.P. Bertsekas, Nonlinear Programming, Athena Scientific, Belmont, MA, 1999.
[2] T. Cong Chen, D. Jian Han, F. Au, L. Tham, Acceleration of Levenberg-Marquardt
training of neural networks with variable decay rate, in: Neural Networks, 2003.
Proceedings of the International Joint Conference on, vol. 3, 2003, pp. 1873-1878.
[3] A. Cochocki, R. Unbehauen, Neural Networks for Optimization and Signal Processing,
John Wiley & Sons, Inc., New York, 1993.
[4] C.-H. Lee, C.-C. Teng, Identification and control of dynamic systems using recurrent
fuzzy neural networks, IEEE Trans. Fuzzy Syst. 8 (4) (2000) 349-366.
[5] M.A. Shoorehdeli, M. Teshnehlab, A.K. Sedigh, M.A. Khanesar, Identification using
ANFIS with intelligent hybrid stable learning algorithm approaches and stability analysis
of training methods, Appl. Soft Comput. 9 (2) (2009) 833-850.

70
Fuzzy Neural Networks for Real Time Control Applications
[6]
M.A. Shoorehdeli, M. Teshnehlab, A.K. Sedigh, Training ANFIS as an identifier with
intelligent hybrid stable learning algorithm based on particle swarm optimization and
extended Kalman filter, Fuzzy Sets Syst. 160 (7) (2009) 922-948.
[7] K.J. Ã…strÃ¶m, B. Wittenmark, Adaptive control, Courier Corporation, Mineola, NY,
2013.
[8] M.S. Bazaraa, H.D. Sherali, C.M. Shetty, Nonlinear Programming: Theory and Algo-
rithms, John Wiley & Sons, New York, 2013.

CHAPTER 6
Extended Kalman Filter Algorithm
for the Tuning of Type-2 Fuzzy
Neural Networks
Contents
6.1
Introduction
71
6.2
Discrete Time Kalman Filter
72
6.3
Square-Root Filtering
78
6.4
Extended Kalman Filter Algorithm
78
6.5
Extended Kalman Filter Training of Type-2 Fuzzy Neural Networks
78
6.6
Decoupled Extended Kalman Filter
83
6.7
Conclusion
83
References
84
Abstract
In this chapter, the EKF algorithm is used to optimize the parameters of T2FNNs. The
basic version of KF is an optimal linear estimator where system is linear and is subject
to white uncorrelated noise. However, it is possible to use Taylor expansion to extend
its applications to nonlinear cases. Finally, the decoupled version of the EKF is also
discussed, which is computationally more efficient than EKF to tune the parameters of
T2FNNs.
Keywords
Extended Kalman filter, Kalman filter, Estimation, Type-2 fuzzy neural networks
6.1 INTRODUCTION
KF uses a series of measurements observed over time containing noise, and
produces state estimations that tend to be more precise than those based on a
single measurement alone. This method is an efficient algorithm that needs
to store only the last estimation of the states and the covariance matrix of the
error to estimate the current value of the states of the system. Because of the
algorithmâ€™s recursive nature, it can run in real time using only the present
Fuzzy Neural Networks for Real Time Control Applications
Copyright Â© 2016 Elsevier Inc.
http://dx.doi.org/10.1016/B978-0-12-802687-8.00006-2
All rights reserved.
71

72
Fuzzy Neural Networks for Real Time Control Applications
input measurements and the previously calculated states and its covariance
matrix; no additional past information is required. Although this algorithm
was originated from linear dynamic system theory, where it serves as an
observer for the unmeasured states of the system, it has been successfully
extended to nonlinear estimation applications using Taylor expansion of
the nonlinear functions.
6.2 DISCRETE TIME KALMAN FILTER
Consider a linear discrete time dynamic system in state space form as follows:
Î¸k+1 = Fk+1,kÎ¸k + Gk+1,kuk + wk
yk = HkÎ¸k + vk
(6.1)
where Î¸k âˆˆRn is the kth sample of the state vector of the system, uk âˆˆRm is
the kth input of the system, w âˆˆRn is the process noise, v is the measurement
noise, y âˆˆRl is the measured output of the system, and Fk,k+1, Gk,k+1, and
Hk are the state matrices of the system with appropriate dimensions. The
noise processes wk and vk are considered to be white, uncorrelated and zero-
mean with the following properties:
E[wk] = 0
E[vk] = 0
E[wkwT
j ] = QkÎ´jâˆ’k
E[vkvT
j ] = RkÎ´jâˆ’k
E[vkwT
j ] = 0
We have the following theorems for optimal estimates of the states of the
system:
Theorem 6.1 (Conditional Mean Estimator [1â€“3]). If the stochastic
processes {Î¸k} and {yk} are jointly Gaussian, then the optimum estimate Ë†Î¸k
minimizes the mean-square error Jk, which is defined as follows:
Jk = E[(Î¸k âˆ’Ë†Î¸k)2]
(6.2)
The following is the conditional mean estimator:
Ë†Î¸k = E[Î¸k|y1, y2, . . . , yk].
(6.3)

Extended Kalman Filter Algorithm for the Tuning of Type-2 Fuzzy Neural Networks
73
Theorem 6.2 (Principle of Orthogonality [1â€“3]). Let the stochastic
processes {Î¸k} and {yk} be of zero means, i.e.,
E[Î¸k] = E[yk] = 0,
âˆ€k
(6.4)
If either:
(i) the stochastic process {Î¸k} and {yk} are jointly Gaussian; or
(ii) if the optimal state estimate Ë†Î¸k, which is the solution to the mean-square cost
function is restricted to be a linear function of the observed variables,
then the optimum state estimate Ë†Î¸k, given the observed variables y1, y2, . . . , yk,
is the orthogonal projection of Î¸k on the space spanned by these observations.
The variable Ë†Î¸âˆ’
k is defined as a priori estimate of the states of the system
at kth sample, based on the measurements up to (k âˆ’1)th sample i.e.
Ë†Î¸âˆ’
k = E[Î¸k|ykâˆ’1].
Ë†Î¸âˆ’
k = E[Î¸k|ykâˆ’1]
= E[Fk,kâˆ’1Î¸kâˆ’1 + Gk,kâˆ’1ukâˆ’1 + wkâˆ’1|ykâˆ’1]
(6.5)
Since w is a white noise that is uncorrelated from the output, we have
the following:
Ë†Î¸âˆ’
k = E[Fk,kâˆ’1Î¸kâˆ’1 + Gk,kâˆ’1ukâˆ’1|ykâˆ’1]
(6.6)
The posteriori estimate of the states of the system at the (k âˆ’1)th
sample using the measurement up to the (k âˆ’1)th sample is called Ë†Î¸kâˆ’1
and is defined as Ë†Î¸kâˆ’1 = E[Î¸kâˆ’1|ykâˆ’1]. Therefore, it is possible to use Ë†Î¸kâˆ’1
to estimate a priori estimate for Î¸k using the dynamic equation of the system
as follows:
Ë†Î¸âˆ’
k = Fk,kâˆ’1 Ë†Î¸kâˆ’1 + Gk,kâˆ’1ukâˆ’1
(6.7)
Furthermore, we have the following equations:
Fk,kâˆ’1Î¸kâˆ’1 = Fk,kâˆ’1 Ë†Î¸âˆ’
kâˆ’1 + Fk,kâˆ’1(Î¸kâˆ’1 âˆ’Ë†Î¸âˆ’
kâˆ’1)
= Fk,kâˆ’1 Ë†Î¸âˆ’
kâˆ’1 + Fk,kâˆ’1eâˆ’
kâˆ’1
(6.8)
where eâˆ’
kâˆ’1 is the error of a priori estimate, which is defined as follows:
eâˆ’
kâˆ’1 = Î¸kâˆ’1 âˆ’Î¸âˆ’
kâˆ’1
(6.9)

74
Fuzzy Neural Networks for Real Time Control Applications
The principle of orthogonality requires that eâˆ’
kâˆ’1 and ykâˆ’1 are orthogo-
nal, i.e.,
E[eâˆ’
kâˆ’1|ykâˆ’1] = 0
(6.10)
Moreover, using (6.7), we have the following equation for the error between
a priori estimate Ë†Î¸âˆ’
k and Î¸k:
eâˆ’
k = Î¸k âˆ’Ë†Î¸âˆ’
k
= Fk,kâˆ’1Î¸kâˆ’1 + Gk,kâˆ’1ukâˆ’1 + wkâˆ’1 âˆ’Fk,kâˆ’1 Ë†Î¸kâˆ’1 âˆ’Gk,kâˆ’1ukâˆ’1
= Fk,kâˆ’1ekâˆ’1 + wkâˆ’1
(6.11)
where ekâˆ’1 is defined as ekâˆ’1 = Î¸kâˆ’1 âˆ’Ë†Î¸kâˆ’1. A priori covariance matrix is
defined as follows:
Pâˆ’
k = E[eâˆ’
k (eâˆ’
k )T]
(6.12)
Using (6.11), we have the following equation for Pâˆ’
k :
Pâˆ’
k = E[eâˆ’
k (eâˆ’
k )T]
(6.13)
= E[(Fk,kâˆ’1ekâˆ’1 + wkâˆ’1)(Fk,kâˆ’1ekâˆ’1 + wkâˆ’1)T]
(6.14)
Since ekâˆ’1 and wkâˆ’1 are uncorrelated, we have:
Pâˆ’
k = Fk,kâˆ’1E[ekâˆ’1(ekâˆ’1)T]Fk,kâˆ’1 + E[wkâˆ’1wT
kâˆ’1]
(6.15)
A posteriori covariance matrix at (k âˆ’1)th sample (Pkâˆ’1) is defined as
follows:
Pkâˆ’1 = E[ekâˆ’1eT
kâˆ’1]
(6.16)
Using (6.16) and the properties of w as in (6.2), we have:
Pâˆ’
k = Fk,kâˆ’1Pkâˆ’1Fk,kâˆ’1 + Qkâˆ’1
(6.17)
A posteriori estimate of Î¸k is considered to be a linear combination of
Î¸âˆ’
k and the current measured output of the system yk as follows:
Ë†Î¸k = LkÎ¸âˆ’
k + Kkyk
(6.18)

Extended Kalman Filter Algorithm for the Tuning of Type-2 Fuzzy Neural Networks
75
From the principles of orthogonality, in order to have the optimal
estimate, the error of the estimation Î¸k âˆ’Ë†Î¸k and yk are required to be
orthogonal and hence the following equation holds:
0 = E[Î¸k âˆ’Ë†Î¸k|yk]
= E[Î¸k âˆ’Lk Ë†Î¸âˆ’
k âˆ’KkHkÎ¸k âˆ’Kkvk|yk]
= E[Î¸k âˆ’LkÎ¸k + Ikeâˆ’
k âˆ’KkHkÎ¸k|yk]
= (I âˆ’Lk âˆ’KkHk)E[Î¸k|yk]
(6.19)
where:
eâˆ’
k = Î¸k âˆ’Ë†Î¸âˆ’
k
(6.20)
Since we have E[Î¸k|yk] Ì¸= 0, from (6.19), it is required that:
Lk = I âˆ’KkHk
(6.21)
Using Lk as in (6.21) and the estimator equation of (6.18) and the
dynamic equation of the system (6.1), we have the following:
ek = Î¸k âˆ’Ë†Î¸k
= Î¸k âˆ’(1 âˆ’KkHk) Ë†Î¸âˆ’
k âˆ’Kkyk
= Fk,kâˆ’1Î¸kâˆ’1 + Gk,kâˆ’1ukâˆ’1 + wkâˆ’1 âˆ’(I âˆ’KkHk) Ë†Î¸âˆ’
k âˆ’Kkyk
= Fk,kâˆ’1 Ë†Î¸kâˆ’1 + Gk,kâˆ’1ukâˆ’1 + Fk,kâˆ’1ekâˆ’1 + wkâˆ’1 âˆ’Ë†Î¸âˆ’
k
+ KkHkeâˆ’
k âˆ’Kkvk
(6.22)
Using (6.7), we have:
ek = Fk,kâˆ’1 Ë†Î¸kâˆ’1 + Gk,kâˆ’1ukâˆ’1 + Fk,kâˆ’1ekâˆ’1 + wkâˆ’1
âˆ’(Fk,kâˆ’1 Ë†Î¸kâˆ’1 + Gk,kâˆ’1ukâˆ’1) + KkHkeâˆ’
k âˆ’Kkvk
= Fk,kâˆ’1ekâˆ’1 + wkâˆ’1 âˆ’KkHkeâˆ’
k âˆ’Kkvk
(6.23)
Using (6.11) and (6.23), we have:
ek = Fk,kâˆ’1ekâˆ’1 + wkâˆ’1 âˆ’KkHk(Fk,kâˆ’1ekâˆ’1 + wkâˆ’1)
= (I âˆ’KkHk)Fk,kâˆ’1ekâˆ’1 + (I âˆ’KkHk)wkâˆ’1 âˆ’Kkvk
(6.24)

76
Fuzzy Neural Networks for Real Time Control Applications
Therefore, we have the following equations for a posteriori covariance
matrix:
Pk = E[ekeT
k ]
= (I âˆ’KkHk)Fk,kâˆ’1E[ekâˆ’1eT
kâˆ’1]FT
k,kâˆ’1(I âˆ’KkHk)T
+ (I âˆ’KkHk)E[wkâˆ’1wT
kâˆ’1](I âˆ’KkHk)T
+ KkE[vkvT
k ]KT
k
= (I âˆ’KkHk)Fk,kâˆ’1Pkâˆ’1FT
k,kâˆ’1(I âˆ’KkHk)T
+ (I âˆ’KkHk)Qkâˆ’1(I âˆ’KkHk)T
+ KkRkKT
k
(6.25)
Furthermore, the state estimation error Î¸k âˆ’Ë†Î¸k and the output estimation
error are uncorrelated, so that:
0 = E[(Î¸k âˆ’Ë†Î¸k)(yk âˆ’Hk Ë†Î¸âˆ’
k )T]
= E[(Î¸k âˆ’(I âˆ’KkHk) Ë†Î¸âˆ’
k âˆ’Kkyk)(yk âˆ’Hk Ë†Î¸T
k )T]
= E[(eâˆ’
k âˆ’KkHkeâˆ’
k âˆ’Kkvk)(Hkeâˆ’
k + vk)T]
= (I âˆ’KkHk)E[eâˆ’
k (eâˆ’
k )T] âˆ’KkE[vkvT
k ]
(6.26)
Considering (6.2) and (6.13), from (6.26) we have:
(I âˆ’KkHk)Pâˆ’
k HT
k âˆ’KkRk = 0
(6.27)
which can be rewritten as:
Kk(HkPâˆ’
k HT
k + Rk) = Pâˆ’
k HT
k
(6.28)
Hence, Kk, the Kalman gain, is obtained as follows:
Kk = Pâˆ’
k HT
k (HkPâˆ’
k HT
k + Rk)âˆ’1
(6.29)

Extended Kalman Filter Algorithm for the Tuning of Type-2 Fuzzy Neural Networks
77
By applying (6.29) to (6.25) we have the following equation for the
covariance matrix:
Pk = (I âˆ’KkHk)Pâˆ’
k + KkHkPâˆ’
k HT
k KT
k âˆ’Pâˆ’
k HT
k KT
k + KkPkKT
k
= (I âˆ’KkHk)Pâˆ’
k + Kk(HkPâˆ’
k KT
k + Rk)KT
k âˆ’Pâˆ’
k HT
k KT
k
= (I âˆ’KkHk)Pâˆ’
k + Pâˆ’
k HT
k KT
k âˆ’Pâˆ’
k HT
k KT
k
= (I âˆ’KkHk)Pâˆ’
k
(6.30)
Table 6.1 summarizes the KF which is required to update the estimated
values of the states of the system (6.1). It can be shown that if wk and vk are
white, zero-mean and uncorrelated, then KF is an optimal linear estimator
in terms of a weighted mean squared error cost function [4]. In other words,
it is possible to have a nonlinear filter that performs better than KF, but no
linear filter can act better than it. In Ref. [4], it is stated that it is not necessary
to have Gaussian noise as an optimality requirement for KF.
Table 6.1 Summary of the KF
Let the state space model of the system be:
Î¸k+1 = Fk+1,kÎ¸k + Gk+1,kuk + wk
yk = HkÎ¸k + vk
where wk and vk are uncorrelated, zero-mean, and Gaussian noise processes with
their covariance matrix equal to Qk and Rk, respectively.
The initial conditions for the estimator are considered to be:
Ë†Î¸0 = E[Î¸0]
P0 = E[(Î¸0 âˆ’E[Î¸0])(Î¸0 âˆ’E[Î¸0])T]
A priori estimate: a priori estimate of Î¸k is obtained from a past estimate and the
dynamics of the system as:
Ë†Î¸âˆ’
k = Fk,kâˆ’1 Ë†Î¸kâˆ’1 + Gk,kâˆ’1ukâˆ’1
A priori estimate of covariance matrix:
Pâˆ’
k = Fk,kâˆ’1Pkâˆ’1FT
k,kâˆ’1 + Qk,kâˆ’1
The Kalman gain:
Kk = Pâˆ’
k HT
k [HkPâˆ’
k HT
k + Rk]âˆ’1
State estimate update:
Ë†Î¸k = Î¸âˆ’
k + Kk(yk âˆ’HkÎ¸âˆ’
k )
Error covariance update
Pk = (I âˆ’KkHk)Pâˆ’
k

78
Fuzzy Neural Networks for Real Time Control Applications
6.3 SQUARE-ROOT FILTERING
Although KF is an efficient and optimal linear estimator in the present of
noise, it may suffer from numerical difficulties, which happens mostly as a
result of finite length of words used by a digital computer [4]. One of the
main problems, as discussed in Ref. [4], is that it is possible the covariance
matrix becomes ill condition as a result of large condition number value.
A possible solution to this problem may be using the square root of the
covariance matrix. The square root of Pk is called Sk. The condition number
of Sk is proven to be equal to the square root of the condition number
of Pk, i.e.:
Ïƒ(Sk) =

Ïƒ(Pk)
(6.31)
where Ïƒ(.) indicates the condition number of its corresponding argument.
This modification may solve some of the problems caused by a large number
of Pk. The square-root filtering algorithm, which is discussed here is based
on Potterâ€™s algorithm [5, 6], which was further modified to deal with vector
measurements [7].
It is considered that Rk is a diagonal matrix as Rk
=
diag
(R1k, . . . , Rik, . . . , Rlk). Therefore, Rik is the variance of measurement noise
on ith measurement. Furthermore, Hik is defined as the ith row of the matrix
Hk. Table 6.2 summarizes this algorithm. Although root-square type of KF
is numerically more robust, its computational cost is much higher than the
original version of KF.
6.4 EXTENDED KALMAN FILTER ALGORITHM
It is possible to extend the original version of KF to nonlinear dynamic
systems using first-order Taylor expansion of nonlinear functions. Table 6.3
summarizes this algorithm [4].
6.5 EXTENDED KALMAN FILTER TRAINING OF TYPE-2
FUZZY NEURAL NETWORKS
As a powerful parameter estimator, EKF is one of the most successful training
algorithms applied to NNs [8] and FNN structure [9â€“11]. In order to discuss
the EKF training method for T2FNN structure, a simple type-2 structure

Extended Kalman Filter Algorithm for the Tuning of Type-2 Fuzzy Neural Networks
79
Table 6.2 Summary of the root square type of KF [4]
1. Let Î¸âˆ’
k and Sâˆ’
k be the a priori state estimate and the a priori covariance
square root, respectively.
Initialize:
Ë†Î¸0k = Ë†Î¸âˆ’
k
S+
0k = Sâˆ’
k
2. For i = 1, . . ., r (where r is the number of measurements. Perform
the following steps:
(a) Let Hik be the ith row of Hk, and yik be the ith measurement, and
Rik be the variance of the ith measurement.
(b) Perform the following to find the square root of the variance after the
ith measurement has been processed:
Ï†i = S+T
iâˆ’1,kHT
ik
ai =
1
Ï†T
i Ï†i+Rik
Î³i =
1
1Â±âˆšaiRik
S+
ik = S+
iâˆ’1,k(I âˆ’aiÎ³iÏ†iÏ†T
i )
(c) Compute the Kalman gain for the ith measurement as:
Kik = aiS+
ikÏ†i
(d) Update the estimation of the states due to ith measurement as follows:
Ë†Î¸ik = Ë†Î¸iâˆ’1,k + Kik(yik âˆ’Kik Ë†Î¸iâˆ’1,k)
3. Set the a posteriori covariance square root and the a posteriori state
estimate as:
S+
k = S+
rk
Ë†Î¸k = Ë†Î¸rk
is investigated in this section. Consider a T2FNN with two inputs and I
number of MFs for the first input and J number of MFs for the second
input. The upper MFs for the first input x1 are considered to be Î¼11(x1),
Î¼12(x1), . . ., and Î¼1I(x1), and its corresponding lower MFs are considered
to be Î¼11(x1), Î¼12(x1), . . . and Î¼1I(x1). The upper MFs for the second
input (x2) are considered to be Î¼21(x2), Î¼22(x2), . . ., and Î¼2I(x2), and its

80
Fuzzy Neural Networks for Real Time Control Applications
Table 6.3 Summary of the EKF algorithm
1.
Let the nonlinear state space model of the system be:
Î¸k = fkâˆ’1(Î¸kâˆ’1, ukâˆ’1, wkâˆ’1)
yk = hk(Î¸k, vk)
where wk and vk are uncorrelated, zeroâ€“mean and Gaussian noise processes
with following properties:
E[wkwT
k ] = Qk
E[vkvT
k ] = Rk
2.
The initial conditions for the estimator are considered to be as:
Ë†Î¸0 = E[Î¸0]
P0 = E[(Î¸0 âˆ’E[Î¸0])(Î¸0 âˆ’E[Î¸0])T]
3.
For k = 1, 2, . . ., perform the following steps:
(a) Compute the following partial derivatives:
Fkâˆ’1 = âˆ‚fkâˆ’1
âˆ‚Î¸ | Ë†Î¸kâˆ’1
Lkâˆ’1 = âˆ‚fkâˆ’1
âˆ‚w | Ë†Î¸kâˆ’1
(b) A priori estimate: a priori estimate of Î¸k is obtained as follows:
Ë†Î¸âˆ’
k = fkâˆ’1( Ë†Î¸kâˆ’1, ukâˆ’1, 0)
(c) A priori estimate for covariance matrix:
Pâˆ’
k = Fkâˆ’1Pkâˆ’1FT
kâˆ’1 + Lkâˆ’1Qkâˆ’1LT
kâˆ’1
(d) Compute the following partial derivatives:
Hk = âˆ‚hk
âˆ‚Î¸ | Ë†Î¸âˆ’
k
Mk = âˆ‚hk
âˆ‚v | Ë†Î¸âˆ’
k
(e) Update the Kalman gain as:
Kk = Pâˆ’
k HT
k [HkPâˆ’
k HT
k + MkRkMT
k ]âˆ’1
(f) Update the state estimate as:
Ë†Î¸k = Î¸âˆ’
k + Kk(yk âˆ’hk(Î¸âˆ’
k , 0))
(g) Update the error covariance matrix as:
Pk = (I âˆ’KkHk)Pâˆ’
k

Extended Kalman Filter Algorithm for the Tuning of Type-2 Fuzzy Neural Networks
81
corresponding lower MFs are considered to be Î¼21(x2), Î¼22(x2), . . ., and
Î¼2J(x2). The terms Î¼1i(x1), Î¼1i(x1), Î¼2j(x2), and Î¼2j(x2) are considered
to be defined as follows:
Î¼1i(x1) = exp

âˆ’
x1 âˆ’c1i
Ïƒ 1i
2
(6.32)
Î¼1i(x1) = exp

âˆ’
x1 âˆ’c1i
Ïƒ 1i
2
(6.33)
Î¼2j(x2) = exp
âŽ›
âŽâˆ’

x2 âˆ’c2j
Ïƒ 2j
2âŽž
âŽ 
(6.34)
Î¼2j(x2) = exp

âˆ’
x2 âˆ’c2j
Ïƒ 2j
2
(6.35)
The upper and the lower rules of the system are derived as follows:
w1 = Î¼11(x1)Î¼21(x2), . . ., w1Ã—J = Î¼11(x1)Î¼2J(x2)
w1Ã—J+1 = Î¼12(x1)Î¼21(x2), . . ., w2Ã—J = Î¼12(x1)Î¼2J(x2)
...
w(Iâˆ’1)Ã—J+1 = Î¼1I(x1)Î¼21(x2), . . ., wIÃ—J = Î¼1I(x1)Î¼2J(x2)
(6.36)
and
w1 = Î¼11(x1)Î¼21(x2), . . ., w1Ã—J = Î¼11(x1)Î¼2J(x2)
w1Ã—J+1 = Î¼12(x1)Î¼21(x2), . . ., w2Ã—J = Î¼12(x1)Î¼2J(x2)
...
w(Iâˆ’1)Ã—J+1 = Î¼1I(x1)Î¼21(x2), . . ., wIÃ—J = Î¼1I(x1)Î¼2J(x2)
(6.37)
The output of the T2FNN is obtained as follows:
yN = q
N=IÃ—J

r=1
fr Ëœwr + (1 âˆ’q)
N=IÃ—J

r=1
fr Ëœwr
(6.38)

82
Fuzzy Neural Networks for Real Time Control Applications
where:
Ëœwr =
wr
N
l=1 wl
, Ëœwr =
wr
N
l=1 wl
(6.39)
and:
fr = arixi + br,
âˆ€r = 1, 2, . . . , N,
and
i = 1, 2, . . . , n
(6.40)
In order to apply EKF to T2FNN, all unknown parameters of T2FLS
must be gathered in a vector. The vector of unknown parameters has the
following form:
Î¸ = [c11, . . . , c1i, . . . , c1I, c21, . . . , c2j, . . . , c2J, Ïƒ 11, . . . , Ïƒ 1i, . . . , Ïƒ 1I, Ïƒ 11,
. . . , Ïƒ 1i, . . . , Ïƒ 1I, Ïƒ 21, . . . , Ïƒ 2j, . . . , Ïƒ 2J, Ïƒ 21, . . . , Ïƒ 2j, . . . , Ïƒ 2J, a11, . . . ,
a1n, . . . , al1, . . . , aln, . . . , aN1, . . . , aNn, b1, . . . , bN]
(6.41)
The partial derivative of the output of the fuzzy system with respect to
Î¸ in each sample k must be calculated. These calculations have been pre-
viously discussed in Chapter 5. Having calculated these partial derivatives,
Hk = âˆ‚yN
âˆ‚Î¸ is derived. Since the parameters of T2FNN are time-invariant,
the process equation is derived as follows:
Î¸k+1 = Î¸k
(6.42)
This equation implies that fkâˆ’1(Î¸kâˆ’1) = Î¸kâˆ’1 and Fkâˆ’1 is equal to the
identity matrix. The rest of the algorithm is as that in Table 6.3.
However, since all the adaptive parameters of T2FNN are gathered in
a vector, the computational cost of EKF is of the order of AB2 where A
is the dimension of the output of dynamical system and B is the number
of the parameters. The total number of parameters for a T2FNN structure
depends on the number of inputs and the number of MFs considered for
each input. For a T2FNN that has n inputs, c Gaussian type-2 MFs with
uncertain Ïƒ value for each input, N number of rules, and an output, the
total number of adaptive parameters is 3c + (n + 1)N. The total number
of the rules N is a large number and is multiplied by the number of MF of
inputs. The computational cost of EKF when applied to T2FNN will be
9c2+(n+1)2N2+6c(n+1)N, which is a very big number. Furthermore, the
size of covariance matrix will be equal to 9c2 + (n + 1)2N2 + 6c(n + 1)N,
which occupies a large amount of memory. In order to reduce the

Extended Kalman Filter Algorithm for the Tuning of Type-2 Fuzzy Neural Networks
83
computational cost of EKF, its decoupled version is mostly used in the
training of a complex structure such as NNs [3, 8], FNNs [10] and
T2FNNs [11].
6.6 DECOUPLED EXTENDED KALMAN FILTER
In order to reduce the computational cost of EKF and reduce its memory
consumption, most researchers prefer to use it in a decoupled form. The
DEKF is based on the assumption that certain parameters interact with each
other only at a second-order level. The DEKF has been previously used in
Refs. [8, 11]. For the T2FNN considered in this chapter, there are three
adaptive parameters for each type-2 MF. The parameters of the consequent
part are put in one vector, and they make one group of parameters. In this
way, four groups of parameters are considered:
Î¸1 = [c11, . . . , c1i, . . . , c1I, c21, . . . , c2j, . . . , c2J]
(6.43)
Î¸2 = [Ïƒ 11, . . . , Ïƒ 1i, . . . , Ïƒ 1I, Ïƒ 21, . . . , Ïƒ 2j, . . . , Ïƒ 2J]
(6.44)
Î¸3 = [Ïƒ 11, . . . , Ïƒ 1i, . . . , Ïƒ 1I, Ïƒ 21, . . . , Ïƒ 2j, . . . , Ïƒ 2J]
(6.45)
Î¸4 = [a11, . . . , a1n, . . . , al1, . . . , aln, . . . , aN1, . . . , aNn, b1, . . . , bN] (6.46)
Each group of the parameters has its own covariance matrix and Kalman
gain. The computational cost of estimation is greatly reduced and it is of the
order of 3c2 + (n + 1)2N2. The ratio of computational cost of decoupled
EKF to standard EKF is shown as follows:
Decoupled EKF computational cost
Standard EKF computational cost
=
3c2 + (n + 1)2r2
9c2 + (n + 1)2N2 + 6c(n + 1)N
(6.47)
The memory consumption is also reduced with the same order.
6.7 CONCLUSION
In this chapter, after a short introduction to KF and its variants, it is extended
to estimate the parameters of nonlinear systems, particularly T2FNNs. The
extended version of KF is called EKF, which is a powerful tool to optimize
the parameters of T2FNNs. Although KF can be shown to be an optimal
linear estimator for linear systems subject to process and measurement white
noise, its extended version is a suboptimal method because it uses the

84
Fuzzy Neural Networks for Real Time Control Applications
first-order Taylor expansion of nonlinear functions. Furthermore, because
of the large number of adaptive parameters in T2FNN structure, EKF is
shown be computationally very demanding with huge memory consump-
tion. In order to deal with this problem and reduce the size of the covariance
matrix, it is possible to assume that certain parameters interact with each
other only at a second-order level. In this way, it is possible to consider
small groups for the parameters, which greatly reduces the computational
cost and memory consumption of the algorithm.
REFERENCES
[1] R.E. Kalman, A new approach to linear filtering and prediction problems, J. Fluids
Eng. 82 (1) (1960) 35-45.
[2] H.L. Van Trees, Detection, Estimation, and Modulation Theory, John Wiley Sons,
New York, 2004.
[3] S.S. Haykin, Kalman Filtering and Neural Networks, Wiley Online Library, New York,
2001.
[4] D. Simon, Optimal State Estimation: Kalman, H infinity, and Nonlinear Approaches,
John Wiley & Sons, New York, 2006.
[5] R.H. Battin, Astronautical guidance, McGraw-Hill, 1964.
[6] P. Kaminski, A.E. Bryson, S. Schmidt, Discrete square root filtering: A survey of current
techniques, IEEE Trans. Automat. Control 16 (6) (1971) 727-736.
[7] A. Andrews, A square root formulation of the Kalman covariance equations, AIAA J.
6 (6) (1968) 1165-1166.
[8] D. Simon, Training radial basis neural networks with the extended Kalman filter,
Neurocomputing 48 (1-4) (2002) 455-475.
[9] M.A. Shoorehdeli, M. Teshnehlab, A.K. Sedigh, Training ANFIS as an identifier with
intelligent hybrid stable learning algorithm based on particle swarm optimization and
extended Kalman filter, Fuzzy Sets Syst. 160 (7) (2009) 922-948.
[10] D. Simon, Training fuzzy systems with the extended Kalman filter, Fuzzy Sets Syst. 132
(2) (2002) 189-199.
[11] M. Khanesar, E. Kayacan, M. Teshnehlab, O. Kaynak, Extended Kalman filter based
learning algorithm for type-2 fuzzy logic systems and its experimental evaluation, IEEE
Trans. Indust. Electron. 59 (11) (2012) 4443-4455.

CHAPTER 7
Sliding Mode Control Theory-
Based Parameter Adaptation
Rules for Fuzzy Neural Networks
Contents
7.1
Introduction
86
7.2
Identification Design
86
7.2.1 Identification Using Gaussian Type-2 MF with Uncertain Ïƒ
89
7.2.1.1 Parameter Update Rules for the T2FNN
89
7.2.1.2 Proof of Theorem 7.1
91
7.2.2 Identification Using T2FNN with Elliptic Type-2 MF
96
7.2.2.1 Parameter Update Rules for the T2FNN
97
7.2.2.2 Proof of Theorem 7.2
98
7.3
Controller Design
105
7.3.1 Control Scheme Incorporating a T2FNN Structure
105
7.3.1.1 T2FNN
106
7.3.2 SMC Theory-Based Learning Algorithm for T2FNN with Gaussian MFs
with Uncertain Ïƒ Values
109
7.3.2.1 Parameter Update Rules For T2FNN
110
7.3.2.2 Proof of Theorem 7.3
111
7.3.3 SMC Theory-Based Learning Algorithm for T2FNN with Elliptic MFs
116
7.3.3.1 Proof of Theorem 7.4
119
7.3.3.2 Proof of Theorem 7.5
129
7.4
Conclusion
130
References
131
Abstract
In this chapter, in order to deal with nonlinearities, lack of modeling several uncertainties,
and noise in both identification and control problems, SMC theory-based learning
algorithms are designed to tune both the premise and consequent parts of T2FNNs.
Furthermore, the stability of the learning algorithms for control and identification
purposes are proved by using appropriate Lyapunov functions. In addition to its well-
known feature which is robustness, the most significant advantage of the proposed
learning algorithm for the identification purposesisthat the algorithm has aclosed form,
and thus it is easier to implement in real-time compared to other existing methods.
Fuzzy Neural Networks for Real Time Control Applications
Copyright Â© 2016 Elsevier Inc.
http://dx.doi.org/10.1016/B978-0-12-802687-8.00007-4
All rights reserved.
85

86
Fuzzy Neural Networks for Real Time Control Applications
Keywords
Sliding mode control theory based adaptation rules, Type-2 fuzzy neural networks,
Feedback error learning controller, Identification, Control
7.1 INTRODUCTION
SMC is a robust control algorithm that can handle nonlinear dynamic
systems in the presence of bounded input disturbances and uncertainties.
However, this controller suffers from several drawbacks:
1. Its basic version is sensitive to noise.
2. Its corresponding control signal exhibits high frequency oscillations.
3. In the design of an SMC, the nominal values of the nonlinear functions
that exist in the dynamic of the system must be known.
In order to overcome the problems mentioned above, intelligent meth-
ods such as ANNs, fuzzy logic, and FNNs can be preferred. With its
proven general function approximation property, flexibility and capability
of using expert knowledge of FNNs, this structure is one of the most
important structures to overcome the drawbacks of SMC [1]. The FNN-
based methods are known to result in less chattering in control signal and
eliminate the knowledge about the nominal functions of the dynamic model
of the system. The use of fuzzy systems in parallel with SMC approaches
has been considered in number of papers. Sliding mode fuzzy controllers
have the advantages of both fuzzy systems and SMCs. Fuzzy systems can be
used as powerful and flexible approximators, and the sliding mode approach
adds the possibility of thorough stability analysis and robustness against
uncertainties both for the system and the adaptation laws [2â€“4].
In this chapter, not only is the fusion of T2FNNs with SMC theory-
based learning algorithms in a control scheme considered, but also SMC
theory-based learning algorithms are used to train the parameters of a
T2FNN as an identifier. This combination makes it possible to train the
parameters of T2FNNs with guaranteed stability analysis. In other words,
during the training of the parameters of the T2FNN using SMC theory-
based learning algorithms, it is guaranteed that the identification error will
converge to zero, and the parameters of T2FNN will not diverge.
7.2 IDENTIFICATION DESIGN
A first-order interval type-2 TSK fuzzy if-then rule base with r input
variables is used in this chapter for identification purposes. The consequent
part of which is composed of crisp numbers, and its premise part is type-2

Sliding Mode Control Theory-Based Parameter Adaptation Rules for Fuzzy Neural Networks
87
MFs. The definitions and the structure of this type of fuzzy system can be
found in the previous chapters, but here we summarize it for convenience.
The rth rule is as follows:
Rr : If x1 is ËœA1j . . . and xi is ËœAik and . . . and xI is ËœAIl then
fr =
I

i=1
arixi + br
(7.1)
where xi(i = 1, . . . , I) are the inputs of the type-2 TSK model, ËœAik is
the kth type-2 fuzzy MF (k = 1, . . . , K) corresponding to the input ith
variable, and K is the number of MFs for the ith input that can be different
for each input. The parameters ar and br stand for the consequent part and
fr(r = 1, . . . , N) is the output function. The MFs used in this chapter are
type-2 fuzzy Gaussian MFs and elliptic type-2 MFs. The upper and lower
type-2 fuzzy Gaussian MFs with an uncertain standard deviation can be
represented as follows:
Î¼ik(xi) = exp

âˆ’1
2
(xi âˆ’cik)2
Ïƒ 2
ik

(7.2)
Î¼ik(xi) = exp

âˆ’1
2
(xi âˆ’cik)2
Ïƒ 2
ik

(7.3)
where cik is the center value of the kth type-2 fuzzy set for the ith input.
The parameters Ïƒ ik and Ïƒ ik are standard deviations for the upper and lower
MFs, and the upper and lower type-2 fuzzy Elliptic MFs can be represented
as follows:
Î¼ik =

1 âˆ’

xi âˆ’cik
dik

a2,ik1/a2,ik
H(xi, cik, dik)
(7.4)
Î¼ik =

1 âˆ’

xi âˆ’cik
dik

a1,ik1/a1,ik
H(xi, cik, dik)
(7.5)
in which Î¼ik and Î¼ik are the lower and upper bound of kth MF con-
sidered for the ith input and H(xi, cik, dik) is the rectangular function
defined as:
H(xi, c, d) =
 1
c âˆ’d < xi < c + d
0
otherwise

88
Fuzzy Neural Networks for Real Time Control Applications
The structure used in this investigation is called the A2-C0 fuzzy system
[5]. In such a structure, first, the lower and upper membership degrees Î¼
and Î¼ are determined for each input signal being fed to the system. Next,
the firing strengths of the rules using the prod t-norm operator are calculated
as follows:
wr = Î¼ ËœA1(x1) âˆ—Î¼ ËœA2(x2) âˆ—Â· Â· Â· Î¼ ËœAI(xI)
wr = Î¼ ËœA1(x1) âˆ—Î¼ ËœA2(x2) âˆ—Â· Â· Â· Î¼ ËœAI(xI)
(7.6)
The consequent part corresponding to each fuzzy rule is a linear
combination of the inputs x1, x2, . . . , xI. This linear function is called fr
and is defined as in (7.1). The output of the network is approximated as
follows:
yN = q
N

r=1
fr Ëœwr + (1 âˆ’q)
N

r=1
fr	wr
(7.7)
where 	
wr and 	
wr are the normalized values of the lower and upper output
signals from the second hidden layer of the network as follows:
	
wr =
wr

N
i=1 wr
and 	
wr =
wr

N
i=1 wr
(7.8)
The design parameter, q, is a coefficient which determines the sharing
of the lower and the upper firing levels of each fired rule. This parameter
can be a constant (equal to 0.5 in most cases) or a time-varying parameter.
In this investigation, the latter is preferred. In other words, the parameter
update rules and the proof of the stability of the learning process are given
for the case of a time-varying q.
The following vectors can be specified:

W (t) =

	
w1 (t) 	
w2 (t) . . . 
wN (t)
T ,

W (t) =
	
w1 (t) 	
w2 (t) . . . 
wN (t)
T and F = [f1 f2 . . . fN]
The following assumptions have been used in this investigation: The
time derivative of both the input and output signals can be considered
bounded:
|Ë™xi(t)| â‰¤BË™x,
min(x2
i (t)) = Bx2, (i = 1 . . . I) and |Ë™y(t)| â‰¤BË™y
âˆ€t
(7.9)
where BË™x, Bx2, and BË™y are assumed to be some known positive constants.

Sliding Mode Control Theory-Based Parameter Adaptation Rules for Fuzzy Neural Networks
89
7.2.1 Identification Using Gaussian Type-2 MF
with Uncertain Ïƒ
Since Gaussian type-2 MF with uncertain Ïƒ is the only MF that does not
have any undifferentiable point, it is one of the first choices in type-2 fuzzy
control and identification applications. The error between the measured
output of the system and the output of the T2FNN can be defined as a
time-varying sliding surface. In this section, it is shown that under certain
conditions, it is guaranteed that the system will keep on being on the sliding
surface. The use of sign function results in finite time convergence of the
sliding surface to zero, which, in turn forces the output of the network,
yN(t), to perfectly follow the desired output signal, y(t), for all time t > th.
The time instant th is defined to be the hitting time for being e(t) = 0. The
sliding surface is defined as follows:
S (e(t)) = e(t) = yN(t) âˆ’y(t) = 0
(7.10)
Definition: A sliding motion will appear on the sliding manifold
S (e(t)) = e(t) = 0 after a finite time th, if the condition S(t)Ë™S(t) < 0
is satisfied for all t in some nontrivial semi-open subinterval of time of the
form [t, th) âŠ‚(0, th).
The main goal of the design of the adaptation laws is to design an online
learning algorithm for the parameters of the T2FNN, such that the sliding
mode condition of the above definition is enforced.
7.2.1.1 Parameter Update Rules for the T2FNN
The parameter update rules for the T2FNN are given by the following
theorem.
Theorem 7.1. If the adaptation laws for the parameters of the considered
T2FNN are chosen as [6]:
Ë™cik = Ë™xi + (xi âˆ’cik)Î±1sgn (e)
(7.11)
Ë™Ïƒ ik = âˆ’

Ïƒ ik +
Ïƒ 3
ik
(xi âˆ’cik)2

Î±1sgn (e)
(7.12)
Ë™Ïƒ ik = âˆ’

Ïƒ ik +
Ïƒ 3
ik
(xi âˆ’cik)2

Î±1sgn (e)
(7.13)

90
Fuzzy Neural Networks for Real Time Control Applications
Ë™ari = âˆ’xi
qËœwr + (1 âˆ’q) Ëœwr
q Ëœwr + (1 âˆ’q) Ëœwr
TqËœwr + (1 âˆ’q) Ëœwr
Î±sgn (e)
(7.14)
Ë™br = âˆ’
qËœwr + (1 âˆ’q) Ëœwr
q Ëœwr + (1 âˆ’q) Ëœwr
TqËœwr + (1 âˆ’q) Ëœwr
Î±sgn (e)
(7.15)
Ë™q = âˆ’
1
F(
W âˆ’
W)T Î±sgn(e)
(7.16)
where Î± is an adaptive learning rate with an adaptation law as follows:
Ë™Î± = Î³ (I + 2) | e | âˆ’Î½Î³ Î±, 0 < Î³ , Î½
(7.17)
then, given an arbitrary initial condition e(0), the learning error e(t) will converge
to zero within a finite time th.
Remark 7.1. The adaptation laws of (7.17) show that the learning rate
does not have a fixed value and its value is evolving during identification.
The existence of this adaptation law makes it possible to choose a small
initial value for Î± and it grows based on the requirement of identification
during the training phase. Note that in (7.17), the parameter Î³ has a small
positive real value that is interpreted as the learning rate for the adaptive
learning rate. As can be seen for the adaptation law, the first term of the
adaptation law of (7.17) is always positive, which may cause bursting in the
parameter Î±. In order to avoid this phenomenon, the second term is added
to the adaptation law, which avoids a possible parameter bursting in Î±. The
value of Î½ should be selected very small to keep it from interrupting the
adaptation mechanism.
Remark 7.2. Another benefit of the adaptive learning rate using (7.17) is
that the upper bound of the states of the system does not need to be known
as a priori and the knowledge of their existence is enough.
Remark 7.3. It is possible that the denumerator in the adaptation laws
of (7.11)-(7.16) become zero, which may cause instability in the system.
In order to avoid this, the denominator should be equal to a small number
(e.g. 0.001) when its calculated value is smaller than this threshold.
In order to avoid the possibility of high-frequency oscillations in the
control input, which is called chattering, the following are the two common
methods used [7]:

Sliding Mode Control Theory-Based Parameter Adaptation Rules for Fuzzy Neural Networks
91
1. Using a saturation function instead of the signum function,
2. Using a boundary layer so that an equivalent control replaces the
corrective one when the system is inside this layer.
In order to reduce the chattering effect, the following function is used
a small number, e.g. Î´s = 0.05 instead of the signum function:
sgn(e) :=
e
|e| + Î´s
(7.18)
Remark 7.4. In different tests it is observed that the output of the
T2FNN is quite sensitive to the changes in the consequent part parameters.
Hence, it is logical to use larger values for the parameters of the consequent
part and smaller values for the parameters of the antecedent part. Thus, a
smaller value (Î±1) is chosen for the antecedent parts.
7.2.1.2 Proof of Theorem 7.1
The time derivative of (7.8) is calculated as follows:
Ë™Ëœwr = âˆ’ËœwrKr + Ëœwr
N

r=1
ËœwrKr;
Ë™Ëœwr = âˆ’ËœwrKr + Ëœwr
N

r=1
ËœwrKr
(7.19)
where
Aik = xi âˆ’cik
Ïƒ ik
and Aik = xi âˆ’cik
Ïƒ ik
Kr =
I

i=1
Aik Ë™Aik and Kr =
I

i=1
Aik Ë™Aik
Applying (7.11)-(7.13) into the equations above, the following equation
is obtained:
Kr = Kr =
I

i=1
Aik Ë™Aik =
I

i=1
Aik Ë™Aik = IÎ±sgn (e)
(7.20)
The following Lyapunov function is used to prove the stability of the
proposed adaptation laws:
V = 1
2e2 + 1
2Î³ (Î± âˆ’Î±âˆ—)2,
0 < Î³
(7.21)

92
Fuzzy Neural Networks for Real Time Control Applications
The stability conditions require the time derivative of the Lyapunov
function to be negative. The time derivative of (7.21) can be calculated
as follows:
Ë™V = Ë™ee + 1
Î³ Ë™Î±(Î± âˆ’Î±âˆ—) = e(Ë™yN âˆ’Ë™y) + 1
Î³ Ë™Î±(Î± âˆ’Î±âˆ—)
(7.22)
The time derivative of the Lyapunov function requires the time deriva-
tive of yN to be calculated. Hence, the time derivative of (7.7) is obtained
as follows:
Ë™yN = Ë™q
N

r=1
fr Ëœwr + q
N

r=1
(Ë™fr Ëœwr + fr Ë™Ëœwr) âˆ’Ë™q
N

r=1
fr Ëœwr
+ (1 âˆ’q)
N

r=1
(Ë™fr Ëœwr + fr Ë™Ëœwr)
(7.23)
By using (7.19), (7.20), and (7.23), the following term can be obtained:
Ë™yN =Ë™q
N

r=1
fr Ëœwr + q
N

r=1

Ë™fr Ëœwr + fr(âˆ’ËœwrKr + Ëœwr
N

r=1
ËœwrKr)

âˆ’Ë™q
N

r=1
fr Ëœwr + (1 âˆ’q)
N

r=1

Ë™fr Ëœwr + fr(âˆ’ËœwrKr + Ëœwr
N

r=1
ËœwrKr)

=Ë™q
N

r=1
fr Ëœwr + q
N

r=1

Ë™fr Ëœwr âˆ’IÎ±sgn (e) fr( Ëœwr âˆ’Ëœwr
N

r=1
Ëœwr)

âˆ’Ë™q
N

r=1
fr Ëœwr + (1 âˆ’q)
N

r=1

Ë™fr Ëœwr âˆ’IÎ±sgn (e) fr( Ëœwr âˆ’Ëœwr
N

r=1
Ëœwr)

Equation (7.24) is correct by definition:
N

r=1
Ëœwr = 1 and
N

r=1
Ëœwr = 1
(7.24)
It is possible to use (7.14), (7.15), (7.16), and (7.24) to obtain the
following equation:

Sliding Mode Control Theory-Based Parameter Adaptation Rules for Fuzzy Neural Networks
93
Ë™yN = âˆ’
1
F( ËœW âˆ’ËœW)T Î±sgn (e)
N

r=1
fr( Ëœwr âˆ’Ëœwr)
+
N

r=1
Ë™fr
qËœwr + (1 âˆ’q) Ëœwr

= âˆ’Î±sgn (e) +
N

r=1

I

i=1
(Ë™arixi + ariË™xi) + Ë™br

qËœwr + (1 âˆ’q) Ëœwr


(7.25)
By substituting (7.25) into (7.22), the following equation can be
obtained:
Ë™V = Ë™ee + 1
Î³ Ë™Î±(Î± âˆ’Î±âˆ—) = e(Ë™yN âˆ’Ë™y) + 1
Î³ Ë™Î±(Î± âˆ’Î±âˆ—)
= e

âˆ’Î±sgn (e) +
N

r=1

I

i=1
(Ë™arixi + ariË™xi) + Ë™br


qËœwr + (1 âˆ’q) Ëœwr

âˆ’Ë™y

+ 1
Î³ Ë™Î±(Î± âˆ’Î±âˆ—)
(7.26)
By inserting the adaptation laws of (7.14)-(7.16) into (7.26), the following
equation is obtained:
Ë™V = e

âˆ’Î±sgn (e) +
N

r=1

I

i=1

âˆ’(xiÎ±sgn (e)
(qËœwr + (1 âˆ’q) Ëœwr)
(qËœwr + (1 âˆ’q) Ëœwr)T(qËœwr + (1 âˆ’q) Ëœwr)
)xi + ariË™xi

âˆ’Î±sgn (e)
(qËœwr + (1 âˆ’q) Ëœwr)
(qËœwr + (1 âˆ’q) Ëœwr)T(qËœwr + (1 âˆ’q) Ëœwr)

(qËœwr + (1 âˆ’q) Ëœwr)

âˆ’Ë™y

+ 1
Î³ Ë™Î±(Î± âˆ’Î±âˆ—)
(7.27)

94
Fuzzy Neural Networks for Real Time Control Applications
= e

âˆ’Î±sgn (e)
+
N

r=1

I
i=1

âˆ’Î±sgn (e) x2
i + ariË™xi

qËœwr + (1 âˆ’q) Ëœwr

âˆ’Î±sgn (e)

âˆ’Ë™y

+ 1
Î³ Ë™Î±(Î± âˆ’Î±âˆ—)
(7.28)
Furthermore:
Ë™V = âˆ’| e | 2Î± + e
 N

r=1

I

i=1

âˆ’Î±sgn (e) x2
i
+ariË™xi

qËœwr + (1 âˆ’q) Ëœwr

âˆ’Ë™y

+ 1
Î³ Ë™Î±(Î± âˆ’Î±âˆ—)
(7.29)
Using the upper bounds of the signals as:
âˆ¥xiâˆ¥â‰¤Bx2, âˆ¥Ë™yâˆ¥â‰¤BË™y, âˆ¥Ë™xiâˆ¥â‰¤BË™x, âˆ¥arâˆ¥â‰¤Ba
(7.30)
we have the following inequalities:
Ë™V < âˆ’| e | 2Î± + | e |
 N

r=1

I

i=1

âˆ’Î±Bx2
+ BaBË™x(qËœwr + (1 âˆ’q) Ëœwr)

+ BË™y

+ 1
Î³ Ë™Î±(Î± âˆ’Î±âˆ—)
< âˆ’| e | 2Î± + | e |
 N

r=1

âˆ’IÎ±Bx2 + IBaBË™x(qËœwr
+ (1 âˆ’q) Ëœwr)

+ BË™y

+ 1
Î³ Ë™Î±(Î± âˆ’Î±âˆ—)
< âˆ’| e | 2Î± + | e |

âˆ’IÎ±Bx2 + IBaBË™x(q
N

r=1
Ëœwr

Sliding Mode Control Theory-Based Parameter Adaptation Rules for Fuzzy Neural Networks
95
+ (1 âˆ’q)
N

r=1
Ëœwr) + BË™y

+ 1
Î³ Ë™Î±(Î± âˆ’Î±âˆ—)
< âˆ’| e |

2Î± + IÎ±Bx2

+ | e |

IBaBË™x + BË™y

+ 1
Î³ Ë™Î±(Î± âˆ’Î±âˆ—)
The parameter Î±âˆ—is considered to be an unknown parameter that is
determined during the adaptation of the learning rate and its true value is
considered to satisfy the following inequality:
Î±âˆ—â‰¥2(IBaBË™x + BË™y)
2 + IBx2
Hence, using an adaptation law for the learning rate makes it possible to
estimate the parameters of the T2FNN with less a priori knowledge. The
adaptation law of the learning rate is obtained in the next few steps:
Ë™V â‰¤âˆ’| e | (2Î± + IÎ±Bx2)+ | e | (IBaBË™x + BË™y)
+ (2 + IBx2)Î±âˆ—| e | âˆ’(2 + IBx2)Î±âˆ—| e |
+ 1
Î³ (Î± âˆ’Î±âˆ—)Ë™Î±
(7.31)
Ë™V â‰¤| e | (IBaBË™x + BË™y) âˆ’(2 + IBx2)Î±âˆ—| e |
âˆ’(2 + IBx2)(Î±âˆ—âˆ’Î±) | e | + 1
Î³ (Î± âˆ’Î±âˆ—)Ë™Î±
(7.32)
and further:
Ë™V â‰¤| e | (IBaBË™x + BË™y) âˆ’(2 + IBx2)Î±âˆ—| e |
+ (Î±âˆ—âˆ’Î±)

(2 + IBx2) | e | âˆ’1
Î³ Ë™Î±

(7.33)
Using the adaptation law for the adaptive learning rate (Î±) as:
Ë™Î± = (2 + IBx2)Î³ | e | âˆ’Î½Î³ Î±
(7.34)

96
Fuzzy Neural Networks for Real Time Control Applications
in which Î½ has a small real value, the time derivative of the Lyapunov
function can be rewritten as:
Ë™V â‰¤| e | (IBaBË™x + BË™y)
âˆ’(2 + IBx2)Î±âˆ—| e | +(Î±âˆ—âˆ’Î±)Î½Î±
(7.35)
so that:
Ë™V â‰¤| e | (IBaBË™x + BË™y)
(7.36)
âˆ’(2 + IBx2)Î±âˆ—| e | âˆ’Î½(Î± âˆ’Î±âˆ—
2 )2 + Î½Î±âˆ—2
4
considering the fact that Î±âˆ—â‰¥
2(IBaBË™x+BË™y)
2+IBx2
, we have | e | (IBaBË™x + BË™y)
âˆ’Î±âˆ—
2 (2 + IBx2) | e |â‰¤0 and consequently:
Ë™V â‰¤âˆ’Î±âˆ—
2 (2 + IBx2) | e | +Î½Î±âˆ—2
4
(7.37)
Therefore, error converges to a very small region around zero in which
| e |â‰¤
Î±âˆ—Î½
2(2+IBx2), and it remains there. It should also be noted that Î½ is a
small user-defined positive number that can be selected as small as desired
to make this neighborhood as narrow as requested by the user.
7.2.2 Identification Using T2FNN with Elliptic Type-2 MF
In this section, the SMC theory-based parameter update rules for the
T2FNN with elliptic MFs are discussed. The introduced novel training
algorithm is simple and computationally less expensive than the gradient-
based methods, and the adaptation laws have closed forms. The stability of
the proposed method is proved using the Lyapunov approach. Similar to
the previous section, the error between the measured output of the system
and the output of the T2FNN is defined as a time-varying sliding surface.
Similar to identification using Gaussian type-2 MF, the sliding surface is
defined as follows:
S (e(t)) = e(t) = yN(t) âˆ’y(t) = 0
(7.38)

Sliding Mode Control Theory-Based Parameter Adaptation Rules for Fuzzy Neural Networks
97
The sliding surface for the nonlinear system is defined as:
Sp = e
(7.39)
7.2.2.1 Parameter Update Rules for the T2FNN
The parameter update rules for the T2FNN are given by the following
theorem.
Theorem 7.2. Consider a T2FNN structure that benefits from elliptic type-2
MF. Using the following assumptions:
|xi| < Bx, x2
i < Bx2, |Ë™xi| < BË™x, |Ë™y| < BË™y
if the parameter update rules for the T2FNN are chosen as (7.40)-(7.47) with the
selection of the adaptive learning rate (Î±) as in (7.47), e(t), the error between the
desired output and the output of the T2FNN converges asymptotically to zero in
finite time and sliding motion will be achieved.
Ë™a2,ik = Î³1
âŽ§
âŽ¨
âŽ©
ln

1 âˆ’
xiâˆ’cik
dik

a2,ik
a2
2,ik
+
xiâˆ’cik
dik

a2,ik ln
xiâˆ’cik
dik

a2,ik(1 âˆ’T2,ik)
âŽ«
âŽ¬
âŽ­
âˆ’1
H(xi, cik, dik)
(7.40)
Ë™a1,ik = Î³1
âŽ§
âŽ¨
âŽ©
ln

1 âˆ’
xiâˆ’cik
dik

a1,ik
a2
1,ik
+
xiâˆ’cik
dik

a1,ik ln
xiâˆ’cik
dik

a1,ik(1 âˆ’T1,ik)
âŽ«
âŽ¬
âŽ­
âˆ’1
H(xi, cik, dik)
(7.41)
Ë™cik = âˆ’Î³1
|dik|a2,ik(1 âˆ’T2,ik)sgn(xi âˆ’cik)
|xi âˆ’cik|a2,ikâˆ’1
(7.42)
Ë™dik = Î³1
(1 âˆ’T2,ik)|dik|a2,ik+1
|xi âˆ’cik|a2,ik
sgn(dik)
(7.43)
Ë™br = âˆ’q
W r + (1 âˆ’q)
W r
Tr r
Î±sign(e)
(7.44)

98
Fuzzy Neural Networks for Real Time Control Applications
in which:
r =
 R

k=1
(q
W r + (1 âˆ’q)
W r)

(7.45)
Ë™ari = âˆ’xi
q
W r + (1 âˆ’q)
W r
Tr r
Î±sign(e)
(7.46)
Ë™Î± = Î³ (I + 2) | e | âˆ’Î½Î³ Î±, 0 < Î³ , Î½
(7.47)
In the above Î³1 and Î³ are the learning rates considered for the adaptation of the
learning rate and they need to be selected as positive.
7.2.2.2 Proof of Theorem 7.2
As mentioned earlier, the elliptic type-2 MFs are used so that:
Î¼ik =

1 âˆ’

xi âˆ’cik
dik

a2,ik1/a2,ik
H(xi, cik, dik)
(7.48)
in which Î¼ik is the lower bound of the kth MF considered for the ith input
and H(xi, cik, dik) is the rectangular function defined as:
H(xi, c, d) =
 1
c âˆ’d < xi < c + d
0
otherwise
If cik âˆ’dik < xi < cik + dik, then the following equation is valid:
ln(Î¼ik) =
1
a2,ik
ln

1 âˆ’

xi âˆ’cik
dik

a2,ik
(7.49)
Define T2,ik =
xiâˆ’cik
dik

a2,ik so that:
Ë™T2,ik = Ë™a2,ik

xi âˆ’cik
dik

a2,ik
ln

xi âˆ’cik
dik

+ a2,ik
 Ë™xi âˆ’Ë™cik
|dik| sgn(xi âˆ’cik)

Sliding Mode Control Theory-Based Parameter Adaptation Rules for Fuzzy Neural Networks
99
âˆ’Ë™dik
|xi âˆ’cik|
d2
ik
sgn(dik)
 
xi âˆ’cik
dik

a2,ikâˆ’1
(7.50)
and the time derivative of Î¼ik is achieved as:
Ë™Î¼ik =
âˆ’Î¼ikË™a2,ik
a2
2,ik
ln

1 âˆ’

xi âˆ’cik
dik

a2,ik
âˆ’
Î¼ik
a2,ik
Ë™T2,ik
1 âˆ’T2,ik
(7.51)
The upper bound of the kth type-2 MFs for the ith input is defined as:
Î¼ik =

1 âˆ’

xi âˆ’cik
dik

a1,ik1/a1,ik
H(xi, cik, dik)
(7.52)
Define T1,ik =
xiâˆ’cik
dik

a1,ik so that:
Ë™T1,ik = Ë™a1,ik

xi âˆ’cik
dik

a1,ik
ln

xi âˆ’cik
dik

+ a1,ik
 Ë™xi âˆ’Ë™cik
|dik| sgn(xi âˆ’cik)
âˆ’Ë™dik
|xi âˆ’cik|
d2
ik
sgn(dik)
 
xi âˆ’cik
dik

a1,ikâˆ’1
(7.53)
and further:
Ë™Î¼ik = âˆ’Î¼ikË™a1,ik
a2
1,ik
ln

1 âˆ’

x1 âˆ’cik
dik

a1,ik
âˆ’Î¼ik
a1,ik
Ë™T1,ik
1 âˆ’T1,ik
(7.54)
Î¼ik
1 âˆ’T1,ik

xi âˆ’cik
dik

a1,ikâˆ’1
=
Î¼ik âˆ’Î¼ik + Î¼ik
1 âˆ’T2,ik + T2,ik âˆ’T1,ik

xi âˆ’cik
dik

a2,ikâˆ’1
+

xi âˆ’cik
dik

a1,ikâˆ’1
âˆ’

xi âˆ’cik
dik

a2,ikâˆ’1
=
Î¼ik
(1 âˆ’T2,ik)

xi âˆ’cik
dik

a2,ikâˆ’1 
1 +
Î¼ik âˆ’Î¼ik
Î¼ik

Ã—

100
Fuzzy Neural Networks for Real Time Control Applications
âŽ›
âŽ1 +

xi âˆ’cik
dik

a1,ikâˆ’1
âˆ’

x1 âˆ’cik
dik

a2,ikâˆ’1
Ã—

xi âˆ’cik
dik

a2,ikâˆ’1âˆ’1 âŽž
âŽ 
âŽ›
âŽ
1
1 âˆ’T1,ikâˆ’T2,ik
1âˆ’T2,ik
âŽž
âŽ 
(7.55)
Considering the fact that the MFs have nonzero value only if cik âˆ’dik <
xi < cik + dik we have:
0 < T1,ik < 1
T2,ik < T1,ik
(7.56)
so that:
0 < T1,ik âˆ’T2,ik < 1 âˆ’T2,ik â‡’0 < T1,ik âˆ’T2,ik
1 âˆ’T2,ik
< 1
(7.57)
This means it is possible to use the Maclaurin series as:
Î¼ik
1 âˆ’T1,ik

xi âˆ’cik
dik

a1,ikâˆ’1
=
Î¼ik
(1 âˆ’T2,ik)

xi âˆ’cik
dik

a2,ikâˆ’1
1 +
Î¼ik âˆ’Î¼ik
Î¼ik

Ã—

1 + T1,ik âˆ’T2,ik
1 âˆ’T2,ik
+
T1,ik âˆ’T2,ik
1 âˆ’T2,ik
2
+ H.O.T.

Ã—

1 +
 
xi âˆ’cik
dik

a1,ikâˆ’1
âˆ’

xi âˆ’cik
dik

a2,ikâˆ’1 
Ã—

xi âˆ’cik
dik

a2,ikâˆ’1âˆ’1 
(7.58)
It should also be noted that:
Î¼ik
(1 âˆ’T1,ik)

xi âˆ’cik
dik

a1,ikâˆ’1
=
Î¼ik
(1 âˆ’T2,ik)

xi âˆ’cik
dik

a2,ikâˆ’1
(1 + ik)
=
Î¼ik
(1 âˆ’T2,ik)

xi âˆ’cik
dik

a2,ikâˆ’1
+ 
â€²
ik
(7.59)

Sliding Mode Control Theory-Based Parameter Adaptation Rules for Fuzzy Neural Networks
101
in which:

â€²
ik =
Î¼ik
(1 âˆ’T2,ik)

xi âˆ’cik
dik

a2,ikâˆ’1
ik
(7.60)
and:
ik =

1 +
Î¼ik âˆ’Î¼ik
Î¼ik

Ã—

1 + T1,ik âˆ’T2,ik
1 âˆ’T2,ik
+
T1,ik âˆ’T2,ik
1 âˆ’T2,ik
2
+ H.O.T.

Ã—

1 +

x2 âˆ’cik
dik

a1,ikâˆ’1
âˆ’

xi âˆ’cik
dik

a2,ikâˆ’1
Ã—

xi âˆ’cik
dik

a2,ikâˆ’1âˆ’1âŽž
âŽ âˆ’1
(7.61)
Considering the adaptation laws as in (7.40)-(7.47), we have:
Ë™Î¼ik = âˆ’Î³ Î¼ik âˆ’
Î¼ik
1 âˆ’T2,ik

xi âˆ’cik
dik
 Ë™xisgn(xi âˆ’cik)
(7.62)
Ë™Î¼ik = âˆ’Î³ Î¼ik + 	ik
(7.63)
where 	ik is defined as:
	ik = Î³ Î¼ik âˆ’Î³ Î¼ik âˆ’
Î¼ik
1 âˆ’T1,ik

xi âˆ’cik
dik
 Ë™xisgn(xi âˆ’cik)
(7.64)
and further:
Ë™wr = âˆ’Î³ wrsgn(e) + wr
(7.65)
and
Ë™wr = âˆ’Î³ wrsgn(e) + wr
(7.66)

102
Fuzzy Neural Networks for Real Time Control Applications
where wr and wr are bounded and satisfy the following equation:
|wr|, |wr| < Bw
(7.67)
By using the following Lyapunov function, the stability condition is
analyzed:
V = 1
2e2 + 1
2Î³ (Î± âˆ’Î±âˆ—)2,
0 < Î³
(7.68)
The time derivative of (7.68) can be calculated as follows:
Ë™V = Ë™ee + 1
Î³ Ë™Î±(Î± âˆ’Î±âˆ—) = e(Ë™yN âˆ’Ë™y) + 1
Î³ Ë™Î±(Î± âˆ’Î±âˆ—)
(7.69)
By taking the time-derivative of (7.7), the following term is obtained:
Ë™yN = Ë™q
N

r=1
fr Ëœwr + q
N

r=1
(Ë™fr Ëœwr + fr Ë™Ëœwr) âˆ’Ë™q
N

r=1
fr Ëœwr
+ (1 âˆ’q)
N

r=1
(Ë™fr Ëœwr + fr Ë™Ëœwr)
(7.70)
Using the adaptation laws as in (7.40)-(7.47), the following term can be
obtained:
Ë™yN = Ë™q
N

r=1
fr Ëœwr + q
N

r=1
(Ë™fr Ëœwr + fr Ëœwr) âˆ’Ë™q
N

r=1
fr Ëœwr
+ (1 âˆ’q)
N

r=1
(Ë™fr Ëœwr + fr Ëœwr)
(7.71)
and further we have:
Ë™yN = âˆ’
1
F( ËœW âˆ’ËœW)T Î±sgn (e)
N

r=1
fr( Ëœwr âˆ’Ëœwr)
+
N

r=1
Ë™fr

qËœwr + (1 âˆ’q) Ëœwr


Sliding Mode Control Theory-Based Parameter Adaptation Rules for Fuzzy Neural Networks
103
= âˆ’Î±sgn (e)
+
N

r=1

I

i=1
(Ë™arixi + ariË™xi) + Ë™br


qËœwr + (1 âˆ’q) Ëœwr

+ fr Ëœwr + fr Ëœwr

(7.72)
If (7.72) is inserted into the candidate Lyapunov function, (7.73) can be
obtained:
Ë™V = Ë™ee + 1
Î³ Ë™Î±(Î± âˆ’Î±âˆ—) = e(Ë™yN âˆ’Ë™y) + 1
Î³ Ë™Î±(Î± âˆ’Î±âˆ—)
= e

âˆ’Î±sgn (e) +
N

r=1

I
i=1
(Ë™arixi + ariË™xi) + Ë™br

Ã—

q Ëœwr + (1 âˆ’q) Ëœwr

+ fr Ëœwr + fr Ëœwr

âˆ’Ë™y

+ 1
Î³ Ë™Î±(Î± âˆ’Î±âˆ—)
(7.73)
Following some steps similar to the previous section, we obtain the
following equation:
Ë™V = âˆ’| e | 2Î± + e
 N

r=1

I

i=1

âˆ’Î±sgn (e) x2
i
+ariË™xi

q Ëœwr + (1 âˆ’q) Ëœwr

âˆ’Ë™y + fr Ëœwr + fr Ëœwr

+ 1
Î³ Ë™Î±(Î± âˆ’Î±âˆ—)
Ë™V < âˆ’| e | 2Î±+ | e |
 N

r=1

I
i=1

âˆ’Î±Bx2
+BaBË™x
q Ëœwr + (1 âˆ’q) Ëœwr

+ BË™yBf Bw

+ 1
Î³ Ë™Î±(Î± âˆ’Î±âˆ—)
Considering the upper bounds of xiâ€™s, Ë™xi, and ariâ€™s as in the assumptions
of Theorem 7.2, we have:
Ë™V < âˆ’| e | 2Î±+ | e |
 N

r=1

âˆ’IÎ±Bx2 + IBaBË™x

qËœwr
+ (1 âˆ’q) Ëœwr

+ BË™y + Bf Bw

+ 1
Î³ Ë™Î±(Î± âˆ’Î±âˆ—)

104
Fuzzy Neural Networks for Real Time Control Applications
< âˆ’| e | 2Î±+ | e |

âˆ’IÎ±Bx2 + IBaBË™x

q
N

r=1
Ëœwr
+ (1 âˆ’q)
N

r=1
Ëœwr
 + BË™y

+ 1
Î³ Ë™Î±(Î± âˆ’Î±âˆ—)
So that:
Ë™V < âˆ’| e | 2Î±+ | e |

âˆ’IÎ±Bx2 + IBaBË™x + BË™y + Bf Bw

+ 1
Î³ Ë™Î±(Î± âˆ’Î±âˆ—)
and further:
Ë™V < âˆ’| e | 2Î± + IÎ±Bx2
 + | e | IBaBË™x + BË™y + Bf Bw

+ 1
Î³ Ë™Î±(Î± âˆ’Î±âˆ—)
The parameter Î±âˆ—is considered to satisfy the following inequality:
Î±âˆ—â‰¥2(IBaBË™x + BË™yBf Bw)
2 + IBx2
where this parameter is considered as determined during the adaptation of
the learning rate.
Ë™V â‰¤âˆ’| e | (2Î± + IÎ±Bx2)+ | e | (IBaBË™x + BË™y + Bf Bw)
+ (2 + IBx2)Î±âˆ—| e | âˆ’(2 + IBx2)Î±âˆ—| e | + 1
Î³ (Î± âˆ’Î±âˆ—)Ë™Î±
(7.74)
so that:
Ë™V â‰¤| e | (IBaBË™x + BË™y + Bf Bw) âˆ’(2 + IBx2)Î±âˆ—| e |
âˆ’(2 + IBx2)(Î±âˆ—âˆ’Î±) | e | + 1
Î³ (Î± âˆ’Î±âˆ—)Ë™Î±
(7.75)

Sliding Mode Control Theory-Based Parameter Adaptation Rules for Fuzzy Neural Networks
105
and further:
Ë™V â‰¤| e | (IBaBË™x + BË™y + Bf Bw) âˆ’(2 + IBx2)Î±âˆ—| e |
+ (Î±âˆ—âˆ’Î±)

(2 + IBx2) | e | âˆ’1
Î³ Ë™Î±

(7.76)
Using the adaptation law for the adaptive learning rate (Î±) as:
Ë™Î± = (2 + IBx2)Î³ | e | âˆ’Î½Î³ Î±
(7.77)
in which Î½ has a small real value. Using this adaptation law, the time
derivative of the Lyapunov function can be rewritten as:
Ë™V â‰¤| e | (IBaBË™x + BË™y + Bf Bw)
(7.78)
âˆ’(2 + IBx2)Î±âˆ—| e | + (Î±âˆ—âˆ’Î±)Î½Î±
so that:
Ë™V â‰¤| e | (IBaBË™x + BË™y + Bf Bw)
(7.79)
âˆ’(2 + IBx2)Î±âˆ—| e | âˆ’Î½(Î± âˆ’Î±âˆ—
2 )2 + Î½Î±âˆ—2
4
Considering the fact that Î±âˆ—â‰¥
2(IBaBË™x+BË™y+Bf Bw)
2+IBx2
, we have | e | (IBaBË™x +
BË™y + Bf Bw) âˆ’Î±âˆ—
2 (2 + IBx2) | e |â‰¤0 and consequently:
Ë™V â‰¤âˆ’Î±âˆ—
2 (2 + IBx2) | e | +Î½Î±âˆ—2
4
(7.80)
So that error converges to a very small region around zero in which
| e |â‰¤
Î±âˆ—Î½
2(2+IBx2).
7.3 CONTROLLER DESIGN
7.3.1 Control Scheme Incorporating a T2FNN Structure
In this section, the control scheme that benefits from a T2FNN whose
parameters are adapted by a SMC theory-based approach is presented.
Figure 7.1 shows the control scheme in which a PD controller works in
parallel with a T2FNN. The PD controller is there to guarantee the global

106
Fuzzy Neural Networks for Real Time Control Applications
PD controller
d/dt
Desired
value +
+
âˆ’
âˆ’
Plant
T2FNN adaptive
learning rate
Output
u
e
e
uc
uf
Î£
Figure 7.1 Structure of the proposed controller.
asymptotic stability in compact space and establishes a sliding manifold for
the states of the system. The PD control signal is as follows:
uc = kPe + kDË™e
(7.81)
in which e is defined as e = xd âˆ’x and represents the error of the system,
xd is the reference input, and kP, kD are the gains of the PD controller.
7.3.1.1 T2FNN
A2-C0 fuzzy system is preferred as the structure of the fuzzy system. All the
models have their closed forms to enable their use for further mathematical
analysis. The ijth rule of a A2-C0 fuzzy system with two inputs can be
written as:
IF x1 is ËœA1i and x2 is ËœA2j THEN uf = fij
i = 1, .., I j = 1, .., J
(7.82)
where x1 and x2 are the input variables, I and J are the number of MFs for
x1 and x2, respectively, uf is the output variable, fijâ€™s are the parameters in
the consequent part of the fuzzy system, and ËœA1i, ËœA2j are the type-2 fuzzy
sets for the first and second input and their corresponding fuzzy MFs are
ËœÎ¼1i, ËœÎ¼2j, respectively. The upper and lower MFs of ËœÎ¼1i are considered to be
Î¼1i and Î¼1i and the upper and lower MFs of ËœÎ¼2j are considered to be Î¼2j
and Î¼2j. The final output of the system can be written as [8]:
y = q

J
j=1

I
i=1 W ij fij

J
j=1

I
i=1 W ij
+ (1 âˆ’q)

J
j=1

I
i=1 W ij fij

J
j=1

I
i=1 W ij
(7.83)

Sliding Mode Control Theory-Based Parameter Adaptation Rules for Fuzzy Neural Networks
107
where W ij = Î¼1iÎ¼2j, W ij = Î¼1iÎ¼2j, and the parameter q is the weighting
parameter that reflects the sharing of the contribution of the upper and
lower MFs.
Type-2 MFs, which are used for the control purposes, can be Gaussian
type-2 MF with uncertain Ïƒ value or elliptic type-2 MF.
The A2-C0 fuzzy system that benefits from type-2 MFs in the premise
part and crisp numbers for the consequent part is used for control purposes.
Different from the identification part in which first order TSK fuzzy system
is used, in the control part its zero order variant is used. The fuzzy if-then
rule Rij of a zero-order TSK model with two input variables where the
consequent part has a constant value is defined as follows:
Rij : If e is ËœA1i,k and Ë™e is ËœA2j, then uf = fij
where fij are constant values that are updated during the training. Using
uncertain values for Ïƒ the type-2 MF has a footprint of uncertainty that
is bounded with an upper and lower MF. The upper and lower MFs are
denoted as Î¼(x) and Î¼(x), respectively. The firing strength of the rule
Rij is obtained as a T-norm of the MFs in the premise part (by using a
multiplication operator):
W ij = Î¼1i(e)Î¼2j(Ë™e)
(7.84)
W ij = Î¼1i(e)Î¼2j(Ë™e)
(7.85)
where the real constants Ïƒ, Ïƒ > 0, and c are among the tunable parameters
of the above T2FNN structure.
Hence, (7.84) and (7.85) can be rewritten as follows:
W ij = exp

âˆ’(e âˆ’c1i
Ïƒ 1i
)2 âˆ’(Ë™e âˆ’c2j
Ïƒ 2j
)2

(7.86)
W ij = exp

âˆ’(e âˆ’c1i
Ïƒ 1i
)2 âˆ’(Ë™e âˆ’c2j
Ïƒ 2j
)2

(7.87)
The computational output of A2-C0 structure is derived as:
uf =
q 
I
i=1

J
j=1 fijW ij

I
i=1

J
j=1 W ij
+
(1 âˆ’q) 
I
i=1

J
j=1 fijW ij

I
i=1

J
j=1 W ij
(7.88)

108
Fuzzy Neural Networks for Real Time Control Applications
After the normalization of (7.88), the output signal of the T2FNN will
acquire the following form:
uf = q
I
i=1
J
j=1
fij
W ij + (1 âˆ’q)
I

i=1
J
j=1
fij
W ij
(7.89)
where 
W ij and 
W ij are the normalized values of the lower and upper outputs
corresponding to the ijth rule of the fuzzy system:

W ij =
W ij

I
i=1

J
j=1 W ij
(7.90)

W ij =
W ij

I
i=1

J
j=1 W ij
(7.91)

W (t) =


W 11 (t) 
W 12 (t) . . . 
W 21 (t) . . . 
W ij (t) . . . 
W IJ (t)
T
are the
vector of the normalized lower output signals of the neurons from the
second hidden layer of the first and second T2FNNs, respectively; 
W (t) =

W 11 (t) 
W 12 (t) . . . 
W 21 (t) . . . 
W ij (t) . . . 
W IJ (t)
T
is the vector of
the normalized upper output signals of the neurons from the second
hidden layer of the IT2FNNs; and Ïƒ 1 = Ïƒ 11 . . . Ïƒ 1i . . . Ïƒ 1I
T, Ïƒ 2 =

Ïƒ 21 . . . Ïƒ 2j . . . Ïƒ 2J
T
are vectors of the tuning parameters Ïƒ of the
lower bounds of type-2 Gaussian MFs with uncertain variance. Ïƒ 1 =
[Ïƒ 11 . . . Ïƒ 1i . . . Ïƒ 1I]T, Ïƒ 2 =

Ïƒ 21 . . . Ïƒ 2j . . . Ïƒ 2J
T are vectors of the
tuning parameters Ïƒ of the upper bounds of type-2 Gaussian MFs with un-
certain variance. c1 = [c11 . . . c1i . . . c1I]T and c2 =

c21 . . . c2j . . . c2J
T
are vectors of the tuning parameters c of the lower and upper bounds of
type-2 Gaussian MFs with uncertain variance.
It is assumed that due to the control scheme used in Fig. 7.1, where
the conventional controller serves to guarantee global asymptotic stability
in compact space, the input signals, e(t) and Ë™e(t), and their time derivatives
can be considered bounded:
|e(t)| â‰¤Be, |Ë™e(t)| â‰¤Be |Â¨e(t)| â‰¤BË™e
âˆ€t
(7.92)

Sliding Mode Control Theory-Based Parameter Adaptation Rules for Fuzzy Neural Networks
109
where Be and BË™e are assumed to be some unknown positive constants. The
adaptation law for the tunable parameters is considered such that Ïƒ, Ïƒ, and
c of the Gaussian MFs are bounded as follows:
BÏƒ â‰¤
""Ïƒ 1
"" â‰¤BÏƒ , BÏƒ â‰¤
""Ïƒ 2
"" â‰¤BÏƒ ,
(7.93)
âˆ¥c1âˆ¥â‰¤Bc,
âˆ¥c2âˆ¥â‰¤Bc
BÏƒ â‰¤âˆ¥Ïƒ 1âˆ¥â‰¤BÏƒ , BÏƒ â‰¤âˆ¥Ïƒ 2âˆ¥â‰¤BÏƒ ,
(7.94)
âˆ¥c1âˆ¥â‰¤Bc,
âˆ¥c2âˆ¥â‰¤Bc
"""f ij
""" â‰¤Bf
where BÏƒ, BÏƒ, Bc, and Bf are some unknown positive constants.
Similar to the previous case, it follows that 0 < 
W ij < 1 and
0 < 
W ij < 1. In addition, by definition, 
I
i=1

J
j=1 
W ij
=
1 and

I
i=1

J
j=1 
W ij = 1. It is also considered that u and Ë™u are also bounded
signals, i.e.,
|u (t)| â‰¤Bu,
|Ë™u (t)| â‰¤BË™u
âˆ€t
(7.95)
where Bu and BË™u are two unknown positive constants.
7.3.2 SMC Theory-Based Learning Algorithm for T2FNN
with Gaussian MFs with Uncertain Ïƒ Values
The zero value of the learning error coordinate uc (t) can be defined [9] as
time-varying sliding surface, i.e.,
Sc

uf , u

= uc (t) = uf (t) + u (t)
(7.96)
The sliding manifold works as a guideline to train the parameters of
T2FNN. This sliding surface acts as the condition that the T2FNN is trained
to become a nonlinear regulator to obtain the desired response during
the tracking-error convergence by compensating the nonlinearity of the
controlled plant. The sliding surface for the nonlinear system under control
Sp (e, Ë™e) is defined as:
Sp (e, Ë™e) = Ë™e + Ï‡e
(7.97)
with Ï‡ being a constant determining the slope of the sliding surface.
Definition: An sliding motion will appear on the sliding line Sc
uf , u =
uc (t) = 0 after a time th, if the condition Sc(t)Ë™Sc(t) = uc (t) Ë™uc (t) < 0 is

110
Fuzzy Neural Networks for Real Time Control Applications
satisfied for all t in some nontrivial semi-open subinterval of time of the
form [t, th) âŠ‚(âˆ’âˆž, th).
It is best to design an online learning algorithm for the parameters of
T2FNN such that the sliding mode condition of the above definition is
enforced.
7.3.2.1 Parameter Update Rules For T2FNN
Theorem 7.3. If the adaptation laws for T2FNN parameters are chosen as:
Ë™c1i = âˆ’Î²1
Ïƒ 2
1i
e âˆ’c1i
sgn(uc)
(7.98)
Ë™c2j = âˆ’Î²1
Ïƒ 2
2j
Ë™e âˆ’c2j
sgn(uc)
(7.99)
Ë™Ïƒ 1i = âˆ’Î²1
Ïƒ 3
1i
(e âˆ’c1i)2 sgn(uc)
(7.100)
Ë™Ïƒ 2j = âˆ’Î²1
Ïƒ 3
2j
(Ë™e âˆ’c2j)2 sgn(uc)
(7.101)
Ë™Ïƒ 1i = âˆ’Î²1
Ïƒ 3
1i
(e âˆ’c1i)2 sgn(uc)
(7.102)
Ë™Ïƒ 2j = âˆ’Î²1
(Ïƒ 2j)3
(Ë™e âˆ’c2j)2 sgn(uc)
(7.103)
Ë™fij = âˆ’Î±
q
W ij + (1 âˆ’q)
W ij
q
W + (1 âˆ’q)
WTq
W + (1 âˆ’q)
Wsgn(uc)
(7.104)
Ë™Î± = Î³1 |uc| âˆ’Î½Î³1Î±
(7.105)
then, given an arbitrary initial condition uc(0), the learning error uc(t) will converge
firmly to zero during a finite time th.
There is a relation between the sliding line Sp and uc(t) (the classical
controller signal). If Ï‡ is taken as Ï‡ =
kP
kD , the following equation is
obtained:
Sc = uc = kDË™e + kPe = kD

Ë™e + kP
kD
e

= kDSp
(7.106)

Sliding Mode Control Theory-Based Parameter Adaptation Rules for Fuzzy Neural Networks
111
Remark. Equation (7.106) indicates that the convergence of Sc toward
zero guarantees the convergence of Sp toward zero and there exists a sliding
motion in the states of the system.
7.3.2.2 Proof of Theorem 7.3
The stability analysis of the learning algorithm is considered in this section.
The following variables are defined:
A1i =

âˆ’(e âˆ’c1i
Ïƒ 1i
)2

(7.107)
U1i =

âˆ’(e âˆ’c1i
Ïƒ 1i
)2

(7.108)
A2j =

âˆ’(Ë™e âˆ’c2j
Ïƒ 2j
)2

(7.109)
U2j =

âˆ’(Ë™e âˆ’c2j
Ïƒ 2j
)2

(7.110)
Considering (7.107)-(7.110) we have:
Ë™Î¼1i(e) = âˆ’2A1i Ë™A1iÎ¼1i(e)
(7.111)
Ë™Î¼1i(e) = âˆ’2U1i Ë™U1iÎ¼1i(e)
(7.112)
Ë™Î¼2j(Ë™e) = âˆ’2A2j( Ë™A2j)Î¼2j(Ë™e)
(7.113)
Ë™Î¼2j(Ë™e) = âˆ’2U2j( Ë™U2j)Î¼2j(Ë™e)
(7.114)
In addition, considering (7.90) and (7.91), the time derivatives of 
W ij
and 
W ij are obtained as:

W ij =
W ij

I
i=1

J
j=1 W ij
â‡’Ë™
W ij =

Î¼1i(e)Î¼2j(Ë™e)
â€² 
I
i=1

J
j=1 W ij

 
I
i=1

J
j=1 W ij
2
âˆ’
W ij
 
I
i=1

J
j=1 Î¼1i(e)Î¼2j(Ë™e)
â€²
 
I
i=1

J
j=1 W ij
2
(7.115)

112
Fuzzy Neural Networks for Real Time Control Applications

W ij =
W ij

I
i=1

J
j=1 W ij
â‡’Ë™
W ij =
Î¼1i(e)Î¼2j(Ë™e)â€² 
I
i=1

J
j=1 W ij

 
I
i=1

J
j=1 W ij
2
âˆ’
W ij
 
I
i=1

J
j=1 Î¼1i(e)Î¼2j(Ë™e)
â€²
 
I
i=1

J
j=1 W ij
2
(7.116)
Since
ËœW ij
=
(W ij)/(
I
i=1

J
j=1 W ij) and
ËœW ij
=
(W ij)/
(
I
i=1

J
j=1 W ij), we have:
Ë™
W ij =
Ë™Î¼1i(e)Î¼2j(Ë™e) + Î¼1i(e) Ë™Î¼2j(Ë™e)

I
i=1

J
j=1 W ij
âˆ’

W ij

I
i=1

J
j=1
 Ë™Î¼1i(e)Î¼2j(Ë™e) + Î¼1i(e) Ë™Î¼2j(Ë™e)

I
i=1

J
j=1 W ij
=
âˆ’2A1i Ë™A1iÎ¼1i(e)Î¼2j(Ë™e) âˆ’2A2j Ë™A2jÎ¼1i(e)Î¼2j(Ë™e)

I
i=1

J
j=1 W ij
âˆ’

W ij

I
i=1

J
j=1
 âˆ’2A2j Ë™A2jÎ¼1i(e)Î¼2j(Ë™e)

I
i=1

J
j=1 Wij
âˆ’

W ij

I
i=1

J
j=1
 âˆ’2A1i Ë™A1iÎ¼1i(e)Î¼2j(Ë™e)

I
i=1

J
j=1 W ij
(7.117)
and further:
Ë™
W ij = âˆ’
W ij Ë™Nij + 
W ij
I

i=1
J
i=1

W ij Ë™Nij
(7.118)
Ë™
W ij = âˆ’
W ij Ë™Nij + 
W ij
I

i=1
J
i=1

W ij Ë™Nij
(7.119)
in which:
Ë™Nij = 2A1i Ë™A1i + 2A2j Ë™A2j, Ë™Nij = 2U1i Ë™U1i + 2U2j Ë™U2j
(7.120)

Sliding Mode Control Theory-Based Parameter Adaptation Rules for Fuzzy Neural Networks
113
Ë™A1i = (Ë™e âˆ’Ë™c1i)Ïƒ 1i âˆ’(e âˆ’c1i)Ë™Ïƒ 1i
Ïƒ 2
1i
, Ë™A2j =
(Â¨e âˆ’Ë™c2j)Ïƒ 2j âˆ’(Ë™ek âˆ’c2j)Ë™Ïƒ 2j
Ïƒ 2
2j
Ë™U1i = (Ë™e âˆ’Ë™c1i)Ïƒ 1i âˆ’(e âˆ’c1i) Ë™Ïƒ 1i
Ïƒ 2
1i
, Ë™U2j = (Â¨e âˆ’Ë™c2j)Ïƒ 2j âˆ’(Ë™e âˆ’c2j) Ë™Ïƒ 2j
Ïƒ 2
2j
It is possible to use Maclaurin series expansion to obtain following
equations:
Ë™x1 âˆ’Ë™c1i
Ïƒ 1i
=
Ë™e âˆ’Ë™c1i
Ïƒ 1i âˆ’Ïƒ 1i + Ïƒ 1i
= Ë™e âˆ’Ë™c1i
Ïƒ 1i
âŽ›
âŽœâŽœâŽœâŽ1 âˆ’Ïƒ 1i âˆ’Ïƒ 1i
Ïƒ 1i
+ (Ïƒ 1i âˆ’Ïƒ 1i)2
Ïƒ 2
1i
+ H.O.T
$
%&
'
D1i
âŽž
âŽŸâŽŸâŽŸâŽ (7.121)
and:
e âˆ’c1i
Ïƒ 1i
=
e âˆ’c1i
Ïƒ 1i âˆ’Ïƒ 1i + Ïƒ 1i
= e âˆ’c1i
Ïƒ 1i

1 âˆ’Ïƒ 1i âˆ’Ïƒ 1i
Ïƒ 1i
+ (Ïƒ 1i âˆ’Ïƒ 1i)2
Ïƒ 1i
+ H.O.T

(7.122)
Using (7.121), we have:
A1i Ë™A1i = U1i Ë™U1i + (Ë™e âˆ’Ë™c1i)
Ïƒ 1i
(e âˆ’c1i)
Ïƒ 1i
2D1i + D2
1i

(7.123)
and similarly using (7.122), we find that:
A2j Ë™A2j = U2j Ë™U2j + (Â¨e âˆ’Ë™c2j)
Ïƒ 2j
(Ë™e âˆ’c2j)
Ïƒ 2j

2E2j + E2jE2
2j

(7.124)
in which:
E2j = âˆ’
Ïƒ 2j âˆ’Ïƒ 2j
Ïƒ 2j
+
(Ïƒ 2j âˆ’Ïƒ 2j)2
Ïƒ 2
2j
+ H.O.T
(7.125)

114
Fuzzy Neural Networks for Real Time Control Applications
It can be proved that |D1i| and |E2j| are bounded as |D1i| < BD and
|E2j| < BE. In order to analyze the stability of the controller with an
adaptive learning rate, the following Lyapunov function is proposed:
Vc = 1
2u2
c (t) + 1
2Î³1
(Î± âˆ’Î±âˆ—)2
(7.126)
The time derivative of the Lyapunov function (7.126) is derived as:
Ë™Vc = ucË™uc = uc(Ë™uf + Ë™u) + Ë™Î±
Î³1
(Î± âˆ’Î±âˆ—)
(7.127)
Since:
uf =
q 
I
i=1

J
j=1 fijW ij

I
i=1

J
j=1 W ij
+
(1 âˆ’q) 
I
i=1

J
j=1 fijW ij

I
i=1

J
j=1 W ij
= q
I

i=1
J
j=1
fij
W ij + (1 âˆ’q)
I
i=1
J
j=1
fij
W ij
(7.128)
and:
Ë™uf = q
I

i=1
J
j=1
(Ë™fij ËœW ij + fij Ë™ËœWij) + (1 âˆ’q)
I
i=1
J
j=1
(Ë™fij
W ij + fij Ë™

W ij) (7.129)
we obtain:
Ë™uf = q
I
i=1
J
j=1
âŽ›
âŽ
âŽ›
âŽâˆ’ËœW ij Ë™Kij + ËœW ij
I

i=1
J
j=1
ËœW ij Ë™Kij
âŽž
âŽ fij + ËœW ijË™fij
âŽž
âŽ 
+ (1 âˆ’q)
I
i=1
J
j=1
âŽ›
âŽ
âŽ›
âŽâˆ’
W ij Ë™Kij + 
W ij
I

i=1
J
j=1

W ij Ë™Kij
âŽž
âŽ fij + 
WijË™fij
âŽž
âŽ 
(7.130)

Sliding Mode Control Theory-Based Parameter Adaptation Rules for Fuzzy Neural Networks
115
Considering adaptation for sigma and center we have:
Ë™Vc = uc
âŽ›
âŽq
I

i=1
J
j=1

2

âˆ’
W ij
 Ë™e
Ïƒ 1i
A1i +
Â¨e
Ïƒ 2j
A2j

+ 
W ij
I

i=1
J
j=1

W ij
 Ë™e
Ïƒ 1i
A1i +
Â¨e
Ïƒ 2j
A2j

fij + 
W ijË™fij

+ q
I
i=1
J
j=1
âŽ›
âŽ2

âˆ’
W ij

2D1i + D2
1i + 2D2j + D2
2j

+
W ij
I

i=1
J
j=1

W ij
2D1i + D2
1i + 2D2j + D2
2j

fij
âŽž
âŽ 
+ (1 âˆ’q)
I

i=1
J
j=1
âŽ›
âŽ2

âˆ’
W ij
 Ë™e
Ïƒ 1i
A1i +
Â¨e
Ïƒ 2j
A2j

+
W ij
I

i=1
J
j=1

W ij
 Ë™e
Ïƒ 1i
A1i +
Â¨e
Ïƒ 2j
A2j

fij + 
W ijË™fij
âŽž
âŽ + Ë™u
âŽž
âŽ 
+ Ë™Î±
Î³1
(Î± âˆ’Î±âˆ—)
(7.131)
Considering assumptions (7.93) and (7.94), (7.131) can be rewritten as:
Ë™Vc â‰¤4Br |uc| + uc
âŽ›
âŽ
I
i=1
J
j=1
Ë™fij(q
W ij + (1 âˆ’q)
W ij) + Ë™u
âŽž
âŽ + Ë™Î±
Î³1
(Î± âˆ’Î±âˆ—)
â‰¤4Br |uc| âˆ’Î±âˆ—|uc| + (Î±âˆ—âˆ’Î±) |uc| + |uc| BË™u + Ë™Î±
Î³1
(Î± âˆ’Î±âˆ—) < 0
(7.132)
in which:
Br = Bf

3BD + 3BE + B2
e + B2
Ë™e + BcBe + BcBË™e
B2
Ïƒ

(7.133)

116
Fuzzy Neural Networks for Real Time Control Applications
using the adaptation law for Î± as:
Ë™Î± = Î³1 |uc| âˆ’Î½Î³1Î±
and taking Î±âˆ—as:
BË™u + 4Br < 1
2Î±âˆ—
we have:
Ë™Vc â‰¤âˆ’1
2Î±âˆ—|uc| + Î½Î±(Î± âˆ’Î±âˆ—)
= âˆ’1
2Î±âˆ—|uc| + Î½(Î± âˆ’Î±âˆ—)2 âˆ’Î½Î±âˆ—2
4
(7.134)
Furthermore:
Ë™Vc â‰¤âˆ’1
2Î±âˆ—|uc| + Î½(Î± âˆ’Î±âˆ—)2
(7.135)
Therefore, the Lyapunov function uc converges exponentially until |uc| â‰¤
2 Î½
Î±âˆ—(Î± âˆ’Î±âˆ—)2 and the parameters of the controller are bounded. Conse-
quently, the system states converge to a compact set R in which:
R =
)
u| |u| â‰¤2 Î½
Î±âˆ—(Î± âˆ’Î±âˆ—)2*
(7.136)
It should be noted that this region can be chosen to be as small as desired
by choosing an appropriate value for Î½. Consequently, u can be made as small
as desired.
7.3.3 SMC Theory-Based Learning Algorithm for T2FNN
with Elliptic MFs
In this section, the SMC theory-based parameter update rules for the
T2FNN with elliptic MFs are discussed. As will be seen later from the
parameter update rules, the introduced novel training algorithm is simple
and computationally less expensive than the gradient-based methods and the
adaptation laws have closed forms. Although the gradient-based methods
may result in instability, the stability of the proposed method is proved using
the Lyapunov approach.

Sliding Mode Control Theory-Based Parameter Adaptation Rules for Fuzzy Neural Networks
117
The sliding surface for the nonlinear system is defined as:
Sp(e, Ë™e) = Ë™e + Ï‡e
(7.137)
where Ï‡ is a positive variable that determines the desired trajectory of the
error signal.
Definition: A sliding motion will appear on the sliding manifold
Sp(e, Ë™e) = 0 after a time th if the condition Sp(t)Ë™Sp(t) < 0 is satisfied
for all tâ€™s in some nontrivial semi-open subinterval of time of the form
[t, th) âŠ‚(0, th).
Theorem 7.4. Consider the control structure that consists of a PD controller
that works in parallel with a T2FNN. The PD controller is responsible for
guaranteeing the stability of the system to be controlled in the compact set and it
further guarantees that:
|fij| < Bf , |x1| < Bx, |x2| < Bx,
|Ë™x1| < BË™x, |Ë™x2| < BË™x
Bd,min < |d1i|, Bd,min < |d2j|, |c1i| < Bc,
|c2j| < Bc, |u| < Bu
(7.138)
Then, if the parameter update rules for the T2FNN are chosen as (7.139)-(7.149)
with the selection of the adaptive learning rate (Î±) as in (7.149), for given arbitrary
initial conditions uc(0), uc(t) converges asymptotically to zero in finite time and
sliding motion will be achieved.
Ë™a2,1i = Î³
âŽ§
âŽ¨
âŽ©
ln

1 âˆ’
x1âˆ’c1i
d1i

a2,1i
a2
2,1i
+
x1âˆ’c1i
d1i

a2,1i ln
x1âˆ’c1i
d1i

a2,1i(1 âˆ’T2,1i)
âŽ«
âŽ¬
âŽ­
âˆ’1
H(x1, c1i, d1i)
(7.139)
Ë™a1,1i = Î³
âŽ§
âŽ¨
âŽ©
ln

1 âˆ’
x1âˆ’c1i
d1i

a1,1i
a2
1,1i
+
x1âˆ’c1i
d1i

a1,1i ln
x1âˆ’c1i
d1i

a1,1i(1 âˆ’T1,1i)
âŽ«
âŽ¬
âŽ­
âˆ’1
H(x1, c1i, d1i)
(7.140)

118
Fuzzy Neural Networks for Real Time Control Applications
Ë™a2,2j = Î³
âŽ§
âŽ¨
âŽ©
ln

1 âˆ’

x2âˆ’c2j
d2j

a2,2j
a2
2,2j
+

x2âˆ’c2j
d2j

a2,2j ln

x2âˆ’c2j
d2j

a2,2j(1 âˆ’T2,2j)
âŽ«
âŽ¬
âŽ­
âˆ’1
H(x2, c2j, d2j)
(7.141)
Ë™a1,2j = Î³
âŽ§
âŽ¨
âŽ©
ln

1 âˆ’

x2âˆ’c2j
d2j

a1,2j
a2
1,2j
+

x2âˆ’c2j
d2j

a1,2j ln

x2âˆ’c2j
d2j

a1,2j(1 âˆ’T1,2j)
âŽ«
âŽ¬
âŽ­
âˆ’1
H(x2, c2j, d2j)
(7.142)
Ë™c1i = âˆ’Î³ |d1i|a2,1i(1 âˆ’T2,1i)sgn(x1 âˆ’c1i)
|x1 âˆ’c1i|a2,1iâˆ’1
(7.143)
Ë™d1i = Î³ (1 âˆ’T2,1i)|d1i|a2,1i+1
|x1 âˆ’c1i|a2,1i
sgn(d1i)
(7.144)
Ë™c2j = âˆ’Î³ |d2j|a2,2j(1 âˆ’T2,2j)sgn(x2 âˆ’c2j)
|x2 âˆ’c2j|a2,2jâˆ’1
(7.145)
Ë™d2j = Î³ (1 âˆ’T2,2j)|d1i|a2,2j+1
|x2 âˆ’c2j|a2,2j
sgn(d2j)
(7.146)
Ë™fij = âˆ’
q
W ij + (1 âˆ’q)
W ij
T
ij ij
Î±sign(uc)
(7.147)
in which:
ij =
âŽ›
âŽ
I

i=1
J
j=1
(q
W ij + (1 âˆ’q)
W ij)
âŽž
âŽ 
(7.148)
Ë™Î± = 2Î³1|uc|
(7.149)
In the above, Î³1 and Î³ are the learning rates considered for the adaptation
of the learning rate and they need to be selected as positive.

Sliding Mode Control Theory-Based Parameter Adaptation Rules for Fuzzy Neural Networks
119
Remark 7.5. As can be seen from (7.149), an adaptive learning rate is
considered for updating the parameters of the consequent parts. Thus, a
priori knowledge about the upper bound of the states of the system is not
needed.
If the system under control is a second-order dynamic system in the
form of:
Ë™x1 = x2
Ë™x2 = f (x1, x2) + u
(7.150)
then it is possible to extend the results of Theorem 7.1 and prove the stability
of the system without any assumptions on any bounds on the states of the
system. The following theorem summarizes these results.
Theorem 7.5. Consider the control structure as shown in Fig. 7.1, which
consists of a PD controller working in parallel with a T2FNN and a robustness
term (i.e., u = Kpe + KdË™e + ur âˆ’uf ) and ur = Krsign(Ë™e) + Â¨xd in which
Ë™Kr = Î³ |Ë™e|, 0 < Î³ . In addition, the plant under control is a second-order dynamic
system in the form of (7.150). It is assumed that the function f (x1, x2) is bounded
in a compact set (i.e., f (x1, x2) < Bf ) and the output of the T2FNN (uf ) has an
upper bound if the parameter update rules of the T2FNN are chosen as (7.139)-
(7.149) with the selection of adaptive learning rate (Î±) as in (7.149), then given
arbitrary initial conditions uc(0), uc(t) converges asymptotically to zero in a finite
time and sliding motion will be achieved.
The stability conditions in Theorem 7.5 do not need the PD controller
to ensure the stability of the control loop and the boundedness of the states
of the system any longer. This will greatly relax the conditions required by
Theorem 7.4 for the stability of the system. However, note that this theorem
is only valid for second-order nonlinear dynamic systems, which is a wide
class of nonlinear systems.
7.3.3.1 Proof of Theorem 7.4
As mentioned earlier, the elliptic type-2 MFs are used so that:
Î¼1i =

1 âˆ’

x1 âˆ’c1i
d1i

a2,1i1/a2,1i
H(x1, c1i, d1i)
(7.151)
in which Î¼1i is the lower bound of the ith MF considered for the error and
H(x1, c1i, d1i) is the rectangular function defined as:

120
Fuzzy Neural Networks for Real Time Control Applications
H(x1, c, d) =
 1
c âˆ’d < x1 < c + d
0
otherwise
If c1i âˆ’d1i < x1 < c1i + d1i, then the following equation is valid:
ln(Î¼1i) =
1
a2,1i
ln

1 âˆ’

x1 âˆ’c1i
d1i

a2,1i
(7.152)
Define T2,1i =
x1âˆ’c1i
d1i

a2,1i so that:
Ë™T2,1i = Ë™a2,1i

x1 âˆ’c1i
d1i

a2,1i
ln

x1 âˆ’c1i
d1i
 + a2,1i
 Ë™x1 âˆ’Ë™c1i
|d1i|
sgn(x1 âˆ’c1i)
âˆ’Ë™d1i
|x1 âˆ’c1i|
d2
1i
sgn(d1i)
 
x1 âˆ’c1i
d1i

a2,1iâˆ’1
(7.153)
and the time derivative of Î¼1i is achieved as:
Ë™Î¼1i =
âˆ’Î¼1iË™a2,1i
a2
2,1i
ln

1 âˆ’

x1 âˆ’c1i
d1i

a2,1i
âˆ’
Î¼1i
a2,1i
Ë™T2,1i
1 âˆ’T2,1i
(7.154)
The upper bound of the ith type-2 MFs for e is defined as:
Î¼1i =

1 âˆ’

x1 âˆ’c1i
d1i

a1,1i1/a1,1i
H(x1, c1i, d1i)
(7.155)
Define T1,1i =
x1âˆ’c1i
d1i

a1,1i so that:
Ë™T1,1i = Ë™a1,1i

x1 âˆ’c1i
d1i

a1,1i
ln

x1 âˆ’c1i
d1i
 + a1,1i
 Ë™x1 âˆ’Ë™c1i
|d1i|
sgn(x1 âˆ’c1i)
âˆ’Ë™d1i
|x1 âˆ’c1i|
d2
1i
sgn(d1i)
 
x1 âˆ’c1i
d1i

a1,1iâˆ’1
(7.156)
and further:
Ë™Î¼1i = âˆ’Î¼1iË™a1,1i
a2
1,1i
ln

1 âˆ’

x1 âˆ’c1i
d1i

a1,1i
âˆ’Î¼1i
a1,1i
Ë™T1,1i
1 âˆ’T1,1i
(7.157)

Sliding Mode Control Theory-Based Parameter Adaptation Rules for Fuzzy Neural Networks
121
The lower bound of the jth type-2 MFs for x2 = Ë™e is defined as:
Î¼2j =

1 âˆ’

x2 âˆ’c2j
d2j

a2,2j1/a2,2j
H(x2, c2j âˆ’d2j, c2j + d2j)
(7.158)
Define T2,2j =

x2âˆ’c2j
d2j

a2,2j so that:
Ë™T2,2j = Ë™a2,2j

x2 âˆ’c2j
d2j

a2,2j
ln

x2j âˆ’c2j
d2j
 + a2,2j
 Ë™x2 âˆ’Ë™c2j
|d2j|
sgn(x2 âˆ’c2j)
âˆ’Ë™d2j
|x2 âˆ’c2j|
d2
2j
sgn(d2j)
 
x2 âˆ’c2j
d2j

a2,2jâˆ’1
(7.159)
and further:
Ë™Î¼2j =
âˆ’Î¼2jË™a2,2j
a2
2,2j
ln

1 âˆ’

x2 âˆ’c2j
d2j

a2,2j
âˆ’
Î¼2j
a2,2j
Ë™T2,2j
1 âˆ’T2,2j
(7.160)
The upper bound of the jth type-2 MFs for x2 is defined as:
Î¼2j =

1 âˆ’

x2 âˆ’c2j
d2j

a1,2j1/a1,2j
H(x2, c2j, d2j)
(7.161)
Define T1,2j =

x2âˆ’c2j
d2j

a1,2j so that:
Ë™T1,2j = Ë™a1,2j

x2 âˆ’c2j
d2j

a1,2j
ln

x2j âˆ’c2j
d2j
 + a1,2j
 Ë™x2 âˆ’Ë™c2j
|d2j|
sgn(x2 âˆ’c2j)
âˆ’Ë™d2j
|x2 âˆ’c2j|
d2
2j
sgn(d2j)
 
x2 âˆ’c2j
d2j

a1,2jâˆ’1
(7.162)
and further:
Ë™Î¼2j =
âˆ’Î¼2jË™a1,2j
a2
1,2j
ln

1 âˆ’

x2 âˆ’c2j
d2j

a1,2j
âˆ’
Î¼2j
a1,2j
Ë™T1,2j
1 âˆ’T1,2j
(7.163)

122
Fuzzy Neural Networks for Real Time Control Applications
We have:
Î¼1i
1 âˆ’T1,1i

x1 âˆ’c1i
d1i

a1,1iâˆ’1
=
Î¼1i âˆ’Î¼1i + Î¼1i
1 âˆ’T2,1i + T2,1i âˆ’T1,1i

x1 âˆ’c1i
d1i

a2,1iâˆ’1
+

x1 âˆ’c1i
d1i

a1,1iâˆ’1
âˆ’

x1 âˆ’c1i
d1i

a2,1iâˆ’1
=
Î¼1i
1âˆ’T2,1i

x1 âˆ’c1i
d1i

a2,1iâˆ’1 
1+
Î¼1iâˆ’Î¼1i
Î¼1i

Ã—
âŽ›
âŽ1+

x1âˆ’c1i
d1i

a1,1iâˆ’1
âˆ’

x1 âˆ’c1i
d1i

a2,1iâˆ’1
Ã—

x1 âˆ’c1i
d1i

a2,1iâˆ’1âˆ’1 âŽž
âŽ 
âŽ›
âŽ
1
1 âˆ’T1,1iâˆ’T2,1i
1âˆ’T2,1i
âŽž
âŽ 
(7.164)
Considering the fact that the MFs have nonzero value only if c1i âˆ’d1i <
x1 < c1i + d1i we have:
0 < T1i < 1
T2i < T1i
(7.165)
so that:
0 < T1i âˆ’T2i < 1 âˆ’T2i â‡’0 < T1i âˆ’T2i
1 âˆ’T2i
< 1
(7.166)
This means it is possible to use the McLauren series as:
Î¼1i
1 âˆ’T1,1i

x1 âˆ’c1i
d1i

a1,1iâˆ’1
=
Î¼1i
(1 âˆ’T2,1i)

x1 âˆ’c1i
d1i

a2,1iâˆ’1 
1 +
Î¼1i âˆ’Î¼1i
Î¼1i

Ã—

1 + T1,1i âˆ’T2,1i
1 âˆ’T2,1i
+
T1,1i âˆ’T2,1i
1 âˆ’T2,1i
2
+ H.O.T.

Ã—

Sliding Mode Control Theory-Based Parameter Adaptation Rules for Fuzzy Neural Networks
123
âŽ›
âŽ1 +

x1 âˆ’c1i
d1i

a1,1iâˆ’1
âˆ’

x1 âˆ’c1i
d1i

a2,1iâˆ’1
Ã—

x1 âˆ’c1i
d1i

a2,1iâˆ’1âˆ’1 âŽž
âŽ 
(7.167)
It should also be noted that:
Î¼1i
(1 âˆ’T1,1i)

x1 âˆ’c1i
d1i

a1,1iâˆ’1
=
Î¼1i
(1 âˆ’T2,1i)

x1 âˆ’c1i
d1i

a2,1iâˆ’1
(1 + 1i)
=
Î¼1i
(1 âˆ’T2,1i)

x1 âˆ’c1i
d1i

a2,1iâˆ’1
+ 
â€²
1i (7.168)
in which:

â€²
1i =
Î¼1i
(1 âˆ’T2,1i)

x1 âˆ’c1i
d1i

a2,1iâˆ’1
1i
(7.169)
and:
1i
=

1 +
Î¼1i âˆ’Î¼1i
Î¼1i

Ã—

1 + T1,1i âˆ’T2,1i
1 âˆ’T2,1i
+
T1,1i âˆ’T2,1i
1 âˆ’T2,1i
2
+ H.O.T.

Ã—
âŽ›
âŽ1 +

x1 âˆ’c1i
d1i

a1,1iâˆ’1
âˆ’

x1 âˆ’c1i
d1i

a2,1iâˆ’1
Ã—

x1 âˆ’c1i
d1i

a2,1iâˆ’1âˆ’1 âŽž
âŽ âˆ’1
(7.170)
Furthermore, the following equation should be taken into the account:
Î¼2j
(1 âˆ’T1,2j)

x1 âˆ’c2j
d2j

a1,2jâˆ’1
=
Î¼2j
(1 âˆ’T2,2j)

x1 âˆ’c2j
d2j

a2,2jâˆ’1
(1 + 2j)
=
Î¼2j
(1 âˆ’T2,2j)

x1 âˆ’c2j
d2j

a2,2jâˆ’1
+ 
â€²
2j
(7.171)

124
Fuzzy Neural Networks for Real Time Control Applications
in which:
2j
=

1 +
Î¼2j âˆ’Î¼2j
Î¼2j

Ã—

1 + T1,2j âˆ’T2,2j
1 âˆ’T2,2j
+
T1,2j âˆ’T2,2j
1 âˆ’T2,2j
2
+ H.O.T.

Ã—
âŽ›
âŽ1 +

x1 âˆ’c2j
d2j

a1,2jâˆ’1
âˆ’

x1 âˆ’c2j
d2j

a2,2jâˆ’1
Ã—

x1 âˆ’c2j
d2j

a2,2jâˆ’1âˆ’1 âŽž
âŽ âˆ’1
(7.172)

â€²
2j =
Î¼2j
(1 âˆ’T2,2j)

x1 âˆ’c2j
d2j

a2,2jâˆ’1
2j
(7.173)
The following adaptation laws are considered:
Ë™a2,1i = Î³
âŽ§
âŽ¨
âŽ©
ln

1 âˆ’
x1âˆ’c1i
d1i

a2,1i
a2
2,1i
+
x1âˆ’c1i
d1i

a2,1i ln
x1âˆ’c1i
d1i

a2,1i(1 âˆ’T2,1i)
âŽ«
âŽ¬
âŽ­
âˆ’1
H(x1, c1i, d1i)
(7.174)
Ë™a1,1i = Î³
âŽ§
âŽ¨
âŽ©
ln

1 âˆ’
x1âˆ’c1i
d1i

a1,1i
a2
1,1i
+
x1âˆ’c1i
d1i

a1,1i ln
x1âˆ’c1i
d1i

a1,1i(1 âˆ’T1,1i)
âŽ«
âŽ¬
âŽ­
âˆ’1
H(x1, c1i, d1i)
(7.175)
Ë™a2,2j = Î³
âŽ§
âŽ¨
âŽ©
ln

1 âˆ’

x2âˆ’c2j
d2j

a2,2j
a2
2,2j
+

x2âˆ’c2j
d2j

a2,2j ln

x2âˆ’c2j
d2j

a2,2j(1 âˆ’T2,2j)
âŽ«
âŽ¬
âŽ­
âˆ’1
H(x2, c2j, d2j)
(7.176)

Sliding Mode Control Theory-Based Parameter Adaptation Rules for Fuzzy Neural Networks
125
Ë™a1,2j = Î³
âŽ§
âŽ¨
âŽ©
ln

1 âˆ’

x2âˆ’c2j
d2j

a1,2j
a2
1,2j
+

x2âˆ’c2j
d2j

a1,2j ln

x2âˆ’c2j
d2j

a1,2j(1 âˆ’T1,2j)
âŽ«
âŽ¬
âŽ­
âˆ’1
H(x2, c2j, d2j)
(7.177)
Ë™c1i = âˆ’Î³ |d1i|a2,1i(1 âˆ’T2,1i)sgn(x1 âˆ’c1i)
|x1 âˆ’c1i|a2,1iâˆ’1
(7.178)
Ë™d1i = Î³ (1 âˆ’T2,1i)|d1i|a2,1i+1
|x1 âˆ’c1i|a2,1i
sgn(d1i)
(7.179)
Ë™c2j = âˆ’Î³ |d2j|a2,2j(1 âˆ’T2,2j)sgn(x2 âˆ’c2j)
|x2 âˆ’c2j|a2,2jâˆ’1
(7.180)
Ë™d2j = Î³ (1 âˆ’T2,2j)|d1i|a2,2j+1
|x2 âˆ’c2j|a2,2j
sgn(d2j)
(7.181)
Considering these adaptation laws we have:
Ë™Î¼1i = âˆ’Î³ Î¼1i âˆ’
Î¼1i
1 âˆ’T2,1i

x1 âˆ’c1i
d1i
 Ë™x1sgn(x1 âˆ’c1i)
(7.182)
Ë™Î¼2j = âˆ’Î³ Î¼2j âˆ’
Î¼2j
1 âˆ’T2,2j

x2 âˆ’c2j
d2j
 Ë™x2sgn(x1 âˆ’c2j)
(7.183)
Ë™Î¼1i = âˆ’Î³ Î¼1i + 	1i
(7.184)
where 	1i is defined as:
	1i = Î³ Î¼1i âˆ’Î³ Î¼1i âˆ’
Î¼1i
1 âˆ’T1,1i

x1 âˆ’c1i
d1i
 Ë™x1sgn(x1 âˆ’c1i)
(7.185)
and further:
Ë™Î¼2j = âˆ’Î³ Î¼2j âˆ’
Î¼2j
1 âˆ’T1,2j

x2 âˆ’c2j
d2j
 Ë™x2sgn(x2 âˆ’c2j)
(7.186)

126
Fuzzy Neural Networks for Real Time Control Applications
so that:
Ë™Î¼2j = âˆ’Î³ Î¼2j + 	2j
(7.187)
in which 	2j is defined as follows:
	2j = Î³ Î¼2j âˆ’Î³ Î¼2j âˆ’
Î¼2j
1 âˆ’T1,2j

x2 âˆ’c2j
d2j
 Ë™x2sgn(x2 âˆ’c2j)
(7.188)
In addition, by definition of 
W ij and 
W ij as:

W ij =
W ij

I
i=1

J
j=1 W ij

W ij =
W ij

I
i=1

J
j=1 W ij
the time derivatives of 
W ij and 
W ij are obtained as:

W ij =
W ij

I
i=1

J
j=1 W ij
â‡’
Ë™
W ij =

Î¼1i(e)Î¼2j(Ë™e)
â€² 
I
i=1

J
j=1 W ij

 
I
i=1

J
j=1 W ij
2
âˆ’
W ij
 
I
i=1

J
j=1 Î¼1i(e)Î¼2j(Ë™e)
â€²
 
I
i=1

J
j=1 W ij
2
(7.189)

W ij =
W ij

I
i=1

J
j=1 W ij
â‡’
Ë™
W ij =
Î¼1i(e)Î¼2j(Ë™e)â€² 
I
i=1

J
j=1 W ij

 
I
i=1

J
j=1 W ij
2
âˆ’
W ij
 
I
i=1

J
j=1 Î¼1i(e)Î¼2j(Ë™e)
â€²
 
I
i=1

J
j=1 W ij
2
(7.190)

Sliding Mode Control Theory-Based Parameter Adaptation Rules for Fuzzy Neural Networks
127
Since ËœW ij = (W ij)/(
I
i=1

J
j=1 W ij) and ËœW ij = (W ij)/(
I
i=1

J
j=1
W ij) we have:
Ë™
W ij =
Ë™Î¼1i(e)Î¼2j(Ë™e) + Î¼1i(e) Ë™Î¼2j(Ë™e)

I
i=1

J
j=1 W ij
âˆ’

W ij
 
I
i=1

J
j=1
 Ë™Î¼1i(e)Î¼2j(Ë™e) + Î¼1i(e) Ë™Î¼2j(Ë™e)

I
i=1

J
j=1 W ij
Ë™
W ij =
Ë™Î¼1i(e)Î¼2j(Ë™e) + Î¼1i(e) Ë™Î¼2j(Ë™e)

I
i=1

J
j=1 W ij
âˆ’

W ij
 
I
i=1

J
j=1
 Ë™Î¼1i(e)Î¼2j(Ë™e) + Î¼1i(e) Ë™Î¼2j(Ë™e)


I
i=1

J
j=1 W ij
The following Lyapunov function is considered to analyze the stability
of the system:
Vc = 1
2u2
c (t) + 1
2Î³1
(Î± âˆ’Î±âˆ—)2
(7.191)
The time derivative of the Lyapunov function (7.191) is derived as:
Ë™Vc = ucË™uc = uc(Ë™uf + Ë™u) + Ë™Î±
Î³1
(Î± âˆ’Î±âˆ—)
(7.192)
It should be noted that we have:
Ë™uf = q
I

i=1
J
j=1
(Ë™fij ËœW ij + fij Ë™ËœWij) + (1 âˆ’q)
I

i=1
J
j=1
(Ë™fij
W ij + fij Ë™

W ij) (7.193)
So the time derivative of the Lyapunov function can be further manipu-
lated as:
Ë™Vc â‰¤uc(q
I

i=1
J
j=1
Ë™fij ËœW ij + (1 âˆ’q)
I
i=1
J
j=1
Ë™fij
W ij)

128
Fuzzy Neural Networks for Real Time Control Applications
+ |uc|(|Bu| + Bf BË™x
Bx + Bc
Bd,min
+ Bf ) + Ë™Î±
Î³1
(Î± âˆ’Î±âˆ—)
(7.194)
in which:
|fij| < Bf , |x1| < Bx, |x2| < Bx,
|Ë™x1| < BË™x, |Ë™x2| < BË™x,
Bd,min < |d1i|, Bd,min < |d2j|, |c1i| < Bc,
|c2j| < Bc, |u| < Bu,
(7.195)
In addition, the following adaptation law for fij is considered:
Ë™fij = âˆ’
q
W ij + (1 âˆ’q)
W ij
T
ij ij
Î±sign(uc)
(7.196)
in which:
ij =
âŽ›
âŽ
I
i=1
J
j=1

q
W ij + (1 âˆ’q)
W ij

âŽž
âŽ 
(7.197)
So that we have the following equation:
Ë™Vc â‰¤âˆ’Î±|uc| + B|uc| + Ë™Î±
Î³1
(Î± âˆ’Î±âˆ—)
(7.198)
in which:
B = |Bu| + Bf BË™x
Bx + Bc
Bd,min
+ Bf
(7.199)
It is considered that Î±âˆ—is as large as:
B < Î±âˆ—
2
(7.200)
and the adaptation law for the learning rate Î± is considered to be:
Ë™Î± = 2Î³1|uc|
(7.201)

Sliding Mode Control Theory-Based Parameter Adaptation Rules for Fuzzy Neural Networks
129
so that:
Ë™Vc â‰¤âˆ’Î±
2 |uc|
(7.202)
This means uc converges asymptotically to zero and this ends the proof.
7.3.3.2 Proof of Theorem 7.5
Consider a second-order system as:
Ë™x1 = x2
Ë™x2 = f (x1, x2) + u
(7.203)
As noted earlier e and Ë™e are defined as:
e = xd âˆ’x1, Ë™e = Ë™xd âˆ’Ë™x1
(7.204)
so we get the following equation:
Â¨e = Â¨xd âˆ’f (x1, x2) âˆ’u
(7.205)
Let the candidate Lyapunov function be in the following form:
V = 1
2Kpe2 + 1
2Ë™e2 + 1
2Î³ (Kr âˆ’Kâˆ—
r )2, 0 < Î³
(7.206)
The time derivative of this Lyapunov function is calculated as:
Ë™V = KpeË™e + Ë™e(Â¨xd âˆ’f (x1, x2) âˆ’u) + 1
Î³
Ë™Kr(Kr âˆ’Kâˆ—
r )
(7.207)
The control signal consists of three parts: a classical controller, which
is here PD, the output of T2FLS (uf ), and the term to guarantee the
robustness (ur):
u = Kpe + KdË™e + ur âˆ’uf
(7.208)
so that:
Ë™V = âˆ’KdË™e2 + Ë™e(Â¨xd âˆ’f (x1, x2) âˆ’ur âˆ’uf ) + 1
Î³
Ë™Kr(Kr âˆ’Kâˆ—
r )
(7.209)

130
Fuzzy Neural Networks for Real Time Control Applications
Here, ur is added for robust stability and has the following form:
ur = Krsign(Ë™e) + Â¨xd
(7.210)
It is considered that f (x1, x2) is bounded in a compact set as |f (x1,
x2)| < Bf :
Ë™V < âˆ’KdË™e2 + |Ë™e|(Bf + Buf ) + Â¨xdË™e âˆ’urË™e + 1
Î³
Ë™Kr(Kr âˆ’Kâˆ—
r )
(7.211)
Considering (7.210) we have:
Ë™V < âˆ’KdË™e2 âˆ’Kr|Ë™e| + Kâˆ—
r |Ë™e| + 1
Î³
Ë™Kr(Kr âˆ’Kâˆ—
r )
(7.212)
It is further considered that Kâˆ—
r is the final and unknown value of Kr,
which is defined as:
Bf + Buf < Kâˆ—
r
(7.213)
The adaptation law for Kr is taken as follows:
Ë™Kr = Î³ |Ë™e|
(7.214)
In this way, we have the following equation for the time derivative of
the Lyapunov function:
Ë™V < âˆ’KdË™e2
(7.215)
This concludes the stability analysis and means the error signal will
converge to zero.
7.4 CONCLUSION
In this chapter, some novel parameter adaptation laws for T2FNN are
derived for both identification and control purposes. These adaptation laws
benefit from SMC theory. The use of such a nonlinear control approach
makes it possible to benefit from its well-established mathematical stability
analysis. In the identification part, a novel fully sliding mode parameter

Sliding Mode Control Theory-Based Parameter Adaptation Rules for Fuzzy Neural Networks
131
update rules are used for the training of an interval T2FNN. The stability
analysis of the training is done by using an appropriate Lyapunov function.
In the controller design part, in the case when the system is of nth order,
the stability of the training of the T2FNN is considered. Furthermore, the
stability of the system (not only the stability of the learning but also the
overall stability of the system), in the case of having a second-order system,
is shown.
REFERENCES
[1] E. Kayacan, O. Cigdem, O. Kaynak, Sliding mode control approach for online learning
as applied to type-2 fuzzy neural networks and its experimental evaluation, IEEE Trans.
Indust. Electron. 59 (9) (2012) 3510-3520.
[2] O. Kaynak, K. Erbatur, M. Ertugnrl, The fusion of computationally intelligent method-
ologies and sliding-mode controlâ€”a survey, IEEE Trans. Indust. Electron. 48 (1) (2001)
4-17.
[3] X. Yu, O. Kaynak, Sliding-mode control with soft computing: a survey, IEEE Trans.
Indust. Electron. 56 (9) (2009) 3275-3285.
[4] S. Masumpoor, H. yaghobi, M.A. Khanesar, Adaptive sliding-mode type-2 neuro-fuzzy
control of an induction motor, Expert Syst. Appl. ISSN 0957-4174.
[5] M. Begian, W. Melek, J. Mendel, Parametric design of stable type-2 TSK fuzzy systems,
in: Fuzzy Information Processing Society, 2008. NAFIPS 2008. Annual Meeting of the
North American, 2008, pp. 1-6.
[6] E. Kayacan, M. Khanesar, Identification of nonlinear dynamic systems using type-2 fuzzy
neural networks a novel learning algorithm and a comparative study, IEEE Trans. Indust.
Electron. 62 (3) (2015) 1716-1724.
[7] J.-J. Slotine, W. Li, Applied Nonlinear Control, Prentice Hall, Upper Saddle River, NJ,
1991.
[8] M. Biglarbegian, W. Melek, J. Mendel, On the stability of interval type-2 tsk fuzzy logic
control systems, IEEE Trans. Syst. Man Cybernet. B Cybernet. 40 (3) (2010) 798-818.
[9] V. Utkin, Sliding Modes in Control Optimization, Springer-Verlag, New York, 1992.

CHAPTER 8
Hybrid Training Method for
Type-2 Fuzzy Neural Networks
Using Particle Swarm
Optimization
Contents
8.1
Introduction
134
8.1.1 Fully PSO Training Algorithms
135
8.1.2 Hybrid PSO with Computation-based Training Methods
136
8.2
Continuous Version of Particle Swarm Optimization
136
8.3
Analysis of Continuous Version of Particle Swarm Optimization
137
8.3.1 Stability Analysis
137
8.3.2 Dynamic Behavior
138
8.3.3 Higher-Order Continuous PSO
138
8.4
Proposed Hybrid Training Algorithm for Type-2 Fuzzy Neural Network
142
8.4.1 Proposed Hybrid Training Algorithm for T2FNN with Type-2 MFs with
Uncertain Ïƒ
142
8.4.1.1 T2FNN with Type-2 MFs with Uncertain Ïƒ
142
8.4.1.2 Proposed Hybrid Training Algorithm for T2FNN by Using Type-2 MFs
with Uncertain Ïƒ
144
8.4.1.3 Stability Analysis
145
8.4.2 Proposed Hybrid Training Algorithm for T2FNN With Type-2 MFs with
Uncertain c
150
8.4.2.1 T2FNN with Type-2 MFs with Uncertain c
150
8.4.2.2 Proposed Hybrid Training Algorithm for T2FNN with MFs with
Uncertain Center Value
152
8.4.2.3 Stability Analysis
154
8.4.3 Further Discussion About the Hybrid Training Method
158
8.5
Conclusion
159
References
159
Abstract
Even if we have highly capable computers today, nature still has the best engineering
designs. For example, it is not coincidence that the nose of an airplane is very similar
to that of a dolphin. Not only the physical appearance but also the behavior of animals
Fuzzy Neural Networks for Real Time Control Applications
Copyright Â© 2016 Elsevier Inc.
http://dx.doi.org/10.1016/B978-0-12-802687-8.00008-6
All rights reserved.
133

134
Fuzzy Neural Networks for Real Time Control Applications
in nature are opening the doors for new theories. This chapter shows that nature is still
helping humans make the most efficient and brilliant engineering designs.
In this chapter a novel hybrid training method based on continuous version of PSO
and SMC-based training method for T2FNNs is proposed. The proposed approach uses
PSO for the training of the antecedent parts of T2FNNs, which appear nonlinearly in
the output of T2FNNs and SMC-based training methods for the training of parameters
of their consequent parts. The use of PSO makes it possible to lessen the probability
of entrapment of the parameters in local minima while proposing simple adaptation
laws for the parameters of the antecedent parts of T2FNN as compared to popular
approaches like GD, LM and extended KF. Moreover, the SMC theory-based rules for
the parameters of the consequent part are simple, have closed-form and benefit from a
rigorous mathematics. In addition, using SMC theory-based, the responsible parameter
for sharing the contributions of the lower and upper parts of the type-2 fuzzy MFs is
tuned as well. The stability of the proposed hybrid training method is proved by using
an appropriate Lyapunov function.
Keywords
Hybrid training, Particle swarm optimization, Type-2 fuzzy neural networks, Levenberge-
Marquardt
8.1 INTRODUCTION
Even if we have highly capable computers today, nature still has the best
engineering designs. For example, it is not coincidence that the nose of an
airplane is very similar to that of a dolphin. Not only the physical appearance
but also the behavior of animals in nature are opening the doors for new
theories. This chapter shows that nature is still helping humans make the
most efficient and brilliant engineering designs.
The PSO is a population-based search algorithm. This optimization
algorithm is based on the observations made from the flight of birds within
a flock, school of fish, and so on. Each individual bird in a flock flies in-
dependently and unpredictably, but in such a way that each stays connected
and forms a social group. What can be observed from birdsâ€™ behavior when
flying or fish when swimming is that they have tendency to move to the
best position for their neighbors. They even communicate with each other
in order to obtain the best position and continue flying synchronously.
PSO has become one of the most popular and most preferable opti-
mization algorithms. Since it searches a large space of possible solutions,
its applications to most real-world applications have been successful, and it
has outperformed other meta-heuristic optimization algorithms considering
the precision of the solutions and speed of convergence.

Fuzzy Neural Networks Training Using Particle Swarm Optimization
135
The simplified version of this concept is formulated with a two-state
dynamic system. The two state dynamic system is comprised of the position
as the first state and velocity as the second state of the system. It is then
assumed that each particle has a multidimensional position vector. The num-
ber of dimensions considered for each particle is the same as the dimensions
of the optimization problem. The optimality is also defined based on the
objective function to be optimized. PSO requires no assumptions, or just a
few assumptions, about the objective function to be optimized. The change
in the position of a particle is defined by its corresponding velocity vector,
which determines the moving direction of the particle.
The moving direction has three different terms that can be put into two
categories: exploration and exploitation terms. The exploration is basically
composed of a momentum term that tries to maintain the latest moving
direction of the particle which is a random direction and makes it possible to
visit more points. The exploitation terms are the terms that direct a particle
toward its own best experience visited and toward the best position found by
all particles in the swarm. Since the best position found in the first epochs are
far from the true optimum of the objective function, the gain of exploration
term is higher than that of exploitation to visit more random positions at
first few epochs. However, as more points are visited by the swarm the best
experience of each individual and the best experience of the whole swarm
are more likely to be meaningful. Hence, at larger values of epochs the
exploitation terms become dominant.
The original version of PSO was discrete time. However, its continuous
version has also been studied in number of papers, making it easier to
analyze mathematically. Furthermore, the stability analysis of its discrete
and continuous version has been studied. It has been shown that in
order to have stable behavior for each particle toward best position found
by individuals and the best position found by all members of a swarm,
certain constrains must be considered in the selection of the parameters
of PSO.
One of the applications that PSO could be successfully used for is the
estimation of the parameters of FNNs in which PSO is used alone or in
combination with other estimation algorithms.
8.1.1 Fully PSO Training Algorithms
In these methods PSO is used to estimate all the parameters of FNNs. They
are easy to implement because of the simple nature of the PSO method and
do not necessitate the gradient calculation of FNN output with respect to its

136
Fuzzy Neural Networks for Real Time Control Applications
parameters as is common with most computation-based training algorithms.
Moreover, since PSO proposes multiple evolving individuals as the solution
to the problem, the solutions entrap less in local minima. However, multiple
solutions require more memory to be used. Moreover, they require multiple
feed-forward calculation of FNN, which is time consuming. Examples of
PSO-based methods for training FNNs can be found in Refs [1â€“4].
8.1.2 Hybrid PSO with Computation-based Training Methods
These methods benefit from the advantages of the above mentioned
algorithm. In these approaches, the population-based algorithms are used for
the estimation of the antecedent part parameters because they are gradient-
free. In other words, since the parameters of the antecedent part appear
nonlinearly in the output of FNN, the calculation of the gradient of the cost
function with respect to them is difficult and computation-based methods
may even fail to optimize these parameters globally. However, since the
parameters of the consequent part appear linearly in the output of a FNN,
the computation-based method can be easily applied to them. The use
of these methods for the consequent part parameters makes it possible to
benefit from the mathematical background of these methods. Moreover,
the use of computational methods for the estimation of the parameters
of the consequent part instead of using population-based methods highly
reduces the dimension of individuals. Examples of hybrid training methods
using PSO with other computation-based approaches that are applied to the
training of FNNs can be found in Refs [5â€“7].
8.2 CONTINUOUS VERSION OF PARTICLE SWARM
OPTIMIZATION
In this section, the continuous version of PSO is briefly discussed, and its
mathematical formulation is investigated. The dynamics of each dimension
of particles in this optimization algorithm are independent of each other.
The position of the lth dimension of the sth particle is denoted by xsl and its
velocity is depicted using Î½sl. Therefore, the dynamics of the lth dimension
of the sth particle are formulated as follows [8]:
Ë™xsl(t) = Î½sl(t)
Ë™Î½sl(t) = (Î² âˆ’1)Î½sl(t) + Î³1 (Plsl(t) âˆ’xsl) + Î³2

Pgsl(t) âˆ’xsl

xsl(0) = xsl0, Î½sl(0) = Î½sl0, l = 1, . . . , n, s = 1, . . . , N
(8.1)

Fuzzy Neural Networks Training Using Particle Swarm Optimization
137
in which Plsl(t) is the lth dimension of the best particle achieved for
sth particle and Pgsl(t) is the lth dimension of the best particle achieved
for all particles. Furthermore, Pls(t)
=
[Pls1, Pls2, . . . , Plsn], Pg(t)
=
[Pg1, Pg2, . . . , Pgn], and n is the number of dimensions. Pls(t) and Pg(t) are
updated as follows:
Pls(t) = arg min{ f (Pls(tâˆ’)), f (xs(t))}
Pg(t) = arg min{ f (Pg(tâˆ’)), f (x1(t)), . . . , f (xN(t))}
in which N is the number of individuals in the swarm, f (.) is the cost
function to be minimized. If Psl = Î³1Plsl(t)+Î³2Pgsl(t) and Î± = Î³1+Î³2, the
mathematical description of continuous time PSO is modified as follows:
Ë™xsl(t) = Î½sl(t)
Ë™Î½sl(t) = (Î² âˆ’1)Î½sl(t) âˆ’Î±xsl + Î±Psl(t)
xsl(0) = xsl0, Î½sl(0) = Î½sl0, l = 1, . . . , n, s = 1, . . . , N
(8.2)
8.3 ANALYSIS OF CONTINUOUS VERSION OF PARTICLE
SWARM OPTIMIZATION
8.3.1 Stability Analysis
In order to analyze the stability of CPSO, the equation of motion of PSO
(8.2) can be reformulated as follows:
Â¨xsl(t) âˆ’(Î² âˆ’1)Ë™xsl(t) + Î±xsl = Î±Psl(t)
(8.3)
which is a second-order dynamic system whose input is Psl(t). Since Psl(t)
is selected from the best visited positions met by an individual member
of the swarm and by the whole swarm, it is finite if its corresponding
position vector xsl(t) is finite and stable. On the other hand, (8.3) shows
that the governing equations of the position vector xsl(t) is a second-order
dynamic system. Using the Routh-Hurwitz stability analysis, in order to
have a stable motion, the coefficients of xsl(t) and its derivatives must be
positive. Therefore, the stability conditions are as follows:
Î² < 1
Î± > 0
(8.4)

138
Fuzzy Neural Networks for Real Time Control Applications
8.3.2 Dynamic Behavior
Since the basic CPSO can be considered as a linear second-order system,
the damping ratio of this system is equal to 1âˆ’Î²
2âˆšÎ± and its natural frequency
is obtained as âˆšÎ±. If 0 < 1âˆ’Î²
2âˆšÎ± < 1, the motion of a single dimension of
a particle oscillates around Psl, and finally converges toward it in infinite
time. This motion behavior is called under-damped motion behavior.
However, if
1âˆ’Î²
2âˆšÎ±
> 1 the system converges slowly, but without any
oscillations toward Psl(t). This behavior is called over-damped. Figure 8.1
presents the motion behavior of one dimension of a single particle with
respect to time and its phase plane when it is drawn with respect to its
corresponding velocity. The Psl is taken as Psl = 1; in other words, the
reference signal for the lth dimension of the position of the particle xsl is
taken equal to 1. As can be seen from Fig. 8.1, different values for the
Î² and Î± parameters may result in different motion behavior in xsl. The
motion behavior may be without any oscillation but slow or over-damped
(Fig. 8.1(a), (b)), fast but oscillating (Fig. 8.1(c), (d)) or super-fast but with
high level of oscillations (Fig. 8.1(e), (f)). The two latter behaviors are called
under-damped. As expected, the continuous version of PSO acts differently
when different values for poles are used.
8.3.3 Higher-Order Continuous PSO
In order to increase the degrees of freedom of basic CPSO, HCPSO can be
used in which the dynamics of xls are modified as follows:
Ë™xsl = Î½sl
Ë™Î½sl = (Î² âˆ’1)Î½sl âˆ’Î±xsl + Î±P âˆ’Î±2Î½3
sl
(8.5)
Type the existence of the term Î½3
sl makes it possible for HCPSO to
cover a wider range of behaviors for a particle. Moreover, the existence
of this term makes it possible to have a wider selection of possibilities for
the parameters of PSO. For example, Fig. 8.2 shows the behavior of basic
CPSO and HCPSO when the value of Î² = 1.1. As can be seen from this
figure, although the response of CPSO is unstable with this selection for w,
no instability happens in HCPSO. Another benefit of using HCPSO over
CPSO is that it is possible to achieve a limit cycle that is not possible in a
linear system such as CPSO. In this way, HCPSO can search a wider area.

Fuzzy Neural Networks Training Using Particle Swarm Optimization
139
0
10
20
30
40
50
60
1
1.5
2
2.5
3
3.5
4
4.5
5
Time (s)
(a)
xls
(b)
1
1.5
2
2.5
3
3.5
4
4.5
5
âˆ’0.4
âˆ’0.35
âˆ’0.3
âˆ’0.25
âˆ’0.2
âˆ’0.15
âˆ’0.1
âˆ’0.05
0
xls
nls
Figure 8.1 (a) Motion behavior of xls when Î² = 0.1 and Î± = 0.1; (b) phase plane output
of Î½ls with respect to xls when Î² = 0.1 and Î± = 0.2;
(Continued)

140
Fuzzy Neural Networks for Real Time Control Applications
(c)
0
5
10
15
20
0
1
2
3
4
5
Time (s)
xls
(d)
0
1
2
3
4
5
âˆ’1.2
âˆ’1
âˆ’0.8
âˆ’0.6
âˆ’0.4
âˆ’0.2
0
0.2
xls
nls
Figure 8.1, contâ€™d (c) motion behavior of xls when Î² = 0.4 and Î± = 0.3; (d) phase plane
output of Î½ls with respect to xls when Î² = 0.4 and Î± = 0.3;

Fuzzy Neural Networks Training Using Particle Swarm Optimization
141
(e)
0
10
20
30
40
âˆ’2
âˆ’1
0
1
2
3
4
5
Time (s)
xls
(f)
âˆ’2
âˆ’1
0
1
2
3
4
5
âˆ’4
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
xls
nls
Figure 8.1, contâ€™d (e) motion behavior of xls when Î² = 0.8 and Î± = 0.8; (f) phase plane
output of Î½ls with respect to xls when Î² = 0.8 and Î± = 0.8.

142
Fuzzy Neural Networks for Real Time Control Applications
0
10
20
30
40
âˆ’30
âˆ’20
âˆ’10
0
10
20
30
Time (s)
xls
CPSO
HCPSO
Figure 8.2 Motion behavior of xsl for CPSO and HCPSO.
8.4 PROPOSED HYBRID TRAINING ALGORITHM FOR TYPE-2
FUZZY NEURAL NETWORK
8.4.1 Proposed Hybrid Training Algorithm for T2FNN with
Type-2 MFs with Uncertain Ïƒ
8.4.1.1 T2FNN with Type-2 MFs with Uncertain Ïƒ
The upper and lower type-2 fuzzy Gaussian MFs with an uncertain standard
deviation (Fig. 8.3) can be represented as follows:
Î¼ik(xi) = exp

âˆ’1
2
(xi âˆ’cik)2
Ïƒ 2
ik

(8.6)
Î¼ik(xi) = exp

âˆ’1
2
(xi âˆ’cik)2
Ïƒ 2
ik

(8.7)
where cik is the center value of the kth type-2 fuzzy set for the ith input. The
parameters Ïƒ ik and Ïƒ ik are standard deviations for the upper and lower MFs.
After computing the lower and upper membership degrees Î¼ and Î¼ for
each input, the firing strengths of the rules using the prod t-norm operator
are calculated as follows:
wr = Î¼ ËœA1(x1) âˆ—Î¼ ËœA2(x2) âˆ—Â· Â· Â· Î¼ ËœAI(xI)

Fuzzy Neural Networks Training Using Particle Swarm Optimization
143
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
c (center)
m(x)
Lower MF
Upper MF
Ïƒ lower
Ïƒ upper
Figure 8.3 Type-2 Gaussian fuzzy MF with uncertain center.
wr = Î¼ ËœA1(x1) âˆ—Î¼ ËœA2(x2) âˆ—Â· Â· Â· Î¼ ËœAI(xI)
(8.8)
The consequent part corresponding to each fuzzy rule is a linear
combination of the inputs x1, x2, . . . , xI. This linear function is called fr and
is defined as fr = br+I
i=1 arixi. The output of the network is approximated
as follows:
yN = q
R

r=1
fr Ëœwr + (1 âˆ’q)
R

r=1
frwr
(8.9)
where R is the number of the rules of T2FNN, and 
wr and 
wr are the
normalized values of the lower and upper output signals from the second
hidden layer of the network as follows:

wr =
wr
R
i=1 wr
and

wr =
wr
R
i=1 wr
(8.10)
The following vectors can be specified:
	
W (t) =


w1 (t) 
w2 (t) . . . 	
wR (t)
T, 	
W (t) =


w1 (t) 
w2 (t) . . . 	
wR (t)
T,
and F = [f1 f2 . . . fR]
The following assumptions have been used in this investigation: The
time derivative of both the input signals and target signal can be considered
bounded:

144
Fuzzy Neural Networks for Real Time Control Applications
|Ë™xi(t)| â‰¤BË™x, |xi(t)| â‰¤Bx, (i = 1 . . . I) and |Ë™y(t)| â‰¤BË™y
âˆ€t
(8.11)
where BË™x, Bx, and BË™y are assumed to be some known positive constants.
8.4.1.2 Proposed Hybrid Training Algorithm for T2FNN by Using Type-2
MFs with Uncertain Ïƒ
The proposed hybrid PSO and SMC training method is as follows:
Ë™Ïƒ sl = Î½Ïƒ sl
(8.12)
Ë™Î½Ïƒ sl = (Î² âˆ’1)Î½Ïƒ sl âˆ’Î±Ïƒ sl + Î³1PlÏƒ sl + Î³2PgÏƒ l âˆ’Î±2Î½3
Ïƒ sl, l = 1, . . . , n,
s = 1, . . . , N
Ë™Ïƒ sl = Î½Ïƒ sl
(8.13)
Ë™Î½Ïƒ sl = (Î² âˆ’1)Î½Ïƒ sl âˆ’Î±Ïƒ sl + Î³1PlÏƒsl + Î³2PgÏƒ sl âˆ’Î±2Î½3
Ïƒ sl, l = 1, . . . , n,
s = 1, . . . , N
Ë™csl = Î½csl
(8.14)
Ë™Î½csl = (Î² âˆ’1)Î½csl âˆ’Î±csl + Î³1Plcsl + Î³2Pgcl âˆ’Î±2Î½3
csl, l = 1, . . . , n,
s = 1, . . . , N
Ë™aris = âˆ’xi
qËœwr + (1 âˆ’q) Ëœwr

q Ëœwr + (1 âˆ’q) Ëœwr
T 
qËœwr + (1 âˆ’q) Ëœwr
Î±sgn (e) , s = 1, . . . , N,
r = 1, . . . , R
(8.15)
Ë™brs = âˆ’
qs Ëœwrs + (1 âˆ’qs) Ëœwrs
qs Ëœwrs + (1 âˆ’qs) Ëœwrs
T qs Ëœwrs + (1 âˆ’qs) Ëœwrs

Î±1es + Î±sgn (es)

s = 1, . . . , N, r = 1, . . . , R
(8.16)
Ë™qs = âˆ’
1
Fs(	
W s âˆ’	
W s)T Î±sgn (es) , s = 1, . . . , N
(8.17)

Fuzzy Neural Networks Training Using Particle Swarm Optimization
145
In (8.12)-(8.17), n is the number of the type-2 MFs, es is identification
error corresponding to the sth particle, subscript s refers to the sth particle,
and subscript l refers to lth dimension. For example, csl means the lth
dimension of the sth particle corresponding to c, the centers of MFs. The
cost function f (es(t)) is defined as:
f (es(t)) = 1
2

e2
s (t), s = 1, . . . , N
(8.18)
where the limits of the summation depend on the window size decided by
the programmer and the local best experiences of each particle PlÏƒ s, PlÏƒ s,
and Plcs are updated using this cost function (f (.)) as follows:
PlÏƒ s, PlÏƒ s, and Plcs = arg min{f (e(t)|PlÏƒ s(tâˆ’), PlÏƒ s(tâˆ’), Plcs(tâˆ’)),
(8.19)
f (e(t)|Ïƒ s(t), Ïƒ s(t), cs(t))}
PgÏƒ , PgÏƒ, and Pgc = arg min{f (e(t)|PgÏƒ(tâˆ’), PgÏƒ (tâˆ’), Pgc(tâˆ’)),
f (e(t)|Ïƒ s(t), Ïƒ s(t), cs(t))}
âˆ€s âˆˆN
(8.20)
Theorem 8.1. The adaptation laws of (8.12)-(8.17) guarantee that the
parameters of T2FNN in the form of (8.9) remain bounded provided that:
|Ë™xi(t)| â‰¤BË™x, |xi(t)| â‰¤Bx, (i = 1 . . . I) and |Ë™y(t)| â‰¤BË™y
âˆ€t
(8.21)
where BË™x, Bx, and BË™y are assumed to be some known positive constants.
8.4.1.3 Stability Analysis
Since the stability of each particle is independent of the others, the stability
of one particle is considered. The time derivative of (8.10) is calculated as
follows:
Ë™Ëœwr = âˆ’ËœwrKr + Ëœwr
R

r=1
ËœwrKr;
Ë™Ëœwr = âˆ’ËœwrKr + Ëœwr
R

r=1
ËœwrKr
(8.22)
where:
Aik = xi âˆ’cik
Ïƒ ik
and Aik = xi âˆ’cik
Ïƒ ik

146
Fuzzy Neural Networks for Real Time Control Applications
Kr =
I

i=1
Aik Ë™Aik and Kr =
I

i=1
Aik Ë™Aik
By using the following Lyapunov function, the stability of a single
particle is analyzed:
V = 1
2e2 + Î±1
2 Ïƒ TÏƒ + 1
2Î½T
Ïƒ Î½Ïƒ



I1
+ Î±1
2 Ïƒ TÏƒ + 1
2Î½T
Ïƒ Î½Ïƒ



I2
+ Î±1
2 cTc + 1
2Î½T
c Î½c



I3
(8.23)
where Ïƒ = [Ïƒ 11, . . . , Ïƒ ik, . . . , Ïƒ IK], Ïƒ = [Ïƒ 11, . . . , Ïƒ ik, . . . , Ïƒ IK], c =
[c11, . . . , cik, . . . , cIK], and I Ã— K = n. In order to make the time derivative
of the Lyapunov function easier to calculate, the following assumption is
made:
I1 = Î±1
2 Ïƒ TÏƒ + 1
2Î½T
Ïƒ Î½Ïƒ
(8.24)
Therefore, we have the following equation for the time derivative of I1:
Ë™I1= Î±1

i

k
Ïƒ ikÎ½Ïƒ ik+

i

k
Î½Ïƒ ik

(Î² âˆ’1)Î½Ïƒ ikâˆ’Î±1Ïƒ ik + Î±PÏƒ ik âˆ’Î±2Î½3
Ïƒ ik

(8.25)
and further:
Ë™I1 =

i

k

(Î² âˆ’1)Î½2
Ïƒ ik + Î±PÏƒ ik âˆ’Î±2Î½4
Ïƒ ik

(8.26)
Similar to the calculation of Ë™I1, the following equations for Ë™I2 and Ë™I3 are
obtained:
Ë™I2 =

i

k

(Î² âˆ’1)Î½2
Ïƒ ik + Î±P Ïƒik âˆ’Î±2Î½4
Ïƒ ik

(8.27)
Ë™I3 =

i

k

(Î² âˆ’1)Î½2
cik + Î±Pcik âˆ’Î±2Î½4
cik

(8.28)
The time derivative of (8.23) can be calculated as follows:
Ë™V = Ë™ee + Ë™I1 + Ë™I2 + Ë™I3 = e(Ë™yN âˆ’Ë™y) + Ë™I1 + Ë™I2 + Ë™I3
(8.29)

Fuzzy Neural Networks Training Using Particle Swarm Optimization
147
Taking the time derivative of (8.9), the following term can be obtained:
Ë™yN =Ë™q
R

r=1
fr Ëœwr + q
R

r=1
(Ë™fr Ëœwr + fr Ë™Ëœwr) âˆ’Ë™q
R

r=1
fr Ëœwr
+ (1 âˆ’q)
R

r=1
(Ë™fr Ëœwr + fr Ë™Ëœwr)
(8.30)
By using (8.22) and (8.30), the following term can be obtained:
Ë™yN = Ë™q
R

r=1
fr Ëœwr âˆ’Ë™q
R

r=1
fr Ëœwr + q
R

r=1

Ë™fr Ëœwr + fr(âˆ’ËœwrKr + Ëœwr
R

r=1
ËœwrKr)

+ (1 âˆ’q)
R

r=1

Ë™fr Ëœwr + fr(âˆ’ËœwrKr + Ëœwr
R

r=1
ËœwrKr)

â‰¤Ë™q
R

r=1

fr( Ëœwr âˆ’Ëœwr)

+
R

r=1
 Ë™fr(qËœwr + (1 âˆ’q) Ëœwr)

+ M1Bf
R

i=1
âˆ¥Krâˆ¥+ M2Bf
R

i=1
âˆ¥Krâˆ¥
(8.31)
in which âˆ¥.âˆ¥denotes the Euclidean norm of its corresponding argument and
Bf is such that âˆ¥Fâˆ¥< Bf and the value of âˆ¥Fâˆ¥is kept bounded using a pro-
jection algorithm. Considering the fact that xi, Ë™xi, Ë™y are bounded we have:
R

i=1
âˆ¥Krâˆ¥â‰¤M3 + M4âˆ¥Ë™câˆ¥+ M5âˆ¥Ë™Ïƒâˆ¥
R

i=1
âˆ¥Krâˆ¥â‰¤M6 + M7âˆ¥Ë™câˆ¥+ M8âˆ¥Ë™Ïƒâˆ¥
in which M3, M4, M5, M6, M7, and M8 are positive real values. Considering
the adaptation laws as:
Ë™ari = âˆ’xi
qËœwr + (1 âˆ’q) Ëœwr

qËœwr + (1 âˆ’q) Ëœwr
T 
qËœwr + (1 âˆ’q) Ëœwr
Î±sgn (e) , Î±1, Î± > 0
(8.32)

148
Fuzzy Neural Networks for Real Time Control Applications
Ë™br = âˆ’
qËœwr + (1 âˆ’q) Ëœwr

q Ëœwr + (1 âˆ’q) Ëœwr
T 
qËœwr + (1 âˆ’q) Ëœwr


Î±1e + Î± sgn (e)

(8.33)
Ë™q = âˆ’
1
F(	
W âˆ’	
W)T Î±sgn
(8.34)
we have:
Ë™yN â‰¤âˆ’2Î±sgn(e) âˆ’Î±1e âˆ’Î± sgn(e)
R

r=1
I

i=1
x2
i
+ Îº0 + Îº1âˆ¥Ë™câˆ¥+ Îº2âˆ¥Ë™Ïƒâˆ¥+ Îº3âˆ¥Ë™Ïƒâˆ¥
= âˆ’2Î±sgn(e) âˆ’Î±sgn(e)
R

r=1
I

i=1
x2
i
+ Îº0 + Îº1âˆ¥Î½câˆ¥+ Îº2âˆ¥Î½Ïƒâˆ¥+ Îº3âˆ¥Î½Ïƒ âˆ¥
in which:
(M1M3 + M2M6)Bf â‰¤Îº0
(M1M4 + M2M7)Bf â‰¤Îº1
M1M5Bf â‰¤Îº2
N2M8Bf â‰¤Îº3
The time derivative of the Lyapunov function is achieved as:
Ë™V â‰¤e(âˆ’2Î±sgn(e) âˆ’Î±1e âˆ’Î±Rsgn(e)
I

i=1
x2
i
+ Îº0 + Îº1âˆ¥Î½câˆ¥+ Îº2âˆ¥Î½Ïƒâˆ¥+ Îº3âˆ¥Î½Ïƒ âˆ¥âˆ’Ë™y)
+

i

k

(Î² âˆ’1)Î½2
Ïƒ ik + Î±PÏƒ ikÎ½Ïƒ ik âˆ’Î±2Î½4
Ïƒ ik

+

i

k

(Î² âˆ’1)Î½2
Ïƒ ik + Î±P Ïƒ ikÎ½Ïƒ ik âˆ’Î±2Î½4
Ïƒ ik

+

i

k

(Î² âˆ’1)Î½2
cik + Î±PcikÎ½cik âˆ’Î±2Î½4
cik

From linear algebra, we have the following lemma:

Fuzzy Neural Networks Training Using Particle Swarm Optimization
149
Lemma 8.1. For any positive real value Î» and real values for a and b we have:
|a||b| â‰¤Î»a2 + Î»âˆ’1b2
(8.35)
Considering Lemma 8.1, we get:
Ë™V â‰¤âˆ’2Î±|e| âˆ’Î±1e2 âˆ’Î±R|e|
I

i=1
x2
i + (Î»âˆ’1
1
+ Î»âˆ’1
2
+ Î»âˆ’1
3 )e2
+ Îº0|e| + Îº1Î»1âˆ¥Î½câˆ¥2 + Îº2Î»2âˆ¥Î½Ïƒâˆ¥2 + Îº3Î»3âˆ¥Î½Ïƒâˆ¥2 + BË™y|e|
+

i

k

(Î² âˆ’1)Î½2
Ïƒ ik + Î±PÏƒ ikÎ½Ïƒ ik âˆ’Î±2Î½4
Ïƒ ik

+

i

k

(Î² âˆ’1)Î½2
Ïƒ ik + Î±PÏƒ ikÎ½Ïƒ ik âˆ’Î±2Î½4
Ïƒ ik

+

i

k
(Î² âˆ’1)Î½2
cik + Î±PcikÎ½cik âˆ’Î±2Î½4
cik

in which Î»1, Î»2, and Î»3 have positive values. Considering the design
variables Î±1, Î±, and Î² as:
Î»âˆ’1
1
+ Î»âˆ’1
2
+ Î»âˆ’1
3
â‰¤Î±1
(8.36)
Îº0 + BË™y â‰¤Î±
(8.37)
Î² â‰¤1 âˆ’max(2Îº1Î»1, 2Îº2Î»2, 2Îº3Î»3)
(8.38)
we have:
Ë™V â‰¤âˆ’Î±|e| âˆ’Î±R|e|
I
i=1
x2
i
+

i

k
Î² âˆ’1
2
Î½2
Ïƒ ik + Î±(P2
Ïƒ ik + Î½2
Ïƒ ik) âˆ’Î±2Î½4
Ïƒ ik

+

i

k
Î² âˆ’1
2
Î½2
Ïƒ ik + Î±(P2
Ïƒ ik + Î½2
Ïƒ ik) âˆ’Î±2Î½4
Ïƒ ik

+

i

k
Î² âˆ’1
2
Î½2
cik + Î±(P2
cik + Î½2
cik) âˆ’Î±2Î½4
cik


150
Fuzzy Neural Networks for Real Time Control Applications
It is further possible to choose Î² as Î² â‰¤âˆ’(4Î± + 1) so that we achieve
the following equation for the time derivative of the Lyapunov function:
Ë™V â‰¤âˆ’Î±|e| âˆ’Î±R|e|
I

i=1
x2
i
+

i

k
Î² âˆ’1
4
Î½2
Ïƒ ik + Î±P2
Ïƒ ik âˆ’Î±2Î½4
Ïƒ ik

+

i

k
Î² âˆ’1
4
Î½2
Ïƒ ik + Î±P2
Ïƒ ik âˆ’Î±2Î½4
Ïƒ ik

+

i

k
Î² âˆ’1
4
Î½2
cik + Î±P2
cik âˆ’Î±2Î½4
cik

The above equation implies the time derivative of the Lypapunov
function is negative until the parameters of the T2FNN converge to a
neighborhood near zero in which:
|Î½cik| â‰¤
4Î±
1 âˆ’Î² Pcik
|Î½Ïƒ ik| â‰¤
4Î±
1 âˆ’Î² PÏƒ ik
|Î½Ïƒ ik| â‰¤
4Î±
1 âˆ’Î² P Ïƒik
and hence all the parameters of T2FNN are bounded.
8.4.2 Proposed Hybrid Training Algorithm for T2FNN With
Type-2 MFs with Uncertain c
8.4.2.1 T2FNN with Type-2 MFs with Uncertain c
The upper and lower type-2 fuzzy Gaussian MFs with an uncertain center
(Fig. 8.4) are presented as follows:
Î¼ik(xi) =
âŽ§
âŽªâŽªâŽªâŽªâŽ¨
âŽªâŽªâŽªâŽªâŽ©
exp

âˆ’1
2
(xi âˆ’cik)2
Ïƒ 2
ik

xi < cik
1
xik â‰¤xi â‰¤cik
exp

âˆ’1
2
(xi âˆ’cik)2
Ïƒ 2
ik

cik < xi
(8.39)

Fuzzy Neural Networks Training Using Particle Swarm Optimization
151
m(x)
0
0.2
0.4
0.6
0.8
1
c1
c2
Upper MF
Lower MF
Figure 8.4 Type-2 Gaussian fuzzy MF with uncertain standard deviation.
Î¼ik(xi) =
âŽ§
âŽªâŽªâŽ¨
âŽªâŽªâŽ©
exp

âˆ’1
2
(xi âˆ’cik)2
Ïƒ 2
ik

xi < cik+cik
2
exp

âˆ’1
2
(xi âˆ’cik)2
Ïƒ 2
ik

cik+cik
2
â‰¤xi
(8.40)
where cik has an interval value [cik, cik] and is the center parameter of the
kth type-2 fuzzy set for the ith input. The parameter Ïƒik is the standard
deviation for the MFs and has a certain value.
The firing strength of the rules, which is itself an interval value, is
calculated using the prod t-norm operator as follows:
wr = Î¼ ËœA1(x1) âˆ—Î¼ ËœA2(x2) âˆ—Â· Â· Â· Î¼ ËœAI(xI)
wr = Î¼ ËœA1(x1) âˆ—Î¼ ËœA2(x2) âˆ—Â· Â· Â· Î¼ ËœAI(xI)
(8.41)
The consequent part corresponding to each fuzzy rule is a linear
combination of the inputs x1, x2 . . . xI. This linear function is called fr and
is defined as fr = br + I
i=1 arixi. The output of the network is calculated
as follows:
yN = q
R

r=1
fr Ëœwr + (1 âˆ’q)
R

r=1
frwr
(8.42)

152
Fuzzy Neural Networks for Real Time Control Applications
where 
wr and 
wr are the normalized values of the lower and upper output
signals from the second hidden layer of the network as follows:

wr =
wr
R
i=1 wr
and 
wr =
wr
R
i=1 wr
(8.43)
The following vectors can be specified:
	
W (t) =


w1 (t) 
w2 (t) . . . 	
wR (t)
T, 	
W (t) =


w1 (t) 
w2 (t) . . . 	
wR (t)
T,
and F = [ f1 f2 . . . fR]
The following assumptions have been used in this investigation: The
time derivative of both the input and output signals can be considered
bounded:
|Ë™xi(t)| â‰¤BË™x, |xi(t)| â‰¤Bx,
(i = 1 . . . I)
and
|Ë™y(t)| â‰¤BË™y
âˆ€t
(8.44)
where BË™x, Bx, and BË™y are assumed to be some known positive constants.
It is clear that the equations of T2FNN with uncertain center values are
quite similar to those of T2FNN with an uncertain Ïƒ value. The main
difference is that when the center parameter of a type-2 MF is an interval,
the MFs become undifferentiable in three points.
8.4.2.2 Proposed Hybrid Training Algorithm for T2FNN with MFs with
Uncertain Center Value
The proposed hybrid PSO and SMC training method is as follows:
Ë™csl = Î½csl
(8.45)
Ë™Î½csl = (Î² âˆ’1)Î½csl âˆ’Î±csl + Î³1Plcsl + Î³2Pgcsl âˆ’Î±2Î½3
csl, l = 1, . . . , n,
s = 1, . . . , N
Ë™csl = Î½csl
(8.46)
Ë™Î½csl = (Î² âˆ’1)Î½csl âˆ’Î±csl + Î³1Plcsl + Î³2Pgcsl âˆ’Î±2Î½3
csl, l = 1, . . . , n,
s = 1, . . . , N
Ë™Ïƒsl = Î½Ïƒsl
(8.47)
Ë™Î½Ïƒsl = (Î² âˆ’1)Î½Ïƒsl âˆ’Î±Ïƒsl + Î³1PlÏƒsl + Î³2PgÏƒl âˆ’Î±2Î½3
Ïƒsl, l = 1, . . . , n,
s = 1, . . . , N

Fuzzy Neural Networks Training Using Particle Swarm Optimization
153
Ë™aris = âˆ’xi
qËœwr + (1 âˆ’q) Ëœwr

q Ëœwr + (1 âˆ’q) Ëœwr
T 
qËœwr + (1 âˆ’q) Ëœwr
Î±sgn (e) ,
s = 1, . . . , N, r = 1, . . . , R
(8.48)
Ë™brs = âˆ’
qs Ëœwrs + (1 âˆ’qs) Ëœwrs

qs Ëœwrs + (1 âˆ’qs) Ëœwrs
T 
qs Ëœwrs + (1 âˆ’qs) Ëœwrs


Î±1es + Î±sgn (es)

s = 1, . . . , N, r = 1, . . . , R
(8.49)
Ë™qs = âˆ’
1
Fs(	
W s âˆ’	
W s)T Î±sgn (es) , s = 1, . . . , N
(8.50)
In (8.45)-(8.50), es is the identification error corresponding to sth
particle, n is the number of type-2 MFs with uncertain centers, subscript s
refers to the sth particle, and subscript l refers to lth dimension. For example,
Ïƒsl means the lth dimension of sth particle corresponding to Ïƒ value of MFs.
The cost function f (es(t)) is defined as:
f (es(t)) = 1
2

e2
s (t), s = 1, . . . , N
(8.51)
where the limits of the summation depend on the window size decided by
the programmer, and the best local experiences of each particle Plcs, Plcs,
and PlÏƒs are updated using this cost function:
Plcs, Plcs, and PlÏƒs = arg min{ f (e(t)|Plcs(tâˆ’), Plcs(tâˆ’), PlÏƒs(tâˆ’)), f (e(t)|cs(t),
cs(t), Ïƒs(t))}
Pgc, Pgc, and PgÏƒ = arg min{ f (e(t)|Pgc(tâˆ’), Pgc(tâˆ’), PgÏƒ(tâˆ’)), f (e(t)|cs(t),
cs(t), Ïƒs(t))}
âˆ€s âˆˆN
Theorem 8.2. The adaptation laws of (8.12)-(8.17) guarantee that the
parameters of T2FNN in the form of (8.42) remain bounded provided that:
|Ë™xi(t)| â‰¤BË™x, |xi(t)| â‰¤Bx, (i = 1 . . . I) and |Ë™y(t)| â‰¤BË™y
âˆ€t
(8.52)
where BË™x, Bx, and BË™y are assumed to be some known positive constants.

154
Fuzzy Neural Networks for Real Time Control Applications
8.4.2.3 Stability Analysis
Since the stability of each particle is independent of the others, the stability
of one particle is considered. Furthermore, for brevity the s in the indices
does not appear. Since âˆ¥xâˆ¥â‰¤Bx and âˆ¥Ë™xâˆ¥â‰¤BË™x, the time derivative of
(8.43) has the following properties:
R

r=1
Ë™Ëœwr â‰¤B3 + B4âˆ¥Ë™câˆ¥+ B5âˆ¥Ë™Ïƒâˆ¥
(8.53)
R

r=1
Ë™Ëœwr â‰¤B6 + B7âˆ¥Ë™câˆ¥+ B8âˆ¥Ë™Ïƒâˆ¥
(8.54)
in which B3, B4, B5, B6, B7, and B8 are positive real values. By using the
following Lyapunov function, the stability of a single particle is analyzed.
V = 1
2e2 + Î±1
2 cTc + 1
2Î½T
c Î½c



I1
+ Î±1
2 cTc + 1
2Î½T
c Î½c



I2
+ Î±1
2 Ïƒ TÏƒ + 1
2Î½T
Ïƒ Î½Ïƒ



I3
(8.55)
To reduce the calculations required for the Lyapunov function and its
time derivative the following assumption is made:
I1 = Î±1
2 cTc + 1
2Î½T
c Î½c
(8.56)
Therefore, we have the following equation for the time derivative of I1:
Ë™I1 = Î±1

i

k
cikÎ½cik +

i

k
Î½cik

(Î² âˆ’1)Î½cik âˆ’Î±1cik + Î±Pcik âˆ’Î±2Î½3
cik

(8.57)
and further:
Ë™I1 =

i

k

(Î² âˆ’1)Î½2
cik + Î±Pcik âˆ’Î±2Î½4
cik

(8.58)
Similar to the calculation of Ë™I1, the following equations for Ë™I2 and Ë™I3 are
obtained:
Ë™I2 =

i

k

(Î² âˆ’1)Î½2
cik + Î±Pcik âˆ’Î±2Î½4
cik

(8.59)

Fuzzy Neural Networks Training Using Particle Swarm Optimization
155
Ë™I3 =

i

k

(Î² âˆ’1)Î½2
Ïƒik + Î±PÏƒik âˆ’Î±2Î½4
Ïƒik

(8.60)
The time derivative of the Lyapunov function is obtained as follows:
Ë™V = Ë™ee + Ë™I1 + Ë™I2 + Ë™I3 = e(Ë™yN âˆ’Ë™y) + Ë™I1 + Ë™I2 + Ë™I3
(8.61)
Taking the time derivative of (8.42), the following term can be
obtained:
Ë™yN = Ë™q
R

r=1
fr Ëœwr + q
R

r=1
(Ë™fr Ëœwr + fr Ë™Ëœwr) âˆ’Ë™q
R

r=1
fr Ëœwr
+ (1 âˆ’q)
R

r=1
(Ë™fr Ëœwr + fr Ë™Ëœwr)
(8.62)
By using (8.22), (8.53), and (8.62), the following can be obtained:
Ë™yN = Ë™q
R

r=1
fr Ëœwr âˆ’Ë™q
R

r=1
fr Ëœwr + q
R

r=1

Ë™fr Ëœwr + fr(âˆ’ËœwrKr + Ëœwr
R

r=1
ËœwrKr)

+ (1 âˆ’q)
R

r=1

Ë™fr Ëœwr + fr(âˆ’ËœwrKr + Ëœwr
R

r=1
ËœwrKr)

â‰¤Ë™q
R

r=1

fr( Ëœwr âˆ’Ëœwr)

+
R

r=1
 Ë™fr(qËœwr + (1 âˆ’q) Ëœwr)

+ Bf (B1 + B4) + Bf B2âˆ¥Ë™câˆ¥+ Bf B5âˆ¥Ë™câˆ¥+ Bf (B3 + B6)âˆ¥Ë™Ïƒâˆ¥
(8.63)
Considering the adaptation laws as:
Ë™ari = âˆ’xi
qËœwr + (1 âˆ’q) Ëœwr

qËœwr + (1 âˆ’q) Ëœwr
T 
qËœwr + (1 âˆ’q) Ëœwr
Î±sgn (e)
(8.64)
Ë™br = âˆ’
qËœwr + (1 âˆ’q) Ëœwr

q Ëœwr + (1 âˆ’q) Ëœwr
T 
qËœwr + (1 âˆ’q) Ëœwr


Î±1e + Î±sgn (e)

(8.65)
Ë™q = âˆ’
1
F(	
W âˆ’	
W)T Î±sgn
(8.66)

156
Fuzzy Neural Networks for Real Time Control Applications
we have:
Ë™yN â‰¤âˆ’2Î±sgn(e) âˆ’Î±1e âˆ’Î±sgn(e)
R

r=1
I
i=1
x2
i
+ Îº0 + Îº1âˆ¥Ë™Ïƒâˆ¥+ Îº2âˆ¥Ë™câˆ¥+ Îº3âˆ¥Ë™câˆ¥
= âˆ’2Î±sgn(e) âˆ’Î±Rsgn(e)
I

i=1
x2
i
+ Îº0 + Îº1âˆ¥Î½Ïƒ âˆ¥+ Îº2âˆ¥Î½câˆ¥+ Îº3âˆ¥Î½câˆ¥
in which:
(B1 + B4)Bf â‰¤Îº0
(B3 + B6)Bf â‰¤Îº1
B2Bf â‰¤Îº2
B5Bf â‰¤Îº3
The time derivative of the Lyapunov function is achieved as:
Ë™V â‰¤e(âˆ’2Î±sgn(e) âˆ’Î±1e âˆ’Î±Rsgn(e)
I

i=1
x2
i
+ Îº0 + Îº1âˆ¥Î½Ïƒâˆ¥+ Îº2âˆ¥Î½câˆ¥+ Îº3âˆ¥Î½câˆ¥âˆ’Ë™y)
(8.67)
+

i

k

(Î² âˆ’1)Î½2
cik + Î±PcikÎ½cik âˆ’Î±2Î½4
cik

+

i

k
(Î² âˆ’1)Î½2
cik + Î±PcikÎ½cik âˆ’Î±2Î½4
cik

+

i

k

(Î² âˆ’1)Î½2
Ïƒik + Î±PÏƒikÎ½Ïƒik âˆ’Î±2Î½4
Ïƒik

(8.68)
Considering Lemma 8.1 as is in (8.35) we get:
Ë™V â‰¤âˆ’2Î±|e| âˆ’Î±1e2 âˆ’Î±R|e|
I

i=1
x2
i + (Î»âˆ’1
1
+ Î»âˆ’1
2
+ Î»âˆ’1
3 )e2
+ Îº0|e| + Îº1Î»1âˆ¥Î½Ïƒ âˆ¥2 + Îº2Î»2âˆ¥Î½câˆ¥2 + Îº3Î»3âˆ¥Î½câˆ¥2 + BË™y|e|

Fuzzy Neural Networks Training Using Particle Swarm Optimization
157
+

i

k

(Î² âˆ’1)Î½2
cik + Î±PcikÎ½cik âˆ’Î±2Î½4
cik

+

i

k
(Î² âˆ’1)Î½2
cik + Î±PcikÎ½cik âˆ’Î±2Î½4
cik

(8.69)
+

i

k

(Î² âˆ’1)Î½2
Ïƒik + Î±PÏƒikÎ½Ïƒik âˆ’Î±2Î½4
Ïƒik

in which Î»1, Î»2, and Î»3 has positive values. Considering the design variables
Î±1, Î±, and Î² as:
Î»âˆ’1
1
+ Î»âˆ’1
2
+ Î»âˆ’1
3
â‰¤Î±1
Îº0 + BË™y â‰¤Î±
Î² â‰¤1 âˆ’max(2Îº1Î»1, 2Îº2Î»2, 2Îº3Î»3)
we have:
Ë™V â‰¤âˆ’Î±|e| âˆ’Î±R|e|
I
i=1
x2
i
+

i

k
Î² âˆ’1
2
Î½2
cik + Î±(P2
cik + Î½2
cik) âˆ’Î±2Î½4
cik

+

i

k
Î² âˆ’1
2
Î½2
cik + Î±(P2
cik + Î½2
cik) âˆ’Î±2Î½4
cik

+

i

k
Î² âˆ’1
2
Î½2
Ïƒik + Î±(P2
Ïƒik + Î½2
Ïƒik) âˆ’Î±2Î½4
Ïƒik

It is further possible to choose Î² as Î² â‰¤âˆ’(4Î± + 1), so that we achieve
the following equation for the time derivative of the Lyapunov function:
Ë™V â‰¤âˆ’Î±|e| âˆ’Î±R|e|
I
i=1
x2
i
+

i

k
Î² âˆ’1
4
Î½2
cik + Î±P2
cik âˆ’Î±2Î½4
cik


158
Fuzzy Neural Networks for Real Time Control Applications
+

i

k
Î² âˆ’1
4
Î½2
cik + Î±P2
cik âˆ’Î±2Î½4
cik

+

i

k
Î² âˆ’1
4
Î½2
Ïƒik + Î±P2
Ïƒik âˆ’Î±2Î½4
Ïƒik

The above equation implies that the time derivative of the Lypapunov
function is negative until the parameters of the T2FNN converge to a
neighborhood near zero in which:
|Î½Ïƒik| â‰¤
4Î±
1 âˆ’Î² PÏƒik
|Î½cik| â‰¤
4Î±
1 âˆ’Î² Pcik
|Î½cik| â‰¤
4Î±
1 âˆ’Î² Pcik
and hence all of the parameters of T2FNN are bounded.
8.4.3 Further Discussion About the Hybrid Training Method
Since swarm-based optimization algorithms benefit from multiple solutions
to the problem and simple update rules for the particles, they are used
in combination with the SMC-based training algorithm to estimate the
parameters of T2FNNs. The CPSO is used to estimate the parameters of
the antecedent part and the SMC-based training method is used to estimate
the parameters of the consequent part. The hybrid use of PSO for the
training of the parameters of the antecedent part and SMC-based training
algorithm for the parameters of the consequent part parameters has the
following advantages over other training methods for T2FNNs:
â€¢
CPSO benefits from multiple solutions of the optimization problem,
and it has less possibility of entrapment in local minima.
â€¢
Since CPSO benefits from simple rules, it may be a preferable choice
with respect to GD and its variants, which necessitate the calculation
of derivatives of output of FNN with respect to its parameters, be-
cause these parameters appear nonlinearly and their calculation is a
difficult task.
â€¢
Another benefit of CPSO with respect to GD and its variants is that the
selection of learning rate in these algorithms is very important because

Fuzzy Neural Networks Training Using Particle Swarm Optimization
159
a large value for the learning rate may cause instability while a small
learning rate highly increases the possibility of the entrapment of the
training algorithm in local minima.
â€¢
Unlike most of training methods which suffer from the lack of rigorous
stability analysis, the stability analysis of the hybrid combination of
CPSO and SMC-based training algorithm is fully considered using an
appropriate Lyapunov function in this chapter.
â€¢
The hybrid combination of the CPSO and SMC-based training al-
gorithm for the estimation of the parameters of T2FNN does not
necessitate any matrix manipulations as is common with some of the
training algorithms of T2FNN (e.g., extended KF and LM).
Furthermore, the proposed approach benefits from a new version of CPSO,
called HCPSO, that has a nonlinear term and more degrees of freedom,
which makes it possible to come up with better results. What is more,
CPSO and HCPSO separately and/or in combination with the SMC-based
training method are never used to train a FNN previously.
8.5 CONCLUSION
This chapter deals with a hybrid CPSO and SMC theory-based training
method for the estimation of T2FNN parameters. Since CPSO benefits
from a simple mathematical formulation, its use for the antecedent part
parameters of T2FNNs makes it easier to train them. Moreover, since CPSO
benefits from number of different solutions for the the antecedent part
parameters, it is less probable that it will get stuck in local minima. The
hybrid use of CPSO with SMC allows us have rigorous stability analysis
using an appropriate Lyapunov function for the identification process.
REFERENCES
[1] C.-J. Lin, An efficient immune-based symbiotic particle swarm optimization learning
algorithm for TSK-type neuro-fuzzy networks design, Fuzzy Sets Syst. 159 (21) (2008)
2890-2909.
[2] B. Allaoua, A. Laoufi, B. Gasbaoui, A. Abderrahmani, Neuro-fuzzy dc motor speed
control using particle swarm optimization, Leonardo Electron. J. Pract. Tech. 15 (2009)
1-18.
[3] C.-J. Lin, S.-J. Hong, The design of neuro-fuzzy networks using particle swarm
optimization and recursive singular value decomposition, Neurocomputing 71 (1) (2007)
297-310.
[4] H. Shayeghi, H. Shayanfar, PSO based neuro-fuzzy controller for LFC design including
communication time delays, Int. J. Tech. Phys. Probl. Eng. 1 (2) (2010) 28-36.

160
Fuzzy Neural Networks for Real Time Control Applications
[5] M.A. Shoorehdeli, M. Teshnehlab, A.K. Sedigh, M.A. Khanesar, Identification using
ANFIS with intelligent hybrid stable learning algorithm approaches and stability analysis
of training methods, Appl. Soft Comput. 9 (2) (2009) 833-850.
[6] M.A. Khanesar, M. Teshnehlab, E. Kayacan, O. Kaynak, A novel type-2 fuzzy member-
ship function: Application to the prediction of noisy data, in: 2010 IEEE International
Conference on Computational Intelligence for Measurement Systems and Applications
(CIMSA), IEEE, 2010, pp. 128-133.
[7] M. Khanesar, M. Shoorehdeli, M. Teshnehlab, Hybrid training of recurrent fuzzy neural
network model, in: International Conference on Mechatronics and Automation, 2007,
ICMA 2007, IEEE, 2007, pp. 2598-2603.
[8] H.M. Emara, H.A.A. Fattah, Continuous swarm optimization technique with stability
analysis, in: Proceedings of the 2004 American Control Conference, vol. 3, IEEE, 2004,
pp. 2811-2817.

CHAPTER 9
Noise Reduction Property of
Type-2 Fuzzy Neural Networks
Contents
9.1
Introduction
161
9.2
Type-2 Fuzzy Neural System Structure
162
9.2.1 Elliptic MF
162
9.2.2 Structure of the T2FLS
163
9.2.3 Noise Reduction Property of the Proposed Type-2 MF
165
9.2.3.1 Case I
165
9.2.3.2 Case II
168
9.3
Conclusion
172
References
172
Abstract
In this chapter, an attempt is made to show the effect of input noise in the rule base in
a general way. There exist number of papers in literature claiming that the performance
of T2FLSs is better than its type-1 counterparts under noisy conditions. We try to justify
this claim by simulation studies only for some specific systems. However, in this chapter,
suchananalysisisdoneindependentof thesystemto becontrolled.Forsuchananalysis,
a novel type-2 fuzzy MF (elliptic MF) is proposed. This type-2 MF has certain values on
both ends of the support and the kernel, and some uncertain values for other values of
the support. The findings of the general analysis in this chapter and the aforementioned
studies published in literature are coherent.
Keywords
Type-1 fuzzy neural networks, Type-2 fuzzy neural networks, TSK models, Artificial
intelligence, Fuzzy logic, Neural networks
9.1 INTRODUCTION
In control theory, there is a direct relationship between the performance of
the controller and the accuracy of the model of the system. What is more,
in real life, it is always difficult to obtain a precise model to be controlled.
Furthermore, the system is generally subjected to noise from both inside
Fuzzy Neural Networks for Real Time Control Applications
Copyright Â© 2016 Elsevier Inc.
http://dx.doi.org/10.1016/B978-0-12-802687-8.00009-8
All rights reserved.
161

162
Fuzzy Neural Networks for Real Time Control Applications
and outside of the system. In such cases, the use of FLCs is preferable
since they can handle lack of modeling and noise better than model-based
controllers.
The performance of type-2 fuzzy sets in the presence of measurement
noise has been considered in number of different papers. In many papers, it
is concluded that T2FLSs give more promising control performance under
noisy working environments [1]. For instance, in Ref. [2], the effect of
measurement noise in type-1 and type-2 FLCs is simulated to perform
comparative analysis of the responses of the systems in the presence of uncer-
tainty. It is shown that the use of a T2FLC in real world applications which
exhibit measurement noise can be preferable. In Ref. [3], type-2 fuzzy logic
theory is applied to predict Mackey-Glass chaotic time-series with uniform
noise. The comparison between type-1 and type-2 fuzzy systems shows the
superiority of type-2 fuzzy systems in the presence of noise in the inputs.
A common claim in literature is that the performance of the T2FLSs is
superior over their type-1 counterparts, especially under noisy conditions.
However, the justifications offered for the claim made about the noise
reduction property are generally limited to simulation studies carried out for
specific systems. In this chapter, we propose a novel type-2 MF that enables
us to come up with some metrics. The parameters of this function that
represent uncertainty are de-coupled from the parameters that determine
the center and support of the MF. This allows us to analyze the distortion
of the rule base by the uncertainties in the inputs to the rule base. For this
analysis, a simple T2FLS with the proposed novel MF is considered in which
the effect of input noise in the rule base can be shown in a general way.
9.2 TYPE-2 FUZZY NEURAL SYSTEM STRUCTURE
9.2.1 Elliptic MF
A novel type-2 fuzzy MF is introduced. It has certain values on both ends
of the support and kernel, and some uncertain values on other values of the
support. The mathematical expression for the novel MF is expressed as:
ËœÂµ(x) =
 
1 âˆ’
xâˆ’c
d
a 1
a
ifc âˆ’d < x < c + d
0
else
(9.1)
where c and d are the center and the width of the MF and x is the input
vector. The parameters a1 and a2 (a2 < a < a1) determine the width of the

Noise Reduction Property of Type-2 Fuzzy Neural Networks
163
uncertainty of the proposed MF, and these parameters should be selected as
follows:
a1 > 1
(9.2)
0 < a2 < 1
Figures 9.1(a), (b), and (c) show the shapes of the proposed MF for a1 =
a2 = 1, a1 = 1.2, a2 = 0.8, and a1 = 1.4, a2 = 0.6, respectively. As can
be seen from Fig. 9.1(a), the shape of the proposed type-2 MF is changed
to a type-1 triangular MF when its parameters are selected as a1 = a2 = 1.
These parameters can be selected as some constants or they can be tuned
adaptively.
9.2.2 Structure of the T2FLS
The interval T2FLS considered in this chapter benefits from type-2 MFs in
the premise part and crisp numbers in the consequent part. Such a structure
is called a A2-C0 fuzzy system in the literature [6], and the rule base is as
follows:
IF x1 is ËœAj1 and x2 is ËœAj2 and â€¦and xn is ËœAjn
THEN uj =
n

i=1
wijxi + bj
(9.3)
where x1, x2, . . . ,xn are the input variables, uj(j = 1, . . . , M) are the output
variables, and ËœAij is a type-2 MFs for the jth rule and the ith input. wij and
bj (i = 1, . . . , n, j = 1, . . . , M) are the parameters in the consequent part of
the rules. The final output of the system can be written as [6]:
YTSK/A2âˆ’C0 =

f 1âˆˆ[f 1,f 1]
Â· Â· Â·

f Mâˆˆ[f M,f M]
1/
M
j=1 f juj
M
j=1 f j
(9.4)
where f j and f
j are given by:
f j(x) = ÂµËœF j
1(x1) âˆ—Â· Â· Â· âˆ—ÂµËœF j
n(xn)
(9.5)
f
j(x) = ÂµËœF j
1(x1) âˆ—Â· Â· Â· âˆ—ÂµËœF j
n(xn)

164
Fuzzy Neural Networks for Real Time Control Applications
m(x)
0
0.2
(a)
0.4
0.6
0.8
1
c
c â€“d
c + d
(b)
m(x)
0
0.2
0.4
0.6
0.8
1
c
câ€“ d
c+ d
(c)
m(x)
0
0.2
0.4
0.6
0.8
1
c
câ€“ d
c + d
Figure 9.1 Shapes of the proposed type-2 MF with different values for a1 and a2.

Noise Reduction Property of Type-2 Fuzzy Neural Networks
165
in which âˆ—represents the t-norm, which is the prod operator in this study.
Although the exact computation of (9.4) requires Karnik-Mendel algorithm
to be used, an approximate algorithm to compute the output of this fuzzy
system in closed form is achieved by [6, 7]:
YTSK1 =
M
j=1 f juj
M
j=1 f j + M
j=1 f
j +
M
j=1 f
juj
M
j=1 f j + M
j=1 f
j
(9.6)
YTSK1 =
M
j=1 (f j + f
j)uj
M
j=1 f j + M
j=1 f
j
(9.7)
In this way, the firing of each rule is defined as follows:
rj =
f j + f
j
M
j=1 f j + M
j=1 f
j
(9.8)
9.2.3 Noise Reduction Property of the Proposed Type-2 MF
In this chapter, effort is made to prove the noise reduction property of
T2FLSs. We consider two different fuzzy logic systems with the proposed
novel MF. The first one is a T2FLS with one input with two MFs, and the
other one is with two inputs with two MFs for each. As the parameters
responsible for the width of uncertainty and the parameters responsible for
the center and the support of the proposed MF (c and d, respectively) are
decoupled from each other in the type-2 MF, it is possible to analyze how
the width of uncertainty of the MF and the DCN in the rule base of the
fuzzy system are related.
9.2.3.1 Case I
Let us consider a single input T2FLS that uses two MFs ËœÂµ1 and ËœÂµ2 such that:
Â¯Âµ2 = 1 âˆ’Âµ1
(9.9)
Âµ2 = 1 âˆ’Â¯Âµ1
(9.10)

166
Fuzzy Neural Networks for Real Time Control Applications
The lower (Âµ1) and upper ( Â¯Âµ1) MFs with the parameters c1, d1, a1, and
a2 are defined as follows:
Â¯Âµ1(x) =
 	
1 âˆ’|xâˆ’c1
d1 |a1

1/a1
ifc1 âˆ’d1 < x < c1 + d1
0
else
(9.11)
Âµ1(x) =
 	
1 âˆ’|xâˆ’c1
d1 |a2

1/a2
if c1 âˆ’d1 < x < c1 + d1
0
else
(9.12)
The fuzzy system has two rules. Using (9.8), the firing strength of the
first rule is calculated as:
r1(x) =
Â¯Âµ1(x) + Âµ1(x)
2
(9.13)
The firing strength will be distorted by the noise in the data as:
r1(x + n) =
Â¯Âµ1(x + n) + Âµ1(x + n)
2
(9.14)
where n indicates the noise added on the signal.
The total DCN over the support set can be found by the following
integral:
DCN =
 n=n1
n=âˆ’n1
 x=d1+c1
x=âˆ’d1+c1
[r1(x) âˆ’r1(x + n)]2 dx dn
(9.15)
To simplify the limits in (9.15), the following definitions are done:
t = x âˆ’c1
d1
, nâ€² = n
d1
(9.16)
Using the definitions in (9.16), the following equation is achieved:
DCN = d2
1
 nâ€²=n1/d1
nâ€²=âˆ’n1/d1
 t=1
t=âˆ’1
[r1(x) âˆ’r1(x + n)]2 dt dnâ€²
(9.17)
In above, the parameter n1 is the magnitude of the amplitude of the noise
added onto the input of the fuzzy system.

Noise Reduction Property of Type-2 Fuzzy Neural Networks
167
The integral in (9.17) cannot be calculated explicitly. Therefore, a
numerical solution of this integral is obtained for each pair of a1 and
a2, and the distortion is drawn with respect to the a1 and a2 parameters.
Figure 9.2 shows the numerical solution of the integral above for the noise
level SNR = 0 dB. Note that to achieve SNR = 0 dB, n1 is selected as
being equal to d1. As can be seen from the figure, the parameter a2 is more
critical in the noise reduction property of the proposed type-2 MF when
compared to the parameter a1. We can also see that the DCN in the case
of a1 = a2 = 1, which corresponds to type-1 MF, is higher than the other
values of a1 and a2, which correspond to the case of type-2 MFs. The figure
indicates that by an appropriate selection of the parameters a1 and a2, it is
possible to achieve better performance in the presence of noise. Although
it has already been stated that the parameter a1 should be selected bigger
than 1 and the parameter a2 between 0 and 1, Fig. 9.2 shows that there is
a proper selection area for a1 and a2. Although the figure shows that it is
better to select the parameter a2 bigger than 0.3 and any value for a1, a very
small value for a2 results in such a lower MF that the membership grade is
1
1.2
1.4
1.6
1.8
2
0
0.2
0.4
0.6
0.8
1
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
a1
a2
Distortion caused by noise / d 1
2
Figure 9.2 3D figure of DCN (9.17) w.r.t. a1 and a2 for high levels of noise (SNR = 0 dB).

168
Fuzzy Neural Networks for Real Time Control Applications
1
1.2
1.4
1.6
1.8
2
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
Ã— 10â€“5
a2
a1
Distortion caused by noise/d 1
2
Figure 9.3 3D figure of DCN (9.17) w.r.t. a1 and a2 for low levels of noise (SNR = 100 dB).
close to zero for a large portion of its support. Similarly, a very large value
for a1 results in such an upper MF that the membership grade is close to
one for a large portion of its support. Such MFs are not very desirable. An
appropriate selection for a1 and a2 would therefore be a1 < 2 and a2 > 0.5.
Figure 9.3 shows the numerical solution of the integral for a low level
of noise (SNR = 100 dB). This figure indicates that the noise reduction
property of T1FLSs is comparable with T2FLSs in the presence of very low
levels of noise.
9.2.3.2 Case II
Consider a T2FLS with two inputs and two type-2 MFs for each input. The
MFs for the first input are selected as ËœÂµ11 and ËœÂµ12, and the MF selected for
the second input are as ËœÂµ21 and ËœÂµ22. The type-2 fuzzy MF ËœÂµ11 is defined as:
Â¯Âµ11(x) =
 	
1 âˆ’|xâˆ’c11
d11 |a111

1/a111
if|x âˆ’c11| < d11
0
else
(9.18)

Noise Reduction Property of Type-2 Fuzzy Neural Networks
169
Âµ11(x) =
 	
1 âˆ’|xâˆ’c11
d11 |a211

1/a211
if|x âˆ’c11| < d11
0
else
(9.19)
The type-2 fuzzy MF ËœÂµ21 is defined as:
Â¯Âµ21(x) =
 	
1 âˆ’|xâˆ’c21
d21 |a121

1/a121
if |x âˆ’c21| < d21
0
else
(9.20)
Âµ21(x) =
 	
1 âˆ’|xâˆ’c21
d21 |a221

1/a221
if |x âˆ’c21| < d21
0
else
(9.21)
In order to see the effect of a111 and a211 on the DCN, a121 and a221 are
set to 1. The other MFs are considered as:
Â¯Âµ12 = 1 âˆ’Âµ11
(9.22)
Âµ12 = 1 âˆ’Â¯Âµ11
Â¯Âµ22 = 1 âˆ’Âµ21
Âµ22 = 1 âˆ’Â¯Âµ21
This fuzzy system has four rules. The firing strength for the first rule is
written as:
r1(x1, x2) = 1
2
	
Â¯Âµ11(x1) Â¯Âµ21(x2) + Âµ11(x1)Âµ21(x2)

(9.23)
The firing strength will be distorted by the noise in the data as:
r1(x1 + n, x2) = 1
2
	
Â¯Âµ11(x1 + n) Â¯Âµ21(x2) + Âµ11(x1 + n)Âµ21(x2)

(9.24)
The total DCN over the support set can be calculated by the following
integral:
DCN =
 n=n1
n=âˆ’n1
 x2=d21+c21
x2=âˆ’d21+c21
 x1=d11+c11
x1=âˆ’d11+c11
[r1(x1, x2)
(9.25)
âˆ’r1(x1 + n, x2)]2 dx1 dx2 dn

170
Fuzzy Neural Networks for Real Time Control Applications
To simplify the limits in (9.25), the following terms are defined:
t1 = x1 âˆ’c11
d11
, t2 = x2 âˆ’c21
d21
, nâ€² = n
d11
(9.26)
Using the definitions in (9.26), the following equation is achieved:
DCN = d2
11d21
 nâ€²=n1/d11
nâ€²=âˆ’n1/d11
 t2=1
t2=âˆ’1
 t1=1
t1=âˆ’1
[r1(x1, x2)
(9.27)
âˆ’r1(x1 + n, x2)]2 dt1 dt2 dnâ€²
Similar to Case I, the integral in (9.27) cannot be calculated explicitly.
Therefore, a numerical solution of this integral is obtained for each pair
of a1 and a2, and the distortion is drawn with respect to the a1 and a2
parameters in Fig. 9.4. This figure is obtained with n1 equal to d1, which
corresponds to SNR = 0 dB. Figure 9.5, which is the contour diagram
of Fig. 9.4, shows that there is an appropriate selection area for a1 and a2.
1
1.2
1.4
1.6
1.8
2
0
0.2
0.4
0.6
0.8
1
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
a1
a2
Distortion cause by noise / (d 2
11.d21)
Figure 9.4 3D figure of DCN (9.27) w.r.t. a1 and a2 for high levels of noise (SNR = 0 dB).

Noise Reduction Property of Type-2 Fuzzy Neural Networks
171
0.15
0.15
0.15
0.2
0.2
0.2
0.25
0.25
0.25
0.3
0.3
0.3
0.35
0.35
0.35
0.4
0.4
0.4
0.45
0.45
a1
a2
1
1.2
1.4
1.6
1.8
2
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 9.5 Contour figure of DCN (9.27) w.r.t. a1 and a2 (SNR = 0 dB).
1
1.2
1.4
1.6
1.8
2
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
1.2
Ã— 10â€“5
a2
a1
Distortion caused by noise /(d 2
11.d21)
Figure 9.6 The 3D figure of distortion caused by noise (9.27) w.r.t. a1 and a2 for low
levels of noise (SNR = 100 dB).

172
Fuzzy Neural Networks for Real Time Control Applications
Based on similar arguments, an appropriate selection for a2 and a1 would
therefore be a2 > 0.5 and a1 < 2.
Similar to Fig. 9.3, the numerical solution of the integral for a low
level of noise (SNR = 100 dB) is shown in Fig. 9.6. This figure indicates
that the noise reduction property of T1FLSs is comparable with T2FLSs in
the presence of very low levels of noise. Therefore, in the presence of low
levels of noise, T1FLS is preferable when compared to T2FLS because of
its computational simplicity.
9.3 CONCLUSION
A novel type-2 fuzzy MF, elliptic type-2 MF, that has certain values on both
ends of the support and the kernel and some uncertain values on other
points of the support, is elaborated. With such a function, the parameters
responsible for the width of uncertainty are de-coupled from the parameters
responsible for the center and support of the MF. This allows us to analyze
how the uncertainty in the input distorts the inference of the T2FLS.
REFERENCES
[1] C.-F. Juang, C.-H. Hsu, Reinforcement interval type-2 fuzzy controller design by online
rule generation and Q-value-aided ant colony optimization, IEEE Trans. Syst. Man
Cybernet. B Cybernet. 39 (2009) 1528-1542.
[2] R. Sepulveda, P. Melin, A. Rodriguez, A. Mancilla, O. Montiel, Analyzing the effects
of the Footprint of Uncertainty in Type-2 Fuzzy Logic Controllers, Eng. Lett. 13 (2006)
138-147.
[3] J.M. Mendel, Uncertainty, fuzzy logic, and signal processing, Signal Process. 80 (2000)
913-933.
[4] M. Khanesar, E. Kayacan, M. Teshnehlab, O. Kaynak, Analysis of the noise reduction
property of type-2 fuzzy logic systems using a novel type-2 membership function, IEEE
Trans. Syst. Man Cybernet. B Cybernet. 41 (5) (2011) 1395-1406.
[5] J.M. Mendel, R.I.B. John, Type-2 fuzzy sets made simple, IEEE Trans. Fuzzy Syst. 10
(2002) 117-127.
[6] M. Begian, W. Melek, J. Mendel, Stability analysis of type-2 fuzzy systems, in:
FUZZ-IEEE 2008 IEEE World Congress on Computational Intelligence, London, UK,
2008, pp. 947-953.
[7] J.M. Mendel, General Type-2 Fuzzy Logic Systems Made Simple: A Tutorial, in: Fuzzy
Systems, IEEE Transactions on 22 (5) (2014) 1162-1182.

CHAPTER 10
Case Studies: Identification
Examples
Contents
10.1 Identification of Mackey-Glass Time Series
173
10.2 Identification of Second-Order Nonlinear Time-Varying Plant
174
10.3 Analysis and Discussion
179
10.4 Conclusion
182
References
183
Abstract
In this chapter, the learning algorithms proposed in the previous chapters (GD-based,
SMC theory-based, EKF and hybrid PSO-based learning algorithms) are used to iden-
tify and predict two nonlinear systems, namely Mackey-Glass and a second-order
nonlinear time-varying plant. Several comparisons are made, and it has been shown
that the proposed SMC theory-based algorithm has faster convergence than existing
methods such as GD-based and swarm intelligence-based methods. Moreover, the
proposed learning algorithm has an explicit form, and it is easier to implement than
other existing methods. However, for offline algorithms for which computation time
is not an issue, the hybrid training method based on PSO and SMC theory may be a
preferable choice.
Keywords
Simulations, Mackey-Glass, Nonlinear time-varying system identification
10.1 IDENTIFICATION OF MACKEY-GLASS TIME SERIES
The first case study is the identification of a chaotic time series, namely the
Mackey-Glass time series. This chaotic system is a well-known benchmark
problem in literature described by the following dynamic equation [1]:
Fuzzy Neural Networks for Real Time Control Applications
Copyright Â© 2016 Elsevier Inc.
http://dx.doi.org/10.1016/B978-0-12-802687-8.00010-4
All rights reserved.
173

174
Fuzzy Neural Networks for Real Time Control Applications
Ë™x(t) = 0.2
x(t âˆ’Ï„)
1 + x10(t âˆ’Ï„) âˆ’0.1x(t)
(10.1)
The numerical values selected for the chaotic system above are Ï„ = 17,
x(0) = 1.2 in this case study. The first 118 data are eliminated. The predictor
goal is to predict x(t +6) using the inputs x(t âˆ’18), x(t âˆ’12), x(t âˆ’6) and
x(t). For each input, two type-2 fuzzy Gaussian MFs with uncertain Ïƒ values
are used. For the consequent part parameters, constant values are used. The
number of rules in the system are therefore equal to 16. The number of
training data is selected as 1000, and the number of test data is 200.
Figure 10.1(a) shows the RMSE versus the epoch number of SMC-
theory based training algorithm, which indicates stable learning. As can be
seen from this figure, the T2FNN gives accurate modeling results. In Fig.
10.1(b), the evolution of the adaptive learning rate is presented. Thanks to
the stable adaptation law for this parameter, the learning rate converges to
an appropriate value and there is no need for any trial and error stage for the
selection of the learning rate. In Fig. 10.1(c), the correlation between the
true output of the system and its estimated value is presented, which shows
a satisfactory result. The histogram of the training error is depicted in Fig.
10.1(d). As can be seen from this figure, the probability density function of
the error is almost Gaussian, which is highly desirable.
The results of applying different training algorithms for the prediction
of Mackey-Glass are illustrated in Fig. 10.2(a). Moreover, the training error
of different training algorithms as applied to the prediction of Mackey-Glass
are presented in Fig. 10.2(b).
10.2 IDENTIFICATION OF SECOND-ORDER NONLINEAR
TIME-VARYING PLANT
In the second case study, the proposed identification procedure is applied to
a second-order nonlinear time-varying plant [2] described by the following
equation:
y(k) = x1(k)x2(k) + x3(k)
x4(k)
(10.2)
where x1 = y(k âˆ’1)y(k âˆ’2)y(k âˆ’3)u(k âˆ’1), x2 = y(k âˆ’3) âˆ’b(k),
x3 = c(k)u(k), and x4 = a(k) + y(k âˆ’2)2 + y(k âˆ’3)2.
The time-varying parameters a, b and c in (10.2) are given by the
following equations:

Case Studies: Identification Examples
175
a(k) = 1.2 âˆ’0.2 cos(2Ï€k/T)
b(k) = 1 âˆ’0.4 sin(2Ï€k/T)
c(k) = 1 + 0.4 sin(2Ï€k/T)
(10.3)
where T = 1000 is the time span of the test.
0
20
40
60
80
100
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
RMSE
Epoch
(a)
(b)
0
50
100
150
200
0
1
2
3
4
5
6
7  10â€“3
Learning rate alpha
Epoch
Figure10.1 (a)RMSEoftrainingversus epochs;(b)theevolutionof theadaptivelearning
rate versus the epoch number;
(Continued)

176
Fuzzy Neural Networks for Real Time Control Applications
(c)
0
0.2
0.4
0.6
0.8
1
âˆ’0.2
0
0.2
0.4
0.6
0.8
1
1.2
True data
Estimated data
(d)
âˆ’0.06 âˆ’0.04 âˆ’0.02
0
0.02
0.04
0.06
0
50
100
150
200
Number of samples in the interval
Figure 10.1, contâ€™d (c) the correlation analysis between the desired and the estimated
values of T2FNN obtained by using SMC theory-based training algorithm; (d) the his-
togram of error obtained by using SMC theory-based learning algorithm.
Figure 10.3(a) shows the RMSE values versus epoch number, which
indicates stable learning with the proposed learning algorithm. Thanks to
the fully SMC theory-based parameter update rules as discussed in the
previous chapters, the learning rate is also adaptive. The evolution of this
parameter is illustrated in Fig. 10.3(b), and it is observed that the true value

Case Studies: Identification Examples
177
0
200
400
600
800
1000
0
0.5
1
1.5
Sample
(a)
True data
GD
EKF
PSO+SMC
SMC
(b)
0
200
400
600
800
1000
âˆ’0.1
âˆ’0.05
0
0.05
0.1
0.15
Sample
GD
EKF
PSO+SMC
SMC
Figure 10.2 (a) The results obtained by using different training methods; (b) the error of
training obtained by different training algorithms.
of the learning parameter is found automatically by the learning algorithm
without any prior knowledge. Figure 10.3(c) shows the correlation between
the true output of the system and its estimated value. Although there are
some outliers in this figure, the overall performance is satisfactory. The
histogram of the training error is depicted in Fig. 10.1(d). As can be seen

178
Fuzzy Neural Networks for Real Time Control Applications
0
50
100
150
200
0
0.1
0.2
0.3
0.4
RMSE
Epoch
(a)
(b)
0
50
100
150
200
0
1
2
3
4
5
6
7  10âˆ’3
Learning rate alpha
Epoch
Figure10.3 (a)RMSE oftrainingversus epochs;(b)theevolutionof theadaptivelearning
rate versus the epoch number;
from this figure, the probability density function of error is close to Gaussian,
which is highly desirable.
The identification results and training error of different training algo-
rithms to a time-varying nonlinear system are illustrated in Fig. 10.4(a)
and (b), respectively.

Case Studies: Identification Examples
179
(c)
0
0.2
0.4
0.6
0.8
1
âˆ’0.2
0
0.2
0.4
0.6
0.8
1
1.2
True data
Estimated data
(d)
âˆ’0.06 âˆ’0.04 âˆ’0.02
0
0.02
0.04
0.06
0
50
100
150
200
Number of samples in the interval
Figure 10.3, contâ€™d (c) the correlation analysis between the desired and the estimated
values of T2FNN obtained by using SMC theory-based training algorithm; (d) the his-
togram of error obtained by using SMC theory-based learning algorithm.
10.3 ANALYSIS AND DISCUSSION
In order to have a numerical comparison between the RMSE values of
different algorithms and their computation time, Tables 10.1 and 10.2

180
Fuzzy Neural Networks for Real Time Control Applications
0
200
400
600
800
0
0.5
1
1.5
Sample
(a)
True data
GD
EKF
PSO+SMC
SMC
(b)
0
200
400
600
800
1000
âˆ’0.4
âˆ’0.2
0
0.2
0.4
0.6
0.8
1
Sample
GD
EKF
PSO+SMC
SMC
Figure 10.4 (a) Results obtained by using different training methods; (b) the error of
training obtained by different training algorithms.
are presented. As can be seen from these two tables, the identification
performances of the hybrid training method based on PSO and SMC
theory are the best. However, since this algorithm needs different solutions
to be run, it is the slowest. EKF and the proposed SMC theory-based
learning algorithm are similar to each other, and they seem to be the best

Case Studies: Identification Examples
181
Table 10.1 Comparison of different learning techniques
Performance
Training
Testing
Computation time (s)
GD
0.0291
0.0250
55.79
EKF
0.0360
0.0190
61.4206
SMC
0.0224
0.0246
52.65
PSO+SMC
0.0187
0.0208
3039.25
Table 10.2 Comparison of different learning techniques
Performance
Training
Testing
Computation time (s)
GD
0.0540
0.0613
124.1194
EKF
0.0275
0.0261
229.7074
SMC
0.0360
0.0390
84.3878
PSO+SMC
0.0199
0.0390
7086.78
when compared to other techniques. However, the computation time of
the proposed SMC theory-based learning algorithm is significantly lower
than the other methods. Moreover, the implementation of EKF requires
the update of high-dimensional matrices and it is computationally very
demanding. This conclusion results in the fact that although SMC theory-
based learning algorithm may result in slight loss of performance, it is more
practical in real-time applications.
As has been already mentioned, the reason for having the largest
computation time for the EKF is that this algorithm includes the ma-
nipulation of high-dimensional matrices. A large amount of memory is
used by these high-dimensional matrices, which makes the algorithm
difficult to implement in most real-time applications. On the other hand,
there are no matrix manipulations in the proposed SMC theory-based
rules. Moreover, GD, LM, EKF and other gradient-based training methods
include the calculations of the partial derivatives of the output with
respect to the parameters, which is very difficult, and do not have any
explicit form. The divergence issues are other problems of gradient-based
training algorithms, which are solved by the SMC theory-based learning
algorithm.
According to our observations, the overall most accurate algorithm
regardless of the computation time, is the hybrid training method based
on PSO and SMC theory. The basic idea behind this algorithm is that

182
Fuzzy Neural Networks for Real Time Control Applications
the parts that appear nonlinearly in the output can be trained by using
HCPSO, while the parameters that appear linearly in the output can be
trained using the SMC theory-based learning algorithm. This is because
PSO benefits from a random optimization technique, and the use of it for
the parameters of the consequent part reduces the speed of convergence
considerably. However, the computational backbone of SMC theory-based
learning algorithms makes them more logical than random optimization
methods. This method is also implemented on a A2-C0 fuzzy model as
was discussed in previous chapters. The results are summarized in Tables
10.1 and 10.2. As can be seen from these tables, as an offline training
method, the hybrid training method based on PSO and SMC theory
may be preferable. However, its computational time is higher than other
training methods. The reason behind this computational burden is that
the hybrid training method based on PSO and SMC theory necessitates
too much feed-forward computation of T2FNN, which is a complex and
time-consuming task.
10.4 CONCLUSION
The simulation results indicate the potential of the proposed SMC theory-
based structure in real-time systems since the computation time of the
proposed algorithm is significantly lower than that of the other methods,
while keeping its high identification accuracy. Note that these parameter
update rules can also be used for control purposes in which the computation
time is prominent. One issue that should be taken into account is that
the adaptation laws proposed in this chapter are continuous. However, for
the simulation of the method in a computer, an optimal sampling time
should be chosen. The choice of optimal sampling time may be a problem,
because a very large value for the sampling time may cause instability in the
system.
Moreover, for offline cases, where the computational time and mem-
ory consumption is not an issue, the hybrid training algorithm based
on PSO and SMC theory is a preferable choice. The use of PSO as
the optimizer for the parameters that appear nonlinearly in the output
of T2FNN makes it possible to lessen the possibility of entrapment of
the algorithm in a local minima and improves the performance of the
results.

Case Studies: Identification Examples
183
REFERENCES
[1] M.C. Mackey, L. Glass, et al., Oscillation and chaos in physiological control systems,
Science 197 (4300) (1977) 287-289.
[2] R.H. Abiyev, O. Kaynak, Type 2 fuzzy neural structure for identification and control of
time-varying plants, IEEE Trans. Indust. Electron., 57 (12) (2010) 4147-4159.

CHAPTER 11
Case Studies: Control Examples
Contents
11.1 Control of Bispectral Index of a Patient During Anesthesia
186
11.1.1 Realistic Patient Model
187
11.1.2 Simulation Results
189
11.2 Control of Magnetic Rigid Spacecraft
193
11.2.1 Dynamic Model of a Magnetic Satellite
195
11.2.2 Simulation Results
197
11.3 Control of Autonomous Tractor
203
11.3.1 Mathematical Description of Tractor
205
11.3.1.1 Kinematic Model
205
11.3.1.2 Yaw Dynamics Model
207
11.3.2 Overall Control Scheme
207
11.3.2.1 Kinematic Controller
207
11.3.2.2 Dynamic Controllers
209
11.3.3 Experimental Results
211
11.4 Conclusion
214
References
215
Abstract
In this chapter, three real-world control problems, namely anesthesia, magnetic rigid
spacecraft and tractor-implement system are studied by using SMC theory-based
learning algorithms for T2FNNs. For all the systems, the FEL scheme is preferred in
which a conventional controller (PD, etc.) works in parallel with an intelligent structure
(T1FNN, T2FNN, etc.). The proposed learning algorithms have been shown to be able
to control these real-world example problems with satisfactory performance. Note that
the proposed control algorithms do not need a priori knowledge of the system to be
controlled.
Keywords
Anesthesia, Elliptic membership function, Tractor and implement, Bispectral index
Fuzzy Neural Networks for Real Time Control Applications
Copyright Â© 2016 Elsevier Inc.
http://dx.doi.org/10.1016/B978-0-12-802687-8.00011-6
All rights reserved.
185

186
Fuzzy Neural Networks for Real Time Control Applications
11.1 CONTROL OF BISPECTRAL INDEX OF A PATIENT DURING
ANESTHESIA
Over the last few decades, control engineering has widely influenced
modern medicine. One of the most influenced fields of modern medicine
is pharmacology. Closed-loop insulin delivery [1], closed-loop control of
arterial pressure by infusion of sodium nitroprusside [2] and many more
can be listed as the examples of the applications of control engineering
in pharmacology. Closed-loop drug delivery for anesthesia [3, 4] is also
one of the most important applications of control engineering in medicine.
The first step to control the depth of anesthesia is to measure the depth of
anesthesia. There are different methods to monitor the depth of anesthesia
during surgery. One of the most well-known methods to monitor depth
of anesthesia is called BIS. The use of different control methodologies in
anesthesia makes it possible to use less volume of drug while maintaining the
BIS of patient in an appropriate level for performing the surgery. Using less
volume of drug means less side effects and less recovery time after surgery,
which reduces the cost of the surgery.
The dynamical model of a patient is naturally uncertain since it depends
on different parameters, e.g., gender, age, weight, height, and heart rate.
For such an uncertain system, the use of model-free approaches, i.e.,
fuzzy systems and neural networks, is more preferable than model-based
approaches. In this chapter, among the model-free approaches, T2FNNs
are preferred to cope with high levels of uncertainties. Such a selection is
more promising to control the BIS of a patient during anesthesia in which
the model is highly uncertain and differs from one patient to another, and
may even vary during the surgery for a single patient.
In this case study, the proposed novel FEL scheme is used to control
BIS during anesthesia in which a T2FNN works in parallel with a PD
controller (see Fig. 11.1). This control strategy has already been studied in
Chapter 7. The T2FNN has two inputs: the error and the time derivative
of the error. The MFs considered for the system are Gaussian type-2 MFs
with uncertain variance. The SMC theory-based parameter update rules are
derived for such a structure, and the stability of the learning algorithm was
already considered in Chapter 7.
T2FLCs were previously applied to control BIS during anesthesia in Ref.
[5]. However, in this study the T2FLC does not include any adaptation and
hence this study is the first study in which BIS is controlled using T2FLS
that results in an adaptive control scheme. Moreover, since the proposed

Case Studies: Control Examples
187
+
âˆ’
T2FNN
constructed by
Gaussian MFs
with uncertain
variance with
adaptive learning
rate
BIS
u
PD controller
Desired
BIS
interval
d/dt
+
âˆ’
uc
uf
e
e
Î£
Figure 11.1 Block diagram of the proposed T2FNN scheme.
method uses type-2 MFs, it is expected that the system outperforms its
type-1 counterpart especially when measurement noise is significant in
the system. The simulation results show that the proposed approach can
control BIS level during anesthesia in the presence of measurement noise
and uncertainties that necessarily exist in the model of the patient. The
performance of the T2FNN is also compared with that of its type-1
counterpart in the presence of noise. It is also shown that the states of the
system follow the predefined sliding motion.
11.1.1 Realistic Patient Model
There exist several models that describe the dynamics of the patient when a
specific drug (in this case Propofol) is injected. This relationship is described
by pharmacokinetic and pharmacodynamic models. Pharmacokinetic mod-
els describe the distribution of drugs in the body and pharmacodynamic
models represent the relationship between blood concentration of a drug
and its clinical effects. The pharmacokinetics can be illustrated using a three-
compartment model as follows [4, 6]:
Ë™x1(t) = âˆ’(k10 + k12 + k13)x1(t) + k21x2(t) + k31x3(t)
Ë™x2(t) = k12x1(t) âˆ’k21x2(t)
Ë™x3(t) = k13x1(t) âˆ’k31x3(t)
(11.1)
where x1(t) denotes the amount of drug in the central compartment. The
peripheral compartments number two and number three model the drug
exchange of the blood with well and poorly perfused body tissues. The

188
Fuzzy Neural Networks for Real Time Control Applications
amount of the blood in these compartments is represented by x2(t) and
x3(t), respectively. The constants kij represent the transfer rate of the drug
from the jth compartment to the ith compartment. The constant k10 is the
rate of drug metabolism and u(t) is the infusion rate of the Propofol into
the blood. Other constants for the Schider model [6] for Propofol are as
follows:
V1 = 4.27[l], V2 = 18.9 âˆ’0.391(age âˆ’53)[l],
Cl1 = 1.89+0.0456(weightâˆ’77)âˆ’0.0681(lbm âˆ’59)+0.0264(heightâˆ’177)
Cl2 = 1.29 âˆ’0.024(age âˆ’53), Cl3 = 0.836, k10 = Cl1
V1
, k12 = Cl2
V1
,
k13 = Cl3
V1
, k21 = C12
V2
, k31 = Cl3
V3
(11.2)
The lbm depends on gender: for a male person we have:
lbm = 1.1 weight âˆ’128weight2
height2
(11.3)
while for a female person we have:
lbm = 1.07 weight âˆ’148weight2
height2
(11.4)
The pharmacodynamics is characterized by a first-order differential
equation as follows:
Ë™Ce(t) = âˆ’0.456Ce(t) + 0.456Cp(t)
(11.5)
in which Cp is the concentration of the drug in the blood and is
calculated as:
Cp = x1
V1
(11.6)
and Ce is the concentration of the drug in the effect compartment. In this
chapter, the depth of anesthesia is monitored using BIS. The BIS can have
any value from 0 to 100. If the BIS is equal to zero, the patient does not have
any cerebral activity. On the other hand, if it is equal to 100, the patient is

Case Studies: Control Examples
189
fully awake. BIS and Ce are related to each other by the following static
equation:
BIS(t) = E0 âˆ’Emax
CÎ³
e (t)
CÎ³
e (t) + CÎ³
50
(11.7)
in which E0 is the BIS for the awake state and is typically taken equal to
100. Emax denotes the maximum effect of the drug, and C50 and Î³ are the
drug concentration at half-maximal effect and the steepness of the curve,
which vary from one patient to another. Also note that C50 depends on the
heart rate and varies during surgery. The following equation illustrates the
relation between C50 and HR [7]:
C50 = C0 log(HR)
(11.8)
in which C0 is a constant value. This fact makes the patient model an
uncertain nonlinear dynamic system.
11.1.2 Simulation Results
The parameters of the three compartment models considered in this chapter
vary from one patient to another, or may be time-varying for a single patient.
These values for 12 different patients are given in Table 11.1.
The state of a patient during surgery includes three phases: induction,
maintenance and recovery. At the beginning of the induction phase, the
injection of Propofol starts and the patient loses consciousness completely.
Table 11.1 Characteristic variables for each of the 12 patients used in this study
Patient
Age
Length
Weight
Gender
C50
E0
Emax
Î³
1
40
163
54
F
6.33
98.80
94.10
2.24
2
36
163
50
F
6.76
98.60
86.00
4.29
3
28
164
52
F
8.44
91.20
80.70
4.10
4
50
163
83
F
6.44
95.90
102.00
2.18
5
28
164
60
M
4.93
94.70
85.30
2.46
6
43
163
59
F
12.10
90.20
147.00
2.42
7
37
187
75
M
8.02
92.00
104.00
2.10
8
38
174
80
F
6.56
95.50
76.40
4.12
9
41
170
70
F
6.15
89.20
63.80
6.89
10
37
167
58
F
13.70
83.10
151.00
1.65
11
42
179
78
M
4.82
91.80
77.90
1.85
12
34
172
58
F
4.95
96.20
90.80
1.84

190
Fuzzy Neural Networks for Real Time Control Applications
In the maintenance phase, in which the surgery is performed, the adminis-
tration of Propofol continues and the patient is kept unconscious. Finally, at
the end of the surgery, the injection of Propofol is stopped and some other
drugs may be used to awaken the patient. In this chapter, only the first two
stages are considered. The initial values of the parameters of the controller
for different patients are considered to be the same. The MFs of the fuzzy
controller are uniformly distributed over the support set and the parameters
of the consequent part are taken to be equal to zero. As can be seen in
Fig. 11.2, the desired operating region for a typical patient during a surgery
with automated anesthesia is in the range of [40, 60]. This range is suggested
by the manufacturers of BIS monitoring systems. It is further observed from
Fig. 11.2 that any BIS level higher than 70 corresponds to light sedation
and may cause the patient to unexpectedly wake up or experience pain.
Moreover, any BIS level less than 30 is called deep anesthesia and should
be prevented during surgery. BIS level is observed and controlled by an
anesthetist during surgery. It has been shown that the mortality rate of the
older people is 16.7% for an average BIS value less than 40 as compared to
4.2% for an average BIS less than 60. This study implies that keeping middle-
aged and older patients in a deep hypnotic stage increases the possibility of
mortality [8]. In this study, any BIS level less than 40 is considered to be
Awake
Light/moderate
sedation
Deep sedation
General anesthesia
Deep hypnosis
Increasing burst
suppression
Isoelectric EEG
Deep
hypnotic
state
Moderate
hypnotic
state
Light
hypnotic
state
BIS
Low probability of
explicit recall
Memory
intact
100
80
60
40
20
0
Low probability of
consciousness
Memory function
lost
Figure 11.2 Meaning of different levels of BIS [9].

Case Studies: Control Examples
191
undesirable and is prevented during simulations. Furthermore, since it is
impossible to eliminate the drug from the patientâ€™s body, the control signal
cannot be negative. The injection rate of Propofol to the body of the patient
also has an upper limit that is defined by the maximum injection rate of
the automatic pump used during surgery and the maximum permissible
injection rate that do not harm the patient. In this case study, the maximum
value of the injection rate is considered to be 3.2 mg/s as in ref. [4].
The settling time during the induction phase is defined as the time it
takes for the BIS of a patient to reach the interval of [45, 55] and to remain
in this interval. The results obtained by applying an MPC to 12 patients is
considered in a paper, which shows that the settling time varies from 90 sec
to 190 sec and the BIS level of patient no. 10 falls down 45 during induction
phase [4]. Moreover, the settling times for the condition when EPSC is
used vary from 75 sec to 200 sec [4]. Furthermore, the results of applying
EPSC show that the BIS of patients no. 5 and no. 12 fall down to 45 during
induction, which is undesirable.
The results of the proposed controller for the 12 patients during the
induction phase are shown in Fig. 11.3. As can be seen from Fig. 11.3, the
settling times of the proposed algorithm vary from 240 sec to 742 sec and
are longer than those of MPC and EPSC. However, the BIS does not fall
to 45 for any patient, which is highly desirable. Another advantage of the
proposed method over MPC and EPSC is that the control signals (the rates
of injection of Propofol) do not meet their saturation value for any patients
when applying MPC or EPSC (see Fig. 11.3). In summary, if the proposed
method is used for the control of anesthesia, although it takes approximately
30 s to 11 min more time for a patient to become ready for surgery, the
depth of the patientâ€™s anesthesia never becomes deeper than needed during
the induction phase. In other words, by the use of the proposed method, the
patient changes to the moderate hypnotic state slowly but more safely, and
the maximum rate of drug injection to a patient never hits its upper bound.
The response of the proposed controller in the maintenance phase is
shown in Figs. 11.4 and 11.5. In order to investigate the performance of the
controller under more realistic conditions, it is assumed that the heart rate
varies considerably every 5 min. As mentioned earlier, the variations in heart
rate influence the parameter C50. Thus, it is assumed that this parameter
varies to a new value within 90% to 110% of its initial value every 5 min.
From a control engineer point of view, the system would be an uncertain
system considering these variations. As can be seen, the BIS level falls to 40
for a few seconds and is almost always within the desired range of [40, 60].

192
Fuzzy Neural Networks for Real Time Control Applications
0
(a)
500
1000
1500
2000
40
50
60
70
80
90
100
Time (s)
BIS
0
(b)
500
1000
1500
2000
0
0.2
0.4
0.6
0.8
Time (s)
Propofol rate (mg/s)
Figure 11.3 (a) Closed-loop response of first 2000 s of the output signal, BIS, during
surgeryfordifferent patients;(b)closed-loop responseoffirst 2000 s ofthecontrol signal,
the rate of the injection of Propofol, during surgery for different patients.
It is further observed from the figures that the control signal during the
maintenance phase never reaches its upper bound and hence it is a safe
injection. Generally speaking, Propofol may cause some side effects and its
injection rate should be kept as low as possible.

Case Studies: Control Examples
193
0
2000
4000
6000
8000
10000
0
0.05
0.1
0.15
0.2
0.25
Time (s)
(a)
Propofol rate (mg/s)
0
2000
4000
6000
8000
10000
40
50
60
70
80
90
100
Time (s)
(b)
BIS
Figure 11.4 (a) Closed-loop response of the output variable, BIS, during surgery for
one patient; (b) closed-loop response of the control signal, the rate of the injection of
Propofol, during surgery for one patients.
11.2 CONTROL OF MAGNETIC RIGID SPACECRAFT
The dynamic behavior of a magnetic rigid spacecraft is known to be chaotic
[10, 11], and its control has been considered in a number of works. The
input-output feedback linearization approach is one of the methods that has
been successfully applied to the control of a magnetic rigid spacecraft [12].

194
Fuzzy Neural Networks for Real Time Control Applications
0
2000
4000
6000
8000
10000
30
40
50
60
70
80
90
100
Time (s)
(a)
(b)
BIS
0
2000
4000
6000
8000
10000
0
0.2
0.4
0.6
0.8
Time (s)
Propofol rate (mg/s)
Figure 11.5 (a) Closed-loop response of the output signal, BIS, during surgery for
different patients; (b) closed-loop response of the control signal, the rate of the injection
of Propofol, during surgery for different patient.
In order to control the chaotic attitude motion of the spacecraft, a FEL
control scheme, a T2FNN working in parallel with a PD controller, is used
[19]. The T2FNN structure benefits from elliptic type-2 fuzzy MFs. The
SMC theory-based parameter adaptation rules, proposed in Chapter 7, are
used to tune the parameters of elliptic type-2 fuzzy MFs and the parameters
of the consequent part.

Case Studies: Control Examples
195
11.2.1 Dynamic Model of a Magnetic Satellite
The magnetic rigid spacecraft is supposed to move in a circular orbit with
the orbital angular velocity Ï‰c in the gravitational and the magnetic fields
of the Earth. It is assumed that the inertial reference frame (Oe âˆ’X0Y0Z0)
has the origin Oe at the mass center of the Earth with the polar axis of the
Earth as Z0-axis and the line from Oe to the ascending node as X0-axis.
Let O âˆ’XYZ denote the orbital reference frame such that X is in the anti-
nadir direction and Z is in the direction of the normal vector of the orbital
plane XY. Let x1 be the libration angle that represents the deviation of the
spacecraft-fixed x and y axes from the orbital axes X and Y, respectively,
the dynamics of the magnetic rigid spacecraft can then be expressed as (see,
e.g., [10, 12]):
Ë™x1 = x2
Ë™x2 = âˆ’K sin(2x1) âˆ’Î³ x2
âˆ’Î±s(2 sin(x1) sin(x3) + cos(x1) cos(x3)) + u(t)
Ë™x3 = 1
(11.9)
in which Î³ is the damping parameter, Î±s is the magnetic parameter, t is the
dimensionless time defined as the product of the orbital angular velocity and
the actual time, and K is defined as:
K = 3(B âˆ’A)
2C
(11.10)
where A, B, and C are the principal inertia moments of the arbitrarily
shaped spacecraft. The parameter K is equal to 1.1. Furthermore, the
control signal u is u =
Mc
CÏ‰2c , where Mc is the control torque. The dynamic
behavior of the magnetic rigid spacecraft is known to show chaotic motion
and its phase plane is highly sensitive to the values of Î³ and Î±s and the
initial conditions of the system. Figure 11.6 shows the phase plane of the
zero-input system (u = 0) with different values for its parameter and
initial conditions for the system. The system is simulated for 20 000 sec
and the results are shown in Fig. 11.6, which shows that slight changes
in the parameters of the spacecraft can greatly influence the trajectories of
the system and the shape of the chaotic attractor.

196
Fuzzy Neural Networks for Real Time Control Applications
âˆ’0.5
(a)
0
0.5
âˆ’0.5
0
0.5
1
Phase plane
x1
x2
âˆ’5
(b)
0
5
10
15
20
25
30
âˆ’2
âˆ’1
0
1
2
Phase plane
x1
x2
Figure 11.6 Phase portrait of magnetic rigid satellite with different values for the pa-
rameters and different initial states of the system. (a) The phase portrait with Î±s = 0.69,
K = 1.1, Î³ = 0.2, x1 = âˆ’0.5, x2 = âˆ’0.5; (b) Î±s = 0.7, K = 1.1, Î³ = 0.28, x1 = âˆ’0.5,
x2 = 0.2;
(Continued)

Case Studies: Control Examples
197
0
2
4
6
8
âˆ’1.5
(c)
âˆ’1
âˆ’0.5
0
0.5
1
1.5
2
2.5
Phase plane
x1
x2
0
(d)
5
10
âˆ’2
âˆ’1
0
1
2
Phase plane
x1
x2
Figure 11.6, contâ€™d (c) Î±s = 0.7, K = 1.1, Î³ = 0.285, x1 = âˆ’0.5, x2 = 0.25; (d) Î±s = 0.7,
K = 1.1, Î³ = 0.297, x1 = 0.4, x2 = âˆ’0.75.
11.2.2 Simulation Results
The proposed SMC-based learning algorithm for T2FNN is simulated
on the attitude control of a magnetic satellite. The sample time for the
simulation is selected as 0.001 sec. Furthermore, in order to make the

198
Fuzzy Neural Networks for Real Time Control Applications
0
5
10
(a)
15
20
âˆ’0.5
âˆ’0.4
âˆ’0.3
âˆ’0.2
âˆ’0.1
0
0.1
0.2
x1
t
The response
of the proposed control system
The response
of the PD controller when
it is used alone
0
(b)
5
10
15
20
0
0.5
1
1.5
2
t
The adaptation of the learning rate (a)
Figure 11.7 The regulation response of the magnetic satellite: (a) the libration angle in
the orbital plane x1(t); (b) the evolution of the adaptive learning rate Î±;
(Continued)
spacecraft behave chaotically its parameters are selected as Î±s and Î³ , equal
to 0.6984 and 0.2, respectively.
Figure 11.7 compares the regulation performance of the proposed FEL
control structure and a PD controller working alone. The gains of the PD

Case Studies: Control Examples
199
0
5
10
15
20
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
t
(c)
Robustness term (Kr)
0
(d)
5
10
15
20
âˆ’1
0
1
2
3
4
t
The control signals
The output of the PD controller
The output of the T2FLC
The overall control signal
Figure 11.7, contâ€™d (c) the evolution of the robustness term Kr; (d) the control signals.
controller, KP, and KD are selected to be equal to 9 and 5, respectively. The
initial conditions considered for the system are selected to be x1 = âˆ’0.5
and x2 = 0.2. As can be seen from Fig. 11.7, although the PD controller
ensures the error signal is bounded in the neighborhood of zero, it cannot
eliminate it. The interesting result is that although the system gives a steady-

200
Fuzzy Neural Networks for Real Time Control Applications
state error when only a PD controller is used, the fusion of the PD controller
with a T2FNN eliminates the steady-state error. Moreover, the evolution
of the robustness parameter Kr is shown in Fig. 11.7(c). As can be seen
from Fig. 11.7, the initial value of Kr is selected to be equal to zero, and
adaptation law as in Chapter 7 is used to find the optimal value of this
parameter. In addition, Fig. 11.7(c) shows that the value of the parameter
becomes as large as needed to ensure the robustness of the system. The
evolution of the adaptive learning rate Î± is shown in Fig. 11.7(b). The
adaptation law for the learning rate Î± makes it possible to control the system
without any a priori knowledge about the upper bound of the states of the
system. Figure 11.7(d) shows the overall control signal (Ï„), the output of
T2FNN (Ï„f ), and the output of the conventional PD controller (Ï„c). As
can be seen from the figure, at the beginning of the simulation, the overall
control signal is mostly due to the conventional PD controller. After finite
time, T2FNN learns the dynamics of the system and takes responsibility
for the system. Simultaneously, the output of the PD controller tends to go
to zero.
Figure 11.8 compares the tracking performance of the proposed control
approach and a PD controller when it is used alone. Furthermore, it can be
seen from this figure that the PD controller makes the error signal bounded
in a neighborhood near zero but it cannot make it zero. Figure 11.8(b) shows
the evolution of the learning rate (Î±) over time. The use of the adaptation
law for the learning rate makes it possible to adjust it during training, and
no more trial and error is needed to tune this parameter. The evolution of
the parameter Kr is depicted in Fig. 11.8(c). The initial value of Kr is zero,
and the adaptation law as in Chapter 7 is used to tune this parameter. Figure
11.8(d) shows the overall control signal (Ï„), the output of T2FNN (Ï„f ), and
the output of the conventional PD controller (Ï„c).
Figure 11.9(a) compares the tracking performance of the proposed
algorithm with that of the PD controller when the reference signal is
sinusoidal. As can be seen from Fig. 11.9, the proposed FEL structure also
outperforms the PD controller for the case of the sinusoidal reference input,
and there is no steady-state error in the system.
T2FLSs have more degrees of freedom to deal with noise and are
a better choice when there is a high level of noise in the system. In
order to compare the performance of T2FNN with its type-1 counterpart
under noisy conditions, uniformly distributed white noise with a standard
deviation of 0.03 is added to the measurement. Figure 11.10 shows the
percentage of improvement of T2FLS over a type-1 counterpart for different

Case Studies: Control Examples
201
0
5
(a)
10
15
20
0
0.2
0.4
0.6
0.8
x1
t
The response of the
proposed control system
The response of PD controller
when it is used alone
The reference signal
t
0
5
(b)
10
15
20
0
0.5
1
1.5
2
2.5
3
The adaptation of the learning rate (a)
Figure 11.8 The tracking response of the magnetic satellite: (a) the libration angle in the
orbital plane x1(t); (b) the evolution of the adaptive learning rate Î±;
(Continued)
initial values of the a1 and a2 parameters of T2FLS. As can be seen from
Fig. 11.10, it is possible to select such initial values for a1 and a2 that the
system is controlled with 40% less integral of squared error than T1FLS. The
figure also shows that an appropriate interval for the selection of the initial

202
Fuzzy Neural Networks for Real Time Control Applications
0
5
(c)
10
15
20
0
0.2
0.4
0.6
0.8
1
t
The adaptation of the robust term (Kr)
0
5
(d)
10
15
20
âˆ’4
âˆ’2
0
2
4
6
8
t
The output of the PD controller
The output of the T2FLC
The overall control signal
Figure 11.8, contâ€™d (c) the evolution of the robustness term Kr; (d) the control signals.
values a1â€™s and a2â€™s is 1.65 < a1 < 1.85 and 0.75 < a2 < 1. These findings
comply with those of other works seen in the literature [13â€“17], suggesting
that the use of type-2 fuzzy systems adds more degrees of freedom to the
design. The end result is that it is possible to cope with noisy measurements
and uncertainties in the system more effectively.

Case Studies: Control Examples
203
0
5
(a)
10
15
20
âˆ’0.5
0
0.5
1
1.5
x1
t
The response of
the proposed control system
The response of the PD
controller when it is used alone
The sinusoidal reference signal
0
(b)
5
10
15
20
0
0.5
1
1.5
2
t
The adaptation of the learning rate (a)
Figure 11.9 Tracking response of the magnetic satellite when the reference signal
is sinusoidal: (a) the libration angle in the orbital plane x1(t); (b) the evolution of the
adaptive learning rate Î±;
(Continued)
11.3 CONTROL OF AUTONOMOUS TRACTOR
Autonomous navigation of an agricultural vehicle involves the control of
different dynamic subsystems, such as the yaw angle dynamics and the
longitudinal speed dynamics. In this section, a PID controller is used to

204
Fuzzy Neural Networks for Real Time Control Applications
0
5
(c)
10
15
20
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
t
The adaptation of the robustness term (Kr)
0
5
(d)
10
15
20
25
30
âˆ’4
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
t
The control signals
The output of the PD controller
The output of the T2FLC
The overall control signal
Figure 11.9, contâ€™d (c) the evolution of the robustness term Kr; (d) the control signals.
control the longitudinal velocity of the tractor. For the control of the
yaw angle dynamics, a PD controller works in parallel with a T2FNN.
In this way, instead of modeling the interactions between the subsystems
prior to the design of a model-based control, we develop a control
algorithm that learns the interactions online from the measured feedback

Case Studies: Control Examples
205
âˆ’20
âˆ’20
âˆ’20
âˆ’20
0
0
0
0
0
0
20
20
20
20
40
40
40
40
40
6080
x1
x2
1.4
1.5
1.6
1.7
1.8
1.9
2
0.75
0.8
0.85
0.9
0.95
1
Figure 11.10 Percentage of improvement of type-2 fuzzy system over type-1 counter-
part in noisy conditions.
error. In addition to the control of the stated subsystems, a kinematic
controller is needed to correct the errors in both the x and y axis for
the trajectory tracking problem of the tractor. To demonstrate the real-
time abilities of the proposed control scheme, an autonomous tractor is
equipped with the use of reasonably priced sensors and actuators. Exper-
imental results show the efficacy and efficiency of the proposed learning
algorithm.
The motivation behind the use of self-learning controllers instead of
conventional controllers for the control of agricultural production machines
is that there are different subsystems interacting with each other in these
machines, and well tuning of the controller coefficients simultaneously
is a difficult task. Even if the operator becomes proficient in proper
adjustment of the different controller coefficients, crop/animal variability
and environmental conditions force the operator to change the machine
settings continuously.
11.3.1 Mathematical Description of Tractor
11.3.1.1 Kinematic Model
A schematic diagram of the autonomous tractor is presented in Fig. 11.11.

206
Fuzzy Neural Networks for Real Time Control Applications
X
Y
Figure 11.11 Autonomous tractor.
The linear velocities Ë™x, Ë™y and the yaw rate Ë™Ïˆ at the R point are written
as follows [20]:
âŽ¡
âŽ£
Ë™x
Ë™y
Ë™Ïˆ
âŽ¤
âŽ¦=
âŽ¡
âŽ£
u cos Ïˆ
u sin Ïˆ
utanÎ´
L
âŽ¤
âŽ¦
(11.11)
where u, Ïˆ, Î´, and L represent the longitudinal velocity, the yaw angle
defined on the point R on the tractor, the steering angle of the front
wheel, the distance between the front, and the rear axles of the tractor,
respectively.
Considering the center of gravity shown in Fig. 11.11, the linear
velocities Ë™x, Ë™y and the yaw rate Ë™Ïˆ of center of gravity can be written as
follows [20]:
âŽ¡
âŽ£
Ë™x
Ë™y
Ë™Ïˆ
âŽ¤
âŽ¦=
âŽ¡
âŽ£
u cos Ïˆ âˆ’v sin Ïˆ
u sin Ïˆ + v cos Ïˆ
utanÎ´
L
âŽ¤
âŽ¦
(11.12)
where v equals the multiplication of Ë™Ïˆ and lr.

Case Studies: Control Examples
207
11.3.1.2 Yaw Dynamics Model
The velocities and side-slip angles on the rigid body of the tractor are
presented in Fig. 11.12(a). Similarly, the forces on the rigid body of the
tractor are shown in Fig. 11.12(b).
The equations of yaw motion of the autonomous tractor are written in
state-space form as follows [20]:
 Ë™v
Ë™Î³

=
 A11
A12
A21
A22
  v
Î³

+
 B1
B2

Î´(t)
(11.13)
where:
A11 = âˆ’CÎ±,f + CÎ±,r
mu
,
A12 = âˆ’lf CÎ±,f + lrCÎ±,r
mu
âˆ’u,
A21 = âˆ’lf CÎ±,f + lrCÎ±,r
Izu
,
A22 = âˆ’
l2
f CÎ±,f + l2
r CÎ±,r
Izu
,
B1 = CÎ±,f
m
, B2 = lf CÎ±,f
Iz
(11.14)
where m, v, u, Î³ , and Î´ represent the mass of the tractor, the lateral velocity
of the center of gravity, the longitudinal velocity of the center of gravity, the
yaw rate, and the steering angle of the front wheel, respectively. Further-
more, lf , lr and Iz, respectively, represent the distance between the front axle
and the center of gravity of the tractor, the distance between the rear axle,
the center of gravity of the tractor, and the inertial moment of the tractor.
The term CÎ±,i represents the cornering stiffness of the tires of the tractor.
11.3.2 Overall Control Scheme
11.3.2.1 Kinematic Controller
The kinematic model is rewritten in state-space as follows:
âŽ¡
âŽ£
Ë™x
Ë™y
Ë™Ïˆ
âŽ¤
âŽ¦=
âŽ¡
âŽ£
cos Ïˆ
âˆ’lr sin Ïˆ
sin Ïˆ
lr cos Ïˆ
0
1
âŽ¤
âŽ¦
 u
Î³

(11.15)
where lateral velocity v equals Î³ lr.

208
Fuzzy Neural Networks for Real Time Control Applications
X
(a)
Y
(b)
X
Y
Figure 11.12 Dynamic bicycle model for a tractor: (a) velocities and side-slip angles;
(b) forces on the rigid body of the system.

Case Studies: Control Examples
209
An inverse kinematic model is needed to calculate the reference speed
and the yaw rate for the tractor. It is written as follows:
 u
Î³

=

cos Ïˆ
sin Ïˆ
âˆ’1
lr sin Ïˆ
1
lr cos Ïˆ
  Ë™x
Ë™y

(11.16)
The kinematic control law proposed in Ref. [18] to be applied to the
tractor for trajectory tracking control is written as:
 uref
Î³ref

=

cos Ïˆ
sin Ïˆ
âˆ’1
lr sin Ïˆ
1
lr cos Ïˆ
  Ë™xd + ks tanh (keex)
Ë™yd + ks tanh (keey)

(11.17)
where ex = xd âˆ’x and ey = yd âˆ’y are the current position errors in the
axes X and Y, respectively. The parameter ke is the gain of the controller
and ks is the saturation constant. The coordinates (x, y) and (xd, yd) are the
current and the desired coordinates, respectively. The parameters uref and
Î³ref are the generated references for the speed and the yaw rate controllers.
11.3.2.2 Dynamic Controllers
The proposed control scheme used in this study is illustrated in Fig. 11.13.
The arrow in Fig. 11.13 indicates that the output of the PD controller is
used to tune the parameters of the T2FNN. The output of the PD+T2FNN
controller is the steering angle of the front wheel. A low-level controller
is used to control the steering mechanism. A PID controller is used for
the control of longitudinal velocity. For the control of yaw dynamics, a
conventional controller (such as a PD controller) works in parallel with an
intelligent controller. On the other hand, the longitudinal dynamics could,
of course, be selected. Moreover, there could be two T2FNNs running on
the control of the two subsystems (yaw dynamics and longitudinal dynamics)
simultaneously. The reason for such a selection in this study is that the yaw
dynamics control of the tractor is more common in agricultural machines.
Even if a time-based trajectory (both the yaw angle and the longitudinal
speed of the tractor are controlled simultaneously) is given to the system
in this study, a space-based trajectory (the longitudinal speed is fixed and
only the yaw dynamics is controlled) is also very common in agricultural
applications. Based on these concerns, the yaw dynamics of the tractor
have been chosen for the implementation of the novel learning algorithm
proposed here.

210
Fuzzy Neural Networks for Real Time Control Applications
ex
ey
+
+
+
+
+
+
+
+
+
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
Tanh controller
for the x 
direction
Inverse
kinematic
model
T2FNN
PD controller
Yaw motion
dynamics
Global
positioning
system (GPS)
Tractor
dynamics
PID controller
d/dt
gref
g
tn
tc
t
uref
uc
y
x
e
Tanh controller
for the y 
direction
xd
xd
yd
yd
Figure 11.13 Block diagram of the proposed adaptive fuzzy neuro scheme.

Case Studies: Control Examples
211
11.3.3 Experimental Results
An 8-shaped reference trajectory is applied to the system. The reference
and the actual trajectories of the system, both the longitudinal and the
lateral error values on the related trajectory, are shown in Fig. 11.14(a),
x-axis (m)
20
(a)
30
40
50
60
70
80
y-axis (m)
0
20
40
60
80
Reference
PD 1st turn
PD 2nd turn
PD + T2FNN 1st turn
PD + T2FNN 2nd turn
Starting point
Time (s)
0
50
(b)
100
150
200
Error (m)
â€“0.6
â€“0.4
â€“0.2
0
0.2
0.4
0.6
Longitudinal error (PD)
Longitudinal error (PD+T2FNN)
Figure 11.14 (a) The trajectory of the autonomous tractor; (b) the longitudinal error;
(Continued)

212
Fuzzy Neural Networks for Real Time Control Applications
Time (s)
0
50
(c)
100
150
200
Error (m)
â€“1
â€“0.5
0
0.5
1
Lateral error (PD)
Lateral error (PD+T2FNN)
Time (s)
0
50
(d)
100
150
200
Control signals
â€“1.5
â€“1
â€“0.5
0
0.5
1
1.5
tauc
taun
Figure 11.14, contâ€™d (c) the lateral error; (d) the control signals;
(b), and (c) for two different controllers, respectively. The results show that
the control scheme consisting of a T2FNN working in parallel with a PD
controller gives a better trajectory following accuracy than the one where
only a PD controller acts alone. It can be argued that the performance of
the conventional controller acting alone can be improved by better tuning,

Case Studies: Control Examples
213
Time (s)
0
(e)
50
100
150
200
Yaw rate error (m)
â€“0.6
â€“0.4
â€“0.2
0
0.2
0.4
0.6
Yaw rate error (PD)
Yaw rate error (PD + T2FNN)
(f)
Time (s)
0
50
100
150
200
Longitudinal velocity (m/s)
3
3.5
4
4.5
5
Reference velocity
Measured velocity
Figure 11.14, contâ€™d (e) the yaw rate error; (f) actual longitudinal velocity and measured
longitudinal velocity.
but as has already been stated, in real life this is a challenging task because
in addition to the interactions of the subsystems, there exist unmodeled
dynamics and uncertainties in real-world applications. Thus, the proposed
control structure, consisting of an intelligent controller and a conventional
controller, would be preferable in real life.

214
Fuzzy Neural Networks for Real Time Control Applications
In Fig. 11.14(a), while the dotted lines show the first turns, the solid lines
represent the second turns. The control accuracy for the PD controller is the
same for the first and second turn, which is expected. However, the control
accuracy for the T2FNN working in parallel with a PD controller is better
for the second turn. We observe similar behavior in Fig. 11.14(b) and (c).
The results in Fig. 11.14(c) shows performance improvement of 30% in the
case of a PD controller working in parallel with the T2FNN.
Figure 11.14(d) shows the control signals coming from the conventional
PD controller and T2FNN when the PD controller works in parallel
with the T2FNN. As can be seen from Fig. 11.14(d), at the beginning
(in the first turn), the dominating control signal is the one coming from
the PD controller. After the first turn (starting from the 120th second),
the T2FNN is able to take over the control, thus becoming the leading
controller. Moreover, when the reference signal changes, the output of the
PD controller increases. However, after a finite time, the output of the PD
controller again comes back to approximately zero.
Although there exist two independent subsystem controllers in the
autonomous tractor control system, the T2FNN works in parallel with
a PD controller only for the control of the yaw rate of the system.
Thus, the error signal for both PD controller acting alone and in parallel
with the T2FNN are shown in Fig. 11.14(e). As can be seen from Fig.
11.14(e), the T2FNN significantly increases the control accuracy of the
yaw dynamics of the system, since the followed trajectory is a time base
trajectory.
Further, since the reference trajectory to be followed in this experiment
is not a spaced-based trajectory, but a time-based one, the longitudinal
velocity control performance has a direct and significant effect on the
accuracy of the trajectory tracking. For tracking the longitudinal velocity
reference, a structure consisting of a PID controller and a feed-forward
controller is used. As can be seen Fig. 11.14(f), the control performance
of the longitudinal controller is satisfactory.
11.4 CONCLUSION
The simulation results in all these three control case studies show that when
the system is controlled by using a conventional controller in parallel with a
T2FNN, the accuracy of the overall controller increases. In this method,
the conventional controller is responsible for the stability of the system

Case Studies: Control Examples
215
at the beginning of the learning process. After the learning process starts,
the T2FNN controller learns the system dynamics and takes responsibility
for controlling the system gradually. In other words, there is no need to
well-tune the conventional controller. Another advantage of the proposed
scheme is that the SMC theory-based parameters update rules for the
T2FNN proposed are simpler compared to other methods such as gradient
descent, because the proposed parameter adaptation rules have neither
matrix manipulations nor partial derivatives.
REFERENCES
[1] R. Hovorka, Closed-loop insulin delivery: from bench to clinical practice, Nature Rev.
Endocrinol. 7 (7) (2011) 385-395.
[2] J. Reid, G. Kenny, Evaluation of closed-loop control of arterial pressure after cardiopul-
monary bypass, Brit. J. Anaest. 59 (2) (1987) 247-255.
[3] H. Puebla, J. lvarez RamÃ­rez, A cascade feedback control approach for hypnosis, Ann.
Biomed. Eng. 33 (10) (2005) 1449-1463, ISSN 0090-6964.
[4] C.M. Ionescu, R. De Keyser, B.C. Torrico, T. De Smet, M. Struys, J.E. Normey-Rico,
Robust predictive control strategy applied for propofol dosing using BIS as a controlled
variable during anesthesia, IEEE Trans. Biomed. Eng. 55 (9) (2008) 2161-2170.
[5] M. El-Bardini, A.M. El-Nagar, Direct adaptive interval type-2 fuzzy logic controller
for the multivariable anaesthesia system, Ain Shams Eng. J. 2 (3) (2011) 149-160.
[6] T. Bouillon, J. Bruhn, L. Radu-Radulescu, E. Bertaccini, S. Park, S. Shafer, Non-steady
state analysis of the pharmacokinetic interaction between propofol and remifentanil,
Anesthesiology 97 (6) (2002) 1350-1362.
[7] C.S. Nunes, T. Mendonca, S. Bras, D.A. Ferreira, P. Amorim, Modeling anesthetic
drugsâ€™ pharmacodynamic interaction on the bispectral index of the EEG: the influence
of heart rate, in: 29th Annual International Conference of the IEEE on Engineering in
Medicine and Biology Society (EMBS 2007), IEEE, 2007, pp. 6479-6482.
[8] C. Lennmarken, M.-L. Lindholm, S.D. Greenwald, R. Sandin, Confirmation that
low intraoperative BISTM levels predict increased risk of post-operative mortality,
Anesthesiology 99 (3) (2003) A30.
[9] T. Zikov, S. Bibian, G.A. Dumont, M. Huzmezan, C.R. Ries, A wavelet based
de-noising technique for ocular artifact correction of the electroencephalogram, in:
Engineering in Medicine and Biology, 2002, Proceedings of the Second Joint 24th
Annual Conference and the Annual Fall Meeting of the Biomedical Engineering
Society EMBS/BMES Conference, 2002, vol. 1, IEEE, 2002, pp. 98-105.
[10] Y. LIU, L. CHEN, Chaos in attitude dynamics of spacecraft, Springer, New York, 2013.
[11] L.-Q. Chen, Y.-Z. Liu, G. Cheng, Chaotic attitude motion of a magnetic rigid
spacecraft in a circular orbit near the equatorial plane, J. Fran. Inst. 339 (1) (2002)
121-128.
[12] L.-Q. Chen, Y.-Z. Liu, Chaotic attitude motion of a magnetic rigid spacecraft and its
control, Int. J. Non-linear Mech. 37 (3) (2002) 493-504.
[13] R. Sepulveda, P. Melin, A. Rodriguez, A. Mancilla, O. Montiel, Analyzing the effects
of the footprint of uncertainty in type-2 fuzzy logic controllers, Eng. Lett. 13 (2006)
138-147.
[14] J.M. Mendel, Uncertainty, fuzzy logic, and signal processing, Signal Process. 80 (2000)
913-933.

216
Fuzzy Neural Networks for Real Time Control Applications
[15] M.-Y. Hsiao, T.-H.S. Li, J.Z. Lee, C.H. Chao, S.H. Tsai, Design of interval type-2 fuzzy
sliding-mode controller, bibinfojournalInformat. Sci. Int. J. 178 (2008) 1696-1716.
[16] M.A. Khanesar, E. Kayacan, M. Teshnehlab, O. Kaynak, Analysis of the noise reduction
property of type-2 fuzzy logic systems using a novel type-2 membership function, IEEE
Trans. Syst. Man Cybernet. B Cybernet. 41 (5) (2011) 1395-1406.
[17] M.A. Khanesar, E. Kayacan, M. Teshnehlab, O. Kaynak, Extended Kalman filter based
learning algorithm for type-2 fuzzy logic systems and its experimental evaluation, IEEE
Indust. Electron. 59 (11) (2012) 4443-4455.
[18] F.N. Martins, W.C. Celeste, R. Carelli, M. Sarcinelli-Filho, T.F. Bastos-Filho, An
adaptive dynamic controller for autonomous mobile robot trajectory tracking, Control
Eng. Pract. 16 (11) (2008) 1354-1363.
[19] M.A. Khanesar, E. Kayacan, M. Reyhanoglu, O. Kaynak, Feedback Error Learning
Control of Magnetic Satellites Using Type-2 Fuzzy Neural Networks With Elliptic
Membership Functions, Cybernetics, IEEE Transactions on 45 (4) (2015) 858-868.
[20] E. Kayacan, E. Kayacan, H. Ramon, O. Kaynak, W. Saeys, Towards Agrobots:
Trajectory Control of an Autonomous Tractor Using Type-2 Fuzzy Logic Con-
trollers, IEEE/ASME Trans. Mechatron. 20 (1) (2015) 287-298, ISSN 1083-4435, doi:
10.1109/TMECH.2013.2291874.

Appendix A
Contents
A.1 SMC Theory-Based Training Algorithm for Type-2 Fuzzy Neural Network
217
A.1.1 Main Script for SMC Theory-Based Training Algorithm for T2FNN
217
A.1.2 Source Code for Matlab Function t2fnnsmc
220
A.1.3 Source Code for Matlab Function MatrixMultiple.m
222
A.1.4 Source Code for Matlab Function scale.m
223
A.1.5 Source Code for Matlab Function inv_scale.m
223
A.2 GD-Based Theory-Based Training Algorithm for Type-2 Fuzzy Neural Network
223
A.2.1 Main Script
223
A.2.2 Source Code for Matlab Function t2fnngd.m
226
A.3 KF-Based Training Algorithm for Type-2 Fuzzy Neural Network
227
A.3.1 Main Script
227
A.3.2 Source Code for Matlab Function t2fnnkalman.m
230
Abstract
This chapter provides sample source code for number of training algorithms for T2FNNs.
The programming language is Matlab.
Keywords
Simulations, Mackey-Glass, Matlab source code, T2FNN Training using Matlab
A.1 SMC THEORY-BASED TRAINING ALGORITHM FOR TYPE-2
FUZZY NEURAL NETWORK
The main script file for the full SMC theory-based training algorithm
for T2FNN is as follows. In order to execute this script, several func-
tions are needed. These functions are â€œt2fnnsmc.m,â€ â€œMatrixMultiple.m,â€
â€œscale.mâ€™,â€ and â€œinv_scale.m.â€ The source code for these functions must be
written independently. This source code is provided after the main script.
A.1.1 Main Script for SMC Theory-Based Training Algorithm
for T2FNN
1
clear all
2
close all
3
clc
4
217

218
Appendix A
5
%%%%%%%%%%%%%%%%%%%%%%%
6
%%% Data generation %%%
7
%%%%%%%%%%%%%%%%%%%%%%%
8
x(1:18)=1.2; %Initial conditions for Mackey-Glass
9
for t=18:2000
10
x(t+1)=x(t)-0.1*x(t)+0.2*x(t-17)/(x(t-17)^10+1);
11
%The dynamic of mackey-Glass
12
end
13
x=x(18:end);
14
15
%%%%%%%%%%%%%%%%%%%%%%%%%%
16
%%% Normalization part %%%
17
%%%%%%%%%%%%%%%%%%%%%%%%%%
18
minn=0; maxn=1;
19
norm_x=minmax(x); y=scale(x,norm_x(1),norm_x(2),minn,maxn);
20
% Target normalization
21
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
22
%%% Data prepration for T2FNN %%%
23
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
24
tbeg=1; tend=length(y);
25
for t=2:1501
26
Data(t,:)=[y(t) y(t+6) y(t+12) y(t+18) y(t+24)];
27
% [y(t-18) y(t-12) y(t-6) y(t)] is the input and
28
% y(t+6) is the target value of the T2FNN
29
Data_old(t-1,:)=[y(t-1) y(t+5) y(t+11) y(t+17) y(t+23)];
30
end
31
[row,col]=size(Data);
32
33
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
34
%%% T2FNN initialization %%%
35
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
36
q=0.5; % The initial condition for the q
37
alpha=0; % The initial condition for the learning rate of the
38
consequent part
39
center=[0.25 0.75]â€™; % The initial values of the centers of
40
MFs for T2FNN
41
for i=1:col-1
42
C(:,i)=center;
43
end
44
S_upp=0.3*ones(length(center),col-1); % The initial values of
45
% the sigmas
46
S_low=0.2*ones(length(center),col-1); % The initial values of
47
% the sigmas
48
ar=rand(col-1,length(center)^(col-1));
49
% The initial values of the gains of inputs in the consequent
50
% part
51
b=rand(1,length(center)^(col-1))*1e-2;

Appendix A
219
52
% The initial values of the constant terms in the consequent
53
% part
54
55
delta_t=0.0001; %The sample time
56
57
%%%%%%%%%%%%%%%%%
58
%%% Main loop %%%
59
%%% Training %%%
60
%%%%%%%%%%%%%%%%%
61
for epoch=1:100
62
for i=3:1000
63
64
[T2FNN_output(i),C,S_low,S_upp,ar,b,q,alpha] =
65
t2fnnsmc(Data(i,:),...
66
Data_old(i,:),C,S_low,S_upp,ar,b,delta_t,q,alpha);
67
% The calculation of the output of T2FNN and its
68
% update procedure
69
% based on SMC-Theory based training algorithm
70
71
e(i-2)=T2FNN_output(i)-Data(i,col); %The training
error
72
end
73
qq(epoch)=q; % The updated value of q
74
learning_rate(epoch)=alpha;
75
% The value of the learning rate obtained at the end of
76
% each epoch
77
Modeling_Error(epoch)=sqrt(mse(e));
78
% The value of the RMSE obtained at the end of each epoch
79
end
80
81
82
%%%%%%%%%%%%%%%%
83
%%% Testing %%%
84
%%%%%%%%%%%%%%%%
85
for i=1001:1200
86
T2FNN_output(i) = t2fnnsmc(Data(i,:),Data_old(i,:),...
87
C,S_low,S_upp,ar,b,delta_t,q,alpha); %Output
88
% evaluation
89
90
et(i-1000)=T2FNN_output(i)-Data(i,col);
91
end
92
93
%%%%%%%%%%%%%%%%
94
%%% Display %%%
95
%%%%%%%%%%%%%%%%
96
display([â€™The RMSE of the training is equal to â€™
97
num2str(sqrt(mse(e)))])

220
Appendix A
98
display([â€™The RMSE of the testing is equal to â€™
99
num2str(sqrt(mse(et)))])
100
101
figure, plot(Modeling_Error,â€™kâ€™); grid on; % The evolution of
102
% RMSE vs. epoch number
103
ylabel(â€™The RMSEâ€™); xlabel(â€™Epochâ€™);
104
105
T2FNN_output=inv_scale(T2FNN_output,norm_x(1),norm_x(2),
106
minn,maxn);
107
% Denormalization
108
[Data(1:row,col)]=inv_scale(Data(1:row,col),norm_x(1),
109
norm_x(2),minn,maxn);
110
111
112
figure, plot(Data(1:row,col),â€™kâ€™); hold on;
113
plot(T2FNN_output,â€™k-.â€™);
114
ylabel(â€™The outputâ€™)
115
xlabel(â€™Sampleâ€™)
116
legend(â€™The measured outputâ€™,â€™The model outputâ€™)
117
118
figure, plot(learning_rate,â€™kâ€™); hold on;
119
ylabel(â€™The learning rate alphaâ€™);
120
xlabel(â€™Epochâ€™);
121
122
figure
123
plot(qq,â€™kâ€™)
124
ylabel(â€™The parameter (q)â€™)
125
xlabel(â€™Epochâ€™)
A.1.2 Source Code for Matlab Function t2fnnsmc
1
function [yy,Cy,S_low_y,S_upp_y,ary,by,qy,alphay] =
2
t2fnnsmc(x,x_old,...
3
C,S_low,S_upp,ar,b,delta_t,q,alpha)
4
%%%%%%%%%%%%%%%%%%%%%%
5
%%% Initialization %%%
6
%%%%%%%%%%%%%%%%%%%%%%
7
[row1,col1]=size(C);
8
C_dot=zeros(row1,col1);
9
S_low_dot=zeros(row1,col1);
10
S_upp_dot=zeros(row1,col1);
11
alpha1=0.001;
12
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
13
%%% MF value calculation %%%
14
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
15
for i=1:col1
16
M_low(:,i)=max(0,exp(-(x(i) - C(:,i)).^2./

Appendix A
221
17
(2*S_low(:,i).^2)));
18
M_upp(:,i)=max(0,exp(-(x(i) - C(:,i)).^2./
19
(2*S_upp(:,i).^2)));
20
end
21
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
22
%%% The calcuation of the firing of the rules %%%
23
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
24
Wij_low = MatrixMultiple(M_low);
25
Wij_upp = MatrixMultiple(M_upp);
26
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
27
%%% The normalization of the firing of the rules %%%
28
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
29
W_low=Wij_low/(sum(Wij_low));
30
W_upp=Wij_upp/(sum(Wij_upp));
31
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
32
%%% The calculation of the output of T2FNN %%%
33
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
34
yy_low=(x(1:col1)*ar+b)*W_low;
35
yy_upp=(x(1:col1)*ar+b)*W_upp;
36
yy=q*yy_low+(1-q)*yy_upp;
37
e=yy-x(end); % The prediction error
38
signum=e/(abs(e)+0.05); % The approximate sign function
39
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
40
%%% Weight updates for constant parameters %%%
41
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
42
for i=1:row1^(col1)
43
b_dot(1,i)=-alpha*signum*(q*W_low(i)+(1-q)*W_upp(i))...
44
/(((q*W_low+(1-q)*W_upp)â€™*(q*W_low+(1-q)*W_upp)));
45
by(1,i)=b(1,i)+b_dot(1,i);
46
end
47
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
48
%%% Weight updates for the gains of the input in the %%%
49
%%% consequent part %%%
50
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
51
52
for j=1:col1
53
for i=1:row1^(col1)
54
ar_dot(j,i)=-alpha*x(j)*signum*(q*W_low(i)+(1-q)*
55
W_upp(i))...
56
/(((q*W_low+(1-q)*W_upp)â€™*(q*W_low+(1-q)*W_upp)));
57
ary(j,i)=ar(j,i)+ar_dot(j,i);
58
end
59
end
60
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
61
%%% Antecedent part parameters updates %%%

222
Appendix A
62
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
63
for j=1:col1
64
for i=1:row1
65
C_dot(i,j)=(x(i)-x_old(i))/delta_t+(x(i)-C(i,j))
66
*alpha1*signum;
67
% Adaptation laws of centers
68
S_low_dot(i,j)=-(S_low(i,j)+S_low(i,j).^3/
69
(x(i)-C(i,j)).^2) *alpha1*signum;
70
% Adaptation laws of sigma lower
71
S_upp_dot(i,j)=-(S_upp(i,j)+S_upp(i,j).^3/
72
(x(i)-C(i,j)).^2) *alpha1*signum;
73
% Adaptation laws of sigma upper
74
end
75
end
76
Cy=C+C_dot*delta_t; % Update rule for centers
77
S_low_y=S_low+S_low_dot*delta_t; % Update rule for sigma
78
S_upp_y=S_upp+S_upp_dot*delta_t; % Update rule for sigma
79
q_dot =-delta_t*(alpha*signum)/(b*(W_low-W_upp));
80
qy=q+q_dot; % Update rule for q
81
% Restriction of q between â€™0â€™ and â€™1â€™
82
if qy<0
83
qy=0;
84
end
85
if qy>1
86
qy=1;
87
end
88
% Update rule for the adaptive learning rate
89
alpha_dot =8e-6*abs(e)-1e-10*alpha;
90
alphay=alpha+alpha_dot;
A.1.3 Source Code for Matlab Function MatrixMultiple.m
1
function CC=MatrixMultiple(A)
2
% The calculation of the firing of the rules
3
4
m=size(A,2);
5
6
C=DoCalculation(A(:,m-1),A(:,m));
7
for i=1:(m-2)
8
C=DoCalculation(A(:,m-(i+1)),C);
9
end
10
CC=C;
11
12
function C=DoCalculation(A,B)
13
14
k=size(B,1);

Appendix A
223
15
l=size(A,1);
16
C=zeros(l*k,1);
17
start=1; stop=start+k-1;
18
for i=1:l
19
C(start:stop,1)=A(i)*B;
20
start=start+k; stop=start+k-1;
21
end
A.1.4 Source Code for Matlab Function scale.m
1
function [xnew]=scale(xold,minold,maxold,minnew,maxnew)
2
xnew=minnew+(xold-minold)*(maxnew-minnew)/(maxold-minold);
A.1.5 Source Code for Matlab Function inv_scale.m
1
function [xnew]=inv_scale(xold,minold,maxold,minnew,maxnew)
2
xnew=(xold-minnew)*(maxold-minold)/(maxnew-minnew)+minold;
A.2 GD-BASED THEORY-BASED TRAINING ALGORITHM
FOR TYPE-2 FUZZY NEURAL NETWORK
The main script file for the GD-based theory-based training algorithm
for the consequent part parameters of T2FNN. In order to execute this
script, several functions are needed. These functions are â€œt2fnngd.mâ€²â€²,
â€œMatrixMultiple.mâ€, â€œscale.mâ€, and â€œinv_scale.mâ€. The source code for
these functions (except â€œt2fnngd.mâ€) was given in the previous section,
and the source code for â€œt2fnngd.mâ€ is given after the main script file.
A.2.1 Main Script
1
clear all
2
close all
3
clc
4
5
%%%%%%%%%%%%%%%%%%%%%%%
6
%%% Data Generation %%%
7
%%%%%%%%%%%%%%%%%%%%%%%
8
x(1:18)=1.2; %Initial Conditions for Mackey-Glass
9
for t=18:2000
10
x(t+1)=x(t)-0.1*x(t)+0.2*x(t-17)/(x(t-17)^10+1); %The
11
% dynamic of Mackey-Glass
12
end
13
x=x(18:end);

224
Appendix A
14
15
%%%%%%%%%%%%%%%%%%%%%%%%%%
16
%%% Normalization Part %%%
17
%%%%%%%%%%%%%%%%%%%%%%%%%%
18
minn=0; maxn=1;
19
norm_x=minmax(x); y=scale(x,norm_x(1),norm_x(2),minn,maxn);
20
% Target normalization
21
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
22
%%% Data prepration for T2FNN %%%
23
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
24
tbeg=1; tend=length(y);
25
for t=2:1501
26
Data(t,:)=[y(t) y(t+6) y(t+12) y(t+18) y(t+24)];
27
% [y(t-18) y(t-12) y(t-6) y(t)] is the input and
28
% y(t+6) is the target value of the T2FNN
29
end
30
[row,col]=size(Data);
31
32
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
33
%%% T2FNN Initialization %%%
34
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
35
q=0.5; % The initial condition for the q
36
center=[0.25 0.75]â€™; % The initial values of the centers of
37
% MFs for T2FNN
38
for i=1:col-1
39
C(:,i)=center;
40
end
41
S_upp=0.3*ones(length(center),col-1);
42
% The initial values of the sigma upper of MFs for T2FNN
43
S_low=0.2*ones(length(center),col-1);
44
% The initial values of the sigma lower of MFs for T2FNN
45
ar=rand(col-1,length(center)^(col-1))*1e-2;
46
% The initial values of the gains of inputs in the
47
% consequent part
48
b=rand(1,length(center)^(col-1))*1e-2;
49
% The initial values of the constant terms in the
50
% consequent part
51
52
alpha=0.1; % The learning rate
53
54
%%%%%%%%%%%%%%%%%
55
%%% Main loop %%%
56
%%% Training %%%
57
%%%%%%%%%%%%%%%%%
58
for epoch=1:100
59
for i=3:1000
60

Appendix A
225
61
[T2FNN_output(i),ar,b,q] = t2fnngd(Data(i,:),C,S_low,
62
S_upp,ar,b,q,alpha);
63
% The calculation of the output of T2FNN and its
64
% update procedure
65
% based on GD training algorithm
66
67
e(i-2)=Data(i,col)-T2FNN_output(i); %The training
error
68
end
69
qq(epoch)=q; %The updated value of q
70
Modeling_Error(epoch)=sqrt(mse(e));
71
%The value of the RMSE obtained at the end of each epoch
72
end
73
74
75
%%%%%%%%%%%%%%%%
76
%%% Testing %%%
77
%%%%%%%%%%%%%%%%
78
for i=1001:1200
79
T2FNN_output(i) = t2fnngd(Data(i,:),C,S_low,S_upp,ar,
80
b,q,alpha);
81
%Output evaluation
82
83
et(i-1000)=T2FNN_output(i)-Data(i,col);
84
end
85
86
%%%%%%%%%%%%%%%%
87
%%% Display %%%
88
%%%%%%%%%%%%%%%%
89
display([â€™The RMSE of the training is equal to â€™
90
num2str(sqrt(mse(e)))])
91
display([â€™The RMSE of the testing is equal to â€™
92
num2str(sqrt(mse(et)))])
93
94
figure, plot(Modeling_Error,â€™kâ€™); grid on; % The evolution
95
% of RMSE vs. epoch number
96
ylabel(â€™The RMSEâ€™); xlabel(â€™Epochâ€™);
97
98
T2FNN_output=inv_scale(T2FNN_output,norm_x(1),norm_x(2),
99
minn,maxn);
100
% Denormalization
101
[Data(1:row,col)]=inv_scale(Data(1:row,col),norm_x(1),
102
norm_x(2),minn,maxn);
103
104
105
figure, plot(Data(1:row,col),â€™kâ€™); hold on;
106
plot(T2FNN_output,â€™k-.â€™);

226
Appendix A
107
ylabel(â€™The outputâ€™)
108
xlabel(â€™Sampleâ€™)
109
legend(â€™The measured outputâ€™,â€™The model outputâ€™)
110
111
112
figure
113
plot(qq,â€™kâ€™)
114
ylabel(â€™The parameter (q)â€™)
115
xlabel(â€™Epochâ€™)
A.2.2 Source Code for Matlab Function t2fnngd.m
1
function [yy,ary,by,qy] = t2fnngd(x,C,S_low,S_upp,ar,b,q,
alpha)
2
%%%%%%%%%%%%%%%%%%%%%%
3
%%% initialization %%%
4
%%%%%%%%%%%%%%%%%%%%%%
5
[row1,col1]=size(C);
6
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
7
%%% MF value calculation %%%
8
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
9
for i=1:col1
10
M_low(:,i)=max(0,exp(-(x(i) - C(:,i)).^2./
11
(2*S_low(:,i).^2)));
12
M_upp(:,i)=max(0,exp(-(x(i) - C(:,i)).^2./
13
(2*S_upp(:,i).^2)));
14
end
15
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
16
%%% The calcuation of the firing of the rules %%%
17
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
18
Wij_low = MatrixMultiple(M_low);
19
Wij_upp = MatrixMultiple(M_upp);
20
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
21
%%% The normalization of the firing of the rules %%%
22
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
23
W_low=Wij_low/(sum(Wij_low));
24
W_upp=Wij_upp/(sum(Wij_upp));
25
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
26
%%% The calculation of the output of T2FNN %%%
27
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
28
yy_low=(x(1:col1)*ar+b)*W_low;
29
yy_upp=(x(1:col1)*ar+b)*W_upp;
30
yy=q*yy_low+(1-q)*yy_upp;
31
e=x(end)-yy; % The prediction error
32
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
33
%%% Weight updates for constant parameters %%

Appendix A
227
34
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
35
for i=1:row1^(col1)
36
delta_b(1,i)=alpha*e*(q*W_low(i)+(1-q)*W_upp(i));
37
by(1,i)=b(1,i)+delta_b(1,i);
38
end
39
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
40
%%% Weight updates for the gains of the input in the
41
%%% consequent part %%%
42
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
43
for j=1:col1
44
for i=1:row1^(col1)
45
delta_ar(j,i)=alpha*e*x(j)*(q*W_low(i)+(1-q)*W_upp(i))
;
46
ary(j,i)=ar(j,i)+delta_ar(j,i);
47
end
48
end
49
50
qy=q+alpha*e*(yy_low-yy_upp);
51
if qy>1
52
qy=1;
53
elseif qy<0
54
qy=0;
55
end
A.3 KF-BASED TRAINING ALGORITHM FOR TYPE-2 FUZZY
NEURAL NETWORK
The main script file for KF-Based theory-based training algorithm for the
parameters of the consequent part of T2FNN. In order to execute this
script, several functions are needed. These functions are â€œt2fnnkalman.mâ€,
â€œMatrixMultiple.mâ€, â€œscale.mâ€, and â€œinv_scale.mâ€. The source code for
these functions except â€œt2fnngd.mâ€ was given in the previous sections, and
the source code for â€œt2fnnkalman.mâ€ is given after the main script file.
A.3.1 Main Script
1
clear all
2
close all
3
clc
4
5
%%%%%%%%%%%%%%%%%%%%%%%
6
%%% Data Generation %%%
7
%%%%%%%%%%%%%%%%%%%%%%%
8
x(1:18)=1.2; %Initial Conditions for Mackey-Glass
9
for t=18:2000

228
Appendix A
10
x(t+1)=x(t)-0.1*x(t)+0.2*x(t-17)/(x(t-17)^10+1); %The
11
% dynamic of Mackey-Glass
12
end
13
x=x(18:end);
14
15
%%%%%%%%%%%%%%%%%%%%%%%%%%
16
%%% Normalization Part %%%
17
%%%%%%%%%%%%%%%%%%%%%%%%%%
18
minn=0; maxn=1;
19
norm_x=minmax(x); y=scale(x,norm_x(1),norm_x(2),minn,maxn);
20
% Target normalization
21
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
22
%%% Data prepration for T2FNN %%%
23
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
24
tbeg=1; tend=length(y);
25
for t=2:1501
26
Data(t,:)=[y(t) y(t+6) y(t+12) y(t+18) y(t+24)];
27
% [y(t-18) y(t-12) y(t-6) y(t)] is the input and
28
% y(t+6) is the target value of the T2FNN
29
end
30
[row,col]=size(Data);
31
32
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
33
%%% T2FNN Initialization %%%
34
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
35
q=0.5; % The initial condition for q
36
center=[0.25 0.75]â€™; % The initial values of the centers of
37
% MFs for T2FNN
38
for i=1:col-1
39
C(:,i)=center;
40
end
41
S_upp=0.3*ones(length(center),col-1);
42
% The initial values of the sigma upper of MFs for T2FNN
43
S_low=0.2*ones(length(center),col-1);
44
% The initial values of the sigma lower of MFs for T2FNN
45
ar=rand(col-1,length(center)^(col-1))*1e-2;
46
% The initial values of the gains of inputs in the consequent
47
% part
48
b=rand(1,length(center)^(col-1))*1e-2;
49
% The initial values of the constant terms in the consequent
50
% part
51
52
P=eye(size(ar,1)*size(ar,2)+length(b)); % The covariance
matrix
53
R=1; % The covariance of measurement noise
54
Q=0.001*eye(size(ar,1)*size(ar,2)+length(b)); % The
covariance

Appendix A
229
55
% of process noise
56
57
%%%%%%%%%%%%%%%%%
58
%%% Main loop %%%
59
%%% Training %%%
60
%%%%%%%%%%%%%%%%%
61
for epoch=1:100
62
for i=3:1000
63
64
[T2FNN_output(i),ar,b,P] = t2fnnkalman(Data(i,:),
65
C,S_low,S_upp,...
66
ar,b,q,P,Q,R);
67
% The calculation of the output of T2FNN and its
68
% update procedure
69
% based on KF training algorithm
70
71
72
e(i-2)=Data(i,col)-T2FNN_output(i); %The training
error
73
end
74
Modeling_Error(epoch)=sqrt(mse(e)); %The value of the RMSE
75
% obtained at the end of each epoch
76
end
77
78
79
%%%%%%%%%%%%%%%%
80
%%% Testing %%%
81
%%%%%%%%%%%%%%%%
82
for i=1001:1200
83
T2FNN_output(i) = t2fnnkalman(Data(i,:),C,S_low,S_upp,
84
ar,b,q,P,Q,R);
85
%Output evaluation
86
et(i-1000)=T2FNN_output(i)-Data(i,col);
87
end
88
89
%%%%%%%%%%%%%%%%
90
%%% Display %%%
91
%%%%%%%%%%%%%%%%
92
display([â€™The RMSE of the training is equal to â€™
93
num2str(sqrt(mse(e)))])
94
display([â€™The RMSE of the testing is equal to â€™
95
num2str(sqrt(mse(et)))])
96
97
figure, plot(Modeling_Error,â€™kâ€™); grid on; % The evolution of
98
RMSE vs. epoch number
99
ylabel(â€™The RMSEâ€™); xlabel(â€™Epochâ€™);
100

230
Appendix A
101
T2FNN_output=inv_scale(T2FNN_output,norm_x(1),
102
norm_x(2),minn,maxn);
103
% Denormalization
104
[Data(1:row,col)]=inv_scale(Data(1:row,col),norm_x(1),
105
norm_x(2),minn,maxn);
106
107
108
figure, plot(Data(1:row,col),â€™kâ€™); hold on;
109
plot(T2FNN_output,â€™k-.â€™);
110
ylabel(â€™The outputâ€™)
111
xlabel(â€™Sampleâ€™)
112
legend(â€™The measured outputâ€™,â€™The model outputâ€™)
A.3.2 Source Code for Matlab Function t2fnnkalman.m
1
function [yy,ary,by,P] = t2fnnkalman(x,C,S_low,S_upp,ar,
2
b,q,P,Q,R)
3
%%%%%%%%%%%%%%%%%%%%%%
4
%%% initialization %%%
5
%%%%%%%%%%%%%%%%%%%%%%
6
[row1,col1]=size(C);
7
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
8
%%% MF value calculation %%%
9
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
10
for i=1:col1
11
M_low(:,i)=max(0,exp(-(x(i) - C(:,i)).^2./
12
(2*S_low(:,i).^2)));
13
M_upp(:,i)=max(0,exp(-(x(i) - C(:,i)).^2./
14
(2*S_upp(:,i).^2)));
15
end
16
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
17
%%% The calcuation of the firing of the rules %%%
18
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
19
Wij_low = MatrixMultiple(M_low);
20
Wij_upp = MatrixMultiple(M_upp);
21
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
22
%%% The normalization of the firing of the rules %%%
23
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
24
W_low=Wij_low/(sum(Wij_low));
25
W_upp=Wij_upp/(sum(Wij_upp));
26
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
27
%%% The calculation of the output of T2FNN %%%
28
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
29
yy_low=(x(1:col1)*ar+b)*W_low;
30
yy_upp=(x(1:col1)*ar+b)*W_upp;
31
yy=q*yy_low+(1-q)*yy_upp;
32
e=x(end)-yy; % The prediction error

Appendix A
231
33
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
34
%%% Weight updates for constant parameters %%
35
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
36
Phi=q*W_low+(1-q)*W_upp;
37
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
38
%%% Weight updates for the gains of the input in the
39
%%% consequent part %%%
40
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
41
for j=1:col1
42
Phi=[Phi;(q*W_low+(1-q)*W_upp)*x(j)];
43
end
44
45
L=P*Phi*(Phiâ€™*P*Phi+R)^(-1);
46
47
P=P-L*Phiâ€™*P+Q; % The update rule for covariance matrix
48
d=L*e;
49
50
lar=size(ar,2);
51
by=(b+d(1:lar,1)â€™);% Adaptation for b
52
for j=2:size(ar,1)+1
53
ary(j-1,:)=(ar(j-1,:)+d((j-1)*lar+1:j*lar,1)â€™);%
Adaptation
54
% for ar
55
end

Appendix B
Contents
B.1
SMC Theory-Based Training Algorithm for Type-2 Fuzzy Neural Network Controller 233
B.1.1 Main Script
233
B.1.2 Source Code for MATLAB Function neurofuzzy
235
B.1.3 Source Code for MATLAB Function elip.m
239
Abstract
This chapter provides sample source code for the T2FNN training algorithm used as
controller. The programming language is MATLAB.
Keywords
Simulation, Controller, MATLAB source code
The main script file for a full SMC theory-based algorithm for the training
of a T2FNN when used as a controller is given here. In order to execute
this script, two functions are needed: neurofuzzy.m and elip.m. The source
code for these functions must be written as an independent file. This code
is provided after the main script.
B.1 SMC THEORY-BASED TRAINING ALGORITHM FOR TYPE-2
FUZZY NEURAL NETWORK CONTROLLER
B.1.1 Main Script
1
close all;clear all;clc
2
%% The plant parameters%%
3
alpha1=0.6984;K=1.1;gamma=0.2;
4
%% The initial conditions of the system
5
x1=0.6;x2=0.1;
6
%% T2FNN configuration
7
c1=[-1 0 1];
8
c2=[-1 0 1];
9
d1=ones(1,3);
10
d2=ones(1,3);
11
a11=d1;a21=d1;a12=d1;a22=d1;q=0.5;
12
theta=zeros(3,3);kk=1;
13
%% Simulation parameters
14
T=20;
233

234
Appendix B
15
Ts=0.001;
16
%% The parameters of the PD controller
17
Kp=9;Kd=5;u=0;
18
%% The learning rates
19
gamma1=0.01;alpha=4;
20
%% The reference signal
21
rr=0.4*sin(kk*(1:T/Ts+10)*Ts);Kr=0;alpha=0;
22
for t=1:T/Ts
23
%% The dynamic model of the system
24
x1(t+1)=x1(t)+Ts*x2(t);
25
x2(t+1)=x2(t)+Ts*(-K*sin(2*x1(t))-gamma*x2(t)-alpha1*(2*
sin(x1(t))*...
26
sin(t*Ts)+cos(x1(t))*cos(t*Ts))+u(t));
27
%% The classical controller calculations
28
e=rr(t+1)-x1(t+1);
29
de=0.4*kk*cos(kk*t*Ts)-x2(t+1);
30
tauc(t+1)=Kp*(e)+Kd*(de);
31
%% The T2FNN controller calculations
32
[y theta c1 c2 d1 d2 a11 a21 a12 a22]=neurofuzzy1(e,de,q,
tauc(t+1),...
33
theta,c1,c2,d1,d2,a11,a21,a12,a22,gamma1,alpha(t),Ts);
34
%%The overall control signal
35
u(t+1)=tauc(t+1)-y+Kr(t)*de/(abs(de)+0.01);
36
%% The adaptive parameter of the term to guarantee the
robustness
37
Kr(t+1)=Kr(t)+0.001*abs(de);
38
%% The adaptive learning rate
39
alpha(t+1)=alpha(t)+0.001*abs(tauc(t+1));
40
%% The error signal
41
ee(t)=e;
42
end
43
44
figure(1)
45
plot((0:T/Ts)*Ts,x1)
46
hold on
47
figure(4)
48
plot((0:T/Ts)*Ts,tauc)
49
hold on
50
plot((0:T/Ts)*Ts,-u+tauc,â€™râ€™)
51
xlabel(â€™tâ€™)
52
plot((0:T/Ts)*Ts,u,â€™kâ€™)
53
legend(â€™The output of the PD controllerâ€™,...
54
â€™The output of the type-2 fuzzy controllerâ€™,â€™The overall
control signalâ€™)
55
ylabel(â€™The control signalsâ€™)
56
%% The simulations corresponding to the PD controller when
acting alone

Appendix B
235
57
for t=1:T/Ts
58
59
x1(t+1)=x1(t)+Ts*x2(t);
60
x2(t+1)=x2(t)+Ts*(-K*sin(2*x1(t))-gamma*x2(t)-alpha1*(2*
sin(x1(t))*...
61
sin(t*Ts)+cos(x1(t))*cos(t*Ts))+u(t));
62
63
e=rr(t+1)-x1(t+1);
64
de=0.4*kk*cos(kk*t*Ts)-x2(t+1);
65
tauc(t+1)=Kp*(e)+Kd*(de);
66
u(t+1)=tauc(t+1);
67
ee(t)=e;
68
end
69
70
figure(1)
71
plot((0:T/Ts)*Ts,x1,â€™râ€™)
72
TT1=(0:T/Ts)*Ts;
73
RR1=rr(1:20001);
74
plot(TT1(1:500:20001),RR1(1:500:20001),â€™kâ€™)
75
ylabel(â€™x_1â€™)
76
axis([0 20 -0.6 1.6])
77
xlabel(â€™tâ€™)
78
legend(â€™The response of the proposed control systemâ€™,...
79
â€™The response of the PD controller when it is used alone
â€™,...
80
â€™The sinusoidal reference signalâ€™)
81
figure(2)
82
plot((0:T/Ts)*Ts,Kr,â€™kâ€™)
83
xlabel(â€™tâ€™)
84
ylabel(â€™The adaptation of the parameter to guarantee
robustness (K_r)â€™)
85
figure(3)
86
plot((0:T/Ts)*Ts,alpha,â€™kâ€™)
87
xlabel(â€™tâ€™)
88
ylabel(â€™The adaptation of the learning rate (\alpha)â€™)
B.1.2 Source Code for MATLAB Function neurofuzzy
1
function [y thetan c1 c2 d1 d2 a11 a21 a12 a22]=neurofuzzy(e,
de,q,tauc,...
2
theta,c1,c2,d1,d2,a11,a21,a12,a22,gamma1,alpha,Ts)
3
4
%% The calculation of the upper and lower MF for e
5
for i=1:length(c1)
6
mho1_lower(i)=elip(e,c1(i),d1(i),a21(i));
7
mho1_upper(i)=elip(e,c1(i),d1(i),a11(i));
8
end

236
Appendix B
9
%% The calculation of the upper and lower MF for de
10
for i=1:length(c2)
11
mho2_lower(i)=elip(de,c2(i),d2(i),a22(i));
12
mho2_upper(i)=elip(de,c2(i),d2(i),a12(i));
13
end
14
%% The calculation of the upper and lower and upper bounds of
the rules
15
for i=1:length(c1)
16
for j=1:length(c2)
17
rule_lower(i,j)=mho1_lower(i)*mho2_lower(j);
18
rule_upper(i,j)=mho1_lower(i)*mho2_lower(j);
19
end
20
end
21
%% The normalization of the rules
22
rule_lower_tilde=rule_lower*sum(sum(rule_lower))/(sum(sum(
rule_lower)...
23
)^2
24
+0.001);
25
rule_upper_tilde=rule_upper*sum(sum(rule_upper))/(sum(sum(
rule_upper)...
26
)^2+0.001);
27
y=sum(sum(rule_lower_tilde.*theta*q+rule_upper_tilde.*theta
*(1-q)));
28
29
30
if sum(sum(rule_upper))+ sum(sum(rule_lower)) ==0
31
y=0;
32
thetan=theta;
33
else
34
35
%% The adaptation rules
36
for i=1:length(a21)
37
T21(i)=(abs((e-c1(i))/d1(i)))^a21(i);
38
T11(i)=(abs((e-c1(i))/d1(i)))^a11(i);
39
T22(i)=(abs((de-c2(i))/d2(i)))^a22(i);
40
T12(i)=(abs((de-c2(i))/d2(i)))^a12(i);
41
if e<=c1(i)-d1(i)
42
a21(i)=a21(i);
43
elseif e>=c1(i)+d1(i)
44
a21(i)=a21(i);
45
elseif T21(i)==0
46
a21(i)=a21(i);
47
else
48
a21(i)=a21(i)+Ts*gamma1*(log(1-(abs((e-c1(i))/d1(i))
)^a21(i)...
49
)/a21(i)^2+(abs((e-c1(i))...

Appendix B
237
50
/d1(i)))^a21(i)*log(abs((e-c1(i))/d1(i)))/a21(i))
^(-1)*...
51
(1-T21(i))*H(e,c1(i)-d1(i),c1(i)+d1(i));
52
end
53
if e<=c1(i)-d1(i)
54
a11(i)=a11(i);
55
elseif e>=c1(i)+d1(i)
56
a11(i)=a11(i);
57
elseif T11(i)==0
58
a11(i)=a11(i);
59
else
60
a11(i)=a11(i)+Ts*gamma1*(log(1-(abs((e-c1(i))/d1(i))
)^a11(i)...
61
)/a11(i)^2+(abs((e-c1(i))...
62
/d1(i)))^a11(i)*log(abs((e-c1(i))/d1(i)))/a11(i))
^(-1)...
63
*(1-T11(i))*H(e,c1(i)-d1(i),c1(i)+d1(i));
64
end
65
if de<=c2(i)-d2(i)
66
a22(i)=a22(i);
67
elseif de>=c2(i)+d2(i)
68
a22(i)=a22(i);
69
elseif T22(i)==0
70
a22(i)=a22(i);
71
else
72
a22(i)=a22(i)+Ts*gamma1*(log(1-(abs((de-c2(i))/d2(i)
))^a22...
73
(i))/a22(i)^2+(abs((de-c2(i))...
74
/d2(i)))^a22(i)*log(abs((de-c2(i))/d2(i)))/a22(i)
)^(-1)...
75
*(1-T22(i))*H(de,c2(i)-d2(i),c2(i)+d2(i));
76
end
77
if de<=c2(i)-d2(i)
78
a12(i)=a12(i);
79
elseif e>=c2(i)+d2(i)
80
a12(i)=a12(i);
81
elseif T12(i)==0
82
a12(i)=a12(i);
83
else
84
a12(i)=a12(i)+Ts*gamma1*(log(1-(abs((de-c2(i))/d2(i)
))^a12...
85
(i))/a12(i)^2+(abs((de-c2(i))...
86
/d2(i)))^a12(i)*log(abs((de-c2(i))/d2(i)))/a12(i)
)^(-1)...
87
*(1-T12(i))*H(de,c2(i)-d2(i),c2(i)+d2(i));
88
end
89
if a11(i)<1

238
Appendix B
90
a11(i)=1;
91
end
92
if a21(i)>1
93
a21(i)=1;
94
end
95
if a12(i)<1
96
a21(i)=1;
97
end
98
if a22(i)>1
99
a22(i)=1;
100
end
101
end
102
103
for i=1:length(c1)
104
if e-c1(i) ~=0
105
c1(i)=c1(i)-gamma1*Ts*abs(d1(i))^a21(i)*(1-T21(i))*
sign(e-...
106
c1(i))/(abs(e-c1(i)))^(a21(i)-1)*H(e,c1(i)-d1(i),
c1(i)...
107
+d1(i)) ;
108
end
109
if de-c2(i) ~=0
110
c2(i)=c2(i)-gamma1*Ts*abs(d2(i))^a22(i)*(1-T22(i))*
sign(de-...
111
c2(i))/(abs(de-c1(i)))^(a22(i)-1)*H(de,c2(i)-d2(i
),c2(i)...
112
+d2(i));
113
end
114
if e-c1(i) ~=0
115
d1(i)=d1(i)-gamma1*Ts*(1-T21(i))*abs(d1(i))^(a21(i)
+1)/(abs...
116
(e-c1(i)))^(a21(i))*sign(d1(i))*H(e,c1(i)-d1(i),
c1(i)+d1(i));
117
end
118
if de-c2(i) ~=0
119
d2(i)=d2(i)-gamma1*Ts*(1-T22(i))*abs(d2(i))^(a22(i)
+1)/(abs(...
120
de-c2(i)))^(a22(i))*sign(d2(i))*H(de,c2(i)-d2(i),
c2(i)...
121
+d2(i));
122
end
123
end
124
125
126
for i=1:length(c1)
127
for j=1:length(c2)

Appendix B
239
128
theta(i,j)=theta(i,j)-Ts*(q*rule_lower_tilde(i,j)
+(1-q)*...
129
rule_upper_tilde(i,j))...
130
/sum(sum(q*rule_lower_tilde+(1-q)*
rule_upper_tilde))/...
131
sum(sum(q*rule_lower_tilde+(1-q)*rule_upper_tilde
))*...
132
alpha*tauc/(abs(tauc)+0.001);
133
end
134
end
135
thetan=theta;
136
end
137
138
function y=H(x,m,n)
139
if x<m
140
y=0;
141
elseif x>n
142
y=0;
143
else
144
y=x;
145
end
B.1.3 Source Code for MATLAB Function elip.m
1
function y=elip(x,c,d,a)
2
3
if x<=c-d
4
y=0;
5
elseif x>=c+d
6
y=0;
7
else
8
y=(1-(abs((x-c)/d))^a)^(1/a);
9
end

INDEX
Note: Page numbers followed by f indicate figures and t indicate tables.
A
A2-C0 fuzzy system, 88, 106â€“107, 163â€“165
Adaptive learning rate, 55, 61â€“63, 89, 90,
95â€“96, 97, 104â€“105, 114, 117, 119, 174,
176f , 198â€“200, 198f , 201f , 203f
Anesthesia, 186â€“192
Antecedent, 16, 19, 26, 27, 30, 38â€“40, 38t,
41, 57â€“58, 61, 69, 91, 136, 158â€“159
Autonomous tractor, 203â€“214
B
Bispectral index, 186â€“192, 194f
Boolean logic, 14, 21
C
Chain rule, 59â€“60
Chattering, 10, 10f , 11, 86, 90â€“91
Conditional mean estimator, 72
Condition number, 4, 78
Consequent, 19, 26, 27, 30, 37â€“40, 38t, 41,
42â€“43, 47â€“48, 57â€“58, 61, 83, 86â€“87, 88,
91, 95â€“96, 106â€“107, 115â€“116, 119, 136,
143, 151â€“152, 158â€“159, 163â€“165, 174,
189â€“191, 194, 223, 227
Covariance matrix, 71â€“72, 74, 75â€“76, 77,
77t, 78, 80t, 82â€“84
Crisp, 16, 16f , 18â€“19, 20, 27, 30, 35,
38â€“39, 38t, 40, 86â€“87, 107, 163â€“165
D
Decoupled extended Kalman filter, 83
Defuzzification, 18, 20, 30
Discrete time systems, 6
E
Eigenvalues, 4, 63â€“64
Eigenvectors, 3
Elliptic type-2 MF, 86â€“87, 96â€“105, 107,
119â€“120, 172
Extended Kalman filter, 78â€“81, 80t, 82â€“84,
179â€“181, 181t
F
Feedback error learning, 186, 194, 198â€“200
Foot print of uncertainty, 28, 28f , 107
Fuzzification, 18â€“19, 20
G
Gaussian type-2 MF, 82â€“83, 89â€“96,
107, 186
Global minimum, 46â€“47, 48â€“49
Gradient, 4, 46â€“69, 96, 116, 135â€“136, 181,
214â€“215
Gradient descent, 46â€“69, 214â€“215
H
Hessian matrix, 4â€“5, 52â€“53
Higher order continuous PSO, 138
I
Identification, 86â€“105, 107, 130â€“131,
152â€“153, 159, 173â€“178, 179â€“183
Inverse of a matrix, 2
K
Kalman filter, 71â€“77, 78â€“84
Kinematic model, 207â€“209
L
Levenberg Marquardt, 55, 56f , 61, 63â€“69,
159, 181
Linear matrix algebra, 1â€“4
Local minimum, 46â€“47, 47f , 48â€“52
Longitudinal velocity, 203â€“205, 207, 209,
211f , 214
Lyapunov stability theory, 5
M
Mackey-Glass time series, 173â€“174
Magnetic rigid spacecraft, 193â€“202
Matrix inversion lemma, 3, 65
Matrix multiplication, 1â€“2
Momentum term, 56â€“57, 61, 135
241

242
Index
O
Optimization problem, 46, 48â€“49, 135, 158
P
Particle swarm optimization, 134â€“138,
142â€“159
Principle of orthogonality, 73, 74
Prod t-norm operator, 88, 142â€“143, 151
Pseudo inverse of a matrix, 2, 3
R
Reaching (hitting) mode, 7
Real-valued function, 4
S
Second-order nonlinear time-varying plant,
174â€“178
Skew-symmetric, 2
Sliding manifold, 7, 89, 105â€“106, 109,
117â€“119
Sliding mode control, 7, 86â€“131
Square root filtering, 78
Stability analysis, 5â€“6, 61â€“69, 86, 111,
130â€“131, 135, 137, 145â€“150,
154â€“158, 159
Stability analysis of the LM, 63â€“69
Symmetric, 2
T
Taylor series expansion, 5, 47â€“48, 52â€“53,
54â€“55, 61, 63â€“64, 71â€“72, 78, 83â€“84
Tractor-implement system, 195, 203â€“214,
208f , 211f
Transpose, 2
Triangular, 18â€“19, 30, 32â€“33, 41f , 42,
162â€“163
Type-1, 13â€“15, 16â€“17, 18â€“24, 25â€“26,
27â€“28, 30, 33, 35, 37â€“39, 38t, 42â€“43,
46, 57, 162â€“163, 167â€“168, 186â€“187,
200â€“202, 205f
Type-2, 25â€“35, 37â€“43, 46â€“69, 71â€“77,
78â€“84, 86â€“87, 89â€“105, 106â€“107, 108,
119â€“120, 121, 134â€“138, 142â€“159,
161â€“172, 174, 186â€“187, 194,
200â€“202, 205f
U
Uncertainty, 10f , 17, 26, 28, 30, 33, 107,
162â€“163, 165, 172
Y
Yaw dynamics model, 207


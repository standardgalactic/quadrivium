
Fast Reliable Algorithms
for Matrices with Structure

This page intentionally left blank 

Fast Reliable Algorithms
for Matrices with Structure
Edited by
T. Kailath
Stanford University
Stanford, California
A. H. Sayed
University of California
Los Angeles, California
Society for Industrial and Applied Mathematics
Philadelphia

Copyright Â© 1999 by Society for Industrial and Applied Mathematics.
1 0 9 8 7 6 5 4 3 2 1
All rights reserved. Printed in the United States of America. No part of this book may
be reproduced, stored, or transmitted in any manner without the written permission of
the publisher. For information, write to the Society for Industrial and Applied Mathematics,
3600 University City Science Center, Philadelphia, PA 19104-2688.
Library of Congress Cataloging-in-Publication Data
Fast reliable algorithms for matrices with structure / edited by
T. Kailath, A.M. Sayed
p. cm.
Includes bibliographical references (p. - ) and index.
ISBN 0-89871-431-1 (pbk.)
1. Matrices - Data processing. 2. Algorithms. 
I. Kailath,
Thomas. II. Sayed, Ali H.
QA188.F38 1999
512.9'434--dc21 
99-26368
CIP
rev.
513J1L is a registered trademark.

CONTRIBUTORS
Dario A. BINI
Dipartimento di Matematica
Universita di Pisa
Pisa, Italy
Sheryl BRANHAM
Dept. Math, and Computer Science
Lehman College
City University of New York
New York, NY 10468, USA
Richard P. BRENT
Oxford University Computing Laboratory
Wolfson Building, Parks Road
Oxford OX1 3QD, England
Raymond H. CHAN
Department of Mathematics
The Chinese University of Hong Kong
Shatin, Hong Kong
Shivkumar CHANDRASEKARAN
Dept. Electrical and Computer Engineering
University of California
Santa Barbara, CA 93106, USA
Patrick DEWILDE
DIMES, POB 5031, 2600GA Delft
Delft University of Technology
Delft, The Netherlands
Victor S. GRIGORASCU
Facultatea de Electronica and Telecomunicatii
Universitatea Politehnica Bucuresti
Bucharest, Romania
Thomas KAILATH
Department of Electrical Engineering
Stanford University
Stanford, CA 94305, USA
Beatrice MEINI
Dipartimento di Matematica
Universita di Pisa
Pisa, Italy
Victor Y. PAN
Dept. Math, and Computer Science
Lehman College
City University of New York
New York, NY 10468, USA
Michael K. NG
Department of Mathematics
The University of Hong Kong
Pokfulam Road, Hong Kong
Phillip A. REGALIA
Signal and Image Processing Dept.
Inst. National des Telecommunications
F-91011 Evry cedex, France
Rhys E. ROSHOLT
Dept. Math, and Computer Science
City University of New York
Lehman College
New York, NY 10468, USA
Ali H. SAVED
Electrical Engineering Department
University of California
Los Angeles, CA 90024, USA
Paolo TILLI
Scuola Normale Superiore
Piazza Cavalier! 7
56100 Pisa, Italy
Ai-Long ZHENG
Deptartment of Mathematics
City University of New York
New York, NY 10468, USA
v

This page intentionally left blank 

x 
Contents
5.3 Iterative Methods for Solving Toeplitz Systems 
121
5.3.1 
Preconditioning 
122
5.3.2 
Circulant Matrices 
123
5.3.3 
Toeplitz Matrix-Vector Multiplication 
124
5.3.4 
Circulant Preconditioners 
125
5.4 Band-Toeplitz Preconditioners 
130
5.5 Toeplitz-Circulant Preconditioners 
132
5.6 Preconditioners for Structured Linear Systems 
133
5.6.1 
Toeplitz-Like Systems 
133
5.6.2 
Toeplitz-Plus-Hankel Systems 
137
5.7 Toeplitz-Plus-Band Systems 
139
5.8 Applications 
140
5.8.1 
Linear-Phase Filtering 
140
5.8.2 
Numerical Solutions of Biharmonic Equations 
142
5.8.3 
Queueing Networks with Batch Arrivals 
144
5.8.4 
Image Restorations 
147
5.9 Concluding Remarks 
149
5.A Proof of Theorem 5.3.4 
150
5.B Proof of Theorem 5.6.2 
151
6 ASYMPTOTIC SPECTRAL DISTRIBUTION OF TOEPLITZ-
RELATED MATRICES 
153
Paolo Tilli
6.1 Introduction 
153
6.2 What Is Spectral Distribution? 
153
6.3 Toeplitz Matrices and Shift Invariance 
157
6.3.1 
Spectral Distribution of Toeplitz Matrices 
158
6.3.2 
Unbounded Generating Function 
162
6.3.3 
Eigenvalues in the Non-Hermitian Case 
163
6.3.4 
The Szego Formula for Singular Values 
164
6.4 Multilevel Toeplitz Matrices 
166
6.5 Block Toeplitz Matrices 
170
6.6 Combining Block and Multilevel Structure 
174
6.7 Locally Toeplitz Matrices 
175
6.7.1 
A Closer Look at Locally Toeplitz Matrices 
178
6.7.2 
Spectral Distribution of Locally Toeplitz Sequences 
182
6.8 Concluding Remarks 
186
7 NEWTON'S ITERATION FOR STRUCTURED MATRICES 
189
Victor Y. Pan, Sheryl Branham, Rhys E. Rosholt, and Ai-Long Zheng
7.1 Introduction 
189
7.2 Newton's Iteration for Matrix Inversion 
190
7.3 Some Basic Results on Toeplitz-Like Matrices 
192
7.4 The Newton-Toeplitz Iteration 
194
7.4.1 
Bounding the Displacement Rank 
195
7.4.2 
Convergence Rate and Computational Complexity 
196
7.4.3 An Approach Using /-Circulant Matrices 
198
7.5 Residual Correction Method 
200
7.5.1 
Application to Matrix Inversion 
200
7.5.2 
Application to a Linear System of Equations 
201

Contents 
xi
7.5.3 
Application to a Toeplitz Linear System of Equations 
201
7.5.4 
Estimates for the Convergence Rate 
203
7.6 Numerical Experiments 
204
7.7 Concluding Remarks 
207
7.A Correctness of Algorithm 7.4.2 
208
7.B Correctness of Algorithm 7.5.1 
209
7.C Correctness of Algorithm 7.5.2 
209
8 FAST ALGORITHMS WITH APPLICATIONS TO MARKOV
CHAINS AND QUEUEING MODELS 
211
Dario A. Bini and Beatrice Meini
8.1 Introduction 
211
8.2 Toeplitz Matrices and Markov Chains 
212
8.2.1 
Modeling of Switches and Network Traffic Control 
214
8.2.2 
Conditions for Positive Recurrence 
215
8.2.3 
Computation of the Probability Invariant Vector 
216
8.3 Exploitation of Structure and Computational Tools 
217
8.3.1 
Block Toeplitz Matrices and Block Vector Product 
218
8.3.2 
Inversion of Block Triangular Block Toeplitz Matrices 
221
8.3.3 
Power Series Arithmetic 
223
8.4 Displacement Structure 
224
8.5 Fast Algorithms 
226
8.5.1 
The Fast Ramaswami Formula 
227
8.5.2 
A Doubling Algorithm 
227
8.5.3 
Cyclic Reduction 
230
8.5.4 
Cyclic Reduction for Infinite Systems 
234
8.5.5 
Cyclic Reduction for Generalized Hessenberg Systems 
239
8.6 Numerical Experiments 
241
9 TENSOR DISPLACEMENT STRUCTURES AND POLYSPECTRAL
MATCHING 
245
Victor S. Grigorascu and Phillip A. Regalia
9.1 Introduction 
245
9.2 Motivation for Higher-Order Cumulants 
245
9.3 Second-Order Displacement Structure 
249
9.4 Tucker Product and Cumulant Tensors 
251
9.5 Examples of Cumulants and Tensors 
254
9.6 Displacement Structure for Tensors 
257
9.6.1 
Relation to the Polyspectrum 
258
9.6.2 
The Linear Case 
261
9.7 Polyspectral Interpolation 
264
9.8 A Schur-Type Algorithm for Tensors 
268
9.8.1 
Review of the Second-Order Case 
268
9.8.2 
A Tensor Outer Product 
269
9.8.3 
Displacement Generators 
272
9.9 Concluding Remarks 
275

xii 
Contents
10 MINIMAL COMPLEXITY REALIZATION 
OF STRUCTURED
MATRICES 
277
Patrick Dewilde
10.1 Introduction 
277
10.2 Motivation of Minimal Complexity Representations 
278
10.3 Displacement Structure 
279
10.4 Realization Theory for Matrices 
280
10.4.1 Nerode Equivalence and Natural State Spaces 
283
10.4.2 Algorithm for Finding a Realization 
283
10.5 Realization of Low Displacement Rank Matrices 
286
10.6 A Realization for the Cholesky Factor 
289
10.7 Discussion 
293
A USEFUL MATRIX RESULTS 
297
Thomas Kailath and Ali H. Sayed
A.I Some Matrix Identities 
298
A.2 The Gram-Schmidt Procedure and the QR Decomposition 
303
A.3 Matrix Norms 
304
A.4 Unitary and ./-Unitary Transformations 
305
A.5 Two Additional Results 
306
B ELEMENTARY TRANSFORMATIONS 
309
Thomas Kailath and Ali H. Sayed
B.I Elementary Householder Transformations 
310
B.2 Elementary Circular or Givens Rotations 
312
B.3 Hyperbolic Transformations 
314
BIBLIOGRAPHY 
321
INDEX 
339

PREFACE
The design of fast and numerically reliable algorithms for large-scale matrix problems
with structure has become an increasingly important activity, especially in recent years,
driven by the ever-increasing complexity of applications arising in control, communica-
tions, computation, and signal processing.
The major challenge in this area is to develop algorithms that blend speed and nu-
merical accuracy. These two requirements often have been regarded as competitive, so
much so that the design of fast and numerically reliable algorithms for large-scale struc-
tured linear matrix equations has remained a significant open issue in many instances.
This problem, however, has been receiving increasing attention recently, as witnessed
by a series of international meetings held in the last three years in Santa Barbara (USA,
Aug. 1996), Cortona (Italy, Sept. 1996), and St. Emilion (Prance, Aug. 1997). These
meetings provided a forum for the exchange of ideas on current developments, trends,
and issues in fast and reliable computing among peer research groups. The idea of this
book project grew out of these meetings, and the chapters are selections from works
presented at the meetings. In the process, several difficult decisions had to be made;
the editors beg the indulgence of participants whose contributions could not be included
here.
Browsing through the chapters, the reader soon will realize that this project is
unlike most edited volumes. The book is not merely a collection of submitted articles;
considerable effort went into blending the several chapters into a reasonably consistent
presentation. We asked each author to provide a contribution with a significant tutorial
value. In this way, the chapters not only provide the reader with an opportunity to
review some of the most recent advances in a particular area of research, but they do
so with enough background material to put the work into proper context. Next, we
carefully revised and revised again each submission to try to improve both clarity and
uniformity of presentation. This was a substantial undertaking since we often needed to
change symbols across chapters, to add cross-references to other chapters and sections,
to reorganize sections, to reduce redundancy, and to try to state theorems, lemmas, and
algorithms uniformly across the chapters. We did our best to ensure a uniformity of
presentation and notation but, of course, errors and omissions may still exist and we
apologize in advance for any of these. We also take this opportunity to thank the authors
for their patience and for their collaboration during this time-consuming process. In
all we believe the book includes a valuable collection of chapters that cover in some
detail different aspects of the most recent trends in the theory of fast algorithms, with
emphasis on implementation and application issues.
The book may be divided into four distinct parts:
1. The first four chapters deal with fast direct methods for the triangular factorization
xiii

xiv 
Preface
of structured matrices, as well as the solution of structured linear systems of
equations. The emphasis here is mostly on the generalized Schur algorithm, its
numerical properties, and modifications to ensure numerical stability.
2. Chapters 5, 6, and 7 deal with fast iterative methods for the solution of structured
linear systems of equations. The emphasis here is on the preconditioned conjugate
gradient method and on Newton's method.
3. Chapters 8 to 10 deal with extensions of the notion of structure to the block
case, the tensor case, and to the input-output framework. Chapter 8 presents
fast algorithms for block Toeplitz systems of equations and considers applications
in Markov chains and queueing theory. Chapter 9 studies tensor displacement
structure and applications in polyspectral interpolation. Chapter 10 discusses
realization theory and computational models for structured problems.
4. We have included two appendices that collect several useful matrix results that
are used in several places in the book.
Acknowledgments. We gratefully acknowledge the support of the Army Research Of-
fice and the National Science Foundation in funding the organization of the Santa Bar-
bara Workshop. Other grants from these agencies, as well as from the Defense Advanced
Research Projects Agency and the Air Force Office of Scientific Research, supported the
efforts of the editors on this project. We are also grateful to Professors Alan Laub of
University of California Davis and Shivkumar Chandrasekaran of University of Califor-
nia Santa Barbara for their support and joint organization with the editors of the 1996
Santa Barbara Workshop. It is also a pleasure to thank Professors M. Najim of the
University of Bordeaux and P. Dewilde of Delft University, for their leading role in the
St. Emilion Workshop, and Professor D. Bini of the University of Pisa and several of
his Italian colleagues, for the fine 1996 Toeplitz Workshop in Cortona.
October 1998
T. Kailath 
A. H. Sayed
Stanford, CA 
Westwood, CA

NOTATION
N 
The set of natural numbers.
Z 
The set of integer numbers.
R 
The set of real numbers.
C 
The set of complex numbers.
0 
The empty set.
C-2-n 
The set of 27r-periodic complex-valued continuous
functions defined on [â7r,7r].
Co(M) 
The set of complex-valued continuous functions
with bounded support in R.
Cb(R) 
The set of bounded and uniformly continuous
complex-valued functions over R.
â¢T 
Matrix transposition.
â¢* 
Complex conjugation for scalars and conjugate
transposition for matrices.
a = b 
The quantity a is defined as b.
col{a, 6} 
A column vector with entries a and b.
diag{a, 6} 
A diagonal matrix with diagonal entries a and b.
tridiag{a, 6, c} 
A tridiagonal Toeplitz matrix with b along its diagonal,
a along its lower diagonal, and c along its upper diagonal,
a 0 6 
The same as diag{a, b}.
1 
v^
\x] 
The smallest integer ra > x.
[x\ 
The largest integer m < x.
0 
A zero scalar, vector, or matrix.
In 
The identify matrix of size n x n.
Â£(x) 
A lower triangular Toeplitz matrix whose first column is x.
<C> 
The end of a proof, an example, or a remark.
XV

xvi 
Notation
|| â¢ || 2 
The Euclidean norm of a vector or the maximum
singular value of a matrix.
|| â¢ ||i 
The sum of the absolute values of the entries of a
vector or the maximum absolute column sum of a matrix.
|| â¢ 11 oo 
The largest absolute entry of a vector or the maximum
absolute row sum of a matrix.
|| â¢ ||p 
The Probenius norm of a matrix.
|| â¢ || 
Some vector or matrix norm.
\A\ 
A matrix with elements \a,ij\.
\i(A) 
ith eigenvalue of A.
(7i(A) 
ith singular value of A.
K(A) 
Condition number of a matrix A, given by HA^H-A"1!^.
coudk(A) 
Equal to H^llfcllA-1^.
e 
Machine precision.
O(n) 
A constant multiple of n, or of the order of n.
On(e) 
O(ec(n)}, where c(n) is some polynomial in n.
~ 
A computed quantity in a finite precision algorithm.
7 
An intermediate exact quantity in a finite precision algorithm.
CG 
The conjugate gradient method.
LDU 
The lower-diagonal-upper triangular factorization of a matrix.
PCG 
The preconditioned conjugate gradient method.
QR 
The QR factorization of a matrix.

Chapter 1
DISPLACEMENT STRUCTURE
AND ARRAY ALGORITHMS
Thomas Kailath
1.1 
INTRODUCTION
Many problems in engineering and applied mathematics ultimately require the solu-
tion of n x n linear systems of equations. For small-size problems, there is often
not much else to do except to use one of the already standard methods of solution
such as Gaussian elimination. However, in many applications, n can be very large
(n ~ 1000, n ~ 1,000,000) and, moreover, the linear equations may have to be solved
over and over again, with different problem or model parameters, until a satisfactory
solution to the original physical problem is obtained. In such cases, the O(n3) burden,
i.e., the number of flops required to solve an n x n linear system of equations, can become
prohibitively large. This is one reason why one seeks in various classes of applications
to identify special or characteristic structures that may be assumed in order to reduce
the computational burden. Of course, there are several different kinds of structure.
A special form of structure, which already has a rich literature, is sparsity; i.e.,
the coefficient matrices have only a few nonzero entries. We shall not consider this
already well studied kind of structure here. Our focus will be on problems, as generally
encountered in communications, control, optimization, and signal processing, where the
matrices are not sparse but can be very large. In such problems one seeks further
assumptions that impose particular patterns among the matrix entries. Among such
assumptions (and we emphasize that they are always assumptions) are properties such
as time-invariance, homogeneity, stationarity, and rationality, which lead to familiar
matrix structures, such as Toeplitz, Hankel, Vandermonde, Cauchy, Pick, etc. Several
fast algorithms have been devised over the years to exploit these special structures. The
numerical (accuracy and stability) properties of several of these algorithms also have
been studied, although, as we shall see from the chapters in this volume, the subject is by
no means closed even for such familiar objects as Toeplitz and Vandermonde matrices.
In this book, we seek to broaden the above universe of discourse by noting that even
more common than the explicit matrix structures, noted above, are matrices in which
the structure is implicit. For example, in least-squares problems one often encounters
products of Toeplitz matrices; these products generally are not Toeplitz, but on the other
hand they are not "unstructured." Similarly, in probabilistic calculations the matrix of
interest often is not a Toeplitz covariance matrix, but rather its inverse, which is rarely
1

2 
Displacement Structure and Array Algorithms 
Chapter 1
Toeplitz itself, but of course is not unstructured: its inverse is Toeplitz. It is well known
that O(n2) flops suffice to solve linear systems with an n x n Toeplitz coefficient matrix;
a question is whether we will need O(n3) flops to invert a non-Toeplitz coefficient matrix
whose inverse is known to be Toeplitz. When pressed, one's response clearly must be
that it is conceivable that O(n2) flops will suffice, and we shall show that this is in fact
true.
Such problems, and several others that we shall encounter in later chapters, sug-
gest the need for a quantitative way of defining and identifying structure in (dense)
matrices. Over the years we have found that an elegant and useful way is the con-
cept of displacement structure. This has been useful for a host of problems apparently
far removed from the solution of linear equations, such as the study of constrained and
unconstrained rational interpolation, maximum entropy extension, signal detection, sys-
tem identification, digital filter design, nonlinear Riccati differential equations, inverse
scattering, certain Fredholm and Wiener-Hopf integral equations, etc. However, in this
book we shall focus attention largely on displacement structure in matrix computations.
For more general earlier reviews, we may refer to [KVM78], [Kai86], [Kai91], [HR84],
[KS95a].
1.2 TOEPLITZ MATRICES
The concept of displacement structure is perhaps best introduced by considering the
much-studied special case of a Hermitian Toeplitz matrix,
The matrix T has constant entries along its diagonals and, hence, it depends only on
n parameters rather than n2. As stated above, it is therefore not surprising that many
matrix problems involving T, such as triangular factorization, orthogonalization, and
inversion, have solution complexity O(n2) rather than O(n3) operations. The issue is
the complexity of such problems for inverses, products, and related combinations of
Toeplitz matrices such as r-1,TiT2,Ti - T2T^1T^ (TiT2)~lT3 .... As mentioned ear-
lier, although these are not Toeplitz, they are certainly structured and the complexity
of inversion and factorization may be expected to be not much different from that for
a pure Toeplitz matrix, T. It turns out that the appropriate common property of all
these matrices is not their "Toeplitzness," but the fact that they all have (low) displace-
ment rank in a sense first defined in [KKM79a], [KKM79b] and later much studied and
generalized. When the displacement rank is r, r < n, the solution complexity of the
above problems turns out to be O(rn2). Now for some formal definitions.
The displacement of a Hermitian matrix R = [^"J^o â¬ Cnxn was originally1
defined in [KKM79a], [KKM79b] as
1 Other definitions will be introduced later. We may note that the concept was first identified in
studying integral equations (see, e.g., [KLM78]).

Section 1.2. Toeplitz Matrices 
3
where * denotes Hermitian conjugation (complex conjugation for scalars) and Z is the
n x n lower shift matrix with ones on the first subdiagonal and zeros elsewhere,
The product ZRZ* then corresponds to shifting R downward along the main diagonal
by one position, explaining the name displacement for VzR~ The situation is depicted
in Fig. 1.1.
Figure 1.1. VzR is obtained by shifting R downward along the diagonal.
If VzR has (low) rank, say, r, independent of n, then R is said to be structured
with respect to the displacement V^ defined by (1.2.2), and r is called the displacement
rank of R. The definition can be extended to non-Hermitian matrices, and this will be
briefly described later. Here we may note that in the Hermitian case, V^jR is Hermitian
and therefore has further structure: its eigenvalues are real and so we can define the
displacement inertia of R as the pair {p, q}, where p (respectively, q) is the number of
strictly positive (respectively, negative) eigenvalues of V^-R. Of course, the displacement
rank is r = p -f q- Therefore, we can write
where J = J* = (Ip@âIq) is a signature matrix and G e Cnxr. The pair [G, J} is called
a Vz-generator of R. This representation is clearly not unique; for example, {G0, J}
is also a generator for any J-unitary matrix B (i.e., for any 6 such that 9JO* = J).
This is because
Nonminimal generators (where G has more than r columns) are sometimes useful, al-
though we shall not consider them here.
Returning to the Toeplitz matrix (1.2.1), it is easy to see that T has displace-
ment rank 2, except when all Ci, i ^ 0, are zero, a case we shall exclude. Assuming

4 
Displacement Structure and Array Algorithms 
Chapter 1
CQ = 1, a generator for T is {:co,yo,(l Â© â!)}> where rro = col{l,ci,... ,cn_i} and
yQ = col{0, ci,..., cn_i} (the notation col{-} denotes a column vector with the specified
entries):
It will be shown later that if we define T# = IT 1I, where / denotes the reversed
identity with ones on the reversed diagonal and zeros elsewhere, then T# also has Vz-
displacement inertia {!,!}â¢ The product T\T<2 of two Toeplitz matrices, which may
not be Hermitian, will be shown to have displacement rank < 4. The significance of
displacement rank with respect to the solution of linear equations is that the complexity
can be reduced to O(rn2) from O(n3).
The well-known Levinson algorithm [Lev47] is one illustration of this fact. The best-
known form of this algorithm (independently obtained by Durbin [Dur59]) refers to the
so-called Yule-Walker system of equations
where an = [ an,n an,n-i 
â¢â¢â¢ Â«n,iÂ»l ] and cr2 are the (n + 1) unknowns and Tn
is a positive-definite (n + 1) x (n + 1) Toeplitz matrix. The easily derived and now
well-known recursions for the solution are
The above recursions are closely related to certain (nonrecursive2) formulas given by
Szego [Sze39] and Geronimus [Ger54] for polynomials orthogonal on the unit circle,
as discussed in some detail in [KaiQl]. It is easy to check that the {7*} are all less
than one in magnitude; in signal processing applications, they are often called reflection
coefficients (see, e.g., [Kai85], [Kai86]).
While the Levinson-Durbin algorithm is widely used, it has limitations for certain
applications. For one thing, it requires the formation of inner products and therefore
is not efficiently parallelizable, requiring O(nlogn) rather than O(n) flops, with O(n)
processors. Second, while it can be extended to indefinite and even non-Hermitian
Toeplitz matrices, it is difficult to extend it to non-Toeplitz matrices having displace-
ment structure. Another problem is numerical. An error analysis of the algorithm in
[CybSO] showed that in the case of positive reflection coefficients {7*}, the residual er-
ror produced by the Levinson-Durbin procedure is comparable to the error produced
2They defined 7Â» as â ai+i,Â»+i.
and
where

Section 1.2. Toeplitz Matrices 
5
by the numerically well-behaved Cholesky factorization [GV96, p. 191]. Thus in this
special case the Levinson-Durbin algorithm is what is called weakly stable, in the sense
of [Bun85], [Bun87]âsee Sec. 4.2 of this book. No stability results seem to be available
for the Levinson-Durbin algorithm for Toeplitz matrices with general {7i}.
To motivate an alternative (parallelizable and stable) approach to the problem, we
first show that the Levinson-Durbin algorithm directly yields a (fast) triangular fac-
torization of T~l. To show this, note that stacking the successive solutions of the
Yule-Walker equations (1.2.6) in a lower triangular matrix yields the equality
which, using the Hermitian nature of T, yields the unique triangular factorization of
the inverse of Tn:
where Dn = diag{crg,a\,...,a^}.
However, it is a fact, borne out by results in many different problems, that ultimately
even for the solution of linear equations the (direct) triangular factorization of T rather
than T~l is more fundamental. Such insights can be traced back to the celebrated
Wiener-Hopf technique [WH31] but, as perhaps first noted by Von Neumann and by
Turing (see, e.g., [Ste73]), direct factorization is the key feature of the fundamental
Gaussian elimination method for solving linear equations and was effectively noted as
such by Gauss himself (though of course not in matrix notation).
Now a fast direct factorization of Tn cannot be obtained merely by inverting the
factorization (1.2.10) of T~l, because that will require O(n3) flops. The first fast al-
gorithm was given by Schur [Schl7], although this fact was realized only much later in
[DVK78]. In the meantime, a closely related direct factorization algorithm was derived
by Bareiss [Bar69], and this is the designation used in the numerical analysis commu-
nity. Morf [Mor70], [Mor74], Rissanen [Ris73], and LeRoux-Gueguen [LG77] also made
independent rediscoveries.
We shall show in Sec. 1.7.7 that the Schur algorithm can also be applied to solve
Toeplitz linear equations, at the cost of about 30% more computations than via the
Levinson-Durbin algorithm. However, in return we can compute the reflection coef-
ficients without using inner products, the algorithm has better numerical properties

6
Displacement Structure and Array Algorithms 
Chapter 1
(see Chs. 2-4 and also [BBHS95], [CS96]), and as we shall show below, it can be ele-
gantly and usefully extended by exploiting the concept of displacement structure. These
generalizations are helpful in solving the many classes of problems (e.g., interpolation)
mentioned earlier (at the end of Sec. 1.1). Therefore, our major focus in this chapter
will be on what we have called generalized Schur algorithms.
First, however, let us make a few more observations on displacement structure.
1.3 VERSIONS OF DISPLACEMENT STRUCTURE
There are of course other kinds of displacement structure than those introduced in
Sec. 1.2, as already noted in [KKM79a]. For example, it can be checked that
where Z-\ denotes the circulant matrix with first row [ 0 ... 0 
â 1 ]. This fact has
been used by Heinig [Hei95] and by [GKO95] to obtain alternatives to the Levinson-
Durbin and Schur algorithms for solving Toeplitz systems of linear equations, as will
be discussed later in this chapter (Sec. 1.13). However, a critical point is that because
Z_i is not triangular, these methods apply only to fixed n, and the whole solution has
to be repeated if the size is increased even by one. Since many applications in commu-
nications, control, and signal processing involve continuing streams of data, recursive
triangular factorization is often a critical requirement. It can be shown [LK86] that such
factorization requires that triangular matrices be used in the definition of displacement
structure, which is what we shall do henceforth.
One of the first extensions of definition (1.2.2) was to consider
where, for reasons mentioned above, F is a lower triangular matrix; see [LK84], [CKL87].
One motivation for such extensions will be seen in Sec. 1.8. Another is that one can
include matrices such as Vandermonde, Cauchy, Pick, etc. For example, consider the
so-called Pick matrix, which occurs in the study of analytic interpolation problems,
where {iii,t>i} are row vectors of dimensions p and q, respectively, and fi are complex
points inside the open unit disc (|/$| < 1). If we let F denote the diagonal matrix
diag{/o, /i,..., /n-i}, then it can be verified that P has displacement rank (p + q) with
respect to F since
In general, one can write for Hermitian R â¬ Cnxn,
for some triangular F â¬ Cnxn, a signature matrix J = (Ip Â© -Iq] e Crxr, and
G e Cnxr, with r independent of n. The pair {G, J} will be called a V^-generator

Section 1.3. Versions of Displacement Structure 
7
of R. Because Toeplitz and, as we shall see later, several Toeplitz-related matrices are
best studied via this definition, matrices with low V^-displacement rank will be called
Toeplitz-like. However, this is strictly a matter of convenience.
We can also consider non-Hermitian matrices R, in which case the displacement can
hp HpfinpH as
where F and A are n x n lower triangular matrices. In some cases, F and A may
coincideâsee (1.3.7) below. When Vp,AR has low rank, say, r, we can factor it
(nonuniquely) as
where G and B are also called generator matrices,
One particular example is the case of a non-Hermitian Toeplitz matrix T = [cj_j]â¢ ._0 ,
which can be seen to satisfy
This is a special case of (1.3.6) with F = A = Z.
A second example is a Vandermonde matrix,
which can be seen to satisfy
where F is now the diagonal matrix F = diag {cti,..., an}.
Another common form of displacement structure, first introduced by Heinig and
Rost [HR84], is what we call, again strictly for convenience, a Hankel-like structure. We
shall say that a matrix R â¬ Cnxn is Hankel-like if it satisfies a displacement equation
of the form
for some lower triangular F e Cnxn and A e Cnxn, and generator matrices G <E Cnxr
and B â¬ Cnxr, with r independent of n. When R is Hermitian, it is more convenient
to express the displacement equation as

8
Displacement Structure and Array Algorithms 
Chapter 1
for some generator matrix G â¬ Cnxr and signature matrix J that satisfies J = J* and
J2 = 7. To avoid a notation explosion, we shall occasionally use the notation Vp,AR
for both Toeplitz-like and Hankel-like structures.
As an illustration, consider a Hankel matrix, which is a symmetric matrix with real
constant entries along the antidiagonals,
(1.3.13)
We therefore say that H has displacement rank 2 with respect to the displacement
operation (1.3.11) with F = iZ and J as above. Here, i = \/~l and is introduced in
order to obtain a J that satisfies the normalization conditions J = J*, J2 = /.
A problem that arises here is that H cannot be fully recovered from its displacement
representation, because the entries {/in-i, â¢ - -, ^2n-2J do not appear in (1.3.14). This
"difficulty" can be accommodated in various ways (see, e.g., [CK91b], [HR84], [KS95a]).
One way is to border H with zeros and then form the displacement, which will now have
rank 4. Another method is to form the 2n x In (triangular) Hankel matrix with top
row {/IQ, ..., /i2n-i}; now the displacement rank will be two; however, note that in both
cases the generators have the same number of entries. In general, the problem is that
the displacement equation does not have a unique solution. This will happen when the
displacement operator in (1.3.3)-(1.3.6) or (1.3.10)-(1.3.11) has a nontrivial nullspace
or kernel. In this case, the generator has to be supplemented by some additional infor-
mation, which varies from case to case. A detailed discussion, with several examples, is
given in [KO96], [KO98].
Other examples of Hankel-like structures include Loewner matrices, Cauchy matri-
ces, and Cauchy-like matrices, encountered, for example, in the study of unconstrained
rational interpolation problems (see, e.g., [AA86], [Fie85], [Vav91]). The entries of an
n x n Cauchy-like matrix R have the form
It can be verified that the difference ZH â HZ* has rank 2 since

Section 1.3. 
Versions of Displacement Structure 
9
where Ui and Vj denote 1 xr row vectors and the {/$, a,i} are scalars. The Loewner matrix
is a special Cauchy-like matrix that corresponds to the choices r = 2, HI â [ fa 1 ],
and V{ = \ 1 
â Ki ], and, consequently, u^ = fa â & :
where A is the diagonal matrix (assuming ai ^ 0)
Clearly, the distinction between Toeplitz-like and Hankel-like structures is not very
tight, since many matrices can have both kinds of structure including Toeplitz matrices
themselves (cf. (1.2.5) and (1.3.1)).
Toeplitz- and Hankel-like structures can be regarded as special cases of the general-
ized displacement structure [KS91], [Say92], [SK95a], [KS95a]:
where {fi,A,F, A} are n x n and {(7, B} are n x r. Such equations uniquely define
R when the diagonal entries {ui,Si,fi,di} of the displacement operators {fi,A,F, A}
satisfy
This explains the difficulty we had in the Hankel case, where the diagonal entries of
F = A = Z in (1.3.14) violate the above condition. The restriction that {Â£), A, F, A}
are lower triangular is the most general one that allows recursive triangular factorization
(cf. a result in [LK86]). As mentioned earlier, since this is a critical feature in most of
our applications, we shall assume this henceforth.
Cauchy matrices, on the other hand, arise from the choices r = 1 and u^ = 1 = Vi :
It is easy to verify that a Cauchy-like matrix is Hankel-like since it satisfies a displace-
ment equation of the form
where F and A are diagonal matrices:
F = diagonal {/0,..., /n-i}, A = diagonal {a0,..., an_i}.
Hence, Loewner and Cauchy matrices are also Hankel-like. Another simple example is
the Vandermonde matrix (1.3.8) itself, since it satisfies not only (1.3.9) but also

10 
Displacement Structure and Array Algorithms 
Chapter 1
The Generating Function Formulation
We may remark that when {Q, A, F, A} are lower triangular Toeplitz, we can use gen-
erating function notationâsee [LK84], [LK86]; these can be extended to more general
{rfc,A,F, A} by using divided difference matricesâsee [Lev83], [Lev97]. The generat-
ing function formulation enables connections to be made with complex function theory
and especially with the extensive theory of reproducing kernel Hilbert spaces of entire
functions (deBranges spaces)âsee, e.g., [AD86], [Dym89b], [AD92].
Let us briefly illustrate this for the special cases of Toeplitz and Hankel matrices,
T = [ci-j], H = [hi+j]. To use the generating function language, we assume that the
matrices are semi-infinite, i.e., i,j 6 [0, oo). Then straightforward calculation will yield,
assuming CQ == 1, the expression
where c(z) is (a so-called Caratheodory function)
The expression can also be rewritten as
where
In the Hankel case, we can write
where
and
Generalizations can be obtained by using more complex (G(-), J} matrices and with
However, to admit recursive triangular factorization, one must asume that d(z, w) has
the form (see [LK86])

Section 1.3. 
Versions of Displacement Structure 
11
for some {a(.z),b(z)}. 
The choice of d(z,w) also has a geometric significance. For
example, di(z, w} partitions the complex plane with respect to the unit circle, as follows:
Similarly, 6^2(2, w) partitions the plane with respect to the real axis. If we used ^3(2, w) =
z -f w*, we would partition the plane with respect to the imaginary axis.
We may also note that the matrix forms of
will be, in an obvious notation,
while using d-2(z,w] it will be
Here, Z denotes the semi-infinite lower triangular shift matrix. Likewise, {7Â£, <?i,Â£/2}
denote semi-infinite matrices.
We shall not pursue the generating function descriptions further here. They are
useful, inter alia, for studying root distribution problems (see, e.g., [LBK91]) and, as
mentioned above, for making connections with the mathematical literature, especially
the Russian school of operator theory. A minor reason for introducing these descriptions
here is that they further highlight connections between displacement structure theory
and the study of discrete-time and continuous-time systems, as we now explain briefly.
Lyapunov, Stein, and Displacement Equations
When J = /, (1.3.19) and (1.3.20) are the discrete-time and continuous-time Lyapunov
equations much studied in system theory, where the association between discrete-time
systems and the unit circle and continuous-time systems and half-planes, is well known.
There are also well-known transformations (see [KaiSO, p. 180])between discrete-time
and continuous-time (state-space) systems, so that in principle all results for Toeplitz-
like displacement operators can be converted into the appropriate results for Hankel-like
operators. This is one reason that we shall largely restrict ourselves here to the Toeplitz-
like Hermitian structure (1.3.3); more general results can be found in [KS95a].
A further remark is that equations of the form (1.3.10), but with general right sides,
are sometimes called Sylvester equations, while those of the form (1.3.6) are called Stein
equations. For our studies, low-rank factorizations of the right side as in (1.3.10) and
(1.3.6) and especially (1.3.11) and (1.3.3) are critical (as we shall illustrate immediately),
which is why we call these special forms displacement equations.
Finally, as we shall briefly note in Sec. 1.14.1, there is an even more general version of
displacement theory applying to "time-variant" matrices (see, e.g., [SCK94], [SLK94b],
[CSK95]). These extensions are useful, for example, in adaptive filtering applications
and also in matrix completion problems and interpolation problems, where matrices
change with time but in such a way that certain displacement differences undergo only
low-rank variations.
A Summary
To summarize, there are several ways to characterize the structure of a matrix, using for
example (1.3.6), (1.3.10), or (1.3.16). However, in all cases, the main idea is to describe

12 
Displacement Structure and Array Algorithms 
Chapter 1
an n x n matrix R more compactly by n x r generator matrices {G,J9}, with r <C n.
Since the generators have 2rn entries, as compared to n2 entries in R, a computational
gain of one order of magnitude can in general be expected from algorithms that operate
on the generators directly.
The implications of this fact turn out to be far reaching and have connections with
many other areas; see, e.g., [KS95a] and the references therein. In this chapter, we focus
mainly on results that are relevant to matrix computations and that are also of interest
to the discussions in the later chapters.
The first part of our presentation focuses exclusively on strongly regular Hermitian
Toeplitz-like matrices, viz., those that satisfy
for some full rank nxr matrix G, with r Â«C n, and lower triangular F. We also assume
that the diagonal entries of F satisfy
so that (1.3.21) defines R uniquely. Once the main ideas have been presented for this
case, we shall then briefly state the results for non-Hermitian Toeplitz-like and Hankel-
like matrices. A more detailed exposition for these latter cases, and for generalized
displacement equations (1.3.16), can be found in [Say92], [KS95a], [SK95a].
1.4 APPLICATION TO THE FAST EVALUATION OF
MATRIX-VECTOR PRODUCTS
The evaluation of matrix-vector products is an important ingredient of several fast
algorithms (see, e.g., Chs. 5 and 8), and we discuss it briefly here.
Consider an. n x n Hermitian matrix R with (Toeplitz-like) displacement generator
{Z, G, J} as in (1.2.4). Given G, we can deduce an explicit representation for R in terms
of the columns of G. Indeed, using the fact that Z is a nilpotent matrix, viz., Zn = 0,
we can check that the unique solution of (1.2.4) is
Let us partition the columns of G into two sets
It is then easy to see that (1.4.1) is equivalent to the representation
where the notation C(x) denotes a lower triangular Toeplitz matrix whose first column
is x, e.g.,

Section 1.5. 
Two Fundamental Properties 
13
Formula (1.4.2) expresses matrices R with displacement structure (1.2.4) in terms of
products of triangular Toeplitz matrices. The special cases of Toeplitz matrices and
their inverses and products are nice examples.
Among other applications (see, e.g., [KVM78]), the representation (1.4.2) can be
exploited to speed up matrix-vector products of the form Ra for any column vector a.
In general, such products require O(n2) operations. Using (1.4.2), the computational
cost can be reduced to O(rnlog2n) by using the fast Fourier transform (FFT) tech-
nique. This is because, in view of (1.4.2), evaluating the product Ra involves evaluating
products of lower or upper triangular Toeplitz matrices by vectors, which is equivalent
to a convolution operation. For related applications, see [BP94], [GO94c], and Ch. 5.
1.5 TWO FUNDAMENTAL PROPERTIES
Two fundamental invariance properties that underlie displacement structure theory are
(a) invariance of displacement structure under inversion and
(b) invariance of displacement structure under Schur complementation.
Lemma 1.5.1 (Inversion). If R is an n x n invertible matrix that satisfies (1.3.21)
for some full rank G â¬ Cnxr, then there must exist a full rank matrix H 6 crxn such
that
Proof: The block matrix
admits the following block triangular decompositions (cf. App. A):
Now Sylvester's law of inertia (see also App. A) implies that
It follows from (1.3.21) that there must exist a full rank rxn matrix H such that (1.5.1)
is valid.
An immediate application of the above result is to verify that the inverse of a Toeplitz
matrix also has displacement structure. Indeed, we know from (1.2.5) that for a Hermi-
tian Toeplitz matrix T, Inertia (T â ZTZ*) â (1,1). It then follows from Lemma 1.5.1
that the inertia of (T~l - Z'T^Z) is also (1,1). But IT-1! = T~* (since ITI = T*)
and IZ*I = Z, where I is the reverse identity matrix with ones on the antidiago-
nal and zeros elsewhere. Hence, Inertia (T~* â ZT~*Z*) = (1,1), which shows that
T~* â ZT~*Z* has rank 2 with one positive signature and one negative signature. This
discussion underlies the famous Gohberg-Semencul formula.
It is worth noting that the result of Lemma 1.5.1 requires no special assumptions
(e.g., triangularity) on the matrix F.

14 
Displacement Structure and Array Algorithms 
Chapter 1
The second striking result of displacement structure theory is the following, first
stated deliberately in vague terms:
The Schur complements of a structured matrix R inherit its displacement struc-
ture. Moreover, a so-called generalized Schur algorithm yields generator matrices for
the Schur complements.
In this way, we can justify the low displacement rank property of the Toeplitz matrix
combinations that we listed before in the introduction of Sec. 1.2, viz., TiT%, T\ â
T2T3~1T4, and (T^)"^. Assuming for simplicity square matrices {Ti,T2,T3}, we
note that these combinations are Schur complements of the following extended matrices,
all of which have low displacement ranks (for suitable choices of F):
More formally, the result reads as follows.
Lemma 1.5.2 (Schur Complementation). Consider n x n matrices R and F. As-
sume that F is block lower triangular (Fi and FZ need not be triangular),
partition R accordingly with F,
and assume that RU is invertible. Then
where
Proof: The first inequality follows immediately since RU is a submatrix of R. For the
second inequality we first note that
rank
We now invoke a block matrix formula for R'1 (cf. App. A),
and observe that A"1 is a submatrix of .R"1. Hence,
rank
But by the first result in the lemma we have
rank

Section 1.6. 
Fast Factorization of Structured Matrices 
15
We thus conclude that rank (A - F3AF3*) < rank (R - FRF*).
Hence, it follows from the statement of the lemma that if R has low displacement
rank with respect to the displacement R â FRF*, then its Schur complement A has
low displacement rank with respect to the displacement A â F^AF*,. This result for
F = Z was first noted in [MorSO], and in its extended form it was further developed
in the Ph.D. research of Delosme [Del82], Lev-Ari [Lev83], Chun [Chu89], Pal [Pal90],
Ackner [Ack91], and Sayed [Say92].
1.6 
FAST FACTORIZATION OF STRUCTURED MATRICES
The concept of Schur complements perhaps first arose in the context of triangular fac-
torization of matrices. It was implicit in Gauss's work on linear equations and more
explicit in the remarkable paper of Schur [Schl7]. In this section we consider afresh the
problem of triangular factorization, which is basically effected by the Gaussian elimina-
tion technique, and show that adding displacement structure allows us to speed up the
Gaussian elimination procedure. This will lead us to what we have called generalized
Schur algorithms. We shall develop them here for the displacement structure (1.3.21)
and later state variations that apply to non-Hermitian Toeplitz-like and also Hankel-like
structures. More details on these latter cases can be found in [KS95a]. Our study of
the special case (1.3.21) will be enough to convey the main ideas.
We start by reviewing what we shall call the Gauss-Schur reduction procedure.
1.6.1 
Schur Reduction (Deflation)
The triangular decomposition of a matrix R â¬ Cnxn will be denoted by
where D = diag{do, di, ..., dn-i} is a diagonal matrix and the lower triangular factor
L is normalized in such a way that the {di} appear on its main diagonal. The nonzero
part of the consecutive columns of L will be denoted by li. They can be obtained
recursively as follows.
Algorithm 1.6.1 (Schur Reduction). Given R 6 Cnxn, start with RQ = R and
repeat for i = 0,1,... , n â 1:
1. Let li denote the first column of Ri and di denote the upper-left-corner element of
Ri.
2. Perform the Schur reduction step:
The matrix Ri is known as the Schur complement of the leading i x i block of R.
0
We therefore see that the Schur reduction step (1.6.2) deflates the matrix Ri by
subtracting a rank 1 matrix from it and leads to a new matrix Ri+\ that has one less
row and one less column than Ri.

16 
Displacement Structure and Array Algorithms 
Chapter 1
By successively repeating (1.6.2) we obtain the triangular factorization of J?,
It is also easy to verify that a suitable partitioning of L and D provides triangular
decompositions for the leading principal block of jR and its Schur complement.
Lemma 1.6.1 (Partitioning of the Triangular Decomposition). Assume that R,
L, and D are partitioned as
Then the leading principal block Pi and its Schur complement Ri = Si â QiP^~1Q* admit
the following triangular decompositions:
1.6.2Close Relation to Gaussian Elimination
The Schur reduction procedure is in fact the same as Gaussian elimination. To see this,
consider the first step of (1.6.2):
If we partition the entries of IQ into IQ = col{do5*o}> where to is also a column vector,
then the above equality can be written as
or, equivalently, as
where In-i is the identity matrix of dimension (n â 1). This relation shows why (1.6.2),
which we called Schur reduction, is closely related to Gaussian elimination. Schur
reduction goes more explicitly toward matrix factorization. Note also that the above
Schur reduction procedure can readily be extended to strongly regular non-Hermitian
matrices to yield the so-called LDU decompositions (see, e.g., [KS95a]).

Section 1.6. Fast Factorization of Structured Matrices 
17
1.6.3A Generalized Schur Algorithm
In general, Alg. 1.6.1 requires O(n3) operations to factor R. However, when R has
displacement structure, the computational burden can be significantly reduced by ex-
ploiting the fact that the Schur complements Ri all inherit the displacement structure
o f R .
Schur algorithms can be stated in two different, but equivalent, formsâvia a set
of equations or in a less traditional form as what we call an array algorithm. In the
latter, the key operation is the triangularization by a sequence of elementary unitary
or J-unitary operations of a prearray formed from the data at a certain iteration; the
information needed to form the prearray for the next iteration can be read out from the
entries of the triangularized prearray. No equations, or at most one or two very simple
explicit equations, are needed. We first state a form of the algorithm, before presenting
the derivation.
Algorithm 1.6.2 (A Generalized Schur Algorithm). Given a matrix R â¬ Cnxn
that satisfies (1.3.21) and (1.3.22) for some full rank G â¬ Cnxr, start with Go = G and
perform the following steps for i = 0,..., n â 1 :
1. Let gi be the top row of Gi and let F be partitioned as
That is, Fi is obtained by ignoring the leading i rows and columns of F. Now
compute li by solving the linear system of equations3
Define the top element ofli,
2. Form the prearray shown below and choose a (d~l Â© J)-unitary matrix Â£Â» that
eliminates the top row of Gi:
This will give us a right-hand side as shown, where the matrix Gi+i can be used
to repeat steps 1 and 2; a matrix S^ is (d~l Â© J)-unitary if
Notice that Gi+i has one less row than Gi.
3. Then the {li} define the successive columns ofL, D = diag{dj}, andR = LD~1L*.
0
3Fi is not only triangular, but in many applications it is usually sparse and often diagonal or
bidiagonal, so that (1.6.7) is easy to solve and (see (1.6.9)) Fili is easy to form. For example, whenever
F is strictly lower triangular, say, F = ZorF = ZÂ®Z, then li = GiJg*.

18 
Displacement Structure and Array Algorithms 
Chapter 1
Remark 1. Unitary transformations can have several forms, which are discussed in App. B.
A graphic depiction of the algorithm will be given in Sec. 1.6.6. When the matrix F is sparse
enough (e.g., diagonal or bidiagonal), so that the total number of flops required for solving the
linear system (1.6.7) is O(n â i), then the computational complexity of Alg. 1.6.2 is readily
seen to be O(rn2).
Remark 2. We shall show in Lemma 1.6.3 that the matrices {Gi} that appear in the state-
ment of the algorithm are in fact generator matrices for the successive Schur complements of
R, viz.,
Remark 3 (An Explicit Form). It can be shown that Gi+i can be obtained from d by an
explicit calculation:
where Â©i is any J-unitary matrix, and <&i is the so-called Blaschke-Potapov matrix
Remark 4. 
Although the above statement is for Hermitian matrices that satisfy displacement
equations of the form R â FRF* = GJG*, there are similar algorithms for non-Hermitian
Toeplitz-like matrices and also for Hankel-like matrices (see Sees. 1.11 and 1.12). The dis-
cussion we provide in what follows for the Hermitian Toeplitz-like case highlights most of the
concepts that arise in the study of structured matrices.
Remark 5 (Terminology). The ultimate conclusion is that the above generalized Schur
algorithm is the result of combining displacement structure with Gaussian elimination in order
to speed up the computations. We have called it a (rather than the) generalized Schur algorithm
because there are many variations that can be obtained by different choices of the matrices
{Ei, 0t} and for different forms of displacement structure. Generalized Schur algorithms for the
general displacement (1.3.16) can be found in [KS91], [KS95a], [SK95a]. Finally, we mention
that the classical (1917) Schur algorithm is deduced as a special case in Sec. 1.7.6.
1.6.4Array Derivation of the Algorithm
The equation forms of the algorithm (i.e., (1.6.7) and (1.6.11)) can be derived in several
different waysâsee, e.g., [Lev83], [LK84], [LK92], [Say92], [KS95a], and the array-based
algorithm deduced from it. Here we shall present a direct derivation of the array algo-
rithm using minimal prior knowledge; in fact, we shall deduce the equation form from
the array algorithm. The presentation follows that of [BSLK96], [BKLS98a].
The key matrix result is the next lemma. We first introduce some notation. Recall
the factorization (1.6.1),
where L is lower triangular with diagonal entries di while D = diag{di}. Now let us
define the upper triangular matrix
and write

Section 1.6. 
Fast Factorization of Structured Matrices 
19
The factorization (1.6.14) is somewhat nontraditional since we use D~l rather than D.
That is, the same diagonal factor D~l is used in the factorizations (1.6.1) and (1.6.14)
for both jR and R~l. The reason for doing this will become clear soon.
Lemma 1.6.2 (Key Array Equation). Consider a Toeplitz-like strongly regular and
Hermitian matrix R with a full rank generator matrix G e Cnxr, i.e., R â FRF* =
GJG*. Then there must exist a (D~l Â© J}-unitary matrix Q such that
Proof: Recall from Lemma 1.5.1 that there exists a full rank matrix H* such that
Hence, if we reconsider the block matrix
that appeared in the proof of Lemma 1.5.1 and use the displacement equations (1.3.21)
and (1.5.1), we can rewrite the block triangular factorizations in the proof of the lemma
as
The center matrix in both (1.6.17) and (1.6.18) is the same diagonal matrix (D"1Â©./); it
was to achieve this that we started with the nontraditional factorizations R = LD~1L*
and R~l = UD~lU*, with D~l in both factorizations.
The next step is to observe that, in view of the invertibility of L and U and the full
rank assumption on G and H*, the matrices
have full rank (equal to n + r). It then follows from Lemma A.4.3 in App. A that there
must exist a (D~l Â© Â«7)-unitary matrix fi such that (1.6.15) holds.
We shall focus first on the equality of the first block row in (1.6.15). The second block
rows can be used as the basis for the derivation of an efficient algorithm for factoring
.R"1 and for determining H*. We shall pursue this issue later in Sec. 1.10.
By confining ourselves to the first block row of (1.6.15), we note that there always
exists a (D~l Â© J)-unitary matrix fX that performs the transformation
The matrices {F, L, J} are uniquely specified by R and the displacement equation R â
FRF* = GJG*. There is flexibility in choosing G since we can also use G@ for any
J-unitary 9, i.e., one that obeys GJG* = J = 6* J6.

20 
Displacement Structure and Array Algorithms 
Chapter 1
This freedom allows us to conjecture the following algorithmic consequence of (1.6.19):
given any G, form a prearray as shown below in (1.6.20) and triangularize it in any way
we wish by some (D~l Â© J)-unitary matrix Q,
Then we can identify X = L. Indeed, by forming the (D'1 Â© J)-"norms" of both sides
of (1.6.20), we obtain
That is,
Hence, it follows from (1.3.21) that R=XD~1X*. But R = LD~1L*, so by uniqueness
we must have X = L.
Continuing with (1.6.19), at first it is of course difficult to see how (1.6.19) can
be used to compute L and D when only F, G, and J are given, since the unknown
quantity L appears on both sides of the equation. This apparent difficulty is resolved
by proceeding recursively.
Thus note that the first column of L (and hence of FL) can be obtained by multi-
plying the displacement equation by the first unit column vector CQ from the right,
or
where
The inverse exists by our solvability assumption (1.3.22), which ensured that the dis-
placement equation has a unique solution.
Now we can find elementary transformations that successively combine the first
column of FL with the columns of G so as to null out the top entries of G?, i.e., to null
out go. From (1.6.19) we see that the resulting postarray must have, say, the form
where FI and LI are (as defined earlier) obtained by omitting the first columns of F
and L, and all the entries of G\ are known.
Now we can prove that G\ is such that R\ = L\D^L{ obeys
where D\ = diag{di,... ,dn_i}. This equation allows us to determine the first column
of FiLi and then proceed as above. To prove (1.6.21), it will be convenient to proceed
more generally by considering the ith step of the recursion.

Section 1.6. 
Fast Factorization of Structured Matrices21
For this purpose, we recall the partitioning
and therefore partition the pre- and postarrays accordingly:
and
After i iterations, the first i columns of the postarray are already computed, while the
last (n â i) columns of the prearray have not yet been modified. Therefore, the ith
intermediate array must have the following form:
where Gi denotes the nontrivial element that appears in the upper-right-corner block.
All our results will now follow by using the fact that the prearray (1.6.22), the in-
termediate array (1.6.24), and the postarray (1.6.23) are all (D~l @ J) equivalent; i.e.,
their squares in the (D~l Â© J) "metric" are all equal.
We first establish that the entry Gi in the intermediate array (1.6.24) is a generator
matrix of the (leading) Schur complement Ri.
Lemma 1.6.3 (Structure of Schur Complements). Let Gi be the matrix shown in
(1.6.24). Then the Schur complement Ri satisfies the displacement equation
Proof: Since ft and fti are (D~l Â© J}-unitary, the second block rows of the postarray
(1.6.23) and the intermediate array (1.6.24) must satisfy
It follows immediately that
But we already know from (1.6.3) that Ri = I/jZD^1!/*, so (1.6.25) is established.
Prom (1.6.25) it is easy to obtain a closed-form expression for /Â». In fact, equating
the first columns on both sides of (1.6.25) leads to
which is (1.6.7). The solvability condition (1.3.22) ensures that (1.6.7) has a unique
solution for /$.
21

22 
Displacement Structure and Array Algorithms 
Chapter 1
Now recursion (1.6.9) follows by examining the transformation from the iih to ttye
(i + l)th iteration, i.e., at the step that updates the right-hand side of (1.6.24). It can
be succinctly described as
where the irrelevant columns and rows of the pre- and postarrays (i.e., those rows and
columns that remain unchanged) were omitted. Also, Ej is a submatrix of fij, and it is
(d~l 0 J)-unitary. We have argued above in (1.6.25) that the matrix C?i+i in (1.6.27)
is a generator for the Schur complement Ri+i. This completes the proof of the array
Alg. 1.6.2.
Explicit Equations
Although not needed for the algorithm, we can pursue the argument a bit further
and deduce the explicit updating equation (1.6.11). To do this, we first identify the
transformation E*. To do this, note first that E^ is (d~l 0 J)-unitary, i.e., Si(d~1 0
J)E* = (d~l 0 J). Therefore, the inverse of E^ is given by
It then follows from the array equation (1.6.27) that
If we denote the entries of Ej by
where a^ â¬ C, bi â¬ Clxr, Ci â¬ Crxl, and Si â¬ Crxr, we conclude by equating the top row
on both sides of (1.6.28) that we must have
In other words, any (d"1 0 ./^transformation E^ that achieves (1.6.27) must be such
that its entries {a^Ci} are as above. In order to identify the remaining entries {6j,Si},
we note that in view of the (d~l 0 J)-unitarity of E^, the entries {aj,6j,Ci,Si} must
satisfy
Lemma 1.6.4 (Identification of SÂ»). Given {/i,<7i, J}, all pairs {&i,Si} that satisfy
(1.6.31) are given by
for any J-unitary parameter

Section 1.6. 
Fast Factorization of Structured Matrices 
23
Proof: It follows from (1.6.31) that
Using di(l â |/i|2) = 9iJgl, we can verify after some algebra that the right-hand side
of the above expression can be factored as
But the matrix [Â£*] is full rank since E; is full rank (due to its (d~l Â© J)-unitarity).
Likewise, the matrix
is full rank since otherwise there would exist a nonzero vector x such that
This implies that we must have gix = 0, which in turn implies from the equality of the
second block row that x = 0. This contradicts the fact that x is nonzero.
Therefore, in view of the result of Lemma A.4.3, we conclude that there should exist
a ./-unitary matrix 0j such that
as desired.
0
Substituting (1.6.30) and (1.6.33) into (1.6.27) yields (1.6.11).
1.6.5 
An Elementary Section
A useful remark is to note that in (1.6.27), viz.,
we can regard the transformation S* of (1.6.29) as the system matrix of a first-order
state-space linear system; the rows of {Gi} and {Gi+i} can be regarded as inputs and
outputs of this system, respectively, and the entries of {/^F^} can be regarded as the
corresponding current and future states.
Let &i(z) denote the transfer function of the above linear system (with inputs from
the left), viz.,

24 
Displacement Structure and Array Algorithms 
Chapter 1
Using (1.6.32) and (1.6.33), and simple algebra, shows that the above expression col-
lapses to
where
We therefore see that each step of the generalized Schur algorithm gives rise to a first-
order section @i(z). Such elementary sections have several useful properties. In partic-
ular, note that
which shows that Qi(z) has a transmission zero at fi along the direction of gi. This
blocking property can be used, for example, to obtain efficient recursive solutions to
rational interpolation problems (see, e.g., [SKLC94], [BSK94], [BSK99] and Sec. 1.14.3).
1.6.6 
A Simple Example
To see how Alg. 1.6.2 works, we consider a simple example with n = 3 and r = 2. In this
case the pre- and postarrays in (1.6.19) will have the following zero-patterns:
Using (1.6.7) for i â 0, viz.,
we can determine the first column IQ of L and, consequently, the first column of FL. In
this way, all the entries of the first column of the prearray are completely known. Also,
the last two columns of the prearray are known since they are determined by G.
Hence, the (1,2) block entry of the prearray (i.e., the top row of G?) can be eliminated
by pivoting with the top entry of the first column of FL. As a result, the first column
of the prearray and its last two columns are linearly combined to yield the intermediate
array shown below after the transformation Q,Q . The second and third columns of the
prearray remain unchanged:
We now proceed in a similar fashion to the next step. Using (1.6.7) for i = 1, viz.,
we determine the li and, consequently, the second column of FL. The second trans-
formation fii can now be performed as shown above to yield G-2 (see below), and so
on.
The rectangular boxes mark the entries to be eliminated at each step of the recursion
by using elementary (D~l Â© Â«7)-unitary transformations (scaled column permutations,
Givens rotations, and Householder projectionsâsee App. B). The square boxes mark
the position of the pivot elements. The ultimate result of the recursion is that the (1,2)
block of the prearray is eliminated row by row.

Section 1.7. 
Proper Form of the Fast Algorithm 
25
1.7 
PROPER FORM OF THE FAST ALGORITHM
A useful feature of the explicit form of the generator recursion (1.6.11) is that different
choices of the arbitrary J-unitary matrix Qj can be more easily used to obtain different
variants of the general algorithm, which can have different domains of usefulness. One
of these is the so-called proper form.
This means that Gi is chosen so as to eliminate all elements of gi with the exception
of a single pivot element. This pivot has to be in the first p positions when gi Jg* > 0
and in the last q positions when gi Jg* < 0. (Note that the case gi Jg* = 0 is ruled out
by the strong regularity assumption on R. This is because di ^ 0 implies gÂ± Jg* ^ 0 by
(1.6.8).)
1.7.1 
Positive Lengths
When giJg* > 0 holds, we can choose a J-unitary rotation Gi that would reduce gi to
the form
where we have chosen, without loss of generality, the nonzero entry to be in the leading
position of the postarray. With this particular choice, the generator recursion (1.6.11)
can be seen to collapse to
Likewise, the expressions (1.6.7) and (1.6.8) for li and di become
Equation (1.7.2) yields the following simple statement for steps with gi Jg* > 0:
1. Transform Gi into proper form with respect to its first column by using a J-unitary
rotation Gi.
2. Multiply the first column by $i and keep the rest of the columns unaltered.
The first step eliminates all entries in the first row of Gi except the pivot entry in the
upper-left-corner position (see Fig. 1.2 below). The second step is needed to eliminate
the pivot entry.
Figure 1.2. Illustrating the proper form of the generator recursion.
Each step of (1.7.2) can also be depicted graphically as a cascade network of elemen-
tary sections, one of which is shown in Fig. 1.3; Gi is any J-unitary matrix that rotates

26 
Displacement Structure and Array Algorithms 
Chapter 1
Figure 1.3. Proper form when giJg* > 0.
the first row of the ith generator to [ Si 0 ]. The rows of Gi enter the section one
row at a time. The leftmost entry of each row is applied through the top line, while the
remaining entries are applied through the bottom lines. The Blaschke-Potapov matrix
$i then acts on the entries of the top line. When Fi = Z, the lower shift matrix $j
collapses to <frj = Z, a delay unit (see the discussion further ahead on shift-structured
matrices). In general, note that the first row of each $i is zero, and in this sense 3>i
acts as a generalized delay element. To clarify this, observe that when the entries of
the first row of Gi are processed by @i and $i, the values of the outputs of the section
will all be zero. The rows of Gj+i will start appearing at these outputs only when the
second and higher rows of Gi are processed by the section.
1.7.2 
Negative Lengths
A similar derivation holds for steps with giJg* < 0. Now we choose a J-unitary matrix
Qi so that
where the nonzero entry in the postarray is chosen, again for convenience, to be at its
last position.
Equation (1.6.11) now collapses to
Also, expressions (1.6.7) and (1.6.8) for ^ and di become
Equation (1.7.5) admits the following simple interpretation:
1. Transform Gi into proper form with respect to its last column by using a J-unitary
rotation 9^.
2. Multiply the last column by 3>i and keep the rest of the columns unaltered.
1.7.3 
Statement of the Algorithm in Proper Form
We collect the above results into the following statement.

Section 1.7. Proper Form of the Fast Algorithm 
27
Algorithm 1.7.1 (Fast Algorithm in Proper Form). Given a matrix R e Cnxn
that satisfies (1.3.21) and (1.3.22) for some full rank G G Cnxr, start with Go = G and
perform the following steps for i = 0,..., n â 1:
1. Let gi be the top row of Gi and let Fi be the submatrix obtained by deleting the
leading i rows and columns of F.
2. If giJgi > 0, then transform gi to proper form according to (1.7.1), compute li
and di using (1.7.3), and update Gi to Gi+i according to (1.7.2).
3. If giJgi < 0, then transform g^ to proper form according to (1.7.4), compute li
and di using (1.7.6), and update Gi to Gi+i according to (1.7.5).
0
1.7.4 
An Associated Transmission Line
We may return to the elementary section (1.6.34) and note that its form simplifies in
the proper case. Assume giJg* > 0 (a similar argument holds for QiJg* < 0). It then
follows that (1.6.27) collapses to the form
which leads to the transfer matrix
or, equivalently,
Any (p + <?)-row input to the above Gi(z) is processed by the rotation Gi first and
then only the leading entry of the result is filtered by the all-pass function Bi(z). This
is represented schematically in Fig. 1.4, where the top line of the elementary section
includes the factor Bi(z).
When F = Z, the cascade of such sections is a classical discrete transmission line; it
can be used to show that the generalized Schur algorithm gives a natural solution (and
indicates several generalizations) of the classical inverse scattering problems (see, e.g.,
[BK87a], [BK87b], [CSK99]).
1.7.5 
Shift-Structured (F= Z) Positive-Definite Matrices
Several simplifications occur when F = Z and jR is positive definite, say,
To begin with, since the diagonal entries of Z are now zero, the expression (1.6.12) for
4>j becomes simply $i = Z. (We continue to write Z, except when otherwise stated, to
denote a shift matrix of any dimension.)

28 
Displacement Structure and Array Algorithms 
Chapter 1
Figure 1.4. Elementary section Â©i(z) when g%Jg* > 0.
Moreover, since the fi are now all zero, we conclude from (1.6.8) and from the
positivity of the di that Qi Jg* > 0 always. This means that the proper form of the
generalized Schur algorithm in this case becomes
(1.7.10)
Likewise, the expressions (1.6.7) and (1.6.8) for ^ and di become
Equation (1.7.10) yields the following simple statements:
1. Transform Gi into proper form with respect to its first column by using a J-unitary
rotation 0j.
2. Shift down the first column and keep the rest of the columns unaltered.
1.7.6 
The Classical Schur Algorithm
In order to justify our earlier claim that Alg. 1.6.2 is a far-reaching generalization of the
celebrated algorithm of Schur, we focus on a special subclass of (1.7.9), viz., matrices
with displacement rank 2. Our objective is to verify that in this case the generator
recursion (1.7.10) collapses to Schur's original algorithm [Schl7].
For this purpose, we shall now consider semi-infinite matrices 7Â£ with semi-infinite
generator matrices Â£, say,
where we shall denote the individual entries of Q by
We further associate with the columns of Q two power series XQ(Z) and yo(z):
Figure 1.4. Elementary section Â©i(z) when g%Jg* > 0.

Section 1.7. 
Proper Form of the Fast Algorithm 
29
(We assume *R, and Q are such that these power series are well defined in some region
of the complex plane.) Equivalently, these power series can be obtained by multiplying
Q from the left by [ 1 z z2 z3 ... ],
Now the first step of the generator recursion (1.7.10) takes the form
where we recall that the purpose of the J-unitary rotation GO is to annihilate yoo- This
can be achieved by using a hyperbolic rotation of the form (cf. the discussion in App. B)
(The positive definiteness of 72. guarantees |#oo|2 ~~ I Stool
2 > 0 and, hence, #00 cannot be
zero.) After applying GO and then shifting down the first column, we obtain Q\ by
We also associate two power series with the nonidentically zero part of the columns of
Si,
By multiplying both sides of (1.7.13) by [ 1 z z1 z3 ... ] from the left, and by
noting that
we conclude that
This is a functional recursion that tells us how the power series of the successive gener-
ator matrices are related to each other.
The recursive procedure now continues as follows: compute 71 as the ratio of y\\
and xn, multiply the prearray Q\ by GI in order to introduce a zero in the first entry
of the second column of the postarray, shift down the first column of the postarray, and
so on. In function form, for the ith step, we have

30 
Displacement Structure and Array Algorithms 
Chapter 1
where Qi is an elementary hyperbolic rotation determined by a coefficient 7$,
If we introduce the (so-called scattering) function
it then follows easily from (1.7.14) that Si(z) satisfies the recursion:
This is the famous Schur recursion, which was originally derived for checking when
power series are analytic and bounded by unity in the unit disc [Schl7]. We see that it
follows as a special case of our earlier Alg. 1.6.2, which is in this sense a generalization
of Schur's earlier work. Note that the generalization is in several respects: it allows
for displacement ranks larger than 2, it allows for general lower triangular matrices F
instead of the shift matrix Z, and it allows for strongly regular matrices R instead of
positive-definite matrices R. The generalization also extends to non-Hermitian matri-
ces and to other definitions of matrix structure (see further ahead and also [KS95a]).
(The generating function language can continue to be used for certain forms of Fâsee
[Lev97].)
1.7.7 
The Special Case of Toeplitz Matrices
When the matrix R in (1.7.12) is a finite Hermitian Toeplitz matrix, T = [ci_j]â¢J=0,
with c0 = 1, then it is easy to check that a generator matrix is given by
(see (1.2.5)).
It turns out that in this case, the so-called Schur coefficients 7, coincide with the
reflection coefficients introduced in the Levmson-Durbin algorithm (1.2.7)-(1.2.9) for
solving the Yule-Walker equations, which is why we used the same symbols for them.
But, as noted earlier (at the end of Sec. 1.2), we can now present an alternative method
of solving the Yule-Walker equationsâwe compute the {7^} by the Schur algorithm
and then directly use the recursions (1.2.7), thus circumventing the inner-product eval-
uations (1.2.8) of the Levinson-Durbin algorithm. This is sometimes called the hybrid
method of solving the Yule-Walker equationsâthe algorithm can also be extended to
solving Toeplitz linear equations with arbitrary right-hand sides, as will be shown in
Sec. 1.8.4 below.
1.8 SOME APPLICATIONS IN MATRIX COMPUTATION
In the previous sections, we established that displacement structure is invariant un-
der Schur complementation and derived a fast algorithm for recursively updating the
generator matrices of the successive Schur complements of a structured matrix.
We also mentioned earlier that the above invariance property is very useful in han-
dling combinations of structured matrices. Now that we have derived the fast algorithm,

Section 1.8. 
Some Applications in Matrix Computation 
31
we can return to this earlier remark and demonstrate its usefulness by presenting a few
examples. The examples we present here will also highlight the value of extending the
the definition of displacement structure by replacing Z in (1.2.4) by more general (say,
strictly lower) triangular matrices F in (1.3.21). Such extension allows us to handle,
through a technique known as embedding [CKL87], [KC94], many matrix computation
problems that involve combinations of structured matrices.
1.8.1 
Going Beyond F = Z
A first application of the embedding idea is to note that T~l is the Schur complement
of the (1,1) block in the Toeplitz block matrix
Now by examining M â ZinMZ<in, we can see that the displacement rank of M is less
than or equal to 4, where we have employed the notation Z^n to denote the 2n x In
lower shift matrix. Therefore, by Lemma 1.5.2, the rank of the Schur complement, T"1,
must also be less than or equal to 4. However, this is a weak conclusion, because we
know from Lemma 1.5.1 that the displacement rank of T"1 is 2.
If we instead employ the definition
where we use F = (Zn Â© Zn) in the definition R â FRF* rather than F â Zin, then it
is easy to see that the displacement rank of M is now 2. In fact, the reader may wish
to check that for a Hermitian Toeplitz matrix T = [ci_j]â¢~=0, CQ = 1, we obtain
where J = (1 Â© â1) and
Soon we shall give more elaborate examples. However, from the embedding (1.8.1), we
shall show how we can obtain several interesting results on both T and T~l.
1.8.2Simultaneous Factorization of 7" and T~l
We shall see that by applying the generalized Schur algorithm to the matrix M in (1.8.1)
we can not only determine (the generators of) T~x but also simultaneously factor both
T and T"1. Thus consider the situation after we apply n steps of the generalized Schur
algorithm (say Alg. 1.6.2 or, in proper form, Alg. 1.7.1) to the generator G of M, viz.,
(1.8.3). Of course, we shall then get a generator, say, {a, 6}, of the Schur complement
T"1 from which we can recover the matrix T"1 as
Suppose that we also (or only) want the triangular factors of T"1. One way to get
these is to apply the Schur algorithm to the generator {a, b}. But in fact the factors of

32 
Displacement Structure and Array Algorithms 
Chapter 1
T"1 are already available from the results of the first n steps of the generalized Schur
algorithm applied to the generator of M.
To clarify this, assume we apply the first n recursive steps of the generalized Schur
algorithm to a generator of the In x 2n matrix M, with F = (Zn Â® Zn). This provides
us with the first n columns and the first n diagonal entries of the triangular factors of
M, which we denote by Z/2n and D^n. That is, we obtain the first n columns of L<2n
and the first n entries of D^n in the factorization M = LinD^L\n. Let us denote the
leading n x n block of D^n by D and let us partition the first n columns of Lin into the
form
where L is n x n lower triangular and U is an n x n matrix that we shall soon see has
to be upper triangular. It follows from the Schur reduction interpretation of Alg. 1.6.1
that we must have
By equating terms on both sides of the above equality we conclude that U â L~*D,
âT~l = UD~1U*, and â T = LD~1L*. Hence, the first n recursive steps of the
algorithm provide not only the triangular factorization of T but also the triangular
factorization of T"1. In fact, by examining the form (1.8.3) of the generator for the
matrix M, the reader can check that the generalized Schur algorithm for the present
problem (i.e., for M as in (1.8.1)) is exactly what we called a hybrid algorithm in
Sec. 1.7.7.
Unfortunately, this nice embedding idea does not extend to a general F. To see this,
consider again the simple example of a Pick matrix P,
where the {u^, Vi} are row vectors of dimensions p and q, respectively, and the fi are com-
plex points inside the open unit disc (|/j| < 1). Then with F = diag{/o, /i,..., /n-i},
we can check that P has V.p-displacement
However, if we now construct the matrix M as before, i.e., as in (1.8.1) but with P
instead of T, it is easy to check that M â (F Â© F)M(FÂ© F)* does not have low rank
since (/ â FF*) is full rank in general.
A general method that overcomes this difficulty has recently been obtained and will
be described in Sec. 1.10, where this example will be reconsidered.
1.8.3 
QR Factorization of Structured Matrices
We now show how the displacement ideas can be used to suggest a fast algorithm for the
QR factorization of structured matrices (with Q unitary and R upper triangular). This

Section 1.8. 
Some Applications in Matrix Computation 
33
has been a much-studied problem, starting with the dissertation [Swe82]. Some later
papers are those of [BBH86], [Cyb83], [Cyb87], [Swe84j. The displacement approach
described below is much simpler, conceptually and algebraically.
Let X be an n x n matrix4 that has low displacement rank with respect to the
displacement X â ZnXZ^. Form the displacement of
with F = Zn Â© Zn Â© Zn and find a generator for M. A general procedure for doing this
has been given in [Chu89], [KC94]; in many cases, e.g., when X is Toeplitz, one can
obtain a generator of length 5 almost by inspection (see, e.g., the discussion in Sec. 3.4
of this book).
After n steps of the generalized Schur algorithm applied to a generator of M, we
shall have a generator of
After another n steps, we shall have the partial triangularization (where L is n x n lower
triangular and U is an n x n matrix)
By equating terms on both sides of the above equality we conclude that
and (t/Â£>-*/2)(t/.D-*/2)* = /. Therefore, we can identify
as the Q and R factors in the QR factorization of X. Here, D is a positive-definite
diagonal matrix and D1/2 denotes a diagonal matrix whose entries are the square roots
of the diagonal entries of D. In summary, the QR factors of the structured matrix X can
be obtained by applying the Schur recursion to a properly defined extended structured
matrix M.
The above procedure may encounter numerical difficulties in finite precision imple-
mentations. However, in Sec. 3.1, it will be shown how to employ such embedding
constructions to develop the first provably backward-stable algorithm for the solution
of linear systems of equations with structured coefficient matrices (cf. [CS98]).
1.8.4Avoiding Back Substitution in Linear Equations
The previous examples all involved the Schur algorithm for Hermitian matrices. Here is
an example of a problem involving non-Hermitian matrices. The generalized Schur al-
gorithm for non-Hermitian matrices is similar in nature to what we described in Sec. 1.6
and is covered in detail in [KS95a] (and briefly in Sec. 1.11). It is not necessary to know
the exact algorithm to follow the present discussion.
4The argument also applies to rectangular matrices, say, m x n.

34 
Displacement Structure and Array Algorithms 
Chapter 1
Consider a linear system of equations of the form
where T is an n x n strongly regular Hermitian Toeplitz matrix and b is a known column
vector. One possibility for determining the entries of x is the following: compute the
triangular factorization of T, say,
and then solve, via back substitution, the triangular system of equations in y and x,
A major drawback of a backâ^ ~ âves serial operations and
does not lend itself to a parallelizable algorithm.
A way out of this is to employ a bordering (or embedding) technique (see, e.g.,
[KC94]). For this purpose, we define the extended (non-Hermitian) matrix
and note that the Schur complement of âT in R is precisely T~lb, which is equal to
the desired solution x. Now the matrix R itself is also structured since T is Toeplitz.
More precisely, we know that T â ZTZ* has rank 2 and it follows that
also has low rank.
Therefore, after n steps of partial triangularization of R, we shall have a generator of
its Schur complement, from which we can read out the solution x.
There are several other interesting examples and applications (see, e.g., [CXT94],
[AS99]), but let us move on.
1.9 
LOOK-AHEAD (BLOCK) SCHUR ALGORITHM
A standing assumption in all the preceding has been that the structured matrix R is
strongly regular, i.e., all its leading minors are nonzero. However there are applications
where we may have some poorly conditioned or even zero leading minors. In such cases,
one can use the smallest nonsingular leading minor, or a well-conditioned leading minor
of appropriate dimensions, in order to proceed with a block Schur complementation
step. The use of such block pivoting has been studied by several authors trying to
devise effective numerical algorithms for various classes of structured matrices (see,
e.g., [CH92b], [Gut93], [Fre94], and the references therein).
There are also several theoretically interesting studies on the special case of Hankel
matrices with zero minors (for which there is the celebrated Berlekamp-Massey algo-
rithm [Mas69] and related studies of the so-called partial realization problem of system
theory) and the less-studied problem for Toeplitz matrices (see [Pal90], [PK93], and the
references therein).
Here we shall describe a very general algorithm that goes considerably beyond the
results noted so far. The results appeared first in [SK95b], [Say92]; here we present an
array-based derivation.

Section 1.9. 
Look-Ahead (Block) Schur Algorithm 
35
Consider a Hermitian and invertible (but not necessarily strongly regular) matrix
R G Cnxn, and let 770 denote the desired size of the leading invertible block, DQ, with
respect to which a Schur complementation step is to be performed. The 770 may stand
for the size of the smallest nonsingular minor of R or, alternatively, for the size of a
numerically well-conditioned block. If LQ represents the first 770 columns of R, then we
can replace the earlier Schur complementation step (1.6.4) by the block step
where R\ is now an (n â 770) x (n â 770) matrix that is the Schur complement of DO
in R. We are also being explicit about the dimensions of the leading zero block in the
resulting matrix, viz., 770 x 770-
The matrix LQ is n x 770 with a leading 770 x 779 block that is equal to DQ. If we further
let 771 denote the desired size of the leading invertible block of R\ (denoted by D\} and
consider the corresponding first 771 columns of jRi (denoted by LI), then we write for
our second block step
where #2 is an (71 â 770â771) x (n â 770 â 771) matrix that is the Schur complement oD\
in .Ri. Repeating this block Schur reduction procedure, viz.,
we clearly get, after, say, t steps,
where D = (DQ 0 D\ Â©... 0 Dt-i) is now block diagonal and the (nonzero parts of the)
columns of the block lower triangular matrix L are {LQ,..., Lt-i}. Here t is the number
of reduction steps and, hence, n = X^Zg ^
The computational cost of this block reduction procedure is, as mentioned earlier,
O(n3). By exploiting the structure of R we can reduce the computational cost by
deriving an algorithm that operates on the successive generator matrices instead. So
assume R satisfies (1.3.21) and (1.3.22). It is then clear that the same array equation
(1.6.15) still holds, viz.,
except that D and L are now block diagonal and block lower triangular, and U is such
that U = L~*D.
The same recursive argument that we employed in Sec. 1.6.4 shows that the basic
recursion for the generators of the successive (block) Schur complements Ri now takes
the form

36 
Displacement Structure and Array Algorithms 
Chapter 1
where Li satisfies the equation
and Fi is the leading rji x rji block of Fi, which is now partitioned as
That is, Fi+i is now obtained by deleting the leading rji rows and columns of Fi. Like-
wise, Gi denotes the top rji rows of GI and Â£j is (D~l Â© J)-unitary. The quantities
{Fi, Gi,Li,Di} play the role of the quantities {fi,gi, k,di} that we encountered earlier
in Sec. 1.6.4.
If we denote the entries of S* by
where Ai is rfe x 77^, Bi is r/j x r, Q is r x rjit and Si is r x r, we can verify that we must
have
To identify the remaining entries {Bi, Si}, we note that in view of the (Â£)~1Â©Â«7)-unitarity
of S^ the entries {Ai, Bi,Ci,Si} must satisfy
Following the derivation in the proof of Lemma 1.6.4, we can use this relation to identify
{Bi, Si} and obtain the following algorithm [Say92], [SK95b].
Algorithm 1.9.1 (Block or Look-Ahead Schur Algorithm). 
Given a matrix
R e Cnxn that satisfies (1.3.21) and (1.3.22) for some full rank G â¬ Cnxr, start with
FQ = F, GQ = G and perform the following steps:
1. At step i we have Fi and Gi. Let Gi denote the top rji rows ofGi and let Fi denote
the leading r)i x r^ block of Fi.
2. The ith triangular factors Li and Di are the solutions of the equations
3. Update the generator matrix Gi as follows:
where Â©^ is an arbitrary J'-unitary matrix and TI is an arbitrary unit-modulus
scalar. Also, cti = ^1=0^-
The matrix Gi is a generator for Ri,
which is the Schur complement of R with respect to its leading cti x on block.
We may remark that although block algorithms have often been used in connection
with poorly conditioned matrices, Alg. 1.9.1 is quite general and has several other
applications.

Section 1.10. 
Fast Inversion of Structured Matrices 
37
1.10FAST INVERSION OF STRUCTURED MATRICES
Our discussion so far has been mainly concerned with the direct factorization problem,
viz., that of computing the triangular factors of R. In Sec. 1.8.2 we saw how the
embedding technique allowed us to employ the fast Schur algorithm for factoring the
inverse of a Toeplitz matrix as well. We remarked, however, at the end of Sec. 1.8.2
that this technique does not extend readily to more general structured matrices. In fact,
similar results for computing the triangular factors of the inverse matrix R~1 have not
yet been obtained for the general case.
For Toeplitz matrices, as we mentioned in Sec. 1.2, the first and best-known
algorithm for factoring the inverse is the celebrated Levinson-Durbin algorithm.
Early attempts at extending this algorithm beyond the Toeplitz case were made in
[FKML78], [Del82], [DGK85], [Lev83] but the formulas were rather complicated. After
the (re)discovery of the Schur algorithm for directly factoring Toeplitz matrices rather
than their inverses, it was realized by several authors (see [KH83], [Kai85]) that the
Levinson-Durbin algorithm could be replaced by a two-step procedure: use the Schur
algorithm to compute the so-called reflection coefficients, and then use a simple recursion
to compute the triangular factors of the inverse; this is actually the hybrid algorithm
of Sec. 1.8.2. This two-step procedure requires slightly more computation than the
Levinson-Durbin algorithm on a serial machine, but it is significantly less expensive on
a parallel machine. (This is because the classical Levinson-Durbin algorithm obtains
the reflection coefficients via certain inner products, which cannot be parallelized in an
efficient manner.) The hybrid method was extended in [Chu89] and [KC94] to invert
matrices obeying displacement equations of the form R â FRF* = GJG*, where F had
some strictly lower triangular structure. However, these extended algorithms generally
require an intermediate array whose displacement rank can be larger than r.
In the recent works [BSLK96], [BKLS98a], [BKLS98b] we removed all the above-
mentioned restrictions. In this section we provide an overview of the solution method.
However, for simplicity of presentation, here we shall first assume that the following
nondegeneracy condition holds (in addition to (1.3.22)):
This condition simplifies the derivation of the recursions for the factorization of the
inverse of R; it is not needed for the direct factorization problem itself as we saw in the
earlier section. The assumption, however, excludes the important cases of jP = Z or F
strictly lower triangular; the general case is briefly discussed in Sec. 1.10.7.
1.10,1 
Schur Construction (Inflation)
The Schur reduction procedure of Alg. 1.6.1 can be extended to the factorization of the
inverse matrix. While we factored R before by recursively deflating it, we now factor
R-1 by "inflation."
Recall that we expressed the triangular factorizations of R and R~l in the somewhat
nontraditional forms (cf. (1.6.1) and (1.6.14))
where D = diag{ do, di, ..., dn-\} and

38 
Displacement Structure and Array Algorithms 
Chapter 1
The nonzero parts of the columns of L are denoted by {lj} and of U are denoted by
K}-
Algorithm 1.10.1 (Schur Construction for Inversion). Given R â¬ <Dnxn, start
with AQ = [ ] (the empty matrix) and repeat the following steps for i = 0,1,..., n â 1:
1. Let li and di denote the first column and the upper-left-corner element of Ri.
These can be evaluated via the Schur reduction of Alg. 1.6.1.
2. Given {/o,/i, â¢ â¢ â¢, k} and {do, di, â¢ â¢ â¢, di}, compute HI from the equation
where
3. Perform the Schur construction step
Observe that while Alg. 1.6.1 involves reduction steps, i.e., rank 1 matrices are re-
cursively subtracted at each step, the above algorithm involves construction steps in
which rank 1 matrices are added to Aj (after bordering the matrix with zeros). Thus
this procedure successively constructs the rows and columns of .R"1. The intermediate
array A$ is the Schur complement of the trailing (n â i) x (n â i) block in R~l. At the
end of the procedure we obtain An_i =R~1. It is also immediate to verify the following
result.
Lemma 1.10.1 (Partitioning of the Triangular Factorization). // we partition
R-1 and U as
and let D be partitioned as before (after Alg. 1.6.1), then the trailing principal block Ti
and its Schur complement A.i = Wi â V*T~1Vi admit the following triangular decompo-
sitions:
Moreover, we also have that R~l =Ti and A"1 = Pj.
In other words, the inverse of the trailing principal block in .R"1 is equal to the
Schur complement of the leading principal block in R and vice versa: the inverse of the
leading principal block in R is equal to the Schur complement of the trailing principal
block in R~l.

Section 1.10. 
Fast Inversion of Structured Matrices 
39
1.10.2 
Statement of the Fast Inversion Algorithm
Just as for RI, it turns out that Aj inherits the displacement structure of R, as we shall
now proceed to show. Using this property, we shall also be able to reduce the cost of
the above Schur reduction procedure from O(n3) to O(n2).
The algorithm we state below has one very important feature: it does not require
that we know a priori a generator matrix H* for R~* to factor R-1. Instead, it works
directly with the given displacement description of R itself, namely, {F, G, J}, and
constructs {H*,U}\ This will be achieved by recursively computing matrices Hi^ClX1'
(i â Q, 1,..., n â 1) that are generators for the successive Aj,
where, according to (1.6.6), Fi is the leading i x i block of F,
At this point, we encourage the reader to review Alg. 1.6.2 for the factorization of
.R, because it will be used here.
Algorithm 1.10.2 (Schur Algorithm for the Inverse Matrix). 
Given a matrix
R â¬ Cnxn that satisfies (1.3.21), (1.3.22), and (1.10.1) for some full rank G â¬ Cnxr,
start with H0 = [ ] and repeat the following steps for i = 0,..., n â 1 :
1. Let {li,di,gi,Gi} be defined and computed as in Alg. 1.6.2.
2. Let Fi+i and Ui be partitioned as
Compute Ui by solving the linear system
The nondegeneracy condition (1.10.1) ensures that (1.10.7) has a unique solution
forui.
3. Apply the same transformation Sj as in (1.6.9) to the prearray shown below and
obtain H*+l:
Notice that H*+1 has one more row than H*.
In fact, we can further show that H*+l can be obtained from H* explicitly via the
equation
where &i is any J-unitary matrix Gj and

40 
Displacement Structure and Array Algorithms 
Chapter 1
1.10.3 
Algorithm Development
The derivation of the algorithm follows from the array-based arguments that we em-
ployed earlier in Sec. 1.6.4. Recall that in that section we focused on the top block row
of both the pre- and postarrays in (1.6.15). By further considering the effect of the
rotation f2 on the second block row of both arrays, we are led to the above algorithm.
More explicitly, we already know from the argument that led to the array equation
(1.6.15) that there exists a (D~ Â© J)-unitary matrix fi such that
We used this equation earlier to argue recursively that by triangularizing the prearray
through a sequence of (D~l Â© J)-unitary rotations {fio5^i> â¢ â¢ â¢ >^n-i} we obtain the
generalized Schur procedure listed in Alg. 1.6.2. Now by applying these same rotations
to the second block row in (1.10.11), viz.,
we can justify Alg. 1.10.2.
For this purpose, we start by partitioning the pre- and postarrays as
and
After i iterations, the first i columns of the postarray are already computed, while the
last (n â i) columns of the prearray have not yet been modified. Therefore, the ith
intermediate array must have the following form:
where Gi and H* denote the nontrivial elements that appear in the upper- and lower-
right-corner blocks. Note that the prearray (1.6.22), the intermediate array (1.6.24),
and the postarray (1.6.23) are all (D~l Â© J)-equivalent; i.e., their "squares" in the
(D~l Â© J) metric must be equal.
We already know from Lemma 1.6.3 that Gi is a generator matrix for the leading
Schur complement Ri. Now, a similar conclusion follows for H*. Indeed, note first that
the matrix

Section 1.10. 
Fast Inversion of Structured Matrices
41
is n x r and must have full rank r. This is because the prearray in (1.6.24) has full rank
n + r. Now since each of the fij is invertible, we conclude that the postarray in (1.6.24)
must also have full rank n -I- r. It then follows that (1.10.15) has full rank r.
Moreover, it also follows that the Schur complement Aj satisfies the displacement
equation (1.10.5). This is obtained by comparing the prearray and the intermediate
array. The "squared lengths" of the third block rows of (1.6.22) and (1.6.24) must be
equal, i.e.,
Therefore,
But since &i = UiD~lU?, we conclude that (1.10.5) holds.
We now establish (1.10.7) and the generator recursion of Alg. 1.10.2. Recall that we
used (1.6.25) earlier to derive the closed-form expression (1.6.26) for ^. Unfortunately,
a similar argument using (1.10.5) cannot be used to determine Ui. This is because Aj
involves UQ, ..., Ui-i but not u^. However, the (D~l Â© J)-unitary equivalence of the
intermediate array and the postarray shows that the (D~l Â© J)-inner product of the
second and third block rows of (1.6.23) and (1.6.24) must be equal, i.e.,
This implies that
Equating the first columns on both sides leads to the equation
which validates (1.10.7). The nondegeneracy condition (1.10.1) ensures that (1.10.7)
has a unique solution for u^.
Finally, by omitting the irrelevant columns and rows of the pre- and postarrays (i.e.,
those rows and columns that remain unchanged), we can write
where E$ is a submatrix of fij as in (1.6.27). This establishes (1.10.8). Also, by using
the parameters of Â£$ shown in Lemma 1.6.4, we obtain the generator recursion relating
HI and H?+1 (as stated after Alg. 1.10.2).
1.10.4 
An Example
We return to the rotation example we considered in Sec. 1.6.6 with n = 3 and r = 2 and
show how to incorporate the procedure for inverting R as well. In this case the pre- and
postarrays will have the following zero-patterns:

42 
Displacement Structure and Array Algorithms 
Chapter 1
Using (1.6.7) for i = 0, viz.,
we can determine the first column IQ of L and, consequently, the first column of FL. In
this way, all the entries of the first column of the prearray are completely known. Also,
the last two columns of the prearray are known since they are determined by G.
Hence, the (1,2) block entry of the prearray (i.e., the top row of G) can be eliminated
by pivoting with the top entry of the first column of FL. As a result, the first column
of the prearray and its last two columns are linearly combined to yield the intermediate
array shown below after the transformation OQ. The second and third columns of the
prearray remain unchanged.
We now proceed in a similar fashion to the next step. Using (1.6.7) for i = 1, viz.,
we determine the l\ and, consequently, the second column of FL. Likewise, using
(1.10.7) for i = I (in this case FI = /0),
we determine u\. The second transformation fii can now be performed as shown above
to yield G^ and HI, and so on.
The rectangular boxes mark the entries to be eliminated at each step of the recursion
by using elementary (D~l Â© J)-unitary transformations. The square boxes mark the
position of the pivot elements. The ultimate result of the recursion is that the (1,2)
block of the prearray is eliminated row by row ("reduction procedure"), while the (2,2)
block is filled up with nonzero elements ("construction procedure").
1.10.5 
Proper Form of the Algorithm
Analogous to what we did in Sec. 1.7 for the recursion for the generators Gj, we can
reduce the recursion for the H* in Alg. 1.10.2 into proper form.

Section 1.10. 
Fast Inversion of Structured Matrices 
43
Assume again that gi Jg* > 0 and let Gj be a J-unitary matrix that rotates ^ to the
form (1.7.1). Then it can be verified easily that the expression (1.10.7) for Ui reduces
to
while the recursion for H* reduces to
where we defined &i = 6i/di. The above equation has the following interpretation:
1. Multiply H* by 6*.
2. Multiply the first column of H*Qi by \Â£i and keep the rest of the columns unal-
tered.
3. Attach a zero row to the bottom of the array.
4. Add the correction term ai[ 0* 1 ]* to the first column.
Note that initially H* is in proper form. Multiplying the array by 8j will destroy this
properness (see Fig. 1.5). After attaching a zero row to the bottom of the matrix and
adding a correction term to the first column, the resulting matrix H*+1 will emerge in
proper form again.
Figure 1.5. Proper form of the generator recursion for inversion.
When, on the other hand, gi Jg* < 0 we let Gj be a J-unitary matrix that rotates
gi to the form (1.7.4). Then the expression for (1.10.7) Ui becomes
and the recursion for HI now reduces to
This equation has the following interpretation:

44 
Displacement Structure and Array Algorithms 
Chapter 1
1. Multiply H* by 6*.
2. Multiply the last column of H*Qi by \I>i and keep the rest of the columns unaltered.
3. Attach a zero row to the bottom of the array.
4. Add the correction term 0i[ 0* 1 ]* to the last column.
1.10.6 
Application to Pick Matrices
We reconsider the case of Pick matrices (1.8.4), which, as discussed at the end of
Sec. 1.8.2, did not yield to the embedding technique for the factorization of the in-
verse matrix. More specifically, by starting with the matrix P in (1.8.4) and by using
F â diag{/o, /i,..., /n_i}, we saw that the extended matrix
did not have low displacement rank with respect to M â (F Â® F}M(F 0 F}* since
/ â FF* is full rank in general.
However, the fast inversion algorithm just derived overcomes this difficulty. Indeed,
according to (1.8.5) we have
Now Lemma 1.5.1 implies that
for some matrix Hâ¬Crxn. This means that R~l is also a Pick matrix,
where [ e^ 
bi ] denotes the ith row of H*. It further follows from the diagonal struc-
ture of F that
The generator matrix H can then be determined by resorting to the fast inversion
Alg. 1.10.2 (or to the proper form of Sec. 1.10.5).
1.10.7 
The Degenerate Case
The derivation of the fast inversion algorithm of Sec. 1.10.2 was based on the non-
degeneracy condition (1.10.1), viz., that the diagonal entries of F are distinct. This
condition ensured that (1.10.7) had a unique solution for Ui.

Section 1.11. 
Non-Hermitian Toeplitz-Like Matrices 
45
In this section we show how to relax the nondegeneracy assumption (1.10.1). We do
so by focusing on the case when F consists of a single Jordan block so that /o = f\ =
â¢ â¢ â¢ = fn-i holds:
with 1â|/o|2 7^ 0. This case clearly includes the special choice F = Z (which corresponds
to /o =0). The argument we give here, however, can be extended easily to handle the
more general case of a matrix F with multiple (or even repeated) Jordan blocks.
For a matrix F as in (1.10.17), it is easy to verify from (1.10.7) that all the entries
of Hi can be determined uniquely from (1.10.7), except for the top entry of Ui. We
shall denote this top entry by &. This means that the fast inversion algorithm that we
derived in Sec. 1.10 almost completely identifies the upper triangular factor U with the
exception of its top row:
In the above expression, the symbol x denotes known entries. The unknown parameters
{&} can be identified by resorting to the fundamental equality (1.6.13), which provides
an upper triangular system of linear equations in the {&}:
Since the matrices {D,L} can be determined without ambiguity from the recursions of
the generalized Schur algorithm for the direct factorization problem, we can therefore
use the above linear system of equations and determine the {&}.
More specifically, let Li+i and -Dj+i denote the leading (i + 1) x (i + 1) submatrices
of L and D, respectively. Given {Z/i+i,Dj+i} and {Â£1,... ,^_i}, we can determine &
by solving
Therefore, the only additional computation relative to Alg. 1.10.2 is the need to deter-
mine the top entries {&} of the successive u^ as explained above. More can be said
about the inversion algorithm in the degenerate case. We omit the discussion here and
refer instead to [BSLK96], [BKLS98a], [BKLS98b].
1.11 
NON-HERMITIAN TOEPLITZ-LIKE MATRICES
The derivation in the earlier sections was primarily devoted to Hermitian Toeplitz-like
matrices R that satisfy displacement equations of the form R â FRF* = GJG*.
As mentioned before, we can also treat non-Hermitian Toeplitz-like matrices. Such
matrices admit a triangular factorization of the form R = LD~1U, where L is lower
triangular and U is upper triangular with identical diagonal entries, and which are equal

46 
Displacement Structure and Array Algorithms 
Chapter 1
to those of D. In the Hermitian case, U = L*. In what follows, we denote the (nonzero
parts of the) columns and rows of L and U by {k,Ui}, respectively. (Observe that we
are now using the letter U to denote the upper triangular factor of R and not of .R"1.
We are also writing Ui to denote a row of U.)
We shall not repeat the derivation of the Schur algorithm in this context but will
only state one of its forms; derivations can be found in [KS95a]. It will be noted that
the general form of the recursions is still very similar to what we had in the Hermitian
case, except that now we need to propagate two generator matrices. For reasons of
space, we present only the more compactly described (explicit) equation forms.
Algorithm 1.11.1 (Non-Hermitian Toeplitz-Like Matrices). Consider an n x n
strongly regular matrix R with displacement structure
where F and A are n x n lower triangular matrices whose diagonal entries are denoted
by {/i, Oj}, respectively, and G and B are nxr generator matrices. It is further assumed
that
Then the successive Schur complements of R satisfy
where {Fi, Ai} are the submatrices obtained after deleting the first row and column of
the corresponding {Fj_i, Aj_i} and Gi and Bi are (n â i) x r generator matrices that
satisfy the following recursions: start with GQ = (?, BQ = B, FQ = F, AQ = A and repeat
for i > 0:
where Oi and I\ are arbitrary matrices that satisfy 0iF* = /, gi and bi are the top rows
of Gi and Bi, respectively, and
The triangular factors are given by
Array forms of these recursions are also treated in [KS95a] and they can be described
briefly as follows. Choose the parameters O^ and F^ to reduce gi and bi to the forms
respectively, where the nonzero entries Xi and yi are in the same column position, say,
the jth position. (Generalizations of the Givens and Householder transformations can

Section 1.12. 
Schur Algorithm for Hankel-Like Matrices 
47
be obtained for finding {0;,^}âsee Sec. 4.4.3 of [KS95a].) Then it can be verified
that the generator recursions collapse to
wherdi = (xiJjjy*)/(l- fid*}, and
These algorithms are useful in studying (unconstrained) rational interpolation problems
(see Sec. 1.14.3 and [BSK94], [BSK99]).
1.12 
SCHUR ALGORITHM FOR HANKEL-LIKE MATRICES
To round out our discussions of generalized Schur algorithms, we finally consider the case
of strongly regular Hankel-like matrices. As with the Toeplitz-like structure, the Hankel-
like structure is also preserved under Schur complementation. Similar arguments will
show that the following recursions holdâthey are special cases of the general algorithm
first derived in [KS91], [Say92] (see also [SK95a], [KS95b], Sec. 7.2.5 of [KS95a]), and
are used in [GKO95].
Algorithm 1.12.1 (Schur Algorithm for Hankel-Like Matrices). Consider an nx
n strongly regular Hankel-like matrix that satisfies
where the diagonal entries of the lower triangular matrices F and A are denoted by
{/i)ai}5 respectively, and satisfy
Then the successive Schur complements Ri satisfy
where Fi and Ai are the submatrices obtained after deleting the first row and column
of Fi_i and A^i, respectively, and Gi and Bi are (n â i) x r generator matrices that
satisfy, along with ^ and Ui (the first column and row of Ri), the following recursions:

48 
Displacement Structure and Array Algorithms 
Chapter 1
where Qi and I\ are arbitrary parameters that satisfy 0iF* = /. Moreover,
Remark 1. Array forms for these equations also exist and are discussed in [KS95a].
Remark 2. The condition (1.12.2) is necessary to guarantee a unique solution R of the
displacement equation (1.12.1). It further guarantees that the expressions (1.12.6)-(1.12.8) are
well defined and uniquely determine the quantities {di,/i,tti}. When (1.12.2) is violated, so
that the inverses (Fi - a*In-i)~l and (fjn-i - A*)~l in (1.12.7) and (1.12.8) need not exist,
then we need to determine the {di,Zi,iii} by solving the equations
The nonsingularity of (Fi â a*In-i) and (filn-i â Al) would imply that these equations have
many solutions {di,li,Ui}. For this reason, additional information (often known as coupling
numbers) is needed to fully recover the {di,li,Ui}. These issues are not of major concern to
us here since the fundamental equations (1.12.4) and (1.12.5) will be the same. More detailed
discussions can be found in [KO96], [KO98] (see also [KS95a]).
1.13 
STRUCTURED MATRICES AND PIVOTING
An issue that arises in the study of fast factorization algorithms is their numerical sta-
bility in finite precision implementations. It was mentioned in Sec. 1.6.2 that the Schur
reduction procedure, which underlies the generalized Schur algorithm, is equivalent to
the Gaussian elimination procedure, because the latter can be rewritten as
Thus the generalized Schur algorithm amounts to combining Gaussian elimination with
structure. Now it is well known (see, e.g., [GV96], [TB97], and Sec. 4.2.1) that Gaussian
elimination in its purest form is numerically unstable (meaning that the error in the
factorization LD~1L* can be quite large, where L and D denote the computed L and D,
respectively). The instability often can be controlled by resorting to pivoting techniques,
viz., by permuting the order of the rows, and perhaps columns, of the matrices before
the Gaussian elimination steps.
In what is known as complete pivoting, a permutation matrix Pk is chosen at each
iteration k so as to bring the maximal magnitude entry in the entire matrix Rk to the
pivotal (0,0)th position. Such a procedure is computationally intensive since it requires
many comparisons. A less-demanding procedure is partial pivoting. In this case, the
permutation matrix Pk is chosen so as to bring at the fcth step the maximal magnitude
entry of the first column of Rk to the pivotal (0,0)th position. (Although partial pivoting
often performs satisfactorily, there are several examples where the numerical accuracy
of the factorization can still be poorâsee, e.g., [Hig96].) In either case, complete or
partial pivoting leads to the triangular factorization of a permuted verion of R, say,
PR = LD~1L*.

Section 1.13. 
Structured Matrices and Pivoting 
49
1.13.1 
Incorporating Pivoting into Generalized Schur Algorithms
Unfortunately, pivoting can destroy matrix structure and thus can lead to a loss in
computational efficiency. There are, however, matrices whose structure is unaffected by
partial pivoting, e.g., Vandermonde and Cauchy matrices or even Cauchy-like matrices,
as first noted and exploited by Heinig [Hei95].
Recall that Cauchy-like matrices are special cases of the class of Hankel-like matrices
in that they satisfy displacement equations of the form
where {F, A} are now diagonal. Let P denote a permutation matrix that permutes the
rows of R. Then PPT = I and we note that
where PFPT is still diagonal. We therefore see that the permuted matrix PR is still
Cauchy-like with respect to the displacement operators {PFPT, A} and has generator
matrices {PG,B}. In other words, partial pivoting does not destroy the Cauchy-like
structure.
More generally, partial pivoting does not destroy the matrix structure as long as
some displacement operators are diagonal, e.g.,
However, for definiteness, we continue our discussion here by focusing on the Cauchy-like
case; similar arguments apply in the other casesâsee, e.g., Sec. 2.10.
The following algorithm now follows immediately from Alg. 1.12.1 (and is used
in [GKO95]); it incorporates partial pivoting into the generalized Schur algorithm for
Cauchy-like matrices.
Algorithm 1.13.1. (Schur Algorithm for Cauchy-Like Matrices with Pivoting).
Consider an n x n strongly regular Cauchy-like matrix that satisfies
with diagonal {F, A} whose entries satisfy fi â a*j ^ 0 for all i,j. Start with GO = G,
BQ = B, FQ â F, AQ = A and repeat for i = 0,1,.. .:
1. Determine {li,Ui} from
2. Determine the position of the maximal magnitude entry of li, say, at the jth
position, and let Pi be the permutation matrix that exchanges it with the top entry
of li. Let di be equal to this maximal entry.
3. Likewise, exchange the (0,0) th diagonal entry of Fi with its (j,j)th diagonal entry.
Exchange also the first and the jth rows of Gi. At the end of this step, all three
quantities {li, Fi, Gi} have undergone permutation, but we continue to denote them
by the same symbols.

50 
Displacement Structure and Array Algorithms 
Chapter 1
4. Now update the generator matrices {Gi,Bi} using
where 6j and F; are arbitrary parameters that satisfy &iT* = I (e.g., &i = Ti =
I)-
At the end of the procedure we obtain the triangular factorization
where P is the combination of all the individual permutation matrices
Remark 1. For Hermitian Cauchy-like matrices R, viz., those that satisfy FR+RF* = GJG*,
partial pivoting destroys the symmetry. In such cases, one usually employs diagonal pivoting-^
see [KO98].
Remark 2. For the alternative cases (1.13.2)-(1.13.4), we simply incorporate steps similar
to steps 2 and 3 above into the corresponding recursions (in array form or not).
1.13.2 
Transformations to Cauchy-Like Structures
As noted above, incorporation of partial pivoting into the generalized Schur algorithm is
possible for Cauchy-like, Hankel-like, Toeplitz-like, and even generalized structures with
certain diagonal operators F or {F, fi}. But what about matrices not in these classes?
Heinig had the nice idea that one could first transform them into matrices to which
pivoting could be applied; in particular, he proposed transforming them to Cauchy-like
matrices [Hei95]. (Transformations between different kinds of structured matrices were
perhaps first proposed, in a different context, by Bini and Pan (see, e.g., [BP94] and
the references therein).) We illustrate the procedure in the Toeplitz case.
Thus consider an n x n non-Hermitian Toeplitz matrix T. As mentioned at the
begining of Sec. 1.3, there are many forms of displacement structure even for the same
matrix. In particular, T also has displacement rank 2 with respect to the displacement
operation
where Z^ denotes the 0-circulant matrix

Section 1.13. 
Structured Matrices and Pivoting 
51
Heinig [Hei95] showed that the above displacement equation can be transformed to
Cauchy-like form as follows. The matrix Z$ can be diagonalized by the scaled discrete
Fourier matrix
with (jj denoting the primitive nth root of unity. More specifically, it holds that
with
where Â£ is an arbitrary complex number satisfying Â£n = 0. Now define the transformed
matrix
Then R satisfies the Cauchy-like displacement equation
Note that R is in general complex valued even when T is real valued. This increases the
constant factors in the operation count due to the need for complex arithmetic. A study
of this procedure, with several examples and extensions, can be found in [GKO95].
1.13.3 
Numerical Issues
While the transformation-and-pivoting technique of the last two sections can be satis-
factory in many situations, it still suffers from two problems. First, the method applies
only to a fixed matrix size n x n, and the whole solution has to be repeated if the size of
the matrix is increased even by 1. Second, the procedure can still pose numerical prob-
lems because partial pivoting by itself is not sufficient to guarantee numerical stability
even for slow algorithms (see, e.g., the discussion and examples in Ch. 4 by Brent).
A more direct approach to the numerical stability of the generalized Schur algorithm
is to examine the steps of the algorithm directly and to stabilize them without resorting
to transformations among matrix structures. This is pursued in Chs. 2, 3, and 4. For
all practical purposes, the main conclusion of Chs. 2 and 4 is that the generalized Schur
algorithm is numerically stable for a large class of positive-definite structured matrices.
In Ch. 3, it is further shown how this conclusion can be extended to indefinite structured
matrices.
Chapter 4 by Brent provides, among other results, an overview of the conclusions
in [BBHS95]. This reference studied the stability of the generalized Schur algorithm
for the subclass of positive-definite quasi-Toeplitz structured matrices (F = Z and
displacement rank 2) and established that the triangular factorization provided by the
algorithm is in effect asymptotically stable regardless of the hyperbolic rotations. In
[SD97b] it was further shown that for higher displacement ranks, special care is needed
while implementing the rotations in order to still guarantee stable factorizations.
The results in [BBHS95] motivated Chandrasekaran and Sayed [CS96] to study the
stability of the generalized Schur algorithm for a wider class of matrices, viz., positive-
definite matrices R for which the shift structure matrix Z is replaced by a lower trian-
gular F (as in the definition (1.3.21)). Their conclusions are reviewed in Ch. 2, where
it is shown that, for all practical purposes, by incorporating a few enhancements to
the algorithm, it yields backward-stable factorizations for a wide class of structured
matrices.

52 
Displacement Structure and Array Algorithms 
Chapter 1
This is a reassuring conclusion. However, it applies only to positive-definite struc-
tured matrices. In [CS98], Chandrasekaran and Sayed further showed how to employ
the embedding ideas proposed in [KC94] to develop fast backward-stable solvers for lin-
ear systems of equations, say, Tx = 6, with possibly indefinite and even nonsymmetric
structured coefficient matrices T (see Ch. 3). This is achieved by transforming a prob-
lem that involves a nonsymmetric or indefinite structured matrix into an equivalent
problem that involves sign-definite matrices only (either positive definite or negative
definite). This is possible by introducing the larger matrix
and by observing that, regardless of T, the matrix M is always Hermitian. Moreover, its
leading block is positive definite and the Schur complement with respect to it is negative
definite (in fact, equal to â /). When T is structured, the matrix M also has structure
and its factorization can be carried out efficiently by means of the generalized Schur
algorithm. By factoring M fast and stably, the solution x of Tx = b can be determined
fast and stably. These results are reviewed in Ch. 3.
1.14SOME FURTHER ISSUES
Although a wide range of results has already been addressed in this chapter, there are
still several unmentioned results and applications. We give a brief outline of a few of
these items here, some of which are treated at greater length in later chapters. Other
items are covered in the article [KS95a].
1.14.1 
Incorporating State-Space Structure
A very powerful and well-studied structure in system theory is state-space structure.
A typical scenario is the following. We have a stochastic process {yi,i > 0} having a
model of the form
where {u^v^} are zero-mean white noise processes with covariance matrices
The initial state, XQ, is also assumed to be a zero-mean random variable with variance
HO and uncorrelated with {uj,Vi}, i.e.,
The processes are vector valued, with {iij} being g-dimensional, the states x^ being n-
dimensional, and the measurement noise {v^} and the output {y^} being p-dimensional.
It is assumed that {q, n,p} are known, as are all the matrices {F^, (7j, Hi, HO, Qi, Ri, Si}.
The solutions of many different problems associated with such models are closely
related (see, e.g., [KSH99]) to the triangular factorization of the covariance matrix of
the output process {yi}, say,

Section 1.14. 
Some Further Issues 
53
Although Ry is N x N, and N is often very large, the fact that it is the covariance
matrix of a process with an n-dimensional state-space model (where generally n -C N)
means that the triangular factorization should take fewer than the O(N3) flops required
for an arbitrary N x N matrix. In fact, the flop count is O(Nn3), which is achieved
via a so-called discrete-time Riccati recursion for an associated n x n matrix P^; this
is shown in books on state-space estimation and control (see, e.g., [KSH99] and the
references therein).
When the model parameters are time invariant, it turns out that the N x N matrix
Ry has displacement structure, with displacement rank r < n. In this case, the flop
count can be reduced to O(Nn2) by developing an appropriate generalized Schur al-
gorithm (see, e.g., [SLK94a]). The time-invariance assumption can actually be relaxed
somewhat to allow a structured form of time variation, which is encountered, for exam-
ple, in problems of adaptive filtering (see, e.g., [SK94a], [SK94b]). When displacement
structure is present, the Riccati recursion is replaced by certain so-called generalized
Chandrasekhar recursions, first introduced in [Kai73] and [MSK74]. The survey article
[KS95a] gives an outline of how state-space structure can be combined with displacement
structure.
In fact, the above studies inspired a generalization of the definition of displacement
structure, for example, using equations of the form
We refer to [SCK94], [SLK94b] for properties and applications of this extension.
The power of state-space representations makes it useful to seek to obtain them from
input-output descriptions. For time-variant systems, this has been studied in [DV98].
In Ch. 10 of this volume, Dewilde describes how these ideas can be combined with
displacement structure to obtain low-complexity approximations of matrices.
1.14.2 
Iterative Methods
Existing methods for the solution of linear systems of equations of the form Ax = b can
be classified into two main categories: direct methods and iterative methods. A direct
method or algorithm is primarily concerned with first obtaining the triangular or QR
factors of A and then reducing the original equations Ax = b to an equivalent triangular
system of equations. The generalized Schur algorithm of this chapter leads to a direct
method of solution.
An iterative method, on the other hand, starts with an initial guess for the solution
:r, say, #o, and generates a sequence of approximate solutions, {xk}k>i- The matrix A
itself is involved in the iterations via matrix-vector products, and the major concern
here is the speed of convergence of the iterations. To clarify this point, we note that we
can rewrite the equation Ax = b in the equivalent form
for an arbitrary invertible matrix C. This suggests the following iterative scheme (see,
e.g., [GV96]),
The convergence of (1.14.2) is clearly dependent on the spectrum of the matrix /âC~l A.
The usefulness of (1.14.2) from a practical point of view is very dependent on the choice
for C. For Toeplitz matrices, Strang [Str86] proposed certain circulant precondition-
ers C, which allow the use of the FFT technique to carry out the computations in a
numerically efficient and parallelizable manner.

54 
Displacement Structure and Array Algorithms 
Chapter 1
A survey of this method, with emphasis on Toeplitz linear equations and many later
developments, is provided by Chan and Ng in Ch. 5; closer study of the nullspaces
of appropriate displacement operators leads to new families of preconditioners [KO96].
The study of spectral properties of Toeplitz matrices is important in this theory. In
Ch. 6, Tilli provides a review of recent results in this area, especially for block Toeplitz
matrices. Fast algorithms for block Toeplitz matrices are developed in Ch. 8 by Bini
and Meini.
An iterative method that offers faster convergence rates than the above methods is
based on the use of Newton iterations. In Ch. 7, Pan et al. describe how displacement
structure ideas can be used to speed up the Newton iterations.
1.14.3 
Interpolation Problems
Interpolation problems of various types have a long history in mathematics and in
circuit theory, control theory, and system theory. Not surprisingly, this rich subject
can be approached in many ways and in different settings, often involving a lot of quite
abstract operator theory (see, e.g., the monographs [Hel87], [Dym89a], [FF90], [GS94]).
For the rational case, we have the somewhat more concrete state-space approach of
[BGR90].
In [Say92], [SK92], [SKLC94], a recursive approach to rational analytic interpolation
problems has been proposed that relies on the displacement structure framework; it
leads to a computationally efficient procedure that avoids matrix inversions. Reference
[SKLC94] elaborates on connections with earlier works on the subject.
The basis for the approach of [SKLC94] is the generalized Schur algorithm of this
chapter, which leads, as explained in Sec. 1.7.4, to a cascade of J-lossless first-order
sections, each of which has an evident interpolation property. This is due to the fact
that linear systems have "transmission zeros": certain inputs at certain frequencies
yield zero outputs. More specifically, each section of the cascade can be seen to be
characterized by a (p + q) x (p + q) rational transfer matrix, Â©^ (z), say, that has a left
zero-direction vector <ft at a frequency fa, viz.,
which makes evident (with the proper partitioning of the row vector gi and the matrix
function Â©i(-z)) the following interpolation property: aiÂ©i,i2@i~22(/i) = â&t- This sug-
gested to us that one way of solving an interpolation problem is to show how to construct
an appropriate cascade so that the local interpolation properties of the elementary sec-
tions combine in such a way that the cascade yields a solution to the global interpolation
problem. All possible interpolants can then be parametrized by attaching various loads
to the right-hand side of the cascade system. Details are provided in [SKLC94], [KS95a],
where different kinds of analytic interpolation problems are considered, including the
problems of Caratheodory, Nevanlinna-Pick, and Hermite-Fejer. An application to the
so-called four-block problem in HÂ°Â°-control can be found in [CSK94].
Actually, the arguments can also be extended to the very old class of unconstrained
rational interpolation problems. These problems have a long history, associated with
many classical results of Lagrange, Hermite, Prony, Pade, and other famous names. In
[BSK94], [BSK99], we showed how the generalized Schur algorithm for non-Hermitian
Toeplitz-like matrices [KS91], [Say92] (described in Sec. 1.11) can be used to give a
recursive solution.
It is noteworthy that the solution of interpolation problems can be reduced to the de-

Section 1.14. 
Some Further Issues 
55
termination of an appropriate fast matrix triangularization [SK92], [SKLC94], [BSK94].
This constructive view provides a nice complement to the many abstract formulations
of the important topic of interpolation theory.
1.14.4 
Inverse Scattering
An interesting interpretation of Schur's original recursion (1.7.16), when viewed in array
form, is that it arises as the most natural way of solving the inverse scattering problem
for discrete transmission lines (see [BK87b], [BK87a], [Kai87]). This interpretation gives
a lot of insight into and suggests new results and new proofs for a surprisingly diverse
set of problems. For example, references [BK87b], [BK87a] show how the transmission
line picture gives nice interpretations of the classical Gelfand-Levitan, Marchenko, and
Krein equations, and in fact yields various generalizations thereof; reference [BK87c]
discusses discrete Schrodinger equations; see also [BCK88] and [RK84].
Define 7? = \/l â J7i 2. Then the generator recursion of Schur's algorithm (cf.
(1.7.13)) can be depicted graphically as a cascade of elementary sections as shown in
Fig. 1.6.
Figure 1.6. The feedforward structure (cascade network) associated with Schur's recursion.
By reversing the direction of flow in the lower line, we get a physical lossless discrete-
time transmission line, as shown in Fig. 1.7, where each section is now composed of a
unitary gain matrix Ej (SjE* = /) followed by a unit-time delay element,
A physical motivation and derivation of a layered medium structure as in Fig. 1.7 can
be given by showing that it corresponds to a discretization of the wave propagation (or
telegrapher's) equations in an electromagnetic medium with varying local impedance;
the relevant details can be found, for example, in [Kai87]. The name reflection 
coeffi-
cients for the Schur coefficients {7;} arises from the picture in Fig. 1.7; at each section,
a fraction 7^ of the incoming signal is reflected and the rest, 7?, is transmitted.

56 
Displacement Structure and Array Algorithms 
Chapter 1
Figure 1.7. The feedback structure (transmission line) associated with Schur's recursion.
The so-called inverse-scattering problem that is associated with such layered media
is the following: given an arbitrary pair of input-response sequences of a layered medium
as in Fig. 1.7, say, {..., #20, XIQ, x00} and {..., t/2o, 2/io, 2/oo}, determine the medium (or
reflection) parameters {70,71,721 â¢ â¢ â¢}, under the assumption that the line was initially
quiescent. As mentioned above, this is a prototype of a famous problem, which has
been attacked in many ways. The most widely known are methods using special choices
of input sequences, based on which the inversion problem is shown to be equivalent
to the solution of sets of linear equations, special forms of which are as famous as the
Gelfand-Levitan, Marchenko, and Krein equations of classical inverse-scattering theory.
It turns out that a natural solution to the inverse scattering problem is Schur's array
form (see [BK87b], [BK87a]). This fact leads to several useful applications in other areas
including, among others, digital filter design and algebraic coding theory.

Chapter 2
STABILIZED SCHUR
ALGORITHMS
Shivkumar Chandrasekaran
Ali H. Sayed
2.1 INTRODUCTION
As mentioned in Ch. 1, linear systems of equations are generally solved by resorting
to the LDU factorization (or Gaussian elimination) of the coefficient matrix. But for
indefinite or nonsymmetric matrices, the LDU factorization is well known to be nu-
merically unstable if done without pivoting (see, e.g., [GV96], [Hig96], and also the
discussion in Ch. 4). Moreover, since pivoting can destroy the structure of a matrix, it
is not always possible to incorporate it immediately into a fast algorithm for structured
matrices without potential loss of computational efficiency.
It was observed in [Hei95], however, that for Cauchy-like structured matrices, piv-
oting can be incorporated into fast factorization algorithms without reducing the com-
putational efficiency of the algorithms (see the discussion in Sec. 1.13). This is because
for such matrices, the displacement operators are diagonal and, therefore, column and
row permutations do not destroy the Cauchy-like structure. The algorithm proposed
in [Hei95] was of a hybrid type, involving Schur-type and Levinson-type operations.
The technique was further used in [GKO95] to incorporate pivoting into the general-
ized Schur algorithm. This was achieved by first transforming different kinds of matrix
structure into Cauchy-like structure and then using the so-called generator recursions
of the Schur algorithm with partial pivoting. Sectionsl.13 and 4.3 of this book review
this approach to factorization.
While this transformation-and-pivoting addition to the generalized Schur algorithm
can be satisfactory in many situations, it still suffers from two problems. First, the
procedure can pose numerical problems because partial pivoting by itself is not sufficient
to guarantee numerical stability even for slow algorithms (see, e.g., the discussion and
examples in Ch. 4). It also seems difficult to implement complete pivoting in a fast
algorithm without accruing a considerable loss of efficiency. Second, the transformation
into a Cauchy-like structure makes it difficult to solve a linear system of equations of a
higher order by relying on the solution of a linear system of equations of a smaller order.
This is because once the size of the coefficient matrix is modified, say, by appending
one more row and column to it, the transformation to Cauchy-like structures has to be
57

58 
Stabilized Schur Algorithms 
Chapter 2
applied afresh to the new extended matrix and the previous calculations therefore must
be repeated. In this way, one of the major features of the generalized Schur algorithm
is lost, viz., the possibility to solve a sequence of nested linear systems by exploiting the
results of previous calculations (as already explained in Sec. 1.13.3).
2.2 
CONTRIBUTIONS AND PROBLEMS
A more direct approach to the numerical stability of the generalized Schur algorithm is
to examine the steps of the algorithm directly and to stabilize them without resorting to
transformations among matrix structures. In this chapter we follow such a direct route
to improving and ensuring the numerical stability of the generalized Schur algorithm
and, as a by-product, we shall further devise in Ch. 3 a new numerically stable solver for
linear systems of equations Rx = 6, with structured coefficient matrices R. There are
different notions of numerical stability in the literature. We follow the ones suggested
in [Bun85], [Bun87] and reviewed in Ch. 4. More specifically, the error bounds we
present for the algorithms developed here and in Ch. 3 will be such that they guarantee
(backward) numerical stability in the sense defined in Ch. 4.
This chapter and the following one provide an overview of some recent results by the
authors in [CS96], [CS98] and, for this reason, some derivations are not repeated here.
The main ideas and conclusions, however, are emphasized. Also, complete descriptions
of the algorithms are included for ease of reference.
Our exposition highlights three contributions:
1. We first show how to modify the generalized Schur algorithm of Ch. 1 in order
to guarantee a fast and numerically stable triangular factorization procedure for
positive-definite structured matrices R. For all practical purposes, the major
conclusion of this chapter is that the generalized Schur algorithm, with certain
modifications, is backward stable for a large class of structured matrices. This
conclusion extends earlier work by [BBHS95] (see also Ch. 4) on the stability of
a more specialized form of the algorithm. An overview of earlier works in this
direction is provided in what follows.
2. Once it is shown how to obtain a provably stable implementation of the gener-
alized Schur algorithm for positive-definite structured matrices, we then proceed
to show in Ch. 3 how the result can be used to solve in a stable manner linear
systems of equations with indefinite and possibly nonsymmetric structured coef-
ficient matrices R. In other words, we show how to use the stability results of
the positive-definite case to derive stable solvers even for the indefinite and non-
symmetric cases. This is achieved by exploiting in a suitable way the embedding
techniques of [KC94], which are also described in Sec. 1.8.
3. We provide a detailed numerical analysis of the proposed algorithms.
2.3 
RELATED WORKS IN THE LITERATURE
As already mentioned in Sec. 1.2, one of the most frequent structures, at least in signal
processing applications, is the Toeplitz structure, with constant entries along the diago-
nals of the matrix. A classical algorithm for the Cholesky factorization of the inverses of
such matrices is the Levinson algorithm [Lev47], [GV96], an error analysis of which has
been provided by Cybenko [CybSO]. He showed that, in the case of positive reflection
coefficients, the residual error produced by the Levinson procedure is comparable to

Section 2.3. 
Related Works in the Literature 
59
the error produced by the Cholesky factorization [GV96, p. 191]; i.e., the algorithm is
weakly stable (cf. Sec. 4.5.1).
A related analysis has been carried out by Sweet [Swe82] for the Bareiss algorithm
[Bar69], which was later recognized as being closely related to the algorithm of Schur
[Schl7], [Kai86]; these are fast procedures for the Cholesky factorization of the Toeplitz
matrix itself rather than its inverse (cf. Ch. 1 of this book). Sweet concluded that the
Bareiss algorithm is asymptotically stable.
In recent work, Bojanczyk et al. [BBHS95] further extended and strengthened the
conclusions of Sweet [Swe82] by employing elementary downdating techniques [APP88],
[BBDH87], [BS88] that are also characteristic of array formulations of the Schur algo-
rithm [KS95a], [SCK95]. They considered the class of quasi-Toeplitz matrices (viz., with
a generator matrix G having two columns in the definition (1.2.4)âwith p = q â 1),
which includes the Toeplitz matrix as a special case, and provided an error analysis that
established that the Schur algorithm for this class of matrices is asymptotically stable.
Contributions of Our Work
The results of Bojanczyk et al. [BBHS95] motivated us to take a closer look at the
numerical stability of a generalized Schur algorithm [KS95a], [Say92], [SK95a] that
applies to a more general class of structured matrices, viz., all positive-definite matrices
R that satisfy displacement equations of the form R â FRFT = GJGT, where F is
a stable lower triangular matrix (i.e., its diagonal entries have magnitude less than
unity). This class clearly includes the case of quasi-Toeplitz matrices (by choosing
F = Z). Several complications arise in this more general case when a matrix F is
used rather than Z. In this chapter we provide an overview of the results of [CS96],
where we propose several enhancements to the generalized Schur algorithm of Ch. 1
in order to ensure numerical stability while evaluating the Cholesky factor L in the
factorization LLT. Hence, the current chapter is concerned with the numerical stability
of the triangular factorization procedure.
Chapter 3, on the other hand, is concerned with the solution of linear systems of
equations even for more general structured matrices (that need not be positive definite
or even symmetric as above). More specifically, in Ch. 3 we use the stability results of the
current chapter to develop a fast stable solver for linear systems of equations, Tx = 6,
with possibly indefinite or nonsymmetric structured coefficient matrices T [CS98].5 As
is well known, apart from the classical Gaussian elimination procedure, another way to
solve the linear system of equations Tx = b is to compute the QR factorization of the
coefficient matrix T. For structured matrices, the computation has to be performed
rapidly and several fast methods have been proposed earlier in the literature [BBH86],
[CKL87], [Cyb83], [Cyb87], [Swe84], but none of them are numerically stable, especially
since the resulting Q matrix is not guaranteed to be orthogonal (see Sec. 1.8.3 and also
the discussion in Sec. 4.6).
In Ch. 3 we circumvent this difficulty by describing a new fast algorithm by the
authors that provides a modified factorization for the coefficient matrix. The new
algorithm relies on the observation that it is not really necessary to limit ourselves to
LDU or QR factorizations of the coefficient matrix T in order to solve the linear system
of equations Tx = b. If other factorizations for T can be obtained in a fast and stable
manner, and if they are also useful for solving the linear system of equations, then
these factorizations could be pursued as well. In fact, the new algorithm, rather than
5We are now denoting the coefficient matrix by T to distinguish it from the notation R for the R
factor in the QR factorization of a matrix.

60 
Stabilized Schur Algorithms 
Chapter 2
returning Q, returns two matrices A and Q such that A is triangular and the product
A-1Q is "numerically orthogonal"; it provides a factorization for the coefficient matrix
T that is of the form
where we are now using R to denote an upper triangular matrix (just like the notation
used to denote the R factor in the QR factorization of a matrix). The above factorization
is in terms of three matrices {A, Q, -R}. The factorization is of course not unique, since
we can replace A by any invertible matrix. The point, however, is that our algorithm
returns that A that allows us to compensate for the fact that Q is not "numerically"
orthogonal. More important, these factors are then used to solve Tx = b both fast and
in a backward-stable manner. The details are provided in Ch. 3.
2.3.1 
Notation
In the discussion that follows we use || â¢ ([2 to denote the 2-norm of its argument (either
Euclidean norm for a vector or maximum singular value for a matrix). We further
assume, without loss of generality, that F is represented exactly in the computer. Also,
the ~ notation denotes computed quantities, while the 7 notation denotes intermediate
exact quantities. We further let Â£ denote the machine precision and n the matrix
size. We also use subscripted <5's to denote quantities bounded by machine precision in
magnitude and we write On(e) to mean O(ec(n)) for some polynomial c(n) in n, which
we usually do not specify. The special form of c(ri) depends on the norm used and on
other details of the implementation.
We assume that in our floating point model, additions, subtractions, multiplications,
divisions, and square roots are done to high relative accuracy, i.e.,
where o denotes +, â, x, or -j- and |<5| < e. The same is true for the square root
operation. This is true for floating point processors that adhere to the IEEE standards.
2.3.2 
Brief Review of Displacement Structure
A rather detailed exposition of displacement structure can be found in Ch. 1 of this book
(and also in [KS95a] for more general non-Hermitian structures). Here we highlight only
some of the basic equations and notation. We shall focus in this chapter, without loss
of generality, on real-valued matrices. The analysis and results can be extended to the
complex case.
We start with a symmetric matrix R G Enxn that satisfies a displacement equation
of the form
with a "low" rank matrix G, say, G â¬ Rnxr with r <C n. Equation (2.3.1) uniquely
defines R (i.e., it has a unique solution R} if and only if the diagonal entries of the lower
triangular matrix F satisfy the condition
This uniqueness condition will be assumed throughout the chapter, although it can be
relaxed in some instancesâsee Ch. 1 and also [KS95a].
As explained in Ch. 1, the pair (G, J) is said to be a generator pair for R since,
along with F, it completely identifies R. Note, however, that while R has n2 entries,

Section 2.3. 
Related Works in the Literature 
61
the matrix G has nr entries and r is usually much smaller than n. Therefore, algorithms
that operate on the entries of G, with the purpose of obtaining a triangular factorization
for R, will generally be an order of magnitude faster than algorithms that operate on the
entries of R itself. The generalized Schur algorithm is one such fast O(rn2) procedure,
which receives as input data the matrices (F, (7, J) and provides as output data the
Cholesky factor of R.
The notion of structured matrices can also be extended to nonsymmetric matrices
R. In this case, the displacement of R is generally defined with respect to two lower
triangular matrices F and A (which can be the same, i.e., F = Aâsee (2.3.5)),
Again, this displacement equation uniquely defines R if and only if the diagonal entries
of F and A satisfy 1 â faaj ^ 0 for all i,j, a condition that will also be met in this
chapter.
Several examples of matrices with displacement structure are given in Sec. 1.3, in-
cluding Toeplitz, Hankel, Pick, Cauchy, and Vandermonde matrices. The concept is
perhaps best illustrated by considering the much-studied special case of a symmetric
Toeplitz matrix, T = [t^-j^ ._0, to = 1-
Let Z denote the nx n lower triangular shift matrix with ones on the first subdiagonal
and zeros elsewhere (i.e., a lower triangular Jordan block with zero eigenvalues):
It can be checked easily that the difference T â ZTZT has displacement rank 2 (except
when all ti, i 7^ 0, are zero) and a generator for T is {G, (1 0 â 1)}, where
Similarly, for a nonsymmetric Toeplitz matrix T = [ti-j]â¢~=Q , we can easily verify that
the difference T â ZTZT has displacement rank 2 and that a generator (G, B) for T is
This is a special case of (2.3.2) with F = A = Z. In particular, any matrix T (symmetric
or not) for which (T â ZTZT] has rank 2 is called quasi Toeplitz. For example, the
inverse of a Toeplitz matrix is quasi Toeplitz (see Ch. 1). For higher displacement ranks,
but still with F = A = Z, we shall say that the matrix is shift structured. For example,
the product of two Toeplitz matrices is shift structured with displacement rank 4 (see,
e.g., Ch. 1 and [KS95a]). Also, examples of structured matrices with diagonal {F, A}
in (2.3.2) are given in Ch. 1.

62 
Stabilized Schur Algorithms 
Chapter 2
2.4 THE GENERALIZED SCHUR ALGORITHM
In this chapter we focus on symmetric positive-definite matrices R with displacement
rank 2 with respect to a lower triangular matrix F, viz., matrices R that satisfy dis-
placement equations of the form
where UQ and VQ denote the n x 1 column vectors of G and F is lower triangular with
diagonal entries whose magnitude is less than unity. The results and conclusions can be
extended easily to higher displacement ranks (and will be briefly mentioned in Sec. 3.1).
Recall also from Ch. 1 that since the generator matrix G is highly nonunique, it can
always be in the so-called proper form
That is, the top entry of v\, vu, can always be chosen to be zero. Indeed, assume that
a generator G for R is found that does not satisfy this requirement, say,
It then follows from (2.4.1) that the (0,0) entry of R, which is positive, is given by
Consequently, |UQO| > \VQQ\ and a hyperbolic rotation Q can always be found in order to
reduce the row [ UQQ VQQ ] to the form [ Â±\/|uoo|2 ~ l^ool2 
0 ] (see App. B). The
matrix GO can then be used instead of G as a generator for R.
We now restate for convenience the generalized Schur algorithm in array form, which
operates on the entries of (F, G, J) and provides the Cholesky factor of R. This state-
ment is of course a special case of the algorithm derived in Sec. 1.7. We note, however,
that we are now denoting the triangular factorization of the positive-definite R simply
by R = LLT (rather than by R = LD~1LT as in (1.6.1)).
Algorithm 2.4.1 (Generalized Schur Algorithm). Consider a symmetric positive
definite matrix RRnxnsatisfying(2.4.1).
â¢ Input: A stable lower triangular matrix F, a generator GQ â G in proper form,
with columns denoted by UQ and VQ, and J = (1 Â© â 1).
â¢ Output: The lower triangular Cholesky factor L of the unique symmetric positive-
definite matrix R that satisfies (2.4.1), R = LLT.

Section 2.4. The Generalized Schur Algorithm 
63
â¢ Computation: Start with (UQ, VQ), FQ = F, and repeat for i = 0,1,..., n â I:
1. Compute the matrix 3>i = (Fi-fiI}(IâfiFi)~lNote that $i is (n-i}x(nâi)
lower triangular and that its first row is always zero.
2. Form the prearray of numbers [ $iUj 
Vi ]. Since [ Ui Vi ] is assumed
in proper form and since the first entry of 3>iUi is always zero, the prearray
therefore has the form
That is, its top row is zero.
3. Apply a hyperbolic rotation 0j in order to annihilate the (1,2) entry ofGi+i,
thus reducing it to proper form, say,
We denote the resulting columns of Gi+i by {uj+i, v$+i} and write, more
compactly,
4. The ith column of the Cholesky factor L is given by
where the top i entries are zero.
5. Fi+i is obtained by deleting the first row and column of Fi.
After n steps, the algorithm provides the Cholesky decomposition
Recall also from Remark 2 after Alg. 1.6.2 that the successive matrices Gi that are
obtained via the recursion have an interesting interpretation. Let RI denote the Schur
complement of R with respect to its leading i x i submatrix. That is, RQ = R, R\ is
the Schur complement with respect to the (0,0)th top left entry of R, R^ is the Schur
complement with respect to the 2 x 2 top left submatrix of R, and so on. The matrix
Ri is therefore (n â i) x (n â i). Then
In other words, Gi is a generator matrix for the ith Schur complement, which is also
structured. Note that both Gi and Gi can be regarded as generator matrices for the
ith Schur complement R^.

64 
Stabilized Schur Algorithms 
Chapter 2
2.5 
A LIMIT TO NUMERICAL ACCURACY
Given a symmetric positive-definite matrix R (not necessarily structured), if its Cholesky
factor is evaluated by any standard backward stable method that operates on the entries
of R, e.g., by Gaussian elimination (see [GV96, Ch. 4] and also Ch. 1) the corresponding
error bound is given by
where Â£ is the machine precision.
A fundamental question that needs to be answered then is the following: given
(F, G, J) but not R, how accurately can we expect to be able to compute the Cholesky
factorization of R irrespective of the algorithm used (slow or fast}! The example and
discussion that follows justifies the following conclusion [CS96].
Lemma 2.5.1 (Limit of Accuracy). Irrespective of the algorithm we use (slow or
fastj, if the input data is (F, G, J), for a general lower triangular F, we cannot expect
a better bound than
Proof: The claim is established in [CS96] by constructing a simple example. We
highlight the main steps here.
To begin with, note that just representing (F, G] in finite precision already induces
round-off errors. This fact in turn imposes limits on how accurate an algorithm that
employs (F, G] can be.
Consider the following example. Let F be a stable diagonal matrix with distinct
entries {fi} and assume /o is the largest in magnitude. Let the entries of the column
vectors UQ and VQ be constructed as follows:
where 7 is chosen such that 0 < 7 < 1. The unique matrix R that solves (2.4.1) for the
given (F, UQ,VQ) can be shown to be symmetric positive definite.
When the data (UQ,VQ) are stored in finite precision, round-off errors are bound
to occur. Let us assume that only a relative perturbation occurs in the first entry of
UQ, while all other entries of UQ and VQ remain unchanged. That is, let us define the
perturbed vectors UQ and VQ with
where 6 is a small number (for example, for round-off errors, |<5| is smaller than machine
precision). We also assume that F is stored exactly. Hence, the only source of error we
are assuming is in UQQ. 
^
An algorithm that is intended to factor R will in fact be factoring the matrix R
that is defined as the unique solution of the following displacement equation with the
perturbed generator matrix,
The difference between this matrix and the original matrix R is denoted by the error
matrix E = R â R. How big can E be? It is easy to verify that E is the unique solution
of

Section 2.6. Implementations of the Hyperbolic Rotation65
Using this fact, it can be verified that [CS96]
where <8> denotes the Kronecker product. This expression provides a lower bound on
the norm of the error matrix. It is further argued in [CS96] that by choosing /0 and 7
sufficiently close to one, the norm of the original matrix R can be made much smaller
than the above bound.
Hence, in general, we cannot expect the error norm, \\R â LLT||, for any algorithm
(slow or fast) that uses (F, G, J) as input data (but not R) to be as small as co|<5|||.R||
for some constant CQ.
The above perturbation analysis indicates the best accuracy that can be expected
from any finite precision algorithm that uses the generator matrix as the input data.
The issue now is to show that the generalized Schur algorithm can essentially achieve
this bound if certain care is taken during its implementation. We shall show in this
chapter that, in general, this requires that we incorporate four kinds of enhancement:
1. A careful implementation of the hyperbolic rotation 9j that is needed in each step
of the algorithm (Sec. 2.6).
2. A careful implementation of the Blaschke-vector product $iUi that is also needed
in each step of the algorithm (Sec. 2.7).
3. Enforcing positive definiteness of the successive Schur complements to avoid break-
down (Sec. 2.8).
4. Control of potential growth of the norms of the successive generator matrices
(Sec. 2.11). We may remark that pivoting strategies can be useful in controlling
generator growth when F is diagonal with norm close to unity.
For all practical purposes, the major conclusion (see Sec. 2.9) of the analysis will be
that the modified Schur algorithm is a backward-stable procedure for a large class of
positive-definite structured matrices.
2.6 
IMPLEMENTATIONS OF THE HYPERBOLIC ROTATION
Each step (2.4.3) of the generalized Schur algorithm requires the application of a hy-
perbolic rotation Â©j. The purpose of the rotation is to rotate the top row of the GrÂ»+i,
which is the second row of the [ ^u^ Vi ], to proper form. If we denote the top row
of GÂ»+i by
then the expression for a hyperbolic rotation that transforms it to the form
is given by
The positive definiteness of R guarantees \pi\ < 1.
65

66 
Stabilized Schur Algorithms 
Chapter 2
Expression (2.4.3) shows that in infinite precision, the generator matrices Gi+i and
Gi+i must satisfy the fundamental requirement
Obviously, this condition cannot be guaranteed in finite precision. But it turns out that
with an appropriate implementation of the transformation (2.4.3), equality (2.6.2) can
be guaranteed to within a "small" error. The need to enforce the condition in finite
precision was first observed for the F = Z case by Bojanczyk et al. [BBHS95].
2.6.1 
Direct Implementation
A naive implementation of the hyperbolic transformation (2.4.3) can lead to large errors.
Indeed, in finite precision, if we apply 9j directly to GJ+I we obtain a computed matrix
Gi+i such that [CS96]
But since ||9i|| can be large, the computed quantities are not guaranteed to satisfy
relation (2.6.2) to sufficient accuracy. This possibly explains the disrepute into which
fast algorithms have fallen.
Interestingly though, Bojanczyk et al. [BBHS95] showed that for the special case
F = Z and displacement rank r = 2, the direct implementation of the hyperbolic
rotation still leads to an asymptotically backward stable algorithm. This conclusion,
however, does not hold for higher displacement ranks. Stewart and Van Dooren [SD97b]
showed that for F = Z and r > 2, the direct implementation of the hyperbolic rotation
can be unstable.
We proceed to review alternative methods for implementing the hyperbolic rotation
that can be used for general F, including F = Z, and also for higher displacement ranks.
2.6.2 
Mixed Downdating
One possible way to ameliorate the above problem is to employ the mixed downdating
procedure as suggested by Bojanczyk et al. [BBHS95], [BBDH87].
Assume we apply a hyperbolic rotation 0 to a row vector [ x y ], say,
Then, more explicitly,
Solving for x in terms of x\ from the first equation and substituting into the second
equation we obtain
An implementation that is based on (2.6.5) and (2.6.7) is said to be in mixed downdating
form. It has better numerical stability properties than a direct implementation of G as
in (2.6.4).

Section 2.6. 
Implementations of the Hyperbolic Rotation 
67
In the above mixed form, we first evaluate x\ and then use it to compute y\. We
can obtain a similar procedure that first evaluates y\ and then uses it to compute x\.
For this purpose, we solve for y in terms of y\ from (2.6.6) and substitute into (2.6.5)
to obtain
Equations (2.6.6) and (2.6.8) represent the second mixed form.
Using this scheme to implement the hyperbolic transformation (2.4.3) guarantees
(cf. [BBHS95], [BBDH87])
This bound is sufficient, when combined with other modifications suggested in Sees. 2.7
and 2.11, to make the algorithm numerically reliable (Sec. 2.9).
2.6.3 
The Orthogonal-Diagonal Procedure
An alternative scheme that was employed in [CS96] is based on using the singular value
decomposition (SVD) representation of a hyperbolic rotation 0. Its good numerical
properties derive from the fact that the hyperbolic rotation is applied as a sequence of
orthogonal and diagonal matrices, which we shall refer to as the orthogonal-diagonal
(OD) procedure. Its other advantage is that it is a general technique that can be applied
in other situations. It can be implemented with the same operation count as the mixed
downdating algorithm of [BBHS95].
It is straightforward to verify that any hyperbolic rotation of the form (2.6.1) admits
the following eigen(SVD-)decomposition:
where the matrix
is orthogonal (QiQ? = I).
If the eigendecomposition QiDiQj is now applied to the prearray GI+I in (2.4.3),
then it can be shown that the computed generator matrix Gi+i satisfies [CS96]
and
with
Algorithm 2.6.1 (The OD Procedure). Given a hyperbolic rotation O with reflec-
tion coefficient p = ftfa, \p\ < I, and a prearray row vector [ x 
y ], the postarray row
vector [ xi yi ] can be computed as follows:

68 
Stabilized Schur Algorithms 
Chapter 2
The above algorithm guarantees the error bounds
with
2.6.4 
The H Procedure
Another method introduced in [CS96] is the following. Let p = (3fa be the reflection
coefficient of a hyperbolic rotation Â©,
with \p\ < 1. Let [ Xi 
y\ ] and [ x 
y ] be the postarray and prearray rows, respec-
tively:
Algorithm 2.6.2(TheProcedure}. Givenahyperbolicrotation9withreflection
coefficient 
p = (3/a, \p\ < I, and a prearray [ x 
y ] with \x\ > \y\, the postarray
[ x\ yi ] can be computed as follows:

Section 2.7. Implementation of the Blaschke Matrix 
69
The advantage of the H procedure is that the computed quantities x\ and yi will
satisfy the equation
where the error terms satisfy
Comparing with (2.6.9) we see that the prearray is not perturbed. Moreover, we shall
show in Sec. 2.13.2 that by a slight modification we can further enforce that \x\\ > |yi|,
which is needed to prevent breakdown in the algorithm. (If \x\ < |y|, then it can be
seen that [ y x ] 9 = [ yi xi ]. Therefore, without loss of generality, we considered
above only the case |x| > |y|.)
The the H procedure requires 5n to 7n multiplications and 3n to 5n additions. It
is therefore costlier than the OD procedure, which requires 2n multiplications and 4n
additions. But the H procedure is forward stable (cf. (2.6.10)), whereas the OD method
is only stable (cf. (2.6.9)).
Prom now on we shall denote by Ui+i and Vi+i the computed generator columns at
step i, i.e., Gi+i = [ ui+i vi+i ] , starting with Gi = [ 3>iUi 
Vi ] .
2.7 
IMPLEMENTATION OF THE BLASCHKE MATRIX
Each step of the Schur algorithm also requires multiplying the Blaschke matrix 3>i by
Ui. (Recall that the top entry of $iUi is always zero and, hence, can be ignored in the
computation.) In this section, we consider the following two cases:
â¢ F is stable and diagonal, in which case $i itself is diagonal and its entries are
given by
â¢ F is strictly lower triangular, e.g., F = Z, F = (Z Â© Z], or other more involved
choices. In these situations, the matrix $j is equal to Fi itself since the /Â» are all
zero, $i = F^
2.7.\ 
The Case of Diagonal F
The goal of this section is to show how to compute QiUi to high componentwise relative
accuracy (i.e., high relative accuracy for each component of the computed vector). Here,
Ui denotes the computed value of i^.
The numerator of each diagonal entry of 3>j can be computed to high relative accu-
racy as
Computing the denominator x^- = (1 â fifj) to high relative accuracy is a bit trickier,
as the following example shows.
Let /i = /2 = 0.998842. Then in 6-digit arithmetic l-/i/2 Â« 2.31500xlO~3, whereas
the actual answer is 2.31465903600 x 10~3. Therefore, the relative error is approximately
1.5 x 10~4. Using the scheme given below, we find 1 - /i/2 Â« 2.31466 x 10~3. The
relative error is now approximately 4.2 x 10~7.
The scheme we use to compute x^ is as follows:

70 
Stabilized Schur Algorithms 
Chapter 2
It can be shown that this scheme ensures that a?^- is computed to high relative
accuracy [CS96], viz., that
Knowing how to compute <Â£Â» to componentwise accuracy, and since 3>j is diagonal,
the entries of $iUj can be computed to componentwise high relative accuracy. More
specifically,
We should remark that this scheme is not totally successful when F is a general trian-
gular matrix (for example, when F is bidiagonal). A way around this difficulty will be
addressed elsewhere. But for a strictly lower triangular F, the situation is far simpler
as shown in the next subsection.
For now, let us consider how to compute the nonzero part of the /i's. Define
with i leading zeros. We use the expression
to evaluate the nonzero entries of fÂ», with the technique explained above for the denom-
inator (1 â fifj). 
(The notation (ui)k denotes the kth entry of u;.) Then we can also
show that the nonzero entries of li and li satisfy
for some polynomial in n, c\(n}.
2.7.2The Case of Strictly Lower Triangular F
When F is strictly lower triangular we obtain $i = Fj. Hence, here we use the standard
matrix-vector multiplication to evaluate $iUi and the computed quantities will then
satisfy [GV96]
Moreover, since fi = 0,

Section 2.8. 
Enforcing Positive Definiteness 
71
2.8 
ENFORCING POSITIVE DEFINITENESS
The computed successive Schur complements have to be positive definite to guarantee
that the successive reflection coefficients are all strictly less than unity in magnitude.
These facts can be enforced in finite precision by imposing a condition on the smallest
eigenvalue of R and by introducing a computational enhancement during the evaluation
of the reflection coefficients.
Define the matrix Si that solves the displacement equation
If the {ui,Vi} were exact and equal to {ui,Vi}, then the Si would correspond to the zth
Schur complement Ri of R (cf. (2.4.6)). The matrix Rt is necessarily positive definite
since R is positive definite. However, because we are now dealing with computed values
rather than exact values, we need to guarantee that the computed generator columns
{ui,Vi} define a positive-definite matrix Si.
It was shown in [CS96] that this can be guaranteed by imposing a condition on the
smallest eigenvalue of R, viz., by guaranteeing that R is sufficiently positive definite.
Define
where the lt are intermediate exact quantities computed via (2.7.2). Then we can ensure
the positive definiteness of the computed Schur complements if
The condition further guarantees, due to positive definiteness, that |$iUj|2 > \Vi\2- (The
inequality compares the second entries of $iUj and Vi.) This ensures that the reflection
coefficients will be smaller than one in magnitude, a condition that we can enforce in
finite precision as follows:
This enhancement, along with condition (2.8.2), can be shown to guarantee that the
algorithm will be completed without any breakdowns.
2.9 
ACCURACY OF THE FACTORIZATION
Using the above enhancements (regarding the implementations of the hyperbolic rota-
tion and the Blaschke-vector product and the enforcement of positivity), it was shown
in [CS96] that the following error bound holds.
Theorem 2.9.1 (Error Bound). Consider a symmetric positive-definite matrix R â¬
Rnxn satisfying (2.4.1) and (2.8.2), with a stable diagonal or strictly lower triangular F.

72 
Stabilized Schur Algorithms 
Chapter 2
The generalized Schur algorithm, when implemented as detailed above (see also listing
in App. 2.A), guarantees the following error bound:
The term \\(I â F Â® F)~l\\z in the error bound is expected from the perturbation
analysis of Sec. 2.5. However, the presence of the norms of the successive generators
makes the error bound larger than the bound suggested by the perturbation analysis,
which depends only on the norm of the first generator matrix.
The natural question then is, How big can the norm of the generators be? We
consider the following cases separately.
1. Fstrictly lower triangular. Using (2.4.4) we can verify that ||ui||2 < \\R\\2- It then
follows that the error bound is as good as can be expected from the perturbation
analysis of Sec. 2.5.
2. F strictly lower triangular and contractive. More can be said if F is further
assumed to be contractive (H^F1^ < !)â¢ To see this, we first note that the error
bound in the case when F is strictly lower triangular can be rewritten as
When F is contractive, this further implies that
But since we also have ||uj||2 < ||-R||2, we conclude that the factorization algorithm
is backward stable.
This result applies to the important class of positive-definite quasi-Toeplitz matri-
ces, which corresponds to F = Z. In this case, the above conclusion strengthens
the result of Bojanczyk et al. [BBHS95], which states that for quasi-Toeplitz sym-
metric positive-definite matrices, the Schur algorithm is asymptotically backward
stable. Our analysis shows that the modified algorithm proposed here is backward
stable provided the smallest eigenvalue of the quasi-Toeplitz matrix satisfies
If F is strictly lower triangular but noncontractive, then the error norm can pos-
sibly depend on \\(I â F <8> F)"1^, as shown in the previous case.
3. F diagonal. Using the fact that R is positive definite and (2.4.4), we can verify
that

Section 2.10. 
Pivoting with Diagonal F 
73
This shows that the growth of the generators depends on || (/ â F <8> F)~l ([2. This
may suggest that the norm of the error can become very large when the magnitude
of the diagonal entries of F becomes close to one. But this is not necessarily the
case (see also the numerical example in the next section) since we can further
strengthen the error bound as follows.
Define
It then holds in the diagonal case that
where the Ri are defined by
The bound (2.9.3) is independent of the {fi}\ In other words, if the coefficients
Pjti defined above are sufficiently smaller than one, then the algorithm will still
be backward stable irrespective of how close the {|/i|} are to one.
What does this say about the stability of the generalized Schur algorithm for a
diagonal and stable F? Clearly, when the eigenvalues of F are sufficiently far from
one the method has excellent numerical stability. The algorithm degrades as the
eigenvalues of F get closer to one. This is to be expected from the perturbation
analysis (whether we use a slow or a fast algorithm). However, if the generators
grow rapidly (i.e., as fast as (2.9.2)), then the algorithm degrades faster than the
rate predicted by the perturbation analysis.
Is there anything further we can do to ameliorate this problem? One thing we
have not considered yet is pivoting, which is possible only when F is diagonal.
2.10PIVOTING WITH DIAGONAL F
When F is diagonal it is possible to accommodate pivoting into the algorithm [Hei95]; it
corresponds to reordering the //s, Uj^s, and Vj^s identically at the zth iteration of the
algorithm. This has the effect of computing the Cholesky factorization of PRPT', where
P is the product of all the permutations that were carried out during the algorithm.
In finite precision, pivoting strategies are employed in classical Cholesky factorization
algorithms when the positive-definite matrix is numerically singular. In the context of
the generalized Schur algorithm, the main motivation for pivoting should be to keep
the norm of the generator matrices as small as possible (see also Sec. 4.4.2). This is
suggested by the expression for the error bound in (2.9.1), which depends on the norm
of the generators. Note that this motivation has little to do with the size of the smallest
eigenvalue of the matrix.
We would like to emphasize that pivoting is necessary only when the norm of F is
very close to one, as otherwise the generators do not grow appreciably (cf. (2.9.2)).

74 
Stabilized Schur Algorithms 
Chapter 2
2.10.1 A Numerical Example
The first question that arises then is whether there exists a pivoting strategy that guar-
antees a small growth in the norm of the generators. Unfortunately, we have numerical
examples that show that irrespective of which pivoting strategy is employed, the norms
of the generators may not exhibit significant reduction.
Consider the matrix R that satisfies the displacement equation R â FRFT = GJGT
with
>
and
F = diagonal{0.9999999, -0.9999989,0.9999976, -0.9999765}.
The matrix R is positive definite. Table 2.1 lists the values of the sum ]T)â¢~o ll^jlli
(scaled by 10~6 and denoted by S) for all the 24 possible pivoting options of the rows
of the generator matrix G. The results indicate that none of the pivoting options
significantly reduces the size of the generators.
2.10.2 
The Case of Positive F
This raises the next question: Is pivoting useful at all? It is useful when the entries of
the F matrix are strictly positive (or negative). In this case, we permute the entries
of F (and, correspondingly, the entries of u\ and ^i) such that the diagonal of F is in
increasing order in magnitude. Then it can be shown that [CS96]
which makes the first-order term or me upper oouna on E depend only on the first
power of ||(I - F Â® F)-l||2.
Table 2.1. Values of the scaled sum S for all 24 pivoting options.
2.10.3 
The Nonpositive Case
When F is not positive, the example in Sec. 2.10.1 suggests that pivoting may not help
in general. However, it may still be beneficial to try a heuristic pivoting strategy to

Section 2.11. 
Controlling the Generator Growth 
75
control the growth of the generators. Ideally, at the ith iteration we should pick the
row of the prearray Gi+i which would lead to the smallest (in norm) postarray Gi+\.
Since there seems to be no efficient way to do this we suggest picking the row that leads
to the smallest reflection coefficient (in magnitude) for the hyperbolic rotation Â©i. As
suggested by the example of Sec. 2.10.1, an alternate strategy would be to order the
/i's in increasing order of magnitude.
We may stress that pivoting is relevant only when the norm of F is very close to
one, as indicated by the error bound (2.9.1) and by (2.9.2).
2.11 
CONTROLLING THE GENERATOR GROWTH
We have shown before that the generators do not grow (i) if F is strictly lower triangular
and contractive or (ii) if F is a positive diagonal matrix with increasing diagonal entries.
We now show how to control the generator growth in general using an idea suggested
by Gu [Gu95a].
It follows from (2.8.1) that
Let WiKiWf denote the eigendecomposition of GjJGj, where A$ is a 2 x 2 real diagonal
matrix with (Ai)n > 0 and (Aj)22 < 0. Then Wi-\/|Ai| can be taken as a generator for
Si with the desirable property that
where ||.Ri||2 ~ ||-R||2> to first order in e.
Therefore, whenever the generator grows, i.e., ||Gi|J2 becomes larger than a given
threshold (say, 2||.R||2 (1 + ll-^Hi))? we can replace it by Wi^Ail. This computation
can be done in O((n â z)r2 + r3) flops (r = 2 in the case under consideration) by first
computing the QR factorization of Gi, say,
and then computing the eigendecomposition of the 2 x 2 matrix PiJP?. We can then
get Wi by multiplying Qi by the orthogonal eigenvector matrix of Pi JP?.
2.12 
SOLUTION OF LINEAR SYSTEMS OF EQUATIONS
The analysis in the earlier sections suggests that for ||F||2 sufficiently close to one, the
error norm can become large. However, if our original motivation is the solution of a
linear system of equations, say, Rx = b, then the error can be improved by resorting to
iterative refinement if the matrix R either is given or can be computed accurately from
(F, G). In what follows we show that for a diagonal F, the matrix R can be evaluated
to high relative accuracy if u\ and v\ are exact.
2.12.1 Computing the Matrix R
Given a positive-definite structured matrix R that satisfies

76 
Stabilized Schur Algorithms 
Chapter 2
with F diagonal and stable, its entries can be computed to high relative accuracy as we
explain below.
It follows from the displacement equation that
where, by positive definiteness, the ratios
are strictly less than one.
The term Â£ = 1 â iifl.^2. can be evaluated to high relative accuracy as explained
^ 
UiO UjO
earlier in Sec. 2.6.4, viz.,
Likewise, we evaluate // = (1 â fifj) and then r^ = UlÂ°Uj . This guarantees r^- =
ry(l + 0n(e)).
2.12.2 
Iterative Refinement
If the factorization LLT is not too inaccurate, and if R is not too ill-conditioned, then it
follows from the analysis in [JW77] that the solution x of Rx = b can be made backward
stable by iterative refinement.
Algorithm 2.12.1 (Iterative Refinement). Consider a symmetric positive-definite
matrix R â¬ Rnxn and let LLT be a computed Cholesky factorization for it. Let x be a
computed solution for Rx = b. The solution x can be made backward stable as follows:
0

Section 2.13. 
Enhancing the Robustness of the Algorithm 
77
2.13 
ENHANCING THE ROBUSTNESS OF THE ALGORITHM
The performance and robustness of the generalized Schur algorithm can be improved
by further enhancements.
Observe that the positive defmiteness of R imposes conditions on the columns of the
generator matrix:
â¢ For a diagonal and stable F, a necessary condition for the positive definiteness of
the matrix is that we must have
where the inequality holds componentwise.
â¢ For a lower triangular contractive F, a necessary condition for positive definiteness
is
â¢ In all cases, the condition |4>iWj|i+i > |t>i|i+i is required to ensure that the reflec-
tion coefficient of the hyperbolic rotation, 0^, is less than 1.
We have found that if all these necessary conditions are enforced explicitly, the algorithm
is more reliable numerically. An example of this can be found in Sec. 2.13.3.
We now show how the OD and H methods can be modified to preserve the sign of
the J-norm of each row of the prearray.
2.13.1 
Enhancing the OD Method
The OD method can be enhanced to preserve the sign of the J-norm of the row to which
it is applied. More specifically, if the jih row of the perturbed prearray has positive
J-norm, then by adding a small perturbation to the jth row of the computed postarray
we can guarantee a positive J-norm. If the jth row of the perturbed prearray does not
have a positive J-norm, then in general there does not exist a small perturbation for
the j>th row of the postarray that will guarantee a positive J-norm. For such a row,
the prearray must be perturbed to make its J-norm sufficiently positive and then the
hyperbolic rotation must be reapplied by the OD method to that row. The new jth row
of the postarray can now be made to have a positive J-norm by a small perturbation.
The details are given in the algorithm below. For the case of a diagonal and stable F,
all the rows of the prearray should have positive J-norm. The algorithm should enforce
this property.
In the statement of the algorithm, [ x 
y ] stands for a particular row of the pre-
array Gi+i, [ xi 
y i ] stands for the corresponding row of the postarray G;+i, and 0
stands for the hyperbolic rotation. Here we are explicitly assuming that \x\ > \y\,
which is automatically the case when F is diagonal and stable. 
Otherwise, since
[ y x ] G = [ yi xi ], the technique must be used with the elements of the input
row interchanged.

78 
Stabilized Schur Algorithms 
Chapter 2
Algorithm 2.13.1 (Enhanced OD Method).
Assumption: \x\ > \y\.
if\xi\ < \yi\
71 = On(e)(|xi| + \yi\)sign(xi)
72 = On(e)(\xi\ + \yi\)sign(yi)
i/|zi+7i| > |j/i -72! then
xi=xi + 71
y i = yi - 72
else
rji = On(e)(\x\ + \y\)sign(x)
r)2 = On(e)(\x\ + \y\)sign(y)
[ xi yi ] = [ x 4- rji 
y â rj2 ] Q (via the OD method)
if \xi\ > \yi\ then xi = xi and y\ = y\
else
71 = On(e)(\xi\ + \yi\)sign(xi)
72 = On(e)(jxij + \yi\)sign(yi)
xi=xi+ 71
y i = Ui - 72
endif
endif
endif
2.13.2 
Enhancing the H Procedure
Here again, [ x 
y ] stands for a particular row of the prearray Gj+i, and [ x\ y\ ]
stands for the corresponding row of the postarray. We shall again assume that |x| >
|y|. If that is not the case then the procedure must be applied to [ y x ], since
[ y x]Q=[yi 
xl ].
It follows from |a;| > |y| that
for the H procedure. Therefore, by adding small numbers to x\ and y\ we can guarantee
|Â£i|>|yi|.
Algorithm 2.13.2 (Enhanced H Method).
2.13.3 
A Numerical Example
The following example exhibits a positive-definite matrix R for which a direct implemen-
tation of the generalized Schur algorithm, without the enhancements and modifications

Section 2.14. 
Results for the Positive-Definite Case 
79
proposed herein, breaks down. On the other hand, the modified Schur algorithm en-
forces positive definiteness and avoids breakdown as the example shows. The data are
as follows:
A straightforward implementation of the generalized Schur algorithm (i.e., with a
naive implementation of the hyperbolic rotation and the Blaschke matrix-vector multi-
ply) breaks down at the eighth step and declares the matrix indefinite.
On the other hand, our implementation, using the enhanced H procedure and the en-
hanced Blaschke matrix-vector multiply, successfully completes the matrix factorization
and yields a relative error
Furthermore, the relative backward error \\R â LLT||2/||-R||2 is approximately 10~n
(using a machine precision of approximately 10~16).
2.14 
RESULTS FOR THE POSITIVE-DEFINITE CASE
The general conclusion is the following:
The modified Schur algorithm of this chapter is backward stable for a large class of
positive-definite structured matrices. Generally, it is as stable as can be expected from
the perturbation analysis of Sec. 2.5.
More specifically,
â¢ If F is strictly lower triangular and contractive (e.g., F = Z], then the modified
algorithm is backward stable with no generator growth.
â¢ If F is stable, diagonal, and positive, then by reordering the entries of F in in-
creasing order, there will be no generator growth and the algorithm will be as
stable as can be expected from Sec. 2.5. In particular, it will be backward stable
if ||F||2 is not too close to one (e.g., ||F||2 < 1 â ^r).

80 
Stabilized Schur Algorithms 
Chapter 2
â¢ In all other cases, we can use the technique outlined in Sec. 2.11 to control the
generator growth and make the algorithm as stable as can be expected from
Sec. 2.5. In particular, it is backward stable if ||-F||2 is not too close to one
(e.g., ||F||i <!-Â£).
â¢ If R is given or can be computed accurately (e.g., when F is diagonal), iterative
refinement can be used to make the algorithm backward stable for the solution of
linear equations.
As far as pivoting is concerned, in the diagonal F case, our analysis shows that it is
necessary only when ||F||2 is very close to one.
â¢ If F is positive (or negative), a good strategy is to reorder the entries of F in
increasing order of magnitude.
â¢ If F has both positive and negative entries, then our numerical example of
Sec. 2.10.1 indicates that pivoting may not help in controlling the growth of the
generators.
In our opinion, for positive-definite structured matrices, with diagonal or strictly
lower triangular F, the stabilization of the generalized Schur algorithm is critically
dependent on the following:
â¢ Proper implementations of the hyperbolic rotations (using the OD or H proce-
dures).
â¢ Proper evaluation of the Blaschke matrix-vector product.
â¢ Enforcing positive definiteness to avoid early breakdowns.
â¢ Controlling the generator growth.
Acknowledgments
The authors wish to thank Professor Thomas Kailath for comments and feedback on an
earlier draft of this chapter. They also gratefully acknowledge the support of the National
Science Foundation; the work of A. H. Sayed was partially supported by awards MIP-9796147
and CCR-9732376, and the work of S. Chandrasekaran was partially supported by award CCR-
9734290.

Section 2.A. 
Pseudocode for the Stable Schur Algorithm 
81
APPENDIX FOR CHAPTER 2
2.A 
PSEUDOCODE FOR THE STABLE SCHUR ALGORITHM
We include here a listing of the stabilized Schur algorithm. The program assumes that the
input matrix is positive definite and tries to enforce it. The algorithm listed here can be easily
modified to test if a structured matrix is positive definite.6
The H Procedure
Input data: The ratio beta/alpha represents the reflection coefficient, which is smaller than
one in magnitude. Also, y/x is assumed to be smaller than one in magnitude.
Output data: The entries [ x\ 
y\ ] that result by applying a hyperbolic rotation to [ x 
y ],
with |xi| > \y\\.
function [ xl, 
yl ] = hjprocedure(x, y, beta, alpha)
c = (beta * y)/(alpha * x);
if c < 0.5
xi = 1 â c;
else
dl = (abs(alpha) â abs(beta))/abs(alpha);
dl â (abs(x) - abs(7/))/abs(x);
xi = dl + d2 - dl * d2;
end
xl = (abs(alpha) * x * xi)/sqrt((alpha â beta) * (alpha + beta));
yl â xl â sqrt((alpha + beta)/(alpha â beta)) * (x â y)]
if abs(xl) < abs(t/l)
yl = abs(xl) * (1 â 3 * eps) * sign(yl)
end
The Blaschke Matrix-Vector Product
We now list the program that computes <&ii*i for both a diagonal F and a strictly lower trian-
gular F.
Remark. In contrast to the description of the generalized Schur algorithm in Alg. 2.4.1, the codes
given here work with quantities {$i,Ui,Vi} that are always ra-dimensional. The indices also start at 1
rather than 0. Hence, {uj, Vi} are now taken as n X 1 column vectors whose leading (i â 1) entries are
zero. Also, 3>i is taken as an n x n matrix. This convention is used for convenience of description.
â¢ Input: An n x n stable and diagonal matrix F, a vector it, a vector v (such that \v\ < \u\),
and an index i (1 < i < n).
â¢ Output: The matrix vector product z = $iU, where $i = (/ - fiF)~l(F - /i/), and the
vector ub = (I â fiF)~lu.
6 The codes listed here are in a MATLAB-like language.

82 
^ 
Stabilized Schur Algorithms 
Chapter 2
function [ z, ub ] = blaschke-l(F, u,v,i,ri)
ub = u;
z = u;
for ? â i i TI
ifF(z,;)*H7,j)<0.5
xi = l/(l-F(j,j)*F(i,i));
else
dl = l-abs(F(i,*));
d2 = l-abs(F(j,j));
zi=l/(dl + d2-dl*d2);
end
w&(?) = a^ * Z0')>
Â«(j) = (^a,j)-^.0)*w60');
if abs(z(j)) < abs(v(j))
z(j] = abs(v(j)) * (1 + 3 * eps) * sign(z(j));
end
end
For a strictly lower triangular F we use the following.
Input data: An n x n strictly lower triangular matrix F, a vector u, a vector v (such that
|i>| < |u|), and an index i (1 < i < n).
Output data: The matrix-vector product z â Fu and ub = u.
function [ z, ub ] = blaschkeJ2(F, u, v, i, n)
ub = u;
z = F * u;
z(i] = 0;
if abs(z(i + 1)) < abs(v(i + 1))
z(i + 1) = abs(i;(i + 1)) * (1 + 3 * eps) * sign(z(i + 1));
end
The Stable Schur Algorithm
We now list two versions of the stable modified Schur algorithmâone for diagonal stable F
and the other for strictly lower triangular F.
Input data: An n x ra diagonal and stable matrix F, a generator G = [ u v ] in proper form
(i.e., top entry of v is zero), with column vectors u, v.
Output data: A lower triangular Cholesky factor L such that \\R - LLT\\2 satisfies (2.9.1).
function L = stable,schurJ.(u,v,F)
n = size(F, 1);
for i = 1 : n â 1

Section 2.A. 
Pseudocode for the Stable Schur Algorithm 
83
L(:,i) = sqrt((l - F(i, i)) * (1 + F(i, i)}) * ub;
a = v(i + 1);
& = u(i+l);
for j = i + 1 : n
[ u(j], v(j} ] = hjprocedure(u(j),v(j),a,6);
end
v(i+ 1) = 0;
end
L(n,n) = (l/sqrt((l-F(n,n))*(l + F(n,n))))*w(n);
L(l : n â 1,n) = zeros(n â 1,1);
â¢ Input: An n x n strictly lower triangular matrix F, a generator G â [ u v ] in proper
form (i.e., top entry of v is zero), with column vectors u, v.
â¢ Output: A lower triangular Cholesky factor L such that \\R â LLT\\2 satisfies (2.9.1).
function L = stable.schurJ2(u, v, F)
n = size(F, 1);
for i = 1 : n â 1
[ u, ub J = blaschkeJ2(F,u,v,i,ri);
L(:, i) = sqrt((l - F(i, i)) * (1 + F(i, i))) * ub;
a = v(i+ 1);
6 = w(z + l);
for j = i + 1 : n
if abs(u(j)) > abs(?;(j))
[ u(j), v(j) ] = hjprocedure(u(j),v(j),a,6);
else
[ tempjv, temp.u 1 = h-procedure(v(j),u(j),a,6);
V0') = iemp_i;; u(j) = tempju;
endif
end
u(i + 1) = 0;
end
L(n, n) = (l/sqrt((l - F(n, n)) * (1 + F(n, n)))) * u(n);
L(l : n â 1, n) = zeros(n â 1,1);
[ it, ub ] = blaschke-I(F,u,v,i,n);

This page intentionally left blank 

Chapter 3
FAST STABLE SOLVERS FOR
STRUCTURED LINEAR
SYSTEMS
AM H. Sayed
Shivkumar Chandrasekaran
3.1 INTRODUCTION
The derivation in Ch. 2 showed how to obtain a stable implementation of the generalized
Schur algorithm for positive-definite structured matrices R and, also, how to solve Rx =
b when R is structured but still positive definite.
We now use this stable implementation of the Schur algorithm, and the same notation
of Ch. 2, to solve in a stable manner linear systems of equations with possibly indefinite
or even nonsymmetric structured coefficient matrices. In other words, we show how
to use the stability results of the positive-definite case to derive stable solvers for the
indefinite and nonsymmetric cases.
We shall focus on shift-structured coefficient matrices R, viz., those that satisfy
displacement equations of the form
for some (G, B). In fact, we shall denote the coefficient matrix throughout this chapter
by T rather than R and write
The notation T is chosen for two reasons. First, this class of matrices includes as a
special case Toeplitz matrices (which correspond to a special choice for G and Bâsee
(2.3.5)). However, our results apply not only to Toeplitz coefficient matrices but, more
generally, to shift-structured matrices T (obtained for other choices of G and B and also
for multicolumn G and B}. Second, the notation T is also chosen to avoid confusion
with the notation R for the R factor in the QR factorization of a matrix. Such QR
factorizations will be repeatedly invoked in this chapter.
The coefficient matrix T is not required to be symmetric or positive definite. It is
required only to be a structured matrix in the sense defined by (3.1.1). It can thus
85

86 
Fast Stable Solvers for Structured Linear Systems 
Chapter 3
be indefinite or even nonsymmetric. Now given a linear system of equations Tx = b,
one method for solving it fast is to compute the QR factorization of the coefficient
matrix T rapidly. Several fast methods have been proposed for this purpose [BBH86],
[CKL87], [Cyb83], [Cyb87], [Swe84] (see also Sec. 1.8.3) but none of them is numerically
stable since the resulting Q matrix cannot be guaranteed to be orthogonal. In [CS98],
however, the authors of this chapter showed how to circumvent this issue and derived
an algorithm that is provably both fast and backward stable for solving Tx = 6 for
shift-structured matrices T that can be indefinite or even nonsymmetric.
The new algorithm relies on the observation that it is not really necessary to limit
ourselves to only LDU or QR factorizations of the coefficient matrix T in order to
solve the linear system of equations Tx = 6. If other factorizations for T can be
obtained in a fast and stable manner, and if they are also useful for solving the linear
system of equations, then these factorizations should be pursued as well. In fact, the
new algorithm, rather than returning Q, returns two matrices A and Q such that A is
triangular and the product A-1Q is "numerically orthogonal"; it provides a factorization
for the coefficient matrix T that is of the form
This factorization is in terms of the three matrices {A, Q, R} and it is, of course, highly
nonunique, since we can always replace A by any invertible matrix. The point, however,
is that the new algorithm returns that particular A that allows us to compensate for
the fact that Q is not numerically orthogonal. More important, these factors are then
used to solve Tx â b both fast and in a backward stable manner.
Our derivation is based on the following idea. We already know from the discussions
in Ch. 2 how to develop a numerically reliable implementation of the generalized Schur
algorithm for positive-definite structured matrices. This suggests that we should first
transform a problem that involves a nonsymmetric or indefinite structured matrix T to
an equivalent problem that involves sign-definite matrices only (either positive definite
or negative definite). We achieve this by relying on a so-called embedding technique
developed in [Chu89], [KC94] (see also Ch. 1). More specifically, we embed the square
coefficient matrix T into a larger matrix of the form
The product TTT is never formed explicitly. This definition for M is just for explanation
purposes since the algorithm itself will end up working with a generator matrix for M
and not with the entries of M, and this generator for M can be found from a generator
for T, without the need to form TTT (see Sec. 3.4).
Now observe that the leading block of M is positive definite and the Schur comple-
ment of M with respect to the (1,1) block is negative definite (and equal to â/). The
matrix M also turns out to be structured (e.g., M is shift structured when T is quasi
Toeplitz). In this case, and with proper modifications, the stability results of Ch. 2 can
be applied.
3.2 OVERVIEW OF THE PROPOSED SOLUTION
Once the matrices {A, Q, R} are obtained, they are used to solve for x efficiently by
using the expression

Section 3.3. 
The Generalized Schur Algorithm for Indefinite Matrices 
87
Note that in writing this expression we used the fact that A-1Q is orthogonal and,
hence, its inverse is the transpose matrix.
All computations in the above expression can be done in O(n2} operations, where
n is the matrix dimension, and the algorithm turns out to be backward stable (cf. the
definitions of stability in Ch. 4) in the sense that the computed solution x is shown to
satisfy an equation of the form
where the norm of the error matrix T satisfies
where e denotes machine precision.
The factorization for T is obtained as follows. We apply In steps of the generalized
Schur algorithm to a generator of M in (3.1.2) and obtain its computed triangular
factorization, which we partition in the form
where RT and A are nxn lower triangular matrices. The computed matrices {R, Q, A}
are the quantities used in (3.2.1) to determine the computed solution x in a backward
stable manner.
Prom a numerical point of view, the above steps differ in a crucial way from the
embeddings suggested in [Chu89], [KC94] and turn out to mark the difference between
a numerically stable and a numerically unstable implementation. The discussion in
[Chu89, pp. 37, 50, 52] and [KC94] is mainly concerned with fast procedures for the QR
factorization of Toeplitz block and block Toeplitz matrices. It employs an embedding
of the form
where the identity matrix / in (3.2.2) replaces the zero matrix in our embedding (3.1.2).
The derivation in [Chu89], [KC94] suggests applying n (rather than 2n) steps of the
generalized Schur algorithm to a generator of (3.2.2) and then using the resulting R
and Q as the QR factors of T. However, numerical issues were not studied in [Chu89],
[KC94], and it turns out that the above procedure does not guarantee a numerically
orthogonal matrix Q and therefore cannot be used to implement a stable solver for a
linear system of equations Tx = b.
For this reason, we instead proposed in [CS98] to proceed with the previous embed-
ding (3.1.2) since it seems difficult to obtain a stable algorithm that is based solely on
the alternative embedding (3.2.2). We also apply 2n steps (rather than just n steps) of
the generalized Schur algorithm to a generator of (3.1.2). This allows us to incorporate
a correction procedure into the algorithm (by computing A), which is shown later to
ensure backward stability when coupled with the other modifications that we discussed
in Ch. 2 for stabilizing the generalized Schur algorithm.
3.3 
THE GENERALIZED SCHUR ALGORITHM FOR
INDEFINITE MATRICES
We described in Sec. 2.4 the array form of the Schur algorithm for symmetric positive
definite structured matrices with displacement rank 2. Now, in view of the structure of

88 
Fast Stable Solvers for Structured Linear Systems 
Chapter 3
M, we shall need to apply the algorithm to a symmetric but possibly indefinite matrix
with displacement rank larger than 2 with respect to a strictly lower triangular F, say,
For this reason, and for ease of reference, we include here a statement of the algorithm
for this particular case (this form is a special case of Alg. 1.7.1 of Ch. 1).
For displacement ranks larger than 2, we shall say that a generator matrix G is in
proper form if its first nonzero row has a single nonzero entry, say, in the first column
or in the last column
We note that in the statement below we are denoting the triangular factorization of a
symmetric matrix M by M = LDLT', where D is taken as a signature matrix (rather
than M = LD~1LT as in (1.6.1), where D is a diagonal matrix).
Algorithm 3.3.1 (Schur Algorithm for Indefinite Matrices). Consider a sym-
metric strongly regular matrix M â¬ Rnxn satisfying (3.3.1).
â¢ Input: An n x n strictly lower triangular matrix F, an n x r generator GQ = G,
and J = (IpÂ® -/9).
â¢ Output: A lower triangular factor L and a signature matrix D such that M =
LDLT, where M is the solution of (3.3.1) (assumed n x n).
â¢ Computation: Start with GO = G, FQ = F, and repeat for i = 0,1,..., n â 1:
1. Let gi denote the top row of Gi.
2. If QiJgf > 0 (we refer to this case as a positive step):
o Choose a J-unitary rotation Oj that converts gi to proper form with
respect to the first column, i.e.,
Let Gi = Gi&i (i.e., apply 9* to Gi).
o The ith column ofL, denoted by li, is obtained by appending i zero entries
to the first column of Gi,
The ith signature is di = 1.

Section 3.3. The Generalized Schur Algorithm for Indefinite Matrices 
89
o Keep the last columns of Gi unchanged and multiply the first column by
Fi, where Fi denotes the submatrix obtained by deleting the first i rows
and columns of F. This provides a new matrix whose first row is zero
(since Fi is strictly lower triangular) and whose last rows are the rows
of the next generator matrix Gi+\, i.e.,
3. If giJgl[ < 0 (we refer to this case as a negative step):
o Choose a J-unitary rotation 6^ that converts gi to proper form with
respect to the last column, i.e.,
Let Gi = Gi&i (i.e., apply G^ to Gi).
o The ith column ofL, denoted by li, is obtained by appending i zero entries
to the last column of Gi,
The ith signature is di = â1.
o Keep the first columns of Gi unchanged and multiply the last column
by Fi. This provides a new matrix whose first row is zero (since Fi is
strictly lower triangular) and whose last rows are the rows of the next
generator matrix Gi+i, i.e.,
4. The case giJgJ = 0 is ruled out by the strong regularity of M.
Again, after n steps, the algorithm provides the triangular decomposition
at O(rn2) computational cost (see Ch. 1). Moreover, the successive matrices Gi that
are obtained via the algorithm can be interpreted as follows (recall Remark 2 after
Alg. 1.6.2). Let Mi denote the Schur complement of M with respect to its leading i x i
submatrix; then
Hence, Gi constitutes a generator matrix for the ith Schur complement Mi, which is
therefore structured. Note further that Gi is also a generator matrix for the same Schur
complement Mj since, due to the J-unitarity of G^, we have GiJG? = GiGjJGf Gf =
Gi JG?. We now address the main issues of this chapter.

90 
Fast Stable Solvers for Structured Linear Systems 
Chapter 3
3.4 
FAST QR FACTORIZATION OF SHIFT-STRUCTURED
MATRICES
As mentioned in (3.1.1), we shall focus on matrices T e Rnxn that are shift structured
and therefore satisfy a displacement equation of the form
for some generator pair (G,B}. Consider, for now, the following alternative definition
of a 3n x 3n augmented matrix:
The matrix M is also structured (as shown below) with respect to Zn @Zn Â®Zn, where
Zn denotes the n x n lower shift triangular matrix (denoted earlier by Zâhere we
include the subscript n in order to explicitly indicate the size of Z}.
It can be easily verified that M - (Zn Â® Zn Â© Zn}M(Zn Â© Zn 0 Zn}T is low rank
since
where e\ â [ 1 0 ... 0 ] is a basis vector of appropriate dimension. A generator
matrix for M, with 3n rows and (2r + 1) columns, can be seen to be
That is,
where T = (Zn 0 Zn 0 Zn) and (Q, J) are as above.
The nxn leading submatrix of M is negative definite (in fact, equal to â /). There-
fore, the first n steps of the generalized Schur algorithm applied to (F, (j, J] will be
negative steps (cf. step 3 of Alg. 3.3.1). These first n steps lead to a generator ma-
trix, denoted by Qn (with 2n rows), for the Schur complement of M with respect to its
leading n x n leading submatrix, viz.,
where Mn is 2n x 2n and equal to (what we denoted earlier in (3.2.2) by M)
Clearly, M and its Schur complement Mn are related via the Schur complement relation:

Section 3.4. 
Fast QR Factorization of Shift-Structured Matrices 
91
Therefore, (Qn, J] is a generator for Mn with respect to (ZnÂ®Zn), as shown by (3.4.4).
The leading n x n submatrix of Mn is now positive definite (equal to TTT). There-
fore, the next n steps of the generalized Schur algorithm applied to (Zn Â© ZmGmJ}
will be positive steps (cf. step 2 of Alg. 3.3.1). These steps lead to a generator matrix,
denoted by Q^n (with n rows), for the Schur complement of M with respect to its leading
In x In leading submatrix, viz.,
where M^n is now n x n and equal to â/.
Again, Mn and Min are related via a (block) Schur complementation step, written
as
where we have denoted the first n columns of the triangular factor of Mn by
with R an n x n upper triangular matrix and Q an n x n matrix. The R and Q matrices
are thus obtained by splitting the first n columns of the triangular factor of Mn into a
leading lower triangular block followed by a full matrix Q.
By equating terms on both sides of (3.4.6) we can explicitly identify R and Q as
follows:
These relations show that Q and R define the QR factors of the matrix T.
In summary, the above discussion shows the following: given a shift-structured ma-
trix T as in (3.4.1), its QR factorization can be computed efficiently by applying In steps
of the generalized Schur algorithm to the matrices (T, G, J] defined in (3.4.3). The fac-
tors Q and R can be obtained from the triangular factors {li} for i = n, n +1,..., In â 1.
Alternatively, if a generator matrix is directly available for Mn in (3.4.5) (see the
discussion of the Toeplitz case below), then we need only apply n Schur steps to this
generator matrix and read the factors Q and R from the resulting n columns of the
triangular factor.
In what follows we shall establish, for convenience of exposition, the numerical sta-
bility of a fast solver for Tx = b that starts with a generator matrix for the embedding
(3.4.5) rather than the embedding (3.4.2). It will become clear, however, that the same
conclusions will hold if we instead start with a generator matrix for the embedding
(3.4.2).
The augmentation (3.4.2) was used in [Say92], [SK95b], and it is based on embedding
ideas originally pursued in [Chu89], [KC94] (see other augmentations below).
3.4.1 
The Toeplitz Case
In some cases it is possible to find an explicit generator matrix for Mn. This saves the
first n steps of the generalized Schur algorithm.
For example, consider the case when T is a Toeplitz matrix (which is a special case of
(3.4.1) whose first column is [to, ii,..., Â£n-i]T and whose first row is [to, t-i, â¢ â¢ â¢ , Â£_n+i]-
Define the vectors

92 
Fast Stable Solvers for Structured Linear Systems 
Chapter 3
It can be verified that a generator matrix for Mn in (3.4.5) is the following [Chu89]:
where J is 5 x 5,
and Qn is In x 5,
3.4.2 
Other Augmentations
It is possible to compute the QR factors of a structured matrix T satisfying (3.4.1)
by using augmented matrices other than (3.4.2). For example, consider the 3n x 3n
augmented matrix
where an identity matrix replaces the zero matrix in the (3,3) block entry of the matrix
in (3.4.2). A generator matrix for M, with 3n rows and (2r + 2) columns, is now
If T is Toeplitz, as discussed above, then the rank of Q can be shown to reduce to 2r = 4
[Chu89]. (This is in contrast to the displacement rank 5 that follows from the earlier
embedding (3.4.2).)
After 2n steps of the generalized Schur algorithm applied to the above (G,J), we
obtain the following factorization (since now Min = 0),
from which we can again read the QR factors of T from the triangular factors {^} for
i = n,..., 2n â 1. This augmentation was suggested in [Chu89, p. 37] and [KC94].
However, from a numerical point of view, computing the QR factors of a structured
matrix T using the generalized Schur algorithm on the augmented matrices M in (3.4.2)
or (3.4.7) is not stable. The problem is that the computed Q matrix is not necessarily
orthogonal. This is also true for other procedures for fast QR factorization [BBH86],
[Cyb83], [Cyb87], [Swe84].
In the next section we show how to overcome this difficulty and develop a fast and
stable algorithm for solving linear systems of equations with shift-structured coefficient

Section 3.5. Well-Conditioned Coefficient Matrices 
93
matrices T. For this purpose, we proceed with the embedding suggested earlier in (3.4.2)
since it seems difficult to obtain a stable algorithm that is based solely on the alternative
embedding (3.4.7). The reason is that the embedding (3.4.2) allows us to incorporate a
correction procedure into the algorithm in order to ensure stability.
We first derive a stable algorithm for a well-conditioned coefficient matrix and then
modify it for the case when the coefficient matrix is ill-conditioned. The interested
reader may consult the summary of the final algorithm in Sec. 3.7.
3.5 
WELL-CONDITIONED COEFFICIENT MATRICES
In this section we develop a stable algorithm for the case of well-conditioned matrices T.
A definition of what we mean by a well-conditioned matrix is given later (see (3.5.9)).
Essentially this refers to matrices whose condition number is less than the reciprocal
of the square root of the machine precision. Modifications to handle the ill-conditioned
case are introduced later.
We start with an n x n (possibly nonsymmetric) shift-structured matrix T with
displacement rank r,
and assume we have available a generator matrix Q for the 2n x 2n augmented matrix
that is,
where T = (Zn@Zn}. Note that, for ease of exposition, we have modified our notation.
While we earlier denoted the above matrix M by Mn and its generator by Qn and used
F to denote (Zn Â© Zn Â© Zn), we are now dropping the subscript n from (Mn, Qn] and
are using T to denote the 2n x 2n matrix (Zn Â© Zn}.
We discussed in Sec. 3.4.1 an example where we showed a particular generator matrix
Q for the above M when T is Toeplitz. (We repeat that the error analysis of later sections
will still apply if we instead start with the 3n x 3n embedding (3.4.2) and its generator
matrix (3.4.3).)
We indicated earlier (at the end of Sec. 3.4) that by applying n steps of the gener-
alized Schur algorithm to the matrix M in (3.5.2) we can obtain the QR factorization
of T from the resulting n columns of the triangular factors of M. But this procedure
is not numerically stable since the resulting Q is not guaranteed to be unitary. To fix
this problem, we propose some modifications. The most relevant modification we in-
troduce now is to run the Schur algorithm for 2n steps on M rather than just n steps.
As suggested in Ch. 2, we also need to be careful in the application of the hyperbolic
rotations. In particular, we assume that the hyperbolic rotations are applied using one
of the methods suggested in that chapter such as mixed downdating, the OD method,
or the H procedure.
The matrix T is required only to be invertible. In this case, the leading submatrix of
M in (3.5.2) is positive definite, and therefore the first n steps of the generalized Schur
algorithm will be positive steps. Hence, the hyperbolic rotations needed for the first n
steps will perform transformations of the form (3.3.4), where generators are transformed
into proper form with respect to their first column. Likewise, the Schur complement
of M with respect to its leading submatrix TTT is equal to âI, which is negative

94 
Fast Stable Solvers for Structured Linear Systems 
Chapter 3
definite. This means that the last n steps of the generalized Schur algorithm will be
negative steps. Hence, the hyperbolic rotations needed for the last n steps will perform
transformations of the form (3.3.7), where generators are transformed into proper form
with respect to their last column.
During a positive step (a similar discussion holds for a negative step), a generator
matrix Gi will be reduced to proper form by implementing the hyperbolic transformation
Â©i as a sequence of orthogonal transformations followed by a 2 x 2 hyperbolic rotation
(see also [SD97b]). The 2 x 2 rotation is implemented along the lines of [CS96] or Ch. 2,
e.g., via mixed downdating [BBDH87], or the OD method, or the H procedure. Details
are given below.
3.5.1 
Implementation of the Hyperbolic Rotation
When the generalized Schur algorithm is applied to (Â£, F) in (3.5.3), we proceed through
a sequence of generator matrices (Q, Â£1, Â£?2> â¢ â¢ â¢) of decreasing number of rows (2n, In â
1,2nâ2,...). Let gÂ± denote the top row of the generator matrix Qi at step i. In a positive
step, it needs to be reduced to the form (3.3.4) via an (Ip Â© â/9)-unitary rotation 0*.
We propose to perform this transformation as follows:
1. Apply a unitary (orthogonal) rotation (e.g., Householderâsee App. B) to the first
p columns of Qi so as to reduce the top row of these p columns into proper form,
with a nonzero entry in the first column. Let
denote the modified generator matrix. Its last q columns coincide with those of
Gi.
2. Apply another unitary (orthogonal) rotation (e.g., Householder) to the last q
columns of Q^\ so as to reduce the top row of these last q columns into proper
form with respect to their last column,
with a nonzero entry in the last column. Let
denote the modified generator matrix. Its first p columns coincide with those of
Â£M.
3. Employ an elementary rotation 0^ acting on the first and last columns (in mixed-
downdating form or according to the OD or the H methods of Ch. 2) in order to
annihilate the nonzero entry in the last column,

Section 3.5. Well-Conditioned Coefficient Matrices 
95
4. The combined effect of the above steps is to reduce Qi to the proper form (3.3.4)
and, hence,
Expression (3.5.6) shows that, in infinite precision, the generator matrices Qi and Qi
must satisfy the fundamental requirement
Obviously, this condition cannot be guaranteed in finite precision. But, as discussed
in Ch. 2 and in [CS96], with the above implementation of the transformation (3.5.6) (as
a sequence of two orthogonal transformations and a hyperbolic rotation in mixed, OD,
or H forms), equality (3.5.7) can be guaranteed to within a "small" error, viz.,
A similar analysis holds for a negative step, where the rotation Gi is again imple-
mented as a sequence of two unitary rotations and one elementary hyperbolic rotation
in order to guarantee the transformation (3.3.7). We forgo the details here.
We finally remark that in the algorithm, the incoming generator matrix Qi will in
fact be the computed version, which we denote by Qi. This explains why in the error
analysis of the next section we replace Qi by Qi in the error bound (3.5.8).
Note also that we are implicitly assuming that the required hyperbolic rotation O^s
exists. While that can be guaranteed in infinite precision, it is possible that in finite
precision we can experience breakdowns.
3.5.2 
Avoiding Breakdown
To avoid breakdown we need to guarantee that during the first n steps of the algorithm,
the jT-unitary rotations 0i are well defined. This requires that the leading submatrices
of the first n successive Schur complements remain positive definite, a condition that
can be guaranteed by requiring T to be sufficiently well-conditioned [CS98], viz., by
requiring that
We refer to a matrix T that satisfies the above requirement as being well conditioned.
(The scalar multiple 2 is made explicit for convenience in later discussion.)
Now, after the first n steps of the generalized Schur algorithm applied to (F, Q] in
(3.5.3), we let
denote the computed factors that correspond to expression (3.4.6). We further define
the matrix 5n that solves the displacement equation
Note that Sn is an n x n matrix, which in infinite precision would have been equal to
the Schur complementâ/ (cf. (3.4.6)). We can now define

96 
Fast Stable Solvers for Structured Linear Systems 
Chapter 3
We showed in [CS98] that the following error bound holds.
Theorem 3.5.1 (Error Bound). The first n steps of the generalized Schur algorithm
applied to (J~,G} in (3.5.3), for a matrix T satisfying (3.5.9), and with the rotations
Oi implemented^ as discussed in Sec. 3.5.1, guarantees the following error bound on the
matrix (M - M) (with M defined in (3.5.11),):
The natural question then is, How big can the norm of the generator matrices be?
The following remark is motivated by an observation in [SD97b] that for matrices of
the form TTT, with T Toeplitz, there is no appreciable generator growth. Indeed, we
showed in [CS98] that a first-order bound for the sum of the norms of the generators in
(3.5.12) is given by
3.5.3 
Error Analysis of the Last Steps
We now study the last n steps. Assume T satisfies the following normalization:
which can always be guaranteed by proper scaling at the begining of the algorithm. We
showed in [CS98] that under the well-conditioned assumption (3.5.9), the matrix Sn
is guaranteed to be negative definite and well-conditioned. In particular, its condition
number is at most 15.
Hence, we can proceed with the last n steps of the generalized Schur algorithm
applied to Qn, since Qn is a generator matrix for Sn:
All steps will now be negative steps. Hence, the discussion in Sec. 3.5.1 on the imple-
mentation of the hyperbolic rotations applies. The only difference will be that we make
the generator proper with respect to its last column. In other words, the third step of
that implementation should be modified as follows:
Let â AAT be the computed triangular factorization of Sn. It can be shown that
where the norm of the generators {Qi} appearing in the above error expression can be
shown to be bounded [CS98].

Section 3.6. 
Ill-Conditioned Coefficient Matrices 
97
3.5.4Summary
We have shown so far that if we apply 2n steps of the generalized Schur algorithm to the
matrices (F,G} in (3.5.3), with proper implementation of the j7-unitary rotations (as
explained in Sec. 3.5.1), then the error in the computed factorization of M is bounded
as follows:
We have also established (at least in infinite precision) that the norm of the generators
is bounded. Therefore, the computed factorization is (at least asymptotically) backward
stable with respect to M.
3.5.5olving the Linear System of Equations
We now return to the problem of solving the linear system of equations Tx = 6,
where T is a well-conditioned nonsymmetric shift-structured matrix (e.g., Toeplitz, quasi
Toeplitz, product of two Toeplitz matrices).
We showed in [CS98] that A-1Q is numerically orthogonal, viz.,
and that
This shows that we can compute x by solving the nearby linear system
in O(n2) flops by exploiting the fact that A-1Q is numerically orthogonal and A is
triangular as follows:
The fact that this scheme for computing x is backward stable will be established in the
next section (see the remark after expression (3.6.6)).
3.6 
ILL-CONDITIONED COEFFICIENT MATRICES
We now consider modifications to the algorithm when the inequality (3.5.9) is not
satisfied by T. This essentially means that the condition number of T is larger than the
square root of the reciprocal of the machine precision. We will refer to such matrices T
as being ill-conditioned.
There are now several potential numerical problems, all of which have to be elimi-
nated. First, the (1,1) block of M can fail to factorize as it is not sufficiently positive
definite. Second, even if the first n steps of the Schur algorithm are completed success-
fully, the Schur complement Sn of the (2,2) block may no longer be negative definite,
making the algorithm unstable. Third, the matrix A may no longer be well-conditioned,
in which case it is not clear how one can solve the linear system Tx = b in a stable
manner. We now show how these problems can be resolved.

98 
Fast Stable Solvers for Structured Linear Systems 
Chapter 3
To resolve the first two problems we add small multiples of the identity matrix to
the (1,1) and (2, 2) blocks of M, separately:
where a and f3 are positive numbers that will be specified later. (We continue to use M
for the new matrix in (3.6.1) for convenience of notation.) This leads to an increase in
the displacement rank of M. For Toeplitz matrices the rank increases only by one and
the new generators are given as follows:
where J is 6 x 6,
and Q is In x 6,
Had we started instead with the embedding (3.4.2) for more general shift-structured
matrices, we would then modify the generators as explained later in the remark.
For suitably chosen a and (5 (see the statement of the algorithm in Sec. 3.7), it can
be shown that in this case [CS98]
where the norm of the generators is again bounded. Hence, we also obtain a backward-
stable factorization of M.
Since A is no longer provably well-conditioned, we cannot argue that A-1Q is nu-
merically orthogonal. For this reason, we now discuss how to solve the linear system of
equations Tx = b in the ill-conditioned case.
3.6.1 
Solving the Linear System of Equations
Note that if x solves Tx â b, then it also satisfies
Using the above backward-stable factorization for M we can solve the above linear
system of equations to get

Section 3.7. 
Summary of the Algorithm 
99
where the error matrix M satisfies
Note that y is computed by the expression
which is identical to the formula (3.5.18) we obtained earlier by assuming A-1Q is
numerically orthogonal! Therefore, the subsequent error bound holds equally well for
the well-conditioned case. It can be shown that the computed solution y satisfies
where the norm of the error matrix is bounded by
3.6.2 
Conditions on the Coefficient Matrix
For ease of reference, we list here the conditions imposed on the coefficient matrix T in
order to guarantee a fast backward-stable solver of Tx = b:
1. \\T\\2 is suitably normalized to guarantee \\T\\% Â« 1 (cf. (3.5.14)).
2. The condition number of T should essentially be less than the reciprocal of the
machine precision.
Remark. Had we started instead with the embedding (3.4.2), we first perform n steps of
the generalized Schur algorithm to get a generator matrix Qn for the computed version of the
In x In embedding (3.4.5). We then add two columns to Qn as follows:
where the entry >/J3 occurs in the nth row of the last column. The new first column has a
positive signature and the new last column has a negative signature.
3.7 
SUMMARY OF THE ALGORITHM
For convenience we summarize the algorithm here for the case of nonsymmetric Toeplitz
systems. We hasten to add, however, that the algorithm also applies to more general
shift-structured matrices T (other than Toeplitz, such as quasi Toeplitz or with higher
displacement ranks, as demonstrated by the analysis in the earlier sections). The only
difference will be in the initial generator matrix Q and signature matrix J for M in
(3.6.1) and (3.6.2). The algorithm will also be essentially the same, apart from an ad-
ditional n Schur steps, if we instead employ the embedding (3.4.2).

100 
Fast Stable Solvers for Structured Linear Systems 
Chapter 3
â¢ Input: A nonsymmetric Toeplitz matrix T â¬ Rnxn and column vector b 6 Rnxl.
The entries of the first column of T are denoted by [Â£o>Â£i> â¢ â¢ â¢ ?Â£n-i]T> while the
entries of the first row of T are denoted by [to, Â£_i,..., Â£_n+i].
â¢ Output: A backward-stable solution of Tx â b.
â¢ Algorithm:
â¢ Normalize T and b. Since the Probenius norm of T is less than
we can normalize T by setting ti to be Â£j/(57) for all i. Similarly, divide the entries
of 6 by 67. In what follows, T and b will refer to these normalized quantities.
â¢ Define the vectors
â¢ Construct the 6 x 6 signature matrix J â diag[l, 1,1,â!,â!,â1], and the 2n x 6
generator matrix Â£,
where the small positive numbers a and ,3 are chosen as follows (by experimental
tuning):
(If T is well-conditioned (say, Â«(T) < If-^/e), then we can set (3 = 0 = a. and delete
the first columns of Q and J', which then become 2n x 5 and 5x5, respectively.)
â¢ Apply n steps of the generalized Schur algorithm starting with Go = Q and F =
(Zn Â® Zn] and ending with Qn and T â Zn. These are positive steps according to
the description of Alg. 3.3.1 (step 2), where the successive generators are reduced
to proper form relative to their first column. Note that this must be performed
with care for numerical stability as explained in Sec. 3.5.1.
â¢ Apply n more steps of the generalized Schur algorithm starting with Qn. These
are negative steps according to the description of Alg. 3.3.1 (step 3), where the
successive generators are reduced to proper form relative to their last column.
This also has to be performed with care as explained prior to (3.5.15).

Section 3.7. 
Summary of the Algorithm 
101
Table 3.1. Complexity analysis of the fast algorithm.
During each iteration of the algorithm 
Count in flops
Compute two Householder transformations 
3r
Apply the Householder transformations 
4 â¢ i â¢ r
Compute the hyperbolic transformation 
7
Apply the hyperbolic transformation using OD 
6 â¢ i
Shift columns 
i
Total for i = 2n - 1 down to 0 
(14 + 8r)n2 + lOnr + 2In
Cost of 3 back-substitution steps 
3n2
Cost of matrix-vector multiplication 
2n2
Start-up costs 
n(24 log n + r + 52)
Total cost of the algorithm 
(19 + 8r)n2
-I- n(241ogn + llr + 73)
â¢ Each of the above In steps provides a column of the triangular factorization of the
matrix M in (3.6.1), as described in Alg. 3.3.1 (steps 2 and 3). The triangular
factor of M is then partitioned to yield the matrices {R, Q, A},
where R is upper triangular and A is lower triangular.
â¢ The solution x is obtained by evaluating the quantity
via a sequence of back substitutions and matrix-vector multiplications. The com-
puted solution is backward stable. It satisfies (T + T)x = 6, where the norm of
the error matrix is bounded by (3.6.7).
Operation Count
The major computational cost is due to the application of the successive steps of the
generalized Schur algorithm. The overhead operations that are required for the nor-
malization of T and for the determination of the generator matrix Q amount at most
to O(nlogn) flops. Table 3.1 shows the number of flops needed at each step of the
algorithm, (i denotes the iteration number and runs from i = In down to i = 1.) The
operation count given in the table assumes that, for each iteration, two Householder
transformations are used to implement the reduction to the proper form of Sec. 3.5.1,
combined with an elementary hyperbolic rotation in OD form.
The specific costs of the algorithm for the special case of Toeplitz matrices are the
following:
1. For a well-conditioned Toeplitz matrix, the cost is O(59n2 + n(241ogn + 128))
operations.

102 
Fast Stable Solvers for Structured Linear Systems 
Chapter 3
2. For an ill-conditioned Toeplitz matrix, the cost is O(67n2 + n(241ogn -f 139))
operations.
Acknowledgments
The authors wish to thank Professor Thomas Kailath for comments and feedback on an earlier
draft of this chapter. They also gratefully acknowledge the support of the National Science
Foundation; the work of A. H. Sayed was partially supported by awards MIP-9796147 and
CCR-9732376, and the work of S. Chandrasekaran was partially supported by award CCR-
9734290.

Chapter 4
STABILITY OF FAST
ALGORITHMS FOR
STRUCTURED LINEAR
SYSTEMS
Richard P. Brent
4.1 
INTRODUCTION
This chapter surveys the numerical stability of some fast algorithms for solving systems
of linear equations and linear least-squares problems with a low displacement rank
structure. For example, the matrices involved may be Toeplitz or Hankel. We consider
algorithms that incorporate pivoting without destroying the structure (cf. Sec. 1.13)
and describe some recent results on the stability of these algorithms. We also compare
these results with the corresponding stability results for the well-known algorithms of
Schur-Bareiss and Levinson, and for algorithms based on the seminormal equations.
As is well known, the standard direct method for solving dense nxn systems of linear
equations is Gaussian elimination with partial pivoting. The usual implementation
requires arithmetic operations of order n3.
In practice, linear systems often arise from some physical system and have a structure
that is a consequence of the physical system. For example, time-invariant physical
systems often give rise to Toeplitz systems of linear equations (Sec. 4.3.2). An n x n
Toeplitz matrix is a dense matrix because it generally has n2 nonzero elements. However,
it is determined by only O(n) parameters (in fact, by the 2n â 1 entries in its first row
and column). Similar examples are the Hankel, Cauchy, Toeplitz-plus-Hankel, and
Vandermonde matrices (see, e.g., Sec. 1.3 of this book, as well as [GV96] and [GKO95]).
When solving such a structured linear system it is possible to ignore the structure,
and this may have advantages if standard software is available and n is not too large.
However, if n is large or if many systems have to be solved, perhaps with real-time
constraints (e.g., in radar and sonar applications), then it is desirable to take advantage
of the structure. The primary advantage to be gained is that the time to solve a linear
system is reduced by a factor of order n to O(n2). Storage requirements may also be
reduced by a factor of order n, to O(n).
Most works concerned with algorithms for structured linear systems concentrate on
103

Stability of Fast Algorithms for Structured
104 
Linear Systems 
Chapter 4
the speed (usually measured in terms of the number of arithmetic operations required)
and ignore questions of numerical accuracy. However, it is dangerous to use fast algo-
rithms without considering their numerical properties. There is no point in obtaining
an answer quickly if it is much less accurate than is justified by the data.
In this chapter we consider both the speed and the numerical properties of fast algo-
rithms. Because there are many classes of structured matrices, and an ever-increasing
number of fast algorithms, we cannot attempt to be comprehensive. Our aim is to intro-
duce the reader to the subject, illustrate some of the main ideas, and provide pointers
to the literature.
In this chapter, a "fast" algorithm will generally be one that requires O(n2) arith-
metic operations, whereas a "slow" algorithm will be one that requires O(n3) arithmetic
operations. Thus a fast algorithm should (in general) be faster than a slow algorithm if
n is sufficiently large.
The subject of numerical stability and instability of fast algorithms is confused for
several reasons:
1. Structured matrices can be very ill-conditioned [Tyr94b]. For example, the Hilbert
matrix [Hil94], defined by a^- = l/(i+j â 1), is often used as an example of a very
ill-conditioned matrix [FM67]. The Hilbert matrix is a Hankel matrix. Reversing
the order of the rows gives a Toeplitz matrix. Even a stable numerical method
cannot be expected to give an accurate solution when it is applied to a very
ill-conditioned problem. Thus, when testing fast algorithms we must take the
condition of the problem into account and not expect more than is reasonable.
2. The solution may be less sensitive to structured perturbations (i.e., perturbations
that are physically plausible because they preserve the structure) than to gen-
eral (unstructured) perturbations. The effect of rounding errors in methods that
ignore the structure is generally equivalent to the introduction of unstructured
perturbations. Ideally we should use a method that introduces only structured
perturbations, but this property often does not hold or is difficult to prove, even for
methods that take advantage of the structure to reduce the number of arithmetic
operations.
3. The error bounds that can be proved are usually much weaker than those observed
on "real" or "random" examples. Thus, methods that are observed to work well
in practice cannot always be guaranteed, and it is hard to know if it is just the
analysis that is weak or if the method fails in some rare cases. (A classical example
of the latter phenomenon is given in Sec. 4.2.1.)
4. An algorithm may perform well on special classes of structured matrices, e.g.,
positive-definite matrices or Toeplitz matrices with positive reflection coefficients,
but perform poorly or break down on broader classes of structured matrices.
4.1.1 
Outline
Different authors have given different (and sometimes inconsistent) definitions of sta-
bility and weak stability. We follow Bunch [Bun85], [Bun87j. For completeness, our
definitions are given in Sec. 4.2.
The concept of displacement rank, defined in Sec. 4.3 and also in Ch. 1 of this book,
may be used to unify the discussion of many algorithms for structured matrices [KC94],
[KKM79a], [KS95a]. It is well known that systems of n linear equations with a low dis-
placement rank (e.g., Toeplitz or Hankel matrices) can be solved in O(n2) arithmetic op-

Section 4.2. 
Stability and Weak Stability 
105
erations. Asymptotically faster algorithms with time bound 0(nlog2n) exist [BGY80]
but are not considered here because their numerical properties are generally poor and
the constant factors hidden in the "0" notation are large [AG88].
For positive-definite Toeplitz matrices, the first 0(n2) algorithms were introduced
by Kolmogorov [Kol41], Wiener [Wie49], and Levinson [Lev47j. These algorithms are
related to recursions of Szego [Sze39] for polynomials orthogonal on the unit circle. An-
other class of O(n2) algorithms, e.g., the Bareiss algorithm [Bar69], is related to Schur's
algorithm for finding the continued fraction representation of a holomorphic function
in the unit disk [Schl?]. This class can be generalized to cover unsymmetric matrices
and other low displacement rank matrices [KS95a]. In Sections 4.4-4.6 we consider the
numerical stability of some of these algorithms. The GKO-Cauchy and GKO-Toeplitz
algorithms are discussed in Sec. 4.4. The Schur-Bareiss algorithm for positive-definite
matrices is considered in Sec. 4.5.1, and generalized Schur algorithms are mentioned in
Sec. 4.5.2. In Sec. 4.6 we consider fast orthogonal factorization algorithms and the fast
solution of structured least-squares problems. An embedding approach which leads to
a stable algorithm for structured linear systems is mentioned in Sec. 4.6.3.
Algorithms for Vandermonde and many other classes of structured matrices are not
considered hereâwe refer to Ch. 1. Also, we have omitted any discussion of fast "look-
ahead" algorithms [CH92a], [CH92b], [FZ93a], [FZ93b], [Gut93], [GH93a], [GH93b],
[HG93], [Swe93] because, although such algorithms often succeed in practice, in the
worst case they require O(n3) operations.
Much work has been done on iterative methods for Toeplitz and related systems.
Numerical stability is not a major problem with iterative methods, but the speed of
convergence depends on a good choice of preconditioner. Iterative methods are consid-
ered in Chs. 5 and 7, so we do not consider them in detail here. However, it is worth
noting that iterative refinement [JW77], [Wil65] can be used to improve the accuracy
of solutions obtained by direct methods (see, e.g., Sec. 2.12.2).
4.1.2 
Notation
In the following, R denotes an upper triangular or structured matrix, T is a Toeplitz
or Toeplitz-like matrix, P is a permutation matrix, L is lower triangular, U is upper
triangular, and Q is orthogonal. If A is a matrix with elements ajk, then \A\ denotes
the matrix with elements \cijk\- In error bounds, e is the machine precision [GV96],
and On(e) means O(ef(n)), where /(n) is a polynomial in n. We usually do not try to
specify the polynomial /(n) precisely because it depends on the norm used to measure
the error and on many unimportant details of the implementation. Also, the " notation
denotes computed quantities.
4.2 
STABILITY AND WEAK STABILITY
In this section we give definitions of stability and weak stability of algorithms for solving
linear systems. Consider algorithms for solving a nonsingular, n x n linear system
Ax = b. To avoid trivial exceptional cases we always assume that 6^0.
The condition number K = K>(A) is defined to be the ratio (J\jon of the largest and
smallest singular values of the matrix A (see [GV96] for a discussion of condition number
and singular values). If K is close to 1 we say that A is well-conditioned, and if K is large
we say that A is ill-conditioned. The meaning of "large" is a little flexible, but K > l/Â£
is certainly large and, depending on the circumstances, we may regard K > \f\fe as
large.

Stability of Fast Algorithms for Structured
106 
Linear Systems 
Chapter 4
There are many definitions of numerical stability in the literature, for example,
[Bjo87], [Bjo91], [BBHS95], [BS91], [Bun85], [CybSO], [GV96], [JW77], [MW80], [Pai73],
[Ste73]. Definitions 4.2.1 and 4.2.2 are taken from Bunch [Bun87j.
Definition 4.2.1 (Stable Algorithm). An algorithm for solving linear equations is
stable for a class of matrices A if for each A in A and for each b the computed solution
x to Ax = b satisfies Ax = b, where A is close to A and b is close to b.
Definition 4.2.1 says that, for stability, the computed solution has to be the exact
solution of a problem that is close to the original problem. This is the classical backward
stability of Wilkinson [Wil61], [Wil63], [Wil65]. We interpret "close" to mean close in
the relative sense in some norm, i.e.,
It is well known that the perturbation in A can be absorbed into the perturbation in 6,
since
Alternatively, the perturbation in 6 can be absorbed into the perturbation in A, since
Thus, it is not necessary to permit perturbations of both A and 6 in Definition 4.2.1.
Note that the matrix A is not required to be in the class A. For example, A might be
the class of nonsingular Toeplitz matrices, but A is not required to be a Toeplitz matrix.
If we require A â¬ A we get what Bunch [Bun87] calls strong stability. For a discussion of
the difference between stability and strong stability for Toeplitz algorithms, see [GK93],
[GKX94], [HH92], [Var92].
Stability does not imply that the computed solution x is close to the exact solution
x, unless the problem is well-conditioned. Provided KS is sufficiently small, stability
implies that
For more precise results, see Bunch [Bun87] and Wilkinson [Wil61].
4.2.1 
Gaussian Elimination with Pivoting
To provide a basis for comparison when we quote error bounds for fast methods, and to
give an example of a method for which the error bounds are necessarily more pessimistic
than what is usually observed, it is worthwhile to consider briefly the classical method
of Gaussian elimination (see also Sec. 1.6.2). Wilkinson [Wil61] shows that
where g = g(n) is the "growth factor." g depends on whether partial or complete
pivoting is used. In practice g is usually moderate, even for partial pivoting. However,
a well-known example shows that g(n) = 2n-1 is possible for partial pivoting, and it has
been shown that examples where g(n) grows exponentially with n may occasionally arise
in applications, e.g., for linear systems arising from boundary value problems [HH89].
Even for complete pivoting, it has not been proved that g(n) is bounded by a poly-
nomial in n. Wilkinson [Wil61] showed that g(n] < nOogn)/4+o(i)j and Gould [Qou91]

Section 4.2. Stability and Weak Stability 
107
showed that g(n) > n is possible for n > 12; there is still a large gap between these
results. Thus, to ensure that Gaussian elimination satisfies Definition 4.2.1, we must
restrict A to the class of matrices for which g is On(l). In practice this is not a seri-
ous problem, except in certain safety-critical real-time applications, because g can be
computed easily and cheaply as the computation proceeds. In the unlikely event that g
is large, the result can be checked by an independent computation using a more stable
(but slower) method.
4.2.2 
Weak Stability
Although stability is desirable, it is more than we can prove for many useful algorithms.
Thus, following Bunch [Bun87], we define the (weaker, but still useful) property of weak
stability.
Definition 4.2.2 (Weak Stability). An algorithm for solving linear equations is weakly
stable for a class of matrices A if for each well-conditioned A in A and for each b the
computed solution x to Ax = b is such that \\x â x\\/\\x\\ is small.
In Def. 4.2.2, we take "small" to mean On(e) and "well-conditioned" to mean that
K>(A) is 0n(l), i.e., is bounded by a polynomial in n. Prom (4.2.1), stability implies
weak stability. It may happen that we cannot prove the inequality (4.2.1), but we can
prove the weaker inequality
Clearly, in such a case, the method is weakly stable.
Define the residual vector r by
We refer to r/||6|| as the normalized residual. It is easy to compute and gives an
indication of how well the numerical method has performedâwe would be unhappy if
the the size of the normalized residual was large, and in general the smaller it is the
better. It is important to realize that a small normalized residual does not necessarily
mean the that computed solution x is accurate. If the condition number K(A) is large,
then x might not agree with the "correct" solution x to any significant figures, although
the normalized residual is very small. The residual and the solution error satisfy the
following well-known inequalities [Wil63]:
Thus, for well-conditioned A, \\x-x\\f\\x\\ is small if and only if ||r||/||6|| is small. This
observation leads to an alternative definition of weak stability.
Definition 4.2.3 (Equivalent Condition). An algorithm for solving linear equations
is weakly stable for a class of matrices A if for each well-conditioned A in A and for
each b the computed solution x to Ax = b is such that \\Ax â b\\/\\b\\ is small.

Stability of Fast Algorithms for Structured
108 
Linear Systems 
Chapter 4
If we can prove that
then the method is stable; if we can prove that
then the method is (at least) weakly stable.
4.2.3 
Example: Orthogonal Factorization
To illustrate the concepts of stability and weak stability, consider computation of the
Cholesky factor R of ATA,7 where A is an m x n matrix of full rank n. For simplicity
we assume that HA^AH is of order unity. A good O(mn2) algorithm is to compute the
QR factorization
A = QR
of A using Householder or Givens transformations [GV96] (see also App. B). It can be
shown [Wil63] that the computed matrices Q, R satisfy
where QTQ â I, Q is close to Q, and A is close to A. Thus, the algorithm is stable in
the sense of backward error analysis. Note that ||^4T-A â RTR\\ is small, but \\Q â Q\\
and \\R - R\\/\\R\\ are not necessarily small. Bounds on ||Q - Q\\ and \\R - R\\/\\R\\
depend on K,(A) and are discussed in [Gol65], [Ste77], [W1165].
A different algorithm is to compute (the upper triangular part of) ATA and then
compute the Cholesky factorization of ATA by the usual (stable) algorithm. The com-
puted result R is such that RTR is close to ATA. However, this does not imply the
existence of A and Q such that (4.2.4) holds (with A close to A and some Q with
QTQ = I) unless A is well-conditioned [Ste79]. By analogy with Definition 4.2.3 above,
we may say that Cholesky factorization of ATA gives a weakly stable algorithm for
computing R, because the "residual" ATA â RTR is small.
4.3 
CLASSES OF STRUCTURED MATRICES
We consider structured matrices R that satisfy displacement equations of the form8
where F and A have some simple structure (usually lower triangular or banded, with
three or fewer full diagonals), G and B are n x a, and a is some fixed integer. The pair
of matrices (G, B) is called an {F, A}-generator of R.
The number a is called the displacement rank of R with respect to the displacement
operation (4.3.1). We are interested in cases where a is small (say, at most 4). For
a discussion of the history and some variants of (4.3.1), see Ch. 1 of this book and
[KS95a].
7In this subsection, the symbol R denotes an upper triangular matrix.
8We now use the letter R to denote a structured matrix and the letter A to denote a displacement
operator.

Section 4.4. Structured Gaussian Elimination 
109
4.3.1 
Cauchy and Cauchy-Like Matrices
Particular choices of F and A lead to definitions of basic classes of matrices (see also
Sec. 1.3). Thus, for a Cauchy matrix, a = 1,
we have
and
As a natural generalization, we can take G and B to be any n x a rank-a matrices,
with F and A as above. Then a matrix R satisfying (4.3.1) is said to be a Cauchy-like
matrix.
4.3.2 
Toeplitz Matrices
For a Toeplitz matrix R = T = [tjk] â [flj-fe], we can take a = 2,
and
We can generalize to Toeplitz-like matrices by taking G and B to be general nxa rank-a
matrices, a > 2. We can also choose other matrices {F, A} for the Toeplitz case (see
Ch. 1).
4.4 
STRUCTURED GAUSSIAN ELIMINATION
Let an input matrix, RI , have the partitioning
The first step of normal Gaussian elimination is to premultiply RI by (see also Sec. 1.6.2
for a related discussion)

Stability of Fast Algorithms for Structured
110 
Linear Systems 
Chapter 4
which reduces it to
where
is the Schur complement of d\ in RI . At this stage, RI has the factorization
One can proceed recursively with the Schur complement R%, eventually obtaining a
factorization RI = LU.
As discussed in Sec. 1.5, the key to structured Gaussian elimination is the fact that
the displacement structure is preserved under Schur complementation and that the
generators for the Schur complement of Rk+i can be computed from the generators of
Rk in O(n) operations (see also Sec. 1.12).
Row or column interchanges destroy the structure of matrices such as Toeplitz ma-
trices. However, if F is diagonal (which is the case for Cauchy- and Vandermonde-type
matrices), then the structure is preserved under row permutations (recall the discussion
in Sec. 1.13). This observation leads to the GKO-Cauchy algorithm of [GKO95] for fast
factorization of Cauchy-like matrices with partial pivoting (see Sec. 1.13.1). The idea
of using pivoting in a hybrid Schur-Levinson algorithm for Cauchy-like systems was
introduced by Heinig [Hei95]. There have been several variations on the idea in recent
papers [GKO95], [Gu95a], [Ste98].
4.4.1 
The GKO-Toeplitz Algorithm
Heinig [Hei95] showed that, if T is a Toeplitz-like matrix with {Zi, Z^i }-generators
(G,B), then
is a Cauchy-like matrix, where
is the discrete Fourier transform (DFT) matrix,
and the generators of T and R are simply related. In fact, it is easy to verify that
where
and
Thus, the (Df,Da) generator of R is (TG,TDB}.

Section 4.4. Structured Gaussian Elimination 
111
The transformation T <-> R is perfectly stable because f and D are unitary. Note
that R is (in general) complex even if T is real. This increases the constant factors in
the time bounds, because complex arithmetic is required.
Heinig's observation was exploited in [GKO95] (see Sec. 1.13.1): R can be factored
as R = PTLU using the GKO-Cauchy algorithm. Thus, from the factorization
T = f*PTLUFD,
a linear system involving T can be solved in O(n2) operations. The full procedure of
conversion to Cauchy form, factorization, and solution requires 0(n2) complex opera-
tions.
Other structured matrices, such as Hankel, Toeplitz-plus-Hankel, Vandermonde,
Chebyshev-Vandermonde, etc., can be converted to Cauchy-like matrices in a similar
way (e.g., [GKO95], [KO95]).
4.4.2Error Analysis
Because GKO-Cauchy and GKO-Toeplitz involve partial pivoting, we might guess that
their stability would be similar to that of Gaussian elimination with partial pivoting.
Unfortunately, there is a flaw in this reasoning. During GKO-Cauchy the generators
have to be transformed, and the partial pivoting does not ensure that the transformed
generators are small.
Sweet and Brent [SB95] show that significant generator growth can occur if all the
elements of GBT are small compared with those of \G\ â¢ \BT\. This cannot happen for
ordinary Cauchy matrices because a = 1 and the successive generator matrices, Gk and
Bk, have only one column and one row, respectively. However, it can happen for higher
displacement-rank Cauchy-like matrices, even if the original matrix is well-conditioned.
For example, taking a = 2,
where ||a|| is of order unity and ||e|| and ||f|| are very small, we see that all the elements
of GBT = aeT â faT are very small compared with those of |G||BT|. Moreover, because
a, e, and f can be arbitrary except for their norms, the original Cauchy-type matrix
is in general well-conditioned. The problem is that the generators are ill-conditioned,
being close to lower-rank matrices.
There are corresponding examples for Toeplitz-type matrices, easily derived using the
correspondence between generators of Toeplitz-type and Cauchy-type matrices discussed
in Sec. 4.4.1. However, in the strictly Toeplitz case the special form of the matrices B
and G in (4.3.2) and (4.3.3) imposes additional constraints, which appear to rule out
this kind of example. However, it is still possible to give examples where the normalized
solution error grows like K2 and the normalized residual grows like K, where K is the
condition number of the Toeplitz matrix: see Sweet and Brent [SB95, Â§5.2]. Thus, the
GKO-Toeplitz algorithm is (at best) weakly stable.
It is easy to think of modified algorithms that avoid the examples given above and
by Sweet and Brent [SB95], but it is difficult to prove that they are stable in all cases.
Stability depends on the worst case, which may be rare and hard to find by random
sampling.
The problem with the original GKO algorithm is growth in the generators. Gu
[Gu95a] suggested exploiting the fact that the generators are not unique. Recall the
displacement equation (4.3.1). Clearly we can replace G by GM and BT by M~1BT,

Stability of Fast Algorithms for Structured
112 
Linear Systems 
Chapter 4
where M is any invertible a x a matrix, because this does not change the product
GBT'. This holds similarly at later stages of the GKO algorithm. Gu [Gu95a] proposes
taking M to orthogonalize the columns of G (that is, at each stage perform an orthogonal
factorization of the generatorsâsee also Sec. 2.11). Stewart [Ste98] proposes a (cheaper)
LU factorization of the generators. This avoids examples of the type given above. In
both cases, clever pivoting schemes give error bounds analogous to those for Gaussian
elimination with partial pivoting.
The error bounds obtained by Gu and Stewart involve a factor Kn, where K depends
on the ratio of the largest to smallest modulus elements in the Cauchy matrix
Although this is unsatisfactory, it is similar to the factor 2n-1 in the error bound for
Gaussian elimination with partial pivoting. As mentioned in Sec. 4.2.1, the latter factor
is extremely pessimistic, which explains why Gaussian elimination with partial pivoting
is popular in practice [Hig96]. Perhaps the bounds of Gu and Stewart are similarly
pessimistic, although practical experience is not yet extensive enough to be confident of
this. Stewart [Ste98] gives some interesting numerical results suggesting that his scheme
works well, but more numerical experience is necessary before a definite conclusion can
be reached.
4.4.3 
A General Strategy
It often happens that there is a choice of
1. a fast algorithm that usually gives an accurate result but occasionally fails (or at
least cannot be proved to succeed every time), or
2. an algorithm that is guaranteed to be stable but is slow.
In such cases, a good strategy may be to use the fast algorithm but then check
the normalized residual \\Ax â 6||/||6||, where x is the computed solution of the system
Ax = b. If the normalized residual is sufficiently small (say at most 1000s:) we can
accept x as a reasonably good solution. In the rare cases that the normalized residual is
not sufficiently small we can can use the slow but stable algorithm. (Alternatively, if the
residual is not too large, one or two iterations of iterative refinement may be sufficient
and faster [JW77], [W1165]â see also Sec. 2.12.2.)
An example of this general strategy is the solution of a Toeplitz system by Gu
or Stewart's modification of the GKO algorithm. We can use the O(n2) algorithm,
check the residual, and resort to iterative refinement or a slow but stable algorithm
in the (rare) cases that it is necessary. Computing the residual takes only O(nlogn)
arithmetic operations.
We now turn our attention to another class of methods.
4.5 
POSITIVE-DEFINITE STRUCTURED MATRICES
An important class of algorithms, typified by the algorithm of Bareiss [Bar69], finds an
LU factorization of a Toeplitz matrix T and (in the symmetric case) is related to the
classical algorithm of Schur [Bur75], [Goh86], [Schl7] (see also Ch. 1).
It is interesting to consider the numerical properties of these algorithms and compare
them with the numerical properties of the Levinson-Durbin algorithm, which essentially
finds an LU factorization of T"1.

Section 4.5. 
Positive-Definite Structured Matrices 
113
4.5.1 
The Bareiss Algorithm for Positive-Definite Matrices
Bojanczyk, Brent, de Hoog, and Sweet (BBHS) have shown in [BBHS95], [Swe82] that
the numerical properties of the Bareiss algorithm are similar to those of Gaussian elim-
ination (without pivoting). Thus, the algorithm is stable for positive-definite symmetric
Toeplitz matrices. In fact, the results of [BBHS95] establish stability for the larger class
of quasi-Toeplitz positive-definite matrices. (For a definition of this class, see Ch. 1.)
The result of [BBHS95, Sec. 5] is that the computed upper triangular factor U of
the positive-definite matrix T satisfies the backward error bound
where to is a normalizing factor (a diagonal element of T), provided the Cholesky
downdating is done using the "mixed downdating" scheme [BBDH87]. If "hyperbolic
downdating" is used, then the bound (4.5.1) increases to O(n3e), although numerical
experiments reported in [BBHS95, Sec. 7] did not demonstrate much difference between
the two forms of downdating. (Different forms of downdating are equivalent to different
implementations of hyperbolic rotations.) The same numerical experiments showed that
Cholesky factorization usually gave a slightly more accurate solution, but a slightly
larger residual, than the fast algorithm using either form of downdating.
The Levinson-Durbin algorithm can be shown to be weakly stable for bounded n,
and numerical results by [Var93], [BBHS95], and others suggest this is all we can expect.
Thus, the Bareiss algorithm is (generally) better numerically than the Levinson-Durbin
algorithm. For example, the numerical results reported in [BBHS95, Sec. 7] for three
ill-conditioned positive-definite Toeplitz matrices (the Prolate matrix [Var93] and two
matrices whose reflection coefficients have alternating signs) indicate that the Levinson-
Durbin algorithm typically gives solution errors more than twice as large as the other
algorithms (Bareiss with two forms of downdating and Cholesky), and residual errors
4 x 103 to 5 x 105 as large. The condition numbers of the test matrices ranged from
3 x 1014 to 8 x 1015, and the machine precision was e = 2~53 ~ 10~16; for better
conditioned matrices the differences between the methods are generally less apparent.
Cybenko [CybSO] showed that if the so-called reflection coefficients (see Sec. 1.2)
are all positive, then the Levinson-Durbin algorithm for solving the Yule-Walker equa-
tions (a positive-definite Toeplitz system with a special right-hand side) is stable. Un-
fortunately, Cybenko's result usually is not applicable, because most positive-definite
Toeplitz matrices (e.g., the examples just quoted) do not satisfy the restrictive condition
on the reflection coefficients.
4.5.2Generalized Schur Algorithms
The Schur algorithm can be generalized to factor a large variety of structured matri-
ces [KC94], [KS95a] (see Ch. 1). For example, suitably generalized Schur algorithms
apply to block Toeplitz matrices, Toeplitz block matrices, and matrices of the form
TTT, where T is rectangular Toeplitz.
It is natural to ask to what extent the stability results of [BBHS95] can be gener-
alized. This has been considered in recent papers by Stewart and Van Dooren [SD97b]
and by Chandrasekaran and Sayed [CS96] (see also Chs. 2 and 3 of this book).
Stewart and Van Dooren [SD97b] generalized the results of BBHS to matrices with
similar structure but displacement rank larger than 2. They showed that for higher
displacement ranks one has to be careful to implement the hyperbolic rotations properly.
If they are implemented correctly (e.g., as recommended in [BBDH87]), then a backward

Stability of Fast Algorithms for Structured
114 
Linear Systems 
Chapter 4
error bound of the same form as (4.5.1) holds. In contrast, as mentioned in Sec. 4.5.1,
one does not have to be so careful when the displacement rank is 2 because a result of the
same form as (4.5.1), albeit with an extra power of n, holds for other implementations
of hyperbolic rotations. (Of course, the error bound will certainly fail for a 
sufficiently
bad implementation of hyperbolic rotations.)
Chandrasekaran and Sayed [CS96] studied a significantly more general Schur-like
algorithm. (Their results are discussed in Chs. 2 and 3 of this book.) They dropped
the assumption that the matrices F, A of Sec. 4.3.2 are shift matrices Z, although the
corresponding matrices still have to satisfy certain restrictions. This extra generality
causes significant complications in the error analysis and appears to make the Schur
algorithm even more sensitive to the method of implementing hyperbolic rotations.
Provided these hyperbolic rotations and certain "Blaschke factors" (see Sec. 1.6 for
a description of these factors) are implemented correctly, a backward stability result
similar to that of BBHS can be established. The interested reader is referred to [CS96]
(and also Ch. 2) for details.
The overall conclusion is that the generalized Schur algorithm is stable for positive-
definite symmetric (or Hermitian) matrices, provided the hyperbolic rotations and
the Blaschke factors (if any) in the algorithm are implemented correctly.
We now drop the assumption of positive definiteness, and even (temporarily) the
assumption that the matrix T is square, and we consider fast algorithms for orthogonal
factorization.
4.6 
FAST ORTHOGONAL FACTORIZATION
In an attempt to achieve stability without pivoting, and to solve ra x n least squares
problems (m > n), it is natural to consider algorithms for computing an orthogonal
factorization
of an ra x n Toeplitz matrix T. We assume that T has full rank n. For simplicity, in
the time bounds we assume ra = O(ri) to avoid functions of both ra and n.
The first O(n2) (more precisely, O(ran)) algorithm for computing the factoriza-
tion (4.6.1) was introduced by Sweet [Swe82]. Unfortunately, Sweet's algorithm is
unstableâsee [LQ87].
Other O(n2) algorithms for computing the matrices Q and U or U~l were given by
Bojanczyk, Brent, and de Hoog (BBH) [BBH86], Chun, Kailath, and Lev-Ari [CKL87],
Cybenko [Cyb87], and Qiao [Qia88], but none of them has been shown to be stable, and
in several cases examples show that they are unstable.
It may be surprising that fast algorithms for computing an orthogonal factoriza-
tion (4.6.1) are unstable. The classical O(n3) algorithms are stable because they form
Q as a product of elementary orthogonal matrices (usually Givens or Householder ma-
trices [Gol65], [GV96], [Wil65]). Unlike the classical algorithms, the O(n2) algorithms
do not form Q in a numerically stable manner as a product of matrices that are (close
to) orthogonal. This observation explains both their speed and their instability!
For example, the algorithms of [BBH86] and [CKL87] depend on Cholesky down-
dating, and numerical experiments show that they do not give a Q that is close to
orthogonal. This is not too surprising, because Cholesky downdating is known to be a
sensitive numerical problem [BBDH87], [Ste79]. Perhaps it is more surprising that the
authors of [BBHS95] were able to use an error analysis of a form of downdating in their
analysis of the Bareiss algorithm (but only in the positive-definite case)âsee Sec. 4.5.1.

Section 4.6. 
Fast Orthogonal Factorization 
115
4.6.1 
Use of the Seminormal Equations
It can be shown that, provided the Cholesky downdates are implemented in a certain way
(analogous to the condition for the stability of the Schur algorithm for a > 2, discussed
in Sec. 4.5.2), the BBH algorithm computes U in a weakly stable manner [BBH95]. In
fact, the computed upper triangular matrix U is about as good as can be obtained by
performing a Cholesky factorization of TTT, so
Thus, by solving
(the so-called seminormal equations) we have a weakly stable algorithm for the solution
of general full-rank Toeplitz least-squares problems. In the case m = n, this gives a
weakly stable algorithm for the solution of Toeplitz linear systems Tx = b in O(n2)
operations. The solution can be improved by iterative refinement if desired [GW66].
Weak stability is achieved because the computation of Q is avoided. The disadvantage
of this method is that, by implicitly forming TTT, the condition of the problem is
effectively squared. If the condition number AC = Â«(T) is in the range
then it usually will be impossible to get any significant figures in the result (iterative
refinement may fail to converge) without reverting to a slow but stable orthogonal
factorization algorithm. One remedy is to use double-precision arithmetic, i.e., replace
e by Â£2, but this may be difficult if Â£ already corresponds to the maximum precision
implemented in hardware.
Another way of computing the upper triangular matrix Â£/, but not the orthogonal
matrix Q, in (4.6.1) is to apply a generalized Schur algorithm to TTT. This method
also squares the condition number.
4.6.2 
Computing Q Stably
It seems difficult to give a satisfactory O(n2) algorithm for the computation of Q in
the factorization (4.6.1). The algorithm suggested in [BBH86] is unsatisfactory, as are
all other fast algorithms known to us. In Sec. 4.6.1 we sidestepped this problem by
avoiding the computation of Q, but in some applications Q is required. We leave the
existence of a fast, stable algorithm for the computation of Q as an open question.
4.6.3 
Solution of Indefinite or Unsymmetric Structured Systems
Using a modification of the embedding approach pioneered by Chun and Kailath [Chu89],
[KC94] and Chandrasekaran and Sayed [CS98] gives a stable algorithm to compute a
factorization of the form (see Ch. 3 of this book)
in terms of three matrices {A, Q, U}, where A is a lower triangular matrix that com-
pensates for the fact that Q is not numerically orthogonal. (Of course, the factor-
ization (4.6.2) is not unique.) The motivation of [CS98] was to give a backward-stable

Stability of Fast Algorithms for Structured
116 
Linear Systems 
Chapter 4
algorithm for general square Toeplitz or quasi-Toeplitz systems (symmetric but not pos-
itive definite, or possibly unsymmetric). For details of the stability analysis, we refer
to [CS98] and Ch. 3.
The algorithm of [CS98] can be used to solve linear equations but not least-squares
problems (because T has to be square). Because the algorithm involves embedding the
n x n matrix T in a 2n x 2n matrix
the constant factors in the operation count are moderately large: 59n2 + O(nlogn),
which should be compared to 8n2 + O(n log n) for BBH and the seminormal equations
(a weakly stable method, as discussed in Sec. 4.6.1). These operation counts apply for
ra = n: see [BBH95] for operation counts of various algorithms when m > n.
4.7 
CONCLUDING REMARKS
Although this survey has barely scratched the surface, we hope the reader who has
come this far is convinced that questions of numerical stability are amongst the most
interesting, difficult, and useful questions that we can ask about fast algorithms for
structured linear systems. It is not too hard to invent a new fast algorithm, but to find
a new stable algorithm is more difficult, and to prove its stability or weak stability is a
real challenge!
Acknowledgments
A preliminary version of this review appeared in [Bre97]. Thanks to Greg Ammar, Adam
Bojanczyk, James Bunch, Shiv Chandrasekaran, George Cybenko, Paul Van Dooren, Lars
Elden, Roland Freund, Andreas Griewank, Ming Gu, Martin Gutknecht, Georg Heinig, Prank
de Hoog, Franklin Luk, Vadim Olshevsky, Haesun Park, A. H. Sayed, Michael Stewart, Douglas
Sweet, and James Varah for their assistance, and especially to A. H. Sayed for his detailed
comments on a draft of this chapter.

Chapter 5
ITERATIVE METHODS FOR
LINEAR SYSTEMS WITH
MATRIX STRUCTURE
Raymond H. Chan
Michael K. Ng
5.INTRODUCTION
So far in the book, only direct methods for the solution of linear systems of equations
Ax = b have been discussed. These methods involve computing a factorization for
the coefficient matrix A, say, an LDU or a QR factorization, and then solving for the
unknown vector x by solving triangular systems of equations.
There is another class of methods, known as iterative methods, where the solution
x is approximated by successive iterates Xk starting from an initial guess XQ. A major
algorithm in this regard is the so-called conjugate gradient (CG) method. However, its
convergence rate, as we shall demonstrate in Sec. 5.2, is dependent on the clustering of
the eigenvalues of the coefficient matrix A; the more clustered the eigenvalues are the
faster the convergence of Xk to x.
For this reason, one way to speed up the convergence of the CG method is to
precondition the original system. This corresponds to choosing a matrix P and solving
instead the equivalent system P~lAx = P~*b. The preconditioner P is chosen such
that the spectrum of the new coefficient matrix P~1A has better clustering properties
than that of A. This condition, in addition to other requirements on the matrix P,
such as ease of computations with P, results in the so-called preconditioned conjugate
gradient (PCG) method. The method allows us to solve n x n structured linear systems
in O(nlogn) operations and is therefore attractive for large-scale problems.
This chapter provides a survey of some of the latest developments on PCG methods
for the solution of n x n linear systems of equations with structured coefficient matrices,
such as Toeplitz, Toeplitz-like, Toeplitz-plus-Hankel, and Toeplitz-plus-band matrices.
Several preconditioning techniques are discussed for different classes of structured ma-
trices. Application examples are also included. We start with a review of the classical
CG method.
117

118 
Iterative Methods for Linear Systems with Matrix Structure 
Chapter 5
5.2 
THE CG METHOD
The CG method was invented in the 1950s [HS52] as a direct method for solving Hermi-
tian positive-definite systems. It has been widely used in the last 20 years as an iterative
method.
Let us consider the linear system of equations Axt = 6, where A e Cnxn is a
nonsingular Hermitian positive-definite matrix and b e Cnxl. Consider further the
quadratic cost function
The minimizer of (j)(x) over x 6 Cnxl easily can be seen to satisfy Ax â b = 0 and,
hence, coincides with the solution xt of the linear system of equations Axt = b.
Given an initial guess XQ G Cnxl for Xt and the corresponding initial residual TQ =
6 â AXQ, the kih iterate Xk â¬ Cnxl of the CG method is determined by minimizing 0(x)
over all vectors x in the subspace XQ + K&, where K/, is the kih Krylov subspace
By this we mean that the minimization in the kih step is performed over all vectors x
that can be expressed in the form
for some coefficients {ctj}. If we now introduce the norm
then it can be shown that minimizing 0(x) over XQ -f Kfc is the same as minimizing
||xt-a;|U over zo + Kfc,
Using (5.2.2), we can write
But since TQ = b â AXQ = A(xt â XQ}, we obtain
where the polynomial
has degree k and satisfies p(0) = 1. Therefore, our minimization problem becomes

Section 5.2. The CG Method 
119
where P/e is the set of polynomials of degree k.
Now, the spectral theorem for Hermitian positive-definite matrices asserts that A =
UAU*, where U is a unitary matrix whose columns are the eigenvectors of A and A
is a diagonal matrix with the positive eigenvalues of A on the diagonal. Since UU* =
U*U = /, we have Aj = UKJU* and, consequently, p(A) = Up(A)U*.
Define the square root factor A1/2 = UKl/2U*. Then
where || â¢ ||2 denotes the Euclidean norm of a vector argument or the maximum singular
value of a matrix argument.
Together with (5.2.3), this inequality implies that
Here cr(A) is the set of all eigenvalues of A. Clearly, if k = n, we can choose p to be the
nth degree polynomial that vanishes at all the eigenvalues A â¬ o~(A) withp(O) = 1. Then
the maximum in the right-hand side of (5.2.4) will be zero and we have the following
result (see [AB84, p. 24]).
Theorem 5.2.1 (A Property of the CG Method). Let A be a Hermitian positive-
definite matrix of size n. Then the CG algorithm (described below in more detail) finds
the solution of Ax = b within n iterations in the absence of any round-off errors.
In most applications, the number of unknowns n is very large. In these cases, it is
better to regard the CG approach as an iterative method and to terminate the iteration
when some specified error tolerance is reached. The usual implementation of the CG
method is to find, for a given e, a vector x so that ||6 â Ax\\2 < e||6||2- We have the
following statement (see, e.g., [GV96]).
Algorithm 5.2.1 (The CG Algorithm). Consider a Hermitian positive-definite lin-
ear system of equations Ax â b. The inputs of the CG algorithm are the right-hand side
b, a routine that computes the action of A on a vector, and an initial guess XQ, which
will be overwritten by the subsequent iterates Xk. We limit the number of iterations to
kmax and return the solution Xk and the residual norm pk â¢
CG(x, b, A, e, kmax)
1. Start with r = b â AXQ, po = \\r\\%, k = I.
2. Do while ^pk-i > e\\b\\2 and k < kmax:

120 
Iterative Methods for Linear Systems with Matrix Structure 
Chapter 5
Note that the matrix A itself need not be formed or stored; only a routine for
matrix-vector products Aq is required. Now we consider the cost. We need to store
only four vectors x,w,q, and r. Each iteration requires a single matrix-vector product
(to compute w = Aq}, two scalar products (one for q*w and one to compute pk = IHH),
and three operations of the form ax + y, where x and y are vectors and a is a scalar.
Thus, besides the matrix-vector multiplication, each iteration of Alg. 5.2.1 requires O(n)
operations, where n is the size of the matrix A.
For k < n, the convergence rate of the CG method can be determined by the
condition number K,(A) of the matrix A, where K,(A) = Amax(-A)/Amin(.A), the ratio
between the largest eigenvalue of A and the smallest eigenvalue of A. In fact, by
choosing the polynomial p in (5.2.4) to be the kih degree Chebyshev polynomial, one
can establish the following theorem (see [AB84, p. 26]).
Theorem 5.2.2 (Convergence of the CG Method). Let A be Hermitian positive
definite with condition number K,(A). Then the kth iterate Xk of the CG method satisfies
The above theorem shows that the convergence rate of the CG method is linear, viz.,
where r < 1. We note that if we have more information about the distribution of the
eigenvalues of the matrix A, then we can obtain a better bound for the error as the
following theorem shows.
Let us consider a special case where the eigenvalues of A are clustered around 1,
i.e., except for the outlying eigenvalues, all eigenvalues are in the interval [1 â e, 1 -t- e],
where e < 1 is a small positive number. Then the CG method will converge very fast.
Corollary 5.2.1 (The Case of Clustered Eigenvalues). Consider the same setting
as Theorem 5.2.2. If the eigenvalues of A are such that
then

Section 5.3. Iterative Methods for Solving Toeplitz Systems 
121
This result suggests that the performance of the CG method is greatly influenced
by the distribution of the eigenvalues of the coefficient matrix. In this chapter we are
primarily interested in structured coefficient matrices, such as Toeplitz or Toeplitz-like.
For this reason, in the next section we start with the Toeplitz case and review some
basic facts about its spectral distribution. (These are covered in some detail in Ch. 6.)
We then use this discussion to motivate the use of, and the need for, preconditioners.
5.3 
ITERATIVE METHODS FOR SOLVING TOEPLITZ
SYSTEMS
When A is a Toeplitz matrix, we shall denote it by T. In fact, we shall be more explicit
and write Tn instead of T to emphasize the fact that it is an n x n matrix. It turns out
that there is a close relationship between the spectrum of Tn and its so-called generating
function as we now explain.
Let Gin denote the set of all 27r-periodic continuous complex-valued functions defined
on [â7T,7r]. For any / e C^, let
be the so-called Fourier coefficients of /. For all n > 1, let Tn be the n x n Toeplitz
matrix with entries tjtk = tj-k, 0 < 3, k < n, i.e.,
The function / is called the generating function of the sequence of Toeplitz matrices
{Tn} [GS84]. If / is a real-valued function, we have
where the symbol * denotes complex conjugation. It follows that the {Tn} are Hermitian
matrices. Note further that when / is an even real-valued function, the matrices {Tn}
are real symmetric.
We may remark that in several practical applications, the generating function / is
readily available. Typical examples of generating functions are the kernels of Wiener-
Hopf equations (see [GF74, p. 82]), the functions that describe the amplitude character-
istics of recursive digital filters (see [CC82]), the spectral density functions of stationary
stochastic processes (see [GS84, p. 171]), and the point-spread functions in image de-
blurring (see [Jai89, p. 269]).
The following statement clarifies the connection between the spectrum of Tn and its
generating function /, as discussed in Sec. 6.3.1. The proof of the theorem can be found
in [GS84] (see also Thm. 6.3.2).
Theorem 5.3.1 (Spectra and Generating Functions). Let f be a 2n-periodic con-
tinuous real-valued function. Then the spectrum A(Tn) ofTn 
satisfies

122 
Iterative Methods for Linear Systems with Matrix Structure 
Chapter 5
where /mjn and /max Â°>re the minimum and maximum values of f , respectively. More-
over, the eigenvalues \j(Tn), j = 0, l , . . . , n â 1, are equally distributed as f ( 2 i r j / n ) ,
i.e.,
for any continuous function g defined on [â7r,7r].
To illustrate the theorem, consider the one-dimensional discrete Laplacian matrix
(encountered in Sec. 6.3)
Its generating function is 2 â 2cos($). Its eigenvalues are given by
For n â 32, the eigenvalues of Tn are depicted in Fig. 5.1.
Figure 5.1. Spectrum of one-dimensional discrete Laplacian.
5.3.1 
Preconditioning
We saw earlier in Sec. 5.2 that the convergence rate of the CG method for the solution
of a system of linear equations Anx â b depends partly on how clustered the spectra of
the sequence of matrices {An} are. Here, by the clustering of the spectra of a sequence
of matrices we mean the following (where In denotes the n x n identity matrix).
Definition 5.3.1 (Clustered Spectra). A sequence of matrices {An}Â£ci1 is said to
have clustered spectra around 1 if, for any given e > 0, there exist positive integers n\
and HI such that for all n > ni, at most n^ eigenvalues of the matrix An â In have
absolute value larger than e.
The equal distribution of the eigenvalues of Hermitian Toeplitz matrices indicates
that the eigenvalues will not be clustered in general. For this reason, one way to speed
up the convergence rate of the CG method is to precondition the Toeplitz system of
equations. This means that instead of solving Tnx = 6, we solve the preconditioned
system
for some so-called preconditioner matrix Pn that we choose. The preconditioner Pn
should be chosen according to the following criteria:

Section 5.3. 
Iterative Methods for Solving Toeplitz Systems 
123
1. It should be possible to construct Pn within O(nlogn) operations.
2. It should be possible to solve a linear system of equations with coefficient matrix
Pn, say, Pnv â y, in O(nlogn) operations.
3. The spectrum of P^xTn should be clustered or the condition number K(P~lTn]
of the preconditioned matrix should be close to 1 (cf. Thm. 5.2.2 and Cor. 5.2.1).
The statement of the PCG algorithm is now the following.
Algorithm 5.3.1 (The PCG Algorithm). Consider a Hermitian positive-definite
linear system of equations Ax = b. The inputs of the PCG algorithm are the right-hand
side b, a routine that computes the action of A on a vector, a preconditioner P, and an
initial guess XQ, which will be overwritten by the subsequent iterates Xk- We limit the
number of iterations to kmax and return the solution Xk and the residual norm pk.
PCG (x,b,A,P,e,kmax)
1 r _ I. _ A- n _ ||r||2 
L. _ I
1. r â o 
six, PQ â \\r\\2, K â i.
2. Do while ^/Pk-i > ^\\b\\2 and k < kmax
z = P~lr (or solve Pz = r)
rk-i = z*r
if k = I then (3 = 0 and q = z
else
(3 = Tfc_i/Tfc_2 and q = z + (3q
w = Aq
a = rk-i/q*w
x = x + aq
r = r â aw
Pk = \\r\\l
fc = fc + 1.
In the next three subsections, we review three different kinds of preconditioner that
have been developed for Toeplitz systems and that satisfy the above three criteria. We
first provide a brief discussion of circulant matrices.
5.3.2 
Circulant Matrices
A circulant matrix is a Toeplitz matrix whose rows are circular shifts of one another.
More specifically, an n x n circulant matrix Cn has the form

124 
Iterative Methods for Linear Systems with Matrix Structure 
Chapter 5
where
Circulant matrices have the important property that they are diagonalized by the
Fourier matrix Fn, i.e.,
where the entries of Fn are given by
and An is a diagonal matrix holding the eigenvalues of Cn [Dav79, p. 73]. The matrix
Fn is unitary, i.e., FnF* = F*Fn = In.
Moreover, multiplying a vector x by Fn, say, Fnx, results in a column vector whose
entries compose the DFT of x. It is well known that, due to the special structure of Fn,
products of the form Fnx can be computed in O(nlogn) operations by employing the
FFT algorithm [CT65].
Now using (5.3.9), we note that the first column of Fn is -4=col{l,!,...,!}. Hence,
That is, the entries of An can be obtained in O(nlogn) operations by taking the FFT
of the first column of Cn. In fact, expression (5.3.10) shows that the diagonal entries
Afc of An are given by
Once An is obtained, products of the form Cny and C~ly, for any vector y, can be
computed by FFTs in O(nlogn) operations using (5.3.9).
5.3.3 
Toeplitz Matrix-Vector Multiplication
In each PCG iteration, one matrix-vector multiplication Tny is needed. This can be
computed by FFTs by first embedding Tn into a 2n x 2n circulant matrix, i.e.,
where (see Strang [Str86])
Then we can carry out the multiplication in (5.3.12) by using the decomposition (5.3.9).
The matrix-vector multiplication Tny thus requires O(2nlog(2n)) operations. It follows
that the total number of operations per iteration of Algorithm CG in this case is of
O(nlogn) operations.

Section 5.3. Iterative Methods for Solving Toeplitz Systems 
125
5.3.4 
Circulant Preconditioners
In 1986, Strang [Str86] and Olkin [Olk86] independently proposed the use of circulant
matrices to precondition Hermitian Toeplitz matrices in CG iterations. Part of their
motivation was to exploit the fact that circulant matrices can be inverted rather fast.
Strang's Preconditioner
For an n x n Hermitian Toeplitz matrix Tn, Strang's circulant preconditioner [Str86]
is defined to be the matrix that copies the central diagonals of Tn and reflects them
around to complete the circulant requirement. For Tn given by (5.3.2), the diagonals Sj
of Strang's preconditioner Sn = [sk-t]o<k,e<n are given by
Here the notation [^J denotes the largest integer m < ^.
The approach developed in the following convergence proof (Thms. 5.3.2 and 5.3.3
and Cor. 5.3.1) of Strang's preconditioned system have been adapted by other authors
to establish the convergence proof of other circulant preconditioned systems. The main
idea of the proof is to show that the preconditioned matrices 5~JTn can be written in
the form In -f Ln + Un, where In is the nxn identity matrix, Ln is a matrix of low rank,
and Un is a matrix of small ^2-norm. It will follow in this case that the PCG method,
when applied to the preconditioned system, converges super linearly.
The first step of the proof on the clustered spectra of 5~1Tn is to show that Sn and
5"1 are uniformly bounded in ^-norm for a subclass of generating functions [Cha89a].
Theorem 5.3.2 (Uniform Boundedness of Sn and -S"1). Let f be a positive real-
valued function in the Wiener class; i.e., its Fourier coefficients are absolutely summable,
Then, for large n, the circulants Sn and S~l are uniformly bounded in ii-norm.
Proof: By (5.3.11), the jth eigenvalue of Sn is equal to
where ra = [n/2j. Since, for 9 â¬ [âTT, TT], the infinite series X)fcL-oo^fcelfc& 1S absolutely
convergent, then for any given e > 0, there exists N such that for n > N,
We note that

126 
Iterative Methods for Linear Systems with Matrix Structure 
Chapter 5
Since / is positive, fm-m > 6 > 0. The result follows.
The second step of the proof is to establish the clustering of the spectra of Sn â Tn
[Cha89a].
Theorem 5.3.3 (Clustered Spectra). Let f be a real-valued function in the Wiener
class. Let {Tn} be the sequence of Hermitian Toeplitz matrices generated by f . Then
the spectra of Sn â Tn are clustered around zero for large n.
Proof: For simplicity, we assume here and in the following that n is odd, i.e., n = 2ra-fl
for some m. The case where n = 2m can be treated similarly.
Clearly Ln = Sn â Tn is a Hermitian Toeplitz matrix with entries lij = li-j given
by
Since / is in the Wiener class, for all given e > 0, there exists an N > 0 such that
]CfcLw+i Kfcl < e- Let Un be the n x n matrix obtained from Ln by replacing the
(n â N) x (n â N) leading principal submatrix of Ln by the zero matrix. Therefore,
the entries of Un are zero except at its last N columns and last TV rows. Hence
rank(t/1lAr)) < 2N.
LetWn
N) = Mn-Un
N). The leading (n-N) x(n-N) block of W^N) is the leading
(n â N) x (n â N) principal submatrix of Ln\ hence this block is a Toeplitz matrix, and
it is easy to see that the maximum absolute column sum of Wn is attained at the first
column (or the (n â N â l)th column) of Wn . Thus
Since W^N) is Hermitian, we have HW^H^ = HW^Hi. Thus
p. 192] (see also App. A), we see that at most IN eigenvalues of Ln = Sn â Tn have
absolute values exceeding e.
Hence the spectrum of Wn lies in (âe, e). By the Cauchy interlace theorem [ParSO,

Section 5.3. Iterative Methods for Solving Toeplitz Systems 
127
Using the fact that S^"1!^ is similar to Sn TnSn 
along 
with 
the 
result 
of
Thm. 5.3.2 and theequality
we conclude that the spectra of S~lTn are clustered around 1 for large n.
It follows easily from Thms. 5.3.2 and 5.3.3 that the CG method, when applied to
solving the preconditioned system
converges superlinearly for large n in the sense specified below (see [CS89]).
Corollary 5.3.1 (Superlinear Convergence). Let f be a positive real-valued func-
tion in the Wiener class. Let {Tn} be the sequence of Toeplitz matrices generated by f .
Then for any given e > 0, there exists a constant c(e) > 0 such that the error vector ek
of the PCG method at the kth iteration satisfies
where x is the true solution of the linear system of equations Tnx = b and x^ is the kth
iterate of the PCG method. Moreover, the notation ||| â¢ ||| stands for the weighted norm
If extra smoothness conditions are imposed on the generating function / (or on the
sequence {Â£&}), we can get more precise estimates on how \\\x â Xk\\\ in (5.3.15) goes to
zero [Cha89a].
Theorem 5.3.4 (Smooth Generating Functions). Let f be an (Â£ + l)-times 
differ-
entiable real-valued function with /^+1^ Â£ Z/
1[âTT, TT], where Â£ > 0 (i.e., \tj\ < c/jÂ£+1 for
some constant c and therefore f is in the Wiener class). Then there exists a constant c
which depends only on f and v, such that for large n,
Proof: See App. 5.A.
Other precise estimates on how |||z â o:fc||| in (5.3.15) goes to zero under different
smoothness conditions can be found in [Tre90], [KK93b], [KK93c], [CY92].
T. Chan's Preconditioner
For an n x n Toeplitz matrix Tn, T. Chan's circulant preconditioner C(Tn) is defined
as the minimizer of
over all n x n circulant matrices Cn [Cha88]. Here || â¢ ||F denotes the Frobenius norm.
In [Cha88], the matrix C(Tn] is called an optimal circulant preconditioner because it
minimizes (5.3.17).

0
128 
Iterative Methods for Linear Systems with Matrix Structure 
Chapter E
Theorem 5.3.5 (T. Chan's Preconditioner). The jth diagonals ofC(Tn] are giver
by
which are simply the average of the diagonals ofTn, with the diagonals being extendec
to length n by a wrap-around procedure.
Proof: Since the Probenius norm is a unitary-invariant norm, the minimizer of \\Cn â
Tn\\p over all Cn of the form C = F*AF, A a diagonal matrix, is attained at FnAnF*.
Here An is a diagonal matrix with diagonal entries
It is now immediate to verify that the entries of C(Tn) are given by (5.3.18).
0
By using (5.3.11) and (5.3.18), we further see that the eigenvalues \k(C(Tn)) ol
C(Tn] in (5.3.18) are given by
For the performance of C(Tn) as preconditioners for Hermitian Toeplitz matrices
Tn, it is shown in [Cha89b] that under the Wiener class assumptions of Thm. 5.3.3
(i.e., / is a positive function with absolutely summable Fourier coefficients), the spectre
of C(Tn) â Tn and Sn â Tn are asymptotically the same as n tends to infinity, i.e.
limn^00 ||C(Tn) â Sn\\2 = 0. Hence, C(Tn} works as well for Wiener class functions as
Sn does.
Theorem 5.3.6 (Performance of T. Chan's Preconditioner). Let f be a real-
valued function in the Wiener class. Let {Tn} be the sequence of Toeplitz matrices
generated by f . Then
where p(-} denotes the spectral radius.
Proof: By (5.3.13) and (5.3.18), it is clear that Ln = Sn - C(Tn] is circulant with
entries
Here, for simplicity, we are still assuming n = 2m. Using the fact that the jth eigenvalue
Xj(Ln] of Ln is given by YZ=l bke2*ljk/n, we have

Section 5.3. Iterative Methods for Solving Toeplitz Systems 
129
This implies
Since / is in the Wiener class, for all e > 0, we can always find an MI > 0 and an
MI > MI such that
Thus for all ra > M2,
For an n x n general non-Toeplitz matrix An, the circulant minimizer C(An] of
\\Cn â An\\p still can be defined and obtained easily by taking the arithmetic average
of the entries of An, i.e., its diagonals are given by [Tyr92b]
Therefore, T. Chan's preconditioner is particularly useful in solving non-Toeplitz sys-
tems arising from the numerical solutions of elliptic partial differential equations [CC92]
and Toeplitz least-squares problems arising from signal and image processing [CNP94a],
[CNP93], [CNP94b], [CO94], [NP96]. Convergence results for T. Chan's preconditioner
have been established for these problems (see [CN96]). One good property of T. Chan's
preconditioner is that it preserves the positive definiteness of a given matrix [Tyr92b],
[CJY91a].
Theorem 5.3.7 (Positive-Definiteness Property). IfAn is Hermitian positive def-
inite, then C(An) is Hermitian and positive definite. Moreover, we have
Proof: By (5.3.20), it is clear that C(An] is Hermitian when An is Hermitian. Moreover,
since the Probenius norm is a unitary-invariant norm, we conclude as before that the
minimizer of \\Cn â An\\p is attained at FnAnF^. Here An is a diagonal matrix with
diagonal entries
Suppose that \j = Amin(C(An)) and \k = Amax(C(An)). Let &j and 6k denote the jih
and the fcth unit vectors, respectively. Since An is Hermitian, we have

130 
Iterative Methods for Linear Systems with Matrix Structure 
Chapter 5
Similarly,
Prom the inequality above, we can easily see that C(An) is positive definite when An is
positive definite.
5.4 
BAND-TOEPLITZ PRECONDITIONERS
In this section we consider Hermitian Toeplitz matrices Tn that are generated by non-
negative 27r-periodic real-valued functions. We first recall that a function / is said to
have a z/th-order zero at OQ if /(#o) = 0 and v is the smallest positive integer such that
/^(^o) 7^ 0 and f("+l\0) is continuous in a neighborhood of OQ. With the knowledge
of the order of / at its minimum, we can give a better estimate of the spectrum of Tn
than that in (5.3.4) [Cha91],
Theorem 5.4.1 (Spectrum of Tn). Suppose that f ( 0 ) â /mjn has a unique zero of
order 1v on [âTT, TT], Let {Tn} be the sequence of Hermitian Toeplitz matrices generated
by f . Then for all n > 0, we have
and the condition number ft(Tn) ofTn 
satisfies
where {di}*=l are some constants independent ofn.
Thus when /min = 0, the condition number of Tn is not uniformly bounded and the
Toeplitz matrix Tn is ill-conditioned. It was shown in [Tyr95] that Strang's precondi-
tioner will fail in this case. More specifically, if / is such that its /ith derivative /^
is piecewise continuous and has a bounded derivative on each continuity interval, the
number of outlying eigenvalues of 5~xTn is of O(nv^vJrl^). Here v is the order of / at
the zero.
Instead of finding other possible circulant preconditioners, it was suggested in [Cha91]
to use band-Toeplitz matrices as preconditioners. The motivation behind using band-
Toeplitz matrices is to approximate the generating function / by trigonometric polyno-
mials of fixed degree rather than by convolution products of / with some kernels. The
advantage here is that trigonometric polynomials can be chosen to match the zeros of
/, so that the preconditioned method still works when / has zeros [Cha91], [CN93a].
Theorem 5.4.2 (Band-Toeplitz Preconditioner). Let f be a nonnegative piecewise
continuous real-valued function defined on [â7r,7r]. Suppose that f(0)â /mjn has a unique

Section 5.4. Band-Toeplitz Preconditioners 
131
zero of order 1v at 0 = OQ. Let {Tn} be the sequence of Toeplitz matrices generated by
f . Let {En} be the sequence of Toeplitz matrices generated by the function
Then K,(E~lTn} is uniformly bounded for all n > 0.
Proof: We can assume without loss of generality that OQ = 0. Let Gn be generated by
f ( 0 + OQ}. The function f ( 9 + OQ) â /min has a zero at 9 = 0 and
where Vn = diag(l, e~wÂ°, e~2WÂ°,..., e-*(n-i)*o) (see [Cha89b, Lemma 2]).
By assumption, there exists a neighborhood AT of 0 such that / is continuous in N.
Define
Clearly F is continuous and positive for 0 â¬ N \ {0}. Since
is positive, F is a continuous positive function in N. Since / is piecewise continuous
and positive almost everywhere in [âTT, TT] \ N, we see that F is a piecewise continuous
function with a positive essential infimum in [âTT, TT] . Hence there exist constants 61,62 >
0 such that 61 < F(0) < 62 almost everywhere in [â7r,7r]. Without loss of generality,
we assume that 62 > 1 > &i â¢ Then we have
for any n-vector u. Hence K,(E~^Tn) < b%/bi, which is independent of n.
We note that En is a band-Toeplitz matrix with bandwidth 1v + 1 and its diagonals
can be obtained by using Pascal's triangle. We remark that
Hence by the binomial theorem,
where
are the binomial coefficients of (-1)"(1 - ei9)2". Hence the diagonals of En can be
obtained easily from the Pascal triangle.

132 
Iterative Methods for Linear Systems with Matrix Structure 
Chapter 5
The band system Eny = z can be solved by using any band matrix solver (see, e.g.,
[GV96], [Wri91]). The cost of factorizing En is about \v2n operations, and then each
subsequent solve requires an extra (2f -f l)n operations. Hence the total number of
operations per iteration is of O(nlogn) as v is independent of n.
When /min = 0, the band preconditioner improves the condition number from
K,(S~lTn) â O(n2t/) to K,(E~lTn} = O(l). Since the number of iterations required
to attain a given tolerance c is bounded by [AB84, p. 26]
the overall work required to attain the given tolerance is reduced from O(nl/+1 log n) to
O(nlogn) operations. As for the storage, we just need an n x (2i/ + 1) matrix to hold
the factors of the preconditioner En. Thus, the overall storage requirement in the CG
method is about (8 + v)n. Finally, we remark that similar results hold when there are
multiple points on [â7r,7r] where / takes on its minimum value [ChaQl].
5.5 
TOEPLITZ-CIRCULANT PRECONDITIONERS
The main idea behind Thm. 5.4.2 is to approximate the given nonnegative generating
function / by trigonometric polynomials that match the zeros of /. Clearly, any function
g that matches the zeros of / and gives rise to Toeplitz matrices that are easily invertible
can be considered too. This idea is exploited in [DiB95], [DFS93], [CT94], [CC96b].
In [CC96b], products of circulant matrices and band-Toeplitz matrices are considered
as preconditioners for Toeplitz systems generated by nonnegative functions. The band-
Toeplitz part of these Toeplitz-circulant preconditioners is to match the zeros of the given
function, and the circulant part is to speed up the convergence rate of the algorithm.
Instead of using powers of 2 â 2cos# as in (5.7.1) to generate the band-Toeplitz part
of the preconditioner, reference [CC96b] considers using powers of 1 â e10. This results
in preconditioners that can handle complex-valued generating functions with zeros of
arbitrary order.
Theorem 5.5.1 (Toeplitz-Circulant Preconditioner). Suppose that f ( z ) is of the
form
where Zj are the roots of f ( z ) on \z\ = 1 with order tj and h(z) is a nonvanishing func-
tion on \z\ = 1. Let {Tn}, {En}, and {Gn} be sequences of Toeplitz matrices generated
by f , Ylj(z â ZjYj, and h, respectively. Then the sequence of matrices C(Gn}E~lTn
has singular values clustered around 1 for all sufficiently 
large n.
Proof: By expanding the product Ylj(z ~ zjYj we see *^a* *^e Toeplitz matrix Ens a
lower triangular matrix with bandwidth equal to (t +1), where i = ]TV ij. Moreover, its
main diagonal entry is 1 and therefore it is invertible for all n. We see that the matrix
Tn â EnGn only has nonzero entries in the first / 4-1 rows. Hence it is clear that

Section 5.6. 
Preconditioners for Structured Linear Systems 
133
where rank LI < t + 1. Therefore,
where rank Â£2 < i.
Since h(z) has no zeros on \z\ = 1, the matrices C(Gn}~lGn have clustered singular
values. In particular, we can write C(Gn)~lGn = I +1/3 + Â£/, where U is a small norm
matrix and rank Â£3 is fixed independent of n. Hence (5.5.1) becomes
where the rank of Â£4 is again fixed independent of n. By using the Cauchy interlace
theorem [ParSO, p. 192] (see App. A) on
it is straightforward to show that E~1C(Gn)~1Tn has singular values clustered around
1.
In each iteration of the PCG method, we have to solve a linear system of the form
EnC(Gn}y = r. We first claim that EnC(Gn) is invertible for large n. As mentioned
above, the Toeplitz matrix En is invertible for all n. Since h is a Wiener class function
and has no zeros on \z\ = 1, the invertibility of C(Gn] for large n is guaranteed by
Thm. 5.3.7. Hence C(Gn}En is invertible for large n. Let us consider the cost of solving
the system
As the matrix En is a lower triangular matrix with bandwidth (Â£ + 1), the system
involving En can be solved by forward substitution and the cost is O(ln) operations.
Given any vector x, the matrix-vector product C(Gn)x can be done by using FFTs in
O(nlogn) operations. Thus the system C(Gn}Eny = r can be solved in O(nlogn) -f
O(in] operations.
5.6 
PRECONDITIONERS FOR STRUCTURED LINEAR
SYSTEMS
We now study preconditioners for more general kinds of matrix structure.
5.6.1 
Toeplitz-Like Systems
Let Zn denote the n x n lower shift matrix whose entries are zero everywhere except for
the 1's on the first subdiagonal, i.e.,
Let An be an n x n structured matrix with respect to Zn (cf. definition (1.2.4) in Ch. 1),
say,

134 
Iterative Methods for Linear Systems with Matrix Structure 
Chapter 5
for some n x r generator matrix G and r x r signature matrix J = (Ip Â© â Iq}. If we
partition the columns of G into two sets {xi}?~0 and {yi}1=Q,
then we know from the representation (1-4.2) that we can express An as a linear com-
bination of lower triangular Toeplitz matrices,
Here, the subscript n in Cn denotes the size (n x n) of the matrix.
For example, if Tm^n is an m x n Toeplitz matrix with m > n, then T^ nT"min is
in general not a Toeplitz matrix. However, T^ nTm)Tl does have a small displacement
rank, r < 4, and a displacement representation for T^ nTm>n is
where
For structured matrices as in (5.6.1), it was suggested in [CNP94b], [Huc94] to define
the displacement preconditioner to be the circulant approximation of the factors in the
displacement representation of An; i.e., the circulant approximation C of An is
Here, C(Xn) denotes the optimal n x n circu.ant approximation to Xn in the Probenius
norm; see (5.3.18). In the following, we assume that the generating function / of Tm)n
is in the Wiener class; i.e., the diagonals of T^^ are absolutely summable:
For the case of T^ nTmjn, it can be verfied that we can write
where T is a Hermitian Toeplitz matrix with

Section 5.6. Preconditioners for Structured Linear Systems 
135
Using this representation, it was proposed in [CNP94b] to define the displacement pre-
conditioner for T^ nTm>n to be Pn:
In the following, we show that Pn is a good preconditioner.
For simplicity, we will denote by Ui Hermitian matrices with small rank and by
Vi Hermitian matrices with small norm. More precisely, given any e > 0, there exist
integers N and M > 0 such that when n, the size of the matrices Ui and K, is larger
than N, the rank of Ui is bounded by M and \\Vi\\2 < e-
Lemma 5.6.1 (A Decomposition Result). It holds that
Proof: Since the sequence {tj}^=_00 is absolutely summable, for any given e, we can
choose N > 0 such that
Partition Cn(yi) as RN + WN, where the first N columns of RN are the first N columns
of Cn(yi) with the remaining columns zero vectors. Then RN is a matrix of rank N
and
Thus
where
and
We note from Lemma 5.6.1 that it suffices to show that the matrix
Pn-T^nTm,n = {C(T)-T}+{C(Ai(xi))C*(Â£B(x1))-Â£n(xi)Â£;(xi)}+Â£n(yi)Â£;(yi)
is the sum of a matrix of low rank and a matrix of small norm [CNP94b].
Theorem 5.6.1 (Displacement Preconditioner). Let f be a function in the Wiener
class. Then the spectra of Pn â T^jnTm)n are clustered around zero for large n.
Proof: The generating function of Tmjn is in the Wiener class; therefore, the generating
function of T is also in the Wiener class. In fact,
According to Thm. 5.3.3, we have

136 
Iterative Methods for Linear Systems with Matrix Structure 
Chapter 5
Next we show that
The generating function of Cn(x\) is given by
which is a function in the Wiener class. Equation (5.6.8) now follows by Lemma 5 of
[CNP94a] or using the arguments in Thm. 5.3.3. Combining (5.6.7), (5.6.8), and (5.6.5),
we see that
is the sum of a matrix of low rank and a matrix of small norm.
In order to show that H-P^^h is uniformly bounded, we need the following two
lemmas. These two lemmas state some properties of the optimal circulant preconditioner
for non-Hermitian Toeplitz matrices. Their proofs can be found in [CY93] and [CJY91b],
respectively.
Lemma 5.6.2 (Norm Bounds for the Optimal Preconditioner). Let f â¬ C^.
Let {Tn} be a sequence of Toeplitz matrices generated by f . Then we have
Moreover, if f has no zeros, i.e.,
then, for all sufficiently 
large n, we also have
Lemma 5.6.3 (A Limit Result). Let {Tn} be a sequence of Toeplitz matrices with
generating function in the Wiener class. Then
We now are ready to show that IIP^1!^ is uniformly bounded [CNP94b].
Theorem 5.6.2 (Uniform Boundedness of P~*)- Let the generating function f of
Tm,n be a Wiener class function that satisfies
Then \\Pn\\2 < &Y2 for all n and H-P^1!^ is uniformly bounded for n sufficiently 
large.

Section 5.6. 
Preconditioners for Structured Linear Systems 
137
Proof: See App. 5.B.
0
By combining the above results we can show that the spectra of the preconditioned
matrices P~1T^ Tm)n are clustered around 1. Thus the CG method, when applied
to solving the preconditioned system, converges superlinearly. Numerical experiments
in [CNP94b] show the effectiveness of the preconditioners for Toeplitz least-squares
problems.
5.6.2aToeplitz-Plus-Hankel Systems
The systems of linear equations with Toeplitz-plus-Hankel coefficient matrices arise in
many signal processing applications. For example, the inverse scattering problem can
be formulated as Toeplitz-plus-Hankel systems of equations [GL55]. A Hankel matrix
is one with constant elements along its antidiagonals, say, [Hn]ij = hn+i-i-j,
Let Jn be an n x n matrix with 1 's along the secondary diagonal and zeros elsewhere,
i.e.,
It is easy to see that the product of Jn and Hn and the product of Hn and Jn both give
Toeplitz matrices:
or
(We remark that premultiplying Jn to a vector v corresponds to reversing the order of
the elements in v.) Since

138 
Iterative Methods for Linear Systems with Matrix Structure 
Chapter 5
and Hn Jn is a Toeplitz matrix, the Hankel matrix-vector products Hnv can be done in
O(nlogn) using FFTs as shown in as in Sec. 5.3.3.
A Toeplitz-plus-Hankel matrix can be expressed as An = Tn + Hn =Tn + JnJnHn.
Given circulant preconditioners Cn and Cn ' for Toeplitz matrices Tn and JnHn, re-
spectively, it was proposed in [KK93a] to use
as a preconditioner for the Toeplitz-plus-Hankel matrix Tn + Hn.
With the equality Jn = In and (5.6.12), we have
which is equivalent to
Since Cn and Cn are circulant,
/1 \fj~> 
(
r?\
rr
>
By multiplying (5.6.13) with Cn 
and (5.6.14) with Cn 
, we can write the difference
between the two resulting equations as
The solution of z = Pn~1v can also be determined from (5.6.15). We note that the matrix
Cn 
Cn âCn 
Cn is circulant and therefore that P^lv can be found efficiently via
FFT with O(nlogn) operations.
For Toeplitz matrices Tn and JnHn generated by rational functions, it was shown
in [KK93a] that the spectra of the preconditioned Toeplitz-plus-Hankel matrices are
clustered around 1.
Theorem 5.6.3 (Preconditioned Toeplitz-plus-Hankel Matrices). Let Tn + Hn
be a real symmetric n x n matrix. Let the generating functions ofTn and JnHn be
and
with ai^bi^Ci^di^ ^ 0, bijQ = 1, di)0 = 1, polynomials fi,a(Q} and fi,b(&), and fi,c(0)
and fi,d(6) have no common zeros. If
where // is a constant independent ofn, then the spectra of P~1(Tn + Hn] are clustered
around I and the number of outliers is bounded by
where j]c is the number of common zeros in fi 5/1 d and fa 6/2 d-

Section 5.7. 
Toeplitz-Plus-Band Systems 
139
Using Thm. 5.3.3, the following result can be established.
Theorem 5.6.4. Let Tn -f Hn be a real symmetric n x n matrix. Let the generating
functions ofTn and JnHn be fi and /2, respectively. Let fi and f% be functions in the
Wiener class. If
where ^ is a constant independent ofn, then the spectra of P~l(Tn + Hn) are clustered
around 1.
Proof: Recall from (5.6.15) that Pnz = v is equivalent to
For any given n, \\Cn ||i and \\Cn ||oo are both bounded. As a consequence, \\Cn \\2
("2\T
is also bounded. Similar results hold for Cn 
â¢ Thus the right-hand sides of (5.6.15)
are bounded. Under the assumption, the magnitude of any eigenvalue of (Cn 
Cn â
(e2\rr 
/o\
Cn 
Cn ) is also bounded. Therefore, \\P~ \\2 is bounded and the preconditioner P is
invertible.
According to the definitions, the difference matrix Pn â Tn â Hn can be written as
By Theorem 5.3.3, the spectra of the matrices Cn â Tn and and Cn â JnHn are
clustered around zero for large n. Hence the result follows.
0
With the above spectral properties of preconditioned Toeplitz-plus-Hankel matrices,
various preconditioned iterative methods including GMRES and CGS [Saa96] can be
applied effectively. These Toeplitz-plus-Hankel systems can be solved in a finite number
of iterations independent of n so that total operations required are O(nlogn).
5.7 
TOEPLITZ-PLUS-BAND SYSTEMS
In this subsection, we consider the solution of systems of the form (Tn + Bn}x = 6, where
Tn is an n x n Hermitian Toeplitz matrix and Bn is an n x n Hermitian band matrix
with bandwidth 26+1 independent of n. These systems appear in solving Fredholm
integrodifferential equations of the form
Here x(0) is the unknown function to be found, K(9) is a convolution kernel, L is a
differential operator, and 6(0) is a given function. After discretization, K will lead to
a Toeplitz matrix, L to a band matrix, and 6(0) to the right-hand side vector [DM85,
p. 343]. Toeplitz-plus-band matrices also appear in signal processing literature and have
been referred to as peripheral innovation matrices [CKM82].
Unlike Toeplitz systems, there exist no fast direct solvers for solving Toeplitz-plus-
hanrl svsfpms 
Tt. is rnainlv hpr-ansp -hVip rHsnlapprnpnt. rank nf t.hp matrix TL -I- 7?-

140 
Iterative Methods for Linear Systems with Matrix Structure 
Chapter 5
can take any value between 0 and n. Hence, fast Toeplitz solvers that are based on
small displacement rank of the matrices cannot be applied. CG methods with circulant
preconditioners do not work for Toeplitz-plus-band systems either. In fact, Strang's cir-
culant preconditioner is not even denned for non-Toeplitz matrices. T. Chan's circulant
preconditioner, while denned for Tn + Bn, does not work well when the eigenvalues of
Bn are not clustered; see [CN93a]. Also, the matrix C(Tn] + Bn cannot be used as a
preconditioner for it cannot be inverted easily.
In [CN93a] it was proposed to use the matrix En + Bn to precondition Tn + Bn,
where En is the band-Toeplitz preconditioner.
Theorem 5.7.1 (Toeplitz-plus-Band Preconditioner). 
Let f be a nonnegative
piecewise continuous real-valued function defined on [âTT, TT]. Let Tn be generated by
f . Suppose that f(Q] â /min has a unique zero of order 2v at 6 = OQ. Let {En} be the
sequence of Toeplitz matrices generated by the function
Then K((En + Bn)~l(Tn + Bn)) is uniformly bounded for all n > 0.
Note that En is a band matrix with bandwidth 2i/ -f 1 and its diagonals can be
obtained by using Pascal's triangle. The band system (En + Bn)y = z can be solved by
using any band matrix solver (see, e.g., [GV96], [Wri91]). Let
The cost of factorizing En + Bn is about ^72n operations, and then each subsequent
solve requires an extra (27 + l)n operations. Hence, the total number of operations per
iteration is of O(n log n) as v is independent of n. The number of iterations required to
attain a given tolerance e is bounded by
see, for instance, [AB84, p. 26]. The overall work required to attain the given tolerance
is reduced from O(nl/+1 logn) to O(nlogn) operations. As for the storage, we just need
an n x (27 + 1) matrix to hold the factors of the preconditioner En + Bn. Thus, the
overall storage requirement in the CG method is about (8 + 7)71.
5.8 APPLICATIONS
We now provide a brief overview of several examples where structured matrices of the
form studied earlier in this chapter arise.
5.8.1 
Linear-Phase Filtering
Finite impulse response linear-phase niters are commonly used in signal processing.
Such filters are especially important for applications where frequency dispersion due
to nonlinear phase is harmful, such as in speech processing. In this case, the impulse
responses of the niters can be found by solving a Toeplitz-plus-Hankel least-squares
problem of the form

Section 5.8. Applications 
141
where T+H is a rectangular Toeplitz-plus-Hankel matrix; see [MarSO], [MarSl], [Mar82],
[Yag91], [HY93]. The coefficient matrix of the corresponding normal equations can be
written as
where Tn is a Toeplitz matrix, H is a Hankel matrix, and {Vn }|=i are non-Toeplitz
and non-Hankel matrices. In [Ng94], the optimal circulant preconditioner C(Tn) is used
as a preconditioner for the problem.
This preconditioner is different from that proposed in [KK93a] for Toeplitz-plus-
Hankel systems and which we studied earlier. The preconditioner in [KK93a] basically
corresponds to combining the circulant approximations of the Toeplitz and Hankel ma-
trices. Also, under the assumptions in [KK93a], the spectrum of the Hankel matrix is
not clustered around zero.
The motivation behind the preconditioner C(Tn) suggested above is that the Toeplitz
matrix Tn is a sample autocorrelation matrix, which intuitively should be a good esti-
mate of the autocorrelation matrix of the corresponding discrete-time stationary process,
provided that a sufficiently large number of data samples is used. Moreover, under prac-
tical signal processing assumptions, the spectrum of the Hankel matrix Hn is clustered
around zero. Hence it suffices to approximate Tn by a circulant preconditioner.
To prove convergence, the following practical assumptions on the random process
are made [Ng94] (for instance, an autoregressive (AR) progress):
â¢ The process is stationary with constant mean fj,.
â¢ The spectral density function of the process is positive and in the Wiener class.
â¢ There exist positive constants /?i and fa such that
and
Here, the notation Var(-) denotes the variance of the random variable. We note that the
positiveness of the spectral density function can be guaranteed by the causality of the
process [BD91, p. 85], whereas the absolute summability of the autocovariances can be
assured by the invertibility of the process [BD91, p. 86]. With these assumptions, it can
be shown that the spectra of the preconditioned matrices C(fn}~l((T + H}*(T + H))
are clustered around 1 with probability 1, provided that a sufficiently large number of
data samples is taken [Ng94].
Theorem 5.8.1 (Spectra for Preconditioned Toeplitz-plus-Hankel). 
Let the
discrete-time process satisfy the above assumptions. Then for any given e > 0 and
0 < rj < I, there exist positive integers p\ and p% such that for n > p\, the probability
that at most p2 eigenvalues of the matrix I â c(fn)~l((T + H)*(T + H)) have absolute
value greater than e is greater than 1 â rj, provided that m = O(n3~l~I/) with v > 0.

142 
Iterative Methods for Linear Systems with Matrix Structure 
Chapter 5
Hence, when we apply the CG method to the preconditioned system, the method
converges super linearly with probability 1. Since the data matrix T + H is an ra x n
rectangular matrix, the normal equations and the circulant preconditioner can be formed
in O(mlogn) operations. Once they are formed, the cost per iteration of the PCG
method is O(nlogn) operations. Therefore, the total work for obtaining the filter
coefficients to a given accuracy is of O((ra + n) logn).
5.8.2Numerical Solutions of Biharmonic Equations
Boundary value problems for the biharmonic equation in two dimensions arise in the
computation of the Airy stress function for plane stress problems [Mus53] and in steady
Stokes flow of highly viscous fluids [Mil68, Ch. 22]. The integral equations method is a
popular choice for the numerical solution of these equations [GGM92]. The application
of conformal mapping to this problem, although classical, is less well known [Mus53].
Unlike the Laplace equation, the biharmonic equation is not preserved under confor-
mal transplantation. However, a biharmonic function and its boundary values can be
represented in terms of the analytic Goursat functions, and this representation can be
transplanted with a conformal map to a computational region, such as a disk, an ellipse,
or an annulus, where the boundary value problem can be solved more easily.
We wish to find a function u = U(TI, /j.) that satisfies the biharmonic equation
for Â£ = 77 + ifj, â¬ Â£1, where Q is a region with a smooth boundary F and u satisfies the
boundary conditions
on F. The solution u can be represented as
where 0(Â£) and x(C) are analytic functions in J7, known as the Goursat functions.
Letting G = G\ + iG%, the boundary conditions become
where V>(0 = x'(0- The problem is to find </> and t/> satisfying (5.8.1).
Let C = f ( z ) be the conformal map from the unit disc to S7, fixing /(O) = 0 e O.
Then with
(5.8.1) transplants to the unit disc as
Let
The problem now is to find the ajfc's and the fr^'s. For \z\ = 1, define the Fourier series

section 5.8.
Substituting (5.8.3) into (5.8.2) gives a linear system of equations for the a^'s and fr^'s,
If (5.8.4) is solved for the a/t's, then the 6jt's can be computed easily from (5.8.5). These
systems are derived in [Mus53j. We note that the coefficient matrix of the (infinite)
linear system in (5.8.4) is of the form / + HD, where / is the identity matrix, D is
a diagonal matrix, and H is a Hankel matrix. We remark that HD actually can be
represented as a compact operator with a one-dimensional null space; see [CDH97].
In [CDH97], the linear system in (5.8.4) was truncated after n terms and solved
efficiently using the CG method (up to the null vector). To solve the discrete system, the
major computations that are required at each iteration are the matrix-vector products
Hnv for arbitrary vectors v. We note that the Hankel matrix-vector products Hnv can
be done in O(nlogn) using FFTs.
For the convergence rate of the CG method, estimates are given in [CDH98] for the
decay rates of the eigenvalues of the compact operators when the boundary curve is
analytic or in a Holder class. These estimates are used to give detailed bounds for the
r-superlinear convergence which do not depend on the right-hand side. It follows that
the CG method, when applied to solve this kind of Hankel system, converges sufficiently
fast [CDH98].
Theorem 5.8.2. Assume that the coefficient matrix AOQ in the infinite linear system
(5.8.4) is positive semidefinite with exactly one null vector v. Let
Then, for large n, the error vector x â xq at the qth step of the CG method applied to
t/n)~L satisfies the following estimates.
(i) 7/F is analytic, there is an r, 0 < r < 1, such that
(ii) 7/r is of class Cl+1'a, l>2,Q<a<l, then
Here C is a constant that depends on the conformal map and the notation \\\ â¢ \\\ stands
for
The proof follows closely that of Thm. 5.3.4. In case A^ is not semidefinite we can
solve the normal equations by the CG method. It is clear that (/â + An)2 will then be
positive definite on t/â¢)-1. A similar result for the normal equations can be established.
For a discussion of the numerical conformal mapping methods used, see [CDH97].
applications
143

144 
Iterative Methods for Linear Systems with Matrix Structure 
Chapter 5
5.8.3 
Queueing Networks with Batch Arrivals
In this subsection, we consider using the PCG method with the Toeplitz-circulant pre-
conditioners of Sec. 5.5 for solving the stationary probability distribution vectors for
Markovian queueing models with batch arrivals. This kind of queueing system occurs
in many applications, such as telecommunication networks [Oda91] and loading dock
models [SeiQO]. We will see that the generator matrices of these systems have a near-
Toeplitz structure and preconditioners are constructed by exploiting this fact.
Let us first introduce the following queueing parameters. (Definitions of queueing
theory terminologies used below can be found in [Coo72].) The input of the queueing
system will be an exogenous Poisson batch arrival process with mean batch interarrival
time A"1. For k > 1, denote A& to be the batch arrival rate for batches with size k. We
note that
where pk is the probability that the arrival batch size is k. Clearly we have
The number of servers in the queueing system will be denoted by s. The service time
of each server is independent of the others and is exponentially distributed with mean
fjT1. 
The waiting room is of size (n â s â 1) and the queueing discipline is blocked
customers cleared. If the arrival batch size is larger than the number of waiting places
left, then only part of the arrival batch will be accepted; the other customers will be
treated as overflows and will be cleared from the system.
By ordering the state-space lexicographically, i.e., the ith variable corresponds to
the state where there are (i â 1) customers in the system, the queueing model can be
characterized by the infinitesimal generator matrix
where Ti are such that each column sum of An is zero [SeiQO].
Clearly An has zero column sum, positive diagonal entries, and nonpositive off di-
agonal entries. Moreover, the matrix An is irreducible. In fact, if AÂ» = 0 for all
i = 1,... ,n â 2, then r\ = A and the matrix is irreducible. If the A^'s are not all
zero, say, Xj is the first nonzero A;, then rnâj = A, and hence An is also irreducible.
From Perron and Frobenius theory [Var63, p. 30], An has a one-dimensional nullspace
with a positive null vector.
The stationary probability distribution vector p of the queueing system is the nor-
malized null vector of the generator matrix An given above. Much useful information
about the queueing system, such as the blocking probability and the expected waiting
time of customers, can be obtained from p. Since An has a one-dimensional nullspace,

Section 5.8. Applications 
145
p can be found by deleting the last column and the last row of An and solving the
(n â 1) x (n - 1) reduced linear system Qn-iV = (0,..., 0, S)U)T. After obtaining y, the
distribution vector p can then be obtained by normalizing the vector (t/T, 1)T.
Thus let us concentrate on solving nonhomogeneous systems of the form
where
Notice that if all of the A$, i = 1,... ,n â 1, are zeros, then Qn will be a bidiagonal
matrix and can be inverted easily. Therefore, in the following, we assume that at least
one of the A^ is nonzero. Then clearly, Q% is an irreducibly diagonally dominant matrix.
In particular, if the system (5.8.8) is solved by classical iterative methods such as the
Jacobi or the Gauss-Seidel methods, both methods will converge for arbitrary initial
guesses; see for instance [Var63, Thm. 3.4].
We see that the costs per iteration of the Jacobi and the Gauss-Seidel methods are
O(nlogn) and O(n2), respectively. The memory requirement is O(ri) for both methods.
We remark that the system (5.8.8) can also be solved by Gaussian elimination in 0(n2)
operations with O(n2) memory. In the remainder of this subsection, we are interested
in solving (5.8.8) by the PCG method. We will see that the cost per iteration of the
method is O(nlogn) and the memory requirement is 0(n), the same as those of the
Jacobi method.
However, we are able to show that if s is independent of n, then with the Toeplitz-
circulant preconditioner, the PCG method converges superlinearly for all sufficiently
large n. In particular, the method converges in a finite number of steps independent
of the queue size n. Therefore, the total cost of finding the steady-state probability
distribution is O(nlogn) operations.
We observe that in the single server case, i.e., when s = 1, the matrix Qn given above
differs from a lower Hessenberg Toeplitz matrix by only its (1,1) entry. In general, Qn
can be written as
where Tn is a Toeplitz matrix

146 
Iterative Methods for Linear Systems with Matrix Structure 
Chapter 5
and Rn is a matrix of rank s.
From the above expression for Tn, we see that Tn is generated by g(z) given by
with z = e10. We note that by (5.8.7), g(z) belongs to the Wiener class of functions
defined on the unit circle \z\ â 1. Unfortunately, it is also clear from (5.8.11) and (5.8.7)
that g(z) has a zero at z = 1. If we examine the real part of g(z) on the unit circle
\z\ = 1, we see that
Hence the zeros of g(z) can only occur at z = 1. In particular, we can write
where t is the order of the zero of g(z] at z = 1 and b(z) will have no zeros on the unit
circle.
Using the idea developed in Sec. 5.5, we define the preconditioner for Qn as
where En and Gn are the Toeplitz matrices generated by (z â I)1 and &(z), respectively,
and C(Gn) is the optimal circulant approximation of Gn.
Let us consider cases where the quotient function b(z) is a Wiener class function
and b(z) has no zeros on \z\ = 1. We note first that if the radius of convergence p
of the power series Y^Li AfcÂ£
fc in (5.8.11) is greater than 1, then g(z) and hence b(z)
are analytic functions in a neighborhood of \z\ = 1; see Conway [Con73, p. 31]. In
particular, b(z) will be a Wiener class function and b(z) has no zeros on \z\ â 1. A
formula for computing p is given by [Con73, p. 31]
Next we consider the case i = 1 in more depth. By straightforward division of g(z)
in (5.8.11) by (z â 1), we have
Therefore, by (5.8.6) and (5.8.7),
where Â£(B) is the expected value of the arrival batch size. Thus if SJJL ^ \Â£(B] then
6(1) ^ 0 and hence 0, â 1. Moreover, if Â£(B] < oo, then b(z) is again a Wiener class
function and b(z] has no zeros on \z\ = 1. Clearly from (5.8.15), the first n Laurent
coefficients of b(z], i.e., X^=i Aj â A, fc = 1,2,... ,n, can be computed recursively in
O(ri) operations. Hence by using (5.3.18), c(Gn] and also Pn can be constructed in
O(n] operations [CC96b].

Section 5.8. Applications 
147
Theorem 5.8.3 (Singular Values of P~lnQn). Let b(z] be defined as in (5.8.12)
and the number of servers s in the queue be independent of the queue size n. Then the
sequence of preconditioned matrices P~lQn has singular values clustered around 1 for
large n.
Proof: By (5.8.10) and (5.8.13),
where rank Ln < s. Using Thm. 5.5.1, we can show that the sequence of matrices
C(Gn)~lE~lTn has singular values clustered around 1 for all sufficiently large n. There-
fore, the matrix P^lQn has singular values clustered around 1 for sufficiently large n.
It follows from standard convergence theory of the PCG method (see Sec. 5.3) applied
to the normal equations (P-lQnY(P~lQn}x = (P~lQn}*P~^b that the method will
converge superlinearly and, in particular, in a finite number of steps independent of n.
In each iteration of the PCG method, the main computational cost consists of solving a
linear system Pny = r and multiplying Qn to some vector r. We first recall from Sec. 5.5
that the cost of solving Pny = r is of O(nlogn) + O(d.ri) operations. To compute Qnr,
we make use of the partitioning (5.8.10). Note that Rn in (5.8.10) is a matrix containing
only 2s â1 nonzero entries; we therefore need O(s) operations for computing Rnr. Since
Tn is a Toeplitz matrix, Tnr can be computed in O(nlogn) operations by embedding
Tn into a 2n x In circulant matrix [Str86]. Hence Qnr can be obtained in O(nlogn)
operations. Thus the number of operations required for each iteration of the PCG
method is of order O(nlogn).
Finally, we consider the memory requirement. We note that besides some n-vectors,
we have to store only the first column (or eigenvalues) of the matrices En and C(Gn)
but not the whole matrices. Thus we need O(ri) memory for the PCG method.
We remark that circulant-type preconditioners have also been used in solving Marko-
vian network with overflow [CCW96] and Markov-modulated Poisson process queueing
systems [CCZ97]. Further discussions on applications of queueing theory can be found
in Ch. 8.
5.8.4mage Restorations
Image restoration refers to the removal or reduction of degradations (or blur) in an image
using a priori knowledge about the degradation phenomena; see for instance [Jai89].
When the quality of the images is degraded by blurring and noise, important information
remains hidden and cannot be directly interpreted without numerical processing. In
operator notation, the form of the image restoration problem is given as
where the operator A represents the degradation, z is the observed image, and 77 rep
resents an additive noise. Given z and A and possibly the statistics of the noise vector
77, the problem is to compute an approximation to the original signal u. In the digital
implementation of (5.8.17), the integral is discretized using some quadrature rule to
obtain the discrete scalar model

148 
Iterative Methods for Linear Systems with Matrix Structure 
Chapter 5
Writing this in matrix-vector notation, we obtain the linear algebraic form of the image
restoration problem,
where z, 77, and u are N2-vectors and A is an N2 x N2 matrix.
Writing the point-spectrum function (PSF) as a(Â£, 8; a, /?) provides the most general
description of the imaging system. This representation allows the PSF to vary with
position in both the image and object planes. In this case the PSF is said to be spatially
variant. If we assume the PSF is spatially variant, then the matrix A in (5.8.18) has
no special structure. Thus computing a solution to (5.8.18), in this case, can be very
expensive.
In many cases, though, the PSF acts uniformly across the image and object planes.
That is, the PSF is independent of position and, hence, becomes a function of only Â£ â a
and 6 â ft. In this case the PSF is said to be spatially invariant and is written as
Thus the image model (5.8.17) is written as
(5.8.19)
where the integral in (5.8.19) is a two-dimensional convolution. The inverse problem
of recovering u is thus a two-dimensional deconvolution problem. In the discrete imple-
mentation, (5.8.19) becomes
where the matrix A is now a block Toeplitz matrix with Toeplitz blocks, i.e., BTTB. The
image restoration problem, with a spatially invariant PSF, can be reduced to solving
an ill-conditioned BTTB system.
Because of the ill-conditioning of A, naively solving Au = z will lead to extreme
instability with respect to noise in the observed image; see [Jai89]. The method of reg-
ularization can be used to achieve stability for these problems [AE87]. In the classical
Tikhonov regularization [Gro84], stability is attained by introducing a regularization
functional 72., which restricts the set of admissible solutions. Since this causes the regu-
larized solution to be biased, a scalar n, called a regularization parameter, is introduced
to control the degree of bias. Specifically, the regularized solution is computed as
where 7Â£(-) is a certain functional which measures the irregularity of u in a certain
sense. When 7Â£(/) = ||^fe/||i> where Dk is the fcth-order differential operator, it forces
the solution to have a small fcth-order derivative. Notice that if the discretization of the
differential operator is a Toeplitz matrix, then in digital implementation (5.8.20) reduces
to a block Toeplitz least-squares problem. For these Toeplitz least-squares problems,
different preconditioners that are based on the circulant approximations were considered
in [CNP94a], [CNP93], [CNP96]. In [CNP96], [NPT96], restoration of real images by
using the circulant PCG algorithm has been carried out.
The algorithms for deblurring and noise removal have been based on least squares.
The output of these least-squares-based algorithms will be a continuous or smooth
function, which obviously cannot be a good approximation to the original image if
the original image contains edges. To overcome this difficulty, a technique based on

Section 5.9. 
Concluding Remarks 
149
the minimization of the "total variation norm" subject to some noise and blurring
constraints is proposed in [ROF92]. The idea is to use as a regularization function the
so-called total variation norm
The solution to (5.8.20) with R(f) = TV(f) can have discontinuities, thus allowing
us to recover the edges of the original image from the blurred and noisy data. At a
stationary point of (5.8.20), the gradient vanishes, giving
The second term in g is obtained by taking the gradient of a Jn | Vu\dx and then applying
integration by parts from which a Neumann boundary condition results. Since the
gradient equation of (5.8.20) is nonlinear, the fixed point (FP) iteration was employed
in [VO96] to solve this nonlinear gradient equation. The FP iteration will produce a
sequence of approximations Xk to the solution x and can be expressed as
The coefficient matrices T and L correspond to the discretization of the convolution
operator and the elliptic operator, respectively, in the gradient equation (5.8.21). In
[CCW95], the optimal cosine transform-based preconditioner was used to precondition
the linear system. Numerical results showed that this preconditioner works very well.
However, we remark that it is still an active research area to find a good preconditioner
for the linearized equation (5.8.22).
5.9 
CONCLUDING REMARKS
The CG method coupled with a suitable preconditioner can solve a large class of n x n
linear systems with matrix structure in O(nlogn) operations. This chapter summarizes
some of the developments of this iterative method for solving Toeplitz-like, Toeplitz-
plus-Hankel, and Toeplitz-plus-band systems in the past few years. The results show
that the method in some instances works better than traditional methods used specif-
ically for these problems. In practical applications, many linear systems with matrix
structure arise. For instance, in image processing, the restoration of images in nonlin-
ear space-invariant systems involves the solution of Toeplitz-like-plus-band systems; see
[TP91]. Iterative methods provide attractive alternatives to solving these large-scale
linear systems with matrix structure.

150 
Iterative Methods for Linear Systems with Matrix Structure 
Chapter 5
APPENDICES FOR CHAPTER 5
5.A 
PROOF OF THEOREM 5.3.4
We remark that from the standard error analysis of the CG method (Sec. 5.2), we have
where the minimum is taken over polynomials Pk of degree k with constant term 1 and the
maximum is taken over the spectrum of Sn TnSn 
â¢ 
In 
the 
following, 
we 
estimate 
the
minimum in (5.A.I). We first note that the assumptions on / imply that
where c= ||/(l+1)||i,i (see, e.g., [Kat76, p. 24]). Hence,
As in Thm. 5.3.3, we write
where Â£/â is the matrix obtained from Ln by replacing the (n â k) x (nâk) principal submatrix
of Ln by a zero matrix. Using the arguments in Thm. 5.3.3, we see that rank(C/n ) < Ik and
l|Wnfc)||2 < c/kl for all k > 1. Now consider
By Thm. 5.3.3, we have, for large n, rank(^fc)) < 2k and
With C = C/fmin.
Next we note that Wn â Wn + can be written as the sum of two rank-1 matrices of the form
Here Uk is the (n â A;)th unit vector
with lj given by (5.3.14), and wjf = Uk Â±Ufc- Hence by letting z* = Sn ^w^ for k > 0, we have
where VJ. = \ ^2 -=o zt zj* are PÂ°sitive semidefinite matrices of rank k.
Let us order the eigenvalues of Wn as

Section 5.B. 
Proof of Theorem 5.6.2 
151
By applying the Cauchy interlace theorem (cf. App. A) and using the bound of ||Wl ||2 m
(5.A.2), we see that for all k > 1, there are at most k eigenvalues of Wn 
lying to the right of
c/kl, and there are at most k of them lying to the left of âc/kl. More precisely, we have
Using the identity
_ i 
_ i
we see that if we order the eigenvalues of Sn ^ AnSn 5 as
then AjÂ£ = 1 + /*Â£ for all k > 0 with
For A*, the bounds are obtained from Thm. 5.3.1:
(5.A.5)
Having obtained the bounds for A^, we can now construct the polynomial that will give us a
bound for (5. A.I). Our idea is to choose P^q that annihilates the q extreme pairs of eigenvalues.
Thus consider
Between those roots X^, the maximum of |pfc(x)| is attained at the average x = |(<^ + ^^)>
where by (5.A.3), (5.A.4), and (5.A.5) we have
Similarly, for k = 0, we have, by using (5.A.5),
Hence the polynomial Piq â popi â¢ â¢ -Pq-i, which annihilates the q extreme pairs of eigenvalues,
satisfies
for some constant c that depends only on / and /. This holds for all A^ in the inner interval
between A~_: and A+_15 where the remaining eigenvalues are located. Equation (5.3.16) now
follows directly from (5.A.I) and (5.A.6).
5.B 
PROOF OF THEOREM 5.6.2
We note that
It follows from (5.6.6) that

152 
Iterative Methods for Linear Systems with Matrix Structure 
Chapter 5
On the other hand, using Lemma 5.6.2, we have
where g is the generating function of Cn(x\) given in (5.6.9). Thus
Since the generating function g of Cn(x\) is in the Wiener class, it follows from Lemma 5.6.3
that given any e > 0,
where ||V4||2 < e, provided that the size n of the matrix is sufficiently large [CJYQlb]. Hence
where the last equality follows from (5.6.3). Write
where TI is the n x n submatrix of Tm,n. The matrices Tm,n and T\ have the same generating
function / and T^>nTm,n = TfTi + T2T2.
Since / by assumption is in the Wiener class,
where ||V5||2 < e if n is sufficiently large. Thus
Observe that
where the last inequality follows from Lemma 5.6.2. Thus by (5.6.10),
Since T2*T2 + Â£n(yi)Â£n(yi) is a positive semidefinite matrix, C(T2 T2 + Â£n(2/i)Â£*(</i)) is also
a positive semidefinite matrix. Thus we conclude from (5.B.I) that
The lemma follows by observing that e is chosen arbitrarily and 6 depends only on / and not
on n.

Chapter 6
ASYMPTOTIC SPECTRAL
DISTRIBUTION OF
TOEPLITZ-RELATED
MATRICES
Paolo Tilli
6.1 INTRODUCTION
As mentioned in Ch. 5, the convergence rate of the CG algorithm is dependent on the
clustering of the eigenvalues of the coefficient matrix A (assumed symmetric and positive
definite); the more clustered the eigenvalues, the faster the convergence of the iterates
Xk to the solution x of the linear system Ax = b.
The purpose of the current chapter is to provide an overview of several old and
new spectral distribution results for matrices in view of their relevance to the perfor-
mance of iterative methods. In particular, it will be seen that for Toeplitz matrices, the
eigenvalues are not necessarily clustered. Hence, the convergence rate of the (classical)
CG method when applied to a Toeplitz system of equations will not be satisfactory, as
already seen in Ch. 5. This is one reason for incorporating preconditioning into the CG
algorithm to improve the spectral distribution of the resulting coefficient matrix. We
thus have a demonstration of the value of the results on spectral distribution of this
chapter.
6.2 WHAT IS SPECTRAL DISTRIBUTION?
In many applications it is customary to deal with a sequence of matrices An arising from
several discretizations of the same underlying mathematical problem. For example, one
may have to approximate a boundary value problem by finite differences: after reducing
to a discrete problem over a mesh of n points, one has to solve a linear system with a
matrix An of order n (for reasons that we will soon make clear, such matrices are likely
to have some Toeplitz-related structure). On solving such linear systems by iterative
methods (for instance, by the CG method; see [Tyr97] and also Ch. 5), a qualitative
knowledge of eigenvalues or singular values of the involved matrices plays a crucial role,
since the convergence rate of the most popular iterative methods is strictly connected
153

Asymptotic Spectral Distribution of Toeplitz-
154 
Related Matrices 
Chapter 6
with some spectral properties of the matrices themselves.
Suppose An is n x n Hermitian and positive definite, and let Xj(An), j = 1,2,..., n,
denote its eigenvalues, labeled in nondecreasing order. Then it is well known that the
convergence rate of the CG method crucially depends on the quantity (see Sec. 5.2)
i.e., the condition number of An with respect to the operator norm induced on matrices
by the Euclidean norm on vectors. (We recall that if An is not assumed to be positive
definite but merely nonsingular, then Â«(An) equals the ratio of the largest to the smallest
singular value of An\ moreover, we agree to let K(An) â +00 when An is singular.) More
precisely, letting eh = \\x-Xh\\2i where x is the solution to a linear system Anx â 6, Xh
is the vector obtained after h iterations of the CG method starting from any vector XQ,
and || â¢ ||2 is the Euclidean vector norm, we can prove (see [Tyr97] and also Thm. 5.2.2)
that the error eh satisfies the estimate
Prom the above equation we see that the CG method is fast when Â«(An) is close to 1
(observe that it always holds that K(An) > 1). Therefore, to predict the behavior of
the CG method, we should have an estimate of the quantity K(An), and this can be
achieved, in principle, by numerically computing the largest and the smallest eigenvalues
of An.
Nevertheless, in many cases the CG method is quite fast although the ratio An/Ai is
large; for example, this nice feature is apparent from numerical experiments when the
ratio An/Ai is large but the ratio An/A2 is close to 1. Indeed, one can prove [Tyr97]
that after h iterations the following estimate also holds:
Observe that one can think of the ratio Xn/\2 as the condition number of the matrix
An restricted to the subspace generated by the eigenvectors relative to the eigenvalues
A,-, j > 2.
The convergence rate of the CG method was studied in detail in [AL86], [SV86], and
it was proved that, for any natural number k <C n, after a certain number of iterations
depending on A:, the method behaves as if the first k and the last k eigenvalues of the
matrix were not present; in some sense, the ill-conditioning due to the first and the last
k eigenvalues slows down the convergence rate only during the first iterations.
Prom these considerations we see that, to predict the behavior of the CG method, it
is necessary to have some information about the spectra of the involved matrices and,
in particular, to know the behavior of the first and the last k eigenvalues for large values
of ft, not only for k = I.
To achieve this information, an explicit computation of the eigenvalues of An is
not practicable for at least two different reasons. First, the required computational
cost would be much higher than that for solving a linear system with the same matrix
(which was the original task). Furthermore, in a preliminary analysis even the size of
the matrix might be unknown: indeed, one might need qualitative information only on

Section 6.2. What Is Spectral Distribution? 
155
the spectrum of An for all large values of n and not for a single n. (This is clear if, for
example, n is the number of grid points in a finite difference problem and may have to
be increased to achieve a better approximation.)
Some typical questions one might want to answer concerning the matrices An are
the following:
1. Are the condition numbers bounded as n â> oo?
2. If the matrices are ill-conditioned when n is large, are there many small singular
values or just a few?
3. More generally, how are eigenvalues and singular values distributed on the real
line?
The theory of the asymptotic spectral distribution is a useful tool in trying to answer all
the above questions. The underlying idea is, in principle, quite simple: instead of taking
into account every single eigenvalue or singular value, one adopts a global approach by
studying the asymptotic behavior, as n ââ¢Â» oo, of averages of the kind
where F is a given function (called the test function] and \j are the eigenvalues of An.
One can also study singular values by investigating the averages
where <TJ are the singular values.
The knowledge of limn-^oo En(A, F) or limn-xx, Sn(cr, F) for a suitable choice of the
test function F may provide interesting information, as we can see from the following
example.
Example. Suppose that each An is Hermitian and positive definite. Letting F(x} = logs,
one obtains
from which it follows
provided, of course, the limit exists. In this case, we can compute the limit of the geometric
mean of the eigenvalues.
Observe that, in the more general case where the matrices are merely assumed to be normal
(i.e., A^An = AnAn), it holds that
Sometimes one has also to deal with nonsquare matrices (such as in the least-squares
problem; see [CNP93], [GR70], [Tyr97]). The notion of eigenvalue in this case would
not make sense, while singular values are well defined. We recall that, if a matrix A has
HI rows and n^ columns, then it has min{ni,n2} singular values, which coincide with

Asymptotic Spectral Distribution of Toeplitz-
156 
Related Matrices 
Chapter 6
the square roots of the eigenvalues of A A* or A* A, according to whether n\ < ri2 or
HI > ri2 (throughout, A* denotes the complex conjugate transpose of A).
Given a matrix A of order ni x n<2 and a test function F defined on the real line, we
define
where ffj(A) are the singular values of A. If, moreover, A is Hermitian and has order
n, we also define
where Aj(A) are the (necessarily real) eigenvalues of A.
Observe that Â£(<j, A, F) is the average of F over the singular values of A, whereas
S(A, A, F) is the average over the eigenvalues.
The theory of the asymptotic spectral distribution, in its most general form, deals
with the following problem.
Problem 6.2.1 (Spectral Distribution). Given a family of matrices {Aa}a^s, where
S is a partially ordered set of indices, find a class T of test functions F, defined on the
real line, such that the limit
exists for all F G T and compute the limit explicitly in terms of F.
If, moreover, each Aa is Hermitian, then do the same concerning the limits
It is clear that if one wants to retrieve some nontrivial information from the limits
(6.2.1) and (6.2.2), the class of test functions T must be, in some sense, wide; for
example, the class of all constant functions surely works for any sequence of matrices
Aa, but in this case the knowledge of (6.2.1) and (6.2.2) will provide no information at
all. In the next sections, we will mainly consider test functions from the space Co(R) of
all continuous functions with a bounded support (we recall that a function has compact
support if and only if it vanishes outside some bounded interval) or from the space Cfo(R)
of all bounded and uniformly continuous functions. (In both cases, the test functions
are defined over R and are complex valued.) Both Co(R) and Cb(R) are large enough
for many purposes; observe, however, that Co(M) C Cf>(R) and the inclusion is strict, as
one can see by considering any (nonidentically vanishing) continuous periodic function.
Observe further that in the above statement of the problem, we did not specify
the nature of the set of indices S, and hence what lima_Â»oo means may depend on the
context. Of course, the case where S = N is the set of natural numbers is the simplest
one and, in this case, linic^oo has the usual meaning. However, the case where An is a
sequence of matrices of order n is not the only one that may occur when S is the set of
natural numbers; for example, each An might be an n x n block matrix with k x k blocks,
where A; is a fixed natural number, or even an n x n block matrix with h x k blocks.
(Nonsquare matrices are natural, for example, in dealing with least-squares problems.)

Section 6.3. Toeplitz Matrices and Shift Invariance 
157
The reason why we do not restrict ourselves to the case where 5 is the set of natural
numbers is the following. In many cases, it may be convenient to partition a matrix
into n\ x n\ blocks, partition each of these blocks into n-2 x n-z further blocks, and so
on, with p nesting block levels. In this situation, it is natural to let S = Np be the set
of all multi-indices of the kind (n\,..., np), where each entry n^ is a positive natural
number, and label each matrix as An, where n = (ni,... ,np). In this case, (6.2.1) and
(6.2.2) are to be understood as
respectively. In other words, each single entry n^ of the multi-index n (and not only
its magnitude) has to approach infinity. This situation is natural when one deals with
multilevel Toeplitz matrices, which will be discussed later on.
6.3 
TOEPLITZ MATRICES AND SHIFT INVARIANCE
Let us begin with a simple example, which shows a typical problem where Toeplitz
matrices are involved. Consider the boundary value problem
and suppose we want to approximate it by finite differences. The idea behind the
method of finite differences is that of replacing the derivatives of the unknown function
u at a point x with a suitable linear combination of the values u attains at some nearby
points. If u is smooth enough, Taylor's formula yields
where the symbol O(e} denotes a quantity that is infinitesimal of order at least e, as e
tends to zero. After adding and solving with respect to u"(x), we obtain
Letting
we can replace the continuous problem (6.3.1) by a linear system of n equations in n
unknowns. Indeed, forcing (6.3.2) at x = jh, j = 1,... , n, and disregarding the terms
O(h2) yields
which is a discrete analogue of (6.3.1). Observe that (6.3.3) can be written in matrix
form as

Asymptotic Spectral Distribution of Toeplitz-
158 
Related Matrices 
Chapter 6
Let Tn denote the above coefficient matrix, which has order n. It is clear that, whatever
n is, Tn has the same entry 2 along the main diagonal, the entry â1 along the two
secondary diagonals, and zeros elsewhere. The property of having the same entry along
each diagonal characterizes Toeplitz matrices: Tn is therefore a Toeplitz matrix.
As we have seen, Tn (more precisely, Tn/h2) is a reasonable discrete analogue of the
differential operator
Two questions arise naturally: Why does Tn have a Toeplitz structure? Why just the
numbers (â1,2,â!)?
The answer to the second question is simple: the numbers (â1,2,â!) are the
coefficients of the linear combination (6.3.2) used to approximate the second derivative.
Indeed, another finite difference scheme (e.g., a five-point one instead of the three-point
one above) would lead to different entries in the resulting matrices. The reader is re-
ferred to [Col60] for a discussion of many other schemes suitable to approximate the
second derivative and more general differential operators.
The answer to the first question is related to the fact that Tn approximates a dif-
ferential operator with constant coefficients. More precisely, any differential operator
V with constant coefficients, such as (6.3.5), is shift invariant; that is, if 7^ is the shift
operator acting on functions as
then T> and 7/j turn out to commute for all h. (The underlying reason is easy: the set of
all shifts is an Abelian group, and differentiation is naturally defined in terms of shifts.)
It is therefore natural to expect that any discrete analogue of a differential operator
with constant coefficients should be, in some sense, shift invariant. Indeed, this is what
actually happens: all the n equations in (6.3.3) can be obtained from one another, simply
by shifting the unknowns. There is indeed only one single pattern of equation, associated
with the numbers (â1,2,â!), which must hold at each of the n points of the grid {xj}.
This sort of shift invariance is better understood through the matrix representation
(6.3.4), where the jih row of the matrix is associated with the jih equation in (6.3.3);
the (j + l)th row of the matrix is then equal to the jih row, shifted to the right by one
position. In short, the shift invariance of the initial problem (6.3.1) is, in some sense,
inherited by the discrete model (6.3.4), and this reflects into the Toeplitz structure of
the resulting matrix.
We recall that the shift invariance of Toeplitz matrices can be also exploited in
order to devise efficient algorithms for Toeplitz-related matrices by means of the theory
of displacement structure (see, e.g., Ch. 1 and [KS95a]).
6.3.1 
Spectral Distribution of Toeplitz Matrices
In the theory of spectral distribution, we are not interested in a single Toeplitz matrix,
but in a whole sequence {Tn}^L1 (which, for simplicity, we shall continue to denote

Section 6.3. 
Toeplitz Matrices and Shift Invariance 
159
by Tn), such that Tn is a principal submatrix of Tn+i. Letting Tn = {aj-i}?j=l,
we have that the sequence Tn is uniquely determined by the bi-infinite sequence of
complex numbers {dj^^-oa- 
One may think of each Tn as an n x n section of the
infinite Toeplitz matrix T = {aj-i}tj^n vet we snall nÂ°t deal with infinite-dimensional
Toeplitz operators here.
It is customary to associate the sequence Tn with an integrable function /, defined
over the interval (âTT, TT); more precisely, / is the function whose Fourier coefficients are
just the flj's; in formulas, we have
where i is the imaginary unit. It is easy to see that, given a sequence of complex numbers
{dj}, a function / satisfying (6.3.6) will not, in general, exist. For example, from the
Riemann-Lebesgue lemma (see [Edw82]) it follows that a necessary condition for such
an / to exist is that lim^-i^oo dj â 0, but this condition is still far from being sufficient:
unfortunately, there is no characterization of the sequences {a,j} that are the Fourier
coefficients of an integrable function /. (This problem is a difficult one; the interested
reader is referred to the book of Edwards [Edw82] for more details on Fourier series.)
We can overcome this difficulty by choosing the inverse approach: we start from an
integrable function /: (â7r,7r) Hâ> C, we consider its Fourier coefficients according to
(6.3.6), and we build the sequence of Toeplitz matrices
We say that Tn(f) is the sequence of Toeplitz matrices generated by /, and we call /
the generating function.
The spectral theory of Toeplitz matrices, which started with the pioneering work
of the Hungarian mathematician Gabor Szego, relates many properties of the matrices
Tn(/) to some properties of / as a function.
From (6.3.6), one immediately finds Tn(/)* = Tn(/*); therefore, we have the follow-
ing.
Proposition 6.3.1 (Hermitian Toeplitz Matrices). The matrices of the sequence
Tn(f) 
are all Hermitian if and only if f is a real-valued function.
The keystone of the asymptotic spectral theory of Hermitian Toeplitz matrices is a
celebrated result due to Szego, which we state here in its original formulation [GS84].
Theorem 6.3.1 (Asymptotic Distribution Result). Suppose f: (â7r,7r) â¢-Â» R. is a
measurable bounded function, and let Tn be the sequence of Hermitian Toeplitz matrices
generated by f ; then for any function F, continuous on [inf/, sup/], it holds that

Asymptotic Spectral Distribution of Toeplitz-
160 
Related Matrices 
Chapter 6
where Aj(Tn), j = 1,... , n, are the eigenvalues ofTn.
In the Szego theorem the numbers inf / and sup / denote the essential infimum and
the essential supremum of the function / over its domain. The word "essential" means
that we disregard sets of null Lebesgue measure; more precisely, the number sup / is the
smallest real number c such that f ( 0 ) < c holds for all 0 e (â7r,7r) except, at most, on
a subset of null Lebesgue measure (the existence of such a number follows from the fact
that / is supposed to be bounded), and a dual definition holds for inf/. Disregarding
null measure sets is natural when one is concerned with quantities depending on the
integral of a function /; indeed, changing the values of / over a null measure subset of its
domain will not affect the value of its Lebesgue integral. For example, if f ( 0 ) = g(9] for
all 0 except on a null measure set, then the Fourier coefficients of / and g are the same,
and hence the matrices Tn(f} and Tn(g) coincide. Therefore, all properties concerning
the generating function / (such as "/ is positive," "/ is real valued," and so on) will be
tacitly assumed to hold "almost everywhere," that is, everywhere except possibly over
a null measure subset of the domain of /.
The limit relation (6.3.8) is known as the Szego formula. Its importance lies in
that it provides, in one stroke, a lot of information concerning the asymptotic behavior
of the eigenvalues of Tn. Throughout, we will discuss some recent extensions of the
Szego formula, also concerning matrices that are not Toeplitz in the usual sense but
that are closely related to Toeplitz matrices. The Szego formula can be considered
a classic result, which has been generalized and extended by so many authors and in
so many directions that taking into account all the contributions is no easy task and
is perhaps impossible; among the many relevant papers and books, here we mention
[Avr88], [GS84], [HH77], [Par86], [BS90], [Til98a], [Til98b], [Tyr96a], [TZ97], [Wid74],
[WidSO], [Wid75].
Example. Consider the generating function
An elementary computation of the integrals (6.3.6) shows that the Fourier coefficients of / are
Therefore, the Toeplitz matrix Tn(f) coincides with the matrix in (6.3.4). It turns out that
the eigenvalues of Tn can be computed explicitly; they are
as one can prove by checking that, for j â 1,..., n, the vector whose ith component is sin(^Y)
is an eigenvector of Tn, relative to the eigenvalue Aj(Tn).
It is clear that the range [inf/,sup/] is the interval [0,4]; according to the assumptions
of Thm. 6.3.1, let F be continuous over [0,4]. Using (6.3.10), we can make the summation in
(6.3.8) more explicit, and the Szego formula (6.3.8) simplifies to

Section 6.3. 
Toeplitz Matrices and Shift Invariance 
161
Observe that, in this very special case, the above relation could be obtained without the aid
of the Szego theorem; indeed, the left-hand side is just a Riemann sum of the continuous and
bounded function F(2 â 2cos7rt), and therefore it converges to
Finally, letting x = nt and observing that cos re = cos(âx), it is easily seen that the last integral
coincides with the integral in (6.3.11).
In the above theorem, the test function F is supposed to be defined (and continuous)
over the interval [inf /, sup /], and this assumption assures that the integral in the right-
hand side of (6.3.8) makes sense; on the other hand, in the left-hand side of (6.3.8) F
is evaluated at the eigenvalues of Tn. For this to make sense, the spectrum of each Tn
should lie inside the interval [inf/,sup/]; indeed, we have the following [GS84].
Theorem 6.3.2 (Bounds for Eigenvalues). Suppose f is real valued and bounded
and let Tn be the set of Hermitian Toeplitz matrices generated by f . Then, for any
natural number n, if A is an eigenvalue ofTn, it holds that
Moreover, if A = inf / or A = sup /, then f is constant.
In other words, if / is not constant then the inequalities in (6.3.12) are strict and
the spectrum of Tn actually lies inside the open interval (inf/,sup/); if, on the other
hand, / is constant, then the problem is trivial since each Tn is a scalar multiple of the
identity matrix.
In the Szego formula, the assumption that the test functions are continuous cannot
be dropped, as we show in the following example.
Example. Suppose / is defined according to
Then / is clearly bounded, inf/ = 0, and sup/ = 1. Consider the test function
which is continuous over the interval [inf/,sup/] = [0,1] except for y = 1. Since / is not
constant, by Thm. 6.3.2 the eigenvalues of each Tn lie in the open interval (0,1); therefore, for
all n we have
On the other hand, the integral

Asymptotic Spectral Distribution of Toeplitz-
162 
Related Matrices 
Chapter 6
is nonzero and hence F is not a suitable test function in the Szego formula; that is, (6.3.8)
does not hold with this choice of F. We remark, however, that F is discontinuous only at one
point over [inf/,sup/].
The Szego formula opens up many interesting questions:
â¢ What can we say if / is not bounded but merely integrable?
â¢ What can we say if / is not real but complex valued?
â¢ What can we say about singular values?
We shall examine the above three items in some detail in the following sections.
6.3.2Unbounded Generating Function
Perhaps the most unpleasant assumption in the classic Szego theorem 6.3.1 is the bound-
edness of the generating function /. In 1996, Tyrtyshnikov [Tyr96a] gave a new proof of
the Szego formula based on approximation by circulant matrices; curiously enough, his
proof by no means rested on the boundedness of /, but had in the space L2 its natural
environment. For p > 1, we denote by Lp the vector space of all functions defined over
(âTT, TT) such that \f\ p is integrable; we recall that Lp is a Banach space if endowed with
the natural norm
It is customary to denote by LÂ°Â° the space of all essentially bounded functions, which is
a Banach space with the so-called (essential) sup norm (see [Edw82] for more details).
Since the interval (â7r,7r) has finite measure, by the Cauchy-Schwartz inequality
we obtain the (strict) inclusion Lp C Lq whenever p > q. In view of this fact, we can
say that Tyrtyshnikov [Tyr96a] extended the Szego theorem from LÂ°Â° to L2; the results
from [Tyr96a], however, left open the question of whether the Szego formula is valid for
an arbitrary / 6 L1.
This problem was first solved in [TZ97], where the Szego formula was proved for
/ Â£ L1 and when the test functions are from Co(M).
A short and elementary proof of the Szego formula in the L1 case (which, moreover,
allows all test functions from Cb(M); see the discussion after the statement of Prob. 6.2.1)
can be found in [Til98c]. Following [Til98c], the Tyrtyshnikov-Zamarashkin theorem
reads as follows.
Theorem 6.3.3 (Szego Formula, L1 Case). Suppose f : (â?r, TT) i-> R is integrable,
and let Tn be the sequence of Hermitian Toeplitz matrices generated by f; then for any
test function F from C^(R) it holds that
where Xj(Tn), j = 1,... , n, are the eigenvalues ofTn.

Section 6.3. 
Toeplitz Matrices and Shift Invariance 
163
This theorem should be considered as the ultimate version of the classic Szego for-
mula, as long as one considers sequences of Toeplitz matrices associated with a gener-
ating function /; indeed, the assumption /el/ 1 cannot be weakened any further since,
if / is not integrable, then the Fourier coefficients in (6.3.6) are not well defined.
6.3.3Eigenvalues in the Non-Hermitian Case
Now let us drop the assumption that / is real valued. Can we find some sort of Szego
formula in this case?
A look at (6.3.8) suggests that when / is complex valued (and hence some Tn may
fail to be Hermitian), then F should be defined on some domain of the complex plane,
for example, in the disk of radius sup|/|, in order that the right-hand side of (6.3.8)
makes sense.
The following simple example shows that the matter is much more subtle than might
be expected.
Example. Consider f(0) = rel6, where r is any positive real number. It is easy to see that,
in this case,
and hence the eigenvalues of Tn are all null. Therefore, (6.3.8) would reduce to
It is easy to see that (6.3.15) does not hold for all continuous F, since the right-hand side only
depends on the values of F(z) when \z\ â r, which may have nothing to do with -F(O) (see
[Til98a] for more details).
From the above example it is clear that if we want to extend the Szego formula to
the non-Hermitian case, then each admittable test function F should at least satisfy
(6.3.15); that is, the mean value over a circle should equal the value at the center. It is
well known (see, for example, [GT83]) that this property, known as the mean property,
is typical of (indeed, it characterizes) harmonics functions, that is, functions such that
Aw = 0, where A is the Laplace operator. If F is defined over the complex plane, then
we say that F is harmonic if, letting
the bivariate function u(x, y) is harmonic in the usual sense, that is, if
This definition is natural since it says that F is harmonic if it is regarded as a function
of two real variables (namely, the real and imaginary part of the complex variable).

Asymptotic Spectral Distribution of Toeplitz-
164 
Related Matrices 
Chapter 6
Curiously, it turns out that the Szego formula can be extended to the case where /
is complex valued if (and only if!) the test functions are harmonic; more precisely, the
following result holds [Til98a].
Theorem 6.3.4 (Complex-Valued /). Suppose f : (â7r,7r) i-> C is bounded, and let
Tn be the set of Toeplitz matrices generated by f . Then for any function F, harmonic
in an open convex set containing the range of f , it holds that
Conversely, if F is continuous in an open set O C C and is such that (6.3.16) holds for
any bounded f whose range is inside O, then F is harmonic inside O.
The fact that (6.3.16) does not hold (in general) when F is not harmonic prevents
(6.3.16) from having a great practical interest, since the class of harmonic test functions
is too restricted, and hence (6.3.16) does not help to understand, for example, where
the eigenvalues are actually clustered.
On the other hand, the last part of the theorem states that we cannot do better; that
is, the class of harmonic functions is the largest class of test functions that comply with
the Szego formula in the complex case. Perhaps some new tool other than the Szego
formula should be introduced to investigate the distribution, on the complex plane, of
the eigenvalues of non-Hermitian Toeplitz matrices.
6.3.4 
The Szego Formula for Singular Values
When / is complex valued, the lack of a satisfactory Szego formula for eigenvalues is
in some sense compensated by a full Szego formula for singular values. According to
[Til98c], it reads as follows.
Theorem 6.3.5 (Szego Formula for Singular Values). Suppose f: (âTT, TT) H-> C is
integrable, and let Tn be the sequence of (nonnecessarily Hermitian) Toeplitz matrices
generated by f ; then for any function F, uniformly continuous and bounded over R, it
holds that
where o~j(Tn} are the singular values ofTn.
The above theorem is essentially due to Avram [Avr88] and Parter [Par86] who
proved it under very restrictive assumptions on /; later, it was extended by Tyrtysh-
nikov [Tyr96a] to the case / e L2 and by Tyrtyshnikov and Zamarashkin [TZ97] to the
case / â¬ L1, while Tilli [Til98c] proved it in the above formulation.
The analogy with the Szego formula (6.3.8) is apparent: it suffices to replace eigen-
values with singular values in the left-hand side and /(#) with |/(#)| in the right-hand
side. Indeed, the above result is known as the Szego formula for singular values.

Section 6.3. 
Toeplitz Matrices and Shift Invariance 
165
However, one fundamental difference between the eigenvalues and the singular values
setting should be pointed out: Thm. 6.3.2 has no analogue for singular values. In other
words, in the light of Thm. 6.3.2, it might be expected that the singular values of T n ( f )
all lie in the interval [inf |/|, sup |/|], but this is not true in general.
Example. Consider the generating function and the sequence of associated Toeplitz matri-
ces of the first example of Sec. 6.3.3. In this case, since |/(#)| = r for all 0, the interval
[inf |/|, sup |/|] reduces to the single point {r}, but Tn(/) (which is given by (6.3.14)) has
always one null singular value, which therefore lies outside [inf |/| ,sup |/|].
The last example shows that the estimate from below a > inf |/| does not generally
hold; nevertheless, the one from above is always true [Til98a].
Theorem 6.3.6 (Upper Bound for Singular Values). Suppose that f : (â7r,7r) iâ>
C is bounded, and let Tn be the sequence of Toeplitz matrices generated by f . Then, for
any natural number n, if a is a singular value of Tn it holds that
Moreover, if we denote by 7e(n) the number of singular values of Tn that are smaller
than inf |/| â e, then for all e > 0 it holds that
The last part of the theorem states that the set [inf |/|, sup |/|] is a "general cluster"
for the singular values of Tn (the notion of general cluster was introduced by Tyrtysh-
nikov in [Tyr96a]); roughly speaking it means that, although the interval [inf |/|, sup |/|]
does not necessarily contain all the singular values of each Tn, the number of "outliers"
is o(n) as n â> oo. (The notation o(n) denotes a quantity such that o(n)/n 
â>â¢ 0 as
n â> co.)
Now suppose that |/(#)| > 8 for some 6 > 0; from the results stated so far, it is clear
that Tn might be ill-conditioned (or even singular, according to the last example) as
n â>â¢ co, since Tn might have some small singular values closer and closer to zero. In the
light of what we said in the introduction about the convergence rate of the CG method
it is important to estimate (in terms of n) how many singular values of Tn are less than
inf |/| or, keeping the notation of the last theorem, estimate the quantity 7e(n).
When / is continuous, we can prove the following [ST98].
Theorem 6.3.7 (Estimate of 7Â£(n)). Suppose that f: (â7r,7r) H-Â» C is continuous;
then for all e > 0 there exists a constant C6 > 0 such that
According to the terminology introduced by Tyrtyshnikov [Tyr96a], the above the-
orem says that if / is continuous, then the interval [inf |/| ,sup|/|] is a proper cluster
for the singular values of Tn.

Asymptotic Spectral Distribution of Toeplitz-
166 
Related Matrices 
Chapter 6
6.4 
MULTILEVEL TOEPLITZ MATRICES
Suppose we want to approximate by finite differences the Laplace equation
where fi C R is the open square defined by
and dÂ£l is its boundary.
Given a natural number m > 1, consider the mesh of points
where 6 = l/(m + 1) is the mesh size. We want to replace the differential equation
(6.4.1) with a system of finite difference equations, one for each point of the mesh. Let
us set
The boundary condition becomes
If (xi, yj] is an interior point of the mesh, then the Laplacian can be approximated by
(The above formula is known as the five-points approximation to the Laplacian and
can be obtained by reasoning as in the unidimensional case discussed in Sec. 6.3.)
Disregarding the term 0(<52) and recalling (6.4.2), we obtain a linear system of ra2
equations in ra2 unknowns:
It is convenient to partition the unknowns {uij}â¢j=i into m vectors as
and do the same with the right-hand sides of (6.4.3):

Section 6.4. 
Multilevel Toeplitz Matrices 
167
The linear system of (6.4.3) then can be written blockwise as
where Im is the identity matrix of order m and the matrix B is given by
We find out that the matrix of the linear system (6.4.4) has order ra2 x ra2, it has an
ra x m blockwise Toeplitz structure, and each block is an ordinary Toeplitz matrix of
order ra. We can say that the Toeplitz structure of the matrix has two nesting levels;
for this reason, such a matrix is called a two-level Toeplitz matrix.
Matrices of this kind are quite common in many applications. The one above has
a two-level Toeplitz structure, and each level has order ra; more generally, one can
consider matrices with a p-level Toeplitz structure, where the order of the ith level is
a given natural number n^, i = 1,... ,p. We now want to investigate this structure in
more detail.
Given a multi-index n = (n\,..., np), a p-level Toeplitz matrix Tn of size n is defined
recursively as follows. If p = 1, then it is a customary Toeplitz matrix of order n\. If
p > 1, then Tn can be partitioned into n\ x n\ blocks,
and each block Bj is a (p â l)-level Toeplitz matrix of size (r&2,..., np).
In other words, Tn has an outermost block Toeplitz structure of order ni, each block
is itself a block Toeplitz matrix of order ^2, and so on, down to the innermost block
level, made of ordinary Toeplitz matrices of order np.
As an example, for p = 2, HI = 3, and n-2 = 2, the generic two-level Toeplitz matrix
of order n = (3,2) has the following structure:
where the aitj are arbitrary complex numbers. This definition of a p-level Toeplitz matrix
is easy to understand but hard to handle; an equivalent but more suitable definition is

Asymptotic Spectral Distribution of Toeplitz-
168 
Related Matrices 
Chapter 6
via the tensor product. Given two matrices, A of order HI x mi and B of order n^ x 7712,
the tensor product of A and B is defined by
It is easy to see that if A has some structure (e.g., it is a Toeplitz matrix), then A <8> B
inherits the same structure at a block level (e.g., it is a block Toeplitz matrix). We can
exploit this observation in order to handle p-level Toeplitz matrices more easily.
The set of all Toeplitz matrices of order n x n is clearly a vector space of dimension
2n â 1, and the natural basis is provided by the matrices Â«/â , / = 0, Â±1,..., Â±(n â 1),
where Jn denotes the matrix of order n whose (i, j ) entry equals 1 if j â i = I and
equals zero otherwise.
The reader can check that the set of all p-level Toeplitz matrices of a given order
n = (ni,..., rip) is a vector space of dimension (2ni â !)â¢â¢â¢ (2np â 1), whose natural
basis is given by the matrices
Therefore, any p-level Toeplitz matrix Tn, where n = (ni,...,np), can be uniquely
written as
where the complex numbers a^,...,jp are suitable entries of the matrix.
As in the one-level (ordinary Toeplitz) case, it is natural to consider the sequence
of p-level Toeplitz matrices generated by a function /; in order to have a natural corre-
spondence between the coefficients a^,...,^ in (6.4.7) and the Fourier coefficients of the
generating function, we can see that / should be a funcion of p independent variables.
Usually, such an / is defined over the p-cube (âTT, ?r)p, and its Fourier coefficients are
Given / depending on p variables as above, we can consider the family of p-level Toeplitz
matrices Tn(/) generated by the Fourier coefficients of /, according to (6.4.7) and (6.4.8).
Again, the function / is referred to as the generating function.
All the distribution theorems discussed in Sec. 6.3.2 have an analogue in the p-level
case; for instance, it is easily checked that the Tn are all Hermitian if and only if the
generating function is real valued.
Concerning the asymptotic distribution of the eigenvalues when / is real valued,
Thm. 6.3.3 extends in a natural way to handle p-level Toeplitz matrices.
Theorem 6.4.1 (Szego Formula for Eigenvalues, p-Level Case). 
Suppose that
f : (âTT, TT)
P iâ>â¢ R is integrable, and let Tn be the sequence of Hermitian p-level Toeplitz
matrices generated by /; then for any function F, uniformly continuous and bounded
over R, it holds that

Section 6.4. 
Multilevel Toeplitz Matrices 
169
where Aj(Tn) are the eigenvalues o/Tn.
Also, Thm. 6.3.5 has a natural analogue in the multilevel case.
Theorem 6.4.2 (Szego Formula for Singular Values, />Level Case). Suppose f :
(âTT, ?r)p i-> C is integrable, and let Tn be the sequence of (nonnecessarily Hermitian)
p- Toeplitz matrices generated by f ; then for any function F, uniformly continuous and
bounded over R, it holds that
where (Tj(Tn) are the singular values ofTn.
The last two theorems are mainly due to Tyrtyshnikov [Tyr96a] and Tyrtyshnikov
and Zamarashkin [TZ97]; an elementary proof can be found in [Til98c].
We stress that in (6.4.9) and (6.4.10) all the entries r^ of the multi-index n simul-
taneously tend to infinity. The next example shows that this requirement cannot be
dropped in general; that is, letting the size of Tn tend to infinity (instead of each single
entry) is not sufficient for (6.4.9) and (6.4.10) to hold. (It is clear that, if all the n;'s
tend to infinity, then in particular the size of Tn tends to infinity).
Example. If p = 2 and /(0i,02) = 2 - 2cos(0i), then it is easy to check that
where
Tni(g) is the one-level Toeplitz matrix of order n\ generated by <?(0i), and /n2 is the identity
matrix of order ri2. Then the n\ ni eigenvalues of Tn coincide with the n\ eigenvalues of Tni (g),
each with multiplicity equal to ri2- (This follows from (6.4.11) and from some elementary
properties of the tensor product.)
Therefore, in this case (6.4.9) reduces to

Asymptotic Spectral Distribution of Toeplitz-
170 
Related Matrices 
Chapter 6
If we replace the above limit, which involves both m and na, with Iimn2-Â»oo, letting m = 1,
then we obtain
which is easily checked since T\(g) is a scalar and equals 2. It is clear that, in general, F(2)
does not equal the integral in (6.4.12).
Unlike the distribution theorems of Sec. 6.3.2, Thm. 6.3.7 does not extend to the
case where p > 1, as seen by the following example.
Example. Suppose p > 1, and let
Then all the Fourier coefficients a,jlt...,jp are null, except for Oi,...,i, which equals 1. Therefore,
(6.4.7) simplifies to
Since the first column of JAj is null, from the above equation we see that the first ri2 â¢ â¢ â¢ np
columns of Tn are null, and hence at least ni â¢ â¢ â¢ np singular values of Tn are null. This means
that Thm. 6.3.7 does not hold if p > 1, since in our example inf |/| = 1, yet the number of null
singular values of Tn cannot be bounded by any constant.
6.5 
BLOCK TOEPLITZ MATRICES
Suppose one wants to approximate the following convolution operator T with a matrix-
valued kernel:
where
The operator T transforms k-vector-valued functions into h-vector-valued functions
over [0,1]. If we sample the kernel F at the points {Â±Zj}, where
and we let
by replacing the integral with a finite summation we obtain the approximation

Section 6.5. 
Block Toeplitz Matrices 
171
Building the vectors
the system (6.5.2) can be written in compact form as
where the matrix Fn is given by
The matrix Fn has an n x n block Toeplitz structure, where each block has order h x k
(according to the assumptions on the kernel F). On the other hand, the blocks have
no particular structure. (They are a sampling of the matrix-valued function F, which
satisfies no structure assumption.)
Matrices with such a structure are called block Toeplitz matrices and are frequently
encountered in many applications such as signal processing, Markov chains (see, e.g.,
Ch. 8 of this volume), or integral equations. Therefore, it would be desirable to have
Szego-like formulas for block Toeplitz matrices.
To make things precise, a block Toeplitz matrix of order n with h x k blocks is one
that can be partitioned into blocks according to
We caution the reader against confusing the notion of a multilevel Toeplitz matrix with
the more general one of a block Toeplitz matrix. A p-level Toeplitz matrix (with p > 1)
is a block Toeplitz matrix, but its blocks are themselves block Toeplitz matrices (more
precisely, they are (p â l)-level Toeplitz matrices and so on, recursively); on the other
hand, in (6.5.3) the blocks Aj are not required to have any structure. (Indeed, they are
not even required to be square matrices.)
We can think of a block Toeplitz matrix such as Tn in (6.5.3) as an ordinary Toeplitz
matrix whose entries are matrices of a given size h x k. It is therefore easy to extend
the notion of generating function to the case of block Toeplitz matrices: if the blocks
have size h x k, it is natural to consider a function / defined over (âTT, TT), taking values
in the space of h x k matrices. In other words, for all d Â£ (âTT, TT), f ( 0 ) is a matrix with
h rows and k columns. The Fourier coefficients of / are defined in a natural way as

Asymptotic Spectral Distribution of Toeplitz-
172 
Related Matrices 
Chapter 6
Then the block Toeplitz matrix of order n generated by / can be written suitably as
Observe that, in the particular case where h = k = 1, the above definition reduces to
that of ordinary Toeplitz matrices; moreover, the notation is consistent.
Before discussing some possible extensions of the Szego formula to block Toeplitz
matrices, some remarks are needed. Observe that the notion of eigenvalue is meaningless
when h ^ k (that is, when T n ( f ) is not a square matrix); on the other hand, singular
values make sense also in the nonsquare case. The main difficulty, however, is of a
different nature. Suppose h = k, and assume that /(#) is Hermitian for all x, so that
Tn(/) is also Hermitian; if we want to extend the Szego formula to the block case, what
should we put in the right-hand side of (6.3.8)? Of course F(f(Q)) makes no sense, since
F is defined on the real line and /(#) is a matrix. The following example suggests how
(6.3.8) should be modified when / is matrix valued.
Example. Suppose /i and /2 are real-valued functions defined over (â7r,7r), let h = k = 2,
and define the matrix-valued function
According to (6.5.5), each Tn(f) is a square matrix of order 2n. Let Yl^n be the permutation
matrix of order 2n which acts on a vector v = (vi,..., v^nY according to
An elemetary computation shows that
where Tn(fi) is the ordinary Toeplitz matrix of order n generated by /Â». (Recall that each
fi is scalar valued.) In other words, by a suitable permutation of rows and columns Tn(/) is
reduced to the direct sum of two ordinary Toeplitz matrices of order n. The reason why this
can be done is very easy to see: the permutation matrix H2n simply sorts rows and columns
in such a way that the Fourier coefficients of f\ are placed in the upper left block of order n,
those of /2 are placed in the lower right block of order n, and the remaining null entries (which
amount to 2n2) fill the two off-diagonal blocks of order n.
Since permutation matrices are unitary, it follows from (6.5.7) that the 2n eigenvalues of
Tn(/) result from the union of the eigenvalues of Tn(/i) with those of Tn(fz). As a consequence,
for any test function F, we have
Taking the limit as n âÂ»â¢ oo, from the Szego formula separately applied to T n(f\) and Tn(/2),
we obtain

Section 6.5. 
Block Toeplitz Matrices 
173
Since for all 0 â¬ (âTT, TT) the two numbers /i(0) and /2(#) are just the two eigenvalues of /(0),
the above formula can be rewritten suggestively as
This example suggests how the right-hand side of the Szego formula should be mod-
ified in the case of block Toeplitz matrices: in place of F(/(0)) (which makes no sense
if /(0) is a matrix), we use the average of F over the eigenvalues of /(#).
However, we stress that the technique used in the above example cannot be used in
general. Indeed, the block splitting (6.5.7) strongly rests on the diagonal structure of /
given by (6.5.6). When / is not diagonal (that is, in the general case) the trick used in
the above example is bound to fail.
Although the technique used in the last example cannot be applied in general, by
means of more sophisticated tools the following theorem can be proved (see [TM99],
[MT96], [Til98c]).
Theorem 6.5.1 (Szego Formula for Hermitian Block Toeplitz Matrices). Sup-
pose f: (â7T,7r) H-> C x is a matrix-valued integrable function such that f(9) is Hermi-
tian for all x, and let Tn be the sequence of Hermitian block Toeplitz matrices generated
by f . Then for any function F, uniformly continuous and bounded over R, it holds that
where \j(Tn] are the eigenvalues o}Tn and Aj (/(#)) are those of f ( 9 ) .
In the above theorem, the assumption that / is integrable obviously means that each
entry of / is integrable in the usual sense.
It is easy to see that, in the particular case where h = 1, / reduces to a real-valued
function and Thm. 6.5.1 reduces to Thm. 6.3.3.
Concerning singular values, we can drop not only the assumption that /(#) is Her-
mitian but even the assumption that it is a square matrix.
Theorem 6.5.2. (Szego Formula for Non-Hermitian Block Toeplitz Matri-
ces). 
Suppose that the function f: (âTT, TT) i-> (&hxk is integrable, letTn be the sequence
of block Toeplitz matrices generated by f , and let d = min{/i, k}; then for any function
F, uniformly continuous and bounded over R, it holds that
where o-j(Tn] are the singular values ofTn and &j(f(0)} 
are those of f ( 0 ) .
A simple proof of the last two theorems can be found in [Til98c] (see also [Til98a],
[Til96], [MT96]).

Asymptotic Spectral Distribution of Toeplitz-
174 
Related Matrices 
Chapter 6
6.6 COMBINING BLOCK AND MULTILEVEL STRUCTURE
Starting from ordinary Toeplitz matrices (the generating function / is univariate and
scalar valued), we have so far discussed p-level Toeplitz matrices (when / is p-variate and
scalar valued) and block Toeplitz matrices (when / is univariate and matrix valued).
It is therefore natural to consider block multilevel Toeplitz matrices (BMTMs), that
is, the case where the generating function / is simultaneously multivariate and matrix
valued.
For natural numbers p,h,k > 1, suppose the function /: (â7r,7r)p H-> Chxk is inte-
grable over the cube (â7r,7r)p. Then the Fourier coefficients of / are h x k matrices
defined according to
Given a multi-index n = (ni,... , np), we can define the block p-\eve\ Toeplitz matrix
Tn(/) generated by / as
It is immediate to see that Tn has hn\---np rows and k n\ â¢ â¢ â¢ np columns; therefore, it
is a square matrix if and only if h = k.
Observing that the transpose conjugate sequence Tn(/)* is generated by the trans-
pose conjugate function /*, one can easily prove the following.
Proposition 6.6.1 (Hermitian Matrices Tn(/)). All the matrices Tn(f) are Her-
mitian if and only if the generating function f is Hermitian valued.
It turns out that the Szego formulas for singular values and eigenvalues can be
extended to the case of BMTMs. (In the case of eigenvalues, by virtue of Proposition
6.6.1, we have to suppose that h = k and that / is Hermitian valued.)
Theorem 6.6.1 (Szego Formula for Hermitian BMTMs). Suppose
is integrable and Hermitian valued, and let Tn be the sequence of Hermitian block p-level
Toeplitz matrices generated by f ; then for any function F, uniformly continuous and
bounded over R, it holds that

Section 6.7. 
Locally Toeplitz Matrices 
175
where Xj(Tn) are the eigenvalues ofTn and \j(/(#i,..., 0P)) are those of f(6\,..., 9P).
Theorem 6.6.2 (Szego Formula for Non-Hermitian BMTMs). Suppose
is integrable; let Tn be the sequence of BMTMs generated by f , and let d = min{/i, k};
then for any function F, uniformly continuous and bounded over R7 it holds that
where Oj(Tn) are the singular values ofTn and o-j (/(#i,..., Op)} are those 0//(0i,..., Op).
The proofs of the above two theorems can be found in [Til98c]. It is worth observing
that they embrace, as particular cases, all the extensions of the Szego formula we have
discussed so far; indeed, letting p = I we obtain the distribution results of Sec. 6.5,
letting h = k = 1 we obtain the results of Sec. 6.4, while letting p = h = k = lwe
obtain the results of Sec. 6.3.1.
6.7 
LOCALLY TOEPLITZ MATRICES
In Sec. 6.3 we discussed how Toeplitz matrices arise quite naturally when a problem has
some kind of shift invariance; in particular, we discussed the example of the boundary
value problem (6.3.1) which leads to Toeplitz matrices since the associated differential
operator has constant coefficients.
This section is devoted to describing, quite informally indeed, the notion of a "locally
Toeplitz" structure, which was introduced in [Til98bj.
Consider the Sturm-Liouville problem
There are many ways to discretize the above equation via finite differences over the grid
{j/(n + l)}j=i> and the interested reader is referred to the classical book of Collatz
[Col60]. However, a rather standard approach (see [Col60], [FS95]) leads to a linear
system of the kind

Asymptotic Spectral Distribution of Toeplitz-
176 
Related Matrices 
Chapter 6
where
Let us briefly explain how this linear system of equations is obtained. (This is but a
heuristic explanation meant as a motivation; for a rigorous approach, see [ColGO].) For
any small number 0, 0 < 9 <C 1, from Taylor's formula we obtain
As a consequence, from Taylor's formula (now applied to F(x}} we obtain
Substituting this approximation into (6.7.1), we obtain
Finally, letting h = l/(n + !),# = /i/2, and forcing the last equation for x = jh,
j â 1,..., n, yields the linear system (6.7.2).
It is immedate to see that (6.7.2) reduces to (6.3.4) if a(rc) = 1; this is quite natural,
since the differential problem (6.7.1) (which has, in general, nonconstant coefficients
depending on a(x)) also reduces to (6.3.1) when a(z) = 1, that is, when it has constant
coefficients.
Observe that, if a(rr) is not identically constant, then for sufficiently large n there
will exist several pairs (i,j) such that on ^ otj, and hence the matrix in (6.7.2) will have
no Toeplitz structure.
Nevertheless, some structural connection between the matrix in (6.3.4) and that
in (6.7.2) is apparent. Indeed, putting mathematical rigor aside for a while, we can
intuitively associate the jth row in (6.3.4) with the triplet (â1,2,â!) and, similarly,
associate the jth row in (6.7.2) with the triplet (âaj-i,aj-i + otj,âaj). 
Now shift
by one position from the jth to the (j + l)th row and see how the triplets change. In
(6.3.4), we have
(Toeplitz structure),
whereas in (6.7.2) we have
(what structure?).
Suppose that a(x) is smooth, say, differentiate or Lipschitz continuous; then, recalling
(6.7.3), the last transition can be rewritten as
In other words, when n is very large, while moving from the jth to the (j + l)th row
the triplet varies by a very small quantity, whereas in the Toeplitz case it does not vary
at all. By (6.7.3), the triplet associated with the jth row in (6.7.2) can be written as

Section 6.7. 
Locally Toeplitz Matrices 
177
Disregarding the term O(l/n) and moving from the first to the last row in (6.7.2), we
can identify the triplet (âotj-i,otj-i + ctj, âctj) with the triplet (â1,2, â1) multiplied
by the coefficient (or weight) a(j/ri), which almost continuously varies from a(0) to a(l).
A matrix with this structure is called locally Toeplitz (a precise definition, however, will
be given in the next section). In fact, in the matrix (6.7.2) we can find a new type of
Toeplitz structure that is not global (indeed, if |a(l) â a(0)| is large, then the triplet
(âQI, ai + 0:2, â#2) in the second row of (6.7.2) will have nothing to do with the triplet
(âan-2,&n-2 + o:n_i, â QQ) in the (n â l)th row) but only local; that is, when k <C n
any two triplets of the kind
and
will be almost equal, since the quantity
will be very small (for example, it will be O(k/n) if a(x) is Lipschitz continuous).
Recalling the example following Thm. 6.3.1, the triplet (â1,2, â1) is made of the
nonzero Fourier coefficients of the function /(#) = 2 â 2cos#, which is the generating
function of the Toeplitz matrix in (6.3.4). By analogy, we can say that the locally
Toeplitz matrix in (6.7.2) has f ( 0 ) as generating function and a(x) as weight function
(all this will be made rigorous in the next section); intuitively, we can think of f ( 0 ) as
providing the underlying Toeplitz structure (â1,2,â!), by means of its Fourier coeffi-
cients, whereas a(x) determines how this Toeplitz structure changes from row to row,
by means of the sampling a(j/n), j = 1, 2,..., n.
If Tn denotes the Toeplitz matrix in (6.3.4), then from the Szego formula (see the
example following Thm. 6.3.1) we obtain
Now let An denote the locally Toeplitz matrix in (6.7.2); what can we say about the
asymptotic distribution of the eigenvalues of Anl In the next sections we will see that,
since An is locally Toeplitz with f ( 0 ) as generating function and a(x) as weight function,
then the following generalized Szego formula holds:
Observe that, if a(x) = 1, then integration with respect to x can be omitted and (6.7.7)
reduces to the Szego formula (6.7.6); on the other hand, as we have already observed, if
a(x) = 1 then An reduces to Tn. In view of this, (6.7.7) can be thought of as a weighted
Szego formula, with a(x] as weight function.
The spectral distribution of An is completely determined by two functions, a(x] and
/(#), which are independent of each other: indeed, the former comes from the differential

Asymptotic Spectral Distribution of Toeplitz-
178 
Related Matrices 
Chapter 6
problem (6.7.1) and has nothing to do with finite differences, whereas the latter depends
only on the finite difference scheme adopted to discretize the problem. (Indeed, the
triplet (â1,2,â!) comes from the approximation schemes (6.7.4) and (6.7.5), which
hold for any choice of a in (6.7.1).)
6.7.1A Closer Look at Locally Toeplitz Matrices
Given matrices A and B we denote by A <8> B the tensor product defined by (6.4.6) and
denote by A 0 B the 2 x 2 block diagonal matrix with A and B as diagonal blocks; as
usual, we agree to let A Â® B Â© C = (A <g> B} Â© C. Moreover, we denote by \\A\\p the
Probenius norm of A, defined as
Given a function a: [0,1] âÂ» C and a positive integer ra, we set
that is, Dm(a) is the diagonal matrix of order m whose entries are the values of the
function a on the mesh {j/rn}1jL1.
We denote by O^ the null matrix of order k. Given two functions
and two positive integers n > m, we define the matrix-valued operator LTâ¢ as follows:
In the last equation [n/m\ is the integer part of n/m and n mod m denotes the remain-
der n â m [n/raj (it is understood that the null block is not present if n is a multiple of
m).
We stress that LTâ¢(a, /) is a matrix of order n, with block diagonal structure. More
precisely, it consists of m blocks of order \n/m\ and one null block of order n mod m to
match the size. Moreover, the jth diagonal block is the Toeplitz matrix of order L^i/^J
generated by the function
Bearing in mind the above notation, we are in a position to give a precise definition
of locally Toeplitz structure. The reader is referred to [Til98b] for more details.
Definition 6.7.1 (Locally Toeplitz Sequences). A sequence of matrices An> where
An â¬ Cnxn, is said to be locally Toeplitz with respect to a pair of functions 
(a,f)
satisfying (6.7.9) if f is integrable and, for all sufficiently 
large m â¬ N, there exists
nm e N such that the following splittings hold:
with
where c(m) and uj(m) are functions of m and \va\m-,00 uj(m) = 0.

Section 6.7. 
Locally Toeplitz Matrices 
179
Given a sequence of matrices An, we will simply write
to indicate that (the sequence) An is locally Toeplitz with respect to a and /. 
It
is understood that each An has order n, that a is defined over [0,1], and that / is
defined over [â7r,7r] and is integrable; moreover, both a and / are supposed to be
complex valued, unless otherwise specified. We call a the weight function and / the
generating function; moreover, in the splittings (6.7.11), the matrices Rn,m are called
rank corrections, while A/n>m are called norm corrections.
In the last section we claimed that the matrix An in (6.7.2) is locally Toeplitz; in
light of the above definition, this was not quite correct, since being locally Toeplitz is
a property not of a single matrix but of a whole sequence of matrices {An} (which,
for simplicity of notation, still will be denoted by An). In other words, given a single
matrix An, it makes no sense to ask if it is locally Toeplitz. Nevertheless, if a sequence
{An} is locally Toeplitz and A G {.An}, we will improperly say that "A is a locally
Toeplitz matrix." This should not cause confusion, provided one bears in mind the
above definition.
With each locally Toeplitz sequence of matrices An a pair of functions (a, /) is
associated, each of which plays a different role. Intuitively, we regard a(x) as a weight
function, and f(&) as a generating function in the usual Toeplitz sense. The idea
underlying the splittings (6.7.11) is that each matrix An has, up to small-rank and
small-norm corrections, a sort of local Toeplitz structure modeled on T n ( f ) , varying
along each diagonal according to the weight function a.
Pondering over Def. 6.7.1 for a while, the reader will realize that it gives a precise
mathematical meaning to the intuitive remarks of the previous section. In particular,
T T
by explicitly building the splittings (6.7.11), the reader may check that An â> (a,/),
where An is the matrix in (6.7.2), a(x) is the weight function in (6.7.3), and / is as in
the example following Thm. 6.3.1.
LT
On trying to prove that An â> (a,/) when An is given by (6.7.2), the reader will
find out that building the splittings (6.7.11) can be a very difficult task. (However, with
LT
the aid of some more theory, the claim An â> (a, /) can be proved without explicitly
building the splittings (6.7.11), as we will see in the example following Cor. 6.7.2).
Therefore, we will state some theorems that can serve as a tool for generating nontrivial
locally Toeplitz sequences or for proving that a given sequence is locally Toeplitz.
Theorem 6.7.1 (Toeplitz Matrices are Locally Toeplitz). If f : [â7r,7r] iâ> C is
square integrable and Tn(f] 
is the sequence of Toeplitz matrices generated by f , then
Tn(f) is a locally Toeplitz with f as generating function and a(x) = I as weight function,
that is,
Observe that, by virtue of the last theorem, the notion of locally Toeplitz sequence
extends that of Toeplitz sequences of matrices.
Another nontrivial class of locally Toeplitz sequences is provided by the following
examole.

Asymptotic Spectral Distribution of Toeplitz-
180 
Related Matrices 
Chapter 6
Example (Sampling Matrices). Suppose a(x) is continuous over [0,1], and consider the
sequence of diagonal matrices
for n > 1. We claim that
To prove this, we explicitly build the splittings (6.7.11). Let us set
The rank condition in (6.7.12) is trivially satisfied letting c(m) = 0. Concerning the norm
condition in (6.7.12), we observe that Nntm is a diagonal matrix of order n, whose generic
diagonal element can be expressed as a(x) â a(t/), with \x â y\ < 4/rn; therefore, the Probenius
norm of ATn,m cannot exceed ^/nO(4/m), where 6 is the modulus of continuity of a:
Since lime_o #(e) = 0 (this follows from elementary analysis, since a is continuous over [0,1]
and hence uniformly continuous), the norm estimate in (6.7.12) is satisfied by letting u>(m) =
0(4/m).
According to Thm. 6.7.1 and the example above we observe that, in a sense which we
will soon make clear, Toeplitz sequences and sampling sequences are orthogonal: indeed,
the former are locally Toeplitz with weight function a(x) = 1, whereas the latter are
locally Toeplitz with generating function /(#) = 1.
Under certain reasonable assumptions, the termwise product of two locally Toeplitz
sequences is also locally Toeplitz. In the following theorem, we denote by \\A\\z the
spectral norm of the matrix A, that is, the largest among its singular values [Til98b].
LT 
~ 
LT
Theorem 6.7.2 (Product of Two Sequences). Suppose An â> (a, /) and An â>
(a, /), where a, a, /, / are bounded functions. If there exists a constant C > 0 such that
holds in the splittings (6.7.11), then it holds that
It can be shown (see [Til98b]) that the additional assumption (6.7.16) is always
satisfied by a sequence of Toeplitz matrices Tn(/), provided / is bounded. As a conse-
quence, when the generating functions are bounded, the product of a locally Toeplitz
sequence with Toeplitz sequences is also locally Toeplitz; in particular, the product of
any number of Toeplitz sequences is locally Toeplitz. (Observe that, in general, the
product of two Toeplitz matrices is not a Toeplitz matrix.)

Section 6.7. Locally Toeplitz Matrices 
181
T T
Corollary 6.7.1 (Product of Toeplitz Matrices). Suppose An â> (a, /), where a
and f are bounded. I f p i , . . . ,PJ and q\,..., q^ are all bounded functions, then letting
it holds that
T T
Observing that In â> (1,1) (where In is the identity matrix of order n) we obtain
the following.
Corollary 6.7.2 (Product of Toeplitz Matrices). Suppose /i,---,/fc are bounded
functions. Then it holds that
As an application of the above corollary, in the following example we show that the
matrices in (6.7.2) are locally Toeplitz, as we claimed in the last section.
Example. Suppose a(x) is continuous over [0,1] and let An be denned according to (6.7.2).
We want to prove that the sequence An is locally Toeplitz with respect to the pair of functions
(a,/), where /(0) = 2 - 2cos0.
For q(ff) = el6 â 1, consider the bidiagonal Toeplitz matrices
A direct computation shows that
where An is the sampling matrix (6.7.13) and e\ = (1,0, ...,0)* is the first vector of the
canonical basis of En. In particular, the difference
LT
has rank at most 1 for all n and hence, in order to show An â> (a, 2 â 2 cos#), it suffices to
show
Since trivially T*(q) = Tn(q"), from Thm. 6.7.1 and the example following it we obtain
5
and hence from Cor. 6.7.1 it follows that
Finally, it suffices to observe that q*(0)q(0) = 2 â 2 cos0.

Asymptotic Spectral Distribution of Toeplitz-
182 
Related Matrices 
Chapter 6
6.7.2Spectral Distribution of Locally Toeplitz Sequences
It turns out that locally Toeplitz sequences have singular values and (in the Hermitian
case) eigenvalues nicely distributed, according to weighted Szego formulas of the type
(6.7.7).
Concerning singular values, the following result can be proved [Til98b].
Theorem 6.7.3 (Weighted Szego Formula for Singular Values). Suppose the se-
LT
quence of matrices An is such that An â> (a, /), where a is a piecewise continuous
weight function. 
Then for any test function F, continuous with bounded support, it
holds that
Example. We know from the last example that the matrices An in (6.7.2) are locally Toeplitz
with respect to the pair (a,/), where a is given by (6.7.1) and /(#) = 2 â 2cos0. Therefore,
using Thm. 6.7.3 we are able to compute the asymptotic distribution of the singular values of
the matrices An, as follows:
We remark that the above formula holds also when a(x) in (6.7.1) is not real but complex
valued, since Thm. 6.7.3 does not require the involved matrices to be Hermitian.
Example. Consider An(a) given by (6.7.13). Since An(a) is diagonal, its singular values are
LT
If a is continuous, we know from the example following Thm. 6.7.1 that An(a) â> (a, 1).
Therefore, Thm. 6.7.3 can be applied, and (6.7.19) reduces to
that is, the Riemann sums of the continuous function F(|a(aj)|J converge to its Lebesgue
integral.
Given a square matrix A, we let
Observe that Re A and Im A, respectively, reduce to the usual notion of real and imag-
inary part when A has order 1 (i.e., when A is a complex number): therefore, we say
that Re A and Im A are the real and, respectively, the imaginary part of the matrix A.
(It is easily seen that Re A and ImA are always Hermitian matrices.)
Concerning the eigenvalues of a locally Toeplitz sequence, it is possible to prove a
weighted Szego formula for the eigenvalues of the real and the imaginary part of the
sequence and of the sequence itself when it is Hermitian [Til98b].

Section 6.7. 
Locally Toeplitz Matrices 
183
Theorem 6.7.4 (Weighted Szego Formulas for Eigenvalues). Suppose a sequence
T T1
of matrices An satisfies An â> (a, /), and suppose the weight function a is piecewise
continuous. Then for all test functions F, continuous with a bounded support, it holds
that
//, moreover, each An is Hermitian, then it holds that
The right-hand sides of (6.7.19) and (6.7.23) can be thought of as weighted versions of
the right-hand sides of (6.3.17) and (6.3.13); in particular, when a(x] = 1 the integrals in
(6.7.19) and (6.7.23) reduce to those in (6.3.17) and (6.3.13), respectively. This analogy
is more than a formal one; indeed, according to Thm. 6.7.1, if / is square integrable,
LT
then Tn(/) â> (I,/). In view of this, when / is square integrable, Thm. 6.3.5 can be
obtained as a corollary of Thm. 6.7.3 and, similarly, Thm. 6.3.3 can be obtained as a
corollary of Thm. 6.7.4.
Example. 
Let us consider again the matrices An in (6.7.2). Suppose that a(x) is complex
valued and continuous, and define the two real-valued functions it, v according to
a(x) = v(x) + iw(x), 
v(x) = Rea(x), 
w(x) = lma(x).
LT
Prom the example following Cor. 6.7.2 we know An âÂ» (a,/), where f ( 6 ) = 2 â 2cos0.
Therefore, applying Thm. 6.7.4, from (6.7.21) and (6.7.22) we obtain
In the particular case where a is real valued (so that An is Hermitian), from (6.7.23) we have
Suppose a sequence of Hermitian matrices An is locally Toeplitz with respect to a
pair of functions (a,/); then, according to Thm. 6.7.4, formula (6.7.23) holds for all
F. The right-hand side of (6.7.23) deserves some remarks: indeed, the test function F
is defined on the real line, and hence F(z) makes sense only when z is a real number;

Asymptotic Spectral Distribution of Toeplitz-
184 
Related Matrices 
Chapter 6
on the other hand, in Thm. 6.7.4 there seems to be no assumption on a and / as to a
guarantee that the product a(x)f(0) is real.
However, it turns out that the assumption that each An is Hermitian forces the
product function
to be real valued almost everywhere (with respect to the bidimensional Lebesgue mea-
sure) over the rectangle [0,1] x [âTT, TT], and hence the integral in (6.7.23) is well defined.
This fact can be easily proved with the aid of the following result [Til98b].
Theorem 6.7.5 (Uniqueness of Representation). Suppose that a sequence of ma-
trices An can be split, in two different ways, as the sum of p locally Toeplitz sequences
where all the weight functions are piecewise continuous and the generating functions
are integrable. Then for all (y,x) â¬ [0,1] x [âTT, TT], except at most on a subset of null
measure, it holds that
LT 
LT
In particular, if An â> (Â«>/) and An is Hermitian, then trivially ^4* â> (a*,/*)
LT
and hence also An â> (a*,/*). Therefore, the sequence An is locally Toeplitz with
respect to the two pairs of functions (a,/) and (a*,/*). Prom the previous theorem,
applied with p = 1, we obtain that
or, which is the same, that a(x)f(0) is a real number for almost every pair (x, 6} 6
[0,1] x [â7r,7r], so that the integral in (6.7.23) makes sense. Observe, however, that
from the fact that a(x}f(9) is real almost everywhere it does not follows that a(x) and
f(6) are real valued. (For example, both a and / might always take imaginary values.)
In many applications, it often happens that we need to deal with a sequence of
matrices that perhaps is not locally Toeplitz but can be split into the the sum of a
certain number of locally Toeplitz sequences. In this case, the following results (which
generalize the last two theorems) can be applied [Til98b].
Theorem 6.7.6 (Sum of Locally Toeplitz Matrices). Suppose that a sequence of
matrices An can be split as the sum of p locally Toeplitz sequences

Section 6.7. Locally Toeplitz Matrices 
185
where each ai is piecewise continuous. Then for all F e CQ it holds that
Theorem 6.7.7 (Spectral Distribution Results). Under the same assumptions and
notation as in the previous theorem, for all F 6 CQ it holds that
If, moreover, each An is Hermitian, then it also holds that
Unlike Toeplitz sequences, the sum of two locally Toeplitz sequences may fail to be
locally Toeplitz; therefore, the last two theorems by no means follow as consequences
of Thms. 6.7.3 and 6.7.4.
Observe that when An is Hermitian, the integral in (6.7.26) is well defined, since the
sum
is a real number almost everywhere. (This can be proved without difficulty relying on
Thm. 6.7.5, reasoning in exactly the same way as we did for the case p = 1.)
Example (Algebra Generated by Toeplitz Sequences). In [Tyr94a], Tyrtyshnikov proved
that the singular values of a finite product of Toeplitz sequences are distributed as the magni-
tude of the product of their generating functions; that is,
whenever all the /i's are bounded. Observe that we can obtain this result as a consequence of
Cor. 6.7.2 and Thm. 6.7.3; moreover, we can extend it to handle all sequences in the algebra
generated by Toeplitz sequences, as follows.
Let T be the vector space of all Toeplitz sequences Tn(/) with a bounded generating
function /, and let A be the algebra of sequences generated by 7'. It is clear that any sequence
An â¬ A can be written as

Asymptotic Spectral Distribution of Toeplitz-
186 
Related Matrices 
Chapter 6
(If necessary, one can let fap = 1 for some indices a, /3, observing that the identity matrix
Tn(l) does not affect the products.) From Cor. 6.7.2 we know that, for each a,
and hence from Thm. 6.7.6 (omitting integration with respect to x, since a(x) = 1) we can
conclude that
Concerning eigenvalues, in a similar way from Thm. 6.7.7 we obtain
6.8 
CONCLUDING REMARKS
Prom the results discussed in this chapter it appears that the theory of the spectral
distribution of Toeplitz-related matrices is quite well developed; however, the research
in this field is far from being exhausted. There are, in fact, many interesting questions
that deserve a deeper investigation.
If / : (â7r,7r) iâ> R is measurable and bounded, then the Szego formula implies (see
[GS84]) that
for any natural number k. In other words, for any fc, the smallest k eigenvalues of
Tn tend to inf /, whereas the largest k eigenvalues tend to sup /, as n tends to infinity.
Observe that, according to Thm. 6.3.2, all the eigenvalues of any Tn lie inside the interval
[inf /, sup /]. Moreover, since Tn is a principal submatrix of Tn+i, the minimax principle
(see, for example, [HJ85]) implies that the sequence {Afc(Tn)}n is nonincreasing for any
fixed k, and therefore the existence of the limit in (6.8.1) is trivial. (A similar argument
applies to (6.8.2).)
A question that arises quite naturally is, How fast is the convergence in (6.8.1) and
(6.8.2)?
In the case where inf / = 0 and / has a unique zero the convergence rate of Ai(Tn)
is strictly connected with the order of the zero; roughly speaking, if / is sufficiently
smooth in a neighborhood of the zero and if the zero has a finite order 0 > 2, it can be
proved that Afc(Tn) is O(l/ne] (see [GS84], [HH77], and Ch. 5).

Section 6.8. Concluding Remarks 
187
Example. Consider /(0) = 2 - 2cose. Then /(0) > 0, inf / = 0, and /(0) = 0 only at 0 = 0.
Prom Taylor's expansion we obtain
and therefore 6 = 0 is a zero of order two. On the other hand, we know from the example
following Thm. 6.3.1 that
When k is fixed and n is large, again from Taylor's formula we obtain
and hence, for fixed k, \k(Tn) is O(l/n2).
Knowing how fast Ai(Tn) tends to inf/ as n ââ¢> oo is important in applications,
especially when inf / = 0 and 0 < sup/ < +00; indeed, in this case we know from
Thm. 6.3.2 that each Tn is positive definite and hence
Therefore, knowing how fast Ai(Tn) tends to zero allows one to estimate the condition
number of Tn.
It would be desirable to obtain precise estimates on the behavior of Afc(Tn) in more
general cases, for example, when / is not a smooth function, or when it is multivariate,
matrix valued, and so on. Also the behavior of crfc(Tn) should be investigated in more
detail. (For some new results in this direction, see [BG97] and the references therein.)
We remark that, on investigating such questions as the behavior of the kth eigenvalue
or singular value of a sequence of matrices, a Szego-type formula alone is not sufficient,
since it provides only global information on the asymptotic spectra of the involved
matrices; in other words, investigating the behavior of the kth eigenvalue requires a
finer analysis of the spectrum.
To conclude, we recall that there are several classes of structured matrices, in ad-
dition to Toeplitz matrices, that can be associated with the Fourier coefficients of a
generating function; among such classes, we mention Hankel and circulant matrices (see
[Ioh82], [Dav79], and Ch. 1).
For sequences of Hankel or circulant matrices some Szego-type formulas hold, and a
spectral theory similar to that of Toeplitz matrices can be developed (see, for example,
[Tyr96a], [Til97a]); in view of that, the importance of a general theory for the spectral
distribution of a sequence of matrices is apparent. The first step explicitly taken in this
direction is by Tyrtyshnikov, in the fundamental paper [Tyr96a].

This page intentionally left blank 

Chapter 7
NEWTON'S ITERATION FOR
STRUCTURED MATRICES
Victor Y. Pan
Sheryl Branham
Rhys E. Rosholt
Ai-Long Zheng
7.1NTRODUCTION
Preconditioned conjugate gradient (PCG) methods are quite effective for the solution of
a large class of Toeplitz and Toeplitz-like linear systems of equations. In a few iterations,
they provide an approximate solution and then improve the approximation with linear
convergence rate (cf. the discussions in Chs. 5 and 6). While this performance is suffi-
cient in several applications, there are many cases where it is still desirable to compute
a highly accurate solution at a faster convergence rate, assuming that a rough initial
approximation has been computed by a PCG algorithm. Alternatively, any Toeplitz or
Toeplitz-like linear system of equations can be solved effectively by known direct meth-
ods (e.g., [AG88], [Bun85], [GKO95], [Gu95a], and also Chs. 1-3 of this book). The
references [GKO95], [Gu95a] extend the general approach of [Pan90] to yield practical
Toeplitz and Toeplitz-like solvers, but in many cases the computed solution must be
refined to counter the effect of rounding errors.
In this chapter we rely on the techniques of [Pan93a], [Pan93b], [Pan92a], [Pan90],
[PZHD97], and on their variations, to compute the desired refinement by means of
Newton's iteration. We describe and analyze four quadratically convergent algorithms
for the refinement of rough initial approximations to the inverses of n x n nonsingular
Toeplitz and Toeplitz-like matrices and to the solutions of nonsingular Toeplitz and
Toeplitz-like linear systems of n equations. Our first two modifications of Newton's
iteration (Algs. 7.4.1 and 7.4.2) exploit the displacement structure of the input matrices
to simplify the computations. Our third and fourth modifications (Algs. 7.5.1 and 7.5.2)
rely on the inversion formulas known for Toeplitz matrices and simplify the first two
algorithms. By exploiting matrix structure, each iteration step requires O(nlogn) op-
erations.
Algorithm 7.4.1 is a little more costly to perform but yields convergence under
189

190 
Newton's Iteration for Structured Matrices 
Chapter 7
substantially milder assumptions on the initial approximation than Alg. 7.4.2. Algo-
rithm 7.5.2 runs roughly twice as fast as Alg. 7.5.1 and does not require any stronger
initial assumptions. Algorithms 7.4.1 and 7.5.1 are more convenient to apply when the
triangular Toeplitz representation is used for the input and output matrices, whereas
Algs. 7.4.2 and 7.5.2 are more convenient to apply when a factor-circulant (/-circulant)
representation is used. As we will point out later, some of our algorithms can be ex-
tended to other classes of structured matrices.
We may mention that all four algorithms require an initial approximation that lies
sufficiently close to the desired solution (see (7.4.6), (7.4.11), (7.5.9)), since otherwise the
iterations may converge too slowly or even diverge. A partial remedy can be obtained
by means of homotopy techniques (cf. our remark at the end of Sec. 7.4.2 and also the
discussion in [Pan92a]).
Algorithms 7.4.1 and 7.4.2 are presented as Newton's iteration for matrix inversion
and Algs. 7.5.1 and 7.5.2 as residual correction for linear systems. In Sec. 7.5 we show
the equivalence of Newton's iteration and residual correction in our cases. Furthermore,
the inversion and linear system solving are closely related in the Toeplitz and Toeplitz-
like cases, where a displacement generator of the inverse matrix A~l is produced by
the solution of two or a few linear systems Ax. = b^ and the solution to Ax. = b is
immediately obtained from a displacement generator of A~l. Since the approximation
to the inverse is improved together with the improvement of its displacement generator
in each recursive step, our residual correction processes converge quadratically, rather
than linearly. Newton's iteration has an algebraic version [Pan92b], [BP93], [BP94,
pp. 189-190] that we will not cover here.
7.2 
NEWTON'S ITERATION FOR MATRIX INVERSION
Newton's iteration is a major tool for solving a nonlinear equation f ( x ) = 0. Let XQ
be an initial approximation to a solution x. Then the iteration successively produces a
sequence of iterates
with errors &i = \Xi â x\, i = 0,1,.... If f ( x ) is smooth enough in a neighborhood
of x and if CQ is small enough, then the approximations converge to the solution with
quadratic convergence rate [Atk78], i.e.,
For example, suppose that we wish to apply Newton's iteration to approximate I f t
for a given positive binary t of the form
where e is an integer, t\ = 1, and ti Â£ {0,1} for i > 2. By choosing XQ = 2~e and
/(#) = t â 1/x, we obtain
and Newton's iteration takes the form

Section 7.2. Newton's Iteration for Matrix Inversion 
191
It follows that
Recursively, we obtain that 0 < 1 â tXi = (1 â txo)2' < 2~2*. Therefore,
and the iteration very rapidly (quadratically) converges to 1/t.
Assume t = (0.101)2 = 1/2 + 1/8. Then the results of performing four steps of
Newton's iteration on a computer with 8-decimal precision are shown in Table 7.1
(where the values of 1 â txÂ± and 1/t â #4 are shown without several decimal digits lost
at the stage of the subtraction of txÂ± = 0.99999999 from 1).
Table 7.1. Newton's iteration for the reciprocal of 5/8.
i 
Xi 
1 â tXi 
l/t â Xi
~0~ 
l.OOOOOOOe + 00 " 3.7500000e - 01 
6.0000000e - 01"
1 
1.3750000e + 00 
1.4062500e - 01 
2.2500000e - 01
2 
1.5683594e + 00 
1.9775391e - 02 
3.1640649e - 02
3 
1.5993743e + 00 
3.9106607e - 04 
6.2572956e - 04
4 
1.5999998e + 00 
1.3411045e - 07 
2.3841858e - 07
Remark. According to [Boy68], and despite Newton's name, the origin of this iteration can
be traced back four millennia, when the ancient Babylonians applied it in rudimentary form
to solve quadratic equations. For the solution of polynomial equations of higher degree, the
iteration was routinely used by Chinese and Arab mathematicians in medieval times. Among
European predecessors of Newton, F. Viete should be mentioned. The present-day version
of Newton's iteration was given by Joseph Raphson in 1690. We use Newton's name to be
consistent with the commonly accepted (although inaccurate) terminology.
Newton's iteration can also be effectively applied to some matrix equations of the
form f ( X ) = 0, where X is now a matrix rather than a scalar. We demonstrate this fact
by considering the inversion of an n x n nonsingular matrix A by Newton's method. The
idea of using Newton's method for such an application can be traced back to [Sch33]; a
more recent treatment can be found in [PS91].
Assume that a matrix A is given along with an initial approximation â XQ to its
inverse A~l satisfying the bound
for some positive r and some fixed matrix norm. The initial approximation â XQ to
A~l can be obtained by some other computational procedure, for example, by a PCG
algorithm, by an exact solution algorithm (see, e.g., the discussions in Ch. 1 and also
[GKO95], [Gu95a]), or by the homotopy algorithm of [Pan92a]. By choosing f ( X ) =
A + -X""1, Newton's iteration becomes
(Here, we deviated slightly from the customary notation of Newton's iteration by writing
âXi instead of Xi to denote the computed approximations to the inverse matrix A~l.)

192 
Newton's Iteration for Structured Matrices 
Chapter 7
It follows from (7.2.3) that
Consequently,
so that the norm of the residual matrix / + XiA is bounded from above by r2*. This
establishes quadratic convergence of the iteration. Moreover, given a desired residual
norm bound e > 0, the bound
is guaranteed in |"log2(j^f)] recursive steps (7.2.3), where \x] stands for the smallest
integer not exceeded by x. We also deduce from (7.2.4) that
and it follows that
so that the matrix â Xi approximates A~l with a relative error norm less than e.
Each step of the iteration (7.2.3) involves the multiplication of A by Xi, the ad-
dition of 2 to all the diagonal entries of the product (which takes n additions), and
the premultiplication of the resulting matrix by Xi. The most costly steps are the two
matrix multiplications, each taking n3 multiplications and n3 â n2 additions for a pair
of general n x n input matrices A and XQ.
The next example demonstrates the advantage of the quadratic convergence rate
over the linear convergence rate.
Example (Comparison with Linearly Convergent Schemes). Let r = 1/2 and e =
10~6. Then 5 iteration steps (7.2.3) suffice to compute the matrix X$ satisfying the bounds
\\I + X5A\\ < f. and H^"1 â (âXÂ§)\\ < e\\A~1\\. On the other hand, assume any linearly
convergent scheme such as Jacobi's or Gauss-Seidel's iteration is used (cf. [GV96, p. 510]),
where for the same input, the norm of the error matrix of the computed approximation to A"1
decreases by a factor of 2 in each iteration step. Then the error norm decreases to below 2~l~l
in i iteration steps, so that it takes 19 steps to ensure the bound (7.2.4) for e = 10~6.
7.3 
SOME BASIC RESULTS ON TOEPLITZ-LIKE MATRICES
When A and XQ are structured matrices, say Toeplitz or Toeplitz-like matrices, the
computations required for Newton's iteration can be carried out more efficiently. First,
however, let us review some basic facts about Toeplitz-like matrices and introduce some
notation that will be useful in what follows. More details can be found in Ch. 1.
As defined in Ch. 1, a matrix R e Rnxn is said to be Toeplitz-like if its displacement
VzR = R â ZRZT has low rank, say, r Â«C n, where Z is the lower triangular shift
matrix,

Section 7.3. 
Some Basic Results on Toeplitz-Like Matrices 
193
(Here and hereafter, WT denotes the transpose of a matrix or a vector W.) Hence there
exist (so-called generator) matrices G and H 6 Rnxr such that
It is further shown in Ch. 1 that if a nonsingular matrix R satisfies a displacement
equation of the form (7.3.1), then its inverse satisfies a similar equation of the form
for some (generator) matrices C and D â¬ Rnxr. Notice that the roles of {Z,ZT} are
switched. To distinguish between these two kinds of displacement equation, we shall
use the notation (cf. [KKM79a])
That is, we write V+(-R) instead of Vz-R and V-(R) for the other displacement where
ZT comes first. We also denote the corresponding generator matrices by G+,H+,G-,
H- e Rnxr,
The following basic facts are immediate (see Ch. 1 and also [KKM79a]).
Lemma 7.3.1 (A Basic Fact). Given matrices G,H 6 Rnxr, there exist a unique
matrix T+ and a unique matrix T_ satisfying
Indeed, T+ and T_ are given by
where &i and b^ denote the ith columns of G and H, respectively, Â£(v) denotes a lower
triangular Toeplitz matrix whose first column is v, and I denotes the (reflection) matrix
with ones on the antidiagonal and zeros elsewhere; that is, I = (ig,h)^Loi ig,h â 1 if
g + h â n - 1, ig,h = 0 otherwise, so that f[v0, vi,..., vn-i]7' = [vn-i, â¢ â¢ â¢ â¢, VQ]
T for any
vector [u0,t>i,-..,Vn-i]T-
Lemma 7.3.2 (Properties of the Displacement Operators). 
The displacement
operators V_ and V+ satisfy the following properties for any nonsingular matrix R
and for any pair ofnxn matrices X and Y:
(a) rank[V+(JR)] =rank[V_(^-1)].
(b) rank[V_(Xy)j < rank[V_(X)j + rank[V_(y)] -f 1.
(c) Given two pairs of generator matrices of lengths a and b for V-(X) and V-(Y),
respectively, a pair of generator matrices of length at most a -f- b + 1 for V_ (XY)
can be computed using O(nab log n) flops.

194 
Newton's Iteration for Structured Matrices 
Chapter 7
(d) Given a pair of generator matrices of length r for V+(X), a pair of generator
matrices of length f < r + 2 for V_(X) can be computed using O(nr logn) flops.
Proof: For parts (a) and (d) see Ch. 1, [KKM79a], and pp. 176-178 of [BP94]. For
part (b), we first write
Then
We also observe that ZZT â I = âeoe^, where e^ = [1,0,..., 0]T is a unit coordinate
vector. Therefore, we can write V-(XY) = GxyHxy, where
These expressions provide a generator (GxYi HXY) fÂ°r V-(XY) via the three genera-
tors: (Gx,Hx) of the displacement V_(X), (Gy,Hy) of the displacement V_(Y), and
(-ZTXe0,ZTYTe0) of the rank 1 matrix ZTX(ZZT-I}YZ. This establishes part (b).
Finally, observe that the computation of GXY and HXY requires the multiplication
of X by ZGy and of YT by HX- 
By estimating the computational cost of these
multiplications, and by applying Lemma 7.3.1 and recalling that an n x n Toeplitz
matrix can be multiplied by a vector in O(nlogn) flops (cf. Sec. 1.4), we establish
part (c).
7.4 
THE NEWTON-TOEPLITZ ITERATION
We now describe the first version of Newton's iteration (7.2.3) in the Toeplitz-like case.
Until the end of Sec. 7.4.2, we shall assume that both A and XQ are structured matrices
with
and that the matrices A and XQ are given with their respective V_-generators of min-
imum length f and r*o, respectively, where f < r + 2 by Lemma 7.3.2(d) and r and TO
are small relative to n. Then, in view of the above results, a displacement generator for
Xk having length at most f^ = 2fcro + (2* â l)(f + 3) can be computed at the overall
cost of O(n(ffc_i -f f)ffc-i log n) flops, k = 1,2, â The values f^ and Y^i=o fa + r)fi
have orders of 2
fc and 4fc, respectively, so that the iteration (7.2.3) soon becomes quite
costly.
In some cases, a few steps (7.2.3) may already suffice for the refinement of a given
approximation XQ to -A"1, and then the overall computational cost is small enough. If
one needs many steps (7.2.3), then the techniques of the next two sections will help us
to control the growth of the generator length and the computational cost.
Let us outline these techniques. By assumption, rank(V_(A-1)) = r. Hence the
matrices â Xi, which approximate A~l closely for larger i, have a nearby matrix of V_-
displacement rank r for larger i. This fact suggests the following approach to decreasing
the computational cost of the iterative scheme (7.2.3): shift from Xi to a nearby matrix

Section 7.4. The Newton-Toeplitz Iteration 
195
Y; having displacement rank at most r and then restart the iteration with Yi instead
o f X i .
The advantage of this modification is the decrease of the computational cost at
the iih Newton's step and at all subsequent steps. The disadvantage is a possible
deterioration of the approximation, since we do not know A'1, and the transition from
Xi to Yi may occur in a wrong direction. Both Xi and Yi, however, lie close to A~l and,
therefore, to each other. The latter observation enables us to bound the deterioration
of the approximation relative to the current approximation error, so that the quadratic
improvement in the error bound at the next step of Newton's iteration will immediately
compensate for such a deterioration. Moreover, the transition from Xi to a nearby
matrix Yi of a small displacement rank can be achieved at a low computational cost.
We will supply more details in the next two sections.
7.4.1Bounding the Displacement Rank
To estimate the errors of the approximations of A~l by Xi and Y^, we first recall the
following basic result (cf. [GV96, pp. 72, 230]).
Lemma 7.4.1 (Optimal Matrix Approximation). Given a matrix W of rank n, it
holds that
ar= 
min 
\\W - B\\2;
rank (B)<r
that is, the error in the optimal choice of an approximant of W whose rank does not
exceed r is equal to the rth singular value ofW. Moreover, the condition number ofW
is equal to the ratio o~i/crn.
The reader may assume below that the numbers r and TO are small (r = r â 2 for a
Toeplitz matrix A) and that rÂ» is not much larger than r, say, r* < 3r + 3. Hereafter,
until the end of Sec. 7.4.2, we will use the displacement V-(R) = R â ZTRZ.
Algorithm 7.4.1 (Bounding the Displacement Rank of Newton's Iterates). The
following procedure bounds the displacement rank of the successive iterates as follows.
â¢ Input: A positive integer r = rank(V+(.A)) = rank(V_(A-1)) and a displacement
generator {Gi, Hi}, of length at most rÂ» > r, for a matrix Xi, such that V-(Xi) =
GiH?.
â¢ Output: A displacement generator of length at most r for a matrix Yi such that
\\Yi + A-%<(l + 2n(ri-r})\\Xi + A-1\\2.
â¢ Computations:
1. Compute the SVD of the displacement V_(XÂ») = U^Y?'. This step is not
costly (cf. [Pan93b]j since it is performed for GiHf, where Gi,Hi <G realnxri,
and since rÂ» is small.
2. Set to zero the Ti â r smallest singular values ofV-(Xi) 
in the matrix in Si,
thus turning E^ into a diagonal matrix of rank r.

196 
Newton's Iteration for Structured Matrices 
Chapter 7
3. Compute and output the matrices Gi and Hi obtained from the matrices UiEi
and Vi, respectively, by deleting their last n â r columns.
The overall computational cost of this algorithm is O(n r?) flops. Of course, this
covers only the computations that are part of a single step of Newton's iteration.
Now let Yi e Rnxn denote the unique matrix defined by the V_-generator {Gi, Hi}.
The accuracy of the above algorithm is implied by the following result of [Pan93a],
[Pan93b], [Pan92a], which shows that the matrix Yi approximates A~l almost as well
as Xi does.
Theorem 7.4.1 (Accuracy of Approximation). It holds that
Proof: The proof of this basic result was given in [Pan93a], [Pan93b], [Pan92a] and
relies on the following observations. The matrices A~l and â Xi closely approximate
each other; therefore, so do their V_-displacements, V_(>1~1) and V-pQ). Since the
approximation of V-(J'Q) by V_(Yi) is optimal by Lemma 7.4.1, we have
The map from the V_-displacements V-(Yi),V-(Xi},V-(A~1) 
back to the matrices
Yi,Xi,A~l may change this bound by at most a factor of 1 4- 2(r^ â r}n.
We may now modify Newton's iteration (7.2.3) by incorporating the construction of
Alg. 7.4.1. Let a matrix A and an initial approximation X0 of A~* be given with their
V_-generators of length f < r + 2 and r*o, respectively. We recursively compute the
matrices YQ, X\, YI, X%,..., satisfying
where Yi denotes the output matrix of Alg. 7.4.1 applied to Xi. Thus, for each i, we
first apply Alg. 7.4.1 to the matrix Xi followed by the computation of the matrix Xi+\
based on (7.4.2). Since Yi is a Toeplitz-like matrix represented with its displacement
generator of length at most r, the overall computational cost is O(nr2log n) for each
step of (7.4.2).
7.4.2 
Convergence Rate and Computational Complexity
Define
We then deduce from (7.4.2) that / + Xi+iA = (I + YiA)2, so that
Here and hereafter, we write ph(i] to denote (p(i))h. By Thm. 7.4.1, we obtain

Section 7.4. The Newton-Toeplitz Iteration 
197
and hence
where K,(A) denotes the condition number of A and is equal to HAI^H-A"1^. Therefore,
for a positive 6, the inequality
implies a convergence rate of 1 + b, that is, it implies the bound
By observing that rank [V_ (R + I)] < 1 + rank [V_ (R)] for any matrix R and by
applying Lemma 7.3.2(c) to the matrices of (7.4.2), we obtain that
where the length of the V_ -generators of Yi and A is at most r and f, respectively. If
p(i) < I and p(i} satisfies (7.4.4), then (7.4.5) implies that p(i + 1) < p(i}. Thus it
suffices to assume (7.4.4) with p(i] replaced by p(Q) and TI â r replaced by r + f + 3.
The results are summarized below (cf. [Pan93a], [Pan93b], [Pan92a]).
Theorem 7.4.2 (Convergence Rate). Let rank[V_(A~1)] < r and let the matrices
XQ and A be given with their V--displacement generators of length r$ <r andf < r-f-2,
respectively. Let
/orp(O) of (7.4.3) (fori = 0) and for some fixed positive b < 1. Then, fori > 0 we have
and the matrices YI, Xi, Y2, X%, ... , Yi, Xi can be computed at the overall cost of
O(inr2 log n) flops.
We can see that unless the matrix A has a very large condition number (that is,
unless A is very ill-conditioned), (7.4.6) is a mild assumption on the residual norm p(0).
This assumption is sufficient to ensure a rapid improvement of the initial approximation
of A~l by â XQ at a low computational cost of O(inr2 log n) flops. Therefore, the bound
Pi = ||7 + -X"iA||2 < â¬K(A) is ensured already in
Newton-Toeplitz steps (7.4.2).

198 
Newton's Iteration for Structured Matrices 
Chapter 7
Remark. If the initial approximation Xo does not satisfy the bound (7.4.6), various homo-
topy techniques can be remedies. The most straightforward remedy is to apply our algorithm
recursively to compute the matrices
where AQ is a readily invertible matrix, to = 0, IK = 1, and a sufficiently large natural K and
an increasing sequence <o,*i, â¢ â¢ â¢ ,<K are chosen to ensure rapid convergence of our algorithm
in each recursive step. Similar treatment can be applied to other algorithms of this chapter,
and we refer the reader to [Pan92a] for a particular application of homotopy techniques to the
design of fast Toeplitz-like solvers.
7.4.3 
An Approach Using /-Circulant Matrices
We now present an alternative Newton-Toeplitz iteration by using other displacement
operators. Indeed, as already noted in Ch. 1, the displacement structure of Toeplitz-like
matrices can be represented in terms of other operators, e.g.,
where / ^ 0 is a scalar and Zf is a unit /-circulant matrix,
Here and hereafter, GO = [1,0,..., 0]T and en_i = [0,..., 0,1]T denote two unit coordi-
nate vectors. (Circulant matrices are 1-circulant.)
Since Zf = Z + /eoe^lj, we have
for any triple (A, /,<?). Lemmas 7.3.1 and 7.3.2 are easily extended (cf. [GO92] and alsc
pp. 182-195 of [BP94]), and so is Alg. 7.4.1. In particular, we will use the next result.
Lemma 7.4.2 (Matrix Representation). If e ^ f , e ^ 0 (say, f = 1, e = âI), ana
if
then we have
where

Section 7.4. The Newton-Toeplitz Iteration 
199
denotes the f-circulant matrix with first column v = [UQ, ..., vn-i]T, and Z/^c(v) de-
notes the f-circulant matrix with last column v.
The complication due to the extra term Zfjc(Aen-\} in the representation of Lemma
7.4.2 (versus the one of Lemma 7.3.1) is compensated by the simplification of the multi-
plication of the matrices Zf (v) by a vector versus the same operation with the matrices
Â£(v) or Â£T(v). (The former multiplication requires roughly twice fewer flops than the
latter.)
The matrices Zf are nonsingular, and we may use the operators V-^(-A) = ZJ1A â
AZ?/f instead of V/(A) (cf. [HR84], [Pan92b]). This enables a distinct version of New-
ton's iteration, which we will demonstrate next. (Note that a generator {G,H} for
V f ( A ) immediately turns into the generator ZJ1G,H for V-^(-A) and vice versa.) In
particular, Lemma 7.4.2 turns into the following result.
Lemma 7.4.3 (Alternative Representation). Ife^f,e^0 (say, f = 1, e = â I),
and if
then we have
where Zf(v) and Z//c(v) are defined as in Lemma 7.4.2.
For our numerical tests, we chose / = 1 and e = â 1, although other choices of / and
e with / ^ e, e ^ 0 are also possible. (Moreover, the operators V^ can be replaced by
V+(A) = ZA-AZ or V~(4) = ZTA- AZT, to which Lemma 7.4.2 is easily extended;
see p. 184 of [BP94].)
Now suppose that V/(A) = GHT', write X = -A~l, and deduce that V/(J\T) =
GXHT = XVf(A)X = XGHTX so that
Indeed,
where the second-to-last equation follows because Zf., = ZJ1.
Next, based on (7.4.9) and Lemmas 7.4.2 and 7.4.3 we will modify Alg. 7.4.1. (To
distinguish from Alg. 7.4.1, we use the notation Xi rather than Xi for the computed
approximations to X = âA"1, i = 1,2,..., and, as in the case of Alg. 7.4.1, we assume
that an initial approximation XQ is given from outside, in this case in the form of the
V-^-displacement generator matrices GQ, HQ, and the last column vector Xoen_!.)

200 
Newton's Iteration for Structured Matrices 
Chapter 7
Algorithm 7.4.2. (Bounding the Displacement Rank of Newton's Iterates).
The procedure below bounds the displacement rank of the successive iterates as follows.
â¢ Input: A complex f , \f\ â 1, a natural f, and two matrices A and Xi given with
their V^-displacement generators, G = GA, H = HA and Gi = G^., Hi = H^.,
respectively, both of length at most f, and with their last columns, Aen-\ and
Xien-i, respectively.
â¢ Output: A V-^-displacement generator {C?i+i, Hi+i} of length at most f for a
matrix Xi+i and its last column Xi+ien-i satisfying
(cf. Remark in App. 7.A, and where cond\(A] denotes \\A\\\\\A~l\\i).
â¢ Computations: Apply Lemma 7.4.3 to express A and Xi through the input param-
eters. Then compute and output the vector
and the matrices
Lemma 7.4.3 reduces the computations of Alg. 7.4.2 to a sequence of 6f(2f + 1)
multiplications of n x n fo-circulant matrices by vectors for h = f and h = l/e, 2(3f+l)f
additions and subtractions of n-dimensional vectors, and 6f multiplications of such
vectors by e/(e â /), that is, to 0(f2nlogn) flops. Note that the computation of SVDs
is not required in this case.
In App. 7.A, we prove that the output satisfies the bound (7.4.10). This bound
immediately implies both accuracy of the algorithm and convergence of Xi to X with
the rate 1 + b for a positive b < 1 provided that
which extends (7.4.6) and Thm. 7.4.2. The comparison with (7.4.6) shows that a little
stronger upper bound on the error norm of the initial approximation is needed now for
the convergence of the refinement process, but if such a stronger bound is achieved,
then the computation by Alg. 7.4.2 is simpler since the displacement rank of Newton's
iterates is controlled more directly, without involving the SVDs of the displacements.
7.5 
RESIDUAL CORRECTION METHOD
7.5.1 
Application to Matrix Inversion
Newton's iteration is a special case (where / = 2) of the following more general residual
correction algorithm:
where Ri = I + Xi^A. It can be easily deduced [IK66, p. 82] that Ri+i = R{ and,
therefore, pi = \\I + XijA\\ < p\_l < p%. It is a tedious but straightforward exercise to
extend our Algs. 7.4.1 and 7.4.2 and their analysis to the case of I > 2. (See [IK66] for
some advantages of choosing / = 3 in the case of a general unstructured matrix A.)

Section 7.5. 
Residual Correction Method 
201
7.5.2 
Application to a Linear System of Equations
Newton's method can be applied to the solution of a fixed linear system of equations
as well. If Xi and w; denote the current approximations to the matrix X = â A~l
and to the solution w = A~lb of a linear system Aw = b, respectively (computed in i
recursive steps), then the next approximation,
satisfies w âwi+1 â (/ + XiA)(w â w^). Consequently, for any fixed operator norm (for
matrices and vectors), we have
Actually, (7.5.1) is just a restriction of Newton's iteration (7.2.3). Indeed, postmul-
tiply (7.2.3) by the vector b, substitute âwi+i for Xi+ib and âw^ for Xib, and obtain
-wi+i = Xi+lb = Xib + Xitf + AXJb = -Wi + Xi(b + AXi\>) = -Wi + X^b-Aw*).
Therefore, Wj+i = w^ + Xi(Awi â b), which agrees with (7.5.1).
In application to general linear systems, the matrix Xi is invariant in i and residual
correction converges linearly, but in the Toeplitz case we will obtain quadratic conver-
gence because we will correct the matrix Xi simultaneously with Wj.
7.5.3 
Application to a Toeplitz Linear System of Equations
Let A be a Toeplitz matrix, A â T â [tij], t^j = t^j for all i and j. In this case, we
can simplify the above Newton-based recursion by relying on two known formulas that
express the inverse âX = T~l via the solution of two Toeplitz linear systems,
where t = [w, at\ +6ti_n, at% + bt-2-n, ..., aÂ£n_i + &Â£_i]T for three fixed scalars, u>, a,
and b. Each of these two expressions for the inverse relates the two equations of (7.5.1)
for b = CQ and b = t to each other and to the matrix âX â T"1.
In particular, to extend Alg. 7.4.1 in this way, we will use the following known
formula (cf. p. 136 of [BP94] and the second remark at the end of this section):
where t = [w,t\-n,... ,t_i]T, ii_j, denotes the (i, j)th entry of T; w is any fixed scalar,
say, w â 0 or w = max^- \ti-j\\ and / is the reflection matrix, defined in Lemma 7.3.1.
Due to this expression and to (7.5.1), we may update the approximation Xi to X as
follows.
Algorithm 7.5.1 (Residual Correction for a Toeplitz System). The following pro-
cedure updates the estimates X^:
â¢ Input: A Toeplitz matrix T and the approximations Xi to X = âT~l, x^ to
x = T~lt, and yi to y = T'^Q.
â¢ Output: New approximations Xi+i, xi+i, andyi+i.

202 
Newton's Iteration for Structured Matrices 
Chapter 7
â¢ Computations (cf. (7.5.1) for A = T and (7.5.2)): Compute and output
where we set
Algorithm 7.5.1 can be applied recursively for i = 0,1,... provided that some initial
approximations XQ, yo, and XQ to x, y, and X, respectively, are available. Furthermore,
if just some initial approximations XQ and yo are available, we may substitute x = XQ
and y = yo on the right-hand side of (7.5.3) and choose the resulting matrix as â XQ.
By using the operator V-^ and (7.4.8), we may modify (7.5.3) and Alg. 7.5.1 to
replace the lower triangular Toeplitz matrices Â£(x) and Â£(y) by /-circulant matrices and
the upper triangular Toeplitz matrices Â£T(ZJy) and Â£T(ZJx â e0) by (l/e)-circulant
matrices. This can be also viewed as the specialization of Alg. 7.4.2 to solving a Toeplitz
linear system of equations. Namely, instead of (7.5.3), we will use the following result
[GO92] (cf. the second remark at the end of this section and [AG89] for the case where
/ = â e â 1 and T is a Hermitian Toeplitz matrix).
Theorem 7.5.1 (Inverse of a Toeplitz Matrix). Let T = (ti,j)â¢J=0 be a Toeplitz
matrix, tij = ti-j, i, j = 0,1,..., n â 1. Let e, f , and w be three scalars, e ^ 0, ef ^ 1.
Let two vectors x and y satisfy the linear systems of equations Ty â CQ, Tx = t(e,tt;),
where t(e,w) = [w,ti â et\-n,ti â et^-n-,... , tn-i â et-i]T. Then T is a nonsingular
matrix and
where Zg(v) denotes the g-circulant matrix with first column v (cf. Lemma 7A.I). 
Now we will modify Alg. 7.5.1 as follows.
Algorithm 7.5.2 (Residual Correction for a Toeplitz System). 
The following
modifications should be incorporated into Alg. 7.5.1.
(a) Include the scalars e,f, and w of Thm. 7.5.1 (say, write f = â e = 1, w = 0) in
the input set of the algorithm.
(b) Letx.i andx.i+1 approximate T~1t(e,it;), rather than T~lt; in particular, replace
t by t(e, w) in the expression Xj+i = Xi + Xi(Tx^ â t).
(c) Replace the expression (7.5.4) for âXi+i by the following expression (based on
Thm.7.5.1):

Section 7.5. Residual Correction Method 
203
Since multiplication of a matrix Zf(v) by a vector is roughly twice as fast as multi-
plication of Â£(v) or Â£T(v) by a vector, Alg. 7.5.2 is roughly twice as fast as Alg. 7.5.1.
Remark 1. To use (7.5.3)-(7.5.5) and Thm. 7.5.1 more efficiently, one may normalize the
vector XQ by scaling the input matrix T or just change e and / so as to bring the norm ||xo||
close to |l-e/H|eo|| = |1 - e/|.
Remark 2. Various modifications of the inversion formula (7.5.3) and, consequently, of Alg.
7.5.1 are possible. In particular, one may express the matrices xotoX and (if Â£0,0 ^ 0) then
X = âT~l = (xitj) via the columns y = Xeo and x = Xen-i based on the celebrated
Gohberg-Semencul formula (cf. [BP94, p. 135] and [GO92]). We choose (7.5.3) rather than
the latter formula to avoid the division by XQ,O, since such a divisor may vanish or may have too
small a magnitude even for well-conditioned input matrices T. Similar comments apply to the
inversion formulas of Thm. 7.5.1 and Alg. 7.5.2 of this section. The expressions of Thm. 7.5.1
are closely related to the ones of Lemma 7.4.2 in the case of a nonsingular Toeplitz matrix A.
7.5.4 
Estimates for the Convergence Rate
Since Algs. 7.5.1 and 7.5.2 are the specializations of Algs. 7.4.1 and 7.4.2, the estimates
for the convergence rates of Algs. 7.4.1 and 7.4.2 can be extended to Algs. 7.5.1 and 7.5.2.
Next, however, we will estimate the convergence rates of Algs. 7.5.1 and 7.5.2 directly.
Let us write
By combining the latter definition and the norm bounds (7.5.2) for w = x and w = y,
we immediately deduce that
and, consequently,
for p(i] of (7.5.2). In Apps. 7.B and 7.C we derive the following bounds.
Lemma 7.5.1 (Error Bounds). The following bounds hold:
(a) If Alg. 7.5.1 is applied, then
(b) If Alg. 7.5.2 is applied and if f â â e = 1 in (7.5.5) and in Thm. 7.5.1, then

204 
Newton's Iteration for Structured Matrices 
Chapter 7
We also have the bound p(i + 1) < ||.A||i.Ei+i, and by (7.5.2), e(i + 1) < p(i)e(i).
By applying Lemma 7.5.1, we obtain that
and
where Algs. 7.5.1 and 7.5.2 are applied, respectively. Let us write
if Alg. 7.5.1 is applied and
if Alg. 7.5.2 is applied. In both cases, we have p(i + 1) < p(i)e(i)||A||ii/, provided that
p(i)e(i) < p(0)e(0) for all i.
Now suppose that p(0) and e(0) are sufficiently small such that
for a fixed p < 1. Then we obtain that
so that p(l)e(l) < p2p(0)e(0) < p(0)e(0). Recursively, we obtain by induction that
and
This implies quadratic convergence with the base p assuming the initial upper bounds
(7.5.9) on e(0) and p(0), which extend the respective bounds of (7.4.6) and (7.4.10) to
the Toeplitz case.
7.6 
NUMERICAL EXPERIMENTS
The four algorithms developed in this chapter were tested using MATLAB, a software
package marketed by The Math Works, Inc., of Massachusetts. In the first group of
experiments, the input matrices A were generated as 100 x 100 Toeplitz matrices whose
first columns and first rows were filled with random entries chosen from a uniform
distribution on the interval â 1 < a;^ < 1. To generate an initial approximation XQ to
â-A"1, we first numerically computed the actual inverse matrix âX and then perturbed
its entries by adding some small random values. To generate the perturbations, we first
composed 100 x 100 matrices with random entries chosen under a normal distribution
with mean zero and variance one and then scaled the values by a noise factor. We used
a noise factor of 0.001 for Algs. 7.4.1, 7.5.1, and 7.5.2. Algorithm 7.4.2 requires a closer
initial approximation to the solution to yield quadratic convergence, so we decreased

Section 7.6. 
Numerical Experiments 
205
the noise factor in its test to 0.0001. Once the noise matrix was obtained, we added it
to the actual inverse.
We then tested the algorithms, running a group of 40 tests for each algorithm. In
most cases rapid convergence was achieved, with four exceptions in the second group,
one exception in the third group, and one exception in the fourth group. To test
the convergence rate, we computed the column norm p(i) = \\AXi + /||i of the residual
matrix Ri = AXi+I obtained in the ith iteration step. Sample test results are presented
in Tables 7.2-7.8. We also supplied the value
We let / = â e = 1 in Algs. 7.4.2 and 7.5.2, and we performed Algs. 7.5.1 and 7.5.2 for
vectors t with w = 0.
Table 7.2. Tests for inputs of class 1 for Alg. 7.4.1.
~ 
Sample Test 1 ~ Sample Test 2 
Sample Test 3
condi(A) || 2.024643e-|-004 | 1.365480e + 005 | 2.223065e + 004~
T-Q 
1.932960e-002 
1.990289e - 002~ 2.206516e - 002
n 
5.822783e - 005 3.102183e - 005 8.922756e - 005
r2 
9.966761e-011 
1.277523e - 010 8.006189e - 010
r3 
|| 8.220646e-013 | 8.420042e - 012 | 1.920519e - 012
Table 7.3. Tests for inputs of class 1 for Alg. 7.4.2.
~ 
Sample Test 1 
Sample Test 2 
Sample Test 3
condi(A) 
[I 3.274079e + 003 | 5.836559e + 003 | 3.425782e + 003
rp 
4.392883e-003 
2.053855e - 002~ 1.876769e - 002
n 
4.999007e - 005 3.952711e - 003 2.661747e - 003
r2 
9.924380e - 009 3.011774e - 004 
7.458110e - 005
rz 
2.121324e-012 
9.710310e - 007 1.452111e - 007
r4 
I) 
| 2.223482e-011 | 2.891295e - Oil
Table 7.4. Tests for inputs of class 1 for Alg. 7.5.1.
Sample Test 1 
Sample Test 2 
Sample Test 3
~condi(A) | 3.032816e + 003 | 7.932677e + 003 | 2.836675e + 003~
rp 
"772055106-001" 7.046618e-001 ~1.330095e - OPT"
n 
6.074104e-002 
7.454610e - 002 
3.528652e - 003
7-2 
8.19Q064e-004 
1.608024e - 003 3.643328e - 006
r3 
2.160530e-007 
1.015745e - 006 4.971280e - 012
r4 
| 7.357363e-013 | 4.453048e - 012 |
The results of these computations fell into three classes. The first class, represented
in Tables 7.2-7.5, shows the computations where rapid convergence was observed im-
mediately. The second class, represented in Tables 7.6-7.8, shows the computations
with the input matrices for which iteration initially stumbled and then, after a certain
number of steps, s, started to converge rapidly. In fact, we observed only 4 such cases of

206
Newton's Iteration for Structured Matrices 
Chapter 7
Table 7.5. Tests for inputs of class 1 for Alg. 7.5.2.
condi (A)
ron
r?.
r-3
T4
Sample Test 1
1.727930e + 003
2.246859e - 001
9. 875483e - 003
2. 864990e - 005
2.354060e - 010
1.764283e - 013
Sample Test 2
2.435048e + 003
5.085527e - 001
4.438413e - 002
4.1162386-004
4.307276e - 008
3.781741e - 013
Sample Test 3
3.248792e + 003
6.194036e - 001
7.533096e - 002
1.865356e - 003
1.342796e - 006
1.848549e - 012
Table 7.6. Tests for inputs of class 2 for Alg. 7.4.2.
condi (.A)
s
ro
ri
r-i
T3
T4
Sample Test 1
4.637187e + 003
4
6.170632e-003
7.897623e - 004
1.426298e - 005
4.577198e - 009
2.713013e-011
Sample Test 2
5. 763326e + 003
2
1.021014e - 003
5.588185e - 005
1.621575e - 007
7.528647e - 012
Sample Test 3
1.349509e + 003
1
9.937866e - 003
1.241422e - 005
2.137849e-009
6.846061e - 012
Table 7.7. Tests for inputs of class 2 for Alg. 7.5.1.
condi (A)
s
ron
r-2
rj,
T4
Sample Test 1
2.042839e + 004
3
3.699548e - 001
6.345701e - 002
1.766525e - 003
1.372439e - 006
3.605207e - Oil
Sample Test 2
1.483413e + 004
2
9.831201e - 002
4.084126e - 003
7.245445e - 006
2.563352e - Oil
Sample Test 3
1.044744e + 004
2
8.1050606-002
2.120317e-003
1.247355e - 006
3.7759726-011
Table 7.Ji. Tests for inputs of class 2 for Alg. 7.5.2.
condi (A)
s
ro
ri
r<2
Â»"3
Sample Test 1
2.000268e + 004
3
1.097395e - 001
5.1964206-003
1.226878e - 005
8.729462e - Oil

Section 7.7. 
Concluding Remarks 
207
40 in the tests of Alg. 7.4.2 (s < 4), 3 cases of 40 in the tests of Alg. 7.5.1 (s = 2), and
1 case of 40 in the tests of Alg. 7.5.2 (s = 3). In the third class of inputs, the iteration
showed no sign of convergence even after 10 Newton steps. Only 6 such cases occured
in all 160 tests. We display the results for 3 samples in the first class and 2 samples
in the second class, in which case we display the p(i] after s steps of stumbling. In all
tables, the results are displayed only until convergence, up to the step after which the
roundoff errors started to exceed the approximation errors.
In some experiments the iteration seemed to converge irregularly, in the sense that
the residual norm exceeded 1 initially but the iteration still converged. The apparent
reason is that the bound
can be a strict inequality rather than an equation.
On the other hand, the residual norm sometimes decreased more slowly than quadrat-
ically. This is immediately explained by the influence of the stage of adjusting the
matrices to Toeplitz format, for Algs. 7.5.1 and 7.5.2, to the selected Toeplitz-like for-
mat, for Alg. 7.4.2, and to their representation with shorter displacement generators for
Alg. 7.4.1. With these comments in mind, the test results are quite consistent with the
theoretical estimates for the convergence rate.
7.7 
CONCLUDING REMARKS
The algorithms based on Newton's iteration-residual correction run very fast for struc-
tured matrices, are easy to code, and allow their effective parallel implementation (cf.
[BP94]). We note that the techniques discussed in this chapter and used in [Pan93a],
[Pan93b], [Pan92a] can be extended to other classes of structured matrices such as
Cauchy-like and Vandermonde-like (Vandermonde-type) matrices [Pan90], [PZHD97].
Acknowledgments
This work was supported by NSF grant OCR 9625344 and by PSC CUNY awards 667340
and 668365. The chapter was written by Pan, who was joined by Branham and Rasholt in Sec.
7.6 and by Zheng in Sees. 7.3 and 7.4.

208 
Newton's Iteration for Structured Matrices 
Chapter 7
APPENDICES FOR CHAPTER 7
7.A 
CORRECTNESS OF ALGORITHM 7.4.2
Our goal is to prove (7.4.10). We will start with an informal argument. Write
and immediately obtain that 7 + AXi+i = (I + AXi)2, and, consequently,
Therefore, for small p(i) we have Xi+i ~ X, and by extending (7.4.9) we write
and, therefore, Xi+i Â« Xi+i ~ X.
Next we will formally deduce (7.4.10). Substitute (7.A.I) into the equations of Alg. 7.4.2
and rewrite them as follows:
Combine these equations with those of (7.4.9) and obtain that
Let us next estimate \\X - Xi+i||i- We have X - Xi+i = (I + Xi+iA)A~l = (I + XiA)2A~1.
Recall that ||7 + -X"i>l||i = p(i) and obtain that
We also observe that ||Z^(v)||i < ||Z^(v)||i = ||v||i for \h\ â 1 and any vector v of
dimension n.
Now, by applying Lemma 7.4.2 to A = X and A = Xi+i for / = â e and recalling that
|/| = 1, we deduce that
Substitute the expression for HX from (7.4.9) and for Gj+i from Alg. 7.4.2 and obtain that
Since ||7 + Xi+iA||i < p2(i), we also have
Therefore,
This proves the bound (7.4.10).

Section 7.B. 
Correctness of Algorithm 7.5.1 
209
Remark. By (7.A.2), ||X||i < ||Xi+i||i/(l -p2(i)). Therefore, the right-hand side of (7.4.10) can
be expressed entirely in terms of the values given as the input or made available in the process of
performing Alg. 7.4.2.
7.B 
CORRECTNESS OF ALGORITHM 7.5.1
We now prove the result of Lemma 7.5.1 (a). Let Ei+i = \\X â Xi+i\\i and substitute the
expressions for âX and â Xi+i from (7.5.3) and (7.5.4) to obtain that
Now recall that Â£(u) - Â£(v) = Â£(u - v) and \\BC + UV\\i < \\B\\i\\C\\i + ||Â£/||i||V||i for any
choice of four matrices B,C,U,V and two vectors u and v, and deduce that
Now observe that
for any vector u of dimension n. Substitute these bounds and obtain that
Substitute the bounds (7.5.6) and (7.5.7) and deduce that
Substitute (7.5.8) and arrive at the result of Lemma 7.5.1 (a).
7.C 
CORRECTNESS OF ALGORITHM 7.5.2
We now prove the result of Lemma 7.5.l(b). Again let Ei+i = \\X â Xi+i\\i and substitute
the expressions for âX and â Xi+\ from Thm. 7.5.1 and (7.5.5) to obtain that
Now observe that Zh(u) - Zh(v) = Zh(u - v), ||Zg(u)||i = ||u||i for any pair of scalars g and
h, where \g\ â 1, and any pair of vectors u and v of dimension n. Substitute these relations

210 
Newton's Iteration for Structured Matrices 
Chapter 7
and deduce that
Substitute (7.5.6)-(7.5.8) and the trivial bound ||x - 2e0||i < 2 4- ||x||i and obtain that
which immediately implies the result of Lemma 7.5.l(b).

Chapter 8
FAST ALGORITHMS
WITH APPLICATIONS
TO MARKOV CHAINS
AND QUEUEING MODELS
Dario A. Bini
Beatrice Meini
8.1 
INTRODUCTION
A very meaningful application where the exploitation of the Toeplitz-like structure plays
a fundamental role in devising advanced and effective algorithms arises in the field of
queueing theory (see, e.g., [BM96a], [BM96b], [BM97b], [BM97a], [BM98a], [Mei97b]).
This chapter provides an overview of this application area.
Many problems in queueing theory are modeled by Markov chains of M/G/1 type
[Neu89], which are characterized in terms of transition probability matrices P that
have a lower block Hessenberg structure and, except for their first block column, a
block Toeplitz structure. Moreover, these transition matrices are generally infinite di-
mensional and the block entries may have a considerable size. Examples of real-world
situations where such models are encountered include the analysis of telephone networks
with multiple types of traffic (voice, data, video, multimedia) [GHKT94], as well as the
analysis of metropolitan queueing models [ALM97].
The main computational burden in this kind of application consists of solving a
homogeneous linear system of equations of the form
where TT is the probability invariant vector of the chain and the symbol T denotes matrix
transposition.
A related problem arises in the computation of an approximate nonnegative solution
G to the matrix equation
211

Fast Algorithms with Applications to Markov
212 
Chains and Queueing Models 
Chapter 8
for given mxm nonnegative matrices Ai such that Y^o Aj ^s stochastic. This problem
can be reduced to the computation of the top-leftmost block entry of the infinite block
matrix (/ â Q)"1, where / denotes the identity operator and
is an infinite block Toeplitz matrix in block Hessenberg form. The knowledge of G
allows us to recover an arbitrary number of components of the vector TT by means of a
well-known tool in Markov chains known as the Ramaswami formula [Neu89], [Ram88].
The purpose of this chapter is to provide an overview of the most recent efficient
algorithms for matrix problems with such block Toeplitz and Hessenberg structures.
8.2 
TOEPLITZ MATRICES AND MARKOV CHAINS
In this section we recall the basic concepts related to Markov chains, describe the main
computational problems, and present the most recurrent matrix structures arising in
applications.
Let N denote the set of nonnegative integers.
Definition 8.2.1 (Markov Chains). A homogeneous discrete-time Markov chain with
discrete (finite or infinite) state space S = {xi, i e E}, E C N, is a discrete-time
stochastic process {Xn,n â¬ N} such that
That is, the state of the system at time n + 1 depends only on the state of the system at
time n.
The scalar Pij, called the transition probability, represents the probability of making
a transition from state x^ to state Xj when the time parameter increases from n to n +1
for any integer n.
The matrix P = (pij), called the transition probability matrix, is a (finite or infinite)
stochastic matrix, i.e.,
for any i â¬ E.
A relevant problem in the study of Markov chains is to compute a nonnegative vector
7T = ColJTTo, TTi, 7T2, TTa, . . .} SUCh that
Such a vector is called the probability invariant vector and, under suitable conditions,
exists and is unique. In this case, the Markov chain is said to be positive recurrent and
it holds that

Section 8.2. Toeplitz Matrices and Markov Chains 
213
whatever the initial state distribution (Prob{Xo = Zi})i of the chain is.
In most cases, the matrix P (possibly infinite) is strongly structured. One such
example occurs for Markov chains of M/G/1 type that arise in queueing models [Neu89].
For these chains, the transition probability matrix is infinite and has the form
where Ai, Bi+i, i > 0, are raxra nonnegative matrices such that ^^ A? and X)i=T Bf
are stochastic. That is, PT is in block Hessenberg form and the submatrix obtained by
removing its first block column is block Toeplitz.
Another example arises in the study of so-called quasi-birth-death (QBD) problems
[Neu89], where the matrix P, in addition to the M/G/1 structure, is also block tridiag-
onal. That is, P is uniquely defined by the blocks AQ, AI, A?, B\, B2.
More general M/G/1 type Markov chains, called non-skip-free in [GHT97], are de-
fined by transition matrices of the form
where Ai, Bi+itj, i > 0, j = !,...,&, are m x m nonnegative matrices such that
Y^Q A? and X^i^T B?j, 3' = 1> â¢ â¢ â¢ Â» ^ , are stochastic. This class of transition matrices
can be regarded as special cases of the block Hessenberg form (8.2.3) by partitioning
the matrix PT in (8.2.4) into blocks Ai, Bi of dimension p = mfc, where
M/G/1 type Markov chains have been introduced and thoroughly investigated in [Neu89].
However, no explicit attempt has been made to exploit matrix structure in developing
efficient algorithms.

Fast Algorithms with Applications to Markov
214 
Chains and Queueing Models 
Chapter 8
Similar structures arise in the case of finite Markov chains or where the infinite
matrix PT is replaced by a finite stochastic matrix obtained by cutting PT to a finite
(large) size and by adjusting the last row to preserve the stochastic nature of the matrix.
In this case the resulting structure becomes
For block tridiagonal matrices, like the ones arising from QBD problems, P is then
uniquely defined by the blocks AO, AI, A2, B\, B2, Ci, â¬2-
8.2.1 
Modeling of Switches and Network Traffic Control
An example of a general M/G/1 Markov chain of the type (8.2.4) arises in the mod-
eling of the behavior of switches with multiple types of traffic (voice, data, video, and
multimedia) [GHKT94], [KR86].
Figure 8.1. Voice-data multiplexor.
Indeed, consider a voice-data multiplexor where voice and data traffic are multi-
plexed over a single channel [GHKT94], [KR86], [LM85], [SVS83] and where time is
divided into slots (see Fig. 8.1). The time slots are aggregated into frames of k slots
each. There are at most m â 1 < k voice connections actively transmitting during a
time frame, and voice connections become active or inactive at the beginning of a frame.
The number of active voice connections per frame is governed by an m state Markov
chain having an irreducible aperiodic transition matrix Q = (Qr,s), r, s = 0,..., m â 1,
where Qr,a is the probability to switch from r active voice connections in the previous
frame to s active voices connections in the present frame. Each active voice connection
occupies one slot of time during a frame, and the remaining slots are allocated to data
packets. Each data packet is one slot in length. The arrival process of data packets is

Section 8.2. Toeplitz Matrices and Markov Chains 
215
governed by a generating function R(z), and packets that arrive during a frame can be
transmitted only in subsequent frames. There is an infinite buffer for data packets.
Let un be the random variable representing the number of buffered data packets at
the beginning of the nth frame and vn the random variable representing the number
of active connections at the beginning of the nth frame. Then Xn = (un, vn) is a two-
dimensional Markov chain with states (i, j), i = 0,1,..., j = 0,..., m â 1. This is a
Markov chain of the M/G/1 type (8.2.4), where the blocks Ai and Bitj are defined in
terms of matrix power series in the following way:
for j = 0,..., k â 1, and where
is an m x m diagonal matrix.
8.2.2 
Conditions for Positive Recurrence
Computing the probability invariant vector TT of P consists of evaluating the eigenvector,
corresponding to the eigenvalue 1, of the nonnegative matrix PT. A matrix P is said to
be nonnegative, and we write P > 0 if all its entries are nonnegative. (This is distinct
from saying P is nonnegative definite, which means x*Px > 0 for all vectors x.) For
finite Markov chains, the theory of nonnegative matrices [Ste94], and more specifically
the Perron-Frobenius theorem, guarantees the existence of a nonnegative eigenvector
TT such that (8.2.2) holds. The uniqueness of TT, and thus the positive recurrence of
the finite Markov chain, holds if the matrix P is not reducible. (A matrix A is said to
be reducible if there exists a permutation of rows and columns that transforms A by
similarity in the following way:
where the blocks Aiti, AI^ are square matrices and II is a permutation matrix.)
The computation of TT can be carried out by solving the homogeneous linear system
where 7 is the identity matrix, by means of the customary techniques such as Gaussian
elimination. The matrix of the above system is a singular M-matrix [Var63] since the
spectral radius of PT, i.e., the maximum modulus of the eigenvalues, is equal to 1. A
matrix of the form al â B is said to be an M-matrix if B > 0 and a is greater than or
equal to the spectral radius of B.
From the theory of M-matrices it follows that both the LU (lower upper triangular)
and the UL (upper lower triangular) factorizations of / â PT always exist [FP81],
[VC81]. Such factorizations can be used for the computation of TT. Moreover, due to
the properties of M-matrices, this computation is numerically stable provided that the
diagonal adjustment technique of [GTH85], [Oci93] is employed.

Fast Algorithms with Applications to Markov
216 
Chains and Queueing Models 
Chapter 8
For infinite matrices of the form (8.2.3) we can give explicit conditions for the exis-
tence and uniqueness of a solution TT to (8.2.2) in the case where P and X^i^o* -^Â» are not
reducible. Observe that the definition of a reducible matrix can be extended naturally
to infinite matrices where the blocks ^1,1 and ^2,2 in (8.2.7) may have infinite size.
Let us now introduce e = coljl, 1,..., 1} and the m-dimensional vectors a, 6 =
col{&o> &i> â¢ â¢ â¢ ? frm-i} such that
Then the corresponding Markov chain is positive recurrent if and only if p = bTa < I
[Neu89].
8.2.3 
Computation of the Probability Invariant Vector
In order to devise efficient algorithms for the computation of TT in (8.2.2) it is fun-
damental to take advantage of the Toeplitz-like structure of the transition probability
matrix P.
For positive recurrent infinite Markov chains (8.2.3), the vector TT that solves (8.2.2)
can be computed by means of a recursive and numerically stable formula (known as
Ramaswami's formula [Ram88]). The formula first requires that we obtain the solution
of the following nonlinear matrix equation:
where X is m x m. It is possible to establish that for positive recurrent Markov chains,
(8.2.8) has a unique nonnegative solution G such that GT is stochastic [Neu89j.
Now define the matrices
and partition the vector TT into m-dimensional vectors TT^, i > 0, according to the block
structure of (8.2.3). Then the vector TTO solves the system of equations
while the remaining components of TT can be computed by means of the Ramaswami
formula [Ram88]:
It is also possible to rephrase the matrix equation (8.2.8) in terms of a block Toeplitz
block Hessenberg matrix and to reduce the design and analysis of algorithms for solving
(8.2.8) to the design and analysis of algorithms for treating the infinite matrix

Section 8.3. Exploitation of Structure and Computational Tools 
217
Indeed, if G is the unique solution of (8.2.8), then the unique solution of the infinite
system of equations
where Xi are ra x ra matrices, is given by Xi = G1, i > 1. Therefore, solving (8.2.12)
provides the only solution G of (8.2.8) having nonnegative entries.
Observe that in order to compute G it is sufficient to compute the top-leftmost block
entry of H*1, which we shall denote by (H~l)iti. In fact, it holds that G = Ao(H~l)iii.
Observe also that if we consider the finite system obtained by truncating (8.2.12) at
the dimension n, say,
where
then {X^ }n is a sequence of nonnegative matrices that monotonically converges to G
as n tends to infinity [LS96].
We now study how to exploit the Toeplitz structure of Hn in order to speed up the
computations.
8.3 
EXPLOITATION OF STRUCTURE AND COMPUTATIONAL
TOOLS
We start by studying the relations that exist among block Toeplitz matrices, matrix
polynomials, and matrix power series. We show that matrix computations involving
(infinite) block Toeplitz matrices can be rephrased in terms of matrix polynomials or
matrix power series, thus generalizing the correlations between Toeplitz computations
and polynomial computations widely investigated in [BP94]. We also introduce some
basic algorithms for polynomial (power series) computations and for manipulating block
Toeplitz matrices.
The relation between Toeplitz matrices and polynomials has been implicitly used in
many fields but only recently pointed out in a detailed way in [BG95], [BP86], [BP93],
[BP94], [Gem97]. In particular, computations such as polynomial multiplication, poly-
nomial division (evaluation of quotient and remainder), polynomial g.c.d. and l.c.m.,
Pade approximation, modular computations, Chinese remainder, and Taylor expansion
have their own counterparts formulated in terms of Toeplitz or Toeplitz-like computa-
tions and vice versa. Quite surprisingly, almost all the known algorithms for polynomial
division match with corresponding algorithms independently devised for the inversion
of triangular Toeplitz matrices [BP86]. Also several computations involving matrix al-
gebras strictly related with Toeplitz matrices, such as the circulant class, the r class,
and the algebra generated by a Frobenius matrix, can be rephrased in terms of com-
putations among polynomials modulo a given specific polynomial associated with the
algebra [BP94], [BC83], [Boz95].

Fast Algorithms with Applications to Markov
218 
Chains and Queueing Models 
Chapter 8
These results can be extended in a natural way to the case of block Toeplitz matrices,
a fact that does not seem to be well known in the literature. For instance, a remark
in [Ste95] states that "there are fast algorithms for multiplying Toeplitz matrices by a
vector. Unfortunately, they do not generalize to block Toeplitz matrices."
The goal of this section is to derive FFT-based algorithms for performing operations
between block Toeplitz matrices, such as computing the product of a block Toeplitz
matrix and a block vector (and thus extending the discussion of Sec. 1.4) and computing
the inverse of a block Toeplitz block triangular matrix. To achieve these results, we
exploit the relations between block Toeplitz matrices and matrix polynomials.
Let z be a scalar indeterminate and consider the matrix polynomial A(z) = X]f=o ^z%
of degree p [LT85], where Ai, i = 0,... ,p, are mxm matrices having real entries and Ap
is not the null matrix. A matrix polynomial can be viewed as a polynomial in z having
matrix coefficients or, equivalently, as a matrix having entries that are polynomials.
Similarly, a matrix power series is a series having matrix coefficients or, equivalently, a
matrix having entries that are power series.
A matrix polynomial is fully denned by its matrix coefficients AQ, ..., Ap or, equiva-
lently, by the (p +1) x 1 block matrix A having blocks AQ, ..., Ap. We call block column
vector any n x 1 block matrix. Similarly we call block row vector any 1 x n block matrix.
The blocks denning a block (row or column) vector are called block components.
8.3.1 
Block Toeplitz Matrices and Block Vector Product
Given two matrix polynomials A(z) and B(z) of degrees p and </, respectively, we may
consider the matrix polynomial C(z) = A(z)B(z] having degree at most p + q, ob-
tained by means of the row-by-column product of A(z] and B(z). If we denote C(z) â
E?=o c*zi,then we have c<> = ^o#o, Ci = AoBi + AiB0, C2 = A0B2 + AiBi + A2BQ,
..., Cp+q â ApBq. That is, in matrix form, we have
Observe that, by choosing p = 2n â 2, q = n â 1, the middle n block rows of the
matrix equation (8.3.1) yield the following product between a general block Toeplitz
matrix and a block column vector:
This shows that the product of a block Toeplitz matrix and a block vector can be
viewed in terms of the product of two matrix polynomials. More specifically, (8.3.1)

Section 8.3. Exploitation of Structure and Computational Tools 
219
can be used together with an evaluation-interpolation technique at the roots of unity
to efficiently compute the product (8.3.2), as we now clarify.
For this purpose, let us introduce some notation and definitions concerning DFTs.
Let i be the imaginary unit such that i = ^fâ\. and denote
a primitive Nth root of unity such that the powers LJ
J
N, j â 0,..., N â 1, are all the Nth
roots of unity. Moreover, let z* denote the complex conjugate of the complex number
z.
For TV-dimensional complex vectors
such that
or, equivalently,
we denote b = DFT(a), the DFT of a of order AT, and a = IDFT(6), the inverse DFT
of 6 of order N. We recall that both DFT and IDFT can be computed in O(N\ogN)
operations by means of FFT algorithms [ER82].
Observe that evaluating b = DFT(a) corresponds to computing the values that the
polynomial a(z) = Y^k=o akzk takes at the Nth roots of unity. Similarly, computing
a = IDFT(6) corresponds to solving an interpolation problem, viz., that of computing
the coefficients of a polynomial a(z) given its values at the TVth roots of unity.
In summary, the evaluation-interpolation technique for computing a product of the
form (8.3.2) proceeds according to the following scheme:
1. Evaluate A(z] at the ./Vth roots of unity, u>
3
N, j = 0,... , AT â 1, for a choice of
N that satisfies N > 3n â 3, since the product C(z) has degree at most 3n â 3.
This requires that we evaluate all m2 entries of A(z] at the roots of unity, and
this computation can be performed by applying m2 DFTs, each of order N.
1. Evaluate B(z) at the Nth roots of unity, again by performing m2 DFTs, each of
order N.
3. Compute the N matrix products C(u3
N) = A(ujj
N)B(ujj
N), 
j = 0,..., AT - 1. The
total cost of this step is O(m?N) operations.
4. Interpolate the values of the entries of C(u;^) by means of m2 IDFTs, each of
order AT, and recover the coefficients of the m2 polynomials, i.e., the blocks Cj.
The total cost of the above procedure is O(m2N\ogN + m3N} operations, where
O(m2N log N) is due to the FFTs while O(m3N) is the cost of stage 3. Here N is of the
order of n. This cost is below the customary figure of O(m3N2) operations for carrying
out the matrix multiplication (8.3.2).
We may in fact devise a more efficient algorithm involving FFTs of lower order
for computing the product between a block Toeplitz matrix and a block vector if we

Fast Algorithms with Applications to Markov
220 
Chains and Queueing Models 
Chapter 8
consider computations modulo the polynomial ZN â 1. Indeed, given matrix polynomials
P(z), Q(z), R(z) of degree at most N â 1 such that
we may similarly rephrase the above equation in matrix form in the following way:
The matrix on the right-hand side of (8.3.7) is a block circulant matrix, namely, a matrix
whose block entries in the kih block row and in the jth block column depend on j â k
mod N.
The blocks Ri can be efficiently computed, given the blocks PÂ» and Qi, by exploit-
ing the polynomial relation (8.3.6). In fact, from (8.3.6) we deduce that R(u3
N) =
P(u}J
N)Q(uj3
N}^ j = 0,..., N â I. Therefore, the blocks Ri can be computed by means
of the evaluation-interpolation technique at the Nth roots of unity.
Now, given an n x n block Toeplitz matrix A = (Ai-j+n-i)i,j=\,...,n it is possible to
embed A into an. N x N block circulant matrix H, N = In, defined by its first block
column having blocks An-i, An,..., A2n-i,0, A0,..., An_2, i.e.,
The product of H and the block column vector defined by the blocks
where Bj = O for j > n, delivers a block column vector of N block components
Co, C\,..., CN-I such that
Since the leading principal block submatrix of H of block size n coincides with A,
the blocks Co, Ci,..., Cn-i define the block column vector C such that C = AB, where
B is the block column vector defined by 50, B\,..., Bn-\. In this way we arrive at the
following algorithm for the multiplication of a block Toeplitz matrix and a block vector.

Section 8.3. Exploitation of Structure and Computational Tools 
221
Algorithm 8.3.1 (Block Toeplitz and Vector Multiplication).
â¢ Input: The mxm matrices AQ, A\,..., A-m-i defining the nxn block Toeplitz matrix
A = (^4i-j+n-i)i,j=i,...,n;' the mxm matrices BQ, B\,..., Bn-\ defining the block
vector B.
â¢ Output: The mxm 
matrices CQ, C\,..., Cn_i defining the block vector C such that
C = AB.
â¢ Computation:
1. Evaluate the matrix polynomial
at the In roots of I, by means of m2 DFTs of order 2n each, and obtain the
matrices a(u3
2n}, j = 0,..., 2n â 1.
2. Evaluate the matrix polynomial
at the In roots of I, by means of m2 DFTs of order In each, and obtain the
matrices ^(<jj3
2n}, j â 0,..., 2n â 1.
3. Compute, the products 7(0^) = a(Ct;2n)^(a'2n)> J = 0,..., 2n â 1.
4. Interpolate 7(u>2n) % means ofm2 IDFTs of order In each, obtain the coef-
ficients 70,71,..., 72n-i such that
and output Ci = 7^, i = 0,..., n â 1.
8.3.2nversion of Block Triangular Block Toeplitz Matrices
We can also devise efficient algorithms for the inversion of block triangular block Toeplitz
matrices. For this purpose, observe that given three matrix power series A(z) â
EÂ£S?^*'Â» #(*) = Et~Bi*S and C(z) = E,t~C4zÂ«, the relation C(z] = A(z)B(z)
can be rewritten equivalently in matrix form, in a way similar to (8.3.1), as
Analogously, the same functional relation modulo zn, i.e., C(z) â A(z)B(z] mod zn,
can be rewritten as the finite lower block triangular block Toeplitz system
Analogously, the same functional relation modulo

Fast Algorithms with Applications to Markov
222 
Chains and Queueing Models 
Chapter 8
In this way, the product of two matrix power series modulo zn can be viewed as the
product of an n x n lower block triangular block Toeplitz matrix and a block vector.
Now since the inversion of a block triangular block Toeplitz matrix can be viewed as
the inversion of a matrix power series A(z] modulo zn, that is, as the computation of
the coefficients of the matrix polynomial B(z) such that
we see that the inverse of a block triangular block Toeplitz matrix is itself a block
triangular block Toeplitz matrix.
We now describe an algorithm for the inversion of a block triangular block Toeplitz
matrix that has been derived for scalar entries in [Laf75j. In the matrix polynomial
framework, this algorithm extends to matrix polynomials the so-called Sieveking-Kung
algorithm, which is based on Newton's iteration for inverting a polynomial [BP94],
[KnuSl].
Let us denote by Tn the coefficient matrix in (8.3.8) and assume for simplicity that
n = 2q with q a positive integer. We further partition Tn as follows:
where all four blocks are (n/2) x (n/2) block Toeplitz matrices. Moreover, let BQ, ..., Bn-\
denote the block components of the first block column of T~l, i.e., the solution of the
linear system of equations
Now note that, in view of the block lower triangular structure of Tn, we have
which shows that the first block column of T^"1 that solves (8.3.10) can be computed from
the first block column of T~L by means of two multiplications between an (n/2) x (n/2)
block Toeplitz matrix and a block vector. More specifically, we have
This observation leads to the following algorithm, whose cost is O(m3n + m2nlogn)
operations.
Algorithm 8.3.2. (Inversion of a Block Triangular Block Toeplitz Matrix).
This algorithm also solves the congruence relation A(z)B(z) = I mod zn.
â¢ Input: The m x m matrices A0,Ai,...,An-i, n = 2q, det-Ao ^ 0, defining the
first block column of the block triangular block Toeplitz matrix Tn (equivalently,
defining the matrix polynomial A(z) = Y^i=o AiZ1).

Section 8.3. Exploitation of Structure and Computational Tools 
223
â¢ Output: The matrices BQ, B\,..., Bn-\ satisfying (8.3.10) or, equivalently, such that
the polynomial B(z) = Y%=o Biz% solves the congruence A(z)B(z) â I mod zn.
â¢ Computation:
1. Compute BQ = A$l.
2. For i = 0,..., q â I, given the first column U = (B$ ,..., B^i_1}T of T~^,
compute the block vector V â (B^i,.. â¢, B^i+i_l}T, which defines the remain-
ing blocks of the first column o/T^+1; by applying (8.3.11) with n = 2Z+1,
where the products W â H^iU and V = T^W are computed by means of
Alg. 8.3.1.
We may remark that the solution of (8.3.9) can also be computed by formally ap-
plying the Newton-Raphson algorithm to the equation
where the unknown is A(z), thus obtaining the functional iteration
It is easy to show that
Hence the algorithm obtained by rewriting the above formula as
and by implementing the latter equation by means of the evaluation-interpolation pro-
cedure at the roots of unity is equivalent to Alg. 8.3.2 (compare with [BP86] for the
scalar case).
8.3.3 
Power Series Arithmetic
In certain computations related to Markov chains we need to perform several oper-
ations (multiplications and matrix inversions) among infinite lower block triangular
block Toeplitz matrices or, equivalently, among matrix power series. More specifically,
we have to compute rational functions Y = F(W^,..., W^1)) in the matrix power
series W& = W^(z), i = 1,..., h.
Now since all these series are assumed convergent for \z\ = 1, their block coefficients
will tend to zero. Hence we may replace the power series by matrix polynomials and
then apply the algorithms of Sec. 8.3.1. However, to truncate a power series W^'(z) at
a degree that results in a negligible remainder, we need to know its numerical degree.
For a matrix power series A(z) = ^i^o"-^2*) we define its e-degree as the minimum
integer d such that eT Â£)^d+i \Ai\ < eeT. When e is the machine precision, then d is
said to be the numerical degree of the matrix power series.
In this way, the evaluation of a function F by means of a coefficientwise 
arithmetic
requires that we apply several FFTs for all the products and inversions involved. Even
when the numerical degree of the output is rather small, the degrees of the intermediate

Fast Algorithms with Applications to Markov
224 
Chains and Queueing Models 
Chapter 8
power series might be large depending on the number of operands and on the com-
putations performed. For this reason, the fast coefficientwise arithmetic and the fast
Toeplitz matrix computations described in the previous section may be inadequate to
achieve highest performance.
An alternative way of computing the coefficients of the matrix power series F is
the use of pointwise arithmetic. In this approach, assuming knowledge of the numerical
degree d of F, it is sufficient first to evaluate all the individual matrix power series
W^\z) at the (d + 1) roots of unity and then to evaluate (d + 1) times the function F
at these values, thus obtaining y(a^+1), j = 0,1,... ,d.
In this way, by means of a single interpolation stage at the (d-\-1) roots of unity, it is
possible to recover all the coefficients of the matrix power series Y. This strategy allows
us to reduce the number of FFTs to m2(h + 1), and it is particularly convenient in the
case where the numerical degree of the output is smaller than the numerical degree of
the input or of the intermediate matrix power series.
To arrive at an efficient implementation of this technique, a criterion for the dynam-
ical evaluation of the numerical degree is needed. The design and analysis of such a
criterion is performed in Sec. 8.5.4, where the specific properties of the function F and
of the power series are used. For now, we simply assume that the following test function
is available: TEST(Y, A) is true if the numerical degree of Y is less than or equal to
the degree d of the matrix polynomial A(z) that interpolates Y at the (d + 1) roots of
unity.
A dynamic evaluation of the matrix power series Y may proceed according to the
following scheme:
Set d = Q.
Repeat
Set d = 2d+l, compute W^(z) at the (d+1) roots of unity, apply the point-
wise evaluation of F, and compute the coefficients of the matrix polynomial
A(z) of degree d that interpolates Y.
Until TEST(y, A) is true.
Set Y = A.
It is worth observing that to compute the values of W^(z) at the (d + 1) roots
of unity once the values of the same power series have been computed at the (â^ +
1) roots of unity, it is not necessary to apply afresh the FFTs of order d+1. The
same observation applies to the computation of the interpolation stage. Algorithms for
performing these computations, where part of the output has been precomputed, are
described in [BM97a].
8.4 
DISPLACEMENT STRUCTURE
For our applications in Markov chains we shall adopt a definition of displacement struc-
ture that seems to be particularly suitable for dealing with block Toeplitz matrices in
block Hessenberg form (compare with [KVM78], [KKM79a], [BP94], [HR84], and Ch. 1).
We define the n x n block down-shift matrix

Section 8.4. Displacement Structure 
225
where the blocks have dimension m, and consider the block displacement operator
defined for any n x n block matrix H. We also denote by Â£n(W) the n x n lower block
triangular block Toeplitz matrix defined by its first block column W'.
For the sake of notational simplicity, if the dimensions are clear from the context,
we use the notation VH instead of Vn mH, Z instead of Zn m, and C(W] instead of
AÂ»on
We also introduce the following notation: the block vectors En â [/, O,..., O]T,
En 
= [O, â¢ â¢ â¢ ) O, I]T denote mn x m matrices made up by the first and the last m
columns of the identity matrix of order mn, respectively.
Observe that for a general block Toeplitz matrix A the displacement VA is zero
except for the entries in the first block row and in the last block column. Hence the
rank of VA is at most 2m.
Following the discussions in Ch. 1, we shall say that the block matrix H has block
displacement rank r if r is the minimum integer such that there exist block column
vectors t/W and block row vectors V^\ i â l,...,r, satisfying the equation VH =
Â£][=! t/^VW. For an n x n block Toeplitz matrix A we obtain
An interesting property of V(H) is that the block displacement of the inverse matrix
can be explicitly related to the block displacement of the matrix itself; viz., if H is
nonsingular, then
The following result can be easily proved by extending to block matrices the same
proof given in [Bin83], [BP94] for the scalar case.
Theorem 8.4.1 (Displacement Repr sentation). Let Kn be an n x n block matrix
such that VKn = Y^i=i U^V^, where U^ and V^ are n-dimensional block column
and block row vectors, respectively. Then we have
0
The above result allows us to represent any matrix Kn as a sum of products of lower
and upper block triangular block Toeplitz matrices defined by the first block column of
Kn and by the block vectors U^\ V^ associated with the block displacement of Kn.
If the matrix Kn is nonsingular, then the above representation theorem can be
applied to K~l in the light of (8.4.2), thus leading to

Fast Algorithms with Applications to Markov
226 
Chains and Queueing Models 
Chapter 8
For the n x n block Toeplitz matrix Hn in lower block Hessenberg form (8.2.14) we
can easily verify that
and, hence,
This formula is the basis of the algorithm developed in Sec. 8.5.2.
Equations (8.4.3), (8.4.1), and (8.4.4) allow us to write useful inversion formulas for
a generic block Toeplitz matrix A and for the block Hessenberg block Toeplitz matrix
Hn of (8.2.14). We have, in fact,
and
where Cn , Cn denote the first and the last block columns, respectively, of H~l while
RJI , Rfi' denote the first and the last block rows, respectively, of the matrix H~l.
8.5 
FAST ALGORITHMS
In this section we develop efficient algorithms for the solution of the linear system of
(8.2.2), where P is given by (8.2.6) and (8.2.3), as well as the solution of the matrix
equation (8.2.8) by means of the finite or infinite systems of (8.2.13) or (8.2.12).
We start by providing a novel interpretation of the Ramaswami formula (8.2.10) in
terms of Toeplitz computations. We also provide an algorithm for the evaluation of this
formula using FFTs [Mei97bj.
In Sec. 8.5.2 we present a doubling method [LS96] for solving a sequence of the finite
systems of (8.2.13) and, by exploiting displacement structure, we further improve the
performance of the method. More specifically, we explicitly relate the inverse of an
n x n block Hessenberg block Toeplitz matrix Hn to the inverse of its ^ x ^ leading
principal submatrix HÂ«. This fact provides a means for computing the inverses of H^,
i = 0,1,... ,log2n, in O(m2nlogn -f m3n) operations. The algorithm is then used
for solving block Hessenberg block Toeplitz systems and for solving the matrix equation
(8.2.8). The cases of block tridiagonal matrices and of Toeplitz blocks are also discussed.
In Sec. 8.5.3 we apply the cyclic reduction algorithm [BM96a] for finite block He
senberg block Toeplitz-like systems (8.2.2), where PT is given in (8.2.6). We prove that
the Schur complement obtained after one step of cyclic reduction is still a block Hes-
senberg block Toeplitz matrix except for its first block column and its last block row.
This leads to an algorithm for solving block Hessenberg block Toeplitz-like systems in
0(ra2n log n + m3n) operations.
In the case of infinite matrices, by means of the correlation between matrix power
series and block Toeplitz matrices, we rephrase the properties of the Schur complement
in functional form and provide a fast and efficient algorithm for the solution of the
infinite system (8.2.12) based on a pointwise evaluation of the functions defining the
Schur complements. We also analyze the case of block tridiagonal matrices and show

Section 8.5. 
Fast Algorithms 
227
that the cyclic reduction procedure in this case is equivalent to the method of Graeffe
for squaring the roots of a polynomial.
In Sec. 8.5.5 we consider the problem of solving (8.2.12) in the case of non-skip-free
matrices, where the M/G/1 matrix (8.2.3) is obtained by partitioning into mk x mk
blocks the block Toeplitz-like matrix having the generalized block Hessenberg form
(8.2.4). Although the blocks obtained after a cyclic reduction step are no longer block
Toeplitz, they still exhibit displacement structure. This property allows us to devise a
fast algorithm for the solution of banded Toeplitz systems and for the computation of
the matrix G that solves (8.2.8). The algorithm, based on the LU factorization of an
M-matrix and on computing FFTs, has shown a good numerical stability.
8.5.1The Fast Ramaswami Formula
Observe that the relation (8.2.10) can be rewritten equivalently in terms of the UL
factorization of the infinite block Hessenberg block Toeplitz matrix H of (8.2.11), H =
UL, where
G is the solution of (8.2.8), and Ai are defined in (8.2.9). Ramaswami's formula (8.2.10)
can now be derived by solving the system of equations
In fact, by multiplying the above equation by the left inverse of U, we obtain that
Hence, by solving the lower block triangular system (8.5.1) by forward substitution, we
arrive at (8.2.10).
In this way, the computation of the vector TT is reduced to solving the lower block
triangular block Toeplitz infinite system (8.5.1). In [Mei97b] a fast Ramaswami formula
based on Algs. 8.3.1 and 8.3.2 has been devised with a substantial reduction of the
asymptotic computational cost. More specifically, the computational cost to calculate
p block components of the probability invariant vector TT is roughly p(7m2 -f 40 log no)
arithmetic operations, where no is the numerical degree of A(z) = ]Ci^o ^iZ%, whereas
the computational cost of the customary Ramaswami formula is 2pnom2 arithmetic
operations.
8.5.2A Doubling Algorithm
Consider the n x n block Toeplitz matrix Hn in the block Hessenberg form (8.2.14) that
is obtained by truncating the infinite matrix (8.2.11). Let us assume for simplicity that

Fast Algorithms with Applications to Markov
228 
Chains and Queueing Models 
Chapter 8
n = 2q for a positive integer q. Suppose that det H2j ^ 0, j = 0,1,..., q (this condition
is satisfied for a positive recurrent Markov chain), and partition the matrix Hn in the
following way:
where
By applying the Sherman-Morrison-Woodbury matrix inversion formula (compare with
[GV96, p. 3]; see also App. A) to the decomposition
we immediately find the following expression for the matrix inverse of Hn:
where
These relations have been used in [LS96], [Ste95] to devise a doubling algorithm for the
solution of the linear system (8.2.12), truncated at a finite system of block dimension
n, in <9(ra3nlog2 n) operations. The algorithm can be further simplified by exploiting
displacement structure, as shown in [BM98aj.
Indeed, observe that for the matrix H~l of (8.2.14) we have a relation of the form
(8.4.6). Hence the inverse of Hn can be explicitly determined as a sum of products of
block triangular block Toeplitz matrices that are defined by the block AQ, by the block
rows Bji , Rh , and by the block columns Cn , Cn .
The matrix inversion formula allows us to further relate the block vectors Cn , Cn ,
RÂ£\ RÂ£} defining the matrix H~l and the block vectors C(*\ rf2), R(n\ R(? defining
2 
2 
2 
T
the matrix Hn1. Moreover, since such relations involve only operations between block
Toeplitz matrices and block vectors, we may devise an efficient scheme for their imple-
mentation that is based on Alg. 8.3.1. This leads to a procedure for the computation of
the inverses of H2i, i â 0,1,..., q, that requires O(m3n + m2nlogn) arithmetic opera-
tions for n = 2q and a low storage cost; in fact, only four auxiliary block vectors need
to be allocated to carry out our algorithm.
The following result provides the desired correlation among block rows and columns
ofH-1 <mdH^1.
n 
T

Section 8.5. Fast Algorithms 
229
Theorem 8.5.1 (H~^ and H^1). The first and last block columns Cn , Cn and
the first and last block rows Rh , Rn of H~l satisfy the relations
w/iere Q = (/ + A QR(^T^cf}-lAQ.
Once the block vectors {R^. , JRA , Cn , C
n } have been computed, the solution x
of a system ffna; = 6 can be obtained by means of (8.4.6) and Thm. 8.5.1, leaving
unchanged the asymptotic cost O(m3n + m2nlogn).
For block tridiagonal matrices Hn, like the ones arising from QBD problems, the
formulas of Thm. 8.5.1 are simplified and the cost becomes O(m3n).
For non-skip-free matrices, Hn is obtained by reblocking the matrix (8.2.4) leading
to the structure (8.2.5). In this case, it has been proved in [GHT97] that the matrix G
can be represented as G = Ffc, where F is the k x k block Frobenius matrix
that is defined by the first block column of G = (Gij). In particular, the block dis-
placement rank of G is 1 with respect to the operator Vjt,m.
Here we show that, for the non-skip-free case, our algorithm also can be further
simplified. In fact, since Hn is an nk x nk block Toeplitz matrix, in light of (8.4.6) we
may rewrite H~l as
for suitable fcn-dimensional block vectors tin , Vn , i = 1,2.
In this way it is sufficient to relate the block vectors Unâ¢ , Vn with nÂ« , vâ¢ for
T 
f
i = 1,2. This relation is implicitly provided by Thm. 8.5.1. We observe that the
computation of Un , Vn is reduced to performing a finite number of products of block
Toeplitz matrices and block vectors of block dimension nk except for the computation
of the inverse of Q. For this purpose we have the following result [BM98a].

Fast Algorithms with Applications to Markov
230 
Chains and Queueing Models 
Chapter 8
Theorem 8.5.2 (Displacement Rank of Q). 
The block displacement rank of Q,
with respect to the operator Vfc m, is at most 6.
Prom Thms. 8.5.1 and 8.5.2, it follows that the inverse of Q can be represented
as the sum of at most seven products of nk x nk lower and upper block triangular
block Toeplitz matrices. Thus, its inverse can be computed fast in O(k2m3) arithmetic
operations. Hence, each doubling step of size 2* requires O(m2) FFTs of order k2l and
the inversion of a matrix having block displacement rank at most 6. Therefore, the total
cost is O(m2kn\og(kri)) + O(knm3).
8.5.3 
Cyclic Reduction
We now describe a method for the numerical solution of (8.2.2), introduced in [BM96a],
which is based on the block LU factorization of the n x n block matrix HHHT, where
H = I â PT is the block Hessenberg block Toeplitz-like matrix of (8.2.6) and II is a
suitable permutation matrix.
This method relies on a technique originally introduced for solving certain block
tridiagonal block Toeplitz systems arising from the numerical treatment of elliptic equa-
tions [BGN70] and rediscovered in [LR93]. This technique is based on the reduction of
the block tridiagonal matrix into a new block tridiagonal matrix of half the size. The
reduction is repeated cyclically until a system of size 1 is obtained. This technique of
cyclic reduction was called successive state reduction in [LR93].
Let q > 2 be an integer such that n = 2q and consider the matrix in block Hessenberg
form, that is, block Toeplitz except for its first block column and its last block row,
uniquely defined by the blocks HQ = âAo, HI = I â A\, H\ = / â BI , HI = I â C\,
Hi = âAi, Hi = âBi, Hi = âCi, i â 2,...,n â 2, Hn-\ = âJ9n_i, Hn = âCn,
Hn-i = âCn-i-
By interchanging block rows and columns of H^ = H according to the odd-even
permutation (1,3,5,..., n - 1,0,2,4,6,..., n â 2) we obtain the matrix
where n<Â°> is the block odd-even permutation matrix, and T^Â°\ T2
(0), Z^, and W<Â°>
are (n/2) x (n/2) block Toeplitz-like matrices defined by

Section 8.5. 
Fast Algorithms 
231
By applying one step of block Gaussian elimination (see Ch. 1) to the 2 x 2 block matrix
(8.5.3) we find that
Now we show that the Schur complement H^ has the same structure as H^ in
(8.5.2). For this purpose, we define the block column vectors H0dd = (H2i+i)i=o,n/2-2>
Heven = (H2i)i-o,n/2-2 an<i consider the (n/2 â 1) x (n/2 â 1) block matrices
Let us partition .H^1) as
where ,R(1) is an (n/2 - 1) x (n/2 - 1) blc ck submatrix. Then from (8.5.5) and (8.5.4)
it follows that
Since the linear space of lower block triangular block Toeplitz matrices is closed
under multiplication and inversion, i.e., it constitutes a matrix algebra, we deduce that
R^ is a block triangular block Toeplitz matrix. Hence the Schur complement H^ has
the same structure as (8.5.2); i.e., pW1* = I â H^ is still a block Toeplitz-like matrix in

Fast Algorithms with Applications to Markov
232 
Chains and Queueing Models 
Chapter 8
block Hessenberg form, where H^ is uniquely defined by the matrices HQ ,..., H^L_2,
H[1},..., H^2_v and tf !(1),..., Â£$ such that
where Q^1) = (//j )i=o,...,n/2-2- Moreover, it is easy to check that P^ is a stochastic
matrix. Therefore, the cyclic reduction process can be recursively applied q â 1 times
until the 2m x 2m matrix #(?-!) is computed. The resulting algorithm, which computes
the block LU factorization of a matrix obtained by suitably permuting rows and columns
of H, is described by the formulas (8.5.2)-(8.5.7) adjusted to the generic jih step. This
algorithm outputs the blocks H^+l\ i = 0,..., T*-J-1 - 2, H\j+l\ i = 1,..., 29-'-1,
and #P'+1), i = 1,..., y-i-i _ i, defining the block matrices T[J\ W&, K&, H^+1\
j = 0,l,...,?-2.
Now consider the problem of solving the system (8.2.2) and partition the vector
TT into blocks of length m according to the block structure of H, i.e., set TTT =
[7To\7rf,. â¢ â¢ ,7Tn-i]- 
Moreover, denote by TT_ 
the vector having blocks TT^J, i =
0,..., 2q~j - 1, j = 0,1,..., q - 1, where n = 29, q > 2. Similarly, denote by TT^
the vector having blocks Tf(2i+i)2J-1t i = 0,..., 29~-7 â 1, for j = 1,..., q â 1. In this
way, at the jih recursive step of the cyclic reduction algorithm applied to H^'TT = 0
we obtain the system H^TT_ = 0, j = 0,..., q â 1. In fact, after the odd-even permu-
tation on the block rows and the block columns and after performing one step of block
Gaussian elimination we obtain the system of equations
(compare with (8.5.5)). At the end of the cyclic reduction algortihm we have to solv
the 2m x 2m homogeneous system
and we may recover the remaining components of the vector TT by means of back sub-
stitution by using the following relation derived by (8.5.8):
The algorithm thus obtained for the computation of the probability invariant vector
TT can be carried out in the following way.
Algorithm 8.5.1 (Cyclic Reduction for the Computation of TT).
â¢ Input: Positive integers m, q, n, n = 2q, and the nonnegative m x m matrices Ai,
i = 0,1,..., n â 2, Bi, i = 1,..., n â 1, (7$, i = 1,..., n, defining the stochastic
matrix P of (8.2.6).

Section 8.5. Fast Algorithms 
233
â¢ Output: The solution TT of system (8.2.2).
â¢ Computation:
1. Cyclic reduction stage
Let H^ = I â PT and, for j = I,... ,q â I, recursively compute the m x ra
matrices H\3 , H^lt i = 0,1,..., nj â 1, and Â£TJ+1; i = 0,1,..., rij â 1, Uj =
n/li, defining the njXnj block matrices H& = T^~l}-Z^-l^j~l)~lW^-^
by means of the formulas (8.5.6)-(8.5.8) represented for j = 0 and here ad-
justed for the recursive jth step of the cyclic reduction algorithm as follows:
2. Back-substitution stage
Solve, by means of LU or UL factorization, the 2m x 2m system of equations
Compwie 7r5'+1) = -Tpr V^7r!f+1) for j = 9 - 2,..., 0.
3. Normalization stage
Normalize the vector TT.
The most expensive part of each recursive step of the above algorithm is to compute
the blocks of the matrix H^. This computation is reduced to performing operations
among block triangular block Toeplitz matrices. In fact, observe that FÂ± , F% , an
/Q\ â1
F2 
are block triangular block Toeplitz matrices. Thus we may use the algorithms
described in Sec. 8.3.1. In this way the cost of the jth step is O(m2nj logn^ -f m?nj)
arithmetic operations and the overall cost of the algorithm to compute TT by means of
cyclic reduction is O(m2nlogn + m3n) arithmetic operations. On the other hand, it
can easily be verified that the computational cost incurred by applying the customary
block Gaussian elimination is O(m3n2).
In the case where the matrix H is block tridiagonal, i.e., when the Markov chain
comes from a QBD problem, the whole computation can be carried out in a simpler
form and we arrive at the finite-dimensional analogue of the algorithm of [LR93] (see
also [YL91]). In fact, let

Fast Algorithms with Applications to Markov
234 
Chains and Queueing Models 
Chapter 8
then the block tridiagonal matrix H^ obtained at the jth recursive step of the cyclic
reduction procedure, defined by the blocks H\3\ i = 0,1,2, H\ , H\3', i â 1,2, j =
0,..., Iog2 n â 1, is such that
It can be verified that the overall cost for the computation of TT in the QBD case is
reduced to 0(ra3logn -f m2n) operations, against the O(m3n) cost of Gaussian elimi-
nation.
8.5.4 
Cyclic Reduction for Infinite Systems
The technique of successive state reduction, discussed in the previous section, can be
successfully applied even to infinite systems. In this case, at each stage of the cyclic
reduction the size of the problem does not decrease since we obtain a sequence of infinite
block Hessenberg block Toeplitz-like matrices. However, the sequence of problems that
we obtain in this way converges quadratically to a block bidiagonal block Toeplitz system
that can be solved easily. A sort of a back-substitution step allows us to recover the
solution of the starting problem. We now consider the problem of solving (8.2.12).
By performing an odd-even permutation of the block rows and block columns in
(8.2.12) we find that
where Tf , T2
( , and Z^ are infinite block triangular block Toeplitz matrices and W^
is an infinite block Hessenberg block Toeplitz matrix (compare with (8.5.4)). Applied
to the 2 x 2 block system (8.5.9), one step of block Gaussian elimination yields
where Hâ¢ = T2
{0) - Z^T^W^ is the Schur complement of T2
(0). It is interesting
to observe that Q^ = I â H^ is a nonnegative block Hessenberg matrix which, except
for the first block column, has the block Toeplitz structure of the matrix of (8.2.11).
In fact, TI , T2 , and Z^ are infinite block triangular block Toeplitz matrices and
W(Â°) is an infinite block Hessenberg block Toeplitz matrix. Hence JET^
1) is uniquely
determined by the blocks A^ , i > 1, A^\ i > 0, defining the first two block columns
ofQW.
We may recursively apply the same reduction (cyclic reduction) to the matrix equa-
tion (8.5.10) thus obtaining the sequence of systems

Section 8.5. Fast Algorithms 
235
such that
The matrix {H^} is such that Q^ = I â H^ is a nonnegative matrix having,
except for the first block column, the same block Hessenberg and Toeplitz structure of
the matrix in (8.2.11). Each matrix Q^ is uniquely determined by the blocks A\3\
i > 1, Af, i > 0, defining its first two block columns.
Due to the correlation between infinite block triangular block Toeplitz matrices and
matrix power series, the block entries {Af }i>0) {-^i }i>i> obtained at step j' + 1,
can be related to the block entries {Af }i>o, {A? }i>\ that are obtained at step j by
means of functional relations.
More specifically, for any j > 0, let us associate with the matrix sequences {Af }i>0)
{â¢^i }i>i the formal matrix power series
respectively, where A\'=A\'=Ai,i>l,AQ=Ao. Then we can write
where
The relations (8.5.12), in addition to expressing in compact form the recursions of the
cyclic reduction method, provide a tool for the efficient computation of the matrices
{A/ }i>o, {At }i>i, by using the algorithms of Sec. 8.3.
The following results (see [BM96a], [BM96b], [BM97a]) are fundamental for devising
an efficient algorithm that is based on cyclic reduction.
Theorem 8.5.3 (Representation of G). The blocks {A^'}i>Q, {Af }i>i are non-
negative matrices such that J^J^ Af , AQ + X^iJT -^H are stochastic. Moreover, if
the matrix I â Y^o G1'23 Af^ is nonsingular, then
Under mild conditions, usually satisfied in applications, the block entries A^ 
and
A\3) converge quadratically to zero for i > 2 and for j -> oo [BM96a], [BM96b], [BM97a].

Fast Algorithms with Applications to Markov
236 
Chains and Queueing Models 
Chapter 8
Theorem 8.5.4 (Convergence Properties). Let G' = lim^oo Gj. Then the follow-
ing convergence properties hold:
1. lim^oo A\j) = 0 for i > 2.
2. // the entries of the matrix (I â X)i^T^i )-1 are bounded above by a constant,
then the sequence of matrices R^ = AQ (/â^Zi^T ^i }~l converges quadratically
to the matrix G'.
3. // the solution G of (8.2.8) is irreducible, then lim.,--^ AQ (I â Af'}~1 = G' and
lirn^oo A^ = 0, i > 2.
Under the assumptions of Thms. 8.5.3 and 8.5.4, the following algorithm for the
numerical computation of the matrix G can be applied.
Algorithm 8.5.2 (Computation of the Matrix G}.
â¢ Input: Positive integers qo,no,m, HQ = 29Â°, an error bound e > 0, and the non-
negative m x m matrices Ai, i = 0,1,..., no, where no is the numerical degree of
A(?\ â y^+Â°Â° A.~i
A\z) â 2^i=o -A-iZ .
â¢ Output: An approximation of the solution G of (8.2.8).
â¢ Computation:
1. Apply the cyclic reduction procedure to (8.2.12), using (8.5.12), and thus ob-
tain the sequence (8.5.11) of infinite systems defined by the blocks A^, Af',
j = 1,2,..., r, until one of the following conditions is satisfied:
where, at each step j, the matrix R^ is defined by Thm. 8.5.4 and E is the
ra x m matrix having all entries equal to 1.
2. Compute an approximation of the matrix G:
(a) If condition (Cl) or condition (C2) is verified, compute an approximation
ofG by replacing, in the right-hand side o/(8.5.13) for j = r, the positive
powers of G with R^ and by stopping the summation to the numerical
degree of the series .A^z).
(b) If condition (C3) is verified, an approximation of G is given by AQ(! â
#>)-'â¢
The effectiveness of Alg. 8.5.2 relies on the possibility of computing the blocks Af ,
Af , at each step j, in an efficient way. We may truncate the power series A^\z)
and A^(z) to polynomials having e-degree HJ and n^, respectively, according to the
definition given in Sec. 8.3.3. In this way, the matrices Â£2o A(J]T and A$ + Â£?=! A(fT

Section 8.5. Fast Algorithms 
237
are e-stochastic. (A nonnegative matrix A is said to be e-stochastic if |(7 â A)e\ < ee.
If e is the machine precision, A is said to be numerically stochastic.)
With this truncation, we may reduce the computation of the cyclic reduction steps
to the multiplication of matrix polynomials and to the inversion of matrix polynomials
modulo zn for values of n that can be dynamically adjusted step by step according to
the numerical degrees of the power series involved. In this way we obtain an algorithm
similar to the one described in Sec. 8.5.4 in the finite case but which does not require
the truncation of the infinite matrices to a finite size. The cost of this computation,
based on the coefficientwise polynomial arithmetic described in Sec. 8.3.1, amounts to
O(m3n + m2nlogn) operations.
A more efficient technique consists of using the pointwise evaluation-interpolation
procedure at the roots of unity of the series involved in (8.5.12). This approach consists
of performing the following steps at stage j + 1 of the cyclic reduction procedure:
1. Evaluate the series A^d(z)^ AeJen(z), A^d(z): AeJen(z) at the rijth roots of unity,
where Ujâ¢, â I = 2q â I is an upper bound to the e-degree of the above series.
2. Perform a pointwise evaluation of (8.5.12) at the rijth roots of unity.
3. Compute the coefficients of the matrix polynomials P(z] and P(z) of degree HJ â
1, which interpolate the values of the matrix series A^+i^(z) and A^+l\z) of
(8.5.12).
4. Check whether the matrix polynomials P(z) and P(z) are good approximations
of the series A^+l\z), A^+l\z], respectively.
5. If the polynomials P(z) and P(z) are poor approximations of A^+l\z), A^+1\z),
set rij = Inj and repeat steps 1-5 until the required accuracy of the approximation
is reached.
Due to the properties of the FFT in each doubling step, part of the results is already
available at no cost. Moreover, unlike in the version based on coefficientwise polynomial
arithmetic, the computation of the reciprocal of a polynomial is avoided and the order of
the involved DFT and IDFT is kept to its minimum value, thus substantially reducing
the computational cost per iteration.
The following properties [BM97a] of cyclic reduction and of the FFT provide a good
test for checking the accuracy of the approximations P(z), P(z) of the series A^+1^(z),
A^+l\z) needed at step 4.
Theorem 8.5.5 (Two Recursive Relations). For any j > 0, let
Then the following recursive relations hold:

Fast Algorithms with Applications to Markov
238 
Chains and Queueing Models 
Chapter 8
Theorem 8.5.6 (Two Inequalities). At stepj, let P^(z) = ^~Q Prf and P<n> =
]Cr=o ^iZ% ^e the matrix polynomials of degree n â I interpolating the series A^\z),
A^ (z) at the nth roots of unity. Then the following inequalities hold:
Theorems 8.5.5 and 8.5.6 provide a good test with which to check the accuracy of
the approximations of the series A^(z), A^\z) at each step j. Indeed, suppose we
know the coefficients of the series A^(z), A^\z) and compute the coefficients Pi, PJ,
i = 0,..., n - 1, of the approximations P^(z) = Yâ¢=Q PiZ\ P(n}(z) = YZ=o ^ of
the series A^+l\z], A^+l\z) by interpolating the functional relations (8.5.12) at the
nth roots of unity. Prom Thm. 8.5.6 it follows that, if
and
then the matrices Y%=o AlJ+l}T and A% + Â£)Â£=! A\j+l)T are e-stochastic. Hence the
series A^+1^(z), A^+l\z) have e-degree n â I and the matrix coefficients of P^n\z]
and P(n^(z) are approximations of the corresponding coefficients of A^+1\z), A^+l^(z)
within the error e. It is important to point out that (8.5.15) and (8.5.16) can be easily
applied without knowing the coefficients of the series A^+1^(z), A^+l^(z). In fact,
a(j+i) aiKj crj'+1) can be obtained explicitly by means of (8.5.14) at the cost of O(m3)
arithmetic operations. This provides an efficient tool to check the accuracy of the
approximations P<n>(z), PÂ»(z) to A^+l\z], A^+^(z}.
Algorithm 8.5.3 (Computation of A^+^(z), 
AV+V(z)).
â¢ Input: An error bound e > 0, nonnegative integers HJ, qj, and the matrix power
series A^(z), A^(z) having numerical degree bounded above by HJ = 2qj.
â¢ Output: The matrix power series A^+l^(z), A^+l^(z) and an upper bound n^+i =
2qj+l to their numerical degrees.
â¢ Computation:
1. Set qj+1 = qj - l,nj+l = 2^.
2. Compute a^+1^ and crj+1^ by means of (8.5.14).
3. Evaluate the functions A^d(z), A^}en(z) andA^d(z}, AeJen(z) at the (nj+i)th
roots of unity by means of DFTs.

Section 8.5. Fast Algorithms 
239
4. Apply equations (8.5.12) and obtain the coefficients of the matrix polynomials
P(z) and P(z) interpolating A^+l\z], A^j+l\z) at the (nj+i)th roots of
unity.
5. Apply the tests (8.5.15) and (8.5.16) to check whether P(z) and P(z) are
good approximations of the series. If the inequalities (8.5.15) and (8.5.16)
are satisfied, then skip to the next stage. Otherwise, set Uj+i = 2nj+i,
qj+i = QJ+I + 1 and repeat from stage 3.
6. Set A(i+l\z] = P(z), A^+1\z] = P(z).
In the case of QBD processes, where P is a block tridiagonal matrix, the functions
A^(z) and A^\z) are matrix polynomials of degree 2 and 1, respectively. By comparing
terms of the same degree, we obtain the simple relations (compare [LR93]):
Moreover, it is surprising to observe that the cyclic reduction step is equivalent to the
squaring step in the Graeffe algorithm, which is used for factoring polynomials [BP94],
[Ost40j. In fact, the first functional relation of (8.5.12) can be rewritten equivalently as
By evaluating the determinants of both sides of the above equation we obtain
where ^\z] = det(z/ â A^(z)}. The latter relation extends to matrix polynomials
the Graeffe iteration, which is formally obtained from (8.5.17) for ra = 1.
8.5.5 
Cyclic Reduction for Generalized Hessenberg Systems
Since a non-skip-free matrix can be reblocked into an M/G/1 matrix, we may apply the
cyclic reduction technique for solving problems with the structure (8.2.4). However, in
this way we would not exploit the additional structure of the problem, more specifically
the fact that the blocks of the matrix P, being block Toeplitz matrices, have block
displacement rank at most 2 and the fact that the matrix G that solves the equation
(8.2.8) has block displacement rank 1. In this section we present new results that allow
us to fully exploit the problem structure.
So consider the problem of solving the infinite system (8.2.12). (The same technique
can be applied to solve any generalized block Hessenberg block Toeplitz system, for
instance, banded block Toeplitz systems [BM97b].)

Fast Algorithms with Applications to Markov
240 
Chains and Queueing Models 
Chapter 8
Let Ai of (8.2.5) denote the matrices that are obtained by reblocking the matrix
(8.2.4) and consider the sequence of matrix power series A^(z), A^\z] that are ob-
tained by means of the functional relations (8.5.12), where each A is replaced with
A. We note that the matrix power series H^(z) = ^2^nf))zi = zl - A^(z) and
H(Q\Z} = Â£tÂ£ U
(i'
]z
i = 1- A(Q\z] are block Toeplitz matrices.
A direct inspection further shows that the Toeplitz structure is generally lost by the
matrix power series H& (z) = Â£+Â£ H(f}zi = zI-A^ (
z) and H& (z) = Â£tfo W^V =
I â A^\z] for j > 1. However, the displacement structure is preserved by the matrix
power series T-i^(z) and H^\z}. This fact allows us to devise an FFT-based imple-
mentation of the cyclic reduction algorithm [BM97b].
Theorem 8.5.7 (Displacement of Ti^'^(z), 'H^(z)). For the matrix power series
H^(z) = zl â A^\z], Ti^(z) = I â A^\z) generated at jth step of cyclic reduction
we have
for suitable block row vectors U^\z], U^\z) and block column vector V^(z).
As a consequence, the matrix power series H^(z), Ti.^(z) have block displacement
rank not greater than 2 and 3, respectively. Moreover, from the above theorem and
from Thm. 8.4.1, we can derive a suitable representation formula for the matrix power
series W^') (z} and fiW(z).
The explicit equations relating the block vectors at two consecutive steps of the cyclic
reduction algorithm can be derived by generalizing the analogous relations provided in
[BM97b] for banded block Toeplitz matrices. Such relations, expressed in functional
form, consist of performing products and inversions of matrix power series. Hence, the
computational cost of performing the jth step of cyclic reduction by using FFTs reduces
to O(njin3k\og2 k + m2knj logn.,-), where nj is an upper bound to the numerical degree
of the matrix power series A^(z) and A^\z).
In the case where Ai = O for i > 2, i.e., when the reblocked matrix P is block
tridiagonal, the above results can be simplified. We can give explicit expressions for the
blocks H(
0
j} = -A$f\ n{j} =1- A[j\ H%} = -A(
2
j\ H(j) =1- Â£f by means of the
following result.
Theorem 8.5.8 (Displacement ofH^). 
If Ai = O fori > 2, then for the matrices
H(Q\ H(f, H(f, H{J\ j > 0, generated by the cyclic reduction (8.5.12) we have

Section 8.6. Numerical Experiments 
241
for suitable block row vectors K^, U\, IVf , z = 0,1, and block column vectors S^,
V(J} 
C(J} j _ n i
vi 
> ^i 
> t ~ 
u> â¢"â¢â¢
For this case, the computational cost per step is reduced to O(m3k\og2 k) operations.
Moreover, the quadratic convergence properties of the blocks Af , A\ of the algorithms
of Sec. 8.5.4 can be directly extended to the blocks Af , Af . In particular it holds
that the block Af , i > 1, tend to zero and the block AI tends to a matrix of block
displacement rank 2.
The algorithm devised in this way is more efficient than the linearly convergent
method of [GHT97], particularly when the block dimension k is large. In fact, the
computational cost of one step of the latter method is O(m3kn) and many iterations
may need to be computed in order to reach a good approximation of G.
Due to the structure of the blocks Ai, some interesting relations hold between the
zeros of the analytic functions (j)(z) = defH(z) â det(z/ - A(z)) and if>(z) = det(zkl -
A(z}). In fact, in [GHT97] it is proved that
It then follows that if Â£ is zero of the analytic function ij)(z), then Â£k is zero of ^>(z}.
Conversely, if 77 is a zero of (/>(z), then there exists a fcth root of 77 which is zero of t/>(z).
For positive recurrent Markov chains, the analytic function (f)(z) has exactly km â 1
zeros lying in the open unit circle and one zero equal to 1. This property, together with
relations (8.5.18) and (8.5.17), yields the following convergence result [BM98bj.
Theorem 8.5.9 (Convergence Speed). Let Ai = 0 for i > 1. //&, i â 1,2,..., are
the. zeros of cf)(z} ordered such that |Â£i| < (Â£2! < â¢ â¢ â¢ < |Â£mfc| = 1 < |Â£mfc+i| < â¢ â¢ â¢ and if
Ai â O for i > 2, then \\A% \\ â O((\Â£,mk+i\~l + e)k23} for any matrix norm and for
any e > 0.
The above theorem is a special case of a more general convergence result provided
in [BM97b], where cyclic reduction is used for solving a banded Toeplitz system.
8.6 
NUMERICAL EXPERIMENTS
The algorithms described in this chapter for the numerical solution of Markov chains
have been implemented and tested in [ALM97], [BM96a], [BM96b], [BM97a], [Mei97b].
Here we report the most significant results.
In Fig. 8.2 we compare the time needed for the computation of the Ramaswami
formula by means of the fast and the customary techniques for a problem investigated
in [ALM97].
Tables 8.1 and 8.2 display the CPU time in seconds, the number of iterations, and
the residual error of the pointwise cyclic reduction (CR) algorithm and of the fast
functional iteration (FI) method usually used in applications for the computation of G
[Mei97a]. The problem, which has been solved, arises from the modeling of a Metaring
MAC protocol [ALM97]. Its solution depends on a parameter p, related to the positive
recurrence, and involves blocks of size m = 16 and numerical degrees less than 265.

242
Fast Algorithms with Applications to Markov
Chains and Queueing Models 
Chapter 8
Figure 8.2. Fast and customary Ramaswami formula.
The acceleration of the Latouche-Stewart (LS) algorithm, in view of the displace-
ment rank (DR) properties, is shown in Table 8.3, where the algorithms have been tested
on the same Metaring MAC protocol problem for p â 0.8.
The non-skip-free version of cyclic reduction (NSF-CR) has been tested on block
tridiagonal problems of different block sizes k and compared with the ordinary CR and
with the algorithm of [GHT97]. The results are summarized in Table 8.4, where the
order of magnitude of the residual error is shown. An asterisk (*) in the tables means
failure of the algorithm due to lack of memory.
Table 8.1. Cyclic reduction.
p || Time(s.)
0.1
0.8
0.9
0.95
0.96
0.97
0.9
1.5
2.3
2.4
2.4
2.5
Iterations
9
13
14
16
17
20
Residual
1.8 -10~13
5.4 -10-14
1.5 -lO"14
1.8 â¢ 10~14
2.3 â¢ 10-14
4.2 -10~14
Table 8.2. Functional iteration method.
p 
|| Time (s.)
0.1
0.8
0.9
0.95
0.96
0.97
0.3
3.9
10.8
44.4
91.4
96.8
Iterations
22
148
373
1534
3158
3336
Residual
2.2- 10~14
1.1 -HP13
1.2-10-13
1.3 -10~13
1.3- 10~13
1.3-1Q-13
FI/CR |
0.3
2.6
4.7
18.5
38.1
38.7

Section 8.6. Numerical Experiments 
243
Table 8.3. Fast and customary doubling algorithm.
DR
n
256
512
1024
2048
4096
Time
8.7
18.1
39.2
89.3
193.4
Residual
2.5 â¢ 10~3
4.0 -10~4
1.7- 10~5
3.4 â¢ 10~8
1.3 -10-13
LS
Time | Residual
38.3
103.5
264.9
*
*
2.5- 10~3
4.0 â¢ 10~4
1.7-HT5
*
*
Ratio
4.4
5.7
6.8
*
*
Table 8.4. Non-skip-free Markov chains.
k
16
32
64
128
256
512
NSF-CR
Time
14.4
19.2
37.7
89.5
146.4
448.1
Residual
10-16
10-15
io-13
io-n
1Q-15
io-11
CR
Time
1.1
9.8
136.3
*
*
*
Residual
io-16
io-15
io-14
*
*
*
GHT
Time
6.1
13.3
60.4
555.5
816.4
5830.3
Residual
io-11
io-11
io-11
io-11
io-11
io-4

This page intentionally left blank 

Chapter 9
TENSOR DISPLACEMENT
STRUCTURES AND
POLYSPECTRAL MATCHING
Victor S. Grigorascu
Phillip A. Regalia
9.INTRODUCTION
This chapter studies the extension of the notion of structured matrices to tensors. These
are multi-indexed arrays, in contrast to matrices, which are two-indexed arrays. Such
arrays arise while considering higher-order cumulants and the corresponding polyspectra
in applications, particularly in blind model identification and approximation problems.
While matrices are adequate representations for second-order statistics, higher-order
cumulants are more naturally (and more completely) studied in a tensor setting. In
this chapter, we examine the displacement rank concept of Ch. 1 for tensors. After
a semitutorial presentation of Tucker products and cumulant representations of linear
systems, we show links between interpolation of polyspectral values by a linear model
and the Tucker factorability of a certain Pick tensor. We also develop a particular
higher-order extension of a Schur-type algorithm, based on a novel outer product of
tensors. This leads to a pyramidal factorization approach for tensors, which specializes
to triangular factorization in the matrix case.
9.2 
MOTIVATION FOR HIGHER-ORDER CUMULANTS
Recent years have witnessed increasing interest in higher-order cumulants, which convey
more information about an underlying stochastic process than second-order statistics,
including non-Gaussianity, phase information, nonlinearities, and so forth.
An example arises in the blind identification problem, in which one considers a
process {y(-}} generated by
where (y(-)} is observable; {u(-}} is an unobserved but independent, identically dis-
tributed (i.i.d.) stochastic process; and {hi} denote the impulse responses of an unknown
245

246 
Tensor Displacement Structures and Polyspectral Matching 
Chapter 9
channel. The transfer function associated with the channel is
where in this chapter we are using z (instead of z~1} to denote the delay operator, viz.,
z [u(n}\ = u(nâ1). A basic problem in this setting is to estimate the impulse response
{hi} or the transfer function H(z}, given the output sequence {?/(â¢)}â¢
Second-order statistics of the output process {?/(â¢)} allow one to determine the mag-
nitude \H(e^}\ of the channel but not its phase, whereas higher-order statistics of the
output process allow one to deduce both the magnitude and the phase of the chan-
nel frequency response. This added informational content served as the impetus for a
revived interest in higher-order statistics in signal processing in the early 1990s [Men91].
Despite numerous intriguing developments in this field, including the ability to sepa-
rate minimum-phase from nonminimum-phase components of a signal [GS90] and linear
from nonlinear components [TE89], or the ability to locate more sources than sensors in
an array processing context [Car90], practical interest in algorithm development for sig-
nal processing applications has dwindled rapidly. This phenomenon may be attributed
to two basic obstacles underlying cumulant-based signal processing.
The first concerns the computational complexity of estimating cumulants from a
given time series. Although empirical estimation formulas are available, they tend to be
computationally expensive, and in some cases they converge more slowly than estimators
for second-order statistics.
The second obstacle concerns the successful extraction of desired information from
higher-order statistics. Although such statistics often are touted for carrying phase
information, they also convey information on potential nonlinear mechanisms. This
is problematic in applications where the underlying process {?/(â¢)} is linear, since any
estimation errors can result in higher-order (estimated) statistics that are suddenly
incompatible with any linear process. (By a "linear process," we mean the output of a
linear time-invariant system when driven by an i.i.d. process.)
To appreciate this problem further, let us turn momentarily to second-order statis-
tics. Suppose {?/(â¢)} is a real-valued wide-sense stationary stochastic process. We intro-
duce a finite number of autocorrelation lags:
Under the mild constraint that the Toeplitz matrix
be positive definite, the familiar Yule-Walker equations, in which the unknowns
{cr2, ai,..., OM} are obtained according to

Section 9.2. Motivation for Higher-Order Cumulants 
247
allow one to deduce a candidate linear model for the data (y(-)}- In particular, choosing
yields a stable autoregressive transfer function such that, when driven by unit-variance
white noise, the resulting output sequence {$(â¢)} is compatible with the given second-
order statistics, i.e.,
Whether the initial process {y(-}} 1S autoregressive, or even linear, is irrelevant to the
validity of this result.
Many attempts to generalize such relations to higher-order cumulants may be found
in [GM89], [GS90], [JK92], [NM93], [SM90a], [SM90b], under the hypothesis that the
underlying (non-Gaussian) process {?/(â¢)} is linear and generated from a rational trans-
fer function of known degree. In cases where the process is linear, but the degree of the
underlying model is underestimated, the equations so solved do not in general lead to a
model that replicates the cumulant values used for its determination. The incompati-
bility between the resulting model and the cumulant values used to determine it implies
that such methods do not correctly capture the structure of the data.
One of the few results establishing compatibility of higher-order cumulants with a
linear process is given by Tekalp and Erdem [TE89]. Introduce the fcth-order cumulant
lags of a process {?/(â¢)} as
where cum[- â¢ â¢] denotes the cumulant value of the k random variables that form its
argument. (A nice tutorial overview of cumulants in signal processing may be found
in [Men91].) Since our process {?/(â¢)} ls assumed stationary, the cumulant value here
depends only on the relative lags ii, ..., ik-i- The fcth-order polyspectrum of the
process {?/(â¢)} is a (fc â l)-dimensional ^-transform of the sequence {ci1,...,ifc_i}, defined
as [Men91]
whenever the infinite sum converges on the unit polycircle \z\\ = â¢ â¢ â¢ â \Zk-i\ = 1. A
well-known relation [Men91], [NP93] shows that whenever {?/(â¢)} is the output process of
a linear system with transfer function H(z), which in turn is driven by an i.i.d. sequence,
the polyspectrum assumes the form
where jk is the fcth-order cumulant of the i.i.d. input sequence (assumed nonzero).
The polycepstrum is defined as the logarithm [NP93], [TE89] of the polyspectrum
which, for the linear case, gives the separable structure
Assuming H(z) has no poles or zeros on the unit circle \z\ â 1, we may develop a
multidimensional ^-transform expansion of the polycepstrum as

248 
Tensor Displacement Structures and Polyspectral Matching 
Chapter 9
in which the terms {cili...,ik_1} are the polycepstral coefficients. The process {y(-}} 1S
then linear if and only if the polycepstral coefficients are nonzero only on the principal
axes (only one index nonzero) and the main diagonal (il = i2 = â¢ â¢ â¢ = ik-i), with
elementary symmetry relations connecting the nonzero coefficients [TE89]. Since this
result involves infinitely many cumulant lags, its practical application is limited to cases
where the cumulant lags are finite in duration or decay sufficiently rapidly in all indices
as to render truncation effects negligible [NP93].
Similar in spirit to the cumulant matching approach of [Tug87], [Tug95], a generic
problem statement that motivates the present work is the following: Given a finite
number of cumulant lags, or possibly a finite number of evaluations of a polyspectrum,
under what conditions can a linear process be fit to such values?
For second-order statistics, this problem is solved. Various formulations are pos-
sible, including the Yule-Walker equations, the closely connected Levinson recursion,
the Kalman-Yakubovich-Popov lemma (e.g., [FCG79]), and deeper approaches con-
nected with the Schur algorithm [DD84], Darlington synthesis [DVK78], and interpola-
tion problems among the class of Schur functions [Dym89a]. Many of these approaches
admit matrix analogues by way of matrix displacement structure theory [KS95a], and
we examine candidate extensions of displacement structure relations to higher-order
cumulants.
Many algorithmic contributions in recent years aim to manipulate cumulant infor-
mation by way of basic matrix algebra. Since a matrix is a two-indexed structure, while
higher-order cumulants involve more than two indices, a tensorial formulation for cu-
mulants, where tensor here refers simply to a multi-indexed array, would seem a more
natural setting for capturing cumulant-based structures [McC87].
Some recent works have reinforced the utility of tensorial representations of higher-
order statistics. For example, Delathauwer, DeMoor, and Vandewalle [DMV99] have
developed a multilinear singular value decomposition, in which a fcth-order tensor is
reindexed into k different "matrix unwindings," each of whose left singular vectors may
be computed. The overall scheme is then equivalent to applying k unitary transfor-
mations (one for each index dimension) to the tensor to expose a core tensorânot, in
general, diagonalâverifying certain norm and orthogonality properties.
Cardoso and Comon [CC96a] and Comon and Mourrain [CM96] have shown the role
of independent component analysis in many signal processing problems, particularly
source separation. This notion specializes to principal component analysis when applied
to second-order statistics.
Cardoso [Car90], [Car95] has developed supersymmetric tensor diagonalization, mo-
tivated by earlier work involving quadricovariance structures defined from fourth-order
cumulants. Just as a second-order tensor (or matrix) can be understood as an operator
between first-order tensor (or vector) spaces, a fourth-order cumulant structure may be
treated as an operator between matrix spaces, leading to many fruitful extensions of
eigendecompositions familiar in matrix theory.
Our approach aims to exploit displacement structure in a multi-indexed setting,
with the chapter organized as follows. Sec. 9.3 presents a brief overview of displacement
structure in second-order statistical modeling, so that various higher-order extensions
may appear more recognizable. Sec. 9.4 then presents a tutorial overview of a particular
multilinear matrix product (sometimes called the Tucker product), followed by its rela-
tion to cumulant representations in system theory. Sec. 9.6 then introduces displacement
structure for cumulant tensors, along with relations connecting displacement residues
with polyspectra; the relations so studied are valid for all cumulant orders. From these
relations we show in Sec. 9.7 that the existence of a linear model compatible with a

Section 9.3. 
Second-Order Displacement Structure 
249
given set of cumulant or polyspectral values implies the Tucker-factorability of a certain
Pick tensor defined from the data. Sec. 9.8 then presents a candidate extension of a
Schur algorithm to higher-order tensors, based on an apparently novel outer product
involving tensors of successive degrees.
Concluding remarks are made in Sec. 9.9, including some open problems that arise
throughout our presentation.
9.3 
SECOND-ORDER DISPLACEMENT STRUCTURE
We present a brief review of displacement structure in second-order stochastic modeling
to motivate subsequent extensions to higher-order arrays. Further details can be found
in [KS95a] and the references therein as well as in Ch. 1 of this book.
Consider a wide-sense real stationary time series {?/(â¢)} with autocorrelation coeffi-
cients
and the corresponding autocorrelation matrix
of infinite dimensions for now, which assumes a celebrated Toeplitz structure.
Let Z denote the shift matrix with ones on the subdiagonal and zeros elsewhere.
The matrix ZRZT relates to R by shifting all elements one position diagonally; the
Toeplitz structure of R implies that the displacement residue
vanishes except along the borders of the matrix.
Consider now the two-variable (generating function) form
By way of the displacement residue equation (9.3.1), we see that the function R(ZI, z2)
satisfies
where
of infinite dimensions for now, which assumes a celebrated Toeplitz structure.

250 
Tensor Displacement Structures and Polyspectral Matching 
Chapter 9
Setting z2 = z^1 gives the power spectral density function
Since this function along the unit circle \z\\ = 1 is simply the real part of TO 4- 2R+(zi),
the positivity of the power spectrum at (almost) all points on the unit circle reveals
TO + 2R+(zi) as a positive real function, i.e., one that may be continued analytically to
all points in \z\\ < 1 with positive real part.
The spectral factorization problem may be advantageously treated by considering a
dyadic decomposition of the displacement residue from (9.3.1), namely,
in which [â¢] means "repeat the previous vector." The two-variable form (9.3.2) induced
by the displacement residue (9.3.1) may then be rewritten as
in which
According to a celebrated result, whose origins go back to the contributions of Toeplitz,
Caratheodory, and Schur, the Toeplitz matrix R that induces R(z\,z2} is positive (semi-
) definite if and only if there exists a Schur function S(z] (meaning that S(z) is analytic
in \z\ < 1 and strictly bounded by unit magnitude there) that maps a(z] into b(z):
By way of (9.3.4), we see that S(z) must relate to the positive real function TO 4- 2R+(z)
according to
which is simply the Cayley transform. By a well-known property of this transform, S(z)
will indeed be a Schur function if and only if TO 4- 2.R+ (z) is a positive real function.
The virtue of this approach is best appreciated if we consider the case in which only
partial information on the power spectrum is available. To this end, suppose we know
(or have estimated) TO as well as R+(z) at N distinct points z = AI, ..., A^r inside the
unit disk 0 < \z\ < 1. With the convention AQ = 0, this then determines, again by way
of (9.3.4), the value pairs
There then exists a Schur function S(z] fulfilling the system of equations

Section 9.4. Tucker Product and Cumulant Tensors 
251
if and only if a certain Pick matrix P, written elementwise as
is positive (semi-) definite. If positive semidefinite and singular, then S(z) becomes a
rational allpass function of degree equal to the rank of P. If positive definite, infinitely
many solutions exist; they may be parametrized by various recursive constructive pro-
cedures (e.g., [DVK78], [Dym89a], [KS95a], [SKLC94]).
The recursive procedures for constructing S(z] so cited also place in evidence a
complementary Schur function, call it Q(z), fulfilling
When P is positive definite, the solution set for S(z) always includes choices fulfilling
the constraint 1 â S(z) ^ 0 for all \z\ = 1. In this case, S(z) and its complement Q(z]
yield
as a stable and causal function, providing a candidate model for the correlation data
{ro, .R+(Ai)}. This means that if the system H(z] is driven by unit-variance white noise,
its output sequence {Â£(â¢)} fulfills the correlation matching properties
A higher-order extension of this problem will be addressed in Sec. 9.7.
9.4 
TUCKER PRODUCT AND CUMULANT TENSORS
Second-order cumulants reduce to conventional second-order statistics [Men91], i.e.,
cvm[y(nâii),y(nâ1-2}} = E[y(nâi\}y(nâ1-2}} with Â£"[â¢] the expectation operator. This
quantity depends on only two indices i\ and i^ such that calculations involving second-
order cumulants reduce to basic operations on two-indexed arrays (i.e., matrices). Be-
cause higher-order cumulants are multi-indexed quantities, they may be profitably
treated using tools of multi-indexed arrays, or tensors. Cumulants are also multilinear
functions of their arguments [Men91], so it is useful to introduce some basic concepts of
multilinear algebra applied to multi-indexed arrays. A particularly useful tool in this
regard, to be reviewed in this section, is a multilinear matrix product called the Tucker
product, in view of its early application to three-mode factor analysis in [Tuc64], [Tuc66].
Illustrations of its utility in cumulant analysis of system theory are included as well.
For notational convenience, all vectors, matrices, and tensors will be indexed starting
from zero rather than one.
Consider k matrices {Ai}^=l, each of dimensions Mi x L. A feth-order tensor Â£>, of
dimensions M\ x M<2 x â¢ â¢ â¢ x Mfc, may be defined from a Tucker product as

252 
Tensor Displacement Structures and Polyspectral Matching 
Chapter 9
Example 1. Suppose each matrix Ai reduces to a column vector. The summation in the
above definition becomes superfluous, and T> may be written elementwise as
The reader may think of this as a fcth-order "outer product" since, if we consider the case
k = 2, the tensor Pi^ becomes a matrix and the relation Pi^ = (A\)il(A2)i2 implies that
the matrix P is the outer product A\A\.
Example 2. Suppose instead that each matrix Ai is a row vector. The above definition yields
the scalar
The reader may think of this as a fcth-order "inner product" since, for the case A; = 2, we
recognize the sum as the the standard inner product of two real vectors AI and A%.
When all the factors Ai are matrices, the resulting tensor T> may be considered as
an array collecting all possible fcth-order inner products of the rows of each factor or as
the sum of L higher-order outer products.
Example 3. Consider k = 3 matrices A, B, and C, each having L = 2 columns. Partition
these matrices columnwise as
Their third-order Tucker product may be written as
involving L â 2 vector outer products.
One can also consider a weighted version, using a fcth-order tensor T as a kernel.
The T-product of matrices Ai, i = 1,..., &, is the fcth-order tensor
where all dimensions are assumed compatible. One may check that if T is the identity
tensor [7^,...^ = Â£(ii,. ..,Â«&)], the weighted Tucker product reduces to the standard
Tucker product.
Example 4. Suppose we are given three column vectors v, w, x containing random variables,
whose third-order cross cumulants are collected into the tensor T:
Suppose each vector undergoes a linear transformation, using matrices A, B, and C:

Section 9.4. Tucker Product and Cumulant Tensors 
253
Let T> be the new third-order cross-cumulant tensor, i.e.,
Cumulants are multilinear functions of their arguments, and the new tensor V relates to the
old one T by the multilinear transformation
using the weighted Tucker product.
Some further properties are summarized for the reader's convenience:
1. When specialized to a second-order tensor (or matrix) T,
in which T denotes matrix or vector transposition, with the usual matrix product
interpretation on the right-hand side.
2. If x = [EI, x2) â¢ â¢ â¢] is a row vector and / is the identity matrix, then
yields a tensor of order A; â 1.
3. If Cij, ..., â¬ik are A; unit row vectors each having a 1 in the position indexed ifc,
then
4. If vec(-) is the operator which rearranges a tensor into a vector according to
then the equation
is equivalent to
where (g> denotes the conventional Kronecker product of matrices [Bre78], [RM89].
T> 
"D 
T>
5. Composition property. If T = BI * BI * â¢ â¢ â¢ * J3& with X> some fcth-order tensor,
7 - 7 - 7 -
then Â«S = AI Vr ^42 * â¢ â¢ â¢ * Ak implies that

254 
Tensor Displacement Structures and Polyspectral Matching 
Chapter 9
9.5 
EXAMPLES OF CUMULANTS AND TENSORS
We now show some simple examples relating the cumulants of the output process of a
linear system to tensors constructed from the Tucker product. We set
where {u(-}} is an i.i.d. sequence and the transfer function H(z) = ^JihiZl ls strictly
causal. (If H(z] were causal but not strictly causal, the system zH(z) would be strictly
causal and would generate the same output cumulants; the strictly causal constraint on
H(z) leads to simpler relations later on.)
Example 5. 
Consider a strictly causal system initially at rest. The output sequence becomes
where H is the convolution matrix of the system. If (w(-)} is an i.i.d. sequence, its /cth-order
cumulants become
The cumulant tensor built from such an i.i.d. sequence is clearly 7fc J (where T is the identity
tensor). The A:th-order cumulant tensor T with elements indexed from zero,
then becomes
This tensor is, of course, symmetric (i.e., invariant to any permutation of the indices) since
cumulants are symmetric functions of their arguments [MenQl].
Example 6. 
Consider rewriting the input-output relation in the form
We suppose that n is sufficiently large for any initial conditions to have died out, thus yielding
a stationary process for {?/(â¢)}. Taking now the fcth-order output cumulant tensor as
with elements again indexed from zero, we obtain

Section 9.5. Examples of Cumulants and Tensors 
255
Note that this tensor is Toeplitz (or invariant along any diagonal: (72)ti,...,tfc = (T2)i1+i,...,ik+i),
due to the stationarity of the process {?/(â¢)}â¢
We shall give special attention to the Toeplitz tensor of Ex. 6. Consideration of the
structured tensor of Ex. 5, however, leads to the following interesting identity. For any
cumulant of order &, we have [Gri96]
where !# is the infinite Hankel matrix
Hankel matrices take a special significance in system theory [AAK71], [Glo84]. For now
we note that, since H*- â¢ -*H vanishes along all faces (where any index equals zero), the
faces of HT * â¢ â¢ â¢ * HT coincide with those of F# *â¢â¢â¢*!#, which will prove convenient
in what follows.
When specialized to second-order arrays, the identity (9.5.1) reads as
This implies the existence of an orthogonal matrix Q (satisfying QQT = QTQ = I)
fulfilling
One is naturally led to inquire whether there exist square matrices that appear
"orthogonal" with respect to higher-order Tucker products, i.e., square matrices Q for
which Q * Q * â¢ â¢ â¢ * Q â X. If Q is an infinite matrix, it is readily verified that the choice
Q = ZT (the "up-shift" matrix) yields ZT * â¢ â¢ â¢ * ZT = Z, the fcth-order identity tensor,
for any order k > 2. Similarly, choosing Q as any permutation matrix likewise leads
to Q * â¢ â¢ â¢ * Q = X, for any order k > 2. And, for k even, choosing Q as any signed
permutation matrix (i.e., having a sole entry of Â±1 in each row) still works.
The following result shows that, in finite dimensions, the list of "higher-order or-
thogonal" matrices is short.
Theorem 9.5.1 (Higher-Order Orthogonal Matrices). A square matrix Q of fi-
nite dimensions fulfills the kth-order orthogonality
if and only if Q is a permutation matrix (k odd) or a signed permutation matrix (k
even).

256 
Tensor Displacement Structures and Polyspectral Matching 
Chapter 9
Proof: Suppose Q has dimensions L x L, with L arbitrary, and write out the matrix as
If VT = [vi,..., VL] and WT = [wi,..., WL] are two row vectors, their Hadamard (or
componentwise) product will be denoted as
Now, the constraint (9.5.2) can be written in vector inner product form as
for any 1 < m < A:. Upon choosing i\ = i2 = â¢ â¢ â¢ = ifc, we see immediately that none of
the row vectors <j^ can be the zero vector.
Let us show next that the row vectors must be linearly independent. First note from
(9.5.3) that
Suppose to the contrary that the vectors are linearly dependent. We can then find
nonzero constants QI, ..., a/, such that
If only one of the terms ai were nonzero, the corresponding vector q\ would be zero, in
contradiction with all vectors being distinct from the zero vector. Suppose then that
two or more terms from cui, ..., a^ are nonzero. By permuting the indices if necessary,
we may suppose that ai ^ 0 and 0.1 ^ 0 (and possibly others as well). We may then
write q\ as
This yields a contradiction as
Accordingly, the vectors <?i, ..., <?L must be linearly independent.
Now, from (9.5.3) we can write

Section 9.6. 
Displacement Structure for Tensors 
257
for any choice of n between 2 and k â 1. Since the vectors q\, ..., qi, are linearly
independent, they span RL. The only vector in EL orthogonal to EL is, of course, the
zero vector. The previous expression then implies that
which reads componentwise as
This simplifies to
As such, each column of Q can have only one nonzero entry. The same must now apply
to each row of Q, for if a given row were to have two or more nonzero entries, then
another row would be left with no nonzero entries, giving a zero vector. Prom the
constraint
it follows easily that the sole nonzero entry of each row qf must be +1 if k is odd, or
Â±1 if A; is even, giving Q as a (signed) permutation matrix.
9.6 
DISPLACEMENT STRUCTURE FOR TENSORS
In this section we develop various relations that connect the displacement structure of
a cumulant tensor to polyspectral functions. Relations to cumulant interpolation will
follow in Sec. 9.7.
Let T be a given /cth-order tensor and let Z still denote the shift matrix with ones
on the subdiagonal and zeros elsewhere. The fcth-order tensor
relates to T as
Example 7. If we take for T the Toeplitz t nsor of Ex. 6, then its displacement residue
will coincide with T along all faces (when at least one index equals zero) and will vanish at all
interior points (where all indices are nonzero).
If we instead consider a Hankel-based tensor, i.e.,
we can obtain an interesting relation for the up-shifted displacement residue, using the
up-shift matrix ZT.

258 
Tensor Displacement Structures and Polyspectral Matching 
Chapter 9
Lemma 9.6.1 (Displacement Residue). The tensor T from (9.6.1) fulfills
with h = [hi,hz, ^3,.. .}T.
Proof: We recall that a Hankel matrix satisfies (by definition of Hankel) the shift
equation
Since T = F/f * â¢ â¢ â¢ * F//, we see by a direct calculation that
in which the final line comes from the composition property with J = Z * â¢ â¢ â¢ * Z. This
latter tensor reads elementwise as
This allows us to continue as
giving X â J = CQ * â¢ â¢ â¢ ^ ejf, in which CQ is the unit column vector with a 1 in the
leading entry. This then gives, again by the composition property,
as claimed.
9.6.1 
Relation to the Polyspectrum
We return now to the Toeplitz tensor
with the assumption that {?/(â¢)} is a stationary process, although not necessarily a linear
process. Its cumulant lags of order A; involve the relative lags of the first k â l arguments
with respect to the final argument, i.e.,
These are simply the elements of the final face of T, i.e.,
The fcth-order polyspectrum is defined as the bilateral (A; â l)-dimensional Fourier trans-
form of the cumulants,
in which I-J vanishes everywhere except in the leading entry, which equals one, thus

Section 9.6. Displacement Structure for Tensors 
259
whenever the sum converges along the unit polycircle \zi\ = â¢ â¢ â¢ = \Zk-i\ â 1-
Introduce the infinite row vector
containing successive powers of the complex variable 2$. We may then introduce the
fc-variable scalar "generating function," whose coefficients are the elements of the tensor
T, as
Note that this function depends on k complex variables, whereas the polyspectrum from
(9.6.2) involves only k â 1 complex variables. We pursue now how to reconcile these
two functions.
Introducing the displacement residue
0 
C
S(zi,...,Zk) = zi *aâ¢â¢â¢ *z
Because T is a Toeplitz tensor, its displacement residue S vanishes at all interior points.
We shall call S(zi,... ,Zfc) the polyspectral residue function, based on the following
identity.
Lemma 9.6.2 (Polyspectral Residue Function). With
the polyspectrum is obtained by setting Zk = (z\ â¢ â¢ â¢ Zk-\)~l:
Proof: This identity comes from exploiting various symmetry relations linking cumu-
lants of stationary processes. We illustrate the proof for third-order cumulants, as the
verification for higher-order cumulants is quite similar.
Introduce the constant, one-dimensional causal, and two-dimensional causal parts of
S(zi,Z2,z3) as
its multivariable function becomes

260 
Tensor Displacement Structures and Polyspectral Matching 
Chapter 9
Now, S(zi, ^2)z 3) is a symmetric function of the complex variables z\, z2, and 23 and
may be expressed as
Setting z3 = (ziz2}~1 gives
By exploiting stationarity, we see that
so that
which coincides with the 2-transform of the negative diagonal slice c-^-i-
In a similar way, stationarity again gives
so that
and, in the same way,
Figure 9.1 illustrates the (zi,i2)-plane, in which each point represents a sample value
Ci1(i2 and the dashed lines indicate which samples enter into which sum from (9.6.3).
The sum from (9.6.3) is seen to incorporate the doubly two-sided sequence c^,^, i.e.,
yielding the bispectrum as claimed.
Remark. The decomposition into S+(zi) and Sk+Czi,^) can ^e considered, for this third-
order case, a type of analytic continuation into the unit bidisk (\z\\ < 1, \Z2\ < 1) of the
bispectrum, with the bispectrum obtained along the boundary \z\\ = \z^\ = 1 by the symmetry
relation (9.6.3). An open problem here is to determine the set of admissible functions for
S+(zi) and 52+ (zi,22) for which (9.6.3) yields a valid bispectrum, i.e., corresponding to some
stationary process. An analogous open question applies to higher orders as well; for second-
order statistics, admissibility reduces to a positive real constraint.

Section 9.6. 
Displacement Structure for Tensors 
261
Figure 9.1. Illustrating cumulant sample values Cilti2 in the (ii, i2)-plane and which terms enter into
which sum from (9.6.3).
9.6.2The Linear Case
In this section we further examine the structure of the polyspectral residue function
for the special case in which the Toeplitz tensor T is obtained from the cumulants of a
linear process. We recall from Ex. 6 that the Toeplitz tensor T is Tucker factorable as
in which H is the convolution matrix of the linear system and 7fe is the fcth-order
cumulant of the i.i.d. driving sequence to the system. We shall study the (k â 1)-
variable function obtained from T(;ZI, ... ,Zfc) by setting the final complex variable Zk
to zero:
Observe that this function involves unilateral z-transforms in each index and differs
from the polyspectrum of (9.6.2), which involves bilateral z-transforms in each index.
We seek a closed-form expression for T(z\,... , Zfc_i,0) in terms of the system H(z),
when this transfer function is rational.
We exploit the fact that the faces of the Toeplitz tensor T coincide with those of
the Hankel-based tensor 7fe â¢ !# * â¢ â¢ â¢ * I# [cf. (9.5.1)], i.e.,
whose multidimensional z-transform is easier to treat.

262 
Tensor Displacement Structures and Polyspectral Matching 
Chapter 9
Suppose now that H(z) is rational; this means that we have a realization of the form
in which H(z) = zc(I â zA)-1b. We let M denote the state vector dimension. The
Hankel matrix I// can then be decomposed as
in terms of the infinite horizon observability and controllability matrices, O and C,
respectively. The full fcth-order Hankel-based tensor becomes
using the composition property in the second line, in which P is the fcth-order tensor
The following lemma is in direct analogy with the second-order case.
Lemma 9.6.3 (Higher-Order Lyapunov Equation). The kth-order tensor P from
(9.6.4) fulfills the (higher-order) Lyapunov equation
If \i, ..., \M o,re the eigenvalues of A, then this equation admits a unique solution P
provided
With respect to the state equation x(n+l) = Ax(ri) +bu(n), with A stable (\\i\ < 1), P
is the state cumulant tensor
Proof: The verification follows closely that familiar from second-order statistics [AM79]
and is included for completeness. To begin, with P = 7fc â¢ C* â¢ â¢ -*C, the composition
property gives
Here we observe that

Section 9.6. Displacement Structure for Tensors 
263
so that
in which J7" = TâZ*- â¢ -*Z. Since <7 has a 1 in the leading entry and vanishes elsewhere,
we can write
where e^ is the unit column vector with a 1 in its leading entry. This combines with
(9.6.7) to give
yielding the Lyapunov equation (9.6.5).
For existence and uniqueness, the vectorized tensor vec("P) fulfills
With {\i} denoting the eigenvalues of A, those of its fc-term Kronecker product A Â®
â¢ â¢ â¢ <8> A become A^ â¢ â¢ â¢ Ajfc as the indices ii, ..., i^ range over all Mfc possibilities. The
relation (9.6.6) is then equivalent to invertibility of I â AÂ® â¢â¢â¢ Â® A, which in turn is
equivalent to existence and uniqueness of a solution vec("P) and hence of P itself.
For the final part, (asymptotic) stationarity implies that
It suffices to show that these values build a state cumulant tensor which indeed sat-
isfies the given Lyapunov equation. Now, x(ri) depends only on past values u(nâ1),
u(nâ2),... of the input. By the i.i.d. assumption on (w(-)}, cross cumulants involving
x(n) and u(n) vanish, and a simple calculation shows that the state cumulant tensor
satisfies the given Lyapunov equation.
Let T^ denote the face of the Toeplitz tensor T, i.e.,
This face coincides with that of its Hankel-based counterpart, giving
In particular, an expression for each output cumulant contained on the face T^> becomes
Analogous formulas are found in [SM90b], using conventional Kronecker product for-
malisms.
The multidimensional 2-transform of the face of T is then readily computed as

264 
Tensor Displacement Structures and Polyspectral Matching 
Chapter 9
This shows, in particular, that whenever {?/(â¢)} is a rational process, the polyspectral
function T(ZI, ..., Zk-i, 0) will likewise be rational with a separable denominator of the
form A(ZI) â¢ â¢ â¢ .A(zfc_i), where A(z) = det(I â zA) is the denominator of H(z}. This
fact has been used in numerous works [GM89], [JK92], [SM90a], [SM92] to extract the
poles of the model from a one-dimensional cumulant "slice," obtained by varying only
one index. The 2-transform of such a slice appears as
This function may suffer pole-zero cancellations in certain cases [Men91], such that
certain system poles are hidden from the cumulant slice in question and in some cases
no matter which cumulant slice is taken [Men91]. As shown in [RG94], however, this
phenomenon is limited to very special classes of systems H(z).
9.7 
POLYSPECTRAL INTERPOLATION
We consider some explicit solutions to higher-order Lyapunov equations and how such
equations relate to specific evaluations of the polyspectral residue function S(z\,..., Zfc).
We then establish a necessary condition for the existence of a linear process compatible
with these polyspectral evaluations.
Let F be a square (M x M) matrix with all eigenvalues in the open unit disk and g
an M x 1 column vector, and suppose (F,g) is a controllable pair, i.e., the M rows of
the infinite horizon controllability matrix
are linearly independent. We suppose in what follows that the eigenvalues of F are
distinct for ease of presentation, although the various relations to follow extend readily
to the case of repeated eigenvalues. By controllability of the pair (F,g], there exists an
invertible matrix W, which renders the transformed pair (W~1FW, W~lg) in canonic
parallel form [KaiSO], i.e.,
The controllability matrix in this parallel coordinate system is simply
A simple calculation then shows that the solution to the Lyapunov equation

Section 9.7. 
Polyspectral Interpolation 
265
is given by (indexed from one)
Since
P 
P
the solution to the Lyapunov equation P â F * â¢â¢ â¢ * F = g*- --kg relates to V by the
congruence transformation
Note that although the elements of P are real whenever F and g are real, the elements
of T> will in general be complex.
Consider now the state recursion
where (y(-)} is a stationary (possibly nonlinear) process, F is a stable matrix, and
the time index n is sufficiently large for Â£(â¢) to be a stationary vector process. With
Â£(n) = W~l^(n}^ we have an equivalent parallel realization of the form
We examine in the remainder of this section state cumulant tensors from such
recursions and how they relate to evaluations of the polyspectral residue function
S(zi,..., Zfc) obtained from {y(-}} at specific points in the open unit polydisk \z\\ < 1,
..., \Zk\ < 1. This will lead to a necessary condition in Thm. 9.7.1 for the existence of a
linear process which is compatible with (or replicates) a set of polyspectral evaluations.
We begin with the following identity.
Lemma 9.7.1 (An Identity). Let T> be the M x M x â¢ â¢ â¢ x M tensor (indexed from
one)
Then T> can be written as
Proof: To verify the identity (9.7.3), we have that
in which Cp denotes the controllability matrix in the parallel coordinate system. The
cumulant tensor T> from (9.7.2) then becomes

266 
Tensor Displacement Structures and Polyspectral Matching 
Chapter 9
in which T is the Toeplitz cumulant tensor constructed from the stationary process
{y(-}}. Consider now the displaced tensor
which reads elementwise as
Using the relation ACP = CPZ, we see that
in which
is the displacement residue of the Toeplitz tensor T. Now, the multidimensional z-
transform of the elements of S gives S(z\,..., Zk). Considering the special structure of
Cp exposed in (9.7.1), we see that V â Â£ may be written elementwise as
The relation (9.7.4) connecting Â£ to X? then allows us to solve for D as in (9.7.3).
0
Remark. If we let
be the state cumulant tensor in the original (real) coordinate system, then again it relates to
T> by the congruence transformation
with T* now containing all real entries. The reader may wish to check that, for the second-order
case (k = 2), the expression (9.7.3) reduces to the Pick matrix from (9.3.5).
0
Note that the M eigenvalues AI, ..., AM contained in A lead in fact to Mk evalua-
tions of the polyspectral residue function S(zi,... ,Zk)- A natural question is whether
these evaluations might display some form of redundancy, allowing their determination
from a reduced set. The following lemma shows that setting successive complex vari-
ables to zero (which simplifies the corresponding z-transform evaluations) suffices for
retrieving the Mfc evaluations of the previous lemma.
Lemma 9.7.2 (Determining Polyspectral Values). The polyspectral values
are uniquely determined from the set
as each index ii ranges over its M possibilities.

Section 9.7. Polyspectral Interpolation 
267
Remark. This reduces the number of evaluations of S(z\, . . . , Z k ) from Mk to M + M2 +
â¢â¢â¢ + Mk~l.
Proof: Set Â£0(n) = 2/(n) and introduce the augmented vector [^ : ]; its cumulant tensor
s\nJ
(now indexed from zero) is denoted T>. Its interior elements (all indices greater than
zero) yield the elements of T> from (9.7.2), since by stationarity
whenever all indices are greater than zero. The elements on the faces of T> become
which is one of the values from the set (9.7.5) multiplied by the scale factor A^ â¢ â¢ â¢ Aj,.
Observe now that
so that the cumulant tensor formed from 7/J^j) relates to V by the congruence trans-
formation
This tensor, in turn, has interior elements coinciding with those of T> in view of (9.7.6)
and vanishes on all faces because the leading entry of the vector (9.7.8) is zero. We
deduce that T> satisfies the Lyapunov equation
in which dV vanishes at all interior points and has faces coinciding with those of T>.
Since [Â°^] is a stable matrix, the existence claim of Lemma 9.6.3 implies that T) is
uniquely determined from the evaluation points {A^}, which build up A, and the face
values gathered in (9.7.5), which generate dT>.
0
Theorem 9.7.1 (A Pick Condition). Given the polyspectral values 5(Ai1)..., AÂ»fc),
a linear process may be fit to these values only if the Pick tensor from (9.7.3) is Tucker
factorable.
Proof: To see this, suppose that

268 
Tensor Displacement Structures and Polyspectral Matching 
Chapter 9
is a linear process whose polyspectral function 5(zi,..., Zfc) fulfills
We know that the cumulant tensor T> generated from S(\i^,..., \k) can be written as
where T is the cumulant tensor built from the sequence {Â£(â¢)}. By design, T is Tucker
factorable as
because the process {$(â¢)} is linear, and this gives T> = V as
which is Tucker factorable.
0
One may be tempted to conjecture a result in the converse direction; let us pinpoint
a difficulty here. As remarked at the end of Sec. 9.6.1, admissibility conditions for
S(zi,..., Zk) to be a valid polyspectral residue function are not known. Without this,
one is confronted with the difficulty of how to verify whether a candidate solution is
valid.
9.8 
A SCHUR-TYPE ALGORITHM FOR TENSORS
We develop now a candidate procedure for a Schur-type algorithm adapted to higher-
order arrays. For the benefit of the nonexpert, we review first the algorithm for second-
order arrays, which consists of subtracting off vector outer products to expose the
Cholesky factor of a symmetric factorable (or positive-definite) matrix.
We then introduce a tensorial outer product which replicates the faces of a symmetric
tensor in terms of a tensor of one lower dimension. Successive subtraction operations
annihilate successive faces of a tensor, leading to a type of pyramidal decomposition
that reduces to a Cholesky decomposition in the second-order case. The relations with
displacement residues conclude this section.
9.8.1 
Review of the Second-Order Case
Consider a symmetric matrix R, assumed positive definite and hence factorable in the
form R = L * L â LLT. Let us partition R in the form
where TQ is a scalar, r is a column vector, and #1 is a submatrix. Upon setting
one verifies that the outer product aaT coincides with R on the borders of the matrix
and when subtracted from R reveals

Section 9.8. 
A Schur-Type Algorithm for Tensors 
269
in which the lower right block contains the Schur complement RI ârrT/ro with respect
to the leading entry (see also Sec. 1.6.1).
Now, if R is positive definite, we may write R = LLT', in which L is a lower triangular
Cholesky factor of R. The vector a from (9.8.1) is simply the first column of L.
The determination of successive columns of the Cholesky factor L has an intimate
connection with the displacement structure of R and the Schur algorithm (see, e.g.,
Ch. 1 and [KS95a]). Consider the displacement residue of a matrix R, written as the
sum and difference of vector dyads:
The generator vectors {a^} and {bi} are said to be proper (see, e.g., [LK84] and Ch. 1)
provided that a\ is the sole vector that is nonzero in its leading entry. One checks
readily that the border of R must then be aia^f, such that ai yields the first column of
the Cholesky factor of R.
To obtain the remaining columns of the Cholesky factor, let P = R\ ârrT/ro denote
the Schur complement. It turns out, when the generator G is proper, that P satisfies
an analogous displacement equation [KS95a], viz.,
whose generator matrix G' relates to G by a down-shift operation on the leading column.
(The matrix G' now has zeros on the top row.)
Now, if 6 is any ./-unitary matrix, i.e., fulfilling 0JOT = J, then G'Q remains a
generator matrix for the displacement structure of (9.8.2). Whenever the leading entry
of the Schur complement P is positive, then one can always determine a J-unitary
matrix G which renders G'Q in proper form, i.e., for which only the first column vector
of G'Q has a nonzero leading entry (occuring now in the second position); see Ch. 1.
The leading column of the resulting G'Q then generates the next column of the Cholesky
factor L. Successive shift and rotate operations are then applied to yield the successive
columns of L. We refer the reader to Ch. 1 and [KS95a], and the references therein, for
more detail and applications of these recursions.
We turn now to a candidate extension of this procedure for higher-order tensors.
9.8.2 
A Tensor Outer Product
A vector outer product, in the form aia^ = D, generates a matrix D (i.e., a second-order
tensor) obtained from two vectors 01 and a-z (i.e., first-order tensors), by projecting the
indices onto the respective entries: Rilti2 = (ai)i1(Â«2)t2- An analogous operation is to
consider k tensors of order k â 1, to generate a fcth-order tensor by the formulation in
the following definition.

270 
Tensor Displacement Structures and Polyspectral Matching 
Chapter 9
Definition 9.8.1 (Tensor Outer Product). Given k tensors Â£\, ..., 6k, each of
order k â I, their outer product, denoted
is the kth-order tensor with elements
If Â£1 and Â£2 are column vectors, then (Â£1 0 Â£2)^,^ = (Â£i)i2(^2)u = (^^"i)^,^,
which reduces to the conventional outer product of vectors. If Â£1, Â£2, and Â£3 are three
matices, the generation of the third-order tensor B\ Â©Â£2Â© Â£3 is as illustrated in Fig. 9.2.
Each matrix is set adjacent to a face of the cube, and each element of the cube is
obtained by projecting the coordinates onto the three faces and multiplying the matrix
elements occuring on the three faces.
Figure 9.2. Geometric interpretation of the outer product of three matrices: Each matrix occupies
one face of the cube (front, left, and bottom), and each interior element of the tensor is obtained by
projecting its coordinates onto the three faces and multiplying the resulting elements.
The utility of this product can be appreciated by returning to the Toeplitz tensor T
containing output cumulants. Consider first the third-order case, i.e.,
with Tilti2Â£ = cilji2 (the third-order cumulant with lags i\ and 12). The displacement
T ' T
residue T â Z * Z * Z coincides with T along the faces and vanishes at all interior
points. The displacement residue can be expressed as

Section 9.8. A Schur-Type Algorithm for Tensors 
271
in which
and
The first term Â£\ QÂ£\ QÂ£\ generates the faces of T, whereas the second term Â£2 0^2 Â©Â£2
vanishes along each face while replicating the interior terms of S\ 0 E\ 0 Â£1. Observe
that Si also can be written directly in terms of cumulant values c^ ^ as
This decomposition extends readily to high-order tensors as well. If T is a fcth-order
Toeplitz cumulant tensor, its displacement residue can be decomposed as
where now
and [Gri96]
When specialized to the matrix case, the decomposition reduces to the outer prod-
uct decomposition of the displacement structure of a Toeplitz matrix, as illustrated in
(9.3.3). Some potential weaknesses of this higher-order extension, however, are worth
noting. First, the generation of Â£\ involves various division and rooting operations, and
a potential division by zero is not to be discarded immediately, indicating that this de-
composition need not always exist. For the second-order case by contrast [cf. (9.3.3)], all
divisions are by ^/TQ, whose positivity is assured whenever the stochastic process {y(-)}
is nontrivial. Second, even if T has all real entries, those of Â£\ and Â£2 may be complex,
due to the various radicals involved in their generation. Finally, the transform domain
relation is more complicated with this product. To illustrate, consider the third-order
case, for which Â£\ becomes a matrix. Let us set

272 
Tensor Displacement Structures and Polyspectral Matching 
Chapter 9
and V = Â£ 0 Â£ 0 Â£. The function T>(zi,z2,z3) = X^i.fc.is zf1 z2
2 43 may then be
expressed as the convolutional formula [Gri96]
For the second-order case, by contrast, one finds that T> = S 0 Â£ induces the much
simpler multiplicative structure T>(z\,z<i) â Â£(z\}Â£(z2).
Despite these complications, some interesting relations with displacement generators
do fall out, which we now address.
9.8.3 
Displacement Generators
It is natural to consider a "Schur complement" of a tensor as that obtained by sub-
tracting from a given symmetric tensor an outer product of lower-order tensors that
eliminates the face elements. Although the expressions to be developed likewise ap-
ply to nonsymmetric tensors [Gri96], the symmetric case pursued here affords simpler
notations.
For third-order symmetric tensors, this Schur complement operation appears as
or
in which
thereby eliminating the faces of T. For higher-order tensors, the operation appears as
where a = (tl *2'" V V+1 '" *.
fe ) is a circular permutation of indices.
v31 32 â¢â¢â¢ 31 3i+i â¢â¢â¢ 3k' 
r
Suppose now that the symmetric tensor T has as displacement structure the outer
product decomposition
in which Â£fk denotes the fc-term product Â£i 0 â¢ â¢ â¢ 0 Â£i. In analogy with the second-order
case, the generator tensors {Â£1} will be termed proper provided that Â£\ is the only
generator that is nonzero on its faces. The displacement decomposition (9.8.3)-(9.8.5)
for a Toeplitz tensor is proper, for example. The face elements of T then come from the
contribution of Â£\ alone, such that the Schur complement of T, by the above definition,
becomes TM=T-Â£?k.
Extraction of the next Schur complement appears as

Section 9.8. A Schur-Type Algorithm for Tensors
273
Figure 9.3. Illustrating a pyramidal structure for a third-order tensor.
in which (Â£[ ')0fc generates each first nonzero face of T^l\ such that T^ now vanishes
in its first subfaces as well. Upon iterating this process, we generate successive (k â 1)-
dimensional tensors Â£J', vanishing in the first / subfaces and such that the tensor
difference
vanishes whenever all indices are less than n. A new fcth-order tensor, defined as
for successive values of /, then assumes a "pyramidal" structure, which reduces to a tri-
angular (or Cholesky) structure when specialized to second-order tensors (or matrices).
Figure 9.3 illustrates the pyramidal structure for third-order tensors.
We now relate successive subtensors of T> from (9.8.7) to the displacement structure
of (9.8.6). Thus, suppose that the generator tensors in (9.8.6) are proper, i.e., only Â£\
is nonzero along any face. Define a shifted first generator Â£{ by shifting all elements of
Â£1 one position along its main diagonal, i.e.,
We then have the following result.
Lemma 9.8.1 (Structure of Schur Complements). The Schur complement T^ =
T â Â£1 0 â¢ â¢ â¢ 0 Â£\ satisfies the displacement equation
obtained by shifting the first generator of a proper set.
Proof: Note that shifting each entry of Â£\ by one index simply shifts each entry of the
outer product Â£Â®fc by one index, so that

274 
Tensor Displacement Structures and Polyspectral Matching 
Chapter 9
We then find by direct calculation that
as claimed.
The next step in the algorithm is to transform the resulting generators Â£[ and {Â£j}/>2
into a proper set, since the leading generator tensor would then be identified as the next
subtensor S^ ' of the pyramidal factor V from (9.8.7).
At this point a major difference arises compared to the second-order (or matrix)
case. In the second-order case, the transformation amounts to arranging the generator
vectors into a matrix, which is then multiplied by a J-unitary transformation to zero out
preassigned terms. The higher-order analogy of this operation instead involves nonlinear
operations, which we shall illustrate with a simplified example adapted from [Gri96].
Suppose we begin with a third-order Toeplitz tensor for T, and let VT^ = T^ â
T(i) 
7-(i)
Z * Z * Z be the displacement residue from the first Schur complement. By
construction, this tensor vanishes along all faces, so we consider only nonzero indices.
For the Toeplitz case considered, the displacement equation involves only two generators,
We now seek a pair of proper generators F\ and .T^, compatible with VT^\ i.e.,
We require that TI vanish along its first two faces and hence that only T\ be nonzero
along its first face. From the equation
we deduce the first nonzero element of T\ as
The conditions
give the elements of the first nonzero row and column of F\ as
To obtain the remaining elements of T\, let us set
i.e.,

Section 9.9. Concluding Remarks 
275
Using the general relation
we obtain
A similar procedure may be carried out for .F2, whose first nonzero elements begin
with indices 2 or greater, since it must vanish along its first two faces. Starting now
with the pivot (VT^1^)2,2,2? we have that
or
which gives the leading entry. From the relation
we deduce the elements of the first nonzero row and column of TI as
Finally, from the general relation
we obtain the formula for the remaining elements of ^"2 as
We observe that obtaining a proper generator set from a nonproper set does not
reduce to linear transformations. Whether a more expedient procedure to that outlined
above may be obtained, and whether some physically relevant interpretation may be
attached to the intermediate terms of this procedure, are topics which require further
study.
9.9 
CONCLUDING REMARKS
We have presented a semitutorial account of tensorial representations for cumulants in
system theory, with special emphasis on Tucker factorability for linear process. The key
result concerning the Tucker factorability of a Pick tensor for the linear case is apparently
one of the few results establishing compatibility of cumulants with a linear process given
only partial information. Usable conditions guaranteeing Tucker factorability of a given
tensor require further study, as do methods for reliably deducing such a factor when

276 
Tensor Displacement Structures and Polyspectral Matching 
Chapter 9
one exists. Further study is likewise required to establish "admissibility" conditions for
higher-order spectra.
Another avenue worthy of further study relates to the faces of a Toeplitz cumulant
tensor coinciding with its Hankel-based counterpart in the linear case. Given thus the
faces of a Toeplitz tensor, a useful query asks how to reconstruct candidate interior
points compatible with a Hankel-based tensor. If such a procedure could be rendered
successful, then Lemma 9.6.1 shows that a simple displacement operation reveals a rank
1 tensor built from the system impulse response.
A candidate Schur algorithm for pyramidal factorization of higher-order tensors has
also been proposed which, when specialized to second-order arrays, reduces to a recursive
algorithm for triangular factorization. Whether the given procedure may have some
utility in checking for Tucker factor ability is not immediately clear. Similarly, whether
deeper connections with modeling filter synthesis in the linear case [LK84] are inherited
by this procedure remains to be investigated.
Acknowledgments
The authors would like to thank P. Comon and L. Delathauwer for their critical comments on
some earlier versions of this work. They are also grateful to Professors A. H. Sayed and T.
Kailath for their assistance in adapting this work to the present volume.

Chapter 10
MINIMAL COMPLEXITY
REALIZATION OF
STRUCTURED MATRICES
Patrick Dewilde
10.1 
INTRODUCTION
The earlier chapters considered the class of matrices that satisfy displacement equations
(cf. Ch. 1) and, hence, have small displacement ranks. There are also other kinds of
structured matrices. As a general working definition, we propose "matrices whose entries
satisfy class generic constraints that reduce the number of algebraically free parameters."
Class generic constraints on the entries can be of several kinds:
â¢ Linear constraints between entries. Examples are the following:
(i) Toeplitz, Hankel, or even Cauchy matrices.
(ii) Their generalizations to matrices of low displacement rank, as studied exten-
sively by the Kailath school and its many ramifications (see Ch. 1). This is
the class of matrices studied in the earlier chapters.
â¢ Hard value constraints on entries. Examples are the following:
(i) Banded or multibanded matrices.
(ii) Inverses of banded matrices and (possibly continuous) products of banded
matrices with inverses of banded matrices.
â¢ Matrices described by a low-complexity time-varying state-space model.
â¢ Nonlinear algebraic constraints. (Unitary matrices may seem to be of this type,
but they can be brought into the class with linear constraints via the transforma-
tion U = elH in which H is a Hermitian matrix and i = \/âT.)
There are connections among the types described above. A banded matrix or its
inverse has a low-complexity state-space realization; the collection of matrices described
by a low-order state-space model is a generalization of the "banded" case. Products
of these may also have low-complexity state-space realizations. An upper triangular
Toeplitz matrix can be interpreted as a partial transfer operator of a time-varying linear
277

278 
Minimal Complexity Realization of Structured Matrices 
Chapter 10
system and has a state-space model derived from it. Some generic constraints do not
reduce the number of parameters involved. Positivity in one form or the other is one;
the fact that a parameter or an algebraic expression involving parameters is restricted
to an interval does not decrease the "algebraic freedom," at least not in infinite precision
arithmetic. We shall not be concerned with such cases.
10.2 
MOTIVATION OF MINIMAL COMPLEXITY
REPRESENTATIONS
Why are we interested in structured matrices? We are interested for at least two reasons:
1. Structured matrices may be represented by (often many) fewer parameters.
2. Computations involving structured matrices may be much more efficient, some-
times at the cost of numerical accuracy or stability, but in important cases even im-
proving on these two factors (see, e.g., the discussions in Sec. 1.13 and in Chs. 2, 3,
and 4 in this book).
Two important additional reasons may be the following:
3. The reduced complexity is indicative of an underlying physical structure which is
interesting in its own right.
4. The reduced complexity may lead to approximations and "model reduction,"
which reduce the number of necessary parameters even further.
The present chapter treats the combination of two entirely different methods of
using matrix structure for parameter reduction: low displacement rank on the one
hand and representation by low-order time-varying state models on the other. The
first type has been pioneered by Kailath and his coworkers (Ch. 1 of this book gives a
recent survey), while the second type was the subject of [VD94]. In the case of low-
displacement-rank matrices, one computational advantage derives from the fact that in
many cases the matrix or its inverse can be represented by a small sum of by-products
of Toeplitz matrices. If the FFT algorithm is used to execute the product of a Toeplitz
matrix with a vector, then the overall computational complexity of the matrix-vector
multiplication is reduced to ank log n, where a is the "displacement rank" and k is a
small number depending on the type of FFT algorithm chosen. (See [Bla84] for detailed
informationâsee also the discussion in Sees. 1.4 and 8.3.1 of this book.) On the other
hand, matrices with low state representations (e.g., single band matrices) also give rise to
reduced matrix-vector computations either with the original matrix or with its inverse,
the computational complexity now being 26n, where 6 is the maximal state complexity.
We shall see further in the theory that matrices with low state representations may
aptly be called "matrices with low Hankel rank," a term which we shall use in this
chapter. (Another expression would be "with low local degree".)
Matrices with low displacement rank are not the same as matrices with low Hankel
rank. One can easily exhibit examples of matrices which score high for one and low for

Section 10.3. 
Displacement Structure 
279
the other characteristic. For example, the Toeplitz matrix defined by
has low displacement rank but does not have a useful low-order system representation,
since each such representation would either decay exponentially or not decay at all with
increasing index, while the original decays as 1/n, just like the Maclaurin series for
log(l + z). For matrices that score high for one of the criteria, it just pays to use the
corresponding representation method. However, for matrices in the middle category, it
may be advantageous to combine the two techniques. This may be reason enough to
study the combination; there are more reasons, however, which we shall consider in the
concluding section.
10.3 
DISPLACEMENT STRUCTURE
Let R be an n x n positive-definite matrix (the entries of R may actually be N x N
blocks; R is then a positive-definite nN x nN matrix overall), and let us define the lower
triangular shift matrix Z with ones (or unit matrices) on the first subdiagonal and zeros
elsewhere:
We consider the displacement of R with respect to Z (cf. [KKM79a]âsee also Ch. 1),
where â¢* denotes Hermitian transpose, and assume that it has inertia (p, 0,g). This
means that there exist matrices (the Oi are the rows of the matrix G)
of dimensions n x (p + <?) and (p + q) x (p + q), respectively, such that

280 
Minimal Complexity Realization of Structured Matrices 
Chapter 10
in which G has full (column) rank. It is convenient to split each entry a,k according to
the inertia formula
The integer a â p + q is called the displacement rank of R. If it is small compared with
n, then R is said to be of low displacement rank and one can expect great simplifications
of calculus with such matrices.
Another notion leading to low complexity representations of matrices is that of "low
Hankel rank," defined in [DV93]. It leads to reduced state-space models for matrices.
The derivation of such a model from the matrix viewed as an input-output operator is
called a "realization theory."
10.4 
REALIZATION THEORY FOR MATRICES
Let T = [tij] be an upper triangular matrix. Such a matrix often represents a linear
computation, say, of the type y = uT. The matrix T is applied linearly to an input
row vector u to produce an output row vector y. Any computation, including the linear
computation under consideration, will happen in stages. A major, but often hidden,
assumption is that the input vector is presented in sequence; it takes the character of
a signal that flows in the computation and is used sequentially. As soon as sufficient
data are present, a partial calculation can start, produce an intermediate result, store
it, output whatever data it has been able to generate (also in sequence), and then move
to input new data, engage in a new partial calculation, and so on. Viewed in this way,
a linear computation becomes what we would traditionally call a linear system.
Let us analyze such a computing system. We assume that at time k (k = 1,..., n)
the component Uk of the input vector together with some stored information gleaned
from previous samples is used to compute the component yk of the output vector. If
we collect the relevant information from stage A; as a vector x^, which we call the state,
then a data flow scheme of our computation would appear as in Fig. 10.1.
For example, we can write the ordinary vector-matrix multiplication y = uT with
scalar series u and y as
As soon as UQ is available, j/o can be computed and outputed. For the computation of
HI one needs to remember UQ, so the state at t = 1 must contain UQ, but then, as soon
as u\ becomes available, j/i can be computed and outputed; for the computation of 2/2Â»
UQ and ui may be needed, etc. If this continues, then the dimension of the state will
quickly "explode." In many cases (as we shall see soon), a more clever choice of state
leads to a more efficient choice of state vector at each time point.
The linear scheme of computations at each stage is shown in Fig. 10.2.
In the case of the straight vector-matrix multiplication shown in (10.4.1), we have
1. xi = UQ, 2/0 = uotoo', hence
while AQ and CQ are empty since there is no initial state.

Figure 10.1. The realization of a linear computation.
Section 10.4. Realization Theory for Matrices 
281
Figure 10.2. The local linear computing scheme.
2. x2 = [UQ HI] = xi[l 0] + wi[0 1] and y\ = Xi^oi + WI^H; hence
3. x3 = [u0 ui u2] = x2[ o 
* 
o ] + u2[0 0 1] and y2 = x2[ ^ ] -f w2<225 hence
The principle should be clear! In any case, at each stage of the procedure linear com-
putations take place connecting the present state Xk and input Uk with the next state
Â£fc+i and output yk'.

282
Minimal Complexity Realization of Structured Matrices 
Chapter 10
Figure 10.3. Collection of Hankel matrices of T.
However, there is no reason why the state cannot be a linear combination of past inputs,
rather than the inputs themselves. There is also no reason why the calculations should
be restricted to scalar inputs and outputs. In principle, all dimensions may vary; the
input dimensions form a sequence jo? ji, J2, â¢ â¢ â¢ and likewise the output dimensions,
OQ, 0i, 02? ..., as well as the state dimensions, SQ = 0, <5i, 62, â¢â¢â¢â¢ Zero dimensions
means that the entry is empty. (We agree, by definition, that the matrix multiplication
of an empty matrix of dimension m x 0 with one of dimension 0 x n yields a zero matrix
of dimension m x n.) In the case of interest here, however, we work in a framework in
which the input dimensions and output dimensions are the same for all k (j^ = Ok = N),
in which N is a fixed number. (We keep all formulas as general as possible.)
The purpose of realization theory is to find a scheme of minimal state dimensions
which computes y = uT. It turns out that such a scheme exists and that the minimal
dimension of the state at stage k is given by the rank of a certain submatrix of T called
the Hankel matrix H^ [VD94], which we now define. Our utilization of the term "Hankel
matrix" here is not the traditional one (a matrix with second diagonals consisting of
equal entries), but it is in the sense of mathematical system theory: a matrix that maps
past inputs to future outputs. We define the collection of Hankel matrices of T as the
set of matrices Hi represented by the scheme of Fig. 10.3.
More explicitly, each Hi is (j0 + ji-\ 
ji-i) x (oi + oi+i -\ 
h on) and given by
We make the diagonal an exception in the definition. In most cases the diagonal is
special or plays a special role, and it could be included in the calculation by moving all
diagonals one notch up and introducing a new main diagonal consisting exclusively of
zeros.
The important role that Hankel matrices play in realization theory can be deduced
from a reconstruction of the matrix T, which is actually the transfer operator of the
computational scheme of Fig. 10.1, based on the computation model

Section 10.4. 
Realization Theory for Matrices 
283
and hence
We see that the realization induces a factorization of each Hankel operator. A direct
conclusion is that their ranks are at most equal to the dimensions of the respective
local state spaces, rank(-Hfc) = Sk, where <Â§fc is the minimal dimension of xk- 
The
converse appears true as well, and it is also true that any minimal factorization of
each of the Hankel matrices induces a specific realization, so that the corresponding
{Ak, Bk,Ck,Dk} can be recovered from them. A "physical interpretation" of this fact
goes back to the celebrated Nerode state equivalence theory, which is briefly reviewed
next.
10.4.1 Nerode Equivalence and Natural State Spaces
At each time point A;, the Hankel operator maps input signals with support on time
points up to and including k â 1 to the restriction of output signals on the interval
[k, oo). If T is the transfer operator concerned and P[fc,oo) indicates projection on the
Â£2 space based on the interval [A:, oo) (we consider the general case), then Hk is given
by
The image 7i0k of Hk is the set of natural responses that the system is able to generate
at time k, while the image Hk of HÂ£ is the orthogonal complement of the nullspace
at time kâthe space of strict past inputs that generate the zero state. It is a space
of equivalent classes, called Nerode equivalent classes, each of which represents a class
of strictly past input signals that generate the same state, while the output space of
natural responses is actually isomorphic to a natural state space. (See [KFA70] for an
account of the original theory.) Hence, the state dimension is given by the dimension
6k of Hk. To find an {A, C} realization pair, we choose a basis for each H0k and collect
all those base vectors in one observability operator, as shown in Fig. 10.4.
If each block row in Fig. 10.4 is indeed a basis, then it is also left invertible and we
see that the choice determines each Ak and Ck. The corresponding Bk and Dk then
follow straightforwardly from knowledge of the transfer map T. The choice of a basis
either in HQ or dually H, determines the realization.
10.4.2 
Algorithm for Finding a Realization
An algorithm for finding a minimal realization for T then simply proceeds as follows.
Algorithm 10.4.1 (Time Varying System Realization of T). Consider an upper
triangular matrix T = [tij] and define its Hankel matrices as in (10.4.3). A realization
(Afc, Bfc, Ck, Dk) can be found as follows:
1. The Dk are obtained from the diagonal entries ofT.
2. Find a minimal factorization of Hk as Hk = HkOk-
3. Put Bk-i = [nk]k-i (last block entry ofR,k)-

284
Minimal Complexity Realization of Structured Matrices 
Chapter 10
Figure 10.4. Choosing and representing a basis for the space of natural responses at each time point.
4. Put Ck = [Ok]i (first block entry of Ok).
5. Find Ak so that
The matrix Ak is uniquely determined by the condition (10.4.5), since Ok+i is right
invertible by the assumption of minimal factorization. The matrices "R,k and Ok play a
central role in system theory and are called the reachability and observability matrices
of the realization. (For discussion on controllability and observability, see [KFA70],
[KaiSO], [DV93].)
The proof that the algorithm works requires us to show that if the realization is given
by the algorithm, then it reproduces the entries Tkt = BkAk+i â¢ - â¢ At~\Ct exactly for
all 0 < k < t (in which expression some of the Ai factors can disappear when t â k < 2).
This can (fairly simply) be done as a recursion on I â k. Note first that the algorithm
defines the entries of all the observability matrices Ok: the [Ok]i are given directly, while
[Ok]i follows from the definition of A^i and the value just below it, [Ok+i]i-i. Since
all the values of the Bk-\ also are known, we now have all the bottom rows of the Hk
specified and hence all the upper off diagonal entries of T. (As stated above, the diagonal
entries are assumed known, while the lower entries are all zero.) It is now easy to check
that all the reachability matrices are well defined (just put [R]i = Bk-iAk-i+i â¢ â¢ â¢ Ak-i
for i > 1) and that the Hk factor is as expected.
We can also obtain nonminimal realizations through nonminimal factorizations of
the Hk, but then we have to be a little more careful. Examination of the proof in the
previous paragraph shows that one way could be by producing a sequence of well-defined
observability matrices and that this can be done by ensuring that for all k, the rows
of [[<9fc]2, [Ofc]3, â¢ â¢ â¢] lie in the row space of the subsequent Ok+i- Then an Ak can be
found for each k (although now it is not necessarily unique), and all the entries of the
observability matrices follow recursively.

Section 10.4. Realization Theory for Matrices 
285
Dually, a realization could be based on the reachability matrices rather than on the
observability matrices. It should be clear that once the choice for one has been made,
the other follows automatically. 
Clearly, even minimal realizations are not unique.
Each different factorization will produce a different realization: there is a one-to-one
correspondence between the choice of bases for the observability (or dually reachability)
spaces and to be minimal realizations. We say that a realization is in output normal
form if the bases of all the observability spaces have been chosen to be orthonormal.
In that case, the corresponding {Ak, Ck} will be isometric, i.e., they will satisfy (for all
& > 0 )
and vice versa. If (10.4.6) is satisfied, then the corresponding basis for the observability
spaces is orthonormal. (The recursive proof is not difficult; we skip it for the sake of
brevity.) The transformation of one minimal realization to another is accomplished via
a transformation of the state at each point k. If we write, for each point k, x'k = Tkxk,
in which Tk is an invertible matrix, then the state equation in the primed quantities
becomes
and the transformed realization is given by {TkAkTk^vBkTk^TkCk^Dk}. 
In partic-
ular, suppose that we are given a realization and that we wish to bring it to normal
form. Then we should find a collection {Tk} such that the transformed {Ak, C'k} satisfy
(10.4.6). If we define A* = Tk
l(Tk}~ , then this amounts to satisfying the recursive
equation
In the remainder of the chapter we shall treat cases where this equation can indeed
be satisfied uniquely by a (uniformily) invertible collection of {A.k}. It is known that
the existence of such a solution to (10.4.8) requires uniform exponential stability of the
sequence {Ak}. This is known as the Lyapunov condition, and we shall put ourselves
in a situation where this condition is automatically satisfied.
Once a realization for an upper triangular operator T is obtained, it is easy to derive
a realization for its inverse. If T is invertible, then that will also be the case for its main
diagonal. If {Ak,Bk,Ck, Dk} is a realization for T, then each Dk = Tkk will also be
invertible, and from the state equations we obtain
hence {Ak â CkDklBk,Dk1Bk,âCkDkl,Dk1} provides a realization for T~l of the
same state complexity as the original.
Generalizing our framework, suppose that the original operator T is not upper tri-
angular in the traditional sense. Then we have a number of options at our disposal to
make it upper triangular in a generalized sense:
â¢ We can shift the (0,0)th position to the bottom left corner. This strategy requires
the introduction of a more general numbering scheme than that used so far; see
the next section.
â¢ We can additively decompose T as the sum of a lower triangular and an upper
triangular component and realize each of them separately. Needless to say, this
strategy will not be very useful when our actual purpose is to compute the inverse
of T, but it may be very useful in other circumstances.

286 
Minimal Complexity Realization of Structured Matrices 
Chapter 10
â¢ We can also try to do a multiplicative decomposition of T into a lower triangular
matrix that multiplies an upper triangular one (or vice versa); this strategy would
yield good results when we wish to invert T subsequently.
The important point is that the low complexity representation technique remains valid
for the components.
In the next section we shall discuss realizations for the additive and the multiplicative
decompositions of a positive-definite matrix R, which presumably is of low displacement
rank.
10.5 
REALIZATION OF LOW DISPLACEMENT RANK
MATRICES
The goal of the next two sections is the derivation of low-complexity representations for
the additive and the multiplicative decomposition of a positive-definite matrix R. Let
us write R â [r^-j as
in which U and F are upper triangular matrices. Then
If a low state dimension realization exists for F, then one can be found for U as well.
Although this fact can be derived in general (it is a form of a spectral factorization
result), we shall rederive it and specialize the calculation for the case at hand.
Let us write for simplicity R â ZRZ* = X; then it is easy to see that R can be
recovered from X via the formula
The contribution of each term to the Hankel operators for F is easy to evaluate. Indeed,
consider the Hankel operator Hk for F. Then the contributions of the individual terms
to Hk are

Section 10.5. 
Realization of Low Displacement Rank Matrices 
287
Putting these terms together and using the outer product representation of a matrix,
we get
which is of the form (traditional) Toeplitz matrix times (traditional) Hankel matrix.
(The second matrix seems to be like a traditional Toeplitz matrix also, but it is actually
of the classical Hankel type since it maps a past input sequence to a past output se-
quence. We recover the classical Hankel type when we reverse the order of the rows. The
difference between the two types is essential for the discussion here; see the explanation
in Sec. 10.4.1 for added emphasis.) The rank of the Hankel matrix Hk(F) (the term
now refers to its more general meaning) is the essential parameter in the derivation of
the state realization, and we obtain
which is a submatrix of the global (linear time invariant (LTI)) Hankel operator for the
system
which is given by
A (standard) realization for that LTI system can be used as a starting point for the
realization of F. Assuming that the dimension of the state space needed is 5, we find
matrices a, /3,7 of dimensions 6 x <5, (p + q) x 6,6 x N such that, for i > 1,
We choose the realization in output normal form, which means that the matrix [a 7]
is isometric, i.e., ota* +77* = 1. The realization {a, /3,7} may even be an adequate
approximation to the system of aÂ£'s. Use your favorite approximation theory, based
either on Hankel approximation theory in the style of [AAK71] or on balanced realiza-
tions [Kun78j. Notice that in the case of scalar inputs or outputs, the aÂ£'s just form a
single z-dependent column. The relevant Hankel matrix for the series {a*} is now

288 
Minimal Complexity Realization of Structured Matrices 
Chapter 10
in which the last matrix cuts out the first n â k components from an otherwise infinite
series.
The fcth Hankel matrix for F is now
and we find a realization for F with A = a and C = 7 already determined and, from
the last row,
Let us define
then Mfc satisfies the recursive Lyapunov equation
and Bk can easily be computed from Mfc via
Similarly,
and we have found a low rank recursive realization for F.
Algorithm 10.5.1 (Realization of F). Given a symmetric positive-definite matrix
R with displacement structure (10.3.1), a state-space realization (A,Bk,C,Dk) for the
upper triangular additive component F in (10.5.1) can be found as follows:
1. A = a, C = 7 from the LTI system (10.5.2).
2. Bk = 2C*Mka -f 2a0J(3, where MI = (3* J(3 and Mk = a*Mk-ia + j3*Jp when
k>I.
3. Dk = Dk-i + CLkJa*k for k > 1 and DQ = a^Ja^.
This realization is not necessarily (locally) minimal; it can be trimmed at the borders
if needed. (The detailed procedure may be worthwhile but would lead us too far astray
hereâsee [VD94].) A final observation is that the scheme gradually converges to a
time-invariant realization when the operator R, now viewed as semi-infinite for positive
indices, is bounded, since Mfc, .Dfc, Bk converge as k â* oo.

Section 10.6. A Realization for the Cholesky Factor 
289
10.6 
A REALIZATION FOR THE CHOLESKY FACTOR
We had before
To find a realization for the Cholesky factor C7, we keep {A, C} and compute new bk, dk
from the realization {A, Bk,C, D^} of F of the preceding section. First we show how
this can be done in a general way; we deduce the algorithm and we show its correctness.
At this point it is advantageous to introduce block diagonal matrices to represent
time-varying state-space representations. Let
The first transition matrix AQ is usually empty because there is no incoming state in
the calculationâsee Fig. 10.3. Let us assume that the sequence of state dimensions is
{Ai, A2,...} (we take A0 = 0!); then A consists of a sequence of diagonal blocks of
dimensions 0 x AI, AI x A2, A2 x AS, 
It is a block diagonal matrix for which the
first block is empty. (As indicated before, we write matrix-vector multiplication usually
as row vector x matrix: y = uA.}
In addition to block diagonal matrices we introduce the (causal) shift matrix as
Z will shift rows if applied to the left of a matrix and will shift columns when applied
to the right. It is also a block matrix % here the first off diagonal is filled with unit
matrices of possibly varying dimensions. Z actually stands for a collection of block
matrices, because the dimensions of the various blocks may be different. For example, if
Z is applied to the right of matrix A with ' lock column dimensions AI + A2 + AS + â¢ â¢ â¢,
it will also have that sequence of (block) rows, while its sequence of (block) columns
will be AO + AI + A2 + â¢ â¢ â¢â¢ It is understood that AQ = 0 in this case but that an
empty column is there as a placeholder (and actually defines the location of the main
diagonal). The underlying matrix when the block structure is stripped is just a unit
matrix (which would not be the case if T had been doubly infinite to start with). We
find in this way that the computing scheme "realizes" the operator
We show in Fig. 10.5 a semigraphical illustration of how the block matrix mechanics of
(10.6.1) work starting at t = 0. Because of the start-up condition, the state dimension
<5o at t = 0 is zero. The various matrices of importance to the reasoning are also shown

290 
Minimal Complexity Realization of Structured Matrices 
Chapter 10
Figure 10.5. How the operator / â AZ originates.
in the figure. We remark further that
Let us now try to find a realization for U with the same A and C as before but a
new 6 and d. Then we should have
The last term in this expression is quadratic. It can be subjected to a partial fraction
expansion, which in this generalized context works as follows.
Let us define the diagonal shift on any block matrix M (a shift of one block down
the main diagonals in the southeast direction):

Section 10.6. 
A Realization for the Cholesky Factor 
291
Then
if the equation for the block diagonal matrix m,
has a solution. Checking is immediate by pre- and postmultiplication of the expression
with (/ - Z*A*) and (/ - AZ), respectively.
This equation is known as a recursive Lyapunov-Stein equation, and in the present
context it has a (unique) solution which can be computed recursively, provided that
bf.bk is known at each step. Moreover, m is a matrix with square diagonal blocks of
dimensions AO x AO, AI x AI, A2 x A2, ..., in which the A* are the block row
dimensions of A and the first block AO x A0 is empty (row and column dimensions are
zero). Let us write rh = diag(mo+mi+m2 â¢ â¢ â¢) in which mo is empty; then the recursion
(10.6.3) says
When the partial fraction decomposition is now introduced in the equation for U*U
above, and we identify the strictly upper triangular, diagonal, and strictly lower trian-
gular parts, then we see that the block diagonal matrices 6 and d must now satisfy the
set of equations
This set of equations clearly leads to a recursive algorithm if consistent.
Algorithm 10.6.1 (Realization of the Cholesky Factor). Given a symmetric positive-
definite matrix R with displacement structure (10.3.1), a state-space realization (A, 6^, C, dk)
for the Cholesky factor U in (10.5.1) can be found as follows:
1. Keep the (A,C) from Alg. 10.5.1 and consider the (Bk,Dk) from the same algo-
rithm.
2. Step 0. Set d0 = [Â±(D0 + Do")]1/2, b0 = |d^*S0, and nn = 6^0-
3. Step i. Now assume mÂ» is known! Then set di = [\(Di + D*) â C*miC']1/2
(we shall show that the positive square root exists), bi = d~*[^Bi â C*rriiA], and
mi+i = b*bi + A*miA.
One may think that a solution must exist, almost by construction (since the starting
point of the recursion is well knownâmo = 0, the empty set) or by the theory of

292 
Minimal Complexity Realization of Structured Matrices 
Chapter 10
spectral factorization for time-varying systems, but because the realization for F is not
guaranteed minimal, there is reasonable doubt that at the kth step the equation for dk,
cannot be satisfied because the second member is possibly not positive definite. It is
instructive to show that this cannot happen. In doing so we also give an alternative
proof of the spectral factorization theorem for a time-varying R, and we show that it
amounts to a Cholesky factorization. We construct the proof by looking at the Cholesky
factorization of R in a Crout-Doolittle fashionâthe classical method for solving a sys-
tem of linear equations; see [Ste73]. The general Crout-Doolittle method consists of
a recursive construction of a tableau for the lower or upper factorization of a general
matrix T = LU (in which we take U upper with unit diagonal entries and L lower). It
has the form (taking Uij and lij as the entries of U and L, respectively, and assuming
no pivoting is necessary)
It turns out that an entry, say, u^, of this tableau can be computed from the entries
of the original matrix and the entries of the tableau with lower indices k, i with either
k < i and i < j or vice versa. In the next paragraph we give the details for our case.
The Cholesky modification of the Crout-Doolittle tableau for R looks as follows (in
which the u^ are the entries of U):
The right-hand side is not really a useable matrix since its strictly lower part belongs to
the matrix U* and not to U, but it is customary and useful to include it in the tableau.
Moreover, we remark that the entries can be matrices themselves. Filling the Cholesky
tableau recursively then proceeds as follows:
1 /*?
â¢ Step 0. UQO = r^ , u0i = UQQr0i.
â¢ Step i.
The central property in the algorithm is that the pivot is positive definite when R is, so
that its square root can be taken in the case of scalar as well as matrix block entries. A
proof for this fact is found by looking at a partial factorization of R up to the ith step
and is of course classical.
In our case we have, thanks to the realization for F,

Section 10.7. Discussion 
293
We now show that the recursion (10.6.2) in effect generates the (modified) Crout-
Doolittle recursion.
â¢ Step 0. mo = 0, 5[A) + DQ] = ^o^o, 5-^0 â ^o^o are of course solvable and
produce the first row of U as
â¢ Step i. Let us assume that the first i rows (i.e., with indices 0,... ,i â 1) are
computed. We have to show that d*di is well defined (i.e., the expression for it
is positive definite) and also that the rest of the row with index i is correct. The
Crout-Doolittle scheme applies and, thanks to the induction hypothesis, says that
is positive definite. The recursion for m on the other hand gives an expression for
the sum, so that the formula is in fact
which is hence positive definite and can be factored as d*di. A further identifica-
tion with the Crout-Doolittle scheme produces for j > i,
an equation that will be satisfied if (the more demanding) equation
is. We may conclude that the scheme given by (10.6.2) always produces a positive-
definite expression for
when the original R is positive definite.
This concludes the proof of the existence of the realization for U as given by the
algorithm, and we see that it is of the same complexity as the realization for F. In the
following section we shall see that this realization also leads to an attractive computa-
tional scheme for U~i, again of the same complexity.
10.7 
DISCUSSION
The theory presented here is of course a specialization of a theory that can handle more
general types of operators beyond matrices, e.g., general time-varying, time-discrete
systems. For a reasonably complete account see [DV98], The situation considered here is
much simpler for two reasons: (1) matrices have a starting index, and equations become
recursive with a well-defined initial condition; (2) there is an underlying time-invariant
system that can be handled by classical techniques, especially concerning approximation.
The more general case, however, does have some interest even here. Conceivably, the

294
Minimal Complexity Realization of Structured Matrices 
Chapter 10
Figure 10.6. Simple local computation scheme for the inverse of a system.
matrices that we handle can be very large, but even more interesting, they may consist
of subsequent "slices" of different low displacement rank systems. In that case we have
a global time-varying but local low displacement behavior, but the approximation and
complexity reduction theory will work roughly along the same lines as set out above,
now with the use of the general theory.
A legitimate question, already announced in the introductory sections of this chap-
ter, is, What do we gain in complexity reduction of the calculations if we apply the
realization theory detailed in this chapter? A meaningful answer to such a question
requires an understanding of what we mean by "the calculations." We consider two
cases: calculations aiming at the construction of a model for the system or its inverse
and calculations that aim at applying the model to an input. Both the low displace-
ment rank and the low Hankel degree will contribute in both cases and in the expected
manner. The low displacement rank allows for realizations of F and U in which only
the matrices Bk and D^ vary from one point to the next using simple update equations,
depending only on the actual a^, which is itself dependent only on time-invariant data.
If the Hankel rank is 6 and the original system has scalar inputs and outputs, then the
complexity of the realization {a,/3,7} is of the order of (p + q)6. Hence we end up
with a parameter update scheme which can be very efficient depending on the precise
values of the three parameters. The computational efficiency of vector-matrix products
realized by a computational scheme as shown in Fig. 10.1 is directly proportional to the
size of the state 6. As for the inverse (say, of Â£7), we also face two types of computation:
updates and the application of the computing scheme to inputs. Again, the update of
the realization matrices for the inverse is a purely local matter dependent only on the
actual afc or their realization, via the formulas given by (10.4.9), but the computation
can be restricted to the computation of D^1 since the other realization matrices can be
used directly by graph inversion, as shown in Fig. 10.6. (More sophisticated schemes in
which only 6 scalar entries must be inverted exist; see [Vee93].)
Of course, the usage of a model for computation as shown in Fig. 10.1 precludes
the utilization of the FFT as a complexity reducing engine. An FFT scheme, however,
requires a complete shuffle of the data, either at the input side, or in the course of

Section 10.7. 
Discussion 
295
the computations. It is (in some variations) the computational scheme that uses the
smallest number of multiplications and additions possible but at the cost of maximal
shuffling of data. It also does not utilize the fact that relevant impulse responses can
have a lot of structure or can be approximated with very efficient data. In selective
applications, accuracy will suffer. This is the reason why, in many signal processing
applications, filtering algorithms are the preferred mode of implementation, although
they coexist with the FFT. Even intermediate forms are possible, utilized in subband or
multiresolution coding schemes, in which some shuffling of data takes place, combined
with classical filtering. Therefore, a clear-cut statement concerning the advantage of
one or the other is hard to make outside a specific application domain. This holds true
even for the Toeplitz case. Here, Hankel realization theory reduces to the classical LTI
realization theory, and the displacement rank may be just 1, so that vector-matrix mul-
tiplication reduces to a single FFT. The relative computational efficiency then pitches
the system's degree 6 against the logarithm of the time sequence, In n, which might
appear to be to the advantage of the latter. But then, not all items in the complexity
calculation have been included! For example, the "pipeline" (space) complexity of the
FFT is again n against <5, which may be very disadvantageous in concrete cases. And
if selective accuracy is included in the considerations, then the length of the FFT and
the wordlength to be used may be impractical.

This page intentionally left blank 

Appendix A
USEFUL MATRIX RESULTS
Thomas Kailath
Ali H. Sayed

298 
Useful Matrix Results 
Appendix A
We collect in this appendix several matrix facts and formulas, where we assume that
inverses exist as needed.
A.I 
SOME MATRIX IDENTITIES
(i) Block Gaussian Elimination and Schur Complements
Consider a block matrix
that we wish to triangularize by a (block) Gaussian elimination procedure. For
this, note that
so that choosing X = âCA~l gives
where
is called the Schur complement of A in M. Similarly we can find that
Thus we also can obtain
and
where A^> = A â BD~1C is the Schur complement of D in M.
(ii) Determinants
Using the product rule for determinants, the results in (i) give

Section A.I. 
Some Matrix Identities 
299
(iii) Block Triangular Factorizations
The results in (i) can be combined to block diagonalize M:
and
Then by using the easily verified formula
we can obtain the direct factorization formulas
(iv) Recursive Triangularization and LDU Decomposition
An alternative way of writing the above formulas is
which also serves to define the Schur complements A^ and AÂ£>. The above for-
mulas can be used recursively to obtain, respectively, the block lower upper and
block upper lower triangular factorizations of the matrix on the left-hand side.
In particular, by choosing A to be scalar and proceeding recursively we can obtain
the important LDU decomposition of a strongly regular matrix, i.e., one whose
leading minors are all nonzero. To demonstrate this so-called Schur reduction
procedure, let R be an n x n strongly regular matrix whose individual entries
we denote by r^. Let /o and UQ denote the first column and the first row of R,
respectively. In view of (A. 1.3), we see that if we subtract from R the rank 1
matrix Jo^oo^O) then we obtain a new matrix whose first row and column are
zero,
The matrix R\ is the Schur complement of R with respect to its (0,0) entry TOO-
Now, let {rj- , /i, ui} denote the entries, the first column, and the first row of T^i,
respectively, and repeat the above procedure. In general, we can write for the jth
step

300 
Useful Matrix Results 
Appendix A
We conclude that we can express R in terms of the successive {k,Ui,rQQ} as
follows:
where L is lower triangular, D~l is diagonal, and (7 is upper triangular. The
nonzero parts of the columns of L are the {/i}7=To > wn^e the nonzero parts of
the rows of U are the {UI}â¢~Q. Likewise, the entries of D are the {TQQ}}â¢=Q- We
can further normalize the diagonal entries of L and U and define L = LD~l and
U = D~1U. In this case, we obtain R = LDU and the diagonal entries of L and
U are unity.
It is also immediate to verify that the LDU factorization of a strongly regular
matrix is unique. Indeed, assume there exist two decompositions of the form
R = L\D\U\ = L^D^U^-, where {1/1,1/2} are lower triangular with unit diagonal,
{Â£>i, D?} are diagonal, and {Â£/i, U?} are upper triangular with unit diagonal. Then
it must hold that
Now the left-hand matrix in the above equality is lower triangular, while the right-
hand matrix is upper triangular. Hence, equality holds only if both matrices are
diagonal. But since the diagonal entries of L^1Li are unity, it follows that we
must have
from which we conclude that LI = L%, D\ â D%, and U\ = U%.
(v) Inverses of Block Matrices
When the block matrix is invertible, we can use the factorizations in (iii) to write
Alternatively, we can write
By equating the (1,1) and (2,2) elements in the right-hand sides of (A.1.5) and
(A.I.6), we note that

Section A.I. 
Some Matrix Identities 
301
(vi) More Inverse Formulas
Another useful set of formulas can be obtained from the formulas in (v) (and a
little algebra):
and similarly
We also have formulas analogous to (A. 1.3) and (A. 1.4):
(vii) The Matrix Inversion Lemma
For convenience of recall, replacing C by âD and D by C"1, we can rewrite the
above formula for A^1 as
which is often called the modified matrices formula or the matrix inversion lemma,
(viii) Hermitian Matrices
The Hermitian conjugate A* of a matrix A is the complex conjugate of its trans-
pose. Hermitian matrices are (necessarily square) matrices obeying A* = A. Such
matrices have real eigenvalues, say, {Ai}, and a full set of orthonormal eigenvec-
tors, say, {pi}. The so-called spectral decomposition of a Hermitian matrix is the
representation
where
For strongly regular Hermitian matrices, the LDU decomposition takes the form
A = LDL*, L = lower triangular with unit diagonal.
The proof is instructive. If A = LDU, then A* = U*D*L* = U*DL*, since D is
real valued. But by uniqueness of triangular factorization, we must have U â L*.

302 
Useful Matrix Results 
Appendix A
(ix) Inertia Properties
Since the eigenvalues of a Hermitian matrix A = A* are real, we can define the
inertia of A as the triple In{A} = {n+, n_, no}, where n+ is the number of positive
(> 0) eigenvalues of A, n_ is the number of negative (< 0) eigenvalues of A, and
no is the number of zero eigenvalues of A. Note that n+ -f n_ = the rank of A,
while no is often called the nullity of A. The signature of A is the pair {n+,n_}.
We shall define
SA = the signature matrix of A
â a diagonal matrix with n+ ones (+1) and
n_ minus ones (â1) on the diagonal.
It is not necessary to compute the eigenvalues of A to determine its inertia or its
signature matrix. The following property shows that it suffices to compute the
LDL* decomposition of A.
Lemma A.1.1 (Sylvester's Law of Inertia). For any nonsingular matrix B,
In{A} = In{BAB*}.
0
The matrices A and BAB* are said to be congruent to each other, so Sylvester's
law states that congruence preserves inertia. The following useful result follows
easily from the above and the factorizations in (iii).
Lemma A. 1.2 (Inertia of Block Hermitian Matrices). Let
If A is nonsingular, then
where A A â D â CA-1C*, the Schur complement of A in M. If D is nonsingular,
then
where AD = A â C*D~1C, the Schur complement of D in M.
(x) Positive-Definite Matrices
An n x n Hermitian matrix A is positive semidefmite (p.s.d.) or nonnegative
definite (n.n.d.), written A > 0, if it satisfies

Section A.2. The Gram-Schmidt Procedure and the QR Decomposition 
303
It is strictly positive definite (p.d.), written A > 0, if x*Ax > 0 except when
x = 0.
Among the several characterizations of A > 0, we note the nonnegativity of all
its eigenvalues and the fact that all minors are nonnegative. For strict positive
definiteness it is necessary and sufficient that the leading minors be positive. An
often more computationally useful characterization is that n.n.d. matrices can
be factored as LDL* or UDU*, where all entries of the diagonal matrix D are
nonnegative.
Prom the results of (ix), we note that a Hermitian block matrix
is positive definite if and only if either A > 0 and AA > 0 or D > 0 and Ajr> > 0.
A.2 
THE GRAM-SCHMIDT PROCEDURE AND THE QR
DECOMPOSITION
A fundamental step in many algorithms is that of replacing a collection of linearly
independent vectors by a collection of orthonormal vectors that span the same column
space.
Given n independent columns {a^ â¬ C^}, the so-called Gram-Schmidt procedure
finds n orthonormal column vectors {qi â¬ C^} such that, for any 0 < j < n, the column
span of {<7o> â¢ â¢ â¢ > Qj} coincides with the column span of {ao, â¢ . . , a>j}. These vectors are
determined recursively as follows. Start with QQ = ao/\/floao- Now assume for step i
that we have already found the orthonormal vectors {qo,qi,... ,<?i-i}- We project aÂ»
onto the space spanned by these vectors and determine the residual vector rit
We further scale Ti to have unit norm and take the result to be q^.
(The fact that the vectors {a^} are linearly independent guarantees a nonzero r^.) It
follows from this construction that each aj can be expressed as a linear combination of
the resulting {qo, < j f i , . . . , gÂ»}, viz.,
If we now introduce the N x n matrices
we conclude that the above construction leads to the so-called reduced QR factorization
of A,
where R is n x n upper triangular,

304 
Useful Matrix Results 
Appendix A
A full QR factorization of A can be obtained by appending an additional N â n or-
thonormal columns to Q so that it becomes a unitary N x N matrix. Likewise, we
append rows of zeros to R so that it becomes an AT x n matrix:
The orthogonalization procedure so described is not reliable numerically due to the
accumulation of round-off errors in finite precision arithmetic. A so-called modified
Gram-Schmidt procedure has better numerical properties. It operates as follows:
1. We again start with QQ = ao/\A*oao but now project the remaining column vectors
{01,02,... ,on-i} onto go- The corresponding residuals are denoted by c^ ' =
dj â (qQO,j)qo. This step therefore replaces all the original vectors {OQ, .. â¢ ,an_i}
by the new vectors {goÂ» <4 
> <4 
> â¢ â¢ â¢Â»al-i}-
2. We then take q\ = a^ /yOi 
a[ 
and project the remaining column vectors
{a2 ,..., fln_i} onto q\. The corresponding residuals are denoted by aj ' = a^ ' â
(qlct^ }qi. This step replaces {q0, a[^, a2
1},..., a^l} by {q0, qi, a2
2), 42) > â¢ â¢ â¢ > <$-1} â¢
3. We now take q-z = a2 /y ^2 
a2 
ano^ proceed as above.
For both variants of the Gram-Schmidt algorithm, we can verify that the computa-
tional cost involved is O(1Nn2} flops. We should further add that the QR factorization
of a matrix can also be achieved by applying a sequence of numerically reliable rotations
to the matrix (as explained, for example, in App. B).
A.3 
MATRIX NORMS
The 2-induced norm of a matrix A, also known as the spectral norm of the matrix, is
defined by
where ||a;|| denotes the Euclidean norm of the vector x. It can be shown that \\A\\-2 is
also equal to the maximum singular value of A. More specifically, the following two
conclusions can be established. Let crmax denote the largest singular value and let vm\n
denote the smallest singular value. Then
The Frobenius norm of a matrix A = [a^-] is defined by
In terms of the singular values of A, it is easy to verify that if A has rank p with
nonzero singular values {&i,... ,<JP}, then

Section A.4. 
Unitary and J-Unitary Transformations 
305
A.4 
UNITARY AND J-UNITARY TRANSFORMATIONS
The following result plays a key role in the derivation of many array algorithms. One
proof uses the SVDs of the involved matrices.
Lemma A.4.1 (Basis Rotation). Given two nxm (n < m) matrices A and B. Then
AA* = BB* if and only if there exists an m x m unitary matrix G (QQ* = I = O*Gj
such that A = BÂ®.
Proof: One implication is immediate. If there exists a unitary matrix G such that
A = BQ, then AA* = (BQ)(BQ)* = B(QQ*)B* = BB*. One proof for the converse
implication follows by invoking the SVDs of A and B, say,
where UA and UB are nx n unitary matrices, VA and VB are m x m unitary matrices,
and Â£,4 and Â£# are n x n diagonal matrices with nonnegative entries. The squares of
the diagonal entries of H>A (Â£B) are the eigenvalues of AA* (BB*). Moreover, UA (Us]
can be constructed from an orthonormal basis for the right eigenvectors of AA* (BB*).
Hence, it follows from the identity AA* = BB* that we have Y>A = Â£5 and UA = UB-
Let Q = VBVA.Vfe then get 69* = 7 and BQ = A.
We can establish a similar result when the equality AA* = BB* is replaced by
AJA* = BJB* for some signature matrix J. More specifically, we have the following
statement.
Lemma A.4.2 (J-Unitary Transformations). Let A and B be nxm matrices (with
n < m), and let J = (IpÂ®âIq) be a signature matrix withp + q = m. If AJA* = BJB*
is full rank, then there exists a J-unitary matrix G such that A = BQ.
Proof: Since AJA* is Hermitian and invertible,9 we can factor it as AJA* = RSR*,
where R â¬. Cnxn is invertible and S = (IaÂ®âI/3) is a signature matrix (with a+/3 = n).
We normalize A and B by denning A = R~1A and B = R~1B. Then AJA* = BJB* =
S.
Now consider the block triangular factorizations
Using the fact that the central matrices must have the same inertia, we conclude that
In{J â A*SA} = In{J} â In{S} = {p â a,q â /3,n}. Similarly, we can show that
In{J - B*SB} = {p-a,q-/3,n}.
Define the signature matrix Ji = (Ip-aÂ®âIq..p). The above inertia conditions then
mean that we can factor (J - A*SA) and (J â B*SB) as
9This argument was suggested by Professor T. Constantinescu.

306 
Useful Matrix Results 
Appendix A
Finally, introduce the square matrices
It is easy to verify that these matrices satisfy EJ(50 Ji)Ei = J and S^S1Â©^1)^2 = J-
Moreover, EiJSJ = (5Â© Ji) and E2JS^ = (50 J2). These relations allow us to relate
EI and E2 as Si = E2[JE^(5 0 Ji)Ei]. If we set 9 = [JE^S 0 Ji)Si], then it is
immediate to check that Â© is J-unitary and, from the equality of the first block row of
Si = E26, that A = BSl. Hence, A = BQ.
In the above statement, the arrays A and B are either square or fat (n <rri). We
can establish a similar result when n > m instead. For this purpose, first note that if
A is an n x ra and full-rank matrix, with n > m, then its SVD takes the form
where E is n x n and invertible. The left inverse of A is defined by
and it satisfies A^A = 7m, the identify matrix of size m.
Lemma A.4.3 (J-Unitary Transformations). Let A and B be n x m full-rank ma-
trices (with n > m) and let J = (Ip 0 â 7g) be a signature matrix with p -f q â m.
The relation AJA* = BJB* holds if and only if there exists a unique m x m J-unitary
matrix Q such that A = BQ.
Proof: The "if statement is immediate. For the converse, note that since A and B
are assumed full rank, there exist left inverses A^ and B^ such that A^A = Im and
B^B = Im. Now define Â© = B^A. We claim that Â© is J-unitary and maps B to A, as
desired.
The proof that Â©JÂ©* = J is immediate from the equality AJA* = BJB*. Just
multiply it from the left by B^ and from the right by (B^)* and use B^B = Im.
To prove that BQ = A, for the above choice of 0, we start with AJA* = BJB*
again and insert the term B^B into the right-hand side to get
Multiplying from the right by (A^}* and using A^A = Im we obtain BB^(AJ) = AJ.
Since J is invertible and its inverse is J, we conclude by multiplying by J from the right
that B(B^A) = A, which is the desired result. That is, 0 is J-unitary and maps B to
A.
To show that 0 is unique, assume 0 is another J-unitary matrix that maps B to A
and write BQ â BQ. Now multiply by B^ from the left to conclude that 0 = 0.
A.5 
Two Additional Results
Finally, we state two matrix results that are needed in Chs. 5 and 8. The first theorem
is cited in Ch. 5. Its statement was provided by the authors of that chapter.

Section A.5. Two Additional Results 
307
Theorem A.5.1 (Cauchy Interlace Theorem) (see [ParSO]). Let A, W, and Y be
symmetric matrices and A = W + Y. Let the eigenvalues of the matrices be ordered as
or
For any i,j satisfying I<i + j â 1 < n, the following inequalities hold:
and
The next theorem is cited in Ch. 8. Its statement was provided by the authors of
that chapter.
Theorem A.5.2 (PerronâFrobenius). Let A = (a^j) be an n x n matrix with non-
negative entries and denote p its spectral radius, that is, the maximum modulus of its
eigenvalues. Then
1. there exists an eigenvalue X of A such that p = X;
2. there exists an eigenvector v of A with nonnegative components corresponding to
A;
3. if the matrix is not reducible then A > 0 and v has positive components; moreover,
X and v are unique. (A matrix A is said to be reducible if there exists a permutation
of rows and columns that transforms A by similarity in the following way:
where the blocks -Ai,i, ^2,2 are square matrices and P is a permutation matrix.)

This page intentionally left blank 

Appendix B
ELEMENTARY
TRANSFORMATIONS
Thomas Kailath
Ali H. Sayed

310 
Elementary Transformations 
Appendix B
In this appendix we review three families of elementary (unitary and hyperbolic)
transformations that can be used to annihilate selected entries in a vector and thereby
reduce a matrix to triangular form. These are the Householder, Givens, and fast Givens
transformations. Special care needs to be taken when dealing with complex-valued data
as compared to real-valued data, as we show in the sequel.
B.I 
ELEMENTARY HOUSEHOLDER TRANSFORMATIONS
Suppose we wish to simultaneously annihilate several entries in a row vector, for ex-
ample, to transform an n-dimensional vector x = [ x\ x^ ... xn-\ ] into the form
[a 0 0 0 ], where, for general complex data, the resulting a may be complex as
well.
One way to achieve this transformation is to employ a so-called Householder reflec-
tion 0: it takes a row vector x and aligns it along the direction of the basis vector
eo = [ 1 0 ... 0 ]. More precisely, it performs the transformation
for some en to be determined. Since, as we shall promptly verify, the transformation
0 that we shall employ is unitary and also Hermitian (i.e., 00* = / = 0*0 and
0 = Â©*), we can be more specific about the resulting a. In particular, it follows from
(B.I.I) that the magnitude of a must be equal to \\x\\, i.e., |a| = \\x\\. This is because
a;00*a;* = ||a:||2 = |a|2. Moreover, it also follows from (B.I.I) that xQx* = ax\. But
since 0 will be Hermitian, we conclude that xQx* is a real number and, hence, ax\
must be real as well.
This means that by rotating a vector x with a unitary and Hermitian transformation
Â© we can achieve a postarray of the form [a 0 ... 0 ], where a in general will be
a complex number whose magnitude is the norm \\x\\ and whose phase is such that arc*
is real. For example, a = Â±\\x\\e^x^ are the possible values for a, where <j>Xl denotes
the phase of x\. (For real data, Â±\\x\\ are the possible values for a.)
Now, assume we define
and a is a complex number that is chosen as above, a â Â±||o;||e^zi. It can be verified
by direct calculation that, for any g, 0 is a unitary matrix, i.e., 00* = / = 0*0. It is
also Hermitian.
Lemma B.I.I (Complex Householder Transformation). Given a row vector x
with leading entry x\, define 0 and g as in (B.1.2), where a is any complex number that
satisfies the following two requirements: \a\ = \\x\\ and ax* is real. Then it holds that
xQ = âaeQ. That is, x is rotated and aligned with e$; the leading entry of the postarray
is equal to âa.
(Algebraic) proof: We shall provide a geometric proof below. Here we verify our
claim algebraically. Indeed, direct calculation shows that

Section B.I. 
Elementary Householder Transformations 
311
aeo 
â¬Q
Figure B.I. Geometric interpretation of the Householder transformation.
Likewise,
and we obtain
In other words, by defining
we obtain
The choice of the sign in (B.1.4) depends on the choice of the sign in the expression for
g. Usually, the sign in the expression for g is chosen so as to avoid a vector g of small
Euclidean norm, since this norm appears in the denominator of the expression defining
G. (In the real case, this can be guaranteed by choosing the sign in the expression for g
to be the same as the sign of the leading entry of the row vector x, viz., the sign of xi.)
A Geometric Derivation
The result of the above lemma has a simple geometric interpretation: given a vector
x, we would like to rotate it and align it with the vector CQ. (The careful reader will
soon realize that the argument applies equally well to alignments along other vector
directions.) This rotation should keep the norm of x unchanged. Hence, the tip of
the vector x should be rotated along a circular trajectory until it becomes aligned with
CQ. The vector aligned with eo is equal to aeo. Here, as indicated in Fig. B.I, we are
assuming that the rotation is performed in the clockwise direction. The triangle with
sides x and aeo and base g = x â aeo is then an isosceles triangle.
We thus have aeo = x â g. But we can also express this in an alternative form. If
we drop a perpendicular (denoted by g^} from the origin of x to the vector g, it will
divide g into two equal parts. Moreover, the upper part is nothing but the projection

312 
Elementary Transformations 
Appendix B
of the vector x onto the vector g and is thus equal to (x, g) \\g\\~2 g- Therefore,
A similar argument holds for the choice g = aeo + x. The Householder transforma-
tion reflects the vector x across the line g^ to the vector aeo, so it is often called a
Householder reflection.
To represent the transformation in matrix form we have two choices, depending upon
whether we represent vectors as row (1 x n) matrices or column (n x 1) matrices. In
the first case, we have (x,g) = xg* and ||<7||2 = gg*, so that
as in (B.I.4) earlier.
If we represent vectors by columns, then (x, g) = g*x and ||p||2 = g*g, and we shall
have
Triangularizing a Matrix
A sequence of Householder transformations of this type can be used to triangularize a
given m x n matrix, say, A. For this we first find a transformation GO to rotate the first
row to lie along CQ , so that we have A&Q of the form (where a denotes entries whose
exact values are not of current interest):
Now apply a transformation of the type (1 Â© QI), where GI rotates the first row of
AI so that it lies along CQ in an (n â l)-dimensional space, and so on. This so-called
Householder reduction of matrices has been found to be an efficient and stable tool
for displaying rank information via matrix triangularization and it is widely used in
numerical analysis (see, e.g., [GV96], [Hig96]).
B.2 
ELEMENTARY CIRCULAR OR GIVENS ROTATIONS
An elementary 2x2 unitary rotation G (also known as Givens or circular rotation) takes
a 1 x 2 row vector x = [ a b ] and rotates it to lie along the basis vector eo = [ 1 0 ] .
More precisely, it performs the transformation
where, for general complex data, a may be complex as well. Furthermore, its magnitude
needs to be consistent with the fact that the prearray, [ a 6 ], and the postarray,
[a 0 ], must have equal Euclidean norms since

Section B.2. 
Elementary Circular or Givens Rotations 
313
In other words, a must satisfy |a|2 = |a|2 + |6|2, and its magnitude should therefore
be |a| = VH2 + l&l
2-
An expression for G that achieves the transformation (B.2.1) is given by
Indeed, if we write a in polar form, say, a = |a|ej<^a, then it can be verified by direct
calculation that we can write
That is, the above 0 leads to a postarray with a complex value a that has the same mag-
nitude as the Euclidean norm of the prearray but with a phase factor that is determined
by the phase of a up to a sign change. (Note in particular that aa* is real.)
For real data {a, 6}, and hence p = p*, the same argument will show that we get a
postarray of the form
In any case, the main issue is that once a and 6 are given, real or complex, a unitary
rotation 0 can be defined that reduces the prearray [ a b ] to the form [a 0 ] for
some a.
In the trivial case a â 0, we simply choose 0 to be the permutation matrix
We finally note that, in the special case of real data, a general unitary rotation as
in (B.2.2) can be expressed in the alternative form
where the so-called cosine and sine parameters, c and s, respectively, are defined by
This justifies the name circular rotation for 0, since the effect of 0 is to rotate a vector
x along a circle of radius ||x||, by an angle 6 that is determined by the inverse of the
above cosine or sine parameters, 9 = tan"1 p, in order to align it with the basis vector
[ 1 0 ]. The trivial case a = 0 corresponds to a 90 degrees rotation in an appropriate
clockwise (if 6 > 0) or anticlockwise (if 6 < 0) direction.
Triangularizing a Matrix
Matrix triangularization can also be effected by a product of Givens transformations,
each of which introduces a zero in a particular location. For example, suppose that
x = [ . . . , X i , . . . , X j , . . . ] , and we wish to null out x, using ar*. All entries axe possibly

314 
Elementary Transformations 
Appendix B
complex. Then let p = Xj/Xi, and define
Except for the p terms, all off-diagonal entries are 0. Then we can see that
where the entries indicated by ... are arbitrary and unchanged by 6, while the resulting
o: will be of the general form
To systematically triangularize a p x p matrix A, apply a sequence of p â 1 such trans-
formations to zero all entries in the first row of A expect for the first element. Proceed
to the second row of the thus transformed A matrix and apply a sequence of p â 2
transformations to zero all entries after the second one. The first row has a zero in
every column affected by this sequence of transformations, so it will be undisturbed.
Continuing in this fashion for all rows except the last one, we will transform A to lower
triangular form.
In general, the Givens method of triangularization requires more computations than
the Householder method. It requires about 30% more multiplications, and it requires one
scalar square root per zero produced as opposed to one per column for the Householder
method. However, the Givens method is more flexible in preserving zeros already present
in the A matrix and can require fewer computations than the Householder method when
A is nearly triangular to begin with (see [Gen73]). Moreover, there is a fast version,
presented next, that uses 50% fewer multiplications.
B.3 
HYPERBOLIC TRANSFORMATIONS
In many cases, it is necessary to use hyperbolic transformations rather than unitary
transformations. We therefore exhibit here the necessary modifications.
Elementary Hyperbolic Rotations
An elementary 2 x 2 hyperbolic rotation 6 takes a row vector x = [ a b ] and rotates
it to lie either along the basis vector eo = [ 1 0 ] (if |a| > |6|) or along the basis vector
ei = [ 0 1 ] (if |a| < |6|). More precisely, it performs either of the transformations

Section B.3. 
Hyperbolic Transformations 
315
where, for general complex data, o; may be complex as well. Furthermore, its magnitude
needs to be consistent with the fact that the prearray, [ a b ], and the postarray,
[ a 0 ], must have equal Euclidean J-norms; e.g., when |a| > |6| we get
where J = (1 Â© â 1). By the J-norm of a row vector x we mean the indefinite quantity
xJx*, which can be positive, negative, or even zero. Hence, for |a| > |6|, a must satisfy
|o;|2 = |a|2 â |6|2, and its magnitude should therefore be |a| = \/|a|2 â |6|2. When
|6| > |a| we should get |a| = \/|&|2 â |a|2.
An expression for a J-unitary hyperbolic rotation G that achieves (B.3.1) or (B.3.2)
is given by
It can be verified by direct calculation that these transformations lead to postarrays of
the form
For real data, a general hyperbolic rotation as in (B.3.3) or (B.3.4) can be expressed
in the alternative form
where the so-called hyperbolic cosine and sine parameters, ch and s/i, respectively, are
defined by
This justifies the name hyperbolic rotation for Â©, since the effect of O is to rotate a
vector x along the hyperbola of the equation
by an angle 0 that is determined by the inverse of the above hyperbolic cosine or sine
parameters, 9 = tanh"1 p, in order to align it with the appropriate basis vector. Note
also that the special case |a| = |6| corresponds to a row vector x = [ a b ] with zero
hyperbolic norm since a\2 â |6|2 = 0. It is then easy to see that there does not exist a
hyperbolic rotation that will rotate x to lie along the direction of one basis vector or
the other.
There exist alternative implementations of the hyperbolic rotation G that exhibit
better numerical properties. Here we briefly mention two modifications.

316 
Elementary Transformations Appendix B
Mixed Downdating
Assume we apply a hyperbolic rotation 6 to a row vector [ r e y ], say,
Then, more explicitly,
Solving for x in terms of xi from the first equation and substituting into the second
equation we obtain
An implementation that is based on (B.3.6) and (B.3.8) is said to be in mixed down-
dating form. It has better numerical stability properties than a direct implementation
of 0 as in (B.3.5)âsee [BBDH87]. In the above mixed form, we first evaluate x\ and
then use it to compute y\. We can obtain a similar procedure that first evaluates t/i
and then uses it to compute x\. For this purpose, we solve for y in terms of y\ from
(B.3.7) and substitute into (B.3.6) to obtain
Equations (B.3.7) and (B.3.9) represent the second mixed form.
The OD Method
The OD procedure is based on using the SVD of the hyperbolic rotation 0. Assume p
is real and write p = b/a, where |a| > |6|. Then it is straightforward to verify that any
hyperbolic rotation of this form admits the following eigendecomposition:
where the matrix
is orthogonal (QQT = /) and T denotes transposition.
Due to the special form of the factors (Q, D), a real hyperbolic rotation with \p\ < 1
can then be applied to a row vector [ x y ] to yield [ x\ y\ ] as follows (note that
the first and last steps involve simple additions and subtractions):

Section B.3. 
Hyperbolic Transformations 
317
This procedure is numerically stable, as shown in [CS96] (and also Ch. 2). An alternative
so-called H-procedure is described in the same reference. It is costlier than the OD
method, but it is more accurate and can be shown to be "forward" stable, which is a
desirable property for finite precision implementations.
Hyperbolic Householder Transformations
One can also use hyperbolic or J-unitary Householder reflections to simultaneously
annihilate several entries in a row, e.g., to transform [ x x x x ] directly into the
form [ x' 
0 0 0 ].
Let J be an n x n signature matrix such as J = (Ip Â© â Iq] with p + q = n. We
are now interested in a J-unitary Householder transformation 9 that takes a 1 x n row
vector x and aligns it either along the basis vector CQ = [ 1 0 ] (if x Jx* > 0) or along
the basis vector en_i = [ 0 1 ] (if xJx* < 0). (One can also require 0 to align x
along the direction of some other basis vector depending on the sign of x Jx* and on
the order of the sequence of Â±l's in J. We shall, without loss of generality, focus here
on the special directions CQ and en_i and assume that J is of the form J = (Ip Â© â /g).)
Hence, we require 0 to perform either of the transformations
where, for general complex data, the resulting a may be complex as well.
When xJx* > 0, we define
and a is a complex number that satisfies |a|2 = xJx* and ax* is real. It can be verified
by direct calculation that 0 is J-unitary, i.e., 0J0* = J = 0* J0. When xJx* < 0, we
use the same expression for 0 but with
where a is a complex number that satisfies |o;|2 = âxJx* and cxx^_l is real.
Lemma B.3.1 (Complex Hyperbolic Householder Transformation). 
Given a
row vector x with leading entry x\ and xJx* > 0, define Â© and g as in (B.3.13)
where a is any complex number that satisfies the following two requirements (see be-
low): \a\ = VxJx* and ax^ is real. Then it holds that x0 = âaeo. That is, x is
rotated and aligned with CQ; the leading entry of the postarray is equal to âa.
For a vector x that satisfies instead xJx* < 0, and with trailing entry xn_i, we
choose g as in (B.3.14), where a is any complex number that satisfies \a\ = -y/|xJx*|
and cxx^_l is real. Then it holds that x0 = â aen-i-

318 
Elementary Transformations 
Appendix B
(Algebraic) proof: We prove the first statement only since the second one follows
from a similar argument. Direct calculation shows that
Therefore,
Specific choices for a, and hence g, are
and they lead to
q-
Geometric Derivation
The geometric derivation presented earlier for Householder transformations still applies
provided we use "J-inner products," i.e., provided we interpret
Then using rows, we can write, for example, when ||x|| j = \fxJx* 
> 0,
where g = x Â± a CQ. Table B.I collects the expressions for the several rotations that
we have considered in the earlier discussion.

Section B.3. 
Hyperbolic Transformations
319
Table B.I. Unitary and hyperbolic rotations.
Rotation
Circular
or Givens
Permutation
Hyperbolic 1
Hyperbolic II
Unitary
Householder
Hyperbolic
Householder 1
Hyperbolic
Householder II
Expression
Effect

This page intentionally left blank 

BIBLIOGRAPHY
[AA86] A. C. ANTOULAS AND B. D. O. ANDERSON, On the scalar rational interpolation prob-
lem, IMA J. Math. Control Inform., 3, pp. 61-88, 1986.
[AAK71] V. M. ADAMJAN, D. Z. AROV, AND M. G. KREIN, Analytic properties of Schmidt
pairs for a Hankel operator and the generalized Schur-Takagi problem, Math. USSR Sbornik,
15, pp. 31-73, 1971 (transl. of Iz. Akad. Nauk Armjan. SSR Ser. Mat., 6 (1971)).
[AB84] O. AXELSSON AND V. BARKER, Finite Element Solution of Boundary Value Problems,
Theory and Computation, Academic Press, Orlando, FL, 1984.
[Ack91] R. ACKNER, Fast Algorithms for Indefinite Matrices and Meromorphic Functions, Ph.D.
dissertation, Stanford University, Stanford, CA, 1991.
[AD86] D. ALPAY AND H. DYM, On applications of reproducing kernel spaces to the Schur
algorithm and rational J-unitary factorization, Oper. Theory: Adv. Appl., 18, pp. 89-159,
Birkhauser, Boston, 1986.
[AD92] D. ALPAY AND H. DYM, On a new class of reproducing kernel spaces and a new gener-
alization of lohvidov laws, Linear Algebra Appl., 178, pp. 109-183, 1992.
[ADD89] D. ALPAY, P. DEWILDE, AND H. DYM, On the existence and construction of solutions
to the partial lossless inverse scattering problem, with applications to estimation theory, IEEE
Trans. Inform. Theory, 35, pp. 1184-1205, 1989.
[AE87] J. ABBISS AND P. EARWICKER, Compact operator equations, regularization and super-
resolution, in Mathematics in Signal Processing, T. Durrani, J. Abbiss, T. Durrani, J. Hudson,
R. Madan, J. McWriter, and T. Moore, eds., Clarendon Press, Oxford, UK, 1987.
[AG88] G. S. AMMAR AND W. B. GRAGG, Superfast solution of real positive definite 
Toeplitz
systems, SIAM J. Matrix Anal. Appl., 9, pp. 61-76, 1988.
[AG89] G. S. AMMAR AND P. GADER, New decomposition of the inverse of a Toeplitz matrix,
Proc. Int. Symp. MTNS, vol. Ill, pp. 421-428, Birkhauser, Boston, 1989.
[Akl89] S. AKL, The Design and Analysis of Parallel Algorithms, Prentice-Hall, Englewood
Cliffs, NJ, 1989.
[AL86] O. AXELSSON AND G. LINDSKOG, The rate of convergence of the conjugate gradient
method, Numer. Math., 48, pp. 499-523, 1986.
[ALM97] G. ANASTASI, L. LENZINI, AND B. MEINI, Performance evaluation of a worst case
model of the MetaRing MAC protocol with global fairness, Performance Evaluation, 29, pp. 127-
151, 1997.
[AM79] B. D. O. ANDERSON AND J. B. MOORE, Optimal Filtering, Prentice-Hall, Englewood
Cliffs, NJ, 1979.
[APP88] S. T. ALEXANDER, C.-T. PAN, AND R. J. PLEMMONS, Analysis of a recursive least-
squares hyperbolic rotation algorithm for signal processing, Linear Algebra Appl., 98, pp. 3-40,
1988.
321

322 
Bibliography
[AS99] N. AL-DHAHIR AND A. H. SAVED, A computationally efficient FIR MMSE-DFE for
multi-user communications, in Proc. Asilomar Conference on Signals, Systems, and Computers,
Pacific Grove, CA, 1999, to appear.
[Atk78] K. E. ATKINSON, An Introduction to Numerical Analysis, John Wiley, New York, 1978.
[Avr88] F. AVRAM, On bilinear forms on Gaussian random variables and Toeplitz matrices,
Probab. Theory Related Fields, 79, pp. 37-45, 1988.
[Bar69] E. H. BAREISS, Numerical solution of linear equations with Toeplitz and vector Toeplitz
matrices, Numer. Math., 13, pp. 404-424, 1969.
[BBDH87] A. W. BOJANCZYK, R. P. BRENT, P. VAN DOOREN, AND F. R. DE HOOG, A note
on downdating the Cholesky factorization, SIAM J. Sci. Statist. Comput., 8, pp. 210-220, 1987.
[BBH86] A. W. BOJANCZYK, R. P. BRENT, AND F. R. DE HOOG, QR factorization of Toeplitz
matrices, Numer. Math., 49, pp. 81-94, 1986.
[BBH95] A. W. BOJANCZYK, R. P. BRENT, AND F. R. DE HOOG, Stability analysis of a general
Toeplitz systems solver, Numer. Algorithms, 10, pp. 225-244, 1995.
[BBHS95] A. W. BOJANCZYK, R. P. BRENT, F. R. DE HOOG, AND D. R. SWEET, On the
stability of the Bareiss and related Toeplitz factorization algorithms, SIAM J. Matrix Anal.
Appl., 16, pp. 40-57, 1995.
[BC83] D. BINI AND M. CAPOVANI, Spectral and computational properties of band symmetric
Toeplitz matrices, Linear Algebra Appl., 52, pp. 99â126, 1983.
[BCK88] A. M. Bruckstein, T. K. Citron, and T. Kailath, On inverse scattering and partial
realizations, Internat. J. Control, 48, pp. 1537-1550, 1988.
[BD90] D. BINI AND F. Di BENEDETTO, A new preconditioner for the parallel solution of positive
definite Toeplitz systems, in Proc. Second ACM Symp. on Parallel Algorithms and Architec-
tures, Crete, Greece, pp. 220-223, 1990.
[BD91] P. BROCKWELL AND R. DAVIS, Time Series: Theory and Methods, 2nd ed., Springer-
Verlag, New York, 1991.
[BF93] D. BINI AND P. FAVATI, On a matrix algebra related to the discrete Hartley transform,
SIAM J. Matrix Anal. Appl., 14, pp. 500-507, 1993.
[BG95] D. BINI AND L. GEMIGNANI, Fast parallel computation of the polynomial remainder
sequence via Bezout and Hankel matrices, SIAM J. Comput., 24, pp. 63-77, 1995.
[BG97] A. BOTTCHER AND S. M. GRUDSKY, Estimates for the condition numbers of large
Toeplitz matrices, preprint.
[BGN70] B. L. BUZBEE, G. H. GOLUB, AND C. W. NIELSON, On direct methods for solving
Poisson's equation, SIAM J. Numer. Anal., 7, pp. 627-656, 1970.
[BGR90] J. A. BALL, I. GOHBERG, AND L. RODMAN, Interpolation of Rational Matrix Func-
tions, Oper. Theory Adv. Appl. 45, Birkhauser, Boston, 1990.
[BGY80] R. P. BRENT, F. G. GUSTAVSON, AND D. Y. Y. YUN, Fast solution of Toeplitz systems
of equations and computation of Fade approximants, J. Algorithms, 1, pp. 259-295, 1980.
[Bin83] D. BINI, On a Class of Matrices Related to Toeplitz Matrices, Tech. Rep. 83-5, State
University of New York, Albany, NY, 1983.
[Bjo87] A. BJORCK, Stability analysis of the method of semi-normal equations for linear least
squares problems, Linear Algebra Appl., 88/89, pp. 31-48, 1987.
[Bjo91] A. BJORCK, Error analysis of least squares algorithms, in Numerical Linear Algebra,
Digital Signal Processing and Parallel Algorithms, G. H. Golub and P. Van Dooren, eds.,
Springer-Verlag, Berlin, New York, pp. 41-73, 1991.
[BK87a] A. BRUCKSTEIN AND T. KAILATH, An inverse scattering framework for several prob-
lems in signal processing, IEEE ASSP Magazine, pp. 6-20, January 1987.

Bibliography 
323
[BK87b] A. BRUCKSTEIN AND T. KAILATH, Inverse scattering for discrete transmission-line
models, SIAM Rev., 29, pp. 359-389, 1987.
[BK87c] A. BRUCKSTEIN AND T. KAILATH, On discrete Schrodinger equations and their two
component wave-equation equivalents, J. Math. Phys., 28, pp. 2914-2924, 1987.
[BK95] E. BOMAN AND I. KOLTRACHT, Fast transform based preconditioners for Toeplitz equa-
tions, SIAM J. Matrix Anal. Appl., 16, pp. 628-645, 1995.
[BKLS98a] T. BOROS, T. KAILATH, H. LEV-ARI, AND A. H. SAVED, A generalized Schur-
type algorithm for the joint factorization of a structured matrix and its inverse: Part Iâ
Nondegenerate case, preprint.
[BKLS98b] T. BOROS, T. KAILATH, H. LEV-ARI, AND A. H. SAVED, A generalized Schur-type
algorithm for the joint factorization of a structured matrix and its inverse: Part IIâGeneral
case, preprint.
[Bla84] R. E. BLAHUT, Fast Algorithms for Digital Signal Processing, Addison-Wesley, Reading,
MA, 1984.
[BM96a] D. BINI AND B. MEINI, On cyclic reduction applied to a class of Toeplitz-like matrices
arising in queueing problems, in Computations with Markov Chains, W. J. Stewart, ed., Kluwer
Academic Publishers, Norwell, MA, pp. 21-38, 1996.
[BM96b] D. BINI AND B. MEINI, On the solution of a nonlinear matrix equation arising in
queueing problems, SIAM J. Matrix Anal. Appl., 17, pp. 906-926, 1996.
[BM97a] D. BINI AND B. MEINI, Improved cyclic reduction for solving queueing problems, Nu-
mer. Algorithms, 15, pp. 57-74, 1997.
[BM97b] D. A. BlNl AND B. MEINI, Effective 
methods for solving banded Toeplitz systems,
SIAM J. Matrix Anal. Appl., 20, pp. 700-719, 1999.
[BM98a] D. BINI AND B. MEINI, Inverting block Toeplitz matrices in block Hessenberg form by
means of displacement operators: Application to queueing problems, Linear Algebra Appl., 272,
pp. 1-16, 1998.
[BM98b] D. BINI AND B. MEINI, Using displacement structure for solving non-skip-free M/G/l
type Markov chains, in Advances in Matrix Analytic Methods, A. Alfa and S. Chakravarthy,
eds., Notable Publications, Neshanic Station, NJ, 1998, pp. 17-37.
[Boy68] C. A. BOYER, A History of Mathematics, John Wiley, New York, 1968.
[Boz95] E. Bozzo, Algebras of higher dimension for displacement decompositions and compu-
tations with Toeplitz plus Hankel matrices, Linear Algebra Appl., 230, pp. 127-150, 1995.
[BP86] D. BINI AND V. PAN, Polynomial division and its computational complexity, J. Com-
plexity, 2, pp. 179-203, 1986.
[BP93] D. BINI AND V. Y. PAN, Improved parallel computations with Toeplitz-like and Hankel
matrices, Linear Algebra Appl., 188-189, pp. 3-29, 1993.
[BP94] D. BINI AND V. Y. PAN, Matrix and Polynomial Computations, Vol. I: Fundamental
Algorithms, Birkhauser, Boston, 1994.
[Bre91] R. P. BRENT, Parallel algorithms for Toeplitz systems, in Numerical Linear Algebra,
Digital Signal Processing and Parallel Algorithms, G. H. Golub and P. Van Dooren, eds.,
Springer-Verlag, Berlin, New York, pp. 75-92, 1991.
[Bre78] J. W. BREWER, Kronecker products and matrix calculus in system theory, IEEE Trans.
Circuits Systems, 25, pp. 772-781, 1978.
[Bre97] R. P. BRENT, Numerical stability of some fast algorithms for structured matrices, in
Proc. Workshop on Scientific Computing, Hong Kong, March 1997, Springer-Verlag, Berlin,
New York, 1998.
[BS88] A. W. BOJANCZYK AND A. O. STEINHARDT, Matrix downdating techniques for sig-
nal processing, Proc. SPIE Conference on Advanced Algorithms and Architectures for Signal
Processing, San Diego, 975, pp. 68-75, 1988.

324 
Bibliography
[BS91] A. W. BOJANCZYK AND A. O. STEINHARDT, Stability analysis of a Householder-based al-
gorithm for downdating the Cholesky factorization, SIAM J. Sci. Statist. Comput., 12, pp. 1255-
1265, 1991.
[BSK94] T. BOROS, A. H. SAVED, AND T. KAILATH, Structured matrices and unconstrained
rational interpolation, Linear Algebra Appl., 203-204, pp. 155-188, 1994.
[BSK99] T. BOROS, A. H. SAVED, AND T. KAILATH, A recursive method for solving uncon-
strained tangential interpolation problems, IEEE Trans. Automat. Control, 44, pp. 454-470,
1999.
[BSLK96] T. BOROS, A. H. SAVED, H. LEV-ARI, AND T. KAILATH, A generalized Schur-type
algorithm for the joint factorization of a structured matrix and its inverse, Calcolo, 33, pp. 131â
145, 1996.
[BS90] A. BOTTCHER AND B. SiLBERMANN, Analysis of Toeplitz Operators, Springer-Verlag,
Berlin, New York, 1990.
[BT89] D. BERTSEKAS AND J. TSITSIKLIS, Parallel and Distributed Computation: Numerical
Methods, Prentice-Hall, Englewood Cliffs, NJ, 1989.
[Bun85] J. BUNCH, Stability of methods for solving Toeplitz systems of equations, SIAM J. Sci.
Statist. Comput., 6, pp. 349-364, 1985.
[Bun87] J. BUNCH, The weak and strong stability of algorithms in numerical linear algebra,
Linear Algebra Appl., 88/89, pp. 49-66, 1987.
[Bun92] J. BUNCH, Matrix properties of the Levinson and Schur algorithms, J. Numer. Linear
Algebra Appl., 1, pp. 183-198, 1992.
[Bur75] J. P. BURG, Maximum Entropy Spectral Analysis, Ph.D. thesis, Stanford University,
Stanford, CA, 1975.
[Car90] J. CARDOSO, Eigen-structure of the fourth-order cumulant tensor with application to
the blind source separation problem, in Proc. ICASSP, Albuquerque, NM, 1990, pp. 2655-2658.
[Car95] J. CARDOSO, A tetradic decomposition of 4th-order tensors: Application to the source
separation problem, in SVD and Signal Processing, III, M. Moonen and B. D. Moor, eds.,
Elsevier Science Publishers, Amsterdam, 1995.
[CC82] C. CHUI AND A. CHAN, Application of approximation theory methods to recursive digital
filter design, IEEE Trans. Acoustics, Speech Signal Process., 30, pp. 18-24, 1982.
[CC92] R. CHAN AND T. CHAN, Circulant preconditioned for elliptic problems, Numer. Linear
Algebra Appl., 1, pp. 77-101, 1992.
[CC96a] J. CARDOSO AND P. COMON, Independent component analysis, a survey of some alge-
braic methods, in Proc. ISCAS, 1996.
[CC96b] R. CHAN AND W. CHING, Toeplitz-circulant preconditioned for Toeplitz systems and
their applications to queueing networks with batch arrivals, SIAM J. Sci. Comput., 17, pp.
762-772, 1996.
[CCW95] R. CHAN, T. CHAN, AND C. WONG, Cosine transform based preconditioners for total
variation minimization problems in image processing, in Iterative Methods in Linear Algebra,
II, 3, S. Margenov and P. Vassilevski, eds., IMACS Series in Computational and Applied
Mathematics, Proc. Second IMACS International Symposium on Iterative Methods in Linear
Algebra, Bulgaria, pp. 311-329, 1995.
[CCW96] R. CHAN, W. CHING, AND C. WONG, Optimal trigonometric preconditioners for
elliptic problems and queueing problems, SEA Bull. Math., 20, pp. 110-117, 1996.
[CCZ97] W. CHING, R. CHAN, AND X. ZHOU, Circulant preconditioners for Markov modulated
Poisson processes and their applications to manufacturing systems, SIAM J. Matrix. Anal.
Appl., 18, pp. 464-481, 1997.

Bibliography 
325
[CDH97] R. CHAN, T. DELiLLO, AND M. HORN, The numerical solution of the biharmonic
equation by conformal mapping, SIAM J. Sci. Comput., 18, pp. 1571-1582, 1997.
[CDH98] R. CHAN, T. DsLlLLO, AND M. HORN, Superlinear convergence estimates for a con-
jugate gradient method for the biharmonic equation, SIAM J. Sci. Comput., 19, pp. 139-147,
1998.
[CH92a] T. F. CHAN AND P. C. HANSEN, A look-ahead Levinson algorithm for indefinite
Toeplitz systems, SIAM J. Matrix Anal. Appl. 13, pp. 490-506, 1992.
[CH92b] T. F. CHAN AND P. C. HANSEN, A look-ahead Levinson algorithm for general Toeplitz
systems, IEEE Trans. Signal Process., 40, pp. 1079-1090, 1992.
[Cha88] T. CHAN, An optimal circulant preconditioner for Toeplitz systems, SIAM J. Sci. Statist.
Comput., 9, pp. 766-771, 1988.
[Cha89a] R. CHAN, Circulant preconditioners for Hermitian Toeplitz systems, SIAM J. Matrix
Anal. Appl., 10, pp. 542-550, 1989.
[Cha89b] R. CHAN, The spectrum of a family of circulant preconditioned Toeplitz systems, SIAM
J. Numer. Anal., 26, pp. 503-506, 1989.
[Cha91] R. CHAN, Toeplitz preconditioners for Toeplitz systems with nonnegative generating
functions, IMA J. Numer. Anal., 11, pp. 333-345, 1991.
[Chu89] J. CHUN, Fast Array Algorithms for Structured Matrices, Ph.D. dissertation, Stanford
University, Stanford, CA, 1989.
[CJY91a] R. CHAN, X. JIN, AND M. YEUNG, The circulant operator in the Banach algebra of
matrices, Linear Algebra Appl., 149, pp. 41-53, 1991.
[CJY91b] R. CHAN, X. JIN, AND M. YEUNG, The spectra of super-optimal circulant precondi-
tioned Toeplitz systems, SIAM J. Numer. Anal., 28, pp. 871-879, 1991.
[CK91a] J. CHUN AND T. KAILATH, Divide-and-conquer solutions of least-squares problems for
matrices with displacement structure, SIAM J. Matrix Anal. Appl., 12, pp. 128-145, 1991.
[CK91b] J. CHUN AND T. KAILATH, Displacement structure for Hankel, Vandermonde, and
related (derived) matrices, Linear Algebra Appl., 151, pp. 199-227, 1991.
[CKL87] J. CHUN, T. KAILATH, AND H. LEV-ARI, Fast parallel algorithms for QR and trian-
gular factorization, SIAM J. Sci. Statist. Comput., 8, pp. 899-913, 1987.
[CKM82] G. CARAYANNIS, N. KALOUPTSIDIS, AND D. MANOLAKIS, Fast recursive algorithms
for a class of linear equations, IEEE Trans. Acoustics Speech Signal Process., 30, pp. 227-239,
1982.
[CM96] P. COMON AND B. MOURRAIN, Decomposition of quantics in sums of powers of linear
forms, Signal Process., 53, pp. 93-107, 1996.
[CN93a] R. CHAN AND M. NG, Fast iterative solvers for Toeplitz-plus-band systems, SIAM J.
Sci. Comput., 14, pp. 1013-1019, 1993.
[CN93b] R. CHAN AND M. NG, Toeplitz preconditioners for Hermitian Toeplitz systems, Linear
Algebra Appl., 190, pp. 181-208, 1993.
[CN96] R. CHAN AND M. NG, Conjugate gradient methods for Toeplitz systems, SIAM Rev.,
38, pp. 427-482, 1996.
[CNP93] R. CHAN, J. NAGY, AND R. PLEMMONS, FFT-based preconditioners for Toeplitz-block
least squares problems, SIAM J. Numer. Anal., 30, pp. 1740-1768, 1993.
[CNP94a] R. CHAN, J. NAGY, AND R. PLEMMONS, Circulant preconditioned Toeplitz least
squares iterations, SIAM J. Matrix Anal. Appl., 15, pp. 80-97, 1994.
[CNP94b] R. CHAN, J. NAGY, AND R. PLEMMONS, Displacement preconditioner for Toeplitz
least squares iterations, Electron. Trans. Numer. Anal., 2, pp. 44-56, 1994.

326 
Bibliography
[CNP96] R. CHAN, M. NG, AND R. PLEMMONS, Generalization of Strang's preconditioner with
applications to Toeplitz least squares problems, Numer. Linear Algebra Appl., 3, pp. 45-64,
1996.
[CNW96] R. CHAN, M. NG, AND C. WONG, Sine transform based preconditioners for symmetric
Toeplitz systems, Linear Algebra Appl., 232, pp. 237-259, 1996.
[CO94] T. CHAN AND J. OLKIN, Circulant preconditioners for Toeplitz-block matrices, Numer.
Algorithms, 6, pp. 89-101, 1994.
[Col60] L. COLLATZ, The Numerical Treatment of Differential 
Equations, 3rd ed., Springer-
Verlag, Berlin, New York, 1960.
[Con73] J. CONWAY, Functions of One Complex Variable, Springer-Verlag, Berlin, 1973.
[Con96] T. 
CONSTANTINESCU, Schur Parameters, Factorization and Dilation Problems,
Birkhauser, Basel, 1996.
[Coo72] R. COOPER, Introduction to Queueing Theory, 2nd ed., Macmillan, New York, 1972.
[CS89] R. CHAN AND G. STRANG, Toeplitz equations by conjugate gradients with circulant pre-
conditioner, SIAM J. Sci. Statist. Comput., 10, pp. 104-119, 1989.
[CS96] S. CHANDRASEKARAN AND A. H. SAVED, Stabilizing the generalized Schur algorithm,
SIAM J. Matrix Anal. Appl., 17, pp. 950-983, 1996.
[CS98] S. CHANDRASEKARAN AND A. H. SAVED, A fast stable solver for nonsymmetric Toeplitz
and quasi-Toeplitz systems of linear equations, SIAM J. Matrix Anal. Appl., 19, pp. 107-139,
1998.
[CSK94] T. CONSTANTINESCU, A. H. SAVED, AND T. KAILATH, A recursive Schur-based ap-
proach to the four-block problem, IEEE Trans. Automat. Control, 39, pp. 1476-1481, 1994.
[CSK95] T. CONSTANTINESCU, A. H. SAVED, AND T. KAILATH, Displacement structure and
completion problems, SIAM J. Matrix Anal. Appl., 16, pp. 58-78, 1995.
[CSK99] T. CONSTANTINESCU, A. H. SAVED, AND T. KAILATH, Displacement structure and
Hoo problems, in Advances in System Theory, T. Djaferis, ed., Kluwer Academic Publishers,
Norwell, MA, 1999, to appear.
[CT65] J. COOLEY AND J. TUKEY, An algorithm for the machine calculation of complex Fourier
series, Math. Comp., 19, pp. 297-301, 1965.
[CT94] R. CHAN AND P. TANG, Fast band-Toeplitz preconditioners for Hermitian Toeplitz sys-
tems, SIAM J. Sci. Comput., 15, pp. 164-171, 1994.
[CXT94] Y. M. CHO, G. Xu, AND T. KAILATH, Fast identification of state-space models via
exploitation of displacement structure, IEEE Trans. Automat. Control, 39, pp. 2004-2017,
1994.
[CY92] R. CHAN AND M. YEUNG, Circulant preconditioners constructed from kernels, SIAM J.
Numer. Anal., 29, pp. 1093-1103, 1992.
[CY93] R. CHAN AND M. YEUNG, Circulant preconditioners for complex Toeplitz matrices,
SIAM J. Numer. Anal., 30, pp. 1193-1207, 1993.
[CybSO] G. CYBENKO, The numerical stability of the Levins on-Durbin algorithm for Toeplitz
systems of equations, SIAM J. Sci. Statist. Comput., 1, pp. 303-319, 1980.
[Cyb83] G. CYBENKO, A general orthogonalization technique with applications to time series
analysis and signal processing, Math. Comp., 40, pp. 323-336, 1983.
[Cyb87] G. CYBENKO, Fast Toeplitz orthogonalization using inner products, SIAM J. Sci. Statist.
Comput., 8, pp. 734-740, 1987.
[Dan67] J. DANIEL, The conjugate gradient method for linear and nonlinear operator equations,
SIAM J. Numer. Anal., 4, pp. 10-26, 1967.
[Dav79] P. DAVIS, Circulant Matrices, John Wiley, New York, 1979.

Bibliography 
327
[DD84] P. DEWILDE AND H. DYM, Lossless inverse scattering, digital filters, and estimation
theory, IEEE Trans. Inform. Theory, 30, pp. 644-662, 1984.
[Del82] J.-M. DELOSME, Algorithms and Implementations for Linear Least-Squares Estimation,
Ph.D. dissertation, Stanford University, Stanford, CA, 1982.
[DFS93] F. Di BENEDETTO, G. FIORENTINO, AND S. SERRA, C.G. preconditioning for Toeplitz
matrices, Comput. Math. Appl., 25, pp. 35-45, 1993.
[DGK85] P. DELSARTE, Y. V. GENIN, AND Y. G. KAMP, A generalisation of the Levinson
algorithm for Hermitian Toeplitz matrices with any rank profile, IEEE Trans. Acoustics Speech
Signal Process., 33, pp. 964-971, 1985.
[DI86] J.-M. DELOSME AND I. C. F. IPSEN, Parallel solution of symmetric positive definite
systems with hyperbolic rotations, Linear Algebra Appl., 77, pp. 75-111, 1986.
[DiB95] F. Di BENEDETTO, Analysis of preconditioning techniques for ill-conditioned Toeplitz
matrices, SIAM J. Sci. Comput., 16, pp. 682-697, 1995.
[DM85] L. DELVES AND J. MOHAMED, Computational Methods for Integral Equations, Cam-
bridge University Press, Cambridge, UK, 1985.
[DMV99] L. DELATHAUWER, B. DEMOOR, AND J. VANDEWALLE, A multilinear singular value
decomposition, SIAM J. Matrix Anal. Appl., to appear.
[Dur59] J. DURBIN, The fitting of time-series models, Rev. Int. Stat. Inst., 28, pp. 229-249,
1959.
[DV93] P. M. DEWILDE AND A. J. VAN DER VEEN, On the Hankel-norm approximation of
upper-triangular operators and matrices, Integral Equations Operator Theory, 17, pp. 1-45,
1993.
[DV98] P. DEWILDE AND A. J. VAN DER VEEN, Time-Varying Systems and Computations,
Kluwer Academic Publishers, Boston, MA, 1998.
[DVK78] P. DEWILDE, A. C. VIEIRA, AND T. KAILATH, On a generalized Szego-Levins on re-
alization algorithm for optimal linear predictors based on a network synthesis approach, IEEE
Trans. Circuits Systems, 25, pp. 663-675, 1978.
[Dym89a] H. DYM, J-Contractive Matrix Functions, Reproducing Kernel Hilbert Spaces, and
Interpolation, CBMS Regional Conf. Ser. in Math. 71, AMS, Providence, RI, 1989.
[Dym89b] H. DYM, On reproducing kernel spaces, J-unitary matrix functions, interpolation and
displacement rank, Oper. Theory Adv. Appl. 41, pp. 173-239, Birkhauser, Basel, 1989.
[Edw82] R. E. EDWARDS, Fourier Series, Vols. 1 and 2, 2nd ed., Springer-Verlag, Berlin, New
York, 1982.
[ER82] D. F. ELLIOTT AND K. R. RAO, Fast Transform Algorithms, Analyses, Applications,
Academic Press, New York, 1982.
[FCG79] P. FAURRE, M. CLERGET, AND F. GERMAIN, Operateurs Rationnels Positifs, Dunod,
Paris, 1979.
[FF90] C. FoiAS AND A. E. FRAZHO, The Commutant Lifting Approach to Interpolation Prob-
lems, Oper. Theory Adv. Appl. 44, Birkhauser, Basel, 1990.
[Fie85] M. FIEDLER, Hankel and Loewner matrices, Linear Algebra Appl., 58, pp. 75-95, 1985.
[FKML78] B. FRIEDLANDER, T. KAILATH, M. MORF, AND L. LJUNG, Extended Levinson and
Chandrasekhar equations for general discrete-time linear estimation problems, IEEE Trans.
Automat. Control, 23, pp. 653-659, 1978.
[FM67] G. E. FORSYTHE AND C. B. MOLER, Computer Solution of Linear Algebraic Systems,
Prentice-Hall, Englewood Cliffs, NJ, 1967.
[FP81] R. E. FUNDERLIC AND R. J. PLEMMONS, LU decomposition of M-matrices by elimina-
tion without pivoting, Linear Algebra Appl., 41, pp. 99-110, 1981.

328 
Bibliography
[Pre94] R. W. FREUND, A look-ahead Bareiss algorithm for general Toeplitz matrices, Numer.
Math., 68, pp. 35-69, 1994.
[FS95] G. FlORENTlNO AND S. SERRA, Tau preconditioned for (high order) elliptic problems, in
Proc. 2nd IMACS Conf. on Iterative Methods in Linear Algebra, Vassilevski, ed., pp. 241-252,
Blagoevgrad, Bulgaria, 1995.
[FZ93a] R. W. FREUND AND H. ZHA, Formally biorthogonal polynomials and a look-ahead
Levinson algorithm for general Toeplitz systems, Linear Algebra Appl., 188/189, pp. 255-303,
1993.
[FZ93b] R. W. FREUND AND H. ZHA, A look-ahead algorithm for the solution of general Hankel
systems, Numer. Math., 64, pp. 295-321, 1993.
[Gem97] L. GEMIGNANI, Schur complement of Bezoutians with applications to the inversion of
block Hankel and block Toeplitz matrices, Linear Algebra Appl., 253, pp. 39-59, 1997.
[Gen73] M. GENTLEMAN, Least squares computations by Givens transformations, J. Inst. Math.
Appl., 12, pp. 329-336, 1973.
[Ger54] L. Y. GERONIMUS, Polynomials orthogonal on a circle and their applications, Amer.
Math. Soc. Transl., 3, pp. 1-78, 1954 (in Russian, 1948).
[GF74] I. GOHBERG AND I. FEL'DMAN, Convolution equations and projection methods for their
solution, Transl. Math. Monogr., 41, AMS, Providence, RI, 1974.
[GGM92] A. GREENBAUM, L. GREENGARD, AND A. MAYO, On the numerical solution of the
biharmonic equation in the plane, Physica D, 60, pp. 216-225, 1992.
[GH93a] M. H. GUTKNECHT AND M. HOCHBRUCK, Look-ahead Levinson and Schur algorithms
for non-Hermitian Toeplitz Systems, IPS Research Report 93-11, ETH, Zurich, 1993.
[GH93b] M. H. GUTKNECHT AND M. HOCHBRUCK, The stability of inversion formulas for
Toeplitz matrices, IPS Research Report 93-13, ETH, Zurich, 1993.
[GHKT94] H. R. GAIL, S. L. HANTLER, A. G. KONHEIM, AND B. A. TAYLOR, An analysis of
a class of telecommunications models, Performance Evaluation, 21, pp. 151-161, 1994.
[GHT97] H. R. GAIL, S. L. HANTLER, AND B. A. TAYLOR, Non-skip-free M/G/1 and G/M/l
type Markov chains, Adv. Appl. Probab., 29, pp. 733-758, 1997.
[Gia90] G. G. GlANNAKlS, On the identifiability of non-Gaussian models using cumulants, IEEE
Trans. Automat. Control, 35, pp. 18-26, 1990.
[GK93] I. GOHBERG AND I. KOLTRACHT, Mixed, componentwise and structured condition num-
bers, SIAM J. Matrix Anal. Appl., 14, pp. 688-704, 1993.
[GKO95] I. GOHBERG, T. KAILATH, AND V. OLSHEVSKY, Fast Gaussian elimination with par-
tial pivoting for matrices with displacement structure, Math. Comp., 64, pp. 1557-1576, 1995.
[GKX94] I. GOHBERG, I. KOLTRACHT, AND D. XIAO, Condition and accuracy of algorithms for
computing Schur coefficients 
of Toeplitz matrices, SIAM J. Matrix Anal. Appl., 15, pp. 1290-
1309, 1994.
[GL55] I. GELFAND AND B. LEVITAN, On the determination of a differential equation from its
spectral function, Amer. Math. Soc. Transl., 1, pp. 253-304, 1955.
[Glo84] K. GLOVER, All optimal Hankel-norm approximations to linear multivariable systems
and their LÂ°Â°-error bounds, Internat. J. Control, 39, pp. 1115-1193, 1984.
[GM89] G. G. GIANNAKIS AND J. M. MENDEL, Identification of nonminimum phase systems
using higher order statistics, IEEE Trans. Acoustics Speech Signal Process., 37, pp. 360-377,
1989.
[GO92] I. GOHBERG AND V. OLSHEVSKY, Circulant displacements and decomposition of matri-
ces, Integral Equations Operator Theory, 15, pp. 730-743, 1992.
[GO94c] I. GOHBERG AND V. OLSHEVSKY, Complexity of multiplication with vectors for struc-
tured matrices, Linear Algebra Appl., 202, pp. 163-192, 1994.

Bibliography 
329
[Goh86] I. GOHBERG, ED., I. Schur Methods in Operator Theory and Signal Processing, Oper.
Theory Adv. Appl. 18, Birkhauser, Basel, 1986.
[Gol65] G. H. GOLUB, Numerical methods for solving linear least squares problems, Numer.
Math., 7, pp. 206-216, 1965.
[Gou91] N. GOULD, On growth in Gaussian elimination with complete pivoting, SIAM J. Matrix
Anal. Appl., 12, pp. 354-361, 1991.
[GR70] G. H. GOLUB AND C. REINSCH, Singular value decomposition and least squares solu-
tions, Numer. Math., 14, pp. 403-420, 1970.
[Gri96] V. S. GRIGORASCU, Tenseurs Structures, Produits d'Ordre Superieur et Cumulants,
Ph.D. dissertation, University of Paris, Orsay, 1996.
[Gro84] C. GROETSCH, The Theory of Tikhonov Regularization for Fredholm Equations of the
First Kind, Pitman Publishing, London, 1984.
[GS84] U. GRENANDER AND G. SZEGO, Toeplitz Forms and Their Applications, 2nd ed., Chelsea
Publishing, New York, 1984.
[GS90] G. G. GiANNAKis AND A. M. SWAMY, On estimating noncausal nonminimum phase
ARM A models of non-Guassian processes, IEEE Trans. Acoustics Speech Signal Process., 38,
pp. 478-495, 1990.
[GS94] I. GOHBERG AND L. A. SAKHNOVICH, ED., Matrix and Operator-Valued Functionsâ
V. P. Potapov Memorial Volume, Oper. Theory Adv. Appl. 72, Birkhauser, Basel, 1994.
[GT83] D. GILBARG AND N. S. TRUDINGER, Elliptic Partial Differential Equations of Second
Order, 2nd ed., Springer-Verlag, Berlin, New York, 1983.
[GTH85] W. K. GRASSMAN, M. I. TAKSAR, AND D. P. HEYMAN, Regenerative analysis and
steady state distribution for Markov chains, Oper. Res., 33, pp. 1107-1116, 1985.
[Gu95a] M. Gu, Stable and efficient algorithms for structured systems of linear equations, SIAM
J. Matrix Anal. Appl., 19, pp. 279-306, 1998.
[Gu95b] M. Gu, New Fast Algorithms for Structured Least Squares Problems, Tech. Rep. LBL-
37878, Lawrence Berkeley Laboratory, 1995.
[Gut93] M. H. GUTKNECHT, Stable row recurrences for the Pade table and generically superfast
lookahead solvers for non-Hermitian Toeplitz systems, Linear Algebra Appl., 188/189, pp. 351-
422, 1993.
[GV96] G. GOLUB AND C. VAN LOAN, Matrix Computations, 3rd ed., The Johns Hopkins
University Press, Baltimore, MD, 1996.
[GW66] G. H. GOLUB AND J. H. WILKINSON, Note on iterative refinement of least squares
solution, Numer. Math., 9, pp. 139-148, 1966.
[Hay96] S. HAYKIN, Adaptive Filter Theory, 3rd ed., Prentice-Hall, Englewood Cliffs, NJ, 1996.
[Hei95] G. HEINIG, Inversion of generalized Cauchy matrices and other classes of structured
matrices, Linear Algebra Signal Process., IMA Vol. Math. Appl. 69, pp. 95-114, 1995.
[Hel87] J. W. HELTON, Operator Theory, Analytic Functions, Matrices and Electrical Engineer-
ing, Conference Board of the Mathematical Sciences, AMS, Providence, RI, 1987.
[HG93] P. C. HANSEN AND H. GESMAR, Fast orthogonal decomposition of rank deficient Toeplitz
matrices, Numer. Algorithms, 4, pp. 151-166, 1993.
[HH77] L. L. HIRSCHMAN AND D. E. HUGHES, Extreme Eigen Values of Toeplitz Operators,
Lecture Notes in Math., Springer-Verlag, Heidelberg, 1977.
[HH89] N. J. HiGHAM AND D. J. HlGHAM, Large growth factors in Gaussian elimination with
pivoting, SIAM J. Matrix Anal. Appl., 10, pp. 155-164, 1989.
[HH92] D. J. HIGHAM AND N. J. HIGHAM, Backward error and condition of structured linear
systems, SIAM J. Matrix Anal. Appl., 13, pp. 162-175, 1992.

330 
Bibliography
[Hig96] N. J. HiGHAM, Accuracy and Stability of Numerical Algorithms, SIAM, Philadelphia,
1996.
[Hil94] D. HILBERT, Ein Beitrag zur Theorie des Legendre'schen Polynoms, Acta Math., 18, pp.
155-160, 1894.
[HJ85] R. A. HORN AND C. R. JOHNSON, Matrix Analysis, Cambridge University Press, Cam-
bridge, UK, 1985.
[HN94] M. HANKE AND J. NAGY, Toeplitz approximate inverse preconditioner for banded
Toeplitz matrices, Numer. Algorithms, 7, pp. 183-199, 1994.
[HR84] G. HEINIG AND K. ROST, Algebraic Methods for Toeplitz-like Matrices and Operators,
Akademie-Verlag, Berlin, Birkhauser, Boston, 1984.
[HS52] M. HESTENES AND E. STIEFEL, Methods of conjugate gradients for solving linear systems,
J. Res. National Bureau of Standards, Sec. B, 49, pp. 409-436, 1952.
[Huc93] T. HUCKLE, Some aspects of circulant preconditioners, SIAM J. Sci. Comput., 14, pp.
531-541, 1993.
[Huc94] T. HUCKLE, Iterative methods for Toeplitz-like matrices, Report SCCM-94-05, Com-
puter Science Dept., Stanford University, Stanford, CA, 1994.
[HY93] J. HSUE AND A. YAGLE, Fast algorithms for close-to-Toeplitz-plus-Hankel systems and
two-sided linear prediction, IEEE Trans. Signal Process., 41, pp. 2349-2361, 1993.
[IK66] E. ISAACSON AND H. B. KELLER, Analysis of Numerical Methods, John Wiley, New
York, 1966.
[Ioh82] I. S. IOHVIDOV, Hankel and Toeplitz Forms: Algebraic Theory, Birkhauser, Boston,
1982.
[Jai89] A. JAIN, Fundamentals of Digital Image Processing, Prentice-Hall, Englewood Cliffs,
NJ, 1989.
[JK92] B. JELONNEK AND K.-D. KAMMEYER, Improved methods for blind system identification
using higher order statistics, IEEE Trans. Signal Process., 40, pp. 2947-2960, 1992.
[JW77] M. JANKOWSKI AND M. WOZNIAKOWSKI, Iterative refinement implies numerical stabil-
ity, BIT, 17, pp. 303-311, 1977.
[Kai73] T. KAILATH, Some new algorithms for recursive estimation in constant linear systems,
IEEE Trans. Inform. Theory, 19, pp. 750-760, 1973.
[KaiSO] T. KAILATH, Linear Systems, Prentice-Hall, Englewood Cliffs, NJ, 1980.
[Kai85] T. KAILATH, Signal processing in the VLSI era, in VLSI and Modern Signal Processing,
S. Y. Kung, H. J. Whitehouse, and T. Kailath, eds., pp. 5-24, Prentice-Hall, Englewood Cliffs,
NJ, 1985.
[Kai86] T. KAILATH, A theorem of I. Schur and its impact on modern signal processing, Oper.
Theory Adv. Appl., 18, pp. 9-30, Birkhauser, Basel, 1986.
[Kai87] T. KAILATH, Signal processing applications of some moment problems, in Moments in
Mathematics 37, H. Landau, ed., pp. 71-109, AMS, Providence, RI, 1987.
[Kai91] T. KAILATH, Remarks on the origin of the displacement-rank concept, Appl. Math.
Comput., 45, pp. 193-206, 1991.
[Kat76] Y. KATZNELSON, An Introduction to Harmonic Analysis, 2nd ed., Dover Publications,
New York, 1976.
[KC94] T. KAILATH AND J. CHUN, Generalized displacement structure for block-Toeplitz,
Toeplitz-block, and Toeplitz-derived matrices, SIAM J. Matrix Anal. Appl., 15, pp. 114-128,
1994.
[KFA70] R. E. KALMAN, P. L. FALB, AND M. A. ARBIB, Topics in Mathematical System
Theory. Int. Ser. Pure Appl. Math. McGraw-Hill, New York, 1970.

Bibliography 
331
[KH83] S. Y. KUNG AND Y. H. Hu, A highly concurrent algorithm and pipelined architecture for
solving Toeplitz systems, IEEE Trans. Acoustics Speech Signal Process., 31, pp. 66-76, 1983.
[KK92] T. Ku AND C. Kuo, Design and analysis of Toeplitz preconditioned, IEEE Trans.
Signal Process., 40, pp. 129-141, 1992.
[KK93a] T. Ku AND C. Kuo, Preconditioned iterative methods for solving Toeplitz-plus-Hankel
systems, SI AM J. Numer. Anal., 30, pp. 824-845, 1993.
[KK93b] T. Ku AND C. Kuo, Spectral properties of preconditioned rational Toeplitz matrices,
SIAM J. Matrix Anal. Appl., 14, pp. 146-165, 1993.
[KK93c] T. Ku AND C. Kuo, Spectral properties of preconditioned rational Toeplitz matrices:
The nonsymmetric case, SIAM J. Matrix Anal. Appl., 14,, pp. 521-544, 1993.
[KKM79a] T. KAILATH, S. Y. KUNG, AND M. MORF, Displacement ranks of matrices and
linear equations, J. Math. Anal. Appl., 68, pp. 395-407, 1979.
[KKM79b] T. KAILATH, S. Y. KUNG, AND M. MORF, Displacement ranks of a matrix, Bull.
Amer. Math. Soc., 1, pp. 769-773, 1979.
[KLM78] T. KAILATH, L. LJUNG, AND M. MORF, Generalized Krein-Levins on equations for
efficient 
calculation of Fredholm resolvents of nondisplacement kernels, in Topics in Functional
Analysis, I. Gohberg and M. Kac, eds., pp. 169-184, Academic Press, New York, 1978.
[KnuSl] D. E. KNUTH, The Art of Computer Programming: Seminumerical Algorithms, 2,
Addison-Wesley, Reading, MA, 1981.
[KO95] T. KAILATH AND V. OLSHEVSKY, Displacement structure approach to Chebyshev-
Vandermonde and related matrices, Integral Equations Operator Theory, 22, pp. 65-92, 1995.
[KO96] T. 
KAILATH AND V. OLSHEVSKY, Displacement structure approach to discrete-
trigonometric-transform based preconditioners of G. Strang type and T. Chan type, Calcolo,
33, pp. 191-208, 1996.
[KO98] T. KAILATH AND V. OLSHEVSKY, Diagonal pivoting for partially reconstructive Cauchy-
like matrices, with applications to Toeplitz-like linear equations and to boundary rational matrix
interpolation problems, Linear Algebra Appl., 254, pp. 251-302, 1997.
[Kol41] A. N. KOLMOGOROV, Interpolation and extrapolation of stationary random sequences,
Izv. Akad. Nauk SSSR, 5, pp. 3-11, 1941 (in Russian); German summary, pp. 11-14.
[KR86] A. G. KONHEIM AND M. REISER, The moveable-boundary multiplexor: Stability and
Decomposability, in Teletraffic Analysis and Computer Performance Evaluation, O. J. Boxma,
J. W. Cohen, and H. C. Tijms, eds., North-Holland, Amsterdam, pp. 375-394, 1986.
[KS91] T. KAILATH AND A. H. SAYED, Fast algorithms for generalized displacement structures,
in Proc. Internat. Symposium Math. Theory of Networks and Systems, H. Kimura and S.
Kodama, eds., 2, pp. 27-32, Kobe, Japan, 1991.
[KS95a] T. KAILATH AND A. H. SAYED, Displacement structure: Theory and applications,
SIAM Rev., 37, pp. 297-386, 1995.
[KS95b] T. KAILATH AND A. H. SAYED, On lossless cascades in structured matrix factorization,
Arch. Fur Electronik und Ubertragungstechnik, 49, pp. 307-312, 1995.
[KSH99] T. KAILATH, A. H. SAYED, AND B. HASSIBI, State-Space Estimation, Prentice-Hall,
Englewood Cliffs, NJ, 1999.
[Kun78] S. Y. KUNG, A new identification and model reduction algorithm via singular value
decomposition, in Proc. Asilomar Conf. on Circuits, Systems and Comp., pp. 705-714, Asilomar,
CA, 1978.
[KVM78] T. KAILATH, A. VIEIRA, AND M. MORF, Inverses of Toeplitz operators, innovations
and orthogonal polynomials, SIAM Rev., 20, pp. 106-119, 1978.
[LBK91] H. LEV-ARI, Y. BISTRITZ, AND T. KAILATH, Generalized Bezoutians and families of
efficient zero-location procedures, IEEE Trans. Circuits Systems, 38, pp. 170-185, 1991.

332 
Bibliography
[Laf75] J. C. LAFON, Base tensorielle des matrices des Hankel (on de Toeplitz), applications,
Numer. Math., 23, pp. 349-361, 1975.
[LK84] H. LEV-ARI AND T. KAILATH, Lattice filter parametrization and modeling of nonsta-
tionary processes, IEEE Trans. Inform. Theory, 30, pp. 2-16, 1984.
[LK86] H. LEV-ARI AND T. KAILATH, Triangular factorization of structured Hermitian matri-
ces, Oper. Theory Adv. Appl. 18, pp. 301-324, 1986.
[LK92] H. LEV-ARI AND T. KAILATH, State-space approach to factorization of lossless transfer
functions and structured matrices, Linear Algebra Appl., 162-164, pp. 273-295, 1992.
[Lev47] N. LEVINSON, The Wiener RMS (Root-Mean-Square) error criterion in filter design and
prediction, J. Math. Phys., 25, pp. 261-278, 1947.
[Lev83] H. LEV-ARI, Nonstationary Lattice-Filter Modeling, Ph.D. dissertation, Stanford Uni-
versity, Stanford, CA, 1983.
[Lev97] H. LEV-ARI, Displacement structure: Two related perspectives, in Communications,
Computation, Control, and Signal Processing, A. Paulraj, V. Roychowdhury, and C. D.
Schaper, eds., Kluwer, Norwell, MA, pp. 233-241, 1997.
[LG77] J. LfiRoux AND C. GUEGUEN, A fixed-point computation of parcor coefficients, 
IEEE
Trans. Acoustics Speech Signal Process., 25, pp. 257-259, 1977.
[LM85] S. Q. Li AND J. W. MARK, Performance of voice/data integration on a TDM switch,
IEEE Trans. Comm., 33, pp. 1265-1273, 1985.
[LQ87] F. T. LUK AND S. QIAO, A fast but unstable orthogonal triangularization technique for
Toeplitz matrices, Linear Algebra Appl., 88/89, pp. 495-506, 1987.
[LR93] G. LATOUCHE AND V. RAMASWAMI, A logarithmic reduction algorithm for Quasi-Birth-
Death processes, J. Appl. Probab., 30, pp. 650-674, 1993.
[LS96] G. LATOUCHE AND G. STEWART, Numerical methods for M/G/l type queues, in Com-
putations with Markov Chains, W. J. Stewart, ed., Kluwer Academic Publishers, Norwell, MA,
pp. 571-581, 1996.
[LT85] P. LANCASTER AND M. TISMENETSKI, The Theory of Matrices, Academic Press, New
York, 1985.
[MarSO] L. MARPLE, A new autoregressive spectrum analysis algorithm, IEEE Trans. Acoustics
Speech Signal Process., 28, pp. 441-454, 1980.
[MarSl] L. MARPLE, Efficient 
least squares FIR system identification, IEEE Trans. Acoustics
Speech Signal Process., 29, pp. 62-73, 1981.
[Mar82] L. MARPLE, Fast algorithms for linear prediction and system identification filters with
linear phase, IEEE Trans. Acoustics Speech Signal Process., 30, pp. 942-953, 1982.
[Mas69] J. L. MASSEY, Shift-register synthesis and BCH decoding, IEEE Trans. Inform. Theory,
15, pp. 122-127, 1969.
[McC87] P. McCuLLAGH, Tensor Methods in Statistics, Chapman and Hall, London, 1987.
[Mei97a] B. MEINI, New convergence results on functional iteration techniques for the numerical
solution of M/G/l type Markov chains, Numer. Math., 78, pp. 39-58, 1997.
[Mei97b] B. MEINI, An improved FFT-based version of Ramaswami's formula, Comm. Statist.
Stochastic Models, 13, pp. 223-238, 1997.
[Men91] J. M. MENDEL, Tutorial on higher-order statistics (spectra) in signal processing and
system theory: Theoretical results and some applications, Proc. IEEE, 79, pp. 278-305, 1991.
[Mil68] L. MILNE-THOMSON, Theoretical Hydrodynamics, 5th ed., Macmillan Press, London,
1968.
[Mor70] M. MORF, personal communication, 1970.

Bibliography 
333
[Mor74] M. MORF, Fast Algorithms for Multivariable Systems, Ph.D. thesis, Stanford Univer-
sity, Stanford, CA, 1974.
[MorSO] M. MORF, Doubling algorithms for Toeplitz and related equations, in Proc. IEEE In-
ternat. Conf. on Acoustics, Speech and Signal Process., Denver, CO, pp. 954-959, 1980.
[MSK74] M. MORF, G. S. SIDHU, AND T. KAILATH, Some new algorithms for recursive es-
timation in constant, linear, discrete-time systems, IEEE Trans. Automat. Control, 19, pp.
315-323, 1974.
[MT96] M. MIRANDA AND P. TILLI, Block Toeplitz matrices and preconditioning, Calcolo, 33,
pp. 79-86, 1996.
[Mus53] N. MUSKHELISHVILI, Some Basic Problems of the Mathematical Theory of Elasticity,
P. Noordhoff Ltd., Groningen, Holland, 1953.
[MW80] W. MILLER AND C. WRATHALL, Software for Roundoff Analysis of Matrix Algorithms,
Academic Press, New York, 1980.
[Neu89] M. F. NEUTS, Structured Stochastic Matrices of M/G/l 
Type and Their Applications,
Marcel Dekker, New York, 1989.
[Ng94] M. NG, Fast iterative methods for solving Toeplitz-plus-Hankel least squares problems,
Electron Trans. Numer. Anal., 2, pp. 154-170, 1994.
[NM93] C. NIKIAS AND J. MENDEL, Signal processing with higher-order spectra, IEEE Signal
Process. Magazine, 10, pp. 10-57, 1993.
[NP93] C. NIKIAS AND A. PETROPULU, Higher-Order Spectra Analysis. A Nonlinear Signal
Processing Framework, Prentice-Hall, Englewood Cliffs, NJ, 1993.
[NP96] M. NG AND R. PLEMMONS, Fast RLS adaptive filtering by FFT-based conjugate gradient
iterations, SIAM J. Sci. Comput., 7, pp. 920-941, 1996.
[NPT96] J. NAGY, R. PLEMMONS, AND T. TORGERSEN, Iterative image restoration using ap-
proximate inverse, IEEE Trans. Image Process., 5, pp. 1151-1162, 1996.
[Oci93] C. A. O'ClNNEiDE, Entrywise perturbation theory and error analysis for Markov chains,
Numer. Math., 65, pp. 109-120, 1993.
[Oda91] T. ODA, Moment analysis for traffic associated with Markovian queueing systems, IEEE
Trans. Comm., 39, pp. 737-745, 1991.
[Olk86] J. OLKIN, Linear and Nonlinear Deconvolution Problems, Ph.D. thesis, Rice University,
Houston, TX, 1986.
[Ost40] A. M. OSTROWSKI, Recherches sur la methode de Graeffe et les zeros des polynomes et
des series de Laurent, Acta Math., 72, pp. 99-257, 1940.
[Pai73] C. C. PAIGE, An error analysis of a method for solving matrix equations, Math. Comp.,
27, pp. 355-359, 1973.
[Pal90] D. PAL, Fast Algorithms for Structured Matrices with Arbitrary Rank Profile, Ph.D.
dissertation, Stanford University, Stanford, CA, 1990.
[Pan90] V. Y. PAN, Computations with dense structured matrices, Math. Comp., 55, pp. 179-
190, 1990.
[Pan92a] V. Y. PAN, Parallel solution of Toeplitz-like linear systems, J. Complexity, 8, pp. 1â21,
1992.
[Pan92b] V. Y. PAN, Parametrization of Newton's iteration for computation with structured
matrices and applications, Comput. Math., 24, pp. 61-75, 1992.
[Pan93a] V. Y. PAN, Concurrent iterative algorithms for Toeplitz-like linear systems, IEEE
Trans. Parallel Distributive Systems, 4, 5, pp. 592-600, 1993.
[Pan93b] V. Y. PAN, Decreasing the displacement rank of a matrix, SIAM J. Matrix Anal.
Appl., 14, pp. 118-121, 1993.

334 
Bibliography
[ParSO] B. PARLETT, The Symmetric Eigenvalue Problem, Prentice-Hall, Englewood Cliffs, NJ,
1980.
[Par86] S. V. PARTER, On the distribution of the singular values of Toeplitz matrices, Linear
Algebra Appl., 80, pp. 115-130, 1986.
[PK93] D. PAL AND T. KAILATH, Fast triangular factorization and inversion of Hermitian,
Toeplitz, and related matrices with arbitrary rank profile, SIAM J. Matrix Anal. Appl., 14,
pp. 1016-1042, 1993.
[PS91] V. Y. PAN AND R. SCHREIBER, An Improved Newton iteration for the generalized inverse
of a matrix with applications, SIAM J. Sci. Statist. Comput., 12, pp. 1109-1131, 1991.
[PZHD97] V. Y. PAN, A. L. ZHENG, X. H. HUANG, AND O. BIAS, Newton's iteration for
inversion of Cauchy-like and other structured matrices, J. Complexity, 13, pp. 108-124, 1997.
[Qia88] S. QIAO, Hybrid algorithm for fast Toeplitz orthogonalization, Numer. Math., 53,
pp. 351-366, 1988.
[Ram88] V. RAMASWAMI, A stable recursion for the steady state vector in Markov chains of
M/G/1 type, Comm. Statist. Stochastic Models, 4, pp. 183-188, 1988.
[RG94] P. A. REGALIA AND V. S. GRIGORASCU, Analytic criteria underlying the full-rank
cumulant slice problem, 10th IFAC Symp. System Identification, Copenhagen, pp. 1071-1075,
1994.
[Ris73] J. RISSANEN, Algorithms for triangular decomposition of block Hankel and Toeplitz ma-
trices with application to factoring positive matrix polynomials, Math. Comp., 27, pp. 147-154,
1973.
[RK84] S. K. RAO AND T. KAILATH, Orthogonal digital filters for VLSI implementation, IEEE
Trans. Circuit Systems, 31, pp. 933-945, 1984.
[RM89] P. A. REGALIA AND S. K. MITRA, Kronecker products, unitary matrices, and signal
processing applications, SIAM Rev., 31, pp. 586-613, 1989.
[ROF92] L. RUDIN, S. OSHER, AND E. FATEMI, Nonlinear total variation based noise removal
algorithms, Phys. D, 60, pp. 259-268, 1992.
[Saa96] Y. SAAD, Iterative Methods for Sparse Linear Systems, PWS Publishing Company,
Boston, MA, 1996.
[Sav76] J. E. SAVAGE, The Complexity of Computing, John Wiley, New York, 1976.
[Say92] A. H. SAVED, Displacement Structure in Signal Processing and Mathematics, Ph.D.
dissertation, Stanford University, Stanford, CA, 1992.
[SB95] D. R. SWEET AND R. P. BRENT, Error analysis of a fast partial pivoting method for
structured matrices, Proc. SPIE, 2563, Advanced Signal Processing Algorithms, Bellingham,
WA, pp. 266-280, 1995.
[Schl7] I. SCHUR, Uber potenzreihen die im Inneren des Einheitskreises beschrdnkt sind, J. Reine
Angew. Math., 147, pp. 205-232, 1917 (English translation in Oper. Theory Adv. Appl. 18, pp.
31-88, 1986).
[Sch33] G. SCHULTZ, Iterative berechnung der reziproken matrix, Z. Angew. Math. Mech., 13,
pp. 57-59, 1933.
[SCK94] A. H. SAVED, T. CONSTANTINESCU, AND T. KAILATH, Time-variant displacement
structure and interpolation problems, IEEE Trans. Automat. Control, 39, pp. 960-976, 1994.
[SCK95] A. H. SAVED, T. CONSTANTINESCU, AND T. KAILATH, Square-root algorithms for
structured matrices, interpolation, and completion problems, IMA Vol. Math. Appl. 69,
Springer-Verlag, New York, pp. 153-184, 1995.
[SD97b] M. STEWART AND P. VAN DOOREN, Stability issues in the factorization of structured
matrices, SIAM J. Matrix Anal. Appl., 18, pp. 104-118, 1997.

Bibliography 
335
[SeiQO] A. SEILA, Multivariate estimation of conditional performance measure in regenerative
simulation, Amer. J. Math. Management Sci., 10, pp. 17-45, 1990.
[S1183] B. SILBERMANN, On the limiting set of singular values of Toeplitz matrices, Linear Al-
gebra Appl., 182, pp. 35-43, 1983.
[SK92] A. H. SAVED AND T. KAILATH, Recursive solutions to rational interpolation problems,
Proc. IEEE Internat. Symposium on Circuits and Systems, 5, pp. 2376-2379, San Diego, 1992.
[SK94a] A. H. SAVED AND T. KAILATH, Extended Chandrasekhar recursions, IEEE Trans.
Automat. Control, 39, pp. 619-623, 1994.
[SK94b] A. H. SAVED AND T. KAILATH, A state-space approach to adaptive RLS filtering, IEEE
Signal Process. Magazine, 11, pp. 18-60, 1994.
[SK95a] A. H. SAVED AND T. KAILATH, Fast algorithms for generalized displacement structures
and lossless systems, Linear Algebra Appl., 219, pp. 49-78, 1995.
[SK95b] A. H. SAVED AND T. KAILATH, A look-ahead block Schur algorithm for Toeplitz-like
matrices, SIAM J. Matrix Anal. Appl., 16, pp. 388-413, 1995.
[SKLC94] A. H. SAVED, T. KAILATH, H. LEV-ARI, AND T. CONSTANTINESCU, Recursive solu-
tions of rational interpolation problems via fast matrix factorization, Integral Equations Oper-
ator Theory, 20, pp. 84-118, 1994.
[SLK94a] A. H. SAVED, T. KAILATH, AND H. LEV-ARI, Generalized Chandrasekhar recursions
from the generalized Schur algorithm, IEEE Trans. Automat. Control, 39, pp. 2265-2269, 1994.
[SLK94b] A. H. SAVED, H. LEV-ARI, AND T. KAILATH, Time-variant displacement structure
and triangular arrays, IEEE Trans. Signal Process., 42, pp. 1052-1062, 1994.
[SM90a] A. SWAMY AND J. M. MENDEL, ARMA parameter estimation using only output cu-
mulants, IEEE Trans. Acoustics Speech Signal Process., 38, pp. 1257-1265, 1990.
[SM90b] A. SWAMY AND J. M. MENDEL, Time and lag recursive computation of cumulants
from a state space model, IEEE Trans. Automat. Control, 35, pp. 4-17, 1990.
[SM92] A. SWAMY AND J. M. MENDEL, Identifiability of the AR parameters of an ARMA
process using only output cumulants, IEEE Trans. Automat. Control, 38, pp. 268-273, 1992.
[ST98] S. SERRA AND P. TILLI, Extreme eigenvalues of multilevel Toeplitz matrices, preprint.
[Ste73] G. W. STEWART, Introduction to Matrix Computations. Academic Press, New York,
1973.
[Ste77] G. W. STEWART, Perturbation bounds for the QR factorization of a matrix, SIAM J.
Numer. Anal., 14, pp. 509-518, 1977.
[Ste79] G. W. STEWART, The effect of rounding error on an algorithm for downdating a Cholesky
factorization, J. Inst. Math. Appl., 23, pp. 203-213, 1979.
[Ste94] W. J. STEWART, Introduction to the Numerical Solution of Markov Chains, Princeton
University Press, Princeton, NJ, 1994.
[Ste95] G. W. STEWART, On the solution of block Hessenberg systems, Numer. Linear Algebra
Appl., 2, pp. 287-296, 1995.
[Ste98] M. STEWART, Stable pivoting for the fast factorization of Cauchy-like matrices, preprint.
[Str86] G. STRANG, A proposal for Toeplitz matrix calculations, Stud. Appl. Math., 74, pp.
171-176, 1986.
[SV86] A. VAN DER SLUIS AND H. A. VAN DER VORST, The rate of convergence of conjugate
gradients, Numer. Math., 48, pp. 543-560, 1986.
[SVS83] K. SRIRAM, P. K. VARSHNEY, AND J. G. SHANTHIKUMAR, Discrete-time analysis of
integrated voice/data multiplexers with and without speech activity detectors, IEEE J. Selected
Areas Comm., 1, pp. 1124-1132, 1983.

336 
Bibliography
[Swa87] P. SWARZTRAUBER, Multiprocessor FFTs, Parallel Comput., 5, pp. 197-210, 1987.
[Swe82] D. R. SWEET, Numerical Methods for Toeplitz Matrices, Ph.D. thesis, University of
Adelaide, Adelaide, Australia, 1982.
[Swe84] D. R. SWEET, Fast Toeplitz orthogonalization, Numer. Math., 43, pp. 1-21, 1984.
[Swe93] D. R. SWEET, The use of pivoting to improve the numerical performance of algorithms
for Toeplitz matrices, SIAM J. Matrix Anal. Appl., 14, pp. 468-493, 1993.
[Sze39] G. SZEGO, Orthogonal Polynomials, Amer. Math. Soc. Colloq. Publ., 23, AMS, Provi-
dence, RI, 1939.
[TB97] L. N. TREFETHEN AND D. BAU, Numerical Linear Algebra, SIAM, Philadelphia, PA,
1997.
[TE89] M. A. TEKALP AND A. T. ERDEM, Higher-order spectrum factorization in one and two
dimensions with applications in signal modelling and nonminimum phase system identification,
IEEE Trans. Acoustics Speech Signal Process., 37, pp. 1537-1549, 1989.
[Til96] P. TILLI, Some spectral properties of non-hermitian block Toeplitz matrices, Calcolo,
1996.
[Til97a] P. TILLI, Clustering properties of eigen and singular values of block multilevel Hankel
matrices, preprint.
[Til97b] P. TlLLi, On the asymptotic spectrum of Hermitian block Toeplitz matrices with Toeplitz
blocks, Math. Comp., 66, pp. 1147-1159, 1997.
[Til98a] P. TILLI, Singular values and eigenvalues of non-Hermitian block Toeplitz matrices,
Linear Algebra Appl., 272, pp. 59-89, 1998.
[Til98b] P. TILLI, Locally Toeplitz sequences: Spectral properties and applications, Linear Alge-
bra Appl., 278, pp. 91-120, 1998.
[Til98c] P. TILLI, A note on the spectral distribution of Toeplitz matrices, Linear and Multilinear
Algebra, 45, pp. 147-159, 1998.
[TM99] P. TILLI AND M. MIRANDA, Asymptotic spectra of Hermitian block Toeplitz matrices
and preconditioning results, SIAM J. Matrix Anal. Appl., to appear.
[Tis91] M. TISMENETSKY, A decomposition of Toeplitz matrices and optimal circulant precon-
ditioning, Linear Algebra Appl., 154/156, pp. 105-121, 1991.
[TP91] A. TEKALP AND G. PAVLOVIC, Restoration of scanned photographic images, in Digital
Image Restoration, A. Katsaggelos, ed., Springer-Verlag, Berlin, 1991.
[Tre64] W. F. TRENCH, An algorithm for the inversion of finite Toeplitz matrices, J. SIAM, 12,
pp. 515-522, 1964.
[Tre86] W. F. TRENCH, Solution of systems with Toeplitz matrices generated by rational func-
tions, Linear Algebra Appl., 74, pp. 191-211, 1986.
[Tre90] L. N. TREFETHEN, Approximation theory and numerical linear algebra, in Algorithms
for Approximation II, J. Mason and M. Cox, eds., Chapman and Hall, London, 1990.
[Tuc64] L. R. TUCKER, The extension of factor analysis to three-dimensional matrices, in Con-
tributions to Mathematical Psychology, H. Gullikson and N. Frederiksen, eds., pp. 109-127,
Holt, Rinehart and Winston, New York, 1964.
[Tuc66] L. R. TUCKER, Some mathematical notes on three-mode factor analysis, Psychometrika,
31, pp. 279-311, 1966.
[Tug87] J. K. TUGNAIT, Identification of linear stochastic systems via second- and fourth-order
cumulant matching, IEEE Trans. Inform. Theory, 33, pp. 393-407, 1987.
[Tug95] J. K. TUGNAIT, Parameter estimation for noncausal ARMA models of non-Gaussian
signals via cumulant matching, IEEE Trans. Signal Process., 43, pp. 886-893, 1995.

Bibliography 
337
[Tyr91] E. E. TYRTYSHNIKOV, Cauchy-Toeplitz matrices and some applications, Linear Algebra
Appl., 149, pp. 1-18, 1991.
[Tyr92a] E. E. TYRTYSHNIKOV, Singular values of Cauchy-Toeplitz matrices, Linear Algebra
Appl., 161, pp. 99-116, 1992.
[Tyr92b] E. E. TYRTYSHNIKOV, Optimal and superoptimal circulant preconditioned, SIAM J.
Matrix Anal. Appl., 13, pp. 459-473, 1992.
[Tyr94a] E. E. TYRTYSHNIKOV, Influence of matrix operations on the distribution of eigenvalues
and singular values of Toeplitz matrices, Linear Algebra Appl., 207, pp. 225-249, 1994.
[Tyr94b] E. E. TYRTYSHNIKOV, How bad are Hankel matrices?, Numer. Math., 67, pp. 261-269,
1994.
[Tyr95] E. E. TYRTYSHNIKOV, Circulant preconditioners with unbounded inverses, Linear Alge-
bra Appl., 216, pp. 1-24, 1995.
[Tyr96a] E. E. TYRTYSHNIKOV, A unifying approach to some old and new theorems on distri-
bution and clustering, Linear Algebra Appl., 232,, pp. 1-43, 1996.
[Tyr97] E. E. TYRTYSHNIKOV, A Brief Introduction to Numerical Analysis, Birkhauser, Boston,
1997.
[TZ97] E. E. TYRTYSHNIKOV AND N. ZAMARASHKIN, Spectra of multilevel Toeplitz matrices:
Advanced theory via simple matrix relationships, to appear in Linear Algebra Appl.
[Var63] R. S. VARGA, Matrix Iterative Analysis, Prentice-Hall, Englewood Cliffs, NJ, 1963.
[Var92] J. M. VARAH, Backward error estimates for Toeplitz systems, preprint, Computer Sci-
ence Department, University of British Columbia, Vancouver, 1992.
[Var93] J. M. VARAH, The prolate matrix, Linear Algebra Appl., 187, pp. 269-278, 1993.
[Vav91] Z. VAVRIN, A unified approach to Loewner and Hankel matrices, Linear Algebra Appl.,
143, pp. 171-222, 1991.
[VC81] R. S. VARGA AND D.-Y. CAI, On the LU factorization of M-matrices, Numer. Math.,
38, pp. 179-192, 1981.
[VD94] A. J. VAN DER VEEN AND P. M. DEWILDE, On low-complexity approximation of ma-
trices, Linear Algebra Appl., 205/206, pp. 1145-1201, 1994.
[Vee93] A. J. VAN DER VEEN, Time-Varying System Theory and Computational Modeling: Real-
ization, Approximation, and Factorization. Ph.D. thesis, Delft University of Technology, Delft,
The Netherlands, 1993.
[VO96] C. VOGEL AND M. OMAN, Iterative methods for total variation denoising, SIAM J. Sci.
Comput., 17, pp. 227-238, 1996.
[WH31] N. WIENER AND E. HOPF, On a class of singular integral equations, Proc. Prussian
Acad. Math.âPhys. Ser., p. 696, 1931.
[Wid73] H. WiDOM, Toeplitz determinants with singular generating functions, Amer. J. Math.,
95, pp. 333-383, 1973.
[Wid74] H. WiDOM, Asymptotic behaviour of block Toeplitz matrices and determinants, Adv.
Math., 13, pp. 284-322, 1974.
[Wid75] H. WIDOM, On the limit of block Toeplitz determinants, Proc. Amer. Math. Soc., 50,
pp. 167-173, 1975.
[WidSO] H. WIDOM, Szego limit theorem: The higher dimensional matrix case, J. Punct. Anal.,
39, pp. 182-198, 1980.
[Wie49] N. WIENER, Extrapolation, Interpolation and Smoothing of Stationary Time Series,
with Engineering Applications, Technology Press and Wiley, New York, 1949.
[Wil61] J. H. WILKINSON, Error analysis of direct methods of matrix inversion, J. Assoc. Com-
put. Mach., 8, pp. 281-330, 1961.

338 
Bibliography
[W1163] J. H. WILKINSON, Rounding Errors in Algebraic Processes, Prentice-Hall, Englewood
Cliffs, NJ, 1963.
[Wil65] J. H. WILKINSON, The Algebraic Eigenvalue Problem, Oxford University Press, London,
1965.
[Wri91] S. WRIGHT, Parallel algorithms for banded linear systems, SIAM J. Sci. Statist. Corn-
put., 12, pp. 824-842, 1991.
[Yag91] A. YAGLE, New analogues of split algorithms for arbitrary Toeplitz-plus-Hankel matri-
ces, IEEE Trans. Signal Process., 39, pp. 2457-2463, 1991.
[YL91] J. YE AND S.-Q. Li, Analysis of multi-media traffic queues with finite buffer and overload
controlâPart I: Algorithm, in Proc. IEEE Infocom 91, Bal Harbour, FL, pp. 1464-1474, 1991.

INDEX
adaptive filtering, 53
algorithm
array, 18, 59, 62, 86
Bareiss, 5, 59, 102
CG, 118
generalized Schur, 17, 18, 62, 86,
112
hybrid Schur-Levinson, 30, 109
inversion, 37
Levinson-Durbin, 4, 102, 245
PCG, 122
Schur, 28, 102, 245
tensor Schur, 265
back substitution, 33
backward stability, 58, 85, 105
banded block Toeplitz system, 237, 239
banded matrix, 274
Bareiss algorithm, 5, 59, 102
error analysis, 59, 112
biharmonic equation, 141
Blaschke matrix, 18, 26, 69, 113
block circulant matrix, 218
block displacement operator, 223
block displacement rank, 223, 227, 228,
238
block Hessenberg form, 211, 214, 224,
225, 228
block Toeplitz matrix, 169, 173, 209,
211, 214, 216, 218, 223-225
block tridiagonal matrix, 211, 227, 231,
237, 238
boundary value problem, 156
Caratheodory function, 10
Cauchy interlace theorem, 302
Cauchy matrix, 6, 61, 102, 108, 274
Cauchy-like matrix, 8, 49, 57, 108, 109,
205
Cayley transform, 247
CG method, 116, 117, 120, 152, 153
Chandrasekhar equations, 53
Chebyshev-Vandermonde matrix, 110
Cholesky factorization, 58, 59, 63, 64,
73, 107, 112
circulant matrix, 6, 50, 122
clustered eigenvalues, 119
coding theory, 56
condition number, 92, 104
congruence, 298
Crout-Doolittle recursion, 289
cumulants, 242, 244, 248
cyclic reduction, 228, 230-232, 237, 238
Darlington synthesis, 245
deconvolution, 147
deflation, 15
digital filter, 2, 56
direct methods, 53, 116
displacement
for tensors, 254
fundamental properties, 13-15
generalized, 9
inertia, 3
rank, 2, 103, 107, 274, 277
structure, 2, 6-11, 60,157, 222, 246,
276
time-variant, 53
divided difference matrix, 10
doubling algorithm, 225
downdating, 59, 112, 113
elementary section, 23
embedding, 31, 58, 85, 114
entropy, 2
339

340
Index
equation
displacement, 7, 8, 11, 12, 18, 84,
89, 94, 270, 271
Predholm, 2
Lyapunov, 11, 259, 260, 285
Lyapunov-Stein, 288
seminormal, 102
Stein, 11
Wiener-Hopf, 2
Yule-Walker, 30, 243, 245
explicit structure, 1
factorization
block triangular, 35
modified QR, 85
QR, 32, 85, 89, 113
triangular, 2, 5, 6, 15, 58, 61, 62,
85, 86, 242, 273
/-circulant matrix, 196
FFT, 13, 217, 235, 238, 275, 291
floating point processor, 60
four-block problem, 54
Frobenius norm, 300
Gaussian elimination, 5, 16, 48, 57, 102,
105, 108, 294
generalized Hessenberg systems, 237
generalized Schur algorithm, 17-18, 58
derivation, 18-23
error analysis, 59, 64-73, 112
for tensors, 265
proper form, 25-27, 62, 86
pseudocode, 81
generating function, 120, 161
generator
growth, 75, 110
matrix, 3, 60-62, 107
nonminimal, 3
Givens rotation, 307
Gohberg-Semencul formula, 13
Gram-Schmidt procedure, 299
Hankel matrix, 1, 8, 61, 102, 274
Hankel rank, 275
Hankel-like matrix, 7, 47
Hessenberg matrix, 209
higher-order statistics, 243
Hilbert matrix, 103
homotopy technique, 196
Householder matrix
algebraic derivation, 305, 313
complex case, 305, 312
geometric derivation, 306, 313
hyperbolic case, 312
hybrid Schur-Levinson algorithm, 30, 32,
37, 109
hyperbolic rotation, 65, 113, 309
H procedure, 68
mixed downdating, 66, 311
OD procedure, 67, 311
IEEE standards, 60
ill-conditioned matrix, 96, 104
image restoration, 146
implicit structure, 1
inertia of a matrix, 298
inflation, 37
input-output model, 53
interpolation, 2, 11, 54, 245
Caratheodory, 54
Hermite-Fejer, 54
Lagrange, 55
Nevanlinna-Pick, 54
Fade, 55
polyspectral, 261
invariance property, 13
inverse scattering, 2, 55
inversion algorithm, 37
iterative methods, 53, 104, 116, 120
iterative refinement, 76
J-lossless system, 54
J-unitary matrix, 3, 87, 88, 301
Krylov subspace, 117
Levinson-Durbin algorithm, 4, 37, 102,
111, 245
error analysis, 4, 58, 112
linear convergence, 119, 190
linear phase filtering, 139
locally Toeplitz matrix, 174-185
Loewner matrix, 8
look-ahead algorithm, 104
look-ahead Schur algorithm, 34
lossless system, 55
Lyapunov condition, 282
MAC protocol, 239
machine precision, 60, 92, 96, 98, 104,
112

Index
341
Markov chain, 210-215
M/G/1 type, 211
matrix completion, 11
matrix inversion lemma, 297
matrix polynomial, 216
matrix power series, 216, 219-222, 233,
238
matrix-vector product, 12, 123
method
CG, 117
iterative, 53, 116
Newton's, 187, 188
PCG, 122
modified Gram-Schmidt, 300
multibanded matrix, 274
multilevel Toeplitz matrix, 165, 173
Nerode equivalence, 280
network traffic, 212
Newton's iteration, 187-190
Newton-Toeplitz iteration, 192-196
norm of a matrix, 300
numerical stability, 51, 57, 85, 103
backward, 52, 58, 85, 105
strong, 105
weak, 5, 104, 106
observability matrix, 281
orthogonal polynomials, 4
PCG method, 116, 122, 187
permutation, 308
Perron-Frobenius theorem, 303
perturbation analysis, 64
Pick matrix, 1, 6, 32, 44, 61
Pick tensor, 242
pivoting, 48, 57, 73
complete, 48, 57
partial, 48, 57, 105
polyspectrum, 242, 255
positive recurrence, 210
power series arithmetic, 221
preconditioner, 54, 116, 121
band-Toeplitz, 129
circulant, 124
for structured matrices, 132
Strang's, 124
T. Chan's, 126
Toeplitz-circulant, 131
probability invariant vector, 210
probability matrix, 209, 211
proper form, 25, 42, 62, 87
QBD problem, 211, 227, 231, 237
QR factorization, 32, 59, 84, 89-100,
107, 113, 299
fast stable algorithm, 114
quadratic convergence, 190
quasi-Toeplitz matrix, 59, 61, 85
queueing network, 143, 212
Ramaswami's formula, 210, 225
reachability matrix, 281
realization theory, 277
reflection coefficient, 4, 37, 103
regularization, 147
residual
correction, 188, 198
error, 4, 58, 106, 239, 240
matrix, 190
norm, 118, 122, 195
normalized, 106
vector, 106, 117
Riccati equation, 2
Riccati recursion, 53
Riemann sum, 160, 181
Riemann-Lebesgue lemma, 158
rounding error, 64, 103
Schur algorithm, 5, 28-30, 102, 104
Schur complement, 14-17, 21, 30, 31,
63, 71, 85, 88, 89, 96, 224, 229,
232, 269, 270, 294
Schur construction, 37
Schur reduction, 15, 295
second-order statistics, 242, 243, 245
seminormal equations, 114
shift invariance, 156
shift matrix, 3
shift-structured matrix, 61, 84, 89
sparse matrix, 1
spectral decomposition, 297
spectral distribution, 120, 152-156
spectral norm, 300
spectrum, 116
stable algorithm, 105
state-space structure, 52, 274
stochastic matrix, 210
structured perturbation, 103
superlinear convergence, 126

342 
Index
Sylvester equation, 11
Sylvester's law, 13, 298
Szego formula, 158, 159, 162, 163, 181
tensor, 242, 248
time-variant structure, 11, 53
Toeplitz matrix, 1, 2, 37, 61, 102, 108,
116, 274
Toeplitz-like matrix, 7, 45, 109, 116, 132,
190
Toeplitz-plus-band matrix, 116, 138
Toeplitz-plus-Hankel matrix, 102, 110,
116, 136
total variation, 148
transformation to Cauchy-like, 50
transmission line, 27
transmission zero, 24
triangular factorization, 2, 5, 6, 9, 16,
58, 61, 86, 295
Tucker product, 248
unitary matrix, 301
Vandermonde matrix, 1, 6, 7, 61, 102
Vandermonde-like matrix, 205
weak stability, 104
well-conditioned matrix, 92, 104
Wiener-Hopf technique, 5
Yule-Walker equations, 4, 243, 245


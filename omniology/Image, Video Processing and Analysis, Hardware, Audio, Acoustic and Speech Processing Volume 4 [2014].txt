22
CHAPTER
Introduction: Hardware
and Software
Ankur Srivastava
Department of ECE, University of Maryland, College Park, MD, USA
Image analysis, understanding and video processing ﬁelds have long relied on the availability of high
performance computational infrastructure to deliver the required processing needs. However, in the
last decade signiﬁcant algorithmic advances in this area has signiﬁcantly increased the computational
processing needs. Such computational needs were historically satisﬁed by advances in Moore’s law
which resulted in improvements in baseline computational efﬁciency of the general purpose com-
puter systems. Such improvements beneﬁtted the image processing and computer vision areas although
little if any effort was focused towards development of computers which were customized for such
target applications. The last few years has witnessed the gradual slowdown in Moore’s law. This
phenomenon combined with the growing computational needs complicates our ability to implement
advances in image processing and computer vision in real world systems. Another complication in this
regard is the proliferation of embedded image processing and computer vision systems which have
highly stringent cost, form factor and weight constraints. In such systems along with requirements
of high performance, battery power and energy dissipation levels are also of major concern. Hence
standard approaches which “put together” the computational platforms would not be able to deliver
the required performance under battery and energy limitations. Firstly the current set of commercially
available off the shelf (COTS) processors may not be powerful enough for the latest advanced algo-
rithms. Secondly, even if newer processors may be powerful enough, their energy footprint may not
be small enough leading us to use lower power dissipating architectures thereby impacting the perfor-
mance. If latest developments in image processing and computer vision techniques needs to be applied
to real systems, the bottlenecks imposed by the computational platforms need to be addressed. One
approach for addressing this challenge is to develop a uniﬁed cross cutting methodology for image and
vision centric systems where the algorithms, software and hardware are co-designed while consider-
ing the interdependency between power and performance constraints and the desired computational
power.
In this article we start by outlining the advantages of hardware over software and vice versa for
image and vision centric systems. This is followed by a discussion on the new challenges in hardware
oriented systems and a discussion on new developments on semiconductors that would signiﬁcantly
impact development of image and vision systems.
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00022-4
© 2014 Elsevier Ltd. All rights reserved.
629

630
CHAPTER 22 Introduction: Hardware and Software
4.22.1 Hardware and software systems
Generally speaking, in software oriented systems most if not all algorithms are implemented in software
thereby resulting in ﬂexibility, reconﬁgurability, adaptability and upgradability. However such systems
are generally more inefﬁcient in performance and energy when compared to hardware oriented systems.
Hardware oriented systems, while offer more efﬁciency, are disadvantageous when ﬂexibility is a desired
feature. Fundamentally hardware centric implementations can be designed to exploit the pipelining
and parallelism. Pipelining helps in improving the throughput while parallelism helps in reducing the
overall delay per computation. Both approaches for improving performance come at energy overhead,
however due to the customizations possible at hardware level the energy/op is still better than software
implementations. The architecture of the hardware implementation could be optimized around the
algorithmic data ﬂow which helps in improving the overall efﬁciency of the system.
Regardless of whether hardware or software is used to implement the vision centric system, several
signiﬁcant hardware oriented challenges need to be considered while developing the system. We outline
a few below (see Figure 22.1).
1. Communication over computing delay: In hardware platforms developed during early to mid-1990s,
gate delay used to be the primary concern. However since then, with each technology generation the
wire-delays have become more and more relevant. Regardless of whether we are developing hardware
or software oriented systems, communication delay as well as energy needs to be accounted for.
In multi-core CPUs, communication overheads stem from passing messages between different cores
as well as data transfer between memory and processor core. Modern multi-core CPUs have a
sophisticated “network-on-chip” NOC which enables different cores to interact. Multi-core CPUs
also have a very sophisticated memory hierarchy that allows fast data transfer between CPU and
memory. Main memory and the CPU are typically on separate dies, which signiﬁcantly slows down
their interaction. To improve memory-CPU bandwidth an elaborate memory hierarchy is put in
place which incorporates fast on-chip caches as well as sophisticated methods of data movement
Growing Importance of Wires
0
5
10
15
20
25
30
35
40
45
0
100
200
300
400
500
600
Technology
Delay(ps)
Gate Delay
Wire Delay
FIGURE 22.1
Gate delay vs. wire delay and typical CPU memory hierarchy.

4.22.2 New Developments: 3D Integration
631
between main memory and caches for improving the data availability for processing. However, the
programming style may also enable better use of the memory hierarchy by creating scenarios for
improved exploitation of spatial and temporal locality of data.
2. Static and dynamic power dissipation: Two important sources of power dissipation in modern chips
are dynamic and static power. Dynamic power dissipation comes from the switching activity in the
transistors which is a strong function of the computational activity and the nature of data. Static power
dissipation comes from the leakage current associated with nano-scale CMOS devices. This leakage
current is created by a complex set of physical phenomena (sub-threshold effects, gate leakage, band
to band tunneling). In recent technologies, leakage power has become a signiﬁcant component of
chip power dissipation (around 50% according to some estimates). Hence simply reducing switching
activity will not be enough to reduce system power dissipation levels anymore.
3. Temperature: Temperature has always been a major challenge in hardware systems, however in recent
years temperature has become a signiﬁcant concern. Temperature affects the performance, reliability
as well as power dissipation levels in complex ways. Higher temperature increase circuit delay. For
example in CMOS technology, circuit delay generally increases by about 25–30% as temperature
increases from 25 to 85 ◦C. Static leakage power is somewhat exponentially dependent on-chip
temperatures we well. Hence higher temperatures actually increase the chip power dissipation.
Finally temperature also impacts the reliability and lifetime of the system. Image and vision centric
algorithms are generally heavy on computational workload. Hence thermal issues in such systems
need to be actively considered.
4.22.2 New developments: 3D integration
The three dimensional integrated circuit (3D-IC) contains two or more layers of active electronic com-
ponents that are stacked vertically. Stacking of conventional 2D chips helps improve the inter-chip
bandwidth resulting in faster data exchange. This helps in signiﬁcantly improving the performance
of the overall system. 3D integration can also result in overall system energy savings (due to reduced
inter-chip communication overheads), increased integration densities (leading to improved performance
and functionality) and co-integration of heterogeneous components. 3D integration is being touted as a
signiﬁcant approach to counter the slowing of Moore’s law. The improvements in performance as well
as heterogeneity (which can enable co-integration of image sensors and processors for example) can
have a signiﬁcant impact on future computer vision systems.
Figure 22.2 shows a four-tiered stacked 3D-IC. In the 3D-IC, each active layer contains functional
units such as processor cores and memories, or heterogeneous devices such as analog RF circuits,
sensors, etc. Through-silicon-vias (TSVs) are inserted in the 3D-IC to deliver signal/power/ground
among different tiers as well as enabling communication between devices in different layers.
There are different manufacturing technologies for stacked 3D-ICs:
1. Wafer on wafer. The electronic components are ﬁrstly built on two or more wafers. The wafers are
then bounded together.
2. Die on wafer. The electronic components are built on two different wafers. One wafer is diced and
then stacked on the other wafer.
3. Die on die. The electronic components are built on multiple dies, these dies are then bonded together.

632
CHAPTER 22 Introduction: Hardware and Software
FIGURE 22.2
3D-ICs.
Despite the advantages, the 3D-IC also brings forth new challenges:
1. Design challenges: The third dimension brings forth an additional control variable during the design
of the electronic system. Conventional design tools are geared towards 2D technology. New EDA
tools for 3D-ICs are necessary.
2. Thermal issues: In 3D-ICs, since several layers of electronic components that dissipate power are
stacked vertically, the power density is usually higher than 2D-ICs, leading to potential thermal
issues. Moreover, the thermal conductivity of oxide layer (which is in between silicon layers) is
low and hence would reduce the heat transfer towards the ambient. This exacerbates the thermal
problems in 3D-ICs. New cooling solutions for 3D-ICs may be necessary.
3. TSV induced overheads: 3D-ICs incorporate thousands of TSVs for interlayer communication as
well as delivery of power/ground. These TSVs cause extra area overhead. TSVs also induce thermal-
mechanical stress due to the thermal expansion mismatch between silicon and TSV-ﬁlling material.
The thermal stress causes potential reliability problems, such as cracking, and also timing violations
since transistor delay will be inﬂuenced by thermal stress.
4. Cross talk between layers: For example, coupling might occur between the top layer metal wires
and the device on the active layer above it. Furthermore, in heterogeneous integration, the RF signal
might inﬂuence the logic and memory in other layers.
Overall, 3D integration is a signiﬁcant development which can have a major impact on how future
electronic systems are designed and used.

Academic Press is an imprint of Elsevier
The Boulevard, Langford Lane, Kidlington, Oxford OX5 1GB, UK
225 Wyman Street, Waltham, MA 02451, USA
First edition 2014
Copyright © 2014 Elsevier Ltd. All rights reserved.
No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any 
means electronic, mechanical, photocopying, recording or otherwise without the prior written permission of the 
publisher.
Permissions may be sought directly from Elsevier’s Science & Technology Rights Department in Oxford, UK: 
phone (+44) (0) 1865 843830; fax (+44) (0) 1865 853333; email: permissions@elsevier.com. Alternatively you 
can submit your request online by visiting the Elsevier web site at http://elsevier.com/locate/permissions, and 
selecting Obtaining permission to use Elsevier material.
Notice
No responsibility is assumed by the publisher for any injury and/or damage to persons or property as a matter of 
products liability, negligence or otherwise, or from any use or operation of any methods, products, instructions or 
ideas contained in the material herein. Because of rapid advances in the medical sciences, in particular, indepen-
dent verification of diagnoses and drug dosages should be made.
Library of Congress Cataloging in Publication Data
A catalog record for this book is available from the Library of Congress
British Library Cataloguing in Publication Data
A catalogue record for this book is available from the British Library
ISBN: 978-0-12-396501-1
For information on all Elsevier publications
visit our website at www.store.elsevier.com
Printed and bound in Poland.
14  15  16  17  10  9  8  7  6  5  4  3  2  1

xxvii
Signal Processing at Your Fingertips!
Let us flash back to the 1970s when the editors-in-chief of this e-reference were graduate students. 
One of the time-honored traditions then was to visit the libraries several times a week to keep track of 
the latest research findings. After your advisor and teachers, the librarians were your best friends. We 
visited the engineering and mathematics libraries of our Universities every Friday afternoon and poured 
over the IEEE Transactions, Annals of Statistics, the Journal of Royal Statistical Society, Biometrika, 
and other journals so that we could keep track of the recent results published in these journals. Another 
ritual that was part of these outings was to take sufficient number of coins so that papers of interest 
could be xeroxed. As there was no Internet, one would often request copies of reprints from authors 
by mailing postcards and most authors would oblige. Our generation maintained thick folders of hard-
copies of papers. Prof. Azriel Rosenfeld (one of RC’s mentors) maintained a library of over 30,000 
papers going back to the early 1950s!
Another fact to recall is that in the absence of Internet, research results were not so widely dis-
seminated then and even if they were, there was a delay between when the results were published in 
technologically advanced western countries and when these results were known to scientists in third 
world countries. For example, till the late 1990s, scientists in US and most countries in Europe had a 
lead time of at least a year to 18 months since it took that much time for papers to appear in journals 
after submission. Add to this the time it took for the Transactions to go by surface mails to various 
libraries in the world. Scientists who lived and worked in the more prosperous countries were aware of 
the progress in their fields by visiting each other or attending conferences.
Let us race back to 21st century! We live and experience a world which is fast changing with rates 
unseen before in the human history. The era of Information and Knowledge societies had an impact on 
all aspects of our social as well as personal lives. In many ways, it has changed the way we experience 
and understand the world around us; that is, the way we learn. Such a change is much more obvious to 
the younger generation, which carries much less momentum from the past, compared to us, the older 
generation. A generation which has grew up in the Internet age, the age of Images and Video games, the 
age of IPAD and Kindle, the age of the fast exchange of information. These new technologies comprise 
a part of their “real” world, and Education and Learning can no more ignore this reality. Although many 
questions are still open for discussions among sociologists, one thing is certain. Electronic publishing 
and dissemination, embodying new technologies, is here to stay. This is the only way that effective 
pedagogic tools can be developed and used to assist the learning process from now on. Many kids in the 
early school or even preschool years have their own IPADs to access information in the Internet. When 
they grow up to study engineering, science, or medicine or law, we doubt if they ever will visit a library 
as they would by then expect all information to be available at their fingertips, literally!
Another consequence of this development is the leveling of the playing field. Many institutions in 
lesser developed countries could not afford to buy the IEEE Transactions and other journals of repute. 
Even if they did, given the time between submission and publication of papers in journals and the time 
it took for the Transactions to be sent over surface mails, scientists and engineers in lesser developed 
countries were behind by two years or so. Also, most libraries did not acquire the proceedings of confer-
ences and so there was a huge gap in the awareness of what was going on in technologically advanced 
Introduction

xxviii
Introduction
countries. The lucky few who could visit US and some countries in Europe were able to keep up with 
the progress in these countries. This has changed. Anyone with an Internet connection can request or 
download papers from the sites of scientists. Thus there is a leveling of the playing field which will lead 
to more scientist and engineers being groomed all over the world.
The aim of Online Reference for Signal Processing project is to implement such a vision. We all 
know that asking any of our students to search for information, the first step for him/her will be to click 
on the web and possibly in the Wikipedia. This was the inspiration for our project. To develop a site, 
related to the Signal Processing, where a selected set of reviewed articles will become available at a 
first “click.” However, these articles are fully refereed and written by experts in the respected topic. 
Moreover, the authors will have the “luxury” to update their articles regularly, so that to keep up with 
the advances that take place as time evolves. This will have a double benefit. Such articles, besides the 
more classical material, will also convey the most recent results providing the students/researchers with 
up-to-date information. In addition, the authors will have the chance of making their article a more 
“permanent” source of reference, that keeps up its freshness in spite of the passing time.
The other major advantage is that authors have the chance to provide, alongside their chapters, any 
multimedia tool in order to clarify concepts as well as to demonstrate more vividly the performance of 
various methods, in addition to the static figures and tables. Such tools can be updated at the author’s 
will, building upon previous experience and comments. We do hope that, in future editions, this aspect 
of this project will be further enriched and strengthened.
In the previously stated context, the Online Reference in Signal Processing provides a revolutionary 
way of accessing, updating and interacting with online content. In particular, the Online Reference will 
be a living, highly structured, and searchable peer-reviewed electronic reference in signal/image/video 
Processing and related applications, using existing books and newly commissioned content, which 
gives tutorial overviews of the latest technologies and research, key equations, algorithms, applications, 
standards, code, core principles, and links to key Elsevier journal articles and abstracts of non-Elsevier 
journals.
The audience of the Online Reference in Signal Processing is intended to include practicing engi-
neers in signal/image processing and applications, researchers, PhD students, post Docs, consultants, 
and policy makers in governments. In particular, the readers can be benefited in the following needs:
• 
To learn about new areas outside their own expertise.
• 
To understand how their area of research is connected to other areas outside their expertise.
• 
To learn how different areas are interconnected and impact on each other: the need for a 
“helicopter” perspective that shows the “wood for the trees.”
• 
To keep up-to-date with new technologies as they develop: what they are about, what is their 
potential, what are the research issues that need to be resolved, and how can they be used.
• 
To find the best and most appropriate journal papers and keeping up-to-date with the newest, best 
papers as they are written.
• 
To link principles to the new technologies.
The Signal Processing topics have been divided into a number of subtopics, which have also dic-
tated the way the different articles have been compiled together. Each one of the subtopics has been 
coordinated by an AE (Associate Editor). In particular:

xxix
Introduction
	 1.	 Signal Processing Theory (Prof. P. Diniz)
	 2.	 Machine Learning (Prof. J. Suykens)
	 3.	 DSP for Communications (Prof. N. Sidiropulos)
	 4.	 Radar Signal Processing (Prof. F. Gini)
	 5.	 Statistical SP (Prof. A. Zoubir)
	 6.	 Array Signal Processing (Prof. M. Viberg)
	 7.	 Image Enhancement and Restoration (Prof. H. J. Trussell)
	 8.	 Image Analysis and Recognition (Prof. Anuj Srivastava)
	 9.	 Video Processing (other than compression), Tracking, Super Resolution, Motion Estimation, 
etc. (Prof. A. R. Chowdhury)
10.	 Hardware and Software for Signal Processing Applications (Prof. Ankur Srivastava)
11.	 Speech Processing/Audio Processing (Prof. P. Naylor)
12.	 Still Image Compression
13.		 Video Compression
We would like to thank all the Associate Editors for all the time and effort in inviting authors as well  
as coordinating the reviewing process. The Associate Editors have also provided succinct summaries  
of their areas.
The articles included in the current editions comprise the first phase of the project. In the second 
phase, besides the updates of the current articles, more articles will be included to further enrich the 
existing number of topics. Also, we envisage that, in the future editions, besides the scientific articles 
we are going to be able to include articles of historical value. Signal Processing has now reached an age 
that its history has to be traced back and written.
Last but not least, we would like to thank all the authors for their effort to contribute in this new 
and exciting project. We earnestly hope that in the area of Signal Processing, this reference will help 
level the playing field by highlighting the research progress made in a timely and accessible manner to 
anyone who has access to the Internet. With this effort the next breakthrough advances may be coming 
from all around the world.
The companion site for this work: http://booksite.elsevier.com/9780124166165 includes multimedia 
files (Video/Audio) and MATLAB codes for selected chapters.
Rama Chellappa
Sergios Theodoridis

xxxv
CHAPTER 2
Bruce H. Pillman is a senior image scientist with ITT Exelis Geospatial 
Systems, working on leading edge imaging and information systems. Prior 
to taking a position with Exelis, he was at the Kodak Research Laboratories 
as a senior principal scientist, working on still and video imaging. Main top-
ics of his work included measurement, modeling, and psychometric analysis 
of still and video imagery, automatic scene classification, psychometric still 
image and video quality modeling, and flexible image capture in the pres-
ence of motion. He has also been involved in the research and development 
of chemical processes and process control systems, film scanning and image 
enhancement, and digital camera image processing, including camera calibration, sensor artifact 
correction, noise reduction, automatic white balance, and overall image processing chain design. 
He holds 35 patents, mostly related to digital camera technology, and has written several articles 
and book chapters. He has a B.S. in Chemical Engineering (1982) from Northwestern University 
in Illinois, and an M.S. in Electrical Engineering (1992) from the University of Rochester, NY.
James E. Adams, Jr. is a senior principal scientist at the Kodak Research 
Laboratories. He holds 49 patents in the field of digital image processing, most 
notably in the fields of color filter array interpolation and noise reduction. He 
is a Kodak Distinguished Inventor, winner of the Eastman Innovation Award, 
and the 2005 Rochester Inventor of the Year. He has authored several techni-
cal articles and book chapters on all aspects of digital camera image process-
ing. A number of his publications are considered seminal works and have been 
cited hundreds of times. He has a B.S. in Physics (1979) from Monmouth 
College, NJ, and an M.S. in Optics (1983) from the University of Rochester, 
NY. Since joining Eastman Kodak Company in 1979, he has been involved in the research and 
design of innovative optical test fixtures, satellite-based laser communication systems as part of 
the Strategic Defense Initiative, new optical radiation measurement devices for decoupling the 
effects of fluorescence from other material optical properties, and most aspects of digital camera 
image processing, mostly notable color filter array design and interpolation, noise reduction, auto-
matic white balance, and overall image processing chain design. He is currently working in the 
areas of video refocusing, super-resolution, and other computational video photography topics.
CHAPTER 3
Kathrin Berkner leads the Computation Optics & Visual Processing Group at the California 
Innovation Center of Ricoh Innovations where she is responsible for creating innovative tech-
nology for computational imaging systems for computer and human vision applications, starting 
from digital-optical co-design of optics and signal processing algorithms via prototyping to trans-
ferring the innovative concepts into Ricoh’s products.
Authors Biography

xxxvi
Authors Biography
Previously, she was a Senior Research Scientist at Ricoh Innovations, 
conducting research in a variety of imaging areas related to restoration and 
enhancement of images, reformatting of documents, and adaptation of docu-
ments to small-size displays. Her wavelet-based document enhancement 
technology for Ricoh’s MFP products was awarded the Ricoh Minori Award 
for outstanding creativity.
Prior to joining Ricoh in 1998, she was a Postdoctoral Researcher at Rice 
University, Houston, TX, performing research on wavelets in collaboration 
with the Rice DSP group. She holds a Ph.D. degree in Mathematics from the 
University of Bremen, Germany. She has been serving on program committees and as a reviewer 
of several IS&T/SPIE, IEEE, ACM, and OSA conferences and journals. She has authored more 
than 20 technical publications, and has over 30 patents granted or pending.
Her interests include soccer, team handball, and playing violin in the Redwood Symphony 
orchestra.
CHAPTER 4
Oscar C. Au received his B.A.Sc. from University of Toronto in 1986, his 
M.A. and Ph.D. from Princeton University in 1988 and 1991, respectively. 
After being a postdoctoral researcher in Princeton University for one year, he 
joined the Hong Kong University of Science and Technology (HKUST) as an 
Assistant Professor in 1992. He is/has been a Professor of the Department of 
Electronic and Computer Engineering, Director of Multimedia Technology 
Research Center (MTrec), and Director of the Computer Engineering 
(CPEG) Program in HKUST.
His main research contributions are on video and image coding and pro-
cessing, watermarking and light weight encryption, speech and audio processing. Research top-
ics include fast motion estimation for MPEG-1/2/4, H.261/3/4 and AVS, optimal and fast 
suboptimal rate control, mode decision, transcoding, denoising, deinterlacing, post-processing, 
multiview coding, scalable video coding, distributed video coding, subpixel rendering, JPEG/
JPEG2000, HDR imaging, compressive sensing, halftone image data hiding, GPU-processing, 
software-hardware co-design, etc. He has published about 320 technical journals and confer-
ence papers. His fast motion estimation algorithms were accepted into the ISO/IEC 14496-7 
MPEG-4 international video coding standard and the China AVS-M standard. His light-weight 
encryption and error resilience algorithms are accepted into the China AVS standard. He has 8 
US patents and is applying for 60+ more on his signal processing techniques. He has performed 
forensic investigation and stood as an expert witness in the Hong Kong courts many times.
He is an active senior member of the Institute of Electrical and Electronic Engineering 
(IEEE) and is a Board of Governor member of the Asia Pacific Signal and Information Processing 
Association (APSIPA). He is/was Associate Editors of IEEE Transactions on Circuits and Systems 
for Video Technology (TCSVT), IEEE Transactions on Image Processing (TIP), and IEEE 

xxxvii
Authors Biography
Transactions on Circuits and Systems, Part 1 (TCAS1). He is on the Editorial Boards of Journal 
of Signal Processing Systems, Journal of Multimedia, and Journal of Franklin Institute. He is/
was Chair of CAS Technical Committee on Multimedia Systems and Applications (MSATC), 
Vice Chair of SP TC on Multimedia Signal Processing (MMSP), and a member of CAS TC on 
Video Signal Processing and Communications (VSPC), CAS TC on DSP, and SP TC on Image, 
Video and Multidimensional Signal Processing (IVMSP). He served on the Steering Committee 
of IEEE Transactions on Multimedia (TMM), and IEEE International Conference of Multimedia 
and Expo (ICME). He also served on the organizing committee of IEEE Int. Symposium on 
Circuits and Systems (ISCAS) in 1997, IEEE International Conference on Acoustics, Speech and 
Signal Processing (ICASSP) in 2003, the ISO/IEC MPEG 71st Meeting in 2005, International 
Conference on Image Processing (ICIP) in 2010, and other conferences. He was General Chair 
of Pacific-Rim Conference on Multimedia (PCM) in 2007, IEEE International Conference on 
Multimedia and Expo (ICME) in 2010 and Packet Video Workshop (PV) in 2010. He won best 
paper awards in SiPS 2007 and PCM 2007. He is an IEEE Distinguished Lecturer (DLP) in 2009 
and 2010, and has been keynote speaker for a few times.
Lu Fang received the B.S. degree in 2007 from the Department of Electronic 
Engineering and Information Science, University of Science and Technology 
of China (USTC), Hefei, China. She received Ph.D. degree in 2011 from 
the Department of Electronic and Computer Engineering, Hong Kong 
University of Science and Technology (HKUST), Hong Kong. She used to 
visit Northwestern University under the support of Professor Aggelos K. 
Katsaggelos in 2012. From 2011 to 2012, she was the post-doc research fel-
low in HKUST and Singapore University of Technology and Design (SUTD), 
respectively. She is currently an Associate Professor in University of Science 
and Technology of China (USTC), with research interests in Multimedia Processing, 3D Video 
Analysis and Coding, Machine Learning, etc.
CHAPTER 5
Philipp Urban has been Head of an Emmy-Noether research group at the 
Technische Universitat Darmstadt (Germany) since 2009. His research 
focuses on color science and spectral imaging. From 2006 to 2008 he was 
a visiting scientist at the RIT Munsell Color Science Laboratory. He holds a 
M.S. in Mathematics from the University of Hamburg and a Ph.D. from the 
Hamburg University of Technology (Germany).

xxxviii Authors Biography
Simon Stahl received his diploma degree in Mechanical Engineering at the 
Technische Universität Darmstadt in 2009. Since then he is working as 
a Ph.D. student at the Institute of Printing Science and Technology of TU 
Darmstadt. He is a part of the functional printing team, with research topics 
of material characterization and gravure printing.
Edgar Dorsam has been full Professor and Director of the Institute of Printing 
Science and Technology at the Technische Universität Darmstadt, Germany 
since 2003. From 1994 to 2003 he was responsible for research and devel-
opment at MAN Roland AG in O®enbach, Germany. He has more than 30 
patents and is member of IS&T, VDD, VDI, and OE-A. He holds a M.S. 
and a doctoral degree in Mechanical Engineering from Technische Universität 
Darmstadt.
CHAPTER 6
Stanley J. Reeves received the PhD in 1990 from the Georgia Institute of 
Technology.  He is a professor in the Department of Electrical and Computer 
Engineering at Auburn University.  He has served as associate editor for IEEE 
Transactions on Image Processing, IEEE Transactions on Image Processing, and 
IEEE Signal Processing Letters.  He has also served on the Signal Processing 
Theory and Methods Technical Committee of the IEEE Signal Processing 
Society.  His interests include digital signal processing, image restoration and 
reconstruction, optimal image acquisition, medical imaging, and color and 
spectral imaging.
CHAPTER 7
Sebastian Berisha is a Ph.D. student in Computer Science at Emory University. He received 
his M.S. degree in Computer Science from Wake Forest University in 2009, and a B.S. degree 
from Averett University in 2006. His research interests include numerical linear algebra, image 
processing, and high performance computing.
James G. Nagy is a Professor of Mathematics and Computer Science at Emory University. He 
received his Ph.D. in Applied Mathematics from North Carolina State University in 1991. Before 
joining Emory University in 1999 he had postdoctoral research fellowships with the IMA at the 

xxxix
Authors Biography
University of Minnesota, with the NSF at the University of Maryland, and was on the faculty 
at Southern Methodist University. His research interests include numerical linear algebra, struc-
tured matrix computations, and numerical solution of inverse problems in image processing. In 
addition to his many journal publications, he is co-author with Per Christian Hansen and Dianne 
O’Leary of the book “Image Deblurring: Matrices, Spectra and Filtering,” published by SIAM.
CHAPTER 8
Xin Li received the B.S. degree with highest honors in electronic engineering 
and information science from University of Science and Technology of China, 
Hefei, in 1996, and the Ph.D. degree in Electrical Engineering from Princeton 
University, Princeton, NJ, in 2000. He was a Member of Technical Staff with 
Sharp Laboratories of America, Camas, WA from August 2000 to December 
2002. Since January 2003, he has been a faculty member in Lane Department 
of Computer Science and Electrical Engineering. His research interests include 
image/video coding and processing. He received a Best Student Paper Award at 
the Conference of Visual Communications and Image Processing as the junior 
author in 2001, a Runner-up prize of Best Student Paper Award at IEEE Asilomar conference 
on Signals, Systems and Computers as the senior author in 2006, and a Best Paper Award at the 
Conference of Visual Communications and Image Processing as the single author in 2010. He is 
currently serving as a member of Image, Video and Multidimensional Signal Processing (IVMSP) 
Technical Committee and an Associate Editor for IEEE Transactions on Image Processing.
CHAPTER 10
Adrian Barbu received his B.S. degree from University of Bucharest, Romania, 
in 1995, a Ph.D. in Mathematics from Ohio State University in 2000, and a 
Ph.D. in Computer Science from UCLA in 2005.
From 2005 to 2007 he was a research scientist and later project manager 
in Siemens Corporate Research, working in medical imaging.
He received the 2011 Thomas A. Edison Patent Award with his co-authors 
for their work on Marginal Space Learning.
In 2007 he joined the Statistics department at Florida State University as 
Assistant Professor and was promoted to Associate Professor in August 2013. 
His research interests are in computer vision, machine learning, and medical imaging.
CHAPTER 11
Xavier Descombes is a senior researcher at INRIA, France. He is Head of the Morpheme Group, 
a joint team between INRIA Sophia Antipolis Mediterranee, I3S and iBV. He obtained a Master 
in Mathematics from Paris VI University in 1989, a Ph.D. in Computer Science from Telecom 
Paris in 1993, and a HDR from Nice University in 2004. His research topics include stochastic 

xl
Authors Biography
modeling in image processing. He is currently involved in biological imaging. 
He is author or co-author of more than 100 papers. He has obtained the “prix 
de la recherche” in human health in 2008 and has edited a book on stochastic 
geometry in image analysis.
CHAPTER 12
Roman Filipovych conducted this work in the Section of Biomedical Image 
analysis at the University of Pennsylvania. His research interests include mul-
tivariate and multimodal analysis of aging and psychiatric disorders. He is also 
interested in solving various computer vision problems, including detection of 
activities in video and object recognition.
Bilwaj Gaonkar is currently a graduate student at the University of Pennsylvania 
at the section for biomedical image analysis. He works with Dr. Christos 
Davatzikos on developing methods to detect population wide patterns of defi-
cit in various neurological diseases using medical.
Christos Davatzikos is a Professor of Radiology at the University of Pennsylvania 
and Director of the Center for Biomedical Image Computing and Analytics. 
He holds a secondary appointment in Electrical and Systems Engineering at 
Penn as well as at the Bioengineering an Applied Mathematics graduate groups. 
He obtained his undergraduate degree by the National Technical University 
of Athens, Greece in 1989, and his Ph.D. degree from Johns Hopkins, in 
1994, on a Fulbright scholarship. He then joined the faculty in Radiology and 
later in Computer Science, where he founded and directed the Neuroimaging 
Laboratory. In 2002 he moved to Penn, where he founded and directed the 
section of biomedical image analysis. His interests are in medical image analysis. He oversees a 
diverse research program ranging from basic problems of computational anatomy, registration, 
and segmentation, to a variety of clinical studies of Alzheimer’s, schizophrenia, autism, brain and 
prostate cancer, and brain development in mouse models. He has served on a variety of scientific 

xli
Authors Biography
journal editorial boards and grant review committees. imaging. His other interests involve the 
application of advanced machine learning methods to problems in medical image segmentation 
and the development of imaging and genetics based biomarkers for various neurological diseases.
CHAPTER 14
Andrew Floren is a member of the Laboratory for Image and Video Engineering 
(LIVE) and the High-Resolution Brain Imaging Laboratory (HIRBIL) at The 
University of Texas at Austin. He received his B.S. and M.S. in Electrical 
Engineering from the University of Texas at Austin in 2010 and 2012, 
respectively, and is currently pursuing a Ph.D. He is a member of IEEE, 
the Organization for Human Brain Mapping (OHBM), and the Society for 
Neuroscience (SFN). His research interests include computer vision, machine 
learning, and neuroimaging.
Alan C. Bovik is the Curry/Cullen Trust Endowed Chair Professor at The 
University of Texas at Austin, where he is Director of the Laboratory for Image 
and Video Engineering (LIVE). He is a faculty member in the Department of 
Electrical and Computer Engineering and the Center for Perceptual Systems 
in the Institute for Neuroscience. His research interests include image and 
video processing, computational vision, and visual perception. He has pub-
lished more than 650 technical articles in these areas and holds two US pat-
ents. His several books include the recent companion volumes The Essential 
Guides to Image and Video Processing (Academic Press, 2009).
He was named recipient of the Honorary Member Award of the Society for Imaging Science 
and Technology for 2013, received the SPIE Technology Achievement Award for 2012, and was 
the IS&T/SPIE Imaging Scientist of the Year for 2011. He has also received a number of major 
awards from the IEEE Signal Processing Society, including: the Best Paper Award (2009); the 
Education Award (2007); the Technical Achievement Award (2005), and the Meritorious Service 
Award (1998). He received the Hocott Award for Distinguished Engineering Research at the 
University of Texas at Austin, the Distinguished Alumni Award from the University of Illinois at 
Champaign-Urbana (2008), the IEEE Third Millennium Medal (2000), and two journal paper 
awards from the international Pattern Recognition Society (1988 and 1993). He is a Fellow of 
the IEEE, a Fellow of the Optical Society of America (OSA), a Fellow of the Society of Photo-
Optical and Instrumentation Engineers (SPIE), and a Fellow of the American Institute of Medical 
and Biomedical Engineering (AIMBE). He has been involved in numerous professional society 
activities, including: Board of Governors, IEEE Signal Processing Society, 1996–1998; co-founder 
and Editor-in-Chief, IEEE Transactions on Image Processing, 1996–2002; Editorial Board, The 
Proceedings of the IEEE, 1998–2004; Series Editor for Image, Video, and Multimedia Processing, 
Morgan and Claypool Publishing Company, 2003-present; and Founding General Chairman, First 
IEEE International Conference on Image Processing, held in Austin, Texas, in November, 1994.

xlii
Authors Biography
He is a registered Professional Engineer in the State of Texas and is a frequent consultant to 
legal, industrial and academic institutions.
CHAPTER 15
Andres Rodriguez is a research scientist at the Air Force Research Laboratory 
Sensors Directorate. He received his Ph.D. in Electrical and Computer 
Engineering at Carnegie Mellon University in 2012. His research interests 
are in Pattern Recognition and Machine Learning. He received a B.S. and 
M.S. degree in Electrical Engineering from Brigham Young University in 2006 
and 2008, respectively, while working at BYU’s MAGICC laboratory. He was 
awarded the Carnegie Institute of Technology Dean’s Tuition Fellowship in 
2008 and the Frank J. Marshall Graduate Fellowship in 2009. He has pub-
lished over a dozen technical papers and received the Best Student Paper 
award at the SPIE ATR Conference in 2010.
B.V.K. Vijaya Kumar (Professor Kumar) is a Professor of Electrical and Computer 
Engineering and the Associate Dean for Graduate and Faculty Affairs of 
College of Engineering at Carnegie Mellon University. His research interests 
include Pattern Recognition, Biometrics and Coding and Signal Processing for 
Data Storage Systems. His publications include the book entitled Correlation 
Pattern Recognition, 15 book chapters, and more than 500 technical papers. 
He served as a Pattern Recognition Topical Editor for Applied Optics and as an 
Associate Editor for IEEE Transactions on Information Forensics and Security. 
He serves on many biometrics and data storage conference program commit-
tees and was a co-general chair of the 2004 Optical Data Storage conference, a co-chair of the 
2008, 2009 and 2010 SPIE conferences on Biometric Technology for Human Identification, and 
is a co-chair of the 2012 Biometrics: Theory, Applications and Systems (BTAS) conference. He is 
a Fellow of IEEE, a Fellow of SPIE, a Fellow of Optical Society of America, and a Fellow of the 
International Association of Pattern Recognition. He serves on the IEEE Biometric Council and 
was a former member of IEEE Signal processing Society’s Technical Committee on Information 
Forensics and Security. He received the 2003 Eta Kappa Nu award for Excellence in Teaching 
in the ECE Department at CMU and the 2009 Carnegie Institute of Technology’s Outstanding 
Faculty Research Award (jointly with Professor Marios Savvides).
CHAPTER 16
Gianfranco Doretto received the D.Eng. degree in Electronics Engineering 
(with highest honors) from the University of Padua, Italy, in 1998, the M.S. 
and Ph.D. degrees in Computer Science from the University of California, 
Los Angeles, in 2002 and 2005, respectively. From 2005 until 2010 he was a 
lead scientist and a project manager at General Electric Global Research, and 
he is currently an Assistant Professor in the Lane Department of Computer 

xliii
Authors Biography
Science and Electrical Engineering of West Virginia University. His research interests span several 
computer vision areas with a focus on statistical modeling for video analysis. He is a member of 
the IEEE.
Avinash Ravichandran received the B.E. degree in Electronics and Communication 
Engineering from the University of Madras in 2003, the Master’s degree in 
Electrical and Computer engineering in 2007, the Master’s degree in Applied 
Mathematics and Statistics in 2009, and the Ph.D. degree in Electrical and 
Computer Engineering in 2010 from The Johns Hopkins University. Currently, 
he is working as a postdoctoral fellow with the Vision Lab at the University 
of California, Los Angeles. His research interests include analysis of temporal 
events, activities, and dynamic textures. He is a member of the IEEE.
René Vidal received the B.S. degree in Electrical Engineering (highest hon-
ors) from the Pontificia Universidad Catolica de Chile in 1997 and the M.S. 
and Ph.D. degrees in Electrical Engineering and Computer Sciences from the 
University of California, Berkeley, in 2000 and 2003, respectively. He was a 
research fellow at the National ICT Australia in the Fall of 2003 and currently 
is an Associate Professor in the Department of Biomedical Engineering at The 
Johns Hopkins University. He has coauthored more than 150 articles in bio-
medical image analysis, computer vision, machine learning, hybrid systems, 
and robotics. He is a recipient of the 2012 J.K. Aggarwal Prize “for outstand-
ing contributions to generalized principal component analysis (GPCA) and subspace clustering 
in computer vision and pattern recognition,” the 2012 Best Paper Award in Medical Robotics 
and Computer Assisted Interventions, 2011 Best Paper Award Finalist at the IEEE Conference 
on Decision and Control, the 2009 ONR Young Investigator Award, the 2009 Sloan Research 
Fellowship, the 2005 NFS CAREER Award, and the 2004 Best Paper Award Honorable Mention 
at the European Conference on Computer Vision. He also received the 2004 Sakrison Memorial 
Prize for completing an exceptionally documented piece of research, the 2003 Eli Jury award 
for outstanding achievement in the area of systems, communications, control, or signal process-
ing, the 2002 Student Continuation Award from NASA Ames, the 1998 Marcos Orrego Puelma 
Award from the Institute of Engineers of Chile, and the 1997 Award of the School of Engineering 
of the Pontificia Universidad Catolica de Chile to the best graduating student of the school. He 
is a senior member of the IEEE and the ACM.
Stefano Soatto received the D.Eng. degree (highest honors) from the University 
of Padua, Italy, in 1992 and the Ph.D. degree in Control and Dynamical 
Systems from the California Institute of Technology in 1996. He joined the 
University of California Los Angeles (UCLA) in 2000 after being an assis-
tant and then an Associate Professor of Electrical and Biomedical Engineering 
at Washington University and a Research Associate in Applied Sciences at 

xliv
Authors Biography
Harvard University. Between 1995 and 1998, he was also a Ricercatore in the Department of 
Mathematics and Computer Science at the University of Udine, Italy. His general research inter-
ests are in computer vision and nonlinear estimation and control theory. In particular, he is inter-
ested in ways for computers to use sensory information (e.g., vision, sound, touch) to interact 
with humans and the environment. He is a senior member of the IEEE.
CHAPTER 17
Yao-Jen (Kevin) Chang has been with the Siemens Corporation, Corporate 
Technology as a staff scientist since 2011, where he conducts research on 
3D computer vision and image analytics. From 2009 to 2011, he was with 
the School of Electrical and Computer Engineering, Cornell University as a 
research associate focusing on multiview computer vision and content-based 
information retrieval. From 2007 to 2009, he was with the Department 
of Electrical and Computer Engineering, Carnegie Mellon University as a 
Research Scientist focusing on image-based synthesis and modeling. From 
2003 to 2007, he served as a Researcher and the vision team group leader 
at the Advanced Technology Center, Information & Communications Research Labs, Industrial 
Technology Research Institute, where he led research on real-time video-realistic speech anima-
tion and video surveillance. His research interests include computer vision, computer graph-
ics, machine learning, and computational photography. He received his Ph.D. degree from the 
Department of Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan in 2002.
Tsuhan Chen has been with the School of Electrical and Computer 
Engineering, Cornell University, Ithaca, New York, since January 2009, where 
he is Professor and Director. From October 1997 to December 2008, he was 
with the Department of Electrical and Computer Engineering, Carnegie 
Mellon University, Pittsburgh, Pennsylvania, as Professor and Associate 
Department Head. From August 1993 to October 1997, he worked at AT and 
T Bell Laboratories, Holmdel, New Jersey. He received the M.S. and Ph.D. 
degrees in Electrical Engineering from the California Institute of Technology, 
Pasadena, California, in 1990 and 1993, respectively. His research interests 
include multimedia signal processing and communication, multimodal biometrics, audio-visual 
interaction, pattern recognition, computer vision, and computer graphics.
He served as the Editor-in-Chief for IEEE Transactions on Multimedia in 2002–2004. He 
also served in the Editorial Board of IEEE Signal Processing Magazine and as Associate Editor for 
IEEE Transactions on Circuits and Systems for Video Technology, IEEE Transactions on Image 
Processing, IEEE Transactions on Signal Processing, and IEEE Transactions on Multimedia. He 
co-edited a book titled Multimedia Systems, Standards, and Networks.
He received the Charles Wilts Prize at the California Institute of Technology in 1993. He 
was a recipient of the National Science Foundation CAREER Award, from 2000 to 2003. He 

xlv
Authors Biography
received the Benjamin Richard Teare Teaching Award in 2006, and the Eta Kappa Nu Award for 
Outstanding Faculty Teaching in 2007. He was elected to the Board of Governors, IEEE Signal 
Processing Society, 2007–2009, and a Distinguished Lecturer, IEEE Signal Processing Society, 
2007–2008. He is a member of the Phi Tau Phi Scholastic Honor Society and Fellow of IEEE.
CHAPTER 18
Gregory Castanon is a Ph.D. Candidate at Boston University. His interests 
include activity recognition, classification, visual search, surveillance, and ulti-
mate frisbee.
Pierre-Marc Jodoin studied physics at the University of Montreal (1994) and 
received the B.Sc. degree in Computer Science at the École Polytechnique 
de Montréal, Canada (1995–2000). He then obtained the M.Sc. degree in 
computer graphics (2002) and the Ph.D. degree in computer vision and video 
analysis at the University of Montreal (2007). He received both M.Sc. and 
Ph.D. degrees with honors. He is now Associate Professor at the Université 
de Sherbrooke, Canada. He co-founded and now is director of the Research 
center for intelligent environments, the Sherbrooke medical imaging analysis 
platform, as well as the Imeka.ca initialive, a start-up company devoted to 
medical imaging analysis. His research interests are in video surveillance, video analysis, medical 
imaging, and computer vision. In 2012, he co-organized the CVPR change detection workshop 
and released the changedetection.net dataset, the largest dataset in the world devoted to change 
detection. He is currently an Associate Editor of IEEE Transactions on Image Processing. More 
information about his work is available at http://www.dmi.usherb.ca/~jodoin/.
Venkatesh Saligrama is a Faculty Member in the Electrical and Computer 
Engineering Department and the Division of Systems Engineering at Boston 
University. He holds a Ph.D. from MIT. His research interests are in Machine 
Learning, Video Analytics, Information and Control Theory and Statistical 
Signal Processing. He has edited a book on Networked Sensing, Information 
and Control and was a lead editor for a special issue on Anomaly Detection 
published in IEEE Journal of Special Topics in Signal Processing. He is cur-
rently serving as an Associate Editor for IEEE Transactions on Information 
Theory and has previously served as an Associate Editor for IEEE Transactions 
on Signal Processing. He has organized several workshops and invited sessions at many IEEE and 
Machine learning conferences. He is the recipient of numerous awards including the Presidential 

xlvi
Authors Biography
Early Career Award, ONR Young Investigator Award, and the NSF Career Award. More informa-
tion about his work is available at http://iss.bu.edu/srv.
André Caron studied Computer Science at the University of Sherbrooke and 
received the B.Sc. and M.Sc. degrees in computer science at the Université 
de Sherbrooke, Canada (2009, 2011).
CHAPTER 19
Fabio Poiesi received B.Sc. and M.Sc. degrees in Telecommunication 
Engineering from the University of Brescia in 2007 and 2010, respectively. 
I completed my Master thesis at Queen Mary University of London and it 
involved the estimation of the ball position in basketball matches by looking 
at the behavior of the players. Since April 2010 I have been a research student 
in the School of Electronic Engineering and Computer Science and my super-
visor is Professor Andrea Cavallaro. My research field involves multitarget 
tracking in highly populated scenes, behavior understanding, and performance 
evaluation of tracking algorithms.
Andrea Cavallaro is Professor of Multimedia Signal Processing and Director 
of the Centre for Intelligent Sensing at Queen Mary University of London, 
UK. He received his Ph.D. in Electrical Engineering from the Swiss Federal 
Institute of Technology (EPFL), Lausanne, in 2002 and the Laurea (Summa 
cum Laude) in Electrical Engineering from the University of Trieste in 
1996. He was a Research Fellow with British Telecommunications (BT) 
in 2004/2005 and was awarded the Royal Academy of Engineering teach-
ing Prize in 2007; three student paper awards on target tracking and per-
ceptually sensitive coding at IEEE ICASSP in 2005, 2007, and 2009; and 
the best paper award at IEEE AVSS 2009. He is Area Editor for the IEEE Signal Processing 
Magazine, and Associate Editor for the IEEE Transactions on Image Processing and the IEEE 
Transactions on Signal Processing. He is an elected member of the IEEE Signal Processing 
Society, Image, Video, and Multidimensional Signal Processing Technical Committee. 
He served as an Elected Member of the IEEE Signal Processing Society, Multimedia 
Signal Processing Technical Committee, as Associate Editor for the IEEE Transactions on 
Multimedia, and as Guest Editor for several journals including Computer Vision and Image 
Understanding, the International Journal of Computer Vision, the IEEE Signal Processing 
Magazine, and the IEEE Transactions on Circuits and Systems for Video Technology. He was 
General Chair for IEEE/ACM ICDSC 2009, BMVC 2009, M2SFA2 2008, SSPE 2007, and 

xlvii
Authors Biography
IEEE AVSS 2007. He was Technical Program chair of IEEE AVSS 2011, the European Signal 
Processing Conference (EUSIPCO 2008) and of WIAMIS 2010. He has published more than 
100 journal and conference papers, and two books: Multi-camera networks—2009, Elsevier 
and Video Tracking—2011, Wiley.
CHAPTER 20
Ashok Veeraraghavan is currently Assistant Professor of Electrical and 
Computer Engineering at Rice University, Tx, USA. Before joining Rice 
University, he spent three wonderful and fun-filled years as a Research 
Scientist at Mitsubishi Electric Research Labs in Cambridge, MA. He received 
his Bachelors in Electrical Engineering from the Indian Institute of Technology, 
Madras in 2002 and M.S and Ph.D. degrees from the Department of Electrical 
and Computer Engineering at the University of Maryland, College Park in 
2004 and 2008, respectively. His thesis received the Doctoral Dissertation 
award from the Department of Electrical and Computer Engineering at the 
University of Maryland. He loves playing, talking, and pretty much anything to do with the slow 
and boring but enthralling game of cricket. At Rice University, he directs the Computational 
Imaging and Vision Lab. His research interests are broadly in the areas of computational imaging, 
computer vision, and robotics.
Aswin C. Sankaranarayanan is an Assistant Professor in the ECE Department 
at Carnegie Mellon University, Pittsburgh, PA. His research interests lie in 
the areas of computer vision, signal processing, and image and video acqui-
sition. He received his B.Tech in Electrical Engineering from the Indian 
Institute of Technology, Madras in 2003 and M.Sc. and Ph.D. degrees from 
the Department of Electrical and Computer Engineering at the University 
of Maryland, College Park in 2007 and 2009, respectively. He was awarded 
the Distinguished Dissertation Fellowship by the Department of Electrical 
and Computer Engineering at the University of Maryland in 2009. He was a 
postdoctoral researcher at Rice University from October 2009 to December 2012.
Richard G. Baraniuk is the Victor E. Cameron Professor of Electrical and 
Computer Engineering at Rice University. His research interests lie in the 
areas of signal processing, machine learning, and open education. He received 
a NATO postdoctoral fellowship from NSERC in 1992, the National Young 
Investigator award from the National Science Foundation in 1994, a Young 
Investigator Award from the Office of Naval Research in 1995, the Rosenbaum 
Fellowship from the Isaac Newton Institute of Cambridge University in 
1998, the C. Holmes MacDonald National Outstanding teaching Award 
from Eta Kappa Nu in 1999, the University of Illinois ECE Young Alumni 

xlviii
Authors Biography
Achievement Award in 2000, the Tech Museum Laureate Award from the Tech Museum of 
Innovation in 2006, the Wavelet Pioneer Award from SPIE in 2008, the Internet Pioneer Award 
from the Berkman Center for Internet and Society at Harvard Law School in 2008, the World 
Technology Network Education Award and IEEE Signal Processing Society Magazine Column 
Award in 2009, the IEEE-SPS Education Award in 2010, the WISE Education Award in 2011, 
and the SPIE Compressive Sampling Pioneer Award in 2012. In 2007, he was selected as one of 
Edutopia Magazine’s Daring Dozen educators, and the Rice single-pixel compressive camera was 
selected by MIT Technology Review Magazine as a TR10 Top 10 Emerging Technology. He was 
elected a Fellow of the IEEE in 2001 and of AAAS in 2009.
CHAPTER 21
Faisal Z. Qureshi (Ph.D. 2007 U. Toronto) is an Assistant Professor of Computer 
Science and the Founding Director of the Visual Computing Laboratory at the 
University of Ontario Institute of Technology (UOIT), Canada. He obtained a 
Ph.D. in Computer Science from the University of Toronto in 2007.
He also holds an M.Sc. in Computer Science from the University of 
Toronto, and an M.Sc. in Electronics (with distinction) from Quaid-e-Azam 
University, Pakistan. Prior to joining UOIT, he worked as a Software Developer 
at Autodesk. His research interests include sensor networks, computer vision, 
and computer graphics. He is active in conference organizations, serving as the 
general co-chair for the Workshop on Camera Networks and Wide-Area Scene Analysis (co-located 
with CVPR) in 2011–2013, technical program committee chair ICDSC 2013, and publicity chair 
AVSS 2013. He also served as a Guest Editor for IEEE Journal on Emerging and Selected Topics 
in Circuits and Systems (Special issue on Computational and Smart Cameras). He is a member of 
the IEEE and the ACM.
Demetri Terzopoulos (Ph.D. 1984 MIT) is the Chancellor’s Professor of Computer Science at the 
University of California, Los Angeles, where he holds the rank of Distinguished Professor and 
directs the UCLA Computer Graphics & Vision Laboratory. He is or was a Guggenheim Fellow, a 
Fellow of the ACM, a Fellow of the IEEE, a Fellow of the Royal Society of Canada, and a member 
of the European Academy of Sciences. One of the most highly cited authors in engineering and 
computer science, his many awards include an Academy Award for Technical Achievement from 
the Academy of Motion Picture Arts and Sciences for his pioneering research on physics-based 
computer animation, and the inaugural Computer Vision Significant Researcher Award from 
the IEEE for his pioneering and sustained research on deformable models and their applications 
(http://www.cs.ucla.edu/~dt).
CHAPTER 23
Marilyn Wolf is Farmer Distinguished Chair and Georgia Research Alliance Eminent Scholar 
at the Georgia Institute of Technology. She received her B.S., M.S., and Ph.D. in Electrical 
Engineering from Stanford University in 1980, 1981, and 1984, respectively. She was with AT & 

xlix
Authors Biography
T Bell Laboratories from 1984 to 1989. She was on the faculty of Princeton 
University from 1989 to 2007. Her research interests included embed-
ded computing, embedded video and computer vision, and VLSI systems. 
She has received the ASEE Terman Award and IEEE Circuits and Systems 
Society Education Award. She is a Fellow of the IEEE and ACM and an IEEE 
Computer Society Golden Core member.
Jason Schlessman is an Embedded Systems Designer at Adidas miCoach Research & Development. 
He is also pursuing his Ph.D. degree in Electrical Engineering at Princeton University.
CHAPTER 24
Hsiang-Huang Wu received the B.S. and M.S. degrees of Electrical Engineering 
in 2001 and 2003, respectively, from National Tsing Hua University, Hsinchu, 
Taiwan. He received the Ph.D. degree in Electrical and Computer Engineering in 
2013 from the University of Maryland, College Park (UMCP). From February 
2005 to August 2007, he has held industrial positions as a Software Engineer at 
the Incentia Design Systems, Inc. and an Engineer in the Realtek Semiconductor 
Corp. in Taiwan. In the Realtek Semiconductor Corp., he has proposed a new 
test methodology for Ternary Content Addressable Memories, which has been 
published in the International Test Conference and filed as a patent.
During the past few years, his research includes dataflow modeling and scheduling of the 
applications, especially the scheduling techniques of runtime reconfiguration for the differ-
ent platforms. His current research interest is focusing on dataflow modeling of applications 
and mapping onto the Multiprocessor System-on-Chip (MPSoC) and heterogeneous computing 
platforms.
Chung-Ching Shen is currently an Assistant Research Scientist in the 
Department of Electrical and Computer Engineering, and a Part-Time 
Adjunct Faculty Member in the Professional Master of Engineering Program 
at the University of Maryland at College Park, MD. As a researcher at the 
University of Maryland at College Park, MD, he has obtained extensive 
research experience in model-based design, analysis, and synthesis for sig-
nal-processing applications, including speech recognition, image registration, 
software-defined radio, and wireless sensor networks. He has also developed 
novel model-based development flows with a variety of programming lan-
guages, including C, Verilog, and CUDA for implementing signal-processing applications on 
diverse hardware platforms, including programmable digital signal processors, field-program-
mable gate arrays, graphics-processing units, and low-power microcontrollers, coupled with 

l
Authors Biography
radio transceivers. He received the B.S. degree in Computer Science from National Chiao 
Tung University, Taiwan, in 2000 and the M.S. and Ph.D. degrees in Electrical and Computer 
Engineering from the University of Maryland at College Park, College Park, MD in 2004 and 
2008, respectively.
Hojin Kee received the B.S. degree in Electrical Engineering and Mechanical 
Engineering from Pohang University of Science and Technology (POSTECH), 
and the Ph.D. degree in Electrical Engineering from the University of 
Maryland, College Park in 2010. Since 2010 he joined LabVIEW R&D team 
at National Instruments as a Staff Engineer researching on the compilation, 
optimization, and scheduling of next generation graphical language represen-
tations when mapped to FPGA architectures. He has published many papers 
on systematically mapping dataflow models into efficient hardware imple-
mentations and exploring trade-offs contributing to the dataflow-based hard-
ware synthesis framework in peer-reviewed journals and different conferences.
Nimish Sane is a Digital Engineer at the Center for Solar-Terrestrial Research, 
New Jersey Institute of Technology, where he is working on the digital instru-
mentation for the Expanded Owens Valley Solar Array. He received the B.E. 
degree in Electrical Engineering from the University of Mumbai, India, and 
the M.S. and Ph.D. degrees in Electrical Engineering from the University of 
Maryland at College Park. His research interests include dataflow modeling, 
high-level tools for design of high performance signal processing systems, and 
radio astronomy signal processing.
William Plishker, Ph.D. received the B.S. degree in Computer Engineering from 
Georgia Tech, Atlanta, GA, in 2000 and the Ph.D. degree in Electrical Engineering 
from the University of California, Berkeley. His Ph.D. research centered around 
the acceleration of network applications on network processors. Currently, he is 
an Assistant Research Scientist at the University of Maryland at College Park, 
College Park, MD, focusing on application acceleration using dataflow modeling 
and leveraging different forms of parallelism. He has published work on new data-
flow models and scheduling techniques as well as application acceleration on mul-
tiple platforms, including clusters, graphics processing units, field-programmable 
gate arrays, and network processors. His application areas of interest include medi-
cal imaging, software-defined radio, networking, and high-energy physics.
Shuvra S. Bhattacharyya is a Professor in the Department of Electrical and Computer Engineering 
at the University of Maryland, College Park. He holds a joint appointment in the University 

li
Authors Biography
of Maryland Institute for Advanced Computer Studies (UMIACS) and is a 
member of the University of Maryland Energy Research Center (UMERC). 
He is an author of six books, and over 220 papers in the areas of signal pro-
cessing, embedded systems, electronic design automation, wireless commu-
nication, and wireless sensor networks. His research interests include signal 
processing systems and architectures; wireless sensor networks; embedded 
software; and hardware/software co-design. He received the B.S. degree 
from the University of Wisconsin at Madison and the Ph.D. degree from the 
University of California at Berkeley. He is a Fellow of the IEEE.
CHAPTER 25
Yuzhe Xu received the Bachelor’s Degree of Engineering from Shanghai 
JiaoTong University in 2005, and his Master Degree in System Control and 
Robotics program from KTH Royal Institute of Technology in 2011. He is cur-
rently pursuing the Ph.D. Degree at KTH. He is with the Automatic Control 
Department. His research interests include Wireless Sensor Networks, 
Distributed Optimization, Tracking, and Data Fusion.
Vijay Gupta is an Assistant Professor in the Department of Electrical Engineering 
at the University of Notre Dame. He received his B.Tech degree from the 
Indian Institute of Technology, Delhi and the M.S. and Ph.D. degrees from 
the California Institute of Technology, all in Electrical Engineering. Prior to 
joining Notre Dame, he also served as a research associate in the Institute for 
Systems Research at the University of Maryland, College Park. He received 
the NSF CAREER award in 2009, and the Ruth and Joel Spira award for 
excellence in teaching in the Department of Electrical Engineering at the 
University of Notre Dame in 2010. His research interests include cyber-
physical systems, distributed estimation, detection and control, and, in general, the interaction 
of communication, computation, and control.
Carlo Fischione is a tenured Associate Professor at KTH Royal Institute of Technology, Electrical 
Engineering and ACCESS Linnaeus Center, Automatic Control Lab, Stockholm, Sweden. He 
received the Ph.D. degree in Electrical and Information Engineering in May 2005 from University 
of L’Aquila, Italy, and the Dr.Eng. degree in Electronic Engineering (Laurea, Summa cum Laude, 
5/5 years) in April 2001 from the same University. He held research positions at University of 
California at Berkeley, Berkeley, CA (2004–2005, Visiting Scholar, and 2007–2008, Research 

lii
Authors Biography
Associate) and Royal Institute of Technology, Stockholm, Sweden (2005–2007, 
Research Associate). His research interests include optimization and parallel 
computation with applications to wireless sensor networks, networked con-
trol systems, and wireless networks. He has co-authored over 80 publications, 
including book, book chapters, international journals and conferences, and an 
international patent. He received numerous awards, including the best paper 
award from the IEEE Transactions on Industrial Informatics of 2007, the best 
paper awards at the IEEE International Conference on Mobile Ad-hoc and 
Sensor System 2005 and 2009 (IEEE MASS 2005 and IEEE MASS 2009), 
the Best Business Idea award from VentureCup East Sweden, 2010, the “Ferdinando Filauro” 
award from University of L’Aquila, Italy, 2003, the “Higher Education” award from Abruzzo 
Region Government, Italy, 2004, and the Junior Research award from Swedish Research Council, 
2007, the Silver Ear of Wheat award in history from the Municipality of Tornimparte, Italy, 
2012. He has chaired or served as a technical member of program committees of several interna-
tional conferences and is serving as referee for technical journals. Meanwhile, he also has offered 
his advice as a consultant to numerous technology companies such as Berkeley Wireless Sensor 
Network Lab, Ericsson Research, Synopsys, and United Technology Research Center. He is co-
founder and CTO of the sensor networks start-up company Aukoti. He is Member of IEEE (the 
Institute of Electrical and Electronic Engineers), and Ordinary Member of DASP (the academy 
of history Deputazione Abruzzese di Storia Patria).
CHAPTER 27
Meinard Müller studied mathematics (Diplom) and computer science (Ph.D.) 
at the University of Bonn, Germany. In 2002/2003, he conducted postdoc-
toral research in combinatorics at the Mathematical Department of Keio 
University, Japan. In 2007, he finished his Habilitation at Bonn University in 
the field of multimedia retrieval writing a book titled Information Retrieval 
for Music and Motion, which appeared as Springer monograph. From 2007 
to 2012, he was a member of the Saarland University and the Max-Planck 
Institut für Informatik leading the research group Multimedia Information 
Retrieval & Music Processing within the Cluster of Excellence on Multimodal 
Computing and Interaction. Since September 2012, Meinard Müller holds a professorship for 
Semantic Audio Processing at the International Audio Laboratories Erlangen, which is a joint insti-
tution of the University of Erlangen-Nuremberg and Fraunhofer IIS. His recent research inter-
ests include content-based multimedia retrieval, audio signal processing, music processing, music 
information retrieval, and motion processing.
Anssi Klapuri received his Ph.D. degree from the Tampere University of  Technology (TUT), 
Tampere, Finland, in 2004. He visited as a postdoc researcher at Ecole Centrale de Lille, France, 
and Cambridge University, UK, in 2005 and 2006, respectively. He worked as a Lecturer at the 

liii
Authors Biography
Center for Digital Music at Queen Mary University of London, London, UK, in 
2010–2011. He is currently CTO at Ovelin, Finland, and Associate Professor at 
TUT. His research interests include audio signal processing, auditory modeling, 
and machine learning.
CHAPTER 28
Jürgen Herre received his Diplom-Ingenieur degree in electrical engineering 
from the university of Erlangen/Nuremberg, Germany, in 1989, and subse-
quently joined the Fraunhofer Institute for Integrated Circuits (IIS) in Erlangen, 
Germany. Since then he has been involved in the development of perceptual 
coding algorithms for high-quality audio, including the well-known ISO/MPEG-
Audio Layer III coder (aka “MP3”). In 1995, he joined Bell Laboratories for 
a PostDoc term working on the development of MPEG-2 Advanced Audio 
Coding (AAC). By the end of 1996 he went back to Fraunhofer to work on 
the development of more advanced multimedia technology including MPEG-
4, MPEG-7, and MPEG-D, currently as the Chief Scientist for the Audio/Multimedia activities 
at Fraunhofer IIS, Erlangen. In September 2011, he was appointed Professor at the University of 
Erlangen and the International Audio Laboratories Erlangen. He is a Fellow of the Audio Engineering 
Society, co-Chair of the AES Technical Committee on Coding of Audio Signals and Vice Chair of 
the AES Technical Council. He is a member of the IEEE Technical Committee on Audio and 
Acoustic Signal Processing, served as an associate editor of the IEEE Transactions on Speech and 
Audio Processing, and is an active member of the MPEG audio subgroup.
Sascha Disch received his Diplom-Ingenieur degree in electrical engineer-
ing from the Technical University Hamburg-Harburg (TUHH), Germany in 
1999. From 1999 to 2007 he joined the Fraunhofer Institute for Integrated 
Circuits (IIS), Erlangen, Germany. At Fraunhofer, he worked in research and 
development in the field of perceptual audio coding and audio processing. 
In MPEG standardization of parametric spatial audio coding he contributed 
as a developer and served as a co-editor of the MPEG Surround standard. 
From 2007 to 2010 he was a researcher at the Laboratory of Information 
Technology, Leibniz University Hanover (LUH), Germany, from where he 
received his Dr. Ingenieur degree in 2011. During that time, he also participated in the devel-
opment of the Unified Speech and Audio Coding (USAC) standard at MPEG. Since 2010, he 
is affiliated with Fraunhofer IIS again. His research interests include waveform and parametric 
audio signal coding, audio bandwidth extension, and digital audio effects.

liv
Authors Biography
CHAPTER 30
Gerald Enzner received the Dipl.-Ing. degree from the University of Erlangen-
Nuremberg, Erlangen, Germany, in 2000, and the Dr.-Ing. degree from 
RWTH Aachen University, Aachen, Germany, in 2006, both in electrical 
engineering. Since 2007, he has been working as a Principal Scientist at the 
Institute of Communication Acoustics, Ruhr-University Bochum, Bochum, 
Germany. His research interests include signal processing for non-linear adap-
tive systems, blind channel identification and equalization, and the dynamical 
modeling of time-varying systems. He actively conducts research projects in 
the applications of adaptive dereverberation for hands-free voice communica-
tion systems and hearing aids, acoustic echo control for linear and non-linear hands-free systems, 
and signal processing for spatial sound control. He was a member of the Global Young Faculty of 
the University Alliance Metropolis Ruhr (UAMR) from 2009 to 2011.
Herbert Buchner holds two graduate degrees and a doctoral degree in com-
munications engineering and signal processing from the University of 
Applied Sciences, Regensburg, Germany, and the University of Erlangen-
Nuremberg, Germany, respectively. His first research positions include the 
Colorado Optoelectronic Computing Systems Center in 1995 and NTT 
Cyber Space Labs Tokyo from 1996 to 1997. In 1997–1998 he was with 
the Driver Information Systems Department of Siemens AG. From 2000 
to 2006, he was a research and teaching assistant with the University of 
Erlangen-Nuremberg. Dr. Buchner joined the Deutsche Telekom Laboratories 
at Technical University of Berlin, Germany, in 2007 as a senior research scientist in the field of 
adaptive signal processing for human-machine interfaces. At TU Berlin, he currently is a research 
fellow in the Machine Learning group extending his work in the field of machine learning, statisti-
cal data analysis, and to biomedical applications.
Alexis Favrot, M.Sc., received a Master of Science (Engineer) degree in electri-
cal engineering from Supélec Paris, France, and EPF Lausanne, Switzerland, in 
2005. From 2005 to 2006 he worked at Scopein Research, where he worked 
on cocktail party processing and design of public address system algorithms 
on DSP. After working temporarily for Merging Technologies, developing 
numerous audio processing algorithms, he joined Illusonic in 2007 as an audio 
research engineer working on acoustic echo and noise suppression, spatial 
sound capture and spatial reproduction, and spatial hearing.
Fabian Kuech received the Dipl.-Ing. and the Dr.-Ing. degrees in electrical  engineering from the 
Friedrich-Alexander-University Erlangen-Nuremberg, Germany, in 2005. During that time he was 
a member of the audio group at the Chair of Multimedia Communications and Signal Processing. 

lv
Authors Biography
In 2005 he joined the speech technology group at Teleca Systems GmbH, 
Germany. Since 2006, he is at the audio department of the Fraunhofer IIS, 
where he currently holds the position of a group manager. His team focuses 
on research and development in the area of acoustic front-end processing for 
speech enhancement in hands-free communications systems, as well as spatial 
audio recording, reproduction, and processing.
CHAPTER 32
Rudolf Rabenstein has received the degrees “Diplom-Ingenieur” and “Doktor-
Ingenieur” in electrical engineering and the degree “Habilitation” in signal 
processing, all from the University of Erlangen-Nuremberg, Germany in 
1981, 1991, and 1996, respectively. He worked with the Physics Department 
of the University of Siegen, Germany, and now as a Professor with the 
Telecommunications Laboratory at the University of Erlangen-Nuremberg. 
His research interests are in the fields of multidimensional systems theory 
and multimedia signal processing. He had served on the IEEE Technical 
Committee for Signal Processing Education and as an associate editor of the 
IEEE Transactions on Audio, Speech and Language Processing. Currently he is a senior area edi-
tor of the IEEE Signal Processing Letters and an associate editor of the journal Multidimensional 
Systems and Signal Processing.
Sascha Spors is a Professor and heads the group for Signal Theory and  Digital 
Signal Processing at the Institute of Communications Engineering, Universität 
Rostock. From 2005 to 2012, he was heading the audio technology group at 
the Quality and Usability Lab, Deutsche Telekom Laboratories, Technische 
Universität Berlin, as a senior research scientist. From the Electrical, Electronic 
and Communication Engineering Faculty of the University Erlangen-
Nuremberg, he obtained his Doctoral Degree (Dr.-Ing.) with distinction in 
January 2006, as a result of his work as a research scientist at the Chair of 
Multimedia Communications and Signal Processing. The topic of his doctoral 
thesis was the active compensation of the listening room characteristics for sound reproduction 
systems. During his thesis work from 2001 to 2006, he conducted research on wavefield analysis, 
wavefield synthesis, and massive multichannel adaptation problems. Sascha Spors is member 
of the IEEE Signal Processing Society, the “Deutsche Gesellschaft für Akustik (DEGA),” and 
the Audio Engineering Society (AES). In 2011 he has received the Lothar-Cremer-Preis of the 
DEGA. He is an Associate Editor for the IEEE Signal Processing Letters, member of the IEEE 
Audio and Acoustic Signal Processing Technical Committee, and Chair of the AES Technical 
Committee on Spatial Audio.

lvi
Authors Biography
Jens Ahrens has received a Diploma with distinction in Electrical Engineering/
Sound Engineering (equivalent to Master of Science) from Graz University of 
Technology and University of Music and Dramatic Arts Graz, Austria, and the 
Doctoral Degree (Dr.-Ing.) with distinction from University of Technology 
Berlin, Germany. From 2006 to 2011 he was member of the Audio Technology 
Group at Deutsche Telekom Laboratories/TU Berlin where he worked on the 
topic of sound field synthesis. Currently, he is a postdoctoral researcher at 
Microsoft Research in Redmond, Washington, USA. His main research inter-
ests include the physical fundamentals and psychoacoustic properties of spa-
tial audio capture and presentation.
CHAPTER 34
Jon Gudnason is an Assistant Professor at Reykjavik University, Iceland. He 
is a member of the academic staff at the School of Science and Engineering 
and a member of the Center for Analysis and Design of Intelligent Agents 
(CADIA). His professional experience is in the area of speech signal process-
ing and machine learning and he is dedicated to the development of language 
technology for his mother-language Icelandic. He received B.Sc. and M.Sc. 
degrees in Electrical and Computer Engineering from the University of Iceland 
in 1999 and 2000, respectively, and completed a Ph.D. degree at Imperial 
College, London, UK in 2007. From 2001 to 2009 he was a research associate 
at Imperial College, where his research focused on speaker recognition and automatic target rec-
ognition using radar. In 2006–2008 he worked on projects at SpinVox converting voicemail to text 
messages. In 2008 he received the Global Research Award of the Royal Academy of Engineering 
and visited LabROSA at Columbia University, New York. In 2012 he was especially recognized by 
the Icelandic Language Board for organizing the development of speech recognition for Icelandic.
CHAPTER 35
Mike Brookes is a Reader in Signal Processing in the Department of Electrical 
and Electronic Engineering at Imperial College London. After graduating in 
Mathematics from Cambridge University in 1972, he spent 4 years at the 
Massachusetts Institute of Technology before returning to the UK and joining 
Imperial College. Within the area of speech processing, he has concentrated 
on the modeling and analysis of speech signals, the extraction of features for 
speech and speaker recognition and, most recently, on the enhancement of 
poor quality speech signals. Between 2007 and 2012, he was the Director of 
the Home Office sponsored Center for Law Enforcement Audio Research 
(CLEAR) which investigated techniques for processing heavily corrupted 
speech signals.

lvii
Authors Biography
Nikolay D. Gaubitch is a postdoctoral researcher at Delft University of 
Technology in close collaboration with Google. He obtained the M.Eng. 
degree in Computer Engineering from Queen Mary, University of London 
in 2002 and the Ph.D. degree in Acoustic Signal Processing from Imperial 
College London in 2007. Between 2007 and 2012 he was a research associate 
with the Center for Law Enforcement Audio Research (CLEAR). His research 
interests span various topics within the field of enhancement of reverberant 
and noisy speech signals using one or more microphones including blind room 
transfer function estimation and equalization, speech quality and intelligibility 
estimation and audio processing with ad hoc microphone arrays.

xxxi
Rama Chellappa received the B.E. (Hons.) degree in Electronics and Communication 
Engineering from the University of Madras, India in 1975 and the M.E. (with 
Distinction) degree from the Indian Institute of Science, Bangalore, India in 1977. 
He received the M.S.E.E. and Ph.D. Degrees in Electrical Engineering from Purdue 
University, West Lafayette, IN, in 1978 and 1981, respectively. During 1981–
1991, he was a faculty member in the department of EE-Systems at University 
of Southern California (USC). Since 1991, he has been a Professor of Electrical 
and Computer Engineering (ECE) and an affiliate Professor of Computer Science 
at University of Maryland (UMD), College Park. He is also affiliated with the 
Center for Automation Research, the Institute for Advanced Computer Studies (Permanent Member) 
and is serving as the Chair of the ECE department. In 2005, he was named a Minta Martin Professor 
of Engineering. His current research interests are face recognition, clustering and video summariza-
tion, 3D modeling from video, image and video-based recognition of objects, events and activities, 
dictionary-based inference, compressive sensing, domain adaptation and hyper spectral processing.
Prof. Chellappa received an NSF Presidential Young Investigator Award, four IBM Faculty 
Development Awards, an Excellence in Teaching Award from the School of Engineering at USC, and 
two paper awards from the International Association of Pattern Recognition (IAPR). He is a recipient 
of the K.S. Fu Prize from IAPR. He received the Society, Technical Achievement, and Meritorious 
Service Awards from the IEEE Signal Processing Society. He also received the Technical Achievement 
and Meritorious Service Awards from the IEEE Computer Society. At UMD, he was elected as a 
Distinguished Faculty Research Fellow, as a Distinguished Scholar-Teacher, received an Outstanding 
Innovator Award from the Office of Technology Commercialization, and an Outstanding GEMSTONE 
Mentor Award from the Honors College. He received the Outstanding Faculty Research Award and the 
Poole and Kent Teaching Award for Senior Faculty from the College of Engineering. In 2010, he was 
recognized as an Outstanding ECE by Purdue University. He is a Fellow of IEEE, IAPR, OSA, and 
AAAS. He holds four patents.
Prof. Chellappa served as the Editor-in-Chief of IEEE Transactions on Pattern Analysis and Machine 
Intelligence. He has served as a General and Technical Program Chair for several IEEE international 
and national conferences and workshops. He is a Golden Core Member of the IEEE Computer Society 
and served as a Distinguished Lecturer of the IEEE Signal Processing Society. Recently, he completed 
a two-year term as the President of the IEEE Biometrics Council.
About the Editors

xxxii
About the Editors
Sergios Theodoridis is currently Professor of Signal Processing and 
Communications in the Department of Informatics and Telecommunications 
of the University of Athens. His research interests lie in the areas of Adaptive 
Algorithms and Communications, Machine Learning and Pattern Recognition, 
Signal Processing for Audio Processing and Retrieval. He is the co-editor of the 
book “Efficient Algorithms for Signal Processing and System Identification,” 
Prentice Hall 1993, the co-author of the best selling book “Pattern Recognition,” 
Academic Press, 4th ed. 2008, the co-author of the book “Introduction to Pattern 
Recognition: A MATLAB Approach,” Academic Press, 2009, and the co-author of 
three books in Greek, two of them for the Greek Open University. He is Editor-in-Chief for the Signal 
Processing Book Series, Academic Press and for the E-Reference Signal Processing, Elsevier.
He is the co-author of six papers that have received best paper awards including the 2009 IEEE 
Computational Intelligence Society Transactions on Neural Networks Outstanding paper Award. He 
has served as an IEEE Signal Processing Society Distinguished Lecturer. He was Otto Monstead Guest 
Professor, Technical University of Denmark, 2012, and holder of the Excellence Chair, Department of 
Signal Processing and Communications, University Carlos III, Madrid, Spain, 2011.
He was the General Chairman of EUSIPCO-98, the Technical Program co-Chair for ISCAS-2006 
and ISCAS-2013, and co-Chairman and co-Founder of CIP-2008 and co-Chairman of CIP-2010. He 
has served as President of the European Association for Signal Processing (EURASIP) and as member 
of the Board of Governors for the IEEE CAS Society. He currently serves as member of the Board of 
Governors (Member-at-Large) of the IEEE SP Society.
He has served as a member of the Greek National Council for Research and Technology and he 
was Chairman of the SP advisory committee for the Edinburgh Research Partnership (ERP). He has 
served as Vice Chairman of the Greek Pedagogical Institute and he was for 4 years member of the 
Board of Directors of COSMOTE (the Greek mobile phone operating company). He is Fellow of IET, 
a Corresponding Fellow of the Royal Society of Edinburgh (RSE), a Fellow of EURASIP, and a Fellow 
of IEEE.

xxxiii
Joel Trussell received degrees from Georgia Tech (B.S.), Florida State (M.S.), 
and the University of New Mexico (Ph.D.). He joined the Los Alamos Scientific 
Laboratory, Los Alamos, NM, in 1969 where he began working in the image 
and signal processing in 1971. During 1978–1979, he was a Visiting Professor 
at Heriot-Watt University, Edinburgh, Scotland where he worked with both the 
university and with industry on image processing problems. In 1980, he joined 
the Electrical and Computer Engineering Department at North Carolina State 
University, Raleigh, NC, where he is now a Professor. During 1988–1989, he 
was a visiting scientist at the Eastman Kodak Company in Rochester, NY and 
1997–1998 was a visiting scientist at Color Savvy Systems in Springboro, OH. He was a Visiting 
Fellow Commoner at Trinity College Cambridge University in 2007 and a visiting scientist with 
Hewlett-Packard Labs in 2008. From 2002 to 2010, he served as Director of Graduate Programs 
for the ECE Department. He is a past associate editor for the Transactions on ASSP and the Signal 
Processing Letters. He is a past Chairman of and long serving on the Image and Multidimensional 
Digital Signal Processing Committee of the Signal Processing Society of the IEEE. He is Fellow of 
the IEEE and has shared the IEEE-ASSP Society Senior Paper Award (1986 with M.R. Civanlar) 
and the IEEE-SP Society Paper Award (1993 with P.L. Combettes). He is the author of two texts: 
Fundamentals of Digital Imaging (with M.J. Vrhel) and Mathematics: The Language of Electrical and 
Computer Engineering (with Y. Viniotis).
Anuj Srivastava is a professor in the Department of Statistics, Florida State 
University. He obtained M.S. and Ph.D. degrees in electrical engineering from 
Washington University in St. Louis, in the years 1993 and 1996, respectively. 
During 1996-97, he was a visiting research scientist at the Division of Applied 
Mathematics, Brown University, working with Prof. Ulf Grenander. In Fall 
1997, he joined the Department of Statistics at the Florida State University 
as an assistant professor. During 2003-2006, he was an associate professor 
there, and later in 2007 he became a professor. He has held visiting positions 
at the University of Lille, France and INRIA, Sophia Antipolis, France. 
He was awarded FSU’s Developing Scholar Award in 2005 and the Graduate Mentor Faulty Award 
in 2008.
Amit K. Roy-Chowdhury received his Masters degree in systems science and automation from the 
Indian Institute of Science, Bangalore, India, and the Ph.D. degree in electrical engineering from the 
University of Maryland, College Park. He is a Professor of electrical engineering and a Cooperating 
Faculty in the Department of Computer Science, University of California, Riverside. His broad 
Section Editors
Section 1
Section 2
Section 3

xxxiv
﻿Section Editors
research interests include the areas of image processing and analysis, computer 
vision, and statistical signal processing and pattern recognition, where he has over 
100 technical publications. His current research projects include intelligent cam-
era networks, wide-area scene analysis, motion analysis in video, activity recog-
nition and search, video-based biometrics (face and gait), and biological video 
analysis. He is a coauthor of two books - Camera Networks: The Acquisition 
and Analysis of Videos over Wide Areas and Recognition of Humans and Their 
Activities Using Video. He has been on the organizing and program committees 
of multiple computer vision and image processing conferences and is serving on 
the editorial boards of multiple journals.
Ankur Srivastava received the B.S. degree in technology and electrical engineer-
ing from the Indian Institute of Technology Delhi, India, the M.S. degree in com-
puter engineering from the Department of Electrical and Computer Engineering 
(ECE), Northwestern University, Evanston, IL, and the Ph.D. degree from the 
Computer Science, University of California, Los Angeles (UCLA), in 1998, 
2000, and 2002,  respectively.
He is currently an Associate Professor with the ECE Department, University of 
Maryland, College Park, where he has been since 2002, with a joint appointment 
with the Institute for Systems Research. His current research interests include 
design methods and runtime control policies for high-performance, low temperature, and low-power 
VLSI circuits. Dr. Srivastava was a recipient of the Outstanding Ph.D. Award from the Computer Science 
of UCLA and the George Corcoran Memorial Outstanding Teaching Award by the ECE Department, 
University of Maryland. His work has been recognized through several best paper awards and nomina-
tions, including ICCAD 2003, ISPD 2007, AHS 2011, ISVLSI 2012, and DAC 2012.
Patrick A. Naylor received his B.Eng. degree in Electronic and Electrical 
Engineering from the University of Sheffield, UK, in 1986 and the Ph.D. degree 
from Imperial College, London, UK, in 1990. Since 1990 he has been a member 
of academic staff in the Department of Electrical and Electronic Engineering at 
Imperial College London. His research interests are in the areas of speech, audio 
and acoustic signal processing. He has worked in particular on adaptive signal pro-
cessing for dereverberation, blind multichannel system identification and equal-
ization, acoustic echo control, speech quality estimation and classification, single 
and multichannel speech enhancement, and speech production modeling with par-
ticular focus on the analysis of the voice source signal. In addition to his academic research, he enjoys 
several fruitful links with industry in the UK, USA, and in mainland Europe. He is an associate editor 
of IEEE Transactions on Audio Speech and Language Processing and an associate member of the IEEE 
Signal Processing Society Technical Committee on Audio and Acoustic Signal Processing.
Section 4
Sections 5, 6 and 7

1
CHAPTER
Digital Imaging: Capture, Display,
Restoration, and Enhancement
H. Joel Trussell
Department of Electrical and Computer Engineering, North Carolina State University, Raleigh, NC, USA
4.01.1 Introduction
The topic of digital imaging is quite large and diverse. The complete Elsevier reference includes this
basic section along with those on Image and Video Compression, Quality Assessment, Image Analysis
and Recognition, Biomedical Imaging, and Video Processing. This section concentrates ﬁrst on the
most basic elements of imaging: capture and display. Then we move on to the fundamental processing
methods of restoration and then enhancement. It is important to recognize that without proper attention
to capturing the digital image accurately, there is little that can be done subsequently. In the ﬁnal phase
of image processing, an image must be displayed accurately in order for the effect of the processing to
be fully appreciated and evaluated by the observers.
This section will attempt to address these aspects. We will give a short summary of the chapters and
how they relate to each other and to the more general topics presented in the Elsevier reference. We
note here the plan for the future editions of the reference, even though some of the chapters will not be
available with this ﬁrst edition.
Since digital cameras are the most common image capture devices, it is appropriate to start with an
in-depth review of the processing chain of the camera and the quality issues that are considered for the
various steps. The area of document capture, e.g., scanners and copiers, is the other important image
capture modality. On the display side, more images are viewed on softcopy devices, such as monitors
and projection displays, than are printed in hardcopy. While we were able to obtain a good chapter
on printing, we are still searching for a qualiﬁed author for softcopy display devices. In a market that
is expanding rapidly, displays on small mobile devices, which have unique problems, are becoming
important. This area is addressed in two chapters, one on the basic display problems and the second on
what can be done with the small images. Since images are captured under less than ideal conditions,
users must sometimes deal with various degradations. We have two chapters on restoration, one from a
more classical perspective and the other from a more mathematical perspective that emphasizes iterative
methods. Two aspects of image enhancement will be considered in the next edition of the reference:
(1) edge and detail enhancement and (2) color and tonal properties. Finally, we hope to recruit an
author to appropriately review the state of image processing software, which currently includes both
commercial and open software. We will now describe the chapters in more detail.
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00001-7
© 2014 Elsevier Ltd. All rights reserved.
3

4
CHAPTER 1 Digital Imaging: Capture, Display, Restoration, and Enhancement
4.01.2 Image capture
The most important step in any imaging system is the initial one of capturing the image. Little can be
done with poor data. This is true even in the cases of restoration that deal with correcting focus and noise
problems. This section gives two perspectives of digitizing images, the ﬁrst from the view of professional
and consumer cameras, and the second from the consideration of documents instead of pictorial scenes.
4.01.2.1 Digital camera quality issues
In our ﬁrst chapter, James Adams and Bruce Pillman, present a comprehensive review of the factors
that must be considered in the design of commercial and consumer color cameras to create high-quality
cameras that produce images with sufﬁcient accuracy for each market. They note the basic image
processing chain that takes the images from the sensor through:
1. defect concealment,
2. demosaicking,
3. color white balance,
4. noise reduction,
5. color and tone correction,
6. possible edge enhancement,
7. possible resizing, and
8. coding and possible compression
to the ﬁnal RGB image that is stored in the camera’s memory.
The authors consider the engineering issues that give rise to the limitations of the hardware and how
it affects the processing. Problems arise from:
1. optics,
2. geometric distortion,
3. chromatic aberrations,
4. nonuniform sensor response,
5. focus,
6. sampling in both space and time,
7. noise caused by several sources, and
8. resource limitations.
The authors describe these problems and the common methods used by manufacturers to address
them. Of particular interest are the limitations of resources available in a relatively low cost consumer
camera. This is an area that is important for current students in image processing and computer engi-
neering. The authors give excellent examples of the current state-of-the-art in handling problems like
autofocus, automatic white balance and color and tone scaling. Since camera sensors are now available
with more bits per pixel, high-dynamic range (HDR) imaging is discussed and examples are presented.
Noise suppression and restoration (sharpening) are addressed in a more theoretical viewpoint by
the two chapters on restoration. The authors consider what can be done using the limited resources in

4.01.3 Image Displays
5
the camera. They discuss the methods used to conceal pixel defects, and exposure and nonuniformity
problems. They give examples of coding problems encountered by virtually all cameras.
Finally, the authors give their thoughts on the open issues and make suggestions as to where
researchers can have the largest impact. Such advice from industrial researchers is most helpful to
new students in this area. Topics of interest include development of improved image quality metrics,
processing with newer, smaller pixel sizes, more complex color ﬁlter array patterns, and multiframe cap-
ture. They also mention the problems of small image size and limited resources of mobile device cameras
such as phones and tablet computers. These issues are also discussed in the chapters on mobile imaging.
4.01.2.2 Image and document capture
Kathrin Berkner gives a review of document imaging. This differs from the problems of the more general
camera in two important ways: (1) the images are from a relatively ﬂat plane and (2) the illumination of
the image is controlled. The range of applications has increased signiﬁcantly since the ﬁrst days of the
Xerox copier. Today documents are always digitized, scanned, or photographed, even if the output is a
copy of the original. The author reviews the image processing chain for document imaging and notes
where it is similar to and different from that of the camera.
She also notes that documents are evolving into a more general type of image with the use of mobile
devices. Users are taking images with their mobile phones of slides at conferences, white boards at
meetings, and road signs during travel. A major problem that must be addressed now in document
processing is to convert the image into text or graphics.
In all of these image modalities, the processing includes image segmentation, classiﬁcation, and
object recognition. The digital image must be segmented and classiﬁed before being coded, since the
optimal coding method depends on the content of the segment, e.g., text is coded differently from
pictorial images. In the case of text, the recognition of the characters will make the storage of the
information much more efﬁcient.
The application of multispectral document capture is important not only because color images are
needed to be recorded accurately, but because analysis of archival documents, such as the Archimedes
Palimpset, has become increasingly common. In document analysis, the illumination bands extend from
the ultraviolet (UV) into the infrared (IR).
Berkner speculates on document processing in the future with the applications related to mobile
imaging being emphasized. She also asks the readers to consider the extension of the meaning of
document to include three-dimensional information and motion, such as gestures. The generation of
documents for use in education presents an interesting area of speculation where more multimedia is
being used to illustrate concepts.
4.01.3 Image displays
Most images are captured with the intent of displaying them to a user. Once the images are captured and
processed, they should be displayed in the most accurate manner possible for the user to appreciate or
evaluate properly the contents. The most common display is a ﬂat panel computer monitor or television.
Digital projectors are the preferred modality at meetings and conferences. These devices will be covered
in a future edition of this reference. Printing has a long history of displaying images but only recently

6
CHAPTER 1 Digital Imaging: Capture, Display, Restoration, and Enhancement
has digital technology been employed to produce images that surpass anything previously available. At
the other end of the quality spectrum are images displayed on small mobile devices. Here the issue is
not producing quality to compete with printing or television, but to produce an adequately usable image
with limited resources.
4.01.3.1 Printing
Philipp Urban, Simon Stahl, and Edgar Dörsam have contributed an excellent chapter covering com-
mercial and desktop printing. They give the reader insight into the complexities of the processes, of
which most imaging students are unaware. They also review the methods used to produce high quality
graphic hardcopy. While most of us are familiar with magazines and catalogs, the area of printing labels
and containers is also discussed.
A short history of printing is presented to help orient the reader. This is followed by a review of the
various printing technologies for both commercial and desktop devices, which gives the reader an idea
of the capabilities and limitations of the different modalities.
The basic steps for color printing include:
1. gamut mapping,
2. separation,
3. halftoning, and
4. printing.
In this chapter, the authors emphasize the major physical problems of subtractive colorimetry and
printer modeling. While halftoning is a necessary step in the vast majority of printing applications, the
optimal algorithms are dependent on the properties of the printing method in use. Thus, a discussion of
halftoning will be left for a subsequent edition.
The chapter addresses the determination of the printers’ gamuts, the range of colors that can be
reproduced. This is followed by a description of the problems and methods of gamut mapping, which
is needed to transform colors that cannot be exactly printed to colors that can be printed. The goal is
to develop transformations that minimize negative visual effects. The advantages and disadvantages of
several approaches are discussed.
Before printing with a four-color system, the three-dimensional (RGB) color image needs to be
separated into four images corresponding to the cyan, magenta, yellow, and black inks used for printing.
This transformation from a 3-D space to a 4-D space is well described. The reader will learn the
characteristics of various approaches. In particular, the appropriate replacement of the colored inks with
some amount of black ink is the basis for many of the differences in the methods. The authors note
that modern printing systems often use more than four colors of inks in order to achieve higher quality
reproductions and give the reader insight into the implementation of such systems.
The modeling of the highly nonlinear printing process is treated in sufﬁcient detail and with ade-
quate references to give the reader a good understanding of the complexity of obtaining accurate color
reproduction from any printed medium. The models attempt to consider most of the physical problems
associated with putting ink on paper, including ink ﬂowing into a three-dimensional substrate and light
reﬂection and refraction effects within the paper.

4.01.3 Image Displays
7
The problems for future consideration are addressed in two ways. The accuracy of the printing can be
improved by new insights into process control, gamut mapping, and spectral printing. Spectral printing
has the goal of replicating the reﬂective spectrum of the color sample, instead of simply trying to ﬁnd a
metameric match. This will allow the observer to see the color correctly under all illuminations, instead
of just one or two. The other avenue of future research is in improving measurements of quality. This
is particularly important to determine the effectiveness of gamut mapping algorithms. Lastly, a note is
made about the new ﬁeld of functional printing, where functional devices, such as circuits and sensors,
are created by printing with conductors and insulators onto various substrates.
4.01.3.2 Small mobile displays
Oscar Au and Lu Fang have written a chapter about the challenges of displaying images on mobile
devices with small screens and limited power. The cameras in mobile phones are capable of capturing
images that far exceed the resolution of the display on the device. The authors review the major problems
of the small display, then concentrate on what they regard as the most important one.
In order to gain higher resolution on the small devices, it is common to deal with the images in
sub-pixel mode. Instead of treating the pixels of the image as groups of red, green, and blue triads
that are illuminated as a group of three, the pixels of the triad are addressed individually. The authors
review methods to display text and images with the new methods. Of interest to the image processing
students is the mathematical approach presented by the authors. This allows students to better analyze the
methods and extend the results to displays with varying characteristics. The chapter includes appropriate
examples to indicate the state-of-the-art in this particular area.
4.01.3.3 Small mobile image processing
Xin Li has contributed a chapter that considers the applications of image processing that are becoming
important for the mobile imaging user. Li begins with a review of the characteristics of CCD and CMOS
sensors. He then considers the methods of interfacing with the image on the mobile device.
Since the mobile devices do not have the keyboard and mouse of a standard laptop or desktop
computer, the user has limited ways to indicate her intentions for processing. The author discusses
the various options of buttons, sliders, and ﬁnger movements. Of more importance is the idea of
“intelligent capture.” Here the user can deﬁne the area of interest in an image by outlining it by ﬁnger
movements. Such movements are now common on devices such as the iPad and iPhone. The user taps
the screen in the appropriate location to initiate certain actions, or uses the thumb and ﬁnger to enlarge
an area. The tap can be used to indicate the area or object of interest or place to measure the exposure.
The thumb and ﬁnger can be used to crop the image or even outline the object of interest. The user can
indicate how she wants the image processed, for example, if the image is a sign or barcode it needs to be
recognized and decoded. This application was mentioned in Berkner’s chapter on document imaging,
where it was noted that the concept of “document” would likely be greatly extended in the future.
The author also considers using the ﬁngers to quickly correct common problems in consumer pho-
tography. The user can indicate, with taps, local places within the image that need tonal correction or
deblurring. With the appropriate software, it is possible to envision elimination of distracting objects,
like power lines or unwanted people, by using new inpainting methods.

8
CHAPTER 1 Digital Imaging: Capture, Display, Restoration, and Enhancement
Li looks toward the future where sensing and processing can be integrated in the camera. He also
speculates that the enhanced user interaction will more closely combine the roles of research and
development. As users try new techniques and methods, they will indicate to the manufacturers what
needs to be done and even approaches for the required tasks.
4.01.4 Restoration
Image restoration is deﬁned by estimating an original image from data that has been degraded in some
way. Restoration differs from enhancement, which is simply concerned with making an image look
better to the observer. In order to develop a restoration algorithm, the user needs a mathematical model
for the degradation process, a quality metric that deﬁnes the goal of the restoration process and a
mathematical algorithm to achieve the goal. For the most part, it is the last element that is the basis
for image restoration research, although some recent work has focused on using metrics other than
that common mean square error. The algorithms have historically enforced various items of a priori
knowledge, such as nonnegativity and limited region of support. The two chapters in this section review
the classical and iterative methods of obtaining solutions. In general, the iterative methods allow for
greater application of a priori knowledge and a wider variety of metrics, while the classical approach
is usually more computationally efﬁcient.
4.01.4.1 Classical restoration
Stanley Reeves presents a review of classical image restoration methods. He begins with a description
of the mathematical models for the blurring and noise processes encountered in most image capture
processes, which include cameras and scanners. The most common blur models are for uniform motion,
out-of-focus lens, and atmospheric turbulence. The sources of noise are enumerated and discussed.
The goals of the restoration process are described in increasing order of complexity. The author
covers the usual methods based on various mean square error criteria, such as maximum likelihood and
maximum a priori estimation. He then describes the extensions that include additional information such
as constraints and regularization. The regularization can be done on a local rather than global range to
account for the variations in sensitivity of the human visual system. Excellent examples of the various
methods are presented to give the student a feel for the results that can be achieved.
From a computational view, the effects of dealing with artiﬁcially truncated images are covered. This
is important since the image captured by a camera is naturally limited in extent, but the degradation, such
as focus, does not end at the boundary of the recorded image. Treating the boundaries of the recorded
image appropriately can help produce a much improved restoration.
Finally, the methods of blur estimation are presented. Since the model is required for the restoration
process, it is often required to estimate the parameters that deﬁne the model. The blur type and extent
are usually the most important of these. The chapter also describes methods that estimate both the blur
and noise parameters simultaneously.
4.01.4.2 Iterative restoration
Sebastian Berisha and James Nagy extend the coverage of restoration methods to those based on iterative
methods. The linear image formation model is the same one used by the classical work in the previous

4.01.4 Restoration
9
chapter. Here the authors describe the basic problem but almost immediately consider regularization
and spatially varying processes. Several examples are given to illustrate the power and the applicability
of the iterative methods for handling spatially varying blurs.
The approach of this chapter is perhaps more rigorously mathematical, with more emphasis placed
on proofs of properties such as convergence. Of course, convergence is required for a successful iterative
algorithm.
Thesectiononiterativemethodsforunconstrainedproblemsdescribesindetailthevariousalgorithms
for solving the unconstrained restoration problems. This analysis shows the effects of preconditioning
the data and describes the properties of the various approaches to the basic problem.
Since a major advantage of the iterative methods is the ability to enforce constraints, the discussion
of handling nonnegativity is both important and enlightening. In addition to enforcing nonnegativity,
which is required for all imaging problems, the authors also show how to deal with Poisson noise, which
is a major noise source in all current digital imaging systems.
A ﬁnal section presents examples that compare the performance of the various algorithms. Both
unconstrained and constrained problems are presented. The convergence behavior of the methods is
analyzed. Different algorithms can achieve different minimum errors at convergence. Thus, some meth-
ods are preferred over others. It is shown that some care needs to be used in selecting a convergence
criterion, since the error may actually increase after some number of iterations for particular methods.
To help the student, the authors include links to their software. This will allow the user to verify their
results and apply the methods to their own problems.

2
CHAPTER
Image Quality in Consumer
Digital Cameras
Bruce H. Pillman* and James E. Adams, Jr.†
* ITT Exelis Geospatial Systems, 400 Initiative Drive, Rochester, NY 14624, USA
†Kodak Research Laboratories, 1999 Lake Avenue, Rochester, NY 14650, USA
Nomenclature
A
f /number for a taking lens, the ratio of focal length to aperture diameter
c
speed of light (m s−1)
g
random variable representing the Gaussian process of read noise
h
Planck’s constant (J s)
kQ
gain applied to a pixel
kXY Z
normalization factor for CIE tristimulus calculations
λ
wavelength of light (m)
ν
spatial frequency (cycles deg−1)
Q
random variable representing a pixel value
q
random variable representing the Poisson process of charge accumulation
R(λ)
illuminant spectral power distribution (W m−2 nm−1)
S(λ)
illuminant spectral power distribution (W m−2 nm−1)
4.02.1 Introduction
This chapter focuses on image quality in consumer digital cameras for color photography, distinct
from other applications such as computer vision, astrophotography, and remote sensing. While the
physics of light collection and imaging are common to all cameras, cameras for color photography
are designed to capture and render a scene for later reproduction and viewing by a human observer.
This objective implies the camera should capture as much scene information as possible, maximizing
data that supports output of a 3-channel color image while minimizing objectionable image artifacts. In
addition, consumer cameras must operate within the desired cost, size, and operational constraints. This
is a system engineering problem with many possible answers. To illustrate, current camera offerings
include models optimized for:
•
Commercial photography in controlled settings, emphasizing resolution, optical control, and high
signal-to-noise ratio (SNR). These include the highest-resolution digital SLRs and medium format
cameras.
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00002-9
© 2014 Elsevier Ltd. All rights reserved.
11

12
CHAPTER 2 Image Quality in Consumer Digital Cameras
•
Action photography, emphasizing minimal shutter lag, high capture speed, and high sensitivity. The
signature cameras here are usually digital SLRs with relatively large pixels and somewhat lower
resolution than cameras for controlled settings.
•
Video capture, emphasizing high frame rate and low (1080P or smaller) resolution, simplifying
processing as needed to achieve satisfactory performance.
•
Compact cameras, emphasizing price, compact size, convenience, and sometimes style, at the
expense of other features, such as shutter lag and image quality.
Some applications require a blend of capabilities. For example, wedding photography often includes
capture under less controlled conditions than commercial photography, yet without the same need for
high capture speed as action photography.
Commercial digital cameras were introduced in 1991 and they have evolved rapidly since then.
At ﬁrst, quality problems from digital cameras were fairly dramatic, with relatively low resolution,
demosaicking and aliasing artifacts, poor color reproduction, and often signiﬁcant compression artifacts.
The evolution of digital cameras has dramatically improved image quality and increased the importance
of considering image quality in a systematic way. Changes to a camera to improve one aspect of
quality may not improve overall quality in every situation, and can even reduce it if the change drives a
limitation on some other aspect of quality. Further, as cost and convenience factors drive the development
of cameras with a wider variety of physical limitations, development of improved processing techniques
and assessment of the quality improvements increases in importance.
Most work on image quality in design and development of digital cameras has focused on speciﬁc
processing algorithms or hardware design issues. Usually, the work begins with a speciﬁc problem,
chooses an objective function tightly coupled to that problem, and demonstrates an improvement. One
example of this would be the demonstration of improved lossy image compression using mean square
error (MSE) and compression ratios to quantify performance. One weakness of this approach is that
simple objective functions rarely correlate well with human perception over a wide range of conditions.
Other work has speciﬁcally targeted the development of metrics that relate better with perceived
quality. Some work has started with models of the human visual system, such as color difference
models and contrast sensitivity functions, to provide a basis for quality measurement. Other work has
taken more of an information-theoretic approach, using features that have little direct relation to a model
of the human visual system. Work on quality metrics invariably compares quality metrics with subjective
judgments of image quality.
A smaller fraction of the literature focuses on engineering trade-offs, considering a design opti-
mization to balance conﬂicting objectives. One example would be the selection of spectral sensitivity,
maximizing sensitivity while minimizing noise and color errors. Often, this work uses an objective func-
tion based on human vision to provide a more reasonable evaluation of the importance of the various
artifacts.
Consideration of image quality, especially metrics that correlate well with human perception, has
several applications. One application is to compare changes in quality, such as testing which algorithm
provides lower reconstruction errors. If a quality metric supports the notion of a magnitude (e.g., a
just-noticeable difference, JND), it also helps quantify the signiﬁcance of algorithm or design changes.
A difference of one tenth of a JND is unlikely to be noticed much of the time, while three JNDs is a fairly
obvious difference. Further, perceptually based quality metrics can provide a more comprehensive way

4.02.2 Digital Camera Image Processing Chain
13
to evaluate system optimizations. Finally, with the accumulation of stored media assets, quality metrics
can play a role in asset selection and management.
Modeling of image quality for design of color cameras depends primarily on understanding of color
science and image and signal processing. Some familiarity with optics, human vision, and psychomet-
rics is also valuable. This chapter assumes some familiarity with integral calculus, statistics, signal
processing, and Fourier transforms.
This chapter emphasizes the relationships between hardware, processing, and the resultant image
quality. It will introduce a digital camera image processing chain in Section 4.02.2. Section 4.02.3
reviews camera hardware engineering issues affecting image quality, followed by a discussion of sys-
tem and image quality modeling in Section 4.02.4. Following is a more in-depth discussion of image
processing for color cameras in Section 4.02.6. Applications for quality modeling with digital cameras
are discussed in Section 4.02.7 and open issues and trends in digital cameras are discussed in Section
4.02.8. Section 4.02.9 lists sources for software of interest and Section 4.02.10 lists sources of image
collections and quality data.
4.02.2 Digital camera image processing chain
A signiﬁcant amount of image processing is required to convert a raw sensor image into a ﬁnished image
with sufﬁcient image quality. Figure 2.1 shows a ﬂowchart of a typical digital camera image processing
chain, to be used as a point of reference for subsequent discussion [1]. As can be seen in the ﬁgure,
some of the image processing actually occurs before the capture of the color ﬁlter array (CFA) image
(raw sensor image). Another important point to note is that the ordering of the sequence of operations
A*
CFA Image
Defect
Concealment
Noise Reduction
White Balance
Demosaicking
Color and Tone
Correction
Edge
Enhancement
Resizing
RGB Image
Compression
Compressed
Image
FIGURE 2.1
Digital camera image processing chain.

14
CHAPTER 2 Image Quality in Consumer Digital Cameras
in the ﬂowchart is ﬂexible. The noise reduction operation is particularly noteworthy, as not only can it
occur at different locations, e.g., after color and tone correction, it can occur at multiple locations in
the chain. Finally, not all blocks may be necessary, e.g., resizing, and some blocks may be added as
appropriate, e.g., motion deblurring.
Before the camera shutter button is fully depressed, a number of camera control algorithms are
already running to evaluate the focus, exposure, and white balance of the scene and to adjust the camera
capture parameters accordingly. Collectively, these are known as the “A*” algorithms for autofocus,
autoexposure,andautowhitebalance. Beforetheshutterbuttonhasbeeninitiallytouchedbytheuser(the
so-called “S0” state), these algorithms may already be running in a coarse adjustment mode sufﬁcient
for displaying a live image or preview on the camera’s electronic viewﬁnder. When the shutter button
is pressed halfway down (the “S1” state), the A* algorithms may invoke more advanced calculations
based on the implied user feedback that the region of interest in the scene is in the center of the image.
Once the user presses the shutter button completely (the “S2” state), one last reﬁnement iteration of A*
may be performed before capturing the CFA image. As can be seen from this scenario, when a user
does a “punch through” of the shutter button, i.e., goes from S0 directly to S2 without pausing at S1, a
signiﬁcant opportunity for improving the image quality of the raw capture is lost, as some of the more
advanced calculations do not occur.
Once the CFA image has been captured, the ﬁrst image processing task is to conceal the presence
of any defective pixels, which is usually accomplished by replacing defective pixel values with some
average of neighboring operating pixel values. As simple as this sounds, the surprising challenge is
determining which pixel values are defective in the ﬁrst place. While some pixels permanently fail into
full-on or full-off states and can be correspondingly mapped, a large number only partially fail in any
number of ways, both spatially and temporally. It is largely for this reason that explicit defective pixel
concealment operations are often discarded in favor of a more generic noise reduction operation. The
image quality trade-off of this approach is replacing a localized pixel adjustment with minimal artifact
potential by a global image processing operation with signiﬁcantly higher odds of producing unwanted
side effects.
Whereas noise reduction is an extremely well-known and understood image processing operation,
the location of this operation is notably prior to demosaicking, as shown in Figure 2.1. Given that the
image data is sparsely represented, with only one color channel value at each pixel location, adjustments
must be made to the noise reduction operation. One can simply adjust the noise reduction operation
to include only like-colored neighboring pixels. This results in a grayscale or luminance-only noise
reduction scenario at, typically, half the spatial resolution of the corresponding full-color image. To be
able to address Nyquist frequency luminance and chrominance noise of all spatial frequencies requires
more sophisticated strategies. Of course, one may simply move the noise reduction operation to after the
demosaicking operation, when a full-color image is available and standard noise reduction techniques
can be applied. The resulting image quality consequences rest with the robustness of the demosaicking
operation to noise in the CFA image.
The automatic white balance operation following the noise reduction block in Figure 2.1 is a slightly
different opportunity from the earlier A* version to correct the white point of the image. A detailed
analysis of the distribution of colors within the captured image can be performed, in order to discard
the confounding results of highly saturated colors and to estimate the true illuminant source and color
temperature. The accuracy of this white balance operation is important as it directly impacts the results

4.02.2 Digital Camera Image Processing Chain
15
of later color correction operations, and may have a strong inﬂuence on other operations that assume
properly white-balanced image data on input, such as demosaicking.
Demosaicking is the image processing operation most unique to and identiﬁed with digital cameras.
For each pixel location with only one color value to start, neighboring pixel values of all colors are
combined to estimate the missing two color values, thereby producing a full-color triplet. The approach
can be as simple as averaging like-colored pixel values to produce the missing two pixel color values.
At the other extreme, color channel correlation can be leveraged to permit the mixing of two or three
colors to produce a given missing color value. The support region for the averaging can be a standard
square or circular area, or a steerable shape responsive to local edge activity. Additional constraints can
also be applied to avoid unnatural or inconsistent discontinuities between adjacent pixel neighborhoods.
Finally, frequency domain techniques have also been described and found to be effective. From an image
quality perspective, the more computational effort that goes into demosaicking, generally the better the
result. With still photography, where compute time is permitted to take the greater part of a second,
the extra demosaicking calculations to produce a higher quality image are generally acceptable. In the
case of video photography, however, calculations must be done at video rates and a simple nonadaptive
neighborhood averaging is generally all that is feasible. The lower resolution of video frames, the
effects of temporal averaging during video playback, and the lossy compression of the video sequence
combined tend to minimize the impact of the design of the demosaicking algorithm. Still, as these
mitigating factors are improved, a revisiting of the methods chosen for demosaicking of video frames
may eventually be required.
Color and tone correction is the step that converts the uncalibrated color and tone of the image into
a calibrated color and tone space. Up to this point in the image processing chain, the representation
of the scene has been in terms of the image sensing capabilities of the sensor and capture optics. This
means, for example, that the colors are in terms of the spectral responsivities of the color ﬁlter array
components. Due to the difﬁculties and stresses of the manufacturing process and the requirements for
a rugged sensor that has some robustness to environmental changes (temperature, humidity, vibration,
and shock, etc.), there are only a limited number of materials that can be used in the construction of
CFA ﬁlters. This, in turn, constrains the number of possible spectral responsivities that can be achieved.
In a similar manner, the dynamic range of the captured image is limited by the photocharge storage
capacity of the individual pixels in the sensor. Most display scenarios (both hardcopy and softcopy)
require a nonlinear mapping of photocharges to display values in order to “look right” from a human
visual system perspective. The color and tone correction block typically applies a matrix transform to
the colors and a nonlinear mapping to the tone (luminance response) of the image in order to achieve
an acceptable image quality. The resulting colors and tone are in reference to some industry standard in
order to make the resulting image as compatible as possible over a wide range of applications. Note that
this is the ﬁrst block in Figure 2.1 where it is possible to depart from simply producing a scientiﬁcally
accurate rendering of the image and, instead, produce a perceptually preferred result. This could be
done, for example, by producing colors that are more highly saturated and vibrant and a tone that is
higher in contrast than existed in the original scene. Such departures from “reality” are frequently, if
not routinely, considered to be image quality improvements.
Edge enhancement, or sharpening, attempts to restore the ﬁdelity of the higher spatial frequencies
of the image that were distorted by the capture and image processing mechanisms. Typically, the image
is treated as having been passed through some kind of spatial low-pass ﬁlter that diminishes, but not

16
CHAPTER 2 Image Quality in Consumer Digital Cameras
eliminates, higher frequency components. While a sharpening ﬁlter based on the mathematical inversion
of the presumed low-pass operation could be employed, this usually runs afoul of numerical instabilities
intheinversionprocessandsigniﬁcantnoiseampliﬁcationnearthecutofffrequencyofthelow-passﬁlter.
Finally, from an image quality perspective, a mathematically accurate restoration of the high-frequency
spatial components of an image will usually be perceived as producing an image that is undersharp
or softly focused. The term edge enhancement is used to imply that a mathematical restoration of
overall image sharpness has been abandoned in favor of a perceptually preferred level of sharpness.
This preferred level of sharpness is usually achieved by amplifying the high-frequency components of
the image with standard high-pass convolution ﬁlters, followed by some nonlinear processing of the
sharpened result in order to reduce unwanted noise ampliﬁcation.
Resizing is an optional image processing operation that, like noise reduction, can occur at different
locations within the image processing chain. It can serve one of two functions: overall image resolution
change and aspect ratio correction. One example of the former is producing a lower-resolution video
frame from a full-resolution raw image capture. In this case, the resizing operation would generally
occur early in the image processing chain, perhaps even before defect concealment. At the opposite
extreme, a higher-resolution image may be generated from a full-resolution image in order to improve
the image quality when printed as an 8 × 10 in. or larger print. This super-resolution problem attempts
to improve upon the high spatial frequency reconstruction capabilities of purely linear operations, such
as bicubic interpolation. With aspect ratio correction, the resizing operation is usually compensating
for the use of non-square pixel sampling grids in the sensor or anamorphic image capture optics, i.e.,
optics with different magniﬁcations in the horizontal and vertical directions. This compensation takes
the form of differing resolution changes in the horizontal and vertical directions.
The resulting fully processed image (RGB Image in Figure 2.1) usually needs to be saved to some
storage medium for subsequent use. With 10+ megapixel sensors in today’s consumer digital cameras,
the resulting 30+ megabyte fully processed image can lead to a rapid consumption of storage media
and extended time delays due to the amount of data being read or written. Compression is a means
of reducing data redundancy that naturally occurs in digital camera images in order to signiﬁcantly
minimize the size of the corresponding image ﬁle. While mathematically lossless compression provides
only modest ﬁle size reductions of around 2:1, near-perceptually lossless compression, such as JPEG,
can produce ﬁle size reductions of 5:1 or greater. The basic idea behind JPEG and its successor JPEG
2000 is to transform the image into a different representation (spatial frequency or wavelets) and reduce
or discard the resulting components that are of low or negligible perceptual importance. In this way,
image quality and compression ratio can be directly traded off against one another in order to achieve
the desired image ﬁle size compromise. This operation produces the ﬁnal, compressed image as in
Figure 2.1.
4.02.3 Camera engineering
The quality of a captured image is often limited by the interaction of multiple hardware and processing
constraints. Camera design and conditions in the scene determine which of the degradations are most
limiting. For example, the lens, sensor, and pixel size in a digital SLR are normally much larger than

4.02.3 Camera Engineering
17
Taking Lens
Filters
Cover
Glass
Sensor
FIGURE 2.2
Optical image path of a digital camera.
those in a compact camera. Because of this, a capture of a given scene from a digital SLR normally
has a higher signal-to-noise ratio than a capture of the same scene from a compact consumer camera.
Still, most compact cameras can capture a good image of a brightly lit scene. The higher signal-to-
noise ratio in the D-SLR capture provides a relatively small improvement in perceived image quality
when capturing this type of scene. However, the improvement in signal-to-noise ratio makes a large
difference when capturing images of an indoor sporting event, when captures from a compact camera
will be substantially degraded.
The primary camera engineering and design issues are discussed in the following sections, in the
order they occur in the imaging chain. Section 4.02.3.1 discusses the many impacts of the taking lens and
optical system, including interactions with the sensor. Section 4.02.3.2 follows with a brief overview
of color ﬁlter array design. Section 4.02.3.4 delves into spectral sensitivity. Section 4.02.3.5 discusses
camera sensitivity and noise.
4.02.3.1 Optical issues
Figure 2.2 is a simpliﬁed diagram of a digital camera optical imaging path. The taking lens is designed
to focus an image on the surface of the sensor. Antialiasing and infrared cutoff ﬁlters prevent unwanted
spatial and spectral scene components from being imaged. A cover glass protects the imaging sur-
face of the sensor from dust and other environmental contaminants. The sensor converts the inci-
dent radiation into photo-charges, which are subsequently converted to photo-currents that are fed
to an analog-to-digital converter. The output of the A-to-D converter is then stored as raw image
data.
There are several classes of quality problem that depend on the lens and its interaction with the
sensor. Each problem will be discussed in turn, starting with problems that depend solely on the lens
and progressing to problems that depend more on the interaction of lens and sensor.
4.02.3.1.1
Geometric distortion
The ﬁrst quality issue discussed here is geometric distortion, a characteristic that depends solely on the
lens. To keep the cost of the capture (taking) lens as low as possible in consumer digital cameras, its
design is kept as simple as possible. However, the undesirable result of this simplicity is that often too
few degrees of design freedom have been retained to fully address the problems of optical aberrations.

18
CHAPTER 2 Image Quality in Consumer Digital Cameras
H
ΔH
(a)
(b)
H
ΔH
Barrel distortion
Pincushion distortion
FIGURE 2.3
Geometric distortion.
One of the more difﬁcult and expensive aberrations to correct is geometric distortion, a change in
magniﬁcation that is usually quadratic with distance from the center of the image.
The most visible symptom of geometric distortion is curvature in lines that should be straight, as
shown in Figure 2.3. A common measurement of geometric distortion is TV distortion, which quantiﬁes
the bending of lines near the edge of the image. The European Broadcasting Union (EBU) standardized
on a distortion that measures the bend in a single line near the top of the image. EBU TV distor-
tion is D = H
H . H is a signed value, positive for pincushion distortion and negative for barrel
distortion.
Geometric distortion is commonly classiﬁed as either barrel or pincushion distortion. Barrel distor-
tion, shown in Figure 2.3a, refers to a magniﬁcation that decreases with distance. Pincushion distortion,
shown in Figure 2.3b, refers to magniﬁcation that increases with distance. Parametrically, barrel dis-
tortion is characterized by negative TV distortion values, while pincushion distortion is characterized
by positive values. More complex patterns, usually occurring in lenses and cameras with substantial or
partially corrected geometric distortion, are not completely described by a single TV distortion value.
As more complex distortion becomes more common with compact and inexpensive cameras, more
comprehensive methods to quantify distortion continue to be developed.
The quality impact of distortion is scene dependent, since the visibility of distortion is greatly
increased by straight edges near the edge of the ﬁeld of view. Figure 2.4 illustrates geometric distortion
with a scene. Compared to the undistorted image in Figure 2.4a, the image points in the center of
Figure 2.4b are spread apart more than the points at the edges, somewhat like the lines on the surface
of a barrel. Figure 2.4c presents the opposite effect, with the image points in the center of the image
being closer together than at the edges, as if someone had stuck a big pin into the middle of the image.
The quality impact also depends on expectations for the distortion. Distortion is much more difﬁcult
to correct and therefore more common with wide angle lenses, so it tends to be tolerated more in wide
angle lenses than telephoto lenses. Algorithms that combine multiple frames, such as stitching frames
into a panoramic image, require essentially zero distortion. Such algorithms often require or include
distortion correction, discussed in Section 4.02.6.5.1.

4.02.3 Camera Engineering
19
(a)
(b)
(d)
(c)
No distortion
Barrel distortion
Chromatic aberrationn
Pincushion
FIGURE 2.4
Distortion example. (For interpretation of the references to color in this ﬁgure legend, the reader is referred
to the web version of this book.)
4.02.3.1.2
Chromatic aberration
The second class of quality problems to be discussed is chromatic aberration, which also depends almost
completely on the lens. The two main types of chromatic aberration are illustrated in Figure 2.5. Lateral
chromatic aberration is illustrated in Figure 2.5a, in which magniﬁcation (geometric distortion) varies
signiﬁcantly with wavelength, i.e., color channel. In the ﬁgure, each of the three color channels has a
different magniﬁcation. Figure 2.4d shows an example of how color dependent distortion can lead to

20
CHAPTER 2 Image Quality in Consumer Digital Cameras
Sensor
magniﬁcation
(a) Lateral
Sensor
focal plane
(b) Longitudinal
FIGURE 2.5
Chromatic aberration. (For interpretation of the references to color in this ﬁgure legend, the reader is referred
to the web version of this book.)
lateral chromatic aberration, producing colored edge fringes in a scene. In this case, the distortion of
the green channel was slightly larger than the distortion in the red and blue channels. This results in
green and magenta (red plus blue) fringing. By changing which color channels are different and by how
much, a number of fringe color combinations can be created, e.g., blue and yellow.
Lateral chromatic aberration can be corrected by applying geometric distortion correction that
depends on color channel as well as position. Small amounts (a pixel or less) of lateral chromatic
aberration are common near the corners of images. Small aberrations are sometimes handled in image
processing steps designed to mask small errors, such as with color interpolation. Larger amounts (mul-
tiple pixels) of lateral color quickly become objectionable and are usually corrected using techniques
designed speciﬁcally for lateral color, such as the previously mentioned channel-dependent correction
of geometric distortion.
Longitudinal chromatic aberration, illustrated in Figure 2.5b, occurs when the focus distance varies
with wavelength. This results in only one color band of an image being in best focus, while the others
are slightly out of focus. In Figure 2.5b, the green channel is in best focus. The red channel is focused
below the surface of the sensor, and the blue channel is focused above the surface of the sensor. Many
cameras use a sensor with a Bayer color ﬁlter array (CFA) and image processing based on a luminance-
chrominance model, taking most of their image detail from the green channel [2]. Such cameras tend
to be more tolerant of small amounts of longitudinal chromatic aberration, since the image processing
expects lower resolution red and blue channel input. Larger amounts of longitudinal color aberration
can create quality problems unless mitigated by additional processing.
4.02.3.1.3
Response nonuniformity
The third class of quality problem that depends on the lens is relative illumination. It is controlled
mostly by lens design, although the interaction of sensor and lens plays a role as well. Many lenses have

4.02.3 Camera Engineering
21
object
focal plane
F
FIGURE 2.6
Image space telecentricity.
internal elements that block a portion of the cone of light rays destined for the corners of the image,
known as vignetting. This effect is largest at wider apertures, since the cone of rays is larger.
Even without the effect of vignetting, a Lambertian source of light (radiating light uniformly at all
angles) illuminating an area sensor will have falloff in image brightness depending on the angle of light
rays relative to the sensor. This is alleviated by having a telecentric lens, illustrated in Figure 2.6. The
cone of rays coming from the top of the object, in red, has the same width and orientation at the focal
plane as the cone of rays coming from the base of the object, in blue. The advantage of a telecentric
taking lens is that the cone of rays incident on each pixel is the same in size and orientation regardless
of location within the image. As a result lens falloff, i.e., the darkening of image corners relative to the
center of the image, is avoided.
In a telecentric lens, the size of the rear element of the lens must be larger than the sensor, and
the overall size of the lens must be large enough to accommodate all of the light ray bundles. This is
particularly challenging with a large aperture. In the low-cost imaging environment of consumer and
mobile phone digital cameras, such cost and size issues can become severe. As a consequence, perfect
telecentricity in digital cameras is usually sacriﬁced, either partially or completely. These latter cameras
can exhibit lens falloff, which may be considered an acceptable tradeoff at the price point of the camera.
Nonuniformity caused by a non-telecentric lens is exacerbated when the sensor pixel pitch is small.
As image sensors evolve toward smaller pixel sizes, the room available to collect photons is reduced
because the circuitry needed to operate the sensor also requires space. For example, an active CMOS
sensor pixel design is illustrated in Figure 2.7. This ﬁgure shows a design in which four pixels share
common readout circuitry, reducing the space required for the circuitry. Still, it is apparent the circuitry,
shown by colored traces, takes roughly 75% of the area, leaving only 25% of the area for sensing light,
shown in gray. This problem is not unique to CMOS sensors. CCD sensors, especially interline sensors,
face this problem as well.
Image sensors often have arrays of microlenses, or lenslets, applied to the surface of the image
sensor, to focus light rays for each pixel into the photosensitive portion of the pixel. Figure 2.8 shows
a side view of a CMOS sensor design with lenslets, above the color ﬁlter array, focusing light rays on
the collection sites of the sensor. The layers of wires (gray rectangles) above the photosites forces the

22
CHAPTER 2 Image Quality in Consumer Digital Cameras
FIGURE 2.7
Example active CMOS pixel layout.
CFA
FIGURE 2.8
Lenslet example.
lenslets and color ﬁlter array to be located several µm above the photosites, but the lenslets direct most
of the light for each pixel into the photosensitive area. This ﬁgure is based on a design for a 1.75 µm
pixel pitch.
Figure 2.9 illustrates the same design, but with light rays entering at different angles. In actual
cameras, light ray entry angles will not change this much for ﬁve adjacent pixels on the sensor, but it
is fairly common for the chief ray angle to vary this much over the area of the sensor. This illustrates

4.02.3 Camera Engineering
23
CFA
FIGURE 2.9
Lenslet example—varying incident angles.
two signiﬁcant problems that occur with angled illumination. The ﬁrst is that with an increased angle
of incidence, many light rays strike the wiring layers rather than the photosensitive layer of the sensor,
lowering the quantum efﬁciency of the pixels. The second problem is that at larger angles, the light rays
strike the adjacent pixel, increasing crosstalk between pixels. As emphasized by the color ﬁlter array
in the ﬁgure, adjacent pixels are intended to sense different colors, so varying crosstalk between pixels
creates spatially varying color sensitivity.
The market demand for compact imaging devices drives most of the sensor industry toward smaller
pixels. Several techniques are used to reduce the variation in sensitivity as a function of angle of
incidence. One is the use of “light pipes,” using transparent material with a high index of refraction over
the photosensitive area, to do a better job of reﬂecting light striking the wiring layers downward toward
the photosensitive area. The second technique is backside illumination, effectively ﬂipping the sensor
upside down just below the color ﬁlter array and moving the wires below the photosensitive surface.
This complicates the manufacturing process and has other problems, such as relatively high electronic
crosstalk, but the variation in response due to changes in chief ray angle is greatly reduced.
The human retina provides an interesting comparison to the sensor. The retina of the human eye has
layers of nervous tissue and blood vessels on top of the light sensitive cells, similar to the frontside
illumination of most image sensors. Fortunately, these layers do not create the same problems in the
eye that circuitry does in the sensor. The layers of tissue in the eye are more transparent than the wires
in the sensor, and the spectral sensitivity of the cones in the eye is controlled by pigmentation in the
cone cells, not in a separate color ﬁlter array layer.
4.02.3.1.4
MTF—sharpness, aliasing
The ﬁnal major impact of the optical system on image quality is the ability of the lens to image points
in the scene as points on the image sensor. This can be described by the point spread function, the shape
a point in the scene takes when projected onto the sensor. This can also be described by the Fourier
Transform of the point spread function, known as the Optical Transfer Function (OTF). An ideal lens
has OTF = 1.0 at all spatial frequencies. Most of the quality impact can be described by the magnitude
of the OTF, known as the Modulation Transfer Function (MTF).

24
CHAPTER 2 Image Quality in Consumer Digital Cameras
0
50
100
150
200
250
300
350
400
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
f/2.8
f/4.0
f/5.6
f/8.0
f/11
1.5 μm
2.0 μm
3.0 μm
5.0 μm
Frequency, cycles ⋅ mm−1 on sensor
Response
FIGURE 2.10
Diffraction-limited lens MTF.
The MTF of the taking lens varies with the size of the aperture, affecting image sharpness. At large
apertures (small f/numbers), lens aberrations limit the MTF, while the lens response usually approaches
a diffraction-limited MTF at high f/numbers. As shown in Figure 2.10, the MTF of a diffraction-limited
lens drops rapidly as the f/number increases. Many real lenses provide the best response when the
aperture is closed down two stops from wide open (1/2 the maximum aperture). Their MTF is lower at
both larger apertures (due to aberrations) and smaller apertures (due to diffraction). For image sensors
with small pixels, the individual pixels can easily be smaller than the resolution limit of the lens. The
resolution limit of a diffraction-limited lens is usually considered to be d = 1.22λA, where λ is the
wavelength of light and A is the f/number. The spatial frequency corresponding to the resolution limit for
eachdiffraction-limitedMTFis showninFigure 2.10 by a small circle on the curve near 9% response. For
comparison, the half-sampling frequencies for several different pixel pitches are also shown in the ﬁgure.
Lens MTF and the sensor can interact to produce aliasing artifacts when the lens has signiﬁcant
response past the half-sampling frequency (sometimes known as the Nyquist frequency). Cameras
with larger pixels can easily demonstrate highly objectionable aliasing artifacts. Two simple treatments
for aliasing in cameras with large pixels are to stop down the lens to a small enough aperture, or to
deliberately defocus the image. Neither of these approaches is always convenient or desirable, however.
It is common for cameras with larger pixels to include an anti-aliasing ﬁlter, speciﬁcally designed to
attenuate higher spatial frequencies, while preserving lower spatial frequencies as much as possible [3].
The most common type is a birefringent ﬁlter that splits an incoming point of light into multiple spots,
typically four or more, that insert extra zeros into the MTF, depending on the spacing of the spots.
Figure 2.11 illustrates a four-spot pattern where the spacing of the spots from the anti-aliasing ﬁlter

4.02.3 Camera Engineering
25
FIGURE 2.11
Four spot anti-aliasing ﬁlter pattern.
matches the pixel pitch of the sensor, introducing a zero at exactly the half-sampling frequency in
both the horizontal and vertical directions. This is a common approach, although the spot spacing
need not precisely match the pixel pitch, depending on the color sampling pattern, the lens, and the
image processing path. Bireﬁngent anti-aliasing ﬁlters preserve lower frequencies slightly better than
a stopped-down or defocused lens for a given spatial cutoff, thus providing a sharper image while
mitigating aliasing problems.
Aliasing is highly objectionable when observed, but it occurs relatively rarely, even when a camera’s
design makes it vulnerable to aliasing. This is because aliasing is most objectionable when the scene
includes areas with regular high-frequency patterns, such as textiles, fences, or other man-made items.
With this kind of content, the lower frequency spatial sampling of the high-frequency scene content
causes the higher frequencies to fold over to very low spatial frequencies, producing moiré patterns, with
dramatic results such as illustrated in Figure 2.12. Most scenes do not include regular high-frequency
patterns. For example, while tree branches silhouetted against a sky exhibit high contrast at high spatial
frequencies, the irregularity of the scene content usually prevents moiré patterns.
4.02.3.2 Sensor sampling
The Bayer color ﬁlter array sampling pattern, using a checkerboard of pixels to sense luminance detail
and two more coarsely sampled color channels is the most common color ﬁlter array used in digital
cameras as of 2011 [2]. Several variations on the Bayer CFA are illustrated in Figure 2.13. Figure 2.13a
shows the most widely used version, where red, green, and blue color channels are sensed directly
and the green channel provides the luminance image as well as capturing green spectral information.
Figure 2.13b illustrates another variation used in a few cameras, in which a subtractive set of pri-
maries (magenta, yellow, cyan) is sensed, using the yellow channel for the luminance channel. This
pattern increases the quantum efﬁciency of each pixel, improving the signal-to-noise ratio in the raw
image, but also making color correction back to a set of RGB primaries for display more difﬁcult.
The difﬁculty of color correction is discussed in more depth in Section 4.02.3.4. The pattern in Figure
2.13c uses panchromatic pixels with essentially no color ﬁlter for the high-resolution data. This pattern
also improves sensitivity at the expense of a more challenging color correction operation. All of these
patterns are more vulnerable to aliasing in the less densely sampled color or chrominance channels.

26
CHAPTER 2 Image Quality in Consumer Digital Cameras
(a) Color moiré scene
(b) Color moiré inset
FIGURE 2.12
Color moiré example.
(a) RGB
(b) MYC
(c) RPB
FIGURE 2.13
Example Bayer color ﬁlter patterns. (For interpretation of the references to color in this ﬁgure legend, the
reader is referred to the web version of this book.)

4.02.3 Camera Engineering
27
(a) High sensitivity
(b) Frequency-domain demosaicking
FIGURE 2.14
Example color ﬁlter patterns designed to meet different objectives.
In these sampling patterns, the pixels used to sense the luminance image are generally not sensing a
colorimetrically accurate luminance response, discussed in more detail in Section 4.02.3.4. Rather, it is
usually the channel with the highest sensitivity that is selected for the most dense sampling.
While variations on the Bayer pattern are very common, other patterns have been designed to achieve
different objectives. Two example patterns are shown in Figure 2.14. The pattern shown in Figure 2.14a
was designed at Eastman Kodak Company to provide improved sensitivity in the context of a color
camera with very small pixels and an inexpensive lens [4]. Because of the small pixel size, aliasing,
even color aliasing, is unlikely, so the color sampling was made more coarse to provide space for the
more sensitive panchromatic pixels. The sampling pattern was selected to provide a color image with a
Bayer pattern when read out at half-resolution, allowing some re-use of processing steps developed for
the Bayer pattern.
A digital SLR with larger pixels and a high lens MTF has different quality constraints. The ability
to collect light is much greater, as is the potential for aliasing, depending on the lens characteristics.
The pattern shown in Figure 2.14b was designed by Hirakawa and Wolfe to minimize the overlap
of the luminance and chrominance channel peaks in the frequency domain, and simulations showed
excellent results using a frequency-domain demosaicking [5]. This is one example of several recent
works considering optimal color sampling patterns from a theoretical perspective.
4.02.3.3 Temporal sampling
Sampling in a digital camera is most often considered spatially, as in Section 4.02.3.2. However, video
cameras sample a scene temporally as well. When capturing video, the frame time (such as 1/30 s)
controls the displacement of moving scene content between frames, while the exposure time controls
the motion blur. The ratio of the exposure time to the frame time, sometimes known as the shutter

28
CHAPTER 2 Image Quality in Consumer Digital Cameras
duty cycle, controls the motion blur within a frame relative to the displacement between frames. A
very low duty cycle can produce a video sequence with very sharp frames that have large motion
displacements, similar to some stop-motion animation. A higher duty cycle increases the motion blur
in each frame, reducing the stop-motion effect. Empirically, a duty cycle of 50% usually provides a
reasonable compromise of quality degradation caused by motion blur and quality degradation caused
by stop-motion artifacts.
Digital cameras with CMOS sensors (such as those in compact video cameras and mobile phones)
normally have a rolling shutter mechanism for electronic exposure control, leading to further complexity
in temporal sampling. Rather than an entire frame being exposed at the same time, each row of the sensor
is exposed and read out at a slightly different time. Often, the last row in one frame is read out from
the sensor just before the ﬁrst row in the next frame. With this rolling shutter, rapid motion, caused by
unstable video capture or fast-moving scene content, can produce complex geometric distortions within
a captured image. A fairly simple example is shown in Figure 2.15. Figure 2.15a shows a merry-go-round
with some geometric distortion arising from a short focal length lens, but note the pipes near the center
of the image are vertical, whether they are in the foreground or the background. Figure 2.15b shows
the same scene, but with the merry-go-round rotating and the pipes in the foreground moving to the
left of the image. While the fence posts in the background are unchanged, the pipes on the rotating
merry-go-round exhibit a shear artifact. Because the pipes in the foreground are moving right to left, the
pipes are farther to the left when the bottom of the pipes are exposed, slanting the pipes. The direction
and amount of shear depends on the direction and velocity of the pipe, leading to different shear in
different pipes. If the motion changes from frame to frame, a video will contain a series of images with
different complex distortions, providing effects that can be quite disturbing.
In contrast, sensors with a global shutter mechanism, such as interline CCD sensors, allow transfer
of captured photo-charges from all pixels to light-shielded buffer locations (e.g., vertical shift registers)
simultaneously, where they are stored for subsequent readout. In the example of Figure 2.15, a capture
of the moving scene with a camera using global shutter shows no distortion, since all rows of the image
are exposed at the same time.
(a) Still
(b) Moving
FIGURE 2.15
Rolling shutter motion distortion.

4.02.3 Camera Engineering
29
The quality degradations arising from temporal sampling are not easily quantiﬁed, because they
can produce complex perceptual artifacts. The trend in camera development is to increase the speed of
sensor readout to minimize temporal artifacts, especially those caused by rolling shutter exposure.
4.02.3.4 Spectral sensitivity
Color is the human response to the shape of the spectral power distribution of light entering the eye
from a scene, essentially based on the following three integrals over wavelength:
X = kXY Z
 λmax
λmin
S(λ) R(λ) x(λ) dλ,
Y = kXY Z
 λmax
λmin
S(λ) R(λ) y(λ) dλ,
Z = kXY Z
 λmax
λmin
S(λ) R(λ) z(λ) dλ.
In these equations, λ is wavelength, S(λ) is an illuminant spectral power distribution, and R(λ) is
spectral reﬂectance. The functions x(λ), y(λ), and z(λ) are color matching functions standardized by
the CIE, shown in Figure 2.16. The constant kXY Z is a normalization factor and X, Y, and Z are standard
400
450
500
550
600
650
700
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Wavelength (nm)
Relative Sensitivity
 
 
xy
z
FIGURE 2.16
XYZ color matching functions. (For interpretation of the references to color in this ﬁgure legend, the reader
is referred to the web version of this book.)

30
CHAPTER 2 Image Quality in Consumer Digital Cameras
tristimulus values. Different spectral stimuli are perceived as having matching colors if they produce
the same three tristimulus values [6].
The fundamental requirement for accurate color capture is to have spectral sensitivity that matches
a set of color matching functions, either the curves from Figure 2.16 or linear combinations of them. If
a camera satisﬁes this objective well, accurate color reproduction can be achieved under all conditions.
Color correction cannot completely compensate for deviations from color matching functions, since the
fundamental problem is that spectra perceived as near matches by human observers will not be sensed as
matching by the camera, and vice versa. No matter how color processing transforms the colors measured
by the camera, this confusion of matching and non-matching colors will cause errors.
Deviations from color matching functions tend to be driven by manufacturing processes that constrain
thespectralresponseofcomponentsaffectingthespectralquantumefﬁciencyofthecamera.Inparticular,
the infra-red (IR) ﬁlter has a large effect on color reproduction. The solid colored curves in Figure 2.17
are an example set of peak-normalized sensor spectral sensitivity data. The solid black curve is the
transmittance of an example IR cut ﬁlter, and the colored dashed curves are the overall spectral response
of the camera including the IR ﬁlter. Without the IR ﬁlter, response outside the visual range is signiﬁcant,
especially in the near IR wavelength range. The visual impact of this spurious sensitivity is most obvious
under an illuminant with a high proportion of infra-red energy, such as tungsten ﬁlament illumination.
Cameras with poor IR ﬁlters usually have noticeable desaturation of colors under tungsten illumination.
400
450
500
550
600
650
700
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Wavelength (nm)
Relative Sensitivity
FIGURE 2.17
Example camera spectral sensitivities with IR ﬁlter. (For interpretation of the references to color in this ﬁgure
legend, the reader is referred to the web version of this book.)

4.02.3 Camera Engineering
31
Heat sources such as candle ﬂames can also produce striking quality problems, as they tend to image
as magenta without adequate IR ﬁltration.
Aside from the IR cut ﬁlter, the spectral sensitivity of the camera is largely controlled by the spectral
sensitivity of silicon and by the color ﬁlter array on the sensor. Electronic and optical crosstalk (dis-
cussed in Section 4.02.3.1.3) provide a smaller effect on the spectral sensitivity of the sensor, generally
increasing the off-peak response of each color channel.
As shown in Figure 2.17, typical camera spectral sensitivities are not very close to the XYZ color
matching functions in Figure 2.16. Selection of spectral sensitivity for a camera involves balancing the
cost of errors in captured colors against the cost of encoding the data and the cost of correcting the colors
for display. The color matching functions shown in Figure 2.16 correspond to the XYZ primaries shown
in Figure 2.18. Because the XYZ primaries lie outside the spectral locus [7], they are not physically
realizable for any display and a large part of the encoded color space is wasted. Three other standard
sets of primaries are also shown in Figure 2.18. The sRGB primary set, based on the primaries speciﬁed
for standard HDTV displays, is used by many softcopy displays, but has a relatively restricted color
gamut. The Adobe® RGB (1998) color encoding, developed by Adobe Systems Incorporated, uses a
set of primaries that are a compromise between the gamut of sRGB and the XYZ primaries, encoding
more colors than sRGB without encoding as many colors that cannot be displayed. These color spaces
and several others are reviewed in [8]. The RIMM (Reference Input Medium Metric) color encoding,
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
CIE x
CIE y
 
 
360
500
520
550
600
830
Spectral Locus
XYZ
sRGB
RIMM RGB
AdobeRGB
FIGURE 2.18
Color gamuts for different sets of display primaries. (For interpretation of the references to color in this ﬁgure
legend, the reader is referred to the web version of this book.)

32
CHAPTER 2 Image Quality in Consumer Digital Cameras
developed by Kodak, uses primaries enclosing a gamut somewhat larger than the Adobe® RGB (1998)
primaries [9].
Captured colors must be color corrected to an output set of primaries for display. Generally, if the
display gamut (e.g., sRGB) is smaller than the capture gamut (e.g., RIMM), the color correction step
ampliﬁes noise, discussed further in Section 4.02.3.5. Captured colors that lie outside the displayable
gamut cannot be accurately displayed, limiting the value of capturing them at the onset. While advanced
gamut-mapping algorithms exist for mapping a captured color gamut to a display color gamut, consumer
cameras often simplify the task by capturing a more restricted gamut, generally somewhere between
the sRGB and RIMM gamuts.
4.02.3.5 Sensitivity and noise
Cost and design objectives drive most compact consumer cameras to use compact lenses and image
sensors with small pixel sizes typically between 1 and 2.5 µm. Because of these decisions, compact
digital cameras routinely operate with limited signal, making sensitivity a particularly important camera
attribute, and noise in the rendered image a typical quality degradation. Capturing a 1/30 s exposure
of a gray object under typical ofﬁce lighting, a typical camera with an f/2.8 lens and 1.5 µm square
pixels collects roughly 200 photo-charges. This limits the signal-to-noise ratio in the raw capture to 14
or less. For comparison, the ISO standard for camera sensitivity measurement refers to an image with a
signal-to-noise ratio of 10 as “the ﬁrst acceptable image” and an image with a signal-to-noise ratio of 40
as “the ﬁrst excellent image” [10]. A digital SLR might capture 8–32 times as much signal, primarily
through a larger pixel size. In the previous example, a digital SLR would capture between 1600 and 6400
photo-charges for the gray reﬂector, enabling the signal-to-noise ratio to approach 40–80. In addition
to a sensitivity advantage, larger pixels have a capacity advantage, as the maximum photo-charge that
can be accumulated within a pixel scales with pixel size.
There are two primary sources of random noise in a digital camera. The ﬁrst is Poisson-distributed
noise from the random process of photons being absorbed and converted to photo-charge within a pixel,
also called shot noise. The second is electronic read noise, modeled with a Gaussian distribution. A pixel
Q may be modeled as Q = kQ(q + g), where kQ is a gain to adjust sensitivity, q is a Poisson random
variable with mean mq and variance σ 2
q = mq, and g is a Gaussian random variable with mean mg and
variance σ 2
g . Because the two processes are independent, the signal-to-noise ratio is mq/

kQ

σ 2
q +σ 2
g

,
where mq is the mean original signal level (captured photo-charges) for and σ 2
g is the variance of the read
noise. Improvement in the signal-to-noise ratio in the raw image is accomplished by some combination
of increase in the collected photo-charges or reduction in the read noise.
As mentioned in Section 4.02.3.4, color correction normally ampliﬁes noise present in the captured
image. Color correction is usually implemented with a 3×3 matrix M. Table 2.1 shows example matrices
for cameras with typical spectral sensitivity and with ideal XYZ color matching functions. The column
labeled M shows the matrix to correct balanced camera data to sRGB primaries and the column labeled
K A shows the color ampliﬁcation matrix, K A = MMT . The variance in the color corrected image
is scaled by the values on the diagonal of K A. As shown in the table, ampliﬁcation of noise through
color correction with the typical spectral sensitivities is signiﬁcant, and the ampliﬁcation with the XYZ
sensitivities is much larger for the red and green channels. Some cameras adjust the color correction

4.02.3 Camera Engineering
33
Table 2.1 Summary of Color Correction and Color Ampliﬁcation Matrices
Primary set
M
KA
Typical
1.558
−0.531
−0.027
2.709
−0.895
0.292
−0.078
1.477
−0.399
−0.895
2.346
−1.340
0.039
−0.508
1.469
0.292
−1.340
2.418
XYZ
3.240
−1.537
−0.498
13.112
−6.045
−0.033
−0.969
1.876
0.0416
−6.045
4.460
−0.393
0.056
−0.204
1.057
−0.033
−0.393
1.163
matrix as a function of the ISO setting, reducing the color saturation of the image at higher ISO values
and lowering the noise level at the expense of less pleasing color reproduction.
4.02.3.6 Resource limitations
The quality of images from a digital camera is routinely limited by the resources available to the camera.
In addition to the optical limitations discussed in Section 4.02.3, cameras are designed with cost and
power constraints, and they must operate within time constraints.
Cost and power constraints place limits on the RAM available for buffering image data during pro-
cessing. Many cameras allow buffering of raw capture data for multiple images, but memory bandwidth
limitations are an incentive to avoid storing full images at various processing steps. Thus, the desire is
to process the image within a minimum of local data involved, such as processing a tile corresponding
to a JPEG compression block. This in turn makes it difﬁcult for spatial processing steps to affect very
low frequencies, such as for noise reduction.
While wavelet and other pyramid-based processing architectures have demonstrated beneﬁts for
applications such as adaptive tone scaling and noise reduction, the cost of implementing such architec-
tures has slowed their adoption. Most low-cost cameras use a specialized processing chain implemented
in proprietary hardware. At the camera integration and speciﬁcation level, the speciﬁcs of the process-
ing chain are sometimes not disclosed. This separation of processing chain from overall camera design
for low-cost cameras slows adoption of sensor designs with new color patterns, since the sensor and
processing path chips are often provided by different suppliers. Sensor suppliers have an incentive to
provide sensors compatible with existing processing paths, while suppliers of processing chips have an
incentive to be compatible with existing sensors. Evolution of processing paths and sensors is faster at
higher price points, where performance and image quality are given a higher priority.
Consumer cameras are expected to capture a still image in a fraction of a second, although some
low-cost cameras may require over half a second for capture. The rate limiting step is usually the
autofocus process. Processing the image after capture generally is required to be less than a second, and
often a small fraction of a second. Given the pixel counts typical for consumer digital cameras, this is
a signiﬁcant limitation on the complexity of the processing path.

34
CHAPTER 2 Image Quality in Consumer Digital Cameras
Similarly, consumer video cameras are expected to capture, process, compress, and store video frames
at high pixel rates, commonly over 60 million pixels per second (1920 × 1080 resolution at 30 frames
per second). The development of dual use cameras, capturing both video and still, creates a dichotomy
in the processing hardware. The readout and processing requirements are signiﬁcantly different in terms
of size and frame rate, although the overall pixel processing rate may be similar.
4.02.4 Quality modeling
Section 4.02.3 discussed the primary hardware attributes affecting quality of the captured image. How-
ever, objective metrics such as variance, signal-to-noise ratio, and MTF do not completely quantify
perceived image quality. Because the purpose of a consumer camera is to provide an image for human
viewing, quality is in the eye of the observer. While human vision is complex, models of speciﬁc aspects
of human vision allow useful prediction of human responses. Human color vision and spatial contrast
sensitivity are the two aspects most commonly used to convert objective measures to psychometric
correlates. This section adds the perceptual dimension of image quality to the preceding discussion of
system design attributes. Some approaches considering image quality begin with a digital image and
model the quality impact of distortions to that image. These approaches have indirect applicability to
the camera design problem, the focus of this section, but will be mentioned again in Section 4.02.8.
Because perception depends on what an observer sees, quality in a viewed image depends not only
on the captured image, but also on the viewing conditions for the image. A spatial contrast sensitivity
function is deﬁned with frequency expressed in cycles deg−1 at the eye, not in cycles mm−1 on the
sensor. Similarly, the brightness of the viewed image and the surroundings both affect the state of viewer
adaptation and perception of color reproduction [11].
Assessment of perceptual image quality is further complicated because it is multivariate. For example,
Section 4.02.3.5 mentioned that some cameras change color correction as a function of ISO setting,
producing less colorful images to limit ampliﬁcation of noise. Similarly, most cameras limit sharpening
in the image processing chain as the ISO setting increases, reducing image sharpness while limiting
noise ampliﬁcation. Optimization of the design and processing in these instances requires a suitable
metric to evaluate the trade-offs between different perceptual problems.
The perception of overall image quality has been modeled as a sum of multiple perceptual degra-
dations from an ideal or preferred position. This approach breaks overall image quality into multiple
perceptually independent attributes (e.g., sharpness, noise, aliasing, geometric distortion), models each
independently, then sums the quality effects in a perceptual space. Converting each quality attribute to
perceptual units—just-noticeable differences (JNDs) provides even scaling for the different attributes.
The ﬁrst step in this modeling process is to relate an objective metric to a perceptual response, nearly
always a nonlinear relationship. A generic model for relating an objective metric to a single perceptual
quality attribute is shown in Eqs. (2.1) and (2.2), taken from [12]:
i =
 i −Ti
if i > Ti,
0
otherwise,
(2.1)

4.02.4 Quality Modeling
35
0
0.5
1
1.5
2
2.5
3
−18
−16
−14
−12
−10
−8
−6
−4
−2
0
Ωnoise
ΔQnoise
FIGURE 2.19
Example IHIF.
Qi

i

=
Ri
2
i,∞
ln

1 + i,∞· i
Ri

−
i
i,∞
.
(2.2)
This is an Integrated Hyperbolic Increment Function (IHIF) relating an objective metric to the JNDs
of image quality degradation attributable to that metric. In Eq. (2.1), i is the amount by which an
objective metric, i, exceeds a threshold, Ti, the value of i below which there is no quality degradation.
In Eq. (2.2), Qi refers to the degradation of image quality, in units of JNDs, attributable to operating at
a speciﬁc value of i. i,∞is the increment in i that provides one JND of quality degradation at
large values of i. Ri is the radius of the ﬁt, controlling the curvature of the transition from zero slope at
low values of i to constant nonzero slope at high values of i. For example, i could be an objective
metric related to exposure error (overexposure or underexposure) and Qi would be the perceptual
quality degradation arising from that exposure error. An example IHIF is shown in Figure 2.19, with
Ti = 0.4627, Ri = 0.1418, and i,∞= 0.02312.
To allow this simple form to ﬁt many possible quality attributes, the objective metric, i, must satisfy
several criteria:
•
i > 0 for all conditions of interest;
•
below a threshold level, the attribute is visually undetectable, so variations in i below that level
have no effect on quality;

36
CHAPTER 2 Image Quality in Consumer Digital Cameras
•
increasing i corresponds to decreasing image quality;
•
at large values of i, the incremental effect on quality approaches a constant.
Developing an objective image quality metric that meets these criteria and relates to a speciﬁc perceptual
quality attribute is non-trivial, but some image quality metrics in current use come reasonably close.
For example, the curve shown in Figure 2.19 was ﬁt by Keelan et al. [13], relating subjective image
quality to a log of a weighted sum of variances in a rendered image. The curve plotted in Figure 2.19
ﬁts subjective quality data with an objective metric calculated using Eq. (2.3):
noise = log

1 + 100 · σ 2 
L∗
+ 5 · σ 2 
a∗
+ 12 · σ 2 
L∗a∗	
.
(2.3)
In this example, an image is converted to a CIELAB representation and the variance of the L∗and a∗
channels is used, along with the covariance of the L∗and a∗channels, to compute an objective metric of
overall noise, noise. noise is then related to perceived quality degradation, Qnoise, as shown in Figure
2.19, using an equation of the form in Eq. (2.2). In general, Weber’s law provides a psychophysical
justiﬁcation for using logarithmic objective metrics. The IHIF is one form of nonlinear function to
relate an objective metric with psychometric responses. Other functional forms can also be used and
are especially appropriate if an objective metric does not meet all of the criteria listed earlier.
Whenmultiplequalityattributes areinvolved, theycanbecombinedwithavariablepower Minkowski
sum as shown below, taken from [12]:
Qmin = min {Q1, Q2, . . .} ,
(2.4)
pm = 1 + 2 · tanh
−Qmin
16.9

,
(2.5)
QO = Qr −


i

−Qi
pm
1/pm
.
(2.6)
Equation (2.4) is used to calculate Qmin, the worst of the independent image quality degradations, and
Eq. (2.5) calculates the variable Minkowski power, pm, which can range from one to three. Equation
(2.6) calculates QO, the overall image quality, based on Qr, a reference (maximum) image quality value,
and the Minkowski sum in square brackets. Figure 2.20 shows a contour plot of a quality surface for two
independent perceptual attributes, such as sharpness and noise. The horizontal and vertical axes for this
plot are JNDs of quality degradation associated with the two attributes. The contours show the overall
quality degradation from their combination. The axes of this plot are JNDs of quality degradation, so the
nonlinear relationships between objective metrics (such as exposure error, acutance, or noise variance)
and quality degradation are not visible in this plot. Any two independent visual quality attributes would
produce the same contour plot.
This sum has two important properties for image quality modeling. The ﬁrst is that small quality
degradations tend to add linearly, illustrated by the contours approaching straight lines near the upper
right corner of the ﬁgure. The second is that overall quality is increasingly dominated by the worst
quality problem as quality degrades. This is illustrated by the quality contours approaching horizontal
at the right-hand side of Figure 2.20 and approaching vertical at the top of the ﬁgure. The length of the
horizontal and vertical portions of each contour increases as the power of the exponent increases.

4.02.4 Quality Modeling
37
−30
−28
−28
−26
−26
−26
−24
−24
−24
−24
−24
−22
−22
−22
−22
−22
−20
−20
−20
−20
−18
−18
−18
−18
−16
−16
−16
−16
−14
−14
−14
−12
−12
−12
−10
−10
−8
−8
−6
−5
−4
−3
−2
−1
ΔQ1
ΔQ2
−25
−20
−15
−10
−5
0
−25
−20
−15
−10
−5
0
FIGURE 2.20
Example Minkowski sum contour plot.
This modeling approach of breaking the complex quality problem into independent attributes, then
summing the effects, is a powerful way to simplify model development. Still, the relationship between
system design parameters, such as pixel pitch, and objective metrics in a viewed image, such as σ

L∗
,
is complex and scene dependent. As shown earlier, the noise variance in a captured image depends
strongly on the number of captured photo-charges. In a brightly lit scene, the full-well capacity of the
image sensor may be collected during a short exposure time. In a dimly lit scene, the full-well capacity of
the image sensor can still be collected, but this requires a longer exposure time. If the scene and camera
are both still, the long exposure time works well. If something moves, then quality degradation will
be caused by motion blur. The velocity of the motion and the rate of photo-charge collection together
determine the optimum exposure time for a scene with motion. Because the limiting quality problems
depend both on camera design and on the scene, camera system optimization is most effective when
considering a distribution of quality levels over a sampling of scenes spanning the scene conditions
of interest. This provides a conceptual foundation for the cameras optimized for speciﬁc applications
mentioned in Section 4.02.1.

38
CHAPTER 2 Image Quality in Consumer Digital Cameras
4.02.5 System interactions
Having discussed overall perceptual quality in Section 4.02.4 and camera engineering issues in
Section 4.02.3, we will brieﬂy review the primary interactions between them. From a camera hard-
ware perspective, the major design parameters and their associated perceptual attributes are:
•
Geometric distortion, chromatic aberration, and response nonuniformity have relatively little inter-
action with other perceptual quality attributes. Lens designers optimize performance using various
objective metrics, with few published models relating these quality problems with overall perceived
quality.
•
The MTF of the camera affects both sharpness and aliasing, also interacting with the processing
path. Cameras with a lower MTF and lower aliasing potential can use simpler processing without
visible aliasing artifacts, but will require more sharpening to achieve a similar scene sharpness.
•
Color sampling primarily affects aliasing and color reconstruction errors, also interacting with the
image processing path.
•
Spectral sensitivity affects color reproduction quality and camera sensitivity, interacting with the
portions of the image processing path affecting both color and noise.
•
The size of the camera (both lens aperture and pixel size) primarily affects the sensitivity of the
camera.
0
10
20
30
40
50
60
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Frequency, cycles⋅ degree   as viewed
−1
Response
CSF
SQS 32
SQS 29
SQS 26
SQS 20
SQS 14
SQS  8
FIGURE 2.21
System MTF for SQS quality levels. (For interpretation of the references to color in this ﬁgure legend, the
reader is referred to the web version of this book.)

4.02.5 System Interactions
39
•
The exposure time for a capture affects the signal level (and thus the signal-to-noise ratio) in the
captured image and also the risk of motion blur.
•
The frame readout rate for a capture affects the ability to capture multiple images and thus effectively
capture moving scenes. Rapid capture of multiple still images makes capture at just the right instant
in time easier, while higher frame rates improve the ﬁdelity of fast motion in video capture.
An example of relating camera design parameters with a perceptual attribute is discussed below.
The primary visual attribute related to MTF is the sharpness of the captured image, comprising the
impression of clear detail, texture, and edge contrast. Since the 1970s, evidence has shown that a
cascaded modulation transfer (CMT) function provides a system MTF that can be related to perceived
sharpness. The system MTF must include all system components in the imaging chain, including the
taking lens, image sensor, all the steps in the processing chain, the output (display or printer) MTF,
and ﬁnishing with an MTF for the human visual system. The human visual system is usually included
through a human spatial contrast sensitivity function (CSF) as the ﬁnal component in the cascade. The
visibility of edge detail is modeled by the CSF, but a CSF based on the minimum detectable modulation
is incomplete when considering sharpness. Sharpness is the perception of texture and edge modulation
that is usually greater than a visibility threshold.
A recent standard for image quality evaluation, ISO 20462, uses a family of MTF curves based on
a diffraction-limited lens model to relate MTF with differences in sharpness [14]. To illustrate that
relationship between MTF and quality, Figure 2.21 shows the MTF curves for a selection of standard
qualityscale(SQS)levels. Adifferenceof1ontheSQSscalecorrespondstoaJNDinquality. Figure2.21
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
5
10
15
20
25
30
35
Acutance
SQS
FIGURE 2.22
SQS quality vs. acutance.

40
CHAPTER 2 Image Quality in Consumer Digital Cameras
shows solid curves for three higher quality levels, each separated by three JNDs. The three dashed curves
correspond to lower quality levels, each separated by six JNDs. The dash-dotted curve superimposed
on the ﬁgure is a luminance CSF published by Johnson and Fairchild [15]. Because the human visual
system is complex and nonlinear, the optimal CSF to use for a given application is an open area of
research.
The curves in Figure 2.21 illustrate how MTF affects quality, but the relationship can be taken further.
The MTF and CSF curves in Figure 2.21 can be used to calculate an acutance using Eq. (2.7):
C =
 ∞
0
MTF(ν) · CSF(ν) dν
 ∞
0
CSF(ν) dν
,
(2.7)
where ν is spatial frequency. Figure 2.22 plots SQS vs. this acutance. This relationship does not quite
satisfy all of the criteria listed previously for objective metrics of image quality but the correlation is
fairly good, and a reasonable metric can be constructed based on this acutance.
4.02.6 Processing methods and algorithms
Once efforts have been made to produce captured image data with the best image quality potential, it
is the responsibility of the subsequent image processing chain to achieve that potential. This can be
somewhat likened to the transition from uncut diamond to fully-polished gem. As with the hardware
design, image processing algorithm design is a series of compromises attempting to extract the highest
image quality within the given computational constraints of the system. A fundamental understanding
of the human visual system and how this impacts the perception of image quality helps guide these
engineering trade-offs.
4.02.6.1 Improving images
When the subject of producing a fully-processed image from a raw capture is ﬁrst approached, it is
natural to begin with the assumption that a scientiﬁcally accurate replication of the scene will produce
the most desirable image. Figure 2.23 shows this, unfortunately, is not true. The human visual system
containsapsychologicalcomponentthatinterpretssensoryinput,i.e.,whatisseen,intermsofbeliefsand
preferences. This is particularly true in the case of memory colors: colors associated with certain familiar
objects that represent the icons and cornerstones of our perceptual version of the world. Examples of
memory colors are green grass, blue sky, and skin tones. These colors are almost always remembered by
the observer as being “enhanced” (e.g., brighter, more saturated, hue shifted to be warmer or cooler, etc.)
over how they were in actuality. There are similar psychological mechanisms in play with our perception
of contrast and image sharpness. Observers will remember the scene as being of higher contrast and
sharper image deﬁnition than was truly present at the time of capture. Actually, the situation can manifest
in the extreme with observers remembering high-frequency details that were not resolved by the capture
optics or unaided human eye, but still are expected to be extant in the fully-processed image.
For these reasons, it is important that image processing algorithms be able to render enhanced images
in order to meet the image quality expectations of the human observer. While inserting missing detail
into the captured image would be a challenge (however, this is precisely the goal of super-resolution),

4.02.6 Processing Methods and Algorithms
41
(a) Accurate rendering
(b) Enhanced rendering
FIGURE 2.23
Accurate versus enhanced scene rendering.
it is fairly pragmatic to require that image processing algorithms be able to amplify or at least modify
certain image qualities and quantities in order to match the psychophysical expectations of the observer.
4.02.6.2 Camera control algorithms (A*)
The A* algorithms operate before the actual image capture, as discussed in Section 4.02.2. As such,
the image quality requirements of these algorithms are not targeted at image rendering as much as at
capturingasmuchundistortedimageinformationaspossible.Avoidanceofsignalclipping,quantization,
and nonlinear effects are the primary ways these algorithms contribute to the image quality of the fully-
processed image.
4.02.6.2.1
Autofocus
According to consumer focus groups, perhaps the biggest dissatisﬁer is blurry digital camera images.
Blur can manifest in many ways. Deferring the topic of motion blur associated with the scene to
Section 4.02.6.2.2, the autofocus algorithm is concerned with blur due to misfocus. Admittedly, misfocus
is still a broad condition, for it may refer to a number of scenarios: global misfocus, focus on the wrong

42
CHAPTER 2 Image Quality in Consumer Digital Cameras
scene element, or undesirable depth of focus (either too short or too long). The simplest of these
conditions to address is global misfocus and its solution will carry the seeds of fruitful approaches for
addressing the other forms of misfocus.
It would be enough of a challenge to pose the image quality aspect of the autofocus problem as
attempting to capture as much high-frequency spatial detail as possible. Simplistically, this is often
posed as adjusting the taking lens focus to maximize the local contrast in the captured image. This
requires capture of multiple images at different focus settings, analysis of each for local contrast, then
adjustment of focus to a setting that maximizes the local contrast. Analysis of each image usually
begins with a band-pass ﬁlter, followed by computation of localized statistics, such as minimum, mean,
and maximum of the magnitude of the high-frequency data. This approach is fairly simple in concept,
but it has several challenges, including responding to scene modulation rather than noise, measuring
focus consistently despite a wide range of scene content, and operating with the minimum number of
captures. While contrast maximization is standard with compact consumer cameras, some larger ones,
especially digital SLR cameras, employ phase-based techniques. These tend to be much faster but also
more expensive than contrast maximization.
Due to the needs of the autofocus process to run quickly, the image is usually divided into a number of
low-resolution zones. The full-resolution images of Figure 2.24a and b provide an abundant amount of
(a) Close focus
(b) Far focus
(c) Close focus low resolution
(d) Far focus low resolution
FIGURE 2.24
Autofocus example.

4.02.6 Processing Methods and Algorithms
43
high-resolution data that makes focus determination relatively straightforward. However, the resolution
of the data likely available to an autofocus algorithm is more in line with Figure 2.24c and d. In this
latter case, there is essentially no difference between focus positions and, therefore, no focus cues for
the autofocus algorithm. Figure 2.24 is an idealization, and more sophisticated autofocus arrangements
would provide higher image resolution in the center of the focus frames to mitigate situations such as
Figure 2.24c and d. However, this only underscores the importance of evaluating the scene in a manner
similar to the human visual system, i.e., with some higher-resolution “foveal” data, in order to produce
a result that will permit the eventual generation of a high-quality fully-processed image.
Figure 2.24 also illustrates some of the other potential autofocus failure modes. In the scene there
are clearly two distinct object ranges: the doll in the foreground at the lower right and the children in
the background. What is the proper focus for this image: foreground in focus, background in focus, or
a compromise setting everything nearly in focus? One can impose some general additional constraints
to the autofocus problem, such as center weighting of the scene or attaching higher importance to areas
with skin tone colors. However, these constraints would not be necessarily relevant to scenes without
people or without an object of interest in the center. It is for this reason that more advanced autofocus
algorithms attempt to work with additional information by allowing the user to select the focus zones
of the image. The algorithm can also begin by detecting the presence of faces in the scene and allow
the user to turn off face detection, if desired, when capturing a landscape scene.
Autofocus algorithms generally work in one of two modes: coarse and ﬁne focus. The coarse focus
mode may use the majority of the scene for an overall focus position determination. If the user takes
several captures of the same scene, on subsequent shots the autofocus algorithm may restrict itself to a
small portion of the scene (usually the center) and perform a more detailed ﬁne autofocus analysis in
order to tune the settings. However, if the coarse focus was on the background of the scene when the
object of interest was in the foreground, the ﬁne focus mode may be restricted from responding to any
foreground information. In this case, the autofocus algorithm may need to be forced to return to coarse
focus mode in order to eventually focus on any objects in the foreground. This is usually accomplished
by recomposing the scene and focusing on another object that is roughly at the same range as the original
object of interest and then using that focus “lock” (held with the “S1” shutter button position) on the
original scene. Hopefully, the subsequent switch to ﬁne focus mode will stay centered on the object
of interest. All this may seem to be an inordinate amount of user input to acquire in order to produce
high image quality results. However, this is largely a consequence of the minimal data gathering and
constrained computing environment of the autofocus algorithm.
4.02.6.2.2
Autoexposure
One of the most signiﬁcant shortcomings of the digital camera compared to the human visual system
is its limited dynamic range, i.e., the span in perceived luminance from the darkest dark to the light-
est light. For a given illumination scenario, the dynamic range of the human visual system is around
10,000:1. Consumer digital cameras have dynamic ranges more on the order of 100:1, with profes-
sional digital cameras providing an incremental improvement in this regard. For classic high dynamic
range scenes such as the bride and groom portrait, the digital camera is sorely taxed in its ability to
simultaneously keep both the bride’s white dress and the groom’s black suit from clipping or being
otherwise distorted. However, for a large portion of scene space, a 100:1 dynamic range is adequate

44
CHAPTER 2 Image Quality in Consumer Digital Cameras
provided the camera’s exposure system is properly set. This, therefore, becomes the responsibility of
the autoexposure algorithm.
Rather than minimize clipping at both ends of the luminance range, the goal of a typical autoexposure
algorithm is to identify the 18% gray point in the scene and then to make camera adjustments so that this
eventually maps to an appropriate location in the tone scale of the fully processed image. The 18% gray
point is a product of the gray world hypothesis, the assumption that the average color of all possible
natural images is a middle gray of approximately 18% reﬂectance. Clearly, any given image need not
conform to the gray world hypothesis, e.g., a forest ﬂoor of brightly lit autumn leaves or a ﬁeld of
green grass. However, even with such apparent counterexamples, it is possible to ﬁnd elements within
most images that more or less conform to the gray world hypothesis, e.g., the average of pixel values
associated with strong edges in the scene. With a properly exposed mid-level gray, the fully processed
image will appear to have the correct overall brightness and any clipping in the shadows or highlights,
if not too egregious, will generally be acceptable. For most scenes, less than two percent of the pixels
are clipped at either end of the tone range, often less then half of a percent. Scenes with fog or haze
rarely have very dark areas to clip, while scenes with no clouds or haze in direct sunlight will often clip
on both ends of the range.
Once the autoexposure algorithm has analyzed an image for its 18% gray point, there are three degrees
of freedom for adjusting the camera’s exposure system: exposure time, aperture ratio (f-number), and
exposure index (ISO). These are related to the average scene brightness by (2.8):
B = K A2
T S .
(2.8)
A is the relative f-number, T is the exposure time in seconds, S is the ISO speed, K is a calibration con-
stant, and B is the resulting average scene brightness. Since this system is underdetermined, additional
constraints need to be imposed in order to determine a unique camera setting adjustment.
The f-number of the camera system controls the diameter of the capture lens diaphragm. Opposing
the indiscriminate setting of the f-number are lens aberration, depth of ﬁeld, and signal-to-noise consid-
erations. The lower the f-number the larger the diaphragm diameter. This results in three consequences:
increased distortions in the image due to lens aberrations, depth-of-ﬁeld narrowing, which selectively
brings one scene distance into focus and blurs all others, and signal-to-noise improvement due to more
light being captured. Higher signal-to-noise performance is almost always an advantage. Narrowing the
depth of ﬁeld may be desirable if the user is interested in producing the bokeh effect. However, increasing
the impact of lens aberrations is almost always considered a liability. Increasing the f-number reduces
the size of the diaphragm opening. In this case the impact of lens aberrations is reduced, depth of ﬁeld
is increased to bring more of the scene into focus, and image noise is increased due to the reduction
of captured light. While the reduction of image distortions due to lens aberrations is a favorable effect,
having a large depth of ﬁeld can remove visual cues of depth from the resulting image, leaving it looking
ﬂat and two dimensional. Of course, increased image noise is something generally best avoided.
The exposure time of the capture relates to how long light is allowed to integrate within the sensor’s
pixels. Increasing the exposure time allows more light to be captured and more photocharges to be
produced in the pixel wells. One minor consideration when increasing the exposure time is the possibility
of overﬂowing the wells, which would result in signal clipping and possible corruption of neighboring
pixel values if the overﬂow charges migrate. However, most pixel designs in current use have overﬂow

4.02.6 Processing Methods and Algorithms
45
(a) Short exposure
(b) Long exposure
FIGURE 2.25
Autoexposure example.
mechanisms to inhibit photocharge leakage and the ﬁeld of high dynamic range imaging is devoted to
addressing the signal clipping problem. Perhaps of greater concern, recalling Section 4.02.6.2.1, is the
increased potential of motion blur with longer exposure times, as shown in Figure 2.25. While this may
be desirable when giving the water a silky appearance in a waterfall or in producing other artistic effects,
motion blur generally detracts signiﬁcantly from image quality. The most common cause of motion blur
is an unsteady, usually handheld, camera. The obvious response is to shorten the exposure time in order to
reduce or eliminate visible motion blur. The consequence of this approach is the increase in image noise
due to underﬁlling, perhaps signiﬁcantly, the pixel wells. Since noise-generated photocharges, such as
those due to thermal noise, are largely independent of the captured light photocharges, underﬁlling the
wells permits the noise photocharges to potentially outnumber the signal photocharges, leading to poor
signal-to-noise performance.
The exposure index (ISO) is a setting normally used to control a signal gain, analog or digital, applied
to the raw pixel signal. If the gain is analog, it is applied with a programmable gain ampliﬁer before the
pixel signal is read and digitized by the A to D converter. If the gain is digital, it is applied after signal
digitization. Since there is noise associated with the pixel digitization process, an analog gain will not
amplify this noise, whereas a digital gain will. Since the gain is a multiplicative operation, high ISO

46
CHAPTER 2 Image Quality in Consumer Digital Cameras
values can lead to signiﬁcant signal contouring and quantization. For instance, 50 photocharges can
translate into a digital signal of 500 counts, and 51 photocharges translate into a signal of 510 counts,
leaving the signal range from 501 to 509 unpopulated. Changing the exposure index generally ampliﬁes
signal and noise alike with little to no opportunity to discriminate between the two. For this reason,
one almost always uses the lowest ISO setting possible. The main reason to use a higher ISO setting
is to reduce motion blur, since a higher ISO combined with a shorter exposure time will produce the
same signal level, though with increased noise. Therefore, the exposure index image quality trade-off
is between reducing noise (by lowering ISO) and reducing motion blur (by raising ISO).
4.02.6.2.3
Automatic white balance
The automatic white balance algorithm has the task of simulating the white point adaptation of the
human visual system. Usually, this is stated loosely as keeping neutral (gray) objects looking neutral
regardless of the color of the scene illuminant. As with autoexposure (Section 4.02.6.2.2), one begins
with the gray world hypothesis: the average color of all possible natural images is a middle gray of
approximately 18% reﬂectance. Since automatic white balance is one of the A* algorithms that must
execute quickly during the S0 or S1 shutter button states, there is a strong urge to simply average all of
the color information in the scene and force it to gray by scaling the captured color pixel data. The fact
that this can be done quickly on a very low-resolution version of the image, since spatial information is
not required, adds to the attractiveness of this approach.
Unfortunately, as mentioned previously, the gray world hypothesis does not apply to individual
images. It is an ensemble statement only. Examples of scene failures are images dominated by green
grass, blue sky, or autumn leaves. However, signiﬁcant improvements to the “average everything”
approach can be made by simply identifying the brighter and more saturated colors in the scene and
excluding them from the averaging process. In Figure 2.26, a scene consisting of many brightly colored
hats is shown. The original image, Figure 2.26a, is reasonably well white balanced. If this image is
converted from RGB to YCBCR space, the averages of the chrominance values forced to zero, and the
resulting image transformed back into RGB space, one arrives at Figure 2.26b. Note that the yellow
cap has turned yellow-green and the cement wall and plywood board have turned blue. Modifying
the process, before computing the averages of the chrominance values, pixels with at least one large
chrominance value are removed from the averaging computation. In the example, this removes the
inﬂuence of the highly colored caps. The resulting image shown in Figure 2.26c looks much like the
properly white-balanced original. This kind of adaptive algorithm can also be augmented by user input,
such as through selection of a scene mode. For example, most landscapes, beach, and snow scenes are
illuminated with daylight. Portraits may be taken with any illumination, but face-based automatic white
balance can be especially effective for those scenes.
While it is scientiﬁcally neat to state the auto white balance problem as identifying the scene gray
point and forcing it to be pure gray, the psychological component of the human visual system will object
to that always being the “right” answer. Referring back to Figure 2.26b, a number of observers would
ﬁnd the rendering of the blue sky superior when compared to Figure 2.26a and c. While there is too
much of an overall blue cast to Figure 2.26b, it is conceivable that some smaller amount of blue cast may
be preferred if an improvement in the sky is noted before a degradation in the cap colors. While this is,
perhaps, a subtle example of psychological inﬂuences on image quality, the more familiar and obvious
example is the sunset scene. As with other memory colors, humans have a distinct preference for sunset

4.02.6 Processing Methods and Algorithms
47
(a) Original
(b) Simple average
(c) Clipped average
FIGURE 2.26
Automatic white balance example. (For interpretation of the references to color in this ﬁgure legend, the
reader is referred to the web version of this book.)
images having warm, if not ruddy, color casts. Any attempt to impose a “scientiﬁcally correct” white
balance on such scenes will generally ruin the resulting image from a preference perspective. Keeping
in mind the requirement for rapid computing schemes, the simplest thing to do is to only apply a fraction
of the computed white balance correction. As a result, sunset scenes will retain some of their inherent
warm color cast while scenes requiring smaller white balance corrections will not necessarily suffer a
signiﬁcant image quality loss due to undercorrection. The ﬁnal advantage of this conservative approach
is that the impact of any inaccuracies in the white balance computation is reduced.
4.02.6.3 Color and tone
As their names suggest, the color correction and tone scaling operations within a digital camera image
processing chain are responsible for producing images that have accurate and preferred color and tone
renderings. As mentioned in previous sections, “accurate” and “preferred” can often be at odds, and this
is perhaps most true for color and tone processing. A moment’s thought will reveal that the human visual
system’s ability to recognize objects in the world around it is more closely tied to the color and tone
of an image than its spatial details. An apple viewed through a piece of wax paper is still recognizable
as an apple. A ﬁery red cardinal darting through one’s ﬁeld of view is still identiﬁed. With such a high
dependence on color and tone cues, it is no surprise that the psychological aspect of the human visual

48
CHAPTER 2 Image Quality in Consumer Digital Cameras
system plays a signiﬁcant, if not dominant, role in interpreting these inputs. As a consequence, strong
preferences can develop and overrule the objective observations of the scene. For these reasons, color
and tone algorithms must be able to produce results that deviate in some meaningful way, sometimes
signiﬁcantly, from scientiﬁcally accurate renderings.
4.02.6.3.1
Color correction
When the raw image is captured by the digital camera, its color information is in terms of the spectral
sensitivities of the capture optics. These spectral sensitivities are largely deﬁned by the color ﬁlter array
and the IR cutoff ﬁlter. As these spectral sensitivities vary from camera model to camera model, they
are generally viewed as deﬁning an uncalibrated or nonstandard color space. It is the responsibility of
the color correction algorithm to convert the image from this uncalibrated color space into one that is
standardized (Figure 2.27). This is usually implemented as a 3 × 3 matrix multiply:
⎛
⎝
R′
G′
B′
⎞
⎠=
⎛
⎝
aRR
aRG
aRB
aGR
aGG
aGB
aBR
aBG
aBB
⎞
⎠
⎛
⎝
R
G
B
⎞
⎠.
(2.9)
(a) Without color correction
(b) With color correction
FIGURE 2.27
Color correction example.

4.02.6 Processing Methods and Algorithms
49
{R, G, B} represent the input color values and

R′, G′, B′
represent the output color values. The
process of populating the matrix coefﬁcients, {a}, determines the image quality of the ﬁnished image.
Before discussing this process, it is noted that (2.9) really only has six degrees of freedom, rather than
nine. Three degrees of freedom are used to keep neutrals unchanged by forcing the rows in the matrix
to sum to unity. One way to do this is to deﬁne the last column coefﬁcients as in (2.10):
⎧
⎨
⎩
aRB = 1 −aRR −aRG,
aGB = 1 −aGR −aGG,
aBB = 1 −aBR −aBG.
(2.10)
As with a number of the previous algorithms discussed, a scientiﬁcally accurate color correction
matrix does not necessarily produce perceptually desired results. In some ways this is fortunate, as a
trulyaccuratematrixfortheentirecolorspaceisnotpossiblewithonlysixdegreesoffreedom.Thecolors
of the world are created in a wide variety of manners: absorption, dispersion, diffraction, emission, etc.
Even when producing the same perceptual colors, the different mechanisms characteristically produce
differentspectralstimuli.AsdiscussedinSection 4.02.3.4,unlessthecameraspectralsensitivitymatches
a set of color matching functions, the camera will produce different responses to the different stimuli.
As a result, each mechanism would require a different set of matrix coefﬁcients even though they are
all nominally producing the same colors. Therefore, the color correction matrix coefﬁcients are usually
derived using an optimization approach that minimizes the differences between the measured colors
in a color test chart and the corresponding colors produced by the camera matrix. However, there are
many degrees of freedom in this process.
Consider the composition of the color test chart. Not all colors are equally important to the human
visual system. The ability to reproduce memory colors (green grass, blue sky, skin tones, etc.) plays
a major role in determining the image quality of the resulting color correction. Coupled with this is
the human visual system’s sensitivity to small changes in color. Small inaccuracies in the reproduction
of greens may go unnoticed, although the same small inaccuracies in the reds and purples may be
quite apparent. With respect to the optimization calculation, the results can be inﬂuenced signiﬁcantly
by constructing, for example, a color test chart with more skin tones and less highly saturated colors.
Equivalently, the data from each color patch can be numerically weighted during optimization based
on the perceived importance of the given color.
Leaving scientiﬁc accuracy aside and considering color enhancement, the colors produced by the
color matrix need not be made to match the measured color test chart values, but may, instead, be
compared to numerically adjusted color test chart values. For example, the saturation of the measured
test color patches can be increased or decreased based on a given design aim. Increasing the color
saturation in an image will also generally amplify the noise in the image. Therefore, color saturation
(ampliﬁcation) can be traded off against noise ampliﬁcation. Camera manufacturers attempt to straddle
this wide range of color enhancement options by giving users several color matrices to choose from,
e.g., “high,” “normal,” and “low” color settings. Each matrix is produced by adjusting the amount of
color saturation that is applied to the color test chart values prior to optimization.
With only six degrees of freedom, it is evident that not all colors can be reproduced with equivalent
accuracy. One apparent solution to this problem is to increase the number of degrees of freedom.
A step in this direction is to expand the color correction matrix to permit additional correction terms.
For example, (2.9) implies that the new green value is computed using just linear amounts of the

50
CHAPTER 2 Image Quality in Consumer Digital Cameras
original colors, i.e., G′ = aGR R + aGGG + aGB B. Quadratic terms could be added to this expression:
G′ = aGR R + aGGG + aGB B + aGR2R2 + aGG2G2 + aGB2B2 + aGRG RG + aGRB RB + aGGBGB.
This results in a 3 × 9 matrix with twenty-one degrees of freedom (plus six degrees of freedom to
preserve neutrals, e.g., aGB = 1 −aGR −aGG and aGGB = −aGR2 −aGG2 −aGB2 −aGRG −aGRB).
A fully generalized color correction approach would be the use of three-dimensional lookup tables.
For an image with a code value range of 0–255, a maximum sized lookup table would contain 2563 =
16, 777, 216 coefﬁcients, allowing every possible input {RGB} triplet to be mapped exactly to the
desired output

R′, G′, B′
triplet. Fortunately, one need not determine lookup table coefﬁcients from
such an oppressively large table as the human visual system combined with image display device
limitations cannot distinguish between that many colors. In addition, for a suitable sampling of color
space, interpolationbetweentableentries will produceresults withsufﬁcient accuracyfor most purposes.
If the {RGB} values are sampled every eight code values, this corresponds to a sampling of thirty-two
values along each color dimension and a lookup table size of 323 = 32, 768 coefﬁcients. This is a
much more manageable block of data, both for table population and for computational storage and data
I/O requirements. Population of the three-dimensional color table is then achieved by inserting explicit
transforms (coefﬁcients) for a given number of test colors and then providing smooth transitions between
these seed coefﬁcients. The image quality of the resulting color transform is controlled by the number of
reference colors speciﬁed and their location in color space, e.g., skin tones may be much more densely
sampled than “beach ball” colors.
Given the preceding discussion, there are still issues inﬂuencing color reproduction image quality
that are beyond the transform mathematics. As mentioned in earlier sections, it is essential that the input
image is properly white balanced. If there is an overall color cast to the image, the color correction
process will most likely amplify this color bias to the point of objectionability. This will be noticeable
especially in the neutral regions of the image. In addition to the white balance considerations, there are
also the limitations brought on by the native camera spectral sensitivities. For ideal 3 × 3 matrix color
correction, the camera spectral sensitivities must be linear combinations of the human visual system’s
color matching functions. Departures from the color matching functions indicate there will be colors
that the camera is incapable of capturing and producing accurately, regardless of the color correction
algorithm. The only real solutions in this case are hardware improvements to the components controlling
the camera’s spectral sensitivities.
4.02.6.3.2
Tone scaling
As statedearlier, theworldperceivedbythehumanvisual system has aphotometric(brightness) dynamic
range of, at least, 10,000:1. This is an extremely broad range that is signiﬁcantly beyond the display
capabilities of today’s computer monitors or printed page. This range can be, in principle, captured by
high-quality sensor pixels with well depths in the multiple 10,000 photocharge range. In practice, the
photocharge count from these pixels are quantized to a thousand levels or less to reduce the impact
of noise. However, the full range of digitized pixel values can still represent the full 10,000:1 scene
dynamic range. Even less expensive consumer grade sensors that have pixel well depths of only a few
thousand photocharges can produce signals that can represent a large dynamic range, though with fewer
output signal levels.

4.02.6 Processing Methods and Algorithms
51
(a) Linear tone scaling
(b) Nonlinear tone scaling
FIGURE 2.28
Tone scaling example.
Therefore, there needs to be a means of mapping the large dynamic range of the scene onto the
much smaller dynamic range of the display that produces high image quality. A linear mapping fails
in this regard, causing the image to look extremely dark and lacking in contrast (Figure 2.28a). Taking
a cue from Weber’s Law, which attempts to relate physical stimuli with perception, it is noted that the
human visual system’s perception of brightness assumes more of a power law response than a linear
one. Due to some fortunate coincidences in the development of CRT display technology, correction
for the standardized HDTV response is similar to the human visual system’s brightness response. As a
result, the HDTV display response is approximately x1/2.2, where x is the normalized (x ≤1) linear
scene brightness.
Unfortunately, the rendering of a desired tone scale often requires something more complicated
than what a simple nonlinear transform can provide. Figure 2.29a shows an image processed with a
standard HDTV tone scale and Figure 2.29b shows the same image processed with a custom tone scale.
The plot in Figure 2.29c shows plots of the custom tone scale (blue curve) and the standard tone scale
(red curve). Examining Figure 2.29b more closely, it can be seen that in order to increase the detail in the
sky (highlights), the distant trees are largely forced to black. Further custom tone scale manipulations
reveal that there is insufﬁcient image detail captured in the shadow regions of the initial image to
produce a high-quality result at both ends of the tone scale. High dynamic range processing attempts

52
CHAPTER 2 Image Quality in Consumer Digital Cameras
(a) Standard tone scaling
(b) Custom tone scaling
0
64
128
192
256
0
64
128
192
256
Input
Output
Standard Tone Scale
Custom Tone Scale
(c) Tone scale plot
FIGURE 2.29
Standard vs. custom tone scaling example. (For interpretation of the references to color in this ﬁgure legend,
the reader is referred to the web version of this book.)
to address this lack of digital camera dynamic range. In its simplest form, two images with different
exposure settings are captured and the results fused together to produce an image with improved tone
rendering. In Figure 2.30a it can be seen that a best compromise exposure of the scene leaves the tree
in the foreground too dark and the mountain side in the background too light. Therefore, two captures
are made: one with the mountain side properly exposed and another with the tree properly exposed.
These two images are fused together to produce Figure 2.30d. In this case the fusing process was an

4.02.6 Processing Methods and Algorithms
53
(a) Standard capture
(b) Highlight capture
(c) Shadow capture
(d) HDR result
FIGURE 2.30
High dynamic range example.
adaptive averaging of the two exposure captures. More sophisticated fusion algorithms would produce
correspondingly higher-quality images. An image such as Figure 2.30d is highly inaccurate from a
simple scene measurement perspective, but the goal is a visually pleasing, rather than scientiﬁcally
accurate, rendering of the scene.
4.02.6.4 Spatial processing
While color and tone processing can be thought of as addressing the macroscopic aspects of the image,
spatial processing can be thought of as addressing the microscopic aspects of the image. The operations
discussed below are focused on image resolution and spatial frequency ﬁdelity, especially with respect to
higher spatial frequencies. In addition to the ampliﬁcation and enhancement of genuine signal content,
the reduction and elimination of noise, both stochastic and ﬁxed pattern, is also of chief concern.
Finally, it is the responsibility of image compression to identify and reduce redundancies in the image
data without compromising the results of the other spatial operations.

54
CHAPTER 2 Image Quality in Consumer Digital Cameras
4.02.6.4.1
Resizing
Increasing the number of pixels in the camera’s sensor in order to increase the resolution of the captured
image comes with a number of undesirable consequences. Adding pixels increases the manufacturing
cost of the sensor and the power consumption requirements. It also places added I/O demands on the
supporting electronics due to the larger dataset. From an image quality perspective, adding pixels is
usually done with the constraint of keeping the overall sensor the same size, or even making it smaller.
This requires that the pixels themselves shrink in size. Smaller pixels lead to smaller pixel wells for
storing signal-generated photocharges. This, in turn, leads to lower signal-to-noise performance, for
the number of noise-generated photocharges does not change with pixel well size. Finally, as discussed
in Section 4.02.3.1.4, smaller pixels require higher-quality capture optics with smaller point spread
functions. As this, too, is an expensive proposition, it is often sacriﬁced in the name of overall camera
cost, leading to a decrease in captured high spatial frequency detail as the pixel size decreases.
One way around this conundrum is to capture an image with fewer (and larger) pixels, and then
resize the resulting image to the desired pixel count. This, unfortunately, is a solution fraught with its
own liabilities. Figure 2.31a is a low-resolution version of Figure 2.31f and has been resized to the
original resolution in a number of ways to produce the other images in Figure 2.31. Nearest neigh-
bor, or pixel replication, produces blocking artifacts and ampliﬁes the aliasing patterns in the fence
(Figure 2.31b). Bilinear and bicubic interpolations (Figure 2.31c and d) leave the image looking soft
and also do not address the aliasing in the fence. In Figure 2.31e, several copies of Figure 2.31a,
each with a random amount of rotation and translation applied to Figure 2.31f prior to generating the
low-resolution image, are pixel registered on a high-resolution grid and then averaged together. This
simulates a multiframe capture approach, although super-resolution techniques are still not generally
used in consumer cameras because of their complexity. There is a clear improvement with most of the
aliasing in the fence disappearing. There are still artifacts along the lighthouse catwalk, although even
these are improvements over the previous methods.
Resizing can occur at any number of locations within the image processing chain. One natural
location is at the end of the chain. When producing a higher-resolution image, this has the signiﬁcant
advantage of reducing the computational effort required of the rest of the image processing operations
by working with a smaller image until after the ﬁnal resizing step. The disadvantage to resizing at the
end is that there is no opportunity for operations such as noise reduction and edge enhancement to
address artifacts and distortions caused by the resizing operation. Moving the resizing operation earlier
in the chain permits some post-resizing improvement to the image at the expense of a fairly signiﬁcant
increase in computational effort due to the increased image size. Finally, it is possible to place the
resizing operation before the demosaicking operation. However, the resizing algorithm then becomes
much more complicated as it must produce a resized CFA image. Treating the CFA image as three
interleaved and sparsely sampled grayscale images will probably lead to substandard results, as this is
a worst-case means of working with the data: lowest resolution and not using color channel correlation.
If this liability is addressed by incorporating color channel correlation with neighboring pixels, then the
distinction between resizing and demosaicking can begin to blur and the order of these two operations
may need to be re-examined. Fortunately, the primary application for resizing before demosaicking
is to produce a lower-resolution video frame or a live preview display image. Since these types of
images generally have lower image quality requirements compared to full-resolution still images, the

4.02.6 Processing Methods and Algorithms
55
(a) Low resolu-
tion
(b) Nearest neighbor
(c) Bilinear
(d) Bicubic
(e) Super-resolution
(f) Original
FIGURE 2.31
Resizing example.
resizing task is less restrictive from an image quality perspective and can usually be achieved through
straightforward applications of nearest-neighbor subsampling or bilinear interpolation.
A ﬁnal consideration is that the resizing magniﬁcation need not be the same in the horizontal and
vertical directions. Since most resizing algorithms are inherently separable in these two dimensions,
there is usually little problem with adjusting the resizing algorithm accordingly. The main application
for anamorphic resizing operations is to correct for the case when the pixel sampling grid on the sensor
or the pixel display grid on the camera display are not square. As discussed above, this correction of the
aspect ratio can occur at several locations within the image processing chain, with generally the same
pros and cons as in the symmetric resizing case.
4.02.6.4.2
Demosaicking
It is a remarkable demonstration of the inherent redundancy in digital camera images that fully two-
thirds of the image data can be discarded and an acceptable reconstruction of the full image can be

56
CHAPTER 2 Image Quality in Consumer Digital Cameras
(a) Full original
(b) Original
(c) Mosaicked
(d) Bilinear
(e) Hard
(f) Soft
(g) Original
(h) Mosaicked
(i) Bilinear
(j) Hard
(k) Soft
FIGURE 2.32
Demosaicking example.
recovered from the remaining information. However, the means of sampling the captured data, i.e.,
mosaicking, and of reconstructing the full image, i.e., demosaicking, must be chosen with care in order
to achieve the desired image quality objectives [16]. Figure 2.32a is a scene containing a large number
of man made structures with a signiﬁcant amount of high spatial frequency detail. This type of scene
can produce signiﬁcant image quality challenges to a demosaicking algorithm. Figure 2.32b and g are
crops of two regions of interest from Figure 2.32a. Figure 2.32c and h is mosaicked (decimated) using
the well-known Bayer color ﬁlter array pattern (Figure 2.33) [2].
Using the mosaicked image as a starting point, the ﬁrst demosaicking approach usually tried is
simple bilinear interpolation of the missing color values, as shown in Figure 2.32d and i. This operation
treats the three color channels as independent grayscale images. What is notable about these results
from an image quality perspective are not the random errors made by the demosaicking operation, but
the structured artifacts. An alternating pixel artifact along straight horizontal and vertical edges, called

4.02.6 Processing Methods and Algorithms
57
G
R
B
G
FIGURE 2.33
Bayer color ﬁlter array pattern.
zipper, is immediately detected by the human visual system due to its high sensitivity to edge details.
It is noted that an image with the equivalent amount of random noise would generally be judged as
having higher image quality. This is due to the disproportionate sensitivity of the human visual system
to structured artifacts over random noise. As a result, common image quality metrics that make no
distinction between structured artifacts and random noise, such as peak signal-to-noise ratio (PSNR),
are fairly useless in evaluating the image quality of various demosaicking algorithms. Some efforts have
been made to generate an image quality metric for evaluating demosaicking algorithms that incorporate
the effects of zipper and other structured artifacts [17].
Using nonlinear (adaptive) demosaicking algorithms can signiﬁcantly reduce the amount of struc-
tured artifacts in the ﬁnal image [18–20]. However, when these algorithms fail, they can produce artifacts
that have signiﬁcant image quality impact. Figure 2.32e and j was generated using a very simple two-
branch decision tree with a hard decision criterion based on local edge information: either demosaick
horizontally or vertically. The fence railing in Figure 2.32e is distinctly improved over Figure 2.32d,
though there are notable errors with some of the reconstructed railings running horizontally. Figure
2.32j is, again, a signiﬁcant improvement over Figure 2.32i, though the colored artifacts created by
wrong choices in the hard decision process are extremely distracting. If the two-branch decision tree
is modiﬁed to blend both directional choices based on local edge activity, a soft-decision demosaick-
ing algorithm results. Such an algorithm was used to generate Figure 2.32f and k. The results of the
soft decision algorithm represent a compromise between the bilinear and hard decision demosaicking
results. Computationally, a hard decision algorithm tends to be simpler in construction than a soft deci-
sion algorithm and may require less computational effort as a result. However, as shown in Figure 2.32,
while the results of the two nonlinear approaches are different, it may be problematic to determine
which one is better from an image quality perspective.
The ideal solution to the aforementioned image quality dilemma is to use a more sophisticated
demosaicking algorithm that reduces the number of artifacts overall. In addition, color ﬁlter arrays other
than Bayer can be used to provide different kinds of image sampling that may be advantageous when
combined with speciﬁc demosaicking approaches. In the ﬁnal analysis, however, there will generally be
residual artifacts after demosaicking. Due to the nature of the mosaicking/demosaicking process, these
artifacts will tend to occur as spurious structures that will have a greater impact in image quality than
the equivalent random noise.
4.02.6.4.3
Edge enhancement
Edge enhancement (sharpening) was one of the ﬁrst digital image processing algorithms to become
ubiquitous once personal computers became equally commonplace. The initial form of sharpening,

58
CHAPTER 2 Image Quality in Consumer Digital Cameras
(a) Original
(b) Boost record
(c) Sharpened
(d) Oversharpened
FIGURE 2.34
Sharpening example.
unsharp masking, has its roots in photographic history with darkroom procedures now replaced by their
much more convenient digital equivalents. Unsharp masking is a simple image processing operation.
One blurs a copy of the image to be sharpened (Figure 2.34a) and subtracts the blurred copy from
the original. The resulting boost record (Figure 2.34b) is added to the original to produced the ﬁnal
sharpened image (Figure 2.34c).
As mentioned in Section 4.02.2, the preferred sharpness of an image is usually much higher than
what actually existed in the scene. However, a number of image quality factors oppose the arbitrary
increase of image sharpness. In Figure 2.34d, the boost record has been ampliﬁed by a factor of eight
before being added back to the original image. Perhaps the most obvious image quality degradation is
the ampliﬁcation of noise in the young boy’s face. Typically, the strong observer preference for smooth,
ﬂawless skin in portrait shots would result in this image being rejected, even at less aggressive sharpening
levels. Another consequence of oversharpening is the objectionable amount of ringing around the edges
in the image (Figure 2.35). Ringing manifests itself as a dark band on one side of the edge and a bright
band on the other side. Interestingly, the dark band does not have nearly as large an impact on the
image quality as the bright band. As a consequence, the boy’s hair and sweater in Figure 2.34d appear
much lighter than in Figure 2.34a. This is due to the bright bands from the edges ringing as a result
of oversharpening. Finally, if one looks past the noise ampliﬁcation and the edge ringing, the image

4.02.6 Processing Methods and Algorithms
59
(a) Original
(b) Oversharpened
FIGURE 2.35
Ringing example.
still suffers from a distortion of depth cues. The boy’s face in Figure 2.34d looks ﬂatter and more two-
dimensional than in Figure 2.34a or c. There is also a vague sense of the boy appearing closer to the
camera in the oversharpened sample.
There are, of course, a wide variety of more sophisticated edge enhancement algorithms available
that address the aforementioned problems. The most notable improvements come from applying noise
reduction (Section 4.02.6.4.4) to the boost record prior to adding it to the original image. This noise
reduction can take two forms: amplitude-based and edge activity-based. In a typical image, small
amplitude edge detail is not as visually important as larger amplitude edge detail. However, most noise
in a typical image exists as small amplitude variations. Therefore, if all small values in the boost
record are reduced or eliminated, the ampliﬁcation of noise will be correspondingly diminished in the
sharpened image. (Note that the noise, itself, has not been impacted. It simply has not been “made
worse.”) This process is called coring and is nearly a de facto addition to the unsharp masking process.
While this is a signiﬁcant step forward, a reconsideration of the image quality impact of sharpening
noise reveals that the same noise that is signiﬁcantly visible in ﬂat, smooth regions may be undetectable
in regions of high-frequency texture. In the latter sample, the blanket reduction of small-amplitude
boost detail may signiﬁcantly impact the rendering of the textured region. One solution is to compute a
local edge activity value using, for example, a Laplacian edge detector, and to reduce boost detail only
in regions of low edge activity. In this application, a soft thresholding technique is usually preferred
over a hard thresholding approach to reduce switching artifacts: the abrupt transition from sharpened
to unsharpened regions. Finally, it is important to note that ﬂat regions can be rendered too smooth,
leading to unnatural or contoured appearances. For example, it is important that a portrait of a person
does not begin to look like a portrait of a mannequin.
4.02.6.4.4
Noise reduction
Broadly stated, noise reduction is the process of separating desirable image content (i.e., the ideal
image) from undesirable image content (i.e., noise) and reducing or eliminating the latter. The reducing
or eliminating portion is trivially easy. The separation of noise from the image is a vexing problem
of the highest order. As a result of imperfect noise segmentation, either the noise reduction operation
will be unable to eliminate the noise sufﬁciently or it will be too destructive of genuine image detail.

60
CHAPTER 2 Image Quality in Consumer Digital Cameras
(a) Original
(b) Gaussian noise
(c) Impulse noise
(d) Noise-reduced
FIGURE 2.36
Noise reduction example.
Unfortunately, in practice both effects tend to occur simultaneously. For these reasons, noise reduc-
tion is frequently viewed as a necessary evil when attempting to produce high-quality images from
a digital camera. For all of the beneﬁts this operation may yield, it generally produces just as many
problems.
Figure 2.36 shows versions of an image with two types of noise common to digital cameras added.
Gaussian noise (Figure 2.36b) is usually addressed through linear noise reduction techniques whereas
impulse (long-tailed) noise (Figure 2.36c) generally requires nonlinear noise reduction algorithms.
Applying Wiener ﬁltering to Figure 2.36b produces Figure 2.36d. Applying median ﬁltering to
Figure 2.36c will produce an image visually similar to Figure 2.36d. Comparing the noise-reduced
result to the original image shows that the restoration of image quality was only partially successful.
The major artifacts of the noise reduction process are low-frequency mottle and loss of high-frequency
detail. In the latter case, there is signiﬁcant loss of detail around the eyes and in the hair. These dis-
tortions would be psychologically among the most objectionable. In the case of the former, there is a
mottling visible in the face and shoulders as well as the background that is not in the original image.
This mottling, especially in the face, would reduce strongly the perceived quality of the resulting image.

4.02.6 Processing Methods and Algorithms
61
The sources of the noise reduction artifacts are usually due to compromises made in order to produce
tractable and pragmatic implementations. In the case of the low-frequency mottle in Figure 2.36d, this
was due to limiting the Wiener ﬁlter to a 5 × 5 support region. The size of the support region limits the
size of the noise artifacts that can be addressed. However, increasing the support region size to reduce the
low-frequency mottle will signiﬁcantly increase the computational time and effort required for noise
reduction. Another factor opposing the growth of the support region size is the increasingly fragile
assumption that all of the image detail within the support region is from the same statistical distribution.
Abrupt changes in the statistics due to the presence of strong edges and texture in the support region
invalidate this assumption. As a result, edge and texture details can be degraded signiﬁcantly by the
noise reduction operation.
Through analyzing the causes of noise reduction artifacts, it becomes clear that adaptive support
regions that attempt to conform to the single statistical distribution assumption are more successful.
One such ﬁlter, the sigma ﬁlter, rejects all data within the support region that does not conform to an a
priori statistical noise distribution. Other ﬁlters use steerable support regions that are responsive to local
edge detail so that noise reduction occurs along edges, but not across edges. These approaches come
with the price of additional computational effort and the requirement of some a priori knowledge. This,
then, becomes the implementation compromise: the amount of computational effort vs. the resulting
quality of the noise-reduced image.
4.02.6.4.5
Compression
After the raw capture from the digital camera sensor has been processed fully into a ﬁnal image, one
last image processing step remains. Compression, in a direct relation to desired image quality, reduces
redundancies in the image data in order to produce a smaller representation for storage. The reference to
image quality is key, as the redundancies targeted by the compression operation are usually visual rather
than simply mathematical. In essence, the task is to discard as much image information as possible before
the image degradation becomes objectionable. Figure 2.37a presents an original image that requires a
ﬁle size of 1.5 MB to store. Reducing the ﬁle size by an aggressive 150:1 (Figure 2.37b) turns the
image into a distorted cartoon version of the original. However, if subject identiﬁcation is all that is
required, it is still quite evident that this is a picture of two parrots. Compressing the image at a more
moderate 100:1 (Figure 2.37c) does not inﬂate the corresponding ﬁle size that much, but the resulting
image quality improves remarkably. Some compression artifacts are still visible, so a ﬁnal compression
ratio of 50:1 is presented in Figure 2.37d. This last image is essentially visually identical to the original
image in Figure 2.37a at one-ﬁftieth the ﬁle size.
Figure 2.37 was produced using the industry standard JPEG compression algorithm [21]. JPEG
divides the image into 8 × 8 blocks and transforms each block into its discrete cosine transform (DCT)
representation. Assuming the full precision of the DCT coefﬁcients is not visually important, the result-
ing 64 DCT coefﬁcients are quantized in order to produce a data representation that compresses more
efﬁciently. It is this quantization step that controls the image quality of the subsequent decompressed
image. Each 8 × 8 block is processed independently of the other blocks in the image. Therefore, the
quantization of coefﬁcients, having no information about the boundary conditions with adjacent blocks,
can result in signiﬁcant blocking artifacts, evident in Figure 2.37b.
One solution to the blocking artifact dilemma is the newer JPEG 2000 compression algorithm [22].
Rather than subdividing the image into blocks, JPEG 2000 can treat the entire image as a single tile.

62
CHAPTER 2 Image Quality in Consumer Digital Cameras
(a) Original (1.55 MB)
(b) 150:1 (10.1 KB)
(c) 100:1 (14.5 KB)
(d) 50:1 (33.5 KB)
FIGURE 2.37
Compression example.
Each tile is transformed into a wavelet space deﬁned by the JPEG 2000 standard and the corresponding
wavelet coefﬁcients quantized to reduce the complexity of the data representation. Due to a lack of block
boundaries and the wavelets doing a better job than DCTs of representing the image data, the image
quality is notably improved over JPEG. Figure 2.38 shows a comparison of JPEG and JPEG 2000 for
a compression ratio of 150:1. The image quality improvement is evident. However, this improvement
in image quality is usually only realized at higher compression ratios. Figure 2.39 compares JPEG and
JPEG 2000 at the more modest 50:1 compression ratio. In this case, it is difﬁcult to see any difference
between the two images.
This last example underscores that good image quality at lower compression ratios can be achieved
by a variety of compression algorithms. The more challenging proposition of maintaining good image
quality at higher compression ratios begins to narrow the range of algorithmic choices. As with other
image processing algorithms, better performance usually comes at the cost of more computational
effort. In the case of standardized compression algorithms such as JPEG and JPEG 2000, this additional
computational complexity is offset partially by the availability of streamlined software libraries and
dedicated hardware designs that provide the beneﬁts of mature and robust code bases that can be
integrated easily into a digital camera image processing chain.

4.02.6 Processing Methods and Algorithms
63
(a) JPEG
(b) JPEG 2000
FIGURE 2.38
JPEG and JPEG 2000 comparison at 150:1.
(a) JPEG
(b) JPEG 2000
FIGURE 2.39
JPEG and JPEG 2000 comparison at 50:1.
4.02.6.5 Camera-speciﬁc algorithms
Up to this point in the section, the tacit assumption of an ideal raw image formation process (apart from
random noise) has been made. Unfortunately, this is almost never the case in reality. Capture optics
will impart the effects of lens aberrations, individual pixels will malfunction, and the consequences of
nonuniformities due to the color ﬁlter array and the silicon wafer itself will appear in the captured raw
image.
There are two general approaches to compensating for these effects. The ﬁrst approach is the ideal
one of posing a physical model for how the artifact was created and then reversing that model to
produce a correction process. When a viable model can be identiﬁed, an excellent level of correction
can be achieved. Optical effects governing how photons are distributed across the surface of the sensor
tend to fall into this category. However, when it comes to the creation and migration of photocharges,

64
CHAPTER 2 Image Quality in Consumer Digital Cameras
the underlying physics can become murky and deﬁnitive physical models elusive. In that case, more
heuristic methods become the main tools for addressing the corresponding artifacts.
4.02.6.5.1
Geometric and optical corrections
Geometric distortion and lateral chromatic aberration are both caused by variations in magniﬁcation,
as discussed in Sections 4.02.3.1.1 and 4.02.3.1.2. Geometric distortion is one of the more difﬁcult
and expensive aberrations to correct in lens design. Further mitigation in the processing chain can be
cost-effective, although not easy. Algorithmic correction of distortion is simple in theory, but can be
somewhat vexing in practice. Since distortions are approximately quadratic in nature, a simple second-
order transformation of the image data produces a satisfactory physical model of the aberration
u = a0 + a1x + a2y + a3xy + a4x2 + a5y2,
v = b0 + b1x + b2y + b3xy + b4x2 + b5y2.
(2.11)
In (2.11), (x, y) are the coordinates of the undistorted image point and

u, v

are the corresponding
coordinates of the distorted image point. {a, b} are the coefﬁcients of the transform. A minor difﬁculty
with this expression is that (u, v) will generally not be integer pairs and, therefore, not directly available
from the pixel data. However, bilinear or bicubic interpolation of neighboring pixel values to produce
the required pixel values at (u, v) is usually sufﬁcient from an image quality perspective. The major
difﬁculty is in determining {a, b}. If the camera has a ﬁxed focal length lens (no zoom capability), then
the coefﬁcients can be determined through a straight forward calibration process, i.e., imaging a test
chart with grid lines and then measuring the resulting distortions in the captured image. Regression
is used to compute the coefﬁcients from this data. When a zoom lens is considered, this calibration
process quickly mushrooms into an onerous task, for the coefﬁcients are generally not linear with zoom
setting. Therefore, a signiﬁcant number of sets of coefﬁcients may be needed to cover the zoom range
with sufﬁcient density so that any intermediate zoom position can be handled by a linear interpolation
of adjacent transform coefﬁcients. The maddening aspect of this process is that the lens position and
correspondingzoomsettingmaynotbereportedaccuratelybythecameraifthelensissmallandthezoom
mechanism inexpensive (a fairly common occurrence these days). For this reason, geometric distortion
in inexpensive digital cameras is either partially corrected or not corrected at all. This is not as untenable
a situation as it may ﬁrst appear. The human visual system has a remarkable ability to automatically
normalize effects such as distortion so that, very quickly, images with moderate amounts of distortion
will appear relatively undistorted and acceptable. Therefore, from an image quality perspective, it may
ultimately only be necessary to correct a portion of the distortion and rely on the human visual system
to deal with the remaining artifacts.
Correction of distortion-induced lateral chromatic aberration follows the same lines as described
above with the main difference being that each color channel has its own set of transform coefﬁcients.
Unfortunately, color fringing is more objectionable to the human visual system than normal distortion
effects. To address this, lens designers make efforts to reduce chromatic aberration of all types through
standard design practices, such as using lens element materials with differing dispersive properties.
However,thisprocessisgenerallyonlypartiallysuccessfulinacost-constrainedenvironment.Therefore,
as before, a partial correction approach is usually adopted.

4.02.6 Processing Methods and Algorithms
65
Finally, it is possible to dispense with the calibration process and allow the user in a heuristic
manner to apply distortion correction on the ﬁnished image by manually adjusting the coefﬁcients in
the transform. While this process can be time consuming and requires some computational skills, it has
the virtue of avoiding all of the calibration issues around coefﬁcient determination.
The algorithms for correcting geometric distortion do not address longitudinal chromatic aberration,
because it is a spatially dependent defocus of one or two color channels in the image. Correction or
mitigation usually involves convolution with a kernel that varies with the location in the image. This is
noticeably more complex than a geometric correction, and also more vulnerable to failure because of
variation in focus itself. While scene content that is at best focus may have the green channel in best
focus with red and blue slightly out of focus, scene content at other distances may have the red or blue
channel in best focus, or no channel in best focus. In general, the differences in MTF between color
channels change dramatically depending on overall focus, distance to the scene content, and position in
the image. Because of this, a calibration-based approach is usually avoided in all but the most controlled
situations (such as a ﬁxed focal length lens with a ﬁxed focus position), usually relying on interactive
adjustment of the correction if it is provided at all. While speciﬁc correction of longitudinal aberration
is rarely used, algorithms that adaptively reduce color fringes regardless of the cause are used in some
cameras [23]. Converting the image to a luma-chroma color space and adaptively blurring the chroma
channels will reduce colored fringes, although at some risk of blurring high contrast colored edges as
well. This approach is most effective with compact cameras with relatively poor optical MTF. Since
these cameras cannot image high-frequency scene detail, any high-frequency content is unlikely to
represent scene content.
4.02.6.5.2
Dark correction
One may take for granted that when no photons fall on a pixel, then no photocharges are generated and
a value of zero is produced. Unfortunately, this is generally not the case. Pixels that receive no light
can still collect signal due to mechanisms such as thermal generation of photocharges or diffusion of
photocharges from adjacent pixels. Pixels can also be defective and always generate non-zero signals
regardless of the number of photocharges present. Finally, the related effect of veiling glare produced
by the capture optics can redirect photons coming from brighter regions in the scene to the pixels in the
darker regions of the image, preventing them from producing appropriately low signals.
From an image quality perspective, blacks being rendered as blacks is extremely important and is,
perhaps, the most important location on the tone scale in this regard. As with memory colors, certain
important scene elements are “remembered” and expected to be blacker than they really were at the time
of capture. The pupil of the eye is a classic example of this. Generally, the blacker the pupil, the better
the image will appear, as long as corneal highlights and other three-dimensional cues are preserved.
Figure 2.40a is a slightly underexposed capture of an owl, which is, of course, the quintessential icon
when discussing large eyes. Simply scaling all of the code values in the image to brighten it, as in Figure
2.40b, does not produce the desired image quality improvement. Underexposure has been replaced by
low contrast and a general “washed out” appearance. Close ups of these images are seen in Figure 2.41a
and b. It can be seen that the large pupils in Figure 2.41b are no longer black, but a dark gray.
The solution to keeping blacks black is to apply dark correction to the image. The basic opera-
tion is simple: subtract some value, e.g., the blackest black within the image, from the entire image.

66
CHAPTER 2 Image Quality in Consumer Digital Cameras
(a) Original
(b) Exposure corrected
(c) Dark and exposure corrected
FIGURE 2.40
Dark correction example.
(a) Original
(b) Exposure corrected
(c) Dark and exposure corrected
FIGURE 2.41
Dark correction example (detail).

4.02.6 Processing Methods and Algorithms
67
The result of this can be see in Figures 2.40c and 2.41c. The pupils of the eyes now have the desired
deep blackness and the perceptions of the overall image contrast are improved, as well.
Unfortunately, as with other image processing operations, there are subtle interactions between dark
correction and image quality. The dark correction amount that is subtracted from the image can be
determined subjectively or objectively. Subjectively, one can simply adjust the dark correction in an
iterative manner until the image looks as pleasing as possible. Objectively, one may calibrate the camera
by imaging a test chart with black regions and determine the correction required to produce the desired
output code value. Selecting a desired output code value takes some care. A desired output code value
of zero, surprisingly, may not be preferred. Shadows, pupils, and other important scene elements may
become quantized and begin to look artiﬁcial and two-dimensional when set to a uniform zero code
value. For this reason, the dark correction usually aims to put blacks at a few code values above zero to
avoid quantization.
Finally, dark correction has far-reaching effects on color and tone processing. Both of these operations
assume a zero code value means zero signal, i.e., black. Transforming images with non-zero blacks
through the color correction matrix and the tone scale nonlinearity will tend to brighten the image and
lower the contrast, not unlike Figure 2.40b. This is because the non-zero black signal value will be
ampliﬁed along with the rest of the image signal as a result of color and tone scale correction.
4.02.6.5.3
Defect concealment
Withtoday’sdigitalcamerasensorscontaining12millionpixelsormore,itisunrealisticfromastatistical
perspective to expect that every pixel will operate properly under all circumstances. In fact, it could be
prohibitively expensive to produce such a sensor with every pixel functioning correctly. Fortunately,
from an image quality perspective, this is not necessary. Surprisingly, a large number of pixels can be
defective, and with simple image processing the presence of these defective pixels can be concealed.
Figure 2.42a shows a crop of an original image, which contains ﬁne detail in the railing and staircase.
Figure 2.42b is an extreme example of adding a signiﬁcant number of impulse noise events to represent
fully defective pixels. Assuming that the defective pixels are permanently defective (an assumption
that can be surprisingly weak), they can be mapped and a list of the locations of the defective pixels
passed to the defect concealment algorithm. This operation would then simply average the pixels around
each defective pixel, or, in the case of Figure 2.42c, compute a neighborhood median to be used as the
defective pixel replacement value. As a result, Figure 2.42a and c is visually identical. Rather than map
the locations of defective pixels and then restrict the defective pixel concealment to these locations,
a simple median ﬁlter could be applied to the entire image, in effect treating defective pixels as just
another noise source. The result shown in Figure 2.42d demonstrates that while the defective pixels are
concealed, much of the high-frequency detail in the railing and staircase have been degraded. It should
be emphasized that the median ﬁlter operations used in Figure 2.42c and d are the same. The only
difference between the processing for the two ﬁgures is the more selective application of the ﬁlter in
Figure 2.42c.
As parenthetically remarked above, determining if a pixel is defective is not always easy. Pixels can
be partially defective, failing for only certain ranges in value or for certain exposure index (analog gain)
settings. Manually accumulating a defective pixel map is usually too prohibitive due to the number of
defective pixels. Therefore, automatic defective pixel map generation is generally the only viable alter-
native. Due to the difﬁculty in identifying all the defective pixels and under which conditions they fail,

68
CHAPTER 2 Image Quality in Consumer Digital Cameras
(a) Original
(b) Defective pixels
(c) Defect concealed
(d) Median filtered
FIGURE 2.42
Defective pixel concealment example.
usually a compromise is made. Pixels that are clearly defective are mapped and corrected individually.
Pixels that are only partially defective are not mapped and treated as noise. It is in determining the
deﬁnition of what “clearly defective” is that provides the engineering degree of freedom.
A signiﬁcantly greater challenge is the case of structured pixel defects. The two main artifact types
are defective rows or defective columns and cluster defects. In the former, certain sensor architectures
can have common points through which all the signals from a row or column of pixels must pass, e.g.,
an output ampliﬁer or a pixel well at the end of the line. If that common point becomes defective,
then the entire row or column cannot be read and becomes correspondingly “defective” in the image,
although the pixels themselves may be functional. As with individual pixel defects, row or column

4.02.6 Processing Methods and Algorithms
69
defects can be mapped. However, a simple averaging of adjacent valid rows and columns may fail if it
occurs in the vicinity of a region of high spatial frequency. Figure 2.43 illustrates such a failure mode.
Figure 2.43a contains many narrow black lines superimposed over several broad colored bands.
A number of pixel columns are eliminated from this image in Figure 2.43b. Simple averaging of
adjacent columns produces the artifacts shown in Figure 2.43c where the defective columns crossed
the dark line designs. One solution to preventing these artifacts is to use a steerable averaging process
that attempts to average along edges rather than across them [24]. In Figure 2.44 the value X5 from
(a) Original
(b) Defective columns
(c) Defective columns concealed
(d) Adaptive concealment
FIGURE 2.43
Defective columns concealment example.

70
CHAPTER 2 Image Quality in Consumer Digital Cameras
P1
X4
P7
P2
X5
P8
P3
X6
P9
FIGURE 2.44
Defective column pixel neighborhood.
the defective column can be estimated by either

P1 + P9

/2,

P2 + P8

/2, or

P3 + P7

/2. A local
edge-based classiﬁer is used to determine which of these estimates is best. The result of using this
approach is shown in Figure 2.43d. The vast majority of the artifacts in Figure 2.43c are eliminated.
As a segue into cluster defects, double column and double row defects also occur. These can be
extremely hard to conceal in a satisfactory manner to the point that it is common to outright reject
sensors with double column or double row defects. While adaptive averaging techniques can be applied
as with single column and single row defects, the results are usually not as successful.
The most challenging situation is with defective pixel clusters. Sometimes this occurs with the failure
of an electronic element shared by several pixels, such as a ﬂoating diffusion node. At other times it is
simply the result of a ﬂaw in the sensor materials or manufacturing process or dirt on the cover glass. The
result can be a “blob” of defective pixels that is large enough in extent that straightforward averaging
techniques are insufﬁcient. As a result, more advanced methods such as inpainting are required, although
good results are not guaranteed [25]. Since the computational complexity of these advanced methods
can escalate signiﬁcantly, it is not uncommon to reject sensors with cluster defects outright, as with
sensors with double rows or double columns.
4.02.6.5.4
Correction of nonuniformities
Section 4.02.6.5.1 discussed correcting distortions and nonuniformities in the geometric locations of
the pixel data. This section addresses correcting distortions and nonuniformities in the photometric
response that vary with location within the image. These nonuniformities can be generalized into two
categories: errors in black offset and errors in pixel gain.
Figure 2.45a is an image suffering from nonuniformities in black offset and pixel gain. Perhaps the
most obvious consequence is that the right side of the image is brighter than the rest of the image. If the
same camera system and setup were used to capture a black frame (e.g., a lens cap shot), an image like
Figure 2.45c might be produced. This kind of black offset error can sometimes be caused by one side
of the sensor being too close to certain camera components such as the power supply or rear display.
With a black ﬁeld capture in hand, the correction algorithm is to simply subtract the black ﬁeld from
the original image. It can be seen in Figure 2.45b that the right- and left-hand sides of the image are
now balanced in brightness.
Returning to Figure 2.45a, the sky appears darker in the corners than in the center of the image.
Assuming the sky in the scene is actually relatively uniform in brightness, this could be explained by
either nonuniform pixel sensitivity or vignetting due to the capture optics. The correction scheme would
be the same in both cases. If the camera were pointed upward so that the ﬁeld of view was completely

4.02.7 Applications
71
(a) Original
(b) Corrected
(c) Black frame
(d) White frame
FIGURE 2.45
Nonuniformity correction example.
ﬁlled with blue sky of uniform illuminance, then a resulting white frame as shown in Figure 2.45d
would result. It can be seen that the captured image is brightest in the center and darkest in the corners.
The corresponding gain correction for each pixel location would be the brightest pixel value in the entire
white frame (in this case, the central value) divided by the white frame value at the pixel location in
question. Therefore, the gain correction would be close to one in the middle of the image and somewhat
greater than one at the corners. This correction has also been applied to Figure 2.45b and restores the
brightness in the corners of the image, most visibly in the upper left-hand corner.
From an image quality perspective, the human visual system appears to adapt well to darkened
corners to the point that small amounts of this nonuniformity may not be noticed with a casual glance
by the observer. However, a nonuniform black level tends to be much more apparent and objectionable.
As pointed out in Section 4.02.6.5.2, it is important from an image quality perspective that blacks appear
black. This observation can now be augmented to blacks must appear black everywhere.
4.02.7 Applications
Consideration of image quality is useful in a wide range of applications related to digital cameras.
The most obvious application is development of improved algorithms for processing raw captured
images, whether processed in the camera or on another computer. Processing for raw digital camera

72
CHAPTER 2 Image Quality in Consumer Digital Cameras
images continues to evolve, and it is critical to understand which processing changes make the most
important improvements to an application or operating regime.
A related application is the development of improved camera designs and operating modes. Under-
standing the quality impact of hardware trade-offs enables the development of cameras and operating
modes that extend the envelope of quality, convenience, and performance. As readout rate and process-
ing power continue to increase, imaging modes involving synthesis of multiple captures into improved
images are more feasible. Several consumer cameras include multiframe modes as special features, such
as multi-frame super-resolution capture or multi-frame panoramas, usually selected with an option on
a dial or menu. A simpler mode selection is to reduce the resolution of the camera at higher ISO values.
To the best of our knowledge, little work has yet been published on automatic switching between modes.
Development of post-processing algorithms also beneﬁts from an understanding of quality problems
and artifacts in images from digital cameras, whether the purpose is to mitigate existing quality problems,
such as reduce noise, or to create images serving a new purpose, such as creating super-resolution still
images from a video.
4.02.8 Open issues and future directions
Development of metrics relating design parameters and system measurements to perceptual quality
remains an open area of research. Some current work combines color difference models and human
contrast sensitivity functions to do a better job of measuring the visibility of differences in color images,
especially those caused by processing variations. Other efforts addressing quality attributes, such as
loss of texture, seek to complement measures of system response to step edges and noise in ﬂat ﬁelds.
Improved standards for camera testing, especially measuring the visual impact of noise and sharpness
in more complex scenes, are currently being developed. The temporal nature of vision is beginning to
be considered more widely, especially related to video [26]. Video compression is an especially active
area of research.
Much of the work in image and video quality is referred to as “full-reference,” meaning it begins
with full uncompressed, high-quality images, such as [27,28]. Thus, analysis comparing a degraded
image or video to the original is feasible. This is very useful in applications of compression and
transmission, where the problem is to predict the quality degradation from speciﬁc compression and
transmission problems. This is somewhat different from the camera design problem, where camera
design and processing parameters are being optimized to provide satisfactory results over a range of
capture scenarios. These distortion-based quality models must be complemented with models relating
design parameters to image distortions in order to be applied in camera optimization.
Consideration of image quality in still and video image selection and management is also an open
area, especially because image quality for image management is strongly affected by image aesthetics,
often relegating technical image quality issues to a secondary role. Again, this problem is somewhat
different from the camera design, or the compression and transmission applications. Even without the
aesthetics issues, this case is complicated because it is a form of the “no-reference” quality problem,
since the goal is to estimate image quality without any reference image corresponding to the image
being analyzed.

4.02.9 Implementations
73
Several technology trends affect image quality in digital cameras. One dominant trend has been the
shrinkage of pixel sizes, from a pitch of 7 µm for the ﬁrst consumer grade digital camera sensors to
1.12 µm by 2011. This trend has nearly obviated anti-aliasing ﬁlters for most consumer cameras, while
tending to starve them for signal. This trend is slowing somewhat and will probably change, especially
since pixels with a size near one wavelength of the light they are capturing behave in more complex
ways than larger pixels.
To mitigate the problems of pixel shrinkage, back-side illumination (BSI) technology is becoming
more common. Researchers are exploring pixels with highly complex circuitry that may become more
practical with the ability to put circuitry underneath the pixel, rather than compete for space in the plane
of the pixel.
Camera electronics continue to support increased readout speed. Processing power in consumer
cameras also continues to increase, although much of the increased processing capability is required
simply to accommodate increased pixel counts and higher frame rates.
At the low end of the camera market, such as cameras within devices meant primarily for other
purposes (mobile phones, notepads), explorations of cost, size, and quality boundaries continue. Some
of these devices require processing steps to correct substantial distortions and nonuniformities, to the
extent that other secondary quality artifacts can arise. One example is noise spatially varying within
the image, caused by gain correction of nonuniformity. This can be mitigated by more aggressive noise
reduction, but then loss of sharpness or texture tends to result. Overall, the image quality from some of
these cameras can be relatively poor, so improving their performance is a continuing need.
More complex CFA patterns are being considered, especially from a theoretical perspective.
Commercial adoption of more complex patterns is slower, because of the cost of signiﬁcant changes in
the processing path.
Research in multi-frame capture approaches is continuing and growing in importance. The most
common applications are capture of scenes with high dynamic range, capture of scenes with motion,
and capture of scenes with signiﬁcant depth. All have a common difﬁculty in that the scene cannot be
completely captured in a single image.
4.02.9 Implementations
Implementation issues for the constrained environment of a digital camera have already been discussed
elsewhere in this chapter. Publicly available implementations of software of interest are listed below,
emphasizing links to open source image and video quality analysis:
•
The Cornell University Visual Communications Lab hosts Metrix Mux, a MATLAB package sup-
porting the development and testing of visual quality metrics, along with links to several of the
analysis algorithms included in the software. See the URL: http://foulard.ece.cornell.edu/gaubatz/
metrix_mux/.
•
The Independent JPEG Group (IJG) hosts a widely used free library for working with JPEG (DCT)
compression. See the URL: http:/www.ijg.org/.
•
ImageJ is an open source package for image processing and analysis written in Java, primarily
developed at the United States National Institute of Health. See the URL: http://rsbweb.nih.gov/ij/.

74
CHAPTER 2 Image Quality in Consumer Digital Cameras
•
The Joint Photographic Experts Group (JPEG) hosts many links of interest, including several JPEG
2000 implementations. See the URL: http:/www.jpeg.org/.
•
libRaw is an open source package for digital camera processing chains, including reading various
proprietary RAW ﬁle formats. See the URL: http://www.libraw.org/.
•
The Louisiana State University Computer Vision Laboratory hosts implementations of algorithms
for demosaicking, denoising, and super-resolution. See the URL: http://www.ece.lsu.edu/ipl/.
•
The Oklahoma State Computational Perception and Image Quality Lab hosts links to code for the
Visual Signal-to-noise ratio (VSNR) algorithm measuring visibility of image differences and the
Most Apparent Distortion (MAD) algorithm. See the URL: http://vision.okstate.edu.
•
OpenCV is an open source computer vision library that includes standard library support for several
still image formats as well as machine vision algorithms. See the URL: http://opencv.willowgarage.
com/wiki/.
•
The Society for Imaging Science and Technology (IS&T), has software and resources for analysis
of camera performance on their web site. See the URL: http://www.imaging.org/.
•
Stanford University hosts the Camera 2.0 project, with publicly available code for controlling cam-
eras for research in computational photography, including their open source camera that runs Linux.
See the URL: http://graphics.stanford.edu/projects/camera-2.0/.
•
The University of Texas at Austin Laboratory for Image and Video Engineering hosts code for a
numberofalgorithmsrelatedtoqualityassessment.SeetheURL http://live.ece.utexas.edu/research/.
4.02.10 Datasets
For image processing and image quality work, the most commonly used datasets are sample images.
Below is a list of image collections, emphasizing those with associated subjective quality data:
•
The Institut de Recherche en Communications et Cybernétique de Nantes (IRCCyN) maintains a
database of images and associated subjective data for quality evaluation. See the URL http://www2.
irccyn.ec-nantes.fr/ivcdb/ .
•
The Society for Imaging Science and Technology (IS&T) hosts a set of images implementing the ISO
20462–3 Standard Quality Scale as a softcopy ruler. These images are publicly available, although
not free, at the IS&T web site. See the URL: http://www.imaging.org/.
•
Kodak released a disk of PhotoCD (PCD0992) of images into the public domain. The set is
currently available courtesy of Rich Franzen. The site listed here has no associated subjective
data, but the Tampere database, TID2008, used these images as a starting point. See the URL
http://r0k.us/graphics/kodak/.
•
The Oklahoma State University Computational Perception and Image Quality Lab maintains the
CSIQ database, a collection of images and associated subjective data. See the URL http://vision.
okstate.edu/index.php?loc=csiq.
•
The Tampere University of Technology, Finland, developed the Tampere image database, TID2008,
a collection of images and associated subjective data. See the URL http://www.ponomarenko.info/.
tid2008.htm.
•
The University of Southern California Signal and Image Processing Institute maintains the USC-SIPI
Image Database, a collection of standard images. See the URL http://sipi.usc.edu/database/.

4.02.10 Datasets
75
•
The University of Texas at Austin Laboratory for Image and Video Engineering hosts the LIVE Image
Quality and Video Quality Assessment Databases, collections of images or videos and associated
quality data. See the URL http://live.ece.utexas.edu/research/Quality/index.htm.
•
The University of Toyama Media Information and Communication Technology Laboratory has
published a database of images and associated See the URL http://mict.eng.u-toyama.ac.jp/.
•
The University of Waterloo maintains an image repository with many of the standard test images.
See the URL http://links.uwaterloo.ca/Repository.html.
•
The Video Quality Experts Group was created to advance the ﬁeld of video quality analysis, includ-
ing performing and publishing subjective experiments. Their web site also includes links to other
sites of interest, especially to image and video databases used for quality research. See the URL
http://www.its.bldrdoc.gov/vqeg/.
Glossary
Acutance
originally, referred to the local contrast of edges in photographic
systems. Since the development of Fourier analysis of optical
systems, more commonly used as an integral of a system modu-
lation transfer function (MTF) cascaded with a human contrast
sensitivity function (CSF), to compute a value correlating with
system sharpness
Bokeh
the appearance or texture of out of focus regions in an image.
Usually used with images that have deliberate separation
between a well-focused subject and an out-of-focus background
Chrominance
a channel of an image that contains information corresponding
approximately to the human sensation of color (blue-yellow,
green-red), as opposed to the human sensation of lightness.
Usually not corresponding to a precise colorimetric response,
but rather as an image channel that is processed to provide
information on color rather than lightness
CIELAB
a uniform color space standardized by the CIE (Commission
Internationale de l’Éclairage), in which a one-unit difference is
approximately one just-noticeable difference throughout color
space
Contrast sensitivity function (CSF)
afunctiondescribingtherelativesensitivityof thehumanvisual
system to modulation at different spatial frequencies
Demosaick
the process of converting a single image captured with a color
ﬁlter array pattern into a full-color image
Diffraction-limited
describing an MTF matching the theoretical response due to
diffraction from an aperture of a speciﬁc size. Practically, the
best lens MTF possible for a given aperture, since it allows for
no lens aberrations

76
CHAPTER 2 Image Quality in Consumer Digital Cameras
Just-noticeable difference (JND)
a difference that is considered just signiﬁcant in human percep-
tion. For example, a pair of images can be presented and users
asked to select one of the two images. If each of the images
is selected half of the time, there is essentially no difference
between them. Conversely, if one image is selected 75% of the
time, then the images are different by a 50% JND—the differ-
ence is detected half of the time, and randomly selected in half
of the instances when the difference is not detected
Luminance
referstoimagechannelscontaininginformationprimarilyabout
the lightness of a scene. In this chapter, does not correspond to
an accurate colorimetric luminance response, but rather to an
image channel that is processed to provide scene lightness detail
Nyquist frequency
as used in this chapter, one half of the sampling frequency
Photo-charge
a quantum of charge representing a captured photon. In some
sensors, this is a photo-electron. Other sensors actually store
a hole caused by the photon absorption, rather than the photo-
electron itself
Pixel pitch
the spatial sampling interval for an image sensor. Most sen-
sors have “square pixels,” with the horizontal pitch equal to the
vertical pitch
Psychometric
pertaining to quantitative measurement and modeling of human
mental processes. In this chapter, used to refer to measuring and
modeling the subjective perception of image quality
References
[1] J.E. Adams, J.F. Hamilton, Digital camera image processing chain design, Single-Sensor Imaging: Methods
and Applications for Digital Cameras, CRC Press, 2009, pp. 67–103.
[2] B.E. Bayer, Color Imaging Array, United States Patent 3 971 065, July 1976.
[3] R. Palum, Optical antialiasing ﬁlters, Single-Sensor Imaging: Methods and Applications for Digital Cameras,
CRC Press, 2009, pp. 105–135.
[4] J.E. Adams, J.F. Hamilton, M. Kumar, E.O. Morales, R. Palum, B.H. Pillman, Single capture image fusion,
Computational Photography: Methods and Applications, CRC Press, 2011, pp. 1–62.
[5] K. Hirakawa, P. Wolfe, Spatio-spectral color ﬁlter array design for enhanced image ﬁdelity, in: Proceedings
of the ICIP, 2007, pp. II-81–II-84.
[6] Colorimetry, CIE Publication No. 15.2, Central Bureau of the CIE, Vienna, 1986.
[7] R.W.G. Hunt, Measuring Colour, second Ed., Ellis Horwood, London, England, 1991.
[8] S. Süsstrunk, R. Buckley, S. Sven, Standard RGB color spaces, in: Seventh Color Imaging Conference: Color
Science, Systems and Applications, Scottsdale, Arizona, USA, November 1999, pp. 127–134.
[9] K.E. Spaulding, E.J. Giorgianni, G. Woolfe, Optimized extended gamut color encodings for scene-referred
and output-referred image states, J. Imag. Sci. Technol. 45 (2001) 418–426.

References
77
[10] International Organization for Standardization, Geneva, Switzerland, ISO 12232:2006, Photography Digital
Still Cameras—Determination of Exposure Index, ISO Speed Ratings, Standard Output Sensitivity, and
Recommended Exposure Index, 2006.
[11] R.W.G. Hunt, The Reproduction of Colour, sixth ed., John Wiley & Sons, Ltd., Chichester, England, 2004.
[12] B.W. Keelan, Handbook of Image Quality, Marcel Dekker, New York, NY, USA, 2002.
[13] B.W. Keelan, E.W. Jin, S. Prokushkin, Development of a perceptually calibrated objective metric of noise,
in: Proc. SPIE, vol. 7867, 2011, pp. 786707-1–786707-12.
[14] International Organization for Standardization, Geneva, Switzerland, ISO 20462–3:2005, Photography
Psychophysical Experimental Methods for Estimating Image Quality—Part 3: Quality ruler method, 2005.
[15] G. Johnson, M. Fairchild, A top down description of S-CIELAB and CIEDE2000, Color Res. Appl. 28 (6)
(2003) 425–435.
[16] J.E. Adams, Interaction between color plane interpolation and other image processing functions in electronic
photography, in: Proceedings of the SPIE, San Jose, California, vol. 2416, February 1995, pp. 144–151.
[17] R. Ramanath, W.E. Snyder, D. Hinks, Image comparison measure for digital still color cameras, in: Proceed-
ings IEEE International Conference on Image Processing, vol. 1, 2002, pp. 629–632.
[18] J.E. Adams, Design of practical color ﬁlter array interpolation algorithms for digital cameras, in: Proceedings
of the SPIE, San Jose, California, vol. 3028, February 1997, pp. 117–125.
[19] J.E. Adams, J.F. Hamilton, Adaptive color plan interpolation in single sensor color electronic camera, United
States Patent 5 506 619, April 1996.
[20] J.F. Hamilton, J.E. Adams, Adaptive color plan interpolation in single sensor color electronic camera, United
States Patent 5 629 734, May 1997.
[21] W.B. Pennebaker, J.L. Mitchell, JPEG Still Image Data Compression Standard, Van Nostrand Reinhold,
New York, 1993.
[22] A. Skodras, C. Christopoulos, T. Ebrahimi, The JPEG 2000 still image compression standard, IEEE Signal
Process. Mag. 18, (2001) pp. 36–58.
[23] R.J. Palum, B.H. Pillman, L.V. Larsen, Color fringe desaturation for electronic imagers, United States Patent
7 577 311, August 2009.
[24] J.F. Hamilton, Correcting defects in a digital image caused by a pre-existing defect in a pixel of an image
sensor, United States Patent 6 900 836, May 2005.
[25] T. Shih, R.-C. Chang, Digital inpainting—survey and multilayer image inpainting algorithms, in: Third
International Conference on Information Technology and Applications, 2005, ICITA 2005, vol. 1, July
2005, pp. 15–24.
[26] K. Seshadrinathan, A. Bovik, Temporal hysteresis model of time varying subjective video quality, in:
IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2011, May 2011,
pp. 1153–1156.
[27] Z. Wang, A. Bovik, H. Sheikh, E. Simoncelli, Image quality assessment: from error visibility to structural
similarity, IEEE Trans. Image Process. 13 (2004) 600–612.
[28] D. Chandler, S. Hemami, VSNR: a wavelet-based visual signal-to-noise ratio for natural images, IEEE Trans.
Image Process. 16 (2007) 2284–2298.

3
CHAPTER
Image and Document
Capture—State-of-the-Art and
a Glance into the Future
Kathrin Berkner
Ricoh Innovations, Inc., Menlo Park, CA, USA
4.03.1 Introduction
An image capture process converts a speciﬁc scene information into digital numbers. The scene content
can be a photographic object, e.g., a group of people or a sunset, a scientiﬁc object, such as cells on a
microscope slide, or stars in an astronomy application, or a document, which contains text printed or
written on a ﬂat surface, e.g., paper. In this chapter we provide an overview of the latter category, the
capture of documents. The capture process differs from that for the other two image categories due to the
following facts: Conventional documents are considered to have content that is printed or written on a ﬂat
surface and does not inherit any depth compared to a photographic scene. Those conventional documents
contain mostly text with very little visible color information since text is written or printed typically
in dark ink or toner of one color on a light background. Another important factor is the purpose of the
document capture, which has been two-folded. One purpose is storage of the document for potential
future reading or printing, e.g., in legal scenarios. Another one is to extract text automatically and enter
it into a backend database. Such text extraction is performed via optical character recognition (OCR)
technologies. In that case, the consumer of the captured digital data is not a human observer, but a
computer. As a consequence, processing of captured document information is performed via pattern
recognition and machine learning. In contrast, photographic scene content is processed with algorithms
targeting the human visual system and human preferences in general.
Document capture technology has been researched and commercialized for many decades, starting
with FAX technology over analog and digital document scanners to today’s mobile scanning technology
via smartphones. These ubiquitous mobile capture platforms have changed the paradigm of document
capture not just from the device architecture point of view, but also the deﬁnition of a document from
being paginated printed text to a snapshot of whiteboards, bar codes, and street signs.
We start with a historic review of the conventional stationary document scanning technology which
has reached a very mature stage and has been commercialized in many different ways. Then we point
out a few research directions that have evolved over the past few years with still interesting open
research questions: multispectral document capture and processing, image capture/processing/user
experience with mobile devices, and a glance at an evolving change in the sensing paradigm from
conventional image capture to information sensing via new camera architectures and data processing
algorithms.
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00003-0
© 2014 Elsevier Ltd. All rights reserved.
79

80
CHAPTER 3 Image and Document Capture—State-of-the-Art
4.03.2 Basic steps of conventional document capture processing
Image-based document capture has been performed traditionally in two different ways, in an ofﬁce
to capture content from a limited number of loose or stapled sheets of paper, such as letters, papers,
receipts, or in a commercial setting to capture content from large amounts of pages from bound books.
In the ofﬁce work place, a document scanner has been used to capture content printed on paper and
convert it into digital data. Such a scanner can be a stand-alone device or can be integrated into a
Multifunction Device, that bundles the scanning capability together with routing, paper handling, and
printing (see Figure 3.1). Whereas such scanners are designed for feeding one individual document
page at a time, there are special book scanners, often camera based, that are able to capture content
of a book without removing the binding. Figure 3.2 shows examples from [1] of stationary camera-
based scanning technologies for digitization of books. In the personal technology space, digital cameras
have been used for more than 20 years, capturing visual information and creating “pretty pictures” for
people’s entertainment. Their design space today includes digital single-lens reﬂex (DSLR) and point-
and-shoot cameras, as well as highly capable mobile phone cameras. Digital camera technology is also
FIGURE 3.1
Today’s document capture devices: desktop scanner, multi function device, and mobile phone.
FIGURE 3.2
Different camera set-ups for book capture from [1].

4.03.2 Basic Steps of Conventional Document Capture Processing
81
used in scientiﬁc settings such as laboratories or on satellites for analysis and inspection tasks. Consumer
digital cameras have recently gotten very popular to use for document capture, replacing the need for a
document scanner in situations where only a few pages, a poster, a whiteboard, or a slide show need to
be captured and complicated paper handling is not required.
In the following we give an overview of the entire imaging pipeline of a document capture process
and the required signal processing steps. The capture process needs to convert light focused on a detector
surface into electrons that are converted into digital data. Focusing of light reﬂected from the object onto
the detector is done via mirrors and lenses. The detector is either a conventional 2D image sensor (CCD or
CMOS), or a linear array as used in scanners. Signal processing methods for digitized document images
span areas from pixel-level processing (e.g., demosaicing, tone mapping), to higher-level semantic and
application speciﬁc processing (e.g., text enhancement or layout extraction).
4.03.2.1 Image capture pipeline
In a conventional 2D image scanner, a detector containing a pixel array with very small number of
pixels in the vertical direction, and a large number of pixels in the horizontal direction, is moved over
a document capturing information along small stripes of a document (ﬂatbed scanner). Mechanically
driven scanners that move the document are typically used for large-format documents, where a ﬂatbed
design would be impractical. The ﬂatbed scanning architecture allows for very high speed complex
document processing, e.g., processing of 100 pages per minute (ppm). A digital camera image sensor,
in contrast, contains a full 2D array with similar large numbers of pixels in both directions. When such
a 2D array is used, the detector does not have to move with respect to the document. The high-speed
ppm-performance of scanners, however, cannot be achieved with those 2D images sensors in digital
cameras due to the higher data read out complexity and memory requirements of 2D sensor arrays.
When image capture is performed in a controlled environment via scanning or ﬁxed-mounted cam-
eras, illumination setting and positioning of the object is controlled. Knowledge of the data acquisition
parameters enable either real-time adjustment before capture or digital post-processing algorithms to
guarantee acceptable image quality. Such a controlled environment for document scanning is still the
norm in scenarios where large numbers of pages are scanned at high speed, e.g., in law ofﬁces. With
the increasing capabilities of mobile phones, however, capturing of single page documents is for many
people easier to perform by taking a picture with the mobile phone camera. In this case, illumination
variations and hand movement may severely inﬂuence the quality of the captured image. A big chal-
lenge in the hand-held scenario is then to estimate the variations in illumination and movement such
that digital post-processing algorithms can correct for the degraded quality [1].
4.03.2.2 Digital image processing steps
Image processing algorithms that typically need to be performed for complete image capture can
be categorized into low-level methods, such as color enhancement and noise removal, medium-level
methods such as compression and binarization, and higher-level methods involving segmentation, detec-
tion, and recognition algorithms extract semantic information from the captured data. An overview of
the signal processing for processing of scanned document data is given in Figure 3.3 and contrasted
with the imaging pipeline for photographic images, as captured with a consumer digital camera.

82
CHAPTER 3 Image and Document Capture—State-of-the-Art
Signal processing pipeline for captured photographic image
Signal processing pipeline for captured scanned document image
Sensor data
Demosaicing,
       color 
   balancing
Noise removal,
          text
  enhancement
Binarization,
layout analysis
Optical Character
Recognition
Compression for
transmission or
archiving
Rendering for
display or printing
Context information
extraction for
Semantic analysis
(e.g. object recongition,
tagging)
Compression for
transmission or
archiving
Rendering for
display or printing
Object analysis
    Noise removal,
image, HDR imaging,
  enhancement for
 human perception,
 image stabilization
Demosaicing,
       color
   balancing
Sensor data
FIGURE 3.3
Overview of a imaging pipelines for scanned document images (top) and photographic images captured with
a consumer digital camera (bottom).
4.03.2.2.1
Color balancing
Most document scanners today are color scanners and require color processing similar to that performed
in digital cameras. Low-level image enhancement methods work directly on sensor measurements.
In order to capture color information of an object, the sensor is typically covered with red, green, and
blue spectral ﬁlters, placed over individual sensor pixels during manufacturing in form of the Bayer
pattern. Linear arrays are also used in ﬂatbed scanners where each row contains one of the red, green,
or blue ﬁlters. As a consequence of this spectral separation of information at each pixel, a demosaicing
algorithm has to be applied to the raw detector data in order to create a red, green, and blue image of the
scene. Applying state-of-the-art color models, these three-channel RGB data are then processed to create
color images as speciﬁed by the target applications [2]. For the human observer, RGB data are adjusted
according to the human perception. For example, skin colors are preferred to be presented in pictures
in a speciﬁc way, similarly parts of a natural scene, such as grass and sky. On the contrary, displays
or printers have their own restrictions on color rendering, e.g., linearity in luminance, color stability.
Established color management standards try to address and compensate for those restrictions applying
tone mapping methods as well as color space conversions. For general reference on management of
digital color see, e.g., [3,4]. Creating a faithful color reproduction of an original scene using digital
data from only three color channels captured with an RGB sensor is a challenging problem since
the RGB ﬁlters cannot reproduce the entire color gamut. Often the dynamic range of a scene is too
large compared to the dynamic range of the detector. Dynamic range compression and quantization
limit the ﬁdelity of the scanned document in addition to constraints given by displays or ink and toner.
Adaptation to illumination and local contrast have long been topics researched intensively in the imaging
science community. With the arrival of abundant sensors on image capture devices such as smartphones,

4.03.2 Basic Steps of Conventional Document Capture Processing
83
problems like illumination sensing may be addressed with new algorithms taking into account luminance
estimations from different sensors on the device, such as ambient light sensors. Enrichment of the color
gamut can be performed on the display side using, e.g., high dynamic range displays [5] or additional
ink for a printing application. Introducing additional color channels during capture requires quite a
change in the hardware to include additional spectral ﬁlters in the optical path or on the sensor [6].
4.03.2.2.2
Noise removal and image enhancement
During the conversion of photons to electrons the detector introduces noise into the digital data. Such
detector noise is typically categorized into signal-dependent shot noise, detector-dependent dark- and
read-noise, and quantization noise that is detector- and signal-dependent [7]. Different color channels
very often have different noise characteristics caused by, e.g., wavelength-dependent sensitivity of the
detector and wavelength-dependent quantization noise. A noise model often used in the literature for
detector noise in digital camera images is Gaussian noise of zero mean and certain standard deviation.
Even though this model is convenient to handle in signal processing algorithms, it is only a coarse
approximation to real sensor noise and often limits its applicability in product implementations. Sensor
technology has advanced to a point where the signal-independent read noise, that is modeled as Gaussian
noise with zero mean, is very low compared to signal-dependent shot noise, which is better modeled
as Poisson noise. The difference is especially important in low-light conditions where only a small
number of photons reaches the sensor. Limitations of both models for consumer photography are due
to a complex sensitivity of the human visual system to high frequency patterns. Human observers often
prefer some level of noise or high frequencies in image regions and refer to completely noisy-free
areas as “too ﬂat” [8]. In very specialized imaging applications it is often necessary to carefully derive
system-speciﬁc noise models, e.g., low-photon imaging applications, such as low-light photography.
For example, images of whiteboards captured with mobile phones are often captured in low-light
conditions, introducing severe background noise, that might need to be removed before printing a copy
of the whiteboard image [9] (Figure 3.4).
FIGURE 3.4
An image of a whiteboard captured with a mobile phone camera, and a version that is processed for
printing [9].

84
CHAPTER 3 Image and Document Capture—State-of-the-Art
FIGURE 3.5
One page of the Archimedes palimpsest showing artifacts from overwriting of original ink [10] (left), and
bleed through from printing double-sided on thin paper [11] (right).
A document to be captured is printed originally, i.e., some frequency characteristics inherent to the
printing process are apparent, e.g., screen halftone or error diffused halftone patterns. These printing
frequencies can be considered as noise in the scanned image and have to be taken into account in any
further processing such as text sharpening, downsampling, or rotation [12].
Other appearances of noise, that are considered visual artifacts, include bleed through in documents
due to writing with or printing ink on both sides of a thin paper (Figure 3.5, right) [13,14], or by
writing text on top of other content as it appears in some historical documents. The special case of
overwriting old text has received a lot of publicity when the Archimedes palimpsest was discovered and
analyzed [15] (Figure 3.5, left). More details on capture and imaging of historical documents is given
in Section 4.03.3.1.
4.03.2.2.3
Data compression
With the number of detector pixels in an imaging sensor increasing and wireless data connection often
a preferred choice for data transfer, the need for compression of data is unavoidable. Even though still
image compression has been a vibrant area of research in the 1980s and 1990s, performances seemed
to have reached a limit where improvement is only possible with severely increased complexity. For
still image capture, JPEG [16] is still the most commonly used compression format in the consumer
space. Even though the scalable wavelet-based JPEG2000 format introduced in the 1990s [17] seemed
to have very desirable features, it has not been adapted as the mainstream compression format as many
researchers had hoped for. For text-speciﬁc compression, JBIG and JBIG2 compression technologies are
used for efﬁcient compression of bilevel images [18] focusing on text and halftone compression. JBIG2
segments the image into regions of text, regions of halftone images, and regions of other data. Each
of these categories uses a different encoding/decoding technology to best compress the information in
each category.
In video mode, however, streaming applications from and to mobile devices still have headroom for
novel technology. One scenario where document content is transmitted via streaming video is during

4.03.3 Document and Image Capture Applications that are Still Challenging Today
85
lectures in an educational setting. As an example, the ClassX project at Stanford University utilized
novel video encoding and streaming technology to let users on smartphones zoom in on regions of
interest, such as slides presented in a lecture [19].
4.03.2.2.4
OCR pre-processing
OCR technology is based on pattern recognition technologies and requires an input that is pre-processed
to be handled appropriately by the recognizer. Pre-processing steps include a binarization of the docu-
ment, skew correction, and layout analysis. Binarization converts a gray scale or color image into a binary
image, where ideally one color represents the background and the other the text. Skew correction rectiﬁes
the image and compensates for rotation and warping of a piece of paper during capture. Layout analysis
breaks the binarized rectiﬁed image into logical text components such as header, title, columns, footer.
Those document pre-processing steps have become commodity processing steps, are part of commercial
OCR software packages, and have little headroom for innovation in traditional scanning scenarios.
4.03.2.2.5
Segmentation, classiﬁcation, recognition
In traditional document processing, higher level tasks such as segmentation, classiﬁcation, and recog-
nition are performed as a part of compression or an optical character recognition process. In an OCR
engine, identiﬁed blobs of potential text or ﬁgures need to be grouped into semantic units, e.g., lines or
words, and then being recognized as individual characters. OCR technology is very mature, available
in commercial products and open source (e.g., [20]). In a compression process, text, line, and pictorial
scenes need to be segmented in order to be coded in different ways.
With the arrival of Web 2.0 and social networking technologies, much effort has been focussed
on semantic interpretation of images leading to applications such as geo-tagging of faces detected
in photographs. Another driver for semantic analysis is the rising number of augmented reality (AR)
applications. In these applications, certain features in an image are recognized, meta-data associated
with those features retrieved via the Internet and overlayed on real-world images. The technology is
used for consumer applications on smartphones (e.g., [21]), but also for military or medical applications
in form of head-mounted displays or virtual retinal displays [22].
Segmentation, tracking, and rendering technologies are necessary components for these applications
that feed back into information display assisting in the image capture process. Whereas advanced sensor
technology and computational power on mobile platforms have driven AR technology into the consumer
space, there are still some severe barriers to overcome. One is the difﬁculty of creation of content that can
be linked to the extracted features. Someone has to ﬁrst collect the content, then create an index. Another
barrier is the inconvenience of wearable display technology. Today’s heads-up display technology still
has bulky form factors that are preventing users from using the technology for a longer period of time
or in a public setting.
4.03.3 Document and image capture applications that are still
challenging today
Single-page document capture of modern printed documents is a very mature area of research. Some
headroom still exists for processing of certain classes of document images that contain less popular

86
CHAPTER 3 Image and Document Capture—State-of-the-Art
languages or complex components such as math formulas or graphics. For these categories, OCR pre-
processing,OCR,andhigher-levelsemanticprocessingisnotasolvedproblem.Therearestillchallenges
across the entire imaging pipeline that need to be overcome in the area of capturing and processing of
historical documents. Other classes of document images that still has some research opportunities is
that of multi-page documents or large collections of documents. For those classes, adaptive learning
algorithms for OCR pre-processing, OCR, and higher-level semantic processing such as categorization
into chapters or other groupings. The following sections describe those challenges in more details.
4.03.3.1 Multispectral analysis of documents
Historical documents often suffer from degradations not present in modern printed documents. Such
degradations are faded handwritten characters, bleed through due to multiple writings on top of each
other on a single page, or availability of only parts of a printed page as a result of torn or damaged paper.
Faded characters and multiple writings can be analyzed using multispectral imaging technology, where
the capture process involves not just wide-band red, green, and blue color channels, but a combination
of multiple wavelengths spanning the visible and UV and IR part of the light spectrum [23]. Challenges
whendealingwithmultiplewavelengthsespeciallyoutsidethevisiblespectruminclude,e.g.,registration
of different spectral images [24].
In [6] the authors describe details of a set-up for a multispectral image acquisition system for doc-
ument capture using spectral ﬁlters in the visual and near-infrared section of the light spectrum. The
captured images are then processed to perform a foreground-background separation [25]. In the analysis
of the Archimedes palimpsest multispectral imaging technology was successful in separating ink that
contained a lot of iron which absorbed ultraviolet light [26] from the parchment background.
In all of the research mentioned in this section, the image acquisition is not possible with today’s
portable digital cameras, but requires a stationary set-up in the lab. Digital camera architecture has
to advance in new ways in order to enable portable multispectral imaging acquisition devices (see
Section 4.03.5).
4.03.3.2 Adaptive learning for document analysis algorithms
In the traditional document analysis and recognition community, researchers are still working on prob-
lems related to character recognition techniques for different languages, recognizing of math formulas
or graphics, and development of lower-complexity analysis algorithms to enable faster document scan-
ners for digital library applications. As explained in Section 4.03.2.2, compared to all the research that
has been done in this ﬁeld over the last 20 years, the headroom for innovative research using traditional
methods is pretty small. A paradigm shift is necessary to solve the new problems that arise with the
availability of large data sets, crowd-sourcing, and high-performance mobile platforms. In an interesting
overview chapter on the status of document recognition technologies in 2011 [27], the author calls for a
change in approaching the algorithms design to enable more adaptive learning mechanism. On the one
hand application-speciﬁc machine learning algorithms improve with more accurate models and enough
data to ﬁt the models, but are often expensive to implement. On the other hand, weak models that are
generic, often informal and less precise, are easier to implement, less complex, but might be very useful
when data computing resources are limited. The research question is then how to choose between weak
and strong models.

4.03.4 Looking into the Future of Document and Image Capture
87
Some research has started to address such adaptation through crowd-sourcing, i.e., utilizing people,
e.g., through Amazon Mechanical Turk [28], to make decisions that are hard to do for computers, leaving
pure computational decisions to the machines. The challenge in this framework is then to evaluate the
crowd-sourced data and merge them with the computer generated data [29].
4.03.4 Looking into the future of document and image capture
Whereas document capture had been traditionally performed using a stationary scanner that simply
digitizes a document and routes it for copying or storage, the capture process itself and the purpose of
document and image capture in recent years has changed due to the rise of mobile computing devices
and powerful sensing technologies. In the following we describe details of how these two areas can
improve, enrich, and change the document capture process.
4.03.4.1 Riding the mobile wave—document image capture with digital cameras
Document capture today is often performed by simply taking a picture of a document with a mobile
phone and then emailing it to a speciﬁed destination [9]. Such a document is not necessarily a page-size
piece of paper, but a whiteboard, a presentation, a bar code, or a street sign. Another usage scenario of
mobile image capture is to utilize the captured document or a part of it as a query to retrieve digital
material that is related to the captured content. This application of document and image capture is
called “Visual Search” and is performed on mobile devices utilizing their image sensing capabilities,
processing power, and connectivity to the Internet [30]. Content that can be retrieved with a simple
“point-and-shoot” interaction with a phone needs to be linked to the printed or displayed document in
the ﬁrst place before any additional material can be retrieved. Many research groups and companies have
emphasized work in Visual Search technology over the past years. The article in [30] introduces Visual
Search technology as creating visual bookmarks for a variety of media that are retrieved and viewed on
the mobile device. The article describes algorithms for robust and discriminative local feature extraction
for general photographic scene content. Technology challenges in this area include the extraction of
features from smartphone images of degraded quality, especially from document images [31], matching
of features extracted from parts of a document with features extracted from entire pages, and scalability
issues related to data storage on the phone or some cloud service, as well as retrieval time [32,33].
In [34] the application of Visual Search technology to retrieving video instructions for driver’s license
training by taking a picture of a section of the driver’s license handbook is described (Figure 3.6).
In response to the capture a video of driving instructions is retrieved on the phone.
Visual Search applications also enable smartphones to function as bar code scanners by using the
digital camera on a phone for imaging of a bar code, i.e., capturing, detecting, and recognizing, and then
retrieving the encoded information on the phone (e.g., [35]). Such applications have become attractive
through the capabilities of capturing printed material everywhere and anywhere with the reasonable
camera quality as well as powerful sensing/processing capabilities, and connectivity of mobile phones.
The technology challenges when going away from capture and recognition of two-dimensional
documents to arbitrary objects in a three-dimensional space expands the image capture and analysis
area to the ﬁeld of computer vision where parameters such as lighting conditions, camera viewpoints,
and tracking of objects under hand movements play an important role. The interplay between lightning

88
CHAPTER 3 Image and Document Capture—State-of-the-Art
FIGURE 3.6
Example of a Visual Search application: Driver’s license training with videos obtained via point-and-shoot
interaction with a printed driver’s license handbook [34].
conditions and camera viewpoints lead to shadows and under- or over-exposure of document images
taken with a hand-held camera. As a consequence, binarization algorithms may not work as desired.
Additionalsensorsonamobilephone,suchasanambientlightsensor,however,couldassistinmeasuring
illumination conditions. Other sensor conﬁgurations such as stereo cameras on a mobile phone could
assist layout analysis processing by determining the depth of document in a 3D scene [36], or could
even detect text rendered in 3D fonts.
4.03.4.2 Other mobile phone applications
Other mobile phone applications that take over tasks conventionally performed by ofﬁce scanners or
printers are scanning individual pages [37], attaching a W-2 form to the tax return [38], or displaying the
boarding pass for airplane travel (“A Boarding Pass on Your Screen,” NY Times, January 2011) [39].
4.03.5 Data capture via novel sensor multiplexing techniques
Another emerging trend in image capture is that of using alternative sensing architectures to capture
information of the real world. This trend is based on two technology developments, the increase of
photodetector capabilities, such as the increase in pixel size of CCD and CMOS image sensors, and

4.03.5 Data Capture via Novel Sensor Multiplexing Techniques
89
the advances in estimation and processing of high-dimensional data. The increased detector capabilities
allow in combination with modiﬁed optical architectures a multiplexing of high-dimensional scene
information on the detector. Examples of such multiplexing architectures are plenoptic and coded
aperture camera architectures [40–42], cameras with special optics designed to enhance removal of
motion blur [43], or the so-called single-pixel camera [44]. These designs produce a superposition
of spatial, directional, spectral, or temporal scene information on the detector array that is integrated
into sensor measurements. With detailed knowledge of object characteristics and the optical path, the
desired information can be reconstructed using appropriate de-multiplexing techniques, e.g., [45–47].
The optical path of these architectures is altered to include micro-optics, special lens elements, or masks
to alter the response function of the optical system. On the signal processing side, de-multiplexing of
sensor data ﬁts the framework of inversion of ill-conditioned systems. In this area of research, theory
and algorithms have advanced tremendously over the past decade, and have found their way into image
reconstruction techniques, e.g., via applications of the Bayesian framework [48] for depth estimation
and resolution enhancement or multi-modal compressive sensing under sparsity assumption [49]. As a
result, we see portable cameras that can capture multispectral, depth, and dynamic range information
of an image in a single data acquisition step, overlaying spectral measurements at various depth planes
on the sensor, requiring speciﬁc de-multiplexing techniques on the digital processor [42,50]. Such
cameras can enable portable multi-modal document and image capture. On the other hand, we can
envision mobile phones with bar code speciﬁc lenses and processing that enable an extended depth of
ﬁeld image capture, allowing accurate capture of gray scale objects and bar codes over a variety of
distances from the mobile phone [51,52]. These novel sensing paradigms provide powerful insights
into how image and document capture might turn into information sensing in the future.
An overview of tasks needed to be performed for document capture in the future assuming hand-held
multi-modal capture devices is described in Figure 3.7.
FIGURE 3.7
Overview of a expansion of imaging pipeline for a document capture system in the future including processing
components to support future hand-held multi-modal imaging applications.

90
CHAPTER 3 Image and Document Capture—State-of-the-Art
4.03.6 Data sets and open source code
Since conventional document capture via stationary scanners have been around for years, there are
commercial products that are often used to analyze and process the captured digital data. More recently
there have been attempts to provide open source software for various steps necessary for document
analysis. One of such technologies is OCRopus [20] which features pluggable layout analysis, pluggable
character recognition, statistical natural language modeling, and multi-lingual capabilities.
A rich collection of lists of various databases containing document images obtained conventional
static scanning technology and ground truth data that have been commonly used in document analysis
research is described [53]. This reference also includes links to competitions that are run at conferences
and workshops on the document analysis and recognition ﬁeld. A data set of document images captured
with a digital camera is introduced in [54] and can be downloaded at [55].
For testing Visual Search technologies, a free data set containing images of CDs, books, outdoor
landmarks, business cards, text documents, museum paintings, and video clips is available at [56] and
described in [57].
Developments are also under way to provide an open source platform for evaluation of document
analysis algorithms and testing procedures at [58].
For compression of bilevel document compression, code for JBIG2 encoding and decoding is avail-
able at [59,60].
The theory and algorithms for de-multiplexing of multiplexed sensor data are still at a core research
phase and are tightly coupled to the hardware for image capture [50,61]. Most of these hardware plat-
forms are not yet available to the general public to perform research on document-speciﬁc applications.
The image reconstruction techniques often require detailed knowledge of the optical path of the system
and sensor speciﬁcs. Common to most of those is that the solution is the output of an L1 optimization
solver that produces solutions that are sparse in some high-dimensional space. An introduction to these
solvers, possible applications and Matlab code can be found in [62].
4.03.7 Conclusions and future trends
Document and image capture has been researched for decades. The conventional capture process via
stationary scanning using traditional 2D image processing has reached a performance plateau and pro-
vides little headroom for innovative research. Scanners, copiers, and printers have become commodity
products where the emphasis is on lowering cost and not improving quality, which is good enough for
most applications. This perception changes when going away from that conventional imaging toward
information sensing. This shift opens up a variety of directions for promising research. One direction is
tightly coupled to mobile devices, their integrated sensing capability, processing power, and connectiv-
ity, and their availability everywhere at any time. Point-and-shoot applications utilizing Visual Search
technologies augmented with various layers of meta-data as in [21,32] will be ubiquitous in a few years.
The image or document capture process is just the entry into a world of rich user experience enabled
trough huge amount of information being available at our ﬁngertips. Challenges in this evolution are
how to process and extract information valuable to the user out of a very large data set (e.g., through
adaptive machine learning technologies), and how to obtain a good user experience (e.g., privacy and

References
91
security concerns need to be considered when capturing information locally, but processing or storing
it in a cloud).
One can only speculate what a “document” might be in the future in addition to what it has been in
the past, namely piece of paper with text printed on it. It might be a slide show given at a presentation,
a video, or a bit text on a street sign in an unfamiliar city that the user wants to capture and process.
Text or, more general, formatted content might become part of a 3D scene that users interact with via
gestures. It might be captured through interaction with a 3D scene and embedded in an augmented or
virtual reality application.
Another promising research direction is the development of modern sensing frameworks, consisting
of hardware architectures and signal processing solutions. Image sensors have become very powerful in
terms of pixel count. Now it is required to use the pixels in novel ways to capture real world information,
e.g., through ﬂexible architectures that can capture different modalities of light [42,63–66]. Once such
high-dimensional data are available, the signal processing ﬁeld has to embrace the challenge of how to
process these data efﬁciently in software and hardware to provide end-to-end solutions for portable and
ﬂexible imaging devices.
References
[1] Jian Liang, David Doermann, Huiping Li, Camera-based analysis of text and documents: a survey, Int. J. Doc.
Anal. Recogn. 7 (2005) 84–104.
[2] Roy S. Berns, Billmeyer and Saltzman’s Principles of Color Technology, Wiley-Interscience, 2000.
[3] Gaurav Sharma, Raja Bala, Digital Color Imaging, CRC Press, 2002.
[4] Edward J. Giorgianni, Thomas E. Madden, Michael A. Kriss, Digital Color Management: Encoding Solutions,
Wiley, 2009.
[5] R.L. Heckaman, M.D. Fairchild, Expanding display color gamut beyond the spectrum locus, Color Res. Appl.
(2006) 475–482.
[6] Martin Lettner, Robert Sablatnig, Multispectral imaging for analyzing ancient manuscripts, in: Proceedings
of 17th European Signal Processing Conference, 2009, pp. 1200–1204.
[7] Gerald C. Holst, CCD Arrays, Cameras, and Displays, SPIE Press, 1998.
[8] S. Shrinivasan, N. Balram, Adaptive contrast enhancement for digital video, SID Symp Digest Tech. Papers
42 (1) (2007) 440–443.
[9] Michael Gormish, Berna Erol, Daniel G. Van Olst, Tim Li, Andrea Mariotti, Whiteboard sharing: capture,
process, and print or email, in: Proceedings of SPIE Electronic Imaging 2011, vol. 7879, 2011, p. 78790D.
[10] The Digital Archimedes Palimpsest. <http://archimedespalimpsest.net/Data>.
[11] Hirobumi Nishida, Takeshi Suzuki, Correcting show-through effects on scanned color document images by
multiscale analysis, Pattern Recogn. 36 (12) (2003) 2835–2847.
[12] Kathrin Berkner, Enhancement of scanned documents in Besov spaces with applications to image enhancement
using wavelet domain representations, Proc. SPIE 4670 (2002) 143–154.
[13] Christian Wolf, Document ink bleed-through removal with two hidden Markov random ﬁelds and a single
observation ﬁeld, IEEE Trans. Pattern Anal. Mach. Intell. 32 (3) (2010) 431–447.
[14] Gaurav Sharma, Show-through cancelation in scans of duplex printed documents, IEEE Trans. Image Process.
10 (5) (2001) 736–754.
[15] Reviel Netz, William Noel, The Archimedes Codex: How a Medieval Prayer Book is Revealing the True
Genius of Antiquity’s Greatest Scientist, Da Capo Press, 2007. <http://www.archimedespalimpsest.org>.

92
CHAPTER 3 Image and Document Capture—State-of-the-Art
[16] W.B. Pennebaker, J.L. Mitchell, JPEG: Still Image Data Compression Standard, Springer, 1992.
[17] David S. Taubman, Michael W. Marcellin, JPEG2000: Image Compression Fundamental, Standards and
Practice, Springer, 2002.
[18] Fumitaka Ono, William Rucklidge, Ronald Arps, Corneliu Constantinescu, JBIG2—the ultimate bi-level
image coding standard, in: Proceedings of International Conference on Image Processing (ICIP), vol. 1, 2000,
pp. 140–143.
[19] A. Mavlankar, P. Agrawal, D. Pang, S. Halawa, N.-M. Cheung, B. Girod, An interactive region-of-interest
video streaming system for online lecture viewing, in: Proceedings of International Packet Video Workshop
(PV2010), December 2010, Hong Kong, China, 2010. <http://classx.stanford.edu>.
[20] OCRopus. <http://code.google.com/p/ocropus/>.
[21] Layar. <http://www.layar.com/>.
[22] Rick van Krevelen, Ronald Poelman, A survey of augmented reality technologies, applications and limitations,
Int. J. Virt. Real. 9 (2) (2010) 1–20.
[23] S.J. Kim, S. Zhou, F. Deng, C.-W. Fu, M.S. Brown, Interactive visualization of hyperspectral images of
historical documents, in: Proceedings of IEEE Visualization Conference, vol. 16, 2010.
[24] M. Lettner, M. Diem, R. Sablatnig, P. Kammerer, H. Miklas, Registration of multi-spectral manuscript images
as prerequisite for computer aided script description, in: M. Grabner, H. Grabner (Eds.), Proceedings of the
12th Computer Vision Winter Workshop, 2007, pp. 51–58.
[25] Florian Kleber, Martin Lettner, Markus Diem, Maria Vill, Robert Sablatnig, Heinz Miklas, Melanie Gau,
Multispectral acquisition and analysis of ancient documents, in: M. Ioannides, A. Addison, A. Georgopoulos,
L. Kalisperis (Eds.), Proceedings of the 14th International Conference on Virtual Systems and MultiMedia
(VSMM 2008), Dedicated to Cultural Heritage, Limassol, Cyprus, 2008, pp. 184–191.
[26] R.L. Easton, K.T. Knox, W.A. Christens-Barry, Multispectral imaging of the Archimedes palimpsest, in: 32nd
Applied Image Pattern Recognition Workshop (AIPR), Washington, DC, 2003, pp. 111–118.
[27] Henry Baird, Document recognition without strong models, in: Proceedings of International Conference on
Documents Analysis and Recognition, 2011, pp. 414–423.
[28] Amazon Mechanical Turk. <https://www.mturk.com/>.
[29] J. Chazalon, B. Couasnon, A. Lemaitre, Iterative analysis of pages in document collections for efﬁcient user
interaction, in: Proceedings of International Conference on Document Analysis and Recognition (ICDAR),
2011, pp. 503–507.
[30] Bernd Girod et al., Mobile visual search, IEEE Signal Process. Mag. 28 (4) (2011) 61–76.
[31] J.J. Hull, B. Erol, J. Graham, Q. Ke, H. Kishi, J. Moraleda, D.G. Van Olst, Paper-based augmented reality,
in: International Conference on Artiﬁcial Reality and Telexistence, 2007, pp. 205–209.
[32] Xu Liu, Jonathan J. Hull, Jamey Graham, Jorge Moraleda, Timothee Bailloeul, Mobile visual search, linking
printed documents to digital media, in: Proceedings of IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2010.
[33] Berna Erol, Jamey Graham, Jonathan J. Hull, Daniel G. Van Olst, Mobile web browsing initiated by
visual search, in: Proceedings of IEEE International Conference on Multimedia and Expo (ICME), 2009,
pp. 1843–1844.
[34] Jamey Graham, Jorge Moraleda, Jonathan J. Hull, Timothee Bailloeul, Xu Lui, Andrea Mariotti, Visual search
applications for connecting published works to digital material, in: Proceedings of the International Conference
on Multimedia, MM’10, ACM, New York, NY, USA, 2010, pp. 1525–1526.
[35] Jamey Graham, Jonathan J. Hull, Icandy: a tangible user interface for itunes, in: CHI’08 Extended Abstracts
on Human Factors in Computing Systems, CHI EA’08, ACM, New York, NY, USA, 2008, pp. 2343–2348.
[36] M.Z. Afzal, M. Kraemer, S.S. Bukhari, F. Shafait, T.M. Breuel, Improvements to uncalibrated feature-based
stereo matching for document images by using text-line segmentation, in: Proceedings of International Work-
shop on Document Analysis Systems, 2012.

References
93
[37] Ricoh Innovations. <http://www.ricohinnovations.com/scanpages-iphone>.
[38] Verne G. Kopytoff, New Task for Phone: File Taxes, New York Times, January 14, 2011.
[39] Susan Stellin, A Boarding Pass on Your Screen, New York Times, November 2, 2011.
[40] Ren Ng, Fourier slice photography, in: ACM SIGGRAPH 2005 Papers, SIGGRAPH’05, ACM, New York,
NY, USA, 2005, pp. 735–744.
[41] A. Levin et al., Image and depth from a conventional camera with a coded aperture, ACM Trans. Graphics 3
(2007).
[42] Roarke Horstmeyer et al., Flexible multimodal camera using a ﬁeld architecture, in: IEEE Conference
Computational Photography, 2009.
[43] Ramesh Raskar, Amit Agrawal, Jack Tumblin, Coded exposure photography: motion deblurring using ﬂuttered
shutter, in: ACM SIGGRAPH 2006 Papers, SIGGRAPH’06, ACM, New York, NY, USA, 2006, pp. 795–804.
[44] Marco Duarte et al., Single-pixel imaging via compressive sampling, IEEE Signal Process. Mag. 25 (3) (2008)
83–91.
[45] C. Zhou, S. Nayar, What are good apertures for defocus deblurring? in: IEEE, Proceedings of International
Conference on Computational Photography, 2009.
[46] T.E. Bishop, S. Zanetti, P. Favaro, Light ﬁeld superresolution, in: Proceedings of International Conference of
Computational Photography, vol. 1, 2009.
[47] Marco F. Duarte, Mark A. Davenport, Dharmpal Takhar, Jason N. Laska, Ting Sun, Kevin F. Kelly, Richard G.
Baraniuk, Single-pixel imaging via compressive sampling, IEEE Signal Process. Mag. 25 (2) (2008) 83–91.
[48] Anat Levin, William Freeman, Fredo Durand, Understanding camera trade-offs through a Bayesian analysis
of light ﬁeld projections, in: David Forsyth, Philip Torr, Andrew Zisserman (Eds.), Computer Vision ECCV
2008, Lecture Notes in Computer Science, vol. 5305, Springer, Berlin, Heidelberg, 2008, pp. 88–101.
[49] M.F. Duarte, R.G. Baraniuk, Kronecker compressive sensing, IEEE Trans. Image Process. 21 (2) (2012)
494–504.
[50] Wagadarika et al., Single disperser design for coded aperture snapshot spectral imaging, Appl. Opt. 47 (10)
(2008) B44–B51.
[51] M. Dirk Robinson, David Stork, Joint digital-optical design of imaging systems for grayscale objects,
in: Proceedings of the SPIE European Optical Design Conference, September 2008.
[52] C.-L. Tisse, H.P. Nguyen, R. Tessires, M. Pyanet, F. Guichard, Joint digital-optical design of imaging systems
for grayscale objects, in: Proceedings of the SPIE Novel Optical Systems Design and Optimization XI,
vol. 7061, 2008.
[53] Simone Marinai, Introduction to document analysis and recognition, in: Simone Marinai, Hiromichi Fujisawa
(Eds.), Machine Learning in Document Analysis and Recognition, Studies in Computational Intelligence,
vol. 90, Springer, 2008, pp. 1–20.
[54] S.S. Bukhari, F. Shafait, T. Breuel, The IUPR dataset of camera-captured document images, in: Proceedings
of Fourth International Workshop on Camera-Based Document Analysis and Recognition, CBDAR11, 2011.
[55] IUPRdataset. <http://www.sites.google.com/a/iupr.com/bukhari/>.
[56] Stanford Mobile Visual Search Data Set. <http://www.stanford.edu/∼dmchen/mvs.html>.
[57] Vijay R. Chandrasekhar, David M. Chen, Sam S. Tsai, Ngai-Man Cheung, Huizhong Chen, Gabriel Takacs,
Yuriy Reznik, Ramakrishna Vedantham, Radek Grzeszczuk, Jeff Bach, Bernd Girod, The Stanford mobile
visual search data set, in: Proceedings of the Second Annual ACM Conference on Multimedia Systems,
MMSys’11, ACM, New York, NY, USA, 2011, pp. 117–122.
[58] DAE. <http://dae.cse.lehigh.edu/DAE/>.
[59] JBIG2 encoder. <https://github.com/agl/jbig2enc/>.
[60] JBIG2 decoder. <http://jbig2dec.sourceforge.net/>.
[61] Ren Ng, Fourier slice photography, in: ACM SIGGRAPH 2005 Papers, SIGGRAPH’05, ACM, New York,
NY, USA, 2005, pp. 735–744.

94
CHAPTER 3 Image and Document Capture—State-of-the-Art
[62] L1-magic. <http://users.ece.gatech.edu/justin/l1magic/>.
[63] Ren Ng et al., Light ﬁeld photography with a hand-held plenoptic camera, Technical Report CSTR 2005-02,
Stanford University, April 2005.
[64] Rui Shogenji et al., Multispectral imaging using compound optics, Opt. Express 12 (8) (2004) 1643–1655.
[65] C. Birklbauer, S. Opelt, O. Bimber, Rendering Gigaray Light Fields, Eurographics 32 (2) (2013).
[66] R. Robucci, J.D Gray, Leung Kin Chiu, J. Romberg, P. Hasler, Compressive Sensing on a CMOS Separable-
Transform Image Sensor, in: Proc. IEEE 98 (6) (2010) 1089–1101.

4
CHAPTER
Image Display—Mobile Imaging
and Interactive Image Processing
Oscar C. Au* and Lu Fang†
*Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology,
Clear Water Bay, Hong Kong
†Department of Electronic Engineering and Information Science, University of Science and Technology of China, China
4.04.1 The small screen challenge in mobile imaging
In many mobile applications such as smart phones, portable multimedia players (PMP), digital SLR
(DSLR) cameras, and in-car video entertainment centers, image and video need to be displayed and
are often viewed at very short distance. Many of these devices are capable of capturing high resolu-
tion images (e.g., 10 mega-pixel in DSLR) or even video. The limited battery power supply in the
mobile devices often prevent these mobile systems to use high-power large LCD displays. Instead, the
mobile devices often have a LCD screen with small physical size (e.g., 3 cm × 2 cm for smartphone or
DSLR) and with much lower pixel resolution (e.g., 0.15 mega-pixel for 480 × 320 display) than actual
image/video resolution. Thus the high resolution image and video are down-sampled before being
displayed. Unfortunately, the anti-aliasing ﬁlter often leads to rather severe blurring. While the blurring
may be minor when the viewing distance is large, it can be rather disturbing in mobile applications due
to the short viewing distance.
Tocopewiththeblurringproblem, onepossiblesolutionis touseaLCD screenwithhigher resolution.
But such hardware solution tends to be expensive and often not welcomed by the consumer electronic
companies. Another possible solution is to continue to use the low-resolution LCD screen but use some
software technique to enhance the apparent image/video resolution. In this chapter, we discuss a novel
way to improve the apparent resolution of these down-sampled image/video using a technique called
subpixel rendering, which controls something smaller than a pixel in a high-precision manner. Such
subpixel rendering technique can be useful on computer monitors also.
4.04.2 Subpixel-based hardware design in mobile display
A single pixel on a color LCD contains several individual color primaries, typically three colored
elements ordered (on various displays) either as blue, green, and red (BGR), or as red, green, and
blue (RGB). Some displays may have more than three primaries, often called Multi-Primary, such as
the combination of red, green, blue, and yellow (RGBY), or red, green, blue, and white (RGBW),
or even red, green, blue, yellow, and cyan (RGBYC) [1]. These colored primaries, sometimes called
sub-pixels, are fused together to appear as a single color to human due to the blurring by the optics and
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00004-2
© 2014 Elsevier Ltd. All rights reserved.
95

96
CHAPTER 4 Image Display—Mobile Imaging and Interactive Image Processing
(a)
(b)
(c)
(d)
FIGURE 4.1
Rendering of a sloping edge on Apple II display. (a) Pixel-based rendering result, (b) pixel-based rendering
(actual color pattern), (c) subpixel rendering (actual color pattern), and (d) subpixel rendering (conceptual)
result. (For interpretation of the references to color in this ﬁgure legend, the reader is referred to the web
version of this book.)
spatial integration by nerve cells in the human eyes [2,3]. Methods that take the interaction between
display technology and human visual system into account are called subpixel rendering algorithms
[4–6]. Subpixel rendering technology is well-suited to Liquid Crystal Display (LCD) where each (log-
ical) pixel corresponds directly to three or more independent colored sub-pixels, but less so for CRTs.
In a CRT the light from the pixel components often spreads across pixels, and the outputs of adjacent
pixels are not perfectly independent.
About 20 years ago, the Apple II personal computer introduced a proprietary high resolution LCD
graphics display in which each pixel has two vertical stripe subpixels with green and magenta colors
respectively. The system controlled the individual subpixels in such a way that it could display fonts with
better details than pixel-based rendering displays. Without subpixel technology, a diagonal white line on
Apple II display could only be drawn using “whole” white pixels composed of paired green and purple
subpixels, as shown in Figure 4.1a [7]. Thanks to Apple’s built-in subpixel technology, white pixels are
often composed of adjacent subpixels to yield a much smoother result, as shown in Figure 4.1d.
Similar situation exists for today’s subpixel rendering technology with modern-day RGB vertical
stripe LCD panels. Figure 4.2 shows a common problem when a sloping edge is displayed by pixel
rendering, and how it can be suppressed by subpixel rendering. Simple pixel-based rendering causes
sawtooth in the sloping edge in Figure 4.2a. Thanks to the fact that a pixel is composed of three
separable subpixels, we can “borrow” subpixels from adjacent whole pixels. Figure 4.2b depicts that
using subpixel rendering, the apparent position of the sloping edge is micro-shifted by a one or two
subpixel width, giving a much smoother result compared to Figure 4.2a. However, subpixel rendering
may cause local color imbalance called “color fringing artifact” [3,8,9], because, for some pixels, only
one or two subpixels are turned on/off, as shown in Figure 4.2c.
Several companies consider the problem of the need for 2-D rectangular subpixel geometries, mostly
from a manufacturing point of view [10–12]. The components of the pixels (primary colors: red, green,
and blue) in an image sensor or display can be ordered in different patterns or pixel geometry. The

4.04.2 Subpixel-Based Hardware Design in Mobile Display
97
(a)
(b)
(c)
FIGURE 4.2
Rendering of a sloping edge on RGB vertical stripe display. (a) Pixel-based rendering, (b) subpixel rendering
(conceptual) result, and (c) subpixel rendering (actual color pattern). (For interpretation of the references to
color in this ﬁgure legend, the reader is referred to the web version of this book.)
geometrical arrangement of the primary colors within a pixel can be varied depending on usage. In com-
puter monitors such as LCDs that are mostly used to display edges or rectangles, the companies would
typically arranged the subpixel components in vertical stripes. However, in displays for motion pictures,
companies would tend to arrange the components to have delta (or triangular) or other 2-D patterns so
that the image variation is perceived better by the viewer.
In 2000, Clairvoyante developed the “PenTile” matrix as a new approach to build and drive color ﬂat
panel displays [10]. The PenTile design takes advantage of the way the human eye and brain process
visual information and optimizes the pixel layout to match this process more effectively than does the
traditional RGB stripe. Various subpixel layouts have been proposed by Clairvoyante/Nouvoyance (and
demonstrated by Samsung) as members of the PenTile matrix family of layouts speciﬁcally designed for
subpixel rendering efﬁciency [10]. Illustrated in Figure 4.3 are a conventional RGB vertical stripe sub-
pixelarrangement(Figure 4.3a)and higher-efﬁciencyPenTileRGBTM (Figure4.3b),PenTileRGBWTM
(Figure 4.3c) subpixel arrangements.
PenTile RGBG layout uses green pixels interleaved with alternating red and blue pixels. The human
eye is most sensitive to green, especially for high resolution luminance information. Thus the RGBG
scheme creates a color display with one-third fewer subpixels than a traditional RGB-RGB scheme but
with the same measured luminance display resolution. This is similar to the Bayer ﬁlter commonly used
in digital cameras. Using fewer subpixels, the PenTile RGB pattern on the right renders information
at the same resolution as a conventional RGB vertical stripe pattern. The PenTile RGB pattern offers
improvements in cost performance and power efﬁciency compared to conventional RGB stripe displays,
due to the combined effect of increased aperture ratio in LCD devices or decreased current density in

98
CHAPTER 4 Image Display—Mobile Imaging and Interactive Image Processing
(a)
(b)
(c)
(d)
(e)
(f)
FIGURE 4.3
Pixel geometry of (a) RGB vertical stripe display with RGB rectangular subpixel arrangement, (b) RGB delta
display whose even rows are shifted to right with half subpixel width, (c) PenTile RGB subpixel arrangement
utilizing 33% fewer subpixels, (d) PenTile RGBW subpixel arrangement utilizing 33% fewer subpixels, (e)
VPX (with 3 subpixel/pixel) display whose even rows are shifted to right with one subpixel width, (f) VPW
(with four subpixels/pixel, red, green, blue, white). (For interpretation of the references to color in this ﬁgure
legend, the reader is referred to the web version of this book.)
OLED devices. The Google/HTC Nexus One Android phone uses an AMOLED display by Samsung
that uses PenTile RGBG technology as does the Samsung S8000. The Samsung i9000 Galaxy S and
Samsung Wave S8500 series phones use a Super AMOLED PenTile RGBG panel. The Samsung NX10
camera also uses the PenTile AMOLED display for the rear screen. The newly released Galaxy Nexus
phone also carries a PenTile RGBG Super Amoled 720p HD display [13].
In RGBW layout, one pixel contains two subpixels only and every two consecutive pixels would
have these four subpixels: red, green, blue, white. And for any two consecutive rows, the color pattern
of the second row is shifted to the right by 1 pixel location. Thus, all the subpixels in PenTile appear
to have delta conﬁguration, which should be good for displaying edges in many orientations. Displays
made using the PenTile RGBWTM pattern offer improvements in cost performance and power efﬁciency
compared to conventional RGB Stripe displays, due to the combined effect of increased aperture ratio
and improved light transmission through the white (clear) subpixel. Note that Motorola Atrix 4G phone
uses PenTile RGBWTM pixel geometry display.

4.04.3 Subpixel-Based Software Design in Mobile Display
99
VP (Visual Perception) Dynamics is another company working on displays with special dedicated
subpixel rendering technologies. They have two major products: VPX and VPW [11,12]. In their VPX
LCD panel, they modify the regular RGB stripe pixel geometry by shifting every other line to the right by
one subpixel location, as shown in Figure 4.3e, making it similar to the delta conﬁguration (Figure 4.3d).
With this modiﬁcation, the VPX LCD panel combined with a subpixel rendering driver can achieve three
times (3×) higher horizontal resolution than the regular RGB stripe LCD panel. As they only change the
arrangement of the color ﬁlter for the subpixels, the VPX panel can be manufactured with essentially the
same process as regular LCD. In their VPW panel, they modiﬁed the LCD panel such that a regular RGB
stripe pixel with three subpixels (RGB) is replaced by a VPW pixel with 4 square-shaped subpixels cor-
responding to red, green, blue and white color (RGBW), as shown in Figure 4.3f. The main advantages
of VPW (RGBW) technology is four times (4×) higher resolution (2× horizontal resolution and
2× vertical resolution) and lower power consumption. As the shapes of the VPW subpixels are different
from the regular RGB stripe LCD, VPW manufacturing probably requires more modiﬁcation than VPX.
4.04.3 Subpixel-based software design in mobile display:
font rendering
Subpixel rendering techniques originate from the problem of monochromatic font rendering on LCDs.
Previously, simple pixel-based font display was used and the smallest level of details that a computer
could display on an LCD was a single pixel. However, researchers found that, by controlling the subpixel
values of neighboring pixels, the number of points that may be independently addressed to reconstruct
the image is increased, and it is possible to micro-shift the apparent position or orientation of a line
(such as the edge of a font), by one or two subpixel width, to achieve higher edge sharpness [4,5].
In 1998, Microsoft announced a subpixel-based font display technology called “ClearType” [2]. Note
that Microsoft ClearType is software-only subpixel technique capable of improving the readability of
text on regular LCD with three vertical stripe subpixels (red, green, and blue), which requires no change
of display hardware. With ClearType running on an LCD monitor, features of text as small as a fraction
of a pixel in width can be displayed. Figure 4.4 illustrates an example of displaying the letter “m” with
traditional pixel rendering and ClearType [2]. It is obvious that ClearType can reduce staircase artifacts
effectively and reconstruct the shape information more faithfully. Microsoft ClearType is especially
suitable when rendering relatively small-size font, and the width of consecutive font size probably
differs by subpixel only.
Recall that subpixel rendering may cause local color imbalance (color fringing artifact). Microsoft
ClearType suppresses such color artifact using “energy sharing,” where each subpixel’s “energy” spreads
acrossitanditstwoneighboringsubpixelsbyturningonsuchasubpixelanditstwoimmediatelyadjacent
neighbors each with 1/3 intensity. Hence, the energy of a single subpixel is shared with its two neighbors
instead of putting all the energy entirely within it [4,5]. Such energy sharing always turns on a set of
R-G-B (or G-B-R or B-R-G) sub-pixels by the same amount.
One negative side effect of this “energy sharing” is blurring artifact, which is caused by the neigh-
boring subpixels having a little too much energy compared with the primary center subpixel. Gibson [3]
propose to simply repeating the ﬁltering process again by having each of the three ﬁrst-stage recipient
subpixels share the energy with their three neighbors. Since two sets of division by three are performed,
the resulting intensity has the energy distribution that equals to spread the original subpixel’s energy out

100
CHAPTER 4 Image Display—Mobile Imaging and Interactive Image Processing
(a)
(b)
(c)
FIGURE 4.4
(a) Letter “m” in italic (left), (b) whole-pixel rendered “m” with jagged edges (middle), and (c) subpixel
rendered “m” with smooth edges (right).
across the closest ﬁve subpixels using a ﬁve element inter-color low-pass ﬁlter with [1/9, 2/9, 3/9, 2/9,
1/9] coefﬁcients [3]. The ﬁve coefﬁcients of low-pass ﬁlter sum to 1, indicating that the total energy of
the original center subpixel is fully represented by the spread of ﬁve subpixels. Due to relatively higher
value of the center coefﬁcient than neighbors, the majority of the energy is kept in the center of the spread.
ClearType’s inter-color three-tap ﬁlter or Gibson’s inter-color ﬁve-tap ﬁlter relieve color fringing
artifact by spreading the energy of one subpixel to three or ﬁve subpixels, thus the R, G, B values within
any pixel tend to be very similar, if not identical, making the resulting image appear monochrome.
Figure 4.5 shows the font image processed by pixel rendering and subpixel rendering, where subpixel
rendering achieves higher apparent resolution especially for small fonts by the expense of color fring-
ing artifacts. As veriﬁed in Figure 4.5c, the Gibson’s subpixel rendering method is more effective in
suppressing color artifacts.
4.04.4 Subpixel-based software design in mobile display:
color image down-sampling
Another major application of subpixel rendering for mobile imaging processing is displaying high-
resolution images on low-resolution mobile display terminals, in which case, subpixel-based down-
sampling that make use of the RGB striped or other dedicated subpixel arranged mobile display can
be used to achieve superior sharpness of the mobile images. For example, while digital pictures are
usually captured at very high resolution (e.g., 10 mega-pixels), many of them would be displayed
on small LCD screens on mobile phones or PDAs, which have considerably lower resolutions (e.g.,
0.8 mega-pixel on SVGA, or 0.2 mega-pixel on some smart phones). To view high-resolution image
on low-resolution mobile device, a down-sampling procedure is required. As one pixel in the vertical
stripe LCD contains three subpixels, there exists natural down-sampling pattern for 3:1 down-sampling.

4.04.4 Subpixel-Based Software Design in Mobile Display
101
(a)
(b)
(c)
FIGURE 4.5
(a) Pixel rendering, (b) microsoft ClearType subpixel rendering, and (c) Gibson’s subpixel rendering.
For simplicity, we assume that an input high resolution image L (meaning large) of size 3M × 3N is to
be down-sampled to a low resolution image S (meaning small) of size M × N, and to be displayed on
a M × N device. (Note that if L is not of size 3M × 3N, i.e., the down-sampling ratio is not 3, we can
use regular interpolation or decimation methods to resize L to be 3M × 3N.)
4.04.4.1 Spatial-domain algorithm Design of Subpixel-based Down-sampling
4.04.4.1.1
Direct Pixel-based Down-sampling (DPD)
A simple way, called Direct Pixel-based Down-sampling (DPD) in this paper, is to perform simple
down-sampling by selecting one out of every N pixels. (In this paper, the term Direct means no anti-
aliasing ﬁlter is applied.) It can incur severe aliasing artifacts in regions with high spatial frequency
(such as staircase artifacts and broken lines as shown in Figure 4.6b). An improved scheme is called
Pixel-based Down-sampling with Anti-aliasing Filter (PDAF) in which an anti-aliasing ﬁlter is applied
before Direct Pixel-based Down-sampling. It suppresses aliasing artifacts at the price of blurring the
image, as only the low frequency information can be retained in the process [14]. Note that both DPD
and PDAF are pixel-based methods and do not incur color artifacts.

102
CHAPTER 4 Image Display—Mobile Imaging and Interactive Image Processing
(a)
(b)
(c)
(d)
FIGURE 4.6
(a) Direct Pixel-based Down-sampling (DPD), (b) magniﬁed result of DPD, where “grass” is broken due to
aliasing artifacts, (c) Direct Subpixel-based Down-sampling (DSD), and (d) magniﬁed result of DSD, where
“grass” is smooth but has color fringing artifacts.
4.04.4.1.2
Direct subpixel-based down-sampling (DSD)
Since the number of individual reconstruction points in LCD can be increased by three times by consid-
ering subpixels, application of subpixel rendering in down-sampling schemes may lead to improvement
in apparent resolution [15]. Higher apparent resolution is always attractive to consumers because, for
a given physical size of display, higher resolution can give much more detail making the image more

4.04.4 Subpixel-Based Software Design in Mobile Display
103
realistic. For example, for two displays with same physical size of 4 in. × 3 in., a 704 × 576 display
would look much better with much more details than a 352 × 288 resolution display. This is also the
reason why HDTV looks nicer than SDTV, on displays with the same physical size.
Daly et al. propose a simple subpixel-based down-sampling pattern, which we call Direct Subpixel-
based Down-sampling (DSD). DSD decimates the red, green, and blue components alternately in hor-
izontal direction [8,16,17]. Let (ri, j, gi, j, bi, j) be the (i, j)th pixel of S. DSD copies red, green,
and blue components (i.e., the three subpixels) of the pixel from three different pixels in L, such that
ri, j = R3i−2,3 j−2, gi, j = G3i−2,3 j−1, bi, j = B3i−2,3 j as shown in Figure 4.6c, where R3i−2,3 j−2 is the
red component of the (3i −2, 3 j −2)th pixel of L and so on. It is clear that DSD considers only the
horizontal direction, but not the vertical.
Figure4.6depictstheresultantimagesoftwodown-samplingpatterns:DPDandDSD.Itisinteresting
to see that DSD can potentially preserve more details than DPD thanks to the increase in the number of
individual reconstruction points. A close examination of Figure 4.6d reveals that DSD ﬁlls in the gaps
of the grass, making the grass continuous and sharp at the expense of annoying color artifacts.
4.04.4.1.3
Diagonal Direct Subpixel-based Down-sampling (DDSD)
In [18], Fang and Au observe that the improvement of apparent resolution in DSD tends to happen at
regions with vertical edges or edges with vertical component. There is typically no improvement at
smooth regions or regions with horizontal edges, due to the fact that in DSD the sampling pattern is
merely in horizontal way, which is parallel to horizontal edges. To achieve improved resolution in both
horizontal and vertical directions, they proposed a Diagonal Direct Subpixel-based Down-sampling
(DDSD) pattern, changing the sampling direction from horizontal to diagonal. They divide original
image L into 3 × 3 blocks so that there are M × N blocks, one for each pixel in the down-sampled
low resolution image S, such that the (i, j)th block in L corresponds to the (i, j)th pixel in S. For the
(i, j)th pixel in S, DDSD copies the red, green, and blue components from three different pixels in the
(i, j)th block of L along diagonal direction. (DDSD works also for anti-diagonal direction.) Figure 4.7
shows an example of DDSD
ri, j = R3i−2,3 j−2,
gi, j = G3i−1,3 j−1,
bi, j = B3i,3 j.
(4.1)
To further understand the potential and limitation of various down-sampling schemes (DPD, DSD,
and DDSD), we repeat the experiment in [19] to generate an artiﬁcial large image (L) of size 420×420,
containing four sub-images as shown in Figure 4.8. The four sub-images named as Subimage-V,
Subimage-H, Subimage-AD, and Subimage-D, contain 15 pairs of black and white lines in horizontal,
vertical, diagonal, and anti-diagonal directions, respectively. The width of each black or white line is
7 pixels (with a total of 21 subpixels). In the experiment, L is down-sampled by a factor of 3 with DPD,
DSD, and DDSD to produce three 70 × 70 images, as shown in Figure 4.8b–d respectively.
A subpixel-based regularity measure for each sub-image is given by
μ =
m
k=1 wk
m
−w0
3 ,
σ 2 =
m
k=1 (wk −w0/3)2
m
,
(4.2)
where m is the number of black lines, w0 is the width of black lines in the L image and wk (k = 1, . . . , m)
is the width of the kth black line in DPD, DSD, or DDSD image, and the unit of w0 and wk is subpixel.

104
CHAPTER 4 Image Display—Mobile Imaging and Interactive Image Processing
FIGURE 4.7
Proposed Diagonal Direct Subpixel-based Down-sampling (DDSD) pattern.
(a)
(b)
(c)
(d)
FIGURE 4.8
Artiﬁcial image with four sub-images (a) original L image, (b) result of DPD, (c) result of DSD, and (d) result
of DDSD.
In the experiment, m = 15 and w0 = 21. The mean μ and variance σ 2 of the line width of DPD, DSD,
and DDSD are shown in Table 4.1. To account for color fringing artifacts caused by subpixel-based
down-sampling, a simple color distortion measure for each sub-image is introduced as
RGB =
M,N

i=1, j=1
min

|ci, j −0|, |ci, j −255|

/255,
(4.3)

4.04.4 Subpixel-Based Software Design in Mobile Display
105
Table 4.1 Line Width and Color Distortion of DPD, DSD, and DDSD
Subimage-V
Subimage-H
Subimage-AD
Subimage-D
DPD
DSD
DDSD
DPD
DSD
DDSD
DPD
DSD
DDSD
DPD
DSD
DDSD
μ
0
0
0
0
0
0
0
0
0
0
0
0
σ 2
2
0
0
2
2
0
2
0
0
2
0
2
RGB
0
2
2
0
0
2
0
2
2
0
2
0
where ci, j =
ri, j gi, j bi, j
T , 0 =
0 0 0 T , and 255 =
255 255 255 T . Examining (4.3), the value
of min

|ci, j −0|, |ci, j −255|

would be either 0 or 255. And RGB indicates the frequency (how often)
of color artifacts happen for i = 1, 2, . . . , M and i = 1, 2, . . . , N. Due to 3:1 down-sampling ratio, the
behavior (frequency) of color artifacts is periodic every three black/while lines (21 pixel width) in L. In
other words, the frequency of color artifacts is given by computing RGB for a 7 × 7 (M = 7, N = 7)
block in S, as shown in Table 4.1. According to deﬁnition, RGB = 0 indicates the result is free of color
distortion, while RGB ̸= 0 suggests the result has color artifacts.
As expected, μ = 0 for all methods, suggesting that the average line width is correct for all methods.
For DPD image, σ 2 is non-zero in all the four directions, indicating that the line spacing of DPD is
irregular, as veriﬁed in Figure 4.8b. For DSD image, σ 2 is non-zero in Subimage-H, indicating that
DSD may not manage to keep the horizontal line spacing regular, as veriﬁed in Figure 4.8c, due to the
horizontal decimation of DSD. On the contrary, DDSD manages to keep the line spacing regular for both
horizontal and vertical lines at the expense of color fringing artifacts in Figure 4.8d. Of course, DDSD
has its own limitation too, which cannot keep the line spacing regular for Subimage-D. Fortunately,
diagonal edges tend to occur less frequently than horizontal and vertical edges in real situations and our
human eyes tend to be less sensitive to luminance error across the diagonals [20,21]. In terms of the
color artifacts, RGB(DPD) = 0 for all sub-images, suggesting that DPD is free of color artifacts. Both
DSD and DDSD have non-zero RGB for 3 of the 4 sub-images. For DSD, the three are Subimage-V,
Subimage-D, Subimage-AD. For DDSD, the three are Subimage-H, -V, and -AD. So DSD and DDSD
achieve higher apparent resolution at the expense of color artifacts. As the line width of original image
is 7 pixels which is not a multiple of 3, both DSD and DDSD would sample across the boundary of the
black and white lines in four possible ways: (black, white, white), (white, white, black), (white, black,
black), or (black, black, white), leading to four corresponding colors: cyan (0, 255, 255), yellow (255,
255, 0), red (255, 0, 0), and blue (0, 0, 255), as shown in Figure 4.8c.
From above discussion, exploiting subpixels in down-sampling brings both opportunity as well as
problem. The opportunity is that we can potentially increase the apparent resolution of a patterned
display up to the subpixel resolution. The problem is the associated color distortion. The challenge of
subpixel-based down-sampling is to achieve subpixel resolution (i.e., apparent luminance resolution)
while suppressing color artifacts (i.e., chrominance distortion). Thus some ﬁltering is needed to suppress
the color fringing artifacts without signiﬁcant damage to the improved apparent resolution. In [8,16], an
algorithm based on Human Visual System (HVS) is proposed to suppress visible chrominance aliasing.
Kim and Kim [22] proposed a 1-D reconstruction model to generate virtual pixels from down-sampled

106
CHAPTER 4 Image Display—Mobile Imaging and Interactive Image Processing
image, with which a 1-D MMSE-based ﬁlter is derived to suppress the color fringing artifacts. How-
ever, they only perform horizontal subpixel-based down-sampling without any vertical down-sampling,
resulting in images with incorrect aspect ratios. Thus an additional pixel-based down-sampling in the
vertical direction is further required, which introduces blurring artifacts. In [23], Fang and Au formulate
subpixel-based down-sampling as a directional Min–Max problem and show that the Min–Max solution
can give superior performance over other subpixel-based down-sampling methods in terms of apparent
sharpness.However,thereisstillconsiderableremainingcolorfringingartifacts.Notethatallthesemeth-
ods process subpixel-based down-sampling horizontally. In other words, all these proposed ﬁlters are
designed for conventional horizontal subpixel-based down-sampling (DSD). Researchers typically do
not attempt to apply subpixel-based down-sampling to vertical down-sampling as there is a common con-
ceptionthatlittlecanbegainedintheverticaldirectionduetothehorizontalarrangementofthesubpixels.
In [19], a spatial-domain ﬁlter design for DDSD is investigated. To compare the similarity between the
original high-resolution image and a down-sampled low-resolution image that generated using DDSD,
Fang and Au extend existing 1-D reconstruction method in Kim and Kim [22] to a 2-D model to recon-
struct a virtual large image. Then, they formulate subpixel-based down-sampling as a MMSE problem
between the original large image and virtual large image, and derive the optimal solution called MMSE-
SD (Minimum Mean Square Error for subpixel-based down-sampling). Unfortunately, straight-forward
implementation of MMSE-SD is computational intensive, especially for large images. They further
prove that the solution is equivalent to a 2-D linear ﬁlter followed by DDSD, which is much simpler.
Figure 4.9 compares the results of pixel-based down-sampling with anti-aliasing ﬁlter and subpixel-
based MMSE-SD algorithm. It is obvious that subpixel-based down-sampling achieves higher apparent
resolution than pixel-based method, leading to much sharper down-sampled image.
4.04.4.2 Frequency-domain analysis of subpixel-based down-sampling
Recall that DPD, DSD, and DDSD have obviously different spatial multiplexing of RGB components.
In this subsection, we will introduce a frequency-domain analysis approach to theoretically and sys-
tematically analyze the frequency characteristics of pixel-based (DPD) and subpixel-based (DSD and
DDSD) down-sampling patterns [24].
To mathematically represent the down-sampling pattern of DPD, DSD, and DDSD, an auxiliary
image L′ of size M × N is deﬁned in [24] and (R′
i, j, G′
i, j, B′
i, j) are used to represent the (i, j)th pixel
of L′. Take DSD as example, the red component of L′
DSD is deﬁned such that R′
i, j = Ri, j for i = 3i′ −1
and j = 3 j′ −2 for all possible (i′, j′), and R′
i, j = 0 elsewhere, such that
R′
i, j =
 Ri, j,
for i = 3i′ −1, j = 3 j′ −2, i′ = 1, 2, . . . , m, j′ = 1, 2, . . . , n,
0,
otherwise.
(4.4)
Effectively, out of every 3×3 sub-matrix of L′, one R′ value is copied from L and the other eight are
zero. Similarly, G′
i, j = Gi, j for i = 3i′ −1, j = 3 j′ −1, and B′
i, j = Bi, j for i = 3i′ −1, j = 3 j′, and
zero elsewhere. As such, if DSD is applied to either L or L′, the same small image S will be obtained. We
deﬁne the auxiliary luminance component I(i, j) for the (i, j)th pixel as I(i, j) = 1
3(R′
i, j +G′
i, j +B′
i, j).

4.04.4 Subpixel-Based Software Design in Mobile Display
107
(a)
(b)
(c)
(d)
FIGURE 4.9
Experiment result of pixel-based down-sampling with anti-aliasing ﬁlter (left of image) and MMSE-SD in [19]
(right of image).
By deﬁnition, IDSD can be written as
IDSD(i, j) = 1
3
	
R′
i, j + G′
i, j + B′
i, j

= 1
3
	
R′
i, jR
DSD(i, j) + G′
i, jG
DSD(i, j) + B′
i, jB
DSD(i, j)

= 1
3
	
Ri, jR
DSD(i, j) + Gi, jG
DSD(i, j) + Bi, jB
DSD(i, j)

=

p
C p(i, j)p
DSD(i, j),
(4.5)

108
CHAPTER 4 Image Display—Mobile Imaging and Interactive Image Processing
where
R
DSD(i, j) =
 1,
for i = 3i′ −1, j = 3 j′ −2, i′, j′ = 1, 2, . . .
0,
otherwise
= 4
9
1
2 + cos 2π(i −2)
3
 1
2 + cos 2π( j −1)
3

(4.6)
and
G
DSD(i, j) = 4
9
1
2 + cos 2π(i −2)
3
 1
2 + cos 2π( j −2)
3

B
DSD(i, j) = 4
9
1
2 + cos 2π(i −2)
3
 1
2 + cos 2π j
3

(4.7)
are the RGB modulation functions, and C p(i, j) are the three color components of L (R, G or B for
p = 1, 2, or 3). The Fourier transform of IDSD is
ˆIDSD(u, v) = 1
3

p
ˆC p(u, v) ∗ˆp
DSD(u, v),
(4.8)
where ˆ· represents the Discrete Time Fourier transform (with zero padding outside the support of IDSD
and p
DSD) and ∗is the convolution operator.
As the three RGB modulation functions in (4.6) and (4.7) are based on the cosine function, their
Fourier transforms are expressed as Diracs, i.e.,
R
DSD(i, j) = 4
9
1
2 + cos 2π(i −2)
3
 1
2 + cos 2π( j −1)
3

,
=⇒ˆR
DSD(u, v) = 1
9
⎛
⎜⎜⎜⎜⎜⎜⎜⎝
a1a2δ

u + 1
3, v + 1
3

+ a2δ

u, v + 1
3

+ a2
2δ

u −1
3, v + 1
3

+ a1δ

u + 1
3, v

+ δ(u, v) + a2δ

u −1
3, v

+ a2
1δ

u + 1
3, v −1
3

+ a1δ

u, v −1
3

+ a1a2δ

u −1
3, v −1
3

⎞
⎟⎟⎟⎟⎟⎟⎟⎠
= 1
9

δ

u + 1
3

δ(u)
δ

u −1
3
  ⎡
⎣
a1a2
a1
a2
1
a2
1
a1
a2
2
a2
a1a2
⎤
⎦
⎡
⎢⎢⎢⎢⎣
δ

v + 1
3

δ(v)
δ

v −1
3

⎤
⎥⎥⎥⎥⎦
,
(4.9)

4.04.4 Subpixel-Based Software Design in Mobile Display
109
where a1 = e−j 2π
3 , a2 = e j 2π
3 and δ(u, v) = δ(u)δ(v). Similarly, we can compute the Fourier transform
of G
DSD and B
DSD. We then represent them in a matrix form,
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
ˆR
DSD(u, v) = 1
9(u)T
⎡
⎣
a1a2
a1
a2
1
a2
1
a1
a2
2
a2
a1a2
⎤
⎦(v),
ˆG
DSD(u, v) = 1
9(u)T
⎡
⎣
a2
1
a1
a1a2
a1
1
a2
a1a2
a2
a2
2
⎤
⎦(v),
ˆB
DSD(u, v) = 1
9(u)T
⎡
⎣
a1
a1
a1
1
1
1
a2
a2
a2
⎤
⎦(v),
(4.10)
where (u)T =

δ

u + 1
3
 
, δ(u), δ

u −1
3
 
and (v)T =

δ

v + 1
3
 
, δ(v), δ

v −1
3
 
, and [·]T
denotes transposition.
Rewriting (4.8) with (4.10), and considering the relationship between a1 and a2,
a1a2 = 1,
a2
1 = a2,
a2
2 = a1,
(4.11)
it can be shown that,
ˆIDSD(u, v) = 1
31T
3
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
a1 ˆC2

u + 1
3, v + 1
3

a1 ˆY

u + 1
3, v

a1 ˆC1

u + 1
3, v −1
3

ˆC2

u, v + 1
3

ˆY(u, v)
ˆC1

u, v −1
3

a2 ˆC2

u −1
3, v + 1
3

a2 ˆY

u −1
3, v

a2 ˆC1

u −1
3, v −1
3

⎤
⎥⎥⎥⎥⎥⎥⎥⎦
13,
(4.12)
where
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
ˆY(u, v) = 1
3( ˆR(u, v) + ˆG(u, v) + ˆB(u, v)),
ˆC1(u, v) = 1
3(a1 ˆR(u, v) + a2 ˆG(u, v) + ˆB(u, v)),
ˆC2(u, v) = 1
3(a2 ˆR(u, v) + a1 ˆG(u, v) + ˆB(u, v)).
(4.13)
To compare the difference between DSD and DPD, the corresponding IDPD is deﬁned similarly to
(4.5) of which the three RGB modulation functions are p
DPD as deﬁned in (4.14). Note that p
DPD is
identical for the R, G and B color components in DPD, i.e.,
R
DPD(i, j) = G
DPD(i, j) = B
DPD(i, j) = 4
9
1
2 + cos 2π(i −2)
3
 1
2 + cos 2π( j −2)
3

.
(4.14)

110
CHAPTER 4 Image Display—Mobile Imaging and Interactive Image Processing
The Fourier transform of IDPD is given by
ˆIDPD(u, v) = 1
31T
3
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
a2 ˆY

u + 1
3, v + 1
3

a1 ˆY

u + 1
3, v

ˆY

u + 1
3, v −1
3
 
a1 ˆY

u, v + 1
3

ˆY(u, v)
a2 ˆY

u, v −1
3

ˆY

u −1
3, v + 1
3

a2 ˆY

u −1
3, v

a1 ˆY

u −1
3, v −1
3

⎤
⎥⎥⎥⎥⎥⎥⎥⎦
13. (4.15)
Following a similar deﬁnition to (4.5), the three RGB modulation functions for DDSD are
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
R
DDSD(i, j) = 4
9
1
2 + cos 2π(i −1)
3
 1
2 + cos 2π( j −1)
3

,
G
DDSD(i, j) = 4
9
1
2 + cos 2π(i −2)
3
 1
2 + cos 2π( j −2)
3

,
B
DDSD(i, j) = 4
9
1
2 + cos 2πi
3
 1
2 + cos 2π j
3

.
(4.16)
The Fourier transform of IDDSD can be expressed as
ˆIDDSD(u, v) = 1
31T
3
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
ˆC1

u + 1
3, v + 1
3

ˆC2

u + 1
3, v

ˆY

u + 1
3, v −1
3

ˆC2

u, v + 1
3

ˆY(u, v)
ˆC1

u, v −1
3

ˆY

u −1
3, v + 1
3

ˆC1

u −1
3, v

ˆC2

u −1
3, v −1
3

⎤
⎥⎥⎥⎥⎥⎥⎥⎦
13, (4.17)
where ˆY, ˆC1 and ˆC2 are given in (4.13).
Figure 4.10 shows the magnitude of ˆIDSD in (4.12), ˆIDPD in (4.15), and ˆIDDSD in (4.17). It is clear that
there are nine replicated spectra situated at u ∈{−1
3, 0, 1
3} and v ∈{−1
3, 0, 1
3} in each of ˆIDSD, ˆIDPD and
ˆIDDSD corresponding to the nine Diracs locations. Examining Figure 4.10a, all nine replicated spectra
of ˆIDPD are ˆY with equal magnitude, since the magnitude of a1 and a2 in (4.15) is equal to 1. The
magnitude of ˆIDSD is obviously different from that of ˆIDPD. While the three center replicated spectra of
ˆIDSD are ˆY, the three on the right are ˆC1 and the three on the left are ˆC2 corresponding to (4.12). The
horizontal shifting of sampling locations of the three colors in DSD leads to the replacement of ˆY by ˆC1
and ˆC2 on the two sides of ˆIDSD. It also appears that ˆC1 and ˆC2 are signiﬁcantly different than ˆY. While
the arrangement of the nine replicated spectra in DDSD is obviously different from that in DPD and
DSD. The three replicated ˆY are located in the anti-diagonal direction in DDSD. Both the horizontal
and vertical shifting of the sampling locations of the three colors in DDSD as compared to DPD leads
to the replacement of six ˆY in the DPD spectra by ˆC1 and ˆC2.

4.04.4 Subpixel-Based Software Design in Mobile Display
111
(a)
(b)
(c)
FIGURE 4.10
(a) Frequency spectrum of ˆIDPD, (b) frequency spectrum of ˆIDSD, and (c) frequency spectrum of ˆIDDSD.
(a)
(b)
(c)
FIGURE 4.11
(a) Frequency spectra of ˆY component, (b) frequency spectra of ˆC1 component, and (c) frequency spectra
of ˆC2 component.
To examine the difference between ˆC1, ˆC2 and ˆY, we compare the magnitude of ˆY, ˆC1 and ˆC2 as
shown in Figure 4.11. All three have more energy at low than high frequencies, as expected. Although
ˆC1, ˆC2 and ˆY are weighted sums of ˆR, ˆG and ˆB according to (4.13), their magnitudes in Figure 4.11 can
be quite different. Both ˆC1 and ˆC2 appear to have more compact spectra than ˆY. This can be explained
as follows. Any signal such as R can be decomposed into a low-frequency term Rl and a high frequency
term Rh. It is well known that the high frequency components of different colors tend to be similar [25],
i.e., ˆRh(u, v) ≈ˆGh(u, v) ≈ˆBh(u, v). This implies that

112
CHAPTER 4 Image Display—Mobile Imaging and Interactive Image Processing
ˆC1(u, v) = 1
3
!
a1 ˆR(u, v) + a2 ˆG(u, v) + ˆB(u, v)
"
= 1
3
!
a1
 ˆRl + ˆRh
 
+ a2
 ˆGl + ˆGh
 
+
 ˆBl + ˆBh
 "
≈1
3
!
a1 ˆRl + a2 ˆGl + ˆBl
 
+ (a1 + a2 + 1) ˆGh
"
= 1
3
!
a1 ˆRl(u, v) + a2 ˆGl(u, v) + ˆBl(u, v)
"
,
(4.18)
since a1 + a2 + 1 ≡0. This means that ˆC1 is mainly a low frequency signal, with very little high
frequency energy. A similar argument applies to ˆC2. While ˆC1 and ˆC2 are low-frequency signals, ˆY
contains signiﬁcant high frequency energy because
ˆY(u, v) = 1
3
!
ˆR(u, v) + ˆG(u, v) + ˆB(u, v)
"
≈1
3
! ˆRl + ˆGl + ˆBl
 
+ (1 + 1 + 1) ˆGh
"
= 1
3
!
ˆRl

u, v
 
+ ˆGl(u, v) + ˆBl(u, v) + 3 ˆGh(u, v)
"
.
(4.19)
According to Nyquist criterion, a signal bandlimited to W must be sampled at fs ≥2W [14,26].
Suppose an image X is obtained by sampling (critically) at such a sampling frequency fs = 2W. If X
is to be k:1 down-sampled, the effectively sampling frequency will be reduced to f ′
s = fs/k. One way
to prevent aliasing is to apply a low-pass (anti-aliasing) ﬁlter to the signal with a cutoff frequency of
f ′
s/2 = fs/2k. For the image X, recall that the digital frequency of 1 corresponds to analog frequency
of fs. Thus the digital cutoff frequency of the low-pass ﬁlter is 1/2k. For us, k = 3. Thus the digital
cutoff frequency is 1/6. Nevertheless, with the low-frequency nature of ˆC1 and ˆC2, the horizontal
overlap in DSD between the center ˆY and the ˆC1 on the right and the ˆC2 on the left are signiﬁcantly
lower than the horizontal overlap in DPD among the ˆYs. On the other hand, with the smaller amount of
horizontal overlap in DSD, it is perhaps possible to use a higher cut-off frequency (larger than 1/6) to
retain more of the signal details in ˆY. While the center ˆY in DDSD has signiﬁcantly lower overlapping
with the horizontally and vertically neighboring spectra than DPD. Compared with DSD, DDSD has the
advantage that the center ˆY overlaps less with the vertical neighbors, even though its overlap with the
anti-diagonal neighboring ˆY can be considerable. Therefore, while DSD can extend its cut-off frequency
beyond the Nyquist frequency horizontally, DDSD can be extended both horizontally and vertically.
For the implementation, as noted in [24], a zero-mean circularly symmetric Laplacian model is used to
represent the normalized replicated spectra. Under such approximation, the cut-off frequency is chosen
to be the frequency where the main spectrum has power at least greater than or equal to that of the
aliasing spectra.
Figure 4.12 shows the down-sampled results of conventional pixel-based down-sampling with anti-
aliasingﬁlter[14]andfrequencyanalysisapproachpreviouslydiscussed.Asweexpected,thefrequency-
domain analysis approach for DSD and DDSD achieve higher apparent resolution than pixel-based

4.04.4 Subpixel-Based Software Design in Mobile Display
113
(a)
(b)
(c)
FIGURE 4.12
Down-sampled results using various methods (a) pixel-based down-sampling with anti-aliasing ﬁlter,
(b) frequency-domain analysis approach for DSD in [24], and (c) frequency-domain analysis approach for
DDSD in [24].

114
CHAPTER 4 Image Display—Mobile Imaging and Interactive Image Processing
scheme, leading to sharper images. While the frequency analysis approach for DDSD achieves con-
siderably if not signiﬁcantly more details than DSD, due to relatively smaller overlapping of centering
spectrum and vertical aliasing spectra.
4.04.5 Conclusion
In this chapter, we discuss mobile imaging processing using subpixel rendering techniques to achieve
superior sharpness for mobile displays. However, the increased apparent luminance resolution often
comes at the price of color fringing artifacts. Several down-sampling (both pixel and subpixel-based)
schemes for mobile image displaying are presented as well. Based on the frequency domain analysis
approach for the aliasing behavior of several down-sampling schemes, i.e., DPD (pixel-based), DSD
and DDSD (subpixel-based), it can be theoretically shown that the cut-off frequency of the low-pass
ﬁlter for subpixel-based decimation can be effectively extended beyond the Nyquist frequency using
novel anti-aliasing ﬁlters to achieve a higher apparent resolution than pixel-based down-sampling.
Relevant theory: Signal Processing Theory
See Vol. 1, Chapter 5 Sampling and Quantization
References
[1] D.S. Messing, L. Kerofsky, S. Daly, Subpixel rendering on nonstriped colour matrix displays, in: IEEE
Proceedings of the International Conference on Image Processing (ICIP), vol. 2, 2003.
[2] Microsoft, ClearType Information, retrieved from <http://www.microsoft.com/typography/cleartypeinfo.
mspx>, 2002.
[3] S.
Gibson,
Sub-pixel
font
rendering
technology,
Gibson
Research
Corporation,
retrieved
from
<http://www.grc.com/cleartype.htm>, 2010.
[4] J.C. Platt, Optimal ﬁltering for patterned displays, IEEE Signal Process. Lett. 7 (7) (2000) 179–181.
[5] C. Betrisey, J.F. Blinn, et al., Displaced ﬁltering for patterned displays, in: SID International Symposium
Digest of Technical Papers, vol. 31, 2000, pp. 296–301.
[6] C.H.B. Elliott,S.Han,et al.,Co-optimizationof color AMLCDsubpixel architecture and rendering algorithms,
in: SID International Symposium Digest of Technical Papers, vol. 33, 2002, pp. 172–175.
[7] S. Gibson, The origins of sub-pixel font rendering, Gibson Research Corporation, retrieved from
<http://www.grc.com/ctwho.htm>, 2006.
[8] S. Daly, Analysis of subtriad addressing algorithms by visual system models, in: SID International Symposium
Digest of Technical Papers, vol. 32, 2001, pp. 1200–1204.
[9] M.A. Klompenhouwer, G. De Haan, R.A. Beuker, Subpixel image scaling for color matrix displays, J. Soc.
Inform. Display 11 (2003) 176–180.
[10] C.H.B. Elliot, Active matrix display layout optimization for subpixel image rendering, in: Proceedings of the
1st International Display Manufacturing Conference, September 2000, pp. 185–187.
[11] Virtual resolution banishes pixel limits in mobile displays. <www.vp-dynamics.com>.
[12] Mobile HDTV displays ﬁnd their ways into automotive applications. <www.vpdynamics.com>.
[13] R. Lai, Under the microscope: Samsung galaxy S III’s HD Super AMOLED display, retrieved from
<http://www.engadget.com/2012/05/03/galaxy-s-iii-microscope-hd-super-amoled/>, 2012.

References
115
[14] R.C. Gonzalez, E. Richard, Woods, Digital Image Processing, Publishing House of Electronics Industry, 2005,
pp. 420–450.
[15] L. Fang, O.C. Au, K. Tang, X. Wen, Increasing image resolution on portable displays by subpixel rendering—a
systematic overview, APSIPA Trans. Signal Inform. Process. 1 (1) (2012) 1–10.
[16] S. Daly, R.R.K. Kovvuri, Methods and Systems for Improving Display Resolution in Images Using Sub-pixel
Sampling and Visual Error Filtering, Google Patents, US Patent App. 09/735 424, 2000.
[17] D.S. Messing, S. Daly, Improved display resolution of subsampled colour images using subpixel addressing,
in: IEEE Proceedings of the International Conference on Image Processing (ICIP), vol. 1, 2002.
[18] L. Fang, O.C. Au, Novel 2-D MMSE subpixel-based image down-sampling for matrix displays, in: Proceeding
of IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP), 2010, pp. 986–989.
[19] L. Fang, O.C. Au, K. Tang, X. Wen, H. Wang, Novel 2-D MMSE subpixel-based image down-sampling, IEEE
Trans. Image Process 22 (5) (2012) 740–753.
[20] B.A. Wandell, Foundations of Vision, Sinauer Associates, 1995, ISBN:0878938532.
[21] C.A. Piraga, G. Brelstaff, T. Troscianko, I. Moorhead, Color and luminance information in natural scenes,
J. Opt. Soc. Am. A 15 (3) (1998) 563–569.
[22] J.S. Kim, C.S. Kim, A ﬁlter design algorithm for subpixel rendering on matrix displays, in: Proc. of European
Signal Processing Conference (EUSIPCO), 2007.
[23] L. Fang, O.C. Au, Subpixel-based image down-sampling with Min-Max directional error for stripe display,
IEEE J. Sel. Top. Signal Process. 5 (2) (2011) 240–251.
[24] L. Fang, O.C. Au, K. Tang, A.K. Katsaggelos, Anti-aliasing ﬁlter design for subpixel down-sampling via
frequency domain analysis, IEEE Trans. Image Process 21 (3) (2012) 1391–1405.
[25] N.X. Lian, L. Chang, Y.P. Tan, V. Zagorodnov, Adaptive ﬁltering for color ﬁlter array demosaicking, IEEE
Trans. Image Process. 16 (10) (2007) 2515–2525.
[26] P.S.R. Diniz, Digital Signal Processing, Cambridge University Press, 2010, ISBN:0521887755.

5
CHAPTER
Image Display—Printing
(Desktop, Commercial)
Philipp Urban, Simon Stahl, and Edgar Dörsam
Technische Universität Darmstadt, Institute of Printing Science and Technology, Darmstadt, Germany
4.05.1 Introduction
Printed products affect everybody almost everywhere. Even though magazines, brochures, books or
newspapers might become less important and partially displaced by other display technologies in future,
printing will remain the dominant display technology in many important industries particularly in
packaging.
The history of printing reaches back more than 5000 years. The oldest known techniques for repro-
ducing images and characters were invented in the middle east around 3000 BC. Plate or cylinder seals
were impressed on clay tablets to create raised ﬁgures and characters.
The ﬁrst books printed on paper were invented in China during the Tang Dynasty (618–907 AC).
Chinese characters were pressed to the paper by wooden relief forms. This technique is named wood-
block printing and is still used for stylistic art printing.
Johannes Gutenberg (1397–1468 AC) is deemed to be the inventor of systematic letter press printing.
He developed movable types and the printing press. Using oil-based inks, he printed a bible in two
volumes with 1282 pages between 1453 and 1454. The technical quality and esthetic appearance is
praised even today.
Gutenberg’s letter press printing was more than a technical revolution. Because of the quick distri-
bution across Europe and the world, a wide variety of people got access to information and particularly
education.
Relief and gravure printing have been a long time the dominant printing techniques. Alois Senefelder
invented the ﬁrst planographic printing process in 1797, called lithography printing. Further technical
developments of this technique led to offset printing that is widely used today.
Another important printing technique, ﬂexographic printing, uses a ﬂexible printing plate. It was
originated from aniline printing a relief printing technique that used aniline oil-based inks and rubber
printing plates.
At the outset, printing presses employed ﬂat printing plates. In 1858, the ﬁrst rotary printing press
was invented. Here, cylindric printing plates are used allowing a continuous print run with signiﬁcantly
increased productivity.
The printing techniques mentioned before are called conventional. They all have in common that they
use some sort of printing plates. Techniques that do not employ a printing plate are called non-impact
printing techniques. Most digital printing techniques belong to this category.
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00005-4
© 2014 Elsevier Ltd. All rights reserved.
117

118
CHAPTER 5 Image Display—Printing (Desktop, Commercial)
In 1858, a patent describing an ink-drop-based recording device was granted to Lord Kelvin. This
can be seen as the advent of inkjet printing. In 1938, Chester F. Carlson invented the principles of
electrophotography that are still used in modern laser printers.
The printing techniques mentioned in this historical sketch are explained in the next Section 4.05.2.
Research in the area of printing is highly multidisciplinary covering areas from chemistry, physics,
engineering, computer science, psychology or economics. In this article, we will focus on the signal
processing aspect related to the display of images. The corresponding concepts and methods are almost
independent from the actual printing technology.
Printing is a passive way of displaying images, i.e., external light is required to see the image. For
accurately reproducing images, it is important to understand and to model the interaction of the inks,
substrate and incident photons. In addition to pure reﬂectance properties of inks and substrate, effects
such as subsurface scattering of light within the substrate or multiple reﬂections at the substrate-air
interface need to be considered. Accurate spectral printer models incorporate these effects and relate
the control parameters, mostly tonal values that control the amount and mixture of the available inks, with
the physically measurable optical properties of the resulting printout. Such models are the prerequisite
for accurately reproducing images by printing systems. More details on the involved effects and the
most common spectral printer models can be found in Section 4.05.4.
Printer models are still an active research area. Especially, if it comes to printing systems employing
many inks or unusual inks such as ﬂuorescent, metallic or goniochromatic inks (e.g., interference effect
inks).
Modeling printing systems is only one aspect required for image reproduction. Other important
aspects are how to handle colors that are not reproducible by the actual printing system (gamut map-
ping) or how to transform continuous-tone color images into a dot-arrangement of available inks so
that both images are visually equivalent when viewed from a particular distance (halftoning). These
problems are strongly connected to human color and spatial vision and require appropriate percep-
tion models (e.g., adopted from CIE colorimetry or models of chromatic and achromatic contrast
sensitivity).
Another challenge is to ﬁnd appropriate mixtures of inks to reproduce given colors (separation).
This requires the numerical inversion of an accurate highly non-linear printer model, which is an ill-
posed problem in general. Furthermore, every computed separation band needs to possess similar spatial
correlations as the input image to avoid banding artifacts in the resulting print. Separation algorithms
considering such spatial constraints are still an active research area, particularly for printing systems with
a large number of inks. The prepress workﬂow consisting of gamut mapping, separation and halftoning
is discussed in Section 4.05.3.
Traditional printing workﬂows adjust printed reproductions to match with the original input image
for speciﬁc viewing conditions (e.g., under daylight). One relatively new research direction is to ﬁnd
approaches to create printouts that match with the original under many or all viewing conditions. This
is referred to spectral printing and discussed in Section 4.05.5.1.
New technologies and inks allow applications unimaginable in the past. One of these is the reproduc-
tion of the optical appearance of materials allowing a copy that matches for different viewing and illu-
minating geometries. Which requires the reproduction of the spatially varying bidirectional reﬂectance
distribution function (BRDF). Or printing with ﬂuorescent or metallic inks for security applications.
Corresponding research is in its infancy today.

4.05.2 Printing Technologies
119
In addition to traditional graphical printing, functional printing is a very active research ﬁeld today.
Functional printing uses functional materials as inks to inexpensively manufacture electronic circuits
or devices, e.g., transistors, solar panels, batteries or Organic Light Emitting Diodes (OLEDs). Even
though such applications are far beyond the scope of this article its potential impact in industry is
noteworthy. To see the big picture in the area of printing it is sketched in the end.
4.05.2 Printing technologies
Various printing technologies were invented to meet the needs of distinct applications such as speed,
quality or individualization. The technologies sketched in this section are described in more detail by
Kipphan [1].
4.05.2.1 Gravure printing
Gravure printing requires only two rollers as shown in Figure 5.1 and is the simplest printing technology
sketched in this chapter.
The image information is carried by the gravure cylinder that contains engraved cells corresponding
to the mirror-inverted image halftone. These cells are ﬁlled with ink by rotating the gravure cylinder that
is partly immersed in an ink reservoir. Excess ink covering non-printing areas is removed by a doctor
blade. The image is transferred from the gravure cylinder to the substrate by pressure of the impression
cylinder. When the ﬁlled cells come in contact with the substrate, surface tension forces are responsible
for the ink transfer.
For multi-color printing, each ink is printed in a separate printing unit. Drying units are required to
ensure that the previously printed ink is dry when entering the next printing unit.
The gravure technology allows high speed printing combined with high quality. The major disadvan-
tage of the technology is the cost for creating the gravure cylinder. This is the reason why gravure printing
is mostly used for very large print runs where web-fed machines are used to produce millions of copies.
The printing process described above is called rotogravure. Another related gravure technology is
called intaglio printing that does not use a halftone representation of the image on the cylinder. Instead,
image elements such as lines are incised in total into the cylinder’s surface. There are also indirect
FIGURE 5.1
Principle of gravure printing. (Source: IDD, TU-Darmstadt.)

120
CHAPTER 5 Image Display—Printing (Desktop, Commercial)
gravure techniques such as intaglio offset and gravure offset that transfer the image from the gravure
cylinder onto an additional cylinder from which it is transferred further to the substrate. Another indirect
gravure technology is pad printing. It allows printing on curved objects such as bottles or keyboards.
More details on gravure printing can be found in [2].
4.05.2.2 Flexographic printing
Flexography is a relief printing technology. Figure 5.2 illustrates the printing principle.
Intheﬁrststageoftheprintingprocess,theengravedcellsoftheaniloxrollareﬁlledbyink.Theanilox
roll is a hard cylinder coated by an industrial ceramic. Its surface is completely covered by engraved
cells that are used for exact metering of ink (see Figure 5.3). Filling the cells can be achieved by rotating
the anilox roll within an ink reservoir and remove the ink that covers non-cell areas (walls between the
cells) by a doctor blade, similar to gravure printing. Another technology to ﬁll only the cells of the anilox
FIGURE 5.2
Principle of ﬂexographic printing. (Source: IDD, TU-Darmstadt.)
(a)
(b)
1200 μm
950 μm
FIGURE 5.3
(a) Photo of an anilox roll’s surface. (b) White-light interferometric topography measurement of partly ﬁlled
cells of an anilox roll. (Source: IDD, TU-Darmstadt.)

4.05.2 Printing Technologies
121
roll is a chamber doctor blade system: Here, the ink circulates through a chamber that is in direct contact
with the anilox roll. Two doctor blades seal the chamber and remove excess ink. Please note that the
anilox roll is completely covered by cells unlike the gravure cylinder described in the previous section.
The ink is then transferred by slight pressure and surface tension forces to the ﬂexible relief printing
plate (ﬂexoplate) that carries the image information on raised areas. Only these printing areas are covered
by ink, the other areas remain clean. In the ﬁnal stage of the process the image is transfered from the
ﬂexoplate to the substrate by slight pressure of the impression cylinder.
The ﬂexoplate is typically made of a photopolymer that hardens by UV-exposure. For platemaking,
a mask is used to ensure that only the printing areas of the plate are exposed by UV-light. The remaining
material is removed. The mask carrying the image information may consist of conventionally developed
ﬁlm. Today, mostly computer-guided lasers are used to expose the mask directly on top of the printing
plate. Flexoplates may also consist of an elastomer. For these plates, laser-engraving is used to create
the printing areas.
Flexographic printing is used most notably in the packaging industry. Card board, paper, foil and
also glass or silicon can be used as substrates. More details on ﬂexographic printing can be found in [3].
4.05.2.3 Offset printing
Offset printing is the most common conventional printing technology, despite the high complexity of
the printing unit that is shown in Figure 5.4.
The inking unit consists of many rotating rollers (between 10 and 20) for liquefying the highly
viscous ink by shear forces. This allows an adequate ink supply and printing of multiple inks without
intermediate drying. The image carrier is nearly completely ﬂat, i.e., the printing and non-printing parts
FIGURE 5.4
Principle of offset printing. (Source: IDD, TU-Darmstadt.)

122
CHAPTER 5 Image Display—Printing (Desktop, Commercial)
of the printing plate are all on one level. Ink separation is based on hydrophobic (water-repellent) and
hydrophilic (water-accepting) surface properties of the printing plate. In a ﬁrst stage of the printing
process, the dampening unit covers the hydrophilic areas of the printing plate by a dampening solution.
In the subsequent application of ink only the hydrophobic areas of the printing plate are bewetted by
ink. The image is then transfered to the blanket cylinder and from there to the substrate.
A further development is the waterless offset. This technology uses a silicon coating of the non-
printing plate areas for ink separation. This allows ﬁner screens at the prize of higher costs, which
prevented a larger dissemination of this technology.
For conventional and waterless offset printing, short ink units can be employed. In short ink units,
ink dosing is controlled by a ﬂexo-like anilox roll that replaces the rollers used to liquefy the ink. By
adjusting the temperature of the anilox roll the ink’s viscosity and therewith the amount of transfered
ink is controlled.
Offset printing plates are made of a hydrophilic material (almost always aluminum) covered by a
special thin coating that possesses hydrophobic properties. For platemaking, a computer-guided laser
is used to expose the printing areas, which hardens the coated layer. The remaining coating is removed.
Also hydrophobic coating materials are used that are not hardened but removed by a laser. In this case,
the non-printing areas are exposed by the laser.
Offset printing is used for almost all products in the graphic art industry, particularly for ﬂyers,
brochures and newsletters. The range of substrates goes from foil, thin newsprint up to card board.
Sheet-fed as well as web-fed presses are used. One of the main advantages of offset printing is the
simple platemaking, the standardized process, and the high print quality.
4.05.2.4 Inkjet printing
Inkjet printing is a digital printing technology that allows a contactless assembling of an halftone image
on the substrate by means of ink drops. The ink is transported from a ﬂuid reservoir to the print head and
ejected by nozzles onto the printing substrate. For desktop printing, the print head typically traverses
perpendicularly to the substrate’s feed direction. For high performance printing multiple nozzle arrays
covering the whole print width are mounted perpendicularly to the feed direction.
Multiple inkjet technologies are established today. They can be classiﬁed into two main categories:
Drop-on-demand (DoD) and continuous inkjet.
The drop-on-demand technology allows the creation of single drops by increasing the pressure within
an ink chamber forcing ink to leave the nozzle. The pressure can be established by a piezo element as
shown in Figure 5.5 or by a heating element that creates a bubble of vaporized ink within the chamber
as shown in Figure 5.6.
The continuous inkjet technology uses a pump to create a continuous stream of ink that is partitioned
by a piezoelectric transducer into a high-frequency drop stream. The ink drops are electrically charged
to allow a deﬂection of single drops by high voltage plates. Drops can be deﬂected and, if not required,
collectedandrecycled(binarydeﬂection—seeFigure5.7)ordirectedtodesiredlocationsonthesubstrate
(multiple deﬂection). The continuous inkjet technology is mostly used in roll-to-roll printing systems
e.g., for printing labels in packaging.
Inkjet printing systems can be found in small-ofﬁce-home-ofﬁce (soho) environments where mostly
DoD-systems utilizing few nozzles are used as well as in high performance roll-to-roll presses utilizing

4.05.2 Printing Technologies
123
Ink
Piezo
Imaging 
Signal
Nozzle
FIGURE 5.5
Principle of the piezo inkjet DoD technology. (Source: IDD, TU-Darmstadt.)
Ink
Nozzle
Bubble
Heat Source
Imaging
Signal
FIGURE 5.6
Principle of the bubble inkjet DoD technology. (Source: IDD, TU-Darmstadt.)
Ink
Charging
Electrode
Imaging
Signal
Drop
Deflection
Gutter
Pump
Substrate
FIGURE 5.7
Principle of the continuous inkjet technology with binary deﬂection system. (Source: IDD, TU-Darmstadt.)

124
CHAPTER 5 Image Display—Printing (Desktop, Commercial)
FIGURE 5.8
Principle of electrophotographic printing. (Source: IDD, TU-Darmstadt.)
large nozzle arrays. Also conventional printing presses are combined with smaller inkjet units for
individualized printing. More details on desktop inkjet printing can be found in Ref. [4].
4.05.2.5 Electrophotography
The electrophotographic printing technology is commonly used in commercial copiers and laser printers.
The core of the method is the photoconductor drum that is uniformly charged at the beginning of the
printing process. The halftone image is created on the drum by discharging the non-printing locations
using a controlled light beam (laser or LED). Subsequently, the inking system covers the drum with
powderedtoner.Liquidtonermayalsobeused.Thetoneradherestothechargedlocationsbyelectrostatic
forces and is transfered to the substrate by slight pressure or by an inversely charged transfer roller on
the reverse side of the substrate. Finally the toner is ﬁxated on the substrate by heat. The drum will
be discharged and automatically cleaned before the next run. Figure 5.8 illustrates the concept of
electrophotographic printing. More details can be found in Ref. [4].
Paper, thin cardboard and foil can be employed as substrates. Separated printing units in various
arrangements are used for multi-color printing. Also multiple successive imaging steps using different
toner colors in a single drum revolution are possible. The electrophotographic technology is mostly
employed for sheet fed printing. Some industrial systems print also on paper web.
4.05.3 Workﬂow
The typical prepress workﬂow for generating a printed hardcopy from a digital image is shown in
Figure 5.9 and consists of a sequence of image processing steps leading to halftone images for the
employed colorants. These halftones are then used to create printing plates or directly by digital printing
systems.

4.05.3 Workﬂow
125
Gamut 
Mapping
FIGURE 5.9
Workﬂow. Separations are shown for a CMYK printer. (For interpretation of the references to color in this
ﬁgure legend, the reader is referred to the web version of this book.)
Please note that any esthetic modiﬁcations or appearance related adjustments of the image (e.g., chro-
matic adaptation [5]) to reach speciﬁc color reproduction objectives (see [6,7]) are already performed in
a preliminary step. Such image modiﬁcations are not limited to printing and therefore beyond the scope
of this article. The only prerequisite is that the desired printed output is colorimetrically speciﬁed, i.e.,
that the image data is represented in the device-independent CIEXYZ color space (see Section 4.05.3.1).
It is noteworthy that most color cameras already provide colorimetrically speciﬁed images. They are
typically encoded in standard RGB color spaces that are deﬁned upon CIEXYZ [8]. The required pro-
cessing workﬂow that transforms the camera’s sensor responses into device-independent color spaces
is included in the cameras (see e.g., [9]).
4.05.3.1 Basic colorimetry used in printing
We assume that the image used as the input of the workﬂow is represented in the CIEXYZ color space
for a distinct colorimetric observer and desired illuminant. CIEXYZ colors are independent from the
printing device and are obtained from reﬂectances by
⎡
⎣
X
Y
Z
⎤
⎦= K


r(λ)l(λ)
⎡
⎣
¯x(λ)
¯y(λ)
¯z(λ)
⎤
⎦dλ,
(5.1)
where ¯x, ¯y, and ¯z are color matching functions (CMFs) of the CIE 2◦or 10◦colorimetric standard
observer, r is the reﬂectance spectrum, l the viewing illuminant and  = [380 nm, 730 nm] the human
visible wavelength range. K is a constant that normalizes Y to one for the perfect reﬂecting diffuser, i.e.,

126
CHAPTER 5 Image Display—Printing (Desktop, Commercial)
r(λ) = 1. The CMFs are linearly related to the cone fundamentals, which are the cone photoreceptor
sensitivities multiplied with the pre-retinal transmission of the human eye (lens, vitreous, etc.) [10].
Please note that multiple reﬂectances may be transformed to the same CIEXYZ color for a dis-
tinct illuminant. This is referred to as illuminant metamerism. Corresponding reﬂectances are called
metameric reﬂectances or simply metamers.
The CIEXYZ color space is linearly related to the intensity of the stimulus r(λ)l(λ). However, human
perceptionshowsanon-linearrelationshiptointensitywhichcanbewellmodeledbyapowerlowaccord-
ing to Stevens [11]. This is particularly important for predicting perceived color differences between two
different stimuli. Such differences are required for process control or quality assurance of the ﬁnal print.
For this purpose, a perceptually uniform color space is desired, i.e., a color space for that Euclidean
distances agree with perceived differences. The CIEXYZ color space is far from being perceptu-
ally uniform. A color space that shows improved perceptual uniformity is the CIELAB color space.
CIELAB coordinates (L∗, a∗, b∗) are deﬁned by an invertible transformation from CIEXYZ coordinates
(X, Y, Z):
L∗:= 116 f
 Y
Yw

−16,
a∗:= 500
	
f
 X
Xw

−f
 Y
Yw

,
(5.2)
b∗:= 200
	
f
 Y
Yw

−f
 Z
Zw

,
f (ξ) :=

ξ
1
3 ,
ξ ≥0.008856,
7.787ξ + 16
116, else,
where Xw, Yw, Zw are the white point’s CIEXYZ values corresponding to the perfect reﬂecting diffusor,
i.e., r(λ) = 1. The white point’s CIEXYZ values are required to model the chromatic adaptation
of the observer to the white point using the terms X/Xw, Y/Yw, and Z/Zw (see wrong van Kries
transformation [12]). CIELAB is a, so called, opponent color space that empirically mimics to some
extent the post-retinal wiring of the visual signals. It has a lightness axis represented by the L∗coordinate
as well as a red-green (a∗) and a blue-yellow axis (b∗).
Transforming CIELAB’s Cartesian into cylindrical coordinates leads to the CIELCH color space with
coordinates predicting the color attributes: lightness, chroma and hue. The radial coordinate represents
chroma C∗
ab and the angular coordinate hue hab. Please note that CIELAB has some shortcomings with
respect to hue linearity [13], i.e., perceived hue varies along lines of constant predicted hue (especially
in the blue and red regions of CIELAB). This has to be considered by gamut mapping algorithms as
described below.
In various standards of the graphic art industry, the CIE 2◦colorimetric standard observer and the
CIED50 illuminant are used. More details on CIE colorimetry including the deﬁnition of CIE observers
and illuminants can be found in [10,14].
The aim of the prepress workﬂow is to create halftone images of the employed colorants so that
the resulting print has a minimum perceived difference to the input image under the desired viewing
conditions (observer, illuminant). It is worth mentioning that a particular viewing geometry of the print
is implicitly assumed that has to agree with the measurement geometry used to characterize the printer.

4.05.3 Workﬂow
127
In the graphics art industry a 45◦/0◦geometry is used, i.e., the print is illuminated from 45◦and viewed
from 0◦measured to the substrate’s normal.
In the following, we will discuss the three major image processing modules of the workﬂow:
(1) Gamut mapping, (2) separation, and (3) halftoning.
4.05.3.2 Gamut mapping
The color media gamut is deﬁned as the set of all CIEXYZ values reproducible by a printer on a speciﬁc
medium (substrate) for speciﬁed viewing conditions. It is limited for any real printing system. Especially
for newspaper printing the media gamut is very small. But even for printing systems utilizing many
inks and high quality paper, the media gamut does not cover all colors as shown for a CMYKRGB
printer in Figure 5.10. Therefore, it is necessary to transform non-reproducible colors of an image into
the media gamut aiming to minimize perceived image differences. This transformation is denoted as
gamut mapping.
4.05.3.2.1
Determining the media gamut
The gamut mapping transformation requires the knowledge of the media gamut and a quick access to
its boundaries. Usually CIELAB, as a nearly perceptually uniform color space, is used for gamut repre-
sentation. Color spaces with improved perceptual uniformity [15–17] are even better suited especially
for comparing media gamuts or optimizing system parameters to increase the gamut’s volume.
0
100
−100
−100
0
0
100
100
50
L*
a*
b*
FIGURE 5.10
Color gamut of a CMYKRGB printer on glossy paper shown within the CIELAB color space. (For interpretation
of the references to color in this ﬁgure legend, the reader is referred to the web version of this book.)

128
CHAPTER 5 Image Display—Printing (Desktop, Commercial)
2
2
π
π
π
π
A
GBD
8,8
θ
θ
8
8
-120
-60
0
60
120
-120
-60
0
60
120
0
20
40
60
80
100
r
a*
b*
L*
(b)
(a)
φ
φ
−
−
FIGURE 5.11
(a) Partitioning of the CIELAB color space into 16 × 16 spherical segments. (b) Corresponding GBD matrix.
For determining the gamut’s boundaries various methods were proposed that can be classiﬁed into
an empirical and a model-based category. Methods within the empirical category use some printed
and measured test colors covering the printable range to characterize the boundaries by tessellation
and interpolation [18–20] or by empirical functions [21,22]. Methods belonging to the model-based
category [23–26] use the printer model that allows a gamut computation with a smaller number of
printed test patches. Especially challenging are printing systems utilizing many colorants [27].
A widely used empirical method to determine the gamut boundary is the segment maxima method
proposed by Moroviˇc and Luo [20]. In the following the CIELAB color space will be used for repre-
senting the media gamut as an example. The idea is to partition the CIELAB color space into n × n
segments centered in the middle gray value (L∗, a∗, b∗) = (50, 0, 0) (see Figure 5.11a). The segments
are deﬁned by intervals of the spherical angle coordinates φ and θ, where φ corresponds to the hue angle.
Every printed and measured test color is located at least within one segment. For each of such segments,
the color with the largest distance r to the middle gray value is stored within the corresponding gamut
boundary descriptor (GBD) matrix AGBD (see Figure 5.11b). The rows and columns of the GBD matrix
correspond to the intervals of spherical coordinates θ and φ that deﬁne the segments. Please note that
a partitioning of the CIELAB color space in cylindrical segments may be used as well. In this case the
rows of the GBD matrix correspond to lightness L∗intervals. In cases were segments do not contain
colors the corresponding empty GBD matrix entries are interpolated by adjacent entries. To ensure that
all GBD matrix entries are on the gamut boundary a model-based modiﬁcation of the segment maxima
method was proposed [26].
The GBD matrix is a compact representation of the gamut boundary and allows a quick and simple
calculation of boundary intersections with lines of constant hue using the Flexible Sequential Line
Gamut Boundary (FSLGB) method illustrated in Figure 5.12. This is especially important since many
gamut mapping algorithms map colors along such lines into the media gamut. In a ﬁrst step of the
FSLGB method, the intersection of the gamut boundary and the plane with constant hue ¯φ that contains

4.05.3 Workﬂow
129
-80 -60 -40 -20 0
20
40
60
80
-50
0
50
100
-20
0
20
40
60
80
100
120
0
10
20
30
40
50
60
70
80
20
30
40
50
60
70
80
90
100
-80
-60
-40
-20
0
20
40
60
80
-50
0
50
100
−π
2
π
2
−π
π
θ
φ
(b)
(a)
(d)
(c)
φ
φ
φ
L*
a*
b*
a*
b*
L*
Cab*
Intersection
Out-of Gamut
Color
–
–
FIGURE 5.12
Illustration of the FSLGB method.
the line is computed (see Figure 5.12a and b). The angle ¯φ corresponds to a column of the GBD matrix,
more precisely to a line within the column as shown in Figure 5.12c. The intersections between this
line and the interconnection lines of neighboring row entries form a polygon that represents the gamut
boundary at hue angle ¯φ (see Figure 5.12c and d). The desired intersection is ﬁnally calculated in two
dimensions between the polygon and the given line as shown in Figure 5.12d.
4.05.3.2.2
Gamut mapping algorithms
Gamut mapping algorithms (GMA) are still an active research area. Since GMAs are not exclusively
developed for printing applications, we will only sketch the most important aspects of gamut map-
ping in this article. Interested readers are referred to Moroviˇc [28] who gives a good overview about
existing GMAs.
The aim of minimizing perceived image differences between the original and gamut-mapped image
requires an objective function that highly correlates with image difference perception. Unfortunately,
current image difference measures that could act as objective functions are far from accurately reﬂecting

130
CHAPTER 5 Image Display—Printing (Desktop, Commercial)
human difference judgments. This and the complexity of minimizing even simple image difference
measures makes such GMAs [29,30] more of academic interest. Hence, nearly all gamut mapping
algorithms are based on heuristics, such as preserving gray colors, minimizing hue shifts and maintaining
the relationship between colors. For this purpose nearly all GMAs work within hue linear, nearly
perceptually uniform color spaces (e.g., hue linearized CIELAB or IPT color space [31]).
GMAs may be classiﬁed into two categories: clipping and compression. Clipping GMAs map only
non-reproducible colors, i.e., colors that are outside of the gamut, and do not touch reproducible colors.
For large media gamuts, clipping algorithms may be advantageous. However, if many image colors
are outside of the media gamut, it is perceptually better to achieve a smooth transition by using a
more continuous mapping that affects both the in-gamut and out-of-gamut colors. This is particularly
important to avoid visually disturbing color-banding artifacts. GMAs that affect also in-gamut colors
belong to the compression category.
Almost all GMAs incorporated in commercial color management systems are based on pixel-wise
transformations. Colors are typically mapped along lines of constant hue toward distinct points on the
gray axes, e.g., the middle gray value or the CUSP lightness (the CUSP is the color within the gamut
that has the largest chroma value for a given hue angle). In commercial color management systems such
pixel-wise GMAs are encoded in multidimensional color lookup tables. In order to allow a quick gamut
mapping transformation of high resolution images these tables are usually not optimized to the actual
image and therefore do not exploit the full potential of the employed GMA.
In recent years the research focused on spatial gamut mapping [32–35]. The aim is to retain local color
contrasts within the image which is impossible if colors are mapped independently from their spatial
neighborhood. Most of the spatial GMAs use a multi-scale representation of the image to construct the
transformation [36]. A performance evaluation of some spatial GMAs can be found in Ref. [37].
4.05.3.2.3
Media-relative workﬂow and black point compensation
The reference white used in the workﬂow corresponds to the speciﬁed illuminant and the perfect
reﬂecting diffuser according to Eq. (5.2). Substrates employed in printing are usually far from being
such an ideal surface since they posses unwanted absorptions resulting in lightness values much lower
than L∗= 100 with non-vanishing chroma. However, they are mostly still perceived as whites. For this
reason and to ensure that the input white is reproduced by the pure substrate (media white) all colors
(Xabs, Yabs, Zabs) predicted by the colorimetric printer model are normalized by the media white point to
the white point deﬁned by the perfect reﬂecting diffuser
⎡
⎣
Xrel
Yrel
Zrel
⎤
⎦=
⎡
⎣
Xw/Xmw
0
0
0
Yw/Ymw
0
0
0
Zw/Zmw
⎤
⎦
⎡
⎣
Xabs
Yabs
Zabs
⎤
⎦,
(5.3)
where (Xw, Yw, Zw) are the CIEXYZ values corresponding to r(λ) = 1 and (Xmw, Ymw, Zmw) are
the media white point’s CIEXYZ values. All subsequent calculations are performed using these media-
relative colors. The resulting media-relative gamut used by the subsequent gamut mapping algorithms
includes the white point (L∗, a∗, b∗) = (100, 0, 0).
Adobe introduced a method called black point compensation as a preliminary step for mapping
between gamuts with different black points [38]. This method is particularly useful to preserve details

4.05.3 Workﬂow
131
in dark image areas. Assuming media relative colors (Xrel, Yrel, Zrel), black point compensation can be
computed as follows:
⎡
⎣
Xrel,bpc
Yrel,bpc
Zrel,bpc
⎤
⎦=
⎡
⎣
Xw
Yw
Zw
⎤
⎦−Yw −Yb,dst
Yw −Yb,src
⎡
⎣
Xw −Xrel
Yw −Yrel
Zw −Zrel
⎤
⎦,
(5.4)
where Yb,src is the media-relative luminance of the source gamut’s black point and Yb,dst is the media-
relative luminance of the printing system’s black point. Equation (5.4) is a linear scaling in the inten-
sity domain and can be interpreted as a color change due to a change of illumination intensity [28].
A subsequent gamut mapping within an appearance space as sketched previously yields mostly to natural
(not artiﬁcial-looking) color changes.
4.05.3.3 Separation
The result of the gamut mapping transformation are pixel colors that are printable by the device on the
considered substrate. In the next step, the amount and mixture of available inks need to be computed
to reproduce each pixel’s CIELAB color accurately. This process is called separation. Mathematically,
it is equivalent to the inversion of a colorimetric printer model P that predicts CIELAB colors of the
printoutfromcontrolvalues,i.e.,foraCMYK-printer P : CMYK →CIELAB.Pleasenote,thatspectral
printermodelsasdiscussedinSection4.05.4deﬁnecolorimetricprintermodelssincecolorimetricvalues
may be computed from predicted reﬂectances for any illuminant and observer according to Eqs. (5.1)
and (5.2). For printer models that are analytically not invertible, a constraint optimization problem needs
to be solved, e.g., for a CMYK-printer
minimize E(P(C, M, Y, K), c)
subject to C + M + Y + K < Tmax
(5.5)
C ≥0, M ≥0, Y ≥0, K ≥0,
where c ∈CIELAB is the in-gamut color to be reproduced, Tmax corresponds to the maximum total
ink coverage of the substrate and E(·, ·) is a color difference formula [39,40]. Problem (5.5) may be
solved using numerical optimization methods. Since c is within the gamut, the color difference deﬁning
the objective function vanishes if the solution is inserted.
For printing systems utilizing more than the basic CMY-inks (e.g., CMYK, CMYKRGB), a CIELAB
color might be reproduced by multiple combinations of the available inks. For instance, gray can be
reproduced either by the black ink or by a combination of cyan, magenta and yellow. Therefore, a
well predicting colorimetric printer model is not injective, i.e., for a CMYK-printer model: there exists
v, w ∈CMYK, v ̸= w : P(v) = P(w). Consequently this model is not unambiguously invertible. Each
in-gamut CIELAB color c can be reproduced by any control value within a non-empty set Mc ⊂CMYK,
i.e., v ∈Mc ⇒P(v) = c. Neglecting this ambiguity in the separation process by randomly picking
control values from these sets leads generally to banding artifacts in smooth color transitions due to
inaccuracies of the printer model, instabilities of the printing process and metameric mismatches caused
by deviations from the speciﬁed viewing conditions. Furthermore, widely used halftoning algorithms
compare the separation bands with arrays of threshold values to obtain dot clusters (see Section 4.05.3.4).
The size of these clusters corresponds to the local ink area coverages speciﬁed by the separation.

132
CHAPTER 5 Image Display—Printing (Desktop, Commercial)
C      M      Y       K
0%
100%
C      M      Y       K
0%
100%
Gray 
Component
min GCR
max GCR
FIGURE 5.13
In the case of ideal inks, a CIELAB color can be printed by replacing the gray component in the cyan,
magenta and yellow inks by the black ink. The left graph illustrates a minimum GCR strategy (minimum
black is used) and the right graph the maximum GCR. Please note that ideal inks posses perfect reﬂectance
and absorption properties. Different percentages of cyan, magenta and yellow inks are affected by GCR if
real inks are used. (For interpretation of the references to color in this ﬁgure legend, the reader is referred
to the web version of this book.)
A separation that shows high spatial variations in smooth image regions might lead to inconsistent
halftone screens unable to reproduce the desired CIELAB colors.
Therefore, one important aim of the separation is to ensure that every resulting band shows a spa-
tial correlation that agrees with the correlation of the corresponding input colors. This is particularly
important in image regions with high spatial correlations. However, an image-dependent separation con-
sidering spatial correlations is computational expensive. A widely used image-independent approach
is to use a continuously differentiable smooth transformation S from CIELAB to the printer’s control
space (e.g., CMYK) satisfying P(S(c)) = c for all in-gamut colors c. Such a transformation allows a
pixel-wise separation and can be optimized for additional needs e.g., minimum ink usage or low grain-
iness. Furthermore, it is possible to efﬁciently encode such transformations using multidimensional
lookup tables that are embedded in device proﬁles [41] allowing high-volume, real-time separations.
The smoothness of these transformations may be measured by the magnitude of their second derivatives.
The smaller this magnitude the smoother is the transformation. In the case of a lookup table, encoding
discrete derivatives of the nodes may be used.
It should be mentioned that for printing systems with many inks, e.g., CMYKRGB printers, a continu-
ouslydifferentiabletransformationdoesnothavetoexistonthewholegamut.Therefore,thegamutneeds
to be reduced by removing gamut regions that do not allow the deﬁnition of a smooth function S [42].
For CMYK-printers there are various strategies to achieve a smooth CIELAB to CMYK transfor-
mation. They relay on the property that any combination of cyan, magenta and yellow possess a gray
component that can be replaced by black without affecting the printed CIELAB color. Gray component
replacement (GCR) is illustrated in Figure 5.13 that shows the two extremes of GCR.
4.05.3.3.1
Gray component replacement (GCR)
In the case a printing system uses ideal inks (block dyes—see Figure 5.14), reﬂectances of printouts
can be well predicted by the Neugebauer model (see Section 4.05.4.4), and the ink mixture is perfectly

4.05.3 Workﬂow
133
400
550
700
0
1
400
550
700
0
1
wavelength [nm]
wavelength [nm]
Reflectance
Reflectance
Magenta
Yellow
400
550
700
0
1
wavelength [nm]
Cyan
FIGURE 5.14
Doted lines: Spectral reﬂectances of ideal cyan, magenta and yellow inks (block dyes). Solid lines: Spectral
reﬂectances of real inks (example). The transition wavelenghts of the ideal inks shown in this ﬁgure are
optimized to maximize the gamut volume in CIELAB for the CIE 2◦colorimetric standard observer and
CIED50 illuminant [43]. Please note that ideal CMY-inks produce the larger gamut in terms of gamut volume.
However, this gamut does not necessarily enclose the gamut produced by real CMY-inks [7]. (For interpretation
of the references to color in this ﬁgure legend, the reader is referred to the web version of this book.)
subtractive, GCR can be described by the following formulas [44]
Kt = t · min (C, M, Y),
Ct = C −Kt
1 −Kt
,
Mt = M −Kt
1 −Kt
,
(5.6)
Yt = Y −Kt
1 −Kt
,
where C, M, Y ∈[0, 1] are the control values required to reproduce a given CIELAB color c;
t ∈[0, 1] is the parameter that allows the adjustment of GCR and Ct, Mt, Yt, Kt are the control
values after GCR. Using Eq. (5.6) the set Mc can be parameterized by t, i.e., Mc = {(Ct, Mt, Yt, Kt) |
t ∈[0, 1], P(C0, M0, Y0, 0) = c}. Keeping the parameter t constant for any in-gamut color eliminates
the ambiguity for the separation and yields generally to a smooth S function.
For real printing systems the assumptions required above are invalid and a variation of the parameter
t in Eq. (5.6) results usually in different CIELAB colors. To account for this issue, various GCR
approaches were proposed for real printing systems [45–47].
In addition to solving the ill-posed separation problem of CMYK-printing, GCR has many other
beneﬁcial properties [46]: The ink consumption and, as a consequence, the required drying time can
be reduced which allows higher printing speeds. Instabilities of the printing system that usually affect
the ink balance particularly for grays are less critical for large GCR. Furthermore, the resulting printout
shows an improved color constancy, which is the tendency of a surface color to remain constant when
the illuminant changes [10]. This is especially apparent for grays.
Keeping the percentage of GCR constant (constant t parameter) for any in-gamut color leads to
increased apparent graininess for colors of high lightness and chroma. To reduce this adverse effect on

134
CHAPTER 5 Image Display—Printing (Desktop, Commercial)
print quality, the percentage of GCR may be adjusted to the color space location. A common strategy
is to employ large GCR for dark and achromatic colors and continuously reduce the GCR percentage
for chromatic and light colors. This approach is referred to as under color removal (UCR).
Another way to reduce graininess of the halftone dots, is to employ light inks, such as light cyan,
light magenta or gray [4]. When printed, these inks show a smaller contrast with the typically white
substrate than normal inks. With respect to graininess, they are better suited for reproducing colors
of higher lightness. Usually, transition curves between light and corresponding normal inks are used,
e.g., Minput 
→[Mlight(Minput), M(Minput)] for light and normal magenta. This allows a convenient
CMYK control even though much more inks (light cyan, light magenta, and different grays) are used.
The separation is then similar to a CMYK-based separation with a subsequent insertion of the results
into the transition curves to obtain the ratio between light and corresponding normal inks. Please note
that such transition curves are only meaningful if the corresponding inks printed on the same substrate
show a similar but scaled reﬂectance or at least the same hue [4]. Otherwise, model-based approaches
are proposed for separation, which is more ﬂexible than employing transition curves [48].
One important aspect that needs to be considered is the maximum total ink coverage Tmax of the
substrate. Especially if printing systems with many inks are used, this is a major problem since exceeding
the maximum ink coverage results in artifacts such as ink bleeding or bronzing. In Eq. (5.5), a constraint
is used to avoid such artifacts. This constraint can further be simpliﬁed if ink limitation is treated as
a subsequent transformation following the separation. In this case the constraints in Eq. (5.5) may
be replaced by C, M, Y, K ∈[0, 1], which allows a simpler algorithm for solving the optimization
problem [49]. The subsequent ink limitation may be performed by multi-linear interpolation of ink-
limited Neugebauer primary ink-combinations as described in Ref. [49].
4.05.3.4 Halftoning
After separation, the image is represented as a multi-band image. Typically each band is encoded by
8 bits and corresponds to one speciﬁc ink employed by the printing system. The 8 bit value corresponds
to the amount (or area coverage) of the ink required to reproduce the color resulting from gamut mapping.
In the last step of the workﬂow the 8 bit values need to be reduced to two levels (0 = no ink, 1 = ink).
This process is called halftoning.
The arrangement of the printed dots is crucial for a pleasing appearance of the printout. For designing
halftoning algorithms, properties of the human visual system and of the printing technology need to be
considered. The human visual system is relatively insensitive to high spatial frequencies [50]. Therefore,
a high visual quality might be obtained by small randomly arranged isolated dots with an area coverage
that corresponds to the local image content. However, not any print technology supports such isolated dot
arrangements. Neglecting the print technology may result in artifacts that in turn drastically decrease the
print quality. Optimal halftoning algorithms arrange the dots to maximize the image quality by utilizing
distinct properties of the human visual system and considering the capabilities of the underlying printing
technology.
In this section, we will introduce the basic concepts of halftoning and sketch some algorithms.
Adetaileddescriptionofthiscomprehensivetopicgoesfarbeyondthescopeofthisarticle.Theinterested
reader is referred to the books of Ulichney [51], Kang [52], or Lau and Arce [53].

4.05.3 Workﬂow
135
Halftoning techniques can be classiﬁed into three categories: amplitude-modulated (AM) halftones,
frequency-modulated (FM) halftones and AM-FM hybrid halftones. Figure 5.15 illustrate the difference
between the three categories.
AM halftones are characterized by dot clusters of different size but similar distance as shown in
Figure 5.15. To compute the dot clusters an n × m array of thresholds, sometimes called dither matrix,
is tiled over the image separating it into multiple halftone cells (see Figure 5.16). Thus, each pixel
of the image corresponds to a threshold value. If the threshold value is larger than the pixel value
(or band value for a multi-color image) the resulting binary pixel is set to one, otherwise to zero. The
shape of the clusters is characterized by a spot function that is used to compute the thresholds within
AM
FM
AM-FM Hybrid
FIGURE 5.15
Categories of halftones. AM halftoning produces dot clusters with different size but similar distance. FM
halftoning varies the spacing between similar sized dots. AM-FM hybrid halftoning varies spacing and size.
Original
AM Halftone
FIGURE 5.16
Original and corresponding AM halftone. The dither array is tiled over the image as shown on the left. The
pixel values of each sub-image are compared with the corresponding thresholds within the dither matrix
resulting in the halftone image on the right (round cluster shape).

136
CHAPTER 5 Image Display—Printing (Desktop, Commercial)
3 
10 
19 
28 
27 
16 
7 
1
11 
31 
38 
46 
44 
36 
25 
6
21 
39 
51 
58 
56 
49 
34 
14
30 
47 
59 
63 
61 
54 
42 
22
29 
45 
57 
62 
60 
53 
40 
20
17 
37 
50 
55 
52 
48 
33 
12
9 
26 
35 
43 
41 
32 
18 
4
2 
8 
15 
24 
23 
13 
5 
0
3 
11 
23 
39 
28 
15 
6 
1
10 
22 
38 
51 
43 
29 
16 
7
21 
37 
50 
59 
54 
44 
30 
17
36 
49 
58 
63 
61 
55 
45 
31
32 
46 
56 
62 
60 
53 
42 
27
18 
33 
47 
57 
52 
41 
26 
14
8 
19 
34 
48 
40 
25 
13 
5
2 
9 
20 
35 
24 
12 
4 
0
Dither Matrix (Round Cluster Shape)
Dither Matrix (Diamond Cluster Shape)
FIGURE 5.17
8 × 8 dither matrices (64 levels).
the dither matrix. Frequently used spot functions are based on p-norms, e.g., Euclidean norm (round
cluster shape) or the Manhattan norm (diamond cluster shape). Cost values are computed by the spot
function for each matrix entry and converted to thresholds. The thresholds are scaled to have a uniform
distribution between zero and the number of considered levels (typically 256 for an 8 bit encoding).
Figure 5.17 shows two examples of threshold matrices for 64 levels.
The resolution of a printing system that uses AM halftones is measured by lines per inch (LPI). LPI
is the so called screen frequency that is deﬁned by the number of halftone cells per inch. High resolution
printing systems have a resolution of more than 150 LPI. Typical values for offset printing are between
85–185 LPI. The resolution range of common laser printers is 65–105 LPI.
The human visual system is least sensitive to directional distortions oriented along the 45◦diagonal
[54]. Therefore, monochromatic screens should possess an angle of 45◦. Figure 5.18 illustrates the
deﬁnition of the screen angle. For multi-color printing, halftones are generated for each ink separately
and superimposed on the substrate. There are three ways to create and superimpose such AM screens:
dot-on-dot, dot-off-dot, or rotated dots. For dot-on-dot and dot-off-dot printing, the screens have all the
same angle. The only difference is that in the ﬁrst case dots are printed on top of each other and in the
latter case the screens have a ﬁxed offset that allows a minimal overlap of the dots. Both concepts are very
sensitive to misregistration that might create low-frequency patterns called color banding [9]. Rotated
dots are less sensitive to misregistration but exhibit undesired moiré patterns as shown in Figure 5.19.
For CMYK printing, screen angles that minimize moiré are 15◦for cyan, 75◦for magenta, 0◦for yellow,
and 45◦for black [7]. These angles produce a visually pleasant rosetté pattern as shown in Figure 5.20.
Many algorithms were developed to produce AM halftone screens. Holladay [55,56] patented a
highly efﬁcient method to generate halftone screens of any angle possessing a rational tangent, i.e., the
screen angle φ can be written as φ = arctan a/b, where a and b are integers. In the case of rational
tangent screens, the cells tiled over the image are similar in shape and size. Unfortunately, this is not
possible for irrational tangent screens, which makes their computation much more expensive. Such
screen angles are not uncommon, e.g., the cyan angle to minimize moiré in CMYK printing is φ = 15◦,
which is an irrational tangent screen because tan(15◦) = 2 −
√
3. For calculating irrational tangent
screens in a computational efﬁcient way, so called super cells are used. A super cell consists of M × M

4.05.3 Workﬂow
137
Screen
Angle
Halftone Cell
Inverse Screen Frequency 
FIGURE 5.18
AM halftone screen.
5 Degrees
10 Degrees
15 Degrees
30 Degrees
FIGURE 5.19
Moiré patterns created by superimposing a magenta and a black screen of different screen angles. The degree
speciﬁcation indicates the screen angle difference.
rational tangent cells of different shape and size for approximating the desired irrational tangent screen.
More details can be found, for instance, in Refs. [9,53].
Frequency modulated (FM) halftoning is used to minimize the visibility of dots in halftones. It
allows higher apparent resolution of the printout but requires printing systems that are able to consis-
tently print isolated dots, such as inkjet printers. One category of methods developed for FM halftoning
is based on dither matrices that are precomputed, tiled over the image and compared pixel-wise with

138
CHAPTER 5 Image Display—Printing (Desktop, Commercial)
FIGURE 5.20
Superimposed CMYK rotated AM screens produce a rosetté pattern. (For interpretation of the references to
color in this ﬁgure legend, the reader is referred to the web version of this book.)
the separation bands similarly to AM dithering. This point process allows a very fast computation of
halftones for a given dither matrix. Bayer proposed a recursive algorithm to generate dither matrices
for FM halftones [57]:
B1 = [0],
(5.7)
B2k =
	
4Bk
4Bk + 3
4Bk + 2 4Bk + 1

.
(5.8)
After the matrix of desired size is computed, the matrix entries bi j, i, j = 1, . . . , k, need to be adjusted
to specify evenly spaced thresholds between 0 and 255 (for an 8 bit encoding) as follows:
ti j = 255bi j + 0.5
k2
,
(5.9)
where ti j is the threshold entry at index position (i, j). In contrast to AM dither halftoning, the resulting
dots within a cell are not clustered but dispersed. This halftoning method is optimal in the sense that
the lowest spatial frequency required to reproduce a given gray level is maximized. A drawback of
Bayer’s method is that low frequency components given by the screen period are visible and result in
characteristic cross-hatch pattern artifacts (see Figure 5.21a). By discrete rotation of the dither arrays
or by modifying the dither matrices this effect can be reduced [58,59]. This type of FM halftoning as
well as the clustered dot AM halftoning is referred to as ordered dither halftoning.

4.05.3 Workﬂow
139
(a) Ordered Dispersed Dither
(b) Blue-Noise Error Diffusion
(c) Blue-Noise Error Diffusion 
with Edge Preserving 
(d) Green-Noise Error Diffusion
FIGURE 5.21
Examples of FM and AM-FM hybrid halftones. (a) Bayer’s ordered dispersed dither [57]. (b) Floyd and
Steinberg’s blue-noise error-diffusion [60]. (c) Jarvis et al. blue-noise error diffusion [61] with edge
preserving [62]. (d) Green-Noise error-diffusion [63]. The images are generated using the halftoning toolbox
for matlab [64].

140
CHAPTER 5 Image Display—Printing (Desktop, Commercial)
Another category of FM methods is error-diffusion. Error-diffusion methods mostly yield to visually
more pleasing results than dispersed ordered dither algorithms. The idea is to move the quantization
error between halftone and original image to higher spatial frequencies by diffusing it over a large
image area. The resulting power spectra exhibit strong high frequency components, which is why error-
diffusion halftoning belongs to the so-called blue-noise halftoning. The term blue refers to blue light
whose spectrum exhibits dominantly high frequency components.
In contrast to a point process where a pixel is compared to a threshold independently from its
neighborhood, error-diffusion is an adaptive technique considering neighbor pixels. Therefore, it is
referred to as a neighborhood process. Error-diffusion was pioneered by Floyd and Steinberg [60].
Their algorithm scans the image line by line, thresholds each pixel against mid-gray, computes the
resulting quantization error and distributes it by adding fractions of the error to the gray values of its
four neighbor pixels using the following scheme:
×
7
16
3
16
5
16
1
16
where × represents the position of the current pixel. Figure 5.21b shows a halftone produced by Floyd
and Steinberg’s method. Diffusion schemes considering more neighboring pixels were also proposed
[61]. Even though Floyd and Steinberg’s error-diffusion algorithm does not produce periodic artifacts,
other apparent distortions are created, such as sharpening or worm artifacts. Many attempts were made
to improve the perceived quality in error-diffusion, e.g., by modulating thresholds to enhance edges
[62,65] (see Figure 5.21c). Modern error-diffusion methods produce visually pleasing halftones that
are mostly free of artifacts. A survey of methods is given in Ref. [66].
In order to combine the computational beneﬁts of a point process with the visual quality of error-
diffusion, so-called blue-noise masks were proposed [67]. Halftoning using blue-noise masks produces a
dot arrangement with blue-noise characteristic similar to error-diffusion. A blue-noise mask is a dither
array that is used similarly as in ordered dither halftoning. However, it is usually much larger (e.g.,
256 × 256) than an array employed by ordered dithering (e.g., 16 × 16). Furthermore, it must possess
the wrap-around property to avoid visual artifacts on the tiling edges. Algorithms used to compute the
dither array often include cost functions based on properties of the human visual system, e.g., the contrast
sensitivity [68]. An overview of methods to produce blue-noise masks is given by Spaulding et al. [69].
As already mentioned, FM halftoning is not suitable for printing systems that are unable to con-
sistently produce isolated dots, such as laser printers. For such printing systems a better suited dot
arrangement may be produced by AM-FM hybrid halftoning that minimizes error-visibility and whose
coarseness is adjusted to the abilities of the printing system. Analyzing the power spectra of typical
AM-FM hybrid halftones show that mid-frequency components are dominant. Hence, such halftoning
is referred to as green-noise halftoning [70]. One of the ﬁrst methods to produce green-noise halftones
was proposed by Levien [63] who added output-dependent feedback to error-diffusion, which allows
a tuning of dot cluster sizes. Figure 5.21d shows an example of green-noise halftoning. A comparison
between blue- and green-noise halftoning is given by Lau et al. [71]. Similar to the concept of blue-noise
masks, green-noise masks allow halftoning by a point process that is able to reduce the computational
time drastically [72].

4.05.4 Printer Models
141
Since blue and green-noise halftones are not periodic, color printing can be performed by superim-
posing separately computed halftones for each separation band without showing the adverse effect of
periodic moiré patterns. However, low-frequency graininess might be observed, which is referred to
as stochastic moiré. Stochastic moiré is maximized for uncorrelated halftones with the same principal
wavelength [73]. For green-noise halftoning, stochastic moiré can be minimized by using halftones
possessing a different level of coarseness for each separation band [71]. Lau et al. proposed green-noise
masks that are designed in this way to minimize stochastic moiré [74].
In addition to point processes and neighborhood processes, iterative approaches are a third method-
ology to produce halftones. Iterative methods are generally computationally much more expensive than
algorithms belonging to the other two groups. However, they have the potential to generate halftones of
superior quality optimally adjusted to given printing systems. The main idea is to use a perception-based
error measure as an objective function and to iteratively adjust the halftone to minimize this error subject
to constraints of the printing system. In order to compute the perceived difference between the input
image and an halftone image, error measures must simulate the dot-blurring by our visual system. This
is typically performed by convolving the halftone pattern with the human’s contrast sensitivity function
considering speciﬁc printer dot proﬁles. Please note that various contrast sensitivity functions can be
found in literature having a different performance if used to optimize halftones [75].
An iterative halftoning method is direct binary search proposed by Analoui and Allebach for gray-
scale images [76] and extended by Agar and Allebach to color images [77]. The method starts with
an initial halftone image produced by a point or neighborhood process and computes the perceived
error between this image and the input image. Than it scans the halftone image multiple times. In each
of these iterations every pixel of the image is toggled or swaped with its eight neighbors in order to
minimize the perception-based error measure. In the case the error measure accurately predicts human
perception, the iteration converges to a halftone image that is visually closer to the input image.
Since iterative methods are computationally expensive, they are likely not implemented in current
printers. However, they can be used to optimize point or neighborhood processes, e.g., dither arrays [78].
4.05.4 Printer models
A printer model is a transformation from the printer’s control value space into a color or reﬂectance
space predicting the output of the device on a particular medium. In the following text we only discuss
so-called spectral printer models, which are printer models that predict reﬂectances. Hence, the optical
characteristics of the printout are separated from any further signal processing by the human visual
system. If desired, color attributes can be calculated from reﬂectances for a variety of viewing conditions
using CIELAB or the CIECAM02 [79] model. It is worth mentioning that spectral printer models predict
reﬂectances for one speciﬁc measurement geometry, typically for 45◦/0◦.
Spectral reﬂectances are usually sampled equidistantly in the visible wavelength range from 380 nm
to 730 nm and described by N-dimensional vectors. In general, reﬂectance spectra of printouts do
not show high frequency components and a sampling interval of 10 nm is sufﬁcient for color related
applications [80]. Most spectrophotometers used in the graphic art industry provide measurements as
36-dimensional vectors describing an equidistant sampling from 380 nm to 730 nm.
There are two major effects that need to be considered for modeling printed reﬂectance: mechanical
and optical dot gain.

142
CHAPTER 5 Image Display—Printing (Desktop, Commercial)
(a)
(b)
FIGURE 5.22
White-light interferometric topography measurements of (a) a single dot and (b) a screen. (Source: IDD,
TU-Darmstadt.) (For interpretation of the references to color in this ﬁgure legend, the reader is referred to
the web version of this book.)
4.05.4.1 Mechanical dot gain
Mechanical dot gain is referred to the increase of lateral dot size due to mechanical impact causing the
ink to spread on the substrate. As a result, the area on the paper covered by ink is larger than the area
deﬁned by the halftone screen. Figure 5.22 shows the topography of a printed dot and a dot screen. The
non-uniform boundaries and slopes of the dots indicate ink spread. Emmel and Hersch investigated ink
spread that leads to mechanical dot gain and proposed models to describe it [81,82].
All printing technologies exhibit mechanical dot gain. The extent of this effect depends strongly on
the rheology of the ink and the absorbency of the paper. In relief printing, e.g., ﬂexographic printing, the
dot fringe effect yields to additional mechanical dot gain: When the printing plate comes in contact with
the substrate, raised areas may squeeze some ink to the spare space creating a halo of ink around the dots.
4.05.4.2 Optical dot gain
Optical dot gain (also called the Yule-Nielsen effect [83]) is referred to the increase of observed lateral
dot size due to subsurface scatter of light. Photons might enter the print at an area covered by ink,
scatter within the substrate, and be emitted from the print on an uncovered area and vice versa. This
phenomenon causes the ink dots appear larger than the area effectively covered by ink. The subsurface
scatter of light within the substrate is shown in Figure 5.23 and varies among printing substrates [84,85].
The impact of optical dot gain on the light that is back-emitted from the printed area depends highly
on the pitch of the halftone pattern in comparison to the mean distance of light scatter. Viggiano [86]
distinguishes three cases: (1) complete—the mean distance of light scatter is larger than the halftone
pitch, (2) not signiﬁcant—the mean distance of light scatter is small in comparison with the smallest
halftone dot, and (3) incomplete—the mean distance of light scatter is between the size of the smallest
dot and the pitch of the halftone pattern.

4.05.4 Printer Models
143
(a)
(b)
0.9 mm
0.9 mm
FIGURE 5.23
Illustration of subsurface scattering of light in paper. In both images only the right part of the area is
illuminated. (a) First surface mirror with negligible subsurface scattering. Nearly no light is emitted from
the left part of the area. (b) Inkjet paper with high subsurface scattering. Light scatters from the illuminated
right part of the area into the left part resulting in a distinct blurring of the edge. Both images are captured
by the setup presented in [87].
A common but simpliﬁed approach for modeling optical dot gain separates the ink layer from the
substrate layer. Light scatters only within the substrate. The transmittance of the ink layer is modeled for
two angles of incident light by spatially varying functions T 45◦(x, y) and T0◦(x, y). The point spread
function (PSF) of light in the substrate layer is denoted by P(x, y). The resulting reﬂectance R(x, y)
is then given by
R(x, y) =

T45◦(x, y) ∗P(x, y)

· T0◦(x, y),
(5.10)
where ∗is the convolution operator. Figure 5.24 explains the model.
This concept can be extended to multiple inks by calculating transition probabilities for photons that
enter an area covered by one ink and leave the print at an area covered by another ink or any possible
overprint combination of the employed inks [88–91].
VariousfunctionswereproposedtomodelthePSFoflightinpaper,suchasGaussian[92],exponential
[93,94], or rational functions [92]. Also methods were proposed to directly measure the light’s point
spread function within the substrate [87,95–97].
4.05.4.3 Murray-Davies model
The Murray-Davies model [98] in its spectral form is designed to predict the reﬂectance of printouts
utilizing a single ink. Only the spectral reﬂectance of the substrate RS and of the area completely covered
by ink RI is required. The reﬂectance is predicted by
R(a) = (1 −ae(a))RS + ae(a)RI,
(5.11)

144
CHAPTER 5 Image Display—Printing (Desktop, Commercial)
T45°
T0°
I1
I2
I3
I1
I1
I1
I2
I2
I2
I3
I3
I3
light scattered
incompletely
light scattered
insignifcantly
light scattered
 completely
FIGURE 5.24
Concept of modeling optical dot gain: I1 is the light that is transmitted by the ink layer and penetrates into
the substrate. The light that is back-emitted from the substrate after scattering is denoted by I2. This light
passes the ink layer again yielding to I3. The images shown below illustrate the spatial light distribution for
the not signiﬁcant, incomplete and complete scattering case. (Source: PhD thesis Happel [99].)

4.05.4 Printer Models
145
where a is the theoretical area coverage of the ink, i.e., the area coverage deﬁned by the halftone screen,
and ae(a) is the effective area coverage on the paper as a result of mechanical dot gain. The model
predicts the overall reﬂectance as a weighted average of the reﬂectances RS and RI based on the area
fractions covered and not covered by ink, i.e., ae(a) and 1 −ae(a). The transformation ae(a) that maps
theoretical to effective area coverages is usually approximated by printing and spectrally measuring
a ramp of a few patches 0 = a0, . . . , ak = 100% and deducing ae(ai), i = 0, . . ., k from the model
by inserting the corresponding measurements Ri, i = 0, . . ., k into Eq. (5.11) [100,101]. Intermediate
values are obtained by interpolation. Please note that such calculation is biased by optical dot gain that
is not considered by the Murray-Davies model.
4.05.4.4 Neugebauer model
Neugebauer extended the Murray-Davies model to multi-ink printing [102]. The Neugebauer model con-
siders the reﬂectances of all combinations of overprints, referred to as Neugebauer primaries. A printer
utilizing m inks has 2m Neugebauer primaries. For a CMY-printer these are (1) no ink (paper white),
(2) cyan, (3) magenta, (4) yellow, (5) overprint of magenta and yellow (red), 6. overprint of cyan and
yellow (green), (7) overprint of cyan and magenta (blue), and (8) overprint of cyan, magenta, and yellow
(black). Figure 5.25 shows an example of Neugebauer primaries belonging to a real CMY-printer.
If the superposed screens are statistically independent, the Demichel equations [103,104] can be
used to calculate the fraction of the covered area belonging to each Neugebauer primary from the area
covered by each ink. Statistical independence of two screens means that the probability that a position
on the paper is covered by both inks is the product of the individual probabilities that each ink covers
this position. For a CMY-printer the Demichel equations have the following form:
No ink d0(c, m, y) = (1 −c)(1 −m)(1 −y),
Cyan d1(c, m, y) = c(1 −m)(1 −y),
Magenta d2(c, m, y) = (1 −c)m(1 −y),
Yellow d3(c, m, y) = (1 −c)(1 −m)y,
(5.12)
Red d4(c, m, y) = (1 −c)my,
Green d5(c, m, y) = c(1 −m)y,
Blue d6(c, m, y) = cm(1 −y),
Black d7(c, m, y) = cmy.
Assuming superposed screens that satisfy the independence conditions the Neugebauer model for a
CMY-printer may be written as:
R(c, m, y) =
7

i=0
di(c, m, y) · Ri,
(5.13)
where Ri, i = 0, . . . , 7 are the reﬂectances belonging to each Neugebauer primary and c = c(C),
m = m(M), and y = y(Y) are the effective area coverages corresponding to the theoretical area
coverages C, M, and Y. The functions transforming theoretical into effective area coverages are often
approximated as described for the Murray-Davies model.

146
CHAPTER 5 Image Display—Printing (Desktop, Commercial)
400
550
700
0
1
wavelength [nm]
Reflectance
Cyan
White
Yellow
Magenta
Red
Green
Black
Blue
FIGURE 5.25
Neugebauer primaries of a CMY-printer. (For interpretation of the references to color in this ﬁgure legend,
the reader is referred to the web version of this book.)
The Neugebauer model as shown in Eq. (5.13) can be seen as a multi-linear interpolation using the
Neugebauer primary reﬂectances as nodes. The model can easily be extended to more than three inks.
The Demichel equations for estimating the area coverages of the Neugebauer primaries are usually
valid for random screens that are often used in ink jet printing. Using superposed regular screens with
the same frequency the validity of the Demichel equations depends on the angle between the screens
[105]. For the 3- or 4-screen superpositions that are traditionally used in color printing, the assumptions
to use the Demichel equations in the Neugebauer model do not hold [106]. However, the color prediction
errors of such models applied on these screens are not excessively large [106].
For screens that are far from being statistically independent, e.g., dot-on-dot screens, the Demichel
equations must be replaced by other weights [107,108].
The Neugebauer model does not account for optical dot gain which might result in large prediction
errors. Particularly modern inkjet prints show small halftone pitches in combination with high subsurface
scattering of light within the substrate [96]. Reﬂectances of such printouts cannot be accurately predicted
by the Neugebauer model [109].
4.05.4.5 Yule-Nielsen modiﬁed spectral Neugebauer (YNSN) model
Yule and Nielsen found that the effect of optical dot gain can be well modeled by a slight modiﬁcation
of the Murray-Davies model by utilizing a power law [83,110]. Viggiano transformed the initially
density-based model into its spectral form [111] and extended it to multi-ink printing [112]. This model
is known as the Yule-Nielsen modiﬁed spectral Neugebauer (YNSN) model. For a CMY-printer the
reﬂectance is predicted by the YNSN model as follows:
R(c, m, y) =
 7

i=0
di(c, m, y) · R1/n
i
n
,
(5.14)

4.05.4 Printer Models
147
where the constant n is denoted as the Yule-Nielsen value and exponentiation is performed compo-
nentwisely (for each wavelength). Again, c = c(C), m = m(M), and y = y(Y) are the effective area
coverages corresponding to the theoretical area coverages C, M, and Y. Similar to the basic Neugebauer
model the YNSN model can be expanded to more than three inks.
Please note that the n-value is purely empirical. However, investigations were made to relate the
n-value to screen characteristics and the mean distance of light scatter [92,113,114]. In practice, n is
in the range [1, ∞] [115]. Also reasonable cases are reported where the n-value is negative [115,116].
This can occur if there is intense light scattering within the inks.
To optimally adjust the parameters of the YNSN model non-linear optimization is required to ﬁt the
Yule-Nielsen n-value and obtain the theoretical to effective area coverage functions. It should be noted
that the effects of mechanical and optical dot gain are not separated by the ﬁtted parameters due to the
empirical nature of the YNSN model.
In the following, we denote reﬂectances that are componentwisely (for each wavelength) raised to
the power of 1/n to be in the 1/n-space. Please note that the YNSN model-prediction in 1/n-space is
the Neugebauer model-prediction computed for Neugebauer primaries in 1/n-space.
4.05.4.6 Enhanced YNSN-based models
A straightforward way to increase the prediction accuracy of a YNSN model is to divide the control
value space into smaller subcells and deﬁne in each cell a YNSN model using the cell primaries (vertices
of the cell) instead of the Neugebauer primaries [117]. The resulting model is referred to as the cellular
YNSN model. Since the YNSN model-prediction in 1/n-space is a multi-linear interpolation of the
Neugebauer primaries in 1/n-space, a ﬁner sampling of the colorant space leads to smaller interpolation
errors. However, a larger number of measurements is required for ﬁtting a cellular YNSN model. If
a regular sampling is used with k nodes for each dimension of the control value space a total of km
patches have to be printed and measured, where m is the number of inks.
Strategies to ﬁnd node locations in control value space to increase the prediction performance of the
cellular YNSN model were proposed by different authors [118,119].
Also various approaches were proposed to modify parts of the YNSN model. Iino and Berns replaced
the constant Yule-Nielsen n-value by a wavelength varying nλ-value and observed signiﬁcantly smaller
model errors [47,120]. Even though it can be assumed that the distance that light scatters in paper
depends on the wavelength this approach can be considered as purely empirical.
Balasubramanian replaced the Neugebauer primary reﬂectances by optimized reﬂectances to increase
the prediction accuracy of the YNSN model [121]. The new primary reﬂectances were determined by
weighted least square regression to minimize the model error for a set of printed and measured training
patches.
A recent approach is related to the mechanical dot gain that differs if ink is printed on the substrate
or in superposition with other inks. If theoretical area coverages are mapped to effective area coverages
by functions that depend only on one ink (i.e., c = c(C), m = m(M), y = y(Y) in Eq. (5.14)),
deviations of the mechanical dot gain caused by superposition with other inks are disregarded. Hersch
et al. [122,123] modiﬁed this part of the YNSN model. For a CMY-printer there are 12 cases of an ink
being superposed with other inks. For each of these cases a function deﬁnes the transformation between

148
CHAPTER 5 Image Display—Printing (Desktop, Commercial)
theoretical and effective area coverages:
ac(C)
Cyan superposed with no ink,
am(M)
Magenta superposed with no ink,
ay(Y)
Yellow superposed with no ink,
ac|m(C)
Cyan superposed with solid magenta,
ac|y(C)
Cyan superposed with solid yellow,
am|c(M)
Magenta superposed with solid Cyan,
am|y(M)
Magenta superposed with solid Yellow,
(5.15)
ay|c(Y)
Yellow superposed with solid Cyan,
ay|m(Y)
Yellow superposed with solid Magenta,
ac|my(C)
Cyan superposed with solid Magenta and Yellow,
am|cy(M)
Magenta superposed with solid Cyan and Yellow,
ay|cm(Y)
Yellow superposed with solid Cyan and Magenta.
These functions may be sampled by a few printed and spectrally measured test patches using the YNSN
model. Intermediate values are interpolated.
Assuming the screens are independent of each other the effective area coverage c′, m′, and y′ of the
inks are calculated by the Demichel equations which result in the following equation system
c′ = (1 −m′)(1 −y′)ac(C) + m′(1 −y′)ac|m(C) + (1 −m′)y′ac|y(C) + m′y′ac|my(C),
m′ = (1 −c′)(1 −y′)am(M) + c′(1 −y′)am|c(M) + (1 −c′)y′am|y(M) + c′y′am|cy(M),
y′ = (1 −c′)(1 −m′)ay(Y) + c′(1 −m′)ay|c(Y) + (1 −c′)m′ay|m(Y) + c′m′ay|cm(Y).
(5.16)
For each theoretical area coverages C, M, and Y this system of equations must be solved with respect
to c′, m′, and y′ to obtain a much better representation of the effective area coverages of each ink. The
only modiﬁcation of the YNSN model as shown in Eq. (5.14) is the replacement of c(C), m(M), and
y(Y) by c′, m′, and y′. Hersch and Crété reported a very signiﬁcant improvement of prediction accuracy
especially for inkjet prints. Please note that this approach can be also extended to more than three inks.
A related modiﬁcation of the cellular YNSN model was also proposed [124].
4.05.4.7 Clapper-Yule model
The Clapper-Yule model [125,126] is a physical model taking into account multiple internal reﬂections
at the substrate-air interface. For the sake of simplicity, we describe the model for a printing system
employing only one ink and adopting the notation from [127].
The halftone layer printed on the substrate has an effective area coverage of a. For describing the
model we trace incident light through the air, halftone and substrate layer. Figure 5.26 illustrates the
multiple internal reﬂections considered by the Clapper-Yule model.
A fraction rs of incident light is specularly reﬂected from the substrate’s surface. The remaining
fraction (1 −rs) of light penetrates the halftone layer. From this light, a fraction of a is traversing
the ink with transmittance t and (1 −a) has no contact with the ink. The fraction of light that ﬁnally

4.05.4 Printer Models
149
FIGURE 5.26
Illustration of multiple internal reﬂections considered by the Clapper-Yule model.
penetrates the substrate is (1 −rs)(1 −a + at). A fraction rg of this light is diffusely transmitted
from the substrate towards the halftone layer, where only a fraction of a is traversing the ink a second
time and the remaining fraction (1 −a) of light has no contact with the ink. The fraction of light
reaching the substrate-air interface is (1 −a + at)r1 where r1 = rg(1 −rs)(1 −a + at). From this
light the fraction ri (Fresnel reﬂection) is reﬂected back into the substrate partly traversing the ink
again. The total back-emitted fraction of incident light is ri(1 −a + at2)r1. At the ﬁrst emergence a
fraction of R1 = (1 −ri)(1 −a + at)r1 of the incident light exits the substrate. The light that does not
leave the substrate contributes to the second emergence, and so on. At the n-th emergence a fraction
of R1[rgri(1 −a + at2)]n−1 exists the surface. Adding all the fractions of light that emerge from the
substrate forms a geometric series that converges to the ﬁnal reﬂectance
R(a) = rs + (1 −rs)(1 −ri)rg(1 −a + at)2
1 −rgri(1 −a + at2)
.
(5.17)
Please note that the reﬂectances rs,ri,rg and the transmittance t depend on the wavelength.
This model can simply be extended to more inks using the transmittances of the Neugebauer primaries
and the Demichel equations (see Eq. (5.13)) to obtain the corresponding area coverages. For a CMY-
printer this results in
R(c, m, y) = rs +
(1 −rs)(1 −ri)rg
7
j=0 d j(c, m, y)t j
2
1 −rgri
7
j=0 d j(c, m, y)t2
j
,
(5.18)

150
CHAPTER 5 Image Display—Printing (Desktop, Commercial)
where c, m, y are the effective area coverages, t j, j = 0, . . . , 7 are the transmittances of the Neugebauer
primaries, and d j(c, m, y), j = 0, . . . , 7 are the Demichel equations.
The parameters of the model can be deduced by printing and spectrally measuring some sample
colors [127]. It is worth to mention that various extensions of the model are proposed [122,128].
4.05.4.8 Other models
The models described in the previous section are probably the most popular spectral printer models
used in practice. It is worth to mention that many more models have been proposed so far. Some of
them employ different methodologies or consider additional physical effects.
One of such models was proposed by Emmel and Hersch [81]. They generalized the Kubelka–
Munk turbid-media theory [129] to halftones and show that, for instance, the Clapper-Yule model (see
Section 4.05.4.7) is a particular case of their model.
A nice and nearly comprehensive overview of physical effects as well as spectral printer models is
given by Viggiano [86].
4.05.5 Research directions in printing
4.05.5.1 Spectral printing
Almost all printing workﬂows used today are based on metameric matches between the printout and
the image to be reproduced (see Section 4.05.3). Hence, these workﬂows are referred to as metameric
workﬂows. Since nearly all commercially available cameras or scanners provide RGB images that are
colorimetrically speciﬁed by a standardized RGB color space (e.g., sRGB or Adobe RGB) the spectral
information of the captured scene is already heavily reduced to only three values per pixel. To reproduce
these images for viewing conditions (i.e., observer and illuminant) speciﬁed by the RGB color spaces,
a metameric workﬂow is mostly sufﬁcient. Even an adjustment to a different viewing illuminant is
possible by a foregoing chromatic adaptation transformation [5].
The major problem of a metameric approach is that the viewing illuminant (and observer) needs
to be speciﬁed in advance. However, in reality it is not an exception but the rule that the viewing
illuminant changes frequently (e.g., mixed between daylight and artiﬁcial light). Because of this issue,
some applications require that the match between the printed reproduction and the original image is
invarianttochangesoftheviewingilluminant.Thisrequiresthereproductionofreﬂectancespectrarather
than color space coordinates (i.e., cone photoreceptor responses). Spectral reproduction addresses this
problem.Itisarelativelynewandactiveresearchdirectioninprinting.Applicationsinclude,forinstance,
artwork reproduction [130], industrial color communication and spectral prooﬁng (i.e., simulation of
press-printouts by inexpensive inkjet-printouts).
It is worth mentioning that much research and engineering effort was invested for spectral image
capture in the past decade. Today, spectral cameras and scanners are commercially available and able
to provide accurate spectral image representations of acquired scenes that can be used as inputs for
spectral printing.

4.05.5 Research Directions in Printing
151
4.05.5.1.1
Challenges in spectral printing
The structure of the spectral printing workﬂow is similar to the metameric workﬂow (see Figure 5.9):
spectral gamut mapping followed by spectral separation and halftoning. However, spectral gamut
mapping and spectral separation have different objectives and new algorithms are required to meet
them. The major challenges for developing such algorithms are summarized in the following.
Modeling multi-ink printers: The set of all reﬂectances printable by a printing system on a given
medium is called spectral gamut. For CMYK-printers the effective dimensionality of this set is much
smaller than the dimensionality of natural reﬂectance [131]. This indicates that CMYK-printers are
impractical for spectral printing since most reﬂectances of spectral images are likely to be outside the
spectral gamut. Hence, many more than the CMYK inks have to be used for spectral printing to increase
the spectral gamut of the system (e.g., CMYKRGB). A major problem of using such printing systems is
their spectral modeling. The most accurate printer models are based on many parameters (e.g., cellular
YNSN model) and require a number of printed and measured training colors that is exponentially related
to the number of inks. Models or methodologies need to be developed to reduce the number of training
colors ensuring a high accuracy of spectral predictions.
Spectral gamut mapping: Even with an extended set of inks, spectral gamut mapping cannot be
avoided unless reproductions are restricted to a distinct subset of natural reﬂectances for which appro-
priate inks can be selected [132,133]. Spectral gamut mapping is fundamentally different to metameric
gamut mapping. The main problem is that metrics in spectral space are not well correlated to perceived
differences. Using spectral metrics for spectral gamut mapping might result in reproductions that show
large deviations from the original for any viewing conditions. In order to obtain reproductions that are
as good as metameric reproductions for speciﬁed viewing conditions and for other viewing conditions
superior, properties of human color vision must be incorporated. In addition, the spectral gamut is only
implicitly given by the spectral printer model. New methods are required that allow a quick access to
its boundaries.
Spectral separation: Another challenge is the high dimensionality of the problem. Obviously, the
input image of a spectral workﬂow must be represented in reﬂectance space, i.e., each pixel is an
N-dimensional vector, where usually N = 36 for a typical 10 nm sampling distance in the visible
wavelength range. A spectral printer model that predicts 36-dimensional reﬂectance vectors from high
dimensional control values (e.g., 7 dimensions for a CMYKRGB printer) needs to be inverted for any
pixel of potentially mega-pixel images. The high dimensionality of the inputs is furthermore a major
problem to encode the transformation by multi-dimensional lookup tables.
4.05.5.1.2
Spectral gamut mapping
Spectral metrics [134,135] could be used for selecting in-gamut reﬂectances with a minimal distance
to given reﬂectances not reproducible by the printing system. A problem related to this approach is the
poor correlation of spectral distances to perceived differences especially for dark and chromatic colors.
Resulting reproductions may show large deviations from the original image for any viewing conditions.
To show the beneﬁt of spectral reproduction, it must be as good as a metameric reproducton for
the speciﬁed viewing conditions (illuminant and observer) and better for other viewing conditions. The
approaches explained in the following are designed to meet this objective.
LABPQR spectral gamut mapping [136–138]: The basic idea is to combine metameric and spec-
tral gamut mapping using a hybrid space that has three colorimetric and three spectral dimensions.

152
CHAPTER 5 Image Display—Printing (Desktop, Commercial)
The colorimetric dimensions are adjusted to viewing conditions that are important for the particular
application and are represented in the CIELAB color space. The remaining three dimensions (called
PQR) contain additional information to allow an accurate reconstruction of original reﬂectances.
A discrete formulation of CIE colorimetry is useful to explain the concept. Considering discrete
spectra the integral in Eq. (5.1) turns into a sum which can be written as a matrix-vector multiplication
for equidistant wavelength sampling
q =
⎡
⎣
X
Y
Z
⎤
⎦=
1
N
i=1 l(λi) ¯y(λi)
⎡
⎣
l(λ1)¯x(λ1) · · · l(λN)¯x(λN)
l(λ1) ¯y(λ1) · · · l(λN) ¯y(λN)
l(λ1)¯z(λ1)
· · ·
l(λN)¯z(λN)
⎤
⎦
⎡
⎢⎣
r(λ1)
...
r(λN)
⎤
⎥⎦
(5.19)
= r.
For determining the PQR dimensions, reﬂectances r1, . . . , rk, ri = (ri(λ1), . . . ,ri(λN))T , k ≫N,
are required that should be representative for the images to be reproduced. Corresponding CIEXYZ
values q1, . . . , qk are computed by Eq. (5.19). For any CIEXYZ value q, a reﬂectance r satisfying
q = r can be determined by
r = Hq = SQT (QQT )−1q,
(5.20)
where S = (r1, . . . , rk) and Q = (q1, . . . , qk) are matrices having the representative reﬂectances and
corresponding CIEXYZ values as column vectors. The matrix H is the linear least squares solution of
the equation S = HQ. The matrix S −HQ contains the residuals as column vectors. To determine a
three component basis that optimally represents these residuals, singular value decomposition is used
S −HQ = (I −H)S = VDUT ,
(5.21)
where V is a N ×N dimensional orthogonal matrix, D is a N ×k dimensional diagonal matrix containing
singular values in the diagonal that are sorted in descending order and U is a k×k dimensional orthogonal
matrix. The ﬁrst three columns of V = {v1, . . . , vN} belong to the largest singular values and are used
as the PQR basis. Putting it all together a reﬂectance r is linearly transformed into three colorimetric
dimensions and three spectral dimensions as follows:
r 
→
	
r
VT
3 (I −H)r

,
(5.22)
where V3 = (v1, v2, v3) is a N × 3 matrix consisting of the ﬁrst three columns of V (see Eq. (5.21)).
CIELAB values from the ﬁrst three CIEXYZ components are then computed according to Eq. (5.2).
The transformation from LABPQR back to reﬂectances is deﬁned by the matrices V3 and H as well.
Spectral gamut mapping can be performed by a four step algorithm using the LABPQR space:
1. Transform the spectral image into a LABPQR image.
2. Perform a metameric gamut mapping within the colorimetric dimensions (see Section 4.05.3.2).
PQR values are not modiﬁed.
3. Since a set Mc of ink combinations may produce the same CIEXYZ color c but different reﬂectances,
a nested PQR gamut Pc is deﬁned for every in-gamut color c, i.e., Pc = {VT
3 (I−H)R(v) | v ∈Mc},

4.05.5 Research Directions in Printing
153
where Mc = {v | R(v) = c} and R is a discretized spectral printer model. After the metameric
gamut mapping, every pixel’s colorimetric components are in-gamut. For an image pixel with a
CIEXYZ color c, the PQR components are mapped onto the corresponding nested PQR gamut Pc,
e.g., by minimizing the Euclidean distance in PQR space.
4. The gamut-mapped LABPQR image can be transformed to a spectral image or directly used as an
input for the subsequent separation.
A beneﬁcial property of the LABPQR approach is that it enables a lookup table encoding that can
be embedded into printer proﬁles [41]. This is possible because of the low dimensionality of LABPQR.
Also lookup tables combining gamut mapping and separation can be constructed allowing a quick
spectral prepress workﬂow.
Metamer mismatch-based spectral gamut mapping [139]: A hybrid space for gamut mapping, such
as LABPQR, allows a transformation that is colorimetrically adjusted to only one illuminant. Distances
within the non-colorimetric subspace might be poorly correlated with perceived distances. The resulting
image could show undesired color deviations from the original for other illuminants. The metamer
mismatch-based spectral gamut mapping was developed to account for this shortcoming by performing
the gamut mapping in multiple perceptual color spaces, e.g., CIELAB. Each of these color spaces is
adjusted to a different illuminant and these illuminants need to be sorted with respect to their importance
in the particular application.
For the sake of simplicity, we describe the metamer mismatch-based spectral gamut mapping algo-
rithm for only two illuminants:
1. Transform the spectral image into two CIELAB images associated with the two illuminants.
2. Perform a metameric gamut mapping for the ﬁrst CIELAB image that was rendered for the most
important illuminant (see Section 4.05.3.2).
3. For the ﬁrst illuminant, every in-gamut CIEXYZ color c is associated with a set Mc of ink com-
binations with Mc = {v | 1R(v) = c}, where 1 is the matrix deﬁned in Eq. (5.19) using the
ﬁrst illuminant and R is a discretized spectral printer model. For the second illuminant, the set Mc
deﬁnes a metamer mismatch gamut Gc = {2R(v) | v ∈Mc}, where 2 is the matrix deﬁned
in Eq. (5.19) using the second illuminant. If a pixel was mapped to c for the ﬁrst illuminant the
(spatially) corresponding pixel for the second illuminant must be mapped to a color within Gc. Gc
can be transformed into the CIELAB color space where color difference formulas [39,40] can be
used to pick the color from Gc with the smallest perceived distance to the original.
4. The two gamut-mapped CIELAB images can be directly used for the subsequent separation.
The concept of the metamer mismatch-based gamut mapping can be extended to more illuminants
[139]. Please note in this regard that if two images match for a set of different illuminants then they also
match for any mixture of these illuminants.
Also a modiﬁcation of the method was proposed to improve the colorimetric accuracy under less
important illuminants by allowing colorimetric deviations below the just noticeable threshold for impor-
tant illuminants [140].
A comparison between the LABPQR and the metamer mismatch-based spectral gamut mapping
methods can be found in Ref. [141].

154
CHAPTER 5 Image Display—Printing (Desktop, Commercial)
4.05.5.1.3
Spectral separation
Spectral separation can be written as a constraint optimization problem
minimize
∥R(v) −r∥2
2
(5.23)
subject to
v ∈[0, 1]m,
where r is the in-gamut reﬂectance spectrum to be reproduced, R(v) is the discretized spectral printer
model and v is the m-dimensional vector of control values for the employed m inks. Please note that
the constraints in (5.23) are only applicable if the printer model is adjusted to an ink-limited printing
system [49]. For image separations, the spatial correlation in each separation band has to be similar to
the spatial pixel-correlation of the input image. This agrees with the requirements stated to separations
in metameric workﬂows (see Section 4.05.3.3).
Some attempts were made to solve the constraint optimization problem (5.23) by standard mathe-
matical methods [142]. If properties of the printer models are utilized, simple methods can be developed
as shown for the YNSN model in the following.
The linear regression iteration (LRI) method [143] makes use of the afﬁne multi-linearity of the
YNSN-model in 1/n-space, where n is the Yule-Nielsen value. For reasons of simplicity, we explain
the LRI method applied to a printing system that uses only two inks: cyan c = c(C) and magenta
m = m(M), where C, M are the theoretical area coverages and c, m the effective area coverages. The
concept is similar for printing systems with more inks.
For a CM-printer, the YNSN model in 1/n-space can be rewritten as follows:
R(c, m)1/n = amc + bm
(5.24)
= acm + bc,
(5.25)
where
am = mR1/n
0
−mR1/n
1
−mR1/n
2
+ mR1/n
3
−R1/n
0
+ R1/n
1
,
(5.26)
bm = −mR1/n
0
+ mR1/n
2
+ R1/n
0
,
(5.27)
ac = cR1/n
0
−cR1/n
1
−cR1/n
2
+ cR1/n
3
−R1/n
0
+ R1/n
2
,
(5.28)
bc = −cR1/n
0
+ cR1/n
1
+ R1/n
0
,
(5.29)
and R0, . . . , R3 are the discretized Neugebauer primaries (see Section 4.05.4). Please note that am, bm
are N-dimensional vectors independent of c and ac, bc are N-dimensional vectors independent of m.
Keeping the effective magenta ink coverage m constant, linear regression can be used to ﬁnd the
optimum effective cyan ink coverage copt(m)
copt(m) = argmin
c∈[0,1]
∥R(c, m)1/n −r1/n∥2
2 =
⎧
⎨
⎩
0, cm < 0,
1, cm > 1,
cm, else,
(5.30)
cm = aT
m(r1/n −bm)
aTmam
.
(5.31)

4.05.5 Research Directions in Printing
155
Analogously, an optimum value mopt(c) can be found for constant c. Iterating these linear regressions
yields to the LRI method:
1. (c, m) = (0, 0);
2. REPEAT
3.
c = copt(m);
4.
m = mopt(c);
5. UNTIL TERMINATION
Theiterationconvergestoavalidcontrolvalue(c, m) ∈[0.1]2 thatminimizestheexpression∥R(c, m)1/n
−r1/n∥2
2. For in-gamut reﬂectances r, this solution solves also the problem stated in (5.23). An interest-
ing property of the LRI method is that it is possible to break the linear regression down into two matrix
vector multiplications which allows a very efﬁcient software implementation. The LRI method can
be accelerated by performing the iteration in a vector subspace spanned by the Neugebauer primaries
[144]. It can also be extended to the cellular YNSN model [145].
A different approach is to treat a multi-ink printer (e.g., CMYKRGB) as a combination of multiple
printers each employing a subset of the available inks [146]. Each of these printers can be described by
a lower-dimensional spectral printer model allowing a separation by processing a large number of ink
combinations through the model and selecting the combination that minimizes the objective function
(brute force) [140].
4.05.5.2 Printed reproductions beyond color
The optical properties of any opaque material are described by the bidirectional reﬂectance distribution
function (BRDF) that is the ratio of reﬂected radiance and incident irradiance deﬁned for any angle
of illumination and detection. Reproductions discussed in the previous sections are adjusted to only
one angle of illumination and detection, typically for a 45◦/0◦geometry. This means that for other
viewing/illuminating geometries a match between such reproductions and originals cannot be guaran-
teed. In fact, it is rather unlikely since many materials possess goniochromatic properties that are not
reproducible by conventional inks with almost constant BRDFs. One example are printed reproductions
of wood (e.g., parquet) that have often an unnatural appearance because the color stays constant for
almost all viewing/illuminating geometries.
In the packaging industry, goniochromatic inks, such as metallic or interference-effect inks, are used
as spot colors for many years. Mixing these inks with conventional inks expands the BRDF-gamut [147]
and enables a reproduction that better matches with the original under multiple viewing/illuminating
geometries. For producing such reproductions, the workﬂow shown in Figure 5.9 must be adjusted to
BRDF-data, which requires the development of new gamut mapping, separation and halftoning methods.
The prerequisite of such a BRDF-reproduction workﬂow is a printer model that accurately predicts the
BRDF of the printout. For process control and quality assurance, a perceptual-based distance measures
between BRDFs is required. This research area in printing is currently in its infancy and only a few
articles have been published so far.
Hersch et al. combined metallic and non-metallic inks to produce patterns that match under a non-
specular geometry and highly mismatch under specular reﬂection [127]. First approaches were also
proposed to reproduce spatially-varying BRDFs utilizing metallic inks [148].

156
CHAPTER 5 Image Display—Printing (Desktop, Commercial)
Another research area in printing is watermarking, i.e., hiding information within printed images.
One approach is to use conventional inks and utilize the optical brighteners that are added to the
majority of available paper substrates [149,150]. Since the yellow ink exhibits strong absorption at
short wavelengths it blocks the ﬂuorescent emission caused by the optical brightener. As a result it
appears black under UV-light. If an appropriate halftone is used, information may be hidden under
daylight and becomes visible under UV-light.
Rossier and Hersch proposed to use ﬂuorescent inks and conventional absorption inks [151]. The
patterns were hidden by establishing a metameric match between ﬂueorescent and non-ﬂuorescent inks
under daylight. Under tungsten or UV-light the patterns were revealed.
4.05.5.3 Functional printing
In addition to reproducing image content, printing can also be used as a fabrication process. In such
functional printing applications printing inks consist of functional materials used e.g., as conductors,
semiconductors or isolators. These materials are printed onto the substrate to manufacture electronic
circuits. The resulting printed electronic has the potential to exploit new application areas or replace
establishedtechniques duetoits relativelyinexpensiveproductionprocess. Functional printingis already
used in many devices, such as OLEDs, sensors, solar panels [152]. For manufacturing functional devices
extremely homogeneous and thin layers of ink need to be produced. This requires further development
in printing plate design, inks and substrates. Improvements in the printable spatial resolution and reg-
istration accuracy is particularly important for electronic circuits where multiple layers of ink need to
be printed onto each other.
Glossary
a∗
red-green coordinate of the CIELAB color space
b∗
blue-yellow coordinate of the CIELAB color space
E(·, ·)
color difference formula
di(·)
demichel equation
λ
wavelength
l(λ)
spectral power distribution of the viewing illuminant

visible wavelength range [380 nm,730 nm]
L∗
lightness coordinate of the CIELAB color space
N
dimensionality of a discrete spectrum
P(·)
colorimetric printer model
r(λ)
reﬂectance spectrum
r
discrete reﬂectance spectrum
R(·)
spectral printer model (wavelenght continuous or discretized)
Ri
Neugebauer primary (wavelenght continuous or discretized)
Tmax
maximum total ink coverage
¯x(λ)
color matching function
¯y(λ)
color matching function
¯z(λ)
color matching function

References
157
Relevant theory: Signal Processing Theory and Machine Learning
See Vol. 1, Chapter 1 Introduction: Signal Processing Theory
See Vol. 1, Chapter 5 Sampling and Quantization
See Vol. 1, Chapter 21 Unsupervised learning and latent variable models (PCA, ICA, NMF, ...)
References
[1] H. Kipphan, Handbook of Print Media: Technologies and Production Methods, Springer-Verlag, New York,
Inc, 2001.
[2] Gravure Association of America (GAA) and Gravure Education Foundation (GEF), Gravure–Process and
Technology, second ed., GAA and GEF, Rochester, New York, 2003.
[3] Flexographic Technical Association, Flexography: Principles and Practices, ﬁfth ed., vol. 1–6, Flexographic
Technical Association, Ronkonkoma, NY, 2000.
[4] N. Ohta, M. Rosen, Color Desktop Printer Technology, CRC Press, 2006.
[5] M.D. Fairchild, Color Appearance Models, ﬁrst ed., Addison Wesley Longman, Inc., Massachusetts USA,
1998.
[6] R.W.G. Hunt, Objectives in colour reproduction, J. Phot. Sci 18 (1970) 205–215.
[7] R.W.G. Hunt, The Reproduction of Colour, vol. 1, sixth ed., John Wiley & Sons, Ltd, 2004.
[8] S. Süsstrunk, R. Buckley, S. Swen, Standard RGB color spaces, in: Seventh Color Imaging Conference,
IS&T/SID, Scottsdale Ariz., 1999, pp. 127–134.
[9] G. Sharma, Digital Color Imaging Handbook, ﬁrst ed., CRC Press, USA, 2003.
[10] R.S. Berns, Billmeyer and Saltzman’s: Principles of Color Technology, third ed., John Wiley and Sons, Inc,
New York, 2000.
[11] S.S. Stevens, To honor fechner and repeal his law, Science 133 (3446) (1961) 80–86.
[12] M.D. Fairchild, Color Appearance Models, second ed., John Wiley and Sons, Inc, West Sussex, England,
2005.
[13] R.G. Kuehni, Hue Uniformity and the CIELAB Space and Color Difference Formula, Color Res. Appl. 23
(5) (1998) 314–322.
[14] N. Ohta, A.R. Robertson, Colorimetry: Fundamentals and Applications, vol. 1. Wiley, 2005.
[15] K. Thomsen, A euclidean color space in high agreement with the CIE94 color difference formula, Color Res.
Appl. 25 (2000) 64–65.
[16] P. Urban, D. Schleicher, M.R. Rosen, R.S. Berns, Embedding non-euclidean color spaces into euclidean
color spaces with minimal isometric disagreement, J. Opt. Soc. Am. A 24 (6) (2007) 1516–1528.
[17] I. Lissner, P. Urban, Towards a uniﬁed color space for perception-based image processing, IEEE Trans. Image
Process. 21 (3) (2012) 1153–1168.
[18] G.J. Braun, M.D. Fairchild, Techniques for gamut surface deﬁnition and visualisation, in: IS&T/SID,
Scottsdale Ariz., 1997, pp. 147–152.
[19] T.J. Cholewo, S. Love, Gamut boundary determination using alpha-shapes, in: IS&T/SID, Scottsdale Ariz.,
1999, pp. 200–204.
[20] J. Morovic, M.R. Luo, Calculating medium and image gamut boundaries for gamut mapping, Color Res.
Appl. 25 (2000) 394–401.
[21] P.G. Herzog, Analytical color gamut representations, J. Imag. Sci. Technol. 40 (1996) 516–521.
[22] P.G. Herzog, Specifying color gamut boundaries, Electron. Imag. SPIE/IS&T Int. Tech. Group Newslett.
9 (1999) 1796–1806.

158
CHAPTER 5 Image Display—Printing (Desktop, Commercial)
[23] M. Mahy, Gamut calculation of color reproduction devices, in: IS&T/SID, Scottsdale Ariz., 1995,
pp. 145–150.
[24] M. Mahy, Calculation of color gamuts based on the neugebauer model, Color Res. Appl. 22 (1996) 365–374.
[25] M. Mahy, Insight into the solution of the neugebauer model, in: Proceedings of SPIE, 1998, pp. 118–128.
[26] P. Urban, R.-R. Grigat, Gamut boundary determination by sampling an inverse printer model, in: IS&T’s
NIP18, San Diego, Calif., 2002, pp. 778–781.
[27] J. Moroviˇc, Fast computation of multi-primary color gamut, in: 15th Color Imaging Conference, IS&T/SID,
2007, pp. 228–232.
[28] J. Morovic, Color Gamut Mapping, John Wiley and Sons, 2008.
[29] S. Nakauchi, S. Hatanaka, S. Usui, Color gamut mapping based on a perceptual image difference measure,
Color Res. Appl. 24 (4) (1999) 280–291.
[30] R. Kimmel, D. Shaked, M. Elad, I. Sobel, Space-dependent color gamut mapping: a variational approach,
IEEE Trans. Image Process. 14 (6) (2005) 796–803.
[31] F. Ebner, M.D. Fairchild, Development and testing of a color space (IPT) with improved hue uniformity. in:
IS&T/SID, Scottsdale Ariz., 1998, pp. 8–13.
[32] J.J. McCann, Color gamut mapping using spatial comparisons, in: Proceedings of SPIE, Color Imaging:
Device-Independent Color, Color Hardcopy, and Graphic Arts VI, vol. 4300, 2001, pp. 126–130.
[33] J. Morovic, Y. Wang. A multi-resolution full-colour spatial gamut mapping algorithm, in: 11th Color Imaging
Conference, IS&T/SID, Scottsdale Ariz., 2003, pp. 282–287.
[34] R. Bala, R. deQueiroz, R. Eschbach, W. Wu, Gamut mapping to preserve spatial luminance variations,
J. Imag. Sci. Technol. 45 (5) (2001) 436–443.
[35] P. Zolliker, K. Simon, Retaining local image information in gamut mapping algorithms, IEEE Trans. Image
Process. 16 (3) (2007) 664–672.
[36] I. Farup, C. Gatta, A. Rizzi, A multiscale framework for spatial gamut mapping, IEEE Trans. Image Process.
16 (10) (2007) 2423–2435.
[37] N. Bonnier, F. Schmitt, H. Brettel, Evaluation of spatial gamut mapping algorithms, in: 14th Color Imaging
Conference, IS&T/SID, Scottsdale Ariz., 2006, pp. 56–61.
[38] Adobe Systems’ Implementation of Black Point Compensation. <http://www.color.org/AdobeBPC.pdf>
(2012/07/10).
[39] CIE Publication No. 116, Industrial Colour-Difference Evaluation, Technical Report, Central Bureau of the
CIE, Vienna, Austria, 1995.
[40] CIE Publication No. 142, Improvement to Industrial Colour-Difference Evaluation, Technical Report, Central
Bureau of the CIE, Vienna, Austria, 2001.
[41] ICC, File Format for Color Proﬁles, 4.3.0.0 ed., 2010. <http://www.color.org>.
[42] J.P. Van de Capelle, Method and a device for determining multi-ink color separations, October 17 2006, US
Patent 7 123 380.
[43] K. Happel, E. Dörsam, Gamut volume optimization of ideal printing inks, in: 11th Congress of the Interna-
tional Colour Association, Sydney, Australia, 2009.
[44] K. Sayanagi, Black printer, UCR and UCA–gray component replacement, in: TAGA Proceedings, 1987.
pp. 711–724.
[45] J.A.S. Viggiano, Gray component replacement: a practical approach, in: Advanced Printing of Conference
Summaries, SPSE’s 43rd Annual Conference, Springﬁeld, Virginia: Society for Imaging, Science and Tech-
nology, 1990. pp. 204–206.
[46] H.R. Kang, Color Technology for Electronic Imaging Devices, SPIE-International Society for Optical Engi-
neering, 1997.
[47] K. Iino, R.S. Berns, Building color-management modules using linear optimization II, Prepress System for
Offset Printing, J. Imag. Sci. Technol. 42 (1998) 99–114.

References
159
[48] A.U. Agar, Model based color separation for cmykcm printing, in: Nineth Color Imaging Conference,
IS&T/SID, Scottsdale Ariz., 2001, pp. 298–302.
[49] P. Urban, Ink limitation for spectral or color constant printing, in: 11th Congress of the International Colour
Association, Sydney, Australia, 2009.
[50] P.G.J. Barten, Contrast Sensitivity of the Human Eye and Its Effects on Image Quality, SPIE Press, 1999.
[51] R. Ulichney, Digital Halftoning, The MIT Press, 1987.
[52] H. Kang, Digital Color Halftoning, ﬁrst ed., SPIE Optical Engeneering Press, Washington, USA, 1999.
[53] D.L. Lau, G.R. Arce, Modern Digital Halftoning, third ed., CRC Press, Taylor & Francis Group, LLC.
[54] F.W. Campbell, J.J. Kulikowski, J. Levinson, The effect of orientation on the visual resolution of gratings,
J. Phys. 187 (2) (1966) 427–436.
[55] T.M. Holladay, Variable angle electronic halftone screening, April 10 1979, US Patent 4 149 194.
[56] T.M. Holladay, Electronic halftone screening, January 22 1980, US Patent 4 185 304.
[57] B.E. Bayer, An optimum method for two-level rendition of continuous-tone pictures, in: IEEE International
Conference on Communications, Seattle, WA, 1973, pp. 11–15.
[58] V. Ostromoukhov, R.D. Hersch, I. Amidror, Rotated dispersed dither: a new technique for digital halftoning,
in: Proceedings of the 21st annual conference on Computer graphics and interactive techniques (Siggraph),
ACM, 1994, pp. 123–130.
[59] V. Ostromoukhov, R.D. Hersch, Halftoning by rotating nonbayer dispersed dither arrays, in: Proceeding of
SPIE: Human Vision, Visual Processing, and Digital Display VI, vol. 2411, 1995, pp. 180–197.
[60] R.W. Floyd, L. Steinberg, An adaptive algorithm for spatial grey scale, in: Proceedings of the Society of
Information Display, SID, 1976, pp. 75–77.
[61] J.F. Jarvis, C.N. Judice, W.H. Ninke, A survey of techniques for the display of continuous tone pictures on
bilevel displays, Comput. Graph. Image Process. 5 (1) (1976) 13–40.
[62] R. Eschbach, K.T. Knox, Error-diffusion algorithm with edge enhancement, JOSA A 8 (12) (1991)
1844–1850.
[63] R. Levien, Output dependant feedback in error diffusion halftoning, in: Proceeding of IS&Ts 8th International
Congress Advances in Non-Impact Printing Technologies, IS&T, 1992, pp. 280–282.
[64] V. Monga, N. Damera-Venkata, H. Rehman, B.L. Evans, Halftoning Toolbox for Matlab, Version 1.2, 2005.
[65] N. Damera-Venkata, B.L. Evans, Adaptive threshold modulation for error diffusion halftoning, IEEE Trans.
Image Process. 10 (1) (2001) 104–116.
[66] B.L. Evans, V. Monga, N. Damera-Venkata, Variations on error diffusion: retrospectives and future trends,
in: Proceeding of SPIE Color Imaging: Processing, Hardcopy and Applications VIII, vol. 5008, 2003,
pp. 371–389.
[67] T. Mitsa, K.J. Parker, Digital halftoning technique using a blue-noise mask, JOSA A 9 (11) (1992) 1920–1929.
[68] J.R. Sullivan, L.A. Ray, R. Miller, Design of minimum visual modulation halftone patterns, Systems, IEEE
Trans. Man Cyberne. 21 (1) (1991) 33–38.
[69] K.E. Spaulding, R.L. Miller, J.S. Schildkraut, Methods for generating blue-noise dither matrices for digital
halftoning, J. Electron. Imag. 6 (02) (1997) 208–230.
[70] D.L. Lau, G.R. Arce, N.C. Gallagher, Green-noise digital halftoning, Proc. IEEE 86 (12) (1998) 2424–2444.
[71] D.L. Lau, R. Ulichney, G.R. Arce, Blue and green noise halftoning models, IEEE Signal Process. Mag. 20
(4) (2003) 28–38.
[72] D.L. Lau, G.R. Arce, N.C. Gallagher, Digital halftoning by means of green-noise masks, JOSA A 16 (7)
(1999) 1575–1586.
[73] D.L. Lau, A. Khan, G.R. Arce, Stochastic moiré, in: IS&T PICS Conference Society for Imaging, Science
and Technology, 2001, pp. 96–100.
[74] D.L. Lau, A.M. Khan, G.R. Arce, Minimizing stochastic moire in frequency-modulated halftones by means
of green-noise masks, JOSA A 19 (11) (2002) 2203–2217.

160
CHAPTER 5 Image Display—Printing (Desktop, Commercial)
[75] S.H. Kim, J.P. Allebach, Impact of hvs models on model-based halftoning, IEEE Trans. Image Process. 11
(3) (2002) 258–269.
[76] M. Analoui, J.P. Allebach, Model-based halftoning using direct binary search, in: Proc. SPIE 1666 (1992)
96–128.
[77] A.U. Agar, J.P. Allebach, Model-based color halftoning using direct binary search, IEEE Trans. Image
Process. 14 (12) (2005) 1945–1959.
[78] J. Allebach, Q. Lin, Fm screen design using dbs algorithm, in: Proceedings of IEEE International Conference
on Image Processing 1996, vol. 1, 1996, pp. 549–552.
[79] CIE Publication No. 159. A colour appearance model for colour management systems: CIECAM02, CIE
Central Bureau, Vienna, 2004.
[80] H.J. Trussell, M.S. Kulkarni, Sampling and processing of color signals, IEEE Trans. Image Process. 5 (1996)
677–681.
[81] P. Emmel, R.D. Hersch, A uniﬁed model for color prediction of halftoned prints, J. Imag. Sci. Technol. 44
(2000) 351–359.
[82] P. Emmel, R.D. Hersch, Modeling ink spreading for color prediction, J. Imag. Sci. Technol. 46 (3) (2002)
237–246.
[83] J.A.C. Yule, W.J. Nielsen, The penetration of light into paper and its effect on halftone reproduction, in:
Tech. Assn. Graph. Arts 4 (1951) 65–76.
[84] M. Wedin, B. Kruse, Mechanical and optical dot gain in halftone colour prints, IS&T: Recent Progress in
Digital Halftoning II, 1999, pp. 400–403.
[85] K. Happel, P. Urban, E. Dörsam, X. Ludewig, Classifying papers according to their light scatter properties,
in: Midterm Meeting of the International Colour Association (AIC), Zurich, Switzerland, 2011, pp. 138–141.
[86] J.A.S. Viggiano, New Models for the Reﬂectance Spectra Produced by Halftone-based Hardcopy, PhD
Thesis, Center for Imaging Science, Rochester Institute of Technology, Rochester, NY, USA, 2010.
[87] K. Happel, M. Walter, P. Urban, E. Dörsam, Measuring anisotropic light scatter within graphic arts papers
for modeling optical dot gain, in: 18th Color and Imaging Conference, IS&T/SID, San Antonio, Texas, 2010,
pp. 347–352.
[88] J.S. Arney, A Probability Description of the Yule-Nielsen Effect, J. Imag. Sci. Technol. 41 (1997) 633–636.
[89] J.S. Arney, M. Katsube, A Probability description of the Yule-Nielsen effect II: the impact of halftone
geometry, J. Imag. Sci. Technol. 41 (1997) 637–642.
[90] G.L. Rogers, Effect of light scatter on halftone color, JOSA A 15 (7) (1998) 1813–1821.
[91] G.L. Rogers, Optical dot gain: lateral scattering probabilities, J. Imag. Sci. Technol. 42 (4) (1998) 341–345.
[92] F.R. Ruckdeschel, O.G. Hauser, Yule-nielsen effect in printing: a physical analysis, Appl. Opt. 17 (21) 1978
3376–3383.
[93] H. Wakeshima, T. Kunishi, S. Kaneko, Light scattering in paper and its effect on halftone reproduction,
JOSA 58 (2) (1968) 272–273.
[94] P.G. Engeldrum, B. Pridham, Application of turbid medium theory to paper spread function measurements,
in: TAGA, Technical Association of the Graphic Arts, 1995, pp. 339–339.
[95] J.S. Arney, P.G. Engeldrum, C.D. Arney, M. Katsube, A probability description of the Yule-Nielsen effect,
J. Imag. Sci. Technol. 40 (1996) 19–25.
[96] C. Koopipat, N. Tsumura, Y. Miyake, M. Fujino, Effect of ink spread and optical dot gain on the mtf of ink
jet image, J. Imag. Sci. Technol. 46 (4) (2002) 321–325.
[97] M. Ukishima, H. Kaneko, T. Nakaguchi, N. Tsumura, M. Hauta-Kasari, J. Parkkinen, Y. Miyake, A simple
method to measure mtf of paper and its application for dot gain analysis, IEICE Trans. Fundamentals Electron.
Commun. Comput. Sci. 92 (12) (2009) 3328–3335.
[98] A. Murray, Monochrome reproduction in photoengraving, J. Franklin Inst. 221 (1936) 721–744.

References
161
[99] K. Happel. LED-Based Light Scattering Measurements of Papers for Printing Applications, PhD Thesis,
Technische Universität Darmstadt, Germany, 2011.
[100] D.R. Wyble, R.S. Berns, A critical review of spectral models applied to binary color printing, Color Res.
Appl. 25 (1) (2000) 4–19.
[101] D.R. Wyble, R.S. Berns, ERRATA: a critical review of spectral models applied to binary color printing,
Color Res. Appl. 25 (3) (2000) 159.
[102] H.E.J. Neugebauer, Die theoretischen Grundlagen des Mehrfarbenbuchdrucks, Zeitschrft fnr wis-
senschaftliche Photographie, Photophysik und Photochemie (reprint in [153], translation in [154]), 3 (1937)
73–89.
[103] E. Demichel, Le procédé 26 (3) (1924) 17–21.
[104] E. Demichel, Le procédé 26 (4) (1924) 26–27.
[105] G.L. Rogers, Neugebauer revisited: random dots in halftone screening, Color Res. Appl. 23 (2) (1998)
104–113.
[106] I.Amidror,R.D.Hersch,Neugebaueranddemichel:dependenceandindependenceinn-screensuperpositions
for colour printing, Color Res. Appl. 25 (4) (2000) 267–277.
[107] R. Balasubramanian, Printer model for dot-on-dot halftone screens, in: Proc. SPIE 2413 (1995) 356–364.
[108] R.D. Hersch, A.K. Singla, An ink spreading model for dot-on-dot spectral prediction, in: 14th Color Imaging
Conference, IS&T/SID, Scottsdale Ariz., 2006, pp. 38–43.
[109] R. Rolleston, R. Balasubramanian, Accuracy of various types of Neugebauer model, in: IS&T/SID, Scottsdale
Ariz., 1993, pp. 32–36.
[110] J.A.C. Yule, R.S. Colt, Colorimetric investigations in multicolor printing, in: Proceedings of TAGA, 1951,
pp. 77–82.
[111] J.A.S. Viggiano, The color of halftone tints, in: Proceedings of TAGA, 1985, pp. 647–661.
[112] J.A.S. Viggiano, Modeling the color of multi-color halftones, in: Proceedings of TAGA, 1990, pp. 44–62.
[113] J.S. Arney, T. Wu, C. Blehm, Modeling the Yule-Nielsen effect on color halftones, J. Imag. Sci. Technol. 42
(1998) 335–340.
[114] J.S. Arney, S. Yamaguchi, The physics behind the Yule-Nielsen equation, in: PICS: Image Processing, Image
Quality, Image Capture, Systems Conference, 1999, pp. 381–385.
[115] J.A.S. Viggiano, Physical signiﬁcance of negative yule-nielsen n-value, in: Proceedings of the International
Congress of Imaging Science (ICIS), Society for Imaging Science and Technology, Rochester, NY, 2006,
pp. 607–610.
[116] A. Lewandowski, M. Ludl, G. Byrne, G. Dorffner, Applying the yule-nielsen equation with negative n, JOSA
A 23 (8) (2006) 1827–1834.
[117] K.J. Heuberger, Z.M. Jing, S. Persiev, Color transformations from lookup tables, in: Proceedings of TAGA,
1992, pp. 863–881.
[118] A.U. Agar, J.P. Allebach, An iterative cellular ynsn method for color printer characterization, in: IS&T/SID,
Scottsdale Ariz., 1998, pp. 197–200.
[119] Y. Chen, R.S. Berns, L.A. Taplin, Six color printer characterization using an optimized cellular Yule-Nielsen
spectral Neugebauer model, J. Imag. Sci. Technol. 48 (2004) 519–528.
[120] K. Iino, R.S. Berns, Building color-management modules using linear optimization I. desktop color system,
J. Imag. Sci. Technol. 42 (1998) 79–94.
[121] R. Balasubramanian, Optimization of the spectral neugebauer model for printer characterization, J. Electron.
Imag. 8 (1999) 156–166.
[122] R.D. Hersch, P. Emmel, F. Collaud, F. Crété, Spectral reﬂection and dot surface prediction models for color
halftone prints, J. Electron. Imag. 14 (2005) 33001–33012.
[123] R.D. Hersch, F. Crété, Improving the yule-nielsen modiﬁed spectral neugebauer model by dot surface cover-
ages depending on the ink superposition conditions, in: Proceedings of SPIE, vol. 5667, 2005, pp. 434–445.

162
CHAPTER 5 Image Display—Printing (Desktop, Commercial)
[124] R.D. Hersch, R. Rossier, Introducing ink spreading within the cellular Yule-Nielsen modiﬁed Neugebauer
model, in: 18th Color Imaging Conference, IS&T/SID, San Antonio, Texas, 2010, pp. 295–300.
[125] F.R. Clapper, J.A.C. Yule, The effect of multiple internal reﬂections on the densities of half-tone prints on
paper, JOSA 43 (7) (1953) 600–603.
[126] F.R. Clapper, J.A.C. Yule, Reproduction of color with halftone images, in: Proceedings of Seventh Annual
Technical Meeting TAGA, 1955, pp. 1–14.
[127] R.D. Hersch, F. Collaud, P. Emmel, Reproducing color images with embedded metallic patterns, in: ACM
Transactions on Graphics (TOG), vol. 22, ACM, 2003, pp. 427–434.
[128] G. Rogers, A generalized clapper–yule model of halftone reﬂectance, Color Res. Appl. 25 (6) (2000)
402–407.
[129] P. Kubelka, F. Munk, Ein beitrag zur optik der farbanstriche, Zeitschrift fnr Technische Physik 12 (1931)
593–601.
[130] R.S. Berns, L. Taplin, P. Urban, Y. Zhao, Spectral color reproduction of paintings, in: CGIV, Barcelona,
Spain, 2008, pp. 484–488.
[131] J.Y. Hardeberg, On the spectral dimensionality of object colours, in: CGIV, Poitiers, France, IS&T, 2002,
pp. 480–485.
[132] D.-Y. Tzeng, R.S. Berns, Spectral-based ink selection for multiple-ink printing I. colorant estimation of
original objects, in: IS&T/SID, Scottsdale Ariz., 1998, pp. 106–111.
[133] D.-Y. Tzeng, R.S. Berns, Spectral-based ink selection for multiple-ink printing II. optimal ink selection, in:
IS&T/SID, Scottsdale Ariz., 1999, pp. 182–187.
[134] F.H. Imai, M.R. Rosen, R.S. Berns, Comparative study of metrics for spectral match quality, in: CGIV,
Poitiers, France, IS&T, 2002, pp. 492–496.
[135] J.A.S. Viggiano, Metrics for evaluating spectral matches: a quantitative comparison, in: CGIV, Aachen,
Germany, IS&T, 2004, pp. 286–291.
[136] M.R. Rosen, M.W. Derhak, Spectral gamuts and spectral gamut mapping, in: Spectral Imaging: Eighth
International Symposium on Multispectral Color Science, SPIE, San Jose, CA, 2006.
[137] M.W. Derhak, M.R. Rosen, Spectral colorimetry using LabPQR—an interim connection space, J. Imag. Sci.
Technol. 50 (2006) 53–63.
[138] S. Tsutsumi, M.R. Rosen, R.S. Berns, Spectral gamut mapping using LabPQR, J. Imag. Sci. Technol. 51 (6)
(2007) 473–485.
[139] P. Urban, M.R. Rosen, R.S. Berns, Spectral gamut mapping framework based on human color vision, in:
CGIV, Barcelona, Spain, 2008, pp. 548–553.
[140] P. Urban, R.S. Berns, Paramer mismatch-based spectral gamut mapping, IEEE Trans. Image Process. 20 (6)
(2011) 1599–1610.
[141] M.W. Derhak, R.S. Berns, Comparing LabPQR and the spectral gamut mapping framework, in: 18th Color
Imaging Conference, IS&T/SID, San Antonio, Texas, 2010, pp. 206–212.
[142] S. Zufﬁ, R. Schettini, Spectral-based printer characterization, in: CGIV, Poitiers, France, IS&T, 2002,
pp. 598–602.
[143] P. Urban, R.-R. Grigat, Spectral-based color separation using linear regression iteration, Color Res. Appl.
31 (2006) 229–238.
[144] P. Urban, M.R. Rosen, R.S. Berns, Accelerating spectral-based color separation within the Neugebauer
subspace, J. Electron. Imag. 16 (2007) 043014.
[145] P. Urban, M.R. Rosen, R.S. Berns, Fast spectral-based separation of multispectral images, in: 15th Color
Imaging Conference, IS&T/SID, Albuquerque, New Mexico, 2007, pp. 178–183.
[146] D.-Y. Tzeng, R.S. Berns, Spectral-based six-color separation minimizing metamerism, in: IS&T/SID, Scotts-
dale Ariz., 2000, pp. 342–347.

References
163
[147] K. Kehren, P. Urban, E. Dörsam, Bidirectional reﬂectance and texture database of printed special effect
colors, in: 19th Color and Imaging Conference, IS&T/SID, San Jose, CA, 2011, pp. 316–321.
[148] W. Matusik, B. Ajdin, J. Gu, J. Lawrence, H. Lensch, F. Pellacini, S. Rusinkiewicz, Printing spatially-varying
reﬂectance, in: ACM Transactions on Graphics (TOG), vol. 28, ACM, 2009, p. 128.
[149] R. Bala, R. Eschbach, Y. Zhao, Substrate ﬂuorescence: bane or boon, in: 15th Color Imaging Conference:
Color, Science, Systems and Applications, Albuquerque, New Mexico, 2007, pp. 12–17.
[150] Y. Zhao, R. Bala, Printer characterization for uv encryption applications, in: 16th Color Imaging Conference,
IS&T/SID, Portland, Oregon, 2008, pp. 79–83.
[151] R. Rossier, R.D. Hersch, Hiding patterns with daylight ﬂuorescent inks, in: 19th Color and Imaging Confer-
ence, IS&T/SID, San Jose, CA, 2011, pp. 223–228.
[152] V. Subramanian, J.B. Chang, A. de la Fuente Vornbrock, D.C. Huang, L. Jagannathan, F. Liao, B. Mattis,
S. Molesa, D.R. Redinger, D. Soltman, S.K. Volkman, Q. Zhang, Printed electronics for low-cost electronic
systems: technology status and application development. in: 34th European, Solid-State Circuits Conference,
2008, ESSCIRC 2008, IEEE, 2008, pp. 17–24.
[153] H. E. J. Neugebauer, Die theoretischen Grundlagen des Mehrfarbenbuchdrucks, in: Proceedings of SPIE:
Neugebauer Memorial Seminar on Color Reproduction, vol. 1184, Tokyo, Japan, 1989, pp. 194–202.
[154] H. E. J. Neugebauer, The theoretical basis of multicolor letterpress printing (Translated D. Wyble,
A. Kraushaar), Color Res. Appl. 30 (2005) 322 331.

6
CHAPTER
Image Restoration: Fundamentals
of Image Restoration
Stanley J. Reeves
Electrical and Computer Engineering Department, Auburn University, Auburn, AL, USA
4.06.1 Introduction
An image is acquired in order to obtain a two-dimensional (2-D) representation of a three-dimensional
(3-D) scene. Unfortunately, many images represent scenes in an unsatisfactory manner. Because physical
imaging systems are imperfect and imaging conditions are frequently less than ideal, a recorded image
often represents a degraded version of the ideal 2-D mapping of the scene. The imaging process, the
atmosphere, and the recording medium all introduce degradations into the captured image so that the
image that is actually recorded often fails to represent the scene adequately. Image restoration addresses
the problem of unsatisfactory scene representation. The goal of image restoration is to manipulate an
image in such a way that it will in some sense more closely depict the scene that it represents.
The image restoration problem appears in many ﬁelds. Virtually all disciplines in which images are
acquired under less-than-ideal conditions ﬁnd restoration techniques useful—astronomy [1,2], medicine
[3,4], forensics [5], and military reconnaissance [6,7], for example. Photo printing labs may also ﬁnd
restoration techniques a viable tool in touching up special photographs [8]. An example of image
restoration in astronomy is shown in Figure 6.1. The blurred images are two different views of Jupiter
taken in the early 1990s from the uncorrected Hubble Space Telescope, which had a design ﬂaw in
the main mirror [9]. These images were restored using the Richardson-Lucy restoration algorithm, an
iterative method that has found wide use in astronomy [10,11].
In addition to these applications, image restoration has captured the attention of researchers because
it is a visually oriented, relatively easily understood form of an inverse problem. Inverse problems arise
in an array of scientiﬁc, medical, industrial and theoretical problems. The restoration of images has
become a popular way to visualize and test out more general solutions to such inverse problems.
4.06.2 Observation model
The image formation process represents a 2-D mapping of a 3-D scene. Figure 6.2 shows a schematic rep-
resentationofthisprocess.Thesystemthatformstheimage—theopticsandthesensor,forinstance—may
introduce distortions into the mapping process. Furthermore, the process of recording the 2-D mapping,
such as relative motion or defocusing, may introduce distortions as well. These distortions fall into two
general categories: deterministic and stochastic. Deterministic distortion can be further divided into a
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00006-6
© 2014 Elsevier Ltd. All rights reserved.
165

166
CHAPTER 6 Image Restoration: Fundamentals of Image Restoration
FIGURE 6.1
Two views of Jupiter taken from the Hubble Space Telescope with ﬂawed mirror (top) and restored images
(bottom).
blurring process and a nonlinear sensor response. Stochastic distortions arise from random disturbances
that introduce uncertainty into the recorded spatial measurements at the sensor.
The goal is to recover an ideal mapping of the scene onto a 2-D representation. A faithful recovery of
the ideal 2-D representation requires a mathematical description of the image formation process. Some
of the more common distortions are considered below.

4.06.2 Observation Model
167
Image Formation
System
Original Scene                                                       Observed Image
FIGURE 6.2
Image formation process.
4.06.2.1 Blur models
Blurring may result from optical diffraction, long-term atmospheric turbulence, defocusing, and/or
relative motion between the recording medium and the scene. Although these distortions can be quite
complex, they are generally characterized by blurring of the recorded scene. Since the blurring takes
place in continuous space, the blur processes are described in continuous coordinates. The blurring
operation can be characterized by the way in which a point of light (an impulse) is spread out by the
image formation process. This is called the point-spread function (PSF), which is equivalent to a 2-D
impulse response of the imaging process. Knowledge of the PSF is essential for classical restoration
techniques, though some techniques have been developed to estimate the PSF directly from the blurred
image.
Foralinearimageformationsystem,thefollowingequationdescribesthecontinuous-imageformation
process:
g(x, y) =
 ∞
−∞
 ∞
−∞
h(x, y; x′, y′) f (x′, y′) dx′ dy′.
(6.1)
The function f (·) represents the ideal 2-D mapping of the 3-D scene, and h(·) is the impulse response,
or PSF, of the image formation system. The PSF describes the local averaging effects of the formation
process. If the PSF has a space-invariant averaging effect over the entire image, then (6.1) takes the
form of a convolution:
g(x, y) =
 ∞
−∞
 ∞
−∞
h(x −x′, y −y′) f (x′, y′) dx′ dy′
= h(x, y) ∗f (x, y),
(6.2)
where ∗denotes 2-D convolution.
A number of common blur forms are presented in this section [12]. These blurs appear frequently
in practice. Because blurring is a continuous process, the PSFs are presented in their continuous forms.
In digital image processing, these PSFs must ﬁrst be discretized.

168
CHAPTER 6 Image Restoration: Fundamentals of Image Restoration
4.06.2.1.1
Uniform linear motion
Motion blur incorporates a number of speciﬁc blurring phenomena, including general translations,
rotations, and scale changes. Probably the most common of these is uniform linear motion blur, which
has a simple functional form. Assume that the scene is translating with respect to the image plane at a
constant velocity v at an angle of φ from the horizontal axis for the time period T . Then the length of
motion during exposure is given by L = vT and the PSF is
h(x, y; L, φ) =
 1
L δ(x cos φ −y sin φ),
if

x2 + y2 ≤L
2 ,
0,
elsewhere.
(6.3)
If the entire scene is moving, the PSF is space-invariant. However, if an object within the scene is
moving independently of the background, the PSF varies spatially.
4.06.2.1.2
Out-of-focus lens
The PSF describing an out-of-focus lens must take into account the focal length, the aperture of the lens,
the distance between camera and object, and the wavelength of the light [13]. If the degree of defocusing
is large relative to the wavelengths of the light passing through the optical system, geometrical optics
provides an adequate functional representation of the PSF, which is given by
h(x, y; R) =

1
π R2 ,
if

x2 + y2 ≤R,
0,
elsewhere.
(6.4)
In the PSF above, R represents the radius of the PSF and is a function of the focal length and the distance
of the object from the camera. This model assumes that the distance of the object from the camera does
not vary across the image; however, if the object distance varies across the observed scene, the PSF
varies spatially.
4.06.2.1.3
Long-term atmospheric turbulence
Atmospheric turbulence blur is a factor in image acquisition over a signiﬁcant distance through the
atmosphere, such as in aerial imaging for weather forecasting or astronomical imaging [1,2]. Although
turbulence is not a deterministic process, the long-term effects can be modeled adequately as a Gaussian
PSF by
h(x, y; σR) =
1
2πσ 2
R
exp

−x2 + y2
2σ 2
R

.
(6.5)
The parameter σR controls the severity of the degradation.
Table 6.1 shows the optical transfer function (OTF)—the Fourier transform of the PSF—of each of
these blur forms. In the table, f =

f 2x + f 2y , and fφ = fx cos φ + fy sin φ. The zeros of the OTFs
are also shown.
4.06.2.2 Sensor nonlinearity
Further deterministic distortions may be introduced by nonlinearities in the sensor response. For exam-
ple, the response of photographic ﬁlm to light intensity is nonlinear [14]. CCD sensors have a linear

4.06.2 Observation Model
169
Table 6.1 Some Common Types of Blurring
Cause
OTF
Zeros in OTF
Uniform linear motion
sin (πLfφ)
πLfφ
fφ = ± 1
L , ± 2
L , ± 3
L , . . .
Out of focus lens
J1(2πRf )
πRf
f = 0.610
R
, 1.117
R
, 1.619
R
, . . .
Long-term turbulence
1
2πσ 2
R
exp

−2σ 2
Rf 2	
none
response, but CMOS may be either linear or logarithmic [15]. The nonlinear response appears in the
image model after blurring. However, in many cases one can assume that the image has been recorded
by an approximately linear region of the sensor. Such an assumption greatly simpliﬁes the restoration
task. The linearity assumption makes a host of numerical techniques available that would otherwise be
unavailable. The remainder of the discussion assumes that all images have been recorded in a linear
region of the sensor response or that the nonlinear sensor response has been inverted.
4.06.2.3 Discretization
Digitalrestorationofimagesrequiresthatadiscreterepresentationoftheimagebeobtained.Acontinuous-
to-discrete mapping has the form
g(m, n) =
 ∞
−∞
 ∞
−∞
l(x, y)g(m −x, n −y) dx dy,
(m, n) ∈S1,
(6.6)
where  is the pixel spacing, l(x, y) is the impulse response of a lowpass ﬁlter describing the combined
effect of the aperture, anti-aliasing ﬁlter, and sensor size prior to the sampling operation, and S1 is the
region of support of the discrete image. This lowpass ﬁltering operation may be intentionally included
in the sampling operation to prevent aliasing, for example, by including a birefringent ﬁlter on the
sensor [16,17]. However, in some cases, such as CCD cameras, lowpass ﬁltering is an inherent part
of the spatial integration operation of each array element. This may or may not adequately prevent
aliasing. Generally, the sampling process does not provide an exact representation of the continuous
image. Continuous images are rarely strictly bandlimited, and optical lowpass ﬁlters do not ﬁlter the
image ideally.
The discrete version of the blurring operation can be expressed as a summation rather than an integral.
In this case, the image formation system can be written as a discrete convolution of the image with the
discretized version of the PSF:
g(m, n) =

(k,l)∈R
h(k,l) f (m −k, n −l),
(m, n) ∈S1
= h(m, n) ∗f (m, n),
(6.7)
where R is the region of support of the PSF. A discrete version of the PSF is obtained from the continuous
version by an integral formula of the form (6.6).

170
CHAPTER 6 Image Restoration: Fundamentals of Image Restoration
4.06.2.4 Noise models
Stochasticdistortions,ornoise,areanothersourceofimagedegradations.Thesedistortionsmaybeintro-
duced by random variations in the image formation system, the transmission medium, or the recording
medium. Short-term atmospheric turbulence introduces stochastic distortions into the observed image.
Film-grain noise in the case of photographically recorded images and thermal noise in the case of pho-
toelectronic images are also sources of stochastic degradations. Furthermore, images must be digitized
for processing, and this digitizing introduces further degradation in the form of quantization noise.
Some type of stochastic degradation is virtually always present; thus, the convolution in (6.7) must be
modiﬁed to reﬂect the uncertainty associated with the observed image. This noise is usually assumed
to be additive white Gaussian noise. Although this assumption does not always reﬂect reality (as in the
case of ﬁlm-grain noise), other models have usually resulted in only modest gains in the accuracy of
restorations [6]. Various efforts have been made to incorporate a Poisson noise model into the restoration
process to reﬂect, for example, the low light levels where photon counting is a signiﬁcant contributor
to the randomness of the data [18,19]. In some cases, Poisson noise can be modeled as a Gaussian
distribution with a spatially varying variance that depends on the intensity at each pixel [20].
Modifying (6.7) to reﬂect the presence of noise yields
g(m, n) =

(k,l)∈R
h(k,l) f (m −k, n −l) + u(m, n)
(m, n) ∈S1
= h(m, n) ∗f (m, n) + u(m, n),
(6.8)
where u(m, n) is a noise term reﬂecting an appropriate distribution. The image degradation process
can be represented in a more compact form by stacking or lexicographically ordering the image. This
notation consists of representing the image as a vector and the blur as a matrix, as follows:
g = H f + u.
(6.9)
This notation is used throughout for the sake of simplicity.
4.06.3 Restoration algorithms
One can gain some insight into the problem and some avenues toward a solution by beginning with
some simplifying assumptions. Assume that the image has been blurred by the same PSF over the
entire image so that the blurring process can be represented by convolution. The noise is modeled as
independent, identically distributed (i.i.d.) Gaussian. Any nonlinearity in the sensor response is assumed
to be negligible. With these assumptions in place, a basic understanding of the restoration problem and
solutions can be obtained.
4.06.3.1 Inverse ﬁlter
The inverse ﬁlter is the simplest solution to the deblurring problem. If we take the Fourier transform of
(6.8), we obtain
G(ωm, ωn) = H(ωm, ωn)F(ωm, ωn) + U(ωm, ωn).
(6.10)

4.06.3 Restoration Algorithms
171
To obtain a workable algorithm, we sample the Fourier transform to obtain a DFT expression:
G(k,l) = H(k,l)F(k,l) + U(k,l).
(6.11)
The process of sampling the Fourier transform introduces spatial-domain aliasing, causing the spatial-
domain image to be periodically repeated. This means that the underlying image will be assumed to be
periodic. Boundaries on one side of the image will be equivalent to the values on the other side, and
likewise for top and bottom. If the actual image does not conform to this assumption (and it usually will
not), artifacts will arise in the deblurring process due to errors in the image formation model introduced
by the frequency-sampling process. In this section, we assume that the image is actually periodic.
Boundary issues will be addressed in Section 4.06.4.
If we ignore the noise term, we can solve for F(k,l) by dividing both sides of (6.11) by the DFT
of h(m, n) and performing an inverse DFT of the result. This approach is called an inverse ﬁlter. As
an example, inverse ﬁltering has been applied to a 512 × 512 image blurred by out-of-focus blur with
R = 6 pixels and corrupted by noise at various PSNR levels. PSNR is deﬁned as
PSNR = 10 log10
[max (g(m, n)) −min (g(m, n))]2
σ 2u
,
(6.12)
where σ 2
u is the noise variance. The image was blurred using circular convolution so that the spatial
periodicity assumption holds. Results are shown in Figure 6.3.
For the cases where noise is nearly zero, the method is effective. However, the resulting restorations
are nearly unrecognizable even for noise levels that are invisible to a human observer. To understand
why the inverse ﬁlter behaves so badly, consider the effect of the method of solution in light of the
image formation equation (6.11):
ˆF(k,l) = G(k,l)
H(k,l)
= F(k,l) + U(k,l)
H(k,l).
(6.13)
The inverse ﬁlter recovers the original image without any blur. However, a ﬁltered noise term is super-
imposed on the solution. The spectrum of i.i.d. noise is ﬂat, so the ﬁltered noise term U(k,l)
H(k,l) can become
quite large for high frequencies where the divisor term H(k,l) becomes small. This situation can be
seen in Figure 6.4a, which depicts the MTF (magnitude of the OTF) of the blur. The frequency response
falls off rapidly away from the origin, causing a division by small values that ampliﬁes the noise.
A simple solution to this problem can be seen in the pseudoinverse restoration in Figure 6.4b.
The pseudoinverse solution is deﬁned as
ˆF(k,l) =
 G(k,l)
H(k,l),
|H(k,l)| ≥ε
0,
|H(k,l)| < ε
(6.14)
for some small value ε. If the MTF falls below a threshold, the signal information recorded at those
frequencies is considered to be dominated by noise and therefore unrecoverable. Those frequencies are
then set to zero. While the results are dramatically better than inverse ﬁltering, even better results can
be obtained with other methods.

172
CHAPTER 6 Image Restoration: Fundamentals of Image Restoration
original
110 dB PSNR
100 dB PSNR
blurred
90 dB PSNR
80 dB PSNR
FIGURE 6.3
Inverse ﬁlter for out-of-focus blur with R = 6 and various PSNR levels.
4.06.3.2 Wiener ﬁlter
The Wiener ﬁlter is deﬁned as the restoration ﬁlter that minimizes the mean-square error (MSE). The
Wiener ﬁlter assumes knowledge of the autocorrelation of the image and the noise and further assumes
that these two processes are independent. In the frequency (DFT) domain, the ﬁlter is designed by
minimizing
E[|F(k,l) −W(k,l)G(k,l)|2]
(6.15)
with respect to the choice of W(k,l). The minimizer of this expression is then given by
W(k,l) =
H∗(k,l)
|H(k,l)|2 + Su(k,l)/S f (k,l),
(6.16)
where S f (k,l) and Su(k,l) are the power spectra of the image and noise and H∗denotes the complex
conjugate of H. Note that these functions are the DFTs of the corresponding autocorrelation functions.
To apply the Wiener ﬁlter, we must have knowledge of these power spectra even though we do
not know the original image. The noise power spectrum is generally assumed to be a constant, which
corresponds to the variance of the noise. The noise variance may be known based on knowledge of
the image acquisition process or may be estimated from the local variance of a smooth region of the
image.

4.06.3 Restoration Algorithms
173
(a)
(b)
FIGURE 6.4
MTF and pseudoinverse restoration: (a) MTF, (b) pseudoinverse at 80 dB PSNR.
The signal power spectrum is a little more challenging in principle, since it is not ﬂat. However, two
factors work in our favor: (1) most images have fairly similar power spectra, and (2) the Wiener ﬁlter
is insensitive to small variations in the image power spectrum. One can use a similar image to calculate
an estimate of the spectrum, or one can average a large number of images to approximate the idea of
an expected value.
Another option is to use a simple model of the power spectrum or autocorrelation function. A common
model for the image autocorrelation function is
r f (m, n) = σ 2
f ρ−
√
m2+n2 + ( ¯F)2,
(6.17)
where ¯F is the mean value of the image and ρ is the correlation coefﬁcient between pixels one pixel
apart [21].
The performance of the Wiener ﬁlter is illustrated in Figure 6.5. The original image was used to
calculate the power spectrum for the Wiener ﬁlter. Note that the best-case PSNR in this ﬁgure is
equivalent to the worst-case PSNR in Figure 6.3. The Wiener ﬁlter provides a dramatic improvement
over the inverse ﬁlter as well as a noticeable improvement over the pseudoinverse ﬁlter.
Figures 6.6 and 6.7 explore the impact of the choice of power spectrum estimate. Figure 6.6 shows
three sources of spectral estimates—the original image, a completely unrelated image, and the autocor-
relation model given in (6.17). The resulting spectral estimates are shown below the source. In each case,
the spectral estimates are obviously different but have a similar structure. Wiener restorations with these

174
CHAPTER 6 Image Restoration: Fundamentals of Image Restoration
80 dB PSNR
original
60 dB PSNR
blurred
40 dB PSNR
20 dB PSNR
FIGURE 6.5
Wiener ﬁlter for out-of-focus blur with R = 6 and various PSNR levels.
three spectra for the 40-dB PSNR case are shown in Figure 6.7 along with root-mean-square (RMS)
error values for each of the restorations. Visually, the three results are comparable. Numerically, the
result with the actual spectrum is better, although not dramatically better than the others. Remarkably,
the restoration obtained using the power spectrum from an entirely different image is still nearly as
good as the image that used the original spectrum.
4.06.3.3 Constrained least squares
The inverse ﬁlter fails because it allows the noise to grow uncontrolled in the restoration process. The
noise ampliﬁcation comes about because the data is treated as though it is noiseless, which forces the
reblurred solution H ˆf to be equal to the blurred image g. However, according to (6.9), g −H f = u,
which is not zero. In fact, for an M × N image,
E[∥g −H f ∥2] = E[uT u] = M Nσ 2
u .
(6.18)
As a result of forcing the residual g −H ˆf to be zero, the image is ﬁlled with high-frequency artifacts
that obscure the original image. The residual must be small because the reblurred restoration ought to
correspond closely to the blurred image. However, if it is too small, then noise will be ampliﬁed to an

4.06.3 Restoration Algorithms
175
original
unrelated image
correlation model
corresponding
spectra
FIGURE 6.6
Sources of image power spectrum (top row) with corresponding spectra (bottom row).
original spectrum
RMSE = 8.36
beach spectrum
RMSE = 10.40
model spectrum
RMSE = 9.29
FIGURE 6.7
Wiener restorations with different image spectra. RMSE values indicated.
objectionable level. We can deﬁne a family of solutions that satisfy the small-residual constraint:
∥g −H f ∥2 ≤ε2.
(6.19)
One way to formulate the image restoration problem is to ﬁnd a solution that satisﬁes this constraint
without excessive noise ampliﬁcation. A measure of the high-frequency ampliﬁed noise component

176
CHAPTER 6 Image Restoration: Fundamentals of Image Restoration
can be deﬁned by highpass ﬁltering the image and measuring the resulting energy. If we deﬁne L as
a highpass ﬁlter in matrix form, then ∥L f ∥2 measures the high-frequency energy in f . This measure
can steer the solution away from excessive noise ampliﬁcation. The problem is formulated as follows
[22,23]:
min
f
∥L f ∥2 subject to ∥g −H f ∥2 ≤ε2.
(6.20)
This formulation tells us to minimize the high-frequency energy or roughness of the restored image
without allowing the reblurred restoration to be very different from the blurred image data.
Typically, the highpass ﬁlter L is chosen to be a 2-D discrete Laplacian convolution with the kernel
[0 1 0; 1 −4 1; 0 1 0] or [1 1 1; 1 −8 1; 1 1 1]. The upper bound ε2 is usually chosen to be M Nσ 2
u because
of the relationship in (6.18). This technique requires knowledge of the noise variance, but this can often
be estimated from the image when it is unknown. Also, this choice of ε2 is known to oversmooth the
image [24].
4.06.3.4 Regularized least squares
The constrained restoration method can be reformulated into a more ﬂexible approach using Lagrange
multipliers. The problem can be formulated as
min
f {∥L f ∥2 + λ(∥g −H f ∥2 −ε2)}
(6.21)
with λ chosen so that ∥g −H f ∥2 = ε2 is satisﬁed. Since λε2 is constant with respect to f , we can
drop the term in the optimization problem. Because a scale factor on the overall criterion has no effect
on the solution, we can multiply through by α = 1/λ and restate the problem as
min
f {∥g −H f ∥2 + α∥L f ∥2}.
(6.22)
The parameter α can be chosen to satisfy ∥g −H f ∥2 = ε2 or to meet some other criterion. This
formulation of the problem is referred to as regularized restoration. The criterion represents a tradeoff
between deblurring (minimizing the ﬁrst term) and smoothing (minimizing the second term). The
regularization parameter α controls the relative importance of these two goals.
Regularization is a well-established technique for dealing with instability and non-uniqueness in
inverse problems [25]. The term encompasses various techniques for dealing with the numerical insta-
bility inherent in ill-posed inverse problems such as image restoration. Regularization imposes the
assumption that the true image is reasonably smooth (more so than the inverse solution, for which
α = 0). Thus, a solution can be obtained that is more satisfactory than the inverse by imposing a rough-
ness penalty on the result. This roughness penalty can be viewed as a stabilizing term in the expression
to be minimized.
The performance of regularized image restoration is demonstrated in Figure 6.8. The image is blurred
as before and has a PSNR of 40 dB. Results with various settings for α are shown, and RMSE values
are indicated for each image. As α increases, the image moves from noisy and sharp to less noisy but
blurrier. Large changes in α are required to make a noticeable difference in the result.

4.06.3 Restoration Algorithms
177
original
blurred (13.18)
α = 0.001 (13.38)
α = 0.01 (8.69)
α = 0.1 (9.39)
α = 1 (10.81)
FIGURE 6.8
Effect of the regularization parameter (RMSE in parentheses).
If the blur H and the regularizing ﬁlter L are shift-invariant and the image is assumed to be periodic,
the solution can be calculated using DFTs. The solution to (6.22) can be written as
ˆF(k,l) =
H(k,l)∗
|H(k,l)|2 + α|L(k,l)|2 G(k,l),
(6.23)
where L(k,l) is the DFT of the regularization ﬁlter kernel. Notice the similarity between this equation
and the Wiener ﬁlter solution. If α|L(k,l)|2 is chosen to be equal to Su(k,l)/S f (k,l), then regularized
least squares is equivalent to Wiener ﬁltering.
The effect of the regularization ﬁlter is explored in Figure 6.9. As expected, a highpass energy penalty
works better than an energy penalty that does not emphasize high-frequency energy. The Wiener ﬁlter
equivalent works best, as it incorporates more speciﬁc knowledge of the image power spectrum.
The regularized formulation allows for some useful extensions. We can replace the squared residual
error term with a weighted term:
∥g −H f ∥2
R = (g −H f )T R(g −H f ),
(6.24)
where R is a diagonal matrix that locally weights the data errors. These weights can be used to model
cases where the variance is different at each pixel or where some pixel values have been lost due to bad
pixels, saturated pixels, or other cases where speciﬁc data points should be ignored.

178
CHAPTER 6 Image Restoration: Fundamentals of Image Restoration
Laplacian
RMSE = 8.57
identity
RMSE = 9.49
Wiener ﬁlter
RMSE = 7.84
FIGURE 6.9
Effect of the regularization ﬁlter with RMSE indicated: (a) discrete Laplacian, (b) identity ﬁlter, (c) Wiener
equivalent.
The roughness penalty can be extended in a similar way:
∥L f ∥2
S = (L f )T S(L f ),
(6.25)
where S is a diagonal weight matrix that controls the local degree of smoothing. In locations that are
known to be less smooth, the roughness penalty can be reduced, whereas in locations known to be
smooth, the roughness penalty can be increased. In some cases, the locations of edges can be inferred
from the data (or an initial restoration), and in those places the roughness penalty can be reduced so
that sharp edges are not overly smoothed in the effort to compensate for noise.
The resulting solution in matrix-vector notation is
ˆf = (HT RH + αLT SL)−1HT Rg.
(6.26)
While the solution can be expressed compactly this way, it generally cannot (and should not!) be
calculated this way. The matrix size is prohibitively large for an ordinary image size, and the calculation
of the system solution requires an inordinate amount of computation. DFT-based methods that exploit
FFTs cannot be applied directly because R and S destroy the shift-invariance necessary to reduce
the problem to a frequency-domain solution. This type of problem is usually solved using iterative
algorithms [26–28].
Example restorations that exploit these weighted terms are shown in Figure 6.10. An image was
blurred with 13-pixel uniform horizontal motion blur, and noise was added to a level of 50 dB PSNR.
The original scene was 256 × 256, but the blurred image was windowed to 256 × 244 to represent only
those blurred pixels that have contributions from inside the original 256 × 256 unblurred scene. In the
ﬁrst case, the image was restored by setting both weight matrices to identity (constant weights). In the
second case, local variances were calculated from the original image to illustrate the effect of varying
the regularization weights based on local image features. In the third case, the image was restored from
only 20% of the pixels, with the other 80% given zero weight because they were considered lost data.
Although the image is degraded signiﬁcantly, the ability to recover from 80% data loss is remarkable.

4.06.3 Restoration Algorithms
179
(a)
(b)
(c)
(d)
(e)
(f)
FIGURE 6.10
Spatially varying regularized restoration: (a) original image, (b) blurred image, (c) blurred image with 80%
bad pixels, (d) spatially-invariant restoration, (e) spatially-variant regularized restoration, and (f) restoration
from 20% good data.
A number of methods have been proposed to determine the optimal regularization parameter α
automatically. Some of these are general techniques for the formulation in (6.22), while others are
speciﬁc to image restoration. At least several dozen estimates for α have been published [29], some
of which are covered by the following Refs. [29–37]. Many of these techniques, however, require an
estimate of the noise variance. Other techniques do not require prior knowledge of the noise variance.
Generalized cross-validation (GCV) is based on the minimization of a prediction error and is not
explicitly based on a probabilistic interpretation [38–40]. It does not require knowledge of the noise
variance. Another class of techniques is based on a probabilistic interpretation. Generalized maximum
likelihood (GML) [41] and Bayesian or maximum a posteriori (MAP) methods [42–44] have also been
developed. A more recent technique has been proposed that minimizes an unbiased estimate of the
mean-square error (MSE) in the restoration [45]. This method appears to outperform GCV but requires
knowledge of the noise variance.

180
CHAPTER 6 Image Restoration: Fundamentals of Image Restoration
4.06.3.5 Bayesian solution
Bayesian methods provide another approach to formulating a solution to the image restoration problem.
In this framework, the image and noise are viewed as random processes. By viewing the problem
this way, information about typical image characteristics can be formulated in terms of probability
models. This formulation opens up new approaches to the restoration problem, the estimation of the
regularization parameter, as well as the identiﬁcation of the blur process.
The image is represented by a probability density function of the form
p( f ) = 1
Z exp

−1
λ

c∈C
ρ

dT
c f

,
(6.27)
where Z is a normalizing constant, λ is a scaling parameter of the density, and dc is a coefﬁcient
vector representing a particular weighted combination of neighboring pixels c. ρ(·) is a function that
captures the distribution of variation in each pixel combination. This is often chosen for simplicity to
be ρ(x) = x2, and the dc are often ﬁrst differences in each direction. Other choices of ρ(x) that grow
less rapidly with x are often chosen to encourage the preservation of discontinuities. A common choice
that encourages sharper edges is ρ(x) = |x|.
The noise is usually assumed to be Gaussian. Since u = g −H f , we can express the conditional
density of g given f as
p(g| f ) =
1
(2πσ 2)
M N
2
exp
−∥g −H f ∥2
2σ 2

,
(6.28)
where σ 2 is the variance of the noise, M and N are the dimensions of the image, and ∥·∥is the Euclidean
norm.
The MAP solution is the one that maximizes the probability of a restored image given the blurred
image data. That is,
ˆf = arg max
f
log p( f |g),
(6.29)
where log p( f |g) is the log-likelihood function. From Bayes formula, we have
log p( f |g) = log p(g| f ) + log p( f ) −log p(g).
(6.30)
Since log p(g) is a constant with respect to f , we can maximize log p( f |g) by minimizing the ﬁrst
two terms above:
φ( f ) = −log p(g| f ) −log p( f ),
(6.31)
=
1
(2πσ 2)
M N
2
∥g −H f ∥2 + 1
λ

c∈C
ρ

dT
c f

+ K.
(6.32)
The constant K can be ignored in the optimization process. Note that this expression has a remark-
able similarity to (6.22). However, this formulation introduces more ﬂexibility in the penalty term, which
allows for sharper edge reconstruction. The method of minimizing total variation can be

4.06.4 Boundary Effects
181
formulated in this framework [46]. This method has gained some traction in recent years due to its
connection to compressed sensing [47].
4.06.4 Boundary effects
Boundaries in an image can be a particular challenge for image restoration. Each pixel in a blurred image
has contributions from surrounding neighbor pixels in the original image. As a result, a blurry image
requires a larger original image to fully explain it in terms of the image formation model. This means
that in (6.9), g is smaller than f , and H is non-square—shorter but wider. Using an iterative restoration
algorithm, this size difference can easily be accounted for. The boundaries are estimated along with
the internal part of the image in such a way that they are consistent with the captured pixels near the
boundary and have the appropriate smoothness characteristic imposed by the restoration algorithm.
If an FFT-based algorithm is used, the input and output images will be the same size. The underlying
mathematics of the FFT (the DFT) considers the image to be periodic. Since neither the sides nor the
top and bottom of a blurred image will generally match up smoothly, strong false discontinuities are
introduced into the periodic blurred image. These discontinuities will be ampliﬁed by the highpass
ﬁltering effect of the image restoration process, leading to serious artifacts unless the boundary discon-
tinuities are addressed. Two general approaches will be mentioned. In one approach, the boundaries of
the periodically extended blurred image are pre-smoothed so that the discontinuities are reduced prior
to restoration. The periodic blurred image is blurred again using the original PSF, and then the result and
the original blurred image are weighted and added together. The doubly blurred image is emphasized
near the boundaries but transitions to the original blurred image away from the boundaries. When the
restoration takes place, the result will be a deblurred image toward the middle that gradually transitions
to a singly blurred image near the boundaries [48].
In another approach, artiﬁcial boundaries are added to the blurred image so that the size of the
expanded image is equal to (or greater than) the size of the original image that contributed to the
captured image frame. The artiﬁcial boundaries are constructed so that they smoothly connect the
actual boundaries in the blurred image [49]. This eliminates the sharp discontinuities without altering
the captured image pixels. The expanded image is restored to obtain an original of the same expanded
size. A few steps of an iterative method can be used to reﬁne the result if a single step does not yield
satisfactory results. The expanded restoration can either be kept at the larger size or truncated back
to the size of the actual blurred image. While the restoration in the expanded region often has little
correspondence to the original in that region, the introduction of a smooth boundary and a larger image
frame can signiﬁcantly reduce boundaries.
The car image from Figure 6.10 was used to illustrate these two approaches. The image with no pre-
processingisshowninFigure6.11a.TheboundaryblurredimagefortheﬁrstapproachisshowninFigure
6.11b, while the expanded blurred image is shown in Figure 6.11c. All three images were restored using
an FFT-based regularized restoration with the same regularization parameter. The resulting restora-
tions are shown below each input image. Results for the two boundary compensation methods are
much improved over the restoration with no boundary compensation. The image with extended bound-
aries is slightly better in some areas, particularly with artifacts less visible in the lower portion of the
image.

182
CHAPTER 6 Image Restoration: Fundamentals of Image Restoration
(a)
(b)
(c)
(d)
(e)
(f)
FIGURE 6.11
(a) Blurred image frame with no boundary processing, (b) frame from (a) with boundaries periodically blurred,
(c) frame from (a) with boundaries expanded smoothly, (d) restoration of (a), (e) restoration of (b), and (f)
restoration of (c).
4.06.5 Blur identiﬁcation
The algorithms discussed so far assume that the PSF is known. However, in many cases the PSF is
unknown or follows a known model with an unknown parameter. In these cases the PSF must be
estimated directly from the blurry image. A number of approaches have been taken to this problem.
4.06.5.1 Background
Sometimes this problem is called blind deconvolution [50], blind deblurring [51], or, most commonly,
blur identiﬁcation [52]. In most cases the blur degradation process is assumed to be spatially invariant so
that it can be modeled by convolution in the spatial domain or multiplication in the frequency domain.

4.06.5 Blur Identiﬁcation
183
In cases where the PSF varies spatially, the image can often be segmented so that the space-invariant
assumption holds reasonably well over each segmented region. In that case, a space-invariant blur
identiﬁcation technique can be used in each region.
If we consider the problem in the frequency domain and neglect noise, we have (6.11) without the
noise term. To solve the blur identiﬁcation problem, we must ﬁnd both H(k,l) and F(k,l) for each
G(k,l), which is plainly impossible without any prior information about either signal. A successful
blur identiﬁcation technique must exploit partial information about the signal and/or the PSF. Some
techniques assume a speciﬁc model for the PSF while others assume certain image characteristics that
allow the PSF effect to be distinguished from the image. Some of the assumptions and models that are
used to distinguish the image and PSF are:
•
nonnegativity of the image and/or PSF,
•
ﬁnite extent of the PSF,
•
intensity preservation of the PSF,
•
parametric PSF model,
•
simple parametric image autocorrelation model,
•
known image power spectrum,
•
neural-network model for PSF and/or image.
Techniques for blur identiﬁcation can be categorized in different ways. Some methods assume the
PSF ﬁts a speciﬁc model, whereas others make no assumption on the PSF model other than in some
cases ﬁnite support. Some blur identiﬁcation algorithms estimate the blur and then require a separate
restoration step. Other algorithms simultaneously estimate both the blur and the image. A variety of
techniques have been proposed for this problem.
4.06.5.2 Classical techniques
4.06.5.2.1
Frequency-plot inspection
Many common PSFs have very distinct patterns of zeros in the frequency domain (such as the ﬁrst two
entries in Table 6.1). Frequency-plot inspection, developed by Gennery [12], exploits this property for
cases like motion blur and defocus. To see how this works, consider (6.11) but without the noise term.
This equation shows that the DFT of the blurred image, G(k,l), must be zero whenever the optical
transfer function (OTF), H(k,l), is zero. A plot of |G(k,l)| may therefore reveal zeros that form a
recognizable pattern. The underlying image itself is unlikely to have such a regular pattern of zeros.
Therefore, if the zero pattern in the blurred image corresponds closely to a known form, it is safe to
assume that the degradation in G was caused by an OTF of that form.
Suppose, for example, that the zeros of the frequency magnitude plot of an image are arranged in a
pattern of concentric rings. Such a pattern might reﬂect out-of-focus blur, which can be modeled by a
cylindrical PSF. The spacing of the zero rings can be used to compute the radius of blur for the cylindrical
PSF model. Gennery recommends that the blurred image be preprocessed with a 2-D differentiator
before identifying the blur. Differentiation accentuates the high-frequency content, which is typically
small, and suppresses the low-frequency content, which is typically very large. Differentiation also
removes the local mean of the image, which reduces false discontinuities at the edges due to the
periodic assumption of the DFT [53]. Another approach is simply to display the log spectrum instead of

184
CHAPTER 6 Image Restoration: Fundamentals of Image Restoration
the spectrum. This approach avoids the zero pattern associated with the differentiation operators, and
the log spectrum maintains the same intensity ordering as the spectrum.
To see the method in action, consider the image in Figure 6.12a. This image was originally
1024×1024 to simulate blurring in continuous space. The image was blurred with a cylindrical PSF hav-
ing a radius of 12.5 pixels. The image was then downsampled by a factor of 4 by averaging 4×4 blocks
down to single pixels. Noise was added to the resulting 256×256 image at a level of 40 dB PSNR. This
image is shown in Figure 6.12b. The log-magnitude of the FFT is shown in Figure 6.12c. Note that the
blur pattern appears to be a ring. From this it can be inferred that the cause of blur is a lack of focus. As
a contrasting example, if the blur had been caused by horizontal motion blur, the spectrum would show
vertical-stripe zeros. Only the ﬁrst ring is visible. Rings at higher radii are obscured by the noise ﬁlling in
those frequencies. The radius is measured to be 29.6% of the image dimension. According to Table 6.1,
theﬁrstzeroradiusofout-of-focusbluroccursatanormalizedfrequencyof 0.610
R . Settingthisequaltothe
measured radius and solving, we obtain ˆR = 3.38. If we reduce the original radius by the downsampling
factor of 4, the actual radius is R = 3.125. The image was restored using a regularized restoration with
Laplacian smoothing penalty and α = 0.01. The result is shown in Figure 6.12d. The restoration using
the actual radius is nearly indistinguishable from the restoration obtained with the estimated radius.
Frequency-plot inspection has certain limitations. The PSF must have a known form, like the ﬁrst
two in Table 6.1, in which the zero locations identify the blur. The original scene must have enough
high-frequency content without an interfering zero pattern so that the spectral zero pattern is visible.
Finally, the noise must be low enough so that the zeros are not obscured. If these conditions are fulﬁlled,
this method can yield useful results. However, Cannon [54] has observed that the noise in real-life
images usually prevents the zero locations from being clearly visible. Our example above seems to
conﬁrm this, as only a moderate amount of noise made the radius of the zero ring difﬁcult to determine
precisely. Furthermore, some common blurs, such as long-term atmospheric turbulence, have no regular
spectral pattern of zeros or no zeros at all. For a limited class of blurs with low noise levels, this method
is both simple and intuitive and may yield satisfactory results.
4.06.5.2.2
Homomorphic deconvolution
The challenge of blur identiﬁcation is to separate the product of two unknown functions, the OTF of the
blur and the spectrum of the image. Assuming the blur is shift-invariant, we can exploit the fact that the
OTF is the same over different sections of the image while the spectrum of the image varies by section.
If we can reinforce the OTF contribution and somehow average out the image spectrum contribution, we
can separate the two terms and identify the OTF. This is the basis for the method proposed by Stockham
et al. [55], called homomorphic deconvolution or cepstral averaging. To begin the analysis, we consider
blurred subimages in the frequency domain. Neglecting noise,
Gi(ωm, ωn) = H(ωm, ωn)Fi(ωm, ωn),
(6.33)
where Gi represents the ith subimage of G. If we take the log-magnitude of both sides, we can make
the problem additive. Then we can sum over subimages to obtain
1
n
n

i=1
log |Gi(ωm, ωn)| ≃1
n
n

i=1
log |Fi(ωm, ωn)| + log |H(ωm, ωn)|.
(6.34)

4.06.5 Blur Identiﬁcation
185
(a)
(b)
(c)
(d)
FIGURE 6.12
(a) Original high-resolution image (scaled down by a factor of 4 for easier comparison), (b) blurred, decimated,
and noisy image, (c) log spectrum of (b), and (d) restoration with estimated radius.

186
CHAPTER 6 Image Restoration: Fundamentals of Image Restoration
We can see that the original spectrum F(ωm, ωn) is averaged over subimages, which should tend
to cancel the image- and position-speciﬁc characteristics of the spectrum and leave us with a more
predictable image-block spectrum. The ﬁrst term on the right side of the equation is a sample average of
the log-magnitude distribution over frequency of the original signal. An estimate of this quantity may
be subtracted out of the equation if a reliable one can be obtained. Such an estimate can be computed
from an unblurred version of a scene similar to the blurred image. The choice of this scene does not
appear to be critical to the estimate [54].
An inverse log of (6.34) yields an estimate of |H(ωm, ωn)|. The equation does not address the phase
of the OTF. Stockham et al. simply restricted the blur to either motion or out-of-focus blur so that the
PSF could be determined from the magnitude alone.
Our discussion assumes that noise is absent. Stockham et al. showed that the presence of noise
causes the estimated ﬁlter to be the geometric mean of the noise-free inverse and the Wiener ﬁlter
assuming known noise statistics. Thus, the noise actually moves the ﬁlter away from an ill-conditioned
inverse.
The applicability of this method is limited by several factors. Since this method requires the image to
be divided into subimages, the PSF must be signiﬁcantly smaller than the image [54]. Furthermore, we
must have access to a fairly accurate estimate of the original-image power spectrum. Cannon [54] claims
that the representative image chosen is not critical, but this requirement creates some uncertainty in the
performance of the technique. In addition, the noise level must be fairly low so that the ill-conditioning of
the estimated ﬁlter is not too severe. Finally, the PSF must have a phase that can easily be identiﬁed from
a simple model (like motion blur or out-of-focus blur). If all these factors are favorable, homomorphic
deconvolution can serve as a viable blur identiﬁcation technique.
Stockham et al. proposed a variation on the homomorphic deconvolution method. Instead of summing
log spectra, this approach sums power spectra of the blocks:
1
n
n

i=1
|Gi(ωm, ωn)|2 ≃1
n |H(ωm, ωn)|2
n

i=1
|Fi(ωm, ωn)|2.
(6.35)
|H(ωm, ωn)| is then isolated by taking a log of both sides and subtracting a log estimate obtained from
a similar image. This method has performance and conditions similar to homomorphic deconvolution.
These early methods restricted the blurs to speciﬁc pre-deﬁned classes. Later methods, on the other
hand, are able to identify and remove more general classes of blurs.
4.06.5.3 Parametric methods
Parametric methods simplify the estimation problem by adopting a model for both the image and the
PSF. The model can be quite ﬂexible so that more general blurs can be identiﬁed as compared to the early
techniques based on regular zero patterns. In this category, autoregressive moving-average (ARMA)
models have been adopted. The image is assumed to follow an autoregressive (AR) model, while the
PSF is assumed to follow a moving-average (MA) model. Since PSFs generally have ﬁnite support,
an MA model is appropriate for PSFs. Images generally have large-scale correlations that decay in
something like an exponential form, making AR models an appropriate choice for images.

4.06.5 Blur Identiﬁcation
187
Thus, the blur identiﬁcation problem is formulated as the identiﬁcation of a 2-D ARMA model [52].
The model is deﬁned as follows:
f (p, q) =

(k,l)∈R⊕+
a(k,l) f (p −k, q −l) + u(p, q),
(6.36)
g(p, q) =

(m,n)∈R
h(m, n) f (p −i, q −j) + n(p, q),
(6.37)
where f (p, q) is the original image at (p, q), g(p, q) is the image with space-invariant blur and additive
noise, and u(p, q) and n(p, q) are independent, zero-mean, white Gaussian noise signals with variances
σ 2
u and σ 2
n . R⊕+ is often assumed to be a non-symmetric half-plane (NSHP) support of the AR process,
although this restriction is sometimes unnecessary. The support R of the MA process {h(m, n)} is not
causal. The PSF is assumed to conserve energy, which preserves the mean value of the image:

(m,n)∈R
h(m, n) = 1.
(6.38)
In matrix-vector notation,
f = Af + u,
(6.39)
g = H f + n.
(6.40)
Combining these into a single equation, we obtain
g = H(I −A)−1u + n.
(6.41)
A number of methods have been proposed to estimate these models. Tekalp et al. [52], Biemond et
al. [56], Lagendijk et al. [57–60], and Katsaggelos and Lay [61,62] have developed various approaches
to estimating the model parameters using a maximum-likelihood (ML) criterion. Promising results have
been obtained with this method. However, it is limited to symmetric PSFs.
A similar approach was proposed by Reeves and Mersereau [63]. They adopted the same ARMA
model but used generalized cross-validation (GCV) as the optimality criterion. GCV does not assume
Gaussian noise or image statistics. It is based on a type of prediction error formulated by leaving out one
pixel in the restoration and then predicting that pixel and calculating the squared error in the prediction.
The overall criterion is a weighted sum of these squared prediction errors. The technique is in many
ways comparable to ML but may perform better in the face of model mismatch.
4.06.5.4 Dual smoothing methods
The blurred image can in concept be approximately factored into many different PSF and image pairs.
The blurred image can be explained by a PSF equal to an impulse and an original image equal to the
blurred image. It can also be explained by the correct PSF and original image or by a too-large PSF
and an overly restored image. Note, however, that the level of smoothness of PSF and image will vary
inversely with one another. The problem can be formulated, then, as a problem of ﬁnding the optimal
tradeoff in smoothness between the two. This has motivated a variety of algorithms.

188
CHAPTER 6 Image Restoration: Fundamentals of Image Restoration
4.06.5.4.1
Dual Wiener-type iteration
Davey et al. [64] proposed a dual iteration for estimating image and blur simultaneously that does not
require an explicit PSF model. The method accounts for the presence of noise by using a Wiener-type
structure, which alternately estimates a smooth PSF and a smooth restored image. It has the following
form:
Fi(ωm, ωn) =
G(ωm, ωn)H∗
i−1(ωm, ωn)
|Hi−1(ωm, ωn)|2 +
α
|Fi−1(ωm,ωn)|2
,
(6.42)
Hi(ωm, ωn) =
G(ωm, ωn)F∗
i−1(ωm, ωn)
|Fi−1(ωm, ωn)|2 +
α
|Hi−1(ωm,ωn)|2
,
(6.43)
where α is set to reﬂect the noise level. Davey et al. indicate that a support constraint may be necessary
for their method to work.
4.06.5.4.2
Dual regularization methods
A number of techniques have been developed that simultaneously enforce a regularization penalty on
the image and the PSF. The level of success appears to be a function of the particular penalty term,
the accuracy of the relative weights of each term, and other constraints that are applied to the problem.
Among the earliest of these techniques is an approach based on maximizing the combined entropy
of the estimated image and PSF. The concept of entropy has been used not only to restore images
but identify blur as well. This method has been developed by Newton [65], Newman and Hildebrandt
[66], and others. Entropy is deﬁned as the average amount of information in events Ei with probability
P(Ei) = pi, where  pi = 1. If an image is viewed as a realization of a random process, then the
entropy of the image is
S = −

pi log pi,
(6.44)
where pi is the normalized value of pixel fi. Since both image and PSF are unknown, a weighted sum
of the entropy of the image and the PSF is maximized:
S( f , H) = ϵS( f ) + (1 −ϵ)S(H).
(6.45)
The parameter ϵ determines the relative smoothness of the image versus the PSF. This method has
produced satisfactory results for high-contrast test images [66]. However, no rationale is given for a
proper choice of ϵ. This parameter apparently has the potential to inﬂuence the solution dramatically.
More recently, several algorithms have been developed that build upon the idea of a dual smoothness
penalty. A representative approach was proposed by You and Kaveh [67]. Their method incorporates
a data error term and two regularization terms, one for the image and one for the PSF. An advance
over previous methods is to use an edge-preserving regularization term for the image. This will lead to
better restorations of high frequencies that may feed back into better estimates of the PSF. Other tech-
niques that build on this concept have been proposed. Kundur and Hatzinakos [68] directly estimated

4.06.5 Blur Identiﬁcation
189
0
2
4
6
8
0
2
4
6
8
0
0.01
0.02
0.03
0.04
0.05
0
2
4
6
8
0
2
4
6
8
0
0.01
0.02
0.03
0.04
0.05
(a)
(b)
(c)
(d)
−3
−2
−1
0
1
2
3
−2
0
2
0
0.05
0.1
0.15
0.2
−3
−2
−1
0
1
2
3
−2
0
2
0
0.05
0.1
0.15
0.2
(e)
(f)
(g)
(h)
FIGURE 6.13
(a) Image with out-of-focus blur R = 3.125 and 40 dB PSNR noise, (b) restored image, (c) true PSF,
(d) estimated PSF, (e) image with linear blur L = 5.775 and 40 dB PSNR noise, (f) restored image, (g) true
PSF, (h) estimated PSF.
an inverse ﬁlter from a regularized restoration. Chan and Wong have used total-variation (TV) regular-
ization to formulate a similar approach [69]. In essence, these problems formulate a criterion as:
E( f , h) =

[g(m, n) −h(m, n) ∗f (m, n)]2 +

w f (m, n)φ(D f f (m, n))
+

wh(m, n)φ(Dhh(m, n)).
(6.46)
These methods appear to hold promise for a fairly ﬂexible approach to blur identiﬁcation while simul-
taneously yielding a restored image.
To illustrate this general technique, the image in Figure 6.12b was restored using a dual regularization
method. The image was regularized using a TV penalty and the blur using a quadratic Laplacian-ﬁltered
error penalty. Regularization weights were determined by trial and error. Results for the PSF estimation
and the restored image are shown in the ﬁrst row of Figure 6.13. The same original image was blurred
with a horizontal motion blur and then restored using the same dual-regularization blind deblurring
method. Results are shown in the second row of Figure 6.13. In both cases the images are restored
adequately, even though the PSF estimate is visibly different from the original. More accurate models
for the PSF regularization may yield more accurate PSF estimates. Better choices of the regularization
parameters may also improve the results. Recently, Liao and Ng have identiﬁed the importance of

190
CHAPTER 6 Image Restoration: Fundamentals of Image Restoration
appropriate choices of the regularization parameters and have developed a GCV-based method in the
context of total-variation blur identiﬁcation [70].
4.06.6 Conclusion
The basic concepts in image restoration are well established, but research continues. Researchers are
continually developing algorithms that better model the characteristics of real-world images. This kind
of advance ordinarily occurs in the face of greater computational complexity. However, advances are
being made in computational efﬁciency as well, even for more demanding algorithms. Furthermore,
applications demand more automation in extracting model parameters that guide and optimize existing
algorithms. Image restoration also continues to teach us about related problems in image processing
and related inverse problems.
Although image restoration is a mature ﬁeld, a number of challenges and opportunities remain.
Practical restoration problems are being driven by the proliferation of digital cameras in portable elec-
tronic devices. New opportunities for camera design may incorporate blurring/deblurring as a design
strategy [71]. Consumers will also expect more options to clean up photos after the fact. More sophisti-
cated and computationally complex restoration techniques are being developed. Recent work in the area
of dictionary-based models and non-local smoothing show great promise [72,73]. These techniques will
in turn require more clever processing to obtain results in a reasonable time. Furthermore, blur identiﬁ-
cation and blind deblurring must be extended to more general and more complex blurs, such as motion
due to jitter and spatially varying blurs due to varying depth of ﬁeld. These problems may yield to
better prior models for the unknown PSF. A great deal of progress has been made in these areas, but the
increasing computational power available and new concepts in signal modeling suggest the potential of
further progress.
Glossary
Bayesian method
a method that is based on Bayes’ rule, which allows the conditional
probability of the original image given the data to be expressed in
terms of the conditional probability of the data given the original
and the probability of the original
Optical transfer function (OTF) the frequency response of the blur; the Fourier transform of the point-
spread function
Point-spread function (PSF)
the impulse response of the optical system; the pattern created when
a point of light is imaged
Regularization
a technique that biases the solution toward a more stable and usually
smoother result
Wiener ﬁlter
a ﬁlter incorporating second-order statistics about the original image
and the noise that minimizes the mean-squared error in the restored
image

References
191
Relevant Theory: Signal Processing Theory
See Vol. 1, Chapter 4 Signals and Stochastic Processes
See Vol. 1, Chapter 5 Sampling and Quantization
See Vol. 1, Chapter 9 Discrete Multi-Scale Transforms in Signal Processing
References
[1] R.A. Muller, A. Bufﬁngton, J. Opt. Soc. Am. 64 (1974) 1200–1210.
[2] R.L. White, R.J. Hanisch, in: SPIE Applications of Digital Image Processing XIV, vol. 1567, pp. 308–316.
[3] H. Fujita, K. Doi, L.E. Fencil, Med. Phys. 14 (1987) 549–556.
[4] K.J. Coakley, J. Llacer, in: SPIE Medical Imaging V: Image, Physics, vol. 1443, pp. 226–233.
[5] C.-Y. Wen, C.-H. Lee, Forensic Sci. J. 1 (2002) 15–16.
[6] H.C. Andrews, B.R. Hunt, Digital Image Restoration, Prentice-Hall, New Jersey, 1977.
[7] R.R. Parenti, Lincoln Lab. J. 8 (1995) 29–48.
[8] C. Chen, in: Proceedings of the Twenty-Fourth Annual Allerton Conference on Communication, Control, and
Computing, Monticello, Illinois, pp. 665–674.
[9] L. Allen, The Hubble Space Telescope Optical Systems Failure Report, Technical Report NASA Technical
Report NASA-TM-103443, NASA, 1990.
[10] W.H. Richardson, J. Opt. Soc. Am. 62 (1972) 55–59.
[11] L.B. Lucy, Astron. J. 79 (1974) 745–754.
[12] D.B. Gennery, J. Opt. Soc. Am. 63 (1973) 1571–1577.
[13] H.C. Lee, Opt. Eng. 29 (1990) 405–421.
[14] T. Tani, Photographic Sensitivity: Theory and Mechanisms, Oxford University Press, New York, 1995.
[15] D. Joseph, S. Collins, IEEE Trans. Instrum. Meas. 51 (2002) 996–1001.
[16] M. Sato, S. Nagahara, K, Takahashi, Optical ﬁlter for color imaging device, 1986.
[17] D. Kessler, A.C.G. Nutt, R.J. Palum, Anti-aliasing low-pass blur ﬁlter for reducing artifacts in imaging
apparatus, U.S. Patent No. 6937283, 2005.
[18] T.J. Hebert, R. Leahy, IEEE Trans. Signal Process. 40 (1992) 2290–2303.
[19] F.-X. Dupe, J. Fadili, J.-L. Starck, IEEE Trans. Image Process. 18 (2009) 310–321.
[20] A.K. Jain, Fundamentals of Digital Image Processing, Prentice-Hall, Englewood Cliffs, NJ, 1989.
[21] A.K. Jain, Proc. IEEE 69 (1981) 502–528.
[22] K. Miller, SIAM J. Math. Anal. 1 (1970) 52–74.
[23] B.R. Hunt, IEEE Trans. Comput. C-22 (1973) 805–812.
[24] A.M. Thompson, J.C. Brown, J.W. Kay, D.M. Titterington, IEEE Trans. Pattern Anal. Mach. Intell. 13 (1991)
326–339.
[25] A.N. Tikhonov, V.Y. Arsenin, Solutions of Ill-Posed Problems, Winston, 1977.
[26] R.L. Lagendijk, J. Biemond, D.E. Boekee, IEEE Trans. Acoust. Speech Signal Process. 36 (1988) 1874–1888.
[27] A.K. Katsaggelos, Opt. Eng. 28 (1989) 735–748.
[28] M.R. Banham, A.K. Katsaggelos, IEEE Signal Process. Mag. 14 (1997) 24–41.
[29] G.H. Golub, M. Heath, G. Wahba, Technometrics 21 (1979) 215–223.
[30] R.S. Anderssen, P. Bloomﬁeld, Technometrics 16 (1974) 69–75.
[31] G. Wahba, SIAM J. Numer. Anal. 14 (1977) 651–667.
[32] P. Hall, D.M. Titterington, J. Roy. Statist. Soc. B 49 (1987) 184–198.
[33] H. Gfrerer, Math. Comput. 49 (1987) 507–522.
[34] J. Michaelsen, J. Climatol. Appl. Meteorol. 26 (1987) 1589–1600.
[35] H.W. Engl, H. Gfrerer, Appl. Numer. Math. 4 (1988) 395–417.

192
CHAPTER 6 Image Restoration: Fundamentals of Image Restoration
[36] A. Neubauer, Appl. Numer. Math. 4 (1988) 507–519.
[37] B. Shahraray, D.J. Anderson, IEEE Trans. Pattern Anal. Mach. Intell. 11 (1989) 600–610.
[38] S.J. Reeves, R.M. Mersereau, Opt. Eng. 29 (1990) 446–454.
[39] N.P. Galatsanos, A.K. Katsaggelos, IEEE Trans. Image Process. 1 (1992) 322–336.
[40] S.J. Reeves, IEEE Trans. Image Process. 3 (1994) 319–324.
[41] G. Wahba, Ann. Statist. 13 (1985) 1378–1402.
[42] R. Molina, A.K. Katsaggelos, J. Mateos, IEEE Trans. Image Process. 8 (1999) 231–246.
[43] A.M. Thompson, J. Kay, Inv. Prob. 9 (1993) 749.
[44] G. Archer, D.M. Titterington, IEEE Trans. Image Process. 4 (1995) 989–995.
[45] Y.C. Eldar, IEEE Trans. Signal Process. 57 (2009) 471–481.
[46] L. Rudin, S. Osher, in: Proceedings of IEEE International Conference Image Processing, ICIP-94, vol.
1, 1994, pp. 31–35.
[47] V.M. Patel, R. Maleh, A.C. Gilbert, R. Chellappa, IEEE Trans. Image Process. 21 (2012) 94–105.
[48] J. Skilling, S.F. Gull, Maximum-Entropy and Bayesian Methods in Inverse Problems, University of Wyoming,
Springer, pp. 83–132.
[49] S.J. Reeves, IEEE Trans. Image Process. 14 (2005) 1448–1453.
[50] A.V. Oppenheim, R.W. Schafer, T.G. Stockham Jr., Proc. IEEE 56 (1968) 1264–1291.
[51] C. Chen, M.I. Sezan, A.M. Tekalp, in: Proceedings of the IEEE International Conference on Acoustics, Speech,
and Signal Processing, 1987, pp. 1201–1204.
[52] A.M. Tekalp, H. Kaufman, J.W. Woods, in: IEEE Trans. Acoust. Speech Signal Process. ASSP-34 (1986)
963–972.
[53] M.J. McDonnell, R.H.T. Bates, Opt. Commun. 13 (1975) 347–349.
[54] M. Cannon, IEEE Trans. Acoust. Speech Signal Process. ASSP-24 (1976) 58–63.
[55] T.G. Stockham Jr., T.M. Cannon, R.B. Ingebretsen, Proc. IEEE 63 (1975) 678–692.
[56] J. Biemond, F.G. van der Putten, J.W. Woods, IEEE Trans. Circ. Syst. 35 (1988) 385–393.
[57] R.L. Lagendijk, A.K. Katsaggelos, J. Biemond, in: Proceedings of the IEEE International Conference on
Acoustics, Speech, and Signal Processing, 1988, pp. 992–995.
[58] R.L. Lagendijk, J. Biemond, D.E. Boekee, in: Proceedings of the IEEE International Conference on Acoustics,
Speech, and Signal Processing, 1989, pp. 1397–1400.
[59] R.L. Lagendijk, D.L. Angwin, H. Kaufman, J. Biemond, in: EUSIPCO 88—Proceedings of the Fourth Euro-
pean Conference on Signal Processing.
[60] R.L. Lagendijk, J. Biemond, D.E. Boekee, IEEE Trans. Acoust. Speech Signal Process. 38 (1990) 1180–1191.
[61] A.K. Katsaggelos, K.T. Lay, in: SPIE Visual Communications and Image Processing IV, vol. 1199.
[62] K.T. Lay, A.K. Katsaggelos, Opt. Eng. 29 (1990) 436–445.
[63] S.J. Reeves, R.M. Mersereau, IEEE Trans. Image Process. 1 (1992) 301–311.
[64] B.L.K. Davey, R.G. Lane, R.H.T. Bates, Opt. Commun. 69 (1989) 353–356.
[65] T.J. Newton, Blind Deconvolution and related topics, Ph.D. Thesis, Girton College, Cambridge, 1986.
[66] B.B. Newman, J. Hildebrandt, Aust. Comput. J. 19 (1987) 126–133.
[67] Y.-L. You, M. Kaveh, IEEE Trans. Image Process. 5 (1996) 416–428.
[68] D. Kundur, D. Hatzinakos, IEEE Trans. Signal Process. 46 (1998) 375–390.
[69] T.F. Chan, C.-K. Wong, IEEE Trans. Image Process. 7 (1998) 370–375.
[70] H. Liao, M.K. Ng, IEEE Trans. Image Process. 20 (2011) 670–680.
[71] T. Ma, S.J. Reeves, J. Electron. Imag. 20 (2011) 033013.
[72] A. Buades, B. Coll, J. Morel, Multiscale modeling and simulation 4 (2005) 490–530.
[73] K.Dabov,A.Foi,V.Katkovnik,K.O.Egiazarian,in:ImageProcessing:AlgorithmsandSystemsVI:Electronic
Imaging Symposium, vol. 6812, SPIE, San Jose, CA.

7
CHAPTER
Iterative Methods for Image
Restoration
Sebastian Berisha and James G. Nagy
Department of Mathematics and Computer Science, Emory University, Atlanta, GA, USA
4.07.1 Introduction
Image restoration is the process of reconstructing an approximation of an image from blurred and noisy
measurements. In this chapter, we use the standard linear image formation model,
g(x, y) =

R2 k(x, y; ξ, η) f (ξ, η)dξdη + η(x, y),
(7.1)
where f is the true object, g is the observed image, and η is additive noise. The kernel function k models
the blurring operation, and is called the point spread function (PSF). In many applications the kernel
satisﬁes k(x, y; ξ, η) = k(x −ξ, y −η), and the blur is said to be spatially invariant, otherwise the
blur is said to be spatially variant. In the case of spatially invariant blurs, the integration in Eq. (7.1) is
a convolution operation, and thus the image restoration problem is often referred to as deconvolution.
Typically we do not have a precise function deﬁnition for g because the observed image is recorded
digitally, and thus is known only at discrete values. Moreover, in many cases it is necessary to estimate
k from measured data. Therefore, it is natural to consider the digital image restoration problem
g = Kf true + η,
(7.2)
which is obtained from Eq. (7.1) by discretizing the functions and approximating integration with a
quadrature rule. If the images are assumed to have m × n pixels, then K ∈Rmn×mn and g, f, η ∈Rmn.
Two aspects of the digital image restoration problem (7.2) make it computationally challenging:
•
In most image restoration problems involving images with m × n pixels, K is an N × N matrix with
N = mn = number of pixels in the image.1 For example, if m = n = 103, then K is a 106 × 106
matrix. Thus, the problem is large-scale. Fortunately the matrix K can usually be represented by
a compact data structure, such as when the blur is spatially invariant. Approximation techniques
for more complicated spatially variant blurs include geometrical coordinate transformations [1–3],
sectioning [4–6], PSF interpolation [7–10], and in some cases a sparse matrix data structure can be
used for K [11,12].
1There are situations, such as multi-frame problems, where K is M × N with M ̸= N. The algorithms discussed in this
chapter are not restricted to square matrices, and can be applied to these situations as well.
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00007-8
© 2014 Elsevier Ltd. All rights reserved.
193

194
CHAPTER 7 Iterative Methods for Image Restoration
•
The matrix K is severely ill-conditioned, with singular values decaying to, and clustering at 0. This
means that regularization is needed to avoid computing solutions that are corrupted by noise. Reg-
ularization can be enforced through well-known techniques such as Wiener ﬁltering and Tikhonov
regularization, and/or by incorporating constraints such as nonnegativity [13–16].
Efﬁcient implementation of an image restoration algorithm is obtained by exploiting structure of the
matrix K. For example, if the blur is spatially invariant and we assume periodic boundary conditions,
then K is a block circulant matrix with circulant blocks. In this case many image restoration algorithms,
such as the Wiener ﬁlter, can be implemented in the Fourier domain, using fast Fourier transforms
(FFT) [17,18]. If spatial invariance is a poor approximation of the actual blur, or periodic boundary
conditions are poor approximations to the actual true image scene, then the quality of reconstructions
will be limited. Moreover, it is not possible to incorporate additional constraints, such as nonnegativity,
into simple ﬁltering methods.
Iterative image restoration algorithms have many advantages over simple ﬁltering techniques [16,19,
20]. Iterative methods can be very efﬁcient for spatially invariant as well as spatially variant blurs, they
can incorporate a variety of regularization techniques and boundary conditions, and can more easily
incorporateadditionalconstraints,suchasnonnegativity[21,22].Thecostofaniterativeschemedepends
on the amount of computation needed per iteration, as well as on the number of iterations needed to reach
a good restoration of the image. Convergence can be accelerated using preconditioning, but if not done
carefully,itcanleadtoerraticconvergencebehaviorthatresultsinfastconvergencetoapoorapproximate
solution. In this chapter, we describe a variety of iterative methods that can be used for image restoration,
and also describe some preconditioning techniques that can be used to accelerate convergence. We show
thatmanywell-knowniterativemethodscanbeviewedasabasicmethodwithaparticularpreconditioner.
This viewpoint provides a natural mechanism to compare a variety of iterative methods.
4.07.2 Background
In this section, we present the essential background material needed to develop and implement iterative
image restoration methods, focusing on approaches that have the following general form:
f 0 = initial estimate of f true
for k = 0, 1, 2, . . .
•
f k+1 = computations involving f k, K
and other intermediate quantities
•
determine if stopping criteria are satisﬁed
end
Most well-known iterative methods have this basic form, including conjugate gradient type schemes,
the expectation-maximization method (sometimes referred to as the Richardson-Lucy method), and
many others. Since any one iterative method is not optimal for all image restoration problems, the study
of iterative methods is an important and active area of research. The speciﬁc computational operations
that are required to update f k+1 at each iteration depend on the particular iterative scheme being used,

4.07.2 Background
195
but the most intensive part of these computations usually involves matrix vector products with K and
its transpose, KT.
The rest of this section is outlined as follows. In Section 4.07.2.1, we provide a brief introduction
to regularization, and in Section 4.07.2.2, we discuss efﬁcient approaches to perform matrix-vector
multiplications with K for various types of blurs, including spatially invariant and spatially variant. In
Section 4.07.2.3, we describe a general preconditioning approach.
4.07.2.1 Regularization
The continuous image formation model (7.1) is useful for analysis purposes, since it is a classic example
of an ill-posed inverse problem, and a substantial amount of research has been done to understand the
properties of such problems; see, for example [13,15,16,19]. We will not discuss these theoretical
properties in detail, but we do mention that even in the continuous problem there may not be a unique
function f that solves Eq. (7.1) and small changes in the noise η can cause arbitrarily large changes in
f [13,15,16,23].
The discrete problem (7.2) inherits these properties, which may be summarized as follows.
Deﬁne gtrue = Kf true to be the noise-free blurred image, and let K = UVT be the singular value
decomposition of the N × N matrix K. That is, U and V are orthogonal matrices and  is a diagonal
matrix with entries satisfying σ1 ≥σ2 ≥· · · ≥σN ≥0. If K arises from discretizing the integral Eq.
(7.1), then, assuming the problem is scaled so that σ1 = 1, we have:
•
The singular values, σi, decay to, and cluster at 0, without a signiﬁcant gap to indicate numerical rank.
•
The components |uT
i gtrue|, where ui is the ith column of U, decay on average faster than the singular
values σi. This is referred to as the discrete Picard condition [14].
•
The singular vectors vi (i.e., the columns of V) corresponding to small singular values tend to have
more oscillations than the singular vectors corresponding to large singular values.
With these properties we can easily see what effect the error term η has on the inverse solution:
f inv = K−1g
= K−1(gtrue + η)
= V−1UT(gtrue + η)
=
N

i=1
uT
i (gtrue + η)
σi
vi
=
N

i=1
uT
i gtrue
σi
vi +
N

i=1
uT
i η
σi
vi
= f true + error.
From this relation we can see that the high frequency components in the error are highly magniﬁed by
division of small singular values. The computed inverse solution is dominated by these high frequency
components, and is in general a very poor approximation of the true solution, f true.
In order to compute an accurate approximation of f true, or at least one that is not horribly corrupted
by noise, the solution process must be modiﬁed. This process is usually referred to as regularization

196
CHAPTER 7 Iterative Methods for Image Restoration
[13,14,16,23]. One class of regularization methods, called ﬁltering, can be formulated as a modiﬁcation
of the inverse solution [14]. Speciﬁcally, a ﬁltered solution is deﬁned as
f ﬁlt =
N

i=1
φi
uT
i g
σi
vi,
(7.3)
where the ﬁlter factors, φi, satisfy φi ≈1 for large σi, and φi ≈0 for small σi. That is, the large singular
value components of the solution are reconstructed, while the components corresponding to the small
singular values are ﬁltered out.
It is sometimes possible to replace the singular value decomposition with the alternative factorization,
K = QHQ,
where  is a diagonal matrix, and QH is the Hermitian (complex conjugate) transpose of Q, with
QHQ = I. We refer to this as a spectral value decomposition; the columns of Q are eigenvectors and
the diagonal elements of  are the eigenvalues of K. Although every matrix has an SVD, only normal
matrices (i.e., matrices that satisfy KTK = KKT) have a spectral value decomposition. However, if K has
a spectral factorization, then it can be used, in place of the singular value decomposition, to implement
the ﬁltering methods described above. The advantage is that it is sometimes more computationally
convenient to compute a spectral value decomposition than it is a singular value decomposition; for
example in the case of a spatially invariant blur with periodic boundary conditions, the eigenvalues are
obtained by taking a Fourier transform of the PSF (this is often also referred to as the optical transfer
function, or OTF), the eigenvectors of K are the Fourier vectors, and fast Fourier transforms (FFT) can be
used for operations involving Q (forward Fourier transform) and QH (inverse Fourier transform) [17,18].
Generally we will not distinguish between a singular or spectral value decomposition, and use the
generic acronym SVD to mean methods based on one or the other. We remark that different choices
of ﬁlter factors lead to different methods; popular choices are the pseudo-inverse (or truncated SVD),
Tikhonov, and Wiener ﬁlters [14,16,24]. Although the focus of this chapter is on problems for which
it is not computationally feasible to compute an SVD of K, we do consider using SVD approximations
to construct preconditioners; this will be discussed in more detail in Section 4.07.2.3.
If it is not computationally feasible to use an SVD based ﬁltering method then it may be preferable
to use a variational form of regularization, which amounts to computing a solution of
min
f

∥g −Kf∥2
2 + λ2J ( f)

,
(7.4)
where the regularization operator J and the regularization parameter λ must be chosen. The variational
form provides a lot of ﬂexibility. For example, one could include additional constraints on the solution,
such as nonnegativity, or it may be preferable to replace the least squares criterion with the Poisson log
likelihood function [25–27]. As with ﬁltering, there are many choices for the regularization operator,
J , such as:
•
Tikhonov regularization [23,28–30]: J (f) = ∥Lf∥2
2.
•
Total variation [16,31,32]: J ( f) =


Dh f
	2 +

Dv f
	2

1
.
•
Sparsity constraints [33–35]: J ( f) = ∥f∥1.

4.07.2 Background
197
Tikhonov regularization, which was ﬁrst proposed and studied extensively in the 1960s and 1970s
[28–30,36,37], is perhaps the most well known approach to regularizing ill-posed problems. L is typ-
ically chosen to be the identity matrix, or a discrete approximation to a derivative operator, such as
the Laplacian. If L = I, then it is not difﬁcult to show that the resulting variational form of Tikhonov
regularization, namely
min
f

∥g −Kf∥2
2 + λ2∥f∥2
2

,
(7.5)
can be written in an equivalent ﬁltering framework by replacing K with its SVD [14].
In the case of total variation regularization, Dh and Dv denote discrete approximations of horizontal
and vertical derivatives of the 2D image f, and the approach extends to 3D images in an obvious way.
Efﬁcient and stable implementation of total variation regularization is a nontrivial problem; see [16,31]
and the references therein for further details.
In the case of sparse reconstructions, the matrix  represents a basis in which the image, f, is sparse.
For example, for astronomical images that contain a few bright objects surrounded by a signiﬁcant
amount of black background, an appropriate choice for  might be the identity matrix. Clearly, the
choice of  is highly dependent on the structure of the image f. The usage of sparsity constraints for
regularization is currently a very active ﬁeld of research, with many open problems [34].
Iterative methods applied directly to the least squares problem
min
f
∥g −Kf∥2
offer yet another choice of enforcing regularization. If the iterative method is applied directly to this
least squares problem then the iteration converges to f inv = K−1g, which, as was discussed above, is
typically a poor approximation of f true. Fortunately, many iterative methods exhibit a semi-convergence
behavior with respect to the relative error,
∥f k −f true∥2
∥f true∥2
,
(7.6)
where f k is the approximate solution at the kth iteration. Although this is discussed in more detail
in Section 4.07.4, here it is worth mentioning that the term semi-convergence refers to the property
that in the early iterations the relative error begins to decrease and, after some “optimal” iteration, the
error then begins to increase. By stopping the iteration when the error is low, we obtain a regularized
approximation of the solution. Because the true object, f true, is not known, the relative error (7.6) cannot
be used as a practical measure for determining a stopping iteration, and alternative approaches, such as
methods based on knowledge of the noise (e.g., discrepancy principle) must be used.
As was mentioned in the previous paragraph, in the case of iterative regularization, it is necessary
to have a practical approach to determine an appropriate stopping iteration. In fact, each of the regular-
ization methods discussed above require choosing a regularization parameter. For ﬁltering methods,
one has to decide what is meant by “large” and “small” singular values. In the variational form, one
has to choose the scalar λ, where as for iterative regularization methods the index where the iteration is
terminated plays the role of the regularization parameter. In each case, it is a nontrivial matter to choose
an “optimal” regularization parameter. Although there are methods (e.g., generalized cross validation
[38], discrepancy principle [13], L-curve [14], L-ribbon [39], and many others [13,14,16]) that may be

198
CHAPTER 7 Iterative Methods for Image Restoration
used to estimate the regularization parameter, it may be necessary to solve the problem for a variety of
parameters, and use knowledge of the application to help decide which solution is best.
We also mention that when the majority of the elements in the image f are zero or near zero, as may
be the case for astronomical or medical images, it may be wise to enforce nonnegativity constraints
on the solution [16,26,27]. This requires that each element of the computed solution f is not negative,
which is often written as f ≥0. Though these constraints add a level of difﬁculty when solving, they
can produce results that are more feasible than when nonnegativity is ignored. Some iterative methods
that enforce nonnegativity will be discussed in Section 4.07.5.
4.07.2.2 Matrix structures and matrix-vector multiplications
The efﬁciency of iterative methods depends on the number of iterations needed to compute the desired
solution, as well as on the cost required for each iteration. The computational cost of each iteration can
depend on many factors, but all methods will require matrix-vector multiplication with K, and often also
with its transpose KT. Because the size of K is extremely large, an efﬁcient storage scheme, exploiting
structure and sparsity, should be used. In this subsection we brieﬂy discuss structures that arise most
often in image restoration problems.
In most of our discussion, we assume images are represented as vectors, for example by storing the
pixel values in lexicographical order. This is mathematically convenient when describing algorithms
using linear algebra notation. However, we will sometimes need to think of images as 2-dimensional
arrays (i.e., m×n matrices). It will therefore be helpful to have notation that describes the transformation
from image array to vector, and from vector to image array. Speciﬁcally, if F is an m × n image array
of pixels, then we use the notation
ν(F) = f ∈Rmn
to transform the image array into a vector with mn elements. Similarly, we will use μ(·) to denote the
inverse transformation from a vector to an array,
μ( f) = F ∈Rm×n.
Of course, f = ν(μ( f)) and F = μ(ν(F)).
4.07.2.2.1
Boundary conditions
Pixels near the edges of a blurred image are likely to have been affected by information outside the
ﬁeld of view. This situation often causes ringing artifacts in deblurred images. These ringing artifacts
can be reduced by incorporating boundary conditions, which are used to approximate the image scene
outside the ﬁeld of view. Boundary conditions can be incorporated explicitly in the matrix K, but for
iterative methods it is usually easier to pad images before performing the matrix-vector multiplication.
Commonly used boundary conditions include:
•
Periodic boundary conditions imply that the image repeats itself in all directions.
•
Zero boundary conditions imply a black boundary, so that the pixels outside the borders of the image
are zero.
•
Replicating boundary conditions repeat the boarder elements outside the ﬁeld of view.
•
Reﬂective boundary conditions imply that the scene outside the image boundaries is a mirror image
of the scene inside the image boundaries.

4.07.2 Background
199
periodic
zero
replicating
reﬂective
FIGURE 7.1
Examples of padding to incorporate boundary conditions. The top row is the image before padding and the
bottom row shows the result after padding. The red dotted line indicates the boundary of the image before
padding. (For interpretation of the references to color in this ﬁgure legend, the reader is referred to the web
version of this book.)
Figure 7.1 illustrates these choices for boundary conditions with a particular image. There are many
other choices for boundary conditions; see, for example, [40,41]. We have found that reﬂective boundary
conditions are often better than either periodic, zero and replicating, and they are relatively easy to
implement. Thus, reﬂective boundary conditions are the default we use in our software. However, we
also mention work by Reeves [42], which can be implemented very efﬁciently if the blur is spatially
invariant and the support of the PSF is fairly narrow. Speciﬁcally, the approach decomposes the problem
into a sum of two independent restorations, one uses a standard FFT-based ﬁltering algorithm, while the
otherreconstructsvaluesassociatedwiththeunknownpixelsoutsidetheboundaryoftheobservedimage.
4.07.2.2.2
Spatially invariant blurs
In the case of spatially invariant blurs, the matrix K generally has a Toeplitz structure, but this can
depend on the imposed boundary condition [18]. However, the details are not extremely important for
iterative methods because we need only perform matrix-vector multiplications with K and KT. Consider
the multiplication
Kf.
If the blur is spatially invariant and periodic boundary conditions are used, then this multiplication can
be done by simply convolving the PSF (which deﬁnes K) with f. If we want to enforce a different
boundary condition, then we simply pad the vector f, as described in the previous subsection, convolve
the PSF with this padded object, and then extract the portion of the result corresponding to the ﬁeld
of view. Note that this approach to compute the matrix-vector multiplication requires storing only the
PSF, and does not require explicitly constructing the mn × mn matrix K. The convolution of the PSF

200
CHAPTER 7 Iterative Methods for Image Restoration
and the padded vector can be done very efﬁciently using FFTs; see, for example [22]. Matrix-vector
multiplications with KT is done similarly.
4.07.2.2.3
Locally spatially invariant blurs
In the case of spatially variant blurs, there is not a single PSF that can be used to represent the blurring
operation; point sources in different locations of the image may result in different PSFs. In the most
general case, there is a different PSF for each pixel in the image. It is not computationally feasible
to construct all of these PSFs, so other approaches to representing K need to be considered. Here we
consider situations where there is a continuous and smooth variation in the PSF with respect to the
position of the point source, such as is shown in Figure 7.2.
In this case it may be appropriate to assume that the blur is approximately locally spatially invariant.
Exploiting local spatial invariance was considered in [5,6]. Here we describe an approach where K is
approximated with interpolation [7–10,22]. Speciﬁcally, by partitioning the image into regions, and
assuming that a spatially invariant PSF can be obtained for each region, the blurring matrix can be
approximated by
K ≈
r

i=1
KiDi,
(7.7)
where Ki is deﬁned by the PSF corresponding to the point source located at the center of the ith region
and r is the overall number of regions. The masking matrix Di is diagonal, with D1 + · · · + Dr = I.
In the case of piecewise constant interpolation, the diagonal entries of Di are equal to one for pixels
in region i, and zero otherwise. Higher order interpolation can also be used, but requires additional
computational cost [9,10].
FIGURE 7.2
PSFs for a spatially variant blur that changes smoothly and continuously with respect to position of points
sources.

4.07.2 Background
201
4.07.2.2.4
Sparse spatially variant blurs
If there is more severe spatial variance that cannot be approximated well by the local spatially invariant
model, then it is important to exploit other structure in the problem. In some cases (we will see a speciﬁc
example in Section 4.07.3.4) it is possible to use a sparse matrix data format, such as compressed row
[43], to represent K. This requires only keeping track of the nonzero entries and their locations in the
matrix K. Efﬁcient multiplications can be done with such matrices; see, for example [43].
4.07.2.2.5
Separable blurs
In some cases, the horizontal and vertical components of the blur can be separated. In this case, K can
be decomposed as a Kronecker product,
K = Kh ⊗Kv.
If the image has m × n pixels, then Kv ∈Rm×m represents the vertical component of the blurring,
Kh ∈Rn×n represents the horizontal component of the blurring, and
Kf =

Kh ⊗Kv
	
f = ν

Kvμ(f)KT
h

,
where we recall that μ(·) transforms a vector to an image array, and ν(·) transforms an image array to
a vector. Usually it is not computationally practical to explicitly construct the full matrix K, however
it is not so difﬁcult to explicitly construct the much smaller matrices Kh and Kv, and by exploiting
properties of Kronecker products, it is possible to efﬁciently implement SVD based ﬁltering methods
[18]. Although in most realistic situations the blur is unlikely to be exactly separable, it is possible to
efﬁciently compute separable approximations of K, which can be used to construct preconditioners.
4.07.2.3 Preconditioning
Preconditioning is a classical approach used in many areas of scientiﬁc computing to accelerate conver-
gence of iterative methods. Much is known about constructing effective preconditioners for well-posed
problems [44–46]. However, if not done carefully for ill-posed problems, such as image restoration,
preconditioning can lead to erratic convergence behavior that results in fast convergence to a poor
approximate solution. Some work has been done to overcome these difﬁculties for problems where the
blurring operator is known [47–49], including for spatially variant blurs [9,10,50], and multi-frame
problems [51]. Although there is an additional cost when using preconditioning, for typical iterative
methods the number of iterations can be reduced dramatically, resulting in a substantial reduction in
overall cost of the iterative scheme. We discuss some of these issues in this section, and describe a
general approach to construct preconditioners for image restoration.
Speed of convergence of iterative methods is typically dictated by certain spectral properties of the
matrix K. Preconditioning refers to a process of modifying the spectral properties of the matrix to
accelerate convergence, and is often presented in the context of solving linear systems Kf = g. The
standard approach to preconditioning is to construct a matrix, R, that satisﬁes the following properties:
•
It should be relatively inexpensive to construct R.
•
It should be relatively inexpensive to solve linear systems of the form Rz = w and RTz = w.

202
CHAPTER 7 Iterative Methods for Image Restoration
•
The preconditioned system should satisfy K = KR−1 ≈I (right preconditioning) or K = R−TK ≈I
(left preconditioning).
Then instead of applying the iterative method to the linear system Kf = g, we apply it to a modiﬁed
system Kˆf = ˆg, where
Right preconditioning: K = KR−1,
ˆf = Rf,
ˆg = g,
Left preconditioning:
K = R−TK,
ˆf = f,
ˆg = R−Tg.
One can also use a combination of right and left preconditioning, but for this brief discussion we focus
only on one sided preconditioning.
If the iterative method is applied to the preconditioned system, the most intensive part of the com-
putation at each iteration is matrix-vector multiplications with K and KT, or equivalently, matrix-vector
multiplications with K and KT and linear system solves with R and RT. Thus, the ﬁrst two of the afore-
mentioned requirements in constructing a preconditioner are related to the additional computational
costs of preconditioning; constructing R is a one time cost, where as solving linear systems with matri-
ces R and RT are required at each iteration. The last requirement determines the speed of convergence;
better approximations K ≈I, usually imply faster convergence.
Note that if we design a preconditioner such that the singular values of K are clustered around 1, then
K ≈I. That is, more singular values clustered around one, as well as tighter clusters, usually implies
fasterconvergence.Althoughthisapproachworkswellforwell-posedproblems,itdoesnotworkwellfor
ill-posedproblemssuchasimagerestoration,unlessregularizationhasalreadybeenincorporatedintothe
matrix. For example, if we apply the iterative method to the Tikhonov regularized least squares problem
min
f

 g
0

−
 K
λL

f

2
then clustering the singular values of
 K
λL

R−1
is the right idea.
However, if the iterative method is applied directly to
min
f
∥g −K f∥2,
as in the case of iterative regularization (recall the discussion in Section 4.07.2.1), then clustering all
singular values around 1 will likely lead to very poor reconstructions. Indeed the SVD analysis out-
lined in Section 4.07.2.1 suggests that the large singular values correspond to signal information we
want to reconstruct, while small singular values correspond to noise information we do not want to
reconstruct. By clustering all singular values around one, the signal and noise information becomes
mixed together, and it is impossible for the iterative method to distinguish between signal informa-
tion and noise information. In this situation, we get fast convergence to the (noise corrupted) inverse
solution.

4.07.2 Background
203
Another difﬁculty, in the case of left preconditioning, is the risk of changing the statistical charac-
teristics of the problem. In particular, with left preconditioning, the problem is modiﬁed as:
R−Tg = R−TKf + R−Tη.
Thus, if an iterative method (such as maximum likelihood) implicitly assumes particular statistical
characteristics of the data and noise, then the left preconditioned system may not continue to have these
properties.
We will describe some speciﬁc preconditioning approaches when we begin to introduce particular
iterative methods. As we will see, many well-known iterative methods are simply variations of the same
basic scheme but with different preconditioners.
4.07.2.4 MATLAB notes
Implementation of even the most basic iterative method for image restoration is not trivial, especially if
we want to have the ﬂexibility to use a variety of blurring operators and boundary conditions. We have
developed a MATLAB toolbox to simplify the process of using iterative methods for image restoration.
Thesoftwarecanbefoundat www.mathcs.emory.edu/∼nagy/RestoreTools,withtheoriginalimplemen-
tation described in [22]. A signiﬁcant update to this software has been done while preparing this chapter.
The software uses an object oriented approach to hide the difﬁcult implementation details from the
user. Throughout this chapter we include some information on the important aspects of the software in
case readers wish to test our iterative methods and preconditioning techniques on their own data. We
also provide sample test data with the software; this test data is described in Section 4.07.3. We omit
most of the implementation details, and instead just provide an overview of the more important tools in
the software, and examples on how to use them.
4.07.2.4.1
Spatially invariant blurs
For iterative methods, probably the most important object is the psfMatrix, which deﬁnes the matrix
K implicitly using a compact data structure. In the case of spatially invariant blurs, we assume that there
is an array containing a PSF and there is a vector containing the row and column indices of the location
of the corresponding point source (i.e., the center of the PSF). For example, if a 256×256 PSF resulted
from a point source located at row 128 and column 127, then we would deﬁne a vector.
≫center = [128, 127];
With this data, the simple MATLAB statement.
≫K = psfMatrix(PSF, center);
constructs an object containing information about the blurring operator. Some preprocessing is done
to prepare K for efﬁcient matrix-vector multiplications, and the “∗” operator is overloaded so that an
operation such as g = Kf can be computed with the simple statement:
≫g = K*f;
Operatoroverloadingallowsforimplementingiterativemethodsusingstandard MATLABlinearalgebra
operations, such as the matrix-vector multiplication K*f, just as if K was an explicitly deﬁned matrix.

204
CHAPTER 7 Iterative Methods for Image Restoration
The above example assumes compatibility between K and f. That is, f can be either an m × n image
array, in which case after multiplication g is also an image of the same size, or f can be a vectorized
form of the image, in which case after multiplication g is also a vector. We remark that the constructor
routine psfMatrix can accept additional input parameters, including a boundary condition. The
default boundary condition is ‘reflective’, which is typically much better than zero and periodic
boundary conditions at reducing ringing effects if signiﬁcant details are located near the edges of the
observed image.
4.07.2.4.2
Locally spatially invariant blurs
For locally spatially invariant blurs, we assume that the data PSF and center are cell arrays containing
PSFs and corresponding point source locations for various regions of the image. The dimension of the
cell arrays are assumed to be the same as the region partitioning of the image. That is, for example, if
the image is partitioned into 6 × 6 regions of size k × k each, and a PSF is taken from each region, then
we store all of the PSFs in a 6 × 6 cell array. Similarly we store the coordinates of each point source
in a 6 × 6 cell array. If these cell arrays are denoted as PSF and center, then we can use any of the
same statements used to construct the psfMatrix, and to perform matrix-vector multiplications as
we used in the spatially invariant case described above.
We again emphasize that an advantage of using this object oriented approach, with operator over-
loading, is that iterative methods developed in the scientiﬁc computing community typically use matrix-
vector notation. With our psfMatrix, these iterative methods can be easily used for image restoration
problems.
4.07.2.4.3
Sparse spatially variant blurs
In the case of sparse spatially variant blurs, we simply make use of MATLAB’s very efﬁcient built-in
sparse matrix tools. An example of this is given in Section 4.07.3.4.
4.07.2.4.4
Separable blurs
In the case of separable blurs, we have implemented a kronMatrix object that efﬁciently works with
Kronecker products. In general, we do not expect a blur to be separable, so this object is mainly used
to implement certain preconditioners; this is discussed in more detail in Section 4.07.2.3.
4.07.3 Model problems
In this section, we describe several examples that can be used to test the iterative methods and precon-
ditioning techniques described in this chapter. These examples include spatially invariant and spatially
variant blurs of the types described in the previous section.
4.07.3.1 Spatially invariant Gaussian blur
Gaussian PSFs are often used to test image deblurring methods. A general Gaussian PSF is given by
k(s, t) =
1
2π√γ exp

−1
2
 s t 
C−1
s
t

,
(7.8)

4.07.3 Model Problems
205
where
C =

α2
1 ρ2
ρ2 α2
2

,
and γ = α2
1α2
2 −ρ4 > 0.
The shape of the Gaussian PSF depends on the parameters α1, α2, and ρ. If ρ = 0, then
k(s, t) =
1
2πα1α2
exp

−1
2

s2
α2
1
+ t2
α2
2

and if, additionally, α ≡α1 = α2, then
k(s, t) =
1
2πα2 exp

−1
2α2

s2 + t2
.
Notice that in the simplest case, where ρ = 0 and α1 = α2, the PSF is circularly symmetric and
separable (that is, the blur can be decomposed into a product of two 1-dimensional blurs, one each
for the horizontal and vertical directions). Figure 7.3 shows three examples of Gaussian PSFs, and
corresponding blurred images are shown in Figure 7.4. The PSFs are displayed using a false colormap,
rather than in grayscale, for better visualization.
5
10
15
20
25
30
5
10
15
20
25
30
0
1
2
3
4
5
6
7
8
9
x 10
−3
5
10
15
20
25
30
5
10
15
20
25
30
0
0.002
0.004
0.006
0.008
0.01
0.012
0.014
0.016
0.018
5
10
15
20
25
30
5
10
15
20
25
30
0
0.005
0.01
0.015
0.02
5
10
15
20
25
30
5
10
15
20
25
30
5
10
15
20
25
30
5
10
15
20
25
30
5
10
15
20
25
30
5
10
15
20
25
30
α1 = α2 = 4, ρ = 0
α1 = 4, α2 = 2, ρ = 0
α1 = 4, α2 = 2, ρ = 2
FIGURE 7.3
Examples of Gaussian PSFs. The top row is a mesh plot of the PSF, and the bottom row shows how the
contours differ for various values of α1, α2, and ρ (a false colormap is used, rather than grayscale, for better
visualization). Images blurred with these PSFs are shown in Figure 7.4.

206
CHAPTER 7 Iterative Methods for Image Restoration
α 1 = α 2 = 4, ρ = 0
α 1 = 4, α 2 = 2, ρ = 0
α 1 = 4, α 2 = 2, ρ = 2
FIGURE 7.4
Examples of blurred images using the Gaussian PSFs shown in Figure 7.3. The image in the top row is the
true object, and the bottom row shows the blurred images.
We remark that in generating the simulated blurred images shown in Figure 7.4, we actually used
a true image with 618 × 600 pixels. After convolving a 256 × 256 Gaussian PSF with this image, we
cropped the result down to size 256×256. This approach simulates obtaining a ﬁnite dimensional image
of an inﬁnite scene, and will allow us to illustrate how the choice of boundary condition can affect the
quality of the restored image.
4.07.3.2 Spatially invariant atmospheric turbulence blur
When imaging objects in space using ground based telescopes, the PSF depends on the wavefront of
incoming light at the telescope’s mirror; if the wavefront function is known, then k is known. More
speciﬁcally, k(x, y; ξ, η) = k(x −ξ, y −η), with
k(s, t) =
F−1 
P(s, t)ei(1−ω(s,t))
2
=
F−1 
P(s, t)eiφ(s,t)
2
,
(7.9)
where ω(s, t) is a function that models the shape of the wavefront of incoming light at the telescope,
i = √−1, P(s, t) is a characteristic function that models the shape of the telescope aperture (e.g., a
circle or an annulus), F−1 is the 2-dimensional inverse Fourier transform, and φ(s, t) = 1 −ω(s, t) is
the phase error, or the deviation from planarity of the wavefront ω.

4.07.3 Model Problems
207
In the ideal situation, where the atmosphere causes no distortion of the incoming wavefront,
ω(s, t) = 1 and φ(s, t) = 0. In this diffraction limited case,
k0(s, t) =
F−1 {P(s, t)}

2
,
where P(s, t) is the pupil aperture function. Note that if P(s, t) = 1 for all s and t, then k0(s, t) is
a delta function, and (except for noise) there is no distortion in the observed image g. However, in a
realistic situation, P(s, t) = 1 in at most a ﬁnite region (e.g., within a circle or annulus deﬁned by the
telescope aperture), and thus it is impossible to obtain a perfect image. The best result we can hope to
obtain is the noise free, diffraction limited image
f0(x, y) =

R2 k0(x −ξ, y −η) f (ξ, η).
The distortion of the wavefront from atmospheric turbulence depends on many factors, includ-
ing weather, temperature, wavelength, and the diameter of the telescope. For example, viewing objects
directly above the telescope site on a clear night will have signiﬁcantly better seeing conditions than look-
ing during daylight hours at objects close to the horizon. Astronomers often quantify seeing conditions
in terms of the ratio d/r0, where d is the diameter of the telescope and r0 is called the Fried parameter,
which is related to the wavelength, and provides a statistical description of the level of atmospheric tur-
bulence at a particular site [52]. It is not essential to understand the precise deﬁnitions and characteristics
of the Fried parameter, except that:
•
Good seeing conditions correspond to “small” d/r0, such as d/r0 ≈10.
•
Poor seeing conditions correspond to “large” d/r0, such as as d/r0 ≈50.
Figure 7.5 shows examples of wavefronts and corresponding PSFs for d/r0 = 10, 30, 50. The wave-
fronts and PSFs are displayed using a false colormap, rather than in grayscale, for better visualization.
Note that the proﬁle of the wavefronts looks similar, but the color bar shows that there is substantially
more ﬂuctuation in the wavefront during poor seeing conditions. Blurring caused by these PSFs is
illustrated in Figure 7.6.
4.07.3.3 Spatially variant Gaussian blur
It is more difﬁcult to simulate a spatially variant blur, but one fairly simple example can be obtained by
modifying the Gaussian PSF to include spatial variation as follows:
k(s, t) =
1
2πα1(s)α2(t) exp

−1
2

s2
α2
1(s) +
t2
α2
2(t)

.
(7.10)
Spatially variant Gaussians have been used to test image restoration algorithms, see for example [53,54].
Because the blur is spatially variant, one cannot use simple convolution to apply the blur. However, in
this case the blur is separable,
k(s, t) =

1
√
2πα1(s)
exp

−1
2

s2
α2
1(s)
 
1
√
2πα2(t)
exp

−1
2

t2
α2
2(t)

,

208
CHAPTER 7 Iterative Methods for Image Restoration
d/r0 = 10
d/r0
d/r0
= 30
= 50
−10
−8
−6
−4
−2
0
2
4
6
8
10
−25
−20
−15
−10
−5
0
5
10
15
20
−40
−30
−20
−10
0
10
20
30
FIGURE 7.5
Examples of wavefronts (top row) and PSFs (bottom row) for d/r0 = 10, 30, 50 (a false colormap is used,
rather than grayscale, for better visualization). Blurred images for these PSFs are shown in Figure 7.6.
and so the matrix K can be represented as a Kronecker product,
K = Kh ⊗Kv,
where we refer readers to the discussion of separable blurs in Section 4.07.2.2.5. Usually it is not
computationally practical to explicitly construct the full matrix K, however it is not so difﬁcult to
explicitly construct the much smaller matrices Kh and Kv.
The amount of variation in the blur is deﬁned by α1(s) and α2(t). Note that if these are constants,
then we get a spatially invariant Gaussian blur discussed in Section 4.07.3.1. We illustrate with three
different examples:
•
First, we consider a case in which the center of the image has only a mild distortion, and the blur
becomes more severe as we move away from the center of the image. This can be simulated by using:
α1(s) = 3
4(3|s| + 1) and α2(t) = 3
4(3|t| + 1),
where we assume −1 ≤s, t ≤1.
•
Next, we consider the alternative situation where the blur is more severe near the center of the image,
and becomes less so near the edges. This can be simulated using:
α1(s) = 3
4(−3|s| + 4) and α2(t) = 3
4(−3|t| + 4),
where we assume −1 ≤s, t ≤1.

4.07.3 Model Problems
209
d/r0 = 10
d/r0 = 30
d/r0 = 50
FIGURE 7.6
Examples of blurred images using PSFs shown in Figure 7.5. The image in the top row is the true object,
and the bottom row shows the blurred images.
•
Finally, we consider a situation where there is only a small amount of blurring in the lower right
portion of the image, and it becomes more severe as we move toward the upper left corner of the
image. This can be simulated using:
α1(s) = 3 −s
2
and α2(t) = 3 −t
2,
where we assume −1 ≤s, t ≤1.
To visualize the variation in the blur, Figure 7.7 shows blurred point sources in various locations in the
image domain. Examples of blurred images with these spatially variant PSFs are shown in Figure 7.8.
The purpose of this example is to create data that can be used to test the approximation of spatially
variant blur with a locally spatially invariant model, as described in Section 4.07.2.2.3. That is, because
we are unlikely to have an explicit formula for the PSF, such as is given by Eq. (7.10), we approximate
K using the interpolation model given in Eq. (7.7). Recall that to form such an approximation, all that
is needed is a set of PSFs in different regions of the spatial domain.
4.07.3.4 Spatially variant motion blur
In this section, we consider blur caused by motion of a rigid object, or equivalently motion blur caused
by rigid movements of the imaging device. If the relative motion has constant speed and direction, then

210
CHAPTER 7 Iterative Methods for Image Restoration
case 1
case 2
case 3
FIGURE 7.7
Spatially variant Gaussian blurred images of point sources in various regions of the image domain. Blurred
images for these PSFs are shown in Figure 7.8.
case 1
case 2
case 3
FIGURE 7.8
Examples of blurred images using spatially variant Gaussian PSFs shown in Figure 7.7. The image in the
top row is the true object, and the bottom row shows the blurred images.

4.07.3 Model Problems
211
the blurring operation is spatially invariant. However, if the speed and direction varies during the image
acquisition time, then the blur is spatially variant, and difﬁcult to describe with a concise mathematical
formula, as in the case of the spatially variant Gaussian blur. However, if it is possible to continuously
measure the relative position of the object and the imaging device, then it is possible to construct a
sparse approximation of the blurring matrix K.
Although it may not be possible to know precisely and continuously the relative position of the
object and detector, there are applications in which this information can be either approximated [5], or
measured to high accuracy [11,12]. The purpose of this subsection is not to describe motion estimation
techniques, but to describe how to construct the matrix if the relative position information is known, so
that we can use it as a test case for iterative image restoration algorithms.
To describe how to construct the matrix K, and to simulate spatially variant motion blur, assume the
observed image g is the (normalized) sum of images at incremental times during acquisition. Each of
the individual images represents a snapshot of the object in a ﬁxed position. To obtain a mathematical
model, let f (x, y) be a continuous function representing the object, and let F be a discrete image,
whose (k, ℓ) entry is given by
F(k, ℓ) = f (xk, yℓ),
k = 1, 2, . . . n,
ℓ= 1, 2, . . . n.
Now suppose F1 is a discrete image obtained from the object f after a rigid movement. Then there
is an afﬁne transformation A ∈R3×3 such that
⎡
⎣
ˆxk
ˆyℓ
1
⎤
⎦=
⎡
⎣
a11 a12 a13
a21 a22 a23
0
0
1
⎤
⎦
⎡
⎣
xk
yℓ
1
⎤
⎦
and
F1(k, ℓ) = f (ˆxk, ˆyℓ).
Note that because the continuous image f is not known at every point (x, y) (all that is known is the
discrete image F), it may not be possible to evaluate f (ˆxk, ˆyℓ), unless ˆxk = x ˆk and ˆyℓ= y ˆℓfor some 1 ≤
ˆk ≤n and 1 ≤ˆℓ≤n. However, an approximation of f (ˆxk, ˆyℓ) can be computed by interpolating known
valuesof f near f (ˆxk, ˆyℓ).Suppose(asillustratedinFigure7.9)that f (x ˆk, y ˆℓ), f (x ˆk+1, y ˆℓ), f (x ˆk, y ˆℓ+1),
and f (x ˆk+1, y ˆℓ+1) are four known pixel values surrounding the unknown value f (ˆxk, ˆyℓ). Nearest
neighbor interpolation uses the known pixel value closest to f (ˆxk, ˆyℓ); for example, in the illustration
in Figure 7.9 we have
F1(k, ℓ) = f (ˆxk, ˆyℓ) ≈f (x ˆk, y ˆℓ+1).
In the case of bilinear interpolation, a weighted average of the four pixels surrounding f (ˆxk, ˆyℓ) is used
for the approximation:
F1(k, ℓ) = f (ˆxk, ˆyℓ)
≈(1 −xk)(1 −yℓ) f (x ˆk, y ˆℓ) + (1 −xk)yℓf (x ˆk, y ˆℓ+1)
+ xk(1 −yℓ) f (x ˆk+1, y ˆℓ) + xkyℓf (x ˆk+1, y ˆℓ+1),

212
CHAPTER 7 Iterative Methods for Image Restoration
Bilinear
Nearest neighbor
FIGURE 7.9
Illustration of interpolation. The graphic on the left illustrates using the nearest (known pixel) neighbor to
f (ˆxk, ˆyℓ) to approximate its value. The graphic on the right illustrates bilinear interpolation (weighted average
of the four known pixels) to approximate f (ˆxk, ˆyℓ).
where xk = ˆxk −x ˆk and yℓ= ˆyℓ−y ˆℓ. The above discussion assumes that each pixel is a square
with sides of length one; incorporating arbitrary size dimensions for pixels is not difﬁcult.
If we deﬁne vectors f = ν(F) and f 1 = ν(F1) from the discrete image arrays (e.g., through
lexicographical ordering), we can write
f 1 = K1 f ,
where K1 is a sparse matrix that contains the interpolation weights. Speciﬁcally, the kth row of K1
contains the weights for the pixel in the kth entry of f 1. For example, in the case of bilinear interpolation,
there are at most four nonzero entries per row, given by
(1 −xk)(1 −yℓ),
(1 −xk)yℓ,
xk(1 −yℓ),
xkyℓ.
In the case of nearest neighbor interpolation, there is just one nonzero entry in each row. We emphasize
that by using a sparse data format (e.g., compressed row [43]) to represent K, we need only keep
track of the nonzero entries and their locations in the matrix K. Moreover, this discussion assumes
the afﬁne transformation A is known, because this provides the necessary information to construct the
interpolation weights.
As previously discussed, we assume the observed motion blurred image is the (normalized) sum of
images at incremental times during the acquisition. That is, we assume
g =
T

t=1
wtf t + η,
where f t = Kt f is a vector representing the discrete image at time t, wt is the normalization weight
for the tth image (for example, we could simply use wt = 1
T ), and η is additive noise. Thus, we obtain
the deblurring problem given in Eq. (7.2), where the matrix modeling the motion blur is
K =
T

t=1
wtKt.
(7.11)

4.07.3 Model Problems
213
The sparsity structure of K depends on the severity of the motion blur, as well as on the number
of known positions of the object. To illustrate, suppose that the motion is described by a shift and a
rotation; speciﬁcally, suppose the object at the tth position is shifted by (δxt, δyt) and rotated (about
the center) by θt. An afﬁne transformation for this, assuming the image is centered at (x, y) = (0, 0) is
Aℓ=
⎡
⎣
cos (θt) sin (θt) δxt
−sin (θt) cos (θt) δyt
0
0
1
⎤
⎦.
To generate test problems, we take random values for δxt, δyt, and θt. We can control the severity of
motion through how large we choose these values. Consider three examples, where in each case we use
50 positions (i.e., t = 1, 2, . . . , 50):
•
First we consider the case where the object moves only slightly during the acquisition time. In this
case the values for δxt and δyt are chosen from a uniform distribution in the interval (0, 2), and the
values for θt are chosen from a normal distribution with mean 0 and standard deviation of π/40.
•
In the second example we increase the severity of the motion by taking δxt and δyt from a uniform
distribution in the interval (0, 5), and the values for θt are chosen from a normal distribution with
mean 0 and standard deviation of π/20.
•
In the third example we further increase the severity of the motion by taking δxt and δyt from
a uniform distribution in the interval (0, 10), and θt from a normal distribution with mean 0 and
standard deviation of π/10.
The sparsity pattern for the matrices K for these speciﬁc cases, using bilinear interpolation, is shown in
Figure 7.10. Notice that more severe motion results in a decrease in sparsity of K. Blurred images for
these cases are shown in Figure 7.11.
small motion
medium motion
severe motion
FIGURE 7.10
Sparsity pattern of the matrix K for various amounts of object movement. Blurred images for these matrices
are shown in Figure 7.11.

214
CHAPTER 7 Iterative Methods for Image Restoration
small motion
medium motion
large motion
FIGURE 7.11
Examples of motion blurred images, using the blurring matrices shown in Figure 7.10. The image in the top
row is the true object, and the bottom row shows the blurred images.
4.07.3.5 MATLAB notes
Test data for the examples described in this section are available at www.mathcs.emory.edu/∼nagy/
RestoreTools. The data are saved as mat ﬁles, and can be opened in MATLAB with the load command.
Each of the mat ﬁles contains the true image f true and the blurred, noisy image g, as well as some
statistical information about the noise, η. The noise was generated using a combination of Poisson
(background photon) and Gaussian white (readout) noise, with
∥η∥2
∥Kf true∥2
= 0.01.
The speciﬁc details of the noise model will be discussed in more detail in Section 4.07.5.2.2. For the
data discussed in Sections 4.07.3.1–4.07.3.3 we include PSFs and their centers, while for the motion
blur examples described in Section 4.07.3.4 we include the sparse matrices K
A summary of the data is as follows:
• Three examples of spatially invariant Gaussian blurs, using the PSF given in Eq. (7.8), and with
the following parameters:

4.07.4 Iterative Methods for Unconstrained Problems
215
GaussianBlur440.mat corresponds to the case α1 = α2 = 4 and ρ = 0.
GaussianBlur420.mat corresponds to the case α1 = 4, α2 = 2 and ρ = 0.
GaussianBlur422.mat corresponds to the case α1 = 4, α2 = 2 and ρ = 2.
• Three examples of blurring caused by atmospheric turbulence. The PSF is described by Eq. (7.9),
and wavefronts were created with three different values of d/r0. Speciﬁcally,
AtmosphericBlur10 corresponds to d/r0 = 10.
AtmosphericBlur30 corresponds to d/r0 = 30.
AtmosphericBlur50 corresponds to d/r0 = 50.
• Spatially variant Gaussian blur, using the PSF given by Eq. (7.10).
VariantGaussianBlur1.mat uses α1(s) = 3
4(3|s| + 1) and α2(t) = 3
4(3|t| + 1).
VariantGaussianBlur2.mat uses α1(s) = 3
4( −3|s| + 1) and α2(t) = 3
4( −3|t| + 1).
VariantGaussianBlur3.mat uses α1(s) = 3 −2
2 and α2(t) = 3 −t
2.
For each of these we generated 49 PSFs in a 7 × 7 region partitioning of the image domain. Thus,
when this data is loaded into MATLAB, PSF and center will be cell arrays as described in
Section 4.07.2.4.
• Spatially variant motion blur, using the examples described in Section 4.07.3.4. Speciﬁcally,
VariantMotionBlur_small.mat contains data for the small motion simulation.
VariantMotionBlur_medium.mat contains data for the medium motion simulation.
VariantMotionBlur_large.mat contains data for the large motion simulation.
4.07.4 Iterative methods for unconstrained problems
In this section, we describe some iterative methods that can be used to solve unconstrained image
restoration problems. We focus on methods that can be applied directly to
min
f
∥g −Kf∥2
in an iterative regularization scheme, or to the Tikhonov least squares problem
min
f

g
0

−
 K
λL

f

2
.
We note that if we deﬁne the quadratic function ψ( f) = 1
2 f TKTKf −f TKTg, then the following
minimization problems are equivalent:
min
f
ψ(f) and
min
f
∥g −Kf∥2.
This equivalence is often used to describe and design iterative methods.
Iterative methods for unconstrained image restoration problems have the general form,
f (k+1) = f (k) + τkd(k),
where d(k) is a step direction and τk is a step length. Different choices for these terms leads to different
iterative methods.

216
CHAPTER 7 Iterative Methods for Image Restoration
4.07.4.1 Richardson iteration
The Richardson iteration, which is often called the Landweber method [13–16], is generally used as an
iterative regularization method. The basic iteration takes the form:
f (k+1) = f (k) + τKT 
g −Kf (k)
.
That is, the step
d(k) = KT 
g −Kf (k)
is the steepest descent direction for the quadratic function ψ(f), and the step size τ remains constant
for each iteration. It can be shown that to ensure convergence the step size must satisfy
0 < τ <
2
σ 2max
,
where σmax is the largest singular value of K. Although it might be difﬁcult to compute σ 2max =
∥KTK∥2, a default choice for τ can be found using the well-known bound [55]
σ 2max = ∥KTK∥2 ≤∥K∥1∥K∥∞.
In general, the matrix norms ∥K∥1 and ∥K∥∞are easy to compute. In fact, if K has no negative entries,
as is the case in image restoration problems, and if we deﬁne 1 to be a vector with every entry equal to
1, then
∥K∥∞= max element in the vector K1,
and
∥K∥1 = max element in the vector KT1.
Thus, using this bound, a default choice for τ can be
τ =
1
∥K∥1∥K∥∞
≤
1
σ 2max
<
2
σ 2max
.
It can be shown [14] that the Richardson iteration can be interpreted as an SVD ﬁltering method as
described in Eq. (7.3). In particular, if the initial guess is f (0) = 0, then the ﬁlter factors for the kth
iteration are given by
φ(k)
i
= 1 −(1 −τσi)k,
i = 1, 2, . . . , N,
where σi is the ith largest singular (or spectral) value of K. To visualize what this tells us about
convergence properties of the Richardson iteration, assume that σmax = τ = 1. In this case, we see that in
the early iterations solution components corresponding to large singular values are reconstructed, while
we need many iterations before components corresponding to small singular values are reconstructed.
That is,
0 ≈φ(k)
N ≤φ(k)
N−1 ≤· · · ≤φ(k)
1
≈1,
as should be the case for an SVD ﬁltering method. However, it may take many iterations to reconstruct
components corresponding to intermediate singular values. For example, using again σ1 = τ = 1, then

4.07.4 Iterative Methods for Unconstrained Problems
217
with the intermediate singular value σ j = 0.01, we obtain ﬁlter factors φ(k)
j
= 1 −(1 −τσ j)k =
1 −(0.99)k, and thus k would need to be very large to obtain φ(k)
j
≈1. Whether or not we want to
reconstruct this component of the solution depends on the problem, but because we expect the singular
values to decay to zero, a value of σ j = 0.01 is, in general, not very small and is likely to correspond
to signal subspace rather than noise subspace information.
From this analysis, we expect that the basic Richardson iteration converges very slowly. However,
as will be seen below, the method can be accelerated with preconditioning. It also provides a good
introduction to discussing other iterative methods, including ones that enforce a nonnegativity constraint.
An algorithm for the Richardson iteration can be stated as follows:
Algorithm: Richardson Iteration
given:
K, g
choose:
initial guess for f
step length parameter τ
compute:
r = g −Kf
d = KTr
while (not stop) do
f = f + τd
r = g −Kf
d = KTr
end while
As with any iterative regularization method, it is important to choose appropriate criteria for termi-
nating the while loop. This is a nontrivial topic. About this topic we mention only an approach known
as the Morozov’s discrepancy principle [56], which terminates the iteration when
∥g −Kf (k)∥2 ≤δε,
where ε is the expected value of the noise η and δ > 1. The discrepancy principle is easy to implement,
but it does require a good estimate of ε. Other approaches to estimating a stopping iteration are described
in [15,16].
4.07.4.2 Preconditioned Richardson methods
We now consider preconditioning of the basic Richardson method. Recall that preconditioning is a
technique used to modify the problem in such a way as to accelerate convergence. In the case of right
preconditioning, we replace K with K = KR−1 and f with ˆf = Rf. After some algebraic manipulation,
the main part of the iteration can be written as:
f (k+1) = f (k) + τM−1KT 
g −Kf (k)
,
(7.12)
where M = RTR.

218
CHAPTER 7 Iterative Methods for Image Restoration
In the case of left preconditioning, we replace K with K = R−TK and g with ˆg = R−Tg. After some
algebraic manipulation, the main part of the iteration can be written as:
f (k+1) = f (k) + τKTM−1 
g −Kf (k)
,
(7.13)
where M = RTR.
Algorithms for these preconditioned Richardson iterative methods can be written as:
Algorithm: Richardson iteration
right preconditioned
given:
K, g
choose:
initial guess for f
step length parameter τ
compute:
r = g −Kf
v = KTr
d = M−1v
while (not stop) do
f = f + τd
r = g −Kf
v = KTr
d = M−1v
end while
Algorithm: Richardson iteration
left preconditioned
given:
K, g
choose:
initial guess for f
step length parameter τ
compute:
r = g −Kf
v = M−1r
d = KTv
while (not stop) do
f = f + τd
r = g −Kf
v = M−1r
d = KTv
end while
It is important to keep in mind that the convergence behavior, and the choice of τ depend on the
singular values of the preconditioned system. For example, consider the right preconditioned system,
K = KR−1. Then the similarity transformation
R−1KTKR =

RTR
−1
KTK = M−1KTK,
shows that KTK and M−1KTK have the same eigenvalues. Thus,
ˆσ 2max = ∥KTK∥2 = ∥M−1KTK∥2 ≤∥M−1∥2∥KTK∥2.
Thus, as with the case of the unpreconditioned Richardson iteration, a default choice for τ can be
τ =
1
∥M−1∥2∥K∥1∥K∥∞
≤
2
∥M−1KTK∥2
=
2
∥KT K∥2
,
assuming that we can compute an estimate of ∥M−1∥2. A similar result holds for left preconditioning.
We now give a few examples of preconditioners, and show that some of them lead to other well-known
iterative methods.

4.07.4 Iterative Methods for Unconstrained Problems
219
4.07.4.2.1
A diagonal matrix preconditioner: Cimmino’s method
We begin with a very simple preconditioner that results in the Cimmino method. Speciﬁcally, we consider
using the following diagonal matrix as a left preconditioner:
R = diag
∥k1∥2
√c1
, ∥k2∥2
√c2
, . . . , ∥kN∥2
√cN

,
M = RT R = diag

∥k1∥2
2
c1
, ∥k2∥2
2
c2
, . . . , ∥kN∥2
2
cN

,
where kT
i is the ith row of K and ci are positive numbers. We could choose τ to be
τ =
1
∥M−1∥2∥K∥1∥K∥∞
,
where ∥M−1∥2 = max1≤i≤N
 
ci/∥ki∥2
2
!
. Another choice, proposed by Cimmino [15,57], is to use
τ =
N

i=1
ci.
It is not difﬁcult to show that with this value, τ < 2/ˆσ 2max. Observe that if we choose c1 = · · · =
cN = 1, then we obtain
M−1 = diag

1
∥k1∥2
2
,
1
∥k2∥2
2
, . . . ,
1
∥kN∥2
2

and τ = 2/N,
where N is the number of pixels in the image. In this case, τ is less than one, resulting in small steps at
each iteration. This may or may not lead to slow convergence; if the direction vector d is good, then a
small step size might be appropriate.
In an implementation of Cimmino’s method, inverting the diagonal matrix M is trivial, but construc-
tion of the preconditioner requires computing ∥ki∥2
2, the squared norms of rows of K. If the blur is
spatially invariant, then we can assume that these norms are approximately constant, equal to the sum
of squares of the PSF pixel values; exact equality occurs in the case of periodic boundary conditions.
4.07.4.2.2
Triangular matrix preconditioners: SOR methods
The methods of successive over-relaxation (SOR) have been well studied in the applied mathematics
and scientiﬁc computing community, especially in the context of solving partial differential equations
[45,46,55,58]. They have recently been proposed for use in image restoration [59,60], so we give a
brief description of these methods here.
The SOR methods are based on matrix splittings. Speciﬁcally, since the matrix KTK is symmetric,
we can split it up as
KTK = T + D + TT,
where D is the diagonal part and T is the strictly lower triangular part of KTK. If we then set
M = D + τT,

220
CHAPTER 7 Iterative Methods for Image Restoration
we obtain the standard SOR method. A similar approach is the symmetric successive over-relaxation
(SSOR) method [46], which uses the preconditioner
M =
1
2 −τ

D + τT
	
D−1 
D + τT
	T .
The term over-relaxation is used because τ is considered a relaxation parameter, and in well-posed
problems arising, for example, from discretization of partial differential equations, an optimal value for
τ is chosen greater than one (i.e., the method uses over-relaxation).
The SOR preconditioner requires less work to implement than SSOR, but it is not obvious how to
provide a general analysis of the ﬁltering properties of SOR for ill-posed problems. However, in the
SSOR case M can be written as
M = RTR,
where R =
"
1
2 −τ D−1/2 
D + τT
	T .
Thus the ﬁltering properties of SSOR are explained by examining the singular values of the precondi-
tioned system K = KR−1.
We mention that the properties of SOR for image restoration, in the simpliﬁed case of spatially
invariant blurs with periodic boundary conditions, were studied in [59,60]. They show that, contrary
to well-posed problems arising in partial differential equations, the relaxation parameter should satisfy
τ ≪1; that is, the method should use severe under-relaxation instead of over-relaxation. We can see
that this makes sense by supposing that KTK and M have the same eigenvectors; that is, suppose
KTK = QT kQ and M = QT mQ,
where k is a diagonal matrix containing the eigenvalues of KTK and m is a diagonal matrix containing
the eigenvalues of M. In this case,
M−1KTK = QT Q,
where the diagonal matrix  contains the ratio of eigenvalues of KTK and M. In the worst case, it could
happen that one of the diagonal entries of  is the largest eigenvalue of KTK divided by the smallest
eigenvalue of M. This value could be very large, and so the upper bound for τ, 2/∥M−1KTK∥2, could
be very small.
It is important to observe that by choosing τ ≪1 we obtain stability in the preconditioned iteration.
However,becauseτ isalsothestepsizeinthealgorithm,small τ meanssmallsteps,andthusconvergence
can be very slow. We say can be slow, because the initial direction vectors could be very good, and it
therefore could be the case that we need only a few iterations to reconstruct a good approximation of
f true before noise begins to corrupt the solution. It is difﬁcult to provide a general analysis of this for the
SOR and SSOR preconditioners. It should be mentioned that, in the context of set theoretic methods
[61–63], Combettes [64] has proposed a scheme to actually increase the relaxation parameter to a value
greater than 2, with the goal to improve the rate of convergence. It would be interesting to see if such
an approach can be applied to the SOR and SSOR methods. Another disadvantage of the SOR and
SSOR preconditioners is that solving systems with M can be relatively expensive compared to other
preconditioners. We discuss these issues in more detail for ﬁltering based preconditioners.

4.07.4 Iterative Methods for Unconstrained Problems
221
4.07.4.2.3
Filtering based preconditioning
In this subsection, we describe an approach to construct preconditioners for image restoration problems
that was originally proposed in 1993 [48]. We ﬁrst explain the basic idea in the ideal situation where
we can compute a singular value decomposition (SVD) of K. The SVD is deﬁned as
K = UVT,
where U and V are orthogonal matrices, and  = diag(σ1, σ2, . . . , σN) is a diagonal matrix containing
the singular values of K. If we deﬁne R = UrVT, where r = diag(σ (r)
1 , σ (r)
2 , . . . , σ (r)
N ), then in the
case of left preconditioning,
K = R−1K = V ˆV T,
where ˆ = diag(ˆσ1, ˆσ2, . . . , ˆσN), ˆσi = σi/σ (r)
i
. Recalling the convergence analysis and ﬁltering prop-
erties of the basic Richardson iteration, to obtain fast convergence of components corresponding to large
singular values (i.e., signal information), we should choose σ (r)
i
so that ˆσi ≈1 for large σi. On the other
hand, to make sure we do not have fast convergence of the noise subspace information (i.e., components
ofthesolutioncorrespondingtosmallsingularvalues)weshouldchooseσ (r)
i
sothat ˆσi issmall.Thereare
many ways to do this. In [47,48] a truncated SVD, or pseudo inverse like approach was proposed, where
σ (r)
i
≈
σi
σi ≥tol,
1,
σi < tol,
where tol is a speciﬁed truncation tolerance. Here we see that the singular values larger than tol
of the preconditioned system are clustered at one, and are well separated from the remaining “small”
singular values. Determining how to choose the truncation tolerance is related to determining regular-
ization parameters. Various techniques can be used, including the discrete Picard condition, L-curve,
and generalized cross validation (GCV); see [22,47,48] for more details.
Another approach for choosing σ (r)
i
is to use a Tikhonov regularization type approach, where
σ (r)
i
=

σ 2
i + λ2,
where λ is a speciﬁed regularization parameter for the preconditioner. As with the truncation tolerance,
varioustechniquestochoose λ,includingthediscretePicardcondition,L-curve,andGCV.Acomparison
of these and other choices for choosing σ (r)
i
is given in [65].
Computing the SVD of K is typically too expensive for large scale problems, and in the cases where
it is possible to compute, we might as well use direct ﬁltering methods instead of iterative methods.
However, this discussion suggests that ﬁltering preconditioners can be constructed with SVD approx-
imations. A general approach to computing an SVD approximation is to choose, a priori, orthogonal
(or unitary, if we want to consider complex bases) matrices #U and #V, and determine a diagonal matrix
# such that
# = arg min
 ∥#UTK#V −∥F,
where ∥·∥F denotes the Frobenius norm, and where the minimization is done over all diagonal matrices,
. We then use as a preconditioner R = #Ur#VT, where we use one of the approaches discussed above
for choosing the diagonal entries σ (r)
i
from ˜σi.

222
CHAPTER 7 Iterative Methods for Image Restoration
To make this approach efﬁcient, the construction of, and computations with #U and #V should be
inexpensive. Several approaches have been proposed in the literature:
•
By choosing #U = #V = F, the discrete Fourier transform matrix, computations with #U and #V can
be done very efﬁciently with FFTs. The resulting approximation, #K = #U##VT is a block circulant
matrix with circulant blocks, and is the best such approximation of K; see [66] for more details. The
cost of constructing the approximation, and computations with the preconditioner are O(N log N).
•
Instead of using the FFT basis to build #U and #V, we could use another fast transform, such as
the discrete cosine transform (DCT) [66]. As with FFTs, construction of this approximation, and
computations with the resulting preconditioner are O(N log N).
•
Another alternative is use a separable approximation of U and V, so that #U and #V have the form
#U = Uh ⊗Uv
and #V = Vh ⊗Vv,
where ⊗denotes Kronecker product. This approximation can be obtained by ﬁnding the best sep-
arable (i.e., x-y) approximation of the PSF. Construction of this approximation, and computations
with it, are O(N 3/2). This is slightly more than the O(N log N) computations of the fast transforms,
but it is still very efﬁcient, and a separable basis may prove to be a much better approximation than
the FFT or DCT for some problems [67–69].
Note that the cost of matrix vector multiplications with K can usually be done with FFTs (including the
spatially variant case; see [9,10]), so even if preconditioning is not used, the cost of each iteration is at
least O(N log N). Thus, if convergence of the iterative method is much faster when using precondition-
ing, there can be a dramatic overall savings in computational cost compared to using no preconditioning.
We refer to the approach described in this subsection as a ﬁltering preconditioner because the idea is
very similar to applying an approximate pseudo-inverse ﬁlter at each iteration. These preconditioners can
be constructed for both spatially invariant and spatially variant blurs [22]. It is not possible to say that one
approach is better than the others; the optimal approach depends on the PSF as well as on the image data.
4.07.4.3 Steepest descent methods
In the steepest descent method, as in the case of the Richardson iteration, we take a step direction that
is the negative gradient of ψ(f), but we allow the step size to change at each iteration. In particular, the
step size is chosen to minimize ψ(f) in the direction of the negative gradient. That is,
f (k+1) = f (k) + τkd(k),
where d(k) = KT 
g −Kf
	
and τk = arg min
τ
ψ(f (k) + τkd(k)). It is not difﬁcult to show that
τk = arg min
τ
ψ(f (k) + τd(k)) = ∥d(k)∥2
2
∥Kd(k)∥2
2
.
Thus, the basic iteration requires computing
r(k) = g −Kf (k),
d(k) = KTr(k),

4.07.4 Iterative Methods for Unconstrained Problems
223
τk = ∥d(k)∥2
2/∥Kd(k)∥2
2,
f (k+1) = f (k) + τkd(k).
A direct implementation of these steps would require three matrix-vector multiplications—two with K
and one with KT. This can be reduced to two matrix-vector multiplications by observing that
r(k+1) = g −Kf (k+1) = g −K( f (k) + τkd(k)) = r(k) −τkKd(k).
(7.14)
Using this recursion for r(k+1), the steepest descent algorithm can be written as.
Algorithm: Steepest descent
given:
K, g
choose:
initial guess for f
compute:
r = g −Kf
while (not stop) do
d = KTr
w = Kd
τ = ∥d∥2
2/∥w∥2
2
f = f + τd
r = r −τw
end while
As with the Richardson iteration, the discrepancy principle can be used as a stopping criterion if
steepest descent is used for image restoration problems. Because of the variable step size, it is difﬁcult
to provide a theoretical analysis of the ﬁltering properties of steepest descent, but some experimental
results investigating this topic, and application of ﬁltering based preconditioners for steepest descent
can be found in [70].
We mention that other criteria can be used for choosing the step length, such as the Brakhage ν
methods [71], and Barzilai and Borwein’s lagged steepest descent scheme [72].
4.07.4.4 Conjugate gradient methods
Conjugate gradient methods are usually much more efﬁcient than gradient descent based methods, such
as the Richardson iteration and steepest descent. There are two closely related conjugate gradient based
methods for least squares problems: CGLS [58] and LSQR2 [73,74]. We focus our discussion on LSQR.
2Note that while the acronym CGLS is obviously obtained from the phrase “conjugate gradient method for least squares,” the
precise meaning of the acronym LSQR is less clear (it was not explicitly deﬁned in the original papers by Paige and Saunders
[73,74]). LSQR is an iterative method to solve “least squares” problems, and it uses an efﬁcient “QR” factorization updating
scheme at each iteration.

224
CHAPTER 7 Iterative Methods for Image Restoration
4.07.4.4.1
LSQR and ﬁltering
LSQR is based on the Golub-Kahan (sometimes referred to as Lanczos) bidiagonalization (GKB)
process, which computes the factorization
K = WBYT,
(7.15)
where W and Y are orthogonal matrices, and B is bidiagonal,
B =
⎡
⎢⎢⎢⎣
γ1
β2
γ2
β3
γ3
...
...
⎤
⎥⎥⎥⎦.
Because W and Y are orthogonal matrices, we can rewrite Eq. (7.15) as
KY = W B
and KTW = Y BT.
From these relationships, we obtain
Kyk = γkwk + βk+1wk+1
and KTwk = βkyk−1 + γkyk,
(7.16)
where yk is the kth column of Y and wk is the kth column of W. The recursions in Eq. (7.16) can be
used to iteratively compute the columns of W and Y, as well as the coefﬁcients γk and βk that deﬁne B.
Speciﬁcally, given K and g, we compute:
GKB iteration
β1 = ∥g∥2
w1 = g/β1
ˆy1 = KTw1
γ1 = ∥ˆy1∥2
y1 = ˆy1/γ1
for k = 2, 3, . . .
ˆwk = Kyk−1 −γk−1wk−1
βk = ∥ˆwk∥2
wk = ˆwk/βk
ˆyk = KTwk −βkyk−1
γk = ∥ˆyk∥2
yk = ˆyk/γk
If we deﬁne Wk+1 =
 w1 · · · wk wk+1

, Yk =
y1 · · · yk

, and
Bk =
⎡
⎢⎢⎢⎢⎢⎣
γ1
β2
γ2
...
...
βk
γk
βk+1
⎤
⎥⎥⎥⎥⎥⎦
.
(7.17)

4.07.4 Iterative Methods for Unconstrained Problems
225
then it is not difﬁcult to show that the GKB iteration results in the matrix relations
KTWk+1 = YkBT
k + γk+1yk+1ekT+1,
(7.18)
KYk = Wk+1Bk,
(7.19)
where ek+1 denotes the (k + 1)st standard unit vector.
Given these relations, and recalling that the ﬁrst column of Wk is g/∥g∥2, an approximate solution
f k of f true can be computed from the projected least squares problem
min
f∈R(Yk) ∥Kf −g∥2
2 = min
ˆf
∥Bk ˆf −β1e1∥2
2,
(7.20)
where β1 = ∥g∥2, and f k = Yk ˆf. It can be shown that f k converges to the solution of the least squares
problem min ∥g −Kf∥2. We omit speciﬁc implementation details, and refer to [73,74]. However, we
do mention the following important points:
•
When k is small, solving the projected problem in Eq. (7.20) is very simple because the matrix Bk
is a sparse (k + 1) × k matrix. In fact, using plane rotations [55], it is possible to efﬁciently update
the solution of the projected least squares problem from iteration k −1 to iteration k.
•
If we only want to compute f, then it is not necessary to save all vectors in the matrices Wk and Yk;
updating vectors and coefﬁcients in the kth iteration only requires information from iteration k −1.
An important property of GKB is that for small values of k the singular values of the matrix Bk
approximate very well certain singular values of K, with the quality of the approximation depending
on the relative distance between the singular values of K; speciﬁcally, the larger the relative spread, the
better the approximation [58,75,76]. For ill-posed inverse problems, the singular values of K decay to
and cluster at zero, such as σi = O(i−c) where c > 1, or σi = O(ci), where we use the big-O notation
to mean “on the order of,” 0 < c < 1 and i = 1, 2, . . . , n [77,78]. Thus the relative gap between
large singular values of K is generally much larger than the relative gap between small singular values.
Therefore, if the GKB iteration is applied to a linear system arising from discretization of an ill-posed
inverse problem, such as in image restoration, then the singular values of Bk converge very quickly to
the largest singular values of K.
This property implies that if LSQR is applied to the least squares problem minf ∥Kf −g∥2, then
at early iterations the approximate solutions f k will be in a subspace that approximates a subspace
spanned by the large singular components of K. Thus for k ≪n, f k is a regularized solution. However,
eventually f k should converge to the inverse solution, which is corrupted with noise (recall the discussion
in Section 4.07.2.1). This means that the iteration index k plays the role of a regularization parameter; if
k is too small, then the computed approximation f k is an over smoothed solution, while if k is too large,
f k is corrupted with noise. More extensive theoretical arguments of this semi-convergence behavior of
conjugate gradient methods can be found elsewhere; see [79] and the references therein.
Finally, we mention that the ﬁltering based preconditioners described in Section 4.07.4.2.3 can be
used with LSQR.
4.07.4.4.2
A hybrid method
One of the main disadvantages of iterative regularization methods is that it can be very difﬁcult to
determine appropriate stopping criteria. To address this problem, work has been done to develop hybrid

226
CHAPTER 7 Iterative Methods for Image Restoration
methods that combine variational approaches with iterative methods. That is, an iterative method, such
as the LSQR implementation of the conjugate gradient method, is applied to the least squares problem
minf ∥Kf −g∥2
2, and variational regularization is incorporated within the iteration process.
Instead of early termination of the iteration, hybrid approaches enforce regularization at each iteration
of the GKB method. Hybrid methods were ﬁrst proposed by O’Leary and Simmons [80], and later by
Björck [81]. The basic idea is to regularize the projected least squares problem (7.20) involving Bk,
which can be done very cheaply because of the smaller size of Bk. More speciﬁcally, because the singular
values of Bk approximate those of K, as the GKB iteration proceeds, the matrix Bk becomes more ill-
conditioned. The iteration can be stabilized by including Tikhonov regularization in the projected least
squares problem (7.20), to obtain
min
ˆf

∥Bk ˆf −β1e1∥2
2 + λ2∥ˆf∥2
2

,
(7.21)
where again β1 = ∥g∥2 and f k = Yk ˆf. The regularization parameter λ needs to be chosen, either a
priori using knowledge of the data, or through regularization parameter choice methods. Thus at each
iteration it is necessary to solve a regularized least squares problem involving the (k +1)×k bidiagonal
matrix Bk. Notice that since the dimension of Bk is very small compared to K, it is much easier to
solve for ˆf in Eq. (7.20) than it is to solve for f in the full Tikhonov regularized problem (7.5). More
importantly, when solving Eq. (7.21) one can use sophisticated parameter choice methods [82,83] to
ﬁnd a suitable λ at each iteration.
To summarize, hybrid methods have the following beneﬁts:
•
Powerful regularization parameter choice methods can be implemented efﬁciently on the projected
problem.
•
Semi-convergence behavior of the relative errors observed in LSQR is avoided, so an imprecise
(over) estimate of the stopping iteration does not have a deleterious effect on the computed solution.
Realizing these beneﬁts in practice, though, is nontrivial. Thus, various authors have considered com-
putational and implementation issues, such as robust approaches to choose regularization parameters
and stopping iterations; see for example, [80,82–88]. The biggest disadvantage of the hybrid approach
is that all yk vectors must be kept throughout the iteration process, and thus the storage requirements
grow as the iteration proceeds.
4.07.4.5 MATLAB notes
In describing the algorithms, we refer to some of our MATLAB implementations. Most of the methods
have the form:
[f, info] = IRmethod(K, g, options),
where IR denotes “Iterative Restoration,” method will refer to the particular method being described;
for example, IRlsqr uses LSQR. In the case of hybrid methods, we use the HyBR (hybrid bidiagonal
regularization) implementation described in [82], and refer to it as IRhybr.

4.07.4 Iterative Methods for Unconstrained Problems
227
The input and output variables are as follows:
•
K (required input) is a matrix that can be a MATLAB full or sparse matrix, or a psfMatrix object.
Other objects can be used as well, provided the appropriate operators (e.g., mtimes) and functions
(e.g., norm) are overloaded with appropriate methods.
•
g (required input) is a blurred, noisy image.
•
options (optional) is a structure, built using IRset.m, that can be used to set certain optional
input values:
•
Initial guess for f; default is f 0 = KTg.
•
Maximum number of iterations; if K is an m × n matrix, then the default is min (m, n, 100).
•
Stopping tolerance for the relative residual ∥g −Kf∥2/∥g∥2; default is 10−6.
•
Stopping tolerance for the normal equations residual ∥KT(g −Kf)∥2/∥KTg∥2; default is 10−6.
•
Flag to indicate whether or not an error bar should be displayed on the screen to indicate progress
relative to the maximum number of iterations; default action is to show the error bar.
•
The true object, f true, which can be used to return relative error information about the iterations,
∥f k −f true∥2/∥f true∥2.
For example, the statement.
options = IRset(’MaxIter’, 27, ’Rtol’, 1e-4);
sets the maximum number of iterations to 27, and the relative residual tolerance to 10−4,
•
f is the computed restoration after the iteration completes,
•
info (optional) is a structure that contains information about the iteration, including:
•
The number of iterations performed.
•
A vector containing the residual norm ∥g −Kf k∥2 at each iteration. The ﬁrst component of this
vector is the residual norm corresponding to the initial guess, so the length of the vector is one
more than the number of iterations performed by the method.
•
A vector containing the normal equations residual norm ∥KT(g −Kf k)∥2 at each iteration. The
ﬁrst component of this vector is the normal equations residual norm corresponding to the initial
guess, so the length of the vector is one more than the number of iterations performed by the
method.
•
A vector containing the norm of the solution ∥f k∥at each iteration. The ﬁrst component of this
vector corresponds to the initial guess, so the length of the vector is one more than the number
of iterations performed by the method.
•
If the true solution f true was set in options, then this is a vector containing the relative error
norm ∥f k −f true∥2/∥f true∥2 at each iteration. The ﬁrst component of this vector corresponds to
the initial guess, so the length of the vector is one more than the number of iterations performed
by the method.
•
Flag that provides information about why the iteration terminated:

228
CHAPTER 7 Iterative Methods for Image Restoration
1 ⇒residual tolerance was satisﬁed,
2 ⇒normal equations residual tolerance was satisﬁed,
3 ⇒maximum number of iterations was reached.
For moredetails, seetheMATLAB help or doc informationon IRset as well as for any IRmethod.
4.07.5 Iterative methods with nonnegativity constraints
Since pixels in an image generally represent measured intensity values, it is natural to include a nonneg-
ativity constraint in the iterative algorithm. For example, in the case of iterative regularization methods,
we may want to develop algorithms that solve
min
f≥0 ∥g −Kf∥2
2,
or in the case of variational regularization, we modify the minimization problem given in Eq. (7.4) to
include the constraint f ≥0:
min
f≥0

∥g −Kf∥2
2 + λ2J (f)

.
As in the case of the unconstrained problem, the regularization operator J and the regularization
parameter λ must be chosen.
In this chapter we focus only on iterative regularization methods that can be obtained as natural exten-
sions of the unconstrained iterative algorithms discussed in previous sections. In the case of variational
regularization and nonnegativity constraints in the context of image restoration, we refer to [16,27].
More general optimization methods can also be used for the variational case; see for example [89,90].
4.07.5.1 Projection methods
In the case of Richardson and steepest descent methods, a nonnegativity constraint can be easily incor-
porated into the iteration by projecting the iteration f (k) onto the nonnegative orthant. That is, the ith
entry of a projected vector f is given by

P(f)

i =
 fi
if fi ≥0,
0
if fi < 0.
Thus, to incorporate nonnegativity in each of the algorithms discussed in Sections 4.07.4.1–4.07.4.3
all that is needed is to choose an initial guess f ≥0 and to replace the update step
f = f + τd
with
f = P(f + τd).
Properties of the projected Richardson method, including preconditioning have been studied; see, for
example [65,91]. The speciﬁc case of projected SOR for well posed problems was ﬁrst studied by Cryer
[92], while its use for ill-posed problems in image restoration has been considered more recently in
[59,60]. Properties of a general projected steepest (gradient) descent method can be found in one of

4.07.5 Iterative Methods with Nonnegativity Constraints
229
many references on numerical optimization methods, such as [89,90], and a discussion in the context of
image restoration can be found in [16]. In the case of the Barzilai and Borwein’s lagged steepest descent
method with nonnegativity constraints see [93]. Incorporating nonnegativity into conjugate gradient
methods is not as simple; for some suggestions at how this might be done see [94,95], or for more
general set theoretic approaches, see [61–63].
4.07.5.2 Statistically motivated methods
In this subsection we again consider the linear image formation model
g = Kf true + η
and develop iterative methods to approximate f true that incorporate statistical information.
4.07.5.2.1
A general iterative scheme
We begin by assuming the noise vector, η, is a random vector with mean E(η) = 0, and we denote the
covariance matrix as Cη, where

Cη

i, j = E(ηiη j). The Gauss-Markov theorem [96,97] states that, if
K has full rank, then the best unbiased linear estimator (BLUE) for f true can be found by solving
KTC−1
η

g −Kf
	
= 0.
To incorporate nonnegativity constraints into this model, deﬁne a diagonal matrix D with diagonal
entries

D

i,i =

1
if

f true

i > 0
0
if

f true

i = 0
so that Kf true = KDf true. With this notation, it can be shown (see [21]) that the BLUE approximation
for f true can be found by solving
DK TC−1
η

g −KDf
	
= 0,
or equivalently by solving
f ⊙DK TC−1
η

g −KDf
	
= 0,
(7.22)
where ⊙denotes component wise multiplication. Unfortunately it is not possible to directly use the
relationship given in Eq. (7.22) because D is deﬁned by the unknown true object, f true. However, observe
that if f is a best unbiased linear estimator, then f is zero whenever f true is zero. Furthermore, for any
vector v, we have f ⊙Dv = f ⊙v. It therefore follows that the best unbiased linear estimator is a
solution of the nonlinear equation
f ⊙KTC−1
η

g −Kf
	
= 0.
One simple approach to solving this nonlinear equation is to use the ﬁxed point iteration
f (k+1) = f (k) + τkd(k),
where the direction vector is d(k) = f (k) ⊙KTC−1
η (g −Kf (k)). Notice the similarity of this basic
iteration with that used in the unconstrained steepest descent method. In this case, since we want to

230
CHAPTER 7 Iterative Methods for Image Restoration
maintain nonnegative values in the updated f (k+1), care is needed in choosing the step length τk. We do
this through a bounded line search. Speciﬁcally, we ﬁrst ﬁnd the step length τuc corresponding to the
unconstrained minimization problem
τuc = arg min
τ
C−1/2
η
g −C−1/2
η
K( f (k) + τd(k))

2
2 =
d(k)Tv(k)
∥C−1/2
η
Kd(k)∥2
2
,
(7.23)
where v(k) = KTC−1
η (g −Kf (k)) = KTC−1
η r(k). Next we ﬁnd the step length that reaches the boundary
of the set of nonnegative solutions,
τbd = min

−[f (k)]i
[d(k)]i
 [d(k)]i < 0

,
(7.24)
and use τk = min{τuc, τbd}. Thus, if we set r(0) = g −Kf (0), a basic iteration to compute a nonnegative
approximation of f true is
v(k) = KTC−1
η r(k),
d(k) = f (k) ⊙v(k),
w(k) = Kd(k),
τk = min{τuc, τbd} (see Eqs.(7.23) and (7.24)),
f (k+1) = f (k) + τkd(k),
r(k+1) = r(k) −τkw(k),
where the recursion for computing r(k+1) is the same as in the unconstrained steepest descent algorithm;
see Eq. (7.14).
4.07.5.2.2
A general noise model
We now discuss what to use for C−1
η
by considering a speciﬁc statistical model. In particular, we use
the model described in [98,99] (see also [16,27]) for CCD arrays, in which each pixel is assumed to be
the sum of realizations from three random variables

g

i = Poisson

Kf true

i
	
+ Poisson(β) + Normal(0, σ 2),
where
•
[g]i is the ith pixel in the observed image.
•
Poisson

Kf true

i
	
is a Poisson random variable with parameter (i.e., mean and variance) equal to
the ith pixel value of the noise-free blurred image,

Kf true

i. This term represents the number of
object dependent photoelectrons measured at the ith pixel of the CCD array.
•
Poisson(β) is a Poisson random variable with parameter (i.e., mean and variance) equal to β. This
term represents the number of background photoelectrons, arising from both natural and artiﬁcial
sources, measured at the ith pixel of the CCD array.

4.07.5 Iterative Methods with Nonnegativity Constraints
231
•
Normal(0, σ 2) is a Gaussian random variable with mean zero and variance σ 2. This term represents
random errors caused by CCD electronics and the analog-to-digital conversion measured in voltages.
It is often referred to as readout noise.
These random variables are assumed to be independent of each other, as well as independent for each
pixel. Thus, we can write the observed image as a realization of a random vector:
g = Poisson(K f true) + Poisson(β1) + Normal(0, σ 2I),
where 1 is a vector of all ones, and 0 is a vector of all zeros.
We use the central limit theorem [96,97] to approximate this model so that it involves a single
distribution. First note that in image restoration problems the entries in K are nonnegative, as are the
pixel values of f true. Thus,

Kf true

i + β + σ 2 > σ 2, and so
K f true + β1 + σ 21 + Normal(0, diag(K f true + β1 + σ 21))
= Normal(K f true + β1 + σ 21, diag(K f true + β1 + σ 21))
≈Poisson(K f true + β1 + σ 21)
= Poisson(K f true) + Poisson(β1) + Poisson(σ 21)
≈Poisson(K f true) + Poisson(β1) + Normal(σ 21, σ 2I)
= Poisson(K f true) + Poisson(β1) + Normal(0, σ 2I) + σ 21
= g + σ 21,
where the approximations are due to the central limit theorem, and where the notation diag(z) is used to
denote a diagonal matrix whose ith diagonal entry is given by the ith entry in the vector z. We therefore
obtain the approximate image formation model
g = K f true + β1 + Normal(0, diag(K f true + β1 + σ 21),
or
g −β1 = Kf true + Normal(0, diag(K f true + β1 + σ 21).
(7.25)
Observe that Eq. (7.25) is in the form discussed in Section 4.07.5.2.1, where
Cη = diag(K f true + β1 + σ 21),
and where we use g −β1 in place of g. Since we do not know f true we cannot explicitly construct Cη,
but we can consider some ways to approximate it.
4.07.5.2.3
Algorithms
First we remark that often β and σ 2 are known, or can be estimated, and that incorporating this infor-
mation into Cη is not difﬁcult. The real issue is how to approximate K f true. Three possibilities are to
use 0, or g −β1, or an iteration dependent approximation Kf (k).
If we replace K f true with 0 when constructing an approximation of Cη, and if we assume that β = 0
(i.e., there is only Gaussian white noise), then the basic iteration described in Section 4.07.5.2.1 results
in the modiﬁed residual norm steepest descent (MRNSD) algorithm [21,95,100].

232
CHAPTER 7 Iterative Methods for Image Restoration
Algorithm: MRNSD
given:
K, g
choose:
initial guess for f ≥0
compute:
r = g −Kf
while (not stop) do
v = KTr
d = f ⊙v
w = Kd
τuc = dTv/∥w∥2
2
τbd = min (−f(d < 0) ⊘d(d < 0))
τ = min (τuc, τbd)
f = f + τd
r = r −τw
end while
In the above algorithm, ⊙denotes component wise multiplication and ⊘denotes component wise
division. Furthermore, the computation f(d <0) ⊘d(d <0) means only perform element wise division
when the components of d are negative.
In the case where we replace K f true with g −β1 when constructing an approximation of Cη, we
obtain the weighted modiﬁed residual norm steepest descent (WMRNSD) algorithm [21].
Algorithm: Weighted MRNSD
given:
K, g, σ 2, β
choose:
initial guess for f ≥0
compute:
c = g + σ 21
g = g −β1
r = g −Kf
while (not stop) do
v = KT(r ⊘c)
d = f ⊙v
w = Kd
τuc = dTv/∥w ⊘c1/2∥2
2
τbd = min (−f(d < 0) ⊘d(d < 0))
τ = min (τuc, τbd)
f = f + τd
r = r −τw
end while

4.07.5 Iterative Methods with Nonnegativity Constraints
233
The WMRNSD algorithm can be interpreted as a preconditioned version of MRNSD; see [100] for
further details. In the algorithm we store the diagonal entries of Cη in a vector c, the square root c1/2
is done component wise, and component wise division with c and c1/2 is used in place of computing
matrix inverses C−1
η
and C−1/2
η
.
Finally we consider the case where we replace K f true with the iteration dependent approximation
Kf (k) when constructing an approximation of Cη.
Algorithm: k-weighted MRNSD
given:
K, g, σ 2, β
choose:
initial guess for f ≥0
compute:
c = Kf + β1 + σ 21
g = g −β1
r = g −Kf
while (not stop) do
v = KT(r ⊘c)
d = f ⊙v
w = Kd
τuc = dTv/∥w ⊘c1/2∥2
2
τbd = min (−f(d < 0) ⊘d(d < 0))
τ = min (τuc, τbd)
f = f + τd
r = r −τw
c = Kf + β1 + σ 21
end while
An interesting observation about using Kf (k) when constructing an approximation of Cη was made in
[21]. Note that if the blur is spatially invariant and we use periodic boundary conditions, then KT1 = 1
(in fact, this relation is approximately true for most blurs and boundary conditions). Then, if we take
τk = 1 for all k, the iteration for f (k+1) can be written as
f (k+1) = f (k) + τkd(k)
= f (k) + f (k) ⊙KT(g −β1 −Kf (k)) ⊘(Kf (k) + β1 + σ 21)
= f (k) + f (k) ⊙KT 
(g + σ 21) ⊘(Kf (k) + β1 + σ 21) −1

= f (k) + f (k) ⊙

KT 
(g + σ 21) ⊘(Kf (k) + β1 + σ 21)

−1

= f (k) + f (k) ⊙KT 
(g + σ 21) ⊘(Kf (k) + β1 + σ 21)

−f (k)
= f (k) ⊙KT 
(g + σ 21) ⊘(Kf (k) + β1 + σ 21)

.

234
CHAPTER 7 Iterative Methods for Image Restoration
Summarizing this, we obtain the Richardson-Lucy algorithm [16,101,102]:
Algorithm: Richardson-Lucy
given:
K, g, σ 2, β
choose:
initial guess for f ≥0
compute:
c = Kf + β1 + σ 21
g = g + σ 21
r = g −Kf
while (not stop) do
v = KT(g ⊘c)
f = f ⊙v
r = g −Kf
c = Kf + β1 + σ 21
end while
Although the residual is not needed in the above algorithm, we compute it anyway since it is often
used in the stopping criteria.
It was observed in [21] that the weighted MRNSD method (i.e., the algorithm that uses C−1
η
=
diag(g + σ 21) often preforms much better (in terms of speed of convergence as well as in quality of
solution) than the other approaches discussed in this section. Some of these issues will be discussed in
more detail in the next section.
4.07.5.3 MATLAB notes
The algorithms described in this section can be implemented very much like those for the unconstrained
image restoration problem. In the case of the statistically motivated problems, it is necessary to include
additional information about the noise, such as β and σ 2, if it is available. In all of the data described in
Section 4.07.3.5, and which is available at www.mathcs.emory.edu/∼nagy/RestoreTools, we include
the noise information in a structure NoiseInfo:
•
NoiseInfo.PoissonParam is the Poisson parameter, β.
•
NoiseInfo.GaussianStdev is the standard deviation, σ, for the white Gaussian noise.
4.07.6 Examples
In this section we present some numerical results illustrating the effectiveness of the iterative methods
described in this chapter. We use the test data described in Section 4.07.3 to see how the methods
perform on a variety of images and blurring models. We use the relative restoration error
∥f (k) −f true∥2
∥f true∥2

4.07.6 Examples
235
as a measure for the accuracy of the reconstructions. We present the relative errors achieved within the
ﬁrst 100 iterations. The blurred and noisy images g contain both Poisson and Gaussian components, as
described in Section 4.07.5.2.2, with Poisson parameter β = 10 for the background noise, and standard
deviation σ = 5 for the Gaussian readout noise. KT g is used as an initial guess. All computations were
done in MATLAB 7.10.
4.07.6.1 Unconstrained iterative methods
In this subsection we show results obtained for the unconstrained iterative methods. In our implemen-
tation of the Richardson iteration we use τ =
1
∥K∥1∥K∥∞as a default choice for the step length.
Our ﬁrst example is a test problem that is widely used for testing algorithms in astronomical image
restoration. The true object and blurred image (d/r0 = 30), both of size 256 × 256, are shown in
Figure 7.6. In Figure 7.12 we compare the performance of the unconstrained iterative methods for this
example. The results show that LSQR, CGLS, and HyBR (the hybrid method) signiﬁcantly outperform
the Richardson iteration and steepest descent in achieving better relative errors at the same number of
iterations. Note that in Figure 7.12, the convergence history of CGLS and LSQR are indistinguishable
for this example. Although this makes sense since the two algorithms are mathematically equivalent, it
is often stated in the literature that LSQR has slightly better numerical stability properties than CGLS.
0
20
40
60
80
100
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Iterations
Relative error
 
 
RI
SD
LSQR
CGLS
HyBR
RI
SD
LSQR
HYBR
FIGURE 7.12
Convergence history of the unconstrained methods and restorations computed at 47th iteration, using the
spatially invariant atmospheric turbulence blur data.

236
CHAPTER 7 Iterative Methods for Image Restoration
The semi-convergence behavior of LSQR and CGLS, which was discussed in Section 4.07.4.4, is
clearlyseen;afterachievingaminimalvalueatiterationk = 47,theerrorsforthesetwomethodsincrease
if the iterations are allowed to proceed. Notice that HyBR avoids this semi-convergence behavior. We
note that HyBR has a default stopping criteria, which is based on generalized cross validation [82]. For
this test data it suggests terminating at iteration k = 39, but we force it run for 100 iterations so that
we can observe how it avoids the semi-convergence behavior that occurs with LSQR and CGLS. The
solution obtained by CGLS is not shown since its restoration error is the same as that of LSQR. The
LSQR solution has the smallest restoration error and is visually the best; see Figure 7.12.
For the second experiment, we choose the spatially invariant Gaussian blur example. The 256 × 256
true image and the blurred image (α1 = 4, α2 = 2, ρ = 2) with noise are shown in Figure 7.4.
Figure 7.13 shows the restoration errors and the restored images for different unconstrained iterative
methods. We observe small differences in the convergence history of this test problem compared with
the previous one. LSQR and CGLS, which again are indistinguishable, reveal the semi-convergence
behavior earlier when the methods are run to high iteration counts. Richardson iteration and steepest
0
20
40
60
80
100
0.26
0.28
0.3
0.32
0.34
0.36
0.38
Iterations
Relative error
RI
SD
LSQR
CGLS
HyBR
RI
SD
LSQR
HYBR
FIGURE 7.13
Convergence history of the unconstrained methods and restorations computed at 29th iteration, using the
spatially invariant Gaussian blur data.

4.07.6 Examples
237
descent are again the slowest. However the restoration errors corresponding to these methods tend
to approach more quickly the minimum restoration error achieved by LSQR and CGLS. The hybrid
method shows a similar convergence behavior as in the previous example but it achieves the minimum
error of LSQR and CGLS within 100 iterations. We note that HyBR suggests to stop at iteration 16 for
this test problem, but we run it for 100 iterations to observe the stable convergence behavior. LSQR and
CGLS achieve the minimum error at iteration 29.
In the third experiment, we address the performance of the unconstrained iterative methods in the
spatially variant Gaussian blur example. The true image is 316 × 316 and we use the blurred noisy
image corresponding to case 2 in Figure 7.8. The minimum restoration error for LSQR and CGLS is
achieved at iteration 20 (Figure 7.14). It can be seen that HyBR has the lowest relative errors for this test
problem. HyBR suggests to stop at iteration 16 for this test problem, but, as with previous examples,
we run it for 100 iterations.
As a last experiment we address the spatially variant motion blur example. The true object and its
blurred and noisy image (medium motion) are shown in Figure 7.11. The image size is 256 × 256.
0
20
40
60
80
100
0.36
0.37
0.38
0.39
0.4
0.41
0.42
0.43
0.44
0.45
0.46
Iterations
Relative error
RI
SD
LSQR
CGLS
HyBR
RI
SD
LSQR
HYBR
FIGURE 7.14
Convergence history of the unconstrained methods and restorations computed at 20th iteration, using the
spatially variant Gaussian blur data.

238
CHAPTER 7 Iterative Methods for Image Restoration
0
20
40
60
80
100
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
Iterations
Relative error
RI
SD
LSQR
CGLS
HyBR
RI
SD
LSQR
HYBR
FIGURE 7.15
Convergence history of the unconstrained methods and restorations computed at 35th iteration, using the
variant motion blur data.
The convergence history and the restored images for this test problem are shown in Figure 7.15. The
solution with minimum error for LSQR and CGLS is reached at iteration count k = 35. We again
note that the hybrid method would normally stop at iteration 95 for this test problem (with a stopping
condition corresponding to ﬂat GCV curve) but we let it perform the maximum number of iterations.
In general, we can observe that as expected Richardson iteration is the slowest unconstrained iterative
method. Steepest descent shows lower relative errors than Richardson iteration but it has a similar
slow convergence rate. LSQR and CGLS outperform both Richardson iteration and steepest descent.
Howeverbothofthesemethodsshowthesemi-convergencebehaviorwhichiscommonamongconjugate
gradient type methods. The hybrid method avoids this semi-convergence behavior and in some of the
test problems reaches an overall lower relative error compared to LSQR and CGLS.
4.07.6.2 Nonnegativity constrained iterative methods
In this section we use methods that enforce nonnegativity at each iteration. For the spatially invariant
atmospheric turbulence blur example we compare the convergence history of all the nonnegatively

4.07.6 Examples
239
0
20
40
60
80
100
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Iterations
Relative error
RINN
SDNN
MRNSD
WMRNSD
KWMRNSD
RL
FIGURE 7.16
Convergence history of the nonnegatively constrained methods, using the spatially invariant atmospheric
turbulence blur data.
0
20
40
60
80
100
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Iterations
Relative error
LSQR
WMRNSD
KWMRNSD
LSQR
WMRNSD
KWMRNSD
FIGURE 7.17
Convergence history of WMRNSD, KWMRNSD, the unconstrained LSQR, and restorations computed at 47th
iteration, using the spatially invariant atmospheric turbulence blur data.

240
CHAPTER 7 Iterative Methods for Image Restoration
0
20
40
60
80
100
0.26
0.28
0.3
0.32
0.34
0.36
0.38
Iterations
Relative error
 
 
LSQR
WMRNSD
KWMRNSD
LSQR
WMRNSD
KWMRNSD
FIGURE 7.18
Convergence history of WMRNSD, KWMRNSD, the unconstrained LSQR, and restorations computed at 29th
iteration, using the spatially invariant Gaussian blur data.
constrained iterative methods explained in Section 4.07.5. Figure 7.16 shows the restoration errors
obtained for the ﬁrst test problem. Observe that, as with the unconstrained version, the nonnegatively
constrained Richardson iteration has the slowest convergence rate. We see that KWMRNSD is the most
effective algorithm for the atmospheric blur data. Also, we observed that the rest of our test problems
produced similar results, in that WMRNSD and KWMRNSD (k-weighted MRNSD) outperform all
other constrained algorithms. Thus for the rest of this section, we show only the convergence history
of WMRNSD and KWMRNSD compared to the convergence history of the unconstrained LSQR. This
comparison for the atmospheric turbulence blur is shown in Figure 7.17.
We also compare WMRNSD and KWMRNSD with LSQR for the spatially invariant Gaussian blur
test problem. The convergence history and restored images are shown in Figure 7.18. LSQR reaches its
minimum relative error at iteration k = 29. Compared with LSQR, the convergence of the KWMRNSD
relative errors is very similar at early iterations. After LSQR reaches its minimum error, the relative
errors for KWMRNSD keep decreasing. Visually, the restorations look similar.

4.07.6 Examples
241
0
20
40
60
80
100
0.37
0.38
0.39
0.4
0.41
0.42
0.43
0.44
0.45
0.46
Iterations
Relative error
LSQR
WMRNSD
KWMRNSD
LSQR
WMRNSD
KWMRNSD
FIGURE 7.19
Convergence history of WMRNSD, KWMRNSD, the unconstrained LSQR, and restorations computed at 20th
iteration, using the spatially variant Gaussian blur data.
We now examine the performance of WMRNSD, KWMRNSD, and LSQR when applied to the
spatially variant Gaussian blur example. The convergence history of the relative errors and the restored
images are shown in Figure 7.19. The restored images correspond to the turning point in the convergence-
history curve of LSQR. It can be seen that in this test problem the unconstrained LSQR outperforms the
other two constrained methods. Note that one difﬁculty with the constrained algorithms presented in
this chapter is that if a pixel value becomes zero in the iteration, it usually remains zero for subsequent
iterations. By looking at the center of the image, such a situation appears to occur with this test problem,
and some of the ﬁne details are lost.
We also observe from the convergence plot for this example that the system is highly ill-conditioned.
Speciﬁcally, inversion of noise corrupts the approximations after just a few iterations; relative errors
corresponding to LSQR quickly increase after iteration k = 20. We can also observe that the relative
errors corresponding to WMRNSD and KWMRNSD show a tendency of increasing at higher iterations.
In such cases it is important to have a reliable stopping criteria. Recalling the convergence history

242
CHAPTER 7 Iterative Methods for Image Restoration
0
20
40
60
80
100
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
Iterations
Relative error
 
 
LSQR
WMRNSD
KWMRNSD
LSQR
WMRNSD
KWMRNSD
FIGURE 7.20
Convergence history of WMRNSD, KWMRNSD, the unconstrained LSQR, and restorations computed at 35th
iteration, using the variant motion blur data.
(Figure 7.14) for the unconstrained problem, HyBR seems to be the best choice for this test problem
since it can control the semi-convergence and estimate an appropriate stopping iteration.
The results for the spatially variant motion blur example are shown in Figure 7.20. We can observe that
KWMRNSD has superior convergence properties compared to WMRNSD at early iterations. Also, both
nonnegatively constrained methods perform better than the unconstrained LSQR at higher iterations.
The WMRNSD and KWMRNSD restorations show less artifacts compared to LSQR.
4.07.7 Concluding remarks and open questions
In this chapter we have described a variety of iterative methods that can be used for image restoration,
and compared their performance on a variety of test problems. The data and MATLAB codes used in this
chapterareavailableat http://www.mathcs.emory.edu/∼nagy/RestoreTools.Thenumericalexperiments
illustrate that there is not one best method for all problems, which explains the vast literature on this topic.
It is important to emphasize that our presentation is far from being a complete survey. For example, we

References
243
did not discuss Bayesian methods, such as those described in [103], or more sophisticated constrained
algorithms such as those described in [25,27]. Moreover, we only brieﬂy mentioned techniques for
choosing regularization parameters and stopping criteria; for some additional work on these topics, see,
for example, [26,83]. One other class of methods that we did not discuss, but is worth further study in the
context of constrained iterative algorithms, is projections onto convex sets (POCS) [61–63]. The POCS
approach provides a powerful mechanism, with rigorous mathematical justiﬁcation, to incorporate a
variety of constraints on the object and PSF.
Acknowledgments
The research of J. Nagy is supported by the United States National Science Foundation (NSF) under grant DMS-
0811031, and by the United States Air Force Ofﬁce of Scientiﬁc Research (AFOSR) under grant FA9550-09-1-0487.
Relevant theory: Signal Processing
See Vol. 1, Chapter 4 Random Signals and Stochastic Processes
See Vol. 1, Chapter 5 Sampling and Quantization
See Vol. 1, Chapter 6 Digital Filter Structures and Their Implementations
See Vol. 1, Chapter 7 Multirate Signal Processing for Software Radio Architectures
See Vol. 1, Chapter 9 Discrete Multi-Scale Transforms in Signal Processing
See Vol. 1, Chapter 11 Parametric Estimation
See Vol. 1, Chapter 12 Adaptive Filters
References
[1] S.R. McNown, B.R. Hunt. Approximate shift-invariance by warping shift-variant systems, in: R.J. Hanisch,
R.L. White (Eds.), The Restoration of HST Images and Spectra II, 1994, pp. 181–187.
[2] G.M. Robbins, T.S. Huang, Inverse ﬁltering for linear shift-variant imaging systems, Proc. IEEE 60 (1972)
862–872.
[3] A.A. Sawchuk, Space-variant image restoration by coordinate transformations, J. Opt. Soc. Am. 64 (1974)
138–144.
[4] D.A. Fish, J. Grochmalicki, E.R. Pike, Scanning singular-value-decomposition method for restoration of
images with space-variant blur, J. Opt. Soc. Am. A 13 (1996) 1–6.
[5] H.J. Trussell, S. Fogel, Identiﬁcation and restoration of spatially variant motion blurs in sequential images,
IEEE Trans. Image Process. 1 (1992) 123–126.
[6] H.J. Trussell, B.R. Hunt, Image restoration of space-variant blurs by sectional methods. IEEE Trans. Acoust.
Speech Signal Process. 26 (1978) 608–609.
[7] A.F. Boden, D.C. Redding, R.J. Hanisch, J. Mo, Massively parallel spatially-variant maximum likelihood
restoration of Hubble Space Telescope imagery, J. Opt. Soc. Am. A 13 (1996) 1537–1545.
[8] M. Faisal, A.D. Lanterman, D.L. Snyder, R.L. White, Implementation of a modiﬁed Richardson-Lucy method
for image restoration on a massively parallel computer to compensate for space-variant point spread function
of a charge-coupled device camera, J. Opt. Soc. Am. A 12 (1995) 2593–2603.
[9] J.G. Nagy, D.P. O’Leary, Fast iterative image restoration with a spatially varying PSF, in: F.T. Luk (Ed.),
Advanced Signal Processing Algorithms, Architectures, and Implementations VII, vol. 3162, SPIE, 1997,
pp. 388–399.

244
CHAPTER 7 Iterative Methods for Image Restoration
[10] J.G. Nagy, D.P. O’Leary, Restoring images degraded by spatially-variant blur, SIAM J. Sci. Comput. 19
(1998) 1063–1082.
[11] T.L. Faber, N. Raghunath, D. Tudorascu, J.R. Votaw, Motion correction of PET brain images through decon-
volution: I. Theoretical development and analysis in software simulations, Phys. Med. Biol. 54 (3) (2009)
797–811.
[12] N. Raghunath, T.L. Faber, S. Suryanarayanan, J.R. Votaw, Motion correction of PET brain images through
deconvolution: II. Practical implementation and algorithm optimization, Phys. Med. Biol. 54 (3) (2009)
813–829.
[13] H.W. Engl, M. Hanke, A. Neubauer, Regularization of Inverse Problems, Kluwer Academic Publishers,
Dordrecht, 2000.
[14] P.C. Hansen, Rank-Deﬁcient and Discrete Ill-Posed Problems, SIAM, Philadelphia, PA, 1997.
[15] P.C. Hansen, Discrete Inverse Problems: Insight and Algorithms, SIAM, Philadelphia, PA, 2010.
[16] C.R. Vogel, Computational Methods for Inverse Problems, SIAM, Philadelphia, PA, 2002.
[17] H.C. Andrews, B.R. Hunt, Digital Image Restoration, Prentice-Hall, Englewood Cliffs, NJ, 1977.
[18] P.C. Hansen, J.G. Nagy, D.P. O’Leary, Deblurring Images: Matrices, Spectra and Filtering. SIAM,
Philadelphia, PA, 2006.
[19] M.Bertero,P.Boccacci,IntroductiontoInverseProblemsinImaging,InstituteofPhysicsPublishing,London,
1998.
[20] R.L. Lagendijk, J. Biemond, Iterative Identiﬁcation and Restoration of Images, Kluwer Academic Publishers,
Boston/Dordrecht/London, 1991.
[21] J.M. Bardsley, J.G. Nagy, Covariance-preconditioned iterative methods for nonnegatively constrained astro-
nomical imaging, SIAM J. Matrix Anal. Appl. 27 (2006) 1184–1197.
[22] J.G. Nagy, K.M. Palmer, L. Perrone. Iterative methods for image deblurring: a Matlab object oriented appro-
ach, Numer. Algorithms 36 (2004) 73–93, See also: <http://www.mathcs.emory.edu/∼nagy/RestoreTools>.
[23] C.W. Groetsch, The Theory of Tikhonov Regularization for Fredholm Integral Equations of the First Kind,
Pitman, Boston, 1984.
[24] A.K. Jain, Fundamentals of Digital Image Processing, Prentice-Hall, Englewood Cliffs, NJ, 1989.
[25] J.M. Bardsley, An efﬁcient computational method for total variation-penalized Poisson likelihood estimation,
Inverse Probl. Imag. 2 (2) (2008) 167–185.
[26] J.M. Bardsley, Stopping rules for a nonnegatively constrained iterative method for ill-posed Poisson imaging
problems, BIT 48 (4) (2008) 651–664.
[27] J.M. Bardsley, C.R. Vogel, A nonnegatively constrained convex programming method for image reconstruc-
tion, SIAM J. Sci. Comput. 25 (4) (2003) 1326–1343.
[28] A.N. Tikhonov, Regularization of incorrectly posed problems, Soviet Math. Doklady 4 (1963) 1624–1627.
[29] A.N. Tikhonov, Solution of incorrectly formulated problems and the regularization method, Soviet Math.
Doklady 4 (1963) 501–504.
[30] A.N. Tikhonov, V.Y. Arsenin, Solutions of Ill-Posed Problems, Winston, Washington, D.C., 1977.
[31] T.F. Chan, J. Shen, Image Processing and Analysis: Variational, PDE, Wavelet, and Stochastic Methods,
SIAM, Philadelphia, PA, 2005.
[32] L.I. Rudin, S. Osher, E. Fatemi, Nonlinear total variation based noise removal algorithms, Physica D 60
(1992) 259–268.
[33] E.J. Candès, J.K. Romberg, T. Tao, Stable signal recovery from incomplete and inaccurate measurements,
Comm. Pure Appl. Math. 59 (8) (2006) 1207–1223.
[34] M.A.T. Figueiredo, R.D. Nowak, S.J. Wright, Gradient projection for sparse reconstruction: application to
compressed sensing and other inverse problems, IEEE J. Sel. Top. Signal Process. 1 (4) (2007) 586–597.
[35] Y. Tsaig, D.L. Donoho, Extensions of compressed sensing, Signal Process. 86 (3) (2006) 549–571.

References
245
[36] K. Miller, Least squares methods for ill-posed problems with a prescribed bound, SIAM J. Math. Anal. 1 (1)
(1970) 52–74.
[37] D.L. Phillips, A technique for the numerical solution of certain integral equations of the ﬁrst kind, J. Assoc.
Comput. Machinery 9 (1) (1962) 84–97.
[38] G.H. Golub, M. Heath, G. Wahba, Generalized cross-validation as a method for choosing a good ridge
parameter, Technometrics 21 (2) (1979) 215–223.
[39] C. Calvetti, P.C. Hansen, L. Reichel, L-curve curvature bounds via Lanczos bidiagonalization, Electron.
Trans. Numer. Anal. 14 (2002) 20–35.
[40] Y.-W. Fan, J.G. Nagy, Synthetic boundary conditions for image deblurring, Linear Algebra Appl. 434 (2010)
2244–2268.
[41] S. Serra-Capizzano, A note on antireﬂective boundary conditions and fast deblurring models, SIAM J. Sci.
Comput. 25 (4) (2003) 1307–1325.
[42] S.J. Reeves, Fast image restoration without boundary artifacts, IEEE Trans. Image Process. 14 (2005)
1448–1453.
[43] T.A. Davis, Direct Methods for Sparse Linear Systems, SIAM, Philadelphia, PA, 2006.
[44] M. Benzi, Preconditioning techniques for large linear systems: a survey, J. Comput. Phys. 182 (2002)
418–477.
[45] A. Greenbaum, Iterative Methods for Solving Linear Systems, SIAM, Philadelphia, 1997.
[46] Y. Saad, Iterative Methods for Sparse Linear Systems, second ed., SIAM, Philadelphia, 2003.
[47] M. Hanke, J.G. Nagy, Restoration of atmospherically blurred images by symmetric indeﬁnite conjugate
gradient techniques, Inverse Probl. 12 (1996) 157–173.
[48] M. Hanke, J.G. Nagy, R.J. Plemmons, Preconditioned iterative regularization for ill-posed problems, in:
L. Reichel, A. Ruttan, R.S. Varga (Eds.), Numerical Linear Algebra, de Gruyter, Berlin, 1993, pp. 141–163.
[49] J.G. Nagy, R.J. Plemmons, T.C. Torgersen, Iterative image restoration using approximate inverse precondi-
tioning, IEEE Trans. Image Process. 15 (1996) 1151–1162.
[50] J.G. Nagy, V.P. Pauca, R.J. Plemmons, T.C. Torgersen, Space-varying restoration of optical images, J. Optical
Soc. Am. A 14 (1997) 3162–3174.
[51] R.H. Chan, J.G. Nagy, R.J. Plemmons, FFT-based preconditioners for Toeplitz-block least squares, SIAM
J. Numer. Anal. 30 (1993) 1740–1768.
[52] S.M. Jefferies, M. Hart, Deconvolution from wave front sensing using the frozen ﬂow hypothesis, Opt.
Express 19 (2011) 1975–1984.
[53] E.S. Angel, A.K. Jain, Restoration of images degraded by spatially varying pointspread functions by a
conjugate gradient method, Appl. Opt. 17 (1978) 2186–2190.
[54] J.G. Nagy, J. Kamm. Kronecker product and SVD approximations for separable spatially variant blurs,
in: F.T. Luk (Ed.), Advanced Signal Processing Algorithms, Architectures, and Implementations VIII,
vol. 3461. SPIE, 1998, pp. 358–369.
[55] G.H. Golub, C. Van Loan. Matrix Computations, third ed., Johns Hopkins Press, 1996.
[56] V.A. Morozov, Regularization Methods for Ill-Posed Problems, CRC Press, Boca Raton, FL, 1993.
[57] M. Benzi, Gianfranco Cimmino’s contributions to numerical mathematics, in: Atti Del Seminario di Analisi
Matematica, Volume Speciale: Ciclo di Conferenze in Ricordo di Gianfranco Cimmino, Marzo-Maggio
2004, Dipartimento di Matematica dellUniversitá di Bologna, 2005, pp. 87–109.
[58] Å. Björck, Numerical Methods for Least Squares Problems, SIAM, Philadelphia, PA, 1996.
[59] V.N. Strakhov, S.V. Vorontsov, Digital image deblurring with SOR, Inverse Probl. 24 (2) (2008),
<http://dx.doi.org/10.1088/0266-5611/24/2/025024>.
[60] S.V. Vorontsov, V.N. Strakhov, S.M. Jefferies, K.J. Borelli, Deconvolution of astronomical images using
SOR with adaptive relaxation, Opt. Express 19 (14) (2011) 13509–13524.

246
CHAPTER 7 Iterative Methods for Image Restoration
[61] P.L. Combettes, The foundations of set theoretic estimation, Proc. IEEE 91 (1993) 182–208.
[62] M.I. Sezan, H.J. Trussell, Prototype image constraints for set-theoretic image restoration, IEEE Trans.
Signal Process. 39 (1991) 2275–2285.
[63] G. Sharma, H.J. Trussell, Set theoretic signal restoration using an error in variables criterion, IEEE Trans.
Image Process. 6 (1997) 1692–1697.
[64] P.L. Combettes, Convex set theoretic image recovery by extrapolated iterations of parallel subgradient
projections, IEEE Trans. Image Process. 6 (1997) 493–506.
[65] P. Brianzi, F. Di Bendetto, C. Estatico, Improvement of space-invariant image deblurring by preconditioned
Landweber iterations, SIAM J. Sci. Comput. 30 (2008) 1430–1458.
[66] R.H. Chan, M.K. Ng, Conjugate gradient methods for Toeplitz systems, SIAM Rev. 38 (1996) 427–482.
[67] J. Kamm, J.G. Nagy, Kronecker product and SVD approximations in image restoration, Linear Algebra
Appl. 284 (1998) 177–192.
[68] J. Kamm, J.G. Nagy, Optimal Kronecker product approximation of block Toeplitz matrices, SIAM J. Matrix
Anal. Appl. 22 (2000) 155–172.
[69] J.G. Nagy, M.K. Ng, L. Perrone, Kronecker product approximation for image restoration with reﬂexive
boundary conditions, SIAM J. Matrix Anal. Appl. 25 (2004) 829–841.
[70] J.G. Nagy, K.M. Palmer, Steepest descent, CG, and iterative regularization of ill-posed problems, BIT 43
(5) (2005) 1003–1017.
[71] H. Brakhage, On ill-posed problems and the method of conjugate gradients, in: H.W. Engl, C.W. Groetsch
(Eds.), Inverse and Ill-Posed Problems, Academic Press, Boston, 1987, pp. 165–175.
[72] J. Barzilai, J.M. Borwein, Two-point step size gradient methods, IMA J. Numer. Anal. 8 (1) (1988)
141–148.
[73] C.C. Paige, M.A. Saunders, Algorithm 583 LSQR: sparse linear equations and sparse least squares problems,
ACM Trans. Math. Softw. 8 (1982) 195–209.
[74] C.C. Paige, M.A. Saunders, LSQR: an algorithm for sparse linear equations and sparse least squares, ACM
Trans. Math. Softw. 8 (1982) 43–71.
[75] G.H. Golub, F.T. Luk, M.L. Overton, A block Lanczos method for computing the singular values and
corresponding singular vectors of a matrix, ACM Trans. Math. Softw. 7 (2) (1981) 149–169.
[76] Y. Saad, On the rates of convergence of the Lanczos and the block-Lanczos methods, SIAM J. Numer. Anal.
17 (5) (1980) 687–706.
[77] J.M. Varah, Pitfalls in the numerical solution of linear ill-posed problems, SIAM J. Sci. Stat. Comput. 4 (2)
(1983) 164–176.
[78] C.R. Vogel, Optimal choice of a truncation level for the truncated SVD solution of linear ﬁrst kind integral
equations when data are noisy, SIAM J. Numer. Anal. 23 (1) (1986) 109–117.
[79] M. Hanke. Conjugate Gradient Type Methods for Ill-Posed Problems, Pitman Research Notes in Mathemat-
ics, Longman Scientiﬁc and Technical, Harlow, Essex, 1995.
[80] D.P. O’Leary, J.A. Simmons, A bidiagonalization-regularization procedure for large scale discretizations of
ill-posed problems, SIAM J. Sci. Stat. Comput. 2 (4) (1981) 474–489.
[81] Å. Björck, A bidiagonalization algorithm for solving large and sparse ill-posed systems of linear equations,
BIT 28 (3) (1988) 659–670.
[82] J. Chung, J.G. Nagy, D.P. O’Leary, A weighted GCV method for Lanczos hybrid regularization, Electron.
Trans. Numer. Anal. 28 (2008) 149–167.
[83] M.E. Kilmer, D.P. O’Leary, Choosing regularization parameters in iterative methods for ill-posed problems,
SIAM J. Matrix Anal. Appl. 22 (4) (2001) 1204–1221.
[84] Å. Björck, E. Grimme, P. van Dooren, An implicit shift bidiagonalization algorithm for ill-posed systems,
BIT 34 (4) (1994) 510–534.
[85] D. Calvetti, L. Reichel, Tikhonov regularization of large linear problems, BIT 43 (2) (2003) 263–283.

References
247
[86] M. Hanke, On Lanczos based methods for the regularization of discrete ill-posed problems, BIT 41 (5)
(2001) 1008–1018.
[87] M.E. Kilmer, P.C. Hansen, M.I. Español, A projection-based approach to general-form Tikhonov regular-
ization, SIAM J. Sci. Comput. 29 (1) (2007) 315–330.
[88] R.M. Larsen. Lanczos bidiagonalization with partial reorthogonalization, Ph.D. Thesis, Department of
Computer Science, University of Aarhus, Denmark, 1998.
[89] C.T. Kelley, Iterative Methods for Optimization, SIAM, Philadelphia, 1999.
[90] J. Nocedal, S. Wright, Numerical Optimization, Springer, New York, 1999.
[91] M. Piana, M. Bertero, Projected Landweber method and preconditioning, Inverse Probl. 13 (1997) 441–463.
[92] C.W. Cryer, The solution of a quadratic programming problem using systematic overrelaxation, SIAM J.
Control 9 (3) (1971) 385–392.
[93] D. Kim, S. Sra, I.S. Dhillon. A non-monotonic method for large-scale nonnegative least squares, 2011.
<http://www.optimization-online.org/DB_FILE/2010/05/2629.pdf>.
[94] D. Calvetti, G. Landi, L. Reichel, F. Sgallari, Non-negativity and iterative methods for ill-posed problems,
Inverse Probl. 20 (2004) 1747–1758.
[95] L. Kaufman, Maximum likelihood, least squares, and penalized least squares for PET, IEEE Trans. Med.
Imaging 12 (1993) 200–214.
[96] M.H. DeGroot, Probability and Statistics, Addison-Wesley, Reading, MA, 1989.
[97] W. Feller, An Introduction to Probability Theory and Its Applications, Wiley, New York, 1971.
[98] D.L. Snyder, C.W. Hammoud, R.L. White, Image recovery from data acquired with a charge-coupled-device
camera, J. Opt. Soc. Am. A 10 (1993) 1014–1023.
[99] D.L. Snyder, C.W. Helstrom, A.D. Lanterman, Compensation for readout noise in CCD images, J. Opt. Soc.
Am. A 12 (1994) 272–283.
[100] J.G. Nagy, Z. Strakoš. Enforcing nonnegativity in image reconstruction algorithms, in: D.C. Wilson, et al.
(Eds.), Mathematical Modeling, Estimation, and Imaging, vol. 3461. SPIE, 2000, pp. 182–190.
[101] B. Lucy, An iterative method for the rectiﬁcation of observed distributions, Astron. J. 79 (1974) 745–754.
[102] W.H. Richardson, Bayesian-based iterative methods for image restoration, J. Opt. Soc. Am. 62 (1972) 55–59.
[103] D. Calvetti, E. Somersalo, Introduction to Bayesian Scientiﬁc Computing, Springer-Verlag, New York, 2007.

8
CHAPTER
Image Processing at Your
Fingertips: The New Horizon
of Mobile Imaging
Xin Li
Lane Department of Computer Science and Electrical Engineering,
West Virginia University, Morgantown, WV, USA
4.08.1 Historical background and overview
Rapid evolution of sensing, computing, and communication technologies has redeﬁned the way we
collect, share, and interact with digital media. In particular, the art of electronic imaging (image sensing
or acquisition) has witnessed two new trends in recent years. First, the miniaturization of imaging
devices has facilitated their use in various mobile applications ranging from smartphones and tablets
to wireless and sensor networks. Second, the integration of sensing with computing has given birth to
several emerging ﬁelds such as computational photography, compressed sensing, and cyber-physical
systems. These technological advances motivate us to take a fresh look at a conventional ﬁeld such as
image processing—namely, how will image processing evolve as the consequence of miniaturization
and integration?
The impact seems at least twofold. On one hand, mobile devices such as smartphones often have
more severe power and bandwidth constraints than non-mobile ones. Therefore, it is desirable to allocate
limited resources to preserve the image content of the highest interest—e.g., objects in the foreground
should be given higher priority than those in the background. In fact, such an object-oriented concept
has already been implemented by single-lens reﬂex (SLR)1 imaging in an optical way at the price of
higher hardware cost. Given the low-cost of CMOS sensors, can image processing be combined with
the interactivity of smartphones to facilitate the realization of this concept under the framework of
mobile imaging? On the other hand, despite the success of multi-touch and pressure-sensitive interfaces
with large touchscreens, their potential on small touch devices remains unleashed. At the intersection
of mobile computing and human-computer interaction, there still exists plenty of “no-man’s land” to
be explored. The long-term objective along this line of reasoning is to think of mobile imaging as an
extended sensing platform and to enrich one’s sensing capability through tapping into the network of
peer users.
As the ﬁrst step, we propose that image processing at the ﬁngertips is a timely topic deserving our
community’s attention. Why is it important? We argue that recasting image processing under a mobile
imaging/computing framework could shed new insights to various analysis and restoration tasks that can
beneﬁt from the interaction between human and mobile devices. Meantime, new technical challenges
related to small size of display screen, limitation of battery power and constraint on viewing conditions
1 http://en.wikipedia.org/wiki/Single-lens_reﬂex_camera.
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00008-X
© 2014 Elsevier Ltd. All rights reserved.
249

250
CHAPTER 8 Image Processing at Your Fingertips
imply plenty of opportunities for engineers to optimize their design and implementation. In particular,
joint optimization of imaging and processing in a close-loop fashion is likely to play an increasingly
important role in various emerging ﬁelds including computational photography and cyber-physical
systems.
Inthisnote,wewillﬁrstreviewthetwothreadsoftechnologicalevolution:mobileimagingandmobile
OS in Sections 4.08.2 and 4.08.3. They have followed different evolutionary paths but shared many
similar patterns such as the co-evolution with microelectronics and networks. Then we will elaborate
on the implications of intertwining these two threads in Section 4.08.4. At their intersection, there exist
plenty of fascinating opportunities for new developments and applications of image processing at the
ﬁngertips. A sampler of applications will de discussed in Section 4.08.5 and we will address several
open issues in Section 4.08.6.
4.08.2 Mobile imaging: following Feynman’s idea
on inﬁnitesimal machinery
“There’s Plenty of Room at the Bottom” is the title of a lecture [1] given by the legendary physicist
Richard Feynman at an American Physical Society meeting held at Caltech on December 29, 1959.
At least two prophecies made by Feynman have come true: write-small (nanoscale lithography) and
miniaturization of computers (VLSI technologies). Although the problem of manipulating and con-
trolling image sensors on a small scale did not appear in the original list of Feynman, it has followed
an evolutionary path similar to other technologies—the competition between charge-coupled device
(CCD) and complementary metal oxide semiconductor (CMOS) technologies in the past four decades
represents the challenge and promise of miniaturizing image sensors.
CCDs and CMOS imagers were both invented in the late 1960s to early 1970s. However, their
growth patterns have varied signiﬁcantly as pointed out in [2]—CCD has dominated the ﬁrst 25 years
and CMOS only started to catch up since mid-1990s after the break of bottleneck caused by lithography
technology. The increasing market share of CMOS sensors in the recent ﬁve years has been largely
attributed to the growing popularity of smartphones. Therefore, we deem it a good lesson to learn from
the winding roads of CMOS sensors (as compared against the straight path of CCD sensors—more
details can be found at http://www.teledynedalsa.com/corp/markets/ccd_vs_cmos.aspx). The moral of
this story is that like many other technology sectors, the co-evolution of imaging and microfabrication
offers a more reliable prediction than an isolated point of view.
Why did CCD win at the start? According to [2], the competing edge of CCD sensors is largely due to
their higher image quality (as measured by signal-to-noise-ratio)—CMOS image sensors required more
uniformity and smaller features than silicon wafer foundries could deliver at the time. The excellent
noise performance of CCD make it less demanding for engineers to deal with than CMOS as constrained
by the fabrication technology available in 1960–1970s. In fact, the design of CCD sensors is relatively
so straightforward that it can be viewed as an over-simpliﬁed solution to the CMOS design (refer
to [2]). The advantages of CMOS over CCD including lower power consumption, camera-on-a-chip
integration (e.g., [3]), and lowered fabrication costs from the reuse of mainstream logic and memory
device fabrication were not technologically feasible until 1990s when lithography and process control
of microfabrication have reached their maturization.

4.08.3 Mobile Computing: Interacting with Computer without an Interface
251
How did CMOS catch up? As mentioned above, it was originally limited by lithography—a tech-
nology of writing—at the atomic scale. Earlier lithography technology relied on chemical processes for
printing; photolithography marks the recent advance in using light to control the process of microfab-
rication [4]. It is interesting to point out the key milestone of photolithography technology is related to
the use of light sources. Before 1982, lamp-based photolithography was the mainstream but it could not
catch up with the semiconductor industry’s demands for higher-resolution and higher throughput. The
pioneering invention and development made by IBM researchers in 1982 [5]—namely Excimer Laser
Lithography—has delivered what is required by the semiconductor industry. From the sensor technol-
ogy perspective, excimer laser lithography also plays the crucial role in revitalizing CMOS imaging
since mid-1990s.
It should be noted that CCD and CMOS will remain as complementary solutions in the future.
For example, CCD remains the favorite solution to high-end or professional-grade photography (e.g.,
expensive single-lens reﬂex cameras); while CMOS becomes more suitable to low-end or consumer-
grade photography (e.g., cost-effective wedcams or smartphone cameras). It has been shown that CCD
and CMOS sensors have converged to practically indistinguishable solutions to low-resolution imaging
(VGA and below) [6]; while there is still a lot of room for the development of CMOS sensors for high-
resolution imaging. Among various novel ideas, we choose to highlight the solution based on microlens
[7]—a device used to redirect light to the active area in the pixel and the principle of plenoptic imaging
[8] (e.g., Plenoptic camera with an array of 25 micro-camera developed by Pelican Imaging2). It is
difﬁcult to predict how fast the science of miniaturization will evolve in the new century.
4.08.3 Mobile computing: interacting with computer without an interface
Social interaction plays the essential role in the fabric of human societies. The advance of computing
and communication technologies enable people to interact with each other anytime and anywhere. A key
elementtosuchcomputer-basedsocialcomputing/communicationinfrastructureistheinterfacebetween
humans and computers/networks. From command lines to graphic interfaces and from sketch-pad to
multi-touch screen, the user interface facilitates the interaction between humans and machines. It is more
like an art than science because the goodness or badness of an interface often varies from individual to
individual. The success of any product such as a mouse or a tablet is often jointly determined by various
factors (e.g., marketing strategy, human psychology and cultural background) more confounding than
technology itself.
Historically, human-computer interaction (HCI) has experienced several phases: text-based (before
1980s), graphics-based (1990–2000s) and touch-based (since 2005). Their evolutionary path can be
understood from several complementary viewpoints. The trend is to interact with the computer without
a mechanical interface—from a physical keyboard to a virtual one, from carrying a mouse to using one’s
own ﬁngers. Such trend of design is in principle aligned with the idea of multi-tool (e.g., swiss army
knife and smartphones) and Feynman’s tiny machines because space limitation is a constant constraint
to all technology-based gadgets. The design often sacriﬁces certain features (e.g., the speed of typing or
2 http://www.pelicanimaging.com/.

252
CHAPTER 8 Image Processing at Your Fingertips
precision of control [9]) for the other beneﬁts such as mobility. We will take multi-touch as a concrete
example to understand such a design tradeoff.
The idea of touchscreen dated back to as early as early 1970s [10]; the concept of multi-touch has
also been studied by both academia (e.g., Univ. of Toronto [11]) and industry (e.g., Bell Labs [12])
since early 1980s. One of the early breakthroughs was Pierre Wellner’s digital desk developed at the
University of Cambridge [13]. Other notable multi-touch systems developed in the past decade include
Diamondtouch developed by Mitsubishi [14] and the system developed by Han developed at NYU [15].
No matter if you like the Android or Symbian or iPhone, the leverage of multi-touch technology into
mobile devices has announced the new era of mobile computing. There has been increasing evidence
that software/apps development for mobile platform is going to take over those for desktops.
So why do people like to tap or press or slide? On one hand, touch-based interface offers new
experience for users—the curiosity of learning is the driving force underlying the growth of many new
technologies including smartphones. It is interesting to compare mouse-clicking and screen-tapping—
eventhoughtheformerismoreprecise,thelatterimplementsashortcutbetweenhumansandmachines—
there is no need to look for a mouse or its connections any more. Meantime, touchscreen admits a rich set
of extending modes beyond tapping—namely press, tilt, scratch, shake, slide (single-ﬁnger vs. double-
ﬁnger). Those fancy ﬁnger movements—when imposed under a proper application context (e.g., gaming
vs. reading)—open a door to create novel and exciting experiences. On the other hand, touchscreen is
supported by many other desirable features of mobile devices including their portability, the ease of
turning on/off and software distribution. As the population of smartphone users keeps increasing, social
networking applications are likely to more tightly tie with mobile platforms. Under this context, there
exist plenty of opportunities for mobile imaging and image processing at ﬁngertips to further enrich the
experience of social networking.
4.08.4 Image processing at ﬁngertips: where mobile imaging meets
mobile computing
Rapid advances of sensing and interface technologies enables us to take a fresh look at several conven-
tional image processing tasks including acquisition, segmentation, mosaicing and restoration. The novel
insight comes from how user feedback can improve the accuracy of manipulating images or enhance
the experience of interacting with the image content.
4.08.4.1 Intelligent image acquisition
In conventional wisdom, image acquisition is largely viewed as a disconnected component from the
pipeline of an image processing system. The quality of acquired images is primarily determined by
the hardware—e.g., CMOS sensors embedded into smartphone cameras are often viewed as of inferior
quality to the CCD sensors used by single-lens reﬂex (SLR) cameras. However, such view can be
challenged from several perspectives such as the deﬁnition of quality and the scope of application.
Smartphone-based image acquisition can be made more intelligent along the same lines of reasoning as
the emerging ﬁeld of computational photography [16]—i.e., a hybrid (hardware + software) approach

4.08.4 Image Processing at Fingertips
253
is more fruitful and could reshape our thinking about sensor design. In this subsection, we will review
three scenarios where a hybrid approach can shed novel insights to various image-related applications.
First, the quality of acquired images can be deﬁned not under the context of human visual inspection
but that supporting vision-related applications (e.g., barcode scanner by a smartphone). It is important
to note that when understood at the system level, what truly matters to many vision-related applications
is whether the image quality is sufﬁcient to support various recognition tasks. In an image processing
system designed by the reduction principle, such an objective is difﬁcult to meet because the sensor part
has little knowledge about the recognition part (it is often called open-loop in the literature of control
theory). A more conceptually appealing approach is allowing the sensor component to adjust itself
based on the feedback from the recognition component (so-called close-loop design). How does such
adjustment help? Several smartphone applications such as RedLaser and Scan allow users to move the
camera back and forth so the image content of interest such as barcode or Quick Response (QR)-code
can be acquired in focus and recognized by the prioritized barcode or QR-code recognition algorithm.
We argue that such close-loop design is a signiﬁcant advantage supported by mobile imaging where
mobility can be put into better use at the system level.
Second, we believe intelligent image acquisition implies not only higher quality but also better
security and privacy features. Security and privacy related applications involve the authentication of
image content, copyright protection and so on. It has been long recognized that the nature of digital
images make them vulnerable to copy, manipulation and distribution in the virtual world. In addition
to various existing technological means (e.g., watermarking and forensic analysis), smartphones offers
a new opportunity of enhancing the security/forensics features of mobile imaging—namely, the spatial
location of smartphones recorded by the GPS sensor, as well as the time capsule information become
the unique spatio-temporal signature. Such authentication collected at the physical layer—after proper
protection such as cryptography—could offer a powerful line of defense against unauthorized distribu-
tion or malicious manipulation of image content. We expect that such a line of research could become
more fruitful under the context of social networking (e.g., privacy protection and identity management).
Third, a hybrid approach toward mobile imaging can support the acquisition of photos with artistic
ﬂavor. A most common example is to slow the shutter speed for the purpose of creating a light trail in the
image or an intentionally motion-blurred image (with artistic effect). Several iPhone applications (such
as Slow Shutter Cam and WaterMyPhoto) have attracted wide attention and inspired the development of
new photography applications on iOS platform. When compared with the main stream of computational
photography, we believe artistic rendering has not received sufﬁcient attention. In parallel, to extend
the capabilities of digital photography by a computer, exploitation of motion-clues (both object-related
and sensor-related) could offer refreshing ideas about how visual thinking could support reasoning.
4.08.4.2 Interactive image matting
Matting—a classical cinematography technique used to composite the foreground element into a new
scene—has been widely used in the creation of special effects [17]. The technique of digital mat-
ting was ﬁrst studied by Porter and Duff in [18]. The underlying mathematical model or so-called
composition equation is given by C = αF + (1 −α)B, where 0 < α < 1 denotes the pixel’s opacity
component and C, F, B correspond to the pixel’s composite, foreground and background colors respec-
tively. It is often assumed that some anchor regions of foreground and background are available as

254
CHAPTER 8 Image Processing at Your Fingertips
a priori knowledge (e.g., supplied by the user) to facilitate matting. One of the earliest principled
solutions to digital matting is based on a Bayesian approach [19]; since then, several alternative
approaches (e.g., Poisson matting [20], spectral matting [21], closed-form solution [22]) have been
proposed in the literature. Despite their subtle technical differences, the fundamental assumption about
the anchor regions of foreground/background is required and the quality of such anchor regions is often
crucial.
Smartphone-based mobile imaging makes it convenient to develop the protocol of anchor region
selection. After pressing the button of acquisition, one can proceed with the selection of anchor regions
by dancing his/her ﬁngers. For example, one might slide two ﬁngers in opposite direction to deﬁne
a rectangular region (e.g., such anchor region can be used by GrabCut [23]) or apply varying pres-
sures to tap the objects in the foreground and background (e.g., such anchor region can be used by
spectral matting [21]). Furthermore, one could even supply rough contour tracing results by draw-
ing consecutive line segments on the screen (e.g., such anchor region can be used by LazySnap [24])
or reﬁned anchor regions by tapping on the zoomed image (i.e., to facilitate the user’s interaction).
Figure 8.1 includes several examples of anchor regions selected by different touch protocols; a third-
party implementation of interactive segmentation toolbox (including GrabCut and LazySnap) is avail-
able at http://www.cs.cmu.edu/˜mohitg/segmentation.htm.
Wenotethattheaccuracyofanyimagemattingtechniqueisdeterminedbyhowthetheoreticalmatting
model matches the image data in practical acquisition. None of the published matting algorithms can
achieve error-free segmentation results on all test images. Therefore, it becomes plausible to design a
veriﬁcation protocol so the user can reﬁne/polish the result of image matting iteratively. Such line of
reasoning would lead to the design of image matting on mobile devices in a closed-loop fashion—the key
idea is to bring the user into the loop and supply valuable feedback information to the matting algorithm.
Webelievesuchparadigmofinteractiveimagemattingisofinteresttobothimageprocessingcommunity
and human-computer interaction researchers. Existing iPhone application such as Color Effects appears
to represent a ﬁrst attempt along this direction; however, its object segmentation functionality is far from
being optimized (partially due to complexity constraint). It is expected that image matting with user
feedback can more easily lends itself to mobile tablet applications (e.g., iPad or Galaxy) with larger
touchscreens and more computational resources.
4.08.4.3 Dynamic image mosaicing
Image mosaicing refers to the stitching of several images of the same scene together so one can obtain
a panoramic view [25]. Homography estimation—the process of deriving the geometric transformation
relating one image to another—is at the foundation of all practical mosaicing techniques. A homogra-
phy is essentially a 2D planar projective transform that can be estimated from a given pair of images.
Depending on the nature of scene geometry and camera motion, its corresponding transform matrix
A3×3 could admit 3, 6, and 8 degrees of freedom for rigid, afﬁne and projective transformation respec-
tively. To estimate transform matrix A3×3, it is often desirable to have a sufﬁcient number of cor-
responding feature points from a given pair of images [26]. In the literature of computer vision, the
problem of ﬁnding corresponding feature points has been extensively studied in the past 20 years.
The scale-invariant feature transform (SIFT) [27,28] and its variants (e.g., SURF [29], afﬁne-SIFT
[30]) have been widely used in various applications including image mosaicing.

4.08.4 Image Processing at Fingertips
255
(a)
(c)
(d)
(b)
FIGURE 8.1
Examples of producing anchor regions on touchscreens: (a) sliding thumb and index deﬁnes a rectangular
foreground; (b) tapping with varying pressure deﬁnes foreground (color-coded by yellow) and background
(color-coded by green); (c) ﬁnger sliding generates a rough contour of object; (d) reﬁning the deﬁnition of
F/B by tapping on zoomed regions. (For interpretation of the references to color in this ﬁgure legend, the
reader is referred to the web version of this book.)
However, it should be noted that there is still a signiﬁcant gap between those artiﬁcial algorithms
(the best engineering endeavor) and human vision system (nature’s evolutionary end-result). Therefore,
it would be desirable to exploit the interaction between human and computer to improve the accuracy of
feature point correspondences. A MATLAB tutorial of image mosaicing based on simple user feedback
can be found at http://www.pages.drexel.edu/˜sis26/MosaickingTutorial.htm. Figure 8.2 shows two
possible protocols for users to specify corresponding feature points on the touch-screen. Tapping or
sliding the ﬁngers is arguably more convenient on a mobile device with a relatively large display screen
(i.e., ipad instead of iPhone). However, it is also possible to automatically ﬁnd a pool of potential
matching points (e.g., by SIFT) and ask a user to only control the threshold parameter by sliding his

256
CHAPTER 8 Image Processing at Your Fingertips
(a)
(b)
FIGURE 8.2
(a) User speciﬁes corresponding feature points by tapping on touch-screen of a tablet (the ﬁrst and second
pairs are color-coded differently); (b) an alternative way of specifying corresponding anchor regions by sliding
thumb and index ﬁngers (red rectangles indicate the selection results after sliding). (For interpretation of
the references to color in this ﬁgure legend, the reader is referred to the web version of this book.)
ﬁnger on a virtual display bar; alternatively, a mobile device can display all matching points and ask
the user to select those visually more plausible (e.g., slide through the lines connecting correspondent
SIFT keypoints).
We believe that mobile imaging could redeﬁne image mosaicing by enriching the user’s interactive
experience. For example, it has been proposed that the user can use either the accelerometer of a
smartphone or ﬁnger movement to control the view angle toward the panoramic image (please refer
to Figure 8.3). We can also imagine that functionality of streetview offered by Google Earth can be
implemented on a mobile platform so a user can not only vary the viewpoint but also wander around the
point of interest (including zoom in or out). As demonstrated by the same authors of ztitch, interactive
weaving of video content (called veaver) could deliver a richer set of multimedia experience than still
image-based only (e.g., moving objects across different views could be stitched together and played in
synchronization).
4.08.4.4 Supervised image restoration
Image restoration refers to the recovery of an image from its degraded version. Depending on the degra-
dation model, image restoration includes inpainting, deblurring, denoising, and so on. In the past, image
restoration research has been primarily focusing on ﬁnding good prior models for photographic images
and deriving so-called regularized restoration algorithms. However, in many practical scenarios related
to image restoration such as cultural heritage preservation and personal photo repairing, identiﬁcation of
degraded regions is a difﬁcult task for computer vision. For example, despite the abundance of published
papers on image inpainting (e.g., [31–34]), it is often assumed that the inpainting domain is given as a
priori; most existing works on motion deblurring (e.g., [35–38]) assume a global (spatially-invariant)
blur model and therefore cannot handle the blur caused by object motion.

4.08.4 Image Processing at Fingertips
257
FIGURE 8.3
An interactive image stitching application ztitch developed on Windows Phone 7 by Andrew Au and Jie Liang
at Simon Fraser University. Click the video ﬁles Veaver, Ztitch1 and Ztitch2 to see the demonstrations.
Human-computer interaction can facilitate the task of image restoration in several complementary
manners. First, humans can identify degraded regions and mark them as shown in Figure 8.4. One might
wonder if we should pursue an automatic approach toward this objective; we argue that the deﬁnition
of “problematic regions” is context-dependent and varies from application to application. Therefore, it
is more desirable and efﬁcient to bring humans into the loop than to enumerate all possible degradation
models. Second, humans can supply valuable clues to help machines with the restoration task. As shown
in Figure 8.4b), line constraints along the edges of a building can be conveniently highlighted by a user
and facilitate the task of inpainting [39]; user-speciﬁed region to search for similar patches can also
greatly reduce the computational complexity of various patch-based inpainting techniques.
Second, touch-based control can allow a user to adjust the exposure time while focusing with a
second ﬁnger or selectively focus on the object in the foreground. Although the standard camera on
smartphones does not have the capability of zoom as SLR cameras do, the rapid advances of interpolation
or super-resolution (SR) techniques [40–43] have signiﬁcantly reduced the performance gap between
optical and digital zooming. The stabilizer—insensitivity to the shaking of hands—is also facilitated by
a computational camera. One can either take a sequence of pictures consecutively (so-called burst mode)
or control the shutter speed (e.g., coded-aperture). The accelerometer equipped with a smartphone could
also offer relevant tilting information to a motion deblurring algorithm (e.g., [35]).
Third, the idea of putting restoration-and-recognition in a single loop has received increasingly more
attention in recent year [44,45]. As mentioned before, putting image acquisition and recognition in
a loop has shown promising results in barcode-related applications. It is natural to extend this idea
further by putting restoration into the loop. For example, we can design a speciﬁc SR technique for 1D

258
CHAPTER 8 Image Processing at Your Fingertips
(a)
(b)
(c)
FIGURE 8.4
Examples of interactive image restoration: top—inpainting domain is marked by magenta (line constraints are
marked by R/G/B); bottom—the region suffering from local motion blur can be ﬁnger picked on a touchscreen.
(For interpretation of the references to color in this ﬁgure legend, the reader is referred to the web version
of this book.)
barcodes [46] or particular blind deconvolution algorithm for optical blur that can potentially reduce the
scanning time. The conventional face recognition problem could also beneﬁt from such new ideas of
restoration-and-recognition in a loop—namely, under the assumption that a face recognition algorithm
is chosen, how to exploit the mobility of sensor to maximize its performance? In contrast to software-
based approaches such as [45], mobile imaging offers an attractive hardware-based alternative solution
with little computational burden on the computing device.

4.08.5 Applications
259
4.08.5 Applications
In this section, we discuss a range of potential applications at the intersection of mobile imaging and
computing. The new philosophy of designing novel applications on the mobile platform is twofold.
One is similar to the principle of miniaturization—there is plenty of room to explore at the bottom of
the scale; the other is the principle of mobility—how does this new feature reshape our thinking about
the tradeoff between resource limitation and quality delivery. We hope this section can motivate both
researchers and developers to explore further along the road of image processing at ﬁngertips.
4.08.5.1 Mobile photography
Dozens of applications under the category of photography (e.g., Camera+, 360 Panorama, Camera
Genius, and Slow Shutter Cam) have been developed on the iOS platform. Many functionalities men-
tionedabovesuchasﬁnger-controlledimageacquisition,panoramicimagemosaicing,andmotion-based
artistic effect have been at least partially implemented by those applications. A more fundamental issue
that we attempt to address here is: how will the mobile imaging/computing platform impact the ﬁeld
of photography? It is our conviction that a good vision at the high level often offers useful guiding
principles for developing killer applications.
First, the gap between professionals and amateurs could diminish substantially. Historically, the
ﬁnest art of photography has been mastered by only a small portion of professionals who have access to
expensive equipment and educational program. As the technology advances, the barrier to high-quality
equipment has been removed enabling millions of amateurs to become artistic photographers on a daily
basis. The new generation who grow up with mobile devices such as iPhone and iPad are likely to have
a better intuition about photography than our ancestors. New applications helping amateurs to master
the art by casual playing instead of formal training could reshape the society’s perception about the
profession of photography.
Second, commercial photography will operate on a unprecedented scale but face new challenges
(e.g., copyright and privacy protection). Advertisement, fashion, food, landscape, and wildlife pho-
tography are the examples of old-fashioned commercial uses of photography. As the gap between
professionals and amateurs decreases, a new business model could appear—namely, anyone can get
paid by taking a good picture at the right place and time (e.g., tornados, ﬂoods, etc.). As a concrete
example, little things such as taking a passport photo will not hold any commercial value; while the
operational cost of photojournalism (e.g., National Geographic) could be dramatically reduced. New
apps supporting such paradigm shift is likely to gain momentum.
Third, mobile photography might become an indispensable tool in forensics. Forensic photography is
a technical challenge largely due to the location uncertainty of crime scenes. Video surveillance evidently
addresses this challenge to some extent; but mobile photography offers a valuable supplement. People
already use smartphones to collect evidences for car accidents and vandalism; cameras installed on
trafﬁc lights have the potential of catching red-light beaters or illegal U-turners. Any new apps devoted
to high-quality acquisition of objects of interest (e.g., human face or vehicle license plate) in a timely
manner could be useful to law enforcement.

260
CHAPTER 8 Image Processing at Your Fingertips
4.08.5.2 Computer vision and pattern recognition
Traditionally barcode scanners were designed and adopted by stores to help customers with price-ﬁnding
or self-checkout. The barcode/QR code scanning and recognition functionality provided by smartphones
has made it convenient to compare the prices on the web or self-check in at the airport. A conceivable
useful application along this line is to “integrate” various barcode-based ID cards into a smartphone
device. Many competing grocery stores, game stores, and public library require their customers to
present the membership cards before receiving service or promotion. The apparent disadvantages of
carrying those plastic cards include too-many-to-carry, inevitable physical damage, and easy-to-lose. It
will be highly desirable to store a variety of barcode-based membership information into a single device
such as a smartphone. How to assure that the barcode image stored by a smartphone can be recognized
by conventional devices (e.g., laser-based) is a technical challenge for the future.
Another collection of tools facilitated by mobile imaging are related to optical character recogni-
tion (OCR). Historically many OCR-related techniques have been studied and developed to facilitate
various applications including tax return, law enforcement, digital library, and so on. The mobility of
smartphones enables us to turn information content in the physical world into an image format (e.g.,
printed phone-number or bus schedule, presentation slides at a conference, handwritten recipe at a
friend’s house). If the text information acquired in pictorial format can be converted back to textural, it
will save both space and make it convenient for editing and searching. Some iPhone applications (e.g.,
ImageToText developed by Ricoh) have received favorable reviews due to their ease of usability and
good accuracy of conversion. If someone could develop a similar application converting the image of
line drawings to its original format, that will be another cool application.
Biometrics such as ﬁngerprint or face scanners have been conceived as the desirable tool to protect the
security of smartphones. The hardware of smartphones can easily support the acquisition of ﬁngerprint
and face images. What is still lacking seems to be on the software side—namely, robust and efﬁcient
recognition algorithm that can operate on a light-weight Android or iOS system. How to exploit the
tradeoff between recognition performance and computational complexity to support mobile computing
deserves the attention from both biometrics and embedded system communities. In particular, how to
make the best use of a hybrid approach (similar to barcode recognition) under the context of biometrics
recognition appears to be an under-researched problem.
4.08.5.3 Human network interaction
Social networks have made their way into our lives through facebook, linkedin, twitter, and Google+.
The mobility and communication capability of smartphones enable their users to interact with not only
one machine but a network of smartphone users. What can we foster at the intersection of social networks
and touch computing? We will sketch several proof-of-concept apps that can be viewed as novel ways
of human interaction enabled by the network:
1. Mobile veriﬁcation: Some smartphones are already equipped with the capability of scanning the
ﬁngerprint by a swipe sensor and using it as a biometrics to verify the user’s identity; other modalities
such as face and voice are also potential candidates for biometric identity. It is our biased opinion
that smartphones offer an attractive platform for testing biometrics. Just as 2D bar-codes are getting
more popular, mobile veriﬁcation of humans could bring commercial values.

4.08.6 Open Issues and Problems
261
2. Touch-to-know: Imagine at a conference, instead of staring at the name tag, you can use your
smartphone to ﬁnd out who is this guy in front of the poster. Take a picture of him/her; touch the
picture and the smartphone will be smart enough to connect you through linkedin. The idea is to
help people make more friends in a mobile way.
3. Who-is-nearby: GPS-based driving assistant system already has some capability of sharing trafﬁc
information among nearby drivers. Similarly, you might want to know whether your old friends are
nearby at a conference or track where they are. Instead of supplying the coordinate of some place
to GPS, you can command your smartphone to take you to where your friend is.
Another important class of applications enabled by human network interaction is education. As
the internet keeps growing, it has become an important educational resource accessible to everyone.
To ﬁnd out the answers, people can search on the web instead of in the library. Therefore, the design
of interactive educational software could follow a similar pattern to that of computer games—i.e., from
conventional platforms (e.g., PC, consoles) to mobile ones. Plenty of opportunities exist there—e.g.,
when a kid spots a plant or animal and curious about its name, he/she can take a picture and look it up
on the web; or a child can learn a second language by touching the word in his/her native language;
or any person can look up a word in a dictionary or wikipedia by simply tapping on it with varying
pressure. As the visualization/display technology evolves, one can imagine in the long run that people
can interact with the cyber-world by dancing their ﬁngers not on the touchscreen but in the air—as in
the movie “Minority Report.”
4.08.6 Open issues and problems
Mobility has played a fundamental role in the evolution of human species. Therefore, it is not surprising
to observe that mobile imaging and mobile operating systems (OS) represent the new trend of sensing
and computing in the next decade. We believe that this new trend will have dramatic impact on the ﬁeld
of image processing—from keyboard/mouse on desktop to ﬁngertips on tablets/smartphones. As the
pioneer of cybernetics Norbert Wiener says, “the most fruitful areas for the growth of sciences were
those which had been neglected as a no-man’s land between the various established ﬁelds.” It is likely
that image processing at ﬁngertips will occupy no-man’s land at the intersection of microelectronics,
embedded systems, signal processing, computer vision, human-computer interaction, social networking,
and software engineering. This article has only scratched the surface of this emerging area; many issues
are still open and technical problems remain to be attacked. In this last section, we will supply a list of
open issues and problems for future study.
•
The impact of miniaturization: Many achievements of mobile devices would not have been possible
without the advance in the art of miniaturization. Making things smallers is well aligned with the
principle of mobility, which supports the integration at the system level. How do we make smartphone
even smarter? It has already integrated phone, GPS, camera, LCD display into one device. What if
it can become a more versatile device, so it can be also used in ﬁnancial transactions (i.e., integrate
smartcards into smartphones), personal identiﬁcation (e.g., integrate your driver’s license to your
phone), access control (so real estate agents do not need the password to retrieve keys), and cosmetic
accessories (e.g., ladies do not need to carry a mirror for make-up because their smartphones can
provide cosmetic parameter information)? In particular, if our phone can become our third eye—the

262
CHAPTER Image Processing at Your Fingertips
one that helps us see far, see small, see in the dark or even see through our bodies, such a device
could become an indispensable component of our daily lives.
•
Integratesensingandprocessing:Historically,imageacquisitionandprocessinghaveevolvedalmost
independently (one tied to hardware design and the other belongs to software development). For
the ﬁrst, mobile devices such as smartphones and tablets provide a platform supporting joint opti-
mization of sensing and processing. The popularity of various barcode and QR code scanners is
only representative of this remarkable trend. Many other image processing and analysis tasks could
beneﬁt from our advocated closed-loop design philosophy—it is not like fancy mathematics such as
compressed sensing but more like practical engineering such as quality control. However, we argue
its intellectual merit lies in being experimentally reproducible because the mobile device has to work
with real-world image data and adapt to them through physical motion. Even though this is only the
ﬁrst step towards understanding the fundamental role of mobility in the evolution of intelligence, we
argue it marks a signiﬁcant advantage of advocating experimentally reproducible research.
•
Support research via development: Conventional wisdom emphasizes that development comes after
research because the latter is believed to be more fundamental. Unfortunately, the boundary between
basic and applied research has become more and more vague; moreover, basic research is often
thought of as a luxury under a stressed economical environment. Apple’s miraculous success in
business might suggest an alternative path—starting from real-world applications and logically rea-
son backwardly. What kind of tools do we need or need to invent to solve a particular problem? If Karl
Popper’sfalsiﬁcationofscientiﬁctheoryiscorrect,thenexperiments(orexperimentally-reproducible
research) could also suggest a fruitful approach towards creating new theories. Nevertheless, without
our ancestors’ diligent observation data, it would have been much more difﬁcult for even a genius
like Issac Newton to establish his theory of classical mechanics. Maybe we—as engineers—can be
more proud of our profession if we not only improve life qualities through inventions but perceive
it as an experimental approach toward understanding how nature works.
A. Appendix: course material, source codes and datasets
In this Appendix, we provide a list of links to various online resources related to image processing on
ﬁngertips. Since this is an emerging ﬁeld, we expect that more resources will become available as the
technology evolves.
•
Teaching material collection: At this point, the course EE368 offered by Prof. Bernd Girod at Stan-
ford seems to be the only one covering image processing on smartphones. Some useful background
material related to Android OS can be found at http://www.stanford.edu/class/ee368/Android/. A
list of course projects in previous years can be accessed from
http://www.stanford.edu/class/ee368/Project_10/index.html.
The well-known open-source project developed by vision community (OpenCV) is still migrating
to the Android platform http://opencv.willowgarage.com/wiki/Android.
•
Source code collection: A list of links to reproducible research in image processing, computer
vision/graphics, and machine learning can be found at http://www.csee.wvu.edu/˜xinl/source.html.
Additionally, the authors of ztitch software (Andrew Au and Jie Liang) have kindly made their source
codes available online http://www.ztitch.com/source.html. Anther related open-source software
developed for Windows Phone can be found at http://picfx.codeplex.com/.

References
263
•
Image dataset collection: A collection of links to various video/image databases on the web can be
found at http://www.csee.wvu.edu/˜xinl/database.html. The most well-known application for shar-
ing photos acquired by smartphones is likely to be Instagram http://en.wikipedia.org/wiki/Instagram.
Online photo website ﬂickr has created user groups with different iOS—e.g., http://www.ﬂickr.
com/groups/iphone/ and http://www.ﬂickr.com/groups/android/.
References
[1] R. Feynman, There’s plenty of room at the bottom, Eng. Sci. 23 (5) (1960) 22–36.
[2] D. Litwiller, CCD vs. CMOS: facts and ﬁction, Photon. Spectra 1 (2001) 154–158.
[3] E. Fossum, CMOS image sensors: electronic camera-on-a-chip, IEEE Trans. Electron Dev. 44 (10) (1997)
1689–1698.
[4] M. Madou, Fundamentals of Microfabrication: The Science of Miniaturization, CRC, 2002.
[5] K. Jain, S. Rice, B. Lin, Ultrafast deep UV lithography using excimer lasers, Polym. Eng. Sci. 23 (18) (1983)
1019–1021.
[6] B. Carlson, Comparison of modern CCD and CMOS image sensor technologies and systems for low resolution
imaging, in: Sensors, Proceeding of IEEE, vol. 1, IEEE, 2002, pp. 171–176.
[7] A. Moini, Image sensor architectures, Smart Cameras, Springer, 2010, pp. 81–96.
[8] R. Ng, M. Levoy, M. Bredif, G. Duval, M. Horowitz, P. Hanrahan, Light ﬁeld photography with a hand-held
plenoptic camera, Computer Science Technical Report CSTR, vol. 2, 2005.
[9] H. Benko, A. Wilson, P. Baudisch, Precise selection techniques for multi-touch screens, in: Proceedings of
the SIGCHI Conference on Human Factors in Computing Systems, 2006, pp. 1263–1272.
[10] F. Beck, B. Stumpe, European Organization for Nuclear Research, Two devices for operator interaction in the
central control of the new CERN accelerator, CERN, 1973.
[11] N. Mehta, A ﬂexible machine interface, Master’s Thesis, University of Toronto, Department of Electrical
Engineering, 1982.
[12] L. Nakatani, J. Rohrlich, Soft machines: a philosophy of user-computer interface design, in: Proceedings of
the SIGCHI Conference on Human Factors in Computing Systems, ACM, 1983, pp. 19–23.
[13] P. Wellner, The digitaldesk calculator: tangible manipulation on a desk top display, in: Proceedings of the
Fourth Annual ACM Symposium on User Interface Software and Technology, 1991, pp. 27–33.
[14] P. Dietz, D. Leigh, Diamondtouch: a multi-user touch technology, in: Proceedings of the 14th Annual ACM
Symposium on User Interface Software and Technology, 2001, pp. 219–226.
[15] J. Han, Low-cost multi-touch sensing through frustrated total internal reﬂection, in: Proceedings of the 18th
Annual ACM Symposium on User Interface Software and Technology, 2005, pp. 115–118.
[16] R. Raskar, J. Tumblin, Computational Photography: Mastering New Techniques for Lenses, Lighting, and
Sensors, AK Peters, Ltd., Natick, MA, USA, 2009.
[17] R. Fielding, The Technique of Special Effects Cinematography, Focal Press, 1985.
[18] T. Porter, T. Duff, Compositing digital images, ACM SIGGRAPH Comput. Graph. 18 (3) (1984) 253–259.
[19] Y.-Y. Chuang, B. Curless, D.H. Salesin, R. Szeliski, A Bayesian approach to digital matting, in: IEEE
Computer Society Conference on Computer Vision and Pattern Recognition, vol. 2, 2001, pp. 264–271.
<http://grail.cs.washington.edu/projects/digital-matting/image-matting/>.
[20] J. Sun, J. Jia, C. Tang, H. Shum, Poisson matting, in: ACM SIGGRAPH, 2004, pp. 315–321. <http://www.cse.
cuhk.edu.hk/˜leojia/all_project_webpages/Poisson˜matting/poisson_matting.html>.
[21] A. Levin, A. Rav-Acha, D. Lischinski, Spectral matting, in: IEEE Conference on Computer Vision and Pattern
Recognition, CVPR ’07, June 2007, pp. 1–8. <http://www.vision.huji.ac.il/SpectralMatting/>.

264
CHAPTER Image Processing at Your Fingertips
[22] A. Levin, D. Lischinski, Y, Weiss, A closed-form solution to natural image matting, IEEE Trans. Pattern Anal.
Mach. Intell. (2007) 228–242.
[23] C. Rother, V. Kolmogorov, A. Blake, Grabcut: interactive foreground extraction using iterated graph cuts,
ACM Trans. Graph. (TOG) 23 (2004) 309–314.
[24] Y. Li, J. Sun, C. Tang, H. Shum, Lazy snapping, ACM Trans. Graph. (TOG) 23 (2004) 303–308.
[25] R. Szeliski, Image mosaicing for tele-reality applications, IEEE Comput. Graph. Appl. 16 (2) (1996) 22–30.
[26] R. Hartley, A. Zisserman, Multiple View Geometry in Computer Vision, second ed., Cambridge University
Press, 2003.
[27] D. Lowe, Object recognition from local scale-invariant features, in: Proceedings of IEEE International Con-
ference on Computer Vision, 1999.
[28] D.G. Lowe, Distinctive image features from scale-invariant keypoints, Int. J. Comput. Vis. 60 (2004) 91–110.
[29] H. Bay, T. Tuytelaars, L. Van Gool, Surf: speeded up robust features, in: ECCV 2006, 2006, pp. 404–417.
[30] J. Morel, G. Yu, ASIFT: a new framework for fully afﬁne invariant image comparison, SIAM J. Imag. Sci. 2
(2) (2009) 438–469.
[31] M. Bertalmio, G. Sapiro, V. Caselles, C. Ballester, Image inpainting, in: Proceedings of SIGGRAPH,
New Orleans, LA, 2000, pp. 417–424.
[32] A. Criminisi, P. Perez, K. Toyama, Region ﬁlling and object removal by exemplar-based image inpainting,
IEEE Trans. Image Process. 13 (2004) 1200–1212.
[33] J. Sun, L. Yuan, J. Jia, H.-Y. Shum, Image completion with structure propagation, in: ACM SIGGRAPH,
2005.
[34] I. Drori, D. Cohen-Or, H. Yeshurun, Fragment-based image completion, in: ACM SIGGRAPH 2003, 2003,
pp. 303–312.
[35] R. Fergus, B. Singh, A. Hertzmann, S.T. Roweis, W.T. Freeman, Removing camera shake from a single
photograph, ACM Trans. Graph. 25 (3) (2006) 787–794.
[36] Q. Shan, J. Jia, A. Agarwala, High-quality motion deblurring from a single image, in: ACM SIGGRAPH 2008
Papers, 2008, pp. 1–10.
[37] S. Cho, S. Lee, Fast motion deblurring, ACM Trans. Graph. (TOG) 28 (2009) 145.
[38] O. Whyte, J. Sivic, A. Zisserman, J, Ponce, Non-uniform deblurring for shaken images, in: IEEE Computer
Society Conference on Computer Vision and Pattern Recognition, 2010, pp. 491–498.
[39] C. Barnes, E. Shechtman, A. Finkelstein, D. Goldman, Patchmatch: a randomized correspondence algorithm
for structural image editing, ACM Trans. Graph. (TOG) 28 (2009) 24. <http://gfx.cs.princeton.edu/pubs/
Barnes_2009_PAR/>.
[40] X. Li, M. Orchard, New edge directed interpolation, IEEE Trans. Image Process. 10 (2001) 1521–1527.
[41] W.T. Freeman, T.R. Jones, E.C. Pasztor, Example-based super-resolution, IEEE Comput. Graph. Appl. 22
(2002) 56–65.
[42] D. Glasner, S. Bagon, M. Irani, Super-resolution from a single image, in: ICCV, 2009. <http://www.wisdom.
weizmann.ac.il/˜vision/SingleImageSR.html>.
[43] G. Freedman, R. Fattal, Image and video upscaling from local self-examples, ACM Trans. Graph. (TOG) 30
(2) (2011) 12. <http://www.cs.huji.ac.il/˜raananf/projects/lss_upscale/>.
[44] M. Gupta, S. Rajaram, N. Petrovic, T. Huang, Restoration and recognition in a loop, in: IEEE Computer
Society Conference on Computer Vision and Pattern Recognition, 2005, pp. 638–644.
[45] Y.Z. Haichao Zhang, Jianchao Yang, T. Huang, Lose the loop: joint blind image restoration and recognition
with sparse representation prior, in: IEEE Conference on Computer Vision, 2011.
[46] F. Champagnat, C. Kulcsar, Le G. Besnerais, Continuous super-resolution for recovery of 1-D image features:
algorithm and performance modeling, in: IEEE Computer Society Conference on Computer Vision and Pattern
Recognition, vol.1, 2006, pp. 916–926.

9
CHAPTER
Image Analysis and Recognition
Anuj Srivastava
Department of Statistics, Florida State University, Tallahassee, FL, USA
4.09.1 General background
The problem of image analysis and understanding has gained high prominence over the last decade, and
has emerged at forefront of signal and image processing research. There are multiple reasons for such a
rapid growth in image analysis research. Firstly, images have become one of the largest sources of data
in our digital society. From medical diagnostics and trafﬁc monitoring to underwater exploration and
homeland security, images provide the prime source of information in a comprehensive and automated
fashion. The progress in design and manufacturing of high-resolution imaging devices, high-capacity
storage medium, and high-speed transmission of digital data has enabled the use of image analysis in
nearly every aspect of human life. Even modalities that naturally belong to signal processing, often
end up producing data in form of images for human visualization. Examples include synthetic aperture
radar (SAR) and sonar data (SAS) where one-dimensional signals for different imaging parameters are
collated to form higher-dimensional images. Secondly, the desire for automating systems that study vast
amounts of image data have led scientists to ask more ambitious questions. Instead of restricting them-
selves to the traditional problems of image denoising, compression, and transmission, the researchers
have started asking more comprehensive questions: What are the contents of images? Which objects are
present in an image and, furthermore, what are their actions and intents? Such questions require more
ambitious mathematical representations of objects contained in images, more sophisticated statistical
models to capture modes of variability in such object data and efﬁcient computer algorithms to gen-
erate statistical inferences for such image data. In the following, we provide an overview of emerging
techniques in the area of image analysis and computer vision, that are feeding a rapid rise in image
analysis.
Fundamentally speaking an image is created by recording the light spectrum reﬂected from a region
(containing objects) visually accessible to the camera. There are many sources of variability in the
recorded image. The variability in image pixels can be due to differences in objects being imaged, view-
ing angles, imaging environments, backgrounds, camera properties, and so on. Consequently, images
of all types—of natural or artiﬁcial objects, indoor or outdoor images, static or video images, etc.—
contain a large variability in observed pixels values. Even images of the same objects taken in similar
imaging environments can exhibit a decent amount of variability. For anyone designing automated meth-
ods for image analysis, a challenging part is to quantify and model the statistical variability in given
data. Another major challenge in developing image analysis techniques is the sheer size of data. Even
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00009-1
© 2014 Elsevier Ltd. All rights reserved.
267

268
CHAPTER 9 Image Analysis and Recognition
ordinary cameras observing simple scenes can generate large volumes of data in short time. The prob-
lems of search, annotation, and classiﬁcation of videos on Youtube are examples of challenges where the
size of image data becomes a problem. Images form an integral part of the landscape of big data that is
attracting new research initiatives. It, thus, becomes imperative to ﬁnd low-dimensional representations
of images to enable statistical analysis. The research efforts in the joint problems of dimension reduction
and statistical modeling of pixels (and eventually generating inferences under these dimension-reduced
statistical models) have historically proceeded in several ways.
One direction is to use pixel locations in images as nodes in a graph and to model the interaction
between pixels values using edges. A prominent example of this idea is a Markov Random Field
(MRF) model where the neighboring pixels form cliques that completely describe conditional and joint
distributions of pixel values [1–3]. Typically, the vertical and horizontal neighbors of a pixel are included
in a clique, but one can generalize this to more complicated neighborhood structures. Such structures
simplify the task of generating statistical inferences by breaking the problem of joint estimation of large
number of pixels in sequential updates of smaller numbers of pixel, sometimes even one pixel, at a
time. Driven by advances in Monte-Carlo methods [4], MRFs have found many applications in image
denoising, segmentation, and related applications [5].
Another area of active research has been to study empirical distributions of pixels in natural and man-
made images, and develop models motivated by these observed statistics. Extensive empirical studies
of natural images point to highly non-Gaussian behavior. For instance, the distributions of coefﬁcient
of images under commonly-used wavelet basis functions were found to have heavy tails, sharp cusps at
zero, and invariance of statistics to scales. Several parametric models, supporting heavy-tailed statistics,
have been used in modeling image variability and have been shown to improve performance in image-
based inferences [6]. Some of these characteristics have also found basis in neuroscientiﬁc analysis of
human vision [7,8]. This has motivated development of non-Gaussian models that can handle image
data and explain such observed behavior. A related topic area is to develop generative statistical models
for studying image analysis. The central idea here is to write down explicit statistical models for
generating images as random realizations. Examples include models for image patches based on sparse
basis learned from training data and exponential probability models based on histograms of ﬁltered
images [5].
A substantial effort has also been spent on low-dimensional representations of images, for use in
analysis and inferences. Such efforts are motivated by the fact that the fundamental sources of variability
in certain subsets of images are often low dimensional, e.g. pose variability in image rigid objects, or
changes in facial expressions in face images, and so on. Earlier efforts relied on linear projections,
such as principal component analysis or Fisher’s discriminant analysis, but researchers recognized
the need for nonlinear dimension reduction methods. Motivated in part by advances in techniques for
manifoldlearning,avarietyofalgorithmshavebeendevelopedtolearnthegeometryoflow-dimensional
substructures on which the images of interest lie [9–11]. One can utilize these estimated manifolds for
classiﬁcation and analysis of future images.
Another general approach that uses manifold idea, but perhaps more comprehensively, is based
on the recognition that the certain image “features” of interest are not elements of vector spaces but
rather nonlinear manifolds. Examples of such features include shapes, subspaces, histograms, and so
on. For instance, the analysis of shapes objects present in images is important for detection, tracking,
and classiﬁcation of objects in a variety of applications. Since shapes are deﬁned as features that

4.09.2 Chapter Introductions
269
are invariant to similarity transformations, one cannot use standard Euclidean calculus and multivariate
statistics for analyzing them [12–14]. The same goes for normalized histograms that represent estimates
of probability density functions governing the underlying variability. The basic idea here is to utilize
the geometry of these known manifolds, choose proper Riemannian metrics for analysis, and develop
tools for statistical analysis of these feature spaces. Such differential geometric methods have been used
successfully in many applications involving image data.
In some contexts, especially those involving medical image analysis, an interesting and powerful
idea for capturing image variability is the deformable template approach [15,16]. Here one constructs a
representative or a template for a class of images so that the other elements of this class can be obtained
by deforming this template. This idea is useful in situations where it is easier to represent/analyze
the deformation of an image, rather than the image itself. The deformations are modeled as smooth
(diffeomorphism)mapsofanimagedomaintoitself.Theproblemofﬁndingadeformationthatoptimally
matches a template to a given image is called registration and is amongst the most common problem
studied in medical image analysis. The choice of energy functions or metrics for registration and template
estimation becomes important, and is often decided based on the application context.
4.09.2 Chapter introductions
There are three chapters in this section that address the problems of object detection, image modeling
and pattern analysis. A brief description of these chapters follows.
The ﬁrst chapter by Barbu focuses on the problem of object detection, using the ideas from a recently
developed technique called marginal space learning. The interest aspect of this method is that in high-
dimensional spaces, where it is difﬁcult to deﬁne and estimate probability densities, one can restrict
analysis to some select dimensions using marginal densities and increase dimensions slowly during
inference. The speciﬁc method studied here uses a particle ﬁlter in a chain of subspaces of increasing
dimensions, using trained detectors to prune the particles in the subspaces. While the previous use of
this technique is in medical image analysis, the current chapter considers general object detection such
as face detection in 2D images.
The second chapter by Descombes highlights the role of Markov models and Bayesian inferences
in image analysis. The high dimensionality of data in image analysis precludes a broad family of
joint probability models on observation spaces and Markov models have taken a central stage in this
modeling process. These models are usually expressed through the Bayesian formulation and can thus
be divided into a prior and a likelihood. Markov models are particularly interesting as they provide
a global formulation through local energy potentials. In this chapter, the authors introduce the two
main family of Markov models, that are the discrete Markov Random Fields and the continuous spatial
processes.
The third chapter by Filipovych et al. discusses the joint pattern recognition of multiple pixels
using techniques from machine learning. Using the problem of medical diagnosis using neuroimaging
data, they present the use of machine learning methods, such as the support vector machines and
other classiﬁcation methods, in a variety of diagnostic applications, such as aging, schizophrenia, and
autism.

270
CHAPTER 9 Image Analysis and Recognition
Together these chapters provide a brief snapshot of different techniques that are currently used for
statistical data analysis and inferences in dealing with image data.
References
[1] R. Chellappa, A. Jain, Markov Random Fields: Theory and Applications, Academic Press, 2003.
[2] S.Z. Li, Markov Random Field Modeling in Image Analysis, second ed., Springer-Verlag, Tokyo, Japan, 2001.
[3] G. Winkler, Image Analysis, Random Fields and Dynamic Monte Carlo Methods, second ed., Springer-Verlag,
2003.
[4] C.P. Robert, G. Casella, Monte Carlo Statistical Methods, Springer-Verlag, New York, NY, 1999.
[5] S.C. Zhu, Y.N. Wu, D.B. Mumford, Frame: ﬁlters, random ﬁelds, and maximum entropy—towards a uniﬁed
theory for texture modeling, Int. J. Comput. Vis. 27 (2) (1998) 1–20.
[6] A. Srivastava, A.B. Lee, E.P. Simoncelli, S.-C. Zhu, On advances in statistical modeling of natural images,
J. Math. Imaging Vis. 18 (2003) 17–33.
[7] D.J. Field, Relations between the statistics of natural images and the response properties of cortical cells,
J. Opt. Soc. Am. 4 (12) (1987) 2379–2394.
[8] B.A. Olshausen, D.J. Field, Natural image statistics and efﬁcient coding, Network Comput. Neural Syst. 7
(1996).
[9] M. Belkin, P. Niyogi, Laplacian eigenmaps for dimensionality reduction and data representation, Neural
Comput. 15 (6) (2003) 1373–1396.
[10] R. Pless, R. Souvenir, A survey of manifold learning for images, IPSJ Trans. Comput. Vis. Appl. 1 (2009)
83–94.
[11] S. Roweis, L. Saul, Nonlinear dimensionality reduction by locally linear embedding, Science 290 (5500)
(2000) 2323–2326.
[12] I.L. Dryden, K.V. Mardia, Statistical Shape Analysis, John Wiley & Son, 1998.
[13] A. Srivastava, E. Klassen, S.H. Joshi, I.H. Jermyn, Shape analysis of elastic curves in euclidean spaces, IEEE
Trans. Pattern Anal. Mach. Intell. 33 (7) (2011) 1415–1428.
[14] L. Younes, P.W. Michor, J. Shah, D. Mumford, R. Lincei, A metric on shape space with explicit geodesics,
Mat. Appl. 19 (1) (2008) 25–57.
[15] U. Grenander, General Pattern Theory, Oxford University Press, 1993.
[16] U. Grenander, M.I. Miller, Computational anatomy: an emerging discipline, Quart. Appl. Math. LVI (4) (1998)
617–694.

10
CHAPTER
Multi-Path Marginal Space
Learning for Object Detection
Adrian Barbu
Department of Statistics, Florida State University, Tallahassee, FL, USA
4.10.1 Introduction
One of the main computational challenges in object detection is dealing with the size and position vari-
ability of the objects in real-world images. There are even more challenges, however, since the objects
also exhibit different rotations, and other parameters (out of plane rotation, illumination, internal param-
eters such as limb locations for pedestrians, etc). Previous works deal with this curse of dimensionality
in different ways, which can be grouped in two main approaches.
The ﬁrst main approach is to use features that are invariant to the object parameters [1–3], such as
rotation/scale invariant features [4] or illumination invariant features [5,6]. With this approach, some
of the object parameters are ignored and a computational gain is obtained. The main challenge with this
approach is to ﬁnd features that are invariant yet discriminative. In some cases [4], these features are
more computationally expensive than the simple and non-invariant ones, such as the Haar features [7],
that are used in the second main approach.
The second main approach is to exhaustively search the object parameters using fast classiﬁers based
on simple features [8–12]. Different approaches are used for speeding up the detection [7] and for
obtaining more discriminative features [13,14]. For computational efﬁciency, most exhaustive search
approaches are based on a cascade of increasingly complex classiﬁers, and still make a compromise by
ignoring some of the object parameters, e.g., rotation.
Recently, a new computational approach named Marginal Space Learning (MSL) permits the use
of an arbitrarily large number of parameters in the object of interest. The method was applied to many
medical imaging problems [15–21] where the objects to be detected had between 9 and 165 parameters.
The method involves training classiﬁers for a sequence of increasingly larger subspaces, such that the
relative dimension between two consecutive spaces is small. These subspaces are Marginal Spaces,
since some of the parameters of the ﬁnal classiﬁer have been ignored (marginalized). In [21], all nine
parameters were needed to align a PCA shape model of a heart chamber for object segmentation.
The contributions of this chapter are the following:
1. It makes a connection through Marginal Space Learning between the object detection approach based
on invariant features and the approach based on simple and non-invariant features with exhaustive
search.
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00010-8
© 2014 Elsevier Ltd. All rights reserved.
271

272
CHAPTER 10 Multi-Path Marginal Space Learning for Object Detection
2. It introduces multiple computational paths in Marginal Space Learning, in which detections obtained
through different MSL paths are aggregated into the ﬁnal detection result. This new method offers
improved performance over using a single MSL path.
3. It presents an application of the proposed multi-path MSL approach to four parameter face detection
from grayscale images.
4. It observes experimentally that MSL and multi-path MSL have more compact classiﬁers than reg-
ular object detectors of comparable accuracy. Consequently, MSL and multi-path MSL need fewer
manually annotated examples than other object detection algorithms (e.g., 1494 instead of 4916 in
[7]) to obtain similar or even better generalization performance.
This work does not aim to go beyond the state of the art in face detection. Instead, it shows a
new approach to object detection using Marginal Space Learning that can handle a large number of
parameters, obtains a more compact model and needs fewer training examples. These advantages make
the approach applicable to many object detection tasks.
4.10.2 Related work
The fast detection in a Marginal Space can be considered one form of selective attention [22]. Further-
more, the three levels of computation presented in [22]—edge fragments, local and global groupings—
represent particles in different marginal spaces, when the object of interest is modeled using edges and
their spatial relationships. Similarly, in our previous work on Hierarchical Learning of Curves [15],
Marginal Space Learning was used to detect ﬂexible curves in X-ray images using ridge fragments,
short curves and long curves as marginal spaces.
Marginal Space Learning is related to the Highest Conﬁdence First (HCF) algorithm [23] in that
it propagates the most promising partial solutions. However, HCF is greedy while MSL propagates a
number of partial solutions, so it can avoid many local optima. Moreover, learning the marginal space
models ensures that there will usually in the end be solutions close to the true optimum.
Learning in marginal spaces has been used for learning-based edge detection [24], by learning the
probability of a pixel to be on an edge while ignoring the edge direction. However, that was the ﬁnal
goal in [24] and the process was not continued by increasing number of parameters to obtain a more
accurate edge model.
Skin detection [25,26] is used as a ﬁrst step in Color Face Detection [27]. Skin detection can be seen
as a detection step in a marginal space where all other face parameters are ignored except for the skin
position, obtained purely based on skin color.
More generally, Marginal Space Learning is related to part based object detection [28] and Pictorial
Structures [29,30], since object parts are detected in smaller dimensional Marginal Spaces of the full
object parameter space. There are many differences though. In these works, the full object model is con-
structed from the part models purely based on the geometric conﬁguration of the parts whereas in MSL
the full object model is learned using both spatial and appearance features. Moreover, in MSL many inter-
mediate subspaces could be used between the part models and the full-object model, which could further
speed-up the algorithm. One advantage of the Pictorial Structures is that they are robust to the occlusion
of any of the parts, as long as sufﬁciently many other parts are present, while MSL relies too heavily on
one of the parts. This issue is addressed by the multi-path MSL introduced in this work in Section 4.10.5.

4.10.2 Related Work
273
The And-Or graph representation of the object by parts [31] has α and β inference processes that
detect the objects directly (α process) or predict the object from a detected part (β process). These
inference processes can be considered different MSL computational paths. While the And-Or graph is
focused on a part-based representation, MSL can use other computational paths that are not based on
parts. For example in our heart segmentation work [32] one of the marginal spaces for detecting the
Left Ventricle (LV) was the position of the LV center, which is not a part, but a simpler representation
of the object in which the orientation and scale are ignored.
Many face detection papers [7,11,33,34] ignore some of the face parameters, training a classiﬁer that
is invariant to the ignored parameters. The same holds true for many object detection papers [13,35,36].
Because of the high computational burden associated with inferring shape, 3D position, deformation,
and illumination, most authors disregard the large parameter spaces that would offer a more accurate
characterization of the detected object. However, the beneﬁts in terms of accuracy of the object model
are multiple. For example, in [37] an accurate face model with more than 200 parameters is obtained in
about 50 min using variational techniques.
Otherauthorsdescribeafacedetectionmethodinwhichanumberofmoreandmorespecializeddetec-
tors are learned in a tree structure [38]. Speciﬁcally, a generic face detector is trained at the root of the tree
in the marginal space of all face rotations. In subsequent layers of the tree, more and more accurate detec-
torsareemployedtoprunethesearchspace.ThisworkcanbeviewedasanearlyMSLprecursor,however
the authors did not present their approach as a general learning-based optimization methodology.
Marginal Space Learning is different from a detector cascade [7,34]. In the detector cascade, all
detectors work in the same parameter space, whereas in Marginal Space Learning the models (detectors)
work on spaces of increasingly larger dimensionality. Marginal Space Learning draws its power from
this increasing dimensionality, since rejecting one location in a marginal space virtually eliminates
thousands or even millions of locations from searching in the full parameter space.
The Soft Cascade [33] could be used to train each marginal classiﬁer instead of the regular cascade
or the Probabilistic Boosting Tree [39], methods currently used in our work. This way, each marginal
classiﬁer can be further tuned to balance speed and accuracy.
Recent work on Recursive Compositional Models (RCM) [40–43] is related to Marginal Space
Learning, since partial object models are obtained in a sequence of subspaces of increasing dimension,
and these models are used to efﬁciently propagate a set of particles. However, Marginal Space Learning is
a general training and optimization methodology that can be used in many applications (object detection,
segmentation, inference) and is not tied to a particular model.
Marginal Space Learning is similar to a degenerate decision tree [44], where each node is a boosted
classiﬁertrainedinamarginalspace.ThemajordifferenceisthatMSLobtainsafractionalnumberoffea-
ture evaluations per location, (e.g., on the order of 10−5 for Left Ventricle detection in CT), whereas in the
decision tree, the number of feature evaluations per location is at least 1 (the ﬁrst node of the tree). More-
over, the MSL classiﬁers are tuned based on the ROC curve to certain detection rates that obtain a desired
trade-off between the overall detection speed and accuracy. Because of the simplicity of the MSL chain,
this tuning can be performed jointly on all the classiﬁers, which is impractical in a generic decision tree.
As already mentioned, the MSL optimization can be performed stochastically using sequential Monte
Carlo on the marginal spaces [45]. However, in [45] the authors use a generic approach without learn-
ing the marginal probabilities, which is not practical for most computer vision or medical imaging
applications.

274
CHAPTER 10 Multi-Path Marginal Space Learning for Object Detection
4.10.3 Marginal Space Learning overview
Given a trained classiﬁer p(x|I), x ∈, the object detection problem is to ﬁnd the object parameters
x in the image I
ˆx = argmax
x∈
p(x|I).
If the parameter space  is high dimensional (e.g., 9D in the case of ﬁnding the Left Ventricle from a
CT image), it is computationally expensive or even prohibitive to search the entire space , even using
a coarse-to-ﬁne approach.
Marginal Space Learning addresses this optimization problem through learning. It is a simple and
intuitive idea that can be regarded as a particle ﬁlter in a sequence 1 ⊂2 ⊂· · · ⊂n =  of
increasingly larger subspaces, with the last space  being the parameter space of the object that needs
to be detected. In each subspace i ⊂ some of the object parameters are ignored (marginalized) and
a trained classiﬁer is used to prune the particles. The particles are then propagated to the next space of
the sequence by adding more parameters with all the possible values on a grid, and again pruning them
with the associated classiﬁer, as illustrated in Figure 10.1.
The procedure is repeated until the set of particles reaches the full space  of object parameters. The
particles can be pruned in a deterministic fashion by always keeping the most promising particles, or
stochastically by sampling them according to their probabilities given by the classiﬁer.
The ﬁrst or ﬁrst few marginal spaces can be considered as following the invariant-feature based
approach, since they ignore many of the object’s parameters. Example of ﬁrst marginal spaces could be
the space of decisions whether the object is present or not in the scene, as in [35], or the space of the
object’s position only [8].
The next marginal classiﬁers add more and more parameters to the object, and use features that are
less invariant and usually faster and more discriminative. Directly using a classiﬁer in large dimensional
space can computationally expensive or even prohibitive. However, by using the marginal classiﬁers to
focus the object detectors to a small number of locations, a large number of parameters can be easily
handled without a computational burden.
FIGURE 10.1
Marginal Space Learning propagates a set of particles in a sequence of increasingly larger subspaces of
object parameters. In each subspace k+1, the particles from k are extended by adding more parameters
and a trained classiﬁer is used to prune these extended particles.

4.10.4 Face Detection with Marginal Space Learning
275
The last classiﬁer is based on all the parameters that are relevant to the problem at hand (scale, rotation,
illumination, etc.) and gives the ﬁnal detection result. This last level could have complex and accurate
models (e.g., generative models such as [46,47]) in the last classiﬁer as a ﬁnal validation step to further
improve the accuracy, with minimal computational loss. For example, this last veriﬁcation step for 3D
lymph node detection [16,48] is performed in the 165 dimensional space of lymph node segmentations.
Marginal Space Learning presents some advantages and disadvantages:
•
If the marginal classiﬁers can be trained well, speed-up by many orders of magnitude (six order of
magnitude speedup in [21]) can be obtained with virtually no loss in accuracy.
•
It is easy to control the speed/accuracy trade-off through the number of particles that are propagated.
A larger number of particles means an increased detection rate for a larger computational expense.
•
The total size of the classiﬁers is smaller than in a cascaded approach, because each classiﬁer is
trained on a more representative sample set. Consequently, the MSL approach usually needs fewer
training examples for the same generalization power.
•
Training the marginal classiﬁers, especially the ﬁrst one, require invariant features that are discrim-
inative. This is the same challenge faced by the invariant-feature based approaches [1–6].
4.10.4 Face detection with Marginal Space Learning
Face detection is one application well suited for Marginal Space Learning. There are many different
marginal subspaces that could be chosen as part of the MSL chain. Some of them correspond to different
face parts such as left or right eye or ear, nose, or mouth. Other marginal spaces could be simpler
representations of the whole face, such as a low resolution face patch. In this application, we will use
a 2-space chain 1 ⊂, where 1 is the marginal space of possible right eye positions with a coarse
scale (xe, ye, se). The space  is a four-dimensional space (x, y, s, θ) ∈ parameterizing face position
(x, y), ﬁne scale s and orientation θ. The right eye was chosen because the eye is a very salient part
of the face. By training different face part detectors (eyes, nose, mouth, ears), we observed that the
eye detector has a more compact classiﬁer than the others, so it is more suited to be an intermediate
subspace in the MSL chain. The diagram of the face detection application is shown in Figure 10.2.
The eye marginal space consists of the right eye locations (xe, ye) for faces in a certain range of
scales. Because of that, the eye locations are detected on a Gaussian pyramid [49] hence they also have
a coarse scale se that is a power of 2.
The ﬁnal detector detects faces with four parameters (x, y, s, θ). It does so by using the candidate
eye locations (xe, ye, se) returned by the marginal classiﬁer and adding a many possible rotation and
ﬁne scale parameters relative to the right eye. The classiﬁer trained in this space will output a number
of detected faces, which will be the result of our algorithm after non-maximal suppression.
4.10.4.1 Scale and rotation invariant eye detection
This ﬁrst marginal space 1 in our framework is the right eye location and a classiﬁer is trained to detect
the right eye, independent of the face orientation, for faces in a certain range of sizes (between 15 and
50 pixels wide). To make sure that the faces are in this range, the detector is run for images reduced by
a power of 2. Thus, the detector works in the marginal space of right eye locations (xe, ye, se), where
the eye is detected at position (xe, ye) in an image that was reduced 2se times.

276
CHAPTER 10 Multi-Path Marginal Space Learning for Object Detection
FIGURE 10.2
Example of MSL for face detection. The ﬁrst marginal classiﬁer detects the person’s right eye (xe, y e, se),
ignoring the rotation and with a rough scale. The detected eyes are transformed into face candidates
(x, y, s, θ) by adding rotations and scales on a grid. These candidates are pruned by the face detector
to obtain the ﬁnal result.
Face features have also been used for face detection before [50], but the features were obtained as ﬁlter
responses, and not by a trained classiﬁer. Moreover, the features were combined purely based on geom-
etry, without any use of the appearance after the feature detection step. This is not the case in this work.
The eye detector is trained as a cascade of LogitBoost [51] classiﬁers, based on Haar features [7] and
the integral image, with more details given in Section 4.10.6.2. Other eye detection techniques [52–55]
could be more appropriate than the Haar-based approach that we used.
The output of this marginal classiﬁer is a number of detected right eyes, that are used to constrain
the search in the next space.
Examples outputs of this marginal classiﬁer are shown in Figure 10.3. As one could see, the classiﬁer
takes advantage of the area surrounding the eye (nose, glasses, etc.) and makes a correct detection even
when the eye is completely occluded, as shown in the middle image in Figure 10.3.
This level eliminates more than 99% of the total number of windows that would have to be evaluated
in the four dimensional space, at the cost of eliminating some of the true eye locations. If better features
(e.g., rotation/illumination invariant) were used to obtain a better eye detection, the whole system’s
performance in both speed and accuracy could be further improved.
4.10.4.2 Four parameter face detection
The obtained particles (candidates) (xe, ye, se) from the eye marginal space 1 are extended to
4-parameter face candidates (x, y, s, θ) , where (x, y) is the face center position, s is the isotropic
scale and θ is the face orientation. This is done by adding to each eye candidate a set of 135 possible
orientation-scale combinations, with 15 discrete orientations between −35◦and 35◦and 9 discrete
relative scales s = se(1 + jδs), j = 0, . . . , 8 where δs = 1/7.

4.10.4 Face Detection with Marginal Space Learning
277
FIGURE 10.3
Right eye locations detected by the marginal classiﬁer. The classiﬁer takes advantage of the eye context
(nose, glasses, etc.) and detects the eye even when it is occluded, as shown in the middle image.
Algorithm 1. Face Detection By MSL
Input: Input Image I.
Output: Set D of detected faces.
1:
Construct a Gaussian pyramid I r,r = 1, n by reducing the image I by powers
of 2 as long as the size is at least lmin.
2:
for r = 1 to n do
3:
Detect eye candidates (xe
i , ye
i , se
i ), i = 1, . . . , dr in I r.
4:
Generate face candidates using Eq. (10.1)
ci jk = (xi jk, yi jk, si j, θk), i = 1, . . . , dk, j = 0, . . . , ns, k = 0, . . . , nθ,
where si j = se
i (1 + jδs), θk = −θmax + kδθ.
5:
Compute p(ci jk|Ir) and discard ci jk if p(ci jk|Ir) < τ
6:
end for
7:
Perform Non-Maximal Suppression (Algorithm 2) on the remaining candidates ci jk.
Anthropometric face measures [56–58] are used to predict the face center (x, y) given the eye location
(xe, ye), scale s and angle θ as
(x, y) = (xe, ye) + 0.5 · 7sd + 0.4 · 7sd⊥,
(10.1)
where d = ( cos θ, sin θ) and d⊥= (−sin θ, cos θ). An example of face candidates obtained this way
at two different scales is shown in Figure 10.4.
The face candidates close to true faces were used as positive examples while the ones that were far
from faces were used as negatives. The face detector was trained using these positives and negatives,
Haar features and a cascade of Logitboost classiﬁers, with more details given in Section 4.10.6.2.
The whole face detection algorithm using MSL is summarized in Algorithm 1. There are usually
many overlapping detections on the same face with slightly different centers, scales and orientations,
as illustrated in Figure 10.5.

278
CHAPTER 10 Multi-Path Marginal Space Learning for Object Detection
FIGURE 10.4
Four parameter face candidates (xi, yi, si, θi) at two scales generated from the detected eyes.
FIGURE 10.5
Detections rescaled to the original image size. There are usually multiple detections on the same face, which
will be handled by the non-maximal suppression Algorithm 2.
To deal with this overlapping detection issue, a non-maximal suppression step is used, as described in
Algorithm 2. The algorithm starts with a set of detected candidates ci = (xi, yi, si, θi) with probabilities
pi above a predeﬁned threshold τ. It keeps the highest probability candidate and removes all candidates
that are close to it (i.e., have centers inside the box determined by its parameters (x, y, s, θ)). Then it
keeps the best of the remaining candidates and again removes all candidates that are close to it, and so
on, until no candidates are left.
The threshold τ can be used to control the detection and false positive rate. By varying the thresh-
old τ, a ROC (Receiver Operating Characteristic) curve can be obtained, such as the red curve from
Figure 10.10.
Examples of detections obtained using the face detection algorithm are shown in Figure 10.6.

4.10.5 Multiple Computational Paths in Marginal Space Learning
279
Algorithm 2. Non-maximal Suppression
Input: Candidates ci = (xi, yi, si, θi) with scores pi > τ and bounding boxes bi.
Output: Set D of detected faces.
1:
Find the candidate ci with highest score pi.
2:
if ci exists then initialize D = {i} else D = ∅, stop.
3:
while true do
4:
Remove candidates c j with centers inside any box bi, i ∈D.
5:
Find remaining candidate c j of highest score p j.
6:
if c j exists then add j to detected set: D ←D ∪{ j} else stop.
7:
end while
FIGURE 10.6
Detected faces after non-maximal suppression.
4.10.5 Multiple computational paths in Marginal Space Learning
When the dimensionality of the parameter space  is large or when the marginal spaces correspond
to parts that are occluded, it is possible that the propagated particles will not reach the ﬁnal space
 for detecting the object of interest. The Recursive Compositional Models (RCM) [40–43], which
are similar in spirit to MSL, have a built-in degree of robustness to missing data that translates into a
certain degree of robustness of the optimization. When failures occur in the RCMs, the obtained model
parameters x ∈ have some missing parameters that are ﬁlled in with their most probable values,
without taking the image into consideration. This could result in an inaccuracy of the ﬁnal result, and
has been addressed in the RCMs by a post-processing step of data driven segmentation using Grab-Cut
[59], using the RCM result for initialization.
To avoid the disadvantages of using a preselected MSL path, multiple MSL paths can be used,
based on different sequences of marginal spaces. However, a direct application of this idea does not

280
CHAPTER 10 Multi-Path Marginal Space Learning for Object Detection
FIGURE 10.7
Face detection using two Marginal Space Learning paths, using the left and respectively right eye locations
as marginal subspaces of the 4D face model. Even if one eye is occluded, the face can still be detected
successfully through the other MSL path.
show much improvement in performance because while the number of detections increases due to the
multiple paths, the number of false positives also increases.
However, by aggregating the results from the different MSL paths instead of just merging them, sig-
niﬁcant performance improvements can be obtained. By using a simple aggregation scheme described
below, signiﬁcant improvements have been observed for the face detection with MSL application
described in Section 4.10.4. This is because two different MSL paths could serve as independent
conﬁrmations of a detection result. This idea is similar to the approach taken by the newspapers to
publish information only if it is conﬁrmed through two different sources. Furthermore, this idea is also
related to co-training [60,61], a method for semi-supervised learning that uses two independent sets
of features for conﬁrmation of detection results on unseen data. In view of the co-training paradigm,
semi-supervised learning based on multi-path MSL might be feasible.
We performed an experiment using two MSL paths for face detection, using left and respectively
right eye detectors as intermediate marginal spaces. The diagram of these two MSL paths is illustrated in
Figure 10.7. In Figure 10.8 are shown the detected left and right eyes in cyan and yellow1 respectively.
Two different face detectors were trained because the 4D faces are aligned differently if coming
through the left or right eye path. Examples of detected faces through the left and right eye MSL paths are
shown in cyan respectively yellow in Figure 10.9. As one could see, most of the time the face is detected
by both MSL paths, but there are some isolated cases when only one of the detectors ﬁnds the face.
We propose the following way to aggregate the results obtained through the multiple MSL paths:
•
An object detected with strong conﬁdence from any of the paths is considered detected.
•
An object detected from one MSL path is considered detected if it is conﬁrmed from another MSL
path, i.e., when a sufﬁciently close object detected from another MSL path exists.
Hence a face is detected if either it has strong conﬁdence through one MSL path or it is conﬁrmed
from both paths.
1For interpretation of color in Figures 8 and 9, the reader is referred to the web version of this book.

4.10.5 Multiple Computational Paths in Marginal Space Learning
281
FIGURE 10.8
Detected right (yellow) and left (cyan) eye locations. (For interpretation of the references to color in this
ﬁgure legend, the reader is referred to the web version of this book.)
FIGURE 10.9
Detected face location locations through the left (cyan) and right (yellow) MSL paths respectively. (For
interpretation of the references to color in this ﬁgure legend, the reader is referred to the web version of this
book.)

282
CHAPTER 10 Multi-Path Marginal Space Learning for Object Detection
FIGURE 10.10
Face detection on unseen data (the MIT-CMU dataset) for two-path MSL, MSL left eye, MSL right eye,
Convolutional Face Finder, FloatBoost, and Viola-Jones.
Using this aggregation scheme we obtained the ROC curve shown in Figure 10.10 as a black solid
line. It shows that using Multiple MSL paths leads to signiﬁcant improvements in accuracy, obtaining
results comparable to some recent results in face detection [62–64].
This multi-path MSL idea is motivated by our work in guide-wire localization [15], where an MSL
approach was used to detect and segment a thin and ﬂexible wire in ﬂuoroscopy (real-time X-ray). In the
guide-wire work, different parts of the wire were modeled using the same discriminative classiﬁer, even
though they correspond to different possible subspaces 1. It just so happened that for the guidewire,
all these subspaces can use the same model, but this would not extend to other objects, e.g., a snake
that have a different appearance for the head than for the body. In practice we observed that different
sequences of subspaces were used in different images, depending on what parts of the wire were more
visible. Moreover, the chain of subspaces could be longer or shorter depending on the length of the
wire in the image. For these reasons, the guide-wire localization work [15] served as a ﬁrst proof of
feasibility of the Multiple Path MSL idea.
4.10.6 Experimental validation
Two MSL face detectors as described in Section 4.10.4 were trained, one using the right eye as the
marginal space and another one using the left eye. The multi-path MSL was constructed from these two
detectors as described in Section 4.10.5 above.

4.10.6 Experimental Validation
283
4.10.6.1 The training dataset
The eye and face classiﬁers were trained on a database of 160 images obtained from the Internet,
containing 2600 faces that were manually annotated. To obtain a larger set of negative examples [12],
215 images that do not contain any faces were added from the Berkeley dataset [65].
The faces were manually annotated by placing the two eye locations. Given the two eyes, the square
window surrounding the face is aligned with the line connecting the eyes, has width twice the distance
d between the eyes and height 2.6d. The face window center is at equal distance from the eyes and at
distance 0.8d from the line connecting the eyes. These dimensions are similar to the anthropometric
measures from [58].
4.10.6.2 Implementation details
The eye and the face detectors were trained as cascades of three LogitBoost classiﬁers [51], with
parameters given in Table 10.1.
Eye detectors: The left and right eye detectors were trained as cascade of Logitboost classiﬁers
with 20, 60, and 180 locally constant weak regressors, each weak regressor being based on only one
feature. The features are based on Haar features [7,66], restricted in a window of size 19 × 19 pixels
centered at the location (x, y). To obtain a more robust training, the number of positive examples was
increased with rotated versions by ±5◦. This way 6500 positive examples were used for training.
On the training data, this level detects about 99% of the eyes with a false alarm rate of about 0.1%.
We also performed an evaluation of this classiﬁer on unseen data, namely the MIT + CMU frontal
face test set [67], containing 130 images and 507 faces. Here, the right eye detector detects 92.7% of
the right eyes with an error of at most 1 pixel, and has a false alarm of 0.15%. This shows that there
were not enough training examples to capture all the variability in the test data.
Face detectors: From the eye detector, a number of eye position candidates (xe
i , ye
i , se
i ) are obtained.
A total of 135 face candidates with four parameters (x, y, s, θ) are obtained from each eye candidate,
as described in Section 4.10.4.2.
The face detector is also a three-level cascade of Logitboost classiﬁers with 50, 125, and 312 locally
constant weak regressors respectively. Each Logitboost classiﬁer is trained using 124,917 Haar features
restricted in an image of size 21 × 23, working on integral images of rotated and rescaled versions of
the original image. For each face candidate (x, y, s, θ), the feature parameters were scaled by s and
extracted from the integral image of the image rotated by −θ.
As one could see, the total number of features used in this multi-path MSL approach is 1494, smaller
than the other face detection systems (4916 in [7] and 2546 in [10]). One of the reasons is that an eye
has less variability in appearance than an entire face, at the resolution where the whole face has about
Table 10.1 Training Details for the Two Classiﬁers Including the Number of Weak Classiﬁers,
Detection Rate and False Positive Rate
Classiﬁer
# Features
Clf 1
Clf 2
Clf 3
TPR train (%)
FPR train (%)
Eye
66,024
20
60
180
99.2
0.1
Face
124,917
50
125
312
95.6
0.5

284
CHAPTER 10 Multi-Path Marginal Space Learning for Object Detection
25×25 pixels. Thus an eye detector can be trained using a more compact classiﬁer than a face detector,
with good generalization power.
4.10.6.3 Evaluation
The three MSL versions (two one path and one multi-path) were evaluated on the MIT + CMU dataset
[67], containing 130 images and 507 faces that were not used for training.
As already mentioned in [68], very few papers disclose the criterion on which they report a face as
detected or not. Quite similar to [69], we declare a face correctly detected if the following two criteria
are satisﬁed:
•
The distance from the detected window center to the true face window center is less than 0.3 times
the true face height.
•
The ratio between the heights of detected window and the true face is in the interval [0.5, 1.5].
Based on these evaluation criteria, the ROC curves obtained on the MIT + CMU dataset are shown
in Figure 10.10. The ROC curves obtained by Viola-Jones [7], FloatBoost [10], and Convolutional Face
Finder [63] are also shown for comparison.
We also present in Table 10.2 a comparison with other face detection methods, including the neural-
network based face detection [7,34,67] and the Convolutional Face Finder [63]. For [34], we show in
parentheses the actual number of false positives, since they did not report the detection rates for the
same false positives as the other papers, but at the same time, they reported detection rates out of 506
faces, not 507 as in the other papers.
From the experiments, one could see that the multi-path MSL approach outperforms the single path
MSL versions. It also outperforms some face detection algorithms such as the FloatBoost [7,10] while
being trained on fewer faces. Furthermore, the multi-path MSL obtains results comparable (at 10 false
Table 10.2 Face Detection Rates for Different Numbers of False Positives Obtained by Different
Methods on the MIT + CMU Frontal Face Dataset Containing 130 Images and 507 Faces
False detections
Detector
Train faces
0
10
31
MSL right eye
2600
74.8% (1)
88.5%
89.2%
MSL left eye
2600
64.8%
87.1%
89.9%
Multi-path MSL
2600
80.7% (1)
90.1%
92.3%
Rowley et al. [67]
1046
–
83.2%
86.0%
FloatBoost [10]
6000
–
83.6%
90.2%
[7]
4916
–
76.1%
88.4%
Viola-Jones (voting)
4916
–
81.1%
89.7%
[34]
–
–
89.7% (6)
94.4% (29)
Convolutional face ﬁnder
3702
88.8%
90.3%
91.5%
CART [62]
–
–
90.5%
93.1%
Dynamic cascade
40,857
86.9% (1)
89.8%
92.2%

4.10.6 Experimental Validation
285
FIGURE 10.11
Face detection results obtained by our method.

286
CHAPTER 10 Multi-Path Marginal Space Learning for Object Detection
FIGURE 10.12
More face detection results obtained by our method.

4.10.9 Datasets
287
positives) to the CART based classiﬁer from [62], to the Convolutional Face Finder [63] trained on 3700
faces, and to the Dynamic Cascade [64] trained with 40,000 faces.
In the future, we plan to enlarge the number of training faces by adding images from the FERET
dataset [70] and using some of the smoothing and contrast reduction techniques from [63]. Our algorithm
uses only 2600 faces for training, as opposed to the state of the art algorithms for face detection such
as [33,64] that use between 10,000 and 40,000 faces.
The detection time depends on the image complexity. For simple images, the marginal classiﬁers
will remove most of the false positives, resulting in a fast detection. As a result, for a 384 × 288 image
the detection time varies between 0.06 and 0.30 s on a 2.4 GHz dual core PC.
Examples of face detections on some test images are given in Figures 10.11 and 10.12.
These results can be improved in accuracy by using better features such as illumination-invariant
features [34], or CART features [62], or by joint training of all cascade levels [71], learning the features
[72], and of course by training with more faces.
4.10.7 Applications
The Marginal Space Learning and the multi-path Marginal Space Learning can be applied to most object
detection problems. Furthermore, it can be applied to object segmentation, since an object segmentation
is a more accurate description of an object, with more parameters than in object detection.
4.10.8 Open issues and problems
One open issue is to give a mathematical characterization of the quality of different MSL paths to help
deciding which paths are the best.
Another issue that remains open for multi-path Marginal Space Learning is how to coordinate
the different MSL paths for an efﬁcient use of computation. According to the way the results from
different MSL paths are aggregated, if a detection through one path is strong enough, any detec-
tions through other paths that are close to this strong detection are not necessary. Thus redundant
computation could be saved by avoiding to pursue any detections close to strong detections from
one path.
4.10.9 Datasets
The following publicly available datasets have been mentioned in this chapter:
1. The CMU + MIT Face dataset [67]. A dataset of 130 grayscale images containing 507 manually
annotated faces.
2. The FERET Dataset [70]. A dataset containing thousands of images of faces from different angles
(frontal, half-proﬁle, proﬁle, etc.), with different facial expressions and illuminations.

288
CHAPTER 10 Multi-Path Marginal Space Learning for Object Detection
4.10.10 Conclusions and future trends
In this chapter we presented a fast and robust method for object detection that combines different
computational paths of Marginal Space Learning for improved robustness against occlusion and detector
failures. In this approach, invariant detectors are used to select a set of good object candidates in different
marginal spaces while detectors based on non-invariant features are used to increase the number of object
parameters and obtain the ﬁnal solution.
We showed that this multi-path MSL method can achieve good detection rates and low false positive
rates, comparable with some state of the art approaches, while at the same time using less training data
than these approaches. With better features (e.g., rotation/illumination invariant), more training data
and better models for the ﬁnal classiﬁer, further performance improvements are possible.
The multi-path MSL method can be used in the future for detecting hard-to-ﬁnd objects such as
boats, that pose challenges to other state of the art techniques.
Glossary
Classiﬁer
a function that takes one or more variables called features as input
and returns a discrete class label (e.g., object/non-object) or a prob-
ability over the possible class labels
Detection rate
the percentage of true positives (e.g., faces) that were correctly
detected by an object detection algorithm
False positive rate
the percentage of detections of an algorithm that are not close to
true positives (e.g., faces)
Haar feature
a function that takes an input image and returns a linear combination
of sums of intensities inside different rectangles. The computation
of sums inside a rectangle can be done in constant time using the
integral image
Integral image
an image as large as the original image, containing at location (x, y)
the sum of all pixel intensities in the rectangle from (0, 0) to (x, y)
Receiver Operating
Characteristic (ROC) Curve
a curve displaying the detection rate vs. the false positive rate of an
algorithm when a detection parameter (usually a detection thresh-
old) is varied
Strong classiﬁer
a classiﬁer that is constructed from a number of weak classiﬁers or
weak regressors and is much better than any of them
Weak classiﬁer
a classiﬁer that has a non-zero correlation with the class label, hence
it is better than random guessing
Weak regressor
a function that takes one or more variables as input and returns a
continuous value that has a non-zero correlation with the value of
interest (e.g., age or GPA score)

References
289
Relevant Theory: Machine Learning
See Vol. 1, Chapter 14 Learning Theory
See Vol. 1, Chapter 15 Neural Networks
See Vol. 1, Chapter 16 Kernel Methods and Support Vector Machines
See Vol. 1, Chapter 17 Online Learning in Reproducing Kernel Hilbert Spaces
See Vol. 1, Chapter 18 Introduction to Probabilistic Graphical Models
References
[1] D. Forsyth, J. Mundy, A. Zisserman, C. Coelho, A. Heller, C. Rothwell, Invariant descriptors for 3D object
recognition and pose, IEEE Trans. PAMI 13 (10) (1991) 971–991.
[2] J. Wood, Invariant pattern recognition: a review, Pattern Recogn. 29 (1) (1996) 1–17.
[3] A. Zisserman, D. Forsyth, J. Mundy, C. Rothwell, J. Liu, N. Pillow, 3D object recognition using invariance,
Artif. Intell. 78 (1–2) (1995) 239–288.
[4] R. Fergus, P. Perona, A. Zisserman, Object class recognition by unsupervised scale-invariant learning, CVPR
2003.
[5] H. Chen, P. Belhumeur, D. Jacobs, In search of illumination invariants, in: CVPR 2000.
[6] D. Slater, G. Healey, The illumination-invariant recognition of 3D objects using local color invariants,
Illumination 18 (2) (1996) 206–210.
[7] P. Viola, M. Jones, Robust real-time face detection, Int. J. Comput. Vis. 57 (2) (2004) 137–154.
[8] F. Fleuret, D. Geman, Coarse-to-ﬁne face detection, Int. J. Comput. Vis. 41 (1) (2001) 85–107.
[9] B. Heisele, T. Serre, S. Prentice, T. Poggio, Hierarchical classiﬁcation and feature reduction for fast face
detection with support vector machines, Pattern Recogn. 36 (9) (2003) 2007–2017.
[10] S. Li, L. Zhu, Z. Zhang, A. Blake, H. Zhang, H. Shum, Statistical learning of multi-view face detection, in:
ECCV 2002.
[11] D. Roth, M. Yang, N. Ahuja, A snow-based face detector, in: NIPS 2001.
[12] K. Sung, T. Poggio, Example-based learning for view-based human face detection, IEEE Trans. PAMI 20 (1)
(1998) 39–51.
[13] H. Schneiderman, T. Kanade, Statistical method for 3D object detection applied to faces and cars, CVPR 1
(2000) 746–751.
[14] J. Wu, J. Rehg, M. Mullin, Learning a rare event detection cascade by direct feature selection, in: NIPS 2003.
[15] A. Barbu, V. Athitsos, B. Georgescu, S. Boehm, P. Durlak, D. Comaniciu, Hierarchical learning of curves
application to guidewire localization in ﬂuoroscopy, in: CVPR 2007, pp. 1–8.
[16] A. Barbu, M. Suehling, X. Xu, D. Liu, S. Zhou, D. Comaniciu, Automatic detection and segmentation of
lymph nodes from ct data, IEEE Trans. Med. Imag. (2011).
[17] S. Feng, S. Zhou, S. Good, D. Comaniciu, Automatic fetal face detection from ultrasound volumes via learning
3d and 2d information, in: CVPR 2009, pp. 2488–2495.
[18] J. Feulner, S. Zhou, M. Huber, J. Hornegger, D. Comaniciu, A. Cavallaro, Lymph node detection in 3-d chest
ct using a spatial prior probability, in: CVPR 2010, pp. 2926–2932.
[19] H. Ling, S. Zhou, Y. Zheng, B. Georgescu, M. Suehling, D. Comaniciu, Hierarchical, learning-based automatic
liver segmentation, in: CVPR 2008, pp. 1–8.
[20] S. Seifert, A. Barbu, S. Zhou, D. Liu, J. Feulner, M. Huber, M. Suehling, A. Cavallaro, D. Comaniciu,
Hierarchical parsing and semantic navigation of full body ct data, Med. Imaging 7259 (2009) 725902.
[21] Y. Zheng, A. Barbu, B. Georgescu, M. Scheuering, D. Comaniciu, Fast automatic heart chamber segmentation
from 3D CT data using Marginal Space Learning and steerable features, in: ICCV 2007.

290
CHAPTER 10 Multi-Path Marginal Space Learning for Object Detection
[22] Y. Amit, D. Geman, A computational model for visual selection, Neural Comput. 11 (7) (1999) 1691–1715.
[23] P. Chou, C. Brown, The theory and practice of Bayesian image labeling, Int. J. Comput. Vis. 4 (3) (1990)
185–210.
[24] P. Dollar, Z. Tu, S. Belongie, Supervised learning of edges and object boundaries, in: CVPR 2006.
[25] M. Jones, J. Rehg, Statistical color models with application to skin detection, Int. J. Comput. Vis. 46 (1) (2002)
81–96.
[26] H. Wang, S. Chang, A highly efﬁcient system for automatic face region detection in MPEG video, IEEE Trans.
Circ. Syst. Video Technol. 7 (4) (1997) 615–628.
[27] R. Hsu, M. Abdel-Mottaleb, A. Jain, Face detection in color images, IEEE Trans. PAMI (2002) 696–706.
[28] S. Agarwal, D. Roth, Learning a sparse representation for object detection, in: ECCV 2002.
[29] M. Andriluka, S. Roth, B. Schiele, Pictorial structures revisited: people detection and articulated pose esti-
mation, in: CVPR 2009.
[30] P. Felzenszwalb, D. Huttenlocher, Pictorial structures for object recognition, Int. J. Comput. Vis. 61 (1) (2005)
55–79.
[31] T. Wu, S. Zhu, A numerical study of the bottom-up and top-down inference processes in and-or graphs, Int.
J. Comput. Vis. 93 (2) (2011) 226–252.
[32] Y. Zheng, A. Barbu, B. Georgescu, M. Scheuering, D. Comaniciu, Four-chamber heart modeling and automatic
segmentation for 3-D cardiac CT volumes using Marginal Space Learning and steerable features, IEEE Trans.
Med. Imaging 27 (11) (2008) 1668–1681.
[33] L. Bourdev, J. Brandt, Robust object detection via soft cascade, in: CVPR 2005.
[34] H. Schneiderman, Feature-centric evaluation for efﬁcient cascaded object detection, in: CVPR, vol. 2, 2004.
[35] L. Fei-Fei, R. Fergus, P. Perona, One-shot learning of object categories, IEEE Trans. PAMI 28 (4) (2006)
594–611.
[36] A. Torralba, K. Murphy, W. Freeman, Sharing features: efﬁcient boosting procedures for multiclass object
detection, in: CVPR 2004.
[37] V. Blanz, T. Vetter, A Morphable model for the synthesis of 3D faces, in: SIGGRAPH, 1999, pp. 187–194.
[38] C. Huang, H. Ai, Y. Li, S. Lao, Vector boosting for rotation invariant multi-view face detection, in: ICCV
2005, pp. 446–453.
[39] Z.Tu,Probabilisticboosting-tree:learningdiscriminativemodelsforclassiﬁcation,recognition,andclustering,
in: ICCV 2005.
[40] L. Zhu, Y. Chen, X. Ye, A. Yuille, Structure-perceptron learning of a hierarchical log-linear model, in: CVPR
2008.
[41] L. Zhu, Y. Chen, A. Yuille, Unsupervised learning of a probabilistic grammar for object detection and parsing,
in: NIPS 2007, p. 1617.
[42] L. Zhu, Y. Chen, A. Yuille, Unsupervised learning of probabilistic Grammar-Markov Models for object
categories, IEEE Trans. PAMI 31 (1) (2009) 114–128.
[43] L. Zhu, C. Lin, H. Huang, Y. Chen, A. Yuille, Unsupervised structure learning: hierarchical recursive compo-
sition, suspicious coincidence and competitive exclusion, in: ECCV 2008, pp. 759–773.
[44] L. Kuncheva, Combining Pattern Classiﬁers: Methods and Algorithms, Wiley-Interscience, 2004.
[45] A. Doucet, Particle markov chain monte carlo, in: Third Cape Cod MCMC Workshop, 2007.
[46] C. Liu, A Bayesian discriminating features method for face detection, IEEE Trans. PAMI 25 (6) (2003)
725–740.
[47] Z. Tu, Learning generative models via discriminative approaches, in: CVPR 2007.
[48] A. Barbu, M. Suehling, X. Xu, D. Liu, S. Zhou, D. Comaniciu, Automatic detection and segmentation of
axillary lymph nodes, in: MICCAI 2010, pp. 28–36.
[49] P. Burt, E. Adelson, The Laplacian pyramid as a compact image code, IEEE Trans. Commun. 31 (4) (1983)
532–540.

References
291
[50] T. Leung, M. Burl, P. Perona, Finding faces in cluttered scenes using random labeled graph matching, in:
ICCV 1995, pp. 637–644.
[51] J. Friedman, T. Hastie, R. Tibshirani, Additive logistic regression: a statistical view of boosting, Ann. Stat. 28
(2) (2000) 337–407.
[52] G. Feng, P. Yuen, Multi-cues eye detection on gray intensity image, Pattern Recogn. 34 (5) (2001) 1033–1046.
[53] P. Wang, M. Green, Q. Ji, J. Wayman, Automatic eye detection and its validation, in: CVPR 2005, pp. 164–164.
[54] Z. Zhou, X. Geng, Projection functions for eye detection, Pattern Recogn. 37 (5) (2004) 1049–1056.
[55] Z. Zhu, Q. Ji, Robust real-time eye detection and tracking under variable lighting conditions and various face
orientations, Comput. Vis. Image Understand. 98 (1) (2005) 124–154.
[56] D.DeCarlo,D.Metaxas,M.Stone,Ananthropometricfacemodelusingvariationaltechniques,in:Proceedings
of the 25th Annual Conference on Computer Graphics and Interactive Techniques, ACM, New York, NY, USA,
1998, pp. 67–74.
[57] T. Horprasert, Y. Yacoob, L. Davis, Computing 3-D head orientation from a monocular image sequence,
in: Proceedings of the Second International Conference on Automatic Face and Gesture Recognition, 1996,
pp. 242–247.
[58] V. Popovici, J. Thiran, Y. Rodriguez, S. Marcel, On performance evaluation of face detection and localization
algorithms, in: Indian Council of Philosophical Research, vol. 1, 2004.
[59] C. Rother, V. Kolmogorov, A. Blake, GrabCut: interactive foreground extraction using iterated graph cuts,
ACM Trans. Graph. (TOG) 23 (3) (2004) 309–314.
[60] A. Blum, T. Mitchell, Combining labeled and unlabeled data with co-training, in: COLT, New York, NY, USA,
1998, pp. 92–100.
[61] K. Nigam, R. Ghani, Analyzing the effectiveness and applicability of co-training, in: Proceedings of the Ninth
International Conference on Information and Knowledge Management, ACM, New York, NY, USA, 2000,
pp. 86–93.
[62] S. Brubaker, M. Mullin, J. Rehg, Towards optimal training of cascaded detectors, in: ECCV 2006.
[63] C. Garcia, M. Delakis, Convolutional face ﬁnder: a neural architecture for fast and robust face detection, IEEE
Trans. PAMI 26 (11) (2004) 1408–1423.
[64] R. Xiao, H. Zhu, H. Sun, X. Tang, Dynamic cascades for face detection, in: ICCV 2007.
[65] D. Martin, C. Fowlkes, D. Tal, J. Malik, A database of human segmented natural images and its application
to evaluating segmentation algorithms, in: ICCV, vol. 2, 2001, pp. 416–425.
[66] C. Papageorgiou, M. Oren, T. Poggio, A general framework for object detection, in: ICCV 1998, pp. 555–562.
[67] H. Rowley, S. Baluja, T. Kanade, Neural network-based face detection, in: CVPR 1996, pp. 203–208.
[68] E. Hjelmas, B. Low, Face detection: a survey, Comput. Vis. Image Understand. 83 (3) (2001) 236–274.
[69] M. Osadchy, Y. Le Cun, M. Miller, Synergistic face detection and pose estimation with energy-based models,
J. Mach. Learn. Res. 8 (2007) 1197–1215.
[70] <http://face.nist.gov/colorferet/>.
[71] M. Dundar, J. Bi, Joint optimization of cascaded classiﬁers for computer aided detection, CVPR 2007.
[72] P. Wang, Q. Ji, Learning discriminant features for multi-view face and eye detection, in: CVPR 2005.

11
CHAPTER
Markov Models and MCMC
Algorithms in Image Processing
Xavier Descombes
Morpheme, Joint Group, INRIA/I3S/IBV, Sophia-Antipolis cedex, France
4.11.1 Introduction: the probabilistic approach in image analysis
Probabilistic approaches have been brought to image analysis starting with the famous paper by Besag
[1], inspired by the Gibbs ﬁeld theory developped in statistical physics. Later on, this approach mostly
developped in the Bayesian framework has become very popular in the image processing community
from the texture modeling proposed by Cross and Jain [2] and the restoration scheme proposed by
Geman and Geman [3] to the recent developments based on marked point processes for multiple objects
detection [4]. Adapted to solve ill posed inverse problems, the stochastic approach in image analysis
includes, but is not restricted to image restoration, denoising, deblurring, classiﬁcation, segmentation,
feature extraction, surface reconstruction, stereo matching.
The main idea is to model independently the data, including the sensors property and the noise, and
somepriorknowledgewemayhaveonthesolution.Thesetwomodels,respectivelynamedthelikelihood
and the prior, are combined to deﬁne the posterior distribution, by applying the Bayes rule. One key
point is to consider Markovian models which can be rewritten as a Gibbs model, using the Hammersley-
Clifford theorem. The posterior is then deﬁned by an energy function written as a sum of local functions,
so called the potentials. These functions can be interpreted as local constraints that are very intuitive. For
example, in the case of an image restoration problem, the term issued from the likelihood measures the
similarity between the solution and the data whereas the prior energy measures the similarity between
neighbourgh pixels. The solution consists then of the conﬁguration minimizing the energy, interpreted
as a cost function, that is a compromise between an uniform image and the data. In a probabilistic
setting, this solution maximizes the posterior and is referred as the Maximum A Posteriori (MAP).
Once the model deﬁned, the second issue consists of the solution computation. This optimization
problem has three main characteristics. The different variables, for example associated to each pixel, are
mutuallydependentbecauseoftheinteractionintroducedinthepotentialsfunction.Therefore,wecannot
perform the optimization independently on each variable. The posterior embeds a normalizing function,
also named the partition function, which is neither anatically nor numerically computable. Therefore,
the posterior cannot be directly simulated, it requires iterative methods such as Markov Chain Monte
Carlo (MCMC) simulations. Finally, the energy function is rarely convex. Classical approaches based
on gradient descent cannot be applied, or at most only provide an approximation of the solution. In some
speciﬁc cases, some combinatorial algorithms, based on graph theory, can provide the exact solution.
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00011-X
© 2014 Elsevier Ltd. All rights reserved.
293

294
CHAPTER 11 Markov Models and MCMC Alogorithms in Image Processing
In this chapter, we derive the two main frameworks based on stochastic modeling in image analysis.
The most traditional concerns Markov Random Fields. We brieﬂy give the main deﬁnitions and derive
associated algorithms to perform the optimization. Several examples illustrate the wide range of MRF
modeling. In a second step, we generalize the random ﬁeld approach to the point process approach that
allows geometry to be taken into account. Here again, after a brief description of the main deﬁnitions
and optimization techniques, we exemplify on several applications.
4.11.2 Lattice based models and the Bayesian paradigm
4.11.2.1 Modeling
In this section we consider the most classical case. The data consist of an image, deﬁned on a discrete
lattice (set of sites), and the expected output is another image on the same lattice. This includes image
segmentation, restoration and classiﬁcation. Let consider a ﬁnite discrete lattice  ⊂Z2. For simplicity
we consider dimension 2 but the generalization to higher dimension is straightforward. An image is an
application from  to a state space S. Typically, S is the gray level space {0, 1, . . . , 255}. The set of all
possible images is called the conﬁguration space and is deﬁned by  = Sm, where m is the cardinal of
. An image, or equivalently a conﬁguration, is deﬁned as follows:
X ∈ = Sm,
X = {xi, i ∈},
xi ∈S.
(11.1)
We consider some data given by an image Y and we want to estimate a second image X, representing
a denoised or a deblured version of Y, a segmentation or a classiﬁcation of Y, etc. The transformation
from Y to X is modeled by the posterior P(X|Y). We want to estimate a conﬁguration maximizing
the posterior. In the modeling point of view we have access to the degradation model, for example the
blurring model due to optic and the noise model of the CCD. This model is given by P(Y|X). To link
both probabilities we consider the Bayes rule:
P(X|Y) = P(Y|X)P(X)
P(Y)
∝P(Y|X)P(X),
(11.2)
where X (resp. Y) represents the solution (resp. the data) and P(Y|X) is the likelihood, P(X) the prior.
The prior contains all informations we may have on the solution. For example, we can impose
smoothness for a restored image or spatial homogeneity for a segmentation map. In this context, Markov
Random Fields are attractive as they enable to consider local constraints, which are easier to model,
while providing a global formulation of the random ﬁeld, numerically tractable for simulations.
Consider a random ﬁeld X = (xs), s ∈. We deﬁne a system of neighborhood Ns, s ∈, satisfying
the following properties:
•
∀s ∈, s /∈Ns,
•
t ∈Ns =⇒s ∈Nt.
We have the following deﬁnition:
Deﬁnition 1.
X is a Markov Random Field (MRF) if and only if:
•
∀X0 ∈, P(X = X0) > 0 (positivity),
•
∀s ∈, P(xs|xt, t ∈/{s}) = P(xs|xt, t ∈Ns) (Markov property).

4.11.2 Lattice Based Models and the Bayesian Paradigm
295
The existence of MRFs has been proved by Dobrushin [5]. The Markov property implies that a MRF
is completely characterized by local conditional probabilities, this local description inducing a global
expression of the random ﬁeld. In practice, the modeling task is highly simpliﬁed by the formulation of
MRFs as Gibbs ﬁelds.
Deﬁnition 2.
A Gibbs ﬁeld is a random ﬁeld which can be written as follows:
P(X) = 1
Z exp[−U(X)] = 1
Z exp

−

c∈C
Vc(xs, s ∈c)

,
(11.3)
where Z is called the partition function (normalizing constant), Vc are the potentials and C is the set of
cliques (a clique is a ﬁnite subset of sites).
The Hammersley-Clifford theorem states the equivalence between MRFs and Gibbs Fields. We thus
can re-write the local conditional probabilities of an MRF as follows:
P(xs/xt, t ∈Ns) =
1
Zl(xt, t ∈Ns) exp

−

c∈C:s∈c
Vc(xt, t ∈c)

.
(11.4)
To summarize, an MRF is deﬁned by a set of cliques, inducing a neighborhood system, and a set of
associated potentials.
An historical Gibbs ﬁeld, so called the Ising model, has been intensively studied in statistical physics
for modeling ferromagnetic phenomenon. Cliques consist of pair of adjacent sites and the associated
potential is as follows:
Vc(xs, xt) =
−β
if xs = xt,
0
otherwise.
(11.5)
In dimension 2, the cliques are deﬁned by the pairs {(i, j), (i + 1, j)} and {(i, j), (i, j + 1)}. The
neighborhood of pixel s = (i, j) is then given by Ns = {(i −1, j), (i + 1, j), (i, j −1), (i, j + 1)}.
Figure 11.1 shows some Ising model simulations for different values of parameter β, the bigger β the
stronger attractive effect. In image modeling, this can be interpreted as a smoothing or a regularizing
effect as neighbor pixels tend to have the same value. This model, generalized to multiple label, leads
to the Potts model which is widely used for image segmentation. In the case of image restoration,
continuous potentials are considered.
FIGURE 11.1
Simulation of the Ising model (from left to right β = 0.1; 0.5; 1; 5).

296
CHAPTER 11 Markov Models and MCMC Alogorithms in Image Processing
4.11.2.2 Optimization
MRFs are characterized by potentials which are local functions, easy to compute. However, the normal-
ized density depends on the partition function, which is written as a sum over all possible conﬁgurations.
Therefore, its computation is numerically intractable. Besides, the analytical formulation is not known
except in very special cases, as for example the Ising model in two dimensions. To simulate and opti-
mize MRFs we have to consider iterative algorithms avoiding the computation of the partition function.
Markov Chain Monte Carlo algorithms (MCMC) enable simulation of any stochastic process that is
described by an unnormalized density [6]. These sampling techniques simulate a discrete time Markov
chain, (Xt)t∈N , over the conﬁguration space , which converges towards the target distribution. This
chain is designed so as to be ergodic so that the convergence does not depend on the initial conditions.
The transitions of the chain represent, in practice, relatively simple perturbations, meaning that only a
few sites or variables are perturbed at once.
Among MCMC approaches, the Metropolis-Hastings algorithm is particularly adapted for simulat-
ing MRFs [7]. One iteration is composed of two steps. From the current conﬁguration X(n), a new
conﬁguration X(n+1) is proposed. This new proposition is obtained by simulating a sample of a pro-
posal Q(X(n), .). A key point is to be able to draw a sample from the proposal with few computations.
Therefore the proposition is usually a simple perturbation of the current conﬁguration, for example
changing the value of a single pixel in image processing. In the second step this new conﬁguration is
accepted following a probability that depends on the difference of energy between the current and the
proposed conﬁgurations. The Metropolis-Hastings algorithm is summarized in Algorithm 1.
Initialize X = X(0);
if at iteration n, X(n) then
•
Propose a new state X according to Q(X(n),.);
•
Calculate the Hastings ratio deﬁned by :
R
P(X )Q(X , X(n))
P(X(n))Q(X(n), X )
•
Choose X(n+1) =
=
X with a probability min(1,R), and X(n+1) =X(n) otherwise;
end
(11.6)
Algorithm 1.
Metropolis-Hastings algorithm
If we consider the image processing case where we change a single pixel at each step, an iteration
consists of scanning the image and addressing the pixels one by one. When considering pixel i, we have
X(n)
(i) =

x(n+1)
0
, . . . , x(n+1)
i−1
, x(n)
i
, x(n)
i+1, . . . , x(n)
m

,
and
X′ =

x(n+1)
0
, . . . , x(n+1)
i−1
, x′, x(n)
i+1, . . . , x(n)
m

,

4.11.2 Lattice Based Models and the Bayesian Paradigm
297
where x′ has been chosen according to q(x(n)
i
, ·). Using the Markov property, the Hastings ratio is
computed as follows:
R =
P(X′)q(x′, x(n)
i
)
P(X(n)(i))q(x(n)
i
, x′)
=
q

x′, x(n)
i

q

x(n)
i
, x′
 exp

−U

X′) + U(X(n)
(i)
	
(11.7)
=
q

x′, x(n)
i

q

x(n)
i
, x′
 exp

−

c∈C:i∈c
Vc(x′, xt, t ∈c/{i}) −Vc

x(n)
i
, xt, t ∈c/{i}

,
which only requires the computation of local potentials associated to site i.
In practice there is a tradeoff between the complexity of the proposal simulation and the Hastings ratio
value giving the acceptation rate, which leads to a tradeoff between the computation required for one
iteration and the number of iterations required before convergence. The most simple proposal, that is the
uniform distribution, leads to the Metropolis algorithm while an Hastings ratio equal to one is obtained
with the Gibbs sampler for which the proposal q(xi, x′
i) is given by the local conditional law p(x′
i|Ni).
In most applications, the searched conﬁguration is the one that maximizes the posterior but not
simply a sample. Therefore we are searching for a Maximum A Posteriori estimator:
ˆX = arg max
X∈
P(Y|X)P(X).
(11.8)
This is a non convex optimization problem for which direct solutions cannot be obtained, except for
speciﬁc cases. A general framework to estimate the solution is to embed a simulation algorithm into a
simulated annealing scheme [8,9]. This consists in sampling the law P(X|Y)
1
T during a cooling process
where parameter T , so called the temperature, is slowly decreasing to 0. At high temperature the distri-
bution P(X|Y)
1
T tends to a uniform distribution and the sampling process visits the whole conﬁguration
space. As T tends to 0, this distribution tends to a Dirac distribution on the MAP conﬁguration. The
simulated annealing algorithm is written as follows:
Initialize X = X(0) randomly and set T = T0, n = 0;
While convergence not reached do
•
Propose a new state X according to Q(X(n), .);
•
Calculate the Hastings ratio deﬁned by :
R =
P(X |Y)
P(X(n)|Y)
1
T Q(X , X(n))
Q(X(n), X )
•
Choose X(n+1) = X with a probability min(1,R), and X(n+1) = X(n) otherwise
•
decrease T and set n to n + 1;
end
(11.9)
Algorithm 2.
Simulated annealing algorithm

298
CHAPTER 11 Markov Models and MCMC Alogorithms in Image Processing
The simulated annealing requires to set up three quantities that are the initial temperature, the
temperature decreasing law and the convergence criterion. If the initial temperature is high enough
and if the decreasing scheme for the temperature is slow enough (for example logarithm scheme in
terms of iteration number), the convergence to the MAP estimator has been proved [3]. In practice, the
theoretical cooling scheme is too slow. Instead, a geometrical decreasing scheme is used, Tn = αN T0,
where α is close to one, typically 0.95. As for the convergence criterion, one can stop the algorithm when
the conﬁguration has not changed during a few iterations or by ﬁxing a maximum number of iterations.
The simulated annealing is particularly interesting because of the proof of convergence. However,
due to computation time considerations, the theoretical law for the temperature decrease is not used in
practice. In some cases, fast deterministic algorithms are employed. This is the case of the Iterated Con-
ditional Mode (ICM) algorithm [1]. In this algorithm we iterate local optimizations until convergence.
We have no guaranty of convergence to the MAP but, providing that we can start from a “good” initial
conﬁguration, we obtain satisfactory results in practice. The ICM algorithm is written as follows:
Initialize X = X(0);
While convergence not reached do
•
Scan the image and for each site i,
X = (xt) = (x(n+1)
0
, . . . , x(n+1)
i−1
, x(n)
i
, x(n+1)
i+1
,. . . , x(n)
N )x set:
x(n+1)
i
= arg max
u
p(xi = u|xt, t ∈Ni)
end
(11.10)
Algorithm 3.
Iterated Conditional Mode
At each iteration of the ICM the energy is decreasing. If the energy is bounded from below the
algorithm converges. However, it converges to a local minimum of the energy, contrarily to the simulated
annealing. In practice, we stop iterations when no pixel has changed during one iteration. In some
speciﬁc cases, some combinatorial approaches, issued from the graph theory, can be used to minimize
the energy. The ﬁrst task here is to model the energy minimization problem as a max ﬂow problem in a
graph. Consider a binary MRF with pairwise interactions, such that the energy can be written as follows:
U(X) =

i∈
V1(xi) +

c={i, j}∈C
V2(xi, x j),
(11.11)
where xi ∈{0, 1}. Let us assume that we have V2(1, 1) = V2(0, 0) and V2(1, 0) = V2(0, 1). Consider
the graph given by the lattice, that is: a node is associated to each pixel and an edge represents each
clique c. To each edge (i, j), a cost is associated and is equal to V2(1, 1) −V2(1, 0). We add two nodes
referred to as the sink and the source associated respectively to state 0 and 1 (see Figure 11.2). An edge
is added between each pixel and the source, with cost V1(0), and the sink, with cost V1(1). If we consider
a cut of the constructed graph, each pixel remains linked with either the source or the sink. Therefore,
a conﬁguration is associated with each possible cut. Besides, minimizing the energy is equivalent to
minimizing the cut cost (namely the sum of the cost associated with cut edges). Therefore, the minimal
cut, which can be obtained by a max ﬂow algorithm, is also a minimizer of the energy [10]. Under certain
assumptions, such as the convex inequality [11], the max ﬂow algorithm provides the MAP estimation.

4.11.3 Some Inverse Problems
299
FIGURE 11.2
Graph construction from a pairwise MRF, two nodes (the sink in bottom and the source in top) are added to
the lattice nodes.
4.11.3 Some inverse problems
In this section, we consider the most classical applications of Markov Random Field modeling in image
processing. More detailed descriptions of this methodology and more complete reviews of applications
can be found in books dedicated to MRFs [12–14].
4.11.3.1 Denoising and deconvolution: the restoration problem
Image restoration is often the ﬁrst step before analyzing the information content of an image. It mainly
consists of the image quality improvement. It is one of the ﬁrst problems which have been addressed by
an MRF modeling [3,14,15]. We consider an image Y corrupted by some distorsion due to the optics
of the sensor, for example a convolution modeling the bluring effect due to defocussing, and noise due
to the sensor itself or to the transmission process. If we assume the distorsion to be linear, we can write:
Y = K(X) ⊕η,
(11.12)
where η represents the noise and K is a linear operator that is the convolution in case of blurring effect.
The restoration problem consists in recovering X from Y. Embeding the problem into a Bayesian
framework, we maximize the following posterior:
P(X|Y) = P(Y|X)P(X)
P(Y)
∝P(Y|X)P(X).
(11.13)

300
CHAPTER 11 Markov Models and MCMC Alogorithms in Image Processing
Assuming for example that we have an additive independent Gaussian noise, in the case of image
denoising (K is the identity), we then have:
P(Y|X) = exp

−

s∈
(ys −xs)2
2σ 2
−log

2πσ 2

,
(11.14)
where σ 2 is the noise variance.
The prior P(X) aims at regularizing the solution, i.e., smoothing the resulting image and is usually
modeled by a pairwise interaction Gibbs ﬁeld:
P(X) = 1
Z exp
⎧
⎨
⎩−

c=<s,s′>
Vc(xs, xs′)
⎫
⎬
⎭.
(11.15)
A Gaussian Gibbs ﬁeld with Vc(xs, xs′) = (xs −xs′)2 could achieve this goal. However, a Gaussian
prior ﬁeld leads to blur edges. To avoid blurring, more sophisticated priors are considered to preserve
edges [16,17], as for example, the 	-model:
Vc(xs, xs′) =
−β
1 + (xs−xs′)2
δ2
,
(11.16)
where β is a parameter representing the strength of the prior and δ is the minimum gray level gap to
deﬁne an edge. To summarize, the energy to be minimized, in the case of image denoising is written as
follows:
Uden(X|Y) =

c=<s,s′>
−β
1 + (xs−xs′)2
δ2
+

s∈
(ys −xs)2
2σ 2
,
(11.17)
where c are sets of two neighboring pixels (sites). We usually consider for each pixel the four or eight
closest pixels. Another classical example is image deconvolution, when the image is blurred due to
movements of the camera or defocusing. In this case the operator K is a convolution by a kernel K and
the energy is given by:
Udec(X|Y) =

c=<s,s′>
−β
1 + (xs−xs′)2
δ2
+

s∈
(ys −(K ∗X)s)2
2σ 2
.
(11.18)
A denoising result obtained by the energy described in Eq. (11.17) using the Langevin dynamics is
shown on Figure 11.3 for different levels of noise [18].
As stated before, one key point is to smooth the noise without smoothing edges. The ﬁrst solution is
to consider adapted potentials, such as the 	-functions. Another possibility is to consider an additional
process on the dual lattice, so called a line process [16]. A binary random variable is associated to each
clique. The potential between the two sites is then inhibited when the line variable is equal to one. A
prior on the line process model the edge continuity of the conﬁguration. We then have to optimize two
random ﬁelds.

4.11.3 Some Inverse Problems
301
Initial image
Noisy image (   =20)
Restored image
Noisy image (   =50)
Restored image
σ
σ
FIGURE 11.3
Image denoising using a 	-model.
4.11.3.2 Segmentation problem
A segmentation map is a partition of the plane. Each region represents an object or a speciﬁc area on the
image. Consider a random ﬁeld Y = (ys)s∈, where ys ∈S. The likelihood term P(Y|X) model the
gray level distribution of the pixels belonging to a given class or region. For example, we may consider
that each class represents a given feature (sea, sand, crops in remote sensing data or gray, white matter,
and CSF for brain images, as in the example given on Figure 11.4) and exhibits a Gaussian distribution,
characterized by its means and variance. The likelihood is then written as follows:
P(Y|X) = exp

−

s∈

i∈I

(ys −μi)2
2σ 2
i
−log

2πσ 2
i

δ(xs = i)

,
(11.19)
where δ(a) is equal to one if a is true and 0 otherwise. μi (resp. σ 2
i ) is the mean (resp. variance) of class
i, I being the set of classes.

302
CHAPTER 11 Markov Models and MCMC Alogorithms in Image Processing
Initial image
Maximum Likelihood
Potts model
FIGURE 11.4
Magnetic Resonnance Image segmentation using a Potts model.
By maximizing the likelihood function alone, we obtain a noisy segmentation (see Figure 11.4). To
regularize the solution, i.e., to obtain smooth regions without holes, we consider a Gibbs ﬁeld P(X) as
prior to impose spatial homogeneity in the solution. Derived from statistical physics, the most famous
prior in image segmentation is the Potts model [1,3], which is written as follows:
P(X) = 1
Z exp
⎧
⎨
⎩−

c={s,s′}
[βδ(xs ̸= xs′)]
⎫
⎬
⎭,
(11.20)
where β > 0 represents the strength of the prior. The total energy is then written as follows:
Useg(X|Y) =

c={s,s′}
βδ(xs ̸= xs′) +

s∈

i∈I

(ys −μi)2
2σ 2
i
−log

2πσ 2
i

δ(xs = i)

.
(11.21)
Figure 11.4 shows the segmentation results with and without the Potts model. We can see that the
prior removes the local errors due to data.
Although the Potts model succeeds in regularizing the solution, it is not always well suited for image
segmentation [19]. Let us consider a connected component and its energy given by Eq. (11.21). The
energy cost to keep this connected component is proportional to its surface if we consider the data
term and to its boundary length is we consider the prior. Therefore, the longer the boundary the more
penalized the object. The Potts model tends to remove ﬁne structures in the solution. To overcome this
problem, several models have been proposed [20,21]. The main idea of these models is to differentiate
local noise on conﬁgurations from edges and lines. To deﬁne edges, higher range interactions are needed.
An example of such a model is given in [22,23] and referred to as Chien model. This model preserves
ﬁne structures and linear shapes in images while regularizing the solution. In this model, the set of
cliques is composed of 3 × 3 squares. Three parameters (n,l, and e) are associated to these patterns.

4.11.3 Some Inverse Problems
303
C(15)
8
C(3)
C(4)
8
16
C(5)
8
C(6)
8
C(7)
16
C(8)
4
C(9)
8
C(10)
8
C(11)
4
C(12)
16
8
C(13)
8
C(14)
16
C(16)
16
C(17)
16
C(18)
C(19)
16
8
C(20)
8
C(24)
8
C(21)
8
C(22)
16
C(23)
8
C(25)
4
C(26)
8
C(27)
4
C(28)
16
C(29)
8
C(30)
16
C(31)
8
C(32)
16
C(36)
8
C(35)
16
C(34)
8
C(33)
8
C(42)
16
C(40)
16
C(39)
16
C(38)
16
C(37)
8
C(41)
C(46)
8
C(44)
16
C(43)
16
C(45)
8
8
C(47)
8
C(49)
2
C(50)
8
C(51)
2
C(48)
2
2
C(2)
C(1)
FIGURE 11.5
The different classes induced by a binary 3 × 3 model and their number of elements.
Before constructing the model, the different conﬁgurations induced by a 3 × 3 square are classiﬁed
using the symmetries (symmetry black-white, rotations, etc.). This classiﬁcation and the number of
elements in each class are described in Figure 11.5. A parameter C(i) is associated to each class that
refers to the value of the potential function for the considered conﬁguration. So, under the hypothesis
of isotropy of the model, which induces some symmetries, plus the black/white symmetry, we have for
such a topology (cliques of 3×3) ﬁfty-one degrees of freedom. The construction of the model consists in
imposing constraints that provide relations between its parameters. Two energy functions that differ only
by a constant are equivalent, so we suppose that the minimum of the energy is equal to zero. We suppose
that uniform realizations deﬁne the minimum of the energy, so that the ﬁrst equation for the parameters
is given by C(1) = 0. We then deﬁne the different constraints with respect to those two uniform
realizations. The ﬁrst class of constraints concerns the energy of edges per unit of length which is noted
e.Duetosymmetriesandrotationsweonlyhavetodeﬁnethreeorientationsofedgescorrespondingtothe
eight ones induced by the size of cliques. These constraints and the derived equations are represented in
Figure 11.6. Similar constraints are considered to deﬁne the energy associated with lines and undesirable
local conﬁgurations are set to n, representing the noise. The potential associated with each conﬁguration
is then a linear combination of the three parameters e,l, and n:
∀i = 0, . . . , 51 C(i) = ϵ(i)e + λ(i)l + η(i)n,
(11.22)
and coefﬁcients ϵ(i), λ(i), η(i) are deﬁned through the relations between potentials C(i). Then the
resulting distribution is written:
Pe,l,n(X) =
1
Z(e,l, n) exp [−eN0(X) −lN1(X) −nN2(X)],
(11.23)

304
CHAPTER 11 Markov Models and MCMC Alogorithms in Image Processing
+
=
=
+
+
+
=
+
+
FIGURE 11.6
Equations associated with edge constraints.
where:
N0(X) =

i=1,...,51
ϵ(i)#i(X),
N1(X) =

i=1,...,51
λ(i)#i(X),
N2(X) =

i=1,...,51
η(i)#i(X).
#i(X) being the number of conﬁgurations of type i in the realization X.
A comparison of Potts and Chien models for ﬁne structures segmentation is shown on Figure 11.7.
We have reversed 15% of the pixels in this binary image. The Chien model appears to be much more
adapted to image modeling than the Potts model.
4.11.3.3 Texture modeling
AnotherimportantdomainofimageprocessingwhereGibbsﬁeldsplayaleadingroleistexturemodeling
[2,24–26]. To characterize objets or speciﬁc land cover in an image, the pixel scale is not always the most
appropriate. On Figure 11.8, the radiometry (gray level information) is adapted for distinguishing the
different ﬁelds. But within the urban area, the gray level are almost uniformly distributed. Therefore, to
decide if a pixel belong to an urban area or not, the gray level information is not sufﬁcient. To distinguish
urban areas, we then have to consider the local distribution of gray levels or a texture parameter to
characterize it. In the MRF approach, we assume that, locally, the gray levels are distributed according
to a Gibbs distribution, and we estimate the parameters associated with this Gibbs distribution. The
relevant feature to discriminate the different areas in an image is given by these parameters, instead
of the gray level values. To analyze textures, for example to delineate urban areas in remote sensing
images, simple models leading to fast and robust estimation techniques are preferred. When the goal is
to model texture themselves in order to synthetize them, generic model are addressed. In this context,
the relevance of Gibbs modeling is shown in [25] in the context of Brodatz textures. High range pairwise

4.11.3 Some Inverse Problems
305
Initial image
Noisy image
Segmentation using Ising model 
Segmentation using the Chien mod
FIGURE 11.7
Ising model versus Chien model.
interactions are considered to model macro and micro properties of the texture. Herein, we consider a
simpler model, that is the four connected isotropic Gaussian Markov Random Field, to extract urban
areas from satellite images [24]. See UrbanDetect for details.
Let us consider an image X = (xs), s ∈, where xs ∈S. The gray level space is typically S =
{0, 1, . . . , 255}. We assume that locally the considered image is a realization of a Gibbs ﬁeld with the
following energy:
Uθ(X) = β
⎛
⎝
c={s,s′}
(xs −xs′)2 + λ

s∈
(xs −μ)2
⎞
⎠,
(11.24)

306
CHAPTER 11 Markov Models and MCMC Alogorithms in Image Processing
Initial image
β map
Uran area
FIGURE 11.8
Urban areas detection using a GMRF.
where θ = (β, λ, μ) are the model parameters. μ is the mean, λ weight the external ﬁeld and the
interactions, and β is an inverse temperature parameter. Different estimators can be used to obtain the
parameter value. For instance, the Maximum Likelihood (ML) estimator is given by:
ˆθML = arg max
θ
Pθ(X).
(11.25)
The normalization constant being untractable numerically, we prefer to consider the Pseudo Likelihood
which only depends on a local normalization constant. In case of MRFs, the Maximum of the Pseudo
Likelihood (MPL) is given by:
ˆθMPL = arg max
θ

s∈
P(xs|xt, t ∈Ns).
(11.26)
Parameter β is a local conditional variance and can be interpreted as an urban indicator. The higher β,
the higher probability to be in an urban area. After estimating β on each pixel by considering a local
window, the β map is segmented to delineate urban areas, as shown on Figure 11.8 [26].
4.11.4 Spatial point processes
When considering high resolution images, the geometrical information is crucial. Indeed, objects are
often better characterized and recognized by their shape rather than their radiometry. Shape properties
are hardly modeled by local interactions. Besides, interactions between objects, such as repulsion or
alignment, are even more tricky to model at the pixel scale. One can imagine to consider a MRF on a
graph, where each node represents an object and edges are set between interacting objects [27]. Node
attributes can model the object geometry. However, this scheme requires to construct the graph, which
implies to know the number of objects to deﬁne the nodes and their relations to deﬁne the edges,
knowledge that we do not have in practice. To overcome this limitation, we describe in this section a

4.11.4 Spatial Point Processes
307
second modeling framework which can be seen as an extension of the MRF approach. We consider
conﬁgurations of an unknown number of objects described by parametric shapes and local interactions
between objects. We consider the marked point process framework that was previously developed in
the spatial statistical community [28,29]. A review of their utility in the context of image analysis can
be found in [4].
4.11.4.1 Modeling
Consider K, a compact subset of Rn. K represents the image support, i.e., the image coordinates are
embedded in a continuous space. A conﬁguration of points, denoted by x, is a ﬁnite unordered set of
points in K, such as {x1, . . . , xn}. The conﬁguration space, denoted by , is therefore written as:
 =

n∈N
n,
(11.27)
where 0 = {∅} and n = {{x1, . . . , xn}, xi ∈K, ∀i} is the set of the conﬁgurations of n unordered
points for n ̸= 0. For every Borel set A in K, let NX(A) be the number of points of X that fall in the
set A. A point process is then deﬁned as follows:
Deﬁnition 3.
X is a point process on K if and only if, for every Borel set A in K, NX(A) is a random
variable that is almost surely ﬁnite.
Deﬁnition 4.
A point process X on K is called a Poisson process with intensity measure ν(·) if and
only if:
•
NX(A) follows a discrete Poisson distribution with expectation ν(A) for every bounded Borel set A
in K,
•
for k non-intersecting Borel sets A1, A2, . . . , Ak, the corresponding random variables NX(A1),
NX(A2), . . . , NX(Ak) are independent.
For every Borel set, B, the probability measure πν(B), associated with the Poisson process, is given
by [28]:
πν(B) = e−ν(K)

1[∅∈B] +
∞

n=1
πνn(B)
n!

,
(11.28)
with:
πνn(B) =

K
. . .

K
1[{x1,...,xn}∈Bn]ν(dx1) . . . ν(dxn),
(11.29)
where Bn is the subset of conﬁgurations in B which contain exactly n points.
We now deﬁne more general processes by considering a density with respect to the Poisson measure.
Let f be a probability density with respect to the πν(·) law of the Poisson process, such that:
f :  →[0, ∞),


f (x)dπν(x) = 1.
(11.30)
The measure deﬁned by P(A) =

A f (x)dπν(x), for every Borel set A in , is a probability measure
on  that deﬁnes a point process.

308
CHAPTER 11 Markov Models and MCMC Alogorithms in Image Processing
Such a model can favor or penalize geometric properties such as clustering effect or points alignment,
which leads to interesting possibilities for modeling the scene under study. Similarly to MRFs, we
introduce the following Markov Property:
Deﬁnition 5.
Let X be a point process with density f . X is a Markov process under the symmetric
and reﬂexive relation ∼if and only if, for every conﬁguration x in  such that f (x) > 0, X satisﬁes:
•
f (y) > 0 for every y included in x (heredity),
•
for every point u from K, f (x ∪{u})/ f (x) only depends on u and its neighborhood ∂({u}) ∩x =
{x ∈x : u ∼x} (Markov property).
A result similar to the Hammersley-Clifford theorem allows the density of a Markov point process to
be decomposed as the product of local functions deﬁned on cliques:
Theorem 1.
A density which is associated to a point process f :  →[0, ∞[ is Markovian under the
neighborhood relation ∼if and only if there exists a measurable function φ :  →[0, ∞[ such that:
∀x ∈, f (x) = α

y⊆x,y∈Cx
φ(y),
(11.31)
where the set of cliques is given by Cx = {y ⊆x : ∀{u, v} ⊆y, u ∼v}.
Just as for the random ﬁeld case, we can then write the density in the form of a Gibbs density:
f (x) = 1
c exp
⎡
⎣−

y⊆x,y∈Cx
V (y)
⎤
⎦,
(11.32)
where U(x) = $
y⊆x,y∈Cx V (y) is called the energy and V (y) is a potential.
A typical example of a Markov/Gibbs process, which is often used, is the pairwise interaction process.
In this case, a neighborhood relation ∼is deﬁned between pairs of points. For example, xi ∼x j if and
only if d(xi, x j) < r, where d(·, ·) is the Euclidean distance and r is a radius of interaction. In this case,
the unnormalized density is written as:
h(x) =
n(x)

i=1
b(xi)

1≤i< j≤n(x);xi∼x j
g(xi, x j),
(11.33)
where n(x) is the number of points in the conﬁguration x.
In image analysis applications, the goal is to extract a set of objects rather than a set of points.
We therefore associate a low dimensional parametric object to each point. A marked point is then
deﬁned by its location xi and a random vector mi ∈M, called the mark, which deﬁnes the underlying
object (for example the radius in the case of a circle). We then have the following deﬁnition:
Deﬁnition 6.
A marked point process on χ = K × M is a point process on χ for which the positions
of the points are in K and the marks are in M, such that the unmarked point process is a well deﬁned
point process on K.

4.11.4 Spatial Point Processes
309
4.11.4.2 Optimizing
The Metropolis-Hastings algorithm does not allow the simulation of distributions in spaces of variable
dimension. However, in the case of marked point processes, where the number of points is itself a
parameter to be simulated, the sampler must be able to visit spaces of variable dimension. The state
space, in that case, is deﬁned as the union of the continuous subspaces, as
 =

k∈N
k.
(11.34)
It is therefore necessary to realize dimensional jumps between the subspaces in order to be able to reach
any state x ∈. Note that this is also the case for problems including the choice of a model, i.e., when
several models, deﬁned by a different number of parameters, are compared with each other.
Green was the ﬁrst to propose a global formulation, allowing the extension of the Metropolis-Hastings
algorithm to handle state spaces of variable dimension [30]. He generalized the algorithm proposed by
Geyer and Møller [31] that was restricted to birth and death movements, that is adding or remov-
ing one object in the conﬁguration. This algorithm, which is known as the Reversible Jump Markov
Chain Monte Carlo (RJMCMC) method considers distributions rather than densities. The general prin-
cipal remains very close to that of the Metropolis-Hastings algorithm. The main differences are as
follows:
1. The unnormalized density h is replaced by an unnormalized distribution π on the state space
 = % k.
2. The proposal q(x, y) is also replaced by a proposition kernel Q(x, A) deﬁned on  × B().
3. A symmetric measure ϕ(·, ·) is deﬁned on × such that π(·)Q(·, ·) is absolutely continuous with
respect to ϕ(·, ·). We can deﬁne f as follows:
f (x, y) = π(dx)Q(x, dy)
ϕ(dx, dy)
.
(11.35)
The Hastings ratio is then written as follows:
R = f (y, x)
f (x, y) = π(dy)Q(y, dx)
π(dx)Q(x, dy),
(11.36)
since the distribution ϕ(·, ·) is symmetric.
In his work, Green also proposed to consider a linear combination of kernels, Qi(x, A), i ∈I such
that:
Q(x, A) =

i∈I
Qi(x, A).
(11.37)
The proposition kernels must satisfy the points detailed below.
•
Qi(x, ) is known whatever the value of i ∈I.
•
$
i∈I
Qi(x, ) ≤1,
∀x ∈.

310
CHAPTER 11 Markov Models and MCMC Alogorithms in Image Processing
•
∀i ∈I, there exists a symmetric measure, ϕi(dx, dy), deﬁned on  × , such that π(dx)Qi(x, dy)
is absolutely continuous with respect to ϕi(dx, dy). The associated Radon-Nikodym derivative is
then denoted fi(x, y) and we have for all i ∈I:
fi(x, y) = π(dx)Qi(x, dy)
ϕi(dx, dy)
.
(11.38)
•
∀i ∈I, ∀x ∈, we can simulate realizations starting from the normalized proposals:
Pi(x, .) = Qi(x, .)
Qi(x, ).
(11.39)
If we denote the probability of choosing the proposition kernel Qi in state Xt = x by pi(x) =
Qi(x, ), then we can formulate the RJMCMC sampler by applying Algorithm 4.
Initialize X0 = x0 at t = 0;
If at iteration t, Xt=x then
•
Choose a proposition kernel Qi according to probability pi(x),
while leaving the state unchanged with probability 1 −i∈I pi(x) i.e., takingXt+1=x;
•
Simulate a new state, y, according to Pi(x, .);
•
Calculate the Hastings ratio
R = fi(y, x)
fi(x, y) = π(dy)Qi(y, dx)
π(dx)Qi(x, dy)
•
Choose Xt+1 = y with probability min (1, R), and Xt+1 = x otherwise;
end
(11.40)
Algorithm 4.
RJMCMC algorithm with mixture of kernels
This formulation of the RJMCMC algorithm allows different types of kernels during sampling.
It constitutes the most general version of the MCMC algorithms. The efﬁciency of an RJMCMC algo-
rithm depends on the good choice of the kernel. Adding and removing objects is necessary to obtain the
chain irreducibility. However, to speed-up the convergence, additional kernels consisting in modifying
an object or a couple of objects are necessary.
The birth and death kernel that we denote by QBD allows us to add and remove an object from the
current state. This type of perturbation was introduced initially by Geyer and Møller [31]. They proposed
an MCMC algorithm for point processes in which perturbations of the current state are exclusively births
and deaths of objects. In practice, Geyer and Moller’s algorithm is quite slow, since it does not include
simple perturbations for locally adjusting an object: indeed it is necessary to ﬁrst remove the object,
and then to insert a better located object. Birth and death kernel is necessary to estimate the number of
objects in the conﬁguration. If we consider a uniform birth on K and an uniform death on the current
conﬁguration x, containing n(x) objects, we have:
•
in the case of a birth, y = x ∪u,
QBD(y, dx)
QBD(x, dy) = pd(y)
pb(x)
ν(K)
n(y) ,
(11.41)
where n(y) = n(x) + 1,

4.11.4 Spatial Point Processes
311
•
in the case of a death, y = x \ u,
QBD(y, dx)
QBD(x, dy) = pb(y)
pd(x)
n(x)
ν(K).
(11.42)
The RJMCMC algorithms are founded on the acceptance/rejection concept. Indeed, a number of
propositions are rejected and, consequently, a number of calculations are done without improving the
conﬁguration. It is even commonly admitted that a good sampler has an acceptance rate of around 0.34,
i.e., two thirds of the calculations lead to a rejection. In addition, during simulated annealing, the density
becomes increasingly concentrated on the solution as the relaxation parameter is decreased, which leads
to a very high rejection rate. At the end of relaxation, the algorithm therefore does not accept any more
ﬂuctuations that requires a ﬁne tuning of the temperature decrease scheme. In practice, the setting of
the cooling scheme is delicate, particularly for marked point processes. Besides, the computation of the
acceptation ratio prevents to consider sophisticated kernels involving numerous objects. An alternative
approach, based on the discretization of a stochastic differential equation, allows multiple objects birth
during a single iteration. Besides, the birth of this new conﬁguration of objects is not subject to an
acceptation ratio. A selection step, called the death step, then allows the chain to converge to the right
distribution [32].
Each step of the chain therefore contains a birth step, given by x2, and a death step, given by x\x1.
The transition probability associated with the birth of an object in the elementary volume x ∈K is
given by:
qx,δ =
zxδ
if x →x ∪x,
1 −zxδ
if x →x (no birth in x).
(11.43)
The transition probability associated with the death of an object x from the conﬁguration x is written as:
px,δ =

 exp[β(U(x)−U(x\x))]δ
1+exp[β(U(x)−U(x\x))]δ =
axδ
1+axδ
if x →x\x,
1
1+axδ
if x →x (x survives).
(11.44)
The convergence of this discrete procedure towards the continuous procedure is presented in [32], and
the resulting algorithm is called the multiple births and deaths algorithm:
Initialize x(0), n = 0, δ = δ0, T = T0;
While convergence had not been attained do
Draw a realization of a Poisson process with intensity δ, call it y, and update the conﬁguration
x ←x ∪y
For every object x in x calculate ax = exp
U(x)−U(x\x)
T
, draw p from a uniform distribution
over [0, 1];
If p <
axδ
1+axδ then
|
remove x : x ←x\x
end
n ←n + 1, δ ←δ × αδ, T ←T × αT
end
Algorithm 5.
Multiple births and deaths algorithm
The parameters αδ and αT are coefﬁcients, less than 1. In practice, convergence is obtained when
all the objects added during the birth phase, and only those, are removed during the death phase. Note

312
CHAPTER 11 Markov Models and MCMC Alogorithms in Image Processing
that too great a decrease in δ, and therefore in the birth rate, can freeze the conﬁguration, since objects
will no longer be added. Compared to the RJMCMC scheme, the birth phase add to the current conﬁg-
uration a collection of objects, without any rejection. Therefore, ﬂuctuations between two consecutive
conﬁgurations are larger, especially at low temperature. This implies a much more robust algorithm
with respect to the temperature cooling scheme. To accelerate the convergence of the multiple births
and deaths algorithm, the birth choice can be made according to the data instead of using a uniform
Poisson law. Suppose that we have a birth map, B(s), s ∈S, on the image lattice that favors certain
positions during the birth phase. Without losing the convergence properties, the multiple births and
deaths algorithm can then be modiﬁed as follows:
For every pixel s calculate the value of the birth map B(s)
Normalize the birth map : ∀s ∈S, b(s) =
B(s)
s∈S B(s)
Initialize x(0), n = 0, δ = δ0, T = T0;
While convergence has not been attained do
For every pixel s ∈S, if no object from x is centered on s, add an object, centered on s with
probability b(s)δ.
For every object x of x calculate ax = exp
U(x)−U(x\x)
T
, draw p according to the uniform
distribution on [0, 1];
If p < a+xδ
1+axδ then
|
remove x : x ←x \ x
end
n ←n + 1, δ ←δ × αδ, T ←T × αT
end
Algorithm 6.
Addition of the birth map
The death step can also be improved by ordering the objects in the conﬁguration. For example, the
objects can be visited in order of decreasing data energy, which means that we ﬁrst propose to remove
the objects which are badly localized on the data. This algorithm has been mixed with a graph cut
algorithm to perform the death step [33].
4.11.5 Multiple objects detection
4.11.5.1 Trees counting
Aerial and satellite images play a more and more important role in the ﬁeld of natural resource man-
agement, and in particular for forests. High resolution data provide accurate enough information to
perform a count of individuals in a population. In this context, the objective of modeling by marked
point processes, as presented here, is to extract the tree crown from very high resolution aerial images
of the forests (see TreeCounting for details).
We consider ellipses as objects. The position space associated with the image is K
=
[0, X M] × [0, YM], and the mark space associated with the ellipse conﬁgurations, M = [am, aM] ×

4.11.5 Multiple Objects Detection
313
1
0
INTERSECTION
V2
V1
2ε
−V1
−V2
X
Y
FIGURE 11.9
Left: object intersections and the associated coefﬁcient of intersection. Right: object alignments favored by
the prior.
[bm, bM] × [0, π[, where X M and YM are the height and width of the image I respectively, and where
a ∈[am, aM] is the major axis, b ∈[bm, bM] is the minor axis, and θ ∈[0, π[ is the orientation of the
ellipses.
For images of plantations, we model three main properties in the prior:
•
Trees are not superposed. To avoid detecting the same tree crown with two objects (cf Figure 11.9 left)
we consider a repulsion term between two objects xi ∼r x j that intersect each other. A coefﬁcient,
Qr(x1, x2) ∈[0, 1], more or less penalizes two objects as a function of their area of intersection:
Ur(x) = γr

xi∼rxj
Qr(xi, xj), . . . , γr ∈R+.
(11.45)
•
Plantations are characterized by a regular repartition of trees. An attraction term is deﬁned to favor
regular alignments (see Figure 11.9 right). A quality function, Qa(x1, x2) ∈[0, 1] quantiﬁes the
alignment between two objects x1 ∼a x2, by comparing the vector −−→
x1x2 to the two vectors that
represent the principal directions of the plantation:
Ua(x) = γa

xi∼axj
Qa(xi, xj), . . . , γa ∈R−.
(11.46)
•
For process stability reasons, we forbid two objects closer than the minimum admissible distance,
taken to be equal to 1 pixel:
Uh(x) =
+∞if ∃(xi, x j) ∈x|d(xi, xj) < 1,
0
otherwise.
(11.47)
Figure 11.10 shows a plantation and a simulation of the prior. The prior provides dense conﬁgurations
of non intersecting ellipses.

314
CHAPTER 11 Markov Models and MCMC Alogorithms in Image Processing
FIGURE 11.10
Left: image of a poplar plantation. Right: simulation of the prior.
The data on which the tests are carried out are scanned aerial photographs from infra-red ﬁlm, which
highlights the zones containing chlorophyll. In [34], a Bayesian model, in which the data term is the
likelihood of the observations, is presented: Ud(x) = −log (L(I|x)). It assumes that we can construct
a statistical model given a conﬁguration of objects x, inside and outside the tree crowns. The approach
consists of considering the pixels of the image as belonging to one of two Gaussian classes: either the
background (low gray level) or else the trees (high gray level), whose parameters can be estimated using
a K-means classiﬁcation algorithm. This Bayesian data term gives good results for images that only
contains trees and ground such as on Figure 11.11.
For more complex scenes, the energy of the data term is deﬁned as the sum of local terms for each
of the objects [35]:
Ud(x) = γd

xi∈x
Ud(xi).
(11.48)
The objects are favored by a negative data energy and penalized by a positive data energy. The local
data energy can be interprated as the output of a local ﬁlter. In the images, the trees are distinguished
thanks to their shadows, which create a dark zone around the tree. The foliage and branches of a tree can

4.11.5 Multiple Objects Detection
315
FIGURE 11.11
Result of extraction on the image in Figure 11.10 (200 × 140 pixels) with a Bayesian model, 10 millions
iterations (15 min).
thus be considered as a light shape surrounded by a dark zone. This property is reﬂected by a distance
quantifying the contrast between the ellipse x and its neighborhood Fρ(x), as deﬁned on Figure 11.12.
For example, we can consider the following distance:
dB(x, F(x)) = (μx −μF(x))2
2

σ 2x + σ 2
F(x)
−log

σxσF(x)
σ 2x + σ 2
F(x)

.
(11.49)
A sigmoid function maps the considered distance to the interval [−1, 1] as follows:
Qa(d) =
⎧
⎪⎨
⎪⎩

1 −d
d0
	
if d < d0,

exp

−(d−d0)
d0

−1
	
otherwise,
(11.50)
where d0 is the distance threshold from which the data term favors the object in the conﬁguration.

316
CHAPTER 11 Markov Models and MCMC Alogorithms in Image Processing
Light pixels inside the ellipse
Dark pixels inside the frontier
ρ
FIGURE 11.12
Ellipse x and its border Fρ(x).
The optimization is performed using an RJMCMC algorithm embedded into a simulated annealing.
The proposition kernel contains birth and death, simple perturbation of objects (dilation, translation,
rotation), and a speciﬁc movement consisting in splitting and merging ellipses.
As shown on Figure 11.13, the Bayesian model tends to ﬁt ellipses on the chlorophyll areas. The
model considered a data term as a sum of “local ﬁlters” and avoids false alarms on meadows providing
a more precise count.
4.11.5.2 Road network detection
With this second example, we tackle the traditional problem of remote sensing image analysis consisting
of extracting the line networks and especially road networks. Traditionally, we distinguish two main
types of methods for extracting a road network. The ﬁrst type relates to semi-automatic methods for
which some seeds are provided by the user, and from these a tracking algorithm extracts the network [36].
This kind of approach can be made unsupervised, the second kind of methods, by automatic detection of
the seeds [37]. When using the marked point process framework, we address the problem at a segment
scale [38,39]. We introduce a marked point process for road network extraction (see RoadDetection for
details). The line network S is deﬁned by a set of segments that is the realization of a Markov object
process S. We specify the process by its density h relative to a homogeneous Poisson process. The
density contains a data term g and a prior term f that contains the constraints on the structure, the
connectivity and the average curvature of the network:
h(S) ∝g(S) f (S),
(11.51)
where S = {si, i = 1, . . . , n} is a set of segments.

4.11.5 Multiple Objects Detection
317
FIGURE 11.13
Top: test image (166 × 224 pixels). Middle: result of extraction with the Bayesian model. Below: result of
extraction with the non-Bayesian model, in 10 million iterations (20 min) (from [40]).

318
CHAPTER 11 Markov Models and MCMC Alogorithms in Image Processing
Double segment
Free segment
Simple segment
Disk of radius ε
FIGURE 11.14
The different types of segments.
Two segments are said to be connected if the closest distance between their endpoints is less than a
constant ϵ. This relation makes it possible to deﬁne three types of segments as shown on Figure 11.14.
Free segments are unconnected segments, simple segments are connected at only one endpoint, and
double segments are connected at both endpoints. The connectivity of the network is modeled in the
density by penalizing free segments and, to a lesser extent, by penalizing simple segments. To avoid
superposition of segments, or intersections with acute angles, while still permitting intersections with
(approximate) right angles, we deﬁne the connectivity relation Rc by stating that two segments are
connected if the angle between them is large. For example, on Figure 11.16, s1 and s2 are considered as
connected but not s1 and s3. Moreover, in order to favor segment pairs whose endpoints and orientations
are close, as for the (s1, s2) pair of Figure 11.16, a potential function VRc is deﬁned as follows:
for si ∼c s j,
VRc(si, s j) = Vτ(τi j) + Vϵ(di j)
2
,
(11.52)
with
⎧
⎨
⎩
Vτ(τi j) =
−σ(τi j, τmax) if |τi j| < τmax,
1 otherwise,
Vϵ(di j) = −σ(di j, ϵ).
Vτ favors segment pairs, (si, s j), whose difference in orientation τi j is less than a threshold τ and
penalizes other cases. Vϵ relates to the distance di j between the endpoints, and tends to connect close
segments. The attractive terms of these functions are given by a quality function σ which is described
by Eq. (11.53):
σ(·, M) : [−M, M] −→[0, 1],
x
−→σ(x, M) =
1
M2

1+M2
1+x2 −1

.
(11.53)
This function is positive over [−M, M], and it is maximal at 0, as shown on Figure 11.15.
Segment pairs which form an angle that is less than a (small) constant, denoted c, are prohibited. For
other conﬁgurations, we consider the same quality function σ which is of a repulsive nature, according
to the difference between the orientations of the two segments. Thus, for each pair (si, s j) such that
si ∼io s j, we consider:
VRio(si, s j) =
∞if τi j < c,
1 −σ(τi j, π/2 −δmin) otherwise,
(11.54)

4.11.5 Multiple Objects Detection
319
σ( . ,π/2)
σ( . ,π/4)
−π/2
π/2
π/4
−π/4
1
0
FIGURE 11.15
Quality function.
1
4
S
14
τ
3
S
max
τ
2
S
12
τ
S
ε
FIGURE 11.16
Different types of connection—(s1, s2): attractive connection—(s1, s3): not considered as a connection—
(s1, s4): repulsive connection due to orientation.
where τi j is the orientation difference between si and s j, δmin is the minimum deviation from a right
angle for which two segments are considered as badly oriented, and c is the minimum difference between
the orientations that can exist in the conﬁguration.

320
CHAPTER 11 Markov Models and MCMC Alogorithms in Image Processing
To summarize, the prior density is then deﬁned as follows:
f (S) ∝exp
⎡
⎣−n f ω f −nsωs −

r∈R
ωr

<si,s j>r
Vr(si, s j)
⎤
⎦,
(11.55)
where :
⎧
⎨
⎩
n f , ns are the number of free and single segments,
<, >r represents a segment pair in interaction via the relationr,
Vr(·, ·) is a potential function.
As for the trees example we consider the data term as a sum of local terms:
g(S) ∝exp
⎡
⎣−γd

si∈S
δi
⎤
⎦,
(11.56)
where δi is a statistical value that is calculated over the region corresponding to pixels of segment si
and of its neighborhood. γd is a positive weighting coefﬁcient.
The potential δi is based on two assumptions concerning the segment and its neighborhood. We
assume, ﬁrst of all, that adjacent regions are different, at least on average, to the region of the segment
(see Figure 11.17 top). Then, the set of pixels of a segment must be homogeneous, to avoid the contours
being detected as roads (see Figure 11.17 bottom).
To check these assumptions, for a given segment, we divide it into several bands, b1, . . . , bn (note
that by the term segment we mean an elongated rectangle in the discrete space). In addition, we consider
two bands on each side, Ri
1 and Ri
2, at a distance d from the segment, as illustrated in Figure 11.18.
The values of the pixels in each band constitute a population. A Student t-test is calculated to
determine if the averages of two populations are signiﬁcantly different. For two populations x and y,
we thus calculate the following:
t-test(x, y) =
|¯x −¯y|
'
˜σ 2x
nx +
˜σ 2y
ny
,
(11.57)
where ¯x, ˜σ 2, and n refer to the mean, the variance, and the number of observations respectively. We
would like a signiﬁcant difference between the segment and its two sides, so we consider the statistical
test (H1) deﬁned by the minimum of the two tests:
TH1(si) = min
l∈{1,2}[t-test(Ri
l , si)].
(11.58)
The assumption of homogeneity H2 is given by the maximum value of the tests between two adjacent
bands within the segment:
TH2(si) =
max
j∈{1,...,nb−1}[t-test(b j, b j+1)].
(11.59)
Finally, the selected potential is the ratio of these two quantities, while taking the precaution to under-
value TH2(si):
Ti =
TH1(si)
max[1, TH2(si)].
(11.60)

4.11.5 Multiple Objects Detection
321
FIGURE 11.17
H1—signiﬁcant difference with adjacent zones (top), H2—homogeneity of the segment (bottom).
Moreover, a sigmoid function makes it possible to convert values from [0, ∞] into [−1, 1]. Thus, we
have:
δi =
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
1
if
Ti < t1,
1 −2 Ti−t1
t2−t1
if t1 < Ti < t2,
−1
if
Ti > t2,
(11.61)
where t1 and t2(t1 < t2) are two thresholds that parameterize the data term.
The optimization is performed using an RJMCMC algorithm. The proposition kernel includes birth
and death, segment translation and rotation. A speciﬁc movement, which speed up the process, consists
in deﬁning a birth kernel where the new segment is proposed in order to be connected with a segment
lying in the current conﬁguration. This movement intuitively induces a tracking of connected chains of
segments. The reverse movements consisting in removing a connected segment has also to be included
in the proposition kernel.
The RJMCMC algorithm is integrated in a simulated annealing framework for tests which are carried
out on real data. Figure 11.20 shows that the marked point process prior allows the network detection
even on the shadow part of the road, where there is a lack of contrast between the road and the border.

322
CHAPTER 11 Markov Models and MCMC Alogorithms in Image Processing
R2
is
R1
b
b
b
1
2
3
d
FIGURE 11.18
Division of a segment into several bands.
FIGURE 11.19
Test 2: aerial image (892 × 652 pixels).
4.11.6 Conclusion
Stochastic modeling belongs to the history of image processing and computer vision. The major advan-
tage of these models is their ability to embed prior information on the solution. They are therefore a very
powerful tool for solving inverse problem which are often ill posed and for which the data themselves

References
323
FIGURE 11.20
Results of extraction of the road network in the image from Figure 11.19 for the continuous potentials Candy
model (from [41]).
may be not sufﬁcient to provide a robust solution. Another key point on favor of probabilistic modeling
is the algorithm corpus available for simulating, optimizing and estimating. They sometimes suffer from
a bad reputation concerning their resources needs. However, the recent development in algorithmic and
the evolution of computer performances is usually a good answer to these criticisms. Initially dedicated
to pixelwise modeling, they follow the recent development of sensors. New models have been devel-
oped to take into account for the increasing resolution of data, embedding geometrical information.
Stochastic models still have a large future for analyzing images.
Relevant theory: Machine Learning
See Vol. 1, Chapter 18 Introduction to Probabilistic Graphical Models
See Vol. 1, Chapter 19 A Tutorial Introduction to Monte Carlo Methods, Markov Chain Monte Carlo
and Particle Filtering
References
[1] J. Besag, Spatial interaction and the statistical analysis of lattice systems, J. Roy. Stat. Soc. Ser. B 36 (2)
(1974) 192–236.
[2] G. Cross, A. Jain, Markov random ﬁeld texture models, IEEE Trans. Pattern Anal. Mach. Intell. 5 (1) (1983)
25–39.
[3] S. Geman, D. Geman, Stochastic relaxation, Gibbs distribution, and the Bayesian restoration of images, IEEE
Trans. Pattern Anal. Mach. Intell. 6 (6) (1984) 721–741.
[4] X. Descombes (Ed.), Stochastic Geometry for Image Analysis, Wiley/Iste, 2011.
[5] R.L. Dobrushin, The description of a random ﬁeld by means of conditional probabilities and conditions of its
regularity, Theory Probab. Appl. 13 (2) (1968) 197–224.
[6] C. Robert, G. Casella, Monte Carlo Statistical Methods, second ed., Springer-Verlag, 2004.

324
CHAPTER 11 Markov Models and MCMC Alogorithms in Image Processing
[7] W.K. Hastings, Monte Carlo sampling methods using Markov chains and their applications, Biometrika 57 (1)
(1970) 97–109.
[8] S. Kirkpatrick, C.D. Gelatt, M.P. Vecchi, Optimization by simulated annealing, Science 220 (1983) 671–680.
[9] B. Hajek, Cooling schedules for optimal annealing, Math. Oper. Res. (1987) (Preprints).
[10] Y. Boykov, O. Veksler, Graph cuts in vision and graphics: theories and applications, in: Handbook of Mathe-
matical Models in Computer Vision, Springer, 2006.
[11] V. Kolmogorov, R. Zabih, What energy functions can be minimized via graph cuts? IEEE Trans. Pattern Anal.
Mach. Intell. 26 (2) (2004) 147–159.
[12] R. Chellappa, A. Jain (Eds.), Markov Random Fields: Theory and Applications, Academic Press, 1993.
[13] S. Li, Markov Random Field Modeling in Computer Vision, second ed., Springer-Verlag, 2001.
[14] G. Winkler, Image analysis, random ﬁelds and Markov Chain Monte Carlo methods, in: A Mathematical
Introduction, Springer, NY, 2003.
[15] D.Geman,Randomﬁeldsandinverseproblemsinimaging,LectureNotesinMathematics,vol.1427,Springer-
Verlag, 1991, pp. 113–193.
[16] S. Geman, G. Reynolds, Constrained restoration and recovery of discontinuities, IEEE Trans. Pattern Anal.
Mach. Intell. 14 (3) (1992) 367–383.
[17] M. Nikolova, Analysis of the recovery of edges in images and signals by minimizing non convex regularized
least-squares, SIAM J. Multiscale Model. Simul. 4 (3) (2005) 960–991.
[18] X. Descombes, M. Lebellego, E. Zhizhina, Image deconvolution using a stochastic differential equa-
tion approach, in: Proceedings of International Conference on Computer Vision Theory and Applications,
Barcelona, Spain, 2007.
[19] R.D. Morris, X. Descombes, J. Zerubia, The Ising/Potts model is not well suited to segmentation tasks,
in: Proceedings of Digital Signal Processing Workshop IEEE, Loean, Norway, 1996.
[20] G. Wolberg, T. Pavlidis, Restoration of binary images using stochastic relaxation with annealing, Pattern
Recogn. Lett. 3 (1985) 375–388.
[21] H. Tjelmeland, J. Besag, Markov Random Fields with higher-order interactions, Scand. J. Stat. 25 (3) (1998)
415–433.
[22] X. Descombes, J.-F. Mangin, E. Pechersky, M. Sigelle, Fine structures preserving Markov model for image
processing, in: Proceedings of 9th Scandinavian Conference on Image Analysis, 1995.
[23] X. Descombes, R.D. Morris, J. Zerubia, M. Berthod, Estimation of Markov Random ﬁeld prior parameters
using Markov chain Monte Carlo Maximum Likelihood, IEEE Trans. Image Process. 8 (7) (1999) 954–963.
[24] X. Descombes, M. Sigelle, F. Préteux, GMRF parameter estimation in a non-stationary framework by a
renormalization technique: application to remote sensing imaging, IEEE Trans. Image Process. 8 (4) (1999)
490–503.
[25] G.L. Gimel’farb, Image textures and Gibbs Random Fields, Springer, 1999.
[26] A.Lorette,X.Descombes,J.Zerubia,TextureanalysisthroughaMarkovianmodelingandFuzzyclassiﬁcation:
application to urban area extraction from satellite images, Int. J. Comput. Vis. 36 (3) (2000) 221–236.
[27] F. Tupin, H. Maitre, J.-F. Mangin, J.-M. Nicolas, E. Pechersky, Detection of linear features in SAR images:
application to road network extraction, IEEE Trans. Geosci. Remote Sens. 36 (2) (1998) 434–453.
[28] M.N.M. Van Lieshout, Markov Point Processes and Their Applications, Imperial College Press, London,
2000.
[29] D. Stoyan, W.S. Kendall, J. Mecke, Stochastic Geometry and Its Applications, second ed., John Wiley & Sons,
1987.
[30] P.J. Green, Reversible jump Markov chain Monte Carlo computation and Bayesian model determination,
Biometrika 82 (4) (1995) 711–132.
[31] C.J. Geyer, J. Moller, Simulation and likelihood inference for spatial point process, Scand. J. Stat. B 21 (1994)
359–373.

References
325
[32] X. Descombes, R. Minlos, E. Zhizhina, Object extraction using a stochastic birth-and-death dynamics in
continuum, J. Math. Imag. Vis. 33 (3) (2009) 347–359.
[33] A. Gamal Eldin, X. Descombes, G. Charpiat, J. Zerubia, Multiple birth and cut algorithm for multiple object
detection, J. Multimedia Process. Technol. (2011).
[34] G. Perrin, X. Descombes, J. Zerubia, A marked point process for tree crown extraction in plantations,
in: Proceedings of International Conference on Image Processing, Genoa, Italy, 2005.
[35] X. Descombes, Interacting adaptive ﬁlters for multiple objects detection, in: L. Florack, R. Duits, G. Jongbloed,
M.N.M. Van Lieashout (Eds.), Mathematical Methods for Signal and Image Analysis and Representation,
Springer-Verlag, 2012 (Chapter 13).
[36] D. Geman, B. Jedynak, An active testing model for tracking roads from satellite images, IEEE Trans. Pattern
Anal. Mach. Intell. 18 (1) (1996) 1–14.
[37] M. Barzohar, D.B. Cooper, Automatic ﬁnding of main roads in aerial images by using Geometric-Stochastic
models and estimation, IEEE Trans. Pattern Anal. Mach. Intell. 18 (7) (1996) 707–721.
[38] R. Stoica, X. Descombes, J. Zerubia, A Gibbs process for road extraction in remotely sensed images, Int. J.
Comput. Vis. 57 (2) (2004) 121–136.
[39] C. Lacoste, X. Descombes, J. Zerubia, Point processes for unsupervised line network extraction in remote
sensing, IEEE Trans. Pattern Anal. Mach. Intell. 27 (10) (2005) 1568–1579.
[40] G. Perrin, Etude du couvert forestier par processus ponctuels marqués, Ph.D. Thesis, Ecole Centrale Paris,
October 2006.
[41] C. Lacoste, Extraction de Réseaux Linéiques à partir d’Images Satellitaires et Aériennes par Processus
Ponctuels Marqués, Ph.D. Thesis, Université de Nice Sophia Antipolis, September 2004.

12
CHAPTER
Identifying Multivariate Imaging
Patterns: Supervised,
Semi-Supervised, and
Unsupervised Learning
Perspectives
Roman Filipovych, Bilwaj Gaonkar, and Christos Davatzikos
Section of Biomedical Image Analysis, Department of Radiology, University of Pennsylvania,
Philadelphia, PA, USA
4.12.1 Introduction
Morphological analysis of medical images has been used in a variety of research and clinical studies
that investigate the effect of diseases and treatments on anatomical structures. In order to identify
brain structures associated with a disease, traditional neuroimaging approaches analyze target (patient)
population with respect to a control group, or to another patient population. Analysis of group differences
has often been performed with the help of univariate statistical methods (e.g., voxel-based morphometry
(VBM)). Unfortunately, mass-univariate statistical methods may not be applicable in whole-brain MRI
studies with relatively low sample size due to the very high dimensionality of the data. More importantly,
mass-univariate group-comparison methods have very limited diagnostic power, since in every single
region detected by these methods there is typically a signiﬁcant overlap between healthy individuals
and patients.
On the other hand, high-dimensional pattern classiﬁcation has gained signiﬁcant attention in recent
years, and represents a promising technique for capturing complex spatial patterns of pathological brain
changes. Importantly, pattern classiﬁcation methods have begun to provide tests of high sensitivity
and speciﬁcity on an individual patient basis, in addition to characterizing group differences. As the
result, these methods can potentially be used as diagnostic and prognostic tools. Pattern classiﬁcation
approaches were shown to work particularly well in the task of classifying patient populations from
normal cohort in various clinical studies (e.g., Alzheimer’s [1–6], autism [7], schizophrenia [2–4], etc.).
The state-of-the-art brain image classiﬁcation methods work by learning a classiﬁcation function from
a set of labeled training examples, and then apply the learned classiﬁer to predict labels of the test
data. In most cases these approaches assume the availability of two distinct populations (e.g., healthy
individuals and patients), and aim at categorizing a novel image into one of the two groups. These
methods belong to the family of supervised classiﬁcation approaches and assume that the labels for all
training data are available.
Typical classiﬁcation methods work under the assumption that the nosological boundaries between
the conditions are distinct and well-deﬁned, which may not always be the case. In various studies,
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00012-1
© 2014 Elsevier Ltd. All rights reserved.
327

328
CHAPTER 12 Identifying Multivariate Imaging Patterns
populations of patients are highly heterogeneous, and categorization of subconditions within many
diseases is yet to be established. Additionally, a number of diseases are characterized by pathologies
that form continuous or nearly-continuous spectra spanning from complete absence of pathology to very
pronounced pathological changes. Autism Spectrum Disorder (ASD), schizophrenia, mild cognitive
impairment (MCI) are but a few examples of conditions where pathologies are highly heterogeneous,
as well as form continuous spectra:
1. Autism Spectrum Disorder (ASD): ASD is a Pervasive Developmental Disorder (PDD) character-
ized by deﬁcits in language, presence of stereotypic or repetitive behaviors, and social impairments
[8,9]. While the boundaries between ASD, its comorbidities, and healthy state are blurred, sev-
eral diagnostic subcategories within ASD were deﬁned: Autism, Asperger’s disorder, and Pervasive
Developmental Disorder-Not Otherwise Speciﬁed (PDD-NOS). At the same time, it has been often
argued that the Asperger disorder criteria do not work in the clinic [10]. As the result, there has been
an intense debate as to the distinctiveness of Asperger disorder from other subgroups within the
autism spectrum [11]. Appropriate computational tools would allow assessing whether Asperger’s
disorder forms a distinct subgroup within the autism spectrum with respect to, for example, imaging
phenotypes.
2. Schizophrenia: There is an intense debate in the schizophrenia research community between the pro-
ponents of a categorical disease classiﬁcation system vs. the supporters of a dimensional/syndromal
perspective because of (1) the lack of evidence for decisive biological boundaries and (2) the consid-
erable similarities in the clinical presentations and courses of schizophrenic and affective psychoses
[12–16]. The comparative genetic studies suggest that a considerable degree of neurobiological het-
erogeneity within the psychiatric diseases blurs the established diagnostic divisions. This hypothesis
hasbeensupportedbyﬁndingsof(1)distinctneuroanatomical(endo)phenotypesunderlyingdifferent
psychopathological symptom dimensions in schizophrenia [17], (2) a signiﬁcant sexual dimorphism
in the structural abnormalities associated with the disorder, and (3) a progression of structural brain
abnormalities during the course of the disease [18]. At the same time, the early prodromal state
of the disease largely overlaps with depressive syndromes [19] and nonspeciﬁc psychopathological
phenomena found in the general population [20].
3. Aging: It was shown in the studies of aging that cognitive performance of some older adults may
declinegraduallyatdifferentrates,whilesomemayremainstableforalongperiodoftime[21].More-
over, some may develop MCI, or even proceed to developing Alzheimer’s disease (AD). Additionally,
as normal aging increases cognitive heterogeneity [22,23], there is a need to identify neuroimaging
proﬁles that characterize cognitive differences in aging populations.
In this chapter, we provide a summary of selected works that address the task of identifying descrip-
tive imaging patterns from different learning perspectives, including supervised, semi-supervised, and
unsupervised learning paradigms. We ﬁrst discuss the potential of fully supervised machine learning in
the task of providing computational image-based markers of diseases. We do not attempt to be thorough
in our presentation of available methods, but rather use example works to highlight the range of prob-
lems a typical fully-supervised methodology is designed to address. We then describe a semi-supervised
learning solution to the problem of designing predictive models when there is a lack of reliable diag-
nostic information. Finally, we discuss tools that rely on unsupervised learning techniques to perform

4.12.3 Supervised learning of predictive models
329
exploratory analysis of heterogeneous conditions. The application focus of this paper is on the studies
of aging, Alzheimer’s disease, mild cognitive impairment, and schizophrenia.
4.12.2 Materials
The data used in the papers, discussed below as examples of the methodologies, are from the Alzheimer’s
disease Neuroimaging Initiative (ADNI), and from the Baltimore Longitudinal Study of Aging (BLSA).
The goal of ADNI (www.loni.ucla.edu/ADNI) is to recruit 800 adults, ages 55–90, to participate in the
research: approximately 200 normal control older individuals to be followed for 3 years, 400 people
with mild cognitive impairment (MCI) to be followed for 3 years, and 200 people with early AD to
be followed for 2 years. For up-to-date information see www.adni-info.org. In the ADNI, a subset of
MCI subjects was diagnosed with AD during the study, i.e., MCI converters (MCI-C), while a vast
majority of MCI subjects did not exhibit change in diagnosis during the follow-up period, i.e., MCI
non-converters (MCI-NC).
One of the current limitations of ADNI is its rather short follow-up evaluations period. On the
other hand, BLSA [24] is a prospective longitudinal study of aging, with the neuroimaging component
currently in its 16th year. BLSA has followed 158 individuals (age 55–85 years at enrollment) with
annual or semi-annual imaging and clinical evaluations. The neuroimaging sub-study of the BLSA is
described in detail in [24].
All analyzes detailed in this chapter were performed on T1-weighted structural MR images. The
ADNI data included standard T1-weighted images obtained using volumetric 3D MPRAGE or equiva-
lent protocols with varying resolutions (typically 1.25 × 1.25 mm in-plane spatial resolution and 1.2 mm
thick sagittal slices). Only images obtained using 1.5 T scanners were used. The sagittal images were
preprocessed according to a number of steps detailed under the ADNI website, which corrected for ﬁeld
inhomogeneities and image distortion, and were resliced to axial orientation. The BLSA T1-MR data
was acquired on a 1.5 T GE scanner with the following parameters: 2 × NEX (two signal averages),
repetition time = 24 ms, echo time = 5 ms, ﬁeld of view = 30 cm. The data was acquired sagittally with
slice thickness of 1.2 mm, pixel resolution of 1.2 mm × 1.2 mm and acquisition matrix size 256 × 256.
Image processing: All MR images were preprocessed following mass-preserving shape transformation
framework [25]. Gray matter tissue (GM), white matter (WM), and cerebrospinal ﬂuid (CSF) were
segmented out from each skull-stripped MR brain image by a brain tissue segmentation method [26].
Each tissue-segmented brain image was spatially normalized into a template space, by using a high-
dimensional image warping method [27]. The total tissue mass was preserved in each region during
the image warping, and tissue density maps were generated in the template space. These tissue density
maps give a quantitative representation of the spatial distribution of tissue in a brain, with brightness
being proportional to the amount of local tissue volume before warping.
4.12.3 Supervised learning of predictive models
In order to make MRI useful for diagnostic and prognostic purposes there is a need to design computa-
tional analytic tools that have high sensitivity and speciﬁcity. The task of using population wide group

330
CHAPTER 12 Identifying Multivariate Imaging Patterns
FIGURE 12.1
The paradigm of two-class categorization used by COMPARE and related methods.
difference patterns to make accurate disease diagnosis falls into the realm of pattern classiﬁcation. The
simplest form of pattern classiﬁcation is based on a two-class categorization paradigm (Figure 12.1)
where a single pattern associated with the condition is learnt from labeled data. The presence of this
pattern in a test subject’s brain image reﬂects the presence of the disease condition. Following this
two-class categorization paradigm, we developed a whole-brain MRI classiﬁcation approach COM-
PARE [28] (available upon request from www.rad.upenn.edu/sbia) that identiﬁes a set of brain regions
whose volumes jointly maximally differentiate between the two groups on an individual scan basis. After
the two-class classiﬁer is estimated, the pattern classiﬁcation method provides a structural phenotypic
score for every new subject, which reﬂects the degree to which the disease-like pathology is present in
the subject’s brain. Positive scores indicate the presence of the disease-like phenotypic pattern in the
brain, while negative scores indicate normal structure.
It has to be mentioned that while in this chapter we discuss the predictive potential of the supervised
classiﬁcation using applications of COMPARE as our examples, a number of alternative classiﬁcation
approaches have been proposed in the recent years. A review and a benchmark comparison of the
state-of-the-art two-class classiﬁcation methods can be found in [29].
We applied COMPARE in the context of several diseases and successfully used it with data obtained
via several imaging modalities. In particular, we demonstrated the potential of the supervised classiﬁ-
cation in a study of schizophrenia using T1-MR brain images [28]. We evaluated two models on the
following datasets: (a) female subjects, with 23 schizophrenia (SC) patients and 38 normal controls
(NC) and (b) male subjects, with 46 SC and 41 NC. The models were evaluated using the leave one
out cross-validation (LOOCV) procedure. Each step of this procedure involves leaving one subject out,
training the algorithm on all remaining subjects, and using the trained model to predict the label of the
left out subject. The performance of the learning algorithm was measured in terms of the percentage
of labels correctly predicted during the leave one out procedure. In the task of identifying patients with
schizophrenia in a female population COMPARE achieved the accuracy of 90.2%. The accuracy of
discriminating between controls and patients in the population of males was equally high (i.e., 90.8%).
It needs to be emphasized that the high accuracy of identifying patients was achieved when using only

4.12.4 Semi-supervised Learning of Predictive Models
331
the neuroimaging information. One can expect that by incorporating additional clinical, demographic
and genetic data it would be possible to improve the automatic prediction even further.
We also successfully applied COMPARE to the problem of detecting phenotypic patterns associated
with Alzheimer’s disease within the Alzheimer’s disease neuroimaging initiative study [2–4]. The
COMPARE model discriminative between AD and control populations was trained using 66 control
individuals (mean age ± SD, 75.18 ± 5.39) and 56 AD patients (77.40 ± 7.02 years). The classiﬁer
achieved an LOOCV accuracy of 94.3% in classifying the AD patients from normal controls. In order
to test the prognostic ability of the algorithm, we used the trained classiﬁer to classify a set of patients
clinically diagnosed as MCI (mild cognitive impairment—the prodromal form AD), some of whom
progressed to AD over the course of the study. We found that MCI subjects that were marked by the
classiﬁer as having the AD-like pattern of pathology at baseline had a statistically signiﬁcantly higher
rate of conversion to AD in the follow-up period.
4.12.4 Semi-supervised learning of predictive models
4.12.4.1 Support vector machines (SVM): supervised vs. semi-supervised
classiﬁcation
In the two-class classiﬁcation scenario, the task of classifying images into two classes (e.g., patients vs.
controls) can be viewed as the task of ﬁnding a decision function that separates the two classes in a high-
dimensional space. Traditional linear SVM algorithm [30] ﬁnds this decision function as the separating
hyperplane with the largest margin, where the margin is the distance from the separating hyperplane to
the closest training examples. Given a set of high-dimensional points (i.e., images) X = {x1, . . . , xn}
and their respective labels Y = {y1, . . . , yn}, the task of ﬁnding a separating, i.e., classiﬁcation, function
f (x) = wT x + b within the framework of traditional linear SVM could be formulated as the following
optimization problem:
min
w,b,ξ
1
2wT w + β
n

i=1
ξi,
s.t.
yi(wT xi + b) ≥1 −ξi,
∀i = 1, . . . , n,
ξi ≥0,
∀i = 1, . . . , n,
where the slack variables ξi are introduced to allow some amount of misclassiﬁcation in the case of
non-separable classes, and constant β implicitly controls the tolerable misclassiﬁcation error.
Figure 12.2a shows a simpliﬁed example of designing a classiﬁer between AD and controls. Training
examples that lie on the margin deﬁne the decision boundary and are called support vectors. A more
detailed description of the SVM can be found in [31]. Notice, that the supervised SVM formulation
assumes that labels for all training points are available during training.
Semi-supervised SVM, originally introduced as Transductive SVM (TSVM) [32], build upon the the-
oryofSVMandconsiderpartiallylabeleddatasets.GivenasetofpointsX = {x1, . . . , xl, xl+1, . . . , xn},
where the ﬁrst l points in X are labeled as yi ∈{−1, +1} and the labels y j ∈{−1, +1} of the remaining

332
CHAPTER 12 Identifying Multivariate Imaging Patterns
(a)
(b)
FIGURE 12.2
Examples of supervised and semi-supervised learning. (a) Supervised learning of a classiﬁer between AD and
controls based solely on the labeled information. (b) Semi-supervised learning of a classiﬁer between MCI-
converters and MCI-non-converters where the labeled and unlabeled data are used to build the predictive
model.
u = n −l points are unknown, the task of ﬁnding a separating function within the framework of
semi-supervised SVM could be formulated as follows:
min
yl+1,...,yn
min
w,b,ξ
1
2wT w + βl
l+1

i=1
ξi + βu
n

j=l+1
ξ j,
s.t.
yi(wT xi + b) ≥1 −ξi,
∀i = 1, . . . ,l,
y j(wT x j + b) ≥1 −ξ j,
∀j = l + 1, . . . , n,
ξi ≥0,
ξi ≥0,
∀i = 1, . . . ,l,
∀j = l + 1, . . . , n,
where constants βl and βu reﬂect prior conﬁdence in labels (y1, . . . , yl) and in the separability of the
unlabeled data points, respectively. Figure 12.2b shows an example of semi-supervised classiﬁer that,
in addition to the limited labeled information regarding the conversion from MCI to AD, uses unlabeled
MCI subjects to build a more accurate classiﬁer between MCI-converters and MCI-non-converters.
4.12.4.2 Predicting conversion from MCI to AD via
semi-supervised classiﬁcation
The task of predicting short term conversion to AD from MCI has been addressed in the past with the help
of fully supervised techniques that aim at deducing a decision function from a set of labeled images (e.g.,
normal control, AD, MCI-converters, etc.) [2–6,33]. However populations of individuals with MCI are
highly heterogeneous. Previous studies suggest that some MCIs are close to AD and will convert soon,

4.12.4 Semi-supervised Learning of Predictive Models
333
whereas some will remain stable for over a decade. Moreover, while some individuals with MCI may
convertatafasterratethanotherstoAD,somewillneverdevelopADandothersmaydevelopotherforms
of dementia. At the same time, some individuals might be labeled with relatively higher reliability. For
example, AD patients are undoubtedly converters, as well as normal control subjects are non-converters.
Semi-supervised SVM do not make use of uncertain labels when building a classiﬁcation function, but
rather attempt to separate unlabeled data into two classes in such a way that the heterogeneity of the
data is disentangled, and that the classiﬁer agrees with the reliably labeled part of the data. As the result,
classiﬁcation of MCI populations is likely to beneﬁt from the semi-supervised SVM.
In [34], we assessed the potential of the semi-supervised learning, and in particular of the semi-
supervised SVM. In our semi-supervised classiﬁcation study using ADNI data [34] we considered 63
normal individuals (CN) (age range: 75.18 ± 5.39), 54 AD patients (77.40 ± 7.02), and 242 MCI
participants (age range: 74.99 ± $7.38), of which 68 (MCI-C, 76.22 ± 7.20) were classiﬁed as having
undergone conversion to AD based on changes in Global CDR from 0.5 to 1. The remaining 174 MCI
participants (MCI-NC: 74.49 ± 7.41) were classiﬁed as non-converters. A detailed list of the subjects
can be found in [34].
Treating the images of all available control and AD subjects as labeled data, and MCI participants as
unlabeled data, we obtained predicted labels of the MCI subjects within the semi-supervised framework.
As the result, 79.4% of all converters were classiﬁed by the method as AD-like. The remaining 21.6%
of converters were classiﬁed as normal-like. At the same time, 51.7% of non-converters were classiﬁed
as normal-like, and the remaining 48.3% of non-converters were classiﬁed as AD-like. The area under
the ROC curve (AUC) for the MCI-C/MCI-NC classiﬁcation was 0.69. Large number of AD-like non-
converters may indicate that many of the non-converters will actually convert to AD in the future.
To further show the potential of the semi-supervised classiﬁcation in the scenarios where the labeled
information is unreliable, we considered only MCI subjects from ADNI, and formed labeled sets using
the subjects that corresponded to the extreme values of rates of change in cognitive evaluations. More
speciﬁcally, out of the 68 MCI-C subjects we selected 20 subjects with the lowest slopes of the Mini-
Mental State Examination (MMSE) score to represent the positive labeled subset. Similarly, out of the
174 MCI-NC subjects we selected 20 subjects with the highest slopes of the MMSE score to represent
the negative labeled subset. The remaining 202 MCI subjects were considered to be unlabeled. Notice,
that while we considered only the baseline scans, cognitive evaluations at the follow-ups were used to
calculatetheslopes(i.e.,ratesofchange)ofMMSEscores.ThemeanMMSEslopeinthepositivelabeled
subset was −3.73 ± 1.54, and the mean MMSE slope in the negative labeled subset was 1.36 ± 1.03.
After performing the semi-supervised classiﬁcation of the unlabeled MCI subjects we found that the
AUC of the MCI-C/MCI-NC classiﬁer was 0.69. At the same time, a fully supervised classiﬁer trained
on the selected labeled subsets, and applied to the unlabeled data resulted in AUC equal 0.61. Given
that an AUC of 0.5 represents classiﬁcation by chance, the improvement provided by semi-supervised
approach over the fully-supervised method was quite signiﬁcant.
Overall, the MCI-C/MCI-NC classiﬁcation problem is characterized by exceptionally low separabil-
ity and difﬁculty, and predicting short-term cognitive decline from baseline scans is bound to be very
limited, albeit it is largely improved by semi-supervised classiﬁcation. Moreover, MMSE scores are
very noisy and are not sufﬁciently good indicators of conversion to AD. As the result, the labels of the
subjects in the labeled subsets were uncertain, which can hamper the performance of the fully-supervised

334
CHAPTER 12 Identifying Multivariate Imaging Patterns
classiﬁer. At the same time, the availability of the unlabeled data allows the semi-supervised classiﬁer
to better learn the manifold of MCI subjects, and hence to build a more reliable separation function. On
the other hand, if the labeled sets are formed from AD and CN subjects, the labels of the subjects in the
labeled subsets are much more reliable, and given a sufﬁciently large number of labeled subjects it is
possible to obtain a fully-supervised classiﬁer that performs on a par with the semi-supervised approach.
4.12.5 Unsupervised learning as the means to
disentangle heterogeneity
While the two-class (or multi-class) categorization approaches can provide highly sensitive and speciﬁc
individualized biomarkers of AD, they assume a simplistic picture of the disease. These methods attempt
to categorize subjects at a very high level (i.e., AD, MCI, normal), and are oblivious to the details of
the underlying heterogeneity. At the same time, different subconditions may exist within the disease,
and potentially may require different treatment options. Therefore, in order to ﬁnd multiple patterns of
pathology associated with a disease condition there is a need to break away from the traditional two
class paradigm and allow for more than one pathological proﬁle to be associated with a speciﬁc disease.
Below, we provide examples of several approaches that instead of focusing on identifying patterns that
are characteristic of the multivariate differences between two subpopulations, target the problem of
recovering neuroimaging patterns that characterize the heterogeneity associated with diseases.
4.12.5.1 Exploring disease heterogeneity: a sparse dictionary learning
based approach
In [35], we described a dictionary learning approach to the image-based analysis of heterogeneity. The
underlying idea behind dictionary learning is in the assumption that signals generated in nature (for
instance brain images) have a signiﬁcant common statistical structure and can therefore be represented
as a combination of a set of simpler signals (basis signals). Consequently, the task of dictionary learning
algorithms is to ﬁnd this set of basis signals (dictionary) from a given set of signals (data), where each
element of this set of basis signals is called a dictionary element. The problem of trying to represent a
single natural signal using a dictionary usually has multiple solutions. One criterion for choosing the
best solution is that of sparsity which is supported by the evidence of sparse coding in many biological
systems. A sparse dictionary methods attempt to use as few dictionary elements as possible to represent
eachsignal.Suchasparserepresentationencouragesasimplerepresentationofthesignalbeingmodeled.
Our approach [35] follows a sparse dictionary learning paradigm and allows associating multiple
patterns of deﬁcit with a single disease. To achieve this, we ﬁrst compute difference images by voxel-wise
subtraction of tissue density maps of patients and controls. The difference images are then stacked into a
matrix M ∈Rd×n, where d is the number of voxels in each image and n is the total number of difference
images generated. We then decompose the matrix X via a speciﬁc method of matrix factorization. We
ﬁnd matrices B ∈Rd×k and C ∈Rk×n such that M ≈BC (Figure 12.3). The matrix B is known as
the basis matrix and its columns contain patterns of interest associated with disease. Note that k, the
number of columns in B, can be set to be more than one. This allows exploration of the cases when
more than one pattern of deﬁcit characterizes a particular disease state.

4.12.5 Unsupervised Learning as the Means to Disentangle Heterogeneity
335
FIGURE 12.3
Dictionary learning approach to medical image analysis.
To illustrate how dictionary learning works on imaging data we simulated a heterogeneous disease
by deliberately introducing either of two separate patterns of atrophy in image sets derived from a
normal population (Figure 12.4). A separate set of normal images was used as a control population.
Figure 12.4 shows patterns of deﬁcits detected by the sparse dictionary learning algorithm. Obviously,
the two distinct neuroimaging proﬁles in the pathological population could not have been identiﬁed
via an analysis of univariate differences between the normal and pathological group if the pathological
subgroups were not provided in advance.
Additionally, Figure 12.5 shows patterns associated with AD that were discovered using sparse
dictionary learning. The algorithm identiﬁed three main patterns of deﬁcits in the AD population as
compared to the reference population. The ﬁrst pattern contained the hippocampus, the brain region
believed to be affected among the ﬁrst in AD. The second pattern shows atrophy of the hippocampus
and parietal areas, the areas known to be associated with advanced AD. Finally, the third pattern shows
greater involvement the prefrontal cortex (PFC), consistent with the clinical evidence suggesting that
AD patients with PFC atrophy decline much faster than those without it [36].
4.12.5.2 Understanding heterogeneity of normal aging via clustering
In [37], we described a clustering-based method that is speciﬁcally designed to ﬁnd coherent sub-
populations within heterogeneous distributions. The method involves little supervision at the stage of
dimensionality reduction, and employs a hierarchical clustering technique to automatically group MR
images of healthy older adults with respect to the underlying brain pathology. After analyzing a set
of 875 scans of healthy normal adults from the BLSA study using our approach we discovered that
the most cognitively stable subjects form a small, but extremely coherent subpopulation (i.e., cluster).
At the same time, all cognitively less stable individuals form a less coherent and dispersed cluster,
which in turn can be viewed as a denser relatively cognitively stable subpopulation, and even more
dispersed cognitively less stable subpopulation. The results suggest that the population of subjects with
relatively better cognitive performance is more densely distributed than the population of less normal
brain images. Overall, the clustering approach indicates that a population of normal adults consists of

336
CHAPTER 12 Identifying Multivariate Imaging Patterns
FIGURE 12.4
Sparse dictionary learning on simulated data. Ground truth patterns (top); detected patterns (bottom).
two subpopulations, one of which is a small, yet coherent, subpopulation of cognitively more stable
individuals. The other subpopulation is much more heterogeneous and on average less cognitively sta-
ble. The cognitively less stable subpopulation in turn also consists of two subgroups of relatively more
stable and relatively less stable adults, and so on. These results suggest that at the very early stages,
cognitive performance degrades in a continuous manner, without going through any distinct phases.
Additionally, we explored ways of using discovered subpopulations to quantify the severity of brain
pathology in individual brains [37]. Unlike the common two-class categorization paradigm that typically
assumes the availability of patient and normal populations, all individuals in the BLSA study are normal,
and no prior categorization of the population is available. Nonetheless, we showed that it is possible to
design an individualized marker that reﬂects subject’s cognitive decline. Given a cluster of images of
cognitively less stable subjects CP, and a cluster of cognitively more stable individuals CN, we deﬁned
a measure of the level of pathology for an image I as follows:
D = d(I, CN) −d(I, CP),
where d(I, CN) and d(I, CP) are the distances between I and the centers of more stable and less stable
clusters, respectively. As the result, individuals with better cognitive performance are expected to have
lower level of pathology, and vice versa.
We found that there is a very strong relationship between the proposed measure and cognitive
performance. The subpopulations of healthy older adults that correspond to the extreme quartiles of the
pathology measure (i.e., subjects with the level of pathology in the upper 25%, and in the lower 25%,

4.12.5 Unsupervised Learning as the Means to Disentangle Heterogeneity
337
FIGURE 12.5
Dictionary learning based morphometric analysis of ADNI data reveals three distinct disease patterns,
hippocampi only (top), hippocampi + parietal-temporal regions (middle), hippocampi + prefrontal cortex
(bottom).
respectively) showed signiﬁcantly different cognitive performance with respect to most cognitive tests.
Table 12.1 shows group differences in the Mini-Mental State Exam [38] that assesses mental status,
the immediate free recall score (sum of ﬁve immediate recall trials) on California Verbal Learning Test
(CVLT) [39], and the long-delay free recall score on CVLT, that assess verbal learning and immediate
and delayed recall, and the total number of errors from the Benton Visual Retention Test (BVRT) [40]
that assesses short-term visual memory.
Additionally, we did not observe signiﬁcant age difference between the lower and the upper quartile
groups (p = 0.322), which suggested that the method captures pathology that is not solely induced by
age. Overall, our results suggest that clustering-based pattern recognition allows us to get a better under-
standing of the underlying heterogeneity, and at the same time has the potential to provide quantitative
markers of cognitive decline at the very early stages.

338
CHAPTER 12 Identifying Multivariate Imaging Patterns
Table 12.1 Relationship Between Cognitive Performance and Level of Pathology. p-Values of
One-Sided t-Test Obtained for the Group with Level of Pathology in Upper 25% vs. the Group
with Level of Pathology in Lower 75%
CVLT list A sum
CVLT long-delay free
BVRT errors
MMSE
Mean scores
0.001
<0.001
0.007
0.048
First-visit scores
0.001
0.001
0.011
0.137
Last visit scores
<0.001
0.001
0.042
0.038
4.12.6 Summary
In this chapter, we discussed selected works that address the task of identifying descriptive imaging
pattern from different learning perspectives, including supervised, semi-supervised, and unsupervised
learning paradigms. Depending on the availability and reliability of the diagnostic data for building
a predictive model and based on the separability of the diagnostic categories one can make a choice
between supervised and semi-supervised learning strategies. It is reasonable to expect that for well-
separable problems a fully-supervised approach model would be sufﬁcient to reliably discriminate
between the target conditions. In the task of discriminating between healthy controls and Alzheimer’s
patients this is evidenced by the consistently high predictive accuracies reported by numerous methods
in the AD/controls classiﬁcation task. However, conditions characterized by transient diagnoses may be
abletobeneﬁtfromasemi-supervisedstrategythatmakesuseofbothreliablydiagnoseddataandthedata
characterized by volatile diagnostic labels. In general, if the labeled data captures the manifold properties
of the underlying images well, a fully-supervised strategy can be expected to be sufﬁcient to build a pre-
dictor. At the same time, if unlabeled samples are good representatives of the manifold geometry that is
notentirelycapturedbythelabeleddata,asemi-supervisedmayleadtoamoreaccuratemodelofthedata.
Finally, unsupervised learning of high-dimensional imaging patterns presents a promising tool for
investigating highly heterogeneous diseases with unknown nosological boundaries between subcondi-
tions. Unlike the supervised and semi-supervised two-class categorization approaches, unsupervised
analysis is concerned with discovering new categories in the data and is typically used as the means
for the exploratory analysis. In the context of aging, the clustering-based approach indicates that in the
very early stages cognitive performance declines in a continuous manner. Moreover, cognitively less
stable populations seem to be extremely heterogeneous, whereas cognitively more stable individuals
form rather homogeneous populations. Additionally, it is possible to derive quantitative image-based
indicators of cognitive decline even if categorical labels are not available.
In summary, machine learning methods present a promising tool for building diagnostic and prognos-
tic markers of diseases. While application of multivariate pattern recognition methods for investigation
of some disease becomes common (e.g., Alzheimer’s disease, schizophrenia), other areas are only start-
ing to make use of these tools (e.g., autism, attention deﬁcit hyperactivity disorder). As machine learning
methods become increasingly popular in neuroimaging research, we can expect to witness the emer-
gence of approaches that not only improve accuracy and robustness of the diagnostic and prognostic

References
339
tools, but address problems that go beyond the identiﬁcation of known diagnoses and allow us to explore
the disease at different levels of detail.
Acknowledgment
This work was supported in part by R01AG-14971.
Relevant theory: Machine Learning
See Vol. 1, Chapter 16 Kernel Methods and Support Vector Machines
See Vol. 1, Chapter 21 Unsupervised Learning and Latent Variable Models (PCA, ICA, NMF, ...)
See Vol. 1, Chapter 22 Semi-supervised Learning
References
[1] S. Duchesne et al., Amnestic MCI future clinical status prediction using baseline MRI features, Neurobiol.
Aging 31 (9) (2010) 1606–1617.
[2] Y. Fan, N. Batmanghelich, C.M. Clark, C. Davatzikos, Spatial patterns of brain atrophy in MCI patients,
identiﬁed via high-dimensional pattern classiﬁcation, predict subsequent cognitive decline, NeuroImage 39
(4) (2008) 1731–1743.
[3] Y. Fan et al., Unaffected family members and schizophrenia patients share brain structure patterns: a high-
dimensional pattern classiﬁcation study, Biol. Psychiatry 63 (1) (2008) 118–124.
[4] Y. Fan, S.M. Resnick, X. Wu, C. Davatzikos, Structural and functional biomarkers of prodromal Alzheimer’s
disease: a high-dimensional pattern classiﬁcation study, NeuroImage 41 (2) (2008) 277–285.
[5] S. Kloppel et al., Automatic classiﬁcation of MR scans in Alzheimer’s disease, Brain 131 (3) (2008) 681–689.
[6] C. Misra, Y. Fan, C. Davatzikos, Baseline and longitudinal patterns of brain atrophy in MCI patients, and their
use in prediction of short-term conversion to AD: results from ADNI, NeuroImage 44 (4) (2009) 1415–1422.
[7] C. Ecker et al., Investigating the predictive value of whole-brain structural MR scans in autism: a pattern
classiﬁcation approach, NeuroImage 49 (1) (2010) 44–56.
[8] DSMIV, Diagnostic and Statistical Manual of Mental Disorders DSM-IV-TR, fourth ed., American Psychiatric
Association, 1994.
[9] ICD10, The ICD-10 classiﬁcation of mental and behavioral disorders: clinical descriptions and diagnostic
guidelines, 1992.
[10] S.D. Mayes, S. Calhoun, D. Crites, Does DSM-IV Asperger’s disorder exist? J. Abnorm. Child Psychol. 29
(2001) 263–271.
[11] J. Matson, J. Wilkins, Nosology and diagnosis of Asperger’s syndrome, Res. Autism Spect. Dis. 2 (2) (2008)
288–300.
[12] N. Craddock, M.C. O’Donovan, M.J. Owen, The genetics of schizophrenia and bipolar disorder: dissecting
psychosis, J. Med. Genet. 42 (3) (2005) 193–204.
[13] N. Craddock, M.C. O’Donovan, M.J. Owen, Genes for schizophrenia and bipolar disorder? Implications for
psychiatric nosology. Schizophr. Bull. 32 (1) (2006) 9–16.
[14] E.K. Green et al., The bipolar disorder risk allele at CACNA1C also confers risk of recurrent major depression
and of schizophrenia, Mol. Psychiatry 15 (10) (2010) 1016–1022.
[15] V. Moskvina et al., Gene-wide analyzes of genome-wide association data sets: evidence for multiple common
risk alleles for schizophrenia and bipolar disorder and for overlap in genetic risk, Mol. Psychiatry 14 (3)
(2009) 252–260.

340
CHAPTER 12 Identifying Multivariate Imaging Patterns
[16] M.J. Owen, N. Craddock, A. Jablensky, The genetic deconstruction of psychosis, Schizophr. Bull. 33 (4)
(2007) 905–911.
[17] N. Koutsouleris et al., Structural correlates of psychopathological symptom dimensions in schizophrenia: a
voxel-based morphometric study, NeuroImage 39 (4) (2008) 1600–1612.
[18] E.M. Meisenzahl et al., Structural brain alterations at different stages of schizophrenia: a voxel-based mor-
phometric study, Schizophr. Res. 104 (1–3) (2008) 44–60.
[19] J. Klosterkötter et al., Early self-experienced neuropsychological deﬁcits and subsequent schizophrenic dis-
eases: an 8-year average follow-up prospective study, Acta Psychiatr. Scand. 95 (5) (1997) 396–404.
[20] L.C. Johns et al., Prevalence and correlates of self-reported psychotic symptoms in the British population,
B. J. Psychiatry 185 (2004) 298–305.
[21] S.M. Resnick et al., Longitudinal cognitive decline is associated with ﬁbrillar amyloid-beta measured by
[11C]PiB, Neurology 74 (10) (2010) 807–815.
[22] H. Christensen et al., Age differences and interindividual variation in cognition in community-dwelling elderly,
Psychol. Aging 9 (3) (1994) 381–390.
[23] C.K. Morse, Does variability increase with age? An archival study of cognitive measures, Psychol. Aging 8
(2) (1993) 156–164.
[24] S.M. Resnick et al., Longitudinal magnetic resonance imaging studies of older adults: a shrinking brain,
J. Neurosci. 23 (8) (2003) 3295–3301.
[25] C. Davatzikos, A. Genc, D. Xu, S.M. Resnick, Voxel-based morphometry using the RAVENS maps: methods
and validation using simulated longitudinal atrophy, Neuroimage 14 (6) (2001) 1361–1369.
[26] D.L. Pham, J.L. Prince, Adaptive fuzzy segmentation of magnetic resonance images, IEEE Trans. Med.
Imaging 18 (9) (1999) 737–752.
[27] D. Shen, C. Davatzikos, HAMMER: Hierarchical Attribute Matching Mechanism for Elastic Registration,
IEEE Trans. Med. Imaging 21 (11) (2002) 1421–1439.
[28] Y. Fan et al., COMPARE: Classiﬁcation of Morphological Patterns using Adaptive Regional Elements, IEEE
Trans. Med. Imaging 26 (2007) 93–105.
[29] R. Cuingnet et al., Automatic classiﬁcation of patients with Alzheimer’s disease from structural MRI: a
comparison of ten methods using the ADNI database, Neuroimage 56 (2) (2011) 766–781.
[30] V.N. Vapnik, The Nature of Statistical Learning Theory, Springer-Verlag Inc., NY, USA, 1995.
[31] C. Burges, A tutorial on support vector machines for pattern recognition, Data Mining Knowl. Discov. 2
(1998) 121–167.
[32] V.N. Vapnik, Statistical Learning Theory, Wiley-Interscience, 1998.
[33] S. Duchesne et al., MRI-based automated computer classiﬁcation of probable ad versus normal controls, IEEE
Trans. Med. Imaging 27 (4) (2008) 509–520.
[34] R. Filipovych, C. Davatzikos, Semi-supervised pattern classiﬁcation of medical images: application to mild
cognitive impairment (MCI), NeuroImage 55 (3) (2011) 1109–1119.
[35] B. Gaonkar, K. Pohl, C. Davatzikos, Pattern based morphometry, Med. Image Comput. Comput. Assist. Interv.
(MICCAI) 14 (Pt 2) (2011) 459–466.
[36] U.M. Mann, E. Mohr, M. Gearing, T.N. Chase, Heterogeneity in Alzheimer’s disease progression rate segre-
gated by distinct neuropsychological and cerebral metabolic proﬁles, J. Neurol. Neurosurg. Psychiatry (1992)
956.
[37] R. Filipovych, S.M. Resnick, C. Davatzikos, Semi-supervised cluster analysis of imaging data, NeuroImage
54 (3) (2011) 2185–2197.
[38] M.F. Folstein, S.E. Folstein, P.R. McHugh, Mini-mental state. A practical method for grading the cognitive
state of patients for the clinician, J. Psychiatr. Res. 12 (3) (1975) 189–198.
[39] D.Delis,J.Kramer,E.Kaplan,B.Ober,CaliforniaVerbalLearningTest—ResearchEdition,ThePsychological
Corporation, New York, 1987.
[40] A. Benton, Revised Visual Retention Test, The Psychological Corporation, New York, 1974.

13
CHAPTER
Video Processing—An Overview
Amit K. Roy-Chowdhury
Department of Electrical Engineering, Department of Computer Science (Cooperating Faculty), University of
California, CA, USA
Video processing can be deﬁned as analysis of the content of the video to obtain an understanding of
the scene that it describes. It is an essential component of a number of technologies, including video
surveillance, robotics, and multimedia. From a basic science perspective, methods in video analysis are
motivated by the need to develop machine algorithms that can mimic the capabilities of human (and
other animal) visual systems. It is an area of research that has seen huge growth in the recent past.
Researchers in video analysis have varied backgrounds, including signal/image processing, computer
science, systems theory, statistics, and applied mathematics.
In this book, we will start by providing a broad overview of the tasks involved in the analysis of videos,
including a summary of the research challenges in each, followed by a brief tour of the application areas
and their signiﬁcance, and ﬁnally an overview of the articles in this section. Note that we use the terms
video processing and video analysis interchangeably.
4.13.1 Basic tasks in video analysis
The various tasks in video analysis can be categorized as low-level, mid-level, and high-level. Although
there is some ambiguity on which tasks belong to each category, there are some broad trends in the
literature. For example, some researchers consider edge detection and segmentation as low-level, 3D
reconstruction as mid-level, while recognition is a high-level task. However, the categorization is prob-
ably less important than the actual tasks. Below, we provide a brief summary of the most important
tasks. This section on Video Processing includes chapters that contain details of many of them.
At the level of a single image, two basic operations are fundamental to most video analysis appli-
cations. These include computing gradients which highlight the portions of the image where there is a
substantial change in the image intensity, and analysis of the image intensity value, be it a gray-scale or
color value. Computing gradients can lead to identifying the edges in the image, which can be combined
together to identify structures like lines and corners. These basic structures can be the building blocks
for identifying higher-level cues like shapes of objects.
Gradient estimation is also the foundation upon which different methods of image segmentation
and object detection are built. Segmentation is the process by which coherent regions of the image are
identiﬁed. These regions can be the basis of a higher-level analysis of the entire image. For example,
an efﬁcient segmentation algorithm of a natural scene may be able to divide the scene to consist of a
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00013-3
© 2014 Elsevier Ltd. All rights reserved.
343

344
CHAPTER 13 Video Processing—An Overview
green meadow, a blue sky, and a man-made structure like a house. The reason the algorithm is able to
identify these regions separately is because each of them have characteristics that are common within
that region and change signiﬁcantly from one region to another. This involves analysis of the image
intensity values to estimate the extent of similarities and differences.
Detection is also a basic low-level image analysis task as it can help identify the interesting objects in
the scene, e.g., people, which can then lead to an understanding of the scene or an analysis of the actions
of the objects. There are a number of detectors that have been built for different kinds of objects, the
most common being person and vehicle detectors. Again, they combine analysis of the image intensities
and their variations, often building statistical models that can serve as a signature for that object.
One of the factors that affects the performance of image analysis algorithms is the quality of the
image. There could be a number of sources for this, including sensor noise, environmental conditions
like lighting, and occluding objects that may temporarily mask the ones of interest. It is one of the most
active areas of research and includes various trends—statistical modeling of image quality, machine
learning based approaches to compensate for the variation in quality, and physics-based approaches that
model the environmental factors to account for their effects.
Given a sequence of images (i.e., a video), which are usually highly correlated, an additional task is
to compute the motion of the objects over the video. Again, a number of methods have been proposed
for this purpose. Optical ﬂow is a method for estimating the motion of each individual pixel. Combined
with segmentation, it can provide a sense of how each part of the scene is changing over time. Tracking
involves computing the location of each object over time, given the detections of the objects in each
frame. Bayesian tracking approaches like the Kalman ﬁlter or the particle ﬁlter, combined with suitable
data association strategies, have been adapted to the video analysis tasks. Other approaches have looked
at how the distributions of certain object characteristics, like image intensity values, change over time to
compute a track of these objects. Although motion analysis, including both ﬂow computation and track-
ing, has been the mainstay of video understanding research for some time, robustness to environmental
variations, as well as scene occlusions and clutter, remains dominant challenges for existing methods.
One of the special cases that needs particular attention is a multi-camera environment. In addition
to the above, such an application domain introduces its own challenges. Images of the same object
captured by different cameras need to be registered so that similar features are combined together for
further analysis. Reidentiﬁcation of a target over a network of non-overlapping cameras after it is not
visible in any camera for a signiﬁcant period of time is another important challenge. Moreover, multi-
camera environments bring up interesting research problems in distributed image processing, whereby
an understanding of the scene needs to be obtained by each camera acting as independent agents in
coordination with other cameras in a local neighborhood, rather than sending all the data to a central
processor.
Building upon the above-mentioned tasks, video analysis applications can obtain a higher-level
understanding of the scene. Since an image is a 2D representation of the 3D world, a natural question
to ask is whether it is possible to recreate the 3D scene given the images. This is an inverse estimation
problem and is ill-posed unless proper constraints are imposed on the structure of the scene and the
image acquisition process. The imaging process, which involves developing a mathematical model of
the camera that maps the 3D world and 2D image, is a necessary ﬁrst step in the process. Given the
model and a collection of corresponding points in the image and the world, it is possible to then calibrate
the camera.

4.13.2 Applications in Video Analysis
345
Various cues can be used for 3D reconstruction of a scene from a set of images. One possible
grouping is based on the use of multiple images. When there are two cameras used to image the same
scene, stereo reconstruction approaches can be used. From a single camera, the motion between the
frames in a sequence of images can provide an estimate of the 3D depth of the scene (structure from
motion). Additionally, shading and defocus have also been used for estimating the depth of a scene.
For a complex scene, like a city street, the basic depth estimation strategies can be combined together
with natural constraints to provide a more robust estimate of the overall scene structure.
The overall goal of video analysis is to obtain an understanding of the scene. The tasks described ear-
lier provide the building blocks for this. Scene understanding requires recognition of objects and events.
Object recognition can be achieved at the level of a single image, while recognition of activities and
events usually requires multiple images. In fact, various time scales can be used for recognizing activities
over different temporal horizons. Moreover, the complexity of the scene can deﬁne the kinds of activities
that need to be recognized, e.g., from a single-person activity to interactions between a group of people.
An active area of research in recognition is the use of contextual information. For example, the
human eye recognizes a table not only from its looks, but also from the setting that it is in. An interesting
question in video processing is how to model this surrounding information for more effective recognition
of objects and activities. Machine learning based approaches that model the interrelationships between
various detected objects and activities have become popular in this regard.
An overarching challenge in video analysis is to account for the errors in the lower-level modules,
e.g., detection and segmentation, in higher-level modeling tasks. Integrated approaches that combine
these various levels are increasingly becoming the trend in this regard. In these approaches, higher-level
analysis, like recognition, can provide cues for better segmentation, detection, and 3D reconstruction.
This requires designing suitable objective functions that can combine the results of the individual
modules to satisfy the overall goal of the system. Alternatively, some researchers have taken the approach
that precise segmentation, detection, and tracking which may not be necessary; rather, it may sufﬁce to
learn time-varying patterns of image features and use these learned patterns for recognition.
Over the past few decades, great strides have been made in video analysis leading to the development
of many advanced technologies. Even as basic researchers work on some of the most challenging issues,
application domains often provide enough constraints to lead to satisfactory solutions in that setting.
Moreover, the growth in the Internet, social media, and video capture devices, coupled with the needs
of security applications, has led to huge video repositories. Searching through them is opening up new
research problems, which will probably call for novel solution strategies. Rather than fully autonomous
applications, the convergence of man and machine may be the future trend in many practical video
analysis solutions in the near future.
4.13.2 Applications in video analysis
Below is a list of some of the most common applications of video analysis. Many of them are covered
in the chapters in this section.
Video surveillance: This is a traditional application domain that includes all the research tasks
outlined above, with applications ranging from national security to environmental monitoring. The
extraction of biometrics for person identiﬁcation is a specialized sub-area of this application.

346
CHAPTER 13 Video Processing—An Overview
Social media and the Internet: The preponderance of videos, and the manner in which these are
uploaded and used, on the Internet provide novel challenges to index and search such “big data”
repositories.
Mobile communications: Advances in video analysis can lead to more efﬁcient use of band-
width and power in handheld devices, like smartphones, with users sharing videos among them-
selves and with cloud computing servers.
Virtual reality: Virtual environments can be made more realistic if information can be gleaned
from videos of natural scenes and incorporated into the rendering process. This requires automated
analysis of the content of the video.
Vision-based robotics: Robots equipped with cameras, working independently or alongside
humans, can help in navigating through complex environments, like a disaster zone. Such applica-
tionsrequireadvancesinvariousaspectsofvideoprocessing—tracking,recognition,anddistributed
processing.
Computational photography: Recent advances in areas like compressed sensing have opened the
possibility of designing efﬁcient cameras that would reduce the amount of data that is collected,
without affecting the ﬁdelity of the analysis on the data.
Biomedical applications: Video processing can help medical practitioners in their diagnoses, as
well as researchers working in various biological ﬁelds with automated analysis of larger volumes
of data that is being collected, e.g., time-lapse microscopy images.
4.13.3 Overview of chapters
The chapters in this section span the broad spectrum of topics that have been identiﬁed above. While it
is not a comprehensive review of all possible approaches, the section encapsulates some of the major
trends in video processing. It provides a sampling of ideas that include the effects of the human visual
system on video search, mathematical techniques from signal processing, and systems theory for video
modeling, application domains like biometrics, rendering, and surveillance, core video analysis tasks
like video tracking, and large multi-camera systems analysis and evaluation.
Chapter 14 titled Foveated Image and Video Processing and Search explains the role of foveation
in the human visual system and how an understanding of this phenomenon can be useful for designing
efﬁcient detection and search systems. It is the only chapter that relates the human visual system
with engineering applications in video analysis. The authors, Floren and Bovik, describe methods
for modeling the effects of ﬁxation, which can lead to certain interesting areas of the video to be
represented at a higher resolution. Applications domains including teleconferencing, teleoperation,
wide band imaging, and search are described, among others.
Rodriguez and Vijaya Kumar provide an overview of biometric recognition in Chapter 15 titled
Segmentation-Free Biometric Recognition Using Correlation Filters. As noted earlier, low-level errors
like registration and segmentation affect higher-level recognition tasks and reduce their performance.
The authors in this chapter provide an overview of correlation ﬁlters that can achieve segmentation-free
recognition. Applications are shown on recognizing people using their eye regions, recognizing faces,
localizing pedestrians, and recognizing their activities.
Analysis of videos relies on understanding the dynamics in the scene. It is, therefore, natural to
leverage upon a large body of work in the systems sciences on dynamical modeling of video sequences.

4.13.3 Overview of Chapters
347
Chapter 16 on Dynamical Systems in Video Analysis, authored by Doretto, Ravichandran, Vidal, and
Soatto, provides an overview of linear dynamical systems and their application in video analysis. The
role of dynamic textures is studied, including learning the model parameters from natural videos, and
synthesizing novel videos based on these models. Low-level video processing tasks like registration
and segmentation are addressed.
One of the important application domains of video analysis is image-based rendering. As deﬁned in
Chapter 17, titled Image-Based Rendering by Chang and Chen, it is “a process of synthesizing images
at novel viewpoints based on a set of existing images.” The authors take a signal processing approach
to what is traditionally a computer graphics problem, by posing it as a sampling and reconstruction
problem of the plenoptic function which is used to represent the light ﬁeld. The chapter starts with a
historical perspective on the problem, describes the main challenges, and outlines a solution framework
and application domains.
Video surveillance is the most direct application of video processing methods. Given that many tens
of millions of surveillance cameras are being sold all over the world (over 30 million just in the US over
the last decade), it is necessary to design effective methods to search through such datasets. Effective
exploration and exploitation of such “big data” video repositories is the theme of Chapter 18 titled
Activity Retrieval in Large Surveillance Videos by Castanon, Jodoin, Saligrama, and Caron. The authors
deﬁne the challenges and then present an approach along with a thorough experimental evaluation.
Tracking is probably the most basic task for analysis of videos. It is often the theme on which the
maximum number of papers is presented at major conferences in video analysis. Chapter 19, titled
Multi-Target Tracking in Video by Poiesi and Cavallaro, provides a summary of the major research
challenges that keep this topic at the forefront of work in video analysis, and presents a description of
the various steps that are needed to develop efﬁcient tracking algorithms. Both the batch and sequential
approaches are described. The recent trends on exploiting contextual information for effective tracking
are addressed at the end of the chapter.
A challenge that comes up again and again in current video analysis research is the preponderance
of data that is being collected by large-scale sensor deployments. While other chapters in this section
have addressed this problem from the perspective of efﬁcient processing, Chapter 20, titled Compres-
sive Sensing for Video Applications by Veeraraghavan, Sankaranarayanan, and Baraniuk, argues for
the development of a “scalable theory of sensing.” Building upon the recently developed theory of
compressive sensing, they demonstrate the interplay of efﬁcient signal models, imaging architectures,
and signal recovery algorithms for designing effective visual sensing systems.
The last chapter in this section relates to an area that is of high interest currently. As large networks
of cameras are deployed, it is important to develop methods that would scale up to these levels. While
research in camera networks picks up, it is also important to understand how the developed algorithms
would be evaluated since it is difﬁcult to access a real-life large-scale camera network. In Chapter
21, Virtual Vision for Camera Networks Research, the authors Qureshi and Terzopoulos propose the
development of a simulation paradigm that would enable efﬁcient and rapid development of intelligent
surveillance systems involving large numbers of camera spread over wide areas.

14
CHAPTER
Foveated Image and Video
Processing and Search
Andrew Floren and Alan C. Bovik
The University of Texas at Austin, Austin, TX, USA
Nomenclature
Research revolving around foveation has a number of inter-related and confusing terms. In this section,
we develop the terminology we will use throughout the rest of the article.
The word fovea and its various conjugations are some of the most used, and perhaps misused, words
in foveation research. Traditionally, the word fovea refers to a small pit in the retina of the eye, and in
fact, fovea means ditch or pit in Latin. The fovea is located along the optical axis of the eye, which
results in images at the center of gaze being focused directly onto the fovea. The human visual system
has considerably higher resolution, and devotes considerably more processing power, at the fovea than
anywhere else on the retina. This spatial resolution degrades smoothly in all directions from the fovea.
The term foveation refers to this smoothly varying resolution. We will refer to the rate at which
the resolution declines from the fovea as the roll off of the foveation. Foveation ﬁltering refers to the
application of an image ﬁlter that mimics the effects of this smoothly varying resolution. To foveate an
image is to sample or ﬁlter the image in a manner similar to the foveation of the human eye. Figure 14.1
shows an example of a foveated image. A foveated algorithm is an algorithm that is developed to cope
with, or take advantage of, foveated images.
The point of maximum resolution in a foveated image is commonly called the fovea. The point of
maximum resolution is also called the ﬁxation point because that is the term for the point in a scene that
is focused onto the fovea. In general, we will use ﬁxation point when referring to the point of maximum
resolution in the image, and fovea when referring to the anatomical region of the eye.
Regions of an image or scene that are sufﬁciently close to the ﬁxation point can also be called foveal.
Whereas the periphery of an image or scene can also be called non-foveal. Eccentricity is a measure
of the distance from the fovea in degrees. The fovea is located at zero degrees eccentricity, and the
eccentricity increases equally in all directions away from the fovea.
Although these terms and ideas are derived from studies of the human visual system, we will use
them in a more general sense. For example, foveation will refer to any smoothly varying resolution
distribution. Additionally, we will discuss the concept of multiple fovea or ﬁxation points in an image
even though the human eye has a single anatomical fovea. Multiple ﬁxation points make it possible to
create more complicated resolution distributions to better capture the important content of an image.
A region of an image located near any of these ﬁxation points may be referred to as foveal. Furthermore,
while the roll off of the foveation is ﬁxed by physiology in the human eye, we will use the roll off as a
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00014-5
© 2014 Elsevier Ltd. All rights reserved.
349

350
CHAPTER 14 Foveated Image and Video Processing and Search
FIGURE 14.1
An example of a foveated image.
variable parameter. In this sense, foveation and a variable resolution can be used almost interchangeably.
We make the subtle distinction that foveation refers to smoothly varying resolution to differentiate it
from an arbitrary variable resolution.
4.14.1 Introduction
The concept of foveation, as well as the word itself, was inspired directly by the neural anatomy of
the human visual system. In particular, the human eye has a number of mechanisms that result in a
smoothly varying sensor resolution around a central point of maximum resolution. This central point of
maximum resolution coincides with a region in the eye called the fovea. This is where the terms fovea
and foveation are derived from. Although inspired by physiology, the use of image and video foveation
has found applications beyond anatomical modeling.
Human beings are decidedly visual animals. We use the phrases “to see” and “to understand” inter-
changeably. From an evolutionary perspective, vision has played a key role in our ability to survive.
Whether it is identifying threateing predators or ﬁnding food, the human visual system is incredibly
efﬁcient at performing a variety of visual tasks. In fact, the human visual system is still able to vastly
outperform computers in complex search and detection tasks. Therefore, it makes sense to study how
the brain is able to perform these tasks so efﬁciently. We can think of the various mechanisms in the eye,
including foveation, as a preprocessing step for visual information. The brain then uses this information
to perform a staggering array of complex visual tasks. It should not be surprising to ﬁnd that foveation
is useful in all manner of image and video applications.

4.14.1 Introduction
351
To begin with, it is helpful to study how foveation arises in the human visual system. Fortunately, the
eye, retinal ganglion cells, and primary visual cortex have been methodically studied [1–5]. In cats and
primates, microelectrodes have been used to directly measure the electromagnetic signal induced on
retinal ganglion cells and neurons in the primary visual cortex by controlled stimulus patterns presented
to the eye. Less invasive human studies have also been explored in the ﬁeld of psychophysics [6]. From
these kinds of experiments, it has been possible to acquire a good understanding of precisely how spatial
resolution varies with eccentricity.
A number of foveated image and video coding techniques have been developed using insights gleaned
from studies of the human visual system [7–9]. These coding techniques try to approximate the ﬁltering
performed by the early visual system, including the eye, retinal ganglion cells, and primary visual cortex.
Although based on the human visual system, many of these coding techniques have been extended to
support multiple ﬁxation points and variable roll-off to produce more arbitrary variable resolution
distributions.
Researchers have also begun to study how the brain efﬁciently utilizes foveation when performing
visual tasks [10,11]. Research in this area is centered on understanding and modeling how the brain
selects ﬁxation points in complicated and potentially non-static scenes. Fixation point selection is an
important task for any system that utilizes foveated imagery, because the ﬁxation point effectively
selects which regions will be represented in high resolution and which regions will be represented
in low resolution. Despite a considerable volume of research in this area, a reliable model has yet to
be found for predicting human ﬁxation selections. One of the primary difﬁculties researchers have
uncovered when modeling ﬁxation selection is the dependence on task [12]. That is, viewers assigned
different tasks when viewing a scene will ﬁxate radically different points.
Although a comprehensive model for predicting human ﬁxations has yet to be found, a survey
of the current literature is still helpful when developing foveated algorithms. The task dependence
of human ﬁxation selection implies that no simple ﬁxation selection algorithm exists. Rather, ﬁxa-
tions should be chosen based on the intended application of the foveated images and videos. In some
applications it is admissible to have a human operator select the ﬁxation point. However, in general
it is desirable to instead automate this process. For example, video intended for human consumption
should be foveated according to where a human viewer is likely to ﬁxate [13]. On the other hand,
for search and detection tasks, ﬁxation points should be selected that maximize the probability of
detection in minimal time [14]. Some researchers have considered the use of information theory to
choose the ﬁxation point that minimizes entropy or that maximizes knowledge gained [15]. A com-
plete theoretical framework for ﬁxation selection in foveated algorithms is still very much an open
problem.
An area of research that is currently wide open is the exploration of why the brain employs foveation.
That is, few researchers are trying to understand what the mathematical properties of foveation are
that make it useful, and under what conditions is a foveated approach superior to a uniform resolution
approach. Intuitively, the brain utilizes this non-uniform spatial resolution to more efﬁciently employ its
ﬁxed sampling and processing resources. However, peripheral vision is still important because it allows
the brain to detect and assess dangers, or other important stimuli, rapidly over a wide area. Therefore,
we expect foveation to be useful in settings where resources are constrained, either bandwidth or
computational power, and response time is pivotal. If bandwidth and computational power are sufﬁcient
to support full uniform resolution in real-time then foveation will confer little beneﬁt. However, this

352
CHAPTER 14 Foveated Image and Video Processing and Search
situation is not usually the case, and in many applications foveation can be used to reduce bandwidth
and computational requirements.
Foveation has been found effective in numerous applications, and as foveation is further explored
we expect the number of applications to increase. One of the primary beneﬁts of foveation is that it
makes images and video more compressible. This makes foveation widely applicable to video trans-
mission and storage. With the ever increasing resolution and number of videos that are transmitted
over the Internet, the ability to compress these high bandwidth signals for transmission and storage
is invaluable. In some sense, foveation is itself a form of compression. However, unlike most tradi-
tional compression techniques, foveation is non-uniform. That is, regions near the ﬁxation point may
be nearly unaltered while regions in the periphery are severely degraded. This is a useful property
for applications where certain regions of the image are more important, such as people or faces in a
surveillance video. Foveation is also useful for compressing video intended for human consumption.
If the exact ﬁxation point of a viewer is known, foveation can be used to perform perceptually lossless
compression. That is, compression which is lossy but is perceptually indistinguishable from lossless.
Speciﬁcally, the image or video is ﬁltered to remove information that is undetectable by the human
visual system.
Foveation is also useful for search and detection applications [16]. The low resolution peripheral
vision simultaneously identiﬁes candidate locations over a wide area. The ﬁxation point can then be
adjusted to further assess these candidate locations and reduce false positives. Many detection and
search algorithms that operate on uniform resolution imagery work in a similar course to ﬁne approach.
However, as previously discussed, foveation only confers a beneﬁt when the system is incapable of
capturing or processing the entire visual ﬁeld at full resolution. Super high resolution video feeds from
satellites and unmanned aerial vehicles (UAVs) are one such example where coping with the huge inﬂux
of data is impractical if not impossible.
Researchers are also applying foveation to compressive sensing [17,18]. Compressive sensing appli-
cations in the non-visible spectrum of light, where the time it takes to collect and process samples is
non-trivial, are another good example of where non-uniform sampling can be valuable. By directing
more samples towards areas of expected interest, it is possible to improve the quality of images within
a limited processing time.
Foveation research touches on a wide variety of topics, including anatomy, perception, statistics, and
image and video processing. The degree to which an understanding of these various topics is necessary is
directly dependent on the proposed application of foveation. For example, developing a foveated model
of the human visual system for perceptual image quality assessment will require a strong understanding
of the relevant anatomy and models of human perception. On the other hand, only a cursory introduction
to these topics is necessary when developing a foveated video coding scheme for detection and search
where a background in image and video processing is signiﬁcantly more helpful.
This article is written primarily for readers with a background in signal processing, and more speciﬁ-
cally image and video processing. It is assumed that the reader has a ﬁrm grasp of sampling theory, digital
ﬁltering, and statistics [19]. For example, the reader should be familiar with the Shannon-Nyquist sam-
pling rate and the construction of low-pass image ﬁlters. Basic concepts from geometry and trigonometry
are also employed. Furthermore, the reader should be familiar with Fourier and wavelet analysis, includ-
ing the application of the discrete Fourier transform (DFT), discrete wavelet transform (DWT), and the
discrete cosine transform (DCT). Basic concepts of image and video compression techniques are also

4.14.2 The Human Visual System
353
highly recommended to the reader, such as intra- and predictive-frame coding and motion estimation
and compensation [20].
Knowledge of relevent neurophysiology and computational neurology may also be helpful, but the
portions relevants to foveation will be introduced in this article. Similarly, some knowledge of optics
[21] is helpful for understanding the operation of the eye, but is not necessary.
The general layout of this article is as follows. First, we will introduce a number of topics as they
are related to foveation. These introductions are not meant to be deﬁnitive, and we invite interested
readers to explore the citations in the respective sections. We will introduce the ﬁeld of psychophysics
and discuss those areas that are relevant for exploring foveation in the human visual system. We will
provide an extremely rudimentary introduction to information theory and especially the concept of
entropy. We will discuss the anatomy and models of operation for the eye and primary visual cortex.
We will also introduce the reader to the efﬁcient coding hypothesis and its relation to the study of the
statistics of natural images.
After covering the relevant background information, in particular foveated models of the human
visual system and perception, we will go onto introduce methods for foveating images and video. We
will discuss foveation as a compression technique. We will also introduce methods for performing
foveated quality assessment. We will discuss efﬁcient implementations of foveation ﬁltering. We will
also discuss the integration of foveation into image and video coding schemes.
Next, we will explore how humans perform visual tasks using a foveated visual system. Speciﬁcally,
wewilllookatthemostrecentliteratureregardinghowhumanschooseﬁxationpoints.Modelsofﬁxation
selection are an important aspect of foveated algorithms. In any foveated algorithm, choosing the next
ﬁxation point is one of the most important and difﬁcult questions. We will then discuss the few existing
foveated algorithms for visual tasks and compare them to traditional uniform resolution solutions.
Then, we will discuss the concept and use of foveated hardware. Hardware that is capable of directly
sampling foveated imagery holds great promise for delivering efﬁciency gains in foveated algorithms.
Next, we will discuss the various applications of foveation in more detail. Following that, we will discuss
what we consider to be open issues and problems in the foveation literature. We will present several
simple prototype implementations as well as direct the reader towards publicly available data sets which
may be helpful for exploring foveation. Finally, we wrap up the article with our view of future trends
in the ﬁeld.
4.14.2 The human visual system
The human visual system collects foveated images. We present a high level overview of the anatomy,
optics, and current understanding of low level vision processing.
4.14.2.1 Anatomy of the eye
The eye is the primary imaging device in the human visual system. Figure 14.2 illustrates the basic
anatomy of the human eye. The eye is approximately spherical, and is, on average, 24 mm long by
22 mm across [21]. The main body of the eye, or the white part of the eye, is called the sclera. The
cornea is transparent and is located at the front of the eye. The primary function of the cornea is to
focus the light entering the eye. The pupil, which is situated behind the cornea, is a hole that allows

354
CHAPTER 14 Foveated Image and Video Processing and Search
Pupil
Lens
Sclera
Retina
Fovea
Blind
Spot
Optic Nerve
Cornea
Zonular Fibers
FIGURE 14.2
The basic anatomy of the human eye.
light to enter the eye. The iris is a muscle and it controls the diameter of the pupil. The iris adjust the
diameter of the pupil in order to control the amount of light entering the eye. In low light, the iris dilates
or increases the diameter of the pupil which allows more light into the eye. In bright light, it constricts
or reduces the diameter of the pupil which allows less light into the eye. Located behind the pupil is the
lens. Similar to the cornea, the lens focuses the light entering the eye. Unlike the cornea, the focusing
power of the lens is adjustable via the zonular ﬁbers. These ﬁbers contort the shape of the lens which
changes its focal length. The retina is a thin layer of cells located on the back of the inside of the eye.
Photoreceptors in the retina are responsible for converting impinging light into electrical signals. The
fovea is a small pit in the retina that is aligned along the optical axis of the cornea and lens. The highest
concentration of photoreceptors in the retina is found in the fovea. The electrical signals from the retina
are pooled and transmitted, along the optic nerve, to the brain for processing.
4.14.2.2 Optics of the eye
Both the cornea and the lens can be approximated as thin lenses. The pupil, situated between these
two lenses, functions as an aperture. This system can, in turn, be approximated by a single thin lens
with an aperture. This simple optical approximation is illustrated in Figure 14.3. This model can be
parameterized using a single focal length and aperture diameter. By accommodation of the lens and
dilation of the pupil, both of these parameters will vary depending on the scene being viewed. The
diameter of the pupil can vary from about 2 to 8 mm [21]. However, the minimum and maximum pupil
sizes can vary dramatically between individuals. The focal length, when the lens is not accommodating,
should be approximately equal to the distance from the lens to the retina. This results in objects located

4.14.2 The Human Visual System
355
Light Entering
the Eye
Cornea
Pupil
Lens
Retina
FIGURE 14.3
A simple optical approximation of the eye.
at inﬁnity, and most objects located at a moderate distance from the viewer, to be properly focused
on the retina. This gives an average unaccommodated focal length of 24 mm, the diameter of the eye.
When focusing on objects extremely close to the viewer, this focal length can change signiﬁcantly.
4.14.2.3 The retina and fovea
The retina is a thin layer of cells located on the inside of the back of the eye. The retina captures the
image produced by the optics of the eye and encodes it into electrical synapses for the brain to process.
The layers of the retina are illustrated in Figure 14.4. Counterintuitively, the photoreceptors, the cells
that actually transform photons into electrical synapses, are located in the bottom layer of the retina.
Ganglion Cells
Amacrine Cells
Bipolar Cellls
Cones
To Optic
Nerve
FIGURE 14.4
A cross section of the retina depicting its various layers. Light entering the eye travels from the top of the
ﬁgure to the bottom.

356
CHAPTER 14 Foveated Image and Video Processing and Search
Light must travel through the other layers of the retina before reaching the photoreceptors. Fortunately,
the cells of the retina are mostly transparent to avoid altering or distorting the focused image.
The fovea is a small pit in the retina. It is located along the optical axis of the cornea and the lens.
That is, the fovea is located at the center of the image produced by the optics of the eye. Due to the
distribution of photoreceptors and retinal ganglion cells, the fovea is the point of highest visual acuity.
4.14.2.4 Photoreceptors
Photoreceptors are cells, located in the retina, that transform light into a cell membrane voltage potential.
These photoreceptors capture the image produced by the optical system of the eye. There are two types
of photoreceptors: rods and cones. Rods are primarily responsible for our low-light (scotopic) and
peripheral vision. Cones, on the other hand, are responsible for the majority of our acute color vision
Synaptic Terminal
Nucleus
Photopigments
FIGURE 14.5
The basic structure of a cone cell.

4.14.2 The Human Visual System
357
Cells per Degree
Eccentricity (deg)
Ganglion Cells
cones
Rods
Blind Spot
300
100
10
3 -40
-20
0
20
40
FIGURE 14.6
The density of cone cells and ganglion cells with eccentricity from the fovea.
under daylight (photopic) conditions. For this reason, we will focus primarily on the properties and
distribution of cones in the retina. Figure 14.5 illustrates a cone photoreceptor cell.
Special proteins in the photoreceptor cells, called photopigments, convert incoming photons to cell
membranevoltagepotentials.Therearethreedifferenttypesofphotopigments,eachsensitivetodifferent
wavelengths of light. This, in turn, results in three different types of cones: short wavelength sensitive
cones, medium wavelength sensitive cones, and long wavelength sensitive cones. These cones are often
designatedS,M,andLconesrespectively.Sconestypicallyhaveapeakresponsearound420–440 nm,M
cones typically have a peak response around 535–545 nm, and L cones typically have a peak response
564–580 nm [22]. These varying sensitivities allow humans to perceive color. The sensitivities are
proportional to the probability that an incoming photon of a given wavelength will trigger a synaptic
response and induce a change in the cell membrane voltage potential. This cell membrane voltage
potential is then propagated along the synaptic terminal to bipolar and ganglion cells, also located in
the retina, for pooling and further processing.
Cones are very densely packed in and around the fovea. However, the density of cones drops off very
quickly moving away from the fovea. Figure 14.6 illustrates the precipitous falloff of cone density with
increasing eccentricity from the fovea. This spatially varying distribution of cone cells about the fovea
is one of the primary properties of the eye that contributes to the foveated, or multi-resolution, nature
of the human visual system.
4.14.2.5 Retinal ganglion cells
Retinal ganglion cells pool information from the photoreceptors and transmit it along the optic nerve to
the brain. Further ﬁltering is performed by the amacrine and bipolar cells which are situated between
the photoreceptors and the retinal ganglion cells. Recall Figure 14.4, which depicts a cross section
of the retinal layers. In the ﬁgure, it may appear that each ganglion cell is connected to a single

358
CHAPTER 14 Foveated Image and Video Processing and Search
photoreceptor. In actuality, ganglion cells are generally connected, and pool information from, a number
of photoreceptors. The density of retinal ganglion cells varies with eccentricity, similar to the distribution
of cones. This distribution is illustrated in Figure 14.6. The number of photoreceptors that each ganglion
cell pools information from is also correlated with the eccentricity from the fovea. Near the fovea, retinal
ganglion cells may communicate with only a few photoreceptors. Whereas in the extreme periphery,
retinal ganglion cells may pool information from thousands of photoreceptors. The spatially varying
distribution and connectedness of retinal ganglion cells is another property that contributes to the
foveated, or multi-resolution, nature of the human visual system. The axons from the retinal ganglion
cells form the optic nerve which transmits information to the lateral geniculate nucleus, or LGN. The
LGN is located inside the thalamus of the brain and is the primary pathway for visual information in
the brain as it passes from the eyes to the primary visual cortex.
4.14.2.6 Primary visual cortex
The primary visual cortex, or area V1, of the brain is located in the occipital lobe (Figure 14.7).
Scientists have been able to characterize this area empirically through direct experimentation on the
primary visual cortex of other mammals, such as cats and primates. The cell membrane potentials of
neurons in the primary visual cortex are measured while speciﬁc patterns are presented to the animal’s
eye. The animal’s eye is paralyzed and an artiﬁcial pupil is introduced so that stimuli can be delivered
with high accuracy and repeatability. These experiments have shown that neurons in the primary visual
cortex respond selectively to both location and frequency. That is, these neurons will only synapse when
a particular frequency is present at a particular point in the visual ﬁeld. Additionally, researchers have
found there is a direct mapping from signals in the retina to cortical layers in the primary visual cortex.
Stimuli in the left half of the visual ﬁeld affect neurons on the right side of the primary visual cortex.
Similarly, stimuli on the right half of the visual ﬁeld affect neurons on the left side of the primary
cortex. The mapping is very precise. That is, stimuli affecting a small portion of the retina will affect
a small and speciﬁc portion of the primary visual cortex. However, the ratio of the stimulated area in
the retina to affected area in the primary visual cortex is non-uniform across the retina. The fovea, for
example, maps to a large portion of primary cortex. This is known as cortical magniﬁcation.
Occipital Lobe
FIGURE 14.7
The occipital lobe of the brain is the location of the primary visual cortex.

4.14.3 Modeling the Human Visual System
359
4.14.3 Modeling the human visual system
The cornea, pupil, and lens focus an image on the retina. Photoreceptors, embedded in the retina,
sample this image. Signals from the photoreceptors are ﬁltered and pooled by the retinal ganglion cells.
Finally, the signals from the ganglion cells are propagated to the primary visual cortex. We can think
of these processes as a series of ﬁlters that prepare the data for processing by the brain. By analyzing
the properties of these processes one can gain insights into how the brain efﬁciently produces foveated
imagery.
4.14.3.1 Psychophysics
Analyzing the properties of the eye and primary visual cortex offers some insight into how the brain
processes visual data, but it does not tell us how the brain interprets and uses that data. Although there
has been much research in this area, directly analyzing processes in the brain beyond the primary visual
cortex has proven difﬁcult. Fortunately, at least with humans, we can simply ask subjects their perception
of presented stimuli. Psychophysics is an area of study that seeks to accurately and precisely quantify
the relationship between physical stimuli and the induced perceptions.
One of the most popular procedures for psychophysical testing is the two-alternative forced-choice
test. In this type of experiment, one of two stimuli are presented and the subject must choose which
stimuli was presented or decide on some relative aspect of the stimuli, such as brightness, visibility, or
quality. Often, the two choices are either the presence of a stimuli or its absence instead of two discrete
stimuli. When the two stimuli are indistinguishable, the test subject will still be correct at chance level.
When the two stimuli are fully distinguishable, the test subject will answer correctly nearly all of the
time. In between these two regimes the test subject will answer at an above chance level.
Psychophysics experiments are often designed to determine the point of just noticeable difference
between stimuli. There are various methods for determining the point of just noticeable difference. The
most common involves the estimation of a psychometric function. A psychometric function describes
the relationship between some parameter of the stimuli and the subjects response. For example, a
function that relates the difference between two stimuli and the probability that the subject will correctly
distinguish the two stimuli in a two-alternative forced-choice experiment is a psychometric function.
Psychometric functions are typically estimated by ﬁtting experimental data to a generalized logistic
function. The generalized logistic function is given by the following equation:
Y(t) = A +
K −A

1 + Qe−B(t−M)1/v ,
(14.1)
where A is the lower asymptote, K is the upper asymptote, B is the growth rate, and M is the point
of inﬂection. The parameters v and Q further control the shape of the curve. In many applications, the
assumption that v = Q = 1 is common. The point of just noticeable difference is often deﬁned to be the
point on the curve half-way between the lower and upper asymptotes. In two-alternative forced-choice
tests, this typically corresponds to the point at which the subject is expected to be correct three quarters
of the time. However, other deﬁnitions of just noticeable difference are used in practice. In general,
it is a good idea to ﬁnd the deﬁnition used for a particular experiment when attempting to compare
results.

360
CHAPTER 14 Foveated Image and Video Processing and Search
4.14.3.2 Optical ﬁltering
The ﬁltering effects of the optical system are approximately spatially uniform. There are some discrep-
ancies at extreme eccentricities where spherical aberration of the lens becomes an issue. This effect only
occurs at large pupil diameters and can largely be ignored. The primary ﬁltering effect of the optical
system is due to the diffraction limit imposed by the pupil [21]. The diffraction limit is a byproduct of
the wave nature of light. A wave passing through a small aperture causes constructive and destructive
interference on the imaging plane. The result is that the image of a point of light produces a blur, which
is known as the point spread function. This function is conceptually similar to an impulse response
function, and characterizes the ﬁltering effects of the optical system.
The diffraction limit restricts the resolvability of high frequency signals. Ignoring all other factors,
the maximum resolvable frequency is given by the Rayleigh Criterion:
sin θR = 1.22λ
d ,
(14.2)
where θR is the maximum resolvable frequency, λ is the wavelength of the light, and d is the diameter of
the pupil. From this equation, we can see that the maximum resolvable frequency actually depends on
the wavelength, or color, of the light. Additionally, the maximum resolvable frequency also depends on
ambient light levels because they will affect the size of the pupil. When the image is directly on the fovea
and under ideal lighting conditions, human subjects can approach the resolvability limit predicted by
the Rayleigh Criterion. However, other factors also affect resolvability, especially for non-foveal vision.
4.14.3.3 Photoreceptor sampling
We can think of cones as discrete sampling devices that sample the intensity of the light incident
on them. As with any discrete sampling device, we must consider the effects of aliasing [19]. The
Shannon-Nyquist Sampling Theorem speciﬁes the necessary sampling rate to unambiguously recon-
struct a continuous signal.
fs > 2 fmax.
(14.3)
The sampling frequency fs must be greater than twice the maximum frequency fmax of the signal being
sampled. In other words, the eye cannot unambiguously resolve signals whose frequency content is
greater than fcutoff
fcutoff =
1
2Cs
,
(14.4)
where Cs is the spacing between cones. Near the fovea, the cones are tightly packed. This results in Cs
being small and fcutoff is near or above the diffraction limit. However, in the periphery the cones are
more sparsely spaced, and the sampling limit is lower than that imposed by the diffraction limit.
4.14.3.4 Retinal ganglion cell models
The responses of retinal ganglion cells to controlled visual stimuli have been explored for some time [3].
The most common model for retinal ganglion cells has been termed center-surround. In this model, the
sensed light intensity values over a central visual region are spatially summed. At the same time, light
intensity values from the surrounding visual region are also summed. In an on-center ganglion cell, the

4.14.3 Modeling the Human Visual System
361
FIGURE 14.8
The visual ﬁeld of off-center/on-surround and on-center/off-surround retinal ganglion cells.
summed value in the surround is subtracted from the summed value in the center. While in an off-center
ganglion cell, the summed value in the center is subtracted from the summed value in the surround.
The response of the retinal ganglion cell is proportional to this ﬁnal value. Visual ﬁelds for both types
of ganglion cells are illustrated in Figure 14.8. Due to this arrangement, retinal ganglion cells respond
primarily to local contrast rather than pure intensity. This sensitivity to contrast is an important property
of the human visual system which we will use to develop the concept of a contrast sensitivity function.
4.14.3.5 The primary visual cortex and gabor ﬁlters
The response properties of neurons in the primary visual cortex have been studied for some time
[4,23]. It has been found that the spatial and frequency response functions of these neurons can be well
approximated using two dimensional Gabor functions [5]. That is, these neurons can be approximated
as two dimensional Gabor ﬁlters. A Gabor function is the product of a sinusoid and an exponential:
g(x, y) = exp

−π

(x −x0)2a2 + (y −y0)2b2
× exp

−2πi(u0(x −x0) + v0(y −y0))

.
(14.5)
Gabor ﬁlters provide optimal spatial and frequency localization in the sense of conjoint uncertainty
[24]. This makes them extremely well suited to extract image features for visual tasks. Of course, it is
not surprising that techniques evolved in the brain for performing visual tasks are highly effective. A
number of popular algorithms have been inspired by or incorporate ideas from Gabor ﬁlters.
Another popular application of Gabor functions is their use as visual search targets. They are espe-
cially popular in visual search experiments involving humans because they will stimulate a highly local-
ized portion of the primary visual cortex. This allows human observers to resolve the target quickly and
unambiguously. However, Gabor functions are still useful in visual search experiments not involving
subjects, due to their optimal spatial and frequency localization property. For example, Gabor functions
allow experimenters to test a search algorithm’s acquisition rate with respect to target frequency.
4.14.3.6 Eccentricity
Eccentricity is a common term in foveated image and video processing. Eccentricity is deﬁned as the
angular distance from the fovea, along the retina, expressed in degrees. The eccentricity of a point on

362
CHAPTER 14 Foveated Image and Video Processing and Search
Eye
Screen
v
w
xf
x
θ
FIGURE 14.9
The relevant geometry for calculating the eccentricity of a point from the fovea.
the retina is the angle between the optical axis and the line that intersects the nodal point of the lens and
the point in question. This geometry is illustrated in Figure 14.9. It should be noted that the optical axis
is the line that intersects the nodal point of the lens and the fovea, therefore the fovea has an eccentricity
of zero degrees. It is also worth noting that eccentricity is circularly symmetric. That is, a point located
ﬁve degrees toward the temple (temporal) and ﬁve degrees toward the nose (nasal) are indistinguishable
in terms of eccentricity. When dealing with digital images, the location on a screen or monitor must be
translated into eccentricity. Let x = (x1, x2)T be some location on the screen measured in pixels. Let
x f =

x f
1 , x f
2
T
be the location of the ﬁxation point also measured in pixels. Let N be the number
of pixels per some unit of distance on the screen. Let v be the distance of the viewer from the screen
measured in the same unit of distance as N. Eccentricity can be calculated as
d(x, x f ) = ∥x −x f ∥2 =

x1 −x f
1
2
+

x2 −x f
2
2
,
(14.6)
e(v, x) = tan−1
d(x, x f )
Nv

.
(14.7)
A common unit of distance for measuring N and v is the image width. Then N is simply the width of the
image in pixels and v is the distance of the viewer from the image measured in image widths. Regardless
of the measure used, the product Nv will be identical if care is taken with the units of measurement.
4.14.3.7 Spatial frequency
Spatial frequency measurements related to the eye are generally measured in cycles per degree. This is
because the constituent spatial frequencies of a signal projected onto the retina from a particular source is
dependent on the distance of the observer from that source. As the observer moves away from a scene or
object, its projected spatial frequency content expands to higher frequencies, and as the observer moves
towards a the source, its frequency content contrast to a lower spatial frequency range. A measurement
in cycles per degree captures the frequency that is projected onto the retina. Occasionally frequencies
are measured in cycles per millimeter, but this practice is uncommon. For a rough estimate of cycles
per degree, the visual angle subtended by the width of an adult thumb at arm length is approximately
two degrees. Therefore, a sin wave grating with a spatial frequency of one cycle per degree should have
about two cycles in the same amount of visual space taken up by the width of a thumb at arm length.

4.14.3 Modeling the Human Visual System
363
Although most anatomical and psychophysical frequency measurements are taken with respect to
cycles per degree, digital image frequencies are expressed in terms of cycles per pixel. This is the notion
of frequency used in discrete Fourier and discrete wavelet transformations. It takes at least two pixels
to represent a single cycle. Therefore, the maximum frequency on a computer monitor is 0.5 cycles per
pixel. Frequencies above this rate cannot be represented without being aliased to lower frequencies.
Whenever applying psychophysical measurements to algorithms that deal with digital images, it is
necessary to translate cycles per degree to cycles per pixel. This conversion depends on the size of the
pixels as well as the observers distance from the screen. A right triangle is formed between the eye and
the edges of the pixel. Refer to Figure 14.9 for an illustration of the geometry. Therefore, the size of the
pixel in radians is given approximately by
tan θ = w/v,
(14.8)
where w is the width of the pixel in some unit of measure, v is the distance of the viewer from the screen
in the same unit of measure, and θ is the angle subtended by the pixel in radians. For small values of
θ, tan θ ≈θ. Ignoring gaps between pixels, the number of pixels per unit distance N is approximately
equal to 1/w. It is generally easiest to measure the width of the monitor and divide the horizontal
resolution by the width to ﬁnd N. Therefore, the number of radians per pixel is approximately
θ =
1
Nv ,
(14.9)
and the number of pixels per degree is then
π Nv
180 .
(14.10)
The equivalent cycles per degree can now be calculated by multiplying the cycles per pixel by this value.
Cycles
Degree = Cycles
Pixel × π Nv
180 .
(14.11)
Conversions between these two units will be implicit when discussing applications of psychophysical
models to digital image and video processing algorithms throughout this article. This conversion is
fairly standard throughout the literature, however it is worth noting that it is only an approximation.
For observers placed far from the screen and viewing images located directly on the fovea, the errors
incurred by this approximations are small. However, when the subject is placed very close to the screen
or when viewing images in the periphery, it may be necessary to use more precise calculations. Correctly
handling frequencies in the periphery is difﬁcult because, unless the screen is curved, the distance from
the eye to the screen increases with eccentricity. The assumption was also implicitly made that the
pixels were square. That is, the pixels per distance horizontally is identical to the pixels per distance
vertically. If this is not the case, both vertical pixels per degree and horizontal pixels per degree must
be calculated. However, on most monitors this discrepancy is small enough to ignore.
4.14.3.8 The contrast sensitivity function
The most studied and applied psychometric function in the human visual system is the contrast sensitivity
function. This function measures the ability of an observer to discern the presence of a visual stimuli
with varying contrast and frequency. There are several existing deﬁnitions for computing the contrast

364
CHAPTER 14 Foveated Image and Video Processing and Search
of an image patch, however we will be using the RMS (root mean square) contrast. The RMS contrast
is deﬁned as the standard deviation σ of the pixel intensity values in an image patch.
I =
1
M N
N−1
	
i=0
M−1
	
j=0
Ii j,
(14.12)
σ =



 1
M N
N−1
	
i=0
M−1
	
j=0
(Ii j −I)2,
(14.13)
where Ii j corresponds to the pixel intensity value of image patch I at pixel (i, j), and M and N are the
width and height of the image patch respectively. The pixel intensity values are generally normalized
to the range [0, 1]. Figure 14.10 illustrates how sensitivity varies with contrast and frequency. Contrast
varies along the Y-axis while frequency varies along the X-axis. At some level of contrast the sinusoid is
no longer visible. As can be seen in the ﬁgure, this point varies with frequency. The sinusoid is visible at
lowerlevelsofcontrastinthemiddlefrequencyrangethanitisatthelowerorhigherendsofthefrequency
spectrum. For a given frequency, the contrast at which the sinusoid is just barely visible is called the con-
trast threshold. Psychovisual experiments have been carried out to model this contrast threshold. These
experiments found that the contrast threshold varied not only with frequency but also with eccentricity.
The contrast threshold varies between individuals and may change during the aging process. However,
the following model, as introduced in [7], provides a good ﬁt to the available psychophysical data:
CT ( f , e) = CT0 exp

α f e + e2
e2

,
(14.14)
where f is spatial frequency in cycles per degree, e is retinal eccentricity in degrees, CT0 is the mini-
mal contrast threshold, α is a spatial frequency decay constant, and e2 is a half-resolution eccentricity
FIGURE 14.10
Figure illustrating the effects of contrast sensitivity. Contrast decreases along the Y -axis, and frequency
increases along the X -axis. Notice how the pattern is no longer visible at certain frequency and contrast levels.

4.14.3 Modeling the Human Visual System
365
constant. As reported in [7], the best ﬁtting parameters values are α = 0.106, e2 = 2.3, and CT0 = 1/64.
These parameters provide a good ﬁt to both their own data as well as the data from similar psychovisual
experiments carried out by different groups. However, this model is not perfectly accurate for everyone
due to variations between individuals.
For a normalized measure of contrast, the maximum possible value is 1.0. Therefore, for a given
eccentricity we may calculate a cut-off frequency, or the spatial frequency that a viewer will be unable
to perceive regardless of contrast, by setting CT = 1.0 and solving for f .
fc(e) =
e2 ln

1
CT0

α(e + e2) ,
(14.15)
where fc is the cut-off frequency in cycles per degree. However, for eccentricities close to zero, the cut-
off frequency can exceed the highest frequency that can be represented on the screen without aliasing.
As previously explained, this frequency is 0.5 cycles per pixel. From Eq. (14.10), we can compute the
pixels per degree and thus the maximum representable frequency in cycles per degree.
1
2 × pixels per degree = π Nv
360 .
(14.16)
Combining Eqs. (14.7), (14.15), and (14.16), we can produce a new equation that calculates the cutoff
frequency for a particular pixel x at a viewing distance v.
fc(v, x) = min
⎛
⎝
e2 ln

1
CT0

α(e(v, x) + e2), π Nv
360
⎞
⎠.
(14.17)
The contrast sensitivity is deﬁned as the reciprocal of the contrast threshold:
CS( f , e) =
1
CT ( f , e).
(14.18)
Figure 14.11 illustrates the eccentricity dependence of the contrast threshold and contrast sensitivity
function. Contrast sensitivity attempts to quantify how noticeable the frequency components of a given
contrast are to a human viewer. Both the cut-off frequency and contrast sensitivity function are used
extensively in foveated techniques as models of the frequency response of the human visual system.
4.14.3.9 Information theory and entropy
Information theory is a ﬁeld of study that seeks to quantify and study the properties of information.
Information theory was originally developed by Claude E. Shannon in [25]. Shannon applied the theories
he developed to derive fundamental limits for information compression and transmission.
One of the key concepts in information theory is the idea of entropy. As originally deﬁned by
Shannon, entropy is a measure of how uncertain we are of the outcome of a random variable. For a
discrete random variable with n possible outcomes, where the probability of each outcome is given to

366
CHAPTER 14 Foveated Image and Video Processing and Search
Distance from Fovea (pixels)
Frequency (Cycles per Degree)
−250
−200
−150
−100
−50
0
50
100
150
200
250
0
5
10
15
20
25
30
35
40
FIGURE 14.11
The contrast sensitivity with respect to foveal distance in pixels. The solid white line corresponds to the
contrast threshold.
be (p1, p2, . . . pn), the entropy of that random variable is deﬁned by the following equation:
H = −K
n
	
i=1
pi log pi,
(14.19)
where K is any positive constant. Entropy is directly related to compressibility. If a language is composed
of n symbols, and each symbol appears with probability (p1, p2, . . . pn), the entropy is equivalent to
the average number of bits needed to encode each symbol in the language.
One interesting quality of entropy is that for a ﬁxed number of symbols the maximum entropy is
obtained when each symbol appears equally likely, or the random variable has a uniform distribution.
The less uniform the distribution, the lower the entropy, and the less bits required to encode each symbol.
4.14.3.10 Efﬁcient coding hypothesis and natural scene statistics
Theefﬁcientcodinghypothesiswasintroducedin[2].Thehypothesisstatesthatthebrainmustefﬁciently
encode all sensory data. From a biological point of view, the brain must minimize the energy required
to transmit and process all of the incoming sensory data. If it did not, humans would have to eat more
calories per day in order to support the energy requirements of the brain.
Using concepts from information theory, we can derive encoding schemes that approach the fun-
damental limits for compression of images based on entropy. If all possible images appeared equally

4.14.4 Foveated Images and Video
367
likely it would be impossible to perform lossless compression. However, the human brain only deals
with, and has therefore likely adapted to compress, natural images. By natural images we do not mean
images of nature, such as trees, grass, and sky. Although those images are included in the domain of
natural images, we deﬁne natural images as any image that the human visual system is likely to process
in a natural setting. This deﬁnition includes images of cities and other man made objects. However,
it does not, for example, include images of white noise. To gain some insight into how non-uniform
the distribution of natural images is, consider the following. If we construct a random image by draw-
ing each pixel’s intensity value from identical and independent uniform random distributions, then
the probability that we will construct an image that appears to be natural is extremely improbable.
Therefore, we expect natural images to be highly compressible. To determine how to compress natural
images, we must quantify their statistics [26]. The statistics of natural images then dictate the optimal
compression scheme. By the efﬁcient coding hypothesis, we would predict that the brain’s encoding
mechanisms should approach this optimal compression scheme. Therefore, the study of statistics of nat-
ural images provides insights into the encoding mechanisms of the brain. In [27], the relation between
the responses of cells in the primary visual cortex and natural scene statistics were explored. From this
point of view, studying the statistics of natural scenes is a dual problem to studying the human visual
system.
4.14.4 Foveated images and video
Now that we have discussed how the human visual system produces variable resolution imagery, we
will explore applications to image and video processing. Foveated images and videos refer to images
and videos that have been sampled or ﬁltered in a spatially varying manner similar to the human
visual system. However, as we have seen, the exact processing methods employed by the human visual
system are quite complex. Instead, simple approximations are often used to reduce the computational
complexity and improve performance on modern computers.
Foveation can be considered a technique for efﬁcient bandwidth allocation. Not every part of an
image is equally useful or important. What qualiﬁes as useful or important is highly dependent on
application. We do not want to completely throw away information in other regions. Foveation allows
allocation of bandwidth to areas of interest while still maintaining some information over a wide area.
4.14.4.1 Foveation as compression
Foveation itself is, in fact, a form of compression. This is obvious when the foveated image is produced
through irregular sampling, such as by the photoreceptors in the eye. There are fewer pixels in the
irregularly sampled image than there would be in a uniformly sampled image However, foveation
by this method is uncommon. Few sensors can capture data using irregular samples and even fewer
displays are capable of displaying irregularly sampled images. Instead, it is common to produce foveated
images using spatially-varying low-pass ﬁlters applied to the uniform resolution image. Such a foveation
ﬁltered image can be the same size as the original unﬁltered image or it might be nonuniformly sampled.
However, the foveation ﬁltering makes the image more compressible. Most compression techniques,
including DCT (discrete cosine transform) and DWT (discrete wavelet transform) based techniques,
can compress foveation ﬁltered images to much lower bit-rates than the image’s unﬁltered counterpart.

368
CHAPTER 14 Foveated Image and Video Processing and Search
This increase in compressibility can be considered as an increase in sparsity or a decrease in entropy of
the image.
If a signal is compressible then it should be sparse in some domain, and images are generally
sparse in the fourier and wavelet domains. This is one reason that the DCT and DWT are common in
modern image compression algorithms. According to the contrast sensitivity function, the coefﬁcients
are imperceptible beyond some eccentricity at each level in the wavelet domain. Foveation algorithms
generally remove or zero-out these coefﬁcients by low pass ﬁltering or direct masking in the transform
domain. This results in a drastic increase in the sparsity of the transformed image. It is not uncommon
to mask more than half of the transform coefﬁcients. This increase in sparsity is directly responsible for
the increased compressibility of foveated images.
Optimal coding theory states that the average number of bits required to code each pixel is equal to
the entropy of each pixel. If this entropy can be reduced then the number of bits per pixel required for
encoding will also be reduced. In [28], the authors make an intuitively appealing geometric argument.
They argue that there exists a transformation that maps the irregularly sampled image onto a uniform
image. The area, or number of pixels, of this transformed image Ac is smaller than the area of the
original uniformly sampled image Ao. If H(X) is the average entropy of a pixel, then the reduction
in entropy is proportional to (Ao −Ac)H(X). Of course, this ignores the mutual information between
pixels. Interestingly, the authors of [29] discuss how the Gaussian pyramid representation of an image,
which is similar to models of the primary visual cortex, reduces this mutual information between pixels.
Furthermore, in [30], the authors discuss the application of their ideas to non-uniform spatial sampling
and similarities with the human visual system. Although not directly concerned with producing foveated
images, this is some of the earliest work involving spatially varying resolution with respect to the human
visual system.
Of course, sparsity and entropy are not unrelated. Entropy is maximized when the underlying proba-
bility distribution for a random variable is uniform, i.e., when the probabilities of all values that a random
variable can take are equal. A sparse representation means many pixels are at or near zero in value. This
signiﬁcantly skews the pixel value probability distribution and reduces the entropy of the data.
4.14.4.2 Foveated quality assessment
It is important to be able to assess the quality of the output of foveated algorithms for the purpose of
making comparisons. The gold standard for assessing the quality of foveation should be the human
visual system itself. However, creating a perfect simulation of the human visual system is a difﬁcult, if
not impossible, task. Psychophysical measurements and perceptual quality assessment provide a way
to estimate how well a foveated algorithm matches the human visual system.
In this section, we will discuss some of the more common foveated quality assessment algorithms. In
particular, we discuss algorithms well suited for real-time implementation. The computational efﬁciency
of these algorithms makes them excellent optimization targets for dynamic rate control and other real-
time optimization techniques. All of these algorithms are full-reference quality assessment algorithms.
That is, they compare an undistorted or reference image to a distorted or compressed version. This
makes these algorithms unsuitable for quality assessment at the decoder where the full uncompressed
image is not available. No-reference or blind foveated quality assessment is an area of research that has
not yet been explored.

4.14.4 Foveated Images and Video
369
More complex quality assessment algorithms that incorporate the effects of foveation do exist. These
algorithms are much better suited to off-line quality assessment for algorithmic comparisons and will
be discussed further in the applications section on quality assessment.
4.14.4.2.1
FMSE
The foveated mean square error (FMSE) is a weighted estimation of the mean square error between
a foveated image and the original unaltered image. Weighting is introduced such that errors near the
ﬁxation point are weighted more heavily than errors in the periphery. In [28] the authors weight each
residual by its local bandwidth or cut-off frequency. Let I be the original image, I be the distorted
image, and x = (x1, x2)T be some location in the image measured in pixels. Then
FMSE =
1

x∈I
fc(x)2
	
x∈I

I(x) −I(x)
2 fc(x)2,
(14.20)
where fc is the cut-off frequency deﬁned in Eq. (14.15). A lower value of FMSE corresponds to a better
quality image. If the two images being compared are identical then the FMSE will be exactly zero. An
image that has been foveated will be receive a better score, or a score closer to zero, from the FMSE
than it would from the MSE. However, the score of a foveated image will still be worse than the score
of an image identical to the reference image.
4.14.4.2.2
FPSNR
The foveated peak signal to noise ratio (FPSNR) is a natural extension of the FMSE. The PSNR and
MSE are monotonically related and result in identical quality assessment prediction performance. The
PSNR is just another way of expressing the measure. In [28], the FPSNR is deﬁned as follows:
FPSNR = 10 log max[I(x)]2
FMSE
.
(14.21)
A higher PSNR corresponds to a higher quality image, and the same is of course true of the FPSNR.
When there is no distortion or noise, then the FMSE is zero and the corresponding FPSNR is inﬁnity. On
the other hand, for the FPSNR to be zero, the FMSE must be equal to max[I(x)]2. This is the maximum
possible value that the FMSE can take, and therefore the FPSNR cannot be negative.
4.14.4.2.3
FWQI
The foveated wavelet quality index (FWQI) is a foveated quality measure in the wavelet domain [31].
Fast discrete wavelet transforms make the calculation of this value quite efﬁcient. However, the main
advantage of this quality assessment algorithm arises when it is used in conjunction with compression
techniques that operate in the wavelet domain.
First, combine the cut-off frequency function and the contrast sensitivity function to produce a
foveated error sensitivity function.
S(v, f , x) =

CS( f ,e(v,x))
CS( f ,0)
,
f ≤fc(x),
0,
f > fc(x).
(14.22)

370
CHAPTER 14 Foveated Image and Video Processing and Search
FIGURE 14.12
A typical wavelet decomposition structure.
This error sensitivity is normalized such that the maximum value is equal to one. To apply this sensitivity
function in the wavelet domain, it is necessary to calculate the eccentricity and frequency of the wavelet
coefﬁcients in each of the sub-bands. Figure 14.12 illustrates a typical discrete wavelet transform
decomposition structure. First, compute the location of the assumed ﬁxation point in each sub-band.
Let γ represent the level and θ the orientation of a given sub-band. Then the ﬁxation point in each
sub-band can be computed using the following equations for the appropriate orientation:
LL: x f
λ,θ =

x f
1
2λ , x f
2
2λ

,
(14.23)
LH: x f
λ,θ =

x f
1 + N
2λ
, x f
2
2λ

,
(14.24)
HL: x f
λ,θ =

x f
1
2λ , x f
2 + N
2λ

,
(14.25)
HH: x f
λ,θ =

x f
1 + N
2λ
, x f
2 + N
2λ

.
(14.26)
The distance of a wavelet coefﬁcient x located in subband (λ, θ) from the ﬁxation point is given by the
following equation.
dλ,θ(x) = 2λ∥x −x f
λ,θ∥2.
(14.27)

4.14.4 Foveated Images and Video
371
Now deﬁne a foveated error sensitivity function in the wavelet domain.
S f (v, λ, θ, x) = S

v,r2−λ, dλ,θ(x)

.
(14.28)
In [9] the authors present psychophysical measurements for the error sensitivity of wavelet decompo-
sition sub-bands. A model that ﬁts these measurements is given by
log Y = log a + k( log f −log gθ f0)2,
(14.29)
where Y is the visually detectable noise threshold and f = r2−λ is the spatial frequency. The best
parameter values were found in [32] to be a = 0.495, k = 0.466, f0 = 0.401, and gθ = 1.501, 1, 0.534
fortheLL,LH/HL,andHHsubbandsrespectively.Theerrorsensitivityinsubband(λ, θ)isthengivenby
Sw(λ, θ) = Aλ,θ
Yλ,θ
,
(14.30)
where Aλ,θ is the basis function amplitude given in [32]. By combining Eqs. (14.28) and (14.30), the
following foveated error sensitivity model in the wavelet domain is produced:
S f w(v, λ, θ, x) =

Sw(λ, θ)
β1 
S f (v, λ, θ, x)
β2,
(14.31)
where β1 and β2 control the relative scaling between the two error sensitivity models. The authors of
[9] use β1 = 1 and β2 = 2.5. This error sensitivity projected into the wavelet domain is illustrated in
Figure 14.13.
FIGURE 14.13
The wavelet sensitivity projected into the wavelet domain.

372
CHAPTER 14 Foveated Image and Video Processing and Search
Then deﬁne a foveated wavelet image distortion (FWD) function.
FWD =

1
M
M
	
n=1

S f w(v, λn, θn, xn) × |c(xn) −˜c(xn)|
Q
 1
Q
,
(14.32)
where λn, θn, and xn are the level, orientation, and position of the nth wavelet coefﬁcient respectively,
M is the total number of wavelet coefﬁcients, c(xn) is the value of the wavelet coefﬁcient located at
xn in the original image, and ˜c(xn) is the value of the wavelet coefﬁcient located at xn in the distorted
image. The authors of [9] use Q = 2. Finally, the foveated wavelet quality index (FWQI) is expressed
FWQI = exp

−FWD

.
(14.33)
The FWD is exactly zero when the reference coefﬁcients and distorted coefﬁcients are identical. The
FWD is also exactly zero when discrepancies occur between coefﬁcients that have zero weighting,
such as coefﬁcients above the cut-off frequency. Similar to the FMSE, the FWD is upper bounded by
max |c(xn) −˜c(xn)|Q. Therefore, a FWQI value of one corresponds to a perfect score, and the quality
decreases as the FWQI decreases to a minimum value bounded below by exp (−max |c(xn) −˜c(xn)|Q).
4.14.4.3 Foveation ﬁltering
Foveation ﬁltering is an image processing technique that produces images of spatially varying resolution
or foveated images. As previously discussed, foveating an image increases the compressibility of that
image. Therefore, foveation ﬁltering can be used as a preprocessing step before applying any standard
imagecompressionalgorithm.Combiningfoveationﬁlteringwithanystandardcompressionalgorithmis
the simplest technique for producing a foveated image compression scheme. Similarly, a foveated video
compression scheme can be produced by applying foveation ﬁltering to each frame of the video before
processing. We will discuss a number of the more common methods for applying foveation ﬁltering.
4.14.4.3.1
Irregular sampling and reconstruction
The most physically accurate method involves irregularly sampling the image in a similar fashion to the
distribution of the photoreceptors [33,34]. A uniform resolution image is then reconstructed from these
irregular samples. To avoid aliasing, the output must be ﬁltered according to the local sampling rate.
However, this method is computationally inefﬁcient and has a tendency to produce artifacts in the periph-
ery. The use of this method is primarily relegated to complex models of the human visual system [13].
4.14.4.3.2
Spatial warping
The basic concept behind spatial warping is to develop an image transformation that maps pixels
from a uniform resolution representation of an image to a non-uniform resolution representation of
an image [35]. This transformation is applied to a uniform-resolution image to transform it into non-
uniform resolution space. This is a direct form of compression because dimensionality is reduced in
the non-uniform resolution space. Image data is then stored or transmitted in this compressed space.
The inverse transformation is then applied to produce the ﬁnal foveated image for presentation on a
uniform resolution screen. Unfortunately, this method is also prone to heavy artifacts and aliasing in
the periphery.

4.14.4 Foveated Images and Video
373
4.14.4.3.3
Spatially varying low-pass ﬁlters
Spatially varying low-pass ﬁlters, and approximations to spatially varying low-pass ﬁlters, are the most
common and effective form of foveation ﬁltering. In some sense, irregular sampling, spatial warping, and
low-pass ﬁltering are equivalent. Both irregular sampling and spatial warping produce a representation
of the image in a lower dimensionality space and then reconstruct the image at uniform resolution. To
avoid aliasing in these techniques, spatially varying low-pass ﬁlters must be applied with bandwidths
deﬁned as function of the local sampling rate. According to the sampling theorem, if the image is
band-limited and sampled correctly then it should be possible to perfectly reconstruct it. Therefore, the
image produced by low-pass ﬁltering should be identical to that reconstructed by irregular sampling
and spatial warping. The main advantage of irregular sampling and spatial warping is that they provide
direct dimensionality reduction and thus compression. However, it has been found that the band-limited
images produced by spatially varying low-pass ﬁlters are equally compressible.
Low-pass ﬁltering techniques can be employed in a variety of domains, including the spatial domain,
the wavelet domain, and even the DCT domain. However, every implementation employs some form of
the contrast threshold (Eq. (14.14)) or cut-off frequency (Eq. (14.15)). In space domain techniques, the
cut-off frequency is used to construct ideal low-pass ﬁlters. The cut-off frequency varies with eccentricity
so a different ﬁlter is used for each location in the image.
The authors of [28] develop an efﬁcient method for performing foveation ﬁltering using low-pass
ﬁlters with continuously varying cut-off frequencies. The authors employ a different formulation of
the cut-off frequency, which they term local bandwidth. The local bandwidth is modeled using the
equation fdx = γ/(ex + η), where ex is the eccentricity of the point x, and γ and η are parameters
that control the spatial frequency decay. In their paper, γ = 18 and η = 0.2 are used for these values.
This local bandwidth model is conceptually and mathematically similar to the previously introduced
cut-off frequency equation. In practice, either this local bandwidth model or the previously introduced
cut-off frequency may be used with little impact on the resulting image. The local bandwidth is then
calculated for each pixel in the image in terms of cycles per pixel f pn, where pn corresponds to the nth
pixel. This value is clamped between the maximum representable frequency 0.5 and some arbitrarily
small minimum frequency (0.07). Low-pass ﬁlters are then applied to the original image using the f pn
values as cut-off values to produce the foveation ﬁltered image. That is, the nth pixel in the foveation
ﬁltered image corresponds to the output of a low-pass ﬁlter centered on the nth pixel in the unﬁltered
image with cut-off frequency f pn.
An ideal low-pass ﬁlter requires an inﬁnite number of taps. However, as the authors point out, the
error of a non-ideal ﬁlter can be bounded by the number of taps used. This error bound depends on
the cut-off frequency of the ﬁlter. Therefore, for a ﬁxed error bound, the number of taps will vary with
eccentricity. Of course, a large enough number of taps could be used on every ﬁlter such that the worst
error bound is still below some constant, but the fewer the taps used, the faster the ﬁltering can be
performed. The frequency response of the ﬁlter is then smoothed using a hamming window. To reduce
the required number of multiplication and addition operations for each ﬁlter, the authors modify the
ﬁlters to be separable even symmetric and circularly symmetric. The authors found that the responses
of the two ﬁlters are very difﬁcult to differentiate perceptually. Therefore, the authors propose using
the separable even symmetric ﬁlter, because the required number of operations grows much faster with
respect to the number of taps for the circularly symmetric ﬁlter than the separable even symmetric
ﬁlter.

374
CHAPTER 14 Foveated Image and Video Processing and Search
In [36], the authors develop real-time foveation techniques for low bit rate video coding. In particular,
the authors develop techniques for producing foveated video on embedded devices. Similar to other
foveation techniques, the authors begin by developing an approximation of the frequency response of the
human visual system. The authors use the contrast sensitivity function and related cut-off frequency from
psychovisual measurements. However, the authors note that humans generally change ﬁxation points
three to ﬁves times per second via ballistic saccades. If the contrast sensitivity and cut-off frequency
must be calculated for each pixel then these computations can quickly become a bottleneck. Therefore,
the authors propose the use of an approximation of this model. The ﬁrst major approximation used by the
authors is the introduction of disjoint foveation regions, or regions with the same cut-off frequency. In
a macro-block based compression scheme, it is natural to allow foveation regions and macro-blocks to
coincide.Thatis,eachmacro-blockwouldonlyneedtocalculateasinglecut-offfrequency.Furthermore,
the authors propose discretizing the cut-off frequencies into eight levels. The authors found that fewer
levels resulted in visual distortions at block boundaries while more levels did little to improve bit rate.
The authors also propose discretizing viewing distance into distinct levels. The authors then construct
a look-up table that returns the squared pixel distance for a given cut-off frequency at a given viewing
distance. The size of this look-up table is simply the number of discrete cut-off frequencies times the
number of discrete viewing distances. The proper cut-off frequency for a foveation region can then be
found by calculating the squared distance from the center of the region to the ﬁxation point, and then
ﬁnding the smallest cut-off frequency that yields a larger distance when plugged into the look-up table.
Using this look-up table is signiﬁcantly faster as it only requires several multiplications and additions
with no division, square root, or arctangent computations. This approximation is especially useful when
the ﬁxation point, or the position of the viewer, is changing rapidly throughout the video sequence.
Using this approximate model for the cut-off frequency of a foveation region, the authors are able
to signiﬁcantly speed up ﬁltering in the spatial domain. The discretization of cut-off frequencies means
that only eight ﬁlters must be calculated. Furthermore, these ﬁlters may be precalculated and stored in
memory. Splitting the image into foveation regions also represents a signiﬁcant computational savings
in terms of eccentricity calculations. However, edge artifacts are occasionally visible at the boundary
between foveation regions. To alleviate these artifacts, pixels on the boundaries of foveation regions
are computed by averaging the output of the two adjacent ﬁlters. Interestingly, the authors found that
foveating the chrominance values had little effect on the compressibility of the image. Furthermore,
leaving the chrominance values uniform and foveating only the luminance values had little effect on
the visual quality of the image. Therefore, the authors propose foveating only the luminance values to
further reduce computational requirements.
The authors of [36] also introduce a DCT based method for performing foveation ﬁltering. The
DCT does not encode spatial information which makes it non-ideal for implementing a spatially vary-
ing low-pass ﬁlter. However, the introduction of foveation regions makes the application of the DCT
feasible, especially when the foveation regions coincide with macro-blocks. In many image and video
compression techniques, the image is divided into macro-blacks and the DCT is calculated for each
macro-block. The DCT coefﬁcients are then quantized for storage or transmission. The authors of [36]
propose the application of a weighting mask to the DCT coefﬁcients in order to approximate a low-
pass ﬁlter. This mask is calculated according to the eccentricity of the macro-block with respect to the
ﬁxation point. This DCT implementation is very efﬁcient. However, coefﬁcient masking in the DCT
domain does not produce an ideal low pass ﬁlter and blocking artifacts are evident, especially at the

4.14.4 Foveated Images and Video
375
boundaries of foveation regions. To partially solve this problem, the authors present triangular transition
weights which reduce these blocking artifacts. Unfortunately, these triangular transition weights do not
eliminate the blocking artifacts entirely.
Regarding image foveation techniques implemented in the wavelet domain, the cut-off frequency is
solved in terms of eccentricity to produce the cut-off eccentricity
ec( f ) = e2
α f ln
 1
CT0

−e2.
(14.34)
Cut-off eccentricity is a function of frequency. Each level in the wavelet domain corresponds to a
frequency, and each level, therefore, has a different cut-off eccentricity. All coefﬁcients in each level
whose eccentricity exceeds that level’s cut-off eccentricity are masked out. The location of the fovea
and corresponding eccentricity for each level can be calculated using the same equations used in the
FWQI. This method is employed in [37]. However, as the results of that paper illustrate, this method has
a tendency to produce blocking artifacts. In [9], the authors employ the wavelet error sensitivity function
used in the FWQI to weight the coefﬁcients in the wavelet domain. The smoothly varying nature of
the wavelet error sensitivity function helps to reduce these blocking artifacts. However, the algorithm
presented in [9] does not use foveation as a preprocessing step and instead integrates foveation more
tightly into the video coding scheme.
A similar masking technique can be employed for image pyramid representations. In [7], the authors
present a method for foveating images in a Laplacian image pyramid. A blending function is applied at
each level to minimize the visibility of artifacts at foveation region boundaries. This blending function
is given by
b(e) =
⎧
⎪⎨
⎪⎩
0.5 cos

π(e−ec+w)
w

+ 0.5, ec −w < e < ec,
1,
e ≤ec −w,
0,
ec ≤e.
(14.35)
Using this blending function instead of a simple mask greatly reduces the visibility of artifacts.
Using this foveation technique, temporal artifacts also appear when the ﬁxation point is moved. The
ﬁxation point must be moved by an increasing number of pixels before it moves one element in the
lower image levels. When it does move in these lower levels, the boundaries of the foveation region can
appear to jump in the ﬁnal image. The authors propose a solution where the size of the foveation region
is slightly increased and then interpolated as the ﬁxation point moves. Let L(x, y) correspond to the
value of some level at location (x, y). Then let L(xl, yl) and L(xh, yh) correspond to the lower left and
upper right corners of the foveation region at that level. Let δx and δy correspond to the sub-element
location of the ﬁxation point at that level. Then interpolating the values of L(x, y) along the boundary
according to the following equations will greatly diminish the visibility of temporal artifacts.
L(xl, y) = (1 −δx)L(xl, y),
∀yl ≤y ≤yh,
L(x, yl) = (1 −δy)L(x, yl),
∀xl ≤x ≤xh,
L(xh + 1, y) = δxL(xh + 1, y),
∀yl ≤y ≤yh,
L(x, yh + 1) = (1 −δy)L(x, yh + 1),
∀xl ≤x ≤xh.
(14.36)
Low-pass ﬁlters with continuously varying cut-off frequencies in the spatial domain generally yield
the best visual results, and if they are implemented intelligently can still be quite computationally

376
CHAPTER 14 Foveated Image and Video Processing and Search
efﬁcient. However, the ability to apply this technique in the DCT or DWT domains is quite useful when
attempting to integrate foveation more tightly with other elements of common image and video coding
schemes, such as motion estimation and rate control.
4.14.4.4 Foveated motion estimation and compensation
Using foveation purely as a preprocessing step to increase compressibility is effective, however further
efﬁciency gains can be made by incorporating foveation into other steps of the coding process. Moving
ﬁxations and non-uniform resolution impose some interesting and unique constraints on motion esti-
mation and compensation. In [36], the authors explore some of the inefﬁciencies introduced by using
foveation solely as a preprocessing technique with regards to motion estimation and compensation.
The primary problem introduced is exempliﬁed when the scene is static and only the ﬁxation point is
moving. When employing motion estimation, the encoder must send the difference between the motion
compensated frame and the actual frame. In the case of a static scene and a moving fovea, discrepancies
between the motion compensated frame and the actual frame arise in regions where the cut-off frequency
has changed. When the cut-off frequency is increased, the encoder is sending previously unavailable
information. However, when the cut-off frequency is decreased, the encoder is sending unnecessary
information. The same image could be formed at the decoder by applying the proper low-pass ﬁlter to
that foveation region. This would of course require the foveation ﬁltering to be more tightly integrated
into the encoding and decoding process instead of a simple preprocessing step.
In [36], the authors incorporate their DCT based foveation ﬁltering directly into the encoder to
alleviate this problem. The foveation mask is only applied to the difference image transmitted after
motion estimation and compensation. Therefore, if a region remains otherwise unchanged between
frames, but the ﬁxation point moves away (thus reducing that regions cut-off frequency), the encoder
willnotsendanyinformationforthatregion.Onetheotherhand,ifaregionremainsotherwiseunchanged
but the ﬁxation point moves closer (increasing that regions cut-off frequency), the encoder will send
information for that region. Furthermore, this results in static regions of a scene that were previously
ﬁxated to remain at higher resolutions at the decoder. For example, if the ﬁxation point were to scan
across a static scene then the entire scene would eventually be represented at maximum resolution. This
integration of information across ﬁxations is a very interesting feature of this algorithm that mirrors
the operation of the human visual system. Foveated vision itself does not make much sense unless the
observer is able to successfully integrate information across ﬁxations. Incorporating this feature directly
into the motion compensation process also further increases compression gains introduced by foveation.
In [28], the authors discuss a modiﬁed block matching scheme for foveated video. Motion estimation
and compensation is an important tool for compressing video sequences. It is most often performed by
matching image patches in the previous frame to image patches in the current frame. The matching is
performed using some distance or similarity measure, such as the mean absolute distortion (MAD). In
[38], the authors develop a method for performing motion estimation and compensation for foveated
video. The reason that a foveated motion estimation and compensation technique needs to be developed
at all is due to the non-uniform sampling of the retina. If an image patch moves away from the ﬁxation
point, or the ﬁxation point itself moves, then the resolution of that patch will be decreased. Conversely, if
an image patch moves toward the ﬁxation point then the resolution of that patch will be increased. When
attempting to match image patches, this shift in resolution needs to be taken into account. To this end,

4.14.4 Foveated Images and Video
377
the authors introduce the foveal mean absolute distortion (FMAD), which is simply the MAD weighted
by the local cut-off frequency and then normalized. Interestingly, the authors show that optimizing with
respect to the FMAD will produce the exact same image patch pairs as optimizing with respect to the
MAD. However, there are still gains to be made in computational efﬁciency by incorporating foveation.
The authors make the assumption that object movement between frames is small compared to foveation
movement. First, the previous and current frames are ﬁltered such that corresponding pixels between
frames have the same local bandwidth. By appropriately sub-sampling the ﬁltered image patches when
calculating the MAD, the authors were able to reduce the computational complexity of the algorithm by
nearly an order of magnitude. In the original paper, the ﬁltering is accomplished by foveating the original
uncompressed current and previous frames using the intersection of ﬁxation points from the previous
frame and the current frame. The fundamental insight is that there is no reason to waste computational
resources estimating the motion of high resolution content that will be low-pass ﬁltered in the next frame.
In [7], the authors perform motion estimation and compensation in a Laplacian image pyramid.
However, only patches contained within foveation regions have motion estimation performed. Further,
by performing motion estimation at the highest level ﬁrst, the search space of lower levels can be
restricted. Not only does this method side step the problem of motion estimation between patches
of varying resolution, but it also drastically reduces the number of patches that must be matched as
compared to a uniform resolution approach.
In [39], the authors introduce a method for performing motion estimation and compensation of a rate
scalable foveated coding technique. The output of this coding technique may be truncated at any point
and will still produce a valid bit stream. The rate scalable nature of this coding technique signiﬁcantly
complicates the task of motion estimation. In normal motion estimation, the encoder uses a reconstructed
version of the previous frame as feedback. The encoder then estimates the motion between this feedback
frame and the new frame and transmits the motion vectors along with the error signal. However, the
decoder may truncate the bit-stream at any point based on local bandwidth and computational demands.
This means that the encoders feedback image may not match the actual image produced at the decoder.
The authors introduce a clever solution to minimize the expected error between the feedback image and
the actual decoded image. First, a lower limit or baseline bit rate is introduced. The decoder will not
truncate the received bit-stream below this bit rate. Let Ws(v, x) be an error sensitivity measure in the
spatial domain given
Ws(v, x) =
 fc(v, x)
fc(v, x f )
γ
,
(14.37)
where γ is a shape parameter. If the viewing distance v is ﬁxed, then this parameter can be ignored.
Let PO(x) be the pixel intensity value at location x of the original reference frame, and let PB(x) be
the pixel intensity value at location x of the image decoded at the baseline bit rate. Then the combined
encoder prediction value PE(x) is given by
PE(x) = (1 −Ws(x))PO(x) + Ws(x)PB(x).
(14.38)
Let PD(x) be the pixel intensity value at location x of the reconstructed frame at the decoder at the
current decoding bit rate. Then the combined decoder prediction value PD(x) is given by
PD(x) = (1 −Ws(x))PC(x) + Ws(x)PB(x).
(14.39)

378
CHAPTER 14 Foveated Image and Video Processing and Search
The difference between the predicted frame at the encoder and the predicted frame at the decoder then
becomes:
PE(x) −PD(x) = (1 −Ws(x))(PO(x) −PC(x)).
(14.40)
At locations where the error sensitivity is high, the base rate is used and the error drift is extremely
close to zero. This means that locations near the ﬁxation point will be represented more accurately. On
the other hand, locations with low error sensitivity, that is locations in the periphery, will be represented
less accurately. However, due to the effects of foveation, the difference between PO(x) and PC(x) is
expected to be low in the periphery.
This motion estimation and compensation strategy is employed primarily to deal with the variable
decoding bit rate. It uses the structure of foveated images to limit the expected error in the prediction
scheme. However, unlike previous foveated motion estimation and compensation techniques, it does not
address the problem of variable cut-off frequencies due to moving fovea. Creating a motion estimation
and compensation technique that is capable of dealing with both of these issues is still an open problem.
4.14.4.5 Foveated rate control
Rate control is the ability to dynamically adjust the compression level of a coding scheme based on
bandwidth or computation limitations. It can be used to encode videos at approximately ﬁxed bit rates
or quality levels. An optimal rate control scheme encodes each frame at the highest possible quality for
some ﬁxed number of bits.
In [40], the authors also discuss optimal rate control of foveated video. In many video compression
standards it is common for the image to be divided into macroblocks which are then encoded individu-
ally. The quantization levels, or quantization parameters, are also allowed to vary across macroblocks
and these parameters form a vector. This vector is usually optimized using the MSE or PSNR of the
entire frame as the cost function. However, for foveated video, the MSE and PSNR may no longer be
accurate cost functions. Instead, the authors propose to use the FPSNR (Eq. (14.21)) to optimize the
quantization parameters. In streaming applications, performing a full optimization of the quantization
parameters is not always possible. Therefore, the authors also propose varying the quantization levels
across macroblocks according to the average local bandwidth, or cut-off frequency, of the macroblock.
The authors found that this method, while still sub-optimal, outperformed uniform quantization levels.
In [9], the authors develop an embedded foveated image coding scheme which they call EFIC. Their
algorithm is based on a modiﬁed SPIHT (Set partitioning in hierarchical trees) coding algorithm. SPIHT
is a wavelet based coding scheme that is rate scalable [41]. That is, the output bit-stream of the SPIHT
algorithm may be truncated at any point to produce a valid image for some ﬁxed bit rate. The truncated
bit-stream is approximately the highest quality image that could be produced at that bit rate. SPIHT
accomplishes this feat by ordering each bit in the image, or rather each bit in the wavelet domain, by
its approximate error sensitivity. The bits that contribute most to error are transmitted ﬁrst and the bits
that contribute the least to the error are transmitted last. The resulting bit-stream is then fed through an
entropy coder to achieve additional compression.
EFIC modiﬁes SPIHT by altering the error sensitivity function. EFIC uses the foveated error sen-
sitivity function introduced by the FWQI for this purpose. The end result is that bits from wavelet
coefﬁcients located nearer the ﬁxation point will be transmitted before bits from coefﬁcients located in
the periphery. An interesting side effect of this process is that if the bit-stream is not truncated then the

4.14.4 Foveated Images and Video
379
compression is approximately lossless and the reconstructed image will not appear foveated. However,
as the bit rate is decreased, the foveation effects will become more and more apparent. In order to
properly decode the bit-stream, the decoder must be able to produce exactly the same foveated error
sensitivity weighting function. This means the decoder must know the location of the ﬁxation point and
potentially the viewing distance used by the encoder. However, compared to the size of the frame, the
overhead of transmitting these values with each frame is small. In many cases, it may be feasible to set
these parameters, in particular the viewing distance, to ﬁxed values.
Foveated rate control has not been as well explored as foveation ﬁltering or even foveated motion
estimation and compensation. However, it is an important area of research. Foveation is at its most useful
in imaging environments having constrained bandwidth or computational resources. In an unconstrained
environment, uniform resolution coding techniques may as well be used. Foveated rate control allows
coding to dynamically vary between uniform resolution and foveation based on the underlying resource
constraints.
4.14.4.6 Multiple ﬁxation points
Although modeling the human visual system has proved useful, computer algorithms are not limited
by the same anatomical constraints. For this reason, it is useful to generalize the concepts developed
from the human visual system. For example, The eye has a single fovea with a ﬁxed distribution of
photoreceptors, but we need not use a ﬁxed distribution or even a single fovea in our algorithms.
Figure 14.14 is an example of an image containing multiple ﬁxation points.
FIGURE 14.14
An example of an image containing multiple ﬁxation points.

380
CHAPTER 14 Foveated Image and Video Processing and Search
Most of the foveated video processing algorithms we have discussed can support multiple fovea.
Multiple fovea can easily be incorporated into any foveated algorithm by generalizing the concept of
foveal distance (Eq. (14.6)). The foveal distance of each pixel is simply calculated as the distance to the
nearest fovea:
dm(x) = min

dx f 1(x), dx f 2(x), . . . , dx f n(x)

,
(14.41)
where dx f i (x) is the foveal distance calculated with respect to the ith fovea, and n is the total number
of fovea. Due to the monotonicity of the contrast threshold function, this distance corresponds to
the minimum contrast threshold, maximum contrast sensitivity, and maximum cut-off frequency with
respect to each of the n fovea. A more arbitrary smoothly varying resolution distribution can be formed
using multiple fovea.
Toexpandontheconceptofconstructinganarbitrarysmoothlyvaryingresolutiondistribution,wecan
let the ﬁxation point become a random variable. This is especially useful in quality applications. A human
has only one ﬁxation point, so incorporating multiple ﬁxation points into a foveated quality metric is
not straightforward. By allowing the ﬁxation point to be a random variable with some distribution, we
can instead compute the expected quality. We then foveate the image according to the ﬁxation point
probability distribution. In a more general sense, this makes it possible to sample the image using an
arbitrary smoothly varying distribution.
Let f (I, I ′, p) be some measure on a reference image I, the foveated image I ′, and at ﬁxation point
p. The measure is arbitrary. It could, for example, be a perceptual quality measure. If we let p become
a random variable p then the expected value of the measure is
E

f (I, I ′, p)

=
	
p∈p
f (I, I ′, p)Pp(p),
(14.42)
where Pp(p) is the probability that p takes the value p. For our example of a quality measure, we
would want to ﬁnd the image I ′ that maximizes this expected value. Eye tracking devices can be used
to empirically construct the distribution of p for a speciﬁc image, but algorithms capable of estimating
the distribution are preferable. In the section on ﬁxation selection, we will discuss ongoing research
that could be used to estimate this ﬁxation selection distribution.
4.14.4.7 Hardware foveation
Traditionally, foveation has been relegated to software. This is inefﬁcient; Why waste time capturing
information that is going to be thrown away? Foveation is at it’s most useful in power and process-
ing restricted environments. Capturing foveated imagery at a hardware level signiﬁcantly improves the
power and bandwidth efﬁciency of foveated algorithms compared to uniform resolution approaches. In
[42,43], foveated CMOS cameras are developed. These devices attempt to directly mimic the retina.
Fixed in the center of these imaging devices is a region of maximum resolution, with resolution decreas-
ing smoothly in all directions from the synthetic fovea. Similar to the eye, these devices would need
to be able to move in order to scan their ﬁxation point across a scene. The images reconstructed from
these devices look very similar to that produced by the spatial warping technique for foveation ﬁltering.
Unfortunately, these similarities include the sharp boundary artifacts in the periphery of the image. In
[44], the authors introduce an adaptable foveated imaging device. The concept of this device is similar

4.14.5 Fixation Selection
381
to the previous cameras, however the main difference is that there is no ﬁxed synthetic fovea. In fact,
this chip is not even restricted to a single fovea. The image can be split up into arbitrary regions of
high and low resolution. This property allows the adaptable foveated imaging device to scan across a
scene without actually having to move. Although we may expect the efﬁciency of these devices to be
greater than that of a uniform resolution imaging device, this comparison is never explicitly studied.
The authors of [45] introduce a similar adaptable foveated imaging device and explore its application
to visible and infrared systems. This device has a highly conﬁgurable spatial resolution. Pixels can be
arbitrary grouped into what the authors refer to as super-pixels. The pixels that make up a super-pixel
share an underlying photocharge. This allows a single sample to effectively record the average response
from all of the individual pixels in the super-pixel. When discussing the useful properties of this device
for applications, the authors focus primarily on the increased frame rate. The frame rate is often limited
by the available bandwidth, especially in embedded applications. Transmitting and processing even a
gray-scale image of resolution of 512 × 512 at 60 frames per second requires available bandwidth in
excess of 15 MHz. Foveating the image at the time of capture considerably decreases this bandwidth
requirement, thereby increasing the effective frame rate. In [46], the authors compare performance
between conventional and foveated large format infrared focal plane arrays. The authors show that the
bandwidth requirements of a foveated array are similar to that of a much smaller uniform resolution
array. This result is precisely what we would expect from a foveated imaging device. However, with-
out integrating the device into an actual system, it is difﬁcult to make an accurate comparison of the
advantages and disadvantages.
4.14.5 Fixation selection
Fixation selection is an important aspect of any foveated algorithm. The ﬁxation point selects which
regions will be represented with high resolution and which regions will be represented with low resolu-
tion. Intuitively, optimal ﬁxation selection is highly dependent on application. For video compression,
we would like to choose the ﬁxation that maximizes perceptual quality. However, in visual search tasks,
we would like to use a ﬁxation selection strategy that maximizes detection rate while minimizing search
time. It is unlikely that there exists a single ﬁxation selection strategy that is optimal for both of these
applications.
4.14.5.1 Human ﬁxation prediction
We discussed the anatomy and operation of the human visual system in order to understand how, and to
a certain extent why, foveated images are formed. Similarly, we will explore models of human behavior
when performing a variety of visual tasks. We hypothesize that humans have evolved a foveated visual
system because it is highly effective for performing common visual tasks in natural environments. Due
to the foveated nature of the human visual system, ﬁxation selection is a very important process during
these visual tasks. By understanding how humans select ﬁxations, we can gain insight into how to best
construct ﬁxation selection strategies. Models of human ﬁxation strategies can also be directly integrated
into foveated algorithms, especially for maximizing perceptual visual quality.
Human ﬁxation prediction is an area of active research. The primary goal of researchers in this area
is to develop algorithms that predict where on a scene a human observer will ﬁxate. To accomplish this

382
CHAPTER 14 Foveated Image and Video Processing and Search
task, human ﬁxation data is collected using specialized eye tracking hardware. Interestingly, regions of
ﬁxation are very similar between different subjects, but they are not identical. For this reason, it would be
difﬁcult, if not impossible, to construct a model that can perfectly predict ﬁxations. However, it should
be possible to construct a model of the ﬁxation distribution that correlates highly with observed human
ﬁxations. Additionally, it has been shown that the statistics of ﬁxation selection change signiﬁcantly with
task. This is in agreement with the intuition that optimal ﬁxation strategy depends on application or task.
Intuitively, a person asked simply to observe an image will ﬁxate on very different regions than a person
tasked with ﬁnding a target in that image. The bulk of experiments attempt to model ﬁxations for such
simple tasks as viewing, detection, and search. However, a number of experiments have begun modeling
ﬁxation selection for considerably more complex tasks, such as playing cricket or making a sandwich.
The most common method for ﬁxation prediction employs the concept of image saliency [47].
Saliency can be though of as conspicuity, or the measure of how much something stands out from its
surroundings. In [47], the authors extract a large number of features that are reminiscent of the center-
surround responses of retinal ganglion cells. For each feature, a map is calculated across the entire
image. These maps are then combined to produce a ﬁnal saliency map. The maps are weighted such that
maps that appear to be relatively uniform are weighted less than maps that appear to have some useful
information. Once the ﬁnal saliency map is calculated, the ﬁxation is selected as the point of maximum
saliency. The saliency of the ﬁxated region is then masked to zero so that a new ﬁxation point will be
chosen in the next time step. Without this inhibition of return mechanism, the saliency map prediction
would never change ﬁxations.
Despite the popularity of the saliency map approach in the literature, it suffers from a number of
problems. In [48], the authors discuss ﬁve major assumptions that are made in most saliency based
human ﬁxation prediction algorithms. The ﬁrst assumption is that ﬁxation selection is driven purely
by pre-attentive features. That is, low level image features completely control the direction of gaze.
The authors argue that even under ideal conditions where these low-level features maximally predict
ﬁxation the correlation is still low and the introduction of a visual task can remove this correlation
altogether [49]. As previously discussed, ﬁxation selection clearly depends on the task at hand. The
second assumption is that there exists a default mode of looking in the absence of a task. In experiments
where the subject is asked to simply view an image, the assumption is that this results in a task-free
mode of looking. However, the authors argue that it is more likely that this simply leaves the task up to
interpretation by the subject. They may try to memorize the image or instead look for inconsistencies
in the image. The end result is that the mode of operation of the subject is not reliable and therefore
difﬁcult to analyze. The third assumption involves the method of target selection. In most saliency
algorithms, the foveated nature of the human visual system is ignored and the region with the highest
saliency is chosen as the ﬁxation point [47]. In order to allow for multiple ﬁxation points, an inhibition
of return is imposed on the salience map so that another region is chosen for ﬁxation in the next time
step. The authors believe that the variance of retinal sampling with eccentricity is important in any
model that seeks to explain ﬁxation selection. This seems obvious, as the foveated nature of the human
visual system is one of the primary reasons we move our eyes at all. That is, if our eyes sampled
the entire scene with uniform resolution there would be very little beneﬁt to ﬁxating different points.
Furthermore, experiments have shown that, depending on the statistics of the local environment, the
existing inhibition of return model is not accurate [50]. The fourth assumption is that information for
ﬁxation selection lies entirely in the spatial domain. That is, there is an assumption that ﬁxations can

4.14.5 Fixation Selection
383
be accurately predicted for some time step using only information available at that time step. More
recent research has shown that this is not the case [51]. For example, observers will frequently ﬁxate
locations that they predict will contain useful information. This prediction requires prior knowledge of
the scene and environment. For example, batsmen playing cricket consistently ﬁxate the point on the
ground where the ball will bounce before the ball arrives. Additionally, the duration of ﬁxations can
vary signiﬁcantly. When ﬁxation of a location continues to provide useful information, such as when
avoiding an obstacle, the viewer may hold their ﬁxation for several seconds or more [49]. The ﬁnal
assumption is that eye saccades to points of ﬁxation are precise. We assume that the location that the
eye ﬁxates is the exact location that it intended to ﬁxate. In some cases this is accurate, as evidenced
by small corrective saccades that bring the eye into exact alignment with its target [48]. However, in
natural tasks such as object avoidance, it has been shown that the eye will not correct ﬁxations that are
off by as much as three degrees.
Despite the various problems associated with studying ﬁxation selection, the ﬁeld continues to
steadily advance. In [52], the authors discuss three recent advances that have improved our understand-
ing of the linkage between ﬁxation selection and cognitive function. The ﬁrst is a better understanding
of the effect of task on ﬁxation selection. This advance has grown largely out of improved eye track-
ing hardware. Traditionally, eye tracking hardware has required the subject being analyzed to remain
stationary. This restricts the domain of possible natural tasks that can be analyzed. The most common,
and perhaps least useful, task is that of simply viewing an image. In this setting, it is very difﬁcult for
the experimenter to control or assess the behavior of the subject in a reliable way. New low-proﬁle
head-mounted eye tracking hardware allows a much wider range of tasks including pouring tea, making
a sandwich, and playing cricket [51,52]. In these much more speciﬁc and directed tasks, not only are
ﬁxation selections closely related to task, but they are also relatively consistent between different sub-
jects. The second advance is the application of the concept of reward in analyzing eye movement [53].
Task oriented ﬁxations also demonstrate that ﬁxation selection must be learned. For example, in cricket,
it was found that batsmen ﬁxated on the point that the ball would bounce before the ball reached that
location. The batsmen must have learned to predict this location and ﬁxate there as no image features
could provide this information. Most machine learning algorithms utilize a cost or reward function for
direct learning. In much the same way, we expect some kind of cost or reward system to be present for
training eye ﬁxations. New research is currently exploring possible physiological sources of a reward
system. The results of this area of research have been very promising in ﬁxation selection prediction.
Finally, the third advance has been in the area of virtual reality graphics environments and virtual humans
[54]. These environments allow the simulation of complete visuo-motor loops through extended periods
of time. In other words, the virtual scene is rendered to an image using a model of the human visual
system. This image is then incorporated in the virtual agents working memory for performing a task.
The user agent then attempts to perform the task using the available information. The effects of the user
agents actions are also simulated and update the environment. This loop continues as the virtual agent
attempts to complete its task. The advantage of a virtual agent versus for example, a robotic agent is
that they are simpler to construct and the scenario may be reliably reproduced. In this setting, concepts
from game theory and other ﬁelds of computer science can be used to explain human behavior. For
example, the authors of [52] discuss a humanoid simulation named Walter who learns how to navigate
along a path ﬁlled with obstacles. The hierarchical processing levels used in this simulation could be
extremely useful for exploring more general foveated visual task algorithms.

384
CHAPTER 14 Foveated Image and Video Processing and Search
4.14.5.2 Video compression
For perceptually lossless video compression, we would like to match the ﬁxation point used for foveated
processing to the actual ﬁxation point of the viewer as closely as possible. For some applications,
eye tracking hardware can be used to directly measure the ﬁxation location of the viewer. In other
applications, such as teleoperation and surveillance, the point of ﬁxation can be controlled directly
by the operator. However, in many applications, the use of eye tracking hardware or manual ﬁxation
selection is not practical. Instead, it would be useful to be able to predict where a viewer is likely to look.
The previously discussed models of human ﬁxation prediction are a step in the right direction, but the
existing studies tend to focus on subjects viewing static images. Temporal effects in video, including
motion, clearly play a large role in human ﬁxation selection. Additionally, most models completely
ignore high level visual attractors, such as human faces, which are ﬁxated disproportionately compared
to any low level image statistic. A number of ﬁxation selection schemes have been proposed in the
foveated video compression community, but as of yet none have been general enough to ﬁnd wide
spread use.
In [39], the authors also introduce a method for automatic ﬁxation selection in video. For I frames,
ﬁxations are selected on human faces. The authors note that the human face draws an inordinate amount
of attention in most videos and select it as the primary feature for ﬁxation selection. To detect human
faces, ﬁrst regions containing skin color are identiﬁed. Then, within these regions, binary template
matching is used to identify faces, and a measure of local variance is used to identify false positives.
However, the authors introduce a different ﬁxation selection mechanism for P frames. In some sense,
P frames measure the introduction of new information into the scene. Therefore, the goal of ﬁxation
becomes maximizing knowledge of this new information. The authors propose placing ﬁxation points
in regions with high prediction error signals. However, the authors also propose weighting the error
signal from face regions higher than other regions in order to capture the importance of even small
changes in a face.
In [8], the authors introduce a modiﬁcation to the popular H.264 video coding algorithm that incor-
porates a model of visual saliency. Unlike many of the previously discussed saliency models for static
images, the authors use a motion based approach to saliency estimation. Motion consistency and mag-
nitude is also considered to reduce ﬁxations on non-salient regions due to temporal noise. The imple-
mentation is extremely efﬁcient because the motion vectors calculated during motion estimation are
also used for constructing the saliency map. Foveation about the chosen ﬁxations is then achieved by
appropriately scaling the quantization parameters of the macro-blocks. The authors tested their algo-
rithm on a short video clip from a game of soccer where the moving players and ball are all excellent
targets for ﬁxation selection. However, ﬁxating motion may be less appropriate in other types of video.
In [15], the authors explore ﬁxation selection by minimizing contrast entropy. The authors recognize
that foveation introduces spatially varying uncertainty in the viewed image. That is, there is more
uncertainty about the true image in the periphery than there is near the fovea. When the goal is to
maximize knowledge of a scene, it is a reasonable ﬁxation strategy to minimize uncertainty or entropy
in the image. However, calculating the prior and conditional probability for all possible image statistics
is a very difﬁcult problem. Instead, the authors chose to focus on the statistics of local contrast. As the
authors point out, this is not unreasonable because local contrast is one of the primary statistics collected
by the early visual system. By analyzing a large collection of natural images, the authors were able
to model the prior probability of local contrast with respect to eccentricity as well as the conditional

4.14.5 Fixation Selection
385
probability of local contrast across varying levels of eccentricity. The authors used the contrast sensitivity
function to ﬁlter images in the dataset at discrete levels of eccentricity. By randomly selecting points in
images at a speciﬁc eccentricity and calculating the local contrast, the authors approximated the prior
probability distribution of local contrast for each discrete level of eccentricity. By randomly selecting
a location in an image and then computing the local contrast at that point for all discrete levels of
eccentricity, the authors approximated the conditional probability distribution of local contrast across
the discrete levels of eccentricity. Speciﬁcally, this allowed them to calculate the distribution of local
contrast at zero degrees eccentricity given the contrast measured at some other level of eccentricity.
Using these approximated distributions, the authors constructed a Bayesian ideal observer. The ideal
observer calculates the expected reduction in entropy or uncertainty for each possible ﬁxation point and
then selects the ﬁxation that maximizes this probability. Although the observer is only able to estimate
the reduction in entropy before making a ﬁxation, the authors showed that this ideal observer was, in fact,
very nearly optimal with regards to entropy reduction. Furthermore, the authors also showed that this
method of ﬁxation selection signiﬁcantly outperformed simple ﬁxation selection methods with regards
to the mean squared error of the reconstructed image. This is a very interesting paper that combines
insights from the human visual system with information theory and ideal observer analysis.
4.14.5.3 Search and detection
Search and detection is an interesting problem in a foveated environment. Potential target locations
should be assessed with local bandwidth restrictions in mind. That is, a high spatial frequency target
appears quite different in the periphery than near the ﬁxation point. Then, based on the probability of
potential target locations, a new ﬁxation point should be selected that maximizes the probability of
detection. Despite having broader applicability, most foveated search algorithms have been developed
for modeling human ﬁxations. Little work has been done to implement more general foveated search
algorithms or to compare them to traditional uniform resolution approaches.
The work in [14] is an example of a foveated visual search algorithm developed for modeling
human ﬁxations. The authors construct a Bayesian ideal visual searcher for the simpliﬁed visual search
task of ﬁnding a Gabor patch embedded in background noise. Ideal observer analysis is a method for
analyzing human behavior in visual tasks, including search and detection. The main concept of this
method involves the construction of an ideal observer that performs the task to be analyzed optimally
given the requirements of the task and available information. The performance of human subjects is
then compared to the ideal observer for analysis. A Bayesian ideal observer utilizes Bayesian statistical
decision theory. The Bayesian ideal visual searcher uses template matching weighted by visibility to
calculate the probability of the presence of the target at every point in the visual ﬁeld. This prior
probability is accumulated across every ﬁxation. At each time step, the ideal searcher considers every
possible ﬁxation point and chooses the point that maximizes the probability of detection. Interestingly,
the human test subjects performed similarly to this Bayesian ideal visual searcher. This experiment
shows that human visual search is nearly Bayesian optimal within this constrained test environment.
Despite being developed for analyzing human behavior, the concepts developed for this ideal visual
searcher could easily be applied to a general foveated visual search algorithm.
The authors of [16] implemented a foveated targeting and tracking algorithm intended for more
general applications. Furthermore, the authors compare their algorithm to a uniform resolution targeting

386
CHAPTER 14 Foveated Image and Video Processing and Search
and tracking algorithm. The experimental setup is similar to that of [14]. The algorithms are looking for
Gabor patches embedded in background noise. In general, the foveated search implementation compared
favorablytotheuniformsearchimplementationforﬁxedimagebandwidth.Furthermore,whenhardware
foveation is employed, the increased frame rate allows the foveated algorithm to potentially identify
targets much faster than the uniform resolution algorithm. The advantage of foveated search is, however,
signiﬁcantly reduced when the bandwidth is not restricted. This should not be surprising because, as we
have previously discussed, foveation is generally most effective in resource constrained environments.
Further research is still required to compare foveated and uniform search algorithms under more realistic
conditions. Locating Gabor patches in background noise may be too simplistic to properly capture the
advantages and disadvantages of foveated search.
4.14.6 Applications
In this section, we present some practical applications of foveated video processing. As previously
discussed, foveation is a form of compression and is therefore highly applicable to the transmission
and storage of video. However, due to the properties of foveation, it is more appropriate in some
applications than others. Compressing video for consumption by human viewers is an application
where foveation can be extremely useful. Due to the foveated nature of the human visual system,
perceptually lossless compression can be attained. However, perceptually lossless foveation coding
requires knowledge of the viewer’s location and ﬁxation point. Obviously, the viewer’s location and
ﬁxation point may vary considerably between different viewings of a video. Therefore, storing video
intended for human consumption in a foveated compression scheme is not necessarily a good idea.
However, for video that is streaming and being encoded on the ﬂy, foveated video coding can be used
to considerably reduce bandwidth requirements with little loss in perceptual quality.
4.14.6.1 Teleconferencing
Foveated video coding has been shown to be extremely useful in teleconferencing [55]. Teleconferencing
is an excellent example of an application where video is encoded in real-time, the ﬁxation location
of the viewer is relatively easy to predict, and bandwidth constraints signiﬁcantly impact quality. In
teleconferencing, it is a reasonable assumption that a viewer will ﬁxate on the face of the person they
are talking to. This causes the ﬁxation point to be relatively static and makes eye tracking hardware
unnecessarytoproducenearlyperceptuallylosslesscompression.Furthermore,latencyduetobandwidth
constraints can make communication difﬁcult or awkward. Therefore, the reduced bandwidth required
by foveated video can make a large impact on the quality of the teleconferencing call.
4.14.6.2 Teleoperation
Similar to teleconferencing, foveation is also extremely useful for video coding in teleoperation. In tele-
operation, video must be encoded and transmitted in real time and bandwidth constraints are extremely
important. Latency in the video feed or a dropped frame could result in the operator making a costly
mistake, such as crashing a remotely operated vehicle. Unlike teleconferencing, there is no longer
an assumed ﬁxation location. Instead, the operator can be given direct control of the ﬁxation point.

4.14.6 Applications
387
However, in many teleoperation applications, the ﬁxation point can simply be set to the center of the
image. The operator than manipulates the vehicle in order to properly position the ﬁxation point.
4.14.6.3 Super high resolution images
Foveation is also being utilized on images and videos that are not necessarily intended for direct
consumption by human viewers. Super resolution imagery, such as that delivered by satellites or UAVs,
is incredibly cumbersome to transmit and store. Much of the information in these images is not very
useful, e.g., large swathes of empty ﬁelds. By foveating around objects of interest, such as cars, high
resolution is maintained where it is useful. The lower resolution imagery maintained in the periphery is
still useful for context but the reduction in resolution allows for huge gains in compression. An important
area of ongoing research is the use of search and tracking algorithms on these super resolution images
with hundreds of, if not more, ﬁxation points.
4.14.6.4 Infrared and wide band imaging
Another excellent example of a scenario where bandwidth and computation are at a premium is in deep
infrared and wide band imaging modalities. In these regimes, imaging is expensive with respect to both
time and power. Foveation can be extremely helpful for reducing both of these limitations. However,
in order to be effective, foveation must be introduced at the sensor level. In the section on hardware
foveation, we discussed several devices capable of capturing foveated imagery, including devices that
operate in the infrared spectrum. Another approach is to incorporate foveation into compressive sensing
[17,18]. Compressive sensing is used to minimize the cost of the device by avoiding large and expensive
sensor arrays. However, the time it takes to sample and reconstruct an image increases dramatically. By
using a spatially varying sampling mechanism, such as foveation, it is possible to reduce the number
of required samples, and thus reduce the sampling and reconstruction time. This approach is similar to
hardware foveation, because at no point is the full resolution image ever read into the device.
4.14.6.5 Detection and search
Some preliminary research has gone into the use of foveated imaging systems for targeting and acqui-
sition in more general usage scenarios [16]. The general conclusion from these experiments is that
foveated search and tracking systems perform favorably compared to uniform resolution systems when
bandwidth and computation time are limited. Foveated video allows a detection and search algorithm
to operate on a wide ﬁeld of view simultaneously. A detection and search system utilizing a uniform
resolution camera with the same bandwidth restrictions would have to physically sweep the camera
in order to search the same ﬁeld of view. Additionally, the high resolution fovea allows the foveated
algorithm to be able to discriminate potential targets equally as well as the uniform resolution algorithm.
The primary downside to a foveated approach is that multiple ﬁxations, and thus more frames, may be
necessary in a detection task. However, foveated imaging systems are able to run at faster frame rates.
In [16], the authors showed that these effects tend to cancel out and the detection time, in terms of
seconds and not frames, of a foveated algorithm is very close to that of non-foveated algorithms. This
is an emerging application and considerable work remains to be done to properly compare foveated
search and detection algorithms to uniform resolution approaches.

388
CHAPTER 14 Foveated Image and Video Processing and Search
4.14.7 Open issues and problems
The problem of efﬁciently foveating images has been largely solved by variable low-pass ﬁltering
techniques. However, performing this low-pass ﬁltering in other domains, such as the DWT and DCT
domains, is useful when integrating foveation into other coding schemes. In these other domains, low-
pass ﬁlter approximations can diverge signiﬁcantly from the ideal and this divergence often results
in visual artifacts. Blending has been used to diminish the visibility of these artifacts, but a strong
mathematical derivation of the divergence of these approximations from ideal low-pass ﬁlters would be
extremely helpful.
Incorporating foveation into motion estimation and compensation methods can further increase the
improved compression and efﬁciency afforded by foveation. We have introduced a foveated motion
estimation method that accounts for the varying local bandwidth to minimize the transmitted information
required for motion compensation. We have also introduced a foveated motion estimation method
that uses the varying local resolution to reduce the number of operations required during estimation.
However, no existing motion estimation and compensation algorithm exists that is able to support both
of these advances. The integration of these two methods is especially interesting because of the broad
applicability to foveated video coding.
Another problem related to foveated motion estimation is incorporating visual memory into foveated
video coding schemes. The concept of foveation does not make sense without some form of memory.
A foveated algorithm must be able to integrate information across multiple ﬁxations in order to make
informed decisions. Foveated motion estimation and compensation allows a certain level of information
to be integrated across frames and thus ﬁxation points. If properly implemented, a static point in a scene
that is ﬁxated should remain at high resolution even if the ﬁxation point moves away. However, if that
point is changed substantially and the ﬁxation point is no longer located nearby, then that location
should degrade to the appropriate resolution. The previously presented foveated motion estimation and
compensation algorithm largely solves this problem when the original unfoveated frame is available.
However, in certain applications, such as when foveated hardware is employed to directly capture
foveated images, this is not the case and a new approach must be developed.
When employing foveation as a perceptually lossless form of compression, the location and ﬁxation
point of the viewer must be known. It is this requirement that has kept foveation from being used in a
broader array of applications. However, eye tracking hardware has improved substantially recently and
soon may be a reasonable source of information for ﬁxation location. If this does happen, techniques
will need to be developed to account for multiple viewers as well as potential latency between eye
tracking data and frame presentation.
A possible alternative to eye tracking hardware is the use of improved human ﬁxation prediction
algorithms. Despite a large volume of research devoted to the topic, models of human ﬁxation selection
arestillnotaccurateenoughtoreliablypredictﬁxations.Althoughtherehavebeenanumberofpromising
advances in this ﬁeld, it is unlikely that any model will be able to perfectly predict human ﬁxations.
Instead, it is more convenient to think in terms of a ﬁxation distribution. Techniques for efﬁciently and
accurately calculating this ﬁxation distribution, and techniques for maximizing expected perceptual
quality given this ﬁxation distribution, remain to be developed.
Foveation has been largely developed based on intuitions arrived at by studying the human visual
system. However, strong mathematical concepts that support the optimality of foveated algorithms

4.14.8 Implementation/Code
389
have not yet been developed. Such a formulation would allow foveated algorithms to be compared
to non-foveated algorithms more directly rather than relying on intuition. Furthermore, it would help
in determining precisely under what conditions foveation improves efﬁciency. A sufﬁciently complete
theorywouldalsoallowanoptimalresolutiondistributiontobeobtainedforsolvingaparticularproblem.
We would expect the optimal distribution for most tasks not to diverge too far from the human visual
system. However, for tasks that the human visual system performs poorly at, we might expect the optimal
resolution distribution to diverge signiﬁcantly.
Another open issue is integration of foveated video coding and foveated algorithms for general
visual tasks. Due to the foveated nature of the human visual system, foveation is readily applicable to
compression techniques aimed at video for human consumption. Additionally, we expect that foveated
video and foveated techniques should also be helpful in a variety of visual tasks, such as detection and
search. Foveated detection and search has been explored with regards to modeling human behavior, but it
hasseenlittleuseinnon-humanbasedtasks.Moreworksneedstobedoneinordertoexploreandquantify
the relationship between foveation and efﬁciency in visual tasks. For example, how does the ﬁxation
selection scheme and foveation roll-off effect the detection rate of a foveated detection algorithm?
Furthermore, these foveated algorithms need to be compared to uniform resolution approaches to assess
to what extent foveation provides any beneﬁt.
4.14.8 Implementation/code
In this section, we provide complete MATLAB implementations, along with helpful comments, for
a number of the models and algorithms that we have discussed. These algorithms are intended to be
straightforward to understand and easy to implement. They should allow the reader to begin experiment-
ing with foveation in a hands-on fashion. The full MATLAB implementations for all of these algorithms
are provided in an online repository at https://github.com/aﬂoren/foveation_toolbox. If you use these
algorithms, we request that you cite this article. These algorithms employ the Stanford WaveLab MAT-
LAB toolbox for the discrete wavelet transform [56].
4.14.8.1 Psychophysical models
Most of the algorithms that we have discussed rely on the contrast threshold or contrast sensitivity
function in some way. Here we provide MATLAB implementations for several of the more commonly
used psychophysical functions. Additionally, the psychophysics toolbox [57], is an extremely useful
resource for both collecting psychophysical data and ﬁtting psychophysical models to collected data.
4.14.8.1.1
Contrast threshold
function [ CT ] = contrast_threshold ( f, e )
%CONTRASTTHRESHOLD Calculates the contrast threshold
%
f −f requency measured in cycles per degree
%
e −eccentricity measured in degrees
%
CT −T he contrast that is just barley visible
CT0 = 1/64;

390
CHAPTER 14 Foveated Image and Video Processing and Search
e2 = 2.3;
alpha = 0.106;
CT = CT0 ∗exp ( alpha ∗f .∗( abs(e) + e2 ) / e2 );
end
4.14.8.1.2
Contrast sensitivity
function [ CS ] = contrast_sensitivity ( f, e )
%CONTRAST_SENSITIVITY Calculates the contrast sensitivity
%
f −f requency measured in cycles per degree
%
e −eccentricity measured in degrees
%
CS −thecontrastsensitivity
CS = 1 ./ contrast_threshold ( f, e );
end
4.14.8.1.3
Cut-off frequency
function [ fc ] = cutoff_frequency ( e )
%CUTOFF_FREQUENCY Calculates the cutof f f requency
%
e −eccentricity measured in degrees
%
f c −the cutof f f requency
CT0 = 1/64;
e2 = 2.3;
alpha = 0.106;
fc = ( e2 ∗log( 1/CT0 ) ) ./ ( alpha ∗( abs(e) + e2 ) );
end
4.14.8.1.4
Cut-off eccentricity
function [ ec ] = cutoff_eccentricity ( f )
%CUTOFF_ECCENTRICITY Calculates the cutof f eccentricity
%
f −f requency measured in cycles per degree
%
ec −the cutof f eccentricity
CT0 = 1/64;
e2 = 2.3;
alpha = 0.106;
ec = e2 / ( alpha ∗f ) ∗log( 1/CT0 ) – e2;
end

4.14.8 Implementation/Code
391
4.14.8.2 Conversion functions
A number of conversion functions are required throughout the following implementations. Implementa-
tions of these conversion functions are presented here. Although conversion factors can be incorporated
directly into the necessary functions, keeping all conversions explicit greatly increases the readability
of the code.
4.14.8.2.1
Eccentricity
function [ e ] = eccentricity ( d, N, v )
%ECCENTRICITY Calculates eccentricity in degrees
%
d −distance to f ixation point measured in pixels
%
N −the pitch or density of the pixels
%
v −distance of the viewer f rom the image
%
e −eccentricity measured in degrees
e = atan ( d/ (N∗v) ) ∗180 / pi;
end
4.14.8.2.2
Cycles per pixel
function [ fp ] = cycles_per_pixel ( fd, N, v )
%CYCLES_PER_PIXEL Converts cycles per degree to cycles per pixel
%
f d −f requency measured in cycles per degree
%
N −the pitch or density of the pixels
%
v −distance of the viewer f rom the image
%
f e −f requency measured in cycles per pixel
fp = 180 ∗fd / ( pi ∗N ∗v );
end
4.14.8.2.3
Cycles per degree
function [ fd ] = cycles_per_degree ( fp, N, v )
%CYCLES_PER_PIXEL Converts cycles per degree to cycles per degree
%
f p −f requency measured in cycles per pixel
%
N −the pitch or density of the pixels
%
v −distance of the viewer f rom the image
%
f d −f requency measured in cycles per degree
fd = fp ∗pi ∗N ∗v / 180;
end
4.14.8.3 Foveated quality assessment
Foveated quality assessment algorithms are useful for quantifying the quality of foveation algorithms
for comparison. Here we provide implementations for several of the more common foveated quality
assessment algorithms.

392
CHAPTER 14 Foveated Image and Video Processing and Search
4.14.8.3.1
FMSE
function [ score ] = fmse ( I, Ip,N, v, xf )
%F MSE Calculates the f oveated mean squared error
%
I −the ref erence image
%
I p −the distorted image
%
N −the pitch or density of the pixels
%
v −distance of the viewer f rom the image
%
x f −array of f ixation points
%
score −the F MSE score
r = I – Ip;
[x,y] = meshgrid ( 1: size (I ,1) ,1: size (I,2) );
d = sqrt( (x−xf (1)).ˆ2 + (y−xf (2) ).ˆ2 );
e = eccentricity ( d, N, v );
fc = cutoff_frequency ( e );
score = sum(sum(r .ˆ2 .∗fc .ˆ2) ) / sum(sum(fc . ˆ2) );
end
4.14.8.3.2
FPSNR
function [ score ] = fpsnr ( I, Ip, N, v, xf )
%FPSNR Calculates the f oveated peak signal to noise ratio
%
I −the ref erence image
%
I p −the distorted image
%
N −the pitch or density of the pixels
%
v −distance of the viewer f rom the image
%
x f −array of f ixation points
%
score −the F PSN R score
score = 10 ∗log10 ( 255 / fmse (I, Ip, N, v, xf) );
end
4.14.8.3.3
FWQI
function [ score ] = fwqi ( I, Ip, N, v, xf, L )
%FWQI Calculates the f oveated wavelet quality index
%
I −the ref erence image
%
I p −the distorted image
%
N −the pitch or density of the pixels
%
v −distance of the viewer f rom the image
%
x f −array of f ixation points
%
L −the number of wavelet decomposition levels
%
score −the FW QI score

4.14.8 Implementation/Code
393
S = size ( I ,1 ):
sf = wavelet_contrast_sensitivity (S, L, N, v, xf );
qmf = MakeONFilter ( ’Daubechies ’ ,8);
wc = FWT2_PO ( double ( I ), log2(S)-L, qmf );
wcp = FWT2_PO ( double ( Ip ), log2(S)-L, qmf );
fwd = sqrt ( sum(sum( (sf.∗(wc −wcp) ) .ˆ2) ) / numel (wc) );
score = exp( −fwd);
end
4.14.8.4 Foveation ﬁltering
Foveation ﬁltering is broadly applicable as a preprocessing step before further compression and coding
in both images and video. Here we provide implementations for performing foveation ﬁltering in several
different domains. Although these algorithms may be used directly to foveate images and videos, they
are intended to be straightforward enough to incorporate into more advanced image and video coding
techniques.
4.14.8.4.1
Spatial domain foveation ﬁlter
function [ If ] = spatial_foveation_ﬁlter ( I, N, v, xf, b )
%SPATIAL_FOVEATION_FILTER Per f orms f oveation f iltering in
%
the spatial domain
%
I −the ref erence image
%
N −the pitch or density of the pixels
%
v −distance of the viewer f rom the image
%
x f −array of f ixation points
%
b −number of discrete cutof f f requency levels
%
L −the number of wavelet decomposition levels
%
I f −the f oveation f iltered image
S = size ( I ,1 ):
[x,y]=meshgrid (1:S ,1:S ) ;
d = sqrt( (x−xf (1 , 1) ) .ˆ2 + (y−xf (1 ,2) ) .ˆ2 ) ;
for i = 2:size(xf ,1 )
d = min( d , sqrt( (x−xf (i ,1 ) ) .ˆ2 + (y−xf (i ,2) ) .ˆ2) ) ;
end
e = eccentricity ( d, N, v ) ;
fc = cycles_per_pixel ( cutoff_frequency (e), N, v ) ;
bins = linspace ( min(fc (:)), max(fc (:)), b + 1) ;
If = I;
for i = 1 : b

394
CHAPTER 14 Foveated Image and Video Processing and Search
BW = (fc >= bins (i) ) & ( fc <= bins (i + 1) ) ;
sigma = 2 ∗sqrt (2 ∗log(2)) / (2 ∗pi ∗bins ( i ) ) ;
h = fspecial (’ gaussian ’ ,[50 50] , sigma ) ;
If = roiﬁlt2 (h, If ,BW) ;
end
end
4.14.8.4.2
DWT domain foveation ﬁlter
function [ If ] = dwt_foveation_ﬁlter ( I, N, v, xf, L )
%DWT_FOVEATION_FILTER Per f orms f oveation f iltering in
%
the wavelet domain
%
I −the ref erence image
%
N −the pitch or density of the pixels
%
v −distance of the viewer f rom the image
%
x f −array of f ixation points
%
L −the number of wavelet decomposition levels
%
I f −the f oveation f iltered image
S = size( I ,1 ) ;
qmf = MakeONFilter (’ Daubechies’,8) ;
wc = FWT2_PO ( double ( I ), log2 ( S )−L, qmf ) ;
sf = dwt_contrast_sensitivity (S, L, N,v, xf ) ;
wcf = sf .∗wc;
If = uint8 (IWT2_PO ( wcf, log2( s )−L, qmf ) ) ;
end
4.14.8.4.3
DCT domain foveation ﬁlter
function [ If ] = dct_foveation_ﬁlter ( I, N, v, xf, B )
%DCT_FOVEATION_FILTER Per f orms f oveation f iltering in
%
the DCT domain
%
I −the ref erence image
%
N −the pitch or density of the pixels
%
v −distance of the viewer f rom the image
%
x f −array of f ixation points
%
B −the macro block size
%
I f −the f oveation f iltered image
If = uint8 (Zeros (size ( I ) ) ) ;
S = size( I ,1 ) ;
NB = S/B;

4.14.9 Data Sets
395
BM = construct_block_masks (S ,B ) ;
for
i = 1:NB
for
j = 1:NB
x = (i-1)∗B + B/2;
y = (j-1)∗B + B/2;
d = sqrt( ( x−xf (1 ,1) ) .ˆ2 + ( y−xf (1 ,2 ) ) .ˆ2 ) ;
for k = 2:size ( xf,1 )
d = min( d, sqrt( ( x−xf ( k ,1 ) ) .ˆ2 + ( y−xf ( k ,2 ) ) .ˆ2 ) );
end
e = eccentricity (d ,N , v ) ;
fc = cycles_per_pixel (cut-off_frequency ( e ) , N,v ) ;
kc = ﬂoor( 2∗fc ∗B ) ;
wk = 1 :B ;
wk ( wk<=kc ) = 1 ;
wk ( wk==kc ) = 0 .5 ;
wk ( wk > kc + 1 ) = 0 ;
Wk = wk,∗wk ;
cc = dct2 (reshape ( I (BM{i, j}) ,B, B ) ) ;
If (BM{i, j}) = idct2(cc .∗Wk) ;
end
end
end
4.14.9 Data sets
In this section, we provide a list and related links to publicly available sites of data sets.
4.14.9.1 Psychophysics
Many of the algorithms in this article rely in some way on the contrast threshold function, and this
function is estimated from psychophysical measurements. Surprisingly, we do not know of any publicly
available repositories of psychophysical measurements. However, data used for estimating the contrast
threshold function is presented in [1,6,58].
4.14.9.2 Perceptual quality
Properly analyzing perceptual quality assessment algorithms requires a database of images or videos
that have had their quality rated by human viewers. Using these opinion scores, the correlation between
a quality assessment algorithm and the actual scores given by human viewers can be examined. Col-
lecting these opinion scores from a large sample audience for a variety of images or videos is a non-
trivial task. Fortunately, a number of databases are publicly available. The LIVE [59], IVC [60], and

396
CHAPTER 14 Foveated Image and Video Processing and Search
TID2008 [61] image databases, located at http://live.ece.utexas.edu/research/quality/subjective.htm,
http://www2.irccyn.ec-nantes.fr/ivcdb/, and http://www.ponomarenko.info/tid2008.htm respectively,
all contain reference and distorted images that have been rated by human viewers. The website for
the Video Quality Experts Group [62], or VQEG, is located at http://www.its.bldrdoc.gov/vqeg/. The
VQEG website provides a number of databases containing reference and distorted video clips along
with viewer opinion scores as well as links to a number of other useful resources for perceptual quality
assessment. Another video quality assessment database, the LIVE video database [63], is located at
http://live.ece.utexas.edu/research/quality/live_video.html.
4.14.9.3 Human ﬁxation prediction
Design and analysis of human ﬁxation prediction algorithms requires human ﬁxation data. This data is
usually collected from specialized eye tracking devices. The Database of Visual Eye Movements [64] is
available at http://live.ece.utexas.edu/research/doves/. This database contains eye movements of 29 sub-
jects viewing 101 natural calibrated images. Another database containing eye movements [65] from 15
subjects on 1003 images is available at http://people.csail.mit.edu/tjudd/WherePeopleLook/index.html.
A large database of recorded eye movements from 16 subjects watching 100 short video clips is available
at https://crcns.org/data-sets/eye/eye-1. This database is split into two separate studies. The ﬁrst dataset
is comprised of eye movement recordings from 8 distinct subjects watching 50 different video clips
[66,67]. The second dataset is comprised of eye movement recordings from 8 different subjects watching
50 video clips that were made by randomly scrambling 1 to 3 s sections of the original video clips [68,69].
4.14.10 Conclusions and future trends
The human visual system employs a variable resolution, or foveated, imaging system, and is able to
outperform computer algorithms in complex visual tasks. Therefore, we expect foveation to be a useful
technique for efﬁciently processing visual information. Based on this intuition, we have explored how
the human visual system captures and processes data in a non-uniform manner. Speciﬁcally, we have
considered models of the optics of the eye, the distribution of photoreceptors, the visual ﬁelds of retinal
ganglion cells, and the responses of neurons in the primary visual cortex.
Based on insights gleaned from studying the human visual system, we have constructed efﬁcient
algorithms capable of processing visual information in a similar fashion. Images processed in this way
have been ﬁltered to match the frequency response of the human visual system. In this way, foveation
provides a method for compression that is perceptually lossless. Only data which would be ﬁltered by
the human visual system, and would thus be imperceptible, is removed. However, to properly employ
foveation as a perceptual compression technique, the position and ﬁxation point of the viewer must be
known. This has severely restricted a broader adoption of foveation as a method for video compression.
However, foveation has advantages beyond maximizing perceptual quality. Intuitively, the human
visual system must have good reason for ﬁltering out information. One interpretation is that the distri-
bution of useful content in an image is not generally uniform. In this setting, foveation can be employed
as a non-uniform compression technique that retains more information in the important regions of an
image. What makes a region of an image important, or not important, is highly dependent on application
or task. For this reason, it is difﬁcult to discuss the utility of foveation in the absence of a speciﬁc task.

4.14.10 Conclusions and Future Trends
397
However, the human visual system is capable of efﬁciently performing a wide variety of tasks, therefore
foveation should similarly be useful in a wide variety of tasks. Researchers have begun exploring how
humans use foveated vision when performing various tasks, but these results have yet to be applied
more generally. There have been some attempts to utilize foveation in search and detection tasks, but
little work has been done to properly compare these implementations to more conventional algorithms.
We predict that, in the future, foveated compression techniques, and, variable resolution compression
techniques in general, will become more popular for a number of reasons. One of the biggest problems
currently keeping foveation from being more widely applied in streaming video is the requirement that
the ﬁxation point be known. Eye tracking hardware is continuously improving and is nearly to the point
of being a reasonable solution for many applications. Someday soon, it may even be directly integrated
into displays. It may seem like a very complex solution just to improve compression. However, we are
starting to approach the limits of lossless compression and society’s demand for streaming videos does
not seem to be slowing down. Cutting bandwidth requirements while maintaining the same level of
perceptual quality will help alleviate some of the stress on overburdened networks. Wireless networks
in particular will beneﬁt due to their ﬁxed channel capacity. At the same time, human ﬁxation prediction
models are also rapidly improving. Although no model will ever be able to perfectly predict a viewer’s
ﬁxation point, a model that can produce a reasonable ﬁxation distribution will be invaluable. An accurate
ﬁxation distribution will allow video coding techniques to focus more of their available bandwidth on
regions in the scene that are perceptually important. The concept is similar to variable bit rate encoding
schemes, where higher bit rates are used during more complex scenes in order to maintain a constant
level of perceptual quality. Fixation distributions will allow encoders to utilize variable bit rates in the
spatial domain as well as the temporal domain.
The anatomy and electrochemical properties of the human visual system have been thoroughly
explored. We do not expect any major changes to the existing models in the near future. However, insights
into the higher levels of visual processing, those beyond the primary visual cortex, may still provide
new insights for foveation and foveated algorithms. Speciﬁcally, research into how the brain chooses
ﬁxation points and performs visual tasks will be extremely helpful for creating foveated algorithms.
Furthermore, as we better understand how and why the brain utilizes a foveated visual system, we will
better understand where foveated algorithms will be effective. Intuitively, we expect foveation to be
useful where visual resources are constrained. In many applications, this is certainly the case, and as
our understanding of foveation improves so will the prevalence of foveated algorithms.
Glossary
Eccentricity
the degree to which something is eccentric, or deviates from the norm. In the context
of this article, it is the degree to which a point in a scene deviates from the optical axis
Entropy
a measure of the disorder of a system. Here, it is used as a measure of the uncertainty
associated with a random variable
Fovea
an anatomical region on the inside of the back of the eye. It is aligned with the optical
axis of the eye and is the region of the retina with the greatest spatial acuity
Foveation
the act of being foveated, or having variable resolution. Derived from the variable
resolution of the retina about the fovea

398
CHAPTER 14 Foveated Image and Video Processing and Search
Optical Axis
a line along which there exists rotational symmetry in an optical system. In the human
eye, the optical axis coincides with the point of gaze
Psychophysics the scientiﬁc study of the relation between stimulus and sensation
Retina
a layer of cells on the inside of the back of the eye. Converts the image formed by the
optical system of the eye into electrical signals that are processed by the brain
Saccade
the rapid movement of the eye from one point of gaze to another. Our eyes tend to
move from distinct point to distinct point using these rapid eye movements
Relevant Theory: Signal Processing Theory
See Vol. 1, Chapter 5 Sampling and Quantization
See Vol. 1, Chapter 6 Digital Filter Structures and Their Implementation
See Vol. 1, Chapter 7 Multirate Signal Processing for Software Radio Architectures
See Vol. 1, Chapter 8 Modern Transform Design for Practical Audio/Image/Video Coding Applications
See Vol. 1, Chapter 9 Discrete Multi-Scale Transforms in Signal Processing
References
[1] M. Banks, A. Sekuler, S. Anderson, Peripheral spatial vision: limits imposed by optics, photoreceptors, and
receptor pooling, J. Opt. Soc. Am. 8 (11) (1991) 1775–1787.
[2] H.B. Barlow, Understanding natural vision, Phys. Biol. Process. Images 11 (1983) 2–14.
[3] C. Enroth-Cugell, J. Robson, The contrast sensitivity of retinal ganglion cells of the cat, J. Physiol. 187 (3)
(1966) 517.
[4] D.H. Hubel, T.N. Wiesel, Receptive ﬁelds of single neurones in the cat’s striate cortex, J. Physiol. 148 (3)
(1959) 574–591.
[5] J.P. Jones, L.A. Palmer, An evaluation of the two-dimensional Gabor ﬁlter model of simple receptive ﬁelds
in cat striate cortex, J. Neurophysiol. 58 (6) (1987) 1233–1258.
[6] J.G. Robson, N. Graham, Probability summation and regional variation in contrast sensitivity across the visual
ﬁeld, Vis. Res. 21 (3) (1981) 409–418.
[7] W.S. Geisler, J.S. Perry, A real-time foveated multiresolution system for low-bandwidth video communication,
Proc. SPIE 3299 (1998) 294–305.
[8] H. Ha, J. Park, S. Lee, Perceptually scalable extension of H. 264, IEEE Trans. Circ. Syst. Video Technol. 21
(11) (2011) 1667–1678.
[9] Z. Wang, A.C. Bovik, Embedded foveation image coding, IEEE Trans. Image Process. 10 (10) (2001)
1397–1410.
[10] U. Rajashekar, L.K. Cormack, A.C. Bovik, Point of gaze analysis reveals visual search strategies, Hum. Vis.
Electron. Imag. IX 5292 (1) (2004) 296–306.
[11] G.J. Zelinsky, A theory of eye movements during target acquisition, Psychol. Rev. 115 (4) (2008) 787–835.
[12] D.H. Ballard, M.M. Hayhoe, Modelling the role of task in the control of gaze, Vis. Cogn. 17 (6) (2009)
1185–1204.
[13] B.T. Vincent, T. Troscianko, I.D. Gilchrist, Investigating a space-variant weighted salience account of visual
selection, Vis. Res. 47 (13) (2007) 1809–1820.
[14] J. Najemnik, W.S. Geisler, Optimal eye movement strategies in visual search, Nature 434 (7031) (2005)
387–391.

References
399
[15] R. Raj, W.S. Geisler, R.A. Frazor, A.C. Bovik, Contrast statistics for foveated visual systems: ﬁxation selection
by minimizing contrast entropy, J. Opt. Soc. Am. 22 (10) (2005) 2039–2049.
[16] S. Dubuque, T. Coffman, P. McCarley, A.C. Bovik, C.W. Thomas, A comparison of foveated acquisition and
tracking performance relative to uniform resolution approaches, Proc. SPIE 7321 (2009).
[17] I.B. Ciocoiu, Foveated compressed sensing, in: European Conference on Circuit Theory and Design, vol. 1,
2011, pp. 29–32.
[18] R. Larcom, T. Coffman, Foveated image formation through compressive sensing. in: IEEE Southwest Sym-
posium on Image Analysis Interpretation, 2010, pp. 145–148.
[19] B.P. Lathi, Signal Processing and Linear Systems, Berkeley Cambridge Press, 1998.
[20] I.E.G. Richardson, H. 264 and MPEG-4 Video Compression, vol. 20, Wiley Online, Library, 2003.
[21] E. Hecht, Optics, fourth ed., vol. 1, Addison Wesley, 2001.
[22] Wandell, B.A. Foundations of Vision, vol. 21. Sinauer Associates, 1995.
[23] D.H. Hubel, T.N. Wiesel, Shape and arrangement of columns in cat’s striate cortex, J. Physiol. 165 (3) (1963)
559–568.
[24] J. Daugman, Uncertainty relation for resolution in space, spatial frequency, and orientation optimized by
two-dimensional visual cortical ﬁlters, J. Opt. Soc. Am. 2 (7) (1985) 1160–1169.
[25] C.E. Shannon, A mathematical theory of communication, Bell Syst. Tech. J. 27 (1) (1948) 379–423.
[26] P. Reinagel, A.M. Zador, Natural scene statistics at the centre of gaze, Netw. Comput. Neural Syst. 10 (4)
(1999) 341–350.
[27] D.J. Field, Relations between the statistics of natural images and the response properties of cortical cells,
J. Opt. Soc. Am. 4 (12) (1987) 2379–2394.
[28] S. Lee, A.C. Bovik, Fast algorithms for foveated video processing, IEEE Trans. Circuits Syst. Video Technol.
13 (2) (2003) 149–162.
[29] P.J. Burt, E.H. Adelson, The Laplacian pyramid as a compact image code, IEEE Trans. Commun. 31 (4)
(1983) 532–540.
[30] P. Burt, Smart sensing within a pyramid vision machine, Proc. IEEE 76 (8) (1988) 1006–1015.
[31] Z. Wang, A.C. Bovik, L. Lu, Foveated wavelet image quality index, in: Proceedings of SPIE Applications of
Digital Image Processing, 2001.
[32] A.B. Watson, G.Y. Yang, J.A. Solomon, J. Villasenor, Visibility of wavelet quantization noise, IEEE Trans.
Image Process. 6 (8) (1997) 1164–1175.
[33] J.J. Clark, M.R. Palmer, P. Lawrence, A transformation method for the reconstruction of functions from
nonuniformly spaced samples, IEEE Trans. Acoust. Speech Signal Process. 33 (5) (1985) 1151–1165.
[34] Y. Zeevi, E. Shlomot, Nonuniform sampling and antialiasing in image representation, IEEE Trans. Signal
Process. 41 (3) (1993) 1223–1236.
[35] R.S. Wallace, P.W. Ong, B.B. Bederson, E.L. Schwartz, Space variant image processing, Int. J. Comput. Vis.
13 (1) (1994) 71–90.
[36] H.R. Sheikh, B.L. Evans, A.C. Bovik, Real-time foveation techniques for low bit rate video coding, Real-Time
Imag. 9 (1) (2003) 27–40.
[37] E.-C. Chang, C.K. Yap, A wavelet approach to foveating images, Comput. Geometry (1997) 397–399.
[38] S. Lee, A.C. Bovik, Motion estimation and compensation for foveated video, IEEE Int. Conf. Image Process.
2 (1999) 615–619.
[39] Z. Wang, L. Lu, A.C. Bovik, Foveation scalable video coding with automatic ﬁxation selection, IEEE Trans.
Image Process. 12 (2) (2003) 243–254.
[40] S. Lee, M.S. Pattichis, A.C. Bovik, Foveated video compression with optimal rate control, IEEE Trans. Image
Process. 10 (7) (2001) 977–992.
[41] A. Said, W. Pearlman, A new, fast, and efﬁcient image codec based on set partitioning in hierarchical trees,
IEEE Trans. Circ. Syst. Video Technol. 6 (3) (1996) 243–250.

400
CHAPTER 14 Foveated Image and Video Processing and Search
[42] F. Pardo, B. Dierickx, D. Scheffer, CMOS Foveated image sensor: signal scaling and small geometry effects,
IEEE Trans. Electron Dev. 44 (10) (1997) 1731–1737.
[43] R. Widnicki, G.W. Roberts, M.D. Levine, A foveated image sensor in standard CMOS technology, in: Pro-
ceedings of the IEEE Custom Integrated Circuits Conference, 1995, vol. 1, pp. 357–360.
[44] T.G. Constandinou, P. Degenaar, C. Toumazou, An adaptable foveating vision chip, in: IEEE International
Symposium on Circuits and Systems, 2006.
[45] P.L. McCarley, M.A. Massie, J.P. Curzan, Large format variable spatial acuity superpixel imaging: visible and
infrared systems applications, Nova (2004).
[46] M. Massie, J. Curzan, R. Coussa, Operational and performance comparisons between conventional and foveat-
ing large format infrared focal plane arrays, Proc. SPIE 5783 (2005) 260–271.
[47] L. Itti, C. Koch, E. Niebur, A model of saliency-based visual attention for rapid scene analysis, IEEE Trans.
Pattern Anal. Mach. Intell. 20 (11) (1998) 1254–1259.
[48] B.W. Tatler, M.M. Hayhoe, M.F. Land, D.H. Ballard, Eye guidance in natural vision: reinterpreting salience,
J. Vis. 11 (2011) 1–23.
[49] C.A. Rothkopf, D.H. Ballard, Image statistics at the point of gaze during human navigation, Vis. Neurosci.
26 (1) (2009) 81–92.
[50] S. Farrell, C.J.H. Ludwig, L.A. Ellis, I.D. Gilchrist, Inﬂuence of environmental statistics on inhibition of
saccadic return, Proc. Natl. Acad. Sci. 107 (2) (2010) 929–934.
[51] R.S. Johansson, G. Westling, A. Bäckström, J.R. Flanagan, Eye-hand coordination in object manipulation,
J. Neurosci. 21 (17) (2001) 6917–6932.
[52] M. Hayhoe, D. Ballard, Eye movements in natural behavior, Trends Cogn. Sci. 9 (4) (2005) 188–194.
[53] J.A. Droll, M.M. Hayhoe, J. Triesch, B.T. Sullivan, Task demands control acquisition and storage of visual
information, J. Exp. Psychol. 31 (6) (2005) 1416–1438.
[54] N. Sprague, D. Ballard, A. Robinson, Modeling embodied visual behaviors, ACM Trans. Appl. Percept. 4 (2)
(2007) 11.
[55] A. Basu, K.J. Wiebe, Enhancing videoconferencing using spatially varying sensing, IEEE Trans. Syst. Man
Cyb. 28 (2) (1998) 137–148.
[56] J. Buckheit, D.L. Donoho, Wavelab and reproducible research, in: A. Antoniadis (Ed.), Wavelets and Statistics,
vol. 103, Springer, 1995, pp. 55–81.
[57] D. Brainard, The psychophysics toolbox, Spat. Vis. 10 (4) (1997) 433–436.
[58] T.L. Arnow, W.S. Geisler, Visual detection following retinal damage: predictions of an inhomogeneous retino-
cortical model, SPIE Int. Soc. Opt. Eng. (1996) 119–130.
[59] H.R. Sheikh, Z. Wang, L. Cormack, A.C. Bovik, LIVE Image Quality Assessment Database Release 2,
<http://live.ece.utexas.edu/research/quality>.
[60] P. Le Callet, F. Autrusseau, Subjective quality assessment IRCCyN/IVC database, <http://www.irccyn.
ec-nantes.fr/ivcdb/>, 2005.
[61] N. Ponomarenko, V. Lukin, A. Zelensky, K. Egiazarian, M. Carli, F. Battisti, TID2008-a database for evaluation
of full-reference visual quality assessment metrics, Adv. Mod. Radioelectron. 10 (4) (2009) 30–45.
[62] A. Webster, F. Speranza, Video Quality Experts Group, <http://www.its.bldrdoc.gov/vqeg/>.
[63] K. Seshadrinathan, R. Soundararajan, A.C. Bovik, L.K. Cormack, Study of subjective and objective quality
assessment of video, IEEE Trans. Image Process. 19 (6) (2010) 1427–1441.
[64] I. Van Der Linde, U. Rajashekar, A.C. Bovik, L.K. Cormack, DOVES: a database of visual eye movements,
Spat. Vis. 22 (2) (2009) 161–177.
[65] T. Judd, K. Ehinger, F. Durand, A. Torralba, Learning to predict where humans look, in: IEEE International
Conference on Computer Vision, 2009.
[66] L. Itti, Automatic foveation for video compression using a neurobiological model of visual attention, IEEE
Trans. Image Process. 13 (10) (2004) 1304–1318.

References
401
[67] L. Itti, Quantitative modeling of perceptual salience at human eye position, Vis. Cogn. 14 (4–8) (2006)
959–984.
[68] R. Carmi, L. Itti, The role of memory in guiding attention during natural vision, J. Vis. 6 (9) (2006) 898–914.
[69] R. Carmi, L. Itti, Visual causes versus correlates of attentional selection in dynamic scenes, Vis. Res. 46 (26)
(2006) 4333–4345.

15
CHAPTER
Segmentation-Free Biometric
Recognition Using Correlation
Filters
Andres Rodriguez and B.V.K. Vijaya Kumar
Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA, USA
4.15.1 Introduction
Biometrics are characteristics that differ from person to person (or group of persons), and are important
in many security applications such as accessing a secure building or system, identifying a person of
interest in a scene, and determining whether a person’s actions present a threat. Correlation ﬁlters
(CFs) have been investigated for biometric recognition. Attractive properties of CFs such as shift-
invariance, noise robustness, graceful degradation, and distortion tolerance can be useful in a variety of
biometric recognition applications including face recognition [1–3], face localization [4], face tracking
[5], biometric encoding [6], ﬁngerprint recognition [7], iris recognition [8], ocular recognition [9],
pedestrian localization [10], and pedestrian action recognition [11,12]. The term “correlation ﬁlter” is
in part due to the history of using optical correlators to obtain cross-correlations. A review of optical (and
digital) correlators, and the basic linear algebra, signal processing, probability theory, and estimation
background to understand CFs can be found elsewhere [13].
In the CF approach, a carefully designed biometric template (loosely called a “ﬁlter” and in some
literature a “sliding window template”) h(m, n) is cross-correlated with a query image x(m, n) (e.g., a
face image) to produce the output g(m, n). This operation is efﬁciently carried out in the Fourier domain,
G(u, v) = X(u, v)H∗(u, v),
(15.1)
where∗denotestheconjugate,and G(u, v), X(u, v),and H(u, v)arethe2-DdiscreteFouriertransforms
(DFTs) of the correlation output, the query image, and the template, respectively. When the query image
is an authentic match (also called true-class or Class-1) g(m, n) should exhibit a sharp peak at the center
of the biometric signature’s location, and when the query image is an impostor (also called false-class
or Class-2) g(m, n) should not have any signiﬁcant peak. Figure 15.1 shows a diagram summarizing the
overall training and testing stages. The higher the correlation peak, the higher the probability that the
image is authentic. The location of the peak indicates the location of the target in the image. Thus CFs
offer the ability to simultaneously localize and identify or verify a biometric. The peak-to-correlation-
energy (PCE) or the peak-to-sidelobe ratio (PSR) (explained in Section 4.15.3.4) may be used to provide
some normalization in the peak values for different testing scenarios. Throughout this paper we used the
following terminology. Classiﬁcation means ﬁnding the class-label of a biometric signature. We refer to
a “biometric signature” as a “target.” Localization means ﬁnding the target’s location in a given scene
(we do not assume that the query signatures are centered). Recognition means both classiﬁcation and
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00015-7
© 2014 Elsevier Ltd. All rights reserved.
403

404
CHAPTER 15 Segmentation-Free Biometric Recognition
FIGURE 15.1
A ﬁlter is designed using ocular images of the same subject. A test image is correlated with the ﬁlter. If the
test image is an ocular image from the same subject, the correlation plane has a sharp peak, otherwise the
correlation plane does not have any signiﬁcant peak.
localization.Therearetwotypesofclassiﬁcationproblems:veriﬁcationandidentiﬁcation.Inveriﬁcation
applications we threshold the output to decide whether the biometric is authentic or not. In identiﬁcation
applications we compare multiple correlation outputs and select the best peak as the biometric match.
CFs are used for biometric video applications in one of two general ways. The ﬁrst is to use the conti-
nuity in the video frames to improve recognition performance (e.g., a video of a person walking through
an airport scanner provides several image frames that may result in better recognition performance than
a single frame). Recently, a novel Multi-Frame Correlation Filter (MFCF) framework useful for video
processing was introduced by Kerekes and Kumar [14] which combines information from the current
correlation output with previous correlation outputs to enhance recognition. Similarly using a tracker
[15] can improve recognition, i.e., when both the tracker and the CFs indicate the biometric-of-interest
is at a given location, the conﬁdence measure increases. The second use of video is in the area of activity
recognition (e.g., a video of one or more people each doing a different activity such running or walking
has security applications such as detecting unusual behavior). This area has not been widely explored
but recent results are promising [12].
This paper is organized as follows. We give an extensive review of CF theory and biometric applica-
tions in Section 4.15.2, and explain some of the pre- and post-processing techniques that are sometimes

4.15.2 Advanced Correlation Filters
405
used in Section 4.15.3. In Section 4.15.4 we review the usages of CF for biometrics in multiple video
applications including the MFCF, Kalman Correlation Filter (KCF), and activity recognition. In Section
4.15.5 we describe in detail the experimental setup and show our results for one type of such application:
identifying people in controlled video scenarios based on the ocular region. In Section 4.15.6 we offer
concluding remarks.
4.15.2 Advanced correlation ﬁlters
During the past three decades, advances in CF design have led to their effective use in biometric
recognition. In this approach, a carefully designed biometric template is cross-correlated with a query
image to produce a correlation output. This output is then searched for the highest peak or some other
relevant metric such as the peak-to-correlation energy (PCE) or peak-to-sidelobe ratio (PSR) (explained
in Section 4.15.3.4) to determine whether the query image is from the authentic class or not. CFs offer
several advantages including graceful degradation and built-in shift invariance; that is, when an input
image is translated by a certain amount, the correlation output is translated by that same amount.
The most basic CF is the Matched ﬁlter (MF). Although the MF is optimal for detecting a known
image in the presence of additive white noise, its recognition rate decreases signiﬁcantly when the
image has small distortions (e.g., from rigid body motion, lighting conditions, background, etc.). Thus,
it requires one MF for every possible distortion, making their use costly and impractical.
In 1980, Hester and Casasent introduced the design of one correlation ﬁlter from multiple training
images called the Equal Correlation Peak Synthetic Discriminant Function (ECPSDF) ﬁlter [16]. This
ﬁlter (and most CFs) depends on a set of training images that captures the expected distortions (in
testing) and builds a template from these images. Although the ECPSDF ﬁlter recognition rate perfor-
mance is usually inadequate for images not in the training set, it opened the way for future CFs. The
Minimum Variance Synthetic Discriminant Function (MVSDF) ﬁlter [17] was designed to minimize
the output noise variance in the correlation plane, but also suffers from poor recognition performance.
The Minimum Average Correlation Energy (MACE) ﬁlter [18] was a signiﬁcant advance in CF designs.
This ﬁlter is designed to reduce the energy of the correlation output resulting in a sharp peak at the loca-
tion of the target facilitating target recognition. The Optimal-Tradeoff Synthetic Discriminant Function
(OTSDF) ﬁlter [19] and the Minimum Noise and Correlation Energy (MINACE) ﬁlter [20] are exten-
sions of the MACE ﬁlter to achieve robustness to additive noise. The Minimum Squared Error Synthetic
Discriminant Function (MSESDF) ﬁlter [21] is a general form of the MACE ﬁlter that allows the user to
specify the desired correlation output. Each of these ﬁlters were constrained to have a certain value for
the inner product between the training image and the ﬁlter. This inner or dot product value is referred
to as the value at the origin (for centered images) or the correlation peak in the correlation plane (also
known as the correlation output). For example the correlation peak can be constrained to be 1 for the
authentic images and 0 for impostor images so that in testing the ﬁlter produces a high value (near 1) for
authentic images and a low value (near 0) for impostor images. Note that referring to this dot product
as the correlation peak is a slight abuse in terminology. The correlation peak is known as the highest
value in the correlation plane, and this dot product value, in theory, is not necessarily the highest value.
However, in practice the correlation peak is usually at the origin for centered images from the desired
class and therefore we can get away with this abuse in terminology.

406
CHAPTER 15 Segmentation-Free Biometric Recognition
Another advance in CF designs was removing the correlation peak constraints. Removing these peak
constraints increases the solution space and improves the chances of ﬁnding a ﬁlter with better recogni-
tion performance. The ﬁrst of these ﬁlters are the Maximum Average Correlation Height (MACH) ﬁlter
[22], the Unconstrained MACE (UMACE) ﬁlter [22], and the Unconstrained MSESDF (UMSESDF)
ﬁlter [22]. More recent designs include the Average of Synthetic Exact ﬁlter (ASEF) [4] and the Mini-
mum Output Sum of Squared Error (MOSSE) ﬁlter [10]. These unconstrained ﬁlters are unconstrained
forms of the mentioned constrained CFs (meaning that they have the same objective function but do not
have the hard correlation peak constraints) with the addition that MACH ﬁlter also minimizes a measure
of the scatter of the correlation outputs. Another linear CF is the Distance Classiﬁer Correlation Filter
(DCCF) [23] which transforms the training images so that their classes become more compact and more
separated from each other.
A different type of CF design is used when the ﬁlter requires invariance to in-plane rotations or
invariance to scale changes. Circular Harmonic Function (CHF) ﬁlters are used for in-plane rotations
[24,25] and Mellin Radial Harmonic Function (MRHF) ﬁlters are used for scale changes [26,27]. More
recent designs that are built on these methods are the Optimal Tradeoff CHF (OTCHF) ﬁlters [28] and
the MACE Mellin Radial Harmonic (MACE-MRH) ﬁlters [29].
Another type of CF is non-linear CFs. Typically they exhibit superior recognition rate performance,
but require more computation. The Polynomial Correlation Filters (PCFs) [30] design uses a set of
linear ﬁlters applied to point nonlinear versions of the input, and their outputs are added to produce a
single output. The Quadratic Correlation Filter (QCF) [31] determines and uses a quadratic nonlinearity
to maximize the separation between two classes.
Before we review these ﬁlters in more detail, we ﬁrst we introduce our notational conventions:
•
lower case non bold letters represent spatial domain 2-D arrays (also called planes), e.g., xi(m, n) ,
•
upper case non bold letters represent frequency domain 2-D arrays, e.g., Xi(u, v),
•
lower case bold letters represents vectors usually formed by vectorizing 2-D frequency domain
arrays, e.g, xi represents Xi(u, v).
–
we loosely refer to xi as the ith image when we mean the vectorized 2-D DFT representation
of the ith image,
–
we loosely refer to g as the correlation plane when we mean the vectorized 2-D DFT represen-
tation of the correlation plane,
–
we loosely refer to h as the template (or ﬁlter) when we mean the vectorized 2-D DFT repre-
sentation of the template,
•
lower case bold letters with a tilde represents vectors formed by vectorizing 2-D spatial domain
arrays, e.g, ˜xi represents xi(m, n),
•
upper case bold letters represents matrices, e.g., X = [x1 · · · xN],
–
some matrices are diagonal and represent vectors, e.g., Xi = diag(xi),
•
the ¯· symbol represents the mean of a set, e.g., ¯x = 1
N
N
i=1 xi,
•
the ˙· symbol represents the desired output, e.g., ˙g represents the desired g,
•
the ·∗symbol represents the conjugate, e.g., X∗
i ,
•
the ·H symbol represents the conjugate transpose, e.g., hH,
•
the ·T symbol represents the transpose, e.g., hT .

4.15.2 Advanced Correlation Filters
407
4.15.2.1 Matched ﬁlter and efﬁcient computation of correlation ﬁlters
Summary: The most basic CF is the matched ﬁlter (MF) whose origins are in detecting signals in received
radar returns. It is well known that the MF maximizes the Signal-to-Noise Ratio (SNR) for additive
white noise. In radar the MF output can be used to estimate the relative time shift between transmitted
and received signal, and from that estimate the distance to the target.
In practice, MFs do not work well for target recognition. These ﬁlters perform poorly when a target
is distorted. Therefore, too many MFs would be required to account for all the different distortions (e.g.,
from rigid body motion, illumination, and/or background changes). However, MFs serve as a good
theoretical foundation to explain how linear CFs works and their shift-invariance property.
We describe how to apply a MF through an example. This same technique is used to apply many
CFs. Suppose that we want to know if and where an image h(m, n) of a given object (e.g., face, eye,
etc.) is found in the larger image x(m, n). In this example the image h(m, n) of the desired object is the
CF template and x(m, n) is the test image. In most other CFs, h(m, n) represents a set of images and
not just one image. A naive technique is to use a shifting window, i.e., comparing h(m, n) with every
block (referred hereafter as test chip) within x(m, n) that is of size equal to h(m, n). We compute the
Hadamard (i.e., point-wise) product between h(m, n) and every test chip. A large product (above some
pre-speciﬁed threshold) may represent the presence of image h(m, n) at the location of that test chip.
The shifting window operation, referred to as correlation is expressed as
g(m, n) =

k,l
x(m + k, n + l)h(k,l)
= x(m, n) ⊗h(m, n)
= x(m, n) ⋆h( −m, −n),
(15.2)
where ⊗and ⋆denotes the correlation and convolution operator, respectively, and g(m, n) is called
the correlation output. Note that correlating is equivalent to convolving with the spatial-reversed (i.e.,
ﬂipped) template.
The correlation operator is efﬁciently computed in the discrete Fourier domain as
g(m, n) = F−1 
F{x(m, n)}F∗{h(m, n)}

,
(15.3)
where F{·} and F−1{·} denote the 2-D DFT and Inverse DFT (IDFT) operators, respectively, and the
superscript ∗symbol denotes the conjugate. Recall from DFT theory that x and h require zero-padding
in order to avoid circular correlation.
Derivation: The goal is to ﬁnd if signal x is a good match for the desired pattern h. A simple method
is to ﬁnd the squared error e, i.e.,
e = |h −x|2
= (h −x)H(h −x)
= hHh + xHx −2hHx.
(15.4)
Assuming that we have normalized the energy of both x and h, i.e., hHh = xHx = 1, then minimizing
the error e is equivalent to maximizing the correlation term hHx. This dot product is maximized when
the vectors are in the same direction. Since the vectors are unit norm then −1 ≤hHx ≤1 is maximum
when h and x are parallel (hHx = 1), i.e., they are the same.

408
CHAPTER 15 Segmentation-Free Biometric Recognition
4.15.2.2 Equal Correlation Peak Synthetic Discriminant Function (ECPSDF) ﬁlter
Summary: The ECPSDF ﬁlter was introduced in 1980 [16], and it is also known as the conventional-
SDF. This ﬁlter (like all composite CFs) is designed using a set of training images that attempts to
capture the distortions expected to show up in testing. The ﬁlter is designed to be a linear combination
of training images where the combination weights are chosen so that the correlation outputs (at the
origin) corresponding to the training images yield pre-speciﬁed values (usually 1 for the authentic class
and 0 for the impostor class). When the ﬁlter is applied to a test image (not in the training set) the
correlation plane tends to have a higher value (close to the pre-speciﬁed value of 1) at the location of
the object of interest.
In practice this ﬁlter is not used as it gives peaks with very large side lobes which makes recognition
more challenging because the center of the peak is not as distinguishable, and it is easily confused with
the peaks and side-lobes from impostor images. This is due to the ﬁlter not being designed to capture
the high frequency components of the training images, and, thus, easily confuses similar targets.
Derivation: The desired peak value is hHxi = ui, where xi represents the ith image, h represents
the template, and ui is the pre-speciﬁed ﬁlter response (usually ui = 1 ∀i). Another constraint in
the ECPSDF ﬁlter design is that the template h is a linear combination of the training images, i.e.,
h = 
i αixi. Let X = [x1 · · · xN], then the constraints are
XHh = u,
(15.5)
and
h = Xa,
(15.6)
where u = [u1, ..., uN]T , and a = [α1, ..., αN]T . Substituting Eq. (15.6) into Eq. (15.5) gives
XHXa = u.
(15.7)
The weights are
a = (XHX)−1u.
(15.8)
Substituting the weights in Eq. (15.8) into Eq. (15.6) gives the ﬁlter equation:
h = X(XHX)−1u.
(15.9)
It is worth noting that the same derivation applies in both the spatial and frequency domain with one
minor difference. The pre-speciﬁed response in the frequency is d times the pre-speciﬁed response in
the spatial domain, where d is the dimensionality of the template. This can be easily veriﬁed using
Parseval’s relation which states
hHxi = d ˜hH ˜xi,
(15.10)
where ˜h and ˜xi represent the template and image i, respectively, in the spatial domain.
Note this ﬁlter (and other CFs) may produce a higher response for non-training images than training
images. Even if the training images have unit energy, h will not have unit energy. In fact hHh > 1 when
xH
i xi = 1 and ui = 1 for all i. Since xi has unit energy and the projection to h is constrained to be 1,
then hHxi = |h∥xi| cos β = |h| cos β = 1, where β is the angle between h and xi. Assuming β > 0

4.15.2 Advanced Correlation Filters
409
then |h| > 1. Since h is designed such that hHxi = 1, there exists test chips z different than the training
images for which hHz > 1, e.g., if z is equal or similar to h. Assuming the test chip to be normalized
|z| = 1, then hHz > 1 when the angle between h and z is less than the angle between h and xi (note
that if h exists, the angle between h and any authentic xi is equal for all i). Although it is theoretically
possible that a test chip that produces this response is an impostor, an image that resembles h that much
is very likely an authentic image if the template was well designed.
4.15.2.3 Minimum Variance Synthetic Discriminant Function (MVSDF) ﬁlter
Summary: The MVSDF ﬁlter was introduced in 1986 [17]. This ﬁlter is designed to minimize the
correlation output’s noise variance (ONV) when the input training images is corrupted by additive
zero-mean noise, while simultaneously satisfying the correlation output constraints (i.e., hHx).
In practice this ﬁlter is not commonly used for the same reasons as the ECPSDF ﬁlter. In addition,
the ﬁlter requires the knowledge of the covariance of the additive noise. If the noise is assumed to be
white noise then the MVSDF ﬁlter equals the ECPSDF ﬁlter.
Derivation: In the spatial domain, the desired peak value is ˜hH ˜xi = ci, where ˜xi represents the ith
image, ˜h represents the template, and ci is the pre-speciﬁed ﬁlter response. The images are corrupted
with noise so the actual correlation peak is ˜hH(˜xi + ˜n) = ci + c˜n, where c˜n = ˜hH ˜n and ˜n is modeled
as a zero-mean wide sense stationary random process. The ONV is deﬁned as
var(c˜n) = E{(˜hH ˜n)2|}
= ˜hH E{˜n˜nH}˜h
(15.11)
= ˜hHC˜h,
where C = E{˜n˜nH} is the d × d covariance matrix of the noise, where d is the dimension of the
template.
var(cn) can be expressed in the frequency domain as follows. The variance is the value at the origin
of the autocorrelation function, and therefore var(cn) can be computed by summing over all frequencies
of Sc˜n, the PSD of c˜n. Further, Sc˜n can be computed as the PSD of ˜n times the square of the magnitude
of the ﬁlter frequency response. Thus, the ONV can be represented in vector notation as follows
var(cn) =

f
Sc˜n( f )
=

f
p( f )|h( f )|2
(15.12)
= hHPh,
where p represents the PSD of the input noise ˜n, and diagonal matrix P contains p along its diagonal.
The quadratic hHPh is minimized subject to the linear constraints XHh = u (note that ui = dci) when
(using the Lagrange multipliers method explained in Appendix A.1)
h = P−1X(XHP−1X)−1u.
(15.13)
When the noise is white, i.e., P = I, the MVSDF ﬁlter equals the ECPSDF ﬁlter. This means that the
ECPSDF ﬁlter minimizes the ONV under additive white noise.

410
CHAPTER 15 Segmentation-Free Biometric Recognition
4.15.2.4 Minimum Average Correlation Energy (MACE) ﬁlter
Summary: The MACE ﬁlter was introduced in 1987 [18]. This is the ﬁrst ﬁlter designed to control the
shape of the correlation plane (gi(m, n) = xi(m, n) ⊗h(m, n)) and not just the peak value (hHxi).
The correlation output shape is controlled by minimizing the average correlation energy (ACE) of the
correlation plane from to the training images while simultaneously satisfying the correlation outputs
(i.e., hHxi) to yield a pre-speciﬁed value.
The MACE ﬁlter facilitates recognition by producing very sharp (i.e., delta-function-like) peaks
with minimum sidelobes for desired class training images and no such sharp peaks for impostor class
images.Howevertherecognitionperformancesigniﬁcantlydecreasesfornon-trainingintra-classimages
[18,32]. In practice many images of interest have strong low frequency components. Since the MACE
ﬁlter effectively whitens the spectrum, it enhances high frequency components. It is therefore very
sensitive to distortions, i.e., to images outside of the training set, as well as to additive (high frequency)
noise. In practice, although this ﬁlter has been successfully used in pattern recognition applications
[33], variations of this ﬁlter are more robust (e.g., the OTSDF [19], GMACE [32], MSESDF [21], and
MINACE [20] ﬁlters discussed below).
Derivation: The hard constraints are given by hHxi = ui, where xi represents the ith image,
h represents the ﬁlter, and ui is the pre-speciﬁed ﬁlter response. The correlation plane in response to
image xi is represented by gi = X∗
i h, where diagonal matrix Xi contains xi along its diagonal. The
energy of gi is Ei = gH
i gi = d ˜gH
i ˜gi (from Parseval’s theorem). Since all the Ei (i = 1, . . . , N) cannot
be simultaneously minimized subject to XHh = u, the ACE is minimized instead. The ACE can be
expressed as
Eavg = 1
N

i
Ei
= 1
N

i
gH
i gi
= 1
N

i
hHXiX∗
i h
= 1
N

i
hHDih
= hH

1
N

i
Di

h
= hHDh,
(15.14)
where diagonal matrix Di = XiX∗
i contains the power spectrum of xi along its diagonal, and diagonal
matrix D =
1
N

i Di contains the average power spectral density of the training images along its
diagonal.

4.15.2 Advanced Correlation Filters
411
The quadratic hHDh is minimized subject to the linear constraints XHh = u when (using the
Lagrange multipliers method explained in Appendix A.1)
h = D−1X(XHD−1X)−1u.
(15.15)
Premultiplying by the inverse of the average power spectral density D−1 is equivalent to whitening the
average spectrum of the training images, resulting in sharper peaks (see Section 4.15.3.3 for a detailed
explanation). In practice, the d × d matrix D is not constructed. Instead large matrix multiplication is
avoided taking advantage of D’s diagonality.
One measurement of quality of the ﬁlter is the correlation output variance for the training images in
the presence of additive zero-mean noise. Assuming white noise with variance σ 2
n then
var(hHn) = E{hHnnHh}
= hH E{nnH}h
= σ 2
n hHh
= σ 2
n Eh,
(15.16)
where Eh is the energy of the ﬁlter. Note that a ﬁlter with higher energy produces higher variance in
the ﬁlter output.
A modiﬁcation of the MACE ﬁlter suggested in the original paper is as follows: Correlation planes
will have different energy values. In order to decrease the variation in the energy of these planes, average
energy is modiﬁed as ¯Eavg =
1
N

i αi Ei. Start with αi = 1 ∀i, compute h, and alter αi according
to αi = [Ei/Emax]R, where Emax = max(Ei), and R is a constant which determines the rate of
convergence. This suboptimal ﬁlter will increase the ACE but reduce the variation in the energy of the
planes.
Extensions: Boddeti et al. [6] used a modiﬁed form of the MACE ﬁlter for biometric encryption.
The traditional MACE ﬁlter produces a sharp peak at the correlation output of centered images. The
modiﬁed MACE ﬁlter produces multiple peaks at different locations by adjusting the phase of the
images in the frequency domain. This ﬁlter is used for face veriﬁcation and the peak locations are used
to encode a secret key of authorized user.
4.15.2.5 Optimal tradeoff synthetic discriminant function (OTSDF) ﬁlter
Summary: The OTSDF ﬁlter was introduced in 1990 [19] with contributions from Figue in 1991 [34].
The ONV is denoted by E1 = hHPh and the ACE is denoted by E2 = hHDh. Minimizing E1 typically
leads to low-frequency emphasizing ﬁlters whereas minimizing E2 leads to high-frequency emphasizing
ﬁlters. Thus, minimizing one criterion signiﬁcantly deteriorates the performance from the point of view
of the other criterion. An optimal ﬁlter is deﬁned such that for a given value of E1, E2 is minimized.
Typically, slightly increasing the value of E1 from its minimum, greatly improves E2, and vice-versa
as shown in Figure 15.2.
In practice this ﬁlter is widely used. Since sharp peaks are usually more desirable than noise robust-
ness, the ﬁlter is usually designed to perform more like the MACE ﬁlter than the MVSDF ﬁlter.

412
CHAPTER 15 Segmentation-Free Biometric Recognition
FIGURE 15.2
Tradeoff between output noise variance (ONV) and average correlation energy (ACE).
Derivation: The goal is to minimize E1 subject to a speciﬁed value for E2 and to the linear constraints
XHh = u. Lagrange multipliers are used to obtain the following functional
Lδ(h, δ, ) = E1 + δE2 −H(XHh −u),
(15.17)
where δ ≥0 is a scalar Lagrange multiplier and  ̸= 0 is a vector of nonzero Lagrange multipliers. Note
that negative values for δ are not considered because for efﬁcient ﬁlters the values lie between δ = 0
when only E1 is optimized and δ = ∞when only E2 is optimized. δ is replaced with δ = 1
λ(1 −λ) and
 by  = 1
λ	 where 0 ≤λ ≤1. Note that Lλ(h, λ, 	) = λLδ(h, δ, ) leads to the same minimization
as Lδ(h, δ, ). The Lagrangian equation can be rewritten as a linear combination of E1 and E2 (this
method could be generalized for more than two criteria), i.e.,
Lλ(h, λ, 	) = λE1 + (1 −λ)E2 −H(XHh −u).
(15.18)
For the two selected criteria,
Lλ(h, λ, 	) = λhHPh + (1 −λ)hHDh −H(XHh −u)
= hH(λP + (1 −λ)D)h −H(XHh −u)
= hHTh −H(XHh −u),
(15.19)
where s
T = λP + (1 −λ)D,
(15.20)
and the solution (see Appendix A.1) is
h = T−1X(XHT−1X)−1u.
(15.21)
For λ = 0 the OTSDF ﬁlter is the MACE ﬁlter and for λ = 1 it is the MVSDF ﬁlter. In many experiments
λ takes on a small nonzero value to ensure a sharp peak and some noise robustness.

4.15.2 Advanced Correlation Filters
413
4.15.2.6 Minimum Noise and Correlation Energy (MINACE) ﬁlter
Summary: The MINACE ﬁlter was introduced in 1992 [20]. The authors ﬁrst developed a ﬁlter similar
to the MACE ﬁlter called the Minimum Correlation Energy (MICE) ﬁlter. Although the MACE ﬁlter
guarantees the leastaverage correlation plane energy, it provides little control over the variability of the
correlation plane energies and may result in a biased treatment of a particular training image (e.g., a
training images whose correlation energy is signiﬁcantly different from the mean correlation energy
of the remaining training images). While the MACE ﬁlter averages the spectra of the training images,
MICE uses a tight envelope of the training image spectra. Although the authors claim the MICE reduces
variability of the correlation plane energies, they show no proof to support this claim.
The MINACE ﬁlter adds a ﬂat frequency spectrum (the spectrum of white noise) into the MICE
envelope. The MINACE ﬁlter is similar to the OTSDF ﬁlter providing a tradeoff between the peak
sharpness and noise robustness. However, when the noise is additive noise, OTSDF ﬁlter is designed
to outperform (i.e., reduce the ONV given an ACE) than any other tradeoff ﬁlter (the MINACE ﬁlter
included).
In practice MINACE has similar recognition performance to the OTSDF ﬁlter. Some publications
claim a slight superior recognition performance over the OTSDF ﬁlter [20,35] and others inferior
recognition performance [36,37] (the experiments cited for inferior recognition performance assumed
additive white noise). Because sensor noise and clutter are not strictly additive noise we cannot show
theoretically which ﬁlter has superior performance.
Derivation: The goal of MICE is to minimize a tight upper bound Emax for all the energies in the
training images, i.e., ﬁnd Emax so that
Emax ≥Ei,
(15.22)
where
Ei = hHDih,
(15.23)
where diagonal matrix Di contains the power spectrum of xi along its diagonal. The next step is to ﬁnd
an Emax of the form (the authors chose the quadratic assumption probably to emulate the ACE form)
Emax = hHTh
(15.24)
that satisﬁes Eq. (15.22). One solution is to ﬁnd a T(k) where
T(k) ≥Di(k)
(15.25)
for all i and for all frequencies k (i.e., for all the diagonal elements of T and Di).
One possible solution is
TSUM =

i
Di
(15.26)
which is a scaled version of the T used in the MACE ﬁlter, i.e., 1
N

i Di. TSUM, however, may result
in a biased treatment of a particular training image. The authors claim that using
T(k) = max
n [D1(k), . . . , DN(k)]
(15.27)

414
CHAPTER 15 Segmentation-Free Biometric Recognition
for each k diagonal element in the matrices provides a tigher bound. However, the authors do not point
out that T may also result in a biased treatment of a particular training image.
The quadratic hHTh is minimized subject to the linear constraints XHh = u when (using the
Lagrange multipliers method explained in Appendix A.1)
h = T−1X(XHT−1X)−1u.
(15.28)
The MICE ﬁlter is computed substituting the T in Eq. (15.27) into Eq. (15.28).
The MICE ﬁlter suffers from the same drawbacks as the MACE ﬁlter in that it is not designed to be
robust to additive noise. In order to not emphasize the high frequency components of noise, the power
spectrum of the noise P is added to the MICE envelope, i.e.,
T = max[D1(k), . . . , DN(k), λP(k)],
(15.29)
where 0 ≤λ < ∞is used as a tradeoff between peak sharpness and noise robustness. When the power
spectrum of the (usually high) frequency components fall below λP(k) for some frequency k, λP(k) is
used for frequency k in order to not overemphasize the high frequency components when computing
the inverse of T. Substituting the T in Eq. (15.29) into Eq. (15.28) results in the MINACE ﬁlter.
4.15.2.7 Gaussian MACE (GMACE) ﬁlter
Summary: The GMACE ﬁlter was introduced in 1991 [32]. To improve the intraclass recognition
performance of the MACE ﬁlter, an additional constraint is added requiring the ﬁlter’s correlations
planes to approximate a speciﬁed shape function. This is achieved by minimizing the sum of squared
errors between the ﬁlter’s outputs and the desired shape of the training images, while simultaneously
satisfying the correlation output constraints (i.e., hHxi = ui). Unlike the MACE ﬁlter, the GMACE
ﬁlter is not designed to minimize the ACE. However if the desired shape function is delta-function-like,
the MACE and GMACE ﬁlters are equivalent. The shape the authors chose is a Gaussian-function-like
that allows control of the peak’s sharpness by adjusting the width of the Gaussian. The sharper the
peak, the easier the recognition at the expense of a decrease in recognition performance for non-training
images. The ﬁlter becomes more tolerant to distortion and noise as the width of the Gaussian increases.
In practice, a Gaussian-function-like with a very small width (almost a delta-function-like) as the
speciﬁed correlation shape is usually used.
Derivation: The average of the sum of squared errors between the ﬁlter’s output and the desired
shape ˙G(u, v) of the training images is given as follows
1
N

i

u,v
Xi(u, v)H∗(u, v) −˙G(u, v)
2 = 1
N

i
X∗
i h −˙g
2
= 1
N

i
	
hHXiX∗
i h + ˙gH ˙g −2hHXi ˙g

= hHDh + ˙gH ˙g −2hH ¯X˙g,
(15.30)
where diagonal matrix Xi contains xi along its diagonal, D = 1
N

i XiX∗
i , ¯X = 1
N

i Xi, and ˙g is a
column vector whose elements are the 2-D DFT of the desired Gaussian-function-like correlation.

4.15.2 Advanced Correlation Filters
415
The expression hHDh + ˙gH ˙g −2hH ¯X˙g is minimized subject to the linear constraints XHh = u
when (using the Lagrange multipliers method explained in Appendix A.1)
h = D−1X(XHD−1X)−1u +

I −D−1X(XHD−1X)−1XH
D−1 ¯X˙g
= hMACE + hshape,
(15.31)
where hMACE ensures sharp peaks and hshape serves to control the shape of the correlation plane.
Extensions: The Minimum Squared Error Synthetic Discriminant Function (MSESDF) ﬁlter intro-
duced in 1992 [21] presents a small extension to the GMACE ﬁlter. The derivation includes using
different desired shape functions as the correlation output for each of the training images, i.e., 1
N

i
|X∗
i h −˙gi|2 (MSESDF), instead of 1
N

i |X∗
i h −˙g|2 (GMACE). The only difference in the derivation
is that ¯X˙g is replaced by p = 
i Xi ˙gi allowing each correlation output to be unique, i.e.,
h = D−1X(XHD−1X)−1u +

I −D−1X(XHD−1X)−1XH
D−1p
= hMACE + hshape.
(15.32)
4.15.2.8 Unconstrained CFs: MACH and UMACE and UMSESDF ﬁlters
Summary: The ﬁrst unconstrained CFs were introduced in 1994 [22]. This paper introduces the max-
imum average correlation height (MACH) ﬁlter, the unconstrained MACE (UMACE) ﬁlter, and the
unconstrained MSESDF (UMSESDF) ﬁlter. Previous SDF ﬁlters were constrained to produce a value
of hHxi = ui for training images, but such hard constraints are not necessarily satisﬁed by non-training
images.
The MACH ﬁlter is designed to minimize the average (dis-)similarity measure (ASM), i.e., the scatter
of the correlation planes, and simultaneously minimize the ACE and maximize the average correlation
peak intensity (|¯g|2 = |hH ¯x|2).
The Generalized MACH (GMACH) ﬁlter is designed by minimizing the variance in the correlation
peaks, and including that as an additional constraint to those used in the MACH ﬁlter.
The MACH ﬁlter has been investigated for many applications [12,38–41]. The draw-back with all
the ﬁlters in this paper is the high dependency on the average of all the training images ¯x = 1
N

i xi
which, in some cases, may be too blob-like. This issue is addressed below in this section’s “Extensions.”
The UMACE and UMSESDF ﬁlters are variations of the MACH ﬁlter that ignore the ASM or the
ACE, respectively. Numerical results show [22] that MACH ﬁlter outperforms these other ﬁlters in
recognition performance.
Derivation: The ﬁrst step is to compute the ASM between an ideal shape ˙g and the actual correlation
outputs gi. The ideal shape that minimizes the distortion of the correlation outputs with respect to ˙g
measured as the mean squared error (MSE)
e = 1
N

i
|gi −˙g|2
(15.33)
is found by taking the gradient with respect to ˙g, setting it equal to zero, and solving for ˙g. This gives
˙gOPT = 1
N

i
gi = ¯g.
(15.34)

416
CHAPTER 15 Segmentation-Free Biometric Recognition
Substituting Eq. (15.34) into Eq. (15.33) gives the ASM,
ASM = 1
N

i
|gi −¯g|2
= 1
N

i
X∗
i h −¯X∗h
2
= hH

1
N

i
(Xi −¯X)(Xi −¯X)∗

h
= hHSh,
(15.35)
where diagonal matrix Xi contains xi along its diagonal, and diagonal matrix S =
1
N

i (Xi −¯X)
(Xi −¯X)∗represents a measure of the similarity (or more correctly, dissimilarity) of the training images
to the authentic class mean. The ACE is the previously derived quadratic (see Eq. (15.14))
ACE = hHDh,
(15.36)
where diagonal matrix D = 1
N

i XiX∗
i contains the average power spectrum of the training images
along its diagonal.
The average peak intensity may be expressed as
|¯c|2 = |hH ¯x|2 = hH ¯x¯xHh.
(15.37)
The ﬁlter h that simultaneously maximizes the average peak intensity |¯c|2 and minimizes both ASM
and ACE is obtained using the following Rayleigh Quotient (RQ),
J(h) =
hH ¯x¯xHh
hHSh + hHDh
(15.38)
which is maximized when
hMACH = (D + S)−1¯x.
(15.39)
The GMACH ﬁlter is obtained by minimizing the following variance of the correlation peaks
σ 2
g0 = 1
N

i
hHxi −hH ¯x

2
= hH

1
N

i
(xi −¯x)(xi −¯x)H

h
= hHVh,
(15.40)
where V = 1
N

i (xi −¯x)(xi −¯x)H. To minimize this variance and the ASM and ACE, Eq. (15.38) is
rewritten as
J(h) =
hH ¯x¯xHh
hH 
δV + S + D

h,
(15.41)

4.15.2 Advanced Correlation Filters
417
where δ is used to control the emphasis on V. The solution to Eq. (15.41) is
hGMACH = (δV + D + S)−1¯x.
(15.42)
Although this matrix is no longer diagonal it can be efﬁciently computed [40] requiring the inversion
of an N×N matrix (instead of d×d), where the number of training images N is usually much less than
the number of pixels d.
The UMACE ﬁlter is obtained by ignoring S and V, i.e.,
hUMACE = D−1¯x.
(15.43)
This looks like the MACE ﬁlter, i.e.,
hMACE = D−1X(XHD−1X)−1u
= D−1Xa
= D−1ˆx,
(15.44)
where ˆx is a weighted average of the training images and a = (XHD−1X)−1u is the weight vector
necessary to satisfy the hard constraints on the training images. It is interesting to note that by choosing
u carefully hMACE can equal hUMACE.
The UMSESDF ﬁlter is obtained by ignoring D and 
, i.e.,
hUMSESDF = S−1¯x.
(15.45)
However hUMSESDF is not used in practice because it does not produce sharp peaks.
The authors noted that, by normalizing the amplitude to 1 of the Fourier transform of the training
images, the equations for the ﬁlters are simpliﬁed by replacing D = I since 1
N

i XiX∗
i = I.
Extensions: Alkanhal et al. [42] proposed the Extended MACH (EMACH) ﬁlter to address the high
dependence on the mean training image. They claim that the response of the MACH ﬁlter (and other
SDF ﬁlters) to a training image follows too closely the response of the mean training image. However the
mean training image is not always a good representation of the authentic class. In addition, the MACH
ﬁlter gives a biased treatment to the low-frequency components represented by the mean training images
(the high frequency components in the training images where much of the discriminatory information
is found can be blurred when computing the mean). EMACH ﬁlter modiﬁes the objective function in
Eq. (15.38) to
J(h) =
hHEh
hHSh + hHDh,
(15.46)
where
S = 1
N

i

Xi −(1 −β) ¯X
 
Xi −(1 −β) ¯X
∗,
(15.47)
and
E = 1
N

n
(xi −β ¯x)(xi −β ¯x)H,
(15.48)

418
CHAPTER 15 Segmentation-Free Biometric Recognition
where β is a value between 0 and 1 (β = 0 is the MACH ﬁlter). This is maximized (see Appendix in
Section A.2) by the eigenvector corresponding to the largest eigenvalue of (D + S)−1E.
Kumar and Alkanhal [43] proposed the Eigen-EMACH (EEMACH) ﬁlter. This follows the same
design as the EMACH ﬁlter but instead approximates the matrix S by its dominant eigenvectors. This
causes the ﬁlter to be less speciﬁc on the training images and generalize better to authentic images
outside the training set.
A comparative study by Kerekes and Kumar [37] did not show that the EEMACH and EMACH ﬁlters
outperformed the MACH ﬁlter using gray-scaled images. Another study by Van Nevel and Mahalanobis
[40] showed that the GMACH ﬁlter outperformed the MACH and EMACH ﬁlters using Ladar data.
A natural extension to the MACH ﬁlter is the Optimal Tradeoff MACH (OTMACH) ﬁlter presented
by Kumar et al. [44] using the ideas from the OTSDF ﬁlter (see Section 4.15.2.5). Note that in their paper
they referred to that ﬁlter as the OTSDF ﬁlter but others call it the OTMACH ﬁlter to avoid confusion
with the OTSDF ﬁlter introduced in Section 4.15.2.5. In this design they minimize ASM (or modify
ASM–see Eq. (15.47)), ONV, and ACE. The solution is similar to Eq. (15.39),
h = T−1¯x,
(15.49)
where T = αP + βD + γ S is a linear combination of the ONV, ACE, and ASM. Banerjee et al. [45]
proposed using a neural network to ﬁnd suitable parameters α, β, γ for face recognition.
A comparative study by Singh and Kumar [46] showed that the OTMACH ﬁlter outperformed the
MACH, EMACH, DCCF, and Polynomial DCCF (Section 4.15.2.13) ﬁlters, and that the EMACH ﬁlter
outperformed the MACH ﬁlter on Synthetic Aperture Radar (SAR) data.
4.15.2.9 Average of synthetic exact ﬁlters (ASEF)
Summary: The ASEF was introduced in 2009 [4]. This unconstrained ﬁlter is designed by building one
ﬁlter hi for each training image and then taking the average of all the ﬁlters to obtain ¯h. Averaging
the ﬁlters results in a ﬁlter that avoids over-ﬁtting to the training set. Each ﬁlter hi exactly maps a
training image to a desired correlation plane ˙gi. Similar to the GMACE ﬁlter, the authors chose a
Gaussian-function-like shape centered at the biometric image as the desired correlation plane and used
the Gaussian’s width to tradeoff between sharp peaks for easy recognition (small width), and broad
peaks for distortion and noise tolerance (large width). In their design the images do not need to be
centered as long as the Gaussian’s center is at the signature’s location. In fact, it is possible to have
multiple targets in one training image [10] with Gaussian-function-like shapes centered at each of the
signature locations.
The ﬁlter performs well in practice. It has successfully been applied to ocular localization (the eye
and surrounding regions) [4] and pedestrian localization [10]. The disadvantage of ASEF is the large
number of training images required (in the order of hundreds) for good recognition performance. This
disadvantage is addressed by the MOSSE ﬁlter discussed in the next section.
Derivation: Given the 2-D DFT of a training image Xi(k,l) and a desired 2-D DFT of a correlation
plane ˙Gi(k,l), the exact ﬁlter is
Hi(k,l) =
˙Gi(k,l)
X∗
i (k,l).
(15.50)

4.15.2 Advanced Correlation Filters
419
The ASEF is
H(k,l) = 1
N

i
Hi(k,l).
(15.51)
If ASEF is trained on a small number of images, the ﬁlter can become unstable when some frequencies
of Xi are close to zero.
4.15.2.10 Minimum Output Sum of Squared Error (MOSSE) ﬁlter
The MOSSE ﬁlter was introduced in 2010 [5]. This ﬁlter is designed to minimize the MSE between
the desired correlation plane and the actual correlation plane. The difference between the MOSSE and
MSESDF ﬁlter is that no hard constraints are placed on the correlation peak in the MOSSE ﬁlter (recall
that the MSESDF ﬁlter required XHh = u), and in that the targets do not need to be centered in the
training set (this second difference is insigniﬁcant when viewed in the Fourier domain). The simplicity
of the MOSSE ﬁlter allows training in real time.
The ﬁlter performs well in practice, and can adapt in real-time to changes in rigid-body motion,
deformation, and/or lighting.
Derivation: The MSE between the ﬁlter’s output and the desired shape ˙gi of the training images is
given by
1
N

i
X∗
i h −˙gi
2 = 1
N

i
	
hHXiX∗
i h −2hHXi ˙gi + ˙gH
i ˙gi

= hHDh −2hHp + E ˙g,
(15.52)
where diagonal matrix Xi contains the entries of the xi along its diagonal, D = 1
N

i XiX∗
i (same as
the MACE ﬁlter), p = 1
N

i Xi ˙gi, and ˙g is a column vector whose elements are the FT of the desired
Gaussian-function-like. The h that minimizes the ASE is found by taking its gradient and setting it
equal to zero, i.e.,
2Dh −2p = 0,
(15.53)
h = D−1p
=

i
XiX∗
i
−1 
i
Xi ˙gi

,
(15.54)
which can be expressed as
H(k,l) =

i Xi(k,l) ˙G∗
i (k,l)

i Xi(k,l)X∗
i (k,l).
(15.55)
This ﬁlter can be modiﬁed so that it can be adapted to new training images. For a new frame Xi(k,l)
the ﬁlter is updated as follows
Hi(k,l) = Ai(k,l)
Bi(k,l),
(15.56)
where
Ai(k,l) = η

Xi(k,l) ˙G∗
i (k,l)

+ (1 −η)Ai−1(k,l),
(15.57)

420
CHAPTER 15 Segmentation-Free Biometric Recognition
and
Bi(k,l) = η

Xi(k,l)X∗
i (k,l)

+ (1 −η)Bi−1(k,l),
(15.58)
where η is the learning rate (in their paper they use η = 0.125).
4.15.2.11 Optimal tradeoff circular harmonic function (OTCHF) ﬁlter
Summary: The OTCHF ﬁlter was introduced in 2000 [28]. Circular harmonic functions (CHFs) were
used in 1982 [24] to design in-plane rotation invariant ﬁlters giving a constant output as the input is
rotated. However, that design uses only one harmonic and, therefore, ignores a lot of the discriminatory
pattern information. In 1985, Schils and Sweeney [47] showed that the ECPSDF (see Section 4.15.2.2)
can be used to produce rotationally-invariant ﬁlters using multiple in-plane rotations as training images
and taking the limit of the interval between each rotation to zero (i.e., having an inﬁnite number of
images). In 1986, Kumar and Ng [48] introduced a ﬁlter design where the correlation output can be
varied in a speciﬁed manner with input rotation. The OTCHF ﬁlter improves upon this design and
includes the ACE, ONV, and ASM criteria discussed previously. The result is a ﬁlter whose correlation
planes have sharp peaks at the object’s locations for a speciﬁc range of rotations (e.g., between 45◦and
90◦) and low values elsewhere.
Derivation: Let X(ρ, φ), H(ρ, φ), and G(ρ, φ) be the Fourier domain polar representation of
X(u, v), H(u, v), and G(u, v), respectively, where ρ = (u2 + v2)
1
2 corresponds to the magnitude
and φ = arctan ( v
u ) corresponds to the angle between u and v, respectively. The correlation value at the
origin (the correlation peak) is
g0 =
 ∞
−∞
 ∞
−∞
G(u, v)du dv
=
 ∞
0
 2π
0
G(ρ, φ)ρ dφ dρ
=
 ∞
0
 2π
0
X(ρ, φ)H∗(ρ, φ)ρ dφ dρ
(15.59)
(note that in Cartesian coordinates the integration is over du dv and in polar coordinates is over ρ dφ dρ).
Periodic signals can be expressed using a Fourier series expansion. Since X(ρ, φ), H(ρ, φ), and
G(ρ, φ) are periodic in φ with period 2π they can be expressed as
X(ρ, φ) =

k
Xk(ρ)e jkφ,
(15.60)
and
Xk(ρ) = 1
2π
 2π
0
X(ρ, φ)e−jkφdφ,
(15.61)

4.15.2 Advanced Correlation Filters
421
where Xk(ρ) is the kth CHF of X(u, v), and similarly for H(ρ, φ) and G(ρ, φ). Substituting these
Fourier series expansions back into Eq. (15.59) yields
g0 =
 ∞
0
 2π
0

k
Xk(ρ)e jkφ 
l
H∗
l (ρ)e−jlφρ dφ dρ
=

k

l
 2π
0
e j(k−l)φdφ
 ∞
0
Xk(ρ)H∗
l (ρ)ρ dρ.
(15.62)
Noting that
 2π
0
e j(k−l)φdφ =
2π k = l,
0
k ̸= l,
(15.63)
Equation (15.62) can be rewritten as follows
g0 =

k
2π
 ∞
0
Xk(ρ)H∗
k (ρ)ρ dρ
=

k
Ck,
(15.64)
where
Ck = 2π
 ∞
0
Xk(ρ)H∗
k (ρ)ρ dρ
(15.65)
is loosely referred to as the kth CHF weight.
A rotation in the input Cartesian image is equivalent to a circular shift in the polar image along the φ
axis which is equivalent to a phase change in the frequency polar domain (note that Eq. (15.60) changes
to X(ρ, φ + θ) = 
k Xk(ρ)e jk(φ+θ)). The correlation peak value of an input image rotated by θ is
therefore
gθ =

k
2π
 ∞
0
	
Xk(ρ)e jkθ
H∗
k (ρ)ρ dρ
=

k
Cke jkθ.
(15.66)
In order to have a constant correlation peak for all rotations, i.e., gθ = c, ∀θ, only one CHF weight
can be used in Eq. (15.66). This is because using more than one CHF weight includes oscillatory terms
from e jkθ. However, if only one CHF weight is nonzero then all but one of the terms of the Fourier
expansion are ignored which leads to poor discrimination. One solution is to ﬁnd coefﬁcients Ck that
approximate a desired gθ, e.g., a high value between 45◦and 90◦and low values everywhere else. This
is equivalent to the ﬁnite impulse response (FIR) ﬁlter design problem. There are excellent FIR designs
methods presented elsewhere [49] that can be used to ﬁnd the coefﬁcients Ck.
Once the coefﬁcients Ck are computed, Eq. (15.65) is used to solve for the CHFs Hk(ρ). There are
inﬁnite solutions, and therefore other constraints such as ONV, ACE and ASM can be included to ﬁnd
a unique solution.

422
CHAPTER 15 Segmentation-Free Biometric Recognition
The ONV is obtained by computing the variance at the correlation peak due to additive noise.
Assuming the noise to be isotropic (i.e., Pn(ρ, φ) = Pn(ρ)),
ONV =
 ∞
−∞
 ∞
−∞
Pn(u, v)|H(u, v)|2du dv
=
 ∞
0
 2π
0
Pn(ρ, φ)|H(ρ, φ)|2ρ dφ dρ
=
 ∞
0
 2π
0
Pn(ρ)|H(ρ, φ)|2ρ dφ dρ
=
 ∞
0
 2π
0
Pn(ρ)

k

l
Hk(ρ)H∗
l (ρ)e j(k−l)φρ dφ dρ
=

k

l
 ∞
0
Pn(ρ)Hk(ρ)H∗
l (ρ)ρ dρ
 2π
0
e j(k−l)φdφ

= 2π

k
 ∞
0
Pn(ρ)|Hk(ρ)|2ρ dρ.
(15.67)
The ACE is obtained by averaging the correlation energies for each angle θ, i.e.,
ACE = 1
2π
 2π
0
 ∞
0
 2π
0
|G(ρ, φ + θ)|2ρ dφ dρ dθ
= 1
2π
 2π
0
 ∞
0
 2π
0
|X(ρ, φ + θ)|2|H(ρ, φ)|2ρ dφ dρ dθ
= 1
2π
 2π
0
 ∞
0
 2π
0

k

l
Xk(ρ)X∗
l (ρ)e j(k−l)(φ+θ)

×

m

n
Hm(ρ)H∗
n (ρ)e j(m−n)φ

ρ dφ dρ dθ
=

k

l

m

n
 ∞
0
Xk(ρ)X∗
l (ρ)Hm(ρ)H∗
n (ρ)ρ dρ
×
 2π
0
e j(k−l+m−n)φdφ
  1
2π
 2π
0
e j(k−l)θdθ

=

k

m

n
 ∞
0
|Xk(ρ)|2Hm(ρ)H∗
n (ρ)ρ dρ
 2π
0
e j(m−n)φdφ

= 2π

k

m
 ∞
0
|Xk(ρ)|2|Hm(ρ)|2ρ dρ
= 2π

m
 ∞
0
PX(ρ)|Hm(ρ)|2ρ dρ,
(15.68)

4.15.2 Advanced Correlation Filters
423
where
PX(ρ) =

k
|Xk(ρ)|2
(15.69)
is the sum of the power spectra of the CHF Xk(ρ).
The ASM measures the dissimilarities in the correlations planes for all possible angles θ, i.e.,
ASM = 1
2π
 2π
0
 ∞
0
 2π
0
|G(ρ, φ + θ) −¯G(ρ, φ)|2ρ dφ dρ dθ
= 1
2π
 2π
0
 ∞
0
 2π
0
|X(ρ, φ + θ)H(ρ, φ) −¯X(ρ, φ)H(ρ, φ)|2ρ dφ dρ dθ
= 1
2π
 2π
0
 ∞
0
 2π
0
|X(ρ, φ + θ) −¯X(ρ, φ)|2|H(ρ, φ)|2ρ dφ dρ dθ,
(15.70)
where
¯X(ρ, φ) = 1
2π
 2π
0
X(ρ, φ + ψ)dψ
= 1
2π
 2π
0

z
Xz(ρ)e jz(φ+ψ)dψ
=

z
Xz(ρ)e jzφ
 1
2π
 2π
0
e jzψdψ

= X0(ρ).
(15.71)
Substituting Eq. (15.71) into Eq. (15.70) gives
ASM = 1
2π
 2π
0
 ∞
0
 2π
0
|X(ρ, φ + θ) −X0(ρ)|2|H(ρ, φ)|2ρ dφ dρ dθ
= 1
2π
 2π
0
 ∞
0
 2π
0
⎡
⎣
k̸=0

l̸=0
Xk(ρ)X∗
l (ρ)e j(k−l)(φ+θ)
⎤
⎦· · ·

m

n
Hm(ρ)H∗
n (ρ)e j(m−n)φ

ρ dφ dρ dθ
= 2π

k̸=0

m
 ∞
0
|Xk(ρ)|2|Hm(ρ)|2ρ dρ
= 2π

m
 ∞
0
PASM(ρ)|Hm(ρ)|2ρ dρ,
(15.72)
where
PASM(ρ) =

k̸=0
|Xk(ρ)|2.
(15.73)

424
CHAPTER 15 Segmentation-Free Biometric Recognition
To minimize the ASM, ACE, and ONV, Refregier [19] showed that an optimal tradeoff among quadratic
criteria can be obtained by minimizing a weighted sum of the criteria. The ﬁlter is obtained minimizing
the following ﬁgure of (de-)merit (FOM) subject to Eq. (15.65),
FOM = αONV + βACE + γ ASM
= 2π

m
 ∞
0
PFOM(ρ)|Hm(ρ)|2ρ dρ,
(15.74)
where
PFOM(ρ) = αPn(ρ) + β PX(ρ) + γ PASM(ρ)
(15.75)
and α, β, γ ≥0. It can be shown that minimizing this quadratic criterion subject to the linear constraints
yields
Hk(ρ) = λ∗
k
Xk(ρ)
PFOM(ρ),
(15.76)
where
λk =
Ck
 2π
0
|Xk(ρ)|2
PFOM(ρ)ρ dρ
.
(15.77)
4.15.2.12 MACE-Mellin Radial Harmonic (MACE-MRH) ﬁlter
Summary: The MACE-MRH ﬁlter in 2006 [29]. The Mellin transform is invariant to scale changes in the
spatial domain (this is similar to the Fourier transform magnitude being invariant to shifts in the spatial
domain) and is commonly used in pattern recognition applications; however, it requires the images to be
centered. Mellin Radial Harmonics (MRH) were ﬁrst used in 1988 [26] to design shift-invariant ﬁlters
that were also invariant to scale. However, that design only uses one harmonic and therefore ignores
much of the discriminatory pattern information. The MACE-MRH ﬁlter only provides scale invariance
for a limited range of scales but is able to use more harmonics, therefore improving discrimination.
In addition it applies the ACE criterion discussed previously to have sharp peaks at the object’s location
in the correlation plane.
Derivation: Let X(ρ, φ), H(ρ, φ), and G(ρ, φ) be the polar representations of X(u, v), H(u, v),
and G(u, v), respectively, where ρ = (u2 + v2)
1
2 corresponds to the magnitude and φ = arctan ( v
u )
corresponds to the angle between u and v, respectively. The MRH expansion of a signal X(ρ, φ) is [26]
X(ρ, φ) =
∞

k=−∞
Xk(φ)ρ j2πk−1,
(15.78)
and
Xk(φ) = L−1
 R
r0
X(ρ, φ)ρ−j2πk−1ρ dρ,
(15.79)
where
L = ln R −ln r0
(15.80)

4.15.2 Advanced Correlation Filters
425
is a positive integer value (this is to satisfy the orthogonality required among the MRH components
[26]), and the limits of integration are chosen to expand the radial bandwidth of the pattern, e.g., r0 is
a positive number close to zero and R a number greater than the radial bandwidth (ρmax).
The correlation peak using the MRH expansion is (see Eq. (15.59)) given as follows
g0 =
 ∞
0
 2π
0
X(ρ, φ)H∗(ρ, φ)ρ dφ dρ
≈
 R
r0
 2π
0
X(ρ, φ)H∗(ρ, φ)ρ dφ dρ
=
 R
r0
 2π
0

k
Xk(φ)ρ j2πk−1
 
l
H∗
l (φ)ρ−j2πl−1

ρ dφ dρ
=

k

l
 2π
0
Xk(φ)H∗
l (φ)dφ
 R
r0
ρ j2π(k−l)−2ρ dρ

,
=

k

l
 2π
0
Xk(φ)H∗
l (φ)dφ

Lδ(k −l)

= L

k
 2π
0
Xk(φ)H∗
k (φ)dφ
= L

k
Ck,
(15.81)
where
Ck =
 2π
0
Xk(φ)H∗
k (φ)dφ.
(15.82)
The radial integral
 R
r0 ρ j2π(k−l)−2ρ dρ = Lδ(k −l) because of the orthogonality of MRH components.
This can be shown mathematically as follows. Note that
 R
r0
ρ j2π(k−l)−2ρ dρ =
 r0eL
r0
ρ j2π(k−l)−1dρ
(15.83)
(R = r0eL comes from Eq. (15.80)). When k = l
 r0eL
r0
ρ j2π(k−l)−1dρ =
 r0eL
r0
ρ−1dρ
= ln ρ

r0eL
ρ=r0
= ln
	
r0eL
−ln r0
= ln r0 + ln
	
eL
−ln r0
= L,
(15.84)

426
CHAPTER 15 Segmentation-Free Biometric Recognition
and when k ̸= l
 r0eL
r0
ρ j2π(k−l)−1dρ =
1
j2π(k −l)ρ j2π(k−l)

r0eL
ρ=r0
=
1
j2π(k −l)
	
r0eL
 j2π(k−l)
−

r0
 j2π(k−l)

=
1
j2π(k −l)

e j2π(k−l)L 
r0
 j2π(k−l) −

r0
 j2π(k−l)
= 0
(15.85)
noting that (k −l)L is an integer and e j2πτ = 1 for any integer τ.
When an input image is scaled by β > 0, i.e., ˆx(m, n) = x(βm, βn) the Fourier transform is given
by ˆX(u, v) =
1
β2 X
	
u
β , v
β

and the polar 2-D FT is given by
ˆX(ρ, φ) = 1
β2 X
ρ
β , φ

(15.86)
with MRHs
ˆXk(φ) = L−1
 R
r0
ˆX(ρ, φ)ρ−j2πk−1ρ dρ,
= L−1
 R
r0
1
β2 X
ρ
β , φ

ρ−j2πk−1ρ dρ
= L−1

R
β
r0
β
1
β2 X( ˜ρ, φ)(β ˜ρ)−j2πk−1(β ˜ρ)β d ˜ρ
= β−j2πk−1L−1

R
β
r0
β
X( ˜ρ, φ) ˜ρ−j2πk−1 ˜ρ d ˜ρ
≈β−1β−j2πk Xk(φ).
(15.87)
If β is a large value, R is chosen to be large enough so that R
β still encompasses the radial bandwidth of
X(ρ, φ) and the approximation is valid. The correlation peak using this scaled image is (see Eq. (15.81))
g(β) = β−1L

k
Ckβ−j2πk
= β−1L

k
Ckeln

β−j2πk
= β−1L

k
Cke−j2πk ln β.
(15.88)
Selecting only one MRH weight we get
˜g(β) = β−1LCMe−j2π M ln β.
(15.89)

4.15.2 Advanced Correlation Filters
427
Then, a scale change results in only an additional phase factor in the correlation output. The relative
intensity distribution (e.g., using PCE or PSR–see Sections 4.15.3.4.2 and 4.15.3.4.3) remains unaf-
fected. Therefore the ﬁlter is invariant to shifts and scales. However, if only one MRH weight is nonzero
then all but one of the terms of the Fourier expansion are ignored which leads to poor discrimination.
The novelty of MACE-MRH is that it provides a tradeoff between discrimination and scale-invariance.
If a desired scale response gβ is speciﬁed for different β values then the problem becomes ﬁnding the
Ck that give such a response. This can be accomplished by taking advantage of ﬁnite impulse response
(FIR) design methods. For this purpose, an invertible logarithmic transformation L is deﬁned as follows
gL(β) = L{g(β)} = 1
L e
Lβ
2π g
	
e
Lβ
2π

.
(15.90)
Applying it to Eq. (15.88) yields
gL(β) = 1
L e
Lβ
2π
	
e
Lβ
2π

−1
L
∞

z=−∞
Cze
−j2πz ln

e
Lβ
2π

=
∞

z=−∞
Cze−jβ(zL)
=
∞

k=−∞
Cke−jβk
≈
K

k=−K
Cke−jβk,
(15.91)
where k = zL. Equation (15.91) is equivalent to the ﬁnite impulse response (FIR) ﬁlter design prob-
lem. There are excellent FIR designs methods presented elsewhere [49] that can be used to ﬁnd the
coefﬁcients Ck.
Once the coefﬁcients Ck are computed, Eq. (15.82) is used to solve for the MRHs Hk(φ). There are
inﬁnite solutions, and therefore other constraints such as ONV, ACE and ASM can be included to ﬁnd
a unique solution.
The MACE-MRH ﬁlters uses only the ACE criterion. Let PX(φ) represent the average of X(ρ, φ)
along the radial axis (from r0 toR). Then the ACE can be computed as
ACE =
 R
r0
 2π
0
PX(φ)|H(ρ, φ)|2ρ dφ dρ
=
 2π
0

k

l
Hk(φ)H∗
l (φ)
 R
r0
ρ j2π(k−l)−2ρ dρ dφ
= L

k
 2π
0
PX(φ)|Hk(φ)|2dφ.
(15.92)

428
CHAPTER 15 Segmentation-Free Biometric Recognition
The MRH Hk(φ) is obtained by solving
min
Hk(φ)
 2π
0
(PX(φ) + α)|Hk(φ)|2dφ
(15.93)
s.t.
 2π
0
Xk(φ)H∗
k (φ)dφ = Ck.
The regularization parameter α is added in case PX(φ) = 0 for some φ. This is the equivalent to
including the ONV criterion and assuming that the power spectrum of the noise along the radial axis is
Pn(φ) = α, ∀φ. The solution to Eq. (15.93) is
Hk(φ) = λ∗
k
Xk(φ)
PX(φ) + α ,
(15.94)
where
λk =
Ck
 2π
0
|Xk(φ)|2
PX(φ)+α dφ
.
(15.95)
4.15.2.13 Distance Classiﬁer Correlation Filter (DCCF)
Summary: The DCCF for two-class classiﬁcation was introduced in 1993 [23] and extended to multiclass
in 1996 [50]. The DCCF transforms the images for each class so that transformed images of each class
are compact and separated from the other classes. In other words, the goal is to maximize the distance
between transformed class means and minimize the transformed in-class scatter. The formulation is
similartolineardiscriminantanalysis(LDA)whenusingtheFourierdomainoftheimages;thedifference
being in how the in-class scatter is computed. LDA minimizes the scatter of theprojected (a single value)
in-class images while DCCF minimizes the scatter of the transformed (an entire plane) in-class images.
This ﬁlter does not appear to be widely used. There are two problems with DCCF. First, unlike other
CFs, DCCF is not designed to provide a sharp peak at the target’s location. This reduces the shift-
invariance advantage of CFs. In their experiments the authors used centered images so no localization
(i.e., shift-invariance) performance is given. Second, the metric used during training is not the metric
used for testing. That is, DCCF is trained to maximize the squared difference of the projected class
means (or correlation peak values), but in testing the entire correlation plane is used. The authors claim
that by making the in-class correlation planes similar, then the in-class peak values should be similar to
each other. However, overall consistency in the correlation planes does not necessarily mean consistency
in the correlation peak values. The DCCF may be better used as a post-localization stage to classify the
signature between a group of already localized signatures.
Derivation: The goal is to maximize the separation between two classes. Under transform h, the
squared difference between two class means is
A(h) =
hH ¯x1 −hH ¯x2

2
= hH(¯x1 −¯x2)(¯x1 −¯x2)Hh
= hHSAh,
(15.96)

4.15.2 Advanced Correlation Filters
429
where ¯xc represents the mean image for Class c, and SA = (¯x1 −¯x2)(¯x1 −¯x2)H is not a diagonal matrix.
For multiple classes, each class mean is pushed away from the global mean, i.e.,
A(h) =

c
hH ¯xc −hH ¯x

2
= hH

c
(¯xc −¯x)(¯xc −¯x)H

h
= hHSAh,
(15.97)
where SA = 
c (¯xc −¯x)(¯xc −¯x)H is not a diagonal matrix, and ¯x represents the global mean.
In addition, the in-class scatter is minimized. For this purpose, the in-class scatter can be represented
by adding the ASM (see Eq. (15.35)) over all classes, i.e.,
B(h) =
C

c=1
N

i=1
1
d h†(Xic −¯Xc)(Xic −¯Xc)∗h
= 1
d hH
 C

c=1
N

i=1
(Xic −¯Xc)(Xic −¯Xc)∗

h
= hHSBh,
(15.98)
where diagonal matrix SB = 
c

i (Xic −¯Xc)(Xic −¯Xc)∗, diagonal matrix Xic contains the ith image
of class c along its diagonal, and diagonal matrix ¯Xc contains ¯xc along its diagonal. For comparison, in
LDA under transform h the in-class scatter is
σ 2
c =

i
hHxic −hH ¯xc

2
= hH

i
(xic −¯xc)(xic −¯xc)H

h,
(15.99)
where xic is the ith image in Class c. The total scatter in LDA can be represented as the sum of each
in-class variance, i.e.,
C(h) =

c
σ 2
c
= hH

c

i
(xic −¯xc)(xic −¯xc)H

h
= hHSCh,
(15.100)
where SC = 
c

i (xic −¯xc)(xic −¯xc)H is not a diagonal matrix. It is worth noting that the diagonal
elements of SC are the same as the diagonal elements of SB. Also if the number of dimensions d is

430
CHAPTER 15 Segmentation-Free Biometric Recognition
greater than N −c (this is usually the case in most CF problems) then SC is singular, whereas SB is
non-singular.
In order to maximize A(h) and minimize B(h) simultaneously, the ratio
J(h) = A(h)
B(h)
(15.101)
is maximized with respect to h. The optimum solution (see Appendix A.2) is the dominant eigenvector
of S−1
B SA.
To test a test chip z, the squared difference between the transformed image and the transform of each
class mean is computed, i.e.,
l(z) = min
c
H∗z −H∗¯xc
2
= min
c

¯xH
c HH∗¯xc −2zHHH∗¯xc + zHHH∗z

= min
c

¯xH
c HH∗¯xc −2zHHH∗¯xc

= min
c

bc −zHhc

= max
c

zHhc −bc

,
(15.102)
where l(z) represents the class-label of image z, diagonal matrix H contains h along its diagonal,
bc = ¯xH
c HH∗¯xc, and hc = 2HH∗¯xc. To test an image z larger than the training image, the spatial domain
representation of z and hc is cross-correlated, the scalar bc is subtracted from the entire correlation plane
(i.e., from each value in the plane), and then the maximum value for all the classes c is computed.
Extensions: Alkanhal and Kumar [51] combine ideas from PCF (discussed next) and DCCF to form
the Polynomial DCCF (PDCCF). It is a DCCF design but uses a series of images transformed by a
function as shown in Eq. (15.109). They present an extensive analysis of PDCCF, but because they
used centered images, their experiments fail to show any localization (shift-invariance) performance.
A comparative study by Singh and Kumar [46] showed that PDCCF outperformed DCCF in almost all
cases in their synthetic aperture radar (SAR) data experiments.
4.15.2.14 Polynomial Correlation Filters (PCFs)
Summary: PCFs were introduced in 1997 [30]. The PCF output is a nonlinear function of the input that
can enhance recognition performance. The application of this ﬁlter can easily be extended to data fusion
from multiple sensors.
Derivation: The PCF output can be expressed as
g(m, n) =

p
h p(m, n) ⊗x p(m, n),
(15.103)
where x p(m, n) represents the elements of image x(m, n) raised to the pth power. This is expressed in
the Fourier domain as
g =

p
Xp∗hp,
(15.104)

4.15.2 Advanced Correlation Filters
431
where diagonal matrix Xp contains xp along its diagonal. The goal is to ﬁnd corresponding ﬁlters hp
that maximize some objective, e.g., to maximize the average peak value at the origin and minimize
some other metric (e.g., ASM, ONV, ACE). The authors used ASM in their paper, i.e., maximize
J(h) = |hH ˆ¯x|2
hHSh
=

p hH
p ¯xp
2

pq hHp Spqhq
,
(15.105)
where
h =
⎡
⎢⎣
h1
...
hP
⎤
⎥⎦,
ˆ¯x =
⎡
⎢⎣
¯x1
...
¯xP
⎤
⎥⎦
(15.106)
is a vector form by concatenating a series of vectors that represent the mean image raised to various
powers,
S =
⎡
⎢⎣
S11
· · ·
S1P
...
...
...
SP1
· · · SP P
⎤
⎥⎦,
(15.107)
and Spq = 1
N

n (Xp
n −¯Xp)(Xq
n −¯Xq)∗. The solution that maximizes J(h) is
h = S−1ˆ¯x.
(15.108)
This method is not restricted to power nonlinearities. It can use any point nonlinear function fp(x(m, n)),
i.e.,
g(m, n) =

p
h p(m, n) ⊗f p(x(m, n)).
(15.109)
The solution is the same as Eq. (15.108) with
ˆ¯x =
⎡
⎢⎣
f1(¯x)
...
fP(¯x)
⎤
⎥⎦,
(15.110)
where f p(¯x) is f p(¯x(m, n)) arranged as a vector, and
Spq = 1
N

n

f p(Xn) −f p( ¯X)
 
fq(Xn) −fq( ¯X)
∗.
(15.111)
This extension can be used for sensor fusion by treating the data processed from different sensors as a
nonlinear transformation.
Extensions: Al-Mashouq et al. [52] presented the Constrained PCF (CPCF) that constrains the values
at the peak. In addition an analysis is presented suggesting that using the power functions in Eq. (15.103)
helps recognition performance for low values of p. As p increases the contributions to the total peak
from the pth order ﬁlter decreases and the variance due to clutter increases. This suggests that the
highest order should be restricted to p = 4.

432
CHAPTER 15 Segmentation-Free Biometric Recognition
4.15.2.15 Quadratic correlation ﬁlters
Summary: Quadratic CFs (QCFs) were introduced in 2004 [53]. A disadvantage of linear CFs is that
several CFs are required to handle the wide variety of target appearances. When these CFs are applied
to a test image, their outputs are compared to select a winner. A quadratic CF (QCF) requires several
linear CFs as well but has the advantage that these CFs are designed to work together to produce a single
correlation output. In addition, quadratic classiﬁers are able to exploit the higher-order statistics of the
data, potentially leading to superior recognition performance. This ﬁlter is sometimes referred to as the
Fukunaga-Koontz QCF (FKQCF) because it can be derived using the Fukunaga-Koontz transform [54].
In one set of experiments using gray-scale images, QCFs were shown to outperform other CFs [37].
Derivation: QCF maximizes a metric of separation between the overall outputs for two classes of
targets. To express this separation metric, let ˜x(c) (all the notation in this derivation is in the spatial
domain) be a vectorized image from Class c where c ∈{1, 2} with d elements and let Q be a d × d
matrix. The goals is to make y =

˜x(c)T Q˜x(c) large and positive when c = 1 (i.e., when the target is
from Class 1) and large and negative when c = 2 (i.e., when the target is from Class 2). Let
φc = E
	
˜x(c)
T
Q˜x(c)
 
(15.112)
indicate the mean QCF output for all training images from Class c. In the most basic QCF design, Q is
computed such that
J(Q) = |φ1 −φ2|
(15.113)
is a large value. In other words, a large between-class separation is desired.
To enable a correlation-type QCF implementation architecture, Q is assumed to be of the form
Q =
N1

i=1
˜fi ˜fT
i −
N2

i=1
˜bi ˜bT
i .
(15.114)
It can be shown [53] that J(Q) is maximized for a given N1 and N2 when ˜f1, . . . , ˜fN1 and ˜b1, . . . , ˜bN2
are the eigenvectors corresponding to the N1 largest positive and the N2 largest negative eigenvalues,
respectively, of R = R1 −R2, where
Rc = E

˜x(c)
i
	
˜x(c)
i

T  
(15.115)
is the correlation matrix for all the training images in Class c.
To apply the QCF to a test image x(m, n), ﬁrst let test chip ˜z denote a vectorized subregion (equal
in size to a training image) of x(m, n) with QCF output
y = ˜zT Q˜z
= ˜zT
 N1

i=1
˜fi ˜fT
i

˜z −˜zT
 N2

i=1
˜bi ˜bT
i

˜z
= ˜zT FFT ˜z −˜zT BBT ˜z
= ˜vT ˜v −˜wT ˜w,
(15.116)

4.15.2 Advanced Correlation Filters
433
FIGURE 15.3
Efﬁcient architecture to apply the QCF to image x(m, n).
where F = [˜f1 · · · ˜fN1], B = [˜b1 · · · ˜bN2], ˜v = FT ˜z, and ˜w = BT ˜z. The value of vi (the ith element of
˜v) can be obtained for all locations within the image x(m, n) by means of 2-D correlation:
vi(m, n) = x(m, n) ⊗fi(m, n),
(15.117)
where the eigenﬁlter fi(m, n) is the ˜fi eigenvector reshaped as 2-D ﬁlter. A similar derivation can be
shown for wi (the ith element of ˜w).
Figure 15.3 shows the architecture to apply the QCF to image x(m, n). The QCF output g(m, n) is
g(m, n) =
N1

i=1
|vi(m, n)|2 −
N2

i=1
|wi(m, n)|2
=
N1

i=1
|x(m, n) ⊗fi(m, n)|2 −
N2

i=1
|x(m, n) ⊗bi(m, n)|2,
(15.118)
which can be efﬁciently implemented in the Fourier domain as
g(m, n) =
N1

i=1
F−1 
F{x(m, n)}F∗{ fi(m, n)}

2
−
N2

i=1
F−1 
F{x(m, n)}F∗{bi(m, n)}

2
.
(15.119)
Note that when the QCF is applied to multiple images (e.g., in a video), the 2-D DFTs of the eigenvectors
only need to be computed once. Therefore in video processing, QCF requires N1 + N2 + 1 2-D DFTs
per video frame (combining both IDFTs and DFTs).

434
CHAPTER 15 Segmentation-Free Biometric Recognition
In QCF design the number of eigenﬁlters affects the recognition performance. More training images
results in better recognition performance. Some CFs that use the average of thetraining imagesfor their
designs (e.g., the MACH ﬁlter [44]) limit the number of training images to prevent the average from
becoming a blurry image. However, QCF uses the average of the QCF output(see Eq. (15.112)), and
therefore over-ﬁtting is not a problem. For example, Mahalanobis et al. [31] used over 20,000 training
images (over 5000 authentic class images and over 15,000 impostor class images). As we later show,
we use a small number of training images in our experiments and show that QCF still achieves good
recognition performance.
Extensions: Rodriguez and Kumar [15] proposed a fast method to compute the eigenvectors. A ﬁlter
with N = N1 + N2 training images of size d requires computing the eigenvalues of a d ×d matrix. The
required computations are of the order O(d3). The required computations can be reduced to order O(N 3)
using the fact that the QCF only requires the eigenvectors corresponding to the non-zero eigenvalues of
R = R1 −R2. To the authors’ knowledge in all QCF applications N ≪d. First, it is well known that
given an m × n matrix A where m ≫n, the eigenvectors corresponding to the non-zero eigenvalues
of m × m matrix AAT can be obtained by (1) ﬁnding the eigenvectors of n × n matrix AT A and (2)
premultiplying those eigenvectors by A. Both matrices will have the same non-zero eigenvalues. The
trick is therefore to express matrix R as R = AAT (note the transpose and not the hermitian symbol).
Let the ﬁrst N1 columns of A be the lexicographically rearranged training images from Class 1 and let
the next N2 columns be the lexicographically rearranged training images from Class 2. Then multiply
the ﬁrst N1 columns by
!
1
N1 and multiply the next N2 columns by j
!
1
N2 , where j = √−1, so that the
columns of A are
A =
"
1
N1
˜x(1)
1 ,
"
1
N1
˜x(1)
2 , . . . ,
"
1
N1
˜x(1)
N1, j
"
1
N2
˜x(2)
1 , j
"
1
N2
˜x(2)
2 , . . . , j
"
1
N2
˜x(2)
N2

,
(15.120)
where ˜x(c)
n
is the nth training image of Class c. Once matrix A is arranged, the required eigenvectors of
R = AAT
= 1
N1
N1

i=1
˜x(1)
i
	
˜x(1)
i

T
−1
N2
N2

i=1
˜x(2)
i
	
˜x(2)
i

T
= R1 −R2,
(15.121)
can be efﬁciently computed.
Mahalanobis et al. [31,53] proposed the Rayleigh Quotient QCF (RQQCF) that, in addition to maxi-
mizing the class separation, it attempts to reduce the class-scatter by equalizing the average output from
Class 1 with the average negative output of Class 2, i.e., E
#
˜x(1)T Q˜x(1)$
= −E
#
˜x(2)T Q˜x(2)$
(see
Eq. (15.112)). However, note that there is no theoretical justiﬁcation that this will reduce the class-scatter,
and in fact it may reduce performance by emphasizing this criterion over maximum class separation.
This objective can be accomplished by rewriting the objective function in Eq. (15.113) as
J(Q) = |φ1 −φ2|
φ1 + φ2
.
(15.122)

4.15.3 Pre- and Post-Processing Images
435
ThesolutionisthesameasEq.(15.114)with ˜f1, . . . , ˜fN1 and ˜b1, . . . , ˜bN2 astheeigenvectorscorrespond-
ing to the N1 largest positive and N2 largest negative eigenvalues, respectively, of (R1+R2)−1(R1−R2)
(see Eq. (15.115)). Sims and Mahalanobis [55] showed that RQQCF outperforms the MACH ﬁlter when
tested with infrared imagery.
Muise et al. [56] proposed the Subspace Quadratic Synthetic Discriminant Function (SSQSDF).
The SSQSDF matrix assumes the form
Q =

n
βn ˜xn ˜xT
n .
(15.123)
The outputs ˜xT
n Q˜xn are constrained to be 1 for authentic training images and 0 for impostor training
images. The goal of SSQSDF is to design a QCF that reduces the class-scatter (a challenge in the
RQQCF design). Experiments show similar recognition performance between SSQSDF and RQQCF
[37,55].
4.15.3 Pre- and post-processing images
Pre- and post-processing techniques may be used to reduce the effect of different illumination, to register
and center the targets in the training images, to select an adequate background for the training images, to
emphasize certain frequencies over others, and to measure the peak-sharpness in the correlation plane.
Different scenarios may beneﬁt from different techniques. The following discussion summarizes the
common techniques that are sometimes used.
4.15.3.1 Preprocessing the training images
Some of the techniques used to preprocess the images are:
•
To reduce edge effects that arise at the boundaries of the image, it is suggested that setting all the
values along the entire u- and v-axis in the Fourier domain of the training images to zero.
•
Another method to reduce the edge effects is to have the background of the training images be equal
to the mean of the image.
•
Another method to reduce the edge effects is to apply a cosine window to the training images [4].
•
To reduce the effects of shadows and intense lighting, one method suggested is to apply the loga-
rithmic function log (x + 1) to the image pixels for both training and testing [4,57].
•
To reduce intensity differences, a common technique is to zero-mean the training images and then
normalize the energy.
•
Another method to reduce intensity differences is to use adaptive histogram equalization (similar to
imadjust command in Matlab).
4.15.3.2 Selecting and registering training images
It is important to choose an adequate set of images to train the ﬁlter to represent the possible distor-
tions of the target. Using too many or too few images may lower the ﬁlter’s recognition performance.
In addition the training images require registration, i.e., alignment, for better performance. In fact, poor
registration signiﬁcantly decreases the ﬁlter’s performance.

436
CHAPTER 15 Segmentation-Free Biometric Recognition
Suppose that a ﬁlter capable of recognizing 180◦of frontal head rotation is required for face recog-
nition, and a set of images that sample this range is available. One well-known technique to train the
ﬁlter is to ﬁrst choose one image from the set and train the ﬁlter with just that one image. Then that ﬁlter
is tested against all the images in the set. The image where the ﬁlter performed the worst is selected as
an additional training image, and the two training images are registered. To register, the second image
is shifted so that the cross-correlation peak is centered. Then a new ﬁlter is trained using these two
images, and all the images in the set are tested. The image where the ﬁlter performs the worst is selected
and shifted so that the correlation peak is centered (i.e., registered), and added to the training set. The
ﬁlter is retrained with these three images and this process is repeated until the ﬁlter recognizes all the
images in the set, i.e., until the ﬁlter output for all the images in the set is above some threshold value.
The number of training images is usually much less than the number of images in the training set.
During this process, however, the ﬁlter recognition performance may decrease with additional images
before reaching this threshold. When this happens, it means that the ﬁlter is not capable of capturing
all the different distortions in the image set. In this case, the set of images is divided into two or more
groups, e.g., the ﬁrst 90◦of rotation and the second 90◦of rotation. Then, for each subset, a different
ﬁlter is trained.
4.15.3.3 Transforming the images and the ﬁlter to produce sharp peaks
The ECPSDF ﬁlter equals the OTSDF ﬁlter if the training and testing images are transformed by
ˆxi = T−1
2 xi (see Eq. (15.20) for a description of T). Kumar and Mahalanobis [58] presented this
idea for the MVSDF ﬁlter (instead of the OTSDF ﬁlter) but the same principles apply here. To verify
this, the ECPSDF ﬁlter is computed using transformed images (note that T is diagonal with positive
entries–therefore symmetric and positive deﬁnite):
ˆhECPSDF = ˆX( ˆXH ˆX)−1u
=
	
T−1
2 X

 	
(T−1
2 X)H(T−1
2 X)

−1
u
= T−1
2 X(XHT−1X)−1u.
(15.124)
Applying this template ˆhECPSDF to a transformed test chip ˆz is the same as applying the OTSDF ﬁlter
to the original test chip z,
ˆhH
ECPSDFˆz = ˆhH
ECPSDF
	
T−1
2 z

=
	
T−1
2 ˆhECPSDF

H
z
=
	
T−1X(XHT−1X)−1u

H
z
= hH
OTSDFz.
(15.125)
This shows that applying the ECPSDF ﬁlter on transformed training and testing images with T−1
2
has the same effect as applying the OTSDF ﬁlter on original (non-transformed) images. Also note
from Eq. (15.125) that transforming the testing images is the same as transforming the template itself,

4.15.3 Pre- and Post-Processing Images
437
which can be used to achieve some computational advantages, i.e., transforming all the test images is
equivalent to transforming the template. This technique also applies to any ﬁlter that can be represented
as h = T−1X(XHT−1X)−1u such as the MACE and the MVSDF ﬁlters (both subsets of the OTSDF
ﬁlter–see Eq. (15.20)), and the MINACE ﬁlter (see Eq. (15.29)).
Typically, most images have the discriminative features at higher frequencies, i.e., at the edges, and
most of the energy at the lower frequencies. In the MACE ﬁlter, this T−1
2 transformation ﬂattens the aver-
age power spectrum of the training images, thereby emphasizing the high frequency components. This
also explains the sensitivity of the MACE ﬁlter to additive noise (the high frequency, where there is more
noise than signal, are emphasized). The MVSDF ﬁlter, on the other hand, emphasizes the low frequency
components.Therefore,theOTSDFﬁlterandMINACEﬁltersareusefulbecausetheyallowforatradeoff
between ONV and ACE. In the CF literature this transformation is sometimes referred as prewhitening
the images. This is somewhat abusing the terminology because each image is not strictly prewhitened,
i.e., the spectrum of the images after the transformation is somewhat ﬂattened but not strictly ﬂat.
Some ﬁlters such as the ECPSDF ﬁlter and QCF are not designed to produce sharp peaks. It was
shown that prewhitening the images and the ECPSDF ﬁlter is equivalent to using the OTSDF ﬁlter
which is designed to produce sharp peaks. It is expected that prewhitening the images and the ﬁlter in
ﬁlters such as the QCF results in OTSDF-like behavior, i.e., sharp peaks with built-in noise tolerance.
Recent experiments conﬁrm this [37,59].
4.15.3.4 Normalizing correlation output
Thezero-mean/unit-energymethods,and/orthePCEorPSRmetricsprovidesomeformofnormalization
to the correlation output, e.g., due to illumination changes in the test image scene (one part of the scene
is lighter while another part is darker) and/or in different test images (one test image is lighter while
another one is darker). In addition, PCE and PSR are useful metrics for quantifying the sharpness of
the correlation peak. Peak sharpness is a measure of how high a peak is in relation to the surrounding
values. We will discuss these in more detail in the following.
4.15.3.4.1
Zero-mean and unit energy test chips
In Section 4.15.3.1 the importance of zero-meaning and normalizing the energy in the training images
was discussed. In testing it is not always desirable to do this for a large scene. If one area of the scene
has more illumination, those values will be unfairly emphasized. Instead it may be better to normalize
every test chip.
We use 1-D notation for simplicity. The mean of each test chip (of length K) in image x(n) is
computed as follows
μx(n) = 1
K
K−1

k=0
x(n + k)
= 1
K
K−1

k=0
x(n + k)z(k)
= 1
K x(n) ⊗z(n),
(15.126)

438
CHAPTER 15 Segmentation-Free Biometric Recognition
where
z(n) =
1
0 ≤n ≤K −1,
0
n > K −1.
(15.127)
In 2-D, K represents the dimensions of the template h(m, n) before zero-padding (the template is usually
zero-padded to be greater than or equal to the test image size in order to do correlation in the Fourier
domain). The energy of zero-mean test chips is computed as follows
e(n) =
K−1

k=0

x(n + k) −μx(n)
2
=
K−1

k=0
x2(n + k) −
K−1

k=0
2x(n + k)μx(n) +
K−1

k=0
μ2
x(n)
=
K−1

k=0
x2(n + k)z(k) −2Kμ2
x(n) + Kμ2
x(n)
= x2(n) ⊗z(n) −Kμ2
x(n)
(15.128)
= x2(n) ⊗z(n) −1
K (x(n) ⊗z(n))2.
(15.129)
The correlation output is computed using zero-mean unit-energy test chips as follows
g(n) =
K−1

k=0
⎡
⎣x(n + k) −μx(n)

e(n)
 1
2
⎤
⎦h(k)
=
K−1
k=0 x(n + k)h(k) −μx(n) K−1
k=0 h(k)

e(n)
 1
2
= x(n) ⊗h(n) −μx(n)hs

e(n)
 1
2
=
x(n) ⊗h(n) −1
K

x(n) ⊗z(n)

hs
	
x2(n) ⊗z(n) −1
K

x(n) ⊗z(n)
2
 1
2
,
(15.130)
where hs = 
k h(k) is the sum of the template’s values. Using zero-mean unit-energy test chips
requires two additional correlations (or four DFTs).
4.15.3.4.2
Peak-to-correlation energy (PCE)
For test images that are about the same size as training images and when there is only one authentic
target in the image, the PCE is computed for each test image as follows:
PCE =
|gmax|2

m,n |g(m, n)|2 −|gmax|2 ,
(15.131)
where gmax = max (g(m, n)).

4.15.3 Pre- and Post-Processing Images
439
FIGURE 15.4
In this correlation output array, there is one large peak and another one small. The PSR compares the peak
values to the surrounding values. We observe in this example that the PSR values of these two correlation
peaks are similar, even though the correlation peak values are not similar.
4.15.3.4.3
Peak-to-sidelobe ratio (PSR)
Another commonly used correlation peak sharpness metric is the peak-to-sidelobe ratio (PSR) [2]
deﬁned by
PSR ≜peak −μ
σ
,
(15.132)
where μ and σ are the mean and the standard deviation, respectively, of the region surrounding the
correlation peak (in our experiments we refer to the size of this region as “PSR outer window size”)
possibly excluding a small area around the peak (in our experiments we refer to the size of this region as
“PSR inner window size”) to avoid contributions from broad correlation peaks. A PSR value of g = η
indicates that the peak is η standard deviations above the mean of the surrounding values. For example
a value of g = 6 or greater is unlikely to be the result of an impostor target. Figure 3.4.3 shows an
example of applying PSR to every value in correlation output. Figure 15.4
This metric can be efﬁciently computed for an entire correlation output using two correlations (or
four DFTs) for each image [14]. A more efﬁcient method is to compute the PSR only for the highest
correlation values. However, computing PSR at only high correlation outputs may overlook some (not
so high) peak values that stand out in comparison to their surrounding values (e.g., an authentic image in
a dark (low intensity) location in the large scene). The PSR for each value is computed as follows [14].
We use 1-D notation for simplicity. We can compute the mean of each region surrounding each
correlation value in image g(n) as follows
μ(n) = 1
K
K−1

k=0
g(n + k)
=
K−1

k=0
g(n + k)w(k)
= g(n) ⊗w(n),
(15.133)

440
CHAPTER 15 Segmentation-Free Biometric Recognition
where
w(n) =
% 1
K
0 ≤n ≤K −1,
0
n > K −1.
(15.134)
In 2-D, K represents the dimensions of the “PSR outer window” possibly excluding the “PSR inner
window.” The variance is computed as follows
σ 2(n) = 1
K
K−1

k=0

g(n + k) −μ(n)
2
= 1
K
K−1

k=0
g2(n + k) −1
K
K−1

k=0
2g(n + k)μ(n) + 1
K
K−1

k=0
μ2(n)
=
K−1

k=0
g2(n + k)w(k) −2μ2(n) + μ2(n)
= g2(n) ⊗w(n) −μ2(n).
= g2(n) ⊗w(n) −

g(n) ⊗w(n)
2 .
(15.135)
The entire PSR plane is computed as follows
gPSR(n) = g(n) −μ(n)
σ(n)
=
g(n) −g(n) ⊗w(n)
	
g2(n) ⊗w(n) −

g(n) ⊗w(n)
2
 1
2
.
(15.136)
4.15.3.5 Selecting parameters
Most CFs require the selection of parameters. One method is to use a validation set of images (images not
used in training or testing) and select parameters that produce a large Fisher Ratio (FR). The FR metric
is often used for ﬁlter performance evaluation [37,60,61]. The FR is a measurement of the separation
between two sets of ﬁlter responses expressed as
F R = |μ2 −μ1|2
σ 2
1 + σ 2
2
,
(15.137)
where μ1, μ2, σ 2
1 , and σ 2
2 are the means and variances of the two sets. Set 1 is deﬁned as the correlation
peak values in response to authentic images and Set 2 as the correlation peak values in response to
impostor images. The correlation peak values can either be the highest peak in the correlation plane,
the highest PSR value, or the PCE value.

4.15.4 Correlation Filters for Videos
441
4.15.4 Correlation ﬁlters for videos
4.15.4.1 Frame to frame CFs
Summary: One naive method to use CFs in video is by simply applying a CF to each frame. Linear CFs
require two 2-D DFTs per video frame, i.e.,
g(m, n) = F−1 
F {x(m, n)} H∗(u, v)

.
(15.138)
Note that the ﬁlter is already in Fourier domain and does not need a transform. Using FFTs, this operation
can be efﬁciently computed for most images of interest. A typical personal desktop can usually compute
the 2-D DFT of 30 (the standard video frame rate in the US) high deﬁnition (HD) images (1280 × 720
pixels) in less than 1 s.
4.15.4.2 Adaptive CFs
Summary: A better design is to use an adaptive method that can be trained online and can adapt to
varying data streams caused by changes in illuminations, backgrounds, and/or different views (e.g., due
to rotation, scale, pose, non-rigid deformation, etc). Savvides and Kumar [2] showed that the UMACE
and UOTSDF ﬁlters can be efﬁciently trained online, thus reducing the computational requirement, and
that the MACE ﬁlter can be efﬁciently trained online without inverting a matrix at each iteration. The
applicationinvestigatedwasbiometricauthenticationsystems.Bolmeetal.[5]showedthatMOSSEﬁlter
can be trained online and can be adapted in real-time by weighting new images more, with weights for
older images decaying exponentially over time. The application investigated was adaptively recognizing
faces and other objects-of-interest as the images go through different changes in illuminations and poses.
Bolme et al. reported updating the ﬁlter at a frame rate of 669 frames per second using a 2.4 GHz Core
2 Duo CPU.
4.15.4.3 Multi-frame correlation ﬁlter
Summary: The Multi-Frame Correlation Filter (MFCF) was introduced in 2009 [14]. The MFCF com-
bines information from the current correlation output with previous correlation outputs to enhance
recognition in a Bayesian framework. Each correlation value is mapped to a probability value, i.e., the
probability that there is an authentic target centered at the location given the correlation value at that
location. The array of probability values is convolved with a circular Gaussian-function-like motion
model to get a prior probability array to use for the next mapping. Use of the circular Gaussian motion
model assumes that the target can move in any direction (e.g., Brownian motion). It appears to work
well for situations where the targets move by only a few pixels from frame to frame.
Derivation: The MFCF is based on a Bayesian framework where the ﬁlter’s output is mapped into
a probability array
P

gi(m, n)|Ti(m, n)

,
(15.139)
where gi(m, n) represents the correlation output in response to the ith image, and Ti(m, n) denotes the
event that a target centered at pixel (m, n) and ˆTi(m, n) denotes the event that there is no target centered

442
CHAPTER 15 Segmentation-Free Biometric Recognition
at pixel (m, n). Using training examples, the distributions of the likelihood P(gi(m, n)|Ti(m, n)) and
of P(gi(m, n)| ˆTi(m, n)) can be estimated. The posterior probability is computed as follows
P(Ti(m, n)|gi(m, n)) =
P(gi(m, n)|Ti(m, n))P(Ti(m, n))
P(gi(m, n)|Ti(m, n))P(Ti(m, n)) + P(gi(m, n)| ˆTi(m, n))P( ˆTi(m, n))
(15.140)
where P(Ti(m, n)|gi(m, n)) represents the probability that a target is centered at location (m, n) given
a correlation value at that location at frame i, P(Ti(m, n)) represents the prior probability that a target
is centered at location (m, n), and P( ˆT (m, n)) = 1 −P(T (m, n)).
Next, a ﬁxed 2-D Gaussian τ(m, n) ∼N(0, σ 2) is convolved with the posterior to estimate the prior
probability of the object at any location in the next frame,
P(Ti+1(m, n)) = P(Ti(m, n)|gi(m, n)) ⋆τ(m, n),
(15.141)
where ⋆represent the convolution operator. This is a simple motion model and it implies that the target
may move in any direction with equal probability, with decreasing probability of moving by larger
amounts. The standard deviation of τ(m, n) is proportional to the maximum velocity (in pixels/time)
of the target.
As successive input frames become available, the probability array is updated using Bayes theorem to
reﬂect the probability based on both the estimates and the observed data. Kerekes and Kumar have shown
that this Bayesian approach improves the recognition performance of conventional CFs by suppressing
false alarms and improving the ability to detect targets in the presence of clutter.
In addition to computing the correlation plane (and possibly the PSR plane), the MFCF requires
two additional 2-D DFTs for the required convolution with the 2-D circular Gaussian. Note that other
motion models can be used instead of Gaussian.
Extensions: Mahalanobis et al. [11] used MMCF with two motion models (instead of one), one for
stationary and another for non-stationary objects. They tested their models with walking humans and
showed that MMCF outperformed single-frame correlation ﬁltering even when using a tracker.
4.15.4.4 Kalman correlation ﬁlter
Summary: The Kalman Correlation Filter (KCF) was introduced in 2010 [62]. It address one of the
problems that MFCF has. The MFCF motion model is ﬁxed and does not take into account the target’s
velocity. For example, in pedestrian detection, it is inadequate to represent the case where there may be
multiplesubjectsmovingwithdifferentvelocities(e.g.,somewalkingandsomerunning)and/orasubject
with a varying velocity (walking and then running). If the subject’s movement is large between frames,
the convolution shown in Eq. (15.141) will place the actual subject’s location under a low probability
region in the posterior probability shown in Eq. (15.140). As a result, the subject may not be detected.
In addition the MFCF may not be very robust to deal with occlusions. When a subject is occluded, the
correlation peak becomes small leading to a low value in the probability image in Eq. (15.139).
These problems may be overcome by using a tracker. One of the most basic trackers is the Kalman
ﬁlter (KF). Although there are more advanced trackers, the purpose of KCF is to show that combining
a CF with a tracker improves recognition. The KCF approach allows recursive minimum mean squared
error (MMSE) estimation of a moving target’s state.

4.15.4 Correlation Filters for Videos
443
Derivation: The KF consists of two models. The state model describes how the system evolves in
time and the observation model describes how the observations are related to the states (note that all
the notation in this derivation is in the spatial domain):
˜χt = A ˜χt−1 + ˜ϵt (State model)
˜zt = C ˜χt + ˜δt (Observation model)
(15.142)
(we use ˜χt to represent a state instead of the traditional KF notation xt to avoid confusion between an
image and a state).
A “discrete white noise acceleration model” [63] is used to allow for velocity changes. The state
vector is (KFs assume a multi-variate Gaussian distributed state)
˜χt = [px, py, vx, vy]T ,
(15.143)
where px and py represent the target location, and vx and vy represent the target’s velocities in pixels
per frame in the x−and y- directions, respectively. Note that the state represents the target in the image
plane and not in the world coordinates.
The state transition matrix is
A =
⎡
⎢⎢⎣
1 0
T
0
0 1
0
T
0 0
1
0
0 0
0
1
⎤
⎥⎥⎦,
(15.144)
where T (set to 1 without loss of generality) represents the time between the measurements. The random
variable vector ˜ϵt is modeled as
˜ϵt =
⎡
⎢⎢⎣
εx × 0.5T 2
εy × 0.5T 2
εx × T
εy × T
⎤
⎥⎥⎦,
(15.145)
where εx and εy are zero-mean Gaussian random variables with variance σ 2
a = E{ε2
x} = E{ε2
y} being
a constant chosen to represent the noise level. It is suggested [63] that the standard deviation σa be of
the order of the maximum acceleration magnitude aM. A practical range is 0.5aM ≤σa ≤aM. This
has the effect of introducing zero-mean noise propagated into the velocities and positions. Expanding
the state model (or prediction model) equations:
˜χt = A ˜χt−1 + ˜ϵt
⎡
⎢⎢⎣
pxt
pyt
vxt
vyt
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
1 0
T
0
0 1
0
T
0 0
1
0
0 0
0
1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
pxt−1
pyt−1
vxt−1
vyt−1
⎤
⎥⎥⎦+
⎡
⎢⎢⎢⎣
εx × 1
2T 2
εy × 1
2T 2
εx × T
εy × T
⎤
⎥⎥⎥⎦
=
⎡
⎢⎢⎣
pxt−1 + T vxt−1 + εx 1
2T 2
pyt−1 + T vyt−1 + εy 1
2T 2
vxt−1 + εxT
vyt−1 + εyT
⎤
⎥⎥⎦.
(15.146)

444
CHAPTER 15 Segmentation-Free Biometric Recognition
The various noise terms are correlated and the resulting noise terms have the following covariance
matrix (assume that noise terms in the x and y directions are independent, i.e., E{εxεy} = 0),
Rt = E{˜ϵt ˜ϵT
t }
= σ 2
a
⎡
⎢⎢⎣
1
4T 4
0
1
2T 3
0
0
1
4T 4
0
1
2T 3
1
2T 3
0
T 2
0
0
1
2T 3
0
T 2
⎤
⎥⎥⎦.
(15.147)
The location of the maximum peak in g(m, n) is used as the observed position of the target in the KF
model, i.e.,
˜zt =
ox
oy

,
(15.148)
where gmax = (ox, oy) is the peak location. Because the position is the only observed parameter, the
observation matrix is
C =
 1 0 0 0
0 1 0 0

.
(15.149)
Expanding the observation model (or update model) equations:
˜zt = C ˜χt + ˜δt
 oxt
oyt

=
 1 0 0 0
0 1 0 0

⎡
⎢⎢⎣
pxt
pyt
vxt
vyt
⎤
⎥⎥⎦+
 δxt
δyt

.
(15.150)
The Gaussian-distributed random vector ˜δt in the update equation is assumed to be zero-mean with
covariance matrix
Q = E{˜δt ˜δ
T
t } =
σ 2
p 0
0 σ 2
p

.
(15.151)
The variance σ 2
p is related to the peak height of the correlation output gmax. A higher peak represents
more conﬁdence in target location estimates and therefore a small variance; a smaller peak represents
lower conﬁdence and therefore a large variance. The relationship between σ 2
p and gmax (shown in
Figure 15.5) can be modeled (somewhat arbitrarily, many other such models might work) as
σ 2
p = 2.5(k−gmax) + 0.001,
(15.152)
where gmax is the highest correlation output observed and k is a constant chosen as the average gmax for
the visible target case. A small value of 0.001 is added to keep at least a very small degree of uncertainty
even for high correlation outputs.
The KCF allows recursive MMSE estimation of a dynamic target’s random vector state represented
by a mean vector μ and a covariance matrix . Algorithm 1 contains the basic pseudocode for the KCF
algorithm for one target. The ﬁve equations shown towards the bottom of Algorithm 1 are known as the
KF equations. The Kalman Gain matrix K weights conﬁdence in the prediction versus the observation.
Complete conﬁdence in the prediction corresponds to a K with zero values.

4.15.4 Correlation Filters for Videos
445
FIGURE 15.5
Relationship between gmax and the uncertainty in position measurements σ 2
p .
Algorithm 1 Single Target KCF
1: init params: A, R, C
2: while get another image ̸= NULL do
3:
compute g (apply QCF to image)
4:
observe ˜z = [(ox, oy)]T as location of gmax
5:
compute measurement error σ 2
p and Q
6:
if not ﬁrst image then ˜μ,  =UPDATE( ˆ˜μ, ˆ, Q, C)
7:
else initialize ˜μ = [˜z, 0] and  = diag(σ 2
p, σ 2
p, σ 2
v , σ 2
v )
8:
ˆ˜μ, ˆ =PREDICTION( ˜μ, , A, R)
9: end while
1: function ˜μ,  =UPDATE( ˆ˜μ, ˆ, Q, C)
2: K = ˆCT (C ˆCT + Q)−1
3: ˜μ = ˆ˜μ + K(˜z −C ˆ˜μ)
4:  = ˆ −KC ˆ
1: function ˆ˜μ, ˆ =PREDICTION( ˜μ, , A, R)
2: ˆ˜μ = A ˜μ
3: ˆ = AAT + R

446
CHAPTER 15 Segmentation-Free Biometric Recognition
This KCF algorithm provides a framework which suppresses noise by combining information from
current observations with information from all previous correlation outputs. In the MMSE sense, it
optimally computes the probability of a target before and after observing each correlation output. It is
robust to variations in velocities and acceleration and can accommodate target occlusions. For example,
if the subject is running and temporarily disappears behind some other object for a few frames, the KCF
can still assign a high probability of detection: it would place low conﬁdence in the observations (since
a low gmax yields a very high σ 2
p), and more conﬁdence in the prediction, giving an estimated position
of the target.
Algorithm 4.15.4.5, however, does not use the full information of the correlation output and therefore
only works for one target. In order to take full advantage of the properties of CFs (i.e., the ability to
locate multiple targets at unknown locations), a few modiﬁcations are made to Algorithm 4.15.4.5.
Algorithm 4.15.4.5 contains the basic pseudocode for the KCF algorithm for multiple targets.
To locate more targets in the ﬁrstimage, the highest correlation peak is found. Then, the area around
the peak is set to zero. This is repeated for the next highest peak, and repeated again until all peaks above
some threshold are located. Setting the region around each peak to zero is done to prevent confusing
one peak that extends over a few pixels with multiple targets. For each peak, a state xi is assigned with
that position and with zero velocity. The uncertainty of the position depends on the peak height, and the
uncertainty in velocity is chosen to be a constant (σ 2
v in Algorithm 4.15.4.5) for the initial states. The
prediction equations in Algorithm 4.15.4.5 are then applied to each state.
For subsequent images, the highest peak in an area near a state is found (starting with the state with the
smallest uncertainty in position). The size of the area is proportional to the state’s position uncertainty.
This peak is assigned to the state and a small area around the peak is set to zero. This is repeated for all
the states in the order of increasing position uncertainty. The update equations in Algorithm 4.15.4.5 are
then applied to each state. They ﬁnd new targets by repeating the process for the initial image using the
remaining peaks in the correlation output. Finally the update equations in Algorithm 4.15.4.5 are applied
to each state. If a state’s location uncertainty grows above some threshold (the standard deviation in
position is larger than the image), then the target is deleted either because it was lost or it never existed.
A state xi is declared to correspond to a target if the uncertainty of the state’s position after the update
equations is smaller than some threshold.
In addition to computing the correlation plane (and possibly the PSR plane), KCF requires two 4×4
matrix multiplies (and a few more smaller matrix multipliers) per state which are negligible compared
to the MFCF’s FFT computations.
4.15.4.5 Action MACH ﬁlter
Summary: The Action MACH ﬁlter was introduced in 2008 [12]. Although the authors used the MACH
ﬁlter, other linear CFs extensions should be possible. Instead of using a set of 2-D training images,
the Action MACH ﬁlter uses a set of 3-D training videos. The 3-D DFT of each video is computed,
vectorized, and used for training in the same exact way that vectorized images are used to train the
MACH ﬁlter.
The Action MACH ﬁlter can be designed using both scalar and vector features with some modiﬁca-
tions to the DFT computations when using feature vectors. The types of features used in training must
also be used for testing. For scalar features the temporal derivative of each pixel is computed. Vector

4.15.5 Experiments: Recognizing Subjects in Video
447
features used the Spatio-temporal Regularity Flow (SPREF) [64] features. Each pixel is replaced by a
3-D feature vector that represents the direction along which the intensity changes the least. In addition,
the Clifford Fourier transform [65] is used in order to compute Fourier transforms of the videos with
vector features.
Algorithm 2. Multi Targets KCF
1: init params: A, R, C
2: while get another image ̸= NULL do
3:
If not ﬁrst image then
4:
for each state ˜χi do (from smallest to largest position uncertainty)
5:
observe z = [(ox, oy)]T as loc of gmax in area
	
ˆμxi ± τ ˆ
x,xi , ˆμyi ± τ ˆ
y,y

6:
zero a small area in g around and including ˜z
7:
compute measurement error σ 2
p and Q
8:
˜μi, i =UPDATE( ˆ˜μi, ˆi, Q, C)
9:
end for
10:
end if
11:
while gmax > threshold T ′ do /∗ﬁnd new states ∗/
12:
observe z = [(ox, oy)] as loc of gmax
13:
zero a small area in g around and including ˜z
14:
compute measurement error σ 2
p and Q
15:
init ˜μi = [˜z, 0] and i = diag(σ 2
p, σ 2
p, σ 2
v , σ 2
v )
16:
end while
17:
ˆ˜μi, ˆi =PREDICTION( ˜μi, i, A, R)
18:
for all states ˜χi: if σxi > size of img, then delete state
19:
detection: for all states ˜χi: if σxi < threshold T ′′, then label state as authentic target
20: end while
Derivation: The derivation is the same as the MACH ﬁlter (see Section 4.15.2.8) using 3-D (instead
of 2-D) DFTs for the training and testing videos.
4.15.5 Experiments: recognizing subjects in video only using
ocular regions
Iris recognition is a well-known technique used to identify persons [66,67]. In general, iris recognition
requires the localization of the eye region in an image, then the segmentation of the iris and conversion of
the segmented region from Cartesian coordinates to polar coordinates. This is followed by the extraction
of features, and then the application of a classiﬁer. Low resolution images challenge iris recognition
techniques, particularly because segmenting the iris becomes very difﬁcult. In contrast, recognition
using CFs does not require any segmentation. In our experiments we assume that, in scenarios where

448
CHAPTER 15 Segmentation-Free Biometric Recognition
FIGURE 15.6
Examples of left eye ocular images from two subjects used for training in Experiment 1 at the actual resolution
used.
the iris can be captured, the whole ocular region is also captured. Ocular region refers to the area
surrounding and including the eye as shown in Figure 15.6.
There are many CFs to choose from for this work. We decided to use QCFs (see Section 4.15.2.15)
because in a previous experiment [37] it showed superior recognition performance over many other
CFs. In addition, we computed the PSR (see Section 4.15.3.4.3) of the correlation planes. We conducted
three types of experiments (the ﬁrst two experiments have been published [9] and are included here for
convenience). In Experiment 1 we have videos of two subjects (one subject in each video), in Experiment
2 we have one video of four subjects (all four subjects in one video), and in Experiment 3 we have
videos of ten subjects (one subject in each video). All videos were captured with a Canon VIXIA HD
Camcorder (a home video camera) with a resolution of 1440 × 1080 pixels at 30 frames per second.
In some videos there is motion blur, and/or the subjects have partially closed eyes, and/or the gaze is
not frontal, and/or the camera is moving, and/or the subject is moving their head, and/or the subject is
moving their eyes, making the problem more challenging. It is worth noting that for these experiments
face recognition is possible. However, we do not use face recognition because we want to show the
performance of ocular recognition since in some scenarios the full face may not be available.
From each video, we collected 10 video frames spaced evenly through the video and manually
selected the ocular region in these ten frames. The images of these ocular regions were registered (see
Section 4.15.3.2) and then used to train the QCF. To test the QCF we used every 10th frame of the video
(thereby reducing the frame rate to 3 frames per second). We note that training frames were not used as
testing frames.
In order to facilitate recognition, we desire sharp peaks. We prewhitened the training images and the
resulting templates as discussed in Section 4.15.3.3 with T−1/2 (see Eq. (15.20)).
In order to ﬁnd a good number of eigenﬁlters to use in the QCF and a good tradeoff parameter α
for the whitening factor, we computed several FRs (see Section 4.15.3.5) using different numbers of
eigenﬁlters and α values. We observed the maximum FR at α = 0.1 and 8 eigenvalues. Table 15.1
contains a list of the parameters used to train the QCF.
Experiment 1 has the goal of demonstrating the QCF’s ability to detect and distinguish two subjects
using low resolution images based on ocular data. We designed one QCF to yield sharp positive peaks
for Class 1 and sharp negative peaks for Class 2 while exhibiting low response to non-ocular regions.
The concept of using one QCF to simultaneously recognize two different types of classes by replacing
the impostor class training images with Class 2 training images is a recent novelty. We expect to get
a low response to all non-ocular regions because the ﬁlter is designed to produce a sharp peak at the

4.15.5 Experiments: Recognizing Subjects in Video
449
Table 15.1 QCF Training Parameters
Whitening alpha
0.1
No. eigenﬁlters from positive eigenvalues
4
No. eigenﬁlters from negative eigenvalues
4
PSR outer window size
10
PSR inner window size
3
ocular regions of Subjects 1 and 2 and by default a low response for everything else. The resolution of
the ocular region in the training images is 120 × 90. The radius of the boundary between the iris and
the sclera is approximately 10 pixels. Note that the pupil is not part the region used in iris recognition;
subtracting the radius of the pupil from the outer boundary radius results in approximately 6 pixels.
Iris recognition at this low resolution is infeasible as we will demonstrate at the end of this section.
We designed one QCF with ten left-eye ocular images of Subject 1 for Class 1 and ten left-eye ocular
images of Subject 2 as Class 2. Examples of these training images are shown in Figure 15.6.
This video has 50 test frames (after downsampling to 3 frames per second). The PSR output for
Frames 1 and 30 are shown in Figure 15.7. We denote correct recognition when the largest positive
peak occurs at the location of a Class 1 ocular region (denoted by the blue dot) and/or when the largest
negative peak occurs at the location of a Class 2 ocular region (denoted by the red dot). Recall that
FIGURE 15.7
The PSR output for Frames 1 and 30. The blue and red dots represent Class 1 and 2 left-ocular regions,
respectively. These dots are placed at the location of the positive and negative PSR peaks, respectively.
In this video the camera is moving so Frame 30 is shifted vertically with respect to Frame 1. In these frames
all the authentic ocular regions are correctly detected and identiﬁed.

450
CHAPTER 15 Segmentation-Free Biometric Recognition
FIGURE 15.8
The PSR output for Frame 25. This blurred and smeared frame is still able to detect and identify one of the
two left-ocular regions.
FIGURE 15.9
Examples of bi-ocular images used for training shown here at 70% of the actual resolution that was used.
correct recognition means both correct localization and classiﬁcation. The QCF correctly recognizes
99 out of 100 ocular regions. It was unable to detect one of the ocular regions in Frame 25 due to the
high blurriness and smearing in the frame shown in Figure 15.8.
In addition, we designed a QCF with right-eye ocular region images. This QCF correctly detects 100
out of 100 ocular regions (which includes the blurred Frame 25 image). We also designed a QCF with
bi-ocular images. These are images that include both eyes and their surrounded regions as shown in
Figure 15.9. The PSR output for Frames 1 and 25 are shown in Figure 15.10. Note that the PSR output
for the blurred and smeared Frame 25 correctly shows the peaks at the centers of the bi-ocular regions.

4.15.5 Experiments: Recognizing Subjects in Video
451
FIGURE 15.10
The PSR output for Frames 1 and 25 when using bi-ocular training images. Note that the PSR output for
the blurred and smeared Frame 25 correctly shows the peaks at the centers of the bi-ocular regions.
FIGURE 15.11
Examples of ocular images used for training for Experiment 2 shown here at 30% of the actual resolution
that was used.

452
CHAPTER 15 Segmentation-Free Biometric Recognition
FIGURE 15.12
Four PSR outputs per frame for four different frames with subjects (top left) N, (top right) M, (bottom left)
S, (bottom right) B. Note that, in these examples, the correct QCF produces a peak at the location of the
left-ocular region (denoted by a blue dot) while the other three QCF either produce a negative peak or no
signiﬁcant peak at that location.
This QCF correctly detects 100 out of 100 ocular regions. These results also show that, in addition to
ocular recognition, bi-ocular recognition is possible.
Experiment 2 has the goal of demonstrating the QCF’s ability to detect and distinguish between
multiple subjects. We designed a QCF to yield sharp peaks for the ocular region of a particular subject
while exhibiting low response to non-ocular regions and ocular regions of other (i.e., impostor) subjects.
The resolution of the ocular region is 400×376, which is higher than the ﬁrst video, yet still challenging
for iris recognition. The radius of the outer boundary between the iris and the sclera is approximately

4.15.5 Experiments: Recognizing Subjects in Video
453
40 pixels. Subtracting the radius of the pupil results in an iris width of approximately 25 pixels. We
designed four QCFs, each with left-eye ocular images of one subject as the authentic class and left-eye
ocular images of the other three subjects as the impostor class. Examples of training images used to train
these QCFs are shown in Figure 15.11. Note that some training images are blurred, making recognition
more challenging.
This video has 36 test frames. The PSR output for the four QCFs for four different frames containing
each of the subjects are shown in Figure 15.12. In that ﬁgure, each frame shown has four PSRs. Each
PSR is titled by a letter representing the subject that the QCF is trained to recognize. A key is also
shown above each video frame (it is the same key in all frames). A correct recognition is when the
largest positive peak occurs at the location of the ocular region of the correct subject (denoted by the
blue dot).
The results show correct recognitions for 27 out of 36 frames or 75% recognition rate. This demon-
strates that the QCF is able to detect the authentic class but, unlike Experiment 1, is challenged by
having to detect among more than two subjects. Some examples of failed recognitions are shown in
Figure 15.13.
Experiment 3 has the goal of demonstrating the QCF’s ability to detect and distinguish between ten
subjects under more challenging conditions. We designed a QCF to yield sharp peaks for the ocular
region of a particular individual while exhibiting low response to non-ocular regions and ocular regions
of other (i.e., impostor) subjects. In each video, there is one subject that moves their head and their eyes.
We used two different resolutions: high and low resolution (the high resolution is lower than Experiment
2). The resolution of the ocular region (training images) is 160 × 94 for high resolution and 60 × 36 for
low resolution. The iris width (radius of iris minus radius of pupil) is approximately 6 pixels for high
resolution and 3 pixels for low resolution. We trained one QCF ﬁlter per subject per resolution for a
total of 20 QCFs. Each QCF was designed using 10 left-eye ocular images from the authentic class and
FIGURE 15.13
The PSR output for frames with subjects N (left) and B (right) when recognition failed.

454
CHAPTER 15 Segmentation-Free Biometric Recognition
90 left-eye ocular images from the impostor subjects (10 from each impostor). Examples of training
images used to train these QCFs are shown in Figure 15.14.
We used 36 test frames per subject per resolution. We applied ten ﬁlters to each video of a given
resolution and classiﬁed the subject by the ﬁlter that gives the highest PSR. We repeated the experiment
with right-eye ocular regions. Table 15.2 has the recognition rates for each subject at high and low
resolutions using the left-eye ocular and right-eye ocular regions. These results show excellent overall
performance.
Note that the test images in the experiments were not cropped. We apply the QCF to the full test
images and are able to recognize the subjects, therefore achieving segmentation-free ocular recognition.
In order to illustrate the QCF’s superiority over iris recognition when using low resolution images, we
unfairly assist the iris recognition algorithm by manually selecting the location of the center of the irises.
Even with this assistance, the iris recognition algorithm fails to properly segment any irises (we used the
algorithm developed by Thornton et al. [8]). Figure 15.15 shows two examples of failed segmentation.
Each ﬁgure shows a mesh showing the iris regions that the algorithms found (which it unwraps to the
polar domain). Since iris recognition fails with these images, we can expect that it fails when using
even lower resolution images in Video 1. Also note that a face recognition algorithm may be able to
FIGURE 15.14
Examples of ocular images used for training for Experiment 3 shown here at 90% of the actual resolution
that was used.

4.15.5 Experiments: Recognizing Subjects in Video
455
Table 15.2 Rank-1 Recognition rates (%) for Experiment 3 for Low Resolution (low res) and
High Resolution (high res) for Subjects (sub) 1 Through 10 Using the Left-Ocular Region (L)
and Right-Ocular Region (R)
Low res
High res
Sub
L (%)
R (%)
L (%)
R (%)
1
93
96
97
99
2
91
99
98
97
3
98
100
100
99
4
84
99
94
96
5
99
99
79
84
6
96
99
100
98
7
93
96
98
97
8
95
95
88
93
9
94
99
75
78
10
97
93
95
96
FIGURE 15.15
Examples of failed attempts by the iris recognition algorithm to segment the iris region. Each ﬁgure shows
a mesh showing the iris regions that the algorithms found (which it unwraps to the polar domain).
distinguish the subjects. We do not use these algorithms to show that QCFs can distinguish subjects
by only using ocular data. We assume that only ocular data is available for training as this may be the
case in some applications. Finally, note that additional work is required to test the QCF performance

456
CHAPTER 15 Segmentation-Free Biometric Recognition
under different gaze orientations, eyelid changes, illuminations, and scales. This requires using a larger
dataset that has these variations so that we can use in training. Recall that QCF’s performance improves
with the number of training images; therefore, we expect the QCF to continue performing well with
these variations.
4.15.6 Conclusion
InthispaperwereviewedCFsandcitedworkswhereCFshavebeensuccessfullyusedforavarietyofbio-
metric applications, including face recognition, face localization, face tracking, identiﬁcation encoding,
ﬁngerprint recognition, iris recognition, ocular recognition, pedestrian localization, and human actions.
We showed that CFs can be used in videos to aid single-frame recognition or for action recognition.
We described in detail ocular recognition experiments. Although iris recognition works well at high
resolution, it often fails under low resolution, where CFs can succeed because no segmentation is
required. We conducted three experiments, one with two subjects and another one with four subjects.
CFs were able to detect and classify most of the subjects in these experiments in low resolution, where
traditional iris recognition techniques fail.
A Appendix
A.1 Minimizing a quadratic subject to linear constraints
The goal is to minimize a quadratic subject so some linear constraints, i.e.,
min
h
hHTh
(15.153)
s.t. XHh = u,
where T is assumed to be a Hermitian (symmetric if T is real) matrix. We use Lagrange multipliers,
take the gradient, set it equal to zero and solve for x, i.e.,
L(h, ) = hHTh −2H(XHh −u),
(15.154)
and taking the gradient gives,
dL(h, )
dh
= 2Th −2X = 0,
(15.155)
and solving for h gives,
h = T−1X.
(15.156)
Substituting h into the original constraint we can solve for the Lagrange multiplier vector, i.e.,
XHh = u
XH(T−1X) = u,
(15.157)
and solving for  gives,
 = (XHT−1X)−1u.
(15.158)

References
457
Finally substituting this back into h gives the ﬁnal answer, i.e.,
h = T−1X
= T−1X(XHT−1X)−1u.
(15.159)
A.2 Minimizing a ratio of quadratic terms
The goal is to ﬁnd the h that minimizes
J(h) = hHAh
hHBh ,
(15.160)
where A and B are assumed to be Hermitian matrices. This is accomplished by taking the gradient and
setting it equal to zero as follows
J(h) = 2Ah(hHBh) −2Bh(hHAh)
(hHBh)2
= 0,
Ah(hHBh) = Bh(hHAh),
Ah = BhhHAh
hHBh ,
B−1Ah = hJ(h),
B−1Ah = λh,
(15.161)
noting that hHBh ̸= 0, J(h) = λ, and hHAh are scalar values. This is an eigenvalue problem. Since
J(h) = λ represents the eigenvalues of B−1A, then J(h) is maximized by the eigenvector corresponding
to the largest eigenvalue of B−1A.
Relevant Theory: Signal Processing Theory and Machine Learning
See Vol. 1, Chapter 9 Discrete Multi-Scale Transforms in Signal Processing
See Vol. 1, Chapter 13 Introduction: Machine Learning
References
[1] M. Savvides, J. Heo, J. Thornton, P. Hennings, C. Xie, K. Venkataramani, R.A. Kerekes, M. Beattie, B.V.K.
Vijaya Kumar, Biometric identiﬁcation using advanced correlation ﬁlter methods, in: Springer-Verlag Lecture
Notes in Computer Science, Ambient Intelligence 2005.
[2] M.Savvides,B.V.K.VijayaKumar,Efﬁcientdesignofadvancedcorrelationﬁltersforrobustdistortion-tolerant
face recognition, in: IEEE Conference on Advanced Video and Signal Based Surveillance, 2003.
[3] C. Xie, M. Savvides, B.V.K. Vijaya Kumar, Quaternion correlation ﬁlters for face recognition in wavelet
domain, in: Proceedings of the International Conference on Acoustic, Speech and Signal Processing, 2005.
[4] D.S. Bolme, B.A. Draper, J.R. Beveridge, Average of synthetic exact ﬁlters, in: IEEE Conference on Computer
Vision and Pattern Recognition, 2009.
[5] D.S. Bolme, J.R. Beveridge, B.A. Draper, Y.M. Lui, Visual object tracking using adaptive correlation ﬁlters,
in: IEEE Conference Computer Vision and Pattern Recognition, 2010.

458
CHAPTER 15 Segmentation-Free Biometric Recognition
[6] V.N. Boddeti, F. Su, B.V.K. Vijaya Kumar, A biometric key-binding and template protection framework using
correlation ﬁlters, in: Lecture Notes on Computer Science, 2009.
[7] B.V.K. Vijaya Kumar, M. Savvides, C. Xie, K. Venkataramani, J. Thornton, A. Mahalanobis, Biometric
veriﬁcation with correlation ﬁlters, Appl. Opt. 43 (2) (2004) 391–402.
[8] J. Thornton, M. Savvides, B.V.K. Vijaya Kumar, A Bayesian approach to deformed pattern matching of iris
images, IEEE Trans. Pattern Anal. Mach. Intell. 29 (4) (2007) 596–606.
[9] A. Rodriguez, B.V.K. Vijaya Kumar, Segmentation-free ocular detection and recognition, in: Proceedings of
the SPIE, 2011.
[10] D.S. Bolme, Y.M. Lui, B.A. Draper, J.R. Beveridge, Simple real-time human detection using a single cor-
relation ﬁlter, in: IEEE International Workshop on Performance Evaluation of Tracking and Surveillance,
2010.
[11] A. Mahalanobis, R. Stanﬁll, K. Chen, A bayesian approach to activity detection in video using multi-frame
correlation ﬁlters, in: Proceedings of the SPIE 2011.
[12] M.D. Rodriguez, J. Ahmed, M. Shah, Action MACH–a spatio-temporal maximum average correlation height
ﬁlter for action recognition, in: IEEE Conference on Computer Vision and Pattern Recognition, 2008.
[13] B.V.K. Vijaya Kumar, A. Mahalanobis, R.D. Juday, Correlation Pattern Recognition, Cambridge University
Press, 2005.
[14] R.A. Kerekes, B.V.K. Vijaya Kumar, Enhanced video-based target detection using multi-frame correlation
ﬁltering, IEEE Trans. Aerosp. Electron. Syst. 45 (1) (2009) 289–307.
[15] A. Rodriguez, B.V.K. Vijaya Kumar, Automatic target recognition of multiple targets from two classes with
varying velocities using correlation ﬁlters, in: IEEE International Conference of Image Processing, 2010.
[16] C.F. Hester, D. Casasent, Multivariant technique for multiclass pattern recognition, Appl. Opt. 19 (11) (1980)
1758–1761.
[17] B.V.K. Vijaya Kumar, Minimum-variance synthetic discriminant functions, J. Opt. Soc. Am. A 3 (10) (1986)
1579–1584.
[18] A. Mahalanobis, B.V.K. Vijaya Kumar, D. Casasent, Minimum average correlation energy ﬁlters, Appl. Opt.
26 (5) (1987) 3633–3640.
[19] Ph. Réfrégier, Filter design for optical pattern recognition: multicriteria optimization approach, Opt. Lett. 15
(15) (1990) 854–856.
[20] G. Ravichandran, D. Casasent, Minimum noise and correlation energy (MINACE) optical correlation ﬁlter,
Appl. Opt. 31 (11) (1992) 1823–1833.
[21] B.V.K. Vijaya Kumar, A. Mahalanobis, S. Songs, S. Sims, J. Epperson, Minimum squared error synthetic
discriminant function, Opt. Eng. 31 (5) (1992) 915–922.
[22] A. Mahalanobis, B.V.K. Vijaya Kumar, S. Song, S.R.F. Sims, J.F. Epperson, Unconstrained correlation ﬁlters,
Appl. Opt. 33 (17) (1994) 3751–3759.
[23] A. Mahalanobis, B.V.K. Vijaya Kumar, S.R.F. Sims, Distance classiﬁer correlation ﬁlters for distortion toler-
ance, discrimination and clutter rejection, in: Proceedings of the SPIE, 1993.
[24] Y.N. Hsu, H.H. Arsenault, Optical character recognition using circular harmonic expansion, Appl. Opt. 21
(22) (1982) 4016–4019.
[25] R. Wu, H. Stark, Rotation-invariant pattern recognition using a vector reference, Appl. Opt. 23 (6) (1984)
838–840.
[26] D. Mendlovic, E. Marom, N. Konforti, Shift and scale invariant pattern recognition using Mellin radial
harmonics, Opt. Comm. 67 (3) (1988) 172–176.
[27] E. Tajahuerce, A. Moya, J. Garcia, C. Ferreira, Real ﬁlter based on Mellin radial harmonics for scale-invariant
pattern recognition, Appl. Opt. 33 (14) (1994) 3086–3093.
[28] B.V.K. Vijaya Kumar, A. Mahalanobis, A. Takessian, Optimal tradeoff circular harmonic function correlation
ﬁlter methods providing controlled in-plane rotation response, IEEE Trans. Image Process. 9 (6) (2000)
1025–1034.

References
459
[29] R.A. Kerekes, B.V.K. Vijaya Kumar, Correlation ﬁlters with controlled scale response, IEEE Trans. Image
Process. 15 (7) (2006) 1794–1802.
[30] A. Mahalanobis, B.V.K. Vijaya Kumar, Polynomial ﬁlters for higher-order and multi-input information fusion,
in: Euro American Opto-Electronic Information Processing Workshop, 1997.
[31] A. Mahalanobis, R. Muise, S.R. Stanﬁll, Quadratic correlation ﬁlter design methodology for target detection
and surveillance applications, Appl. Opt. 43 (27) (2004) 5198–5205.
[32] D. Casasent, G. Ravichandran, S. Bollapraggada, Gaussian minimum average correlation energy correlation
ﬁlters, Appl. Opt. 30 (35) (1991) 5176–5181.
[33] M. Savvides, B.V.K. Vijaya Kumar, P. Khosla, Face veriﬁcation using correlation ﬁlters, in: IEEE Workshop
on Automatic Identiﬁcation Advanced Technologies, 2002.
[34] Ph. Réfrégier, J. Figue, Optimal trade-off ﬁlters for pattern recognition and their comparison with the wiener
approach, Opt. Comput. Process. 1 (3) (1991) 245–266.
[35] R.K. Shenoy, Object detection and classiﬁcation in sar images using minace correlation ﬁlters, Master’s Thesis,
Carnegie Mellon University, Pittsburgh, April 1995.
[36] J. Figue, P. Refregier, Optimality of trade-off ﬁlters, Appl. Opt. 32 (11) (1993) 1933–1935.
[37] R.A. Kerekes, B.V.K. Vijaya Kumar, Selecting a composite correlation ﬁlter design: a survey and comparative
study, Opt. Eng. 47 (6) (2008) 1–18.
[38] A. Mahalanobis, H. Singh, Application of correlation ﬁlters for texture recognition, Appl. Opt. 33 (11) (1994)
2173–2179.
[39] A. Mahalanobis, B.V.K. Vijaya Kumar, R. Frankot, Intraclass and between-class training-image registration
for correlation-ﬁlter synthesis, Appl. Opt. 39 (17) (2000) 2918–2924.
[40] A. Nevel, A. Mahalanobis, Comparative study of maximum average correlation height ﬁlter variants using
ladar imagery, in: Proceedings of the SPIE, 2003.
[41] B. Walls, A. Mahalanobis, Performance of the MACH ﬁlter and DCCF algorithms in the presence of data
compression, in: Proceedings of the SPIE, 1999.
[42] M. Alkanhal, B.V.K. Vijaya Kumar, A. Mahalanobis, Improving the false alarm capabilities of the maximum
average correlation height correlation ﬁlter, Opt. Eng. 39 (5) (2000) 1133–1141.
[43] B.V.K. Vijaya Kumar, M. Alkanhal, Eigen-extended maximum average correlation height (EEMACH) ﬁlters
for automatic target recognition, in: Proceedings of the SPIE, 2001.
[44] B.V.K. Vijaya Kumar, D.W. Carlson, A. Mahalanobis, Optimal trade-off synthetic discriminant function ﬁlters
for arbitrary devices, Opt. Lett. 19 (19) (1994) 1556–1558.
[45] P. Banerjee, J. Chandra, A. Datta, Feature based optimal trade-off parameter selection of frequency domain
correlation ﬁlter for real time face authentication, in: Proceedings of the International Conference on Com-
munication, Computing and Security, 2011.
[46] R. Singh, B.V.K. Vijaya Kumar, Performance of the extended maximum average correlation height (EMACH)
ﬁlter and the polynomial distance classiﬁer correlation ﬁlter (PDCCF) for multiclass SAR detection and
classiﬁcation, in: Proceedings of the SPIE, 2002.
[47] G.F. Schils, D.W. Sweeney, Rotationally invariant correlation ﬁltering, J. Opt. Soc. Am. A 2 (9) (1985)
1411–1418.
[48] B.V.K. Vijaya Kumar, T. Ng, Multiple circular-harmonic-function correlation ﬁlter providing speciﬁed
response to in-plane rotation, Appl. Opt. 11 (11) (1996) 1871–1878.
[49] A.V. Oppenheim, R.W. Schafer, J.R. Buck, Discrete-Time Signal Processing, Prentice Hall, 2009.
[50] A. Mahalanobis, B.V.K. Vijaya Kumar, S.R.F. Sims, Distance classiﬁer correlation ﬁlters for multiclass auto-
matic recognition, Appl. Opt. 35 (17) (1996) 3127–3133.
[51] M. Alkanhal, B.V.K. Vijaya Kumar, Polynomial distance classiﬁer correlation ﬁlter for pattern recognition,
Appl. Opt. 42 (23) (2003) 4688–4708.
[52] K. Al-Mashouq, B.V.K. Vijaya Kumar, M. Alkanhal, Analysis of signal-to-noise ratio of polynomial correla-
tion ﬁlters, in: Proceedings of the SPIE, 1999.

460
CHAPTER 15 Segmentation-Free Biometric Recognition
[53] A. Mahalanobis, R. Muise, S.R. Stanﬁll, A. Van Nevel, Design and application of quadratic correlation ﬁlters
for target detection, Appl. Opt. 40 (3) (2004) 837–850.
[54] K. Fukunaga, W.L.G. Koontz, Representation of random processes using the ﬁnite karhunen-loève expansion,
IEEE Trans. Inform. Contr. 16 (1) (1970) 85–101.
[55] S.R.F. Sims, A. Mahalanobis, Performance evaluation of quadratic correlation ﬁlters for target detection and
discrimination in infrared imagery, Opt. Eng. 43 (8) (2004) 1705–1711.
[56] R. Muise, A. Mahalanobis, R. Mohapatra, X. Li, D. Han, W. Mikhael, Constrained quadratic correlation ﬁlters
for target detection, Appl. Opt. 43 (2) (2004) 304–314.
[57] M. Savvides, B.V.K. Vijaka Kumar, Illumination normalization using logarithm transforms for face authenti-
cation, in: Proceedings of the International Conference on Advances in Pattern Recognition, 2003.
[58] B.V.K. Vijaya Kumar, A. Mahalanobis, Alternate interpretation for minimum variance synthetic discriminant
functions, Appl. Opt. 25 (15) (1986) 2484–2485.
[59] A. Rodriguez, B.V.K. Vijaya Kumar, Automatic multi-target recognition from two classes using quadratic
correlation ﬁlters, in: Proceedings of the SPIE, 2010.
[60] R. Singh, Advanced correlation ﬁlters for multi-class synthetic aperture radar detection and classiﬁcation,
Master’s Thesis, Carnegie Mellon University, Pittsburgh, May 2002.
[61] A. Van Nevel, A. Mahalanobis, Comparative study of maximum average correlation height ﬁlter variants using
ladar imagery, Opt. Eng. 42 (2) (2004) 541–550.
[62] A.Rodriguez, J.Panza, B.V.K.VijayaKumar, Automaticrecognitionofmultipletargetswithvaryingvelocities
using quadratic correlation ﬁlters and kalman ﬁlters, in: IEEE Radar, 2010.
[63] Y. Bar-Shalom, X.R. Li, T. Kirubarajan, Estimation with applications to tracking and navigation, Wiley,
New York, 2001.
[64] O. Alatas, P. Yan, M. Shah, Spatio-temporal regularity ﬂow (SPREF): its estimation and applications, IEEE
Trans. Circuits Syst. Video Technol. 17 (5) (2007) 584–589.
[65] J. Ebling, G. Scheuermann, Clifford Fourier transform on vector ﬁelds, IEEE Trans. Visual. Comput. Graphics
11 (4) (2005) 469–479.
[66] J.G. Daugman, High conﬁdence visual recognition of persons by a test of statistical independence, IEEE
Trans. Pattern Anal. Mach. Intell. 15 (11) (2002) 1148–1161.
[67] J.G. Daugman, How iris recognition works, IEEE Trans. Circuits Syst. Video Technol. 14(1) (2004) 21–30.

16
CHAPTER
Dynamical Systems in Video
Analysis
Gianfranco Doretto*,1, Avinash Ravichandran†,1, René Vidal‡, and Stefano Soatto†
*West Virginia University, USA
†University of California, Los Angeles, USA
‡Johns Hopkins University, USA
4.16.1 Introduction
Consider a sequence of images of a moving scene. Each image is an array of positive numbers that
depends upon the shape, pose, viewpoint (geometry), material reﬂectance properties, and light distri-
bution (photometry) of the scene, as well as upon the changes of all of these factors over time, i.e.,
upon the dynamics of the scene. In principle, to fully analyze and understand the properties of a video
sequence, one would want to recover the physical model of the scene that has generated the images.
Unfortunately, it is well known that the joint reconstruction of photometry, geometry, and dynamics
of the scene (visual reconstruction problem) is an intrinsically ill-posed problem: From any number
of images it is not possible to uniquely recover all the unknowns (shape, pose, reﬂectance, light dis-
tribution, and viewpoint). The ill-posedness of the visual reconstruction problem can be turned into a
well-posed inference problem within the context of a speciﬁc task, where a particular class of models
is chosen to represent the data based on the needs, and where the extra degrees of freedom are set by
additional optimality criterions, typically chosen to the beneﬁt of the application at hand.
This chapter focusses on the use of a simple class of statistical models, named (stochastic) linear
dynamical systems (LDS) [1], for addressing a number of video analysis tasks. The use of LDSs to
extract information from video has a long history, which, until recently, it mainly had to do with the
use of the Kalman ﬁlter [2]. In fact, they have been used for vision-based motion estimation [3–5],
autonomous navigation [6,7], visual servoing for unmanned vehicles [8,9], and object tracking [10].
Starting with the work of [11–13], during the last decade LDSs have been used also as a statistical
model of the video signal. In this role, such models fail to capture the correct photometry, geometry,
and dynamics of the scene. However, they capture a mixture of the three that is sufﬁcient to provide a
representation of the measured signal, and is able to support several video analysis tasks. This chapter
will discuss the theory and applications of LDSs in this latter role.
The introduction of LDSs as a representation for video stemmed from the need to model a certain
type of moving scenes containing non rigid objects, such as a sequence of ﬁre, smoke, water, foliage
or ﬂowers in wind, clouds, crowds of waving people, etc. What such video sequences have in common
is that, loosely speaking, they exhibit some form of stochastic motion repetitiveness. If the frames of a
1G. Doretto and A. Ravichandran contributed equally to this work.
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00016-9
© 2014 Elsevier Ltd. All rights reserved.
461

462
CHAPTER 16 Dynamical Systems in Video Analysis
video sequence are interpreted as a realization from a vector-valued temporal stochastic process, then
such characteristic can be described statistically by the notion of temporal stationarity of the process.2
These types of video sequences are referred to as dynamic textures [11].
While there has been a considerable amount of work for analyzing and understanding 2D textures,
starting with the pioneering work of Julesz [14], until the more recent statistical models (see [15]
and references therein), there has been comparatively little work in the speciﬁc area of dynamic (or
time-varying) textures. The problem has been ﬁrst addressed by Nelson and Polana [16], who classify
regional activities of a scene characterized by complex, non-rigid motion. Szummer and Picard’s work
[17] on temporal texture modeling uses the spatio-temporal auto-regressive model, which imposes a
neighborhood causality constraint for both spatial and temporal domains. This restricts the range of
processes that can be modeled, and does not allow to capture rotation, acceleration and other simple
non translational motions. Bar-Joseph et al. [18] uses multi-resolution analysis and tree-merging for
the synthesis of 2D textures and extends the idea to dynamic textures by constructing trees using a
3D wavelet transform. Parallel to these approaches there is the work of Wang and Zhu [19,20] where
images are decomposed by using a dictionary of Gabor or Fourier bases to represent image elements
called “movetons,” or by computing their primal sketch. The model captures the temporal variability of
movetons, or the graph describing the sketches.
The missing piece of the puzzle that brings LDSs inside the big picture is one of the most important
theorems in time series analysis and stochastic systems, which states that a stationary stochastic process
can be modeled as the output of an LDS with white noise as input [21]. This means that for a given task,
whenever representing a video as a dynamic texture is good enough, then modeling it with an LDS is
the right thing to do. Such representation is referred to as dynamic texture model [11]. Section 4.16.2
introduces this framework, which has the major advantage of allowing to make analytical statements,
as well as to draw from the rich literature on linear state-space systems.
The problem of estimating the dynamic texture model of a given video sequence is called system
identiﬁcation. Section 4.16.3 describes several aspects of this problem. First of all, the nature of the input
data (i.e., images) imposes computational constraints that prevent from using classical identiﬁcation
toolsinastraightforwardway.Second,theidentiﬁcationofamodelisnotunique,butuptoanequivalence
class. This imposes to express the model in a suitable canonical form to accommodate the given task
at hand. Finally, a particular application might require the identiﬁcation to satisfy certain constraints.
An example could be the need to identify a model that is stable. Another one could be to identify models
that share the same dynamics. This one and the former cases are brieﬂy discussed.
Once a video sequence is translated into a dynamic texture model, in principle a given task could
be approached in the space of models. Comparing models, as a way to compare sequences, is probably
the most important basic operation that is the key enabler of most sophisticated ones. Section 4.16.4
addresses several approaches for comparing dynamic texture models. The ﬁrst one is to design distance
functions between models. This is a nontrivial problem because the space of model parameters is the
Cartesian product of spaces with a well deﬁned structure, which turns out to be a nonlinear manifold.
The second approach entails computing the distance between the probability distributions of the output
2A stochastic process is stationary (of order k) if the joint statistics (up to order k) are time-invariant. For instance a process
{I(t)} is second-order stationary if its mean ¯I .= E[I(t)] is constant and its covariance E[(I(t1)−¯I)(I(t2)−¯I)] only depends
upon t2 −t1.

4.16.3 Identiﬁcation
463
of LDSs. The third approach uses the notion of kernels to measure model discrepancies. Finally, Section
4.16.4.2 describes a family of kernels able to compare LDSs while taking into account all the model
parameters, including the initial condition.
In Section 4.16.5 the dynamic texture modeling framework is exploited to approach several tasks.
Section 4.16.5.1 describes how the generative nature of the model is used to synthesize novel video
sequences, and how model parameter changes map into deﬁned visual perceptual changes. Section
4.16.5.2 describes how models can be compared to recognize dynamic textures corresponding to the
same scene. Section 4.16.5.3 explains how a video sequences can be thought of as a composition of
multiple dynamic textures. In such a case one should be able to compute models and boundaries between
dynamic textures, a task called segmentation. Section 4.16.5.4 addresses the task of video registration
based on dynamic textures, where videos of the same scene need to be temporally and spatially aligned.
The approach has the advantage of fully decoupling, in a principled way, the spatial from the temporal
alignment problems, while exploiting the entire dataset, which improves robustness.
Finally, Section 4.16.6 describes the most popular datasets currently available for testing approaches
based on the dynamic texture framework, and Section 4.16.7 points to a few open issues and closes the
chapter.
4.16.2 Model
Given a video sequence with F frames each of p pixels, {I(t) ∈Rp}F
t=1, the pixel intensities of each
frame, I(t), are modeled as the output of a linear dynamical system (LDS), i.e.,
z(t + 1) = Az(t) + Bv(t),
I(t) = C0 + Cz(t) + w(t),
(16.1)
where z(t) ∈Rn is the hidden state at time t, A ∈Rn×n models the dynamics of the hidden state,
C ∈Rp×n maps the hidden state to the output of the system, C0 ∈Rp is the mean of the video sequence,
and w(t) ∼N(0, R) and Bv(t) ∼N(0, Q) are the measurement and process noise, respectively.
As it is often the case, the transformation, Q = BB⊤is used to incorporate the B matrix in the noise
covariance and Bv(t) is replaced by v′(t). The dimension of the hidden state, n, is the order of the
system.
Note that the representation (16.1) is an exact parametric generative model for the video sequence,
only if {I(t)} can be interpreted as a realization from a stochastic process that is at least second-order
stationary [21]. Such video representation is referred to as dynamic texture model [11].
4.16.3 Identiﬁcation
Given a video sequence {I(t)}F
t=1, estimating the parameters (C0, A, C, Q, R), of the dynamical model
(16.1) is called system identiﬁcation and it is a well studied problem in the control community [21].
Several optimal methods such as N4SID [22] and Expectation Maximization (EM) [23] can be used to
learn these parameters. However, due to the high dimensionality of the output (number of pixels p), these

464
CHAPTER 16 Dynamical Systems in Video Analysis
procedures become computationally intractable. In order to combat this issue, this section describes a
closed form suboptimal identiﬁcation method as proposed in [11].
Let Z and W be such that Z .= [z(1), z(2), . . . , z(F)] ∈Rn×F and W .= [w(1), w(2), . . . , w(F)] ∈
Rp×F. An estimate of the mean of the video sequence is obtained as ˆC0 = 1
F
F
t=1 I(t). It can be shown
that the matrix I = [I(1) −C0, I(2) −C0, . . . , I(F) −C0] ∈Rp×F, obtained by stacking together
the mean-subtracted vectorized version of the frames of the video sequence can be written as
I = CZ + W.
(16.2)
Now let I = U SV ⊤be the singular value decomposition of I, where U ∈Rp×n, V ∈RF×n, and
S ∈Rn×n is the diagonal matrix with the singular values {σi}n
i=1. The system identiﬁcation problem
can be cast as
( ˆC, ˆZ) = arg min
C,Z ∥W∥F,
(16.3)
where ∥· ∥F indicates the Frobenius norm. By the ﬁxed rank approximation property of the SVD [24],
the parameters can be obtained as
ˆC = U,
ˆZ = SV ⊤.
(16.4)
Given C and Z, in a similar manner A is determined as the solution to the following minimum least
squares problem
ˆA = arg min
A
Z F
2 −AZ F−1
1

F ,
(16.5)
where Z F
2
.= [z(2), z(3), . . . , z(F)], and Z F−1
1
.= [z(1), z(2), . . . , z(F −1)]. The solution to problem
(16.5) is obtained in closed form as3
ˆA = Z F
2 Z F−1
1
†.
(16.6)
Finally, the input noise covariance Q is estimated from
ˆQ = 1
F
F

t=1
ˆv′(t)ˆv′(t)⊤,
(16.7)
where ˆv′(t) = z(t + 1) −ˆAz(t). The algorithm outlined above assumes that the input video sequence
is gray-scale. However, it can be extended in several ways for the case of color video sequences.
The approach in [11] concatenates the RGB channels into one single vector, i.e., I(t) ∈R3p instead
of I(t) ∈Rp. Another relevant extension, which can be found in [25], is the ability to identify the
parameters of the model causally, in an online fashion. This is very useful for developing fast, real-time
algorithms required to produce results given the data collected so far.
Notice that the above discussion assumes that the order of the system, n, is known. In practice,
one has to estimate it. There are several techniques to address this model selection problem [21].
The simplest approach consists of analyzing the singular values of I and selecting the order based on
them [26]. Moreover, the above method of identiﬁcation does not respect the nature of the model, which
3The super-script † indicates the computation of the pseudo-inverse matrix.

4.16.3 Identiﬁcation
465
imposes z(t) and z(t +1) to be related by z(t +1) = Az(t)+v′(t). This problem is overcome by optimal
approaches, such as those based on EM [23], or N4SID [22]. However, they can be used only when p is
sufﬁciently small (typically O(F)), which might be true for a very small image patch size. In particular,
the EM-based method is an iterative approach that uses a maximum likelihood objective function. It alter-
nates between two steps. The E-step estimates the hidden states Z, while the M-step updates the model
parameters, namely (A, C, Q, R). On the other hand, the N4SID is an algebraic algorithm that uses the
past and future observations to determine the model parameters. Unlike EM, N4SID has the advantage
of estimating the parameters in closed form. However, since it uses the entire set of observations at
once, the memory, the computation, as well as the data requirements grow quickly as p or F grow.
Change of basis and canonical forms
The parameters of the LDS that model a given video sequence {I(t) ∈Rp}F
t=1, are not unique. In fact,
in lieu of (A, C, B) one could substitute (P−1 AP, C P, P−1B) in Eq. (16.1), where P ∈GL(n) is any
invertible matrix, and obtain the same outputs i.e., the video sequence {I(t) ∈Rp}F
t=1. This issue does
not pose a problem when dealing with a single video sequence. However, when comparing multiple
video sequences based on their models, the fact that different sets of parameters may represent the same
sequence, only with respect to different basis, could pose an issue. In order to overcome it, one can
ﬁrst identify the parameters via system identiﬁcation, and then transform them with respect to a chosen
basis. The resulting model is called a canonical form with respect to such basis [1].
The control community has studied several canonical forms for LDSs. In principle, it should not
matter which one to use, because a canonical form is a representative of an entire equivalence class
of LDS parameters. However, different applications pose different restrictions on the choice of the
canonical form. In what follows, we review different canonical forms that have been used in the literature
and outline their advantages and disadvantages.
The simplest canonical form is the diagonal form for the A matrix. In this case, the matrix P
transforming the original model parameters is such that P−1 AP is diagonal. One drawback of this
canonicalformisthattheresultingdiagonalmatrixcanbecomplex.However,thereexistsothercanonical
forms such as the Reachability Canonical Form (RCF), the Observability Canonical Form (OCF), and
the Jordan Canonical Form (JCF) that do not suffer from this issue. The RCF is given by
(16.8)
where {ai} are the coefﬁcients of the characteristic polynomial of A, i.e., An + an−1An−1 + · · · + a0I
= 0 and In−1 is the identity matrix of size n −1. The RCF uses the pair (A, B) to convert the system
into canonical form. However, if one prefers having a canonical form based on the parameters (A, C),

466
CHAPTER 16 Dynamical Systems in Video Analysis
a suitable candidate is the Observability Canonical Form (OCF) [27], given by
(16.9)
Another canonical form that uses the (A, C) parameters todeterminethebasis is theJCF. If A ∈Rn×n
has 2q complex eigenvalues and n −2q real eigenvalues, the JCF is given by
Ac =
⎡
⎢⎢⎢⎢⎢⎣
σ1
ω1
0
· · ·
0
−ω1
σ1
0
· · ·
0
...
...
...
0
0
0
0
0
λ2q−n−1
0
0
0
. . .
0
λ2q−n
⎤
⎥⎥⎥⎥⎥⎦
∈Rn×n and Cc =
1 0 1 0 . . . 1 1 
∈R1×n,
(16.10)
where the eigenvalues of A are {σ1 ± iω1, σ2 ± iω2, . . . , σq ± iωq, λ1, . . . , λn−2q}.
In addition to the above canonical forms, given two dynamical systems, one can choose either of them
as a reference, and convert the parameters of the other one with respect to the basis of the reference. One
choice for such a canonical form is to restrict the columns of the reference C matrix to be orthogonal.
Then, the C matrices of other dynamical systems are projected onto the subspace spanned by the refer-
ence C. Such an approach was used in [28]. While simple, the drawback of this strategy is that a reference
system needs to be chosen, and this raises the question of which one might be a “good” reference.
The choice of the canonical form is a function of the application. The diagonal canonical form was
used for optical ﬂow estimation in [29], the RCF and the JCF were used for dynamic texture registration
in [30] and [31], respectively. One drawback of the OCF is that the estimation of the transformation
that converts the system parameters to the canonical form is numerically unstable [27]. Hence, existing
methods have not used this canonical form.
Once the model parameters are obtained via system identiﬁcation, they can be mapped to any of the
canonical forms discussed above. This will ensure that they are all in the same basis. Consequently, com-
paring different dynamical systems becomes “easier.” However, as will be explained later, there are met-
rics for comparing dynamical systems which are invariant to the particular choice of basis with respect
to which the parameters are expressed. In such cases, one can omit the conversion to a canonical form.
4.16.3.1 Constrained identiﬁcation
In some instances, one might like to impose constraints on the system parameters to satisfy the purpose
of the application. For instance, in the case of synthesis, the estimated system should be stable. This is
because an unstable system would synthesize “exploding” outputs corresponding to image intensities
outside of the visible range. This section reviews some of the constraints that are imposed on dynamical
systems, as well as how parameters should be estimated to satisfy them.

4.16.3 Identiﬁcation
467
Stable systems
In the case of discrete time linear systems, the system is said to be BIBO (bounded input bounded
output) stable if all the eigenvalues of the A matrix are within the unit circle of the complex plane, i.e.,
ρ(A) < 1, where ρ(·) denotes the spectral radius. When system identiﬁcation is performed on real data,
the resulting system might not be stable by a small margin [27], even though there might be expectations
that it should be, like for the case of synthesizing dynamic texture videos. One practical provision to
address this issue (identiﬁed by ρ(A) > 1) is to rescale the eigenvalues of the matrix A towards the
origin by enough margin to guarantee stability. This is done by replacing A with ˜A .= γ A/ρ(A), where
γ < 1 (typically very close to 1) sets how big the margin of stability should be.
Another approach, which seems to be more principled because it does not require to set a margin of
stability at the outset, is illustrated in [32]. There, the problem (16.5) is replaced with
ˆA = arg
min
{A|ρ(A)<1}
Z F
2 −AZ F−1
1

F ,
(16.11)
where the estimation of the matrix A is restricted to the set of sable matrices. Since the space of stable
matrices is not convex, the constraint on the spectral radius has to be replaced with a stronger, surrogate
constraint on the eigenvalues of the matrix A, which allows one to do optimization within the convex
space of matrices whose singular values are less than one. The algorithm converges to a stable A matrix
that can be further reﬁned. The interested reader is referred to [32] for more details on this approach
and other stable identiﬁcation methods.
Marginally stable systems
A discrete time LDS is marginally stable when the spectral radius of A, i.e., ρ(A), is 1. There are
applications where it might be of interest to consider this particular situation. One such case is the
modeling and synthesis of dynamic texture sequences that are periodic signals over time. An example
is shown in Figure 16.1, depicting an escalator, whereas Figure 16.2a shows the periodic nature of the
state Z by plotting ∥z(t) −z(1)∥F. The dynamic texture model (16.1) suitable for this sequence is such
that Q = 0, which means that the system is not excited by noise, and all the eigenvalues of A (the poles
of the LDS) are located on the unit circle of the complex plane. Eigenvalues strictly within the unit circle
should not be present because they would be associated with decaying modes of the LDS [27], which
never get excited due to the lack of driving noise. Therefore, A should be not only marginally stable
(i.e., ρ(A) = 1), but also an orthogonal matrix in order to have all the poles with unit norm. System
identiﬁcation of such a dynamic texture model is done by replacing problem (16.5) with the following
ˆA = arg
min
{A|A⊤A=In}
Z F
2 −AZ F−1
1

F .
(16.12)
Luckily, problem (16.12) is a form of Procrustes problem [24], which can still be solved in closed form.
More precisely, if the SVD of Z F
2 Z F−1
1
⊤is given by UASAV ⊤
A , the estimate of A becomes ˆA .= UAV ⊤
A .
The bottom row of Figure 16.1 shows some synthesized frames of the escalator video sequence. The
reader may notice that the quality of the synthesis makes the frames indistinguishable from the original
ones. Figure 16.2b instead, shows that all the eigenvalues of the matrix ˆA lie on the unit circle of the
complex plane. The interested reader is referred to [33] for further details.

468
CHAPTER 16 Dynamical Systems in Video Analysis
FIGURE 16.1
Periodic dynamic texture. Example of a dynamic texture that is a periodic signal. Top row: samples from
the original sequence (120 training images of 168 × 112 pixels). Bottom row: extrapolated samples (using
n = 21 components). The original dataset comes from the MIT Temporal Texture dataset [17]. (Figure
borrowed from [33].)
0
20
40
60
80
100
120
0
500
1000
1500
2000
2500
3000
3500
4000
4500
Time
||z(t)−z(1)|| F
−1
−0.5
0
0.5
1
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
Real Part 
Imaginary Part
(a)
(b)
FIGURE 16.2
Marginally stable systems. (a) Plot of the quantity ∥z(t) −z(1)∥F , representing the periodic nature of the
state Z of the escalator sequence in Figure 16.1. (b) Plot of the complex plane with the eigenvalues of
ˆA for the escalator sequence. (Figure borrowed from [33].)
Same dynamics for multiple LDSs
Another constraint explored by the existing literature is the assumption that the dynamics of multiple
video sequences of the same scene are the same. Therefore, the A parameter of the LDSs that model each

4.16.3 Identiﬁcation
469
of the video sequences must be the same. However, in the presence of noise, such identiﬁed parameters
may be different. This issue can be addressed by a simple modiﬁcation to the suboptimal identiﬁcation
algorithm.
Given H video sequences, each represented as Ii(t) ∈Rpi, t ∈{1 . . . F}, i ∈{1 . . . H} for which
the same dynamics needs to be enforced, the ﬁrst step is to form the matrix I, analogous to the case
of the suboptimal identiﬁcation, by stacking together the matrices Ii = [Ii(1) −C0
i . . . Ii(F) −C0
i ] of
each sequence. This matrix is then factorized using the SVD as
I =
⎡
⎢⎣
I1(1) −C0
1 . . . I1(F) −C0
1
...
IH(1) −C0
H . . . IH(F) −C0
H
⎤
⎥⎦= U SV ⊤.
(16.13)
To characterize the parameters obtained by this method of identiﬁcation, the noise terms can be
ignored for ease of analysis. The state evolution is now z(t) = Atz0, where z0 is the initial state of the
system. Since all the video sequences must have the same dynamics, they will all have the same state
evolution, modulo the initial condition. If the video sequence Ii(t) has a temporal lag τi ∈Z with respect
to the initial state, then the evolution of the corresponding hidden state is given by zi(t) = Aτi z(t).
Therefore, the SVD of I corresponds to the following representation for I
I =
⎡
⎢⎣
C1Aτ1z(1) · · · C1Aτ1z(F)
...
CH AτH z(1) · · · CH AτH z(F)
⎤
⎥⎦
=
⎡
⎢⎣
C1Aτ1
...
CH AτH
⎤
⎥⎦
z(1) · · · z(F) .= CZ.
(16.14)
Like in the suboptimal identiﬁcation algorithm, the above equation allows to estimate a single state
ˆZ .= SV ⊤, which now is common to all video sequences. This in turn enables the estimation of a
common dynamic parameter A by solving problem (16.5).
Given A, Ci could be recovered from C by removing the factor Aτi . However, τi is unknown, and
this path is not viable. On the other hand, one can observe that for the ith video sequence the following
holds
[Ii(1) −C0
i . . . Ii(F) −C0
i ] = Ci Aτi · [z(1) · · · z(F)],
(16.15)
= Ci Aτi (Aτi )−1[z(τi + 1) · · · z(F + τi)],
(16.16)
which shows that the estimated parameters of the ith model are the original parameters of the system,
only in a different basis, deﬁned by the transformation Pi .= Aτi . Therefore, by converting the parameters
to a common convenient canonical form, it is possible to remove the trailing Aτi . Hence, in this case
the use of canonical forms is essential to complete the system identiﬁcation step. The reader is referred
to [31] for more details on this approach.

470
CHAPTER 16 Dynamical Systems in Video Analysis
4.16.4 Comparing dynamical models
Since LDSs are generative models, when applied to video sequences their parameters simultaneously
capture information about appearance and dynamics. Although simulating such models produces com-
pelling videos, their representational power can be exploited in a multitude of other applications.
In general, the ability to compare models of the same kind is the key enabler for many more appli-
cations. Likewise, being able to compare two video sequences by means of comparing the dynamic
texture models that represent them becomes a necessary step for tasks such as categorization, regis-
tration, segmentation, etc. Such comparison entails the use of a speciﬁc distance function, or a kernel.
Choosing a distance or a kernel for LDSs is a challenging problem because even if LDSs are linear
models, the space where their parameters are deﬁned is a non-Euclidean manifold. For instance, the A
parameter is deﬁned over GL(n), the space of all invertible n × n matrices, the covariance matrix Q is
deﬁned over SPD(n), the space of positive semideﬁnite matrices of size n×n, and so on. In addition, any
distance or kernel should be invariant to the particular basis in which the LDS parameters are deﬁned,
unless it is commonly agreed that a speciﬁc canonical form is going to be used. The following sections
discuss several ways for comparing LDSs while taking into account the issues outlined above.
4.16.4.1 Distances between LDSs
Distances based on subspace angles
This family of distances is based on the geometric properties between a subset of the parameters of two
models. Such distances owe their origin to the control community and are inspired by the Martin distance
[34], which is based on the cepstrum coefﬁcients of Single Input Single Output (SISO) systems. Based on
the Martin distance, De Cock and De Moor proposed in [35] a distance between Multiple Input Multiple
Output (MIMO) ARMA4 systems based on the subspace angles {θi}2n
i=1 between the observability
subspaces associated with the state-space representation of two ARMA models. More speciﬁcally, given
two LDSs, M1 = (A1, B1,C1, Q1, R1, z1(0)) and M2 = (A2, B2, C2, Q2, R2, z2(0)), the observability
subspaces of the models are deﬁned as the range spaces of their extended observability matrices, which
in turn are deﬁned as
O∞(M1) =

C⊤
1 , (C1A1)⊤, (C1A2
1)⊤, . . .
⊤∈R∞×n,
(16.17)
O∞(M2) =

C⊤
2 , (C2 A2)⊤, (C2 A2
2)⊤, . . .
⊤∈R∞×n.
(16.18)
The subspace angles are deﬁned as the principal angles between the observability subspaces associated
with the two models. The computation of the subspace angles is performed by ﬁrst solving for P from
the Lyapunov equation A⊤PA −P = −C⊤C, where
P =
 Q11
Q12
Q21
Q22

∈R2n×2n,
A =
 A1
0
0
A2

∈R2n×2n
and C =
C1
C2

∈Rp×2n.
(16.19)
4ARMA stands for Autoregressive Moving Average.

4.16.4 Comparing Dynamical Models
471
The cosines of the subspace angles {θi}2n
i=1 are then calculated as
cos2 θi = ith eigenvalue

Q−1
11 Q12Q−1
22 Q21

.
(16.20)
Given the subspace angles between two models, they can be used to deﬁne several distances. Some
examples are reported below
Martin distance: dM(M1, M2)2 = −ln
2n

i=1
cos2 θi,
(16.21)
Finsler distance: dF(M1, M2) = θmax,
(16.22)
Frobenius distance: d f (M1, M2)2 = 2
2n

i=1
sin2 θi,
(16.23)
Gap distance: dG(M1, M2)2 = sin (θmax),
(16.24)
where θmax is the largest subspace angle.
An advantage of using distances based on subspace angles is that they are invariant to a change of
basis of the system parameters. Although such change would affect individual parameters, the range
spaces of the observability matrices remain the same. Consequently, the subspace angles between them
remain unchanged, and hence the distance is invariant to a change of basis. Additionally, subspace
angles based distances allow for comparing LDSs with different orders, but same dimension of the
output. This means that by using LDSs to model video sequences, one can only compare sequences
with the same spatial resolution. Such distances can be used for clustering [36], while for recent work
on invariant distances using a group action, the reader is referred to [37].
Distances based on the Kullback-Leibler divergence
Another metric, initially used in [11], and further studied by Chan et al. [28], is based on the Kullback-
Leibler (KL) divergence between the probability distributions of the outputs of two LDSs. The stan-
dard Kullback-Leibler divergence between two normal distributions N1(μ1, 	1) and N2(μ2, 	2) is
deﬁned as
dKL(N1∥N2) = 1
2

log
det	2
det	1

+ trace

	−1
2 	1

+ (μ2 −μ1)⊤	−1
2 (μ2 −μ1) −N

, (16.25)
where N is the dimension of the normal distributions, 	1 ∈RN×N and 	2 ∈RN×N are the covariance
matrices, and μ1 ∈RN and μ2 ∈RN are the means.
For the case of two LDSs, M1 and M2, one could apply Eq. (16.25) to the probability distributions
of the outputs of the systems, which are normally distributed. However, following this path leads to an
intractable computation because the mean and covariance of the output are inﬁnite dimensional. Instead,
in [28] it is shown that the KL divergence dKL(M1∥M2), between M1 and M2, can be approximated by an
iterative procedure that involves using a ﬁnite-dimensional approximation of the mean and covariance
of the outputs of the system. It can be shown that as the length of the sequences approaches inﬁnity,
such approximation converges to the KL divergence.

472
CHAPTER 16 Dynamical Systems in Video Analysis
It should be noted that the KL divergence is not symmetric, which is undesirable when seeking for
a similarity measure. Hence, the symmetric version of the KL divergence is used instead, which is
deﬁned as
dKL(M1, M2) = 1
2(dKL(M1∥M2) + dKL(M2∥M1)).
(16.26)
Note that even the symmetric version (16.26) is not a distance function, and this is because it does not
satisfy the triangle inequality property. This is why it is referred to as a pseudo-distance. However, it can
still be used as a similarity measure between two LDSs. One of the drawbacks of using the KL divergence
is that it is computationally expensive. Moreover, unlike the subspace angles based distances, by the
way it is computed, the KL divergence is not invariant to changes of basis. Hence, care needs to be taken
to make sure that the models under comparison are expressed with respect to the same basis [28]. For
generalizationsoftheKLdivergenceandalternativemethodsforcomputingit,wereferthereaderto[38].
4.16.4.2 Kernels between LDSs
Kernels, like distances, are an approach to measuring the similarity between two entities, which in our
case are two LDSs. While distances between any two points measure the similarity in the space in which
the points exist, kernels can be used to map these points into a higher dimensional space and measure
the similarity in this space without explicitly applying the map to the points. It can be shown that the
map that transforms the points to the higher dimensional space can be incorporated into the deﬁnition
of the kernels. This is one of the key advantages of using kernels over distances.
Kernels based on distances
Given any distance function between two models M1 and M2, a kernel can be formed based on such
function. Some distances lead to a closed form solution for computing the kernel. For example, the
Martin kernel induced by the corresponding distance is deﬁned as
KM(M1, M2) =
2n

i=1
cos2 θi.
(16.27)
However, for other distances, the kernel is induced through the radial basis function (RBF) kernel, i.e.,
K(M1, M2) = e−γ (d(M1,M2))2,
(16.28)
where γ is a free parameter, usually learnt from data.
Binet-Cauchy kernels on LDSs
An alternative approach to computing kernels between dynamical models is based on the family of
Binet-Cauchy kernels, and was proposed by Vishwanathan et al. [39]. These kernels are obtained by
computing the trace of a matrix of order q built from the output trajectories of the LDSs. Speciﬁcally, if
V1 = [I1(0), . . . , I1(∞)] ∈Rp×∞and V2 = [I2(0), . . . , I2(∞)] ∈Rp×∞represent the set of output
trajectories from two LDSs, the Binet-Cauchy kernel of dimension q is deﬁned as
Kq(M1, M2) = trace Cq(V1
V ⊤
2 ),
(16.29)

4.16.5 Applications
473
where Cq(·) denotes the compound matrix of order q, which is comprised of all the q × q minors
of V1
V ⊤
2 , and 
 is a diagonal matrix whose diagonal entries are given by λt, t ≥0, 0 ≤λ ≤1.
Such kernels are called the Binet-Cauchy kernels since they are based on a generalization of the Binet-
Cauchy theorem. Different values of q deﬁne different kernels, of which the trace kernel (q = 1) and
the determinant kernel (q = p) have been used in existing work. The case q = 1 leads to the so-called
trace kernel KT (M1, M2) = trace(V1
V ⊤
2 ) = ∞
t=0 λt I1(t)⊤I2(t), while the case q = p leads to the
determinant kernel KD(M1, M2) = det (V1
V ⊤
2 ).
The trace kernel can be directly calculated using the parameters of the LDS. In the case of two
ARMA models Mi = (zi(0), Ai, Bi, Ci), i = 1, 2, driven by the same noise realization v(t), the trace
kernel can be computed explicitly as
KT (M1, M2) = z⊤
1 (0)Pz2(0) +
λ
1 −λtrace

B⊤
1 P B2	v

,
(16.30)
where 0 ≤λ < 1 is a free parameter, 	v = E

v(t)v⊤(t)

, and P ∈Rn×n is the solution to the
Sylvester’s equation
λA⊤
1 P A2 −P = −C⊤
1 C2.
(16.31)
The advantage of using the Binet-Cauchy kernels is that, the initial state can also be accounted for
while calculating the kernel as seen in Eq. (16.30). This turns out to be an important consideration in
applications such as gait-recognition [40]. The kernel can also be calculated without taking the initial
state into account. In this case, the calculation of the Binet-Cauchy kernels reduces to
KT (M1, M2) = trace(P),
(16.32)
KD(M1, M2) = | det (P)|,
(16.33)
where P is calculated as the solution to Eq. (16.31). Furthermore, it can be shown that when λ = 1, and
A1 and A2 are stable, the determinant kernel is the same as the Martin kernel. Using this relationship,
it is possible to relate the subspace angles to the output of the LDSs. The readers are referred to [41]
for the proof and a more detailed analysis of this connection.
4.16.5 Applications
This section outlines a few applications for which dynamical systems have been used to model video
sequences. A class of objects that lend themselves to be modeled using LDSs are video sequences of
non-rigid dynamical objects such as ﬁre, rippling waves on the surface of a water body, ﬂuttering ﬂags,
etc. Loosely speaking, such videos exhibit some form of temporal stochastic repetitiveness, which in
more precise statistical terms translates into saying that they can be seen as realizations from stationary
stochastic processes. Hence, they are accurately modeled with LDSs. Such objects are referred to as
dynamic textures in the computer vision community [11]. Note that LDSs have been used as generative
models also for other data modalities. One example is motion capture data, where LDSs are used to
analyze human activities [40]. However, this compendium will focus on modeling video sequences, and
consequently dynamic textures.

474
CHAPTER 16 Dynamical Systems in Video Analysis
4.16.5.1 Synthesis and editing
Given a video sequence {I(t)}F
t=1, the goal of the synthesis task is to generate a new video sequence
{ ˆI(t)} ˆF
t=1, where ˆF > F, such that both the temporal dynamics, as well as the appearance of the
original sequence is mimicked in the new sequence. If the original sequence is modeled by a dynamic
texture model, which is an LDS, then the parameters (A, B, C, Q, R) can be identiﬁed as explained
in Section 4.16.3. Such model can now be used to easily synthesize a new video sequence of arbitrary
length. A new video frame can be generated in three steps: (a) draw an i.i.d. sample5 v′(t) from a
Gaussian distribution with covariance Q; (b) update the model state z(t) according to the ﬁrst row of
Eq. (16.1), and (c) compute the image according to the second row of Eq. (16.1).6
One of the key issues to keep in mind when synthesizing a dynamic texture is the stability of the
LDS. Sufﬁcient training data should lead to the identiﬁcation of a stable system. However, ﬁnite-length
realizations of an ARMA process can lead to an instable system. In such a case, the intensities will
grow as a function of time. Consequently, after a while, the intensity values of the frames will exceed
the admissible range. Section 4.16.3.1 describes methods to address this issue, which involves the
adjustment of the A parameter of the model.
Besides merely synthesizing video sequences, the dynamic texture model can be modiﬁed
“on-the-ﬂy,” or edited, for the purpose of synthesizing video sequences whose appearance and dynamics
are different from those of the video from which the model was originally identiﬁed. This is a common
goal in computer graphics applications. However, as it is described in [42], this is not a trivial task
because model parameters cannot be changed arbitrarily, but according to certain rules, in order to per-
form meaningful video editing tasks. Sufﬁce it to say that a wrong modiﬁcation to A might easily lead
to the instability of the LDS. Instead, Doretto and Soatto [42] show that by changing A appropriately,
it is possible to change the speed or reverse the motion direction of the perceived visual dynamics.
For instance, from a video of rapids going from left to right, it is possible to synthesize a calm ﬂow
of water moving from right to left. The recipe to do so was derived by expressing A with its eigen-
decomposition A = V 
V −1, where 
 is the diagonal matrix of eigenvalues, and V is the matrix of
eigenvectors of A. In order to reverse the dynamics of the video sequence, it can be shown that it is
sufﬁcient to replace A with ˜A = V ∗
V ∗−1, where V ∗indicates the complex conjugate of the matrix
V . Similarly, if {λi .= |λi|e−jωi }n
i=1 indicates the set of eigenvalues of A, it can be shown that changing
the speed of the perceived visual dynamics by a factor s is achievable by updating the eigenvalue phases
with ˜ωi = sωi, where sωi ≤π ∀i. In [42] it is also shown how other parameters, such as C and Q can
be altered to ensure meaningful video editing. In particular, it is described how to change the spatial
frequency content of the video sequences in order to perceive spatial structures at different scales more
or less strongly. Also, it is explained how to vary the intensity of the driving noise (deﬁned by Q) in
order to vary the visual perception of the image content.
Figure 16.3 shows synthesized frames from four dynamic texture models (left column), as well as
synthesized frames after editing such models (right column). Even though dynamic changes cannot be
observed on paper, the reader may appreciate how changing the parameters appropriately allows one to
apply meaningful modiﬁcations to the visual perception of the scene. The reader is referred to [42] for
5i.i.d stands for independent identically distributed.
6Typically there is no interest in simulating the residual measurement noise process w(t), and therefore there is no need to
draw i.i.d. samples from a Gaussian distribution with covariance R.

4.16.5 Applications
475
FIGURE 16.3
Synthesis and editing. Left column: four video frames synthesized with four different dynamic texture models.
Right column: four video frames synthesized after editing the dynamic texture model parameters, according
to the sliding bars on the right. In particular, the ﬁrst row produces a rougher sea movement with bigger
waves, the second row shows a hazier smoke diffusion, the third row makes the fountain appear “spurtier,”
and the last row seems to modify the nature of the ﬂame. (Figure borrowed from [42], © 2003 IEEE.)
more examples, and further details on the approach. Finally, Yuan et al. [43] show how to add feedback
control to improve the rendering performance of the dynamic texture model.
As mentioned before, the problem of synthesizing dynamic textures has been tackled also by the
computer graphics community, although from a different perspective. In particular, the typical approach

476
CHAPTER 16 Dynamical Systems in Video Analysis
is to synthesize new video sequences using procedural techniques, entailing clever concatenation or
repetition of training image data. The interested reader is referred to [44–47] and references therein.
4.16.5.2 Recognition
The problem of recognizing a particular dynamic texture out of many, intrinsically implies that multiple
video sequences are involved, unlike just one for the case of synthesis and editing. In particular, it is
assumed that a set of video sequences {Il(t)}Fl
t=1,l ∈{1, . . . , L}, and a set of labels {yl}L
l=1 indicating
which class each sequence belongs to, is given. Based on this prior knowledge, given a new, unseen
video sequence, the goal of recognition algorithms is to identify the class this sequence belongs to.
This problem was ﬁrst addressed in [48] for the case of dynamic textures. Most subsequent methods
follow a similar pipeline, which proceeds by ﬁrst modeling each video sequence using a LDS. Then, a
measure of similarity between LDSs is chosen and distances between all pairs of LDSs representing the
training video sequences are computed. Once such pairwise distances are available, a classiﬁer (typically
k-Nearest Neighbor or Support Vector Machine (SVM)) is used to classify a new test video sequence.
Existing work has used several distances as similarity measures between LDSs. For instance, Saisan
et al. [48] used the Martin distance, Chan and Vasconcelos [28] used the KL divergence, Woolfe and
Fitzgibbon [49] used the cepstrum distances, Vishwanathan et al. [39] used the Binet-Cauchy Kernels,
and ﬁnally Chan and Vasconcelos [50] used the Martin distance by mapping the video sequences to a
high dimensional space.
As described in Section 4.16.4 the major efforts of the research summarized above is to develop a
similarity measure that respects the non-Euclidean nature of the LDS parameter space. Failing to do
so will lead to poor recognition results. Figure 16.4 shows two pairwise similarity measures computed
on a dataset of 40 dynamic texture videos. One row shows a color coded similarity of a given video
with respect to all the others. Also, the ﬁrst and second closest sequences are marked with a ◦and a
×, respectively. Sequences listed within one diagonal box belong to the same class. Therefore, in case
of correct recognition, all the ◦s and all the ×s should be within a diagonal box. In Figure 16.4a the
similarity measure is the Frobenius norm, which assumes the LDS parameter space to be Euclidean.
As expected, many ◦s and ×s fall outside of the diagonal boxes, conﬁrming the poor recognition
performance. On the other hand, Figure 16.4b shows that using the Martin distance, which is designed
to work for LDSs, leads to a much better performance, as most of the ﬁrst and second nearest neighbor
fall on the diagonal.
Whileeachofthesedistanceshaveadifferentsetofmeritsanddrawbacks,asoutlinedinSection 4.16.4,
the approaches that exploit them do not account for other nuisance factors encountered in video recogni-
tion, such as changes in viewpoint, scale, illumination, and occlusions. Recent work [51–54] has started
to account for these changes by adopting ideas from object recognition, namely the bag-of-words
approach. The interested reader is referred to these works for further details.
In addition, besides methods based on LDSs, there exist others that use a non-LDS approach for
recognition. Typically, they extract a feature descriptor for the space-time volume of a video sequence,
which can then be used in conjunction with a classiﬁcation schema. Among them, the ones that have
received more attention are [55], which introduced the popular Local Binary Patterns (LBP) descriptor,
and [56], which is based on extracting features related to optical ﬂow and texture descriptors.

4.16.5 Applications
477
Water1
Candle1
Flowers−c1
Flowers−d1
Plants−Far1
Plants−Near1
Ocean1
Smoke1
Water Fall a1
Water Fall b1
Water2
Candle2
Flowers−c2
Flowers−d2
Plants−Far2
Plants−Near2
Ocean2
Smoke2
Water Fall a2
Water Fall b2
Water3
Candle3
Flowers−c3
Flowers−d3
Plants−Far3
Plants−Near3
Ocean3
Smoke3
Water Fall a3
Water Fall b3
Water4
Candle4
Flowers−c4
Flowers−d4
Plants−Far4
Plants−Near4
Ocean4
Smoke4
Water Fall a4
Water Fall b4
Water1
Candle1
Flowers−c1
Flowers−d1
Plants−Far1
Plants−Near1
Ocean1
Smoke1
Water Fall a1
Water Fall b1
Water2
Candle2
Flowers−c2
Flowers−d2
Plants−Far2
Plants−Near2
Ocean2
Smoke2
Water Fall a2
Water Fall b2
Water3
Candle3
Flowers−c3
Flowers−d3
Plants−Far3
Plants−Near3
Ocean3
Smoke3
Water Fall a3
Water Fall b3
Water4
Candle4
Flowers−c4
Flowers−d4
Plants−Far4
Plants−Near4
Ocean4
Smoke4
Water Fall a4
Water Fall b4
Water1
Candle1
Flowers−c1
Flowers−d1
Plants−Far1
Plants−Near1
Ocean1
Smoke1
Water Fall a1
Water Fall b1
Water2
Candle2
Flowers−c2
Flowers−d2
Plants−Far2
Plants−Near2
Ocean2
Smoke2
Water Fall a2
Water Fall b2
Water3
Candle3
Flowers−c3
Flowers−d3
Plants−Far3
Plants−Near3
Ocean3
Smoke3
Water Fall a3
Water Fall b3
Water4
Candle4
Flowers−c4
Flowers−d4
Plants−Far4
Plants−Near4
Ocean4
Smoke4
Water Fall a4
Water Fall b4
Water1
Candle1
Flowers−c1
Flowers−d1
Plants−Far1
Plants−Near1
Ocean1
Smoke1
Water Fall a1
Water Fall b1
Water2
Candle2
Flowers−c2
Flowers−d2
Plants−Far2
Plants−Near2
Ocean2
Smoke2
Water Fall a2
Water Fall b2
Water3
Candle3
Flowers−c3
Flowers−d3
Plants−Far3
Plants−Near3
Ocean3
Smoke3
Water Fall a3
Water Fall b3
Water4
Candle4
Flowers−c4
Flowers−d4
Plants−Far4
Plants−Near4
Ocean4
Smoke4
Water Fall a4
Water Fall b4
(a)
(b)
FIGURE 16.4
Similarity measures. The confusion matrices for 10 dynamic texture classes (40 sequences) of the UCLA-50
dataset [48]. The gray level of a unit area represents the similarity measure between the corresponding
pair of dynamic texture models: (a) shows the Frobenius norm of the difference between the two LDS
model parameters; (b) shows the Martin distance between the two LDS models. (Figure borrowed from [48],
© 2001 IEEE.)
4.16.5.3 Segmentation
The problem of dynamic texture segmentation can be thought of as the extension to time of the problem
of image texture segmentation. In the latter, the goal is to partition the image plane into regions with
homogeneous spatial statistics. In the former, the goal is to obtain a similar partition, only that the region
has to have homogeneous spatial and temporal statistics. In this way, it is possible to exploit the scene
motion information, jointly with the appearance, to obtain the desired partition.
Dynamic texture segmentation has been an active area of research. Several approaches have been pro-
posed over the past few years [57–61]. The typical segmentation algorithm models the video sequence
locally at a patch level [57], or at a pixel level [58–61], as the output of an LDS. The parameters of these
LDSs are then used as cues for segmentation as opposed to using traditional features (e.g., intensity
and spatial ﬁlter responses). Such cues are then clustered with additional spatial regularization con-
straints to ensure proper growth of the partition segments. Usually, computing the segmentation entails
iterating between clustering and updating the parameters of LDS models, either within an expectation
maximization (EM) framework [57–59], or a level set based approach [60,61].
The ﬁrst method for segmenting dynamic textures was proposed by Doretto et al. [60], where a level
set approach was used. The algorithm ﬁrst starts by ﬁnding a signature for each pixel of the video
sequence. Such signature is made by the subspace angles between the LDS in a spatial neighborhood

478
CHAPTER 16 Dynamical Systems in Video Analysis
FIGURE 16.5
Segmentation. Three examples of videos containing two dynamic textures. The red lines represent the seg-
mentation results: (a) shows steam over ﬂawing water; (b) shows a square and a circle of sea waves segmented
over a background of sea waves moving in a different direction; (c) shows a ﬂame with signiﬁcant motion
boundary segmented over a sea waves background. (Figure borrowed from [60], © 2003 IEEE.) (For interpre-
tation of the references to color in this ﬁgure legend, the reader is referred to the web version of this book.)
around the pixel and a reference model. Given an initial contour, the algorithm seeks to minimize the
difference between the signatures in a particular region and consequently evolves the segmentation
towards achieving the same. Figure 16.5 shows three examples of video sequences, each of which
contains two dynamic textures. They are all artiﬁcially made to highlight the segmentation results
against the groundtruth partition boundaries, with the exception of Figure 16.5c, where the boundaries
of the ﬂame are varying over time with a background of sea waves. Figure 16.5a shows steam over
ﬂowing water, Figure 16.5b shows a square and a circle of sea waves over a background of sea waves
with different motion. The red lines identify the segmentation of the image plane at convergence.
Figure 16.8 shows more examples of video sequences containing more than just two dynamic textures.
Ghoreyshi et al. [61] follows a similar approach. However, the signature at each pixel location is the
model parameters of an AutoRegressive eXogenous (ARX) model that describes the temporal evolution
of either the pixel intensities or a texture descriptor (Ising). Using this as the feature, a level set approach
is employed to obtain the segmentation.
Chan et al. [57] proposed to model a video sequence as a mixture of dynamic textures. The given
video sequence is modeled as a collection of patches, which are assumed to be drawn from a mixture of
dynamic textures.7 Using an expectation maximization (EM) algorithm, the parameters of the mixture
model, as well as the membership of each patch to this mixture are calculated. The segmentation is
then induced by the component label of the mixture under which the patch has the highest probability.
The drawback of this method is that there is no spatial regularization built into the approach. Hence,
the segmentation is regularized using a simple voting scheme within a neighborhood.
In order to address the drawback of the previous approach, Chan et al. [58] proposed a segmentation
method that explicitly accounts for the spatial regularity. Here, instead of using a mixture of dynamic
textures, the video sequence is assumed to be generated from one of K models, where K is the number
7The relation between a dynamic texture model and a mixture of dynamic texture models is analogous to the relation between
a Gaussian distribution and a mixture of Gaussian distributions.

4.16.5 Applications
479
of segments in the image. Associated with each pixel is a label that indicates which of the K groups the
pixel belongs to, based on the LDS associated with that pixel. A Markov random ﬁeld (MRF) is used
to enforce the spatial regularity of the pixel labels, and an EM algorithm is deployed to estimate all
the unknown parameters of the model. An extension to this method was proposed [59] to address the
problem of having a moving boundary segmentation. Another extension that combines MRFs with a bag
of dynamic appearance features representation for joint segmentation and categorization of dynamic
textures was proposed [54].
4.16.5.4 Registration
Given two or more video sequences of the same scene, taken from different viewpoints and possibly
with a temporal lag between them, the goal of video registration is to align each of them spatially as well
as temporally, to a common reference frame. Such reference is usually deﬁned by selecting one of the
given videos. While this problem is well studied for the case of images, in the case of video sequences
of dynamic textures it is a very challenging task. This can be attributed to the fact that dynamic textures
constantly change their shape or appearance.
In the case of image registration, there are two main approaches: direct methods and feature-based
methods. Direct methods rely on matching the entire intensity proﬁle of the image. This is done by
deﬁning a registration metric (e.g., the sum of squared image intensity differences), and then optimizing
a cost function based on such metric. In medical imaging, where there is the need to register different
modalities, a popular metric is the mutual information [62,63]. On the other hand, feature-based methods
extract a few feature points (e.g., scale invariant feature transform (SIFT) features [64], multiscale-
oriented patches (MOPSs) [65], etc.), and establish pairwise matchings by coupling a feature description
matching measure (e.g., normalized cross-correlation, or L1-norm of histogram of oriented gradient
differences), with robust statistics techniques (e.g., RANSAC [66]), to minimize outlier matches. Given
the feature matchings, registration entails optimizing a cost function that aligns the features. The reader
is referred to [67] for a more detailed review on image registration.
When aligning videos with dynamic textures one has to take into account that appearance and
shape change constantly. Hence, matching features across different frames becomes very hard, and
consequently using methods that register the sequences based on feature trajectories will not perform
very well. In order to address this issue, registration approaches for dynamic textures have proposed to
extract features from the parameters of the LDSs that model a pair of video sequences, as opposed to
directly extracting them from the sequences.
Such methods exploit the fact that appearance and dynamics of each video sequence is represented by
two distinct parameters in the LDS model, namely the C and the A parameters, respectively. This allows
to make the fundamental observation that two video sequences of the same scene might have different
appearances (because they are taken from different viewpoints), but must share the same dynamics, A.
Moreover, if two sequences are separated by a time lag τ, the temporal alignment of one sequence with
respect to the other simply entails changing the initial state. To illustrate this, let us ignore the noise
processes for simplicity. The dynamic texture model can be rewritten as
I(t) = C Atz(0).
(16.34)

480
CHAPTER 16 Dynamical Systems in Video Analysis
Now, if the initial state is modiﬁed as ˜z(0) = z(τ), then the new sequence is related to the previous as
follows
˜I(t) = C At ˜z(0),
(16.35)
= C Atz(τ) = C At Aτ z(0),
(16.36)
= C At+τ z(0) = I(t + τ).
(16.37)
Therefore, the temporal alignment does not affect either the appearance or the dynamics of a video
sequence, which means that temporal and spatial alignments can be decoupled. This removes the need
for tracking features through the frames of the video sequence. Such property can be exploited to ﬁrst,
register a pair of sequences by comparing their respective C matrices, and then perform a simple line
search for the best time lag that superimposes the sequences within the same spatial reference frame.
In particular, the ﬁrst step can be carried out by adopting a standard image registration method, and the
readers are referred to [30,31] for further details.
All of the above holds only if the dynamic parameter A is the same for both of the video sequences.
In practice, due to noise and outliers, there is no guarantee that this would be the case if we separately
identify the parameters of the sequences. In order to address this issue, and also ensure that the C param-
eters become comparable for the sake of extracting the spatial alignment, one can use the constrained
identiﬁcation framework in conjunction with a canonical form, as it is outlined in Section 2. Figure 16.6
shows the example of different dynamic scenes imaged with two different cameras from two different
vantage points. The ﬁrst row of Figure 16.6 shows the superposition of the red channel of one sequence
and the green and blue channels of the other to highlight the degree of spatial displacement that is
involved. Finally, by using the same color channel coding, the second row of Figure 16.6 shows the
Parking
Fireworks
Fountain
Flag
Palm
FIGURE 16.6
Registration. Five dynamic scenes acquired by two different cameras. First row shows the color channel
coded superposition of the ﬁrst frames for each scene, and the second row shows the color channel coded
superposition of two frames after the spatial and temporal alignment. (Figure borrowed from [31], © 2010
IEEE.) (For interpretation of the references to color in this ﬁgure legend, the reader is referred to the web
version of this book.)

4.16.6 Datasets
481
superposition of the sequences after spatial and temporal alignment. The color channel coding highlights
the nature of the spatial transformation and potential misalignment errors.
4.16.6 Datasets
In order to test the performance for a speciﬁc task such as recognition and segmentation, different
algorithmshavetestedtheirperformanceonseveralstandardizeddatasets. Thissectiondescribespopular
existing datasets and highlights their merits and demerits.
4.16.6.1 Recognition datasets
UCLA-50
This dataset, introduces by [48], was the ﬁrst designed for testing dynamic texture recognition algo-
rithms. It consists of 200 video sequences which span 50 classes of dynamic textures with 4 video
sequences per class. In addition, each video sequence contains the spatial location of a 48 × 48 image
patch denoting the region of the image that contains the best sample of the particular class of dynamic
textures. Although this dataset has been heavily used for benchmarking recognition algorithms, it also
has some limitations. Firstly, each class contains a dynamic texture from a particular viewpoint, scale,
and illumination. Secondly, using the 48 × 48 patch may not be representative of a real life scenario,
because it disregards certain nuisance factors, present in the wider ﬁeld of view, such as occlusions
and background clutter. These limitations have been addressed in the variants of the UCLA-50 dataset,
namely UCLA-8 and UCLA-9.
UCLA-8 and UCLA-9
These datasets are a reorganized versions of the UCLA-50 dataset. If one rearranges the classes of the
UCLA-50 dataset semantically, there exits only 9 classes of video sequences. Consequently, each class
in the UCLA-8 or UCLA-9 datasets contains a single semantic dynamic texture class acquired from
different viewpoints, scales, and illuminations, as opposed to a class in the UCLA-50 which contains
a dynamic texture corresponding to only one imaging condition. In addition, the tests with these two
datasets have been used with respect to the whole image plane of the video sequence, as opposed to
using only the 48 × 48 patch deﬁned in the UCLA-50. The UCLA-8 omits one of the 9 semantic
classes that are present in the UCLA-50. This is because this class, namely the plant class, contains
a large number of video sequences. Hence, in order to prevent training bias, this class is not used in
UCLA-8, while it is retained in the UCLA-9. Note that, even if the UCLA-8 is a subset of the UCLA-50
dataset, given the new interpretation, the classiﬁcation task on this dataset becomes signiﬁcantly more
challenging. This is because recognition algorithms have to account for the large intra-class variation,
due to changes in imaging conditions. The interested reader can consult [52] for more details on how
to use these variations of the UCLA-50 dataset.
Highway trafﬁc dataset
While the UCLA datasets contains different dynamic textures, the Highway Trafﬁc dataset [28] is
focussed on video sequences of one phenomenon, namely highway trafﬁc. This dataset consists of

482
CHAPTER 16 Dynamical Systems in Video Analysis
FIGURE 16.7
Recognition datasets. Sample frames from several dynamic texture recognition datasets [28,48,68,69].
sequences captured by a ﬁxed camera on a highway, at different times of the day, and under different
weather conditions. The viewpoints of all the video sequences are almost the same. The 254 video
sequences of this dataset span 3 classes namely heavy, medium, and light trafﬁc. This dataset can be
used to highlight the fact that recognition algorithms can work when there are subtle variations in the
dynamics of the video. Like the UCLA-50, for each sequence this dataset also contains annotations
denoting manually extracted regions.
Dyntex
The Dyntex dataset is the largest dynamic texture dataset. It contains video sequences of dynamic
textures taken using static as well as moving (pan/zoom) cameras. In addition, the dataset contains video
sequences of rigid objects as well. Although introduced in 2006, until recently [69], the sequences did
not have annotations. Consequently, several subsets of this dataset have been used in existing work.
Currently, there are 3 classiﬁcation subsets: Alpha, Beta and Gamma. Each of them contains a different
number of classes, as well as a different number of video sequences.
Dyntex++
The last dataset is the Dyntex++ [68]. It is derived from the Dyntex dataset, and as opposed to the others,
it is a patch based dataset. Here, 345 Dyntex sequences are used, and 36 classes with 100 patches each
are formed. The advantage of this dataset is that the classes are all equally sampled. However, some of
them only contain patches from a single sequence. Figure 16.7 shows samples from several recognition
datasets.
4.16.6.2 Segmentation datasets
Synthetic video textures dataset
This dataset, introduced by Chan and Vasconcelos [57], consists of synthetically generated video
sequences that contain multiple dynamic textures. Each synthetic video sequence contains either 2, 3, or
4 dynamic textures with the ground truth provided for the boundary of each texture. The spatial extent of
each dynamic texture in any given video sequence remains constant throughout the video sequence and

4.16.6 Datasets
483
this is known as the ﬁxed boundary segmentation case. This dataset has 300 video sequences, generated
synthetically using segments randomly chosen from 12 different dynamic textures. Several segmenta-
tion algorithms have validated their performance on this dataset and the results are publicly available
for comparisons. Given the absence of a standardized annotated dataset for real video sequences, this
dataset is an excellent source for making quantitative comparisons among segmentation algorithms.
Figure 16.8 shows samples form this dataset.
Dyntex for segmentation and recognition
This dataset, introduced [54], is suitable for testing simultaneous segmentation and recognition algo-
rithms. The video sequences contain multiple dynamic textures within the image plane. The dataset
is built off of the Dyntex and focusses on dynamic texture classes that have enough data for training
and testing purposes. This has restricted the original dataset to three classes, namely waves, ﬂags and
fountains, for a total of 119 video sequences. In addition, every sequence is manually annotated at the
pixel level. Figure 16.9 shows samples from the dataset, along with the corresponding annotations.
FIGURE 16.8
Synthetic segmentation dataset. Sample frames from the synthetic video textures dataset [57].
FIGURE 16.9
Segmentation and recognition dataset. Sample frames from the Dyntex dataset (top row) along with corre-
sponding annotations (bottom row) for simultaneous segmentation and recognition [54]. (Color convention:
Black for background, red for ﬂags, green for fountain and blue for waves.) (For interpretation of the refer-
ences to color in this ﬁgure legend, the reader is referred to the web version of this book.)

484
CHAPTER 16 Dynamical Systems in Video Analysis
4.16.7 Discussion
The task of representing dynamic scenes made of objects constantly changing their shape and appearance
has led to the introduction of the class of dynamical system models for representing video signals. This
is in contrast to modeling scenes where the objects of interest preserve their shape and appearance over
time. The purpose of this chapter was to provide the reader with a guide to quickly access the main theo-
retical challenges that such modeling framework inherits, the main results obtained so far to address such
challenges, and a list of relevant references to probe further several research directions and applications.
After a decade of research on dynamic textures there seem to be two areas that have become more
mature than others. The ﬁrst one is the system identiﬁcation. Even though estimating the parameters
of a dynamic texture model is computationally challenging, there are efﬁcient sub-optimal closed form
solutions to the identiﬁcation problem that can even account for constraints (stability, marginal stability,
and shared dynamics), and can work online. Most of the applications do not seem to suffer from the
sub-optimality of such algorithms. The second area is the ability to compare models, which requires to
face the challenges of the space of models being non-Euclidean, and of the models being represented in
canonical form. Here several distance functions have been tested, and even distances based on kernels
and probability distributions can now be computed efﬁciently. Perhaps the use of the Martin distance
to compare models is one of the results that seem to stick the most, and be used more than others in
different applications.
Conversely, there are still several problems and computer vision applications, that are either open
issues or still far from being mature. For instance, endowing the space of LDSs with a probability
structure is an open problem. This would facilitate the development of new distance functions, or the
formulation of a principled statistical dynamic texture prior, and would enable the computation of
statistics. For example, even computing a simple statistic such as the mean LDS over a given space is
still a matter of debate (the interest reader is referred to [37] for some recent progress in that direction).
The extension of the linear dynamic texture model to non-linear models has not been fully explored.
Although important extensions, such as the “kernelization” of the measurement equation of the LDS
has been proposed [50,70], non-linear variations of the dynamic equation are limited. Given that almost
all of the literature has assumed a static vantage point, future efforts should consider focussing on
generalizing current approaches to the case of a moving camera [26,29]. Finally, although a sizable
amount of research has been done on recognition, there is not a single dynamic texture class (e.g., ﬁre
or water) for which the state-of-the-art can claim a maturity that is close to, for instance, face or people
detection and recognition. Achieving this entails further developments in the area of segmentation and
recognition, where the modeling has to account for intra-class variations induced by nuisance factors
such as viewpoint, illumination, scale, background clutter, and so on.
Relevant Theory: Signal Processing Theory, Machine Learning, and Statistical Signal Processing
See Vol. 1, Chapter 3 Discrete-Time Signals and Systems
See Vol. 1, Chapter 4 Random Signals and Stochastic Processes
See Vol. 1, Chapter 11 Parametric Estimation
See Vol. 1, Chapter 16 Kernel Methods and Support Vector Machines
See Vol. 1, Chapter 25 A Tutorial on Model Selection
See Vol. 3, Chapter 2 Model Order Selection

References
485
References
[1] T. Kailath, Linear systems, Prentice Hall, Inc., 1980
[2] R. Kalman, A new approach to linear ﬁltering and prediction problems, Trans. ASME-J. Basic Eng. (1960)
35–45.
[3] T.J. Broida, R. Chellappa, Estimation of object motion parameters from noisy images, IEEE Trans. Pattern
Anal. Mach. Intell. (1986) 90–99.
[4] L.Matthies,T.Kanade,R.Szeliski,Kalmanﬁlter-basedalgorithmsforestimatingdepthfromimagesequences,
Int. J. Comput. Vision 3 (1989) 209–236.
[5] S. Soatto, R. Frezza, P. Perona, Motion estimation via dynamic vision, IEEE Trans. Automat. Contr. 41 (1996)
393–413.
[6] R. Gregor, M. Lutzeler, M. Pellkofer, K.-H. Siedersberger, E.D. Dickmanns, EMS-vision: A perceptual
system for autonomous vehicles, in: Proceedings of the International Symposium on Intelligent Vehicles,
2000, pp. 52–57.
[7] Malik, J., Taylor, C.J., Mclauchlan, P., Kosecká, J., 1998. Development of Binocular Stereopsis for Vehicle
Lateral Control, PATH MOU-257 Final Report.
[8] E.D. Dickmanns, Vision for ground vehicles: history and prospects, Int. J. Vehicle Autonomous Syst. 1
(2002) 1–44.
[9] O. Shakernia, R. Vidal, C. Sharp, Y. Ma, S. Sastry, Multiple view motion estimation and control for landing
an unmanned aerial vehicle, in: Proceedings of International Conference on Robotics and Automation, 2002.
[10] A. Yilmaz, O. Javed, M. Shah, Object tracking: A survey, ACM Comput. Surv. (2006) 38.
[11] G. Doretto, A. Chiuso, Y.N. Wu, S. Soatto, Dynamic textures, Int. J. Comput. Vision 51 (2003a) 91–109.
[12] A. Fitzgibbon, Stochastic rigidity: image registration for nowhere-static scenes, in: Proceedings of IEEE
International Conference on Computer Vision, 2001, pp. 662–669.
[13] S. Soatto, G. Doretto, Y.N. Wu, Dynamic textures, in: Proceedings of IEEE International Conference on
Computer Vision, 2001, pp. 439–446.
[14] B. Julesz, Visual pattern discrimination, IEEE Trans. Inform. Theory 8 (1962) 84–92.
[15] J. Portilla, E. Simoncelli, A parametric texture model based on joint statistics of complex wavelet coefﬁcients,
Int. J. Comput. Vision 40 (2000) 49–71.
[16] R.C. Nelson, R. Polana, Qualitative recognition of motion using temporal texture, Comput. Vis. Graph. Image
Process.: Image Und. 56 (1992) 78–89.
[17] M. Szummer, R.W. Picard, Temporal texture modeling, in: Proceedings of IEEE International Conference
on Image Processing, 1996, pp. 823–826.
[18] Z. Bar-Joseph, R. El-Yaniv, D. Lischinski, M. Werman, Texture mixing and texture movie synthesis using
statistical learning, IEEE Trans. Vis. Comput. Gr. 7 (2001) 120–135.
[19] Y.Z. Wang, S.C. Zhu, A generative method for textured motion: analysis and synthesis, in: Proceedings of
European Conference on Computer Vision, 2002, pp. 583–598.
[20] Y.Z. Wang, S.C. Zhu, Modeling complex motion by tracking and editing hidden Markov graphs,
in: Proceedings of International Conference on Computer Vision and Pattern Recognition, 2004, pp. 856–863.
[21] L. Ljung, System Identiﬁcation: Theory for the User, second ed., Prentice-Hall, Inc., 1999.
[22] P. Van Overschee, B. De Moor, N4SID: subspace algorithms for the identiﬁcation of combined deterministic-
stochastic systems, Automatica 30 (1994) 75–93.
[23] R.H. Shumway, D.S. Stoffer, An approach to time series smoothing and forecasting using the em algorithm,
J. Time Ser. Anal. 3 (1982) 253–264.
[24] G.H. Golub, C.F. Van Loan, Matrix Computations, third ed., The Johns Hopkins University Press, 1996.

486
CHAPTER 16 Dynamical Systems in Video Analysis
[25] S.J. Kim, G. Doretto, J. Rittscher, P. Tu, N. Krahnstoever, M. Pollefeys, A model change detection approach
to dynamic scene modeling, in: Proceedings of IEEE International Conference on Video and Signal Based
Surveillance, 2009, pp. 490–495.
[26] G. Doretto, S. Soatto, Dynamic shape and appearance models, IEEE Trans. Pattern Anal. Mach. Intell. 28
(2006) 2006–2019.
[27] W.J. Rugh, Linear System Theory, second ed., Prentice Hall, 1996.
[28] A. Chan, N. Vasconcelos, Probabilistic kernels for the classiﬁcation of auto-regressive visual processes, in:
IEEE Conference on Computer Vision and Pattern Recognition, 2005, pp. 846–851.
[29] R. Vidal, A. Ravichandran, Optical ﬂow estimation and segmentation of multiple moving dynamic textures,
in: IEEE Conference on Computer Vision and, Pattern Recognition, 2005, pp. 516–521.
[30] A. Ravichandran, R. Vidal, Mosaicing nonrigid dynamical scenes, in: Workshop on Dynamic Vision, 2007.
[31] A. Ravichandran, R. Vidal, Video registration using dynamic textures, IEEE Trans. Pattern Anal. Mach.
Intell. 33 (2011) 158–171.
[32] S.M. Siddiqi, B. Boots, G.J. Gordon, A constraint generation approach to learning stable linear dynamical
systems, in: Advances in Neural Information Processing Systems, 2007.
[33] G. Doretto, Dynamic Textures: Modeling, Learning, Synthesis, Animation, Segmentation, and Recognition,
Ph.D. Thesis, University of California, Los Angeles, CA, 2005.
[34] A. Martin, A metric for ARMA processes, IEEE Trans. Signal Process. 48 (2000) 1164–1170.
[35] K. De Cock, B. De Moor, Subspace angles and distances between ARMA models, Syst. Control Lett. 46
(2002) 265–270.
[36] J. Boets, K. De Cock, M. Espinoza, B. De Moor, Clustering time series, subspace identiﬁcation and cepstral
distances, Commun. Inform. Syst. 5 (2005) 69–96.
[37] B. Afsari, R. Chaudhry, A. Ravichandran, R. Vidal, Group action induced distances for averaging and
clustering linear dynamical systems with applications to the analysis of dynamic scenes, in: IEEE Conference
on Computer Vision and Pattern Recognition, 2012, pp. 2208–2215.
[38] X. Jiang, L. Ning, T. Georgiou, Distances and riemannian metrics for multivariate spectral densities, IEEE
Trans. Automat. Contr. 57 (2012) 1723–1735.
[39] S. Vishwanathan, A. Smola, R. Vidal, Binet-Cauchy kernels on dynamical systems and its application to the
analysis of dynamic scenes, Int. J. Comput. Vision 73 (2007) 95–119.
[40] A. Bissacco, A. Chiuso, S. Soatto, Classiﬁcation and recognition of dynamical models: the role of phase,
independent components, kernels and optimal transport, IEEE Trans. Pattern Anal. Mach. Intell. 29 (2007)
1958–1972.
[41] R. Chaudhry, R. Vidal, Recognition of Visual Dynamical Processes: Theory, Kernels and Experimental
Evaluation, Technical Report 09–01, Department of Computer Science, Johns Hopkins University, 2009.
[42] G. Doretto, S. Soatto, Editable dynamic textures, in: Proceedings of International Conference on Computer
Vision and Pattern Recognition, 2003, pp. 137–142.
[43] L. Yuan, F. Wen, C. Liu, H.Y. Shum, Synthesizing dynamic texture with closed-loop linear dynamic systems,
in: Proceedings of European Conference on Computer Vision, vol. 3022, 2004, pp. 603–616.
[44] K.S. Bhat, S.M. Seitz, J.K. Hodgins, P.K. Khosla, Flow-based video synthesis and editing, in: Proceedings
of SIGGRAPH, 2004, pp. 360–363.
[45] V. Kwatra, A. Schödl, I. Essa, G.F.B.A. Turk, Graphcut textures: image and video synthesis using graph cuts,
in: Proceedings of SIGGRAPH, 2003, pp. 277–286.
[46] A. Schödl, R. Szeliski, D.H. Salesin, I. Essa, Video textures, in: Proceedings of SIGGRAPH, 2000,
pp. 489–498.
[47] L.Y. Wei, M. Levoy, Fast texture synthesis using tree-structured vector quantization, in: Proceedings of
SIGGRAPH, 2000, pp. 479–488.
[48] P. Saisan, G. Doretto, Y.N. Wu, S. Soatto, Dynamic texture recognition, in: Proceedings of International
Conference on Computer Vision and Pattern Recognition, 2001, pp. 58–63.

References
487
[49] F. Woolfe, A. Fitzgibbon, Shift-invariant dynamic texture recognition, in: Proceedings of European Conference
on Computer Vision, 2006, pp. II: 549–562.
[50] A. Chan, N. Vasconcelos, Classifying video with kernel dynamic textures, in: Proceedings of International
Conference on Computer Vision and Pattern Recognition, 2007, pp. 1–6.
[51] A. Chan, E. Coviello, G.R.G. Lanckriet, Clustering dynamic textures with the hierarchical em algorithm,
in: Proceedings of International Conference on Computer Vision and Pattern Recognition, 2010,
pp. 2022–2029.
[52] A. Ravichandran, R. Chaudhry, R. Vidal, View-invariant dynamic texture recognition using a bag of dynamical
systems, in: Proceedings of International Conference on Computer Vision and Pattern Recognition, 2009,
pp. 1651–1657.
[53] A. Ravichandran, R. Chaudhry, R. Vidal, Categorizing dynamic textures using a bag of dynamical systems,
IEEE Trans. Pattern Anal. Mach. Intell. 99 (2012).
[54] A. Ravichandran, P. Favaro, R. Vidal, A uniﬁed approach to segmentation and categorization of dynamic
textures, in: R. Kimmel, R. Klette, A. Sugimoto (Eds.), ACCV, Springer Berlin Heidelberg, volume 6492 of
Lecture Notes in Computer Science, 2010, pp. 425–438.
[55] G. Zhao, M. Pietikainen, Dynamic texture recognition using local binary patterns with an application to facial
expressions, IEEE Trans. Pattern Anal. Mach. Intell. 29 (2007) 915–928.
[56] R. Péteri, D. Chetverikov, Dynamic texture recognition using normal ﬂow and texture regularity,
in: Proceedings of the Iberian conference on Pattern Recognition and Image Analysis, 2005, pp. 223–230.
[57] A. Chan, N. Vasconcelos, Modeling, clustering, and segmenting video with mixtures of dynamic textures,
IEEE Trans. Pattern Anal. Mach. Intell. 30 (2008) 909–926.
[58] A.B. Chan, N. Vasconcelos, Layered dynamic textures, IEEE Trans. Pattern Anal. Mach. Intell. 31 (2009)
1862–1879.
[59] A.B. Chan, N. Vasconcelos, Variational layered dynamic textures, in: Proceedings of International Conference
on Computer Vision and Pattern Recognition, 2009b.
[60] G. Doretto, D. Cremers, P. Favaro, S. Soatto, Dynamic texture segmentation, in: Proceedings of IEEE
International Conference on Computer Vision, 2003b, pp. 1236–1242.
[61] A. Ghoreyshi, R. Vidal, Segmenting dynamic textures with ising descriptors, arx models and level sets,
in: Proceedings of the International Workshop on Dynamical Vision, 2006.
[62] F. Maes, A. Collignon, D. Vandermeulen, G. Marchal, P. Suetens, Multimodality image registration by
maximization of mutual information, IEEE Trans. Med. Imaging 16 (1997) 187–198.
[63] P. Viola, W.M. Wells III, Alignment by maximization of mutual information, Int. J. Comput. Vision 24 (1997)
137–154.
[64] D. Lowe, Distinctive image features from scale-invariant key points, Int. J. Comput. Vision 60 (2004)
91–110.
[65] M. Brown, R. Szeliski, S. Winder, Multi-image matching using multi-scale oriented patches, in: Proceedings
of International Conference on Computer Vision and, Pattern Recognition, vol. 1, 2005 pp. 510–517.
[66] M.A. Fischler, R.C. Bolles, Random sample consensus: a paradigm for model ﬁtting with applications to
image analysis and automated cartography, Commun. ACM 24 (1981) 381–395.
[67] R. Szeliski, Image alignment and stitching: a tutorial, Found. Trends Comput. Graphics Vision 2 (2006) 1–104.
[68] B. Ghanem, N. Ahuja, Maximum margin distance learning for dynamic texture recognition, in: Proceedings
of European Conference on Computer Vision, 2010.
[69] R. Péteri, S. Fazekas, M.J. Huiskes, Dyntex: A comprehensive database of dynamic textures, Pattern Recogn.
Lett. 31 (2012) 1627–1632.
[70] R. Chaudhry, A. Ravichandran, G. Hager, R. Vidal, Histograms of oriented optical ﬂow and binet-cauchy
kernels on nonlinear dynamical systems for the recognition of human actions, in: IEEE Conference on
Computer Vision and Pattern Recognition, 2009, pp. 1932–1939.

17
CHAPTER
Image-Based Rendering
Yao-Jen Chang* and Tsuhan Chen†
*Siemens Corporation, Corporate Technology, Princeton, NJ, USA
†224 Phillips Hall, Cornell University, Ithaca, NY, USA
4.17.1 Introduction
Bullet time, where an actor moves in an extremely slow motion while the camera fast changes its
viewpoint along a very smooth trajectory, is one of the most impressive special visual effects used in
the movie, The Matrix. In this effect, instead of using a physical camera moving at an extremely fast
speed to shoot the video, a large amount of cameras are placed on a pre-arranged path and triggered at
extremely close intervals to shoot an image frame from different viewpoints [1]. Once these frames are
captured, more images are synthesized by interpolation to create a smooth camera motion trajectory.
The approach of creating this special visual effect is based on image-based rendering (IBR).
In conventional graphics pipelines, detailed 3D models with textures are utilized for rendering an
object or a scene at arbitrary viewpoints. With recent advancements in computer graphics hardware
like GPU, a vast amount of processors are able to operate in parallel for rendering highly sophisticated
models of 3D scenes and objects. However, it is still difﬁcult to render an arbitrary natural scene with
photorealism due to the inability to measure every detail of the scene in order to create a good 3D model.
Therefore, IBR aims to compensate for the insufﬁciency of model-based approaches by using a large
amount of real photos, so that the scene can be synthesized at novel viewpoints in high ﬁdelity.
Even though IBR is considered as a task in computer graphics for novel view synthesis, it can be
treated as a signal processing problem: How do we perform sampling, representation, and reconstruction
for a high-dimensional function? These three basic operations correspond to light ﬁeld sampling, scene
representation, and rendering in IBR. The basic idea is to sample the space consisting of all light rays
at different locations with various directions and then reconstruct the light rays from the sampled data
for novel view synthesis. This space, named the light ﬁeld, was ﬁrst deﬁned by Gershun [2] to describe
the radiometric properties of light in a space. However, the concept of light ﬁeld was utilized in the
early 1900s purely by optics. The technique of light ﬁeld sampling and reconstruction by optics was
ﬁrst outlined by Lippmann in his work of integral photography [3]. Instead of using multiple cameras
for light ﬁeld sampling, integral photography utilizes lenslet array to capture the light ﬁeld. Later,
this technique was extended to incorporate digital recording and display to form the study of integral
imaging. Commercialized products in the consumer’s market are even available to shoot a photo ﬁrst
and adjust its focus afterwards. The history of integral imaging has been rarely mentioned in most IBR
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00017-0
© 2014 Elsevier Ltd. All rights reserved.
489

490
CHAPTER 17 Image-Based Rendering
studies. Due to its similarities with the 4D light ﬁeld in IBR, it is helpful to ﬁrst review the ﬁeld of
integral imaging, although its historical merits are also of interest.
Starting from Section 4.17.3, we introduce the sampling, representation, and rendering techniques
employed for IBR in the literature. In sampling, the plenoptic function is utilized to parameterize the
light ﬁeld. Depending on the assumptions imposed on the light rays or constraints on the viewpoints,
the dimensionality of the plenoptic function can be reduced for efﬁcient sampling and storage. In
representation, either no geometry, implicit geometry, or explicit geometry can be used to represent the
scene. Although a detailed 3D model is not supposed to be used for IBR, a rough model plus multiple
images are commonly utilized in the literature to synthesize a scene with high ﬁdelity. In rendering,
different approaches have been developed based on the sufﬁciency of the sampled data and geometry
representation for reconstructing the light rays from a novel viewpoint. Various methods, from simple
interpolation to computation-intensive depth estimation and occlusion reasoning, are introduced for
realistic image-based rendering.
IBR can be applied to various applications such as video stabilization, video surveillance, and creating
the bullet time effect in movie production. In Section 4.17.6, we present the virtual view synthesis on
a video on demand system as a case study to demonstrate the usage of IBR. In Section 4.17.7, we
point out some open issues and problems in the IBR research. Implementations and Dataset publicly
available are listed in Sections 4.17.8 and 4.17.9. And ﬁnally, Section 4.17.10 concludes this chapter
and addresses possible future trends in the IBR research.
4.17.2 Integral imaging
The history of integral imaging can be traced back to the Nobel Laureate, Gabriel Lippmann, in 1908.
He was awarded the Nobel Prize for his invention on reproducing colors photographically based on the
phenomenon of interference [4], which formed the foundation of holography. He proposed the idea of
integral photography (La Photographie Integral), which could be generated using a ﬁlm formed with a
thin transparent strip comprising a large amount of small spherical protrusions. These spherical segments
were utilized to record and playback the complete spatial image with parallax in all directions [3].
In 1911, Sokolov [5] performed the ﬁrst experiment to verify Lippmann’s idea using a pin-hole
aperture sheet [6], which served as a pin-hole array. As illustrated in Figure 17.1, each pin-hole captures
a portion of the light rays from the 3D object from different viewpoints and records the associated color
intensity on the pickup plane, which can then be used to form a 3D image. However, the small aperture
of the pin-hole array resulted in a relatively dark image [6]. Lenticular sheets were later utilized during
late 1920s as a simpliﬁcation of Lippmann’s design. Lenticular sheets have a large amount of convex
cylindrical segments on the front side as the lens and a ﬂat surface on the rear side as the focal plane.
Although cylindrical lenticular sheets are easier to manufacture, either the vertical or horizontal parallax
is sacriﬁced compared to the spherical lens.
One big issue with Lippmann’s scheme is that the images generated were pseudoscopic, which means
they were depth reversed. To resolve this issue, Ives proposed a two-step method, in which a secondary
exposure was made by another lens sheet to re-invert the depth [7]. The operation of two-step integral
photography was equivalent to conventional integral photography followed by reordering of the “pixels”

4.17.2 Integral Imaging
491
Pin-hole Array
Object
FIGURE 17.1
The pin-hole array.
acquired on the ﬁlm, which can be done efﬁciently in a digital form. That is, we can replace the ﬁlm with
CCD/CMOS sensors, store the color intensities as a digital image, and display the manipulated (pixel
reordered) image with a high-resolution LCD display through the lens array to form an orthoscopic
image.
Integral imaging can be conceptualized as a 4D light ﬁeld sampling and rendering in terms of
image-based rendering terminology, but it is operated purely with optics without any computation.
Thanks to the advancements of high-resolution imaging sensor and display technology, hybrid models
that combine optics and digital representation have been proposed. For example, integral images can
be generated using computer graphics by ray tracing. These computer-generated integral images can
then be displayed optically through a lens array from an LCD. In this way, observers from different
viewpoints can perceive different aspects of the recorded object. On the contrary, integral images can
be acquired optically, stored in digital format, and then rendered with a LCD display without the optical
lens. For example, the plenoptic camera was ﬁrstly proposed by Adelson and Wang [8] by putting a
lenticular sheet between the camera’s main lens and photosensor. A more compact and portable design
was realized by Ng et al. [9], and recently commercialized [10] to capture the 4D light ﬁeld. The image
can be shot very fast without performing focus adjustment on the desired target; then, the image can be
post-processed to regenerate images focusing on arbitrary depths or all in-focus. It can also synthesize

492
CHAPTER 17 Image-Based Rendering
images at different viewpoints to demonstrate the parallax effects. However, due to the limitation of the
display, usually only a limited number of viewpoints can be generated at the same time.
Instead of using the pin-hole array or microlens array for multi-view image acquisition, the mirror
array and refractive sphere array can also be utilized to acquire images from multiple viewpoints with a
single camera. Figure 17.2 shows a mirror array by attaching 25 mirrors to clay. Each mirror is tilted at
different angles so that an object placed in front of the mirror array can be reﬂected to the camera. As
shown in Figure 17.3, the setup of a mirror array is equivalent to multiple cameras placed behind each
mirror looking at the object, although image resolution is sacriﬁced. As the image resolution of digital
cameras keeps increasing, it is advantageous to trade image resolution with the ability to take images
FIGURE 17.2
The setup of a mirror array.
FIGURE 17.3
Image formation of the mirror array.

4.17.2 Integral Imaging
493
FIGURE 17.4
An image of a dice (left) and the multi-view image taken by inserting a convex lens array in front of the
camera (right).
from multiple viewpoints. The mirror array provides larger view coverage but less mobility than the
light ﬁeld camera. Therefore, it can be treated as a cheaper alternative for a camera array with a large
amount of real cameras.
To use mirror array for image-based rendering, the mirror doesn’t have to be ﬂat. Omni-directional
cameras comprising a hyperbolic mirror and a single camera have long been used in video surveillance
applications to obtain a wider ﬁeld of view. Based on the refraction principle, convex lens or concave
lens array may also be used to capture multi-view images with a single camera. Figure 17.4 shows the
multi-view image of a dice captured through a convex lens array.
Taguchi et al. [11,12] proposed a mirror array comprising multiple rotationally symmetric conic or
quadric mirrors with a perspective camera. They presented an analytical forward projection formulation
to characterize the light path from a 3D point to its 2D projection on the camera by solving a 4th order
equation for the spherical mirror array and a 10th order equation for the refractive sphere array. With an
additional non-linear optimization (e.g., bundle adjustment), both the camera poses and 3D point clouds
can be adjusted by minimizing the re-projection error. Similar to other IBR work, plane sweeping is
utilized to estimate the scene depth. Figure 17.5 shows the spherical mirror array and the refractive
sphere array designed by Taguchi et al. [11].
Further research and development along the line of integral imaging is still very active in various
perspectives. For example, Veeraraghavan et al. [13] used an attenuating mask to modulate the acquired
light ﬁeld, which provided 4D light ﬁeld and full-resolution focused image at the same time. Cho and
Javidi proposed to perform 3D tracking of occluded objects using integral imaging [14]. Its idea shared
similar thoughts with the synthetic aperture tracking [15], and occlusion surface reconstruction [16] in
the IBR literature. Wang, Gill and Molnar [17] proposed to design a light ﬁeld sensor directly with “angle
sensitive pixels,” which respond to both the intensity and angular distribution of perceived light rays.
Their sensor can be manufactured in a very small scale, which may be useful as part of an implantable
probe for biomedical applications.
Insummary, integral imaginghas alonghistoryinacquiringanddisplayingthelight ﬁeldbyoptics. Its
study attracts increasing attentions from researchers in optical engineering, computer vision, computer

494
CHAPTER 17 Image-Based Rendering
FIGURE 17.5
The spherical mirror array and refractive sphere array. (Figure courtesy of Yuichi Taguchi.)
graphics, and computational photography. As we show later, integral imagining basically resembles
a camera array capturing 4D light ﬁeld in a planar setting. In the next section, we introduce various
approaches in light ﬁeld sampling including the most commonly used 4D light ﬁeld representation.
4.17.3 Sampling
4.17.3.1 The light ﬁeld and the plenoptic function
Even though the idea of light ﬁeld has existed since the early 1900s, the term “Light Field” was deﬁned
by Gershun [2] to describe the radiometric properties of light in a space. To fully capture the properties
of the light ﬁeld, Adelson and Bergin proposed the plenoptic function [18]. In its full form, the light ﬁeld
can be described by a seven-dimensional plenoptic function of light positions (Vx, Vy, Vz), orientation
angles (θ, φ), wavelength (λ), and time (t):
l(7)(Vx, Vy, Vz, θ, φ, λ, t).
(17.1)
With the plenoptic function to capture light ﬁeld, image-based rendering can be deﬁned as a process
of two stages: sampling and rendering. In the sampling stage, samples are taken from the plenoptic
function for representation and storage. In the rendering stage, the continuous plenoptic function is
reconstructed from the captured samples.
Depending on different assumptions on its parameters, the plenoptic function could be reduced
to a lower-dimensional function. However, some restrictions would be imposed when the plenoptic
function is reduced to a lower-dimensional function to facilitate efﬁcient computation, representation,
and storage. One intuitive scenario is to restrain the viewing space such that we don’t have to use three
dimensions to represent the position. In the next subsections, we will introduce the 2D panorama mosaic,
3D concentric mosaics, and the most commonly used 4D light ﬁeld representations.

4.17.3 Sampling
495
FIGURE 17.6
A 360◦cylindrical panorama of the Confucius Temple, Shandong, China [24].
4.17.3.2 The 2D and 3D light ﬁelds
As the most extreme case, the light ﬁeld is represented by a panorama mosaic as a 2D function. The
panorama mosaic is like a regular image, but composed from multiple images at different viewing
angles. It can be either camera-centric or object-centric. In the former case, the images can be captured
by rotating a single camera at a ﬁxed position, or by multiple cameras with different viewing directions
but conceptually sharing the same optical center like the Point Grey LadyBug sensor [19]. In the latter
case, the camera viewing direction of each image intersects a ﬁxed point (e.g., the object center). So
the object can be rendered at arbitrary viewpoint but at a ﬁxed distance. In either case, the plenoptic
function can be represented by the angle of light rays:
l(2)(θ, φ).
(17.2)
Depending on whether camera is moving in one angle to capture 360◦by rotating the vertical axis,
or moving in two angles to increase ﬁeld-of-view in both horizontal and vertical directions, cylindrical
mosaic or spherical mosaic is utilized as the scene representation, respectively (Figure 17.6 shows an
example of the cylindrical panorama mosaic.).
Even though it’s highly restricted to just allow the light ﬁeld reconstruction at a single point, the
QuickTime VR™technology [20] successfully ﬁnds its promising applications in on-line advertisement
for browsing hotel rooms, estate properties, and resort facilities. Many point-and-shoot cameras also
have built-in functions for creating panorama mosaic by guiding users to take 360◦views and then
stitching images together.
To relax the restrictions a little bit, we can model light ﬁeld with a 3D plenoptic function. The way
to achieve this is to restrict the camera motion to be on a speciﬁc path, as has been done with branch
movies [21,22] and concentric mosaics [23]. In branch movies, video segments along multiple paths
are recorded. The user is allowed to follow a path and then switch to a different path at some predeﬁned
branching points. This technique is useful for applications like virtual touring, where the viewer can
walk around inside a large scene by following and switching different paths. In concentric mosaics,
the camera motion follows a circle on a plane, but interestingly it allows novel view synthesis inside a
disc. This is achieved by ignoring the vertical parallax for novel view rendering. To see how it works,
let’s take the case as shown in Figure 17.7 where a camera is mounted on a beam to shoot images as
the beam rotates. The 3D light ﬁeld can be represented by the beam’s rotation angle α and the pixel
locations (u, v) inside each captured image:
l(3)(α, u, v).
(17.3)

496
CHAPTER 17 Image-Based Rendering
R
Camera path
Camera
Beam
Rotate
v
u
Rendering circle
FOV
α
FIGURE 17.7
Concentric mosaic capturing [24].
FIGURE 17.8
Parallax observed from concentric mosaics rendered scenes [24].
The reason why it’s called the concentric mosaic is because the presentation can be viewed as having
multiple slit cameras rotating around a common center at various radiuses and taking vertical line images
at the tangent direction. Therefore, to synthesize a novel view inside the viewing disc, one only has to
pick the corresponding slit from the cylindrical mosaic obtained from each slit camera, and compose
them together to form a panorama at the desired point. As shown in Figure 17.7, the size the viewing
disc is determined by the ﬁeld of view of the real camera FOV and the radius of the camera path R. The
maximal radius of the viewing disc will be R sin (FOV/2).
Figure 17.8 shows a rendered scene using concentric mosaic, with which the horizontal parallax
can be observed. Since the synthesized image is composed by the vertical slits, the vertical parallax
is not captured by concentric mosaic. Therefore vertical distortion will be observed when the viewer

4.17.3 Sampling
497
changes its position by forward or backward movements. In [23], depth correction is proposed to
utilize the knowledge of scene geometry to reduce the distortion. However, recording or estimating
the scene depth along each vertical slit would require one more dimension in the representation, and
make it 4D in practice if detailed scene geometry is utilized. Nevertheless, the concentric mosaic is
very easy to capture, efﬁcient in processing, and realistic in rendering by providing strong sense of
horizontal parallax. In the next subsection, we will introduce the 4D plenoptic function which is the
most commonly used representation for the light ﬁeld of a static scene.
4.17.3.3 The 4D light ﬁeld
The 4D light ﬁeld is a commonly used representation in IBR where no restrictions are imposed on
the viewing space. Several reasonable assumptions are made on the light ray properties and the scene,
which induce almost indistinguishable impacts on the rendering quality of IBR.
1. Constant wavelength: In most cases, the light we perceived is incoherent, where the revealed color is
actually a distribution over different wavelengths. In practice, we can simplify the representation of
wavelength as a composition of three different color channels: Red, Green, and Blue. This has long
been used in digital image processing as well as display manufacturing. If we treat each channel sep-
arately, we can drop the wavelength parameter to reduce one dimension from the plenoptic function.
2. Constant radiance: When a light ray propagates through the air, it would suffer from the atmo-
spheric turbulence and its radiances would attenuate along the way. The attenuation is negligible
in short distance range, which is true in most applications of IBR. By assuming the radiance is
constant along a light ray path, we can reduce one dimension from the plenoptic function by reduc-
ing the ﬁve-dimensional light ray space (Vx, Vy, Vz, θ, φ) to a four-dimensional space. One way
to interpret this, as addressed by Zhang and Chen [24], is to surround the scene with an arbitrary
surface, where each point on the surface captures the light rays from all directions. Therefore, a 4D
plenoptic function is formed by parameterization of the surface in two dimensions and light ray
directions in the other two dimensions. An alternative interpretation is to use the two-plane param-
eterization used in various IBR work including the Stanford Light Field by Levoy and Hanrahan
[25] and Lumigraph by Cohen et al. [26]. Note that this assumption does suffer from resolution
sensitivity caused by ﬁnite resolution of real cameras. A pixel in an image is contributed from a
set of light rays from a certain area on the scene surface, where the support of area is dependent on
the distance between the camera and the scene surface. Therefore, care must be taken to handle the
resolution variations when using light rays from multiple cameras for interpreting the same scene
surface area observed at quite different distances.
3. Static scene: If we assume the scene is static, we can drop the time parameter from the plenop-
tic function to reduce one more dimension. One beneﬁt from making this assumption is that we
can acquire the light ﬁeld with a camera moving around the scene to take as many images as we
need, which is commonly used in image-based modeling with structure-from-motion framework.
Therefore, it can be considered as a way to increase the spatial resolutions by trading the temporal
resolution. The assumption is also valid for multiple synchronized cameras shooting a dynamic
scene if they are all triggered at the same time to shoot images. This is how bullet time is created in
movie production as mentioned at the beginning. Even though it is possible to utilize temporal cue
from a dynamic scene, this would incur an increase in both processing time and storage to keep

498
CHAPTER 17 Image-Based Rendering
track of all pixel movements. (In practice, images taken at different spatial and temporal dimensions
can be mixed for IBR. One interesting experiment done by Wilburn et al. [27] is to trigger cameras
at different time periods to capture and render a dynamic scene as if shot by a high-speed video
camera. Another recent work in SIGGRAPH is to investigate the temporal dimension such that the
motion blur can be captured and modeled [28].)
With the above assumptions, we can reduce the plenoptic function to a 4D function by dropping the
wavelength and time parameters, and one more dimension from positional/directional representation.
To parameterize this 4D plenoptic function, both the light ﬁeld [25] and Lumigraph [26] adopted the
two-plane parameterization:
l(4)(s, t, u, v),
(17.4)
where (s, t) and (u, v) are coordinates of the two planes. Following the notation of Lumigraph [26], the
(s, t) plane is the camera plane, and the (u, v) plane is the focal plane as shown in Figure 17.9, where
two planes are parallel for ease of illustration. This is exactly the same as the integral imaging, where
one can use (s, t) to represent the location of each lens on the plane of lenslet array, and use (u, v) to
represent the point coordinate on the recording ﬁlm. By replacing each lens by a camera, the ﬁlm with
the CMOS/CCD sensors, we can build a 2D camera array within which each camera captures a 2D
image for acquiring the 4D light ﬁeld. It’s exactly the way how we capture/represent the 4D light ﬁeld
in Lumigraph [26] and the light ﬁeld [25] (see Figure 17.10).
Except for light rays parallel to these two planes, any unique light ray k in the 3D space would pass
through these two planes at a unique pair of end points: (sk, tk, uk, vk). Therefore, we can imagine a
bounding box placed outside the convex hull of the object of interest, where each face of the box is
parameterized with two planes to capture all the possible light rays emitted from the object inside the
box. For all the light rays passing through a common point on the camera plane, it forms an image as if
there is a camera with its optical center located at this point. Therefore, the novel view synthesis can be
easily achieved without knowing the scene geometry by picking the light rays through the optical center
of the virtual camera. However, due to the physical constraints that real cameras occupy a non-empty
space in 3D with limited image resolutions, both (s, t) and (u, v) are discrete instead of continuous.
v
u
s
t
(u0,v0)
(s0,t0)
Light ray
z
Object
Focal plane
Camera plane
Discretized point
Novel view
position
FIGURE 17.9
The parameterization of the light ﬁeld used in Lumigraph [26].

4.17.3 Sampling
499
FIGURE 17.10
A sample light ﬁeld image array: fruit plate [24].
A camera array arranged on a plane actually subsamples the 4D light ﬁeld at discrete positions on
the camera plane and focal plane. Therefore, to synthesize a novel view, quadrilateral interpolation is
utilized to approximate each light ray of the virtual camera from nearby sampled light rays [26]. To
visualize this, let’s consider the light rays are sampled at the discrete points as shown in Figure 17.9.
The 16 light rays contributed by 4 solid points on the camera plane and 4 points on the focal plane are
linearly interpolated to approximate the light ray (s0, t0, u0, v0). The interpolation process can be done
very efﬁciently, thereby achieving real-time synthesis regardless of scene complexity [29,30].
4.17.3.4 Spectral analysis on light ﬁeld sampling
With different representations of plenoptic functions, one natural question to light ﬁeld sampling is how
many samples we need in order to reconstruct the light ﬁeld perfectly. From the signal processing point
of view, this question can be answered by conducting the spectral analysis on the sampled data [31–33].
Namely, performing the Fourier transform on the original data and analyzing its frequency spectrum
to see if aliasing occurs. As the plenoptic function is a high-dimensional function, the required amount
of samples would be highly dependent on the scene complexity, surface reﬂection, lighting conditions,
etc. By looking at its spectrum bandwidth, we will be able to know whether the current setup is under-
sampling or over-sampling for the scene under observation.
The earliest IBR sampling analysis was conducted by Lin and Shum [34] on both the light ﬁeld and
concentric mosaics. Their analysis is based on the scale-space theory, where the world is modeled at
a single point sitting from a certain distance to the cameras. Assuming rendering is based on constant
depth and bilinear interpolation, the bounds can be derived from the aspect of geometry and based on
the goal that no “spurious detail” should be generated during the rendering (referred as the causality
requirement). Although the viewpoint of their analysis is rather interesting, this method is constrained
by the simple world model they chose. The texture and the reﬂection model of the scene surface and
occlusions are hard to analyze with such a method [24].

500
CHAPTER 17 Image-Based Rendering
f
t
v
0
'v
( )
v
t
z ,
( )
v
t
z
ft
,
0
0
v
focal line
camera line
Object
(a)
(b)
t
Ω
v
Ω
π
π
−
0
max
=
Ω
+
Ω
t
v
z
f
0
min
=
Ω
+
Ω
t
v
z
f
B
Δ
FIGURE 17.11
(a) The light ray correspondence in a 2D light ﬁeld. (b) The spectrum of light ﬁeld obtained in [35].
Therefore, Chai et al. [35] proposed to perform the light ﬁeld sampling analysis in the classic signal
processing framework by performing Fourier transform to the light ﬁeld signal, and then sampling it
based on its spectrum. Assuming surface is Lambertian and there are no occlusions, they found that
the light rays represented by the plenoptic function have certain correspondence among themselves.
For ease of illustration, we use a simpliﬁed 2D light ﬁeld in Figure 17.11a as an example. Under this
simpliﬁcation, the camera plane and focal plane in Figure 17.9 degenerate to two 2D lines. A local
discretization of the focal line was adopted in their analysis.
As illustrated in Figure 17.11a, given z(t, v) as the scene depth of the light ray (t, v), we can map
the light ray to the origin as follows:
l(2)(t, v) = l(2)

0, v −
f t
z(t, v)

.
(17.5)
When the scene is at a constant depth z(t, v) = z0, its Fourier transform can be written as
L(2)(t, v) = L′(v)δ
 f
z0
v + t

,
(17.6)
where L′(v)is the Fourier transform of l(2)(0, v), and δ(·) is the 1D Dirac delta function. Therefore, the
spectrum has non-zero values only along a line. When the scene depth is varying between a certain range,
the spectral support of a light ﬁeld signal would be bounded by the minimum and maximum depths
of objects in the scene as shown in Figure 17.11b, regardless the scene complexity. Such an analysis
provides a fairly good approximation of the spectrum analysis of IBR. However, the dependency on
mapping light rays captured at arbitrary position to that at the origin of the camera plane hinders it from
being applied to more sophisticated scenes such as non-Lambertian surface, scenes with occlusions and
other IBR methods such as concentric mosaics.
To resolve these issues in spectral analysis, Zhang and Chen [36] proposed to utilize the relationship
between the surface plenoptic function (SPF) and IBR representation for spectral analysis. Based on
the constant radiance assumption, the plenoptic function is equivalent to the SPF. Again we use a 2D
case as shown in Figure 17.12 for illustration. The SPF represents the light rays emitted/reﬂected from

4.17.4 Scene Representation
501
(
)
0
,
1
=
y
x
S
x
y
θ
Or
( )
( )
⎩
⎨
⎧
=
=
s
y
y
s
x
x
1
1
(
)
0
,
2
=
y
x
S
Or
( )
( )
⎩
⎨
⎧
=
=
s
y
y
s
x
x
2
2
Object 1
Object 2
Camera path
α
β
(
)
0
,
=
y
x
Sc
Or
( )
( )
⎩
⎨
⎧
=
=
t
y
y
t
x
x
c
c
FIGURE 17.12
2D surface plenoptic function and general IBR capturing [24].
the scene surface as l(2)
i
(s, θ), where s is the arc length on the scene surface curve, θ is the light ray
direction, and i is the index of objects. The IBR representation can be written as l(2)
c (t, α) where t is
the arc length on the camera path, α is the light ray direction. There exists a mapping from l(2)
i
(s, θ)
to l(2)
c (t, α) based on the light ray correspondence. By assuming certain spectrum property of the SPF,
they showed that it is possible to obtain the spectrum of the IBR representation, even when the scene
is non-Lambertian or occluded. Moreover, the same methodology is applicable for concentric mosaics.
Interested readers can refer the detail analysis presented by Zhang and Chen [36].
4.17.4 Scene representation
In this section, we brieﬂy introduce various scene representations used in image-based rendering. The
decision of scene representation is closely dependent on how the light ﬁeld has been sampled and
how the sampled light ﬁeld will be reconstructed for novel view synthesis. Ideally the light ﬁeld is
reconstructed from the sampled data without any speciﬁc scene representation. However, appropriate
scene representations such as the scene geometry, the texture map, and the surface reﬂection model can
greatly reduce the required sampling of light ﬁeld. Depending on the scene complexity and the amount
of available prior knowledge about the scene, detailed scene geometry, texture map, and reﬂection model
may not be easily acquired in advance or estimated on the ﬂy. Therefore, there is a trade-off between the
choice of sampling, representation, and rendering. With the scene geometry, we will be able to ﬁnd the
point correspondences across different view point. Based on the assumption of constant radiance along
a light ray, if we further assume that most surfaces in the scene are close to Lambertian, or at least locally
color consistent (light rays from the same surface point share the same color if their reﬂection directions
are similar), we can save a lot on acquiring light ray samples for light ﬁeld reconstruction. As pointed
out by Kang and Shum [37,38], there is a wide geometry-image continuum in the scene representations
for image-based rendering, where different approaches vary from its usage of the knowledge of scene
geometry and the amount of light ﬁeld samples acquired to render the scene. Basically, the more we
know about the scene, the smaller amount of images we need to acquire for good rendering [39].

502
CHAPTER 17 Image-Based Rendering
The scene geometry can be represented in various forms. In [38], the scene geometry is divided into
three categories: no geometry, implicit geometry, and explicit geometry.
4.17.4.1 No geometry
At one extreme of the geometry-image continuum, the light ﬁeld can be well reconstructed without
using geometry representation when it has been sampled densely enough. By no geometry we mean
that no depth information of the surfaces in the scene is utilized for rendering. It is achievable espe-
cially for constrained settings where lower-dimensional plenoptic functions can be easily captured and
economically stored. For example, the panorama mosaic captures the light ﬁeld at a ﬁxed point as a 2D
plenoptic function. This 2D function can be easily stored as a 2D texture map like a normal image, and
rendered by using a cylindrical or a spherical modal. Similarly, every horizontal light ray inside a disc
can be captured by concentric mosaic. Hence it is possible to perform reconstruction for any viewpoint
at any point on the bounded disc. However, as addressed in Section 4.17.3.2, the vertical parallax is
sacriﬁced in concentric mosaics. To resolve this issue, depth correction is proposed in [23] to utilize
the knowledge of scene geometry to reduce the distortion. For 4D Lumigraph and light ﬁeld, a large
amount of cameras on a 2D plane form an array for capturing light ﬁeld within a bounding box. Even
though physical constraints of cameras and image resolution limit its ability to capture all the light rays
inside the bounding box, good approximations in reconstruction can be achieved by using quadrilateral
interpolation when the arrangement of the camera array is relatively dense with respect to the distance
to the scene and scene complexity.
4.17.4.2 Implicit geometry
When the light ﬁeld is only sparsely sampled, scene geometry would be a great help to compensate the
insufﬁciency of sampled light ﬁeld. With scene geometry, the corresponding light rays from the existing
cameras can be easily established for forming the light rays of a virtual camera. Note that what matters
is the light ray correspondence. Hence, it is possible to ﬁnd and use the correspondence directly for
rendering without using explicit scene geometry. These correspondences can be either established by
using feature point correspondences, optical ﬂow, or the disparity map measured or estimated between
acquired images.
One early representative approach using implicit geometry was based on image warping [40] or
image morphing [41]. Manual setup for specifying some feature correspondences as the anchor points
or a control mesh is often required. In [40], a novel view is generated by warping the control mesh
through spline interpolation. Recently, image warping based on moving least squares also provides nice
properties that can preserve the rigidness of the object under deformation [42]. Instead of using points,
Beier and Neely used a set of corresponding line segments for image warping [41]. For any novel view
sitting in between the two reference views, the matched line segments are ﬁrst interpolated, which then
the transform from one of the reference views to the novel view are derived. In [43], Zhang et al. also
proposed a feature-based light ﬁeld morphing approach that relies on the ray correspondence derived
from the manually speciﬁed feature polygons on the two reference views.

4.17.4 Scene Representation
503
Instead of using manual speciﬁed points, line segments, or polygons, Chen and Williams proposed
View Interpolation [44] that utilizes optical ﬂow between the two reference images to establish dense
point correspondences. To synthesize a novel view in-between these two reference views, the ﬂow
vectors are linearly interpolated to warp and blend pixels in the reference images. The underlying
assumption of View Interpolation is that the camera motion is perpendicular to its viewing direction.
It will perform very well if the assumption holds or the two reference images are close to each other.
Otherwise, the synthesized image wouldn’t be physically valid.
To address this problem, Seitz and Dyer proposed view morphing [45,46] that guarantees the rendered
view to be physically valid by introducing a pre-warping stage and a post-warping stage. This is similar
to the image rectiﬁcation widely utilized in stereo image matching, where corresponding epipolar lines
between these two reference images will be aligned horizontally [46]. After the rectiﬁcation, the two
images will share the same image plane and camera motion will be perpendicular to its viewing direction.
Linear interpolation is then used to get the intermediate view, followed by postwarping to compensate
the rectiﬁcation effect on that view.
The epipolar constraint describes the relationship between point correspondences on two images.
For three images, a three-way constraint is represented by the trifocal tensor [47]. Given two views
in correspondence and a tensor, the corresponding third view can be generated by a warping function.
Avidan and Shashua proposed a view synthesis algorithm based on this principle [48], where the
speciﬁcation of a novel view is more direct compared with the epipolar constraint based methods.
Moreover, trifocal tensor based method is often more stable than the epipolar constraint based ones
under certain singular camera conﬁgurations (e.g., when the camera centers are collinear). In tri-view
morphing [49], Xiao and Shah extended this idea for rendering a novel view within a triangle spanned
by three camera centers can be synthesized without explicit geometry. The camera calibration and point
correspondences between the existing cameras are established by using the trifocal tensor with a small
amount of manually labeled feature lines.
To summarize, image-based rendering based on implicit geometry provides good rendering under
constraints that novel image stays in-between reference views. While this is sufﬁcient for some appli-
cations, explicit geometry is preferred to allow higher ﬂexibility for free-view rendering.
4.17.4.3 Explicit geometry
Explicit geometry model is widely adopted in computer graphics for free-view rendering. While recent
advancements in computer graphics hardware is capable of rendering sophisticated geometry models
and simulating various lighting effects, the detailed geometry model of objects inside an arbitrary scene
may not be easily obtained. As mentioned previously, at one extreme of the geometry-image continuum
of scene representations of IBR, the densely sampled light ﬁeld can be utilized for novel view synthesis
without implicit or explicit geometry. At another extreme of the geometry-image continuum, only one
single texture map is sufﬁcient for free-view rendering when the detailed geometry model is available.
As it is relatively easier to acquire a few images of the scene, rather than a large amount of images with
a camera array used in Lumigraph [26], a trade-off can be resorted between the amount of acquired
images and the details of the scene geometry.
Various explicit scene/object geometry representations have been used in the literature. The bill-
boards, or sprites, have long been used in computer graphics as a simple representation for objects

504
CHAPTER 17 Image-Based Rendering
Camera
viewpoint
Object 1
Object 3
Object 2
Image plane
a
b
c
d
0l
FIGURE 17.13
The layered depth image [24].
like trees. A billboard can be simply modeled as a rectangular mesh with a texture and an opacity mask
(alpha channel) indicating foreground pixel on the mesh, which can only be rendered from a single
angle to the viewer. However, it can be scaled to simulate perspective projections, and overlapped with
other objects to simulate occlusions.
To describe more details about an object, models derived from a depth map are commonly utilized.
The depth map can be easily derived from synthetic scenes generated by computer graphics. For real
objects and scenes, the depth map can be acquired by using 3D depth sensing devices such as time-of-
ﬂight laser range ﬁnders or structured light 3D scanners. Thanks to the advances in multi-view stereo
vision research community [50,51], the depth map (or disparity map) can also be directly estimated
from multiple images acquired from different viewpoints.
In [52], Shade et al. proposed the layered depth image (LDI). LDI is a view of the scene from a
single camera viewpoint, but with multiple pixels along each light ray to record the depths of surfaces,
which can be interpreted as a multi-valued depth map. As depicted in Figure 17.13, on the path of the
light ray l0, the depth and color values of point a, b, c, and d are all recorded. Extensions to the LDI
include the layered depth cube [53] and the LDI tree [54].
The rendering algorithms of scene representations with dense depth map are often based on 3D
warping. In [55], McMillan proposed a 3D warping algorithm to render novel views by back-projecting
pixels from the reference view to their 3D locations, and then re-projecting to the novel view. In
[56], Oliveira and Bishop proposed to factorize the warping process into a pre-warping stage followed
by a standard texture mapping to speed-up the warping process. The pre-warp handles the parallax
effects resulting from the depth map and the view direction. The subsequent texture-mapping handles
the scaling, rotation, and remaining perspective transformation, which can be accelerated by standard
graphics hardware. A similar factoring algorithm was performed for the LDI [52], where the depth map
is ﬁrst warped to the output image with visibility check, and colors are pasted afterwards.
One major problem in the 3D warping approach is that holes may be resulted in the rendered view due
to undersampling or disocclusion (uncovered regions occluded in the reference view but visible in the
novel view). The disocclusion problem is partially resolved in the LDI representation [52] by introducing

4.17.4 Scene Representation
505
multiple depth values along a light ray. In [57], holes due to disocclusion are also not serious as multiple
images are available for rendering. The undersampling problem can also be resolved by taking more
images. For example, a modiﬁed LDI approach, the LDI tree [54], combines multiple reference views
in to a single hierarchical representation, which maintains the resolution of each reference view in the
data structure. On the other hand, even if holes do happen, they may be removed through algorithms
such as splatting [52,58] or inpainting [59].
Instead of directly using depth map as the geometry representation which may involve a huge amount
of 3D points or voxels, mesh model representation has long been used in computer graphics pipeline for
efﬁcient manipulation, storage, and rendering. For synthetic scenes, the mesh model is usually available
for free-view rendering with texture mapping. For real-world scenes, the acquired or estimated geometry
model like a depth map is often in a volumetric form [60–62]. Although the volumetric model can be
converted to a mesh model [63], sometimes it may be preferable to render with the volumetric model
directly. One issue with the volumetric model is that it has a ﬁnite resolution. Model smoothing like the
one proposed in [60] may need to be applied during the rendering to improve the visual quality.
In the work of unstructured Lumigraph rendering [64], Buehler et al. utilized mesh representations
for real-time rendering. The mesh is derived from the scene geometry proxy and serves as the camera
blending ﬁeld for fusing images from different cameras, where the vertex blending weights for each
camera are determined by a set of factors including angular penalty, resolution penalty, and ﬁeld-
of-view penalty. These factors are guided by the eight goals that they considered for the ideal IBR
rendering, including the use of geometric proxies, unstructured input, epipole consistency, minimal
angular deviation, continuity, resolution sensitivity, equivalent ray consistency, and real-time.
In computer graphics, a ﬁxed mesh model is utilized for rendering an object. But for IBR, the mesh
can also be adaptive according to the rendering viewpoint and position. For example, Rademacher
proposed the view-dependent geometry [65], where the scene geometry may vary during the rendering
when the view position changes. Such approach is attractive for scenes where the geometry model is
estimated on the ﬂy and only locally consistent. In Section 4.17.5.2, the scene geometry estimated by
plane-sweeping algorithm with neighboring images is also an example of view-dependent geometry.
Similar to the view-dependent geometry, the texture mapping can also be view dependent. Texture
map has been widely used in computer graphics pipeline to increase the visual realism of the synthesis.
Instead of using ﬁxed texture mapping based on the Lambertian surface assumption, view-dependent
texture mapping (VDTM) [66] would enable capturing of the details that cannot be easily represented by
geometry itself, such as highlight, reﬂections, and refractions caused by transparent objects. VDTM was
ﬁrst proposed by Debevec et al. to composite multiple images from reference views to form the texture
map through a weighting scheme. The weights are inversely proportional to the angular deviation from
the reference views to the virtual view to be rendered. Later a more efﬁcient implementation of VDTM
was proposed in [67] by using projective texture mapping, which is now a common feature available in
computer graphics hardware and part of OpenGL standard. The per-pixel weight calculation in VDTM
was replaced by a per-polygon search in a pre-computed lookup table. Note that VDTM is in fact a
special case of the later proposed unstructured Lumigraph rendering [64], where angular deviation is
one of the factors that decide the blending weights of each reference view.
The image-based visual hull (IBVH) algorithm proposed by Matusik et al. [68] also adopted the
idea of VDTM. In IBVH, the scene geometry can be efﬁciently reconstructed from the segmented
foreground silhouettes through an image space visual hull [69] algorithm, especially in the surround

506
CHAPTER 17 Image-Based Rendering
camera setting with a simple background. A texture pixel was generated from the reference views by
back projection using only the light ray with the smallest angular deviation. IBVH has demonstrated its
usage in free-view TV synthesis with dynamic objects in the scene.
4.17.5 Rendering
As addressed in previous sections, light ﬁeld sampling, geometry representation, and rendering are
highly dependent on the density of acquired images and the scene complexity. Once sampling and
representation have been determined, different degrees of effort for rendering are required according
to whether the acquired images are sufﬁcient to represent the scene complexity. Here, we present sev-
eral examples to demonstrate general techniques of image-based rendering: light ﬁeld fusion, plane
sweeping and spherical surfaces sweeping in a dense camera setting, image-based visual hulls and
voxel sweeping in a surround camera setting, depth sweeping, and depth-image-based rendering for
complicated scenes.1 Note that in these settings, we assume that the cameras are fully calibrated with
known poses, where we know the cameras’ intrinsic parameters as well as their positions and orien-
tations. The calibration can be easily conducted with some calibration patterns [70] or estimated with
objects inside the scene using the structure-from-motion framework (SFM) [71].
4.17.5.1 Light ﬁeld fusion
In contrast to conventional geometry-based rendering methods, the light ﬁeld fusion approach proposed
by Kubota et al. [72] renders a virtual scene without using scene geometry. To synthesize a virtual view,
the approach starts by rendering multiple images at a given viewpoint with different focal plane depths.
By approximating a scene with a set of planar object surfaces at different depths, each rendered image can
be treated as the super-position of ﬁltered images of the object surfaces from different depths. Therefore,
each rendered image is blurred except for regions located exactly at the speciﬁc depth. The explicit
formulation for each rendered image can be derived, and each different depth layer can then be recovered
by joint optimization to create an all in-focus virtual view. More formally, given a scene containing N
different depth planes at {zi}N
1 , we can deﬁne the object surface’s texture at the nth depth plane as
fn(x, y) ≡
 f (x, y), if d(x, y) = zn
0,
otherwise
,
for n = 1, 2, ..., N,
(17.7)
where f (x, y) is the ideal all in-focus view, and d(x, y) is the depth map from the novel viewpoint.
With this, each rendered image at different focal plane depths can be modeled as
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
g1 = f1 + h12∗f2 + h13∗f3 + · · · + h1N∗fN,
g1 = h21∗f1 + f2 + h23∗f3 + · · · + h2N∗fN,
...
gN = hN1∗f1 + hN2∗f2 + · · · + hN N−1∗fN−1 + fN,
(17.8)
1Interested readers can refer [38] for a broader survey on various techniques in the literature.

4.17.5 Rendering
507
where hnm is a spatial varying ﬁlter that offsets surface textures according to the depth displacement
between planes. With this explicit formulation, we can recover surface textures at different depths
iteratively by the projection onto convex sets (POCS) method [73]:
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
f (k)
1
= g1 −h12∗f (k−1)
2
+ h13∗f (k−1)
3
+ · · · + h1N∗f (k−1)
N
,
f (k)
2
= g2 −h21∗f (k)
1
−h23∗f (k−1)
3
+ · · · + h2N∗f (k−1)
N
,
...
f (k)
N
= gN −hN1∗f (k)
1
−hN2∗f (k)
2
+ · · · + hN N−1∗f (k)
N−1,
(17.9)
where each surface texture at the kth iteration is linearly reconstructed by the latest textures recovered
by previous iterations. The all in-focus view can be simply composed by
f (k) = f (k)
1
+ f (k)
2
+ · · · + f (k)
N .
(17.10)
Even though this approach is easy to implement, it assumes no occlusions and dense sampling where
the separation between different cameras is small. Figure 17.14 shows some rendering results by the
light ﬁeld fusion. It utilizes 81 real images captured with a 9 × 9 camera array with cameras separated
by 20 mm and object depths range from 590 to 800 mm. Compared to conventional light ﬁeld rendering
by quadrilateral interpolation, the light ﬁeld fusion provides much sharper rendering.
4.17.5.2 Plane sweeping and spherical surface sweeping
The plane-sweeping algorithm originated from Collins’ Space-Sweep approach [74]. It provides efﬁ-
cient multi-image matching and 3D reconstruction using cameras with known intrinsic and extrinsic
parameters. In contrast to the conventional approach that chooses one image as reference and then
searches for point correspondences on images from the other cameras based on the epipolar constraints
to derive the depth by triangulation, the plane sweeping approach quantizes the scene with multiple
depth planes and hypothesizes the depth of each light ray to be intersecting with one of these planes.
Collins’ Space-Sweep approach only focuses on point-like features such as corners and edgels (edge
segments) sparsely distributed on each image, which can be efﬁciently tested by counting the number of
light rays back-projected from all other cameras that intersect on each cell of a depth plane. However,
image-based rendering in principle requests depth values for all the light rays of a virtual camera in
order to render the novel scene. Therefore, instead of counting the intersecting light rays of each cell
on each depth plane, the plane-sweeping projects the intersecting point of each light ray on each depth
plane and then performs forward projection to ﬁnd the correspondences across multiple cameras.
As shown in Figure 17.15, a light ray intersects each hypothetical depth with an intersecting point
in the space. The corresponding points on the other cameras’ image plane can be easily found by
projecting the intersecting point toward the camera’s optical center. Thus, the optimal depth of the
light ray is obtained by choosing the plane containing the intersection point with the highest color
consistency from its projections among all other cameras. Figure 17.16 shows a rendered scene based
on plane sweeping, where the cameras are arranged roughly along a line as shown in Figure 17.16b. It can
be observed that the depth recovered by plane sweeping is not perfect, especially for object boundaries
and homogeneous regions. The major reason is that plane sweeping estimates depth for each light ray

508
CHAPTER 17 Image-Based Rendering
(a)
(b)
(c)
FIGURE 17.14
Novel view synthesis from light ﬁeld fusion and conventional light ﬁeld rendering: (a) synthesized images by
using conventional light ﬁeld rendering, (b) synthesized images using light ﬁeld fusion, which are recovered
from (c) rendered images at different focal depth planes [72].
individually; no visibility testing or global optimization is conducted to resolve occlusions and matching
ambiguity. To deal with these issues, a depth-based approach like multi-view stereo matching may be
required if the scene contains severe object occlusions.
Plane sweeping is suitable for cameras arranged either on a line or a plane. For example, Collins
[74] applied the algorithm to aerial imagery, such that the hypothetical planes could be easily set
up as parallel to the ground plane. For non-planar camera settings or the surround camera setting
where cameras cover a 360◦view of the scene (e.g., The Matrix), a ﬁxed set of hypothetical planes
would be less appropriate, as the distance of each light ray intersecting the depth planes from different
viewpoints varies greatly. Some cameras are positioned behind the planes, so they may observe different
aspects of the surface even for the same 3D point. Therefore, it is preferable to use view-dependent
depth planes instead of a ﬁxed depth planes. Furthermore, it is more efﬁcient to perform a depth
hypothesis that is invariant to the viewing direction. Similar to the idea proposed by Zabulis et al.
[75] for stereo reconstruction, we may use view-dependent spherical surfaces as the hypothetical depth
surfaces. As shown in Figure 17.17, the spherical hypothetical surfaces are dependent on the virtual
camera’s location and orientation. Each light ray from the virtual camera intersects those surfaces with
exactly the same set of distances. By projecting the intersecting points to existing nearby cameras, we
can derive the correspondences, calculate color consistency among multiple cameras, and then ﬁnd

4.17.5 Rendering
509
Virtual 
Camera
Camera Array
Object
Light Rays
Hypothetical
Planes
FIGURE 17.15
Plane sweeping.
FIGURE 17.16
Image-based rendering based on plane sweeping: (a) one novel view image, (b) the camera arrangement
and the estimated 3D textured model, (c) the mesh model projected on the 2D image plane, and (d) the
reconstructed mesh model in 3D.

510
CHAPTER 17 Image-Based Rendering
 
Virtual 
Camera
Camera Array
Object
FIGURE 17.17
Spherical surface sweeping.
FIGURE 17.18
Image-based rendering based on spherical surface sweeping: (a) one novel view image, (b) the camera
arrangement and the estimated 3D textured model, (c) the mesh model projected on the 2D image plane,
and (d) the reconstructed mesh model in 3D.

4.17.5 Rendering
511
(a)
(b)
(c)
FIGURE 17.19
3D object modeling and rendering with image-based visual hull: (a) sample input images, (b) segmentation
of the object of interest, and (c) novel view synthesis of the object [77].
the optimal depth for each light ray. In Figure 17.18, we show some rendering results from spherical
surface sweeping on a synthetic scene. A real scene example is presented in the case study described
in Section 4.17.6.
4.17.5.3 Image-based visual hull and voxel sweeping
When the cameras are in a surround setting that covers a 360◦view of the scene, it is possible to infer the
scene depth even more efﬁciently without using color consistency. As mentioned in Section 4.17.4.3,
the image-based visual hulls algorithm proposed by Matusik et al. [68] only requires the foreground
silhouette in order to shape the volume of the foreground objects. Light rays back-projected from a
camera’sopticalcenterthroughtheforegroundsilhouetteforma3Dvolumeinthespace.Theintersection
of all of these back-projected volumes constraints the space where the foreground object could exist,
which is called the image-based visual hull. In practice, it can be implemented by forming a volume of
voxels in the space and then removing a voxel whose projection is outside of the foreground silhouette
of any camera. Thus, it can be performed very efﬁciently as long as the foreground silhouette is easy to
obtain, as in the case where blue screens are utilized.
In Figure 17.19, we show an example of 3D object rendering using the structure-from-motion
algorithm proposed by Snavely et al. [71] to estimate the camera parameters and an octree-based
structure-from-silhouette algorithm proposed by Chen et al. [76] to obtain a volumetric 3D reconstruc-
tion of the object. For scenes with background clutters, the object of interest must be segmented out
ﬁrst to facilitate further processing. Figure 17.20 shows an example of 3D modeling and rendering with
background clutters based on the interactive co-segmentation proposed by Kowdle et al. [77].
In addition, it is also possible to combine the image-based visual hull algorithm with plane/spherical
sweeping. The advantage of combining both methods is that we do not have to explicitly reconstruct
the volumetric model of the scene, and it ﬁts well in the plane/spherical sweeping framework when
foreground silhouettes are available. Here we use spherical sweeping as it is more appropriate for
the surround camera setting. Similar to the original spherical sweeping algorithm, it ﬁrst places a set

512
CHAPTER 17 Image-Based Rendering
(a)
(b)
(c)
FIGURE 17.20
3D object modeling and rendering with background clutter: (a) sample input images, (b) segmentation of
the object of interest, and (c) novel view synthesis of the statue [77].
of hypothetical surfaces with respect to the virtual camera. Instead of testing the color consistency
across multiple cameras, we test whether the projection of an intersecting point is inside the foreground
silhouette on the projected image. We choose the intersecting point within the visual hull that is closest
to the viewpoint as the light ray depth. We call this method as voxel sweeping as it conceptually performs
surface sweeping on the voxels of the object volumetric models. Figure 17.21 shows some examples
of voxel sweeping compared to pure spherical sweeping. It is clear that object occlusions are better
handled with the voxel sweeping approach.
FIGURE 17.21
Comparisons between spherical sweeping (top) and voxel sweeping (bottom) under surround camera setting.

4.17.5 Rendering
513
4.17.5.4 Depth sweeping
As mentioned in Section 4.17.5.2, although plane sweeping is very efﬁcient, it does not handle occlusion.
When occlusion occurs, a surface point may only be visible to a subset of cameras. Without knowing
the visibility in advance, it would be difﬁcult to distinguish whether the low color consistency is a result
of wrong depth hypothesis or occlusion, as illustrated in Figure 17.22.
To handle this problem, the multi-view stereo (MVS) approach could be used to infer point corre-
spondences and occlusions simultaneously [51]. However, it could be time-consuming to perform MVS
for joint optimization. One possible alternative is to pick one camera as the reference view, and perform
plane sweeping or spherical sweeping based on the reference camera.
As shown in Figure 17.23, for each light ray of the reference camera (the middle one), the color
consistency for a depth hypothesis is measured by the agreement of corresponding pixels on neighboring
views with respect to the light ray’s color intensity of the reference view. As there is no occlusion for
each pixel on the reference view, we are conﬁdent that its correspondences in other cameras, when not
occluded, must meet the brightness constancy constraint with the pixel on the reference image. In this
way, we obtain a better depth map for the reference view.
Figure 17.24 shows a real scene containing multiple persons with heavy occlusions. To speed-
up the depth estimation, plane sweeping is operated in a coarse-to-ﬁne matter. The resulting depth
map is ﬁltered with the foreground mask obtained by background segmentation and Gaussian ﬁlter to
generate a smoother depth map. By performing the same procedure for each view, we obtain depth
maps for each camera as the reference view. These depth maps can be fused together to generate a
FIGURE 17.22
Examples of occlusions affecting the color consistency veriﬁcation in plane sweeping.

514
CHAPTER 17 Image-Based Rendering
FIGURE 17.23
Depth reasoning by color consistency veriﬁcation against the reference view.
3D volume of the scene and then project the 3D volume to the virtual camera via 3D warping, as
addressed in Section 4.17.4.3. Alternatively, we can adopt a similar concept from voxel sweeping and
perform plane sweeping with respect to the virtual camera by projecting an intersecting point onto
each view and verifying whether it is inside the object volume by performing the Z-buffer test. This
is conceptually equivalent to performing plane sweeping on fused depth maps from multiple views.
This approach, called depth sweeping, provides a simple way to fuse multiple depth maps in order to
form a view-dependent depth map for novel view synthesis. Figure 17.25 shows the rendering results
of depth sweeping compared to plane sweeping without occlusion reasoning. It is clear that the object
boundaries are much better preserved in the depth sweeping approach.
4.17.5.5 Depth-image-based rendering
As described in the previous subsection, depth sweeping provides a simple way to fuse depth maps
from multiple views. In reality, it does not matter whether the depth map is acquired from a depth
sensor or estimated from multiple images. As long as it can be transformed to a common world coor-
dinate system, it will be easy to ﬁnd the correspondences across multiple views by projection. Recent
advancements in depth sensing provide a robust and efﬁcient way to acquire the depth map. Nowadays,
depth sensors like time-of-ﬂight sensors or structured infrared sensors provide depth map acquisition
with moderate accuracy in real-time. Therefore, it is advantageous to combine depth sensors and color
sensors in order to form a depth-based IBR system. In fact, these two types of sensors complement

4.17.5 Rendering
515
FIGURE 17.24
Example of depth estimation using plane sweeping with respect to a reference view: (a) the input image,
(b) depth map generated with plane sweeping, (c) foreground segmentation, and (d) ﬁltered depth map.
each other for depth sensing. On the one hand, color sensors estimate depth based on color consistency.
Therefore, it performs well for capturing sharp edges, but not as well for homogenous regions. On the
other hand, the depth sensors perform extremely well for surfaces with homogenous color intensity but
not as well for sharp depth discontinuities. Therefore, the depth derived from depth sensors can serve as
the geometry proxy of the scene, which can be reﬁned using color consistency with color sensors. Here,
we use the Microsoft Kinect device with a color camera array to demonstrate how to integrate depth
sensors and color sensors for depth-image-based rendering (DIBR). The depth sensing technology used
by the Kinect sensor device is based on the infrared structured light developed by PrimeSense. Since
the sunlight contains infrared components that interfere with the infrared structured light pattern, it is
mainly for indoor use with a depth ranging limit from 1.2 to 3.5 m.
4.17.5.5.1
Color/depth camera calibration for Kinect
To integrate a depth sensor with a color sensor, it is crucial to know the point correspondence between
the depth sensor and the color sensor. To this end, joint depth/color camera calibration is required to
obtain the intrinsic parameters of each sensor and the relative pose between these two sensors. Recently,
various approaches have been proposed for Kinect sensor calibration [78]. Here, we use the method
proposed by Herrera et al. [78] for demonstration. It utilizes a checkerboard pattern attached to a planar

516
CHAPTER 17 Image-Based Rendering
FIGURE 17.25
Comparison of plane sweeping (top) and depth sweeping (bottom) on a scene with severe object occlusions.
surface with sharp corners (e.g., a rectangular table) as the calibration object. The checkerboard grid
corners and surface corners serve as the points for initial calibration of the color sensor and the depth
sensor, respectively. Basically it follows Zhang’s approach [70] to calibrate each sensor separately with
multiple images under various camera poses or various calibration object poses. As the Kinect device
has built-in depth and color sensors with a ﬁxed relative position, it is preferable to move the Kinect
sensor under different poses rather than move the calibration object when the object is much heavier
than the device. Using the fact that surface coordinate and checkerboard coordinate are co-planar but
not aligned, Herrera et al. derived the relative pose based on the method proposed by Unnikrishnan and
Hebert [79]. The color camera projection is derived with respect to the checkerboard coordinate, while
the depth sensor projection is derived with respect to the surface coordinate. Let extrinsic matrices be
[Rc tc] and [Rd td] for color and depth sensors, respectively, where R is the rotation matrix and t is the
translation vector. Let the rotation matrices Rc and Rd be decomposed as column vectors [rc1 rc2 rc3]
and [rd1 rd2 rd3]. Then we can compute the surface normal and distance to the origin δ of the calibration
plane in each sensor’s coordinate. That is
nT
c xc −δc = 0,
where nc = rc3, δc = rT
c3tc
(17.11)
and
nT
d xd −δd = 0,
where nd = rd3, δd = rT
d3td.
(17.12)
According to Unnikrishnan and Hebert [79], the relative pose between the depth and color sensors can
be computed by
R′
r = MdMT
c
and tr = (McMT
c )−1Mc(bc −bd)T ,
(17.13)
where Mc and Md are matrices from concatenated surface normals, and bc and bd are vectors from
concatenated distance offsets. As R′
r may not be an orthonormal matrix, the relative rotation matrix is

4.17.5 Rendering
517
Rr derived by UVT where USVT is the singular decomposition of R′
r. Non-linear optimization by the
Levenberg-Marquardt algorithm is further carried out to jointly minimize the pixel projection error for
the color sensor and depth displacement error for the depth sensor.
4.17.5.5.2
Coordinate transfer
Once the joint calibration is complete, it is easy to ﬁnd the point correspondence between these two
sensors. Here, we use the open source libfreenect [80] for implementation. We ﬁrst need to convert
disparity to depth, and then calculate a depth pixel’s 3D location in the depth sensor’s coordinate. With
the estimated relative pose, the 3D point is projected to the color sensor to derive the corresponding pixel.
More speciﬁcally, given the disparity γ at depth pixel (ud, vd)T , the depth is related to the disparity by
zd =
1
α(γ −β)′ ,
(17.14)
where α and β are parameters obtained with calibration. Neglecting the possible lens distortion on the
depth sensor, the point’s 3D location in the depth sensor coordinate pd = [xd yd zd]T can be calculated
with the simple pin-hole model, i.e.,
xd = zd(ud −ud0)
fd
and
yd = zd(vd −vd0)
fd
,
(17.15)
where fd is the focal length, and ud0 and vd0 are the principal points of the depth sensor. With the
estimated relative pose, the point’s 3D location in the color sensor’s coordinate pc = [xc yc zc]T can
be derived as
pc = Rrpd + tr.
(17.16)
Then, the point can be projected to the image plane of the color sensor via perspective projection with
estimated intrinsic parameters including focal length, principal points, and radial distortion coefﬁcients.
Note that the operation in Eq. (17.16) can also be reversed to transfer a point from color sensor’s
coordinate to depth sensor’s coordinate, i.e.,
pd = RT
r (pc −tr).
(17.17)
So we can easily integrate the Kinect sensor device with a camera array for image-based rendering with
global pose calibration.
4.17.5.5.3
Depth sensors and camera array integration
For DIBR integration, global pose estimation is required to set a common world coordinate for all
cameras in the camera array and sensors of the Kinect devices. As the depth sensor and the color sensor
are ﬁxed in a Kinect device, their relative pose will not change. Therefore, the global pose estimation
only has to be conducted on the cameras in the camera array and on the color sensors of the Kinect
devices, which is identical to global pose calibration for conventional camera arrays.
The depth sensors from the Kinect devices provide dense depth maps with moderate accuracy. Some
post-processing, such as image de-noising and smoothing may be applied to remove outliers and ﬁll
holes from these depth maps. There may be several ways to utilize these depth maps. One approach

518
CHAPTER 17 Image-Based Rendering
FIGURE 17.26
The DIBR camera array with 26 cameras and two Kinect devices.
is to fuse these depth maps as the scene geometry and perform 3D warp with respect to the virtual
camera to generate a depth map for novel view synthesis. This works well when the virtual camera
is very close to the depth sensors. Another approach is to use these depth maps as soft constraints to
regularize the depth estimation for all other cameras. Thus, the depth map from each camera is close
to the one estimated from the depth sensor but captures subtle details neglected by the depth sensor.
Later, these depth maps can be fused together to form a depth map for the virtual view with the depth
sweeping approach described in Section 4.17.5.4. Other elaborate approaches are possible to provide
even better visualization. For example, we may try to exploit the conﬁdence of the depth map estimated
by the depth sensor. As mentioned before, the depth sensor performs better on textureless surfaces but
poorly near the depth discontinuities. By examining the acquired color images and depth images, we
may develop a better weighting scheme for combining the depth maps from different types of sensors
to provide better scene geometry for image-based rendering.
To demonstrate depth-image-based rendering, we built a camera array with 26 Axis M1100 net-
work cameras and two Kinect sensor devices as shown in Figure 17.26. Figure 17.27 shows images
acquired from the DIBR camera array. The camera separations are 6 in. horizontally and 7 in. vertically.
The cameras are connected via Ethernet to a PC, while two Kinect devices are connected directly to
the PC via USB ports. The processing frame rate is about 8 fps. Figure 17.28 shows examples rendered
from a real scene. We compare the rendering results and 3D reconstruction between conventional IBR
purely with color cameras, 3D reconstruction purely with Kinect devices, and DIBR with color cameras
plus two Kinect devices. The synthesized image from IBR looks ﬁne except for homogenous regions
like the wall and blue curtain. However, its 3D reconstruction is quite poor. The 3D reconstruction
from Kinect devices provides much better depth estimation. However, the synthesized image is not
pleasing, especially on regions with depth discontinuities like object boundaries. The DIBR solution
provides good reconstruction and better view synthesis. With the rapid advancement in depth sensing,
depth-image-based rendering is very promising and worth further investigation.

4.17.6 Applications
519
FIGURE 17.27
Images acquired by the DIBR camera array. The last four images are obtained from Kinect color and depth
sensors.
4.17.6 Applications
Image-based rendering has long been used for enhancing the visualization in various applications. As
mentioned in Section 4.17.2, the plenoptic camera has reached the consumer market that allows users to
adjust the focus after taking an image [10]. In addition, IBR has been applied to video stabilization with
a hand-held Point Gray ProFUSION-25C camera array [81], video surveillance with synthetic apertures
[15], and view synthesis for the Super Bowl XXXV with the EyeVision system developed by CBS and
CMU [82]. Here we present a case study of a multi-view video on demand system to demonstrate the
construction and usage of image-based rendering.
4.17.6.1 Motivation and background
Compared to conventional televisions, 3DTV extends user’s viewing experiences by using stereoscopic
image capturing. To further open a new horizon such that users at different view angles can observe
different aspects of the captured scene, multi-view image capturing are utilized to provide images from
different viewpoints. Either a dense camera setting or computation-intensive 3D scene reconstruction
is required for realistic free-view rendering. Here we present an image-based rendering method which

520
CHAPTER 17 Image-Based Rendering
(a) IBR
Kinect
DIBR
(b)
(c)
FIGURE 17.28
Comparisons between IBR, Kinect reconstruction, and DIBR. The top row shows rendered images and the
bottom row shows the reconstructed 3D geometry.
aims at real-time free-view rendering with a set of sparsely spaced cameras. The system implementation
is based on the framework proposed by Zhang [83], in which the plane sweeping for scene geometry
estimation is replaced by the spherical surface sweeping to accommodate the camera array in surround
camera setting.
There are various approaches for IBR in the literature. For static scenes, there are approaches involve
moving a camera around the scene and capturing many images. Novel views can then be synthesized
from the captured images, with or without the scene geometry. In contrast, when the scene is dynamic,
an array of cameras is needed. For instance, Matusik et al. [68] used 4 cameras for rendering using
image-based visual hull (IBVH). Yang et al. [84] had a 5-camera system for real-time rendering with
the help of modern graphics hardware; Schirmacher et al. [85] built a 6-camera system for on-the-ﬂy
processing of generalized Lumigraphs; Naemura et al. [86] constructed a system of 16 cameras for
real-time rendering. Zitnick et al. [87] used 8 cameras for high-quality video synthesis. Several large
arrays consisting of tens of cameras have also been built, such as the Stanford multi-camera array [25],
the MIT distributed light ﬁeld camera [88] and the CMU 3D room [89]. These three systems have 128,
64, and 49 cameras, respectively.
In the above camera arrays, those with a small number of cameras can usually achieve real-time
rendering [68,84]. On-the-ﬂy geometry reconstruction is widely adopted to compensate for the lack
of cameras, and the viewpoint is often limited. Large camera arrays, despite their increased viewpoint
ranges, often have difﬁculty in achieving satisfactory rendering speed due to the large amount of data to

4.17.6 Applications
521
Scene Depth
Estimation
Rendering
Camera 
Calibration
Multi-camera
Images
Rendered
Images
Virtual Viewpoint
Control
Camera 
Intrinsic & 
Extrinsic 
Parameters
Virtual Camera
Position & Orientation
FIGURE 17.29
System architecture of multi-view capturing for 3DTV.
be handled. The Stanford system focused on grabbing synchronized video sequences onto hard drives.
It certainly can be used for real-time rendering but no such results have been reported in literature. The
CMU 3D room was able to generate good-quality novel views both spatially and temporarily [60]. It
utilized the scene geometry reconstructed from a scene ﬂow algorithm that took several minutes to run.
While this is affordable for off-line processing, it cannot be used to render scenes on-the-ﬂy. The MIT
system did render live views at a high frame rate. Their method assumed constant depth of the scene,
however, and suffered from severe ghosting artifacts due to the lack of scene geometry. Such artifacts
are unavoidable according to plenoptic sampling analysis [35,36].
In this work, we proposed an efﬁcient IBR algorithm that generates high-quality virtual views by
reconstructing the scene geometry on-the-ﬂy. Differing from previous work [84,85], the geometric
representation we adopted is a view-dependent multi-resolution 2D mesh with depth information on its
vertices. This representation greatly reduces the computational cost of geometry reconstruction, making
it possible for real-time novel view synthesis.
4.17.6.2 System architecture
A set of components including camera calibration, depth estimation, and view-dependent texture ren-
dering is required to facilitate multi-view capturing and rendering for 3DTV. The system architecture
and operation ﬂow is shown in Figure 17.29.
The system starts from images acquired and decoded from the multi-camera array. Given acquired
images, camera calibration is performed to estimate the camera intrinsic and extrinsic parameters such
as focal length, position, and orientation for each camera in the multi-camera array. If cameras are
assumed to be ﬁxed without any camera motion, the procedure of camera calibration only needs to
be executed once. Otherwise, per frame calibration is required as done in the reconﬁgurable camera
array proposed by Zhang and Chen [90], where cameras may rearrange their physical positions and
viewing directions. Afterwards, Scene Depth Estimation is performed with the acquired images and the
camera parameters. Virtual camera information including its position and orientation speciﬁed via the
user interface of Virtual Viewpoint Control is also incorporated in Scene Depth Estimation to reconstruct
a view-dependent scene depth ﬁeld. Finally, Rendering procedure synthesizes images taken from the
virtual camera by using a multi-texture rendering technique in computer graphics.

522
CHAPTER 17 Image-Based Rendering
Within the whole system, Scene Depth Estimation plays the most signiﬁcant role. Instead of using
computation-intensive 3D structure reconstruction methods, we adopt the plane sweeping algorithm as
described in Section 4.17.5.2 to achieve efﬁcient depth estimation. In the plane-sweeping algorithm,
multiple hypothetical depths are set in the scene. Optimal depth is found by choosing the depth that gives
thehighestcolorconsistencyamonglightraysfromnearbycameras.Dependingonthescenecomplexity,
depth estimation may not be required at each pixel. Patch-based depth estimation can be performed in
a coarse-to-ﬁne approach, in which depth ﬁeld is ﬁrstly estimated with large-sized patch. When depths
at vertices belonging to the same patch differ too much, subdivision is performed to generate smaller
patches and depth is estimated at vertices of newly generated patches. Patches at different size levels
are veriﬁed iteratively, result in a depth ﬁeld with sufﬁcient resolution for view interpolation.
4.17.6.3 Camera calibration
For camera calibration, the well-known algorithm proposed by Zhang [70] is adopted. The algorithm
requires at least three checkerboard images at different relative viewing directions for each camera.
Our implementation is based on OpenCV that automatically detects the checkerboard grid inside an
image. Homography veriﬁcation is conducted to ensure that the detected checkerboard corners are on a
plane. The global calibration is performed with the checkerboard placing on the center of the stage that
serves as the global world coordinate. Figure 17.30 shows sample images utilized in calibration, within
which a checkerboard with size 1.75 m × 1.25 m serves as the calibration pattern.
4.17.6.4 Scene Depth Estimation
Within the whole system, Scene Depth Estimation plays the most signiﬁcant role. Different from the
plane-sweeping algorithm used in the planar camera setting, we use the spherical surface sweeping
algorithm for surround camera setting. In spherical surface sweeping algorithm, multiple hypothetical
depths centered at the virtual camera center are set in the scene as shown in Figure 17.17. These
hypothetical depths are view-dependent, instead of ﬁxed in the 3D scene. Therefore, cameras can be
arranged in a surround setting to view a 3D scene from viewpoints across 360◦. Similar to the plane-
sweepingalgorithm,optimaldepthisfoundbychoosingthedepththatgivesthehighestcolorconsistency
among light rays from nearby cameras.
Care needs to be taken in applying the above method. First, the location of the depth surfaces should
be equally spaced in the disparity space instead of in depth. Let dmin be the minimum scene depth,
and dmax be the maximum scene depth, N be the number of depth planes used. The nth depth plane
(n = 1, . . . , N) is located at:
dn =
1
1
dmax + n−1
N−1
	
1
dmin −
1
dmax

.
(17.18)
The above equation is a direct result from the sampling theory by Chai et al. [35]. In the same paper
they also developed a sampling theory on the relationship between the number of depth planes and the
number of captured images, which is helpful in selecting the number of depth planes. Second, when
projecting the test depth planes to the neighboring images, lens distortions must be corrected. Third, to

4.17.6 Applications
523
FIGURE 17.30
Sample images used in calibration (top) and detected boundaries of white blocks overlaid on the input
images (bottom).
improve the robustness of the color consistency matching among the noisy input images, a patch on
each nearby image is taken for comparison. The patch window size relies heavily on the noise level
in the input images. A larger patch is required to accommodate larger noise. The patch is ﬁrst down-
sampled horizontally and vertically by a factor of 2 to reduce some computational burden. Different
patches in different input images are then compared to give an overall color consistency score. Fourth,
as our cameras have large color variations, color consistency measures such as the sum of squared
differences (SSD) do not perform very well. We applied mean-removed correlation coefﬁcient for the
color consistency veriﬁcation (CCV), which was widely used in traditional stereo matching algorithms.
The overall CCV score of the nearby input images is one minus the average correlation coefﬁcient of
all the image pairs. The depth plane resulting in the lowest CCV score will be selected as the scene
depth.
To speed-up the depth estimation, the process is performed in a coarse-to-ﬁne manner to reconstruct
the scene geometry as a 2D multi-resolution mesh (MRM) with depths on its vertices. Since the spher-
ical surface sweeping is conducted with respect to the virtual camera center, the scene geometry is
view-dependent. The MRM solution signiﬁcantly reduces the amount of computation spent on depth
reconstruction, making it possible to be implemented efﬁciently in software. Depth is ﬁrstly estimated
with an initial regular and sparse 2D mesh. When depths at vertices belonging to the same patch differ
too much, subdivision is performed to generate smaller patches. During the subdivision, the midpoint
of each edge of the triangle is selected as the new vertices, and the triangle is subdivided into four
smaller ones. The depths of the new vertices are reconstructed under the constraints that they have

524
CHAPTER 17 Image-Based Rendering
to use neighboring images of the three original vertices, and their depth search range is limited to the
minimum and maximum depth of the original vertices. Note that the size of the triangles in the initial
regular 2D mesh cannot be too large, since otherwise we may miss certain depth variations in the scene.
In the current implementation, triangle subdivision is limited to no more two levels. Patches at different
size levels are veriﬁed iteratively, result in a depth map with sufﬁcient resolution for view interpolation.
4.17.6.5 Rendering
When the scene geometry has been obtained, novel view synthesis is straightforward. Our rendering
algorithm is very similar to the one in the ULR [64] except that our imaging plane has already been
triangulated. The basic idea of ULR is to assign weights to nearby images for each vertex, and render
the scene through multi-texture blending. In our implementation, the weights of the nearby images are
assigned according to its angular differences. As the input images may have severe lens distortions,
we cannot use the 3D coordinates of the mesh vertices and the texture matrix in graphics hardware
to specify the texture coordinates as in [85]. Instead, we perform the projection with lens distortion
correction ourselves and provide 2D texture coordinates to the rendering pipeline. Fortunately, such
projections to the nearby images have already been calculated during the depth reconstruction stage and
can simply be reused.
4.17.6.6 Post-processing and video broadcasting
Figure 17.31 shows the setup of the multi-view video on demand system with 36 Point Gray Flea2 CCD
cameras. These cameras are arranged in a ring with a 5-m radius. In our preliminary test, we utilize 19
cameras with 5◦angle separation between successive cameras to cover 90◦viewing angle of the subject
at the center. An example video is recorded with a performer exercising with a gymnastic ball. As the
FIGURE 17.31
System setup for the multi-view video on demand system.

4.17.6 Applications
525
FIGURE 17.32
Image composition of the foreground object and background scene: (a) initial synthesized image, (b) color-
based segmentation result, (c) synthesized background, and (d) the composite image.
plane sweeping or spherical surface sweeping doesn’t address the occlusion problem, erroneous depth
estimation occurred in the background regions near object boundaries. Therefore, we ﬁrst acquire clean
background images without the performer and synthesize background images from new viewpoints.
Spherical surface sweeping is performed to synthesize virtual view images containing the performer.
The foreground performer is extracted from the virtual view image and composited with the synthesized
background image. Ideally the background covered by green curtain serves as the blue screen for ease
of foreground extraction. However, imperfect lighting condition and background objects complicate
the task. We adopt the interactive co-segmentation [91] approach to provide a better segmentation.
Figure 17.32 shows the results from spherical surface sweeping, segmentation, and composition. In the
end, novel view videos are created to synthesize 120 virtual cameras arranged on two rings with 4-m
and 5-m radii. The angle separation is 1.25◦to enable smooth view transitions. Figure 17.33 shows
some examples of the rendered scene from several different viewpoints. During video playback, viewers
at home can change viewpoints freely by sending commands through the set-top box with a remote
control. Corresponding videos at different viewpoints are instantly switched and retrieved from a video
streaming server to enable ﬂuent watch of multi-view video programs.

526
CHAPTER 17 Image-Based Rendering
FIGURE 17.33
Rendered scene from different viewpoints.
4.17.7 Open issues and problems
In signal processing, how to sample, represent, and reconstruct signals are always the central topics of
interest. Similarly, how to perform efﬁcient light ﬁeld sampling, obtain informative scene representation,
and provide realistic rendering are still the major issues in image-based rendering. As the light ﬁeld
in general is a high-dimensional function, it’s difﬁcult to acquire sufﬁcient sampling for perfect light
ﬁeld reconstruction. One way to handle this problem is to use non-uniform sampling to distribute
available sampling resources more efﬁciently. In [90], Zhang and Chen designed a self-reconﬁgurable
camera array that adapted its camera arrangement to the scene under observation. A simple yet effective
algorithm was proposed to adjust the camera positions such that regions of high complexity will receive
higher attentions from more cameras. Alternatively, one may focus on the scene geometry reconstruction
to compensate the insufﬁciency of light ﬁeld sampling. High-quality multi-view stereo matching and
occlusion reasoning are still difﬁcult problems with pervasive attentions [50,51]. Recent interests in
using time-of-ﬂight or structured infrared depth sensors are getting increasing attentions. However, how
toeffectivelycombinedepthsensorandcolorsensorisstillanopenissuewaitingforfurtherinvestigation.
Another open issue not covered by this chapter is the compression. The light ﬁeld acquired with
multiple cameras contains huge redundancy in both spatial domain and temporal domain. With recent
interests of 3DTV in consumer electronics, H.264/MPEG-4 AVC standard deﬁned a uniﬁed syntax,
multiview video coding (MVC), for efﬁcient encoding and decoding multi-view video streams [92].
The task of how to best exploit the spatial-temporal redundancy to achieve the lowest rate and the least
distortion would remain an open problem for experts in video compression.
4.17.8 Implementation/code
The following IBR implementations are publicly available, which serve good starting points for study-
ing IBR:
•
CAView (http://chenlab.ece.cornell.edu/projects/MobileCamArray):

4.17.9 Data Sets
527
CAView stands for Camera Array Viewer. It is the simpliﬁed version of the IBR implementation
for the self-reconﬁgurable camera array [90] developed by Cha Zhang at Carnegie Mellon Uni-
versity, and now maintained by Advanced Multimedia Processing Lab, ECE, Cornell University. It
implements plane sweeping to reconstruct a view-dependent mesh with view-dependent texture for
real-time realistic rendering. It is written in C language, and use OpenGL for GUI and rendering.
CAView serves as the codebase for several rendering methods introduced in Section 4.17.5 and the
implementation of the multi-view video on demand system in Section 4.17.6.
•
Stanford Light Field Viewer (http://lightﬁeld.stanford.edu/lfs.html):
Stanford Light Field Viewer is a viewer used in the Stanford light ﬁeld archive. It was written
in Adobe ﬂex/actionscript by Andrew Adams at Computer Graphics Laboratory, Stanford Uni-
versity. The viewer provides an intuitive interface for operations like changing the pinhole aper-
ture and focal plane, so that users can understand how different settings affect the light ﬁeld
rendering.
4.17.9 Data sets
The following data sets are publicly available for IBR research:
•
CAView Dataset (http://chenlab.ece.cornell.edu/projects/MobileCamArray):
CAView Dataset contains synthetic scenes rendered with POV-Ray, Stereo data from Middlebury
stereo vision dataset, and real scenes captured by a camera array with 48 cameras. CAView conﬁgu-
ration and calibration ﬁles are accompanied with each scene, which is ready for use by CAView. This
dataset is maintained by Advanced Multimedia Processing Lab, ECE, Cornell University. Future
plans for this dataset will also include scenes captured from a camera array with Kinect sensor
devices for Depth-image-based rendering.
•
Stanford Light Field Archive (http://lightﬁeld.stanford.edu/lfs.html):
Stanford Light Field Archive contains various real scenes captured by the Stanford Multi-Camera
Array, the light ﬁeld gantry, a simple Lego Mindstorms gantry, and the light ﬁeld microscope. There
scenes can be viewed online with the Stanford Light Field Viewer.
•
MSR3DVideoDataset(http://research.microsoft.com/en-us/um/people/sbkang/3dvideodownload/):
MSR 3D Video Dataset contains two multi-view video sequences captured by eight video cameras
placed along a 1D arc. Each sequence is 100 frames long with camera resolution 1024 × 768 and
frame rate 15 fps. Camera calibration parameters are provided. Reference depth maps estimated
by the technique proposed by Zitnick et al. [87] are also included, which are useful for studies in
multi-view stereo matching, and multi-view video coding.
•
Wisconsin Light Field Video Dataset (http://pages.cs.wisc.edu/∼lizhang/projects/lfstable):
Wisconsin Light Field Video Dataset contains four multi-view sequences captured by 5 cameras
within a Point Gray ProFUSION-25C camera array. Different from other IBR datasets, the cameras
have small baselines. Both the camera array and scene objects are in motion, which is ideal for
evaluation of video stabilization algorithms. This dataset was released with the work proposed by
Smith et al. on light ﬁeld video stabilization [81].

528
CHAPTER 17 Image-Based Rendering
4.17.10 Conclusions and future trends
In this chapter, we interpret image-based rendering in a signal processing point of view. The basic
operations of sampling, representation, and reconstruction in signal processing correspond to light ﬁeld
sampling, scene representation, and rendering of the plenoptic function in image-based rendering. Based
on different assumptions, the high-dimensional plenoptic function can be reduced for efﬁcient sampling
and compact representation. Among various light ﬁeld representations, the 4D light ﬁeld imposes least
restrictions on the viewing positions. It is also closely related to the study of integral imaging that
captures and renders light ﬁeld through optics. For scene representation, knowledge of scene geometry
compensates the insufﬁciency from sampling and further facilitates efﬁcient and realistic rendering.
For rendering, various approaches have been proposed ranging from simple signal reconstruction to
computationally intensive depth estimation and occlusion reasoning.
For future trends, we notice two directions worth further investigation. The ﬁrst one is computational
photography with image-based rendering. Recent interests in computational photography take com-
putations from the back-end PC to the sensor device. Operations such as high dynamic range (HDR)
viewﬁnding and video stabilization can be easily done on a programmable camera [93]. When combined
with the plenoptic camera that captures 4D light ﬁeld, it’s possible to sample, represent, and render light
ﬁeld directly on a single portable device. Furthermore, instead of using a microlens array with limited
ﬁeld of view, other types of lens or setup like refractive spheres array or conic mirror arrays also receive
increasing attentions. Another trend is the depth-image-based rendering. With recent advancements in
depth sensing devices, real-time and robust depth map generation becomes available and affordable.
Depth sensor and color sensor compensate each other in depth sensing, which would make deep impact
to conventional multi-view geometry modeling mainly based on color sensors. Moreover, with its fast
depth acquisition, it may be possible to take the time dimension back to the light ﬁeld representa-
tion to deal with dynamic scene in a more rigorous way. With these two promising directions, readers
who are interested in this topic are encouraged to keep exploiting this exciting ﬁeld of image-based
rendering.
Glossary
Plenoptic function
a function used to model the light ﬁeld. In its full form, the light ﬁeld can be
described by a 7D plenoptic function of light wavelength (1D), time (1D), posi-
tions (3D), and angles (2D). Depending on different assumptions on its parame-
ters, the plenoptic function could be reduced to a 5D, 4D, or a 3D function
Light ﬁeld
a collection of light rays described by the plenoptic function. Given a densely
sampled light ﬁeld, image-based rendering can be casted as a simple table lookup
operation or an interpolation operation from existing sampled light rays
Light ﬁeld sampling a process of acquiring discrete samples from the plenoptic function. Usually it
refers to taking several digital images of the scene at different viewpoints in the
3D space

References
529
Plane sweeping
an approach to estimate a 3D point’s depth according to the color consistency of
the corresponding pixels across multiple cameras derived from a limited set of
depth hypotheses
Camera calibration
a process of estimating a camera’s intrinsic properties such as its focal length,
principal points, and extrinsic properties such as its pose and position with some
known calibration targets such as a checkerboard
Relevant Theory: Signal Processing Theory
See Vol. 1, Chapter 2 Continuous-Time Signals and Systems
See Vol. 1, Chapter 3 Discrete-Time Signals and Systems
See Vol. 1, Chapter 5 Sampling and Quantization
See Vol. 1, Chapter 9 Discrete Multi-Scale Transforms in Signal Processing
References
[1] Bullet time, Wikipedia, <http://en.wikipedia.org/wiki/Bullet_time>
[2] A. Gershun, The light ﬁeld, Moscow, 1936, J. Math. Phys. 18 (1939) 51–151 (translated by P. Moon and
G. Timoshenko).
[3] G. Lippmann, La photographie int’egrale, C.R. Acad. Sci. 146, 446–551 (translated by F. Durand).
[4] Nobelprize.org. The Nobel Prize in Physics 1908, Grabriel Lippmann. <www.nobelprize.org/nobelprizes/
physics/laureates/1908/lippmann-bio.html>.
[5] A.P. Sokolov, Autostereoscopy and Integral Photography by Professor Lippmann’s Method, Izd, Moscow
State University Press, MGU, 1911.
[6] D. E. Roberts, T. Smith, The history of integral print methods, an excerpt from Lens Array Print Techniques,
<www.integralresource.org/integralhistory.html>.
[7] H.E. Ives, Optical properties of a Lippmann lenticulated sheet, J. Opt. Soc. Am. 21 (1931) 171–176.
[8] T. Adelson, J.Y.A. Wang, Single lens stereo with a plenoptic camera, IEEE Trans. Pattern Anal. Mach. Intel.
14 (2) (1992) 99–106.
[9] R. Ng, M. Levoy, M. Brédif, G. Duval, et al., Light Field Photography with a Hand-Held Plenoptic Camera,
Stanford University Computer Science, Technical Report CSTR 2005-02.
[10] Lytro, Inc. <www.lytro.com/>.
[11] Y. Taguchi, A. Agrawal, A. Veeraraghavan, S. Ramalingam, R. Raskar, Axial-cones: modeling spherical
catadioptric cameras for wide-angle light ﬁeld rendering, in: Proceedings of SIGGRAPH Asia, Seoul, Korea,
December 2010, pp. 15–18.
[12] A. Agrawal, Y. Taguchi, S. Ramalingam, Analytical forward projection for axial non-central dioptric and
catadioptric cameras, in: European Conference on Computer Vision, Crete, Greece, September 2010, pp. 5–11.
[13] A. Veeraraghavan, R. Raskar, A. Agrawal, A. Mohan, J. Tumblin, Dappled photography: mask enhanced
cameras for heterodyned light ﬁelds and coded aperture refocusing, in: ACM SIGGRAPH, San Diego, CA,
USA, August 2007, pp. 5–9.
[14] M. Cho, B. Javidi, Three-dimensional tracking of occluded objects using integral imaging, Opt. Lett. 33
(2008) 2737–2739.
[15] N. Joshi, S. Avidan, W. Matusik, D. Kriegman, Synthetic aperture tracking: tracking through occlusions, in:
Proceedings of IEEE International Conference on Computer Vision, Rio de Janeiro, Brazil, October 2007,
14–20.

530
CHAPTER 17 Image-Based Rendering
[16] V. Vaish, R. Szeliski, C.L. Zitnick, S.B. Kang, M. Levoy, Reconstructing occluded surfaces using synthetic
apertures: stereo, focus and robust measures, in: Proceedings of IEEE Conference on Computer Vision and
Pattern Recognition, New York, NY, USA, June 2006, pp. 17–22.
[17] A. Wang, P.R. Gill, A. Molnar, An angle-sensitive CMOS imager for single-sensor 3D photography, in: IEEE
International Solid-State Circuits Conference, 2011.
[18] E.H. Adelson, J.R. Bergen, The plenoptic function and the elements of early vision, in: Computational
Models of Visual Processing, MIT Press, 1991, pp. 3–20.
[19] Point Grey Ladybug Sensor, <www.ptgrey.com/products/spherical.asp>.
[20] S.E. Chen, QuickTime VR—an image-based approach to virtual environment navigation, in: Computer
Graphics (SIGGRAPH’95), August 1995, pp. 29–38.
[21] A. Lippman, Movie maps: an application of the optical videodisc to computer graphics, in: Computer
Graphics (Proc. SIGGRAPH’80), 1980, pp. 32–43.
[22] G. Miller, E. Hoffert, S.E. Chen, et al., The virtual museum: interactive 3D navigation of a multimedia
database, J. Visual. Comput. Animat. 3 (3) (1992) 183–197.
[23] H.Y. Shum, L.W. He, Rendering with concentric mosaics, in: Computer Graphics (SIGGRAPH’99), August
1999, pp. 299–306.
[24] C. Zhang, T. Chen, A survey on image-based rendering—representation, sampling and compression,
EURASIP Signal Process. Image Commun. 19 (1) (2004) 1–28.
[25] M. Levoy, P. Hanrahan, Light ﬁeld rendering, in: Computer Graphics (SIGGRAPH’96), August 1996,
pp. 31–42.
[26] S.J. Gortler, R. Grzeszczuk, R. Szeliski, M.F. Cohen, The Lumigraph, in: Computer Graphics
(SIGGRAPH’96), August 1996, pp. 43–54.
[27] B. Wilburn, N. Joshi, V. Vaish, M. Levoy, M. Horowitz, High speed video using a dense camera array, in:
Proceedings of IEEE Conference on Computer Vision, and Pattern Recognition, 2004.
[28] J. Lehtinen, T. Aila, J. Chen, S. Laine, F. Durand, Temporal light ﬁeld reconstruction for rendering distribution
effects, ACM Trans. Graph. 30 (4) (2011) 55:1–55:12.
[29] H. Lensch, Techniques for Hardware-Accelerated Light Field Rendering, Friedrich-Alexander-Universität
Erlangen-Nürnberg, Master Thesis, 1999.
[30] P.P. Sloan, M.F. Cohen, S.J. Gortler, Time critical Lumigraph rendering, in: Symposium on Interactive 3D
Graphics, Providence, RI, USA, 1997, pp. 17–23.
[31] S.C. Chan, H.Y. Shum, A spectral analysis for light ﬁeld rendering, in: Proceedings of IEEE International
Conference on Image Processing, Vancouver, Canada, September 2000.
[32] A.V. Oppenheim, A.S. Willsky, S.H. Nawab, Signals and Systems, second ed., Prentice Hall, 1996.
[33] D.E. Dudgeon, Mersereau, R.M. Multidimensional Digital Dignal Processing, Prentice-Hall Signal Processing
Series, 1984.
[34] Z.C. Lin, H.Y. Shum, On the number of samples needed in light ﬁeld rendering with constant-depth
assumption, in: Proc. CVPR’00, Hilton Head Island, South Carolina, USA, June 2000.
[35] J.X. Chai, X. Tong, S.C. Chan, H.Y. Shum, Plenoptic sampling, in: Computer Graphics (SIGGRAPH’00),
July 2000, pp. 307–318.
[36] C. Zhang, T. Chen, Spectral analysis for sampling image-based rendering data, IEEE Trans. CSVT, 13 (11)
(2003) 1308–1050 (Special Issue on Image-based Modeling, Rendering and Animation).
[37] S.B. Kang, R. Szeliski, P. Anandan, The geometry-image representation tradeoff for rendering, in: Proceedings
of IEEE International Conference on Image Processing, Vancouver, Canada, September, 2000.
[38] H.Y. Shum, S.C. Chan, S.B. Kang, Image-Based Rendering, Springer, 2007.
[39] J. Lengyel, The convergence of graphics and vision, Technical Report, IEEE Computer, July 1998.
[40] G. Wolberg, Digital Image Warping, IEEE Computer Society Press, 1990.

References
531
[41] T. Beier, S. Neely, Feature-based image metamorphosis, in: Computer Graphics (SIGGRAPH’92), July 1992,
pp. 35–42.
[42] S. Schaefer, T. McPhail, J. Warren, Image deformation using moving least squares, in: ACM SIGGRAPH
2006, New York, NY, USA, 25 (3), July 2006, pp. 530–540.
[43] Z.P. Zhang, L.F. Wang, B.N. Guo, H.Y. Shum, Feature-based light ﬁeld morphing, in: Computer Graphics
(SIGGRAPH’02), July 2002, pp. 457–464.
[44] S.E. Chen, L. Williams, View interpolation for image synthesis, in: Computer Graphics (SIGGRAPH’93),
August 1993, pp. 279–288.
[45] S.M. Seitz, C.R. Dyer, Physically-valid view synthesis by image interpolation, in: Proceedings of the
Workshop on Representation of Visual Scenes, 1995, pp. 18–25.
[46] S.M. Seitz, C.M. Dyer, View morphing, in: Computer Graphics (SIGGRAPH’96), August 1996, pp. 21–30.
[47] R. Hartley, A. Zisserman, Multiple View Geometry in Computer Vision, second ed., Cambridge University
Press, 2003.
[48] S. Avidan, A. Shashua, Novel view synthesis in tensor space, in: Proceedings of IEEE Conference on
Computer Vision and Pattern Recognition, San Juan, Puerto Rico, 1997, pp. 1034–1040.
[49] J. Xiao, M. Shah, Tri-view morphing, Comput. Vis. Image Underst. 96 (3) (2004) 345–366.
[50] D. Scharstein, R. Szeliski, A taxonomy and evaluation of dense two-frame stereo correspondence algorithms,
Int. J. Comput. Vis. 47 (1/2/3) (2002) 7–42.
[51] S. Seitz, B. Curless, J. Diebel, D. Scharstein, R. Szeliski, A comparison and evaluation of multi-view stereo
reconstruction algorithms, in: Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,
vol. 1, 2006, pp. 519–526.
[52] J. Shade, S. Gortler, L.W. He, R. Szeliski, Layered depth images, in: Computer Graphics (SIGGRAPH’98),
August 1998, pp. 231–242.
[53] D. Lischinski, A. Rappoport, Image-based rendering for non-diffuse synthetic scenes, in: Rendering
Techniques ’98, pp. 301–314.
[54] C. Chang, G. Bishop, A. Lastra, LDI tree: a hierarchical representation for image-based rendering, in:
Computer Graphics (SIGGRAPH’99), August 1999, pp. 291–298.
[55] L. McMillan, An Image-Based Approach to Three-Dimensional Computer Graphics, Ph.D. Thesis, Depart-
ment of Computer Science, University of North Carolina at Chapel Hill, 1997.
[56] M. Oliveira, G. Bishop, Relief Textures, Technical Report, UNC Computer Science TR99-015, March 1999.
[57] H. Schirmacher, W. Heidrich, H.P. Seidel, High-quality interactive lumigraph rendering through warping,
Graphics Interface 2000, Montreal, Canada, May 2000.
[58] W.R. Mark, L. McMillan, G. Bishop, Post-rendering 3D warping, in: Proceedings of the Symposium on
Interactive 3D Graphics, ACM Press 1997, pp. 7–16.
[59] M. Bertalmio, G. Sapiro, V. Caselles, C. Ballester, Image inpainting, in: ACM SIGGRAPH, 2000, pp. 417–424.
[60] S. Vedula, S. Baker, T. Kanade, Spatio-temporal view interpolation, in: Proceedings of the 13th ACM
Eurographics Workshop on Rendering, June 2002.
[61] S.M. Seitz, C.R. Dyer, Photorealistic scene reconstruction by voxel coloring, in: Proceedings of IEEE
Conference on Computer Vision and Pattern Recognition, San Juan, Puerto Rico, June 1997, pp. 1067–1073.
[62] P. Eisert, E. Steinbach, B. Girod, 3-D shape reconstruction from light ﬁelds using voxel back-projection, in:
Vision, Modeling and Visualization Workshop 1999, Erlangen, Germany, November 1999, pp. 67–74.
[63] W. Lorensen, H. Cline, Marching cubes: a high resolution 3-D surface construction algorithm, in: Computer
Graphics (SIGGRAPH’87), Anaheim, CA, July 1987, pp. 163–169.
[64] C. Buehler, M. Bosse, L. McMillan, S. Gortler, M. Cohen, Unstructured Lumigraph rendering, in: Computer
Graphics (SIG-GRAPH’01), August 2001, pp. 425–432.
[65] P. Rademacher, View-dependent geometry, in: Computer Graphics (SIGGRAPH’99), August 1999,
pp. 439–446.

532
CHAPTER 17 Image-Based Rendering
[66] P. Debevec, C.J. Taylor, J. Malik, Modeling and rendering architecture from photographs: a hybrid geometry-
and image-based approach, in: Computer Graphics (SIGGRAPH’96), August 1996, pp. 11–20.
[67] P. Debevec, Y.-Z. Yu, G. Borshukov, Efﬁcient view-dependent image-based rendering with projective
texture-mapping, in: 9th Eurographics Rendering Workshop, Vienna, Austria, June 1998.
[68] W. Matusik, C. Buehler, R. Raskar, S. Gortler, L. McMillan, Image-based visual hulls, in: Computer Graphics
(SIGGRAPH’00), July 2000, pp. 369–374.
[69] A. Laurentini, The visual hull concept for silhouette based image understanding, IEEE Trans. Pattern Anal.
Mach. Intel. 16 (2) (1994) 150–162.
[70] Z. Zhang, Flexible camera calibration by viewing a plane from unknown orientations, in: International
Conference on Computer Vision, 1999.
[71] N. Snavely, S.M. Seitz, R. Szeliski, Photo tourism: exploring photo collections in 3D, ACM Trans. Graph.
(SIGGRAPH Proceedings) 25 (3) (2006) 835–846.
[72] A. Kubota, K. Aizawa, T. Chen, Virtual view synthesis through linear processing without geometry, in: IEEE
International Conference on Image Processing 2004, Suntec City, Singapore, October 2004.
[73] D.C. Youla, H. Webb, Image restoration by the method of convex projections: Part I—theory, IEEE Trans.
Med. Imaging MI-1 (1982) 81–94.
[74] R. T. Collins, A space-sweep approach to true multi-image matching, in: Proceedings of International
Conference on Computer Vision, and Pattern Recognition, 1996.
[75] X. Zabulis, G. Kordelas, K. Mueller, A. Smolic, Increasing the accuracy of the space-sweeping approach
to stereo reconstruction, using spherical backprojection surfaces, in: International Conference on Image
Processing (ICIP) 2006, Atlanta GA, October 2006, pp. 8–11.
[76] Z. Chen, H.L. Chou, W.C. Chen, A performance controllable octree construction method, in: Proceedings of
International Conference on Pattern Recognition, 2008.
[77] A. Kowdle, D. Batra, W. C. Chen, T. Chen, iModel: interactive co-segmentation for object of interest 3D
modeling, in: Workshop on Reconstruction and Modeling of Large-Scale 3D Virtual Environments, European
Conference on Computer Vision (ECCV), 2010.
[78] C.D. Herrera, J. Kannala, J. Heikkilä, Accurate and practical calibration of a depth and color camera pair, in:
International Conference on Computer Analysis of Images and Patterns, 2011.
[79] R. Unnikrishnan, M. Hebert, Fast Extrinsic Calibration of a Laser Range Finder to a Camera, Robotics
Institute, Pittsburgh, Technical Report CMU-RI-TR-05-09, 2005.
[80] OpenKinect project, <openkinect.org/>.
[81] B. M. Smith, L. Zhang, H. Jin, A. Agarwala, Light ﬁeld video stabilization, in: IEEE International Conference
on Computer Vision (ICCV), September 29–October 2, 2009.
[82] CMU EyeVision, <www.ri.cmu.edu/events/sb35/tksuperbowl.html>.
[83] C. Zhang, On Sampling of Image-Based Rendering data, Ph.D. Thesis, Carnegie Mellon University, June 2004.
[84] R. Yang, G. Welch, G. Bishop, Real-time consensus-based scene reconstruction using commodity graphics
hardware, in: Proceedings of Paciﬁc Graphics, 2002.
[85] H. Schirmacher, M. Li, H.P. Seidel, On-the-ﬂy processing of generalized lumigraph, Comput. Graph. Forum
20 (3) (2001) 165–174.
[86] T. Naemura, J. Tago, H. Harashima, Realtime video-based modeling and rendering of 3D scenes, IEEE
Comput. Graph. Appl. 22 (2) (2002) 66–73.
[87] C.L. Zitnick, S.B. Kang, M. Uyttendaele, S. Winder, R. Szeliski, High-quality video view interpolation using
a layered representation, ACM SIGGRAPH 23 (3) (2004) 600–608.
[88] J.C. Yang, M. Everett, C. Buehler, L. Mcmillan, A real-time distributed light ﬁeld camera, in: Eurographics
Workshop on Rendering 2002, 2002, pp. 1–10.
[89] T. Kanade, H. Saito, S. Vedula, The 3d Room: Digitizing Time-Varying 3d Events by Synchronized Multiple
Video Streams, Technical Report, CMU-RITR-98-34, 1998.

References
533
[90] C. Zhang, T. Chen, Active rearranged capturing of image-based rendering scenes—theory and practice, IEEE
Trans. Multimedia 9 (3) (2007) 520–531.
[91] D. Batra, A. Kowdle, D. Parikh, J. Luo, T. Chen, Interactively co-segmentating topically related images with
intelligent scribble guidance, Int. J. Comput. Vis. (IJCV) 93 (3) (2011) 273–292.
[92] MPEG-4 MVC, Introduction to Multiview Video Coding, <mpeg.chiariglione.org/technologies/mpeg-4/
mp04-mvc/index.htm> .
[93] A. Adams, E. Talvala, S. H. Park, et al., The Frankencamera: an experimental platform for computational
photography, in: H. Hoppe (Ed.), ACM SIGGRAPH 2010 Papers, ACM, New York, NY, pp. 1–12 (Los
Angeles, California, July 26–30, 2010).

18
CHAPTER
Activity Retrieval in Large
Surveillance Videos⋆
Greg Castanon*, Pierre-Marc Jodoin†, Venkatesh Saligrama*, and Andre Caron†
*Boston University Boston, MA, USA
†Université de Sherbrooke, Sherbrooke, Canada
4.18.1 Introduction
Surveillance video camera networks are increasingly ubiquitous, providing pervasive information gath-
ering capabilities.1 In many applications, surveillance video archives are used for forensic purposes to
gather evidence after the fact. This calls for content-based retrieval of video data matching user deﬁned
queries with robustness to clutter and distortion. Typical content-based retrieval systems assume that
the user is able to specify their information needs at a level to make the system effective. Being fun-
damentally situation-driven, the nature of queries depends widely on the ﬁeld of view of the camera,
the scene itself and the type of observed events. Moreover, independent events frequently overlap in
the same video segment. Formulating an accurate query in presence of these complex video dynam-
ics is fundamentally difﬁcult. Thus, unlike typical document retrieval applications, video surveillance
systems call for a search paradigm that goes beyond the simple query/response setting. The retrieval
engine should allow the user to iteratively reﬁne its search through multiple interactions.
Exploratory search [1] is a specialization of information-seeking developed to address such situa-
tions. It describes the activity of attempting to obtain information through a combination of querying
and collection browsing. Development of exploratory search systems requires addressing two criti-
cal aspects: video browsing and content-based retrieval. The focus of this chapter is the latter. The
material of this chapter is based on an ACM MM paper [2] for fast content-based retrieval adapted to
characteristics of surveillance videos. The most challenging of these are:
1. Data lifetime: Since video is constantly streamed, there is a perpetual renewal of video data. This
calls for a model that can be updated incrementally as video data is made available. The model must
also scale well with the temporal mass of the video.
2. Unpredictable queries: The nature of queries depends on the ﬁeld of view of the camera, the scene
itself and the type of events being observed. The system should support queries of different nature
that can retrieve both recurrent events such as people entering a store and infrequent events such as
abandoned objects and cars performing U-turns.
⋆This research was supported by NSERC Discovery Grant 371951, ONR Grant N000141010477, NGA Grant HM1582-09-
1-0037, NSF Grant CCF-0905541, and DHS Grant 2008-ST-061-ED0001.
1NBC News reported in 2011 that more than 30 million surveillance cameras have been sold in the US since 9/11 [3].
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00018-2
© 2014 Elsevier Ltd. All rights reserved.
535

536
CHAPTER 18 Activity Retrieval in Large Surveillance Videos
FIGURE 18.1
From streaming video, local features for each document are computed and inserted into a fuzzy, lightweight
index. A user inputs a query, and partial matches (features which are close to parts of the query) are inserted
into a dynamic programming (DP) algorithm. The algorithm extracts the set of video segments which best
matches the query.
3. Unpredictable event duration: Events are unstructured. They start anytime, vary in length, and
overlap with other events. The system is nonetheless expected to return complete events regardless
of their duration and whether or not other events occur simultaneously.
4. Clutter and occlusions: Tracking and tagging objects in urban videos is challenging due to occlusions
and clutter; especially when real-time performance is required.
5. Retrieval speed: Surveillance videos are very lengthy. Search systems must be fast if multiple user
interactions are expected.
The proposed system is summarized in Figure 18.1. As video steams in, low-level video features
are archived in a light-weight LSH index table. The user can retrieve the spatio-temporal position of
these features with simple, yet generic, queries. The computational burden of matching these features
to queries is left to the search engine. This implements a two-step search procedure. First, a lookup is
performed in the index to retrieve video segments that partially match the query. Then, the dynamic
matching algorithm groups these partial matches to ﬁnd optimal relevance to the query. Concretely,
while video streams in, only low-level activity features are archived in the lookup table. Then, given
a query such as “show me every red vehicle going to the right and then turning left,” the system ﬁrst
retrieve partial matches, i.e., any document matching with “red vehicles,” “vehicles going to the right,”
or “vehicles turning left.” The dynamic matching algorithm then keeps consecutive videos segments
whose content corresponds to red vehicles ﬁrst going to the right and then turning left.
Contributions: The proposed system includes many contributions to content-based retrieval applied
to video surveillance.
1. The inverted indexing scheme of local video features exhibits constant-time update and constant-time
identiﬁcation of video segments with a high-likelihood or relevance to the query.
2. The dynamic matching algorithm allows matching variable-length video segments and is robust to
concurrent video activity.

4.18.1 Introduction
537
3. Theorem 1 guarantees results with a low probability of false positives.
4. A novel retrieval system which allows for light-weight storage and high-level queries.
4.18.1.1 Previous work
Most video papers devoted to summarization and search focus on broadcast videos such as music clips,
sports games, movies, etc. These methods typically divide the video into “shots" [4–7] by locating and
annotating key frames corresponding to scene transitions. The search procedure exploits the key frames
content and matches either low-level descriptors [6] or higher-level semantic meta-tags to a given query
[8].
Unfortunately, surveillance videos are fundamentally different than conventional videos. Not only
do surveillance videos have no global motion induced by a moving camera, it often shows unrelated
moving and static objects. For that reason, surveillance videos can hardly be decomposed into “scenes"
separated by key frames that one could summarize with some meta tags or a global mathematical model.
Furthermore, surveillance video have no closed-caption or audio track one could rely on [8].
For that reason, search in a surveillance video is all about properly understanding and indexing
the dynamic content of the video in a way that is compatible with arbitrary upcoming user-deﬁned
queries. In that perspective, most scene-understanding video analytic methods work on a two-stage
procedure: (1) learn patterns of activities via some clustering/learning procedure and than (2) recognize
new patterns of activity via some classiﬁcation stage. Since activities in public areas often follow some
basic rules (think of trafﬁc lights, highways, building entries, etc.) the training stage often quantiﬁes
space and time into a number of states with transition probabilities. Common models are HMMs [9–13],
Bayesian networks [14,15], context free grammars [16], and other graphical model [17–19]. As for the
classiﬁcation stage, it is either used to recognize pre-deﬁned patterns of activity [18,20–27] (useful
for counting [22,28]) or detect anomalies by ﬂagging everything that deviates from what has been
previously learned [11,29–34]. Let us mention that methods working on global behavior understanding
often rely on tracking [11,12,14,17,35] while those devoted to isolated action recognition relies more
on low-level features [21,23,25,27,36].
Although these methods could probably be tuned to index the video and facilitate search, very few
papers explicitly addressed that question. One such paper is the one by Wang et al. [19]. There method
decomposes the video into clips in which the local motion is quantized into words. These words are
than clustered into so-called topics such that each clip is modeled as a distribution over these topics.
Queries being a combination of these topics, their search algorithm fetches every clip containing all of
the topics mentioned in the query. A similar approach can be found in [9,15,37] But search techniques
focused on global explanations operate at a competitive disadvantage: the preponderance of clutter
(requirement four) in surveillance video makes the training step of scene understanding prohibitively
difﬁcult. Second, since these techniques often focus on understanding recurrent activities, they are
unsuited for retrieving infrequent events—this can be a problem, given that queries are unpredictable
(requirement two). Finally, the training step in scene understanding can be prohibitively expensive,
violating requirement three, large data lifetimes.
Other papers have been written on the topic of search for speciﬁc applications. For example, Stringa
and Regazzoni [38] describe a system to recover abandoned objects, Lee et al. [39] describe a user
interface to retrieve basic events such as the presence of a person, and Meesen et al. [40] presents an

538
CHAPTER 18 Activity Retrieval in Large Surveillance Videos
object-based dissimilarity measure to recover objects based on low-level features. Let us also mention
the work by Yang et al. [41] which stores human-body meta tags in a SQL table to humanoid shapes
based on their skin color, body size, height, etc.
In this chapter, we extract a full set of features as we have no a priori knowledge of what query will be
asked. Unlike scene understanding techniques, we have no training step; this would be incompatible with
the data lifetimes and magnitudes of the corpus. Instead, we develop an approach based on exploiting
temporal orders on simple features, which allows us to ﬁnd arbitrary queries quickly while maintaining
low false alarm rates. We demonstrate a substantial improvement over scene-understanding methods
such as [9,15] on a number of datasets in Section 4.18.5.
4.18.2 Feature extraction
4.18.2.1 Structure
For the purpose of feature extraction, a video is considered to be a spatio-temporal volume of size
H × W × F where H × W is the image size in pixels and F the total number of frames in the video.
The video is divided along the temporal axis into contiguous documents each containing A frames. As
shown in Figure 18.2, each frame is divided into tiles of size B × B. An atom is formed by grouping
the same tile over A frames. These values vary depending on the size of the video—for our videos, we
chose B equal to 8 or 16 pixels, and A equal to 15 or 30 frames, depending on frame rate. As the video
streams in, features are extracted from each frame. Whenever a document is created, each atom n is
assigned a set of features (see Section 4.18.2.3) describing the dynamic content over that region.
FIGURE 18.2
(Left) Given an W × H × F video, documents are non-overlapping video clips each containing A frames.
Each of the frames are divided into tiles of size B × B. Tiles form an atom when aggregated together over A
frames. (Right) Atoms are rouped into two-level trees—every adjacent set of four atoms is aggregated into a
parent, forming a set of partially overlapping trees.

4.18.2 Feature Extraction
539
4.18.2.2 Resolution
Choosing a suitable atom size (A and B) is an important issue. Small-sized atoms are sensitive to noise
while large-sized atoms aggregate features extracted from unrelated objects and smooth out object
details, which are important for precise retrieval. As a solution, we construct a pyramidal structure to
robustify our algorithm to location and size variability in detected features. Each element of the pyramid
has four children. This structure is made of k-level trees, each containing M = k
l=1 l2 nodes (Figure
18.2). In this approach, a document containing U × V atoms will be assigned (U −k +1)×(V −k +1)
partially overlapping trees to be indexed. For instance, in Figure 18.2, we draw a depth two tree on a
document which is 2 × 3 atoms, resulting in two overlapping trees, each containing M = 5 nodes.
Each node of a tree is assigned a feature vector obtained by aggregating the feature vector of its
children. Let n be a non-leaf node and a, b, c, d its four children. The aggregation process can be
formalized as
⃗x(i)
f
= ψ f

⃗x(a)
f , ⃗x(b)
f , ⃗x(c)
f , ⃗x(d)
f

,
where ψ f is an aggregation operator for feature f . Section 4.18.2.3 presents more details on the
aggregation operator. Given that several features are extracted for each atom, aggregating a group of
k × k atoms results in a set of feature trees {tree f }, one for each feature f . Given that a k-level tree
contains M nodes, each tree f contains a list of M feature instances, namely tree f =

⃗x(i)
f

where i
stands for the ith node in the tree.
4.18.2.3 Features
As reported in the literature [38,40,41], atom features can be of any kind such as color, object shape,
object motion, tracks, etc. We chose to use local processing due to the computational efﬁciency which
makes it better suited to the constant data renewal constraint and real-time feature extraction. Because
our focus is surveillance video, we assume a stable camera. To be effective on a moving or zooming
camera, motion and zoom compensation would have to be applied in advance of feature extraction.
Feature extraction computes a single value for each atom, which is aggregated into feature trees. Our
method uses the following ﬁve features:
1. Activity xa: Activity is detected using a basic background subtraction method [42]. The initial
background is estimated using a median of the ﬁrst 500 frames. Then, the background is updated
using the running average method. At the leaf level, xa contains the proportion of active pixels within
the atom. Aggregation for non-leaf nodes in feature trees, ψa, is the mean of the four children.
2. Object Size xs: Objects are detected using connected components analysis of the binary activity mask
obtained from background subtraction [42]. Object size is the total number of active pixels covered
by the connected component. The aggregation operator ψs for non-leaf nodes in feature trees is the
median of non-zero children. Whenever all four children have a zero object size, the aggregation
operator returns zero.
3. Color ⃗xc: Color is obtained by computing the quantized histogram over every active pixel in the atom.
RGB pixels are then converted to the HSL color space. Hue, saturation and luminance are quantized
into 8, 4, and 4 bins respectively. The aggregation operator ψa for non-leaf nodes in feature trees is
the bin-wise sum of histograms. In order to keep relative track of proportions during aggregation,
histograms are not normalized at this stage.

540
CHAPTER 18 Activity Retrieval in Large Surveillance Videos
4. Persistence x p: Persistence is a detector for newly static objects. It is computed by accumulating the
binary activity mask obtained from background subtraction over time. Objects that become idle for a
long periods of time thus get a large persistence measure. The aggregation operator ψp for non-leaf
nodes in feature trees is the maximum of the four children.
5. Motion ⃗xm: Motion vectors are extracted using Horn and Schunck’s optical ﬂow method [43]. Motion
is quantized into eight directions and an extra “idle” bin is used for ﬂow vectors with low magnitude.
⃗xm thus contains a 9-bin motion histogram. The aggregation operator ψm for non-leaf nodes in
feature trees is a bin-wise sum of histograms. In order to keep relative track of proportions during
aggregation, histograms are not normalized at this stage.
As mentioned previously, these motion features are extracted while the video streams in. When-
ever a document is created, its atoms are assigned ﬁve descriptors, namely

xa, xs, ⃗xc, x p, ⃗xm

. These
descriptors are then assembled to form the ﬁve feature trees {treea} , {trees} , {treec} ,

treep

, {treem}.
These feature trees are the basis for the indexing scheme presented in section 4.18.3. After feature trees
are indexed, all extracted feature content is discarded, ensuring a lightweight representation.
It is worth noting that these features are intentionally simple. This speeds up feature extraction and
indexing while being robust to small distortions because of the coarse nature of the features. While
motion can be sensitive to poorly-compensated camera motion or zoom, and color can be sensitive to
illumination changes, the other features have been shown to relatively robust to these effects [42]. In
addition, we leverage on the dynamic programming in Section 4.18.4.3.2 to limit false activity detection.
4.18.3 Indexing
When a document is created, features are aggregated into (U −k + 1) × (V −k + 1) k-level trees.
Each tree is made of ﬁve feature trees, namely {treea} , {trees} , {treec} ,

treep

, {treem}. To index a
given feature tree tree f efﬁciently, our method uses an inverted index for content retrieval. Inverted
index schemes, which map content to a location in a database, are popular in content-based retrieval
because they allow extremely fast lookup in very large document databases. For video, the goal is to
store the document number t and the spatial position (u, v) in the database based on the content of
tree f . This is done with a mapping function which converts “tree f ” to an entry in the database where
(t, u, v) is stored. Two trees with similar contents, therefore, should be mapped to proximate locations
in the index; by retrieving all entries which are near a query tree, we can recover the locations in the
video of all features trees that are similar.
This mapping and retrieval can be made for which update and lookup exhibit ﬂat performance (O(1)
complexity). Because similar content at different times is mapped to the same bin, the time required
to ﬁnd the indices of matching trees does not scale with the length of the video. Obviously, the total
retrieval time must scale linearly with the number of matching trees, but this means that the run-time
of the retrieval process scales only with the amount of data which is relevant. In videos where the
query represents an infrequently-performed action, this optimization yields an immense improvement
in runtime.

4.18.3 Indexing
541
4.18.3.1 Hashing
A hash-based inverted index uses a function to map content to an entry in the index. This is done with
a hashing function h such that h : tree f →j, where j is a hash table bucket number. Usually, hash
functions attempt to distribute content uniformly over the hash space by minimizing the probability of
collisions between two non-equal entries:
⃗x ̸= ⃗y ⇒P {h(⃗x) = h(⃗y)} ≈0.
However, in a motion feature space, descriptors for two similar events are never exactly equal.
Moreover, it is unlikely that a user query can be translated to feature vectors with sufﬁcient accuracy
for such a strict hash function.
As a solution, we resort to a locality-sensitive hashing (LSH) [44] technique. LSH is a technique
for approximation of nearest-neighbor search. In contrast to most hashing techniques, LSH attempts
to cluster similar vectors by maximizing the probability of collisions for descriptors within a certain
distance of each other:
⃗x ≈⃗y ⇒P {h(⃗x) = h(⃗y)} ≫0.
If feature trees are close in Euclidian distance (the element-wise square of the distances between node
values in the two trees is small), then the probability of them having the same hash code is high. Because
our feature trees contain M real-valued variables, LSH functions can be drawn from the p-stable family:
h⃗a,b,r(tree f ) =
	 ⃗a · tree f + b
r

,
where ⃗a is a M-dimensional vector with random components drawn from a stable distribution, b is a
random scalar drawn from a stable distribution and r is an application-dependent parameter. Intuitively,
⃗a represents a random projection, an alignment offset b and a radius r controlling the probability of
collision inside the radius.
Indices are built and searched independently for each feature. Thus, the database is made of ﬁve
indices I f , one for each feature f . Each index I f is composed of a set of n hash tables

T f ,i

, ∀i =
1, . . . , n. Each hash table is associated its own hash function H f ,i drawn from the p-stable family h⃗a,b,r.
The parameter r can be adjusted to relax or sharpen matches. In our implementation, r is ﬁxed for a
given feature.
Given a feature tree tree f with hash code H f ,i(tree f ) = j, T f ,i[ j, u, v] denotes the set of document
numbers {t} such that feature trees at (t, u, v) have similar content. Lookup in the index I f consists of
taking the union of document numbers returned by lookup in all tables

T f ,i

:
I(tree f , u, v) = ∪n
i=1T f ,i

H f ,i(tree f ), u, v

.
Figure 18.3 illustrates several feature trees partitioned into groups, where trees in the same group
have been given the same hashing key. For a given video, we plotted the content of four of the most
occupied buckets for the motion feature treem. As one can see, the trees associated to similar motion
patterns in various parts of the scene have been coherently hashed into similar buckets.

542
CHAPTER 18 Activity Retrieval in Large Surveillance Videos
FIGURE 18.3
Contents of four buckets of a hash table for the motion feature. Arrows size is proportional to the number
of hits at the speciﬁed location across the entire video. The hash buckets are associated to activity (a) side
walk (b) upper side of the street (c) lower side of the street (d) crosswalk.
4.18.3.2 Data structure
As reported previously, the inverted index stores in the same bucket the spatio-temporal position
{(t, u, v)} of all trees whose content is similar. As shown in Figure 18.4, each bucket is a (U −k + 1) ×
(V −k + 1) matrix (see Section 4.18.2.1) whose cells contain a list of document numbers {t}. As will
be explained later, queries are often related to a spatial position in the video. For example, a query such
as show me every object going to the left on the upper bound of the highway has a strong notion of
spatial position (the upper bound of the highway). In that way, having an (u, v) matrix lookup located
right after the hashing lookup ensures a retrieval time of O(1).
Absence of Activity: In order to minimize the storage requirements of the index I f , features are only
retained for areas of the video containing non-zero activity. During the indexing phase, if the activity
⃗xa is lower than a certain threshold T for all nodes in the tree, the tree is not indexed. This is a useful
feature for surveillance video, which can have persistently inactive spaces or times throughout a video.

4.18.3 Indexing
543
...
FIGURE 18.4
Hash table structure. For a given tree treef with Hf ,i(treef ) = j, lookup Tf ,i[j, u, v] is performed in two steps:
(1) fetch the bucket at position j and (2) fetch the list of document numbers at position (u, v) in the bucket.
Storage Requirements: In contrast to approaches relying on a distance metric, such as K-nearest
neighbor search, the hash-based index representation does not require storage of feature descriptors.
T f ,i contains only document numbers {t}, which are stored in 4-byte variables. As such, both our
indexing times and storage scale linearly with the amount of indexable content. Assuming the amount
of indexable content in the video is a proportion α of the video length, the size (in bytes) of the hashing
table can be estimated using:
size(T f ,i) = (U −k) × (V −k) × F
A × 4 × α,
where (U −k)×(V −k)× L
A corresponds to the total number of trees in the video and α is the proportion
of trees that contain enough activity to be indexed. For example, a typical setup calls for the settings
B = 16, A = 30, H = 240, W = 320, k = 2, an activity rate α = 2.5%, and a total of ﬁve features,
each using three tables. The total size of the index for a 5 h video requires only 5 Mb while the input
color video requires almost 7 Gb (compressed).
4.18.3.3 Building the lookup table
As video streams in, the index for each feature is updated by a simple and fast procedure. The updating
procedure is simple and fast. The index is updated online and does not require to be re-built when new
data arrives. Thus, the index is guaranteed to be up to date and search is ready to proceed at any time.
This is an important property for surveillance videos for which data is constantly renewed. Algorithm
1 contains pseudo-code for the update procedure.
After extraction of feature f for document t is completed, the extracted features are grouped into
trees, as described in Section 4.18.2. Then, I f is updated with the mapping tree f →(t, u, v) for each
tree position (u, v) covering an area with signiﬁcant activity. This is repeated for each feature f.

544
CHAPTER 18 Activity Retrieval in Large Surveillance Videos
Algorithm 1. Updating the LSH-based Index
1:
procedure UpdateIndex(I, t)
2:
for each feature f do
3:
{atom f } ←features extracted from document t
4:

tree f

←k-level trees for

atom f

5:
for each tree (u, v) do
6:
for each table i do
7:
if treea ≥T then
8:
h ←H f ,i(tree f at (u, v))
9:
T f ,i[h, u, v] ←T f ,i[h, u, v] ∪t
10:
end if
11:
end for
12:
end for
13:
end for
14:
end procedure
4.18.4 Search engine
We showed in Section 4.18.3 how to extract low-level features from a video sequence, bundle them into
trees and index them for O(1) content-based retrieval. Here we explain how to use feature index for
high-level search.
4.18.4.1 Queries
In video search without exemplars, it’s essential to provide a straightforward way for a user to input a
query. For our purposes, a query is deﬁned as an ordered set of action components (for simple queries,
a single component frequently sufﬁces), each of which is a set of feature trees. To create a query, the
user types in the number of action components, and is then presented with a GUI, shown in Figure 18.5
containing the ﬁrst frame of the video to search for each of the action components. The user then selects
the type of feature he wishes to search for, and draws the region of interest (ROI) that he wishes to ﬁnd
it in. These regions and the features (directions of motion, in this case) are shown in Figure 18.8 as
green areas and red arrows, respectively.
As an example, to describe a U-turn, the user might describe three action components: one detecting
motionapproachinganintersection,onedetectingmotionturningaround,andonedetectingmotionleav-
ing the intersection. Likewise, a man hopping a subway might be represented by somebody approaching
the subway, jumping up, and then continuing past the subway.
Because our features (activity, size, color, persistence, and motion) are semantically meaningful,
this method of input is a relatively accessible way to deﬁne a query. After the features and the ROI
are selected, a query feature tree tree f is created to represent that action component. Note that this

4.18.4 Search Engine
545
FIGURE 18.5
The query creation GUI provides a straightforward way to construct queries. The user draws each action
component (shown in blue), and can additionally specify features. (For interpretation of the references to
color in this ﬁgure legend, the reader is referred to the web version of this book.)
formulation provides the user with a way to produce a complex query vocabulary. A single component
could describe a search for “small red stationary objects” or “large objects moving to the right.” While
our claims to simplicity are theoretical, they are also supported by anecdotal evidence. People unfamiliar
with the system are able to create their own queries in under a minute after a brief explanation of the
tools.
4.18.4.2 Partial matches
A query feature tree q = tree f provided by the GUI can be used as a simple lookup index. The resulting
spatio temporal positions (t, u, v) are called partial matches. Lookup of partial matches in a single
feature index I f is expressed as:
L I f (q) = ∪(u,v) ∈ROI{(t, u, v), ∀t ∈I f (q, u, v)}.
A richer query language can be made using compound queries. A compound search use more than
one lookup which allows to express queries such as “ﬁnd small red or green objects” or “ﬁnd large
stationary objects.” The query Q “ﬁnd small red or green objects” is a combination of three single
queries: q1 (small objects), q2 (red objects) and q3 (green objects). Given the color index Ic, the object
size index Is, and a compound query Q, the set of partial matches M(Q) can be expressed as:
M(Q) = L Is(q1) ∩

L Ic(q2) ∪L Ic(q3)

.

546
CHAPTER 18 Activity Retrieval in Large Surveillance Videos
This query language can express arbitrarily large queries, provided that the query logic can be
expressed using lookups and standard operators: union, intersection, and set difference.
These feature trees is used to query the inverted index of Section 4.18.3 to produce a set of documents
and locations called partial matches M(q) that contained similar trees to query q. In the case where the
query contains multiple feature trees, the set of matching locations is the intersection of the matches for
individual trees. Figure 18.8 presents 10 different queries with their ROI.
4.18.4.3 Full matches
Search in a surveillance video requires more than partial matches. Activities in a video are inherently
complex and show signiﬁcant variability in time duration. For instance, a fast car taking a U-turn will
span across fewer documents and generate different motion features than a slow moving car. Also, due
to the limited size of a document (typically between 30 and 100 frames), partial matches may only
correspond to a portion of the requested event. For example, partial matches in a document t may only
correspond to the beginning of a U-turn. The results expected by a user are so-called full maches, i.e.,
video segments [t, t + ] containing one or more documents ( > 0). For example, the video segment
R = {t, t + 1, t + 2} corresponds to a full U-turn match when documents t, t + 1, t + 2 contain the
beginning, the middle and the end of the U-turn. Given a query q and a set of partial matches M(q), a
full match starting at time τ is deﬁned as
Rq,τ() = {(u, v)|(t, u, v) ∈M(q), ∀t ∈[τ, τ + ]} .
(18.1)
Thus, Rq,τ() contains the set of distinct coordinates of trees partially matching q in the video
segment [τ, τ + ].
We propose two algorithms to identify these full matches from the set of partial matches. The ﬁrst is
a greedy optimization procedure based on the total number of partial matches in a document that does
not exploit the temporal ordering of a query. The second approach (Section 4.18.4.3.2), uses dynamic
programming to exploit temporal structure of the of a query’s action components.
Algorithm 2. Greedy search algorithm
1:
procedure Search (Q)
2:
r ←∅
3:
τ ←1
4:
while τ < number of documents do
5:
∗←arg max>0 vQ,τ()
6:
r ←r ∪RQ,τ(∗)
7:
τ ←τ + ∗
8:
end while
9:
return r
9:
end procedure

4.18.4 Search Engine
547
4.18.4.3.1
Full matches using a greedy algorithm
The main difﬁculty in identifying full matches comes with the inherent variability between the query
and the target. This includes time-scaling, false detections and other local spatio-temporal deformations.
Consequently, we are faced with the problem of ﬁnding which documents to fuse into a full match given
a set of partial matches. We formulate it in terms of the following optimization problem:
∗= arg max
>0 vq,τ(),
(18.2)
where q is the query, τ is a starting point and  the length of the retrieved video segment. The value
function vq,τ() maps the set of partial matches in the interval [τ, τ + ] to some large number when
the partial matches ﬁt q well and to a small value when they do not. To determine the optimal length of
a video segment starting at time τ we maximize the above expression over .
While many value functions are viable, depending upon user preference, a simple and effective
vq,τ() is:
vq,τ() = |Rq,τ()| −λτ,
(18.3)
where Rq,τ() is deﬁned by Eq. (18.1) and |Rq,τ()| is the total number of distinct matching locations
found in the interval [τ, τ + ]. The value function is time-discounted since Rq,τ() is increasing in
 (by deﬁnition, Rq,τ() ⊆Rq,τ( + 1)). The parameter λ is a time-scale parameter and loosely
controls size of retrieved video segment.
We can determine  by a simple and fast greedy algorithm. The algorithm ﬁnds a set of non-
overlapping video segments and a natural ranking based the value function provided above. As will be
shown in Section 4.18.5, Eq. (18.3) produces compact video segments while keeping low false positives
and negatives rates.
It may seem strange that such a simple value function provides accurate results over a wide range
of queries and videos. Intuitively, this is because the more complex a query is, the less likely it is to be
generated by unassociated actions. We state this formally in Theorem 1.
Theorem 1.
Suppose that we have a random video: we sample independently across time and at
each instant uniformly from the set of all trees with replacement. Suppose the query q consists of
|q| distinct trees and the random video has Rq,τ() matches. For  = γ |q| the probability that
log (vq,τ()) ≥α|q| for some α ∈(0, 1) is smaller than O(1/|q|2).
This result suggests that the false alarm probability resulting from an algorithm that is based on
thresholding vq,τ() is small. This result is relevant because we expect log (vq,τ()) for video segments
that match the query to have a value larger than α|q| for some α when  = 	(|q|).
Proof.
For simplicity we only consider each document to contain a single random tree drawn from
among |H| trees. The general result for a ﬁxed number of trees follows in an identical manner. We
compute the value of random video of length τ, i.e.,
P

vq,0() ≥exp (α|q|)

= P

|Rq,0()| ≥α|q| + γ

,
where we substituted 
λ = γ and taken logarithms on both sides to obtain the second equation. Let,
 j be the number of documents before we see a new document among the set q after just having seen

548
CHAPTER 18 Activity Retrieval in Large Surveillance Videos
the j −1th new document. This corresponds to the inter-arrival time between the ( j −1)th and jth
document. Given our single tree random model we note that Rq,0() is a counting process in  and so
we have the following equivalence,
Rq,0() ≥ℓ⇐⇒
ℓ

j=1
 j ≤.
Thus we are now left to determine the P
ℓ
j=1  j ≤

where ℓ= α|q| + γ . Next, 1, 2, . . .
are independent geometric random variables. The jth random variable  j has a geometric distribution
with parameter pi = |q|−j
|H| . Using these facts we can determine the mean value and variance of the sum
using linearity of expectations. Speciﬁcally, it turns out that
E
⎛
⎝
ℓ

j=1
 j
⎞
⎠=
ℓ

j=1
1
pi
;
Var
⎛
⎝
ℓ

j=1
 j
⎞
⎠=
ℓ

j=1
1 −pi
p2
i
.
Upon computation the mean turns out to be O(|H|), while the variance turns out to be O(|H|2/|q|2).
We now apply Chebyshev inequality to conclude P

vq,0() ≥exp (α|q|)

≤O(1/|q|2), which estab-
lishes the result.
4.18.4.3.2
Full matches with dynamic programming (DP)
We can improve upon the performance of the greedy algorithm in Section 4.18.4.3.1 by exploiting the
order of the action components. For example, in the example of a man hopping a subway turnstile,
he has to approach the turnstile from the wrong direction, hop over it, and continue. For a car taking
a U-turn, it has to approach the intersection, turn across it, and depart the way it came. While these
component actions have many valid conﬁgurations when examined independently, there is only one
order in which they represent the desired full action. This is illustrated in Figure 18.6.
In the greedy optimization of Section 4.18.4.3.1, we ignore the time-ordering that a set of queries
contains. The value function Rq,τ() does not differentiate between the sequences of actions (Forward,
Left, Back) and the sequence of actions (Back, Forward, Left). Intuitively, this should hurt performance—
false matches are more likely to appear than were we to exploit this causality.
After a user uses the GUI described in Section 4.18.4.1 to create a query containing a set of N action
components, we search through the inverted index to retrieve N sets of matching locations within the
video, one for each action component.
After this pre-processing step, we adapt the Smith-Waterman dynamic programming algorithm [45],
originally developed for gene sequencing, to determine the best set of partial matches in the query. Our
algorithm, described in Algorithm 3, operates over the set of matches mτ,α which contains matches in
document α to action component τ to recover a query that has been distorted. For the purposes of our
videos, we consider three types of distortion, namely insertion, deletion, and continuation.
1. Insertion covers documents where we believe the query is happening but there are no partial matches.
This can happen if unrelated documents are inserted, if there is a pause in the execution of the activity,
or if the activity is obscured.

4.18.4 Search Engine
549
FIGURE 18.6
Three sequences of actions. All have equivalent values of RQ,τ(); only the third row is valid U-turn.
2. Deletion covers documents where sections of the query are missing. If a deletion has occurred, one
(or more) of the action components in the query will not be present in the video. Deletions can
happen because of obscuration, or simply because somebody does not perform one component of
the full action.
3. Continuation covers documents where we continue to see an action component which we have
already seen. This is important because of time distortion; a single action component does not
necessarily occur in multiple consecutive documents before the next action component is reached.
As described in Algorithm 3, to search for a query with |q| action components in a video with
N documents, our dynamic programming approach creates an N × |q| matrix, V, which is ﬁlled out
from the top left to the bottom right. A path through the matrix is deﬁned as a set of adjacent (by an
8-neighborhood) matrix element, where each element of the path represents a hypothetical assignment
of an action component taking place in a document. A path containing element Va,b would indicate that
path believed that action component b occured in document a.
As the matrix is ﬁlled out, each element chooses to append itself to the best possible preceding
path—which, by deﬁnition, ends immediately above it, immediately to the left of it, or both. It stores
the resulting value, and a pointer to the preceding element, in the value matrix V. When the matrix
is fully ﬁlled out, the optimal path can be found by starting at the maximal value in the matrix and
tracing backwards. In order to ﬁnd multiple matches, we repeatedly ﬁnd the maximal value in V,
the optimal path associated with it, set the values along that path to zero, and repeat until the maxi-
mum value in V is below a threshold T . An example of this matrix, with paths overlaid, is shown in
Figure 18.7.

550
CHAPTER 18 Activity Retrieval in Large Surveillance Videos
Algorithm 3. Dynamic programming (DP) algorithm
1:
procedure Search (m, W, T)
2:
V ←0; paths ←∅; τ ←1; α ←1
3:
while τ ≤number of documents do
4:
while α ≤number of action components do
5:
Vτ,α ←max
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
0
(Vτ−1,α−1 + Wmatch) ∗mτ,α
(Vτ−1,α + Wcont) ∗mτ,α
(Vτ−1,α + Wdelete) ∗(1 −mτ,α)
(Vτ,α−1 + Winsert) ∗(1 −mτ,α)
6:
Let (a, b) be the index which was used to generate the maximum value
7:
if V(τ,α) > 0 then
8:
pathsτ,α ←pathsa,b ∪(τ, α)
9:
else
10:
pathsτ,α ←pathsa,b
11:
end if
12:
α ←α + 1
13:
end while
14:
τ ←τ + 1
15:
end while
16:
Matches ←∅
17:
while max (V ) > T do
18:
Let (a, b) be the index of V containing the maximum value
19:
Matches ←Matches ∪pathsa,b
20:
for τ, α ∈pathsa,b do
21:
Vτ,α = 0
22:
end for
23:
end while
24:
Return Matches, the set of paths above threshold T
25:
end procedure
For a given penalty on each type of distortion WI, WD, WC (corresponding to insertion, deletion,
and continuation) and a given bonus for each match, WM, the DP algorithm (Algorithm 3) is guaranteed
to ﬁnd the set of partial matches which maximizes the sum of the penalties and bonuses over an interval
of time. For our queries, we were relatively certain that elements of the query would not be obscured,
but we were uncertain about our detection rate on features and how long an event would take. Thus,
we set WI = −2, WD = −10, WC = 1, and WM = 8. These values preclude optimal solutions which
involve deletions, look for longer sequences that match, and are relatively robust to missed detection.
We note that because it reasons over speciﬁc partial matches, our dynamic programming approach
also ﬁnds the locations in the video segments where the event occurs, but this is not exploited in the
results of this paper.

4.18.5 Experimental Results
551
FIGURE 18.7
An example of the V matrix. The query is actions A, C, A, T, and the seven documents in the video corpus
each contains a single action, T, A, A, C, A, G, T. The values for WI, WD, WC, and WM are −1, −2, 1and 3
respectively. The optimal path, A, A, C, A, G, T, involves an insertion, a continuation, and a deletion. It is
found by tracing backwards from the maximal element, valued at 11.
4.18.5 Experimental results
4.18.5.1 Datasets
In order to evaluate performance of the two-step problem formulation, and the DP approach in particular,
we initially tested our two-step approach on seven surveillance videos (see Table 18.1 and Figure 18.8).
These videos were selected to test the application of this basic approach to multiple domains, as well
as to provide a basis for comparison to other algorithms. The Winter driveway, U-turn and Abandoned
object sequences were shot by us, PETS and Parked-vehicle and MIT-trafﬁc come from known databases
[46,47], MIT-trafﬁc was made available to us by Wang et al. [48]; Subway from Adam et al. [49]. Note
that every video is stored independently.
As listed in Table 18.1, we tested different queries to recover moving objects based on their color,
size, direction, activity, and persistence. We queried for rare and sometimes anomalous events (cat in
the snow, illegal U-turns, abandoned objects and people passing turnstile in reverse) as well as usual
events (pedestrian counting, car turning at a street light, and car parking). Some videos featured events
at a distance MIT-trafﬁc, while others featured people moving close to the camera Subway. We searched
for objects, animals, people, and vehicles. Given these queries, we watched the videos and created a
ground-truth list for each task.
4.18.5.2 Comparison with HDP-based video search
For the purposes of comparison, we employed high-level search functions based on scene understanding
techniques using Hierarchical Dirichelet Processes (HDP) [9,15]. We chose this, because unlike [17,31],
it does not require tracking, which can be difﬁcult to do in complex scenes and is computationally
prohibitive. At each iteration, the HDP-based learning algorithm assigns each document to one or more
high-level activities. This classiﬁcation is used as input to the next training iteration. Xiang and Gong
[15] propose a search algorithm that uses learned topics as high-level semantic queries. The search

552
CHAPTER 18 Activity Retrieval in Large Surveillance Videos
Table 18.1 Tasks’ Number, Videos, Search Query, Associate Features, Video Size, and Index Size. Tasks 1, 8, 9, 10,
and 11 Use Compound Search Operators. The Index Size can be Several Orders of Magnitude Smaller than Raw Video.
Our Use of Primitive Local Features Implies that Index Times and Index Size are Both Proportional to the Number of
Foreground Objects in the Video. Consequently, Index Size Tends to be a Good Surrogate for Indexing Times
Task
Video
Search query
Features
Video size
Index size
1
Winter driveway
Black cat appearance
Color and size
6.55 GB
147 KB
2
Subway
People passing turnstiles
Motion
2.75 GB
2.3 MB
3
Subway
People hopping turnstiles
Motion
2.75 GB
2.3 MB
4
MIT Trafﬁc
Cars turning left
Motion
10.3 GB
42 MB
5
MIT Trafﬁc
Cars turning right
Motion
10.3 GB
42 MB
6
U-turn
Cars making U-turn
Motion
1.97 GB
13.7 MB
7
U-turn
Cars turning left, no U
Direction
1.97 GB
13.7 MB
8
Abandoned object
Abandoned objects
Size and persistence
682 MB
2.6 MB
9
Abandoned object
Abandoned objects
Size, persistence, and color
682 MB
2.6 MB
10
PETS
Abandoned objects
Size and persistence
1.01 GB
5.63 KB
11
Parked-vehicle
Parked vehicles
Size and persistence

4.18.5 Experimental Results
553
FIGURE 18.8
Screen shots of the 10 tasks. These images show the search queries (with green ROI) and a retrieve frame
(with a red rectangle). The red dots correspond to the tree whose proﬁle ﬁt the query. (For interpretation of
the references to color in this ﬁgure legend, the reader is referred to the web version of this book.)
algorithm is based on the classiﬁcation outputs from the ﬁnal HDP training iteration. We compare our
method to this HDP-based search algorithm.
Queries are speciﬁed as the ideal classiﬁcation distribution and the search algorithm compares each
document’s distribution over the learned topics against this ideal distribution. Comparison is performed
using the relative entropy (Kullback-Leibler divergence) between the two distributions. The Kullback-
Leibler divergence gives a measure of distance between the query q and the distribution p j for document
j over the K topics:
D(q, p j) =
K

k=1
q(k) log q(k)
p j(k).
The query q is created by looking at the ideal documents and assigning to q a uniform distribution over
the topics present in them. The search procedure evaluates D(q, p j) for each document j and ranks the
documents in order of increasing divergence.

554
CHAPTER 18 Activity Retrieval in Large Surveillance Videos
4.18.5.3 Eleven tasks
Once we had deﬁned the queries, we watched all of the videos and created a ground-truth list for each
task. Ground truth was obtained manually by noting the range of frames containing each of the expected
results. Comparison is obtained by computing the intersection of the ranges of frames returned by the
search procedure to the range of frames in the ground truth. Events are counted manually by viewing
the output results. An event is marked as detected if it appears in the output video and at least one partial
match hits objects appearing in the event.
The greedy-optimization results are displayed in Figure 18.8 and summarized in Table 18.2. The
comparable results for HDP are also summarized in Table 18.2. The “Duration” column of Table 18.2
indicates video length (in minute). The “Ground truth” column indicates the true number of events
which exist in the dataset. The “Greedy True” column indicates the number of correct detections (true
positives) for the Greedy algorithm and “HDP True” the number of correct detections (true positives)
for the HDP-based search [19]. Likewise, the “Greedy False” and “HDP False” indicate the number
of false alarms that were found for those 11 tasks. The “Runtime” column shows the amount of time
required by the greedy method to perform search. The reader shall note that our current implementation
is in Matlab.
Table 18.2 demonstrates the robustness of the two-step method in a wide-array of search applications,
outperforming the HDP baseline signiﬁcantly in detection and false alarm rate. The ﬁgures in the table
are given for results of total length approximately equal to that of the ground truth. As can be seen from
the ﬁgures in the table, the absolute detection rate is strong.
In Table 18.2, we learn that HDP-search deals well with search of recurring large-scale activities
and poorly otherwise. While several queries could not be executed because of a lack of topics that
could be used to model the query, the results nonetheless demonstrate some of the shortcomings of
the algorithm. The HDP search scales linearly with the number of documents, an undesirable quality
with large datasets. Further, the cost of the training phase is prohibitive (approximately 2 days for the
“subway” sequence) and must be paid again every time thatmore video data is included.
4.18.5.4 Dynamic programming
Of the 11 tasks described in Table 18.1, only tasks 2–7 had temporal structure which could be exploited
through dynamic programming. As it turned out, the features used in these tasks were purely motion.
In order to demonstrate the potential gain from exploiting this structure, we chose tasks 3, 4, and 6 and
performed dynamic programming using the full query, as well as greedy and HDP search algorithms.
The ROC curves for those three scenarios are provided in Figure 18.9, contrasting dynamic programming
with HDP and greedy optimization.
Figure 18.9 demonstrates the type of improvement that can be attained by the two-step approach to
search. The ROC curves for LSH-based greedy optimization dominate the HDP curves, and there is
clear improvement from employing time-ordering with DP. These improvements come as no surprise—
HDP is doing a global search, attempting to create a topic for each action. LSH does a compelling
local search which is fast and produces low false alarm rate. This is largely due to the global nature of
HDP—the topics it discovers are more likely to be common events, so infrequent events such as U-turns
and turnstile-hopping pose a problem. Note that the gap between local and global searches narrow on the

4.18.5 Experimental Results
555
Table 18.2 Results for the 11 Tasks Using the Greedy Optimization and HDP Search. Crossed-Out Rows Correspond to
Queries for Which There was No Corresponding Topic in the HDP Search [9]
Task
Video
Duration
Ground truth
Greedy true
HDP true
Greedy false
HDP false
Runtime (s)
(min)
(events)
(events found)
(events found)
(events found)
(events found)
1
Winter driveway
253
3
2
–
1
–
7.5
2
Subway
79
117
116
114
1
121
0.3
3
Subway
79
13
11
1
2
33
3.0
4
MIT Trafﬁc
92
66
61
6
5
58
0.4
5
MIT Trafﬁc
92
148
135
54
13
118
0.5
6
U-turn
3.4
8
8
6
0
23
1.2
7
U-turn
3.4
6
5
4
1
14
0.6
8
Abandoned object 13.8
2
2
–
0
–
4.8
9
Abandoned object 13.8
2
2
–
0
–
13.3
10
PETS
7.1
4
4
–
0
–
20.2
11
Parked-vehicle
32
14
14
–
0
–
12.3

556
CHAPTER 18 Activity Retrieval in Large Surveillance Videos
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
false positive rate
true positive rate
LSH + DP
LSH
HDP
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
false positive rate
true positive rate
LSH + DP
LSH
HDP
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
false positive rate
true positive rate
LSH + DP
LSH
HDP
FIGURE 18.9
ROC curves for the U-turn, subway and trafﬁc datasets. Both our greedy search and DP signiﬁcantly outper-
form scene-understanding methods such as HDP methods [9,15].
MIT-trafﬁc dataset, where the event being found (left turns at a light) is a relatively common occurrence
with enough repetition to ﬁll a topic model.
4.18.5.5 Discussion
This approach represents a fundamentally different way of approaching the video search problem.
Rather than relying on an abundance of training data or ﬁnely-tuned features to differentiate actions of
interest from noise, we rely on simple features and causality. In addition to the clear beneﬁts in terms of
a run-time which scales sub-linearly with the length of the video corpus, the simple features and hashing
approach render the approach robust to user error as well as poor-quality video. The results of Section
4.18.5.4 demonstrate clearly that causality and temporal structure can be powerful tools to reduce false
alarms. Another added beneﬁt is how the algorithm scales with query complexity. Whereas algorithms
such as topic modeling or a feature-based matching suffer as queries becomes more complex due to
efforts to characterize the query, the two-step approach becomes more successful—the more action
components in a query, the more likely it is to differentiate itself from noise. There is, of course, non-
temporal structure that we have yet to exploit. Spatial positioning of queries, such as “The second action
component must occur to the northeast of the ﬁrst,” or “The second action component must be near the
ﬁrst” is a simple attribute which may further differentiate queries of interest from background noise.
This is not to say that the approach is not without its limitations. It requires that the activity being
described contain discrete states, each of which is describable by a simple feature vocabulary. Complex
actions like sign language or actions which are to fast or too small to be identiﬁed at the atom level will
be difﬁcult to search for.
4.18.6 Conclusion
We presented a method that summarizes the dynamic content of a surveillance video in a way that is
compatible with arbitrary user-deﬁned queries. We divide the video into documents each containing a
series of atoms. These atoms are grouped together into trees all containing a feature list. These features

References
557
describing the size, the color, the direction, and the persistence of the moving objects. The coordinates
of these trees are then stored into a hash-table based feature index. Hash functions group trees whose
content is similar. In this way, search becomes a simple lookup as user-speciﬁed queries are converted
into a table index. Our method has many advantages. First, because of the indexing strategy, our search
engine has a complexity of O(1) for ﬁnding partial matches. Second, the index requires only minimal
storage. Third, our method requires only local processing, making it suitable for facing constant data
renewal. This makes it simple to implement and easy to adjust. Fourth, our method summarizes the entire
video, not just the predominant modes of activity: it can retrieve any combination of rare, abnormal,
and recurrent activities.
References
[1] R.W. White, R.A. Roth, Exploratory Search: Beyond the Query-Response Paradigm, Morgan-Claypool, VT,
USA, 2009.
[2] G. Castanon, V. Saligrama, A. Caron, P.M. Jodoin, Exploratory search of long surveillance videos, in: Pro-
ceedings of ACM Multimedia, 2012.
[3] Nbcnews, <msnbc.msn.com/id/44163852/ns/business-usbusiness/t/post-surveillance-cameras-everywhere/>.
[4] A. Doulamis, N. Doulamis, Optimal content-based video decomposition for interactive video navigation,
IEEE Trans. Circ. Syst. Video Technol. 14 (6) (2004) 757–775.
[5] S. Shipman, A. Divakaran, M. Flynn, Highlight scene detection and video summarization for PVR-enabled
high-deﬁnition television systems, in: Proceedings of IEEE International Conference Consumer Electronics,
2007, pp. 1–2.
[6] J. Sivic, A. Zisserman, Video Google: a text retrieval approach to object matching in videos, in: Proceedings
of the IEEE International Conference on Computer Vision, vol. 2, 2003, pp. 1470–1477.
[7] X. Song, G. Fan, Joint key-frame extraction and object segmentation for content-based video analysis, IEEE
Trans. Circ. Syst. Video Technol. 16 (7) (2006) 904–914.
[8] X. Zhu, X. Wu, A. Elmagarmid, Z. Feng, L. Wu, Video data mining: semantic indexing and event detection
from the association perspective, IEEE Trans. Know. Data Eng. 17 (2005) 665–677.
[9] D. Kuettel, M. Breitenstein, L. Gool, V. Ferrari, What’s going on? discovering spatio-temporal dependencies
in dynamic scenes, in: Proceeding of IEEE International Conferene Computer Vision Pattern Recognition,
2010, pp. 1951–1958.
[10] N.M. Oliver, B. Rosario, A.P. Pentland, A Bayesian computer vision system for modeling human interactions,
IEEE Trans. Pattern Anal. Machine Intell. 22 (8) (2000) 831–843.
[11] I. Pruteanu-Malinici, L. Carin, Inﬁnite hidden Markov models for unusual-event detection in video, IEEE
Trans. Image Process. 17 (5) (2008) 811–821.
[12] N. Vaswani, A. Roy-Chowdhury, R. Chellappa, Shape activity: a continuous state HMM for moving/deforming
shapes with application to abnormal activity detection, IEEE Trans. Image Process. 14 (10) (2005) 1603–1616.
[13] D. Zhang, D. Gatica-Perez, S. Bengio, I. McCowan, Semi-supervised adapted HMMs for unusual event
detection, in: Proceedings of IEEE Conference Computer Vision Pattern Recognition, 2005, pp. 611–618.
[14] S. Calderara, R. Cucchiara, A. Prati, A distributed outdoor video surveillance system for detection of abnormal
people trajectories, in: Proceedings IEEE Conference on Distributed Smart Cameras, 2007, pp. 364–371.
[15] T. Xiang, S. Gong, Video behavior proﬁling for anomaly detection, IEEE Trans. Pattern Anal. Machine Intell.
30 (5) (2008) 893–908.
[16] H. Veeraraghavan, N. Papanikolopoulos, P. Schrater, Learning dynamic event descriptions in image sequences,
in: Proceedings of IEEE Conference on Computer Vision Pattern Recognition, pp. 1–6, 2007.

558
CHAPTER 18 Activity Retrieval in Large Surveillance Videos
[17] G. Medioni, I. Cohen, F. Bremond, S. Hongeng, R. Nevatia, Event detection and analysis from video streams,
IEEE Trans. Pattern Anal. Machine Intell. 23 (8) (2001) 873–889.
[18] C. Simon, J. Meessen, C, DeVleeschouwer, Visual event recognition using decision trees, Multimed. Tools
Appl. 50 (1) (2010) 95–121, Kluwer Academic Publishers Hingham, MA, USA.
[19] X. Wang, X. Ma, E. Grimson, Unsupervised activity perception in crowded and complicated scenes using
hierarchical bayesian models, IEEE Trans. Pattern Anal. Machine Intell. 31 (3) (2009) 539–555.
[20] M. Bennewitz, G. Cielniak, W. Burgard, Utilizing learned motion patterns to robustly track persons, in:
Proceedings of IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking
and Surveillance, 2003, pp. 1–8.
[21] A.F. Bobick, J.W. Davis, The recognition of human movement using temporal templates, IEEE Trans. Pattern
Anal. Machine Intell. 23 (3) (2001) 257–267.
[22] R. Cutler, L.S. Davis, Robust real-time periodic motion detection, analysis, and applications, IEEE Trans.
Pattern Anal. Machine Intell. 22 (8) (2000) 781–796.
[23] L. Gorelick, M. Blank, E. Shechtman, M. Irani, R. Basri, Actions as space-time shapes, IEEE Trans. Pattern
Anal. Machine Intell. 29 (12) (2007) 2247–2253.
[24] Y.A. Ivanov, A.F. Bobick, Recognition of visual activities and interactions by stochastic parsing, IEEE Trans.
Pattern Anal. Machine Intell. 22 (8) (2000) 852–872.
[25] E. Shechtman, M. Irani, Space-time behavior based correlation or how to tell if two underlying motion ﬁelds
are similar without computing them? IEEE Trans. Pattern Anal. Machine Intell. 29 (11) (2007) 2045–2056.
[26] C. Yeo, P. Ahammad, K. Ramchandran, S. Sastr, High speed action recognition and localization in compressed
domain videos, IEEE Trans. Circ. Syst. Video Technol. 18 (8) (2008) 1006–1015.
[27] A. Yilmaz, M. Shah, A differential geometric approach to representing the human actions, Comput. Vis. Image
Und. 109 (3) (2008) 335–351.
[28] Y.-L. Tian, A. Hampapur, L. Brown, R. Feris, M. Lu, A. Senior, C.-F. Shu, Y. Zhai, Event detection, query,
and retrieval for video surveillance, in: Z. Ma (Ed.), Artiﬁcial Intelligence for Maximizing Content Based
Image Retrieval, ﬁrst ed., Information Science Reference, 2008.
[29] A. Basharat, A. Gritai, M. Shah, Learning object motion patterns for anomaly detection and improved object
detection, in: Proceedings of the IEEE Conference Computer Vision, Pattern Recognition, 2008, pp. 1–8.
[30] C. Chuang, J.-W. Hsieh, K.-C. Fan, Suspicious object detection and robbery event analysis, in: ICCCN, 2007,
pp. 1189–1192.
[31] W. Hu, X. Xiao, Z. Fu, D. Xie, T. Tan, S. Maybank, A system for learning statistical motion patterns, IEEE
Trans. Pattern Anal. Machine Intell. 28 (9) (2006) 1450–1464.
[32] C. Piciarelli, C. Micheloni, G. Foresti, Trajectory-based anomalous event detection, IEEE Trans. Circ. Syst.
Video Technol. 18 (11) (2008) 1544–1554.
[33] I. Saleemi, K. Shaﬁque, M. Shah, Probabilistic modeling of scene dynamics for applications in visual surveil-
lance, IEEE Trans. Pattern Anal. Machine Intell. 31 (8) (2009) 1472–1485.
[34] V. Saligrama, J. Konrad, P.-M. Jodoin, Video anomaly identiﬁcation: a statistical approach, IEEE Signal
Process. Mag. 27 (2010) 18–33.
[35] X. Wang, K. Tieu, E. Grimson, Learning semantic scene models by trajectory analysis, in: Proceedings of
European Conference Computer Vision, 2006, pp. 111–123.
[36] Q. Dong, Y. Wu, Z. Hu, Pointwise motion image (PMI): a novel motion representation and its applications
to abnormality detection and behavior recognition, IEEE Trans. Circ. Syst. Video Technol. 19 (3) (2009)
407–416.
[37] T. Hospedales, S. Gong, T. Xiang, A markov clustering topic model for mining behaviour in video, in:
Proceeding of IEEE International Conferene Computer Vision, 2009, pp. 1165–1172.

References
559
[38] E. Stringa, C. Regazzoni, Content-based retrieval and real time detection from video sequences acquired
by surveillance systems, in: Proceedings of the International Conference on Image Processing, 1998, pp.
138–142.
[39] H. Lee, A. Smeaton, N. O’Connor, N. Murphy, User-interface to a CCTV video search system, in: International
Symposium on Imaging for Crime Detection and Prevention, 2005, pp. 39–43,.
[40] J. Meessen, M. Coulanges, X. Desurmont, J.-F. Delaigle, Content-based retrieval of video surveillance scenes,
in: MRCS, 2006, pp. 785–792.
[41] Y. Yang, B. Lovell, F. Dadgostar, Content-based video retrieval (cbvr) system for CCTV surveillance videos,
in: Proceedings of Digital Image Processing Computer Technology and Application, 2009, pp. 183–187.
[42] Y.Benezeth,P.-M.Jodoin,B.Emile,H.Laurent,C.Rosenberger,Comparativestudyofbackgroundsubtraction
algorithms, J. Electron. Imaging 19 (3) (2010) 1–12.
[43] B. Horn, B. Schunck, Determining optical ﬂow, Artif. Intell. 17 (1–3) (1981) 185–203.
[44] A. Gionis, P. Indyk, R. Motwani, Similarity search in high dimensions via hashing, in: Proceedings of Inter-
national Conference on Very Large Databases, 1999, pp. 518–529.
[45] T. Smith, M. Waterman, Identiﬁcation of common molecular subsequences, J. Mol. Biol. 147 (1981) 195–197.
[46] i-lids, <computervision.wikia.com/wiki/I-LIDS>.
[47] Pets, <http://ftp.pets.rdg.ac.uk/>, 2006.
[48] Mit trafﬁc, <people.csail.mit.edu/xgwang/HBM.html>.
[49] A. Adam, E. Rivlin, I. Shimshoni, D. Reinitz, Robust real-time unusual event detection using multiple ﬁxed-
location monitors, IEEE Trans. Pattern Anal. Machine Intell. 30 (3) (2008) 555–560.

19
CHAPTER
Multi-Target Tracking
in Video
Fabio Poiesi and Andrea Cavallaro
School of Electrical Engineering and Computer Science, Queen Mary University of London, UK
4.19.1 Introduction
The demand for the automated analysis of the behavior of people, animals and moving objects such as
vehicles has grown considerably in the past years. For example, systems for the recognition of human
actions and the detection of abnormal behaviors are key to support surveillance tasks [13]. To this
end, video trackers enable motion pattern analysis of single and multiple targets [85]. Single-target
trackers help analyzing motion patterns and behaviors of individuals, separately. Multi-target trackers
help quantifying target interactions and comparing motion patterns of different objects simultaneously
(Figure 19.1). Surveillance systems (Figure 19.2a) use trackers to monitor behaviors [74], to follow
selected people and to recognize them in the view of other cameras [11,54]. The analysis of collective
and individual trajectories can be exploited to recognize abnormal behaviors in crowds [65]. Trajectories
can be used to recognize interactions among humans [72] and to monitor the activity of people in order
to analyze social behaviors [24]; to observe interactions among objects and humans, to help studying
collaborative behaviors in meeting rooms [81], or to monitor the position of people with respect to
abandoned objects [74]. Tracking is also used in video-based sport analysis (Figure 19.2b) for automatic
summarization [17] and statistics gathering [30,73]. In trafﬁc scenes, tracking using ﬁxed or airborne
cameras [86] helps to automatically detect unlawful U-turns, vehicles driving in the wrong direction,
people crossing roads [31] (Figure 19.2c) and to collect statistics on typical and atypical behaviors of
vehicle and pedestrian ﬂows [4].
This chapter is organized as follows. In Section 4.19.2, we introduce the general framework of multi-
target trackers and formulate the multi-target tracking problem. In Section 4.19.3, we discuss real-world
challenges for video-based tracking. Section 4.19.4 describes features that can be extracted from a
video to enable tracking. In Section 4.19.5, we discuss prediction models used to estimate the location
of targets. Section 4.19.6 describes sequential estimation and batch association to generate tracks,
whereas in Section 4.19.7, we analyze different methods for track initialization and track termination.
Next, Section 4.19.8 describes the use of contextual information to facilitate multi-target tracking.
Finally, in Section 4.19.9, we summarize the chapter and discuss open research problems.
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00019-4
© 2014 Published by Elsevier Ltd. All rights reserved.
561

562
CHAPTER 19 Multi-Target Tracking in Video
FIGURE 19.1
Example of multi-target tracking in video. The colored tracks are associated to the respective targets. (For
interpretation of the references to color in this ﬁgure legend, the reader is referred to the web version of this
book.)
4.19.2 Problem formulation
The goal of multi-target tracking in video is to generate accurate estimations of target trajectories (tracks)
within the ﬁeld of view of a camera. A tracker can be divided into four main stages (Figure 19.3): feature
extraction, localization or association (which exploits features to identify the position of the targets on
the image plane), prediction (which models the motion of targets to predict their future locations), and
track post-processing. In order to enhance the localization task, the output tracks can be used to extract
contextualinformationbylearningtheenvironment[48]andbyupdatingthemodelofthetargets[38,67].
Let V = {v(n)}N
n=1 be a video sequence, where v(n) is the nth frame and N is the total number of
frames. The feature extraction stage generates at time n a set D(n) of ﬁltered features
D(n) = {dh(n) : n, h ∈N},
(19.1)
where
dh(n) = [uh(n) vh(n) Ih(n)]T
(19.2)
is the hth feature, uh(n) and vh(n) are the positions with respect to the horizontal and vertical axes,
respectively; Ih(n) ∈R[0,1] is a scalar value between 0 and 1 that indicates the conﬁdence of that
feature representing a target, and T is the transpose of a matrix. Features belonging to the same targets
are then linked over time in order to estimate tracks. Generally, target localization or association are
deﬁned as a function f (·) such that
τm(n) = f (D(n −γ1), . . . , D(n + γ2)),
(19.3)
where τm(n) is the mth track up to frame n and D(n−γ1), . . . , D(n+γ2) are the input features measured
in the interval [n −γ1, n + γ2] (measurements), where γ1, γ2 ∈N0. The track τm(n) belongs to the

4.19.2 Problem Formulation
563
(a)
(b)
(c)
FIGURE 19.2
Examples of video-based applications that beneﬁt from multi-target tracking: (a) surveillance (image from
iLids dataset for AVSS 2007 [36]); (b) sport analysis (image from APIDIS dataset [5]); and (c) automatic
pedestrian ﬂow monitoring (image from MIT Trafﬁc dataset [85]).

564
CHAPTER 19 Multi-Target Tracking in Video
Feature 
extraction 
Localization 
Buffer 
Track 
post-processing 
Buffer 
Prediction 
)
(n
v
)
(n
)
(n
)
(n
Model update 
Environment 
learning 
User 
Context 
Target 
models 
User 
FIGURE 19.3
Block diagram of a tracker with sequential localization. The buffer accumulates measurements to allow
processing over temporal windows. The localization stage can be externally initialized or can use contextual
information, such as a map of the scene and target model. The track post-processing stage improves the
quality of the ﬁnal result, for example by linking short tracks or by deleting spurious tracks.
set of tracks T = {τm}M
m=1, where M is the total number of tracks computed within the sequence V .
The track of a generic target m is a time series
τm = {xm(n) : 1 ≤n ≤N},
(19.4)
where xm(n) ∈Rd is the state of the target at frame n and d represents the dimension of the state.
The information encoded in the state xm(n) is used to describe the status of the target at frame n.
The deﬁnition of xm(n) is application-dependent. For example, xm(n) may encode the position and
velocity of the target [62], or also size information, such as width and height of the target [15]. The
simplest representation of a target onto the image plane is its 2D-position (d = 2),
xm(n) = [xm(n)ym(n)]T ,
(19.5)
where xm(n) and ym(n) represent the target position on the horizontal and the vertical axes, respectively.

4.19.3 Challenges
565
Some approaches formulate the problem of simultaneously tracking M targets as a problem of
single-target tracking, M times. The target-tracker association is performed by an external algorithm
that guarantees that one tracker is exclusively associated to a target [28]. Alternatively, multi-target
tracking can be formulated as a problem of jointly tracking all the targets by using a single tracker [15].
When the number of targets increases, maintaining the identities of all the tracks correctly associated
to the targets becomes challenging. Therefore, interactions among neighboring targets can be modeled
[28,36].
To improve target localization, one can calculate the state xm(n) based on the predicted state ˆxm(n)
and the current measurements [62]. The predicted state ˆxm(n) is calculated using a function h(·) applied
on the state at the previous frame n −1, such that
ˆxm(n) = h(xm(n −1)).
(19.6)
The function h(·) is also known as motion model or evolution model. The motion model is used to
draw state hypotheses from the current frame to the next, mostly using kinematic models [62]. These
hypotheses are further validated using features extracted from the current image frame. Hence, the state
xm(n) is estimated using the previous state xm(n −1) and the measurements from the image at time n.
Motion models can be either pre-learned [2,28,38,44,63] or ﬁxed [1,7,9,11,15,29,30,56,70,80,84,88].
Trackers can be causal or non-causal ﬁlters. Causal trackers (γ1 > 0, γ2 = 0) only use features
extracted from the past and the current time step n to estimate tracks (see Figure 19.4a). Causal trackers,
such as tracking methods based on particle ﬁltering [5] and Markov Chain Monte Carlo (MCMC) [3],
are used for time-critical applications. Non-causal trackers (γ1 ≥0, γ2 > 0) use also future time steps,
thus resulting in a delayed decision (see Figure 19.4b). Non-causal tracking [8,32,86,87] is typically
formulated as a global optimization problem to retrieve target tracks throughout the video sequence [61]:
the candidate target locations for the whole sequence [12] are obtained at the feature extraction stage and
are then linked together using optimization processes [32,45]. Motion models are implicitly included
into the optimization algorithm and they are commonly expressed as constant velocity models [12].
Non-causal methods can be divided into two categories: (i) methods that iteratively compute long tracks
by associating time-independent features and (ii) methods that build long tracks in multiple steps, by
extracting short-term track either with causal or sub-optimal association trackers and then, by associating
shorter tracks (i.e., tracklets) into longer tracks. Examples of non-causal trackers [59] include Detection
Association Trackers (DAT), such as Multiple Hypothesis Tracking (MHT) [61] and HybridBoosted
tracker [45].
4.19.3 Challenges
The challenges a tracker may face are due to color similarities among objects, illumination changes,
pose variations, occlusions, various noise components, abrupt or unpredicted motion variations, and the
density of targets in the scene.
Color similarities can mislead the target-background and the target-target discrimination. When
another region in the image has similar color to that of a target, then a track can be lost. Similarly,
when targets with similar color move close to each other, their identities can be swapped [41,83].
Illumination changes caused, for example, by different light sources (Figure 19.5) lead to color variations

566
CHAPTER 19 Multi-Target Tracking in Video
n-3 
n-2 
n-1 
n 
n+1 
n+2
(a)
n-3 
n-2 
n-1 
n 
n+1 
n+2
(b)
FIGURE 19.4
Causal and non-causal multi-target tracking: (a) causal trackers operate using measurements from the current
and past instants and (b) non-causal trackers generate the results using past, current and future observations.
that can induce target losses. This problem can be addressed by using illumination invariant features or
by updating the color model of the targets [67].
Shape similarities can also generate ambiguities (Figure 19.6) between a target and an object in
the background, or among similar targets [82]. Examples include people tracking when the shape is
encoded as the head-and-shoulder or full-body outline [21]. Pose variations leading to shape changes
(Figure 19.7) require a tracker to be capable of adapting the corresponding appearance models to avoid
track inaccuracies or losses [50].
When occlusions happen (Figure 19.8), the only data available to the tracker are the measured
target dynamics and the appearance features before (and after) the occlusion itself [11,83]. Using this

4.19.3 Challenges
567
FIGURE 19.5
Example of color variations of a target due to illumination differences: (a) in a shop and (b) in a corridor.
Images from CAVIAR dataset [13]. (For interpretation of the references to color in this ﬁgure legend, the
reader is referred to the web version of this book.)
FIGURE 19.6
Example of clutter: the person to be detected is not clearly distinguishable due to appearance similarity with
the background and with other objects (e.g., a mannequin). Image from CAVIAR dataset [13].
information, a tracker can estimate the likely location of a target by interpolating spatio-temporal data
when the target is not observable.
Noise components can be introduced during video acquisition or compression, and may lead to
corrupted measurements that generate unreliable features for the estimation of the target locations.
Motion-related challenges can be due to abrupt variations (e.g., sudden accelerations) or unusual
dynamics (e.g., deviations for a predictable path to avoid obstacles). Most tracking algorithms rely on
prior models for motion prediction [11]. These models are mainly linear with additional terms that
represent small variations as noise components [62].
Finally, the difﬁculty of tracking depends on the density of targets in the scene. In the case of people
tracking, when the crowd motion tend to be coherent in one direction and does not vary over time because
of the high spatial density of people, crowded scenes are deﬁned as structured. In unstructured crowded

568
CHAPTER 19 Multi-Target Tracking in Video
FIGURE 19.7
Example of shape change due to pose variation: (a) front view and (b) side-rear view. Images from CAVIAR
dataset [13].
FIGURE 19.8
Example of a largely occluded person (man with the black jumper). Images from CAVIAR dataset [13]. (For
interpretation of the references to color in this ﬁgure legend, the reader is referred to the web version of this
book.)
scenes, instead, groups of people may move simultaneously in different directions [63]. Successful
trackers rely on part-based detectors and perform non-causal tracking using target re-identiﬁcation [41].
4.19.4 Feature extraction
The tracks can be estimated either by extracting features in each frame and linking them over time, or
by extracting features at initialization (i.e., when an object appears in the scene) and then by letting
the tracker perform the subsequent location hypotheses and conﬁrming them over time using the newly
extracted features. Example of features are discussed below and summarized in Table 19.1.

4.19.4 Feature Extraction
569
Table 19.1 Taxonomic Summary of Multi-Target Trackers. Key: Ref: Reference; TI: Track Initialization; TT: Track Termi-
nation; BS: Background Subtraction; RGB: Red Green Blue Colorspace; HSV: Hue Saturation Value Colorspace; HOG:
Histogram of Oriented Gradients; ISM: Implicit Shape Model; WFD: Weighted Frame Difference; SIFT: Scale-Invariant
Feature Transform; I: Implicit; A: Automatic; T&T: Tag and Track; “–”: No Information Provided
Ref
Feature extraction
Motion model
Sequential
Batch
TI
TT
BS
Color
Shape
Texture
Pre-learned
Fixed
localization
association
[32]
RGB
Edgelets
✓
✓
✓
I
I
[87]
RGB
Edgelets
✓
✓
I
I
[45]
RGB
Edgelets
✓
✓
I
I
[40]
RGB
Edgelets +
HOG
Covariance
matrix
✓
✓
✓
I
I
[41]
RGB
Edgelets +
HOG
Covariance
matrix
✓
✓
✓
I
I
[83]
RGB
Edgelets +
HOG
Covariance
matrix
✓
✓
✓
I
I
[12]
✓
✓
I
I
[59]
HSV
✓
✓
I
I
[88]
Gaussian
RGB
✓
✓
A
A
[56]
HSV
✓
✓
A
–
[15]
RGB
✓
✓
A
A
[7]
WFD
✓
✓
A
A
[9]
HOG
✓
✓
✓
I
I
[64]
HOG
✓
✓
T&T
–
[63]
–
✓
✓
T&T
–
[2]
–
✓
✓
T&T
–
[38]
Gradient
✓
✓
T&T
–
[84]
RGB
Elliptical
model
SIFT
✓
✓
A
–
[11]
HOG/ISM
✓
✓
A
A
[28]
–
RGB
✓
✓
A
–
[1]
HSV
✓
✓
A
A
[70]
Gaussian +
vessel
✓
✓
A
A
[29]
HSV
✓
✓
A
A

570
CHAPTER 19 Multi-Target Tracking in Video
Target candidate locations can be deﬁned using color histograms [26,40,41,59,83,87]. Color
histograms are used to distinguish targets over time while addressing the challenges of targets with
similar color. In order to reduce the sensitivity to light variations, histograms are generally quantized
with 8 bins per RGB channel [40].
Detections indicating candidate targets, represented as vectors or binary maps [9,82], can be provided
by cascade classiﬁers [79,90] or by multi-valued maps [70] representing the conﬁdence of having targets
in speciﬁc locations. The latter case allows one to deal with targets with low signal-to-noise ratios [62].
When the scene background is ﬁxed, it is possible to detect moving targets by calculating the difference
between the current frame and a reference background frame [58]. One can use a simple weighted frame
difference between the actual and the background frame [7,13], or a mixture of Gaussians [88] where
each component of the mixture belongs to a color channel (e.g., three components for RGB). In this
case, the mixture of Gaussians is learned on the background and the probability of each pixel of a new
incoming frame being considered a part of background or of target is then calculated. Lastly, trackers
can also rely on the detections generated, for example, by an AdaBoost classiﬁer [56,79].
Shapes can be used to represent targets, for example, in the form of Histograms of Oriented Gradients
(HOG) [16]. This representation is popular for describing heads [9] and bodies [11,70]. Alternatively,
edgelets, a large pool of short lines and curve segments (based on intensity gradients), can be used to
represent human shapes [82]. This method employs descriptors for head, torso, leg and full body, which
are combined to address the problem of occlusions. Similarly, shapelets are combinations of oriented
gradient responses learned in a discriminative manner on local patches [66].
Targets can also be described with covariance matrices as texture descriptors. In this case, a dense
model of covariance features (e.g., spatial location, intensity, higher-order derivatives) is used inside
a detection area [77]. A target can be represented with several covariance descriptors of overlapping
regions, where the best descriptors are determined with a greedy feature-selection algorithm combined
with boosting. The covariance matrix descriptor is applied on image patches to characterize and distin-
guish targets [40,41]. The Scale-Invariant Feature Transform (SIFT) [47] can be also used to capture
texture characteristics in order to describe for example human torso regions [84].
The choice of the type of features provided to the localization or association stages is important
regarding the use of a tracker with static or moving cameras [18,42]. If the feature extraction relies only
on target information, such as outline of targets [82], the tracker can be extended to moving camera
applications [41]. Instead if the feature extraction relies on the background information for extracting
candidate target locations (e.g., using background subtraction) and if the localization stage is highly
dependent on this feature, major modiﬁcations to the localization stage are needed to extend the tracker
from static to moving cameras [88]. An adaptation to moving cameras is also needed when contextual
information (e.g., entry/exit points) is included in the localization stage [83].
4.19.5 Prediction
Predictive models generate target hypotheses that the localization stage validates using current mea-
surements [49,62]. Predictive models can use for example kinematic equations (e.g., constant velocity)
[62] or motion estimation models (e.g., [55]), and can involve a training phase of the target evolution
[38]. Learning-based models mostly exploit a time interval at the beginning of a video sequence for
training [2].

4.19.6 Localization and Association
571
Particle ﬁlter algorithms [5] use autoregressive motion models for linearly predicting future target
locations [1,11,15,28,29,56,70]. Equation (19.6) for a generic autoregressive motion model takes the
following form:
ˆxm(n) = Fxm(n −1) + ξ(n −1),
(19.7)
where F is a d × d matrix deﬁning the linear function h(·) and ξ(n −1) is random noise with a given
distribution (e.g., Gaussian). Prediction models can be built using motion estimation algorithms [55]
between consecutive frames. The resulting motion ﬂow is exploited to build predictive models. A pre-
dictive motion model for consecutive features can be designed using a constant velocity model with the
contribution of Kanade-Lucas-Tomasi (KLT) point tracks [9,64,76]. Speciﬁcally, the prediction state
is deﬁned as
ˆxm(n) = xm(n −1) + γ ˆvm(n −1),
(19.8)
where ˆvm(n −1) is the velocity estimation coming from the KLT tracks in the frame prior the current
state and γ is the time interval between the states where the velocity is calculated.
Mode-seeking trackers, such as the Mean-Shift tracker [14], follow neighboring modes of clusters
generated with features extracted from the frames. Clusters are represented as modes and tracking is
performed by seeking the closest mode in the subsequent frame [7]. The predicted location of the target
in the subsequent frame lies in the area deﬁned by a kernel that is dependent on the target position in
the previous frame. Each mode displacement is therefore assumed to be smaller than the kernel size.
Learned models are used to improve performance when motion is predictable, for example, in case
of high target density [2,63]. Assuming that each target follows a coherent direction with respect to
the other targets, it is possible to learn motion models and to include them into the tracker to help the
prediction of target positions. For example, in crowded scenes, a set of motion constraints can be trained
from the behavior of humans [2]. These motion constraints are properties retrieved from exit regions
and dominant/common paths of people, inﬂuences generated by barriers or walls, and people behavior
around the tracked person. The tracker may rely on a grid of particles over the image plane and tracking
is performed by maximizing the transition probability of a particle from one cell to another. Hence, the
transition probability is determined by two factors: (i) the color similarity between the current and the
next location and (ii) the inﬂuence of the learned motion constraints in this location.
Scene dynamics can be learned using optical ﬂow features [55] (i.e., position and velocity) and
encoded according to a codebook, where each word of the vocabulary is associated to a speciﬁc
dynamic [63]. The target location is computed using a weighted mean of the displacement of the obser-
vations based on the learned dynamics and the predicted displacement. Alternatively, time-varying
dynamics of people across different spatial locations can be modeled using Hidden Markov Models
(HMM) [38,60]. The hidden states of the HMM encode possible motion patterns that are likely to be
present at each spatial location.
4.19.6 Localization and association
Localization and association rely on the measurements coming from the feature extraction stage and
validates feature similarities over time to estimate reliable tracks. The validation can be performed
sequentially or as a batch process (Figure 19.9). Sequential localization extracts tracks recursively.

572
CHAPTER 19 Multi-Target Tracking in Video
n-3 
n-2 
n-1 
n 
(a)
n-2 
n-1 
n 
n+1 
(b)
FIGURE 19.9
Comparison between a sequential localization and a batch association approach. (a) Sequential localiza-
tion uses predictive motion models to explore potential target areas. (b) Batch association generates track
hypotheses (colored lines) that are selected based on an optimization procedure. (For interpretation of the
references to color in this ﬁgure legend, the reader is referred to the web version of this book.)

4.19.6 Localization and Association
573
Bach processes are used when features are collected within a time interval and tracks are extracted by
optimizing temporal links among features.
4.19.6.1 Sequential localization
Particle ﬁlter recursively ﬁnds targets using the Bayesian recursion for the sequential estimation of
the target states over time [62]. The Bayesian recursion involves the estimation of the target state1
x(n) calculated by constructing the posterior probability density function (pdf) using motion models
(see Section 4.19.5) and measurements gathered from the current frame. The posterior pdf can be a
multimodal distribution, where the modes of the distribution represent likely target locations. In order
to make the Bayesian recursion computationally tractable, the posterior pdf is approximated with a
Monte Carlo method [19], which consists of a set of random samples, or particles, drawn from the
posterior pdf with associated weights.
The extension from single to multi-target particle ﬁlter requires the size of the state to be made
proportional to the number of targets, i.e., x ∈Rd′ where d′ = d·M, with M being the number of
targets [33]. Generally, when a single particle ﬁlter has to deal with multiple targets and the distribution
of the states is represented with a mixture of distributions, one of the major problems is to cope with
the maintenance of the multi-modality [78]. Hence, a mechanism based on AdaBoost for maintaining
the multi-modality can be included into the tracker [56]. Alternatively, a baseline version of the particle
ﬁlter is applied on conﬁdence maps generated with a Cascaded Conﬁdence Filtering (CCF) which
incorporates constraints on the size of the objects, on the preponderance of the background and on
the smoothness of trajectories [70]. A feature extraction stage relying on geometric structures along
with background ﬁltering produces preliminary conﬁdence maps for a certain time interval. Spurious
features within this time interval are ﬁltered out with a temporal smoothing method based on the Vessel
ﬁlter [22]. The resulting conﬁdence maps are used as observations for the multi-modal particle ﬁlter.
Trackers can be composed of multiple particle ﬁlters, each of them operating on one target [1].
The communication among ﬁlters is performed with a heuristic method relying on the spatial locations
of features. The feature-trajectory association is performed using the AdaBoost classiﬁer conﬁrming the
trackingresultforeachtrajectory.Theassignmentoftheparticleﬁlterstoeachtargetcanbeperformedby
imposingapseudo-independenceamongﬁltersthatislearnedinatrainingphase[28].Parametersinclude
trained features into the weighting function of the ﬁlter, such as measures of the distance between target
states and predicted locations. Features include the probability of the target being in a certain location
with respect the predicted estimation, color similarity with respect to trained templates, dissimilarity
with the background, penalty scores with target regions overlapping each other and neighboring targets.
Furthermore, the Hungarian algorithm [39] can be used to associate particle ﬁlters to targets [84].
The assignment matrix is constructed using a Bayesian formulation among features and track states.
The association between features and tracks can also be performed by greedy algorithms [11]. A single
particle ﬁlter is employed for each target and, in order to discriminate the tracked targets, an on-line
AdaBoost classiﬁer is trained for each target against all the others. Each weak learner represents a
feature computed for both positive and negative training images.
1The subscript m has been removed to generalize the problem.

574
CHAPTER 19 Multi-Target Tracking in Video
Random Finite Sets (RFS) can also be used along with the Bayesian formulation [49,51]. RFS treat
state and measurements as realizations of random variables and the Bayesian formulation with RFS can
be approximated with Monte Carlo methods [29]. The feature extraction stage can be embedded into
the tracker to deﬁne the appearance model and, like the motion model, it is deﬁned a priori.
Markov Chain Monte Carlo (MCMC) methods can alternatively be used to drawn samples from
posterior distributions, since with particle ﬁlter is very hard to deal with large number of targets and
hence large state spaces. In fact, maintaining the multi-modality requires very precise mathematical
methods [78] and the computational cost for handling high-dimensional state spaces is still prohibitive.
For these reasons, in order to avoid expensive integration steps, MCMC methods have been introduced
[3]. For example, Smith et al. [69] used MCMC to deal with 10-dimensional states. Moving humans can
be represented with 3D models using camera calibration parameters after being detected via background
subtraction. The information about their locations is employed to build a multi-person joint likelihood
function and used to ﬁnd person locations in consecutive frames, leading to a dimensionality of the
space proportional to the number of persons in the scene [88]. Kalman ﬁlters are then used to build the
posterior pdf for consecutive frames employing a ﬁxed motion model describing persons at constant
velocity and affected by Gaussian noise. Since a joint likelihood is used, which involves both discrete and
continuous variables, MCMC is employed to sample from the posterior pdf and to obtain the estimation
of the target states calculating the Maximum A Posteriori (MAP). Alternatively, track hypotheses can
be extracted within a four-second window using Minimum Description Length (MDL) [9]. Features
such as scale, location and motion computed with Kanade-Lucas-Tomasi (KLT) are associated over
time using likelihood functions. A reﬁnement stage relying on the likelihood functions is built to allow
two types of modiﬁcations to track pairs, namely the move of certain features from one track to the
other or the swapping of all the features belonging to both tracks at a chosen time instant. MCMC is
then used to take decisions about the acceptance of such modiﬁcations and to conﬁrm the ﬁnal track
decision.
Finally, sequential localization can be performed with ad hoc methods, either based on thresholds or
on combinations of different algorithms. For example, the link between two features can be deﬁned by
a probability (i.e., the link probability) calculated as a product of three independent afﬁnities calculated
from feature characteristics, such as position, size, and appearance [32]. The ﬁnal linking between two
features is then conﬁrmed by using a two-threshold strategy. The ﬁrst threshold is used to check if the
link probability is high enough; whereas the second threshold is used to determine if the afﬁnity of any
of their conﬂicting pairs is high enough.
4.19.6.2 Batch association
Features can be associated over time with a batch process through maximization algorithms applied
on posterior probability, which quantiﬁes the likelihood of the tracks given the set of features [87]. Let
the set of Hd features D = {dh}Hd
h=1 be gathered from the video sequence and T ∗be the set of track
hypotheses obtained by associating features over time. The goal is to maximize the posterior probability
of T ∗given the set D (Figure 19.9b), that is
T = arg max
T ∗p(T ∗|D) = arg max
T ∗p(D|T ∗)p(T ∗) = arg max
T ∗
Hd

h=1
p(dh|T ∗)p(T ∗),
(19.9)

4.19.6 Localization and Association
575
where T = {τm}M
m=1 is the set of tracks and the likelihood probabilities are assumed to be conditionally
independent given the hypothesis T ∗. Such maximization can be calculated with an iterative method that
cycles through the sequence and ﬁnds optimal solutions between each consecutive frame pair [12]. The
two-frame optimal solutions are calculated by using 2D target locations with the Hungarian algorithm
[39]. The iterative cycling method, similar to the Iterated Conditional Modes (ICM) algorithm [10],
updates joint solutions of multiple variables in order to ﬁnd stronger local optima, and the iterations
continue until no further improvement are achieved.
There are methods to iteratively compute optimal tracks using the complete set of features [12], and
methods that reduce the complexity of the problem by pruning negligible hypotheses and by ﬁnding
sub-optimal solutions in multiple steps [32,45].
An alternative method is formulated with a cost-ﬂow network. Instead of using thresholds to link
features [32], it is possible to use the algorithm for min-cost ﬂow networks proposed by Goldberg
[25]. Within this network, each ﬂow is interpreted as a track of a single target and the cost of the ﬂow
corresponds to the log-likelihood of the link hypothesis. The log-likelihood linking is calculated by
taking into account size, position, appearance, and time gap of the features by considering independence
among them. Features can be associated within a temporal window using Mean-Shift clustering [14] on
the feature space [8]. For each cluster, which ideally represents a target, PCA is applied and the features
are associated by considering the direction of the principal components. PCA allows one to represent
the local trend in the data distribution and measure the reliability of the associated features.
Final tracks can be obtained by linking tracklets. This problem can be formulated as a joint problem of
ranking and classiﬁcation [45], by using HybridBoost, a combination of RankBoost [23] and AdaBoost
[68]. The role of RankBoost is to build the tracklet afﬁnity model considering relative preferences over
any tracklet pairs as well as low values for those tracklet pairs that should not be associated. AdaBoost
is composed of weak classiﬁers relying on a single type of features for tracklet afﬁnity measurements,
such as appearance, motion, and frame gap between a tracklet pair.
An algorithm for optimal tracklet association (OLDAM) [40] uses a temporal shifting window for
the online learning of discriminative appearance features. Positive samples are extracted within the
same tracklet and collected for all the tracklets in a temporal window. Negative samples are collected by
extracting features from tracklets not belonging to the same target and by taking into account their spatio-
temporal properties. The model learning problem is formulated as a binary classiﬁcation problem using
AdaBoost. Afﬁnity measurements of appearance features (i.e., color and HOG) are adopted in AdaBoost
to learn weak classiﬁers. The predicted conﬁdence output of AdaBoost is combined with motion and time
features in order to compute the link probability between tracklets. OLDAM has been further improved
with PIRMPT [41], which includes a method to automatically select the most discriminative features
from each tracklet by an online learning method based on appearance descriptors. Such descriptors
are used to create a target model for each tracklet and further employed to link consecutive tracklets
[32]. Tracking improvements can be achieved with Explicit Occlusion Model (EOM), which includes
in the tracking problem occlusion hypotheses [87]. The EOM method generates a set of occlusion
hypotheses and constraints, and combines them with the input associations. This combination avoids
spurious associations due to large temporal gaps between the associated features.
In order to make the tracklet linking generic, methods shall be independent of feature extractors, for
example, by employing an optimization process based on common afﬁnity models along with social
grouping behaviors [59]. The nonlinear equations used for the association can have terms approximated

576
CHAPTER 19 Multi-Target Tracking in Video
with Lagrange theory and solved using an iterative algorithm that employs the Hungarian algorithm
and K-mean clustering [20].
4.19.7 Track initialization and termination
Initialization and termination of tracks are two important track management issues. The initialization for
causal trackers can be performed automatically, i.e., a new track starts when new features are available
and are not associated to any of the existing tracks. The initialization for non-causal trackers can be
performed as for the causal trackers or with an implicit modality, i.e., when the initial location is
associated to a track obtained as optimal solution. An alternative is manual track initialization, used for
example in tag-and-track applications [2,38].
Tracking methods performing batch association of features or tracklets [12,40,41,83,87] implicitly
initialize and terminate tracks. In fact, when the optimal track solution is computed, the start and the
end of each track are implicitly encoded into the solution. Instead, methods performing sequential
localization need criteria for track initialization and termination.
TrackerssuchasTrack-Before-Detectbasedonparticleﬁlter[62]performjointdetectionandtracking
of targets without relying on any external mechanism for initialization or termination. The initialization
and termination of tracks are embedded in the ﬁlter and modeled using a Markov chain [57], where the
number of states corresponds to the number of targets in the scene. Alternatively, if the target states are
represented as a collection of random variables that create a ﬁnite-set-valued state modeled with a multi-
Bernoulli RFS [49], the tracker can handle track initialization and termination by relying on probabilities
of target appearance and disappearance [29]. The RFS framework can handle a time-varying number
of targets as well as missing and noisy features by employing for example the Probability Hypothesis
Density (PHD) ﬁlter [52].
External mechanisms for track initialization or termination can be based on the extracted features
[1,11,56,79]. New tracks are initialized when none of the running trackers are associated to the local-
ized targets [1,56]. This process can be enhanced when multiple features are generated along the image
borders, which is an indication on new incoming targets in the scene [11]. The termination of a track
occurs when the tracker is unable to validate the features for a number of consecutive frames. Also, ad
hoc methods for initializing and terminating tracks can be used by implementing clustering strategies
on the extracted features and comparing the number of clusters in the current frame with those in the
previous frame [7].
4.19.8 Scene contextual information
Scene contextual information includes the knowledge of the scene background, occlusion areas, entry/
exit regions, and dynamic textures (Figure 19.10). Context is exploited to distinguish targets from
clutter [48] and to improve initialization and termination of tracks [83]. For example, background
informationcanbeusedtoenhancetheseparabilitybetweentargetfeaturesandbackgroundfeatures[70],
or object-level information can be used to model spatio-temporal relationships in order to improve
tracking in indoor scenarios [37,46].

4.19.8 Scene Contextual Information
577
FIGURE 19.10
Examples of scene contextual information. Multi-target tracking can be enhanced by exploiting knowledge
about the scene layout and objects such as trees that may occlude targets or may generate dynamic textures.
Typical walking paths can also be used to narrow the search of human targets. Entry and exit regions can help
track initialization and track termination. (Key: D: dynamic textures; W: walking path; E: entry/exit regions.)
(a,b) Images from CAVIAR dataset [13], (c,d) images from MIT Trafﬁc dataset [85]. (For interpretation of
the references to color in this ﬁgure legend, the reader is referred to the web version of this book.)
Contextual information can be extracted by learning the environment from user annotations or auto-
matically from the output of the tracker. User annotation of entry/exit regions [83] may be required
in order to provide the tracker with reliable contextual information. For example, detections of targets
located in manually selected entry regions can be used to initialize tracks [11]. Alternatively, entry/exit
regions, typical paths and stopping regions can be automatically extracted from long-term tracks [35,53]
or tracklets [83,89]. In unstructured scenes, contextual information can be used to perform online learn-
ing of motion maps [83]. A motion map can be constructed by relying on entry/exit regions of the scene

578
CHAPTER 19 Multi-Target Tracking in Video
and by using motion patterns gathered from tracks. Entry/exit regions are used to draw likely target
paths when reliable target features are collected. The learning of non-linear motion patterns is used
to enhance the diversity among different track hypotheses, to improve the afﬁnity estimations among
extracted features and to build robust appearance models [83]. In structured scenes, scene context can be
incorporated by automatically learning ﬂoor ﬁelds [2], which model directions of people on dominant
paths and towards preferred exit regions, and to improve the motion prediction in these regions. Finally,
with an interactive environment learning, models can be learned for clutter areas and for initialization
areas. The clutter model improves the capability of the tracker to discard noisy measurements. The
initialization model can reduce the delay of track initialization in locations where targets are likely to
appear [48].
Scene contextual information is also modeled and used to improve tracking accuracy when linking
tracklets [32,83]. For example, the scene model (i.e., entry/exit regions and static occluders) projected on
the ground plane with homography from the image plane [27] can be used to reduce track fragmentation
and prevent identity switches of linked tracklets. Long-range trajectory association is performed using an
Expectation-Maximization (EM) algorithm. The E-step estimates the scene model in terms of entry/exit
regions with a Bayesian inference. Then these regions are used to specify initialization and termination
of each tracklet. The M-step links tracklets using the information from the E-step and long tracks
are obtained through Hungarian algorithm [32,83,86]. The assignment matrix used by the Hungarian
algorithm is formulated as a MAP problem, relying on link probabilities calculated with associated
detection responses.
4.19.9 Summary and outlook
In this chapter we discussed state-of-the-art multi-target trackers and presented a general ﬂow diagram
that allowed us to highlight the major tracking steps. We also presented a survey on recent multi-target
trackers, discussing their major steps that include feature extraction algorithms, prediction models,
localization and association methods, and techniques for track initialization and termination. We ﬁnally
discussed how contextual information can be employed to improve tracking performance.
Interesting open challenges in multi-target tracking include the effective extension of feature selection
for target-background separability from ofﬂine [71] to on-line approaches [67], deﬁning motion models
that are ﬂexible to deal with different dynamics of a scene [64], and predicting tracking failures by
identifying image regions where trackers are likely to fail [34]. These failures can be detected by
employing interaction models based on track information [36] and potentially solved by strengthening
the trackers with methods for self-tuning parameters [43] (e.g., resampling strategy for particle ﬁlter
[62]). Removing the dependence of user interaction is also desirable to make the environment learning
stage ﬂexible to context changes [35,53] and independent from user feedbacks [48].
Finally, there is a growing interest in tracking targets using multiple cameras for increasing the
overall ﬁeld of view [75]. In this case, the target discrimination and identity association techniques need
to consider the appearance variability of targets across cameras due to changes in illumination, pose,
and appearance.

References
579
References
[1] I. Ali, M.N. Dailey, Multiple human tracking in high-density crowds, in: Proceeding of Conference on
Advanced Concepts for Intelligent Vision Systems, Bordeaux, France, September 2009.
[2] S. Ali, M. Shah, Floor ﬁelds for tracking in high density crowd scenes, in: Proceeding of European Conference
on Computer Vision, Marseille, France, October 2008.
[3] C. Andrieu, N.D. Freitas, A. Doucet, M.I. Jordan, An introduction to MCMC for machine learning, Mach.
Learn. 50 (1) (2003) 5–43.
[4] N. Anjum, A. Cavallaro, Multi-feature object trajectory clustering for video analysis, IEEE Trans. Circ. Syst.
Video Technol. 18 (11) (2008) 1555–1564.
[5] M. Arulampalam, S. Maskell, N. Gordon, T. Clapp, A tutorial on particle ﬁlters for online nonlinear/non-
gaussian bayesian tracking, IEEE Trans. Signal Process. 50 (2) (2002) 174–188.
[6] Y. Bar-Shalom, Tracking and Data Association, Academic Press Professional, Inc., 1988.
[7] C. Beleznai, B. Fruhstuck, H. Bischof, Human tracking by fast Mean-Shift mode seeking, J. Multimedia 1 (1)
(2006) 1–8.
[8] C. Beleznai, D. Schreiber, Multiple object tracking by hierarchical association of spatio-temporal data, in:
Proceeding of International Conference on Image Processing, Hong Kong, China, September 2010.
[9] B. Benfold, I. Reid, Stable multi-target tracking in real-time surveillance video, in: Proceeding of Computer
Vision and Pattern Recognition, Colorado Springs, USA, June 2011.
[10] J. Besag, On the statistical analysis of dirty pictures, J. Roy. Statist. Soc. Ser. B Methodological 48 (3) (1986)
259–302.
[11] M.D. Breitenstein, F. Reichlin, B. Leibe, E. Koller-Meier, L.V. Gool, Online multiperson tracking-by-detection
from a single, uncalibrated camera, IEEE Trans. Pattern Anal. Mach. Intell. 33 (9) (2011) 1820–1833.
[12] R. Collins, Multitarget data association with higher-order motion models, in: Proceeding of Computer Vision
and Pattern Recognition, Providence, Rhode Island, USA, June 2012.
[13] R. Collins, A. Lipton, T. Kanade, H. Fujiyoshi, D. Duggins, T. Tsin, D. Tolliver, N. Enomoto, O. Hasegawa,
P. Burt, L. Wixson, A system for video surveillance and monitoring, Tech. Rep. CMU-RI-TR-00-12, The
Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, 2000.
[14] D. Comaniciu, V. Ramesh, P. Meer, Kernel-based object tracking, IEEE Trans. Pattern Anal. Mach. Intell.
25 (5) (2003) 564–577.
[15] J. Czyz, B. Ristic, B. Macq, A particle ﬁlter for joint detection and tracking of color objects, Image Vis.
Comput. 25 (2007) 1271–1281.
[16] N. Dalal, B. Triggs, Histograms of oriented gradients for human detection, in: Computer Vision and Pattern
Recognition, San Diego, CA, USA, January 2005.
[17] F. Daniyal, A. Cavallaro, Multi-camera scheduling for video production, in: Proceeding of Conference on
Visual Media Production, London, UK, November 2011.
[18] P. Dollar, C. Wojek, B. Schiele, P. Perona, Pedestrian detection: an evaluation of the state of the art, IEEE
Trans. Pattern Anal. Mach. Intell. 34 (4) (2012) 743–761.
[19] A. Doucet, J. de Freitas, N. Gordon, Sequential Monte Carlo Methods in Practice, Springer-Verlang, 2001
(Chapter An introduction to Sequential Monte Carlo Methods).
[20] R. Duda, P. Hart, D. Stork, Pattern Classiﬁcation, Wiley, 2001.
[21] P.F. Felzenszwalb, R.B. Girshick, D. McAllester, D. Ramanan, Object detection with discriminatively trained
part based models, IEEE Trans. Pattern Anal. Mach. Intell. 32 (9) (2010) 1627–1645.
[22] A. Frangi, W. Niessen, K. Vinken, M. Viergever, Multiscale Vessel enhancement ﬁltering, in: Proceeding of
Medial Image Computing and Computer-Assisted Intervention, Cambridge, MA, USA, October 1998.

580
CHAPTER 19 Multi-Target Tracking in Video
[23] Y. Freund, R. Iyer, R. Schapire, Y. Singer, An efﬁcient boosting algorithm for combining preferences, J. Mach.
Learn. Res. 4 (6) (2003) 933–969.
[24] J. Gall, A. Yao, N. Razavi, L.V. Gool, V. Lempitsky, Hough forests for object detection, tracking, and action
recognition, IEEE Trans. Pattern Anal. Mach. Intell. 33 (11) (2011) 2188–2202.
[25] A. Goldberg, An efﬁcient implementation of a scaling minimum-cost ﬂow algorithms, J. Algorithm 22 (1)
(1997) 1–29.
[26] R.C. Gonzalez, R.E. Woods, Digital Image Processing, Pearson Prentice Hall, 2008.
[27] R. Hartley, A. Zisserman, Multiple View Geometry in Computer Vision, Cambridge University Press, 2003.
[28] R. Hess, A. Fern, Discriminatively trained particle ﬁlters for complex multi-object tracking, in: Proceeding
of Computer Vision and Pattern Recognition, Miami, FL, USA, June 2009.
[29] R. Hoseinnezhad, B.-N. Vo, D. Suter, B.-T. Vo, Multi-object ﬁltering from image sequence without detection,
in: Proceeding of International Conference on Acoustic, Speech and Signal Processing, Dallas, Texas, USA,
March 2010.
[30] M.-C. Hu, M.-H. Chang, J.-L. Wu, L. Chi, Robust camera calibration and player tracking in broadcast
basketball video, IEEE Trans. Multimedia 13 (2) (2011) 266–279.
[31] W. Hu, X. Xiao, Z. Fu, D.X.T. Tan, S. Maybank, A system for learning statistical motion pattern, IEEE Trans.
Pattern Anal Mach. Intell. 28 (9) (2006) 1450–1464.
[32] C. Huang, B. Wu, R. Nevatia, Robust object tracking by hierarchical association of detection responses,
in: Proceeding of European Conference on Computer Vision, Marseille, France, October 2008.
[33] C. Hue, J.-P.L. Cadre, P. Perez, Sequential Monte Carlo methods for multiple target tracking and data fusion,
IEEE Trans. Signal Process. 50 (2) (2002) 309–325.
[34] Z. Kalal, K. Mikolajczyk, J. Matas, Forward-backward error: automatic detection of tracking failures, in: Pro-
ceeding of International Conference on Pattern Recognition, Istambul, Turkey, August 2010.
[35] A. Kembhavi, T. Yeh, L.S. Davis, Why did the people cross the road (there)? Scene understanding using
probabilistic logic models and common sense reasoning, in: Proceeding of European Conference on Computer
Vision, Crete, Greece, September 2010.
[36] Z. Khan, T. Balch, F. Dellaert, MCMC-based particle ﬁltering for tracking a variable number of interacting
targets, IEEE Trans. Pattern Anal. Mach. Intell. 27 (11) (2005) 1805–1819.
[37] R. Kindermann, J.L. Snell, Markov Random Fields and Their Applications, American Mathematical Society,
Providence, Rhode Island, 2000.
[38] L. Kratz, K. Nishino, Tracking pedestrians using local spatio-temporal motion patterns in extremely crowded
scenes, IEEE Trans. Pattern Anal. Mach. Intell. 34 (5) (2012) 987–1002.
[39] H. Kuhn, The Hungarian method for the assignement problem, Nav. Res. Logist. Q. 2 (1955) 843–854.
[40] C. Kuo, C. Huang, R. Nevatia, Multi-target tracking by on-line learned discriminative appearance models,
in: Proceeding of Computer Vision and Pattern Recognition, San Francisco, CA, USA, June 2010.
[41] C. Kuo, R. Nevatia, How does person identity recognition help multi-person tracking? in: Proceeding of
Computer Vision and Pattern Recognition, Colorado Springs, USA, June 20–25, 2011.
[42] B. Leibe, K. Schindler, N. Cornelis, L.V. Gool, Coupled object detection and tracking from static cameras
and moving vehicles, IEEE Trans. Pattern Anal. Mach. Intell. 30 (10) (2008) 1683–1698.
[43] M. Li, T. Tan, W. Chen, K. Huang, Efﬁcient object tracking by incremental self-tuning particle ﬁltering on
the afﬁne groups, IEEE Trans. Signal Process. 21 (2012) 1298–1313.
[44] M. Li, Z. Zhang, K. Huang, T. Tan, Rapid and robust human detection and tracking based on Omega-shape
features, in: Proceeding of International Conference on Image Processing, Cairo, Egypt, November 2009.
[45] Y. Li, C. Huang, R. Nevatia, Learning to associate: HybridBoosted multi-target tracker for crowded scene,
in: Proceeding of Computer Vision and Pattern Recognition, Miami, FL, USA, June 2009.

References
581
[46] Y. Li, R. Nevatia, Key object driven multi-category object recognition, localization and tracking using
spatio-temporal context, in: Proceeding of European Conference on Computer Vision, Marseille, France,
October 2008.
[47] D. Lowe, Object recognition from local scale-inveriant feature, in: Proceeding of International Conference
on Computer Vision, Corfu, Greece, September 1999.
[48] E. Maggio, A. Cavallaro, Learning scene context for multiple object tracking, IEEE Trans. Image Process.
18 (8) (2009) 1873–1884.
[49] E. Maggio, A. Cavallaro, Video Tracking: Theory and Practice, Wiley, 2011.
[50] E. Maggio, F. Smeraldi, A. Cavallaro, Adaptive multi-feature tracking in a particle ﬁltering framework, IEEE
Trans. Circ. Syst. Video Technol. 17 (10) (2007) 1348–1359.
[51] R. Mahler, Random-Set Approach to Data Fusion, SPIE, 1994.
[52] R. Mahler, A theoretical foundation for the Stein-Winter probability hypothesis density (phd) multitarget
tracking approach, in: Proceeding of MSS National Symposium on Sensor and Data Fusion, San Antonio,
Texas, USA, 2002.
[53] D. Makris, T. Ellis, Learning semantic scene models from observing activity in visual surveillance, IEEE
Trans. Syst. Man Cybern. Part B 35 (3) (2005) 397–408.
[54] R. Mazzon, S.F. Tahir, A. Cavallaro, Person re-identiﬁcation in crowd, Pattern Recog. Lett. 33 (14) (2012)
1828–1837.
[55] A. Mitiche, P. Bouthemy, Computation and analysis of image motion: a synopsis of current problems and
methods, Int. J. Comput. Vis. 19 (1) (1996) 29–55.
[56] K. Okuma, A. Talenghani, N.D. Freitas, A boosed particle ﬁlter: multitarget detection and tracking, in:
Proceeding of European Conference on Computer Vision, Prague, Czech Republic, May 2004.
[57] A. Papoulis, S. Pillai, Probability, Random Variables and Stochastic Processes, McGraw Hill, 2002.
[58] M. Piccardi, Background subtraction techniques: a review, in: Proceeding of International Conference on
Systems, Man and Cybernetics, The Hauge, Netherlands, October 2004.
[59] Z. Qin, C. Shelton, Improving multi-target tracking via social grouping, in: Proceeding of Computer Vision
and Pattern Recognition, Provence, Rhode Island, USA, June 2012.
[60] L. Rabiner, A tutorial on Hidden Markov Models and selected applications in speech recognition, Proc. IEEE
77 (0018–9219) (1989) 257–286.
[61] D. Reid, An algorithm for tracking multiple targets, IEEE Trans. Autom. Control 24 (6) (1979) 843–854.
[62] B. Ristic, S. Arulampalam, N. Gordon, Beyond the Kalman Filter: Particle Filters for Tracking Applications,
Artech House, 2004.
[63] M. Rodriguez, S. Ali, T. Kanade, Tracking in unstructured crowded scenes, in: Proceeding of International
Conference on Computer Vision, Kyoto, Japan, September 2009.
[64] M. Rodriguez, I. Laptev, J. Sivic, J. Audibert, Density-aware person detection and tracking in crowds,
in: Proceeding of International Conference on Computer Vision, Barcelona, Spain, November 2011.
[65] M. Rodriguez, J. Sivic, I. Laptev, J.-Y. Audibert, Data-driven crowd analysis in videos, in: Proceeding of
International Conference on Computer Vision, Barcelona, Spain, November 2011.
[66] P. Sabzmeydani, G. Mori, Detecting pedestrians by learning shapelet features, in: Proceeding of Computer
Vision and Pattern Recognition, Minneapolis, Minnesota, USA, 2007.
[67] S. Salti, A. Cavallaro, L. Di Stefano, Adaptive appearance modeling for video tracking: survey and evaluation.
IEEE Trans. Image Process 21 (10) (2012) 4334–4348.
[68] R. Schapire, Y. Singer, Improved Boosting Algorithms Using Conﬁdence-Rated Predictions, Machine
Learning, 1999.
[69] K. Smith, S. Ba, J.-M. Odobez, D. Gatica-Perez, Tracking the visual focus of attention for a varying number
of wandering people. IEEE Trans. Pattern Anal. Mach. Intell. 30 (7) (2008) 1–17.

582
CHAPTER 19 Multi-Target Tracking in Video
[70] S. Stalder, H. Grabner, L.V. Gool, Cascaded conﬁdence ﬁltering for improved tracking-by-detection, in:
Proceeding of European Conference on Computer Vision, Crete, Greece, September 2010.
[71] B. Stenger, T. Woodley, R. Cipolla, Learning to track with multiple observers, in: Proceeding of Computer
Vision and Pattern Recognition, Washington, DC, USA, June 2009.
[72] H.-I. Suk, A. Jain, S.-W. Lee, A network of dynamic probabilistic models for human interaction analysis,
IEEE Trans. Circ. Syst. Video Technol. 21 (7) (2011) 932–945.
[73] M. Taj, A. Cavallaro, Multi-camera track-before-detect, in: Proceeding of Conference on Distributed Smart
Cameras, Como, Italy, September 2009.
[74] M. Taj, A. Cavallaro, Recognizing Interactions in Video, Intelligent Multimedia Analysis for Security
Applications, vol. 282/2010, Springer, 2010.
[75] M. Taj, A. Cavallaro, Distributed and decentralized multi-camera tracking, IEEE Signal Process. Mag. 28
(3) (2011) 46–58.
[76] C. Tomasi, T. Kanade, Detection and tracking of point features, Technical Report, CMU-CS-91-132, Carnegie
Mellon University, April 1991.
[77] O. Tuzel, F. Porikli, P. Meer, Pedestrian detection via classiﬁcation on Riemannian manifolds, IEEE Trans.
Pattern Anal. Mach. Intell. 30 (10) (2008) 1–15.
[78] J. Vermaak, A. Doucet, P. Perez, Maintaining multi-modality through mixture tracking, in: Proceeding of
International Conference on Computer Vision, Nice, France, October 2003.
[79] P. Viola, M. Jones, D. Snow, Detecting pedestrians using patterns of motion and appearance, International
J. Comput. Vis. 63 (2) (2005) 153–161.
[80] B.-N. Vo, B.-T. Vo, N.-T. Pham, D. Suter, Joint detection and estimation of multiple objects from image
observations, IEEE Trans. Signal Process. 58 (10) (2010) 5129–5241.
[81] A. Waibel, T. Schultz, M. Bett, M. Denecke, R. Malkin, I. Rogina, R. Stiefelhagen, Smart: the smart meeting
room task at ISL, in: Proceeding of Conference on Acoustics, Speech, and Signal Processing, Hong Kong,
China, April 2003.
[82] B. Wu, R. Nevatia, Detection and tracking of multiple, partially occluded humans by bayesian combination
of edgelet based part detectors, Int. J. Comput. Vis. 75 (2) (2007) 247–266.
[83] B. Yang, R. Nevatia, Multi-target tracking by online learning of non-linear motion patterns and robust
appearance model, in: Proceeding of Computer Vision and Pattern Recognition, Providence, Rhode Island,
USA, June 2012.
[84] M. Yang, F. Lv, W. Xu, Y. Gong, Detection driven adaptive multi-cue integration for multiple human tracking,
in: Proceeding of IEEE International Conference on Computer Vision, Kyoto, Japan, September 2009.
[85] A. Yilmaz, O. Javed, M. Shah, Object tracking: a survey, Journal ACM Comput. Survey 38 (4) (2006) 1–45.
[86] Q. Yu, G. Medioni, Motion pattern interpretation and detection for tracking moving vehicles in airborne
videos, in: Proceeding of Computer Vision and Pattern Recognition, Miami, FL, USA, June 2009.
[87] L. Zhang, Y. Li, R. Nevatia, Global data association for multi-object tracking using network ﬂows, in:
Proceeding of Computer Vision and Pattern Recognition, Anchorage, Alaska, USA, June 2008.
[88] T. Zhao, R. Nevatia, Tracking multiple humans in crowded environments, in: Proceeding of Computer Vision
and Pattern Recognition, Washington, DC, USA, July 2004.
[89] B. Zhou, X. Wang, X. Tang, Random ﬁeld topic model for semantic region analysis in crowded scenes from
tracklets, in: Proceeding of Computer Vision and Pattern Recognition, Colorado Springs, USA, June 2011.
[90] Q. Zhu, S. Avidan, M.-C. Yeh, K.-T. Cheng, Fast human detection using a cascade of histograms of oriented
gradients, in: Proceeding of Computer Vision and Pattern Recognition, New York, USA, June 2006.

20
CHAPTER
Compressive Sensing for Video
Applications
Ashok Veeraraghavan, Aswin C. Sankaranarayanan, and Richard G. Baraniuk
Department of ECE, Rice University, Houston, TX, USA
4.20.1 Introduction
Nyquist sampling theorem: The design of conventional sensors is based heavily on the Shannon-
Nyquist sampling theorem which states that a signal x band-limited to W Hz is determined completely
by its discrete time samples provided the sampling rate is greater than 2W samples per second. This
theorem is at the heart of modern signal processing since it enables signal processing in the discrete time
or digital domain without any loss of information. However, for many applications, the Nyquist sampling
rate is high as well as redundant and unnecessary. As a motivating example, consider a modern high
resolution digital camera. A 10 mega-pixel camera, in effect, takes 10 million linear measurements of the
scene. Yet, almost immediately after capture, redundancies in the image are exploited to compress the
image signiﬁcantly, often by compression ratios of 100:1 for visualization and even higher for detection
and classiﬁcation tasks. This suggests immense redundancies in the overall design of the conventional
camera.
BeyondNyquist:Compressivesensing(CS)referstoasamplingparadigmwhereadditionalstructure
on the signal is exploited to enable sub-Nyquist sampling rates. The structure most commonly associated
with CS is that of signal sparsity in a transform basis. As an example, the basis behind most image
compression algorithms is that images are sparse (or close to sparse) in transform bases such as wavelets
and DCT. In such a scenario, a CS camera takes under-sampled linear measurements of the scene. Given
these measurements, the image of the scene is recovered by searching for the image that is sparsest
in the transform basis (wavelets or DCT) while simultaneously satisfying the measurements. This
search procedure can be shown to be convex. Much of CS literature revolves around the design of
linear measurement matrices, characterizing the number of measurements required and the design of
image/signal recovery algorithms.
4.20.1.1 A brief tour of compressive sensing
Notation: For the sequel, we use the following notation. Measurement matrices are denoted by 
and individual rows of the measurement matrix are denoted using φ. Frames of the video are denoted
using x and compressive measurements by y. We use the “subscript” notation of indexing with time.
For example, the video frame at time t as xt. Sequences are represented as follows: a:b denotes the
sequence {a, a + 1, . . . , b −1, b}.
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00020-0
© 2014 Elsevier Ltd. All rights reserved.
583

584
CHAPTER 20 Compressive Sensing for Video Applications
N
M
K-sparse
y
y
Φ
Θ
Ψ
S
(a)
(b)
S
x
=
=
FIGURE 20.1
(a) Compressive sensing measurement process with a random Gaussian measurement matrix  and discrete
cosine transform (DCT) matrix . The vector of coefﬁcients s is sparse with K = 4. (b) Measurement process
with y = x. There are four columns that correspond to nonzero si coefﬁcients; the measurement vector y is
a linear combination of these columns. Figure courtesy [1].
Compressive sensing [2–4] enables reconstruction of sparse signals from under-determined set of
linear measurements.1 A vector s is termed K-sparse if it has at most K non-zero components, or
equivalently, if ∥s∥0 ≤K, where ∥·∥0 is the ℓ0 norm or the number of non-zero components. Consider a
signal (for example, an image or a video) x ∈RN, which is sparse in a basis , that is, s ∈RN, deﬁned
as s = T x, is sparse. Examples of the sparsifying basis  for images includes DCT and wavelets. The
main problem of interest is that of sensing the signal x from linear measurements. With no additional
knowledge about x, N linear measurements of x are required to form an invertible linear system. In a
conventional digital camera, an identity sensing matrix is used so that each pixel is sensed directly. For
sensing reﬂectance ﬁelds, optimal linear sensing matrices have been designed [5].
The theory of compressive sensing shows that it is possible to reconstruct x from M linear mea-
surements even when M ≪N by exploiting the sparsity of s = T x. Consider a measurement vector
y ∈RM obtained using an M × N measurement matrix , such that
y = x + e = s + e = s + e,
(20.1)
where e is the measurement noise (see Figure 20.1) and  = . The components of the measurement
vector y are called the compressive measurements or compressive samples. For M < N, estimating x
from the linear measurements is an ill-conditioned inverse problem. However, when x is K sparse in
the basis , then CS enables recovery of s (or alternatively, x) from M ∝K measurements, for certain
classes of matrices .
The guarantees on the recovery of signals extend to the case when s is not exactly sparse but
compressible. A signal is termed compressible if its sorted transform coefﬁcients delay according to
power-law, i.e., the sorted coefﬁcient of s decay rapidly in magnitude [6].
1There are many excellent tutorial style articles now that provide an introduction to compressive sensing [1,7,8]. We refer
the interested reader to these.

4.20.1 Introduction
585
Restricted isometry property (RIP): A key criterion for recovery of signals is that no two K-sparse
vectors be mapped to the same measurement vector, i.e., for any pair of K-sparse vectors, x1 and
x2, ∥s1 −s2∥2 = ∥(s1 −s2)∥2 > 0. Given that the difference (s1 −s2) is 2K-sparse, a necessary
condition for recovery of K-sparse vectors is that no 2K-sparse vector lies in the null-space of .
A stronger notion of this condition leads to the so-called restricted isometry property (RIP) [3,9].
We say that a measurement matrix  satisfy RIP for S-sparse signals with constants 0 < δS < 1 if
for all K-sparse vectors s
(1 −δS)∥s∥2
2 ≤∥s∥2
2 ≤(1 + δ)∥s∥2
2.
(20.2)
Stable recovery is guaranteed when (20.2) is satisﬁed for S = 2K; this automatically guarantees that
no two K-sparse vectors are mapped to same measurement vector.
In particular, when  is a ﬁxed basis, it can be shown that using a randomly generated sub-Gaussian
measurement
matrix
,
ensures
that

satisﬁes
RIP
with
a
high
probability
provided
M = O(K log (N/K)). Typical choices for  (or equivalently, ) are matrices whose entries are
independently generated using the Radamacher or the sub-Gaussian distribution. Other choices that
satisfy RIP include sub-sampled Fourier matrices; it can be shown that such matrices satisfy RIP when
M = O(K log4 N) [10]. For large-scale problems, the fast operations inherent to Fourier transforms
make them a computationally efﬁcient alternative to random matrices which become cumbersome to
store and implement. Other popular alternatives for measurement matrices include sub-sampling and
permutations of Hadamard matrices and noiselets [11].
Signal recovery: Estimating sparse vectors that satisfy the measurement equation of (20.1) can be
formulated as the following ℓ0-norm optimization problem:
(P0) : min ∥s∥0 s.t. ∥y −s∥2 ≤ϵ,
(20.3)
with ϵ being a bound for the measurement noise e in (20.1). The ℓ0-norm of a vector s, ∥s∥0, counts
the number of non-zero entries in the vector. Solving (P0) is a NP-hard problem. However, we can
replace that ℓ0-norm with its convex relaxation, the ℓ1-norm. This allows us to reformulate (P0) as one
of ℓ1-norm minimization.
(P1) : ˆs = arg min ∥s∥1 s.t. ∥y −s∥≤ϵ.
(20.4)
A key question to address is the equivalence of the two problems. Speciﬁcally, we wish to understand
the conditions when the solution to (P1) is the same as that of (P0).
ℓ0 −ℓ1 equivalence: To understand when the convex problem (P1) deﬁned in (20.4) solves the non-
convex problem (P0) in (20.3), we ﬁrst need to characterize problems in terms of their difﬁculty. Speciﬁ-
cally, consider δ = M/N, the undersampling factor and ρ = K/M, the sparsity-to-measurements level.
Intuitively, as δ →1, M →N, the undersampling factor approaches the Nyquist sampling rate and
we can expect the (P1) to succeed. Similarly, as ρ →0, K ≪M which reduces the number of free
parameters in the solution thereby making the problem easier, and we expect the (P1) to succeed. We
can study the equivalence of the solutions of (P0) and (P1) as a function of ρ and δ.
It turns out that, especially at large values of N, there is a sharp transition between regimes
where ℓ0 −ℓ1 equivalence fails and holds [12]. Figure 20.2 characterizes this in the noiseless (ϵ = 0),

586
CHAPTER 20 Compressive Sensing for Video Applications
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
ρ
δ
FIGURE 20.2
Phase transition lines depicting ℓ0 −ℓ1 equivalence for three different ensembles of sparse vectors: (red)
non-negative sparse vectors; (blue) signed sparse vectors; and (green) sparse vectors that take values of pm
1. In each setting, ℓ0 −ℓ1 equivalence holds only for (ρ, δ) pairs below the corresponding phase transition
line. Figure courtesy [13]. (For interpretation of the references to color in this ﬁgure legend, the reader is
referred to the web version of this book.)
asymptotic setting N →∞for Gaussian measurement matrices. With high probability, for all (ρ, δ)
pairs above the phase transition, ℓ1-recovery fails and for all (ρ, δ) pairs below the phase transition,
the ℓ0 −ℓ1 equivalence holds. There has been a signiﬁcant amount of work in characterizing the phase
transition under other settings including ﬁnite dimensional problems, under presence of measurement
noise, various measurement ensembles, different sparse vector ensembles as well as different recovery
algorithms; the interested reader is referred to [14] for details on these. A key ﬁnding is that the behavior
of the ℓ0 −ℓ1 equivalence as a function of ρ and δ remains much the same for the non-asymptotic, but
large N scenario. A notable difference is that the transition is no longer sharp; yet, it is centered around
the phase transition at the asymptotic case.
Recovery guarantees for compressible signals: In the context of signals that are not exactly K-sparse,
the solution of (P1) gives a good approximation to the best K-term approximation of the signal. Let
us consider an instance of (20.1) where ∥e∥2 ≤ϵ and the sensed signal s is not exactly K-sparse. Let
sK is the best K-sparse approximation of s. In this context, if the measurement matrix satisﬁes RIP for
2K-sparse signals with isometry constant δ2K <
√
2 −1, then (P1) recovers [2] an estimate ˆs that
satisﬁes
∥ˆs −s∥2 ≤Cϵ +
1
√
K
∥s −sK ∥1.

4.20.1 Introduction
587
Here, C and D are both constants that are dependent only on δ. This suggests that, even for non-
sparse signals, the performance of the ℓ1-recovery algorithm is bounded only by the model error, which
is given by the error to the K-term approximation of the signal.
Sparse optimization algorithms: There exist a wide range of algorithms that solve (P1) to various
approximations or reformulations. These can be grouped into two main classes of algorithms.
Convex methods: A large class of methods solve the convex problem outlined in (20.4) and its refor-
mulations using optimization techniques that exploit the speciﬁc structure of the problem. Examples of
such techniques include basis pursuit (BP) [15], iterative soft thresholding (IST) [16], ﬁxed point contin-
uation (FPC) [17], and approximate message passing (AMP) [13]. Convex methods are among the most
precise for solving the l1-recovery problem. However, in many instances, the convergence to the solution
can be slow requiring a large amount of time before the problem is solved to the desired precision.
Greedy pursuit: Greedy algorithms iteratively estimate the sparse vector by actively selecting, updat-
ing and pruning the support of the signal; as their suggests, the criterion for support update is typically the
solution of a greedy selection process. Examples of greedy algorithms include matching pursuit (MP)
[18], orthogonal matching pursuit (OMP) [19], CoSaMP [20], and iterated hard thresholding (IHT) [21].
Greedy techniques are often computational efﬁcient and come with strong theoretical guarantees on
convergence. In most cases their performance, in terms of accuracy to the true solution lags behind that
of convex techniques. Yet, greedy techniques offer signiﬁcant computational advantages. In addition
to this, an alluring property of greedy methods is that we can easily incorporate other problem-speciﬁc
properties such as structured support patterns into the recovery algorithm. An example of this is the
model-based CoSaMP algorithm [22].
In addition to these, there are a large of class of optimization methods that rely on non-convex
optimization methods as well as Bayesian techniques. The interested reader is referred to [23] for a
detailed survey dedicated to sparse optimization techniques.
4.20.1.2 Video compressive sensing
Problem statement: Our goal is to recover a video x1:T = {x1, x2, . . . , xT } from linear measurements
of the form
Y = A(x1:T ) + noise.
We discuss examples of the measurement operator A in the next section. At this point, we do not impose
any additional constraints on the form of the measurement process other than the following:
•
Linear: Linearity of light is assumed since we are not dealing with coherent imaging.
•
Non-negative: Each entry of the linear operator A is non-negative.
•
Passive: Passive sensing suggests that photons cannot be ampliﬁed and hence, each entry of the
linear operator is less than 1.
In addition to this, we consider scenario of compressive sensing; i.e., the number of measurements,
equal to the number of entries in Y, is signiﬁcantly smaller the dimensionality of the video.
Causality/ephemeral nature of time: A key distinction of the video sensing problem is the funda-
mental difference between time and space. The ephemeral nature of time poses signiﬁcant limitations
on the measurement process; clearly, we cannot obtain newer measurements of an event after it has
happened.

588
CHAPTER 20 Compressive Sensing for Video Applications
Redundancies in videos: The key to circumventing the challenges raised by the ephermeral nature of
time is the use of signal models for videos. Our motivation stems from the success of video compression.
The intuition here is that most videos are highly redundant and compressible and require very few bits
for storage; this implies that the number of degrees of freedom for a typical video is signiﬁcantly lesser
than its ambient dimensionality. While this is true for images as well, in the case of videos we can expect
to get a much more signiﬁcant level of compression. A key point to note is that sensing is fundamentally
different from compression. Sensing operates with no a priori knowledge of the video. In contrast,
compression algorithms have access to the original frames of the video which can be used to estimate
useful construct such as motion ﬂow between frames.
Organization:Therestofthemanuscriptisdevotedtoalgorithmicapproachesandimagingplatforms
that enable CS for various video models.
4.20.2 Imaging architectures
4.20.2.1 Classiﬁcation of imaging architectures
The basic requirement for compressive sensing video acquisition systems is an imaging architecture that
performs programmable multiplexing. Ideally, we would like to create a multiplexing matrix A, which
is incoherent with respect to the basis in which videos are sparse (e.g., wavelet basis). Unfortunately,
creating such arbitrarily complex mixing matrices are prohibitively expensive. Fortunately, multiplexing
matrices that are non-ideal are still very useful and result in signiﬁcant performance improvements over
traditional imaging systems. Depending upon the nature of the multiplexing achieved in a particular
compressive video camera, the multiplexing can be classiﬁed into one (or more) of the following
categories:
•
Spatial Multiplexing Cameras (SMC): Imaging architectures that result in mixing only in space
are called Spatial Multiplexing Cameras (SMC). In SMC’s the multiplexing matrix performs only
spatial mixing, i.e., each observed frame (or set of measurements) yi(t), is a mixing of different
video voxels at that same instant, i.e., yi(t0) = 
u,v Wi(u, v, t0)V (u, v, t0). Therefore, there is no
mixing across time in SMC’s.
•
Temporal Multiplexing Cameras (TMC): Imaging architectures that result in mixing only in time are
called Temporal Multiplexing Cameras (TMC). In TMC’s the multiplexing matrix performs only
time-domain mixing, i.e., each observed frame (or set of measurements) yi, is a mixing of different
video voxels at that same pixel, i.e., yi(u0, v0) = 
t Wi(u0, v0, t)V (u0, v0, t). Therefore, there
is no mixing across space in TMC’s.
•
Spatio-Temporal Multiplexing Cameras (STMC): Imaging architectures that result in mixing both in
space and in time are called Spatio-Temporal Multiplexing Cameras (STMC). STMC’s are the most
general form of linear mixing that can be accommodated in compressive video sensing architectures.
In STMC’s each observed measurement yi, is a general mixing of different video voxels i.e., yi =

u,v,t Wi(u, v, t)V (u0, v0, t). Of course, in a speciﬁc imaging architecture incorporating STMC,
there may be other architecture speciﬁc constraints on the mixing coefﬁcients W that result in a
mixing matrix A.

4.20.2 Imaging Architectures
589
•
Local mixing: In compressive video cameras, that achieve local mixing, each observation
yi(u0, v0, t0) is only composed of voxels in the neighborhood of the voxel, i.e.,
y(u0, v0, t0) =
u0+δu,v0+δv,t0+δt

u0−δu,v0−δv,t0−δt
W(u, v, t)V (u, v, t).
(20.5)
Since several natural imaging architectures preserve locality in their transfer functions (both in space
and time), this results in local mixing. Local mixing comes with both advantages and disadvantages.
Since the mixing is local, both noise and error propagation tend to be local, i.e., noise or errors made
at a particular voxel does not affect voxels that are far away signiﬁcantly. But since local mixing
restricts the choice of mixing matrices and usually results in mixing matrices that are partially
coherent with the sparisfying basis, this results in sub-optimal reconstruction performance.
•
Global mixing: When the spatio-temporal extent of mixing is large in a compressive video camera
then it is referred as global mixing. Global mixing provides the advantage of ability to select mixing
matrices that are fairly incoherent with the sparsifying basis, but sometimes results in long-range
error propagation that is objectionable.
4.20.2.2 Single pixel camera
One of the ﬁrst architectures for compressive imaging is the Rice Single Pixel Camera (SPC) [24]. In
the SPC architecture, shown in Figure 20.3, the scene is focussed onto a spatial light modulator (digital
micro-mirror device array or DMD) using the main objective lens in the imaging system. The DMD
which is at the conjugate plane to the image plane, is composed of approximately 1000 × 1000, array
of micro-mirrors. Each of the micro-mirrors can be individually controlled to reﬂect the incoming light
into one of two directions. In one of the two directions (shown as white in this ﬁgure), the reﬂected light
is then redirected and focused onto a single photo-detector shown in the Figure 20.3. The light reﬂected
into the other direction is discarded. Each observation recorded at the photo-detector is thus a dot-product
betweentheincomingimageandthespatialpatternontheDMD.ThespatialpatternontheDMDisvaried
at about 10 kHz and thus 104 linear measurements are obtained at the photo-detector every second. If we
would like to capture a video at say 1 Hz, then the scene is assumed to be static or changing slowly and
these 104 linear measurements serve as the compressed observations. Notice that the SPC architecture
is an example of a Spatial Multiplexing Camera and involves global mixing. Each observation recorded
at the photo-detector consists only of spatial mixing, i.e., yi(t0) = 
u,v Wi(u, v, t0)V (u, v, t0) where
Wi(u, v, t0), is the binary image projected on the DMD at time t0. Further, since each observation yi
contains information from the entire ﬁeld of view (spatially), we consider this an example of global
mixing.
4.20.2.3 Flutter shutter video camera
Flutter Shutter or Coded Exposure was ﬁrst introduced by Raskar et al. [25] in order to tackle the
problem of motion blur in images. The ﬂutter shutter camera consisted of a global exposure control
either achieved by an external global shutter (typically ferro-electric) or by controlling the sensor
integration in electronics. The problem with a tradition box function for the exposure duration was that
the corresponding point-spread function was low pass thereby severely attenuating the high frequency

590
CHAPTER 20 Compressive Sensing for Video Applications
FIGURE 20.3
Rice single pixel camera. Figure courtesy [24].
textures in the captured image to such an extent that even deconvolution could not completely recover
high frequency texture. The goal of ﬂutter shutter was to make the point-spread function of motion blur
broadband so that high frequency information is still contained in the captured image thereby allowing
deconvolution algorithms to effectively recover the texture. Unfortunately, ﬂutter shutter requires the
knowledge of both motion magnitude and motion direction in order to perform the deconvolution.
Recently, Veeraraghavan et al. [26] and Holloway et al. [27] have proposed extending the ﬂutter shutter
camera to video by changing the exposure duration independently for each frame. In such an architecture,
each captured frame is a summation of a subset of the high speed frames within the exposure duration
as shown in Figure 20.4. In the example shown in Figure 20.4, the total exposure duration of the FSVC
camera is 8 times larger than the high speed video that we wish to recover. Thus, there are 8 sub-frames
within each exposure and an independent 8-bit binary sequence controls the exposure duration. Shown
in Figure 20.4 is an example 8-bit code in which 5 are “1” corresponding to shutter being open. This
results in a captured image, which is a summation of those 5 selected sub-frames. Each subsequent
captured frame corresponds to an independent known binary exposure code. The FSVC architecture is
an example of a temporal multiplexing camera since there is no spatial multiplexing in this architecture.
Further, the architecture results in local mixing since each observation is composed of only voxels
within a small neighborhood.
4.20.2.4 Voxel sub-sampling camera
Motivated by the progress in video upsampling and video super-resolution, voxel sub-sampling based
architectures have been proposed and have found to be very effective for compressive sensing of videos.
Two examples of architectures for voxel sub-sampling are the ﬂexible voxels proposed by Gupta et al.
[28] and the coded photograph exposure video (CPEV) proposed by Hitomi et al. [29]. Voxel sub-
sampling was achieved using a co-located projector-camera system (shown in Figure 20.5) in [28],
while voxel sub-sampling was achieved via a Liquid Crystal on Silicon (LCoS) spatial light modulator
in [29]. Both these implementations suffer from signiﬁcant limitations: (a) projector-camera systems are

4.20.2 Imaging Architectures
591
Integrate sub - frames during exposure
Blurry image captured 
using a conventional 
low-speed camera
Encoded long exposure 
image using FSVC
Dynamic scene
Coded exposure
FIGURE 20.4
Rice ﬂutter shutter video camera. Figure courtesy [27].
FIGURE 20.5
Voxel sub-sampling video camera (ﬂexible voxels) achieved using a co-located projector camera system.
Figure courtesy [28].

592
CHAPTER 20 Compressive Sensing for Video Applications
unable to handle strong ambient illumination, result in large unwieldy imaging system with a low signal
to noise ratio, (b) LCoS based implementation requires a long optical path with beamsplitter resulting in
loss of contrast and light-throughput. Further, both systems require sub-pixel spatial calibration between
the modulator and the imaging array. Inspite of these limitations, the main allure of the architecture
is that it adheres to practical constraints imposed by current sensor technologies and therefore can be
implemented directly on image sensors by making a simple modiﬁcation to the control unit on the
sensors. Once such a modiﬁcation is done to the sensors, then the required modulation is directly done
on the sensor removing the need for external modulator thereby eliminating many of the limitations of
the current implementations. The voxel sub-sampling architecture, results in a reconstruction problem
that resembles hole-ﬁlling in videos, in the sense that at every pixel location we observe the intensity at
a different point in time, which is controlled by the modulator. This architecture, results in local mixing
and is a temporal multiplexing camera since there is no spatial multiplexing.
4.20.2.5 Programmable pixel compressive camera
The imaging architecture, termed programmable pixel compressive camera (P2C2) consists of a normal
25 fps, low resolution video camera, with a high resolution, high frame-rate modulating device such as
a Liquid Crystal on Silicon (LCoS) or a digital micro-mirror device (DMD) array (as shown in Figures
20.6 and 20.7). The modulating device modulates each pixel independently in a pre-determined random
fashion at a rate higher than the acquisition frame rate of the camera. Further, the modulating device may
have a higher spatial resolution that the sensor array as shown in Figure 20.6. Thus, each observed frame
at the camera is a coded linear combination of the voxels of the underlying highspeed video frames. Both
low frame-rate video cameras and high frame-rate amplitude modulators (DMD/LCoS) are inexpensive
and this results in signiﬁcant cost reduction. Further, the capture bandwidth is signiﬁcantly reduced due
to P2C2s compressive imaging architecture. This architecture can be used to implement both FSVC and
voxel sub-sampling by the careful choice of modulation pattern on the LCoS or DMD modulator. In
fact, Hitomi et al. use a similar architecture to implement voxel sub-sampling. P2C2 acts as a temporal
multiplexing camera when the spatial resolution of the modulator and the spatial resolution of the
sensor are identical. When the spatial resolution of the modulator is larger than the spatial resolution
of the sensor array, P2C2 acts as a spatio-temporal multiplexing camera. Further, since only voxels in
a neighborhood are multiplexed, the P2C2 architecture also results in local mixing.
4.20.2.6 Coded aperture video camera
Coded aperture imaging systems were originally pioneered as a replacement for pinhole imaging in
astronomical imaging [30], primarily to increase the light throughput in such systems. It was shown
that modiﬁed uniformly redundant arrays (MURA) codes were optimal in terms of reconstruction
performance. Recently, Veeraraghavan et al. [31] and Levin et al. [32] extended these coded aperture
techniques to work in lens based consumer imaging applications. In both these applications, the key
insight is to make the out-of-focus point spread function broadband so that the resulting deconvolution
problem is well posed. More recently, Marcia et al. [33,34] and Harmany et al. [35] have extended
this to develop coded aperture video cameras for compressive video sensing. In this architecture, each
observed image is a low resolution version of a convolution between the current frame and the code
in the aperture at that instant. Since, typically the size of the spatial point spread function is small,

4.20.2 Imaging Architectures
593
FIGURE 20.6
Programmable pixel compressive camera architecture. Figure courtesy [36].
FIGURE 20.7
Programmable pixel compressive camera. Figure courtesy [36].
the corresponding multiplexing is local. Further, in this architecture, there is no temporal multiplexing,
thereby making it an example of spatial multiplexing camera.
4.20.2.7 Hyper-spectral video camera
Brady and colleagues at Duke University have pioneered compressive hyperspectral imagers and com-
pressive hyper-spectral video cameras that typically use a diffraction grating or a prism to perform the
spectral separation, followed by a static or dynamic amplitude mask that performs the modulation before
being sensed on a monochrome sensor array [37–41]. The key idea in these architectures is that a prism
or a diffraction grating separates the spectral components so that the images at different wavelengths
are shifted at the intermediate image-plane. A spatial amplitude mask at this intermediate image plane
performs spectrally selective coding at each pixel, since the hyper-spectral data has been sheared by
the prism/diffraction grating. The light after modulation is then focussed back to a sensor array through

594
CHAPTER 20 Compressive Sensing for Video Applications
relay optics. The multiplexing is most of these systems is local both in space and in spectrum. Further,
this is an example of spatial (and spectral) multiplexing since there is no multiplexing across time.
4.20.2.8 Common multiplexing constraints
All the compressive video sensing architectures discussed above use some sort of spatial light modulator
to accomplish the multiplexing required. There are three common constraints on the multiplexing matrix
that arise from the physical constraints of these spatial light modulators.
Positivity constraint: Since all the designs that we considered involved the use of incoherent light
and modulators, the elements of the multiplexing matrix are constrained to be all positive.
Binary modulation: Most of the spatial light modulators that we have considered so far such as the
digital micro-mirror array device and the liquid crystal on silicon modulator can only achieve binary
modulation, that this they can either allow the light to pass through the rest of the system or block the
light. This implies that each element of the corresponding multiplexing matrix is constrained to be either
α or 0, where α ≤1.
Energy constraint: Since all the video compressive sensing architectures that we have discussed so
far are all passive, there is also an energy constrained on the multiplexing matrix which ensures that the
sum of each column of the multiplexing matrix is less than 1.
4.20.2.9 Comparison of architectures
The various compressive video sensing architectures presented so far can be compared in terms of two
important attributes:
•
Hardware complexity: Among the hardware architectures presented, the ﬂutter shutter video camera
is probably the simplest hardware architecture since it is already a feature available in almost all
CMOS sensors and is available in several machine vision cameras such as the Pointgrey Dragonﬂy2.
The voxel sub-sampling architecture adheres to practical constraints imposed by current sensor
technologies and therefore can be implemented directly on image sensors by making a simple
modiﬁcation to the control unit on the sensors. Once such a modiﬁcation is done to the sensors, then
the required modulation is directly done on the sensor removing the need for external modulator
thereby eliminating many of the limitations of the current implementations.
•
Conditioning of the multiplexing matrix: The conditioning of the resultant mixing matrix is directly
related to the design freedom in the choice of the multiplexing matrices. The FSVC architecture
allows only local temporal multiplexing and therefore results in multiplexing matrices that are poorly
conditioned compared to most other architectures. Thus, in practive while compressive video sensing
architectures such as P2C2, voxel sub-sampling and single-pixel camera can provide compression
ratios of the order of 20–30, the FSVC architecture can only achieve a compression ratio of around
10 in practice.
4.20.3 Signal models and algorithms
We now describe different reduced-order models for videos and discuss pros and cons for each model.

4.20.3 Signal Models and Algorithms
595
4.20.3.1 Wavelet sparsity
Wavelet-based representation have long been used for image compression. The core idea behind this is
that most images are highly compressible in a wavelet representation; i.e., most of the energy of signal is
compacted into a few wavelet coefﬁcients and the remainder of the coefﬁcients take very small values.
The same ideas can be adapted to 3D signals such as videos.
In the context of compressive sensing, wavelet sparsity priors are applied as follows. We seek to
solve the following optimization problem:
min ∥(x1:T )∥0 s.t. ∥Z −A(x1:T )∥2 ≤ϵ.
(20.6)
Here, we denote the 3D wavelet transformation using the operator (·), Z, and A(·) are the compressive
measurements and the measurement operator respectively. Recall, from Section 4.20.1.2, that the choice
of the measurement operator depends on the imaging architecture.
The ℓ0-norm used in (20.6) to promote sparsity can be replaced with the ℓ1-norm, its convex relation.
This leads to the following optimization problem:
min ∥(x1:T )∥1 s.t. ∥Z −A(x1:T )∥2 ≤ϵ.
(20.7)
As mentioned earlier in Section 4.20.1, there are many specialized algorithms for solving this problem
and its reformulation/variants. We refer the interested reader to the following resource [42].
Pros: Wavelet models are fast and easy to use. Most programming languages provide inbuilt imple-
mentations of traditional wavelets. A second advantage is that there are many fast solvers for the
optimization problem in (20.7).
Cons: A key drawback of wavelet-based representation is that it treats temporal variations of the
video much like its spatial variations. Clearly, the temporal behavior of a video is highly inﬂuenced by
physics of motion such as inertia. Wavelets, which capture spatial variations to remarkable accuracy,
are too general to capture the highly structured temporal variations. As a consequence, videos are in
general not very sparse in wavelet bases.
4.20.3.2 Total variation
A nice property exhibited by videos of natural images is that of sparse gradients. We can characterize
this using the total variation (TV) norm.
T V (x) =

u,v

(xx(u, v))2 + (xy(u, v))2,
(20.8)
where x(u, v) is the intensity of the image x at pixel location (u, v), xx, and xy are the image gradients
along x and y directions respectively. The TV norm can be intuitively understood as an ℓ1-norm on
the magnitude of image gradients; much like the ℓ1-norm it is sparsity promoting as well albeit for
the gradients. We can easily extend the deﬁnition of TV norm to 3D signals like videos by adding a
temporal gradient as well.
A key property of the TV norm is that it is a convex function. This is very useful in formulating
tractable recovery problem in the context of CS. Indeed, we can recovery videos from their compressive

596
CHAPTER 20 Compressive Sensing for Video Applications
measurements by solving the following optimization problem:
min T V (x1:T ) s.t. ∥Z −A(x1:T )∥2 ≤ϵ.
(20.9)
In essence, the optimization problem in (20.9) attempted to ﬁnd a video with sparse gradients that
satisﬁes the measurement constraints. Given that the optimization problem is convex and hence, its
solution is tractable and there do exist specialized solvers for (20.9). However, there has not been much
progress in terms of specialized solvers capable of solving large scale problems.
Pros: Total variation is often a better model for video CS especially at low measurement rates or
equivalently, higher compressions.
Cons: Total variation norm, much like sparse wavelet priors, fails to treat the temporal statistic of
the video differently from its spatial statistics. This misses out on redundancies, that stem from inertia
of motion, in temporal statistics; as a consequence, much like wavelets, minimum TV norm-based
optimization does require a large number of compressive measurements (as compared to models we
discuss next). A second drawback is that, unlike sparse priors, minimum TV norm optimization has not
been well studied and its theoretical performance, especially in the context of compressive recovery
problems, is largely unknown.
4.20.3.3 Dictionary learning
Dictionary models extend traditional transform basis sparse representations to data dependent priors.
Given a training dataset of videos, we can learn a dictionary or overcomplete basis that offer sparser
representations than conventional transform bases such as wavelets and DCT.
The problem formulation underlying dictionary learning can be broadly stated as follows. Given
training data {v1, v2, . . . , vQ}, vi ∈RN of a signal (say video patches), we are interested in learning
a dictionary D ∈RN×T such that there exists a sparse vector si ∈RT that satisﬁed vi ≈Dsi. Letting
V = [v1 v2 · · · vQ] and S = [s1 · · · sQ], we can write the above problem deﬁnition as the following
optimization problem:
min
D,S ∥V −DS∥2 + λ

k
∥si∥1.
The optimization problem above is bilinear in D and S and hence, non-convex. However, it is convex
in both D and S individually. Most approaches exploit this property to solve for D and S iteratively.
The interested reader is referred to a popular dictionary learning technique called K-SVD [43].
Pros: Dictionaries enables tailoring the recovery algorithm to speciﬁc classes of videos. The key
promise of dictionaries is in enabling very sparse representations as compared to wavelet or DCT
models.
Cons: Training even reasonable sized dictionaries required enormous amount of training data as well
as computational resources for the actual training algorithm. In addition to this, recovery algorithms
employing dictionaries have to cope with the high storage as well as processing costs in using them.
4.20.3.4 Motion ﬂow
State of the art video compression algorithms critically rely on motion estimation and motion com-
pensation to achieve very high compression without signiﬁcant loss in quality. Motion compensation

4.20.4 Existing Systems for Video Compressive Sensing
597
reduces the number of free variables (degrees of freedom) in a video by registering frames of the video
against a common reference; this is very much similar to registering frames so that a time-varying scene
is converted to a static one.
Under brightness constancy, motion ﬂow is well approximated by optical ﬂow which has had an
illustrious history and rich literature in computer vision problems [44,45]. Given two images, say x1
and x2, optical ﬂow provides per-pixel correspondences of the form:
x1(x0, y0) = x2(x0 + u0, y0 + v0).
(20.10)
Here, (u0, v0) is the optical ﬂow at pixel location (x0, y0) in image x1. These relationships can be written
as linear expressions of the components of x1 and x2. Hence, any of the optimization problems stated
earlier can be modiﬁed with these additional constraints.
Pros: Motion ﬂow is at the heart of state-of-the-art video compression techniques. In that sense,
they are most promising in delivering high quality video reconstructions even at very high levels of
compression. Optical ﬂow estimation has matured signiﬁcantly over the last few years with many
publicly available toolboxes [45–47].
Cons: Getting high quality motion estimates requires high quality video estimates. The two are highly
coupled with each other; this is a chicken-and-egg problem that is requires specialized techniques to
resolve. In addition to this, motion ﬂow computation (especially optical ﬂow) can be time consuming
especially for high resolution videos.
4.20.4 Existing systems for video compressive sensing
4.20.4.1 Coded strobing camera
The coded strobing camera [26] is an example of ﬂutter shutter video camera architecture adapted
to image high-speed videos of periodic/quasi-periodic phenomena. Periodic signals are all around us.
Several human and animal biological processes such as heart-beat, breathing, vocal fold vibrations,
several cellular processes, industrial automation processes and everyday objects such as hand-mixer
and blender all generate periodic processes. The key insight of the coded strobing camera is the fact
that signals that are periodic or quasi-periodic are extremely sparse in the Fourier basis. Since the
temporal signal at every pixel is extremely sparse in the Fourier basis, obtaining a limited number of
linear observations of the signal using a ﬂutter shutter video camera provides enough information to
accurately and efﬁciently reconstruct the high speed videos. Shown in Figure 20.8 (top) is the signal
model and the observation model for a single pixel. The authors use basis pursuit denoising the recover
the Fourier coefﬁcients from the under-determined set of linear equations at each pixel. Figure 20.8
(bottom) shows an example of 80× temporal super-resolution on a tool bit rotating at 3000, 6000,
9000, and 12,000 rpm respectively. The shutter operates at 2000 Hz and so the reconstructed videos are
obtained at 2000 Hz, eventhough the video camera is recording data only at about 25 fps.
4.20.4.2 Single pixel camera
There have been many video recovery algorithms designed speciﬁcally for the single pixel camera (SPC)
[48–57]. Recall that the SPC is a pure spatial multiplexing camera with no temporal multiplexing. At

598
CHAPTER 20 Compressive Sensing for Video Applications
FIGURE 20.8
Top: Observation model shows the capture process of the coded strobing camera. Bottom: (a)–(d) Tool bit
rotating at different rpm captured using coded strobing: top row shows the coded images acquired by a PGR
Dragonﬂy2. Figure courtesy [26].
each instant, we obtain a single linear measurement of the scene. Given that the scene under observation
is time-varying, consecutive compressive measurements are obtained from a slightly different scene.
Yet, as we accumulate measurements over larger time intervals, the motion blur induced by the scene
motion can become signiﬁcant; as a consequence, failure to model the motion in the scene during the
measurement acquisition process invariably leads to poor performance [52].
We discuss three broad algorithms that have been proposed to handle this challenge.
Multi-scale video recovery: Park and Wakin [50,51] observe that the perception of motion blur is
heavily dependent on the spatial resolution of the video. Speciﬁcally, for a given scene, reducing its
spatial resolution lowers the error caused by the static-scene assumption. Simultaneously, decreasing
the spatial resolution reduces the dimensionality of the individual video frames. Both observations
build the foundation of the multi-scale recovery approach, where several compressive measurements

4.20.4 Existing Systems for Video Compressive Sensing
599
are acquired at multiple scales for each video frame. The recovered video at coarse scales (low spatial
resolution) is used to estimate motion, which is then used to boost the recovery at near scales (high
spatial resolution).
Linear dynamical systems: Another model for videos that has been very popular and successful is the
linear dynamical systems. Linear dynamical systems have been shown to be very effective ways of mod-
eling videos containing dynamic textures such as ﬁre, clouds, water, etc. They have also been very good
at modeling the ﬂow of trafﬁc in videos and also in modeling the hyper-spectral datacube. Linear dynam-
ical systems, in the context of video CS, have received much attention. Observability of LDSs from
compressive measurements has been studied in [58]; here, sufﬁcient conditions are derived for identiﬁ-
cation of the state sequence given compressive measurements of the state sequence. Sankaranarayanan
et al. [53] have developed a new framework for video CS that models the evolution of the scene as a
linear dynamical system (LDS). Given the measurements from the SPC architecture, the video recovery
problem is reduced to ﬁrst estimating the model parameters of the LDS from compressive measurements,
and then reconstructing the image frames. They exploit the low-dimensional dynamic parameters (the
state sequence) and high-dimensional static parameters (the observation matrix) of the LDS to devise
a novel compressive measurement strategy that measures only the dynamic part of the scene at each
instant and accumulates measurements over time to estimate the static parameters. This enables them
to lower the compressive measurement rate considerably. Figure 20.9 shows reconstructions obtained
by the method on a variety of videos and a hyper-spectral dataset.
Motion-ﬂow models: Sankaranarayanan et al. [52] extend the multi-scale recovery process proposed
in [50] with the construction of special dual-scale measurement matrices. The speciﬁc matrices proposed
hereexhibitdifferentpropertiesattwodifferentspatialscales:atalowerspatialscales,thematrixisclose-
to-optimal for least-squares recovery; and at the full resolution, the matrix enables random projections
for recovery using ℓ1 programs. A recovery algorithm is proposed that exploits the special structure
of the measurement matrix. First, a low spatial resolution recovery is obtained simply by least-square
recovery. The scenes optical ﬂow is estimated from the low spatial resolution video and feed it into a
convex optimization algorithm to recover the high-resolution video. This works under the assumption
that motion ﬂow images are inherently of low spatial resolution and hence, can be upsampled without
much error.
4.20.4.3 Coded aperture snapshot spectral imager (CASSI)
CASSI is a video-rate spectral imaging system that captures the spatial and spectral properties of a
time varying scene [38,39,41]. Each frame of the video is a hyper-spectral cube that describes the
spatio-spectral content of the scene. Each frame is also reconstructed separately; in this sense, CASSI is
a snapshot imager. The imaging architecture of CASSI involves a coded aperture followed by a double
Amici prism which introduces a wavelength-dependent spatial shift onto the incident light. Light from
thesceneisﬁrstfocusedontothecodedaperture;arandombinarypatternisusedfortheaperture.Second,
a bandpass ﬁlter is introduced that selects the spectral wavelengths of interest. Third, a double Amici
prism is used to introduce a wavelength-dependent shift onto the incident light. Finally, a high-spatial
resolution sensor with a broadband spectral response is used to image. The shift introduced by the double
Amici prisms imply that each pixel on the sensor integrates over different spatio-spectral locations of the
incident hyper-spectral cube. The presence of the coded aperture introduces randomness in the weights

600
CHAPTER 20 Compressive Sensing for Video Applications
FIGURE 20.9
(a)–(c) A gallery of reconstruction results using the CS-LDS framework on three different videos. (d) Recon-
struction of a hyper-spectral datacube with 2301 spectral bands; the image at each spectral band has
128 × 64 pixels. Figure courtesy [59].
associated at each sensor pixel. Each sensor image now can be modeled as a linear measurement of
the hyper-spectral cube. Recovery of the hyper-spectral cube is done using a novel algorithm called
NeAREst that solves for the hyper-spectral cube at multiple spatio-spectral cube.
4.20.4.4 Coded exposure video camera
Hitomi et al. [29] have developed a video compressive sensing camera which exploits the characteristics
of modern CMOS sensor arrays to enhance the temporal resolution of captured videos. Modern CMOS
image sensors can provide pixel-wise control of the exposure duration if a small modiﬁcation is made
to the control unit. But this is possible only if each pixel has a single continuous exposure instead of
multiple exposures. The start time and the length of the exposure duration can change for each pixel
independently. Keeping these (and other practical) constraints in mind, they develop a compressive
sensing video camera with the voxel sub-sampling architecture. They implement the architecture by
using an LCoS array as the modulator since direct modiﬁcations to the control unit on the CMOS sensors

4.20.4 Existing Systems for Video Compressive Sensing
601
FIGURE 20.10
Experimental results from the CPEV video CS camera. First column: input coded exposure images. Numbers
in parentheses denote the camera integration time. Second column: close-ups illustrate the coded motion
blur. Third to ﬁfth columns: the reconstructions maintain high spatial resolution. Notice the spatial details
inside the eye. Figure courtesy [29].
is not straightforward. Once the compressed sensed data is acquired, they perform reconstruction by
using sparse regularization on over-complete dictionaries. They learn dictionaries for video patches from
a gallery of videos, and use the orthogonal matching pursuit for reconstruction. Figure 20.10 shows an
example of a reconstructed video acquired from their camera at a 9× temporal super-resolution. Notice
the reconstructed spatial detail in the eye.
4.20.4.5 Flexible voxels
Gupta et al. [28] have developed the ﬂexible voxels compressive sensing video camera, which is based on
the voxel sub-sampling architecture. The goal is to allow post-capture ﬂexibility in the choice of spatial
and temporal resolution. With this goal in mind they pose the problem of ﬁnding the best voxel sub-
sampling function as a binary integer program. The solution of the binary integer program provides the
optimal voxel sub-sampling pattern, optimal in terms of ability to choose the spatio-temporal resolutions
post-capture. Using the motion information in the captured data, the correct resolution for each location
is decided automatically. Our techniques make it possible to capture fast moving objects without motion
blur, while simultaneously preserving high-spatial resolution for static scene parts within the same video
sequence. Figure 20.11 shows an example of a scene containing static as well as fast moving regions.
A high spatial resolution camera captured the static portions of the scene well but results in motion
blur on the moving regions. A high frame-rate camera results in loss of spatial resolution, even in the
static regions of the scene. With the ﬂexible voxels camera, high spatial resolution is obtained both on
the static and the dynamic regions of the scene. In the example shown, they achieve 16× compression,
retaining image ﬁdelity.
4.20.4.6 Programmable pixel compressive camera (P2C2)
Reddy et al. [36] developed the programmable pixel compressive video camera in which each pixel
has an independent coded exposure. They implemented a prototype of the system using an LCoS array
shown in Figure 20.6. In their system, the camera was recording videos at about 23 fps, while the LCoS
modulator was operating at about 184 fps. Each pixel in the camera was independently exposure coded

602
CHAPTER 20 Compressive Sensing for Video Applications
High Spatial Resolution
High Temporal Resolution 
Flexible Voxels
FIGURE 20.11
Different samplings of the space-time volume: for conventional video cameras, the sampling of the space-
time volume is decided before the scene is captured. Given a ﬁxed voxel budget, a high spatial resolution
(SR) camera results in large motion blur and aliasing. A high-speed camera results in low SR even for the
static/slow-moving parts of the scene. With ﬂexible voxels, the spatio-temporal resolution can be decided
post-capture, independently at each location in a content-aware manner. Figure courtesy [28].
FIGURE 20.12
Results on P2C2 LCoS prototype: two images from normal 23 fps camera and four recovered images are
shown. Notice that motion blur is signiﬁcantly reduced even though the camera is operating only at 23 fps.
Figure courtesy [36].
and several resulting images were captured. In order to perform the reconstruction of the high speed
video from the captured samples, they used a optical ﬂow (or brightness constancy) enhanced recovery
procedure. First, they assumed that the video has low spatial resolution, but high temporal resolution
and reconstructed a preview estimate based on this assumption. The preview estimate is then upsampled
and optical ﬂow is computed on the preview video. The optical ﬂow then serves as additional brightness

4.20.5 Discussion
603
FIGURE 20.13
Results of ﬂutter shutter video camera on a video of a toy bike translating with uniform velocity using
Total Variation based reconstruction. The top row shows one frame of the reconstructed video for various
compression factors. As the compression factor increases, the output degrades gracefully. The bottom two
rows show the rotated XT and the YT slices corresponding to the column and row marked yellow and green in
the ﬁrst row. The XT and YT slices clearly show the quality of the temporal upsampling. Figure courtesy [27].
(For interpretation of the references to color in this ﬁgure legend, the reader is referred to the web version
of this book.)
constancy constraints, leading to enhanced recovery in the next step of the reconstruction. Further,
they also regularized the reconstruction using a sparse prior on the wavelet coefﬁcients of the video.
Figure 20.12 shows an example result obtained from their prototype. Notice that the painted numbers
on the fan are blur-free in the reconstructed videos. On a related note, similar ideas have also been to
sense videos using MRI. The interested reader is referred to [60] for more details.
4.20.4.7 Flutter shutter video camera (FSVC)
Holloway et al. [27] have shown that the FSVC architecture can go beyond a restricted class of
periodic/quasi-periodic scenes (as in [26]). They have shown that the FSVC architecture can han-
dle arbitary motion in scenes as long as the compression factor that is desired is less than about 10.
Each exposure of the sensor is temporally coded using an independent pseudo-random sequence. Such
exposure coding is easily achieved in modern sensors and is already a feature of several machine vision
cameras. They show two algorithms for reconstructing the high speed video; the ﬁrst based on min-
imizing the total variation of the spatio-temporal slices of the video and the second based on a data
driven dictionary based approximation. Figure 20.13 shows an example result using their total varia-
tion based reconstruction algorithm. Notice that the reconstructed XT and YT slices clearly show the
improvements in temporal resolution.
4.20.5 Discussion
In this book chapter we provided a brief introduction to the theory of compressive sensing and described
several examples of its application to tackling resolution and bandwidth related issues in video capture.

604
CHAPTER 20 Compressive Sensing for Video Applications
Flutter 
shutter 
Parabolic 
motion 
Invertible 
motion 
SPC 
CS-LDS Coded 
strobing
P2C2 
CPEV 
Coded 
rolling 
shutter 
FSVC  
Bandwidth 
reduction 
No 
No 
No 
Yes 
Yes 
Yes 
Yes 
Yes 
Yes 
Yes 
Motion 
model 
Linear 
Linear 
Linear 
None 
LDS 
Periodic 
None 
None 
Linear 
None 
Side 
information 
Motion 
magnitude 
Motion 
direction 
None 
None 
None 
None 
None 
None 
None 
None 
Hardware 
complexity 
Simple 
Simple 
Simple 
Complex Complex Simple Complex Medium 
Medium 
Simple 
Commercial 
availability 
Yes 
No 
Yes 
No 
No 
Yes 
No 
No 
No 
Yes 
Light 
throughput 
50% 
100% 
50% 
50% 
50% 
50% 
1/(Comp 
factor) 
- 
60% 
Typical 
compression 
shown* 
1 
1 
1 
FIGURE 20.14
A comparison of several video CS algorithms/architectures.
Figure 20.14 provides an overview comparison of many of the methods discussed in this book chapter.
This ﬁeld of compressive video sensing remains an active research area with several publications
appearing in the major conferences and journals of the ﬁeld every year. We encourage the interested
reader to look at the more recent conference and journal proceedings to keep abreast of this rapidly
evolving area of research.
Acknowledgments
The authors gratefully acknowledge support from NSF Grants NSF-IIS:1116718, NSF-CCF:1117939 and by a
research grant through the Samsung GRO program.
References
[1] R. Baraniuk, Compressive sensing, IEEE Signal Process. Mag. 24 (4) (2007) 118–120, 124.
[2] E. Candès, J. Romberg, T. Tao, Robust uncertainty principles: exact signal reconstruction from highly incom-
plete frequency information, IEEE Trans. Inform. Theory 52 (2) (2006) 489–509.
[3] E. Candès, T. Tao, Near optimal signal recovery from random projections: universal encoding strategies?
IEEE Trans. Inform. Theory 52 (12) (2006) 5406–5425.
[4] D. Donoho, Compressed sensing, IEEE Trans. Inform. Theory 52 (4) (2006) 1289–1306.
[5] Y.Y. Schechner, S.K. Nayar, P.N. Belhumeur, Multiplexing for optimal lighting, IEEE Trans. Pattern Anal.
Mach. Intell. 29 (8) (2007) 1339–1354.

References
605
[6] J. Haupt, R. Nowak, Signal reconstruction from noisy random projections, IEEE Trans. Inform. Theory 52
(9) (2006) 4036–4048.
[7] E.J. Candès, M.B. Wakin, An introduction to compressive sampling, IEEE Signal Process. Mag. 25 (2)
(2008) 21–30.
[8] Massimo Fornasier, Holger Rauhut, Compressive sensing, in: Handbook of Mathematical Methods in Imag-
ing, vol. 1, Springer, 2011, pp. 187–229.
[9] M. Davenport, J. Laska, P. Boufouons, R. Baraniuk, A simple proof that random matrices are democratic,
Technical Report TREE 0906, Rice University, ECE Department, November 2009.
[10] E. Candès, T. Tao, Decoding by linear programming, IEEE Trans. Inform. Theory 51 (12) (2005) 4203–4215.
[11] R. Coifman, F. Geshwind, Y. Meyer, Noiselets, Appl. Comput. Harm. Anal. 10 (1) (2001) 27–44.
[12] D.L. Donoho, J. Tanner, Precise undersampling theorems, Proc. IEEE 98 (6) (2010) 913–924.
[13] D.L. Donoho, A. Maleki, A. Montanari, Message-passing algorithms for compressed sensing, Proc. Natl.
Acad. Sci. USA 106 (45) (2009) 18914–18919.
[14] A. Maleki, D.L. Donoho, Optimally tuned iterative reconstruction algorithms for compressed sensing, IEEE
J. Sel. Top. Signal Process. 4 (2) (2010) 330–341.
[15] S. Chen, D. Donoho, M. Saunders, Atomic decomposition by basis pursuit, SIAM J. Sci. Comput. 20 (1)
(1998) 33–61.
[16] I. Daubechies, M. Defrise, C. De Mol, An iterative thresholding algorithm for linear inverse problems with
a sparsity constraint, Commun. Pure Appl. Math. 57 (11) (2004) 1413–1457.
[17] E. Hale, W. Yin, Y. Zhang, A ﬁxed-point continuation method for ℓ1-regularized minimization with appli-
cations to compressed sensing, Technical Report TR07-07, Rice University, CAAM Department, 2007.
[18] S. Mallat, Z. Zhang, Matching pursuits with time-frequency dictionaries, IEEE Trans. Signal Process. 41
(12) (1993) 3397–3415.
[19] J. Tropp, Greed is good: algorithmic results for sparse approximation, IEEE Trans. Inform. Theory 50 (10)
(2004) 2231–2242.
[20] D. Needell, J. Tropp, CoSaMP: iterative signal recovery from incomplete and inaccurate samples, Appl.
Comput. Harm. Anal. 26 (3) (2009) 301–321.
[21] T. Blumensath, M. Davies, Iterative hard thresholding for compressive sensing, Appl. Comput. Harm. Anal.
27 (3) (2009) 265–274.
[22] R. Baraniuk, V. Cevher, M. Duarte, C. Hegde, Model-based compressive sensing, IEEE Trans. Inform.
Theory 56 (4) (2010) 1982–2001.
[23] J. Tropp, S. Wright, Computational methods for sparse solution of linear inverse problems, Proc. IEEE 98
(6) (2010) 948–958.
[24] M.F. Duarte, M.A. Davenport, D. Takhar, J.N. Laska, T. Sun, K.F. Kelly, R.G. Baraniuk, Single-pixel imaging
via compressive sampling, IEEE Signal Process. Mag. 25 (2) (2008) 83–91.
[25] R. Raskar, A. Agrawal, J. Tumblin, Coded exposure photography: motion deblurring using ﬂuttered shutter,
ACM Trans. Graphics 25 (3) (2006) 795–804.
[26] A. Veeraraghavan, D. Reddy, R. Raskar, Coded strobing photography: compressive sensing of high speed
periodic events, IEEE Trans. Pattern Anal. Mach. Intell. 33 (4) (2011) 671–686.
[27] J. Holloway, A. Sankaranarayanan, A. Veeraraghavan, S. Tambe, Flutter shutter video camera for compressive
sensing of videos, in: IEEE Int. Conf. Comput. Photogr., 2012.
[28] M. Gupta, A. Agrawal, A. Veeraraghavan, S. Narasimhan, Flexible voxels for motion-aware videography,
in: Eur. Conf. Comput. Vis., Crete, Greece, September 2010.
[29] Y. Hitomi, J. Gu, M. Gupta, T. Mitsunaga, S.K. Nayar, Video from a single coded exposure photograph using
a learned over-complete dictionary, in: IEEE Int. Conf. Comput. Vis., Barcelona, Spain, November 2011.
[30] E.E. Fenimore, T.M. Cannon, Coded aperture imaging with uniformly redundant arrays, Appl. Opt. 17 (3)
(1978) 337–347.

606
CHAPTER 20 Compressive Sensing for Video Applications
[31] A. Veeraraghavan, R. Raskar, A. Agrawal, A. Mohan, J. Tumblin, Dappled photography: mask enhanced
cameras for heterodyned light ﬁelds and coded aperture refocusing, ACM Trans. Graphics 26 (3) (2007) 69.
[32] A. Levin, R. Fergus, F. Durand, W.T. Freeman, Image and depth from a conventional camera with a coded
aperture, in: ACM Trans. Graphics, vol. 26, 2007, p. 70.
[33] R. Marcia, Z. Harmany, R.M. Willett, Compressive coded aperture imaging, in: Proc. SPIE Symp. Electron.
Imag. Comput. Imag., San Jose, CA, 2009.
[34] R. Marcia, R.M. Willett, Compressive coded aperture video reconstruction, in: Proc. Eur. Signal Process.
Conf. (EUSIPCO), 2008.
[35] Z.T. Harmany, R.F. Marcia, R.M. Willett, Spatio-Temporal Compressed Sensing with Coded Apertures and
Keyed Exposures, preprint arXiv:1111.7247, Arxiv, 2011.
[36] D. Reddy, A. Veeraraghavan, R. Chellappa, P2C2: programmable pixel compressive camera for high speed
imaging, in: IEEE Conf. Comput. Vis. Pattern Recog., Colorado Springs, CO, USA, June 2011.
[37] D.J. Brady, Optical Imaging and Spectroscopy, Wiley Online Library, 2009.
[38] M.E. Gehm, R. John, D.J. Brady, R.M. Willett, T.J. Schulz, Single-shot compressive spectral imaging with
a dual-disperser architecture, Opt. Express 15 (21) (2007) 14013–14027.
[39] A. Wagadarikar, R. John, R. Willett, D. Brady, Single disperser design for coded aperture snapshot spectral
imaging, Appl. Opt. 47 (10) (2008) 44–51.
[40] A. Wagadarikar, N.P. Pitsianis, X. Sun, D.J. Brady, Spectral image estimation for coded aperture snapshot
spectral imagers, in: Proc. SPIE, vol. 7076, 2008, p. 707602.
[41] A. Wagadarikar, N.P. Pitsianis, X. Sun, D.J. Brady, Video rate spectral imaging using a coded aperture
snapshot spectral imager, Opt. Express 17 (8) (2009) 6368–6388.
[42] Rice DSP compressive sensing webpage. <http://dsp.rice.edu/cs/>.
[43] M. Aharon, M. Elad, A. Bruckstein, K-svd: an algorithm for designing overcomplete dictionaries for sparse
representation, IEEE Trans. Signal Process. 54 (11) (2006) 4311–4322.
[44] B.K.P. Horn, B.G. Schunck, Determining optical ﬂow, Artif. Intell. 17 (1–3) (1981) 185–203.
[45] D. Sun, S. Roth, M.J. Black, Secrets of optical ﬂow estimation and their principles, in: IEEE Conf. Comput.
Vis. Pattern Recog., San Francisco, CA, USA, June 2010.
[46] T. Brox, J. Malik, Large displacement optical ﬂow: descriptor matching in variational motion estimation,
IEEE Trans. Pattern Anal. Mach. Intell. 33 (3) (2011) 500–513.
[47] C. Liu, Beyond pixels: exploring new representations and applications for motion analysis, PhD Thesis,
Mass. Inst. Tech., 2009.
[48] V. Cevher, A.C. Sankaranarayanan, M.F. Duarte, D. Reddy, R.G. Baraniuk, R. Chellappa, Compressive
sensing for background subtraction, in: Eur. Conf. Comput. Vis., Marseille, France, October 2008.
[49] J.E. Fowler, Block-based compressed sensing of images and video, Found. Trends Signal Proc. 4 (4) (2010)
297–416.
[50] J.Y. Park, M.B. Wakin, A multiscale framework for compressive sensing of video, in: Pict. Coding Symp.,
Chicago, IL, USA, May 2009.
[51] J.Y. Park, M.B. Wakin, Multiscale algorithm for reconstructing videos from streaming compressive mea-
surements, 2012 (preprint).
[52] A.C. Sankaranarayanan, C. Studer, R.G. Baraniuk, CS-MUVI: video compressive sensing for spatial-
multiplexing cameras, in: IEEE Int. Conf. Comput. Photogr. (ICCP), 2012.
[53] A.C. Sankaranarayanan, P. Turaga, R. Baraniuk, R. Chellappa, Compressive acquisition of dynamic scenes,
in: Eur. Conf. Comput. Vis., Crete, Greece, September 2010.
[54] N. Vaswani, Kalman ﬁltered compressed sensing, in: IEEE Conf. Image Process., San Diego, CA, USA,
October 2008.
[55] N. Vaswani, W.Lu. Modiﬁed-cs, Modifying compressive sensing for problems with partially known support,
in: Int. Symp. Inf. Theory, June 2009.

References
607
[56] M.B. Wakin, J.N. Laska, M.F. Duarte, D. Baron, S. Sarvotham, D. Takhar, K.F. Kelly, R.G. Baraniuk,
Compressive imaging for video representation and coding, in: Pict. Coding Symp., Beijing, China, April
2006.
[57] A.E Waters, A.C. Sankaranarayanan, R.G. Baraniuk, SpaRCS: recovering low-rank and sparse matrices from
compressive measurements, in: Adv. Neural Inf. Proc. Syst., December 2011.
[58] M.B. Wakin, B.M. Sanandaji, T.L. Vincent, On the observability of linear systems from random, compressive
measurements, in: IEEE Conf. Decision Control, December 2010.
[59] CS-LDS project webpage. <http://www.ece.rice.edu/∼as48/research/cslds>.
[60] M. Salman Asif, Lei Hamilton, Marijn Brummer, Justin Romberg, Motion-adaptive spatio-temporal regu-
larization (master) for accelerated dynamic MRI, Magn. Reson. Med. (2012).

21
CHAPTER
Virtual Vision for Camera
Networks Research
Faisal Z. Qureshi* and Demetri Terzopoulos†
*University of Ontario Institute of Technology, Canada
†University of California, Los Angeles, USA
4.21.1 Introduction
Multi-camera systems are rapidly evolving from highly specialized wired networks of stationary passive
and active cameras that provide visual coverage of the scene to ad hoc wireless networks of smart
camera nodes, capable of near-autonomous operation in a variety of applications, such as urban and
participatory sensing, disaster response, plant and animal habitat monitoring, etc. Whereas traditional
multi-camera systems focus primarily on wide-area scene analysis, smart camera networks are also
concerned with camera coordination and control, in-network processing and storage, and resource-
aware visual analysis. Pre-recorded video, while useful, is inadequate in the study of camera control
and coordination strategies. Rather, one needs online access to the entire network in order to control
and study its behavior under different sensing regimes. This observation, together with the fact that
most researchers who are motivated to study camera networks do not have access to physical camera
networks of suitable complexity, led us to propose the “Virtual Vision” paradigm for camera networks
research (see Figure 21.1).
4.21.1.1 Virtual vision
Virtual vision advocates employing visually and behaviorally realistic 3D virtual environments, popu-
lated with lifelike, self-animating objects (pedestrians, automobiles, etc.), to carry out camera networks
research. Camera networks are simulated in these environments by deploying virtual cameras that mimic
the characteristics of physical cameras. Virtual vision offers several advantages over the use of physical
camera networks during the ideation, prototyping, and evaluation phases of camera networks research,
among them:
•
The virtual vision simulator runs on (high-end) commodity PCs, obviating the need to grapple with
special-purpose hardware.
•
Thevirtualcamerasareveryeasilyinstantiated,relocated,andreconﬁguredinthevirtualenvironment.
•
The virtual world provides readily accessible ground-truth data for the purposes of algorithm/system
validation.
•
Experiments are perfectly repeatable in the virtual world, so we can easily modify algorithms and/or
their parameters and immediately determine the effect.
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00021-2
© 2014 Published by Elsevier Ltd. All rights reserved.
609

610
CHAPTER 21 Virtual vision for Camera Networks Research
Machine Vision  
Algorithms: 
Tracking, pedestrian 
recognition etc. 
High-level Processing: 
Camera control, 
assignment, handover, 
etc. 
Synthetic video feed
Camera Model:
Pan, tilt, zoom, camera
jitter, color response,
lens distortion, etc.
Environment models:  geometry, 
texture, illumination
Pedestrian models:  appearance,
movement, behavior
Reality Emulator
FIGURE 21.1
The virtual vision paradigm.
•
Our simulated camera networks run in “real time” within the virtual world, with the virtual cameras
actively controlled online by the vision algorithms. By prolonging virtual-world time relative to
real-world time, we can evaluate the competence of computationally expensive algorithms, thereby
gauging the potential payoff of accelerating them through more efﬁcient software and/or dedicated
hardware implementations.
Espousing our virtual vision paradigm, we have developed novel camera control strategies that enable
simulated camera nodes to collaborate both in tracking pedestrians of interest that move across the
ﬁelds of view (FOV) of different cameras and in capturing close-up videos of pedestrians as they travel
through a designated area. These virtual camera networks demonstrate the advantages of the virtual
vision paradigm in designing, experimenting with, and evaluating prototype large-scale surveillance
systems. Speciﬁcally, we have studied control and collaboration problems that arise in camera networks
by deploying simulated networks within a virtual train station and a virtual ofﬁce space. Our simulated
networks exhibit performance characteristics similar to those of physical camera networks; e.g., video
compression artifacts, latency, limited bandwidth, communication errors, camera node failures, etc.
An important issue in camera network research is the comparison of camera control algorithms.
Simple video capture sufﬁces for gathering benchmark data from time-shared physical networks of
passive, ﬁxed cameras, but gathering benchmark data for networks that include any smart, active PTZ
cameras requires scene reenactment for every experimental run, which is almost always infeasible when
many human subjects are involved. Costello et al. [1], who compared various schemes for scheduling
an active camera to observe pedestrians, ran into this hurdle and resorted to Monte Carlo simulation
to evaluate camera scheduling approaches. They concluded that evaluating scheduling policies on a
physicaltestbedcomprisingevenasingleactivecameraisextremelyproblematic.Byofferingconvenient
and limitless repeatability, our virtual vision approach provides a useful alternative to physical active
camera networks for experimental purposes.
Nevertheless, skeptics may argue that virtual vision relies on simulated data, which can lead to inac-
curate results. Fretting that virtual video lacks all the subtleties of real video, some may cling to the
dogma that it is impossible to develop a working machine vision system using simulated video. However,

4.21.2 Related Work
611
our high-level camera control routines do not directly process any raw video. Instead, these routines are
realistically driven by data supplied by low-level recognition and tracking routines that mimic the per-
formance of a state-of-the-art pedestrian localization and tracking system, including its limitations and
failure modes. This enables us to develop and evaluate camera network control algorithms under realistic
simulated conditions consistent with physical camera networks. We believe that the ﬁdelity of our virtual
vision emulator is such that algorithms developed through its use will readily port to the real world.1
4.21.1.2 Overview
The remainder of this article is organized as follows: We review related work in the next section.
Section 4.21.3 reviews the two virtual environments that we have employed in our work—a 3D recon-
struction of a train station and a 3D model of a ﬂoor of an ofﬁce tower. We then describe in Section 4.21.4
the camera networks that we have developed using these virtual vision simulators. Section 4.21.5 con-
cludes this article with a summary.
4.21.2 Related work
Preceding virtual vision, a closely related software-based approach to facilitating active vision research
was proposed, called animat vision [2], which prescribed eschewing the hardware robots that are typ-
ically used by computer vision researchers in favor of biomimetic artiﬁcial animals (animats) situated
in physics-based virtual worlds. Salgian and Ballard describe another early use of virtual reality sim-
ulation, which employed synthetic video imagery as seen from the driver’s position of a simulated car
cruising the streets of a virtual town [3], in order to develop a suite of visual routines running in a
real-time image processor to implement an autonomous driving system.
Rabie and Terzopoulos demonstrated their animat vision approach by implementing biomimetic
active vision systems for artiﬁcial ﬁshes and for virtual humans [4]. Their active vision systems com-
prised algorithms that integrate motion, stereo, and color analysis to support robust color object tracking,
vision-guided navigation, visual perception, and obstacle recognition and avoidance abilities. Together,
these algorithms enabled the artiﬁcial animal to sense, understand, and interact with its dynamic virtual
environment. The animat vision approach appeared to be particularly useful for modeling the powerful
vision systems found in animals. Furthermore, it obviated the need to grapple with physical hardware—
cameras, robots, and other paraphernalia—at least during the initial stages of research and development,
thereby yielding substantial savings in money and time to acquire and maintain the hardware. The algo-
rithms developed within the animat vision approach were subsequently adapted for use in a mobile
vehicle tracking and trafﬁc control system [5], which afﬁrmed the usefulness of the animate vision
approach in designing and evaluating complex computer vision systems.
The virtual vision paradigm for video surveillance systems research was proposed in [6]. Its cen-
tral concept was to design and evaluate video surveillance systems using Reality Emulators, virtual
environments of considerable complexity, inhabited by autonomous, lifelike agents. The workreviewed
1We are currently validating our virtual vision paradigm in a collaborative project with the University of California, Riverside,
through the development of a virtual vision simulator that emulates an existing large-scale physical camera network.

612
CHAPTER 21 Virtual vision for Camera Networks Research
in this chapter realizes that concept within the reality emulator developed by Shao and Terzopoulos
[7,8]—a virtual train station populated with autonomous pedestrians—and by Starzyk et al. [9]—a 3D
ofﬁce ﬂoor inhabited by scripted pedestrians.
In concordance with the virtual vision paradigm, Santuari et al. [10,11] advocate the development
and evaluation of pedestrian segmentation and tracking algorithms using synthetic video generated
within a virtual museum simulator containing scripted characters. Synthetic video is generated via a
sophisticated 3D rendering scheme, which supports global illumination, shadows, and visual artifacts
such as depth of ﬁeld, motion blur, and interlacing. They have used their virtual museum environment
to develop static background modeling, pedestrian segmentation, and pedestrian tracking algorithms.
Their work focuses on low-level computer vision.
By contrast, our work has focused on high-level computer vision issues, especially multi-camera
control in large-scale camera networks, which is a fundamental problem that must be tackled in order
to develop advanced surveillance systems [12–14].
In recent years there has been much interest in distributed algorithms for tracking in camera networks
comprising passive and active cameras. Sankaranarayanan et al. tackle object detection, tracking and
recognition in multi-camera systems [15]. They exploit real-world constraints, such as the presence of
a three-dimensional scene model, consistency of color and texture, etc., for their purposes. Ding et al.
develop a distributed optimization strategy to select the pan, tilt, and zoom settings of multiple active
cameras in order to maximize various scene understanding performance criteria [16]. Song et al. [17]
studied the problem of tracking and activity recognition in distributed camera networks. Refs. [16,17]
are noteworthy; they evaluate the performance of the sensing strategy using a simulation environment,
as well as on a small-scale physical camera network. The simulation environment used is much simpler
then the virtual vision simulator that we have developed, as it does not support the full vision pipeline,
from image acquisition to tracking.
Virtual vision simulators can play a key role in studying collaborative sensing strategies in PTZ
camera networks. For example, a virtual vision simulator can be readily used to compare different
sensing (control) strategies in PTZ camera networks. The only way to compare two sensing (control)
strategies on a physical PTZ camera network is through scene re-enactment, since pre-recorded videos
cannot be used for comparing sensing (control) algorithms. Scene re-enactments clearly have a very
high human cost.
4.21.3 Virtual vision simulators
In this section we describe the 3D environments that we have used in our work. Virtual cameras situated
in these environments capture synthetic video footage, which is then passed on to video analysis routines,
such as background subtraction, blob detection, and pedestrian tracking. High-level camera control and
coordination algorithms rely upon these routines when deciding how best to control a camera network
in order to achieve one or more observation goals.
4.21.3.1 A train station
Our ﬁrst virtual vision simulator was developed using an advanced pedestrian animation system that
combines behavioral, perceptual, and cognitive human simulation algorithms [7]. The simulator

4.21.3 Virtual Vision Simulators
613
(a)
(b)
(c)
(d)
FIGURE 21.2
A large-scale virtual train station populated by self-animating virtual humans [7]. (a) Waiting Room.
(b) Concourses. (c) Arcade. (d) Platforms.
reconstructs the original Penn Station in New York City, which was demolished in 1963 to make
way for the current Penn Station and Madison Square Garden complex (Figure 21.2). The station simu-
lation can efﬁciently synthesize well over 1000 self-animating pedestrians performing a rich variety of
activities in the large-scale indoor urban environment. Like real humans, the synthetic pedestrians are
fully autonomous. They perceive the virtual environment around them, analyze environmental situa-
tions, make decisions, and behave naturally within the train station. They can enter the station, avoiding
collisions when proceeding though portals and congested areas, queue in lines as necessary, purchase
train tickets at the ticket booths in the main waiting room, sit on benches when they feel tired, purchase
food/drinks from vending machines when they feel hungry/thirsty, etc., and proceed from the concourse
area down the stairs to the train platforms if they wish to board a train. A graphics pipeline renders the
busy urban scene with considerable geometric and photometric detail, as shown in Figure 21.2.
4.21.3.2 A ﬂoor of an ofﬁce tower
More recently, we have developed a distributed virtual vision simulator that depicts a ﬂoor in a typical
ofﬁce tower in downtown Toronto [9]. This simulator can animate and render up to 100 pedestrians
at 15 frames per second (fps). Here, pedestrians follow scripted paths as they move around in their
workplace. The simulator features advanced lighting effects, such as sunlight ﬁltering through large
glass windows, static and dynamic objects casting shadows on one another, etc. More importantly,
however, this simulator is highly scalable and can support much larger camera networks than is possible
with the Penn Station virtual vision simulator. In some recent tests we have been able to simulate
networks of 100 + cameras. Figure 21.3 shows images captured by virtual cameras installed on the
simulated ofﬁce ﬂoor.
The simulator is based upon the open-source game engine, Panda3D [18]. Panda3D is a 3D rendering
framework, into which programmers insert 3D models. These 3D models can be inanimate, such as
buildings, furniture, etc., or animated, such as people and vehicles. Many packages are available to
create 3D models that can be imported into Panda3D. Additionally pre-made 3D models, both inani-
mate and animate, can be purchased from multiple vendors who specialize in creating digital assets for

614
CHAPTER 21 Virtual vision for Camera Networks Research
FIGURE 21.3
A view of an upper ﬂoor of a virtual ofﬁce building [9]. The city skyline is visible through ﬂoor-to-ceiling
panoramic windows. Our scripted pedestrians use motion-capture data to simulate realistic motion and they
cast dynamic shadows on their environment.
computer games and movies. Our virtual vision simulator contains the control, sensing, and communi-
cation routines needed to simulate active and passive camera networks. It also contains the algorithms
needed to animate 3D pedestrian models to simulate human trafﬁc in 3D environments. It is relatively
easy to customize our virtual vision simulator to simulate other environments. What is needed is (1)
an appropriate 3D model of the environment and (2) motion scripts for the pedestrians, so that the
pedestrians do not collide with walls and each other.
4.21.3.3 Simulated cameras
Each virtual camera node in the sensor network is able to render the scene from its own vantage
point in order to generate synthetic video suitable for visual surveillance. It is an active sensor that is
able to perform low-level visual processing and it has a repertoire of autonomous camera behaviors.
Furthermore, it is capable of communicating (wirelessly) with nearby nodes in the network. We assume
the following communication model: (1) nodes can communicate with their neighbors, (2) messages
from one node can be delivered to another node if there is a path between the two nodes, and (3) messages
can be broadcast from one node to all the other nodes. Furthermore, we assume the following network
model: (1) messages can be delayed, (2) messages can be lost, and (3) nodes can fail. These assumptions
ensure that our virtual camera network faithfully mimics the important operational characteristics of a
real visual sensor network. Our imaging model can emulate various camera sensing artifacts, such as
video interlacing, camera color response, camera jitter, compression artifacts, and sensor resolution.
4.21.3.4 Visual analysis
We have developed tracking routines that are able to detect, identify and track pedestrians in the synthetic
video feed generated by virtual cameras deployed in our 3D environments. The pedestrian tracker
faithfully mimics the characteristics of a state-of-the-art pedestrian tracker that one might use to track
people in video footage captured by physical cameras. Our tracker, for example, will momentarily fail
due to occlusions, poor illumination, in crowded conditions, or when multiple individuals have a similar
appearance. Our virtual vision simulator, however, affords us the beneﬁt of ﬁne tuning the performance
of this module by taking into consideration the ground truth data readily available in the virtual world.
Thus far, we have employed appearance-based models to track pedestrians (Figure 21.4). Pedestrians

4.21.3 Virtual Vision Simulators
615
(a)
(b)
(c)
FIGURE 21.4
Tracking Pedestrians 1 and 3. Pedestrian 3 is tracked successfully; however, (a) track is lost of Pedestrian
1 who blends in with the background. (b) The tracking routine loses Pedestrian 3 when she is occluded by
Pedestrian 2, but it regains track of Pedestrian 3 when Pedestrian 2 moves out of the way (c).
are either segmented automatically (against a static background) or manually identiﬁed by the operator
to construct an appearance model. Appearance models are then matched across the successive frames to
track pedestrians. A distinctive characteristic of our pedestrian tracking routine is its ability to operate
over a range of camera zoom settings. Note that we do not assume that the active cameras are calibrated.
We have recently been developing a visual analysis pipeline that contains computer vision routines
typically employed in surveillance systems; e.g., automatic camera calibration, pedestrian detection,
tracking, and re-identiﬁcation, face detection and identiﬁcation, and head detection [9]. Figure 21.5a
depicts the visual analysis pipeline responsible for pedestrian detection, segmentation and tracking
in video feeds captured by passive wide FOV cameras. The passive visual analysis pipeline is able to
track multiple pedestrians without any user input. The visual analysis pipeline for active PTZ cameras is
depicted in Figure 21.5b. The PTZ visual analysis pipeline can also track multiple pedestrians; however,
the appearance signatures for those pedestrians must be provided by another camera in the vicinity or
manually by a human operator who can identify a person of interest by drawing a stroke over the camera
image of that individual (Figure 21.7).
It is noteworthy that these visual analysis pipelines have been evaluated on real video footage recorded
by physical cameras (Figure 21.6). This should assuage concerns that the synthetic imagery may lack
the subtlety and richness of real footage, and should enable us to port the camera network software
implemented within our virtual vision simulators to physical camera networks in the real world.
4.21.3.5 Image-driven PTZ zoom and ﬁxation
A PTZ camera can ﬁxate and zoom in on an object of interest. The ﬁxation and zooming routines are
image-driven and do not require camera calibration or any 3D information such as a global frame of
reference. The ﬁxate routine brings the region of interest—e.g., the bounding box of a pedestrian—into

616
CHAPTER 21 Virtual vision for Camera Networks Research
(a)
(b)
FIGURE 21.5
Visual analysis pipelines are realized as a collection of reusable vision routines. (a) Visual analysis pipeline
for tracking pedestrians in PTZ cameras. (b) Visual analysis pipeline for tracking pedestrians in wide-FOV
cameras.
the center of the image by rotating the camera about its local x and y axes. The zoom routine controls
the FOV of the camera such that the region of interest occupies the desired percentage of the image.
4.21.3.6 Behavior-based camera nodes
The camera controller determines the overall behavior of the camera node, taking into account the
information gathered through visual analysis by the vision routines (bottom-up) and the current task (top-
down). We model the camera controller as an augmented hierarchical ﬁnite state machine (Figure 21.8).
In its default state, Idle, the camera node is not involved in any task. It transitions into the Computing-
Relevance state upon receiving a queryrelevance message from a nearby node. Using the description
of the task that is contained within the queryrelevance message, and by employing its visual analysis
routines, the camera node can compute its relevance to the task [14]. For example, it can use visual
search to ﬁnd a pedestrian that matches the appearance-based signature forwarded by the querying node.
The relevance encodes the expectation of how successful a camera node will be at a particular sensing
task. The camera node returns to the Idle state if it fails to compute its relevance because it cannot
ﬁnd a pedestrian matching the description. Otherwise, when the camera successfully ﬁnds the desired

4.21.3 Virtual Vision Simulators
617
FIGURE 21.6
Our visual analysis pipeline is designed from the ground up to work with both synthetic (right) and real
video (left) without any modiﬁcation. Consequently, our vision pipeline faithfully mimics the performance of
a vision pipeline implemented on physical cameras.
FIGURE 21.7
A stroke gesture is provided to select a pedestrian to be tracked in active PTZ cameras. Appearance signatures
computed by passive wide-FOV cameras can also be used to track individuals in active PTZ cameras.
pedestrian, it returns its relevance value to the querying node. The querying node passes the relevance
value to the supervisor node of the group, which decides whether or not to include the camera node in
the group. The camera goes into the PerformingTask state upon joining a group, where the embedded
child ﬁnite state machine hides the sensing details from the top-level controller and enables the node to
handle transient sensing (tracking) failures. All states other than the PerformingTask state have built-in

618
CHAPTER 21 Virtual vision for Camera Networks Research
Idle
PerformTask
Done/ Timeout
Computing Relevance
ComputeRelevance
Done/Timeout
Performing Task
Search
Track
Wait
Acquired
Lost
Acquired
Timeout
Track
Timeout
Done
FIGURE 21.8
The top-level camera controller consists of a hierarchical ﬁnite state machine (FSM). The inset (right)
represents the child FSM embedded within the PerformingTask and ComputingRelevance states in the
top-level FSM.
timers (not shown in Figure 21.8) that allow the camera node to transition into the Idle state rather than
wait indeﬁnitely for a message from another node.
The child FSM (Figure 21.8 (inset)) starts in Track state, where video frames are processed to track
a target without panning and zooming a camera. Wait is entered when track is lost. Here camera zoom
is gradually reduced in order to reacquire track. If a target is not reacquired during Wait, the camera
transitions to the Search state, where it performs search sweeps in PTZ space to reacquire the target.
A camera node returns to its default state after ﬁnishing a task, using the reset routine, which is a
proportional-derivative (PD) controller that attempts to minimize the difference between the current
zoom/tilt settings and the default zoom/tilt settings.
4.21.4 Prototype camera networks
We have used our virtual vision simulators to study the problems of active camera scheduling, collabora-
tive sensing in ad hoc networks of smart active sensors, proactive PTZ camera control, and multi-tasking
PTZ cameras. We have been able to rapidly develop novel camera control strategies to address these
problems by deploying virtual camera networks in our simulated indoor urban environments.
4.21.4.1 Active camera scheduling
In 2005, we introduced a camera scheduling strategy for intelligently managing multiple, uncalibrated
active PTZ cameras, supported by several static, calibrated cameras in order to satisfy the challenging
task of automatically recording close-up biometric videos of pedestrians present in a scene [12]. Our
approach assumes a non-clairvoyant model of the scene, supports multiple cameras, supports preemp-
tion, and allows multiple observations of the same pedestrian.

4.21.4 Prototype Camera Networks
619
To conduct camera scheduling experiments, we populated the virtual train station with up to twenty
autonomous pedestrians, who enter, wander, and exit the main waiting room of their own volition. We
tested our scheduling strategy in various scenarios using anywhere from 1 to 18 PTZ active cameras.
For each trial, we placed a wide-FOV passive camera at each corner of the main waiting room. We also
0 
5 
10 
15 
20 
0
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
1 
Trials 
Success Rate 
NW 
W 
(a)
0 
5 
10 
15 
20 
0
20 
40 
60
80
100
120
140 
Trials 
Times
Times
NW 
W 
(b)
0 
5 
10 
15 
20 
0
20 
40 
60
80
100
120
140 
Trials 
NW 
W 
(c)
0 
5 
10 
15 
20 
0 
20 
40 
60 
80 
100 
120 
140 
160 
Trials 
Wait Times  
NW 
W 
(d)
FIGURE 21.9
Comparisons of Weighted (W) and Non-Weighted (NW) scheduling schemes. The weighted scheduling strat-
egy, which takes into account the suitability of a camera for recording a particular pedestrian, outperforms its
non-weighted counterpart, as is evident from its (a) higher success rates and (b) shorter lead, (c) processing,
and (d) wait times. The displayed results are averaged over several runs of each trial scenario. Trials 1–6
involve 5 pedestrians and 1, 2, 3, 4, 5, and 6 cameras, respectively. Trials 7–12 involve 10 pedestrians and 3,
4, 5, 6, 7, and 8 cameras, respectively. Trials 13–18 involve 15 pedestrians and 5, 6, 9, 10, 11, and 12 cam-
eras, respectively. Trials 19–24 involve 20 pedestrians with 5, 8, 10, 13, 15, and 18 cameras, respectively.

620
CHAPTER 21 Virtual vision for Camera Networks Research
afﬁxed a ﬁsh-eye lens camera to the ceiling of the waiting room. These passive cameras were used to
estimate the 3D location of the pedestrians.
We formulated the multi-camera control strategy as an online scheduling problem and proposed a
solution that combines the information gathered by the wide-FOV cameras with weighted round-robin
scheduling to guide the available PTZ cameras, such that each pedestrian is observed by at least one
PTZ camera while in the designated area. Figure 21.9 compares weighted and non-weighted scheduling
schemes for active PTZ camera assignment.
4.21.4.2 Collaborative persistent surveillance
In [13], we developed a distributed coalition formation strategy for collaborative sensing tasks in camera
sensor networks. The proposed model supports task-dependent node selection and aggregation through
an announcement/bidding/selection strategy combined with a constraint satisfaction problem (CSP)
based conﬂict resolution mechanism. Our technique is scalable as it lacks any central controller, and it
is robust to node failures and imperfect communication. In response to a sensing task, such as, “observe
Pedestrian i during their presence in the region of interest,” wide-FOV passive and PTZ active cameras
organize themselves into groups with the objective of fulﬁlling the task. These groups evolve as the
pedestrian enters and exits the ﬁelds of view of different cameras, ensuring that the pedestrian remains
persistently under surveillance by at least one camera. Figure 21.10 illustrates the 15-min persistent
observation of a pedestrian of interest as she makes her way through the train station. For this example,
we placed 16 active PTZ cameras in the train station, as shown in Figure 21.2.
4.21.4.3 Proactive PTZ control
PTZ camera networks that are able to anticipate future sensing requirements do a better job of man-
aging sensing resources, avoiding assignments that might lead to sensing failures in the future. In [14]
we developed a cognitive PTZ camera network that is able to plan ahead when determining camera
assignments. The reasoning process—which considers both the immediate and the future consequences
of different camera assignments when constructing a “plan” that is most likely to succeed (in a prob-
abilistic sense) at the current observation task(s)—is capable of performing camera assignments and
handoffs in order to provide persistent coverage of a region. In [19], we extended the reasoning engine
with the ability to generalize and store the results of the reasoning process (Figure 21.11). Whenever the
camera network encounters a previously unseen situation, it invokes the planning process to construct
optimal camera assignment. The results of this process are stored as rules in a production system. Later,
when the network again encounters a similar situation, it bypasses the reasoning process and uses the
stored rules to perform camera assignments. Initially, the camera network relies mostly on the reasoning
process; over time, however, camera assignments become instinctive.
4.21.4.4 Multi-tasking PTZ cameras
Reference [20] develops a behavior-based PTZ camera controller that automatically tunes the sensing
parameters of PTZ cameras in response to the scene activity, choosing to capture close-up video when

4.21.4 Prototype Camera Networks
621
(a) Cam 1; 0.5min
(b)
Cam 9; 0.5min
(c)
Cam 7; 0.5min
(d)
Cam 6; 0.5min
(e)
Cam 7; 1.5min
(f)
Cam 7; 2.0min
(g) Cam 6; 2.2min
(h) Cam 6; 3.0min
(i) Cam 2; 3.0min
(j) Cam 7; 3.5min
(k)  Cam 2; 4.0min
(l)
Cam 3; 4.0min
(m)
Cam 6; 4.2min
(n)
Cam 2; 4.3min
(o)
Cam 3; 5.0min
(p)
Cam 3; 6.0min
(q)
Cam 3; 13.0min
(r)
Cam 10; 13.4m
(s)
Cam 11; 14.0min
(t)
Cam 9; 15.0min
FIGURE 21.10
Fifteen-minute persistent observation of a pedestrian of interest as she makes her way through the train
station. (a–d) Cameras 1, 9, 7, and 8 monitoring the station. (e) The operator selects a pedestrian of interest
in the video feed from Camera 7. (f) Camera 7 has zoomed in on the pedestrian, (g) Camera 6, which is
recruited by Camera 7, acquires the pedestrian. (h) Camera 6 zooms in on the pedestrian. (i) Camera 2.
(j) Camera 7 reverts to its default mode after losing track of the pedestrian and is now ready for another
task. (k) Camera 2, which is recruited by Camera 6, acquires the pedestrian. (l) Camera 3 is recruited by
Camera 6; Camera 3 has acquired the pedestrian. (m) Camera 6 has lost track of the pedestrian. (n) Camera
2 observing the pedestrian. (o) Camera 3 zooming in on the pedestrian. (p) Pedestrian is at the vending
machine. (q) Pedestrian is walking towards the concourse. (r) Camera 10 is recruited by Camera 3; Camera 10
is observing the pedestrian. (s) Camera 11 is recruited by Camera 10. (t) Camera 9 is recruited by Camera 10.

622
CHAPTER 21 Virtual vision for Camera Networks Research
(a)
(b)
FIGURE 21.11
(a) The three rows show three cameras observing two pedestrians as they cross each other on their way to
opposite sides of the lobby. The three cameras are able to perform handoff while keeping both pedestrians
in view. This is achieved through a reasoning mechanism that considers both the short-term and long-term
consequences of camera assignments. (b) The virtual vision simulator comprised one VW and four VP modules
spread over three computers.
the number of pedestrians present in the scene is low and electing to capture lower-resolution video as
the number of pedestrians increases, thus continually keeping every pedestrian in view. These cameras
enable the video surveillance system to intelligently respond to scene complexity, automatically captur-
ing close-up imagery of the pedestrians present in the scene when possible, and behaving as wide-FOV
cameras when the number of pedestrians increases. Figure 21.12 shows a multi-tasking PTZ camera:
The PTZ camera is able to capture higher resolution video of the pedestrians in the scene when there are

4.21.5 Conclusion
623
(a)
(b)
FIGURE 21.12
PTZ cameras automatically decide how best to observe a scene. (a) When possible, the PTZ camera selects a
higher zoom to capture higher resolution images of the individuals present in the scene. (b) As the individuals
spread out and move away from the camera, the PTZ camera selects a lower zoom setting to keep all of them
in view, albeit at a lower resolution.
only a few pedestrians present; however, it begins to behave like a wide-FOV camera as the pedestrians
present in the scene spread out and move away from the camera.
4.21.5 Conclusion
Virtual vision is a unique synthesis of virtual reality, artiﬁcial life, computer graphics, computer vision,
and sensor network technologies, with the objective of facilitating computer vision research applied to
human surveillance using camera sensor networks. Through the faithful emulation of physical vision
systems, any researcher can investigate, develop, and evaluate camera sensor network algorithms and
systems in virtual worlds, without having to deal with special-purpose surveillance hardware. We have
demonstrated our prototype surveillance systems in two simulated virtual environments populated by
self-animating pedestrians, which have facilitated our ability to design visual sensor networks and
experiment with them on commodity personal computers.
The future of advanced simulation-based approaches for the purposes of low-cost prototyping and
facile experimentation appears promising and our virtual vision approach will continue to beneﬁt from
long-term efforts to increase the complexity of virtual worlds. Imagine an entire city, including indoor
and outdoor environments, subway stations, automobiles, shops and market places, homes and public
spaces, all richly inhabited by autonomous virtual humans. Such city-scale virtual worlds will one day
provide unprecedented opportunities to develop and assess large-scale camera sensor networks in ways
not yet possible with our current simulators.

624
CHAPTER 21 Virtual vision for Camera Networks Research
Glossary
Smart camera
a self-contained vision system, which includes an image sensor, on-board
processing and storage capabilities, power, and (often wireless) commu-
nication interfaces
Camera network
a network of camera nodes; unlike traditional multi-camera systems,
smart camera networks typically comprise smart camera nodes
Virtual vision
theuseofvisuallyandbehaviorallyrealisticthree-dimensional(3D)envi-
ronments to carry out camera networks research
Reality emulator
a virtual world richly populated with lifelike, self-animating agents
approaching the realism and the complexity of the physical world
Autonomous virtual humans self-animating synthetic human agents whose motor, perception, behav-
ior, and cognition routines enable them to function autonomously in their
simulated environment
Virtual vision simulator
a reality emulator with autonomous virtual humans (pedestrians) that
includes the computer vision and camera network machinery necessary
to simulate active vision systems. A typical virtual vision simulator uses
computer graphics rendering to simulate video capture by passive wide
ﬁeld-of-view(FOV)andactivepan/tilt/zoom(PTZ)cameras,anditincor-
porates computer vision routines that operate upon the synthetic video
streams, providing video analysis capabilities similar to those found in
physical camera networks; e.g., object detection and tracking
Acknowledgments
We thank Wei Shao for developing and implementing the train station simulator and Mauricio Plaza-Villegas for
his valuable contributions. We thank Tom Strat, formerly of DARPA, for his generous support and encouragement.
We also thank Adam Domurad for his work on visual analysis pipelines and Wiktor Starzyk for developing and
implementing the ofﬁce ﬂoor simulator.
References
[1] C.J. Costello, C.P. Diehl, A. Banerjee, H. Fisher, Scheduling an active camera to observe people, in: Proceed-
ings of the ACM International Workshop on Video Surveillance and Sensor Networks, ACM Press, New York,
NY, 2004, pp. 39–45.
[2] D. Terzopoulos, T. Rabie, Animat vision: Active vision in artiﬁcial animals, Videre: J. Comput. Vis. Res. 1
(1) (1997) 2–19.
[3] G. Salgian, D.H. Ballard, Visual routines for autonomous driving, in: Proceedings of the Sixth International
Conference on Computer Vision, Bombay, India, January 1998, pp. 876–882.
[4] T. Rabie, D. Terzopoulos, Active perception in virtual humans, in: Vision Interface (VI 2000), Montreal,
Canada, May 2000, pp. 16–22.

References
625
[5] T. Rabie, A. Shalaby, B. Abdulhai, A. El-Rabbany, Mobile vision-based vehicle tracking and trafﬁc control,
in: Proceedings of the IEEE International Conference on Intelligent Transportation Systems (ITSC 2002),
Singapore, September 2002, pp. 13–18.
[6] D. Terzopoulos, Perceptive agents and systems in virtual reality, in: Proceedings of the ACM Symposium on
Virtual Reality Software and Technology, Osaka, Japan, October 2003, pp. 1–3.
[7] W. Shao, D. Terzopoulos, Autonomous Pedestrians, Graphical Models 69 (5–6) (2007) 246–274.
[8] W. Shao, D. Terzopoulos, Environmental modeling for autonomous virtual pedestrians, in: Proceedings of the
SAE Digital Human Modeling Symposium, Iowa City, Iowa, June 2005.
[9] W. Starzyk, A. Domurad, F.Z. Qureshi, A virtual vision simulator for camera networks research, in: Pro-
ceedings of the Nineth Conference on Computer and Robot Vision, Toronto, Canada, May 2012, pp. 1–8,
in press.
[10] F. Bertamini, R. Brunelli, O. Lanz, A. Roat, A. Santuari, F. Tobia, Q. Xu, Olympus: An ambient intelligence
architecture on the verge of reality, in: Proceedings of the International Conference on Image Analysis and
Processing, Mantova, Italy, September 2003, pp. 139–145.
[11] A. Santuari, O. Lanz, R. Brunelli, Synthetic movies for computer vision applications, in: Proceedings of the
IASTED International Conference: Visualization, Imaging, and Image Processing (VIIP 2003), number 1,
Spain, September 2003, pp. 1–6.
[12] F.Z. Qureshi, D. Terzopoulos, Surveillance camera scheduling: A virtual vision approach, ACM Multimedia
Syst. J. 12 (3) (2006) 269–283.
[13] F.Z. Qureshi, D. Terzopoulos, Smart camera networks in virtual reality, Proceedings of the IEEE 96 (10)
(2008) 1640–1656 (Special Issue on Smart Cameras).
[14] F.Z. Qureshi, Demetri Terzopoulos, Planning ahead for PTZ camera assignment and control, in: Proceedings
of the Third ACM/IEEE International Conference on Distributed Smart Cameras (ICDSC 09), Como, Italy,
August 2009, pp. 1–8.
[15] A.C. Sankaranarayanan, A. Veeraraghavan, R. Chellappa, Object detection, tracking and recognition for
multiple smart cameras, Proceedings of the IEEE 96 (10) (2008) 1606–1624 (Special Issue on Smart Cameras).
[16] C. Ding, B. Song, A. Morye, J.A. Farrell, A.K. Roy-Chowdhury, Collaborative sensing in distributed PTZ
camera network, IEEE Trans. Image Process. 21 (7) (2012) 3282–3295.
[17] B. Song, A.T. Kamal, C. Soto, C. Ding, J.A. Farrell, A.K. Roy-Chowdhury, Tracking and activity recognition
through consensus in distributed camera networks, IEEE Trans. Image Process. 19 (10) (2010) 2564–2579.
[18] Panda3D Game Engine Manual. Retrieved on May 9, 2013, from <http://panda3d.org>.
[19] F.Z. Qureshi, Wiktor Starzyk, Learning proactive control strategies for PTZ cameras, in: Proceedings of the
Fifth ACM/IEEE International Conference on Distributed Smart Cameras (ICDSC 2011), Ghent, Belgium,
August 2011, pp. 1–6.
[20] W. Starzyk, F.Z. Qureshi, Multi-tasking smart cameras for intelligent video surveillance systems, in: Proceed-
ings of the Eighth IEEE International Conference on Advanced Video and Signal-Based Surveillance (AVSS
11), Klagenfurt, August 2011, p. 6.

23
CHAPTER
Distributed Smart Cameras for
Distributed Computer Vision
Marilyn Wolf* and Jason Schlessman†
*School of Electrical and Computer Engineering, Georgia Institute of Technology, USA
†Department of Electrical Engineering, Princeton University, USA
4.23.1 Introduction
Sensor networks are now widely used in a wide range of scientiﬁc and commercial applications.
Distributed smart cameras are an important category of sensor network. Distributed smart cameras
have been made possible by advances in semiconductor technology that provide both high-quality
image sensors and powerful embedded processors. The design of distributed smart camera systems
leverages work in computer vision but distributed algorithms require some special approaches.
Covering a scene with multiple cameras helps to handle two problems, occlusion and pixels-on-target,
both of which can limit accuracy. Covering a scene with cameras at physically separated locations results
in different occlusion combinations between the objects in the scene. Algorithms can combine views to
obtain a more complete view of the scene. Multiple locations also generally reduce the average distance
from a target to a camera. We refer to the image size of a target as pixels-on-target. More pixels-on-target
is important for algorithms that classify the shape, size, or other characteristics of the target.
Centralized architectures that bring all video to a central server and process it centrally have limi-
tations. Bandwidth and cabling are clearly limitations of centralized schemes. But centralized algorithms
that combine data from several cameras don’t scale well. As we move to very large camera networks, we
need to architect the computer vision system to scale properly. City-scale computer vision networks, for
example, require careful design of both the physical components and the computer vision algorithms.
Fortunately, VLSI technology and Moore’s Law provide both inexpensive, high-quality cameras
and powerful embedded processors. Systems-on-chips of today can perform complex computer vision
tasks in real time with low power consumption, small size, and low power consumption [1]. Advances
in semiconductor manufacturing and the introduction of CMOS image sensors [2] have dramatically
lowered the cost of cameras.
The design of a distributed smart camera system introduces several challenges:
•
Data bandwidth is limited due to the potentially large volume of data in live video streams.
•
We have a limited ability to synchronize cameras over a large geographic space.
•
We may see signiﬁcant variation in exposure, color, and other parameters.
Distributed computer vision algorithms often ﬁnd abstract representations for targets and objects of
interest. This approach takes advance of distributed computation to perform low-level vision algorithms
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00023-6
© 2014 Elsevier Ltd. All rights reserved.
633

634
CHAPTER 23 Distributed Smart Cameras for Distributed Computer Vision
that do not require data from other cameras. It also reduces the amount of data transmitted between
cameras since these representations are generally much smaller than the pixels.
However, even with smaller representations, we may need to limit the communication between
cameras. A very large network may have tens of thousands of cameras. A fully-connected communica-
tion graph is infeasible. In many cases it is also not necessary. Computer vision tasks rely on locality of
information, and information about a target can often be localized spatially. Localization allows much
sparser communication patterns between cameras.
This article surveys algorithms for distributed smart cameras. Section 4.23.2 reviews previous work
in computer vision. Section 4.23.3 looks at camera calibration, and Section 4.23.4 considers gesture
recognition. The ﬁnal sections discuss several forms of tracking problems.
4.23.2 Basic techniques in computer vision
Several algorithms that are used in traditional computer vision problems such as tracking also play
important roles as components in distributed computer vision systems. In this section we brieﬂy review
some of those algorithms.
Many algorithms start by separating areas of motion, known as foreground, from areas of non-
interest known as background. Foreground and background are usually separated by motion. A simple
background elimination or background subtraction algorithm uses a simpliﬁed change/no change rule.
We compare each frame F of the video against a reference frame R. For each pixel ⟨i, j⟩, the pixel is
background if ∥Fi, j −Ri, j∥≤ε where ε is a threshold value.
Naive background elimination is simple but has obvious problems. Even small amounts of motion
can cause objects to be classiﬁed as foreground; such objects can pop back and forth between foreground
and background, confusing later analysis stages. Many algorithms have been proposed to mitigate these
issues, largely using adaptive algorithms. Ridder et al. [3] used a Kalman ﬁlter to estimate the state
of the background, while Wren et al. used an adaptive Gaussian model [4]. Lv et al. [5] computed
motion vectors to estimate camera motion, thus reducing the inappropriate identiﬁcation of pixels as
foreground. Their algorithm computes a motion vector at each pixel, then estimates the displacement
and rotation of the camera using a ﬁrst-order Taylor series approximation of the transformation.
One very successful approach to adaptive background elimination is mixture-of-Gaussians [6] which
can compensate for a number of problems. Assume the pixel value is represented as a vector of luminance
and chrominance information X ∈(Y, Cb, Cr) and αx =

|X−μx|
σx
. Several Gaussian models are kept,
each with a different estimate of the state of each pixel. The algorithm ﬁrst compares Gaussians of each
model to ﬁnd matches; it then updates Gaussian mean and variance and updates the weights. We can
compare a current-frame and reference-frame pixels using a threshold:
αY
aY
2
+
αCb
aCb
2
+
αCr
aCr
2
< T .
(23.1)
Matching weights are updated over n frames:
N =
 n,
n < Nmax,
Nmax, n ≥Nmax,
(23.2)

4.23.3 Camera Calibration
635
μX n = μX n−1 + (Xn −μX n−1)
N
,
(23.3)
σX n = σX n−1 + (Xn −μX n−1)(Xn −μX n) −σX n−1
N
.
(23.4)
Each model 1 ≤m ≤Mmax is evaluated to determine the weight or conﬁdence wm gives model i in
that model and pixel position:
M =
m,
m < Mmax,
Mmax, m ≥Mmax,
(23.5)
wm = wm−1 + (ρ −wm−1)
M
.
(23.6)
Given a set of pixels of interest, we need to further reﬁne our analysis and classiﬁcation. Many
algorithms use feature extraction as an intermediate step. For example, scale-invariant feature transform
(SIFT) extracts set of feature vectors as the min/max of the difference-of-Gaussians function. Extracted
features are typically clustered based on position, color, etc., to help identify objects of interest.
Once we have identiﬁed a set of pixels as being of interest or target, we need to create an abstract
model for the region. Many different metrics can be used to model a target: shape, color, etc. One very
simple model for shape is a bounding box, simply the length and width of the smallest box containing
the target pixels. Complex curve-based models may also be used; we will see below the use of ﬁtted
elipses as shape models.
Histograms are used to represent color or luminance distributions; they can be used as a signature for
appearance. The Bhattacharya coefﬁcient is widely used to measure the similarity of two histograms.
Given n partitions in each histogram, the two histograms a and b have ai or bi elements in the ith
partition. The Bhattacharya coefﬁcient is then
B =

1≤i≤n

aibi.
(23.7)
The particle ﬁlter [7] and Kalman ﬁlter [8] are the two most widely used algorithms for single-camera
tracking of targets. Particle ﬁlters use Monte Carlo methods to estimate a probability distribution of
samples of the hidden states as weighted by Bayesian estimates of probability masses. Coates [9]
describes a distributed algorithms for computing particle ﬁlters. In their architecture, each sensor node
ran one particle ﬁlter, with random number generators on each node that were synchronized. They
described two approaches to the problem of distributing observations over the network, one parametric
and one based on adaptive encoding.
Javed et al. [10] use brightness transfer functions (BTFs) to model variations in appearance of
targets from camera to camera. Such variations can come from camera parameters, illumination, or
pose. A training phase uses object histograms to generate inter-camera BTFs for pairs of cameras. The
inter-camera BTFs can then be used to reﬁne tracking by adjusting the similarity metrics.
4.23.3 Camera calibration
A single camera needs to be calibrated; if the system includes multiple cameras then they also need to be
calibrated relative to each other. Intrinsic calibration determines internal camera parameters. Extrinsic

636
CHAPTER 23 Distributed Smart Cameras for Distributed Computer Vision
calibration determines the position of the camera relative to the scene. Temporal calibration determines
the timing of frame capture. Temporal and spatial intrinsic calibrations determine the accuracy with
which we can combine information from multiple cameras.
Several researchers have developed calibration algorithms that account for the speciﬁc calibration
needs of distributed camera systems. Algorithms have been developed to determine the spatial position
of cameras relative to each other as well as to compare the exposure and color representation of one
camera relative to another.
Radke et al. [11] developed a distributed algorithm for the metric calibration of camera networks—
their position in space relative to each other. Measuring the position of cameras externally is difﬁcult
and time-consuming; it also assumes that cameras never move. Radke et al. instead use computer vision
to spatially calibrate the cameras. Devarajan et al. model the camera network using a communication
graph and a vision graph. The communication graph is based on network connectivity—it has an edge
between nodes that directly communicate; this graph can be constructed using standard ad hoc network
techniques. The vision graph is based on signal characteristics—it has an edge between two nodes that
have overlapping ﬁelds-of-view; this graph needs to be constructed during the calibration process. Each
camera 1, . . . , M is described by a 3 × 4 matrix Pi that gives the rotation matrix and optical center of
the camera. The intrinsic parameter matrix for camera i is known as Ki. A set of feature points Xi is
mapped onto the perspective cameras using the projective depth λi j:
λi j
 ui j
1
	
= Pi
 X j
1
	
.
(23.8)
The vision graph has vertices for the cameras and an edge between two vertices when an estimate of
the epipolar geometry between the two cameras can be obtained.
An initial estimate is constructed in several steps. A nucleus of common scene points is estimated
using projective reconstruction; RANSAC is used to reject outliers. A bundle adjustment step uses
nonlinear minimization to improve the calibration. The accuracy of the estimate is improved using
pairwise Markov random ﬁelds. The joint density problem can be described as
p(Y1, . . . , YM|Zz, . . . , Z M) =

i∈V
p(Zi|Yi)

(i, j)∈E
p(Yi, Y j),
(23.9)
where Yi is a true state and Zi is observed. Pairs of random vectors Yi and Y j must agree in some
variables; these constraints are described by binary selector matrices Ci j. Given these constraints,
belief propagation can be formulated as φi(Yi) ∝p(Zi|Yi), ψi j(Yi, Y j) ∝δ(Ci jYi −C jiY j). This
formulation allows the belief propagation to be solved using updates on the vision graph edges.
Synchronization, also known as temporal calibration, is necessary to provide an initial time base for
video analysis. Cameras often display wide variations in frame rate. Some multiple-camera systems
have slaved all camera shutters to an external synchronization signal but such methods are very sensitive.
Pollefeys et al. [12] jointly calibrated a network of cameras in time and space. They track a target in
each camera and select representative points for the target, then use a RANSAC to match points from
frames from cameras. The RANSAC search operates in both space and time—it determines both the
epipolar geometry of the frames and their temporal offset. To generate large camera networks, they start
with three cameras and generate a consistent set of fundamental matrices. They then add one cameras

4.23.4 Gesture Recognition
637
at a time. The algorithm stops when all cameras have been calibrated or when calibration errors become
too large.
Velipasalar and Wolf [13] synchronized a set of cameras by ﬁrst extrinsically calibrating their posi-
tions, then tracking a target and matching the positions of targets. The algorithm selects a sequence of
frames Di from each camera i as the basis for temporal calibration. It selects an anchor point in each
frame by dropping a line from the bounding box to the ground plane. A search algorithm compares the
positions of the anchor points to minimize the distance D1,2
i, j between frame sequences 1 and 2 at offsets
i and j.
Porikli and Divakaran [14] studied color calibration; variations in the representation of color between
cameras can interfere with algorithms that use color to identify targets. They record images of identical
objects from each camera. A correlation matrix of the histograms generated for the object from each
camera is used to generate correction coefﬁcients for one camera relative to another.
4.23.4 Gesture recognition
Gesturerecognitioncanbeusedinavarietyofapplications,rangingfromthecontrolofhuman–computer
interfaces to the analysis of activity by surveillance systems. Some gesture recognition systems rely on
the target being covered with ﬁducial marks. We concentrate here on markerless tracking, which is in
general less accurate but can be used in a wider range of environments.
Van den Bergh et al. [15] estimate pose by constructing a three-dimensional hull over the subject
using voxel carving. They achieve real-time performance by using a ﬁxed lookup table for each voxel
that gives for each pixel the set of voxels that project onto that pixel. They use an example-based
classiﬁer to bin the 3D hulls into poses.
Lin et al. [16] developed a peer-to-peer algorithm for gesture recognition based on a single-camera
gesture recognition system [17] that analyzes a gesture as a time sequence of poses—each pose is the
position of body parts. A pose is extracted from each frame by ﬁnding boundaries for foreground pixel
regions then ﬁtting each foreground region with an elliptical model. A graph is used to describe the
spatial relationship between body parts: relative position, color, and other features are used to label the
graph model. The graph describing the frame is compared to a library of graphs describing poses in
order to classify the frame’s pose. A hidden Markov model is used to match a collection of pose tokens
onto a library of HMMs describing gestures.
Lin’s distributed gesture recognition system assumes that the view of a target may be split between
several cameras, so that data must be combined over cameras. It does so without sharing frames,
which substantially reduces bandwidth. Instead, it uses pixel-level region identiﬁcation, which can be
transmitted at lower cost. Lin’s system combines the features created by pixel-level region identiﬁcation.
Theregionsfromdifferentcamerasareeasilyfusedbuttheycanbetransmittedatlowercost.Eachcamera
identiﬁes pixel regions and extracts their shapes. If a shape crosses a frame boundary, the camera must
share data with the adjacent camera. One camera becomes the lead for the analysis, obtaining region
information from the other camera and performing the ﬁnal pose classiﬁcation after combining with its
own region information. A distributed protocol determines which camera will take the lead position in
analyzing a pose. The lead camera is determined by the centerline of the target. As a target moves away
from that camera the token move.

638
CHAPTER 23 Distributed Smart Cameras for Distributed Computer Vision
4.23.5 Tracking with overlapping ﬁelds-of-view
Tracking with overlapping ﬁelds of view generates an estimate of the trajectory of the target based on
a target model. Many different algorithms have been developed for multi-camera tracking.
Sheng et al. [18] developed two distributed particle ﬁlter algorithms. One leverages the fact that
nearby sensors tend to have correlated data; it uses sensor cliques which exchange information. The
other algorithm forwards sensor data to a fusion center.
Fleuret et al. [19] divide a planar tracking surface into discrete regions. They can then use dynamic
programming to track multiple people in a scene, determining the most likely position of each person
in the scene, based on an analysis of overlapping windows of 100 frames. They reduce the size of the
search space by tracking one person at a time rather than simultaneously searching over all targets. The
search is constrained both by models of likely motion and color models of the targets.
Velipasalar et al. [20] developed a peer-to-peer tracking. A system calibration step distributes infor-
mation to each camera as to what other cameras have ﬁelds-of-view that overlap with its own. The
ﬁeld-of-view information is used to determine at run time what cameras may have tracking information
available about the target. Each camera tracks the target and maintains an appearance model for it. Cam-
eras also share information about targets. All cameras whose ﬁeld-of-view includes the target form a
group; group structure changes dynamically as the target moves. The group is organized algorithmically
as a logical ring structure. The members of the group use the appearance model as well as position to
determinethatagivensetofviewsinfactrefertothesametarget. Theycollaboratetoﬁndacommonlabel
for the target to be used by all members of the group. The cameras in a group share information about tar-
get position every n frames. Exchanging information allows cameras to more accurately estimate target
position; it also provides some amount of fault tolerance. Cameras share information about a target posi-
tion whenever the target is in its ﬁeld-of-view, even if the target is occluded at any given time. A camera
can ﬁnd the target’s current position from other cameras in its group whose view is not occluded.
4.23.6 Tracking in sparse camera networks
Sparse camera networks do not assume that cameras have overlapping ﬁelds-of-view. The relationships
between the cameras is modeled as a graph that shows how targets can move directly from one camera’s
ﬁeld-of-view to another; no metric information is available on the exact position of observations in
different cameras. Because the cameras have no metric information by which to compare simultaneous
views of a target, we must ﬁnd other ways to determine that targets seen by different cameras at different
times are in fact the same target. Bayesian algorithms are widely used to ﬁnd the most likely assignment
of observations to target identities; search algorithms use Bayesian quantities to determine the most
likely assignments of observations to target identities and tracks.
Oh et al. [21] used a Markov chain Monte Carlo algorithm; they called their approach Markov chain
Monte Carlo multi-target tracking (MCMC-MTT). At any given time, the set of smart cameras has
generated a set of target positions xi and a set of observations yi. Our goal is to partition the available
observations into a set of tracks τ = {y1, . . . , yt} where the set of all tracks for a given scene is ω.
When the cameras generate an additional set of observations yt+1, we need to assign them to tracks.
New observations may cause us to change the assignment of older observations to tracks. This gives an

4.23.6 Tracking in Sparse Camera Networks
639
updated set of tracks ω′. Our goal is to maximize the posterior of ω′. Oh et al. showed that the posterior
of ω′ is:
P(ω|Y) = 1
Z

1≤t≤T
pztz (1 −pz)ct pdt
d (1 −pd)ut λat
b λ ft
f
×

τ∈ω/{τ0}

1≤i≤|τ|−1
N(τ(ti+1|¯xti+1(τ), bti+1(τ)).
(23.10)
In this formula, Z is a normalizing constant and N() is the Gaussian density function with mean μ
and covariance matrix . In the approach of Oh et al. MCMC multi-target tracking makes use of several
types of moves to generate new candidate tracks:
•
Birth/death: A birth move creates a new track while death destroys an existing one.
•
Split/merge: A split breaks one track into two pieces while a merge combines two tracks into one.
•
Extension/reduction: Extension lengthens an existing track by adding new observations at the end
of the track while reduction shortens a track.
•
Track update: An update adds an observation to a track.
•
Track switch: A track switch exchanges observations between two tracks.
Moves are accepted with probability:
A(ω, ω′) = min

1, π(ω′)q(ω′, ω)
π(ω)q(ω, ω′)

,
(23.11)
where π(ω) is the stationary distribution of the states and q(ω, ω′) is the proposal distribution.
One challenge to the search process that assigns observations to paths is that a given path graph has
many possible paths, many of which are unlikely to occur. Kim et al. [22] used a modiﬁed graph model
to encapsulate information about priors that guides the search process. For example, cycles at entry/exit
nodes (nodes that can generate or eliminate targets) can induce long paths in which the target moves
back and forth between two nodes. Kim introduced a supernode, which is a set of nodes and associated
edges. The supernode embeds prior knowledge about unlikely cycles in the graph.
Based on their work on improved graph models, Kim and Wolf [23] then developed a distributed
algorithm to assign observations to paths. Each camera can communicate only with local nodes; those
nodes do not need to be immediate neighbors but the radius of communication is limited. Cameras
exchange both observations and paths. Each camera is responsible for generating candidate paths for
the targets using its own and shared observations. Tracks are shared across iterations, and less-likely
pathsareeliminatedstep-by-step. Anindividualcamera’spathdoesnotingeneralcreateglobalpathsthat
span a target’s entire movement; local paths are combined to create those global paths. They formulate
the search over possible path assignments by reducing relationships between observation to adjacent
observations in a path. Assignment of observations to paths is a maximum a posteriori (MAP) problem
that is performed by maximizing
log p(ωK |Y) ∝(k) + (k),
(23.12)
where
(k) =

1≤k≤K
log p(xk,1|v0)p(yk,0yk,1|v0, xk,1)
(23.13)

640
CHAPTER 23 Distributed Smart Cameras for Distributed Computer Vision
and
(k) =

1≤k≤K

2≤i≤|Tk|
log p(xx,i|xk,i−1)p(yk,i−1, yk,i|xk,i−1, xk,i).
(23.14)
The MAP partition is equivalent to the assignment problem, which ﬁnds a maximum weighted bipartite
matching. This can be solved by the Hungarian algorithm, which ﬁnds a perfect matching (all vertices
are covered) that minimizes the potential of the graph; the potential satisﬁes yi + y j ≤c(i, j) where i
is a node in the ﬁrst set, j is a node in the second set, and c() is the cost function.
4.23.7 Summary
Camera networks can cover large geographic regions. Both the large volume of data and the difﬁculties
of synchronizing cameras argue for distributed algorithms to solve computer vision problems in large
networks. Distributed smart camera systems research has developed a variety of abstractions for objects
of interest that can be used to model targets without moving video between cameras. While a great deal
of work has been done, research over the past decade has shown that distributed algorithms can be built
that can process large quantities of video efﬁciently and accurately.
Acknowledgment
This work was supported in part by the National Science Foundation under Grant 0720536.
References
[1] Wayne Wolf, High Performance Embedded Computing, Morgan Kaufman, 2006.
[2] Wayne Wolf, Modern VLSI Design: IP-Based Design, fourth ed., PTR Prentice Hall, 2009.
[3] C. Ridder, O. Munkelt, H. Kirchner, Adaptive background estimation and foreground detection using Kalman-
ﬁltering, in: Proceedings of International Conference on Recent Advances in Mechatronics, ICRAMG95,
UNESCO Chair on Mechatroncs, 1995, pp. 193–199.
[4] C. R Wren, A. Azarbayejani, T. Darrell, A. Pentland, Pﬁnder: real-time tracking of the human body, IEEE
Trans. Pattern Anal. Mach. Intell. 19 (7) (1997) 780–785.
[5] Tiehan Lv, Burak Ozer, Wayne Wolf, A real-time background subtraction method with camera motion com-
pensation, in: Proceedings International Conference on Multimedia and Exhibition, vol. 1, IEEE, 2004,
pp. 331–334.
[6] T. Horprasesert, D. Harwood, L.S. Davis, A statistical approach for real-time robust background subtraction
and shadow detection, in: IEEE International Conference on Computer Vision FRAME-RATE Workshop,
1999.
[7] James V. Candy, Boostrap particle ﬁltering, IEEE Signal Process. Mag. 73 (2007) 73–85.
[8] V. Boykov, D. Huttenlocher, Adaptive bayesian recognition in tracking rigid objects, in: Proceedings IEEE
Conference on Computer Vision and Pattern Recognition, IEEE, 2000, pp. 697–704.
[9] Mark Coates, Distributed particle ﬁlters for sensor networks, in: Third International Symposium on Informa-
tion Processing in Sensor Networks 2004, IPSN 2004, IEEE, 2004, pp. 99–107.

References
641
[10] O. Javed, K. Shaﬁque, M. Shah, Appearance modeling for tracking in multiple non-overlapping cameras, in:
IEEE Computer Society Conference on Computer Vision and Pattern Recognition 2005, CVPR 2005, vol. 2,
2005, pp. 26–33.
[11] R. Radke, D. Devarajan, Z. Cheng, Calibrating distributed camera networks, Proc. IEEE 96 (10) (2008)
1625–1639.
[12] Marc Pollefeys, Sudipta N. Sinha, Li Guan, Jean-Sebastien Franco, Multi-view calibration, synchronization,
and dynamic scene reconstruction, in: Hamid Aghajan, Andrea Cavallaro (Eds.), Multi-Camera Networks:
Principles and Applications, Academic Press, 2009 (Chapter 2).
[13] Senem Velipasalar, Wayne H. Wolf, Frame-level temporal calibration of video sequences from unsynchronized
cameras, Mach. Vis. Appl. J. 2008, <http://dx.doi.org/10.1007/s00138-008-0122-6>.
[14] Fatih Porikli, Ajay Divakaran, Multi-Camera Calibration, Object Tracking and Query Generation, Technical,
Report TR-2003-100, 2003.
[15] Michael Van den Bergh, Esther Koller-Meier, Roland Kehl, Luc Van Gool, Real-time 3d body pose estimation,
in: Hamid Aghajan, Andrea Cavallaro (Eds.), Multi-Camera Networks: Principles and Applications, Academic
Press, 2009 (Chapter 14).
[16] Chang Hong Lin, Tiehan Lv, Wayne Wolf, I. Burak Ozer, A peer-to-peer architecture for distributed real-time
gesture recognition, in: Proceedings, International Conference on Multimedia and Exhibition, IEEE, 2004,
pp. 27–30.
[17] Wayne Wolf, Burak Ozer, Tiehan Lv, Smart cameras as embedded systems, IEEE Comput. 35 (9) (2002)
48–53.
[18] X. Sheng, Y.-H. Hu, Parameswaran Ramanathan, Distributed particle ﬁlter with gmm approximation for
multiple targets localization and tracking in wireless sensor network, in: Fourth International Symposium on
Information Processing in Sensor Networks 2005, IPSN 2005, IEEE, 2005, pp. 181–188.
[19] F. Fleuret, J. Berclaz, R. Lengagne, P. Fua., Multicamera people tracking with a probabilistic occupancy map,
IEEE Trans. Pattern Anal. Mach. Intell. 30 (2) (2008) 267–282.
[20] Senem Veliapasalar, Jason Schlessman, Cheng-Yao Chen, Wayne H. Wolf, Jaswinder P. Singh, A scalable
clustered camera system for multiple object tracking, EURASIP J. Image Video Process. 2008 (article ID
542808).
[21] S. Oh, S. Russell, S. Sastry, Markov chain Monte Carlo data association for general multiple-target tracking
problems, in: Proceeding of 43rd IEEE Conference Decision and Control, December 2004.
[22] Honggab Kim, Justin Romberg, Wayne Wolf, Multi-camera tracking on a graph using markov chain Monte
Carlo, in: Proceedings ACM/IEEE International Conference on Distributed Smart Cameras 2009, ACM, 2009.
[23] Honggab Kim, Marilyn Wolf, Distributed tracking in a large-scale network of smart cameras, in: Proceedings
of the Fourth ACM/IEEE International Conference on Distributed Smart Cameras, ACM Press, 2010, p. 816.

24
CHAPTER
Mapping Parameterized Dataﬂow
Graphs onto FPGA Platforms
Hsiang-Huang Wu*, Chung-Ching Shen*, Hojin Kee†, Nimish Sane*,
William Plishker*, and Shuvra S. Bhattacharyya*
*Department of Electrical and Computer Engineering, Institute for Advanced Computer Studies,
University of Maryland, College Park, MA, USA
†National Instruments Austin, TX, USA
4.24.1 Introduction
As the speed and logic capacity of ﬁeld programmable gate arrays (FPGAs) have been improving
steadily, FPGAs have become increasingly attractive for a wide variety of signal processing systems.
FPGAs are increasingly employed in the form of platform FPGAs, which are integrated circuits that
combine signiﬁcant amounts of conﬁgurable logic fabric along with additional subsystems, such as
application-speciﬁc accelerators, processor cores, memory blocks, and input/output interfaces, to facil-
itate FPGA-based, system-on-chip design [1]. FPGA fabric is also integrated into application speciﬁc
integrated circuits (ASICs) to allow implementations that provide a mix of programmable and custom
hardware (e.g., see [2]).
Through support for dynamic reconﬁguration, modern FPGAs allow customization of hardware
structures both statically and at run-time, thus allowing streamlining of processing conﬁgurations in
response to application requirements or data characteristics that are not known at design time. In addition
to allowing for dynamic changes in system functionality, dynamic reconﬁguration, when carried out
effectively, can enhance performance, resource utilization, and energy efﬁciency (e.g., see [3]).
However, in addition to such potential for improved operation, incorporating dynamic reconﬁguration
into the digital system design space also brings increased design complexity. Model-based design
methodologies have been evolving steadily over the years to help address issues of design complexity
in embedded systems [4]. In model-based design, applications are represented and analyzed in terms of
formal models of computation, which promote analysis of functionality as well as hardware and software
structure at a high level of abstraction. In the domain of signal processing, model-based techniques based
on dataﬂow models of computation are particularly popular, and are employed in a growing variety of
design tools [5].
While dataﬂow techniques allow for high level reasoning about and manipulation of application
dynamics, there are important challenges in mapping dataﬂow models into FPGA platforms in ways
that systematically and effectively exploit the dynamic reconﬁguration capabilities of the platforms. This
paper provides a review of state-of-the-art model-based design techniques and FPGA implementation
techniques for signal processing systems, and explores the challenges involved in effectively mapping
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00024-8
© 2014 Elsevier Ltd. All rights reserved.
643

644
CHAPTER 24 Mapping Parameterized Dataﬂow Graphs onto FPGA Platforms
high level application models into efﬁcient implementations on dynamically reconﬁgurable FPGA
platforms.
The exploration presented in this paper on mapping models into implementations builds on our
earlier work in this area, which was presented in preliminary form in [6]. The reconﬁguration-aware
mapping techniques presented in this paper (Sections 4.24.4 and 4.24.5) go beyond the developments of
[6] in a number of ways. Speciﬁcally, this extended paper enhances the hardware architecture mapping
methodology of [6] and provides two alternative perspectives on scheduling. These two perspectives
affect important trade-offs between performance and modularity. An important new aspect integrated
into one of these scheduling perspectives involves integration of the recently-developed dataﬂow sched-
ule graph model [7] into processes for FPGA mapping of dynamically reconﬁgurable signal processing
systems.
4.24.2 Background
4.24.2.1 FPGA technology
As shown in Figure 24.1, an FPGA can be viewed as a matrix of cells, which can encapsulate vari-
ous kinds of hardware structures, such as programmable logic subsystems, memories, special purpose
hardware subsystems (e.g., multipliers or higher level signal processing accelerators), and embedded
processor cores. A ring of I/O (input/output) modules is placed along the periphery for connection to the
outside world, and routing channels, driven through conﬁgurable switches, are used for communication
among cells.
Although the details of how programmable logic subsystems are constructed varies among different
vendors, their basic structure, illustrated in Figure 24.2a, comprises M-input look up tables (LUTs) and
D ﬂip-ﬂops (DFFs), which are integrated into programmable logic blocks. Collections of such logic
blocks can be programmed to implement digital logic functions of arbitrary complexity. Figure 24.2b
E
I/O
I/O
I/O
I/O
I/O
I/O
I/O
I/O
L: logic
M: memory
E: embedded unit
I/O
I/O
I/O
I/O
I/O
I/O
I/O
I/O
L
L
L
L
M
M
L
L
L
L
E
L
routing channel
L
L
L
FIGURE 24.1
Basic architecture of an FPGA.

4.24.2 Background
645
input
M−input LUT
DFF
clock
output
Primitive structures for constructing programmable logic
blocks.
Arithmetic and carry logic
M−input LUT
M−input LUT
M−input LUT
M−input LUT
DFF
DFF
DFF
DFF
A simpliﬁed illustration of logic blocks employed in
FPGA devices from Xilinx and Altera.
(a)
(b)
FIGURE 24.2
Programmable logic blocks.
illustrates, in simpliﬁed form, the structure of logic blocks that are employed in FPGA devices made
by Xilinx and Altera. These blocks contain sub-structures for arithmetic and carry logic to support the
ﬂexible construction of computational building blocks.
To connect logic blocks, ﬂexible routing architectures are provided to accommodate different kinds
of interconnection patterns. The island-style routing architecture is widely adopted in commercial FPGA
devices. An island-style global routing architecture involves routing channels on all four sides of the
logic blocks. The numbers of wires contained in a channel is set to a constant W during fabrication, and is
one of the key design decisions made in the FPGA architecture design. Island-style routing architectures
generally employ wire segments of different lengths in each channel to allow optimized selection of
lengths based on the speciﬁc connections that are made. The end points of wire segments are typically
staggered so that logic blocks can be connected at the end points of wires that have appropriate lengths.
Figure 24.3 illustrates channels and switches in an island-style FPGA architecture, and also shows a
routing example. The ﬁgure shows an interconnection that starts from logic block 5, and goes through
logic block 4 to logic block 2. Two switch nodes, s1 and s2, are used. To connect logic block 5 to logic
block 4, programmable switch D in switch node s2 is conﬁgured. Similarly, programmable switch B
in switch node s1 is conﬁgured for the connection between logic block 4 and logic block 2.
There are six possible connections for a programmable switch, as illustrated in Figure 24.4, which
is why six switches (A through F) are depicted in Figure 24.3.

646
CHAPTER 24 Mapping Parameterized Dataﬂow Graphs onto FPGA Platforms
B
block
logic
block
logic
block
logic
block
logic
wire segment
programmable switch
switch box
logic
block
logic
block
1
2
3
4
5
6
s1
3
s
2
s
A
C
D
E
F
FIGURE 24.3
Island-style routing architecture.
F
A
B
C
D
E
FIGURE 24.4
Six possible connections for a programmable switch.
The island-style architecture facilitates optimization of the physical layout of logic blocks and their
surrounding routing channels, and its regularity facilitates efﬁcient delay estimation (e.g., see [8]).
4.24.2.2 Model-based design of signal processing systems
As described in Section 4.24.1, model-based techniques based on dataﬂow models of computation are
used widely in the design of signal processing systems. Dataﬂow can be viewed as a special case of
Kahn process networks (KPNs), which are composed of processes that communicate through unbounded
communication channels [9]. Each communication channel is a ﬁrst-in ﬁrst-out (FIFO) queue of tokens,
where tokens encapsulate data values as they pass between processes. FIFO communication channels
in KPNs can be written to and read only from the processes that are connected to them in the enclosing
KPN. An important restriction on execution of KPNs is that processes can access data from incident
FIFOs only through blocking read operations, which effectively suspend the processes if requested
data is not available, and allow the processes to resume only after the associate read operations are
completed. This restriction helps to ensure determinacy of KPN-based representations. Variations on

4.24.2 Background
647
Kahn process networks have been explored in recent years, such as polyhedral process networks and
reactive process networks, which provide useful new capabilities for modeling and analysis of process
and network execution [10–12].
In the context of design and implementation of signal processing systems, dataﬂow is a restricted
form of KPN in which processes (called actors in the context of dataﬂow representations) execute in
terms of well-deﬁned, discrete units of execution, called ﬁrings, of the associated actors [13]. Such
a discrete approach to process ﬁring can be enforced, for example, through enable-invoke semantics,
whereﬁreability(availabilityofsufﬁcientinputdata)isfullyseparatedfromprocessexecution(invoking)
functionality [14].
Model-based design methods based on dataﬂow models of computation have provided designers of
signal processing systems with representations of intuitive correspondence to signal processing block
diagrams. In such representations, DSP applications are modeled as directed graphs, where vertices
correspond to dataﬂow actors and represent computational modules for executing (or ﬁring) the corre-
sponding tasks, and edges represent inter-actor FIFO channels, as in KPNs. Actors produce and consume
tokens from their input and output edges, respectively, as they are ﬁred. In enable-invoke dataﬂow, an
actor ﬁring cannot begin execution until all of the input data required by the ﬁring is present on the
relevant input edges, and thus blocking reads are not employed [14].
Scheduling is a critical issue when implementing dataﬂow representations of signal processing sys-
tems. Scheduling is the process of constructing a schedule, which assigns each actor ﬁring to a processing
resource, and determines the ordering of ﬁrings that share the same resource. In addition to affecting
overall system functionality (a schedule that is not constructed properly can cause deviations from
expected functionality), scheduling generally has signiﬁcant effect on performance, resource utiliza-
tion, and memory requirements.
Synchronous dataﬂow (SDF), proposed in [15], is a restricted form of dataﬂow in which each actor
ﬁring consumes and produces constant amounts of data from each of its input and output edges, respec-
tively. The SDF restriction is orthogonal to enable-invoke dataﬂow. In an enable-invoke context, an SDF
ﬁring can execute only after all of the required input data has arrived at the actor inputs. On the other
hand, if enable-invoke semantics is not enforced, then even though the ﬁring consumes constant amounts
of data, it can begin execution when only part of its input data is available, and employ blocking reads
to read the rest of the data—in a manner that is interleaved with the computations associated with the
ﬁring—until the ﬁring is complete. An important advantage of SDF is its support for static scheduling,
and a wide variety of scheduling techniques have evolved that exploit this support (e.g., see [15–20]).
Cyclo-static dataﬂow (CSDF) [21] is one of the most popular extensions of SDF. CSDF generalizes
SDF by allowing the consumption or production rate of an actor port to vary as long as the pattern of
variations forms a periodic sequence that is statically known.
An example of an SDF actor is illustrated in Figure 24.5a. Here, actor D represents a downsampler
that consumes three tokens generated from actor A and transfers one among them onto the edge that
is directed to actor B. An example of a schedule for this graph is S1 = (3A)DB. This schedule is
expressed in looped schedule notation, where each parenthesized term represents a schedule loop that
is iterated a number of times speciﬁed by a positive integer iteration count that is given as the ﬁrst item
in the term [22]. Thus, the schedule S1 corresponds to the ﬁring sequence (A, A, A, D, B).
A CSDF version of this downsampler-based example is shown in Figure 24.5b. Here, ﬁrings of actor
D execute based on a periodic sequence of three distinct phases, and the inputs and outputs of the actor
are annotated with the numbers of tokens consumed and produced in these phases. Thus, D consumes

648
CHAPTER 24 Mapping Parameterized Dataﬂow Graphs onto FPGA Platforms
)
A
D
B
3
1
1
1
schedule: (3A  DB
SDF speciﬁcation and a corresponding
schedule.
ADADBAD
A
D
B
1
1
(1, 1, 1)
(0, 1, 0)
schedule:
CSDF speciﬁcation and a corresponding
schedule.
(a)
(b)
FIGURE 24.5
Alternative dataﬂow models for a simple multirate signal processing example.
one token on each phase and produces one token on every third phase, starting with the second phase.
This CSDF graph can be executed with the schedule S2 = ADADB AD, which differs from S1 in that
less buffer space (one unit of storage versus three) is needed for the edge (A, D). Indeed, the potential for
improved memory requirements is a useful feature of the CSDF model. A detailed comparison of SDF
and CSDF is developed in [23]. In addition, CSDF can be integrated with parameterized dataﬂow (this
will be discussed in the following section), and the resulting model is called parameterized cyclo-static
dataﬂow (PCSDF). Such integration, explored in [24,25], provides further ﬂexibility in the modeling
of application dynamics compared to PSDF.
4.24.2.3 Parameterized dataﬂow
Parameterized dataﬂow is a meta-modeling technique that integrates dynamic parameterization into
dataﬂow modeling in a systematic and general (applicable to different dataﬂow models) manner.
In parameterized dataﬂow, sets of actor parameters, domains in which such parameters can “reside,”
and conﬁgurations for such sets (bindings to actual values within the respective parameter domains) are
important aspects of application models in addition to more conventional forms of dataﬂow information,
such as speciﬁcations on token production and consumption rates associated with actor ﬁrings.
Settings or updates to parameter conﬁgurations can be distinguished as happening statically, pre-
execution, or dynamically. Static conﬁgurations are determined at compile time—i.e., when the code
for an implementation is derived. Pre-execution conﬁguration refers to conﬁguration that occurs after
compile time but before a given execution of the application. In dynamic conﬁguration or dynamic
reconﬁguration, parameter values can be initialized or updated while the application is executing.
For example, in an FPGA implementation, an addition component can be conﬁgured by instantiating
a look-ahead adder or a ripple carry adder, depending on a trade-off assessment in terms of relevant area
and performance constraints, and such an assessment-conﬁguration sequence can in general be carried
out statically, pre-execution or dynamically depending on factors such as the overhead of performing
the conﬁguration, and the degree to which this overhead is amortized by the beneﬁts of applying the
selected conﬁguration.

4.24.3 Dynamic Reconﬁguration Techniques in FPGAs
649
In this paper, we focus on methods for dynamic reconﬁguration, which are increasingly important
in the development of signal processing systems that can adapt in response to dynamically changing
application constraints, data characteristics, and other operating conditions [5].
The organization of the rest of this paper is summarized as follows. We ﬁrst review hardware methods
for dynamic reconﬁguration. We then show how parameterized dataﬂow techniques integrated with the
SDF model of computation can be applied as an abstract model for design and implementation of
dynamically reconﬁgurable signal processing systems. This integrated model, called parameterized
synchronous dataﬂow (PSDF), has been studied in a variety of useful design contexts before (e.g., see
[5,26]); in this paper, we present novel methods for applying this model to the systematic hardware
mapping of dynamically reconﬁgurable signal processing systems.
Next, to represent adaptive schedules efﬁciently, we apply a schedule representation called the
dataﬂowschedulegraph(DSG),whichprovidesaformalapproachforrepresentingdynamicinteractions
between applications and the architectures on which they execute. We show that using the DSG model,
hardwaremappingofPSDFcanbeinterpretednaturally,andastructuredpathtoefﬁcientimplementation
can be achieved. Using our methods based on the parameterized dataﬂow, SDF and DSG representations,
conventional, ad hoc methods for dynamic reconﬁguration can be replaced by formally-rooted, model-
based techniques that promote efﬁciency, reliable integration, and modularity through a systematic,
dataﬂow-based design ﬂow.
4.24.3 Dynamic reconﬁguration techniques in FPGAs
FPGAs are widely employed as signal processing platforms for both rapid prototyping and optimized
implementation. Because they allow customization of digital hardware structures with signiﬁcantly
higher ﬂexibility, lower cost, and lower turnaround time, they provide a valuable alternative to ASIC
implementations when key application subsystems can be mapped efﬁciently into FPGA structures.
Hybrid architectures that integrate FPGA fabric within an ASIC have been explored to provide a
wider range of trade-offs between efﬁciency and ﬂexibility (e.g., see [2]). When applying such a hybrid
approach, key challenges include the partitioning of designs into ASIC and FPGA parts, and integrating
the ASIC and FPGA design ﬂows. For example, the ASIC/FPGA partition determines the size (area)
of the required FPGA subsystem, which in turns affects the FPGA placement. Furthermore, timing
characteristics between the boundaries of the FPGA and custom logic subsystems complicate timing
closure. For more details on such challenges and proposed solutions, we refer the reader to [27].
Using modern FPGAs, designers can exploit high speed partial reconﬁguration technology (e.g.,
reconﬁguration involving relatively small subsets of the blocks shown in Figure 24.1) to incorporate
dynamic hardware reconﬁguration into practical implementations. Our approach to applying dynamic
reconﬁguration involves careful selection of FPGA components that will be reconﬁgured at run-time.
The reconﬁgured circuits must satisfy the given area and timing constraints. That is, the FPGA logic
block and routing channel structures illustrated in Figures 24.2 and 24.3 need to be taken into account
when determining the circuits that will be loaded in and out during run-time reconﬁguration. While
FPGA design ﬂows for static conﬁguration perform a full conﬁguration that can program the entire
target FPGA, partial reconﬁguration preserves the programming in some regions of the FPGA, and
makes changes to other regions.

650
CHAPTER 24 Mapping Parameterized Dataﬂow Graphs onto FPGA Platforms
Commercial tools are available that provide support for dynamic reconﬁguration. For example,
PlanAhead, a tool from Xilinx, allows users to specify regions of a target FPGA that can be conﬁgured
dynamically [28]. Communication between such regions and statically- or pre-execution-conﬁgured
regions relies on the insertion of certain kinds of bus macros. In terms of the design hierarchy, regions
set up for dynamic reconﬁguration must be represented through top-level design modules.
Software programmable reconﬁguration is a methodology that provides dynamic reconﬁguration
capabilities for FPGAs in a more software-oriented way—i.e., through interfacing with a soft-core
processor [29]. Such dynamic reconﬁguration is achieved, in a manner analogous to software context
switching, by changing routing paths at run-time through control of the associated soft-core processor.
JBits SDK, another tool developed by Xilinx, contains a set of software modules and application
programming interfaces to create Xilinx Virtex bitstreams from Java code [30]. Two tools for partial
and remote reconﬁguration that apply JBits SDK are presented in [31]. One is the circuit customization
tool, which helps to create an interface to conﬁgure parameters of a circuit. The other is the core uniﬁer
tool, which allows designers to manipulate connections between cores using partial reconﬁguration.
When applying the JBits SDK, there are two types of cores called controller and slave cores. Controller
cores are downloaded statically onto an FPGA, and can communicate with slave cores, which can be
loaded dynamically. During execution, controllers can switch between slave cores or replace existing
slave cores by downloading new ones.
To facilitate simulation of designs that employ dynamic reconﬁguration, simulation techniques devel-
opedin[32]havebeenintegratedintotraditionalFPGAdesignﬂow.Thisallowsdynamicreconﬁguration
capabilities to be integrated into simulations that employ existing simulation tools, such as NCVerilog
and ModelSim. In such simulations, circuits that are to be reconﬁgured (reconﬁguration circuits) need to
be speciﬁed and encapsulated up front. Such design capture for reconﬁguration circuits helps to specify
a reconﬁguration schedule, which deﬁnes the conditions under which each reconﬁguration circuit in the
system is loaded or replaced. Specialized components, such as isolation switches and schedule control
modules, are typically applied for the encapsulation, and reconﬁguration, respectively.
Building on platform-based tools for dynamic reconﬁguration, various design approaches have been
developed for FPGA system design. For example, methods for real-time and power-aware implemen-
tation are developed in [33]. The application of dynamic instruction sets in soft-core microprocessors
is explored in [34]. In this approach, if an FPGA cannot accommodate all of the relevant instructions
with their associated hardware support, support for selected instructions can be conﬁgured dynami-
cally as needed by the application. In [35,36], a design methodology called dynamic hardware/software
partitioning is proposed in which FPGAs are employed as coprocessors to accelerate computationally-
intensiveloops.In[37,38],anarchitectureisdevelopedthatcontainsaprocessor,bit-levelreconﬁgurable
part (similar to conventional FPGA fabric), and components (tiles) for composing a novel form of recon-
ﬁgurable subsystem called a ﬁeld programmable function array (FPFA). Such tiles can be viewed as
word-level, reconﬁgurable building blocks that are composed of ALUs and lookup tables. An FPFA is
constructed from FPFA tiles to accelerate intensive, regularly-structured computations, such as linear
interpolation or fast Fourier transforms. A class of heterogeneous processing arrays that integrate signal
processors and FPGA subsystems, and are amenable to dataﬂow-based design and mapping techniques,
is explored in [39].
Development of applications on platform FPGAs, FPGAs that employ soft-core processors, and
host-FPGA combinations often involves hardware/software co-design as a key aspect. Sophisticated

4.24.4 Modeling Dynamic Reconﬁguration Using PSDF Techniques
651
algorithms have been developed for optimization of timing, area and energy in HW/SW systems. For
example, the work presented in [40] takes as input a library containing general-purpose processors,
dynamically reconﬁgurable FPGAs, communication links, and memory modules, and applies an evolu-
tionary algorithm to instantiate hardware resources, and assign tasks and communication events to the
resources. A dynamic priority multirate scheduling algorithm determines the times at which the tasks
and communication events in the system occur. A framework called Nimble is presented in [41]. This
framework takes as input application speciﬁcations in C, and maps them into implementations on a
heterogeneous platform that includes a general-purpose processor, an FPGA, and a memory hierarchy.
An overview of various hardware/software co-synthesis approaches for signal processing systems is
presented in [42].
Various methods have also been developed to help minimize overhead associated with dynamic
reconﬁguration. For example, in [43], methods are developed for evaluating the degree of computation-
reconﬁguration overlap in dynamically reconﬁgurable systems. These methods are based on modeling
of the dynamic reconﬁguration process, and identiﬁcation of functional commonality between reconﬁg-
uration circuits. In another approach, an incremental elaboration model is applied to streamline requests
for new reconﬁguration operations by using set theoretic techniques to leverage known characteristics
of existing (currently active) conﬁgurations [44]. In this section, we have provided a brief overview
of platform-based technologies and tools for dynamic reconﬁguration in FPGAs. For more details
on design ﬂows for dynamically reconﬁgurable FPGA system implementation, we refer the reader to
[45,46]. Methods for consistency analysis of dynamic reconﬁguration functionality in dataﬂow graphs
are discussed in [47,48]. For comprehensive reviews of FPGA technology and system design methods,
we refer the reader to [1,49].
4.24.4 Modeling dynamic reconﬁguration using PSDF techniques
In the remainder of this paper, we develop methods for systematic mapping of model-based signal
processing application representations into efﬁcient implementations on dynamically reconﬁgurable
hardware.
We apply a speciﬁc form of dataﬂow modeling referred to as parameterized synchronous dataﬂow
(PSDF), which offers valuable properties in terms of modeling systems with dynamic parameters,
supporting efﬁcient scheduling techniques, and natural integration with popular SDF modeling
techniques [50]. Compared to enable-invoke dataﬂow [14], PSDF has lower expressive power, but
is equipped with streamlined scheduling techniques for the subclass of application models that are
amenable to PSDF semantics. Compared to scenario-aware dataﬂow [51], PSDF can be viewed as
having a more strict separation between data and parameters, which facilitates symbolic scheduling
techniques based on parameterized looped schedules.
As described in Section 4.24.2.3, PSDF is based on parameterized dataﬂow, which is a meta-modeling
technique that can signiﬁcantly improve the expressive power of an arbitrary dataﬂow model that
possesses a well-deﬁned concept of a graph iteration [26]. Parameterized dataﬂow provides a method to
systematically integrate dynamic parameter reconﬁguration into such models, while preserving many
of the original properties and intuitive characteristics of the original models. The integration of the
parameterized dataﬂow meta-model with SDF provides the model of computation that we refer to as
parameterized synchronous dataﬂow (PSDF).

652
CHAPTER 24 Mapping Parameterized Dataﬂow Graphs onto FPGA Platforms
Efﬁcient quasi-static scheduling techniques have been demonstrated previously for PSDF speciﬁ-
cations [50]. Here, by quasi-static scheduling, we refer to a general approach to scheduling in which
signiﬁcant portions of schedule structure are ﬁxed at compile time, while some amount of run-time
schedule adjustment can be made in response to input data or changes in operational requirements.
4.24.4.1 Execution of PSDF graphs
The PSDF model allows the behavior of subsystems to be controlled by sets of parameters that can be
conﬁgured dynamically. Such parameter control is modeled by mapping selected dataﬂow graph outputs
of certain graphs, which are dedicated to computing parameter updates, to parameters of the graphs that
they control. This coordination between parameter update computations and parameter reconﬁgura-
tion operates under a carefully structured framework that promotes predictability and efﬁciency. Basic
concepts associated with PSDF modeling and execution are outlined as follows.
•
A PSDF subsystem consists three distinct PSDF graphs, called the init, subinit, and body graphs.
•
The interface dataﬂow behavior of a PSDF subsystem (i.e., the rates of token production and con-
sumption at the subsystem inputs and outputs) can only be changed only by the init graph.
•
The init graph can conﬁgure both the subinit and body graphs.
•
The subinit graph is allowed to conﬁgure the body graph but not allowed to change the interface
dataﬂow behavior of its enclosing subsystem.
•
The body graph is executed immediately after the execution of the subinit graph.
•
A hierarchical PSDF actor encapsulates a PSDF subsystem; such nesting in terms of PSDF semantic
hierarchy an be arbitrarily deep based on how a design is constructed.
We use the downsampler shown in Figure 24.6 to illustrate these concepts. Here, node H represents
a PSDF downsampler (i.e., a subsystem), which has two parameters, the factor and phase. These param-
eters represent the consumption rate F and the index P of the token that is selected (for transfer to the
output port) among the F tokens that are consumed in a single execution of the downsampler. Thus, for
example, if F = 5 and P = 2, then downsampling by a factor of 5 is performed, and on each execution,
the downsampler outputs the second token from among the window of 5 tokens that it consumes.
D
C
A
H
B
downsampler H
init
subinit
body
1
1
B
E
G
H
1
1
1
F
A
C
FIGURE 24.6
A PSDF-based downsampler example.

4.24.4 Modeling Dynamic Reconﬁguration Using PSDF Techniques
653
The consumption or production rate associated with a subsystem input or output port is viewed as
interface dataﬂow behavior, and can only be conﬁgured by the init graph (or kept ﬁxed at a statically
conﬁgured value), as described above. Thus, the factor F is conﬁgured by the init graph only whereas
the phase P can be conﬁgured either by the init or subinit graph since it does not change the interface
dataﬂow behavior.
In this example, P is conﬁgured by the subinit graph to allow a ﬁner granularity of (more frequent)
control compared to conﬁguration by the init graph. Initially, actor E conﬁgures F to the value 3, which
yields an SDF graph that maintains its given SDF properties while the parameter F remains ﬁxed at
this value. To execute the graph in this SDF conﬁguration, we can apply any valid SDF schedule for
the conﬁguration—one such schedule is (3A)BHC. This schedule is repeated some number of times
before the downsampler value is changed. In particular, changes must occur between iterations of this
schedule, as governed by PSDF semantics so that within any given iteration the graph operates as an
SDF graph, while the SDF graph conﬁguration can be changed between iterations.
PSDF-based design and implementation is supported by a Java-based PSDF simulator, called
PSDFsim [6], which provides modeling and functional simulation capabilities for PSDF speciﬁcations
as part of the dataﬂow interchange format (DIF) environment [52].
4.24.4.2 PSDF design methodology
PSDF-semantics can be applied for model-based design at the front end of the FPGA/ASIC design
ﬂow shown in Figure 24.7. Such an approach provides a structured framework for modeling adaptive
behaviors and dynamic reconﬁguration, and deriving corresponding adaptations to scheduling strategies
and resource allocations (e.g., see [6,50]).
Our PSDF-based approach for FPGA system design involves two key phases—high level modeling
and validation (modeling) and hardware architecture mapping (mapping). These two phases can in
general be applied iteratively to implement and experimentally reﬁne dataﬂow based parallel processing
structures for FPGA- or ASIC-based signal processing systems.
Front End
Application
Design
Specification
RTL
Development
Functional
Verification
Synthesis
Timing
Verification
Verification
Sign−Off
Post
Place & Route
Floor Planing
FIGURE 24.7
FPGA/ASIC design ﬂow overview.

654
CHAPTER 24 Mapping Parameterized Dataﬂow Graphs onto FPGA Platforms
The modeling phase ensures correct application functionality as well as the correct formulation of the
functionality in terms of dataﬂow and PSDF principles. Through its direct connection to the concurrency
modeling capabilities of dataﬂow, this phase helps provide a framework for efﬁcient implementation
even though the focus on this phase is on functional validation rather than detailed hardware mapping.
In this phase, procedural software code is used to specify the internal functionality of the actors, while
a dataﬂow language is used to specify the high-level (inter-actor) application model. In PSDFsim,
the Java and DIF languages are used for these purposes of intra-actor and inter-actor, modeling-phase
speciﬁcation, respectively.
In the mapping phase, the designer applies the individual actor models as functional references to
derive corresponding hardware implementations using a hardware description language (HDL). The
functionality of these “hardware actors” can be validated using the same testbenches as those used in
the modeling phase. Use of the formal dataﬂow methodology to encapsulate design components (actors)
facilitates this reuse of testbenches. Similarly, edges in the DIF-based application model are mapped
into corresponding FIFO implementations using the targeted HDL and associated design library.
By developing the actors based on PSDF principles, and connecting them through standard FIFO
semantics, functional correctness of the overall, application-level hardware implementation follows
directly from correctness of the original PSDF application model, and correct mappings of the individual
actor models into hardware. Additionally, the application level model from the modeling phase can be
used as a testbench to begin application-level testing of the hardware, where both functional and timing
constraints must be taken into account. Insight from timing analysis of the hardware implementation
can then be used to optimize the hardware actors and possibly to iterate back to the modeling phase to
explore reﬁnements or alternatives to the high level dataﬂow architecture.
The simulation and implementation tools discussed in Section 4.24.3 focus on mapping hardware
description language (HDL) programs into FPGA implementations. In contrast, our proposed meth-
ods show how to map higher level, model-based speciﬁcations into monolithic, FPGA-targeted, HDL
programs, which can then be further processed by tools such as those discussed in Section 4.24.3. In
our experiments, we have not integrated the tools discussed in Section 4.24.3 in this way (i.e., as a back
end to our proposed methods); this is a useful direction for further study.
4.24.5 Hardware mapping
In this section, we present two hardware architecture mapping methods that apply PSDF modeling, and
supportdynamicreconﬁgurationwithtwousefulobjectives—modularityandperformanceoptimization.
Both of these methods exploit the modular design representation format facilitated by PSDF, which is
discussed in Section 4.24.5.1. We present a novel form of co-design between PSDF application modeling
and scheduling in Section 4.24.5.2, and we demonstrate the utility of this approach in deriving efﬁcient,
model-based implementations of dynamically reconﬁgurable signal processing systems.
4.24.5.1 Modular mapping
We develop a systematic approach for mapping PSDF speciﬁcations into hardware implementations.
Because of natural correspondences that are used between dataﬂow design objects (actors and edges)
and corresponding hardware structures, as well as between specialized PSDF modeling features and

4.24.5 Hardware Mapping
655
Table 24.1 Mapping PSDF Constructs to Hardware
PSDF modeling components
Hardware components
Actor
Circuit block
Edge
Buffer (e.g., FIFO)
Schedule
Graph controller
Parameter propagation path
Wire
Operational semantics
Subsystem controller
their implementations, the approach provides a high degree of modularity. In this modular approach,
implementations are composed in terms of smaller building blocks that can be tested independently and
integrated precisely through our mapping of PSDF semantics into hardware control.
Previous work on mapping dataﬂow structures into hardware include the work on VLSI dataﬂow
arrays [53], multidimensional arrayed dataﬂow [54], and SystemC [55]. The methods developed in this
paper are different from these approaches in their support for parameterized dataﬂow modeling, and
the novel features of dynamic parameter reconﬁguration and reconﬁgurable dataﬂow modeling that are
providedbyPSDFsemantics[26,50].Duetothepotentialforapplyingparameterizeddataﬂowsemantics
with arbitrary dataﬂow models of computation (subject to suitable deﬁnitions of graph iterations), the
integration of the techniques presented in this paper with the models used in the aforementioned works
is an interesting direction for further study.
PSDF and PSDFsim modeling constructs—in particular, PSDF actors, edges, schedules, parameter
propagation paths, and operational semantics—map naturally into corresponding hardware structures.
Table 24.1 summarizes our methodology for deriving such mappings.
Although the complexity of circuit blocks can vary widely, the top-down application of PSDF prin-
ciples provides a standardized design style for the interaction between different circuit blocks and
for the interaction between circuit blocks and the associated control for scheduling and parameter
management for the blocks. This allows for signiﬁcant reuse of parameterized HDL “glue code,” as
well as corresponding streamlining of veriﬁcation effort.
We employ self-timed scheduling and control of dataﬂow actors within a PSDF context. In a such a
self-timed approach, actors can ﬁre as soon as they have sufﬁcient data on their input ports, have access
to sufﬁcient empty buffer slots on their output ports, and have their current parameter values available,
as determined by the associated subinit and init graphs. Such self-timed hardware mapping is natural
for signal processing oriented dataﬂow models of computation (e.g., see [5]). The modularity of the
approach is enhanced because the controllers for individual actors are structured independently of any
global scheduling control, which allows scheduling strategies to be changed efﬁciently, conveniently,
and reliably. In this approach, only loop counts associated with actor control vary with changes in the
schedule control, and such adaptation of loop counts can be carried out naturally as actor parameter
updates through the overall framework of parameterized dataﬂow.
Figure 24.8 illustrates the architecture of a standard wrapper for PSDF-based interfacing of actor
circuit blocks. Here, the blocks labeled counter, controller, and loop count handle control and iteration
management within the functional unit of the actor, which can be of arbitrary complexity. The blocks

656
CHAPTER 24 Mapping Parameterized Dataﬂow Graphs onto FPGA Platforms
unit
counter
controller
loop
count
Buffer
Buffer
circuit
prod
Circuit Block
cons
circuit
function
FIGURE 24.8
Interface and control architecture for a circuit block.
Controller
F
S
A
E
A
E
B
C
D
S
F
B
C
D
I
init
body
I
Buffer
subinit
H
top level
Subsystem
Controller
Graph
Controller
Graph
FIGURE 24.9
An illustration of subsystem-level hardware mapping.
labeled cons circuit and prod circuit handle input and output interfacing of the actor based on dataﬂow
rates that may be parameterized and dynamically conﬁgured.
The structure of hardware mapping at the PSDF subsystem level is illustrated in Figure 24.9. The
controllers associated with the structures of Figures 24.8 and 24.9 are illustrated in Figure 24.10. In
comparison with the circuit block, the other hardware components are relatively less complicated, and to
provide ﬂexibility, we do not constrain the implementations of these components to any particular styles.
For example, FIFOs can be implemented using D Flip-Flops or SRAMs. The subsystem controllers and
graph controllers (i.e., the controllers for the init, subinit and body graphs) follow PSDF operational
semantics and the generated schedule to guide execution of the graph controllers and the circuit blocks,
respectively. These two types of controllers are ﬁnite state machines in which control remains in a given
state while there is no triggering input. For instance, a graph controller remains in the EXE state until
the corresponding circuit block completes execution.

4.24.5 Hardware Mapping
657
SUBINIT
PARAM
CONS
EXE
DONE
PROD
EXE
IDLE
DONE
DONE
IDLE
LESS
(a)
(b)
(c)
(d)
IDLE
DONE
INIT
BODY
FIGURE 24.10
Finite state machines for (a) a circuit block, (b) a graph controller, (c) consumption and production circuits,
and (d) a subsystem controller.
The circuit block control, illustrated in Figure 24.10a, is a key part of our proposed method for
self-timed, PSDF hardware implementation. Such a circuit block control structure provides control for
an individual PSDF actor. At the beginning of a control iteration (the state labeled PARAM), the circuit
block conﬁgures any dynamically managed parameters based on the current settings and attempts to
consume data from the actor input port. The controller will block in the CONS state until all data has
arrived from the corresponding producer actor, and has been consumed for processing by the circuit
block. Then the controller enters the EXE state and activates the encapsulated functional unit to process
the input data and generate any output values. When the output data is ready, the prod circuit pushes
the output data onto the corresponding output edges in the PROD state. Finally, after all output data
has been written, the controller enters the DONE state. In the DONE state, if the ﬁring count within the
current loop execution matches the loop count, then the controller transitions back to the PARAM state
and waits for another circuit block iteration before proceeding; otherwise, the controller transitions to
the CONS state to consume tokens for the next ﬁring.
Based on the modularity of our hardware mapping approach, described earlier, the control structures
for actors need to be conﬁgured only by setting their respective loop counts. Such loop counts can
be derived and validated efﬁciently at the functional prototyping stage, using the PSDFsim tool in
conjunction with techniques to determine repetitions vectors of speciﬁc SDF conﬁgurations [6]. The
repetitions vector of an SDF graph gives the number of times each actor needs to be ﬁred in a periodic
schedule for the graph [15].
Thus, the overall design ﬂow involves applying PSDFsim for functional prototyping, performing
systematic hardware mapping using the approach described in this section, and then synthesizing and
deploying the resulting self-timed implementation on the target FPGA.

658
CHAPTER 24 Mapping Parameterized Dataﬂow Graphs onto FPGA Platforms
4.24.5.2 Schedule-based mapping
Effective scheduling is important in deriving efﬁcient implementations of dynamically reconﬁgurable
signal processing systems. However, scheduling in the presence of dynamic reconﬁguration is challeng-
ing because of the increased dynamics in the scheduling process, as well as the increased difﬁculty in
modeling and manipulating schedules whose structures change during execution.
The dataﬂow schedule graph (DSG), proposed in [7], is a dataﬂow based schedule representation
that helps to address these challenges. The DSG can be viewed as a model for representing schedules of
dataﬂow graphs that is itself rooted in dataﬂow semantics. This allows an integrated modeling approach
among applications, schedules, and their interactions.
DSG-based modeling of actors centers around two special kinds of actors, called schedule control
actors (SCAs) and reference actors (RAs). A DSG for a given processor (i.e., the DSG-based schedule
model for a given processor) can have at most one token at any given time within the graph. This token
serves to enable the next actor to be executed on the processor. The restriction that the total token count
be bounded by 1 enforces the constraint that a processor can execute at most one task at any given
time. Here, a “processor” can represent any computational resource that executes actors in a dedicated
(exactly one actor assigned to the resource) or time-multiplexed manner.
In contrast to conventional dataﬂow actors, which represent functional components from the original
application speciﬁcation (application actors), SCAs are dataﬂow actors that are dedicated to coordinat-
ing control ﬂow in derived schedules. On the other hand, RAs can be viewed as “pointers” to application
actors. These pointers are equipped with optional auxiliary computations. Intuitively, an RA represents
a scheduling “wrapper” that speciﬁes the computation that is executed when the corresponding actor is
“visited” during schedule execution. A basic form of RA is one that simply performs a guarded execu-
tion of the actor that it points to. A guarded execution of an actor does nothing if the actor does not have
sufﬁcient data on its inputs to complete its next ﬁring; if sufﬁcient input data is available, a guarded exe-
cution executes a single ﬁring of the actor. However, more capabilities—beyond just performing guarded
executions—can be incorporated into RAs using the optional auxiliary computations mentioned above.
Table 24.2 gives examples of several types of SCAs and summarizes properties of these actors. The
loop actor has two pairs of inputs and outputs. One pair is used to perform computations within the
loop repeatedly, while the other pair is used for conditionally branching into and exiting the loop based
on certain control conditions. Since there is only one token in the enclosing DSG, execution always
proceeds unambiguously either inside or outside the loop.
SCA actors can be paired with other SCA actors to provide special control functions that involve their
coordination.Forexample,caseandendcaseprovideDSGswiththecapabilityofselectingcomputations
conditionally. The number of outputs for a given case actor must match the number of inputs to the
Table 24.2 Examples of SCAs
SCA
# of inputs
# of outputs
loop
2
2
case
1
≥2
endcase
≥2
1

4.24.5 Hardware Mapping
659
corresponding endcase actor to provide conditional selection of the computations that are enclosed by
the matching case and endcase pair.
As described earlier, tokens that ﬂow along edges of the DSG serve to enable actors for execution (as
it becomes their turn to execute). DSG tokens can also contain values that are manipulated and queried
during execution of the DSG to achieve various forms of data- or parameter-dependent schedule control.
The execution of a PSDF speciﬁcation involves careful coordination, based on details of PSDF
semantics, among the init, subinit, and body graphs within the reconﬁgurable subsystems that are
enclosed by the speciﬁcation. By modeling this PSDF-driven coordination in terms of DSGs, we can
precisely represent PSDF execution (i.e., the operational semantics of PSDF) in terms of pure dataﬂow
concepts, thereby enabling analysis and manipulation of schedules based on dataﬂow techniques rather
than having to rely on specialized PSDF-based methods. Moreover, such a PSDF to DSG transformation
allows PSDF graphs to be implemented through reuse of dataﬂow techniques rather than through
specialized implementation structures that are derived for PSDF. Such a transformation thus combines
the high level modeling ﬂexibility and analysis potential offered by PSDF with streamlined paths to
implementation offered through the use of the DSG as an intermediate representation.
A general DSG model for PSDF execution is illustrated in Figure 24.11. Parameter reconﬁguration
is achieved through communication between RAs and application actors through DSG tokens that
encapsulate updated parameter values. For example, if the init graph changes the value of a parameter
associated with a body graph actor A, the DSG token can “carry” this value (e.g., within a list of
pending parameter updates) to A for reconﬁguration. Once the body graph is executed, and the DSG
token “reaches” the RA that encapsulates A, the parameter update can be “unpacked” from the DSG
token and applied to A before A executes. A similar approach can be used to achieve parameter control
of the subinit graph by the init graph, and of the body graph by the subinit graph.
1
C
B
DSG for
the subinit graph
RB
RA
the init graph
DSG for
1oop
D
DSG for
the body graph
RC
D
body
init
subinit
subsystem
1
1
1 1
1
A
FIGURE 24.11
Modeling PSDF execution using DSGs.

660
CHAPTER 24 Mapping Parameterized Dataﬂow Graphs onto FPGA Platforms
Thus, using the DSG model, processes associated with dynamic reconﬁguration can be abstracted
in a way that precisely and ﬂexibly represents the relevant functionality while hiding platform-speciﬁc
implementation details about how the reconﬁguration is achieved. For example, whether the init graph
stores parameter values in registers or memory, and how the DSG token “points to” such storage locations
to reference the associated conﬁguration settings, are left as implementation details that can be reﬁned
from the DSG-based model.
DSGs can be applied not only as a target for automated schedule generation techniques, but also as
a model in which designers specify, experiment with, and iteratively reﬁne schedules. Because they are
rooted in familiar dataﬂow principles, rather than specialized or esoteric scheduling notations, designers
can work with DSG representations using well understood modeling concepts. In such a way, designers
can experiment ﬂexibly with schedules that interact with application actors, and apply platform-based
featuresfordynamicreconﬁguration.Byprovidingsuchﬂexibilityandfacilitatingsuchexperimentation,
DSG-based hardware mapping of PSDF graphs can help designers explore complex design spaces
associated with dynamically reconﬁgurable signal processing systems, and tailor implementations based
on the speciﬁc implementation constraints for a given application.
In the following section, we explore this integrated PSDF- and DSG-driven design methodology
using two case studies that involve relevant signal processing applications.
4.24.6 Case studies
In this section, we present two case studies with which we concretely demonstrate our proposed methods
for model-based implementation of dynamically reconﬁgurable signal processing systems.
4.24.6.1 Reconﬁgurable phase-shift keying
First, we demonstrate our PSDF-based design methodology and modular hardware mapping techniques
using a reconﬁgurable phase-shift keying (PSK) application that can be conﬁgured as binary PSK
(BPSK), quadrature PSK (QPSK) or 8PSK. We construct PSDF models of the modulator and demod-
ulator for this system, and develop Java-based functional DIF code to specify the internal functionality
of each actor. The resulting PSDF program is simulated and tested using PSDFsim, and then hardware
mapping is applied to the modulator to derive a Verilog implementation. HDL simulation and synthesis
is then applied to validate the derived hardware.
Figure 24.12 illustrates our PSDF model of the targeted system for reconﬁgurable PSK. Here, D rep-
resents an input interface that injects samples from the incoming data stream into the dataﬂow graph; T
and P are parameterized lookup tables; I1 is an actor that conﬁgures the consumption rate (based on M)
of T ; S2 and S4 provide trigonometric functions that are selected based on a dynamic parameter setting;
I3 conﬁgures the production rate of P; A is an adder; X12 and X34 are constant multipliers whose asso-
ciated constants (scaling factors) are managed as dynamic parameters; and B is an output interface for the
storing or further processing of the resulting binary sequence. The input interface D makes two copies
of each input token on its output since two separate multiplications are required for each input sample.
Our PSDF model involves a parameter M, which determines which form of PSK to employ. For
M = 1,2,3, an SDF graph associated with BPSK, QPSK, and 8PSK, respectively, is effectively acti-
vated. After the system model is constructed, we use PSDFsim to simulate the system and validate the

4.24.6 Case Studies
661
C
S
T
X12
A
I1
S2
init
subinit
H1
H2
1
1
1
2
1
2
M
(a) PSK modulator.
C
D
X34
P
I3
S4
init
subinit
H3
H4
B
1
M 1
(b) PSK demodulator.
2
1
1
1
FIGURE 24.12
PSDF-based models of PSK modulator and demodulator.
functionality for the different values of M. This initial simulation is performed assuming no distortion
of data in the channel.
Since channel quality is critical to the choice of PSK, we can modify actor C to model the noise in
the channel, and analyze the simulation results under different PSK conﬁgurations. PSDFsim enables
such multi-mode application simulation to be executed in an integrated manner—i.e., as a single simu-
lation that includes all PSK conﬁgurations along with simulation control functionality that dynamically
changes the conﬁguration.
Our hardware mapping of the modulator is illustrated in Figure 24.13. Here, the ﬁller block represents
an actor that is inserted to help maintain PSDF operational semantics. Since the init and subinit graphs
here both contain one node each, their associated graph controllers can be removed. Note also that the
circuit blocks associated with blocks T and X12 are parameterized and receive parameter value updates
from circuit blocks I1 and S2.
To provide an area comparison, we instantiate three separate PSK circuits that support BPSK, QPSK,
and 8PSK individually using SDF-based models. We compare this pure-SDF-based implementation with
our PSDF implementation, which is derived using PSDFsim and our proposed design methodology.
Synthesis results generated by the Cadence Encounter RTL Compiler are shown in Table 24.3. Although
there is some area overhead in the PSDF implementation due to the controllers and auxiliary circuits
used for the init and subinit graphs, this overhead is more than compensated for by the hardware reuse
that is facilitated by the ﬂexible, dynamic parameterization capabilities of PSDF.
This modular hardware mapping approach is readily applied due to its generality, and is also useful as
it provides a standard method to realize hardware implementations of PSDF graphs. Our schedule-based

662
CHAPTER 24 Mapping Parameterized Dataﬂow Graphs onto FPGA Platforms
Controller
T
S2
A
S
buffer
I1
X12
Controller
Subsystem
Controller
Graph
Controller
Graph
filler circuit block
Subsystem
FIGURE 24.13
Hardware mapping for modulator.
Table 24.3 Comparisons for PSK Modulator System
Area of PSDF design and SDF design (modular hardware mapping)
PSDF (cell)
SDF (cell)
Reduction
20,004
33,602
40.47% (1.68X)
hardware architecture mapping approach using DSGs provides a complementary method, which can
be used (e.g., in later stages of the design process) to specialize the hardware mapping for a speciﬁc
application, and capture the structure of such specialized mappings in an abstract form that can be
targeted subsequently to platform-speciﬁc, hardware control structures (see Figure 24.12a).
From its formal, dataﬂow-based structure, the DSG is well-suited for transformation into optimized
ﬁnite state machine (FSM) structures that provide control logic for hardware implementation of the
associated schedules. Figure 24.14b illustrates a DSG representation for the reconﬁgurable PSK appli-
cation, along with an FSM that is derived from the DSG. Most of the states map to distinct RAs, and
execute the functionality associated with the associated RAs. Since the loop iteration count of loop2 is
ﬁxed, the state RS2 is designed to implement loop control as well as ﬁring the actor S2.
In our experiments with schedule-based hardware mapping, we targeted ASIC implementation using
the Cadence Encounter RTL Compiler for back-end synthesis. The results reported here are syn-
thesis results only (the design was tested thoroughly but not actually fabricated). Table 24.4 shows
the improvement in area that is achieved by the streamlined DSG representation compared to the

4.24.6 Case Studies
663
D
RS
RT
RA
loop2
loop1
D
S
R
RI1
RX12
2
(a) A DSG for the reconﬁguration PSK modulator of Figure 24.12(a).
1
S
IDLE
RT
RA
DONE
1. fire the actor I
2. query the parameter
1. get response from query
2. fire the actor S
fire the actor X
RX12
S
R
2
I
R
1
if counter = 2
go to state R
else
fire the actor S
go to state R
counter++
A
X12
2
12
R
(b) An FSM for the DSG in Figure 24.15(a).
FIGURE 24.14
Hardware architecture mapping for a DSG.
modular PSDF-to-hardware mapping approach of Section 4.24.5.1. This improvement is accompanied
by a formal, dataﬂow based representation of schedule logic, which can be retargeted systematically to
other types of platforms for rapid prototyping and experimentation with platform-speciﬁc implementa-
tion trade-offs.

664
CHAPTER 24 Mapping Parameterized Dataﬂow Graphs onto FPGA Platforms
subsystem 2
1
S
1I
preprocessing
video
init
subinit
body
reader
video
2
S
2I
frame
current
BG
model
switch
FG
extractor
result
init
body
subinit
subsystem 1
FIGURE 24.15
A general PSDF model for FG/BG extraction.
Table 24.4 Area Comparison for Reconﬁgurable PSK Modulator Under Constant Speed
(100 MHz)
Hardware mapping
Schedule-based
Modular
Reduction
Area (cell)
18,949
20,004
5.27%
4.24.6.2 Foreground/background extraction
Video surveillance is widely used for security enhancement and environmental monitoring. As the
demand for video surveillance increases, the volume of data that must be analyzed for surveillance
applications increases dramatically as well. Pattern recognition helps to incorporate automation in this
analysis process, and make it practical with limited human resources for monitoring surveillance data.
To be effective, pattern recognition techniques are often task speciﬁc with signiﬁcant ﬁne tuning of
system conﬁgurations and algorithm parameters in terms of the kinds of data being analyzed and the
objectives of the analysis. At the same time, the vast amount of data that needs to be processed in typical
applications can make software-based implementation (e.g., using MATLAB and C) impractical.
Considering these two issues—the need for task-speciﬁc tuning and high performance—PSDF map-
ping to FPGAs provides a potential solution method, where both parameter adaptation and high perfor-
mance hardware mapping can be supported and optimized through an integrated design process.
In a workload analysis study of video surveillance systems, it has been shown that the most expensive
computation is foreground/background (FG/BG) extraction [56]. We demonstrate a general PSDF model
of FG/BG extraction that can accommodate a variety of FG/BG extraction algorithms. This model is
shown in Figure 24.15.
The FG/BG extraction algorithms represented by this model generally involve two phases—training
and differentiating. In the training phase, the construction of the BG model is based on features
extracted from a set of training frames. This model construction process involves determining appropri-
ate threshold values for pixels. Then, in the differentiating phase, the BG model is applied to recognize

4.24.6 Case Studies
665
current frame
block 1
block 2
block 3
block 4
block 5
block 6
block 7
block 8
block 9
1t
2t
3t
block 2
differentiate
BG model
FIGURE 24.16
A non-uniform array of threshold values.
the foreground—if a given pixel of the current frame exceeds the associated threshold value, it is
recognized as a foreground pixel; otherwise, it is recognized as a background pixel. The training meth-
ods and threshold values vary with different algorithms and applications, and careful tuning of these
key aspects is typically important to achieve high accuracy [57].
Our PSDF model shown in Figure 24.15 contains two subsystems, which are used to specify algo-
rithms for video preprocessing and FG/BG extraction. The video preprocessing subsystem here can
be viewed as an auxiliary subsystem, which processes raw data, and transforms it into a form that is
appropriate for the extraction algorithms and the underlying processing platforms. In subsystem 2, actor
switch passes video frames to actor BG_model and current_frame in the training phase. At that
point, actor FG_extractor produces no foreground. In the differentiating phase, actor switch stops
sending frames to actor BG_model, and continues to send frames to actor current_frame. If a
pixel of the current frame exceeds the corresponding threshold value, actor FG_extractor indicates
that the pixel is part of the foreground.
We apply a speciﬁc FG/BG extraction algorithm—the running average algorithm—using the general
PSDF model of Figure 24.15. The running average algorithm averages the values of the pixels in the
training frames to create the BG model. The target implementation platform for our experiments with
the running average algorithm is the Xilinx Spartan 3E Starter (XC3S500E). RS232 and VGA ports are
selected as the input and output interfaces, respectively.
Since the total capacity of block RAM (BRAM) on the target platform is only 360K bits, only one
monochrome 640 × 480 (307,200 bits) frame can be accommodated. Here, every bit represents one
pixel and the difference between the same pixel of two frames is either 0 or 1. For natural mapping from
our general PSDF model of FG/BG extraction, we need one storage subsystem for the BG model and
another one for the current frame.
In this implementation of the model in Figure 24.15, actors BG_model and current_frame
“own”theirassociatedimagestorageandarecontrolledbyactorswitch.ActorFG_extractortakes
two frames from actor BG_model and current_frame for differentiation and determines which
parts of the frame are foreground. A non-uniform array of threshold values, shown in Figure 24.16, is
derived from the training processes. The actual threshold values represented in Figure 24.16 are regarded
as parameters, which can be adapted based on video stream characteristics.

666
CHAPTER 24 Mapping Parameterized Dataﬂow Graphs onto FPGA Platforms
FIGURE 24.17
Foreground/background extraction on FPGA, and associated experimental setup.
In this thresholding approach, multiple pixels are grouped into individual blocks, and the sum
(number of 1-valued pixels) in a block is computed to characterize the block and compare it with
the corresponding block-based threshold. These block-based thresholds characterize entire blocks with
a single operations—that is, the entire block is characterized as foreground and background based on
the associated threshold comparison. For example, if the pixel sum associated with block 2 is larger
than t2, the block is classiﬁed as being part of the foreground. For more detailed background on this
thresholding approach, we refer the reader to [58].
In our experiments, the threshold values are derived from the process of BG model training and
stored in actor BG_model. One block is composed of eight pixels. The parameters of subsystem 2 are
summarized as follows.
•
Init graph:
•
baud rate of RS232 receiver,
•
number of frames for BG_model training,
•
subinit graph:
•
switching on/off the path from actor switch to BG_model,
•
threshold value of discrimination between FG and BG.

4.24.6 Case Studies
667
The overall implementation involves heterogeneous design languages and platforms—subsystem
1 is implemented in MATLAB and executes on a host PC, subsystem 2 is implemented in Verilog
and executes on the targeted FPGA, and the dataﬂow edge between the video_preprocessing
and switch components represents the RS232 channel, where the transmitter and receiver are in the
video_preprocessing and switch components, respectively. The baud rates of the transmit-
ter and receiver should be consistent. The video_preprocessing component selects frames and
converts them into monochrome format based on luminance levels. The modular hardware mapping
process developed in Section 4.24.5 is adopted throughout the implementation process. The parameters
of subsystem 1 are summarized as follows.
•
Init graph:
•
baud rate of RS232 transmitter,
•
luminance level;
•
subinit graph:
•
frame selection.
Figure 24.17 shows our experimental setup. We use MATLAB to read the video from ﬁles on the
host platform, and divide the video data into frames. To reduce the level of serial communication, we
select every tenth frame and convert it to monochrome format based on a luminance level threshold
of 0.3. The threshold value is set to 4, which was the value that we obtained from the training process
described in Section 4.24.6.
Results from our experimentation are illustrated in Figures 24.18 and 24.19. Figure 24.18 shows six
frames from a video sequence. Figure 24.18a shows the common background scene for the sequence,
while Figure 24.18b–f show frames in which a man runs from left to right. These six frames, after
processing by our implementation of foreground/background extraction on the FPGA, are shown in
Figure 24.19a–f, respectively. Here, the red rectangle indicates the subtracted foreground. The existence
of black pixels outside of the subtracted foreground is due to the perturbation of trees caused from
ambient breeze. Distortion from this phenomenon can be reduced by a sophisticated algorithm [59].
Our results shown in Figure 24.19a–f demonstrate that the image of the running man can be extracted
correctly as foreground.
Table 24.5 summarizes synthesis results obtained when deriving the FPGA implementation. The
maximum frequency is 84.062 MHz. By the maximum frequency, we mean the clock frequency that
the FPGA logic can execute at without violating the timing constraints. The video frame rate is set in
our experiments to 30 frames/s. The utilization of block RAMS (BRAMs) is high since they are used
to store the video frames, whereas the utilization of FPGA slices is relatively low because the running
average algorithm does not require complex computation.
In summary our experiments involving foreground/background extraction on an FPGA demonstrate
the correctness and completeness of our proposed PSDF-based approach for FPGA mapping on a
practical video processing system. Tuning application parameters at run-time is an important feature for
advanced image processing applications, which we seek to support in this work. However, conventional
dataﬂow approaches, including SDF, do not allow such run-time parameter tuning. For this reason, we

668
CHAPTER 24 Mapping Parameterized Dataﬂow Graphs onto FPGA Platforms
(a)
(b)
(c)
(d)
(e)
(f)
FIGURE 24.18
Selected frames from a video sequence.
Table 24.5 Device Utilization Summary
Selected device: 3s500efg320-4
Number of slices
133 out of 4656
2%
Number of slice Flip ﬂops
90 out of 9312
0%
Number of 4 input LUTs
245 out of 9312
2%
Number of IOs
16
Number of bonded IOBs
11 out of 232
4%
Number of BRAMs
16 out of 20
80%
Number of MULT18 × 18SIOs
1 out of 20
5%
Number of GCLKs
1 out of 24
4%
have focused in our experiments on the PSDF model, and novel application of this model to dynamically
parameterized image processing on FPGAs. The experiments demonstrate the capability of the PSDF
model to express the behavior of this application, and show that the abstract properties of PSDF, which
provide formal, model-based manipulation of scheduling and dynamic reconﬁguration, can be integrated
with platform-speciﬁc details required to achieve a fully operational implementation.

4.24.7 Conclusion
669
(a)
(b)
(c)
(d)
(e)
(f)
FIGURE 24.19
Results for Figure 24.18 after processing by our FPGA-based implementation of foreground/background
extraction.
4.24.7 Conclusion
In this paper, we have motivated the use of dynamically reconﬁgurable hardware platforms for signal
processing systems, and have presented background on hardware methods for dynamic reconﬁguration.
We have then motivated how parameterized dataﬂow techniques integrated with the synchronous
dataﬂow model of computation, which results in the parameterized synchronous dataﬂow (PSDF) mod-
eling approach, can be applied as an abstract model for design and implementation of dynamically
reconﬁgurable signal processing systems.
We have demonstrated a PSDF-based design methodology and associated simulation tool, called
PSDFsim, for design and implementation of signal processing systems on dynamically reconﬁgurable
platforms. We have also demonstrated the use of dataﬂow schedule graphs as a formal model for rep-
resenting and manipulating hardware mappings of PSDF graphs throughout the design process. We
have discussed the use of these methods to help streamline the processes of rapid prototyping, hetero-
geneous system design, hardware mapping, and implementation. Our experiments show improvements
in simulation efﬁciency and in the quality of synthesized solutions. Furthermore, in contrast to ad hoc
techniques for applying dynamic parameter control to SDF graphs or other kinds of design subsystems,
the PSDF-based approach that we have presented provides for well-structured integration of parameter

670
CHAPTER 24 Mapping Parameterized Dataﬂow Graphs onto FPGA Platforms
management into the SDF framework. This leads to more efﬁcient and reliable techniques for application
of dynamically reconﬁgurable platforms.
Important directions for further work include exploration of hardware mapping techniques for more
general forms of parameterized dataﬂow, such as parameterized cyclo-static dataﬂow and parameterized
fractional rate dataﬂow [21,25,60], and techniques for mapping parameterized dataﬂow graphs into
platform FPGAs considering more thoroughly the available sets of heterogeneous resource groups
(e.g., hard and soft-core processors and application-speciﬁc accelerators).
Relevant Theory: Signal Processing Theory
See Vol. 1, Chapter 6 Digital Filter Structures and Their Implementation
References
[1] W. Wolf, FPGA-Based System Design, Prentice Hall, 2004.
[2] P.S. Zuchowski, C.B. Reynolds, R.J. Grupp, S.G. Davis, B. Cremen, B. Troxel, A hybrid ASIC and FPGA
architecture, in: Proceedings of the 2002 IEEE/ACM International Conference on Computer-Aided Design,
ACM, New York, NY, USA, 2002, pp. 187–194.
[3] C. Kao, Beneﬁts of partial reconﬁguration, Xilinx Xcell 55 (2005) 65–67.
[4] E.A. Lee, S.A. Seshia, Introduction to Embedded Systems, A Cyber-Physical Systems Approach, 2011.
<http://LeeSeshia.org> (ISBN 978-0-557-70857-4).
[5] S.S. Bhattacharyya, E. Deprettere, R. Leupers, J. Takala (Eds.), Handbook of Signal Processing Systems,
Springer, 2010.
[6] H. Wu, H. Kee, N. Sane, W. Plishker, S.S. Bhattacharyya, Rapid prototyping for digital signal processing
systems using parameterized synchronous dataﬂow graphs, in: Proceedings of the International Symposium
on Rapid System Prototyping, Fairfax, Virginia, June 2010, pp. 1–7.
[7] H. Wu, C. Shen, N. Sane, W. Plishker, S.S. Bhattacharyya, A model-based schedule representation for het-
erogeneous mapping of dataﬂow graphs, in: Proceedings of the International Heterogeneity in Computing
Workshop, Anchorage, Alaska, May 2011, pp. 66–77.
[8] P. Maidee, C. Ababei, K. Bazargan, Timing-driven partitioning-based placement for island style FPGAs, IEEE
Trans. Computer-Aided Design Integrat. Circ. Syst. 24 (3) (2005) 395–406.
[9] G. Kahn, The semantics of a simple language for parallel programming, in: Proceedings of the IFIP Congress,
1974.
[10] M. Geilen, T. Basten, Reactive process networks, in: Proceedings of the International Workshop on Embedded
Software, September 2004, pp. 137–146.
[11] S. Meijer, H. Nikolov, T. Stefanov, Throughput modeling to evaluate process merging transformations in
polyhedral process networks, in: Proceedings of the Conference on Design, Automation and Test in Europe,
Leuven, Belgium, European Design and Automation Association, 2010, pp. 747–752.
[12] S. Verdoolaege, Handbook of Signal Processing Systems, ﬁrst ed., Springer, 2010 (Chapter 4).
[13] E.A. Lee, T.M. Parks, Dataﬂow process networks, Proc. IEEE (1995) 773–799.
[14] W. Plishker, N. Sane, M. Kiemb, K. Anand, S.S. Bhattacharyya, Functional DIF for rapid prototyping,
in: Proceedings of the International Symposium on Rapid System Prototyping, Monterey, California, June
2008, pp. 17–23.
[15] E.A. Lee, D.G. Messerschmitt, Synchronous dataﬂow, Proc. IEEE 75 (9) (1987) 1235–1245.

References
671
[16] A.H. Ghamarian, M.C.W. Geilen, S. Stuijk, T. Basten, A.J.M. Moonen, M.J.G. Bekooij, B.D. Theelen,
M.R. Mousavi, Throughput analysis of synchronous data ﬂow graphs, in: Proceedings of the International
Conference on Application of Concurrency to System Design, June 2006.
[17] R. Govindarajan, G.R. Gao, P. Desai, Minimizing buffer requirements under rate-optimal schedule in regular
dataﬂow networks, J. VLSI Signal Process. 31 (3) (2002) 207–229.
[18] C. Hsu, M. Ko, S.S. Bhattacharyya, S. Ramasubbu, J.L. Pino, Efﬁcient simulation of critical synchronous
dataﬂow graphs, ACM Trans. Design Automat. Electron. Syst. 12 (3) (2007) 28.
[19] H. Oh, N. Dutt, S. Ha, Memory optimal single appearance schedule with dynamic loop count for synchronous
dataﬂowgraphs,in:ProceedingsoftheInternationalConferenceonAsiaandSouthPaciﬁcDesignAutomation,
IEEE Press, 2006, pp. 497–502.
[20] S. Sriram, S.S. Bhattacharyya, Embedded Multiprocessors: Scheduling and Synchronization, second ed., CRC
Press, 2009.
[21] G. Bilsen, M. Engels, R. Lauwereins, J.A. Peperstraete, Cyclo-static dataﬂow, IEEE Trans. Signal Process.
44 (2) (1996) 397–408.
[22] S.S. Bhattacharyya, E.A. Lee, Memory management for dataﬂow programming of multirate signal processing
algorithms, IEEE Trans. Signal Process. 42 (5) (1994) 1190–1201.
[23] T.M. Parks, J.L. Pino, E.A. Lee, A comparison of synchronous and cyclo-static dataﬂow, in: Proceedings of
the IEEE Asilomar Conference on Signals, Systems, and Computers, November 1995.
[24] F. Haim, M. Sen, D. Ko, S.S. Bhattacharyya, W. Wolf. Mapping multimedia applications onto conﬁgurable
hardware with parameterized cyclo-static dataﬂow graphs, in: Proceedings of the International Conference on
Acoustics, Speech, and Signal Processing, May 2006, pp. III-1052–III-1055.
[25] S. Saha, S. Puthenpurayil, S.S. Bhattacharyya, Dataﬂow transformations in high-level DSP system design,
in: Proceedings of the International Symposium on System-on-Chip, Tampere, Finland, November 2006, pp.
131–136 (Invited paper).
[26] B. Bhattacharya, S.S. Bhattacharyya, Parameterized dataﬂow modeling of DSP systems, in: Proceedings
of the International Conference on Acoustics, Speech, and Signal Processing, Istanbul, Turkey, June 2000,
pp. 1948–1951.
[27] I. Kuon, J. Rose, Measuring the gap between FPGAs and ASICs, in: Proceedings of the ACM International
Symposium on Field Programmable Gate Arrays, 2006.
[28] Xilinx, PlanAhead User Guide, vol. 11.4, Xilinx, Inc., 2009.
[29] Altera, FPGA Run-time Reconﬁguration: Two Approaches, White Paper, Altera, March 2008.
[30] Xilinx, JBits SDK, 2004. <http://www.xilinx.com/products/jbits/>.
[31] D. Mesquita, O Moraes, J. Palma, R. M˝oller, N. Calazans, Remote and partial reconﬁguration of FPGAs:
tools and trends, in: Proceedings of the International Symposium on Parallel and Distributed Processing,
2003, pp. 22–26.
[32] J. Stockwood, P. Lysaght, A simulation tool for dynamically reconﬁgurable ﬁeld programmable gate arrays,
IEEE Trans. VLSI Syst. 4 (1995) 381–390.
[33] J. Becker, M. Hübner, M. Ullmann, Run-time FPGA reconﬁguration for power-/cost-optimized real-time
systems, IFIP Int. Federation Inform. Process. 200 (2006) 119–132.
[34] M.J. Wirthlin, B.L. Hutchings, A dynamic instruction set computer, in: Proceedings of IEEE Workshop on
FPGAs for Custom Computing Machines, 1995, pp. 99–107.
[35] R. Lysecky, F. Vahid, A conﬁgurable logic architecture for dynamic hardware/software partitioning, in: Pro-
ceedings of the Conference on Design, Automation and Test in Europe, IEEE Computer Society, Washington,
DC, USA, 2004, p. 10480.
[36] R. Lysecky, F. Vahid, A study of the speedups and competitiveness of FPGA soft processor cores using
dynamic hardware/software partitioning, in: Proceedings of the conference on Design, Automation and Test
in Europe, IEEE Computer Society, Washington, DC, USA, 2005, pp. 18–23.

672
CHAPTER 24 Mapping Parameterized Dataﬂow Graphs onto FPGA Platforms
[37] P.M. Heysters, J. Smit, G.J.M. Smit, P.J.M. Havinga, Mapping of DSP algorithms on ﬁeld programmable func-
tion arrays, in: Proceedings of the International Workshop on Field-Programmable Logic and Applications,
London, UK, Springer-Verlag, 2000, pp. 400–411.
[38] G.J.M. Smit, P.J.M. Havinga, L.T. Smit, P.M. Heysters, M.A.J. Rosien, Dynamic reconﬁguration in mobile
systems, in: Proceedings of the Conference on Field Programmable Logic and Applications, Springer-Verlag,
2002, pp. 162–170.
[39] H. Wu, C. Shen, S.S. Bhattacharyya, K. Compton, M. Schulte, M. Wolf, T. Zhang, Design and implementation
of real-time signal processing applications on heterogeneous multiprocessor arrays, in: Proceedings of the
IEEE Asilomar Conference on Signals, Systems, and Computers, Paciﬁc Grove, California, November 2010,
pp. 2121–2125 (Invited paper).
[40] L. Shang, R.P. Dick, N.K. Jha, SLOPES: hardware/software cosynthesis of low-power real-time distributed
embedded systems with dynamically reconﬁgurable FPGAs, IEEE Trans. Computer-Aided Design of Integrat.
Circ. Syst. 26 (3) (2007) 508–526.
[41] Y. Li, T. Callahan, E. Darnell, R. Harr, U. Kurkure, J. Stockwood, Hardware-software co-design of embedded
reconﬁgurable architectures, in: Proceedings of the Annual Design Automation Conference, ACM, New York,
NY, USA, 2000, pp. 507–512.
[42] S.S. Bhattacharyya, Hardware/software co-synthesis of DSP systems, in: Y.H. Hu (Ed.), Programmable Digital
Signal Processors: Architecture, Programming, and Applications, Marcel Dekker, Inc., 2002, pp. 333–378.
[43] J. Harkin, T.M. McGinnity, L.P. Maguire, Modeling and optimizing run-time reconﬁguration using
evolutionary computation, ACM Trans. Embed. Comput. Syst. 3 (2004) 661–685.
[44] A. Derbyshire, T. Becker, W. Luk, Incremental elaboration for run-time reconﬁgurable hardware designs,
in: Proceedings of the International Conference on Compilers, architecture and synthesis for embedded sys-
tems, New York, NY, USA, ACM, 2006, pp. 93–102.
[45] E.J. McDonald, Runtime FPGA partial reconﬁguration, in: IEEE Aerospace Conference, March 2008, pp.
1–7.
[46] D. Rupe, An FPGA framework supporting software programmable reconﬁguration and rapid development of
SDR applications, in: SDR Forum Technical Conference, November 2007.
[47] B. Bhattacharya, S.S. Bhattacharyya, Consistency analysis of reconﬁgurable dataﬂow speciﬁcations, in: E.F.
Deprettere, J. Teich, S. Vassiliadis (Eds.), Embedded Processor Design Challenges, Lecture Notes in Computer
Science, Springer, 2002, pp. 1–17.
[48] S. Neuendorffer, E. Lee, Hierarchical reconﬁguration of dataﬂow models, in: Proceedings of the International
Conference on Formal Methods and Models for Codesign, June 2004.
[49] R. Sass, A.G. Schmidt, Embedded Systems Design with Platform FPGAs: Principles and Practices, Morgan
Kaufmann Publishers Inc., 2010.
[50] B. Bhattacharya, S.S. Bhattacharyya, Quasi-static scheduling of reconﬁgurable dataﬂow graphs for DSP
systems, in: Proceedings of the International Workshop on Rapid System Prototyping, Paris, France, June
2000, pp. 84–89.
[51] B.D. Theelen, M.C.W. Geilen, T. Basten, J.P.M. Voeten, S.V. Gheorghita, S. Stuijk, A scenario-aware data
ﬂow model for combined long-run average and worst-case performance analysis, in: Proceedings of the
International Conference on Formal Methods and Models for Codesign, July 2006.
[52] C. Hsu, M. Ko, S.S. Bhattacharyya, Software synthesis from the dataﬂow interchange format, in: Proceedings
of the International Workshop on Software and Compilers for Embedded Systems, Dallas, Texas, September
2005, pp. 37–49.
[53] S.Y. Kung, P.S. Lewis, S.C. Lo, Performance analysis and optimization of VLSI dataﬂow arrays, J. Parallel
Distributed Comput. (1987) 592–618.
[54] J. Mcallister, R. Woods, R. Walke, D. Reilly, Multidimensional DSP core synthesis for FPGA, J. VLSI Signal
Process. Syst. Signal Image Video Technol. 43 (2–3) (2006).

References
673
[55] C. Haubelt, J. Falk, J. Keinert, T. Schlichter, M. Streubnhr, A. Deyhle, A. Hadert, J. Teich, A SystemC-based
design methodology for digital signal processing systems, EURASIP J. Embed. Syst. 2007, p. 22 (Article ID
47580).
[56] T.P. Chen, H. Haussecker, A. Bovyrin, R. Belenov, K. Rodyushkin, A. Kuranov, V. Eruhimov, Computer vision
workload analysis: case study of video surveillance systems, Intel Technol. J. 9 (2005).
[57] M. Piccardi, Background subtraction techniques: a review, in: Proceedings of the IEEE International Confer-
ence on Systems, Man, and Cybernetics, vol. 4, October 2004, pp. 3099–3104.
[58] R. Laganiere, OpenCV 2 Computer Vision Application Programming Cookbook, Packt Publishing, 2011.
[59] L. Li, W. Huang, I.Y.H. Gu, Q. Tian, Foreground object detection from videos containing complex background,
in: Proceedings of the ACM International Conference on Multimedia, ACM, New York, NY, USA, 2003,
pp. 2–10.
[60] H. Oh, S. Ha, Fractional rate dataﬂow model and efﬁcient code synthesis for multimedia applications, ACM
SIGPLAN Notices, vol. 37, July 2002.

25
CHAPTER
Distributed Estimation⋆
Yuzhe Xu*, Vijay Gupta†, and Carlo Fischione*
*School of Electrical Engineering, ACCESS Linnaeus Center, KTH Royal Institute of Technology, SE, Stockholm, Sweden
†Department of Electrical Engineering, University of Notre Dame, Notre Dame, IN, USA
4.25.1 Notation
Given a stochastic variable x, let E[x] denote its expected value, while Var[x] = E[x −E[x]]2 is its
variance. With Ex y(x) we mean that the expected value is taken with respect to the probability density
function (pdf) px(·) of x, where y is some function of the random variable x. Given a set of K nodes
at time n = 0, 1, 2, . . . , let xn,k denote the variable x from kth node at time n for all k = 1, 2, . . . , K.
Furthermore, let Xn denote the vector [xn,1, xn,2, . . . , xn,K ]T ∈RK at time n. With ˆx we denote the
estimate of the random variable x. With ∥· ∥we denote the ℓ2-norm of a vector or the spectral norm
of a matrix. Given a matrix A, ℓm(A) and ℓM(A) denote the minimum and maximum eigenvalue (with
respect to the absolute value of their real part), respectively, and its largest singular value is denoted by
γ (A). If A is a square matrix, we use tr(A) denote the trace of the matrix A, the sum of the elements
on its diagonal. Suppose the matrix B having same size of A, A ◦B is the Hadamard (element-wise)
product between A and B. With A† we denote the Moore-Penrose pseudo-inverse of the matrix A. With
a ⪯b and a ⪰b denote the element-wise inequalities. With I and 1 we denote the identity matrix and
the vector (1, . . . , 1)T , respectively, whose dimensions are clear from the context.
4.25.2 Network with a star topology
In this section, we assume that the network is organized as a star, where multiple sensors make mea-
surements that are transmitted with no messages losses to a fusion center, which is assumed to be the
star of the network. An example is illustrated in Figure 25.1.
4.25.2.1 Static sensor fusion
Here we study the problem of estimating a static phenomenon that is observed by a number of sensors.
The observations of these sensors are then reported to a central unit that fuses them with the aim of
extracting an estimate of higher accuracy.
⋆This work is supported by the Swedish Research Council and by the EU projects STREP Hydrobionets and NoE Hycon2.
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00025-X
© 2014 Elsevier Ltd. All rights reserved.
675

676
CHAPTER 25 Distributed Estimation
FIGURE 25.1
An example of star topology network with nodes and links (solid lines indicating that there is message
communication between nodes). In this network, node 9 can receive information from all other nodes. Thus
node 9 is the central unit.
4.25.2.1.1
Combining estimators
In this subsection, we study distributed Minimum Mean Square Estimators (MMSE). In Appendix A.1,
we recall the general result of MMSE for centralized linear estimators. Here, we rewrite those results
in an alternative form.
Proposition 1.
Let y = Hx + v, where H is a matrix and v is a zero mean Gaussian noise with
covariance matrix RV independent of X. Then the MMSE estimate of X given Y = y is
P−1 ˆx = H T R−1
V y
with P is the corresponding error covariance given by
P−1 =

R−1
X + H T R−1
V H

.
Proof.
The expression for P follows by applying the matrix inversion lemma in Appendix A.2. For
the estimate, consider
P−1 ˆx =

R−1
X + H T R−1
V H

RX H T 
H RX H T + RV
−1
y
= H T 
H RX H T + RV
−1
y + H T R−1
V H RX H T 
H RX H T + RV
−1
y

4.25.2 Network with a Star Topology
677
= H T R−1
V

H RX H T + RV
 
H RX H T + RV
−1
y
= H T R−1
V y.
□
This alternate form is useful because it combines local estimates directly without recourse to sending
all the measurements to a central data processing unit that runs a giant estimator. This is called static
sensor fusion.
4.25.2.1.2
Static sensor fusion for star topology
Proposition 2.
Consider a random variable x being observed by K sensors that generate measurements
of the form
yk = Hkx + vk,
k = 1, . . . , K,
where the noises vk are all uncorrelated with each other and with the variable x. Denote the estimate
of x based on all the n measurements by ˆx and the estimate of x based only on the measurement yk by
ˆxk. Then ˆx can be calculated using
P−1 ˆx =
K

k=1
P−1
k
ˆxk,
where P is the estimate error covariance corresponding to ˆx and Pk is the error covariance correspond-
ing to ˆxk. Further
P−1 =
K

k=1
P−1
k
−(K −1)R−1
X .
Proof.
Denote y as the stacked vector of all the measurements yks, H the corresponding measurement
matrix obtained by stacking all the Hks and v the noise vector obtained by stacking all the noises vks.
The global estimate ˆx is given by
P−1 ˆx = H T R−1
V y.
But all the vks are uncorrelated with each other. Hence RV is a block diagonal matrix with blocks RVk.
Thus the right hand side can be decomposed as
H T R−1
V y =
K

k=1
H T
k R−1
Vk yk.
But each of the terms H∗
k R−1
Vk yk can be written in terms of the local estimates
P−1
k
ˆxk = H T
k R−1
Vk yk.
Thus
P−1 ˆx =
K

k=1
P−1
k
ˆxk.
The proof for the expression for the global error covariance is similar.
□

678
CHAPTER 25 Distributed Estimation
This result is useful since it allows the complexity of calculation at the fusion center to go down
considerably.1 Of course it assumes that the sensors can do some computation, but that is reasonable. The
form of the global estimator shows that what we really want is a weighted mean of the local estimates.
Each estimate is weighted by the inverse of the error covariance matrix. Thus more conﬁdence we have
in a particular sensor, more trust do we place in it.
4.25.2.1.3
Sequential measurements from one sensor
The same algorithm can be extended to the case when there are multiple measurements from one sensor.
Furthermore, the processing can be done in a sequential manner. Consider a random variable evolving
in time as
Xn+1 = AXn + wn,
where wn is white zero mean Gaussian noise with covariance matrix Q. The sensor generates a mea-
surement at every time step according to the equation
Yn = C Xn + vn,
where vn is again white zero mean Gaussian noise with covariance matrix R. We wish to obtain an
estimate of Xn given all the measurements {Y0, Y1, . . . , Yn}. Suppose we divide the measurements into
two sets:
1. The measurement Yn.
2. The set Y of the remaining measurements Y0 through Yn−1.
Now note that the two sets of measurements are related linearly to Xn and further the measurement
noises are independent. Thus we can combine the local estimates to obtain a global estimate. First we
calculate the estimate of Xn based on Yn. It is given by
M−1 ˆX = CT R−1Yn,
where M is the error covariance given by
M−1 = R−1
Xn + CT R−1C.
Let ˆXn−1|n−1 be the estimate of Xn−1 based on Y and Pn−1|n−1 be the corresponding error covariance.
Then the estimate of Xn given Y is given by
ˆXn|n−1 = A ˆXn−1|n−1
with the error covariance
Pn|n−1 = APn|n−1AT + Q.
1As an exercise, compare the number of elementary operations (multiplications and additions) for the two algorithms.

4.25.2 Network with a Star Topology
679
Thus the estimate of Xn given all the measurements is given by the combination of local estimates and
can be seen to be
P−1
n|n ˆXn|n = P−1
n|n−1 ˆXn|n−1 + M−1 ˆX
= P−1
n|n−1 ˆXn|n−1 + CT R−1Yn.
The corresponding error covariance is
P−1
n|n = P−1
n|n−1 + M−1 −R−1
Xn = P−1
n|n−1 + CT R−1C.
These equations form the time and measurement update steps of the Kalman ﬁlter. Thus the Kalman
ﬁlter can be seen to be a combination of estimators. This also forms an alternative proof of the optimality
of the Kalman ﬁlter in the minimum mean squared sense under the stated assumptions. We will give
more detail on Kalman ﬁltering, and in particular on distributed Kalman ﬁltering below.
4.25.2.2 Dynamic sensor fusion
Suppose there are multiple sensors present that generate measurements about a random variable that
is evolving in time. We can again ask the question about how to fuse data from all the sensors for an
estimate of the state Xn at every time step n. This is the question of dynamic sensor fusion. We will
begin by seeing why this question is difﬁcult.
To begin with, the problem can be solved if all the sensors transmit their measurements at every
time step. The central node in that case implements a Kalman ﬁlter (which we will refer to from now
as the centralized Kalman ﬁlter). However, there are two reasons why this may not be the preferred
implementation.
1. The central node needs to handle matrix operations that increase in size as the number of sensors
increases. We may want the sensors to shoulder some of the computational burden.
2. The sensors may not be able to transmit at every time step. Hence we may want to transmit after
some local processing, rather than transmit raw measurements.
We will initially assume that the sensors can transmit at every time step and concentrate on reducing
the computational burden at the central node.
4.25.2.2.1
Transmitting local estimates
Our ﬁrst guess would be to generate a local estimate at each sensor that extracts all the relevant infor-
mation out of the local measurements and then to combine the estimates using methods outlined above.
However, in general, it is not possible to use above method. Consider K sensors being present with the
kth sensor generating a measurement of the form
yn,k = Ckxn + vn,k.
Suppose we denote by Yk the set of all the measurements from the sensor k that can be used to estimate
the state xn, i.e., the set {y0,k, y1,k, . . . , yn,k}. We wish to see if the local estimates formed by the sets
Yks can be combined to yield the optimal global estimate of xn. We can think of two ways of doing this:

680
CHAPTER 25 Distributed Estimation
1. We see that the set Yi is linearly related to x(k) through an equation of the form
⎡
⎢⎢⎢⎣
yn,k
yn−1,k
...
y0,k
⎤
⎥⎥⎥⎦=
⎡
⎢⎣
Ck
Ck A−1
...
⎤
⎥⎦xn +
⎡
⎢⎣
vn,k
vn−1,k −C A−1wn−1
...
⎤
⎥⎦.
However we notice that the process noise w appears in the noise vector. Thus even though the
measurement noises vn,ks may be independent, the noise entering the sets Yk become correlated
and hence the estimates cannot be directly combined. Of course, if the process noise is absent, the
estimates can be combined in this fashion (see, e.g., [1] where the optimality in this special case was
established.Forageneraldiscussionabouttheeffectsintroducedbytheprocessnoisesee,e.g.,[2–6]).
2. We see that xn can be estimated once the variables x0, w0, . . . , wn−1 are estimated. Now Yk is
linearly related to these variables through
⎡
⎢⎢⎢⎣
yn,k
yn−1,k
...
y0,k
⎤
⎥⎥⎥⎦=
⎡
⎢⎣
Ci Ak
Ci Ak−1 · · · C
Ci Ak−1
· · ·
C 0
...
⎤
⎥⎦
⎡
⎢⎢⎢⎣
wn−1
wn−2
...
x0
⎤
⎥⎥⎥⎦+
⎡
⎢⎢⎢⎣
vn,k
vn−1,k
...
v0,k
⎤
⎥⎥⎥⎦.
Now the measurement noises for different sensors are uncorrelated and the estimates can be com-
bined. However, the vector being transmitted from either of the sensors is increasing in dimension
as the time step n increases. Moreover the computation required is increasing since a matrix of size
growing with time needs to be inverted at every time step. Hence this is not a practical solution.
Thus we see that it is not straight-forward to combine local estimates to obtain the global estimate.
We can ask the question if it is possible at all to obtain the global estimate from the local estimates.
Thus imagine that the local estimates ˆxn,k were being combined in the optimal fashion. Is it possible to
generate the global estimate ˆxn? As noted above, for the special case when there is no process noise,
this is indeed true. However, in general, it is not possible.
Proposition 3 (from [7]).
Suppose two sets of measurements Y1 and Y2 are used to obtain local
estimates ˆx1 and ˆx2. Let
 ˆx1
ˆx2

= L
 Y1
Y2

≜LY.
Then the global estimate ˆx can be obtained from the local estimates ˆx1 and ˆx2 if and only if
RYY LT 
L RYY LT −1
L RY X = RY X.
Proof.
The global estimate generated from the measurements is given by
ˆx = RXY R−1
YY Y.

4.25.2 Network with a Star Topology
681
If it is generated from the local estimates, it is given by
ˆx = RXY LT 
L RYY LT −1
LY.
The result is thus obvious.
□
If L is invertible, the condition is satisﬁed and hence the global estimate can be generated from
the local estimates. In general, however, L would be a fat matrix and hence the condition will not be
satisﬁed. We thus have two options:
1. Find the best possible global estimator from the space spanned by the local estimates. This is left as
an exercise.
2. Find the extra data that should be transmitted that will lead to the calculation of the global estimate.
We will now describe some such schemes. For these and more such strategies see, e.g., [1,6–19].
4.25.2.2.2
Distributed Kalman ﬁltering
For this section we will assume that the sensors are able to transmit information to the central node at
every time step. We will use the following information form of the Kalman ﬁlter update equations.
Proposition 4.
Consider a random variable evolving in time as
xn+1 = Axn + wn.
Suppose it is observed through measurements of the form
yn = Cxn + vn.
Then the measurement updates of the Kalman ﬁlter can be given by this alternate information form
P−1
n|n ˆxn|n = P−1
n|n−1 ˆxn|n−1 + CT R−1yn,
P−1
n|n = P−1
n|n−1 + CT R−1C.
Proof.
The equations were derived in Section 4.25.2.1.3.
□
The basic result about the requirements from the individual sensors can be derived using the above
result.
Proposition 5.
The global error covariance matrix and the estimate are given in terms of the local
covariances and estimates by
P−1
n|n = P−1
n|n−1 +
K

k=1

P−1
n,k|n−1 −P−1
n,k|n

,
P−1
n|n ˆxn|n = P−1
n|n−1 ˆxn|n−1 +
K

k=1

P−1
n,k|n ˆxn,k|n −P−1
n,k|n−1 ˆxn,k|n−1

.

682
CHAPTER 25 Distributed Estimation
Proof.
Proof follows by noting that the global estimate is given by
P−1
n|n ˆxn|n = P−1
n|n−1 ˆxn|n−1 + CT R−1yn,
P−1
n|n = P−1
n|n−1 + CT R−1C.
Since R is block diagonal, the terms CT R−1yn and CT R−1C are decomposed into the sums
CT R−1yn =
K

k=1
CT
k R−1
k yn,k,
CT R−1C =
K

k=1
CT
k R−1
k Ck.
Noting the for the kth sensor, the estimate and the error covariance are given by
P−1
n,k|n ˆxn,k|n = P−1
n,k|n−1 ˆxn,k|n−1 + CT
k R−1
k yn,k,
P−1
n|n = P−1
n|n−1 + CT
k R−1
k Ck,
the result follows immediately.
□
Based on this result we now give two architectures for dynamic sensor fusion.
1. In the ﬁrst, rather obvious, architecture, the individual sensors transmit the local estimates ˆxn,k|n.
The global fusion center combines the estimates using the theorem given above. Note that the terms
ˆxn|n−1 and ˆxn,k|n−1 can be calculated by the fusion node by using the time update equation
ˆxn|n−1 = A ˆxn−1|n−1.
Similarly all the covariances can also be calculated without any data from the sensor nodes. This
method is simple, especially at the sensor level. However, the fusion node has to do a lot of
computation.
2. This method makes the computation at the fusion node simple at the expense of more data transmitted
from the sensor node. The essential point is the observation as developed, e.g., in [20,21] that the
term P−1
n|n−1 ˆxn|n−1 can be written in terms of contributions from individual sensors, i.e.,
P−1
n|n−1 ˆxn|n−1 =
K

k=1
zn,k.
(25.1)
This can be proved using straight-forward algebraic manipulation as follows:
P−1
n|n−1 ˆxn|n−1 = P−1
n|n−1A ˆxn−1|n−1
= P−1
n|n−1APn−1|n−1P−1
n−1|n−1 ˆxn−1|n−1

4.25.3 Non-Ideal Networks with Star Topology
683
= P−1
n|n−1APn−1|n−1

P−1
n−1|n−2 ˆxn−1|n−2
+
K

k=1

P−1
n−1,k|n−1 ˆxn−1,k|n−1 −P−1
n−1,k|n−2 ˆxn−1,k|n−2

.
Thus zi(k) evolves according to the relation
zn,k = P−1
n|n−1APn−1|n−1zn,k|n−1
+

P−1
n−1,k|n−1 ˆxn−1,k|n−1 −P−1
n−1,k|n−2 ˆxn−1,k|n−2

,
(25.2)
which depends only on the kth sensor’s data. The covariances do not depend on the data and can be
calculated anywhere. Hence each sensor transmits the quantity
γn,k =

P−1
n,k|n ˆxn,k|n −P−1
n,k|n−1 ˆxn,k|n−1

+ zn,k,
(25.3)
and the fusion node just calculates the sum of these quantities. Thus at expense of more data
transmitted from the sensor nodes, we have made the central node very simple.
4.25.3 Non-ideal networks with star topology
In this section, we will give some strategies or algorithms for sensors to perform distributed estimation
if the communication network suffers from limited bandwidth, transmit range, and message loss. We
consider various cases in the sequel.
4.25.3.1 Sensor fusion in presence of message loss
This research direction considers the following problem. Consider multiple sensors as above with a
central fusion center. The sensors transmit data to the fusion center across an analog erasure link that
drops messages stochastically. More formally, an analog erasure link accepts as input a real vector
i(n) ∈Rt for a bounded dimension t. At every time n, the output o(n) is given by
o(n) =
i(n) with probability 1 −p
∅
otherwise.
•
The case when o(n) = ∅is referred to as an erasure event. It implies that the channel drops the mes-
sages and the receiver does not receive any information apart from that an erasure event has occurred.
•
This model assumes that the erasure events occur according to a Bernoulli process with erasure
probability 1 −p. Other models, in which such events occur according to a Markov chain or other
more general processes, can be considered.
•
If the transmitter also knows that an erasure event has occurred, then we say that the receiver trans-
mits an acknowledgment to the transmitter. Such an acknowledgment may always be available, may
itself be transmitted across an erasure channel so that it is stochastically available, or may not be
available at all.

684
CHAPTER 25 Distributed Estimation
The basic effect of the sensors transmitting across such channels is that information from the sensors
is not available at the fusion center at every time step. This fact also requires some care in how the
performance of the estimator is deﬁned. Consider a realization of the erasure process such that at time
n, the last transmission from sensor k was received at the fusion center at time nk. Obviously, there is
no algorithm that can provide a better estimate than the MMSE estimate of x(n) given measurements
{y0,1, . . . , yn1,1}, {y0,2, . . . , yn2,2}, . . . , {y0,K , . . . , ynK ,K } (where we assume K sensors are present).
Denote the error covariance of this estimator by Popt
n . Due to the stochastic erasure process, it may be
more convenient to consider the expected value of this covariance E[Popt
n ] where the expectation is
taken with respect to the erasure processes. Several questions arise:
1. What information should the sensors transmit to enable the fusion center to achieve the covariance
Popt
n
at every time step?
2. If this covariance is not achievable, what is the best covariance that any algorithm can achieve?
3. Clearly, the error covariance at the fusion center degrades as the erasure probabilities increase.
What are the conditions on the erasure probabilities so that any algorithm can achieve stability of the
estimateerrorcovariance,i.e.,ensurethattheexpectederrorcovarianceremainsboundedasn →∞?
We discuss below some recent work on these questions, although a complete solution is unavailable at
this time.
It should be clear that an algorithm may lead to stability of the error covariance without being optimal
in the sense of achieving the covariance Popt
n . In other words, the requirement in the third question posed
above is less strenuous than the requirement in the ﬁrst question. The third question was answered in [21]
which presented conditions on erasure probabilities and the process matrices for stability. We present
the result below for the case when two sensors transmit data to the fusion center across individual analog
erasure links with Bernoulli erasures with erasure probabilities 1−pk, k = 1, 2. Various generalizations
are available in the cited reference.
Theorem 1 (from [21]).
Consider a process evolving as
xn+1 = Axn + wn
being observed using two sensors that generate measurements of the form
yn,k = Ckxn + vn,k,
i = 1, 2,
where wn and vn,k are white zero mean independent noises. Let the sensors transmit information through
a real vector with bounded dimension to a fusion center across analog erasure channels with Bernoulli
erasures with erasure probabilities p1 and p2 respectively. Denote by ρ(Ak) the spectral radius of the
unobservable part of matrix A when the pair (A, Ck) is written in the observer canonical form and by
ρ(A) the spectral radius of matrix A. Assume that the pair (A, [CT
1 , CT
2 ]T ) is observable.
1. Irrespective of the information transmitted by the sensors, and the algorithm used by the fusion
center, the quantity E[Popt
n
] is not bounded as n →∞if at least one of the following inequalities
is not satisﬁed:
p1ρ(A2)2 ≤1,
(25.4)

4.25.3 Non-Ideal Networks with Star Topology
685
p2ρ(A1)2 ≤1,
(25.5)
p1 p2ρ(A)2 ≤1.
(25.6)
2. Conversely, if the inequalities 25.4–25.6 are satisﬁed, then there is an algorithm such that the
corresponding expected error covariance at the fusion center is bounded as time increases.
Thus, this result solves the third problem posed above. It is important to note that the necessity of
the inequalities 25.4–25.6 holds irrespective of the availability of acknowledgments at the sensors. The
necessity part of the result follows from system theoretic considerations. The sufﬁciency part of the
result is proved by constructing an algorithm that guarantees stability of the estimator error covariance,
even though the error covariance is not Popt
n
(i.e., the algorithm is not optimal in the sense of achieving
the minimal error covariance at every step). Perhaps somewhat surprisingly, the algorithm is based on
the sensors transmitting local estimates of the process state based on their own measurements. Specif-
ically, sensor 1 transmits the estimate ˆx1
n,1 of the modes of the process observable only from sensor 1,
and ˆx2
n,1 of the modes observable from both sensors. Similarly sensor 2 transmits the estimate ˆx1
n,2 of
the modes of the process observable only from sensor 2, and ˆx2
n,2 of the modes observable from both
sensors. The fusion center maintains an estimate ˆx1
n of the modes observable only from sensor 1, ˆx2
n of
the modes observable only from sensor 2, and ˆx3
n of the modes observable from both sensors. At any
time step, it updates the modes as follows:
ˆx1
n =
 ˆx1
n,1
transmission successful from sensor 1
A ˆx1
n−1
otherwise
,
ˆx2
n =
 ˆx1
n,2
transmission successful from sensor 2
A ˆx2
n−1
otherwise
,
ˆx2
n =
⎧
⎨
⎩
ˆx2
n,1
transmission successful from sensor 1
ˆx2
n,2
transmission successful from sensor 2 but not from sensor 1
A ˆx1
n−1
otherwise
.
The estimate of the state xn can be formed from the three components ˆxk
n. Given this algorithm, the
sufﬁciency of the inequalities 25.4–25.6 for stability of the expected error covariance can then be proved.
Given Proposition 3, it is not surprising that this algorithm cannot lead to the calculation of the
optimal global estimate at the fusion center. In fact, the optimal information processing algorithm at
the sensors remains unknown in most cases. Most recent advances (e.g., [20–23]) build from the basic
algorithm identiﬁed in Eqs. (25.1)–(25.3). Thus the sensors transmit the quantity γn,k at every time
step and the fusion center sums these quantities to generate the estimate ˆxn. If there are no erasures,
this estimate is indeed the global estimate with the optimal error covariance Popt
n . However, if there
are erasures, then the calculation of γn,k requires some global knowledge. In particular, the quantity
Pn−1|n−1 in (25.2) at each sensor requires the knowledge of the last time step at which the transmission
from every sensor to the fusion center was successful. Notice that the data transmitted by other sensors
is not required, merely the conﬁrmation of successful transmission is enough.
One mechanism for such global knowledge can be acknowledgments transmitted from the fusion
center. If such acknowledgments are available, then it was shown in [21] that minor modiﬁcations of the

686
CHAPTER 25 Distributed Estimation
algorithm outlined in Eqs. (25.1)–(25.3) will generate the optimal global estimate at the fusion center.
Dependingontheproblemscenario,suchanassumptionmayormaynotberealistic.Ifacknowledgments
are also transmitted across an analog erasure link [23], presented some further modiﬁcations to the
algorithm that guaranteed that the estimation error covariance degraded continuously as a function of
the probability of loss of acknowledgment. However, the optimal algorithm when acknowledgments
are not available, or only available intermittently, is not known.
Other special cases where such global knowledge is available can be if only one of the sensors
transmits across an analog erasure link [20] or if only one sensor transmits at any time [22]. Once
again, in these cases, the optimal global estimate can be calculated. However, it remains unknown if the
optimal global estimate can be calculated outside of these cases, or if it cannot be calculated, then what
is the best performance that is achievable.
4.25.3.2 Sensor fusion with limited bandwidth
4.25.3.2.1
Static sensor fusion
Consider a limited bandwidth communication network, in which K sensors measure an unknown para-
meter θ ∈[−U,U]. The measurement xk, from kth sensor, is corrupted by noise nk, which is assumed
independent, zero mean, and with a pdf p(u), namely Pr(nk = u) = p(u).
xk = θ + nk
for k = 1, 2, . . . , K.
(25.7)
Depending on the distribution of the noise, and on the amount of information that it is transmitted, there
can be the cases studied in the following subsections:
An ϵ-estimator with known noise pdf: Here an ϵ-estimator is deﬁned as an estimator providing
estimates with MSE lower than ϵ2. Assume the limited bandwidth forces each sensor to send just one
bit messages mk(xk) to the fusion center. The message is deﬁned as
mk(xk) =
 1, if xk ∈Sk,
0, if xk /∈Sk,
(25.8)
where Sk is a subset of R and is independent of the noise pdf. Let R+ denote the subset of R for all
positive real number.
Example 1 (from [24]).
Suppose that the noise is uniformly distributed over the interval [−U, U].
Let Sk = R+ for all k. Suppose a linear fusion function that gives the estimator ˆθ as
ˆθ := f (m1, . . . , mk) = −U + 2U
K
K

k=1
mk.
Then, the estimator is unbiased:
E[ ˆθ] = −U + 2U
K
K

k=1
E[mk]
= −U + 2U
K K U + θ
2U
= θ.

4.25.3 Non-Ideal Networks with Star Topology
687
Furthermore, since mks are independent,
E[ ˆθ −θ]2 = 4U 2
K 2 E
 K

k=1

mk −U + θ
2U
2
= 4U 2
K 2
K

k=1
E

mk −E[mk]
2 ≤U 2
K ,
where we used that the variance of a binary random variable is bounded above by 1/4. It indicates that,
even with the binary message constraint, a total number of K = U 2/ϵ2 sensors are still sufﬁcient to
perform an ϵ-estimator for θ.
Generally, if the p(u) is given, we can still choose the message function as Eq. (25.8) with Sk = R+
for all k. Then
Pr(mk = 1) = Pr(nk > −θ) =
 ∞
−θ
p(u)du,
Pr(mk = 0) = Pr(nk ≤−θ) =
 −θ
−∞
p(u)du.
Then the expectation value for E[mk] is obtained by
E[mk] =
 ∞
−θ
p(u)du = 1 −F( −θ),
k = 1, 2, . . . , K,
where F(·) is the cumulative distribution function (cdf) of the noise. If one chooses the ﬁnal fusion
function for ˆθ as introduced in [24], then
ˆθ := f (m1, . . . , mk) = −F−1

1 −1
K
K

k=1
mk

,
(25.9)
where F−1 is the inverse of F. By the strong law of large numbers, it follows:
lim
K→∞
ˆθ = −F−1

1 −lim
K→∞
1
K
K

k=1
mk

= −F−1(1 −E[mk]) = −F−1(F( −θ)) = θ.
Suppose the noise pdf p(u) is known and bounded over [−U, U], then ˆθ obtained by Eq. (25.9) is an
ϵ-estimate of θ implying a total number of O(1/ϵ2) sensors, by the following theorem:
Theorem 2 (from [24]).
Suppose the noise pdf p(u) is known and bounded from below by μ > 0 over
[−U,U]. Let K ≥1/(4μ2ϵ2). Then the decentralized estimation scheme (25.8) and (25.9) produces
an ϵ-estimator of θ.

688
CHAPTER 25 Distributed Estimation
Proof.
Notice that
|F( −θ) −F( −θ′)| = |1 −F( −θ) −(1 −F( −θ′))|
=

 −θ′
−θ
p(u)du
 ≥μ|θ −θ′| ∀θ, θ′ ∈[−U, U]
⇒|F−1(v) −F−1(v′)| ≤1
μ|v −v′|
∀v, v′ ∈[0, 1],
Then
| ˆθ −θ| =
−F−1

1 −1
K
K

k=1
mk

+ F−1(1 −E(mk))

≤1
μ

1
K
K

k=1
mk −E(mk)

⇒E[ ˆθ −θ]2 ≤1
μ2 E

1
K
K

k=1
mk −E(mk)
2
≤
1
4μ2K .
Thus, the variance of the estimator given by Eq. (25.9) is lower than ϵ2 as long as
K ≥
1
4μ2ϵ2 ,
which concludes the proof.
□
A universal ϵ-estimator for unknown noise pdf: The ϵ-estimator introduced in Section 4.25.3.2.1
needs the explicit pdf p(u) for the noise. However, sometimes for a large number of sensors, to charac-
terize the measurement noise distribution would cost too much, or could be even impossible in a dynamic
environment. To cope with these situations, a distributed estimator providing accurate estimates regard-
less the noise pdf under the bandwidth constraint is required. In this subsection, we summarize a
universal distributed estimator for unknown noise pdf.
The idea, proposed in [24], is to represent the estimates in binary form by quantizing the sensor
measurements into the corresponding bit positions. Speciﬁcally, it tries to quantize 2−i of the sensors’
measurements into the ith most signiﬁcant bit (MSB), e.g., 1/4 of the sensors quantize their measurement
to the second MSB. Then it can be shown that the statistics average of these message functions (m1 +
m2 + · · · + mK )/K is a unbiased estimator for θ, while its MSE is upper bounded by 4U 2/K.
The procedure of this distributed estimation scheme is described as follows [25,26]:
1. Each measurement, xk, in node k is quantized into the ith MSB with probability 2−i, being converged
to a binary message. Then this message is sent to the fusion center. This step can be described as
following:
Pr(a = i) =

2−i
i = 1, 2, 3, . . . ,
0
otherwise,
(25.10a)
mk(x, a) = [b(2U + x; a); a],
(25.10b)

4.25.3 Non-Ideal Networks with Star Topology
689
where the value of the random variable a indicates the position for MSB, and the notation b(z; a)
denotes the ith MSB of a real number z.
2. The fusion center recursively computes the average of all received binary messages that are distinct
(determined by, say, the sender’s ID), and uses it as estimator of θ.
Suppose the fusion center, which also has measurement capability, has received a total of j indepen-
dent messages. Based on these messages, it can ﬁrst form the sets
Ni = {k|ak = i, 1 ≤k ≤j},
i = 1, 2, 3, . . . .
(25.11)
Then, based on the received messages and its own observation x, the center can be proceed to form
yi = b(2U + x; i) +

k∈Ni
b(2U + xk; ak),
i = 1, 2, 3, . . . ,
(25.12)
and perform the estimate of θ
ˆθ j = f j(x, m(x1, a1), . . . , m(x j, a j)) = −2U + 4U
∞

i=1
2−i
|Ni| + 1 yi.
(25.13)
Theorems 3 and 4 show that this distributed estimator is unbiased and has an expected MSE of 4U 2/K,
where K is the number of sensors in the network:
Theorem 3.
Let
f j(x, m(x1, a1), . . . , m(x j, a j)) be deﬁned by Eq. (25.13). Then for all
0 ≤j ≤K −1
E[ f j(x, m(x1, a1), . . . , m(x j, a j))] = θ,
∀θ ∈[−U, U],
∀p ∈MU,
(25.14)
where the expectation is taken with respects to the distribution of a and unknown noise, and where
MU =

p(u) :
 U
−U
p(u)du = 1,
 U
−U
up(u)du = 0, p(u) ≥0, Supp(p) ⊆[−U, U]

.
Proof.
From Eqs. (25.12) and (25.13), using that xk is i.i.d to each others, we obtain
E[ f j(x, m(x1, a1), . . . , m(x j, a j))]
= −2U + 4U
∞

i=1
E
⎡
⎣
2−i
|Ni| + 1
⎛
⎝b(2U + x; i) +

k∈Ni
b(2U + xk; ak)
⎞
⎠
⎤
⎦
= −2U + 4U
∞

i=1
2−iE[b(θ + 2U + n; i)]
= −2U + E[θ + 2U + n] = θ,
where note that every number u in [0, 4U] can be represented in binary as
u = 4U
∞

i=1
2−ib(u; i),
which concludes the proof.
□

690
CHAPTER 25 Distributed Estimation
Theorem 4.
Let ˆθ be the distributed estimator of Eq. (25.13). Then
E[ ˆθ j −θ]2 ≤4U 2
j + 1.
Proof.
Similarly, from Eqs. (25.12) and (25.13), using that xk is i.i.d to each others,
E[( ˆθ j −θ)2|a1, . . . , ai] = 16U 2
∞

i=1
Var
⎡
⎣
2−i
|Ni| + 1
⎛
⎝b(2U + x; i) +

k∈Ni
b(2U + xk; ak)
⎞
⎠
⎤
⎦
= 16U 2
∞

i=1
2−2i Var[b(2U + x; i)]
|Ni| + 1
≤4U 2
∞

i=1
2−2i
1
|Ni| + 1,
where in the last step follows from that the upper bound of Var(b(2U + x; a)) is 1/4. Furthermore,
notice that
Pr(Ni = r) =
 i
r

2−ir(1 −2−i)( j−r),
0 ≤r ≤j,
and
E

1
|Ni + 1|

=
j

r=0
1
r + 1
 i
r

2−ir(1 −2−i)( j−r)
=
1
i + 1
1 −(1 −2−i) j+1
2−i
.
Therefore, the MSE is
E[ ˆθ j −θ]2 = E[E[( ˆθ j −θ)2|a1, . . . , ai]]
≤4U 2
∞

i=1
2−2iE

1
|Ni| + 1

≤4U 2
j + 1,
which concludes the proof.
□
Remark 1.
The average message length is  ∞
i=1 2−i(1 + ⌈log (a)⌉) and is upper bounded by
2.5078 [26].
4.25.3.2.2
Dynamic sensor fusion: sign of innovations-KF
Analternativesolutioninthepresenceof limitedbandwidthis basedontheKalmanﬁlter. First, recall that
generally distributed Kalman ﬁlter includes a prediction step and a correction step. Consider the system
xn = Anxn−1 + wn,

4.25.3 Non-Ideal Networks with Star Topology
691
yn,k = CT
n,kxn + vn,k,
where the driving input wn is normally distributed with zero mean and variance Qn and the observation
noise vn,k is zero mean AWGN and independent across sensors with noise R [25]. In this case, we have
R = σvI. Suppose that ˆxn−1|n−1 and Pn−1|n−1 are available at time n, the predicted estimate ˆxn|n−1 and
its corresponding covariance matrix Pn|n−1 are given by
ˆxn|n−1 = An ˆxn−1|n−1,
(25.15a)
Pn|n−1 = An Pn−1|n−1AT
n + Qn.
(25.15b)
The innovation sequence
˜yn := yn −CT
n ˆxn|n−1
is chosen to obtain the corrected estimate ˆxn|n. To deal with the limited bandwidth, the sign of the
innovation (SOI) is used to ensure that the required exchange of information among sensors is possible
under one bit message constraint
m(n) := sign[ ˜yn] = sign[yn −˜yn|n−1].
(25.16)
Due to the sign non-linearity, p[xn|m0:n−1] is non-Gaussian and computation of the exact MMSE
estimate requires numerical integrations and propagation of the posterior pdf. However, base on cus-
tomary simpliﬁcations made in nonlinear ﬁltering, we can approximate the MMSE with following
correction recursions [27]:
ˆxn|n = ˆxn|n−1 + mn
(√2/π)Pn|n−1Cn
!
CTn Pn|n−1Cn + σ 2v
,
(25.17a)
Pn|n = Pn|n−1 −(2/π)Pn|n−1CT
n Pn|n−1
CTn Pn|n−1Cn + σ 2v
.
(25.17b)
Even at a minimal communication cost, the SOI-KF is strikingly similar to the clairvoyant KF [25].
To prove it, let us rewrite the SOI-KF correction as
ˆxn|n = ˆxn|n−1 +
Pn|n−1Cn
CTn Pn|n−1Cn + σ 2v
˜mn|n−1,
(25.18)
where
˜mn|n−1 :=
!
(2/π)E[ ˜y2
n|n−1]mn.
Notice that the units of ˜mn|n−1 and ˜yn|n−1 are the same, and
E[ ˜mn|n−1] = E[ ˜yn|n−1] = 0,
E[ ˜mn|n−1]2 = 2
π E[ ˜yn|n−1]2,
which indicates that Eq. (25.18) is identical to the KF update if replacing ˜mn|n−1 with the innovation
˜yn. It is not difﬁcult to show that the MSE increases when using the SOI-KF is as much as the KF would
incur when applied to a model with π/2 higher observation noise variance [25,27].

692
CHAPTER 25 Distributed Estimation
4.25.4 Network with arbitrary topology
The results above assumed the presence of a star topology in which one central node had access to local
estimates from every other node. It was essentially a two step procedure: ﬁrst all the nodes transmit local
estimates or local measurements to the central node and then the central node calculates and transmits
the weighted sum of the local estimates back. However, what is required is a weighted average. Thus,
we can generalize the approach to an arbitrary graph at the expense of more time being employed. The
generalization is along the lines of average consensus algorithms that have been recently considered by
many people (see, e.g., [28–30]). An example of arbitrary topology networks is illustrated in Figure 25.2.
For now, we will only cover the basics.
4.25.4.1 Static sensor fusion with limited communication range
Due to the limited communication range, some of the sensors can not send message to the fusion center.
In such cases, we can treat the networks as the static sensor fusion for arbitrary graphs without a fusion
center.
Consider K nodes each with access to a scalar value being connected according to an arbitrary (but
time-invariant) graph. Suppose we want each node to calculate the average of all the numbers. One way
to do that is if each node implements the dynamical system
xn+1,k = xn,k + h

j∈Ni
"
xn, j −xn,k
#
,
FIGURE 25.2
An example of arbitrary topology networks with nodes and links (solid lines indicating that there is message
communication between nodes). In this network, there is no node acting as fusion center.

4.25.4 Network with Arbitrary Topology
693
where xn,k is the value for state xk at time n, and h is a small positive constant. On stacking the states
of all the nodes, the entire system evolves as
Xn+1 = (I −hL)Xn,
where Xn = [xn,1, . . . , xn,K ]T , and L is the Graph Laplacian matrix. If the underlying graph is con-
nected, L has the following properties:
1. It is a symmetric positive-deﬁnite matrix. Thus the dynamics is stable (assuming h is small enough)
and reaches a steady-state.
2. Each row sum is 0. Thus any vector with identical components is an equilibrium.
3. Each column sum is 0. Thus the sum of entries Xn is conserved at every time step.
Because of these three properties, it is easy to see that each entry must converge to the average of the sum
of the initial conditions. This algorithm can then be readily extended for calculating weighted averages
of vectors [31,32]. If the initial values are given by the vectors x0,k, each node calculates the following:
xn+1,k = xn,k + hW −1
k

j∈Ni
"
xn, j −xn,k
#
,
where Ni denotes the set of sensors connected to ith sensor. In our case, we let x0,k to be the local
estimate values and Wk to be inverse of the local estimation error covariance, and obtain the required
weighted sum.
4.25.4.2 Dynamic sensor fusion
4.25.4.2.1
With limited communication range
Consider a WSN with K > 1 sensor nodes placed at random and static positions in space. At every
time instant, each sensor in the network takes a noisy measurement yk(n) of a scalar signal x(n) for
n ∈N0 and for all k = 1, . . . , K as described by Eq. (25.19):
xn = axn−1 + δn−1,
(25.19a)
yn,k = ckxn + vn,k,
(25.19b)
where vn,k for all k is normal distribution with zero means and variance σ 2
vn,k respectively and
E(vn,ivn, j) = 0 for all n ∈N0, i ̸= j. Moreover, δn models the disturbance for the scalar signal.
In this case, each sensor has limited communication range. To estimate the dynamic state xn in the
networked manner, we assume every sensor k computes an estimate ˆxn,k of xn by taking a linear com-
bination of its own and of its neighbors’ estimates and measurements. Deﬁne ˆxn = (ˆxn,1, . . . , ˆxn,K )T
and similarly yn = (yn,1, . . . , yn,K )T , then each node computes
ˆxn,k = aκT
n,k ˆxn−1 + hT
n,k
"
yn −aCˆxn−1
#
(25.20)
with ˆx0 = y0, C is a diagonal matrix diag({c1, . . . , cK }) with κT
n,k ∈RK×1, in which the jth element is
the weight coefﬁcient used by node k for information coming from node j at time n, as seen from node
k with respect to all nodes of the network.

694
CHAPTER 25 Distributed Estimation
Let denote en = (en,1, . . . , en,K )T , with en,k = xn −ˆxn,k, k = 1, . . . , K, be the vector of the
estimation errors. Assume that κT
n,k1 = 1, then for each node k, the error dynamics can be obtained by
en,k = a(κn,k −Chn,k)T en−1 + (κn,k −Chn,k)T δn−1 −hT
n,kvn
= agT
n,ken−1 + gT
n,kδn−1 −hT
n,kvn
with gn,k = κn,k −Chn,k and where vn = (vn,1, . . . , vn,K )T .
Deﬁne Gn the matrix with kth row given by the vector gn,k, for k = 1, . . . , K. The average estimation
error of the estimator (25.20) is bounded throughout the network provided that a condition on the
maximum singular value of the matrix Gn, γmax(Gn), holds:
Proposition 6.
Assume that
i. γ (Gn) ≤γmax < min (1, 1/a) for all n ∈N0, where γ (·) is the singular value of matrix.
ii. δn = dn + wn, where |dn| <  represents the disturbances and wn ∼N(0, σ 2
w) is Gaussian noise
for all n ∈N0.
Then the correlation function of the estimation error, computed with respect to the measurement noise
and message losses, is
lim
n→+∞∥Ew,ven∥≤
√
Kγmax
1 −γmax
.
(25.21)
Proof.
The dynamics of en are given by a stochastic time-varying linear system. Consider the function
Vn = ∥Ev,wen∥. Simple algebra gives that
V (t) ≤∥aGn∥Vn + ∥Gn∥
√
K
≤(aγmax)nV0 + γmax
1 −γ n−1
max
1 −γmax

√
K,
from where, taking the limit n →+∞, the proposition follows.
□
The previous proposition is useful because gives us a constraint on the weights so that the estimation
error is stable. Moreover, we compute the weights so that the estimation error variance is minimized
under the stability constraint. Every node computes the weights by solving at each time step the following
optimization problem:
min
gn,k,hn,k,ψn,k
gT
n,kn−1,kgn,k + hT
n,kQn−1hn,k,
(25.22a)
s.t.
"
gn,k + Chn,k
#T 1 = 1,
(25.22b)
∥gn,k∥2 ≤ψn,k,
(25.22c)
where
n,k = (a2Pn,k + σ 2
wI),
Pn,k = E(en,k −Een,k)(en,k −Een,k)T ,
while Qn = n with  which is a diagonal matrix, diag
"$
σ 2
v1, . . . , σ 2
vK
%#
. If the σ 2
w is unknown, let
n,k = a2Pn,k.

4.25.4 Network with Arbitrary Topology
695
In this optimization problem, the objective function is the average variance of the estimation error
at node k. The ﬁrst constraint is motivated by assumption κT
n,k1 = 1 , whereas the last constraint
is a consequence of Proposition 6 and the Proposition III.1 in [33]. Speciﬁcally, the last constraint
in problem (25.22) guarantees that γ (Gn) ≤γmax, provided that there exists some positive scalars
ψn,k, k = 1, . . . , K, such that
Sk(ψn,k) = ψn,k +
&
ψn,k

j∈k
&
ψn, j −γmax ≤0,
(25.23)
where k = { j ̸= k : Nk ∩N j ̸= ∅} ∪{Nk}, which is the collection of communicating nodes located
at two hops distance from node k plus communicating neighbors of k at time n.
The optimal solution to problem (25.22) is obtained in two steps: ﬁrst, ψn,k is assumed ﬁxed and
the problem is solved by applying Lagrange dual theory for the variables gn,k and hn,k, thus achieving
expressions of gn,k and hn,k as function of ψn,k. Finally, these expressions are used in the cost function,
which is then minimized in the valuable ψn,k. Details follows in the sequel:
By the ﬁrst step, given a covariance matrix Pn−1,k, the weights that solve the optimization problem
(25.22) are
gn,k =
"
n−1,k + λn,kI
#−1 1
1T
"
n−1,k + λn,kI
#−1 + CQ−1C

1
,
(25.24)
hn,k =
Q−1C1
1T
"
n−1,k + λn,kI
#−1 + CQ−1C

1
,
(25.25)
λn,k =
'
0
if
(
gT
n,kgn,k
)
λn,k=0 ≤ψn,k
λ∗
n,k
otherwise
.
(25.26)
Here λ∗
n,k is determined by equation
(
gT
n,kgn,k
)
λ∗
n,k
= ψn,k.
(25.27)
The value of λ∗
n,k is in the interval

0, max

0,
1
1T CQ−1C1
*
1
ψn,k
−a2ℓ(Pn−1,k)

,
where ℓ(Pn,k) is the minimum eigenvalue of matrix Pn,k. Then λ∗
n,k can be computed by a simple
bisection algorithm [33].
Theweightsgn,k andhn,k,whoseexpressionaregivenin(25.24)and(25.25),dependonthethresholds
ψn,k, through the values λn,k, and on the error covariance matrix Pn−1,k. In case of perfect communica-
tion, each node could estimate efﬁciently the error covariance matrix from data. In particular, let ˆPn−1,k

696
CHAPTER 25 Distributed Estimation
the estimation of the covariance matrix computed by node k, then
ˆPn−1,k = 1
n
n−1

τ=0
"
ˆϵτ,k −ˆmτ,n
# "
ˆϵτ,k −ˆmτ,k
#T ,
(25.28)
where
ˆmn,k = 1
n
n

τ=0
ˆϵτ,k
is the sample mean. The vector ˆϵn,k is the vector of the estimation errors of the neighboring nodes
available at node k, which is obtained by a Tichonov regularization approach, as discussed in [33] with
a different matrix A given by
A =
 1 I
C1 0

.
Now let us compute the values of ψn,k that solve the optimization problem (25.22). By substituting
(25.24) and (25.25) in the cost function of (25.22), we see that the larger is ψn,k, the lower is the cost
function. In other words, the larger is ψn,k, the lower is the estimation error variance at node k. Since
ψn,k must be maximized for k = 1, . . . , K, it follows that ψn,k, k = 1, . . . , K, is given by the solution
to the following multi-criterion optimization problem
max
ψn,k
ψn,k,
(25.29)
s.t. S(ψn) ⪯0,
(25.30)
ψn,k ≻0,
where S(ψn) = (S1(ψn,1), . . . , SK (ψn,K ))T .
Notice that the cost function is a vector whose components are coupled by the constraints (25.30).
Thus the problem is a multi-criterion optimization problem and each threshold ψn,k, k = 1, . . . , K,
must be optimized simultaneously. This problem is a Fast-Lipschitz optimization problem [34]. The
solution is given in [33].
Now let us investigate the performance of this estimator in MSE.
Proposition 7.
The optimal value of κn,k and hn,k are such that the error variance at node k satisﬁes
E[ek −Eek]2 <
1
1T

k +
 
j∈Nk
1
1T  j1 +
1
1T k1
!
1
ψn,k
−1
1
,
where i = CkQ−1
k Ck.
Proof.
First, by using (25.24) and (25.25) in the expression of the estimation error variance it follows
that
E[en,k −Een,k]2 ≤
1
1T "
n−1,k + λn,kI
#−1 1 + 1T k1
<
1
1T k1.

4.25.4 Network with Arbitrary Topology
697
Then notice that
trn,k =

j∈Nk
E
"
en−1, j −Een−1, j
#2 <

j∈Nk
1
1T  j1.
Thus we have
ℓmax(n−1,k + λn,kI) <

j∈Nk
1
1T  j1 + max

0,
1
1T k1
*
1
ψn,k
−a2ℓmin(Pn−1,k)

<

j∈Nk
1
1T  j1 +
1
1T k1
*
1
ψn,k
.
Since
1T (n−1,k + λn,kI)−11 ≥
1
1T (ℓmax(n−1,k + λn,kI))1,
we have that
E(en,k −Een,k)2 ≤
1
1T "
n−1,k + λn,kI
#−1 1 + 1T k1
<
1
1T

k +
 
j∈Nk
1
1T  j1 +
1
1T k1
!
1
ψn,k
−1
1
.
□
The previous proposition guarantees that the estimation error at each time n, in each node k, is always
upper-bounded by the Cramer-Rao lower bound.
4.25.4.2.2
With message losses
Suppose that over a link, messages may be dropped because of bad channel conditions or radio interfer-
ence. Let φn,kj, with k ̸= j, be a binary random variable associated to the message losses from sensor
k to j at time n. For k ̸= j, we assume that the random variables φn,kj are independent with probability
mass function:
Pr(φn,kj = 1) = pkj,
Pr(φn,kj = 0) = qkj = 1 −pkj,
where pkj ∈[0, 1] denotes the successful message reception probability.
Example 2.
Suppose each sensor computes an estimates ˆxi(n) by taking a linear combination of its
own and of its neighbors’ estimates and measurements. Then with the message losses, the estimator can
be written as
ˆxn+1,k = ˆxn,k + hW −1
k

j∈Nk
φn,kj
"
ˆxn, j −ˆxn,k
#
,
where Nk denotes the set of neighbors of sensor k plus the node itself.

698
CHAPTER 25 Distributed Estimation
Example 3 (from [35]).
Suppose that the prediction phase of the Kalman Filter is independent of the
observation process as Eq. (25.15). The measurement update is stochastic as the received measurements
are determined by φn,kj.
Then the covariance update may have following compact expression
P(K)
n
= An Pn−1AT
n + Qn −
K

k=1
k+
i=1
φn,ip
K−k
+
j=1
(1 −φn, jp)(k)
n (Pn−1),
(25.31)
where we use the simpliﬁed notation Pn = Pn|n−1, R(k) =
 k
i=1 σ −1
i
−1
, in which σn is the variance
for noise vn, and
(k)
n (Pn) = An PnCT
n

Cn PnCT
n + R(k)−1
Cn Pn AT
n .
Intuitively, the more sensors report, the better estimation performance becomes.
Let us extend the results in Section 4.25.4.2.1 with the message losses for the scenario described by
Eq. (25.19). Considering the message losses, each node computes estimates, instead of Eq. (25.20), by
ˆxn,k = aκϕT
n,k ˆxn−1 + hϕT
n,k
"
yn −aCˆxn−1
#
,
(25.32)
where
κϕn,k = κn,k ◦ϕn,k =
"
κn,1, κn,2, . . . , κn,K
#T ◦
"
ϕn,k1, ϕn,k2, . . . , ϕn,kK
#T ,
where ◦is the element-wise product between two matrices, with ϕn,k ∈RK×1 denotes the vector of the
message reception process realization of the process φn,k at time n, as seen from node k with respect to
all nodes of the network. Speciﬁcally, let the jth element of ϕn,k, with j ̸= k, be ϕn,kj. Notice that at a
given time instant, the jth component of ϕn,k is zero if no data messages are received from node j. Let
Nϕk = { j ∈Nk : ϕn,kj ̸= 0}, namely such a set collects the nodes communicating with node k at time
n. The number of nodes in the set is |Nϕk| = ϕT
n,kϕn,k. The vector hϕn,k ∈RK×1 is constructed from
the elements hn,k, similarly to κϕn,k.
Similarly, we extend the optimization problem in Eq. (25.22) to the following:
min
gn,k,hn,k,ψn,k
gT
n,k′
n−1,kgn,k + hT
n,kQ′
n−1hn,k,
(25.33a)
s.t.

(gn,k + Chn,k)T ◦ϕn,k

1 = 1,
(25.33b)
∥gn,k ◦ϕn,k∥2 ≤ψn,k,
(25.33c)
where ′
n,k = n,k ◦(ϕn,kϕT
n,k), while Q′
n = Qn ◦(ϕn,kϕT
n,k). The optimal weights are
gn,k =

(n−1,k + λn,kI) ◦ϕn,kϕT
n,k
†
ϕn,k
ϕT
n,k

(n−1,k + λn,kI) ◦ϕn,kϕT
n,k
†
+ CkQ−1
n−1Ck

ϕn,k
,
(25.34)
hi(t) =
Q−1
n−1Ckϕn,k
ϕT
n,k

(n−1,k + λn,kI) ◦ϕn,kϕT
n,k
†
+ CkQ−1
n−1Ck

ϕn,k
,
(25.35)

4.25.5 Computational Complexity and Communication Cost
699
λn,k =
'
0
if
(
ϕT
n,kgT
n,kgn,kϕn,k
)
λn,k=0 ≤ψn,k
λ∗
n,k
otherwise
.
(25.36)
We have following proposition for the MSE:
Proposition 8.
It holds
EφEv[en,k −Even,k]2 ≤
a2(
√
5 −1)√γmax + 2N
(a2 + 1)(
√
5 −1)√γmax + 2N
·
|Nk|−1

i=0
χ(i)
i + 1σ 2
max.
Observe that the estimation error variance given by the previous proposition depends on the message
loss probabilities qkj, on the maximum number of neighbors for each node |Nk|, the total number of
nodes in the networks K, and the largest singular value of the matrix κϕn,k. If the number of neighbors
is greater than 2, with a loss of qkj = 0.3 for all j, we have that the product of the two coefﬁcients does
not exceed 0.65 and it is only a 30% higher than the case when no packet losses are present.
4.25.5 Computational complexity and communication cost
The efﬁciency of implementation of estimation algorithms can be characterized in terms of computa-
tional complexity and communication cost. Conventionally, the computational complexity of an algo-
rithm is measured by the amount of basic operations such as ﬂoat-point arithmetic performed. The
computational complexity is commonly expressed by using the O notation, which describes the lim-
iting behavior of a function. The O notation suppresses the multiplicative constants and lower order
terms. For example, if the time running requirement for an algorithm is at most 5n3 + 100n2, then we
say that the computational complexity is O(n3). On the other hand, communication cost of an algorithm
refers to the communication resources required in terms of amount and size of the exchanged messages
in bytes or bits. We express the communication cost by using the O notation as well.
It is important to analyze the computational complexity and communication cost for distributed esti-
mation algorithms, especially when one designs algorithms for a sensor network. In these networks,
larger computational complexity requirement and communication cost always entail the high risk of
slower response speed, smaller transmit rate and thus poorer performance in practice, though the the-
oretical performance for the algorithm might be much better. In fact, due to the limited computational
capability of sensors, sometimes we have to redesign Aalg, or implement an approximate algorithm ˜Aalg
which has lesser computational complexity but may still provide acceptable performance.
4.25.5.1 On computational complexity
Before deploying an algorithm for sensors networks, it is desirable to check whether the sensor nodes
have enough computational capability to perform the computation as designed. Suppose that the sen-
sors are designed to produce their estimates using some algorithm Aalg. We analyze Aalg’s worst-case
computational cost requirements as a function of the size of its input (in terms of the O-notation). Here
we assume that arithmetic or basic operation with individual elements has complexity O(1). Thus, the
computational complexity of a matrix addition, multiplication, and inversion are O(m2), O(m3) and
O(m3) respectively, where m × m is the size of the matrix.

700
CHAPTER 25 Distributed Estimation
Example 4.
Consider the combining estimator of Section 4.25.2.1.1. According to Proposition 1, some
matrix inversions and multiplications to ﬁnd the MMSE estimate of X are needed. Denote the largest
size of the vector Y and X is M. Then the computational complexity of the estimators is O(M3).
Example 5.
Consider the static sensor fusion of Section 4.25.2.1.2. According to Proposition 2,
similarly to Example 4, we need to perform the matrix inversions and multiplications. Notice that in this
case, we need K times matrix inversions and multiplications in each iteration. Thus the computational
complexity of the static sensor fusion is O(K M3).
Example 6.
Consider the local estimator of Section 4.25.2.2.1. According to the method, we need to
perform the matrix inversions and multiplications. Notice that in this case, the size of the matrix is not
M, but nM, where n is the time step. Thus the computational complexity of the estimator is O(n3M3).
Example 7.
Consider the Kalman ﬁltering of Section 4.25.2.2.2. According to the method, we need
to perform the matrix inversion and multiplications. Similar to Example 5, we need K times matrix
inversions and multiplications per iteration. Thus computational complexity of distributed Kalman
ﬁltering is O(K M3).
Example 8.
Consider the computational complexity of the methods in Sections 4.25.4.2.1 and
4.25.4.2.2. It is given by three components: the computational complexity of matrix operations to
ﬁnd the optimal weights, the computational complexity of a bisection algorithm, and the computational
complexity for the estimation of the covariance matrix. To ﬁnd the optimal weights gn,k and hn,k, it is
required to compute a matrix pseudo-inversion, and matrix multiplications with matrices of size Nk.
Matrix pseudo-inversion: In this case, we can shrink the matrix

(n−1,k + λn,kI) ◦ϕn,kϕT
n,k

from the full-zero rows and columns to form a new matrix of size Nϕk. Thus the complexity is
still O

N 3
ϕk

.
Matrix multiplication: Similarly the complexity is O

N 3
ϕk

for these shrunk matrices.
As a result, the computational complexity needed to ﬁnd optimal weights gn,k, hn,k can be obtained as
O

N 3
ϕk

+ O

N 3
ϕk

+ O

N 2
ϕk

∼O

N 3
ϕk

.
We use the bisection method to ﬁnd the optimal value for λn,k, which fulﬁls the following equations:
(
gT
n,kgn,k
)
λ∗
n,k
−ψn,k = 0,
where λ∗
n,k is in the interval:

0, max
"
0, 
#
,
in which
 =
1
1T CQ−1C1
*
1
ψn,k
−a2ℓ(Pn−1,k).
Let us assume that ϵ is the required accuracy for the bisection test. Then the bisection would search at
most log2 /ϵ times to determine the number, since each search halves the interval. Furthermore, the

4.25.5 Computational Complexity and Communication Cost
701
complexity of searching for the bisection method is
O

O

N 3
ϕk

min ( log2 /ϵ, MaxIter)

in which Maxiter is a number of maximum iterations.
From the above analysis, we can conclude that the methods used in Section 4.25.4.2.2 needs
several operations. If we set MaxIter large enough, the computational complexity is approximately
O

N 3
ϕk log2 /ϵ

.
4.25.5.2 On communication cost
Due to the limited communication resources for the network, before implementing a distributed estima-
tor, we need to analyze the communication cost as well. We deﬁne the number of messages exchanged
by the sensors as the communication cost.
Example 9.
In the network with star topology mentioned in Section 4.25.2, every sensor need sharing
sending messages to the center fusion. Thus the total communication cost is O(K) for each iteration,
where K is the number of sensors in the network.
Example 10.
In the network with arbitrary topology mentioned in Section 4.25.4, every sensor
needs sending its messages with its neighbors. Since via wireless communication channels, sensor can
broadcast its messages to all sensors inside its communication range, the total communication cost is
O(K) per iteration.
4.25.5.3 Summary of the computational complexity and communication cost
In this section, we have studied the computational complexities and communication cost for the dis-
tributed estimation methods summarized in the preceding sections. Now, we summarize the result in
Table 25.1.
Table 25.1 Summary of the Time Complexity and Communication Cost. M is the at Most Size
of the Tracked Signal, n is the Time Step, While K is the Number of the Sensors in the Network.
R Represents the Extra Communication Cost for the Routing, Which Will be Explained Later
Topology
Signal
Algorithm
Complexity
Cost
Star
RM
Combining estimators in Section 4.25.2.1.1
O(M3)
O(K ) + R
RM
Static sensor fusion in Section 4.25.2.1.2
O(KM3)
O(K ) + R
RM
Transmission local in Section 4.25.2.2.1
O((n + M)3)
O(K ) + R
RM
Distributed Kalman ﬁlter in Section 4.25.2.2.2
O(KM3)
O(K ) + R
Arbitrary
R1
Static sensor fusion in Section 4.25.4.1
O(N 3
k )
O(K )
R1
Dynamic sensor fusion in Section 4.25.4.2.2
O(O(N 3
k )
O(K )
log2 /ϵ)

702
CHAPTER Distributed Estimation
In Table 25.1, Signal represents the signal tracked by the sensors network, Complexity represents
the computational complexity, whereas Cost represents the communication cost. It is worth mentioning
that the R in the Cost represents the extra communication cost used for routing when the sensors only
have limited communication range.
The table shows that without considering routing, the communication costs are approximately same
for the networks with star and arbitrary topology. However, the computational complexities vary for
different algorithms. Generally, transmitting local estimation (in Section 4.25.2.2.1) needs most running
time compared to other algorithms used in star topology. Moreover, dynamic sensor fusion (in Section
4.25.4.2.2) needs more running time than the static sensor fusion.
4.25.6 Conclusion
This chapter introduced basic notions of distributed estimation theory, and some implications for the
applications with and without considering the limitations in the networks. Moreover, an analysis of
the computational complexity and communication cost of these distributed algorithms was performed.
Generally, the less the limited capability of the network and the greater the knowledge of the physical
phenomenon, the lower the complexity of the resulting estimators. Nevertheless, it is often possible to
establish accurate distributed estimators in the networks. We remark that, this study we gave here is an
essential overview on some key aspects of distributed estimation. Much more can be summarized (e.g.,
the convergence or consensus properties of the distributed estimators).
Appendix
A.1 Optimal mean square estimate of a random variable
We will be interested in Minimum Mean Square Error (MMSE) estimates. Given a random variable Y
that depends on another random variable X, obtain the estimate ˆX such that the mean square error given
by E[X −ˆX]2 is minimized. The expectation is taken over the random variables X and Y.
Proposition A.1 (Lemma 1 in Henrik’s Kalman Filtering Lecture [36]).
The Minimum Mean
Square Error estimate is given by the conditional expectation E

X|Y = y

.
Proof.
The arguments are standard. Consider the functional form of the estimator as g(Y). Let
fX,Y (x, y) denote the joint probability density function of X and Y. Then the cost function C is given by
E
(
X −ˆX
)2
=

x

y
(x −g(y))2 fX,Y (x, y)dxdy
=

y
dy fY (y)

x
(x −g(y))2 fX|Y (x|y)dx.
Now consider the derivative of the cost function with respect to the function g(y).
∂C
∂g(y) =

y
dy fY (y)

x
2(x −g(y)) fX|Y (x|y)dx

4.25.6 Conclusion
703
= 2

y
dy fY (y)(g(y)) −

x
x fX|Y (x|y)dx
= 2

y
dy fY (y)(g(y)) −E

X|Y = y

.
Thus the only stationary point is g(y) = E

X|Y = y

. Moreover it is easy to see that it is a
minimum.
□
The result holds for vector random variables as well.
MMSE estimates are important because for Gaussian variables, they coincide with the Maximum
Likelihood (ML) estimates. Of course, for non-Gaussian random variables, other notions of optimality
may be better (Recall Moving Horizon Estimation [36]).
It is also a standard result that for Gaussian variables, the MMSE estimate is linear in the state value.
Proof was given in the lecture on Kalman ﬁltering. So we will restrict our attention to linear estimates
now. Also, from now on we will assume zero mean values for all the random variables. All the results can
however be generalized. The covariance of X will be denoted by RX and the cross-covariance between
X and Y by RXY .
Proposition A.2.
The best linear MMSE estimate of X given Y = y is
ˆx = RXY R−1
Y y
with the error covariance
P = RX −RXY R−1
Y RY X.
Proof.
Let the estimate be ˆx = K y. Then the error covariance is
C = E
(
(x −K y)(x −K y)T )
= RX −K RY X −RXY K T + K RY K T .
Differentiating C w.r.t. K and setting it equal to zero yields
−2RXY + 2K R−1
Y
= 0.
The result follows immediately.
□
In the standard control formulations, we are also interested in measurements that are related linearly
to the variable being estimated (usually the state).
Proposition A.3.
Let y = Hx + v, where H is a matrix and v is a zero mean Gaussian noise with
covariance RV independent of X. Then the MMSE estimate of X given Y = y is
ˆx = RX H T 
H RX H T + RV
−1
y
with the corresponding error covariance
P = RX −RX H T 
H RX H T + RV
−1
H RX.

704
CHAPTER Distributed Estimation
Proof.
Follows immediately by evaluating the terms RXY and RY and substituting in the result of
Proposition A.2.
□
A.2 Matrix inversion formula
Proposition A.4.
For compatible matrices A, B, C and D,
"
A + BC D
#−1 = A−1 −A−1B(C−1 + DA−1B)−1DA−1,
assuming the inverses exist.
Proof.
Begin by considering the block matrix
M =
 A B
C D

.
By doing the LDU and UDL decomposition of M and equating them, we obtain

I
0
C A−1 0
  A
0
0 D −C A−1B
 
I A−1B
0
I

=

I BD−1
0
I
 
A −BD−1C 0
0
D
 
I
0
D−1C I

.
Thus inverting both sides yields

I −A−1B
0
I
  A−1
0
0
"
D −C A−1B
#−1
 
I
0
−C A−1 0

=

I
0
−D−1C I
  "
A −BD−1C
#−1
0
0
D−1
 
I −BD−1
0
I

.
Equating the (1, 1) block shows
(A −BD−1C)−1 = A−1 + A−1B(D −C A−1B)−1C A−1.
Finally substituting C →−D and D →C−1, we obtain
"
A + BC D
#−1 = A−1 −A−1B(C−1 + DA−1B)−1DA−1.
□
Relevant Theory: Signal Processing Theory and Statistical Signal Processing
See Vol. 1, Chapter 3 Discrete-Time Signals and Systems
See Vol. 1, Chapter 4 Random Signals and Stochastic Processes
See Vol. 1, Chapter 5 Sampling and Quantization
See Vol. 3, Chapter 9 Diffusion Adaptation over Networks

References
705
References
[1] A. Willsky, M. Bello, D. Castanon, B. Levy, G. Verghese, Combining and updating of local estimates and
regional maps along sets of one-dimensional tracks, IEEE Trans. Autom. Control 27 (4) (1982) 799–813.
[2] Y. Bar-Shalom, On the track-to-track correlation problem, IEEE Trans. Autom. Control 26 (2) (1981) 571–572.
[3] Y. Bar-Shalom, L. Campo, The effect of the common process noise on the two-sensor fused-track covariance,
IEEE Trans. Aerosp. Electron. Syst. AES-22 (6) (1986) 803–805.
[4] J. Roecker, C. McGillem, Comparison of two-sensor tracking methods based on state vector fusion and
measurement fusion, IEEE Trans. Aerosp. Electron. Syst. 24 (4) (1988) 447–449.
[5] K. Chang, R. Saha, Y. Bar-Shalom, On optimal track-to-track fusion, IEEE Trans. Aerosp. Electron. Syst. 33
(4) (1997) 1271–1276.
[6] S. Mori, W. Barker, C.-Y. Chong, K.-C. Chang, Track association and track fusion with nondeterministic
target dynamics, IEEE Trans. Aerosp. Electron. Syst. 38 (2) (2002) 659–668.
[7] C.Y. Chong, Hierarchical estimation, in: Proceedings of the Second MIT/ONR C3 Workshop, 1979.
[8] C.B.C.D. Willner, K.P. Dunn, Kalman ﬁlter algorithms for a multisensor system, in: Proceedings of the 15th
Conference on Decision and Control, 1976.
[9] M. Hassan, G. Salut, M. Singh, A. Titli, A decentralized computational algorithm for the global kalman ﬁlter,
IEEE Trans. Autom. Control 23 (2) (1978) 262–268.
[10] J. Speyer, Computation and transmission requirements for a decentralized linear-quadratic-gaussian control
problem, IEEE Trans. Autom. Control 24 (2) (1979) 266–269.
[11] B.C. Levy, D.A. Castañon, G.C. Verghese, A.S. Willsky, A scattering framework for decentralized estimation
problems, Automatica 19 (4) (1983) 373–384.
[12] C.-Y. Chong, S. Mori, K.-C. Chang, Information fusion in distributed sensor networks, in: American Control
Conference, June 1985, pp. 830–835.
[13] H. Hashemipour, S. Roy, A. Laub, Decentralized structures for parallel kalman ﬁltering, IEEE Trans. Autom.
Control 33 (1) (1988) 88–94.
[14] N. Carlson, Federated square root ﬁlter for decentralized parallel processors, IEEE Trans. Aerosp. Electron.
Syst. 26 (3) (1990) 517–525.
[15] B. Rao, H. Durrant-Whyte, Fully decentralised algorithm for multisensor kalman ﬁltering, Control Theory
Appl. IEE Proc. D 138 (5) (1991) 413–420.
[16] T. Berg, H. Durrant-Whyte, Distributed and decentralized estimation, in: Proceedings of Singapore Inter-
national Conference on Intelligent Control and Instrumentation, 1992 (SICICI ’92), vol. 2, February 1992,
pp. 1118–1123.
[17] O.E. Drummond, Tracklets and a hybrid fusion with process noise, in: Proceedings of the SPIE, Signal and
Data Processing of Small Targets, 1997.
[18] M.E. Liggins II, C.-Y. Chong, I. Kadar, M. Alford, V. Vannicola, S. Thomopoulos, Distributed fusion archi-
tectures and algorithms for target tracking, Proc. IEEE 85 (1) (1997) 95–107.
[19] C.-Y. Chong, S. Mori, W. Barker, K.-C. Chang, Architectures and algorithms for track association and fusion,
IEEE Aerosp. Electron. Syst. Mag. 15 (1) (2000) 5–13.
[20] V. Gupta, B. Hassibi, R.M. Murray, Optimal LQG control across packet-dropping links, Syst. Control Lett.
56 (6) (2007) 439–446.
[21] V. Gupta, N.C. Martins, J.S. Baras, Stabilization over erasure channels using multiple sensors, IEEE Trans.
Autom. Control 54 (7) (2009) 1463–1476.
[22] P. Hovareshti, V. Gupta, J.S. Baras, On sensor scheduling using smart sensors, in: Proceedings of the IEEE
Conference on Decision and Control (CDC), December 2007.

706
CHAPTER Distributed Estimation
[23] V. Gupta, N.C. Martins, On fusion of information from multiple sensors in the presence of analog erasure
links, in: Proceedings of the IEEE Conference on Decision and Control (CDC), December 2009.
[24] Z.Q. Luo, Universal decentralized estimation in a bandwidth constrained sensor network, IEEE Trans. Inf.
Theory 51 (6) (2005) 2210–2219.
[25] J.-J. Xiao, A. Ribeiro, Z.-Q. Luo, G. Giannakis, Distributed compression-estimation using wireless sensor
networks, IEEE Signal Process. Mag. 23 (4) (2006) 27–41.
[26] Z.Q. Luo, An isotropic universal decentralized estimation scheme for a bandwidth constrained ad hoc sensor
network, IEEE J. Sel. Areas Commun. 23 (4) (2005) 735–744.
[27] A. Ribeiro, G.B. Giannakis, S. Roumeliotis, SOI-KF: distributed Kalman ﬁltering with low-cost communi-
cations using the sign of innovations, in: Proceedings of the IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), vol. 2, Toulouse, 2006, pp. 153–156.
[28] R. Olfati-Saber, R. Murray, Consensus problems in networks of agents with switching topology and time-
delays, IEEE Trans. Autom. Control 49 (9) (2004) 1520–1533.
[29] W. Ren, R. Beard, Consensus seeking in multiagent systems under dynamically changing interaction topolo-
gies, IEEE Trans. Autom. Control 50 (5) (2005) 655–661.
[30] A. Jadbabaie, J. Lin, A. Morse, Coordination of groups of mobile autonomous agents using nearest neighbor
rules, IEEE Trans. Autom. Control 48 (6) (2003) 988–1001.
[31] D. Spanos, R. Olfati-Saber, R. Murray, Dynamic consensus on mobile networks, in: Information Processing
in IFAC World Congress, 2006.
[32] L. Xiao, S. Boyd, S. Lall, A scheme for robust distributed sensor fusion based on average consensus, in:
Fourth International Symposium on Information Processing in Sensor Networks (IPSN 2005), April 2005,
pp. 63–70.
[33] A. Speranzon, C. Fischione, K.H. Johansson, A. Sangiovanni-Vincentelli, A distributed minimum variance
estimator for sensor networks, IEEE J. Sel. Areas Commun. 26 (4) (2008) 609–621 (special issue on Control
and Communications).
[34] C. Fischione, Fast-lipschitz optimization with wireless sensor networks applications, IEEE Trans. Autom.
Control 56 (10) (2011) 2319–2331.
[35] B. Zhu, B. Sinopoli, K. Poolla, S. Sastry, Estimation over wireless sensor networks, in: American Control
Conference, 2007, pp. 2732–2737.
[36] H.
Sandberg,
Ncs:
Kalman
ﬁltering.
<http://www.cds.caltech.edu/murray/wiki/index.php?title=NCS:
KalmanFiltering>.

26
CHAPTER
Introduction to Audio Signal
Processing
Patrick A. Naylor
Department of Electrical and Electronic Engineering, Imperial College, Exhibition Road, London, UK
4.26.1 Background
This section addresses selected topics concerning signal processing techniques applied to audio signals.
In general, the term audio refers to sound, particular when such sound is recorded, reproduced, trans-
mitted or stored. Audio signal processing therefore refers to the processing of signals representative of
sound, in one form or another. A key feature of audio in this context is that it is normally intended to be
heard by human listeners and therefore the bandwidth of interest extends over the range of frequencies
covered by human hearing, typically quoted as 20 Hz to 20 kHz, but varying depending on the age and
acuity of the listener.
The link between audio and acoustics has been and remains close. However, in recent times, audio
signal processing has been used rather more speciﬁcally to refer to aspects of the processing of sound
signals and not sound capturing or rendering, though the demarcation is neither rigid nor universally
clear. Audio signals themselves may represent any type of sound—speech, music, movie soundtracks,
bird song or noise being examples. There is in existence a very signiﬁcant body of scientiﬁc knowledge
and ongoing research into speech signals. Therefore, though speech is an audio signal, is it common
in the scientiﬁc literature to reserve the term speech processing purely for speech signals which leaves
audio processing to refer to audio generally but very often to music speciﬁcally.
From an historical perspective, audio signal processing is a long-standing topic dating back to include
the early phonographic recording and reproduction devices of Thomas Edison from around 1877. Such
devices captured acoustic signals and stored them in a physical form from which the sound could later
be rendered repeatedly. Transmission of audio signals, whether live broadcasts or recordings was driven
initially by popularity of the wireless and transistor radio which offered consumers low-cost access to
a wealth of music, news and entertainment.
The advent of the digital era had a huge impact on audio. High quality audio became easily available
thanks to the introduction of the compact disc around 1983. Recording of audio gradually switched
to digital from analog means and the scope for storage and transmission of audio in digital formats
opened up. It was soon evident that efﬁcient techniques were required to encode audio signals for
storage and transmission, where the concept of efﬁciency here implies an objective to reduce the bit
rate required to represent the signals while not reducing the perceived quality of the rendered signal
after corresponding decoding. The topic now known as audio coding was therefore born to address
this objective. A well known landmark in the development of audio coding is the MP3 standard which
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00026-1
© 2014 Elsevier Ltd. All rights reserved.
709

710
CHAPTER 26 Introduction to Audio Signal Processing
was particularly effective in enabling the development of portable music players capable of storing
very large numbers of music tracks. This format and a host of variants is now ubiquitously featured in
many mobile electronic devices including mobile phones. Many new challenges exist for audio coding
including not only further reduction in bit rate and enhancement of perceptual quality but also several
extensions of the technology so as to be able to support multichannel audio, for example.
Given wide availability of digitally stored music signals, the capability to analyze and manage music
programme material becomes a requirement in order, for example, to search for a favorite song within a
large repository of tracks. Examples of tasks that may be required include music genre classiﬁcation, tune
identiﬁcation and beat detection. Such processing of music signals may also be helpful in monetizing
commercial audio material and/or protecting legal rights. Music analysis and music information retrieval
is a relatively new topic which has gained impetus from recent technology advances in the music and
entertainment industries.
4.26.2 Overview of the chapters
4.26.2.1 Music signal processing
In this chapter on music signal processing, the focus is on the analysis of music signals and the extraction
(or retrieval) of information pertaining to the music content of the signals, rather than the intrinsic
characteristics of the signals themselves. Whereas information can be extracted from signals in a variety
of different ways for different purposes, this task speciﬁcally aims to extract music-speciﬁc information.
Indeed, the point is clearly made that it is only by exploiting music-speciﬁc information extraction
that useful tasks can then be performed on the music represented by the signal. Accordingly, the
chapter begins by setting out some underlying principles of music signals, particularly western music.
Features including harmony, rhythm, timbre and melody are explained and, in the context of equal-
tempered tuning, chroma-based features are presented as a powerful characterization and the associated
chromagram as a very useful visualization tool.
The chapter then turns the discussion to the musical aspects of note onsets, beat and tempo. The con-
ceptsoftimbreandinstrumentationarethenconsideredandappropriatefeature-setsarediscussed.Chord
recognition and audio matching are example tasks which are demonstrated with experimental results.
The subtopic of music transcription is intriguing. Given a music audio signal containing polyphonic
music, the task of transcription involves analyzing the signal so as to determine the musical score (or
other musical notation such as MIDI) which, if performed and recorded, would recreate the same music
audio signal at least in the key musical aspects. Extracting a transcription of a single instrument from
a polyphonic recording can be seen as a special case of the general transcription task. This case is
studied in more detail in the latter part of this chapter in the context of separation of a lead vocal from a
polyphonic accompaniment. This speciﬁc case has several useful applications including content-based
retrieval such as query-by-humming, or can be used to create a karaoke version of a given song.
4.26.2.2 Audio coding
Perceptual audio coding is the key focus of this chapter. Perceptual audio coding enables very efﬁcient
storage of audio signals on devices with limited memory or, equally importantly, the transmission over

4.26.2 Overview of the Chapters
711
communications links with limited capacity. Unlike waveform coding, perceptual coders aim to preserve
the perceived quality of the signal when rendered as sound and listened to; there is no speciﬁc attempt
made to preserve any characteristics of the signal that do not contribute to the perceived quality of the
sound.
The chapter begins with an overview of the main concepts underpinning perceptual audio coding for
which it is necessary to consider redundancy in the context of auditory perception and psychoacoustics
in relation to sound quality. It then goes onto discuss the structure and components found is modern
audio coding schemes for monophonic audio coding and schemes for stereophonic coding including
joint stereo and mid/side coding.
When multiple channels of audio are available and can be coded, there is the potential to render
spatial audio—sound for which the listener perceives the effects of the source location and potentially
also the impact of the environment due to reverberation. Spatial audio coding is discussed with some
details showing example schemes which aim to give a perceptually satisfying impression of the original
spatial sound but without directly encoding all the multiple channels. Instead, the coder extracts spatial
information as a compact set of parameters which, when combined with a so-called downmix signal,
can be used to synthesize a multichannel representation of the spatial audio sound-image. This approach
allows for an extremely compact representation of spatial audio and relies heavily on spatial cues that
are psychoacoustically effective.
The discussion next introduces the concept of scalable audio coding that is useful when the capacity
of a channel via which audio is transmitted is not known or ﬁxed in advance, such as for example
when transmitting over the Internet network. Scalable coding refers to a coding scheme which supports
decoding of a reduced-rate subset of the transmitted information in a useable way—meaning that the
data subset is useable for decoding the audio signal though usually with some reduction in audio quality.
The deployment of audio coding technology has beneﬁtted greatly from the compatibility of devices
achieved by standardization of various coding schemes. Devices can be designed to be compliant with
one or more of the standards so that they can decode and play audio encoded elsewhere, provided
that the decoder knows of and supports the encoder’s standardized coding scheme. The latter part of
this chapter is a substantial overview of some of the key audio coding standards, giving an historical
perspective, some of the technical speciﬁcs and indications of appropriate deployment cases.

27
CHAPTER
Music Signal Processing
Meinard Müller* and Anssi Klapuri†
*International Audio Laboratories Erlangen, Semantic Audio Processing,
Am Wolfsmantel 33, 91058, Erlangen, Germany
†Tampere University of Technology, Korkeakoulunkatu 1, Tampere, Finland
4.27.1 Introduction
The objective of this chapter is to give an overview of recent developments in the ﬁeld of music signal
processing, with a focus on techniques for music analysis and retrieval. The extraction of meaningful
information from audio waveform data is an important application area of digital signal processing.
When dealing with speciﬁc audio domains such as speech or music, it is crucial to properly under-
stand and apply the appropriate domain-speciﬁc properties, be they acoustic, linguistic, or musical.
For example, language models play a very important role in speech processing and are an essential
part of modern speech recognition systems. In recent years, many techniques and representations have
been transferred from the speech domain to the music domain. However, music signals possess speciﬁc
acoustic and structural characteristics that are not shared by spoken language or audio signals from
other domains. To account for musical dimensions such as harmony, rhythm, timbre, or melody, spe-
cialized signal processing methods that exploit musical characteristics are indispensable. For example,
exploiting the fact that most Western music is based on the equal-tempered scale, chroma-based audio
features have turned out to be a powerful mid-level representations now being used as a standard tool
in music information retrieval. Furthermore, instrument or sound models yield valuable information on
the spectral energy distribution or the percussiveness of certain acoustic events, which can be exploited
for designing semantically expressive audio features. Such specialized audio features often allow for
tackling otherwise infeasible music analysis problems.
To consider music-speciﬁc properties, the subsequent sections of this chapter are organized along
the different musical dimensions: harmony, rhythm, timbre, and melody. In Section 4.27.2, we discuss
various time-frequency representations that account for pitch and harmony. In Section 4.27.3, we address
the musical aspects of note onsets, beat, and tempo. In Section 4.27.4, we discuss acoustic features
and models that can be used to describe timbre and instrumentation. Finally, in Section 4.27.5, we
explain how acoustic and musical characteristics of vocal melodies can be utilized to transcribe and
separate the lead vocals from polyphonic music. In all sections, we also discuss a number of selected
music analysis tasks in order to illustrate the potential and applicability of the respective processing
techniques.
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00027-3
© 2014 Elsevier Ltd. All rights reserved.
713

714
CHAPTER 27 Music Signal Processing
0
1
2
3
4
−0.5
0
0.5
0
1
2
3
4
500
1000
1500
2000
2500
3000
0
1
2
3
4
−0.5
0
0.5
0
1
2
3
4
500
1000
1500
2000
2500
3000
0
1
2
3
4
−0.2
0
0.2
0.4
0.6
0
1
2
3
4
500
1000
1500
2000
2500
3000
(a)
(b)
(c)
(d)
(e)
(f)
Time [sec]
Time [sec]
Time [sec]
Amplitude
Frequency [Hz]
FIGURE 27.1
Various music signals represented as waveform (top) and magnitude spectrogram (bottom). The ﬁgure shows
examples for a C4 note played by (a)/(b) a piano, (c)/(d) a violin, and (e)/(f) a ﬂute.
4.27.2 Pitch and harmony
To ﬁx some basic notation, we start with some remarks on music representations (Section 4.27.2.1)
and then introduce the spectrogram representation (Section 4.27.2.2), which is used as the basis for
most signal processing techniques discussed in this chapter. Exploiting the model assumption that most
Western music is based on the equal-tempered scale, we then derive various musically meaningful audio
representations including the log-frequency spectrogram (Section 4.27.2.3) as well as the chromagram
(Section 4.27.2.4). In particular, chroma-based audio features, which closely relate to the musical
aspect of harmony, have turned out to constitute a powerful mid-level representation for analyzing and
comparing music signals. Exemplarily, we discuss a number of music analysis applications based on
these representations (Section 4.27.2.5).
4.27.2.1 Music representations
The processing of music data poses many challenging problems, since musical information can be
represented in many different ways and modalities, which differ fundamentally in their respective
structure and content. In the following, we summarize some basic properties of three widely used music
representations and refer to [1,2] for further details and links to the literature.
A musical score, also referred to as sheet music, gives a visual or symbolic description of what
we commonly refer to—in particular for Western classical music—as the “piece of music.” The score
encodes a musical work in a formal language and depicts it in a graphical-textual form, which allows
a musician to create a performance by following the given instructions, see Figure 27.2a. In particular,

4.27.2 Pitch and Harmony
715
(a)
(b)
(c)
C4
D4
E4
F4
G4
A4
B4
C5
C 4 D 4
F 4 G 4 A 4
Note
p
fMIDI (p) fMIDI (p −0.5)
fMIDI (p + 0.5) bandwidth
Q
C
D
F
G
A
C4 60
261.63
254.18
269.29
15.11
17.31
4 61
277.18
269.29
285.30
16.01
17.31
D4 62
293.66
285.30
302.27
16.97
17.31
4 63
311.13
302.27
320.24
17.97
17.31
E4 64
329.63
320.24
339.29
19.04
17.31
F4 65
349.23
339.29
359.46
20.18
17.31
4 66
369.99
359.46
380.84
21.37
17.31
G4 67
392.00
380.84
403.48
22.65
17.31
4 68
415.30
403.48
427.47
23.99
17.31
A4 69
440.00
427.47
452.89
25.41
17.31
4 70
466.16
452.89
479.82
26.93
17.31
B4 71
493.88
479.82
508.36
28.53
17.31
C5 72
523.25
508.36
538.58
30.23
17.31
FIGURE 27.2
(a) Musical score of a C major scale (C4 to C5). (b) Part of piano keyboard with keys ranging from C4 to C5.
(c) MIDI pitch p, center frequency fMIDI(p), cutoff frequencies fMIDI(p −0.5) and fMIDI(p + 0.5) to the left
and right, bandwidth, and Q factor.
a score contains information on the notes such as musical onset time, pitch, duration, and further hints
concerning dynamics, articulation, and tempo.
In contrast, the waveform of an audio signal as used for CD recordings shows the deviation of
the air pressure from the average air pressure, see Figure 27.1. These deviations are caused by some
vibrating object such as the vocal chords of a singer, the vibrating string and sound board of a violin, or
the membrane of a kettle drum. The waveform representation, as opposed to the score representation,
encodes all information needed to reproduce the acoustic realization of a speciﬁc musical interpretation.
This includes the temporal, dynamic, and tonal micro-deviations that make the music seem to be alive.
However, in a waveform no note parameters such as onset times, neither pitches nor note durations
are given explicitly. This makes the analysis and comparison of music signals a difﬁcult tasks, in
particular when regarding polyphonic music where different instruments and voices are superimposed
upon each other.
Finally, the MIDI format may be thought of as a hybrid of the last two data formats that explicitly
represents content-based information such as note onsets and pitches but may also encode agogic and
dynamic subtleties of some speciﬁc interpretation. In particular, a MIDI ﬁle contains a list of MIDI
messages together with time stamps, which are required to determine the timing of the messages.
In particular, the note-on and the note-off messages correspond to the start and the end of a note,
respectively. Furthermore, pitches are encoded by integers between 0 and 127, which are also referred
to as MIDI note numbers or MIDI pitches. For example, for an electronic piano, the musical pitch A0
is typically encoded by the MIDI pitch p = 21 and the musical pitch C8 by p = 108.
4.27.2.2 Spectrogram
The Fourier transform is used as front-end transform for deriving a large number of different musi-
cally relevant audio features. Let x be a real-valued discrete signal obtained by equidistant sam-
pling of a waveform with respect to a ﬁxed sampling rate Fs given in Hertz (Hz). For example,

716
CHAPTER 27 Music Signal Processing
one has Fs = 44, 100 Hz for a CD recording. Using an N-point tapered window w (e.g., Hamming
w(n) = 0.54 −0.46 cos (2πn/N) for n ∈[0 : N −1] := {0, 1, . . . , N −1}) and an overlap of half
the window length (N assumed to be even), we obtain a spectrogram X = (X(t, k))t∈[0:T −1],k∈[0:K] by
applying a short-time Fourier transform (STFT):
X(t, k) =
N−1

n=0
w(n)x(n + t · N/2) exp{−j2πkn/N}
(27.1)
with t ∈[0 : T −1] and k ∈[0 : K]. Here, T determines the number of frames, K = N/2 is the
frequency index corresponding to the Nyquist frequency, and X(t, k) denotes the kth Fourier coefﬁcient
for time frame t. Thus, each Fourier coefﬁcient X(t, k) is associated with the physical time position
t · N/(2Fs) given in seconds and with the physical frequency
fcoeff(k) := k
N · Fs
(27.2)
given in Hertz (Hz). For example, using Fs = 44, 100 and N = 4096 results in time resolution of
46.4 ms and frequency resolution of 10.8 Hz.
Theanalysisofmusicrecordingsconstitutesadifﬁcultproblem,largelyduetothecomplexityofmusi-
cal sounds. For example, the sound of even a single note played on an instrument is already a complex
mixture of different tonal as well as noise-like elements. This fact is illustrated by Figure 27.1 showing
the spectrograms of three music signals, where the note C4 is played by a piano, a violin, and a ﬂute,
respectively. For many instruments such as the piano, the sound of a note is loudest immediately after
it is played and then decays exponentially with time, see Figure 27.1a. Striking a piano causes a sudden
increase in energy typically spread across the entire frequency range—also referred to as attack phase
or attack transient of the sound. The resulting vertical structure in the spectrogram is a clear indicator of
the note’s onset time, see Figure 27.1b. In contrast, the horizontal lines correspond to the fundamental
frequency of the note C4 (roughly at 262 Hz) as well as the harmonics, which are approximately integer
multiples of the fundamental frequency. The characteristics of sounds produced by other instruments
may differ signiﬁcantly from the piano sound. For example, the violin sound has a less pronounced
attack, see Figure 27.1d. Also, the energy distribution across the harmonics, which is a determining
factor for the timbre of the sound, varies widely between instruments. Additionally, musical effects such
as vibrato or tremolo are used to further shape the quality of a sound. Here, vibrato, as clearly visible
in the violin’s spectrogram (Figure 27.1d), appears as periodic variation in frequency, whereas tremolo,
as can be seen in the ﬂute’s spectrogram (Figure 27.1f), appears as periodic variation in amplitude.
4.27.2.3 Log-frequency spectrogram
The pure tone, which corresponds to a sinusoidal waveform, can be considered as the prototype of
an acoustic realization of a musical note. The property of a sound that correlates with the perceived
frequency is commonly referred to as the pitch. For example, the middle A, also known as the concert
pitch, has a frequency of 440 Hz. It is a well-known fact that the human perception of pitch is logarithmic
in nature, see [3]. For example, the perceived distance between the pitches of the A3 (220 Hz) and A4
(440 Hz) is the same as the perceived distance between the pitches A4 (440 Hz) and A5 (880 Hz).

4.27.2 Pitch and Harmony
717
The interval between two sounds with half or double the frequency is called an octave. The close relation
between sounds separated by one octave, also referred to as octave equivalency, is closely related to
our sensation of harmony and lays the foundation of the music notation based on the chromatic scale
as used in traditional Western music.
In particular for Western music, the equal-tempered scale, where each octave is split up into 12 loga-
rithmically spaced units, has become the predominant tuning scheme. Note, however, that using such a
scale already constitutes a strong model assumption, which is often violated for music of other cultural
regions that use different musical scales and tuning systems. Using Western music notation, the 12 units
are denoted by the twelve pitch spelling attributes C, C♯, D, . . . , B, where in the equal-tempered scale
different pitch spellings such C♯and D♭refer to the same unit. A musical pitch is then determined by
its pitch spelling attribute and the octave number. In MIDI notation, the pitches of the equal-tempered
scale are serially numbered, where the pitch A4 corresponds to the MIDI pitch p = 69. In the following,
we do not distinguish between the different notations and often simply speak of a note while meaning
a pitch. As mentioned above, the notes of the equal-tempered scale depend on their center frequencies
in a logarithmic fashion. Let fMIDI(p) denote the center frequency of the pitch p ∈[0 : 127], then
fMIDI(p) = 2(p−69)/12 · 440.
(27.3)
The logarithmic perception of frequency motivates the use of a time-frequency representation with a
logarithmic frequency axis labeled by the MIDI pitches corresponding to the notes of the equal-tempered
scale. We now discuss a basic procedure to derive such a representation from a given spectrogram
X = (X(t, k))t∈[0:T −1],k∈[0:K], for more reﬁned methods we refer to [4]. In this basic procedure, we
assign each spectral coefﬁcient X(t, k) to the pitch with center frequency that is closest to the frequency
fcoeff(k), see (27.2). More precisely, we deﬁne for each pitch p ∈[0 : 127] the set
P(p) := {k : fMIDI(p −0.5) ≤fcoeff(k) < fMIDI(p + 0.5)}.
(27.4)
From this, we obtain a log-frequency (magnitude) spectrogram YLF : [0 : T −1]×[0 : 127] deﬁned by
YLF(t, p) :=

k∈P(p)
|X(t, k)|2.
(27.5)
As an illustrating example, we consider a chromatic scale played on a piano starting with the note
A0 (p = 21) and ending with C8 (p = 108), see Figure 27.3. The resulting spectrogram, as shown
in Figure 27.3a, reveals the exponential dependency of the fundamental frequency on the pitches of
played notes. Also, as already discussed in Section 4.27.2.2, the harmonics and the notes’ onset positions
(vertical structures) are clearly visible. Figure 27.3b shows the corresponding log-frequency spectro-
gram, which is obtained by pooling Fourier coefﬁcients as described in (27.5). Now, the frequency axis
is partitioned logarithmically and labeled linearly according to MIDI pitches. As a consequence, the
played notes of the chromatic scale now appear in a linearly increasing fashion. Note that the vertical
structures as visible in the magnitude spectrogram correspond to the onsets of the played notes. In
general, as particularly revealed by the log-frequency spectrogram and the chromagram, the sounds for
higher notes possess a much cleaner harmonic spectrum than the ones for lower notes. Furthermore,
the energy distribution in the harmonics not only depends on the respective instrument, but also on the

718
CHAPTER 27 Music Signal Processing
 
 
0
10
20
30
40
50
60
70
80
90
0
10
20
30
40
50
60
70
80
90
0
10
20
30
40
50
60
70
80
90
0
500
1000
1500
2000
2500
3000
3500
4000
4500
−30
−20
−10
0
10
20
30
 
21
30
40
50
60
70
80
90
100
108
−30
−20
−10
0
10
20
30
 
C
C#
D
D#
E
F
F#
G
G#
A
A#
B
−50
−40
−30
−20
−10
0
10
(a)
(b)
(c)
Time [sec]
Frequency [Hz]
Frequency [pitch]
FIGURE 27.3
Chromatic scale. (a) Magnitude spectrogram. (b) Pitch-based log-frequency (magnitude) spectrogram.
(c) Chromagram. For visualization purposes the values are color-coded using some logarithmic scale.
sound intensity and on the pitch. For lower notes, the signal’s energy is often contained in the higher
harmonics, while the listener may still have the perception of a low-pitched sound.
Although conceptually simple, the pooling strategy based on a single spectrogram as described in
(27.5) becomes problematic for low pitches. Note that the bandwidth BW(p) := fMIDI(p + 0.5) −
fMIDI(p−0.5) depends on the pitch p and becomes smaller for decreasing pitches, see also Figure 27.2c.

4.27.2 Pitch and Harmony
719
Actually, for each pitch the Q-factor deﬁned by Q = fMIDI(p)/BW(p) is constant. For example, for
MIDI pitch p = 66 one has a bandwidth of roughly 21.4 Hz, whereas for p = 54 the bandwidth
falls below 10.7 Hz. In both cases, the Q-factor is 17.31. Now, using for example a sampling rate of
Fs = 44, 100 and a window size of N = 4096, the resulting spectrogram has a frequency resolution of
10.8 Hz, see (27.2). In this case, the frequency resolution of the spectrogram does not sufﬁce to separate
the central frequencies of adjacent MIDI pitches below p = 54. As a result, the set P(p) as deﬁned in
(27.4) may contain only very few spectral coefﬁcients or may be even empty, which typically leads to
a poor log-frequency representation in the lower pitch range.
Obviously,thefrequencyresolutioncanbeincreasedbyenlargingthewindowsize N,which,however,
results in a decreased temporal resolution. As a consequence one may loose important short-time
information such as note onsets. One popular alternative to using a single spectrogram is to construct a
multirate bank of bandpass ﬁlters, each ﬁlter corresponding to a MIDI pitch with an appropriately tuned
bandwidth [1, Section 3.1]. Although this loses the famed computational efﬁciency of the fast Fourier
transform, some of this may be regained by processing the highest octave with an STFT-based method,
downsampling by a factor of 2, then repeating for as many octaves as are desired [5]. This results
in different sampling rates for each octave of the analysis, which decreases the overall computational
complexity. A toolkit for such an analysis, also referred to as constant-Q transform, has been created
by Schörkhuber and Klapuri [6].
Finally, note that when playing a note on a real instrument, the fundamental frequency of the sound
may deviate substantially from the note’s theoretical center frequency. Such deviations may be due
to above mentioned effects such as vibrato or to some sliding of the voice (portamento) to smoothly
connect subsequent pitches. Furthermore, global frequency deviations may be the result of instruments
being tuned lower or higher than the expected reference pitch A4 (also referred to as concert pitch) with
center frequency 440 Hz. For example, many modern orchestras are using a tuning frequency slightly
above 440 Hz, whereas ensembles playing Baroque music are often tuned lower than the concert pitch.
To compensate for tuning effects, one should perform an additional tuning estimation step (prior to the
pooling step). A tuning parameter can be derived, e.g., by looking at the maximum spectral coefﬁcient
of a spectral vector averaged over the entire recording [7]. This parameter can then be used to suitably
adjust the center frequencies of the MIDI pitches and cut off frequencies in the pooling step (27.4).
4.27.2.4 Chromagram
It is a well-known phenomenon that human perception of pitch is periodic in the sense that two pitches
are perceived as similar in “color” (playing a similar harmonic role) if they differ by an octave. Based on
this observation, a pitch can be separated into two components, which are referred to as tone height and
chroma, see [8]. Here, the tone height refers to the octave number and the chroma to the respective pitch
spelling attribute contained in the set {C, C♯, D, . . . , B}. Enumerating the chroma values, we identify
this set with [0 : 11] where 0 refers to chroma C, 1 to C♯, and so on. A pitch class is deﬁned to be the set
of all pitches that share the same chroma. For example, the pitch class corresponding to the chroma C is
the set {. . . , C0, C1, C2, C3, . . .} consisting of all pitches separated by an integral number of octaves.
For simplicity, we use the terms chroma and pitch class interchangeably.
The main idea of chroma-based audio features, sometimes also referred to as pitch class proﬁles, is to
aggregate all spectral information that relates to a given pitch class into a single coefﬁcient. As a result

720
CHAPTER 27 Music Signal Processing
chroma features show a high degree of robustness to variations in timbre while closely correlating with
the musical aspect of harmony. Given a pitch-based log-frequency representation YLF : [0 : T −1]×[0 :
127] as deﬁned in (27.5), a chroma representation or chromagram C : [0 : T −1] × [0 : 11] can be
derived simply by pooling all pitch coefﬁcients that belong to the same chroma:
C(t, c) :=

{p∈[0:127] | p mod 12=c}
YLF(t, p)
(27.6)
for c ∈[0 : 11]. As ﬁrst example, Figure 27.3c shows the chromagram of the chromatic scale, where
the cyclic nature of chroma features becomes evident. Because of the octave equivalence, the increasing
notes of the chromatic scale are “wrapped around” the chroma axis. As for the log-frequency spectro-
gram, the resulting chromagram is rather noisy in particular for the lower notes. Furthermore, because of
the presence of higher harmonics, the energy is typically spread across various chroma bands even when
playing a single note at a time. For example, playing the note C3, the third harmonics corresponds to G4
and the ﬁfth harmonics to E5. Therefore, when playing the note C3 on the piano, not only the chroma
band C, but also the chroma bands G and E may contain a substantial portion of the signal’s energy.
There are many ways for computing chroma-based audio features, and the features’ properties can
change signiﬁcantly when applying pre- and postprocessing that concern spectral, temporal, and dynam-
ical aspects. For example, integrating a weighting scheme into the coefﬁcient pooling can increase the
robustness to noise [9,10]. Considering harmonics additionally to the fundamental frequency has an
inﬂuence on the robustness of chroma features to changes in timbre [10]. Furthermore, preprocessing
steps in the chroma computation based on spectral whitening [11], the estimation of the instantaneous
frequency [9], or peak picking of spectrum’s local maxima [10] may have a signiﬁcant impact on fea-
tures’s quality. Generalized chroma representations with 24, 36 or even more dimensions (instead of the
usual 12 dimensions) allow for dealing with tuning issues [10]. Adding a further degree of abstraction by
considering short-time statistics over energy distributions within the chroma bands, one obtains CENS
(Chroma Energy Normalized Statistics) features, which constitute a family of scalable and robust audio
features, see [1]. To boost the degree of timbre invariance, a novel family of chroma-based audio fea-
tures has been introduced in [12]. Here, the general idea is to discard timbre-related information similar
to that expressed by certain mel-frequency cepstral coefﬁcients (MFCCs). Various implementations
of chroma-based audio features are publicly available such as the chroma-variants by Ellis1 and the
Chroma-Toolbox2 including extractors for a variety of pitch- and chroma-based audio features [13]. As
illustration, Figure 27.4 shows various chroma-based feature representations for an audio recording of
the ﬁrst six measures of Op. 100, No. 2 by Friedrich Burgmüller. For example, note that the piece starts
with four repetitions of the chord consisting of the notes A3, C4, and E4. All shown chroma representa-
tions clearly indicate the concentration of the signal’s energy in the corresponding chroma band C, E and
A.HoweverthereisalsosomeenergyvisibleinthechromabandG,whichistheresultofthenotes’higher
harmonics (e.g., G5 is the third harmonics of C4). In the following section, we show that these chroma
variants may show a quite different behavior in the context of a speciﬁc music analysis application.
1 http://www.ee.columbia.edu/∼dpwe/resources/matlab/chroma-ansyn/.
2 http://www.mpi-inf.mpg.de/resources/MIR/chromatoolbox/.

4.27.2 Pitch and Harmony
721
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
5.5
−0.2
−0.1
0
0.1
0.2
 
 
0
1
2
3
4
5
C 
C#
D 
D#
E 
F 
F#
G 
G#
A 
A#
B 
0
2
4
6
8
10
 
 
0
1
2
3
4
5
C 
C#
D 
D#
E 
F 
F#
G 
G#
A 
A#
B 
0
0.2
0.4
0.6
0.8
1
 
 
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
5.5
C 
C#
D 
D#
E 
F 
F#
G 
G#
A 
A#
B 
0
0.2
0.4
0.6
0.8
1
 
 
0
1
2
3
4
5
C 
C#
D 
D#
E 
F 
F#
G 
G#
A 
A#
B 
−1
−0.5
0
0.5
1
Waveform
(b)
Score
(a)
(c) CP (basic chroma feature)
(d) CLP[100] (chroma feature with amplitude compression)
(e) CENS 21
5 (see [1])
(f) CRP[55] (see [12])
Time [sec]
Time [sec]
FIGURE 27.4
Score, waveform, and various chroma-based feature representations for an audio recording of the ﬁrst four
measures of Op. 100, No. 2 by Friedrich Burgmüller, see [13].
4.27.2.5 Applications
Chroma-based audio features have become a well-established tool for processing and analyzing music
data [1,10,14]. As mentioned before, chroma features possess a signiﬁcant degree of robustness to
variations in instrumentation and timbre, while closely correlating with the short-time harmonic content
of the underlying audio signal. Therefore, basically every chord recognition procedure relies on some
kind of chroma representation [15–17]. Also, chroma features have become the de facto standard for
tasks such as music synchronization and alignment [1,18–20], as well as audio structure analysis [21].
Finally, chroma features have turned out to be a powerful mid-level feature representation in content-
based audio retrieval such as cover song identiﬁcation [9,22] or version identiﬁcation, which is also
referred to as audio matching [23,24]. By means of two applications, chord recognition and audio
matching, we now want to demonstrate in more detail to which extent the ﬁnal analysis results may
depend on the used feature variant and the parameter settings. Here, rather than expressing that one
feature type is superior to others, our goal is to emphasize the importance of the feature design step in
music analysis and retrieval tasks.
4.27.2.5.1
Chord recognition
Loosely speaking, a musical chord consists of a set of two or more notes that are played simultaneously or
successively and are perceived as some kind of harmonic unit. A harmonic analysis of a piece of music,
at least as understood for Western tonal music, consists in determining the progression of chords as well
as their mutual relation and musical function. For example, the song “Let it be” by the Beatles as shown

722
CHAPTER 27 Music Signal Processing
0
2
4
6
8
10
12
    C
C#/Db
    D
D#/Eb
 E/Fb
 E#/F
F#/Gb
    G
G#/Ab
    A
A#/Bb
 B/Cb
    c
c#/db
    d
d#/eb
 e/fb
 e#/f
f#/gb
    g
g#/ab
    a
a#/bb
 b/cb
0
2
4
6
8
10
12
−0.4
−0.2
0
0.2
0.4
C
FN
FP
Time [sec]
CP
CLP[100] CENS1
1 CENS11
1
CRP[55] CRP[55]11
1
Tb
0.612
Ta
0.667
HMM 0.527
0.725
0.460
0.553
0.458
0.546
0.528
0.418
0.610
0.430
0.554
0.583
0.584
0.638
0.716
0.720
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
Tb
Ta
HMM
Feature types
F-measure
CP
CLP[100]
CENS11
CENS11
1
CRP[55]
CRP[55] 11
1
FIGURE 27.5
Left: Illustration of the chord recognition task (the beginning of the Beatles song “Let it be” is shown). Each
frame of the audio recording is labeled according to 24 possible chord categories (12 major chord and 12
minor chords). The colors indicate correctly labeled frames (C), incorrectly labeled frames or false positives
(FP), as well as non-labeled correct frames or false negatives (FN). Right: Recognition rates of different
chord recognition procedures in dependency of the choice of chroma variant (evaluation is based on Beatles
dataset and 3-fold cross validation). For details on the chroma variants, see Figure 27.4 and [13]. Top:
F-measures. Bottom: Visual representation of the results. (For interpretation of the references to color in this
ﬁgure legend, the reader is referred to the web version of this book.)
in Figure 27.5 starts with C major followed by G major, A minor, and so on. The song being written in the
C major key, these three chords act as tonic, dominant, and tonic parallel regarding their musical function
[25]. The task of automatically transforming a given music representation into a progression of chords
is often referred to as chord recognition or chord labeling, where the complexity of the task depends on
the respective music representation, the temporal resolution, the level of abstraction, the conﬁguration
of notes (if they occur simultaneously or successively), or the chords to be considered in the analysis.
Here, we assume that the piece of music is given in the form of a recorded performance. Then the chord
recognition task consists in splitting up the recording into segments and in assigning a chord label to
each of the segments. The segmentation speciﬁes the start time and end time of a chord, and the chord
label speciﬁes which chord has been played during this time period. For simplicity, one often considers
only 24 possible chord categories consisting of the 12 major and 12 minor chords. Most of the described
chord recognition procedures proceed in a similar fashion. In the ﬁrst step, the given music recording is
converted into a sequence of chroma-based audio features. These features are often further processed, for
example, by applying suitable smoothing ﬁlters to even out temporal outliers or by applying logarithmic
compression or whitening procedures to enhance small yet perceptually relevant spectral components.
In the next step, pattern matching techniques are applied to map the chroma features to chord labels
that correspond to the various musical chords to be considered. In the last step, further post-ﬁltering
techniques are applied to smooth out local misclassiﬁcations. Often, hidden Markov models (HMMs)

4.27.2 Pitch and Harmony
723
are used which jointly perform the segmentation, pattern matching, and temporal ﬁltering steps within
one optimization procedure. For further details and pointers to the literature, we refer to [15–17,26,27].
We now demonstrate by a small experiment, to which extent the ﬁnal recognition rates may depend
on the underlying chroma variant. To this end, we revert to three different pattern matching techniques.
The ﬁrst two approaches are simple template-based approaches, referred to as Tb and Ta, where the ﬁrst
approach uses data-independent binary templates and the second one data-dependent average templates.
As third approach, we employ hidden Markov models denoted by HMM. Using the annotated Beatles
dataset as described in [28], which consists of 180 Beatles songs, we computed recognition rates
based on conventional F-measures using 3-fold cross validation. Figure 27.5 shows the recognition
rates for the three pattern matching techniques in combination with different chroma variants. As these
experimental results indicate, the choice of chroma representation can have a signiﬁcant inﬂuence on
the chord recognition accuracy. In particular, a logarithmic compression step in the chroma extraction
(e.g., by applying a logarithm on the magnitude spectrogram values similar to [29]) turns out to be
crucial. Furthermore, the results reveal that temporal feature smoothing plays an important role in chord
recognition—in particular for recognizers that work in a purely framewise fashion. (For example, in
Figure 27.5, CENS11
1 is a smoothed version of CENS[55]1
1, and CRP[55]11
1 is a smoothed version of
CRP[55].) Here, an interesting observation is that the Viterbi decoding in the HMM-based recognizer
introduces a different kind of smoothing in the classiﬁcation stage so that feature smoothing has a less
signiﬁcant impact in this case, see [26] for details.
4.27.2.5.2
Audio matching
As second application scenario, we consider the task of audio matching with the goal to automatically
retrieve from a given music collection all audio fragments that musically (in our scenario harmonically)
correspond to a given query audio clip [23,24]. In this task, one challenge is to cope with variations in
timbre and instrumentation as they appear in different interpretations, cover songs, and arrangements
of a piece of music. In a typical procedure for audio matching, the query Q as well as each database
recording D are ﬁrst converted into chroma feature sequences C(Q) and C(D), respectively. Then,
a local variant of dynamic time warping is used to locally compare the query sequence C(Q) with
the database sequence C(D) yielding a distance function ϕ. Each local minimum of ϕ close to zero
indicates a fragment within the database recording that is close to the given query. In view of this
matching application, the following two properties of ϕ are of crucial importance. On the one hand,
the semantically correct matches should correspond to local minima of ϕ close to zero thus avoiding
false negatives. On the other hand, ϕ should be well above zero outside a neighborhood of the desired
local minima thus avoiding false positives. In view of these requirements, the used chroma variant
plays a major role. As an illustrative example, we consider a recording by Yablonsky of Shostakovich’s
Waltz No. 2 from the “Suite for Variety Orchestra No. 1” used as the database recording. The theme
of this piece occurs four times played in four different instrumentations (clarinet, strings, trombone,
tutti/all). Denoting the four occurrences by E1, E2, E3, and E4 and using E3 as the query, Figure 27.6
shows several distance functions based on different chroma variants. Note that one expects four local
minima. Using conventional chroma features such as CP, the expected local minima are not signiﬁ-
cant or not even existing. However, using the chroma variant CRP[55], one obtains for all four true
matches concise local minima, see the black curve of Figure 27.6. For a detailed discussion, we refer
to [12,13].

724
CHAPTER 27 Music Signal Processing
0
20
40
60
80
100
120
140
160
180
0
0.1
0.2
0.3
0.4
0.5
Time [sec]
E3=Query
E4
E2
E1
FIGURE 27.6
Several distance functions shown for the Yablonsky recording of the Shostakovich’s Waltz No. 2 from the
“Suite for Variety Orchestra No. 1” using the excerpt E3 as query. The following feature types were used: CP
(green), CLP[100] (red), CENS41
10 (blue), and CRP[55] (black). For the query, there are 4 annotated excerpts
(true matches). (For interpretation of the references to color in this ﬁgure legend, the reader is referred to
the web version of this book.)
4.27.3 Tempo and beat
The temporal and structural regularities are perhaps the most important stimulations for people to get
involved and to interact with music. It is the beat, the steady pulse that drives music forward and provides
the temporal framework of a piece of music [30]. Intuitively, the beat can be described as a sequence of
perceived pulses that are equally spaced in time and correspond to the pulse a human taps along when
listening to music [31]. Mathematically, the beat is speciﬁed by two parameters: the phase and the period,
see Figure 27.7b. The term tempo then refers to the rate of the pulse given by the reciprocal of the period.
Because tempo and beat are of fundamental musical importance, the automated extraction of this
information from music recordings is another central topic in the ﬁeld of music information retrieval.
Most approaches to tempo estimation and beat tracking proceed in two steps. In the ﬁrst step, positions
0
0.5
1
1.5
2
2.5
0
0.5
1
1.5
2
2.5
−0.5
0
0.5
−0.5
0
0.5
(a)
(b)
Period
Phase
Time [sec]
FIGURE 27.7
Waveform representation of the beginning of “Another one bites the dust” by Queen. (a) Note onsets. (b)
Beat positions.

4.27.3 Tempo and Beat
725
of note onsets within the music signal are estimated, see Figure 27.7a. Here, most approaches capture
changes of the signal’s energy or spectrum and derive a so-called novelty curve [32,33] The peaks
of such a curve yield good indicators for note onset candidates. In the second step, the novelty curve
is analyzed with regard to reoccurring patterns and quasi-periodic pulse trains. For non-percussive
music with soft note onsets, however, the extraction of beat and tempo information becomes a difﬁcult
problem. Even more challenging becomes the detection of local periodic patterns in the presence of
tempo changes. In this section, we address the issue on how to extract onset (Section 4.27.3.1), tempo
(Section 4.27.3.2), and beat (Section 4.27.3.3) information from music signals. Rather then giving
a comprehensive overview over this vibrant and well-studied research area, we introduce some key
techniques while discussing their beneﬁts and limitations.
4.27.3.1 Onset detection and novelty curve
The objective of onset detection is to determine the physical starting times of notes or other musical
events as they occur in a music recording. As for percussive instruments, playing a note often goes along
with a sudden increase of the signal’s energy, see Figure 27.1a or Figure 27.7 for examples. Having such
a pronounced attack phase, note onset candidates may be determined by locating time positions where
the signal’s amplitude envelope starts to increase. Much more challenging, however, is the detection of
onsets in the case of non-percussive music where one often has to deal with soft onsets or blurred note
transitions. This is often the case for vocal music or classical music dominated by string instruments.
For example, the waveform of the violin sound, as shown in Figure 27.1c, exhibits a slowly rising energy
increase rather than an abrupt change as for the piano sound. In such cases, the notion of an onset position
is not well-deﬁned since the note’s beginning corresponds to an entire period rather than a speciﬁc point
in time. The detection of individual note onsets becomes even harder when dealing with complex poly-
phonic music. Here, simultaneously occurring sound events may result in masking effects. As a conse-
quence, more reﬁned methods have to be used for detecting note onsets, e.g., by looking at changes in the
signal’s short-time spectrum. To illustrate these ideas, we now describe a typical spectral-based approach
for onset detection. For comprehensive overviews and further links to the literature we refer to [4,32].
The general idea of onset detection is to locate sudden changes in the music signal, which are typically
caused by the onset of notes or novel events. These changes are often captured by means of a so-called
novelty curve, the peaks of which indicate onset candidates. To obtain such a novelty curve, most
approaches ﬁrst convert the given music recording into a spectrogram X = (X(t, k))t∈[0:T −1],k∈[0:K],
see (27.1).
As mentioned above, simultaneously occurring sound events in polyphonic music may lead to mask-
ing effects that prevent any observation of an energy increase of a low intensity onset. However, as a
side-effect of a note onset, weak noise-like broadband transients may sill be observable in certain fre-
quency regions. In particular, such transients are often well detectable in the higher frequency regions
of the spectrum. Here, techniques such as logarithmic compression can help for enhancing the compar-
itively weak high-frequency information. In our context, this step consists in applying a logarithm to
the magnitude spectrogram |X| of the signal yielding
Y = log (1 + C · |X|)
(27.7)
forasuitableconstantC > 1,seealso[29].Suchacompressionstepnotonlyaccountsforthelogarithmic
sensation of human sound intensity, but also balances out the dynamic range of the signal. In particular,

726
CHAPTER 27 Music Signal Processing
0
1
2
3
4
5
6
0
2000
4000
6000
8000
10000
10
20
30
40
50
60
0
1
2
3
4
5
6
0
2000
4000
6000
8000
10000
1
2
3
4
5
6
0
1
2
3
4
5
6
0
2000
4000
6000
8000
10000
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
0
1
2
3
4
5
6
0
20
40
60
0
1
2
3
4
5
6
0
20
40
60
0
1
2
3
4
5
6
0
20
40
60
(a)
(b)
(c)
Frequency [Hz]
Time[sec]
Time[sec]
Time[sec]
FIGURE 27.8
Illustration of the effect of logarithmic compression (using the same audio excerpt as in Figure 27.9). The
ﬁgure shows the respective magnitude spectrogram (top) and the resulting novelty curve  (bottom). (a) Mag-
nitude spectrogram. (b) Compressed spectrogram using C = 1. (c) Compressed spectrogram using C = 100.
by increasing C, low-intensity values in the high-frequency spectrum become more prominent. On the
downside, a large compression factor C may also amplify non-relevant low-energy noise components.
The effect of the compression step is illustrated by Figure 27.8, which shows the magnitude spectrogram
|X| as well as compressed spectrograms Y for C = 1 and C = 100, respectively. In particular, as shown
by Figure 27.8c, the vertical structures of the note onset transients become visible for large C. Finally,
we note that similar effects can be achieved by using techniques that decreases the range of values in
the magnitude spectrum (also called spectral whitening), see [34].
To obtain a novelty curve, one basically computes the discrete temporal derivative of the compressed
spectrum Y. Here, since one is interested in note onsets (positive derivative) and not note offsets (negative
derivative), one typically only considers positive intensity changes yielding the novelty function ¯ :
[0 : T −2] →R:
¯(t) :=
K

k=0
|Y(k, t + 1) −Y(k, t)|≥0
(27.8)
for t ∈[0 : T −2], where |x|≥0 := x for a non-negative real number x and |x|≥0 := 0 for
a negative real number x. Figure 27.9e shows the resulting curve for an audio excerpt of a Waltz by
Shostakovich. The novelty curve can be further normalized by, e.g., subtracting the local mean (red curve
in Figure 27.9e) from ¯ and only keeping the positive part (half-wave rectiﬁcation), see Figure 27.9f.
Various reﬁnements have been suggested to further enhance the novelty curve. For example, instead of
takingsimpledifferencesin(27.8),onecanuseahigher-ordersmootheddifferentiator[35].Furthermore,
to account for the psychoacoustic hypothesis that the auditory system performs some sort of cross-band

4.27.3 Tempo and Beat
727
0
1
2
3
4
5
6
−0.4
−0.2
0
0.2
0.4
0
1
2
3
4
5
6
 
 
0
1
2
3
4
5
6
0
2000
4000
6000
8000
10000
0.5
1
1.5
0
1
2
3
4
5
6
0
50
100
0
1
2
3
4
5
6
0
50
100
(a)
(b)
(c)
(d)
(e)
(f)
Frequency [Hz]
Time [sec]
FIGURE 27.9
Illustration of the computation of a novelty curve by means of an audio excerpt of Shostakovich’s Waltz No. 2
from the Suite for Variety Orchestra No. 1. (a) Score representation (in a piano reduced version). (b) Waveform
of an audio excerpt (orchestral version conducted by Yablonsky). (c) Ground truth note onsets. (d) Compressed
spectrogram using C = 100. (e) Novelty curve ¯ and local mean (red curve). (f) Novelty curve ¯.
integration, rhythmic processing is often performed in certain frequency bands separately and the results
are then combined at the end, see [36]. In the context of novelty curves, it is often beneﬁcial to ﬁrst split
up the spectrum into logarithmically spaced frequency bands (typically ﬁve to eight bands are used). The
resulting band-wise novelty curves are then weighted and summed up to yield the ﬁnal novelty function.

728
CHAPTER 27 Music Signal Processing
Thepeaksofthenoveltycurveoftencorrespondtothepositionsofnoteonsets.Therefore,toexplicitly
determine the positions of note onsets, one employs peak picking strategies based on ﬁxed or adaptive
thresholding [32,37]. In the case of noisy novelty curves with many spurious peaks, however, this is
a fragile and error-prone step. Here, the selection of the relevant peaks that correspond to true note
onsets becomes a difﬁcult or even infeasible problem. For example, in the Shostakovich example of
Figure 27.9, the ﬁrst beats (downbeats) of the 3/4 m are played softly by non-percussive instruments
leading to relatively weak and blurred onsets, whereas the second and third beats are played staccato
supported by percussive instruments. As a result, the peaks of the novelty curve corresponding to
downbeats are hardly visible or even missing, whereas peaks corresponding to the percussive beats are
much more pronounced, see also Figure 27.8.
4.27.3.2 Periodicity analysis and tempo estimation
When listening to a piece of music, most humans are able to tap along the musical beat and to determine
the tempo without difﬁculty. However, transferring this cognitive process into an automated system that
reliably works for the large variety of musical styles is a challenging task. In particular, the tracking of
beat positions becomes hard in the case that a music recording reveals signiﬁcant tempo changes. This
typically occurs in expressive performances of classical music as a result of ritardandi, accelerandi,
fermatas, and rubato. Furthermore, the extraction problem is complicated by the fact that the notions of
tempo and beat may not be clearly deﬁned due to a complex hierarchical structure of the rhythm [38]. In
particular, there are various levels that are presumed to contribute to the human perception of tempo and
beat. For example, as illustrated by Figure 27.10, one may consider the tempo on the tactus level, which
typically corresponds to the quarter note level and often matches the foot tapping rate. Thinking at a
larger musical scale, one may also perceive the tempo at the measure level, in particular when listening
to very fast music or to highly expressive music with strong rubato. Finally, one may also consider the
tatum (temporal atom) level, which refers to the fastest repetition rate of musically meaningful accents
0
1
2
3
4
5
6
7
8
−0.2
0
0.2
Tatum
Tactus
Measure
FIGURE 27.10
Illustration of various pulse levels. In this example, the tactus level corresponds to the quarter note and the
tatum level to the eighth note level.

4.27.3 Tempo and Beat
729
occurring in the signal [29]. The problem of tempo ambiguity and pulse level confusion will become
more evident in the following discussions.
Most approaches to automated tempo and beat tracking rely on two assumptions. The ﬁrst assump-
tion is that beat positions typically occur at note onset positions, and the second assumption is that
beat positions are more or less equally spaced—at least for a certain period in time. Even though both
assumptions are reasonable in particular for music that can be attributed to Western popular music, there
are also many exceptions. For example, for music with syncopation (where one often has a rest at the
normally stressed beat positions) the ﬁrst assumption may be violated, whereas for music with rubato
(resulting in permanent deviations in the local tempo) the second assumption may not hold. Based on
the two assumptions, typical approaches analyze a previously computed novelty curve with regard to
reoccurring or quasi-periodic patterns. Here, generally speaking, one can distinguish between three dif-
ferent methods. The autocorrelation method allows for detecting periodic self-similarities by comparing
a novelty curve with time-shifted (localized) copies. Another widely used method is based on a bank of
comb ﬁlter resonators, where a novelty curve is compared with templates that consists of equally spaced
spikes covering a range of periods and phases. Thirdly, the short-time Fourier transform can be used to
derive a time-frequency representation of the novelty curve. Here, the novelty curve is compared with
templates consisting of sinusoidal kernels each representing a speciﬁc frequency or tempo. Each of the
methods reveals periodicity properties of the underlying novelty curve from which one can estimate the
tempo or beat structure, see [33,39] for an overview and further pointers to the literature.
As an example, we now introduce the concept of a tempogram while discussing two different peri-
odicity estimation methods in greater detail. Loosely speaking, a tempogram is a spectrogram-like
representation that indicates the intensity of the estimated periodicity given in beats per minute (BPM)
over time [40]. More precisely, let [0 : T −2] denote the sampled time axis (as for the novelty curve),
which we extend to Z to avoid boundary cases in the subsequent considerations. Furthermore, let
 ⊂R>0 be a set of tempi speciﬁed in BPM. Then, a tempogram is mapping T : Z ×  →R≥0 yield-
ing a time-tempo representation for a given time-dependent signal such as a novelty curve. Intuitively,
the value T (t, τ) indicates to which extend a pulse of tempo τ is present at time t. For example, suppose
that a music signal has a dominant tempo of τ = 220 BPM around position t, then the corresponding
value T (t, τ) should be large, see Figure 27.11. As mentioned above, one often has to deal with tempo
ambiguities, where a tempo τ is confused with integer multiples τ, 2τ, 3τ, . . . (referred to as harmonics
of τ) and integer fractions τ, τ/2, τ/3, . . . (referred to as subharmonics of τ). Analogous to the notion
of an octave in the pitch context (see Section 4.27.2.3), the difference between two tempi with half or
double the value is called a tempo octave.
As ﬁrst periodicity estimation method, we show how a short-time Fourier transform can be used to
derive a tempogram from a given novelty curve . To this end, one ﬁxes a window function W : Z →R
of ﬁnite length centered at t = 0 (e.g., a centered Hann window of size 2N +1 for some N ∈N). Then,
for a frequency parameter ω ∈R≥0 and time parameter t ∈Z, the complex Fourier coefﬁcient F(t, ω)
is deﬁned by
F(t, ω) =

n∈Z
(n) · W(n −t) · exp{−j2πωn}.
(27.9)
Note that the frequency parameter ω (measured in Hertz) corresponds to the tempo parameter
τ = 60 · ω (measured in BPM). Therefore, one obtains a discrete (magnitude) Fourier tempogram

730
CHAPTER 27 Music Signal Processing
0
1
2
3
4
5
6
−20
0
20
40
 
 
0
1
2
3
4
5
6
100
200
300
400
500
600
10
20
30
40
50
60
70
80
90
0
1
2
3
4
5
6
0
20
40
 0
1
2
3
4
5
6
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0
1
2
3
4
5
6
0
20
40
 
 
0
1
2
3
4
5
6
100
200
300
400
500
600
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Tempo [BPM]
Tempo [BPM]
Lag [sec]
Time [sec]
Time [sec]
Time [sec]
(c)
(b)
(a)
FIGURE 27.11
Various tempogram representations for the audio excerpt from Figure 27.9. (a) Fourier tempogram. (b)
Autocorrelation (with lag axis). (c) Autocorrelation tempogram (with BPM axis).
T F : Z ×  →R≥0 by
T F(t, τ) = |F(t, τ/60)|.
(27.10)
In other words, an entry T F(t, τ) of the tempogram is obtained by locally comparing the novelty
curve  in a neighborhood of t with a sinusoidal template that represents the tempo τ. As an example,
Figure 27.11a shows the tempogram T F of the Shostakovich excerpt, where T F reveals a slightly
increasing tempo over time starting with roughly τ = 225 BPM. Also, the second tempo harmonic
starting at τ = 450 BPM is clearly visible in T F. Actually, since the novelty curve  locally behaves
like a track of positive clicks, it is not hard to see that Fourier analysis responds to harmonics but not to
subharmonics.
As second periodicity estimation method, we now discuss an autocorrelation-based approach to
estimate local periodicities. To this end, one again ﬁxes a window function W : Z →R centered at
t = 0 with support [−N : N], N ∈N. Then, for a lag parameter ℓ∈[0 : N] and time paramter t ∈Z,
the (real-valued) autocorrelation coefﬁcient A(t, ℓ) is deﬁned by
A(t, ℓ) =

n∈Z (n)W(n −t)(n + ℓ) · W(n −t + ℓ)
2N + 1 −ℓ
.
(27.11)
In other words, an entry A(t, ℓ) is obtained by locally comparing the novelty curve  in a neighbor-
hood of t with a time-shifted copy (shifted by ℓframes) of itself. Here, in the unbiased version of the
autocorrelation, the denominator 2N + 1 −ℓis used to balance out the degree of overlap between the
windowed novelty curve and its time-shifted version. In the next step, the lag parameter is converted into

4.27.3 Tempo and Beat
731
a tempo parameter. To this end, one needs to know the sampling rate of the original signal. Supposing
that each time parameter t ∈Z corresponds to r seconds, then the lag ℓcorresponds to the tempo
τ = 60/(r · ℓ) BPM. From this, one obtains the autocorrelation tempogram T A by
T A(t, τ) = A(t, ℓ)
(27.12)
for each tempo τ = 60/(r · ℓ), ℓ∈[1 : N]. Finally, using standard resampling and interpolation
techniques applied to the tempo domain, one can derive an autocorrelation tempogram T A : Z ×  →
R≥0 that is deﬁned on the same tempo set  as the Fourier tempogram T F. The local autocorrelation A
as well as the autocorrelation tempogram T A for our Shostakovich example are shown in Figure 27.11b
and Figure 27.11c, respectively. Similar to the Fourier tempogram, the autocorrelation tempogram
reveals the dominating tempo at τ = 225 BPM, which corresponds to the quarter note level. However,
opposed to T F, the most dominant tempo revealed by T A is at τ = 75 BPM, which corresponds to the
tempo on the measure level and is the third subharmonics of τ = 225 (here, note that the Waltz has
a 3/4 m). Actually the autocorrelation-based method responds to subharmonics and tends to suppress
harmonics. Therefore, the Fourier tempogram and autocorrelation tempogram yield different types of
tempo information and ideally complement each other.
Assuming a more or less steady tempo, most tempo estimation approaches determine only one global
tempo value for the entire recording. For example, such a value may be obtained by averaging the tempo
values (e.g., using a median ﬁlter) obtained from a framewise periodicity analysis [41]. Dealing with
musicwithsigniﬁcanttempochanges,thetaskoflocaltempoestimation(foreachpointintime)becomes
a harder and even ill-posed problem, see also Figure 27.13 for a complex example. Having computed
a tempogram, the framewise maximum yields a good indicator of the locally dominating tempo—
however, one often has to struggle with confusions of tempo harmonics and subharmonics. Here,
tempo estimation can be improved by a combined usage of Fourier and autocorrelation tempograms.
Furthermore, instead of simply taking the framewise maximum, global optimization techniques based
on dynamic programming have been suggested to obtain smooth tempo trajectories [42,43].
The above mentioned various pulse levels can be seen in analogy to the existence of harmonics in the
pitch context, see Section 4.27.2.2. Inspired by the concept of chroma features (Section 4.27.2.4), the
concept of cyclic tempograms has been introduced in [44]. Here, the idea is to form tempo equivalence
classes by identifying tempi that differ by a power of two. More precisely, we say that two tempi τ1
and τ2 are octave equivalent, if they are related by τ1 = 2kτ2 for some k ∈Z. Then, for a given
tempo parameter τ, the resulting tempo equivalence class is denoted by [τ]. For example, for τ = 120
one has [τ] = {. . . , 30, 60, 120, 240, 480, . . .}. Now, given a tempogram representation T , the cyclic
tempogram is deﬁned by
C(t, [τ]) :=

λ∈[τ]
T (t, λ).
(27.13)
Note that the tempo equivalence classes topologically correspond to a circle. Fixing a reference tempo
ρ (e.g., ρ = 60 BPM), the cyclic tempogram can be represented by a mapping Cρ : R × R>0 →R≥0
deﬁned by
Cρ(t, s) := C(t, [s · ρ]),
(27.14)
for t ∈R and s ∈R>0. Note that Cρ(t, s) = Cρ(t, 2ks) for k ∈Z and Cρ is completely determined
by its values s ∈[1, 2). As illustration, Figure 27.12 shows various tempograms for a click track with

732
CHAPTER 27 Music Signal Processing
0
5
10
15
20
25
30
0
5
0
5
10
15
20
25
30
30
60
120
240
480
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0
5
10
15
20
25
30
10
15
20
25
30
30
60
120
240
480
0
0.2
0.4
0.6
0.8
1
0
5
10
15
20
25
30
1
1.1
1.21
1.33
1.5
1.65
1.81
2
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0
5
1
1.1
1.21
1.33
1.5
1.65
1.81
2
0
0.5
1
1.5
2
2.5
(a)
(b)
(c)
(d)
(e)
Tempo [BPM]
Parameter s
Tempo [BPM]
Parameter s
Time [sec]
Time [sec]
FIGURE 27.12
(a) Novelty curve of click track of increasing tempo (110–130 BPM). (b) Fourier tempogram (showing har-
monics). (c) Cyclic tempogram C60 obtained from (b). (d) Autocorrelation tempogram (showing subharmon-
ics). (e) Cyclic tempogram C60 obtained from (d).
a tempo increasing from τ = 110 to τ = 130 BPM. Figure 27.12b shows a Fourier tempogram with
harmonics and Figure 27.12c the resulting cyclic tempogram. In the pitch context, given a reference
frequency f, the frequency 3 f is an octave plus a ﬁfth higher, and 3 f can be regarded as the dominant
to the tonic f. In analogy to the pitch context, the tempo class [3τ] (corresponding to the third harmonic
3τ) may be coined as tempo dominant. In Figure 27.12c, the tempo dominant is visible as the increasing
line in the middle. Similarly, Figure 27.12d shows an autocorrelation tempogram with subharmonics
and Figure 27.12e the resulting cyclic tempogram. Here, the tempo class [τ/3] (corresponding to the
third subharmonic τ/3) may be coined as tempo subdominant, see the increasing line in the middle
of Figure 27.12e. The cyclic tempo features constitute a robust mid-level representation that reveals
local tempo characteristics of music signals while being invariant to changes in the pulse level. Being
the tempo-based counterpart of the harmony-based chromagrams, cyclic tempograms are suitable for
music analysis and retrieval tasks. For further details, we refer to [44].

4.27.3 Tempo and Beat
733
26
29
33
0
2
4
6
8
10
12
14
16
18
100
200
300
400
500
600
0
0.02
0.04
0.06
0.08
0.1
0
2
4
6
8
10
12
14
16
18
0
2
4
6
8
10
12
14
16
18
0
0.5
0
2
4
6
8
10
12
14
16
18
0
0.5
1
Tempo[BPM]
Time[sec]
(a)
(b)
(c)
(d)
(e)
FIGURE 27.13
Excerpt of an orchestral version conducted by Ormandy of Brahms’s Hungarian Dance No. 5. The score shows
measures 26–38 in a piano reduced version. (a) Score (piano reduced version). (b) Fourier tempogram. (c)
Ground-truth pulses. (d) Novelty curve . (e) PLP curve.
4.27.3.3 Beat tracking
The task of beat tracking can be seen as an extension of tempo estimation in the sense that it additionally
considers the phase of the pulses, see Figure 27.7b. We start by describing a robust state-of-the-art beat
tracking procedure [45], which assumes a roughly constant tempo throughout the music recording. The
input of the algorithm consists of a novelty curve  : [0 : T −2] →R as introduced in Section 4.27.3.1

734
CHAPTER 27 Music Signal Processing
as well as an estimate ˆτ of the global (average) tempo, which also determines the pulse level to be
considered. From ˆτ and the sampling rate used for the novelty curve, one can derive an estimate ˆρ ∈Z
for the average beat period (given in samples). Assuming a roughly constant tempo, the difference δ of
two neighboring beats should be close to ˆρ. To measure the distance between δ and ˆρ, a neighborhood
function N ˆρ : N →R, N ˆρ(δ) = −

log2 (δ/ ˆρ)
2, is introduced. This function takes the maximum
value of 0 for δ = ˆρ and is symmetric on a log-time axis. Now, the task is to estimate a sequence B =
(b1, b2, . . . , bL), for some suitable L ∈N, of monotonically increasing beat positions bℓ∈[0 : T −2]
satisfying two conditions. On the one hand, the value (bℓ) should be large for all ℓ∈[1 : L] (assuming
that novelty peaks are likely note onsets and therefore likely beat locations), and, on the other hand, the
beat intervals δ = bℓ−bℓ−1 should be close to ˆρ. To this end, one deﬁnes the score S(B) of a beat
sequence B = (b1, b2, . . . , bL) by
S(B) =
L

ℓ=1
(bℓ) + α
L

ℓ=2
N ˆρ(bℓ−bℓ−1),
(27.15)
where the weight α ∈R balances out the two conditions. Finally, the beat sequence maximizing S yields
the solution of the beat tracking problem. The score-maximizing beat sequence can be obtained by a
straightforward dynamic programming (DP) approach, see [45] for details.
Such beat tracking procedures work well for music with a strong and steady beat, which is often the
case for modern pop and rock music. However, the above algorithm heavily depends on the assumption
thattherearenoabrupttempochangeswithinthepiece.Thus,itwouldnotworkwellfortheaudioexcerpt
of Brahm’s Hungarian Dance No. 5 as shown in Figure 27.13, which contains a number of abrupt changes
in tempo. Additionally, the novelty curve is rather noisy due to weak note onsets played by strings. To
cope with such complex situations, a robust procedure for extracting musically meaningful local pulse
information has been described in [33]. Here, the idea is to construct a mid-level representation that
explains the local periodic nature of a given (possibly very noisy) onset representation. More precisely,
one ﬁrst computes the complex-valued spectrogram (27.9) of the novelty curve and then derives from
the spectrogram for each time position a windowed sinusoidal kernel (i.e., a windowed sine function
with a window as used in the STFT) that best captures the local peak structure of the novelty curve. Since
these kernels localize well in time, even continuous tempo variations and local changes of the pulse
level can be handled. Now, instead of looking at the windowed kernels individually, the crucial idea is
to employ an overlap-add technique by accumulating all local kernels over time. As a result, one obtains
a single curve that can be regarded as a local periodicity enhancement of the original novelty curve.
Revealing predominant local pulse (PLP) information, this curve is referred to as PLP curve. Continuing
our Brahms example, Figure 27.13d shows the original novelty curve, whereas Figure 27.13e shows
the resulting PLP curve. A manual inspection reveals that the excerpt starts with a tempo of 180 BPM
(measures 26–28, seconds 0–4), then abruptly changes to 280 BPM (measures 29–32, seconds 4–6),
and continues with 150 BPM (measures 33–38, seconds 6–18). The peaks of the resulting PLP curve
correctly indicate the musically relevant eighth note pulse positions in the novelty curve despite of poor
note onset information and the two abrupt tempo changes. For further details, we refer to [33].
The extraction of beat locations from highly expressive performances still constitutes a challenging
task with many open problems. For such music, one often has signiﬁcant local tempo ﬂuctuation

4.27.3 Tempo and Beat
735
caused by the artistic freedom a musician takes, so that the model assumption of local periodicity is
strongly violated. In practice beat tracking is further complicated by the fact that there may be beats
with no explicit note events going along with them. Here, a human may still perceive a steady beat
by subconsciously interpolating the missing onsets. This is a hard task for a machine, in particular in
passages of varying tempo where interpolation is not straightforward. Furthermore, auxiliary note onsets
can cause difﬁculty or ambiguity in deﬁning a speciﬁc physical beat time. In music such as the romantic
piano pieces by Chopin, the main melody is often embellished by ornamented notes such as trills, grace
notes, or arpeggios. Also, for the sake of expressiveness, the notes of a chord need not be played at the
same time, but slightly displaced in time. This renders a precise deﬁnition of a physical beat position
difﬁcult. Such highly expressive music also reveals the limits of purely onset-oriented tempo and beat
tracking procedures. For a more detailed discussion of such phenomena, we refer to [46,47].
4.27.3.4 Applications
The automated extraction of onset, beat, and tempo information is one of the central tasks in music
signal processing and constitutes a key element for a number of music analysis and retrieval applications.
Tempo and beat are not only expressive descriptors per se often characterizing the style of a piece of
music, but also allow for segmenting an audio signal in a natural and musically meaningful way. We
ﬁrst demonstrate how such a segmentation can help to improve general feature extraction and then give
pointers to further applications.
In music analysis, one crucial step consists in transforming the given audio signal into a suitable
feature representation that captures certain musical properties while being invariant to other aspects.
For example, chroma features have turned out to be a powerful audio representation for capturing the
harmonic content of a music recording, see Section 4.27.2.4. Since most musical properties vary over
time, the given audio signal is ﬁrst split up into segments or frames, which are then further processed
individually. Here, the underlying assumption is that the signal stays (approximately) stationary within
each segment with regard to the property to be captured. In practice, as is the case for the short-time
Fourier transform (27.1), a predeﬁned window of ﬁxed length is used for the segmentation, where the
length is determined empirically and optimized for the speciﬁc application in mind. Using a ﬁxed-
length segmentation, however, may lead to a violation of the homogeneity assumption: the boundaries
of the resulting segments often do not coincide with the positions where the signal’s changes occur.
To illustrate this problem, Figure 27.14a shows a chroma representation for an audio excerpt with four
subsequent chords using a ﬁxed window length. Note that the third frame comprises a chord change
leading to a rather “noisy” chroma feature where the chroma bands contain energy from two different
chords. To attenuate the problem, one often decreases the window size at the cost of an increased feature
rate and a poorer frequency resolution. As an alternative to ﬁxed-length segmentation, one can employ
a musically more meaningful adaptive segmentation strategy, where window boundaries are induced by
previously extracted onset and beat positions. Since musical changes typically occur at onset positions,
this often leads to an increased homogeneity within the adaptively determined frames and a signiﬁcant
improvement in the resulting feature representation, see Figure 27.14b.
Adaptive segmentation techniques have been applied to a wide variety of tasks in the ﬁeld of music
information retrieval. In particular, beat-synchronized audio features where segments are determined

736
CHAPTER 27 Music Signal Processing
0
2
4
6
8
−0.5
0
0.5
0
2
4
6
8
−0.5
0
0.5
0
2
4
6
8
C 
C#
D 
D#
E 
F 
F#
G 
G#
A 
A#
B 
0
2
4
6
8
C 
C#
D 
D#
E 
F 
F#
G 
G#
A 
A#
B 
Time [sec]
Time [sec]
(a)
(b)
FIGURE 27.14
Score, audio recording, and chroma representation of a sequence of four chords. (a) Segmentation using a
window of ﬁxed length. (b) Adaptive segmentation resulting in beat-synchronized features.
by two consecutive beat positions have proven to be useful for applications including chord recognition
[48], cover song identiﬁcation [49], audio structure analysis [21], instrumentation analysis [50], and
performance analysis [51], just to name a few. As one major advantage of adaptive segmentation
procedures, beat-synchronized features allow for compensating tempo difference between musically
related music recordings, which alleviates the requirement of using cost-intensive alignment procedures
[49]. Also knowing the beat positions allows for converting a physical time axis (given in seconds) into
a musically meaningful time axis (given in beats or measures), which has huge beneﬁts for presenting
and comparing music analysis results [52].
Finally, we want to mention that the extraction of onset, beat, and tempo information is of fundamental
importance for the determination of higher-level musical structures such as rhythm and meter [31,38].
Generally, the term rhythm is used to refer to a temporal patterning of event durations, which are
determined by a regular succession of strong and weak stimuli [53]. Furthermore, the perception of
rhythmic patterns also depends on other cues such as the dynamics and timbre of the involved sound
events. Such repeating patterns of accents form characteristic pulse groups, which determine the meter
of a piece of music. Here, each group typically starts with an accented beat and consists of all pulses until
the next accent. In this sense, the term meter is often used synonymously with the term time signature,
which speciﬁes the beat structure of a musical measure or bar. It expresses a regular pattern of beat
stresses continuing through a piece thus deﬁning a hierarchical grid of beats at various time scales.
Rhythm and tempo are often sufﬁcient for characterizing the style of a piece of music. This particularly
holds for dance music, where, e.g., a waltz or tango can be instantly recognized from the underlying
rhythmic pattern.

4.27.4 Timbre and Instrumentation
737
4.27.4 Timbre and instrumentation
Timbre, also called tone quality, refers to a perceptual attribute of sounds that mainly characterizes the
source identity. In music, timbre information allows us to tell apart the sounds produced by the oboe
and the violin, for example, even when the pitch and loudness of the sounds are identical. Additionally,
many instruments allow for controlling the tone quality to a certain degree, making timbre a dimension
of musical expression along with pitch and dynamics.
Characteristic to pitched musical sounds is that their waveforms tend to be locally periodic or nearly
periodic. This is because the sounds are intended to evoke a distinct percept of pitch, and pitch perception
is closely related to the time-domain periodicity of sounds. The Fourier theorem states that any periodic
signal can be represented with a series of frequency components located at integer multiples of the
inverse of the period. This leads to the following signal model for pitched musical sounds:
x(n) =
H

h=1
ah cos (2π f0/Fs + φh) + r(n),
(27.16)
where f0 denotes the fundamental frequency of the sound (practically equivalent to the perceived pitch
in the case of musical sounds), Fs is the sampling rate, and ah and φh are the amplitude and phase of
harmonic partial h. The last term, r(n), denotes a residual signal, representing all the elements that are
not covered by the harmonic part. Some instruments—such as the transverse ﬂute or the violin—produce
sounds that include a considerable wide-band noise component r(n) that affects the perceived timbre
along with the harmonic part.
Varying the relative amplitudes and the temporal evolution of the harmonic partials allows for control-
ling the timbre without changing the pitch or the overall loudness of the sound. For string instruments,
the higher-order partials are not exactly integer multiples of f0, but are slightly shifted upwards in
frequency due to the stiffness of real strings. For mallet percussion instruments (e.g., vibraphone and
marimba), only a small subset of the partials are present and some weak high-frequency partials are not
harmonically related to the fundamental frequency.
The term polyphonic timbre refers to the overall timbral mixture of a music signal, the “global
sound” of a piece of music [54]. Human listeners have a remarkable ability to focus on the part played
by an individual instrument in polyphonic music. However, often music is not treated in such an
analytic manner, but a listener considers the music signal as a coherent whole with its polyphonic timbre
contributed by all the instruments together. In computational systems, acoustic features describing the
polyphonic timbre are effective for tasks such as genre identiﬁcation [55] and automatic tagging of
audio with semantic descriptors [56].
Timbre is a multidimensional concept, being determined by several underlying acoustic factors.
Schouten [57] describes timbre as having ﬁve major acoustic parameters: (i) the range between tonal
and noise-like character, (ii) the spectral envelope, (iii) the time envelope, (iv) the changes of spectral
envelope and fundamental frequency, and (v) the onset of the sound differing notably from the sustained
vibration. This list provides a useful overview of the different facets of timbre (for completeness, see
also work on sound texture [58]).
Moreobjectively,theperceptualdimensionsoftimbrehavebeenstudiedbasedondissimilarityratings
of human listeners for pairs of musical instrument sounds (see [59] for a review). Using a technique

738
CHAPTER 27 Music Signal Processing
called multidimensional scaling, the dissimilarity ratings can be projected into a low-dimensional space
where the distances between the sounds match the dissimilarity ratings as closely as possible. Although
a rotational ambiguity remains, this provides an effective means of visualizing the perceptual space of
musical timbre, and acoustic features can be sought that explain the locations of the sounds in this space.
The features found in different studies include spectral centroid, attack time, spectral irregularity, and
spectral ﬂux (see [59] for details). In addition to studies on isolated musical instruments sounds, a few
experiments have been carried out to identify the perceptual dimensions of polyphonic timbre [54].
In the following, we will discuss some of the acoustic features that can be used to describe timbre
in music processing and retrieval systems (Sections 4.27.4.1 and 4.27.4.2). Then some applications are
discussed, in particular musical instrument recognition in polyphonic recordings (Section 4.27.4.3) and
music classiﬁcation (Section 4.27.4.4).
4.27.4.1 Spectral envelope
The acoustic features found in the above-described multidimensional scaling experiments bring insight
into timbre perception, but they are generally too low-dimensional to enable robust musical instrument
identiﬁcation. In signal processing applications, timbre is typically described using a parametric model
for the spectral envelope of sounds. This stems from speech recognition and is not completely satisfac-
tory in music processing as will be seen, but works well as a ﬁrst approximation of timbre. Figure 27.15
illustrates the time-varying spectral envelopes of two example musical tones. Here the spectral infor-
mation is measured at a critical-band resolution in order to discard pitch information that is present
in the ﬁne structure of the spectrum. The different facets of timbre listed by Schouten are reasonably
well captured by this representation, except for the tonal versus noiselike character. The latter can be
addressed by decomposing a music signal into its sinusoidal and stochastic parts and then estimating
the spectral envelope of each part separately, as has been done in [60], for example.
Among the most popular ways of describing the spectral envelope are linear-prediction coefﬁ-
cients and mel-frequency cepstral coefﬁcients (MFCCs), originally used for speech recognition [61].
1
5
10
15
20
25
30
35
0.25
0.5
0.75
1
Frequency [CB]
Time [sec]
Magnitude [dB]
1
5
10
15
20
25
30
35
0.25
0.5
0.75
1
Frequency [CB]
Time [sec]
Magnitude [dB]
FIGURE 27.15
Time-varying spectral envelopes of 260-Hz sounds of the guitar (left) and the violin (right). Note that fre-
quency is measured in critical-band (CB) units.

4.27.4 Timbre and Instrumentation
739
31
63
125
250
500
1000
2000
4000
8000
16000
Frequency (Hz)
0
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27
3rd octaves
1
2
3
4
5
6 7 8 9 10 12 14 16 18 20 22 2426283032343638
Mel frequency / 100
1
3
5
7
9
11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41
ERB critical bands
1
2
3
4
5 6 7 8 9 10111213141516171819202122 23 24
Bark critical bands
24
36
48
60
72
84
96
108
120
132
MIDI
FIGURE 27.16
Perceptually-motivated frequency scales and their relationship to Hertz units. ERB and Bark scales are
critical-band scales that model the frequency resolution of the inner ear. Mel-frequency scale stems from
pitch perception experiments. See [62] for details on different scales.
Especially the latter, MFCCs, are widely used in music processing applications too. MFCCs are obtained
by calculating log-powers of the signal within subbands that are uniformly distributed on the mel-
frequency scale, and then by applying a discrete cosine transform (DCT) to the vector of log-powers.
Using a perceptually motivated frequency scale and the log-magnitude scale leads to the important
property that a small (large) numerical change in the MFCC coefﬁcients corresponds to a small (large)
perceived change. Figure 27.16 illustrates the mel-frequency scale along with other scales that have
been proposed to model the frequency resolution of the human auditory system. The last step in MFCC
calculation, DCT, allows for easy control of the spectral resolution and decorrelates the features, making
them well-suited for statistical modeling.
The MFCC features are often viewed in the light of source-ﬁlter models. The human vocal apparatus
and many musical instruments can be viewed as a coupling of two acoustic entities, a vibrating object,
such as the vocal chords or a guitar string (“source”), and a resonance structure of the vocal tract or the
instrument body (“ﬁlter”) that colors the produced sound. In a single analysis frame, the source-ﬁlter
model can be written as
|X(k)| = S(k)B(k),
(27.17)
where |X(k)| is the kth coefﬁcient of magnitude spectrum of the emitted sound, S(k) is the magnitude
spectrum at the source, and B(k) represents the frequency response of the resonance structure. A key
observation here is that if we take a logarithm on both sides, the model becomes linear ( log |X(k)| =
log S(k)+log B(k)) and thereby more tractable for further analysis. In particular, if log |X(k)| is cosine
transformed and the higher transform coefﬁcients are discarded, the resulting feature vector retains
the ﬁlter part (assuming that B(k) varies slowly as a function of frequency). As a result, since pitch
information is encoded by the spectral ﬁne structure that is mainly present in the higher transform
coefﬁcients, the features become more or less invariant to pitch information present in the source S(k).
Making the spectral envelope features invariant to pitch is highly desirable in tasks such as speech
recognition and musical instrument recognition.

740
CHAPTER 27 Music Signal Processing
The source part S(k) in Eq. (27.17) does not only encode the pitch, but also includes timbral infor-
mation. In music, a wide variety of sound production mechanisms are employed. Depending on the
instrument, the source part S(k) may represent a vibrating string, an air column, or a vibrating bar or
membrane, all of which have a certain acoustic characteristics and provide valuable information about
the instrument identity. Reed instruments, such as the saxophone or the clarinet, are an interesting case
because they involve a vibrating reed that drives the vibration of the air column within the instrument. It
is a matter of interpretation and modeling convenience whether the air column is included in the source
or the ﬁlter part. Here we include it in the source part because its frequency response is sharply tuned
and determines the F0.
It is interesting to note that the regularities in the source S(k) are not best described in terms of fre-
quency, but in terms of harmonic index (denoted by h in Eq. (27.16)). For example, the clarinet involves
an air column vibrating within a half-closed tube, and the resulting sound is therefore characterized by
the odd harmonics being stronger than the even ones. For the piano, every 8th partial is weaker because
the string is usually struck at a point 1/8 along its length. The sound of the vibraphone, in turn, mainly
consists of the ﬁrst and the fourth harmonic and some energy around the tenth partial (the interested
reader is referred to [63] for understanding the acoustics of these instruments). Figure 27.17 illustrates
this situation. MFCCs and other models that describe the properties of an instrument as a function of
0
1
2
3
4
5
−60
−40
−20
0
Frequency [kHz]
Clarinet 290 Hz
Magnitude [dB]
2
4
6
0
1
2
3
−60
−40
−20
0
Frequency [kHz]
Piano 98 Hz
Magnitude [dB]
8
16
0
1
2
3
4
5
6
7
8
−60
−40
−20
0
Frequency [kHz]
Vibraphone 260 Hz
Magnitude [dB]
1
4
10.3
FIGURE 27.17
Example sounds where the source excitation informs about the instrument identity. In the clarinet, even-
numbered harmonics are relatively weak. In the piano, every 8th partial is missing. In the vibraphone, mainly
the ﬁrst and the fourth harmonic are present, the remaining partials being in non-integer relationships to
the fundamental frequency.

4.27.4 Timbre and Instrumentation
741
frequency smear out this information. Instead, a structured model is needed where the spectral informa-
tion is described both as a function of frequency and as a function of harmonic index. In practice this
can be done by parametrizing both the source and the ﬁlter part in Eq. (27.17) and learning the model
parameters based on example sounds [64].
4.27.4.2 Temporal evolution
In our discussion so far, we have ignored the temporal aspect of timbre: the evolution of acoustic features
in time. To better understand the importance of the temporal dimension, consider the musical instrument
recognition system of Barbedo and Tzanetakis [65] that is based on individual harmonic partials and
therefore does not use the spectral envelope information at all. The starting point of their method is
to employ a pitch and onset detection step in order to identify “clean” partials that are not overlapped
by other, concurrent sounds. The amplitude and frequency evolution of each partial is then described
separately by a set of acoustic features that include the ﬁrst few moments of the amplitude trajectory
(temporal centroid, spread, skewness, and kurtosis), crest factor (ratio of the maximum amplitude to
the root-mean-square value), AM modulation frequency and magnitude, onset duration, and slope of
amplitude decay (see [65] for details). Similar features are extracted to describe the frequency trajectory
of the partial. Remarkably, these features sufﬁce to perform quite robust instrument identiﬁcation in
mixturesignals.Theauthorsemployedsupportvectormachineclassiﬁerstorecognizeindividualpartials
as well as a voting mechanism to integrate information across partials.
The most widely used approach to describing the temporal evolution of sounds is to append the ﬁrst
and second time derivatives of features into the feature vector. Given feature vectors z(t) in analysis
frames t = 1, 2, . . . , T , their derivatives can be approximated by
z(t) =
M
m=−Mmz(t + m)
M
m=−Mm2
,
(27.18)
where M is usually 1 or 2. The second derivative is computed by substituting z(t) in place of z(t)
above.
The main limitation of the “delta” features given by Eq. (27.18) is that they cannot represent temporal
structure spanning over several time frames, such as the attack-sustain-release structure characteristic to
many musical sounds. Hidden Markov models (HMMs) are a standard tool for representing sequential
structures—in this context time series of acoustic features [66]. HMMs are ﬁnite state models where
eachstategeneratesacousticfeaturesaccordingtostate-conditionalobservationdensities,andsequential
structures are produced by switching between the states according to a transition probability matrix.
HMMs are particularly useful in that they encode temporal structure and yet allow for duration variation.
Musical sounds generally vary more slowly than speech signals. As a result, musical sounds are not
always accurately represented using a model where each state corresponds to a certain spectral energy
distribution and time-varying sounds are modeled by switching between the states. The left panel of
Figure 27.18 illustrates this with a simple example where the energy envelopes of a piano sound at
three sub-bands are represented using a four-state “staircase” model. Obviously, the model ﬁts the data
poorly and is inaccurate even if more states are added. Contrary to this, most musical sounds can be

742
CHAPTER 27 Music Signal Processing
0
0.3
0.7
1
Time [sec]
Sub−band energy
1 22
3
4
1
Sub−band energy
Time (state occurences shown)
FIGURE 27.18
Black curves show the original, three-dimensional data that represent the energy of a piano tone at three
sub-bands (the bands are displaced vertically for clarity). The red curves on the left illustrate an optimal
four-state “staircase” model ﬁtted to the data. The red curves on the right illustrate a four-state interpolating
model (state occurrence times are indicated with vertical lines). (For interpretation of the references to color
in this ﬁgure legend, the reader is referred to the web version of this book.)
represented accurately by interpolating between spectra that are taken from appropriate positions of the
input signal. Several examples of this can be found in sound synthesis. The right panel of Figure 27.18
illustrates an interpolating model with three states for the same data. For further details on estimating
the parameters of an interpolating state model, see [67].
4.27.4.3 Musical instrument recognition
Systems developed for musical instrument recognition are typically based on the supervised classiﬁca-
tion paradigm. Here, training data is used to learn models for the distribution of acoustic features within
each instrument class, and the models are then used to classify previously unseen samples (see e.g.,
[68] for a review). A number of different classiﬁcation techniques have been employed for this task,
including support vector machines, decision trees, and Bayesian classiﬁers based on Gaussian mixture
models or hidden Markov models.
One of the factors that make instrument recognition hard in real-world scenarios is that most music
is polyphonic, consisting of several instruments playing at the same time. Often the target instrument

4.27.4 Timbre and Instrumentation
743
to be recognized corresponds to only a fraction of the total energy of the mixture, and moreover, the
frequency partials of the target instrument tend to be overlapped by those of other instruments because of
harmonic pitch relationships. The interference caused by the other instruments is highly non-stationary
and unpredictable as the identities of the other instruments are usually not known either.
There are various approaches to deal with polyphonic music signals. Firstly, one can select acoustic
features and classiﬁcation techniques that are maximally robust to the interference of other sources.
For example, Kitahara et al. [69] used linear discriminant analysis to identify such features. Secondly,
one can try to separate the signals of different instruments from the mixture and then classify each
signal separately. This approach has been widely used, and there are a number of different mechanisms
to perform the source separation. Obviously, sound separation is a hard problem in itself and is often
the bottle-neck of these systems—although perfect separation is not needed for classiﬁcation purposes,
especially if the class models are trained using similar data with separation artefacts [70]. Thirdly, one
can perform source separation and recognition jointly, for example based on statistical inference within
parametric signal models that have pre-trained parameter combinations for each instrument [71], or by
employing sparse representations where a mixture signal is represented as a weighted sum of “atoms”
with pitch and instrument labels [72].
The instrument recognition system of Barbedo and Tzanetakis [65] as discussed in Section 4.27.4.2
represents a class of methods that try to segment a mixture signal in the time-frequency domain in order to
identify regions that represent “clean” glimpses of individual instruments, and then use these regions for
the recognition. Missing feature theory provides a general theoretical framework for recognizing sound
sources based on partial information [73]. The idea is to estimate a mask that indicates time-frequency
regions that are dominated by energy from interfering sounds and should therefore be excluded from the
classiﬁcation process. Obviously, estimating the mask automatically from a mixture signal is difﬁcult:
the problem is closely related to sound separation.
4.27.4.4 Music classiﬁcation and similarity
Features describing the polyphonic timbre (the “global sound” of a song) can be extracted directly
from the mixture signal, without separating individual instruments. The features are often very similar
to those used for isolated sounds: in particular MFCCs and their deltas are widely used to describe
the instrumentation of a polyphonic music signal. Supervised classiﬁcation techniques can then be
employed for tasks such as genre recognition [55] and music tagging [56]. A simple genre classiﬁcation
system, for example, can be constructed by using a Gaussian mixture model (GMM) to describe the
distribution of the features in songs belonging to a given genre g (it should be noted that low-level
timbre features alone may not sufﬁce to discriminate between all genres). An unknown song can then
be classiﬁed by extracting a feature sequence Z = (z(1), z(2), . . . , z(T )) and selecting the genre ˆg that
maximizes
ˆg = arg max
g
p(g)p(Z|θg),
(27.19)
where θg denotes the GMM parameters and p(g) prior probability for genre g. Many of the existing
music classiﬁcation systems replace the feature sequence Z with a single feature vector that consists of
means and variances of the framewise features over each song (see the systems submitted to the MIREX
evaluation [74]).

744
CHAPTER 27 Music Signal Processing
In music retrieval tasks, pre-deﬁned categories such as genre are often not sufﬁciently speciﬁc.
Query-by-example provides an alternative retrieval paradigm: the user provides an example music clip
and requests for similar items. At the core of such a system is a measure of similarity between two
music signals. The concept of music similarity is well-deﬁned only when the aspect of similarity is
speciﬁed—let it be instrumentation, harmony, rhythm, or something else. This is closely related to the
selection of acoustic features. For example, instrumentation can be characterized by MFCCs, pitch
content by chroma features, or rhythmic content by ﬂuctuation patterns [75].
Having speciﬁed the features, the similarity of two audio signals is typically calculated based on the
statistics of the extracted features. In such approaches, temporal structure in the data is entirely collapsed.
Simple distance measures based on means and covariances of the features include the Mahalanobis
distance, given by
DMah(A, B) = (mA −mB)T −1(mA −mB),
(27.20)
where mA and mB denote the means of the feature sequences representing audio clips A and B,
respectively, and  is a covariance matrix calculated over all the features.
More sophisticated and accurate distance measures are obtained by learning a parametric model for
the distribution of features in the two songs and then by calculating a distance measure between the two
models. Computationally efﬁcient approximations exist for calculating the Kullback-Leibler divergence
between two GMM models, for example, and these are well-suited for instrumentation-oriented query-
by-example tasks [76].
4.27.5 Melody and vocals
A melody consists of a series of single notes which are arranged in musically expressive succession.
The melody often represents the principal part in a harmonized (polyphonic) piece of music [77] and,
in particular for popular music, tends to be the part that a listener most vividly remembers. Therefore,
transcribing the melody line automatically from a music recording constitutes an important task with
many useful applications. In a query-by-humming scenario, for example, the user may specify a query
by singing or humming a melody to retrieve a song from a music collection. Here, one strategy is to
compare this query input with the melodies automatically extracted from the polyphonic audio material
of the database. A second application are computer games where a user’s singing is compared against
the performance of the original recording.
Figure27.19illustratesthepitchtrackofanexamplemelody,alongwiththecommonmusicalnotation
for the excerpt. As can be seen, the performed pitch track differs considerably from the idealized written
music exhibiting vibrato, glissandi between notes, as well as many expressive nuances that are essential
for the music but not present in the written notation. Consequently, deriving a discrete notation from
a singing performance requires heavy use of musical knowledge in order to resolve ambiguities when
performing note segmentation as well as pitch and time quantization for the performance.
A step further from melody transcription is to separate the signal corresponding to the melody from
the accompanying instruments. Here we consider only the case where the melody is performed by

4.27.5 Melody and Vocals
745
ycomes to me
When I find my self
 in times of trouble Mother  
Time [sec]
Pitch [Hz]
131
165
208
262
330
415
523
629
0
1
2
3
4
5
6
Time [sec]
Frequency [Hz]
128
181
255
361
511
722
1022
1445
2044
2890
4087
5780
0
1
2
3
4
5
6
Mar
(b)
(c)
(a)
FIGURE 27.19
Melody line from “Let It Be” by The Beatles. (a) Music notation and lyrics of an excerpt. (b) Time-pitch
representation estimated from the audio ﬁle using [78], along with white horizontal lines indicating the
notes in the score. (c) Time-frequency spectrogram of the audio signal.

746
CHAPTER 27 Music Signal Processing
singing. Lead vocals carry a lot of meaningful information besides the pitch contour, including lyrics,
singer identity, and musical and emotional expression. Analysis of these aspects becomes signiﬁcantly
easier if the vocals signal can be separated from the polyphonic audio.
In practice, melody and vocals extraction from monaural or stereo music signals becomes possible
only by making some limiting assumptions. Lead vocals in many music types exhibit certain acoustic
and musical characteristics that greatly facilitate their analysis: (i) the pitch range tends to stay between
80 Hz and 1 kHz; (ii) the lead vocals are relatively loud compared to the accompanying instruments;
(iii) vibrato and pitch glides make the vocals stand out from the mixture; (iv) vocal timbre, despite of
being highly varying, is characteristic to the singing voice; (v) in music recordings, the main melody
is usually panned at the center of the stereo ﬁeld; (vi) usually melodic continuity is favoured, meaning
that small jumps in pitch are more likely than large ones when tracking a melody over time. All these
characteristics have been utilized in the existing melody and vocals extraction methods, although not
all of them at the same time.
In the following, melody transcription and vocals separation will be separately discussed in
Sections 4.27.5.1 and 4.27.5.2, respectively. The two tasks are closely related and in fact most vocals
separation systems extract the melody as a preprocessing step or as a by-product. Applications will be
discussed in Section 4.27.5.3.
4.27.5.1 Melody transcription
Automatic transcription of the melody line in polyphonic music has received considerable research
attention during the last ﬁve years. Some of the existing methods track the melody pitch in a continuous
manner [79,80], whereas others extract a sequence of discrete note events with quantized pitch values
and well-deﬁned start and end times [81].
Figure 27.20 shows an overview of a generic melody transcription system. An essential part of
the feature extraction stage is fundamental frequency (F0) analysis: estimating the probabilities or
“saliences” of various candidate F0s (see Eq. (27.16)) to be present in the signal. Some approaches
are based on statistical modeling of the frequency spectrum [80,82], whereas other approaches employ
heuristic algorithms to compute the salience of a fundamental frequency based on its harmonic series
in the observed magnitude spectrum [79,81]. At the next stage, the melody line has to be distinguished
from the notes played by other instruments. This is usually carried out based on some of the assumed
characteristics of vocal melodies as listed above. Tracking the melody over time is usually based on the
music
signal
Feature
extraction
features
Melody
identiﬁcation
and tracking
transcribed
melody
Acoustic
and musical
models
FIGURE 27.20
General overview of a melody transcription system.

4.27.5 Melody and Vocals
747
melodic continuity assumption, but some systems also enforce timbral continuity [79,80,82]. Various
techniques have been used to implement the tracking, including multiple agents [80] and hidden Markov
models [81,82]. Theblocklabeled“Acousticandmusical models”represents pre-trainedinternal models
where information regarding these characteristics is stored.
To discuss melody transcription in more detail, let us consider the method of Ryynänen and Klapuri
[81] as an example. The method analyzes an audio recording and produces a sequence of note events,
each with a discrete pitch value and an onset and offset time. The acoustic features used in this particular
method include information only related to F0 and onset properties; timbral or spatial (stereo) aspects
are not utilized. Figure 27.21 illustrates the acoustic features. The pitch content of the input signal is
analyzed using a method that measures the salience s(t, f0) of various F0 values f0 in analysis frame
t. (Figure 27.21 is produced using an updated algorithm [78]). Comparing the time-F0 representation
in Figure 27.19b with the spectrogram in Figure 27.19c, we can se that harmonic overtones of the
pitched sounds are suppressed and the F0s are more clearly visible. Furthermore, Figure 27.21b shows
a representation that is obtained by calculating frame-to-frame differences of the representation shown
in Figure 27.21a. As the vocal pitch usually varies over time due to vibrato and pitch glides, it stands
out in the differential representation, whereas sounds with stable pitch and amplitude (here piano)
appear only at their onset positions. The third feature employed in the example method is illustrated by
Figure 27.21c. Similar to the novelty function deﬁned in (27.8), the accent a(t) measures the amount of
spectral change from frame t −1 to frame t and is useful for indicating potential note onsets. The accent
feature is based on calculating log-power levels within critical bands (“perceptual spectrum”) in each
analysis frame. The perceptual spectrum in frame t −1 is then subtracted element-wise from that in
0
1
2
3
4
5
6
131
165
208
262
330
415
523
659
Time [sec]
Pitch [Hz]
0
1
2
3
4
5
6
131
165
208
262
330
415
523
659
Time [sec]
Pitch [Hz]
0
1
2
3
4
5
6
Time [sec]
Accent
(b)
(a)
(c)
FIGURE 27.21
Illustration of the acoustic features employed in [81] using the same audio example as in Figure 27.19.
(a) F0 saliences s(t, f0) as a function of time. (b) s(t, f0) obtained by calculating frame-to-frame differences
of the saliences. (c) Accent feature a(t) indicating potential note onsets.

748
CHAPTER 27 Music Signal Processing
frame t, and the resulting positive level differences are summed across bands to obtain a(t). Although
accompanying piano sounds affect the accent function too, it is still meaningful in indicating potential
onset times of the sung notes.
In the next step, HMMs are used as acoustic models to describe the statistics of the three features, F0
salience s(t, f0), delta salience s(t, f0), and accent a(t) within musical notes [81]. The models are
learned from the RWC database which consists of realistic musical recordings with manual annotations
of the melody and other notes [83]. The models consist of a three-state left-to-right HMMs where the
consecutive states can be interpreted to represent the attack, sustain, and release segments of the notes.
In order to separate the melody notes from the polyphonic mixture at the decoding stage, different
models are trained for melody notes, for notes played by other instruments, and for silence or noise.
In addition to the acoustic models, musical context can be utilized to resolve otherwise ambiguous
situations. N-gram models are a common choice for context modeling, both in automatic speech recog-
nition and music transcription. In melody detection, it is natural to model the probability of note pitch
nt at frame t given N −1 preceding note pitches: P(nt|nt−1, . . . , nt−N+1). In the example system
[81], key-dependent bigrams (N = 2) are used. This is implemented by ﬁrst estimating the musical
key and then applying the corresponding bigram model that takes into account the preceding note and
additionally favors the scale notes of the estimated key κ. Figure 27.22 illustrates the bigram proba-
bilities P(nt|nt−1, κ) for the C major/A minor relative key pair. Key-dependent bigrams are helpful in
modeling melodic continuity and resolving ambiguities in the case of slightly out-of-tune sung notes,
for example helping to decide C instead of C♯given a C major key.
The acoustic and musicological models are combined into a network of notes, where each note
is represented by a three-state HMM and the musicological model determines transition probabilities
between notes. The Viterbi algorithm can then be used to ﬁnd the most probable path through the note
models, given a sequence of feature vectors extracted from the input audio signal.
From note 
To note
C4 C#4D4 D#4E4 F4 F#4G4 G#4A4 A#4B4 C5
C4
C#4
D4
D#4
E4
F4
F#4
G4
G#4
A4
A#4
B4
C5
FIGURE 27.22
Key-dependent note transition probabilities used in the example method [81]. The shown example represents
the C major/A minor relative key pair, where all notes without the sharps “#” are scale notes.

4.27.5 Melody and Vocals
749
4.27.5.2 Vocals separation
Vocals separation can be performed, to a certain degree, based on pitch information alone. A commonly
used approach is to select frequency components at integer multiples of the estimated F0 track of the
vocals and to synthesize the voice from these components. This can be implemented using sinusoidal
modeling(seee.g., [84]) or by applying a spectro-temporal mask that selects time-frequency components
corresponding to the overtones of the estimated vocal pitch track (see e.g., [85]).
A limitation of the above approach is that it does not take into account accompanying instruments that
often overlap with the vocals in time and frequency. As a result, the separated sound can be considerably
distortedbytheenergyleakingfromtheotherinstruments.Suchoverlapsareverycommoninpolyphonic
music due to consonant pitch relationships and wide-band percussive sources. In order to deal with the
overlapping elements in the time-frequency domain, one has to estimate the amplitudes of the vocal
partials instead of picking them directly from the mixture spectrum. In practice, this requires modeling
the spectral envelope of the vocals, the accompaniment, or both [82,85].
As an example, let us consider the method for vocals separation suggested by Virtanen et al. [85]. The
method is based on the combined use of a pitch-based harmonic mask for the vocals and a spectrogram
factorization model for suppressing the accompaniment where it overlaps with the vocals. The method
ﬁrst creates a binary mask indicating time-frequency regions where harmonic content of vocals is
present. Then, non-negative matrix factorization (NMF) [86, Chapter 9] is applied on the non-vocal
segments of the magnitude spectrogram in order to learn a model for the accompaniment. The NMF-
based model is then used to predict the amount of accompaniment in the vocal segments, which allows
for separating vocals and background even when they overlap in time and frequency.
Since NMF-based techniques have turned out to be a powerful tool for a wide range of music process-
ing tasks, we describe the above approach in some more detail. Mathematically, NMF approximates a
non-negative matrix as a product of two lower-rank non-negative matrices [87]. In our context, the mag-
nitude spectrogram of the accompaniment A(K×T ) is approximated as a product of matrices W(K×C)
and H(C×T ):
A ≈WH,
(27.21)
where the number C of components must be smaller than the number K of frequency components or
the number T of time frames. Intuitively, matrix W can be thought of as containing C different spectral
vectors and H contains weights to combine them at different times. Estimation of W and H is done by
minimizing a chosen distance measure 
t,k d([A]t,k, [WH]t,k) between the elements of A and WH
(see [87] for details). In order to construct a model for the accompaniment only, Virtanen et al. [85]
ignore the vocal regions in the estimation by using a weighted NMF that minimizes the distance

t,k
[M]t,kd([A]t,k, [WH]t,k),
(27.22)
where the mask M equals zero for vocal regions and equals one elsewhere. Although the model is
estimated using the non-vocal regions only, it can be used to predict the amount of accompaniment
energy at the vocal regions too—exactly what is wanted. Here, it is important that vocal pitch varies
in the segment where W and H are estimated (typically 3–30 s). As a consequence, the vocals do not
occlude the same parts of the spectrum all the time, thus allowing for “glimpses” of the accompaniment

750
CHAPTER 27 Music Signal Processing
in the estimation. The magnitude spectrogram of the vocals, V, is then reconstructed as
ˆV = (1 −M).×|X −WH|≥0,
(27.23)
where X denotes the observed mixture spectrogram, | · |≥0 sets negative elements of the matrix to zero,
and .× denotes element-wise multiplication.
Figure 27.23 illustrates the above-described NMF-based vocals separation method for an excerpt of
popular music. Figure 27.23a shows the magnitude spectrogram of the original mixture signal, whereas
Figure 27.23b shows the mixture spectrogram together with the pitch-based mask M. Here, areas
indicated by the white color correspond to time-frequency regions where the mask is zero. Figure 27.23c
Time [sec]
Frequency [kHz]
1
2
3
4
0
1
2
3
4
5
Time [sec]
Frequency [kHz]
1
2
3
4
0
1
2
3
4
5
Time [sec]
Frequency [kHz]
1
2
3
4
0
1
2
3
4
5
Time [sec]
Frequency [kHz]
1
2
3
4
0
1
2
3
4
5
(a)
(b)
(d)
(c)
FIGURE 27.23
Vocals separation based on melody transcription and NMF for an excerpt of popular music. (a) Magnitude
spectrogram. (b) Mixture spectrogram together with the pitch-based mask M. (c) Estimated accompaniment.
(d) Estimated vocals ˆV.

4.27.5 Melody and Vocals
751
illustrates the magnitude spectrogam of the estimated accompaniment, where parts under the mask are
obtained from the model WH and the other parts are from the observed mixture spectrogram X. Finally,
Figure 27.23d shows the spectrogram of the estimated vocals ˆV. Comparing this with the mixture shown
in Figure 27.23a, it can be seen that especially the amount of percussive energy overlapping the vocals
has been reduced. Finally, the time-domain signal of the vocals is obtained by applying an inverse
Fourier transform to ˆV using phases from the original mixture spectrogram.
Theabove-discussedmethodfocusesonmonauralaudiosignals,althoughmusicisgenerallyrecorded
stereo. Spatial information is an important part of the auditory organization in humans. In music signals,
however, usefulness of the stereo information for source separation depends heavily on genre: often
the stereo information is artiﬁcially constructed by amplitude panning and sometimes instruments are
played separately for the left and right channel to achieve a richer “sound”. In some cases, however, the
stereo information can be very useful, and a meaningful separation can be achieved simply by selecting
spectral components based on their intensity difference in the left and right channels [88]. This can be
combined with bandpass ﬁltering to discriminate between vocals and the bass line, which are often both
panned to the center of the stereo ﬁeld.
4.27.5.3 Applications
Extracting the melody and lead vocals from polyphonic music enables several practical applications.
Among others, melody transcription can be used for lead sheet generation, content-based retrieval such
as query-by-humming, or music-oriented computer games. Furthermore, vocals separation can be used
to create a karaoke version of a given music recording and to facilitate further analysis of the lyrics
and singer identity. In the following, two of these applications, query-by-humming and karaoke version
generation, are brieﬂy discussed.
In the query-by-humming (QBH) scenario, the user may remember a melody but not the name of the
piece of music. In order to retrieve the corresponding music recoding, the user formulates a query by
singing or humming parts of the melody into a microphone. Typically, the query is ﬁrst transcribed into a
pitch trajectory, which is then matched against a database of music recordings where the main melodies
have been extracted in advance. Matching two melodic patterns requires a measure of similarity between
two pitch trajectories. The similarity measure has to be robust to differences arising from mistakes and
inaccuracies in the singing of the user or from errors introduced by the automatic melody transcription
(in practice the matching is often made against human-performed versions of the main melody instead of
an automatically extracted one [89]). One straightforward and relatively robust approach is to calculate
the Euclidean distance between temporally aligned log-pitch trajectories that have been normalized to
zero mean in order to reconcile for differences in absolute pitch. Here, an important consideration is
the computational complexity: the query pattern can in principle occur at any position of the target data
and its time-scale (tempo) may differ from the potential matches in the target data. A number of QBH
systems have been recently compared in MIREX evaluations [74] and some on-line QBH services are
already available (see e.g., Midomi.com).
As another application, we consider the karaoke entertainment scenario, where an amateur singer
sings along with background music with the lyrics being presented synchronously on a screen. The
accompaniment tracks are conventionally produced by professional musicians in a recording studio.
Automated separation of the lead vocals from recorded songs would allow the creation of karaoke

752
CHAPTER 27 Music Signal Processing
versions of a large number of existing songs a user may have in his private collection. Moreover, if the
melody track of the original performance is transcribed too, the user’s singing can be compared with
the original performance (for the purpose of scoring in computer games), or even automatically tuned
to the correct pitch [90].
Relevant Theory: Signal Processing Theory, Machine Learning, and Statistical Signal Processing
See Vol. 1, Chapter 3 Discrete-Time Signals and Systems
See Vol. 1, Chapter 9 Discrete Multi-Scale Transforms in Signal Processing
See Vol. 1, Chapter 21 Unsupervised learning and latent variable models (PCA, ICA, NMF, ...)
See Vol. 1, Chapter 26 Music Mining
See Vol. 3, Chapter 3 Non-Stationary Signal Analysis Time-Frequency Approach
References
[1] M. Müller, Information Retrieval for Music and Motion, Springer Verlag, 2007.
[2] E. Selfridge-Field (Ed.), Beyond MIDI: The Handbook of Musical Codes, MIT Press, Cambridge, MA, USA,
1997.
[3] E. Zwicker, H. Fastl, Psychoacoustics, Facts and Models, Springer Verlag, New York, NY, US, 1990.
[4] M. Müller, D.P.W. Ellis, A. Klapuri, G. Richard, Signal processing for music analysis, IEEE J. Sel. Top. Sig.
Process. 5 (2011) 1088–1110.
[5] J.C. Brown, M.S. Puckette, An efﬁcient algorithm for the calculation of a constant Q transform, J. Acoust.
Soc. Am. 92 (1992) 2698–2698.
[6] C. Schörkhuber, A. Klapuri, Constant-Q transform toolbox for music processing, in: Sound and Music Com-
puting Conference (SMC), Barcelona, 2010.
[7] E. Gómez, Tonal description of polyphonic audio for music content processing, INFORMS J. Comput. 18
(2006), pp. 294–304.
[8] R.N. Shepard, Circularity in judgments of relative pitch, J. Acoust. Soc. Am. 36 (1964) 2346–2353.
[9] D. P. W. Ellis and G. E. Poliner, Identifying ‘cover songs’ with chroma features and dynamic programming beat
tracking, in: Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), vol. 4, Honolulu, Hawaii, USA, April 2007.
[10] E. Gómez, Tonal Description of Music Audio Signals, PhD thesis, UPF Barcelona, 2006.
[11] A. Klapuri, Multipitch analysis of polyphonic music and speech signals using an auditory model, IEEE Trans.
Audio Speech Lang. Process. 16 (2008) 255–266.
[12] M. Müller, S. Ewert, Towards timbre-invariant audio features for harmony-based music, IEEE Audio, Speech,
Language Process. 18 (2010), pp. 649–662.
[13] M. Müller, S. Ewert, Chroma Toolbox: MATLAB implementations for extracting variants of chroma-based
audio features, in: Proceedings of the 12th International Conference on Music Information Retrieval (ISMIR),
Miami, USA, 2011, pp. 215–220.
[14] M.A. Bartsch, G.H. Wakeﬁeld, Audio thumbnailing of popular music using chroma-based representations,
IEEE Trans. Multimedia 7 (2005) 96–104.
[15] T. Cho, R.J. Weiss, J.P. Bello, Exploring common variations in state of the art chord recognition systems, in:
Proceedings of the Sound and Music Computing Conference (SMC), Barcelona, Spain, 2010, pp. 1–8.
[16] T. Fujishima, Realtime chord recognition of musical sound: a system using common lisp music, in: Proceedings
of ICMC, Beijing, 1999, pp. 464–467.

References
753
[17] M. Mauch, S. Dixon, Simultaneous estimation of chords and musical context from audio, IEEE Trans. Audio
Speech Lang. Process. 18 (2010) 1280–1289.
[18] S. Ewert, M. Müller, P. Grosche, High resolution audio synchronization using chroma onset features, in:
Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Taipei,
Taiwan, April 2009, pp. 1869–1872.
[19] N. Hu, R. Dannenberg, G. Tzanetakis, Polyphonic audio matching and alignment for music retrieval, in:
Proceedings of the IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA),
New Paltz, NY, US, October, 2003.
[20] C. Joder, S. Essid, G. Richard, A comparative study of tonal acoustic features for a symbolic level music-
to-score alignment, in: Proceedings of the 35nd IEEE International Conference on Acoustics, Speech, and
Signal Processing (ICASSP), Dallas, USA, 2010.
[21] J. Paulus, M. Müller, A. Klapuri, Audio-based music structure analysis, in: Proceedings of the 11th Interna-
tional Conference on Music Information Retrieval (ISMIR), Utrecht, Netherlands, 2010, pp. 625–636.
[22] J. Serrà, E. Gómez, P. Herrera, X. Serra, Chroma binary similarity and local alignment applied to cover song
identiﬁcation, IEEE Trans. Audio Speech Lang. 16 (2008) pp. 1138–1151.
[23] F. Kurth, M. Müller, Efﬁcient index-based audio matching, IEEE Trans. Audio Speech Lang.
Process. 16 (2008) 382–395.
[24] M. Müller, F. Kurth, M. Clausen, Audio matching via chroma-based statistical features, in: Pro-
ceedings
of
the
6th
International
Conference
on
Music
Information
Retrieval
(ISMIR),
2005,
pp. 288–295.
[25] S.
Kostka,
D.
Payne,
Tonal
Harmony,
ﬁfth
ed.,
McGraw-Hill
Humanities/Social
Sciences/
Languages, 2003.
[26] N. Jiang, P. Grosche, V. Konz, M. Müller, Analyzing chroma feature types for automated chord recognition,
in: Proceedings of the 42th AES Conference, Ilmenau, Germany, 2011.
[27] B. Pardo, W.P. Birmingham, Algorithms for chordal analysis, Comput. Music J. 26 (2002) 27–49.
[28] C. Harte, M. Sandler, S. Abdallah, E. Gómez, Symbolic representation of musical chords: a proposed syntax
for text annotations, in: Proceedings of the International Conference on Music Information Retrieval (ISMIR),
London, GB, 2005.
[29] A.P. Klapuri, A.J. Eronen, J. Astola, Analysis of the meter of acoustic musical signals, IEEE Trans. Audio
Speech Lang. Process. 14 (2006) 342–355.
[30] W.A. Sethares, Rhythm and Transforms, Springer, 2007.
[31] F. Lerdahl, R. Jackendoff, Generative Theory of Tonal Music, MIT Press, 1983.
[32] J.P. Bello, L. Daudet, S. Abdallah, C. Duxbury, M. Davies, M.B. Sandler, A tutorial on onset detection in
music signals, IEEE Trans. Speech Audio Process. 13 (2005) 1035–1047.
[33] P. Grosche, M. Müller, Extracting predominant local pulse information from music recordings, IEEE Trans.
Speech Audio Process. 19 (2011) 1688–1701.
[34] D. Stowell, M. Plumbley, Adaptive whitening for improved real-time audio onset detection, in: Proceedings
of the International Computer Music Conference (ICMC), Denmark, Copenhagen, 2007.
[35] M. Alonso, B. David, G. Richard, Tempo and beat estimation of musical signals, in: Proceedings of the
International Conference on Music Information Retrieval (ISMIR), Barcelona, Spain, 2004.
[36] E.D. Scheirer, Tempo and beat analysis of acoustical musical signals, J. Acoust. Soc. Am. 103 (1998) 588–601.
[37] R. Zhou, M. Mattavelli, G. Zoia, Music onset detection based on resonator time frequency image, IEEE Trans.
Audio Speech Lang. 16 (2008) 1685–1695.
[38] R. Parncutt, A perceptual model of pulse salience and metrical accent in musical rhythms, Music Percept. 11
(1994) 409–464.
[39] F. Gouyon, A. Klapuri, S. Dixon, M. Alonso, G. Tzanetakis, C. Uhle, P. Cano, An experimental comparison
of audio tempo induction algorithms, IEEE Trans. Speech Audio Process. 14 (2006) 1832–1844.

754
CHAPTER 27 Music Signal Processing
[40] A.T. Cemgil, B. Kappen, P. Desain, H. Honing, On tempo tracking: tempogram representation and kalman
ﬁltering, J. New Music Res. 28 (2001) 259–273.
[41] A.J. Eronen, A.P. Klapuri, Music tempo estimation with k-NN regression, IEEE Trans. Audio Speech Lang.
Process. 18 (2010) 50–57.
[42] M. Alonso, G. Richard, B. David, Accurate tempo estimation based on harmonic+noise decomposition,
EURASIP J. Adv. Sig. Process. (2007) 1–14 (Article ID: 82795).
[43] G.Peeters,Template-basedestimationoftime-varyingtempo,EURASIP J.Adv.Sig.Process.(2007) 158–158.
[44] P. Grosche, M. Müller, F. Kurth, Cyclic tempogram—a mid-level tempo representation for music signals,
in: Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP),
Dallas, Texas, USA, March 2010.
[45] D.P.W. Ellis, Beat tracking by dynamic programming, J. New Music Res. 36 (2007) 51–60.
[46] S.
Dixon,
W.
Goebl,
Pinpointing
the
beat:
tapping
to
expressive
performances,
in:
Proceed-
ings of International Conference on Music Perception and Cognition, Sydney, Australia, 2002,
pp. 617–620.
[47] P. Grosche, M. Müller, C.S. Sapp, What makes beat tracking difﬁcult? A case study on Chopin Mazurkas, in:
Proceedings of the 11th International Conference on Music Information Retrieval (ISMIR), Utrecht, Nether-
lands, 2010, pp. 649–654.
[48] H. Papadopoulos, G. Peeters, Joint estimation of chords and downbeats from an audio signal, IEEE Trans.
Audio Speech Lang. 19 (2011) 138–152.
[49] D.P.W. Ellis, C.V. Cotton, M.I. Mandel, Cross-correlation of beat-synchronous representations for music
similarity, in: Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing
(ICASSP), Taipei, Taiwan, 2008, pp. 57–60.
[50] S.-C. Pei, N.-T. Hsu, Instrumentation analysis and identiﬁcation of polyphonic music using beat-synchronous
feature integration and fuzzy clustering, in: Proceedings of the IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP), 2009, pp. 169–172.
[51] C.S. Sapp, Comparative analysis of multiple musical performances, in: Proceedings of the International
Conference on Music Information Retrieval (ISMIR), Austria, Vienna, 2007, pp. 497–500.
[52] V. Konz, M. Müller, S. Ewert, A multi-perspective evaluation framework for chord recognition, in: Proceedings
of the 11th International Conference on Music Information Retrieval (ISMIR), Utrecht, Netherlands, 2010,
pp. 9–14.
[53] E.W. Large, C. Palmer, Perceiving temporal regularity in music, Cogn. Sci. 26 (2002) 1–37.
[54] V. Alluri, P. Toiviainen, Exploring perceptual and acoustical correlates of polyphonic timbre, Music Percept.
27 (2010) 223–241.
[55] N. Scaringella, G. Zoia, D. Mlynek, Automatic genre classiﬁcation of music content: a survey, IEEE Signal
Process Mag. 23 (2006) 133–141.
[56] D. Turnbull, L. Barrington, D. Torres, G. Lanckriet, Semantic annotation and retrieval of music and sound
effects, IEEE Trans. Audio Speech Lang. 16 (2008) 467–476.
[57] J. F. Schouten, The perception of timbre, in: 6th International Congress on Acoustics, Tokyo, Japan, 1968,
pp. GP–6–2.
[58] J.H. McDermott, E.P. Simoncelli, Sound texture perception via statistics of the auditory periphery: evidence
from sound synthesis, Neuron 71 (2011) 926–940.
[59] A. Caclin, S. McAdams, B.K. Smith, S. Winsberg, Acoustic correlates of timbre space dimensions: a conﬁr-
matory study using synthetic tones, J. Acoust. Soc. Am. 118 (2005) 471–482.
[60] H. Rump, S. Miyabe, E. Tsunoo, N. Ono, S. Sagayama, Autoregressive MFCC models for genre classiﬁcation
improved by harmonic-percussion separation, in: Proceedings of the International Conference on Music
Information Retrieval (ISMIR), Utrecht, Netherlands, 2010, pp. 87–92.
[61] L. Rabiner, B.-H. Juang, Fundamentals of Speech Recognition, Prentice Hall Signal Processing Series, 1993.

References
755
[62] B.C.J. Moore (Ed.), Hearing—Handbook of Perception and Cognition, second ed., Academic Press, San
Diego, California, 1995.
[63] N. Fletcher, T. Rossing, The Physics of Musical Instruments, second ed., Springer, Berlin, Germany, 1998.
[64] A. Klapuri, Analysis of musical instrument sounds by source-ﬁlter-decay model, in: IEEE International Con-
ference on Audio, Speech and Signal Processing, Hawaii, USA, 2007.
[65] J.G.A. Barbedo, G. Tzanetakis, Musical instrument classiﬁcation using individual partials, IEEE Trans. Audio
Speech Lang. Process. 19 (2011) 111–122.
[66] L.R. Rabiner, A tutorial on hidden markov models and selected applications in speech recognition, Proc. IEEE
77 (1989) 257–286.
[67] A. Klapuri, T. Virtanen, Representing musical sounds with an interpolating state model, IEEE Trans. Audio
Speech Lang. Process. 18 (2010) 613–624.
[68] C. Joder, S. Essid, G. Richard, Temporal integration for audio classiﬁcation with application to musical
instrument classiﬁcation, IEEE Trans. Audio Speech Lang. Process. 17 (2009) 174–186.
[69] T. Kitahara, M. Goto, K. Komatani, T. Ogata, H.G. Okuno, Instrument identiﬁcation in polyphonic music:
feature weighting to minimize inﬂuence of sound overlaps, EURASIP J. Appl. Sig. Process. (2007) 1–15.
[70] T. Heittola, A. Klapuri, T. Virtanen, Musical instrument recognition in polyphonic audio using source-ﬁlter
model for sound separation, in: Proceedings of the International Society for Music Information Retrieval
Conference (ISMIR), Kobe, Japan, 2009, pp. 327–332.
[71] E. Vincent, X. Rodet, Instrument identiﬁcation in solo and ensemble music using independent subspace
analysis, in: Proceedings of the 5th International Symposium on Music Information Retrieval, Barcelona,
Spain, 2004, pp. 576–581.
[72] P. Leveau, E. Vincent, G. Richard, L. Daudet, Instrument-speciﬁc harmonic atoms for mid-level music repre-
sentation, IEEE Trans. Audio Speech Lang. Process. 16 (2008) 116–128.
[73] J.P. Barker, M.P. Cooke, D.P.W. Ellis, Decoding speech in the presence of other sources, Speech Commun.
45 (2005) 5–25.
[74] J.S. Downie, The music information retrieval evaluation exchange (2005–2007): a window into music infor-
mation retrieval research, Acoust. Sci. Technol. 29 (2008) 247–255.
[75] S. Dixon, E. Pampalk, G. Widmer, Classiﬁcation of dance music by periodicity patterns, in: 4th International
Conference on Music Information Retrieval, Baltimore MD, 2003, pp. 159–165.
[76] M. Helén, T. Virtanen, Audio query by example using similarity measures between probability density func-
tions of features, EURASIP J. Audio Speech Music Process. 2010 (2010).
[77] L. Brown, The New Shorter Oxford English Dictionary, Clarendon Press, Oxford, 1993.
[78] A. Klapuri, A method for visualizing the pitch content of polyphonic music signals, in: Proceed-
ings of the International Society for Music Information Retrieval Conference, Kobe, Japan, 2009,
pp. 615–620.
[79] K. Dressler, Audio melody extraction, in: Proceedings of the International Society for Music Information
Retrieval Conference (ISMIR): Late Breaking session, 2010.
[80] M. Goto, A real-time music-scene-description system: predominant-F0 estimation for detecting melody and
bass lines in real-world audio signals, Speech Commun. (ISCA J.) 43 (2004) 311–329.
[81] M. Ryynänen, A. Klapuri, Automatic transcription of melody, bass line, and chords in polyphonic music,
Comput. Music J. 32 (2008) 72–86.
[82] J.-L.
Durrieu,
G.
Richard,
B.
David,
C.
Févotte,
Source/ﬁlter
model
for
unsupervised
main
melody extraction from polyphonic audio signals, IEEE Trans. Speech Audio Process. 18 (2010)
564–575.
[83] M. Goto, H. Hashiguchi, T. Nishimura, R. Oka, RWC music database: popular, classical and jazz music
databases, in: Proceedings of the International Conference on Music Information Retrieval (ISMIR), France,
Paris, 2002.

756
CHAPTER 27 Music Signal Processing
[84] H. Fujihara, M. Goto, A music information retrieval system based on singing voice timbre,
in: International Conference on Music Information Retrieval, Austria, Vienna, 2007.
[85] T. Virtanen, A. Mesaros, M. Ryynänen, Combining pitch-based inference and non-negative spectrogram
factorization in separating vocals from polyphonic music, in: ISCA Tutorial and Research Workshop on
Statistical and Perceptual Audition, Brisbane, Australia, September, 2008.
[86] A. Klapuri, M. Davy (Eds.), Signal Processing Methods for Music Transcription, Springer, New York, 2006.
[87] D.D. Lee, H.S. Seung, Algorithms for non-negative matrix factorization, in: Neural Information Processing
Systems, USA, Denver, 2001, pp. 556–562.
[88] D. Barry, B. Lawlor, E. Coyle, Sound source separation: azimuth discriminiation and resynthesis, in: Pro-
ceedings of the 7th International Conference on Digital Audio Effects (DAFX-04) Naples, Italy, October
2004.
[89] M. Cartwright, Z. Raﬁi, J. Han, B. Pardo, Making searchable melodies: human vs. machine, in: Workshops
at the 25th AAAI Conference on Artiﬁcial Intelligence, San Francisco, USA, 2011.
[90] M. Ryynänen, T. Virtanen, J. Paulus, A. Klapuri, Accompaniment separation and karaoke application based
on automatic melody transcription, in: IEEE International Conference on Multimedia and Expo, Hannover,
Germany, 2008, pp. 1417–1420.

28
CHAPTER
Perceptual Audio Coding
Jürgen Herre* and Sascha Disch†
*International Audio Laboratories Erlangen, a joint institution of the University of Erlangen-Nuremberg and
Fraunhofer IIS, Erlangen, Germany
†Fraunhofer IIS, Erlangen, Germany
Introduction
Perceptual audio coding technology is today present in most of the devices that deliver audio or mul-
timedia to consumers, including portable audio players (“mp3 players” like iPods), personal digital
assistants (PDAs), cellular phones, digital TV and radio sets (terrestrial and satellite-based), Compact
Disc (CD) or Digital Versatile Disc (DVD) players, Internet radio appliances and, of course, practically
every personal computer. In each case, the motivation for using audio coding is to represent the audio
signal in a format that saves substantially in bitrate compared to an uncompressed representation, e.g.,
Pulse Code Modulation (PCM) data, while preserving its perceived audio quality to the highest extent
possible. This enables the efﬁcient storage of audio content on devices with limited on-board memory
or the transmission over channels with limited capacity, like e.g., the early Internet or radio frequency
transmission/broadcasting in an economic way. In many cases, the substantial reduction in data rate
provided by perceptual audio coding inspired and enabled new applications which would not have been
feasible without this technology at the time they were conceived, examples including portable audio
players, digital audio broadcasting, Internet radio, music streaming and download. The most common
format, “mp3,” is frequently used as a synonym for these types of technologies and is represented in
many devices in virtually every household.
This article provides an overview of the principles behind perceptual audio coding. Starting from the
predominant basic underlying coder structure, the components that can be found in modern audio coding
schemes are introduced and explained with respect to their function and (some) relevant implementation
issues. The ordering of technical aspects within this article is chosen to roughly correspond to the
historical order in which these ideas have been introduced. Finally, reference to some successful audio
coding standards is made, in particular to the audio coding standards created by the International
Standardization Organization’s Moving Pictures Expert Group (ISO/MPEG), which have been deﬁning
the state of the art for at least two decades [1].
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00028-5
© 2014 Elsevier Ltd. All rights reserved.
757

758
CHAPTER 28 Perceptual Audio Coding
4.28.1 Principles and background
In order to appreciate the technical structures employed in perceptual audio coding, this section brieﬂy
discusses some generic underlying principles and general background.
To illustrate the merits of audio coding, it is helpful to consider a simple PCM representation as a
baseline. For example, stereophonic audio signals as contained on CDs are represented by two streams
of sampled audio data, each represented with 16 bits per sample at a sampling rate of 44.1 kHz, leading
to a total bitrate of 1411.2 kbit/s.
4.28.1.1 Redundancy and irrelevancy
The basic aspects that allow perceptual audio coders to perform better than simple PCM coding can be
framed into the two terms redundancy and irrelevancy.
In the area of information theory, the term redundancy is used to describe the idea that a stream of
symbols of a certain alphabet can be encoded more or less compactly by choosing appropriate codes
to represent these symbols [2]. Depending on this choice (i.e., the choice of codes and their associated
codeword lengths), more or fewer bits are consumed to represent or transmit a given message. It can
be shown that there is a theoretical lower bound for the bit consumption, called entropy, given the
probability of occurrence for each symbol. The excess code length of an actual coded representation
compared to its theoretical minimum is called its redundancy. In audio coding, the abstract concept of
redundancy applies in several ways:
•
As different sample values have a different frequency of occurrence, entropy coding can be used to
reduce redundancy and thus increase the coding efﬁciency.
•
Since subsequent samples are not independent from each other but are generally correlated, this
correlation can be exploited to further reduce the data rate.
•
Furthermore, correlations between different audio channels (e.g., left and right channels on a stereo-
phonic audio signal) can be exploited.
Reduction of redundancy does not introduce any error into the decoded signal, i.e., coding by reduc-
tion of redundancy is lossless coding.
Irrelevancy on the other hand is used in communication theory [2] to denote the fact that the receiver
of a certain (transmitted) message is frequently unable to recognize all aspects of the message, i.e.,
it is insensitive to certain irrelevant aspects of the message. Consequently, it is indistinguishable by
the receiver whether a modiﬁed message is transmitted instead which differs from the original only in
irrelevant aspects. In audio coding, this modiﬁed message is chosen to consume much fewer bits to
represent than its original counterpart and thus allows additional compression. Note that, objectively
speaking, there is a loss of information when switching to the modiﬁed message, thus use of irrelevance
for compression leads to a lossy coding process.
4.28.1.2 Auditory perception and psychoacoustics
The concept of irrelevancy can be applied successfully in perceptual audio coding by recognizing
that the intended receiver of the message (i.e., the audio signal) is the human auditory system with

4.28.1 Principles and Background
759
FIGURE 28.1
Masking threshold (solid line) of three narrow band noise maskers (grey bars) with a level of 60 dB and critical
bandwidth fg , located at center frequencies fm. The threshold in quiet is plotted for reference (dotted line).
After [3].
all its amazing sensitivity and limitations. Speciﬁcally, the discipline of psychoacoustics attempts to
connect measurable physical properties of sound signals with the internal percepts (and, of course,
limits of perception) that these sounds evoke in a listener [3–5]. Examples for such internal percepts are
quantities like loudness, pitch and sharpness (in contrast to their physical correlates level, fundamental
frequency and spectral centroid which do not map one-to-one to them).
A particular useful part of psychoacoustics describes the limitations in human auditory perception,
thus deﬁning what can be considered as “irrelevancy” for the purpose of audio coding. The phenomenon
of masking is of paramount importance in this context. Masking denotes the fact that a certain sound
(“probe”) may not be audible when presented in the temporal and frequency neighborhood of another
stronger sound (“masker”). Some basic limitations of human listening are illustrated in Figure 28.1:
•
Threshold in quiet: The threshold in quiet describes the minimum sound pressure level that is needed
for a narrow band sound to be detected by human listeners in the absence of other sounds, given
in dB SPL (sound pressure level) as a function of frequency. Plotted as a curve (see dotted line
in Figure 28.1), it shows that our hearing sensitivity decreases dramatically towards very low and
(more steeply) towards high frequencies, the area of maximum sensitivity being around 2–3 kHz.
•
Simultaneous masking: If a narrowband masker is presented simultaneously along with the probe
signal, the threshold-in-quiet curve changes into a bell-shape nearby the frequency of the narrowband
masker (see solid line in Figure 28.1). In this way, the sound of a particular musical instrument can,
for example, mask the sound of others. The masking ability of a sound depends on many factors,
including its level, frequency content and tone- or noise-likeness.
•
Temporal masking (not shown in Figure 28.1): The masking effect induced by a masker also depends
on the temporal structure of the same. The effect builds up shortly before the onset time of the masker,
and fades away gradually after the offset, further depending on the various characteristics, like level
and duration of the masker.

760
CHAPTER 28 Perceptual Audio Coding
•
Spatial dimension of masking (not shown in Figure 28.1): Listening with two ears allows human
listeners to perceive sound with a spatial dimension and focus on a certain spatial direction (common
example: concentrate on one talker in a cocktail party with many people). Consequently, masking
is decreased if masker and probe have different spatial properties, e.g., are located in different
directions [5].
Beyond this short list, numerous other effects exist which make masking and human sound perception
in general a complex phenomenon. Yet, a ﬁrst-order modeling of human perception is extremely effective
in the area of audio coding.
4.28.1.3 Sound quality
At sufﬁciently low bitrates, perceptual audio coders inevitably introduce some difference (distortion)
into the signal by virtue of the lossy steps that are a part of their processing chain. The signiﬁcance of
this distortion can be assessed by traditional objective distortion measures such as (segmental) Signal-
to-Noise-Ratio (SNR) [6]. In the context of signal processing that exploits perceptual phenomena like
masking, however, it was found that such measures show only a poor correspondence to the actual
subjectively perceived audio quality [7]. Due to the time-frequency noise shaping techniques employed
in perceptual audio coding (see below), it is in fact quite common that coded/decoded signals have an
overall SNR of less than 20 dB and nevertheless maintain perfect sound quality. The so-called “13 dB
miracle” demonstration by Brandenburg and Sporer [7] is a well-known example of such a signal.
Figure 28.2 illustrates the impact of the spectral noise shaping effect for a typical short time frame of
these demonstration signals.
Consequently, the only valid/relevant measure of true sound (audio) quality is subjective quality,
as it is measured by controlled listening tests like BS.1116-1 [8] or BS.1534-1 (MUSHRA) [9]. In
order to provide an economic and repeatable objective alternative to listening tests, perceptual mea-
surement schemes like NMR [7] or PEAQ (BS.1387-1 [10,11]) are designed to provide an estimate of
the expected subjective quality. Nevertheless, such schemes are only able to account for a certain share
of the phenomena present in natural human hearing and are therefore limited in their validity.
FIGURE 28.2
Spectral envelope of original signal, white and shaped error signals with equivalent overall SNR of 13 dB
(stylized). After [7].

4.28.2 Concepts and Architectures
761
4.28.2 Concepts and architectures
This section describes principal concepts for signal processing in perceptual audio coding, ordered from
simple algorithms to increasingly sophisticated structures and, to some extent, in the chronological order
of their introduction.
4.28.2.1 Monophonic audio coding and decoding
Starting from the very beginnings [12–14], perceptual audio coders predominantly employed the struc-
ture depicted in Figure 28.3 for encoding a monophonic input signal.
The structure of these coders is designed to exploit both redundancy and irrelevancy available in the
input signal. It typically consists of the following processing blocks:
•
Analysis Filter bank: The input signal is analyzed by an analysis ﬁlter bank that converts the time
domain signal into a spectral (time/frequency) representation. The conversion into spectral coefﬁ-
cients allows for selectively processing signal components depending on their frequency content
(e.g., different instruments with their individual overtone structures). To this end, many different
ﬁlter bank types can be employed. However, a number of attributes are desirable for ﬁlter banks
used in coding applications, such as listed in the following:
•
Good decorrelation properties to potentially achieve a high coding gain.
•
Critical sampling (i.e., the number of output values is equal to the number of input values).
•
Overlap-add to avoid block processing artifacts/discontinuities at the block boundaries.
•
Low signal delay.
•
Low computational complexity.
•
Perceptual Model: The input signal is analyzed w.r.t. its perceptual properties, i.e., speciﬁcally the
time- and frequency-dependent masking threshold is computed. In many cases, a separate ﬁlter bank
(Discrete Fourier Transform, DFT) is employed to this end, which is better suited to the needs of a
psychoacoustic analysis than typical analysis ﬁlter banks. Some encoder implementations, however,
make use of the spectral coefﬁcients produced by the analysis ﬁlter bank in the interest of a reduced
computational complexity. The time/frequency dependent masking threshold is delivered to the
FIGURE 28.3
Generic structure of a monophonic perceptual audio encoder, consisting of an analysis ﬁlter bank, a quantiza-
tion unit that is controlled by a psychoacoustic model, and a coding unit followed by a bitstream multiplexer.

762
CHAPTER 28 Perceptual Audio Coding
quantization unit through a target coding threshold in the form of an absolute energy value or a
Mask-to-Signal-Ratio (MSR) for each frequency band and coding time frame.
•
Quantization: The spectral coefﬁcients delivered by the analysis ﬁlter bank are quantized or, in
other words, rounded to certain predeﬁned values, to reduce the data rate needed for representing
the signal. This step implies a loss of information and introduces a coding distortion (error, noise)
into the signal. In order to minimize the audible impact of this coding noise, the quantizer step sizes
are controlled according to the target coding thresholds for each frequency band and frame. Ideally,
the coding noise injected into each frequency band is lower than the coding (masking) threshold
and thus no degradation in subjective audio is perceptible (removal of irrelevancy). This control of
the quantization noise over frequency and time according to psychoacoustic requirements leads to
a sophisticated noise shaping effect and is what makes the coder a perceptual audio coder.
•
Coding: Modern audio coders perform entropy coding (e.g., Huffman coding, arithmetic coding) on
the quantized spectral data. Entropy coding is a lossless coding step, which further saves on bitrate.
•
Bitstream Multiplexer: All coded spectral data and relevant additional parameters (side information,
like e.g., the quantizer settings for each frequency band) are packed together (“multiplexed”) into a
bitstream, which is the ﬁnal coded representation intended for ﬁle storage or transmission. Beside
source coding requirements, the assembly of the bitstream is frequently designed to also consider
other application-related issues, such as support of direct skipping to a certain time frame (“random
access”) in the decoder or channel coding issues like resilience against data loss caused by error-prone
transmission channels.
Figure 28.4 depicts the counterpart (decoder) to the previously described encoder structure. First,
the incoming bitstream is disassembled into the individual parameter values and quantized data indices.
Then, the transmitted spectral data (quantizer indices) are mapped back to their original values (inverse
scaling) and are ﬁnally converted back into a listenable time domain representation by the synthesis
ﬁlter bank. Seen as a pair, the analysis and the synthesis ﬁlter bank are usually (nearly) perfectly
reconstructing, i.e., the combination of both ﬁlter banks has a transfer function that is (almost) equivalent
to a delay only.
Examining the overall codec structure, as depicted in Figures 28.3 and 28.4, it can be seen that
reduction of irrelevancy and redundancy happens at various stages of the process:
•
Exploiting redundancy: The fact that the signal is encoded in the spectral domain rather than in the
time domain allows for exploiting correlation between subsequent time samples, as it is utilized by
the notion of Adaptive Transform Coding (ATC) [6]: Correlation between subsequent signal samples
FIGURE 28.4
Generic structure of a monophonic perceptual audio decoder, consisting of a bitstream demultiplexer, a
decoding followed by a restoration of the quantized values from the transmitted quantizer indices and a
synthesis ﬁlter bank.

4.28.2 Concepts and Architectures
763
leads to a non-ﬂat shape of the spectral envelope. Thus, allocating more bits to the representation of
the spectral coefﬁcients with higher magnitude (rule of thump: one additional bit per 6 dB increase
in magnitude) leads to a coding gain (increased SNR) compared to simple PCM encoding [6]. Note
that both transform coding and predictive coding exploit the same signal properties and perform the
same asymptotically. The potential for bitrate savings is determined by the signal’s autocorrelation
function or, equivalently, by its Spectral Flatness Measure (SFM) [6].
•
Exploiting irrelevancy: The fact that the audio signal is converted into a representation that allows
for individual access to its content at different frequencies together with a quantization guided by
psychoacoustics (i.e., the masking threshold) is the key to utilizing the signal’s irrelevancy.
•
Additional redundancy reduction: To further reduce the data rate, the quantized spectral coefﬁcients
can be subjected to entropy coding, which exploits the unequal distribution in the frequency of
occurrence for each quantized value. Usually lower values occur more frequently (and thus receive
fewer bits).
Depending on the nature of the input signal, the contributions of irrelevancy reduction and redun-
dancy reduction to the overall rate reduction vary. Typically, both parts tend to complement each other
in a well-designed audio coder. Tonal signals exhibit lower masking (less irrelevancy), but are usually
rather sparse in frequency and thus can be coded efﬁciently in the spectral domain (redundancy reduc-
tion). Noise-like signals are excellent maskers (high amount of irrelevancy), but are non-sparse and ﬂat
in their spectral representation (little redundancy, the extreme case being white noise). Nonetheless,
the overall reduction of redundancy is strictly determined by the signal and thus limited by principle.
As a rule of thumb, it is known that the maximum obtainable compression over a wide set of musi-
cal signals is in the order of 2:1 when the signals are represented in CD-like format (16 bits/sample
PCM at 44.1 kHz sampling rate) [15]. Higher sampling rates (e.g., 96 kHz, 192 kHz) increase the
available redundancy whereas higher word lengths (e.g., 20…24 bits/sample) decrease the possible
compression factor.
Perceptual Audio Coding vs. Speech Coding: Comparing state-of-the-art perceptual audio coding
schemes with common speech coders (e.g., ACELP), two different strategies for coding become visible.
Low bitrate speech coders achieve their high coding efﬁciency by explicitly exploiting knowledge about
the nature of their input signal, i.e., speech, as it is generated by a human vocal tract). This is reﬂected by
their architecture, which resembles a human vocal tract by an excitation/ﬁlter model and, consequently,
specializes in coding such signals in an extremely efﬁcient way. Perceptual audio coders, on the other
hand, do not assume any particular source model, but exploit generic mechanisms for redundancy
and irrelevancy reduction and thus perform well for many types of audio signal. For the same reason
their efﬁciency for coding speech signals does not achieve the efﬁciency of dedicated speech coders.
Conversely, for non-speech signals (like music) that do not match the assumptions made about the input
signal, speech coders tend to produce objectionable distortions.
4.28.2.2 Practical issues
Given the basic principles described previously, a number of practical issues are important to consider
when using perceptual audio coders. The remainder of this subsection enumerates and discusses a
collection of such issues.

764
CHAPTER 28 Perceptual Audio Coding
4.28.2.2.1
Filter banks
The choice of an appropriate analysis/synthesis ﬁlter bank pair has a fundamental impact on the per-
formance of the audio coder. Besides the desirable properties stated previously, the time/frequency
resolution offered plays an important role: On one hand, the frequency resolution should be high
enough to resolve the ﬁne spectral structure of the input signal (e.g., individual harmonics) to provide
good coding gain by redundancy reduction. On the other hand, high frequency resolution also means
low time resolution, as both time and frequency are mathematical reciprocals of each other. The noise
introduced through quantization of the spectral coefﬁcients extends, with temporally ﬂat envelope, over
the entire duration of a time block. Hence the particular choice of ﬁlter bank length strongly inﬂuences
the temporal shape of the noise and therefore must implicitly obey temporal masking criteria. As a
consequence, high-resolution ﬁlter banks perform poorly when coding signals that vary considerably
over time, such as transient signals (castanets, glockenspiel, drums, etc.—see remarks on “pre-echo”
artifacts below).
This trade-off was investigated in [16] and led the way to the commonly used ﬁlter bank conﬁguration
of a Modiﬁed Discrete Cosine Transform (MDCT) [17] with a resolution of 1024 spectral lines running
at CD-like sampling rates. Furthermore, due to the dynamic nature of audio signals, it was found to be
very effective to adapt the ﬁlter bank’s time/frequency resolution and switch to lower frequency/higher
time resolution whenever necessary by so called window switching (block switching) [18]. A tutorial
overview of classic coder ﬁlter banks is provided in [19].
4.28.2.2.2
Perceptual models
The function of the perceptual model is the computation of the time and frequency varying masking
threshold. Many implementations of perceptual models go back to the models described as part of
the MPEG-1 [20] or MPEG-2 [21,22] audio coding standards. Typically, such models consider at
least the non-uniform frequency scale used to mimic human hearing (BARK [3] or ERB [4]), the
characteristics of the auditory ﬁlters in the inner ear (cochlea) and the difference between tone-like and
noise-like maskers [23]. An overview of classic perceptual models is provided e.g., in [24]. While the
precision of the masking model is undoubtedly important for achieving a good coding performance, it
should be kept in mind that the ﬁnal result delivered by the coder is also inﬂuenced heavily by other
coder parts, such as the quantization and coding stages which frequently have to operate to match
severe bitrate constraints. Speciﬁcally, a good coder implementation should also include strategies in
the perceptual model and/or the quantization and coding stages how to encode a signal violating a given
masking threshold in the least perceptually annoying way at a given bit budget. It is thus more important
that the combination of these modules delivers good performance at the output rather than excellent
precision of the psychoacoustic module alone.
4.28.2.2.3
Quantization and coding
Quantization of the spectral coefﬁcients is mostly carried out using scalar quantizers of uniform or
non-uniform type, although also schemes for vector quantization (VQ) have been proposed, such as
[25]. The quantizer step size for each frequency region is usually transmitted explicitly to the decoder
as side information (as “scale factors” and/or “bit allocation” information) to allow re-scaling of the
transmitted quantized spectral values to their original range.

4.28.2 Concepts and Architectures
765
Beyond the simple “mechanical” part of quantizing the spectral coefﬁcients, the real “intelligence” of
the quantization and coding modules lies in the strategies that are used to constrain the required coding
bitrate to a target value or range while delivering best possible subjective audio quality. Variable rate
coding allows variations in the bitrate over time and can be used together with transmission or storage
channels that accommodate variable rate transmission, such as Internet streaming or storage on Digital
Versatile Disc (DVD). Another mode of operation is constant rate coding, which means that the codec
may not be able to satisfy the masking requirements demanded by the perceptual model if the nominal
bitrate is too low. In practice, hybrid schemes between constant and variable rate coding (so-called bit
reservoir techniques) are frequently used. An extensive overview of common quantization and coding
issues and strategies is provided in [26].
4.28.2.2.4
Coding artifacts
Whenever a codec misses to fulﬁll the intended perceptual requirements for producing transparent
(“indistinguishable from the original”) audio quality for certain audio signals, the processed signal
sounds audibly different from the original signal. While the difference may be very subtle at higher
bitrates, it can become striking at very low bitrates. Typical types of changes in perceptual quality
are called artifacts and can frequently be attributed to an overly coarse quantization of the spectral
coefﬁcients e.g., because of a lack of available bits. A terminology of typical coding artifacts is described
in [27]. Some important kinds of artifacts are listed below:
•
Roughness: A harsh timbre of tonal components that originates from too coarsely quantized spectral
coefﬁcients.
•
Birdies: Fragmented high-frequency components that spuriously appear and disappear in the trans-
mitted signal.
•
Pre-echoes: Audio event that precedes the onset of the actual signal, caused by spread of quantization
noise over time.
•
Tandem coding distortion: Noise introduced by perceptual encoding that accumulates with each
additional coding/decoding pass.
4.28.2.2.5
Coding delay and low delay coding
For some applications, the delay of the coder/transmission/decoder chain is important. As an example, in
bi-directional telecommunication (telephony) the overall delay should not exceed approx. 50 ms [28] in
order to guarantee a natural ﬂow of the conversation between both communication partners. While there
are many factors that inﬂuence the end-to-end delay of an actual system, there are certain limits within
the codec itself that cannot be overcome even at zero time needed for transmission of the bitstream
and inﬁnitely fast computation. This theoretical minimum delay is frequently referred to as algorithmic
delay and can be typically attributed to several factors [29]:
•
The fact that a codec collects a number of input audio samples, called a frame, and jointly processes
them translates into a framing delay.
•
The analysis/synthesis ﬁlter bank pair together introduces a further delay.
•
For certain encoding algorithms, the encoder has to look ahead into the “future” of the signal in
order to make a decision on the encoding mode (example: look-ahead required for block switching).

766
CHAPTER 28 Perceptual Audio Coding
•
If a constant-rate transmission channel is used, allowing local variations of the data rate (“bit reser-
voir”) translates into an additional delay [26].
If overall codec delay is of concern, as is the case for teleconferencing or videoconferencing, the
overall delay imposed by these factors has to be minimized. While common audio codecs have algo-
rithmic delays of several hundreds of milliseconds (depending on bitrate, sampling rate, etc.), [29]
shows how the overall delay of such a codec can be reduced down to just 20 ms, as it is appropriate for
communication coding. Similar considerations have given rise to a family of Low-Delay Audio Coders
[30,31] some of which will be discussed under Section 4.28.3.6. Further reduction of delay below ca.
20 ms while keeping high compression efﬁciency can be achieved by so-called ultra-low delay audio
coders, which deviate signiﬁcantly from the traditional architecture [32].
4.28.2.3 Joint stereo coding
The architectures discussed so far operate on a single (monophonic) input signal. Although it is straight
forward to assume that coding of stereophonic two-channel or multi-channel signals can be obtained
by simply using two or more independent monophonic codecs, it has been shown that “two (or more)
mono coders do not make a good stereo coder” [33]: Reproducing several audio channels introduces
the spatial dimension of human auditory perception which is not accounted for by independent coding.
Thus, the concept of joint stereo coding refers to two aspects:
•
A set of techniques that allow to exploit redundancy (and irrelevancy) between two or more audio
channels and in this way is able to represent n audio channels at less than n times the data rate needed
to represent a single audio channel.
•
A set of techniques that take into account the spatial aspect of human perception for audio coding
of several channels.
In practice, joint stereo coding algorithms have been around since the early nineties for two-channel
stereo and have been implemented as simple additional processing blocks that operate jointly on the
spectral coefﬁcients of left and right audio channels [33] in both encoder and decoder. Figures 28.5
and 28.6 show encoder and decoder architectures extended for joint stereo processing, respectively.
Compared to the generic encoder depicted in Figure 28.3, the encoder of Figure 28.5 has two additional
joint stereo processing blocks that allow for a controlled activation of joint encoding. Please note that
the perceptual model is also inﬂuenced by the joint stereo control to account for spatial perception
phenomena.
Historically seen, two schemes for joint stereo coding have proven to be successful and commonly
used: Mid/Side (M/S) stereo coding [34] and Intensity stereo (IS) coding [35,36]. These schemes, as
different as they are in both design and properties, are discussed in the following.
4.28.2.3.1
Mid/Side (M/S) stereo coding
M/S stereo coding, also called sum/difference coding, in perceptual audio coders has ﬁrst been proposed
by Johnston and owes its name to previous techniques which represent the left and right stereophonic
signal as the sum (“mid”) and difference (“side”) component, such as in microphony [37] or in FM
stereo transmission, where the stereo signal is represented by the mono (sum) signal—transmitted as

4.28.2 Concepts and Architectures
767
FIGURE 28.5
Encoder with joint stereo processing. A unit for joint stereo encoding of the left and right channel spectral
values and a joint stereo control activating the combined channel processing. The quantized and coded
values, along with the activation data, are multiplexed into one bitstream.
baseband audio and a difference signal that is modulated onto a 38 kHz double-sideband suppressed
carrier [38].
Essentially, M/S stereo coding converts the spectral coefﬁcients of the left and right channel into sum
and difference coefﬁcients before further processing by the quantization and coding units. Conversely,
this operation is undone in the decoder preceding the synthesis ﬁlter bank. With respect to Figures 28.5
and 28.6, the joint stereo encoding and decoding blocks for M/S are shown in Figure 28.7:
While the basic processing for M/S stereo coding is simple, a number of notable consequences arise
from it [33]:
•
Perfect reconstruction: The scheme is perfectly reconstructing, i.e., in the absence of quantization,
encoding and subsequent decoding delivers the original signal without any loss in information. M/S
stereo is useful for the entire audio frequency range.

768
CHAPTER 28 Perceptual Audio Coding
FIGURE 28.6
Decoder with joint stereo processing, consisting of two monophonic audio decoders and joint stereo decoding
that is controlled by side information.
FIGURE 28.7
Mid/Side (M/S) stereo encoding and decoding matrices.
•
Best case: M/S stereo coding is most efﬁcient for near-monophonic signals (famous example: song
“Tom’s Diner” by Suzanne Vega), since practically all information is located in the middle of the
stereo image and thus compactly concentrated in the sum channel while the difference channel
almost vanishes and thus can be coded with very few bits.
•
Coding threshold computation: Due to the M/S decoding in the decoder, the quantization noise
contributions injected into the M and S channels appear as combined M + S coding noise at the
decoder output signals which are presented to human listeners and thus need to satisfy perceptual
constraints to guarantee unimpaired audio quality. As a consequence, this needs to be accounted for
in the coding thresholds supplied to the quantization units, i.e., an “M/S stereo perceptual model”
is required [33]. Two examples for such models can be found in [33] and in Annex B of [22].

4.28.2 Concepts and Architectures
769
•
Spatial Masking Considerations: Assuming independent quantization noise contributions in both
channels, it can be seen that the “M” quantization noise contribution produces a correlated noise at
the decoder output which is spatially located in the middle of the stereo image, whereas independent
quantization in the L and R channels produce two uncorrelated noise sources which are located sharp
left and right in the stereo image, respectively. As a consequence, M/S stereo produces noise images
that are spatially hidden well “behind” sounds in the middle of the stereo image (near-monophonic
sounds) whereas the noise sources for independent coding hide well “behind” dissimilar signals
in the left and right audio channels. In this way, M/S stereo helps to obtain appropriate spatial
characteristics for the coding noise with respect to the audio signal. This is consistent with the
psychoacoustic observation of a Binaural Masking Level Difference (BMLD) [5] which describes
the reduction of masking that results from different inter-aural relations between masker and probe
signal in a two-eared (“dichotic”) presentation.
•
M/S stereo coding gain and on/off switching: As a consequence of the spatial masking issues and
the required adaption of “masking” thresholds, the gain provided by M/S stereo coding varies over
independent coding ranges from almost 50% bitrate decrease (for near monophonic signals) down
to negative numbers, i.e., M/S stereo coding can be considerably more expensive than independent
coding (at comparable subjective quality). For this reason, practical implementations of M/S stereo
coding provide an on/off switch for enabling/disabling this coding mode for each frame which can
be done either globally (for the entire frequency range) or selectively for each frequency band.
4.28.2.3.2
Intensity stereo (IS) coding
Intensity stereo exploits the fact known from psychoacoustics that auditory perception of high frequen-
cies (above approximately 4 kHz) is mainly based on signal envelopes (“intensity”) in critical bands
rather than on the actual signal waveform itself [5]. Thus, reproducing merely the envelopes for both
left and right channels of a stereophonic signal should be sufﬁcient to reproduce the spatial sound image
and timbre.
The original idea behind intensity stereo [35] had been related to compacting the two channels by
applying a main axis transform in each frequency band in order to concentrate most of the energy in a
mid channel while minimizing the energy in a side (residual) channel. [Note: Seen from this perceptive,
M/S stereo coding corresponds to a main axis transform with a ﬁxed rotation angle of 45◦.] Practically,
the variable rotation of the two channels has been replaced frequently by a simple summation of the left
and right signal and transmission of the side channel is assumed to be irrelevant (which is not always
appropriate, see below).
Figure 28.8 shows the basic signal ﬂow for intensity stereo processing within an audio codec. In the
encoder, the left and right input channel are reduced to a single one, e.g., by simple summation of the
spectral coefﬁcients. This single channel is then coded and transmitted to the decoder like a regular
monophonic sound. Additionally, information about the original intensity envelope of left and right
channel are extracted for each frequency band and transmitted to the decoder as side information, either
as absolute level information or a measure of left-to-right-relation. In the decoder, the single transmitted
audio channel is decoded into spectral coefﬁcients and left and right channel output signals are con-
structed by frequency band wise scaling for left and right according to the side transmitted information.
The goal is to provide output signals that approximate the original signal in intensity/envelope in each
frequency band as closely as possible. Seen overall, the scheme re-synthesizes a stereophonic output

770
CHAPTER 28 Perceptual Audio Coding
FIGURE 28.8
Basic Scheme of Intensity Stereo (IS) in an audio codec. In a frequency selective way, a sum signal along
with individual scaling factors for spectral bands of the left and right channels are transmitted to the decoder.
signal by frequency selective left/right level/intensity of a monophonic source, similar to the (broad-
band) “pan-pot” control that can be found on mixing desks. Although described here for two-channel
stereo, the approach can be generalized for groups of several channels [39–41].
Similar to M/S stereo coding, intensity stereo coding exhibits several distinct properties:
•
Lossy reconstruction: Due to the reduction of two audio channels into a single channel plus side
information, the scheme generally implies a loss of information even in the absence of quantization.
•
Best case: Intensity stereo is well-suited to code audio material that ﬁts well to the intensity synthesis
method in its source characteristics, e.g., most modern pop recordings which were produced in the
studio by distributing sources within the stereo image using pan-pot controls. Then, intensity stereo
could typically be used above a certain start frequency (say 4 kHz) and achieves a bitrate saving of
almost 50% over independent stereo transmission within that frequency range.
•
Worst case: Due to the associated information loss and the rather restrictive underlying psychoa-
coustic assumptions (“perception of high frequencies is dominated by their envelope”), intensity
stereo is not very well suited for high quality coding of audio signals that exhibit a signiﬁcant
amount of decorrelation between left and right channel signals leading to a wide stereo image, such
as microphone recordings of large classical orchestra. A well-known example of a particularly hard-
to-code signal type are applause recordings that consist of many distinct clap events that occur at
different time in the left and right channels (and in between), as they are known from live recordings
[42]. When applied to such signals, intensity stereo can cause serious artifacts (individual claps
sound “mashed,” stereo image becomes narrower, less convincing and instable, etc.) [27]. These
artifacts are clearly audible for headphone reproduction, and can be less obtrusive for loudspeaker
reproduction (especially in reverberant environments).
•
Intensity stereo coding control: Due to the described limitations, the use of intensity stereo coding
should be controlled carefully in practical implementations, such that the scheme is applied above
a certain start frequency only, and that the start frequency is chosen according to bitrate demand
considerations and/or the type of audio source material. Whenever bitrate constraints allow for it,
the use of intensity stereo is usually omitted.

4.28.2 Concepts and Architectures
771
4.28.2.4 Tools for coding enhancement
The generic coder/decoder architecture presented so far describes the concept behind most perceptual
audio coders that were developed in the late eighties or early nineties (e.g., MPEG-1 or MPEG-2 audio
codecs, see below). Later systems added a number of extra functions (“tools”) in order to boost the
codec performance for certain types of input signals. Some examples are:
•
Dynamic window shape adaptation: The coding gain achieved by transform coding depends on the
characteristics of the selected ﬁlter bank. It can be advantageous to dynamically adapt the window
shape of the ﬁlter bank depending on the input signal characteristics and in this way trade off between
nearby frequency selectivity and far-off rejection [43].
•
Prediction over time: For very tonal signals with a quasi-stationary period that exceeds the frame
duration, further coding gain can be achieved by applying predictive coding over time (across frames)
to the spectral coefﬁcients. In this way, redundancy can be further reduced, see e.g., [44,45].
•
Predictionacrosschannels:Similarly,thespectralcoefﬁcientsofonechannelsignalofastereo/multi-
channel input can be predicted from those of the other channel(s) and thus redundancy can be reduced
[46].
•
Prediction over frequency: For signals that exhibit signiﬁcant changes in their temporal envelope
within one coding frame it has been shown that there is signiﬁcant correlation (redundancy) between
spectral coefﬁcients that are adjacent in frequency [47,48]. This could be used to further reduce
redundancy, but is mostly used instead to better account for irrelevancy (temporal masking) by
providing a temporal shaping to the envelope of the quantization noise which follows that of the
masking signal and is thus called Temporal Noise Shaping (TNS) [47,49].
•
Perceptual Noise Substitution: A compact and perceptually close way of describing audio signals
can be achieved by a parametric coding of groups of spectral coefﬁcients that represent noise-like
signals. In the decoder, such spectral values are substituted by pseudo random noise, thus the name
Perceptual Noise Substitution (PNS) [50].
4.28.2.5 Bandwidth extension
In perceptual audio coding based on ﬁlter banks, the main part of the consumed bitrate is usually spent
on the quantized spectral coefﬁcients. Thus, at very low bitrates, not enough bits may be available to
represent all coefﬁcients in the precision required to achieve perceptually unimpaired reproduction.
For such bitrates, good encoder implementations usually intentionally limit the number of spectral
coefﬁcients to be transmitted and thus the encoded audio bandwidth. By doing so, distortions due to
overly coarse quantization are avoided, which can perceptually be much more disturbing than a static
bandwidth limitation [27]. Thus, low bitrate requirements effectively set a limit to the audio bandwidth
that can be obtained by perceptual audio coding.
Bandwidth extension removes this longstanding fundamental limitation. In Figure 28.9, the signal
ﬂow in a generic audio codec with a bandwidth extension scheme is outlined. The central idea is to
complement a band-limited perceptual codec by an additional high-frequency processor that transmits
and restores the high-frequency content that is not carried by the codec in a compact parametric form.
In the encoder, the characteristics (time/frequency dependent envelope and other aspects) are measured
from the high frequency input signal prior to band limited encoding and sent to the decoder as side
information along with the bitstream of the band limited base band signal. In the decoder, the band

772
CHAPTER 28 Perceptual Audio Coding
FIGURE 28.9
Generic scheme of bandwidth extension in an audio codec. A band-limited perceptual codec is comple-
mented by an additional high-frequency processor that transmits and restores the high-frequency content in
a compact parametric form.
limited audio signal is decoded and used as input for the bandwidth extension processor to reconstruct
a high frequency part according to the transmitted side information. Finally, the band limited base
band signal and the reconstructed high frequency part are combined to form a full-bandwidth audio
output signal.
Taking a closer look at bandwidth extension, a considerable share of the beneﬁt of the scheme comes
from exploiting three factors, i.e.,
•
the commonalities between base band signal and high frequency part, such as ﬁne temporal envelope
structures, noise-like vs. tone-like structure,
•
the high perceptual tolerance of the human auditory system against alterations of the actual waveform
at high frequencies, as it was also exploited by intensity stereo coding (see above), and
•
the extremely compact representation of the high frequency part in the side information.
The ﬁrst successful scheme for bandwidth extension in perceptual audio coding was Spectral Band
Replication (SBR) [51] which was subsequently standardized by the ISO/MPEG standardization group
and became an important part of the family of MPEG High-Efﬁciency Advanced Audio Coding
(HE-AAC) codecs, see section on audio coding standards. SBR is a practical scheme that offers high
audio quality in combination with moderate computational complexity and has the following charac-
teristics [51,52]:
•
Time/frequency analysis/synthesis: For the analysis of the original sound in the encoder, and for
the generation of the high-frequency enhanced output signal in the decoder, a 64-channel Pseudo
Quadrature Mirror Filter (QMF) bank pair is used. The ﬁlter banks are complex-valued and two
times oversampled and thus allow manipulation of the signal’s magnitude and phase with high time
resolution and minimal aliasing. Note that this would have been difﬁcult to achieve by re-using the
coder ﬁlter bank, as regular joint stereo tools have been designed to do owing to constraints on
computational complexity at the time they were conceived.
•
Transposition: The central part of the algorithm that generates the raw high frequency part in the
decoder from the transmitted base band part is called “transposition” since it shifts up spectral
content from certain regions of the base band into the high frequency band in the style of a single
sideband modulation (with suppressed carrier). In practice, this is achieved in an extremely simple

4.28.2 Concepts and Architectures
773
and efﬁcient fashion by just copying the spectral coefﬁcients for a certain source frequency range
up into the target (high frequency) range. Naturally, this processing preserves temporal envelope
modulations that are present in the source spectrum.
•
Envelope adjustment: In order to shape the time/frequency envelope of the ﬁnal synthesized high fre-
quency signal to match the original high frequency part as close as possible, the raw high frequency
signals from the output of the transposer are adjusted according to the transmitted side information.
Therefore, the decoder envelope adjustment process uses a ﬂexible scheme for time/frequency seg-
mentation that adapts to the characteristics of the audio signal for optimal signal quality. This side
information can be transmitted very compactly.
•
Other functions: Besides the adjustment of the time/frequency envelope, the SBR scheme is also
equipped with a number of other functions to further improve the subjective quality of the decoded
audio signal. If the original high-frequency part of the signal is noise-like while the base band signal
is rather tonal, it is possible to reduce the tonal character of the transposed audio signal by an LPC-
like ﬁltering process. Conversely, if the original high-frequency part is tonal while the base band is
rather noise-like, it is possible to insert sinusoids into the synthesized output. For both cases, the
control parameters are carried in the side information.
More recently, other schemes for bandwidth extension in generic audio coders have been success-
fully developed as part of new state-of-the-art audio coding schemes which address the limitations of
the SBR process by deﬁning more sophisticated processing schemes, although at the price of higher
computational complexity [53,54]. One major aspect of this work is that it is designed to provide a con-
sistent continuation of the harmonic structure of tonal harmonic audio signals, which is not maintained
by SBR.
By its very nature, bandwidth extension schemes can be considered semi-parametric coding methods
because they rely on both the transmitted parametric information about the original high frequency signal
part and on the information present in the (conventionally coded) base band signal. They can provide
a signiﬁcant boost in coding performance at low bitrates (e.g., below 48 kbit/s per audio channel) and
have been adopted widely to become a common part of many state-of-the-art coders, providing full
audio bandwidth even at such low rates [55].
As another consequence of the basic approach, coders equipped with bandwidth extension do not
aim at approximating the original signal’s waveform anymore, but instead allow for a totally different
waveform to be synthesized as high frequency signal in the interest of bitrate efﬁciency. This leads to
two further consequences:
•
Whileuseofbandwidthextensionprovidesasigniﬁcantincreaseinthesubjectivequalityofthecoded
signal at low bitrates, it generally does not achieve perceptually fully unimpaired (“transparent”)
coding quality. Thus, common codec implementations shift the cross-over frequency between base
band coding and high frequency synthesis towards higher frequencies, as more bitrate is selected
by the user. Finally, if sufﬁcient bitrate is available for full-quality encoding, bandwidth extension
is switched off entirely.
•
Because of the non-waveform preserving approach, codecs that employ bandwidth extension fre-
quently present problems to perceptual measurement algorithms that attempt to predict subjective
quality from a pair of signals (original and encoded/decoded signal), such as the well-known Percep-
tual Evaluation of Audio Quality (PEAQ) scheme [11]. Typically, an increase in predicted quality

774
CHAPTER 28 Perceptual Audio Coding
degradation is observed for bandwidth extension based coders as compared to its actual quality, as
it would result from a subjective listening test. This effect has been recognized and has triggered
work on enhanced measurement schemes that are suitable also for this purpose. Therefore, the use
of current perceptual measurement schemes for assessing the merits of enhancements in audio cod-
ing frequently leads to invalid results and is discouraged if bandwidth extension methods (or other
parametric coding techniques) are involved.
Methods for bandwidth extension for speech signals have been the subject of research for a consider-
able period of time, see for instance [56,57]. These methods can exploit the properties of human speech
through a dedicated source model. Such methods typically use different techniques that process the
signal in the time domain. In contrast to this, bandwidth extension for generic audio signals including
music can hardly make any assumptions on the nature of the signal to be processed.
Finally, bandwidth extension of audio signals can also be done in a single-sided manner, i.e., without
using encoder-derived side information. Such blind bandwidth extension schemes for coding purposes
have been mostly used in the area of speech processing [56,57].
4.28.2.6 Parametric multi-channel coding/spatial audio coding
As a next step in exploiting knowledge about human auditory perception for audio coding, a new
approach in perceptual coding of multi-channel audio emerged. The concept of parametric multi-channel
coding,sometimesalsoreferredtoas“SpatialAudioCoding”(SAC)[58],extendstraditionalapproaches
for joint intensity stereo coding of two or more channels in a way that provides signiﬁcant advantages
in terms of sound quality and ﬂexibility. Firstly, it enables the transmission of high-quality multi-
channel audio signals at bitrates that previously have been used for the transmission of one or two
channel signals. Secondly, by its underlying structure, the transmitted multi-channel audio signals are
transmitted in a backward compatible way, i.e., spatial audio coding technology can be used to upgrade
existing distribution infrastructures for stereophonic or monophonic audio content (radio channels,
Internet streaming, music downloads, etc.) towards the delivery of multi-channel audio while retaining
compatibility with existing receivers.
Like bandwidth extension of audio signals, Spatial Audio Coding leaves the domain of waveform
coding and instead focuses on delivering a perceptually satisfying replica of the original spatial sound
image. This is only possible by exploiting knowledge about the human perception of a spatial sound
scene, as it is known from psychoacoustics [5]. An essential part of the sound scene perceived by a human
listener is determined by differences between the listener’s ear signals (so called inter-aural differences)
regardless of whether the scene consists of real audio sources or whether it is reproduced via two or
more loudspeakers projecting phantom sound sources (as depicted in Figure 28.10). More speciﬁcally,
the perceptually most relevant cues for determining the lateral position and width of an auditory object
are Inter-Aural Level Difference (ILD), Inter-Aural Time Difference (ITD) or, equivalently, Inter-Aural
Phase Difference (IPD), and Inter-Aural Correlation/Coherence (IC) [5]. Each type of cue is to be
considered in a frequency selective way, i.e., related to the frequency selectivity inherent in the human
hearing process.
The general concept of SAC is as follows. Instead of discretely encoding the individual audio input
channel signals, a system based on SAC captures the spatial image of a multi-channel audio signal into

4.28.2 Concepts and Architectures
775
FIGURE 28.10
Sound scene containing a source. The associated inter-aural spatial cues are the differential properties
of the binaural signals that arrive at the left and the right ear of a listener, e.g., level differences, time
differences and correlation. Such cues are present both for natural sound sources and for stereophonic
sound reproduction.
a compact set of parameters that can be used to synthesize a high quality multi-channel representation
from a transmitted downmix signal. Figure 28.11 illustrates this concept. During the encoding process,
the spatial parameters (cues) are extracted from the multi-channel input signal. These parameters typ-
ically include important spatial cues like Inter-Channel Level Difference (ICLD), Inter-Channel Time
Difference (ICTD), and measures of Inter-Channel Correlation/Coherence (ICC) between the audio
channels. These cues can be represented in an extremely compact way. At the same time, a monophonic
or two-channel downmix signal of the sound material is created and transmitted to the decoder (e.g.,
by traditional audio coding methods) together with the spatial parameter information. On the decoding
side, the transmitted downmix signal is expanded back into a high quality multi-channel output based on
the spatial parameters. Compared to the encoder downmix/parameter extraction, this is a sophisticated
process, as it implies a high-quality synthesis of the target cues.
Along this concept, a number of successful schemes have been developed over time, including Bin-
aural Cue Coding (BCC). BCC [59,60] is a multi-channel scheme using a mono downmix which was
subsequently extended [58] for two-channel downmix. Parametric Stereo (PS) [61] is a standardized
scheme that folds two channels into one downmix channel and back again into two (see section on
standards). MP3 Surround [62] is a scheme that extends the popular mp3 stereo codec for 5.1 surround
sound while keeping backward compatibility with the mp3 bitstream format. MPEG Surround (MPS)
[63,64] is an extremely versatile standardized scheme that encodes n ≥2 input channels into one or
two downmix channels and side information and offers a plethora of additional features like binau-
ralized decoding for multi-channel playback over headphones. All these schemes share the following
characteristics:
•
Time/frequency analysis/synthesis: Like schemes for bandwidth extension, spatial audio coding
algorithms usually use a dedicated analysis/synthesis ﬁlter bank pair rather than sharing the ﬁlter
bank contained in the audio coder. However, efﬁcient combined implementations of bandwidth
extension and parametric multi-channel audio coding can make use of a common ﬁlter bank (e.g.,
HE-AAC v2 codec, see Section 4.28.3.5).

776
CHAPTER 28 Perceptual Audio Coding
FIGURE 28.11
Basic spatial audio coding (SAC) scheme. In the encoder, spatial parameters (cues) are extracted from the
multi-channel input signal and one or several downmix signals are created and transmitted to the decoder
together with the spatial cue information. On the decoding side, the transmitted downmix signal is expanded
back into a high quality multi-channel output based on the spatial cues.
•
Cue synthesis: Unlike intensity stereo coding (which is restricted to synthesizing inter-channel
level/intensity cues at its output), these schemes also provide inter-channel coherence/correlation as
the second most important cue, i.e., they can provide decorrelated output signals which are vital for
properly representing sound material with a wide stereo image, such as classical orchestra music.
Furthermore, also inter-channel time time/phase cues are generated by some schemes. While these
are also relevant for the imaging of sound sources for headphone reproduction, they are weak cues
for loudspeaker reproduction, especially in reverberant environments.
•
Frequency range: Spatial audio processing is applied to the full audio frequency range rather than
only the high frequency range as is the case for intensity stereo or bandwidth extension processing.
•
Quality: Due to the ability to synthesize a much wider range of auditory cues that are relevant to
spatial audio perception, spatial audio coding schemes generally produce much higher sound quality
than simple intensity stereo techniques. Especially the ability to generate decorrelated output signals
gives rise to a signiﬁcant increase in audio quality for signals with a wide and enveloping sound
stage. Nonetheless, applause recordings still present a challenge to spatial audio coding schemes due
to their ﬁne-granular nature, and new approaches to overcoming this obstacle have been proposed
[42,65].
Furthermore, similar notes apply as for bandwidth extension processing:
•
Due to its parametric nature, spatial audio coding is not waveform preserving. As a consequence, it
is hard to achieve fully unimpaired quality for all types of audio signals. Nonetheless, spatial audio
coding is an extremely powerful approach that provides substantial gain at low and intermediate
bitrates. In order to overcome the limitations that come along with the parametric approach and to
extend the quality all the way towards perceptual “transparency” at the expense of additional bitrate,
so-called residual coding techniques have been added to the MPEG Surround scheme. With such an
addition, the scheme becomes waveform preserving again and can scale to very high quality [63].

4.28.2 Concepts and Architectures
777
•
Unless operated substantially in residual coding mode, spatial audio coders rely on parametric signal
reconstruction. Thus, attempts to predict the subjective audio quality of these systems by current
perceptual measurement schemes are likely to produce invalid results.
4.28.2.7 Parametric coding of audio objects
Besides the efﬁcient representation of several audio channels, which are intended for reproduction on a
dedicated loudspeaker setup, also the idea of efﬁcient and interactive representation of audio scenes has
been the subject of active research and development. Considerable time ago, the notion of representing
audiovisual material (a “scene”) by a number of single audiovisual objects plus a scene description
has been conceived as one of the basic concepts behind the MPEG-4 standard [66,67]. For audio,
objects typically consist of dry individual sources (like individual instruments on a multi-track studio
recording), and a scene description deﬁnes how these object signals are combined (level, spatial position,
acoustic environment, etc.) to form the ﬁnal composite audio output. Transmission of such a scene can
be achieved in a straightforward way by discrete transmission of separately encoded objects. Each
object is decoded and rendered according to the scene description and/or the user input. The approach
provides content-based interactivity as a key functionality since the user has the freedom to modify the
mix according to his/her liking by inﬂuencing both object rendering parameters (level, position, etc.) and
scene parameters (e.g., reverberation) on the receiving terminal. This delivers an ideal framework for
interactive applications like Karaoke, play-alongs, personalized audio mixes, dialogue level adjustment
for movies, etc.
In practice, the described approach suffers from several disadvantages regarding the resources
required for its implementation:
•
High bitrate demand: Due to the separate encoding of each sound object, the required bitrate for the
transmission of the whole scene is signiﬁcantly higher than rates used for monophonic/stereophonic
transmission of compressed audio.
•
High computational complexity: Due to the separate decoding of each sound object, the computa-
tional complexity for the decoding process signiﬁcantly exceeds that of regular mono/stereo audio
decoders.
•
High structural complexity: Since the total system involves several audio decoder components plus
a composition unit, its structural complexity is an obstacle to implementation in many real-world
applications.
In order to obtain (most of) the functionality of discrete object-based coding in an efﬁcient and low-
complexityfashion,theconceptofspatialaudiocoding,asdiscussedpreviously,canbeextended[60,68–
70]. This is illustrated in Figure 28.12. Rather than coding individual loudspeaker channel signals, an
object-oriented spatial audio coding (Spatial Audio Object Coding, SAOC) processes individual audio
object (source) signals, extracts relevant side information from them, e.g., Object Level Difference
(OLD) and Inter-Object Cross Correlation (IOC), and produces a downmix (e.g., one or two audio
channels). Both downmix and side information are transmitted from the encoder to the decoder side.
In the decoder, approximations of the individual objects are recovered from the downmix and the
side information. Rather than presenting these signals, however, they are passed into a rendering stage
generating an output for the desired number of output channels and speaker setup. The parameters of

778
CHAPTER 28 Perceptual Audio Coding
FIGURE 28.12
Alternative (“object-oriented”) Spatial Audio Object Coding (SAOC) processes individual audio object signals,
extracts relevant object side information and produces one or more downmix signals. Both the downmix
signals and the side information are transmitted from the encoder to the decoder side. In the decoder,
approximations of the individual objects are recovered from the downmix and the side information and
rendered for playback on the speciﬁc speaker or headphone setup.
the renderer can be varied according to user interaction and thus enable real-time interactive audio
composition.
This approach has attractive properties in terms of implementation resources needed:
•
Compression efﬁciency: Similarly to spatial audio coding, the transmission is extremely bitrate-
efﬁcient through the use of a downmix signal plus accompanying parametric side information. If the
majority of the bits is spent for coding the downmix channels (rather than the side information), the
overall bitrate is essentially independent of the number of objects transmitted.
•
Backward compatibility: Similarly to spatial audio coding, the approach is backward compatible with
respect to existing transmission structures, i.e., existing decoders may simply present the downmix
signal ignoring the SAOC side information. Content originating from existing legacy encoders can
be considered by SAOC-enabled decoders as a stream containing merely one object.
•
Computational complexity: Combining spatial audio decoding and rendering into a single integrated
step is extremely attractive because it leads to very low implementation complexity. Explicit recon-
struction of the object signals as an intermediate step can be avoided in favor of a direct transcoding
from the downmix into the intended rendered output scene and thus the necessary computation is
mainly related to the number of intended output rendering channels. In this way, the computational
complexity is essentially independent from the number of objects in the scene.
Technically speaking, the underlying concept is similar to that of spatial audio coding in that the
decoder/renderer is designed to reinstate the cues for correct spatial reproduction, such as Inter-Channel
Level Difference (ICLD) and Inter-Channel Coherence/Correlation (ICC). To this end, the inter-channel
cues have to be estimated in the decoder/renderer from two factors:
•
Properties of the objects (for instance, the spectral envelope of each audio object). These are trans-
mitted as side information in the bitstream.

4.28.2 Concepts and Architectures
779
•
Rendering parameters for each object (for instance, parameters determining which object is rendered
to which output channel with what gain factor). These are set by the user for the intended reproduction
setup.
It is interesting to note that the underlying SAOC data representation (downmix and side information)
is entirely agnostic of the reproduction speaker setup and can be rendered at will to arbitrary loudspeaker
setups or headphone playback.
Again, like bandwidth extension and spatial audio coding, the approach is parametric and similar
notes apply as for spatial audio coding regarding subjective audio quality and use of current perceptual
measurement schemes.
4.28.2.8 Scalable coding
In the discussions so far it has been assumed that encoding of an audio signal is carried out at a certain
bitrate (be it ﬁxed or variable rate) and the decoding process has access to the generated bitstream in its
entirety, i.e., decoding happens at the same bitrate as encoding. There are, however, application scenarios
for which the available bandwidth of the transmission channel and thus the bitrate limit of the codec are
not known in advance at the time of encoding. Examples for such transmission channels with varying
capacity are the real-time transmission over the Internet (as it happens for streaming audio/Internet
radio) or wireless channels (broadcasting, streaming) where temporary disturbance of reception leads
to a decrease in channel bandwidth/available net data rate after channel decoding. In order to make best
possible use of such transmission channels, it would be necessary to either perform real-time encoding
at the time of transmission, considering the current instantaneous channel state or have a representation
of the audio signal that can simply be “stripped down” (reduced in its rate) starting from a full-rate
representation. While the ﬁrst scenario is rather unattractive due to the computational resources required
for online encoding, the second scenario has been the subject of a considerable body of research known
under as scalable coding [71–75].
An audio coding representation is called scalable if it has the possibility for decoding of a reduced-
rate subset of the bitstream information in a useful way, albeit at reduced audio quality. (Note that in
the area of speech coding the term bitrate scalable has also been used to simply denote the possibility
of encoding at several different bitrates.)
An important special case of scalable coding is called hierarchical (de)coding, i.e., the decoding of the
signal at certain bitrates is subject to a priority ordering (hierarchy) of different bitstream elements. When
scalingdownthebitratestartingfromafull-raterepresentation,thereductionstepshappeninaprescribed
order at certain parts of the full-rate representation. Sometimes this approach is also called embedded
coding because the lower-rate representations are embedded into the higher-rate representations. Also
the term layered (de)coding has become common considering the ordered parts of the representation as
layers which carry different portions of the overall information, rate and quality.
Typically layered/embedded coding can be done in two complementary ways:
•
“Bottom-up scalability” (also sometimes called large-step scalability [67]): Starting from a lowest
layer (sometime called base layer), which is the lowest rate representation, subsequent enhancement
layers are used to correct the coding error remaining from previous layers, in this way increasing
the bitrate with each additional layer.

780
CHAPTER 28 Perceptual Audio Coding
•
“Top-down scalability” (also sometimes called ﬁne-grain scalability [67]): Starting from a full-
quality full-rate representation, more and more parts of the representation can be dropped subse-
quently in small increments to scale down in rate and quality.
Naturally, the added ﬂexibility provided by scalable coding comes at the price of coding efﬁciency.
Scalable coding imposes certain elements of representational overhead and sub-optimality relative
to a non-scalable encoding carried out for a known target data rate. Corresponding to their nature,
both approaches to scalability exhibit complementary characteristics in their performance penalty. For
bottom-up scalability there is no performance penalty on the lowest layer. With each added layer,
however, the efﬁciency penalty increases compared to non-scalable coding. For top-down scalability,
on the other hand, there is no penalty on the full-rate representation. As more and more downscaling is
carried out, however, an increasing penalty occurs compared to non-scalable coding [76].
4.28.3 Standards
For the progress in the ﬁeld of audio coding, international standards have always played a crucial
role from the very beginning at the end of the nineteen eighties. Starting from the ﬁrst audio coding
standard, these standards have been continuing to deﬁne the state of the art for more than two decades.
Most inﬂuential in both their research aspect and in their commercial impact, the standards issued by
the ISO/MPEG (Moving Picture Experts Group) now comprise several generations of successful audio
coding schemes as well as coding-related processing.
This section will introduce some of the relevant standardized technologies in a brief way with special
attention to the ISO/MPEG technologies, as these are widely deployed in many consumer electronic
or communication devices, while fully acknowledging the presence of other proprietary coders which
have been successful in the marketplace for some considerable time (e.g., the “Dolby Digital” family
[41,77] as well as the work of other standardization bodies, such as the International Telecommunication
Union-Telecommunication (ITU-T).
4.28.3.1 MPEG-1
In 1992, MPEG ﬁnalized its ﬁrst International Standard on coding of moving pictures and associated
audio, called MPEG-1 [20]. The audio part of this standard (ISO/IEC 11172-3) speciﬁes a generic stan-
dard for the coding of general audio signals (as opposed to speech signals in telecommunications/speech
applications) and became the driving technology for many important applications relying on audio com-
pression [78]. The standard deﬁnes three operating modes (“layers”) in order to meet the requirements
of a broad range of applications of audio coding concerning target bitrate and complexity:
•
The Layer-2 coding scheme deﬁnes a codec with medium compression performance and compu-
tational complexity, which had its ﬁrst applications in the broadcasting environment, including use
for the European Digital Audio Broadcasting (DAB) system. The codec has been shown to achieve
broadcast quality at a bitrate of 128 kbit/s per channel. It has become part of several application
speciﬁcations, such as the Video Compact Disc (VCD), the European Digital Audio Broadcasting
(DAB) and Digital Video Broadcasting (DVB) systems.

4.28.3 Standards
781
•
Relative to this, the Layer-1 coding scheme represents a simpliﬁed version of the Layer-2 coder with
both lower computational complexity and compression capability. The intended major application
for this scheme was digital recording of audio for consumers (the “Digital Compact Cassette,” DCC)
but no major deployment was achieved for this codec. Broadcast quality is reached at a bitrate of
192 kbit/s per channel.
•
In contrast, the Layer-3 coding scheme was designed to provide best possible compression perfor-
mance by more reﬁned and computationally demanding coding techniques. The audio transmission
via ISDN channels and satellite radio were the ﬁrst major applications of this codec. Widely known
under the name “mp3,” the Layer-3 codec has become over the years the world’s most popular codec
in the context of Electronic Music Distribution (EMD), Internet radio and audio/video streaming,
but has also been used for satellite broadcasting (WorldSpace). The codec has been shown to provide
broadcast quality at a bitrate of 96 kbit/s per channel, although it has been commonly used at lower
bitrates as well.
4.28.3.2 MPEG-2
While the coding schemes provided by MPEG-1 Audio were designed to meet the needs of most
high-quality applications, more capabilities were added to the existing three Layer coder family by the
MPEG-2 Audio standard [21] which was ﬁnalized in 1994. This standard provides the following two
types of extensions:
•
Low Sampling Rates (LSR): These straight-forward extensions expand the set of standard sampling
rates supported by MPEG-1 Audio (i.e., 32 kHz, 44.1 kHz, and 48 kHz) by their half rate counterparts
(i.e., 16 kHz, 22.05 kHz, 24 kHz). While this leads to clear limitations in the obtainable audio
bandwidth, it comes with both an increase in coding efﬁciency for low bitrates and decreased
computational demands. These factors were deemed vital for introducing multimedia on personal
computers at the time of the development of the standard. Typically, MPEG-2 low sampling rate
coding is used in multimedia applications, e.g., in desktop PCs where the best possible quality may
not be of utmost concern.
•
Multi-channel audio coding: This extension deﬁnes a set of coded multi-channel audio formats,
most prominently for the so-called 5.1 loudspeaker setup indicating reproduction by ﬁve full-range
loudspeakers and a Low Frequency Effects (LFE) channel. Multi-channel counterparts were deﬁned
for the entire family of MPEG-1 audio coders (i.e., Layers 1–3) in a backward compatible way.
MPEG-2 multi-channel audio bitstreams can be decoded by MPEG-1 audio decoders, resulting
in reproduction of a complete stereophonic downmix of the multi-channel sound image [39,40].
Due to the technical constraints that were imposed on the codec performance by the requirement
for backward compatibility, however, the MPEG-2 multi-channel audio coders did not achieve
widespreaddeploymentandledthewaytothesubsequentfamilyofMPEG-2andMPEG-4Advanced
Audio Coding schemes.
Table 28.1 provides a synopsis of some key parameters for the three layers of the MPEG-1 and
MPEG-2 coders, as well as for the AAC coder (see further below). In this table, the term granule
denotes the basic unit used for the en/decoding process (block), whereas the term frame refers to a
complete bitstream unit that can be addressed and decoded per se.

782
CHAPTER 28 Perceptual Audio Coding
Table 28.1 Synopsis of Key Parameters for the Three Layers of the MPEG-1 and MPEG-2 Coders and for the AAC Coder
MPEG-1/2
MPEG-1/2
MPEG-1/2
MPEG-2/4
Layer1
Layer2
Layer3 (“mp3”)
AAC
Filter bank
type
Polyphase ﬁlter
bank
Polyphase ﬁlter
bank
Hybrid ﬁlter bank
(Polyph.+MDCT)
MDCT
Filter bank
resolution
[bands]
32
32
192/576
(switchable)
128/1024
(switchable)
Granule size
[samples]
384
384
576
1024
Frame size
[samples]
384
1152
1152
1024
Frame size
[bits]
Fixed
Fixed
Variable (Bit
reservoir)
Variable (Bit
reservoir)
Quantization
scheme
Uniform
Uniform
Non-uniform power
law x3/4
Non-uniform power
law x3/4
Coding
scheme
Block companding
Block
companding
and grouping
Huffman coding
Huffman coding
M/S stereo
coding
n/a
n/a
Yes (global on/off)
Yes individually
switched per band
Intensity
stereo coding
Yes
Yes
Yes
Yes, incl. coupling of
several channels
Number of
audio
channels
1…2a
1…2a
1…2a
1…48
a For MPEG-2 backward compatible multi-channel extension: 5.

4.28.3 Standards
783
4.28.3.3 MPEG-2 AAC
After the completion of the MPEG-2 multi-channel audio standard in 1994, the MPEG Audio standard-
ization group initiated another effort to deﬁne a multi-channel coding standard that should overcome
the limitations imposed by requiring MPEG-1 backward compatibility. This development effort, ini-
tially named “MPEG-2 non-backwards compatible coding” (MPEG-2 NBC) [43], was targeted to reach
broadcast quality according to International Telecommunication Union-Radiocommunication (ITU-R)
requirements at bitrates of 384 kbit/s or lower for ﬁve full bandwidth channel signals. The standardiza-
tion process was successfully completed in 1997 and its results is known as “MPEG-2 Advanced Audio
Coding” (MPEG-2 AAC), which became an addendum to the MPEG-2 standard (ISO/IEC 13818-7)
[22]. While initially developed as a multi-channel audio coder, further tests conﬁrmed the coding per-
formance of the new scheme for stereophonic and monophonic signals as well, achieving broadcast
quality at a bitrate of 64 kbit/s per channel.
Compared to the MPEG-1/2 Layer-3 (mp3) codec, which largely follows the simple generic coding
scheme shown in the “Concepts and Architectures” section shown previously, MPEG-2 AAC exhibits
a number of enhancements to increase its coding performance and provide for greater versatility [43]:
•
Finer frequency resolution for quantization noise shaping.
•
Entropy coding (Huffman coding) is also applied for the coding of side information.
•
The shape of the ﬁlter bank window can be adapted dynamically to best match the characteristics of
the input signal (see Section 4.28.2.2.1).
•
Temporal Noise Shaping (TNS) is used to achieve a ﬁne shaping of the quantization noise envelope
over time (see Section 4.28.2.4).
•
For certain ﬂavors of MPEG-2 AAC (Sampling Rate Scalable proﬁle), also a gain control tool is
available for temporal noise shaping.
•
For certain ﬂavors of MPEG-2 AAC (Main proﬁle), also an intra-channel backward-adaptive pre-
diction tool is available to increase the coding performance for long-time stationary tonal signals
(see Section 4.28.2.4).
•
Sum/Difference (M/S) stereo coding is applied to channel pairs attributed to loudspeakers that are
positioned symmetrically with the median plane of the listener (“left/right pairs”). M/S stereo coding
can be activated in a frequency selective fashion [79] (see Section 4.28.2.3.1).
•
Possibility of performing intensity stereo coding between groups of channels [79] (see
Section 4.28.2.4).
•
Coding of up to 48 audio channels plus up to 16 Low Frequency Enhancement (LFE) channels.
4.28.3.4 MPEG-4
In parallel to MPEG-2 AAC, the MPEG-4 development process had been started in 1994/1995 and
lead to a ﬁrst version of the audio standard in 1999 [80] as well as a number of “Version 2” functional
extensions in 2000 [81]. Compared to previous MPEG coding standards, the goals of MPEG-4 clearly
go beyond just achieving higher coding efﬁciency. More speciﬁcally, MPEG-4 was conceived as a set
of interoperable technologies offering solutions to practically any application scenario (“universality”)
ranging from extremely low bitrates to studio-quality applications. Since no single coding scheme
could serve all these cases equally well (which is particularly true in the area of audio coding), the ﬁrst

784
CHAPTER 28 Perceptual Audio Coding
generation of MPEG-4 Audio includes deﬁnitions of the following technologies:
•
A General Audio coder which adheres to the traditional ﬁlter-bank-based coder model and is built
around MPEG-2 AAC (i.e., MPEG-4 AAC).
•
A family of narrow-band and wide-band speech coders based on the common Code Excited Linear
Prediction (CELP) structure.
•
Parametric coders for the coding of speech, Harmonic Vector Excitation Coding (HVXC) [82], and
music, Harmonic and Individual Lines and Noise (HILN) [83], at very low bitrates, respectively.
•
A scheme for representing arbitrary synthetic sounds by means of an extremely ﬂexible synthesis
language called Structured Audio (SA).
As suggested by its name (“Generic Coding of Audiovisual Objects”), MPEG-4 treats audiovisual
content as a set of objects rather than a ﬂat representation of the entire audio-visual scene (“object-based
representation”). The relation of the coded objects with respect to each other and the way to construct
the scene from these objects (“composition”) is described by a scene description.
The combination of object-based representation and scene description/composition allows for new
capabilities. At the time of presentation, the user can interact with the coded objects and control the
way they are rendered by the composition unit. Examples for this so-called content-based interactivity
include omitting the reproduction for certain objects (Karaoke) or controlling their scene composition
parameters, such as spatial coordinates and reproduction level, etc. (see also section on parametric
coding of audio objects).
MPEG-4 introduced the concept of scalable coding, enabling the transmission and decoding of a
scalable bitstream with a bitrate that can be adapted to dynamically varying requirements, like the
instantaneous transmission channel capacity (see also section of scalable coding). This capability offers
signiﬁcant advantages for transmitting content over channels with a variable channel capacity (e.g.,
Internet, wireless transmission) or connections for which the available channel capacity is unknown at
the time of encoding.
More aspects of coding are addressed by additional functionalities, such as low delay general audio
coding or error resilience. Furthermore, the MPEG-4 speciﬁcation was continually amended to include
latest technology advances for many years, such as parametric coding of wideband signals, bandwidth
extension for the MPEG-4 AAC codec, and MPEG-4 lossless audio coding including Audio Lossless
Coding (ALS) and Scalable Lossless Coding (SLS).
For a comprehensive overview of MPEG-4 and its technologies the reader is referred to, for instance,
[67,84].
In the following, the most prominent and relevant audio coding speciﬁcations within the MPEG-4
standard are brieﬂy discussed.
4.28.3.5 MPEG AAC family (MPEG-2/4 AAC, HE-AAC, HE-AAC v2 and MPEG-D
USAC)
Compared to MPEG-2 AAC, MPEG-4 AAC exhibits a number of enhancements to further increase its
coding performance:
•
Perceptual Noise Substitution (PNS) is available as a tool for efﬁcient coding of noise-like signal
components (see Section 4.28.2.4).

4.28.3 Standards
785
•
For a certain variant of MPEG-4 AAC (AAC-LTP), also an intra-channel Long-Term Prediction
(LTP) tool is available to increase the coding performance for long-time stationary tonal signals (see
Section 4.28.2.4).
•
The regular quantization and coding units of MPEG-4 AAC can be replaced by the so-called TwinVQ
vector quantization based tool [25,85] to provide enhanced performance at very low bitrates, i.e.,
between 6 and 16 kbit/s per channel.
•
For certain ﬂavors of MPEG-4 AAC, an error resilient bitstream syntax is used in order to minimize
the impact of transmission errors on the audio quality [86].
Furthermore, two types of bitrate scalable MPEG-4 AAC coders have been deﬁned (see also
Section 4.28.2.8 for background information on bitrate scalability):
•
MPEG-4 coders with large-step scalability combine several layers of coding information, starting
from a lower-quality base layer and adding one or more layers of enhancement information. The
base layer can either be an AAC-type coder or a CELP-type speech coder [87]. Enhancement layers
are always AAC.
•
MPEG-4 coders with ﬁne-grain scalability start at a full-rate full-quality representation and scale
downsubsequentlyinbitrateandqualitybygradualtruncationofthebitstreamfromitsbackend.This
functionality is achieved by an MPEG-4 AAC coder with a dedicated coding kernel that replaces the
regular Huffman-based coding of the quantized spectral coefﬁcients and associated side information.
The Bit-Sliced Arithmetic Coding (BSAC) kernel [74] represents the quantized spectral coefﬁcients
as slices of binary values, which are ordered according to their estimated perceptual importance
within the bitstream. Also one type of MPEG-4 lossless coding (MPEG-4 SLS) follows this generic
principle, see Section 4.28.3.7.
Among the numerous conﬁgurations of MPEG-4 AAC, one family of algorithms had considerable
success in the marketplace and is discussed subsequently. This family has been developed further with
each new technical advance, starting from a simple MPEG-4 AAC baseline. Extensions were made such
that each subsequent coder generation provided a signiﬁcant performance gain over its predecessor by
inclusion of a new coding tool. Representing a superset of the preceding technology, each new generation
ensured full backward compatibility to bitstreams of the previous generation.
The historical evolution of the MPEG-4 AAC coder family is outlined below:
•
The initial member of the family is (non-scalable) MPEG-4 AAC-LC (where LC stands for “low
complexity”), as it was deﬁned in a ﬁrst version of the MPEG-4 audio speciﬁcation in 1999 [80]. This
codec constitutes a superset of the basic MPEG-2 AAC codec and is in fact backward compatible on
the raw bitstream level, i.e., its decoder can decode MPEG-2 AAC-LC bitstreams. MPEG-4 AAC-LC
has received wide deployment e.g., as the format of Apple’s iTunes music download business and
is implemented on many portable devices including iPods/portable music players, iPhones/cellular
phones/Personal Digital Assistants (PDAs) and iPads.
•
As a next step, MPEG-4 AAC-LC was combined with the Spectral Band Replication (SBR) tool in
a ﬁrst extension of the MPEG-4 Audio speciﬁcation [88] in 2003 (see Section 4.28.2.4) which led to
the MPEG-4 High-Efﬁciency AAC (HE-AAC) codec. This codec provides a bitrate saving of more
than 25% at low bitrates (24 kbit/s per channel) compared to MPEG-4 AAC-LC by representing
the signal’s high frequency range in a semi-parametric way [55]. It has been adopted in many

786
CHAPTER 28 Perceptual Audio Coding
speciﬁcations for digital broadcasting, both satellite and terrestrial, music download on mobile
devices.
•
As a third step, the High-Efﬁciency AAC codec was extended by a so-called Parametric Stereo (PS)
tool (see Section 4.28.2.6), which had been developed in the context of the second amendment of
the MPEG-4 audio speciﬁcation [89] by 2004. The extended codec is called High-Efﬁciency AAC
Version2 (HE-AAC v2) and provides again signiﬁcant performance gain over its predecessor for
stereo coding at low bitrates due to the reduction from two channels to a single (downmix) channel
plus parametric side information. For example, HE-AAC v2 at 32 kbit/s performed comparable to
HE-AAC at 48 kbit/s for stereophonic signals [90]. Similarly to HE-AAC, HE-AAC v2 has been
widely deployed in the broadcasting and mobile music and TV market, including devices according
to 3rd Generation Partnership Project (3GPP) and 3GPP2 speciﬁcations.
•
In 2012, the MPEG-D Uniﬁed Speech and Audio Coding (USAC) codec was ﬁnalized [91–93].
Within USAC, HE-AAC v2 was enriched with certain elements of state-of-the-art speech coding
technology, speciﬁcally Algebraic Code Excited Linear Prediction (ACELP) and Transform Coded
eXcitation (TCX) related components. Moreover, USAC contains numerous detail improvements
and added ﬂexibility with respect to the AAC-based codec parts, for instance, an optionally warped
MDCT ﬁlter bank [94], more ﬁlter bank block sizes and transform windows, efﬁcient coding of
spectral noise shaping data, enhanced redundancy reduction, bandwidth extension and parametric
stereo.MoredetailsofUSACareprovidedinSection4.28.3.10.SinceUSACbuildsonthefoundation
of HE-AAC v2 and, for high bitrates up to transparency, internally resorts to AAC, an “Extended
High Efﬁciency AAC Proﬁle” (Extended HE-AAC) has been deﬁned which describes a superset of
HE-AAC v2 technology and is backward compatible to its predecessor.
Figure 28.13 illustrates the hierarchy of the three MPEG-4 AAC codecs and the Extended HE-AAC
proﬁle of MPEG-D USAC. A tutorial overview of the HE-AAC family is provided in [95].
FIGURE 28.13
Overview of the family of MPEG (HE-)AAC codecs and the MPEG-D USAC Extended HE-AAC proﬁle. Sub-
sequent codec generations provide a signiﬁcant performance gain over their predecessors by inclusion of
new coding tools. Each new generation represents a superset of the preceding technology and thus ensures
backward compatibility to bitstreams of the previous generation.

4.28.3 Standards
787
4.28.3.6 MPEG-4 low delay AAC family (AAC-LD, AAC-ELD and AAC-ELD v2)
While the original domain of the MPEG audio coders was clearly to provide an enabling technology
for storage and transmission of multimedia content, their advantages could also be beneﬁcial in the area
of high-quality bi-directional telecommunication. However, a low end-to-end communication delay is
a crucial factor which has not been addressed by regular MPEG audio coders (the minimum delay for
such coders can be as high as several hundreds of milliseconds, depending on parameters like bitrate
and sampling frequency)—see Section 4.28.2.2.5 for more detailed information. Therefore MPEG has
deﬁned a number of communication audio coders, starting with a low-delay version of the MPEG-4
AAC codec already in version 2 of the MPEG-4 audio speciﬁcation [81] in 2000. Subsequently, a family
of low delay codecs has emerged with a concept that is analogous to that of the MPEG-4 AAC family,
as discussed previously:
•
The ﬁrst generation is the MPEG-4 Low-Delay AAC (AAC-LD) codec [29,30] which is closely
derived from the regular codec and essentially achieves a delay reduction of down to 20 ms. This
is achieved by decreasing the frame length, replacing the ﬁlter bank switching by a combination of
window shape adaptation and Temporal Noise Shaping (TNS), and minimizing the use of the bit
reservoir. Compared to regular MPEG-4 AAC-LC, the bitrate penalty for achieving the required low
delay is approximately 8 kbit/s per channel. This result was measured at bitrates of 32 kbit/s and
64 kbit/s per channel for the low delay coder [76]. Over time, AAC-LD has become widely deployed
as part of many hardware or desktop computer based systems for videoconferencing.
•
As a next step, AAC-LD was extended with a low delay ﬂavor of the SBR bandwidth extension
tool, resulting in an Enhanced Low Delay AAC (AAC-ELD) codec [96] with signiﬁcantly increased
coding performance (a bitrate saving of 25–33%) at low rates [97]. In order to keep the delay of
the combination of baseband coder and bandwidth extension processor low enough, new low-delay
ﬁlter banks were introduced for both parts of the algorithm [98].
•
Finally, a low-delay MPEG Surround (an advanced counterpart of the traditional Parametric Stereo
module) was added resulting in an Enhanced Low Delay AAC Version 2 (AAC-ELD v2) codec [99]
with enhanced stereo coding performance, but also the ability of carrying low-delay multi-channel
audio, as it may become important for high-quality communication applications.
Figure 28.14 illustrates the hierarchy of the three low delay MPEG-4 AAC codecs.
FIGURE 28.14
Overview of the MPEG-4 AAC-LD family of codecs, which follows a concept that is comparable to that of the
MPEG AAC family.

788
CHAPTER 28 Perceptual Audio Coding
Due to these developments, MPEG audio coding has also found its way into the communications
market, which was traditionally served by speech coding technology standardized by ITU-T [100]. Over
time, the availability of higher bitrates and the desire for higher audio quality with full audio bandwidth
and, possibly, stereophonic or even multi-channel transmission has led to an increased adoption of
the MPEG codecs, especially in high-quality teleconferencing, video-conferencing and telepresence
applications. Comparative studies with listening tests conﬁrm the competitiveness of the MPEG low
delay codecs relative to other traditional and speech coding based solutions [101].
4.28.3.7 MPEG-4 lossless audio coding (ALS, SLS)
For a considerable period of time, a signiﬁcant share of development effort in perceptual audio coding has
been targeted at achieving ever-increasing compression performance at low bitrates. This requirement is
imposed by the rising demand for multimedia delivery and communication across limited transmission
channels. Nonetheless, the better availability of storage and transmission capacity in many application
cases together with the desire for very high audio quality has triggered many activities on Lossless Audio
Coding or Near Lossless Audio Coding [102] which focus on the redundancy aspect of audio coding. In
order to provide standardized solutions for this purpose, two types of solutions were developed by the
MPEG group until 2006, both capable of working on a wide range of audio sampling rates and word
lengths:
•
MPEG-4ALS:ALS[103,104]isastraight-forwardlosslesscodecwithatraditionalpredictivecoding
structure (linear predictive coding in time domain combined with entropy coding of the prediction
residual). It is operated as a stand-alone codec for which the trade-off between lossless compression
performance and computational complexity can be controlled. MPEG-4 ALS provides compression
performance [105] that is comparable to other state-of-the-art lossless audio coding schemes [102].
•
MPEG-4 SLS and High-Deﬁnition AAC: As an alternative solution, SLS [15,106] is a lossless cod-
ing scheme that operates on a spectral decomposition of the input signals using newly designed
integer ﬁlter banks (integer MDCT [107]) and accompanying entropy coding. The encoding pro-
cess is performed in a top-down/ﬁne grain scalable fashion similar to Bit-Sliced Arithmetic Coding
(BSAC) (see Sections 4.28.2.8 and 4.28.3.5). As a result, SLS can act as a fully lossless coder or,
by scaling down on the bitrate, as a near lossless coder. This is of particular interest for encoding
sound material with very high quality, such that even after many generations of repeated encod-
ing and decoding (tandem coding) the perceptual quality is unaffected. In this way, it bridges the
large gap in data rate between fully lossless audio coding and perceptually lossless (“transparent”)
coding. For example, at CD sampling conditions −44.1 kHz/16 bit/stereo—state-of-the-art lossless
encoding may consume on average approximate 700 kbit/s while a perceptually transparent repre-
sentation may require only around 128 kbit/s on average. The SLS codec can be operated in two
modes:
•
As a stand-alone system for lossless/near-lossless compression.
•
TogetherwiththeMPEG-4AACcodec.Inthismode,SLSprovidesascalableenhancementlayer
tothelossyAACbasecodecthatcanbescaledupallthewaytolosslesscoding.Thiscombination
is known as the High-Deﬁnition Advanced Audio Coding [15] system, which extends the MPEG-
4 AAC codec family for application scenarios which require excellent audio quality.

4.28.3 Standards
789
Also MPEG-4 SLS provides compression performance [108] that is comparable to other state-of-the-
art lossless audio coding schemes [102]. When run in conjunction with an AAC base layer, the overall
lossless compression is somewhat reduced [15], as it is natural for a bottom-up/large step scalable
coding process (see Section 4.28.2.8).
4.28.3.8 MPEG-D MPEG surround
MPEG Surround [63,64] is the result of technical development work between 2004 and 2006 with the
aim of deﬁning a universal Spatial Audio Coding scheme (see Section 4.28.2.6) that is agnostic of the
type of audio coder that may be used to carry the downmix signal. The speciﬁcation [109] became the
ﬁrst part of the MPEG-D standard containing novel audio technologies.
Beyond adhering to the generic principle of Spatial Audio Coding schemes (i.e., transmitting n audio
channels through m < n downmix channels and parametric side information), MPEG Surround builds
upon some technical aspects from MPEG-4 Parametric Stereo (originally used as part of the HE-AAC v2
codec), generalizes them for application to multi-channel coding and adds many new technical aspects,
including ideas from Binaural Cue Coding (BCC).
Some technical features of MPEG Surround [63] are pointed out in the following:
•
MPEG Surround uses an analysis/synthesis ﬁlter bank similar to the ones used by SBR/PS/HE-AAC
and HE-AAC v2, and can therefore be efﬁciently combined with these schemes.
•
The key spatial parameters/data transmitted in the bitstream are
•
Channel Level Differences (CLDs).
•
Inter-Channel Correlations (ICCs).
•
Channel Prediction Coefﬁcients (CPCs).
•
Prediction errors (residuals).
•
Uses the same type of decorrelators like Parametric Stereo.
MPEG Surround has been equipped with a number of capabilities and features that make it attractive
and useful for a broad range of applications aiming at backward compatible or non backward compatible
transmission of multi-channel sound:
•
Supports one or two (compatible) downmix channels.
•
Supports many output channel conﬁgurations between 2 and 7 output channels (more can be achieved
with a so-called “arbitrary tree extension”).
•
Compatible to any type of downmix coder (including PCM). When combined with other MPEG
coders using bandwidth extension, shared use of the SBR ﬁlter bank allows for an efﬁcient imple-
mentation.
•
Residual coding: The ability of encoding and carrying residual error information enables very high
signal quality, overcoming quality limitations associated with the parametric signal model. In this
way, the system can operate at a wide range of rate/distortion operating points.
•
Artistic downmix: In order to additionally allow the use of downmix signals that were different
from the ones generated by the MPEG Surround encoder (e.g., customized stereophonic mixes),
an artistic downmix mode can describe the differences between the two mixes and compensate for
them.

790
CHAPTER 28 Perceptual Audio Coding
•
Matrix surround compatibility: As matrix surround (e.g., Dolby Prologic) is still in wide use, MPEG
Surround can also provide downmix signals that conform to matrixing requirements and thus can
be decoded by existing matrix decoders, e.g., in home theatre receivers (although at a considerably
lower spatial quality).
•
Operationwithoutsideinformation:MPEGSurroundcanalsobeoperatedonstereodownmixsignals
without side information (with and without the use of a matrix compatible downmix), providing a
spatial sound quality that outperforms that of traditional matrix decoders.
•
Binauralsurround:Inordertosupportmobilesurroundplaybackonportabledevicesviaheadphones,
binaural rendering is supported in two ways:
•
Binauralized downmix: The MPEG Surround encoder can produce a readily binauralized down-
mix signal for direct headphone listening.
•
Binaural decoding/upmix: Alternatively, the surround output may be rendered by the decoder
into a binauralized signal in a computationally very efﬁcient way [110].
•
Low delay mode: In order to support communication applications as well, such as video-conferencing
with multi-channel sound or telepresence, a low delay mode for MPEG Surround has been deﬁned
using low delay ﬁlter banks. This mode was developed in the course of the MPEG-D SAOC [111]
work (see Section 4.28.2.7).
Equipped with a broad palette of interesting capabilities, MPEG Surround can bring multi-channel
audio into existing infrastructures by way of its high compression factor and its inherent stereo/mono
backward compatibility. Together with a modern audio codec for the transmission of the downmix, such
as HE-AAC, MPEG Surround enables multi-channel audio at 64 kbit/s and below. MPEG Surround has
become part of several speciﬁcations for digital broadcasting, such as standards by the Internet Engi-
neering Task Force (IETF) for RTP streaming in IP networks [112,113], by the Consumer Electronics
Association (CEA) [114] and by the European Telecommunications Standards Institute (ETSI), spec-
ifying Digital Radio Mondiale (DRM) [115], Digital Audio Broadcasting (DAB)/DAB+ and Digital
Media Broadcasting (DMB) [116] or Digital Video Broadcasting (DVB) [117,118].
4.28.3.9 MPEG-D MPEG Spatial Audio Object Coding (SAOC)
As a continuation project to MPEG Surround Spatial Audio Coding, MPEG developed a speciﬁcation
for parametric coding of audio objects, which is called MPEG-D Spatial Audio Object Coding (MPEG-
D SAOC) [111] between 2007 and 2011. By using a number of elements deﬁned in MPEG Surround,
MPEG-D SAOC extends parametric spatial coding towards coding of audio objects and scenes providing
user controlled interactivity at the decoder side.
Some technical features of MPEG-D SAOC [70] are listed in the following:
•
Like MPEG Surround, MPEG SAOC uses an analysis/synthesis ﬁlter bank similar to the ones used
by SBR/PS/HE-AAC v1 + v2, and can therefore be efﬁciently combined with these schemes.
•
Sound objects are described by several object parameters and downmix parameters.
•
Object Level Differences (OLDs).
•
Inter-Object Cross-Correlations (IOCs).
•
Prediction errors (residuals).

4.28.3 Standards
791
•
Uses the same type of decorrelators like MPEG Surround.
Like MPEG Surround, MPEG-D SAOC has been equipped with a number of capabilities and features
that are intended to make it attractive and useful for many applications aiming at backward compatible
or non backward compatible transmission of interactive sound scenes:
•
Supports one or two (compatible) downmix channels.
•
The SAOC data representation (downmix plus side information) can be rendered to any desired
loudspeakersetupor,alternatively,headphonepresentation.Foroneortwooutputchannels,decoding
and rendering are done in one single decoding step that directly transcodes the downmix signal(s)
into the output. For multi-channel output, as an intermediate step, an SAOC transcoder converts
the SAOC downmix and side information into an MPEG Surround downmix and side information,
using information about the desired number of loudspeakers and the object rendering information.
Finally, an MPEG Surround decoder serves as the multi-channel rendering engine.
•
Compatible to any type of downmix coder (including PCM). When combined with other MPEG
coders using bandwidth extension, shared use of ﬁlter banks allow for an efﬁcient implementation.
•
Residual coding: The ability of encoding and carrying residual error information allows to achieve
very high signal quality for certain designated audio objects, called Enhanced Audio Objects (EAOs),
overcoming quality limitations associated with the parametric signal model.
•
Binaural surround: In order to support playback on portable devices via headphones, the SAOC
output may be rendered directly into a binauralized signal by a computationally very efﬁcient process.
•
Low delay mode: In order to support communication applications, such as video- or telephone
conferencing with individual object (talker) control, a low delay mode for MPEG-D SAOC has been
deﬁned using low delay ﬁlter banks.
Applications for this scheme reside in several areas:
•
Interactive remix/personalized audio playback: The technology allows the user to create a personal
“re-mix” of SAOC encoded audio by using a control similar to that of a multi-track mixing desk in
order to adjust relative level, spatial position, etc. of instruments, sounds, or dialogue according to
their liking. In this way, the user can
•
suppress/attenuate certain instruments for play-along (like Karaoke),
•
modify the original audio to reﬂect his/her preference (e.g., “more drums & less strings” for a
dance party; “less drums and more vocals” for relaxation music),
•
control the dialog/speech level (or attenuate background music/sounds) in movies / broadcasts,
e.g., for better speech intelligibility for the hearing- impaired or reproduction in an environment
with high background noise level.
•
Networked multi-player games: The technology provides an efﬁcient means for transmitting and
rendering sound objects that are external to a certain player’s terminal for networked multi-player
games, such as other players’ voices, and render them into the local terminal sound scene.
•
Interactive enhanced spatial telecommunication: In a communication connecting several talkers,
information about each individual talker can be carried as hidden side information accompanying
the (usually monophonic) mix of talker signals which is transmitted across the network. Thus, when
receiving an SAOC stream, SAOC-enabled terminals can render the communication to any number
of desired output channels and in this way greatly increase intelligibility by spatially separating the

792
CHAPTER 28 Perceptual Audio Coding
different talkers (“cocktail party effect”) and/or boosting a speciﬁc talker. Legacy terminals will
continue to produce monophonic audio output.
4.28.3.10 MPEG-D uniﬁed speech and audio coding (USAC)
Filter-bank-based coders like AAC are considered to be state-of-the-art for coding of music content at
all bitrates. However, for speech content at low bitrates (less than 32 kbit/s per channel), these coders
were often outperformed by dedicated speech coders like Extended Adaptive Multi-Rate-Wideband
(AMR-WB+) [119]. Conversely, dedicated speech coders perform poorly on non-speech signal like
music. From this, the need emerged to create a uniﬁed codec that can code both music and speech
equally well, even at very low bitrates.
TheUniﬁedSpeechandAudioCodingschemedevelopedbyISO/MPEG(MPEG-DUSAC)addresses
exactly this issue. The collaborative technical development phase at MPEG started in late 2007 and con-
cluded in early 2011. The standard was ﬁnalized in early 2012 [93]. Consistent with the goal to achieve
the best possible subjective quality for speech and music content alike, the quality benchmark for USAC
performance during development was deﬁned to be a “virtual” codec consisting of the better of the two
codecs HE-AAC v2 and AMR-WB+ on a per-test-item basis and a given bitrate.
Technically, USAC uniﬁes the traditionally separated worlds of general audio codecs and dedicated
speechcodecsbyadoptingelementsfromastate-of-the-artspeechcodecsintoasystemthatstillmaintain
the overall structure of HE-AAC v2.
The most prominent modiﬁcations to the uniﬁed coder with respect to HE-AAC v2 are the following
[91,92]:
•
The MDCT coder ﬁlter bank supports a larger set of block lengths, adding the intermediate block
sizes of 512 and 256 to the previously used values of 1024 and 128. Additional transform window
shapes are available (low overlap windows) and the MDCT can optionally be time-warped [94] to
better align to pitched signals that are time-varying. This allows for an improved adaptation to many
signals.
•
As an alternative to the traditional scale factor method, a more efﬁcient way of coding the spectral
shape of the quantization noise is provided by means of Linear Predictive Coding (LPC) based
Frequency Domain LPC Noise Shaping (FDNS).
•
For better redundancy removal, the traditional Huffman coding of quantized spectral data is replaced
with context adaptive arithmetic coding.
•
An enhanced SBR bandwidth extension by introducing harmonic transposition to counteract auditory
roughness [120,121], Predictive Vector Coding (PVC) for efﬁcient coding of the SBR spectral
envelope with high temporal granularity and alternative sampling ratios (1:4, 3:8) enabling lower
cross-over frequencies between core bandwidth and bandwidth extension.
•
An additional codec operation mode which shortens all ﬁlter bank block length to three quarters of
their original length. Together with SBR at sampling ratio 3:8 this allows for a higher time resolution
of the codec at a given audio bandwidth.
•
A new parametric stereo derived from MPEG Surround (MPS 2-1-2) that includes phase coding
through Inter-channel Phase Differences (IPDs) [122] and integrates residual coding into a so-called
Uniﬁed Stereo coding module.

4.28.4 Summary and Conclusions
793
•
For higher bitrates, where the efﬁcient combination of SBR/MPS is typically not used, an improved
joint stereo coding technique has been introduced. Complex Prediction Stereo Coding operates on
MDCT spectra and minimizes the inter-channel redundancy of M/S coding. Thereby, the complex
valued prediction also compensates for inter-channel phase differences [123].
•
Other new tools have been included, like the Inter-subband-sample Temporal Envelope Shaping
(inter-TES) and the Transient Steering Decorrelator (TSD) tool for proper spatial coding of dense
transient mixtures, such as applause signals [42,65].
Within MPEG, a comprehensive veriﬁcation test of USAC was conducted for various bitrates and
conditions in summer of 2011. The test conﬁrmed that the initial goal of achieving a performance
equal or better than the “virtual” codec was successfully reached and often surpassed. Hence, USAC
is the most efﬁcient codec for both signal categories—speech and music—as at the time of writing
this text. Ranging from low bitrates of 8 kbit/s per channel up to high bitrates that allow for perceptual
transparency, USAC represents the current state-of-the-art in combined coding of speech, music and
mixed signal.
In terms of applications, USAC offers a perceptual quality that scales down gracefully with the
available bitrate, regardless of the speciﬁc type of audio content to be transmitted. Therefore, USAC is an
ideal codec for transmission scenarios over narrowband channels or channels that exhibit a time-varying
capacity. Such channels are characteristic of digital radio broadcast or mobile download applications
of multimedia content onto handheld devices. Typical applications are therefore Internet radio, mobile
television or streaming of content such as audio books.
4.28.4 Summary and conclusions
Perceptual audio coding has come a long way during the last two to three decades and evolved from a
research topic to a mainstream technology which is deployed in virtually every household on numerous
portable and stationary devices for entertainment and communication.
From a conceptual point of view, the original idea of exploiting both redundancy and irrelevancy has
been developed towards further reﬁnement (more elaborated signal processing and modeling, additional
coding tools) and to include new levels of knowledge about human auditory perception, such as spatial
hearing.
From a performance point of view, i.e., subjective audio quality offered as a function of bitrate, it
is necessary to differentiate between two cases. If the application requires perceptual quality that is
at or very close to perceptual “transparency,” classic perceptual coding of the audio signal waveform
with a modern audio coder is still the preferred solution (e.g., MPEG-2/4 AAC at bitrates of at least
128 kbit/s stereo). If, however, some minor changes in signal characteristics can be tolerated, the new
technologies developed subsequently over the years demonstrate continuous and signiﬁcant progress.
As an example, Figure 28.15 sketches the bitrate necessary for “good perceptual quality” audio coding
by MPEG audio coders over the last two decades. For these MPEG coders, alternating generations of
standardized coding schemes along with steadily improved encoder implementations paved the way for
constant progress in coding efﬁciency over the years.
From an application point of view, perceptual audio coding evolved from a compression-only tech-
nology that was needed for efﬁcient storage and transmission towards a broad palette of technologies

794
CHAPTER 28 Perceptual Audio Coding
FIGURE 28.15
Bitrate curve for “good perceptual quality” stereophonic coding for various codec generations ranging from
MPEG1/2 Layer 3 (mp3) to MPEG-D USAC.
that also provide means for intelligent and ﬂexible rendering of audio content on many presentation
platforms like stereo and surround loudspeaker setups and headphones.
During all this time, the sources of progress were an enhanced understanding of human auditory per-
ception and improved ways of exploiting this understanding by means of signal processing. Beyond that,
the availability of more and more computational resources gradually removed computational complexity
as an obstacle to deploying new algorithms, as time progresses.
Relevant Theory: Signal Processing Theory
See Vol. 1, Chapter 3 Discrete-Time Signals and Systems
See Vol. 1, Chapter 4 Random Signals and Stochastic Processes
See Vol. 1, Chapter 5 Sampling and Quantization
See Vol. 1, Chapter 6 Digital Filter Structures and Their Implementation
See Vol. 1, Chapter 7 Multirate Signal Processing for Software Radio Architectures
References
[1] L. Chiariglione (Ed.), The MPEG Representation of Digital Media, Springer Verlag, Berlin, Heidelberg,
2012.
[2] R.W. Hamming, Coding and Information Theory, second ed., Prentice Hall, 1986.
[3] E. Zwicker, H. Fastl, Psychoacoustics, Facts and Models, Springer Verlag, Berlin, Heidelberg, 1990.
[4] B.C.J. Moore, Introduction to the Psychology of Hearing, third ed., Academic Press, 1989.
[5] J. Blauert, Spatial Hearing: The Psychophysics of Human Sound Localization, revised ed., MIT Press, 1997.
[6] N. Jayant, P. Noll, Digital Coding of Waveforms, Prentice-Hall, Englewood Cliffs, NJ, 1984.
[7] K. Brandenburg, T. Sporer, NMR and masking ﬂag: evaluation of quality using perceptual criteria, in: 11th
International AES Conference: Test, Measurement, May 1992.

References
795
[8] ITU-R Recommendation BS.1116-1 Methods for the subjective assessment of small impairments in audio
systems including multichannel sound systems, International Telecommunications Union, Geneva, Switzer-
land, 1997.
[9] ITU-R Recommendation BS.1534-1 Method for the subjective assessment of intermediate quality level of
coding systems, International Telecommunications Union, Geneva, Switzerland, 2003.
[10] ITU-R Recommendation BS.1387-1 Method for objective measurements of perceived audio quality, Inter-
national Telecommunications Union, Geneva, Switzerland, 2001.
[11] T. Thiede, W. Treurniet, R. Bitto, et al., PEAQ—the ITU standard for objective measurement of perceived
audio quality, J. AES 48 (1–2) (2000) 3–29.
[12] M. Krasner, The critical band coder-Digital encoding of speech signals based on the perceptual requirements
of the auditory system, in: IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), 1980,
pp. 327–331.
[13] K. Brandenburg, D. Seitzer, OCF: Coding High Quality Audio with Data Rates of 64 kBit/s, 85th AES
Convention, Los Angeles, 1988, Preprint 2723.
[14] G. Stoll, M. Link, G. Theile, Masking-Pattern Adapted Subband Coding: Use of the Dynamic Bit-Rate
Margin, 84th AES Convention, March 1988, Preprint 2585.
[15] R. Geiger, R. Yu, J. Herre, et al., ISO/IEC MPEG-4 high-deﬁnition scalable advanced audio coding, J. AES
55 (1–2) 2007, 27–43.
[16] J.D. Johnston, Audio coding with ﬁlter banks, in: A.N. Akansu, M.J.T. Smith (Eds.), Subband and Wavelet
Transforms, Kluwer Academic Publishers, Norwell, 1996, pp. 287–307.
[17] J. Princen, A. Johnson, A. Bradley, Subband/transform coding using ﬁlter bank designs based on time domain
aliasing cancellation, in: IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), 1987, pp.
2161–2164.
[18] B. Edler, Codierung von audiosignalen mit überlappender transformation und adaptiven fensterfunktionen,
Frequenz 43 (1989) 252–256.
[19] M. Bosi, Filter banks in perceptual audio coding, in: 17th AES Conference, Florence, Italy, 1999.
[20] ISO/IEC JTC1/SC29/WG11 MPEG, International Standard ISO/IEC 11172, Coding of moving pictures and
associated audio for digital storage media at up to about 1.5 Mbit/s,1993.
[21] ISO/IEC JTC1/SC29/WG11 MPEG, International Standard ISO/IEC 13818-3, Generic Coding of Moving
Pictures and Associated Audio: Audio, 1994.
[22] ISO/IEC JTC1/SC29/WG11 MPEG, International Standard ISO/IEC 13818-7, Generic Coding of Moving
Pictures and Associated Audio: Advanced Audio Coding, 1997.
[23] R.P. Hellman, Asymmetry of masking between noise and tone, Percept. Psychophys. 11 (1972) 241–246.
[24] T. Painter, A. Spanias, Perceptual coding of digital audio, Proc. IEEE 88 (4) (2000) 451–513.
[25] N. Iwakami, T. Moriya, S. Miki, High-quality audio-coding at less than 64 kbit/s by using transform-domain
weighted interleave vector quantization (TWINVQ), in: IEEE Int. Conf. on Acoustics, Speech and Signal
Processing (ICASSP), Detroit, 1995, pp. 3095–3098.
[26] J. Herre, Temporal Noise Shaping, Quantization and Coding Methods in Perceptual Audio Coding—A
Tutorial Introduction, 17th AES Conference, Florence, Italy, 1999.
[27] M. Erne, Perceptual Audio Coders What to listen for, 111th AES Convention, November 2001, Preprint
5489.
[28] ITU-T G.107, G.114: The E-model: a computational model for use in transmission planning, International
Telecommunication Union—Telecommunication Standardization Sector.
[29] E. Allamanche, R. Geiger, J. Herre, T. Sporer, MPEG-4 Low Delay Audio Coding based on the AAC Codec,
in: 106th AES Convention, Munich 1999, Preprint 4929.
[30] R. Geiger, M. Lutzky, M. Schmidt, M. Schnell, Structural Analysis of Low Latency Audio Coding Schemes,
119th AES Convention, October 2005. Preprint 6601.

796
CHAPTER 28 Perceptual Audio Coding
[31] M. Lutzky, M. Luis Valero, M. Schnell, J. Hilpert, AAC-ELD V2—The New State of the Art in High Quality
Communication Audio Coding, 131st AES Convention, October 2011, Preprint 8516.
[32] G. Schuller, B. Yu, D. Huang, B. Edler, Perceptual audio coding using adaptive pre- and post-ﬁlters and
lossless compression, IEEE Trans. Speech Audi. Process. 10 (6) (2002) 379–390.
[33] J. Herre, E. Eberlein, K. Brandenburg, Combined Stereo Coding, 93rd AES Convention, San Francisco 1992,
Preprint 3369.
[34] J.D. Johnston, A.J. Ferreira, Sum-difference stereo transform coding, in: IEEE Int. Conf. on Acoustics,
Speech and Signal Processing (ICASSP), 1992, pp. 569–571.
[35] R.G.V.D. Waal, R.N.J. Veldhuis, Subband coding of stereophonic digital audio signals, in: IEEE Int. Conf.
on Acoustics, Speech and Signal Processing (ICASSP), 1991, pp. 3601–3604.
[36] J. Herre, K. Brandenburg, D. Lederer, Intensity Stereo Coding, 96th AES Convention, Amsterdam, 1994,
Preprint 3799.
[37] J. Eargle, The Microphone Book: From Mono to Stereo to Surround—A Guide to Microphone Design and
Application, second ed., Elsevier Science, 2005.
[38] A. Csicsatka, R.M. Linz, The new stereo FM broadcasting system—how to understand the FCC speciﬁcations
and generate the composite signal, J. Audio Eng. Soc. 10/1 (1962) 2–7.
[39] G. Stoll, G. Theile, S. Nielsen, et al., Extension of ISO/MPEG-Audio Layer II to Multi-Channel Coding: The
Future Standard for Broadcasting, Telecommunication, and Multimedia Applications, 94th AES Convention,
Berlin, 1993, Preprint 3550.
[40] B. Grill, J. Herre, K. Brandenburg, et al., Improved MPEG-2 Audio Multi-Channel Encoding, 96th AES
Convention, Amsterdam, 1994, Preprint 3865.
[41] M.F. Davis, The AC-3 Multichannel Coder, 95th AES Convention, New York, October 1993, Preprint 3774.
[42] S. Disch, A. Kuntz, A dedicated decorrelator for parametric spatial coding of applause-like audio signals,
in: Albert Heuberger, Günter Elst, Randolf Hanke (Eds.), Microelectronic Systems: Circuits, Systems and
Applications, Springer, Berlin, Heidelberg, 2011, pp. 355–363.
[43] M. Bosi, K. Brandenburg, S. Quackenbush, et al., ISO/IEC MPEG-2 advanced audio coding, J. AES 45 (10)
(1997) 789–814.
[44] H. Fuchs, Improving MPEG Audio Coding by Backward Adaptive Linear Stereo Prediction, 99th AES
Convention, October 1995, Preprint 4086.
[45] J. Ojanperä, M. Väänänen, L. Yin, Long Term Predictor for Transform Domain Perceptual Audio Coding,
107th AES Convention, New York, 1999, Preprint 5036.
[46] H. Fuchs, Improving joint stereo audio coding by adaptive inter-channel prediction, in: IEEE Workshop on
Applications of Signal Processing to Audio and Acoustics, New Paltz, New York, 1993.
[47] J. Herre, J.D. Johnston, Enhancing the Performance of Perceptual Audio Coders by Using Temporal Noise
Shaping (TNS), 101st AES Convention, Los Angeles, 1996, Preprint 4384.
[48] J. Herre, J.D. Johnston, Exploiting Both Time and Frequency Structure in a System that Uses an Analy-
sis/Synthesis Filterbank with High Frequency Resolution, 103rd AES Convention, New York, 1997, Preprint
4519.
[49] J. Herre, J.D. Johnston, Continuously signal-adaptive ﬁlterbank for high-quality perceptual audio coding, in:
IEEE ASSP Workshop on Applications of Signal Processing to Audio and Acoustics, Mohonk, NY, 1997.
[50] J. Herre, D. Schulz, Extending the MPEG-4 AAC Codec by Perceptual Noise Substitution, 104th AES
Convention, Amsterdam, 1998, Preprint 4720.
[51] M. Dietz, L. Liljeryd, K. Kjorling, O. Kunz, Spectral Band Replication, a Novel Approach in Audio Coding,
112th AES Convention, April 2002, Preprint 5553.
[52] P. Ekstrand, Bandwidth extension of audio signals by spectral band replication, in: First IEEE Benelux
Workshop on Model based Processing and Coding of Audio (MPCA-2002), Leuven, Belgium, November
15, 2002, pp. 73–79.

References
797
[53] F. Nagel, S. Disch, A harmonic bandwidth extension method for audio codecs, in: IEEE Int. Conf. on
Acoustics, Speech and Signal Processing (ICASSP), 2009.
[54] F. Nagel, S. Disch, N. Rettelbach, A phase Vocoder Driven Bandwidth Extension Method with Novel Tran-
sient Handling for Audio Codecs, 126th AES Convention, 2009.
[55] ISO/IEC
JTC1/SC29/WG11
(MPEG)
Document
N6009,
Report
on
the
Veriﬁcation
Tests
of
MPEG-4
High
Efﬁciency
AAC,
Brisbane,
2003,
available
at
the
MPEG
web
site
<http://mpeg.chiariglione.org/qualitytests.php>.
[56] E. Larsen, R. Aarts, Audio Bandwidth Extension: Application of Psychoacoustics, Signal Processing and
Loudspeaker Design, John Wiley and Sons Ltd., 2004 (Chapters 5 and 6).
[57] B. Iser, G. Schmidt, W. Minker, Bandwidth Extension of Speech Signals, Springer Publishing Company
Incorporated, 2008.
[58] J. Herre, C. Faller, S. Disch, et al., Spatial Audio Coding: Next-Generation Efﬁcient and Compatible Coding
of Multi-Channel Audio, 117th AES Convention, San Francisco, 2004, Preprint 6186.
[59] F. Baumgarte, C. Faller, Binaural cue coding—part I: psychoacoustic fundamentals and design principles,
IEEE Trans. Speech Audi. Process. 11 (6) (2003).
[60] C. Faller, F. Baumgarte, Binaural cue coding—part II: schemes and applications, IEEE Trans. Speech Audi.
Process. 11 (6) (2003).
[61] E. Schuijers, J. Breebaart, H. Purnhagen, J. Engdegård, Low Complexity Parametric Stereo Coding, 116th
AES Convention, Berlin, Germany, 2004, Preprint 6073.
[62] J. Herre, C. Faller, C. Ertel, et al., MP3 Surround: Efﬁcient and Compatible Coding of Multi-Channel Audio,
116th AES Convention, Berlin, Germany, 2004, Preprint 6049.
[63] J. Herre, K. Kjörling, J. Breebaart, et al., MPEG surround—the ISO/MPEG standard for efﬁcient and com-
patible multichannel audio coding, J. AES 56 (11) (2008) 932–955.
[64] J. Hilpert, S. Disch, Standards in a nutshell: the MPEG surround audio coding standard, IEEE Signal Process.
Mag. 26 (1) (2009) 148–152.
[65] A. Kuntz, S. Disch, T. Bäckström, J. Robilliard, The Transient Steering Decorrelator Tool in the Upcoming
MPEG Uniﬁed Speech and Audio Coding Standard, 131st AES Convention, October 2011, Preprint 8533.
[66] ISO/IEC
JTC1/SC29/WG11
(MPEG),
International
Standard
ISO/IEC
IS
14496-3:
Information
technology—Coding of audio-visual objects—Part 3: Audio, fourth ed., 2009.
[67] F. Pereira, T. Ebrahimi (Eds.), The MPEG-4 Book, Prentice Hall IMSC Multimedia Series, 2002.
[68] C. Faller, Parametric Joint-Coding of Audio Sources, 120th AES Convention, May 2006, Preprint 6752.
[69] J. Herre, S. Disch, J. Hilpert, O. Hellmuth, From SAC To SAOC—Recent Developments in Parametric
Coding of Spatial Audio, 22nd Regional UK AES Conference, UK, April, Cambridge, 2007.
[70] O. Hellmuth, H. Purnhagen, J. Koppens, et al., MPEG spatial audio object coding—the ISO/MPEG standard
for efﬁcient coding of interactive audio scenes, 129th AES Convention, San Francisco, USA, 2010, Preprint
8264.
[71] S.A. Ramprashad, A two stage hybrid embedded speech/audio coding structure, in: IEEE Int. Conf. on
Acoustics, Speech and Signal Processing (ICASSP), vol. 1, 1998, pp. 337–340.
[72] B. Grill, Bit Rate Scalable Perceptual Coder for MPEG-4 Audio, 103rd AES Convention, New York, 1997,
Preprint 4620.
[73] A. Aggarwal, K.A. Rose, A conditional enhancement-layer quantizer for the scalable MPEG advanced
Audio Coder, in: IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), vol. 2, 2002, pp.
1833–1836.
[74] S.-H. Park, Y.-B. Kim, Y.-S. Seo, Multi-Layered Bit-Sliced Bit-Rate Scalable Audio Coding, 103rd AES
Convention, New York, 1997, Preprint 4520.

798
CHAPTER 28 Perceptual Audio Coding
[75] R. Yu, R. Geiger, S. Rahardja, et al., MPEG-4 Scalable to Lossless Audio Coding, 117th AES Convention,
October 2004, Preprint 6183.
[76] ISO/IEC JTC1/SC29/WG11 (MPEG) Document N3075, Report on the MPEG-4 Audio Version
2 Veriﬁcation Test, Hawaii, 1999, available at the MPEG web site
<http://mpeg.chiariglione.
org/qualitytests.php>
[77] L.D. Fielder, R.L. Andersen, B.G. Crockett, et al., Introduction to Dolby Digital Plus, an Enhancement to
the Dolby Digital Coding System, 117th AES Convention, October 2004, Preprint 6196.
[78] K. Brandenburg, G. Stoll, The ISO/MPEG-Audio Codec: A Generic Standard for Coding of High Quality
Digital Audio, 92nd AES Convention, Vienna, 1992, Preprint 3336.
[79] J.D. Johnston, J. Herre, M. Davis, U. Gbur, MPEG-2 NBC Audio-Stereo and Multichannel Coding Methods,
101st AES Convention, Los Angeles 1996, Preprint 4383.
[80] ISO/IEC JTC1/SC29/WG11 (MPEG), International Standard ISO/IEC IS 14496-3, Coding of Audio-Visual
Objects: Audio, 1999.
[81] ISO/IEC JTC1/SC29/WG11 (MPEG), International Standard 14496-3 Amd. 1, Coding of Audio-Visual
Objects: Audio, 2000.
[82] M. Nishiguchi, MPEG-4 speech coding, in: 17th AES Conference, Florence, Italy, 1999.
[83] H. Purnhagen, N. Meine, HILN—The MPEG-4 parametric audio coding tools, in: Proceedings IEEE Inter-
national Symposium on Circuits And Systems (ISCAS), Geneva, May 2000.
[84] ISO/IEC JTC1/SC29/WG11 (MPEG) Document N4668, Overview of the MPEG-4 Standard, Jeju, 2002,
available at the MPEG website <http://mpeg.chiariglione.org>.
[85] N. Iwakami, T. Moriya, Transform Domain Weighted Interleave Vector Quantization (TWINVQ), 101st AES
Convention, Los Angeles, 1996, Preprint 4377.
[86] R. Sperschneider, Error Resilient Source Coding with Variable Length Codes and Its Application to MPEG
Advanced Audio Coding, 109th AES Convention, Los Angeles 2000, Preprint 5271.
[87] J. Herre, E. Allamanche, K. Brandenburg, et al., The Integrated Filterbank Based Scalable MPEG-4 Audio
Coder, 105th AES Convention, San Francisco, 1998, Preprint 4810.
[88] ISO/IEC
JTC1/SC29/WG11
(MPEG),
International
Standard
ISO/IEC
14496-3:2001/AMD1:
2003, Bandwidth Extension, 2003.
[89] ISO/IEC JTC1/SC29/WG11 (MPEG), ISO/IEC 14496-3:2001/AMD2:2004, High-Quality Parametric Audio
Coding, 2004.
[90] ISO/IEC JTC1/SC29/WG11 (MPEG) Document N7137, Listening test report on MPEG-4 High
Efﬁciency AAC v2, Busan, 2005, available at the MPEG website
<http://mpeg.chiariglione.
org/qualitytests.php>
[91] M. Multrus, M. Neuendorf, J. Lecomte, et al., MPEG Uniﬁed Speech and Audio Coding—Bridging the Gap,
Microelectronic Systems: Circuits, Systems and Applications, in: Albert Heuberger, Günter Elst, Randolf
Hanke (Eds.), Springer Berlin, Heidelberg, 2011, pp. 343–353.
[92] M. Neuendorf, M. Multrus, N. Rettelbach, et al., MPEG Uniﬁed Speech and Audio Coding—The ISO/MPEG
Standard for High-Efﬁciency Audio Coding of all Content Types, 132nd AES Convention, Budapest, Hun-
gary, April, 2012.
[93] ISO/IECJTC1/SC29/WG11 (MPEG), ISO/IEC 23003-3: 2012, MPEG-D (MPEG audio technologies), Part
3: Uniﬁed speech and audio coding, 2012.
[94] B. Edler, S. Disch, S. Bayer, et al., A time-warped MDCT approach to speech transform coding, 126th AES
Convention, 2009, Preprint 7710.
[95] J. Herre, M. Dietz, Standards in a nutshell: MPEG-4 high-efﬁciency AAC coding, IEEE Signal Process.
Mag. 25 (3) (2008) 137–142.

References
799
[96] M. Schnell, M. Schmidt, M. Jander, et al., MPEG-4 Enhanced Low Delay AAC—A New Standard for High
Quality Communication, 125th AES Convention, October 2008, Preprint 7503.
[97] ISO/IEC
JTC1/SC29/WG11
(MPEG)
Document
N10032,
Report
on
the
Veriﬁcation
Test
of
MPEG-4
Enhanced
Low
Delay
AAC,
Hannover,
2008,
available
at
the
MPEG
web
site
<http://mpeg.chiariglione.org/qualitytests.php>.
[98] M. Schnell, R. Geiger, M. Schmidt, et al., Low delay ﬁlterbanks for enhanced low delay audio coding, in:
IEEE ASSP Workshop on Applications of Signal Processing to Audio and Acoustics, Mohonk, NY, 2007.
[99] M. Lutzky, M. Luis Valero, M. Schnell, J. Hilpert, AAC-ELD V2—The New State of the Art in High Quality
Communication Audio Coding, 131st AES Convention, October 2011, Preprint 8516.
[100] International
Telecommunication
Union-Telecommunication
Standardization
Sector,
<http://
www.itu.int/ITU-T/>
[101] U. Wüstenhagen, B. Feiten, J. Kroll, et al., Evaluation of Super-Wideband Speech and Audio Codecs, 129th
AES Convention, November 2010, Preprint 8205.
[102] M. Hans, R.W. Schafer, Lossless compression of digital audio, IEEE Signal Proc. Mag. 18 (2001) 21–32.
[103] ISO/IEC 14496-3:2005/Amd.2:2006, Coding of Audio-Visual ObjectsPart 3: Audio, Amendment 2: Audio
Lossless Coding (ALS), New Audio Proﬁles and BSAC Extensions, International Standards Organization,
Geneva, Switzerland, 2006.
[104] T. Liebchen, T. Moriya, N. Harada, et al., The MPEG-4 Audio Lossless Coding (ALS) Standard-Technology
and Applications, 119th AES Convention, October 2005, Preprint 6589.
[105] ISO/IEC JTC1/SC29/WG11 (MPEG) Document N7686, Veriﬁcation Report on MPEG-4 ALS, Nice, 2005,
available at the MPEG website <http://mpeg.chiariglione.org/qualitytests.php>.
[106] ISO/IEC 14496-3:2005/Amd.3:2006, Coding of Audio-Visual ObjectsPart 3: Audio, Amendment 3: Scalable
Lossless Coding (SLS), International Standards Organization, Geneva, Switzerland, 2006.
[107] R. Geiger, J. Herre, J. Koller, K. Brandenburg, IntMDCT—a link between perceptual and lossless audio
coding, in: IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), Orlando, FL, May 2002.
[108] ISO/IEC JTC1/SC29/WG11 (MPEG) Document N7687, Veriﬁcation Report on MPEG-4 SLS, Nice, 2005,
available at the MPEG web site <http://mpeg.chiariglione.org/qualitytests.php>.
[109] ISO/IEC 23003-1:2007, MPEG-D (MPEG audio technologies), Part 1: MPEG Surround, 2007.
[110] J. Breebaart, Analysis and synthesis of binaural parameters for efﬁcient 3D audio rendering in MPEG
Surround, in: IEEE Int. Conf. on Multimedia and Expo (ICME), Beijing, China, 2007.
[111] ISO/IECJTC1/SC29/WG11 (MPEG), ISO/IEC 23003-2: 2010, MPEG-D (MPEG audio technologies), Part
2: Spatial Audio Object Coding (SAOC), 2010.
[112] Internet Engineering Task Force (IETF), RFC 6416, RTP Payload Format for MPEG-4 Audio/Visual Streams,
2011.
[113] Internet Engineering Task Force (IETF), RFC 5691, RTP Payload Format for Elementary Streams with
MPEG Surround Multi-Channel Audio, 2009 (amendment to RFC 3640 RTP Payload Format for Transport
of MPEG-4 Elementary Streams).
[114] Consumer Electronics Association (CEA), CEA-861.1, Audio Format Extensions, August 2010.
[115] European Telecommunications Standards Institute (ETSI), ETSI ES 201 980, V3.1.1. Digital Radio Mondiale
(DRM); System Speciﬁcation, June 2009.
[116] European Telecommunications Standards Institute (ETSI), ETSI TS 102 563 V1.1.1, Digital Audio Broad-
casting (DAB); Transport of Advanced Audio Coding (AAC) Audio, February 2007.
[117] European Telecommunications Standards Institute (ETSI), ETSI TS 102 005 V1.4.1, Digital Video Broad-
casting (DVB); Speciﬁcation for the use of Video and Audio Coding in DVB services delivered directly over
IP protocols, March 2010.

800
CHAPTER 28 Perceptual Audio Coding
[118] European Telecommunications Standards Institute (ETSI), ETSI TS 101 154 V1.9.1, Digital Video Broad-
casting (DVB); Speciﬁcation for the use of Video and Audio Coding in Broadcasting Applications based on
the MPEG-2 Transport Stream, September 2009.
[119] J. Makinen, B. Bessette, S. Bruhn, et al., AMR-WB+: a new audio coding standard for third generation
mobile audio services, in: IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), vol. 2,
2005, pp. 1109–1112.
[120] H. Zhong, L. Villemoes, P. Ekstrand, et al., QMF Based Harmonic Spectral Band Replication, 131st Audio
Engineering Society Convention, 2011.
[121] L. Villemoes, P. Ekstrand, P. Hedelin, Methods for enhanced harmonic transposition, in: IEEE Workshop on
Applications of Signal Processing to Audio and Acoustics (WASPAA), 2011.
[122] J. Kim, E. Oh, J. Robilliard, Enhanced Stereo Coding with Phase Parameters for MPEG Uniﬁed Speech and
Audio Coding, 127th AES Convention, New York, October 2009.
[123] C. Helmrich, P. Carlsson, S. Disch, et al., Efﬁcient transform coding of two-channel audio signals by means of
complex-valued stereo prediction, in: IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP),
May 2011, pp. 497–500.

29
CHAPTER
Introduction to Acoustic Signal
Processing
Patrick A. Naylor
Department of Electrical and Electronic Enginerring, Imperial College, Exhibition Road, London, UK
4.29.1 Background
This section addresses the processing of audio frequency signals either captured or rendered in a room
or other enclosure that is reverberant to some extent and usually also contains some level of noise. Such
acoustic signals are normally captured using one or more microphones, possibly operating collabora-
tively as an array. Rendering of signals is, on the other hand, performed using one or more loudspeakers
that could also be operated collaboratively as a loudspeaker array or as a spatially sampled approximation
to a spatially continuous sound source.
Prior to the advent of electronics and digital signal processing, only very limited processing of
acoustic signals was feasible, being performed usually by physical means. An example is the iconic
gramophone horn which serves to amplify and acoustically couple the vibrations of a needle tracking
a 78 RPM record. Very early hearing aids used horns in the opposite sense—to capture sound so it can
be heard more clearly by hearing-impaired listeners. It might even be argued that the oldest and, at the
same time, most sophisticated acoustic signal processing system is to be found in the human hearing
system.
The art and science of architectural acoustics have been practiced for many hundreds or even thou-
sands of years and have continued to be developed in both sophistication and aesthetic quality. The
architects of medieval cathedrals exploited highly reﬂective building materials and lofty vaulting to
provide an acoustic impression of an ethereal world. In recording studios, architects and designers have
long since made use of sound absorbing materials for the purposes of both acoustic isolation and control
of reverberation.
The growing popularity and feasibility of Digital Signal Processing (DSP) in the 1970s and 1980s
opened the door to ideas of acoustic signal processing devices employing analog-to-digital convert-
ers, digital-to-analog converters and programmable or ASIC-based DSP devices. This technology was
eagerly fostered by the telecommunications industry as well as the professional audio industry and,
more recently, for widespread use in consumer products. Such is the success of acoustic signal pro-
cessing technology that the reader of this book is likely to have at least one or two devices within arms
reach containing and reliant upon acoustic signal processing algorithms—in mobiles phones, laptop
and desktop computers, TV and other home entertainment equipment to name but a few examples. In
some cases, the acoustic signal processing technology is employed to provide speciﬁc functionality
such as a hands-free mode in a telephone. In other cases, the acoustic signal processing technology is
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00029-7
© 2014 Elsevier Ltd. All rights reserved.
803

804
CHAPTER 29 Introduction to Acoustic Signal Processing
employed to compensate for quality degradations associated with the usage scenario—user mobility
enabling phones to be used while walking on streets with associated trafﬁc noise, for example.
Whatever the application or motivation, acoustic signal processing is now an essential element of
many people’s everyday life. This Section will highlight a selection of the technical challenges and the
methods used to address them.
4.29.2 Overview of the chapters
4.29.2.1 Sound ﬁeld synthesis
Sound ﬁeld synthesis aims to render virtual acoustic environments, typically in a listening room or in
an immersive multimedia scenario, in which the users sense only the complete audiovisual entity rather
than the various constituent elements of sound and vision. A key feature of sound ﬁeld synthesis in
contrast to, for example, binaural rendering is that it aims to produce a listening area—a zone in space
for which the perceptual audio effects can be controlled in a satisfying way—that is signiﬁcantly large,
given the application. Clearly, a listening area of only a few cm span would be impractical for even the
most straightforward audio applications. Synthesizing a sound ﬁeld relies on the use of an organized
collection of loudspeakers. Each loudspeaker must be driven with an appropriate signal such that the
sum of signals from all loudspeakers renders the required sound ﬁeld over the listening zone. A typical
requirement of a sound ﬁeld might be one in which a musical instrument is rendered at a certain distance
and in a certain direction from the listener. Another example might be to render a moving source of
sound such as a circling helicopter in an action movie.
Analysis of acoustic phenomena requires specialized mathematical tools. Analysis techniques based
on Near-ﬁeld Compensated Higher Order Ambisonics, Spectral Division and Wave Field Synthesis are
available in the literature. These mathematical tools usually assume a spatially continuous formulation
of loudspeakers, which has to be to some extent approximated by a ﬁnite set of loudspeaker sources.
In contrast, multipoint approaches begin with a discrete set of loudspeaker sources and are generally
much more ﬂexible in terms of the positioning of those loudspeakers.
These issues are discussed in the chapter after the foundations of physical acoustics and the wave
equation have been set out, along with a consideration of space-time information-bearing signals and the
Green’sfunctiondescription.ThetopicofsoundﬁeldsynthesisisbasedontheKirchhoff-Helmholtzinte-
gral formulation. The three rendering approaches of Near-ﬁeld Compensated Higher Order Ambisonics,
the Spectral Division Method and Wave Field Synthesis are then described initially assuming a spatial
continuous source distribution and subsequently the spatially sampled distributions. Applications of
sound ﬁeld synthesis are also highlighted.
4.29.2.2 Acoustic echo control
Acoustic echo control is a key signal processing element of a wide range of human-machine interfaces—
those using voice, possibly in hands-free mode, and requiring full duplex operational capability. To some
extent it is therefore a relatively mature technology, particularly in the single-channel case, a statement
which is explained by realizing that most mobile phones include at least some processing related to
echo control.

4.29.2 Overview of the Chapters
805
The basic problem which is addressed by acoustic echo control is to suppress annoying echo encoun-
tered typically in telephony by canceling the coupling, also known as feedback, from a sound source
(loudspeaker) to a microphone in a closed loop system. This is often explained by considering the hands-
free mode of a phone in which the microphone for the hands-free near-end user picks up the near-end
user’s voice, as designed, but also picks up the sound of the far-end user’s voice via the loudspeaker. The
far-end user then hears an echo of their own voice after transmission through the network with delay.
Such echo presents the far-end user with great difﬁculty in holding an interactive dialog, the difﬁculty
increasing with both the level of the echo and the amount of delay.
This chapter generalizes and formalizes the problem of acoustic echo and provides an analytical and
practical framework within which to discuss a range of solutions, from the historical to the cutting-
edge. The typical applications of acoustic echo control are set out and, from these, relevant measures of
performance and quality can be seen to arise. Since acoustic echo control aims to reduce the perceived
annoying effect of echo, the relation of instrumental measures to human perception is, of course, very
important. Much study has been undertaken through the standardization processes of recent years that
has led to a good understanding of appropriate evaluation methodologies for acoustic echo control, as
is outlined in Section 5.4.
The Chapter then proceeds to discuss a set of issues of current interest in the topic of acoustic echo
control. It is clear when dealing with real rooms that room acoustic systems are far from stationary so
that adaptive solutions, capable of identifying and tracking time-varying acoustic systems, are essential.
Modeling of time-varying echo paths, together with adaptive ﬁltering approaches, are discussed in
Section 1.6 in which the use of a post-ﬁltering approach is also introduced.
Signiﬁcant recent interest in acoustic echo control has been focused around multichannel audio sys-
tems such as those found in interactive TV, in-car entertainment systems and desktop conferencing.
Echo in the multichannel case is almost always substantially more challenging to control from a tech-
nical perspective and has therefore sparked research into many more advanced approaches than were
previously available for the single-channel case. The chapter discusses the multichannel scenario and
multichannel methods of acoustic echo control in Section 1.8.
Classical linear system theory has always played a strong role in the development of acoustic echo
control methods that use adaptive system identiﬁcation techniques—adaptive systems which converge
towards some theoretically optimum solution such as the well known Weiner solution. Interestingly,
however, in low-cost terminals such as mobile phones, the transducers are far from linear, particularly at
high signal levels. In such scenarios, acoustic echo controllers that employ assumptions of linearity of the
acoustic echo system will suffer some signiﬁcant degradation in performance due to the nonlinearities
arising in the loudspeaker and/or microphone. To overcome such performance degradations, research
on nonlinear modeling and nonlinear acoustic echo control has attempted to introduce a capability
for handling those degrading nonlinearities for relevant applications. These issues are discussed. The
chapter closes by outlining several application scenarios more speciﬁcally and providing a perspective
on the topic in terms of open issues and future trends.
4.29.2.3 Dereverberation
The chapter on Dereverberation introduces the concepts and applications relating to both reverberation
and dereverberation. Most people have experienced reverberation in positive ways such as the beautiful

806
CHAPTER 29 Introduction to Acoustic Signal Processing
beguiling acoustics of concert halls or cathedrals, and even had fun experimenting with the echo in a
subway tunnel or cave. Reverberation is also shown to be an essential element of spatial awareness,
allowing humans to understand better their environment and the sounds in that environment. Sound
recording engineers also use reverberation to provide attractive levels of “naturalness” to music and
other recorded sounds.
The chapter begins by discussing some speciﬁc example applications of reverberation and, in par-
ticular, dereverberation. It then reviews the physical processes of sound propagation that give rise
to reverberation and introduces channel-based room acoustics modeling based on the Room Impulse
Response (RIR). This model is of key importance to much of the understanding and development of
room acoustics. Through use of the model, some insights are offered relating the physical process of
acoustic reverberation to the perceived effects.
The main approaches for dereverberation processing are then overviewed. This is a fast-moving
research topic, with many new articles being published in swift succession. However, the chapter aims
to divide the approaches into the main categories of spatial ﬁltering, speech enhancement and channel-
based equalization. This is not an exhaustive list, of course, but is intended to provide a framework
within which the many diverse approaches can be organized. Having set the framework in place, more
details of the methods in each of the three categories are presented in Sections 5, 6 and 7 respectively,
with references to further relevant reading material.
The measurement of reverberation is a complicated but important topic. Instrumental measures can be
computed from the RIR or from the reverberant and reference signals, and such measures are discussed
in Section 4. An understanding of the signiﬁcance of these measures to practical use of dereverberation
technology is not yet available, and standardized measurement procedures are still to be set out and
agreed. Listening tests are relied on in many cases and studies to relate instrumental measures to listener
perception are of key importance in this endeavor.
This aspect of measurement is one of several open challenges which serve to make dereverberation a
lively research topic, full of fascinating questions and with high potential for commercial applications.

30
CHAPTER
Acoustic Echo Control
Gerald Enzner*, Herbert Buchner†, Alexis Favrot‡ and Fabian Kuech§
*Ruhr-Universität Bochum, Universitätsstraße 150, D-44780 Bochum, Germany
†Machine Learning Group, Technische Universität Berlin, Franklinstraße 28/29, D-10587 Berlin, Germany
‡Illusonic LLC, Chemin du Trabandan 28A, CH-1006 Lausanne, Switzerland
§Fraunhofer Institut für Integrierte Schaltungen, Am Wolfsmantel 33, D-91058 Erlangen, Germany
Nomenclature
List of mathematical symbols
The following general conventions are used in this chapter: Matrices and vectors are written boldface,
e.g., x(n), matrices often uppercase and vectors lowercase, while scalar sequences are written non-bold,
e.g., x(n). Quantities in the frequency-domain are written in uppercase Latin, e.g., X(k, m), or boldface
in the DFT-domain, e.g., S(m). Estimated quantities are labeled with a hat, e.g., ˆs(n). Some special
quantities are denoted by Greek symbols:
d(n)
acoustic echo signal
e(n)
error signal after acoustic echo cancellation
fs
sampling frequency
G(m, ν)
echo path transfer function (delay compensated!)
hn
acoustic echo path coefﬁcients at time-lag n
I
identity matrix
k(n)
time-varying Kalman gain vector
m
block time index, m ∈Z
n
discrete sampling time index, n ∈Z
N
number of FIR ﬁlter coefﬁcients
Rs(n)
time-varying covariance matrix of near-end speech s
s(n)
near-end speech signal (including environmental noise)
x(n)
received signal from the far speaker
y(n)
microphone signal
μ
stepsize factor of LMS-type algorithms
ν
discrete frequency index, ν = 0, 1, . . . , M −1
σ 2
s (n)
time-varying power, i.e., variance, of near-end speech s(n)
( · )H
transposition and complex conjugation of the argument
( · )T
transposition of the argument
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00030-3
© 2014 Elsevier Ltd. All rights reserved.
807

808
CHAPTER 30 Acoustic Echo Control
E{·}
statistical expectation operator
ln, log
natural and base-10 logarithm
List of Abbreviations
A/D
Analog-to-Digital
AEC
Acoustic Echo Control
AES
Acoustic Echo Suppression
AGC
Adaptive (or Automatic) Gain Control
APA
Afﬁne Projection Algorithm
DCR
Diagonal Coordinate Representation
DFT
Discrete Fourier Transform
EEF
Echo Estimation Filter
EOS
Equivalent Orthogonal Structure
ESF
Echo Suppression Filter
ERLE
Echo Return Loss Enhancement
FDAF
Frequency-Domain Adaptive Filter
FFT
Fast Fourier Transform
FIR
Finite Impulse Response
IDFT
Inverse Discrete Fourier Transform
IFFT
Inverse Fast Fourier Transform
IIR
Inﬁnite Impulse Response
IP
Internet Protocol
ITU
International Telecommunication Union
LMS
Least Mean-Square Algorithm
MCAEC
Multichannel Acoustic Echo Cancellation
MMSE
Minimum Mean-Square Error
NLMS
Normalized Least Mean-Square Algorithm
PSD
Power Spectral Density
RLS
Recursive Least-Squares Algorithm
SER
Signal-to-Echo Ratio
SNR
Signal-to-Noise Ratio
STFT
Short-Time Fourier Transform
VAD
Voice Activity Detector
4.30.1 Introduction
Acoustic echo control (AEC) refers to signal processing technology used in communication systems
with full-duplex hands-free acoustic man-machine interfaces. AEC technology is required to com-
bat the undesired acoustic coupling between sound reproduction and acquisition in a system. In this
introductory section, we ﬁrst clarify the problem statement, before we describe typical elements of the

4.30.1 Introduction
809
signal processing chain in sound front-ends of hands-free communication systems. We then refer to
most relevant applications and quality measures for AEC technology.
4.30.1.1 Problem statement and early developments
In the signal model of a hands-free voice communication system in Figure 30.1, the task of the AEC
unit is to reproduce the desired near-end speech s(n), at discrete time n, in the output signal ˆs(n) in
sending direction of the system, while suppressing the undesired echo d(n) of the far-end speech x(n).
The echo signal due to the echo path hn typically consists of the direct sound between loudspeaker and
microphone as well as multiple reﬂections of walls in the enclosure. The larger the roundtrip delay of
the far-end speech in an end-to-end communication, the more echo attenuation is required [1].
The black-box AEC in Figure 30.1 represents different types of signal processing that have been
pursued to achieve the separation of the echo from the near-end speech. Voice controlled switching has
been developed in the 1960s and is still used in many products to suppress the acoustic echo of the far
speaker. In an example of this technique, the hands-free microphone signal y(n) is strongly attenuated
whenever a received signal x(n) from the far speaker side is observed. Alternatively, we could attenuate
the received signal x(n) before reproduction by the loudspeaker if near-end speech is detected and has
priority. In any case, the loop gain from x(n) to ˆs(n) should provide the required echo attenuation.
Voice controlled switching can be implemented easily, analog or digital, but the fundamental problem
is that switching effectively leads to an unacceptable half-duplex connection between both ends of
the communication system. Especially the background noise transmission would suffer from on-off
modulations. Therefore, voice controlled switches (or other gain functions in the system) are nowadays
implemented in conjunction with comfort noise injection [2].
Many of the current developments in AEC can be traced back to the fundamental idea of creat-
ing a replica of the echo path impulse response hn in Figure 30.1 using FIR ﬁlter structures. The
FIR ﬁlter is essentially placed in parallel to the echo path and stimulated by the loudspeaker signal.
h n
s(n)
d(n)
s n)
(
y ( n)
x (n)
x (n)
To Far Speaker
From Far Speaker
Echo
Path
Acoustic
Echo
Controller
FIGURE 30.1
Acoustic front-end of hands-free voice communication systems.

810
CHAPTER 30 Acoustic Echo Control
The resulting echo replica at the ﬁlter output can be subtracted from the microphone signal to achieve
echo cancellation and said FIR ﬁlter is thus termed echo canceler. This principle was originally con-
ceived in the 1960s to solve the problem of line echoes due to non-ideal hybrids in long distance
telephone networks [3,4]. Using this technology for canceling acoustic echoes, however, turns out to be
a quite challenging task, since the duration of the impulse response of an acoustical echo path is usually
many times longer than impulse responses of line echoes. Depending on the acoustic environment and
the sampling frequency, acoustic echo cancellation then requires FIR ﬁlters with a very large number of
coefﬁcients. In the small interior of the car environment and for the low sampling frequency of 8 kHz,
i.e., for telephone quality, it already amounts to hundreds of coefﬁcients, while in the ofﬁce environment
we may even encounter thousands of taps depending on the reverberation time [4]. It has been shown
that IIR ﬁlters will not achieve better echo path modeling with lesser coefﬁcients [5].
4.30.1.2 Typical building blocks of current systems
Virtually all existing systems are based on the early ideas of echo attenuation and echo cancellation.
In practice, it turns out that hybrid solutions often provide the best performance. The mainstream in
system development for acoustic echo control is wrapped up in Figure 30.2, which basically represents
a zoom into the black-box AEC of Figure 30.1. If the echo canceler would exactly match the echo
path, then the echo signal could be eliminated completely from the microphone signal. However, echo
paths are in most of the cases a priori unknown and time-varying systems (e.g., due to moving persons
in the vicinity of loudspeaker and microphone) and for this reason adaptive ﬁlters and adaptive ﬁlter
algorithms have to be considered. The developments for the adaptive echo canceler are reported in great
detail, e.g., in [6–9] and references therein.
Due to the related computational load and numerical requirements, the ﬁeld of acoustic echo
cancellation eventually has been understood as an application of very-high-order adaptive ﬁlters [6]. In
this way, it has also been an important driving force in the development of new and powerful adaptive
ﬁltering algorithms. Among the desirable properties of adaptive echo cancellation ﬁlters, we have low
computational complexity, low memory requirements, fast convergence, and excellent tracking ability.
Due to this multitude of requirements which have to be satisﬁed simultaneously, the ﬁrst successful
implementations and commercial products for the acoustic echo case were available not until the 1980s
[4]. Again considering Figure 30.2, the most frequently used adaptive ﬁlter algorithms in the echo
cancellation context are the gradient-based NLMS, APA, RLS, and FDAF type of algorithms. Their
properties are thoroughly described for example in [10] and brieﬂy summarized here:
•
The normalized least-mean-square (NLMS) algorithm is by far the most popular technique for
adaptive identiﬁcation of the acoustic echo path [2]. The popularity of the NLMS algorithm is due
to its simplicity, low complexity, and robust (i.e., model-independent) performance. In the case of
correlated (i.e., non-white) echo path input signals x(n), such as speech, the convergence rate of
the NLMS algorithm is, however, too slow to track the echo path impulse response of time-varying
acoustic environments. This problem can be resolved partly by applying decorrelation ﬁlters to
the input x(n) and the echo cancellation error e(n) before feeding them into the NLMS algorithm
[11,12].
•
The afﬁne projection algorithm (APA) introduced in [13] utilizes several previous input vectors
to determine the input signal correlation. The APA can be seen as a generalization of the NLMS

4.30.1 Introduction
811
Echo
NLMS
APA
Variable Stepsize
Echo
Canceler
Post
Processor
System Distance Estimation
Path
Speech and Noise
Center Clipper
Gain Functions
Comfort Noise
Postfiltering
−
Various (Combined)
Control Mechanisms
Delay Coefficients Method
Double Talk Detection
Remote Single Talk Detection
Two Echo Path Model
Dynamic Regularization
Robust Statistics
Residual Echo Estimation
RLS
FDAF
Near−end
To Far Speaker
From Far Speaker
Echo
e(n)
y(n)
x(n)
s(n)
d(n)
d(n)
s(n)
FIGURE 30.2
Elements of a hands-free communication system.

812
CHAPTER 30 Acoustic Echo Control
algorithm and converges rapidly for autoregressive input signals [14]. A computationally efﬁcient
realization has been presented in [15].
•
The fastest convergence is achieved by the recursive least-squares (RLS) algorithm. For decorre-
lation purposes, the RLS algorithm utilizes the inverse of the autocorrelation matrix of the input
signal, the numerical computation of which is demanding and needs to be handled with care [16].
Computationally efﬁcient implementations have been developed [17,18], but still the RLS algorithm
is often considered as too cumbersome for practical applications.
•
Subband solutions and frequency-domain adaptive ﬁlters (FDAF) have been developed as interesting
alternatives with good convergence properties, low computational complexity, and often favorable
numerical properties [19–21]. The drawback inherent to most of these techniques is the signal
delay that is caused by analysis and synthesis ﬁltering or windowing. The signal delay related
to the FDAF can be reduced either by using the partitioned-block frequency-domain adaptive ﬁl-
ter (PBFDAF)/multi-delay ﬁlter (MDF) [22–24], or the speciﬁc soft-partitioned frequency-domain
adaptive ﬁlter (SPFDAF) [25], or simply by choosing large frame overlap in the FDAF overlap-
save architecture, e.g., [26–28], the latter at the expense of larger computational load. In all these
variants of the FDAF, the algorithmic signal delay and the actual ﬁlter length can be adjusted inde-
pendently. Delayless subband adaptive ﬁlters, which essentially perform the adaptation process in
the frequency-domain and the actual ﬁltering in the time-domain, are described, e.g., in [29,30].
•
Another class of algorithms, with increasing popularity, exploits the sparseness of impulse responses
in both network and acoustic echo cancellation applications. In particular, the sparseness is taken
into account by updating the ﬁlter coefﬁcients independently of each other with different adap-
tation time constants—in proportion to the magnitude of the already estimated ﬁlter coefﬁcients.
Implementation of this proportionate mechanism obviously requires another feedback loop in the
algorithm, besides the already utilized output error signal. Larger coefﬁcients are then adjusted with
faster convergence, while smaller coefﬁcients converge slower, and consequently the overall speed
of convergence increases. One of the ﬁrst proportionate algorithms is found in [31]. Since that time,
variants and improvements of the basic proportionate NLMS (PNLMS) algorithm have been pre-
sented, e.g., [32,33]. All this exploitation of sparseness in echo cancellation should be viewed in the
context of seminal work on sparse regression [34].
Adaptive algorithms as mentioned here have two (related) fundamental problems in common: track-
ing ability and robustness. It turns out difﬁcult to let the echo canceler coefﬁcients follow the true
time-varying echo path of realistic acoustic environments. On the one hand, the adaptation must be fast
enough to track the time-varying system, while on the other hand, the adaptation must be robust against
interfering near-end speech (so-called double-talk situation) and background noise. Both requirements
are contradicting and herein lies the key problem of acoustic echo control. As a result, sophisticated
control mechanisms were proposed to support the fast and robust adaptation of echo canceler coefﬁ-
cients [2,35]. A perfect solution is, however, not available due to the nature of a statistical optimization
problem. In an attempt to satisfy both requirements, many systems utilize time-varying adaptive stepsize
control to accelerate the adaptation in the absence of near-end speech and noise and to slow down (or
even halt) otherwise. Since there are as many publications on stepsize control as on adaptive ﬁlters, we
can refer to only some contributions.

4.30.1 Introduction
813
An optimum stepsize for the NLMS algorithm has been determined by quite some authors [2,10,36–
39], where the optimization is always based on the idea of minimum mean-square system distance
between echo canceler and echo path. Optimum stepsizes for the FDAF and the PBFDAF have been
derived in [40]. Unfortunately, all these stepsizes cannot be implemented directly as the (frequency-
dependent) time-varying system distance between echo canceler and echo path is also required as an
input parameter, but difﬁcult to measure out of the lab. Thus, suboptimal control mechanisms have
been developed to approximate the optimum stepsize. Some methods have been designed to explicitly
estimate the actual system distance, whereas other methods aim to facilitate system distance estimation
or to control the adaptation directly:
•
The popular delay coefﬁcients method computes the system distance from the leading coefﬁcients
of the adaptive ﬁlter, provided that the echo path has natural or artiﬁcal zeros at the corresponding
impulse response lags [39]. It is important to note that the delay coefﬁcients method alone is not able
to deliver a reliable estimate of the system distance. An additional detector for echo path changes is
required to avoid stalling (freezing) of the adaptation [2,10,36].
•
Double talk detectors (DTD) can be utilized to directly halt the adaptation in the presence of near-end
speech at the microphone [35] or, alternatively, to facilitate system distance estimation [2]. DTD
can be based on cross-correlation measures [41] or on the simple comparison of signal powers or
magnitudes [42]. Remote single talk detection (voice activity detection) as described in [2] is closely
related to DTD.
•
The two echo path model [43] can be used in different ways to control the adaptation. Basically, the
approach models a fast and a slowly changing echo path by a background and a foreground adaptive
ﬁlter, respectively. An evaluation and comparison of their respective echo cancellation errors then
provides means for adaptive stepsize control in the foreground adaptive ﬁlter. This technique ﬁnds
widespread application in cases where its computational complexity is tolerable.
•
Dynamic regularization controls gradient adaptive ﬁlters by means of a time-varying additive quan-
tity in the denominator of the gradient [10,44,45]. It was shown in [46] for the NLMS algorithm
that optimum regularization is equivalent to optimum stepsize control.
•
The term residual echo power estimation [47–50] basically refers to the same thing as system distance
estimation, since the respective quantities are simply related by the available echo path input signal
power. System distance estimation is usually preferable, as it separates system properties from signal
properties.
In recent years, in addition to the above control mechanisms, adaptation algorithms have been devel-
oped which are inherently more robust to double talk. Instead of the least-squares approach as in NLMS,
APA, RLS (which corresponds to Gaussian noise assumption) they are based on more advanced stochas-
tic signal models. Basically, there are three fundamental stochastic signal properties that may be taken
into account in these models, and hence, be exploited by the adaptation algorithm: nonstationarity,
nonwhiteness, and nongaussianity.
The simplest way of exploiting nongaussianity—which is equivalent to taking into account higher-
order statistics (HOS)—is to apply a suitable error nonlinearity within the optimization criterion. In
the context of echo cancellation the error nonlinearity was already mentioned in the early paper [3] in
order to cope with outliers caused by misdetections of the double-talk detector. The outliers generally
exhibit a supergaussian probability density. Later, the concept of robust statistics [51], known from

814
CHAPTER 30 Acoustic Echo Control
the statistics literature to systematically address the need of outlier robustness, has been identiﬁed as a
powerful and systematic statistical framework for the control of adaptive ﬁlters [35,52,53]. It turns out
that this concept can in fact be seen as a systematic and statistically motivated reﬁnement of the use of
an error nonlinearity together with a certain adaptive scaling factor.
Another important and even more general class of HOS-based statistical methods is independent
componentanalysis(ICA)[54]whichcanbeinterpretedfromaninformation-theoreticpointofview,and
was originally developed for the problem of blind source separation. Considering now the acoustic echo
control problem from this information-theoretic point of view, an alternative criterion for acoustic echo
control would be to minimize the mutual information between the transmission signal ˆs(n) according
to Figure 30.1 and the loudspeaker signal x′(n). In other words, the transmission signal ˆs(n) should
be made statistically independent from the echo contribution. Hence, from this point of view, AEC
can be considered as a (supervised) signal separation problem with the accessible loudspeaker signal
as side information. Indeed, using a generic broadband formulation of ICA for convolutive mixtures,
a conceptually simple, yet fundamental relation between AEC and blind adaptive ﬁltering algorithms
was established in this way in [55,56].
In addition to the nongaussianity, the nonstationarity and the nonwhiteness of the signals can be
expoited by the adaptation algorithm. Indeed, all the above-mentioned double-talk handling approaches
are based directly on the nonstationarity property of the near-end speech signal s(n), and in some cases
on the nonwhiteness of the involved signals. Both the nonwhiteness and nonstationarity are captured
by (time-varying) correlation matrices in multivariate second-order statistics.
In [55,57] all three fundamental approaches, i.e., exploiting nongaussianity, nonwhiteness, nonsta-
tionarity, are combined in one generic information-theoretic broadband adaptive ﬁltering framework,
which we call TRINICON (“TRIple-N Independent component analysis for CONvolutive mixtures”).
An interesting ﬁnding from this top-down approach is that the generic framework contains not only
all well-known adaptation algorithms mentioned above (e.g., NLMS, APA, RLS) but it also inherently
includes the various known adaptation controls mentioned above, including robust statistics [55,56].
This has led to various new insights into the adaptation mechanisms and we believe that it continues to
be a valuable source of synergy effects for the development of new and improved adaptation algorithms.
Inadditiontothemodelingandexploitationofthesesignalproperties,laterpartsofthischapterwillbe
concerned also with introducing the somewhat younger treatment of system properties, especially system
uncertainties. Thereby we will mostly focus on minimum mean-square error (MMSE) optimization
based on Gaussian statistics, i.e., the special case of second-order statistics. A joint consideration of
both signal and system properties has not been achieved yet and may thus represent a future endeavor.
Despite the availability of fast and robust adaptive echo cancelers, residual echo always remains
after the echo canceler and it is widely accepted that an echo canceler alone will not be able to deliver
sufﬁcient echo attenuation in all situations. Adaptive echo cancellation combined with voice controlled
switching is thus implemented in real systems to improve the echo attenuation, but the distortion of the
desired signal s(n) due to switching can still be unacceptable. Another major branch of developments in
the ﬁeld of acoustic echo control is thus devoted to more sophisticated post-processing for residual echo
suppression in the sending path of the communication system, as shown by Figure 30.2. In stand-alone
form, i.e., without an echo canceler in the system, we refer to echo suppression technology. The latter
is most applicable if the many requirements to the adaptive echo canceler (such as low complexity,

4.30.1 Introduction
815
fast tracking ability, clock synchronization etc.) cannot be satisﬁed in an application at hand. Further
motivation for stand-alone echo suppression will be provided in Section 4.30.3.
Frequency-selective post-processing, in conjunction with echo cancellation, is also termed postﬁl-
tering for residual echo suppression [58–61]. In principle, the operation of a residual echo suppression
postﬁlter is very similar to that of a noise suppression ﬁlter and it was reported that both functionalities
can be combined efﬁciently [58,62]. The key for a good postﬁlter is the availability of the power spectral
density (PSD) of the residual echo signal. As the residual echo is not a measurable signal, the estimation
of the residual echo PSD from the available signals has to be designed carefully [48,49].
Some publications have shown a tight relationship between the optimum statistical adaptation of echo
canceler and postﬁlter coefﬁcients [48,63,64]. Ideally, both ﬁlters employ the residual echo PSD as a
control parameter. On the one hand, the residual echo PSD governs the adaptive stepsize of LMS- and
FDAF-type adaptive echo cancellation ﬁlters and, on the other hand, it is required in the spectral weight
calculation for adaptive postﬁltering. Exploitation of this relationship leads to intelligent interaction
of both ﬁlters with shared responsibility for acoustic echo control. Interestingly, joint control of echo
canceler and postﬁlter can be realized simpler than their individual control, if the aforementioned
synergy is taken into account. This has been demonstrated, e.g., by the compact and contained algorithm
in [27], which provides the required echo attenuation and also preserves the full-duplex ability of the
system [65].
Regarding the post-processor in general, there are further options how to adjust a ﬁxed or adaptive,
linear or nonlinear, scalar or frequency-dependent echo attenuation. Post-processing techniques are,
however, not so well documented in the literature as adaptive ﬁlters for the echo canceler. Byproducts
of the control mechanisms for the echo canceler are sometimes used to adjust the post-processor. More
hints on post-processors can be found for example in [2,35,66, Chapter 7].
Yet more algorithms and variants of the aforementioned algorithms have been proposed in the
literature, so that our presentation can never be complete. Further references are provided in the course
of this chapter where applicable in the respective context. For complete bibliography, including the
history of acoustic echo control, we recommend further reading in [2,8,9,35].
4.30.1.3 Applications of acoustic echo control
The most important business for acoustic echo control technology is created by hands-free telephones
and speech dialog systems with full-duplex ability. In this chapter, we are mainly concerned with algo-
rithms for hands-free telephony in different environments. Recently, the hands-free telephone market
has been growing due to the advent of modern telecommunication systems, such as
•
mobile phones and smart phones with integrated or external hands-free loudspeaker unit,
•
car hands-free telephones (integrated or based on mobile phone),
•
desktop teleconferencing equipment (e.g., dedicated hardware or PC based solutions for
Voice-Over-IP),
•
and audio-visual environments for tele-presence (e.g., HP Halo, Cisco Telepresence, or
Tandberg T3).
The disturbing effect of the acoustic echo and the requirement for acoustic echo control essen-
tially arise from large transmission delays in modern communication. The echo signal delay (which
is twice the transmission delay) ranges from about 200 ms in mobile radio up to seconds in some

816
CHAPTER 30 Acoustic Echo Control
IP connections. The large transmission delay of IP connections necessitates acoustic echo control even in
handset or headset modes of communication. Low signal-to-noise ratio at the microphone, such as in car
hands-free telephony and mobile phones, further causes uncertainties regarding the precise detection
and separation of the disturbing echo from the desired signal.
In many of the aforementioned conﬁgurations, the hands-free voice interface is supposed to provide
for improved user-friendliness and simplicity of the communication. In the area of car telephony, the
motivation for hands-free solutions is not only given by the additional user convenience. Here, it is
rather the case that legal aspects in the form of safety regulations are the driving force for the use of
hands-free systems.
Speech dialog systems refer to applications which make use of automatic speech recognition (ASR)
units. Originally, speech recognition was mostly relevant for industrial applications, automatic answer-
ing services over the phone, and smart trafﬁc products (navigation). More recently, the availability of
modestly priced processing power has also made the ubiquitous ASR possible in smart home appli-
cations, i.e., in the consumer market. In speech dialog systems with full-duplex ability, acoustic echo
control based on the known reproduction signal is required as a preprocessing unit for ASR to avoid
confusion between user commands and simultaneous loudspeaker output. This line of research is of
high practical importance for various kinds of next-generation multimedia terminals, such as advanced
TV sets, future multimedia workstations, systems for interactive retrieval of multimedia data, computer
games, navigation, and other voice controlled systems with the ambition of continuous recognition and
reproduction.
4.30.1.4 Quality measures
According to product advertisement in the hands-free telephone market, the speech quality of the
respective solution is always excellent, i.e., the sound is loud and clear and the system has full-duplex
ability (simultaneous transmission in both directions). Of course, such statements are derived from
well-known customer needs and, therefore, they give us a ﬁrst insight into the quality aspects of hands-
free telephones. What the customer in fact expects is a quality that cannot be distinguished from the
hand-held telephone. Quality impairments are usually not acceptable and can immediately lead to a
reduced acceptance of the product in the market. This has been recognized as a serious problem, e.g.,
for premium car manufacturers if their pre-installed hands-free telephones do not fully comply with
quality expectations.
The reality of hands-free telephones shows that in quite some cases the speech quality is not sufﬁcient
yet. Typical complaints of users refer to a “thin and metallic (or reverberant) sound of the voice”, “annoy-
ing half-duplex functionality”, “insufﬁcient loudness”, or “low intelligibility due to noise”. The same
complaints were also conﬁrmed by the results of independent test labs for car hands-free telephones, e.g.,
[67,68]. The lack of quality then explains an insufﬁcient consumer acceptance of hands-free products,
as for example in the automotive area—despite the hand-held phone ban enforced by legislation.
Objective prediction and evaluation of the subjective quality of modern hands-free communication
systems is a very complex issue and is still under investigation, e.g., [69]. The difﬁculty is due to a
variety of nonlinear and time-variant signal processing in the hands-free terminals and in the network
(e.g., dynamic level control, echo cancellation, residual echo suppression, noise reduction, comfort
noise injection, coding, jitter buffering, and error concealment). Thus, the description of the perceived

4.30.1 Introduction
817
speech quality by a single value or quality index seems to be out of sight. What can be said is that the
subjective quality of hands-free telephones depends at least on the following quality parameters:
•
loudness and sound quality, i.e., listening speech quality,
•
acoustic echo attenuation, i.e., talking speech quality,
•
double talk speech quality, i.e., duplex ability,
•
and naturalness of (residual) background noise transmission.
At this point, it should be noted that the simultaneous optimization of these quality parameters is
indeed required to achieve sufﬁcient performance. This is the requirement which constitutes the actual
difﬁculty in the ﬁeld of acoustic echo control. Relaxation of just one or two quality aspects stands
for undue simpliﬁcation in the design of hands-free telephones. For instance, by signiﬁcantly reducing
the reproduction loudness of the device, we can easily avoid echo, but obviously it does not help the
communication.
Instrumental speech quality measures, which have been described, e.g., in [70,71], and newer tech-
niques which have been developed for the analysis of coded and transmitted speech, e.g., [72–74], are
not directly applicable to evaluate the quality of acoustic echo control and speech enhancement systems
in general. The reason is that the speciﬁc signal modiﬁcations and distortions which are introduced by
acoustic echo and noise reduction techniques are not explicitly modeled in these approaches.
A variety of subjective and objective quality parameters, as well as testing methods for systems which
rely on acoustic echo control, are described in the following ITU-T recommendations: [1,75–82]. A
comprehensiveandpromisingspeciﬁcationofobjectivetestproceduresforcarhands-freetelephoneshas
been enforced by the association of German car manufacturers (VDA) [83,84]. The VDA speciﬁcation
is based on previously mentioned ITU-T recommendations, but in addition to the standard quality
parameters, the VDA deﬁnes a more detailed analysis of double talk situations and background noise
transmission.
From the algorithm developer’s viewpoint in the ﬁeld of acoustic echo control, a convenient way of
assessing the echo quality of a hands-free system is the calculation of the echo return loss enhancement
(ERLE) as an indicator function for the echo attenuation achieved by the system. Based on the signals
at sampling time index n in the block diagram in Figure 30.2, the ERLE can be deﬁned via statistical
expectation E as follows:
ERLE =
E{d2(n)}
E{(d(n) −ˆd(n))2}
.
(30.1)
This formula can be applied under lab conditions when the echo signal d(n) and thus the residual echo
d(n) −ˆd(n) after echo cancellation are available explicitly. Otherwise, the echo signal d(n) can be
replaced by the noisy microphone signal y(n) and conclusions regarding the echo attenuation can be
drawn as long as the level of near-end speech and noise is low. After the post-processor in Figure 30.2,
a more suitable measure is to evaluate the resulting near-end speech quality in form of a signal-to-echo
ratio (SER) according to the following deﬁnition:
SER =
E{s2(n)}
E{(s(n) −ˆs(n))2}.
(30.2)

818
CHAPTER 30 Acoustic Echo Control
Besides the estimated speech ˆs(n), the isolated near-end speech signal s(n) needs to be available for
SER calculation. Both measures, ERLE and SER, can be represented as time-varying functions when
the statistical expectation is resolved by short-time averaging.
As shown by previous sections, most AEC systems internally rely on an adaptive estimate ˆhn of the
acoustic echo-path impulse response hn in Figure 30.1 to achieve echo cancellation. In that respect, a
normalized echo path misalignment measure, often termed system distance or coefﬁcient error norm,
according to
D =

n∥hn −ˆhn∥2

n∥hn∥2
,
(30.3)
can be very useful for the developer to look inside the system and to prove operation of the adaptive
echo path identiﬁcation algorithm, e.g., NLMS. Under speciﬁc circumstances, i.e., in particular with
white noise excitation x(n), the echo path misalignment and the echo return loss enhancement turn out
to be the inverse of each other [28]. This can be easily recognized by looking at d(n) and ˆd(n) as a
convolution of hn and ˆhn, respectively, with the same input signal x(n). However, for correlated input
signals, such as speech, or multichannel systems with coherent input signals on different input channels,
cf. Section 4.30.4, the equivalence of both measures is lost.
An open issue regarding the currently available instrumental and automatic test procedures is, how-
ever, that the correlation between predicted speech quality and perceived quality is not always guaran-
teed. In order to surpass this issue, ongoing developments take the following strategies into account to
increase the signiﬁcance of existing quality measures:
•
analysis of the acoustic echo control performance in the case of time-varying echo paths,
•
speciﬁcation of the required echo attenuation in the presence of noise,
•
and the measurement of speech quality parameters during the simultaneous presence of speech,
noise, and echo.
This brief survey on speech quality of hands-free telephones has shown that research in this difﬁcult
area cannot be considered as ﬁnalized. Due to its multi-dimensional nature, the objective prediction of
speech quality is probably yet more challenging than the speech enhancement problem itself. A more
comprehensive overview about the current state-of-the-art in the ﬁeld of advanced speech quality testing
for modern (hands-free) telecommunication systems can be found in [85]. A recent update and some
speciﬁc directions for future work are presented in [86,87].
4.30.1.5 Outline of this chapter
After this itemized introduction to history and mainstream technology in acoustic echo control, we shall
now proceed and deepen the understanding in formal terms. In the core sections of this chapter, we will
partly revisit the established technology and add more speciﬁc references, but mainly we will trace the
more recent trends due to research beyond 2000.
In Section 4.30.2, we ﬁrst pick up the single-channel linear AEC problem and present a fresh view in
termsofanuncertaintymodeloftheacousticenvironmentandasuitableadaptivealgorithmdevelopment
in a recursive Bayesian estimation style. In this uncertainty framework, particular objectives are given

4.30.2 Echo Cancellation and Postﬁltering
819
by the rigorous justiﬁcation of the adaptive echo cancellation and post-processing hybrid through joint
optimization, and by the uniﬁcation of adaptive ﬁlters and adaptive ﬁlter control in the form of statistical
Kalman ﬁltering techniques.
In Section 4.30.3, the uncertainty model about the echo path is understood harsh enough to drop the
echo cancellation concept and stick to echo suppression alone. This strategy is justiﬁed, for example,
when the uncertainty is due to data dropouts in the audio stream or clock desynchronization between
sound reproduction and acquisition. For the echo suppression, we then present a dedicated class of
algorithms which do not rely on complex echo path impulse identiﬁcation, but rather on the estimation
of a smoothed magnitude-squared echo power transfer function.
In Section 4.30.4, the treatment of acoustic echo cancellation (i.e., no echo suppression) is generalized
to the case with multiple reproduction channels. The additional problems of multichannel acoustic echo
cancellation are fundamentally different from those of traditional single-channel echo cancellation. Not
only the computational requirements are higher, but also the identiﬁcation of more unknowns is naturally
more difﬁcult, especially when echo path impulse responses with mutually correlated input signals have
to be tackled. As a ﬁeld of its own, we provide major references to multichannel echo cancellation and
speciﬁcally highlight the art of decorrelation of the inputs of the acoustic channels. For generality, the
treatment will be based on the TRINICON framework which includes inherent adaptation control as
mentioned above.
Unfortunately, the linear echo path model (single- or multichannel) does often not capture the real-
ity of today’s telecommunication devices which may include cheap audio hardware introducing non-
negligible nonlinear distortion into the signal played back by the loudspeaker. Informal listening tests
indicate that the human listener tolerates nonlinear distortion up to a level at which it can no longer be
modeled and cancelled by linear AEC with sufﬁcient accuracy. As a result, unacceptable nonlinearly
distorted acoustic echo would remain in the signal chain. Hence, in Section 4.30.5, we treat the extension
to nonlinear acoustic echo cancellation in the form of a survey of the current state-of-the-art in this
ﬁeld.
While these core sections describe the fundamental approaches in the respective ﬁelds, we ﬁnally
move on to a more application-oriented presentation of AEC technology and results in Section 4.30.6.
In this application corner, we are then dealing with different acoustic environments, such as in car
hands-free telephony, desktop teleconferencing, living room, and mobile phones. After this, we draw
our conclusions from this chapter.
4.30.2 Echo cancellation and postﬁltering
Echo cancellation might be considered as the ideal solution to acoustic echo control, since the acoustic
echo could be removed without harm to the desired near-end speech. However, it depends on the working
assumption that the echo path impulse response is determined with sufﬁcient accuracy. In this section,
we develop a generalized view which will explicitly take uncertainty about the acoustic echo path into
account. We shall see the consequences of the uncertainty model of the echo path on the optimum
signal processing for acoustic echo control, where the optimization of the system will explicitly target
the estimation of the desired near-end speech.

820
CHAPTER 30 Acoustic Echo Control
At ﬁrst we formulate the uncertainty model of the echo path by means of a multivariate random
variable with statistical mean and covariance. The covariance of the random variable basically describes
the uncertainty about the true echo path coefﬁcients. From here, we derive the linear minimum mean-
square error (MMSE) estimator for the near-end speech components in the microphone signal. The
resulting estimator consists of a subtractive echo canceler which duplicates the systematic part of the
echo path (i.e., the echo path expectation) and a statistical postﬁlter for residual echo suppression due
to the echo path uncertainty. This result proves, by the presence of uncertainty alone, the coexistence
of echo canceler and postﬁlter for otherwise linear echo path models and unlimited number of echo
canceler coefﬁcients. Echo cancellation with postﬁltering for residual echo suppression had previously
found its justiﬁcation merely by the presence of nonlinearities or undermodeled impulse response tails
of the echo path.
Then we develop a Bayesian adaptive algorithm for joint mean and covariance estimation of the echo
path. The derivation is based on an uncertainty model which represents the typical variability of the
echo path in the form of a stochastic Markov model. Since this time-varying echo path is observed in
the presence of independent near-end speech at the microphone, the Bayesian adaptive algorithm turns
out to be a Kalman ﬁlter.
Throughout this section, it remains a contrast to other literature that the echo path is modeled as a
random process, whereas the known echo path input is treated as a deterministic signal. Nevertheless,
the classical Wiener solution for subtractive echo cancellation is included as a special case.
4.30.2.1 Uncertainty model of the linear echo path
In this section, we prepare for the rigorous joint derivation of acoustic echo canceler and postﬁlter by
setting up the uncertainty model of the linear echo path. In the subsequent Section 4.30.2.2, we then
derive the MMSE optimum ﬁltering solution for the problem at hand. The core procedures in this and
the next section had been outlined in [27,88]. Here, we shall take the opportunity to translate these
procedures into uniﬁed vector notation in order to smoothly establish the relationship with vector-
oriented adaptive ﬁlter algorithms for echo path mean and covariance identiﬁcation in Section 4.30.2.5.
Getting back to the system in Figure 30.1, let us assume that the impulse response hn entirely
models the electroacoustic coupling between the loudspeaker signal x′(n) and the microphone signal
y(n). If sufﬁciently good transducers are used, the linear echo path is widely accepted as a realistic
model for the acoustic environment of hands-free systems. To provide transparent sound at least to the
near-end speaker, who typically is the owner of the system, we set the loudspeaker signal equal to the
received signal,1 i.e., x′(n) = x(n). The microphone signal y(n) comprising near-end speech s(n) and
1In an even more general approach, we may abolish the simpliﬁcation x′(n) = x(n) and consider some kind of signal
processing also in receiving direction of the system. In this case, the relation between the output x′(n) and the inputs x(n)
and y(n) has to be deﬁned in a similar way as for the output ˆs(n) later in Eq. (30.13). Furthermore, an appropriate distortion
measure then has to be deﬁned in receiving direction and has to be minimized simultaneously with the distortion in sending
direction. In this way, the distortion in sending direction might be reduced at the expense of a distortion in receiving direction.
For the sake of simplicity, we are not dealing with ﬁlters in receiving direction here, but it seems promising to treat this issue
in future work.

4.30.2 Echo Cancellation and Postﬁltering
821
convolutive echo d(n) then reads
y(n) = s(n) + d(n)
= s(n) + hn ∗x(n)
= s(n) +
N1

k=0
hkx(n −k)
= s(n) + hT x(n),
(30.4)
where the vector
x(n) =

x(n), x(n −1), . . . , x(n −N1)
T
(30.5)
denotes a collection of the most recent echo path input samples at discrete time n and
h =

h0, h1, . . . , hN1
T
(30.6)
is the ﬁnite set of the N1 corresponding impulse response coefﬁcients.
In the traditional theory of acoustic echo control, the speech signal s(n) and the received signal x(n)
are both modeled as independent random processes, while the echo path hn is treated as an unknown
deterministic parameter. For these model assumptions, the minimization of the mean-square output
error e(n) of the echo cancellation ﬁlter, cf. Section 4.30.1, leads to the well known Wiener solution,
i.e., the echo canceler ideally mimics the true echo path in order to compensate for the acoustic echo
in the microphone signal [2,10]. This solution will be absorbed as a special case of our uncertainty
framework as shown in Section 4.30.2.3.
In the uncertainty framework, to be considered from now, the speech signal s(n) is not observable
alone and is therefore still modeled as a stationary random process with zero mean and autocorrelation
matrix Rs = E{s(n)sT (n)} based on the length N2 vector
s(n) =

s(n), s(n −1), . . . , s(n −N2)
T .
(30.7)
However, the echo signal d(n) is now given as the linear convolution of a measurable (i.e., deterministic)
loudspeaker signal x(n) with unknown echo path coefﬁcients hn. Due to this uncertainty about the
acoustic echo path, the coefﬁcient vector h, too, is modeled as an independent random variable with
some statistical expectation ˆh and covariance p:
ˆh = E{h},
(30.8)
hr = h −ˆh,
(30.9)
p = E{hrhT
r }.
(30.10)
The mean ˆh then represents a systematic (i.e., deterministic) component of the uncertain echo path
h, whereas the residual hr is its truly unpredictable (i.e., zero-mean) component. The signal model as
presented here clearly ﬁts the practical applications of acoustic echo control in which the echo path is
usually unknown, but the echo path input is in fact known to the system. The implications of the echo
path uncertainty model will developed in the following sections in conjunction with the derivation of
optimum ﬁlters.

822
CHAPTER 30 Acoustic Echo Control
4.30.2.2 Generalized Wiener ﬁlter architecture
We initiate the actual derivation of the optimal AEC by a formal deﬁnition of the desired quality of
near-end speech reconstruction in the form of an objective function. On the one hand, full-duplex
operation of the hands-free system in Figure 30.1 ideally requires strong attenuation of the acoustic
echo signal d(n) by the acoustic echo controller. On the other hand, the echo attenuation is subject to the
undistorted reproduction of the desired signal s(n) at the system output. Mathematically, this conﬂict
can be expressed as a statistical optimization problem which aims, for example, at the minimum mean-
square error (MMSE) between s(n) and ˆs(n):
ϵ2 = E

s(n) −ˆs(n)
2	
→min.
(30.11)
In order to facilitate the computation of the system output ˆs(n), we formulate the echo control
problem as a general unconstrained linear ﬁltering problem, where the output signal ˆs(n) is obtained as
a linear combination of the available input signals x(n) and y(n):
ˆs(n) = w′
2,n ∗y(n) + w′
1,n∗x(n)
(30.12)
= w2,n ∗

y(n) −w1,n ∗x(n)

.
(30.13)
Mathematically, the ﬁlter structures in (30.12) and (30.13) are equivalent, since they can be uniquely
transformed into each other. In principle, we can either optimize the linear ﬁlters w′
1,n and w′
2,n, or
alternatively w1,n and w2,n. It turns out, however, that the solution for the ﬁlter structure in (30.13) is
somewhat simpler and more intuitive. To be in line with the common literature on acoustic echo control,
we will then refer to w1,n as the echo canceler and to w2,n as the postﬁlter for residual echo suppression.
The formulation as an unconstrained, possibly IIR ﬁltering problem emphasizes that the echo control
problem is not undermodeled by a strict limitation of the adaptive ﬁlter length here. That implies that
the echo canceler w1,n can entirely cover the span of the linear echo path hn. Since w1,n = hn would
clearly eliminate the echo, it will be interesting to clarify the role of the postﬁlter w2,n in this seemingly
simple conﬁguration.
The ﬁlter structure in (30.13) requires the implementation of two consecutive convolutions, i.e.,
the convolution of x(n) with w1,n, and the subsequent convolution of the difference e(n) = y(n) −
w1,n ∗x(n) with the ﬁlter w2,n. In vector notation, that means that we have to provide a vector
e(n) =

e(n), e(n −1), . . . , e(n −N2)
T
(30.14)
ofinputsamplesforthesecondconvolutionandthusavectorofoutputsamplesfromtheﬁrstconvolution.
That in turn requires the deﬁnition of an input signal matrix
X(n) =

x(n), x(n −1), . . . , x(n −N2)
T ,
(30.15)
which then allows us to write the AEC output according to (30.13) as
ˆs(n) = wT
2 e(n)
= wT
2 [y(n) −X(n)w1]
(30.16)
= [yT (n) −wT
1 XT (n)]w2,
(30.17)

4.30.2 Echo Cancellation and Postﬁltering
823
where
y(n) =

y(n), y(n −1), . . . , y(n −N2)
T
(30.18)
is a vector of most recent microphone samples and
w1 =

w1,0, w1,1, . . ., w1,N1
T
(30.19)
w2 =

w2,0, w2,1, . . ., w2,N2
T
(30.20)
are the coefﬁcient vectors of length N1 and length N2 of echo canceler and postﬁlter, respectively. The
two forms in (30.16) and (30.17) are equivalent.
We further exploit the available vector-matrix notation to write the microphone signal vector y(n) as
y(n) = s(n) + X(n)h
(30.21)
in accordance with the original convolutive signal model in (30.4).
We then proceed by substituting the two-stage linear ﬁlter structure (30.13) into (30.11) and by
computing the partial derivatives of the mean-square error ϵ2 with respect to the coefﬁcients w1 and
w2. The following expressions for the derivatives are obtained using not more than the previously made
assumptions of a deterministic input signal X(n) and independent near-end speech s(n) with zero mean:
∂ϵ2
∂w1
= −2E


s(n) −ˆs(n)
 ∂ˆs(n)
∂w1

= 2E

ˆs(n)

XT (n)w2
= 2

E

yT (n)
	
w2 −wT
1 XT (n)w2

XT (n)w2,
(30.22)
∂ϵ2
∂w2
= −2E


s(n) −ˆs(n)
 ∂ˆs(n)
∂w2

= −2E

s(n) −ˆs(n)
 
y(n) −X(n)w1

= −2E

ˆs(n)

X(n)w1 −2E

y(n)

s(n) −ˆs(n)

.
(30.23)
Here, the reason for the remaining time index n after the evaluation of the statistical expectation lies in
the deterministic nature of X(n).
In the next step of our derivation, we make use of the linear signal model in (30.21) and ﬁnd that
E

yT (n)

= E

hT 
XT (n). Now it can be easily seen that ∂ϵ2/∂w1 = 0 by choosing the optimum
echo canceler coefﬁcients as
w1 = E{h} = ˆh.
(30.24)
In order to ﬁnd the postﬁlter w2 that satisﬁes ∂ϵ2/∂w2 = 0, we consider the last line of Eq. (30.23)
and initially note that E

ˆs(n)

again vanishes due to the choice w1 = ˆh for the echo canceler, as before
in (30.22). The second part of (30.23) can be expanded by inserting the system output ˆs(n) according
to (30.17) and the signal model for y(n) as shown in (30.21):
E

y(n)

s(n) −ˆs(n)

= E

y(n)

s(n) −

yT (n) −wT
1 XT (n)

w2
	
= E

s(n) + X(n)h
 
s(n) −

sT (n) + hT XT (n) −wT
1 XT (n)

w2
	
.

824
CHAPTER 30 Acoustic Echo Control
By then utilizing the uncertainty model h = ˆh + hr of the echo path as shown in (30.9), and invoking
the optimum solution w1 = ˆh in (30.24), and ﬁnally the independence of hr and s(n), we arrive at the
following equation (consisting of deterministic and statistical terms) for the optimum ﬁlter w2:
E

y(n)

s(n) −ˆs(n)

= E

s(n) + X(n)(ˆh + hr)
 
s(n) −

sT (n) + hT
r XT (n)

w2
	
= E {s(n)s(n)} −E

s(n)sT (n)w2
	
−E

X(n)hrhT
r XT (n)w2
	
= rs −Rsw2 −X(n)pXT (n)w2 = 0,
(30.25)
where rs = E {s(n)s(n)}is the autocorrelation vector of the near-end speech being equal to the ﬁrst
column of Rs. From here, we can easily solve for the optimum postﬁlter w2 by rearranging the vector-
matrix equation to
w2 =

Rs + X(n)pXT (n)
−1
rs.
(30.26)
We note that the result for w2 has the structure of a Wiener ﬁlter to perform noise reduction on the
signal
e(n) = y(n) −ˆhT x(n) = s(n) + hT
r x(n),
(30.27)
with hT
r x(n) being the effective noise (here: The residual echo) and s(n) the desired signal. In this
interpretation, the compound quantity X(n)pXT (n) in (30.26) can be considered as the noise autocor-
relation matrix. It is in fact obtained as a weighted short-term autocorrelation X(n)XT (n) of the input
signal x(n), where the weighting matrix p serves as a statistical descriptor of the uncertain residual
echo transmission system hr = h −ˆh. Commutativity of signals and systems further allows for the
equivalent and yet more intuitive understanding of the random quantity hr as the input of a deterministic
transmission system described by the known vector x(n). The latter interpretation right away explains
the covariance X(n)pXT (n) of the output of the convolution hT
r x(n) = xT (n)hr.
In our derivation, echo canceler and postﬁlter have been deduced jointly from the MMSE criterion
and, therefore, we will refer to the combination of (30.24) and (30.26) as the generalized Wiener solution
for acoustic echo control. The optimization was based on a signal model which consists of an uncertain
linear echo path with a deterministic input signal. We assume that this signal model greatly ﬁts the
practical applications of acoustic echo control in which the echo path is usually unknown and the echo
path input is in fact measurable. A block diagram of the resulting optimal ﬁlter structure immediately
follows from Eq. (30.13). It is depicted in Figure 30.3a.
Regarding the practical implementation of the algorithms, however, we have to mention that the
assumption of stationarity of the near-end speech s(n) is of course not realistic. As usual in speech and
audio processing, the optimum ﬁlters thus have to be updated at a time-constant of 10–30 ms on the basis
of short-term stationary signal frames. Moreover, the calculation of the postﬁlter in (30.26) requires
efﬁcient algorithms for matrix inversion, such as the generalized Levinson algorithm [89], or we can
alternatively approach efﬁcient solutions in the frequency-domain [27]. The digital ﬁltering as such,
according to (30.16), can be realized by fast convolution in the DFT domain or by direct convolution
in the time-domain [90].

4.30.2 Echo Cancellation and Postﬁltering
825
−
w1,n
w2,n
hn
s(n)
d(n)
s(n)
d(n)
y(n)
e(n)
x(n)
To far Speaker
From far Speaker
Echo
Echo
Path
Canceler
Postﬁlter
(a) Generalized Wiener solution for the uncertain linear echo path model
with non-zero mean E{h} and non-zero covariance p.
−
w1,n
w2,n
hn
s(n)
d(n)
s(n)
d(n)
y(n)
e(n)
x(n)
To far Speaker
From far Speaker
Echo
Echo
Path
Canceler
Postﬁlter
(b) Classical Wiener solution for the deterministic echo path model with-
out uncertainty, i.e., E{h} = h and p = 0.
−
w1,n
w2,n
hn
s(n)
d(n)
s(n)
d(n)
y(n)
e(n)
x(n)
To far Speaker
From far Speaker
Echo
Echo
Path
Canceler
Postﬁlter
(c) Wiener solution for an unpredictable echo path with zero-mean, i.e.,
E{h} = 0 and p = Rh = E{hhT }.
FIGURE 30.3
The generalized Wiener solution and important special cases.

826
CHAPTER 30 Acoustic Echo Control
4.30.2.3 General and special cases
Before moving on to adaptive algorithms dedicated for the generalized Wiener ﬁlter, we shall clarify its
signiﬁcance in comparison with two special cases of it. We will demonstrate that the general solution
and its better known special cases correspond to different ﬁlter structures, mainly differing in the way
they utilize a priori information in form of the mean and the covariance of the acoustic echo path,
depending on the application at hand.
4.30.2.3.1
General statistical case
For the convenience of the reader, we ﬁrst repeat the two optimum ﬁlters:
w1 = E{h}
(30.28)
w2 =

Rs + X(n)pXT (n)
−1
rs.
(30.29)
It can be seen that the generalized Wiener ﬁlter fully takes the statistical properties of the echo path
h into account. More speciﬁcally, the mean and covariance of the echo path are utilized separately
to determine the optimum echo canceler and postﬁlter coefﬁcients, respectively. The optimum echo
canceler w1 = E{h} = ˆh creates a replica of the echo components which are due to the systematic
component of the echo path, i.e., ˆd(n) = ˆhT x(n). The echo subtraction according to Figure 30.3a
therefore results in the error signal e(n) = y(n) −ˆd(n) = s(n) + hT
r x(n), comprising the desired
signal s(n) plus undesired residual echo hT
r x(n). The signal e(n) is then postﬁltered with coefﬁcients
according to (30.29), thereby taking the echo path covariance p in conjunction with the input signal
X(n) into account.
It turns out in practice that this general two-ﬁlter solution is very much suitable to achieve acous-
tic echo control in hands-free communication systems, such as in car hands-free telephones to be
described in Section 4.30.6.1. In many other applications, too, it is indeed feasible to determine a
(possibly time-varying) systematic component ˆh of the echo path h using the acoustic system iden-
tiﬁcation approach, e.g., [2,35]. Nevertheless, some degree of uncertainty p about the echo path
always remains and, therefore, the postﬁlter is an indispensable component of advanced hands-free
telephones.
4.30.2.3.2
Deterministic case
In special cases without uncertainty about the echo path at all, i.e., if E{h} = h and thus p = 0, our
general solution degenerates to
w1 = h
(30.30)
w2 = u,
(30.31)
where u = [1, 0, 0, . . . , 0]T denotes the unit vector of length N2. Here, the optimum echo canceler w1 is
anidealcopyofthetrueechopathh andtheperfectlyecho-cancelederrorsignale(n) = y(n)−hT x(n) =
s(n) will naturally pass the postﬁlter w2 unprocessed. This procedure is commonly understood as the
Wiener solution for acoustic echo cancellation [2,10] and it is indeed optimal for a deterministic echo
path h. The ﬁlter structure corresponding to this special case is illustrated in Figure 30.3b.

4.30.2 Echo Cancellation and Postﬁltering
827
The purely deterministic echo path model assumed here is often not the best choice in real systems.
It may require extremely sophisticated adaptive ﬁlters and control mechanisms to let the echo canceler
coefﬁcients follow a true time-varying echo path with sufﬁcient accuracy. It is obvious that especially in
noisy and time-varying acoustic environments, as for example in vehicles, the true echo path cannot be
determined exactly at all times. As a consequence, the deterministic strategy may not deliver sufﬁcient
echo attenuation, i.e., residual echo can appear at the system output. The situation can be different
in applications of network or line echo cancellation. Here, the echo path coefﬁcients can often be
measured with sufﬁcient accuracy during call setup and they are not expected to change signiﬁcantly
during conversation. A postﬁlter for residual echo suppression is then not needed.
4.30.2.3.3
Zero-mean case
When no systematic information is available about the echo path at all, i.e., if E{h} = 0 and thus
p = Rh = E{hhT }, we obtain another special case of the generalized Wiener solution:
w1 = 0,
(30.32)
w2 =

Rs + X(n)RhXT (n)
−1
rs.
(30.33)
Obviously, the generalized Wiener ﬁlter degenerates to an MMSE equalizer in sending direction
of the communication system, i.e., the responsibility for acoustic echo suppression is entirely with
the statistical postﬁlter. The corresponding block diagram in Figure 30.3c clearly reminds ourselves
to the methodology of background noise suppression. The difference, however, is that the known far-
end speech X(n) is taken into account to calculate the noise PSD in conjunction with the echo path
covariance Rh.
The practical relevance of the zero-mean case is given by applications in which it turns out difﬁcult
to determine a systematic component of the echo path at all. This situation can appear in extremely
time-varying and noisy systems, e.g., teleconferencing equipment in reverberant environments or hands-
free accessories with unstable microphone and loudspeaker placement. Mobile devices may simply not
provide the computational resources for a sophisticated echo path estimator. At the same time, it should
be noted that the desired speech at the system output can be distorted and the background noise of the
near-end can be modulated by the presence of the far-end speech signal. Section 4.30.3 is dedicated to
the zero-mean case and its implications.
4.30.2.4 Dynamical echo path modeling
In almost all practical applications of acoustic echo control, such as car hands-free systems, telecon-
ferencing equipment, mobile phones, and speech dialog systems, we face a time-varying acoustic echo
path h(n). Depending on the application, the degree of change can be more or less pronounced. In
reality, we further have to expect sometimes quite abrupt changes and sometimes almost static behavior
of the echo path. On average, the acoustic echo path is certainly not standing still, but also not changing
arbitrarily fast, i.e., the dynamical process is governed by ﬁnite bandwidth.
In order to exploit the average smooth dynamical nature of the echo path in adaptive algorithm
development, we shall now reﬁne our previous uncertainty model of the echo path in (30.8)–(30.10) .

828
CHAPTER 30 Acoustic Echo Control
In this respect, a particularly convenient stochastic model for time-varying systems h(n) is the ﬁrst-order
recursive Markov chain [10], i.e.,
h(n + 1) = a · h(n) + h(n),
(30.34)
where two consecutive realizations at times n and n + 1 are related to each other by the transition
coefﬁcient 0 ≤a ≤1 and the independent process noise quantity h(n) with zero mean and covariance
matrix R = E{h(n)hT (n)}. The Markov model therefore represents dynamic behavior in which
the state h(n) gradually changes into an unpredictable direction—very much in agreement with the
nature of time-varying impulse responses in realistic acoustic environments.
Clearly,theMarkovmodelwillserveonlyasasimpliﬁedmodeloftherealworldsituation.However,it
brings along two major relationships with real time-varying systems by, ﬁrstly, restricting the bandwidth
of change according to the transition factor “a” and, secondly, providing an element of uncertainty
throughtheprocess noise h(n). Inorder todescribedifferent degrees of variability, wemight intuitively
adjust the transition factor or the process noise covariance of the model. However, to be sure about the
consequences, we shall more formally consider some properties of the Markov model in conjunction
with the acoustic echo control purpose:
•
With the process noise h(n) assumed to be zero-mean and stationary, where the latter is reﬂected
explicitly by the time-invariant process noise covariance R, and by considering the linear time-
invariant system in (30.34), the echo path h(n) is immediately recognized as a zero-mean and
stationary random process, too, and described by the time-invariant covariance Rh = E{h(n)hT (n)}.
Such properties of h(n) are well in agreement with the acoustic echo control application. Here, the
zero mean, E{h(n)} = 0, in fact represents the average over all possible echo paths when no suitable
a priori information is available about the electroacoustic environment—including gain and phase
of loudspeaker and microphone ampliﬁers, the exact physical distance between loudspeaker and
microphone, and the room characteristics. The time-invariance of the echo path covariance Rh
further expresses the persistence of the acoustic echo path impulse response h(n), independent of
the highly nonstationary echo path input x(n).
•
By applying square expectation on both sides of (30.34), and by exploiting stationarity of h(n), i.e.,
Rh = E{h(n + 1)hT (n + 1)} = E{h(n)hT (n)}, and utilizing the independence of h(n), we can
evaluate the echo path covariance as follows:
Rh = a2Rh + R.
(30.35)
This result can be rearranged to obtain an interesting proportionality between the covariances of
echo path changes and echo path:
R = (1 −a2)Rh.
(30.36)
From this relationship, we learn that we cannot choose the transition factor “a” and the process noise
covariance R of the Markov model in (30.34) independently, since the echo path covariance Rh is
typically given as a somewhat ﬁxed and persistent quantity, despite the possibly changing acoustic
impulse response h(n). Moreover, the relation in (30.36) can even be very useful to determine an
unknown covariance of the echo path changes, R, from an estimated or a priori known echo path
covariance Rh.

4.30.2 Echo Cancellation and Postﬁltering
829
Finally, the state Eq. (30.34) and the linear observation model (30.4) can be formally combined
into a general stochastic state-space model of the unknown echo path state h(n). For convenience and
to include the time-varying echo path into the observation equation, both models are repeated here
together:
h(n + 1) = a · h(n) + h(n),
(30.37)
y(n) = s(n) + xT (n)h(n).
(30.38)
In summary, the echo path state equation is governed by the independent process noise h(n) with
covariance R. The resulting state h(n) is then observed through the microphone signal y(n) in the
presence of near-end speech s(n) which acts as independent observation noise with covariance σ 2
s . This
statement of the AEC problem will now lead us to the utilization of powerful state estimators from
control theory to deduce contained and efﬁcient adaptive algorithms for acoustic echo control.
4.30.2.5 Adaptive algorithms
The previous section has argued for modeling the a priori echo path mean E{h(n)} = ˆh as zero. The
optimum echo canceler in (30.28) would thus degenerate to w1(n) = ˆh = 0, practically meaning that
the generalized Wiener solution turns into the MMSE equalizer in (30.33). In order to exploit the full-
featured Wiener solution, we have to resolve at least partly the uncertainty about the echo path h(n).
This can be done by recasting the uncertainty framework in a way that incorporates the observations
y(n) into the stochastic echo path model. Mathematically, this can be accomplished by deﬁning the
conditional mean and covariance of the echo path at time n, given the observations y(n) up to and
including time n −1:2
ˆh(n) = E{h(n)|y(n −1), y(n −2), . . . , y(0)},
(30.39)
hr(n) = h(n) −ˆh(n),
(30.40)
p(n) = E{hr(n)hT
r (n)}.
(30.41)
This data driven uncertainty model of the echo path basically replaces the a priori echo path model
in (30.8)–(30.10). In place of the a priori mean ˆh, we now have the time-varying conditional mean ˆh(n).
The former residual hr in (30.9) accordingly has been replaced by its time-varying counterpart hr(n),
representing the misalignment between the now conditional echo path mean and the true echo path h(n).
Moreover, the former a priori echo path covariance p has turned into the now time-varying echo path
covariance p(n) based on the conditional mean. Structurally, the deﬁnitions (30.39)–(30.41) are fully
consistent with the previous ones in (30.8)–(30.10). Therefore, the conditional mean ˆh(n) can serve
as an optimum ﬁlter for subtractive echo cancellation in (30.28) along with the conditional covariance
p(n) in place of p in the postﬁlter Eq. (30.29).
2In the application of acoustic echo control, the most recent data y(n) at time n is usually not included into the estimation
of the acoustic echo path h(n) at time n, e.g., consider the LMS, APA, and RLS family of adaptive algorithms [2]. This has
the practical advantage that the estimated echo path is completely determined already at time n −1 and can be employed for
echo cancellation immediately when the input data y(n) is available. In the language of state-space modeling and estimation,
the conditional mean ˆh(n) in (30.40) is called the a priori estimate of the state h(n). An a posteriori estimate ˆh+(n) which
is a reﬁnement of ˆh(n) based on the current data y(n) could be deﬁned as well.

830
CHAPTER 30 Acoustic Echo Control
The computation of the conditional expectation in (30.39) and the corresponding covariance in
(30.41), subject to the state-space model in (30.37) and (30.38), is a well understood problem. The
adequate mathematical instrument for the solution is the statistical Kalman ﬁlter. In literature, we ﬁnd
several principal interpretations of it. In [10], the Kalman ﬁlter is derived as the linear MMSE estimator
of the state of a linear dynamical system. In [91], the Kalman ﬁlter is developed as the MMSE state
estimator under the assumption of Gaussianity of process noise and observation noise. A very intuitive
presentation of the Kalman ﬁlter equations can be found for example in [92]. The original work of
Kalman is documented in [93]. Independent of the particular interpretation, the Kalman ﬁlter delivers
at least a good approximation of the conditional mean ˆh(n) and the corresponding estimation error
covariance p(n) with respect to the unknown parameter vector h(n). The algorithm consists of the
following set of recursive and iteratively coupled matrix equations:
ˆh(n + 1) = a · ˆh+(n),
(30.42)
p(n + 1) = a2 · p+(n) + R,
(30.43)
ˆh+(n) = ˆh(n) + k(n)(y(n) −xT (n)ˆh(n)),
(30.44)
p+(n) = (I −k(n)xT (n))p(n),
(30.45)
k(n) = p(n)x(n)

xT (n)p(n)x(n) + σ 2
s (n)
−1
.
(30.46)
Equations (30.42) and (30.44) recursively determine the conditional mean ˆh(n) in a prediction-
correction style. In doing so, the formulas utilize the Kalman gain k(n) from (30.46) as a weight vector
which essentially depends on the state error covariance p(n). The latter is again determined recursively
through Eqs. (30.43) and (30.45) of the Kalman ﬁlter.
The Kalman gain k(n) can be considered as an intelligent adaptive stepsize parameter in the recursive
learning procedure for the conditional echo path mean ˆh(n), i.e., k(n) basically upgrades the role
of the ﬁxed stepsize μ in the LMS algorithm [2,10]. Through the Kalman gain, the model-based
“system distance” p(n) between the true and the estimated acoustic system interacts with the prediction-
correction procedure for the estimation of ˆh(n). In this way, Kalman ﬁltering can be understood as the
ever sought uniﬁcation of linear adaptive ﬁltering and adaptation control. After all, the Kalman ﬁlter
differs from LMS and RLS by its inherent stability [10], i.e., it does not require additional control
mechanisms (e.g., the double-talk detection) in order to achieve fast and yet robust adaptation in time-
varying and noisy acoustic environments.
So far, the Kalman ﬁlter has been employed for acoustic system identiﬁcation hardly ever. This
can be attributed to its high computational load and to the risk for numerical instability in the case of
higher-order adaptive ﬁlters [10]. Furthermore, a comprehensive signal model for the Kalman ﬁlter,
particularly the availability of observation and process noise covariances for the acoustic state-space
model in (30.38) and (30.37), seemed to be out of sight [2].
In order to tame the exact Kalman ﬁlter, we brieﬂy outline the procedure as described in [94]. At
ﬁrst, we replace the matrix quantity k(n)xT (n) in (30.45) with the inner vector product xT (n)k(n)/N1.
This seemingly brutal simpliﬁcation can be well justiﬁed in the case of broadband input x(n), since
the recursively smoothed matrix quantity k(n)xT (n) resembles the near-diagonal correlation matrix of
x(n). Provided that we specify a diagonal process noise covariance R = σ 2
I, the state error covariance
matrix p(n) then can be treated as a scalar p(n) without further assumption or approximation, as seen

4.30.2 Echo Cancellation and Postﬁltering
831
from (30.43) and (30.45). The normalization by factor N1 in the former replacement xT (n)k(n)/N1
achieves appropriate scaling after the substitution. Due to the broadband rationale behind these rear-
rangements, the resulting algorithm is termed broadband Kalman ﬁlter:
ˆh(n + 1) = a · ˆh+(n),
(30.47)
p(n + 1) = a2 · p+(n) + σ 2
,
(30.48)
e(n) = y(n) −xT (n)ˆh(n),
(30.49)
ˆh+(n) = ˆh(n) + k(n)e(n),
(30.50)
p+(n) =

1 −xT (n)k(n)/N1

p(n),
(30.51)
k(n) = p(n)x(n)

p(n)xT (n)x(n) + σ 2
s (n)
−1
.
(30.52)
By the simpliﬁcations introduced here, naturally, the presented algorithm loses its decorrelation
ability regarding the input signal x(n) if non-white input is processed. However, all the structural
support to handle the estimation of time-varying unknown systems h(n) in the continuous presence
of observation noise s(n), with possibly time-varying level σ 2
s (n), is fully preserved in the broadband
Kalman ﬁlter. Moreover, we have at the same time gained considerable numerical efﬁciency by reducing
the dimension of the original state error covariance p(n) from matrix to scalar.
Next, we mention an opportunity to resolve a possible uncertainty regarding the time-varying obser-
vation noise power σ 2
s (n) in the Kalman gain (30.52), because this quantity is indispensable for the
operation of the Kalman ﬁlter. Unfortunately, the corresponding signal s(n) is not available explicitly
for the calculation of sample covariances, but the error signal e(n) in (30.49) represents at least a good
estimate of the observation noise signal s(n) in case of successful state estimation. Thus, we can approx-
imate σ 2
s (n) ≈σ 2
e (n) and then obtain the error signal power σ 2
e (n), e.g., by recursive averaging of the
explicitly available square error e2(n).
The scalar process noise covariance parameter σ 2
 required in (30.48) can be speciﬁed as
σ 2
 = (1 −a2)E{hT (n)h(n)}/N1, where E{hT (n)h(n)} denotes an expectation of the echo path norm.
This formula is in line with (30.36) by again reducing the process noise covariance from matrix to scalar
dimension as done already in the derivation of the broadband Kalman ﬁlter, R = Iσ 2
, and similarly
Rh = Iσ 2
h = IE{hT (n)h(n)}/N1. The transition parameter “a” has to be determined appropriately for
the application at hand.
Substituting (30.50) and (30.52) into (30.47), while assuming low near-end speech and near-end
noise, i.e., σ 2
s (n) →0, the broadband Kalman ﬁlter reveals structural equivalence with the NLMS
algorithm [10], except for the leaky factor a which appears in the update equation (instead of a stepsize
factor as for the pure NLMS algorithm):
ˆh(n + 1) = a ·

ˆh(n) + k(n)e(n)

,
(30.53)
k(n) =

xT (n)x(n)
−1
x(n),
(30.54)
e(n) = y(n) −xT (n)ˆh(n).
(30.55)

832
CHAPTER 30 Acoustic Echo Control
The resulting NLMS algorithm proves the numerical efﬁciency and robustness obtained through the
simpliﬁcation of the exact Kalman ﬁlter. The broadband Kalman ﬁlter, still with σ 2
s (n) ̸= 0, then in fact
represents an excellent compromise in terms of adaptive performance and structural simplicity between
exact Kalman ﬁlters and very popular LMS-type adaptive algorithms for acoustic system identiﬁcation.
In contrast to our top-down justiﬁcation of the broadband Kalman ﬁlter, a model-based bottom-up
generalization of the NLMS algorithm leads to a similar algorithm [95,96].
Resuming to more general considerations, we ﬁnally want to mention that and alternative and pre-
ferred realization of adaptive algorithms for joint conditional mean and covariance estimation has been
presented in the literature. In [27], a block frequency-domain adaptive Kalman ﬁlter and the underlying
state-space model in the DFT domain have been suggested. It was demonstrated that the aforemen-
tioned drawbacks of the exact Kalman ﬁlter in time-domain can be circumvented very efﬁciently by
diagonalization—through minor approximation—of the matrix algebra in the DFT domain. Intuitive
approaches for process and observation noise covariance learning were suggested, similar to the ones
presented right above, and the impact of model mismatch between the underlying states-space model of
the Kalman ﬁlter and real world dynamics has been reported to be “graceful.” In [97], the DFT-domain
adaptive algorithm was then reformulated as a state-space frequency-domain adaptive ﬁlter (SSFDAF)
in the context of acoustic echo control. In [98], the intuitive way of covariance learning was conﬁrmed
in the maximum-likelihood sense and the superiority of state-space frequency-domain adaptive ﬁltering
in comparison to traditional frequency-domain adaptive ﬁltering was demonstrated.
The block frequency-domain adaptive Kalman ﬁlter is not limited to, but especially suits the adap-
tation of a frequency-domain representation of the generalized Wiener ﬁlter as shown in [27]. This
concept and implementation will be used as the basis for the presentation of numerical results in the
application of car hands-free systems in Section 4.30.6.1.
4.30.3 Echo suppression
An alternative way to prevent acoustic echoes is the use of an acoustic echo suppressor (AES) [99]
providing echo free half-duplex communication. If echo suppression is carried out independently at
each frequency of a short-time spectral domain, a good degree of duplexity can be achieved. Recently,
AES approaches have been introduced [100,101] that are similar to the residual echo postﬁltering as
presented in Section 4.30.2, while completely discarding the acoustic echo canceler (AEC) part. These
approaches do not require the identiﬁcation of the room impulse response as an FIR ﬁlter, but model
parametrically the echo path with a delay and a single real-valued gain at each frequency of short-time
spectra, resulting in lower computational complexity than when using a precise echo path estimation.
Conventional cancellation methods to cope with acoustic echoes have been successfully imple-
mented, see Section 4.30.1. In practice, however, the achievable echo attenuation for these conventional
approaches is not sufﬁcient due to, e.g., the echo tail effect (modeling of too short a portion of the echo
path), nonlinear echo components caused by vibration effects or the nonlinear behavior of low-cost
audio hardware, and convergence problems in case of highly time varying echo paths [102]. Therefore,
AEC are usually combined with a suppression scheme to remove residual echoes which the AEC lets
through [35]. Commonly, the suppression of residual echoes is performed in a frequency selective way
[27,35,58,62,64]. Indeed, virtually all acoustic echo cancellation systems use such a postﬁlter because
they fail too often to reduce the echo to become sufﬁciently inaudible.

4.30.3 Echo Suppression
833
Such a suppressor greatly improves the echo attenuation, but contrary to the linear echo canceler
alone, the resulting audio quality and double talk performance often suffer from it: Distortions on the
desired signal are more likely to appear due to spectral suppression with a requirement to be aggressive
enough to suppress all residual echoes [103]. Consequently, when high echo attenuation is required,
the linear echo canceler can only cover a small fraction of the needed attenuation compared to the
echo suppressor, and thus, the AEC is made unnecessary when the audio quality is bounded to what
the suppressor yields. The beneﬁt of purely subtractive operation by the linear echo canceler is thus
restricted and an AES alone can be advantageously implemented in this case.
Now, we ﬁrst review the general approach of AES as introduced in [101], before we present a
complete implementation of an improved AES.
4.30.3.1 Alternative problem statement
Recall from Section 4.30.2.1 that the microphone signal y(n) is composed of the near-end signal s(n)
and the acoustic echo signal that results from the feedback of the loudspeaker signal x(n), i.e.,
y(n) = hn ∗x(n) + s(n),
(30.56)
where hn is the room impulse response and ∗denotes convolution, as in (30.4). Generally, a room
impulse response hn can be decomposed into a direct sound, early reﬂections, and late reverbera-
tion. Here, only a global delay parameter τ and a ﬁlter gn are used to model parametrically the echo
path in order to capture direct sound and early reﬂections. The microphone signal y(n) can thus be
approximated by:
y(n) = gn ∗x(n −τ) + s(n).
(30.57)
As illustrated in Figure30.4, short-time discrete Fourier transform (STFT) spectra are computed from
the loudspeaker and microphone signals. The STFT-domain representation of (30.57) is then given by
Y(m, ν) = G(m, ν)Xτ(m, ν) + S(m, ν),
(30.58)
where m is the block time index and ν denotes the frequency index. Xτ(m, ν) is the STFT-domain
correspondence of the loudspeaker signal x(n) delayed by τ samples (30.57). An estimate of the echo
power spectrum can be obtained by applying an estimated delay τ and an estimate of the magnitude-
squared ﬁlter |G(m, ν)|2 to the loudspeaker signal power spectrum, i.e.,
Y(m, ν)
2 =

G(m, ν)
2 |Xτ(m, ν)|2.
(30.59)
Since in practice the echo transfer function |G(m, ν)| is not known a priori, it has to be replaced by the
estimate |
G(m, ν)|, corresponding to a real-valued gain at each frequency. In this model, late reﬂections
are not estimated explicitly, but they are later considered by speciﬁc time-smoothing applied to the
echo power spectrum estimate |Y(m, ν)|2. Then, the actual acoustic echo suppression, derived from the
echo estimate (30.59), is performed by modifying the magnitude of the STFT of the microphone signal
Y(m, ν), while keeping its phase unchanged. This can be expressed by
S(m, ν) = F(m, ν)Y(m, ν),
(30.60)
where F(m, ν) represents a real valued, positive gain factor in each bin.

834
CHAPTER 30 Acoustic Echo Control
.
.
n
n
.
ˆG(m, ν )
y(n)
x(n)
X τ (m, ν )
Y (m, ν )
F (m, ν )
ˆs(n)
STFT
STFT
EEF
ESF
SM
ISTFT
τ
FIGURE 30.4
Basic block diagram of a short-time spectral domain acoustic echo suppressor. STFT, ISTFT, EEF, ESF, and
SM stand for short-time Fourier transform, its inverse, echo estimation ﬁlter, echo suppression ﬁlter and
spectral modiﬁcation, respectively.
In the following, the echo path estimate in (30.59) is denoted as the echo estimation ﬁlter (EEF).
Since the EEF is only a real-valued magnitude ﬁlter, it ought to be possible to estimate it without phase
sensitivity. In [104] a technique was proposed to estimate the EEF based on power spectral ﬂuctuations
making the EEF insensitive to the phase relation between loudspeaker and microphone signals, whereas
conventional echo path and EEF estimation processes are known to fail in such scenarios [105]. The
suppression of echoes (30.60) is then implemented analogously to a Wiener ﬁlter [10], referred to as the
echo suppression ﬁlter (ESF), based on a short-time power spectrum estimate of the echo. A weakness
of the described AES systems is that, since a short-time ESF is applied on the microphone signal, the
suppression of a low SNR echo signal tends to introduce artifacts, such as so-called “musical noise”
artifacts [106]. To mitigate this problem, smoothing is applied to the echo estimate (30.59) and to the
ﬁnal gain ﬁlter (30.60).
From the above discussion we conclude that there are two important tasks included in acoustic
echo suppression: On one hand, a suitable EEF has to be estimated in order to obtain a good estimate
of the spectral components of the echo signal included in the microphone signal. On the other hand,
an appropriate computation rule for the ESF is required that maximizes the echo suppression while
keeping the distortions on the desired near-end signal as low as possible. Solutions to these two tasks
are presented in the next two sections. The estimation of the EEF is thoroughly discussed next. Since
the associated delay τ is also not known in advance, it also has to be estimated. We consider only one
single delay for all frequencies. The estimation of the delay is straightforward when using correlation

4.30.3 Echo Suppression
835
methods, e.g. [107], and is not discussed further here. Then, the derivation of the ﬁnal ESF is described
later.
4.30.3.2 Echo path estimation
The computation of the echo estimation ﬁlter (EEF) is a crucial part of acoustic echo suppression. The
estimation of the echo power spectrum |Y(m, ν)|2 is achieved based on the observable loudspeaker
signal and an EEF (30.59), i.e., an estimate of the echo transfer function G(m, ν).
As proposed in [101], a straightforward solution for estimating G(m, ν) results from the signal model
(30.58). Assuming that the near-end speaker is silent, (30.58) implies that the EEF can be estimated as
the magnitude of the least squares estimator (Wiener ﬁlter),

Gw(m, ν) =

E

Y(m, ν)X∗
τ(m, ν)

E

Xτ(m, ν)X∗τ(m, ν)

 ,
(30.61)
where ∗denotes the complex conjugate operator. Since the acoustic echo path is likely to vary in time,

Gw(m, ν) is estimated iteratively by

Gw(m, ν) =

Y X∗τ (m, ν)
Xτ X∗τ (m, ν)
 ,
(30.62)
where
Y X∗τ (m, ν) = αY(m, ν)X∗
τ(m, ν) + (1 −α)Y X∗τ (m −1, ν),
(30.63)
Xτ X∗τ (m, ν) = αXτ(m, ν)X∗
τ(m, ν) + (1 −α)Xτ X∗τ (m −1, ν),
(30.64)
and α ∈[0, 1] is determined as a function of the desired smoothing time constant T,
α = 1 −exp

−K
T fs

,
(30.65)
where fs is the sampling frequency and K the STFT window hop size, T = 1.5 s is a reasonable value.
The above technique effectively estimates the echo path transfer function and takes the magnitude
thereof to obtain the real-valued EEF. Whenever the phase changes abruptly, such as during echo
path changes, this EEF estimation has to re-converge. To make (30.61) insensitive to phase variations,
correlations are modiﬁed to be computed from the power spectra rather than from the complex spectra,
i.e.,

G2
b(m, ν) = E

|Xτ(m, ν)|2|Y(m, ν)|2
E

|Xτ(m, ν)|2|Xτ(m, ν)|2.
(30.66)
In order to illustrate that the proposed technique is insensitive to phase variations in the echo path,
both EEF estimates, (30.61) and (30.66), are compared. The proposed methods are implemented using
a discrete short-time Fourier transform (STFT), running at a sampling rate of 16 kHz. A 512-tap FFT
is used using sine analysis and synthesis windows and successive windows have an overlap of 50%.

836
CHAPTER 30 Acoustic Echo Control
Frequency [kHz]
Time [s]
[dB]
0
1
2
3
4
5
6
7
8
0
2
4
6
8
−10
0
10
FIGURE 30.5
The true EEF G(m,ν) as it would be ideally estimated.
In order to simulate realistic scenarios, a measured room impulse response hn of length 64 ms is used
to model the echo path. Also, as reference, the true EEF G(m, ν) is shown in Figure 30.5 as it would
be ideally estimated over the complete simulation time. The simulations consider a far-end signal with
additive Gaussian noise with a SNR of 24 dB. The microphone signal contains the echo and near-end
Gaussian noise with the same SNR. Figure 30.6 shows, for three different phase variations, the EEF
computed as in (30.61) and (30.66). For Panels (a) and (d) a change in the phase of the room impulse
response hn after 4 seconds has been simulated, resulting in the EEF ﬁlters 
Gw(m, ν) and 
Gb(m, ν),
respectively. While the EEF based on complex spectra diverges from the desired ﬁlter when the phase
of hn is modiﬁed, the EEF based on power spectra stays similar to the true EEF in Figure 30.5. Also,
a sampling rate mismatch of 1 Hz between loudspeaker and microphone signals was simulated. Panels
(b) and (e) show the resulting EEF ﬁlters 
Gw(m, ν) and 
Gb(m, ν), respectively, where only the second
stays similar to the true EEF. The observations are the same between the EEF ﬁlters 
Gw(m, ν) and

Gb(m, ν), in Panels (c) and (f), where random loss of 4 samples in the loudspeaker signal was simulated.
Figure 30.6 indicates that the proposed EEF estimate 
Gb(m, ν) converges quickly and is hardly affected
by the phase and time modiﬁcations of the echo path. In contrast to that, the conventional EEF estimation
method 
Gw(m, ν) does not converge since it relies on phase information.
However, assuming that the non-zero near-end signal S(m, ν) and the far-end signal Xτ(m, ν) are
statistically independent and zero mean, it follows from (30.58) that 
Gb(m, ν) according to (30.66)
gives

G2
b(m, ν) = |G(m, ν)|2 + E

|S(m, ν)|2	 E

|Xτ(m, ν)|2
E

|Xτ(m, ν)|4,
(30.67)
as demonstrated in Section 4.30.3.5. Obviously, any non-negligible near-end signal S(m, ν) included
in the microphone signal Y(m, ν) leads to a positive bias in the estimate 
Gb(m, ν). The biased EEF
leads to too large estimates of the echo power in the spectrum. From (30.67) it follows that this effect
is especially prominent in case of high levels of the near-end signal S(m, ν). In the following, we
additionally describe a method to compute the EEF based on power spectra, however, without bias.

4.30.3 Echo Suppression
837
(a)
Frequency [kHz]
0
2
4
6
8
(d)
[dB]
−10
0
10
(b)
Frequency [kHz]
0
2
4
6
8
(e)
[dB]
−10
0
10
(c)
Frequency [kHz]
Time [s]
0
2
4
6
8
0
2
4
6
8
(f)
Time [s]
[dB]
0
2
4
6
8
−10
0
10
FIGURE 30.6
Effect of phase variations in the room impulse response hn on Gw(m,ν) (on the left) and Gb(m,ν) (on the
right). (a) and (d) Effect of a phase response modiﬁcation after 4 s. (b) and (e) Effect of a sampling rate
mismatch of 1 Hz between loudspeaker and microphone signals. (c) and (f) Effect of random loss of four
samples in the loudspeaker signal.
This is achieved by estimating G(m, ν) based on temporal ﬂuctuations of the power spectra computed
according to
Y(m, ν) = |Y(m, ν)|2 −E

|Y(m, ν)|2	
,
(30.68)
Xτ(m, ν) = |Xτ(m, ν)|2 −E

|Xτ(m, ν)|2	
.
(30.69)
In practice, the expectation operator E{.} is implemented as single pole temporal averaging analogously
to (30.63). Note that the smoothing time constant used in (30.68) and (30.69) is chosen smaller than
the time constant T in (30.65), i.e., about a few hundred of milliseconds. The estimation of the EEF
is then performed analogously to (30.66), but based on the ﬂuctuating spectra of the loudspeaker and

838
CHAPTER 30 Acoustic Echo Control
(a)
Frequency [kHz]
0
2
4
6
8
(c)
[dB]
−10
0
10
(b)
Time [s]
Frequency [kHz]
0
2
4
6
8
0
2
4
6
8
(d)
Time [s]
[dB]
0
2
4
6
8
−10
0
10
FIGURE 30.7
Biased (on the left) and unbiased (on the right) EEF estimates for 24 dB and 6 dB SNR in Panels (a), (d)
and (c), (d), respectively.
microphone signals:

G2(m, ν) = E
Xτ(m, ν)Y(m, ν)

E
Xτ(m, ν)Xτ(m, ν)
.
(30.70)
This, as demonstrated in Section 4.30.3.5, yields an un-biased echo estimation ﬁlter,

G2(m, ν) = |G(m, ν)|2.
(30.71)
It is important to note that the ﬂuctuating power spectra are only used for the estimation of G(m, ν).
The computation of the echo suppression ﬁlter F(m, ν), in (30.60), is still based on the original power
spectra of the loudspeaker and microphone signals. Based on the simulations as deﬁned previously,
we show the bias resulting from the EEF estimate using power spectra (30.66). A far-end signal with
a SNR of 24 dB is considered, while the microphone signal contains the echo and near-end Gaussian
noise with two different SNRs: 24 dB and 6 dB. Figure 30.7 shows in the two left panels the biased EEF
estimates 
Gb(m, ν), and in the two right panels, the unbiased EEF estimates 
G(m, ν), for 24 dB and
6 dB SNR, respectively. While the unbiased EEF estimates 
G(m, ν) are similar for all SNR conditions,
the biased EEF estimates 
Gb(m, ν) are more impaired the lower the SNR is.
Eventually, in order to prevent the EEF (30.71) from diverging, when near-end speech is active, a two
echo path model [43] is used. One background path comprising a fully adaptive echo estimation ﬁlter,
and one foreground path which comprises the echo estimation ﬁlter effectively used for computation
of the echo power spectrum. The values of the foreground echo estimation ﬁlter are refreshed by those
of the background one based on the performance of the algorithm. Also two voice activity detectors

4.30.3 Echo Suppression
839
(a)
Frequency [kHz]
[dB]
0
2
4
6
8
−40
−20
0
20
(b)
Frequency [kHz]
[dB]
0
2
4
6
8
−40
−20
0
20
(c)
Time [s]
Frequency [kHz]
[dB]
0
1
2
3
4
5
6
7
8
0
2
4
6
8
−40
−20
0
20
FIGURE 30.8
A far-end speech signal is considered in Panel (a). The resulting echo estimates corresponding to (30.59)
and (30.72) are respectively shown in Panels (b) and (c). The used exponential decay αRT corresponds to a
time constant of 60 ms.
(VADs) at far-end and near-end sides [2], respectively, are used to discriminate whenever far-end and/or
near-end speech is active.
4.30.3.3 Echo suppression ﬁlter
The estimate of the echo spectrum according to (30.59) covers only a fraction of the length of the
true echo path corresponding to direct sound and early reﬂections. To cope with the echo components
resulting from late reverberation, a temporal smoothing is applied to the echo spectrum estimate in
order to mimic typical exponential decay of late reﬂexions. This is achieved by applying recursively a
forgetting factor on the echo power spectrum estimate of previous frame
|Y(m, ν)| = max

|Y(m, ν)|, αRT|Y(m −1, ν)|

,
(30.72)
where αRT ∈[0, 1] is determined as a function of the amount of late reverberation and is computed
similarly to (30.65). Based on the simulations as deﬁned previously, and considering a far-end speech
signal X(m, ν) shown in Panel (a) of Figure 30.8, the corresponding echo estimate (30.59) is shown in
Panel (b), while the smoothed estimate (30.72) is shown in Panel (c). The described temporal smoothing
does not change signal dynamic but models the exponential decay of the echo.

840
CHAPTER 30 Acoustic Echo Control
From the echo estimate (30.72), the optimum values for the echo suppression ﬁlter (ESF) F(m, ν)
can be derived by minimizing the contribution of the echo components |Y(m, ν)|2 to the output signal
S(m, ν) in the mean square error (MSE) sense. Since the near-end signal S(m, ν) and the loudspeaker
signal X(m, ν) were assumed to be statistically independent, we obtain [10]
Fopt(m, ν) = E

|Y(m, ν)|2
−E

|Y(m, ν)|2
E

|Y(m, ν)|2
.
(30.73)
A practical approach for the computation of the ESF is based on generalized, instantaneous versions
of (30.73). In [101] it has been proposed to use the power spectral subtraction approach analogously
to [108]:
F(m, ν) = |Y(m, ν)|2 −β|Y(m, ν)|2
|Y(m, ν)|2
,
(30.74)
where β represents a design parameter to control the amount of echo to be suppressed [109]. F(m, ν)
can be considered as an estimate of Fopt(m, ν) according to (30.73). To prevent residual echoes, the
ESF in (30.74) is computed to attenuate the microphone signal aggressively such that no residual echo
remains. This is, e.g., achieved by intentionally over-estimating the echo power spectrum (by choosing
β > 1), but also by applying a suitable time-smoothing to F(m, ν). These design parameters have an
important role to address attenuation of residual echoes, resulting from long echo paths, non-linearities,
etc. Considering a near-end speech signal as shown in Figure 30.9, the computation of ESF is simulated
with the same far-end signal as in Panel (a) of Figure 30.8 and the echo path as deﬁned in Figure 30.5.
The resulting ESF is shown in Panel (b) of Figure 30.9, where the contribution for the echo is removed
and the time-frequency tiles corresponding to near-end signal are kept untouched as shown by Panel (c).
The acoustic echo suppression method, as presented above, is derived analogously to spectral sub-
traction used for stationary noise suppression [106]. And since echo control applications often require
in the same time noise suppression, both echo and noise suppression can be advantageously combined
to minimize the resulting distortions on the processed signal like “musical noise” artifacts [110].
4.30.3.4 Perceptual acoustic echo suppression
In order to further reduce computational complexity, the AES processing is not carried out on each STFT
frequency bin separately. The uniformly spaced spectral coefﬁcients can be grouped into a number of
non-overlapping partitions, similar as in [111]. Each such partition (group of bins) corresponds to one
subband in which the processing is carried out. The bandwidth of each group of bins is chosen such
that it roughly follows the frequency resolution of the human auditory system. The partition bandwidth
for example is chosen to be approximately two times the equivalent rectangular bandwidth (ERB)
[112], resulting, for example, in 16 partitions for 16 kHz as shown in Panel (a) of Figure 30.10. The
different statistics and ﬁlters are then only computed once for each partition, instead of once for each
STFT frequency bin, resulting in lower computational complexity. Prior to applying the partition echo
suppression ﬁlter (ESF) to the uniform signal STFT spectrum, it has to be interpolated. An example of
this interpolation is illustrated in Panel (b) of Figure 30.10: At one frame, the values for partitions are
indicated as points (•) and the resulting interpolated values, obtained by interpolation, are indicated as
line. In this way, the proposed interpolation smoothes the ﬁlter values over frequency reducing artifacts

4.30.3 Echo Suppression
841
(a)
Frequency [kHz]
[dB]
0
2
4
6
8
−40
−20
0
20
(b)
Frequency [kHz]
[dB]
0
2
4
6
8
−40
−30
−20
−10
0
(c)
Time [s]
Frequency [kHz]
[dB]
0
1
2
3
4
5
6
7
8
0
2
4
6
8
−40
−20
0
20
FIGURE 30.9
The desired near-end speech signal is shown in Panel (a). The ESF is shown in Panel (b) with a far-end
signal chosen to be the same as in Panel (a) of Figure 30.8. The resulting output signal S(m,ν) is shown in
Panel (c)
which would result from high ﬂuctuations of the ﬁlter. This approach, based on frequency resolution of
the human auditory system, is referred as to perceptual acoustic echo suppression (PAES) [113].
4.30.3.5 Derivation of echo estimation ﬁlters
The present subsection gives the derivation which lead to the biased and unbiased echo estimation ﬁlters
(EEF), (30.67) and (30.71), respectively. First, we consider a general result holding for statistically
independent processes. Let A and B be two independent statistical random processes, and f and g two
arbitrary functions. Then,
E { f (A)g(B)} = E { f (A)} E {g(B)}
(30.75)
holds [114].
Regarding the EEF derivation itself, it is reasonable to assume that loudspeaker signal Xτ(m, ν) and
near-end signal S(m, ν) are statistically independent, zero-mean random processes. In the following,
the indexes m and ν are discarded for presentational simplicity. From (30.58) it follows that the power

842
CHAPTER 30 Acoustic Echo Control
0
0.5
1
(a)
0
1
2
3
4
5
6
7
8
0
0.5
1
(b)
Frequency [kHz]
FIGURE 30.10
Panel (a) shows how the STFT frequency bins are grouped to obtain partitions mimicking the frequency
resolution of the human auditory system. Panel (b) shows an example of interpolation of the ESF F (m,ν)
from partitions to STFT frequency bins to be applied on the microphone spectrum Y (m,ν).
spectrum |Y|2 can be written as:
|Y|2 = YY ∗
= (GXτ + S)(GXτ + S)∗
(30.76)
= |G|2|Xτ|2 + |S|2 + G∗X∗
τ S + GXτ S∗.
Since both, Xτ, and S are statistically independent, zero-mean processes,
E

|Y|2	
= |G|2E

|Xτ|2	
+ E

|S|2	
.
(30.77)
The EEF is estimated by (30.66), whose numerator is
E

|Y|2|Xτ|2	
= E

|G|2|Xτ|2 + |S|2 + G∗X∗
τ S + GXτ S∗
|Xτ|2	
,
(30.78)
considering 30.75, it leads to
E

|Y|2|Xτ|2	
= |G|2E

|Xτ|4	
+ E

|S|2	
E

|Xτ|2	
.
(30.79)
Furthermore, the denominator of (30.66) is
E

|Xτ|2|Xτ|2	
= E

|Xτ|4	
.
(30.80)

4.30.4 Multichannel Acoustic Echo Cancellation
843
The EEF according to (30.79) and (30.80), thus, yields

G2
b = |G|2 + E|S|2 E

|Xτ|2
E

|Xτ|4.
(30.81)
As can be seen, the near-end signal S introduces a bias term into the estimate of the EEF. Furthermore,
(30.81) implies that the bias in the EEF increases with increasing near-end signal variance.
In the proposed method, in order to avoid the bias introduced in (30.66), the EEF in (30.70) is
computed based on the temporal ﬂuctuations of the power spectra (30.68) and (30.69). The numerator
of the EEF (30.70) is given by
E
Y Xτ

= E

|G|2|Xτ|2 + |S|2 + G∗X∗
τ S + GXτ S∗
(30.82)
−|G|2E

|Xτ|2	
−E

|S|2	 
|Xτ|2 −E

|Xτ|2		
.
This simpliﬁes to
E
Y Xτ

= E

|G|2|Xτ|4 −2|G|2|Xτ|2E

|Xτ|2	
+ |G|2E

|Xτ|2	2
,
(30.83)
and ﬁnally, considering (30.75),
E
Y Xτ

= |G|2

E

|Xτ|4	
−E

|Xτ|2	2
.
(30.84)
Also, the denominator of (30.70) is
E
Xτ Xτ

= E

|Xτ|2 −E

|Xτ|2	 
|Xτ|2 −E

|Xτ|2		
= E

|Xτ|4	
−E

|Xτ|2	2
.
(30.85)
Thus, the EEF according to (30.70) yields

G2 = |G|2.
(30.86)
Note that (30.70) leads to an unbiased estimate of the echo power transfer function also in case of
near-end signal included in the microphone signal.
4.30.4 Multichannel acoustic echo cancellation
Multichannel acoustic echo cancellation (MCAEC) is a key technology whenever hands-free and full-
duplex communication in modern systems with multichannel sound reproduction is required. For various
applications, such as home entertainment, virtual reality (e.g., games, simulations, training), or advanced
teleconferencing, multimedia terminals with an increased number of audio channels for sound repro-
duction are highly desirable (e.g., stereo, 5.1 surround systems, or even beyond). Although the basic

844
CHAPTER 30 Acoustic Echo Control
principle of echo cancellation has been well known for several decades, the multichannel case poses
some additional and fundamentally different challenges. Moreover, there are even some notable differ-
ences between the two-channel case and the general multichannel case which has been addressed bit by
bit only in recent years. The aim of this section is twofold. On the one hand, after a brief review of the
problem of multichannel acoustic echo cancellation, this section gives an outline of how the problem
may be tackled based on some fundamental principles. In this sense, the presentation in this section
brings together ideas from the theory on signals and systems, information theory, psychoacoustics, and
also wave physics. Based on this framework, and as the other main contribution, we present in this
section some recent advances in the ﬁeld of MCAEC. Thereby, important issues in the case of more
than two channels are emphasized. Finally, as an outlook, we touch on some ongoing work towards
MCAEC for massive multichannel sound reproduction, such as wave ﬁeld synthesis.
4.30.4.1 Description of signals and systems
Acoustic echo cancellation has already been discussed extensively for stereo sound reproduction (e.g.,
[115–118]). Only in recent years, AEC has been realized for more than two reproduction channels [26,
119,120]. Figure 30.11 describes a typical scenario for stereo and multi-channel AEC. In a transmission
room, a sound source (e.g., a speaker) is picked up by P microphones (P = 2 for stereo). The microphone
signals are transmitted to a receiving room and reproduced via P loudspeakers. At the same time, a
microphone in the receiving room picks up speech from a local user. In order to avoid an echo of
the loudspeaker signals xi(n) in the transmission room, the AEC attempts to cancel the additional
contributions by the loudspeakers to the microphone by subtracting ﬁltered versions of the loudspeaker
signals from the microphone signal. This generally requires that cancellation ﬁlters (assumed to be
s
y
x1
h1
hP
h1
e
xP
hP
g1
gP
Receiving Room
Transmission Room
...
...
...
...
...
...
-
^
^
preprocessor
FIGURE 30.11
Scenario for multi-channel AEC.

4.30.4 Multichannel Acoustic Echo Cancellation
845
length-N FIR ﬁlters) are dynamically adjusted by an adaptation algorithm to achieve minimum error
signal e(n) and thus optimum cancellation. This is the case when the adaptive cancellation ﬁlters
ˆhi(n) =

ˆhi,1(n), . . . , ˆhi,N(n)
T
,
i = 1, 2, . . . , P
(30.87)
accurately model the impulse responses hi from the emitting speakers to the microphone.
It has been shown for stereo AEC that a so-called non-uniqueness problem exists [121]: If the
loudspeaker signals are strongly correlated, then the adaptive ﬁlters generally converge to a solution
that does not correctly model the transfer functions between the speakers and the microphone, but
merely optimizes echo cancellation for the given particular loudspeaker signals [28]. This is due to the
fact that the observation model y = h1 ∗x1 + · · · + h P ∗xP does not provide us with enough linearly
independent equations for resolving the unknowns hi and the problem is thus underdetermined. As a
consequence, a change in the characteristics of the loudspeaker signals (e.g., due to a change of the
geometric position of the sound source in the transmission room) will result in a breakdown of the echo
cancellation performance and requires new adaptation of the cancellation ﬁlters.
From a statistical point of view, the high cross-correlations between the loudspeaker signals lead to a
highly ill-conditioned tap-input correlation matrix Rxx(n) in the normal equation, Rxx(n)h(n) = rxx(n),
to be solved for the minimization of E{e2(n)} [115], where
Rxx(n) = E

x(n)xT (n)
	
(30.88)
=
⎡
⎢⎣
Rx1x1(n) · · · Rx1xP(n)
...
...
...
RxPx1(n) · · · RxPxP(n)
⎤
⎥⎦,
x(n) =

x1(n), . . . , xP(n)

,
xi(n) =

xi(n), . . . , xi(n −N + 1)
T ,
h(n) =

h1(n), . . . , hP(n)

.
(30.89)
To tackle this challenging problem of ill conditioning, various techniques have been proposed mainly
in the stereo context so far. They can be distinguished into two different classes representing separate
system components as shown, e.g., in [116]:
(a) Application of a robust and fast converging adaptation algorithm taking all cross-correlations into
account.
(b) Preprocessing of the signals transmitted from the transmission room prior to their reproduction in
the receiving room in order to partially decorrelate all channels relative to each other.
We then face the two conﬂicting requirements that, on the one hand, the preprocessing must not introduce
any objectionable artifacts into the reproduced audio signals while, on the other hand, we require
a decorrelation for convergence enhancement. Therefore, a systematic design for MCAEC based on
ﬁrst principles of coefﬁcient estimation and optimization, together with a complete stochastic signal
description, and considering human auditory perception, is necessary. The structure of this section is
motivated by a step-by-step incorporation of these principles. Within this framework, we place recent
advances in MCAEC with emphasis on more than two reproduction channels, and deduce various new
insights and practical results.

846
CHAPTER 30 Acoustic Echo Control
4.30.4.2 Elements from estimation and information theory
In general, to optimally exploit the information contained in the involved signals, the coefﬁcient esti-
mation process should take into account all their fundamental stochastic properties: Nongaussianity,
nonwhiteness, nonstationarity. A suitable broadband signal formulation for this purpose was developed
within the so-called TRINICON framework for adaptive multiple-input and multiple-output (MIMO)
ﬁltering [55,57,122,123], as already mentioned in Section 4.30.1.
In [56], the AEC problem was linked explicitly to the more general MIMO system identiﬁcation and
signal separation problem as addressed by TRINICON, and as illustrated by the two dashed boxes in
Figure 30.11. The left and right dashed boxes correspond to a MIMO mixing system and a corresponding
MIMO demixing system, respectively. The demixing system follows rigorously from the ideal MIMO
separation solution derived in [124,125]. This formal connection facilitates the introduction of stochastic
signal models in the form of multivariate probability densities which capture the temporal structure by
multiple time lags and the nonstationarity by time-varying correlation matrices.
The TRINICON optimization criterion for the case of separation (and system identiﬁcation) problems
is based on minimizing the information-theoretic quantity of mutual information between the output
channels of the demixing MIMO system using the multivariate densities mentioned above. In the special
case of AEC, we separate the contributions of the loudspeaker signals from the error signal e(n) at the
AEC output (Figure 30.11 and [56]). In the special case of Gaussian signals, this separation process
corresponds to a simultaneous block-diagonalization of the output correlation matrix for multiple time
instants since the local speech s(n) is assumed to be uncorrelated from the loudspeaker signals [56,123].
Following [126], we show here how to generalize the information-theoretic separation approach in
[56] to multichannel AEC with typically highly correlated loudspeaker signals. Speciﬁcally, the output
channels x1(n), . . . , xP(n) of the mixing system in Figure 30.11 do not require separation from each
other. Figure30.12 illustrates the output correlation matrix Ryx after mixing (left) and the corresponding
desired structure Rex after demixing (right) for the special case of Gaussian signals and P = 2. Hence,
the approach in [56] generalizes straightforwardly to the MCAEC case by just using this modiﬁed matrix
partitioning.
In [56] the update equations for TRINICON-based coefﬁcient adaptation in AEC have been presented
forthesimplecaseofgradient-basedoptimization.However,itisknownthatgradient-descentalgorithms
(e.g., LMS/NLMS [10]) generally exhibit very slow convergence for highly correlated input signals such
as in the multichannel case.
The so-called Newton-Raphson-type optimization procedure is known as the canonical method for
more challenging optimization problems. As detailed in [55], a TRINICON-based Newton update can
be derived in a way analogous to [52]. The Newton algorithm contains virtually all of the well-known
adaptation schemes as special cases, most notably the recursive least-squares (RLS) algorithm. The
important feature of Newton-type/RLS-type algorithms is that they explicitly take all input correlations
(30.88) into account within their Hessian matrix [10,52] which makes them very attractive for the
MCAEC application [116].
In addition to this desirable property of RLS-type algorithms, the more general TRINICON-based
approach inherently leads to a multivariate error nonlinearity to take both the nongaussianity and the
nonwhiteness of the near-end signal into account [56]. This provides an inherent double-talk handling
and a link to the powerful concept of robust statistics, e.g., [51,53]. Moreover, the block online adaptation
and block averaging obtained in [56] further speeds up the convergence (especially in MCAEC).

4.30.4 Multichannel Acoustic Echo Cancellation
847
Each diagonal
represents
one time-lag
auto-correlation Ryy
cross-correlation Ryx1
correlation matrix
Ree of error signal
correlation matrix Rxx
of loudspeaker signals
FIGURE 30.12
AEC process for second-order statistics and P = 2.
Note also that the general TRINICON-based approach also leads to important insights in the case of
AEC for multiple microphone channels in the receiving room, as explained further in Section 4.30.4.4.
Finally, another aspect in the design of a real-time solution to MCAEC is its computational com-
plexity. Unfortunately, straightforward implementations of RLS-type algorithms are computationally
very expensive due to the required (implicit or explicit) inversion of the correlation matrix. A very
efﬁcient practical solution to this problem is to formulate the above-mentioned broadband algorithm
in a mathematically rigorous way in the frequency domain, as shown, e.g., in [26,52,123], followed
by the introduction of carefully selected approximations. The most important features of this con-
cept of frequency-domain adaptive ﬁltering (FDAF) is that in addition to the efﬁcient use of the FFT
(gains for both, adaptation and ﬁltering), all the sub-matrices of the input correlation matrix (30.88)
are approximately diagonalized by the DFT. In this way, it is possible to efﬁciently take into account
all cross-correlations [26]. This is possible for both, second-order and higher-order statistics. A ﬁrst
MCAEC system for 5-channel surround sound applications, based on the multichannel FDAF algo-
rithm has been presented in [26,119]. This real-time implementation also utilizes the concept of robust
statistics [52].
4.30.4.3 Elements from psychoacoustics
As mentioned in Section 4.30.4.1, among the key requirements for the techniques to preprocess the
signals transmitted from the transmission room prior to their reproduction in the receiving room is
the subjective sound quality. While several of the known preprocessing techniques provide enough
decorrelation to achieve proper AEC convergence in the stereo case, considerations of sound quality
have frequently not been addressed adequately. In this section we ﬁrst give a brief overview of the
known two-channel preprocessing approaches. We then describe a recently introduced novel approach

848
CHAPTER 30 Acoustic Echo Control
[120], based on perceptual considerations. It easily generalizes to the multi-channel case and has been
demonstrated to be effective in surround sound echo cancellation.
4.30.4.3.1
Known two-channel preprocessing approaches
A ﬁrst simple preprocessing method for stereo AEC was proposed by Benesty et al. [115,127] and
achieves signal decorrelation by adding non-linear distortions to the signals. While this approach fea-
tures extremely low complexity, the introduced distortion products can become quite audible and objec-
tionable, especially for high-quality applications using music signals. Moreover, the generalization of
this approach to an arbitrary number of channels is not straightforward.
A second well-known approach consists of adding uncorrelated noise to the signals. In [128], this is
achieved by perceptual audio coding/decoding of the signal which introduces uncorrelated quantization
distortion that is masked due to the noise shaping according to the encoder’s psychoacoustic model. The
use of an explicit psychoacoustic model plus analysis/synthesis ﬁlterbanks is able to prevent audible
distortions in audio signals and may be easily generalized to more than two channels. However, the
associated implementation complexity and the introduced delay render this approach unattractive for
most applications.
Other approaches employ switched/time-varying time-delays [118] or variable all-pass ﬁltering [129]
to produce a time-varying phase shift / signal delay between the two channels of a stereo AEC and thus
“decorrelate” both signals. Speciﬁcally, [118] describes a preprocessing system in which the output
signal switches between the original signal and a time-delayed/ﬁltered version of it. As a disadvantage,
this switching process may introduce unintended artifacts into the audio signal. Ali [129] describes a
system in which an allpass preprocessor is randomly modulating its allpass ﬁlter parameter. In [130],
it was proposed to apply this allpass preprocessor only to the low frequency range up to 1 kHz due to
convergence requirements.
4.30.4.3.2
Psychoacoustically motivated method for the multichannel case
In order to obtain a preprocessing method offering both good decorrelation properties for the enhance-
ment of AEC convergence and minimal alteration of the perceived stereo image, the method proposed
in [120] is based on several considerations. From the previously discussed approaches, the time-varying
modulation of the phase of the audio signal, as proposed in [118,129], is an effective method which
is generally unobtrusive in its perceptual effects on audio signals as compared to other methods while
avoiding computationally expensive masking models. Nonetheless, it is difﬁcult to achieve maximum
decorrelation while guaranteeing that introducing a time/phase difference between left and right chan-
nels does not result in an alteration of the perceived stereo image. Several aspects must be accounted
for:
•
Interaural phase/time difference is a relevant perceptual parameter for subjective perception of
a sound stage [131] and has been used extensively in synthesis of stereo images (e.g., [111]).
Consequently,achangeintheperceivedstereoimagecanonlybeavoidediftheintroducedtime/phase
difference stays below the threshold of perception, as it applies to audio signals that are reproduced
via loudspeakers.
•
Optimal AEC convergence enhancement can be achieved if the preprocessing introduces time/phase
differences just at the threshold of perception, i.e., applies the full amount of tolerable modiﬁcation.

4.30.4 Multichannel Acoustic Echo Cancellation
849
•
Known from psychoacoustics, the human sensitivity to phase differences is high at low frequen-
cies, and gradually reduces for increasing frequencies, until it fully vanishes for frequencies above
ca. 4 kHz.
•
Neither a simple time delay modulation nor a low-order time-varying allpass ﬁltering approach
offers the ﬂexibility to tailor the amount of time/phase shifting as a function of frequency, such that
the full potential of perceptually tolerable change is exploited.
Hence, in contrast to earlier phase modulation approaches, the method in [120] is designed to allow
a perceptually motivated frequency-selective choice of phase modulation parameters (modulation fre-
quency, modulation amplitude, and modulation waveform) by employing analysis/synthesis ﬁlterbanks.
The input audio signal is decomposed into subband signals by means of an analysis ﬁlterbank. Then,
the subband phases are modiﬁed based on a set of frequency-dependent modulating signals. According
to the above considerations, subbands belonging to the low frequency part of an audio signal should be
left largely untouched, while subbands corresponding to frequencies above 4 kHz may be modulated
heavily. The frequency-selective phase modulation amplitude can be optimized by a listening procedure.
Finally, the modiﬁed spectral coefﬁcients are converted back into a time-domain representation by a
synthesis ﬁlterbank. To allow easy access to the signal’s phase, a complex-valued ﬁlterbank [132] is
used, and a phase modiﬁcation is implemented by a complex multiplication of the subband coefﬁcient
with ejϕ(t,ν) where ϕ(t, ν) denotes the intended time-varying phase shift in subband ν. It is preferable
to choose a smooth modulating function ϕ(t, ν), such as a sine wave at a relatively low frequency.
Moreover, to account for the symmetry of typical multi-channel speaker setups, such as 5.1 or 7.1, the
modulation of channel pairs is carried out in a complex conjugate fashion. The modulation frequencies
for pairs are chosen such that they provide “orthogonal” modulation activity.
Figure30.13 shows a summary of the results of a standardized subjective MUSHRA (“MUlti Stim-
ulus test with Hidden Reference and Anchor”) listening test carried out with 10 experienced listeners
in a typical surround sound listening setup. The sound quality was quantiﬁed on a scale from 0 to
100 for 5 critical music excerpts and one speech excerpt (see [120] for further details). The different
hidden ref.
lp35
mp3 48
mp3 48 phase
phase
NL 05
0
20
40
60
80
100
bad
poor
fair
good
excellent
FIGURE 30.13
MUSHRA listening test results (averages and 95% conﬁdence).

850
CHAPTER 30 Acoustic Echo Control
preprocessing types are the original reference and a 3.5 kHz band-limited version thereof, both required
by MUSHRA, the individual channel mp3 en/decoding at 48 kbit/s (“mp3 48”), the described perceptual
phase modulation method (“phase”), a combination of mp3 encoding/decoding and phase modulation
(“mp3 48 phase”), and the conventional non-linear processing (“NL 05” after [115,127]). It is visible
from the graph that the phase modulation method emerges as the clear winner in terms of sound qual-
ity. Note that the latter four methods were tuned for comparable convergence speeds of the adaptive
algorithms.
4.30.4.4 MIMO processing and elements from wave physics
4.30.4.4.1
MIMO case for multiple microphones
So far in this section, we have focused on the case of multiple reproduction channels but only one
microphoneinthereceivingroom.ThemoregeneralcaseofafullMIMOloudspeaker-room-microphone
system appears when combining MCAEC with a microphone array, e.g., [26]. Traditionally, in this case
several parallel multiple-input and single-output (MISO) systems are independently applied, which has
been shown to be optimal in terms of least-squares-based coefﬁcient estimation.
As explained in Section 4.30.4.2, TRINICON-based AEC is generally able to exploit the nonwhite-
ness of the signals in the receiving room (upper left sub-matrix in Figure 30.12). By further generalizing
the TRINICON-based AEC to the case of MIMO loudspeaker-room-microphone systems, it is also
able to exploit the spatial nonwhiteness in the receiving room by simultaneously taking into account all
microphone signals for the adaptation process. In other words, the performance may be improved with
multiple microphones.
4.30.4.4.2
Massive multichannel systems and wave physics
Current loudspeaker setups, such as the 5.1 format, still rely on a restrained listening area (“sweet spot”).
A high-quality volume solution for a large listening space is offered by the wave ﬁeld synthesis (WFS)
method which is based on wave physics [133]. The so-called Kirchhoff-Helmholtz integrals which can
be derived from the acoustic wave equation state that at any point within a source-free listening area,
the sound pressure ﬁeld can be calculated if both the sound pressure and its gradient are known on the
contour enclosing this area. Thus, in WFS, closely spaced arrays of a large number P of individually
driven loudspeakers generate a pre-speciﬁed sound ﬁeld. P may lie between 20 and several hundred.
An analogous approach is possible for wave ﬁeld analysis (WFA) using microphone arrays.
Building a full-duplex system with this massive multichannel setup for unrestricted audio content
might be considered as the supreme discipline of MCAEC research since in this case even the P × P
frequency bin-wise correlation matrices of the loudspeaker driving signals are generally still large and
ill-conditioned after the approximate blockwise diagonalization of (30.88) within the frequency-domain
adaptive ﬁltering (FDAF) coefﬁcient update (cf. Section 4.30.4.2).
The basic idea of wave-domain adaptive ﬁltering (WDAF), e.g., [134,135], is to replace the point-to-
point MIMO system model by a more detailed spatial consideration exploiting wave-physics foundations
as in WFS/WFA. In particular, WDAF extends the conventional FDAF approach by a suitable spatio-
temporal transform for efﬁciency. Figure30.14 illustrates this two-step transformation approach from
the RLS via FDAF towards WDAF in terms of the loudspeaker correlation matrix and its approximate
temporal and spatio-temporal diagonalization. It can be seen that each transformation step is supposed

4.30.5 Nonlinear Modeling and Cancellation of Echo
851
MC RLS
Rxx
Rxx
temporal
diag.
diag.
MC FDAF
Sxx
Sxx
grouping into
temporal frequency bins
S(ν)
xx
spatial
diag.
WDAF
Txx
Txx
grouping into
temporal frequency bins
T(ν)
xx
grouping into
spatio-temporal bins
T(ν,kθ)
xx
FIGURE 30.14
WDAF concept and relationship with conventional algorithms.
to achieve yet more sparseness of the multichannel correlation matrix. More sparseness then provides
a more tangible basis for explicit or implicit matrix inversion.
Requirements for the spatio-temporal basis functions are that they should be orthogonal and must
fulﬁll the acoustic wave equation (e.g., circular harmonics). Moreover, since the transducers are only
placed on the contour enclosing the listening area, corresponding transformations taking into account
the Kirchhoff-Helmholtz Integrals are necessary. These transformations depend on the array geometries,
and for certain setups, e.g., circular arrays [134,135], they can in fact be formulated in a compact form.
A rigorous formulation of RLS-type MIMO algorithms in spatio-temporal transform domains using
arbitrary orthogonal bases was developed in [136] as a systematic extension of the FDAF formalism,
e.g., [26,52,123].
Advantages of the approximate MIMO decoupling due to the spatio-temporal transformation are
both an improved convergence and a signiﬁcant complexity reduction, as shown, e.g., in [134,135].
Note also that the WDAF concept can be well applied to the general TRINICON approach. Since all
microphone signals are jointly taken into account by the spatio-temporal transformation, WDAF also
facilitates an efﬁcient exploitation of the spatial nonwhiteness mentioned in the previous subsection.
Recently, a related approach using data-based estimation of optimal decoupling transformation matri-
ces was proposed in [137]. Advantages of the data-based approach are that the resulting transformations
account for arbitrary array geometries and reverberant environments.
4.30.5 Nonlinear modeling and cancellation of echo
The previous approaches for acoustic echo cancellation assumed that the acoustic echo path can be
modeled by a linear system. In practice, however, many loudspeaker systems involve non-negligible
nonlinearities, e.g., caused by overloaded ampliﬁers or low-cost loudspeakers driven at high volume
[138,139]. Beyond a certain degree of nonlinear distortion, purely linear approaches are not able to

852
CHAPTER 30 Acoustic Echo Control
provide sufﬁcient echo attenuation and nonlinear approaches become desirable. Therefore, we recall
different approaches that have been proposed for coping with nonlinear acoustic echoes based on sim-
pliﬁed models of the acoustic echo path. In Section 4.30.5.1, we prepare the discussion by ﬁrst outlining
physical properties of nonlinear audio hardware components that are typical in hands-free or mobile
communication. Then, in Sections 4.30.5.2–4.30.5.4, we present different nonlinear adaptive structures
for application in the nonlinear echo cancellation context. Thereby, we distinguish between memoryless
nonlinearities such as saturation characteristics of ampliﬁers on the one hand, and nonlinearities with
memory as required for nonlinear loudspeakers on the other hand. In the ﬁrst case, nonlinear cascaded
structures [140] and power ﬁlters [141] are considered, whereas in the latter case second-order Volterra
ﬁlters are of interest [142,143].
4.30.5.1 Nonlinear acoustic echo paths
The structure of an acoustic echo path with possible nonlinearities is illustrated in Figure30.15. As can
be seen, it basically consists of the cascade of the ampliﬁer, loudspeaker, and microphone. Additionally,
it comprises the acoustic propagation path between the loudspeaker and the microphone.
The propagation path between loudspeaker and microphone can usually be considered as a linear
system. It is commonly modeled by a linear FIR ﬁlter representing the room impulse response. The
microphone signals that are common with hands-free and mobile telephony have only moderate exci-
tation levels. Thus, it is reasonable to assume a linear behavior for the microphone, too, which is in
accordance with the observations reported in [140]. We then consider two main sources for nonlinear
distortion: The ampliﬁer and the loudspeaker.
Ampliﬁer nonlinearities are especially present in mobile communication devices. There, the dilemma
arises to provide high signal levels while having a low battery voltage. Consumers usually prefer
an overloading of the ampliﬁer over a reduction of the sound volume. The nonlinear behavior of
ampliﬁers can therefore be described as a memoryless saturation characteristic with a soft clipping
of large amplitude values [140].
AEC
d(n)
s(n)
x(n)
y(n)
e(n)
ˆd(n)
FIGURE 30.15
Hardware setup of the acoustic echo cancellation problem.

4.30.5 Nonlinear Modeling and Cancellation of Echo
853
Many researchers have worked on the characterization of the nonlinearities of electrodynamic loud-
speakers (see, e.g., [144,145]). Summarizing their results, one can identify two sources of nonlinear
distortion that are relevant in the AEC context: The nonlinearities in the electromagnetic part are mainly
caused by the asymmetries of the magnetic ﬂux and its decay outside the air gap of the motor. Thus, the
driving force on the voice coil is a nonlinear function of its position. Additionally, in the mechanical
part, the nonlinear dependency of the stiffness of the spider and the outer rim on the position of the
voice coil has to be taken into account. Without looking at further details, we exploit the main result of
[144,145] which imply that the nonlinear behavior of loudspeakers can be modeled by an appropriate
Volterra ﬁlter. More precisely, we follow [142] and consider the loudspeaker as a black box, whose
input/output relation can be approximated sufﬁciently well by a second-order Volterra ﬁlter.
It should also be mentioned that the results presented in [141] indicate that for mobile phones, a
saturation-type behavior of the miniaturized loudspeakers can be expected. In this case, soft clipping
characteristics as in case of overloaded ampliﬁers represent a better model.
Other sources for nonlinear distortion in the acoustic echo path can be rattling and vibration effects
caused by a strong physical coupling between loudspeaker, microphone, and their enclosure, as, e.g.,
common in mobile phones. However, this distortion can hardly be modeled or predicted, as it is of
chaoticnature[102].Itshouldratherbeconsideredasuncorrelatednoise(analogouslytoanybackground
noise) and, thus, be processed accordingly. The problem of vibrating system components is not further
considered here.
4.30.5.2 Cascaded structure
First, we look at the case where the nonlinear distortion is introduced by an overloaded ampliﬁer. From
the discussion in Section 4.30.5.1 it follows, that a simpliﬁed model of the nonlinear echo path is given
by the cascade of a memoryless saturation characteristic, corresponding to the ampliﬁer, followed by a
linear FIR ﬁlter. Here, the linear ﬁlter corresponds to the remaining propagation path of the echo signal
including the loudspeaker, the room, and the microphone. The real nonlinear echo path and its cascaded
model is shown on the left hand side and the center of Figure30.16, respectively. The parallel structure
depicted on the right hand side will be considered later in Section 4.30.5.3.
The following discussion of adaptive realizations of the cascaded model according to Figure 30.16
center is based on [140]. The input/output relation of the memoryless nonlinearity can be expressed by
xnl(n) = f(a, x(n)),
(30.90)
where x(n) is the input signal and a denotes a parameter vector that includes all model parameters
required to specify the function f ( · ). Note that since f ( · ) represents a memoryless nonlinearity, its
output xnl(n) depends only on the current input value x(n). In general, f ( · ) could be any function that
properly models the desired saturation behavior. Possible functions are hard-clipping characteristics
[140], or other parametric functions as proposed in [139].
Another general class of memoryless nonlinearities is given by truncated Taylor series expansions
and has already been successfully applied to nonlinear AEC in [140]. In case of a Taylor series expansion

854
CHAPTER 30 Acoustic Echo Control
gk
h1,k
h2,k
hP,k
(·)1
(·)2
(·)P
=⇒
=⇒
FIGURE 30.16
Illustration of the nonlinear acoustic echo path (left), its cascaded model (center), and a parallelized model
with power ﬁlters (right).
truncated at order P, Eq. (30.90) becomes
xnl(n) =
P

p=1
apx p(n),
(30.91)
where ap represent the coefﬁcients of the Taylor series expansion, i.e., a = [a1, a2, . . . , aP] here. To
give a practical example, in [140] an order P of seven has been proposed.
The overall output of the cascaded structure is obtained as the linear convolution of xnl(n) with the
linear FIR ﬁlter gk. If the AEC in Figure 30.15 is realized accordingly, the estimate of the echo signal
ˆd(n) is given by
ˆd(n) =
Ng−1

k=0
gkxnl(n −k).
(30.92)
Since both, the coefﬁcients of Taylor series expansion ap, and the coefﬁcients of the linear ﬁlter gk are
not known in advance and, moreover, vary in time, they have to be realized adaptively.
The most prominent adaptive algorithm in the AEC context is given by the least mean square (LMS)
algorithm [10]. We seek to minimize the mean square of the error signal e(n) at the output of the AEC,
where
e(n) = y(n) −ˆd(n),
(30.93)
as shown in Figure 30.15. In the following we brieﬂy present the corresponding update equations
without further discussions. For more algorithmic details and rigorous derivations, the interested reader
is referred to [139,140].
Analogously to linear AEC, the update of the ﬁlter coefﬁcients gk using the LMS algorithm yields
gk(n + 1) = gk(n) + μg(n)e(n)xnl(n −k),
(30.94)

4.30.5 Nonlinear Modeling and Cancellation of Echo
855
where xnl(n) is the output of the memoryless nonlinearity according to (30.91). The corresponding
normalized LMS (NLMS) is obtained by normalizing the step-size parameter μg(n) according to
μg(n) =
αg(n)
Ng−1
k=0 x2
nl(n −k)
.
(30.95)
The normalized step-size parameter αg(n) is chosen according to 0 < αg(n) < 2 to assure stable
convergence [10]. To obtain robust adaptation in practice, the step-size parameter has to be controlled
to account for distortions such as background noise or double-talk situations [2].
The LMS-type adaptation of the Taylor series expansion is performed according to
ap(n + 1) = ap(n) + μap(n)e(n)u p(n),
(30.96)
where the auxiliary signal u p(n) is deﬁned as
u p(n) =
Ng−1

k=0
gkx p(n −k).
(30.97)
The normalization of the step-size parameter μap(n) is given analogously to μg(n) and obtained by
replacing xnl(n) with u p(n) in (30.95).
In order to increase the convergence speed of the coefﬁcients of the Taylor series expansion, the
authors of [140] perform their adaptation via RLS. Since the number of coefﬁcients P is generally small
(e.g., P ≤7), the increase in computational complexity compared to the LMS algorithm is rather small,
while the convergence speed is signiﬁcantly improved. A recursive Bayesian algorithm for coupled
estimation of ap and gk, the variational Bayesian state-space frequency-domain adaptive ﬁlter, was
then proposed as a further update featuring inherent step-size control [146].
It should be noted that miniaturized loudspeakers of mobile phones, when driven into saturation,
show similar behavior to overloaded ampliﬁers [141]. Thus, the above considerations analogously apply
in this case.
4.30.5.3 Power ﬁlters
The application of cascaded structures that match the model of the nonlinear acoustic echo path, as
discussed in the previous section, represents a straightforward and computationally efﬁcient approach
to nonlinear AEC. However, it is often challenging to assure convergence to the optimum solution or
even assure stable adaption behavior for adaptive cascaded structures. In this section, we, therefore,
consider so-called power ﬁlters as a practical parallelized model of the nonlinear echo path in case it
includes memoryless saturation characteristics.
The general structure of power ﬁlters is illustrated on the right hand side of Figure 30.16. Assuming
that the AEC in Figure 30.15 is realized as a Pth-order power ﬁlter, its input/output relation reads as
follows
ˆd(n) =
P

p=1
Np−1

k=0
h p,kx p(n −k).
(30.98)

856
CHAPTER 30 Acoustic Echo Control
From (30.98) we notice that power ﬁlters can be considered as linear multiple input/single output
systems, where the input of the pth channel is given by the pth power of x(n). The input of each channel
is then ﬁltered by an associated linear ﬁlter h p,k with memory length Np.
As already indicated in Figure 30.16, there is a close relation between the output of the cascaded
structure according to (30.92) and the corresponding power ﬁlter: Substituting the deﬁnition of xnl(n)
according to (30.91) into (30.92) gives
ˆd(n) =
P

p=1
Ng−1

k=0
apgkx p(n −k).
(30.99)
Comparing (30.98) and (30.99), the power ﬁlter model of the corresponding cascaded structure is
directly obtained by
h p,k = apgk.
(30.100)
It should be noted, that the number of parameters is increased from P + Ng for the cascaded structure
to PNg for the parallel structure. In practice, however, the increase in number of coefﬁcients is usually
much less, as the higher-order channels require less memory compared to the linear channel, i.e.,
Np < Ng for p > 1. The results reported in [141,143] indicate that power ﬁlters of order three already
achieve a remarkable increase in echo attenuation compared to linear approaches in case of both, an
overloaded ampliﬁer and the nonlinear loudspeaker of a mobile phone.
As already mentioned, power ﬁlters can be considered as linear multichannel systems. Thus, a
corresponding adaptive realization is straightforward. Here, we only present the LMS algorithm for the
update of the coefﬁcients of the power ﬁlter h p,k(n), i.e.,
h p,k(n + 1) = h p,k(n) + μp(n)e(n)x p(n −k).
(30.101)
In practice, a control of the step-size parameter μp(n), as well as an appropriate normalization, is
important to achieve a reasonable compromise between convergence speed and robustness against
distortions such as background noise and double-talk. In nonlinear echo cancellation, the adaptation
control additionally has to take into account the inﬂuence of nonlinear distortion. A corresponding
step-size control and normalization for adaptive power ﬁlters has been presented in [143,147].
Again referring to the multi-channel interpretation of power ﬁlters, we recall that the input signals
of the different channels, i.e., x(n), x2(n), . . . , x P(n) are in general correlated. This implies that the
convergence speed of a respective adaptive implementation is rather slow. In order to increase the
convergence speed of adaptive power ﬁlters, it has been proposed in [141,143] to use corresponding
orthogonalized structures instead. The new set of mutually orthogonal input signals for each channel
of the power ﬁlter is then given by
xo,p(n) = x p(n) +
p−1

i=1
qp,i xi(n),
(30.102)
for 1 < p ≤P, while the linear channel remains unchanged. The orthogonalization coefﬁcients qp,i
are chosen such that the cross-correlation between the input signals of different channels becomes zero.

4.30.5 Nonlinear Modeling and Cancellation of Echo
857
The orthogonalization coefﬁcients qp,i can be determined, e.g., by using the Gram-Schmidt orthogo-
nalization method. More details about time-variant orthogonalization for non-stationary input signals
such as speech are discussed in [141,143].
In linear adaptive ﬁltering, frequency-domain approaches are known to increase convergence speed
while even decreasing computational complexity. Due to their close relation to linear multichannel
ﬁltering, an efﬁcient implementation of adaptive power ﬁlters in the frequency domain is well possible,
as it has been discussed in [143,147]. This is achieved by performing the time-domain update Eq.
(30.101) as well as the computation of the output signal ˆd(n) (30.98) in the frequency domain. A
multichannel recursive Bayesian learning algorithm, the multichannel state-space frequency-domain
adaptive ﬁlter was recently proposed as a contained adaptive algorithm for the power-ﬁlter model at
hand [148]. It provides inherent stepsize control according to its underlying Kalman ﬁlter architecture
and has thus proven robustness for noisy and time-varying acoustic environments.
4.30.5.4 Second-order Volterra ﬁlters
For the case that the medium-sized loudspeaker of a hands-free telecommunication device represents
the main source for nonlinear distortion in the echo path, it has to be modeled by a nonlinearity with
memory. As already mentioned in Section 4.30.5.1, second-order Volterra ﬁlters represent a suitable
model for nonlinear loudspeakers which has already been applied in [142,143].
Assuming that the AEC is realized as a second-order Volterra ﬁlter, the AEC output ˆd(n) can be
expressed by the sum of the output of its linear kernel ˆd1(n), and the output of its quadratic kernel ˆd2(n),
i.e.,
ˆd(n) = ˆd1(n) + ˆd2(n),
(30.103)
where the input/output relation of the linear and the quadratic kernel, respectively, are given by
ˆd1(n) =
N1−1

k=0
hkx(n −k),
(30.104)
ˆd2(n) =
N2−1

k1=0
N2−1

k2=k1
hk1,k2x(n −k1)x(n −k2),
(30.105)
respectively.
Analogously to the previous sections, we now look at a simpliﬁed model for the nonlinear acoustic
echo path as shown in Figure 30.17. Assuming that the ampliﬁer of the loudspeaker is sufﬁciently linear,
the echo path can be modeled by the cascade of a second-order Volterra ﬁlter (hk,hk1,k2) representing the
nonlinear loudspeaker, followed by a linear ﬁlter, corresponding to the room impulse response gk. This
cascaded Volterra structure is illustrated in the center of Figure 30.17. It should be mentioned here, that in
[149] the authors propose to realize the nonlinear AEC analogously to such a cascaded Volterra structure.
On the one hand, this approach has a rather low computational complexity. However, on the other hand,
such adaptive implementations of cascaded systems with memory in general exhibit severe convergence
problems, making their application in echo cancellation inappropriate. In the following we thus consider
the structure on the right-hand side of Figure 30.17, which consists of a single second-order Volterra
ﬁlter (hk, hk1,k2) modeling the complete acoustic echo path, i.e., including the room propagation path.

858
CHAPTER 30 Acoustic Echo Control
gk
hk
hk1,k2
hk
hk1,k2
=⇒
=⇒
FIGURE 30.17
Illustration of the nonlinear acoustic echo path (left), its cascaded Volterra ﬁlter model (center), and the
corresponding overall model of a single Volterra ﬁlter (right).
As has been shown in [143,150], the corresponding quadratic Volterra kernel has speciﬁc properties,
namely a large part of the coefﬁcients of the quadratic kernel are known to be zero in advance.
In order to exploit the a priori knowledge about the kernel coefﬁcients for efﬁcient implementations,
it is useful to employ an alternative representation of the quadratic kernel. Following [150], we rewrite
(30.105) using the so-called diagonal coordinate representation (DCR):
ˆd2(n) =
R−1

r=0
N2−r−1

k=0
hk,r+kx(n −k)x(n −r −k).
(30.106)
Comparing (30.105) and (30.106), we notice that the above computation of ˆd2(n) can be interpreted as
the summation over R diagonals within the Cartesian coordinate system constructed by the summation
indices (k1, k2). Thereby, the main diagonal corresponds to r = 0. Obviously, in case of R = N2,
(30.105) and (30.106) are equivalent.
Referring to the cascaded structure shown in the center of Figure 30.17, we now assume that the
leading Volterra ﬁlter has a memory length of L for both, linear and quadratic kernel. Furthermore, we
assume that the linear ﬁlter gk has length Ng. As shown in [150], the resulting overall Volterra ﬁlter
has memory lengths N1 = N2 = L + Ng −1. However, the width R of the corresponding DCR of the
quadratic kernel remains unchanged, i.e., R = L. Considering that L represents the memory effects of
the loudspeaker and Ng corresponds to the reverberation time of the room, it becomes obvious that in
typical applications R ≪N2. As can be seen from (30.106), this special property can easily be taken
into account when using the DCR of Volterra ﬁlters.

4.30.6 Application to Realistic and Real Systems
859
Another interesting property of Volterra ﬁlters can be found when introducing the virtual input signal
xr(n) = x(n)x(n −r) of the rth diagonal into (30.106)
ˆd2(n) =
R−1

r=0
N2−r−1

k=0
hk,r+kxr(n −k).
(30.107)
As can be seen, the inner summation represents a linear convolution between the kernel coefﬁcients
on the rth diagonal with the input signal xr(n). Thus, quadratic Volterra kernels can be considered as
a special type of linear multichannel systems, where each diagonal of the DCR corresponds to one
channel with input xr(n). Regarding this, algorithms known from linear adaptive ﬁltering can easily
be extended to adaptive second-order Volterra ﬁlters. To give an example, the update equation for the
coefﬁcients of the quadratic kernel using the LMS algorithm is given by
hk,r+k(n + 1) = hk,r+k(n) + μk,r+k(n)e(n)xr(n −k).
(30.108)
A detailed discussion of suitable methods for the normalization and control of the step-size parameter
μk,r+k(n) can be found in [143].
Due to the close relation between Volterra ﬁlters and linear multichannel systems, the derivation of
corresponding efﬁcient frequency-domain realizations is straightforward. For instance in [143,150], it
has been proposed to perform the linear ﬁltering required for each diagonal in (30.106) by using fast
block convolution techniques in the frequency domain. Additionally, the update of the kernel coefﬁcients
hk,r+k(n) can also be performed in the frequency domain. It turns out, that the beneﬁts of frequency-
domain approaches as known from linear adaptive ﬁltering also transfer to adaptive Volterra ﬁlters.
Apart from the described potential of nonlinear modeling and identiﬁcation using Volterra ﬁlters, the
huge computational complexity and slow convergence related to the large number of parameters have
been clearly recognized as limitations regarding the usability. As a result, signiﬁcant research has been
devoted recently to the design of fast and robust algorithms using iterated coefﬁcient update [151] and
to complexity reduction via dynamical adjustment of the kernel memory [152].
4.30.6 Application to realistic and real systems
In this section, we describe various acoustic environments with different conﬁguration regarding the
hands-free communication application. In particular, we consider the car, the desktop PC, the living
room, and the mobile phone environment. Essentially, these environments exhibit individual degrees of
environmental noise, length and time-variability of the acoustic echo path, and nonlinearities such
as sampling asynchrony or loudspeaker saturation. As a consequence, different signal processing
approaches have been used by researchers to tackle the acoustic echo control problem in the differ-
ent environments. In the following, the results that have been achieved are outlined along with main
properties of the respective environment.
4.30.6.1 Car environment
Due to the relatively small size acoustic environment of the car interior, we have a relatively short echo
path impulse response of only 30–100 ms duration in most of the cases. However, the natural presence

860
CHAPTER 30 Acoustic Echo Control
and interaction of the user in the environment will cause the echo path impulse response to exhibit
relatively strong variability, which practically means a lack of identiﬁability if at the same time the
natural presence of the car noise is considered. Regarding the quality of electro-acoustic transducers, at
least in the high-end product range, we may assume only a minor degree of nonlinearity of the system.
As a result, the linear state-space echo path model and the respective model-based optimum ﬁltering
approaches as described in Section 4.30.2 were found to best address this environment. In order to reach
out for the limits, here, we evaluate the advanced implementation of the frequency-domain adaptive
Kalman ﬁlter as proposed in [27].
In order to allow for reproducibility of the presented results, while maintaining strong relationship
with the real-world situation, we make use of a time-varying echo path that is generated directly by
the Markov model in (30.34). The variability is chosen such that the echo attenuation of a perfectly
adjusted echo canceler would drop to about 0 dB within 2–3 s after the adaptation of the ﬁlter is halted.
The echo path vector h(n) contains 512 coefﬁcients, which corresponds to 64 ms echo path duration
at 8 kHz sampling frequency. To setup the test signals for the adaptive algorithm, we use real speech
input on both the far-end and near-end side of the communication. The employed speech material
consists of 8 phonetically balanced sentences (male and female) of about 5 s duration each [153]. We
then consider a wide range of signal-to-echo ratios SERy = σ 2
s /σ 2
d at the hands-free microphone. The
SERy = 0 dB simulates a hard double talk situation, SERy = −40 dB corresponds to remote single
talk, and SERy = 40 dB ﬁnally represents near-end single talk. The background noise level at the
hands-free microphone is adjusted such that the signal-to-noise ratio of the near-end speech is 10 dB,
while the received signal from the far-end speaker is almost clean speech with a signal-to-noise ratio of
40 dB—a situation that often exists in car hands-free communication.
The echo attenuation after echo canceler and postﬁlter, cf. (30.28) and (30.29), can be
evaluated in terms of the echo return loss enhancements ERLEw1 = σ 2
d /σ 2
b and ERLEw12 = σ 2
d /σ 2
b′.
Here, b = d −ˆd refers to the residual echo after echo cancellation and b′ = w2∗b represents the total
echo attenuation after both ﬁlters, e.g.,[2,58]. The resulting speech quality is evaluated by means of
the resulting signal-to-echo ratio SERe = σ 2
s /σ 2
s−e after the echo canceler and SERˆs = σ 2
s /σ 2
s−ˆs at
the system output, i.e., after the postﬁlter. The ERLE and SER measures described here are suitable to
characterize the overall performance of echo canceler and postﬁlter, including the adaptive algorithm
with its tracking performance and robustness against observation noise.
When echo canceler and postﬁlter and the adaptive algorithm (i.e., the frequency-domain adaptive
Kalman ﬁlter) are implemented with a block frame-shift (i.e., algorithmic delay) of 8 ms and a DFT
size of 512 (corresponding to 64 ms echo path impulse response length), and when the time-constant
of the Kalman ﬁlter is matched to the dynamical echo path model, we obtain the results in Figure
30.18. The ERLEw1 by the echo canceler ranges from 0 to 20 dB, depending on the input SERy.
The saturation of ERLEw1 at low SERy is due to the time-varying echo path and the fact that the
echo canceler for time n is determined by the “incomplete” data available up to time n −1. For
high SERy, ERLEw1 asymptotically reaches zero, since extremely noisy observations do not allow
the identiﬁcation of the time-varying echo path at all. The total echo attenuation ERLEw12 by echo
canceler and postﬁlter ranges from 0 to 50 dB. This performance matches the industrial requirements
for acoustic echo controllers: More than 40 dB ERLE is indeed recommended during remote single talk
[78,84]; in noisy double talk situations our experience is that 15–20 dB ERLE is sufﬁcient to achieve

4.30.6 Application to Realistic and Real Systems
861
50
40
40
30
30
20
20
10
10
0
0
-40
-30
-20
-10
-10
ERLEw1 by the echo canceler
SERs after echo canceler and postﬁlter
SERe = SERy + ERLEw1
ERLEw12 by echo canceler and postﬁlter
Input SERy [dB]
[dB]
FIGURE 30.18
ERLE and output SER for different input SER.
the required end-user quality; and during near-end single talk an echo attenuation is of course not
required.
The speech quality improvement by the echo canceler can then be expressed analytically: SERe =
SERy +ERLEw1, thus SERe > SERy. The situation is not so straightforward in case of the postﬁlter, but
from Figure 30.18 we observe another consistent improvement in the output SER, i.e., SERˆs > SERe.
For very low input SERy, a surprisingly high output SERˆs ≈0 dB is attained, simply because the entire
microphone signal is strongly attenuated. For high input SERy, the output SERˆs approaches the input
SERy since the microphone signal remains nearly unprocessed. For SERy = 0 dB, i.e., during double
talk, we have SERˆs ≈14 dB. However, together with the effect of perceptual masking, the perceptual
signal quality (“the perceived SER”) is much better than SERˆs ≈14 dB.
4.30.6.2 Desktop conferencing
Based on computers connected to the Internet, a widespread hands-free telecommunication application
is desktop conferencing. To set up a desktop conference call, one only needs a computer connected to the
Internet, a loudspeaker, a microphone (and potentially a camera for display) as shown in Figure30.19.
In order to allow hands-free calls, the computer requires a software carrying out echo control. Some

862
CHAPTER 30 Acoustic Echo Control
computer
input sound card
output sound card
modem
to the
internet
FIGURE 30.19
A general desktop conferencing environment.
operating systems have the required software application already pre-installed. Otherwise, users can
easily download from the Internet any available conferencing software.
The variety of computers and hardwares (loudspeakers, microphones, sound cards, etc.) makes
the design of the echo control software a challenging task. Indeed, the echo control software has to
work despite the various possible computer architectures and operating systems, speaker sizes and
efﬁciencies, microphones types and sensitivities, etc., and even further, the various and uncontrollable
user environments. In order to cope with these numerous unknowns, a robust acoustic echo suppression
(AES) system as described in Section 4.30.3 can be used. It offers a viable solution, since it does not
require an exact identiﬁcation of the impulse response hn, but models parametrically the echo path with
a delay τ and a single real-valued gain 
G(m, ν) at each frequency bin of short-time spectra as shown
by Eq. (30.59). Therefore, AES yields robust insurance against movements of the microphone or other
changes in the acoustic environment.
Another important feature of the AES is that the phase information of the signal spectra is discarded in
the algorithm, making the AES performance independent of any phase changes or distortions introduced
by the components in the acoustic echo path. In the speciﬁc desktop conferencing environment, the two
most common distortions are:
•
Sampling rate mismatch: Leading to time drift, which typically arises when the loudspeaker signal
and the microphone signal are captured using different soundcards or A/D converters.
•
Random loss of audio samples or frames of samples due to transmission over the IP network or due
to drop outs during playback.
As already illustrated in Figure 30.6, the described AES implementation is robust against such common
issues. The estimate of the echo estimation ﬁlter (EEF) function 
G(m, ν) is computed directly from
power spectra with temporal ﬂuctuations instead from complex spectra, which not only makes the
estimate insensitive towards phase distortions, but also makes it independent to the background noise
on the near-end side as seen from Eq. (30.71).
Eventually, the concept of AES, based on a spectral subtraction of the echo estimate from the
microphone spectrum, enables to compute “aggressively” the ﬁnal echo suppression ﬁlter (ESF) F(m, ν)
such that no residual echo remains. This can be achieved by choosing a long reverberation time constant
αRT inEq.(30.72)tomatchtheroomsizeandsuppressthelateechoes,orbyintentionallyover-estimating
the echo power spectrum with a large β parameter in Eq. (30.74). Because the ESF can be adjusted to

4.30.6 Application to Realistic and Real Systems
863
perform more aggressive echo suppression, independent from the estimation of the EEF, a tuning point
can be found where the AES also provides a certain insensitivity against non-linear behavior of the echo
path.
In summary, for the speciﬁc desktop environment, AES provides a ﬂexible approach to perform
echo control. It is a practical solution to cope with the variety of possible hardwares and related system
uncertainties.
4.30.6.3 Living room
In order to demonstrate the potential of multichannel acoustic echo cancellation (MCAEC), as described
in Section 4.30.4, in applications such as home theater, virtual reality, or advanced teleconferencing,
we chose the acoustic environment of a typical living room and consider a surround sound scenario
with P = 5 reproduction channels. The sampling rate of the loudspeaker signals and the preprocessing
stage is 44.1 kHz, while the microphone signal and the echo cancellation is downsampled by a factor
of 4 as typical for speech recognition applications. The length of the echo cancellation ﬁlters were set
to N = 1024, covering the reverberation time in the receiving room. Our evaluation mostly relies on
the MCFDAF algorithm [26], which aims to exploit all cross-correlations between the reproduction
channels. Besides the iterative processing in time, our implementation performs 10 ofﬂine iterations
within each block of samples according to [55,56]. In all simulations, the echo-to-background noise
ratio in the receiving room was set to 30 dB and the regularization of the MCFDAF algorithm was
adjusted so that stability is provided for all preprocessing methods under investigation.
At ﬁrst, we discuss the convergence of the adaptive ﬁlter coefﬁcients to the true echo path coefﬁ-
cients in terms of the multichannel coefﬁcient error norm P
i=1∥hi −ˆhi(n)∥2/P
i=1∥hi∥2 over time
with different preprocessing methods. We chose a somewhat critical test scenario of reproducing a nar-
rowband high-quality male speech signal with alternating spatial positions in the transmission room (see
Figure 30.11). In order to reﬂect the surround sound scenario and the inherent level imbalance problem
in MCAEC appropriately, it is important to choose a realistic recording scenario, ours being inspired
by the so-called Decca Tree and surround microphones [154]. Figure30.20 then shows the correspond-
ing coefﬁcient convergence for baseline approaches without any preprocessing (curve label “without
preproc.”) and with conventional nonlinear preprocessing after [115] (nonlinearity parameter α = 0.5,
label “NL”), as well as for the perceptually tuned frequency selective phase modulation method [120]
(label “Pmod_fs”) and the addition of uncorrelated audio coding noise after [128] (labeled “mp3_48”).
As it can be seen from the data, convergence without any preprocessing is extremely slow, while pre-
processing results in a signiﬁcant convergence boost. The parameters of all preprocessing methods
considered here were chosen such that they yield similar convergence characteristics in order to provide
a common basis for subjective listening tests, such as the MUSHRA in Section 4.30.4.3.
Secondly, by choosing the phase modulation method as a ﬁxed preprocessor, we illustrate the effect
of taking into account the cross-correlations between the loudspeaker channels in the AEC coefﬁcient
update (see also Figure 30.12 in Section 4.30.4.2). We again apply the MCFDAF algorithm for P = 5
loudspeaker channels with the same parameters and the same data as above and then draw the com-
parison with a standard FDAF algorithm in each and every channel, speciﬁcally, the unconstrained
fast least mean-square (UFLMS) algorithm. The results in Figure 30.21 clearly conﬁrm the signiﬁ-
cant convergence improvement regarding ERLE and coefﬁcient error norm by taking into account the
cross-correlations into the adaptation process.

864
CHAPTER 30 Acoustic Echo Control
0
5
10
15
20
25
30
35
-20
-15
-10
-5
0
Time [s]
Coefficient error norm [dB]
position alterations
without preproc.
Pmod_fs
mp3_48
NL
FIGURE 30.20
Comparison of MCAEC processing methods, P = 5.
0
5
10
15
20
25
30
0
10
20
30
40
Time [s]
ERLE [dB]
0
5
10
15
20
25
30
35
−20
−15
−10
−5
0
Time [s]
Coefficient error norm [dB]
classical UFLMS
classical UFLMS
MCFDAF exploiting cross−correlations
MCFDAF exploiting cross−correlations
FIGURE 30.21
Effect of taking cross-correlations into account, P = 5 channels. (a) ERLE convergence, (b) coefﬁcient error
norm.

4.30.6 Application to Realistic and Real Systems
865
4.30.6.4 Mobile phones
In Section 4.30.5, we discussed nonlinear echo path models and adaptive ﬁlter structures which require
only little a priori knowledge about the audio hardware employed in telecommunication devices. If
moderately-sized loudspeakers represent the only source of nonlinear distortion, then second-order
Volterra ﬁlters are generally recommended to model their frequency-dependent nonlinear behavior. In
case of memoryless nonlinearities included in the echo path, as common with nonlinear ampliﬁers
or miniaturized loudspeakers of mobile phones, the nonlinear cascaded structures including truncated
Taylor series expansions or, alternatively, power ﬁlters are better suited. In this section, we explore
the suitability of all these approximations when modeling the real acoustic echo path comprising the
miniature electro-dynamic loudspeaker of a mobile phone.
For recordings of audio signals, the loudspeaker has been mounted in the handset, while the micro-
phone has been separated from it to avoid undesired vibration effects due to physical coupling of the
loudspeaker and the microphone. During the measurements it has been assured that there is no nonlinear
distortion introduced by overloading of the ampliﬁer, i.e., the nonlinearity in the acoustic echo path is
mainly caused by the miniature loudspeaker. In order to focus on the nonlinear behavior of the setup,
the recordings have taken place in a room with low reverberation. The input signal has been wide-sense
stationary correlated Gaussian noise, which has been generated by passing a white Gaussian noise signal
through a second-order recursive ﬁlter.
First, we evaluate the suitability of the different nonlinear structures for modeling the nonlinear
behavior of the loudspeaker. The behavior is examined by applying three different input levels to ﬁve
different adaptive structures. The considered structures are: A linear ﬁlter, a third- and ﬁfth-order orthog-
onalized power ﬁlter, and a second- and third-order Volterra ﬁlter in DCR, respectively. All approaches
have been implemented in the DFT domain to improve the convergence properties for correlated input.
Since the input signal used for the measurements is known in advance, a ﬁxed orthogonalization of the
channel inputs can be used for the power ﬁlters. The memory length of the linear ﬁlter and the linear
channel of the nonlinear approaches has been N1 = 300 taps, which sufﬁciently models the linear
component of the echo path. The ﬁlters associated with the nonlinear channels of both, third-order
and ﬁfth-order power ﬁlter have been implemented with a length of Np = 100 taps. Accordingly, the
memory lengths of the nonlinear kernels of second- and third-order Volterra ﬁlters have been set to
N2 = N3 = 100. Accounting for the cascaded structure of the acoustic echo path, the widths of the
quadratic and cubical kernels have been reduced to R2 = R3 = 10.
The evaluation of the different approaches is based on the maximum ERLE that is achieved after
convergence of the echo canceler. The resulting ﬁnal ERLE values obtained for different input variances
are summarized in Table 30.1. The ﬁrst column corresponds to the case where there is only a low level
of nonlinear distortion in the echo path. This is reﬂected by the fact that the linear adaptive ﬁlter
shows approximately the same performance as the nonlinear counterparts. Thereby, we notice that the
achievableERLEvaluesaregenerallynotsolarge.Thiscanbeexplainedbythefactthattherelativelylow
sound level of the miniature loudspeaker allows not more than 30 dB SNR at the recording microphone.
In the ﬁrst row of the table, the ERLE obtained for the linear adaptive ﬁlter is reduced by approximately
5–6 dB when the input signal level is increased to 5.4σ 2
m,x and 9σ 2
m,x, respectively. This conﬁrms the
nonlinear behavior of the loudspeaker for high excitation levels.

866
CHAPTER 30 Acoustic Echo Control
Table 30.1 Achievable ERLE of Several Adaptive Structures Obtained for Different Input Levels.
The Nonlinear Distortion is Introduced by the Loudspeaker of a Mobile Phone
Variance of the input
σ 2m,x (dB)
5.4σ 2m,x (dB)
9σ 2m,x (dB)
Linear ﬁlter
27.2
22.3
21.1
Third-order power ﬁlter
28.4
25.4
24.4
Fifth-order power ﬁlter
28.3
25.4
24.5
Second-order Volterra ﬁlter
26.9
22.2
22.1
Third-order Volterra ﬁlter
25.9
25.6
25.4
The second-order Volterra ﬁlter does not yield noticeable improvements compared to the linear
ﬁlter. This indicates that the memoryless model for the miniaturized loudspeakers of the mobile phone
is sufﬁcient. By extending the second-order Volterra ﬁlter to a cubical kernel, the echo attenuation
of the linear ﬁlter is surpassed by approximately 3–4 dB for the input variances 5.4σ 2
m,x and 9σ 2
m,x,
respectively. This shows that the nonlinearity of the loudspeaker is at least of third order. Note that
the considered third-order kernel requires 5170 coefﬁcients. Additional simulations have shown that
the memory length of the nonlinear kernels can be reduced to N2 = N3 = 64 without changing the
maximum achievable ERLE. However, this reduction of the region of support of the third-order Volterra
kernel still requires 4070 coefﬁcients. This large number of coefﬁcients and the related difﬁculty of
accurate adaptive identiﬁcation also explains the performance loss of the third-order Volterra ﬁlter that
is observed for the lowest input level σ 2
m,x. Regarding that the ERLE gain is at most 4.3 dB for the
highest input variance, there is no reasonable relation between performance improvement and increase
in computational complexity when applying third-order Volterra ﬁlters instead of linear ﬁlters.
When the region of support of the third-order Volterra ﬁlter only includes the main diagonals of
each kernel, it is simpliﬁed to a third-order power ﬁlter. As can be seen from Table 30.1, this enormous
reduction of the region of support barely affects the achievable echo attenuation. This result again
supports the assumption that the miniature loudspeaker can be considered as a memoryless nonlinearity.
One might expect that increasing the order of the power ﬁlter, and thus its nonlinear modeling ability,
should then lead to yet more echo attenuation. Unfortunately, an extension of the power ﬁlter to ﬁfth
order does not yield further improvements over the third-order case in our practical experiments. Our
understanding is that the misadjustment of the linear and the cubical channels inhibit the convergence
of channels with yet higher orders—in conjunction with the fact that higher order channels of our EOS
(equivalent orthogonal structure) are hardly excited.
From the results presented in Table 30.1 we conclude that the modeling capabilities of the considered
polynomial ﬁlters are not completely satisfying. From a practical point of view, the best compromise with
respect to achievable echo attenuation and computational complexity is provided by the orthogonalized
third-order power ﬁlter. This conﬁguration is therefore used in the following experiment, where we look
at the performance of the adaptive EOS of a third-order power ﬁlter with real speech input.
Except for the speech input, the experimental setup now is the same as before.The variance of the
speech signal has been adjusted such that its amplitude values lie in the same range as the amplitudes
of typical sample functions of the correlated noise signal with variance 9σ 2
m,x as used above. A white

4.30.7 Links to Codes and Recommendations
867
0
1
2
3
4
5
6
7
8
−5
0
5
10
15
20
25
30
time [s]
ERLE [dB]
power ﬁlter
linear ﬁlter
FIGURE 30.22
ERLE obtained for the adaptive EOS of a third-order power ﬁlter and a corresponding linear approach together
with the speech input.
Gaussian noise signal has been added to the recording of the microphone signal in order to simulate a
background noise level corresponding to an SNR of 30 dB with respect to the acoustic echo. Since an
algorithmic delay is not desirable in mobile phones, we now consider the time-domain implementation
of the EOS, where the memory length N1 = 256 for the linear channel and N2 = N3 = 100 for
both, the quadratic and cubical channel has been chosen. The orthogonalization of the channel inputs
has been performed signal-adaptively, where the required moments are estimated recursively with a
time-constant of about 10 ms.
In Figure 30.22, the echo cancellation performance of the adaptive EOS of the third-order power
ﬁlter is compared to a linear approach which corresponds to the linear channel of the power ﬁlter. As
can be noticed, the performance of the linear adaptive ﬁlter is remarkably limited due to the nonlinear
distortion introduced by the loudspeaker. The third-order power ﬁlter succeeds in improving the level of
echo attenuation during almost the whole simulation period. Especially for speech segments that exhibit
high excitation levels, the local increase of the ERLE even exceeds the expectations which would have
been predicted from Table 30.1. While this ERLE gain is observed, note that due to the short ﬁlters in
the nonlinear channels, the computational complexity of the considered orthogonalized power ﬁlter is
only two times higher than that of the linear ﬁlter.
4.30.7 Links to codes and recommendations
Echo control solutions have been developed over years and used in many telecommunication appli-
cations. Therefore the portfolio of existing solutions is large and includes various implementations as
the ones described in Sections 4.30.1–4.30.5 , or all possible combinations to match the application

868
CHAPTER 30 Acoustic Echo Control
speciﬁcations and requirements. Most of the solutions are thus speciﬁc and proprietary meaning that
available free implementations or data sets are rare and not designed to address diverse applications.
Prominent examples of free implementations are:
•
AnacousticechocancelerwithpostﬁlteringispartoftheSpeexspeechcodec(http://www.speex.org).
•
The OSLEC line echo canceler (http://www.rowetel.com/ucasterisk/oslec.html).
While implementations of echo control solutions are often proprietary, common knowledge is
listed in standards and recommendations referenced by applications and ﬁelds of use. The Inter-
national Telecommunication Union (ITU) Telecommunication Standardization Sector (ITU-T at
http://www.itu.int/ITU-T) deﬁnes for a wide range of possible telecommunication appli-
cations a number of recommendations and standards for related echo control solution implementations.
For instance:
•
ITU-T G.131:
Talker Echo and its Control [1].
•
ITU-T G.164:
Echo suppressors [75].
•
ITU-T G.165:
Echo cancelers [76].
•
ITU-T P.832:
Subjective performance evaluation of hands-free terminals [81].
The international recommendations produced by the ITU-T can become mandatory once they are
adopted as part of a national law to regulate telecommunication applications. Many others standardiza-
tion organizations have been created to deﬁne speciﬁcations for echo control solutions in speciﬁc ﬁelds
of use. For examples the 3rd Generation Partnership Project (3GPP at http://www.3gpp.org/) or the
German Automobile Industry (VDA at http://www.vda.de/en/index.html), [84], are two organizations
writing standards for mobile networks and car applications, respectively.
4.30.8 Conclusions, open issues, future trends
Acoustic echo control for hands-free communication has been actively researched in the area of signal
processing since the 1970s. Here, we ﬁrst reviewed the most popular solutions based on adaptive
algorithms according to deterministic least-squares design, with realizations in time- or frequency-
domain, and combined with various possible control strategies.
Based on this brief status, our chapter then mainly reported the comprehensive extensions of the
state-of-the-art according to research work beyond 2000. This includes statistical methods for adaptive
algorithm design, e.g., the uniﬁcation of adaptive ﬁltering and adaptation control based on statistical echo
path modeling and Bayesian estimation, the echo suppression technique based on power spectral echo
path modeling, and the TRINICON framework to incorporate statistical signal properties. Furthermore,
we highlighted the particular issues of multichannel and nonlinear adaptive systems and the respective
developments.
Those new directions in adaptive systems research were triggered by the common need for fast
and robust solutions in real-world applications in which we often face a lot of uncertainty regard-
ing the electroacoustic environment and the speciﬁc usage of hands-free systems. On these common
grounds, however, the new technologies have been conceived and pursued somewhat independently by
different researchers in different applications and in different organizations. In this chapter, we have
sought a presentation with uniﬁed notation, but there remains a lot of work towards a uniﬁed and fully

4.30.8 Conclusions, Open Issues, Future Trends
869
comprehensive theory. Nonetheless, our chapter has proven the usability of the presented algorithms in
practical single-channel applications, such as desktop and car environments, while the perspective for
the realization of systems with multiple reproduction channels and nonlinear characteristics has been
demonstrated in the research environment.
Acoustic echo control continuous as a research topic to serve as the enabling technology in modern
conﬁgurations of hands-free communication with full duplex ability. Future trends include a shift of
the acoustic echo control unit from mobile devices into the core of a cellular network in order to save
processing power on the mobile device [155,156], the development of multichannel echo cancellation
frameworksforsystemswithmassivemultichannelreproductionofspatialaudio[157],andtheevolution
of nonlinear adaptive signal processing beyond the established polynomial modeling [158]. In all the
cases mentioned here, the derivation of fast and robust adaptive algorithms for the respective structure
and system model at hand represents an interesting topic for future research activities.
Glossary
Acoustic echo control
the term generalizes the acoustic echo cancellation to further include
echo suppression and postﬁltering
Double talk
a situation in which the talkers at both ends of a communication system
(or reproduction unit and user of a speech dialog system) are active
simultaneously; echo and target input signal are thus superimposed and
recorded together at the microphone
Duplex ability
it describes to which degree the simultaneous transmission in reproduc-
tion and acquisition direction is preserved by a hands-free system, despite
possible attenuations by signal processing
Echo path
this terms describes the undesirable electroacoustic coupling between
loudspeaker input and microphone output of a hands-free communica-
tion system; most of the times, we describe the echo path in terms of
an acoustic impulse response or frequency response comprising loud-
speaker unit, acoustic coupling, and microphone
Echo cancellation
refers to a family of techniques, where the echo path is mimicked by
an (adaptive) digital ﬁlter in order to regenerate and ideally subtract the
echo from the observed microphone signal
Echo suppression
incontrasttoechocancellation,thisreferstoafamilyoftechniqueswhich
discard (i.e., not mimic) the phase of the echo path; the echo suppression
is then performed in the form of statistical echo reduction based on the
echo power transfer function
Multichannel echo
cancellation
most of the times, this term refers to the problem of echo cancellation
for multiple reproduction channels, e.g., stereophonic echo cancellation
Nonlinear echo cancellation most of the times, this term refers to the problem of echo cancellation
in the presence of a nonlinear power ampliﬁer or nonlinear loudspeaker
within the echo path

870
CHAPTER 30 Acoustic Echo Control
Postﬁltering
it describes echo suppression techniques when they are employed in
conjunction with echo cancellation
Power ﬁlter
a multi-input/single-output, adaptive echo cancellation ﬁlter structure
with higher-order polynomial representations of the reproduction signal
at the multiple inputs; represents a quasi-linear expansion of echo paths
with memoryless nonlinearity
Relevant Theory: Signal Processing Theory
See Volume 1, Chapter 3 Discrete-Time Signals and Systems
See Volume 1, Chapter 4 Random Signals and Stochastic Processes
See Volume 1, Chapter 6 Digital Filter Structures and Their Implementation
See Volume 1, Chapter 7 Multirate Signal Processing for Software Radio Architectures
See Volume 1, Chapter 9 Discrete Multi-Scale Transforms in Signal Processing
See Volume 1, Chapter 12 Adaptive Filters
References
[1] ITU-T Rec. G.131, Talker echo and its control, November 2003.
[2] E. Hänsler, G. Schmidt, Acoustic Echo and Noise Control: A Practical Approach, Wiley, 2004.
[3] M. Sondhi, An adaptive echo canceller, Bell Syst. Tech. J. XLVI (3) (1967) 497–511.
[4] M. Sondhi, W. Kellermann, Echo cancellation for speech signals, in: S. Furui, M. Sondhi (Eds.), Advances
in Speech Signal Processing, Marcel Dekker, New York/Basel/Hong Kong, 1992, pp. 327–356.
[5] A. Liavas, P. Regalia, Acoustic echo cancellation: do IIR models offer better modeling capabilities than their
FIR counterparts, IEEE Trans. Signal Process. 46 (9) (1998) 2499–2504.
[6] C. Breining, P. Dreiseitel, E. Hänsler, A. Mader, B. Nitsch, H. Puder, T. Schertler, G. Schmidt, J. Tilp,
Acoustic echo control, an application of very-high-order adaptive ﬁlters, IEEE Signal Process. Mag. (1999)
42–69.
[7] S.L. Gay, J. Benesty (Eds.), Acoustic Signal Processing for Telecommunications, Kluwer Academic Pub-
lishers, 2000.
[8] E. Hänsler, The hands-free telephone problem – an annotated bibliography, Signal Process. 27(3) (1992)
259–271.
[9] E. Hänsler, The hands-free telephone problem: an annotated bibliography update, Ann. Télécommun. 49
(7–8) (1994) 360–367.
[10] S. Haykin, Adaptive Filter Theory, fourth ed., Prentice-Hall, Upper Saddle River, NJ, 2002.
[11] C. Antweiler, Orthogonalisierende Algorithmen für die digitale Kompensation akustischer Echos, PhD thesis,
RWTH Aachen, in: Peter Vary (Ed.), Aachener Beiträge zu digitalen Nachrichtensystemen, Band 1, Verlag
der Augustinus Buchhandlung, Aachen, February 1995.
[12] S. Yamamoto, S. Kitayama, J. Tamura, H. Ishigami, An adaptive echo canceller with linear predictors, Trans.
IECE Jpn. E62 (12) (1979) 851–857.
[13] K. Ozeki, T. Umeda, An adaptive ﬁltering algorithm using an orthogonal projection to an afﬁne subspace
and its properties, Electron. Commun. Jpn. 67-A (1984) 19–27.
[14] D. Morgan, S. Kratzer, On a class of computationally efﬁcient rapidly converging, generalized NLMS
algorithms, IEEE Signal Process. Lett. 3 (1996) 245–247.

References
871
[15] S. Gay, The fast afﬁne projection algorithm, in: S. Gay, J. Benesty (Eds.), Acoustic Signal Processing for
Telecommunications, Kluwer Academic Publishers, 2000, pp. 23–45.
[16] D. Slock, T. Kailath, Numerically stable fast transversal ﬁlters for recursive least-squares adaptive ﬁltering,
IEEE Trans. Signal Process. 39 (1991) 92–114.
[17] J. Ciofﬁ, T. Kailath, Fast, recursive-least-squares transversal ﬁlters for adaptive ﬁltering, IEEE Trans. Acoust.
Speech Signal Process. 34 (1984) 304–337.
[18] D. Falconer, L. Ljung, Application of fast Kalman estimation to adaptive equalization, IEEE Trans. Commun.
26 (1978) 1439–1446.
[19] E. Ferrara, Frequency-domain adaptive ﬁltering, in: C. Cowan, P. Grant (Eds.), Adaptive Filters, Prentice
Hall, Englewood Cliffs, NJ, 1985, pp. 145–179.
[20] W. Kellermann, Analysis and design of multirate systems for cancellation of acoustical echoes, in: Proceed-
ings of International Conference on Acoustics, Speech, and Signal Processing (ICASSP), New York, April
1988, pp. 2570–2573.
[21] J. Shynk, Frequency-domain and multirate adaptive ﬁltering, IEEE Signal Process. Mag. 9 (1) (1992) 14–37.
[22] E. Moulines, O. Amrane, Y. Grenier, The generalized multidelay adaptive ﬁlter: structure and convergence
analysis, IEEE Trans. Signal Process. 43 (1) (1995) 14–28.
[23] P. Sommen, Adaptive Filtering Methods: On Methods to Use a Priori Information in Order to Reduce
Complexity While Maintaining Convergence Properties, PhD thesis, Technical University of Eindhoven,
1992, ISBN 90-9005143-0.
[24] J.-S. Soo, K. Pang, Multidelay block frequency domain adaptive ﬁlter, IEEE Trans. Acoust. Speech Signal
Process. 38 (1990) 373–376.
[25] G. Enzner, P. Vary, A soft-partitioned frequency-domain adaptive ﬁlter for acoustic echo cancellation, in:
Proceedings of International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Hong Kong
(China), May 2003, pp. 393–396.
[26] H. Buchner, J. Benesty, W. Kellermann, Generalized multichannel frequency-domain adaptive ﬁltering:
efﬁcient realization and application to hands-free speech communication, Signal Process. 85 (3) (2005)
549–570.
[27] G. Enzner, P. Vary, Frequency-domain adaptive Kalman ﬁlter for acoustic echo control in hands-free tele-
phones, Signal Process. 86 (6) (2006) 1140–1156.
[28] P. Vary, R. Martin, Digital Speech Transmission – Enhancement, Coding, and Error Concealment, John
Wiley & Sons, Ltd., Chichester, England, 2006.
[29] R. Merched, P.S. Diniz, M.R. Petraglia, A new delayless subband adaptive ﬁlter structure, IEEE Trans. Signal
Process. 47 (6) (1999) 1580–1591.
[30] D.R. Morgan, J.C. Thi, A delayless subband adaptive ﬁlter architecture, IEEE Trans. Signal Process. 43 (8)
(1995) 1819–1830.
[31] D. Duttweiler, Proportionate normalized least-mean-squares adaptation in echo cancelers, IEEE Trans.
Speech Audio Process. 8 (2000) 508–518.
[32] J. Benesty, S. Gay, An improved PNLMS algorithm, in: Proceedings of International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP), Orlando, FL, March 2002.
[33] A. Khong, P. Naylor, J. Benesty, A low delay and fast converging improved proportionate algorithm for
sparse system identiﬁcation, EURASIP J. Audio Speech Music Process. (2007).
[34] R. Tibshirani, Regression shrinkage and selection via the Lasso, J. Roy. Statist. Soc. Ser. B (Methodol.) 58
(1) (1996) 267–288.
[35] J. Benesty, T. Gänsler, D. Morgan, M. Sondhi, S. Gay, Advances in Network and Acoustic Echo Cancellation,
Springer, 2001.
[36] R. Frenzel, Freisprechen in gestörter Umgebung, PhD thesis, Technical University of Darmstadt, Fortschritt-
Berichte VDI, Reihe 10, Nr. 228, VDI Verlag, Düsseldorf, 1992.

872
CHAPTER 30 Acoustic Echo Control
[37] A. Mader, H. Puder, G. Schmidt, Step-size control for acoustic echo cancellation ﬁlters – an overview, Signal
Process. 80 (9) (2000) 1697–1719.
[38] P. Meissner, R. Wehrmann, J. van der List, A comparative analysis of Kalman and gradient methods for
adaptive echo cancellation, AEÜ, Int. J. Electron. Commun. 34 (12) (1980) 485–492.
[39] S. Yamamoto, S. Kitayama, An adaptive echo canceller with variable step gain method, Trans. IECE Jpn.
E65 (1) (1982) 1–8.
[40] B. Nitsch, A frequency-selective stepfactor control for an adaptive ﬁlter algorithm working in the frequency-
domain, Signal Process. 80 (9) (2000) 1733–1745.
[41] J. Benesty, D. Morgan, J. Cho, A new class of double talk detectors based on cross-correlation, IEEE Trans.
Speech Audio Process. 8 (2000) 168–172.
[42] D. Duttweiler, A twelve-channel digital echo canceler, IEEE Trans. Commun. 26 (1978) 647–653.
[43] K. Ochiai, T. Araseki, T. Ogihara, Echo canceler with two echo path models, IEEE Trans. Commun. 25
(1977) 589–595.
[44] H. Buchner, W. Kellermann, Improved Kalman gain computation for multichannel frequency-domain adap-
tive ﬁltering and application to acoustic echo cancellation, in: Proceedings of International Conference on
Acoustics, Speech, and Signal Processing (ICASSP), Orlando, FL, May 2002, pp. 1909–1912.
[45] V. Myllylä, G. Schmidt, Pseudo-optimal regularization for afﬁne projection algorithms, in: Proceedings of
International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Orlando, FL, May 2002,
pp. 1917–1920.
[46] E. Hänsler, G. Schmidt, Control of LMS-type adaptive ﬁlters, in: S. Haykin, B. Widrow (Eds.), Least-Mean-
Square Adaptive Filters, Wiley, 2003, pp. 175–240.
[47] G. Enzner, R. Martin, P. Vary, On spectral estimation of residual echo in hands-free telephony, in: Proceed-
ings of International Workshop on Acoustic Echo and Noise Control (IWAENC), Darmstadt (Germany),
September 2001, pp. 211–214.
[48] G. Enzner, R. Martin, P. Vary, Partitioned residual echo power estimation for frequency-domain acoustic
echo cancellation and postﬁltering, Eur. Trans. Telecommun. 13 (2) (2002) 103–114.
[49] G. Enzner, R. Martin, P. Vary, Unbiased residual echo power estimation for hands-free telephony, in: Proceed-
ings of International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Orlando (Florida),
May 2002, pp. 1869–1893.
[50] S. Gustafsson, Enhancement of Audio Signals by Combined Acoustic Echo Cancellation and Noise Reduc-
tion, PhD thesis, RWTH Aachen, in: Peter Vary (Ed.), Aachener Beiträge zu digitalen Nachrichtensystemen,
Band 11, Verlag der Augustinus Buchhandlung, Aachen, June 1999.
[51] P. Huber, Robust Statistics, Wiley, New York, 1981.
[52] H. Buchner, J. Benesty, T. Gänsler, W. Kellermann, Robust extended multidelay ﬁlter and double-talk detector
for acoustic echo cancellation, IEEE Trans. Speech Audio Process. 14 (9) (2006).
[53] T. Gänsler, S. Gay, M. Sondhi, J. Benesty, Double-talk robust fast converging algorithms for network echo
cancellation, IEEE Trans. Speech Audio Process. 8 (2000) 656–663.
[54] A. Hyvärinen, J. Karhunen, E. Oja, Independent Component Analysis, John Wiley & Sons, New York, 2001.
[55] H. Buchner, Broadband Adaptive MIMO Filter Theory, Springer-Verlag, Berlin, 2012.
[56] H. Buchner, W. Kellermann, A fundamental relation between blind and supervised adaptive ﬁltering illus-
trated for blind source separation and acoustic echo cancellation, in: Proceedings of Joint Workshop on
Hands-Free Speech Communication and Microphone Arrays (HSCMA), Trento, Italy, May 2008.
[57] H. Buchner, R. Aichner, W. Kellermann, TRINICON: a versatile framework for multichannel blind signal
processing, in: Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing
(ICASSP), vol. 3, Montreal, Canada, May 2004, pp. 889–892.
[58] S. Gustafsson, R. Martin, P. Vary, Combined acoustic echo control and noise reduction for hands-free
telephony, Signal Process. 64 (1) (1998) 21–32.

References
873
[59] R. Martin, Freisprecheinrichtungen mit mehrkanaliger Echokompensation und Störgeräuschreduktion, PhD
thesis, RWTH Aachen, in: Peter Vary (Ed.), Aachener Beiträge zu digitalen Nachrichtensystemen, Band 3,
Verlag der Augustinus Buchhandlung, Aachen, June 1995.
[60] R. Martin, J. Altenhöner, Coupled adaptive ﬁlters for acoustic echo control and noise reduction, in: Proceed-
ings of International Conference on Acoustics, Speech, and Signal Processing (ICASSP), May 1995, pp.
3043–3046.
[61] R. Martin, S. Gustafsson, The echo shaping approach to acoustic echo control, Speech Commun. 20 (3–4)
(1996) 181–190.
[62] R. Le Bouquin Jeannès, P. Scalart, G. Faucon, C. Beaugeant, Combined noise and echo reduction in hands-
free systems: a survey, IEEE Trans. Speech Audio Process. (2001) 808–820.
[63] G. Enzner, P. Vary, Robust and elegant, purely statistical adaptation of acoustic echo canceler and postﬁlter,
in: Proceedings of International Workshop on Acoustic Echo and Noise Control (IWAENC), Kyoto (Japan),
September 2003, pp. 43–46.
[64] E. Hänsler, G. Schmidt, Hands-free telephones – joint control of echo cancellation and postﬁltering, Signal
Process. 80 (11) (2000) 2295–2305.
[65] G. Enzner, M. Pauls, VDA Bewertung der modellbasierten Optimalﬁlterung für die Echounterdrückung in
KFZ-Freisprechsystemen, in: Proceedings of Deutsche Jahrestagung für Akustik (DAGA), Dresden (Ger-
many), March 2008.
[66] P. Eneroth, S. Gay, T. Gänsler, J. Benesty, A real-time stereophonic acoustic subband echo canceler, in: S.
Gay, J. Benesty (Eds.), Acoustic Signal Processing for Telecommunications, Kluwer Academic Publishers,
2000, pp. 135–152.
[67] W. Pauler, Akustik-Test: Kfz-Freisprecheinrichtungen, Funkschau 11 (2000) 20–27.
[68] “test”-Magazine,
Testbericht
Freisprechanlagen:
“Wer
billig
kauft…”,
Stiftung
Warentest,
February 2002, pp. 21–25.
[69] H.W. Gierlich, F. Kettler, Speech quality – a multidimensional problem: an approach to combine different
quality parameters, in: Proceedings of Congrès Français d’Acoustique (CFA), Deutsche Jahrestagung für
Akustik (DAGA), Strasbourg, France, March 2004.
[70] R. Gray, A. Buzo, A. Gray, Y. Matsuyama, Distortion measures for speech processing, IEEE Trans. Acoust.
Speech Signal Process. 28 (4) (1980) 367–376.
[71] S. Quackenbush, T. Barnwell, M. Clements, Objectives Measures of Speech Quality, Prentice-Hall, Engle-
wood Cliffs, New Jersey, 1988.
[72] ITU-T Rec. G.107, The E-model, a computational model for use in transmission planning, March 2003.
[73] ITU-T Rec. P.862, Perceptual evaluation of speech quality (PESQ), an objective method for end-to-end
speech quality assessment of narrowband telephone networks and speech codecs, February 2001.
[74] ITU-T Rec. P.862.1, Mapping function for transforming P.862 raw result scores to MOS-LQO, November
2003.
[75] ITU-T Rec. G.164, Echo suppressors, November 1988.
[76] ITU-T Rec. G.165, Echo cancellers, March 1993.
[77] ITU-T Rec. P.340, Transmission characteristics and speech quality parameters of hands-free terminals, May
2000.
[78] ITU-T Rec. P.342, Transmission characteristics for telephone band (300–3400 Hz) digital loudspeaking and
hands-free telephony terminals, May 2000.
[79] ITU-T Rec. P.800, Methods for subjective determination of transmission quality, August 1996.
[80] ITU-T Rec. P.800.1, Mean opinion score (MOS) terminology, March 2003.
[81] ITU-T Rec. P.832, Subjective performance evaluation of hands-free terminals, May 2000.
[82] ITU-T Rec. P.835, Subjective test methodology for evaluating speech communication systems that include
noise suppression algorithm, November 2003.

874
CHAPTER 30 Acoustic Echo Control
[83] F. Kettler, Kfz-Freisprecheinrichtungen: Test-Speziﬁkation für den Guten Ton, Funkschau 24 (2002)
50–53.
[84] Verband der Automobilindustrie, VDA Speciﬁcation for Car Hands-free Terminals (Version 1.4), Verband
der Automobilindustrie, December 2002.
[85] H.W. Gierlich, F. Kettler, Advanced speech quality testing of modern telecommunication equipment: an
overview, Signal Process. 86 (6) (2006) 1327–1340 (special issue on applied speech and audio processing).
[86] S. Möller, F. Kettler, H.W. Gierlich, S. Poschen, N. Cote, A. Raake, M. Wältermann, Extending the
E-model for capturing noise reduction and echo canceller impairments, J. Audio Eng. Soc. 60 (3) (2012)
165–174.
[87] L. Nunes, F. Avila, A. Tygel, L. Biscainho, B. Lee, A. Said, R. Schafer, A parametric objective quality
assessment tool for speech signals degraded by acoustic echo, IEEE Trans. Audio Speech Lang. Process. 20
(8) (2012) 2181–2190.
[88] G. Enzner, P. Vary, New insights into the statistical signal model and the performance bounds of acoustic
echo control, in: Proceedings of European Signal Processing Conference (EUSIPCO), Antalya, Turkey,
September 2005.
[89] G.H. Golub, C.F. van Loan, Matrix Computations, The Johns Hopkins University Press, Baltimore, 1996.
[90] J.G. Proakis, D.G. Manolakis, Digital Signal Processing: Principles, Algorithms, and Applications, Prentice-
Hall, Upper Saddle River, New Jersey, 1996.
[91] L.L. Scharf, Statistical Signal Processing, Addison-Wesley Publishing Company, 1991.
[92] R. Unbehauen, Systemtheorie 1, seventh ed., R. Oldenbourg Verlag, München Wien, 1997.
[93] R. Kalman, A new approach to linear ﬁltering and prediction problems, Trans. ASME J. Basic Eng. 82 (1960)
35–45.
[94] G. Enzner, Bayesian inference model for applications of time-varying acoustic system identiﬁcation, in:
Proceedings of European Signal Processing Conference (EUSIPCO), Aalborg, Denmark, August 2010.
[95] D. Lippuner, Model-Based Step-Size Control for Adaptive Filters, PhD thesis, ETH Zürich (Diss. No. 14461),
in: Hans-Andrea Loeliger (Ed.), Series in Signal and Information Processing, vol. 8, Hartung-Gorre Verlag,
Konstanz, January 2002.
[96] D. Lippuner, A.N. Kälin, Tracking behavior of model-based adaptive FIR ﬁlters with noise variance estima-
tion, in: Proceedings of International Workshop on Acoustic Echo and Noise Control (IWAENC), Pocono
Manor, Pennsylvania, September 1999, pp. 156–159.
[97] S. Malik, G. Enzner, Model-based vs. traditional frequency-domain adaptive ﬁltering in the presence of
continuous double-talk and acoustic echo path variability, in: Proceedings of International Workshop on
Acoustic Echo and Noise Control (IWAENC), Seattle, WA, USA, September 2008.
[98] S. Malik, G. Enzner, Online maximum-likelihood learning of time-varying dynamical models in block-
frequency domain, in: Proceedings of International Conference on Acoustics, Speech, and Signal Processing
(ICASSP), Dallas, TX, USA, March 2010.
[99] M.M. Sondhi, D.A. Berkeley, Silencing echoes on the telephony network, Proc. IEEE (1980) 948–963.
[100] C. Faller, J. Chen, Suppressing acoustic echo in a sampled auditory envelope space, IEEE Trans. Acoust.
Speech Signal Process. (2005) 1048–1062.
[101] C. Faller, C. Tournery, Estimating the delay and coloration effect of the acoustic echo path for low com-
plexity echo suppression, in: Proceedings of International Workshop on Acoustic Echo and Noise Control
(IWAENC), September 2005.
[102] A.N. Birkett, R.A. Goubran, Limitations of handsfree acoustic echo cancellers due to nonlinear loudspeaker
distortion and enclosure vibration effects, in: Proceedings of IEEE Workshop on Applications of Signal
Processing to Audio and Acoustics, October 1995, pp. 13–16.
[103] V. Turbin, A. Gilloire, P. Scalart, Enhancement of speech corrupted by musical noise, in: Proceedings of
International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 1997, pp. 307–310.

References
875
[104] A.Favrot,C.Faller,M.Kallinger,F.Kuech,M.Schmidt,Acousticechocontrolbasedontemporalﬂuctuations
of short-time spectra, in: Proceedings of International Workshop on Acoustic Echo and Noise Control
(IWAENC), September 2008.
[105] E. Robledo-Arnuncio, T.S. Wada, B.-H. Juang, On dealing with sampling rate mismatches in blind source
separation and acoustic echo cancellation, in: Proceedings of IEEE Workshop on Applications of Signal
Processing to Audio and Acoustics, October 2007, pp. 34–37.
[106] S.F. Boll, Suppression of acoustic noise in speech using spectral subtraction, IEEE Trans. Acoust. Speech
Signal Process. (1979) 113–120.
[107] M. Azaria, D. Hertz, Time delay estimation by generalized cross correlation methods, IEEE Trans. Acoust.
Speech Signal Process. (1984) 3689–3692.
[108] W. Etter, G.S. Moschytz, Noise reduction by noise-adaptive spectral magnitude expansion, J. Audio Eng.
Soc. (1994) 341–349.
[109] M. Berouti, R. Schwartz, J. Makhoul, Enhancement of speech corrupted by musical noise, in: Pro-
ceedings of International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 1979,
pp. 208–211.
[110] F. Kuech, M. Kallinger, M. Schmidt, C. Faller, A. Favrot, Acoustic echo suppression based on separation
of stationary and non-stationary echo components, in: Proceedings of International Workshop on Acoustic
Echo and Noise Control (IWAENC), September 2008.
[111] C. Faller, F. Baumgarte, Binaural cue coding – Part II: Schemes and applications, IEEE Trans. Speech Audio
Process. 11 (6) (2003).
[112] B.R. Glasberg, B.C.J. Moore, Derivation of auditory ﬁlter shapes from notched-noise datas, Hear. Res. (1990)
103–138.
[113] C. Faller, Perceptually motivated low complexity acoustic echo control, in: Preprint 114th Convention of the
Audio Engineering Society, March 2003.
[114] A. Papoulis, S.U. Pillai, Probability, Random Variables and Stochastic Processes, McGraw-Hill, New York,
2002.
[115] J. Benesty, D. Morgan, M. Sondhi, A better understanding and an improved solution to the speciﬁc
problems of stereophonic acoustic echo cancellation, IEEE Trans. Speech Audio Process. 6 (2) (1998)
156–165.
[116] T. Gänsler, J. Benesty, Stereophonic acoustic echo cancellation and two-channel adaptive ﬁltering: an
overview, Int. J. Adapt. Control Signal Process. (2000).
[117] S. Shimauchi, S. Makino, Stereo projection echo canceller with true echo path estimation, in: Proceedings of
IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Detroit, MI, USA,
May 1995, pp. 3059–3062.
[118] A. Sugiyama, Y. Joncour, A. Hirano, A stereo echo canceler with correct echo-path identiﬁcation on an
input-sliding technique, IEEE Trans. Signal Process. 49 (11) (2001) 2577–2587.
[119] H. Buchner, W. Kellermann, Acoustic echo cancellation for two and more reproduction channels, in: Con-
ference Rec. IEEE International Workshop on Acoustic Echo and Noise Control (IWAENC), Darmstadt,
Germany, September 2001, pp. 99–102.
[120] J. Herre, H. Buchner, W. Kellermann, Acoustic echo cancellation for surround sound using perceptually moti-
vated convergence enhancement, in: Proceedigns of IEEE International Conference on Acoustics, Speech,
and Signal Processing (ICASSP), Honolulu, HI, USA, April 2007.
[121] M. Sondhi, D. Morgan, Stereophonic acoustic echo cancellation – an overview of the fundamental problem,
IEEE Signal Process. Lett. 2 (8) (1995) 148–151.
[122] H. Buchner, R. Aichner, W. Kellermann, Blind source separation for convolutive mixtures exploiting non-
gaussianity, nonwhiteness, and nonstationarity, in: Proceedings of International Workshop on Acoustic Echo
and Noise Control (IWAENC), Kyoto, Japan, September 2003, pp. 223–226.

876
CHAPTER 30 Acoustic Echo Control
[123] H.Buchner,R.Aichner,W.Kellermann,Blindsourceseparationforconvolutivemixtures:auniﬁedtreatment,
in: J. Benesty, Y. Huang (Eds.), Audio Signal Processing for Next-Generation Multimedia Communication
Systems, Kluwer Academic Publishers, Boston, April 2004, pp. 255–293.
[124] H. Buchner, R. Aichner, W. Kellermann, Relation between blind system identiﬁcation and convolutive
blind source separation, in: Proceedings of Joint Workshop on Hands-Free Speech Communication and
Microphone Arrays (HSCMA), Piscataway, NJ, USA, March 2005.
[125] H. Buchner, R. Aichner, W. Kellermann, TRINICON-based blind system identiﬁcation with application
to multiple-source localization and separation, in: S. Makino, T.-W. Lee, S. Sawada (Eds.), Blind Speech
Separation, Springer, Berlin, September 2007, pp. 101–147.
[126] H. Buchner, Acoustic echo cancellation for multiple reproduction channels: from ﬁrst principles to real-time
solutions, in: Proceedings of ITG Conference on Speech Communication, ITG Report No. 211, Aachen,
Germany, October 2008.
[127] D. Morgan, J. Hall, J. Benesty, Investigation of several types of nonlinearities for use in stereo acoustic echo
cancellation, IEEE Trans. Speech Audio Process. 9 (5) (2001) 686–696.
[128] T. Gänsler, P. Eneroth, Inﬂuence of audio coding on stereophonic acoustic echo cancellation, in: Proceedings
of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), May 1998, pp.
3649–3652.
[129] M. Ali, Stereophonic acoustic echo cancellation system using time-varying all-pass ﬁltering for signal decor-
relation, in: Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing
(ICASSP), Seattle, WA, USA, May 1998, pp. 3689–3692.
[130] T. Hoya, J. Chambers, P. Naylor, Low complexity of ϵ-NLMS algorithms and subband structures for stereo-
phonic acoustic echo cancellation, in: Proceedings of International Workshop on Acoustic Echo and Noise
Control (IWAENC), Pocono Manor, NJ, USA, September 1999.
[131] J.
Blauert,
Spatial
Hearing:
The
Psychophysics
of
Human
Sound
Localization,
MIT
Press,
Cambridge, MA, 1997.
[132] H. Malvar, A modulated complex lapped transform and its application to audio processing, in: Proceedings
of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Phoenix, AZ,
USA, 1999, pp. 1421–1424.
[133] A. Berkhout, D. de Vries, P. Vogel, Acoustic control by wave ﬁeld synthesis, J. Acoust. Soc. Am. 93 (5)
(1993) 2764–2778.
[134] H. Buchner, S. Spors, W. Herbordt, W. Kellermann, Wave-domain adaptive ﬁltering for acoustic human-
machine interfaces based on waveﬁeld analysis and synthesis, in: Proceedings of European Signal Processing
Conference (EUSIPCO), Vienna, Austria, September 2004.
[135] S. Spors, H. Buchner, R. Rabenstein, W. Herbordt, Active listening room compensation for massive mul-
tichannel sound reproduction systems using wave-domain adaptive ﬁltering, J. Acoust. Soc. Am. 122 (1)
(2007) 354–369.
[136] H. Buchner, S. Spors, A general derivation of wave-domain adaptive ﬁltering and application to acoustic
echo cancellation, in: Proceedings of Asilomar Conference on Signals, Systems, and Computers, Paciﬁc
Grove, CA, USA, October 2008.
[137] K. Helwani, H. Buchner, S. Spors, Source-domain adaptive ﬁltering for MIMO systems with application
to acoustic echo cancellation, in: Proceedings of IEEE International Conference on Acoustics, Speech, and
Signal Processing (ICASSP), Dallas, TX, USA, March 2010.
[138] O. Hoshuyama, A. Sugiyama, An acoustic echo suppressor based on a frequency-domain model of highly
nonlinear residual echo, in: Proceedings of IEEE International Conference on Acoustics, Speech, and Signal
Processing (ICASSP), Toulouse, May 2006.
[139] B.S. Nollett, D.L. Jones, Nonlinear echo cancellation for hands-free speakerphones, in: Proceedings of IEEE
Workshop on Nonlinear Signal and Image Processing (NSIP), Michigan, September 1997.

References
877
[140] A. Stenger, W. Kellermann, Adaptation of a memoryless preprocessor for nonlinear acoustic echo cancelling,
Signal Process. 80 (2000) 1741–1760.
[141] F. Kuech, A. Mitnacht, W. Kellermann, Nonlinear acoustic echo cancellation using adaptive orthogonalized
power ﬁlters, in: Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing
(ICASSP), Philadelphia, March 2005.
[142] A. Stenger, R. Rabenstein, Adaptive Volterra ﬁlters for acoustic echo cancellation, in: Proceedings of IEEE
Workshop on Nonlinear Signal and Image Processing (NSIP), Antalya, June 1999.
[143] F. Kuech, W. Kellermann, Nonlinear acoustic echo cancellation, in: E. Hänsler, G. Schmidt (Eds.), Topics in
Acoustic Echo and Noise Control, Springer, Berlin, 2006.
[144] W. Klippel, Dynamic measurement and interpretation of the nonlinear parameters of electrodynamic loud-
speakers, J. Audio Eng. Soc. 38 (12) (1990) 944–955.
[145] H. Schurer, Linearization of Electroacoustic Transducers, Enschede: Print Partners Ipskamp, 1997.
[146] S. Malik, G. Enzner, Variational Bayesian inference for nonlinear acoustic echo cancellation using adaptive
cascade modeling, in: Proceedings of IEEE International Conference on Acoustics, Speech, and Signal
Processing (ICASSP), Kyoto, March 2012.
[147] F. Kuech, W. Kellermann, Orthogonalized power ﬁlters for nonlinear acoustic echo cancellation, Signal
Process. 86 (2006) 1168–1181.
[148] S.Malik,G.Enzner,State-spacefrequency-domainadaptiveﬁlteringfornonlinearacousticechocancellation,
IEEE Trans. Audio Speech Language Process. 20 (7) (2012) 2065–2079.
[149] A. Guérin, G. Faucon, R.L. Bouquin-Jeannès, Nonlinear acoustic echo cancellation based on Volterra ﬁlters,
IEEE Trans. Acoust. Speech Signal Process. 11 (6) (2003) 672–683.
[150] F. Kuech, W. Kellermann, A novel multidelay adaptive algorithm for Volterra ﬁlters in diagonal coordi-
nate representation, in: Proceedings of IEEE International Conference on Acoustics, Speech, and Signal
Processing (ICASSP), Montreal, May 2004.
[151] M. Zeller, W. Kellermann, Fast and robust adaptation of DFT-domain Volterra ﬁlters in diag-
onal coordinates using iterated coefﬁcient updates, IEEE Trans. Signal Process. 58 (3) (2010)
1589–1604.
[152] M. Zeller, L.A. Azpicueta-Ruiz, J. Arenas-Garcia, W. Kellermann, Adaptive Volterra ﬁlters with evolutionary
quadratic kernels using a combination scheme for memory control, IEEE Trans. Signal Process. 59 (4) (2011)
1449–1464.
[153] J. Sotschek, Sätze für Sprachgütemessungen und ihre phonologische Anpassung an die deutsche Sprache,
in: Proceedings of Deutsche Jahrestagung für Akustik (DAGA), 1984, pp. 873–876.
[154] R. Streicher, F.A. Everest, The New Stereo Soundbook, second ed., Audio Engineering Associates, Pasadena,
CA, USA, 1998.
[155] G. Enzner, P. Vary, On the problem of acoustic echo control in cellular networks, in: Proceedings of Interna-
tional Workshop on Acoustic Echo and Noise Control (IWAENC), Eindhoven, The Netherlands, September
2005, pp. 213–216.
[156] M. Pawig, P. Vary, Energy efﬁciency of network-based acoustic echo control in mobile radio, in: 10. ITG
Conference on Speech Communication, Braunschweig, Germany, September 2012.
[157] K. Helwani, H. Buchner, S. Spors, Spatio-temporal signal preprocessing for multichannel acoustic echo
cancellation, in: Proceedings of International Conference on Acoustics, Speech, and Signal Processing
(ICASSP), Prague, CZ, May 2011.
[158] S. Malik, G. Enzner, Fourier expansion of Hammerstein models for nonlinear acoustic system identiﬁcation,
in: Proceedings of International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Prague,
CZ, May 2011.

31
CHAPTER
Dereverberation
Patrick A. Naylor
Department of Electrical and Electronic Engineering, Imperial College, London, UK
4.31.1 Introduction and overview
Anyone with a sense of hearing will have an innate consciousness of the acoustic spaces in which they
live or work. The human auditory perceptual system is very capable of adapting to different surroundings
with widely different acoustics while maintaining full and functional understanding of the sounds heard,
whether these be approaching footsteps in an echoey cave, sublime music in a vaulted gothic cathedral
or a teacher’s voice in a sparsely furnished classroom. Our understanding of how humans achieve
this capability is far from complete. Ongoing studies have however produced a wealth of knowledge
including, for example, the celebrated book by Cherry [1] in which the now often used term the cocktail
party effect was ﬁrst described.
Imagine a listener hearing sound in a reverberant space. Our own experience probably tells us that
humans are not disturbed by reverberation in live sound, unless it is really extreme. However, now
imagine that the same audio signals from the same reverberant space are captured by a microphone
and then listened to using playback over headphones—in other words, the listening experience is taken
out of it’s natural space. In this case, the human listener becomes very aware of, and usually disturbed
by, the high level of reverberation that is actually present in the signal. People are usually surprised
at the level of reverberation in the playback of the recorded sound because they remember hearing
the corresponding live sound clearly and without reverberation. This indicates that humans employ
sophisticated processing to attenuate the effects of reverberation, possibly based on spatial information
deduced from spatially sampling the sound ﬁeld with two ears, possible exploiting other auditory and
visual cues and possibly due to high-level cognitive processes in the brain.
In many applications of speech and audio processing, it is exactly this latter abstracted situation which
prevails; human listeners must operate outside the acoustic space in which the sound naturally exists, as
when using the telephone for example. Equally, machine “listeners” such as speech recognition systems
are also limited to operating only on the captured sound. Accordingly, without the fantastic capabilities
of the human auditory system to help, reverberation becomes problematic. Dereverberation, deﬁned as
any process that attempts to overcome such problems, then has to be called on to recover the original
nonreverberant sound as accurately as possible.
Research and development of dereverberation techniques has been going on for as long as people
have been able practically to process audio signals. Early references to relevant digital signal processing
techniques include [2–4]. The combination of increased consumer demand for products employing
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00031-5
© 2014 Elsevier Ltd. All rights reserved.
879

880
CHAPTER 31 Dereverberation
distant-talking hands-free audio interfaces coupled with availability of low-cost, high-performance
computing has led to burgeoning activity levels in dereverberation starting around the year 2000. The
level of this activity still continues to grow.
This chapter will begin by describing some of the potential applications of dereverberation processing
in order to establish a context and scope for the later material. It will then present some information on
room acoustics and human perception in order to explain the cause and effect of reverberation in speech
signals, particularly in the context of hands-free telecommunication scenarios, and in music signals.
The next section will present information on the measurement of reverberation and describe how any
reduction in reverberation brought about by dereverberation processing can be quantiﬁed. This issue is
not straightforward because of the need also to take into account any impact the processing may have
on other aspects of the signal such as the naturalness or overall quality.
After completing the discussion of these preliminary topics, methods for dereverberation will then
be considered in three classes according to the approach taken. The ﬁrst class introduces the approach
of spatial ﬁltering using multichannel signals from a microphone array together with beamforming
algorithms to reduce reverberation. The second class formulates dereverberation as a classical signal
enhancement problem in which the reverberation is treated as interference. The third class formulates
dereverberation using an acoustic system modeling paradigm in which the acoustic channel responsible
for the reverberation is explicitly modeled and subsequently inverted in order to design an equalizer to
attenuate the effect of the reverberation. The closing section of this chapter summarizes and discusses
the current state of the art in dereverberation processing and aims to present a brief outlook for future
directions.
The chapter is designed for readers new to the topic of speech dereverberation and those who wish to
see an overview of the topic in a single presentation. A textbook such as [5] is recommended for those
seeking more detailed technical information on the topic as a whole or the speciﬁc techniques. The
style of this current book is intended to be tutorial in nature and will present a mixture of conceptual
information and some analysis where appropriate.
4.31.2 Example applications
Dereverberation is an important, and in some cases critically important, signal processing function in
several application scenarios. Example applications have been selected and summarized here, for which
reverberation is a signiﬁcant issue. This then serves to justify a need for dereverberation processing.
4.31.2.1 Hands-free speech telecommunication
Perhaps the most common application requiring dereverberation is in high quality hands-free terminals
for speech communication [6]. In this scenario, one or more talkers are located some distance, typically
more than 50 cm, from the microphone and therefore the speech signal captured by the microphone
sounds inevitably “distant.” This reduction in the quality of the acquired speech is due to the multipath
acoustic propagation of the speech signal from the talker to the microphone. The microphone picks
up not only the acoustic signal traveling on the direct path (line-of-sight) from the talker but also a
large number of acoustic signals which have travelled from the talker via paths involving one or more

4.31.2 Example Applications
881
reﬂections off, for example, the walls, ceiling, ﬂoor or objects in the room. The popularity of “smart
phones” and computer-based telecommunications based on Voice over Internet Protocol (VoIP) has
encouraged very substantial growth in hands-free use [7].
Using a telephone while driving a vehicle is the subject of legislation normally requiring at least
the use of the hands-free mode and voice control of functions such as dialing [8]. In the car scenario,
background noise is often high especially when traveling at speed, although the level of reverberation
inside the car cabin is usually well controlled by the materials used. Even relatively low levels of
reverberation can be disturbing when combined with noise, and the combination of reverberation and
noise reduces intelligibility much more signiﬁcantly than either of the two degradations separately.
In the case of commercial vehicles such as trucks and vans, the cabin reverberation and noise levels may
be higher than in cars. The use of the hands-free mode when calling in these scenarios is both mandatory
and at the same time technically challenging, making dereverberation technology particularly important.
The criteria for such algorithms may be tuned to match better the requirements of speech recognition for
voice control, or may be tuned differently to match better the requirements of telecommunications. One
example difference in tuning is the latency which can be long in the case of speech recognition—even
1 s of latency may not be a problem—but must be short for interactive telecommunications—typically
less than a few tens of milliseconds.
4.31.2.2 Hearing aids
Reverberation is also a signiﬁcant problem for users of hearing aids who experience difﬁculties in fol-
lowing conversations in reverberant and/or noisy situations. Much recent research has been undertaken
in this scenario [9]. It is now well understood that hearing aid users beneﬁt signiﬁcantly from binaural
cues that enable sounds to be located in space by the listener, to some degree at least [10]. This can
place additional requirements on the design of dereverberation algorithms in that binaural cues should
not be disturbed. Such schemes can potentially be implemented in bilateral hearing aids with inter-aid
communication, for example. Dereverberation approaches using beam-forming technology are also very
relevant [11].
4.31.2.3 Music and ﬁlm production
A further application of dereverberation processing is in the entertainment sector. In this context, the
term dry is used to describe sound free from reverberation and wet refers to reverberant sound.
A ﬁrst example is in audio post-production. It is normally necessary to record in a near-anechoic
room lined with sound absorbing foam [12] as illustrated in Figure 31.1. The three audio clips of
Table 31.1 are taken from the song The Woman in Moon, written and performed by Cat Dove, and are
examples of a dry vocal, the same vocal with added reverberation, and the reverberant vocal mixed
with piano accompaniment, respectively. It is clear to most listeners that the reverberant vocal is more
pleasing because it sounds, perhaps, more natural and balanced in the mix.
In the converse situation in which the recorded sound already contains reverberation, it may need
to be dereverberated in order to be used effectively in the mix. Dereverberation can be understood as
the task of obtaining the dry vocal from the reverberant signal. Software tools for dereverberation are
starting to become available such as the NML RevCon-RR plug-in from TAC System, Inc.

882
CHAPTER 31 Dereverberation
FIGURE 31.1
Audio recording in a near-anechoic room.
Table 31.1 Audio Clips
Dry vocal
Reverberant vocal
Reverberant vocal with accompaniment

4.31.3 Room Reverberation
883
A second example is in ﬁlm and TV production in which sound captured during a “take” may
contain reverberation because of constraints on microphone positioning on the ﬁlm set. The application
of dereverberation processing may remove the need for the sound to be re-recorded in a dryer acoustic
and overdubbed, thereby saving time and cost.
4.31.3 Room reverberation
4.31.3.1 Room acoustics
Reverberation is an effect that occurs when a signal emitted by a source propagates through multiple
paths to a receiver. Multipath acoustic channels occur in many real-life situations. A common example
in which multipath acoustic echoes are encountered is a railway station in which acoustic echoes often
maketheannouncer’svoicehardtounderstand.Largehallsandchurchesoftenalsogiverisetosigniﬁcant
reverberation giving the sound a spacious and sometimes esthetically pleasing characteristic. Multipath
channels are also well known in wireless radio-frequency communications though this chapter will only
focus on reverberation in the context of acoustic propagation of sound.
The diagram of Figure 31.2 shows the plan view of a schematic representation of a room. The room
contains a single sound source and a microphone. Direct path propagation (a) is shown as well as a
ﬁrst-order reﬂection (b) and a second-order reﬂection (c). In the acoustic environment of a real room,
reﬂections can be expected not only from the six principal surfaces—4 walls, ceiling, and ﬂoor—but
also from furniture such as tables and objects such as glazed photograph frames, to name but a few
examples. In general, therefore, the number of audible reﬂections is very large. One may argue that the
number of reﬂections is actually inﬁnite but in reality the amplitude at each reﬂection is attenuated by
the absorption of the reﬂecting surface so that, after some time, the reﬂections are no longer audible. The
duration of reverberation is characterized in terms of the Reverberation Time (T60), as will be discussed
below.
Source
Microphone
(a)
(b)
(c)
FIGURE 31.2
Plan view of a rectangular room illustrating multipath sound propagation: (a) direct path propagation,
(b) ﬁrst-order reﬂection, and (c) second-order reﬂection.

884
CHAPTER 31 Dereverberation
The propagation of sound waves through a material is described by the second-order partial differ-
ential wave equation [13]. However, it is common and practical to employ a point-to-point model of the
acoustics of reverberant rooms in the form of Linear Time Invariant (LTI) acoustic systems [13]. This
approach to modeling has many advantages in that it draws on linear system theory but it nevertheless
implies a simpliﬁcation; in reality, room acoustics vary with time due to movement of people or objects
in the room and, to a lesser degree, air movement due to thermal currents. Even though these effects are
ignored under the LTI assumption, results obtained for such models are useful and LTI models form the
basis for the vast majority of research on this topic.
In discrete time and for sample index n, the Room Impulse Response (RIR) can be denoted h(n). The
signal arriving at the microphone shown in Figure 31.2 due to the source s(n) can then be written as
x(n) =
∞

i=0
h(i)s(n −i)
(31.1)
bearing in mind the limits on h(n) as given in (31.2).
Assuming that the room is a causal acoustic system, we can write that
h(n) =
⎧
⎨
⎩
0,
n < 0,
he(n),
0 ≤n < ne,
hl(n),
n > ne,
(31.2)
which explicitly shows the causality and, for some given index ne, indicates that the RIR is commonly
considered in two sections. For 0 ≤n < ne, h(n) contains the direct path, as labeled (a) in Figure 31.2,
and the early reﬂections arising from strong reﬂections from speciﬁc surfaces, as labeled (b) and (c)
in Figure 31.2. For n > ne, h(n) contains the characteristics of late reverberation arising from higher
order reﬂections that reach the microphone only after several bounces from the reﬂective surfaces in the
room. The choice of ne is not precisely determined in room acoustics. One feasible choice at sampling
frequency fs Hz is
ne
fs
= 30–60 ms,
(31.3)
which corresponds to the mixing time of a typical room. At times earlier than the mixing time, the RIR
contains discrete reﬂections whereas at times later than the mixing time, the RIR is diffuse [14,15].
A commonly used measure of room acoustics is the T60. This characterizes the time taken for a
diffuse sound ﬁeld to decay in energy by 60 dB. Further details regarding T60 are given in Section
4.31.4.1. Another important metric is the critical distance which is deﬁned as the distance from the
source at which the sound energy density due to direct propagation is equal to the sound energy density
due to reverberation. It corresponds, intuitively, to the distance that a listener would have to stand from
a sound source such that the direct and reverberant sound was mixed at 0 dB, that is to say, in equal
amounts. Stepping toward the source would improve the clarity of the sound by making the direct sound
stronger than the reverberation and stepping backward from the source would make the reverberation
stronger than the direct sound.

4.31.3 Room Reverberation
885
4.31.3.2 Listener perception of reverberation
In daily life, human experience of sound is normally free from a sense of reverberation. Most people
can enjoy the novelty of the effect, for example, by calling out as they walk through a tunnel or giving a
gentle whistle or clap when entering a hall or church. In such highly reverberant environments, the effect
of the echo is remarkable. However, in the spaces occupied in daily life—a train carriage, ofﬁce, or
shop—people are unaware of the acoustic reverberation even though it may be measurably signiﬁcant.
Human ability to perceive speciﬁc sounds in isolation from ambient or interfering sound was ﬁrst
documented in the cocktail party effect by Cherry in his book On Human Communication as mentioned
above [1]. This human ability is far from understood even today. However, it is known that many
cues from the environment are employed by the human listener in performing the extraction of the
speciﬁc sound from the background. When the human listener is taken out of the environment so that
they experience the sound in isolation, for example over headphones, the ability to extract the speciﬁc
sound is substantially diminished. It can be concluded therefore that even humans ﬁnd dereverberation
a difﬁcult task when given only the audio signal to work with. This is the formulation of the problem—
having only the audio signal—in the form that it is addressed by dereverberation algorithms in audio
processing systems.
The example RIR of Figure 31.3 shows two regions. The region to the left colored red1 comprises
the direct path and the early reﬂections whereas the region to the right colored blue comprises the late
reﬂections. The early reﬂections give rise to spectral coloration of the signal due to the effect of ﬁltering
the signal with the often strong coefﬁcients representing the discrete reﬂections in the early part of
the RIR. The resulting spectral coloration of the sound may cause listeners to describe it as “thin,”
“mufﬂed,” “unnatural,” etc. This part of the RIR is not normally associated with echo by listeners since,
in general, closely spaced echoes are not distinguished by human hearing due to masking properties of
the ear. In particular, the early reﬂections can have a positive effect on speech intelligibility since the ear
combines together the direct path and the early reﬂections so as effectively to increase the strength of
the direct path sound [16]. Despite this, coloration can degrade the perceived quality of the speech [13].
In contrast, the late reﬂections, which resemble seemingly random coefﬁcients with an exponentially
decaying envelope, cause the sound to become distant or echoey, smearing somewhat one sound into the
next. The late reﬂections are the cause of what most people refer to as reverberation—the characteristic
echo or ringing effect that can be encountered in situations such as in tunnels, caves, or the interior of
large buildings.
4.31.3.3 Simulating room acoustics
Simulation of room acoustics can be useful to add a sense of space to an otherwise “dry” sound
recording. Artiﬁcial spatialization of sound using such techniques can be used in applications including
computer games, music recordings, and movie soundtracks. Such spatialization can be used as well
in video conferencing to locate talkers in a virtual room even if the audio communication channel
does not support multichannel transmission. Localization of talkers in this way requires that the talkers
are identiﬁed by some means, for example using speaker identiﬁcation techniques, video analysis, or
transmission of side information. The beneﬁt is to make the task of following a meeting much easier
1For interpretation of color in Figure 31.3, the reader is referred to the web version of this book.

886
CHAPTER 31 Dereverberation
FIGURE 31.3
Example of a room impulse response showing early and late refections.
because of the increased realism and the ability of the listener to exploit the spatial information to
separate and isolate the speech for individual participants exploiting the cocktail party effect [1]. An
important additional use of room acoustics simulation is in research and development of dereverberation
technology.
Several different methods for simulating room acoustics have been developed over many years. An
early popular example is the spring reverb unit such as frequently found in sound equipment including
electric guitar ampliﬁers and the Hammond organ. These electromechanical devices use, as the name
suggests, a metal spring which is made to vibrate by a transducer. The resulting resonances in the spring
are sensed as an audio signal. Despite their low cost and popularity, such electromechanical devices
have been more recently replaced by signal processing algorithms.
Probably the most widely used room acoustic simulation technique is the method of images [17].
Computer code is freely available for this technique at [18].
4.31.3.4 Overview of the main approaches for dereverberation
Given this background on the cause and effect of room reverberation, it is now possible to see clearly
the motivation for the different dereverberation approaches that could be applied. These are mentioned
brieﬂy in this section by way of an overview and will be described in more detail in the corresponding
Sections 4.31.5–4.31.7.
4.31.3.4.1
Spatial ﬁltering
The ﬁrst and perhaps the most intuitive approach to dereverberation is spatial ﬁltering. This approach
exploits the fact that the direct sound arrives at the microphone(s) from a particular direction whereas

4.31.4 Measurement of Reverberation
887
the reverberant sound, due to reﬂections off the surfaces in the room, arrives at the microphone(s) from
all directions. A spatial ﬁlter, normally realized using an array of microphones and a beamforming
algorithm, can therefore be used to attenuate sounds arriving from all directions except the direction
of the source. It may also be possible to arrange nulls in the beamformer’s spatial response in the
speciﬁc directions of strong early reﬂections. If the beamformer exhibits strong spatial ﬁltering then
the only remaining reverberation in the beamformer output would be that from the same direction as
the source—perhaps from a reﬂective wall behind the talker. This approach will be discussed more in
Section 4.31.5.
4.31.3.4.2
Speech enhancement
A second approach to dereverberation is to consider the reverberation as a type of noise added to the
direct path signal and then apply noise reduction techniques to remove it. Noise reduction, also known
as speech enhancement, is often employed to remove additive noise from speech. Examples of typical
noise types encountered in telecommunications applications include hum, babble, car noise, trafﬁc
noise, etc. It is a straightforward extension to consider reverberation as yet one more noise type—the
difference being that reverberation is highly correlated with the direct path signal whereas other types
of noise are often assumed uncorrelated in the development of noise reduction techniques. This issue
and other details of the approach will be discussed further in Section 4.31.6.
4.31.3.4.3
Acoustic channel-based equalization
A third approach is to frame the problem in terms of the fundamentals of room acoustics. It is known
that reverberation is caused by convolution of the source signal with a point-to-point RIR representing
multipathacousticpropagationfromthesoundsource(talker)tothesoundsensor(listenerormicrophone
array). If the acoustic channel is known in terms of its RIR, or can be estimated, then application of the
inverse channel to the reverberant signal would recover the original (dry) source in a similar fashion
to channel equalization employed in wireless communications. This approach presents several speciﬁc
technical challenges and will be discussed further in Section 4.31.7.
4.31.4 Measurement of reverberation
Measurement and evaluation of the effect of dereverberation methods is a challenging task because any
meaningful method of measurement needs to approximate or predict how much a human listener will
sense the level of reverberation or be disturbed by artifacts that might be introduced by the processing.
Since our knowledge of human perception is limited, so also is our ability to quantify changes in the
level of reverberation.
The starting point for most methods of measurement is to divide the task into two independent steps.
The ﬁrst step aims to measure late reverberation in terms of the duration and power of the reﬂections in
the later part of the RIR and, as such, gives an indication of the level of echo that would be perceived. The
second step aims to measure early reverberation in terms of the coloration introduced by the reﬂections
in the early part of the impulse response and, as such, gives an indication of the speech distortion
that would be perceived. The deﬁnition of early and late in this context should be seen in relation to
Figure 31.3 and Eq. (31.2) with ne
fs = 30–60 ms typically.

888
CHAPTER 31 Dereverberation
At the time of writing, there are no relevant standards for measurement of reverberation but, nev-
ertheless, several researchers have been establishing what might be called good practice. Accordingly,
a favored current approach to measure the effect of dereverberation is to use the Energy Decay Curve
(EDC) and Perceptual Evaluation of Speech Quality (PESQ) measures together. Any change in the late
reverberation due to dereverberation can be observed by comparing the EDCs before and after process-
ing. This will be described in Section 4.31.4.2. Any change in the speech quality can be observed by
changes in PESQ score, or other appropriate instrumental measure. An example of an article describing
the use of this approach is [19].
4.31.4.1 Reverberation time
The reverberation time is a widely used measure of diffuse sound ﬁelds in room acoustics. It is commonly
characterized in terms of the T60 (or sometimes RT60) under the deﬁnition that the T60 is the duration of
time taken for a uniformly distributed sound energy density to decay by 60 dB after the sound source has
been switched off. Table 31.2 shows some examples with their corresponding typical reverberation time.
Obviously, acoustic environments vary widely and so these ﬁgures must be taken only as illustrative.
Measurements of the T60 may be made by exciting a diffuse sound ﬁeld in the room with a broadband
signal, switching off the signal abruptly and the measuring the energy decay directly to ﬁnd the time
taken for the energy to decay by 60 dB. This may be limited and problematic since it is often difﬁcult to
achieve a sufﬁcient dynamic range to make the measurement and difﬁcult to avoid the effects of early
reﬂections, especially in smaller rooms. The T60 may also be measured using the EDC as described
later in Section 4.31.4.2.
Thereverberationtimeis,inmanycase,frequencydependentasillustratedinFigure31.4whichshows
the variation in reverberation time with frequency for two famous cathedral churches in the UK [20].
The work of Sabine, ﬁrst published in 1921 [13,21], formulated a relationship in which the rever-
beration time was proportional to the room volume V and inversely proportional to the absorption in
the room as
T60 = 24ln(10)V
c αSabineA
(31.4)
in which αSabineA quantiﬁes the total sound absorption over the room’s surface area A, and c is the
speed of sound.
It is useful to note that the concept of reverberation time applies to the diffuse sound ﬁeld in a
room and is therefore independent of everything except the room itself. On its own, it gives little or no
Table 31.2 Illustrations of Typical Reverberation Time in Examples of Various Acoustic
Environments
Example
Typical reverberation time T60 (s)
Ofﬁce
0.3
Classroom
0.5
Concert hall
1.7
Large church
4–8

4.31.4 Measurement of Reverberation
889
125
250
500
1000
2000
4000
0
1
2
3
4
5
6
7
8
9
10
Reverberation Time (s)
Frequency (Hz)
Westminster Abbey
Canterbury Cathedral
FIGURE 31.4
Reverberation time as a function of frequency of Canterbury Cathedral and Westminster Abbey—after [20].
information concerning the clarity of a sound source heard in the room since this also depends on the
source-microphone distance as well as the characteristics of the source and the sound sensor.
4.31.4.2 The energy decay curve
The EDC shows the distribution of energy in the RIR as a function of time. To compute the EDC requires
that the RIR is available. It can then be computed using the Schroeder integral [13] as
EDC(t) =
 ∞
t
h2(τ)dτ.
(31.5)
An example of an EDC of a measured impulse response from the MARDY database [22] is shown in
Figure 31.5.
The measurement of T60 was discussed in Section 4.31.4.1. A convenient alternative way to measure
the T60 uses the EDC. The T60 can be obtained from the EDC provided that effects due to the direct
path propagation can be avoided—either by ignoring that early part of the curve associated to the
direct path and strong early reﬂections or by computing the EDC from a RIR measured with large
source-microphone distance so that the microphone captures only a diffuse ﬁeld. Taking these various
factors into account, the T60 can be obtained from the slope of the free decay section of the EDC, that

890
CHAPTER 31 Dereverberation
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
1
0
1 x 10
4
Time (s)
Impulse Response
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
50
40
30
20
10
0
Time (s)
Energy Decay Curve (dB)
FIGURE 31.5
Room impulse response from the MARDY database and its corresponding EDC.
being the part having near constant gradient as seen in Figure 31.5. The estimate of the slope gives the
reverberation decay in dB/s such that the time for 60 dB of decay can be straightforwardly calculated.
4.31.4.3 Channel-based measures
4.31.4.3.1
Overview
If the RIR is available, it is then possible to extract several characteristics of the room acoustics directly.
The EDC described in Section 4.31.4.2 is an example of a channel-based measure already discussed.
Other relevant measures aim to quantify the relative strength of the direct sound to the reverberation and
are formulated in a variety of ways including the Direct-to-Reverberant Ratio (DRR), the Early-to-total
Sound Energy Ratio and the Early-to-late Reverberation Ratio.
In the context of dereverberation, these measures would be applied to the RIR before dereverberation
processing and then to the equalized RIR after dereverberation processing. The change in the measure
would be indicative of the effect of the processing on the level of reverberation. As mentioned previously
however, it is important that these measures are not employed in isolation but only in combination with
a speech quality measure such as, for example, PESQ in the case of speech signals. By considering

4.31.4 Measurement of Reverberation
891
simultaneously the speech quality as well as the dereverberation measure, it is possible to guard against
misinterpretation of improvements due to dereverberation when this is at the expense of speech quality.
4.31.4.3.2
Direct-to-reverberant ratio
The DRR is one of the most intuitive measures of reverberation. The energy of the coefﬁcients of
the RIR representing the direct path indicate the level of the dry desired signal whereas the energy of
the coefﬁcients of the RIR representing the late reverberation indicate the level of the undesired wet
component of the signal. It is therefore intuitive to score a signal according to the ratio of these two
energies. Considering a single RIR of L coefﬁcients in length and denoting the direct path coefﬁcients
as h(n), n = 0, 1, . . . , nd−1 and the late reverberation coefﬁcients h(n), n = nd, . . . , L −1, then the
DRR is given by
DRR = 10 log10
 nd−1
n=0 h2(n)
L−1
n=nd h2(n)
	
dB.
(31.6)
There is some ambiguity in the deﬁnition of the term “direct” in this context. It is most straightforward
to consider the direct path as only those taps associated with the direct path propagation.2 However,
it is common to choose a wider range for the “direct” path corresponding to a range of coefﬁcients
include those representing the actual direct path propagation as well as those representing the strong
early reﬂections. A common choice would be to choose nd similarly to ne in (31.2) resulting in nd
fs =
30–60 ms.
It is important to note that, unlike the T60, the DRR does depend on the distance from the source to
the microphone as well as on the reverberation time of the room.
4.31.4.4 Signal-based measures
4.31.4.4.1
Overview
It has been shown in Section 4.31.4.3 that, when the RIR is available, it is advantageous and natural to
compute measures of reverberation using this information. It has also been mentioned that the effect
of a dereverberation algorithm can be quantiﬁed in terms of the difference between such computed
measures before and after dereverberation processing.
There are some cases, however, when the RIR is not available such as when algorithms of the type
of [23–25] are applied. In these cases, it is necessary to employ measures which operate using the
input and output signals only. Examples of such measures include the Log Spectral Distortion [26], the
Bark Spectral Distortion [27], the Reverberation Decay Tail [28], and the Signal-to-Reverberation Ratio
(SRR) [29]. The strong advantage of the SRR is that, if computed using the correct normalization, it
corresponds to the calculation of the DRR but does not required the RIR to be available. The SRR will
be further discussed below.
2The direct path would correspond to a single pulse in the RIR in continuous time or if the direct path propagation time
corresponds to an integer number of sampling periods. In general, in discrete time, the direct path would be represented by a
sinc function centered on the precise (and likely noninteger) propagation delay from source to sensor.

892
CHAPTER 31 Dereverberation
4.31.4.4.2
Signal-to-reverberation ratio
The SRR is an intrusive measure of reverberation that does not require the RIR but instead makes use
of the reverberant signal and, importantly, the source signal after propagation through the direct path.
This direct path component, sd(n), is easily available when dealing with, for example, computer-based
simulations. The requirement for the direct path component is common to most intrusive signal-based
measures. In typical use, the SRR is computed before and after dereverberation processing and the
change in SRR indicates the effect of the processing.
The SRR can be written as
SRR = 10 log10

∥sd∥2
2
∥ˆs −sd∥2
2
	
dB,
(31.7)
where, for Ls being the length of the signal segment of interest, sd = [sd(0), sd(1), . . . , sd(Ls −1)]T
is the direct path component and the dereverberated signal is ˆs
=
(ˆsd + xr) and xr
=
[xr(0), xr(1), . . . , xr(Ls −1)]. The main concept is that the numerator of the expression in (31.7)
indicates the energy of the direct path component of the signal whereas the denominator indicates
the corresponding reverberant component energy. It may often be convenient to compute SRR on a
segmental basis and average the segmental SRR values over some time window.
A dereverberation algorithm may affect either or both of the direct path signal component sd(n) and
the reverberant component xr(n). A straightforward way to model this is by formulating the derever-
berated speech as
ˆs(n) = γ sd(n) + ¯xr(n),
(31.8)
where γ is a scalar constant and ¯xr(n) is the attenuated reverberant component. Factoring (31.8) into
(31.7) leads to the deﬁnition of the Normalized Signal-to-Reverberation Ratio (NSRR)
NSRR = 10 log10
⎛
⎜⎝
∥sd∥2
2


1
γ

ˆs −sd

2
2
⎞
⎟⎠dB.
(31.9)
In [5,29] it is shown that, over a reasonable range of DRR, the NSRR corresponds accurately to the
DRR if γ is chosen as the solution to the least squares minimization problem
γls = arg min
γ
∥ˆs −γ sd∥2
2
(31.10)
leading to
γls = sT
d ˆs
sT
d sd
.
(31.11)
4.31.5 Spatial ﬁltering for dereverberation
4.31.5.1 Concept overview
The concept of spatial ﬁltering is to capture signals arriving from one speciﬁc directions in space, known
as the look direction, while attenuating signals arriving from other directions. Spatial ﬁltering is most

4.31.5 Spatial Filtering for Dereverberation
893
FIGURE 31.6
The Wing microphone array.
often realized in acoustic signal processing using an array of two or more microphones, as shown in
the example pictured in Figure 31.6, for which the microphone output signals are combined using a
beamforming algorithm [30]. This approach is among the earliest and most intuitive multichannel signal
processing approaches for speech enhancement, allowing the microphones in an array to focus on the
desired sound in the look direction while to some extent ignoring reverberation and noise arriving
from other directions. A general diagram of a beamforming approach to dereverberation is given in
Figure 31.7 which shows sound waves arriving at the M microphones with an angle of incidence of
φ, and the microphone signals xm(n) being processed by functions fm and then combined to form the
output y(n). Beamforming algorithms may be of either a ﬁxed beam type or adaptive.
4.31.5.2 Delay-and-sum beamformer
A baseline beamforming technique is the Delay-and-Sum Beamformer (DSB) in which the microphone
signals are delayed relative to each other to compensate for different times of arrival and then weighted
and summed [31,32]. The output of an M-channel DSB can be written in the form
x(n) =
M

m=1
wmxm(n −τm)
(31.12)

894
CHAPTER 31 Dereverberation
Incident 
sound 
waves
Microphones
Adaptive beamformer
Adaptive algorithm
FIGURE 31.7
Microphone array and beamforming algorithm for dereverberation.
in which wm and τm are, respectively, the weighting factors and delays applied to the mth microphone
signal. The coherent signal components associated with the direct path propagation of the sound to each
microphone are added constructively while the incoherent noise and reverberation signal components
are attenuated.
The effect of the DSB can be seen as forming a beam of sensitivity in the look direction. It can be
understood from this spatial ﬁltering interpretation that dereverberation using a spatial ﬁltering approach
is only effective for strongly localized sources. Its effectiveness when the soundﬁeld is diffuse will be
much less. Fortunately, in important applications such a hands-free telecommunications terminals, the
source is usually a particular talker whose sound output is indeed strongly localized. It can also be seen
that reliable algorithms are needed for steering of the beam in the spatial ﬁltering approach. The beam
steering is usually performed using information obtained from a Direction-of-Arrival (DOA) estimator
[33,34]. The level of dereverberation that can be achieved using the DSB has been analyzed in [35].
Further developments of the DSB can be advantageously employed in dereverberation. For example,
the delay elements in the DSB can be extended into Finite Impulse Response (FIR) ﬁlters in order
to form a ﬁlter-and-sum beamformer [30,36]. A frequency subband approach can also be employed
[37] in which the signals are co-phased in each frequency band and the gain is adjusted based on the
cross-correlation between the channels to remove incoherent components before the summation.
4.31.5.3 Beamforming in 2D and 3D
Two-dimensional microphone arrays permit sounds to be localized, tracked, and dereverberated in terms
of azimuth and elevation, in contrast to linear arrays which can only resolve the DOA of sounds in the

4.31.5 Spatial Filtering for Dereverberation
895
FIGURE 31.8
The em32 Eigenmike® microphone array by mh acoustics.
dimension of the plane of the microphone array and source. An example was described in [38] which
extends the basic DSB concept with a “track-while-scan” approach where the area under consideration is
quantized into overlapping regions that are scanned sequentially. Additional information on the speech
characteristics can be further incorporated into any beamforming approach to distinguish a speech
source from noise, as was also included in [38]. Two-dimensional microphone arrays nevertheless have
an intrinsic ambiguity between the spaces separated by the plane of the microphones, for example a
front-back ambiguity from a vertical planar array.
The extension to three-dimensional arrays has also been considered in both rectilinear and spherical
geometries [39,40]. An example of a spherical microphone array, the em32 Eigenmike® microphone
array by mh acoustics is pictured in Figure 31.8. A three-dimensional microphone array can be used to
obtain improved control over room acoustics and therefore offers better dereverberation performance
by forming additional spatial ﬁltering in the directions of the strong initial reﬂections [41].
4.31.5.4 Further advances in beamforming
To improve the performance of beamforming technology over the levels that can be achieved with a
DSB, advances in beamformer algorithms have been achieved in which multiple linear constraints are
imposed. These beamformers are typically known as Linearly Constrained Minimum Variance (LCMV)
beamformers [42] and operate by minimizing the power of the beamformer output signal with respect to
spatial and spectral constraints [43]. These constraints can be applied in the adaptive algorithm indicated

896
CHAPTER 31 Dereverberation
in Figure 31.7. The case that applies a single constraint in the look direction toward the desired source
is a well-known special case referred to as the Minimum Variance Distortionless Response (MVDR)
beamformer [44].
The Jim and Grifﬁths beamformer, known also as the Generalized Sidelobe Canceler (GSC) [45],
provides an efﬁcient way to implement a LCMV beamformer. Schemes which operate in frequency
subbands can exhibit signiﬁcant advantages [46].
An exciting recent development comes from studying the trade-off between the amount of speech
distortion introduced by the beamformer versus the interference rejection achieved [47]. This has led
to the development of a speech distortion and interference rejection constraint (SDIRC) beamformer
that minimizes the ambient noise power. The minimization of unwanted signal components is per-
formed subject to constraints that can be controlled to adjust the tradeoff between speech distortion and
interference-plus-noise reduction on the one hand, and undesired signal and ambient noise reductions
on the other hand.
4.31.6 Speech enhancement methods for dereverberation
4.31.6.1 Concept overview
Speech enhancement is the name used to refer to any processing that improves either speech quality,
speech intelligibility, or both. Well-established models of speech have been developed over many years
[48] that capture the key characteristics of speech, using such models, in a relatively small number of
parameters. It is clear that reverberation is likely to cause the speech signal to deviate from a clean speech
model and such deviation can give the basis of methods to perform speech enhancement processing for
dereverberation. The use of the Linear Predictive Coding (LPC) model and its prediction residual is
discussed in Section 4.31.6.2.
It is additionally feasible to employ models of the degrading process instead of, or as well as, models
of speech. The most common model underlying speech enhancement processing models the received
signal as containing some combination of the source signal together with additive noise. This has led
to a rich body of research into noise reduction algorithms that aim to estimate a de-noised signal from
the noisy signal usually by ﬁrst estimating the noise and then subtracting it from the noisy signal. This
type of processing is usually performed in the frequency domain and referred to generally as spectral
subtraction. Early examples of such algorithms are [49,50] with subsequent developments presented in
[51–54], to list but a few.
Although reverberation is a convolutive distortion as shown in (31.1), rather than an additive dis-
tortion, it can nevertheless also be considered that the received signal is the sum of the direct path
component plus the reverberant component. Hence, an algorithm that could subtract the reverberant
component would be a suitable estimator for the source signal. A key difference in the case of reverber-
ation is that the degradation is caused by addition of a signal that is highly correlated with the source
signal whereas, in standard noise reduction, the degradation is caused by addition of, normally, an
uncorrelated noise signal. This inﬂuences the manner in which the degradation can be modeled and
estimated.

4.31.6 Speech Enhancement Methods for Dereverberation
897
4.31.6.2 Methods based on linear prediction and harmonicity
The method of LPC is a well-known signal processing technique [55] that models a signal as the output of
an all-pole quasi-stationary linear ﬁlter, sometimes termed the LPC ﬁlter, driven by an excitation signal,
sometimes termed the prediction residual. In the area of speech processing, the LPC ﬁlter represents
the vocal tract while, for voiced speech, the prediction residual represents to a ﬁrst approximation the
action of the vocal folds [48]. The process of LPC is most often applied to telephone speech captured
with a close-talking handset microphone. However, in the context of dereverberation, LPC is applied to
a reverberant speech such as may be captured by a distant microphone in hands-free telephony. It can
be observed that the prediction residual obtained when applying LPC to reverberant speech contains
much of the effect of reverberation, manifest as additional peaks due to the reverberation, as well as
the expected peaks corresponding to the closures of the vocal folds. Examples of the LPC residual
of clean and reverberant speech are illustrated in Figure 31.9(a) and (b) respectively. In contrast, the
reverberation has little effect on the LPC ﬁlter in the single-channel case [56] and has almost no effect
in the multichannel case [57].
The steps for dereverberation are ﬁrst to obtain the LPC ﬁlter of the reverberant speech and the
correspondingreverberantpredictionresidual. Thenextstepafterhavingobtainedthepredictionresidual
is to apply dereverberation processing on the residual and ﬁnally to resynthesize speech by applying
the processed prediction residual to the input of the LPC ﬁlter.
The technique of [58] uses adaptive minimization of the kurtosis of the residual to reduce the level
of reverberation in the synthesized speech. This technique has been further developed and combined
with spectral subtraction as will be discussed in Section 4.31.6.3. A more recent method to reduce the
level of reverberation by processing the prediction residual is to apply spatiotemporal averaging such
as described in [59,60].
In an approach based on harmonicity, speech is modeled as the combination of a pitch-periodic
component together with an harmonic component comprising a set of sinusoids. It is shown in [61] that
a ﬁlter that enhances the harmonic structure of reverberant speech signals is a good approximation to the
inverse ﬁlter of the reverberation process. This concept is known as Harmonicity-based dEReverBeration
(HERB) and is further developed in [62] into two speciﬁc techniques to design a dereverberation
(a)
(b)
FIGURE 31.9
Examples of LPC residual signal from (a) clean speech and (b) reverberant speech (after [5]).

898
CHAPTER 31 Dereverberation
ﬁlter—one that uses transfer function averaging and one that performs a minimization of a mean squared
error function.
4.31.6.3 Methods based on spectral subtraction
Statistical models for the RIR can be used as part of the procedure for estimation of the reverberant
component. A commonly used model is of the type proposed by Polack [14] in which
h(n) =

b(n)e−ζn
for n ≥0
0
otherwise
(31.13)
in which b(n) represents Gaussian distributed zero-mean noise and ζ is given by
ζ = 3 ln (10)
T60 fs
.
(31.14)
This model can be employed, as in [63], to estimate the short-time spectral magnitude of the rever-
beration. Subsequent subtraction in the spectral magnitude domain is then used to dereverberate the
signal. It is unavoidable that this method requires knowledge of, or an estimate of, the T60. Nevertheless,
non-intrusive T60 estimation is an active research topic and several techniques are currently available
including [64,65]. The spectrum of the direct path component of a reverberant speech signal can also
be estimated using the Log Spectral Amplitude estimator presented in [66] which employs a Minimum
Mean Square Error criterion.
Theabovemethodsrequireonlysingle-channelsignalsinordertooperate.Whenmultichannelsignals
are available from a microphone array, it has been discussed in Section 4.31.5 how spatial ﬁltering using
beamforming algorithms, such as the DSB or MVDR beamformer, can be used to attenuate reﬂections
that arrive at the array other than in the look direction. As was pointed out, the attenuation of the
reverberation by spatial ﬁltering depends signiﬁcantly on the distribution of reverberant sound in the
room and the capability of the beamformer for spatial selectivity. Accordingly, it makes sense to apply a
single-channel dereverberation processing algorithm to the output of the spatial ﬁlter, thereby boosting
the overall dereverberation performance. An example would be to exploit the Log Spectral Amplitude
estimator developed in [66] as detailed in [5]. A further approach exploiting a speciﬁc nonlinear spatial
processor was developed in [26] to be used in combination with the Log Spectral Amplitude estimator.
As would be expected, this combination of spatial ﬁltering and single-channel enhancement reduces
reverberation perceptually more than the single-channel spectral enhancement alone and, in addition,
is reported also to reduce the spectral coloration due to early reﬂections.
A two-stage dereverberation approach is proposed in [67] that can operate on single-channel rever-
berant signals. This approach is interesting because it uses separate processing to reduce coloration by
way of inverse ﬁltering and to reduce later reverberation by way of spectral subtraction. The inverse
ﬁlter follows the same design philosophy as presented in [58] in which an inverse ﬁlter is found by
maximizing the kurtosis of LPC residual signal after applying LPC to the reverberant speech signal. It
is then assumed that the power spectrum of late reverberant component of the signal is a smoothed and
shifted version of the power spectrum of the inverse-ﬁltered speech.

4.31.7 Acoustic Channel-Based Methods for Dereverberation
899
4.31.7 Acoustic channel-based methods for dereverberation
4.31.7.1 Concept overview
It is known that the cause of reverberation is the multipath propagation of sound from the source
to the listener or microphone, comprising the direct path together with many paths involving one or
more reﬂections off walls, objects, etc. in the room. The effect of such multipath propagation can be
represented as a ﬁltering operation in which the source signal is convolved with the point-to-point
RIR corresponding to the acoustic channel between the source and microphone positions, as shown in
(31.1). In the acoustic channel-based approach to dereverberation, the concept is to undo the effect of
the acoustic channel by the use of a multichannel inverse ﬁlter. The multichannel inverse ﬁlter may, in
this sense, be seen as an equalizer for the acoustic channel. The system diagram describing this approach
is shown in Figure 31.10.
This approach can be seen immediately to present some very signiﬁcant challenges for algorithm
design and implementation. Firstly, it is necessary to determine or estimate the multipath point-to-point
acoustic channel from the source to the microphone. This would normally be represented in the form
of a set of estimated RIRs. The estimation problem is hard for the reasons as will be discussed below
in Section 4.31.7.2. Secondly, it is necessary to design and implement an inverse ﬁlter, or equalizer, for
Blind System 
Equalization 
Algorithm
Room 
Impulse 
Responses
Additive 
Noise
Multichannel 
Equalizer
FIGURE 31.10
Blind system identiﬁcation and multichannel equalization.

900
CHAPTER 31 Dereverberation
the estimated acoustic channel. This problem is also hard for the reasons as will be discussed in Section
4.31.7.3, and in most cases it is actually impossible.
Many researchers have been working toward a better understanding of these issues in recent years.
In parallel to, and fueled by, this fundamental research, methods have been developed to ﬁnd practical
approximate dereverberation solutions that work around the difﬁculties of the exact problem. One such
approach which is making signiﬁcant progress is that of channel shortening, as will be discussed below
in Section 4.31.7.3.4.
This concept, and the approaches arising from it, are particularly interesting because it aims to
exploit learning behavior of algorithms to estimate the acoustic properties of the room without the need
to render any particular sound to probe the room. The algorithms therefore operate without knowledge
of or control over the source signals. Such algorithms are referred to a blind system identiﬁcation
algorithms. One might also consider systems in which the generation of controlled sounds in the room
might be feasible, perhaps during a calibration procedure. In such cases, supervised or semi-supervised
system identiﬁcation algorithms could be employed and would likely be more robust to noise and
more accurate. Nevertheless, the blind case is the most general and will be the focus of the following
discussion.
4.31.7.2 Blind SIMO acoustic system identiﬁcation
4.31.7.2.1
Cross-relation methods
The aim of blind SIMO acoustic system identiﬁcation is to estimate the acoustic system comprising
the RIRs from the single source to the multiple microphones. Such a system is termed a Single-Input-
Multiple-Output (SIMO) system. It is interesting to ask the question “what is known about this SIMO
system?” The answer is “very little!” The RIRs are not known, the source signal s(n) is not known and
the additive noise υm(n) is not known. It may be possible to assume that the source signal is speech, for
which some reasonable models exist. The only certain fact that can be exploited is that the microphone
signals xm(n) all originate from the same source signal s(n). Algorithms that exploit this fact are usually
formulated in terms of the cross-relation and the cross-relation error.
The common formulation of the blind multichannel system identiﬁcation problem using second-
order statistics employs the cross-relation between two microphone signals x1(n) and x2(n) and the
corresponding two RIRs h1 and h2. In [68] the cross-relation is given by
x1 ∗h2 = s ∗h1 ∗h2 = x2 ∗h1
(31.15)
with ∗indicating convolution, for which the solutions for h1 and h2 can be found only using the
microphone signals x1 and x2. The time index n has been temporarily dropped for clarity. In the noise-
free case, and generalizing from 2 to M microphones, the solutions for h1, h2 up to hM come from
solving the system of equations
Rh = 0,
(31.16)
where R is a correlation-like matrix formed from the microphone signals and speciﬁcally deﬁned
in [69],
h = [hT
1 hT
2 · · · hT
M]T
(31.17)
is a vector of the concatenated RIRs, superscript T indicates the matrix transpose operation and 0 is a null
vector. For the noise-free case and assuming exact modeling, the solution to (31.16) is the eigenvector

4.31.7 Acoustic Channel-Based Methods for Dereverberation
901
corresponding to the zeroth eigenvalue of R. For the case in which the microphone signals contain noise
as shown by υ in Figure 31.10, the solution to (31.16) is found from the eigenvector corresponding to
the smallest eigenvalue.
Several methods of solution have been proposed including a least squares approach in [68] and an
eigendecomposition method in [70]. Gannot and Moonen [71] use eigendecomposition methods for
blind system identiﬁcation both in the full-band case and in frequency subbands. Aside from closed-
form solutions, a pivotal development came from Huang and Benesty who proposed adaptive ﬁlter-based
solutions that enable online algorithms to be developed with the capability to identify and track time-
varying acoustic systems. They derived multichannel LMS and Newton adaptive ﬁlters both in the time
domain [72,73] and in the frequency domain [69]. The Normalized Multichannel Frequency Domain
Least Mean Square (NMCFLMS) algorithm is often used and can be considered as a “work horse”
algorithm for this task.
Recent advances in the blind SIMO system identiﬁcation task include the use of a quasi-Newton
method of solution [74] and the novel use of afﬁne projection algorithms for blind system
identiﬁcation [75].
In order for the solution of (31.16) to exist, the so-called identiﬁability conditions must be
satisﬁed [68]:
1. There must be no zeros common to all M channels.
2. The correlation matrix of the source signal must be full rank.
At this point, it is worth considering the considerable difﬁculties and challenges that blind acoustic
system identiﬁcation algorithms have to overcome.
1. Acoustic channels are normally time-varying and therefore system identiﬁcation must be performed
adaptively.
2. RIRs have a duration typically corresponding to thousands of coefﬁcients and estimation of systems
with such high order requires robust algorithms with high numerical precision and which typically
present high computational requirements.
3. Noise in the observations can cause the adaptive algorithms to misconverge. Some studies of this
phenomenon have been performed [76,77] and it has been shown that the effect of the noise can
be to impose a common ﬁlter onto all of the estimated channels [78]. Several methods have been
developed to improve robustness [73,76,79,80].
4. Many approaches assume knowledge of the order of the unknown system. The order estimation
issue has been addressed in, for example, in [71,81].
5. Solutions for h are normally found only to within a multiplicative scale factor [68,71].
4.31.7.2.2
Alternative methods
The cross-relation approaches described in Section 4.31.7.2.1 exploit second-order statistics of the
microphone signals in order to form the correlation-like matrix R in (31.16). Approaches can alterna-
tively exploit higher order statistics such as [82] that uses the cepstrum for blind system identiﬁcation
between two channels. It is shown that the channels can be reconstructed from their phases using an
iterative approach, where the phases are identiﬁed from the cepstra of the observed data [83] but that
the method is sensitive to zeros close to the unit circle as are often present in acoustic systems [84].

902
CHAPTER 31 Dereverberation
The method shown in [85] employs multichannel LPC to whiten the input signal and subsequently
multichannel linear prediction is used to identify the channels.
While it is most common to employ FIR models of acoustic impulse responses, the approach in [86]
uses Autoregressive (AR) Inﬁnite Impulse Response (IIR) models for the room impulse responses. An
advantage of using AR models of the channel is that the channel order is substantially reduced compared
to FIR channel models, by up to two orders of magnitude. The method then exploits the assumption
that the source signal is a locally stationary AR process but that it is globally nonstationary. In this way,
the parameters of an all-pole channel ﬁlter can be identiﬁed by observing several frames of the input
signal and collecting information regarding the poles either by using a histogram approach or a more
robust Bayesian probabilistic framework. Over several frames, the poles due to the (assumed) stationary
channel become apparent and the channel can thus be identiﬁed.
Considering all the current approaches available, sensitivity to noise and channel order estimation
are commonly problematic and a topic of ongoing research.
4.31.7.3 Inverse ﬁltering
4.31.7.3.1
Introduction to the task
The task of inverse ﬁltering in this context is to apply a linear and normally multichannel ﬁlter to
the microphone signals in order to recover the source signal by a process known as (multichannel)
equalization.WithreferencetoFigure31.10,westartbyassumingthattheRIRsh areeitherknownorcan
be estimated with sufﬁcient accuracy, for example using blind system identiﬁcation. The multichannel
inverse ﬁlter g is then designed by an equalization algorithm and applied in the multichannel equalizer
such that the sum of the outputs of g1, g2, . . ., gM forms ˆs(n), which is the estimate of the source
signal s(n). For clarity of explanation, the time-index n has been temporarily omitted for h and g.
However, in practice, the RIRs h vary with time and hence the multichannel inverse ﬁlters have to vary
correspondingly. The impact on g of time-varying h is very signiﬁcant and was investigated in [87].
4.31.7.3.2
Exact inverse ﬁlter design
Given the acoustic SIMO system h deﬁned in (31.17) and the microphone signals xm(n), for
m = 1, 2, . . . , M, it is potentially possible to perform exact dereverberation using an inverse sys-
tem g as shown in Figure 31.10, constructed in a corresponding form to h in (31.17) and comprising
ﬁlters gm that satisfy
hT g = κδ(n −τ),
(31.18)
where κ and τ are an arbitrary scale factor and delay. It is noted again that any time variation of h has
been temporarily omitted for clarity.
In the single-channel case, when seeking a solution to
hT
mgm = κδ(n −τ)
(31.19)
the direct approach to the solution of (31.19) is, as it turns out, too simplistic for the following reasons.
1. The duration of the RIR is very high, typically comprising thousands of coefﬁcients, and the numer-
ical precision required to perform the inversion of such high order systems usually exceeds the
precision available.

4.31.7 Acoustic Channel-Based Methods for Dereverberation
903
2. Direct inversion is problematic because acoustic channels typically exhibit nonminimum-phase
characteristics such that causal stable channel inverses do not in general exist.
3. The acoustic channels may often exhibit deep spectral nulls that, after inversion, give rise to signif-
icant narrow-band noise ampliﬁcation.
However,inthemultichannelcase,(31.18)canbesolvedexactlyusingtheMultiple-input/outputINverse
Theorem (MINT) algorithm [88], subject to there existing no zeros common to all channels and the
length of the inverse ﬁlter being appropriately chosen. MINT can be used either in its original form
or else in a subband form [89], or in an adaptive version such as discussed in [90]. Unfortunately the
MINT algorithm, while offering an exact solution, has been found to introduce signiﬁcant problems
when employed in practical scenarios in which only inaccurate estimates of the RIR are available, and
cannot normally be used. This has motivated research to seek out alternative and improved methods as
described in Section 4.31.7.3.3.
4.31.7.3.3
Alternative approaches for inverse ﬁlter design
It is common to formulate the problem as a least squares inverse ﬁlter design for example [91,92] as
the solution to the optimization problem
ˆgm = min
gm ∥hT
mgm −δ(n −τ)∥2
2.
(31.20)
Decomposition of the impulse response into a minimum phase component and an all-pass component
has also been exploited in homomorphic approaches [91,93,94]. This permits the magnitude and the
phase of the acoustic equalization to be handled somewhat independently, though it is important to note
that equalization of the magnitude only results in audible distortion on the output [93,95].
For the multichannel case, in practical situations only an estimate ˆh is available and this may deviate
by an error e from the true acoustic system h such that
ˆh = 1
γ

h + e

,
(31.21)
where e = [eT
1 eT
2 · · · eT
M]T is a vector of the errors in the estimated coefﬁcients and γ is a normalizing
gain factor.
In addition, the length of the acoustic system may not be precisely known. In such cases the exact
solution offered by the MINT algorithm is not usually of practical value since, although it is an exact
inverse of ˆh, it is far from being a useful inverse of h. Inverse ﬁltering using MINT in these circumstances
usually serves to add reverberation, rather than reduce it. Since the MINT algorithm is not usable in
practical cases when only an estimate ˆh is available, alternative approaches have been developed. For
example, it can be advantageous in such situations to apply a certain amount of regularization such as
in the regularized p-MINT algorithm [96].
The presence of noise can also severely degrade the performance of inverse ﬁlter design and so noise
robust techniques have been investigated including [97].
4.31.7.3.4
Channel shortening
A further approach to inverse ﬁltering aims not to equalize the acoustic system completely but instead
to shorten the effective duration of the RIR by suppressing the later coefﬁcients. It can be seen then that

904
CHAPTER 31 Dereverberation
channel shortening is used to remove audible effects of late reverberation due to the tail of the RIR while
not attempting to suppress inaudible effects due to early reﬂections. The approach relaxes the design
constraints on an equalization ﬁlter [13,98,99]. The intention behind the use of channel shortening
instead of exact equalization is that the relaxation of the design constraints will result in higher levels
of robustness to additive noise and system identiﬁcation error e as shown in (31.21).
The approach to channel shortening for dereverberation is developed in [100]. Starting from the
deﬁnition of the RIRs given in (31.17), normally obtained from blind system identiﬁcation as described
above, a channel shortening equalizer can be found by maximizing the Rayleigh quotient given by
[98,101]
g = arg max
g
gT Bg
gT Ag,
(31.22)
where
B = HT diag{wd}T diag{wd}H
(31.23)
and
A = HT diag{wu}T diag{wu}H
(31.24)
given that H = [H1H2 · · · HM] and, for equalizer length Li taps, Hm is an (L +Li −1)×Li convolution
matrix of hm. The vectors wd and wu are weight vectors that deﬁne the desired and undesired regions
of the equalized RIR, respectively. The weight vectors can be written as
wd = [0 · · · 0
  
τ
1 · · · 1
  
Lw
0 · · · 0]T
(L+Li−1)×1
wu = 1(L+Li−1)×1 −wd
for which τ is a delay in samples, optionally chosen, and Lw deﬁnes the window area over which part
of the RIR the maximization of the Rayleigh quotient takes place.
ThemaximizationoftheRayleighquotientin(31.22)canbeobtained[101]bysolvingthegeneralized
eigenvalue problem
Bg = λAg
(31.25)
and selecting the eigenvector corresponding to the largest resulting eigenvalue. For rank deﬁcient A, Lw
solutions are obtained as the vectors corresponding to λ = ∞. It is seen that the solution is not unique.
In addition to the Lw channel shorting solutions obtained from the solution of (31.25), any linear
combination of these solutions is also a channel shortening solution. One such solution is the MINT
solution as described above.
Another important method of solution is the Relaxed Multichannel Least Squares (RMCLS) algo-
rithm [100]. This algorithm designs a channel shortening equalizer using a relaxed target function for
the equalized RIR. In the case of MINT, the target function for the equalized RIR is a unit impulse at
delay τ = 0. In general the target equalized impulse response can be arbitrarily chosen and written as
d = [d(0) · · · d(L + Li −2)]T . The RMCLS algorithm uses a relaxation window
w = [1 · · · 1
  
τ
10 · · · 0
  
Lr
1 · · · 1]T
(L+Li−1)×1
(31.26)

4.31.7 Acoustic Channel-Based Methods for Dereverberation
905
showing the offset and range of relaxation τ and Lr, respectively. Now deﬁning
Hm =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
hm,0
0
· · ·
0
hm,1
hm,0
· · ·
0
...
...
...
...
hm,L−1
· · ·
...
...
0
hm,L−1
...
...
...
...
...
...
0
· · ·
0 hm,L−1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
(31.27)
and
H = [H1 · · · HM]
(31.28)
the cost function minimized in RMCLS is written as
J = ∥W(Hg −d)∥2
2,
(31.29)
where W = diag{w} is a diagonal weighting matrix with w = [w0 · · · wL+Li−2]T , wi ̸= 0 ∀i, that is
minimized by
g = (WH)†Wd
(31.30)
using the Moore-Penrose pseudo-inverse [102] indicated by (·)†.
The main idea behind the RMCLS is that the samples of the equalized RIR in the range of relaxation
are unconstrained since the weighting function in (31.26) contains zero-valued weights within the range
of the relaxation window.
An example of the performance of a channel-shortening method is shown in Figure 31.11. The blue
curve is the EDC for h1, the ﬁrst channel of two-channel acoustic system measured in a real room
from the MARDY database [22]. This has length of L = 2000 samples corresponding to 250 ms for a
sampling frequency fs of 8000 Hz. System identiﬁcation has been performed and, in this simulation,
the system identiﬁcation error was set to −33 dB Normalized Projection Misalignment (NPM) [103].
The green curve shows that the MINT algorithm introduces in the region of 10–20 dB of additional
reverberation energy compared to the unprocessed microphone signal. The other curves show the EDCs
for the CIT [104], the RMCLS [100] and hybrid CIT-RMCLS [104] algorithms in cyan, red, and purple
respectively. It can be seen that, for this level of NPM and without additive noise, the channel-shortening
methods can reduce the energy of the late reverberation by up to around 10 dB. The various channel-
shortening solutions give widely different results however in terms of the overall speech quality, often
tending to produce a “thin”-sounding output because of coloration or whitening.
In the development of channel shortening methods, it is useful to keep in mind the main aims which
are to ﬁnd an equalizer that outputs undistorted and dereverberated speech, in comparison with the
reverberant multichannel input, and that the design and performance of this equalizer should be robust
to both additive noise and system identiﬁcation error in the estimates of the RIR ˆh. A key question that
remains open in this context concerns to what degree the channel should be shortened by the equalizer
to obtain the best trade-off between robustness and quality of the dereverberated speech signal.

906
CHAPTER 31 Dereverberation
0
500
1000
1500
2000
2500
3000
3500
−60
−50
−40
−30
−20
−10
0
Time (samples)
EDC (dB)
 
 
h1
MINT
RMCLS
CIT
RMCLS−CIT
FIGURE 31.11
Energy decay curves showing dereverberation performance for channel-shortening methods.
Astudyundertakenin[105]investigatedthisquestionbyperformingasetofexperimentstodetermine
the speech quality after dereverberation as a function of the length of the relaxation window Lr. Two
situations were studied. In the ﬁrst case, the level of noise was varied while the system identiﬁcation
error was kept at zero. In the second case, the level of system identiﬁcation error was varied while the
level of noise was kept at zero. In both cases, the speech quality was estimated using PESQ [106] and
the system identiﬁcation error was measured in terms of NPM [103].
The ﬁrst of these two cases is shown in Figure 31.12. When the relaxation length is longer than
80 ms, the equalization does not affect the PESQ score since the solid lines (after equalization) are
convergent with the dashed lines (before equalization). When the relaxation window length is shorter
than around 40 ms, the equalization improves the speech quality when the Signal-to-Noise Ratio (SNR)
is better than about 40 dB and has no effect or degrades the speech quality when the SNR is worse
than about 30 dB. The second case is shown in Figure 31.13. When the relaxation window length is
between around 5 and 20 ms, the speech quality due to the equalization shows an improvement for all
levels of system identiﬁcation error. For poor system identiﬁcation with errors greater than around −10
to −20 dB NPM, the quality is degraded if the relaxation window is around 1–5 ms. For good system
identiﬁcation with errors smaller than around −30 dB NPM, signiﬁcant improvements in quality can
be obtained, the level of which increase with the accuracy of the system identiﬁcation.
The conclusion from this study is that successful application for dereverberation of the RMCLS
equalizer design, and likely for other similar approaches also, requires noise levels below around 40 dB
SNR and system identiﬁcation errors measuring better than −10 dB NPM. A rule of thumb for the choice
of the relaxation window length is Lr ≃10 ms is a good choice, leading to measurable improvements
in quality and dereverberation in many circumstances.

4.31.7 Acoustic Channel-Based Methods for Dereverberation
907
20
40
60
80
100
120
1
1.5
2
2.5
3
3.5
4
4.5
5
Lr (ms)
PESQ
 
 
10 dB
20 dB
30 dB
40 dB
Inf dB
FIGURE 31.12
PESQ as a function of Lr with variable SNR and ﬁxed system identiﬁcation error. The dashed lines indicate
before equalization and the solid lines indicate after equalization.
20
40
60
80
100
120
1
1.5
2
2.5
3
3.5
4
4.5
5
Lr (ms)
 
 
−10 dB
 
−20 dB
 
−30 dB
 
−40 dB
−Inf dB
PESQ
FIGURE 31.13
PESQ as a function of Lr with variable system identiﬁcation error and ﬁxed SNR. The dashed lines indicate
before equalization and the solid lines indicate after equalization.

908
CHAPTER 31 Dereverberation
4.31.8 Summary and conclusions
This chapter has aimed to provide a technical overview of the current state of dereverberation technology
as applied to room reverberation for speech and music signals. As well as illustrating the problem of
reverberation through some example applications, a few characteristics useful for measuring reverber-
ation have been highlighted. The methods for dereverberation have been organized into three classes
including approaches based on spatial ﬁltering, approaches based on speech enhancement and acoustic
channel-based approaches.
It can be seen from the chapter that dereverberation is a task with many challenges. The performance
of dereverberation is correspondingly limited. For example, reducing the late reverberation by around
10 dB while maintaining acceptable speech quality is a reasonable expectation from, for example, the
acoustic channel-based approaches. However, this assumes that additive noise is not present and that
several other constraints, as mentioned in the earlier part of the chapter, are also satisﬁed.
The future perspective for dereverberation is, based on current trends, that processing capability will
improve, though slowly. The most practical methods at present are those based on speech enhancement
approaches, although these are also limited in terms of overall speech quality and the requirements
for availability, or estimation of, the T60. If multichannel signals are available through the use of a
microphone array, spatial ﬁltering approaches are highly desirable because the level of any artifacts can
be carefully controlled or avoided completely. Needless to say, the combination of a microphone array,
beamforming algorithm, and a single-channel post-processor offers a higher level of dereverberation
performance than any of the constituent parts and examples of such techniques have been seen.
Of great interest will be to see further introduction and exploitation of environment awareness of
dereverberation algorithms through passive sensing of the room acoustics. Blind characterization of
an acoustic environment is a research topic with very high potential value but with very signiﬁcant
challenges yet to be overcome. Meeting those challenges, however, could lead to near-human capability
in terms of dereverberation through advances in the design of equalizing inverse ﬁlters.
Robustness to noise, acoustic system nonstationarity, and other practical factors currently limit the
deployment of dereverberation in commercial applications. A key objective of current research is to
develop dereverberation algorithms to a level of maturity and associated robustness that permits them
to be conﬁdently and beneﬁcially implemented in products.
List of Abbreviations
AR
Autoregressive
DOA
Direction-of-Arrival
DRR
Direct-to-Reverberant Ratio
DSB
Delay-and-Sum Beamformer
EDC
Energy Decay Curve
FIR
Finite Impulse Response. A ﬁlter whose output is a weighted sum of past input values
and whose system function contains only zeros and no poles
GSC
Generalized Sidelobe Canceler

References
909
HERB
Harmonicity-based dEReverBeration
IIR
Inﬁnite Impulse Response. A ﬁlter whose output is a weighted sum of both past input
and past output values and whose system function contains both poles and zeros
LCMV
Linearly Constrained Minimum Variance
LPC
Linear Predictive Coding. An autoregressive model of speech production
LTI
Linear Time Invariant
MINT
Multiple-input/output INverse Theorem
MVDR
Minimum Variance Distortionless Response
NMCFLMS
Normalized Multichannel Frequency Domain Least Mean Square
NPM
Normalized Projection Misalignment
NSRR
Normalized Signal-to-Reverberation Ratio
PESQ
Perceptual Evaluation of Speech Quality
RIR
Room Impulse Response
RMCLS
Relaxed Multichannel Least Squares
SIMO
Single-Input-Multiple-Output
SNR
Signal-to-Noise Ratio
SRR
Signal-to-Reverberation Ratio
T60
Reverberation Time
VoIP
Voice over Internet Protocol
Acknowledgments
The author thanks the many students and co-workers who have contributed to the development of this topic and
who have contributed to the material covered in this chapter. There are too many to mention but particular thanks
go to Nikolay Gaubitch, Emanuel Habets, Daniel Jarrett, Felicia Lim, Mark Thomas, and Wancheng Zhang.
Relevant Theory: Speech Processing
See this Volume, Chapter 34 Speech Production Modeling and Analysis
See this Volume, Chapter 35 Enhancement
References
[1] C. Cherry, On Human Communication, third ed., MIT Press, 1980.
[2] A.V. Oppenheim, R.W. Schafer, T.G. Stockham Jr., Nonlinear ﬁltering of multiplied and convolved signals,
IEEE Trans. Audio Electroacoust. AU-16 (3) (1968) 437–466.
[3] J.B. Allen, Speech dereverberation, J. Acoust. Soc. Am. 53 (1) (1973) 322.
[4] A.V. Oppenheim, R.W. Schafer, Digital Signal Processing, Prentice Hall, 1975.
[5] P.A. Naylor, N.D. Gaubitch (Eds.), Speech Dereverberation, Springer, 2010.
[6] M. Omologo, P. Svazier, M. Matassoni, Environmental conditions and acoustic transduction in hands-free
speech recognition, Speech Commun. 25 (1) (1998) 75–95.
[7] Mobile Operators Association, History of cellular mobile communications, 2005.
<http://www.mobilemastinfo.com/information/history.htm>.

910
CHAPTER 31 Dereverberation
[8] G. Schmidt, Applications of acoustic echo control—an overview, in: Proceedings of European Signal Pro-
cessing Conference EUSIPCO, Vienna, Austria, 2004, pp. 9–16.
[9] H.W. Löllmann, P. Vary, Low delay noise reduction and dereverberation for hearing aids, EURASIP J. Adv.
Signal Process. 2009 (2009) 1:1–1:9. <http://dx.doi.org/10.1155/2009/437807>.
[10] M. Boymans, S. Goverts, S. Kramer, J. Festen, W. Dreschler, Candidacy for bilateral hearing aids: a retro-
spective multicenter study, J. Speech Lang. Hear. Res. 52 (1) (2009) 130–140.
[11] S. Doclo, S. Gannot, M. Moonen, A. Spriet, Acoustic beamforming for hearing aid applications, in:
S. Haykin, K. Ray Liu (Eds.), Handbook on Array Processing and Sensor Networks, Wiley, 2008 (Chapter 9).
[12] P. Newell, Recording Studio Design, Elsevier Science, 2011.
[13] H. Kuttruff, Room Acoustics, fourth ed., Taylor and Francis, London, 2000.
[14] J.D. Polack, La transmission de l’énergie sonore dans les salles, Université du Maine, La Mans, France,
Ph.D. Dissertation, 1988.
[15] G. Defrance, J.D. Polack, Measuring the mixing time in auditoria, in: Proceedings of Acoustics’08, Paris,
2008, pp. 3871–3976.
[16] J.S. Bradley, H. Sato, M. Picard, On the importance of early reﬂections for speech in rooms, J. Acoust. Soc.
Am. 113 (6) (2003) 3233–3244.
[17] J.B. Allen, D.A. Berkley, Image method for efﬁciently simulating small-room acoustics, J. Acoust. Soc.
Am. 65 (4) (1979) 943–950.
[18] E.A.P. Habets, Room impulse response generator for MATLAB, 2010.
<http://home.tiscali.nl/ehabets/rirgenerator.html>.
[19] I. Kodrasi, S. Goetze, S. Doclo, Increasing the robustness of acoustic multichannel equalization by means of
regularization, in: Proceedings of the International Workshop Acoustics Signal Enhancement (IWAENC),
2012, pp. 161–164.
[20] W.R. Stevens, P.W. Barnett, Loudspeaker design for speech intelligibility in reverberant spaces, in:
Proceedings of University of Edinburgh, Institute of Acoustics Meeting, July 1982.
[21] W.C. Sabine, Collected Papers on Acoustics (Originally 1921), Peninsula Publishing, 1993.
[22] J. Wen, N.D. Gaubitch, E. Habets, T. Myatt, P.A. Naylor, Evaluation of speech dereverberation algorithms
using the MARDY database, in: Proceedings of the International Workshop Acoustics Echo Noise Control
(IWAENC), Paris, France, September 2006.
[23] N.D. Gaubitch, P.A. Naylor, D.B. Ward, On the use of linear prediction for dereverberation of speech, in:
Proceedings of the International Workshop Acoustics Echo Noise Control (IWAENC), 2003, pp. 99–102.
[24] S.M. Griebel, M.S. Brandstein, Wavelet transform extrema clustering for multi-channel speech dereverber-
ation, in: Proceedings of the International Workshop Acoustics Echo Noise Control (IWAENC), Pocono
Manor, Pennsylvania, September 1999, pp. 52–55.
[25] B. Yegnanarayana, P.S. Murthy, Enhancement of reverberant speech using LP residual signal, IEEE Trans.
Speech Audio Process. 8 (3) (2000) 267–281.
[26] E.A.P. Habets, Single- and multi-microphone speech dereverberation using spectral enhancement, Ph.D.
Dissertation, Technische Universiteit Eindhoven, 2007. <http://alexandria.tue.nl/extra2/200710970.pdf>.
[27] S. Wang, A. Sekey, A. Gersho, An objective measure for predicting subjective quality of speech coders,
IEEE J. Sel. Areas Commun. 10 (5) (1992) 819–829.
[28] J.Y.C. Wen, P.A. Naylor, An evaluation measure for reverberant speech using tail decay modelling, in:
Proceedings of European Signal Processing Conference (EUSIPCO), Florence, Italy, 2006, pp. 1–4.
[29] P.A. Naylor, N.D. Gaubitch, E.A.P. Habets, Signal-based performance evaluation of dereverberation
algorithms, J. Electr. Comput. Eng. 2010 (2010) 1–5.
[30] M.S. Brandstein, D.B. Ward (Eds.), Microphone Arrays: Signal Processing Techniques and Applications,
Springer-Verlag, Berlin, Germany, 2001.

References
911
[31] G.W. Elko, Microphone array systems for hands-free telecommunication, Speech Commun. 20 (3–4) (1996)
229–240.
[32] B.D. van Veen, K.M. Buckley, Beamforming: a versatile approach to spatial ﬁltering, IEEE Acoust. Speech
Signal Mag. 5 (2) (1988) 4–24.
[33] M.S. Brandstein, J.E. Adcock, H.F. Silverman, A closed-form location estimator for use with room
environment microphone arrays, IEEE Trans. Speech Audio Process. 5 (1) (1997) 45–50.
[34] A. Johansson, G. Cook, S. Nordholm, Acoustic direction of arrival estimation, a comparison between
root-music and SRP-PHAT, in: IEEE Region 10th Conference on TENCON 2004, vol. B, November 2004,
pp. 629–632.
[35] N.D. Gaubitch, P.A. Naylor, Analysis of the dereverberation performance of microphone arrays, in:
Proceedings of the International Workshop Acoustics Echo Noise Control (IWAENC), 2005.
[36] D.B. Ward, R.A. Kennedy, R.C. Williamson, FIR ﬁlter design for frequency invariant beamformers, IEEE
Signal Process. Lett. 3 (3) (1996) 69–71.
[37] J.B. Allen, D.A. Berkley, J. Blauert, Multimicrophone signal-processing technique to remove room
reverberation from speech signals, J. Acoust. Soc. Am. 62 (4) (1977) 912–915.
[38] J.L. Flanagan, J.D. Johnston, R. Zahn, G.W. Elko, Computer-steered microphone arrays for sound
transduction in large rooms, J. Acoust. Soc. Am. 5 (78) (1985) 1508–1518.
[39] J.L. Flanagan, A.C. Surendran, E.E. Jan, Spatially selective sound capture for speech and audio processing,
Speech Commun. 13 (1–2) (1993) 207–222.
[40] J. Meyer, G. Elko, A highly scalable spherical microphone array based on an orthonormal decomposition
of the soundﬁeld, in: Proceedings of IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), vol. 2, May 2002, pp. 1781–1784.
[41] T. Nishiura, S. Nakanura, K. Shikano, Speech enhancement by multiple beamforming with reﬂection signal
equalization, in: Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), vol. 1, May 2001, pp. 189–192.
[42] E.A.P. Habets, J. Benesty, S. Gannot, P.A. Naylor, I. Cohen, On the application of the LCMV beamformer
to speech enhancement, in: Proceedings of IEEE Workshop on Applications of Signal Processing to Audio
and Acoustics, New York, USA, October 2009, pp. 141–144.
[43] O.L. Frost III, An algorithm for linearly constrained adaptive array processing, Proc. IEEE 60 (8) (1972)
926–935.
[44] E.A.P. Habets, J. Benesty, I. Cohen, S. Gannot, J. Dmochowski, New insights into the MVDR beamformer
in room acoustics, IEEE Trans. Audio Speech Lang. Process. 18 (1) (2010) 158–170.
[45] L.J. Grifﬁths, C.W. Jim, An alternative approach to linearly constrained adaptive beamforming, IEEE Trans.
Antennas Propag. 30 (1) (1982) 27–34.
[46] W. Liu, S. Weiss, L. Hanzo, Subband adaptive generalized sidelobe canceller for broadband beamforming, in:
Proceedings of the 11th IEEE Workshop on Statistical Signal Processing, Orchid Country Club, Singapore,
2001, pp. 591–594.
[47] E.A.P. Habets, J. Benesty, P.A. Naylor, A speech distortion and interference rejection constraint beamformer,
IEEE Trans. Audio Speech Lang. Process. 20 (3) (2012) 854–867.
[48] L.R. Rabiner, R.W. Schafer, Digital Processing of Speech Signals, Prentice-Hall, Englewood Cliffs, New
Jersey, USA, 1978.
[49] M. Berouti, R. Schwartz, J. Makhoul, Enhancement of speech corrupted by acoustic noise, in: Proceedings
of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), vol. 4, 1979,
pp. 208–211.
[50] S.F. Boll, Suppression of acoustic noise in speech using spectral subtraction, IEEE Trans. Acoust. Speech
Signal Process. ASSP-27 (2) (1979) 113–120.

912
CHAPTER 31 Dereverberation
[51] Y. Ephraim, D. Malah, Speech enhancement using a minimum-mean square error short-time spectral
amplitude estimator, IEEE Trans. Acoust. Speech Signal Process. 32 (6) (1984) 1109–1121.
[52] R. Martin, Spectral subtraction based on minimum statistics, in: Proceedings of European Signal Processing
Conference, 1994, pp. 1182–1185.
[53] R. Martin, Statistical methods for the enhancement of noisy speech, in: International Workshop on Acoustics
Echo and Noise Control, Kyoto, September 2003.
[54] P.C. Loizou, Speech Enhancement Theory and Practice, Taylor and Francis, 2007.
[55] J. Makhoul, Linear prediction: a tutorial review, Proc. IEEE 63 (4) (1975) 561–580.
[56] M.S. Brandstein, S.M. Griebel, Nonlinear, model-based microphone array speech enhancement, in: S.L. Gay,
J. Benesty (Eds.), Acoustic Signal Processing for Telecommunications, Kluwer Academic Publishers, 2000.
[57] N.D. Gaubitch, D.B. Ward, P.A. Naylor, Statistical analysis of the autoregressive modeling of reverberant
speech, J. Acoust. Soc. Am. 120 (6) (2006) 4031–4039.
[58] B.W. Gillespie, H.S. Malvar, D.A.F. Florêncio, Speech dereverberation via maximum-kurtosis subband
adaptive ﬁltering, in: Proceedings of IEEE International Conference on Acoustics Speech and Signal
Processing (ICASSP), vol. 6, 2001, pp. 3701–3704.
[59] N.D. Gaubitch, P.A. Naylor, D.B. Ward, Multi-microphone speech dereverberation using spatio-temporal
averaging, in: Proceedings of European Signal Processing Conference (EUSIPCO), Vienna, Austria,
September 2004, pp. 809–812.
[60] N.D. Gaubitch, P.A. Naylor, Spatiotemporal averaging method for enhancement of reverberant speech, in:
Proceedings of IEEE International Conference Digital Signal Processing (DSP), Cardiff, UK, July 2007.
[61] T. Nakatani, M. Miyoshi, Blind dereverberation of single channel speech signal based on harmonic structure,
in: Proceedings of IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP),
vol. 1, 2003, pp. 92–95.
[62] T. Nakatani, K. Kinoshita, M. Miyoshi, Harmonicity-based blind dereverberation for single-channel speech
signals, IEEE Trans. Audio Speech Lang. Process. 15 (1) (2007) 80–95.
[63] K. Lebart, J.M. Boucher, P.N. Denbigh, A new method based on spectral subtraction for speech
de-reverberation, Acta Acoust. 87 (2001) 359–366.
[64] J.Y.C. Wen, E.A.P. Habets, P.A. Naylor, Blind estimation of reverberation time based on the distribution
of signal decay rates, in: Proceedings of IEEE International Conference on Acoustics Speech and Signal
Processing (ICASSP), Las Vegas, USA, April 2008.
[65] H.W. Löllmann, P. Vary, Estimation of the reverberation time in noisy environments, in: Proceedings of the
International Workshop Acoustics Echo Noise Control (IWAENC), September 2008, pp. 1–4.
[66] I. Cohen, Optimal speech enhancement under signal presence uncertainty using log-spectral amplitude
estimator, IEEE Signal Process. Lett. 9 (4) (2002) 113–116.
[67] M. Wu, D. Wang, A two-stage algorithm for one-microphone reverberant speech enhancement, IEEE Trans.
Audio Speech Lang. Process. 14 (3) (2006) 774–784.
[68] G. Xu, H. Liu, L. Tong, T. Kailath, A least-squares approach to blind channel identiﬁcation, IEEE Trans.
Signal Process. 43 (12) (1995) 2982–2993.
[69] Y. Huang, J. Benesty, A class of frequency-domain adaptive approaches to blind multichannel identiﬁcation,
IEEE Trans. Signal Process. 51 (1) (2003) 11–24.
[70] M.I. Gürelli, C.L. Nikias, EVAM: an eigenvector-based algorithm for multichannel blind deconvolution of
input colored signals, IEEE Trans. Signal Process. 43 (1) (1995) 134–149.
[71] S. Gannot, M. Moonen, Subspace methods for multimicrophone speech dereverberation, EURASIP J. Appl.
Signal Process. 2003 (11) (2003) 1074–1090.
[72] Y. Huang, J. Benesty, Adaptive multi-channel least mean square and Newton algorithms for blind channel
identiﬁcation, Signal Process. 82 (2002) 1127–1138.

References
913
[73] Y. Huang, J. Benesty, J. Chen, A blind channel identiﬁcation-based two-stage approach to separation and
dereverberation of speech signals in a reverberant environment, IEEE Trans. Speech Audio Process. 13 (5)
(2005) 882–895.
[74] E.A.P. Habets, P.A. Naylor, An online quasi-newton algorithm for blind SIMO identiﬁcation, in: Proceedings
of IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP), Dallas, USA,
March 2010.
[75] E.A.P. Habets, J. Benesty, P.A. Naylor, A cross-relation based afﬁne projection algorithm for blind simo
system identiﬁcation, in: Proceedings of European Signal Processing Conference (EUSIPCO), 2011.
[76] M.K. Hasan, J. Benesty, P.A. Naylor, D.B. Ward, Improving robustness of blind adaptive multichannel
identiﬁcation algorithms using constraints, in: Proceedings of European Signal Processing Conference
(EUSIPCO), 2005.
[77] M.K. Hasan, P.A. Naylor, Analyzing effect of noise on LMS-type approaches to blind estimation of
SIMO channels: robustness issue, in: Proceedings of European Signal Processing Conference (EUSIPCO),
Florence, Italy, September 2006.
[78] M.R.P. Thomas, N.D. Gaubitch, E.A.P. Habets, P.A. Naylor, An insight into common ﬁltering in noisy simo
blind system identiﬁcation, in: Proceedings of IEEE International Conference on Acoustics Speech and
Signal Processing (ICASSP), April 2012.
[79] N.D. Gaubitch, M.K. Hasan, P.A. Naylor, Generalized optimal step-size for blind multichannel LMS system
identiﬁcation, IEEE Signal Process. Lett. 13 (10) (2006) 624–627.
[80] N.D. Gaubitch, Blind identiﬁcation of acoustic systems and enhancement of reverberant speech, Ph.D.
Dissertation, Imperial College London, 2006.
[81] K. Furuya, Y. Kaneda, Two-channel blind deconvolution for non-minimum phase impulse responses, in:
Proceedings of IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP), 1997,
pp. 1315–1318.
[82] S. Subramaniam, A. Petropulu, C. Wendt, Cepstrum-based deconvolution for speech dereverberation, IEEE
Trans. Speech Audio Process. 4 (5) (1996) 392–396.
[83] A. Petropulu, S. Subramaniam, Cepstrum based deconvolution for speech dereverberation, in: Proceedings of
IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP), vol. 1, 1994, pp. 1–12.
[84] X.S. Lin, A.W.H. Khong, P.A. Naylor, A forced spectral diversity algorithm for speech dereverberation in
the presence of near-common zeros, IEEE Trans. Audio Speech Lang. Process. 20 (3) (2012) 888–899.
[85] M. Triki, D.T.M. Slock, Iterated delay and predict equalization for blind speech dereverberation, in: Proceed-
ings of International Workshop Acoustics Echo Noise Control (IWAENC), Paris, France, September 2006.
[86] J.R. Hopgood, P.J.W. Rayner, Blind single channel deconvolution using nonstationary signal processing,
IEEE Trans. Speech Audio Process. 11 (5) (2003) 476–488.
[87] F. Talantzis, D.B. Ward, P.A. Naylor, Performance analysis of dynamic acoustic source separation in
reverberant rooms, IEEE Trans. Audio Speech Lang. Process. 14 (4) (2006) 1378–1390.
[88] M. Miyoshi, Y. Kaneda, Inverse ﬁltering of room acoustics, IEEE Trans. Acoust. Speech Signal Process. 36
(2) (1988) 145–152.
[89] K. Yamada, J. Wang, F. Itakura, Recovering of broad band reverberant speech signal by sub-band MINT
method, in: Proceedings of IEEE International Conference on Acoustics Speech and Signal Processing
(ICASSP), 1991, pp. 969–972.
[90] P.A. Nelson, F. Orduña-Brustamante, H. Hamada, Inverse ﬁlter design and equalization zones in multichannel
sound reproduction, IEEE Trans. Speech Audio Process. 3 (3) (1995) 185–192.
[91] J. Mourjopoulos, P. Clarkson, J. Hammond, A comparative study of least-squares and homomorphic
techniques for the inversion of mixed phase signals, in: Proceedings of IEEE International Conference on
Acoustics Speech and Signal Processing (ICASSP), vol. 7, May 1982, pp. 1858–1861.
[92] J.N. Mourjopoulos, Digital equalization of room acoustics, J. Audio Eng. Soc. 42 (11) (1994) 884–900.

914
CHAPTER 31 Dereverberation
[93] B.D. Radlovi´c, R.A. Kennedy, Nonminimum-phase equalization and its subjective importance in room
acoustics, IEEE Trans. Speech Audio Process. 8 (6) (2000) 728–737.
[94] M. Tohyama, R.H. Lyon, T. Koike, Source waveform recovery in a reverberant space by cepstrum dere-
verberation, in: Proceedings of IEEE International Conference on Acoustics Speech and Signal Processing
(ICASSP), vol. 1, 1993, pp. 157–160.
[95] S.T. Neely, J.B. Allen, Invertibility of a room impulse response, J. Acoust. Soc. Am. 66 (1) (1979) 165–169.
[96] I. Kodrasi, S. Doclo, Robust partial multichannel equalization techniques for speech dereverberation, in:
Proceedings of IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP),
Kyoto, Japan, April 2012.
[97] T. Hikichi, M. Delcroix, M. Miyoshi, Inverse ﬁltering for speech dereverberation less sensitive to noise and
room transfer function ﬂuctuations, EURASIP J. Appl. Signal Process. 2007 (2007).
[98] M. Kallinger, A. Mertins, Multi-channel room impulse response shaping—a study, in: Proceedings of IEEE
International Conference on Acoustics Speech Signal Processing, 2006.
[99] A. Mertins, T. Mei, M. Kallinger, Room impulse response shortening/reshaping with inﬁnity- and p-norm
optimization, IEEE Trans. Audio Speech Lang. Process. 18 (2) (2010) 249–259.
[100] W. Zhang, E.A.P. Habets, P.A. Naylor, On the use of channel shortening in multichannel acoustic system
equalization, in: Proceedings of the International Workshop Acoustics Echo Noise Control (IWAENC), Tel
Aviv, Israel, August 2010.
[101] R.K. Martin, K. Vanbleu, M. Ding, G. Ysebaert, M. Milosevic, B.L. Evans, M. Moonen, C.R. Johnson
Jr., Uniﬁcation and evaluation of equalization structures and design algorithms for discrete multitone
modulation systems, IEEE Trans. Signal Process. 53 (2005) 3880–3894.
[102] R. Rado, Note on generalized inverse of matrices, Proc. Cambridge Philos. Soc. 52 (1956) 600–601.
[103] D.R. Morgan, J. Benesty, M. Sondhi, On the evaluation of estimated impulse responses, IEEE Signal
Process. Lett. 5 (7) (1998) 174–176.
[104] F. Lim, P.A. Naylor, Relaxed multichannel least squares with constrained initial taps for multichannel dere-
verberation, in: Proceedings of the International Workshop Acoustics Signal Enhancement (IWAENC), 2012.
[105] M.R.P. Thomas, N.D. Gaubitch, P.A. Naylor, Application of channel shortening to acoustic channel
equalization in the presence of noise and estimation error, in: Proceedings of the IEEE Workshop on
Applications of Signal Processing to Audio and Acoustics, New Paltz, New York, USA, October 2011.
[106] ITU-T, Perceptual evaluation of speech quality (PESQ), an objective method for end-to-end speech quality
assessment of narrowband telephone networks and speech codecs, Int. Telecommun. Union (ITU-T)
Recommendation (2001) 862.

32
CHAPTER
Sound Field Synthesis
Rudolf Rabenstein*, Sascha Spors†, and Jens Ahrens‡
*University Erlangen-Nürnberg, Erlangen, Germany
†University Rostock, Rostock, Germany
‡Microsoft Research, Redmond, USA
4.32.1 Introduction
This chapter treats the topic of sound ﬁeld synthesis in the context of creation of virtual acoustic
environments. The latter as well as the corresponding rendering methods are introduced in this section.
Then the organization of this chapter is presented.
4.32.1.1 Virtual acoustic environments
The term virtual environment is not well deﬁned and used in a variety of ways. Nevertheless, two aspects
appear to be common to all approaches: From the technical viewpoint, virtual environments are created
by the convergence of different multimedia technologies. From a user perspective, virtual environments
create a sense of immersion, where users perceive no more single displays or loudspeakers but the
complete audiovisual scene as an entity.
The prevalent multimedia technologies in virtual environments are video and audio reproduction,
where mass-produced hardware and media standards are available. These are sometimes complemented
by more experimental devices like data gloves, head-mounted displays, haptic interaction, or alike.
Virtual environments are applied in the entertainment industry for movie reproduction in different
formats and for games, in communications for teleconferencing, and in simulation for the evaluation of
the design of buildings and machines, for operator training, etc.
The term virtual environment is often used for advanced video technologies only, while sound is
considered as an add-on to support the visual content. In contrast, the focus lies here explicitly on the
creation of acoustic virtual environments. They may be used for audio reproduction in its own right or
in conjunction with video reproduction.
The technologies for creating acoustic virtual environments can be roughly classiﬁed into head
related methods and room related methods [1]. Head related methods attempt to create the proper
acoustic signals at both ears of one listener. The set of methods employed for this purpose is called
binaural technology. They are addressed brieﬂy in the beginning of Section 4.32.1.2.
Room related methods establish a sound ﬁeld within a room where one or more listeners can sit or
walk around. A classical approach is to pan a sound source between one or more pairs of loudspeakers
like in two-channel stereo and extensions thereof. More recent approaches to sound ﬁeld synthesis apply
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00032-7
© 2014 Elsevier Ltd. All rights reserved.
915

916
CHAPTER 32 Sound Field Synthesis
a high number of reproduction channels which are treated as spatial samples of a continuous source
distribution around the listening area [2]. Finally, the theory of multiple-input, multiple-output systems
gives rise to the family of multipoint methods. Section 4.32.1.2 reviews aspects of these room related
approaches.
From all room related methods, the approaches to sound ﬁeld synthesis are closest to the reproduction
of the physically correct sound ﬁeld. Their presentation constitutes the main focus of this chapter.
4.32.1.2 Rendering of virtual acoustic environments
The technical process of creating an acoustic virtual environment is also called rendering in parallel
to the rendering of visual scenes on video displays. Some aspects of head related and room related
rendering methods are brieﬂy reviewed here. The presentation starts with a glimpse on head-related
transfer functions. Then various panning methods from classical stereo reproduction to more recent
approaches are discussed. Sound ﬁeld synthesis is put in perspective to the other room related methods
here, but the resulting rendering methods are discussed in more depth in the remainder of the chapter.
The overview on room related methods is concluded by a review of multipoint methods.
4.32.1.2.1
Reproduction based on head-related transfer functions
The human auditory system exploits the acoustic characteristics of the outer ear in order to perform
spatialsceneanalysis[3].Theouterearincludesthepinnae,headanduppertorso.Theacousticproperties
from the outer ear can be captured by measuring the transfer function from an acoustic source to
a deﬁned position in the ear canal of both ears. These functions are known as head-related transfer
functions (HRTFs) and are individual for one person. HRTFs are measured in anechoic, reverberant or
simulated environments. Virtual sound sources are created by ﬁltering the signal of the virtual source
by the its left and right ear HRTF, and reproducing the resulting signals via headphones in order to
have independent control over the signals at the two eardrums. For proper auralization also the transfer
function of the headphone has to be compensated for. This reproduction method is sometimes also
referred to as binaural reproduction.
The HRTFs depend on the position of the listener’s ears, the position of the acoustic source, the
acoustic properties of the source and the environment. Hence, a database of HRTFs is required that covers
all potential parameters. In order to limit the effort, often only the head orientation in the horizontal
plane for one listener and source position is taken into account. For auralization, the head orientation
has to be tracked and the corresponding set of HRTFs has to be applied. A head tracked system is
typically termed as dynamic binaural reproduction system. The beneﬁt of binaural reproduction is its
low complexity in conjunction with the high spatial quality that can be achieved by a well designed
system. Drawbacks are that HRTFs are listener dependent and that binaural reproduction gets complex
for multiple listeners since head-tracking and appropriate HRTF databases are required for each listener
(position). Furthermore, the reproduction via headphones decouples the listener from their environment
so that e.g., the communication with other people in the same room becomes unpleasant.
Binaural reproduction can also be performed via loudspeakers. Here, an appropriate crosstalk can-
cellation [4] has to be employed in order to control the signals at both ears of the listener independently.
Crosstalk cancelation typically exhibits a very pronounced sweet spot and is likely to produce strong
artifacts at positions some few centimeters from the sweet spot.

4.32.1 Introduction
917
4.32.1.2.2
Panning approaches
Panning approaches apply amplitude differences (and/or time delays) to a low number of loudspeakers
(typically a pair, triple, or quadruple) in order to create the impression of phantom sources. Stereophony
is the most widespread variant of these approaches which is based on the use of a pair of loudspeakers.
However, generalizations of the basic concept have been developed like e.g., vector base amplitude
panning (VBAP) [5] and Ambisonics amplitude panning (AAP) [6]. Although all these techniques are
partly physically motivated, it is agreed nowadays that their success can be exclusively attributed to
psychoacoustic properties of the human auditory system.
AnumberofstudieshavebeenconductedinordertoclarifytheperceptionofStereophony,seee.g.,[7]
for references. The assumed underlying psycho-acoustical mechanism is termed summing localization,
e.g., [7, p. 9] and [3, p. 204]. Summing localization refers to the superposition of (typically a low
number of) sound ﬁelds carrying sufﬁciently coherent signals impinging at a time interval smaller than
approximately 1 ms. It is assumed that the superposition of the sound ﬁelds at the listener’s ears leads
to summed signals, the components of which can not be discriminated by the human hearing system.
An extension of the concept of summing location is Theile’s association theory published ibidem.
Panning approaches are typically realized by applying weights (and/or delays) to each loudspeaker
according to a given panning law. These panning laws are derived by either considering the physics of the
problem (e.g., the sine law) or by psychoacoustic experiments. A major beneﬁt of panning approaches
is their low complexity. However it is well known, that they exhibit a very pronounced sweet spot and
that the impression of a phantom source at lateral and rear positions is unreliable for typical setups [5].
Outside of the sweet spot, the spatial impression is heavily distorted and also some impairment in terms
of sound color may occur [7]. Hence, these techniques are only suitable for small audiences if spatial
sound reproduction with high resolution is desired.
4.32.1.2.3
Sound ﬁeld synthesis
In order to provide the potential of satisfying a signiﬁcantly larger receiver area that above mentioned
approaches allow for, methods have been proposed that aim at the physical synthesis of a given sound
ﬁeld. These methods are termed sound ﬁeld synthesis [2].
The problem of sound ﬁeld synthesis may be formulated in words as follows:
A given ensemble of elementary sound sources shall be driven such that the superposition of the
sound ﬁelds emitted by the individual elementary sound sources best approximates a sound ﬁeld with
given desired properties over an extended area.
Such an extended area may be a volume or a surface. The employed elementary sound sources will be
termed secondary sources in the remainder of this chapter [8, e.g., p. 106]. In practical implementations,
loudspeakers will be used as secondary sources. The term “secondary source” has been established in
the context of scattering problems where the inﬂuence of a given object on an incident ﬁeld is described
by a distribution of secondary sources that are located along the surface of the object and that replace
the latter, e.g., [9].
In order to facilitate the mathematical treatment and in order to facilitate the exploitation of results
that have been achieved in closely related problems such as acoustical scattering [9], the ensemble of
secondary sources under consideration will be assumed to be continuous and will therefore be referred
to as a distribution of secondary sources.

918
CHAPTER 32 Sound Field Synthesis
Three basic analytic approaches have been proposed in the literature based on the framework out-
lined above. They are termed Near-ﬁeld Compensated Higher-Order Ambisonics (NFC-HOA), Spectral
Division Method (SDM), and Wave Field Synthesis (WFS), respectively, and they will all be presented
later in this chapter.
4.32.1.2.4
Multipoint approaches
The approaches to sound ﬁeld synthesis introduced so far are based upon a spatially continuous for-
mulation of the underlying physical problem. The spatial sampling is typically introduced at a later
stage into the driving function. Besides these approaches a number of approaches exist that are based
on a spatially discrete formulation of the problem. The published approaches differ in terms of their
discretization scheme and signal domain in which the problem is formulated. Most of the approaches
assume a spatially discrete secondary source and receiver distribution. Typically a number of receiver
points are deﬁned which are located around or within the desired listening area. The pressure ﬁeld of the
virtual source should be synthesized as accurately as possible at these points by a weighted combination
of the secondary sources. This is often formulated in terms of a matrix equation that can be understood
as a discretized version of the synthesis equation (32.134) that constitutes the starting point for all
analytic approaches. The desired pressure at the deﬁned receiver points and the driving function for
the secondary sources are combined into vectors. The acoustic paths from all secondary sources to all
receiver points are characterized by a matrix of transfer functions. The resulting set of linear equations
in form of a matrix equation is then solved with respect to the vector of driving functions. Typically
least-square error (LSE) approaches are used. The basic scheme or variants are formulated either in
the time domain, in the frequency domain [10], or in terms of spherical harmonics [11]. For the latter
methods often only the secondary source distribution is explicitly spatially sampled. The traditional
formulation of Higher Order Ambisonics [12] is a prominent example.
The properties of the multipoint approaches are typically somewhere between NFC-HOA, SDM,
and WFS and depend heavily on the number of secondary sources, receiver points and their spatial
conﬁguration.
The major beneﬁt of the multipoint methods are the potentially very ﬂexible loudspeaker layouts for
which an approximate (least-squares) solution can be found. As for the solution of the integral equation
(32.134), also the solution of the matrix equation in the multipoint approach poses an inverse problem
in acoustics. It is well known that in practice these are often ill-conditioned at high frequencies and
the result therefore becomes unpredictable [13]. Besides this problem, the drawbacks of the multipoint
approaches are numerical complexity and that they provide only little insight into fundamental properties
of the reproduced sound ﬁeld.
4.32.1.3 Organization of this chapter
The remainder of this chapter discusses methods for sound ﬁeld synthesis. It starts with the foundations
from physics that lead to the acoustic wave equation in Section 4.32.2. Then the focus shifts from sound
waves to signals that carry space-time information and their representations in different coordinate
systems and in the frequency domain in Section 4.32.3. The response to sound sources and its description
by the Green’s function is presented in Section 4.32.4. Section 4.32.5 continues with the Kirchhoff-
Helmholtz integral equation as the physical foundation of sound ﬁeld synthesis.

4.32.2 Acoustic Wave Equation
919
The following sections present three distinct methods for sound ﬁeld synthesis: Near-ﬁeld Compen-
sated Higher Order Ambisonics in Section 4.32.6, the Spectral Division Method in Section 4.32.7, and
Wave Field Synthesis in Section 4.32.8. These sections share a common structure: a description of the
rendering method on the basis of a continuous source distribution, the introduction of discrete loud-
speaker positions by spatial sampling and application examples, and extensions of the basic principle.
4.32.2 Acoustic wave equation
The acoustic wave equation is introduced here at ﬁrst in a coordinate free representation. Then those
coordinate systems are presented which are most frequently used for the description of sound ﬁelds.
4.32.2.1 Coordinate free representation
Detailed derivations of the acoustic wave equation can be found in many classical and modern books
on acoustics [14–20]. A concise derivation with a clear indication of the involved assumptions and
simpliﬁcations is found e.g., in [19]. Therefore only a few remarks on the underlying physical principles
are given here.
The description of wave propagation in a ﬂuid involves three ﬁeld quantities, the sound pressure
p(x, t), the particle velocity v(x, t), and the mass density ϱ(x, t). They depend on the space coordinate
vector x and on time t. Three different relations for each pair of variables can be established from the
ﬁrst principles of physics:
•
the conservation of mass which links the mass density ϱ(x, t) and the particle velocity v(x, t),
•
the equations for the thermodynamical state of the ﬂuid involving the mass density ϱ(x, t) and the
pressure p(x, t),
•
the equation of motion (Newton’s second law) for the sound pressure p(x, t) and the particle velocity
v(x, t).
Figure 32.1 shows the interrelations between these three ﬁeld quantities and the governing physical
laws in graphical form.
sound pressure
p(x, t)
Newton’s
2nd law
material
properties
particle velocity
v(x, t)
conservation
of mass
mass density
(x, t)
FIGURE 32.1
Fundamental physical principles for the description of sound propagation.

920
CHAPTER 32 Sound Field Synthesis
The propagation medium for sound waves is assumed to be air at standard conditions. It can be
regarded as an ideal gas for the purpose of room acoustics and for audible frequencies. This assumption
simpliﬁes the relations for the thermodynamical state to the extent that variations in the mass density
can be expressed by variations of the sound pressure. Then the mass density ϱ can be eliminated such
that two expressions for the sound pressure p and the particle velocity v remain.
One relation results from the conservation of mass and the assumption of an ideal gas. It establishes
the relation between the compression of the air and the velocity gradient
1
c2
∂p(x, t)
∂t
+ ϱ0∇v(x, t) = 0.
(32.1)
Here ϱ0 denotes the static density of the air and c the sound speed.
The other relation is Newton’s second law in differential form (Euler’s law). It links the gradient of
the sound pressure p(x, t) and the acceleration of the air particles
∇p(x, t) + ϱ0
∂v(x, t)
∂t
= 0.
(32.2)
Finally the scalar homogeneous wave equation is obtained by eliminating the vector of the particle
velocity v from (32.1) and (32.2). Taking the partial time derivative of (32.1) and the gradient of (32.2)
results in
∇2 p(x, t) −1
c2
∂2 p(x, t)
∂t2
= 0.
(32.3)
The square of the Nabla-Operator for the spatial derivative ∇2 =  is the Laplace operator. It takes
different forms depending on the chosen coordinate system.
4.32.2.2 Coordinate systems
4.32.2.2.1
Introduction
The ability to convert between different systems of coordinates allows to ﬁnd the most suitable one for
a given problem. In many cases it is possible to exploit certain spatial symmetries of typical wave ﬁelds
and to use a coordinate system which reﬂects these symmetries in the most simple way. An example is
the use of spherical coordinates for the sound ﬁeld of a single point source (acoustic monopole). The
radial symmetry of the problem leads to a sound ﬁeld which is constant with respect to the azimuth and
zenith angle of spherical coordinates. In other cases, the choice of the coordinate system is induced by
the shape of the enclosure. The wave propagation in a cylindrical pipe like the bore of a wind instrument
is an example for the favorable use of cylindrical coordinates. Aligning the longitudinal axis with the
center of the bore brings the walls of the enclosure to a constant value of the radial axis. The following
section gives a short overview on frequently used coordinate systems.
4.32.2.2.2
Overview on frequently used spatial coordinate systems
A schedular overview on the most frequently used spatial coordinate systems is given in Table 32.1.
Cartesian coordinates are most frequently used when no inherent spatial structure of a spatial region
of interest is given. An example are world coordinates for general audiovisual scenes in virtual reality.

4.32.2 Acoustic Wave Equation
921
Table 32.1 Frequently Used Coordinate Systems in Two and Three Spatial Dimensions
Two-dimensional
Three-dimensional
Cartesian
Cartesian
Polar
Cylindrical
Spherical
Cartesian coordinates allow object and viewpoint manipulations like translation or rotation with simple
linear transformations. Also the transition between spatially two-dimensional and three-dimensional
representations is easily accomplished by projections from 3D to 2D.
Cartesian coordinates are also the most suitable choice for spatial structures with linear or planar
shape. Examples are shoebox models of acoustic spaces for simpliﬁed analytical modal investigations or
for the mirror image source method in room acoustics. Another natural choice for Cartesian coordinates
are line or planar arrays of microphones or loudspeakers, where the alignment of the spatial structure
with the coordinate axes is obvious.
For user centric systems, polar or spherical coordinate systems are more suitable. They deﬁne
acoustic events relative to a listener who resides naturally in the center of the coordinate systems. Sound
sources are then deﬁned by their distance and by their azimuth and zenith angles in the 3D case. For
sound events in a horizontal plane around the listener’s ears, a 2D polar coordinate system is often
sufﬁcient.
Polar and spherical coordinates are frequently used to describe source localization and for the deﬁ-
nition of head related transfer functions (HRTFs). The Ambisonics sound reproduction method is based
on a sound ﬁeld description with spherical harmonics.
Cylindrical coordinates are suitable for the descriptions of cylindrical waveguides. They are used
in musical acoustics to describe woodwind instruments, organ pipes and alike. In sound reproduction,
cylindrical coordinates are of theoretical value to describe the transition between 3D and 2D descriptions
of sound ﬁelds.
4.32.2.2.3
Cartesian coordinates
In Cartesian coordinates the three-dimensional space coordinates are denoted by
x =
⎡
⎣
x
y
z
⎤
⎦.
(32.4)
The Cartesian coordinates for two dimensions are given in the same way for z = 0.
The Laplace operator in Cartesian coordinates consists of the second order derivatives calculated
along each orthogonal spatial dimension
∇2 =  = div grad = ∂2
∂x2 + ∂2
∂y2 + ∂2
∂z2 .
(32.5)

922
CHAPTER 32 Sound Field Synthesis
x
y
α
β
z
FIGURE 32.2
Spherical coordinates.
4.32.2.2.4
Spherical coordinates
Spherical coordinates specify the distance r from the origin, an azimuth angle α in the horizontal plane
and a zenith angle β for the elevation, see Figure 32.2. Spherical coordinates are expressed in terms of
Cartesian coordinates as
x =
⎡
⎣
x
y
z
⎤
⎦= r
⎡
⎣
cos α sin β
sin α sin β
cos β
⎤
⎦,
(32.6)
with the inverse relations
r2 = |x|2 = x2 + y2 + z2,
(32.7)
tan α = y
x ,
(32.8)
cos β = z
r ,
(32.9)
and the vector notation
r =
⎡
⎣
r
α
β
⎤
⎦.
(32.10)
The Laplace operator in spherical coordinates is given by
 = 1
r2
∂
∂r

r2 ∂
∂r

+
1
r2 sin β
∂
∂β

sin β ∂
∂β

+
1
r2 sin2 β
∂2
∂α2 .
(32.11)

4.32.2 Acoustic Wave Equation
923
y
x
α
r
FIGURE 32.3
Polar coordinates.
The second order partial differentials can be written in different ways. For example the ﬁrst term may
appear as
1
r2
∂
∂r

r2 ∂
∂r p(r)

= ∂2
∂r2 p(r) + 2
r
∂
∂r p(r) = 1
r
∂2
∂r2 (rp(r)).
(32.12)
The equivalence of these three expressions is shown with the chain rule and the product rule of differ-
entiation.
4.32.2.2.5
Polar coordinates
Polar coordinates in space are a two-dimensional representation for the distance r from the origin and
a rotation by the angle α (see Figure 32.3). They correspond to the spherical coordinates with β = π
2 .
The relations to the two-dimensional Cartesian coordinates are given by
x =
 x
y
	
= r
cos α
sin α
	
,
(32.13)
with the inverse relations
r2 = |x|2 = x2 + y2,
(32.14)
tan α = y
x .
(32.15)
For a concise notation, the polar coordinates are also arranged in vector fashion
r =
 r
α
	
.
(32.16)
The Laplace operator in polar coordinates is given by
 = 1
r
∂
∂r

r ∂
∂r

+ 1
r2
∂2
∂α2 .
(32.17)

924
CHAPTER 32 Sound Field Synthesis
4.32.3 Signal representations
This section provides links between the physical quantities of the acoustic wave equation and between
multidimensionalsignalsandtheirnumerousrepresentations.Theserepresentationsdistinguishbetween
signals in the time and space domain and between signals in the associated frequency and wavenumber
domains. Furthermore, space- or wavenumber-dependent signals in two or three spatial dimensions can
be represented in various spatial coordinate systems.
The resulting multitude of different signal representations is not easy to handle with regards to
the mathematical notation. This chapter follows the good practice in signal processing to distinguish
between signals in time, frequency, and possibly other domains. The corresponding notational details
are introduced in due course. They include small and capital letters, subscripts and superscripts, the
tilde and other graphemes, although such signal “decorations” are sometimes deemed superﬂuous in
the literature on acoustics (see e.g., [21, Section A.3]).
4.32.3.1 Introduction
The discussion of the acoustic wave equation in Section 4.32.2 relied on the sound pressure and the
particle velocity as physical quantities. They served to establish fundamental relations like conservation
of mass or Newton’s law of motion. However, for the purpose of sound rendering, also another aspect
of these quantities is of importance. The temporal and spatial variations of the sound pressure carry
information from a sound source to the listeners. This information may be explicit such as speech or
musical notes or it may be implicit such as perceived genre or timbre. In any way, the sound pressure
governed by the acoustical wave equation is not only a physical quantity but also a signal in the sense
of communications. While Section 4.32.2 considered the sound pressure as a physical quantity, this
section and the following ones emphasize the signal character.
Signals in communications are mostly one-dimensional and time dependent, e.g., the varying voltage
receivedbyanantennaorpickedupbyamicrophone.Signalsofthiskindarenotonlyrepresentedbytheir
temporal variations but also—equivalently—by their frequency content. The connection between the
time and the frequency domain is either provided by the phasor approach or by integral transformations
like the Fourier or the Laplace transformation.
•
The phasor approach considers monofrequent signals of the form
u1(t) = Ueiω1t
(32.18)
with the imaginary unit i, the angular frequency ω1, and the complex amplitude U. The signal u1(t)
depends on the time variable t while the angular frequency ω1 is an arbitrary but ﬁxed parameter.
Different values of ω1 deﬁne different signals u1(t). To indicate clearly the different nature of time
and frequency, the ﬁxed angular frequency carries the index 1 and the function u1(t) shares the same
index.
•
The transformation approach works with the equivalence of a signal and its spectrum
u(t)
◦––•
U(ω).
(32.19)

4.32.3 Signal Representations
925
The transformation symbol ◦––• is a shorthand notation for the Fourier transformation U(ω) =
F{u(t)}. A formal deﬁnition is given in Section 4.32.3.3. Here the Fourier transform U(ω) depends
on the frequency variable ω. As a free variable, ω is not indexed.
For signals which depend only on time the dual character of a signal and its spectrum in the form
of (32.19) is quite simple and easy to handle. This situation is different for the sound pressure p(x, t)
which depends on time and space and thus constitutes a multidimensional signal. Therefore two different
transformations are required, one for the time and one for the space variable. The latter has to consider
the number of spatial dimensions. In addition the nature of the spatial transformation depends on the
chosen spatial coordinate system (Cartesian, polar, spherical).
These different representations are introduced now as extensions of the well-known phasor concept.
It turns out that its generalization to time- and space-dependent signals leads to another basic concept,
the so-called plane wave. This extension of the phasor approach to the sound pressure is presented in
Section 4.32.3.2. Section 4.32.3.3 introduces the set of transformations required for space variables in
Cartesian coordinates. Extensions to two-dimensional spatial problems in polar coordinates are given in
Section 4.32.3.4 and to three-dimensional spatial problems in spherical coordinates in Section 4.32.3.5.
4.32.3.2 The phasor approach for the wave equation
4.32.3.2.1
One-dimensional phasors
Phasors are eigenfunctions of one-dimensional linear and time-invariant (LTI) systems. They are signals
of the form
u1(t) = U(ω1)eiω1t
(32.20)
with the angular frequency ω1 and the complex amplitude U as in (32.18). The response of an LTI-
system with the complex frequency response H(ω) to a phasor with the amplitude U = 1 is also a
phasor with a complex amplitude H(ω1) and the same angular frequency, see Figure 32.4.
The frequency response H(ω) is a function of the angular frequency ω. When the corresponding
system is excited by a monofrequent signal then only its value H(ω1) at the excitation frequency ω1
determines the output signal.
4.32.3.2.2
Multidimensional phasors
The analysis of space-time systems requires to extend the phasor concept to multiple dimensions. Similar
to (32.20), a phasor for a system depending on time t and space x has the form
u01(x, t) = ˜U(k0, ω1)ei(ω1t+kT
0 x).
(32.21)
Again, ω1 is the angular frequency with respect to time. Its counterpart with respect to space is the
wave vector k0 which is of the same dimension as the vector of space coordinates x. Similar to the
eiω1t
H(ω)
H(ω1)eiω1t
FIGURE 32.4
Response of an LTI-system to a phasor.

926
CHAPTER 32 Sound Field Synthesis
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
1
−1
0
1
x
t
u01(x, t)}
λ0
T1
FIGURE 32.5
Real part of the (1+1)D multidimensional phasor u01(x, t) from (32.21). The ﬁgure clearly shows the peri-
odicity both in time and space.
one-dimensional phasor, the angular frequency ω1 and the wave number k0 are ﬁxed values and denoted
by an index. The tilde in ˜U(k0, ω1) indicates that ˜U is a function of both spatial and temporal frequencies.
The wave vector k0 can be expressed by its magnitude, the wave number k0, and the unit length
vector n0 as
k0 = k0n0
with k0 = |k0| and n0 = 1
k0
k0.
(32.22)
Due to the periodicity of the complex exponential function in (32.21), also u01(x, t) is periodic both in
time and space
u01(x, t) = u01(x + λ0n0, t + T1),
(32.23)
with the period T1 and the wavelength λ0
T1 = 2π
ω1
,
λ0 = 2π
k0
.
(32.24)
This periodicity is obvious from Figure 32.5 for one spatial dimension, where only the real part
cos (ω1t + k0x) is shown.
4.32.3.2.3
Multidimensional phasors and the acoustic wave equation
For sound rendering, the relation of multidimensional phasors to the acoustic wave equation is of special
interest. It turns out that the multidimensional phasors of the form of (32.21) are solutions of the wave
equation, if certain relations between angular frequency and wave number hold. To see these relations,

4.32.3 Signal Representations
927
insert (32.21) into the acoustic wave Eq. (32.3). Carrying out the derivations with respect to time and
space gives

k2
0 −

ω1
c
2	
u01(x, t) = 0.
(32.25)
There exists a nontrivial solution u01(x, t) only iff
ω1 = ±c k0.
(32.26)
This close tie between the angular frequencies ω1 and k0 of the multidimensional phasor is imposed by
the acoustic wave equation. It is called the dispersion relation.
4.32.3.2.4
Physical interpretation of multidimensional phasors
A closer look at u01(x, t) under the restriction (32.26) shows that u01(x, t) = const for
nT
0 x ± ct = const.
(32.27)
For each of the two signs, (32.27) describes a plane in space which propagates with the speed c in or
against the direction of the normal vector n0 as shown in Figure 32.6. Therefore a solution of the wave
equation with the arbitrary complex amplitude ˜P
p01(x, t) = ˜P(k0, ω1)ei(ω1t+kT
0 x)
(32.28)
−4
−2
0
2
4
6
−4
−2
0
2
4
−2
0
2
4
6
n0
x
y
z
FIGURE 32.6
Plane in space which propagates with the speed c in the direction of the normal vector n0.

928
CHAPTER 32 Sound Field Synthesis
is called a plane wave or due to its complex exponential nature also a harmonic plane wave.
Physics-oriented texts sometimes emphasize the analogy to monofrequent light by the designation
monochromatic plane wave. With the dispersion relation (32.26), the plane wave can be written in
various forms ( ˜P = 1)
p01(x, t) = ei(ω1t+kT
0 x) = eik0(ct+nT
0 x) = eiω1(t+ 1
c nT
0 x).
(32.29)
The periodicity of the plane wave in time and space is reﬂected by the periodicity of the complex
exponential either in dimensionless variables ω1t and kT
0 x0, with respect to time, or with respect to
space, as shown in (32.29)
p01(x, t) = p01(x, t + T1) = p01(x + λ0n0, t)
= ei(ω1t+kT
0 x) = eiω1(t+T1+ 1
c nT
0 x) = eik0(ct+nT
0 x+λ0).
(32.30)
4.32.3.3 Fourier transformations in time and space
The phasor concept introduced in Section 4.32.3.2 does not only describe monofrequent signals and
plane waves in an elegant way. It provides also the link to general kinds of signals and their spectral
representations with respect to time and space. These relations are ﬁrst established for the Fourier
transformation with respect to time and then with respect to space.
4.32.3.3.1
Fourier transformation with respect to time
Consider the one-dimensional phasor u1(t) from (32.20)
u1(t; ω1) = U(ω1)eiω1t
(32.31)
which describes a monofrequent signal with angular frequency ω1. Since the complex amplitude U may
vary with different angular frequencies, it is written here as a function of ω1. Furthermore, ω1 is added
as parameter to the list of variables in u1(t; ω1). Variables and parameters are separated by a semicolon.
A general time dependent signal u(t) can be regarded as a superposition of monofrequent signals
u1(t; ω1) with different angular frequencies ω1 from the range −∞< ω1 < ∞with different corre-
sponding complex amplitudes U(ω1)
u(t) = 1
2π
 ∞
−∞
u1(t; ω1)dω1 = 1
2π
 ∞
−∞
U(ω1)eiω1tdω1.
(32.32)
The special property of the complex exponential function
1
2π
 ∞
−∞
ei(ω1−ω)tdt = δ(ω1 −ω)
(32.33)
allows to invert the relation (32.32) as
 ∞
−∞
u(t)e−iωtdt =
 ∞
−∞
U(ω1)δ(ω1 −ω)dω1 = U(ω).
(32.34)

4.32.3 Signal Representations
929
u1(t; ω1)
2π U (ω1) δ(ω1 −ω)
u(t)
U (ω)
1
2π
dω1
1
2π
dω1
t
t
FIGURE 32.7
Representation of the Fourier transformation in time as integration of a phasor with respect to angular
frequency ω1.
Equations (32.32) and (32.34) constitute the Fourier transform pair
U(ω) = Ft{u(t)} =
 ∞
−∞
u(t)e−iωtdt,
(32.35)
u(t) = F −1
t
{U(ω)} = 1
2π
 ∞
−∞
U(ω)eiωtdω.
(32.36)
The subscript t in Ft denotes the Fourier transformation with respect to time. Its relation to the one-
dimensional phasor by an integration with respect to frequency is shown in Figure 32.7. The Fourier
transformofthephasorintheupperrightcornerfollowsfromtheorthogonality(32.33).Thederivationof
this well-known relation is presented here as an introduction to the more involved Fourier transformation
with respect to space.
4.32.3.3.2
Fourier transformation with respect to space
Similar as in the preceding section, the derivation of the Fourier transformation with respect to space
starts with the multidimensional phasor from (32.21)
u01(x, t; k0, ω1) = ˜U(k0, ω1)ei(ω1t+kT
0 x).
(32.37)
As in (32.31), the dependence of the complex amplitude ˜U(k0, ω1) on the wave vector k0 and the
angular frequency ω1 has been made explicit in the notation.
In a ﬁrst step similar to (32.32) all phasors with the same wave vector k0 but with varying angular
frequency ω1 are used to describe the superposition
u0(x, t; k0) = 1
2π
 ∞
−∞
˜U(k0, ω1)ei(ω1t+kT
0 x)dω1 = F −1
t
{ ˜U(k0, ω1)eikT
0 x}.
(32.38)
Inverting (32.38) gives
˜U(k0, ω0)eikT
0 x = Ft{u0(x, t; k0)}.
(32.39)

930
CHAPTER 32 Sound Field Synthesis
In a second step the superposition is extended to all possible values of the elements of the wave
vector k0
U(x, ω) =
1
(2π)d
 ∞
−∞
Ft{u0(x, t; k0)}dk0 =
1
(2π)d
 ∞
−∞
˜U(k0, ω1)eikT
0 xdk0.
(32.40)
The number of spatial dimensions is equal to d, d = 1, 2, 3; the integrals are understood as multiple
integrals over all components of the wave vector k0.
Using the relation of the complex exponential function in multiple dimensions (compare (32.33))
1
(2π)d
 ∞
−∞
ei(k0−k)Txdx = δ(k0 −k)
(32.41)
allows to turn (32.40) into
 ∞
−∞
U(x, ω)e−ikTxdx = ˜U(k, ω).
(32.42)
The results (32.40) and (32.42) constitute the spatial Fourier transformation in multiple dimensions
˜U(k, ω) = Fx{U(x, ω)} =
 ∞
−∞
U(x, ω)e−ikTxdx,
(32.43)
U(x, ω) = F −1
x { ˜U(k, ω)} =
1
(2π)d
 ∞
−∞
˜U(k, ω)eikTxdk.
(32.44)
4.32.3.3.3
Fourier transformation with respect to space and time
The Fourier transformations with respect to time and space can be compiled as in Figure 32.8 below.
The transformation symbols ◦––• denote the transformation pairs Ft from (32.35, 32.36) and Fx from
(32.43, 32.44).
The sequence of the transformations has been shown above in the order
t
(via lower left corner of Figure 32.8), but the order of the transformations can also be reversed (via
upper right corner).
u(x , t)
˜u(k , t)
U (x , ω )
˜U (k , ω )
t
t
FIGURE 32.8
Fourier transformation in time and space.

4.32.3 Signal Representations
931
u01(x, t; k0, ω1)
2π ˜U (k0, ω1)δ(ω1 −ω)eikT
0 x
(2π)d+1 ˜U (k0, ω1)δ(ω1 −ω)δ(k0 −k)
u0(x, t; k0)
˜U (k0, ω)eikT
0 x
(2π)d ˜U (k0, ω)δ(k0 −k)
u(x, t)
U (x, ω)
˜U (k, ω)
1
2π
dω1
1
2π
dω1
1
2π
dω1
1
(2π)d
dk0
1
(2π)d
dk0
1
(2π)d
dk0
t
t
t
FIGURE 32.9
Representation of the Fourier transformation in time and space as integrals of multidimensional phasors
with respect to angular frequency and wave vector.
Note that the Fourier transform U(x, ω) with respect to time is designated by a capital letter and the
Fourier transforms ˜u(k, t) and ˜U(k, ω) with respect to space by a tilde. These designations are inherited
from the complex amplitudes of the respective phasors (see (32.32) and (32.38)).
The transition from the multidimensional phasor u01(x, t; k0, ω1) to the general signal u(x, t) and
its Fourier transformation in time and space ˜U(k, ω) according to (32.37) through (32.42) is shown in
Figure 32.9.
The Fourier transformations in time and space can also be used for a concise formulation of the
relations (32.33) and (32.41)
Ft{eiω1t} = 2π δ(ω1 −ω),
(32.45)
Fx{eikT
0 x} = (2π)dδ(k0 −k).
(32.46)
They lead to the Fourier transforms in the upper triangle of Figure 32.9.
4.32.3.3.4
Fourier transformation of plane waves
The introduction of the Fourier transformations in time and space so far is valid for general functions
u(x, t) in the sense that they need not represent solutions of the wave equation. Imposing the requirement
that u(x, t) describes an acoustic wave poses also restrictions on its Fourier transforms. These are derived
here for the various cases shown in Figure 32.9. To distinguish general functions u(x, t) of time and
space from solutions of the wave equation the latter are denoted by p(x, t) resembling sound pressure.
In (32.29) the complex-valued monofrequent solution of the wave equation has been written in the
form
p01(x, t; n0, ω1) = ˜P(n0, ω1)eiω1(t+ 1
c nT
0 x) = ˜P(n0, ω1)eiω1(t+t0(x)).
(32.47)

932
CHAPTER 32 Sound Field Synthesis
The time delay
t0(x) = 1
c nT
0 x
(32.48)
is the time that a plane wave with the speed c takes to travel from the origin of the spatial coordinate
system (x = 0 and t0 = 0) to the location x. This time may also be negative, depending on the position
of x relative to the origin and on the direction of the normal vector n0.
The superposition for all possible angular frequencies ω1 leads to
p0(x, t; n0) = 1
2π
 ∞
−∞
p01(x, t; n0, ω1)dω1
= 1
2π
 ∞
−∞
˜P(n0, ω1)eiω1(t+t0(x))dω1
= F −1
t

˜P(n0, ω)eiωt0(x)
= ˜p(n0, t + t0(x)).
(32.49)
The function ˜p(n0, t) describes the waveform of the sound pressure measured at the origin e.g., with a
microphone, i.e.,
pmic(t) = p0(0, t; n0) = ˜p(n0, t).
The Fourier transform of p0(x, t; n0) with respect to time can be read directly from (32.49) as
Ft {p0(x, t; n0)} = P0(x, ω; n0) = ˜P(n0, ω)eiωt0(x)
(32.50)
and Fourier transformation with respect to space results in (see (32.46))
Fx{P0(x, ω; n0)} = ˜P(k, ω; n0) = (2π)d ˜P(n0, ω)δ

ω
c n0 −k

.
(32.51)
The Dirac function in (32.51) restricts the support of the space-time spectrum ˜P(k, ω) to those values
of ω for which the argument of the Dirac function is zero. This statement is again the dispersion relation
(32.26), now valid for all space-time frequencies simultaneously.
The relations between the monofrequent plane wave p01(x, t; n0, ω1), its broadband version
p0(x, t; n0) and their respective Fourier transforms is shown in Figure 32.10.
p01(x, t; n0, ω1)
2π ˜P(n0, ω1)δ(ω1−ω)eiω1t0(x)
(2π)d+1 ˜P(n0, ω1)δ(ω1−ω)δ( ω1
c n0−k)
p0(x, t; n0)
˜P(n0, ω)eiωt0(x)
(2π)d ˜P(n0, ω)δ( ω
c n0−k)
1
2π
dω1
1
2π
dω1
1
2π
dω1
t
t
FIGURE 32.10
Representation of a monofrequent plane wave as a phasor and its integration with respect to angular fre-
quency.

4.32.3 Signal Representations
933
Comparing Figures 32.7 and 32.10 shows that the integration with respect to the wave vector in
Figure 32.7 (bottom row) has not been performed for the plane wave in Figure 32.10. The reason lies
in the dispersion relation which reduces the number of free parameters in the wave vector by one.
In one spatial dimension, the wave vector k0 is equal to the wave number k0, which can take any
real value. The dispersion relation reduces this freedom to only two possible values ±1 for which the
magnitude of the wave number is unity.
In two spatial dimensions, there are two free components in kT
0 = [k0x k0y]. The dispersion relation
reduces the number of free parameters to one since nT
0 = [cos ϕ sin ϕ] depends only on the angle ϕ
which describes the possible angles of incidence. Thus the unit vector n0 describes a circle of radius 1
with the angle ϕ as parameter.
In three spatial dimensions, the wave vector k0 has three independent components, while the normal
vector n0 describes the surface of a unit sphere with two independent parameters, the azimuth and the
zenith angle.
The one-dimensional case is discussed below. The two- and three-dimensional cases require the
formulation in polar and spherical coordinates, respectively. They are presented in Section 4.32.3.4 and
Section 4.32.3.5.
4.32.3.3.5
Plane waves in one spatial coordinate
In one space dimension there are only two possible values for the unit vector n0 = ±1. The time delay
t0 from (32.48) has the values
t0(x) = ±x
c
(32.52)
and (32.50) has the form
P0(x, ω; ±1) = ˜P(±1, ω)e±iωx/c.
(32.53)
The superposition for all possible values (compare (32.40)) is restricted here to two terms
P(x, ω) = ˜P(+1, ω)eiωx/c + ˜P(−1, ω)e−iωx/c.
(32.54)
For a shorter notation in the time domain, introduce
˜p+(t) = F −1
t
{ ˜P(+1, ω)},
˜p−(t) = F −1
t
{ ˜P(−1, ω)}.
(32.55)
Then the superposition of both components in the space-time domain has the form
p(x, t) = ˜p+

t + x
c

+ ˜p−

t −x
c

.
(32.56)
This representation is known as the D’Alembert-solution of the wave equation or the traveling wave
solution. Inspection of (32.56) shows that ˜p−

t −x
c

is a wave traveling in the direction of the positive
x-axis, since ˜p−

t −x
c

= const for all values of t which increase proportional to x. By the same
argument, ˜p+

t + x
c

is a wave traveling in the direction of the negative x-axis.
The corresponding relation in the wavenumber-frequency domain follows from (32.53) by Fourier
transformation with respect to space as
˜P(k, ω) = 2π ˜P(±1, ω)δ

±ω
c −k

.
(32.57)

934
CHAPTER 32 Sound Field Synthesis
−1
−0.5
0
0.5
1
1
0.5
0
0
0.2
0.4
0.6
0.8
1
| ˜P(k, ω)|
ω
k
FIGURE 32.11
Frequency-domain representation of | ˜P(k, ω)| in the k-ω-plane for one space dimension. The k- and ω axes
are labeled in multiples of π. The waves traveling in both directions are assumed to be the same, i.e.,
˜P(+1, ω) = ˜P(−1, ω).
Figure 32.11 illustrates the behavior of ˜P(k, ω) for one dimension in space. The spectrum ˜P(k, ω) takes
the values P(±1, ω) only for ω = ±kc (s. (32.26)) and is zero elsewhere.
4.32.3.4 Circular harmonics
The plane wave solution of the wave equation has been introduced in Section 4.32.3.2 and further
discussed in Section 4.32.3.3 because it has a simple representation in Cartesian coordinates, see e.g.,
(32.29). General sound ﬁelds are however much more complicated than a monofrequent wave from a
ﬁxed direction. Nevertheless the plane wave solution is a valuable building block for the description of
general sound ﬁelds. In particular, a general sound ﬁeld can be described as a superposition of plane
waves from a continuum of directions [22]. This superposition is shown here for plane waves from all
directions in the horizontal plane. Section 4.32.3.5 extends this idea to plane waves from all directions
in the three-dimensional space.
In analogy to Figure 32.9 it is important to distinguish
monofrequent plane waves: waves with a plane wave front indicated by its normal vector n0 and
with a monofrequent time characteristic with the ﬁxed angular frequency ω1 (see also the top row
of Figure 32.10),
broadband plane waves: plane waves with a time characteristic that is composed of (possibly
inﬁnitely many) monofrequent components (see also the bottom row of Figure 32.10),
general sound ﬁelds: sound ﬁelds that are composed of broadband plane waves from (possibly
inﬁnitely many) different directions.
The role of the plane wave for the description of general sound ﬁelds is somewhat similar to the
role of sinusoids or complex exponentials for the description of general sound spectra. Although the

4.32.3 Signal Representations
935
spectra of speech and music signals may be very complex, the Fourier transformation allows to write
any time signal as a superposition of sinusoids. In the same way, a sound ﬁeld can be expressed as a
superposition of plane waves.
4.32.3.4.1
Monofrequent plane wave in polar coordinates
The superposition of plane waves from all directions in a horizontal plane is best accomplished in polar
coordinates introduced in Section 4.32.2.2. First consider the phasor from (32.29) where the notation
has been adapted to the description with polar coordinates
p01(x, t; ϕ0, ω1) = ˜P(ϕ0, ω1)eiω1(t+t0(x)).
(32.58)
The argument of the complex amplitude ˜P(ϕ0, ω1) indicates that its value depends not only on the
angular frequency ω1 but also on the direction ϕ0 of the plane wave. The vector of space variables x
and the normal vector n0 are given by
x =
 x
y
	
= r
 cos α
sin α
	
and n0 =
cos ϕ0
sin ϕ0
	
.
(32.59)
Note that the vector of space variables x may vary with the distance r and angle α, while the wave
vector n0 of a plane wave is ﬁxed for a given direction ϕ0.
The scalar product nT
0 x of x and n0 can be expressed like any scalar product by the lengths of these
vectors and the angle γ which they include
nT
0 x = |n0||x| cos γ = r cos γ
with γ = α −ϕ0.
(32.60)
This general relation follows here from (32.59) with the trigonometric relation for the difference of two
angles
nT
0 x = r( cos α cos ϕ0 + sin α sin ϕ0) = r cos (α −ϕ0).
(32.61)
The plane wave from (32.58) can now be expressed as
p01(r, α, t; ϕ0, ω1) = ˜P(ϕ0, ω1)eiω1tei ω
c r cos (α−ϕ0),
(32.62)
where the dependence on the polar coordinates is now written explicitly by including radius r and angle
α in the list of parameters. To simplify the notation, no separate designation is introduced; i.e., the
somewhat loosely formulated identity p01(r, α, t; ϕ0, ω1) = p01(x, t; ϕ0, ω1) is assumed.
The plane wave representation in polar coordinates is obviously a periodic function with respect to
the angle α due to the periodicity of the cosine
p01(r, α, t; ϕ0, ω1) = p01(r, α + 2π, t; ϕ0, ω1).
(32.63)
Its expansion into orthogonal functions is given by a Fourier series as shown next.

936
CHAPTER 32 Sound Field Synthesis
4.32.3.4.2
Expansion of monofrequent plane waves into orthogonal functions
The Fourier coefﬁcients of the monofrequent plane wave in (32.62) follow from an expansion of the
complex exponential term. It can be obtained from the following deﬁnition of the Bessel functions of
the ﬁrst kind [23]1
in Jn(kr) = 1
2π
 2π
0
ei(kr cos γ −nγ )dγ,
n ∈Z,
(32.64)
which represents the Fourier series coefﬁcients of the function exp (ikr cos γ ). Its Fourier series is then
given by
eikr cos (α−ϕ0) =
∞

n=−∞
inein(α−ϕ0)Jn(kr).
(32.65)
In the literature on acoustics, this relation is called the Jacobi-Anger expansion [22,23].
The Fourier series expansion of (32.62) follows now immediately as
p01(r, α, t; ϕ0, ω1) = ˜P(ϕ0, ω1)
∞

n=−∞
ineiω1teinα Jn

ω1
c r

e−inϕ0.
(32.66)
The importance of this representation lies in the fact that its components
ineiω1t · einα · Jn

ω1
c r

are separated in terms that depend on time t, angle α, and radius r. This is not the case in (32.62) where
both r and α appear in the exponent of the complex exponential function.
The monofrequent plane wave has been treated in great detail here since it is the building block for
the representation of broadband plane waves and general sound ﬁelds.
4.32.3.4.3
Expansion of broadband plane waves into orthogonal functions
A broadband plane wave p0(r, α, t; ϕ0) is composed of monofrequent plane waves with different
angular frequencies ω1 but identical direction ϕ0. This composition is formulated in mathematical
terms by an integration of the monofrequent plane wave (32.62) with respect to ω1. Through the term
exp (iω1t), the integration takes the form of an inverse Fourier transformation
p0(r, α, t; ϕ0) = 1
2π
 ∞
−∞
p01(r, α, t; ω1, ϕ0)dω1
= 1
2π
 ∞
−∞
˜P(ϕ0, ω1)ei ω1
c r cos (α−ϕ0)eiω1tdω1
= F −1
t

˜P(ϕ0, ω1)ei ω1
c r cos (α−ϕ0)
.
(32.67)
This relation is expressed shorter by the Fourier transform of p0(r, α, t; ϕ0) as
P0(r, α, ω; ϕ0) = Ft{p0(r, α, t; ϕ0)} = ˜P(ϕ0, ω)ei ω
c r cos (α−ϕ0).
(32.68)
1The mathematical literature presents various versions of this integral relation which differ in the sign of the summation index
n or in the choice of the trigonometric function sin or cos. The equivalence of these versions can be shown by substitution.

4.32.3 Signal Representations
937
Then, with (32.65) the Fourier transform P0(r, α, ω; ϕ0) of the plane wave can be represented by a
series expansion
P0(r, α, ω; ϕ0) =
∞

n=−∞
˜P(ϕ0, ω)inein(α−ϕ0)Jn

ω
c r

.
(32.69)
This series represents a plane wave from the direction ϕ0 with the arbitrary spectrum ˜P(ϕ0, ω).
4.32.3.4.4
Expansion of general sound ﬁelds into orthogonal functions
Finally, a general sound ﬁeld p(r, α, t) can be obtained by composing broadband plane waves from
all possible directions 0 ≤ϕ0 < 2π. The composition is formulated as an integration of the Fourier
transform of the broadband plane wave (32.69) with respect to ϕ0. By extracting the exponential term
e−inϕ0 and applying (32.69) to the remaining integral, it can be written as the calculation of a Fourier
coefﬁcient
P(r, α, ω) = 1
2π
 2π
0
P0(r, α, ω; ϕ0)dϕ0 =
∞

n=−∞
˘Pn(ω)ineinα Jn

ω
c r

,
(32.70)
where the complex amplitude ˜P(ϕ0, ω) is represented by its Fourier coefﬁcients ˘Pn(ω)
˘Pn(ω) = 1
2π
 2π
0
˜P(ϕ0, ω)e−inϕ0dϕ0,
(32.71)
as
˜P(ϕ0, ω) =
∞

ℓ=−∞
˘Pℓ(ω)eiℓϕ0.
(32.72)
Finally the Fourier transform P(r, α, ω) of a general sound ﬁeld can be expressed as a Fourier series
P(r, α, ω) =
∞

n=−∞
˚Pn(r, ω)einα
(32.73)
with the Fourier series coefﬁcients
˚Pn(r, ω) = ˘Pn(ω)in Jn

ω
c r

.
(32.74)
These Fourier series coefﬁcients characterize the angular structure of the sound ﬁeld, respectively of
its Fourier transform. Higher orders n correspond to a ﬁner angular structure. Note that the radius
dependent part Jn

ωr/c

is independent of the speciﬁc wave ﬁeld.
4.32.3.4.5
Summary of the representations of a general sound ﬁeld
The transition from a monofrequent plane wave in polar coordinates to a broadband plane wave and to
a general sound ﬁeld is shown in Figure 32.12. It corresponds to the center column of Figure 32.10 with
the spatial dependency expressed in polar coordinates as derived in this section.

938
CHAPTER 32 Sound Field Synthesis
P01(r, α, ω; ω1, ϕ0) = 2π ˜P(n0, ω1)δ(ω1 −ω) ei ω1
c r cos(ϕ0−α)
P0(r, α, ω; ϕ0) = ˜P(n0, ω) ei ω
c r cos(ϕ0−α)
P(r, α, ω)
1
2π
dω1
1
2π
2π
0
dϕ0
FIGURE 32.12
Representation of the Fourier transform of a monofrequent plane wave in polar coordinates and its integration
with respect to angular frequency ω1 and direction of arrival ϕ0.
The relations for the Fourier transform of the general sound ﬁeld P(r, α, ω) from (32.69) to (32.74)
are compiled in (32.75) for easier reference
P(r, α, ω) =
∞

n=−∞
˚Pn(r, ω)einα
=
∞

n=−∞
˘Pn(ω)ineinα Jn

ω
c r

,
(32.75)
˜P(ϕ0, ω) =
∞

ℓ=−∞
˘Pℓ(ω)eiℓϕ0.
4.32.3.4.6
Orthogonality properties of the complex exponential functions
The relations in this section are based on Fourier series expansions and have exploited the orthogonality
properties of the complex exponential functions. Similar properties of more complicated functions are
required for three-dimensional sound ﬁelds in spherical coordinates in Section 4.32.3.5. To highlight
the similarities between these two different systems of orthogonal functions, orthogonality properties of
the complex exponential functions are compiled here. They serve as a review on the material on circular
harmonics and as well as for later reference from Section 4.32.3.5.
For the sake of a uniform notation, the designation Vn(ϕ) for the complex exponential functions is
introduced as
Vn(ϕ) = einϕ
(32.76)
with the index n, n ∈Z and the angle ϕ.

4.32.3 Signal Representations
939
Orthogonality relation with respect to the angle: The integration with respect to the angle ϕ over
one period of length 2π is equivalent to the integration around a circle. The associated orthogonality
property is expressed with the Kronecker symbol δμn as
1
2π
 2π
0
V ∗
μ(ϕ)Vn(ϕ)dϕ = 1
2π
 2π
0
ei(n−μ)ϕdϕ = δμn.
(32.77)
It is easily proven by evaluating the integral separately for the cases n = μ and n ̸= μ.
Closure equation with respect to the index: The functions Vn(ϕ) form a complete set of orthogonal
functions and therefore satisfy a closure relation (see [23]). It can be derived by summation with respect
to the index n. Applying the orthogonality property (32.77) gives
1
2π
∞

n=−∞
V ∗
n (ϑ)Vn(ϕ) = 1
2π
∞

n=−∞
ein(ϑ−ϕ) = δ(ϑ −ϕ)
(32.78)
with the delta function δ(ϕ).
Compilation of Fourier series expansions: Table 32.2 compiles the Fourier series expansions used in
Section 4.32.3.4. The ﬁrst one is the closure relation from (32.78). The factor 1/(2π) can be interpreted
as a series of Fourier coefﬁcients that are constant with respect to n; the associated function is the delta
impulse, see (32.79). When the Fourier series coefﬁcients are expressed by Bessel functions as in Jn(kr),
then the Fourier series expansion of a plane wave results in (32.80). Multiplying these Bessel coefﬁcients
by an arbitrary sequence of coefﬁcients ˘Pn(ω) gives the Fourier series expansion of a general sound
ﬁeld P(r, ϕ −ϑ, ω) in (32.81).
4.32.3.5 Spherical harmonics
Building a three-dimensional sound ﬁeld from plane wave components follows the same pattern as
described above for components from the horizontal plane. The main difference is obviously that now
Table 32.2 Compilation of Fourier Series Expansions Used in Section 4.32.3.4
1
2π
∞

n=−∞
V ∗
n (ϑ)Vn(ϕ) =
1
2π
∞

n=−∞
ein(ϑ−ϕ) = δ(ϑ −ϕ)
(32.79)
∞

n=−∞

inJn

ω
c r

V ∗
n (ϑ)Vn(ϕ) =
1
2π
∞

n=−∞

inJn

ω
c r

ein(ϑ−ϕ)
= exp

i ω
c r cos (ϑ −ϕ)

(32.80)
∞

n=−∞
˘Pn(ω)inJn

ω
c r

V ∗
n (ϑ)Vn(ϕ) = P(r, ϕ −ϑ, ω)
(32.81)

940
CHAPTER 32 Sound Field Synthesis
plane waves not only from all around the horizontal plane but also from above and below are involved.
This means in mathematical terms that the integration around a circle that frequently arises in Section
4.32.3.4 is here replaced by an integration on a sphere. The presentation for three-dimensional sound
ﬁelds therefore repeats the same steps as for planar sound ﬁelds in Section 4.32.3.4.
4.32.3.5.1
Monofrequent plane waves in spherical coordinates
The superposition of plane waves from all directions in the three-dimensional space requires spherical
coordinates as introduced in Section 4.32.2.2. At ﬁrst, the deﬁnition of the phasor from (32.29) is updated
such that the argument of the complex amplitude ˜P(ϕ0, θ0, ω1) includes also the zenith angle θ0
p01(x, t; ϕ0, θ0, ω1) = ˜P(ϕ0, θ0, ω1)eiω1(t+t0(x)).
(32.82)
The time delay t0(x) is given by (32.48), but the vector of space variables x and the normal vector n0
are deﬁned in three spatial spherical coordinates according to (32.6) by
x =
⎡
⎣
x
y
z
⎤
⎦= r
⎡
⎣
cos α sin β
sin α sin β
cos β
⎤
⎦,
n0 =
⎡
⎣
cos ϕ0 sin θ0
sin ϕ0 sin θ0
cos θ0
⎤
⎦.
(32.83)
The scalar product nT
0 x follows now from (32.83) by manipulation with standard trigonometric
relations
nT
0 x = r( cos ϕ0 sin θ0 cos α sin β + sin ϕ0 sin θ0 sin α sin β + cos θ0 cos β)
= r( sin θ0 sin β( cos ϕ0 cos α + sin ϕ0 sin α) + cos θ0 cos β)
= r( sin θ0 sin β cos (ϕ0 −α) + cos θ0 cos β) = r cos γ.
(32.84)
The angle γ is again the angle included by the vectors x and n0, now in three-dimensional space, see
Figure 32.13.
The plane wave from (32.82) can be expressed similar to (32.62) as
p01(x, t; ϕ0, θ0, ω1) = ˜P(ϕ0, θ0, ω1)eiω1tei ω1
c r cos γ ,
(32.85)
with the angle γ deﬁned from (32.84). The dependence on the spherical coordinates is written explicitly
by including radius r and the angles α and β in the list of parameters as
p01(r, α, β, t; ϕ0, θ0, ω1) = ˜P(ϕ0, θ0, ω1)eiω1tei ω1
c r cos γ .
(32.86)
This relation looks similar to the corresponding one in (32.62), but the main difference is that the angles
are now deﬁned with respect to a sphere, see Figure 32.13. As a consequence, the expansion into basis
functions has to take the resulting spherical symmetry into account.
4.32.3.5.2
Deﬁnition of the spherical harmonic functions
The spherical harmonic functions or short spherical harmonics are an orthogonal basis for functions
deﬁned on a sphere. Their deﬁnition and the notation varies slightly in the literature on mathematics,

4.32.3 Signal Representations
941
x
y
z
n0
x
γ
FIGURE 32.13
Spherical coordinate system with direction n0 of a plane wave.
acoustics, quantum physics, and other ﬁelds of science. The deﬁnition and the description of their
properties used here follow [24].
The spherical harmonic functions Y m
n (θ, ϕ) may be deﬁned as
Y m
n (θ, ϕ) = C(m, n)P|m|
n
( cos θ)eimϕ,
(32.87)
The set of functions Pm
n ( cos θ) are the associated Legendre functions of nth degree and mth order.
They are deﬁned in terms of the Legendre polynomials Pn(x) by
Pm
n (x) = (−1)m(1 −x2)
m
2 dm
dxm Pn(x),
∀n, m ≥0,
(32.88)
P−m
n
(x) = (n−m)!
(n+m)! Pm
n (x).
(32.89)
The Legendre polynomials themselves are determined by their generating function
(1 −2xy + y2)−1
2 =
∞

n=0
Pn(x)yn
|y| < 1.
(32.90)
Thus the index n indicates the degree of the Legendre polynomial Pn(x) while the index m indicates
the order of the differentiation in (32.88) for the deﬁnition of the associated Legendre function Pm
n (x).
The factor C(m, n) is chosen such that the spherical harmonic functions form an orthonormal basis
with respect to integration on a sphere (see (32.92)), e.g., as
C(m, n) = (−1)m

2n + 1
4π
(n −|m| )!
(n + |m| )!.
(32.91)

942
CHAPTER 32 Sound Field Synthesis
Other deﬁnitions of the spherical harmonic functions are also in use, which differ from (32.87)
and (32.91) mainly with respect to the factor (−1)m. The choice of deﬁnition is a matter of taste and
convention. The present deﬁnition has found to be the most ﬂexible one.
A note on the notation is required here. The designations Pn(x) for the Legendre polynomials and
Pm
n (x) for the associated Legendre functions have been chosen here because they are commonly used in
the mathematical literature. They must not be confused with the Fourier coefﬁcients ˚Pn(r, ω) or ˘Pn(ω)
e.g., in (32.74). This ambiguity in the notation is permissible here since the further presentation uses
almost exclusively the spherical harmonic functions Y m
n (θ, ϕ).
4.32.3.5.3
Orthogonality relations for spherical harmonic functions
The properties of the Legendre polynomials are covered in the mathematical literature on orthogonal
polynomials. The extension to the associated Legendre functions and to the spherical harmonic functions
are found e.g., in [22,23]. Therefore the derivation of the spherical harmonic functions is not repeated
here. Some clues on their role as solution of the acoustic wave equation are given in Section 4.32.3.5.
In the context of sound ﬁeld reproduction, mainly the orthogonality properties of the spherical
harmonics are of importance. They are compiled below following the presentation in [23] and in parallel
to the orthogonality properties of the complex exponentials in Section 4.32.3.4.
Orthogonality with respect to the azimuth and zenith angle: An integration on a sphere is accom-
plished by an integration on a circle in the horizontal plane (azimuth angle ϕ, compare (32.77)) and an
integration of the zenith angle θ. It establishes the following orthogonality relation
 2π
0
 π
0
Y m1∗
n1
(θ, ϕ)Y m2
n2 (θ, ϕ) sin θ dθ dϕ = δn1n2δm1m2.
(32.92)
Closure equation with respect to the indices n and m: A double summation of the spherical harmonics
with respect to the degree n and mth order leads to the closure equation
∞

n=0
n

m=−n
Y m∗
n (θ, ϕ)Y m
n (β, α) = δ(ϕ −α)δ( cos θ −cos β) = δ(ϕ −α)δ(θ −β)
1
sin θ . (32.93)
Orthogonal expansion of a function deﬁned on a sphere: A function f (θ, ϕ) of the azimuth angle
ϕ and the zenith angle θ is called a function which is deﬁned on a sphere. The orthogonality relation
(32.92) allows to expand such functions into spherical harmonics as
f (θ, ϕ) =
∞

n=0
n

m=−n
fmnY m
n (θ, ϕ).
(32.94)
The expansion coefﬁcients fmn are derived by application of (32.92).
Expansion of a plane wave into spherical waves: An example for the expansion of a function deﬁned
on a sphere is the expansion of a plane wave into spherical harmonics
4π
∞

n=0
n

m=−n
in jn(kr)Y m∗
n (θ, ϕ)Y m
n (β, α) = eikr cos γ ,
(32.95)

4.32.3 Signal Representations
943
where γ = γ (α, β, ϕ, θ) is related to α, β, ϕ, θ by (compare (32.84))
cos γ = sin θ sin β cos (ϕ −α) + cos θ cos β.
(32.96)
The expansion coefﬁcients include the spherical Bessel function jn(kr) which are described in mathe-
matical texts on special functions e.g., [23].
4.32.3.5.4
Expansion of broadband plane waves into orthogonal functions
The orthogonality relations for spherical harmonic functions allow to expand the monofrequent plane
wave from (32.82) and its generalizations into orthogonal functions. The presentation follows closely
the circular case from Section 4.32.3.4.
At ﬁrst, the monofrequent plane wave (32.82) is integrated with respect to its angular frequency ω1
to obtain a broadband plane wave (see (32.67))
p0(r, α, β, t; ϕ0, θ0) = 1
2π
 ∞
−∞
p01(r, α, β, t; ω, ϕ0, θ0)dω1.
(32.97)
Similar to (32.68) follows its Fourier transform with respect to time
P0(r, α, β, ω; ϕ0, θ0) = Ft{p0(r, α, β, t; ϕ0, θ0)} = ˜P(ϕ0, θ0, ω)ei ω
c r cos γ ,
(32.98)
where the angle γ is deﬁned on a sphere as shown in Figure 32.13. The expansion into spherical
harmonics is given by (32.95) as
P0(r, α, β, ω; ϕ0, θ0) = 4π ˜P(ϕ0, θ0, ω)
∞

n=0
n

m=−n
in jn

ω
c r

Y m∗
n (θ0, ϕ0)Y m
n (β, α).
(32.99)
4.32.3.5.5
Expansion of general sound ﬁelds into orthogonal functions
The corresponding expansion for general sound ﬁelds in a source-free domain follows by superposition
of plane wave contributions (32.99) from all directions ϕ0 and θ0
P(r, α, β, ω) =
 2π
0
 π
0
P0(r, α, β, ω; ϕ0, θ0) sin θ dθ0 dϕ0.
(32.100)
Applying the spherical integration with respect to ϕ0 and θ0 to the series expansion (32.99) gives
P(r, α, β, ω) = 4π
∞

n=0
n

m=−n
in jn

ω
c r

˘Pm
n (ω)Y m
n (β, α)
(32.101)
with the coefﬁcients
˘Pm
n (ω) =
 2π
0
 π
0
˜P(ϕ0, θ0, ω)Y m∗
n
(θ, ϕ) sin θ dθ dϕ
(32.102)

944
CHAPTER 32 Sound Field Synthesis
of the expansion
˜P(ϕ0, θ0, ω) =
∞

n=0
n

m=−n
˘Pm
n (ω)Y m
n (θ, ϕ).
(32.103)
Similar to (32.73) and (32.74), a general sound ﬁeld in the three-dimensional space is given by the
expansion
P(r, α, β, ω) =
∞

n=0
n

m=−n
˚Pm
n (r, ω)Y m
n (β, α)
(32.104)
with the expansion coefﬁcients
˚Pm
n (r, ω) = 4πin jn

ω
c r

˘Pm
n (ω).
(32.105)
Note that these expansion coefﬁcients are again separable with respect to the radius r and the mode
numbers n, m.
4.32.3.5.6
Relation between spherical harmonics expansions and Fourier series
Since the expansion (32.101) converge uniquely and uniformly above a certain threshold, the order of
summation may be exchanged [24]. If the spherical harmonics Y m
n (β, α) are then expressed by their
explicit formulation (32.87), the Fourier series that is inherent to (32.101) is revealed. It is given by
P(x, ω) =
∞

m=−∞
˚Pm(r, β, ω)eimα
(32.106)
with the Fourier coefﬁcients
˚Pm(r, β, ω) =
∞

n=|m|
4πin ˘Pm
n (ω) jn

ω
c r

C(m, n)P|m|
n
( cos β).
(32.107)
The normalization coefﬁcients C(m, n) are deﬁned by (32.91). Note that the Fourier coefﬁcients
˚Pm(r, β, ω) of the spherical harmonics in (32.107) correspond to the Fourier coefﬁcients ˚Pn(r, ω)
for the circular case in (32.73). On the other hand P|m|
n
( cos β) are the associated Legendre Functions
from (32.87).
4.32.3.5.7
Summary of the representations of a general sound ﬁeld
The transition from a monofrequent plane wave in spherical coordinates to a broadband plane wave and
to a general sound ﬁeld is shown in Figure 32.14 in a similar way as for polar coordinates in Figure 32.12.

4.32.3 Signal Representations
945
P01(r, α, β, ω; ω1, ϕ0, θ0) = 2π ˜P(ϕ0, θ0, ω1)δ(ω1 −ω) ei ω1
c r cos γ
P0(r, α, β, ω; ϕ0, θ0) = ˜P(ϕ0, θ0, ω) ei ω
c r cos γ
P(r, α, β, ω)
1
2π
dω1
2π
0
π
0
sin θ0 dθ0dϕ0
FIGURE 32.14
Representation of the Fourier transform of a monofrequent plane wave in spherical coordinates and its
integration with respect to angular frequency ω1 and direction of arrival ϕ0, θ0.
The relations for the Fourier transform of the general sound ﬁeld P(r, α, ω) from (32.100) to (32.107)
are compiled in (32.108)
P(r, α, β, ω) =
∞

m=−∞
˚Pm(r, β, ω)eimα
=
∞

n=0
n

m=−n
˚Pm
n (r, ω)Y m
n (β, α)
(32.108)
=
∞

n=0
n

m=−n
˘Pm
n (ω)4πinY m
n (β, α) jn

ω
c r

,
˚Pm
n (r, ω) = 4πin jn

ω
c r

˘Pm
n (ω).
4.32.3.5.8
Derivation of the spherical harmonic functions
The derivation of the spherical harmonic functions Y m
n (θ, ϕ) is found in many textbooks on acoustics
[16,22,24] or on mathematical methods in physics [23,25]. The essential steps are
1. Express the Laplace operator in the acoustic wave equation in spherical coordinates.
2. Solve the wave equation by the method of separation of variables to obtain separate ordinary differ-
ential equations with respect to time, radius, azimuth and zenith angle.
3. Obtain the solution of the differential equation with respect to time in the form of complex expo-
nentials for time and angular velocity (t, ω).
4. Obtain the solution of the differential equation with respect to the radius in the form of spherical
Bessel functions with respect to radius and wave number (r, k).

946
CHAPTER 32 Sound Field Synthesis
5. Obtain the solution of the differential equation with respect to the azimuth angle in the form of
complex exponentials for azimuth angle and mode number (ϕ, n).
6. Obtain the solution of the differential equation with respect to the zenith angle in the form of
associated Legendre functions for zenith angle and mode numbers (θ, n, m).
7. Form the spherical harmonic functions as the product of the complex exponentials for azimuth angle
and mode number and the associated Legendre functions.
The complete process is somewhat tedious and involves various special functions from higher math-
ematics. Since it is well covered in the standard literature as referenced above, no derivation is given
here.
4.32.4 Response to sound sources
The response of a sound ﬁeld to an acoustic point source is described by the so-called Green’s function.
This section derives some of its basic properties from the acoustic wave equation. Then the Green’s
function is used to obtain the sound ﬁeld within a certain spatial region when interior sources or boundary
values are given. The latter case serves to introduce the Kirchhoff-Helmholtz integral equation for later
use in Section 4.32.5. Finally the Green’s function for wave propagation in the free-ﬁeld is derived. The
representation follows classic texts like [15–17,25] and has been adapted from [26].
4.32.4.1 The inhomogenous wave equation
From the homogeneous acoustic wave Eq. (32.3) follows the inhomogeneous wave equation by con-
sidering a source term p0(x, t)
p(x, t) −1
c2
∂2
∂t2 p(x, t) = p0(x, t).
(32.109)
Fourier transformation Ft according to (32.35) leads to the frequency domain version of the acoustic
wave Eq. (32.109), the so-called Helmholtz equation
P(x, t) +

ω
c
2
P(x, ω) = P0(x, ω)
x ∈V .
(32.110)
It is valid within a spatial region V which may be bounded by the walls of an enclosure. However, V
may also be an arbitrary volume in the free ﬁeld and not directly related to the walls of an acoustic
environment.
For later reference the operations on the left hand side of (32.110) can be abbreviated by the linear
operator
L{P(x, ω)} = P(x, t) +

ω
c
2
P(x, ω).
(32.111)
This notation allows a very concise notation for the Helmholtz equation as
L{P(x, ω)} = P0(x, ω)
x ∈V .
(32.112)

4.32.4 Response to Sound Sources
947
4.32.4.2 Green’s function
The effect of a source distribution P0(x, ω) on the complete sound ﬁeld in V is described by the Green’s
function. This section presents its properties and emphasizes its role for the calculation of sound ﬁelds.
4.32.4.2.1
Properties
The Green’s function describes the effect of a point source at the location ξ on the sound ﬁeld P(x, ω) at
the location x. This effect depends also on the spectrum of the source signal, therefore the Green’s func-
tion G(x|ξ, ω) has two space variables and one frequency variable. To highlight the close relationship
of the two space variables, they are separated in formulas by a | rather than by a comma.
The Green’s function can be regarded as an equivalent to the impulse response of a one-dimensional
system, because the impulse response h(t, τ) describes the effect of the input signal at time τ on the
output signal at time t. For time-invariant systems, the impulse response depends only on the difference
t −τ and is written as h(t).
The propagation of sound in enclosures depends on the distance of sources to the walls. Therefore
the signal at a receiver varies also when the sources and the receiver move synchronously within an
enclosure. Thus sound propagation in the presence of reﬂecting surfaces is a shift-variant process.
Furthermore there is no preferred direction of sound propagation comparable to the ﬂow of time. This
means that the Green’s function is not one-sided with respect to space. In summary, contrary to the
impulse response of linear and time-invariant systems, the Green’s function for the propagation of
sound is not one-sided and in general also not shift-invariant. For one-dimensional time-dependent
systems, one-sided impulse responses are closely connected to causality. For multidimensional systems
the issue of causality is more involved [27].
4.32.4.2.2
The Green’s function deﬁned by a differential equation
The Green’s function allows to write the sound pressure as a response to the source distribution P0(x, ω)
by integrating all source locations ξ ∈V
P(x, ω) =

V
G(x|ξ, ω)P0(ξ, ω)dV .
(32.113)
The integration is performed with respect to the volume V which encloses all sources with amplitude
P0(x, ω), and all locations where the sound pressure P(x, ω) is of interest. Typically this volume is
deﬁned by the walls of a room. Only in reﬂection-free environments does shift-invariance hold and the
integral (32.113) turns into a convolution.
To obtain an equation for the determination of the Green’s function, apply the differential operator
L from (32.111) to (32.113)
L{P(x, ω)} =

V
L{G(x|ξ, ω)}P0(ξ, ω)dV = P0(x, ω).
(32.114)
For the right equality to be valid, the Green’s function must satisfy the Helmholtz equation with a spatial
delta-impulse as inhomogenity
L{G(x|ξ, ω)} = G(x|ξ, ω) + β2G(x|ξ, ω) = δ(x −ξ).
(32.115)

948
CHAPTER 32 Sound Field Synthesis
In principle, the Green’s function G(x|ξ, ω) can be obtained from the differential equation (32.115)
with the appropriate boundary conditions corresponding the acoustic environment. However, for simple
cases (e.g., free ﬁeld) the Green’s function can also be calculated without solving a boundary-value
problem (see Section 4.32.4.4).
4.32.4.3 Calculation of the response to interior and exterior sources
When the Green’s function is known, it can be used to calculate the response to sources within V using
(32.113). However, sources outside of V are not considered by (32.113). But exactly those sources are
of interest for the synthesis of sound ﬁelds.
This section discusses the complete sound ﬁeld resulting from sources both within and outside of V .
Although the general idea is covered in texts on mathematical physics like [25] a concise self-contained
derivation is presented here. Its essential component is the Gauss integral theorem as a generalization
of the integration by parts.
4.32.4.3.1
Solution of the wave equation
The derivation starts with a so-called divergence expression [25] which contains the Green’s function, the
unknown sound pressure, and their gradients. To simplify the notation, arguments may be omitted, i.e.,
P(x, ω) = P(x) = P
and G(x|ξ, ω) = G(x|ξ) = G.
Now consider the divergence of ∇G · P −G · ∇P. Inserting ±( ω
c )2G P yields an expression which
contains the differential operator L of the wave equation
∇(∇G · P −G · ∇P) =

G +

ω
c
2
G

· P −G ·

P +

ω
c
2
P

= L{G} · P −G · L{P}.
(32.116)
Integrationwithrespecttothevolume V onbothsidesandapplicationoftheGaussintegraltheoremgives

∂V
(∇G · P −G · ∇P)d A =

V
L{G} · P dV −

V
G · L{P}dV .
(32.117)
The surface of the volume V is denoted by ∂V and n is the corresponding unit vector orthogonal to the
surface ∂V . The oriented surface element d A(ξ) is given in terms of the scalar surface element d A(ξ)
as d A = n d A. The dependence on ξ is sometimes omitted in the notation.
Solving for the term with L{G} gives

V
L{G} · P dV =

V
G · L{P}dV +

∂V
(∇G · P −G · ∇P)d A.
(32.118)
So far the derivation is valid for almost arbitrary functions P and G. Now the special properties of P
as solution of the Helmholtz equation according to (32.112) and of G als Green’s function according
to (32.115) are used. Then the term on the left-hand-side of (32.118) becomes

V
L{G(x|ξ)} · P(ξ)dV =

V
δ(x −ξ)P(ξ)dV = P(x).
(32.119)

4.32.4 Response to Sound Sources
949
such that (32.118) turns into
P(x) =

V
G(x|ξ)P0(ξ)dV +

∂V
(∇G · P −G · ∇P)d A.
(32.120)
Thus the solution P(x, ω) of the wave equation consists of two components. The ﬁrst one represents the
response to the sources P0(x, ω) within V , while the second one considers the behavior of the sound
pressure P and of the Green’s function G on the boundary. Both components are now discussed in
detail.
4.32.4.3.2
Discussion of the two components of the solution
Sources within V: When P(x, ω) and G(x|ξ, ω) satisfy the same homogenous boundary conditions
on ∂V , then the boundary integral in (32.120) vanishes and only the response to the sources within V
remains
P(x, ω) =

V
G(x|ξ, ω)P0(ξ, ω)dV .
(32.121)
Typical homogeneous boundary conditions arise from the nature of the volume V . If its surface ∂V
coincides with the walls of an enclosure then the boundary conditions may be
•
P = 0 and G = 0 (pressure release surface, “soft wall”),
•
∇P = 0 and ∇G = 0 (ideally reﬂecting surface, “hard wall”),
•
P + ℓ0nT∇P = 0 and G + ℓ0nT∇G = 0
(impedance boundary conditions with suitable wall factor ℓ0).
On the other hand, the volume V may lie completely in the free ﬁeld such that free ﬁeld boundary
conditions apply. They correspond to the impedance boundary conditions above with the wall factor
replaced by the free ﬁeld impedance.
Boundary values: Now assume that there are no interior sources P0(x, ω), but the sound pressure or
its gradient do not vanish at the boundary. Then the sound pressure within V is only determined by the
behavior on the boundary
P(x, ω) =

∂V

∇G(x|x0, ω)P(x0, ω) −G(x|x0, ω)∇P(x0, ω)

d A(x0).
(32.122)
Note that ∂V denotes the boundary, x0 is a location on the boundary and d A(x0) an inﬁnitesimal
oriented surface element. Without interior sources, the sound pressure at the boundary can only be
caused by sound sources outside of V . The term (32.122) thus reﬂects the response to exterior sources.
It also represents one of several possible forms of the Kirchhoff-Helmholtz integral equation which is
discussed in more detail in Section 4.32.5.1.
4.32.4.4 Calculation of the Green’s function
The determination of the Green’s function requires the solution of a boundary value problem consist-
ing of the differential equation (32.115) and the corresponding boundary conditions. These boundary
conditions are usually given by the surfaces of the enclosure and are hard to describe analytically.

950
CHAPTER 32 Sound Field Synthesis
The most simple case is free ﬁeld propagation. In reproduction rooms where systems for sound ﬁeld
synthesis are installed this case is approximated by preparing the surfaces to be absorbing.
This section presents the Green’s functions for the free ﬁeld case. At ﬁrst one-dimensional spatial
propagation is considered as a preparation for the three-dimensional case.
4.32.4.4.1
One spatial dimension
In one spatial dimension, the vector x of spatial variables becomes a scalar x and the volume V consists
of an interval on the x-axis. For free ﬁeld propagation, i.e., without reﬂections at the boundaries of
the interval, V covers the complete x-axis. The Laplace operator is simply the second derivative with
respect to the space variable  = div grad = ∂2/∂x2.
A monopole source at the location x0 with a signal p0(t) is then described by
p0(x, t) = p0(t)δ(x −x0),
(32.123)
or after Fourier transform with respect to time by
P0(x, ω) = P0(ω)δ(x −x0).
(32.124)
Since the response to this source propagates with the sound speed c way from the source, its Green’s
function is given by the shift operator
G(x|ξ, ω) = exp

−i ω
c r(x, ξ)

.
(32.125)
Here r(x, ξ) = |x −ξ| is the distance between the location x where the sound pressure is observed and
the location of the source ξ. This notation is chosen with respect to the following extension to three
dimensions.
The suitability of this Green’s function is conﬁrmed by inserting (32.124) into (32.121)
P(x, ω) =

V
G(x|ξ, ω)P0(ω)δ(ξ −x0)dξ
= P0(ω)G(x|x0, ω) = P0(ω) exp

−i ω
c r(x, ξ)

(32.126)
and subsequent inverse Fourier transform into the time domain
p(x, t) = p0

t −r(x, ξ)
c

.
(32.127)
The Green’s function (32.125) produces a sound pressure distribution p(x, t) which corresponds to the
source signal p0(t) including a time shift, which results from the distance r to the source and speed of
sound c. This is exactly what is expected from wave propagation.
4.32.4.4.2
Three spatial dimensions
The determination of the Green’s function of a monopole source in three spatial dimensions can be
inferred from the result for one spatial dimension. To this end, a function is required which describes

4.32.5 Physical Foundations of Sound Field Synthesis
951
the sound ﬁeld of a monopole according to the differential Eq. (32.115). Since the sound waves from a
monopole source propagate in a spherical fashion, it is of advantage to use spherical coordinates. Then
the Cartesian coordinates (x, y, z) are replaced by the radius, the azimuth and the zenith (or elevation)
angles (r, ϕ, δ). In these coordinates the Laplace operator is given by
 = 1
r2
∂
∂r

r2 ∂
∂r

+
1
r2 sin δ
∂
∂δ

sin δ ∂
∂δ

+
1
r2 sin2 δ
∂2
∂ϕ2 .
(32.128)
Due to the point symmetry of the solution all derivatives with respect to the angles ϕ and δ are zero
and the Laplace operator contains only derivatives with respect to the radius r. Its effect on the sound
pressure p takes different forms as
p = 1
r2
∂
∂r

r2 ∂
∂r p

= ∂2
∂r2 p + 2
r
∂
∂r p = 1
r
∂2
∂r2 (r p).
(32.129)
The wave equation (32.111) is now written with (32.128) and after multiplication with r as
∂2
∂r2

r p

−1
c2
∂2
∂t2

r p

= 0.
(32.130)
This equation is a wave equation with a scalar space variable r for the unknown r p(r, t).
The Green’s function for the scalar, i.e., spatially one-dimensional case is already known from
(32.125). The relation between the Green’s function for the one-dimensional case (designated here by
G1D) and the Green’s function for the three-dimensional case (designated by G3D) follows as
G1D(x|ξ, ω) = r G3D(x|ξ, ω)
(32.131)
with r = |x −ξ|. Inserting (32.125) gives
G3D(x|ξ, ω) = 1
r exp

−i ω
c r

= exp

−i ω
c |x −ξ|

|x −ξ|
.
(32.132)
The Green’s function for the three-dimensional case G3D(x|ξ, ω) describes a spherical wave that prop-
agates from the location ξ with the speed c. Its amplitude decreases with increasing distance r.
4.32.5 Physical foundations of sound ﬁeld synthesis
This section discusses some of the physical foundations of sound ﬁeld synthesis. They are exploited for
the speciﬁc synthesis methods discussed in the subsequent sections.
4.32.5.1 The Kirchhoff-Helmholtz integral equation
Sound ﬁeld synthesis aims at synthesizing a desired sound ﬁeld within an extended area V by sources
located on the boundary ∂V . In this context the desired sound ﬁeld is assumed to originate from a
virtual source and the sources on ∂V are termed as secondary sources. Figure 32.15 illustrates the

952
CHAPTER 32 Sound Field Synthesis
virtual
source
S(x, ω)
P(x, ω)
x
x0
n
V
∂V
0
FIGURE 32.15
Illustration of the geometry used to discuss the physical foundations of sound ﬁeld synthesis. The Kirchhoff-
Helmholtz integral equation states that the sound pressure distribution within the area V is uniquely deter-
mined by the pressure and the gradient of the pressure of the virtual source S(x, ω) on the boundary ∂V .
situation. The sound pressure P(x, ω) within V can be calculated by interpreting the secondary sources
on ∂V as inhomogeneous boundary condition. The solution of the homogeneous wave equation for
inhomogeneous boundary conditions is provided by (32.122), which is also known as the Kirchhoff-
Helmholtz integral equation.
Here, the integration of a gradient with respect to an oriented surface element d A can also be
expressed by the directional gradient ∂/∂n as ∇P d A = ∂P/∂n d A and similarly for ∇G. The
Kirchhoff-Helmholtz integral equation (32.122) reads then
P(x, ω) =

∂V
 ∂
∂nG(x|x0, ω)P(x0, ω) −G(x|x0, ω) ∂
∂n P(x0, ω)

d A(x0).
(32.133)
The Green’s function G(x|x0, ω) has to fulﬁll the homogeneous boundary conditions imposed on ∂V .
For sound ﬁeld synthesis free-ﬁeld propagation within V is typically assumed, hence that V is free
of any objects and that the boundary ∂V does not restrict propagation. The Green’s function is then
given as the free-ﬁeld solution of the wave equation and is referred to as free-ﬁeld Green’s function
G0(x −x0, ω). It can be interpreted as the spatio-temporal transfer function of an acoustic monopole
placed at location x0 and its directional gradient as the spatio-temporal transfer function of an acoustic
dipole at location x0, whose main axis is parallel to n [22].
Equation (32.133) states that the pressure P(x, ω) inside V is uniquely determined by the pressure
P(x0, ω) and its directional gradient on the boundary ∂V . If the Green’s function is realized by a
continuous distribution of appropriately driven monopole and dipole sources that are placed on the

4.32.5 Physical Foundations of Sound Field Synthesis
953
boundary ∂V , the sound ﬁeld within V can be fully controlled within the volume V . In potential theory
the continuous distribution of secondary monopole/dipole sources is also termed as single/double layer
potential [28].
For sound ﬁeld synthesis it is desired to synthesize the pressure ﬁeld S(x, ω) of the virtual source
inside the area V . Concluding the considerations given so far, this can be achieved by a continuous
distribution of secondary monopole and dipole sources located on the boundary ∂V of the listening area
V , which are driven by the directional gradient and the pressure of the sound ﬁeld S(x, ω) of the virtual
source, respectively. For the application of the introduced principle, all source contributions of S(x, ω)
are assumed to lie outside of V . The scattering occurring due to listeners inside the listening area does
not affect the synthesis in an unfavorable way because the scattered sound ﬁeld does not depend on the
process generating the incident sound ﬁeld. Hence, the scattering is equal either if the incident sound
ﬁeld is emerging from a source with the characteristics of the virtual source or if the sound ﬁeld of
the virtual source is synthesized by the secondary sources [29]. A listener thus experiences the same
scattering in a synthetic sound ﬁeld like in the corresponding natural one.
4.32.5.2 Monopole only synthesis
It is desirable for a practical implementation to discard one of the two types of secondary sources stated
by the Kirchhoff-Helmholtz integral (32.133). Typically the dipole sources are removed, since monopole
sources can be realized reasonably well by (commercially available) loudspeakers with closed cabinets.
The sound ﬁeld for monopole only synthesis is expressed by the synthesis equation
P(x, ω) =

∂V
D(x0, ω)G(x|x0, ω)d A(x0),
(32.134)
where D(x0, ω) denotes the strength of the secondary source at position x0, which is denoted as
secondary source driving signal. Assuming again free-ﬁeld propagation, G(x|x0, ω) in (32.134) can be
specialized to the free-ﬁeld Green’s function G0(x −x0, ω). For sound ﬁeld synthesis, the synthesized
pressure P(x, ω) should be equal to the pressure ﬁeld of the virtual source S(x, ω) within the listening
area V .
A variety of techniques have been proposed in the past decades to obtain a monopole-only formulation
for sound ﬁeld synthesis. Here, we only discuss those techniques that have led to the well-known
approaches that will be outlined later in this chapter. In particular
1. modiﬁcation of the Green’s function employed in the Kirchhoff-Helmholtz integral,
2. the simple source approach, and
3. explicit solution of the single layer potential integral equation.
These three techniques are discussed brieﬂy in the following subsections.
4.32.5.2.1
Neumann Green’s function
The ﬁrst addend in the Kirchhoff-Helmholtz integral (32.133) can be suppressed by choosing a Neumann
Green’s function GN(x|x0, ω) with
∂
∂nGN(x|x0, ω)

x0∈∂V
= 0.
(32.135)

954
CHAPTER 32 Sound Field Synthesis
Under this condition the Kirchhoff-Helmholtz integral (32.133) simpliﬁes to
P(x, ω) = −

∂V
G N(x|x0, ω) ∂
∂n S(x0, ω)d A(x0).
(32.136)
The explicit form of the Neumann Green’s function depends on the geometry of the boundary ∂V . A
closed form solution can only be found for rather simple geometries like spheres or planar boundaries
[22]. The physical boundary condition (32.135) imposed onto the Neumann Green’s function models
the boundary ∂V as acoustically rigid. For frequencies that are not equal to the resonance frequencies
of the rigid cavity V , the synthesized sound ﬁeld is equal to the virtual source within V due to the
uniqueness of (32.136). Hence, the driving signal is given as the directional gradient of the pressure of
the virtual source. A major problem of this approach is that the Neumann Green’s function has to be
realized by physically existing secondary sources. Other acoustic sources than monopoles or dipoles are
typically not available in practice, which renders this approach unfeasible. However, it will be shown
later that a sensible approximation of (32.136) forms the basis of WFS.
4.32.5.2.2
Simple source approach and equivalent scattering problem
The second technique, the simple source approach, is based on constructing an acoustic scenario that
results in a single layer potential formulation. One way of doing so is to follow the procedure discussed
in [22] by constructing two equivalent but spatially disjunct problems. Besides the interior Kirchhoff-
Helmholtz integral, given by (32.133), an equivalent exterior Kirchhoff-Helmholtz integral is formulated
with the same boundary ∂V but outward pointing normal vector. Note that in this case the exterior and
interior regions swap places. It is further assumed that the pressure is continuous and the directional
gradient is discontinuous when approaching the boundary ∂V from both sides. These assumptions
represent the presence of a secondary source layer. Subtracting the resulting interior from the exterior
problem formulation derives
P(x, ω) =

∂V
D(x0, ω)G0(x −x0, ω)d A(x0),
(32.137)
where D(x0, ω) denotes the driving signal of the secondary sources. The continuity conditions for the
pressure and its gradient on the boundary ∂V can be interpreted in terms of an equivalent scattering
problem [30]. Here, V is replaced by a sound soft object with pressure release boundaries that scatters
the ﬁeld of the virtual source. The driving signal for the simple source approach is then given as
D(x0, ω) = ∂
∂n S(x0, ω) + ∂
∂n PS(x0, ω),
(32.138)
where PS(x, ω) denotes the pressure of the scattered ﬁeld in the exterior region. The ﬁeld in the interior
region V matches the ﬁeld of the virtual source S(x, ω). The insights provided by the simple source
approach link the results from acoustic scattering theory to SFS.
4.32.5.2.3
Explicit solution
Equation (32.134) constitutes an integral equation, which can be solved explicitly with respect to the
drivingsignal D(x0, ω).Accordingtooperatortheory[31–33],theintegralin(32.134)canbeunderstood
as a (compact) Fredholm operator of index zero.

4.32.5 Physical Foundations of Sound Field Synthesis
955
As stated by Fredholm’s theory [25], a solution can be found when the secondary source distribution
is simply connected and encloses the target volume. The general solution is found by expanding the
operator and the virtual sound ﬁeld into a series of orthogonal basis functions and a comparison of
coefﬁcients. It is known from operator theory that the solution is not unique at the eigenfrequencies
of the interior homogeneous Dirichlet problem and might be ill-conditioned in practice. Theoretically
suitable basis functions can be found for arbitrary simply connected domains V with a smooth boundary.
In practice analytic basis functions and solutions are only available for regular geometries like spheres,
cylinders and spheroids [15].
The circumstance that the secondary source distribution is required to enclose the target volume
constitutes an essential restriction. As will be shown in Sections 4.32.6.3 and 4.32.7, the Fredholm
solution can also be applied to non-enclosing geometries of the secondary source distributions. Though,
such non-enclosing secondary source distributions exhibit limitations.
4.32.5.3 Three-dimensional synthesis
The particular form of the free-ﬁeld Green’s function and hence the secondary sources depends on the
dimensionality of the problem. For a continuous distribution of secondary sources on a surface ∂V
surrounding the listening volume V , the three-dimensional free-ﬁeld Green’s function (32.132) is the
appropriate choice. It can be approximated reasonably well by loudspeakers in a practical realization.
This scenario is termed as three-dimensional synthesis.
It has been shown [34–36] that a three-dimensional synthesis can be perfect whereby certain restric-
tions can apply that are dependent of the geometry of the secondary source distribution.
4.32.5.4 2.5-dimensional synthesis
Inmanysituationsthesynthesisinaplaneonlyissuitable.Thisconstitutesinprincipleatwo-dimensional
scenario. From a physical point of view, the natural choice for the characteristics of the secondary
sources used for two-dimensional synthesis would be the elementary solution of the wave equation
in two dimensions. The resulting transfer function is given by the two-dimensional free-ﬁeld Green’s
function, which can be interpreted as the ﬁeld produced by a line source [22]. Loudspeakers exhibiting
the properties of acoustic line sources are not practical. Using point sources as secondary sources for the
synthesis in a plane results in a dimensionality mismatch, therefore such methods are often termed as
2.5-dimensional synthesis. Ideally the ears of the listeners should be in the same plane like the secondary
sources, the target plane. However it is well known from WFS and HOA, that even then 2.5-dimensional
synthesis techniques suffer from artifacts [35,37]. Most prominent are amplitude deviations with respect
to the sound ﬁeld of the virtual source. Similar artifacts will also be present in other sound ﬁeld synthesis
approaches that aim at correct synthesis in a plane using secondary point sources. These limitations are
often discarded in the design of numerical sound ﬁeld synthesis approaches, a circumstance that can
lead to excess regularization since the desired result is physically impossible.
4.32.5.4.1
Model- versus data-based rendering
In general, sound ﬁeld synthesis can be performed in either a model-based or a data-based fashion
[38]. With model-based objects, all spatial information such as the location of an object (e.g., a sound
source) or its radiation properties are described by physical models. A given virtual sound source may

956
CHAPTER 32 Sound Field Synthesis
be deﬁned as omnidirectional and being located at a given position that is speciﬁed using an appropriate
coordinate system. The associated audio signal is then the “input signal” to this source, e.g., a human
voice or the performance of a musical instrument captured with a single microphone. Another model-
based object could be the virtual venue, the boundary properties of which may be described by an
appropriate physical model.
The audio signals associated to data-based objects on the other hand do contain spatial information.
Examples are the signals of microphone arrangements that are composed of more than one microphone,
e.g., the main microphones of a Stereophonic recording or a spherical or other microphone array. In
the case of data-based rendering, a given sound ﬁeld synthesis system has to determine the loudspeaker
driving signals such that the spatial information contained in the input signals is preserved in the
presentation. Of course, both model-based as well data-based objects can be apparent in the same
scene. A typical scenario is synthesizing a virtual sound sources of given scene model based and then
adding reverberation obtained from microphone array measurements [39].
Note that the terms model-based and data-based auralization initially referred to auralization based
on either physical room models or databases of measured room impulse responses [40]. Here, the
broader use as explained above is preferred.
We focus in the following on model-based rendering using spatial models for the virtual source.
Often applied models in this context are plane waves, point sources or sources with a prescribed complex
directivity. For the driving signals derived in the following sections we will consider the synthesis of a
plane wave. This special case is suitable to illustrate the basic properties, since other source types can
be expressed as a superposition of plane waves [22].
4.32.6 Near-ﬁeld Compensated Higher Order Ambisonics (NFC-HOA)
Ambisonics is a collective term for a variety of sound ﬁeld synthesis approaches applied mainly to cir-
cular or spherical loudspeaker distributions. Data-based rendering is the traditional rendering technique
used in the context of Ambisonics. The expansion coefﬁcients of the desired sound ﬁeld are extracted
from microphone array recordings, transmitted and then used for derivation of the driving signal, e.g.,
[12,41,42]. This procedure is often referred to as en- and decoding.
For ease of illustration we skip the en- and decoding procedure and consider the direct derivation
of driving functions for the synthesis of virtual sources following the model-based synthesis paradigm.
We review the concept of NFC-HOA based on the analytic formulation presented in [35,43].
4.32.6.1 Outline
Near-ﬁeld Compensated Higher Order Ambisonics (NFC-HOA), which is also known as Ambison-
ics with distance coding, and related techniques [12,32,35,44,45] base on the explicit solution of the
monopole only synthesis Eq. (32.134) by means of decomposing the quantities involved in (32.134)—
P(x, ω), D(x0, ω), and G(x|x0, ω)—into orthogonal basis functions (Section 4.32.5.2). For the con-
sidered circular and spherical secondary source distributions, these orthogonal basis functions are given
by the surface spherical harmonics and circular harmonics respectively (refer to Sections 4.32.3.5 and
4.32.3.4). Exploitation of the orthogonality of the basis functions leads to a comparison of coefﬁcients
that allows for determining the driving function.

4.32.6 Near-Field Compensated Higher Order Ambisonics (NFC-HOA)
957
The synthesis equation (32.134) can be interpreted as a generalized spatial convolution along the
secondary source contour. By identifying the appropriate convolution theorem, the spatial convolution in
(32.134) is turned into a scalar multiplication of given expansion coefﬁcients. It is then straightforward
to calculate the driving signal, if the expansion coefﬁcients of the desired sound ﬁeld and the spatio-
temporal transfer function of the secondary sources are known. As mentioned in Section 4.32.5.2, the
above described procedure theoretically provides also the solution for geometries other than spherical
or circular. However, the applicability is restricted due to the complexity involved.
Common variants of Ambisonics and HOA can be derived from the general theory discussed above
by varying the model used for the virtual source or the secondary sources. In traditional Ambisonics it
is typically assumed that the secondary sources and the virtual source can be modeled as plane waves
in the center of the secondary source arrangement [46]. This results in driving functions that are simple
amplitude panning laws (refer to Section 4.32.1.2).
4.32.6.2 Spherical secondary source distributions
The synthesis equation (32.134) for an acoustically transparent spherical secondary source distribution
with radius R centered around the coordinate origin is given by [30,35,47]
P(x, ω) =

S2
R
D

x0, ω

G

x, g(x0)η2, ω

R2dx0.
(32.139)
η2 = [0 0 R]T denotes the north pole of the spherical surface S2
R and x0 = R [cos α0 sin β0
sin α0 sin β0 cos β0]T a location on S2
R. g(x0) is a rotation matrix the explicit expression of which
is waived here for convenience. Refer to the corresponding rotation in the treatment of circular sec-
ondary source distributions in Section 4.32.6.3 for an explicit example of such a rotation matrix.
G

x, η2, ω

denotes the spatio-temporal transfer function of the secondary source located at η2 =
[0 0 R]T . The factor R2 arises in (32.139) due to the fact that S2
R is of radius R and not 1. Refer to
Figure 32.16 for an illustration of the setup.
Note that (32.139) implies that the spatio-temporal transfer function of the secondary sources is
invariant with respect to rotation around the coordinate origin. In simple words, all secondary sources
need to exhibit similar radiation properties and need to be oriented appropriately. For the considered
free-ﬁeld conditions, this circumstance does not constitute an essential restriction.
Following the procedure outlined in Section 4.32.5.2.3 requires that P(x, ω), D

x, ω

, and G

x, ω

are expanded into appropriate orthogonal basis functions in order to allow for a comparison of the coef-
ﬁcients of the according decomposition. For the geometry under consideration these orthogonal basis
functions are given by the surface spherical harmonics discussed in Section 4.32.3.5. This procedure can
indeed be straightforwardly applied yielding the desired result. However, we will apply the equivalent
procedure from [35], which will later be shown to be applicable also for non-enclosing distributions of
secondary sources.
Equation (32.139) can be interpreted as a convolution along the surface of a sphere in which case
the convolution theorem [48,49]
˚Pm
n (r, ω) = R2

4π
2n + 1
˚Dm
n (ω) · ˚G0
n(r, ω)
(32.140)

958
CHAPTER 32 Sound Field Synthesis
x
y
z
R
R
FIGURE 32.16
Spherical secondary source distribution of radius R centered around the coordinate origin.
applies, which relates the spherical harmonics expansion coefﬁcients (refer to (32.101)) of the involved
quantities via a scalar multiplication. Note that ˚G0
n(r, ω) represents the expansion coefﬁcients of
G

x, η2, ω

, i.e., of the spatio-temporal transfer function of the secondary source located at the north
pole of the sphere.
The asymmetry of the convolution theorem (32.140), ˚Pm
n (r, ω) vs. ˚G0
n(r, ω) is a consequence of
the deﬁnition of (32.139) as left convolution. An according convolution theorem for right convolutions
exists [49].
Rearranging (32.140) yields
˚Dm
n (ω) = 1
R2

2n + 1
4π
˚Pm
n (r, ω)
˚G0n(r, ω)
.
(32.141)
When introducing the explicit expressions for the coefﬁcients ˚Pm
n (r, ω) and ˚G0
n(r, ω) given by (32.105)
into (32.141),
˚Dm
n (ω) = 1
R2

2n + 1
4π
˘Pm
n (ω) · jn
 ω
c r

˘G0n(ω) · jn
 ω
c r
 ,
(32.142)
it can be seen that the parameter r appears both in the numerator as well as in the denominator in
(32.142) in the spherical Bessel function jn
 ω
c r

. It can be shown that the spherical Bessel functions
cancel out except for speciﬁc situations [35]. It can indeed happen that (32.142) can be undeﬁned for
jn
 ω
c r

= 0 and ω
c r ̸= 0. These cases represent resonances of the spherical cavity that can not be
controlled by the secondary source distribution. This lack of controllability has not been reported to be
a restriction is practice.

4.32.6 Near-Field Compensated Higher Order Ambisonics (NFC-HOA)
959
It may be assumed that all Bessel functions in (32.141) cancel out yielding
˚Dm
n (ω) = 1
R2

2n + 1
4π
˘Pm
n (ω)
˘G0n(ω)
.
(32.143)
In order that (32.143) holds, ˘G0
n(ω) may not exhibit zeros. This requirement is fulﬁlled for the three-
dimensional free-ﬁeld Green’s function [35].
The secondary source driving function D(α, β, ω) can be composed from its coefﬁcients ˚Dm
n (ω)
using (32.104) to be [30,35,47]
D(α, β, ω) =
∞

n=0
n

m=−n
1
R2

2n + 1
4π
˘Pm
n (ω)
˘G0n(ω)



= ˚Dmn (ω)
Y m
n (β, α).
(32.144)
In practical applications, the summation in (32.144) can not be performed over an inﬁnite number of
addends but has to be truncated. The choice of the summation limits primarily has impact on the artifacts
arising in practice. This circumstance will be discussed more in detail in Section 4.32.6.4 conjunction
with circular NFC-HOA.
4.32.6.3 Circular secondary source distributions
The specialization of (32.134) to a circular distribution of secondary sources in the horizontal plane and
centered around the coordinate origin is given by
P(x, ω) =
 2π
0
D

x0, ω

G

x, g(α0)η1, ω

R dα0,
(32.145)
where R denotes the radius of, and x0 = R[cos α0 sin α00]T a location on, the circular secondary
source distribution S1
R. η1 = [R 0 0]T denotes that point on the distribution where α0 = 0. g(α0) is a
rotation matrix given by
g(α0) =
⎡
⎣
cos α0
−sin α0
0
sin α0
cos α0
0
0
0
1
⎤
⎦.
G(x, η1, ω) in (32.145) denotes the spatio-temporal transfer function of the secondary source located
at η1. Refer to Figure 32.17 for a sketch of the setup.
Equation (32.145) can be interpreted as a convolution along a circle and thus the convolution theorem
[22]
˚Pm(r, ω) = 2π R ˚Dm(ω) ˚Gm(r, ω)
(32.146)
holds, which relates the Fourier series expansion coefﬁcients (i.e., the circular harmonics coefﬁcients,
refer to (32.70)) of the involved quantities (Section 4.32.3.4). Note that the relation between the Fourier
series expansion coefﬁcients and the spherical harmonics expansion coefﬁcients is given by (32.107).
˚Gm(r, ω) represents the expansion coefﬁcients of G

x, η1, ω

.

960
CHAPTER 32 Sound Field Synthesis
x
y
z
R
FIGURE 32.17
Circular secondary source distribution of radius R in the horizontal plane and centered around the coordinate
origin.
Equation (32.146) can be solved for the coefﬁcients ˚Dm(ω) of the driving function D(α0, ω). The
latter can then be composed from its coefﬁcients via (32.73). Unlike with spherical secondary source
distributions, the driving function D(α0, ω) is generally dependent on the radius r. As a consequence,
the synthesized sound ﬁeld will only be correct on a circle around the center of the coordinate origin.
Deviations arise at all other locations. This is a typical case of 2.5-dimensional synthesis as described
in Section 4.32.5.4.
Referencing the driving function to the origin of the coordinate system has shown to be most conve-
nient. After some mathematical manipulation, the driving function D(α, ω) can ﬁnally be shown to be
[35]
D(α, ω) =
∞

m=−∞
1
2π R
˘Pm
|m|(ω)
˘Gm
|m|(ω)
eimα.
(32.147)
Note that ˘Pm
|m|(ω) and ˘Gm
|m|(ω) denote the spherical harmonics expansion coefﬁcients ˘Pm
n (ω) and ˘Gm
n (ω)
of P( · ) and G( · ) for n = |m|.
The coefﬁcients ˘Pm
|m|(ω) for a plane wave with incidence angle (θpw, ϕpw) can be deduced from
(32.99) to be
˘Pm
|m|(ω) = Y m∗
|m|(θpw, ϕpw).
(32.148)
For omnidirectional secondary sources
˘Gm
|m|(ω) = 1
4π i−|m|−1 ω
c h(2)
|m|

ω
c R

Y m
|m|

π
2 , 0

,
(32.149)
where h(2)
|m| denotes the |m|th-order spherical Hankel function of second kind [22].
Assuming that a desired plane wave P(x, ω) propagates in the horizontal plane (i.e., θpw = π/2)
and carries the signal ˆS(ω), the driving function can be determined to be [50]
Dpw(α0, ω) = ˆS(ω)
∞

m=−∞
2i|m|+1
R ω
c h(2)
|m|
 ω
c R
eim(α0−ϕpw),
(32.150)

4.32.6 Near-Field Compensated Higher Order Ambisonics (NFC-HOA)
961
by exploiting the fact that the associated Legendre functions Pm
n (·), which are contained in the spherical
harmonics Y m
n (·) (see (32.87)), never vanish when their order m equals their degree n [24, Eq. (2.1.50)]
so that they cancel out.
4.32.6.4 Spatial sampling and application example
Continuous secondary source distributions as discussed so far can not be implemented in practice but
discrete setups of a ﬁnite number of loudspeakers have to be used. In the following, we brieﬂy discuss
the effects of this spatial sampling of the secondary source distribution. Detailed discussions can be
found in [2,51,52].
It can be shown that an equiangular sampling leads to repetitions of the Fourier coefﬁcients of the
driving function as [52]
˚Dm,S(ω) =
∞

μ=−∞
˚Dm+μL(ω),
(32.151)
where L denotes the number of secondary sources employed and ˚Dm,S(ω) denotes the Fourier series
coefﬁcients of the sampled driving function.
At ﬁrst stage, the continuous driving function is not bandlimited with respect to the expansion order
m, i.e., according to (32.147) the summation over the coefﬁcients ˚Dm(ω) has to be performed from
m = −∞to m = ∞. As a consequence, the repetitions that are apparent in (32.151) overlap and
interfere.
In order to avoid such overlap the continuous driving function is typically spatially bandlimited, i.e.,
the summation in (32.147) is calculated for m = −L/2+1 to m = L/2−1 for even L and accordingly
for odd L. Obviously, such a spatial band limitation causes a loss of information. This is illustrated
in Figure 32.18, which shows a virtual plane wave impinging from ϕpw = −π/2 synthesized by a
circular distribution of L = 56 monopole secondary sources. The geometry is chosen equal to a system
built by the authors. At low frequencies, neither the bandwidth limitation nor the spectral repetitions
impair the synthesized sound ﬁeld as depicted in Figure 32.18a. The undesired amplitude decay, which
is characteristic for 2.5-dimensional synthesis, is apparent (Section 4.32.5.4).
At higher frequencies though, the imposed bandwidth limitation causes a concentration of the energy
of the desired component of the sound ﬁeld around the center of the secondary source distribution. This
is evident in Figure 32.18b. Outside this artifact-free zone, artifacts arise, which are a consequence of
the spectral repetitions. The size of the artifact-free zone decreases linearly with increasing frequency.
Of course, it is also possible to choose a spatial bandlimit such that is signiﬁcantly higher than the
one chosen in the Ambisonics context. This does indeed essentially change the properties of the arising
artifacts. It has been shown in [52] that a large spatial bandwidth leads to artifacts that are similar to
those arising in Wave Field Synthesis as discussed in Section 4.32.8.
As apparent from Figure 32.18b, considerable artifacts arise even at moderate frequencies. The
perceptual consequences of these artifacts have not been investigated in detail so far. However, typical
loudspeaker setups used for NFC-HOA have shown to provide improved properties for a central listening
position when compared to traditional stereophonic techniques [53].

962
CHAPTER 32 Sound Field Synthesis
x −> [m]
y −> [m]
−2
−1
0
1
2
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
fpw = 1 kHz
x −> [m]
y −> [m]
−2
−1
0
1
2
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
fpw = 2 kHz
(a)
(b)
FIGURE 32.18
Sound ﬁeld P(x, ω) synthesized by a circular distribution of L = 56 secondary monopole sources and of
radius R = 1.50 m when driven with 2.5-dimensional NFC-HOA. The virtual sound ﬁeld is a monochromatic
plane wave of frequency fpw and with incidence angle ϕpw = −π/2. Click Video clips 1 and 2 to see the
animation.
4.32.6.5 Extensions to basic principle
The above presented approach has been extended in various ways to enable the synthesis of complex
virtual sound ﬁelds such as that of focused sources [54–56] or of sound sources with complex radiation
properties [57–60] or to enable the employment of non-spherical enclosing secondary source distribu-
tions [33] and non-omnidirectional secondary sources [61,62] and active listening room compensation
[63,64].
4.32.7 Spectral division method (SDM)
The Spectral Division Method employs the explicit solution to the synthesis equation (32.134) for planar
and linear distributions of secondary sources. The following section provides a brief overview of the
theory as presented in [34,65,66].
4.32.7.1 Planar secondary source distributions
Consider the synthesis equation (32.134) and assume a secondary source distribution ∂V that consists
of a disk V0 and a hemisphere Vhemi of radius rhemi as depicted in Figure 32.19 [22]. As rhemi →∞, the
disk V0 turns into an inﬁnite plane and the volume under consideration turns into a half-space. The latter

4.32.7 Spectral Division Method (SDM)
963
Vi
V0
Vhemi
rhemi
FIGURE 32.19
Cross-section through a boundary consisting of a hemisphere and a disc.
is referred to as target half-space. Additionally, the Sommerfeld radiation condition2 is invoked, i.e.,
it is assumed that there are no contributions to the desired sound ﬁeld to be synthesized that originate
from inﬁnity so that only the planar part of the boundary needs to be considered. We additionally allow
for the synthesis of plane waves that propagate into the target half-space. The Rayleigh integral can
be used to prove that such plane waves can indeed be synthesized by the considered secondary source
distribution [66].
As a consequence, arbitrary sound ﬁelds that are source-free in the target half-space and that satisfy
the Sommerfeld radiation condition (as well as plane waves) may now be described by an integration
over the inﬁnite plane V0. For convenience, it is assumed in the following that the boundary of the target
half-space (i.e., the secondary source distribution) is located in the x-z-plane, and the target half-space
is assumed to include the positive y-axis as depicted in Figure 32.20.
The formulation of the synthesis equation (32.134) for an inﬁnite uniform planar secondary source
distribution is then given by [34,65,66]
P(x, ω) =
 ∞
−∞
D(x0, ω) · G(x −x0, ω)dx0dz0.
(32.152)
with x0 = [x0 0 z0]T . G(x, ω) denotes the spatial transfer function of a secondary source located in the
origin of the coordinate system. The term G(x −x0, ω) in (32.152) implies that the spatio-temporal
transfer function of the secondary sources is invariant with respect to translation along the secondary
source contour [22]. In other words, all secondary sources exhibit equal radiation properties and are
orientated accordingly. Note the resemblance of (32.152) to the ﬁrst Rayleigh integral [22].
2The Sommerfeld radiation condition can be interpreted as a boundary condition at inﬁnity. It assures that no energy originates
from inﬁnity.

964
CHAPTER 32 Sound Field Synthesis
x
y
z
←y = 0
FIGURE 32.20
Illustration of the setup of a planar secondary source situated along the x-z-plane. The secondary source
distribution is indicated by the gray shading and has inﬁnite extent. The target half-space is the half-space
bounded by the secondary source distribution and containing the positive y-axis.
Equation (32.152) essentially constitutes a two-dimensional convolution along the spatial dimensions
x and z respectively. This fact is revealed when (32.152) is rewritten as [34,66]
P(x, ω) =
 ∞
−∞
D

[x0 0 z0]T, ω

G

[x y z]T −[x0 0 z0]T, ω

dx0dz0
=
 ∞
−∞
D(x0, 0, z0, ω)G(x −x0, y, z −z0, ω)dx0dz0
= D

x |y=0 , ω

∗x ∗zG(x, ω),
(32.153)
where the asterisk ∗i denotes convolution with respect to the indexed spatial dimension [67]. Thus, the
convolution theorem [67]
˜P

kx, y, kz, ω

= ˜D

kx, kz, ω

· ˜G

kx, y, kz, ω

(32.154)
holds, which relates the involved quantities in wavenumber domain with respect to kx and kz. Refer to
Section 4.32.3.3 for a discussion of the Fourier transform with respect to space.
The secondary source driving function ˜D

kx, kz, ω

in the wavenumber domain is given by
˜D

kx, kz, ω

=
˜P

kx, y, kz, ω

˜G

kx, y, kz, ω
.
(32.155)

4.32.7 Spectral Division Method (SDM)
965
In order that (32.155) holds, ˜G(kx, y, kz, ω) may not exhibit zeros. This is indeed fulﬁlled e.g., for
monopole secondary sources [34].
Applying an inverse spatial Fourier transform with respect to kx and kz on (32.155) yields [34,65,66]
D(x, z, ω) =
1
4π2
 ∞
−∞
˜P

kx, y, kz, ω

˜G

kx, y, kz, ω
ei

kx x+kzz

dkxdkz.
(32.156)
From (32.155) it is obvious that the driving function is essentially yielded by a division in the spatial
frequency domain. The presented approach is therefore referred to as Spectral Division Method (SDM).
Equation (32.156) suggests that D(x, z, ω) is dependent on the distance y of the receiver to the
secondary source distribution since y is apparent on the right hand side of (32.156). It is shown in
[2,34,66] that y does indeed cancel out provided that the target half-space is free of virtual sound
sources. D(x, z, ω) is thus independent of the location of the receiver.
4.32.7.2 Linear secondary source distributions
Despite the simple driving function for planar secondary source distributions, this setup will be rarely
implemented due to the enormous amount of loudspeakers necessary. Typically, audio presentation
systems employ linear arrays or a combination thereof [68]. Assuming a linear secondary source distri-
bution of inﬁnite length, the situation may be interpreted as a reduced formulation of the setup treated in
Section 4.32.7.1. For convenience, the secondary source distribution is assumed to be along the x-axis
(thus x0 = [x0 0 0]T , refer to Figure 32.21).
x
y
z
y = yref
FIGURE 32.21
Illustration of the setup of a linear secondary source distribution situated along the x-axis. The secondary
source distribution is indicated by the gray shading and has inﬁnite extent. The target half-plane is the
half-plane bounded by the secondary source distribution and containing the positive y-axis. Thin dotted line
indicates the reference line (see text).

966
CHAPTER 32 Sound Field Synthesis
The specialization of the synthesis equation (32.134) to such a linear distribution of secondary sources
along the x-axis is given by
P(x, ω) =
 ∞
−∞
D(x0, ω)G(x −x0, ω)dx0,
(32.157)
where x0 = [x0 0 0]T and G(x, ω) denotes the spatio-temporal transfer function of that secondary
source located in the coordinate origin. Equation (32.157) implies again that the spatio-temporal transfer
function of the secondary sources is invariant with respect to translation along the secondary source
contour.
For the sake of simplicity it is assumed that the listeners’ ears are located in that half of the x-y-plane
that contains the positive part of the y-axis (z = 0, y > 0). Refer to [34] for a generalization. Equation
(32.157) essentially constitutes a spatial convolution of the driving function D(x, ω) with the spatial
transfer function G(x, ω) of the secondary source at the coordinate origin, whereby the convolution
takes place along the x-axis. Again, the convolution theorem (32.154) of the Fourier transformation can
be applied, though in the present case exclusively with respect to kx. Explicitly,
˜P(kx, y, ω) = ˜D(kx, ω) ˜G(kx, y, ω),
(32.158)
where kx denotes the wavenumber in x-direction. Equation (32.158) cannow besolvedstraightforwardly
with respect to the driving function ˜D(kx, ω) in wavenumber domain. D(x, ω) is then obtained by
applying an inverse spatial Fourier transformation (32.44) given by [34,65,66]
D(x, ω) = 1
2π
 ∞
−∞
˜P(kx, y, z, ω)
˜G(kx, y, z, ω)
eikx xdkx.
(32.159)
In order for (32.159) to hold, ˜G(kx, y, z, ω) may not exhibit zeros. This is indeed fulﬁlled e.g., for
monopole secondary sources as can be deduced from (32.162).
As discussed in detail in [34], the driving function (32.159) has to be referenced to a line parallel to
the x-axis, which is then the only location where the synthesized sound ﬁeld is exact. As with circular
distributions of secondary sources (Section 4.32.6.3), the present situation constitutes 2.5-dimensional
synthesis as discussed in Section 4.32.5.4.
Since we are aiming at the synthesis in the horizontal plane, we reference the driving function
(32.159) to z = 0 and y = yref as indicated in Figure 32.21. The referenced driving function D(x, ω)
is then given by
D(x, ω) = 1
2π
 ∞
−∞
˜P(kx, yref, 0, ω)
˜G(kx, yref, 0, ω)
eikx xdkx.
(32.160)
When choosing a plane wave with propagation direction

θpw = π
2 , ϕpw

carrying the signal ˆS(ω) as
desired sound ﬁeld, then ˜P(kx, yref, 0, ω) can be obtained from (32.46) as
˜P(kx, yref, 0, ω) = ˆS(ω)2πδ(kx −kpw,x)eikpw,y yref.
(32.161)
˜G(kx, yref, 0, ω) for omnidirectional secondary sources is given by [34]
˜G(kx, y, z, ω) = −i
4 H(2)
0

ω
c
2
−kx 2

y2 + z2

∀0 ≤|kx| <
ω
c
 ,
(32.162)
where H(2)
0 ( · ) denotes the zeroth-order Hankel function of second kind [69].

4.32.7 Spectral Division Method (SDM)
967
Inserting (32.161) and (32.162) into (32.160) yields [34]
Dpw(x0, ω) = ˆS(ω)
4iei ω
c yref sin ϕpw
H(2)
0 ( ω
c yref sin ϕpw)
ei ω
c x0 cos ϕpw.
(32.163)
4.32.7.3 Spatial sampling and application example
As for NFC-HOA, we brieﬂy discuss the effects of spatial sampling of the secondary source distribution.
Anequidistantsamplingwithdistancex betweenthesecondarysourcescanbemodeledbymultiplying
the driving function (32.163) with a series of spatial Dirac pulses. This results in spectral repetitions in
the spatio-temporal frequency domain [70]
˜DS(kx, ω) = 2π
∞

μ=−∞
˜D

kx −2π
x μ, ω

.
(32.164)
Typical driving functions (e.g., for plane or spherical waves) are not bandlimited with respect to the
spatial frequency kx. The spectral repetitions indicated in (32.164) can therefore overlap and leak into
the baseband. The latter is constituted by the continuous driving function, i.e., (32.164) evaluated for
μ = 0.
Reformulating (32.158) considering the sampled driving function ˜DS(kx, ω) reads
˜PS(kx, y, ω) = ˜DS(kx, ω) ˜G(kx, y, ω).
(32.165)
It is evident from (32.165), the synthesized sound ﬁeld ˜PS(kx, y, ω) is given by the driving function
˜DS(kx, ω) weighted by the secondary sources’ transfer function ˜G(kx, y, ω). Using a simpliﬁed model,
it may be assumed that ˜G(kx, y, ω) is spatially lowpass [51]. This means that it does not alter ˜DS(kx, ω)
when |kx| is smaller but heavily attenuates ˜DS(kx, ω) for large |kx|.
At low angular frequencies ω (and thus at low time frequencies f ), the spectral repetitions indicated
in (32.164) to not corrupt the baseband and they are attenuated by the spatial lowpass property of
˜G(kx, y, ω) [51]. As a consequence, the synthesized sound ﬁeld is not impaired at low time frequencies
f . Refer to Figure 32.22a for an example.
At higher frequencies f , the spectral repetitions due to spatial sampling do corrupt the baseband
and therefore also the synthesized sound ﬁeld. The latter is then composed of the desired sound ﬁeld
superposed by additional undesired wave fronts [51,70]. Refer to Figure 32.22b for an example. For
practical setups, the frequency above which considerable artifacts arise is relatively low. When the
synthesis of a plane wave is considered, the highest frequency f that can be synthesized without or with
only moderate corruption can be calculated via [37]
f =
c
x(1 + | cos ϕpw|).
(32.166)
For the setup depicted in Figure 32.22, this frequency is approximately 1000 Hz.
Recall that the audible bandwidth typically exceeds 16 kHz. A signiﬁcant part of the spectrum is thus
corrupted. Though, the human auditory system does not seem to be very sensitive to spatial aliasing
artifacts for stationary scenarios [71]. The scenario illustrated in Figure 32.22 results in only minor
perceptual impairment even when the entire audible frequency range is employed.

968
CHAPTER 32 Sound Field Synthesis
−2
−1
0
1
2
−1
−0.5
0
0.5
1
1.5
2
2.5
3
x (m)
y (m)
(a) fpw = 800 Hz
−2
−1
0
1
2
−1
−0.5
0
0.5
1
1.5
2
2.5
3
x (m)
y (m)
(b) fpw = 1500 Hz
FIGURE 32.22
Snapshot of the sound ﬁeld P(x, ω) synthesized by a linear distribution of secondary monopole sources
with a spacing of x = 0.2 m driven with SDM. The virtual sound ﬁeld is a monochromatic plane wave of
frequency fpw impinging from direction ϕpw = −(3/4)π referenced to yref = 1 m. Click Video clips 3 and 4
to see the animation.
4.32.7.4 Approximate solution for non-planar and non-linear secondary source
distributions
As pointed out in [30], it is helpful to interpret sound ﬁeld synthesis by considering the equivalent
problem of scattering of sound waves at a sound-soft object whose geometry is identical to that of
the secondary source distribution. Sound-soft objects exhibit ideal pressure release boundaries, i.e., a
homogeneous Dirichlet boundary condition is assumed.
When the wavelength λ of the wave ﬁeld under consideration is much smaller than the dimensions
of the scattering object and when the object is convex the so-called Kirchhoff approximation or physical
optics approximation can be applied [9]. The surface of the scattering object is divided into a region
that is illuminated by the incident wave, and a shadowed area. The problem under consideration is then
reduced to far-ﬁeld scattering off the illuminated region whereby the surface of the scattering object
is assumed to be locally plane. The shadowed area has to be discarded in order to avoid an unwanted
secondary diffraction [9]. The convexity is required in order to avoid re-entry of the scattered sound
ﬁeld.
For such small wave lengths, any arbitrary simply connected convex enclosing secondary source
distribution may also be assumed to be locally plane. Consequently, when the driving function (32.156)
for planar secondary source distributions is applied in such a scenario, a high-frequency approximation
of the driving function is obtained when only those secondary sources are driven that are located in

4.32.7 Spectral Division Method (SDM)
969
kpw
kpw
A
B
FIGURE 32.23
Secondary source selection for a virtual plane wave with propagation direction kpw. Thick solid lines indicate
the area that is illuminated by the virtual sound ﬁeld. The illuminated area corresponds to the active secondary
sources. The dashed line indicates the shadowed part of the secondary source distribution. The two dotted
lines are parallel to kpw and pass the secondary source distribution in a tangent-like manner. In case A
tapering may be applied, in case B not.
that region that is illuminated by the virtual sound ﬁeld. A similar argumentation may be deployed in
conjunction with linear secondary source distributions.
The better the assumptions of the physical optics approximation are fulﬁlled, most notably the wave
length under consideration being signiﬁcantly smaller than the dimensions of the secondary source
distribution, the smaller is the resulting inaccuracy. A further analysis of this inaccuracy may be found
in [51,72].
The illuminated area can be straightforwardly determined via geometrical considerations as indicated
in Figure 32.23. For a virtual plane wave, the illuminated area is bounded by two lines parallel to the
propagation vector kpw of the plane wave passing the secondary source distribution in a tangent-like
manner.
If the proper tangent on the boundary of the illuminated area is not parallel to kpw or is not deﬁned
(like the boundary of a planar distribution of ﬁnite size), a degenerated problem is considered (case A
in Figure 32.23). That means, the illuminated area is incomplete and artifacts have to be expected. The
perceptual prominence of such spatial truncation artifacts can be reduced by the application of tapering,
i.e., an attenuation of the secondary sources towards the edges of the illuminated area. Tapering is a
well-established technique in the context of WFS [73].
It has been shown that the illuminated area does not need to be smooth. Corners are also possible
with only little additional error introduced [37]. This is also evident from Figure 32.24a, which shows
the synthesis of a virtual plane wave by a rectangular distribution of monopoles driven with SDM. The
secondary source driving function was deduced from (32.163) via an appropriate translation and rotation
of the coordinate system. Since this scenario constitutes 2.5-dimensional synthesis, the synthesized
sound ﬁeld exhibits an undesired amplitude decay.

970
CHAPTER 32 Sound Field Synthesis
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
x (m)
y (m)
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
x (m)
y (m)
(a)
(b)
FIGURE 32.24
A cross-section through the horizontal plane of the sound pressure Ppw(x, ω) synthesized by different sec-
ondary monopole distribution driven with SDM synthesizing a virtual plane wave of fpw = 1000 Hz and
unit amplitude with incidence angle ϕpw = 7
6π referenced to yref = 1.0 m. Solid lines indicate the illumi-
nated area; dotted lines indicate the shadowed area. (a) Rectangular secondary distribution and (b) circular
secondary source distribution. Click Video clips 5 and 6 to see the animation.

4.32.8 Wave Field Synthesis (WFS)
971
Figure 32.24b depicts the synthesis of a virtual plane wave by a circular distribution of monopoles
driven with SDM. This circular distribution may be interpreted as a combination of linear sections of
inﬁnitesimal length. Due to the different geometry of the secondary source contour, the amplitude decay
of the synthesized sound ﬁeld is slightly different to the one in Figure 32.24a.
4.32.8 Wave Field Synthesis (WFS)
Wave Field Synthesis constitutes an approximation of the approach to monopole only synthesis that
is based on the application of a Neumann Green’s function. The physical foundation of this approach
is given in Section 4.32.5.2. This section outlines its application in the context of WFS and presents
application examples.
4.32.8.1 Outline
The concept of WFS has initially been developed for linear distributions of secondary sources [74]. It
bases on a sensible approximation of (32.136). Similar arguments as given in Section 4.32.7 for the
SDM, can be used to derive a representation of (32.136) for planar geometries. The required Neumann
Green’s function GN(x|x0, ω) for this specialized geometry is given by two times the free-ﬁeld Green’s
function GN(x|x0, ω) = 2G0(x −x0, ω). The resulting integral is known as Rayleigh’s ﬁrst integral
equation [22]. The initial concept of WFS is implicitly based on a specialization of Rayleigh’s ﬁrst
integral formula to 2.5-dimensional synthesis. This specialization is derived by degenerating the planar
secondary source distribution to a linear distribution using a stationary phase approximation [37,73,74].
WFS has later on been generalized to arbitrary convex secondary source distributions, which may
even only partly enclose the receiver area [36,73]. This generalization can be deduced also from (32.136)
as is illustrated in the following. The Neumann Green’s function in (32.136) has to fulﬁll homogeneous
Neumann boundary conditions imposed on ∂V . Consequently, reﬂections are included that are caused
by the rigid boundary. Since (32.136) provides a unique solution (discarding the eigenfrequencies of
the rigid cavity), these reﬂections will be compensated for by the driving function. As mentioned above,
secondary sources with the characteristics of a Neumann Green’s function are generally not available
for complex geometries. Hence, it is desirable to use secondary sources with the characteristics of a
free-ﬁeld Green’s function. WFS is based on an approximation of (32.136) by
1. replacing the Neumann Green’s function by the free-ﬁeld Green’s function,
2. limiting the integration path, and
3. prescribing a convex secondary source distribution.
In many practical situations the free-ﬁeld Green’s function can be seen as a far-ﬁeld/high-frequency
approximation of the Neumann Green’s function when considering only the interior V of the secondary
source contour [72]. Hence, the Neumann Green’s function used in (32.136) can be realized by sec-
ondary sources exhibiting monopole characteristics. However, as discussed above, the driving function
inherently copes with the reﬂections caused by the Neumann Green’s function. These reﬂections are
not present when using monopole sources and consequently do not need to be compensated. This can
be accounted for by not those driving secondary sources that compensate for these reﬂections, by taking

972
CHAPTER 32 Sound Field Synthesis
care that only those secondary sources are active that are located in the area that is illuminated by the
virtual sound ﬁeld. This can be formulated by introducing the window function a(x0) into (32.136)
P(x, ω) = −

∂V
2a(x0) ∂
∂n S(x0, ω)



D(x0,ω)
G0(x −x0, ω)d A(x0).
(32.167)
An alternative motivation for a(x0) is outlined in Section 4.32.7.4.
The geometry of the boundary ∂V has to be restricted to convex contours in order to avoid that
contributions reenter the listening area V . Equation (32.167) constitutes an approximation of (32.136),
which has been shown to be of special interest for sound reproduction. Equation (32.167) states that the
driving function for WFS is given as
D(x0, ω) = 2a(x0) ∂
∂n S(x0, ω).
(32.168)
The window function for selection of the active secondary sources for a plane wave as virtual source is
given as [75]
apw(x0) =
 1, if ⟨npw, n(x0)⟩> 0,
0, otherwise.
(32.169)
Equation (32.168) is valid for arbitrary convex secondary source contours ∂V . It depends only on
local quantities. This is contrary to NFC-HOA and SDM where the driving functions are restricted
to a particular geometry and their dependence is non-local. The next section illustrates the practical
application of WFS to three- and 2.5-dimensional synthesis.
4.32.8.2 Three-dimensional synthesis
The Green’s function used in the synthesis equation (32.167) determines the characteristics of the
secondary sources. The speciﬁc form of the free-ﬁeld Green’s function depends on the dimensionality
of the problem. The three-dimensional free-ﬁeld Green’s function is given by (32.132) which can be
interpreted as the ﬁeld of a point source with monopole characteristics located at the position x0.
Three-dimensional WFS can be realized by surrounding the listening volume V by a continuous
distribution of point sources placed on the boundary ∂V . These secondary sources are driven by the
secondary source driving function (32.168). The driving function is given by the directional gradient of
the virtual source’s sound ﬁeld and the window function a(x0). Hence, the explicit form of the driving
function depends on the virtual source and the geometry of the secondary source distribution. The
driving function for a plane wave carrying the signal ˆSpw(ω) is determined by the direction gradient of
the sound ﬁeld of a plane wave (32.29) and the window function (32.169) as
Dpw,3D(x0, ω) = 2apw(x0)
nT
pwn(x0)
c
iω ˆSpw(ω)ei ω
c nTpwx0.
(32.170)
A time-domain version of the driving function (32.170) is useful to derive an efﬁcient implementation
of WFS. Inverse Fourier transformation of (32.170) yields
dpw,3D(x0, t) = 2apw(x0)
nT
pwn(x0)
c
d
dt ˆspw

t −
nT
pwx0
c

,
(32.171)

4.32.8 Wave Field Synthesis (WFS)
973
where the differentiation theorem of the Fourier transformation was used. Equation (32.171) states that
the driving signal for a plane wave can be computed efﬁciently in the time-domain by weighting the
derivative of the time-shifted source signal ˆspw(t). However, the differentiation of the virtual source
signal may also be performed by ﬁltering the signal by a ﬁlter with iω-characteristic.
A planar secondary source distribution is the basic building block of a cuboid shaped reproduction
system. A planar distribution will be discussed in detail in the sequel. The closed contour integral
(32.136) over the surface ∂V can be degenerated to an integral over an inﬁnite plane. In brief, this
degeneration is achieved by splitting the closed contour ∂V into a planar boundary and a half-sphere.
The integration over the half-sphere can be omitted by applying the Sommerfeld radiation condition
[22]. This procedure was outlined in Section 4.32.7.1 in conjunction with SDM.
It will be assumed in the following, without loss of generality, that the secondary source distribution
is located on the xz-plane at y = 0 as depicted in Figure 32.20. Other cases can be regarded as simple
translation or rotation of this special case.
The synthesized sound ﬁeld for a planar distribution of secondary point sources on the xz-plane is
given as
P(x, ω) = −
 ∞
−∞
D3D(x0, ω)G0,3D(x|x0, ω)dx0dz0,
(32.172)
with x0 = [x0 0 z0]T . Equation (32.172) is known as the ﬁrst Rayleigh integral. The synthesized sound
ﬁeld P(x, ω) will be mirrored at the secondary source distribution as a consequence of the Neumann
boundary condition (32.135). Hence, the reproduced wave ﬁeld is only correct in one of the two half-
volumes separated by the secondary source distribution. The direction of the normal vector n speciﬁes
the considered half-volume.
The synthesized sound ﬁeld for a planar continuous distribution of inﬁnite size will exactly match
the wave ﬁeld of the virtual source within the listening area. This can be proven by inserting the driving
functions into the synthesis equation (32.172). Artifacts will occur for other geometries of the secondary
source distribution. This is due to the fact that the derived Neumann Green’s function only fulﬁlls the
required Neumann boundary condition exactly in this special case.
4.32.8.3 2.5-Dimensional synthesis
Typical realizations of WFS use secondary source distributions that are located on the boundary ∂V of
a planar listening area V . As already discussed in Section 4.32.5.4, this constitutes a 2.5-dimensional
scenario when point sources are used a secondary sources. Since WFS is not based on an explicit solution
of the underlying mathematical formulation this fact has to be taken into account explicitly.
Traditionally a stationary phase approximation has been applied to (32.167) in order to derive the
driving function for 2.5-dimensional reproduction [37]. This procedure results in a spectral correction
and a listener dependent amplitude correction. A more instructive approach is followed here. A large-
argument approximation of the Hankel function is used to derive the following far-ﬁeld approximation
 ω
c |x −x0| ≫1

of the two-dimensional Green’s function
G2D(x −x0, ω) ≈

2π |x −x0|
i ω
c
1
4π
e−i ω
c |x−x0|
|x −x0| .
(32.173)

974
CHAPTER 32 Sound Field Synthesis
It can be concluded from (32.173) that a line source can be approximated in the far-ﬁeld by a point
source the spectrum and amplitude of which are corrected. The amplitude correction depends on the
observation point x. Hence, this correction holds strictly only for one reference point xref. Calculating
the directional gradient of a plane wave and introducing (32.173) into (32.168) results in the driving
signal for the 2.5-dimensional synthesis of a plane wave
Dpw,2.5D(x0, ω) = 2wpw(x0) ˆSpw(ω)

i ω
c c2.5D(x0)nT
pwn(x0)e−i ω
c nTpwx0,
(32.174)
where npw = [cos θpw sin θpw]T denotes the normal vector of the plane wave and c2.5D(x0) a geometry
dependent amplitude factor for 2.5-dimensional reproduction. The amplitude correction may be derived
from (32.174). However, it has been shown [37] that the amplitude can be corrected also for a reference
line that is parallel to the secondary source distribution.
4.32.8.4 Spatial sampling and application example
As with any other method, practical implementations of WFS systems will not consist of a continuous
secondary source distribution but of a limited number of secondary sources placed at discrete positions.
Two types of artifacts may emerge from spatial truncation and discretization: (1) truncation and (2)
spatial aliasing artifacts. Truncation artifacts can be improved by applying a spatial window function
(tapering window) to the driving function [37].
The spatial sampling can be modeled in a similar manner as shown for NFC-HOA and the SDM in
Sections 4.32.6.4 and 4.32.7.3, respectively. An detailed analysis of the spatial sampling artifacts for
linear and circular WFS systems can be found e.g., in [70].
The same circular geometry as in Section 4.32.6.4 for NFC-HOA is used as application example in
order to facilitate the comparison of both approaches. As for NFC-HOA and SDM this constitutes a
2.5-dimensional scenario. Figure 32.25 shows the synthesized sound ﬁeld for two different frequencies
of the considered virtual plane wave. For 1 kHz, the synthesized sound ﬁeld shows no obvious artifacts.
However, when comparing Figure 32.25a with Figure 32.18a some inaccuracies can be observed close
to the secondary sources for WFS. This is due to the approximations applied for the derivation of
the driving function in WFS. Figure 32.25b shows the situation for 2 kHz. In comparison with Figure
32.18b it is clearly visible that WFS does not exhibit a pronounced area where the synthesis is more
accurate. Sampling artifacts are rather evenly distributed over the receiver area, especially at very high
frequencies. The amplitude decay in the synthesized plane wave, due to the 2.5-dimensional approach,
is clearly visible in Figure 32.25a.
4.32.8.5 Extensions to basic principle
Similarly to NFC-HOA, the basic WFS principle has been extended to the synthesis of various virtual
source types such as focused sources [37], sources with complex radiation properties [76], or moving
sources [77–79], as well as compensation for loudspeaker directivity [80], auralization of microphone
array recordings [39,81], and active listening room compensation [64,82].
Supplementarydataassociatedwiththisarticlecanbefound,intheonlineversion,at http://dx.doi.org/
10.1016/B978-0-12-396501-1.00032-7.

References
975
x −> [m]
y −> [m]
−2
−1
0
1
2
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
(a) fpw = 1 kHz
x −> [m]
y −> [m]
−2
−1
0
1
2
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
(b) fpw = 2 kHz
FIGURE 32.25
Sound ﬁeld P(x, ω) synthesized by a circular distribution of L = 56 secondary monopole sources and of
radius R = 1.50 m when driven with by 2.5-dimensional WFS. The virtual sound ﬁeld is a monochromatic
plane wave of frequency fpw and with incidence angle ϕpw = −π/2. Filled loudspeaker symbols indicate
active secondary sources, hollow loudspeaker symbols indicate in active ones. Video clips 7 and 8 to see the
animation.
Acknowledgments
The authors gratefully acknowledge the help of Peter Steffen in proof reading the manuscript and the contribution
of numerous ﬁgures by Paolo Annibale. The comments by Franz Zotter were helpful in consolidating the ﬁnal
version and are much appreciated.
Relevant Theory: Signal Processing Theory
See Vol. 1, Chapter 2 Continuous-Time Signals and Systems
See Vol. 1, Chapter 3 Discrete-Time Signals and Systems
References
[1] Jens Blauert, Rudolf Rabenstein, Providing surround sound with loudspeakers: a synopsis of current methods,
Arch. Acoust. 36 (1) (2012) 1–14.
[2] Jens Ahrens, Analytic Methods of Sound Field Synthesis, Springer, Berlin, 2012.
[3] J. Blauert, Spatial Hearing: The Psychophysics of Human Sound Localization, MIT Press, 1996.
[4] W.G. Gardner, 3-D audio using loudspeakers, PhD Thesis, Massachusetts Institute of Technology, 1997.

976
CHAPTER 32 Sound Field Synthesis
[5] V. Pulkki, Spatial sound generation and perception by amplitude panning techniques, PhD Thesis, Helsinki
University of Technology, 2001.
[6] R.J. Alexander, Michael Gerzon: Beyond Psychoacoustics, Dora Media Productions, 2008.
[7] G. Theile, On the localisation in the superimposed soundﬁeld, PhD Thesis, Technische Universität Berlin,
1980.
[8] M. Jessel, Acoustique Théorique—Propagation et Holophonie [Theoretical acoustics—propagation and
holophony], Masson et Cie, Paris, 1973 (text in French).
[9] D. Colton, R. Kress, Inverse Acoustic and Electromagnetic Scattering Theory, second ed., Springer, Berlin,
1998.
[10] O. Kirkeby, P.A. Nelson, Reproduction of plane wave sound ﬁelds, J. Acoust. Soc. Am. 94 (5) (1993)
2992–3000.
[11] J. Hannemann, K.D. Donohue, Virtual sound source rendering using a multipole-expansion and method-of-
moments approach, J. Audio Eng. Soc. (JAES) 56 (6) (2008) 473–481.
[12] J. Daniel, Représentation de champs acoustiques, application à la transmission et à la reproduction de scènes
sonores complexes dans un contexte multimédia, PhD Thesis, Université Paris 6, 2000.
[13] F.M.Fazi,P.A.Nelson,Theill-conditioningprobleminsoundﬁeldreconstruction,in:123rdAESConvention,
New York, USA, Audio, Engineering Society (AES), 2007.
[14] D.T. Blackstock, Fundamentals of Physical Acoustics, John Wiley & Sons, 2000.
[15] Philip M. Morse, Herman Feshbach, Theoretical Acoustics, McGraw-Hill Science/Engineering/Math, 1953.
[16] Philip M. Morse, Theoretical Acoustics, Princeton University Press, Uno Ingard, 1987.
[17] A.D. Pierce, Acoustics an Introduction to its Physical Principles and Applications, Acoustical Society of
America, 1991.
[18] L.J. Ziomek, Fundamentals of Acoustic Field Theory and Space Time Signal Processing, CRC Press, Boca
Raton, 1995.
[19] Jens Blauert, Ning Xiang, Acoustics for Engineers, Springer-Verlag, Berlin, 2009.
[20] Yang-Hann Kim, Sound Propagation, an Impedance Based Approach, Wiley & Sons (Asia) Pte Ltd, Singa-
pore, 2010.
[21] Fridolin P. Mechel (Ed.), Formulas of Acoustics, Springer, Berlin, 2002.
[22] E.G. Williams, Fourier Acoustics: Sound Radiation and Nearﬁeld Acoustical Holography, Academic Press,
1999.
[23] George B. Arfken, Hans J. Weber, Mathematical Methods for Physicists, Academic Press, Amsterdam,
Weber, 2001.
[24] N.A. Gumerov, R. Duraiswami, Fast Multipole Methods for the Helmholtz Equation in Three Dimensions,
Elsevier, Amsterdam, 2004.
[25] Richard Courant, David Hilbert, Methods of Mathematical Physics, Wiley-VCH, Weinheim, Germany, 2009.
[26] Rudolf Rabenstein, Jens Blauert, in: Schallfeldsynthese mit Lautsprechern II—Signalverarbeitung, ITG-
Fachtagung Sprachkommunikation, Bochum, 2010.
[27] Alfred Fettweis, Sankar Basu, Multidimensional causality and passivity of linear and nonlinear systems
arising from physics, Multidim. Sys. Signal Process. 22 (1) (2011) 5–25.
[28] D.L. Colton, R. Kress, Integral Equation Methods in Scattering Theory, Wiley, New York, 1983.
[29] J. Ahrens, S. Spors, On the scattering of synthetic sound ﬁelds, in: 130th AES Convention Audio Engineering
Society (AES), May 2011.
[30] F.M. Fazi, P.A. Nelson, R. Potthast, Analogies and differences between three methods for sound ﬁeld repro-
duction, in: Ambisonics Symposium, Graz, Austria, June 2009.
[31] J. Giroire, Integral equation methods for the Helmholtz equation, Integr. Equat. Oper. Theory 5 (1) (1982)
506–517.

References
977
[32] F.M. Fazi, P.A. Nelson, J.E.N. Christensen, J. Seo, Surround system based on three dimensional sound ﬁeld
reconstruction, in: 125th AES Convention Audio, Engineering Society (AES), San Fransisco, USA, 2008.
[33] S. Spors, J. Ahrens, Towards a theory for arbitrarily shaped sound ﬁeld reproduction systems, J. Acoust. Soc.
Am. 123 (5) (2008) 3930.
[34] J. Ahrens, S. Spors, Sound ﬁeld reproduction using planar and linear arrays of loudspeakers, IEEE Trans.
Audio Speech Lang. Process. 18 (8) (2010) 2038–2050. <http://dx.doi.org/10.1109/TASL.2010.2041106>.
[35] J. Ahrens, S. Spors, An analytical approach to sound ﬁeld reproduction using circular and spherical loud-
speaker distributions, Acta Acust. unit. Acustica 94 (6) (2008) 988–999.
[36] S. Spors, R. Rabenstein, J. Ahrens, The theory of wave ﬁeld synthesis revisited, in: 24th AES Convention
Audio Engineering Society (AES), May 2008.
[37] E.N.G. Verheijen, Sound reproduction by wave ﬁeld synthesis, PhD Thesis, Delft University of Technology,
1997.
[38] R. Rabenstein, S. Spors, Multichannel sound ﬁeld reproduction, in: J. Benesty, M. Sondhi, Y. Huang (Eds.),
Springer Handbook on Speech Processing and Speech Communication, Springer, Berlin, 2007, pp. 1095–
1114.
[39] E. Hulsebos, Auralization using wave ﬁeld synthesis, PhD Thesis, Delft University of Technology, 2004.
[40] U. Horbach, M. Boone, Practical implementation of data-based wave ﬁeld reproduction system, in: 108th
Convention of the AES, France, Paris, February 2000.
[41] S. Moreau, J. Daniel, S. Bertet, 3D sound ﬁeld recording with higher order ambisonics—objective measure-
ments and validation of a 4th order spherical microphone, in: 120th Convention of the AES, France, Paris,
May 2006.
[42] M.A. Gerzon, Width-heigth sound reproduction, J. Audio Eng. Soc. (JAES) 21 (1973) 2–10.
[43] Y.J. Wu, T. Abhayapala, Soundﬁeld reproduction using theoretical continuous loudspeaker, in: IEEE Inter-
national Conference on Acoustics, Speech, and Signal Processing (ICASSP), Las Vegas, USA, 2008.
[44] M.A. Poletti, Three-dimensional surround sound systems based on spherical harmonics, J. AES 53 (11)
(2005) 1004–1025.
[45] Franz Zotter, Hannes Pomberger, Markus Noisternig, Energy-preserving ambisonic decoding, Acta Acust.
Unit. Acust. 98 (2012) 37–47.
[46] J.S. Bamford, S. Vanderkooy, Ambisonics sound for us, in: 99th AES Convention Audio Engineering Society
(AES), October 1995.
[47] F. Zotter, H. Pomberger, M. Frank, An alternative ambisonics formulation: modal source strength matching
and the effect of spatial aliasing, in: 126th Convention of the AES, Munich, Germany, May 2009.
[48] J. Ahrens, S. Spors, A modal analysis of spatial discretization of spherical loudspeaker distributions used for
sound ﬁeld synthesis, IEEE Trans. Audio Speech Lang. Process. 20 (9) (2012) 2564–2574.
[49] J.R. Driscoll, D.M. Healy, Computing Fourier transforms and convolutions on the 2-sphere, Adv. Appl.
Math. 15 (2) (1994) 202–250.
[50] J. Ahrens, S. Spors, Analytical driving functions for higher order ambisonics, in: IEEE International Con-
ference on Acoustics, Speech and Signal Processing (ICASSP), Las Vegas, Nevada, March/April 2008.
[51] J. Ahrens, The single-layer potential approach applied to sound ﬁeld synthesis including cases of non-
enclosing distributions of secondary sources, Technische Universität Berlin, Doctoral Dissertation, 2010.
[52] S. Spors, J. Ahrens, A comparison of wave ﬁeld synthesis and higher-order ambisonics with respect to
physical properties and spatial sampling, in: 125th AES Convention Audio Engineering Society (AES),
October 2008.
[53] S.S. Bertet, Formats audio 3D hiérarchiques: caractérisation objective et perceptive des systèmes ambisonics
d’Ordres supérieurs, PhD Thesis, Institut National des Sciences Appliquees de Lyon, 2009.
[54] J. Ahrens, S. Spors, Focusing of virtual sound sources in higher order ambisonics, in: 124th Convention of
the AES, Amsterdam, The Netherlands, May 2008, p. paper 7378.

978
CHAPTER 32 Sound Field Synthesis
[55] D. Menzies, Calculation of near-ﬁeld head related transfer functions using point source representations, in:
Ambisonics Symposium, Graz, Austria, June 2009, pp. 23–28.
[56] F. Fazi, Sound ﬁeld reproduction, PhD Thesis, University of Southampton, 2010.
[57] D. Menzies, Ambisonic synthesis of complex sources, JAES 55 (10) (2007) 864–876.
[58] J. Ahrens, S. Spors, Rendering of virtual sound sources with arbitrary directivity in higher order ambisonics,
in: 123rd Convention of the AES, New York, NY, October 2007.
[59] M.A. Poletti, F.M. Fazi, P.A. Nelson, Sound reproduction systems using variable-directivity loudspeakers,
J. Acoust. Soc. Am. 129 (3) (2011) 1429–1438.
[60] M. Poletti, F.M. Fazi, P.A. Nelson, Sound-ﬁeld reproduction systems using ﬁxed-directivity loudspeakers,
J. Acoust. Soc. Am. 127 (6) (2010) 3590–3601.
[61] J. Ahrens, S. Spors, An analytical approach to 2.5D sound ﬁeld reproduction employing circular distribu-
tions of non-omnidirectional loudspeakers, in: 17th European Signal Processing Conference (EUSIPCO),
Glasgow, Scotland, August 2009, pp. 814–818.
[62] J. Ahrens, S. Spors, An analytical approach to 3D sound ﬁeld reproduction employing spherical distributions
of non-omnidirectional loudspeakers, in: IEEE International Symposium on Control, Communication and
Signal Processing (ISCCSP), Limassol, Cyprus, March 2010, pp. 1–5.
[63] T. Betlehem, T.D. Abhayapala, Theory and design of sound ﬁeld reproduction in reverberant rooms, JASA
117 (4) (2005) 2100–2111.
[64] S. Spors, R. Rabenstein, H. Buchner, W. Herbordt, Active listening room compensation for massive multi-
channel sound reproduction systems using wave-domain adaptive ﬁltering, JASA 122 (1) (2007) 354–369.
[65] J. Ahrens, S. Spors, Reproduction of a plane-wave sound ﬁeld using planar and linear arrays of loudspeakers,
in: Third IEEE-EURASIP International Symposium on Control, Communications, and Signal Processing,
St. Julians, Malta, March 2008.
[66] J. Ahrens, S. Spors, Applying the ambisonics approach on planar and linear arrays of loudspeakers, in: 2nd
International Symposium on Ambisonics and Spherical Acoustics, Paris, France, May 2010.
[67] B. Girod, R. Rabenstein, A. Stenger, Signals and Systems, John Wiley & Sons, Chichester, UK, 2001.
[68] D. de Vries, Wave Field Synthesis, AES Monograph, AES, New York, 2009.
[69] M. Abramowitz, I.A. Stegun, Handbook of Mathematical Functions, Dover Publications, 1972.
[70] S. Spors, R. Rabenstein, Spatial aliasing artifacts produced by linear and circular loudspeaker arrays used
for wave ﬁeld synthesis, in: 120th AES Convention Audio, Engineering Society (AES), Paris, France, May
2006.
[71] H. Wittek, Perceptual differences between waveﬁeld synthesis and stereophony, PhD Thesis, University of
Surrey, 2007.
[72] J. Ahrens, S. Spors, On the secondary source type mismatch in wave ﬁeld synthesis employing circular
distributions of loudspeakers, in: 127th AES Convention Audio, Engineering Society (AES), New York,
USA, October 2009.
[73] E.W. Start, Direct sound enhancement by wave ﬁeld synthesis, PhD Thesis, Delft University of Technology,
1997.
[74] A.J. Berkhout, A holographic approach to acoustic control, J. Audio Eng. Soc. 36 (1988) 977–995.
[75] S. Spors, Extension of an analytic secondary source selection criterion for wave ﬁeld synthesis, in: 123th
AES Convention Audio, Engineering Society (AES), New York, USA, October 2007.
[76] E.Corteel,Synthesisofdirectionalsourcesusingwaveﬁeldsynthesis,possibilitiesandlimitations,EURASIP
J. Adv. Signal Process. Article ID 90509 (2007).
[77] A. Franck, A. Gräfe, T. Korn, M. Strauß, Reproduction of moving virtual sound sources by wave ﬁeld syn-
thesis: an analysis of artifacts, in: 32nd International Conference of the AES, Hillerød, Denmark, September
2007.

References
979
[78] Andreas Franck, Efﬁcient Algorithms for Arbitrary Sample Rate Conversion with Application to Wave Field
Synthesis, Universitätsverlag Ilmenau, Ilmenau, 2012.
[79] J. Ahrens, S. Spors, Reproduction of moving virtual sound sources with special attention to the Doppler
effect, in: 124th Convention of the AES, The Netherlands, Amsterdam, May 2008.
[80] D. de Vries, Sound reinforcement by waveﬁeld synthesis: adaptation of the synthesis operator to the loud-
speaker directivity characteristics, JAES 44 (12) (1996) 1120–1131.
[81] M. Cobos, S. Spors, J. Ahrens, On the use of small microphone arrays for wave ﬁeld synthesis auralization,
in: 45th Conference of the AES, Helsinki, Finland, March 2012.
[82] P.-A. Gauthier, A. Berry, Adaptive wave ﬁeld synthesis with independent radiation mode control for active
sound ﬁeld reproduction: theory, JASA 119 (5) (2006) 2721–2737.

33
CHAPTER
Introduction to Speech Processing
Patrick A. Naylor
Department of Electrical and Electronic Engineering, Imperial College, Exhibition Road, London, UK
4.33.1 Background
This section addresses selected topics in speech signal processing. Speech is arguably the most natural
and effective means of human communication. We are all familiar with the use of speech, both in
face-to-face conversations and in telephony. The usefulness of speech to humans is clear and therefore
devices and services that employ speech in some form or another are always going to be important in
society in general as well as in business and commerce.
There are many applications of speech processing including for example automatic speech recogni-
tion, speech coding, speech synthesis, language identiﬁcation and speaker identiﬁcation. These applica-
tions, which typically process the speech signal and extract information from it, were once the invention
of science ﬁction writers. They are now, at least to some signiﬁcant extent, mature technologies and as
such are found in many everyday applications and devices including personal computers, telephones,
telephony systems and cars.
The physical processes by which speech sounds are generated are complex, involving the interaction
of many biomechanical systems. Nevertheless, understanding of these processes can lead to great
improvements in speech modeling and speech analysis. The study of speech production is therefore a
highly valuable research topic and provides the fundamental theoretical basis on which many of the
other speech processing applications are founded. In speech processing, as in many other successful
branches of science and engineering, a detailed understanding of each and every element of the process
being studied is helpfully substituted by the development of a functionally equivalent model for the
process, the model being more straightforward to work with than the true process. Analysis of speech
in terms of such a model brings both tractability and useful insights. Of course, this rationale relies on
the model being a sufﬁciently accurate representation of the actual process.
The value in, and effectiveness of, speech signals can be judged in terms of intelligibility and quality.
In simple terms, intelligibility is a measure of the proportion of words spoken in a speech signal which
can be correctly understood by a listener, whereas quality is an indication of the perceptual impression
thatthespeechsignalmakesonalistener.Itisoftenthecaseinpracticalcircumstancesthatspeechsignals
can be degraded in a way so as to reduce either or both intelligibility and quality. An example in which
such degradations may occur is the case when speaking on a telephone in an environment with high levels
of ambient noise, such as near to trafﬁc on a busy road. In this case, the signal transmitted to the far-end
telephone user will contain the sum of the intended speech with the degrading noise. The far-end user
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00033-9
© 2014 Elsevier Ltd. All rights reserved.
983

984
CHAPTER 33 Introduction to Speech Processing
would typically perceive a reduced quality in the speech because of the additive noise and, in very strong
noise, may not be able to hear all words correctly, thereby also suffering a reduction in intelligibility.
Speech enhancement is the general term for speech processing that aims to improve speech in some
aspect, usually perceived quality. This might involve, for example, reducing the noise in a speech signal,
removing the impact of clipping or improving the sound quality through application of equalization
techniques.
4.33.2 Overview of the chapters
4.33.2.1 Speech production modeling and analysis
This chapter begins with an historical overview of speech production modeling beginning from the
very early attempts, providing some fascinating introductory reading. A discussion of speech from the
point of view of phonetics and articulation leads on then to a deﬁnition of the various speech units, and
illustrations of how these speech sounds appear when seen on a spectrogram.
Much speech modeling is based on the source-ﬁlter model, a linear model in which an excitation
signal, either voiced or unvoiced, drives a ﬁlter representing the vocal tract. It is shown that the vocal tract
ﬁlter itself can be modeled as a concatenation of lossless acoustic tubes, which is a physically intuitive
model. Equally, the use of an all-pole linear ﬁlter is seen to lend itself to the parameter estimation
procedure of linear predictive coding. A key point of emphasis of the chapter is the modeling and
analysis of the voice source signal during voiced speech excitation. Extraction of descriptive parameters
of voiced excitation as well as analysis and modeling approaches are described.
4.33.2.2 Speech enhancement
The starting point for this chapter is to describe the means by which speech signals can be degraded
in practical applications, whether it be at the point of the acoustic transmission of the signal from the
talker to the microphone or the subsequent capturing, transmission or storage of the speech signal. The
description is supported by an illustrative model for a typical signal path in terms of the transmission
chain. Degradations are then grouped into categories in order that similarities and differences between
the different mechanisms and their corresponding degrading impact can be more clearly seen.
A range of signal processing methods for speech enhancement are discussed within a framework of
different classes of approaches. These include noise cancelation, ﬁltering of ﬁxed and variable types,
time-frequency gain modiﬁcation and time-varying gain methods, minimum mean square error-based
processing and methods applying excision or interpolation. Each of these approaches is targeted towards
a speciﬁc type of degradation and appropriate application scenarios are given. Needless to say, many of
these speech enhancement approaches make use of a variety of lower level processing functions which
are presented here conveniently under the subheading of ‘enabling algorithms’.
The measurement of speech intelligibility and quality is discussed in the later part of this chapter,
together with some comments on suitable test material and procedures. Measures that rely on human sub-
jects (subject-based measures) and measures that are computed from the signals (instrumental measures)
are presented. In the case of instrumental measures, the difference between intrusive and non-intrusive
measures is clariﬁed and examples of both types are given.

34
CHAPTER
Speech Production Modeling
and Analysis
Jon Gudnason
School of Science and Engineering, Reykjavik University, Iceland
4.34.1 Introduction
Speech production is the physical process carried out by humans to produce speech sounds. Speech is
formed by the ﬂow of air from the lungs through the glottis and the vocal folds, through the vocal tract
past the tongue, cheeks, and ﬁnally the lips. The ﬂow of air is disturbed in one way or another throughout
this process to produce the variety of speech sounds that make up spoken language. An understanding
of this process has enabled engineers to develop speech applications such as speech coders and speech
recognizers.
This chapter describes the speech production process, the state-of-the-art techniques used to model
that process and how these techniques can be used to perform analysis of speech. The study of speech
production requires insight into acoustics, physiology, phonetics, signal processing, and system dynam-
ics. A good understanding of psychology, circuit analysis and linguistics can also be helpful.
The chapter provides an overview of speech production, modeling and analysis and serves as a review
of an established area of knowledge in the ﬁeld. A historical overview is provided of several excellent
references that provide much more detail on many of the topics of the chapter. Of particular importance
to speech production are the books by Fant [1], Flanagan [2], Rabiner and Schafer [3], and Deller et al.
[4]. These references are invaluable to any serious researcher of speech production.
4.34.1.1 Mathematical background
Background in the following will prove useful in understanding the material in the chapter:
•
Signals and systems.
•
Adaptive signal processing.
•
Partial differential equations.
Of most importance is basic understanding of signals and systems. The most common way of mod-
eling speech production is the linear source-tract model. It is represented by two or three transfer
functions, each of which is represented by a z-transform. The time-varying nature of speech behooves
us to treat the subject with adaptive processing. Good understanding of linear prediction and autore-
gressive processes is therefore very useful. We will derive a linear vocal tract transfer function from
ﬁrst principles using the wave equation. A familiarity of partial differential equations will therefore be
helpful.
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00034-0
© 2014 Elsevier Ltd. All rights reserved.
985

986
CHAPTER 34 Speech Production Modeling and Analysis
4.34.1.2 Historical overview
It is important to realize that even though the framework for modern–day speech production modeling is
digital signal processing, the study of speech production started with mechanical imitation of the voice
and later electrical circuit analysis paved the way for research in speech processing using computers.
4.34.1.2.1
Mechanical resonators and early work
The early work on speech production was carried out with the purpose of synthesizing speech. The
ﬁrst example that is often cited is the work of Russian Professor Christian Kratzenstein in 1779. He
assembled acoustic mechanical resonators to imitate the a vocal tract and excited them with a vibrating
reed. At a similar time the polymath Wolfgang von Kempelen built a machine that demonstrated how the
voice could be modeled and synthesized. In the middle to late 1800s Charles Weatstone and Alexander
Graham Bell elaborated on von Kempelen’s work [2,5].
In the twentieth century, George Oscar Russell made a signiﬁcant contribution on speech articulation
with his book on the vowel [6] and in 1939, an all-electrical speech synthesizer called the “The Voder”
was demonstrated at the World’s Fair in San Francisco and New York by Homer Dudley [7]. This
required a trained operator to manipulate vocal-tract resonances, excitation carrier and a variable pitch.
Further discussion on early speech production theory can be found in [4,5].
4.34.1.2.2
Analysis with electrical circuits
An early signiﬁcant contribution to the ﬁeld of speech production modeling and analysis is the body
of work related to the modeling of the vocal tract using electrical circuits in the 1950s. Researchers
working at Bell Telephone Laboratories became particularly interested in the modeling of the speech
production as they sought to make telephone communications more efﬁcient. The large number of
telephone lines that was cluttering up dense population areas and major cities was becoming the serious
bottleneck in telephone communications. Analyzing and modeling the speech signal was therefore not
only an interesting theoretical ﬁeld but it was also the basis for addressing this practical problem by
compressing the speech signal.
The analogy between the vocal tract and transmission line theory was presented by Dunn [8]. The
vocal tract was represented as a series of cylindrical tube sections each of which is the analogy of a
T-section of impedances adjusted to ﬁt transfer function produced by the corresponding tube. The vocal
tract dimensions were derived from X-ray photographs in the median plane of the head and the formant
structure was studied using this analysis. Stevens et al. [9] presented a device called the electrical
analog of the vocal tract where speech production is based on concatenated circuit sections and the
source excitation controlled with a mixture of noise and periodic signals. Weibel [10] developed the
transmission matrix from the wave equation solved in a tube and used the development from Dunn [8]
and Stevens et al. [9] to develop the transfer impedance of the vocal system. In 1960, Gunnar Fant put this
theory in context of different phoneme classes in his book Acoustic Theory of Speech Production [1].
The simultaneous estimation of the vocal tract parameters and the voice source signal is called inverse
ﬁltering and was ﬁrst carried out by Miller [11]. The voice source signal was extracted using an electrical
circuit model of the vocal tract. The inverse network was derived and implemented and a sustained vowel
was played through it. By observing the movements of the vocal folds, researchers had realized that a
pitch cycle of the glottal ﬂow must contained a period during which the glottal ﬂow was zero. This fact

4.34.1 Introduction
987
was used to adjust the variable parameters of the inverse network to obtain a period during which zero
current was obtained. This period is called the closed phase of the pitch cycle and the rest of the pitch
cycle is called the open phase. Carr and Trill [12] analyzed four different vowels and found the spectral
slope of the voice source ranged between −8 and −16 dB/octave. The assumption of the voice source
having a constant spectral slope leads to the two-pole model of the voice source. Lindqvist-Gaufﬁn [13]
demonstrated the experimental process involved in inverse ﬁltering speech and observed the effect of
zeros in the speech signal due to the side cavities and the shape of the voice source signal. A description
of the development of the voice source and vocal tract modeling during this time can be found in [2,14].
From the early 1970s to present day, research in speech analysis and production modeling has been
conducted within the framework of digital signal processing.
4.34.1.3 Phonetics and articulation
It is useful to look at how we can categorize speech into distinct units, if we wish to study how speech
is produced. This section gives a brief overview of phonetics that is needed in understanding speech
production and puts this in context of the physiological processes of speech articulation and production.
4.34.1.3.1
Speech units
The fact that almost everyone uses spoken language all the time makes it both easy and difﬁcult to
describe the nature of it. It is easy because everyone can relate to the phenomena that occur in spoken
language. But the ubiquitousness of spoken language can make it difﬁcult to identify important aspects
of it. For example, a recording of a 3 s sentence is shown in Figure 34.1. The ﬁgure shows how different
sound units occur in a sequence representing the phonemes that make up the sentence “Six plus three
equals nine.” A more careful examination reveals that some of those units are periodic and others have
noise-like qualities.
FIGURE 34.1
The speech pressure waveform a male speaker saying: “Six plus three equals nine.”

988
CHAPTER 34 Speech Production Modeling and Analysis
The international phonetic alphabet provides a very detailed categorization of speech sounds but for
the purpose of speech production, it is sufﬁcient to understand three groups of speech sounds. The ﬁrst
group is called voiced speech. We will give quite a lot of attention to voiced speech, since the aggregate
duration of voiced speech during a normal utterance is typically very long. The other two groups are
both considered unvoiced speech, but the way they are produced and the way they manifest themselves
in the speech signal is quite different. These groups are fricatives and plosives. A short description of
each group follows with a few examples. The examples, however are not comprehensive but only meant
to demonstrate the articulatory nature of each group. Jurafsky and Martin [15] provide, for example,
good descriptions of these phoeneme groups.
Fricatives are formed by a constriction of the airﬂow in the vocal tract. The air ﬂows freely past
the vocal folds but the constriction, which is called the place of articulation, produces turbulence. This
creates a noise sound with qualities depending on the place of the articulation and the shape of the vocal
tract. The duration of these sounds can last up to several 100 ms.
An example of a fricative is the ﬁrst sound in the utterance depicted in Figure 34.1. This is the
phoneme /s/ in six. This is an example of aveolar fricative where the place of articulation is the teeth.
Another example of fricatives is /th/ in thought. This is a dental fricative where the place of articulation
is the tip of the tongue and the upper teeth.
Plosives are formed by a full closure of the vocal tract of a short duration. These sounds are charac-
terized by a very low energy region before a burst of energy when the pressure of the closure is released.
The ﬂow of air is temporarily blocked before an abrupt release. The result is an impulse like epoch the
shape of which is mostly determined by the place of articulation. For example the word cop contains
two stops, /k/ and /p/. The ﬁrst one, /k/, is formed by a blockage of the vocal tract at the back of the oral
cavity, whereas the second one, /p/ is formed by a blockage by the lips.
Voiced sounds are normally vowels like /ae/ in dark or /uw/ in tulip. They are typically produced
by an open vocal tract and a vibrating vocal folds that create the fundamental frequency. Vowels are
often described by three qualities: frontness, height, and roundness. Frontness is deﬁned by the lateral
front-back position of the tongue, height is determined by how much the lower jaw is dropped when
forming the vowel and roundness refers to the shape of the lips during the process. For example, the
vowel /uw/ in tulip, is considered to be back, low, and round.
Consonants can also be voiced. An example of this is voiced nasal sounds like /m/ in rum and /n/ in
sun. In this case the nasal cavity forms the last part of the vocal tract as the oral cavity is closed.
4.34.1.3.2
Speech articulation
Speech is formed by the interaction between three systems: the respiratory system, the laryngeal system,
and the articulatory system. The respiratory system consists of the lungs and the trachea. During speech,
air is expelled through the trachea, through the laryngeal system and the articulatory system. The airﬂow
enters the larynx from the trachea. During voiced speech, the airﬂow causes the vocal folds to vibrate.
In unvoiced speech, the vocal folds are open and do not affect the airﬂow signiﬁcantly. The airﬂow
travels through the vocal tract which is either, for most sounds, comprised of the pharynx and the oral
cavity or, for nasal sounds, the pharynx and the nasal cavity.
Voiced speech is formed by the vibrating vocal folds in the larynx which generate pulses of air that
get modulated as they propagate through the vocal tract. The vocal folds are kept tense and the steady
ﬂow from the lungs cause them to self oscillate. The tension in the vocal folds determines the frequency

4.34.1 Introduction
989
FIGURE 34.2
Images from a high-speed video sequence of a sustained /i/-like phonation (vowel approximates to /ae/).
Adult male with no signs or history of voice pathology (Courtesy of Daryush Mehta, Center for Laryngeal
Surgery and Voice Rehabilitation, Massachusetts General Hospital).
of their vibration and the frequency of the air pulses. This is also what determines the fundamental
frequency of the uttered speech signal. The perception of the fundamental frequency is called pitch and
often these terms are interchangeable.
In speech, the pitch varies fast enough so that we don’t perceive it as a musical note. The only
difference between voiced speech and singing is that in speech, the pitch varies but in singing, the pitch
is kept constant long enough for us to perceive it as a musical note. Otherwise, there is no physiological
difference between voiced speech and singing. The duration that is needed for listener to perceive a
musical note from a voiced utterance is about 200–300 ms.
The vocal folds can oscillate in various ways but during a steady voiced speech the vocal folds
oscillation cycle follows a particular pattern. Figure 34.2 shows six endoscopic images of the vocal
folds during one cycle of oscillation. The vocal folds typically remain closed for 0.5–3 ms during the
closed phase of the cycle. During this phase, a pressure difference builds up until the vocal folds start to
open. This is called the glottal opening instant which is followed by an opening phase during which the
folds are pushed open by the airﬂow. The opening phase normally lasts longer than the closed phase.
The vocal folds are pulled together due to the Bernoulli effect. As the pressure drops below a certain
level the tension in the vocal folds causes them to snap shut at the glottal closure instant. This is normally
much more abrupt than the opening instant and causes a greater level of discontinuity in the ﬂow.
Each image in Figure 34.2 shows the vocal folds at a different stage in the glottal cycle. The top of
each image is towards the back of the throat. The ﬁrst image shows the vocal folds in a closed position.
They gradually open and reach maximum opening in image 4 as they snap shut abruptly reaching full
closure again in image 6. High-speed endoscopic videos manage to capture this process at frame rates
up to 8 kHz providing a good analysis of the behavior of the vocal folds.
The vocal tract acts as a wave guide excited by the pulses of air coming from the larynx. It modulates
the spectrum of the air ﬂow as different resonant frequencies give rise to formants in the speech signal
spectrum. As mentioned in Section 4.34.1.3.1, different types of sounds are formed by the articulators
which are the jaw, tongue and lips. The vowel qualities of frontness, height and roundness refer to the
placement of the articulators. From a signal and systems perspective, this process is performed in order

990
CHAPTER 34 Speech Production Modeling and Analysis
FIGURE 34.3
Spectrogram of a the speech waveform a male speaker saying: “Six plus three equals nine.”
to realize a desired set of resonant frequencies of the vocal tract. The resulting sound wave does therefore
have a desired pattern of formant frequencies which is perceived as a particular vowel in the language.
Figure 34.3 shows a spectrogram of the utterance “Six plus three equals nine” spoken by a male.
Consider the difference in formant patterns between the three voiced sounds: the /uh/ in plus between
0.80 and 0.95 s, the /iy/ from the combined three and equal between 1.20 and 1.60 s and the /ay/ in nine
between 2.05 and 2.35. The strongly rounded vowel /u/ has two formants at relatively low frequencies
at ca. 800 Hz and 1000 Hz. The other two sounds are examples of dipthongs where the tongue position
changed considerably during the speech production of the phoneme. This causes the formant frequencies
to change during the uttering and can be seen in the ﬁgure.
For voiced nasal sounds the oral cavity is closed off by the lips but the main waveguide is now
comprised of the pharynx and the nasal cavity that is now open at the velum. The main difference in
conﬁguration is that the mouth forms a resonant chamber for the waveguide creating anti-resonances
in the system.

4.34.2 Speech Production Modeling
991
Fricatives form a much broader group of phonemes. Fricatives are apparent in the speech voice
form as relatively long noise–like sounds. For example, in Figure 34.3, the ﬁrst sound (between 0.35
and 0.5 s) is an /s/. There are no distinct frequency tracks (formants) as the shape of the magnitude
frequency is fairly ﬂat with no sharp peaks or valleys. The same phoneme occurs between 0.9 and 1.2 s.
Plosives are harder to spot due to their short durations. The only plosive to occur in the utterance
shown in Figure 34.3, for example, is the /p/ in plus, around 0.8 s. The impulsive nature of the sound is
manifested as a vertical line of energy in the spectrogram.
It is useful to be aware of how speech articulation works and to know the basic phonetics when
modeling and analyzing the speech signal. We saw how the speech is produced as an interaction between
the respiratory, laryngeal and articulatory system. We will now focus the attention on the articulatory
and the laryngeal system with the view of representing these processes with linear systems and models.
4.34.1.4 Applications of speech modeling
The subject of speech production modeling and analysis has always been closely linked to technological
application. It is therefore a very interesting scientiﬁc subject for the technologically motivated, since
the exploitation of scientiﬁc results in the ﬁeld is often very immediate. For example, the work produced
in the ﬁeld in the 1950s and 1960s produced efﬁcient speech coding methods that are still used in speech
communications today.
Speech synthesis is used in speech coding (at the receiver) [16], intext-to-speechapplications [17,18],
or in other speech modiﬁcation applications [19,20]. Identifying the glottal closure instants makes it
possible, for example, for the speech signal to be processed synchronously minimizing any phase
distortion caused by overlap-adding of frames during the synthesis.
Work on speech production modeling and voice source signal extraction has lead to better speech fea-
ture extraction. This has, for example, been applied to speaker veriﬁcation [21,22]. Recently researchers
have been able to detect Parkinson’s disease using features based on the voice source signal [23] and a
correlation between voice source features and depression has been discovered [24]. Both these methods
use the voice as an indirect channel of assessing neuro-physiological degradation that characterize the
disease in question.
4.34.2 Speech production modeling
The classical way of modeling the voice production system is to derive a linear transfer function
with the speech signal as the output and either an impulse train for voiced speech or white noise for
unvoiced speech as the input. There are two distinct reasons why this is done: (1) practicability and
(2) reasonable assumptions about the underlying physical process. If the linear transfer function is
assumed to be stationary over a small period of time, the parameters of the linear time-invariant system
can be estimated efﬁciently using, for example, the Levinson-Durbin algorithm [25,26]. This is what
makes the linear transfer function approach to voice production so desirable. But the second reason is
also very good. It turns out that a reasonable approach to describe the effect of the vocal tract is to model
it as a wave guide of piecewise constant diameter. This results in an all-pole transfer function.
The development of linear prediction coding [14,16] was based on the work done on speech produc-
tion [27] and the work on the more general all-pole modeling of signals developed in statistics [28] and

992
CHAPTER 34 Speech Production Modeling and Analysis
Glottal pulse 
model G(z)
Vocal-tract
model V(z)
Lip radiation 
R(z)
s(n)
uL(n)
uG(n)
e(n)
Random noise 
generator
Voiced/
Unvoiced
switch
FIGURE 34.4
Speech production model.
time series analysis [29]. Using the fact that the vocal tract model can be viewed as an all-pole system,
researchers proceeded to develop all-pole models for speech. This method lead to a very efﬁcient way
of coding the speech signal and is used in most communication systems nowadays involving speech.
This section describes the linear transfer function approach to modeling the voice production. The
linear source-tract model is introduced ﬁrst. It splits the process between voiced and unvoiced sounds
and between lip radiation effects, vocal tract and voice source. Each part of the process is then described
further. The section is concluded by a discussion on methods that do not use the linear source-tract
assumptions.
4.34.2.1 Linear source-ﬁlter model
The standard approach to modeling short segments of the speech signal is to use the linear source-
ﬁlter model shown in Figure 34.4 [3]. The contribution of the voice source, the vocal tract and the lips
are treated separately and assumed to be linear. Each short segment is assumed to be either voiced or
unvoiced so the source can be modeled either as a pulse train or random noise respectively. The linear
source-ﬁlter model has proven to be extremely powerful in modeling the speech production and it has
been used in a wide variety of speech applications.
The process of the sound wave escaping the vocal tract through the lips (or the nostrils when nasal
sounds are uttered) has a considerable effect on the voice production. The common approach to modeling
this is to consider the mouth to be a round oriﬁce set in a plane bafﬂe of inﬁnite extent. This is considered
to be a reasonable approximation if the lip opening is small compared to the size of the head. This leads
to the lip radiation transfer function [3],
R(z) = CR(1 −z−1),
(34.1)
where CR is a gain constant.
4.34.2.2 Vocal tract modeling
The purpose of vocal tract modeling is to be able to describe the relationship between the glottal ﬂow
signal and the recorded sound speech signal. Sound propagation through the vocal tract is a complicated
process and describing the motion of air in the vocal system is extremely difﬁcult. But it turns out that

4.34.2 Speech Production Modeling
993
deriving a linear transfer function between the voice source signal and the speech signal is possible by
making some fairly reasonable assumptions. One of the earliest work was done by Dunn [8] but this
was developed in the following decades. A comprehensive derivation of the vocal tract transfer function
is given in Rabiner and Schafer [3] and Dellar et al. [4]. This is an overview of this derivation and a
discussion on the assumptions used in the process.
4.34.2.2.1
The area function
Consider ﬁrst the shape of the vocal tract. It varies in diameter, both as a function of time and distance
from the vocal folds. It is conventional to describe the cross-sectional area of the vocal tract with a
function called the area function, A(x, t) where t is time and x is distance from the voice source. This
function has to be approximated as piecewise constant with respect to both time and distance. The
approximation with respect to time addresses directly the time variant nature of speech and implies
that the modeling has to be done on short speech segments that can be assumed to be time invariant. In
practice, the vocal tract model is estimated for a speech segment of size on the order of 25 ms.
The approximation of the area function with respect to distance leads to the so called loss-less tube
model. The area function A(x, t) is depicted in Figure 34.5 for a ﬁxed time window assuming a vocal
tract length of 16 cm. For adults the vocal tract is normally between 15 and 17 cm. The ﬁgure also shows
piecewise approximation to the area function. Each approximation corresponds to a cylindrical tube with
its own cross sectional area Ak. This is shown in Figure 34.6 where the length of the vocal tract is arbitrary
L, the number of tubes used in K and each tube is forced to have the same length xl = L/K. This restric-
tion is necessary to develop a transfer function for discrete signals as is discussed later in the section.
4.34.2.2.2
The sound pressure wave
Another important thing to consider is the sound pressure wave in the vocal tract. Two main assumptions
are made with respect to the sound pressure waveform: it is a planar wave traveling along the axis of the
vocal tract and that the walls of the vocal tract do not cause any losses in energy. The ﬁrst assumption is
reasonable for the frequencies that are normally of interest. For example, the frequency range of 20 Hz
to 10 kHz corresponds to a wavelength range of 17.5 m to 3.5 cm assuming velocity of sound in the
0
2
4
6
8
10
12
14
16
Distance from source, [cm]
Cross−sectional area
FIGURE 34.5
The vocal tract area function, A(x,t), at a speciﬁed time window, and its piecewise constant approximation.

994
CHAPTER 34 Speech Production Modeling and Analysis
FIGURE 34.6
The vocal tract as multiple loss less tubes each of same length but different cross-sectional area.
vocal tract to be c = 350 m/s. These wavelengths are large compared to the maximum diameter of the
vocal tract. The second assumption of losslessness is also reasonable considering the frequency range
of interest.
The propagation of sound in the kth tube is described by considering the pressure p(x, t) and volume
velocity u(x, t) to be functions of time t and distance from the start of the tube x ∈[0, L/P]. These
functions satisfy the two partial differential equations,
−∂pk(x, t)
∂x
= D
Ak
∂pk(x, t)
∂t
,
−∂uk(x, t)
∂x
=
Ak
Dc2
∂uk(x, t)
∂t
,
(34.2)
where D is the density of air in the tube, c is the velocity of sound, and Ak is the cross-sectional area of
the tube (which is now a constant). The solution to these equations is given by,
pk(x, t) = Dc
Ak
[u+
k (t −x/c) + u−
k (t + x/c)],
uk(x, t) = u+
k (t −x/c) −u−
k (t + x/c),
(34.3)
where the functions we can express the functions uk(0, t) = uk(t) = u+
k (t) + u+
k (t) with only one
independent variable, and u+
k (t −x/c) and u−
k (t + x/c) can be seen as the components of the traveling
waves in the positive and negative directions along the tube. If two tubes are connected, the volume
velocity and pressure waves are preserved at the boundary
pk(xl, t) = pk+1(0, t),
uk(xl, t) = uk+1(0, t),
(34.4)
where xl = L/K is the length of the tube segments and using (34.3) the two connected tubes can be
related with,
Ak+1
Ak
[u+
k (t −xl/c) + u−
k (t −xl/c)] = u+
k+1(t) + u−
k+1(t),
u+
k (t −xl/c) −u−
k (t + xl/c) = u+
k+1(t) −u−
k+1(t).
(34.5)

4.34.2 Speech Production Modeling
995
4.34.2.2.3
Discrete model of the vocal tract
These signals need to be sampled to convert this into a discrete process. The sound wave has to travel
twice the length of the tube so the sample period is determined as,
1
fs
= 2xl
c
= 2L
Kc,
(34.6)
where xl = K/L is the length of the tube segment, K is the number of tubes, L = 16 cm is the estimated
length of the vocal tract, c = 20√Tv ≈350 m/s is the speed of sound in the vocal tract, Tv ≈305 ◦K
is the air temperature in the vocal tract, and fs is the sampling frequency. This means, for example, that
K = 2L fs/c ≈15 tube segments are needed to approximate the vocal tract if the sound wave is sampled
at fs = 16 kHz. Each tube segment would be xl = L/K = 1.07 cm. The discrete equivalents of u+
k ( · )
and u−
k ( · ) are u+
k [n] and u−
k [n] whose z-transforms are U +
k (z) and U −
k (z) respectively. The two Eqs.
(34.3) can be solved for u+
k ( · ) and u−
k ( · ) and the corresponding equations in the z-domain become,
U +
k (z)
U −
k (z)

=
1
1 + ρk

z+ 1
2
0
0
z−1
2
  1
−ρk
−ρk
1
 U +
k+1(z)
U −
k+1(z)

=
z
1
2
1 + ρk

1
−ρk
−ρkz−1
1
 U +
k+1(z)
U −
k+1(z)

,
(34.7)
where z1/2 represents the propagation delay in the tube segment and,
ρk = Ak+1 + Ak
Ak+1 −Ak
(34.8)
is the refection coefﬁcient between the two tube segments. Concatenation of K tubes is therefore a
multiple of this system,
U +
G (z)
U −
G (z)

=
z
1
2 K
K
k=1(1 + ρk)
K

k=1

1
−ρk
−ρkz−1
1
 
U +
L (z)
0

,
(34.9)
where U +
G (z) is the glottal ﬂow velocity entering the vocal tract and U +
L (z) is the sound wave exiting
the vocal tract through the lips. The sound wave entering the vocal tract through the lips is assumed
to be zero, U −
L (z) = 0. U −
G (z) is the sound wave exiting the vocal tract through the glottis and the
assumption is that it is not reﬂected back and therefore irrelevant with respect to the vocal tract. The
vocal tract transfer function is therefore obtained as,
V (z) = U +
L (z)
U +
G (z) =
CV z−1
2 K
1 −K
k=1αkz−k ,
(34.10)
where z
1
2 K is the acoustic propagation delay along the vocal tract, CV is a gain constant, and the αk
coefﬁcients form an all-pole ﬁlter.

996
CHAPTER 34 Speech Production Modeling and Analysis
4.34.2.3 Modeling the voice source
Consider Figure 34.4. For unvoiced signals, the input of V (z) is considered to be white noise which
only scales the frequency response of the combined voice source and the vocal tract system. Using
an all-pole model like (34.10) is therefore appropriate. This is especially true for fricatives, where the
group delay is likely to be constant for all frequencies. Plosives (and other short unvoiced segments)
are normally also modeled with an all-pole model even though a source-ﬁlter model might not be the
best choice. The all-pole may on the other hand approximate the speech signals for plosives well, even
though it doesn’t represent the speech production process generating the signal.
For voiced signals, the input of V (z) is represented by another system G(z). The classical approach
to modeling G(z) is presented here, but further analysis and modeling approaches of the voice source
signal is given in Section 4.34.5.
4.34.2.3.1
The two pole model of the voice source for voiced speech
A linear model of the voice source signal is represented by G(z). The model is driven with an impulse
train whose times correspond to the glottal opening instants (see Figure 34.2). The classical model for
these pulses was proposed by Atal and Hanauer [14] and consists of two real poles close to unity (and
no zeros),
G(z) =
CG
(1 −z1z−1)(1 −z2z−1),
(34.11)
where CG is a gain related to the amplitude of the glottal ﬂow, and z1 and z2 are the real poles. Figure 34.7
shows the response of a glottal ﬂow model G(z) to a train of impulses set 14 ms apart and z1 = z2 = 0.9.
The result is a crude approximation to the voice source pulse train.
4.34.2.4 Estimating autoregressive parameters
The vocal tract transfer function (34.10), the two-pole model of the voice source (34.11) and the lip
radiation function (34.1) suggest that speech can be modeled as an autoregressive process. This is by
FIGURE 34.7
The response of the a glottal pulse ﬁlter to a train of impulses. The pitch period is set to 14 ms.

4.34.2 Speech Production Modeling
997
far the most common way of modeling speech production but it has to be noted that this is based on
a very simple approximation of the voice source signal. But the main advantage of this approach, and
the reason why it is so popular is that the literature on discrete autoregressive processes is rich and
extensive and it has produced the efﬁcient algorithms like the Levinson-Durbin algorithm to estimate
the parameters in the model [25,26]. The discussion in this section introduces the two main methods
of estimating the AR parameters but further discussion, analysis and outlines of efﬁcient algorithms
effecting these methods can be found in [4,30–32].
4.34.2.4.1
Model setup
The models of the glottal ﬂow G(z), vocal tract V (z), and the lip radiation function R(z) give rise to
the overall overall transfer function,
H(z) = S(z)
E(z) = G(z)V (z)R(z)
= CRCV (1 −z−1)
1 −K
k=1αkz−k
CG
(1 −z1z−1)(1 −z2z−1)
=
C
1 −M
i=1aiz−i ,
(34.12)
where S(z) is the z-transform of the speech signal, E(z) is the z-transform of the residual signal ϵ(n),
and ak are the autoregressive parameters of the speech signal. The gain constants are also combined in
the parameter C which is related to the variance of the residual signal ϵ(n) and the acoustic propagation
delay of the vocal tract z
1
2 K has been ignored.1 The number of AR parameters is determined from the
number of tube sections K which in turn are determined by the sampling frequency fs, the length of the
vocal tract L, and the sound velocity c (see (34.6)),
M = K + 1 = 2 fs L
c
+ 1.
(34.13)
This is a direct consequence of the spatial sampling (i.e., with respect to x) the area function A(x, t)
for a given value of t.
The transfer function formulation of (34.12) can be converted to the time-domain prediction model,
s(n) = −a1s(n −1) −a2s(n −2) −· · · −aps(n −M) + ϵ(n)
= −
M
	
i=1
ais(n −i) + ϵ(n) = ˆs(n) + ϵ(n),
(34.14)
where ˆs(n) is thought of as the predicted value of s(n) based only on former values of s(n) and ϵ(n) is
the error residual. It is assume that ϵ(n) is uncorrelated with s(n), i.e., E{ϵ(n)s(n −m)} = 0 for all n
and m > 0 and E{·} is the expectation operator.
1It may be thought to have been absorbed into S(z) or E(z).

998
CHAPTER 34 Speech Production Modeling and Analysis
4.34.2.4.2
Autocorrelation method
The autocorrelation method assumes that the covariance sequence cs(m) = E{s(n)s(n −m)} is given.
If the terms in (34.14) are multiplied by s(n −m) where m > 0, the expectation of both sides is taken
and noting that E{ϵ(n)s(n −m)} = 0, then the following equation is obtained,
cs(m) = −a1cs(m −1) −a2cs(m −2) −· · · −apcs(m −M),
m > 1.
(34.15)
The Yule-Walker equations are deﬁned as (34.15) for m ∈{1, 2, . . . , M} and have the following
matrix form,
⎡
⎢⎢⎢⎣
cs(0)
cs(1)
. . . cs( −M + 1)
cs(1)
cs(0)
. . . cs( −M + 2)
...
...
...
...
cs(M −1) cs(M −2) . . .
cs(0)
⎤
⎥⎥⎥⎦
⎡
⎢⎢⎢⎣
a1
a2
...
aM
⎤
⎥⎥⎥⎦= −
⎡
⎢⎢⎢⎣
cs(1)
cs(2)
...
cs(M)
⎤
⎥⎥⎥⎦.
(34.16)
If m = 0 so the terms in (34.14) is multiplied by s(n) and the expectation is taken, leads to the following
equation,
σ 2
ϵ = cs(0) + a1cs(1) + a2cs(2) + · · · + apcs(M),
(34.17)
where σ 2
ϵ = E{ϵ(n)s(n)} = E{ϵ(n)}2. The Eqs. (34.16) and (34.17) are called the Yule-Walker
equations [29] and can be used to compute the values for the AR parameters {a1, . . . , aM} and the
variance of the residual ϵ(n) if the covariance sequence cs(n) is known.
The same results can be obtained by minimizing the mean square error between the signal s(n)
and the predicted value ˆx = 
i ais(n −i) with respect to the parameters {a1, . . . , aM}. The result is
obtained by differentiating E{ϵ(n)}2 with respect to the parameters ai and setting equal to zero. The
autocorrelation method is therefore also called minimum mean square error method.
The AR coefﬁcient deﬁne an all-pole transfer function between ϵ(n) and s(n) and the autocorrelation
method always ﬁnds determines the coefﬁcients {a1, . . . , aM} in such a way that the transfer function
is stable. Furthermore, because of the symmetry of (34.16) and (34.17), the solution can be obtained
in O(M2) multiply-add operations rather than O(M3) which is typical for solving p equations with p
unknown.
The main downside of the autocorrelation method is that the ideal covariance sequence of the signal
is rarely available and so it needs to be estimated from the signal itself. Assuming a zero mean process
the covariance sequence is estimated for m ∈{0, 1, . . . , M} as using the autocorrelation,
ˆcs(m) = 1
N
N−1−m
	
i=0
s(i)s(i + m).
(34.18)
This method works for a large window size, N, but the autocorrelation method is strongly affected by
introduction of the window for small N (see discussion below).
4.34.2.4.3
Covariance method
The covariance method is more suitable when the AR parameters have to be obtained directly from the
signal and the window size, N, is small. One option is to ﬁnd the least squares solution by differentiating

4.34.2 Speech Production Modeling
999
the total square error,
E = 1
2
N−1
	
n=M
ϵ2(n) = 1
2
N−1
	
n=M
(s(n) −ˆx(n))2.
(34.19)
The AR parameters are obtained as the solution to
∂E
∂al
=
N−1
	
n=M
∂ϵ(n)
∂al
ϵ(n)
=
N−1
	
n=M
s(n −l)
 M
	
k=0
aks(n −k)

=
M
	
k=0
ak
 N−1
	
n=M
s(n −l)s(n −k)

=
M
	
k=1
akφl,k + ξl = 0,
1 ≤l ≤M,
(34.20)
where
φl,k =
N−1
	
n=M
s(n −l)s(n −k) and
(34.21)
ξl =
N−1
	
n=M
s(n −l)s(n) for k,l ∈{1, 2, . . . , M}.
(34.22)
These equations can be represented with the matrix equation
⎡
⎢⎢⎢⎣
φ1,1 φ1,2 . . . φ1,M
φ2,1 φ2,2 . . . φ2,M
...
...
...
...
φM,1 φM,2 . . . φM,M
⎤
⎥⎥⎥⎦
⎡
⎢⎢⎢⎣
a1
a2
...
aM
⎤
⎥⎥⎥⎦= −
⎡
⎢⎢⎢⎣
ξ1
ξ2
...
ξM
⎤
⎥⎥⎥⎦.
(34.23)
The AR parameters can be obtained by solving this equation. Furthermore, the variance of the residual
can be computed by using (34.19),
σ 2
ϵ =
1
N −M
N−1
	
n=M
ϵ2(n) =
1
N −M
M
	
k=0
akφ0,k.
(34.24)
An important point about this approach is that there is no windowing (rectangular or otherwise) required.
The method minimizes the error in the prediction of the nonwindowed speech signal for a speciﬁed
range of points.

1000
CHAPTER 34 Speech Production Modeling and Analysis
4.34.2.4.4
Correlation versus covariance method
The covariance method is a very elegant method that does not rely on the estimation of the covariance
sequence cs(m). It is preferred in speech analysis where the the signal segment is short leading to
a distortion due to the short duration of the window. A multiplication in the time-domain of a short
window function is equivalent to a convolution in the frequency domain of the signal with the Fourier
transform of the window.
The covariance method, however, does not guarantee that the poles of H(z) fall within the unit circle,
although this happens very rarely. This leads to an unstable transfer function (and a wrong estimate of
the vocal tract). Also, the solution of the matrix in (34.23) is not Topelitz so it does require O(M3)
operations to solve. These are the reasons why the correlation method is preferred. The problem with
the short duration can sometimes be addressed by choosing an appropriate window function [33].
4.34.2.5 Current work on time-varying estimation
The ﬁrst development of linear predictive coding estimates the vocal tract parameters on a frame-
by-frame basis. This is based on the assumption that the shape of the vocal tract does not change
signiﬁcantly over a short enough time period. An early study of time-varying autoregressive modeling
of speech was done by Liporace [34], but later work on this topic includes work on frequency-warped
modeling [35], forced continuous trajectories of the vocal tract parameters [36], a joint modeling of
time-varying autoregressive parameters and parameters describing the voice source [37] and an efﬁcient
detection of change in vocal tract parameters using time-varying AR modeling [38].
4.34.3 Estimating the voice source signal
In the speech analysis literature, the term inverse ﬁltering of speech refers to the subtraction of the
effect of the vocal tract on the speech signal to reveal the voice source signal. The reason why this is
a hard problem is that both the input of the system (the voice source signal) and the system response
(the vocal tract) is unknown and the only thing which is observed is the output (the speech signal). In
a broad sense, the term “inverse ﬁltering,” therefore, refers to the combined process of estimating the
vocal tract transfer function and the voice source signal from the output. But sometimes, people use the
term “inverse ﬁltering” in a narrow sense, to refer to the application of the inverse vocal tract ﬁlter to
the speech signal. If the aim is to extract the voice source signal from the recorded speech signal, the
joint estimate of the vocal tract and the voice source has to be addressed.
Section 4.34.1 described how inverse ﬁltering was ﬁrst implemented using electrical circuits [11].
Discrete time processing of the speech signal improved inverse ﬁltering techniques. Instead of relying
on electrical circuits with variable components, researchers could compute optimal all-pole parameters
to produce an inverse model. Strube [39] and Wong et al. [40] used the fact that if the analysis window
of the all-pole model was reduced to a fraction of a pitch cycle, the prediction error of the model would
be much lower during a period of glottal closure. Strube [39] used this fact to identify the glottal closure
instants and Wong et al. [40] obtained the glottal ﬂow waveform. Deriving the glottal ﬂow waveform
automatically, without manual adjustment of the vocal tract parameters, has great advantages because
the process of manual adjustment relies on subjective criteria and and may be subject to the whims of
the person/expert doing the adjustment.

4.34.3 Estimating the Voice Source Signal
1001
The joint estimate the vocal tract transfer function and the voice source signal is carried out by
using some assumptions about the system and/or the input signal. Applying the autocorrelation or the
covariance method directly on the speech signal inherently assumes that the input signal has a ﬂat
spectrum, which is not the case for the voice source signal. Instead, it can be characterized as having
low-pass qualities with an average spectral tilt of −8 dB/octave. In the time domain, the voice source is
also considered to be zero during the closed-phase of the larynx cycle. During the closed-phase, no air
enters the vocal tract and the recorded speech signal becomes the product of freely decaying resonances
in the vocal tract.
An inverse ﬁltering approach includes some of these assumptions for the joint estimation of the vocal
tract transfer function and the voice source signal. This section will focus exploiting the feature that the
voice source signal is zero during the closed-phase of the larynx cycle. The way we approach this is to
apply the covariance analysis described in Section 4.34.2.4.3 to a window that shifts one sample at a
time. This allows us to recognize the difference between the closed and open phases in the pitch cycle.
If the closed phases of the speech signal are identiﬁed, the vocal tract estimate and the voice source
signal can then be obtained.
4.34.3.1 Time varying covariance analysis of speech
Consider the linear source-tract model, shown in Figure 34.4 and assume that a short-time sliding
window covariance analysis, explained in Section 4.34.2.4.3 is applied to uL(n) whose z-transform is
given as S(z)/R(z). Generally, for voiced speech, the estimated model obtained from such an analysis
is the joint transfer function V (z)G(z). When the voice source signal uG(n) is zero, the sound wave
is freely decaying resonances in the vocal tract. An all-pole model ﬁts well to such a system and the
relative energy in the prediction residual drops. The estimated model represents the vocal tract function
V (z) as there is no contribution from G(z).
A new set of ﬁlter parameters is obtained for each time sample and the model of the speech signal is,
s(n) = −
M
	
i=1
ai(n)s(n −i) + ϵ(n),
(34.25)
where s(n) is the speech signal, ai(n) are the M model coefﬁcients for time instant n and ϵ(n) is the
prediction residual. For each (sliding) analysis window, the total squared error is,
Eϵ(n) =
n+N−M−1
	
j=n
ϵ2( j),
(34.26)
where N is the length of the analysis window. It is better to observe the normalized total squared error
η(n) = Eϵ(n)/Es(n), where Es(n) is the total squared signal for the analysis window. Figure 34.8 shows
a voiced segment of speech and the energy ratio. As can be seen, for example around t = 1.085 s, the
energy ratio takes a considerable dip during the closed phase of the glottal cycle.
The vocal tract transfer function can be identiﬁed during the closed phase of the glottal cycle.
Figure 34.8 also shows three choices of analysis windows to demonstrate how sensitive the estimation
is with respect to the window time shift. The ﬁrst choice of window, labeled (a) occurs during the closed
phase only. The second choice (b) contains some of the closed phase and some of the open phase.

1002
CHAPTER 34 Speech Production Modeling and Analysis
1.07
1.075
1.08
1.085
1.09
1.095
1.1
1.105
1.11
Time [s]
s(n)
1.07
1.075
1.08
1.085
1.09
1.095
1.1
1.105
1.11
Time [s]
η1/2(n)
(a)
(b)
(c)
FIGURE 34.8
A segment of voiced speech signal shown with the ratio of prediction residual energy to the speech energy.
The extent of analysis windows (a), (b), and (c) are shown in the plot of the ratio.
The third choice (c) includes the open phase and the instant of glottal closure. Figure 34.9 shows the
position of the poles for the three choices of windows and Figure 34.10 shows the estimated power
spectrum of the transfer function. The main difference between of the pole plots for analysis windows
(a) and (b) is the positive real pole appearing closed to z = 1 for window (b). This demonstrates that the
autoregressive modeling is including the effect of the glottal source by including a real positive pole.
This difference is also apparent in the power spectrum in Figure 34.10. There is a negative trend in the
spectrum for window (b) and the low frequency formants are skewed due to the effect of the glottal
source. The choice of (c) for analysis window demonstrates another problem with covariance analysis.
The combined effect of the vocal tract and the voice source results in an all-pole models with poles
outside the unit circle and therefore an unstable ﬁlter.
The result of inverse ﬁltering using the three transfer functions obtained by the windows (a), (b),
and (c) are shown in Figure 34.11. The ﬁgure shows the estimated glottal ﬂow derivative. The only
reasonable estimate is that of analysis window (a). The closed phase is apparent as the ﬂat, albeit noisy
part of the cycle, the open phase is visible as the pulse and the glottal closure instant can be seen as the
abrupt negative spike.

4.34.3 Estimating the Voice Source Signal
1003
−1 −0.8 −0.6 −0.4 −0.2
0
0.2
0.4
0.6
0.8
1
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
Real
Imaginary
(a)
(b)
(c)
−1 −0.8 −0.6 −0.4 −0.2
0
0.2
0.4
0.6
0.8
1
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
Real
Imaginary
−1 −0.8 −0.6 −0.4 −0.2
0
0.2
0.4
0.6
0.8
1
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
Real
Imaginary
FIGURE 34.9
Poles of the transfer functions obtained by using the analysis windows (a), (b), and (c) shown in Figure 34.8.

1004
CHAPTER 34 Speech Production Modeling and Analysis
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
10000
−20
0
20
40
Frequency [Hz]
(a)
(b)
(c)
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
10000
−20
0
20
40
Frequency [Hz]
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
10000
−20
0
20
40
Frequency [Hz]
FIGURE 34.10
Spectrum of the transfer functions obtained by using the analysis windows (a), (b), and (c) shown in
Figure 34.8.
From this example, it can be seen, that the voice source signal can be obtained from the speech signal,
using inverse ﬁltering if we either:
1. know the extent the closed-phases in the pitch cycles or,
2. we can somehow discard vocal tract estimates of time periods where the window includes samples
from the open-phase.
Option 2 requires an estimate of the vocal tract parameters for every time sample. We would also need
to determine whether the analysis window is within the closed phase. This could be done by computing
η(n) which is shown in Figure 34.8. Option 2, is however, computationally less desirable option 1. We
will describe option 1 in more detail in the following section. The detection of GCIs and GOIs (and
therefore the closed phases) is discussed further in Section 4.34.4.

4.34.3 Estimating the Voice Source Signal
1005
1.07
1.075
1.08
1.085
1.09
1.095
1.1
1.105
1.11
Time [s]
(a)
(b)
(c)
1.07
1.075
1.08
1.085
1.09
1.095
1.1
1.105
1.11
Time [s]
1.07
1.075
1.08
1.085
1.09
1.095
1.1
1.105
1.11
Time [s]
FIGURE 34.11
Estimate of the glottal ﬂow derivative using the transfer functions obtained by using the analysis windows
(a), (b), and (c) shown in Figure 34.8.
4.34.3.2 Closed-phase covariance analysis
The glottal closure and opening instants (GCI/GOI) can either be detected directly from the speech or
from a laryngograph recording. Figure 34.12a shows the same segment of speech of that in Figure 34.8
where the closed phases have been identiﬁed and indicated with a yellow shade. The vocal tract param-
eters are identiﬁed from the speech samples of consecutive 25 ms analysis windows by using only those
samples that belong to the closed phases. This is done by using a weighted estimate of the covariance
in (34.21) and (34.22).
φl,k =
N−1
	
n=M
w(n)s(n −l)s(n −k) and
(34.27)

1006
CHAPTER 34 Speech Production Modeling and Analysis
(a)
(b)
FIGURE 34.12
Speech segment (a) with identiﬁed closed phases (shaded). The speech segment is inverse ﬁltered (b) using
covariance analysis relying only on speech samples in the closed phase.
ξl =
N−1
	
n=M
w(n)s(n −l)s(n) for k,l ∈{1, 2, . . . , M},
(34.28)
where w(n) is a weight term that can be used to emphasize important time samples in the signal and
deemphasize unimportant samples. If the closed phase has been identiﬁed the weights can be set as
w(n) =
 1 if n ∈C,
0 if n /∈C,
(34.29)
where C is a set of all samples in the closed phase. This means that (34.23) can still be used to estimate the
autoregressive parameters ap but the covariance matrix is only based on speech samples from within the
closed phase where the voice source signal is assumed to be zero. The result is shown in Figure 34.12b.
The autoregressive parameters are estimated directly on the speech signal and the inverse ﬁltering is

4.34.4 Glottal Closure Instants
1007
also applied directly on the speech signal. The effect of the lip radiation (34.1) has therefore not be com-
pensated for. This voice source waveform is therefore an estimate of the glottal volume ﬂow derivative.
This method has given us an estimation of vocal tract ﬁlter V (z) and the voice source signal uG(n)
(or its derivative). The glottal pulse model G(z) has not been estimated. It is straightforward, however,
to model the glottal pulse given the estimation of uG(n). This is discussed further in Section 4.34.5.
4.34.3.3 Other methods and current research
This section demonstrated how to carry out closed-phase covariance analysis of speech. This method
was based on the time-domain properties of the voice source signal and the vocal tract. We identiﬁed
time periods in the speech signal where the contribution of the voice source signal was negligible and
estimated the vocal tract accordingly.
Drugman et al. [41] used the complex cepstrum to separate the causal and the anti-causal part of
the speech to extract the voice source signal. The assumption here is that the contribution of the voice
source signal is fully described by the anti-causal part of the speech signal. This can be decomposed
using the complex-cepstrum where the signal for negative cepstrum index extracts the anti-causal part
and hence the voice signal.
Other approaches make use of the frequency properties of the vocal tract and the voice source signal.
The result of Section 4.34.2 suggests that the magnitude frequency response of the vocal tract ﬁlter is a
comb-like all-pass response (or rather, with neither low- or high-pass slope) with frequency tops (poles)
at regular frequency intervals. The frequency contribution of the voice source, however, is assumed to
be low-pass with a gradual decline in magnitude with higher frequency.
These properties can also be exploited to extract the voice source signal. The simplest approach is
to apply a high-pass ﬁlter with a zero close on the real-axis close to unity. This cancels out the low-pass
contribution of the voice source so that the poles of the estimated vocal tract can approximate the formant
frequencies. The high-pass ﬁlter is often referred to as pre-emphasis. Another popular method called
the pitch synchronous iterative adaptive inverse ﬁltering [42] uses a more complex high-pass ﬁlter to
cancel out the effect of the vocal fold. This ﬁlter is estimated with an iterative approach.
All these methods, including the closed-phase analysis, suffer from the fact that a ground-truth
signal does not exist. This makes it hard to quantify performance of inverse ﬁltering. The only guide
to researchers are reasonable observations of the behavior of the vocal folds and assumptions about the
glottal ﬂow thereof.
4.34.4 Glottal closure instants
Several algorithms have been proposed for estimating glottal closure instants (GCI) from a speech
waveform. Detecting the GCIs in voiced speech has many applications, ranging from speech synthesis
[17] and speech rate modiﬁcation [20] to identifying the closed-phases in voiced speech for covariance
analysis [40] and feature extraction [22].
Two early approaches to glottal closure instant detection used discontinuities in the all-pole model of
the speech production [39,40]. This approach has since been developed [21,43] but alternative methods
have since proliferated. One possible categorization of these methods depends on which baseline signal
is used to detect the instants. Detecting the energy peaks in the speech waveform or its time-frequency

1008
CHAPTER 34 Speech Production Modeling and Analysis
representation was suggested by some researchers [44–47]. For example, the GCI in [44] is identiﬁed as
the maximum of the Frobenius norm of the signal matrix (extending over one cycle). A very common
approach to detect GCI is to use the linear prediction residual of the speech waveform as the baseline
signal. Cheng and O’Shaughnessy [48] used the residual to identify the GCI. The Hilbert envelope was
used to ﬁlter out harmonic components caused by formants still present in the residual making detection
of the GCI possible. A series of methods relying on the group delay function was also suggested by
Smits and Yegnanarayana [49], Yegnanarayana and Smits [50], and Murthy and Yegnanarayana [51].
The estimates of the time instants of excitation within an analysis frame were identiﬁed by zero-crossings
of the frequency-averaged group delay over a sliding window applied to the LPC residual. Alternative
baseline signals include the voice source signal itself. For example, work on energy ﬂow in the lossless-
tube model [52] has suggested that the signal representing acoustic input power at the glottis could be
used to determine the instants of glottal closure and opening. Section 4.34.4 will describe these methods
in detail.
4.34.4.1 Overview of GCI detection
One important aspect of speech analysis for speech production modeling is the identiﬁcation of signif-
icant epochs in the speech signal. For voiced speech, this means speciﬁcally that glottal closing and
opening instants are identiﬁed.
The instants of closing and opening cause discontinuities in the glottal ﬂow which manifest them-
selves in the speech signal. Therefore, the core of any GCI and GOI detection algorithm is a discontinuity
detection. However, most algorithms include a pre-processing step, where some derived signal is com-
puted in which the discontinuities are more pronounced. This is very often the linear predication residual
but other preprocessing is common too. Most methods also include a post-processing step where spuri-
ous epochs are excluded. The post-processing sometimes rely on ad-hoc rules about regularity of GCI
and GOI occurrence and the nature of the speech signal and the residual at the instants.
The process is demonstrated in Figure 34.13. The ﬁrst panel shows a 40 ms speech segment sampled
at fs = 20 kHz. The second and third panels show the linear prediction residual ϵ(n) and the group
delay function d(n) of the residual. The glottal closure instants are shown as stems in the last trace of
the ﬁgure.
These processing steps are shown in Figure 34.14 are discussed in the following subsections.
4.34.4.2 Preprocessing for GCI detection
The most common preprocessing method for detecting GCIs is the linear prediction residual ϵ(n).
The linear prediction residual shown in Figure 34.13 is produced using the method described in
Section 4.34.2.4 using 22 AR parameters. The instants are apparent in the residual signal as impulse
like features. The rest of the residual is noise-like although the weaker opening instant can be seen as
secondary impulse features, e.g., around 2525 ms and 2543 ms.
A related approach is to compute the determinant of the covariance matrix in (34.23) [39]. A peak
in the computed determinant indicates dependency in the p equations which means accurate prediction.
This is what happens when there is an impulse in the excitation signal followed by a closed interval (or
zeros).

4.34.4 Glottal Closure Instants
1009
2510
2520
2530
2540
2550
2560
s(n)
2510
2520
2530
2540
2550
2560
ε (n)
2510
2520
2530
2540
2550
2560
d(n)
Time [ms]
FIGURE 34.13
Detecting glottal closing instants in the speech signal. A 40 ms speech segment s(n), linear prediction
residual ϵ(n) and the group delay function d(n).
FIGURE 34.14
Processing steps for extracting glottal closing and opening instants.
4.34.4.2.1
Detecting energy peaks in the derived signal
A straight forward way of detecting a peak in a signal u(n) is to look for the center of energy in a
running window over the signal (assuming an odd length window size of M + 1),
xn(m) = w(m)u(n + m) for m = −M/2, . . . , M/2,
(34.30)

1010
CHAPTER 34 Speech Production Modeling and Analysis
where w(m) is a window function. The center of energy is then
d(n) =
M/2
m=−M/2nx2
n(m)
M/2
m=−M/2x2n(m)
.
(34.31)
When there is a signiﬁcant energy peak in the analysis window xn(m) close to m0, the center of energy
takes a value d(n) ≈m0 so when there is a negative-going zero crossing in d(n) then the peak is detected
at that instant. This is shown in the third trace in Figure 34.13 where some of the negative-going zero
crossings do indeed coincide with the glottal closure instants indicated as stems.
Another way of analyzing xn(m) is to use the group delay,
τn(k) = −d arg (Xn(k))
dω
,
(34.32)
where Xn(k)isthediscreteFouriertransformof xn(m).Thegroupdelaytakesavalueoverallfrequencies
but the location of a signiﬁcant energy peak can be extracted by taking an average value over all
frequencies. This is the core idea in the GCI detection method proposed in [49,51].
It was shown in [53] that the center of energy d(n), given in (34.31), is equivalent to taking a weighted
average of the group delay, where the weight for each frequency is determined by the energy,
d(n) =
M
k=0|Xn(k)|2τn(k)
M
k=0|Xn(k)|2
.
(34.33)
Two other methods of extracting a single value from the group delay were proposed in [17] and evaluated
in [53]. It was found that the most robust method for detecting peaks is the energy weighted group delay
which can be computed as the center of energy (34.31).
The fact that a discontinuity in a signal affects all frequencies has been used in at least two GCI
detection methods. Murty and Yegnanarayana [54] implemented a zero-frequency ﬁlter for the differen-
tiated speech signal. The zero-frequency ﬁlter is an ideal resonator at zero frequency which contains two
poles at z = 1 and is therefore unstable. The trend caused by the instability is removed by subtracting
the average of a 10 ms window around the current value. The GCIs are then determined as the positive
going zero crossings.
Another method which uses the fact that a discontinuity affects all frequencies was proposed in [55].
In this method the voice source signal was derived by other methods [42] and the GCI and the GOI
derived from the multiscale product of the stationary wavelet transform of the voice source signal. Both
the closing and opening instants were extracted from the multiscale product since the discontinuities in
the voice source signal affected all the scales in its stationary wavelet transform.
The third trace in Figure 34.13 shows how the impulse features in the linear prediction residual have
been detected by the negative going zero crossings in the group delay function. However, other impulse
like features are detected as well as shown, for example, around 2516 ms and 2525 ms. The reason for
why there are so many other negative going zero crossing is that the analysis window used to calculate
the group delay function was 0.3 ms or much smaller than the pitch period. This means that there are
times when the analysis window does not contain a glottal closure instant so that secondary peaks in
signals can cause negative going zero crossing.

4.34.4 Glottal Closure Instants
1011
One way to ﬁx this is to choose the window size carefully, either by ﬁxing it to an average pitch
period value or to adapt the size of the window. But increasing the size of the analysis window also
increases the probability of missing signiﬁcant energy peaks. The alternative is to use the discontinuity
detection to produce false alarms and then prune the choice afterwards using post processing.
4.34.4.3 Postprocessing: determining the signiﬁcance of an energy peak
Postprocessing can be used to eliminate false GCI candidates. For example a computation “level of
conﬁdence” was proposed in [49]. This measure is based on the gradient of the slope of the group delay
function and is used to eliminate false GCI candidates. Other static measures, such as the energy in the
residual signal around the detected peak have also been used.
The Dynamic Programming Phase-Slope Algorithm (DYPSA) was proposed in [56]. Using dynamic
programming, the question of GCI candidate selection is changed from choosing the best GCIs on indi-
vidual basis, to selecting the best sequence of GCIs. The cost function contained ﬁve terms measuring:
•
speech similarity,
•
pitch deviation,
•
projection (if the group delay didn’t reach zero),
•
slope of group delay function,
•
normalized energy.
Each term takes a value between −1 (negative cost) and 1 (positive cost). The weights of the cost terms
were derived using cross validation data. The most important cost was the speech similarity measure
which is based on the cross correlation between the speech signal around the currently hypothesized
candidate and the previous hypothesized candidate. The pitch deviation measure also weighted quite
strongly in the cost function.
4.34.4.3.1
Performance evaluation
Naylor et al. [56] also deﬁned a set of performance measures for evaluating GCI detection algorithms.
The measures are the identiﬁcation-, miss-, and false alarm rate and the identiﬁcation accuracy produced
by an automatic detection, compared with ground truth obtained from a laryngograph. An identiﬁcation
is achieved if the algorithm produces one GCI in a pitch period. A miss is when no GCI is produced in the
pitch cycle and a false alarm is when two or more GCIs are detected. The identiﬁcation accuracy is the
standarddeviationofthetimedifferencebetweenallidentiﬁedGCIsandtheground-truth.Drugmanetal.
Table 34.1 Results Showing the Performance of Five Methods for GCI Detection
Method
IDR (%)
MR (%)
FAR (%)
IDA (ms)
HE
91.74
5.64
2.62
0.73
DYPSA
96.12
2.24
1.64
0.59
ZFR
98.89
0.59
0.52
0.55
SEDREAMS
98.67
0.82
0.51
0.45
YAGA
98.88
0.52
0.60
0.49

1012
CHAPTER 34 Speech Production Modeling and Analysis
[57] gave a review of ﬁve prominent GCI detection algorithms and compared their results. Table 34.1
shows the results for the APLAWD database.
ThemethodsshownherearetheHilbertEnvelopemethod[48],DYPSA[56],Zero-frequencyﬁltering
[54], the Speech Event Detection using the Residual Excitation and a Mean-based Signal (SEDREAMS)
algorithm [58] and the YAGA algorithm [55].
4.34.5 Voice source modeling
A segment of a recorded speech signal and the derivative of the extracted voice source signal is shown
in Figure 34.15. The aim of modeling the voice source is to describe the voice source using a compact
representation. This can be useful for feature extraction or to compress the speech signal even further.
265
270
275
280
285
290
295
Speech signal
265
270
275
280
285
290
295
Time [ms]
Voice Source
CP
OP
GCI
GOI
FIGURE 34.15
A segment of voiced speech signal and the derivative of the corresponding voice source signal. A glottal
closure instant (GCI), glottal opening instant (GOI), closed phase (CP), and open phase (OP) are identiﬁed.

4.34.5 Voice Source Modeling
1013
4.34.5.1 Piecewise approximation of the voice source
The ﬁrst parametric approach to the modeling of the voice source was presented by Rosenberg [59]
using three free parameters for each pitch cycle. The model is based on the observation that a pitch
cycle of the voice source signal can be split into different phases each of which can be approximated by
a continuous function. The value of the voice source is considered to be zero during the phase where
the vocal folds are closed but the open phase is approximated with trigonometric functions.
Many improvements to this approximation have been proposed. A speech synthesizer using 7 control
parameters to model the voice source signal was developed by Klatt and Klatt [60]. The parameters
determine the amplitude of voicing, open and closed quotients and amplitudes of aspiration noise.
Fujisaki and Ljungqvist [61] performed an autoregressive moving average analysis on the voice source
and vocal tract and developed a four-segment, 6 parameter model of the glottal ﬂow derivative. The LF
model was modiﬁed by adding a skewness factor to the open phase by Brookes and Chan [62]. The
glottal ﬂow was parameterized so that each larynx cycle was represented by a piecewise continuous
functions [63]. More recent work on parameterizing the voice source signal includes the methods by
Alku and Backstrom [64], Backstrom et al. [65].
One of the most widely used/cited improvement to the Rosenberg model is the LF (Liljencrants-Fant)
model [66]. It models the derivative of the voice source signal, shown in Figure 34.15. The important
improvement introduced in the LF model is the addition of an exponential recovery phase to the cycle
adding an extra free parameter to the modeling. This model was developed in a similar fashion to that
of Rosenberg’s by piecewise ﬁtting of trigonometric functions [67,68] and was later reﬁned to add an
exponential recovery phase at the instant of closure [66]. The LF model is deﬁned for one pitch cycle as,
uLF(m) =
A1eα1m sin (πm/M1) for m ≤M2 −1,
A2

1 −e−α2(m−M)
for M2 ≤m ≤M −1.
(34.34)
The seven parameters, A1, A2, α1, α2, M1, M2, and M determine the model and are constrained by
requiring uLF(t) to be: (1) continuous at M2 and (2) sum to zero on the interval zero to M −1 to ensure
that uG(0) = uG(M −1). The glottal volume velocity and its derivative for the LF model are shown in
Figure 34.16.
Some work has been done on joint estimation of voice source parameters and the vocal tract system
[69] but most methods estimate the voice source signal ﬁrst by using inverse ﬁltering, as described in
Section 4.34.3 and then determine the parameters of these models using a least-square ﬁt [62,70].
Plumpe et al. [21] observed that the residual produced by subtracting the synthetic signal produced
by the LF model from the voice source signal contained important features dubbed as the “ﬁne structure”
of the voice source signal. The two main features causing the deviation of the LF model from the voice
source signals were identiﬁed as a ripple in the open phase, caused by the nonlinear coupling between
the vocal tract and the glottal ﬂow and aspiration noise caused by turbulent ﬂow.
4.34.5.2 Data-driven analysis of the voice source
The aim of data-driven analysis is to identify suitable components to represent the voice source signal.
Some of the current work on data-driven analysis is based on segmenting the voice source signal using
GCI detection [55] so that every pair of pitch cycles can be resampled to a ﬁxed size to form a data
matrix. Each vector in the data matrix is therefore a resampled version of the pair of pitch cycles in

1014
CHAPTER 34 Speech Production Modeling and Analysis
0
0.2
0.4
0.6
0.8
g(m)
0
1
2
3
4
5
6
−10
−5
0
Time [ms]
g′(m)
FIGURE 34.16
The glottal ﬂow velocity g(m) and its derivative for the LF model uLF (m).
the voice source signal. If u(n) is the voice source signal, ni−1, ni, and ni+1 are the time samples of the
preceding, current and succeeding GCI time instants and ni = ni+1 −ni−1, then each vector in the
data matrix is deﬁned as,
ui =↕β
α Au(n),
n ∈

ni −ni
2 , . . . , ni + ni
2

,
(34.35)
where A is an appropriate rescaling factor and ↕β
α denotes a resampling operation of factor β/α.
Thomas et al. [71] performed clustering on the mel-frequency cepstrum coefﬁcients calculated for
each vector in the data matrix. The clustering also produced posterior probabilities for each data vector
allowing for prototype voice source vectors to be produced for each cluster. This was used for low-
frequency artiﬁcial bandwidth extension in [72].
Further development of treating the data matrix was presented in [73,74]. An optimal projection
onto a lower dimensional space was derived using principal component analysis (PCA). This gives rise
an approximation of the voice source,
ˆui =
K
	
k=1
zi,kvk + ¯u,
(34.36)
which minimizes the mean square error between ˆui and ui for a ﬁxed number of PCA components, K.
The zi,k’s are the PCA spectra from component k, vk is the kth eigenvector of the sample covariance
matrix of the data matrix and ¯u is the sample mean vector. K can be can be reduced leading to ever
coarser approximation of ui.

4.34.5 Voice Source Modeling
1015
FIGURE 34.17
Approximation using principal component analysis and the LF model. The gray line shows the voice source
waveform ui, the approximation of the voice source ˆui is shown by the black thin line for the PCA and broken
black line for the LF model.
Figure 34.17 shows the two cycles of the voice source signal approximated using K = 16 PCA
components. The PCA approximation (black thin line) is encoded by zi,k with k = 1, 2, . . . , K. The
ﬁgure also shows an approximation using the LF model (black thin broken line) using least-squares to
determine the parameters.
Berezina et al. [37] provided another example of data-driven modeling of the voice source signal
where the voice source is modeled using wavelet basis functions. The AR model parameters and the
voice source parameters are estimated simultaneously.
Relevant theory: Signal Processing Theory
See Vol. 1, Chapter 2 Continuous-Time Signals and Systems
See Vol. 1, Chapter 3 Discrete-Time Signals and Systems
See Vol. 1, Chapter 11 Parameter Estimation

1016
CHAPTER 34 Speech Production Modeling and Analysis
References
[1] G. Fant, Acoustic Theory of Speech Production, Mouton, The Hague, The Netherlands, 1960.
[2] J.L. Flanagan, Speech Analysis, Synthesis and Perception, second ed., Springer-Verlag, New York, 1972.
[3] L.R. Rabiner, R.W. Schafer, Digital Processing of Speech Signals, Prentice-Hall, Englewood Cliffs, New
Jersey, USA, 1978.
[4] J.R. Deller, J.H.L. Hansen, J.G. Proakis, Discrete-Time Processing of Speech Signals, IEEE Press, 1993.
[5] B. Gold, N. Morgan, Speech and Audio Signal rocessing, John Wiley and Sons, Inc., New York, 2000.
[6] G.O. Russell, The Vowel: Its Physiological Mechanism as Shown by X-ray, Ohio State University Press,
Columbus, 1928.
[7] H. Dudley, R. Riesz, S. Watkins, A synthetic speaker, J. Franklin Inst. 227 (1939) 739–764.
[8] H.K. Dunn, The calculation of vowel resonances, and an electrical vocal tract, J. Acoust. Soc. Am. 22 (1950)
740–753.
[9] K. Stevens, S. Kasowski, G. Fant, An electrical analog of the vocal tract, J. Acoust. Soc. Am. 25 (1953)
734–742.
[10] E.S. Weibel, Vowel synthesis by means of resonant circuits, J. Acoust. Soc. Am. 27 (1955) 858–865.
[11] R.L. Miller, Nature of the vocal cord wave, J. Acoust. Soc. Am. 31 (1959) 667–677.
[12] P.B. Carr, D. Trill, Long-term larynx-excitation pectra, J. Acoust. Soc. Am. 36 (1964) 2033–2040.
[13] J. Lindqvist-Gaufﬁn, Studies of the voice source by means of inverse ﬁltering, STL-QPSR 6 (1965) 8–13.
[14] B.S. Atal, S.L. Hanauer, Speech analysis and synthesis by linear prediction of the speech wave, J. Acoust.
Soc. Am. 50 (1971) 637–655.
[15] D.S. Jurafsky, J.H. Martin, Speech and Language Processing, Prentice-Hall, New Jersey, 2000.
[16] J. Makhoul, Linear prediction: a tutorial review, Proc. IEEE 63 (1975) 561–580.
[17] Y. Stylianou, Synchronization of speech frames based on phased data with application to concatenative
speech synthesis, in: Proc. 6th Eur. Conf. Speech Communication and Technology, vol. 5, Budapest, Hungary,
pp. 2343–2346.
[18] T. Drugman, G. Wilfart, A. Moinet, T. Dutoit, Using a pitch-synchronous residual codebook for hybrid
hmm/frame selection speech synthesis, in: Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing
(ICASSP), Taipei, Taiwan.
[19] Y. Stylianou, O. Cappe, E. Moulines, Continuous probabilistic transform for voice conversion, IEEE Trans.
Speech Audio Process 6 (1998) 131–142.
[20] M.R.P. Thomas, J. Gudnason, P.A. Naylor, Application of the DYPSA algorithm to segmented time-scale
modiﬁcation of speech, in: Proc. European Signal Processing Conf. (EUSIPCO), Lausanne, Switzerland.
[21] M.D.
Plumpe,
T.F.
Quatieri,
D.A.
Reynolds,
Modeling
of
the
glottal
ﬂow
derivative
wave-
form with application to speaker identiﬁcation, IEEE Trans. Speech Audio Process 7 (1999)
569–576.
[22] J. Gudnason, M. Brookes, Voice Source cepstrum coefﬁcients for speaker identiﬁcation, in: Proc. IEEE Intl.
Conf. on Acoustics, Speech and Signal Processing (ICASSP), pp. 4821–4824.
[23] A. Tsanas, M. Little, P. McSharry, J. Spielman, L. Ramig, Novel speech signal processing algorithms for
high-accuracy classiﬁcation of parkinson’s disease, IEEE Trans. Biomed. Eng. 59 (2012)1264–1271.
[24] T. Quatieri, N. Malyska, Vocal-source biomarkers for depression: a link to psychomotor activity, in: Inter-
speech.
[25] N. Levinson, The wiener rms error criterion in ﬁlter design and prediction, J. Math. Phys 25 (1947) 261–278.
[26] J. Durbin, The ﬁtting of time series models, Rev. Inst. Int. Stat. 28 (1960) 233–243.
[27] H. Wakita, Direct estimation of the vocal tract shape by inverse ﬁltering of acoustic speech waveforms, IEEE
Trans. Audio Electroacoust. AU-21 (1973) 417–427.

References
1017
[28] N. Wiener, Extrapolation, Interpolation and Smoothing of Stationary Time Series With Engineering Applica-
tions, M.I.T. Press, Cambridge, Mass, 1949.
[29] G.U. Yule, On a method of investigating periodicities in disturbed series, with special reference to wolfer’s
sunspot numbers, Phil. Trans. Roy. Soc 226-A (1927) 267–298.
[30] S. Kay, Modern Spectral Estimation, ﬁrst ed., Prentice Hall, 1988.
[31] B. Porat, A Course in Digital Signal Processing, John Wiley and Sons, 1997.
[32] S. Haykin, Adaptive Filter Theory, in: Prentice-Hall Information and System Sciences Series, Prentice Hall,
2002.
[33] F.J. Harris, On the use of windows for harmonic analysis with the discrete fourier transform, Proc. IEEE 66
(1978) 51–83.
[34] L. Liporace, Linear estimation of nonstationary signals, J. Acoust. Soc. Am. 58 (1975) 1288–1295.
[35] A. Harma, M. Juntungen, J.P. Kaipio, Time-varying autoregressive modeling of audio and speech signals, in:
Proc. EUSIPCO, Tampere Finland.
[36] K. Schnell, A. Lacroix, Time-varying linear prediction for speech analysis and synthesis, in:
IEEE
International
Conference
on
Acoustics,
Speech
and
Signal
Processing,
ICASSP
2008,
pp. 3941–3944.
[37] M. Berezina, D. Rudoy, P.J. Wolfe, Autorecressive modeling of voiced speech, in: Proc. IEEE Intl. Conf. on
Acoustics, Speech and Signal Processing (ICASSP).
[38] D. Rudoy, T.F. Quatieri, P.J. Wolfe, Time-varying autoregressions in speech: Detection theory and applications,
IEEE Trans. Acoust., Speech, Signal Process 19 (2011) 977–989.
[39] H.W. Strube, Determination of the instant of glottal closure from the speech wave, J. Acoust. Soc. Am. 56
(1974) 1625–1629.
[40] D.Y.Wong,J.D.Markel,J.A.H.Gray,Leastsquaresglottalinverseﬁlteringfromtheacousticspeechwaveform,
IEEE Trans. Acoust. Speech Signal Process 27 (1979) 350–355.
[41] T. Drugman, B. ozkurt, T. Dutoit, Complex cepstrum-based decomposition of speech for glottal source esti-
mation, in: Interspeech09, Brighton, UK.
[42] P. Alku, Glottal wave analysis with pitch synchronous iterative adaptive ﬁltering, Speech Commun. 11 (1992)
109–118.
[43] J.G. McKenna, Automatic glottal closed-phase location and analyis by Kalman ﬁltering, in: 4th ISCA Tutorial
and Research Workshop on Speech Synthesis.
[44] C. Ma, Y. Kamp, L.F. Willems, A Frobenius norm approach to glottal closure detection from the speech signal,
IEEE Trans. Speech Audio Process 2 (1994) 258–265.
[45] C.R. Jankowski, Jr., T.F. Quatieri, D.A. Reynolds, Measuring ﬁne structure in speech: Application to speaker
identiﬁcation, in: Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP), pp. 325–328.
[46] V.N. Tuan, C. d’Alessandro, Robust glottal closure detection using the wavelet transform, in: Eurospeech,
Budapest, pp. 2805–2808.
[47] J.L. Navarro-Mesa, E. Lleida-Solano, A. Moreno-Bilbao, A new method for epoch detection based
on the Cohen’s class of time frequency representations, IEEE Signal Process Lett. 8 (2001)
225–227.
[48] Y.M. Cheng, D. O’Shaughnessy, Automatic and reliable estimation of glottal closure instant and period, IEEE
Trans. Acoust., Speech, Signal Process 37 (1989) 1805–1815.
[49] R. Smits, B. Yegnanarayana, Determination of instants of signiﬁcant excitation in speech using group delay
function, IEEE Trans. Speech Audio Process 5 (1995) 325–333.
[50] B. Yegnanarayana, R. Smits, A robust method for determining instants of major excitations in voiced speech,
in: Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP), pp. 776–779.
[51] P.S. Murthy, B. Yegananarayana, Robustness of group-delay-based method for extraction of signiﬁcant instants
of excitation from speech signals, IEEE Trans. Speech Audio Process 7 (1999) 609–619.

1018
CHAPTER 34 Speech Production Modeling and Analysis
[52] D.M. Brookes, H.P. Loke, Modelling energy ﬂow in the vocal tract with applications to glottal closure
and opening detection, in: Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP),
pp. 213–216.
[53] M. Brookes, P.A. Naylor, J. Gudnason, A quantitative assessment of group delay methods for identifying
glottal closures in voiced speech, IEEE Trans. Speech Audio Process 14 (2006).
[54] K.S.R. Murty, B. Yegnanarayana, Epoch extraction from speech signals, IEEE Trans. Audio Speech Lang.
Process 16 (2008) 1602–1613.
[55] M.R.P. Thomas, J. Gudnason, P.A. Naylor, Estimation of glottal closing and opening instants in voiced speech
using the yaga algorithm, IEEE Trans. Acoust. Speech Signal Process 20 (2012) 82–91.
[56] P.A. Naylor, A. Kounoudes, J. Gudnason, M. Brookes, Estimation of glottal closure instants in voiced speech
using the DYPSA algorithm, IEEE Trans. Speech Audio Process 15 (2007) 34–43.
[57] T. Drugman, M. Thomas, J. Gudnason, P. Naylor, T. Dutoit, Detection of glottal closure instants from speech
signals: a quantitative review, IEEE Trans. Audio Speech Lang. Process 20 (2012) 994–1006.
[58] T. Drugman, T. Dutoit, Glottal closure and opening instant detection from speech signals, in: Interspeech.
[59] A.E. Rosenberg, Effect of glottal pulse shape on the quality of natural vowels, J. Acoust. Soc. Am. 49 (1971)
583–590.
[60] D.H. Klatt, L.C. Klatt, Analysis, synthesis and perception of voice quality variations among female and male
talkers, J. Acoust. Soc. Am. 87 (1990) 820–857.
[61] H. Fujisaki, M. Ljungqvist, Estimation of voice source and vocal tract parameters based on ARMA analysis
and a model for the glottal source waveform, in: Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal
Processing (ICASSP), vol. 12, 1987, pp. 637–640.
[62] D.M. Brookes, D.S. Chan, Speaker characteristics from a glottal airﬂow model using glottal inverse ﬁltering,
Proc. Institute of Acoustics 15 (1994) 501–508.
[63] K.E. Cummings, M.A. Clements, Glottal models for digital speech processing—a historical survey and new
results, vol. 5, pp. 21–42.
[64] P. Alku, T. Backstrom, Normalized amplitude quotient for parametrization of the glottal ﬂow, J. Acoust. Soc.
Am. 112 (2002) 701–710.
[65] T. Backstrom, P. Alku, E. Vilkman, Time–domain parameterization of the closing phase of glottal airﬂow
waveform from voices over a large intensity range, IEEE Trans. Speech Audio Process 10 (2002) 186–192.
[66] G. Fant, J. Liljencrants, Q. Lin, A four-parameter model of glottal ﬂow, STL-QPSR 26 (1985) 1–13.
[67] G. Fant, Vocal source analysis, a progress report, in: STL-QPSR 3–4, Department of Speech, Music and
Hearing, KTH, http://www.speech.kth.se, 1979, pp. 31–53.
[68] G. Fant, J. Liljencrants, Perception of vowels with truncated intraperiod decay envelopes, in: STL-QPSR 1,
Department of Speech, Music and Hearing, KTH, http://www.speech.kth.se, 1979, pp 79–84.
[69] P. Milenkovic, Glottal inverse ﬁltering by joint estimation of an AR system with a linear input model, IEEE
Trans. Acoust. Speech Signal Process 34 (1986) 28–42.
[70] H. Strik, B. Cranen, L. Boves, Fitting LF-model to inverse ﬁltered signals, in: Eurospeech, vol. 1, Berlin,
pp. 103–106.
[71] M.R.P. Thomas, J. Gudnason, P.A. Naylor, Data-driven voice source waveform modelling, in: Proc. IEEE Intl.
Conf. on Acoustics, Speech and Signal Processing (ICASSP), Taipei, Taiwan.
[72] M.R.P. Thomas, B. Geiser, J. Gudnason, P.A. Naylor, P. Vary, Voice source estimation for artiﬁcial band-
width extension of telephone speech, in: Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing
(ICASSP), Dallas, USA.
[73] J. Gudnason, M.R.P. Thomas, P.A. Naylor, D.P.W. Ellis, Voice source waveform analysis and synthesis using
principal component analysis and gaussian mixture modelling, in: Proc. Interspeech Conf., Brighton, UK.
[74] J. Gudnason, M.R.P. Thomas, D.P.W. Ellis, P.A. Naylor, Data-driven voice source analysis and synthesis,
Speech Commun. 52 (2012) 199–211.

35
CHAPTER
Enhancement
Mike Brookes and Nikolay D. Gaubitch
Department of Electrical and Electronic Engineering, Imperial College, Exhibition Road, London, UK
4.35.1 Introduction
This chapter gives an overview of algorithms that are used to enhance a speech signal that has been
degraded in some way. The chapter begins by describing some of the mechanisms that result in degraded
speech and then goes on to present a survey of enhancement methods; these have been grouped into
families according to the type of processing they perform. Several of the enhancement methods make
use of common core processing blocks including voice activity detection and noise spectrum estimation;
these enabling algorithms are described separately after the main discussion of enhancement methods.
Finally, the chapter describes both subjective and objective methods of evaluating the intelligibility and
quality of enhanced speech.
A typical speech transmission chain is illustrated in Figure 35.1. In the acoustic domain, the wanted
speech signal passes through an acoustic channel to arrive at the microphone which also receives
signals from other unwanted acoustic noise sources. Following the microphone, the signal is ampliﬁed
and passes through an electronic channel to arrive at a loudspeaker where it is converted back into sound.
Degradations to the speech signal may be introduced at any point in this chain and it is convenient to
categorize these into three groups according to the way in which they alter the wanted speech signal.
1. Additive noise that is uncorrelated with the wanted speech signal may arise in either the acoustic
or electronic domain. Its perceived effect is to degrade listenability and intelligibility and may,
in extreme cases, completely mask the wanted signal. For some types of additive noise, the
spectral characteristics are stationary or change slowly with time. This is typically true of hum
and ampliﬁer noise as well as of some environmental acoustic noise sources. Time-frequency gain
modiﬁcation (Section 4.35.2.4) and and single-channel adaptive ﬁltering (Section 4.35.2.3.4) have
been successful in reducing the perceived level of such stationary noise sources. Other forms of
additive noise are intermittent or highly non-stationary and their identiﬁcation and deletion is
the subject of model-based and missing data methods. Such non-stationary noise sources include
media interference, unwanted co-talkers and some forms of electrical interference.
2. Convolutive effects are perceived as reverberation and poor spectral balance; they differ from the
previous group because the added noise is strongly correlated with the wanted signal. Reverbera-
tion and echo normally arise from acoustic reﬂections and can seriously degrade intelligibility. The
increasing use of distant microphones in hands-free telephony has prompted extensive research
into reducing the effects of reverberation and these are discussed in Chapter 18. Bandwidth
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396501-1.00035-2
© 2014 Elsevier Ltd. All rights reserved.
1019

1020
CHAPTER 35 Enhancement
FIGURE 35.1
Typical speech transmission chain.
restrictions and uneven spectral response may arise from microphone placement, microphone
characteristics and Coder-Decoder (CODEC) limitations.
3. Non-linear distortion frequently arises from amplitude limiting or clipping in the microphone,
ampliﬁer or CODEC. This is perceived as harsh distortion that varies with the signal amplitude.
A similar perceptual effect can result from high bit error rates in the coded signal used by some
CODECs. Clipped portions of a waveform are easy to identify provided that no subsequent
phase distortion is present and techniques exist for reconstruction of the corrupted portions of the
waveform.
4.35.2 Speech enhancement methods
Table 35.1 gives a summary of the speech enhancement algorithms that are discussed in this chapter.
The algorithms have been grouped into families according to the type of processing that they apply
to the speech signal. For each algorithm the table also shows the type or types of degradation that are
targeted by the algorithm, the structure of the algorithm and the domain in which it operates.
ThethreetypesofdegradationwereidentiﬁedinSection4.35.1andareadditivenoise(A),convolutive
effects (C) and non-linear distortions (X).
The algorithm structure depends on how the characteristics of the algorithm are controlled; in feed-
forward algorithms (F) they are controlled by the degraded signal, in feedback algorithms (B) by the
processed output signal and in open-loop algorithms (O) they are ﬁxed in advance and are unaffected
by the signal itself.
The signal may be processed directly in the time domain (T) or it may ﬁrst be divided into a small
number of frequency bands (D). Many algorithms achieve partial separation of speech and noise by
transforming the noisy signal into the frequency domain (Q), the power spectral domain (P) or, at
considerably greater computational cost, the signal-dependent Karhunen-Loève domain (K).

4.35.2 Speech Enhancement Methods
1021
Table 35.1 Summary of Algorithm Attributes. Targets: A = additive noise, C = coloration/
convolution, X = nonlinearities; Structures: B = feedback, F = feedforward, O = open-loop;
Domains: D = sub-band. K = Karhunen-Loève, P = power spectrum, Q = frequency, T = time
Section
Family
Algorithm
Target
Structure
Domain
4.35.2.1
Noise cancelation
A
OF
T
4.35.2.2
Static ﬁlter
AC
O
T
4.35.2.3.1 Adaptive ﬁlter
Adaptive notch
A
FB
T
4.35.2.3.1
Adaptive comb
A
FB
T
4.35.2.3.2
Channel compensation
C
F
TP
4.35.2.3.3
Target spectrum
C
F
P
4.35.2.3.4
Single channel adaptive ﬁlter AC
B
T
4.35.2.3.5
Subspace
A
F
K
4.35.2.3.6
Kalman ﬁlter
A
B
T
4.35.2.4
Time-frequency gain manipulation Spectral subtraction
A
F
P
4.35.2.4
MMSE
A
F
P
4.35.2.6
Temporal gain
ALC
G
F
T
4.35.2.6
Compress
G
F
TDQ
4.35.2.6
Limit
A
F
T
4.35.2.6.1
Expander
A
F
TDQ
4.35.2.6.1
Noise gate
A
F
TDQ
4.35.2.6.2
DNR
A
F
TDQ
4.35.2.7.1 Excision and interpolation
Anti-clip
X
F
TDQ
4.35.2.7.2
Anti-click
A
F
TDQ
4.35.2.7.2
Anti-dropout
X
F
TDQ
4.35.2.7.3
Binary mask
A
F
P
4.35.2.1 Noise cancelation
If the waveform of an additive interfering signal is known accurately, it can be subtracted from the noisy
signal to give perfect cancellation. Situations in which noise cancelation can successfully be applied,
include interference from:
1. a signal that can be synthesised accurately because its characteristics are known.
2. a signal that can be estimated from one or more previous examples that occurred when there was
no speech present.
3. an identiﬁable media feed or other accessible sound source.
In many cases the interfering signal may be subject to a transfer function that includes an unknown
time delay, unknown gain and/or unknown ﬁltering. It is possible to determine the optimum values to
use for these from the peak of the cross-correlation between the interference prototype and the noisy
signal or, more generally, from the results of a system identiﬁcation procedure.

1022
CHAPTER 35 Enhancement
In case 3, the media signal will normally have passed through a separate acoustic channel which can
be estimated by means of a 2-channel adaptive ﬁlter provided that this channel does not vary rapidly
over time [1].
Provided that the enhancer has precise prior knowledge of the interfering signal, it is possible to
obtain very large improvements in SNR. By the same token, if the prior knowledge is incorrect, any
attempt at noise cancelation is likely to introduce new erroneous components into the enhanced signal.
4.35.2.2 Static ﬁltering
Static ﬁltering, or Equalisation (EQ), involves applying to the signal a frequency dependent gain whose
characteristics are set by the operator rather than being controlled adaptively by the signal. Since the
ﬁlter characteristics remain constant, it is a linear process and does not introduce any signal distortion.
Static ﬁltering can be used:
1. to remove unwanted portions of the spectrum in which the SNR is known or expected to be poor.
Examples are the use of
a. a high-pass ﬁlter to remove low-frequency additive noise
b. a low-pass ﬁlter to remove hiss
c. a notch ﬁlter to remove tonal interference of known frequency. However, unless the fre-
quency of the tonal interference is known precisely, it will normally be more effective to
use the adaptive ﬁltering techniques described in Section 4.35.2.3.
2. to correct for any unwanted ﬁltering that was part of the acquisition process. An example is the
correction of resonances arising from microphone placement.
3. to amplify portions of the spectrum in which the target source has low energy. An example is the
application of a pre-emphasis spectral tilt to boost the high frequencies.
Low order ﬁlters may be applied in the time domain, but it is often computationally more efﬁcient
to implement high order ﬁlters either in subbands or in the frequency domain using the techniques
described in Section 4.35.3.1.
The advantage of static ﬁltering is that it is easily understood, is entirely under the control of the
operator and is a linear process. For these reasons, its effects are predictable and it avoids the risk of
adding artefacts to the signal.
Filters whose gain changes very rapidly with frequency (e.g., sharp cut-off high-pass or low-pass
ﬁlters) are likely to introduce ringing artefacts into the signal, i.e., an abrupt transition in the input signal
will trigger a resonance in the ﬁlter for an extended time interval.
Filters whose parameters are adjusted manually by the operator fall into three groups:
1. Filters with a ﬁxed menu of preset frequency responses.
2. Filters with a small number of parameters designed for speciﬁc purposes. These include high-
pass, low-pass, band-pass and band-stop which are speciﬁed by one or two cutoff frequencies and,
in some cases, roll-off slopes. Also in this category are notch or comb ﬁlters which are deﬁned
by the fundamental frequency of the unwanted tone or buzz.

4.35.2 Speech Enhancement Methods
1023
3. Filters that include a large enough number of adjustable parameters that they can approximate an
arbitrary response. These include graphic equalisers and ﬁlters in which the operator draws the
desired response graphically.
A disadvantage of the third group of ﬁlters is that can be difﬁcult for an operator to determine the
optimum ﬁlter settings from the large number of available options. A way around this difﬁculty is to
establish the initial settings of the parameters by using any of the methods described in Section 4.35.2.3
to determine a suggested ﬁlter response. These initial settings can then by adjusted if necessary by the
operator. This approach achieves many of the advantages of an adaptive ﬁlter while retaining those of
a ﬁxed ﬁlter.
4.35.2.3 Signal dependent ﬁltering
Signal-dependent ﬁltering techniques usually operate in the time domain although they may be imple-
mented in the frequency domain to reduce computational costs. Since the applied ﬁlter generally varies
only slowly with time, these techniques are almost linear and do not therefore introduce noticeable arte-
facts into the signal. They differ from the static ﬁlters discussed in Section 4.35.2.2 in that the applied
ﬁlter depends on the noisy signal itself in one of two ways:
Feedforward In this case features extracted from the noisy signal are used to determine the ﬁlter
characteristics.
Feedback In this case the ﬁlter characteristics are automatically adjusted in order to optimize some
measured characteristic of the output from the ﬁlter (e.g., to minimize the average power of an error
signal).
4.35.2.3.1
Adaptive notch and comb ﬁlters
The terms “hum,” “buzz,” and “tonal” noise all refer to periodic interference, most commonly arising
from an electrical source. The terms do not have precise deﬁnitions but “hum” is normally used for low
frequency periodic noise, typically at a multiple of the 50 Hz or 60 Hz mains frequency. A “tone” is
a sinusoidal signal with only a single frequency component while “buzz” is periodic noise containing
perceptible high frequency energy at multiples of the fundamental frequency.
These noise sources can be removed using a ﬁlter with notches at the harmonics of the fundamental
frequency of the interference. If the frequency is known precisely in advance, it is possible to use a
static ﬁlter (Section 4.35.2.2) but if not, it is easier and more effective to use a ﬁlter that automatically
tracks the frequency of the interference. To eliminate a single tone, you require an “adaptive notch ﬁlter”
while to eliminate an entire harmonic series you use instead an “adaptive comb ﬁlter.” The advantage
of these ﬁlters over other techniques is that they can be made very selective and are able to remove
the interference almost completely while having little effect on the wanted signal. Since the approach
involves a slowly varying ﬁlter, it is almost perfectly linear and will not introduce artefacts into the
wanted signal.
Because they have stationary spectral characteristics, periodic noises can be detected easily with the
noise estimation techniques described in Section 4.35.3.3.2 and can be effectively suppressed using
time-frequency gain modiﬁcation methods (Section 4.35.2.4). However, these alternative methods will
not usually have such ﬁne spectral resolution as an adaptive ﬁlter and are also non-linear and more
likely to introduce artefacts.

1024
CHAPTER 35 Enhancement
4.35.2.3.2
Channel compensation
On its way to a listener, the wanted speech signal passes through a channel comprising the acoustic
response between speaker and microphone, the response of the microphone and associated ampliﬁer
and the response of the transmission system. Applying an equalization ﬁlter that is the inverse of this
channel will counteract its effects and make the speech sound more natural.
In some cases the channel can be measured directly by passing a known signal through it, but in
many circumstances it is unknown and may vary with time. Compensating for the channel therefore
involves two distinct stages: (a) estimating the response of the unknown channel and (b) devising a ﬁlter
to compensate for its effects.
If the compensating ﬁlter is constructed to be the exact inverse of the estimated channel, it will include
a very high gain at frequencies whose estimated channel gain is very low. This causes two difﬁculties:
(a) any noise at these frequencies will be greatly ampliﬁed and (b) small errors in the channel gain
estimate will give rise to very large errors in the gain of the compensating ﬁlter. The design of an
equalization ﬁlter will normally incorporate constraints, such as a limit on the maximum gain, in order
to take account of these issues. Channel compensation techniques are widely used as preprocessing
stages for speech recognition but less common for speech enhancement. In [2] a channel compensating
ﬁlter is determined from Autoregressive (AR) models both of the acoustic channel and of the speech
signal. In an alternative approach, [3,4] identiﬁes the channel response by assuming that the speech
follows the Long Term Average Speech Spectrum (LTASS) [5].
4.35.2.3.3
Equalisation to a target spectrum
An alternative to estimating the channel and then applying a compensating ﬁlter is to equalize the signal
so that its average power spectrum has a predetermined shape. This target shape can be the LTASS or
it may include additional gain at high frequencies to compensate for the overall spectral tilt of speech.
The advantage of this approach is that it is not necessary to estimate the channel explicitly.
4.35.2.3.4
Single channel adaptive ﬁltering
In an adaptive ﬁlter the output of a ﬁlter is compared with a target signal and the ﬁlter coefﬁcients are
adjusted to minimize the power of the difference between them. In a Single Channel Adaptive Filter
(SCAF), the target is the same signal as the input to the ﬁlter itself, as illustrated in Figure 35.2. This
structure can be used to identify components of a signal that are correlated with previous samples; such
correlations arise either from periodicity in the speech or the noise or else from channel effects such as
echoes or reverberation.
By varying its parameters, a single channel adaptive ﬁlter can be used in several ways. In all cases it
is used to distinguish between correlated and uncorrelated components of a signal. It can therefore be
used to reject unwanted periodic noise components, to accept wanted periodic speech components or to
reject reverberation (since it is correlated with the wanted speech signal). The behavior is controlled by
the portion of the signal that is processed by the adaptive ﬁlter (selected by a combination of the delay
parameter and the length of the ﬁlter) and by the rate at which the ﬁlter is allowed to adapt. A SCAF
is especially effective at removing predictable interference such as strongly tonal additive noise (e.g.,
hum and buzz) whose frequency varies only slowly.

4.35.2 Speech Enhancement Methods
1025
x(n)
x(n–D)
W(z)
y(n)
s(n)
e(n)
z–D
α
α
1–
Σ
Σ
∧
FIGURE 35.2
Block diagram for a single channel adaptive ﬁlter (SCAF).
The advantage of a SCAF over the Time-Frequency Gain Modiﬁcation (TFGM) techniques described
in Section 4.35.2.4, which are also effective in removing tonal noise components, is that musical noise
artefacts are not introduced and that, if a long ﬁlter is used, it can be more frequency selective. In
addition, a SCAF is able to detect signal correlations that cover a longer time interval than is normally
used in TFGM.
The general structure of a SCAF is shown in Figure 35.2. The noisy speech signal, x(n), is delayed
by D samples and passed through the ﬁlter to give y(n) which is subtracted from x(n) to generate the
error signal e(n) from which the signal correlations have been removed. The ﬁlter response is adjusted
via the feedback path to reduce the power of e(n) and the output of the ﬁlter is formed as a mixture of
e(n) and y(n) according to whether the periodic signal components should be enhanced or suppressed.
For a stationary input signal x(n), the impulse repsonse, w(n), of the ﬁlter that minimizes the power of
e(n) is given by
w = R−1g,
(35.1)
where Ri, j = ⟨x(n)x(n + i −j)⟩and gi = ⟨x(n + i)x(n −D)⟩for i, j ≥0 with ⟨· · · ⟩denoting the
expected value. It is possible for the delay, D, to be negative but in this case we must set w( −D) = 0
and must also introduce a delay of D samples in the input to ensure that the system is realisable. The
frequency resolution of the ﬁlter is approximately equal to the reciprocal of its impulse response length.
The ﬁlter adaptation is most often performed on the w(n) directly using either the Least Mean Square
(LMS) or, more commonly, the Normalized Least Mean Squares (NLMS) gradient descent algorithms
[6]. The LMS and NLMS algorithms can be made “leaky” by making the coefﬁcients decay to zero over
time [7]; this improves the performance with non-stationary inputs such as speech signals. Alternatively,
and at considerably greater computational cost, it is possible to use the Recursive Least Squares (RLS)
or sliding-window RLS algorithms [6] which recursively solve Eq. (35.1) exactly. The computation
can be reduced by processing the signal in blocks and evaluating (35.1) only once per block. It is also
possible to implement the adaptive ﬁlter using a lattice structure and to adapt the lattice coefﬁcients
using gradient descent resulting in a ﬁlter with improved convergence properties [8,9]. It is attractive
to use Inﬁnite Impulse Response (IIR) instead of Finite Impulse Response (FIR) ﬁlters since they can
generate narrow frequency notches with only a small number of coefﬁcients. However it is difﬁcult to
ensure their stability for ﬁlter orders greater than two and so they are rarely used.
The use of adaptive ﬁlters for noise reduction was introduced by [1]. Although the primary concern
of the authors was with two-channel adaptive ﬁltering, in which a separate noise reference signal is

1026
CHAPTER 35 Enhancement
available, they also discuss two applications of the single channel conﬁguration shown in Figure 35.2.
In the ﬁrst of these, the removal of periodic noise from a broadband signal, the output mixing factor is
α = 1 and the delay D is set to be long enough so that s(n) and s(n + D) are uncorrelated. Their second
application is the reverse of the ﬁrst and aims to remove broadband noise from a periodic signal; in
this case setting α = 0 to give ˆs(n) = y(n). A complication of using single channel adaptive ﬁlters for
speech enhancement is that both periodic and broadband components are often present in both speech
and noise; it is therefore necessary to select the parameters of the adaptive ﬁlter carefully to enhance
only the wanted components.
To enhance the periodic component of voiced speech, [10] set α = 0 and used an external pitch
detector to set the delay D to equal a pitch period. A Voice Activity Detector (VAD) was used to
detect the presence of speech and to inhibit adaptation whenever speech was absent. With white noise
interference (the best case), an improvement of 7 dB in SNR was reported for an input SNR of 0 dB. A
similar approach was adopted in [11] incorporating an adaptive pitch tracker based on a 3-tap adaptive
ﬁlter with adaptation in the z-plane. In [12] a negative delay, D = −8, was used with a ﬁlter length of
17 samples. The parameter α was set according to whether they wished to remove broadband (α = 0)
or periodic (α = 1) noise and a VAD was again used to to determine when to adapt the coefﬁcients. A
method for adaptively choosing the step size of the NLMS algorithm was described in [13] where α is
set in the range 0 ≤α ≤0.5. It also suggests that improved enhancement can be obtained by cascading
two adaptive ﬁlters.
In order to remove narrow-band (periodic) noise, [14] sets α = 1. It too uses a VAD to detect
the presence of speech, but now permits adaptation only when speech is absent. The authors observe
that the adaptive ﬁlter can give a high gain at frequencies where x(n) does not contain any correlated
components. They therefore adopted an indirect approach in which a conventional two-channel adaptive
ﬁlter was used with x(n) as the interfering noise and a white noise signal as the desired response.
The adapted ﬁlter coefﬁcients were then transferred periodically to a second ﬁlter which was applied
to the input signal x(n). Other authors have suggested the use of “leaky” NLMS to overcome this
problem.
In order to suppress both periodic and broadband noise [15], propose the cascade of three adaptive
ﬁlters with different parameters that is illustrated in Figure 35.3. The ﬁrst, with a long adaptation time-
constant identiﬁes stationary periodic noise such as that arising from hum or machinery, the second
identiﬁes the periodic components of voiced speech while the ﬁnal ﬁlter subtracts both periodic and
broadband noise from the input signal while retaining the components identiﬁed as voiced speech by the
second ﬁlter. A slightly modiﬁed system is described in [16] which uses a variable step size to reduce
the adaptation rate of the ﬁnal ﬁlter when speech is present.
4.35.2.3.5
Subspace enhancement
One use of a static or adaptive ﬁlter is to suppress regions of the spectrum in which the Signal-to-Noise
Ratio (SNR) is so low that little or no useful information is available. Subspace enhancement methods
extend this idea but, instead of applying the ﬁlter in the time or frequency domain, they use Karhunen-
Loève Transform (KLT) to transform the signal into a domain in which the signal energy is concentrated
into a small number of components and is therefore easier to distinguish from the interference.

4.35.2 Speech Enhancement Methods
1027
FIGURE 35.3
Three cascaded adaptive ﬁlters used to distinguish between periodic noise and voiced speech (adapted from
[15]).
The target of this approach is broadband additive noise that does not have strong tonal components
(which it will normally interpret as speech components). The advantage over both a SCAF (Section
4.35.2.3.4) and and the TFGM approaches (Section 4.35.2.4) is that there is no requirement for the noise
to be stationary as it is reestimated for each time frame.
The disadvantages of this approach are its high computational cost and the difﬁculty in estimating
the appropriate transform for noisy signals, particularly if they include strong tonal components. The
method can result in the introduction of musical noise artefacts.
Acoustic models of the vocal tract justify the widely used model of speech as arising from a low-order
Autoregressive (AR) process normally treated as time-invariant over intervals of around 20 ms [17]. A
consequence of this model is that the sequence of speech samples within a frame of this length lies within
a low-order subspace; the aim of subspace speech enhancement methods is to identify the subspace
and constrain the sequence of enhanced speech samples to lie within it. The approach was introduced
in [18] and became popular following [19] where an eigendecomposition of the the autocovariance
matrix of the input speech signal was used to identify the signal subspace and its complementary noise
subspace. The method assumed that the noise was white and that the autocovariance matrix of the noisy
speech therefore consisted of the sum of a low-rank matrix arising from the speech and a multiple of
the identity matrix arising from the noise. The authors presented linear estimators of the clean speech
which minimized the distortion of the speech subject to constraints on the noise power in either the time
or frequency domain. An alternative formulation, claimed to have numerical advantages and to allow
a recursive procedure, was presented in [20] and in [21] where an efﬁcient method of identifying the
noise subspace was developed.
The original approach copes awkwardly with colored noise and a development is given in [22,23]
whereasinglenon-orthogonaltransformationisusedtodiagonalizeboththespeechandnoisecovariance
matrices; this approach was subsequently generalized in [24]. An alternative approach to dealing with
colored noise, the Rayleigh Quotient method, is given in [25,26].
The subspace enhancement algorithms make an explicit compromise between signal distortion and
noise attenuation and several authors have suggested basing this compromise on perceptual models to
permit higher noise in those spectral regions where it will be masked by the speech, thereby allowing a
reduction in overall distortion [27–29].
A more recent approach that does not rely on any explicit model of noise or speech is to assume
that the speech signal lies in a low-order manifold that is embedded within a high dimension phase
space (typically of dimension 20–30). If, as in [30,31], the phase space is formed from multiple delayed
versions of the signal, then the approach is closely related to that of the previous paragraph.

1028
CHAPTER 35 Enhancement
4.35.2.3.6
Model-based ﬁltering
Model-based speech enhancement uses prior knowledge in the form of an explicit stochastic model of
speech and, in some cases, of the interfering noise. A number of different speech models are available;
the most widely used is the linear Autoregressive (AR) model, but other authors have used cepstral
coefﬁcient models, Hidden Markov Models (HMMs) and neural networks. A review of model-based
techniques is given in [32].
In [33], AR models for the speech and for the noise are used in a Kalman Filter (KF) [34] to enhance
speech. The noise model was assumed to be known in advance, while the coefﬁcients of the speech
model were estimated from the noisy speech during the enhancement procedure. The authors claimed
that the use of an AR model for the noise resulted in signiﬁcantly better performance than earlier
approaches that assumed the noise to be white.
A Kalman ﬁlter using an AR speech model is combined in [35] with one of three alternative excitation
signals: white noise, an impulse train and the Liljencrants-Fant (LF) glottal model [36]. In evaluating
the enhancement of speech with added white noise at 5 dB SNR it was found that the LF model gave
the best performance with an SNR gain over the white noise model of about 1.3 dB. This approach
differs from most model-based enhancement systems in the inclusion of an explicit voice source and
therefore requires accurate pitch tracking. The order of the AR model used in Kalman ﬁltering normally
corresponds to less than 2 ms of speech but in order to model the periodicity of voiced speech, [37] uses
a much higher order and develops an efﬁcient update procedure that takes advantage of the sparseness of
the AR coefﬁcients. Using a ﬁxed-lag KF, in which the speech is estimated using samples from the future
as well as from the past was found in [38] to give a signiﬁcant performance improvement although it
introduces a small algorithmic processing delay. Many authors use an “iterative Kalman ﬁlter” in which
the AR speech model is estimated from past samples of the enhanced speech signal but [39] found that
this approach can become unstable and suggests instead estimating the speech model from the output
of an independent enhancer based on, in their implementation, phase spectrum compensation [40].
If the speech model is a nonlinear function of past speech samples such as, for example, a neural
network, the Kalman ﬁlter cannot be used and it is necessary to use instead the Extented Kalman Filter
(EKF) or the more recently developed Unscented Kalman Filter (UKF) [41,42]. As an alternative to
Kalman ﬁltering [43], use an H∞ﬁlter which does not require an explicit noise model but instead
minimizes the worst possible effects of noise on the signal for any given noise energy. The authors
found that this approach consistently outperformed the use of a Kalman ﬁlter by up to 1 dB SNR.
Rather than modeling the speech with a deterministic AR model [44], uses instead a multi-state
probabilistic AR model in which the AR coefﬁcients in each state follow a multivariate Gaussian
distribution. An iterative Expectation-Maximization (EM) procedure is presented that simultaneously
estimates the state sequence, the AR coefﬁcients of the stationary noise spectrum, and the time-domain
enhanced speech signal. The latter is effectively obtained by a weighted average of state-dependent
Wiener ﬁlters where the weights correspond to mixture probabilities.
Enhancement methods based on an AR model of speech generally place no constraint other than sta-
bility on the estimated set of AR coefﬁcients. In speech coding applications however, strong constraints
are invariably placed on the permitted coefﬁcient values normally by transforming them into the Line
Spectrum Pairs (LSP) domain [45] before quantization [46]. In an enhancement method described in [47]
and developed in [48], the AR coefﬁcients are transformed into the LSP domain and smoothed across

4.35.2 Speech Enhancement Methods
1029
time. In addition, spectral constraints are introduced within each frame to ensure the AR coefﬁcients
correspond to a stable model with realistic formant bandwidths.
A Hidden Markov Model (HMM) is used to model the speech dynamics in [49], in which the speech
power spectrum associated with each state is represented as a mixture of AR processes rather than in
terms of the conventional Mel-frequency Cepstral Coefﬁcients (MFCC) [50]. The paper presents an
approximate method of determining the MAP estimate of the clean speech which, in [51], is extended
to an exact method resulting in a small improvement in performance.
The speech model used in [52] is deﬁned in the log spectral domain as a Gaussian mixture speech
model of both the static and delta (time-derivative) mel-spaced spectral components. The noise is
estimated recursively using the approach described in [53]. A MMSE estimate of the clean-speech log
spectral components was derived and the system was evaluated using a speech recogniser. The authors
found that the short term temporal correlations captured by the use of delta coefﬁcients gave signiﬁcant
beneﬁt.
Gaussian mixture models in the log spectral domain were also used in [54] for both the speech and
the noise using a high spectral resolution of 30 Hz in order to capture the spectral structure of both the
vocal tract and the excitation. The noise model had only a single component while the speech model
comprised 512 mixture components for each gender. They reported that their technique gave enhanced
speech of exceptional quality with an improvement in segmental SNR of between 4 and 7 dB over the
SNR range −5 to +15 dB SNR with no noticeable speech distortion at SNRs above 10 dB.
4.35.2.4 Time-frequency gain modiﬁcation
In the Time-Frequency Gain Modiﬁcation (TFGM) family of methods, the speech is transformed into
the time-frequency domain, normally using the Short Time Fourier Transform (STFT) (see Section
4.35.3.1) and a separate gain is applied at each time-frequency cell. Numerous versions of this approach
have been proposed of which the best known are the Spectral Subtraction (SS) and Minimum Mean
Squared Error (MMSE) families of algorithms. These versions differ in the function used to specify the
gain; the SS methods use an approximation to the Wiener ﬁlter while the MMSE methods minimize the
mean squared error of the output spectral amplitudes or log amplitudes under the assumption of explicit
speech and noise models.
A problem suffered by this family of speech enhancement methods is the introduction of “musical
noise” into the output signal. The enhanced speech includes brief tonal components apparently appearing
randomlyintimeandfrequency.Allpracticalimplementationsattempttoreducetheseannoyingartefacts
usually at the cost of increased residual broadband noise. Musical noise is much less apparent if only a
moderate degree of enhancement is applied than if strong suppression of the noise is attempted.
The spectral subtraction method of speech enhancement was introduced in [55] and remains one of
the most widely used ways of reducing additive noise; a good overview of the method and its variants
is included in [56]. In the simplest form of spectral subtraction, the estimated magnitude spectrum of
the noise, | ˆN(e jω)|, is subtracted from that of the noisy speech to obtain the magnitude spectrum of the
enhanced speech while the phase of each spectral component is left unaltered. This process is illustrated
in Figure 35.4 and can be written in the frequency domain as
ˆS(e jω) = G(e jω)X(e jω),
(35.2)

1030
CHAPTER 35 Enhancement
STFT
Noise
Estimation
Time-Freq
Gain
x(n)
s(n)
Inverse
STFT
X(e jω)
G(e jω)
N(e jω)
FIGURE 35.4
Block diagram for spectral subtraction.
where the real-valued gain function, G(e jω), is given by
G(e jω) = max

|X(e jω)| −| ˆN(e jω)|
|X(e jω)|
, 0

(35.3)
in which the max () function prevents G from becoming negative at low SNRs. Because the gain function
is real-valued, the phase spectrum is uncorrected. However [57] justiﬁed this by demonstrating that little
perceptual improvement resulted from using the true phase spectrum of the clean speech signal. Methods
of estimating the noise spectrum, ˆN(e jω), are discussed in Section 4.35.3.3.
A more general expression for the gain function was proposed in [58] as
G(e jω) = max
⎧
⎪⎨
⎪⎩

|X(e jω)|γ −| ˆN(e jω)|γ 	1/γ
|X(e jω)|
, 0
⎫
⎪⎬
⎪⎭
(35.4)
and the choices of γ = {0.5, 1, 2} were investigated. Despite the ﬁnding in this work that the best
results were obtained with γ = 2, the most popular choice remains γ = 1 which gives increased noise
attenuation at high SNRs.
Subtracting the mean noise spectrum rather than its instantaneous value causes two problems: (i) there
is residual broad-band noise after processing and (ii) individual narrow band spectral spikes remain and
generate tonal noise often referred to as musical noise. A number of improvements have been proposed
to circumvent these problems including the introduction of a gain ﬂoor and of oversubtraction of the
estimated noise spectrum [58,59] which result in the modiﬁed gain function,
G(e jω) = max
⎧
⎪⎨
⎪⎩

|X(e jω)|γ −α| ˆN(e jω)|γ 	1/γ
|X(e jω)|
, β| ˆN(e jω)|
⎫
⎪⎬
⎪⎭
,
(35.5)
where α ≥1 and 0 ≤β ≪1 are coefﬁcients controlling the oversubtraction and the noise ﬂoor
respectively. By setting the noise ﬂoor coefﬁcient β to a small positive value (typically in the range
0.005–0.1) some broadband noise is retained which reduces the perception of musical noise [56,58].
Increasing the oversubtraction coefﬁcient, α, reduces the residual noise but may introduce distortion of
the speech signal if set too high. Under the assumption that the noise power is constant while the speech

4.35.2 Speech Enhancement Methods
1031
power varies from frame to frame, the oversubtraction coefﬁcient α may be varied in each frame so that
less oversubtraction is performed in frames with high SNR.
A frequency-dependent oversubtraction coefﬁcient is used in [60] where α is adaptively updated
using a nonlinear function of the smoothed SNR estimate and the peak noise level in recent frames.
This approach is extended in [56] where both the oversubtraction and the noise ﬂoor coefﬁcients are
controlled adaptively. Here a perceptual threshold function is derived from the spectrum of a signal that
has been enhanced using standard spectral subtraction.
Instead of using the customary Short Time Fourier Transform (STFT) for spectral analysis [61,62],
use a ﬁlterbank that approximates the critical bands of the ear and, to allow for auditory masking effects,
subtract a calculated masking threshold from the estimated noise power when determining the Wiener
ﬁlter response.
A two-state model for speech presence is introduced in [63] in order to improve the performance of
spectral subtraction when there is no speech present. Using a Gaussian model for the noise, an expression
is derived for the probability of speech presence based on the true (or “a priori”) SNR and the ratio
of noisy-signal to noise power (the “a posteriori” SNR). The gain function in (35.4) is then multiplied
by the probability of speech presence which results in greater attenuation when speech is absent. An
alternative approach to eliminating musical noise is followed in [64] which borrows morphological
operations from image processing. The idea is to apply dilation or closure (dilation followed by erosion)
operators to the time-frequency spectrogram that results from spectral subtraction in order to smooth
out the residual noise without blurring speech features. To avoid the latter, the noisy speech signal is
segmented into regions corresponding to voiced intervals, unvoiced intervals, transitions and silence
with the morphological operations applied only within a homogeneous segment.
4.35.2.5 Minimum mean square estimators (MMSE)
In an inﬂuential paper [65] proposed an optimal MMSE estimation of the Short Time Spectral Amplitude
(STSA); its structure is the same as that of spectral subtraction but, in contrast to the Wiener ﬁltering
motivation of spectral subtraction, it derives an optimal estimate of the spectral magnitudes rather than
of the complex spectral amplitudes. Central to this procedure is an estimate of SNR in each frequency
bin for which two algorithms were proposed: a Maximum Likelihood (ML) approach and a “decision
directed” approach which they found gave better performance. Both algorithms assume that the mean
noise power spectrum is known in advance (see Section 4.35.3.3). The ML approach estimates the a
priori SNR by subtracting unity from the low-pass ﬁltered ratio of noisy-signal to noise power (the “a
posteriori” or “instantaneous” SNR) and half-wave rectifying the result so that it is non-negative. The
decision-directedapproachformstheSNRestimatebytakingaweightedaverageofthisMLestimateand
an estimate of the previous frame’s SNR determined from the enhanced speech. The decision-directed
approach can be viewed as applying a low-pass ﬁlter to the ML estimate. The approach was extended
in [66] to account for the time-correlation between successive speech spectral components and also, by
using a non-causal ﬁlter, to reduce the response time to speech onsets and improve the discrimination
between speech onsets and noise irregularities. Subsequently [67], introduced an improved version of
the MMSE enhancer which minimizes the mean square error of the log spectrum, rather than that of the
magnitude spectrum. It was reported that this gave noticeably lower background noise levels without

1032
CHAPTER 35 Enhancement
0
0.5
1
1.5
2
2.5
0
1
2
3
4
5
Spectral amplitude at 500 Hz
Probability density
Empirical
Gamma approx.
Laplace approx.
Rayleigh
FIGURE 35.5
Hisogram of speech spectral amplitudes compared to Rayleigh and approximate Gamma and Laplace distri-
butions (adapted from [71]).
introducing additional distortion. An analysis of why the approach in [65] gives less musical noise than
spectral subtraction is given in [68].
The performance on non-stationary noise is improved in [69] by borrowing techniques from missing
feature estimation (see Section 4.35.2.7.5). If the frame SNR is less than 10 dB, it is assumed that only
the voiced components of speech will be above the noise ﬂoor. Broad subbands are then classiﬁed as
speech or noise using one of several spectral ﬂatness measures and additional attenuation applied in
the subbands classiﬁed as noise. It is claimed that this approach works well for noise consisting of a
stationary component added to impulsive bursts; the MMSE enhancer removes the stationary component
while the postprocessor identiﬁes and attenuates the impulsive bursts.
A number of authors, including [70], have incorporated a perceptual masking model into the enhance-
ment process in order to avoid attenuating noise components that are already inaudible due to masking.
4.35.2.5.1
Super Gaussian estimates
If the correlation length of the speech signal exceeds the analysis window length, the speech spectral
amplitudes will not be Gaussian and a MMSE estimator is not optimal. Using a Rayleigh prior for
the noise spectral magnitudes, [71] derived optimal amplitude estimators and an expression for the
speech prior that, with appropriate parameter settings, could approximate either a Laplacian or Gamma
distribution. It was found that the true histogram of spectral magnitudes does not follow a Rayleigh
distribution but instead lies between a Gamma and Laplace distribution (see Figure 35.5). The use of
super-Gaussian priors was found to give increased noise reduction but with a slight increase in speech
distortion at SNR levels below 5 dB. A similar conclusion was reached by [72] who derived an exact,
albeit complicated, expression for the optimum estimator using a Laplacian speech prior. It found that,
at 10 dB SNR, the use of the Laplacian prior gave >4 dB improvement in segmental SNR and that the
added use of a “speech presence uncertainty” estimate resulted in an additional 0.6 dB improvement.
Optimal estimators of the complex spectral amplitudes for Gamma and Laplacian speech priors
were derived in [73] where it was found that the latter resulted in lower musical noise. However, it was

4.35.2 Speech Enhancement Methods
1033
concluded that although the use of these super-gaussian speech priors gave increased noise supression
this came at the cost of poorer noise quality.
4.35.2.6 Temporal gain changes
Broadband gain control systems adjust the overall gain of the system to control the output level in
some way. They often incorporate a lookahead mechanism (actually implemented as a delay in the
audible signal) that, for example, allows them to reduce the gain slightly before a loud sound occurs.
A block diagram of a general gain modiﬁcation module is shown in Figure 35.6. The path through the
upper block is the main signal path while the lower three blocks constitute the side-chain. Within the
side-chain, the ﬁrst block senses the level of the input signal: either its broadband power or its power
within a particular frequency band. The second block determines the gain to be applied to the signal as
a function of level while the third ﬁlters the value of the gain to prevent it changing too rapidly. The
output of this ﬁlter is then used to control the gain in the main signal path.
In the gain curve shown in Figure 35.6, the horizontal and vertical axes represent the input and output
signal levels respectively. The dashed line shows a gain of unity and applies in the central amplitude
range. Above this “unprocessed” region is a range of input amplitudes for which the gain curve has
a slope of 0.5 meaning that, in this region, the signal is compressed by a ratio of 2:1. At still higher
amplitudes the gain curve is horizontal giving inﬁnite compression so that the output is limited to a
maximum value. Below the unprocessed region is a range of input amplitudes for which the gain curve
has a gradient of 2.5 resulting in expansion by a factor of 2.5:1 and ﬁnally, at very low input levels, the
output is set to zero (or “gated”).
Although the ﬁgure shows only a single gain control module, it is quite common to divide the input
signal into number of frequency bands and to have separate modules for each band. It is possible for
the level sensing block to act not on the level of the selected band but instead on the difference between
the level of the selected band and the level of the broadband signal. In this way, its operation becomes
independent of the overall signal level.
The upper portions of the gain curve (compression and limiting) are used to prevent distortion arising
at subsequent points in the signal chain due to overloading. The lower portions of the gain curve (gating
and expansion) increase the apparent SNR of the signal.
4.35.2.6.1
Expander/noise gate
The purpose of a noise gate is to reduce the apparent noise level by attenuating the input signal when
no speech is present. The noise gate assumes that input signal levels below a threshold are noise and
attenuates them by setting the gain to a low value, sometimes zero as illustrated in Figure 35.6.
The operation of a noise gate is controlled by three parameters: the length of time that the signal must
be below the threshold before it is attenuated (the hold time), the time over which the attenuation takes
place (the release time) and the time over which the attenuation is removed when the signal rises above
the threshold (the attack time). Some noise gates incorporate hysteresis and have higher threshold for
removing the attenuation than is used for applying it; this, together with the incorporation of a longer
hold time, helps prevent rapid switching between the two states.

1034
CHAPTER 35 Enhancement
FIGURE 35.6
Gain change block diagram and gain curve.
4.35.2.6.2
Dynamic noise reduction
Dynamic Noise Reduction (DNR), introduced by National Semiconductor [74], applies a lowpass ﬁlter
to the signal whose cutoff frequency depends on the level of the preemphasised input signal. The
lowpass ﬁlter has a minimum cutoff frequency of 800 Hz but this is increased whenever signiﬁcant high
frequency energy is detected in the input signal. Since the gain at low frequencies remains unchanged,
the system avoids the introduction of noticeable modulation artefacts and the attack time can be much
shorter than that of the expander described in Section 4.35.2.6.1.
4.35.2.7 Excision/interpolation
One approach to speech cleaning is to identify time or frequency intervals that contain only noise and
to excise them completely. The beneﬁt of this approach is that rather than having to estimate the true

4.35.2 Speech Enhancement Methods
1035
speech level in the affected time-frequency cells as in TFGM (Section 4.35.2.4), it is only necessary to
identify those that have very poor SNR. Although it is highly nonlinear, the technique may nevertheless
be less likely to introduce artefacts into the signal.
If the excised interval is very small in time and/or frequency the affected signal can just be set to
zero. For moderate intervals, it is possible to estimate the missing information from the uncorrupted
signal either side of the affected interval. There are therefore two stages: (a) detection of the corrupted
interval and (b) interpolation of replacement information.
4.35.2.7.1
Clipping restoration
Unintentionally applying a level limiter (Section 4.35.2.6) to a signal, such as typically occurs when
signals exceed the available dynamic range, results in a signal that is clipped. This can arise at any
point on the channel, but most commonly in the microphone preampliﬁer stage. The audible effect is to
introduce high frequency harmonics of the signal frequencies giving the signal a harsh sound. Detecting
clipping is straightforward in the time domain if no further signal processing has taken place, but is
more difﬁcult if, for example, the signal has passed through a CODEC or high pass ﬁltering stage.
4.35.2.7.2
Click and dropout removal
A click removal tool is intended to excise high amplitude short duration interference. Such interference
may arise from a variety of sources including impulsive radio frequency interference, faulty electrical
connections and digital recording errors. If clicks occur at a high rate with a regular period, then they
manifest themselves as a buzz and this can be removed using an adaptive comb ﬁlter (Section 4.35.2.3.1).
Dropouts are short periods of silence which may, for example, arise from faulty connections or unreliable
wireless communication channels.
Methods for identifying and cleaning impulsive noise are extensively discussed in [75] and the
references therein. The common techniques often require manual localization of the impulsive noise
but clicks can also be detected by identifying discontinuities in the input signal or in features derived from
it. With knowledge of the time of occurrence of impulse noise, methods can then be used to interpolate
the undamaged signal that exists before and after the noise in order to remove it. Pure interpolation
methods assume that no useful information remains at the instant of the impulsive noise. Other methods
assume that the original data may still be present during the impulsive noise and attempt to model it in
order to achieve better accuracy. The statistical distribution of the samples, whether known, assumed
or deduced, can be used to condition the interpolation processing.
4.35.2.7.3
Binary masks
A binary mask enhancer has the same structure as the Time-Frequency Gain Modiﬁcation (TFGM)
enhancers discussed in Section 4.35.2.4 but, rather than trying to estimate the clean speech signal
explicitly, it attempts only to classify the Time-Frequency (TF) regions into those that include speech
and those containing only noise. The gain of all TF cells identiﬁed as containing speech is kept at unity,
while that of the other TF cells is set to a small value, often zero.
Identiﬁcation of the TF cells that contain speech is a difﬁcult problem and a reliable method remains
elusive. The segmentation of the time-frequency plane into distinct sound sources is known as Com-
putational Auditory Scene Analysis (CASA) and a good overview of the ﬁeld is given in [76]. Most

1036
CHAPTER 35 Enhancement
approaches combine low level cues to aggregate TF regions that are likely to be related with high level
constraints based on prior knowledge.
The advantage of the binary mask approach is that classiﬁcation of TF cells is seen as an easier
problem than that of estimating the clean speech amplitude and that there are many well understood
techniques for performing classiﬁcation. Nevertheless, no method is so far able to perform reliable
classiﬁcation of the TF cells in situations of poor SNR. A disadvantage of this approach is that although
speech from a binary mask system is intelligible [77] it is of poor perceived quality even when the
ideal mask is used. It has been shown in [78] that optimal continuous gain functions result in higher
intelligibility than binary masks.
4.35.2.7.4
Signal interpolation
When a portion of the TF plane has been excised for any of the reasons given above, it is necessary
to interpolate replacement values. For very short intervals, it is possible to use a median ﬁlter. For
larger gaps, the preferred methods use a MMSE approach with some appropriate speech model, e.g.,
an AR model. Autoregressive interpolation assumes the data can be well modeled by an autoregressive
model and estimates the parameters of the autoregressive model using, for example, least squares
error minimization. Pitch information (see Section 4.35.3.4) may also be employed to constrain the
interpolated speech further. A range of methods is discussed in [75].
4.35.2.7.5
Missing feature estimation
Missing feature methods aim to identify arbitrarily shaped regions of the time-frequency plane that are
dominatedbynoiseandeitherremoveorreconstructthem.Thesegmentationofthetime-frequencyplane
into distinct sound sources is known as Computational Auditory Scene Analysis (CASA) and a good
overviewoftheﬁeldisgivenin[76].ForCASA-basedenhancement,thekeystepisthecreationofamask
that identiﬁes the noise-dominated regions of the time-frequency plane having an SNR below a suitable
threshold, typically 0 to −5 dB. Some authors use a “soft” mask whose value denotes the probability (or
sometimes the degree) of noise-domination. The effect of masking on human intelligibility was studied
in [79]; they suggest an SNR threshold of 0 dB for a binary mask.
In [80] the input speech is transformed into the time-frequency domain by an auditory ﬁlterbank.
Within each time-frequency “pixel” a dominant pitch is determined by searching for an autocorrelation
peak. Compatible contiguous pixels are then grouped into segments and the segments are tentatively
assigned to the foreground or background stream on the basis of their dominant pitch consistency. A
pitch contour is then determined for the foreground stream and the foreground/background assignments
are re-evaluated on the basis of compatibility with the pitch contour. Li et al. [81] extends this approach
by using a non-intrusive speech quality measure, ITU-T P.563 (see Section 4.35.4.3.2), to determine
which pixels to include in the foreground stream.
Seltzer et al. [82] uses a Bayesian classiﬁer to identify missing regions by means of a seven element
feature vector which, for each of 20 Mel-spaced subbands, comprises:
1. The log ratio of the energy from a comb ﬁlter at the pitch period and that from the same ﬁlter but
shifted by half the pitch frequency. The intention is to determine what fraction of the subband
energy is contained in the periodic component. The pitch is determined using the RAPT pitch
detector from [83] which combines an autocorrelation method with dynamic programming.

4.35.3 Enabling Algorithms
1037
2. The ratio of the highest to the second highest peak in the autocorrelation.
3. The ratio of the subband energy to the total energy.
4. The kurtosis, or fourth moment, of the subband signal. The assumption is that clean speech is
super-Gaussian (i.e., has a higher kurtosis than a Gaussian) but that noisy speech will, because
of the central limit theorem, have a lower kurtosis.
5. The ratio of the subband energy to the subband noise ﬂoor.
6. The spectral ﬂatness in neighboring subbands.
7. The subband SNR estimated from spectral subtraction (Section 4.35.2.4).
The procedure requires a pitch detector which, since the ﬁrst two features are used only for voiced
frames, also serves as a voicing detector. Separate classiﬁers are trained for voiced and voiceless speech
in each subband with a “noise pixel” deﬁned as one with worse than −5 dB SNR. Clean speech spectra
are reconstructed for the missing regions using cluster-based reconstruction [84] in which clean speech
log spectra are modeled by a Gaussian mixture distribution. The parameters of each mixture are used to
calculate a maximum likelihood estimate of the missing data conditioned on the observed non-masked
bins and on it not exceeding the masked bins. The ﬁnal estimate is then formed by weighting each
of these individual estimates by the likelihood of the corresponding mixture. A second, correlation-
based, reconstruction method was described in [84] which determined correlations between pixels in
the spectrogram and uses these correlations in time and frequency to estimate the missing regions. In
their evaluation tests however, this second reconstruction method performed consistently worse than
the cluster-based method.
The procedure was evaluated using an automatic speech recogniser and, as shown in Figure 35.7,
improved recognition performance at an SNR of 10 dB, from 15% to 60% for factory noise. The
improvement was somewhat better for white noise and signiﬁcantly worse for interference consisting
of music. As can be seen from the ﬁgure, the performance is signiﬁcantly better than using spectral
subtraction to identify the noisy regions of the time-frequency plane and decreases only slightly when
white noise, rather than matched noise, is used to train the classiﬁer. It also decreased for both higher
and lower SNR levels. Kim and Stern [85] used a similar approach and found that using a wider variety
of noises in training improved performance.
4.35.3 Enabling algorithms
We list here a number of enabling algorithms that form part of the enhancement algorithms discussed
above.
4.35.3.1 Time-frequency analysis and synthesis
Many speech cleaning algorithms operate in the time-frequency domain; they must transform the input
signal into that domain and, after processing it, transform it back into the time domain. The most common
approach is to divide the input signal into overlapping frames, window each frame and transform it using
a Discrete Fourier Transform (DFT); this procedure is known as the Short Time Fourier Transform

1038
CHAPTER 35 Enhancement
0
5
10
15
20
25
0
20
40
60
80
100
SNR (dB)
Recognition Accuracy (%)
Bayes masks
Bayes masks - train WGN
Specsub masks
Baseline
FIGURE 35.7
Recognition accuracy for speech corrupted with factory noise when noise corrupted portions of the time-
frequency plane are reconstructed (adapted from [82]).
(STFT). To perform the inverse transformation, the frames are again windowed, transformed with the
inverse DFT and overlap-added to create the output signal. If the windows are chosen appropriately,
this procedure gives perfect reconstruction of the original signal when no Time-Frequency (TF) domain
processing is performed [86].
The kth frame of the sampled input signal x(n) is given by
x(n; k) = w(n)x(n + kM),
(35.6)
for n = 0, . . . , N −1 where w(n) is a windowing function with ﬁnite support and M ≤N is the time
increment between successive frames (in samples). The window length, N, is a compromise between
frequency and time resolution and is typically chosen in the range 10–30 ms for speech signals resulting
in a frequency resolution of around 50 Hz. The frame increment, M, is most commonly set at N
2 although,
as noted below, there are theoretical reasons for using M = N
4 despite its higher computational cost.
A Fourier transform is normally performed on each frame to obtain the STFT. If no processing is done
on the frame spectra, the original time domain signal can be reconstructed exactly with overlap-addition
[87,88]. However, when frequency-domain processing is performed on the frames, distortion artefacts
may be introduced due to signal discontinuities at frame boundaries and aliasing of rapidly changing
spectral coefﬁcients. The reconstruction properties can be controlled by the choice of the windowing
function and the ratio M
N . The use of half-overlapping (M = N
2 ) square-root Hanning windows for
both analysis and synthesis is suggested in [89] to provide perfect reconstruction in the absence of
any processing and, at the same time, to attenuate any frame-boundary discontinuities. An extensive
discussion of these issues is given in [87,88] where it is shown that, for a Hamming analysis window, a
three-quarters overlap (M = N
4 ) is needed in order to ensure that the spectral coefﬁcients are sampled
frequently enough to avoid aliasing.

4.35.3 Enabling Algorithms
1039
4.35.3.2 Voice activity detection
A Voice Activity Detector (VAD) is an algorithm that identiﬁes the time periods during which speech
is present in a signal. VADs are widely used in mobile telephony to identify when it is necessary to
transmit a voice signal; however reliable VADs for signals with poor SNR are difﬁcult to make. To
prevent the identiﬁcation of weak speech tails as noise, most VADs incorporate “hangover” to delay
the decision that speech is absent; this can either be a ﬁxed delay or a more sophisticated HMM based
method.
4.35.3.2.1
Energy histogram
An idea, originally from [90], is extended in [63] to identify speech presence from the signal energy
histogram smoothed over a four-second window. To decide on the presence of speech, the algorithm
determines an adaptive energy threshold that is restricted to lie between ﬁxed upper and lower thresholds.
A target threshold is chosen to lie at the 80th centile of the cumulative histogram of frame energies that
are below the upper threshold and the adaptive threshold is taken as a smoothed version of the target
threshold. This approach is modiﬁed in [91] which ﬁts a 2-component Gaussian mixture model to the
histogram of log energy and assumes that the lower component represents the noise. A similar approach
is used by [92] in which an adaptive threshold is used in each frequency bin to eliminate speech frames
and the peak of the histogram of recent noise frames is used as an estimate of the noise power in that
bin.
Freeman et al. [93] describes the VAD used in cellular phones conforming to the Global System For
Mobile Communications (GSM) standard; it effectively compares the SNR averaged over all frequency
bands to a threshold in order to perform the VAD decision and this criterion was also used in [94].
4.35.3.2.2
Magnitude histogram
The DFT and KLT coefﬁcient magnitudes of speech signals were found by Zhang and Gazor [95,
96] to follow a Laplacian distribution rather than the more commonly assumed Gaussian or Gamma
distributions. Accordingly Zhang and Gazor [97] proposes a voice activity detector that models the
noisy signal DFT/KLT coefﬁcients as the sum of zero-mean Laplacian and Gaussian random variables
respectively. In each frequency bin, the noise variance is estimated by lowpass ﬁltering the energy
in past frames that were classiﬁed as noise and this is then used to estimate the parameter of the
Laplacian distribution representing the speech coefﬁcients. Following this, each frame is classiﬁed as
speech + noise or noise-only using a likelihood ratio test. The authors found that using the KLT rather
than the DFT gave only a marginally better performance.
4.35.3.2.3
Energy waveform
An algorithm for speech pause detection is presented by Marzinzik and Kollmeier [98] that uses the
energy waveform in low, high and full frequency bands. The lowpass-ﬁltered maximum and minimum
values of these are tracked and used to decide when speech pauses are present. This VAD is compared
to that used in the ITU-T G.729 standard [46] and shown to give improved performance.
A VAD described in Annex A of European Telecommunications Standards Institute (ETSI) Standard
ES202050 [99] identiﬁes the increase in energy associated with the onset of voiced speech. It makes
a VAD decision based on the values of three indicators: (a) a sudden increase in overall energy (b) a

1040
CHAPTER 35 Enhancement
sudden increase in low frequency energy and (c) a sudden increase in the variance of energy across the
spectrum.
4.35.3.2.4
Periodicity
Voiced speech is detected in [100] by looking for strong periodicity. The algorithm does this by assuming
that the fundamental is a subharmonic of the largest spectral peak, summing all its aligned harmonics
below 1.25 kHz and measuring how sharp the resultant peak is. Unvoiced speech is assumed to be
possible for a 200 ms hangover interval following a burst of voiced speech. This is preceded by the
speech probability detector of [101] to give a composite VAD which controls noise adaptation.
A periodicity detector based on the Fourier transform of the Average Magnitude Difference Func-
tion (AMDF) function is combined in [102] with a modiﬁed minimum statistic approach (see Section
4.35.3.3.2) to control noise spectrum averaging.
4.35.3.2.5
Statistical
Statistical VADs incorporate models of the speech and noise and use these to identify whether speech
is present. The soft-decision VAD that is used by Sohn and Sung [103] is based on a likelihood ratio
that is equivalent to the Itakura-Saito distortion measure or cross entropy between background noise
and observed signal [104,105]. A similar approach is used in [106] where the estimated SNR averaged
across all frequencies is used to control adaptation together with an additional frequency-dependent
factor that depends on the estimated speech presence in each frequency bin.
4.35.3.3 Noise estimation
Some noise estimation algorithms estimate only the total power of the noise, but more commonly they
try to estimate its power spectrum in each time frame.
4.35.3.3.1
VAD-based noise estimation
The most straightforward method of estimating a stationary noise spectrum is to identify time inter-
vals when speech is absent and to average the noise spectrum observed during these intervals. The
identiﬁcation can be done manually or using a Voice Activity Detector (VAD). Rather than making a
hard decision on whether or not speech is present, several noise estimators instead weight the currently
observed spectrum with a probability of speech presence [107–109].
In [110–112], a weighted noise estimation procedure used in cellphone handsets is described. For
each frame, a raw noise estimate is generated that is approximately equal to the the noisy speech divided
by the estimated SNR. The smoothed noise estimate is then formed by averaging, in each frequency
bin, the N most recent raw estimates for which the SNR was below a ﬁxed threshold.
Instead of a hard VAD decision, [113] determines a “probability of speech presence” in each time-
frequency bin by comparing the input signal power and the noise estimate from the previous frame.
Using this, it is possible to obtain an MMSE estimate of the noise spectrum which has been found to
be robust and accurate.
In most cases, it is assumed that the noise can be well-represented by a single power spectrum
that varies slowly with time. This is extended in [114] where the noise is modeled as the sum of a

4.35.3 Enabling Algorithms
1041
slowly evolving component and a random component. The authors claim that this model is both more
realistic and allows better tracking of the evolving component. A more sophisticated noise model uses
an HMM to represent the time-evolution of the noise spectrum. During non-speech intervals that exceed
100 ms, [115] searches a gain-normalized library of HMM noise models and selects the one with highest
likelihood. The selected noise model is then used for any subsequent speech segment. An alternative
approach that eliminates the need for a pre-trained library is to use an adaptive HMM that is updated
during noise-only intervals [116].
4.35.3.3.2
Minimum statistics
The use of minimum statistics for noise estimation was introduced in [117] and extended in [118].
The assumption is that in any frequency bin there will be times when there is little speech energy and
that the energy will then be dominated by the noise. If we assume that these times occur at least once
per time T , we can estimate the noise power as the minimum power that has arisen within the past T
(typically 0.5–1.5 s). The output of this minimum ﬁlter will inevitably underestimate the true noise and
it is necessary to compensate for this bias. In [118] the ﬁxed compensation factor used in the original
algorithm was replaced with factor that varied with time and frequency; further details of this bias
compensation were given in [119]. The resultant algorithm is very robust and widely used.
Noting that the use of the minimum makes the technique sensitive to outliers [120], investigated the
use of other quantiles instead. The authors came to the conclusion that the median gave the best results
when evaluated using a speech recogniser. Few people, however, appear to have followed up this work
although Manohar and Rao [69] demonstrates that it performs poorly on non-stationary noise.
4.35.3.4 Pitch tracking
The aim of a pitch tracker is to identify when voiced speech is present and, if so, to identify its
fundamental frequency. Pitch trackers can use either parametric or non-parametric approaches. The
parametric methods deﬁne explicit models for the speech and noise and determine the pitch directly or
indirectly by estimating the model parameters from the noisy speech. Parametric methods have high
performance when the modeling assumptions are met but can be fragile when this is not the case. A
good description of several parametric methods for pitch detection is given in [121]. Non-parametric
methods avoid explicit signal and noise models and look either for harmonic structure in the frequency
domain or periodicity in the time domain. Two widely used time-domain algorithms are RAPT [83] and
YIN [122] which both detect peaks in the autocorrelation function, or its equivalent, of the input signal.
The performance of these time-domain algorithms degrades rapidly at SNRs below 0 dB, whereas some
frequency domain methods are able to detect pitch reliably at very poor SNRs [123].
In many cases, the pitch tracker will identify multiple pitch candidates in each time frame and
will then use Dyanamic Programming (DP) to join these candidates into continuous pitch tracks and
identify unvoiced segments [83]. Some algorithms, for example [124], are able to track the pitches of
multiple sources such as might arise when a harmonic noise source is present or several speakers talk
simultaneously.

1042
CHAPTER 35 Enhancement
4.35.4 Intelligibility and quality measures
4.35.4.1 Objectives
Methods for evaluating speech enhancers can be divided into those that aim to evaluate the intelligibility
of the enhanced speech and those that aim to assess its quality or acceptability to a listener. Assessment
methods can be divided into subjective methods that involve the judgment of human listeners and
objective methods that estimate the quality or intelligibility algorithmically from the signal. Objective
methods can be further divided into intrusive methods which require a reference clean speech signal and
non-intrusive methods which determine their estimate from the degraded signal alone. A comprehensive
review of assessment methods is included in [125],
4.35.4.2 Evaluation of speech intelligibility
4.35.4.2.1
Subjective methods
Test material
Early intelligibility tests used Consonant-Vowel-Consonant (CVC) [126] or Vowel-Consonant-Vowel
(VCV) [127] nonsense syllables. The advantage of these is that intelligibility results can be analysed
directly in terms of the acoustic contrasts used by listeners to discriminate different speech sounds.
Nevertheless, to obtain a more realistic context, most tests now use real words rather than nonsense
syllables.
In the Diagnostic Rhyme Test (DRT), [128], listeners must choose between pairs of rhyming words
whose initial consonants differ in only one phonetic feature. The contrasting features are chosen to
reﬂect the consonant confusions that were identiﬁed in [127]. This allows listener judgments to be
analyzed in terms of their performance on different feature dimensions as well as on their overall score.
Even with as few as eight listeners, the DRT has been found to have very high resolution and test-retest
reliability.
Words in isolation are not typical of normal speech and many tests require listeners to identify the
content words in whole sentences. The Hearing-in-Noise Test (HINT) was developed by Nilsson et al.
[129] and consists of 25 sets of 10 short sentences based on those deﬁned originally in [130,131]. Each
set contains approximately the same distribution of phonemes as normal English and the sentence levels
are adjusted to equalize their intelligibility. The widely used “IEEE sentences” were deﬁned in [132]
and consist of 72 sets of ten sentences each containing ﬁve content words in an unpredictable context,
e.g., “The birch canoe slid on the smooth planks.” Because sentences are memorable over long time
intervals, a sentence cannot be presented twice to the same listener; this severely limits the number
of tests that a listener is able perform. To circumvent this issue [133], introduced the idea of a matrix
sentence e.g., “James likes six blue frogs” in which each of the ﬁve words is chosen randomly from a
list of ten possibilities to give 100,000 equiprobable sentences. A disadvantage of this approach is that
some of the resultant sentences may sound unnatural or, as in this example, be semantically unlikely.
Subjective measurements
Measurements can be made of the percentage of words identiﬁed correctly by a listener as a function of
the SNR, or some other measure of degradation severity. The resultant graph is called the psychometric

4.35.4 Intelligibility and Quality Measures
1043
-10
-5
0
5
10
0
0.2
0.4
0.6
0.8
1
Input SNR (dB)
Fraction of correct words
FIGURE 35.8
Example psychometric function showing the fraction of words correctly identiﬁed as a function of the SNR.
function and its characteristic S-shape is illustrated in Figure 35.8. The SNR at which a speciﬁc fraction
(usually 50%) of words is correctly identiﬁed is known as the Speech Reception Threshold (SRT)[134].
It is found experimentally that the slope of the psychometric function at the 50% SRT is fairly constant
at about 10%/dB [135] and so the improvement given by a speech enhancement method can be charac-
terized by the amount by which the graph in in Figure 35.8 is shifted to the left. This shift, expressed in
dB, indicates the amount of additional noise that can be tolerated when applying enhancement and in
[136] was termed the Comparative Tolerance to Noise (CTTN) of the enhancer. To reduce the effects of
listener variability [136], suggests that measurements of the enhanced and unenhanced signals should
be interleaved.
The psychometric function can be measured directly by presenting test sentences over a wide range of
SNRs and measuring the fraction of words correctly identiﬁed at each level. A more efﬁcient procedure
is to vary the SNR adaptively according to the listener’s responses so that most of the presentations
are at SNRs close to the SRT [137]. Alternatively a parametric form of the psychometric function
can be assumed and a Maximum Likelihood (ML) estimate of the parameters determined from the
measurements [138]. By using a Bayesian framework, [139], updates the parameter estimates after each
presentation and also estimates of their uncertainty. Using this information, it is possible to select for
each presentation the SNR that will result in the lowest expected parameter uncertainty. In this way the
number of presentations required for a given measurement precision can be minimized.
Non-human listeners
To avoid the need for trained human listeners, it is possible to replace them in listening tests by a speech
recognition system. Unfortunately, when intelligibility is poor, the performance of speech recognisers
does not match that of human listeners at all well. By the same token however, this is the preferred
way of evaluating an enhancer whose intended use is to preprocess noisy speech before it is passed to
a speech recogniser.

1044
CHAPTER 35 Enhancement
4.35.4.2.2
Objective methods
Intrusive intelligibility metrics
Experimental evidence that different spectral bands contributed additively to intelligibility led to the
deﬁnition of the Articulation Index (AI) [140,141] and subsequently of the Speech Intelligibility Index
(SII) [142]. In these measures, the contribution of each spectral band depends on the effective SNR
within that band. To obtain the effective SNR, the true the noise level in each frequency bin is adjusted to
compensate for the masking effects of lower frequency bands and for internal noise generated within the
ear itself. Extensive evaluation has shown that the SII correlates very well with subjective intelligibility
scores, at least for additive steady-state noise and listeners with normal hearing. Recent extensions to
the SII approach have attempted to account for the damage to intelligibility caused by ﬂuctuating noise
[143,144] and non-linear distortions [145].
Unfortunately, the SII gives very poor predictions of the effect of speech enhancers on intelligibility.
It has been found that many speech enhancers actually degrade intelligibility even though they give
substantial improvements in SNR [135]. A number of recent metrics such as [146,147] compare the
amplitude modulation of both the clean and degraded speech signals in a number of acoustic channels.
From this comparison, they derive an estimate of speech intelligibility that closely matches the results
of listening experiments even for enhanced speech.
Objective speech quality measures, such as Perceptual Evaluation Of Speech Quality (PESQ), have
also been used as predictors of intelligibility and an evaluation of a number of such measures can be
found in [148]. A predictor for intelligibility was built by Yamada et al. [149] from PESQ predictions
of Mean Opinion Score (MOS) for a variety of noise conditions processed by several different noise-
reduction algorithms. The effectiveness of prediction varied with word familiarity, but correlations with
subjective intelligibility scores as high as 0.9 were obtained.
Speech recognition systems have also been used to obtain objective predictions of speech intelligi-
bility, for example [150] compared the performance of a phone recogniser on the original and degraded
signals, to establish a prediction for the loss in intelligibility.
Non-intrusive intelligibility metrics
Using a similar approach to the LCQA [151] speech quality metric, it has been shown in [152] that it is
possible to predict intelligibility for a wide range of signal degradations from the degraded signal alone.
A speech recognition approach was compared with other objective measures in [148] and found
to give good intelligibility prediction at +10 dB SNR, but signiﬁcantly worse prediction than other
measures at lower SNRs where human listener performance is substantially better than that of speech
recognition systems.
4.35.4.3 Evaluation of speech quality
4.35.4.3.1
Subjective methods
There are two types of subjective speech quality test: one in which listeners assign absolute ratings to
individual speech stimuli, and one where listeners express a preference for one speech stimulus over
one or more others. The advantage of the ﬁrst type is that an absolute quality score can be assigned
but a disadvantage is that a large number of listeners is required to achieve satisfactory sensitivity. The

4.35.4 Intelligibility and Quality Measures
1045
Table 35.2 Absolute Category Rating Scale for Listening Quality Using for MOS Measurements
of Speech Quality [155]
Rating
Speech Quality
Level of Distortion
5
Excellent
Imperceptible
4
Good
Just perceptible, but not annoying
3
Fair
Perceptible and slightly annoying
2
Poor
Annoying, but not objectionable
1
Bad
Very annoying and objectionable
advantage of the second type is that a statistically signiﬁcant comparison between two systems can be
made using relatively few listeners [153].
A widely used rating scale for the assessment of speech quality is shown in Table 35.2. Standard
procedures have been published about how the scale should be used within a testing procedure [132,
154,155]. In a training phase, listeners are played a set of reference signals demonstrating the different
quality levels in order to standardize their responses. Following this, the test signals are played and the
Mean Opinion Score (MOS) is obtained by averaging the responses of a recommended minimum of 20
listeners.
The Diagnostic Acceptability Measure (DAM) developed in [156] aims to obtain a more precise
characterization of speech quality by requiring listeners to rate the quality of speech stimuli along
twelve dimensions independently: six for the signal quality, four for the noise and two for the overall
effect. For example, listeners are asked to rate the degree to which the speech is “mufﬂed” and the
degree to which the background is “buzzing.” The test provides detailed information about the quality
of the speech although testing is time-consuming and listeners need extensive training.
The ITU-T recommendation for testing signal enhancement systems, P.835 [157], requires listeners
to give individual assessments of the speech signal and the background in addition to an overall rating.
Each speech stimulus is played three times: ﬁrst listeners are asked to judge the distortion of the speech
itself then they are asked to judge the intrusiveness of the background noise and ﬁnally they are asked
to judge the overall quality using the scale in Table 35.2.
4.35.4.3.2
Objective methods
Although subjective testing is the only way to obtain true judgments of speech quality, a number of
objective methods have been developed which have been found to correlate well with subjective ratings,
such as the MOS. For a review of objective speech quality measures, see [158].
Intrusive quality metrics
Intrusive measures of speech quality compare the processed speech signal with the original clean
signal. Typically the process involves dividing the signals into short frames of between 10 and 30 ms,
and computing a distortion measure between equivalent frames. An average is then taken over all frames
to obtain a global measure. Examples are in [159,160].

1046
CHAPTER 35 Enhancement
The simplest distortion measure is obtained by subtracting the clean signal from the noisy signal and
taking the ratio of the power in this difference signal to that of the clean speech. This ratio is computed
for each frame and its value, expressed in dB, is averaged over all frames to obtain the segmental SNR.
Reliable computation of the segmental SNR requires accurate gain matching and time alignment of
the clean and noisy signals and is in any case not a good predictor of subjective quality measurements
except at high SNRs. Despite this, it is easy to calculate and remains a widely used measure.
In recent years, perceptually motivated measures have dominated techniques for objective measure-
ment of speech quality. These measures attempt to model the signal processing that takes place in the
peripheral auditory system. A widely used perceptual measure is the Perceptual Evaluation Of Speech
Quality (PESQ) algorithm from [161] which has been standardized as ITU-T P.862 [162]. PESQ applies
a sequence of processing steps to obtain a set of distortion scores which vary in frequency and time.
These are cobined to give an overall PESQ score in the range 0.5–4.5 corresponding to the scale used for
MOS shown in Table 35.2. PESQ has been shown to correlate well with subjective listening tests over a
range of channels [161]. It has also been evaluated in [163] on enhanced noisy speech where correlations
with MOS were obtained across a variety of noise types and enhancers. Recently a successor to PESQ
known as Perceptual Objective Listening Quality Analysis (POLQA) has been deﬁned and codiﬁed as
the standard ITU-T P.863 [164]. POLQA addresses some of the limitations of PESQ and, in particular,
is able to handle speech signals with a wider bandwidth.
Five objective measures were compared with subjective measures in [165] on the processed speech
from four enhancement algorithms using speech degraded by a range of real world noises at 5 dB and
10 dB SNR. The study conﬁrmed that segmental SNR is a poor predictor of quality for enhanced speech
and found that the best measure was a modiﬁed form of PESQ.
Non-intrusive quality metrics
If the original clean signal is not available, assessment of speech quality must be based on the noisy
signal alone using a non-intrusive measure. A review of a number of techniques for the non-intrusive
assessment of quality is given in [166].
A standardized method for non-intrusive quality estimation, ITU-T P.563 [167], compares the
degraded speech with a model of human voice production as well as identifying background noise
levels and speciﬁc distortions such as clipping [168]. More recently a number of algorithms have been
developed that claim to give signiﬁcantly better performance than P.563. A low-complexity measure,
LCQA, was found in [151] to give superior prediction of MOS scores with much less computational
cost than P.563 and this has been extended in [169]. The ANIQUE + model [170] is trained on the MOS
results from 24 different speech databases covering a wide variety of distortion conditions. It is claimed
that this model predicts MOS performance better even than the PESQ intrusive method.
Relevant Theory: Signal Processing and Machine Learning
See Vol. 1, Chapter 3 Discrete-Time Signals and Systems
See Vol. 1, Chapter 6 Digital Filter Structures and Their Implementation
See Vol. 1, Chapter 9 Discrete Multi-Scale Transforms in Signal Processing
See Vol. 1, Chapter 12 Adaptive Filters
See Vol. 1, Chapter 20 Clustering

4.35.4 Intelligibility and Quality Measures
1047
List of Abbreviations
AI
Articulation Index
AMDF
Average Magnitude Difference Function. A function with similar properties to the
cross- or autocorrelation but that requires no multiplication to evaluate.
AR
Autoregressive
CASA
Computational Auditory Scene Analysis
CODEC Coder-Decoder
CTTN
Comparative Tolerance to Noise
CVC
Consonant-Vowel-Consonant
DAM
Diagnostic Acceptability Measure
DFT
Discrete Fourier Transform
DNR
Dynamic Noise Reduction
DP
Dynamic Programming
DRT
Diagnostic Rhyme Test
EKF
Extended Kalman Filter
EM
Expectation-Maximization. An iterative technique to solve certain optimization
problems.
EQ
Equalisation
ETSI
European Telecommunications Standards Institute
FIR
Finite Impulse Response. A ﬁlter whose output is a weighted sum of past input
values and whose system function contains only zeros and no poles.
GSM
Global System For Mobile Communications
HINT
Hearing-in-Noise Test
HMM
Hidden Markov Model
IIR
Inﬁnite Impulse Response. A ﬁlter whose output is a weighted sum of both past
input and past output values and whose system function contains both poles and
zeros.
KF
Kalman Filter
KLT
Karhunen-Loève Transform
LF
Liljencrants-Fant. The developers of a glottal waveform model
LMS
Least Mean Squares adaptive ﬁlter
LSP
Line Spectrum Pairs
LTASS
Long Term Average Speech Spectrum
MFCC
Mel-frequency Cepstral Coefﬁcients
ML
Maximum Likelihood
MMSE
Minimum Mean Squared Error
MOS
Mean Opinion Score
NLMS
Normalized Least Mean Squares adaptive ﬁlter
PESQ
Perceptual Evaluation Of Speech Quality
POLQA
Perceptual Objective Listening Quality Analysis
RLS
Recursive Least Squares adaptive ﬁlter

1048
CHAPTER 35 Enhancement
SCAF
Single Channel Adaptive Filter
SII
Speech Intelligibility Index
SNR
Signal-to-Noise Ratio
SRT
Speech Reception Threshold
SS
Spectral Subtraction
STFT
Short Time Fourier Transform
STSA
Short Time Spectral Amplitude
TF
Time-Frequency
TFGM
Time-Frequency Gain Modiﬁcation. An approach to signal enhancement in which
a signal is multiplied by a gain function in the time-frequency domain.
UKF
Unscented Kalman Filter
VAD
Voice Activity Detector
VCV
Vowel-Consonant-Vowel
References
[1] B. Widrow, J.R. Glover Jr, J.M. McCool, J. Kaunitz, C.S. Williams, R.H. Hearn, J.R. Zeidler, E. Dong Jr,
R.C. Goodlin, Adaptive noise canceling: principles and applications, Proc. IEEE 63 (12) (1975) 1692–1716.
[2] J.R. Hopgood, P.J.W. Rayner, Blind single channel deconvolution using nonstationary signal processing,
IEEE Trans. Speech Audio Process. 11 (5) (2003) 476–488.
[3] N.D. Gaubitch, M. Brookes, P.A. Naylor, D. Sharma, Single-microphone blind channel identiﬁcation in
speech using spectrum classiﬁcation, in: Proc. European Signal Processing Conf. (EUSIPCO), Barcelona,
August 2011.
[4] S.J. Wenndt, A.J. Noga, Blind channel estimation for audio signals, in: IEEE Aerospace Conf., vol. 5, March
2004, pp. 3144–3150.
[5] D. Byrne, H. Dillon, K. Tran, S. Arlinger, K. Wilbraham, R. Cox, B. Hayerman, R. Hetu, J. Kei, C. Lui, J.
Kiessling, M.N. Kotby, N.H.A. Nasser, W.A.H.E. Kholy, Y. Nakanishi, H. Oyer, R. Powell, D. Stephens„ T.
Sirimanna, G. Tavartkiladze, G.I. Frolenkov, S. Westerman, C. Ludvigsen, An international comparison of
long-term average speech spectra, J. Acoust. Soc. Am., 96 (4) (1994) 2108–2120.
[6] S. Haykin, Adaptive Filter Theory, fourth ed., Prentice-Hall, 2002.
[7] K. Mayyas, T. Aboulnasr, Leaky LMS algorithm: MSE analysis for Gaussian data, IEEE Trans. Signal
Process. 45 (4) 1997 927–934.
[8] B. Friedlander, Lattice ﬁlters for adaptive processing, Proc. IEEE 70 (8) (1982) 829–867.
[9] L. Grifﬁths, An adaptive lattice structure for noise-canceling applications, in: Proc. IEEE Intl. Conf. on
Acoustics, Speech and Signal Processing (ICASSP), vol. 3, 1978, pp. 87–90.
[10] M.R. Sambur, Adaptive noise canceling for speech signals, IEEE Trans. Acoust., Speech, Signal Process.
26 (5) (1978) 419–423.
[11] L. Varner, T. Miller, T. Eger, A simple adaptive ﬁltering technique for speech enhancement, in: Proc. IEEE
Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP), vol. 8, 1983 pp. 1126–1128.
[12] J. Kim, C. Un, Enhancement of noisy speech by forward/backward adaptive digital ﬁltering, in: Proc. IEEE
Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP), vol. 11, 1986, pp. 89–92.
[13] A. Kawamura, Y. Iiguni, Y. Itoh, A noise reduction method based on linear prediction with variable step-size,
IEICE Trans. Fundam. Electron., Commun. Comput. Sci. E88-A (4) (2005) 855–861.

References
1049
[14] T. Hoya, J.A. Chambers, N. Forsyth, P.A. Naylor, Steady-state solutions of the extended LMS algorithm for
stereophonic acoustic echo cancelation, in: Proc. European Signal Processing Conf. (EUSIPCO), 1998, pp.
977–980.
[15] N. Sasaoka, K. Sumi, Y. Itoh, K. Fujii, A new noise reduction system based on ALE and noise reconstruction
ﬁlter, in: Proc. Intl. Symp. on Circuits and Systems, vol. 1, 2005, pp. 272–275.
[16] N. Sasaoka, M. Watanabe, Y. Itoh, K. Fujii, Noise reduction system based on LPEF and system identiﬁcation
with variable step size, in: Proc. Intl. Symp. on Circuits and Systems, 2007, pp. 2311–2314.
[17] L.R. Rabiner, R.W. Schafer, Digital Processing of Speech Signals, Prentice-Hall, Englewood Cliffs, New
Jersey, USA, 1978.
[18] M. Dendrinos, S. Bakamidis, G. Carayannis, Speech enhancement from noise: a regenerative approach,
Speech Commun. 10 (1) (1991) 45–67.
[19] Y. Ephraim, H.L. Van Trees, A signal subspace approach for speech enhancement, IEEE Trans. Speech
Audio Process. 3 (4) (1995) 251–266.
[20] S.H. Jensen, P.C. Hansen, S.D. Hansen, J.A. Sorensen, Reduction of broad-band noise in speech by truncated
QSVD, IEEE Trans. Speech Audio. Process. 3 (6) (1995) 439–448.
[21] P.S.K. Hansen, Signal subspace methods for speech enhancement, Ph.D. Dissertation, Lyngby, September
1997.
[22] Y. Hu, P.C. Loizou, A subspace approach for enhancing speech corrupted by colored noise, in: Proc. IEEE
Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP), vol. 1, May 2002, pp. 573–576.
[23] Y. Hu, P.C. Loizou, A subspace approach for enhancing speech corrupted by colored noise, IEEE Signal
Process. Lett. 9 (7) (2002) 204–206.
[24] H. Lev-Ari, Y. Ephraim, Extension of the signal subspace speech enhancement approach to colored noise,
IEEE Signal Process. Lett. 10 (4) (2003) 104–106.
[25] U. Mittal, N. Phamdo, Signal/noise KLT based approach for enhancing speech degraded by colored noise,
IEEE Trans. Speech Audio Process. 8 (2) (2000) 159–167.
[26] A. Rezayee, S. Gazor, An adaptive KLT approach for speech enhancement, IEEE Trans. Speech Audio
Process. 9 (2) (2001) 87–95.
[27] Y. Hu, P.C. Loizou, A perceptually motivated approach for speech enhancement, IEEE Trans. Speech Audio
Process. 11 (5) (2003) 457–465.
[28] F. Jabloun, B. Champagne, Incorporating the human hearing properties in the signal subspace approach for
speech enhancement, IEEE Trans. Speech Audio Process. 11 (6) (2003) 700–708.
[29] J.U. Kim, S.G. Kim, C.D. Yoo, The incorporation of masking threshold to subspace speech enhancement,
in: Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP), vol. 1, April 2003, pp.
76–79.
[30] R. Hegger, H. Kantz, L. Matassini, Noise reduction for human speech signals by local projections in embed-
ding spaces, IEEE Trans. Circuits Syst. I 48 (12) (2001) 1454–1461.
[31] M.T. Johnson, A.C. Lindgren, R.J. Povinelli, X. Yuan, Performance of nonlinear speech enhancement using
phase space reconstruction, in: Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP),
vol. 1, April 2003.
[32] Y. Ephraim, Statistical-model-based speech enhancement systems, in: Proc. IEEE, vol. 80, no. 10, October
1992, pp. 1526–1555.
[33] J.D. Gibson, B. Koo, S.D. Gray, Filtering of colored noise for speech enhancement and coding, IEEE Trans.
Signal Process., vol. 39, no. 8, 1991, pp. 1732–1742.
[34] T. Kailath, An innovations approach to least-squares estimation—part I: Linear ﬁltering in additive white
noise, IEEE Trans. Autom. Control, 13 (6) (1968) 646–655.
[35] A. Yasmin, P. Fieguth, L. Deng, Speech enhancement using voice source models, in: Proc. IEEE Intl. Conf.
on Acoustics, Speech and Signal Processing (ICASSP), vol. 2, March 1999, pp. 797–800.

1050
CHAPTER 35 Enhancement
[36] G. Fant, J. Liljencrants, Q. Lin, A four-parameter model of glottal ﬂow, STL-QPSR 26 (4) (1985) 1–13.
[37] Z. Goh, K.-C. Tan, B.T.G. Tan, Kalman-ﬁltering speech enhancement method based on a voiced-unvoiced
speech model, IEEE Trans. Speech Audio Process. 7 (5) (1999) 510–524.
[38] V. Grancharov, J. Samuelsson, B. Kleijn, On causal algorithms for speech enhancement, IEEE Trans. Audio,
Speech, Lang. Process. 14 (3) (2006) 764–773.
[39] S. So, K.K. Wocicki, J.G. Lyons, A.P. Stark, K.K. Paliwal, Kalman ﬁtler with phase spectrum compensation
algorithm for speech enhancement, in: Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing
(ICASSP), 2009, pp. 4405–4408.
[40] A. Stark, K. Wojcicki, J. Lyons, K. Paliwal, Noise driven short-time phase spectrum compensation procedure
for speech enhancement, in: Proc. Intl. Conf. on Spoken Lang. Processing (ICSLP), 2008.
[41] S. Gannot, M. Moonen, On the application of the unscented Kalman ﬁlter to sppech processing, in: Proc.
Intl. Workshop Acoust. Echo Noise Control (IWAENC), Kyoto, Japan, September 2003, pp. 27–30.
[42] N. Ma, M. Bouchard, R.A. Goubran, Dual perceptually constrained unscented Kalman ﬁlter for enhancing
speech degraded by colored noise, in: Proc Intl Conf on Signal Processing, Beijing, September 2004.
[43] X. Shen, L. Deng, A dynamic system approach to speech enhancement using the h∞ﬁltering algorithm,
IEEE Trans. Speech Audio Process. 7 (4) (1999) 391–399.
[44] H. Attias, L. Deng, Speech denoising and dereverberation using probabilistic models, in: Advances in Neural
Information Processing Systems (NIPS), vol. 13, 2001, pp. 758–764.
[45] F. Soong, B. Juang, Line spectrum pair (lsp) and speech data compression, in: Proc. IEEE Intl. Conf. on
Acoustics, Speech and Signal Processing (ICASSP), vol. 9, 1984, pp. 37–40.
[46] ITU-T, Coding of Speech at 8 kbit/s using Conjugate-Structure Algebraic-Code-Excited Line-Prediction
(CS-ACELP), International Telecommunications Union (ITU-T) Recommendation G.729, March 1993.
[47] J.H.L. Hansen, M.A. Clements, Constrained iterative speech enhancement with application to speech recog-
nition, IEEE Trans. Signal Process. 39 (4) (1991) 795–805.
[48] B.L. Pellom, J.H.L. Hansen, An improved (Auto:I, LSP:T) constrained iterative speech enhancement for
colored noise environments, IEEE Trans. Speech Audio Process. 6 (6) (1998) 573–579.
[49] Y. Ephraim, D. Malah, B.-H. Juang, On the application of hidden Markov models for enhancing noisy speech,
IEEE Trans. Acoust., Speech, Signal Process. 37 (12) (1989) 1846–1856.
[50] S.B. Davis, P. Mermelstein, Comparison of parametric representations for monosyllabic word recognition
in continously spoken sentences, IEEE Trans. Acoust., Speech, Signal Process. 28 (4) (1980) 357–366.
[51] Y. Ephraim, D. Malah, B.-H. Juang, Speech enhancement based upon hidden Markov modeling, in: Proc.
IEEE Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP), May 1989, pp. 353–356.
[52] L. Deng, J. Droppo, A. Acero, Estimating cepstrum of speech under the presence of noise using a joint prior
of static and dynamic features, IEEE Trans. Speech Audio Process. 12 (3) (2004) 218–233.
[53] L. Deng, J. Droppo, A. Acero, Recursive noise estimation using iterative stochastic approximation for
stereo-based robust speech recognition, in: Proc. IEEE Workshop on Automatic Speech Recognition and
Understanding, December 2001, pp. 81–84.
[54] T. Kristjansson, J. Hershey, High resolution signal reconstruction, in: Proc. IEEE Workshop on Automatic
Speech Recognition and Understanding, December 2003, pp. 291–296.
[55] S.F. Boll, Suppression of acoustic noise in speech using spectral subtraction, IEEE Trans. Acoust., Speech,
Signal Process. ASSP-27 (2) (1979) 113–120.
[56] N. Virag, Single channel speech enhancement based on masking properties of the human auditory system,
IEEE Trans. Speech Audio Process. 7 (2) (1999) 126–137.
[57] D. Wang, J. Lim, The unimportance of phase in speech enhancement, IEEE Trans. Acoust., Speech, Signal
Process. 30 (4) (1982) 679–681.
[58] M. Berouti, R. Schwartz, J. Makhoul, Enhancement of speech corrupted by acoustic noise, in: Proc. IEEE
Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP), vol. 4, 1979, pp. 208–211.

References
1051
[59] J.S. Lim, A.V. Oppenheim, Enhancement and bandwidth compression of noisy speech, Proc. IEEE 67 (12)
1979 1586–1604.
[60] P. Lockwood, J. Boudy, Experiments with a nonlinear spectral subtractor (NSS), hidden Markov models and
the projection, for robust recognition in cars, Speech Commun. 11 (1992) 215–228.
[61] L. Lin, W.H. Holmes, E. Ambikairajah, Speech denoising using perceptual modiﬁcation of Wiener ﬁltering,
IEE Electronics Lett. 38 (23) (2002) 1486–1487.
[62] L. Lin, E. Ambikairajah, W.H. Holmes, Speech enhancement for nonstationary noise environment, in: Proc.
Asia-Paciﬁc Conf. on Circuits and Systems, vol. 1, 2002, pp. 177–180.
[63] R. McAulay, M. Malpass, Speech enhancement using a soft-decision noise suppression ﬁlter, IEEE Trans.
Acoust., Speech, Signal Process. 28 (2) (1980) 137–145.
[64] J.H.L. Hansen, Speech enhancement employing adaptive boundary detection and morphological based spec-
tral constraints, in: Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP), vol. 2,
Toronto, April 1991, pp. 901–904.
[65] Y. Ephraim, D. Malah, Speech enhancement using a minimum-mean square error short-time spectral ampli-
tude estimator, IEEE Trans. Acoust., Speech, Signal Process. 32 (6) (1984) 1109–1121.
[66] I. Cohen, On the decision-directed estimation approach of ephraim and malah, in: Proc. IEEE Intl. Conf. on
Acoustics, Speech and Signal Processing (ICASSP), vol. 1, May 2004.
[67] Y. Ephraim, D. Malah, Speech enhancement using a minimum mean-square error log-spectral amplitude
estimator, IEEE Trans. Acoust., Speech, Signal Process. 33 (2) (1985) 443–445.
[68] O. Cappe, Elimination of the musical noise phenomenon with the Ephraim and Malah noise suppressor,
IEEE Trans. Speech Audio Process. 2 (2) (1994) 345–349.
[69] K. Manohar, P. Rao, Speech enhancement in nonstationary noise environments using noise properties, Speech
Commun. 48 (1) (2006) 96–109.
[70] J.H.L. Hansen, V. Radhakrishnan, K.H. Arehart, Speech enhancement based on generalized minimum mean
square error estimators and masking properties of the auditory system, IEEE Trans. Audi., Speech, Lang.
Process. 14 (6) (2006) 2049–2063.
[71] T.Lotter,C.Benien,P.Vary,MultichannelspeechenhancementusingBayesianspectralamplitudeestimation,
in: Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP), vol. 1, April 2003.
[72] B. Chen, P.C. Loizou, Speech enhancement using a MMSE short time spectral amplitude estimator with
Laplacian speech modeling, in: Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP),
vol. 1, March 2005, pp. 1097–1100.
[73] R. Martin, Speech enhancement based on minimum mean-square error estimation and supergaussian priors,
IEEE Trans. Speech Audio Process. 13 (5) (2005) 845–856.
[74] M. Giles, A non-complementary audio noise reduction system, National Semiconductor, Application Note
386, 1985.
[75] S.J. Godsill, P.J.W. Rayner, Digital Audio Restoration: A Statistical Model Based Approach, Springer, 1998.
[76] D. Wang, G. Brown (Eds.), Computational Auditory Scene Analysis: Principles, Algorithms, and Applica-
tions, Wiley, 2006 (Online). <http://www.casabook.org>.
[77] U. Kjems, M.S. Pedersen, J.B. Boldt, T. Lunner, D. Wang, Speech intelligibility of ideal binary masked
mixtures, in: Proc. European Signal Processing Conf. (EUSIPCO), Aalborg, Denmark, August 2010, pp.
1909–1913.
[78] J. Jensen, R.C. Hendriks, Spectral magnitude minimum mean-square error estimation using binary and
continuous gain functions, IEEE Trans. Audio, Speech, Language Process. 20 (1) (2012) 92–102.
[79] D.S. Brungart, P.S. Chang, B.D. Simpson, D. Wang, Isolating the energetic component of speech-on-speech
masking with ideal time-frequency segregation, J. Acoust. Soc. Am. 120 (2006) 4007–4018.
[80] G. Hu, D.L. Wang, Monaural speech segregation based on pitch tracking and amplitude modulation, IEEE
Trans. Neural Netw. 15 (5) (2004) 1135–1150.

1052
CHAPTER 35 Enhancement
[81] P. Li, Y. Guan, B. Xu, W. Liu, Monaural speech separation based on computational auditory scene analysis
and objective quality assessment of speech, in: Proc. Intl Conf on Innovative Computing, Information and
Control, vol. 2, August 2006, pp. 742–745.
[82] M. Seltzer, B. Raj, R. Stern, A Bayesian classiﬁer for spectrographic mask estimation for missing feature
speech recognition, Speech Commun. 43 (2004) 379–393.
[83] D. Talkin, A robust algorithm for pitch tracking (RAPT), in: W.B. Kleijn, K.K. Paliwal (Eds.), Speech Coding
and Synthesis, Elsevier, Amsterdam, 1995, pp. 495–518.
[84] B. Raj, M.L. Seltzer, R.M. Stern, Reconstruction of missing features for robust speech recognition, Speech
Commun. 43 (2004) 275–296.
[85] W. Kim, R.M. Stern, Band-independent mask estimation for missing-feature reconstruction in the presence
of unknown background noise, in: Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing
(ICASSP), vol. 1, May 2006, pp. 305–308.
[86] R.E. Crochiere, A weighted overlap-add method for short-time Fourier analysis/synthesis, IEEE Trans.
Acoust., Speech, Signal Process. 28 (1) (1980) 99–102.
[87] J. Allen, L. Rabiner, A uniﬁed approach to short-time Fourier analysis and synthesis, Proc. IEEE 65 (11)
(1977) 1558–1564.
[88] J.B. Allen, Short term spectral analysis, synthesis, and modiﬁcation by discrete Fourier transform, IEEE
Trans. Acoust., Speech, Signal Process. 25 (3) (1977) 235–238.
[89] R. Martin, R.V. Cox, New speech enhancement techniques for low bit rate speech coding, in: Proc. IEEE
Workshop on Speech Coding, June 1999, pp. 165–167.
[90] J. Roberts, Modiﬁcation to piecewise LPC-10E, MITRE, Tech. Rep. WP-21752, 1978.
[91] D.V. Compernolle, Noise adaptation in a hidden Markov model speech recognition system, Comput. Speech
Lang. 3 (1989) 151–167.
[92] H.G. Hirsch, C. Ehrlicher, Noise estimation techniques for robust speech recognition, in: Proc. IEEE Intl.
Conf. on Acoustics, Speech and Signal Processing (ICASSP), vol. 1, 1995, pp. 153–156.
[93] D.K. Freeman, G. Cosier, C.B. Southcott, I. Boyd, The voice activity detector for the pan-european digital
cellular mobile telephone service, in: Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing
(ICASSP), May 1989, pp. 369–372.
[94] J. Yang, Frequency domain noise suppression approaches in mobile telephone systems, in: Proc. IEEE Intl.
Conf. on Acoustics, Speech and Signal Processing (ICASSP), vol. 2, April 1993, pp. 363–366.
[95] W. Zhang, S. Gazor, Statistical modeling of speech signals, in: Proc. Intl. Conf. on Signal Processing, vol.
1, August 2002, pp. 480–483.
[96] S. Gazor, W. Zhang, Speech probability distribution, IEEE Signal Process. Lett. 10 (7) (2003) 204–207.
[97] S. Gazor, W. Zhang, A soft voice activity detector based on a Laplacian-Gaussian model, IEEE Trans. Speech
Audio Process. 11(5) (2003) 498–505.
[98] M. Marzinzik , B. Kollmeier, Speech pause detection for noise spectrum estimation by tracking power
envelope dynamics, IEEE Trans. Speech Audio Process. 10 (2) (2002) 109–118.
[99] ETSI, Speech processing, transmission and quality aspects (STQ); distributed speech recognition; advanced
front-end feature extraction algorithm; compression algorithms, ETSI, ETSI Standard ES 202 050, January
2007.
[100] Z. Lin, R.A. Goubran, R.M. Dansereau, Noise estimation using speech/non-speech frame decision and
subband spectral tracking, Speech Commun. 49 (7–8) (2007) 542–557.
[101] I. Cohen, B. Berdugo, Noise estimation by minima controlled recursive averaging for robust speech enhance-
ment, IEEE Signal Process. Lett. 9 (1) (2002) 12–15.
[102] Z. Lin, R. Goubran, Instant noise estimation using Fourier transform of AMDF and variable start minima
search, in: Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP), vol. 1, 2005.

References
1053
[103] J. Sohn, W. Sung, A voice activity detector employing soft decision based noise spectrum adaptation, in: Proc.
IEEE Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP), vol. 1, May 1998, pp. 365–368.
[104] J. Shore, Minimum cross-entropy spectral analysis, IEEE Trans. Acoust., Speech, Signal Process. 29 (2)
(1981) 230–237.
[105] R. Gray, A. Gray, G. Rebolledo, J. Shore, Rate-distortion speech coding with a minimum discrimination
information distortion measure, IEEE Trans. Inf. Theory 27 (6) (1981) 708–721.
[106] D. Malah, R.V. Cox, A.J. Accardi, Tracking speech-presence uncertainty to improve speech enhancement in
non-stationary noise environments, in: Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing
(ICASSP), vol. 2, March 1999, pp. 789–792.
[107] L. Lin, W.H. Holmes, E. Ambikairajah, Subband noise estimation for speech enhancement using a perceptual
Wiener ﬁlter, in: Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP), vol. 1, 2003,
pp. 80–83.
[108] I. Cohen, Noise spectrum estimation in adverse environments: Improved minima controlled recursive aver-
aging, IEEE Trans. Speech Audio Process. 11 (5) (2003) 466–475.
[109] S. Rangachari, P.C. Loizou, A noise-estimation algorithm for highly non-stationary environments, Speech
Commun. 48 (2) (2006) 220–231.
[110] A. Sugiyama, H.P. Hua, M. Kato, M. Serizawa, Noise suppression with synthesis windowing and pseudo
noise injection, in: Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP), vol. 1,
2002, pp. 545–548.
[111] M. Kato, A. Sugiyama, M. Serizawa, A family of 3GPP-standard noise suppressors for the AMR codec and
the evaluation results, in: Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP), vol.
1, April 2003.
[112] M. Kato, A. Sugiyama, M. Serizawa, Noise suppression with high speech quality based on weighted noise
estimation and MMSE STSA, Electron. Commun. Jpn (Part III: Fundamental Electronic Science), 89 (2)
(2006) 43–53.
[113] T. Gerkmann, R.C. Hendriks, Unbiased MMSE-based noise power estimation with low complexity and low
tracking delay, IEEE Trans. Audio, Speech, Lang. Process. 20 (4) (2012) 1383 –1393.
[114] S. Rennie, T. Kristjansson, P. Olsen, R. Gopinath, Dynamic noise adaptation, in: Proc. IEEE Intl. Conf. on
Acoustics, Speech and Signal Processing (ICASSP), vol. 1, May 2006.
[115] H. Sameti, H. Sheikhzadeh, L. Deng, R.L. Brennan, HMM-based strategies for enhancement of speech
signals embedded in nonstationary noise, IEEE Trans. Speech Audio Process. 6 (5) (1998) 445–455.
[116] J.Bai,M.Brookes,AdaptivehiddenMarkovmodels for noisemodeling,in: Proc.EuropeanSignal Processing
Conf. (EUSIPCO), Barcelona, August 2011.
[117] R. Martin, Spectral subtraction based on minimum statistics, in: Proc. European Signal Processing Conf,
1994, 1182–1185.
[118] R. Martin, Noise power spectral density estimation based on optimal smoothing and minimum statistics,
IEEE Trans. Speech Audio Process. 9 (2001) 504–512.
[119] R. Martin, Bias compensation methods for minimum statistics noise power spectral density estimation, Signal
Process. 86 (6) (2006) 1215–1229.
[120] V. Stahl, A. Fischer, R. Bippus, Quantile based noise estimation for spectral subtraction and Wiener ﬁltering,
in: Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP), vol. 3, 2000, pp. 1875–
1878.
[121] M.G. Christensen, A. Jakobsson, Multi-Pitch Estimation, Ser. Synthesis Lectures on Speech and Audio
Processing, Morgan & Claypool, 2009.
[122] A. de Cheveigne, H. Kawahara, YIN, a fundamental frequency estimator for speech and music, J. Acoust.
Soc. Am. 111 (4) (2002) 1917–1930.

1054
CHAPTER 35 Enhancement
[123] S. Gonzalez , M. Brookes, A pitch estimation ﬁlter robust to high levels of noise (PEFAC), in: Proc. European
Signal Processing Conf. (EUSIPCO), Barcelona, August 2011.
[124] Z. Jin, D.L. Wang, HMM-based multipitch tracking for noisy and reverberant speech, IEEE Trans. Audio,
Speech, Lang. Process. 19 (5) (2011) 1091–1102.
[125] P.C. Loizou, Speech Enhancement Theory and Practice, Taylor & Francis, 2007.
[126] H. Fletcher, J. Steinberg, Articulation testing methods, Bell Syst. Tech. J. 8 (1929) 806–854.
[127] G. Miller, P. Nicely, An analysis of perceptual confusions among some English consonants, J. Acoust. Soc.
Am. 27 (2) (1955) 338–352.
[128] W.D. Voiers, Evaluating processed speech using the diagnostic rhyme test, Speech Technol, 1 (4) (1983)
30–39.
[129] M. Nilsson, S. Soli, J. Sullivan, Development of hearing in noise test for the measurement of speech reception
thresholds in quiet and in noise, J. Acoust. Soc. Am. 95 (2) (1994) 1085–1099.
[130] R.J. Bench, J. Bamford (Eds.), Speech/Hearing Tests And The Spoken Language Of Hearing-impaired
Children, Academic Press, 1979.
[131] J. Bench, Å. Kowal, J. Bamford, The bkb (bamford-kowal-bench) sentence lists for partially-hearing children,
Brit. J. Audiol. 13 (3) (1979) 108–112.
[132] E.H. Rothauser, W.D. Chapman, N. Guttman, M.H.L. Hecker, K.S. Nordby, H.R. Silbiger, G.E. Urbanek, M.
Weinstock, IEEE recommended practice for speech quality measurements, IEEE Trans. Audi. Electroacoust.
17 (3) (1969) 225–246.
[133] B. Hagerman, Sentences for testing speech intelligibility in noise, Scandinavian Audiol. 11 (2) (1982) 79–87.
[134] R. Plomp, A. Mimpen, Speech-reception threshold for sentences as a function of age and noise level, J.
Acoust. Soc. Am. 66 (5) (1979) 1333–1342.
[135] G. Hilkhuysen, N. Gaubitch, M. Brookes, M. Huckvale, Effects of noise suppression on intelligibility:
dependency on signal-to-noise ratios, J. Acoust. Soc. Am. 131 (1) (2012) 531–539.
[136] K. Worrall, R. Fellows, J. Causer, L. Craigie, Intelligibility testing at HM Government Communications
Centre, Proc. Institute of Acoustics 28 (6) (2006) 12.
[137] H. Levitt, Transformed up-down methods in psychoacoustics, J. Acoust. Soc. Am. 49 (2) (1971) 467–477.
[138] S.A. Klein, Measuring, estimating, and understanding the psychometric function: a commentary, Perception
& Psychophysics 63 (8) (2001) 1421–1455.
[139] N.D. Gaubitch, M. Brookes, P.A. Naylor, D. Sharma, Bayesian adaptive method for estimating speech
intelligibility in noise, in: Proc AES Conf on Audio Forensics, Hillerød, Denmark, June 2010.
[140] K. Kryter, Methods for the calculation and use of the articulation index, J. Acoust. Soc. Am. 34 (11) (1962)
1689–1697.
[141] ANSI, Methods for the Calculation of the Articulation Index, American National Standards Institute, New
York, ANSI Standard ANSI, S3.5–1969, 1969.
[142] ANSI, Methods for the calculation of the speech intelligibility index, American National Standards Institute,
ANSI Standard S3.5–1997 (R2007), 1997.
[143] K.S. Rhebergen, N.J. Versfeld, A speech intelligibility index-based approach to predict the speech reception
threshold for sentences in ﬂuctuating noise for normal-hearing listeners, J. Acoust. Soc. Am. 117 (4) (2005)
2181–2192.
[144] K.S. Rhebergen, N.J. Versfeld, W.A. Dreschler, Extended speech intelligibility index for the prediction of
the speech reception threshold in ﬂuctuating noise, J. Acoust. Soc. Am. 120 (2006) 3988–3997.
[145] P.C. Loizou, J. Ma, Extending the articulation index to account for non-linear distortions introduced by
noise-suppression algorithms, J. Acoust. Soc. Am. 130 (2011) 986–995.
[146] S. Jørgensen, T. Dau, Predicting speech intelligibility based on the signal-to-noise envelope power ratio after
modulation-frequency selective processing, 130 (3) (2011) 1475–1487.

References
1055
[147] C.H. Taal, R.C. Hendriks, R. Heusdens, J. Jensen, A short-time objecitve intelligibility measure for time-
frequency weighted noisy speech, in: Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing
(ICASSP), 2010, pp. 4214–4217.
[148] W.M. Liu, K.A. Jellyman, J.S.D. Mason, N.W.D. Evans, Assessment of objective quality measures for speech
intelligibility estimation, in: Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP),
vol. I, 2006, pp. 1225–1228.
[149] T. Yamada, M. Kumakura, N. Kitawaki, Word intelligibility estimation of noise-reduced speech, in: Proc.
Interspeech Conf., Pittsburgh, Pennsylvania, 2006, pp. 169–172.
[150] C. Chernick, S. Leigh, K. Mills, R. Toense, Testing the ability of speech recognizers to measure the effec-
tiveness of encoding algorithms for digital speech transmission, in: Proc. Military Communications Conf,
vol. 2, October 1999, pp. 1468–1472.
[151] V. Grancharov, D.Y. Zhao, J. Lindblom, W.B. Kleijn, Low-complexity, nonintrusive speech quality assess-
ment, IEEE Trans. Audio, Speech, Lang. Process. 14 (6) (2006) 1948–1956.
[152] D. Sharma, G. Hilkhuysen, N.D. Gaubitch, P.A. Naylor, M. Brookes, M. Huckvale, Data driven method
for non-intrusive speech intelligibility estimation, in: Proc. European Signal Processing Conf. (EUSIPCO),
Denmark, August 2010.
[153] Y. Vazquez-Alvarez, M. Huckvale, The reliability of the ITU-P.85 standard for the evaluation of text-to-
speech systems, in: Proc. Intl. Conf. on Spoken Lang. Processing (ICSLP), 2002, pp. 329–332.
[154] ITU-T, Subjective assessment of sound quality, International Telecommunications Union (ITU-R) Recom-
mendation BS. 562–3, 1990.
[155] ITU-T, Subjective performance evaluation of telephone band and wideband codecs, International Telecom-
munications Union (ITU-T) Recommendation P.830, 1998.
[156] W. Voiers, Diagnostic acceptability measure for speech communication systems, in: Proc. IEEE Intl. Conf.
on Acoustics, Speech and Signal Processing (ICASSP), 1977, pp. 204–207.
[157] Subjective test methodology for evaluating speech communication systems that include noise suppression
algorithms, International Telecommunications Union (IT-TU) Recommendation P.835, November 2003.
[158] S.R. Quackenbush, T.P. Barnwell, III, M.A. Clements, Objective Measures of Speech Quality, Prentice Hall,
January 1988.
[159] K. Brandenburg, Evaluation of quality for audio encoding at low bit rates, in: Proc. Audio Eng. Soc. Con-
vention, no. 2433 February 1987.
[160] M.R. Schroeder, B.S. Atal, J.L. Hall, Optimizing speech coders by exploiting masking properties of the
human ear, J. Acoust. Soc. Am. 66 (6) (1979) 1647–1652.
[161] A. Rix, J. Beerends, M. Hollier, A. Hekstra, Perceptual evaluation of speech quality (PESQ) - a new method
for speech quality assessment of telephone networks and codecs, in: Proc. IEEE Intl. Conf. on Acoustics,
Speech and Signal Processing (ICASSP), 2001, pp. 749–752.
[162] ITU-T, Perceptual evaluation of speech quality (PESQ), an objective method for end-to-end speech quality
assessment of narrowband telephone networks and speech codecs, International Telecommunications Union
(ITU-T) Recommendation P.862, February 2001.
[163] N. Kitawaki, T. Yamada, Subjective and objective quality assessment for noise reduced speech, in: ETSI
Workshop on Speech and Noise in Wideband Communication, Sophia Antipolis, France, May 2007.
[164] ITU-T, Perceptual objective listening quality assessment: an advanced objective perceptual method for end-
to-endlisteningspeechqualityevaluationof ﬁxed, mobile, andIP-basednetworks andspeechcodecs covering
narrowband, wideband, and super-wideband signals, Standard P.863, January 2011.
[165] Y. Hu, P.C. Loizou, Evaluation of objective measures for speech enhancement, in: Proc. Interspeech Conf.
2006, pp. 1447–1450.
[166] A. Rix, Perceptual speech quality assessment—a review, in: Proc. IEEE Intl. Conf. on Acoustics, Speech
and Signal Processing (ICASSP), vol. 3, 2004, pp. 1056–1059.

1056
CHAPTER 35 Enhancement
[167] ITU-T, Single-ended method for objective speech quality assessment in narrow-band telphony applications,
International Telecommunications Union (ITU-T) Recommendation P.563, 2004.
[168] L. Malfait, J. Berger, M. Kastner, P.563—the ITU-T standard for single-ended speech quality assessment,
IEEE Trans. Audio, Speech, Lang. Process. 14 (6) (2006) 1924–1934.
[169] D. Sharma, P.A. Naylor, N. Gaubitch, M. Brookes, Short-time objective assessment of speech quality, in:
Proc. European Signal Processing Conf. (EUSIPCO), Barcelona, August 2011.
[170] D.-S. Kim, M. Tarraf, Enhanced perceptual model for non-intrusive speech quality assessment, in: Proc.
IEEE Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP), vol. 1, 2006.

1057
A
Acoustic echo control (AEC)
adaptive algorithms, 829, 854
black box representation, 810
convergence enhancement, 848
decorrelation properties, 848
defined, 808–809
distortion products, 848, 853
linear combination, 822
multi-channel scenario, 844
non-uniqueness problem, 845
phase modulation method, 863
power filter, 855–856
psychoacoustics, 847
room impulse response, 832
second-order statistics, 847
signal processing types, 809
separation problems, 814
TRINICON optimization criterion, 846, 850
Volterra filter, 857
Acoustic echo signal
echo attenuation, 822
room impulse response, 833
Acoustic echo suppression (AES)
background noise, 827
echo estimate, 833
path estimation, 835
perceptual, 840
stationary noise, 840
Acoustic signal processing
application, 804
devices, 803–804
echo control, 804
sound field synthesis, 804
Acoustic wave equation, 920
Advanced correlation filters, 405
matched filter (MF), 405
PCE and PSR, 405
Affine Projection Algorithm (APA), 810–812
Aging, 328
AD, 328
MCI, 328
Algorithm. see specific entries
Alzheimer’s disease neuroimaging initiative  
(ADNI), 329
goal of, 329
limitations of, 329
Amplitude-modulated (AM) halftones, 135
Analog approximation problem,  
digital filters
sampling rate mismatch, 848
And-Or graph, 273
α and β inference processes, 273
part-based representation, 273
Animat vision approach, 611
Anthropometric face measures, 277, 283
Application of image based rendering, 519
camera calibration, 522
IBVH, 520
motivation and background, 519
post-processing and video  
broadcasting, 524
rendering, 524
scene depth estimation, 522
system architecture, 521
Applications of foveated images and  
video, 386
detection and search, 387
infrared and wide band imaging, 387
super high resolution images, 387
teleconferencing, 386
teleoperation, 386
Applications of video analysis, 345, 473
computational photography, 346
datasets, 481
in biomedical, 346
mobile communications, 346
registration, 479
segmentation datasets, 482
segmentation, 477
social media and internet, 346
support vector machine (SVM), 476
synthesis and editing, 474
video surveillance, 345
virtual reality, 346
vision based robotics, 346
Arbitrary topology, 692
with limited communication  
range, 692
dynamic sensor fusion, 693
with limited communication range, 693
with message losses, 697
Audio coding. see also Perceptual  
audio coding
aims, 710–711
composition properties, 778
Index

1058
Index
Audio coding (Continued)
deployment of, 711
enhancements merits, 773–774
error resilience, 784
first-order modeling, 760
international standards, 780
joint stereo coding, 766
lossless or near lossless, 788
masking phenomena, 759
monophonic, 761
MPEG surround, 789
MPEG-2 AAC, 783
MPEG-4, 783–784
MPEG-D SAOC, technical features,  778, 790
multichannel coding parameters, 774, 781
object-oriented spatial audio coding, 777–778
redundancy concept in, 758
SBR process limitation, 773
scalability, 711, 779
spatial coding, 711, 774–776
traditional methods, 774–775
transmitted message, 758
unified speech and audio coding (USAC), 792
Audio compression, 780, 788, 793–794
Audio data rate reduction
channel decoding, 779
in perceptual auto coding, 757
local variations, 766
lossless coding, 788
quantization, filter banks, 762–763
of redundancy, 758
single radio channel, 766
target, 780
Audio objects, parametric coding, 777
Audio signal processing
audio and acoustics, 709
transmission of, 709
Augmented reality (AR), 85
Autism spectrum disorder (ASD), 328
Asperger’s disorder, 328
autism, 328
PDD, 328
PDD-NOS, 328
AutoRegressive eXogenous (ARX) model, 478
Average of synthetic exact filters (ASEF), 406, 418
disadvantage of, 418
B
Baltimore longitudinal study of aging (BLSA), 329
neuroimaging sub-study of, 329
Barcode/QR code scanning, 260
Barrel distortion, 18
Bayesian inferences, 269
Bayesian paradigm, 294
modeling, 294
optimization, 296
Benton visual retention test (BVRT), 336–337
Berkeley dataset, 283
Bhattacharya coefficient, 635
Bi-directional telecommunication, coding  
delay, 765
Binet-Cauchy kernels on LDSs, 472
Biometrics, 403
Bitrate reduction
bandwidth extension, 771, 773
broadcast quality, 780–781
compression efficiency, 778
frequency range, 770
high-quality multichannels, 774
pulse code modulation (PCM) data, 757
residual coding, 776
SBR/MPS combination, 793
scalable coding, 779
signal-to-noise-ratio (SNR), 760
spectral flatness measure (SFM), 762–763
stereo coding control, 770
studio-quality applications, 783–784
variable rate coding, 765
Blue-noise halftoning, 140
C
California verbal learning test (CVLT), 336–337
Cascaded Confidence Filtering (CCF), 573
Chien model, 302–303
Chroma-based audio features, 721
Chromatic aberration, 19
Circular Harmonic Function (CHF) filters, 406
Clapper-Yule model, 148
Classical linear system theory
acoustic echo control, 805
Coarse-to-fine approach, 274
Coded aperture imaging systems, 592
modified uniformly redundant arrays (MURA)  
codes, 592–593
Coded aperture snapshot spectral imager  
(CASSI), 599
Amici prism, 599–600
hyper-spectral cube, recovery, 599–600
imaging architecture of, 599–600
Coded aperture video camera, 592
Coded exposure video camera, 600
CMOS image sensors, 600–601

1059
Index
Coded photograph exposure video (CPEV), 590–592
Coded strobing camera, 597–598
Color face detection, 272
Color filter array (CFA) image, 13–14
Common multiplexing constraints, 594
binary modulation, 594
energy constraint, 594
positivity constraint, 594
Comparison of architectures, 594
hardware complexity, 594
multiplexing matrix, conditioning of, 594
Compressive measurements, 584
Compressive sensing (CS), 583
measurement matrix, 583
phase transition lines, 586
recovery, 586
RIP, 585
signal recovery, 585
signal, 584
sparse optimization algorithms, 587
Compressive sensing (CS)
imaging architectures. See imaging architectures
measurement process, 584
notation, 583
restricted isometry property (RIP), 585
signal models and algorithms, 594
dictionary learning, 596
motion flow, 596
total variation, 595
wavelet sparsity, 595
signal recovery, 585
equivalence, 585
recovery guarantees for compressible signals, 586
sparse optimization algorithms, 587
sparse signals reconstruction, 584
video compressive sensing. See video compressive sensing
Computational photography, 346
Computer vision
and pattern recognition, 260
optical character recognition (OCR), 260
Consumer digital cameras, image quality in
action photography, 12
applications, 71
camera engineering, 16
chromatic aberration, 19
geometric distortion, 17
modulation transfer function (MTF), 23
non-telecentric lens, 21
optical issues, 17
resource limitations, 33
response nonuniformity, 20
rolling shutter mechanism, 28
sensitivity and noise, 32
sensor sampling, 25
shutter duty cycle, 27–28
spectral sensitivity, 29
telecentric lens, 21
temporal sampling, 27
camera-specific algorithms, 63
dark correction, 65
defect concealment, 67
geometric and optical corrections, 64
nonuniformities, correction of, 70
color correction, 48
commercial photography, 11
compact cameras, 12
datasets, 74
digital camera image processing chain, 13
 “A*” algorithms, 14
automatic white balance operation, 14–15
color and tone correction, 15
color filter array (CFA) image, 13–14
demosaicking, 15
edge enhancement, 15–16
frequency domain techniques, 15
noise reduction, 14
resizing, 16
image quality, 12–13
implementations, 73
integrated hyperbolic increment function (IHIF), 35
issues and future directions, 72
processing methods and algorithms, 40
autoexposure, 43
autofocus, 41
automatic white balance, 46
camera control algorithms (A*), 41
color and tone, 47
improving images, 40
quality modeling, 34
spatial processing, 53
compression, 61
demosaicking, 55
edge enhancement, 57
noise reduction, 59
resizing, 54
system interactions, 38
tone scaling, 50
video capture, 12
Convolutional face finder, 282, 284–287
ROC curves obtained, 284
Correlation filters (CFs), 403
action MACH filter, 446
adaptive, 441
ECPSDF, 408
experiments in, 447
for videos, 441

1060
Index
Correlation filters (Continued)
frame to frame, 441
Kalman Correlation Filter (KCF), 442
matched filter (MF) and efficient  
computation of, 407
MFCF, 441
use for, 404
Correlation output, normalization, 437
peak-to-correlation energy (PCE), 438
peak-to-sidelobe ratio (PSR), 439
zero-mean and unit energy test chips, 437
Covariance analysis
AR parameters, 998
autocorrelation method, 998
vs correlation, 1000
duration problem, 1000
glottal closure and opening instants (GCI/GOI), 1005
time varying parameters, 1001
D
Data structures of large surveillance videos, 542
absence of activity, 542
storage requirements, 543
Datasets for video analysis, dynamical system, 481
Dyntex, 482
highway traffic dataset, 481
UCLA-50, 481
UCLA-8 and UCLA-9, 481
Datasets, 287
CMU+MIT, 287
FERET, 287
Decision tree, 273
feature evaluations per location, 273
Deformable template, 269
Demosaicking, 15, 55
Depth-image-based rendering, 514
coordinate transfer, 517
depth sensors and camera array integration, 517
kinect, color/depth camera calibration, 515
Dereverberation
acoustic channel base, 899
blind SIMO acoustic system identification, 900
cross-relation approaches, 901
film production, 881
hands-free speech telecommunication, 880
hearing aids, 881
inverse filtering, 902–903
music production, 881
research and development in, 879–880
spectral subtraction methods, 898
Detector cascade, 273
Deterministic signals
Diagonal coordinate representation (DCR), 858
Diagonal direct subpixel-based down-sampling  
(DDSD), 103
Digital image processing steps, 81
color balancing, 82
data compression, 84
noise removal and image enhancement, 83
OCR pre-processing, 85
Segmentation, classification, recognition, 85
Digital imaging, 3
Berkner speculates, 5
digital camera quality issues, 4
displays, 5
document imaging, review of, 5
image capture, 4
multispectral document capture, application of, 5
printing, 6
restoration, 8
classical image restoration, 8
iterative restoration, 8
small mobile
displays, 7
image processing, 7
Direct pixel-based down-sampling (DPD), 101
Direct subpixel-based down-sampling (DSD), 102
Discrete Fourier Transform (DFT)
2-D discrete Fourier transforms (DFTs), 403–404  
convergence properties, 865
convolution, 824
input correlation matrix, 847
state-space model, 832
Disease heterogeneity, 334
dictionary learning based approach, 334
exploring of, 334
PFC, 335
Distance Classifier Correlation Filter  
(DCCF), 406, 428
Distributed estimation, 675
static sensor fusion, 675
combining estimators, 676
dynamic sensor fusion, 679
transmitting local estimates, 679
distributed Kalman filtering, 681
arbitrary topology, 692
on computational complexity, 699
on communication cost, 701
Distributed Kalman filtering, 681
Distributed smart cameras
calibration
algorithms, 636
color, 637

1061
Index
Distributed smart cameras (Continued)
intrinsic and extrinsic, 635–636
RANSAC, 636–637
temporal, 636
design challenges, 633
gesture recognition, 637
overlapping fields-of-view tracking, 638
sparse camera networks tracking. see sparse  
camera networks tracking
techniques in computer vision, 633
tracking
overlapping fields-of-view, 638
sparse camera networks, 638
Dynamic cascade, 284–287
Dynamic sensor fusion, 679
transmitting local estimates, 679
arbitrary topology, 693
Dynamic exture segmentation, 477
Dynamical models, 470
distances based on subspace angles, 470
Kernels between LDSs, 472
Kullback-Leibler divergence, 471
E
Echo cancellation
adaptive filters, 810, 813
application,  812, 863
convergence problems, 857–858
deterministic case, 826
echo attenuation, 860
error nonlinearity, 813–814
hardware setup, 852
in multiple reproduction channel, 819, 843
nonlinear context, 851
postfiltering, 819
signal description, 844
sparse regression, 812
subjective quality, 816–817
Echo estimation filter (EEF), 834
computation, 835, 838–839, 862
Echo return loss enhancement (ERLE) , 817–818
Echo suppression filter (ESF), 834
computation, 836–838
defined, 834
echo estimate, 839, 862–863
partition of, 840–841
Edge enhancement, 15–16, 57
Edge fragments, 272
Equal Correlation Peak Synthetic Discriminant  
Function (ECPSDF) filter, 405, 408
Equivalent orthogonal structure (EOS)
channel convergence, 866
power filter performance, 866–867
time-domain implementation, 866–867
Error signal
adaptation algorithm, 844–845
echo cancellation, 826
filter coefficients, 812
form loudspeaker, 846
noise covariance, 831
statistical properties, 826, 854
Euclidean calculus, 268–269
Expectation maximization (EM) algorithm, 478
Experimental validation, 282
Explicit Occlusion Model (EOM), 575
Eye detectors, 283
F
Face detection algorithm, 275
using MSL, 275, 277
Face orientation, 276–277
FERET dataset, 287
Feynman’s idea
on infinitesimal machinery, 250
CCDs, 250
win at the start, 250
CMOS, 250
catch up, 251
Field programmable gate arrays (FPGAs), 643
architecture of, 644
dynamic reconfiguration techniques in
commercial tools, 650
dynamic hardware/software partitioning, 650
field programmable function array (FPFA), 650
hybrid architectures, 649
JBits SDK, 650
Nimble framework, 650–651
overhead minization methods, 651
partial reconfiguration technology,  644–646, 649
schedule, 650
software programmable reconfiguration, 650
foreground (FB)/background (BG) extraction, 666
device utilization summary, 668
MATLAB, 667
PSDFmodel, 664
video sequence, selected frames from, 668–669
hardware mapping. See hardware mapping
island-style routing architecture, 645–646
model-based design of signal processing systems
cyclo-static dataflow (CSDF), 647–648
designers of signal processing systems, 647
Kahn process networks (KPNs), 646–647
scheduling, 647

1062
Index
Field programmable gate arrays (Continued)
synchronous dataflow (SDF), 647
modeling dynamic reconfiguration using, 651
downsampler, PSDF-based, 652
FPGA/ASIC design flow, 653
graphs execution, 652
HDL programs, 654
mapping phase, 654
modeling phase, 654
parameterized dataflow, 648
phase-shift keying (PSK), reconfigurable
DSG, 661–663
hardware mapping for modulator, 661–662
modulator and demodulator, 661
modulator system compared, 661–662
PSDF-based models of, 660–661
reconfigurable PSK modulator comparison, 662–664
programmable logic blocks, 644–645
programmable switch, 645–646
Filtering based preconditioning, 221
Finite impulse response (FIR)
adaptation algorithm, 844–845
cascaded structure, 854
echo suppression, 832
filter structure, 809–810
linear system, 852–853
Fisher’s discriminant analysis, 268
linear projections, 268
Fixation selection, 381
human fixation prediction, 381
search and detection, 385
video compression, 384
Flexible voxels camera, 601–602
Flexible voxels, 601
Flexographic printing, 120
Floatboost, 282
ROC curves obtained, 284
Fluoroscopy, 282
Flutter shutter video camera (FSVC), 589, 591, 603
architecture, 589–590
Four parameter face detection, 276, 278
Foveated images and video, 367
applications, 386
compression, form of, 367–368
conversion functions, 391
data sets, 395
filtering, 372, 393
fixation selection, 381
hardware foveation, 380
implementation/code, 389
motion estimation and compensation, 376
multiple fixation points, 379
open issues and problems, 388
quality assessment algorithms, 391
quality assessment, 368
rate control, 378
Foveated mean square error (FMSE), 369
Foveated peak signal to noise ratio (FPSNR), 369
Foveated wavelet quality index (FWQI), 369
Foveation filtering, 349, 372
irregular sampling and reconstruction, 372
spatial warping, 372
spatially varying low-pass filters, 373
Frequency modulated (FM) halftoning, 137–138
Frequency-domain adaptive filter (FDAF)
partitioned block, 812
power filter model, 857
state-space, 832
step-size control, 855
Frequency-plot inspection, 183
Full matches of large surveillance videos, 546
dynamic programming, 554
greedy algorithm, 547
HDP, 554
theorem, 547
with dynamic programming (DP), 548
G
Gamut mapping, 127
boundaries, determination of, 128
media, determination of, 127
segment maxima method, 128
black point compensation, 130
color media, 127
flexible sequential line gamut boundary  
(FSLGB), 128–129
gamut boundary descriptor (GBD) matrix, 128
gamut mapping algorithms (GMA), 129
media-relative workflow, 130
Gaussian MACE (GMACE) filter, 414
Gaussian models, 634–635
Gaussian pyramid, 275
GCI detection
application, 1007
closed-phase covariance analysis, 1005
performance evaluation, 1011–1012
postprocessing, 1011
preprocessing, 1008
Generative statistical models, 268
Geometric distortion, 17
Gibbs field, 295, 302
texture modeling, 304
Global mixing, 589
Grab-cut, 279

1063
Index
Gray component replacement (GCR), 132
Greedy algorithms, 587
Guide-wire localization work, 282
H
Haar features, 276, 283
Halftoning, 134
Hammersley-Clifford theorem, 295
Hardware and software systems
3D-IC. See three dimensional integrated  
circuit (3D-IC))
communication over computing delay, 630–631
power dissipation, 631
temperature, 631
Hardware description language (HDL) programs, 654
Hardware mapping, 654
modular. See modular mapping
schedule-based (See schedule-based mapping)
Hidden Markov Models (HMM), 571
Hierarchical Dirichelet Processes (HDP), 551–553
Highest confidence first (HCF) algorithm, 272
Histograms of Oriented Gradients (HOG), 570
Homomorphic deconvolution, 184
Human-computer interaction, 257
mobility, 261
impact of miniaturization, 261–262
integrate sensing and processing, 262
support research via development, 262
Human network interaction, 260
mobile verification, 260
touch-to-know, 261
who-is-nearby, 261
Human visual system, 353
and natural scene statistics, 366
contrast sensitivity function, 363
eccentricity, 361
efficient coding hypothesis, 366
eye, anatomy, 353
gabor filters, 361
information theory and entropy, 365
optical filtering, 360
optics of eye, 354
photoreceptor sampling, 360
photoreceptors, 356–357
primary visual cortex, 358
psychophysics, 359
retina and fovea, 355
retinal ganglion cells, 357–358, 360–361
spatial frequency measurements, 362
Hyper-spectral video camera, 593
architectures, 593–594
multiplexing, 593–594
spatial amplitude mask, 593–594
I
Image analysis research, 267
probabilistic approach in, 293
Image and document capture
bar code scanners, 87
conventional document capture processing, 80
scanners, 80–81
stationary camera based scanning technology, 80–81
data capture via novel sensor multiplexing  
techniques, 88–89
data sets and open source code, 90
de-multiplexing techniques, 88–90
document analysis algorithms, adaptive  
learning for, 86
future trends, 87, 90
image capture pipeline, 81
mobile phone, 87–88
multispectral analysis of documents, 86
OCR pre-processing, 85–86
Visual Search technology, 87, 90
Image deconvolution, 300
Image denoising, 300
Image features, 268–269
histograms, 268–269
shapes, 268–269
subspaces, 268–269
Image modeling, 269
Bayesian inferences, 269
Markov models, 269
Image processing, 329
at fingertips, 252
mobile imaging meets mobile computing, 252
intelligent image acquisition, 252
interactive image matting, 253
dynamic image mosaicing, 254
supervised image restoration, 256
conventional face recognition, 257–258
MR brain image, 329
Image restoration, 165, 190
algorithms, 170
Bayesian solution, 180
constrained least squares, 174
inverse filter, 170
regularized least squares, 176
Wiener filter, 172
artificial boundaries, 181
blur identification, 182
frequency-plot inspection, 183

1064
Index
Image restoration (Continued)
homomorphic deconvolution, 184
parametric methods, 186
blur model, 167
long-term atmospheric turbulence, 168
out-of-focus lens, 168
uniform linear motion, 168
boundary effects, 181
discretization, 169
dual smoothing methods, 187
regularization methods, 188
Wiener-type iteration, 188
FFT-based algorithm, 181
noise models, 170
observation model, 165
sensor nonlinearity, 168
Image-based rendering (IBR), 489
Image-based visual hull (IBVH), 505–506, 520
Imaging architectures, 588
classification of, 588
coded aperture video camera, 592
common multiplexing constraints, 594
comparison of, 594
flutter shutter video camera, 589, 591
hyper-spectral video camera, 593
P2C2, 22, 592–593
SPC, 589–590
voxel sub-sampling camera, 590–591
Indexing of large surveillance videos, 540
hash-based inverted index, 541
LSH, 541
mapping and retrieval, 536
Integral imaging, 490
LCD display, 491–492
Lippmann’s idea, 490
mirror array, 492–493
omni-directional cameras, 493
pin-hole array, 491
refractive sphere array, 494
spherical mirror array, 494
Integration, 249
Intelligent image acquisition, 252
Interactive image matting, 253
Invariant eye detection, 275
Invariant features, 271
Inverse filtering, dereverberation
alternative approaches, 903
channel shortening, 903
exact design, 902
tasks, 902
Isotropic scale, 276–277
Iterated conditional mode (ICM) algorithm, 298, 574–575
Iterative halftoning method, 141
Iterative methods, for image restoration
Feynman’s idea, 249
mobile computing, 251
image processing, 252
intelligent image acquisition, 252
computationally challenging, 193
effective implementation, 194
advantages on simple filtering techniques, 194
regularization, 195
Tikhonov regularization, 197
sparse reconstructions, 197
matrix structures and matrix-vector multiplications, 198
boundary conditions, 198
periodic boundary conditions, 198
zero boundary conditions, 198
replicating boundary conditions, 198
reflective boundary conditions, 198–199
spatially invariant blurs, 199
locally spatially invariant blurs, 200
sparse spatially variant blurs, 201
separable blurs, 201
preconditioning, 201
MATLAB notes, 203
spatially invariant Gaussian blur, 204
spatially invariant atmospheric turbulence blur, 206
spatially variant Gaussian blur, 207
Iterative methods for unconstrained problems, 215
Iterative methods with nonnegativity constraints, 228
projection methods, 228
statistically motivated methods, 229
general iterative scheme, 229
general noise model, 230
algorithms, 231
MATLAB notes, 234
J
JBits SDK tool, 650
K
Kalman Correlation Filter (KCF), 404–405
Kanade-Lucas-Tomasi (KLT), 574
L
LABPQR spectral gamut mapping, 151–152
Large surveillance videos, 537
clutter and occlusions, 536
comparison with HDP-based video search, 551
contributions, 536

1065
Index
Large surveillance videos (Continued)
data lifetime, 535
data structures, 542
datasets, 551
feature extraction, 538
features, 539
full matches, 546
indexing, 540
lookup table, 543
proposed system, 536
resolution, 539
retrieval speed, 536
search engine, 544
unpredictable event duration, 536
unpredictable queries, 535
Lateral chromatic aberration, 20
Lattice based models, 294
modeling, 294
optimization, 296
Layered depth image (LDI), 504
Light field, 494
2D and 3D light fields, 495
4D light field, 497
Fourier transform, 499–500
spectral analysis, 499
SPF and IBR, 500–501
Line process, 300
Linear dynamical systems (LDS), 461, 599
Linear projections, 268
Fisher’s discriminant analysis, 268
Linear regression iteration (LRI), 154
Liquid Crystal on Silicon (LCoS), 590–592
Local and global groupings, 272
Local mixing, 589
Locality-sensitive hashing (LSH), 541
Logitboost, 276
Longitudinal chromatic aberration, 20
3D lymph node detection, 275
M
MACE Mellin Radial Harmonic (MACE-MRH)  
filters, 406, 424
Marginal classifier, 274–276
Marginal space learning (MSL), 269, 271, 279, 287
advantages and disadvantages, 275
applications of, 287
compact classifiers, 272
face detection, 275
flexible curves in X-ray images, detection of, 272
HCF algorithm, 272
multiple computational paths in, 272, 279
object detection tasks, 272
pictorial structures, 272
RCM, 273
selective attention, 272
sequential Monte Carlo, 273
Markov Chain Monte Carlo (MCMC), 565, 574
algorithms, 296
stimulations, 293
Markov chain Monte Carlo multi-target tracking  
(MCMC-MTT), 638–639
Markov models, 269, 308
continuous spatial processes, 269
denoising and deconvolution, 299
lattice based and Bayesian paradigm, 294
local energy potentials, 269
MRF, 269
probabilistic approach in image analysis, 293
segmentation problem, 301
Markov Random Field (MRF) model, 268
Matched filter (MF), 405
and efficient computation of, 407
MATLAB notes
spatially invariant blurs, 203
locally spatially invariant blurs, 204
sparse spatially variant blurs, 204
separable blurs, 204
for iterative methods for image restoration, 203
iterative methods for unconstrained problems, 226
Iterative methods with nonnegativity  
constraints, 234
Maximum A Posteriori (MAP), 293, 574
Maximum Average Correlation Height (MACH)  
filter, 410
average correlation energy (ACE), 410
correlation output variance for, 411
Maximum of pseudo likelihood (MPL), 305–306
MCMC-MTT. See Markov chain Monte Carlo  
multi-target tracking (MCMC-MTT)
Mechanical dot gain, 142
Medical image analysis, 269
deformable template approach, 269
registration, 269
Mellin Radial Harmonic Function (MRHF)  
filters, 406
Metamer mismatch-based spectral gamut  
mapping, 153
Metropolis-Hastings algorithm, 296
Mild cognitive impairment (MCI), 329
Miniaturization, 249
Minimum Average Correlation Energy (MACE)  
filter, 405, 415
Minimum Description Length (MDL), 574
Minimum Noise and Correlation Energy (MINACE)  
filter, 405, 413

1066
Index
Minimum Output Sum of Squared Error (MOSSE)  
filter, 406, 419, 432
Minimum Squared Error Synthetic Discriminant 
Function (MSESDF) filter, 405
Minimum Variance Synthetic Discriminant Function  
(MVSDF) filter, 409
output’s noise variance (ONV), 409
Mobile computing
interacting with computer without an interface, 251
social interaction, 251
human-computer interaction (HCI), 251–252
touchscreen, 252
peoples’ liking, 252
human network interaction, 260
image processing, 252
Mobile Imaging, 114
ClearType, 99
frequency-domain analysis of subpixel-based  
down-sampling, 106
LCD displays, 95
PenTile matrix, 97
small screen challenge, 95
spatial-domain algorithm design
diagonal direct subpixel-based down-sampling  
(DDSD), 103
direct pixel-based down-sampling (DPD), 101
direct subpixel-based down-sampling (DSD), 102
subpixel-based hardware design, 95
subpixel-based software design
color image down-sampling, 100
font rendering, 99
visual perception (VP) dynamics, 99
Modified uniformly redundant arrays (MURA)  
codes, 592–593
Modular mapping, 654–655
circuit block
control, 657
interface and control architecture, 655–656
PSDFsim tool, 657
subsystem-level hardware mapping, 656
Modulation transfer function (MTF), 23
MPEG AAC family, 784
MPEG-1, 780
MPEG-2 AAC, 783
MPEG-2, 781
MPEG-4 lossless audio coding, 788
MPEG-4 low delay AAC family, 787
MPEG-4, 783
MPEG-D MPEG Spatial Audio Object Coding  
(SAOC), 790
MPEG-D MPEG surround, 789
MPEG-D unified speech and audio coding  
(USAC), 792
Multi-Frame Correlation Filter  
(MFCF), 404, 441
Multiple computational paths, 279
Multiple objects detection, 312
in road networks, 316
trees counting, 312
Multi-target tracking in video, 561
batch association, 574
block diagram, 564
causal trackers, 565
challenges, 565
color histograms, 570
covariance matrix, 570
detections, 570
example of, 562
feature extraction, 568
initialization and termination of, 576
localization and association, 571–573
motion model, 568
motion, 567
noise, 567
non-causal trackers, 565
occlusions, 566–567
predictive models, 570
problem formulation, 562
scene contextual information, 576, 578
scene, density of targets, 567–568
sequential localization, 571–573
shape, 566, 570
SIFT, 570
Multi-tasking PTZ cameras, 620, 623
Multivariate statistics, 268–269
Multi-view stereo (MVS), 513
Murray-Davies model, 143
Music, signal processing
audio matching, 710, 723
beat, 724, 733
chord recognition, 710, 721
chromagram, 719, 721
digital signal processing, 713
harmony, 714
instrument recognition, 742
log-frequency spectrogram, 716
melody, 744, 746, 751
multidimensional scaling, 737–738
novelty curve, 725
onset detection, 724–725
periodicity analysis, 728
pitch, 714
polyphonic music, 710
recent developments, 713
representations, 714
spectral envelope, 738

1067
Index
Music, signal processing (Continued)
spectrogram, 715
tempo, 724, 728, 735
temporal evolution of sounds, 741, 743
tone quality, 737
vocals, 744, 749
N
Network
with star topology, 675
static sensor fusion, 675
with nodes and links, 676
dynamic sensor fusion, 679
arbitrary topology, 692
with limited communication range, 692
Networks research, virtual vision, 609–610
animat vision approach, 611
prototype camera networks. See prototype camera  
networks
video surveillance systems, 618
virtual vision simulators, 612–613
behavior-based camera nodes, 616, 618
office tower floor, 613–614
PTZ zoom and fixation, 615
simulated cameras, 614
train station, 612
visual analysis, 614–616
Neugebauer model, 145
Neuroscientific analysis, 268
non-Gaussian models, 268
Nonnegativity constrained iterative methods, 238
Noise estimation algorithms
minimum statistics, 1041
pitch tracking, 1041
VAD-based, 1040
Non-maximal suppression, 278
Normalized least-mean-square (NLMS) algorithm, 810
Nyquist sampling theorem, 583
O
Object detection, 273
computational challenges in, 271
face detection in 2D images, 269
MSL, 271
Omni-directional cameras, 493
Optical dot gain, 142
Optical flow, 344
Optimal tracklet association (OLDAM), 575
Optimal Tradeoff CHF (OTCHF) filters, 406, 420
Optimal tradeoff synthetic discriminant function  
(OTSDF) filter, 411
Ordered dither halftoning, 137–138
P
Parameter space, 274
Pattern analysis, 269
machine learning, 269
Peak-to-correlation energy (PCE), 405
Perceptual audio coding
applications, 757
artifacts, 765
bandwidth extension, 771
compression performance, 788, 793–794
delay and low delay, 765
enhancement tools, 771
filter banks, 764
intensity stereo (IS) coding, 769
joint stereo, 766
mid/side (M/S) stereo coding, 766
models, 764
parametric multi-channel, 774
principles, 758
psychoacoustics, 758–759
quantization, 764
signal processing in, 761
sound quality, 760
vs speech coding, 763
Pictorial structures, 272
Pincushion distortion, 18
Pixel values, 268
Plenoptic function, 494
2D and 3D light fields, 495
4D light field, 497
Fourier transform, 499–500
spectral analysis, 499
SPF and IBR, 500–501
Poisson process, 307
Polynomial Correlation Filters (PCFs), 406, 430
Postfiltering. see Echo cancellation
Potts model, 302–303
Pre- and post-processing techniques, 435
correlation output, normalization, 437
selecting and registering of, 435
selection of parameters, 440
training images, processing, 435
transforming and filtration, 436
Predictive models, 329
COMPARE, 329–330
LOOCV,  330–331
semi supervised learning of, 331
supervised learning of, 329

1068
Index
Preconditioning Richardson iteration, 217
successive over-relaxation (SOR) method, 219
filtering based preconditioning, 221
steepest descent methods, 222
conjugate gradient methods, 223
LSQR and filtering, 224
hybrid method, 225
MATLAB notes, 226
Preconditioning to accelerate convergence of  
iterative methods, 201
Prefrontal cortex (PFC), 335
Printing
AM-FM hybrid halftoning, 140
amplitude-modulated (AM) halftones, 135
basic colorimetry, 125
bidirectional reflectance distribution function  
(BRDF), 155
blue-noise halftoning, 140
electrophotography, 124
error-diffusion methods, 140
flexographic printing, 120
frequency modulated (FM) halftoning, 137–138
functional printing, 156
gravure printing, 119
halftoning, 134
History, 117
ink-drop-based recording device, 118
Inkjet printing, 122
iterative halftoning method, 141
Offset printing, 121
ordered dither halftoning, 137–138
printed reproductions beyond color, 155
printer models, 141, 150
Clapper-Yule model, 148
mechanical dot gain, 142
Murray-Davies model, 143
Neugebauer model, 145
optical dot gain, 142
Yule-Nielsen modified spectral Neugebauer (YNSN) 
model, 146–147
separation, 131
Definition, 131
gray component replacement (GCR), 132
inks, 131
spectral printing, 150
algorithm, 153
Challenges, 151
metameric workflows, 150
modeling multi-ink printers, 151
spectral gamut mapping, 151
spectral separation, 151, 154
technologies, 119
Workflow, 124
Probabilistic boosting tree, 273
Programmable pixel compressive camera  
(P2C2), 592–593, 601–602 
architecture, 593
DMD/LCoS, 592
LCoS array, 601–603
Prototype camera networks, 618
active camera scheduling, 618
collaborative persistent surveillance, 620–621
multi-tasking PTZ cameras, 620, 623
proactive PTZ control, 620, 622
PSDFsim tool, 657
Psychoacoustics
auditory perception, 758
intensity stereo (IS) coding, 769
signal irrelevancy, 763
Psychophysical models of fovated images and  
videos, 389
contrast sensitivity, 390
contrast threshold, 389
cut-off eccentricity, 390
cut-off frequency, 390
Q
Quadratic Correlation Filter (QCF), 406, 432
Quantization schemes
filter banks, 762–763
R
Random Finite Sets (RFS), 573
Receiver operating characteristic (ROC) curve, 278
convolutional face finder, 282, 284–287
floatboost, 282
Viola-jones, 282
Recursive compositional models (RCM), 271, 273
Rendring images, 506
applications, 519
datasets, 527
depth sweeping, 513
depth-image-based rendering, 514
image-based visual hull and voxel sweeping, 511
implementation/code, 526
light field fusion, 506
open issues and problems, 526
plane sweeping and spherical surface  
sweeping, 507
Restricted isometry property (RIP), 585
recovery, 586

1069
Index
Reverberation measurement
channel-base, 890
direct-to-reverberant ratio, 891
energy decay curve, 889
evaluation effect, 887
signal-base, 891
signal-to-reverberation ratio, 892
time, 888
Reversible Jump Markov Chain Monte Carlo  
(RJMCMC) algorithm, 309
acceptance/rejection concept, 311
formulation of, 310
optimization, 316
Rice single pixel camera (SPC), 589–591
Richardson iteration, 216
preconditioning, 217
Riemannian metrics, 268–269
for image analysis, 268–269
Room reverberation
acoustics, 883
channel-based equalization, 887
listener perception, 885
simulation, 885
spatial filtering, 886
speech enhancement, 887
S
Sampling frequency
acoustic echo cancellation, 809–810
echo path transfer function, 835, 860
Scale-Invariant Feature Transform (SIFT), 570
Scene representation, 501
explicit geometry, 503
geometry-image continuum, 502
IBVH, 505–506
implicit geometry, 502
LDI, 504
VDTM, 505
Schedule-based mapping
dataflow schedule graph (DSG), 658
DSG model for PSDF execution, 659
RAs, 658
SCAs and, 658
Schizophrenia, 328
Search engine of large surveillance videos, 544
partial matches, 545
queries, 544
Segmentation datasets for video analysis, dynamical  
system, 482
Dyntex for segmentation and recognition, 483
synthetic video textures dataset, 482
Selective attention, 272
Sensor fusion
in message loss, 683
static, 675, 686
dynamic, 679, 690
sign of innovations-KF, 690
limited bandwidth, 686
Sequential Monte Carlo, 273
Signal dependent filtering
adaptive notch, 1023
channel compensation, 1024
comb filters, 1023
single channel adaptive filter (SCAF) structure, 1024
target shape, 1024
Signal models and algorithms, 594
dictionary models, 596
motion flow, 596
total variation, 595
wavelet sparsity, 595
Simulated annealing, 298
RJMCMC algorithm, 321
Single pixel camera (SPC), 589–590, 597–598, 600
linear dynamical systems, 599
motion-flow models, 599
multi-scale video recovery, 598–599
Skin detection, 272
Soft cascade, 273
Software programmable reconfiguration, 650
Sound field synthesis
broadband plane wave, 937, 944
Cartesian coordinates, 921–922
circular harmonics, 935
circular secondary source distributions, 960
coordinate free representation, 920
equivalent scattering problem, 955
explicit solution, 955
Fourier transformation, space, 930–931
Fourier transformation, time, 929, 931
head-related transfer functions, 917
Kirchhoff-Helmholtz integral equation, 952
model- versus data-based rendering, 956
monofrequent plane, 936–938, 941
monopole only synthesis, 954
multidimensional phasors, 926, 928
multipoint approaches, 919
near-field compensated higher order ambisonics  
(NFC-HOA), 957
Neumann Green’s function, 954
one-dimensional phasors, 926
orthogonal functions, 938–939, 944
Panning approaches, 918

1070
Index
Sound field synthesis (Continued)
plane waves, 932, 934
Polar coordinates, 924, 936
problems in, 918
simple source approach, 955
spatial sampling, 962
spherical coordinates, 923
spherical harmonics, 940–941, 943, 945–946
spherical secondary source distributions, 958
three-dimensional synthesis, 956
2.5-dimensional synthesis, 956
virtual environment, 915, 917
Sound sources
Green’s function, 948, 950
inhomogenous wave equation, 947
interior and exterior responses, 949
one spatial dimension, 951
three spatial dimension, 951
Sparse camera networks tracking, 638
Bayesian algorithms, 638
maximum a posteriori (MAP) problem, 639–640
MCMC-MTT, 638–639
Sparse optimization algorithms, 587
convex methods, 587
greedy pursuit, 587
Spatial filtering, 892
beam forming in 2D and 3D, 894
delay-and-sum beam former, 893
Spatial multiplexing cameras (SMC), 588
Spatial point processes, 306
modeling, 307
optimiziation, 309
Spatio-temporal multiplexing cameras  
(STMC), 588
Spatio-temporal Regularity Flow  
(SPREF), 446–447
Spectral division method (SDM)
approximate solution, 969
linear secondary source distributions, 966
planar secondary source distributions, 963
spatial sampling, 968
Spectrogram, 715
Speech analysis, 987, 1000, 1008
phonetics, 987
Speech articulation
basic phonetics, 991
fricatives in, 991
pitch variation, 989
plosives in, 991
process, 988
spectogram, 990
voiced speech, 988–989
Speech cleaning algorithms
time-frequency domain, 1037–1038
voice activity detector (VAD), 1039
Speech enhancement methods, 1020
binary masks, 1035
click removal, 1035
clipping restoration, 1035
concept, 896
dynamic noise reduction (DNR), 1034
frequency intervals, 1034
linear prediction, 897
minimum mean square estimators (MMSE)  
estimation, 1031
missing feature estimation, 1036
model-based filtering, 1028
noise cancelation, 1021
noise gate operation, 1033
signal interpolation, 1036
static filtering, 1022
subspace, 1026
super Gaussian estimates, 1032
temporal gain changes, 1033
time-frequency gain modification  
(TFGM), 1029
Speech intelligibility
intrusive metrics, 1044
non-human listeners, 1043
nonintrusive metrics, 1044
objectives, 1042
subjective methods, 1042
Speech processing
background, 983
enhancement, 984
model and analysis, 984
Speech production
circuit sections, 986
described, 985
glottal closure instant detection, 1007–1008
linear source-tract model, 985, 991–992
mechanical resonators, 986
phonetic alphabet, 988
signal generation, 996
spectrogram, 990
technological application, 991
Speech quality
intrusive metrics, 1045
nonintrusive metrics, 1046
speech stimuli, 1044–1045
standard procedures, 1045
Speech signal
anti-causal parts, 1007
autoregressive parameters, 1005–1007

1071
Index
Speech signal (Continued)
coding, 991–992
cost function, 1011
effects of zeros, 987
energy peaks detection, 1009
filter parameters, 1001
fundamental frequency, 988–989
GCI detection, 1008–1009
linear source-filter model, 992
linear transfer function, 991
process, 620, 989–991
vocal tract filter, application, 1000
vocal tract modeling, 992–993
voice segment, 1002, 1012
z-transform, 997
zero-frequency filter, 1010
Speech units
fricatives, 988
plosives, 988
speech pressure waveform, 987
voiced speech, 988
vowels, 988
Speed and precision, tradeoff between
Star topology, 675
non-ideal networks, 683
sensor fusion in message loss, 683
Statistical analysis, 267–268
Static sensor fusion, 675
for star topology, 677
sequential measurements, from one  
sensor, 678
Student t-test, 320–321
Support vector machines (SVM), 331
description of, 331
TSVM, 331–332
Surface plenoptic function (SPF)
and IBR, 500–501
Synchronization. See temporal calibration
Synthetic aperture radar (SAR), 267
T
Temporal calibration, 636
Temporal multiplexing cameras (TMC), 588
Three dimensional integrated circuit  
(3D-IC), 631–632
challenges, 632
stacked 3D-ICs manufacturing technologies, 631
Tikhonov regularization, 197
Touch-based control, 257
Touchscreen, 252
anchor regions, 255
tapping, 256
U
Unconstrained MACE (UMACE) filter, 406, 415
Unconstrained MSESDF (UMSESDF)  
filter, 406, 415
Unconstrained iterative methods, 235
Unsupervised learning, 334
disentangling of heterogeneity, 334
V
Video analysis, dynamical systems
applications, 473
basis and canonical forms, change  
of, 465–466
constrained identification, 466
dynamical models, 470
identification, 463
marginally stable systems, 467
model, 463
multiple LDSs, dynamics for, 468
stable systems, 467
Video compressive sensing, 587
causality/ephemeral nature of time, 587
coded strobing camera, 597
linear dynamical systems (LDS), 599
motion-flow models, 584
multi-scale video recovery, 598–599
organization, 588
redundancies in videos, 588
single pixel camera (SPC), 597–598
Video compressive sensing
CASSI, 599
coded exposure video camera, 600
coded strobing camera, 597–598
flexible voxels camera, 601–602
FSVC, 603
organization, 588
P2C2, 601–602
problem statement, 587
redundancies in videos, 588
SPC. See single pixel camera (SPC)
time, 587
Video processing, 343
applications in, 345
basic tasks in, 343
tracking, 347
Video surveillance systems, 611
Video surveillance, 345
View-dependent texturemapping (VDTM), 505
Viola-jones, 282
ROC curves obtained, 284
Visual perception (VP) dynamics, 99

1072
Index
Vocal tract transfer function
AR parameters, 996
cross-sectional area, 993
discrete model, 995
glottal models, 997
sound pressure wave, 993
Voice activity detection
energy histogram, 1039
energy waveform, 1039
magnitude histogram, 1039
periodicity, 1040
statistical models, 1040
Voice source signal
data driven analysis, 1013
estimation, 1000
modeling, 1012
piecewise approximation, 1013
two-pole model, 996
Voxel sub-sampling camera, 590–591
CPEV and LCoS, 590–592
W
Wave Field Synthesis (WFS), 972
2.5 dimensional, 974
spatial sampling, 975
three dimensional, 973
Y
Yule-Nielsen modified spectral Neugebauer (YNSN)  
model, 146–147


123
SPRINGER BRIEFS IN MATHEMATICAL PHYSICS 26
Giacomo Livan
Marcel Novaes
Pierpaolo Vivo
Introduction to 
Random Matrices
 Theory and 
Practice 

SpringerBriefs in Mathematical Physics
Volume 26
Series editors
Nathanaël Berestycki, Cambridge, UK
Mihalis Dafermos, Princeton, USA
Tohru Eguchi, Tokyo, Japan
Atsuo Kuniba, Tokyo, Japan
Matilde Marcolli, Pasadena, USA
Bruno Nachtergaele, Davis, USA

SpringerBriefs are characterized in general by their size (50–125 pages) and fast
production time (2–3 months compared to 6 months for a monograph).
Briefs are available in print but are intended as a primarily electronic publication to
be included in Springer’s e-book package.
Typical works might include:
• An extended survey of a ﬁeld
• A link between new research papers published in journal articles
• A presentation of core concepts that doctoral students must understand in order
to make independent contributions
• Lecture notes making a specialist topic accessible for non-specialist readers.
SpringerBriefs in Mathematical Physics showcase, in a compact format, topics of
current relevance in the ﬁeld of mathematical physics. Published titles will
encompass all areas of theoretical and mathematical physics. This series is intended
for mathematicians, physicists, and other scientists, as well as doctoral students in
related areas.
More information about this series at http://www.springer.com/series/11953

Giacomo Livan
• Marcel Novaes
Pierpaolo Vivo
Introduction to Random
Matrices
Theory and Practice
123

Giacomo Livan
Department of Computer Science
University College London
London
UK
Marcel Novaes
Instituto de Física
Universidade Federal de Uberlândia
Uberlândia, Minas Gerais
Brazil
Pierpaolo Vivo
Department of Mathematics
King’s College London
London
UK
ISSN 2197-1757
ISSN 2197-1765
(electronic)
SpringerBriefs in Mathematical Physics
ISBN 978-3-319-70883-6
ISBN 978-3-319-70885-0
(eBook)
https://doi.org/10.1007/978-3-319-70885-0
Library of Congress Control Number: 2017958623
© The Author(s) 2018
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made. The publisher remains neutral with regard to
jurisdictional claims in published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
This is a book for absolute beginners. If you have heard about random matrix
theory, commonly denoted RMT, but you do not know what that is, then welcome!
this is the place for you. Our aim is to provide a truly accessible introductory
account of RMT for physicists and mathematicians at the beginning of their
research career. We tried to write the sort of text we would have loved to read when
we were beginning Ph.D. students ourselves.
Our book is structured with light and short chapters, and the style is informal.
The calculations we found most instructive are spelled out in full. Particular
attention is paid to the numerical veriﬁcation of most analytical results. The reader
will ﬁnd the symbol [♠test.m] next to every calculation/procedure for which a
numerical veriﬁcation is provided in the associated ﬁle test.m located at https://
github.com/RMT-TheoryAndPractice/RMT.
We
strongly
believe
that
theory
without practice is of very little use: In this respect, our book differs from most
available textbooks on this subject (not so many, after all).
Almost every chapter contains question boxes, where we try to anticipate and
minimize possible points of confusion. Also, we include To know more sections at
the end of most chapters, where we collect curiosities, material for extra readings
and little gems—carefully (and arbitrarily!) cherry-picked from the gigantic liter-
ature on RMT out there.
Our book covers standard material—classical ensembles, orthogonal polynomial
techniques, spectral densities and spacings—but also more advanced and modern
topics—replica approach and free probability—that are not normally included in
elementary accounts on RMT.
Due to space limitations, we have deliberately left out ensembles with complex
eigenvalues and many other interesting topics. Our book is not encyclopedic, nor is
it meant as a surrogate or a summary of other excellent existing books. What we are
sure about is that any seriously interested reader, who is willing to dedicate some
of their time to read and understand this book till the end, will next be able to read
and understand any other source (articles, books, reviews, tutorials) on RMT,
without feeling overwhelmed or put off by incomprehensible jargon and endless
series of “It can be trivially shown that…”.
v

So, what is a random matrix? Well, it is just a matrix whose elements are random
variables. No big deal. So why all the fuss about it? Because they are extremely
useful! Just think in how many ways random variables are useful: If someone
throws a thousand (fair) coins, you can make a rather conﬁdent prediction that the
number of tails will not be too far from 500. Ok, maybe this is not really that useful,
but it shows that sometimes it is far more efﬁcient to forego detailed analysis of
individual situations and turn to statistical descriptions.
This is what statistical mechanics does, after all: It abandons the deterministic
(predictive) laws of mechanics and replaces them with a probability distribution on
the space of possible microscopic states of your systems, from which detailed
statistical predictions at large scales can be made.
This is what RMT is about, but instead of replacing deterministic numbers with
random numbers, it replaces deterministic matrices with random matrices. Anytime
you need a matrix which is too complicated to study, you can try replacing it with a
random matrix and calculate averages (and other statistical properties).
A number of possible applications come immediately to mind. For example, the
Hamiltonian of a quantum system, such as a heavy nucleus, is a (complicated)
matrix. This was indeed one of the ﬁrst applications of RMT, developed by Wigner.
Rotations are matrices; the metric of a manifold is a matrix; the S-matrix describing
the scattering of waves is a matrix; ﬁnancial data can be arranged in matrices;
matrices are everywhere. In fact, there are many other applications, some rather
surprising, which do not come immediately to mind but which have proved very
fruitful.
We do not provide a detailed historical account of how RMT developed, nor do
we dwell too much on speciﬁc applications. The emphasis is on concepts, com-
putations, tricks of the trade: all you needed to know (but were afraid to ask) to start
a hopefully long and satisfactory career as a researcher in this ﬁeld.
It is a pleasure to thank here all the people who have somehow contributed to our
knowledge of RMT. We would like to mention in particular Gernot Akemann,
Giulio Biroli, Eugene Bogomolny, Zdzisław Burda, Giovanni Cicuta, Fabio D.
Cunden, Paolo Facchi, Davide Facoetti, Giuseppe Florio, Yan V. Fyodorov, Olivier
Giraud, Claude Godreche, Eytan Katzav, Jon Keating, Reimer Kühn, Satya N.
Majumdar, Anna Maltsev, Ricardo Marino, Francesco Mezzadri, Maciej Nowak,
Yasser Roudi, Dmitry Savin, Antonello Scardicchio, Gregory Schehr, Nick Simm,
Peter Sollich, Christophe Texier, Pierfrancesco Urbani, Dario Villamaina, and
many others.
This book is dedicated to the fond memory of Oriol Bohigas.
London, UK
Giacomo Livan
Uberlândia, Brazil
Marcel Novaes
London, UK
Pierpaolo Vivo
vi
Preface

Contents
1
Getting Started . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
One-Pager on Random Variables . . . . . . . . . . . . . . . . . . . . . . .
3
2
Value the Eigenvalue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2.1
Appetizer: Wigner’s Surmise . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2.2
Eigenvalues as Correlated Random Variables . . . . . . . . . . . . . .
9
2.3
Compare with the Spacings Between i.i.d.’s . . . . . . . . . . . . . . .
9
2.4
Jpdf of Eigenvalues of Gaussian Matrices . . . . . . . . . . . . . . . .
11
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
3
Classiﬁed Material . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
3.1
Count on Dirac . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
3.2
Layman’s Classiﬁcation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
3.3
To Know More... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
4
The Fluid Semicircle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
4.1
Coulomb Gas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
4.2
Do It Yourself (Before Lunch) . . . . . . . . . . . . . . . . . . . . . . . .
25
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
5
Saddle-Point-of-View . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
5.1
Saddle-Point. What’s the Point? . . . . . . . . . . . . . . . . . . . . . . . .
33
5.2
Disintegrate the Integral Equation . . . . . . . . . . . . . . . . . . . . . .
35
5.3
Better Weak Than Nothing . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
5.4
Smart Tricks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
5.5
The Final Touch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
5.6
Epilogue. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
5.7
To Know More... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
vii

6
Time for a Change . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
6.1
Intermezzo: A Simpler Change of Variables . . . . . . . . . . . . . . .
45
6.2
...that Is the Question . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
6.3
Keep Your Volume Under Control . . . . . . . . . . . . . . . . . . . . .
46
6.4
For Doubting Thomases... . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
6.5
Jpdf of Eigenvalues and Eigenvectors . . . . . . . . . . . . . . . . . . .
48
6.6
Leave the Eigenvalues Alone. . . . . . . . . . . . . . . . . . . . . . . . . .
49
6.7
For Invariant Models... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
6.8
The Proof . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
7
Meet Vandermonde . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
7.1
The Vandermonde Determinant . . . . . . . . . . . . . . . . . . . . . . . .
53
7.2
Do It Yourself . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
8
Resolve(nt) the Semicircle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
8.1
A Bit of Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
8.2
Averaging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
58
8.3
Do It Yourself . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
8.4
Localize the Resolvent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
8.5
To Know More... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
9
One Pager on Eigenvectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
66
10
Finite N . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
10.1
b ¼ 2 is Easier . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
10.2
Integrating Inwards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
10.3
Do It Yourself . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
10.4
Recovering the Semicircle . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
11
Meet Andréief . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
75
11.1
Some Integrals Involving Determinants . . . . . . . . . . . . . . . . . .
75
11.2
Do It Yourself . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
11.3
To Know More... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
78
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
12
Finite N Is Not Finished . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
12.1
b ¼ 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
12.2
b ¼ 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
13
Classical Ensembles: Wishart-Laguerre . . . . . . . . . . . . . . . . . . . . .
89
13.1
Wishart-Laguerre Ensemble . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
viii
Contents

13.2
Jpdf of Entries: Matrix Deltas... . . . . . . . . . . . . . . . . . . . . . . . .
91
13.3
...and Matrix Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
13.4
To Know More... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
94
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
14
Meet Marčenko and Pastur. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
97
14.1
The Marčenko-Pastur Density . . . . . . . . . . . . . . . . . . . . . . . . .
97
14.2
Do It Yourself: The Resolvent Method . . . . . . . . . . . . . . . . . .
98
14.3
Correlations in the Real World and a Quick Example:
Financial Correlations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
101
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
103
15
Replicas... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
105
15.1
Meet Edwards and Jones . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
105
15.2
The Proof . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
106
15.3
Averaging the Logarithm. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
107
15.4
Quenched versus Annealed . . . . . . . . . . . . . . . . . . . . . . . . . . .
107
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
108
16
Replicas for GOE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
109
16.1
Wigner’s Semicircle for GOE: Annealed Calculation . . . . . . . .
109
16.2
Wigner’s Semicircle: Quenched Calculation . . . . . . . . . . . . . . .
112
16.2.1
Critical Points . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
114
16.2.2
One Step Back: Summarize and Continue . . . . . . . . . .
116
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
117
17
Born to Be Free . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
119
17.1
Things About Probability You Probably Already Know . . . . . .
119
17.2
Freeness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
120
17.3
Free Addition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
121
17.4
Do It Yourself . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
122
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
124
Contents
ix

Chapter 1
Getting Started
Let us start with a quick warm-up. We now produce a N × N matrix H whose entries
are independently sampled from a Gaussian probability density function (pdf)1 with
mean 0 and variance 1. One such matrix for N = 6 might look like this:
H =
⎛
⎜⎜⎜⎜⎜⎜⎝
1.2448
0.0561 −0.8778 1.1058 1.1759
0.7339
−0.1854 0.7819 −1.3124 0.8786 0.3965 −0.3138
−0.4925 −0.6234 0.0307 0.8448 −0.2629 0.7013
0.1933 −1.5660 2.3387 0.4320 −0.0535 0.2294
−1.0143 −0.7578 0.3923 0.3935 −0.4883 −2.7609
−1.8839 0.4546 −0.4495 0.0972 −2.6562 1.3405
⎞
⎟⎟⎟⎟⎟⎟⎠
.
(1.1)
Some of the entries are positive, some are negative, none is very far from 0. There
is no symmetry in the matrix at this stage, Hi j ̸= Hji.
Any time we try, we end up with a different matrix: we call all these matrices
samples or instances of our ensemble. The N eigenvalues are in general complex
numbers (try to compute them for H!).
To get real eigenvalues, the ﬁrst thing to do is to symmetrize our matrix. Recall
that a real symmetric matrix has N real eigenvalues. We will not deal much with
ensembles with complex eigenvalues in this book.2
Try the following symmetrization Hs = (H + H T )/2, where (·)T denotes the
transpose of the matrix. Now the symmetric sample Hs looks like this:
1You may already want to give up on this book. Alternatively, you can brush up your knowledge
about random variables in Sect.1.1.
2...but we will deal a lot with matrices with complex entries (and real eigenvalues).
© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_1
1

2
1
Getting Started
Hs =
⎛
⎜⎜⎜⎜⎜⎜⎝
1.2448 −0.0646 −0.6852 0.6496
0.0807 −0.5750
−0.0646 0.7819 −0.9679 −0.3436 −0.1806 0.0704
−0.6852 −0.9679 0.0307
1.5917
0.0647
0.1258
0.6496 −0.3436 1.5917
0.4320
0.1700
0.1633
0.0807 −0.1806 0.0647
0.1700 −0.4883 −2.7085
−0.5750 0.0704
0.1258
0.1633 −2.7085 1.3405
⎞
⎟⎟⎟⎟⎟⎟⎠
,
(1.2)
whose six eigenvalues are now all real
{−2.49316, −1.7534, 0.33069, 1.44593, 2.38231, 3.42944} .
(1.3)
Congratulations! You have produced your ﬁrst random matrix drawn from the
so-called GOE (Gaussian Orthogonal Ensemble)... a classic—more on this name
later.
You can now do several things: for example, you can make the entries complex
or quaternionic instead of real. In order to have real eigenvalues, the corresponding
matrices need to be hermitian and self-dual respectively3—better have a look at one
example of the former, for N as small as N = 2
Hher =

0.3252
0.3077 + 0.2803i
0.3077 −0.2803i
−1.7115
	
.
(1.4)
You have just met the Gaussian Unitary (GUE) and Gaussian Symplectic (GSE)
ensembles, respectively—and are surely already wondering who invented these
names.
We will deal with this jargon later. Just remember: the Gaussian Orthogonal
Ensembledoesnot containorthogonalmatrices—butrealsymmetricmatricesinstead
(and similarly for the others).
Although single instances can sometimes be also useful, exploring the statistical
properties of an ensemble typically requires collecting data from multiple samples.
We can indeed now generate T such matrices, collect the N (real) eigenvalues for
each of them, and then produce a normalized histogram of the full set of N × T
eigenvalues. With the code [♠Gaussian_Ensembles_Density.m], you may
get a plot like Fig.1.1 for T = 50000 and N = 8.
Roughly half of the eigenvalues collected in total are positive, and half negative—
this is evident from the symmetry of the histograms. These histograms are concen-
trated (signiﬁcantly nonzero) over the region of the real axis enclosed by (for N = 8)
• ±
√
2N ≈±4 (GOE),
• ±
√
4N ≈±5.65 (GUE),
• ±
√
8N ≈8 (GSE).
3Hermitian matrices have real elements on the diagonal, and complex conjugate off-diagonal
entries. Quaternion self-dual matrices are 2N × 2N constructed as A=[X Y; -conj(Y) conj(X)];
A=(A+A’)/2, where X and Y are complex matrices, while conj denotes complex conjugation of all
entries.

1 Getting Started
3
-10
-5
0
5
10
0
0.04
0.08
0.12
0.16
GOE
GUE
GSE
Fig. 1.1 Histograms of GOE, GUE and GSE eigenvalues (N = 8 and T = 50000 samples)
You can directly jump to the end of Chap.5 to see what these histograms look like
for big matrices.
Question 1.1 Can I compute analytically the shape of these histograms? And
what happens if N becomes very large?
▶Yes, you can. In Chaps.10 and 12, we will set up a formalism to compute
exactly these shapes for any ﬁnite N. In Chap.5, instead, we will see that for
large N the histograms approach a limiting shape, called Wigner’s semicircle
law.
1.1
One-Pager on Random Variables
Attributed to Giancarlo Rota is the statement that a random variable X is neither
random, nor is a variable.4
Whatever it is, it can take values in a discrete alphabet (like the outcome of tossing
a die, {1, 2, 3, 4, 5, 6}) or on an interval σ (possibly unbounded) of the real line. For
the latter case, we say that ρ(x) is the probability density function5 (pdf) of X if

 b
a dxρ(x) is the probability that X takes value in the interval (a, b) ⊆σ.
4In the following we may use both upper and lower case to denote a random variable.
5For example, for the GOE matrix (1.2) the diagonal entries were sampled from the Gaussian (or
normal) pdf ρ(x) = exp(−x2/2)/
√
2π. We will denote the normal pdf with mean μ and variance
σ 2 as N(μ, σ 2) in the following.

4
1
Getting Started
A die will not blow up and disintegrate in the air. One of the six numbers will even-
tually come up. So the sum of probabilities of the outcomes should be 1 (=100%).
People call this property normalization, which for continuous variables just means

σ dxρ(x) = 1.
All this in theory.
In practice, sample your random variable many times and produce a normalized
histogram of the outcomes. The pdf ρ(x) is nothing but the histogram proﬁle as the
number of samples gets sufﬁciently large. The average of X is ⟨X⟩=

dxρ(x)x
and higher moments are deﬁned as ⟨Xn⟩=

dxρ(x)xn. The variance is Var(X) =
⟨X2⟩−(⟨X⟩)2, which is a measure of how broadly spread around the mean the pdf is.
The cumulative distribution function F(x) is the probability that X is smaller or
equal to x, F(x) =

 x
−∞dy ρ(y). Clearly, F(x) →0 as x →−∞and F(x) →1
as x →+∞.
If we have two (continuous) random variables X1 and X2, they must be
described by a joint probability density function (jpdf) ρ(x1, x2). Then, the quan-
tity

 b
a dx1

 d
c dx2ρ(x1, x2) gives the probability that the ﬁrst variable X1 is in the
interval (a, b) and the other X2 is, simultaneously, in the interval (c, d).
Whenthejpdfisfactorized,i.e.istheproductoftwodensityfunctions,ρ(x1, x2) =
ρ1(x1)ρ2(x2), the variables are said to be independent, otherwise they are dependent.
When, in addition, we also have ρ1(x) = ρ2(x), the random variables are called i.i.d.
(independent and identically distributed). In any case, ρ(x1) =

ρ(x1, x2)dx2 is the
marginal pdf of X1 when considered independently of X2.
The above discussion can be generalized to an arbitrary number N of random
variables. Given the jpdf ρ(x1, . . . , xN), the quantity ρ(x1, . . . , xN)dx1 · · · dxN is
the probability that we ﬁnd the ﬁrst variable in the interval [x1, x1 +dx1], the second
in the interval [x2, x2 + dx2], etc. The marginal pdf ρ(x) that the ﬁrst variable will
be in the interval [x, x + dx] (ignoring the others) can be computed as
ρ(x) =

dx2 · · ·

dxNρ(x, x2, . . . , xN).
(1.5)
Question 1.2 What is the jpdf ρ[H] of the N 2 entries {H11, . . . , HN N} of the
matrix H in (1.1)?
▶The entries in H are independent Gaussian variables, hence the jpdf is fac-
torized as ρ[H] ≡ρ(H11, . . . , HN N) = N
i, j=1

exp

−H 2
i j/2

/
√
2π

.
If a set of random variables is a function of another one, xi = xi(y), there is a
relation between the jpdf of the two sets
ρ(x1, . . . , xN)dx1 · · · dxN = ρ(x1(y), . . . , xN(y))|J(x →y)|



ρ(y1,...,yN )
dy1 · · · dyN , (1.6)

1.1 One-Pager on Random Variables
5
where J is the Jacobian of the transformation, given by J(x →y) = det

∂xi
∂y j

. We
will use this property in Chap.6.
Question 1.3 What is the jpdf of the N(N −1)/2 entries in the upper triangle
of the symmetric matrix Hs in (1.2)?
▶For Hs, you need to consider the diagonal and the off-diagonal entries sepa-
rately: the diagonal entries are (Hs)ii = Hii, while the off-diagonal entries are
(Hs)i j = (Hi j + Hji)/2. As a result,
ρ((Hs)11, . . . , (Hs)N N ) =
N

i=1

exp

−(Hs)2
ii/2

/
√
2π
 
i< j

exp

−(Hs)2
i j

/√π

,
(1.7)
i.e. the variance of off-diagonal entries is 1/2 of the variance of diagonal entries.
Make sure you understand why this is the case. This factor 2 has very important
consequences (see Question 3.3). From now on, for a real symmetric Hs we
will denote the jpdf of the N(N −1)/2 entries in the upper triangle by ρ[H]—
dropping the subscript ‘s’ when there is no risk of confusion.

Chapter 2
Value the Eigenvalue
In this chapter, we start discussing the eigenvalues of random matrices.
2.1
Appetizer: Wigner’s Surmise
Consider a 2 × 2 GOE matrix Hs =
 x1 x3
x3 x2

, with x1, x2 ∼N(0, 1) and
x3 ∼N(0, 1/2). What is the pdf p(s) of the spacing s = λ2 −λ1 between its
two eigenvalues (λ2 > λ1)?
The two eigenvalues are random variables, given in terms of the entries by the
roots of the characteristic polynomial
λ2 −Tr(Hs)λ + det(Hs) ,
(2.1)
therefore λ1,2 =

x1 + x2 ±

(x1 −x2)2 + 4x2
3

/2 and s =

(x1 −x2)2 + 4x2
3.
By deﬁnition, we have
p(s) =
 ∞
−∞
dx1dx2dx3
e−1
2 x2
1
√
2π
e−1
2 x2
2
√
2π
e−x2
3
√π δ

s −

(x1 −x2)2 + 4x2
3

.
(2.2)
Changing variables as
⎧
⎪⎨
⎪⎩
x1 −x2
= r cos θ
2x3
= r sin θ
x1 + x2
= ψ
⇒
⎧
⎪⎨
⎪⎩
x1
= r cos θ+ψ
2
x2
= ψ−r cos θ
2
,
x3
= r sin θ
2
(2.3)
© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_2
7

8
2
Value the Eigenvalue
and computing the corresponding Jacobian
J = det
⎛
⎜⎝
∂x1
∂r
∂x1
∂θ
∂x1
∂ψ
∂x2
∂r
∂x2
∂θ
∂x2
∂ψ
∂x3
∂r
∂x3
∂θ
∂x3
∂ψ
⎞
⎟⎠= det
⎛
⎝
cos θ/2 −r sin θ/2 1/2
−cos θ/2 r sin θ/2 1/2
sin θ/2
r cos θ/2
0
⎞
⎠= −r/4 ,
(2.4)
one obtains
p(s) =
1
8π3/2
 ∞
0
dr rδ(s −r)
 2π
0
dθ
 ∞
−∞
dψe
−1
2

(
r cos θ+ψ
2
)
2+(
−r cos θ+ψ
2
)
2+ r2 sin2 θ
2

=
√
4π s
8π3/2
 2π
0
dθe
−1
2

s2 cos2 θ
2
+ s2 sin2 θ
2

= s
2e−s2/4 .
(2.5)
Note that we used cos2 θ +sin2 θ = 1 to achieve this very simple result: however, we
could only enjoy this massive simpliﬁcation because the variance of the off-diagonal
elements was 1/2 of the variance of diagonal elements—try to redo the calcula-
tion assuming a different ratio. Observe also that this pdf is correctly normalized,
 ∞
0 ds p(s) = 1.
It is often convenient to rescale this pdf and deﬁne ¯p(s) = ⟨s⟩p (⟨s⟩s), where
⟨s⟩=
 ∞
0 dsp(s)s is the mean level spacing. Upon this rescaling,
 ∞
0
¯p(s)ds =
 ∞
0 s ¯p(s)ds = 1. For the GOE as above, show that ¯p(s) = (πs/2) exp(−πs2/4),
which is called Wigner’s surmise,1 whose plot is shown in Fig.2.1.
Fig. 2.1 Plot of Wigner’s
surmise.
0
1
2
3
4
s
0
0.2
0.4
0.6
0.8
p(s)
1Why is it deﬁned a ‘surmise’? After all, it is the result of an exact calculation! The story goes
as follows: at a conference on Neutron Physics by Time-of-Flight, held at the Oak Ridge National
Laboratory in 1956, people asked a question about the possible shape of the distribution of the

2.1 Appetizer: Wigner’s Surmise
9
In spite of its simplicity, this is actually a quite deep result: it tells us that the
probability of sampling two eigenvalues ‘very close’ to each other (s →0) is very
small: it is as if each eigenvalue ‘felt’ the presence of the other and tried to avoid it
(but not too much)! A bit like birds perching on an electric wire, or parked cars on a
street: not too close, not too far apart. If this metaphor does not win you over, check
this out [1].
2.2
Eigenvalues as Correlated Random Variables
In the previous chapter, we met the N real eigenvalues {x1, . . . , xN} of a ran-
dom matrix H. These eigenvalues are random variables described by a jpdf2
ρ(x1, . . . , xN).
Question 2.1 What does the jpdf of eigenvalues ρ(x1, . . . , xN) of a random
matrix ensemble look like?
▶We will give it in Eq.(2.15) for the Gaussian ensemble. Not for every ensemble
the jpdf of eigenvalues is known.
The important (generic) feature is that the {xi}’s are not independent: their jpdf
does not in general factorize. The most striking incarnation of this property is the so-
called level repulsion (as in Wigner’s surmise): the eigenvalues of random matrices
generically repel each other, while independent variables do not—as we show in the
following section.
2.3
Compare with the Spacings Between i.i.d.’s
It is useful at this stage to consider the statistics of gaps between adjacent i.i.d.
random variables. In this case, we will not see any repulsion.
Consider i.i.d. real random variables {X1, . . . , X N} drawn from a parent pdf pX(x)
deﬁned over a support σ. The corresponding cdf is F(x). The labelling is purely
conventional, and we do not assume that the variables are sorted in any order.
We wish to compute the conditional probability density function pN(s|X j = x)
that, given that one of the random variables X j takes a value around x, there is another
spacings of energy levels in a heavy nucleus. E. P. Wigner, who was in the audience, walked up to
the blackboard and guessed (=surmised) the answer given above.
2We will use the same symbol ρ for both the jpdf of the entries in the upper triangle and of the
eigenvalues.

10
2
Value the Eigenvalue
random variable Xk (k ̸= j) around the position x + s, and no other variables lie in
between. In other word, a gap of size s exists between two random variables, one of
which sits around x.
The claim is
pN(s|X j = x) = pX(x + s) [1 + F(x) −F(x + s)]N−2 .
(2.6)
The reasoning goes as follows: one of the variables sits around x already, so we
have N −1 variables left to play with. One of these should sit around x + s, and the
pdf for this event is pX(x + s). The remaining N −2 variables need to sit either to
the left of x—and this happens with probability F(x)—or to the right of x + s—and
this happens with probability 1 −F(x + s).
Now, the probability of a gap s between two adjacent particles, conditioned on
the position x of one variable, but irrespective of which variable this is is obtained
by the law of total probability
pN(s|any X = x) =
N

j=1
pN(s|X j = x)Prob(X j = x) = NpN(s|X j = x)pX(x) ,
(2.7)
where one uses the fact that the variables are i.i.d. and thus the probability that the
particle X j lies around x is the same for every particle, and given by pX(x).
To obtain the probability of a gap s between any two adjacent random variables,
no longer conditioned on the position of one of the variables, we should simply
integrate over x
pN(s) =

σ
dx pN(s|any X = x) = N

σ
dx pN(s|X j = x)pX(x) .
(2.8)
As an exercise, let us verify that pN(s) is correctly normalized, namely
 ∞
0 ds
pN(s) = 1. We have
 ∞
0
ds pN(s) = N
 ∞
0
ds

σ
dx pX(x + s) [1 + F(x) −F(x + s)]N−2 pX(x) .
(2.9)
Changing variables F(x + s) = u in the s-integral, and using F(+∞) = 1 and
du = F′(x + s)ds = pX(x + s)ds, we get
 ∞
0
ds pN(s) = N

σ
dx pX(x)
 1
F(x)
du[1 + F(x) −u]N−2



1−F(x)N−1
N−1
.
(2.10)

2.3 Compare with the Spacings Between i.i.d.’s
11
Setting now F(x) = v and using dv = F′(x)dx = pX(x)dx, we have
 ∞
0
ds pN(s) =
N
N −1
 1
0
dv(1 −vN−1) = 1 ,
(2.11)
as required.
As there are N variables, it makes sense to perform the ‘local’ change of variables
s = ˆs/(NpX(x)) and consider the limit N →∞. The reason for choosing the scaling
factor NpX(x) is that their typical spacing around the point x will be precisely of
order ∼1/(NpX(x)): increasing N, more and more variables need to occupy roughly
the same space, therefore their typical spacing goes down. The same happens locally
around points x where there is a higher chance to ﬁnd variables, i.e. for a higher
pX(x).
We thus have
pN

s =
ˆs
NpX(x)
X j = x

= pX

x + ˆs/NpX(x)
 
1 + F(x) −F

x + ˆs/NpX(x)
N−2 ,
(2.12)
which for large N and ˆs ∼O(1), can be approximated as
pN

s =
ˆs
NpX(x)
X j = x

≈pX(x)e−ˆs ,
(2.13)
therefore using (2.8)
lim
N→∞ˆpN(ˆs) := lim
N→∞pN

s =
ˆs
NpX(x)
 ds
d ˆs = N × 1
N

σ
dxpX(x)e−ˆs = e−ˆs ,
(2.14)
the exponential law for the spacing of a Poisson process. From this, one deduces
easily that i.i.d. variables do not repel, but rather attract: the probability of vanishing
gaps, ˆs →0, does not vanish, as in the case of RMT eigenvalues!
2.4
Jpdf of Eigenvalues of Gaussian Matrices
The jpdf of eigenvalues of a N × N Gaussian matrix is given by3
ρ(x1, . . . , xN) =
1
ZN,β
e−1
2
N
i=1 x2
i 
j<k
|x j −xk|β ,
(2.15)
3This jpdf goes back to the prehistory of RMT. It is an immediate consequence of Theorem 2 in
[2], a 1939 statistics paper published in the journal Annals of Eugenics (a rather scary title, isn’t
it?). In its full glory, it appeared explicitly for the ﬁrst time in [3].

12
2
Value the Eigenvalue
where
ZN,β = (2π)N/2
N

j=1
Γ (1 + jβ/2)
Γ (1 + β/2)
(2.16)
is a normalization constant,4 enforcing

RN dx ρ(x1, . . . , xN) = 1, and β = 1, 2, 4
is called the Dyson index.5 Henceforth, dx = N
j=1 dx j. Note that the eigenvalues
are considered to be unordered here.
Thisjpdfcorrespondsexactly toeigenvalues6 generatedaccordingtothealgorithm
in Chap.1,7 and provided in the code [♠Gaussian_Ensembles_Density.m].
Where does (2.15) come from? Let us postpone the proof for a while and draw
some conclusions by just staring at it for a few minutes.
The Gaussian factor e−1
2
N
i=1 x2
i kills any conﬁguration of eigenvalues {x} where
some x j’s are “big” (far from zero, in absolute value): the eigenvalues do not like
to stay too far from the origin. On the other hand, the term 
j<k |x j −xk| kills
conﬁgurations where two eigenvalues get “too close” to each other.
The “repulsion” factor 
j<k |x j −xk| has another effect: it makes the eigenvalues
strongly non-independent! Every eigenvalue feels the presence of all the others, and
the jpdf (2.15) does not factorize at all. Hence, the classical tools for independent
random variables are of little use here. We will use (2.15) in the next Chapter to
deduce Wigner’s semicircle law in a few simple steps.
This interplay between conﬁnement and repulsion is the physical mechanism at
the heart of many results in RMT.
As a ﬁnal remark, go back to the spacing pdf in Eq. (2.5), which was obtained
for N = 2 and β = 1 (a 2 × 2 GOE matrix). Armed with (2.15) one may redo the
calculation as
p(s) =
 ∞
−∞
dx1dx2ρ(x1, x2)δ(s −|x2 −x1|) .
(2.17)
Try to compute this integral, and recover Eq. (2.5).
4It can be computed via the so-called Mehta’s integral, a close relative of the celebrated Selberg’s
integral [4].
5The Dyson index is equal to the number of real variables needed to specify one entry of your matrix:
1 for real, 2 for complex and 4 for quaternions. This is usually referred to as Dyson’s threefold way.
For the Gaussian ensemble, then, GOE corresponds to β = 1, GUE to β = 2 and GSE to β = 4.
6For β = 4, each matrix has 2N eigenvalues that are two-fold degenerate.
7Quite often, however, you ﬁnd in the literature a Gaussian weight including extra factors, such
as exp(−(β/2) 
i x2
i ) or exp(−(N/2) 
i x2
i ). One then needs to be very careful when compar-
ing theoretical results (obtained with such conventions) to numerical simulations—in particular, a
rescaling of the numerical eigenvalues by √β or
√
N before histogramming is essential in these
two modiﬁed scenarios.

References
13
References
1. P. Šeba, J. Stat. Mech. L10002 (2009)
2. P.L. Hsu, Ann. Hum. Genet. 9, 250 (1939)
3. C.E. Porter, N. Rosenzweig, Ann. Acad. Sci. Fennicae, Serie A VI Physica 44 (1960), reprinted
in C.E. Porter, Statistical Theories of Spectra: Fluctuations (Academic Press, New York, 1965)
4. P.J. Forrester, S.O. Warnaar, Bull. Amer. Math. Soc. (N.S) 45, 489 (2008)

Chapter 3
Classiﬁed Material
In this Chapter, we continue setting up the formalism and provide a simple
classiﬁcation of matrix models.
3.1
Count on Dirac
Question 3.1 From the jpdf of eigenvalues ρ(x1, . . . , xN), how do I compute
the shape of the histograms of the N × T eigenvalues as in Fig.1.1, for T
sufﬁciently large?
▶To cut a long story short, all you have to do is to take the marginal
ρ(x) =

· · ·

dx2 · · · dxNρ(x, x2, . . . , xN) ,
(3.1)
and this function will reproduce the histogram proﬁle you are after for any ﬁnite
N. Note that ρ(x) is correctly normalized to 1, as your histogram is.
Let us prove (3.1).
Take a single, ﬁxed matrix H with real eigenvalues—no randomness in here—and
perform the following task: deﬁne a counting function n(x) such that
 b
a n(x′)dx′
gives the fraction of eigenvalues xi between a and b.
© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_3
15

16
3
Classiﬁed Material
The way to deﬁne it is to set1
n(x) = 1
N
N

i=1
δ(x −xi) ,
(3.2)
the (normalized) sum of a set of “spikes” at the location xi of each eigenvalue. Using
the following property of the delta function

I
dxδ(x −x0) f (x) = f (x0)
if x0 ∈I and 0 otherwise ,
(3.3)
we can show that indeed (3.2) does the job properly.2
If H is now a random matrix, the function n(x) becomes a random measure on
the real line—a function of x that changes from one realization of H to another. The
average of it over the set of random eigenvalues {x1, . . . , xN} becomes interesting
now3
⟨n(x)⟩:=

· · ·

dxρ(x1, . . . , xN)n(x) = 1
N
N

i=1

· · ·

dxρ(x1, . . . , xN)δ(x−xi) = ρ(x),
(3.5)
where ρ(x) =

· · ·

dx2 · · · dxNρ(x, x2, . . . , xN) is the marginal density of ρ. Try
to prove the last equality in (3.5) using the properties of delta function, and the fact
that ρ(x1, . . . , xN) is symmetric upon the exchange xi →x j. This is indeed the case
for the Gaussian jpdf (2.15) and will remain generally true.
The quantity ⟨n(x)⟩= ρ(x) has many names: most often, it is called the (average)
spectral density. Figure3.1 helps you visualize how T = 4 sets of N = 8 randomly
located “spikes” conspire to produce the continuous shape ρ(x) = ⟨n(x)⟩.
1As we know, the Dirac delta function (or rather distribution) δ(x) is basically an extremely peaked
function at the point x = 0, like the limit of a Gaussian pdf as its variance goes to zero, δ(x) =
limε→0+
1
2√πε e−x2/(4ε).
2Compute
N
 b
a
n(x)dx =
N

i=1
 b
a
δ(x −xi)dx =
N

i=1
χ[a,b](xi) ,
(3.4)
where the indicator function χ[a,b](z) is equal to 1 if z ∈(a, b) and 0 otherwise. This is by deﬁnition
the number of eigenvalues between a and b, as it should.
3We use again the shorthand dx = N
j=1 dx j.

3.1 Count on Dirac
17
Question 3.2 If N becomes very large, what does the spectral density ρ(x) for
the Gaussian ensemble look like?
▶For the jpdf ρ(x1, . . . , xN) given in (2.15), the precise statement for the
spectral density ρ(x) =

dx2 · · · dxNρ(x, x2, . . . , xN) is
lim
N→∞

βNρ(

βNx) = ρSC(x) ,
(3.6)
where ρSC(x) =
1
π
√
2 −x2 has a semicircular—or rather, semielliptical—
shape. This is called Wigner’s semicircle law.
What is the meaning of the unexpected rescaling factor √βN?
This means that the histograms of eigenvalues for larger and larger N become
concentrated over the interval [−√2βN, √2βN], in agreement with our
numerical ﬁndings in Fig.1.1. The points ±√2βN are called (spectral) edges.
Note that:
1. The edges are growing with
√
N—bigger matrices have a wider range of
eigenvalues, can you explain why? To get histograms that do not become
wider and wider with N, we need to divide each eigenvalue by √βN before
histogramming. This is what we do in Fig.3.2, using the very same eigen-
values collected to produce Fig.1.1. You can see that the histograms for
different βs nicely collapse on top of each other, reproducing an almost
perfect semielliptical shape between −
√
2 and
√
2.
2. The edges are at ±√2βN for the jpdf ρ(x1, . . . , xN) given in (2.15). If
you put ad hoc extra factors in the exponential, like exp(−(β/2) 
i x2
i ) or
exp(−(N/2) 
i x2
i ), as you sometimes ﬁnd in the literature, this is tanta-
mount to rescaling the eigenvalues by an appropriate factor. For example,
for the choice exp(−(N/2) 
i x2
i ), the edges are ﬁxed—they do not grow
with N—at ±√2β.
3. The edges of the semicircle are called soft: for large but ﬁnite N, there is
always a nonzero probability of sampling eigenvalues exceeding the edge
points. For example, for a GOE matrix 10×10, you have a tiny but nonzero
probability to sample eigenvalues larger than √2βN ≈4.47.... Other
ensembles have spectral densities with hard edges—this means impene-
trable walls, which the eigenvalues can never cross.

18
3
Classiﬁed Material
-1.5
-1
-0.5
0
0.5
1
1.5
x
M1
M2
M3
M4
0
0.2
0.4
ρ(x)
Fig. 3.1 Sets of N = 8 randomly located “spikes”. A histogram of how many spikes occur around
a given region of the real line is nothing but the average spectral density there
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
x
0
0.1
0.2
0.3
0.4
0.5
0.6
ρ(x)
 Semicircle
 GOE
 GUE
 GSE
Fig. 3.2 Rescaled densities for N = 8 (GOE, GUE, GSE)
3.2
Layman’s Classiﬁcation
We deal here with ensembles of square matrices with real eigenvalues (the entries can
be real, complex or quaternionic random variables). Can we classify these ensembles
according to simple features?
A useful scheme (covering several scenarios encountered in real life) is the fol-
lowing (see Fig.3.3):

3.2 Layman’s Classiﬁcation
19
Independent  
Entries
Rotational 
invariance
Gaussian 
Ensembles
ρ[H] = ρ[UHU −1]
ρ[H] ∝
N
i=1 fi(Hii)
i<j fij(Hij)
Fig. 3.3 Visualization of the layman’s classiﬁcation of random matrix ensembles
1. Independent entries: the ﬁrst group on the left gathers matrix models whose
entries are independent random variables—modulo the symmetry requirements.
Random matrices of this kind are usually called Wigner matrices.
Examples: in this category, you may ﬁnd adjacency matrices of random graphs
[1], or matrices with independent power-law entries (so-called Lévy matrices
[2]), and power-law banded matrices [3] among others. Take a moment to down-
load and read these papers—remember the following sentence, found on Richard
Feynman’s blackboard at the time of his death: “Know how to solve every prob-
lem that has been solved”.
2. Rotational invariance: the second group on the right is characterized by the
so-called rotational invariance. In essence, this property means that any two
matrices that are related via a similarity transformation4 H ′ = U HU −1 occur
in the ensemble with the same probability
ρ[H]d H11 · · · d HN N = ρ[H ′]d H ′
11 · · · d H ′
N N .
(3.7)
This requires the following two conditions:
• ρ[H] = ρ[U HU −1]. This means that the jpdf of the entries retains the same func-
tional form before and after the transformation. This imposes a severe constraint
on the allowable functional forms thanks to Weyl’s lemma [4], which states that
ρ[H] can only be a function of the traces of the ﬁrst N powers of H,
ρ[H] = ϕ

Tr H, Tr H 2, . . . , Tr H N	
.
(3.8)
4U is orthogonal/unitary/symplectic if H is real symmetric/complex hermitian/quaternion self-
dual, respectively. You surely have noticed that this is precisely the origin of the names given to the
ensembles: Orthogonal, Unitary and Symplectic.

20
3
Classiﬁed Material
SinceTr H n = Tr (U HU −1)n bythecyclicpropertyofthetrace,the⇐implication
is trivial.
• d H11 · · · d HN N = d H ′
11 · · · d H ′
N N, i.e. the ﬂat Lebesgue measure is invariant
under conjugation by U. This is a classical result.
The rotational invariance property in essence means that the eigenvectors are not that
important, as we can rotate our matrices as freely as we wish, and still leave their
statistical weight unchanged.
Examples: you may ﬁnd in this category the Wishart-Laguerre (Chap.13) and Jacobi
classical ensembles, the so-called “weakly-conﬁned” ensembles [5] and many others.
The same advice (“download-and-study”) applies here.
3. What about the intersection between the two classes? It turns out that it contains
only the Gaussian ensemble5!
This is a consequence of a theorem by Porter and Rosenzweig [6]. And is bad
news, isn’t it? We have to make a choice: if we insist that the ensemble has
independent entries, then eigenvectors do matter. If we require a high level of
rotational symmetry, then the entries get necessarily correlated. No free lunch
(beyond the Gaussian)!
Question 3.3 I can see that the Gaussian ensemble has independent entries. But
I do not easily see that it has this “rotational invariance”.
▶This can be seen from the jpdf of entries in the upper triangle (1.7). Show
that you can rewrite this jpdf as
ρ[Hs] ∝exp

−1
2Tr(H 2
s )

,
(3.9)
where Tr(·) is the matrix trace (the sum of diagonal element). For example, for
the 2 × 2 real symmetric matrix Hs =

a b
b c

, the trace of Hs is a + c, and the
trace of H 2
s is a2 + c2 + 2b2. You can actually rewrite (1.7) as (3.9) only thanks
to that factor 2...check this! Now, from (3.9), the rotational invariance property
is much easier to see: for a similarity transformation Hs′ = U HsU −1, one has
Tr(Hs′2) = Tr(H 2
s ) (cyclic property of the trace).
5In its three incarnations: GOE, GUE and GSE.

3.3 To Know More...
21
3.3
To Know More...
1. Anything worth mentioning beyond the above classiﬁcation? One important
class is represented by the biorthogonal ensembles: these are non-invariant,
with non-independent entries, and yet their jpdf of eigenvalues is known in
terms of the product of two determinants. Check these papers out [7, 8] for
further information.
2. We suggest the following paper [9] about “histogramming without histogram-
ming”. Solid maths and an insightful and unconventional perspective on RMT
spectra.
3. For a proof of the Porter-Rosenzweig theorem in the simpliﬁed 2 × 2 case, as
well as for a nice and pedagogical introduction to the Gaussian ensembles, we
highly recommend the review [10].
4. For the mathematically oriented reader, who is looking for more formal clas-
siﬁcations of random matrix models, we recommend the mini-review [11] and
references therein.
References
1. R. Kühn, J. Phys. A: Math. Theor. 41, 295002 (2008)
2. P. Cizeau, J.P. Bouchaud, Phys. Rev. E 50, 1810 (1994)
3. A.D. Mirlin, Y.V. Fyodorov, F.-M. Dittes, J. Quezada, T.H. Seligman, Phys. Rev. E 54, 3221
(1996)
4. H. Weyl, Classical Groups (Princeton Univ. Press, Princeton, 1946)
5. K.A. Muttalib, Y. Chen, M.E.H. Ismail, V.N. Nicopoulos, Phys. Rev. Lett. 71, 471 (1993)
6. C.E. Porter and N. Rosenzweig, Annals of the Acad. Sci. Fennicae, Serie A VI Physica 44, 1
(1960), reprinted in C.E. Porter, Statistical Theories of Spectra: Fluctuations (Academic Press,
New York, 1965)
7. A. Borodin, Nuclear Physics B 536, 704 (1998)
8. P. Desrosiers, P.J. Forrester, J. Approx. Theory 152, 167 (2008)
9. J.T. Albrecht, C.P. Chan, A. Edelman, Found. Comput. Math. 9, 461 (2008)
10. Y.V. Fyodorov, Introduction to the Random Matrix Theory: Gaussian Unitary Ensemble and
Beyond (2004), https://arxiv.org/pdf/math-ph/0412017.pdf
11. M. Zirnbauer, Symmetry classes in random matrix theory (2004), https://arxiv.org/pdf/math-
ph/0404058.pdf

Chapter 4
The Fluid Semicircle
In this Chapter, we set up a statistical mechanics formalism to compute Wigner’s
semicircle law for Gaussian matrices. You will learn here the so-called “Coulomb
gas technique”.
4.1
Coulomb Gas
The Coulomb gas (or ﬂuid) technique is usually attributed to Dyson [1]. Actually,
a few years before, Wigner had already used it for the derivation of the semicircle
law [2].
Take the jpdf for the Gaussian ensemble (2.15)
ρ(x1, . . . , xN) =
1
ZN,β
e−1
2
N
i=1 x2
i 
j<k
|x j −xk|β ,
(4.1)
and rescale the eigenvalues as xi →xi
√βN.
The normalization constant now reads (set CN,β = (√βN)N+βN(N−1)/2)
ZN,β = CN,β

RN
N

j=1
dx j e−β
2 N N
i=1 x2
i 
j<k
|x j −xk|β = CN,β

RN
N

j=1
dx j e−βN 2V[x] ,
(4.2)
where the energy term in the exponent is
V[x] =
1
2N

i
x2
i −
1
2N 2

i̸= j
ln |xi −x j| .
(4.3)
© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_4
23

24
4
The Fluid Semicircle
-1.5
-1
-0.5
0
0.5
1
1.5
x
Conﬁning well potential
Fig. 4.1 Sketch of the quadratic conﬁning potential, which prevents the particles from escaping
towards ±∞
The factor 1/2 in front of the logarithmic term is due to the symmetrization from
i < j to i ̸= j.
Stare at (4.2) intensely.
We have just exponentiated the product 
j<k, and obtained a canonical partition
function1!
The Gibbs-Boltzmann weight e−βN 2V[x] corresponds to a thermodynamical ﬂuid
of particles with positions {x1, . . . , xN} on a line, in equilibrium at “inverse tem-
perature” β under the effect of competing interactions: a quadratic (single-particle)
potential (see Fig.4.1), and a repulsive (all-to-all) logarithmic term. The ﬂuid is
“static”, as there is no kinetic term in V[x].
The presence of the pre-factor βN 2 shows—at least formally—that the limit
N →∞is a simultaneous thermodynamic and zero-temperature limit. A standard
thermodynamic argument tells us how to ﬁnd the equilibrium positions at zero tem-
perature of the particles (eigenvalues) under such interactions: all we need to do is
to minimize the free energy F = −(1/β) ln ZN,β of this system. The calculation
greatly simpliﬁes in the limit N →∞.
1We are integrating the Gibbs-Boltzmann weight e−βN2V[x] over all possible positions of the
particles.

4.1 Coulomb Gas
25
Question 4.1 Why is this called a “Coulomb” gas?
▶Because we have a logarithmic interaction among charged particles. More
precisely, we have a 2D “ﬂuid” of charges constrained to a line. We know that
in 2D the electrostatic potential generated by a point charge is proportional to
the logarithm of the distance from it—while in 3D, this potential is inversely
proportional to the distance, and in 1D is proportional to the distance. Therefore,
a 2D charged ﬂuid conﬁned to a line is not quite the same as a 1D ﬂuid!
A simple way to see this is by using Gauss’s law, with a single charge q sitting
at the origin on a 2D plane. If we enclose the charge in a 1-sphere S (i.e. a circle),
then we must have

S E · n ∝q, where n is the normal vector to the circle. If
you assume that the electric ﬁeld E is rotationally symmetric, i.e. E = E(r)ˆr,
this turns into E(r)2πr ∝q, implying that E(r) ∝q/r. Integrating a ﬁeld that
goes like 1/r gives you a logarithmic potential.
4.2
Do It Yourself (Before Lunch)
So, our goal is to ﬁnd the free energy F = −(1/β) ln ZN,β for a large number of
particles N →∞. As in many branches of physics, “larger is easier”.
We now provide a “continuum” description of the ﬂuid, based on the following
steps.
1. Introduce a counting function
Deﬁne ﬁrst a normalized one-point counting function
n(x) = 1
N
N

i=1
δ(x −xi) .
(4.4)
This is a random function, satisfying

R dx n(x) = 1 and n(x) ≥0 everywhere.
For ﬁnite N, this is just a collection of “spikes” at the location of each eigenvalue.
However, for large N, it is natural to assume that it will become a smooth function
of x. We will always work under this assumption.2
2It may be helpful to think that n(x) is nothing but the limit for ε →0+ of a nascent delta function
nε(x) =
1
N
N
i=1
e−(x−xi )2/4ε
2√πε
, where the limit ε →0+ is taken at the very end (after the limit
N →∞).

26
4
The Fluid Semicircle
2. Coarse-graining procedure
Instead of directly summing—or rather integrating—over all conﬁgurations of eigen-
values {x1, . . . , xN}, which in stat-mech we would call microstates of our ﬂuid, we
ﬁrst ﬁx a certain one-point proﬁle n(x) (non-negative, smooth and normalized).
Sketch your favorite function over R and call it n(x)—whatever you like,
really, provided it is non-negative, smooth and normalized. Then, we sum over all
microstates {x1, . . . , xN} compatible with your sketch n(x)—in a sense to be made
clearer. Finally, we sum over all possible (non-negative, smooth and normalized)
n(x) you might have come up with in the ﬁrst place.
This coarse-graining procedure can be put on slightly cleaner grounds introducing
the following representation of unity as a functional integral
1 =

D[n(x)]δ

n(x) −1
N
N

i=1
δ(x −xi)
	
,
(4.5)
which enforces the deﬁnition (4.4). The functional integral runs (so to speak) over
all possible normalized, non-negative and smooth functions n(x). See [3] for more
details on functional integrations.
Inserting this representation of unity inside the multiple integral (4.2) and
exchanging the order of integrations, we end up with
ZN,β = CN,β

D[n(x)]

RN
N

j=1
dx j e−βN 2V[x]δ

n(x) −1
N
N

i=1
δ(x −xi)
	
.
(4.6)
3. Convert sums into integrals
Using the identities3
N

i=1
f (xi) = N

R
n(x) f (x)dx
(4.7)
N

i, j=1
g(xi, x j) = N 2

R2 dxdx′n(x)n(x′)g(x, x′) ,
(4.8)
we can rewrite the two terms in the energy (4.3) as
3Prove them inserting the deﬁnition of n(x) into the integrals and using properties of the delta
function.

4.2 Do It Yourself (Before Lunch)
27
1
2N
N

i=1
x2
i =
1
2N × N

R
n(x)x2dx
(4.9)
1
2N 2

i̸= j
ln |xi −x j| =
1
2N 2
⎡
⎣
i, j
ln |xi −x j| −

i
ln Δ(xi)
⎤
⎦=
1
2N 2 × N 2

R2 dxdx′n(x)n(x′) ln |x −x′| −
1
2N 2 × N

R
dx n(x) ln Δ(x) ,
(4.10)
where Δ(x) is a position-dependent short-distance cutoff. What does this mean?
Note that in the limit ε →0+, the double integral

R2 dxdx′nε(x)nε(x′) ln |x −
x′| is divergent. This physically corresponds to the inﬁnite-energy contribution origi-
nated by two neighboring charges getting “too close” to each other (the term i = j in
the sum 
i, j ln |xi −x j|). The term

R dx nε(x) ln Δ(x) for ε →0+ “renormalizes”
the divergence and produces a ﬁnite result. More on how to plausibly ﬁx Δ(x) later.
4. V[x] →V[n(x)]
Note that in (4.9) and (4.10) the sums over eigenvalues {x1, . . . , xN} have been
expressed through the counting function n(x), which—with a slight abuse of
notation—will denote from now on its smooth limit as N →∞.
Therefore we can write
ZN,β = CN,β

D[n(x)]e−βN 2V[n(x)]

RN
N

j=1
dx j δ

n(x) −1
N
N

i=1
δ(x −xi)
	



IN [n(x)]
.
(4.11)
The functional V[n(x)] reads
V[n(x)] = 1
2

R
dx x2n(x) −1
2

R2 dxdx′n(x)n(x′) ln |x −x′| + 1
2N

R
dx n(x) ln Δ(x) .
(4.12)
5. Evaluate the integral IN[n(x)] for large N
We now have to evaluate
IN[n(x)] =

RN
N

j=1
dx j δ

n(x) −1
N
N

i=1
δ(x −xi)
	
(4.13)
in the limit N →∞.
It is quite easy to give a physical interpretation of this multiple integral. It is
basically counting how many microstates—microscopic conﬁgurations of the ﬂuid
charges—arecompatiblewithagivenmacrostate—thedensityproﬁlen(x).Weknow

28
4
The Fluid Semicircle
from standard statistical mechanics arguments that the logarithm of this number
should be proportional to the entropy of the ﬂuid. Let us see how.
Introducing a ‘functional’ analogue of the standard integral representation for the
delta function [4], we can write
IN[n(x)] =

D[ˆn(x)]

RN
N

j=1
dx j exp

iN

dx n(x)ˆn(x) −i

dx ˆn(x)
N

i=1
δ(x −xi)
	
=

D[ˆn(x)] exp

iN

dx n(x)ˆn(x)
 
R
dy e−i

dx ˆn(x)δ(x−y)
N
=

D[ˆn(x)]eN S[ˆn(x)|n(x)] ,
(4.14)
where
S[ˆn(x)|n(x)] = i

dx n(x)ˆn(x) + Log

R
dy e−iˆn(y) .
(4.15)
This type of integrals is music to the statistical physicist’s ears! It is of the form

d(·) exp[Λf (·)], with Λ ≡N a very large parameter. Hence it can be evaluated
with a Laplace (or saddle-point) approximation [5].
Finding the critical point of the action S[ˆn(x)|n(x)]
0 =
δS
δ ˆn(x) = in(x) −i
e−iˆn(x)

R dy e−iˆn(y) ,
(4.16)
from which we obtain
e−iˆn(x) = n(x)

R
dy e−iˆn(y) ⇒iˆn(x) = −ln n(x) −Log

R
dy e−iˆn(y) ,
(4.17)
where we ignore spurious phases (recall that in the complex ﬁeld Log exp(z) may not
just be equal to z!) that would make the action evaluated at the saddle-point complex.
Substituting in (4.15), we obtain
IN[n(x)] ∼exp

−N

dx n(x) ln n(x)

,
(4.18)
to leading order in N. As expected, the term inside square brackets has precisely the
form of the Shannon entropy of the density n(x).
6. Evaluate Δ(x)
Look back again at (4.12). The short-distance cutoff Δ(x) is yet to be ﬁxed.
A standard, physically motivated argument—going back to Dyson for charges on
a ring—posits that Δ(x)—the so-called self-energy term—should be taken of the
form
Δ(x) ≈
c
Nn(x) ,
(4.19)

4.2 Do It Yourself (Before Lunch)
29
as the higher the density of particles around x, the smaller the average distance
between them.4 Also, N charges spread over a distance of O(1) have a mean spacing
∼O(1/N), and this justiﬁes the 1/N factor. This argument, however plausible, does
not seem to have been made rigorous yet, though. Note, in particular, that the constant
c in (4.19) cannot be ﬁxed by this simple heuristic argument. While conceptually
quite important (see e.g. [6]), this missing bit will prove rather inconsequential in
the following.
7. Final expression
Combining (4.11), (4.12), (4.18) and (4.19), the partition function eventually reads
ZN,β ≃CN,β

D[n(x)]e−βN 2F0[n(x)]+ β
2 N ln N+(
β
2 −1)NF1[n(x)]−β
2 N ln c+o(N) , (4.20)
where
F0[n(x)] = 1
2

dx x2n(x) −1
2

dxdx′n(x)n(x′) ln |x −x′| ,
(4.21)
F1[n(x)] =

dx n(x) ln n(x) .
(4.22)
Note that the term (β/2)N ln N is essentially independent of the potential, and
can be absorbed into the overall normalization constant. The O(N) contribution is
composed by i) the self-energy term, ii) the entropic term, and iii) a contribution
coming from the unknown constant c in (4.19).
8. Flash-forward: cross-check with ﬁnite-N result
We now cheat a bit.
Let us use some information we will actually prove later, namely that the equilib-
rium density of the ﬂuid is Wigner’s semicircle law n⋆(x) ≡ρSC(x) = 1
π
√
2 −x2.
Inserting the semicircle law into (4.21) and (4.22)—and evaluating the corre-
sponding integrals—we obtain
F0[n⋆(x)] = 3
8 + ln 2
4
,
(4.23)
F1[n⋆(x)] = 1
2(1 −ln(2) −2 ln(π)) .
(4.24)
Therefore, the partition function (4.20) reads for large N
4We have already met a similar argument in Sect.2.3.

30
4
The Fluid Semicircle
ZN,β ≃CN,β

D[n(x)]e−βN 2F0[n(x)]+ β
2 N ln N+(
β
2 −1)NF1[n(x)]−β
2 N ln c+o(N)
≈CN,βe−βN 2F0[n⋆(x)]+ β
2 N ln N+(
β
2 −1)NF1[n⋆(x)]−β
2 N ln c+o(N)
≈exp
β
4 N 2 ln N + aβ N 2 +
1
2 + β
4

N ln N + bβ N + o(N)

, (4.25)
where we used the easy asymptotics
ln CN,β ∼β
4 N 2 ln N + β
4 (ln β)N 2 + 1 −β/2
2
N ln N + (1 −β/2) ln β
2
N . (4.26)
The constants aβ and bβ are given as follows:
aβ = β
4 ln β −βF0[n⋆(x)] ,
(4.27)
bβ =
β
2 −1

F1[n⋆(x)] + 1 −β/2
2
ln β −β
2 ln c .
(4.28)
Can we check that this result is plausible?
Note that for β = 2, the partition function ZN,β=2 from (2.16) has a particularly
simple expression at ﬁnite N,
ZN,β=2 = (2π)N/2G(N + 2) ,
(4.29)
where G(x) is a Barnes G-function.5 Hence, if everything was done correctly, the
large-N asymptotics of (4.29) should precisely match the large-N behavior (4.25).
Let us check.
Using known asymptotics of the Barnes G-function, we deduce that
ln ZN,β=2 ∼1
2 N 2 ln N −3
4 N 2 + N ln N + N (ln(2π) −1) + O(1) ,
(4.30)
which coincides (up to the term N ln N included) with the asymptotics of ZN,β in
(4.25) once β is set to 2.
This check should convince you that the “mean-ﬁeld” approach—based on a
continuum description of the charged ﬂuid of eigenvalues—is indeed capable of
capturing the ﬁrst three terms of the free energy, and only fails at the level of O(N)
contributions—as the renormalized self-energy term cannot be precisely determined
by a simple-minded scaling argument.
9. What’s next?
Let us recap what we have done so far. The normalization constant ZN,β of the Gaus-
sian model has been re-interpreted as the canonical partition function of a 2D static
5The Barnes G-function is deﬁned via the recursion G(z + 1) = 
(z)G(z), with G(1) = 1.

4.2 Do It Yourself (Before Lunch)
31
ﬂuid of charged particles conﬁned on a line, in equilibrium at inverse temperature
β. For a large number of particles, among all possible conﬁgurations, the ﬂuid will
choose the one that minimizes its free energy, i.e. the logarithm of this partition
function.
The partition function has been written as a functional integral over the space
of normalized counting functions n(x), see (4.20). For large N, it lends itself to a
saddle-point evaluation, which will be carried out in the next Chapter.
References
1. F.J. Dyson, J. Math. Phys. 3, 140 (1962)
2. E. Wigner, Statistical properties of real symmetric matrices with many dimensions, in Canadian
Mathematical Congress Proceedings (University of Toronto Press, Toronto, 1957), p. 174
3. R. MacKenzie, Path Integral Methods and Applications (2000), https://arxiv.org/abs/quant-ph/
0004090
4. J. Rammer, Quantum Field Theory of Non-equilibrium States (Cambridge University Press,
2007)
5. R. Wong, Asymptotic Approximation of Integrals: Computer Science and Scientiﬁc Computing
(Academic Press, 1989)
6. E. Sandier, S. Serfaty, Ann. Probab. 43, 2026 (2015)

Chapter 5
Saddle-Point-of-View
Let us continue the study of the Coulomb gas method for large random matrices.
5.1
Saddle-Point. What’s the Point?
Earlier we showed that the partition function for the Gaussian model could be rep-
resented as
ZN,β ≃CN,β

D[n(x)]e−βN 2F0[n(x)]+ β
2 N ln N+(
β
2 −1)NF1[n(x)]−β
2 N ln c+o(N) ,
(5.1)
where
F0[n(x)] = 1
2

dx x2n(x) −1
2

dxdx′n(x)n(x′) ln |x −x′| ,
(5.2)
F1[n(x)] =

dx n(x) ln n(x) .
(5.3)
Quite interestingly, the leading term in the exponential is of order ∼O(N 2) and
not of ∼O(N) as in standard short-range models. As a consequence of the all-to-all
coupling between the charged particles, the free energy per particle is dominated by
the “energetic” component at the expenses of the “entropic” part (sub-leading for
large N).
Recall now that the functional integral runs over functions n(x) that are normal-
ized, i.e.

R dx n(x) = 1. We can enforce this constraint introducing another delta
function
δ

R
dx n(x) −1

=

R
dk
2π eik(

R dx n(x)−1) .
(5.4)
© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_5
33

34
5
Saddle-Point-of-View
Rescaling ik →βN 2κ and ignoring sub-leading terms, you end up with the truly
appealing representation
ZN,β ≈CN,β

D[n(x)]

R
dκ e−βN 2S[n(x),κ]+O(N) ,
(5.5)
where the action is
S[n(x), κ] = F0[n(x)] −κ

dx n(x) −1

.
(5.6)
A saddle-point evaluation yields1
ZN,β ≈exp(−βN 2S[n⋆(x), κ⋆]) .
(5.7)
Here, n⋆(x) is the minimizer of the functional (5.2) in the space of normalizable
and non-negative functions n(x).
We set up the minimization problem by searching for the critical points2
⎧
⎪⎨
⎪⎩
0 =
δ
δn(x)S[n(x), κ]
 n=n⋆
κ=κ⋆= x2
2 −

R dx′n⋆(x′) ln |x −x′| −κ⋆,
0 =
∂
∂κ S[n(x), κ]
 n=n⋆
κ=κ⋆⇒

R dx n⋆(x) = 1 ,
(5.8)
for x in the support of n⋆(x).
Effectively, κ⋆(hereafter renamed κ for simplicity) is just a Lagrange multiplier
enforcing the normalization

R dx n⋆(x) = 1.
What is then the intensive free energy
f = −(1/βN 2) ln ZN,β
(5.9)
of our Coulomb gas for N →∞? It is just given by f = S[n⋆(x), κ] ≡F0[n⋆(x)]—
the action evaluated at the saddle-point density.
1The pre-factor CN,β has the large-N behavior (4.26), whose logarithm is ∼O(N 2 ln N) and thus
strictly speaking leading with respect to N 2. However, it is just an overall constant term, and the
‘dynamical’ part of the free energy is of ∼O(N 2).
2Note that the factor 1/2 in front of the double integral disappears because the functional differ-
entiation picks up two counting functions, as in the integrand we have n(x)n(x′). An interesting
account on functional differentiation can be found at [1].

5.1 Saddle-Point. What’s the Point?
35
To summarize, the main task is now to ﬁnd the solution of the integral
equation (5.8)
x2
2 −

R
dx′n⋆(x′) ln |x −x′| −κ = 0 ,
(5.10)
satisfying n⋆(x) ≥0 everywhere, and

R n⋆(x)dx = 1.
5.2
Disintegrate the Integral Equation
...or (in more academic terms), solve it.
As a preliminary observation, note that the support of n⋆(x) (i.e. the set of x-
values for which n⋆(x) > 0) cannot be the full real line. In the limit x →∞, the
integral term

R
dx′n⋆(x′) ln |x −x′| ∼ln x

R
dx′n⋆(x′) = ln x ,
(5.11)
—where we used normalization of the density—which is clearly incompatible with
the behavior ∼x2/2 of the known term in the equation.3
Therefore, we need to look for a solution over an interval (a, b) of the real line.
Indeed, a rather amusing feature of this type of integral equations—of the Carleman
class—is that the support over which the solution is to be found is itself unknown,
and part of the problem!
The solution n⋆≡n⋆(x; a, b) we ﬁnd will then be a parametric function of a, b.
We will then ﬁx the ‘optimal’ a, b by requiring that the resulting free energy f in
(5.9) is minimized—i.e. any other choice of the support (a, b) for normalized and
non-negative function ˜n(x) ̸= n⋆(x), once inserted into (5.9), would produce a larger
value for the free energy.
Let us now ﬁrst convert the integral equation into a “simpler” one.
5.3
Better Weak Than Nothing
The solution to the integral equation (5.10) can be obtained by ﬁrst differentiating
both sides with respect to x. Since ln |x −x′| is not (strictly speaking) differentiable
at x = x′, we consider the derivative in the weak sense.
3This is true in general for potentials growing super-logarithmically at inﬁnity—not just for the
quadratic potential corresponding to Gaussian ensembles.

36
5
Saddle-Point-of-View
Let u be a function in L1([a, b]). We say that v ∈L1([a, b]) is a weak derivative
of u if
 b
a
u(x)ϕ′(x)dx = −
 b
a
v(x)ϕ(x)dx
(5.12)
for all inﬁnitely differentiable functions ϕ with ϕ(a) = ϕ(b) = 0. The notion of
weak derivative extends the standard (strong) derivative to functions that are not
differentiable, but integrable in [a, b]. Also, if u is differentiable in the standard
sense, than its weak and strong derivatives coincide—just using integration by parts.
Setting u(x) =

dx′n⋆(x′) ln |x −x′|, we can write

ϕ′(x)

dx′n⋆(x′) ln |x −x′|

dx = 1
2 lim
ε→0

ϕ′(x)

dx′n⋆(x′) ln[(x −x′)2 + ε2]

dx
= −1
2

ϕ(x)

dx′n⋆(x′)
2(x −x′)
(x −x′)2 + ε2

dx = −

ϕ(x)dx

Pr

dx′ n⋆(x′)
x −x′

,
(5.13)
where Pr stands for Cauchy’s principal value.4
Comparing with (5.12), we obtain that the weak derivative of u(x) is Pr

dx′ n⋆(x′)
x−x′ ,
therefore the new (singular) integral equation to be solved now is
Pr

dx′ n⋆(x′)
x −x′ = x .
(5.14)
To solve (5.14), we invoke a theorem by Tricomi [2], stating that
Pr
 b
a
dx′ f (x′)
x −x′ = g(x) ⇒f (x) =
C −Pr
 b
a
dt
π
√(t−a)(b−t)
x−t
g(t)
π√(x −a)(b −x)
,
(5.15)
provided that [a, b] is a single (compact) support and C is an arbitrary constant.
Question 5.1 Who tells me that the optimal counting function n⋆(x) is support-
ed on a single interval [a, b]?
▶There is some nice physical intuition behind this. The “thermodynamical”
interpretation of the eigenvalues implies that the gas of particles is conﬁned by a
quadratic well with a single minimum (see Fig.4.1). It is then physically reason-
able to foresee that the particles will ﬁll the single minimum of the potential. If
a potential has many minima, then it is possible that n⋆(x) “splits” into as many
connected components as the number of minima of the potential. Any attempt
to use (5.15) in these multiple-support cases will produce unphysical solutions.
4This means precisely the limit limε→0
 x−ε F(x′)dx′ +

x+ε F(x′)dx′
, if x is a singular point
of F(x).

5.3 Better Weak Than Nothing
37
Evaluating the principal value integral with g(t) = t and imposing the normal-
ization
 b
a dx n⋆(x) = 1, we get
n⋆(x) =
1
π√(x −a)(b −x)

1 −x2 + 1
2(a + b)x + 1
8(b −a)2

.
(5.16)
Note that the density in (5.16) is a solution of the integral equation (5.14) between
a and b for any choice of a and b. How to ﬁx the “optimal” a and b will be the subject
of the next sections.
[Of course, do not even consider trusting us on this. You are not allowed to proceed
until you have derived (5.16) yourself. Sorry.]
5.4
Smart Tricks
Now, stare at (5.16) intensely. As promised, the function n⋆(x) (deﬁned for x ∈
(a, b)) indeed depends on two free parameters a and b.
We need now to compute the intensive free energy
f = F0[n⋆(x)] .
(5.17)
It will of course depend as well on the two free parameters a and b, which arose
as a Phoenix from the ashes of the integral equation (5.14).
A couple of smart tricks will make our life easier. First, we would really like to
get rid of the double integral in
f ≡F0[n⋆(x)] = 1
2

dx x2n⋆(x) −1
2

dxdx′n⋆(x)n⋆(x′) ln |x −x′| . (5.18)
To do that, we multiply the saddle point Eq.(5.10)
x2
2 −

dx′n⋆(x′) ln |x −x′| −κ = 0
(5.19)
by n⋆(x) and integrate over x. This way we obtain

dxdx′n⋆(x)n⋆(x′) ln |x −x′| = 1
2

dx n⋆(x)x2 −κ ,
(5.20)
where we used

n⋆(x)dx = 1.

38
5
Saddle-Point-of-View
Next, we ﬁx the Lagrange multiplier κ by setting x = a in (5.19). We obtain
κ = a2/2 −
 b
a dx n⋆(x) ln(x −a). Combining everything, we get
f ≡F0[n⋆(x)] = 1
4
 b
a
dx n⋆(x)x2 + a2
4 −1
2
 b
a
dx n⋆(x) ln(x −a) .
(5.21)
No more κ, and no more double integrals. Nice, uh?
5.5
The Final Touch
Inserting (5.16) into (5.21) and computing the integrals with the help of an abacus,5
we obtain
f ≡f (a, b) =
1
512

−9a4 + 4a3b + 2a2 
5b2 + 48

+ 4ab

b2 + 16

−256 ln(b −a) −9b4 + 96b2 + 512 ln(2)

.
(5.22)
We
now
have
our
(quite
ugly)
intensive
free
energy:
In
the
code
[♠integral_check.m] we provide a simple numerical conﬁrmation that the
above result is equivalent to (5.21).
All we need to do is to minimize it with respect to a and b—the (soft) edge points
of the support of n⋆(x).
If you do that, you will obtain the solution6 a = −
√
2 and b =
√
2, which imply
for n⋆(x) from (5.16) the following form
n⋆(x) ≡ρSC(x) = 1
π

2 −x2 ,
(5.23)
the famous Wigner’s semicircle law. Very appropriate name, given that it is not the e-
quation
of
a
semicircle,
but
rather
of
a
semi-ellipse.
The
code
[♠Tricomi_check.m] offers a numerical veriﬁcation that the semicircle indeed
solves Eq.(5.14) for a = −b = −
√
2.
How to show this analytically, though?
We need to prove that
Pr
 √
2
−
√
2
dx′
√
2 −x′2
π(x −x′) = x .
(5.24)
5It may be useful to ﬁrst change variables z = (x −a)/(b −a). The resulting integrals can then be
handled by most symbolic computation programs.
6The fact that the soft edges are symmetrically located around the origin is a consequence of the
symmetry of the conﬁning potential under the exchange x →−x.

5.5 The Final Touch
39
The primitive of the integrand is—ignoring an additive constant
F(y) =
√
2 −x2 ln (ϕ(x, y)) −
√
2 −x2 ln(x −y) + x arcsin

y
√
2

−

2 −y2
π
,
(5.25)
where
ϕ(x, y) =

2 −x2
2 −y2 −xy + 2 .
(5.26)
Hence all you have to show is
lim
ε→0+

F(x −ε) −F

−
√
2

+ F
√
2

−F(x + ε)

= x,
−
√
2 ≤x ≤
√
2 .
(5.27)
Have a go at it!
5.6
Epilogue
What is again the interpretation of the “semicircular” n⋆(x)? It is just the equilibrium
proﬁle of a gas of many charged particles on a line, which minimizes the free energy
of the gas. In the “eigenvalue” language, it represents the normalized histogram of
the N eigenvalues of a single (very big!) instance of the Gaussian ensemble. The
property that this object also faithfully represents the spectrum averaged over many
samples (i.e. n⋆(x) = ⟨n(x)⟩= ρ(x)) is called self-averaging and we will assume it
to hold.
The code [♠Coulomb_gas.m] provides a numerical veriﬁcation of what we
worked on in this Chapter and the previous one. It simulates the Coulomb gas through
a simple Monte Carlo procedure, which produces the equilibrium density for long
enoughtimes.Also,anumericalcheckofthesemicircledistributioncanbeperformed
directly, i.e. through the numerical diagonalization of random matrices, with the code
[♠Gaussian_finite_N_rescaled.m] (Fig.5.1).
Note that, at the very beginning of the derivation of (5.23), we rescaled the eigen-
values by √βN (Eq.4.2). Therefore, in the simulations we need to perform the same
rescaling of our eigenvalues by √βN before comparing the histogram to the theoret-
ical semicircle. This is in agreement with the precise statement we made in Question
3.2, namely
lim
N→∞

βNρ(

βNx) = ρSC(x) ,
(5.28)
where the function ρSC(x) = 1
π
√
2 −x2 ≡n⋆(x) is β-independent.

40
5
Saddle-Point-of-View
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
x
0
0.1
0.2
0.3
0.4
0.5
ρSC(x)
N = 10
N = 102
N = 103
Fig. 5.1 Numerical check of the semicircle law for GOE. Increasing the value of N, after a suitable
rescaling, the eigenvalue histograms collapse on top of the semicircle curve
As a ﬁnal remark, what happens if the conﬁning potential is not quadratic? In
general, if our invariant ensemble is characterized by a joint probability density of
the entries of the form
ρ[H] ∝exp [−TrV (H)] ,
(5.29)
then the joint law of the eigenvalues is of the form
ρ(x1, . . . , xN) ∝exp

−
N

i=1
V (xi)
 
j<k
|x j −xk|β
(5.30)
and the analogue of the Tricomi equation for the spectral density is
Pr

dx′ n⋆(x′)
x −x′ = V ′(x) .
(5.31)
Try to solve for n⋆(x) in the case V (x) = x −α ln x (x > 0). This will correspond
to the Wishart-Laguerre ensemble of random matrices, which will be extensively
discussed in Chap.13.
Question 5.2 Do all existing random matrix ensembles have the semicircle as
their average spectral density?
▶Certainly not! The spectral density is highly non-universal—i.e. it strongly
depends on the ensemble you consider. This said, it is true that many ensem-
bles share it as their spectral density for large N. This is the case for instance

5.6 Epilogue
41
of Wigner ensembles (non-invariant), when the distribution of entries decays
sufﬁciently fast at inﬁnity (see [3]).
Question 5.3 What are the moments of the semicircle law?
▶They are given by the so called Catalan numbers. More precisely, deﬁning
⟨TrXk⟩=

dx1 · · · dxNρ(x1, . . . , xN)
N

i=1
xk
i = N

dx xkρ(x) ,
(5.32)
where ρ(x1, . . . , xN) is the jpdf for the Gaussian ensemble (2.15) and ρ(x) its
one-point marginal for ﬁnite N, we have the relation
lim
N→∞
⟨TrX2n⟩
βnN n+1 = 1
π
 √
2
−
√
2
dy y2n
2 −y2 = Cn
2n ,
(5.33)
where Cn =
1
n+1
2n
n

is the nth Catalan number. Catalan numbers occur in a
variety of combinatorial problems, for example Cn is the number of ways to
correctly match n pairs of brackets.
Question 5.4 I see that the Coulomb gas treatment is insensitive to the precise
value of β. But is it possible to construct an explicit random matrix ensem-
ble ρ[H], whose eigenvalues are distributed according to a Coulomb gas with
β ̸= 1, 2, 4?
▶Yes! This has been achieved by Dumitriu and Edelman [4], who produced
ensembles of tridiagonal matrices—hence non-invariant—with independent but
not identically distributed nonzero entries, whose jpdf of eigenvalues can be n-
evertheless computed analytically. This jpdf turns out to be equal to the Gaussian
or Wishart-Laguerre ones, albeit with a continuous Dyson index β > 0 (it enters
as a parameter of the distribution of the nonzero entries). These ensembles are
very useful also on the numerical side: they provide a much faster way to sample
GXE-distributed eigenvalues (with X = O, U, S), without having to diagonalize
full Gaussian matrices!

42
5
Saddle-Point-of-View
Question 5.5 If I drop the symmetry requirements on the entries of the ensemble
(Hi j ̸= Hji), what is the resulting analogue of the semicircle law for complex
eigenvalues?
▶This is called the Girko-Ginibre (or circular) law. In essence, for any sequence
of random N × N matrices whose entries are i.i.d. random variables, all with
mean zero and variance equal to 1/N, the limiting spectral density is the uniform
distribution over the unit disc in the complex plane.
5.7
To Know More...
1. The Gaussian ensemble for β = 2. The eigenvalues can be interpreted as the
positions of fermions in a harmonic trap. To understand this mapping, have a
look at [5] and references therein.
2. Recently, the Coulomb gas technique has been improved and modiﬁed to tackle
a wealth of different problems. It all started with a beautiful calculation on the
following problem: what is the probability that all the eigenvalues of a Gaussian
matrix are negative? Check this paper out [6].
3. The Gaussian ensembles can also come in a variant called ﬁxed-trace: this means
that one multiplies the jpdf (4.1) by δ
N
i=1 x2
i −t

, which ﬁxes the squared
trace to the value t (see [7, 8] for details).
4. The normalization constant ZN,β for the Gaussian ensemble can be computed for
ﬁnite N, with simple algebraic manipulations on the so called Selberg integral
 1
0
dx|ΔN(x)|β
N

i=1
xa−1
i
(1 −xi)b−1 .
(5.34)
It was computed by the norwegian mathematician A. Selberg, who showed that,
when it exists, it is given by
N

j=1
Γ (1 + βj/2)Γ (a + (N −j)β/2)Γ (b + (N −j)β/2)
Γ (1 + β/2)Γ (a + b + β(2N −j −1)/2)
.
(5.35)
To know more about recent developments in the beautiful theory of Selberg
integrals, have a look at [9].

References
43
References
1. E.Engel,R.M.Dreizler,DensityFunctionalTheory:AnAdvancedCourse(Springer,Heidelberg,
2011)
2. F.G. Tricomi, Integral Equations (Dover publications, 1985)
3. L. Erdös, Russ. Math. Surv. 66, 507 (2011)
4. I. Dumitriu, A. Edelman, J. Math. Phys. 43, 5830 (2002)
5. R. Marino, S.N. Majumdar, G. Schehr, P. Vivo, Phys. Rev. Lett. 112, 254101 (2014)
6. D.S. Dean, S.N. Majumdar, Phys. Rev. E 77, 041108 (2008)
7. G. Akemann, G.M. Cicuta, L. Molinari, G. Vernizzi, Phys. Rev. E 59, 1489 (1999)
8. G. Akemann, G.M. Cicuta, L. Molinari, G. Vernizzi, Phys. Rev. E 60, 5287 (1999)
9. P.J. Forrester, S.O. Warnaar, Bull. Am. Math. Soc. (N.S.) 45, 489 (2008)

Chapter 6
Time for a Change
In this Chapter, we show how to compute the jpdf of eigenvalues for random matrix
models—whenever possible.
6.1
Intermezzo: A Simpler Change of Variables
Suppose we have to compute the following double integrals
I1 =

R2 dx dy ρ1(x, y)
I2 =

R2 dx dy ρ2(x, y) ,
(6.1)
with ρ1(x, y) = f (x2 + y2) and ρ2(x, y) = x f (x2 + y2). Here, f (t) is a function
of your choice that makes both integrals convergent.
A good strategy is to make the “polar” change of variables {x, y} = {r cos θ,
r sin θ} to write
I1 =
 ∞
0
dr
 2π
0
dθ ˆρ1(r, θ),
I2 =
 ∞
0
dr
 2π
0
dθ ˆρ2(r, θ) ,
(6.2)
where ˆρ1(r, θ) = r f (r2) and ˆρ2(r, θ) = r2 cos θ f (r2). Obviously, we had to include
here the extra Jacobian factor
J(r, θ) =
 ∂x
∂r
∂x
∂θ
∂y
∂r
∂y
∂θ

= r .
(6.3)
Therefore, we can formally write ρ1(x, y)dxdy = ˆρ1(r, θ)drdθ (and similarly
for ρ2), meaning that the two expressions give the same result once integrated over
“corresponding” domains (e.g. R2 →(0, ∞) × (0, 2π)).
This is all trivial and easy. But together with the following two remarks, it is all
you need to know to fully understand what happens in the RMT case, with jpdf of
entries and eigenvalues all over the place.
© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_6
45

46
6
Time for a Change
1. ˆρ1(r, θ) (the new integrand) is nothing but ρ1(r cos θ,r sin θ) × |J(r, θ)| (the old
integrand, written in terms of the new variables, times the Jacobian factor)—and
similarly for ˆρ2.
2. The marginal ˆρ1(r) =
 2π
0
dθ ˆρ1(r, θ) is easier to compute than the corresponding
ˆρ2(r). This for two reasons: i) the original ρ1(x, y), once expressed in the new
polar variables, no longer depends on one of them (θ), and ii) also the Jacobian
does not depend on θ. So the integration in θ becomes trivial and gives just a
constant factor 2π.
6.2
...that Is the Question
Take the case of real symmetric matrices for simplicity—call them H instead of Hs
from now on.
Look again at the jpdf of eigenvalues (2.15) for the GOE ensemble (β = 1)
ρ(x1, . . . , xN) =
1
ZN,β=1
e−1
2
N
i=1 x2
i 
j<k
|x j −xk| .
(6.4)
We gave it without proof.
How to obtain it from the jpdf of entries in the upper triangle, ρ[H]
ρ[H] =
N

i=1
e−H 2
ii/2
√
2π

i< j
e−H 2
i j
√π ?
(6.5)
In this Chapter, we provide an answer to this outstanding question.
6.3
Keep Your Volume Under Control
A real symmetric matrix can be diagonalized by an orthogonal matrix O as H =
O X OT , with X = diag(x1, . . . , xN).
Orthogonal N × N matrices are characterized by the property that OOT = 1,
where 1 is the identity matrix. As a subspace of RN 2, these matrices form a sub-
manifold VN of dimension N(N −1)/2, called the Stiefel manifold. dO is precisely
its “volume element”- the analog of dθ in the warm-up example above.
We know that
 2π
0
dθ = 2π. It is perhaps intuitive to give this number 2π the
meaning of “volume” occupied while dθ spans the entire one-dimensional mani-
fold (the circumference of the unit circle). What is, then, the “volume”occupied by
orthogonal matrices in RN 2?
A relatively simple calculation [1] shows that

6.3 Keep Your Volume Under Control
47
Vol(VN) =

VN
dO = 2Nπ N 2/2
N(N/2) ,
(6.6)
where
m(a) = πm(m−1)/4
m

i=1
(a −(i −1)/2) .
(6.7)
We will use this result in a minute.
If we call
DO =
dO
Vol(VN) ,
(6.8)
this deﬁnes the so-called Haar measure on the orthogonal group. The Haar measure is
invariant under orthogonal conjugation, and deﬁnes a probability space on orthogonal
matrices. For further information, consult [1–4].
6.4
For Doubting Thomases...
Let us compute the volume Vol(V2) for 2 × 2 orthogonal matrices “from ﬁrst prin-
ciples”.1
Let
O =
o11 o12
o21 o22

.
(6.9)
The {oi j} are real variables. The volume we are after is
Vol(V2) =

2

i, j=1
doi jδ

o2
11 + o2
21 −1

δ

o2
12 + o2
22 −1

δ(o11o12 + o21o22) ,
(6.10)
where the delta functions enforce the constraints on the columns of O being orthog-
onal with each other, and each having unit norm.
1Alternatively, one may notice that the elements of V2 can be written either in the form

cos θ
sin θ
−sin θ cos θ

(rotations in the plane by an angle θ) or in the form

cos θ
sin θ
sin θ −cos θ

(rotations
followed by a reﬂection). That is, this group has two disconnected components. Clearly, each of
these components has a volume 2π, so the volume of V2 is 4π.

48
6
Time for a Change
Changing to polar coordinates, we get
Vol(V2) =
 2π
0
dθ
 2π
0
dφ
 ∞
0
dr rδ(r −1)
 ∞
0
d R Rδ(R −1)δ (r R cos(θ −φ))
=
 2π
0
dθ
 2π
0
dφδ (cos(θ −φ)) = 4π ,
(6.11)
in agreement with (6.6) for N = 2 as it should.
6.5
Jpdf of Eigenvalues and Eigenvectors
As in Sect.6.1—but this time with more variables—we are after the change of vari-
ables H →{x, O}
ρ(H11, . . . , HN N)

i≤j
d Hi j = ρ(H11(x, O), . . . , HN N(x, O))
			J(H →{x, O})
			



ˆρ(x1,...,xN ,O)
dO
N

i=1
dxi .
(6.12)
On the left hand side, the jpdf of the N(N + 1)/2 entries of H in the upper
triangle, including the diagonal. On the right hand side, the jpdf ˆρ of both eigenvalues
(N) and independent eigenvector components (N(N −1)/2, the dimension of the
Stiefel manifold spanned by the orthogonal group over the reals). The number of
“degrees of freedom” is OK, thanks to the mind-wrecking and highly nontrivial
identity N(N + 1)/2 = N + N(N −1)/2.
Clearly, on the right hand side we had to include the Jacobian of the change of
variables, which we are going to compute below. While in principle this Jacobian
could depend on the full set of variables {x, O}, it turns out that it only depends on
the eigenvalues {x}, exactly as it happens for the change to polar coordinates (6.3).
In our RMT case, this Jacobian is precisely the so-called Vandermonde determi-
nant,2
J(H →{x, O}) =

j>k
(x j −xk) .
(6.13)
This can be generalized to the hermitian and quaternion self-dual cases. The only
difference is that the Vandermonde is then raised to the power β = 2, 4 respectively.
We will prove this in the next Chapter.
2Why this is indeed a determinant in disguise will become clearer very shortly.

6.6 Leave the Eigenvalues Alone
49
6.6
Leave the Eigenvalues Alone
Now, stare at the right hand side of (6.12) carefully.
The joint probability density of eigenvalues and eigenvectors ˆρ(x1, . . . , xN, O)
is the product of two terms: the jpdf of entries—written as a function of eigenvalues
and eigenvectors—times the Jacobian—which is a function of the eigenvalues alone.
Then the next question is: how can I get the jpdf of eigenvalues alone? Well, you
will need to integrate out the eigenvector components {O} in (6.12). More precisely
ˆρ(x1, . . . , xN)dx = dx

VN
dO ˆρ(x1, . . . , xN, O) ,
(6.14)
exactly as we did earlier on to ﬁnd ˆρ1,2(r) from ˆρ1,2(r, θ). And exactly as in that
case, this integration over VN may or may not be easy/possible to perform explicitly.
It is certainly possible when the original jpdf of entries, once expressed in terms of
eigenvalues and eigenvector components, is itself independent of eigenvectors—in
complete analogy with our previous example with r and θ. In this case, we would
get
ˆρ(x1, . . . , xN, O) ≡ρ(H11(x, 

Z
Z
O ), . . . , HN N(x, 

Z
Z
O ))
			J(H →{x, 

Z
Z
O })
			
= function of x alone ,
(6.15)
and all is left to do in (6.14) is the “volume” integral

VN dO, yielding the simple
constant in (6.6)—much like 2π in the warm-up example with r, θ above.
The prototypes of this favorable case are the rotationally invariant ensembles, see
the next section.3
6.7
For Invariant Models...
We can now formulate a cute little theorem for invariant models [2]. The proof is
given below.
Let the real symmetric N × N matrix H have a jpdf of entries ρ[H] =
φ

Tr H, . . . , Tr H N
, which is evidently invariant under orthogonal similarity trans-
formations.4 Then the jpdf of the N ordered eigenvalues of H (x1 ≥x2 ≥· · · ≥xN)
is
3Instead, for models with independent entries, the jpdf of entries cannot be—in general—written in
terms of the eigenvalues alone. For such models, the jpdf of eigenvalues is therefore not generally
known.
4Recall Weyl’s lemma (3.8).

50
6
Time for a Change
ρord(x1, . . . , xN) =
π N 2/2
N(N/2)φ

i
xi, . . . ,

i
x N
i
 
i< j
(xi −x j) .
(6.16)
Note that there is no absolute value around the Vandermonde, as the eigenvalues
are ordered.
Let us see how this theorem works in practice for the GOE case. We have
ρ[H] =
N

i=1
e−H 2
ii/2
√
2π

i< j
e−H 2
i j
√π =
1
(2π)N/2π
N2−N
4
exp

−1
2Tr H 2

.
(6.17)
Therefore, applying the theorem above
ρord(x1, . . . , xN) =
π N 2/2
N(N/2) ×
1
(2π)N/2π
N2−N
4



1
Z(ord)
N,β=1
e−1
2
N
i=1 x2
i 
i< j
(xi −x j) , (6.18)
which needs to be compared with Eq.(2.15) for β = 1—given without proof at the
time
ρ(x1, . . . , xN) =
1
ZN,β=1
e−1
2
N
i=1 x2
i 
j<k
|x j −xk| ,
(6.19)
with
ZN,β=1 = (2π)N/2
N

j=1
(1 + j/2)
(3/2)
.
(6.20)
Do the two Eqs.(6.18) and (6.19) indeed agree, as they should? Almost.
Notice that (6.18) holds for ordered eigenvalues, while (6.19) holds for unordered
eigenvalues (hence the need to include the absolute value). The two normalization
constants differ indeed by a factor N!
ZN,β=1 = N!Z(ord)
N,β=1 .
(6.21)
6.8
The Proof
Where does the normalizing factor
π N 2/2
N(N/2)
(6.22)
in (6.16) come from? It is instructive to look at this derivation more closely.

6.8 The Proof
51
Recall from (6.14) and (6.15) that (for the favorable case where one can integrate
out the eigenvectors)
jpdf eigenv. = jpdf entries (as function of eigenv. alone) × |Vandermonde| ×

VN
dO .
(6.23)
This means that morally the normalizing factor (6.22) should correspond to the
volume integral

VN dO as in (6.6) (for β = 1, or

˜VN dU over unitary matrix
elements for β = 2 etc.).
There is a subtlety though: the change of variables between entries and eigenvalues
(H →O X OT ) must be one-to-one. But eigenvectors are deﬁned up to a phase,
e.g. if v is a real eigenvector, so is −v. To guarantee the uniqueness of the eigen-
decomposition, it is sufﬁcient to ﬁx the sign of the ﬁrst row of the matrix O, or the
phases of the ﬁrst row of the matrix U. This reduces the volume integral

VN dO by
a factor 2N in the orthogonal case, and the volume integral

˜VN dU by (2π)N in the
unitary case. And the proof is complete.
References
1. R.J. Muirhead, Aspects of multivariate statistical theory (John Wiley & Sons, 2009)
2. A. Edelman, Eigenvalues and Condition Numbers of Random Matrices, MIT Ph.D. Dissertation
(1989)
3. A. Edelman, Finite random matrix theory. Jacobians of matrix transforms (without wedge prod-
ucts) (2005), http://web.mit.edu/18.325/www/handouts/handout2.pdf
4. A.M. Mathai, Jacobians of matrix transformation and functions of matrix arguments (World
Scientiﬁc Publishing Co Inc, 1997)

Chapter 7
Meet Vandermonde
The “repulsive” term between eigenvalues of invariant models 
i< j(x j −xi) can be
written as a determinant, called Vandermonde in honor of the French mathematician
Alexandre-Théophile Vandermonde (who never wrote it [1]).
7.1
The Vandermonde Determinant
We have the following identity
ΔN(x) :=
N

i< j
(x j −xi) = det(xi−1
j
) = det
⎛
⎜⎜⎜⎜⎜⎜⎝
1
. . .
1
x1
. . .
xN
.
.
.
.
.
.
.
.
.
x N−1
1
. . . x N−1
N
⎞
⎟⎟⎟⎟⎟⎟⎠
.
(7.1)
The Vandermonde is clearly a completely anti-symmetric polynomial in N vari-
ables: take for example N = 3. We have Δ3(x) = (x2 −x1)(x3 −x1)(x3 −x2). Now,
exchange any two x js: for example, x3 ↔x2. We get −Δ3(x) (we pick up a minus
sign any time we make any exchange of two x js).
The Vandermonde has a quite funny property: we can understand it already on a
2 × 2 matrix. Take
det

 1 1
x1 x2

= x2 −x1
det

1
1
3x1 + 17 3x2 + 17

= 3(x2 −x1) .
(7.2)
Stare at these two determinants carefully. We have just replaced the second row of
the ﬁrst matrix (containing ﬁrst powers of x1 and x2) with a ﬁrst degree polynomial.
© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_7
53

54
7
Meet Vandermonde
The result is just 3 times the Vandermonde on the left. The 17 has disappeared
altogether! This means that you have a lot of freedom in devising a matrix whose
determinant gives the Vandermonde.
More formally, the entries xk
i in the (k +1)th row can be replaced, up to a constant
factor a0a1 · · · aN−1, by a polynomial of degree k of the form: πk(xi) = akxk
i + · · · ,
where we omit terms of lower order in xi. The important point is that these lower
order terms can be absolutely anything. The result is that:
ΔN(x) =
1
a0a1 · · · aN−1
det
⎛
⎜⎜⎜⎜⎜⎜⎝
π0(x1)
. . .
π0(xN)
π1(x1)
. . .
π1(xN)
.
.
.
.
.
.
.
.
.
πN−1(x1) . . . πN−1(xN)
⎞
⎟⎟⎟⎟⎟⎟⎠
.
(7.3)
Orthogonal polynomials are an important class of polynomials πk(x) that can
be especially useful to play this trick. We will discuss in Chap.10 how this simple
property can actually turn seemingly impossible calculations into feasible ones.
For instance, let us show how the Hermite and Laguerre orthogonal polynomials
can be used to express the Vandermonde. For N = 3 it is easy to see that
det
⎛
⎝
H0(x1) H0(x2) H0(x3)
H1(x1) H1(x2) H1(x3)
H2(x1) H2(x2) H2(x3)
⎞
⎠= det
⎛
⎝
1
1
1
x1
x2
x3
x2
1 −1 x2
2 −1 x2
3 −1
⎞
⎠= Δ3(x), (7.4)
and that −Δ3(x)/2 =
det
⎛
⎝
L0(x1) L0(x2) L0(x3)
L1(x1) L1(x2) L1(x3)
L2(x1) L2(x2) L2(x3)
⎞
⎠= det
⎛
⎜⎝
1
1
1
−x1 + 1
−x2 + 1
−x3 + 1
x2
1
2 −2x1 + 1 x2
2
2 −2x2 + 1 x2
3
2 −2x3 + 1
⎞
⎟⎠.
(7.5)
7.2
Do It Yourself
We now derive the nontrivial relation (6.13) J(H →{x, O}) = ΔN(x) for real
symmetric matrices H. We stress that this proof does not require any assumption on
the rotational invariance of the ensemble.

7.2 Do It Yourself
55
These can be diagonalized through an orthogonal transformation H = O X OT ,
where X = diag(x1, . . . , xN). To ﬁnd the Jacobian, we formally differentiate1 H,
δH = (δO)X OT + O(δX)OT + O X(δOT ) ,
(7.6)
and use δOT = −OT (δO)OT , which follows from OOT = 1. We get
δH = (δO)X OT + O(δX)OT −O X OT (δO)OT .
(7.7)
Pulling out a factor O to the left and OT to the right we obtain δH = O(δ ˆH)OT ,
where
δ ˆH = (δΩ)X −X(δΩ) + δX .
(7.8)
Here, δΩ = OT δO is an antisymmetric matrix.2 Since δH and δ ˆH are related via
an orthogonal transformation, we only have to ﬁnd the Jacobian of δ ˆH →{δX, δΩ}.
Noting that δX is diagonal, we can write
d ˆHi j = dΩi j(x j −xi) + dxiδi j .
(7.9)
This is equivalent to the following differential relations:
d ˆHi j
dxk
= δi jδik,
d ˆHi j
dΩkℓ
= δikδ jℓ(x j −xi) .
(7.10)
Don’t you see the Vandermonde trying hard to crop up here © ?
Let us now construct the Jacobian matrix J for a concrete 3 × 3 case. The gener-
alization to the N × N case will then appear obvious. The matrix J has dimension
N(N+1)
2
, so it is a 6 × 6 matrix for N = 3. We parametrize the antisymmetric matrix
δΩ as follows:
δΩ =
⎛
⎝
0
Ω12 Ω13
−Ω12
0
Ω23
−Ω13 −Ω23
0
⎞
⎠.
(7.11)
Then the Jacobian matrix becomes:
1The matrix element Hi j can be written as Hi j = 
ℓ,m OiℓXℓm O jm = 
ℓOiℓxℓO jℓ. The
inﬁnitesimal matrix δH has entries (δH)i j = d Hi j given by the differential of Hi j. Equation(7.6)
is a shorthand of this explicit differentiation w.r.t. Oiℓ, xℓand O jℓ.
2Obviously, you need to prove it before proceeding.

56
7
Meet Vandermonde
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
d ˆH11
dΩ12
d ˆH11
dΩ13
d ˆH11
dΩ23
d ˆH11
dx1
d ˆH11
dx2
d ˆH11
dx3
d ˆH12
dΩ12
d ˆH12
dΩ13
d ˆH12
dΩ23
d ˆH12
dx1
d ˆH12
dx2
d ˆH12
dx3
d ˆH13
dΩ12
d ˆH13
dΩ13
d ˆH13
dΩ23
d ˆH13
dx1
d ˆH13
dx2
d ˆH13
dx3
d ˆH22
dΩ12
d ˆH22
dΩ13
d ˆH22
dΩ23
d ˆH22
dx1
d ˆH22
dx2
d ˆH22
dx3
d ˆH23
dΩ12
d ˆH23
dΩ13
d ˆH23
dΩ23
d ˆH23
dx1
d ˆH23
dx2
d ˆH23
dx3
d ˆH33
dΩ12
d ˆH33
dΩ13
d ˆH33
dΩ23
d ˆH33
dx1
d ˆH33
dx2
d ˆH33
dx3
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
=
⎛
⎜⎜⎜⎜⎜⎜⎝
0
0
0
1 0 0
x2 −x1
0
0
0 0 0
0
x3 −x1
0
0 0 0
0
0
0
0 1 0
0
0
x3 −x2 0 0 0
0
0
0
0 0 1
⎞
⎟⎟⎟⎟⎟⎟⎠
. (7.12)
Swapping rows and columns, it is possible to bring this to the diagonal form, so
that the determinant becomes trivial to compute. In the general N case, one has:
| det J| =

j<k
|x j −xk| ,
(7.13)
as expected. The proof in the complex hermitian and quaternion self-dual cases is
analogous and is left as an exercise.
For a nice numerical test of the Jacobian identity (7.13), we refer to [2], Sect.3.2,
while for a “back-of-the-envelope” derivation based on counting degrees of freedom,
see [3].
We will make extensive use of the Vandermonde determinant and its properties
in Chap.10.
References
1. B. Ycart, Rev. d’Histoire des Math. 9, 43 (2013)
2. A. Edelman, N. Raj Rao, Acta Numer. 14, 233 (2005)
3. A. Zee, Quantum Field Theory in a Nutshell, 2nd edn. (Princeton Univ. Press, Princeton, 2010)

Chapter 8
Resolve(nt) the Semicircle
In this Chapter, we introduce the so called resolvent, a complex function from which
the spectral density1 can be calculated. The advantage of the resolvent approach is
that one has to solve an algebraic equation (like ax2 + bx + c = 0) instead of a
(singular) integral equation (like Pr

dx′ n⋆(x′)
x−x′ = x, see (5.15)). The disadvantage
is that you need to know a bit of complex analysis.
8.1
A Bit of Theory
We introduce the complex function G N(z), with z ∈C \ {xi}326283
G N(z) = 1
N Tr
1
z −H = 1
N
N

i=1
1
z −xi
,
(8.1)
where the notation
1
z−H means the matrix inverse of z1 −H, and 1 is the N × N
identity matrix.
If H is a random matrix, then G N(z) is a random complex function that has poles
at the locations xi of each eigenvalue.
The second ingredient we need is the Sokhotski-Plemelj formula
lim
ε→0+
1
y ± iε = Pr
1
y

∓iπδ(y),
(8.2)
which should be interpreted as the integral relation (for a real-valued test function
ϕ(x) such that the integrals make sense)
1Unless otherwise stated, we will no longer make a distinction between n⋆(x), ⟨n(x)⟩and ρ(x).
© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_8
57

58
8
Resolve(nt) the Semicircle
lim
ε→0+
 −ε
−∞
+
 ∞
ε
 ϕ(y)
y
dy

∓iπϕ(0) = lim
ε→0+
 ∞
−∞
ϕ(y)
y ± iεdy .
(8.3)
For a one-liner proof, see below (around (8.6)).
Question 8.1 What is the point of introducing this identity?
▶First, stare at (8.2) carefully. You see that, on the right hand side, the imaginary
part is just a delta function. So, this identity is (yet another) way of representing
a delta function, as the imaginary part of a rational function (the left hand side).
Knowing that the spectral density is deﬁned in terms of a delta function ρ(x) =
⟨(1/N) 	N
i=1 δ(x −xi)⟩, you should be spotting an interesting connection here.
More on this later.
8.2
Averaging
Imagine now to take the limit N →∞of ⟨G N(z)⟩, where we average over the
distribution of the matrix H. This average is called resolvent, or Green’s function,
or Stieltjes transform. It is natural to assume (and can be mathematically justiﬁed)
that:
• the sum in (8.1)
G N(z) = 1
N Tr
1
z −H = 1
N
N

i=1
1
z −xi
(8.4)
gets converted into an integral,
• the poles at xi merge into a continuous “cut” on the real line,
• we have to “weigh” the integrand with the average density of eigenvalues ρ(x) at
point x.
The cut on the real line is therefore nothing but the support of the spectral density,
and the average resolvent is deﬁned for all complex values z outside this cut (for
example, outside the interval [−
√
2,
√
2] on the real line for the Gaussian ensemble).
In formulae
⟨G N(z)⟩→G(av)
∞(z) =

dx′ ρ(x′)
z −x′ ,
for N →∞.
(8.5)
If you are inclined to believe that (8.5) is very plausible (to say the least), we can
now proceed smoothly.

8.2 Averaging
59
Compute now G(av)
∞(z) (the averaged resolvent in the large N limit) at z = x −iε.
Carrying out this herculean task, we get
G(av)
∞(x−iε) =

dx′
ρ(x′)
x −iε −x′ =

dx′ ρ(x′)(x −x′)
(x −x′)2 + ε2 +i

dx′ρ(x′)
ε
(x −x′)2 + ε2 ,
(8.6)
where we have multiplied up and down by x −x′ + iε and separated the real and
imaginary parts.
Sending now ε →0+, we are basically proving the Sokhotski-Plemelj formula:
the real part becomes a principal value integral (the so called Hilbert transform),
Pr

dx′ ρ(x′)
x−x′ , while the imaginary part (with the sign reversed with respect to the
argument of G(av)
∞, ± →∓) becomes πρ(x), using the following representation for
the delta function
δ(x) = 1
π lim
ε→0+
ε
x2 + ε2 .
(8.7)
In summary
ρ(x) = 1
π lim
ε→0+ Im G(av)
∞(x −iε) .
(8.8)
So, if you know (or can calculate) the resolvent in the complex plane, you can
from it deduce the spectral density.
All this in theory. Practice in the next section.
Question 8.2 Are there important properties of the resolvent G(av)
∞(z) in (8.5)
that are worth remembering?
▶First of all, if you send |z| →∞in (8.5), you get
G(av)
∞(z) =

dx′ ρ(x′)
z −x′ ≈1
z

dx′ρ(x′) = 1
z + .... ,
(8.9)
where we have used normalization of ρ(x). This asymptotic ∼1/z behavior
can be important in applications.
Next, expanding the denominator in (8.5) to all orders, we observe that the
resolvent is the generating function of moments μk = ⟨Tr(H k)⟩=

dxρ(x)xk
G(av)
∞(z) =

dx′ ρ(x′)
z −x′ = 1
z

dx′
ρ(x′)
1 −x′/z = 1
z
∞

k=0

dx′ρ(x′)
 x′
z
k
=
∞

k=0
μk
zk+1 ,
(8.10)
with μ0 = 1.

60
8
Resolve(nt) the Semicircle
8.3
Do It Yourself
We propose here a truly elementary derivation of the algebraic equation satisﬁed by
the resolvent for the Gaussian ensemble.
Consider the partition function of the standard Gaussian ensemble, after a further
rescaling xi →xi
√
N and ignoring prefactors
ZN,β ∝

R
N

j=1
dx j e−βN
2
	N
i=1 x2
i 
j<k
|x j −xk|β =

R
N

j=1
dx j e−βNV[x] ,
(8.11)
with
V[x] = 1
2

i
x2
i −1
2N

i̸= j
ln |xi −x j| .
(8.12)
Compared to our earlier Coulomb gas treatment, we have pulled out a factor
N (not N 2), so that the xi are now of O(1) for large N. Instead of introducing a
continuous counting function n(x) (as we did in Chap.4), we can directly perform
the saddle point evaluation of the N-fold integral (8.11), obtaining for each variable
xi the equation
∂V[x]
∂xi
= 0 ⇒xi = 1
N

j̸=i
1
xi −x j
.
(8.13)
Multiplying (8.13) by
1
N(z−xi) and summing over i, we get:
1
N

i
xi
z −xi
= 1
N

i

j̸=i
1
xi −x j
1
N(z −xi) .
(8.14)
Adding and subtracting z in the numerator, the left-hand-side L becomes
L = 1
N

i
xi −z + z
z −xi
= −1 + z 1
N

i
1
z −xi
= −1 + zG N(z) .
(8.15)
As for the right-hand-side, let us deﬁne R =
1
N 2
	N
i=1
	
j̸=i
1
z−xi
1
xi−x j . Writing
1
(z −xi)(xi −x j) =
1
z −x j

1
z −xi
+
1
xi −x j

,
(8.16)
one obtains the following self-consistency equation for R
R = G2
N(z) + 1
N G′
N(z) −R
⇒
R = 1
2G2
N(z) + 1
2N G′
N(z) .
(8.17)

8.3 Do It Yourself
61
Equating L to R, we obtain as promised that the saddle-point condition (8.13)
gets converted into an equation for the resolvent
−1 + zG N(z) = 1
2G2
N(z) + 1
2N G′
N(z) .
(8.18)
This is good, but is still a differential equation for G N(z), while we promised an
even simpler algebraic equation. It is actually easy to get rid of the differential term
in (8.18) by noticing that, with xi of O(1), the resolvent as deﬁned in (8.1) is itself
of O(1) and therefore the term
1
2N G′
N(z) is subleading for large N.
Taking the average, the surviving algebraic (at long last!) equation for N →∞
reads
G(av)2
∞
(z) −2zG(av)
∞(z) + 2 = 0 .
(8.19)
It is instructive to solve (8.19) directly as a quadratic equation (recall that quadratic
equations for complex variables admit the same solving formula as their real coun-
terparts), yielding
G(av)
∞(z) = z ±

z2 −2 .
(8.20)
Settingnow, z = x−iε,weobtain G(av)
∞(x−iε) = x−iε±

(x2 −ε2 −2) + i(−2xε).
The square root (with positive real part) of a complex number a + ib can be written
as [1] √a + ib = p + iq, with
p =
1
√
2

a2 + b2 + a
q = sign(b)
√
2

a2 + b2 −a ,
(8.21)
where sign(x) = 1 if x > 0 and = −1 if x < 0.
Hence we obtain (recalling (8.8))
1
π ImG(av)
∞(x −iε) = −ε
π ± sign(−2xε)
π
√
2

(x2 −ε2 −2)2 + 4x2ε2 −x2 + ε2 + 2
ε→0+
−→±sign(−x)
π
√
2

|x2 −2| −x2 + 2 .
(8.22)
From this expression, you see that i) for |x| >
√
2 you obtain that the density is
0, and ii) for |x| <
√
2, you need to select the (−) or (+) sign in front, according
to whether x > 0 or x < 0 respectively. After choosing the right sign, you get
ρ(x) = (1/π)
√
2 −x2 as expected.
To double-check this result, we can insert the semicircle ρSC(x) =
1
π
√
2 −x2
back into the deﬁnition (8.5) and perform the numerical integration for z = x −iε,
with ε a small positive number and separately for the two cases, 0 < x <
√
2 and
−
√
2 < x < 0. This is done with the code [♠check_resolvent.m], where the
results are compared with the two choices of sign in (8.20). You see that the (+)
choice in (8.20) only works with −
√
2 < x < 0, and the (−) choice in (8.20) only
works with 0 < x <
√
2.

62
8
Resolve(nt) the Semicircle
8.4
Localize the Resolvent
Let us now take a step back and “unpack” the deﬁnition of the resolvent in Eq.(8.1).
We wrote that deﬁnition as the trace of the matrix (z −H)−1, with z ∈C \ {xi},
xis being the eigenvalues of H. If we write the trace explicitly, i.e. as a sum over the
diagonal elements of the resolvent, we have G N(z) = (1/N) 	N
i=1 G N,ii(z), where
G N,ii(z) = [(z −H)−1]ii =
N

j=1
(c j
i )2
z −x j
,
(8.23)
and c j
i is the ith component of the normalized eigenvector associated with the jth
eigenvalue of H.
So, you may now ask, what should I make of these matrix elements? Well, it turns
out that they contain precious information about the localization properties of the
matrix ensemble they are associated with.
But, ﬁrst of all, what is localization?
Simply put, the term localization refers to how “spread out” over their components
the eigenvectors of a matrix are. Let us deﬁne the inverse participation ratio (IPR)
of a normalized eigenvector as
IN, j =
N

i=1
(c j
i )4 .
(8.24)
Now, when the eigenvector’s components are all roughly of the same magnitude,
then we must have c j
i ≃1/
√
N, ∀i, due to normalization. Hence, we will have
IN, j ≃1/N, and the IPR will vanish in the large N limit.
If, on the other hand, the eigenvector is signiﬁcantly different from zero only on
a number s of sites, then for those sites we will have c j
i ≃1/√s, and the IPR will
remain roughly equal to 1/s in the large N limit. So, all in all, the IPR is a handy
tool that tells us whether certain eigenvectors of a matrix are extended (i.e. have an
extensive number of non zero components) or instead localized on a ﬁnite number
of sites.
Although this may sound like a mathematical curiosity, the localization properties
of matrix ensembles are related to a number of relevant features of the physical
systems they describe. In particular, it is often crucial to detect the so called mobility
edge, i.e. the critical eigenvalue that separates the part of the spectrum associated
with extended states from the one associated with localized states. For example, it
has famously been shown that the mobility edge determines the Anderson transition
in electronic systems [2].

8.4 Localize the Resolvent
63
All in all, it should be now clear that having analytical access to the distributional
properties of the IPRs corresponding to different segments of a given ensemble’s
eigenvalue spectrum is a valuable thing. Luckily, this is where our diagonal elements
(8.23) come to the rescue. Indeed, it has been shown in [3] that the average value
P(x) of IPRs associated with states whose corresponding eigenvalues lie between x
and x + dx can be written in the large N limit as
P(x) = lim
ε→0+ lim
N→∞
ε
π Nρ(x)
N

i=1

|GN,ii(x −iε)|2
= lim
ε→0+
ε
πρ(x)|G(av)
∞(x −iε)|2 .
(8.25)
8.5
To Know More...
1. The saddle-point evaluation (8.13) based on the partition function (8.11) is
clearly valid when the neglected terms in the exponent are indeed subleading
(O(N)). There are models—rotationally invariant by construction—where the
Dyson index β is allowed to scale with N [4, 5]. These models provide explicit
realizations of invariant β-ensembles, for which the resolvent equation is neces-
sarily more involved. Ref. [5] is also suggested for an elementary derivation of
this “improved” resolvent equation in the presence of a hard wall in the spectrum.
2. Matrix models such as the Gaussian can be constructed introducing a ﬁctitious
time evolution (stochastic) of the entries. In this case, it is possible to show that
the resolvent satisﬁes a partial differential equation of the Burgers type (see the
beautiful paper [6]).
3. The equation for the resolvent can be given a pretty interpretation in terms of
planar diagrams. Diagrammatic methods are at the heart of many beautiful results
in RMT (see [7, 8]).
References
1. S. Rabinowitz, Math. Inf. Q. 3, 54–56 (1993)
2. R. Abou-Chacra, P.W. Anderson, D.J. Thouless, J. Phys. C 6, 1734 (1973)
3. F.L. Metz, I. Neri, D. Bollé, Phys. Rev. E 82, 031135 (2010)
4. R. Allez, J.-P. Bouchaud, A. Guionnet, Phys. Rev. Lett. 109, 094102 (2012)
5. R. Allez, J.-P. Bouchaud, S.N. Majumdar, P. Vivo, J. Phys. A: Math. Theor. 46, 015001 (2013)
6. J.-P. Blaizot, M.A. Nowak, Phys. Rev. E 82, 051115 (2010)
7. J. Jurkiewicz, G. Łukaszewski, M.A. Nowak, Acta Phys. Pol. B 39, 799 (2008)
8. J. Bouttier, Matrix integrals and enumeration of maps, in The Oxford Handbook of Random
Matrix Theory, ed. by G. Akemann, J. Baik, P. Di Francesco (Oxford Univ. Press, Oxford, 2011)

Chapter 9
One Pager on Eigenvectors
Take the GUE ensemble of N × N hermitian matrices. Any given matrix in the
ensemble will have unit-norm eigenvectors having in general complex components.
What is the statistics of such components?
Since eigenvalues and eigenvectors of invariant matrix models are decoupled, the
only constraint on the N components of an eigenvector is that its norm must be one,
therefore their jpdf reads
PGU E(c) = CNδ

1 −
N

n=1
|cn|2

,
(9.1)
where CN is a normalization constant.
It is convenient to compute the marginal distribution of a single component, say
|c1|2, given by
PGU E(y) =

d2c1 · · · d2cNδ(y −|c1|2)PGU E(c) .
(9.2)
Similarly, we can compute the jpdf of eigenvector components (this time all real
numbers) of a GOE matrix.
The calculation in (9.2) is carried out by ﬁrst deﬁning an auxiliary object
PGU E(y; t) =

d2c1 · · · d2cNδ(y −|c1|2)CNδ

t −
N

n=1
|cn|2

,
(9.3)
such that PGU E(y) = PGU E(y; 1). Then, taking the Laplace transform with respect
to t to kill the delta function in (9.3)
 ∞
0
dt e−st PGU E(y; t) = CN

d2c1δ(y −|c1|2)e−s|c1|2 
d2c e−s|c|2N−1
,
(9.4)
and ﬁnally converting the 2d integrals in polar coordinates
© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_9
65

66
9
One Pager on Eigenvectors
 ∞
0
dt e−st PGU E(y; t) = ˆCN
 ∞
0
dr rδ(y −r2)e−sr2  ∞
0
dρ ρ e−sρ2N−1
∝e−sy
sN−1 ,
(9.5)
where we have absorbed the angular constants in the overall normalization.
Inverting the Laplace transform, we obtain
PGU E(y; t) ∝(t −y)N−2θ(t −y) ,
(9.6)
where θ(z) is the Heaviside step function. Setting t = 1 and normalizing, we obtain
PGU E(y) = (N −1)(1 −y)N−2
for 0 ≤y ≤1 .
(9.7)
Similarly, for the GOE one obtains
PGOE(y) =
1
√π
(N/2)
((N −1)/2)
(1 −y)(N−3)/2
√y
for 0 ≤y ≤1 .
(9.8)
Computing the average ⟨y⟩in both cases
⟨y⟩GOE =
 1
0
dy yPGOE(y) =
(N/2)
2(1 + N/2) ∼1
N
⟨y⟩GU E =
 1
0
dy yPGU E(y) = 1
N ,
(9.9)
leads us to consider the scaled variable η = yN and take the limit N →∞. This
produces the scaled densities
PGOE(η) = lim
N→∞
1
N PGOE
 η
N
	
=
1
√2πηe−η/2 ,
(9.10)
PGU E(η) = lim
N→∞
1
N PGU E
 η
N
	
= e−η .
(9.11)
The ﬁrst of these densities is called the Porter-Thomas distribution [1, 2]. Note
also that the Gaussian nature of the matrix ensembles has not been used anywhere in
the derivation (the same densities would be obtained for any orthogonal or unitary
ensemble). The study of eigenvectors of random matrices has been recently revived
due to their importance in quantum system (see, e. g., [3–6]).
References
1. C.E. Porter, R.G. Thomas, Phys. Rev. 104, 483 (1956)
2. T.A. Brody, J. Floris, J.B. French, P.A. Mello, A. Pandey, S.S.M. Wong, Rev. Mod. Phys. 53,
385 (1981)
3. J.T. Chalker, B. Mehlig, Phys. Rev. Lett. 81, 3367 (1998)
4. B. Mehlig, J.T. Chalker, J. Math. Phys. 41, 3233 (2000)
5. Y.V. Fyodorov, On statistics of bi-orthogonal eigenvectors in real and complex Ginibre ensem-
bles: Combining partial Schur decomposition with supersymmetry (2017), http://arxiv.org/abs/
1710.04699
6. K. Truong, A. Ossipov, Europhys. Lett. 116, 37002 (2016)

Chapter 10
Finite N
Look back at Chap.1, where we constructed Gaussian matrices and histogrammed
their eigenvalues. For N →∞, we showed in various ways that the average spectral
density converges to the semicircle law. But what happens for ﬁnite N? Can we
compute analytically the shape of the histogram for, say, a 13×13 Gaussian matrix?
The answer is Yes—and not only for Gaussian matrices, but for any rotationally
invariant ensemble! This is done here. We start from the case β = 2, as it is much
easier.
10.1
β = 2 is Easier
Already in Chap.7, we mentioned that the Vandermonde determinant has some funny
properties: in particular, each row in the Vandermonde matrix can be replaced by
a polynomial of suitable degree, with many a priori unspeciﬁed coefﬁcients. The
freedom in choosing these polynomials is enormous. A judicious choice is the key
of the celebrated orthogonal polynomial technique.
Take the jpdf of the N real eigenvalues of a rotationally invariant ensemble with
β = 2
ρ(x1, . . . , xN) = 1
ZN
N

i=1
e−V (xi)|ΔN(x)|2 ,
(10.1)
which is written in the ‘potential’ form (see Eq.(5.31)). For example, for the Gaussian
ensemble V (x) = x2/2.
© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_10
67

68
10
Finite N
What is the goal then? To compute the average spectral density for ﬁnite N, i.e.
the N −1-fold integral
ρ(x1) =

dx2 · · · dxN ρ(x1, x2, . . . , xN ) =
1
ZN

dx2 · · · dxN
N

i=1
e−V (xi )|ΔN (x)|2 , (10.2)
where the partition function is ZN =

dx1 · · · dxN
N
i=1 e−V (xi)|ΔN(x)|2.
Note that in (10.2) we are integrating over all variables but one. These integrals
are nasty, though! The integrand does not factorize at all, so we need to ﬁnd some
smart trick to carry out the integration. It took a while even to the pioneers of these
calculations (for instance, Gaudin and Mehta) to ﬁgure out how to proceed. The steps
are as follows:
Step 1:
Rewrite the Vandermonde ΔN(x) as a determinant of the matrix A, whose entries
are polynomials πk(x) (to be determined), as in (7.3)
ΔN(x) =
1
a0a1 · · · aN−1
det
⎛
⎜⎜⎜⎜⎜⎜⎝
π0(x1)
. . .
π0(xN)
π1(x1)
. . .
π1(xN)
.
.
.
.
.
.
.
.
.
πN−1(x1) . . . πN−1(xN)
⎞
⎟⎟⎟⎟⎟⎟⎠
.
(10.3)
Step 2:
Use the general relation1
(det A)2 = det(AT A) = det
⎛
⎝
N

j=1
A ji A jk
⎞
⎠,
(10.4)
applied to the matrix A from step 1, to write
Δ2
N(x) =
1
(N−1
j=0 a j)2 det
⎛
⎝
N

j=1
π j−1(xi)π j−1(xk)
⎞
⎠.
(10.5)
Step 3:
Pull the weight exp(−
i V (xi)) inside the determinant2 and shift the index j →
j −1, to write eventually
1Hereafter, inside a determinant the indices of the entries will run from 1 to N.
2Use

ℓαℓ

det( f (i, j)) = det(√αiα j f (i, j)).

10.1 β = 2 is Easier
69
ρ(x1, . . . , xN) =
1
ZN(N−1
j=0 a j)2 det
⎛
⎝
N−1

j=0
φ j(xi)φ j(xk)
⎞
⎠= det (KN(xi, xk))
ZN(N−1
j=0 a j)2 ,
(10.6)
where φi(x) = e−V (x)/2πi(x) and
KN(x, x′) = e−1
2 (V (x)+V (x′))
N−1

j=0
π j(x)π j(x′) ,
(10.7)
which is a central object in RMT: the kernel.
Step 4:
Choose judiciously the (so far undetermined) polynomials π j(x). A great choice is
to pick them orthonormal with respect to the weight3 exp(−V (x))

e−V (x)πi(x)π j(x)dx = δi j .
(10.8)
For instance, for the Gaussian (unitary) ensemble (V (x) = x2/2) the correspond-
ing orthonormal polynomials are
π j(x) = Hj(x/
√
2)
√
2π 2 j j!
,
(10.9)
if Hj(x) are Hermite polynomials satisfying
 ∞
−∞dx Hj(x)Hk(x) exp(−x2) =
√π2 j j!δ jk.
Question 10.1 What is the advantage of choosing polynomials with this
“orthonormality” property?
▶Well, the reason is that the kernel KN(x, x′) in (10.7), if the polynomials are
chosen this way, satisﬁes a quite amazing “reproducing” property

dyKN(x, y)KN(y, x′) = KN(x, x′) .
(10.10)
The proof is very simple: just insert (10.7) into (10.10) and use the orthonor-
mality relation (10.8). This property has a quite unexpected consequence, which
eventually allows to carry out the multiple integrations in (10.2) in a very ele-
gant, iterative way. Another ingredient is necessary, though, and is presented in
the next Section.
3Note that there is a factor (1/2) multiplying V (x) in the kernel (10.7), while there is none in the
weight function of the orthonormal polynomials in (10.8).

70
10
Finite N
10.2
Integrating Inwards
Summarizing, we have to carry out the multiple integration in (10.2) over a jpdf,
which can be written as the determinant of a kernel (see (10.6)), something like

dx2 · · · dxN det(KN(x j, xk)) =?
(10.11)
In normal situations, this would seem a rather hopeless task. But the reproducing
property of the kernel offers an unexpected way around.
First, an illuminating 2×2 example, and then the full-ﬂedged (though dry) theory.
Imagine the following 2 × 2 matrix J2(x), depending on the vector x = {x1, x2}
through a function f (x, y) as follows
J2(x) =
 f (x1, x1) f (x1, x2)
f (x2, x1) f (x2, x2)

.
(10.12)
Suppose now that the function f satisﬁes the “reproducing” property (10.10),
namely

f (x, y) f (y, z) dμ(y) = f (x, z) for a certain measure μ(y). What hap-
pens to the following integral

dμ(x2) det(J2(x))?
(10.13)
Well, we have

dμ(x2) det(J2(x)) =

dμ(x2) [ f (x1, x1) f (x2, x2) −f (x1, x2) f (x2, x1)] =
= q f (x1, x1) −f (x1, x1) = (q −1) f (x1, x1) ,
(10.14)
where q =

dμ(x2) f (x2, x2). We used the reproducing property to evaluate the
second integral.
Maybe this short calculation is not particularly revealing, but it can be actually
extended to the N × N case as follows: let JN(x) be an N × N matrix whose
entries depend on a real vector x = (x1, x2, . . . , xN) and have the form Ji j =
f (xi, x j), where f is some function satisfying the “reproducing kernel” property

f (x, y) f (y, z) dμ(y) = f (x, z) , for some measure dμ(y). Then the following
holds:

det[JN(x)] dμ(xN) = [q −(N −1)] det(JN−1(˜x)) ,
(10.15)
where q =

f (x, x) dμ(x), and the matrix JN−1 has the same functional form as
JN with x replaced by ˜x = (x1, x2, . . . , xN−1). A friendly proof can be found in [1].

10.2 Integrating Inwards
71
This is a quite spectacular result, which is commonly referred to as Dyson-Gaudin
integration lemma.4 First of all, note that the 2 × 2 result (10.14) is in agreement
with the general statement. Second, comparing (10.11) and (10.15), we see that this
lemma actually allows to integrate det(KN(x j, xk)) (essentially, the jpdf) over the
last variable xN, producing as a result a determinant of a smaller kernel matrix

det

KN(xi, x j)

1≤i, j≤N dxN = det

KN(xi, x j)

1≤i, j≤N−1 ,
(10.16)
where we have used q =

dxKN(x, x) = N (immediate from the deﬁnition of the
kernel (10.7)). Basically, the reproducing property carries over from the kernel to
the determinant of the kernel!
Therefore, we can iterate the process N −k times, killing one integral at a time
and reducing the dimension of the determinant by one, with a remarkable domino
effect

. . .

det

KN (xi, x j)

1≤i, j≤N dxk+1 · · · dxN = (N −k)! det

KN (xi, x j)

1≤i, j≤k .
(10.17)
In particular, setting k = 0 we can normalize the jpdf (10.1) as
1 =

dxρ(x) =
1
ZN (N−1
j=0 a j)2

dx det (KN (xi, xk)) ⇒ZN
⎛
⎝
N−1

j=0
a j
⎞
⎠
2
= N!,
(10.18)
so that the two-point marginal ρ(x1, x2)
ρ(x1, x2) =

N

j=3
dx jρ(x1, . . . , xN ) =
1
N(N −1) det
 KN (x1, x1) KN (x1, x2)
KN (x2, x1) KN (x2, x2)

,
(10.19)
(where one uses (N −2)!/N! = 1/[N(N −1)]), while the one-point marginal (the
average spectral density ) is simply
ρ(x1) =

dx2 · · · dxNρ(x1, . . . , xN) = 1
N KN(x1, x1) .
(10.20)
And the problem is solved not just for the one-point marginal, but for any k-
point correlation function—once the kernel is built out of suitable polynomials,
orthonormal with respect to the weight V (x). The fact that all such functions can be
4The most accurate reference seems however to be [3].

72
10
Finite N
expressed in terms of determinants is usually referred to as determinantal structure
of the unitarily invariant ensembles.
For modern extensions of the “integrate-out” lemma and applications, have a look
at [2, 4].
10.3
Do It Yourself
Let us apply the general formalism to the GUE case, for which the orthonormal
polynomials are π j(x) = Hj(x/
√
2)/
√
2π2 j j!, where Hj(x) are Hermite poly-
nomials. Then, we obtain immediately the spectral density at ﬁnite N as
ρ(x) =
1
N
√
2π
e−x2/2
N−1

j=0
H 2
j (x/
√
2)
2 j j!
.
(10.21)
In Fig.12.1 of Chap.12 we show a comparison between a numerically generated
histogram of GUE eigenvalues, and the corresponding theoretical result in (10.21).
Question 10.2 If I send N →∞in (10.21), shouldn’t I recover the semicircle?
I do not see how.
▶Yes, you should, and you will! The precise statement is
lim
N→∞
√
2Nρ(z
√
2N) = 1
π

2 −z2 ,
for −
√
2 < z <
√
2 ,
(10.22)
which requires a bit of work on the asymptotics of Hermite polynomials. We
will give a ﬂavor of the steps you need just below.
10.4
Recovering the Semicircle
First, one injects the so called Christoffel-Darboux formula [5] into the game, a
quite spectacular relation that hugely simpliﬁes sums of orthogonal polynomials.
Specialized to the Hermite polynomials, it reads
n

k=0
Hk(x)Hk(y)
k!2k
=
1
n!2n+1
Hn(y)Hn+1(x) −Hn(x)Hn+1(y)
x −y
.
(10.23)

10.4 Recovering the Semicircle
73
With an eye towards (10.21), with a few manipulations and taking the limit x →y,
we obtain the relation
N−1

k=0
π2
k (x) =
√
N

πN−1(x)π′
N(x) −πN(x)π′
N−1(x)

,
(10.24)
where the orthonormal polynomials with respect to the Gaussian weight were deﬁned
in (10.9).
After huge simpliﬁcations, the GUE spectral density for ﬁnite N—suitably
rescaled—can be rewritten in the form
√
2Nρ(z
√
2N) =
2e−Nz2
√
π N2N 
(N)

N H2
N−1(z
√
N) −(N −1)HN (z
√
N)HN−2(z
√
N)

.
(10.25)
We should now analyze (10.25) in the limit N →∞for z ∼O(1). To do so, we
need to use the following asymptotic formula for Hermite polynomials in the bulk5
HN+m(X
√
2N) =
 2
π
1/4 2m/2+N/2Nm/2−1/4(N!)1/2eN X2
(1 −X2)1/4
gm,N (X)

1 + O
 1
N

,
(10.26)
valid for −1 < X < 1, m ∼O(1) and gm,N(x) given by the following expression
gm,N(x) = cos

Nx

1 −x2 + (N + 1/2) arcsin(x) −Nπ/2 −m arccos(x)

.
(10.27)
We can now apply this asymptotic expansion to (10.25), with m = 0, −1, −2 as
needed, after the identiﬁcation X = z/
√
2.
The two terms H 2
N−1 and HN × HN−2 produce the same N-dependent prefactor
(2/π)1/22N−1N −3/2(N!)eNz2
(1−z2/2)1/2
(check it!), and after simpliﬁcations we get to
√
2Nρ(z
√
2N) ∼
2
Nπ
√
2 −z2

N cos2(α + φ) −(N −1) cos(α) cos(α + 2φ)

,
(10.28)
where the cosine terms come from gm,N(x) in (10.27). Here
5What does in the bulk mean? The point is that Hermite polynomials (and other classical orthogonal
polynomials) have two different asymptotics, according to the way their argument and parameter
scale with N. This in turns corresponds to different regimes, namely different locations x where the
spectrum is looked at, and different zooming resolutions.

74
10
Finite N
α = N(z/
√
2)

1 −z2/2 + (N + 1/2) arcsin(z/
√
2) −Nπ/2 ,
(10.29)
φ = −arccos(z/
√
2) .
(10.30)
Keeping only the leading ∝N terms in the square bracket, and using the identity
cos2(α + φ) −cos(α) cos(α + 2φ) = sin2(φ), we ﬁnally get
√
2Nρ(z
√
2N) ∼
2
π
√
2 −z2 sin2(−arccos(z/
√
2)) = 1
π

2 −z2 ,
(10.31)
as expected.
References
1. Y.V. Fyodorov, Introduction to the Random Matrix Theory: Gaussian Unitary Ensemble and
Beyond (2004), https://arxiv.org/pdf/math-ph/0412017.pdf
2. E. Kanzieper, G. Akemann, Phys. Rev. Lett. 95, 230201 (2005); G. Akemann, E. Kanzieper, J.
Stat. Phys. 129, 1159 (2007)
3. G. Mahoux, M.L. Mehta, J. Phys. I France 1, 1093 (1991)
4. G. Akemann, L. Shifrin, J. Phys. A : Math. Gen. 40, F785 (2007)
5. W. Van Assche, Asymptotics for orthogonal polynomials (Springer, 2006)

Chapter 11
Meet Andréief
In this Chapter, we present a couple of very useful integral identities involving the
Vandermonde determinant, and one cute application.
11.1
Some Integrals Involving Determinants
We start with the Andréief identity—also called sometimes the Gram or Heine
identity [1]. It states that a certain multiple integral involving the product of two
determinants can be written as the determinant of a matrix whose entries are single
integrals.
Confused? Let’s have a closer look.
We are given two sets of N functions, { fk(x)} and {gk(x)}. We also have an
integration measure μ(x). We then have

N

j=1
dμ(x j) det( f j(xk)) det(g j(xk)) = N! det

dμ(x) f j(x)gk(x)

.
(11.1)
This can be proved by just expanding the left hand side as a double sum over
permutations, performing the integrals and then folding the result back into a single
sum. Try to prove it yourself—for example, right now.
If you think about it for a second, this identity seems too good to be true. On the
left hand side, you have, say, a 20-fold integral of a truly nasty object, and on the
right hand side a 20 × 20 determinant, which can be easily handled by any scientiﬁc
software—when not explicitly computable in closed form!
© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_11
75

76
11
Meet Andréief
This identity is especially useful for unitary invariant ensembles (β = 2), because
there you can write the square of the Vandermonde determinant as 
j<k(x j −xk)2 =
det(xk−1
j
) det(xk−1
j
). For example, the partition function of the GUE can be written
as a determinant
ZN,β =

(−∞,∞)N
N

j=1
dx je−1
2 x2
j 
j<k
(x j −xk)2 = N! det
 ∞
−∞
dx e−1
2 x2x j+k−2

= N! det

2
1
2 ( j+k−3) 
(−1) j+k + 1


1
2( j + k −1)

,
(11.2)
which can also be evaluated in closed form as a Selberg-like integral [2].
A nice feature of the ﬁnal determinant—and this happens for all β = 2
calculations—is that it is of the form det(Mi+ j), i.e. it is a Hankel determinant
(the matrix M is constant along the skew-diagonals). This happens because the two
determinants in the integrand on the left hand side are equal, and this produces a
factor (x j−1)(xk−1) on the right hand side.
There are two other identities that are similar in spirit to the Andréief identity (the
de Brujin identities [3]). They read as follows:

x1≤x2≤...≤xN
dμ(x) det[φi(x j)] = Pf
	
sign(x −y)φi(x)φ j(y)dμ(x)dμ(y)

,
(11.3)
where i and j run from 1 to N, and

dμ(x) det

φi(x j) ψi(x j)

= (2N)!Pf
	
dμ(x)(φi(x)ψ j(x) −φ j(x)ψi(x))

,
(11.4)
where i and j run from 1 to 2N.
In both the equations above, Pf denotes a Pfafﬁan. Just like the determinant can
be written as a sum over permutations, a Pfafﬁan is written as a sum over pairings.
Given a set S with an even number of elements, {1, ..., 2n}, a pairing of S is a
collection of n pairs of elements from S. For instance, the set {1, 2, 3, 4} has three
possible pairings: {{1, 2}, {3, 4}}, {{1, 3}, {2, 4}}, and {{1, 4}, {2, 3}}.
We can realize pairings as permutations acting of the trivial pairing {{1, 2}, {3, 4}}.
The previous pairings then correspond to the identity permutation, the transposition
(23) and the cycle (243).
In terms of these permutations, we have
Pf(A) =

P
s(P)
n
j=1
AP(2 j−1),P(2 j) ,
(11.5)
where s(P) is the signature of the permutation and A is a even-dimensional skew-
symmetric matrix.

11.1 Some Integrals Involving Determinants
77
For example, take n = 2 and A the following 4 × 4 matrix
A =
⎛
⎜⎜⎝
0
A12
A13
A14
−A12
0
A23
A24
−A13 −A23
0
A34
−A14 −A24 −A34
0
⎞
⎟⎟⎠.
(11.6)
We have Pf(A) = A12 A34 −A13 A24 + A14 A23 (compare with the pairings listed
above for the set {1, 2, 3, 4}). Note also that Pf(A) = √det(A).
We will encounter Pfafﬁans again in Chap.12.
11.2
Do It Yourself
Let us see a simple example where the Andréief formula turns a nasty problem into
a doable one.
Question: what is the probability that a 9 × 9 GUE matrix has N+ = 7 positive
eigenvalues? From ﬁrst principles, we have in general
PN(N+ = n) =

dx1 · · · dxNρ(x1, . . . , xN)δ

n −
N

i=1
θ(xi)

,
(11.7)
where θ(x) is the Heaviside step function, =1 if x > 0 and 0 otherwise.
Note that the delta function in (11.7) is more correctly a Kronecker delta
δn,N
i=1 θ(xi). We can introduce the generating function
ϕN (z) =
N

n=0
PN (N+ = n)zn =
1
ZN,β=2
 ∞
−∞
N

j=1
dx je−1
2
N
i=1 x2
i +(ln z) N
i=1 θ(xi ) 
j<k
(x j −xk)2 .
(11.8)
This multiple integral seems hopelessly complicated. But spotting that

j<k(x j −xk)2 = det(x j−1
i
) det(x j−1
i
), we can use the Andréief formula to write
ϕN(z) =
det
 ∞
−∞dx e−1
2 x2+(ln z)θ(x)xi+ j−2
det
 ∞
−∞dx e−1
2 x2xi+ j−2

= det

(−1)i+ j + z

ci+ j

det

(−1)i+ j + 1

ci+ j
 ,
(11.9)
where ck = 2
k−3
2 
 k−1
2

. We have used Andréief also to express ZN,β=2 as a deter-
minant, and erased a common N! factor.
Evaluating the integrals, we got a ratio of Hankel determinants, which can easily
be evaluated exactly with a symbolic software.

78
11
Meet Andréief
Note that ϕN(1) = 1, as it should by normalization of PN(N+ = n) (see (11.8)).
The probabilities PN(N+ = n) can then be reconstructed by differentiation
PN(N+ = n) = 1
n!∂(n)
z ϕN(z)

z→0 .
(11.10)
Carrying out this program, we may ﬁnd that for a 9 × 9 GUE matrix,
PN=9(N+ = 7) = 161229045760 −20942589825π2 −9172989000π3 + 3386880000π4
48168960000π4
≃5.67686 × 10−6 .
(11.11)
Note that the evaluation is exact, and can be extended to many other values of
n, N. Of course, it would be desirable to have an exact and explicit formula for these
probabilities at arbitrary n, N – see [4].
For a numerical check of (11.10), see [♠Andreief_check.m].
11.3
To Know More...
1. There is an interesting connection between Hankel determinants, the so-called
Toda equation on a semi-inﬁnite lattice, and Painlevé functions. Deﬁne
τn = det(ai+ j−2)i, j=1,...,n ,
(11.12)
with “initial conditions” τ−1 = 0, τ0 = 1 and τ1 = a0. Imagine that the entries
ak of this Hankel matrix are functions of x (and so is τn, for any ﬁxed n). If the
ak satisfy the following relation, ak = a′
k−1 (where ′ denotes differentiation with
respect to x), then the following hierarchy of equations holds
τ ′′
n τn −(τ ′
n)2 = τn+1τn−1 .
(11.13)
In [♠Toda.m] we test this property for n = 3.
Quite amazingly, the same Toda lattice equation is obeyed by so-called τ-
functions, which arise in the Hamiltonian formulation of the six Painlevé equa-
tions (PI-PVI), other fundamental objects in the theory of nonlinear integrable
systems [5].
These deep connections between Andréief evaluations, Hankel determinants,
Toda lattice and Painlevé functions are at the root of quite spectacular results
(see e.g. [6, 7]).

References
79
References
1. C. Andréief, Mem. de la Soc. Sci. de Bordeaux 2, 1 (1883)
2. P.J. Forrester, S.O. Warnaar, Bull. Amer. Math. Soc. (N.S.) 45, 489 (2008)
3. N.G. de Bruijn, J. Indian Math. Soc. 19, 133 (1955)
4. N.S. Witte, P.J. Forrester, Random Matrices: Theory Appl. 01, 1250010 (2012)
5. P.J. Forrester, N.S. Witte, Commun. Math. Phys. 219, 357 (2001)
6. E. Kanzieper, Phys. Rev. Lett. 89, 250201 (2002)
7. E. Kanzieper, Constr. Approx. 41, 615 (2015)

Chapter 12
Finite N Is Not Finished
In this short Chapter, we compute in the quickest way the spectral density for the
GOE (β = 1) and GSE (β = 4). The symmetry classes beyond the Unitary have a
reputation for being “unfriendly”. We do not aim at giving the most general treatment
of correlation functions for such cases. The goal of this Chapter is just to provide a
smooth and gentle appetizer, allowing you to tackle the nastier bits with your back
covered.
12.1
β = 1
Let us assume N is even for simplicity. Indices i, j run from 1 to N, while k, ℓrun
from 0 to N −1.
Suppose the jpdf of eigenvalues is given by
ρ(x1, . . . , xN) = 1
Z|ΔN(x)|
N

i=1
w(xi) .
(12.1)
For w(x) = exp(−x2/2), we recover the jpdf for the GOE.
Let’s compute the normalization factor, a.k.a. the partition function,
Z =

dx|ΔN(x)|
N

i=1
w(xi) = |ˆaN|

dx| det(R j−1(xi)w(xi))| ,
(12.2)
where Rk(x) = akxk + · · · is a family of polynomials through which the Vander-
monde materializes, and
ˆaN =
N−1

k=0
ak
−1
.
(12.3)
© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_12
81

82
12
Finite N Is Not Finished
To get rid of the absolute value, we restrict integration to the domain where the
variables are ordered:
Z = N!|ˆaN|

−∞<x1<x2<···<∞
dx det(R j−1(xi)w(xi)) .
(12.4)
We may now use the de Brujin identity (11.3) to get
Z = N!|ˆaN|Pf(Ai, j) ,
(12.5)
where
Ai, j =
 ∞
−∞
dx
 ∞
−∞
dyRi−1(y)R j−1(x)w(x)w(y)sign(x −y) ,
(12.6)
and Pf denotes the Pfafﬁan of the skew-symmetric matrix Ai j.
Let us now stop for a second to check on a 2×2 example that, indeed, the expres-
sions in (12.4) and (12.5) coincide. Starting from the integral in (12.4), specialized
to a 2 × 2 case, we have
 +∞
−∞
dy
 y
−∞
dx R0(x)R1(y)w(x)w(y) −
 +∞
−∞
dy
 y
−∞
dx R0(y)R1(x)w(x)w(y)
=
 +∞
−∞
dx
 x
−∞
dyR0(y)R1(x)w(x)w(y) −
 +∞
−∞
dy
 y
−∞
dx R0(y)R1(x)w(x)w(y) ,
(12.7)
where we have simply renamed the variables x →y and y →x in the ﬁrst integrals.
If we now expand the Pfafﬁan in Eq.(12.5) we obtain
Pf(Ai, j) =
 +∞
−∞
dy
 +∞
y
dx R0(y)R1(x)w(x)w(y) −
 +∞
−∞
dy
 y
−∞
dx R0(y)R1(x)w(x)w(y)
=
 +∞
−∞
dx
 x
−∞
dyR0(y)R1(x)w(x)w(y) −
 +∞
−∞
dy
 y
−∞
dx R0(y)R1(x)w(x)w(y) ,
(12.8)
where in the ﬁrst integral we have simply rewritten the integration domain −∞<
y < x < ∞. The above expression coincides with (12.7).
Now, stare at (12.6) for a few seconds. To simplify the notation slightly, we may
deﬁne the following skew-symmetric inner product
⟨f, g⟩1 = 1
2
 ∞
−∞
dx
 ∞
−∞
f (y)g(x)w(x)w(y)sign(x −y)dy ,
(12.9)
so that we can write Ai, j
= 2⟨Ri−1, R j−1⟩1. Note that in general ⟨f, g⟩1 =
−⟨g, f ⟩1—we wouldn’t call it skew-symmetric otherwise, would we?

12.1 β = 1
83
Now, in complete analogy with what we did for β = 2—identifying speciﬁc
polynomials, orthonormal with respect to the given weight—we may choose the
polynomials R that “behave nicely” with respect to this inner product. The nice
properties we require are: evens and odds are orthogonal among themselves,
⟨R2k, R2ℓ⟩1 = ⟨R2k+1, R2ℓ+1⟩1 = 0 ,
(12.10)
and evens are orthogonal to odds unless they are adjacent,
⟨R2k, R2ℓ+1⟩1 = −⟨R2ℓ+1, R2k⟩1 = δkℓ.
(12.11)
With this particular choice, the R’s are called skew-orthogonal polynomials. The
matrix A in (12.6) acquires a simple form,
A =
⎛
⎜⎜⎜⎜⎜⎝
0
2
−2 0
0
2
−2 0
...
⎞
⎟⎟⎟⎟⎟⎠
,
(12.12)
and the expression for Z drastically simpliﬁes: the determinant of A becomes simply
2N, hence its Pfafﬁan becomes 2N/2, and all the information about the speciﬁc weight
function w(x) is contained in ˆaN. As a consequence, from (12.5) we get for the
partition function in (12.2) Z = N!|ˆaN|2N/2.
Let us now generalize this calculation slightly. Consider the quantity
Z[ f ] =|ˆaN|

dx| det(R j−1(xi)w(xi) f (xi))|
=Z[ f = 1]

dx ρ(x1, . . . , xN)
N

i=1
f (xi) ,
(12.13)
where we introduced an arbitrary function f (x) in the game, such that the integral
is convergent. Note that Z[ f = 1] coincides with Z.
From this new partition function, we can recover the density of eigenvalues for
ﬁnite N
ρ(x) =

dx2dx3 · · · dxNρ(x, x2, . . . , xN)
(12.14)
by means of a functional derivative. This is the operator
δ
δf , which satisﬁes all the
properties of a derivative, plus the condition
δ
δf (x) f (y) = δ(y −x) .
(12.15)

84
12
Finite N Is Not Finished
Then,
δ
δf (x)Z[ f ]

f =1
= Z[ f = 1]

dxρ(x1, . . . , xN)
N

i=1
δ(xi−x) = N Z[ f = 1]ρ(x).
(12.16)
Following a calculation perfectly analogous to the previous one, we arrive at
Z[ f ] = N!|ˆaN|Pf(Ai j[ f ]) ,
(12.17)
where
Ai j[ f ] =
 ∞
−∞
dx
 ∞
−∞
dyRi−1(y)R j−1(x)w(x)w(y) f (x) f (y)sign(x −y) .
(12.18)
Computing the functional derivative, and recalling the deﬁnition of Pfafﬁan in
Eq.(11.5), we have
δ
δf (x)Z[ f ]

f =1
= N!|ˆaN|

P
s(P)

δ
δf (x)
N/2

k=1
AP(2k−1),P(2k)[ f ]

f =1
. (12.19)
When we apply the product rule for the derivative, and set f = 1, for each term
in the sum over permutations we get

δ
δf (x) AP(1),P(2)[ f ]

f =1
AP(3),P(4)[1] · · · AP(N−1),P(N)[1] + . . .
+ AP(1),P(2)[1]

δ
δf (x) AP(3),P(4)[ f ]

f =1
· · · AP(N−1),P(N)[1] + . . .
+ AP(1),P(2)[1] · · ·

δ
δf (x) AP(N−1),P(N)[ f ]

f =1
.
(12.20)
The orthogonality relations (12.10), (12.11) imply that the products in the above
expressions are different from zero only when P is the identity permutation, i.e.
when P(2 j −1) and P(2 j) are adjacent numbers for each matrix element in the
product.
Hence, the sum in (12.19) reduces to the expression in (12.20) where P( j) =
j, ∀j. Each element AP(2 j−1),P(2 j) yields a factor 2, from the matrix A in
(12.12) so that each product in (12.20) reduces to an expression of the type
2N/2−1 
δ
δf (x) A2k−1,2k[ f ]

f =1. Comparing (12.16) and (12.19), we eventually ﬁnd
that
ρ(x) = N!|ˆaN|2N/2−1
NZ[ f = 1]
N/2

k=1

δ
δf (x) A2k−1,2k[ f ]

f =1
.
(12.21)

12.1 β = 1
85
Making the result of the functional differentiation of (12.18) explicit, and rear-
ranging indices, we ﬁnally obtain
ρ(x) =
1
2N
N/2−1

k=0
w(x)[R2k(x)Φ2k+1(x) −R2k+1(x)Φ2k(x)] ,
(12.22)
where
Φk(x) =
 ∞
−∞
dyRk(y)w(y)sign(x −y) .
(12.23)
For the Gaussian case, it can be shown [1, 2] that we can choose
R2k(x) =
√
2
π
1
4 2k(2k)!!
H2k(x) ,
(12.24)
R2k+1(x) =
√
2
π
1
4 2k+2(2k −1)!!

−H2k+1(x) + 4kH2k−1(x)

,
(12.25)
where the Hk(x) are Hermite polynomials. This gives, for example,
R0(x) =
√
2
π
1
4
,
R1(x) = −
√
2x
2π
1
4
,
R2(x) =
√
2(2x2 −1)
2π
1
4
,
R3(x) = −
√
2x(2x2 −5)
2π
1
4
.
(12.26)
Since the leading coefﬁcient of Hk(x) is 2k, we have
ˆaN =
(−1)
N
2 2
N(N−2)
4
π
N
4 N/2−1
k=0
(2k)!
,
(12.27)
even though this quantity has completely dropped out from the ﬁnal expression for
the density (12.22).
-10
-5
0
5
10
x
0
0.04
0.08
0.12
0.16
ρ(x)
-10
-5
0
5
10
x
0
0.02
0.04
0.06
0.08
0.1
0.12
ρ(x)
-10
-5
0
5
10
x
0
0.02
0.04
0.06
0.08
0.1
ρ(x)
Fig. 12.1 Comparison between numerically generated eigenvalue histograms of 50000 matrices
of size N = 8 belonging to the Gaussian ensembles (GOE on the left, GUE in the middle, and GSE
on the right) and the corresponding theoretical densities

86
12
Finite N Is Not Finished
For a numerical check of (12.22), see Fig.12.1 below, which was obtained with
the code [♠Gaussian_finite_density_check.m].
12.2
β = 4
Now
ρ(x1, . . . , xN) = 1
Z|ΔN(x)|4
N

i=1
w(xi) .
(12.28)
Start by writing |ΔN(x)|4 as a determinant of size 2N, in which two columns
depend on each variable. This is
|ΔN(x)|4 = det[xk
i
kxk−1
i
] ,
(12.29)
where 1 ≤i ≤N and 0 ≤k ≤2N −1. For instance, for N = 2 we have
(x2 −x1)4 = det
⎛
⎜⎜⎝
1
0
1
0
x1
1
x2
1
x2
1 2x1 x2
2 2x2
x3
1 3x2
1 x3
2 3x2
2
⎞
⎟⎟⎠,
(12.30)
which can be veriﬁed if you have 10min to spare.
We can change xk
i by any family of polynomials Qk(xi) = bkxk
i +· · · that produce
the Vandermonde, and kxk−1
i
by its derivative Q′
k(xi). So
|ΔN(x)|4 = |ˆbN| det[Q j−1(xi)
Q′
j−1(xi)] ,
(12.31)
where 1 ≤j ≤2N and
ˆbN =
N−1

k=0
b2
k
−1
.
(12.32)
In this case, Eq. (12.17) gets modiﬁed as
Z[a] = (2N)!|ˆbN|Pf(Bi, j[a]) ,
(12.33)
where
Bi, j[a] =
 ∞
−∞
dx[Qi−1(x)Q′
j−1(x) −Q′
i−1(x)Q j−1(x)]w(x)a(x) .
(12.34)
We have used here the second De Brujin identity (11.4).
We may consider the above integral as another skew-symmetric inner product

12.2 β = 4
87
⟨f, g⟩4 = 1
2
 ∞
−∞
dx[ f (x)g′(x) −f ′(x)g(x)]w(x) ,
(12.35)
and we may choose the polynomials Q to be skew-orthogonal with relation to this:
evens and odds are orthogonal among themselves,
⟨Q2k, Q2ℓ⟩4 = ⟨Q2k+1, Q2ℓ+1⟩4 = 0 ,
(12.36)
and evens are orthogonal to odds unless they are adjacent,
⟨Q2k, Q2ℓ+1⟩4 = −⟨Q2ℓ+1, Q2k⟩4 = δkℓ.
(12.37)
Computing the functional derivative as before, we have
ρ(x) =
1
2N
N−1

k=0
w(x)[Q2k(x)Q′
2k+1(x) −Q2k+1(x)Q′
2k(x)] .
(12.38)
In the Gaussian case, we can choose
Q2k =
√
2
π
1
4 2k(2k)!!

4kQ2k−2(x) + H2k(x
√
2)

,
Q2k+1(x) =
√
2
π
1
4 2k+1(2k + 1)!!
H2k+1(x
√
2) .
(12.39)
This gives, for example,
Q0 =
√
2
π
1
4 ,
Q1(x) = 2x
π
1
4 ,
Q2(x) =
√
2(4x2 + 1)
2π
1
4
,
Q3(x) = 2x(4x2 −3)
3π
1
4
.
(12.40)
Also, we have
ˆbN = 2⌊N−1
2 ⌋2N(N−1)
π
N
2 N−1
k=0 (k!!)2 .
(12.41)
Again, for a numerical check of (12.38), see Fig.12.1, which was obtained with
the code [♠Gaussian_finite_density_check.m].
References
1. S. Ghosh, A. Pandey, Phys. Rev. E 65, 046221 (2002)
2. M.L. Mehta, Random Matrices (Academic Press, 1967)

Chapter 13
Classical Ensembles: Wishart-Laguerre
In this Chapter, we present one of the “classical” examples of rotationally invariant
models: the Wishart-Laguerre (WL) ensemble.
13.1
Wishart-Laguerre Ensemble
Historically, one of the earliest appearances of a random matrix ensemble1 occurred
in 1928, when the Scottish mathematician John Wishart published a paper on mul-
tivariate data analysis in the journal Biometrika [3].
Wishart matrices are square N × N matrices W with correlated entries. They are
constructed as 2 W = H H †, where H is a N × M matrix (M ≥N) ﬁlled with i.i.d.
Gaussian entries.3 These entries may be real, complex or quaternion (we shall use
again the Dyson index β = 1, 2, 4 for the three cases, respectively), and † stands
for the transpose or hermitian conjugate of the matrix H. For example, for a 2 × 3
complex matrix H
W =
 x11 + iy11 x12 + iy12 x13 + iy13
x21 + iy21 x22 + iy22 x23 + iy23
 ⎛
⎝
x11 −iy11 x21 −iy21
x12 −iy12 x22 −iy22
x13 −iy13 x23 −iy23
⎞
⎠.
(13.1)
Work out the matrix product, and convince yourself that W is hermitian, therefore
has real eigenvalues.
1In Mathematics, however, the 1897 work by Hurwitz on the volume form of a general unitary
matrix is of historical signiﬁcance [1, 2].
2Sometimes you ﬁnd a normalized version of it, with a 1/M factor in front.
3The notation W(N, M) is also used.
© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_13
89

90
13
Classical Ensembles: Wishart-Laguerre
TheWishartensembleisalsoreferredtoas“Laguerre”,sinceitsspectralproperties
involve Laguerre polynomials, and also “chiral” in the context of applications to
Quantum Chromodynamics (QCD) [4]. They are often called LOE, LUE and LSE,
for β = 1, 2, 4, respectively.
While the Gaussian eigenvalues can in principle be anywhere on the real axis,
Wishartmatriceshave N non-negativeeigenvalues,{x1, x2, . . . , xN}.Indeed,Wishart
matrices W are positive semideﬁnite. This means that (e.g. for β = 2) u⋆Wu ≥0 for
all nonzero column vectors u of N complex numbers. The proof is not hard, have a
go at it!
Question 13.1 What is the jpdf of the entries of WL ensemble W?
▶With some effort (see below), it can be computed as
ρ[W] ∝e−1
2 TrW(det W)
β
2 (M−N+1)−1 ,
(13.2)
from which you immediately see that (i) the entries are correlated4 (the deter-
minant easily kills any hope of factorizing this jpdf), and (ii) the model is rota-
tionally invariant.5
From (13.2),thejpdfofeigenvalues canbewrittendownimmediately(justexpress
everything in terms of the eigenvalues and append a Vandermonde at the end)
ρ(x1, . . . , xN) =
1
Z(L)
N,β
e−1
2
N
i=1 xi
N
	
i=1
xαβ/2
i
	
j<k
|x j −xk|β,
(13.3)
where α = (1+M −N)−2/β and the normalization constant Z(L)
N,β can be computed
again using modiﬁcations of the Selberg integral6 [6]. As for the Gaussian ensembles,
one may sometimes ﬁnd in the literature an extra factor β in the exponential.
The conﬁning potential for the Wishart-Laguerre ensemble is thus V (x) =
1
2 x −α
2 ln x, and this clearly motivates the use of (associated) Laguerre polynomials
L(α)
n (x), which are orthogonal with respect to this precise weight (after a simple
rescaling),

 ∞
0
dx xαe−x L(α)
n (x)L(α)
m (x) = Γ (n + α + 1)
n!
δm,n .
(13.4)
4Unless for speciﬁc combinations of β, M, N for which the determinant disappears.
5The jpdf (13.2) is not in contrast with Weyl’s lemma (see Eq.(3.8)). The determinant of a N × N
matrix W can be indeed written as a function of the traces of the ﬁrst N powers of W (see [5]).
6Note that while for Wishart matrices M −N is a non-negative integer and β = 1, 2 or 4, the jpdf
in (13.3) is well deﬁned for any β > 0 and any α > −2/β (this last condition is necessary to ensure
that the jpdf is normalizable). When these parameters take continuous values, this jpdf deﬁnes the
so-called β-Laguerre ensemble.

13.1 Wishart-Laguerre Ensemble
91
Question 13.2 What happens if I take M < N?
▶This situation deﬁnes the so-called Anti-Wishart ensemble ˜W. In this case, one
can show that N −M eigenvalues are exactly 0. The jpdf is similar to (13.3), but
some of the matrix elements of ˜W are non-random and deterministically related
to the ﬁrst M rows of ˜W [7].
The code [♠Wishart_check.m] produces instances of Wishart matrices for
different βs, as well as normalized histograms of their eigenvalues. You can start
having a look at it now, but please come back to it after reading the next chapter.
Question 13.3 What is the limiting spectral density of the WL ensembles for
N →∞?
▶It is called the Marˇcenko-Pastur density [8], which is superimposed to the
histograms produced with the code above in Fig.14.1 of the next chapter. We
are going to derive it using the resolvent method very shortly.
13.2
Jpdf of Entries: Matrix Deltas...
The calculation of the jpdf of entries (13.2) proceeds through a few simple steps. Set
for simplicity β = 2 (hermitian matrices). We can formally write
ρ[W] =

d Hρ[H]δ

W −H H †
.
(13.5)
As usual, the measure d H means that we are integrating over the 2N M degrees of
freedom (dof) 7 of H: each entry of the N × M matrix H is a complex number, so it is
parametrized by two real numbers.8 Therefore, d H = N
i=1
M
j=1 dRe[Hi j]dIm[Hi j].
The matrix delta δ(W −H H †) enforces the constraint that a certain matrix W
must be equal to another matrix H H †. We do have an integral representation for
the scalar delta function, which does the same job for real numbers. It should then
be easy to work out the corresponding integral representation for the delta function
7With “degrees of freedom” we mean the independent real parameters that are necessary to deﬁne
a matrix. For example, a hermitian matrix has N 2 degrees of freedom – the N real entries on the
diagonal, and the real and imaginary parts of the entries in the upper triangle.
8Note, in particular, that for N = M the square matrix H is not hermitian, and has 2N 2 “degrees
of freedom” (dof) instead of N 2.

92
13
Classical Ensembles: Wishart-Laguerre
of, say, a N × N hermitian matrix K – after all, it will just be the product of scalar
deltas, one for each of the real dof.
δ(K) =
N
	
i=1
δ(Kii)
N
	
i=1
	
j>i
δ(K (R)
i j )δ(K (I)
i j ) =

 dT11
2π
· · · dTN N
2π
exp
⎧
⎨
⎩i
N

i=1
Tii Kii
⎫
⎬
⎭

N
	
i=1
	
j>i
dT (R)
i j
2π
dT (I)
i j
2π
ei N
i=1

j>i [T (R)
i j
K (R)
i j
+T (I)
i j
K (I)
i j ] ,
(13.6)
where we have introduced a set of N(N + 1)/2 parameters {T }, one for each delta.
Arranging the parameters {T } into a hermitian matrix, try to show that the ugly
expression in (13.6) can be recast in the more elegant form
δ(K) =
1
2Nπ N 2

dT eiTr[T K] .
(13.7)
We can now perform the multiple integral in (13.5), with Gaussian distributed dof
of H
ρ[H] ≡ρ(H(R)
11 , H(I)
11 , . . . , H(R)
N M, H(I)
N M) =
N
	
i=1
M
	
j=1
 1
2π exp

−1
2 H(R)2
i j
−1
2 H(I)2
i j

=
 1
2π
N M
e−1
2 Tr(H H†) ,
(13.8)
where (R) and (I) denote the real and imaginary part of each of the N M entries of H.
Combining (13.5), (13.7) and (13.8) we have
ρ[W] =
1
2Nπ N 2
 1
2π
N M 
dT

d H e−1
2 Tr(H H †)+iTrT (W−H H †) .
(13.9)
Dividing all the dof of the hermitian matrix T by 1/2 (i.e. changing variables
T →T/2), we obtain
ρ[W] =
1
2Nπ N 2
 1
2π
N M 1
2
N 2 
dT

d H e−1
2 Tr(H H †)+ i
2 TrT(W−H H †) .
(13.10)
13.3
...and Matrix Integrals
Next, we use the following identity for N × N hermitian matrices T

13.3 ...and Matrix Integrals
93
[det(μ1 −T )]−M =
1
(4πi)M N

M
	
k=1
d2sk exp

i
2μ
M

k=1
s†
ksk −i
2
M

k=1
s†
kT sk

,
(13.11)
where 1 is the N ×N identity matrix, sk are k = 1, . . . , M complex (column) vectors,
so that d2sk = N
i=1 dsk,ids⋆
k,i and μ is such that Im[μ] > 0.
Question 13.4 Any hint on how to prove it?
▶Just write T = U †	U, with U the unitary matrix diagonalizing T , and
	 the diagonal matrix of eigenvalues λi. Then make the change of variables
Usk →˜sk, which is unitary and thus has Jacobian equal to 1. The resulting
integral factorizes as

M
	
k=1
d2˜sk →
 N
	
ℓ=1
2

dx dy e
i
2 (x−iy)(μ−λℓ)(x+iy)
M
,
(13.12)
where x, y are real and imaginary part of the ℓth entry of sk, and the factor of 2
is the Jacobian of the change of variables {˜sk,i, ˜s⋆
k,i} →{x, y}. The integral in
{x, y} yields 2πi/(μ −λℓ), from which the claim is immediate.
We can now perform the H integral in (13.10). How? Just imagine that the kth
vector sk is constructed as sk = (H1k, . . . , HNk)T , i.e. it is basically the kth column
of the rectangular matrix H.
Hence, note the identity −(1/2)Tr(H H †) = (i/2)μ M
k=1 s†
ksk, with μ = i.
Finally, we have to calculate the Jacobian of the change of variables {H (R)
ik , H (I)
ik } →
{sk,i, s⋆
k,i}. For each entry of H, we have

sk,i
= H (R)
ik
+ iH (I)
ik
s⋆
k,i
= H (R)
ik
−iH (I)
ik .
(13.13)
The Jacobian from s →H is
⎛
⎜⎜⎝
∂sk,i
∂H (R)
ik
∂sk,i
∂H (I)
ik
∂s⋆
k,i
∂H (R)
ik
∂s⋆
k,i
∂H (I)
ik
⎞
⎟⎟⎠=
1 i
1 −i

= −2i .
(13.14)
Thus, the Jacobian from H →s (the one we need) is (in absolute value) equal to
1/2 for each entry. In total, (1/2)N M.
Therefore, using (13.11)

94
13
Classical Ensembles: Wishart-Laguerre
ρ[W] =
1
2N π N2
 1
2π
N M 1
2
N2+N M
(4πi)M N

dT e
i
2 Tr(T W) [det (i1 −T )]−M .
(13.15)
We now need another matrix integral, with the pompous name “Ingham-Siegel
integral of second type” [9], whose general formula reads (see Appendix A in [10])
JN,M(Q, μ) =

dT eiTr(T Q)[det(T −μ1)]−M = CM,N(det Q)M−NeiμTrQ ,
(13.16)
with CM,N = 2Nπ N(N+1)/2iN M/ M
j=M−N+1 Γ ( j), and the matrix Q is hermitian
and positive deﬁnite, while T is just hermitian. Both are N × N. We also require
Im(μ) > 0 to ensure convergence, and M ≥N.
To use this integral (13.16), we need to multiply back again all the degrees of
freedom of the matrix T by 2, and pull out a factor (−2) from the determinant,
resulting in
ρ[W] = 2−N(1+M)π−N 2i−M N

dT eiTr(T W)

det

T −i
21
−M
,
(13.17)
which can be evaluated using (13.16) as
ρ[W] = 2−N(1+M)π−N 2i−M N × 2Nπ N(N+1)/2iN M
M
j=M−N+1 Γ ( j)
(det W)M−Ne−(1/2)TrW
=
1
2N Mπ
N(N−1)
2
M
j=M−N+1 Γ ( j)
(det W)M−Ne−(1/2)TrW ,
(13.18)
i.e. the jpdf of the entries of Wishart matrices for β = 2, with the correct normaliza-
tion9 (note that all the imaginary factors have correctly disappeared). Well done!
13.4
To Know More...
1. The spectral densities of the Wishart-Laguerre ensemble for ﬁnite N and β =
1, 2, 4 have been given explicitly in [12], together with numerical checks.
2. The large-N behavior of the spectral density and two-point function for the
Wishart-Laguerre ensemble is determined by the asymptotics of Laguerre
polynomials (in complete analogy with the Gaussian case). These are explicitly
given in [13].
3. Non-hermitian analogues of the Wishart-Laguerre ensemble can also be deﬁned
(see [14] for a nice review).
9A reliable source for such normalizations is [11].

13.4 To Know More...
95
4. Readers interested in the diagrammatic approach to ﬂuctuations in the Wishart
ensemble should have a look at [15].
5. For a nice review on usefulness of Wishart-Laguerre ensemble in physics, see
[16]. For speciﬁc applications to QCD, see [4, 17].
References
1. A. Hurwitz, Nachr. Ges. Wiss Göttingen 71 (1897)
2. P. Diaconis, P.J. Forrester, Random Matrices: Theory Appl. 06, 1730001 (2017)
3. J. Wishart, Biometrika 20A, 32 (1928)
4. J.J.M. Verbaarschot, Applications of Random Matrix Theory to QCD, in The Oxford Handbook
of Random Matrix Theory, eds. by G. Akemann, J. Baik, and P. Di Francesco (Oxford University
Press, Oxford, 2011)
5. F. Kleefeld, M. Dillig, Trace evaluation of matrix determinants and inversion of 4×4 matrices
in terms of Dirac covariants (1998), https://arxiv.org/pdf/hep-ph/9806415.pdf
6. P.J. Forrester, S.O. Warnaar, Bull. Amer. Math. Soc. (N.S.) 45, 489 (2008)
7. R.A. Janik, M.A. Nowak, J. Phys. A Math. Gen. 36, 3629 (2003)
8. V.A. Marˇcenko, L.A. Pastur, Math. USSR-Sb 1, 457 (1967)
9. Y.V. Fyodorov, Nucl. Phys. B 621, 643 (2002)
10. E. Kanzieper, N. Singh, J. Math. Phys. 51, 103510 (2010)
11. A. Edelman, Eigenvalues and Condition Numbers of Random Matrices, MIT Ph.D. Thesis
(1989)
12. G. Livan, P. Vivo, Acta Phys. Pol. B 42, 1081 (2011)
13. P.J. Forrester, N.E. Frankel, T.M. Garoni, J. Math. Phys. 47, 023301 (2006)
14. G. Akemann, Acta Phys. Pol. B 42, 0901 (2011)
15. J. Jurkiewicz, G. Łukaszewski, M. Nowak, Acta Phys. Pol. B 39, 799 (2008)
16. S.N. Majumdar, Extreme eigenvalues of wishart matrices: application to entangled bipartite
system, in The Oxford Handbook of Random Matrix Theory, eds. by G. Akemann, J. Baik, and
P. Di Francesco (Oxford University Press, Oxford, 2011)
17. K. Splittorff, J.J.M. Verbaarschot, Lessons from Random Matrix Theory for QCD at Finite
Density (2008), https://arxiv.org/pdf/0809.4503.pdf

Chapter 14
Meet Marˇcenko and Pastur
In this Chapter, we investigate the average spectral density for the Wishart-Laguerre
ensemble.
14.1
The Marˇcenko-Pastur Density
The average density of eigenvalues has the following scaling form for N, M →∞
(such that c = N/M ≤1 is kept ﬁxed)
ρ(x) →
1
βN ρMP
 x
βN

,
(14.1)
where the Marˇcenko-Pastur scaling function (the analogue of the semicircle ρSC(x)
in Eq. (3.6) for the Gaussian ensemble) is independent of β and given by [1]
ρMP(y) =
1
2πy

(y −ζ−)(ζ+ −y) ,
(14.2)
for x ∈[ζ−, ζ+]. The edge-points ζ± are given by ζ−= (1 −c−1/2)2 and ζ+ =
(1 + c−1/2)2.
This scaling function ρMP(y) has a compact support on the positive semi-axis for
c < 1 (with two soft edges), but becomes singular at the origin if c →1 (and the
origin becomes a hard edge). This means that Wishart matrices constructed from
square matrices H exhibit an accumulation of eigenvalues very close to zero.
It is worth stressing that the typical scale of an eigenvalue is ∼O(N) in the WL
case, as opposed to the scale ∼O(
√
N) for the Gaussian ensemble.
© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_14
97

98
14
Meet Marˇcenko and Pastur
14.2
Do It Yourself: The Resolvent Method
Let us now derive the Marˇcenko-Pastur density using the resolvent (or Stieltjes
transform) method. The partition function (normalization constant) for the Wishart-
Laguerre ensemble reads (after a rescaling xi →βNxi)
Z(L)
N,β ∝
 ∞
0
N

j=1
dx j e−βN
2
N
i=1 xi
N

i=1
xαβ/2
i

j<k
|x j −xk|β =
 ∞
0
N

j=1
dx j e−βNV[x] ,
(14.3)
with
V[x] = 1
2

i
xi +
	2/β −1
2N
−M
2N + 1
2

 
i
ln xi −1
2N

i̸= j
ln |xi −x j| . (14.4)
As in Chap.8, the xi are now of O(1) for large N. We can again perform the saddle
point evaluation of the N-fold integral (14.3), but this time there is an additional
subtlety which, if overlooked, leads straight to a nonsensical answer.
The subtlety is that the minimization of the exponent should be carried out within
the set of positive x. In other words, on top of the saddle-point equation, there is an
inequality constraint to satisfy as well, xi > 0
∀i.
One way to handle this constraint is to introduce a penalty function −μ 
i ln(xi)
in the “action” V[x], with a Lagrange multiplier μ. Since −ln(t) →∞for t →0, it
actsasifeachparticlefeltanextra“inﬁnitewall”-typeofrepulsionwhileapproaching
the origin, and thus helps conﬁning the eigenvalues on the positive semi axis. The
extra wall is then “gently” removed (μ →0) at the end of the calculation.
The saddle-point equations now read for anyi (and for N ≫1 and N/M = c ≤1)
1
2 +
1
2 −1
2c −μ
 1
xi
= 1
N

j̸=i
1
xi −x j
.
(14.5)
Multiplying (14.5) by
1
N(z−xi) and summing over i, we get in analogy with
Eq. (8.18)
1
2G N(z) +
1
2 −1
2c −μ
 1
N

i
1
xi(z −xi) = 1
2G2
N(z) + 1
2N G′
N(z) .
(14.6)
The second term can be expressed in terms of G N(z) using
1
N

i
1
xi(z −xi) =
1
zN

i
1
xi
+
1
z −xi

= K + G N(z)
z
,
(14.7)

14.2 Do It Yourself: The Resolvent Method
99
and taking the average G(av)
∞(z) = ⟨G N(z)⟩in the limit N →∞, we obtain
1
2G(av)
∞(z) +
1
2 −1
2c −μ
 K + G(av)
∞(z)
z
= 1
2G(av)2
∞
(z) .
(14.8)
Here K is a constant that we assume ﬁnite (by derivation, we have K
=

dxρ(x)/x).
Note that, had we not included the penalty function parametrized by μ from the
beginning, we would have landed for c = 1 on the equation 1
2G(av)
∞(z) = 1
2G(av)2
∞
(z),
from which no sensible spectral density could be extracted! This is because the
Wishart eigenvalues cannot equilibrate on the entire real line under a potential
V (x) = x (which is not conﬁning for x →−∞).
It is convenient to set γ = (1 −c)/c > 0. Solving now the quadratic equation
(14.8) for μ →0, we get
G(av)
∞(z) = 1
2

±

γ 2 −4γ K z + z2 −2γ z
z
−γ
z + 1

.
(14.9)
Setting now z = x −iε, multiplying up and down by x + iε and using the real
and imaginary part of the square root p and q as in (8.3), we obtain
1
π ImG(av)
∞(x −iε) =
−εx ± qx
2π(x2 + ε2)
ε→0+
−→
√(x −x−(γ, K))(x+(γ, K) −x)
2πx
,
(14.10)
where it is understood that the (±) sign in (14.9) is to be chosen differently in
different x-intervals, in analogy with the Gaussian case. Of course, the right hand
side of (14.10) is only valid for x such that the square root exists. The constants
x±(γ, K) = γ

−2
√
K 2 + K + 2K + 1

.
We now have to ﬁx the constant K by requiring normalization of ρ(x). Using the
integral (for b > a)
 b
a
dx
√(x −a)(b −x)
2πx
= 1
4

−2
√
ab + a + b

,
(14.11)
all we have to do is to assign a ←x−(γ, K) and b ←x+(γ, K), and to solve
1
4

−2
√
ab + a + b

= 1 for K. This gives K = 1/γ .
And for this value of K, the edge points become x±(γ, 1/γ ) →(1 ± 1/√c)2,
which means that we have recovered the MP law (14.2) using the resolvent method.
Congratulations!

100
14
Meet Marˇcenko and Pastur
0
2
4
6
8
10
12
14
16
x
0
0.1
0.2
0.3
0.4
0.5
ρ(x)
c = 1/2
c = 1/8
β = 1
β = 2
β = 4
β = 1
β = 2
β = 4
Fig. 14.1 Comparison between the Marˇcenko-Pastur density for two different values of the rect-
angularity ratio c and the corresponding histograms obtained from the numerical diagonalization
of random Wishart matrices (for all possible values of β). All histograms are obtained from 5000
Wishart matrices of size N = 100
You can now fully enjoy Fig.14.1, where we show a comparison between the
Marˇcenko-Pastur density and the histograms obtained by numerical diagonalization
of WL random matrices for different βs.
Question 14.1 Wait a second...In the derivation, we said that we had to assume
K ﬁnite and equal to K =

dxρ(x)/x (because the constant K arises as the
average ⟨1
N

i
1
xi ⟩). Shouldn’t we check that this is consistent with the ﬁnal
result?
▶Yes, we should! The integral

dxρ(x)/x amounts to computing the following
 b
a
dx
√(x −a)(b −x)
2πx2
= −2
√
ab + a + b
4
√
ab
,
(14.12)
and setting a ←(1 −1/√c)2 and b ←(1 + 1/√c)2. This gives c/(1 −c),
which is precisely equal to K = 1/γ . Bingo!

14.2 Do It Yourself: The Resolvent Method
101
Question 14.2 What if I wanted to use the Coulomb gas technique to derive the
Marˇcenko-Pastur law?
▶The partition function (normalization constant) for the WL ensemble, after
the rescaling xi →xiβ, reads
Z(L)
N,β = C(L)
N,β

(0,∞)N
N

j=1
dx j e−βV[x] ,
(14.13)
where the energy is given this time by V[x] =
1
2

i xi −α
2

i ln xi −
1
2

i̸= j ln |xi −x j|.
In the WL case, the gas is in equilibrium under the competing effect of a
linear+logarithmic conﬁning potential, and the 2D electrostatic repulsion.
Following the same procedure as in Chap.4 (but with the rescaling n(x) →
(1/N)n(x/N)), we obtain for the energy functional V[n(x)] = N 2 ˆV, with
ˆV[n(x)] =

dx v(x) n(x) −1
2

dxdx′n(x)n(x′) ln |x −x′| ,
(14.14)
with v(x) = x/2 −1
2(1/c −1) ln x.
The singular integral equation for the equilibrium density readily follows
Pr

dx′ n⋆(x′)
x −x′ = 1
2 −1
2
1
c −1
 1
x .
(14.15)
Try to apply Tricomi’s formula (5.16) – assuming a single-support solution on
[a, b] - and then determine a, b in such a way that the free energy is minimized.
You will discover that n⋆(x) = ρMP(x) as it should.
14.3
Correlations in the Real World and a Quick Example:
Financial Correlations
A huge number of scientiﬁc disciplines, ranging from Physics to Economics, often
need to deal with statistical systems described by a large number of degrees of free-
dom. Thus, understanding and describing the collective behavior of a large numbers
of random variables is one of the most fundamental issues in multivariate Statistics.
More often than not, the problem can be addressed in terms of correlations.
Suppose we are interested in understanding the correlation structure of a system
described in terms of N random variables {x1, . . . , xN}, drawn from a – potentially

102
14
Meet Marˇcenko and Pastur
unknown, but not changing in time – jpdf p(x). In order to do so, one of the most
obvious operations to perform is to collect, if possible, as many “experimental obser-
vations” of such variables. Such observations can then be used to compute empirical
time averages of quantities expressed in terms of the random variables. So, let us
assume we have collected M observations – say, equally spaced in time – for each
variable. Quite straightforwardly, one can collect all these numbers in a N × M
matrix X whose entries are xt
i (i = 1, . . . , N, t = 1, . . . , M).
Assuming all variables xt
i are adjusted in order that their sample mean1 is zero
and their sample variance is 1, then the quantity
ci j = 1
M
M

t=1
xt
i xt
j
(14.16)
yields the well known Pearson estimator for the correlation between variables xi and
x j. This is an estimator of the true (or population) correlation2 ˜ci j, which would
be measured exactly for M →∞, i.e. as more and more observations are added to
the data. However, real life practice always entails working with ﬁnite-sized datasets
(i.e. with ﬁnite M), which introduces some degree of measurement error.
The estimators for each pair of variables in the system can be collected into a
single N × N matrix C = X X T /M, known as the sample correlation matrix of the
data in X, whose entries are given by Eq. (14.16). These amount to N(N −1)/2
real numbers (diagonal entries are equal to one), which for a large system represent
a whopping amount of information to process. So, what should we make of all
this? Well, a reasonable ﬁrst step could be to compare the empirical correlation
matrix of the system we are interested in with the prediction of a suitably deﬁned
null hypothesis. In the ﬁrst instance, we could for example look for a null model
describing uncorrelated Gaussian random data and see how our empirical data differ
from it.
By any chance, do we know a random matrix ensemble from which we can draw
this kind of random correlation matrices? Well, of course we do! It is precisely the
Wishart-Laguerre ensemble. As we discussed, the density of eigenvalues is well
known for this ensemble, and it is given by the Marˇcenko-Pastur law (14.2). This
means that a zero-th order assessment of the statistical signiﬁcance of the correlations
in a large system can be obtained from the comparison of the empirical eigenvalue
spectrum of its correlation matrix with the Marˇcenko-Pastur law for a system with
the same rectangularity ratio N/M.
A prime example of the procedure outlined above is the analysis of ﬁnancial
correlations. Suppose you want to invest your money in N stocks by forming an
investment portfolio. As the old saying goes, “don’t put your eggs in one basket”,
which in ﬁnancial terms translates into “don’t invest all your money in a portfolio
of highly correlated stocks” – not the most effective punchline, admittedly. Hence,
1The sample mean is ¯xi = (1/M) M
t=1 xt
i , not to be confused with the true mean ⟨xi⟩p(x), which
is a property of the jpdf p(x).
2The true correlation ˜ci j is a property of the jpdf p(x) of the random variables {x1, . . . , xN}.

14.3 Correlations in the Real World and a Quick Example: Financial Correlations
103
distinguishing signal from noise within ﬁnancial correlation matrices is of paramount
importance to build a well diversiﬁed portfolio, where the possible losses due to the
adverse movement of a group of stocks can be offset by other groups of stocks.
When an empirical ﬁnancial correlation matrix is diagonalized, one usually ﬁnds
that several eigenvalues are much larger than the expected upper bound of the
Marˇcenko-Pastur law. The information contained in the associated eigenvectors typ-
ically shows that these are due to the co-movements of groups of highly correlated
stocks belonging to well deﬁned market sectors (e.g. pharmaceutical, ﬁnancial, etc.).
This kind of random matrix approach to ﬁnancial correlations was initiated in [3, 4]
and since then a considerable number of papers has been devoted to it (see [5] for a
recent account).
References
1. V.A. Marˇcenko, L.A. Pastur, Math. USSR-Sb 1, 457 (1967)
2. J. Wishart, Biometrika 20A, 32 (1928)
3. L. Laloux, P. Cizeau, J.-P. Bouchaud, M. Potters, Phys. Rev. Lett. 83, 1467 (1999)
4. V. Plerou, P. Gopikrishnan, B. Rosenow, L.A.N. Amaral, H.E. Stanley, Phys. Rev. Lett. 83, 1471
(1999)
5. J. Bun, J.-P. Bouchaud, M. Potters, Phys. Rep. 666, 1 (2017)

Chapter 15
Replicas...
In this Chapter, we add one more powerful tool to our arsenal. The Edwards-Jones
formula, in conjunction with the celebrated replica trick.
15.1
Meet Edwards and Jones
The Edwards-Jones formula [1] allows to write down a formal expression for the
average spectral density ρ(x) of a completely generic ensemble of real symmetric
random matrices H, taking as a starting point just the jpdf of the entries in the upper
triangle, ρ[H].
The formula reads
ρ(x) = −2
π N lim
ε→0+ Im ∂
∂x

Log Z(x)

,
(15.1)
where
Z(x) =

RN d y exp

−i
2 yT (xε1 −H) y

,
(15.2)
where xε = x −iε.
The average ⟨·⟩is taken with respect to ρ[H], i.e. ⟨·⟩=

d H11 · · · d HN Nρ[H](·).
This formula is remarkable: it allows to compute the spectral density—the
marginal of the jpdf of the eigenvalues—without knowing the jpdf of eigenvalues!
Only the information about the entries is required as input.
While the formula (15.1) is in principle valid for any ﬁnite N, in practice the
calculations can be carried out until the end only in the limit N →∞, where several
simpliﬁcations take place.
© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_15
105

106
15
Replicas...
15.2
The Proof
The proof is not complicated—even though there are several subtleties. Recall from
Chap. 2 how the average spectral density is deﬁned ρ(x) =

1
N
N
i=1 δ(x −xi)

.
Recall also the Sokhotski-Plemelj identity: as ε →0+,
1
x ± iε →Pr
	1
x

∓iπδ(x) .
(15.3)
This equation provides an interesting identity for the delta function, which we
already used in Chap.8. We can therefore write
ρ(x) =
1
π N lim
ε→0+ Im

N

i=1
1
x −iε −xi

= −1
π N lim
ε→0+ Im

N

i=1
1
xi + iε −x

,
(15.4)
where Im stands for the imaginary part, and we changed a sign for later convenience.
Next, we write the denominator in the sum as the derivative of a logarithm. But
the denominator is a complex number: and the logarithms of complex numbers are
nasty beasts1. Anyway, we can choose the principal branch of the logarithm—and
denote it by Log—to write
ρ(x) =
1
π N lim
ε→0+ Im ∂
∂x

N

i=1
Log(xi + iε −x)

.
(15.5)
Next, we use the identity
Z(x) = (2π)N/2 exp

−1
2
N

i=1
Log(xi + iε −x) + i Nπ
4

,
(15.6)
where Z(x) is given by the multiple integral in (15.2). You can check this identity
with the code [♠Zmultiple.m]
Now, compare the last two equations. Clearly, the ﬁnal formula would be easily
established if we could replace N
i=1 Log(xi + iε −x) in (15.5) with something
related to Z(x), using (15.6).
To extract N
i=1 Log(xi + iε −x) from (15.6), we should take the logarithm on
both sides. There is a small glitch, though, due to another mind-boggling feature of
complex logarithms. Namely, Log(exp(z))may not just be equal to z, for z ∈C!2
1For example, Log(z1z2) may not be equal to Log(z1) + Log(z2)!
2For instance, if z = 0.2 −4.4i, then Log(exp(z)) = 0.2 + 1.88319i.

15.2 The Proof
107
However, we can still write
N

i=1
Log(xi + iε −x) = −2Log Z(x) + terms that are killed by ∂
∂x .
(15.7)
Inserting (15.7) into (15.5), we establish the ﬁnal formula (15.1).
15.3
Averaging the Logarithm
The Edwards-Jones formula (15.1) thus requires computing

Log Z(x)

, where the
average is taken over several realizations of the matrix H.
This means that we should compute

Log Z(x)

=

d H11 · · · d HN Nρ[H]Log

RN d y exp

−i
2 yT (xε1 −H) y

,
(15.8)
which is very annoying: the logarithm is right in the way!
We would really need to exchange the order of integrals to perform the average
over H before the average over y—otherwise we would be running the Edwards-
Jones formula backwards and gain nothing!
There are two strategies to circumvent this obstacle, each with their own subtleties.
To know more about the replica method and its applications to spin glass theory see
[2, 3].
15.4
Quenched versus Annealed
Calling the quantity in (15.2) Z(x) is intentional: we wish to interpret it as the
partition function of an associated stat-mech model in the canonical ensemble. The
logarithm of Z will then be the free energy of this model.
Looking again at the multiple integral deﬁning Z(x), Z(x) =

RN d y exp

−i
2 yT
(xε1 −H) y

, we see that it encodes two different ‘levels’ of randomness: (i)
the random matrix H—the so called disorder—and (ii) the dynamical variables
y, which morally3 follow a Gibbs-Boltzmann distribution P(y1, . . . , yN) =
1
Z(x)
exp(−H(y; H, x)).
Computing now

Log Z(x)

—as we should—assumes that the two levels of ran-
domness are unfolding on different timescales: ﬁrst, the dynamical variables y need
to equilibrate according to the Gibbs-Boltzmann distribution for a ﬁxed instance of
3“Morally”, since the “Hamiltonian” H is actually complex, so P(y1, . . . , yN) is not a proper
distribution.

108
15
Replicas...
the random matrix H—and only afterwards the free energy is averaged over the
disorder (different realizations of H).
For these reasons, the disorder is called quenched4: it is there, but it acts slowly.
It only kicks in after the y’s have thermalized.
Computing a quenched disorder average is difﬁcult, but can be attempted—in the
limit N →∞—usingthesocalledreplicatrick,whichgetsridofthelogarithminside
the integral in (15.8) and allows the integrations over H and y to be interchanged.
More on this later.
A second strategy—which simpliﬁes the calculations considerably—is to cheat a
bit and treat the disorder as annealed instead.
This means that the associated stat-mech model is described in terms of the
joint set of dynamical variables {y, H}, leading to a partition function Z(ann)(x) =

d Hd y(· · · ).
The dynamical variables y are no longer integrated over at ﬁxed value of the
disorder H, but rather H and y ﬂuctuate and thermalize together. A questionable but
widespread way to describe in words an annealed average is: instead of computing
the quenched average ⟨Log Z(x)⟩—as we should—move the average inside the
logarithm5, Log⟨Z(x)⟩.
Clearly, this slick maneuver forces the logarithm out of the integrals, and allows
for a much quicker—even though not entirely justiﬁable—computation.
In the following section, we present the annealed calculation to obtain the semi-
circle law for the GOE.6
References
1. S.F. Edwards, R.C. Jones, J. Phys. A: Math. Gen. 9, 1595 (1976)
2. T. Castellani, A. Cavagna, J. Stat. Mech. P05012 (2005)
3. F. Zamponi, Mean ﬁeld theory of spin glasses (2014), https://arXiv.org/pdf/1008.4844.pdf
4Quenched adj. made less severe or intense; subdued or overcome; allayed; squelched.
5For the annealed average, we should more properly write Log Z(ann)(x)—with no further average
over H.
6This is only for training purposes. There is no need to use Edwards-Jones when the jpdf of
eigenvalues is known!

Chapter 16
Replicas for GOE
In this Chapter, we apply the Edwards-Jones formula to compute the average spectral
density of the GOE ensemble.
16.1
Wigner’s Semicircle for GOE: Annealed Calculation
The jpdf of entries in the upper triangle of a GOE is
ρ[H] =
N

i=1

exp

−N H 2
ii/2

/

2π/N
 
i< j

exp

−N H 2
i j

/

π/N

,
(16.1)
where we have already rescaled the unit variance by a factor 1/N. This has the net
effect of rescaling the eigenvalues by 1/
√
N (why?), so the corresponding spectral
density will have edges between −
√
2 and
√
2– not growing with N.
For the annealed calculation, we need to compute
Z(ann)(x) =

RN d y
 
i≤j
d Hi jρ[H] exp
	
−i
2 yT (xϵ1 −H) y

.
(16.2)
Separating diagonal and off-diagonal elements, and using the notation ⟨(·)⟩=
 
i≤j d Hi jρ[H](·), we can write
Z(ann)(x) ∝

RN d y exp
⎡
⎣−i
2 xϵ
N

i=1
y2
i
⎤
⎦

exp
⎡
⎣i
2
N

i=1
Hii y2
i
⎤
⎦
 
exp
⎡
⎣i
N

i< j
Hi j yi y j
⎤
⎦

, (16.3)
where we neglect some overall constant terms.
© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_16
109

110
16
Replicas for GOE
Expanding ez ≈1 + z + z2/2 + . . . and using the fact that the entries of H are
independent with ⟨Hi j⟩= 0 and ⟨H 2
i j⟩= 1/(N(2 −δi j)), we can write

exp

i
2
N

i=1
Hii y2
i

=
N

i=1

1 + i
2 Hii y2
i −1
8 H2
ii y4
i + . . .

=
N

i=1

1 −
1
8N y4
i + . . .

,
(16.4)

exp
⎡
⎣i
N

i< j
Hi j yi y j
⎤
⎦

=

i< j

1 + iHi j yi y j −1
2 H2
i j y2
i y2
j + . . .

=

i< j

1 −
1
4N y2
i y2
j + . . .

. (16.5)
Re-exponentiating, we can write

exp

i
2
N

i=1
Hii y2
i
 
exp
⎡
⎣i
N

i< j
Hi j yi y j
⎤
⎦

≃exp
⎡
⎣−1
8N
N

i, j=1
y2
i y2
j
⎤
⎦= exp
⎡
⎣−1
8N
 N

i=1
y2
i
2⎤
⎦.
(16.6)
Introducing a Gaussian identity
 ∞
−∞
dq exp

−αq2 + iγq

∝exp

−γ2/4α

(16.7)
with γ = N
i=1 y2
i and α = 2N yields
Z(ann)(x) ∝
 ∞
−∞
dq e−2Nq2 
RN d y exp

−i
2 xϵ
N

i=1
y2
i + iq
N

i=1
y2
i

=
 ∞
−∞
dq e−2Nq2 	
R
dy exp
	
−1
2ϵy2 −i
1
2 x −q

y2


N
, (16.8)
where the y-integral is convergent as ϵ > 0. Writing X N = exp

NLogX

, we have
Z(ann)(x) ∝
 ∞
−∞
dq exp
⎡
⎢⎢⎢⎣−N

2q2 −1
2Log

2π
ϵ + i(x −2q)

 
!"
#
ϕx(q)
⎤
⎥⎥⎥⎦.
(16.9)
This integral lends itself to a nice Laplace’s approximation, from which
Z(ann)(x) ≈exp(−Nϕx(q⋆)) .
(16.10)

16.1 Wigner’s Semicircle for GOE: Annealed Calculation
111
The stationary point q⋆is computed as
ϕ′
x(q⋆) = 0 ⇒4q⋆+
1
2q⋆−xϵ
= 0 ⇒q⋆= 1
4

xϵ ±
%
x2ϵ −2

,
(16.11)
where again xϵ = x −iϵ.
Applying now the Edwards-Jones formula—in the annealed version and for
N →∞
ρ(x) = −2
πN lim
ε→0+ Im ∂
∂x Log Z(ann)(x) ≈−2
πN lim
ε→0+ Im ∂
∂x

−Nϕx(q⋆)

. (16.12)
Using now the chain rule
∂
∂x ϕx(q⋆) = q⋆′ ∂
∂q ϕx(q)
&&&
q=q⋆
 
!"
#
=0
+∂xϕx(q)
&&&
q=q⋆=
1
2xϵ −4q⋆,
(16.13)
and substituting q⋆with (16.11), we obtain
ρ(x) = 2
π lim
ε→0+ Im
1
xϵ ∓

x2ϵ −2
= 1
π lim
ε→0+ Im
	
xϵ ±
%
x2ϵ −2

,
(16.14)
after rationalizing the denominator.
Next, we use again the following short lemma. If √a + ib = p + qi, then
p =
1
√
2
%
a2 + b2 + a ,
q = sign b
√
2
%
a2 + b2 −a .
(16.15)
Using this with a = x2 −ϵ2 −2 and b = −2ϵx, and choosing the sign in order
to get a physical solution, we obtain
ρ(x) = 1
π
1
√
2

|x2 −2| −x2 + 2 ,
(16.16)
which is indeed zero outside [−
√
2,
√
2] and equal to Wigner’s semicircle ρ(x) =
1
π
√
2 −x2 inside, as it should.
In the next section, we embark in the tougher task of using Edwards-Jones in
the correct (quenched) version (without shortcuts). This will require the use of the
celebrated replica trick.

112
16
Replicas for GOE
16.2
Wigner’s Semicircle: Quenched Calculation
We use now Edwards-Jones in the full-ﬂedged form
ρ(x) = −2
πN lim
ϵ→0+ Im ∂
∂x
'
Log

RN d y exp
	
−i
2 yT (xϵ1 −H) y

 (
,
(16.17)
where the average ⟨·⟩is taken again with respect to ρ[H], i.e. ⟨·⟩=

d H11 · · · d HN N
ρ[H](·) and xϵ = x −iϵ.
Recall that we cannot perform the y-integral before taking the average over H,
otherwise we would be running the Edwards-Jones formula backwards! On the other
hand, we cannot exchange the two integrations as they stand, due to the logarithm
standing right in the middle. How to proceed then?
Using the replica identity in the form
⟨Log Z(x)⟩= lim
n→0
1
n Log⟨Z(x)n⟩,
(16.18)
we replicate the y-integral n (integer) times, and we blindly hope that the analytical
continuation to n in the vicinity of zero makes sense. The formalism and notation
we shall use in the following are similar to those introduced ﬁrst in [1].
Using again
ρ[H] =
N

i=1

exp

−N H 2
ii/2

/

2π/N
 
i< j

exp

−N H 2
i j

/

π/N

,
(16.19)
we want to compute the replicated partition function
⟨Z(x)n⟩=
 ⎛
⎝
i≤j
d Hi j
⎞
⎠
N

i=1
e−N H 2
ii/2
√2π/N

i< j
e−N H 2
i j
√π/N ×
×

RNn
 n
a=1
d ya

exp
⎡
⎣−i
2
N

i, j=1
n

a=1
yia

xϵδi j −Hi j

y ja
⎤
⎦.
(16.20)
Now that the innermost integral has been “replicated” n-times, we can exchange
the order of integrations to get
⟨Z(x)n⟩=

RNn
 n

a=1
d ya

e−i xϵ
2
N
i=1
n
a=1 y2
ia
  N

i=1
d Hii
√2π/N

e−N N
i=1 H2
ii /2+ i
2
N
i=1 Hii
n
a=1 y2
ia ×
×
 ⎛
⎝
i< j
d Hi j
√π/N
⎞
⎠e−N N
i< j H2
i j +i 
i< j
n
a=1 yia Hi j y ja .
(16.21)

16.2 Wigner’s Semicircle: Quenched Calculation
113
Neglecting constants, we can perform the two multiple Gaussian integrals involv-
ing H using (16.7) repeatedly, with α = N/2 (or N) and γ = (1/2) n
a=1 y2
ia (or
γ = n
a=1 yiay ja) to get
⟨Z(x)n⟩=

RNn
 n
a=1
d ya

exp
⎡
⎣−i xϵ
2
N

i=1
n

a=1
y2
ia −1
8N
N

i=1

a
y2
ia
2
−1
4N

i< j
 n

a=1
yiay ja
2⎤
⎦,
(16.22)
which can be more compactly rewritten as
⟨Z(x)n⟩=

RNn
 n

a=1
d ya

exp
⎡
⎣−i xϵ
2
N

i=1
n

a=1
y2
ia −1
8N
N

i, j=1
 n

a=1
yia y ja
2⎤
⎦.
(16.23)
In order to proceed further, we introduce the following normalized density
μ(−→y ) = 1
N
N

i=1
n
a=1
δ(ya −yia) ,
(16.24)
where the n-dimensional vector −→y = (y1, . . . , yn).
You can now check by direct substitution that the second term in the exponential
in (16.23) can be rewritten as
−1
8N
N

i, j=1
 n

a=1
yiay ja
2
= −N
8

d−→y d−→
w μ(−→y )μ(−→
w )
 n

a=1
yawa
2
,
(16.25)
where d−→y = n
a=1 dya.
We can enforce the deﬁnition (16.24) using the following functional-integral rep-
resentation of the identity
1 =

DμD ˆμ exp
⎡
⎣−i

d−→y ˆμ(−→y )
⎛
⎝Nμ(−→y ) −

i

a
δ(ya −yia)
⎞
⎠
⎤
⎦,
(16.26)
which leads to

114
16
Replicas for GOE
⟨Z(x)n⟩=

DμD ˆμ exp
⎡
⎣−iN

d−→y μ(−→y )ˆμ(−→y ) −N
8

d−→y d−→
w μ(−→y ) μ(−→
w )
 n

a=1
yawa
2⎤
⎦×
×

RNn
 n

a=1
d ya

exp

−i xϵ
2
N

i=1
n

a=1
y2
ia + i

i

d−→y ˆμ(−→y )

a
δ(ya −yia)

.
(16.27)
In the above equations, DμD ˆμ denotes again functional integration, which was
already used in Chap.4. If you want to know more on this, see [2].
The multiple integral

RNn
n
a=1 d ya

(· · · ) is just a collection of N-identical
copies of a single integral, hence

RNn
 n
a=1
d ya

exp

−i xϵ
2
N

i=1
n

a=1
y2
ia + i

i

d−→y ˆμ(−→y )

a
δ(ya −yia)

=
-
Rn d−→y exp

−i xϵ
2
n

a=1
y2
a + i

d−→y ˆμ(−→y )

a
δ(ya −y1a)
.N
=
-
Rn d−→y exp

−i xϵ
2
n

a=1
y2
a + i ˆμ(−→y )
.N
,
(16.28)
where in the last line we used the n delta functions to kill the multiple integral.
Exponentiating the last line of (16.28), we can eventually write
⟨Z(x)n⟩=

DμD ˆμ exp
/
NSn[μ, ˆμ; x]
0
,
(16.29)
where the action is given by
Sn[μ, ˆμ; x] = −i

d−→y μ(−→y ) ˆμ(−→y ) −1
8

d−→y d−→
w μ(−→y )μ(−→
w )
 n

a=1
yawa
2
+ Log

Rn d−→y exp

−i xϵ
2
n

a=1
y2
a + i ˆμ(−→y )

.
(16.30)
The expression (16.29) lends itself to a nice saddle-point evaluation for N →∞.
The only catch is that in doing so we would reverse the right order of limits: instead
of taking n →0 ﬁrst, and N →∞afterwards, we are going to do the opposite! This
procedure is not mathematically justiﬁed, but we will proceed as if it were.
16.2.1
Critical Points
Finding the critical points of this action yields the two equations

16.2 Wigner’s Semicircle: Quenched Calculation
115
δS
δμ = 0 ⇒−i ˆμ⋆(−→y ) = 1
4

d−→
w μ⋆(−→
w )
 n

a=1
yawa
2
,
(16.31)
δS
δ ˆμ = 0 ⇒μ⋆(−→y ) =
exp

−i xϵ
2
n
a=1 y2
a + i ˆμ⋆(−→y )


Rn d−→y ′ exp

−i xϵ
2
n
a=1 y′2
a + i ˆμ⋆(−→y ′)
 .
(16.32)
Inserting (16.32) into (16.31), we get
−i ˆμ⋆(−→y ) =
1
4

d−→
w exp

−i xϵ
2
n
a=1 w2
a + i ˆμ⋆(−→
w )
 −→y · −→
w
2

d−→
w exp

−i xϵ
2
n
a=1 w2a + i ˆμ⋆(−→
w )

,
(16.33)
where both integrals on the r.h.s. run over Rn.
In order to proceed, we have to make assumptions on the behavior of μ⋆and ˆμ⋆
upon permutation of replica indices. There is a good body of research—although
not yet a formal proof—pointing to the exactness of the replica-symmetric high-
temperature solution, i.e. the one preserving permutation-symmetry among replicas,
and rotational symmetry in the space of replicas.
This simply means that we should look for a solution of (16.31) and (16.32) in
the form μ⋆(−→y ) = μ⋆(y), with y = |−→y |, and similarly for ˆμ⋆.
Introducing n-dimensional spherical coordinates, we can rewrite (16.33) under
the replica-symmetric assumption as
−iˆμ⋆(y) =
y2
4
 ∞
0
dω ωn−1 exp[−i
2 xϵω2 + iˆμ⋆(ω)]ω2  π
0 dφ (sin φ)n−2 (cos φ)2
 ∞
0
dω ωn−1 exp[−i
2 xϵω2 + iˆμ⋆(ω)]
 π
0 dφ (sin φ)n−2
,
(16.34)
where φ is taken as the angle between −→y and −→
w , and the other angular integrals
cancel out between numerator and denominator.
Performing the remaining angular integrals, and after an integration by parts in
the denominator, we get
i ˆμ⋆(y) =
(n/2)n
2(1 + n/2)
y2
4
 ∞
0 dω ωn+1G(ω)
 ∞
0 dω ωnG′(ω) ,
(16.35)
where G(ω) := exp[−i
2 xϵω2 + i ˆμ⋆(ω)]. In the replica limit n →0, we obtain
i ˆμ⋆(y) = y2
4
 ∞
0 dω ωG(ω)
 ∞
0 dω G′(ω) = C(x)y2 ,
(16.36)
where C(x) can be determined self-consistently using

116
16
Replicas for GOE
 ∞
0
dω ωG(ω)
 ∞
0
dω G′(ω) =
 ∞
0
dω ω exp

−i
2 xϵω2 + C(x)ω2
 ∞
0
dω exp

−i
2 xϵω2 + C(x)ω2

2ω

−i
2 xϵ + C(x)
 =
1
2

−i
2 xϵ + C(x)
 ,
(16.37)
so that
C(x) =
1
8

−i
2 xϵ + C(x)
 ⇒C(x) = 1
4

ixϵ ±
%
2 −x2ϵ

.
(16.38)
16.2.2
One Step Back: Summarize and Continue
Let us now pause for a second and recap what we are doing. We started from the
Edwards-Jones identity
ρ(x) = −2
πN lim
ε→0+ Im ∂
∂x
'
Log Z(x)
(
,
(16.39)
where
Z(x) =

RN d y exp
	
−i
2 yT (xϵ1 −H) y

,
(16.40)
and xϵ = x −iϵ.
The average of the logarithm is performed by using the replica identity
⟨Log Z(x)⟩= lim
n→0
1
n Log⟨Z(x)n⟩,
(16.41)
which in turn (for large N) can be approximated via a saddle-point evaluation from
(16.29) as
⟨Z(x)n⟩=

DμD ˆμ exp
/
NSn[μ, ˆμ; x]
0
∼exp

NSn[μ⋆, ˆμ⋆; x]

.
(16.42)
Combining (16.39), (16.41) and (16.42), we obtain
ρ(x) = −2
π
lim
ϵ→0+ Im lim
n→0
1
n
∂
∂x Sn[μ⋆, ˆμ⋆; x] .
(16.43)
The derivative with respect to x only acts over the last term in the action (16.30),
because x appears explicitly (not through μ⋆or ˆμ⋆) only there, and the action is sta-
tionary at the saddle point. Taking the derivative and writing the integral in spherical
n-dimensional coordinates, we obtain

16.2 Wigner’s Semicircle: Quenched Calculation
117
ρ(x) = −2
π
lim
ϵ→0+ Im lim
n→0
1
n
−i
2
 ∞
0 dy yn+1 exp

−i xϵ
2 y2 + C(x)y2
 ∞
0 dy yn−1 exp

−i xϵ
2 y2 + C(x)y2
.
(16.44)
Performing the integrals and simplifying
ρ(x) = 1
π lim
ϵ→0+ Re
1
−2C(x) + ixϵ
.
(16.45)
Recalling that C(x) = 1
4
1
ixϵ ±

2 −x2ϵ
2
and xϵ = x −iϵ, we can ﬁrst extract
the real and imaginary part of C(x) using the lemma in (16.15). Therefore we can
write
C(x) = Pϵ(x) + iQϵ(x) ,
(16.46)
with
Pϵ(x) =
1
√
2
%
2 −x2 + ϵ2 +

(2 −x2 + ϵ2)2 + (2ϵx)2
(16.47)
Qϵ(x) = sign(2ϵx)
√
2
%
(2 −x2 + ϵ2)2 + (2ϵx)2 −(2 −x2 + ϵ2) .
(16.48)
Hence
Re
1
−2C(x) + ixϵ
=
−2Pϵ(x)
4Pϵ(x)2 + (x −2Qϵ(x))2 .
(16.49)
In the limit ϵ →0+ and for −
√
2 < x <
√
2, Pϵ and Qϵ converge to
P0(x) = ±
√
2 −x2
4
(16.50)
Q0(x) = x
4 ,
(16.51)
from which
ρ(x) = 1
π

2 −x2 ,
(16.52)
i.e. Wigner’s semicircle law as expected.
References
1. G.J. Rodgers, A.J. Bray, Phys. Rev. B 37, 3557 (1988)
2. R. MacKenzie, Path Integral Methods and applications (2000), https://arxiv.org/abs/quant-ph/
0004090

Chapter 17
Born to Be Free
We have so far dealt with the spectral properties of individual random matrix ensem-
bles. You may have been wondering (or not) what happens when you sum or multiply
random matrices belonging to different ensembles. In this Chapter we present an
overview of the rather complicated tool you will need to tackle this problem: free
probability theory [1, 2].
17.1
Things About Probability You Probably
Already Know
Two random variables X1 and X2, with pdfs ρ1 and ρ2, are said to be statistically
independent when the combined random variable (X1, X2) has a factorized jpdf of
the form
ρ1,2(x1, x2) = ρ1(x1)ρ2(x2) .
(17.1)
Statistical independence means that averages factorize as well (⟨X1X2⟩=
⟨X1⟩⟨X2⟩), which in turn means that their covariance is zero, and is key to ﬁnding
the distribution of the sum of random variables. Let us consider a random variable
X with pdf ρ(x). Its characteristic function ϕ(t) is deﬁned as
ϕ(t) = ⟨eit X⟩=

dx ρ(x) eitx ,
(17.2)
i.e. it is the Fourier transform of its pdf.
You should easily realize that the factorized jpdf in Eq.(17.1) implies that char-
acteristic functions are multiplicative upon the addition of statistically indepen-
dent random variables, i.e. ϕ1,2(t1, t2) = ϕ1(t1)ϕ2(t2). Even more simply, we can
© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_17
119

120
17
Born to Be Free
introduce the logarithm of the characteristic function h(t) = log ϕ(t), the so called
cumulant generating function, which is obviously additive upon the addition of ran-
dom variables:
h1,2(t1, t2) = h1(t1) + h2(t2) .
(17.3)
Therefore, the problem of ﬁnding the pdf of the sum of two independent random
variables X1 and X2 reduces to a simple “algorithm”: compute the characteristic
functions of X1 and X2 from their pdfs, form the the cumulant generating function of
the sum X1+X2 via the additive law (17.3), compute the corresponding characteristic
function via exponentiation, and eventually compute the pdf of the sum X1 + X2 via
inverse Fourier transform.
17.2
Freeness
So, is there a generalization of statistical independence that will allow us to compute
the eigenvalue spectrum of sums of random matrices? At ﬁrst it might be tempting
to guess that the statistical independence of two scalar random variables could be
straightforwardly generalized to the case of two random matrices X1 and X2 by
merely requiring the mutual independence of all entries. Unfortunately, this is not the
case,asindependententriesarenotenoughtodestroyallpossibleangularcorrelations
between the eigenbases of two matrices.
Thepropertythat generalizes statistical independencetorandommatrices is that of
freeness. The theory of free probability was initiated a few years ago by the pioneering
works by Voiculescu and Speicher as an abstract approach to Von Neumann algebras,
and only later it was shown to have a concrete realization in terms of random matrices.
Here is how freeness works. Let us consider two N × N random matrices X1 and
X2, and let us introduce the following operator
τ(X) = lim
N→∞
1
N Tr(X) .
(17.4)
Thetwomatrices X1 and X2 aresaidtobefreeifforallintegersn1, m1, n2, m2, . . . ≥
1 we have
τ

Xn1
1 −τ

Xn1
1
 
Xm1
2 −τ

Xm1
2
 
Xn2
1 −τ

Xn2
1
 
Xm2
2
−τ

Xm2
2

. . .

=
τ

Xn1
2 −τ

Xn1
2
 
Xm1
1 −τ

Xm1
1
 
Xn2
2 −τ

Xn2
2
 
Xm2
1
−τ

Xm2
1

. . .

=
0 .
(17.5)
Not so straightforward, is it?

17.2 Freeness
121
It might help to put the above deﬁnition into words. Two random matrices are free
if the traces of all non-commutative products of matrix polynomials, whose traces
are zero, are zero. Still not very intuitive, right? Well, unfortunately it does not get
much better than that, but some intuition can be gained by exploring some concrete
examples of the above deﬁnition. For example, Eq.(17.5) reduces to τ(X1X2) =
τ(X1)τ(X2) when n1 = m1, and it reduces to τ(X2
1 X2
2) = τ(X2
1)τ(X2
2) when n1 =
m1 = 2. As you should quickly realize, these equations generalize the moment
factorization rules for statistically independent variables, and you can verify that all
such relations for higher order moments can be obtained from Eq.(17.5).
However, the interesting part comes into play when we explore cases in which
matrix non-commutativity kicks in. For example, you can easily work out the fol-
lowing result from (17.5) for n1 = n2 = m1 = m2 = 1:
τ(X1X2X1X2) = τ 2(X1)τ(X2
2) + τ(X2
1)τ 2(X2) −τ(X2
1)τ(X2
2) .
(17.6)
This result has no counterpart in “conventional” probability theory. Hopefully,
this will convince you that freeness essentially represents a generalization of moment
factorization.
17.3
Free Addition
Let us now put freeness to work.
Suppose we want to compute the average spectral density of the sum of large (i.e.
N →∞) random matrices belonging to two different ensembles.
The ﬁrst ingredient we need is our old friend the resolvent, which we introduced
in Chap.8. Now, given the resolvent G(av)
∞(z) of a given ensemble, let us introduce
its functional inverse B(z):
G(av)
∞(B(z)) = B

G(av)
∞(z)

= z .
(17.7)
The above function is known as the Blue function [3]. In case you are wondering:
yes, it is called Blue because it is the inverse of the Green’s function.
The last ingredient we need is the so called R-transform. Blue functions usually
display a singular behavior at the origin, and the R-transform is just deﬁned as a
Blue function minus its singular part:
R(z) = B(z) −1
z .
(17.8)
We are all set now. Let us consider random matrices X1 and X2 belonging to
ensembles characterized by resolvents Gav
∞,1(z) and Gav
∞,2(z), respectively. Let us
form, through Eqs. (17.7) and (17.8), the corresponding R-transforms R1 and R2.

122
17
Born to Be Free
The R-transform of the sum X = X1 + X2 is then simply given by the sum of the
two R-transforms:
R(z) = R1(z) + R2(z) .
(17.9)
The above addition rule is the free counterpart of (17.3) for the moment generating
functions of statistically independent random variables. Just like in that case, this rule
provides a simple addition “algorithm” for free random matrices, whose ﬁrst part
has been outlined above. Once the R-transform of the sum has been computed, the
corresponding Blue function and resolvent can be obtained through Eqs.(17.8) and
(17.7). Once that has been done, the eigenvalue density can be derived from the
resolvent via Eq.(8.8).
17.4
Do It Yourself
Enough with theory now: let us see free calculus at work on a concrete example.
All we need is the spectral density of large hermitian random matrix ensembles.
So, how about the eigenvalue density of the free sum of some of our usual suspects?
For example, let us consider a mixture of a GOE matrix H and a Wishart matrix W
S = pH + (1 −p)W ,
(17.10)
where p ∈[0, 1].
For both ensembles we already have computed the resolvents (Eqs.(8.20) and
(14.9) with K = 1/α). The functional inverse of those functions yield the Blue func-
tions via Eq.(17.7), and the R-transforms are immediately obtained via Eq.(17.8).
Please verify that they are given by the following functions for the GOE and Wishart
ensembles respectively:
RGOE(z) = z
2
RW(z) = α + 1
1 −z .
(17.11)
Using the R-transform’s scaling property RcH(z) = cRH(cz) (see the box below),
we can adapt the addition rule (17.9) to the present problem as follows:
RS(z) = p RGOE(pz) + (1 −p)RW ((1 −p)z) .
(17.12)
Plugging the functions in (17.11) into the equation above gives
RS(z) = p2
2 z + (1 −p)(1 −α)
1 −(1 −p)z
,
(17.13)

17.4 Do It Yourself
123
-1
-0.5
0
0.5
1
1.5
2
2.5
3
3.5
4
x
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
ρ(x)
p = 0.3
p = 0.3
p = 0.5
p = 0.5
p = 0.7
p = 0.7
Fig. 17.1 Numerical check of the density obtained for the free addition of GOE and Wishart
random matrices from the solution of Eq.(17.14). The examples shown refer to different values of
the parameter p that quantiﬁes the relative weight between the two ensembles (see Eq.(17.10))
and the corresponding resolvent is obtained as BS(GS(z)) = z, where BS(z) =
RS(z) + 1/z is the Blue function. The equation for the resolvent reads
z = p2
s GS(z) +
(1 −p)(1 −α)
1 −(1 −p)GS(z) +
1
GS(z) .
(17.14)
This is a third degree equation yielding, in general, one real solution and two
complex conjugate ones for a given ﬁxed z. The relationship between the eigenvalue
density and the resolvent is the one in Eq. (8.8), and that informs us that we will need
to select the solution with a positive imaginary part. All this is done, and numerically
veriﬁed, in the code [♠GOE_Wishart_Sum.m]. An example of the output that
can be obtained is shown in Fig.17.1.
Our goal in this Chapter was just to provide you with a short overview of the
powerful tools free probability has to offer. There are plenty of papers out there if
you’d like to know more. For example, you might have a look at the nice review
article in [4], which also details some of the many applications that free random
matrices have in quantitative ﬁnance.

124
17
Born to Be Free
Question 17.1 Where does the scaling property of the R-transform come from?
▶It is inherited from the scaling properties of our good old friend the resolvent.
Indeed, multiplying a matrix H by a constant c rescales the eigenvalues by
the same factor c. Hence, from Eqs.(8.4) and (8.5) it is easy to prove that
the two corresponding resolvents are related to each other through this simple
relationship: G(av)
∞,cH = G(av)
∞,H(z/c)/c. We can then write the equation for the
Blue function BcH
z = G(av)
∞,cH(BcH(z)) = 1
c G(av)
∞,H
1
c BcH(z)

,
(17.15)
which shows that BcH(z) = cBH(cz). We then have the following for the cor-
responding R functions:
cRH(cz) = cBH(cz) −1
z = BcH(z) −1
z = RcH(z) .
(17.16)
Question 17.2 We know that the sum of two Gaussian scalar random variables is
again Gaussian distributed. Is there an equivalent statement for the free addition
of Gaussian random matrices?
▶Given the tools provided in this Chapter you should be able to show that
the semicircle distribution is stable under free addition, i.e. if you free sum M
matrices each having the semicircle as spectral density, you still end up with a
matrix whose spectral density is a semicircle.
References
1. D.V. Voiculescu, J. Oper. Theory 18, 223 (1987)
2. A. Nica, R. Speicher, Duke Math. J. 92, 553 (1998)
3. A. Zee, Nucl. Phys. B 474, 726 (1996)
4. Z. Burda, A. Jarosz, M.A. Nowak, J. Jurkiewicz, G. Papp, I. Zahed, Quant. Fin. 11, 1103 (2011)


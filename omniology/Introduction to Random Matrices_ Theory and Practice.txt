123
SPRINGER BRIEFS IN MATHEMATICAL PHYSICS 26
GiacomoÂ Livan
MarcelÂ Novaes
PierpaoloÂ Vivo
Introduction to 
Random Matrices
 Theory and 
Practice 

SpringerBriefs in Mathematical Physics
Volume 26
Series editors
NathanaÃ«l Berestycki, Cambridge, UK
Mihalis Dafermos, Princeton, USA
Tohru Eguchi, Tokyo, Japan
Atsuo Kuniba, Tokyo, Japan
Matilde Marcolli, Pasadena, USA
Bruno Nachtergaele, Davis, USA

SpringerBriefs are characterized in general by their size (50â€“125 pages) and fast
production time (2â€“3 months compared to 6 months for a monograph).
Briefs are available in print but are intended as a primarily electronic publication to
be included in Springerâ€™s e-book package.
Typical works might include:
â€¢ An extended survey of a ï¬eld
â€¢ A link between new research papers published in journal articles
â€¢ A presentation of core concepts that doctoral students must understand in order
to make independent contributions
â€¢ Lecture notes making a specialist topic accessible for non-specialist readers.
SpringerBriefs in Mathematical Physics showcase, in a compact format, topics of
current relevance in the ï¬eld of mathematical physics. Published titles will
encompass all areas of theoretical and mathematical physics. This series is intended
for mathematicians, physicists, and other scientists, as well as doctoral students in
related areas.
More information about this series at http://www.springer.com/series/11953

Giacomo Livan
â€¢ Marcel Novaes
Pierpaolo Vivo
Introduction to Random
Matrices
Theory and Practice
123

Giacomo Livan
Department of Computer Science
University College London
London
UK
Marcel Novaes
Instituto de FÃ­sica
Universidade Federal de UberlÃ¢ndia
UberlÃ¢ndia, Minas Gerais
Brazil
Pierpaolo Vivo
Department of Mathematics
Kingâ€™s College London
London
UK
ISSN 2197-1757
ISSN 2197-1765
(electronic)
SpringerBriefs in Mathematical Physics
ISBN 978-3-319-70883-6
ISBN 978-3-319-70885-0
(eBook)
https://doi.org/10.1007/978-3-319-70885-0
Library of Congress Control Number: 2017958623
Â© The Author(s) 2018
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciï¬cally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microï¬lms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciï¬c statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made. The publisher remains neutral with regard to
jurisdictional claims in published maps and institutional afï¬liations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
This is a book for absolute beginners. If you have heard about random matrix
theory, commonly denoted RMT, but you do not know what that is, then welcome!
this is the place for you. Our aim is to provide a truly accessible introductory
account of RMT for physicists and mathematicians at the beginning of their
research career. We tried to write the sort of text we would have loved to read when
we were beginning Ph.D. students ourselves.
Our book is structured with light and short chapters, and the style is informal.
The calculations we found most instructive are spelled out in full. Particular
attention is paid to the numerical veriï¬cation of most analytical results. The reader
will ï¬nd the symbol [â™ test.m] next to every calculation/procedure for which a
numerical veriï¬cation is provided in the associated ï¬le test.m located at https://
github.com/RMT-TheoryAndPractice/RMT.
We
strongly
believe
that
theory
without practice is of very little use: In this respect, our book differs from most
available textbooks on this subject (not so many, after all).
Almost every chapter contains question boxes, where we try to anticipate and
minimize possible points of confusion. Also, we include To know more sections at
the end of most chapters, where we collect curiosities, material for extra readings
and little gemsâ€”carefully (and arbitrarily!) cherry-picked from the gigantic liter-
ature on RMT out there.
Our book covers standard materialâ€”classical ensembles, orthogonal polynomial
techniques, spectral densities and spacingsâ€”but also more advanced and modern
topicsâ€”replica approach and free probabilityâ€”that are not normally included in
elementary accounts on RMT.
Due to space limitations, we have deliberately left out ensembles with complex
eigenvalues and many other interesting topics. Our book is not encyclopedic, nor is
it meant as a surrogate or a summary of other excellent existing books. What we are
sure about is that any seriously interested reader, who is willing to dedicate some
of their time to read and understand this book till the end, will next be able to read
and understand any other source (articles, books, reviews, tutorials) on RMT,
without feeling overwhelmed or put off by incomprehensible jargon and endless
series of â€œIt can be trivially shown thatâ€¦â€.
v

So, what is a random matrix? Well, it is just a matrix whose elements are random
variables. No big deal. So why all the fuss about it? Because they are extremely
useful! Just think in how many ways random variables are useful: If someone
throws a thousand (fair) coins, you can make a rather conï¬dent prediction that the
number of tails will not be too far from 500. Ok, maybe this is not really that useful,
but it shows that sometimes it is far more efï¬cient to forego detailed analysis of
individual situations and turn to statistical descriptions.
This is what statistical mechanics does, after all: It abandons the deterministic
(predictive) laws of mechanics and replaces them with a probability distribution on
the space of possible microscopic states of your systems, from which detailed
statistical predictions at large scales can be made.
This is what RMT is about, but instead of replacing deterministic numbers with
random numbers, it replaces deterministic matrices with random matrices. Anytime
you need a matrix which is too complicated to study, you can try replacing it with a
random matrix and calculate averages (and other statistical properties).
A number of possible applications come immediately to mind. For example, the
Hamiltonian of a quantum system, such as a heavy nucleus, is a (complicated)
matrix. This was indeed one of the ï¬rst applications of RMT, developed by Wigner.
Rotations are matrices; the metric of a manifold is a matrix; the S-matrix describing
the scattering of waves is a matrix; ï¬nancial data can be arranged in matrices;
matrices are everywhere. In fact, there are many other applications, some rather
surprising, which do not come immediately to mind but which have proved very
fruitful.
We do not provide a detailed historical account of how RMT developed, nor do
we dwell too much on speciï¬c applications. The emphasis is on concepts, com-
putations, tricks of the trade: all you needed to know (but were afraid to ask) to start
a hopefully long and satisfactory career as a researcher in this ï¬eld.
It is a pleasure to thank here all the people who have somehow contributed to our
knowledge of RMT. We would like to mention in particular Gernot Akemann,
Giulio Biroli, Eugene Bogomolny, ZdzisÅ‚aw Burda, Giovanni Cicuta, Fabio D.
Cunden, Paolo Facchi, Davide Facoetti, Giuseppe Florio, Yan V. Fyodorov, Olivier
Giraud, Claude Godreche, Eytan Katzav, Jon Keating, Reimer KÃ¼hn, Satya N.
Majumdar, Anna Maltsev, Ricardo Marino, Francesco Mezzadri, Maciej Nowak,
Yasser Roudi, Dmitry Savin, Antonello Scardicchio, Gregory Schehr, Nick Simm,
Peter Sollich, Christophe Texier, Pierfrancesco Urbani, Dario Villamaina, and
many others.
This book is dedicated to the fond memory of Oriol Bohigas.
London, UK
Giacomo Livan
UberlÃ¢ndia, Brazil
Marcel Novaes
London, UK
Pierpaolo Vivo
vi
Preface

Contents
1
Getting Started . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
One-Pager on Random Variables . . . . . . . . . . . . . . . . . . . . . . .
3
2
Value the Eigenvalue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2.1
Appetizer: Wignerâ€™s Surmise . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2.2
Eigenvalues as Correlated Random Variables . . . . . . . . . . . . . .
9
2.3
Compare with the Spacings Between i.i.d.â€™s . . . . . . . . . . . . . . .
9
2.4
Jpdf of Eigenvalues of Gaussian Matrices . . . . . . . . . . . . . . . .
11
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
3
Classiï¬ed Material . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
3.1
Count on Dirac . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
3.2
Laymanâ€™s Classiï¬cation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
3.3
To Know More... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
4
The Fluid Semicircle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
4.1
Coulomb Gas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
4.2
Do It Yourself (Before Lunch) . . . . . . . . . . . . . . . . . . . . . . . .
25
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
5
Saddle-Point-of-View . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
5.1
Saddle-Point. Whatâ€™s the Point? . . . . . . . . . . . . . . . . . . . . . . . .
33
5.2
Disintegrate the Integral Equation . . . . . . . . . . . . . . . . . . . . . .
35
5.3
Better Weak Than Nothing . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
5.4
Smart Tricks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
5.5
The Final Touch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
5.6
Epilogue. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
5.7
To Know More... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
vii

6
Time for a Change . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
6.1
Intermezzo: A Simpler Change of Variables . . . . . . . . . . . . . . .
45
6.2
...that Is the Question . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
6.3
Keep Your Volume Under Control . . . . . . . . . . . . . . . . . . . . .
46
6.4
For Doubting Thomases... . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
6.5
Jpdf of Eigenvalues and Eigenvectors . . . . . . . . . . . . . . . . . . .
48
6.6
Leave the Eigenvalues Alone. . . . . . . . . . . . . . . . . . . . . . . . . .
49
6.7
For Invariant Models... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
6.8
The Proof . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
7
Meet Vandermonde . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
7.1
The Vandermonde Determinant . . . . . . . . . . . . . . . . . . . . . . . .
53
7.2
Do It Yourself . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
8
Resolve(nt) the Semicircle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
8.1
A Bit of Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
8.2
Averaging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
58
8.3
Do It Yourself . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
8.4
Localize the Resolvent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
8.5
To Know More... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
9
One Pager on Eigenvectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
66
10
Finite N . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
10.1
b Â¼ 2 is Easier . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
10.2
Integrating Inwards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
10.3
Do It Yourself . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
10.4
Recovering the Semicircle . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
11
Meet AndrÃ©ief . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
75
11.1
Some Integrals Involving Determinants . . . . . . . . . . . . . . . . . .
75
11.2
Do It Yourself . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
11.3
To Know More... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
78
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
12
Finite N Is Not Finished . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
12.1
b Â¼ 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
12.2
b Â¼ 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
13
Classical Ensembles: Wishart-Laguerre . . . . . . . . . . . . . . . . . . . . .
89
13.1
Wishart-Laguerre Ensemble . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
viii
Contents

13.2
Jpdf of Entries: Matrix Deltas... . . . . . . . . . . . . . . . . . . . . . . . .
91
13.3
...and Matrix Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
13.4
To Know More... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
94
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
14
Meet MarÄenko and Pastur. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
97
14.1
The MarÄenko-Pastur Density . . . . . . . . . . . . . . . . . . . . . . . . .
97
14.2
Do It Yourself: The Resolvent Method . . . . . . . . . . . . . . . . . .
98
14.3
Correlations in the Real World and a Quick Example:
Financial Correlations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
101
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
103
15
Replicas... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
105
15.1
Meet Edwards and Jones . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
105
15.2
The Proof . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
106
15.3
Averaging the Logarithm. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
107
15.4
Quenched versus Annealed . . . . . . . . . . . . . . . . . . . . . . . . . . .
107
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
108
16
Replicas for GOE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
109
16.1
Wignerâ€™s Semicircle for GOE: Annealed Calculation . . . . . . . .
109
16.2
Wignerâ€™s Semicircle: Quenched Calculation . . . . . . . . . . . . . . .
112
16.2.1
Critical Points . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
114
16.2.2
One Step Back: Summarize and Continue . . . . . . . . . .
116
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
117
17
Born to Be Free . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
119
17.1
Things About Probability You Probably Already Know . . . . . .
119
17.2
Freeness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
120
17.3
Free Addition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
121
17.4
Do It Yourself . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
122
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
124
Contents
ix

Chapter 1
Getting Started
Let us start with a quick warm-up. We now produce a N Ã— N matrix H whose entries
are independently sampled from a Gaussian probability density function (pdf)1 with
mean 0 and variance 1. One such matrix for N = 6 might look like this:
H =
â›
âœâœâœâœâœâœâ
1.2448
0.0561 âˆ’0.8778 1.1058 1.1759
0.7339
âˆ’0.1854 0.7819 âˆ’1.3124 0.8786 0.3965 âˆ’0.3138
âˆ’0.4925 âˆ’0.6234 0.0307 0.8448 âˆ’0.2629 0.7013
0.1933 âˆ’1.5660 2.3387 0.4320 âˆ’0.0535 0.2294
âˆ’1.0143 âˆ’0.7578 0.3923 0.3935 âˆ’0.4883 âˆ’2.7609
âˆ’1.8839 0.4546 âˆ’0.4495 0.0972 âˆ’2.6562 1.3405
â
âŸâŸâŸâŸâŸâŸâ 
.
(1.1)
Some of the entries are positive, some are negative, none is very far from 0. There
is no symmetry in the matrix at this stage, Hi j Ì¸= Hji.
Any time we try, we end up with a different matrix: we call all these matrices
samples or instances of our ensemble. The N eigenvalues are in general complex
numbers (try to compute them for H!).
To get real eigenvalues, the ï¬rst thing to do is to symmetrize our matrix. Recall
that a real symmetric matrix has N real eigenvalues. We will not deal much with
ensembles with complex eigenvalues in this book.2
Try the following symmetrization Hs = (H + H T )/2, where (Â·)T denotes the
transpose of the matrix. Now the symmetric sample Hs looks like this:
1You may already want to give up on this book. Alternatively, you can brush up your knowledge
about random variables in Sect.1.1.
2...but we will deal a lot with matrices with complex entries (and real eigenvalues).
Â© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_1
1

2
1
Getting Started
Hs =
â›
âœâœâœâœâœâœâ
1.2448 âˆ’0.0646 âˆ’0.6852 0.6496
0.0807 âˆ’0.5750
âˆ’0.0646 0.7819 âˆ’0.9679 âˆ’0.3436 âˆ’0.1806 0.0704
âˆ’0.6852 âˆ’0.9679 0.0307
1.5917
0.0647
0.1258
0.6496 âˆ’0.3436 1.5917
0.4320
0.1700
0.1633
0.0807 âˆ’0.1806 0.0647
0.1700 âˆ’0.4883 âˆ’2.7085
âˆ’0.5750 0.0704
0.1258
0.1633 âˆ’2.7085 1.3405
â
âŸâŸâŸâŸâŸâŸâ 
,
(1.2)
whose six eigenvalues are now all real
{âˆ’2.49316, âˆ’1.7534, 0.33069, 1.44593, 2.38231, 3.42944} .
(1.3)
Congratulations! You have produced your ï¬rst random matrix drawn from the
so-called GOE (Gaussian Orthogonal Ensemble)... a classicâ€”more on this name
later.
You can now do several things: for example, you can make the entries complex
or quaternionic instead of real. In order to have real eigenvalues, the corresponding
matrices need to be hermitian and self-dual respectively3â€”better have a look at one
example of the former, for N as small as N = 2
Hher =

0.3252
0.3077 + 0.2803i
0.3077 âˆ’0.2803i
âˆ’1.7115
	
.
(1.4)
You have just met the Gaussian Unitary (GUE) and Gaussian Symplectic (GSE)
ensembles, respectivelyâ€”and are surely already wondering who invented these
names.
We will deal with this jargon later. Just remember: the Gaussian Orthogonal
Ensembledoesnot containorthogonalmatricesâ€”butrealsymmetricmatricesinstead
(and similarly for the others).
Although single instances can sometimes be also useful, exploring the statistical
properties of an ensemble typically requires collecting data from multiple samples.
We can indeed now generate T such matrices, collect the N (real) eigenvalues for
each of them, and then produce a normalized histogram of the full set of N Ã— T
eigenvalues. With the code [â™ Gaussian_Ensembles_Density.m], you may
get a plot like Fig.1.1 for T = 50000 and N = 8.
Roughly half of the eigenvalues collected in total are positive, and half negativeâ€”
this is evident from the symmetry of the histograms. These histograms are concen-
trated (signiï¬cantly nonzero) over the region of the real axis enclosed by (for N = 8)
â€¢ Â±
âˆš
2N â‰ˆÂ±4 (GOE),
â€¢ Â±
âˆš
4N â‰ˆÂ±5.65 (GUE),
â€¢ Â±
âˆš
8N â‰ˆ8 (GSE).
3Hermitian matrices have real elements on the diagonal, and complex conjugate off-diagonal
entries. Quaternion self-dual matrices are 2N Ã— 2N constructed as A=[X Y; -conj(Y) conj(X)];
A=(A+Aâ€™)/2, where X and Y are complex matrices, while conj denotes complex conjugation of all
entries.

1 Getting Started
3
-10
-5
0
5
10
0
0.04
0.08
0.12
0.16
GOE
GUE
GSE
Fig. 1.1 Histograms of GOE, GUE and GSE eigenvalues (N = 8 and T = 50000 samples)
You can directly jump to the end of Chap.5 to see what these histograms look like
for big matrices.
Question 1.1 Can I compute analytically the shape of these histograms? And
what happens if N becomes very large?
â–¶Yes, you can. In Chaps.10 and 12, we will set up a formalism to compute
exactly these shapes for any ï¬nite N. In Chap.5, instead, we will see that for
large N the histograms approach a limiting shape, called Wignerâ€™s semicircle
law.
1.1
One-Pager on Random Variables
Attributed to Giancarlo Rota is the statement that a random variable X is neither
random, nor is a variable.4
Whatever it is, it can take values in a discrete alphabet (like the outcome of tossing
a die, {1, 2, 3, 4, 5, 6}) or on an interval Ïƒ (possibly unbounded) of the real line. For
the latter case, we say that Ï(x) is the probability density function5 (pdf) of X if

 b
a dxÏ(x) is the probability that X takes value in the interval (a, b) âŠ†Ïƒ.
4In the following we may use both upper and lower case to denote a random variable.
5For example, for the GOE matrix (1.2) the diagonal entries were sampled from the Gaussian (or
normal) pdf Ï(x) = exp(âˆ’x2/2)/
âˆš
2Ï€. We will denote the normal pdf with mean Î¼ and variance
Ïƒ 2 as N(Î¼, Ïƒ 2) in the following.

4
1
Getting Started
A die will not blow up and disintegrate in the air. One of the six numbers will even-
tually come up. So the sum of probabilities of the outcomes should be 1 (=100%).
People call this property normalization, which for continuous variables just means

Ïƒ dxÏ(x) = 1.
All this in theory.
In practice, sample your random variable many times and produce a normalized
histogram of the outcomes. The pdf Ï(x) is nothing but the histogram proï¬le as the
number of samples gets sufï¬ciently large. The average of X is âŸ¨XâŸ©=

dxÏ(x)x
and higher moments are deï¬ned as âŸ¨XnâŸ©=

dxÏ(x)xn. The variance is Var(X) =
âŸ¨X2âŸ©âˆ’(âŸ¨XâŸ©)2, which is a measure of how broadly spread around the mean the pdf is.
The cumulative distribution function F(x) is the probability that X is smaller or
equal to x, F(x) =

 x
âˆ’âˆdy Ï(y). Clearly, F(x) â†’0 as x â†’âˆ’âˆand F(x) â†’1
as x â†’+âˆ.
If we have two (continuous) random variables X1 and X2, they must be
described by a joint probability density function (jpdf) Ï(x1, x2). Then, the quan-
tity

 b
a dx1

 d
c dx2Ï(x1, x2) gives the probability that the ï¬rst variable X1 is in the
interval (a, b) and the other X2 is, simultaneously, in the interval (c, d).
Whenthejpdfisfactorized,i.e.istheproductoftwodensityfunctions,Ï(x1, x2) =
Ï1(x1)Ï2(x2), the variables are said to be independent, otherwise they are dependent.
When, in addition, we also have Ï1(x) = Ï2(x), the random variables are called i.i.d.
(independent and identically distributed). In any case, Ï(x1) =

Ï(x1, x2)dx2 is the
marginal pdf of X1 when considered independently of X2.
The above discussion can be generalized to an arbitrary number N of random
variables. Given the jpdf Ï(x1, . . . , xN), the quantity Ï(x1, . . . , xN)dx1 Â· Â· Â· dxN is
the probability that we ï¬nd the ï¬rst variable in the interval [x1, x1 +dx1], the second
in the interval [x2, x2 + dx2], etc. The marginal pdf Ï(x) that the ï¬rst variable will
be in the interval [x, x + dx] (ignoring the others) can be computed as
Ï(x) =

dx2 Â· Â· Â·

dxNÏ(x, x2, . . . , xN).
(1.5)
Question 1.2 What is the jpdf Ï[H] of the N 2 entries {H11, . . . , HN N} of the
matrix H in (1.1)?
â–¶The entries in H are independent Gaussian variables, hence the jpdf is fac-
torized as Ï[H] â‰¡Ï(H11, . . . , HN N) = N
i, j=1

exp

âˆ’H 2
i j/2

/
âˆš
2Ï€

.
If a set of random variables is a function of another one, xi = xi(y), there is a
relation between the jpdf of the two sets
Ï(x1, . . . , xN)dx1 Â· Â· Â· dxN = Ï(x1(y), . . . , xN(y))|J(x â†’y)|



Ï(y1,...,yN )
dy1 Â· Â· Â· dyN , (1.6)

1.1 One-Pager on Random Variables
5
where J is the Jacobian of the transformation, given by J(x â†’y) = det

âˆ‚xi
âˆ‚y j

. We
will use this property in Chap.6.
Question 1.3 What is the jpdf of the N(N âˆ’1)/2 entries in the upper triangle
of the symmetric matrix Hs in (1.2)?
â–¶For Hs, you need to consider the diagonal and the off-diagonal entries sepa-
rately: the diagonal entries are (Hs)ii = Hii, while the off-diagonal entries are
(Hs)i j = (Hi j + Hji)/2. As a result,
Ï((Hs)11, . . . , (Hs)N N ) =
N

i=1

exp

âˆ’(Hs)2
ii/2

/
âˆš
2Ï€
 
i< j

exp

âˆ’(Hs)2
i j

/âˆšÏ€

,
(1.7)
i.e. the variance of off-diagonal entries is 1/2 of the variance of diagonal entries.
Make sure you understand why this is the case. This factor 2 has very important
consequences (see Question 3.3). From now on, for a real symmetric Hs we
will denote the jpdf of the N(N âˆ’1)/2 entries in the upper triangle by Ï[H]â€”
dropping the subscript â€˜sâ€™ when there is no risk of confusion.

Chapter 2
Value the Eigenvalue
In this chapter, we start discussing the eigenvalues of random matrices.
2.1
Appetizer: Wignerâ€™s Surmise
Consider a 2 Ã— 2 GOE matrix Hs =
 x1 x3
x3 x2

, with x1, x2 âˆ¼N(0, 1) and
x3 âˆ¼N(0, 1/2). What is the pdf p(s) of the spacing s = Î»2 âˆ’Î»1 between its
two eigenvalues (Î»2 > Î»1)?
The two eigenvalues are random variables, given in terms of the entries by the
roots of the characteristic polynomial
Î»2 âˆ’Tr(Hs)Î» + det(Hs) ,
(2.1)
therefore Î»1,2 =

x1 + x2 Â±

(x1 âˆ’x2)2 + 4x2
3

/2 and s =

(x1 âˆ’x2)2 + 4x2
3.
By deï¬nition, we have
p(s) =
 âˆ
âˆ’âˆ
dx1dx2dx3
eâˆ’1
2 x2
1
âˆš
2Ï€
eâˆ’1
2 x2
2
âˆš
2Ï€
eâˆ’x2
3
âˆšÏ€ Î´

s âˆ’

(x1 âˆ’x2)2 + 4x2
3

.
(2.2)
Changing variables as
â§
âªâ¨
âªâ©
x1 âˆ’x2
= r cos Î¸
2x3
= r sin Î¸
x1 + x2
= Ïˆ
â‡’
â§
âªâ¨
âªâ©
x1
= r cos Î¸+Ïˆ
2
x2
= Ïˆâˆ’r cos Î¸
2
,
x3
= r sin Î¸
2
(2.3)
Â© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_2
7

8
2
Value the Eigenvalue
and computing the corresponding Jacobian
J = det
â›
âœâ
âˆ‚x1
âˆ‚r
âˆ‚x1
âˆ‚Î¸
âˆ‚x1
âˆ‚Ïˆ
âˆ‚x2
âˆ‚r
âˆ‚x2
âˆ‚Î¸
âˆ‚x2
âˆ‚Ïˆ
âˆ‚x3
âˆ‚r
âˆ‚x3
âˆ‚Î¸
âˆ‚x3
âˆ‚Ïˆ
â
âŸâ = det
â›
â
cos Î¸/2 âˆ’r sin Î¸/2 1/2
âˆ’cos Î¸/2 r sin Î¸/2 1/2
sin Î¸/2
r cos Î¸/2
0
â
â = âˆ’r/4 ,
(2.4)
one obtains
p(s) =
1
8Ï€3/2
 âˆ
0
dr rÎ´(s âˆ’r)
 2Ï€
0
dÎ¸
 âˆ
âˆ’âˆ
dÏˆe
âˆ’1
2

(
r cos Î¸+Ïˆ
2
)
2+(
âˆ’r cos Î¸+Ïˆ
2
)
2+ r2 sin2 Î¸
2

=
âˆš
4Ï€ s
8Ï€3/2
 2Ï€
0
dÎ¸e
âˆ’1
2

s2 cos2 Î¸
2
+ s2 sin2 Î¸
2

= s
2eâˆ’s2/4 .
(2.5)
Note that we used cos2 Î¸ +sin2 Î¸ = 1 to achieve this very simple result: however, we
could only enjoy this massive simpliï¬cation because the variance of the off-diagonal
elements was 1/2 of the variance of diagonal elementsâ€”try to redo the calcula-
tion assuming a different ratio. Observe also that this pdf is correctly normalized,
 âˆ
0 ds p(s) = 1.
It is often convenient to rescale this pdf and deï¬ne Â¯p(s) = âŸ¨sâŸ©p (âŸ¨sâŸ©s), where
âŸ¨sâŸ©=
 âˆ
0 dsp(s)s is the mean level spacing. Upon this rescaling,
 âˆ
0
Â¯p(s)ds =
 âˆ
0 s Â¯p(s)ds = 1. For the GOE as above, show that Â¯p(s) = (Ï€s/2) exp(âˆ’Ï€s2/4),
which is called Wignerâ€™s surmise,1 whose plot is shown in Fig.2.1.
Fig. 2.1 Plot of Wignerâ€™s
surmise.
0
1
2
3
4
s
0
0.2
0.4
0.6
0.8
p(s)
1Why is it deï¬ned a â€˜surmiseâ€™? After all, it is the result of an exact calculation! The story goes
as follows: at a conference on Neutron Physics by Time-of-Flight, held at the Oak Ridge National
Laboratory in 1956, people asked a question about the possible shape of the distribution of the

2.1 Appetizer: Wignerâ€™s Surmise
9
In spite of its simplicity, this is actually a quite deep result: it tells us that the
probability of sampling two eigenvalues â€˜very closeâ€™ to each other (s â†’0) is very
small: it is as if each eigenvalue â€˜feltâ€™ the presence of the other and tried to avoid it
(but not too much)! A bit like birds perching on an electric wire, or parked cars on a
street: not too close, not too far apart. If this metaphor does not win you over, check
this out [1].
2.2
Eigenvalues as Correlated Random Variables
In the previous chapter, we met the N real eigenvalues {x1, . . . , xN} of a ran-
dom matrix H. These eigenvalues are random variables described by a jpdf2
Ï(x1, . . . , xN).
Question 2.1 What does the jpdf of eigenvalues Ï(x1, . . . , xN) of a random
matrix ensemble look like?
â–¶We will give it in Eq.(2.15) for the Gaussian ensemble. Not for every ensemble
the jpdf of eigenvalues is known.
The important (generic) feature is that the {xi}â€™s are not independent: their jpdf
does not in general factorize. The most striking incarnation of this property is the so-
called level repulsion (as in Wignerâ€™s surmise): the eigenvalues of random matrices
generically repel each other, while independent variables do notâ€”as we show in the
following section.
2.3
Compare with the Spacings Between i.i.d.â€™s
It is useful at this stage to consider the statistics of gaps between adjacent i.i.d.
random variables. In this case, we will not see any repulsion.
Consider i.i.d. real random variables {X1, . . . , X N} drawn from a parent pdf pX(x)
deï¬ned over a support Ïƒ. The corresponding cdf is F(x). The labelling is purely
conventional, and we do not assume that the variables are sorted in any order.
We wish to compute the conditional probability density function pN(s|X j = x)
that, given that one of the random variables X j takes a value around x, there is another
spacings of energy levels in a heavy nucleus. E. P. Wigner, who was in the audience, walked up to
the blackboard and guessed (=surmised) the answer given above.
2We will use the same symbol Ï for both the jpdf of the entries in the upper triangle and of the
eigenvalues.

10
2
Value the Eigenvalue
random variable Xk (k Ì¸= j) around the position x + s, and no other variables lie in
between. In other word, a gap of size s exists between two random variables, one of
which sits around x.
The claim is
pN(s|X j = x) = pX(x + s) [1 + F(x) âˆ’F(x + s)]Nâˆ’2 .
(2.6)
The reasoning goes as follows: one of the variables sits around x already, so we
have N âˆ’1 variables left to play with. One of these should sit around x + s, and the
pdf for this event is pX(x + s). The remaining N âˆ’2 variables need to sit either to
the left of xâ€”and this happens with probability F(x)â€”or to the right of x + sâ€”and
this happens with probability 1 âˆ’F(x + s).
Now, the probability of a gap s between two adjacent particles, conditioned on
the position x of one variable, but irrespective of which variable this is is obtained
by the law of total probability
pN(s|any X = x) =
N

j=1
pN(s|X j = x)Prob(X j = x) = NpN(s|X j = x)pX(x) ,
(2.7)
where one uses the fact that the variables are i.i.d. and thus the probability that the
particle X j lies around x is the same for every particle, and given by pX(x).
To obtain the probability of a gap s between any two adjacent random variables,
no longer conditioned on the position of one of the variables, we should simply
integrate over x
pN(s) =

Ïƒ
dx pN(s|any X = x) = N

Ïƒ
dx pN(s|X j = x)pX(x) .
(2.8)
As an exercise, let us verify that pN(s) is correctly normalized, namely
 âˆ
0 ds
pN(s) = 1. We have
 âˆ
0
ds pN(s) = N
 âˆ
0
ds

Ïƒ
dx pX(x + s) [1 + F(x) âˆ’F(x + s)]Nâˆ’2 pX(x) .
(2.9)
Changing variables F(x + s) = u in the s-integral, and using F(+âˆ) = 1 and
du = Fâ€²(x + s)ds = pX(x + s)ds, we get
 âˆ
0
ds pN(s) = N

Ïƒ
dx pX(x)
 1
F(x)
du[1 + F(x) âˆ’u]Nâˆ’2



1âˆ’F(x)Nâˆ’1
Nâˆ’1
.
(2.10)

2.3 Compare with the Spacings Between i.i.d.â€™s
11
Setting now F(x) = v and using dv = Fâ€²(x)dx = pX(x)dx, we have
 âˆ
0
ds pN(s) =
N
N âˆ’1
 1
0
dv(1 âˆ’vNâˆ’1) = 1 ,
(2.11)
as required.
As there are N variables, it makes sense to perform the â€˜localâ€™ change of variables
s = Ë†s/(NpX(x)) and consider the limit N â†’âˆ. The reason for choosing the scaling
factor NpX(x) is that their typical spacing around the point x will be precisely of
order âˆ¼1/(NpX(x)): increasing N, more and more variables need to occupy roughly
the same space, therefore their typical spacing goes down. The same happens locally
around points x where there is a higher chance to ï¬nd variables, i.e. for a higher
pX(x).
We thus have
pN

s =
Ë†s
NpX(x)
X j = x

= pX

x + Ë†s/NpX(x)
 
1 + F(x) âˆ’F

x + Ë†s/NpX(x)
Nâˆ’2 ,
(2.12)
which for large N and Ë†s âˆ¼O(1), can be approximated as
pN

s =
Ë†s
NpX(x)
X j = x

â‰ˆpX(x)eâˆ’Ë†s ,
(2.13)
therefore using (2.8)
lim
Nâ†’âˆË†pN(Ë†s) := lim
Nâ†’âˆpN

s =
Ë†s
NpX(x)
 ds
d Ë†s = N Ã— 1
N

Ïƒ
dxpX(x)eâˆ’Ë†s = eâˆ’Ë†s ,
(2.14)
the exponential law for the spacing of a Poisson process. From this, one deduces
easily that i.i.d. variables do not repel, but rather attract: the probability of vanishing
gaps, Ë†s â†’0, does not vanish, as in the case of RMT eigenvalues!
2.4
Jpdf of Eigenvalues of Gaussian Matrices
The jpdf of eigenvalues of a N Ã— N Gaussian matrix is given by3
Ï(x1, . . . , xN) =
1
ZN,Î²
eâˆ’1
2
N
i=1 x2
i 
j<k
|x j âˆ’xk|Î² ,
(2.15)
3This jpdf goes back to the prehistory of RMT. It is an immediate consequence of Theorem 2 in
[2], a 1939 statistics paper published in the journal Annals of Eugenics (a rather scary title, isnâ€™t
it?). In its full glory, it appeared explicitly for the ï¬rst time in [3].

12
2
Value the Eigenvalue
where
ZN,Î² = (2Ï€)N/2
N

j=1
Î“ (1 + jÎ²/2)
Î“ (1 + Î²/2)
(2.16)
is a normalization constant,4 enforcing

RN dx Ï(x1, . . . , xN) = 1, and Î² = 1, 2, 4
is called the Dyson index.5 Henceforth, dx = N
j=1 dx j. Note that the eigenvalues
are considered to be unordered here.
Thisjpdfcorrespondsexactly toeigenvalues6 generatedaccordingtothealgorithm
in Chap.1,7 and provided in the code [â™ Gaussian_Ensembles_Density.m].
Where does (2.15) come from? Let us postpone the proof for a while and draw
some conclusions by just staring at it for a few minutes.
The Gaussian factor eâˆ’1
2
N
i=1 x2
i kills any conï¬guration of eigenvalues {x} where
some x jâ€™s are â€œbigâ€ (far from zero, in absolute value): the eigenvalues do not like
to stay too far from the origin. On the other hand, the term 
j<k |x j âˆ’xk| kills
conï¬gurations where two eigenvalues get â€œtoo closeâ€ to each other.
The â€œrepulsionâ€ factor 
j<k |x j âˆ’xk| has another effect: it makes the eigenvalues
strongly non-independent! Every eigenvalue feels the presence of all the others, and
the jpdf (2.15) does not factorize at all. Hence, the classical tools for independent
random variables are of little use here. We will use (2.15) in the next Chapter to
deduce Wignerâ€™s semicircle law in a few simple steps.
This interplay between conï¬nement and repulsion is the physical mechanism at
the heart of many results in RMT.
As a ï¬nal remark, go back to the spacing pdf in Eq. (2.5), which was obtained
for N = 2 and Î² = 1 (a 2 Ã— 2 GOE matrix). Armed with (2.15) one may redo the
calculation as
p(s) =
 âˆ
âˆ’âˆ
dx1dx2Ï(x1, x2)Î´(s âˆ’|x2 âˆ’x1|) .
(2.17)
Try to compute this integral, and recover Eq. (2.5).
4It can be computed via the so-called Mehtaâ€™s integral, a close relative of the celebrated Selbergâ€™s
integral [4].
5The Dyson index is equal to the number of real variables needed to specify one entry of your matrix:
1 for real, 2 for complex and 4 for quaternions. This is usually referred to as Dysonâ€™s threefold way.
For the Gaussian ensemble, then, GOE corresponds to Î² = 1, GUE to Î² = 2 and GSE to Î² = 4.
6For Î² = 4, each matrix has 2N eigenvalues that are two-fold degenerate.
7Quite often, however, you ï¬nd in the literature a Gaussian weight including extra factors, such
as exp(âˆ’(Î²/2) 
i x2
i ) or exp(âˆ’(N/2) 
i x2
i ). One then needs to be very careful when compar-
ing theoretical results (obtained with such conventions) to numerical simulationsâ€”in particular, a
rescaling of the numerical eigenvalues by âˆšÎ² or
âˆš
N before histogramming is essential in these
two modiï¬ed scenarios.

References
13
References
1. P. Å eba, J. Stat. Mech. L10002 (2009)
2. P.L. Hsu, Ann. Hum. Genet. 9, 250 (1939)
3. C.E. Porter, N. Rosenzweig, Ann. Acad. Sci. Fennicae, Serie A VI Physica 44 (1960), reprinted
in C.E. Porter, Statistical Theories of Spectra: Fluctuations (Academic Press, New York, 1965)
4. P.J. Forrester, S.O. Warnaar, Bull. Amer. Math. Soc. (N.S) 45, 489 (2008)

Chapter 3
Classiï¬ed Material
In this Chapter, we continue setting up the formalism and provide a simple
classiï¬cation of matrix models.
3.1
Count on Dirac
Question 3.1 From the jpdf of eigenvalues Ï(x1, . . . , xN), how do I compute
the shape of the histograms of the N Ã— T eigenvalues as in Fig.1.1, for T
sufï¬ciently large?
â–¶To cut a long story short, all you have to do is to take the marginal
Ï(x) =

Â· Â· Â·

dx2 Â· Â· Â· dxNÏ(x, x2, . . . , xN) ,
(3.1)
and this function will reproduce the histogram proï¬le you are after for any ï¬nite
N. Note that Ï(x) is correctly normalized to 1, as your histogram is.
Let us prove (3.1).
Take a single, ï¬xed matrix H with real eigenvaluesâ€”no randomness in hereâ€”and
perform the following task: deï¬ne a counting function n(x) such that
 b
a n(xâ€²)dxâ€²
gives the fraction of eigenvalues xi between a and b.
Â© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_3
15

16
3
Classiï¬ed Material
The way to deï¬ne it is to set1
n(x) = 1
N
N

i=1
Î´(x âˆ’xi) ,
(3.2)
the (normalized) sum of a set of â€œspikesâ€ at the location xi of each eigenvalue. Using
the following property of the delta function

I
dxÎ´(x âˆ’x0) f (x) = f (x0)
if x0 âˆˆI and 0 otherwise ,
(3.3)
we can show that indeed (3.2) does the job properly.2
If H is now a random matrix, the function n(x) becomes a random measure on
the real lineâ€”a function of x that changes from one realization of H to another. The
average of it over the set of random eigenvalues {x1, . . . , xN} becomes interesting
now3
âŸ¨n(x)âŸ©:=

Â· Â· Â·

dxÏ(x1, . . . , xN)n(x) = 1
N
N

i=1

Â· Â· Â·

dxÏ(x1, . . . , xN)Î´(xâˆ’xi) = Ï(x),
(3.5)
where Ï(x) =

Â· Â· Â·

dx2 Â· Â· Â· dxNÏ(x, x2, . . . , xN) is the marginal density of Ï. Try
to prove the last equality in (3.5) using the properties of delta function, and the fact
that Ï(x1, . . . , xN) is symmetric upon the exchange xi â†’x j. This is indeed the case
for the Gaussian jpdf (2.15) and will remain generally true.
The quantity âŸ¨n(x)âŸ©= Ï(x) has many names: most often, it is called the (average)
spectral density. Figure3.1 helps you visualize how T = 4 sets of N = 8 randomly
located â€œspikesâ€ conspire to produce the continuous shape Ï(x) = âŸ¨n(x)âŸ©.
1As we know, the Dirac delta function (or rather distribution) Î´(x) is basically an extremely peaked
function at the point x = 0, like the limit of a Gaussian pdf as its variance goes to zero, Î´(x) =
limÎµâ†’0+
1
2âˆšÏ€Îµ eâˆ’x2/(4Îµ).
2Compute
N
 b
a
n(x)dx =
N

i=1
 b
a
Î´(x âˆ’xi)dx =
N

i=1
Ï‡[a,b](xi) ,
(3.4)
where the indicator function Ï‡[a,b](z) is equal to 1 if z âˆˆ(a, b) and 0 otherwise. This is by deï¬nition
the number of eigenvalues between a and b, as it should.
3We use again the shorthand dx = N
j=1 dx j.

3.1 Count on Dirac
17
Question 3.2 If N becomes very large, what does the spectral density Ï(x) for
the Gaussian ensemble look like?
â–¶For the jpdf Ï(x1, . . . , xN) given in (2.15), the precise statement for the
spectral density Ï(x) =

dx2 Â· Â· Â· dxNÏ(x, x2, . . . , xN) is
lim
Nâ†’âˆ

Î²NÏ(

Î²Nx) = ÏSC(x) ,
(3.6)
where ÏSC(x) =
1
Ï€
âˆš
2 âˆ’x2 has a semicircularâ€”or rather, semiellipticalâ€”
shape. This is called Wignerâ€™s semicircle law.
What is the meaning of the unexpected rescaling factor âˆšÎ²N?
This means that the histograms of eigenvalues for larger and larger N become
concentrated over the interval [âˆ’âˆš2Î²N, âˆš2Î²N], in agreement with our
numerical ï¬ndings in Fig.1.1. The points Â±âˆš2Î²N are called (spectral) edges.
Note that:
1. The edges are growing with
âˆš
Nâ€”bigger matrices have a wider range of
eigenvalues, can you explain why? To get histograms that do not become
wider and wider with N, we need to divide each eigenvalue by âˆšÎ²N before
histogramming. This is what we do in Fig.3.2, using the very same eigen-
values collected to produce Fig.1.1. You can see that the histograms for
different Î²s nicely collapse on top of each other, reproducing an almost
perfect semielliptical shape between âˆ’
âˆš
2 and
âˆš
2.
2. The edges are at Â±âˆš2Î²N for the jpdf Ï(x1, . . . , xN) given in (2.15). If
you put ad hoc extra factors in the exponential, like exp(âˆ’(Î²/2) 
i x2
i ) or
exp(âˆ’(N/2) 
i x2
i ), as you sometimes ï¬nd in the literature, this is tanta-
mount to rescaling the eigenvalues by an appropriate factor. For example,
for the choice exp(âˆ’(N/2) 
i x2
i ), the edges are ï¬xedâ€”they do not grow
with Nâ€”at Â±âˆš2Î².
3. The edges of the semicircle are called soft: for large but ï¬nite N, there is
always a nonzero probability of sampling eigenvalues exceeding the edge
points. For example, for a GOE matrix 10Ã—10, you have a tiny but nonzero
probability to sample eigenvalues larger than âˆš2Î²N â‰ˆ4.47.... Other
ensembles have spectral densities with hard edgesâ€”this means impene-
trable walls, which the eigenvalues can never cross.

18
3
Classiï¬ed Material
-1.5
-1
-0.5
0
0.5
1
1.5
x
M1
M2
M3
M4
0
0.2
0.4
Ï(x)
Fig. 3.1 Sets of N = 8 randomly located â€œspikesâ€. A histogram of how many spikes occur around
a given region of the real line is nothing but the average spectral density there
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
x
0
0.1
0.2
0.3
0.4
0.5
0.6
Ï(x)
 Semicircle
 GOE
 GUE
 GSE
Fig. 3.2 Rescaled densities for N = 8 (GOE, GUE, GSE)
3.2
Laymanâ€™s Classiï¬cation
We deal here with ensembles of square matrices with real eigenvalues (the entries can
be real, complex or quaternionic random variables). Can we classify these ensembles
according to simple features?
A useful scheme (covering several scenarios encountered in real life) is the fol-
lowing (see Fig.3.3):

3.2 Laymanâ€™s Classiï¬cation
19
Independent  
Entries
Rotational 
invariance
Gaussian 
Ensembles
Ï[H] = Ï[UHU âˆ’1]
Ï[H] âˆ
N
i=1 fi(Hii)
i<j fij(Hij)
Fig. 3.3 Visualization of the laymanâ€™s classiï¬cation of random matrix ensembles
1. Independent entries: the ï¬rst group on the left gathers matrix models whose
entries are independent random variablesâ€”modulo the symmetry requirements.
Random matrices of this kind are usually called Wigner matrices.
Examples: in this category, you may ï¬nd adjacency matrices of random graphs
[1], or matrices with independent power-law entries (so-called LÃ©vy matrices
[2]), and power-law banded matrices [3] among others. Take a moment to down-
load and read these papersâ€”remember the following sentence, found on Richard
Feynmanâ€™s blackboard at the time of his death: â€œKnow how to solve every prob-
lem that has been solvedâ€.
2. Rotational invariance: the second group on the right is characterized by the
so-called rotational invariance. In essence, this property means that any two
matrices that are related via a similarity transformation4 H â€² = U HU âˆ’1 occur
in the ensemble with the same probability
Ï[H]d H11 Â· Â· Â· d HN N = Ï[H â€²]d H â€²
11 Â· Â· Â· d H â€²
N N .
(3.7)
This requires the following two conditions:
â€¢ Ï[H] = Ï[U HU âˆ’1]. This means that the jpdf of the entries retains the same func-
tional form before and after the transformation. This imposes a severe constraint
on the allowable functional forms thanks to Weylâ€™s lemma [4], which states that
Ï[H] can only be a function of the traces of the ï¬rst N powers of H,
Ï[H] = Ï•

Tr H, Tr H 2, . . . , Tr H N	
.
(3.8)
4U is orthogonal/unitary/symplectic if H is real symmetric/complex hermitian/quaternion self-
dual, respectively. You surely have noticed that this is precisely the origin of the names given to the
ensembles: Orthogonal, Unitary and Symplectic.

20
3
Classiï¬ed Material
SinceTr H n = Tr (U HU âˆ’1)n bythecyclicpropertyofthetrace,theâ‡implication
is trivial.
â€¢ d H11 Â· Â· Â· d HN N = d H â€²
11 Â· Â· Â· d H â€²
N N, i.e. the ï¬‚at Lebesgue measure is invariant
under conjugation by U. This is a classical result.
The rotational invariance property in essence means that the eigenvectors are not that
important, as we can rotate our matrices as freely as we wish, and still leave their
statistical weight unchanged.
Examples: you may ï¬nd in this category the Wishart-Laguerre (Chap.13) and Jacobi
classical ensembles, the so-called â€œweakly-conï¬nedâ€ ensembles [5] and many others.
The same advice (â€œdownload-and-studyâ€) applies here.
3. What about the intersection between the two classes? It turns out that it contains
only the Gaussian ensemble5!
This is a consequence of a theorem by Porter and Rosenzweig [6]. And is bad
news, isnâ€™t it? We have to make a choice: if we insist that the ensemble has
independent entries, then eigenvectors do matter. If we require a high level of
rotational symmetry, then the entries get necessarily correlated. No free lunch
(beyond the Gaussian)!
Question 3.3 I can see that the Gaussian ensemble has independent entries. But
I do not easily see that it has this â€œrotational invarianceâ€.
â–¶This can be seen from the jpdf of entries in the upper triangle (1.7). Show
that you can rewrite this jpdf as
Ï[Hs] âˆexp

âˆ’1
2Tr(H 2
s )

,
(3.9)
where Tr(Â·) is the matrix trace (the sum of diagonal element). For example, for
the 2 Ã— 2 real symmetric matrix Hs =

a b
b c

, the trace of Hs is a + c, and the
trace of H 2
s is a2 + c2 + 2b2. You can actually rewrite (1.7) as (3.9) only thanks
to that factor 2...check this! Now, from (3.9), the rotational invariance property
is much easier to see: for a similarity transformation Hsâ€² = U HsU âˆ’1, one has
Tr(Hsâ€²2) = Tr(H 2
s ) (cyclic property of the trace).
5In its three incarnations: GOE, GUE and GSE.

3.3 To Know More...
21
3.3
To Know More...
1. Anything worth mentioning beyond the above classiï¬cation? One important
class is represented by the biorthogonal ensembles: these are non-invariant,
with non-independent entries, and yet their jpdf of eigenvalues is known in
terms of the product of two determinants. Check these papers out [7, 8] for
further information.
2. We suggest the following paper [9] about â€œhistogramming without histogram-
mingâ€. Solid maths and an insightful and unconventional perspective on RMT
spectra.
3. For a proof of the Porter-Rosenzweig theorem in the simpliï¬ed 2 Ã— 2 case, as
well as for a nice and pedagogical introduction to the Gaussian ensembles, we
highly recommend the review [10].
4. For the mathematically oriented reader, who is looking for more formal clas-
siï¬cations of random matrix models, we recommend the mini-review [11] and
references therein.
References
1. R. KÃ¼hn, J. Phys. A: Math. Theor. 41, 295002 (2008)
2. P. Cizeau, J.P. Bouchaud, Phys. Rev. E 50, 1810 (1994)
3. A.D. Mirlin, Y.V. Fyodorov, F.-M. Dittes, J. Quezada, T.H. Seligman, Phys. Rev. E 54, 3221
(1996)
4. H. Weyl, Classical Groups (Princeton Univ. Press, Princeton, 1946)
5. K.A. Muttalib, Y. Chen, M.E.H. Ismail, V.N. Nicopoulos, Phys. Rev. Lett. 71, 471 (1993)
6. C.E. Porter and N. Rosenzweig, Annals of the Acad. Sci. Fennicae, Serie A VI Physica 44, 1
(1960), reprinted in C.E. Porter, Statistical Theories of Spectra: Fluctuations (Academic Press,
New York, 1965)
7. A. Borodin, Nuclear Physics B 536, 704 (1998)
8. P. Desrosiers, P.J. Forrester, J. Approx. Theory 152, 167 (2008)
9. J.T. Albrecht, C.P. Chan, A. Edelman, Found. Comput. Math. 9, 461 (2008)
10. Y.V. Fyodorov, Introduction to the Random Matrix Theory: Gaussian Unitary Ensemble and
Beyond (2004), https://arxiv.org/pdf/math-ph/0412017.pdf
11. M. Zirnbauer, Symmetry classes in random matrix theory (2004), https://arxiv.org/pdf/math-
ph/0404058.pdf

Chapter 4
The Fluid Semicircle
In this Chapter, we set up a statistical mechanics formalism to compute Wignerâ€™s
semicircle law for Gaussian matrices. You will learn here the so-called â€œCoulomb
gas techniqueâ€.
4.1
Coulomb Gas
The Coulomb gas (or ï¬‚uid) technique is usually attributed to Dyson [1]. Actually,
a few years before, Wigner had already used it for the derivation of the semicircle
law [2].
Take the jpdf for the Gaussian ensemble (2.15)
Ï(x1, . . . , xN) =
1
ZN,Î²
eâˆ’1
2
N
i=1 x2
i 
j<k
|x j âˆ’xk|Î² ,
(4.1)
and rescale the eigenvalues as xi â†’xi
âˆšÎ²N.
The normalization constant now reads (set CN,Î² = (âˆšÎ²N)N+Î²N(Nâˆ’1)/2)
ZN,Î² = CN,Î²

RN
N

j=1
dx j eâˆ’Î²
2 N N
i=1 x2
i 
j<k
|x j âˆ’xk|Î² = CN,Î²

RN
N

j=1
dx j eâˆ’Î²N 2V[x] ,
(4.2)
where the energy term in the exponent is
V[x] =
1
2N

i
x2
i âˆ’
1
2N 2

iÌ¸= j
ln |xi âˆ’x j| .
(4.3)
Â© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_4
23

24
4
The Fluid Semicircle
-1.5
-1
-0.5
0
0.5
1
1.5
x
Conï¬ning well potential
Fig. 4.1 Sketch of the quadratic conï¬ning potential, which prevents the particles from escaping
towards Â±âˆ
The factor 1/2 in front of the logarithmic term is due to the symmetrization from
i < j to i Ì¸= j.
Stare at (4.2) intensely.
We have just exponentiated the product 
j<k, and obtained a canonical partition
function1!
The Gibbs-Boltzmann weight eâˆ’Î²N 2V[x] corresponds to a thermodynamical ï¬‚uid
of particles with positions {x1, . . . , xN} on a line, in equilibrium at â€œinverse tem-
peratureâ€ Î² under the effect of competing interactions: a quadratic (single-particle)
potential (see Fig.4.1), and a repulsive (all-to-all) logarithmic term. The ï¬‚uid is
â€œstaticâ€, as there is no kinetic term in V[x].
The presence of the pre-factor Î²N 2 showsâ€”at least formallyâ€”that the limit
N â†’âˆis a simultaneous thermodynamic and zero-temperature limit. A standard
thermodynamic argument tells us how to ï¬nd the equilibrium positions at zero tem-
perature of the particles (eigenvalues) under such interactions: all we need to do is
to minimize the free energy F = âˆ’(1/Î²) ln ZN,Î² of this system. The calculation
greatly simpliï¬es in the limit N â†’âˆ.
1We are integrating the Gibbs-Boltzmann weight eâˆ’Î²N2V[x] over all possible positions of the
particles.

4.1 Coulomb Gas
25
Question 4.1 Why is this called a â€œCoulombâ€ gas?
â–¶Because we have a logarithmic interaction among charged particles. More
precisely, we have a 2D â€œï¬‚uidâ€ of charges constrained to a line. We know that
in 2D the electrostatic potential generated by a point charge is proportional to
the logarithm of the distance from itâ€”while in 3D, this potential is inversely
proportional to the distance, and in 1D is proportional to the distance. Therefore,
a 2D charged ï¬‚uid conï¬ned to a line is not quite the same as a 1D ï¬‚uid!
A simple way to see this is by using Gaussâ€™s law, with a single charge q sitting
at the origin on a 2D plane. If we enclose the charge in a 1-sphere S (i.e. a circle),
then we must have

S E Â· n âˆq, where n is the normal vector to the circle. If
you assume that the electric ï¬eld E is rotationally symmetric, i.e. E = E(r)Ë†r,
this turns into E(r)2Ï€r âˆq, implying that E(r) âˆq/r. Integrating a ï¬eld that
goes like 1/r gives you a logarithmic potential.
4.2
Do It Yourself (Before Lunch)
So, our goal is to ï¬nd the free energy F = âˆ’(1/Î²) ln ZN,Î² for a large number of
particles N â†’âˆ. As in many branches of physics, â€œlarger is easierâ€.
We now provide a â€œcontinuumâ€ description of the ï¬‚uid, based on the following
steps.
1. Introduce a counting function
Deï¬ne ï¬rst a normalized one-point counting function
n(x) = 1
N
N

i=1
Î´(x âˆ’xi) .
(4.4)
This is a random function, satisfying

R dx n(x) = 1 and n(x) â‰¥0 everywhere.
For ï¬nite N, this is just a collection of â€œspikesâ€ at the location of each eigenvalue.
However, for large N, it is natural to assume that it will become a smooth function
of x. We will always work under this assumption.2
2It may be helpful to think that n(x) is nothing but the limit for Îµ â†’0+ of a nascent delta function
nÎµ(x) =
1
N
N
i=1
eâˆ’(xâˆ’xi )2/4Îµ
2âˆšÏ€Îµ
, where the limit Îµ â†’0+ is taken at the very end (after the limit
N â†’âˆ).

26
4
The Fluid Semicircle
2. Coarse-graining procedure
Instead of directly summingâ€”or rather integratingâ€”over all conï¬gurations of eigen-
values {x1, . . . , xN}, which in stat-mech we would call microstates of our ï¬‚uid, we
ï¬rst ï¬x a certain one-point proï¬le n(x) (non-negative, smooth and normalized).
Sketch your favorite function over R and call it n(x)â€”whatever you like,
really, provided it is non-negative, smooth and normalized. Then, we sum over all
microstates {x1, . . . , xN} compatible with your sketch n(x)â€”in a sense to be made
clearer. Finally, we sum over all possible (non-negative, smooth and normalized)
n(x) you might have come up with in the ï¬rst place.
This coarse-graining procedure can be put on slightly cleaner grounds introducing
the following representation of unity as a functional integral
1 =

D[n(x)]Î´

n(x) âˆ’1
N
N

i=1
Î´(x âˆ’xi)
	
,
(4.5)
which enforces the deï¬nition (4.4). The functional integral runs (so to speak) over
all possible normalized, non-negative and smooth functions n(x). See [3] for more
details on functional integrations.
Inserting this representation of unity inside the multiple integral (4.2) and
exchanging the order of integrations, we end up with
ZN,Î² = CN,Î²

D[n(x)]

RN
N

j=1
dx j eâˆ’Î²N 2V[x]Î´

n(x) âˆ’1
N
N

i=1
Î´(x âˆ’xi)
	
.
(4.6)
3. Convert sums into integrals
Using the identities3
N

i=1
f (xi) = N

R
n(x) f (x)dx
(4.7)
N

i, j=1
g(xi, x j) = N 2

R2 dxdxâ€²n(x)n(xâ€²)g(x, xâ€²) ,
(4.8)
we can rewrite the two terms in the energy (4.3) as
3Prove them inserting the deï¬nition of n(x) into the integrals and using properties of the delta
function.

4.2 Do It Yourself (Before Lunch)
27
1
2N
N

i=1
x2
i =
1
2N Ã— N

R
n(x)x2dx
(4.9)
1
2N 2

iÌ¸= j
ln |xi âˆ’x j| =
1
2N 2
â¡
â£
i, j
ln |xi âˆ’x j| âˆ’

i
ln Î”(xi)
â¤
â¦=
1
2N 2 Ã— N 2

R2 dxdxâ€²n(x)n(xâ€²) ln |x âˆ’xâ€²| âˆ’
1
2N 2 Ã— N

R
dx n(x) ln Î”(x) ,
(4.10)
where Î”(x) is a position-dependent short-distance cutoff. What does this mean?
Note that in the limit Îµ â†’0+, the double integral

R2 dxdxâ€²nÎµ(x)nÎµ(xâ€²) ln |x âˆ’
xâ€²| is divergent. This physically corresponds to the inï¬nite-energy contribution origi-
nated by two neighboring charges getting â€œtoo closeâ€ to each other (the term i = j in
the sum 
i, j ln |xi âˆ’x j|). The term

R dx nÎµ(x) ln Î”(x) for Îµ â†’0+ â€œrenormalizesâ€
the divergence and produces a ï¬nite result. More on how to plausibly ï¬x Î”(x) later.
4. V[x] â†’V[n(x)]
Note that in (4.9) and (4.10) the sums over eigenvalues {x1, . . . , xN} have been
expressed through the counting function n(x), whichâ€”with a slight abuse of
notationâ€”will denote from now on its smooth limit as N â†’âˆ.
Therefore we can write
ZN,Î² = CN,Î²

D[n(x)]eâˆ’Î²N 2V[n(x)]

RN
N

j=1
dx j Î´

n(x) âˆ’1
N
N

i=1
Î´(x âˆ’xi)
	



IN [n(x)]
.
(4.11)
The functional V[n(x)] reads
V[n(x)] = 1
2

R
dx x2n(x) âˆ’1
2

R2 dxdxâ€²n(x)n(xâ€²) ln |x âˆ’xâ€²| + 1
2N

R
dx n(x) ln Î”(x) .
(4.12)
5. Evaluate the integral IN[n(x)] for large N
We now have to evaluate
IN[n(x)] =

RN
N

j=1
dx j Î´

n(x) âˆ’1
N
N

i=1
Î´(x âˆ’xi)
	
(4.13)
in the limit N â†’âˆ.
It is quite easy to give a physical interpretation of this multiple integral. It is
basically counting how many microstatesâ€”microscopic conï¬gurations of the ï¬‚uid
chargesâ€”arecompatiblewithagivenmacrostateâ€”thedensityproï¬len(x).Weknow

28
4
The Fluid Semicircle
from standard statistical mechanics arguments that the logarithm of this number
should be proportional to the entropy of the ï¬‚uid. Let us see how.
Introducing a â€˜functionalâ€™ analogue of the standard integral representation for the
delta function [4], we can write
IN[n(x)] =

D[Ë†n(x)]

RN
N

j=1
dx j exp

iN

dx n(x)Ë†n(x) âˆ’i

dx Ë†n(x)
N

i=1
Î´(x âˆ’xi)
	
=

D[Ë†n(x)] exp

iN

dx n(x)Ë†n(x)
 
R
dy eâˆ’i

dx Ë†n(x)Î´(xâˆ’y)
N
=

D[Ë†n(x)]eN S[Ë†n(x)|n(x)] ,
(4.14)
where
S[Ë†n(x)|n(x)] = i

dx n(x)Ë†n(x) + Log

R
dy eâˆ’iË†n(y) .
(4.15)
This type of integrals is music to the statistical physicistâ€™s ears! It is of the form

d(Â·) exp[Î›f (Â·)], with Î› â‰¡N a very large parameter. Hence it can be evaluated
with a Laplace (or saddle-point) approximation [5].
Finding the critical point of the action S[Ë†n(x)|n(x)]
0 =
Î´S
Î´ Ë†n(x) = in(x) âˆ’i
eâˆ’iË†n(x)

R dy eâˆ’iË†n(y) ,
(4.16)
from which we obtain
eâˆ’iË†n(x) = n(x)

R
dy eâˆ’iË†n(y) â‡’iË†n(x) = âˆ’ln n(x) âˆ’Log

R
dy eâˆ’iË†n(y) ,
(4.17)
where we ignore spurious phases (recall that in the complex ï¬eld Log exp(z) may not
just be equal to z!) that would make the action evaluated at the saddle-point complex.
Substituting in (4.15), we obtain
IN[n(x)] âˆ¼exp

âˆ’N

dx n(x) ln n(x)

,
(4.18)
to leading order in N. As expected, the term inside square brackets has precisely the
form of the Shannon entropy of the density n(x).
6. Evaluate Î”(x)
Look back again at (4.12). The short-distance cutoff Î”(x) is yet to be ï¬xed.
A standard, physically motivated argumentâ€”going back to Dyson for charges on
a ringâ€”posits that Î”(x)â€”the so-called self-energy termâ€”should be taken of the
form
Î”(x) â‰ˆ
c
Nn(x) ,
(4.19)

4.2 Do It Yourself (Before Lunch)
29
as the higher the density of particles around x, the smaller the average distance
between them.4 Also, N charges spread over a distance of O(1) have a mean spacing
âˆ¼O(1/N), and this justiï¬es the 1/N factor. This argument, however plausible, does
not seem to have been made rigorous yet, though. Note, in particular, that the constant
c in (4.19) cannot be ï¬xed by this simple heuristic argument. While conceptually
quite important (see e.g. [6]), this missing bit will prove rather inconsequential in
the following.
7. Final expression
Combining (4.11), (4.12), (4.18) and (4.19), the partition function eventually reads
ZN,Î² â‰ƒCN,Î²

D[n(x)]eâˆ’Î²N 2F0[n(x)]+ Î²
2 N ln N+(
Î²
2 âˆ’1)NF1[n(x)]âˆ’Î²
2 N ln c+o(N) , (4.20)
where
F0[n(x)] = 1
2

dx x2n(x) âˆ’1
2

dxdxâ€²n(x)n(xâ€²) ln |x âˆ’xâ€²| ,
(4.21)
F1[n(x)] =

dx n(x) ln n(x) .
(4.22)
Note that the term (Î²/2)N ln N is essentially independent of the potential, and
can be absorbed into the overall normalization constant. The O(N) contribution is
composed by i) the self-energy term, ii) the entropic term, and iii) a contribution
coming from the unknown constant c in (4.19).
8. Flash-forward: cross-check with ï¬nite-N result
We now cheat a bit.
Let us use some information we will actually prove later, namely that the equilib-
rium density of the ï¬‚uid is Wignerâ€™s semicircle law nâ‹†(x) â‰¡ÏSC(x) = 1
Ï€
âˆš
2 âˆ’x2.
Inserting the semicircle law into (4.21) and (4.22)â€”and evaluating the corre-
sponding integralsâ€”we obtain
F0[nâ‹†(x)] = 3
8 + ln 2
4
,
(4.23)
F1[nâ‹†(x)] = 1
2(1 âˆ’ln(2) âˆ’2 ln(Ï€)) .
(4.24)
Therefore, the partition function (4.20) reads for large N
4We have already met a similar argument in Sect.2.3.

30
4
The Fluid Semicircle
ZN,Î² â‰ƒCN,Î²

D[n(x)]eâˆ’Î²N 2F0[n(x)]+ Î²
2 N ln N+(
Î²
2 âˆ’1)NF1[n(x)]âˆ’Î²
2 N ln c+o(N)
â‰ˆCN,Î²eâˆ’Î²N 2F0[nâ‹†(x)]+ Î²
2 N ln N+(
Î²
2 âˆ’1)NF1[nâ‹†(x)]âˆ’Î²
2 N ln c+o(N)
â‰ˆexp
Î²
4 N 2 ln N + aÎ² N 2 +
1
2 + Î²
4

N ln N + bÎ² N + o(N)

, (4.25)
where we used the easy asymptotics
ln CN,Î² âˆ¼Î²
4 N 2 ln N + Î²
4 (ln Î²)N 2 + 1 âˆ’Î²/2
2
N ln N + (1 âˆ’Î²/2) ln Î²
2
N . (4.26)
The constants aÎ² and bÎ² are given as follows:
aÎ² = Î²
4 ln Î² âˆ’Î²F0[nâ‹†(x)] ,
(4.27)
bÎ² =
Î²
2 âˆ’1

F1[nâ‹†(x)] + 1 âˆ’Î²/2
2
ln Î² âˆ’Î²
2 ln c .
(4.28)
Can we check that this result is plausible?
Note that for Î² = 2, the partition function ZN,Î²=2 from (2.16) has a particularly
simple expression at ï¬nite N,
ZN,Î²=2 = (2Ï€)N/2G(N + 2) ,
(4.29)
where G(x) is a Barnes G-function.5 Hence, if everything was done correctly, the
large-N asymptotics of (4.29) should precisely match the large-N behavior (4.25).
Let us check.
Using known asymptotics of the Barnes G-function, we deduce that
ln ZN,Î²=2 âˆ¼1
2 N 2 ln N âˆ’3
4 N 2 + N ln N + N (ln(2Ï€) âˆ’1) + O(1) ,
(4.30)
which coincides (up to the term N ln N included) with the asymptotics of ZN,Î² in
(4.25) once Î² is set to 2.
This check should convince you that the â€œmean-ï¬eldâ€ approachâ€”based on a
continuum description of the charged ï¬‚uid of eigenvaluesâ€”is indeed capable of
capturing the ï¬rst three terms of the free energy, and only fails at the level of O(N)
contributionsâ€”as the renormalized self-energy term cannot be precisely determined
by a simple-minded scaling argument.
9. Whatâ€™s next?
Let us recap what we have done so far. The normalization constant ZN,Î² of the Gaus-
sian model has been re-interpreted as the canonical partition function of a 2D static
5The Barnes G-function is deï¬ned via the recursion G(z + 1) = 
(z)G(z), with G(1) = 1.

4.2 Do It Yourself (Before Lunch)
31
ï¬‚uid of charged particles conï¬ned on a line, in equilibrium at inverse temperature
Î². For a large number of particles, among all possible conï¬gurations, the ï¬‚uid will
choose the one that minimizes its free energy, i.e. the logarithm of this partition
function.
The partition function has been written as a functional integral over the space
of normalized counting functions n(x), see (4.20). For large N, it lends itself to a
saddle-point evaluation, which will be carried out in the next Chapter.
References
1. F.J. Dyson, J. Math. Phys. 3, 140 (1962)
2. E. Wigner, Statistical properties of real symmetric matrices with many dimensions, in Canadian
Mathematical Congress Proceedings (University of Toronto Press, Toronto, 1957), p. 174
3. R. MacKenzie, Path Integral Methods and Applications (2000), https://arxiv.org/abs/quant-ph/
0004090
4. J. Rammer, Quantum Field Theory of Non-equilibrium States (Cambridge University Press,
2007)
5. R. Wong, Asymptotic Approximation of Integrals: Computer Science and Scientiï¬c Computing
(Academic Press, 1989)
6. E. Sandier, S. Serfaty, Ann. Probab. 43, 2026 (2015)

Chapter 5
Saddle-Point-of-View
Let us continue the study of the Coulomb gas method for large random matrices.
5.1
Saddle-Point. Whatâ€™s the Point?
Earlier we showed that the partition function for the Gaussian model could be rep-
resented as
ZN,Î² â‰ƒCN,Î²

D[n(x)]eâˆ’Î²N 2F0[n(x)]+ Î²
2 N ln N+(
Î²
2 âˆ’1)NF1[n(x)]âˆ’Î²
2 N ln c+o(N) ,
(5.1)
where
F0[n(x)] = 1
2

dx x2n(x) âˆ’1
2

dxdxâ€²n(x)n(xâ€²) ln |x âˆ’xâ€²| ,
(5.2)
F1[n(x)] =

dx n(x) ln n(x) .
(5.3)
Quite interestingly, the leading term in the exponential is of order âˆ¼O(N 2) and
not of âˆ¼O(N) as in standard short-range models. As a consequence of the all-to-all
coupling between the charged particles, the free energy per particle is dominated by
the â€œenergeticâ€ component at the expenses of the â€œentropicâ€ part (sub-leading for
large N).
Recall now that the functional integral runs over functions n(x) that are normal-
ized, i.e.

R dx n(x) = 1. We can enforce this constraint introducing another delta
function
Î´

R
dx n(x) âˆ’1

=

R
dk
2Ï€ eik(

R dx n(x)âˆ’1) .
(5.4)
Â© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_5
33

34
5
Saddle-Point-of-View
Rescaling ik â†’Î²N 2Îº and ignoring sub-leading terms, you end up with the truly
appealing representation
ZN,Î² â‰ˆCN,Î²

D[n(x)]

R
dÎº eâˆ’Î²N 2S[n(x),Îº]+O(N) ,
(5.5)
where the action is
S[n(x), Îº] = F0[n(x)] âˆ’Îº

dx n(x) âˆ’1

.
(5.6)
A saddle-point evaluation yields1
ZN,Î² â‰ˆexp(âˆ’Î²N 2S[nâ‹†(x), Îºâ‹†]) .
(5.7)
Here, nâ‹†(x) is the minimizer of the functional (5.2) in the space of normalizable
and non-negative functions n(x).
We set up the minimization problem by searching for the critical points2
â§
âªâ¨
âªâ©
0 =
Î´
Î´n(x)S[n(x), Îº]
 n=nâ‹†
Îº=Îºâ‹†= x2
2 âˆ’

R dxâ€²nâ‹†(xâ€²) ln |x âˆ’xâ€²| âˆ’Îºâ‹†,
0 =
âˆ‚
âˆ‚Îº S[n(x), Îº]
 n=nâ‹†
Îº=Îºâ‹†â‡’

R dx nâ‹†(x) = 1 ,
(5.8)
for x in the support of nâ‹†(x).
Effectively, Îºâ‹†(hereafter renamed Îº for simplicity) is just a Lagrange multiplier
enforcing the normalization

R dx nâ‹†(x) = 1.
What is then the intensive free energy
f = âˆ’(1/Î²N 2) ln ZN,Î²
(5.9)
of our Coulomb gas for N â†’âˆ? It is just given by f = S[nâ‹†(x), Îº] â‰¡F0[nâ‹†(x)]â€”
the action evaluated at the saddle-point density.
1The pre-factor CN,Î² has the large-N behavior (4.26), whose logarithm is âˆ¼O(N 2 ln N) and thus
strictly speaking leading with respect to N 2. However, it is just an overall constant term, and the
â€˜dynamicalâ€™ part of the free energy is of âˆ¼O(N 2).
2Note that the factor 1/2 in front of the double integral disappears because the functional differ-
entiation picks up two counting functions, as in the integrand we have n(x)n(xâ€²). An interesting
account on functional differentiation can be found at [1].

5.1 Saddle-Point. Whatâ€™s the Point?
35
To summarize, the main task is now to ï¬nd the solution of the integral
equation (5.8)
x2
2 âˆ’

R
dxâ€²nâ‹†(xâ€²) ln |x âˆ’xâ€²| âˆ’Îº = 0 ,
(5.10)
satisfying nâ‹†(x) â‰¥0 everywhere, and

R nâ‹†(x)dx = 1.
5.2
Disintegrate the Integral Equation
...or (in more academic terms), solve it.
As a preliminary observation, note that the support of nâ‹†(x) (i.e. the set of x-
values for which nâ‹†(x) > 0) cannot be the full real line. In the limit x â†’âˆ, the
integral term

R
dxâ€²nâ‹†(xâ€²) ln |x âˆ’xâ€²| âˆ¼ln x

R
dxâ€²nâ‹†(xâ€²) = ln x ,
(5.11)
â€”where we used normalization of the densityâ€”which is clearly incompatible with
the behavior âˆ¼x2/2 of the known term in the equation.3
Therefore, we need to look for a solution over an interval (a, b) of the real line.
Indeed, a rather amusing feature of this type of integral equationsâ€”of the Carleman
classâ€”is that the support over which the solution is to be found is itself unknown,
and part of the problem!
The solution nâ‹†â‰¡nâ‹†(x; a, b) we ï¬nd will then be a parametric function of a, b.
We will then ï¬x the â€˜optimalâ€™ a, b by requiring that the resulting free energy f in
(5.9) is minimizedâ€”i.e. any other choice of the support (a, b) for normalized and
non-negative function Ëœn(x) Ì¸= nâ‹†(x), once inserted into (5.9), would produce a larger
value for the free energy.
Let us now ï¬rst convert the integral equation into a â€œsimplerâ€ one.
5.3
Better Weak Than Nothing
The solution to the integral equation (5.10) can be obtained by ï¬rst differentiating
both sides with respect to x. Since ln |x âˆ’xâ€²| is not (strictly speaking) differentiable
at x = xâ€², we consider the derivative in the weak sense.
3This is true in general for potentials growing super-logarithmically at inï¬nityâ€”not just for the
quadratic potential corresponding to Gaussian ensembles.

36
5
Saddle-Point-of-View
Let u be a function in L1([a, b]). We say that v âˆˆL1([a, b]) is a weak derivative
of u if
 b
a
u(x)Ï•â€²(x)dx = âˆ’
 b
a
v(x)Ï•(x)dx
(5.12)
for all inï¬nitely differentiable functions Ï• with Ï•(a) = Ï•(b) = 0. The notion of
weak derivative extends the standard (strong) derivative to functions that are not
differentiable, but integrable in [a, b]. Also, if u is differentiable in the standard
sense, than its weak and strong derivatives coincideâ€”just using integration by parts.
Setting u(x) =

dxâ€²nâ‹†(xâ€²) ln |x âˆ’xâ€²|, we can write

Ï•â€²(x)

dxâ€²nâ‹†(xâ€²) ln |x âˆ’xâ€²|

dx = 1
2 lim
Îµâ†’0

Ï•â€²(x)

dxâ€²nâ‹†(xâ€²) ln[(x âˆ’xâ€²)2 + Îµ2]

dx
= âˆ’1
2

Ï•(x)

dxâ€²nâ‹†(xâ€²)
2(x âˆ’xâ€²)
(x âˆ’xâ€²)2 + Îµ2

dx = âˆ’

Ï•(x)dx

Pr

dxâ€² nâ‹†(xâ€²)
x âˆ’xâ€²

,
(5.13)
where Pr stands for Cauchyâ€™s principal value.4
Comparing with (5.12), we obtain that the weak derivative of u(x) is Pr

dxâ€² nâ‹†(xâ€²)
xâˆ’xâ€² ,
therefore the new (singular) integral equation to be solved now is
Pr

dxâ€² nâ‹†(xâ€²)
x âˆ’xâ€² = x .
(5.14)
To solve (5.14), we invoke a theorem by Tricomi [2], stating that
Pr
 b
a
dxâ€² f (xâ€²)
x âˆ’xâ€² = g(x) â‡’f (x) =
C âˆ’Pr
 b
a
dt
Ï€
âˆš(tâˆ’a)(bâˆ’t)
xâˆ’t
g(t)
Ï€âˆš(x âˆ’a)(b âˆ’x)
,
(5.15)
provided that [a, b] is a single (compact) support and C is an arbitrary constant.
Question 5.1 Who tells me that the optimal counting function nâ‹†(x) is support-
ed on a single interval [a, b]?
â–¶There is some nice physical intuition behind this. The â€œthermodynamicalâ€
interpretation of the eigenvalues implies that the gas of particles is conï¬ned by a
quadratic well with a single minimum (see Fig.4.1). It is then physically reason-
able to foresee that the particles will ï¬ll the single minimum of the potential. If
a potential has many minima, then it is possible that nâ‹†(x) â€œsplitsâ€ into as many
connected components as the number of minima of the potential. Any attempt
to use (5.15) in these multiple-support cases will produce unphysical solutions.
4This means precisely the limit limÎµâ†’0
 xâˆ’Îµ F(xâ€²)dxâ€² +

x+Îµ F(xâ€²)dxâ€²
, if x is a singular point
of F(x).

5.3 Better Weak Than Nothing
37
Evaluating the principal value integral with g(t) = t and imposing the normal-
ization
 b
a dx nâ‹†(x) = 1, we get
nâ‹†(x) =
1
Ï€âˆš(x âˆ’a)(b âˆ’x)

1 âˆ’x2 + 1
2(a + b)x + 1
8(b âˆ’a)2

.
(5.16)
Note that the density in (5.16) is a solution of the integral equation (5.14) between
a and b for any choice of a and b. How to ï¬x the â€œoptimalâ€ a and b will be the subject
of the next sections.
[Of course, do not even consider trusting us on this. You are not allowed to proceed
until you have derived (5.16) yourself. Sorry.]
5.4
Smart Tricks
Now, stare at (5.16) intensely. As promised, the function nâ‹†(x) (deï¬ned for x âˆˆ
(a, b)) indeed depends on two free parameters a and b.
We need now to compute the intensive free energy
f = F0[nâ‹†(x)] .
(5.17)
It will of course depend as well on the two free parameters a and b, which arose
as a Phoenix from the ashes of the integral equation (5.14).
A couple of smart tricks will make our life easier. First, we would really like to
get rid of the double integral in
f â‰¡F0[nâ‹†(x)] = 1
2

dx x2nâ‹†(x) âˆ’1
2

dxdxâ€²nâ‹†(x)nâ‹†(xâ€²) ln |x âˆ’xâ€²| . (5.18)
To do that, we multiply the saddle point Eq.(5.10)
x2
2 âˆ’

dxâ€²nâ‹†(xâ€²) ln |x âˆ’xâ€²| âˆ’Îº = 0
(5.19)
by nâ‹†(x) and integrate over x. This way we obtain

dxdxâ€²nâ‹†(x)nâ‹†(xâ€²) ln |x âˆ’xâ€²| = 1
2

dx nâ‹†(x)x2 âˆ’Îº ,
(5.20)
where we used

nâ‹†(x)dx = 1.

38
5
Saddle-Point-of-View
Next, we ï¬x the Lagrange multiplier Îº by setting x = a in (5.19). We obtain
Îº = a2/2 âˆ’
 b
a dx nâ‹†(x) ln(x âˆ’a). Combining everything, we get
f â‰¡F0[nâ‹†(x)] = 1
4
 b
a
dx nâ‹†(x)x2 + a2
4 âˆ’1
2
 b
a
dx nâ‹†(x) ln(x âˆ’a) .
(5.21)
No more Îº, and no more double integrals. Nice, uh?
5.5
The Final Touch
Inserting (5.16) into (5.21) and computing the integrals with the help of an abacus,5
we obtain
f â‰¡f (a, b) =
1
512

âˆ’9a4 + 4a3b + 2a2 
5b2 + 48

+ 4ab

b2 + 16

âˆ’256 ln(b âˆ’a) âˆ’9b4 + 96b2 + 512 ln(2)

.
(5.22)
We
now
have
our
(quite
ugly)
intensive
free
energy:
In
the
code
[â™ integral_check.m] we provide a simple numerical conï¬rmation that the
above result is equivalent to (5.21).
All we need to do is to minimize it with respect to a and bâ€”the (soft) edge points
of the support of nâ‹†(x).
If you do that, you will obtain the solution6 a = âˆ’
âˆš
2 and b =
âˆš
2, which imply
for nâ‹†(x) from (5.16) the following form
nâ‹†(x) â‰¡ÏSC(x) = 1
Ï€

2 âˆ’x2 ,
(5.23)
the famous Wignerâ€™s semicircle law. Very appropriate name, given that it is not the e-
quation
of
a
semicircle,
but
rather
of
a
semi-ellipse.
The
code
[â™ Tricomi_check.m] offers a numerical veriï¬cation that the semicircle indeed
solves Eq.(5.14) for a = âˆ’b = âˆ’
âˆš
2.
How to show this analytically, though?
We need to prove that
Pr
 âˆš
2
âˆ’
âˆš
2
dxâ€²
âˆš
2 âˆ’xâ€²2
Ï€(x âˆ’xâ€²) = x .
(5.24)
5It may be useful to ï¬rst change variables z = (x âˆ’a)/(b âˆ’a). The resulting integrals can then be
handled by most symbolic computation programs.
6The fact that the soft edges are symmetrically located around the origin is a consequence of the
symmetry of the conï¬ning potential under the exchange x â†’âˆ’x.

5.5 The Final Touch
39
The primitive of the integrand isâ€”ignoring an additive constant
F(y) =
âˆš
2 âˆ’x2 ln (Ï•(x, y)) âˆ’
âˆš
2 âˆ’x2 ln(x âˆ’y) + x arcsin

y
âˆš
2

âˆ’

2 âˆ’y2
Ï€
,
(5.25)
where
Ï•(x, y) =

2 âˆ’x2
2 âˆ’y2 âˆ’xy + 2 .
(5.26)
Hence all you have to show is
lim
Îµâ†’0+

F(x âˆ’Îµ) âˆ’F

âˆ’
âˆš
2

+ F
âˆš
2

âˆ’F(x + Îµ)

= x,
âˆ’
âˆš
2 â‰¤x â‰¤
âˆš
2 .
(5.27)
Have a go at it!
5.6
Epilogue
What is again the interpretation of the â€œsemicircularâ€ nâ‹†(x)? It is just the equilibrium
proï¬le of a gas of many charged particles on a line, which minimizes the free energy
of the gas. In the â€œeigenvalueâ€ language, it represents the normalized histogram of
the N eigenvalues of a single (very big!) instance of the Gaussian ensemble. The
property that this object also faithfully represents the spectrum averaged over many
samples (i.e. nâ‹†(x) = âŸ¨n(x)âŸ©= Ï(x)) is called self-averaging and we will assume it
to hold.
The code [â™ Coulomb_gas.m] provides a numerical veriï¬cation of what we
worked on in this Chapter and the previous one. It simulates the Coulomb gas through
a simple Monte Carlo procedure, which produces the equilibrium density for long
enoughtimes.Also,anumericalcheckofthesemicircledistributioncanbeperformed
directly, i.e. through the numerical diagonalization of random matrices, with the code
[â™ Gaussian_finite_N_rescaled.m] (Fig.5.1).
Note that, at the very beginning of the derivation of (5.23), we rescaled the eigen-
values by âˆšÎ²N (Eq.4.2). Therefore, in the simulations we need to perform the same
rescaling of our eigenvalues by âˆšÎ²N before comparing the histogram to the theoret-
ical semicircle. This is in agreement with the precise statement we made in Question
3.2, namely
lim
Nâ†’âˆ

Î²NÏ(

Î²Nx) = ÏSC(x) ,
(5.28)
where the function ÏSC(x) = 1
Ï€
âˆš
2 âˆ’x2 â‰¡nâ‹†(x) is Î²-independent.

40
5
Saddle-Point-of-View
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
x
0
0.1
0.2
0.3
0.4
0.5
ÏSC(x)
N = 10
N = 102
N = 103
Fig. 5.1 Numerical check of the semicircle law for GOE. Increasing the value of N, after a suitable
rescaling, the eigenvalue histograms collapse on top of the semicircle curve
As a ï¬nal remark, what happens if the conï¬ning potential is not quadratic? In
general, if our invariant ensemble is characterized by a joint probability density of
the entries of the form
Ï[H] âˆexp [âˆ’TrV (H)] ,
(5.29)
then the joint law of the eigenvalues is of the form
Ï(x1, . . . , xN) âˆexp

âˆ’
N

i=1
V (xi)
 
j<k
|x j âˆ’xk|Î²
(5.30)
and the analogue of the Tricomi equation for the spectral density is
Pr

dxâ€² nâ‹†(xâ€²)
x âˆ’xâ€² = V â€²(x) .
(5.31)
Try to solve for nâ‹†(x) in the case V (x) = x âˆ’Î± ln x (x > 0). This will correspond
to the Wishart-Laguerre ensemble of random matrices, which will be extensively
discussed in Chap.13.
Question 5.2 Do all existing random matrix ensembles have the semicircle as
their average spectral density?
â–¶Certainly not! The spectral density is highly non-universalâ€”i.e. it strongly
depends on the ensemble you consider. This said, it is true that many ensem-
bles share it as their spectral density for large N. This is the case for instance

5.6 Epilogue
41
of Wigner ensembles (non-invariant), when the distribution of entries decays
sufï¬ciently fast at inï¬nity (see [3]).
Question 5.3 What are the moments of the semicircle law?
â–¶They are given by the so called Catalan numbers. More precisely, deï¬ning
âŸ¨TrXkâŸ©=

dx1 Â· Â· Â· dxNÏ(x1, . . . , xN)
N

i=1
xk
i = N

dx xkÏ(x) ,
(5.32)
where Ï(x1, . . . , xN) is the jpdf for the Gaussian ensemble (2.15) and Ï(x) its
one-point marginal for ï¬nite N, we have the relation
lim
Nâ†’âˆ
âŸ¨TrX2nâŸ©
Î²nN n+1 = 1
Ï€
 âˆš
2
âˆ’
âˆš
2
dy y2n
2 âˆ’y2 = Cn
2n ,
(5.33)
where Cn =
1
n+1
2n
n

is the nth Catalan number. Catalan numbers occur in a
variety of combinatorial problems, for example Cn is the number of ways to
correctly match n pairs of brackets.
Question 5.4 I see that the Coulomb gas treatment is insensitive to the precise
value of Î². But is it possible to construct an explicit random matrix ensem-
ble Ï[H], whose eigenvalues are distributed according to a Coulomb gas with
Î² Ì¸= 1, 2, 4?
â–¶Yes! This has been achieved by Dumitriu and Edelman [4], who produced
ensembles of tridiagonal matricesâ€”hence non-invariantâ€”with independent but
not identically distributed nonzero entries, whose jpdf of eigenvalues can be n-
evertheless computed analytically. This jpdf turns out to be equal to the Gaussian
or Wishart-Laguerre ones, albeit with a continuous Dyson index Î² > 0 (it enters
as a parameter of the distribution of the nonzero entries). These ensembles are
very useful also on the numerical side: they provide a much faster way to sample
GXE-distributed eigenvalues (with X = O, U, S), without having to diagonalize
full Gaussian matrices!

42
5
Saddle-Point-of-View
Question 5.5 If I drop the symmetry requirements on the entries of the ensemble
(Hi j Ì¸= Hji), what is the resulting analogue of the semicircle law for complex
eigenvalues?
â–¶This is called the Girko-Ginibre (or circular) law. In essence, for any sequence
of random N Ã— N matrices whose entries are i.i.d. random variables, all with
mean zero and variance equal to 1/N, the limiting spectral density is the uniform
distribution over the unit disc in the complex plane.
5.7
To Know More...
1. The Gaussian ensemble for Î² = 2. The eigenvalues can be interpreted as the
positions of fermions in a harmonic trap. To understand this mapping, have a
look at [5] and references therein.
2. Recently, the Coulomb gas technique has been improved and modiï¬ed to tackle
a wealth of different problems. It all started with a beautiful calculation on the
following problem: what is the probability that all the eigenvalues of a Gaussian
matrix are negative? Check this paper out [6].
3. The Gaussian ensembles can also come in a variant called ï¬xed-trace: this means
that one multiplies the jpdf (4.1) by Î´
N
i=1 x2
i âˆ’t

, which ï¬xes the squared
trace to the value t (see [7, 8] for details).
4. The normalization constant ZN,Î² for the Gaussian ensemble can be computed for
ï¬nite N, with simple algebraic manipulations on the so called Selberg integral
 1
0
dx|Î”N(x)|Î²
N

i=1
xaâˆ’1
i
(1 âˆ’xi)bâˆ’1 .
(5.34)
It was computed by the norwegian mathematician A. Selberg, who showed that,
when it exists, it is given by
N

j=1
Î“ (1 + Î²j/2)Î“ (a + (N âˆ’j)Î²/2)Î“ (b + (N âˆ’j)Î²/2)
Î“ (1 + Î²/2)Î“ (a + b + Î²(2N âˆ’j âˆ’1)/2)
.
(5.35)
To know more about recent developments in the beautiful theory of Selberg
integrals, have a look at [9].

References
43
References
1. E.Engel,R.M.Dreizler,DensityFunctionalTheory:AnAdvancedCourse(Springer,Heidelberg,
2011)
2. F.G. Tricomi, Integral Equations (Dover publications, 1985)
3. L. ErdÃ¶s, Russ. Math. Surv. 66, 507 (2011)
4. I. Dumitriu, A. Edelman, J. Math. Phys. 43, 5830 (2002)
5. R. Marino, S.N. Majumdar, G. Schehr, P. Vivo, Phys. Rev. Lett. 112, 254101 (2014)
6. D.S. Dean, S.N. Majumdar, Phys. Rev. E 77, 041108 (2008)
7. G. Akemann, G.M. Cicuta, L. Molinari, G. Vernizzi, Phys. Rev. E 59, 1489 (1999)
8. G. Akemann, G.M. Cicuta, L. Molinari, G. Vernizzi, Phys. Rev. E 60, 5287 (1999)
9. P.J. Forrester, S.O. Warnaar, Bull. Am. Math. Soc. (N.S.) 45, 489 (2008)

Chapter 6
Time for a Change
In this Chapter, we show how to compute the jpdf of eigenvalues for random matrix
modelsâ€”whenever possible.
6.1
Intermezzo: A Simpler Change of Variables
Suppose we have to compute the following double integrals
I1 =

R2 dx dy Ï1(x, y)
I2 =

R2 dx dy Ï2(x, y) ,
(6.1)
with Ï1(x, y) = f (x2 + y2) and Ï2(x, y) = x f (x2 + y2). Here, f (t) is a function
of your choice that makes both integrals convergent.
A good strategy is to make the â€œpolarâ€ change of variables {x, y} = {r cos Î¸,
r sin Î¸} to write
I1 =
 âˆ
0
dr
 2Ï€
0
dÎ¸ Ë†Ï1(r, Î¸),
I2 =
 âˆ
0
dr
 2Ï€
0
dÎ¸ Ë†Ï2(r, Î¸) ,
(6.2)
where Ë†Ï1(r, Î¸) = r f (r2) and Ë†Ï2(r, Î¸) = r2 cos Î¸ f (r2). Obviously, we had to include
here the extra Jacobian factor
J(r, Î¸) =
 âˆ‚x
âˆ‚r
âˆ‚x
âˆ‚Î¸
âˆ‚y
âˆ‚r
âˆ‚y
âˆ‚Î¸

= r .
(6.3)
Therefore, we can formally write Ï1(x, y)dxdy = Ë†Ï1(r, Î¸)drdÎ¸ (and similarly
for Ï2), meaning that the two expressions give the same result once integrated over
â€œcorrespondingâ€ domains (e.g. R2 â†’(0, âˆ) Ã— (0, 2Ï€)).
This is all trivial and easy. But together with the following two remarks, it is all
you need to know to fully understand what happens in the RMT case, with jpdf of
entries and eigenvalues all over the place.
Â© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_6
45

46
6
Time for a Change
1. Ë†Ï1(r, Î¸) (the new integrand) is nothing but Ï1(r cos Î¸,r sin Î¸) Ã— |J(r, Î¸)| (the old
integrand, written in terms of the new variables, times the Jacobian factor)â€”and
similarly for Ë†Ï2.
2. The marginal Ë†Ï1(r) =
 2Ï€
0
dÎ¸ Ë†Ï1(r, Î¸) is easier to compute than the corresponding
Ë†Ï2(r). This for two reasons: i) the original Ï1(x, y), once expressed in the new
polar variables, no longer depends on one of them (Î¸), and ii) also the Jacobian
does not depend on Î¸. So the integration in Î¸ becomes trivial and gives just a
constant factor 2Ï€.
6.2
...that Is the Question
Take the case of real symmetric matrices for simplicityâ€”call them H instead of Hs
from now on.
Look again at the jpdf of eigenvalues (2.15) for the GOE ensemble (Î² = 1)
Ï(x1, . . . , xN) =
1
ZN,Î²=1
eâˆ’1
2
N
i=1 x2
i 
j<k
|x j âˆ’xk| .
(6.4)
We gave it without proof.
How to obtain it from the jpdf of entries in the upper triangle, Ï[H]
Ï[H] =
N

i=1
eâˆ’H 2
ii/2
âˆš
2Ï€

i< j
eâˆ’H 2
i j
âˆšÏ€ ?
(6.5)
In this Chapter, we provide an answer to this outstanding question.
6.3
Keep Your Volume Under Control
A real symmetric matrix can be diagonalized by an orthogonal matrix O as H =
O X OT , with X = diag(x1, . . . , xN).
Orthogonal N Ã— N matrices are characterized by the property that OOT = 1,
where 1 is the identity matrix. As a subspace of RN 2, these matrices form a sub-
manifold VN of dimension N(N âˆ’1)/2, called the Stiefel manifold. dO is precisely
its â€œvolume elementâ€- the analog of dÎ¸ in the warm-up example above.
We know that
 2Ï€
0
dÎ¸ = 2Ï€. It is perhaps intuitive to give this number 2Ï€ the
meaning of â€œvolumeâ€ occupied while dÎ¸ spans the entire one-dimensional mani-
fold (the circumference of the unit circle). What is, then, the â€œvolumeâ€occupied by
orthogonal matrices in RN 2?
A relatively simple calculation [1] shows that

6.3 Keep Your Volume Under Control
47
Vol(VN) =

VN
dO = 2NÏ€ N 2/2
N(N/2) ,
(6.6)
where
m(a) = Ï€m(mâˆ’1)/4
m

i=1
(a âˆ’(i âˆ’1)/2) .
(6.7)
We will use this result in a minute.
If we call
DO =
dO
Vol(VN) ,
(6.8)
this deï¬nes the so-called Haar measure on the orthogonal group. The Haar measure is
invariant under orthogonal conjugation, and deï¬nes a probability space on orthogonal
matrices. For further information, consult [1â€“4].
6.4
For Doubting Thomases...
Let us compute the volume Vol(V2) for 2 Ã— 2 orthogonal matrices â€œfrom ï¬rst prin-
ciplesâ€.1
Let
O =
o11 o12
o21 o22

.
(6.9)
The {oi j} are real variables. The volume we are after is
Vol(V2) =

2

i, j=1
doi jÎ´

o2
11 + o2
21 âˆ’1

Î´

o2
12 + o2
22 âˆ’1

Î´(o11o12 + o21o22) ,
(6.10)
where the delta functions enforce the constraints on the columns of O being orthog-
onal with each other, and each having unit norm.
1Alternatively, one may notice that the elements of V2 can be written either in the form

cos Î¸
sin Î¸
âˆ’sin Î¸ cos Î¸

(rotations in the plane by an angle Î¸) or in the form

cos Î¸
sin Î¸
sin Î¸ âˆ’cos Î¸

(rotations
followed by a reï¬‚ection). That is, this group has two disconnected components. Clearly, each of
these components has a volume 2Ï€, so the volume of V2 is 4Ï€.

48
6
Time for a Change
Changing to polar coordinates, we get
Vol(V2) =
 2Ï€
0
dÎ¸
 2Ï€
0
dÏ†
 âˆ
0
dr rÎ´(r âˆ’1)
 âˆ
0
d R RÎ´(R âˆ’1)Î´ (r R cos(Î¸ âˆ’Ï†))
=
 2Ï€
0
dÎ¸
 2Ï€
0
dÏ†Î´ (cos(Î¸ âˆ’Ï†)) = 4Ï€ ,
(6.11)
in agreement with (6.6) for N = 2 as it should.
6.5
Jpdf of Eigenvalues and Eigenvectors
As in Sect.6.1â€”but this time with more variablesâ€”we are after the change of vari-
ables H â†’{x, O}
Ï(H11, . . . , HN N)

iâ‰¤j
d Hi j = Ï(H11(x, O), . . . , HN N(x, O))
			J(H â†’{x, O})
			



Ë†Ï(x1,...,xN ,O)
dO
N

i=1
dxi .
(6.12)
On the left hand side, the jpdf of the N(N + 1)/2 entries of H in the upper
triangle, including the diagonal. On the right hand side, the jpdf Ë†Ï of both eigenvalues
(N) and independent eigenvector components (N(N âˆ’1)/2, the dimension of the
Stiefel manifold spanned by the orthogonal group over the reals). The number of
â€œdegrees of freedomâ€ is OK, thanks to the mind-wrecking and highly nontrivial
identity N(N + 1)/2 = N + N(N âˆ’1)/2.
Clearly, on the right hand side we had to include the Jacobian of the change of
variables, which we are going to compute below. While in principle this Jacobian
could depend on the full set of variables {x, O}, it turns out that it only depends on
the eigenvalues {x}, exactly as it happens for the change to polar coordinates (6.3).
In our RMT case, this Jacobian is precisely the so-called Vandermonde determi-
nant,2
J(H â†’{x, O}) =

j>k
(x j âˆ’xk) .
(6.13)
This can be generalized to the hermitian and quaternion self-dual cases. The only
difference is that the Vandermonde is then raised to the power Î² = 2, 4 respectively.
We will prove this in the next Chapter.
2Why this is indeed a determinant in disguise will become clearer very shortly.

6.6 Leave the Eigenvalues Alone
49
6.6
Leave the Eigenvalues Alone
Now, stare at the right hand side of (6.12) carefully.
The joint probability density of eigenvalues and eigenvectors Ë†Ï(x1, . . . , xN, O)
is the product of two terms: the jpdf of entriesâ€”written as a function of eigenvalues
and eigenvectorsâ€”times the Jacobianâ€”which is a function of the eigenvalues alone.
Then the next question is: how can I get the jpdf of eigenvalues alone? Well, you
will need to integrate out the eigenvector components {O} in (6.12). More precisely
Ë†Ï(x1, . . . , xN)dx = dx

VN
dO Ë†Ï(x1, . . . , xN, O) ,
(6.14)
exactly as we did earlier on to ï¬nd Ë†Ï1,2(r) from Ë†Ï1,2(r, Î¸). And exactly as in that
case, this integration over VN may or may not be easy/possible to perform explicitly.
It is certainly possible when the original jpdf of entries, once expressed in terms of
eigenvalues and eigenvector components, is itself independent of eigenvectorsâ€”in
complete analogy with our previous example with r and Î¸. In this case, we would
get
Ë†Ï(x1, . . . , xN, O) â‰¡Ï(H11(x, 

Z
Z
O ), . . . , HN N(x, 

Z
Z
O ))
			J(H â†’{x, 

Z
Z
O })
			
= function of x alone ,
(6.15)
and all is left to do in (6.14) is the â€œvolumeâ€ integral

VN dO, yielding the simple
constant in (6.6)â€”much like 2Ï€ in the warm-up example with r, Î¸ above.
The prototypes of this favorable case are the rotationally invariant ensembles, see
the next section.3
6.7
For Invariant Models...
We can now formulate a cute little theorem for invariant models [2]. The proof is
given below.
Let the real symmetric N Ã— N matrix H have a jpdf of entries Ï[H] =
Ï†

Tr H, . . . , Tr H N
, which is evidently invariant under orthogonal similarity trans-
formations.4 Then the jpdf of the N ordered eigenvalues of H (x1 â‰¥x2 â‰¥Â· Â· Â· â‰¥xN)
is
3Instead, for models with independent entries, the jpdf of entries cannot beâ€”in generalâ€”written in
terms of the eigenvalues alone. For such models, the jpdf of eigenvalues is therefore not generally
known.
4Recall Weylâ€™s lemma (3.8).

50
6
Time for a Change
Ïord(x1, . . . , xN) =
Ï€ N 2/2
N(N/2)Ï†

i
xi, . . . ,

i
x N
i
 
i< j
(xi âˆ’x j) .
(6.16)
Note that there is no absolute value around the Vandermonde, as the eigenvalues
are ordered.
Let us see how this theorem works in practice for the GOE case. We have
Ï[H] =
N

i=1
eâˆ’H 2
ii/2
âˆš
2Ï€

i< j
eâˆ’H 2
i j
âˆšÏ€ =
1
(2Ï€)N/2Ï€
N2âˆ’N
4
exp

âˆ’1
2Tr H 2

.
(6.17)
Therefore, applying the theorem above
Ïord(x1, . . . , xN) =
Ï€ N 2/2
N(N/2) Ã—
1
(2Ï€)N/2Ï€
N2âˆ’N
4



1
Z(ord)
N,Î²=1
eâˆ’1
2
N
i=1 x2
i 
i< j
(xi âˆ’x j) , (6.18)
which needs to be compared with Eq.(2.15) for Î² = 1â€”given without proof at the
time
Ï(x1, . . . , xN) =
1
ZN,Î²=1
eâˆ’1
2
N
i=1 x2
i 
j<k
|x j âˆ’xk| ,
(6.19)
with
ZN,Î²=1 = (2Ï€)N/2
N

j=1
(1 + j/2)
(3/2)
.
(6.20)
Do the two Eqs.(6.18) and (6.19) indeed agree, as they should? Almost.
Notice that (6.18) holds for ordered eigenvalues, while (6.19) holds for unordered
eigenvalues (hence the need to include the absolute value). The two normalization
constants differ indeed by a factor N!
ZN,Î²=1 = N!Z(ord)
N,Î²=1 .
(6.21)
6.8
The Proof
Where does the normalizing factor
Ï€ N 2/2
N(N/2)
(6.22)
in (6.16) come from? It is instructive to look at this derivation more closely.

6.8 The Proof
51
Recall from (6.14) and (6.15) that (for the favorable case where one can integrate
out the eigenvectors)
jpdf eigenv. = jpdf entries (as function of eigenv. alone) Ã— |Vandermonde| Ã—

VN
dO .
(6.23)
This means that morally the normalizing factor (6.22) should correspond to the
volume integral

VN dO as in (6.6) (for Î² = 1, or

ËœVN dU over unitary matrix
elements for Î² = 2 etc.).
There is a subtlety though: the change of variables between entries and eigenvalues
(H â†’O X OT ) must be one-to-one. But eigenvectors are deï¬ned up to a phase,
e.g. if v is a real eigenvector, so is âˆ’v. To guarantee the uniqueness of the eigen-
decomposition, it is sufï¬cient to ï¬x the sign of the ï¬rst row of the matrix O, or the
phases of the ï¬rst row of the matrix U. This reduces the volume integral

VN dO by
a factor 2N in the orthogonal case, and the volume integral

ËœVN dU by (2Ï€)N in the
unitary case. And the proof is complete.
References
1. R.J. Muirhead, Aspects of multivariate statistical theory (John Wiley & Sons, 2009)
2. A. Edelman, Eigenvalues and Condition Numbers of Random Matrices, MIT Ph.D. Dissertation
(1989)
3. A. Edelman, Finite random matrix theory. Jacobians of matrix transforms (without wedge prod-
ucts) (2005), http://web.mit.edu/18.325/www/handouts/handout2.pdf
4. A.M. Mathai, Jacobians of matrix transformation and functions of matrix arguments (World
Scientiï¬c Publishing Co Inc, 1997)

Chapter 7
Meet Vandermonde
The â€œrepulsiveâ€ term between eigenvalues of invariant models 
i< j(x j âˆ’xi) can be
written as a determinant, called Vandermonde in honor of the French mathematician
Alexandre-ThÃ©ophile Vandermonde (who never wrote it [1]).
7.1
The Vandermonde Determinant
We have the following identity
Î”N(x) :=
N

i< j
(x j âˆ’xi) = det(xiâˆ’1
j
) = det
â›
âœâœâœâœâœâœâ
1
. . .
1
x1
. . .
xN
.
.
.
.
.
.
.
.
.
x Nâˆ’1
1
. . . x Nâˆ’1
N
â
âŸâŸâŸâŸâŸâŸâ 
.
(7.1)
The Vandermonde is clearly a completely anti-symmetric polynomial in N vari-
ables: take for example N = 3. We have Î”3(x) = (x2 âˆ’x1)(x3 âˆ’x1)(x3 âˆ’x2). Now,
exchange any two x js: for example, x3 â†”x2. We get âˆ’Î”3(x) (we pick up a minus
sign any time we make any exchange of two x js).
The Vandermonde has a quite funny property: we can understand it already on a
2 Ã— 2 matrix. Take
det

 1 1
x1 x2

= x2 âˆ’x1
det

1
1
3x1 + 17 3x2 + 17

= 3(x2 âˆ’x1) .
(7.2)
Stare at these two determinants carefully. We have just replaced the second row of
the ï¬rst matrix (containing ï¬rst powers of x1 and x2) with a ï¬rst degree polynomial.
Â© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_7
53

54
7
Meet Vandermonde
The result is just 3 times the Vandermonde on the left. The 17 has disappeared
altogether! This means that you have a lot of freedom in devising a matrix whose
determinant gives the Vandermonde.
More formally, the entries xk
i in the (k +1)th row can be replaced, up to a constant
factor a0a1 Â· Â· Â· aNâˆ’1, by a polynomial of degree k of the form: Ï€k(xi) = akxk
i + Â· Â· Â· ,
where we omit terms of lower order in xi. The important point is that these lower
order terms can be absolutely anything. The result is that:
Î”N(x) =
1
a0a1 Â· Â· Â· aNâˆ’1
det
â›
âœâœâœâœâœâœâ
Ï€0(x1)
. . .
Ï€0(xN)
Ï€1(x1)
. . .
Ï€1(xN)
.
.
.
.
.
.
.
.
.
Ï€Nâˆ’1(x1) . . . Ï€Nâˆ’1(xN)
â
âŸâŸâŸâŸâŸâŸâ 
.
(7.3)
Orthogonal polynomials are an important class of polynomials Ï€k(x) that can
be especially useful to play this trick. We will discuss in Chap.10 how this simple
property can actually turn seemingly impossible calculations into feasible ones.
For instance, let us show how the Hermite and Laguerre orthogonal polynomials
can be used to express the Vandermonde. For N = 3 it is easy to see that
det
â›
â
H0(x1) H0(x2) H0(x3)
H1(x1) H1(x2) H1(x3)
H2(x1) H2(x2) H2(x3)
â
â = det
â›
â
1
1
1
x1
x2
x3
x2
1 âˆ’1 x2
2 âˆ’1 x2
3 âˆ’1
â
â = Î”3(x), (7.4)
and that âˆ’Î”3(x)/2 =
det
â›
â
L0(x1) L0(x2) L0(x3)
L1(x1) L1(x2) L1(x3)
L2(x1) L2(x2) L2(x3)
â
â = det
â›
âœâ
1
1
1
âˆ’x1 + 1
âˆ’x2 + 1
âˆ’x3 + 1
x2
1
2 âˆ’2x1 + 1 x2
2
2 âˆ’2x2 + 1 x2
3
2 âˆ’2x3 + 1
â
âŸâ .
(7.5)
7.2
Do It Yourself
We now derive the nontrivial relation (6.13) J(H â†’{x, O}) = Î”N(x) for real
symmetric matrices H. We stress that this proof does not require any assumption on
the rotational invariance of the ensemble.

7.2 Do It Yourself
55
These can be diagonalized through an orthogonal transformation H = O X OT ,
where X = diag(x1, . . . , xN). To ï¬nd the Jacobian, we formally differentiate1 H,
Î´H = (Î´O)X OT + O(Î´X)OT + O X(Î´OT ) ,
(7.6)
and use Î´OT = âˆ’OT (Î´O)OT , which follows from OOT = 1. We get
Î´H = (Î´O)X OT + O(Î´X)OT âˆ’O X OT (Î´O)OT .
(7.7)
Pulling out a factor O to the left and OT to the right we obtain Î´H = O(Î´ Ë†H)OT ,
where
Î´ Ë†H = (Î´Î©)X âˆ’X(Î´Î©) + Î´X .
(7.8)
Here, Î´Î© = OT Î´O is an antisymmetric matrix.2 Since Î´H and Î´ Ë†H are related via
an orthogonal transformation, we only have to ï¬nd the Jacobian of Î´ Ë†H â†’{Î´X, Î´Î©}.
Noting that Î´X is diagonal, we can write
d Ë†Hi j = dÎ©i j(x j âˆ’xi) + dxiÎ´i j .
(7.9)
This is equivalent to the following differential relations:
d Ë†Hi j
dxk
= Î´i jÎ´ik,
d Ë†Hi j
dÎ©kâ„“
= Î´ikÎ´ jâ„“(x j âˆ’xi) .
(7.10)
Donâ€™t you see the Vandermonde trying hard to crop up here Â© ?
Let us now construct the Jacobian matrix J for a concrete 3 Ã— 3 case. The gener-
alization to the N Ã— N case will then appear obvious. The matrix J has dimension
N(N+1)
2
, so it is a 6 Ã— 6 matrix for N = 3. We parametrize the antisymmetric matrix
Î´Î© as follows:
Î´Î© =
â›
â
0
Î©12 Î©13
âˆ’Î©12
0
Î©23
âˆ’Î©13 âˆ’Î©23
0
â
â .
(7.11)
Then the Jacobian matrix becomes:
1The matrix element Hi j can be written as Hi j = 
â„“,m Oiâ„“Xâ„“m O jm = 
â„“Oiâ„“xâ„“O jâ„“. The
inï¬nitesimal matrix Î´H has entries (Î´H)i j = d Hi j given by the differential of Hi j. Equation(7.6)
is a shorthand of this explicit differentiation w.r.t. Oiâ„“, xâ„“and O jâ„“.
2Obviously, you need to prove it before proceeding.

56
7
Meet Vandermonde
â›
âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ
d Ë†H11
dÎ©12
d Ë†H11
dÎ©13
d Ë†H11
dÎ©23
d Ë†H11
dx1
d Ë†H11
dx2
d Ë†H11
dx3
d Ë†H12
dÎ©12
d Ë†H12
dÎ©13
d Ë†H12
dÎ©23
d Ë†H12
dx1
d Ë†H12
dx2
d Ë†H12
dx3
d Ë†H13
dÎ©12
d Ë†H13
dÎ©13
d Ë†H13
dÎ©23
d Ë†H13
dx1
d Ë†H13
dx2
d Ë†H13
dx3
d Ë†H22
dÎ©12
d Ë†H22
dÎ©13
d Ë†H22
dÎ©23
d Ë†H22
dx1
d Ë†H22
dx2
d Ë†H22
dx3
d Ë†H23
dÎ©12
d Ë†H23
dÎ©13
d Ë†H23
dÎ©23
d Ë†H23
dx1
d Ë†H23
dx2
d Ë†H23
dx3
d Ë†H33
dÎ©12
d Ë†H33
dÎ©13
d Ë†H33
dÎ©23
d Ë†H33
dx1
d Ë†H33
dx2
d Ë†H33
dx3
â
âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
=
â›
âœâœâœâœâœâœâ
0
0
0
1 0 0
x2 âˆ’x1
0
0
0 0 0
0
x3 âˆ’x1
0
0 0 0
0
0
0
0 1 0
0
0
x3 âˆ’x2 0 0 0
0
0
0
0 0 1
â
âŸâŸâŸâŸâŸâŸâ 
. (7.12)
Swapping rows and columns, it is possible to bring this to the diagonal form, so
that the determinant becomes trivial to compute. In the general N case, one has:
| det J| =

j<k
|x j âˆ’xk| ,
(7.13)
as expected. The proof in the complex hermitian and quaternion self-dual cases is
analogous and is left as an exercise.
For a nice numerical test of the Jacobian identity (7.13), we refer to [2], Sect.3.2,
while for a â€œback-of-the-envelopeâ€ derivation based on counting degrees of freedom,
see [3].
We will make extensive use of the Vandermonde determinant and its properties
in Chap.10.
References
1. B. Ycart, Rev. dâ€™Histoire des Math. 9, 43 (2013)
2. A. Edelman, N. Raj Rao, Acta Numer. 14, 233 (2005)
3. A. Zee, Quantum Field Theory in a Nutshell, 2nd edn. (Princeton Univ. Press, Princeton, 2010)

Chapter 8
Resolve(nt) the Semicircle
In this Chapter, we introduce the so called resolvent, a complex function from which
the spectral density1 can be calculated. The advantage of the resolvent approach is
that one has to solve an algebraic equation (like ax2 + bx + c = 0) instead of a
(singular) integral equation (like Pr

dxâ€² nâ‹†(xâ€²)
xâˆ’xâ€² = x, see (5.15)). The disadvantage
is that you need to know a bit of complex analysis.
8.1
A Bit of Theory
We introduce the complex function G N(z), with z âˆˆC \ {xi}326283
G N(z) = 1
N Tr
1
z âˆ’H = 1
N
N

i=1
1
z âˆ’xi
,
(8.1)
where the notation
1
zâˆ’H means the matrix inverse of z1 âˆ’H, and 1 is the N Ã— N
identity matrix.
If H is a random matrix, then G N(z) is a random complex function that has poles
at the locations xi of each eigenvalue.
The second ingredient we need is the Sokhotski-Plemelj formula
lim
Îµâ†’0+
1
y Â± iÎµ = Pr
1
y

âˆ“iÏ€Î´(y),
(8.2)
which should be interpreted as the integral relation (for a real-valued test function
Ï•(x) such that the integrals make sense)
1Unless otherwise stated, we will no longer make a distinction between nâ‹†(x), âŸ¨n(x)âŸ©and Ï(x).
Â© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_8
57

58
8
Resolve(nt) the Semicircle
lim
Îµâ†’0+
 âˆ’Îµ
âˆ’âˆ
+
 âˆ
Îµ
 Ï•(y)
y
dy

âˆ“iÏ€Ï•(0) = lim
Îµâ†’0+
 âˆ
âˆ’âˆ
Ï•(y)
y Â± iÎµdy .
(8.3)
For a one-liner proof, see below (around (8.6)).
Question 8.1 What is the point of introducing this identity?
â–¶First, stare at (8.2) carefully. You see that, on the right hand side, the imaginary
part is just a delta function. So, this identity is (yet another) way of representing
a delta function, as the imaginary part of a rational function (the left hand side).
Knowing that the spectral density is deï¬ned in terms of a delta function Ï(x) =
âŸ¨(1/N) 	N
i=1 Î´(x âˆ’xi)âŸ©, you should be spotting an interesting connection here.
More on this later.
8.2
Averaging
Imagine now to take the limit N â†’âˆof âŸ¨G N(z)âŸ©, where we average over the
distribution of the matrix H. This average is called resolvent, or Greenâ€™s function,
or Stieltjes transform. It is natural to assume (and can be mathematically justiï¬ed)
that:
â€¢ the sum in (8.1)
G N(z) = 1
N Tr
1
z âˆ’H = 1
N
N

i=1
1
z âˆ’xi
(8.4)
gets converted into an integral,
â€¢ the poles at xi merge into a continuous â€œcutâ€ on the real line,
â€¢ we have to â€œweighâ€ the integrand with the average density of eigenvalues Ï(x) at
point x.
The cut on the real line is therefore nothing but the support of the spectral density,
and the average resolvent is deï¬ned for all complex values z outside this cut (for
example, outside the interval [âˆ’
âˆš
2,
âˆš
2] on the real line for the Gaussian ensemble).
In formulae
âŸ¨G N(z)âŸ©â†’G(av)
âˆ(z) =

dxâ€² Ï(xâ€²)
z âˆ’xâ€² ,
for N â†’âˆ.
(8.5)
If you are inclined to believe that (8.5) is very plausible (to say the least), we can
now proceed smoothly.

8.2 Averaging
59
Compute now G(av)
âˆ(z) (the averaged resolvent in the large N limit) at z = x âˆ’iÎµ.
Carrying out this herculean task, we get
G(av)
âˆ(xâˆ’iÎµ) =

dxâ€²
Ï(xâ€²)
x âˆ’iÎµ âˆ’xâ€² =

dxâ€² Ï(xâ€²)(x âˆ’xâ€²)
(x âˆ’xâ€²)2 + Îµ2 +i

dxâ€²Ï(xâ€²)
Îµ
(x âˆ’xâ€²)2 + Îµ2 ,
(8.6)
where we have multiplied up and down by x âˆ’xâ€² + iÎµ and separated the real and
imaginary parts.
Sending now Îµ â†’0+, we are basically proving the Sokhotski-Plemelj formula:
the real part becomes a principal value integral (the so called Hilbert transform),
Pr

dxâ€² Ï(xâ€²)
xâˆ’xâ€² , while the imaginary part (with the sign reversed with respect to the
argument of G(av)
âˆ, Â± â†’âˆ“) becomes Ï€Ï(x), using the following representation for
the delta function
Î´(x) = 1
Ï€ lim
Îµâ†’0+
Îµ
x2 + Îµ2 .
(8.7)
In summary
Ï(x) = 1
Ï€ lim
Îµâ†’0+ Im G(av)
âˆ(x âˆ’iÎµ) .
(8.8)
So, if you know (or can calculate) the resolvent in the complex plane, you can
from it deduce the spectral density.
All this in theory. Practice in the next section.
Question 8.2 Are there important properties of the resolvent G(av)
âˆ(z) in (8.5)
that are worth remembering?
â–¶First of all, if you send |z| â†’âˆin (8.5), you get
G(av)
âˆ(z) =

dxâ€² Ï(xâ€²)
z âˆ’xâ€² â‰ˆ1
z

dxâ€²Ï(xâ€²) = 1
z + .... ,
(8.9)
where we have used normalization of Ï(x). This asymptotic âˆ¼1/z behavior
can be important in applications.
Next, expanding the denominator in (8.5) to all orders, we observe that the
resolvent is the generating function of moments Î¼k = âŸ¨Tr(H k)âŸ©=

dxÏ(x)xk
G(av)
âˆ(z) =

dxâ€² Ï(xâ€²)
z âˆ’xâ€² = 1
z

dxâ€²
Ï(xâ€²)
1 âˆ’xâ€²/z = 1
z
âˆ

k=0

dxâ€²Ï(xâ€²)
 xâ€²
z
k
=
âˆ

k=0
Î¼k
zk+1 ,
(8.10)
with Î¼0 = 1.

60
8
Resolve(nt) the Semicircle
8.3
Do It Yourself
We propose here a truly elementary derivation of the algebraic equation satisï¬ed by
the resolvent for the Gaussian ensemble.
Consider the partition function of the standard Gaussian ensemble, after a further
rescaling xi â†’xi
âˆš
N and ignoring prefactors
ZN,Î² âˆ

R
N

j=1
dx j eâˆ’Î²N
2
	N
i=1 x2
i 
j<k
|x j âˆ’xk|Î² =

R
N

j=1
dx j eâˆ’Î²NV[x] ,
(8.11)
with
V[x] = 1
2

i
x2
i âˆ’1
2N

iÌ¸= j
ln |xi âˆ’x j| .
(8.12)
Compared to our earlier Coulomb gas treatment, we have pulled out a factor
N (not N 2), so that the xi are now of O(1) for large N. Instead of introducing a
continuous counting function n(x) (as we did in Chap.4), we can directly perform
the saddle point evaluation of the N-fold integral (8.11), obtaining for each variable
xi the equation
âˆ‚V[x]
âˆ‚xi
= 0 â‡’xi = 1
N

jÌ¸=i
1
xi âˆ’x j
.
(8.13)
Multiplying (8.13) by
1
N(zâˆ’xi) and summing over i, we get:
1
N

i
xi
z âˆ’xi
= 1
N

i

jÌ¸=i
1
xi âˆ’x j
1
N(z âˆ’xi) .
(8.14)
Adding and subtracting z in the numerator, the left-hand-side L becomes
L = 1
N

i
xi âˆ’z + z
z âˆ’xi
= âˆ’1 + z 1
N

i
1
z âˆ’xi
= âˆ’1 + zG N(z) .
(8.15)
As for the right-hand-side, let us deï¬ne R =
1
N 2
	N
i=1
	
jÌ¸=i
1
zâˆ’xi
1
xiâˆ’x j . Writing
1
(z âˆ’xi)(xi âˆ’x j) =
1
z âˆ’x j

1
z âˆ’xi
+
1
xi âˆ’x j

,
(8.16)
one obtains the following self-consistency equation for R
R = G2
N(z) + 1
N Gâ€²
N(z) âˆ’R
â‡’
R = 1
2G2
N(z) + 1
2N Gâ€²
N(z) .
(8.17)

8.3 Do It Yourself
61
Equating L to R, we obtain as promised that the saddle-point condition (8.13)
gets converted into an equation for the resolvent
âˆ’1 + zG N(z) = 1
2G2
N(z) + 1
2N Gâ€²
N(z) .
(8.18)
This is good, but is still a differential equation for G N(z), while we promised an
even simpler algebraic equation. It is actually easy to get rid of the differential term
in (8.18) by noticing that, with xi of O(1), the resolvent as deï¬ned in (8.1) is itself
of O(1) and therefore the term
1
2N Gâ€²
N(z) is subleading for large N.
Taking the average, the surviving algebraic (at long last!) equation for N â†’âˆ
reads
G(av)2
âˆ
(z) âˆ’2zG(av)
âˆ(z) + 2 = 0 .
(8.19)
It is instructive to solve (8.19) directly as a quadratic equation (recall that quadratic
equations for complex variables admit the same solving formula as their real coun-
terparts), yielding
G(av)
âˆ(z) = z Â±

z2 âˆ’2 .
(8.20)
Settingnow, z = xâˆ’iÎµ,weobtain G(av)
âˆ(xâˆ’iÎµ) = xâˆ’iÎµÂ±

(x2 âˆ’Îµ2 âˆ’2) + i(âˆ’2xÎµ).
The square root (with positive real part) of a complex number a + ib can be written
as [1] âˆša + ib = p + iq, with
p =
1
âˆš
2

a2 + b2 + a
q = sign(b)
âˆš
2

a2 + b2 âˆ’a ,
(8.21)
where sign(x) = 1 if x > 0 and = âˆ’1 if x < 0.
Hence we obtain (recalling (8.8))
1
Ï€ ImG(av)
âˆ(x âˆ’iÎµ) = âˆ’Îµ
Ï€ Â± sign(âˆ’2xÎµ)
Ï€
âˆš
2

(x2 âˆ’Îµ2 âˆ’2)2 + 4x2Îµ2 âˆ’x2 + Îµ2 + 2
Îµâ†’0+
âˆ’â†’Â±sign(âˆ’x)
Ï€
âˆš
2

|x2 âˆ’2| âˆ’x2 + 2 .
(8.22)
From this expression, you see that i) for |x| >
âˆš
2 you obtain that the density is
0, and ii) for |x| <
âˆš
2, you need to select the (âˆ’) or (+) sign in front, according
to whether x > 0 or x < 0 respectively. After choosing the right sign, you get
Ï(x) = (1/Ï€)
âˆš
2 âˆ’x2 as expected.
To double-check this result, we can insert the semicircle ÏSC(x) =
1
Ï€
âˆš
2 âˆ’x2
back into the deï¬nition (8.5) and perform the numerical integration for z = x âˆ’iÎµ,
with Îµ a small positive number and separately for the two cases, 0 < x <
âˆš
2 and
âˆ’
âˆš
2 < x < 0. This is done with the code [â™ check_resolvent.m], where the
results are compared with the two choices of sign in (8.20). You see that the (+)
choice in (8.20) only works with âˆ’
âˆš
2 < x < 0, and the (âˆ’) choice in (8.20) only
works with 0 < x <
âˆš
2.

62
8
Resolve(nt) the Semicircle
8.4
Localize the Resolvent
Let us now take a step back and â€œunpackâ€ the deï¬nition of the resolvent in Eq.(8.1).
We wrote that deï¬nition as the trace of the matrix (z âˆ’H)âˆ’1, with z âˆˆC \ {xi},
xis being the eigenvalues of H. If we write the trace explicitly, i.e. as a sum over the
diagonal elements of the resolvent, we have G N(z) = (1/N) 	N
i=1 G N,ii(z), where
G N,ii(z) = [(z âˆ’H)âˆ’1]ii =
N

j=1
(c j
i )2
z âˆ’x j
,
(8.23)
and c j
i is the ith component of the normalized eigenvector associated with the jth
eigenvalue of H.
So, you may now ask, what should I make of these matrix elements? Well, it turns
out that they contain precious information about the localization properties of the
matrix ensemble they are associated with.
But, ï¬rst of all, what is localization?
Simply put, the term localization refers to how â€œspread outâ€ over their components
the eigenvectors of a matrix are. Let us deï¬ne the inverse participation ratio (IPR)
of a normalized eigenvector as
IN, j =
N

i=1
(c j
i )4 .
(8.24)
Now, when the eigenvectorâ€™s components are all roughly of the same magnitude,
then we must have c j
i â‰ƒ1/
âˆš
N, âˆ€i, due to normalization. Hence, we will have
IN, j â‰ƒ1/N, and the IPR will vanish in the large N limit.
If, on the other hand, the eigenvector is signiï¬cantly different from zero only on
a number s of sites, then for those sites we will have c j
i â‰ƒ1/âˆšs, and the IPR will
remain roughly equal to 1/s in the large N limit. So, all in all, the IPR is a handy
tool that tells us whether certain eigenvectors of a matrix are extended (i.e. have an
extensive number of non zero components) or instead localized on a ï¬nite number
of sites.
Although this may sound like a mathematical curiosity, the localization properties
of matrix ensembles are related to a number of relevant features of the physical
systems they describe. In particular, it is often crucial to detect the so called mobility
edge, i.e. the critical eigenvalue that separates the part of the spectrum associated
with extended states from the one associated with localized states. For example, it
has famously been shown that the mobility edge determines the Anderson transition
in electronic systems [2].

8.4 Localize the Resolvent
63
All in all, it should be now clear that having analytical access to the distributional
properties of the IPRs corresponding to different segments of a given ensembleâ€™s
eigenvalue spectrum is a valuable thing. Luckily, this is where our diagonal elements
(8.23) come to the rescue. Indeed, it has been shown in [3] that the average value
P(x) of IPRs associated with states whose corresponding eigenvalues lie between x
and x + dx can be written in the large N limit as
P(x) = lim
Îµâ†’0+ lim
Nâ†’âˆ
Îµ
Ï€ NÏ(x)
N

i=1

|GN,ii(x âˆ’iÎµ)|2
= lim
Îµâ†’0+
Îµ
Ï€Ï(x)|G(av)
âˆ(x âˆ’iÎµ)|2 .
(8.25)
8.5
To Know More...
1. The saddle-point evaluation (8.13) based on the partition function (8.11) is
clearly valid when the neglected terms in the exponent are indeed subleading
(O(N)). There are modelsâ€”rotationally invariant by constructionâ€”where the
Dyson index Î² is allowed to scale with N [4, 5]. These models provide explicit
realizations of invariant Î²-ensembles, for which the resolvent equation is neces-
sarily more involved. Ref. [5] is also suggested for an elementary derivation of
this â€œimprovedâ€ resolvent equation in the presence of a hard wall in the spectrum.
2. Matrix models such as the Gaussian can be constructed introducing a ï¬ctitious
time evolution (stochastic) of the entries. In this case, it is possible to show that
the resolvent satisï¬es a partial differential equation of the Burgers type (see the
beautiful paper [6]).
3. The equation for the resolvent can be given a pretty interpretation in terms of
planar diagrams. Diagrammatic methods are at the heart of many beautiful results
in RMT (see [7, 8]).
References
1. S. Rabinowitz, Math. Inf. Q. 3, 54â€“56 (1993)
2. R. Abou-Chacra, P.W. Anderson, D.J. Thouless, J. Phys. C 6, 1734 (1973)
3. F.L. Metz, I. Neri, D. BollÃ©, Phys. Rev. E 82, 031135 (2010)
4. R. Allez, J.-P. Bouchaud, A. Guionnet, Phys. Rev. Lett. 109, 094102 (2012)
5. R. Allez, J.-P. Bouchaud, S.N. Majumdar, P. Vivo, J. Phys. A: Math. Theor. 46, 015001 (2013)
6. J.-P. Blaizot, M.A. Nowak, Phys. Rev. E 82, 051115 (2010)
7. J. Jurkiewicz, G. Åukaszewski, M.A. Nowak, Acta Phys. Pol. B 39, 799 (2008)
8. J. Bouttier, Matrix integrals and enumeration of maps, in The Oxford Handbook of Random
Matrix Theory, ed. by G. Akemann, J. Baik, P. Di Francesco (Oxford Univ. Press, Oxford, 2011)

Chapter 9
One Pager on Eigenvectors
Take the GUE ensemble of N Ã— N hermitian matrices. Any given matrix in the
ensemble will have unit-norm eigenvectors having in general complex components.
What is the statistics of such components?
Since eigenvalues and eigenvectors of invariant matrix models are decoupled, the
only constraint on the N components of an eigenvector is that its norm must be one,
therefore their jpdf reads
PGU E(c) = CNÎ´

1 âˆ’
N

n=1
|cn|2

,
(9.1)
where CN is a normalization constant.
It is convenient to compute the marginal distribution of a single component, say
|c1|2, given by
PGU E(y) =

d2c1 Â· Â· Â· d2cNÎ´(y âˆ’|c1|2)PGU E(c) .
(9.2)
Similarly, we can compute the jpdf of eigenvector components (this time all real
numbers) of a GOE matrix.
The calculation in (9.2) is carried out by ï¬rst deï¬ning an auxiliary object
PGU E(y; t) =

d2c1 Â· Â· Â· d2cNÎ´(y âˆ’|c1|2)CNÎ´

t âˆ’
N

n=1
|cn|2

,
(9.3)
such that PGU E(y) = PGU E(y; 1). Then, taking the Laplace transform with respect
to t to kill the delta function in (9.3)
 âˆ
0
dt eâˆ’st PGU E(y; t) = CN

d2c1Î´(y âˆ’|c1|2)eâˆ’s|c1|2 
d2c eâˆ’s|c|2Nâˆ’1
,
(9.4)
and ï¬nally converting the 2d integrals in polar coordinates
Â© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_9
65

66
9
One Pager on Eigenvectors
 âˆ
0
dt eâˆ’st PGU E(y; t) = Ë†CN
 âˆ
0
dr rÎ´(y âˆ’r2)eâˆ’sr2  âˆ
0
dÏ Ï eâˆ’sÏ2Nâˆ’1
âˆeâˆ’sy
sNâˆ’1 ,
(9.5)
where we have absorbed the angular constants in the overall normalization.
Inverting the Laplace transform, we obtain
PGU E(y; t) âˆ(t âˆ’y)Nâˆ’2Î¸(t âˆ’y) ,
(9.6)
where Î¸(z) is the Heaviside step function. Setting t = 1 and normalizing, we obtain
PGU E(y) = (N âˆ’1)(1 âˆ’y)Nâˆ’2
for 0 â‰¤y â‰¤1 .
(9.7)
Similarly, for the GOE one obtains
PGOE(y) =
1
âˆšÏ€
(N/2)
((N âˆ’1)/2)
(1 âˆ’y)(Nâˆ’3)/2
âˆšy
for 0 â‰¤y â‰¤1 .
(9.8)
Computing the average âŸ¨yâŸ©in both cases
âŸ¨yâŸ©GOE =
 1
0
dy yPGOE(y) =
(N/2)
2(1 + N/2) âˆ¼1
N
âŸ¨yâŸ©GU E =
 1
0
dy yPGU E(y) = 1
N ,
(9.9)
leads us to consider the scaled variable Î· = yN and take the limit N â†’âˆ. This
produces the scaled densities
PGOE(Î·) = lim
Nâ†’âˆ
1
N PGOE
 Î·
N
	
=
1
âˆš2Ï€Î·eâˆ’Î·/2 ,
(9.10)
PGU E(Î·) = lim
Nâ†’âˆ
1
N PGU E
 Î·
N
	
= eâˆ’Î· .
(9.11)
The ï¬rst of these densities is called the Porter-Thomas distribution [1, 2]. Note
also that the Gaussian nature of the matrix ensembles has not been used anywhere in
the derivation (the same densities would be obtained for any orthogonal or unitary
ensemble). The study of eigenvectors of random matrices has been recently revived
due to their importance in quantum system (see, e. g., [3â€“6]).
References
1. C.E. Porter, R.G. Thomas, Phys. Rev. 104, 483 (1956)
2. T.A. Brody, J. Floris, J.B. French, P.A. Mello, A. Pandey, S.S.M. Wong, Rev. Mod. Phys. 53,
385 (1981)
3. J.T. Chalker, B. Mehlig, Phys. Rev. Lett. 81, 3367 (1998)
4. B. Mehlig, J.T. Chalker, J. Math. Phys. 41, 3233 (2000)
5. Y.V. Fyodorov, On statistics of bi-orthogonal eigenvectors in real and complex Ginibre ensem-
bles: Combining partial Schur decomposition with supersymmetry (2017), http://arxiv.org/abs/
1710.04699
6. K. Truong, A. Ossipov, Europhys. Lett. 116, 37002 (2016)

Chapter 10
Finite N
Look back at Chap.1, where we constructed Gaussian matrices and histogrammed
their eigenvalues. For N â†’âˆ, we showed in various ways that the average spectral
density converges to the semicircle law. But what happens for ï¬nite N? Can we
compute analytically the shape of the histogram for, say, a 13Ã—13 Gaussian matrix?
The answer is Yesâ€”and not only for Gaussian matrices, but for any rotationally
invariant ensemble! This is done here. We start from the case Î² = 2, as it is much
easier.
10.1
Î² = 2 is Easier
Already in Chap.7, we mentioned that the Vandermonde determinant has some funny
properties: in particular, each row in the Vandermonde matrix can be replaced by
a polynomial of suitable degree, with many a priori unspeciï¬ed coefï¬cients. The
freedom in choosing these polynomials is enormous. A judicious choice is the key
of the celebrated orthogonal polynomial technique.
Take the jpdf of the N real eigenvalues of a rotationally invariant ensemble with
Î² = 2
Ï(x1, . . . , xN) = 1
ZN
N

i=1
eâˆ’V (xi)|Î”N(x)|2 ,
(10.1)
which is written in the â€˜potentialâ€™ form (see Eq.(5.31)). For example, for the Gaussian
ensemble V (x) = x2/2.
Â© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_10
67

68
10
Finite N
What is the goal then? To compute the average spectral density for ï¬nite N, i.e.
the N âˆ’1-fold integral
Ï(x1) =

dx2 Â· Â· Â· dxN Ï(x1, x2, . . . , xN ) =
1
ZN

dx2 Â· Â· Â· dxN
N

i=1
eâˆ’V (xi )|Î”N (x)|2 , (10.2)
where the partition function is ZN =

dx1 Â· Â· Â· dxN
N
i=1 eâˆ’V (xi)|Î”N(x)|2.
Note that in (10.2) we are integrating over all variables but one. These integrals
are nasty, though! The integrand does not factorize at all, so we need to ï¬nd some
smart trick to carry out the integration. It took a while even to the pioneers of these
calculations (for instance, Gaudin and Mehta) to ï¬gure out how to proceed. The steps
are as follows:
Step 1:
Rewrite the Vandermonde Î”N(x) as a determinant of the matrix A, whose entries
are polynomials Ï€k(x) (to be determined), as in (7.3)
Î”N(x) =
1
a0a1 Â· Â· Â· aNâˆ’1
det
â›
âœâœâœâœâœâœâ
Ï€0(x1)
. . .
Ï€0(xN)
Ï€1(x1)
. . .
Ï€1(xN)
.
.
.
.
.
.
.
.
.
Ï€Nâˆ’1(x1) . . . Ï€Nâˆ’1(xN)
â
âŸâŸâŸâŸâŸâŸâ 
.
(10.3)
Step 2:
Use the general relation1
(det A)2 = det(AT A) = det
â›
â
N

j=1
A ji A jk
â
â ,
(10.4)
applied to the matrix A from step 1, to write
Î”2
N(x) =
1
(Nâˆ’1
j=0 a j)2 det
â›
â
N

j=1
Ï€ jâˆ’1(xi)Ï€ jâˆ’1(xk)
â
â .
(10.5)
Step 3:
Pull the weight exp(âˆ’
i V (xi)) inside the determinant2 and shift the index j â†’
j âˆ’1, to write eventually
1Hereafter, inside a determinant the indices of the entries will run from 1 to N.
2Use

â„“Î±â„“

det( f (i, j)) = det(âˆšÎ±iÎ± j f (i, j)).

10.1 Î² = 2 is Easier
69
Ï(x1, . . . , xN) =
1
ZN(Nâˆ’1
j=0 a j)2 det
â›
â
Nâˆ’1

j=0
Ï† j(xi)Ï† j(xk)
â
â = det (KN(xi, xk))
ZN(Nâˆ’1
j=0 a j)2 ,
(10.6)
where Ï†i(x) = eâˆ’V (x)/2Ï€i(x) and
KN(x, xâ€²) = eâˆ’1
2 (V (x)+V (xâ€²))
Nâˆ’1

j=0
Ï€ j(x)Ï€ j(xâ€²) ,
(10.7)
which is a central object in RMT: the kernel.
Step 4:
Choose judiciously the (so far undetermined) polynomials Ï€ j(x). A great choice is
to pick them orthonormal with respect to the weight3 exp(âˆ’V (x))

eâˆ’V (x)Ï€i(x)Ï€ j(x)dx = Î´i j .
(10.8)
For instance, for the Gaussian (unitary) ensemble (V (x) = x2/2) the correspond-
ing orthonormal polynomials are
Ï€ j(x) = Hj(x/
âˆš
2)
âˆš
2Ï€ 2 j j!
,
(10.9)
if Hj(x) are Hermite polynomials satisfying
 âˆ
âˆ’âˆdx Hj(x)Hk(x) exp(âˆ’x2) =
âˆšÏ€2 j j!Î´ jk.
Question 10.1 What is the advantage of choosing polynomials with this
â€œorthonormalityâ€ property?
â–¶Well, the reason is that the kernel KN(x, xâ€²) in (10.7), if the polynomials are
chosen this way, satisï¬es a quite amazing â€œreproducingâ€ property

dyKN(x, y)KN(y, xâ€²) = KN(x, xâ€²) .
(10.10)
The proof is very simple: just insert (10.7) into (10.10) and use the orthonor-
mality relation (10.8). This property has a quite unexpected consequence, which
eventually allows to carry out the multiple integrations in (10.2) in a very ele-
gant, iterative way. Another ingredient is necessary, though, and is presented in
the next Section.
3Note that there is a factor (1/2) multiplying V (x) in the kernel (10.7), while there is none in the
weight function of the orthonormal polynomials in (10.8).

70
10
Finite N
10.2
Integrating Inwards
Summarizing, we have to carry out the multiple integration in (10.2) over a jpdf,
which can be written as the determinant of a kernel (see (10.6)), something like

dx2 Â· Â· Â· dxN det(KN(x j, xk)) =?
(10.11)
In normal situations, this would seem a rather hopeless task. But the reproducing
property of the kernel offers an unexpected way around.
First, an illuminating 2Ã—2 example, and then the full-ï¬‚edged (though dry) theory.
Imagine the following 2 Ã— 2 matrix J2(x), depending on the vector x = {x1, x2}
through a function f (x, y) as follows
J2(x) =
 f (x1, x1) f (x1, x2)
f (x2, x1) f (x2, x2)

.
(10.12)
Suppose now that the function f satisï¬es the â€œreproducingâ€ property (10.10),
namely

f (x, y) f (y, z) dÎ¼(y) = f (x, z) for a certain measure Î¼(y). What hap-
pens to the following integral

dÎ¼(x2) det(J2(x))?
(10.13)
Well, we have

dÎ¼(x2) det(J2(x)) =

dÎ¼(x2) [ f (x1, x1) f (x2, x2) âˆ’f (x1, x2) f (x2, x1)] =
= q f (x1, x1) âˆ’f (x1, x1) = (q âˆ’1) f (x1, x1) ,
(10.14)
where q =

dÎ¼(x2) f (x2, x2). We used the reproducing property to evaluate the
second integral.
Maybe this short calculation is not particularly revealing, but it can be actually
extended to the N Ã— N case as follows: let JN(x) be an N Ã— N matrix whose
entries depend on a real vector x = (x1, x2, . . . , xN) and have the form Ji j =
f (xi, x j), where f is some function satisfying the â€œreproducing kernelâ€ property

f (x, y) f (y, z) dÎ¼(y) = f (x, z) , for some measure dÎ¼(y). Then the following
holds:

det[JN(x)] dÎ¼(xN) = [q âˆ’(N âˆ’1)] det(JNâˆ’1(Ëœx)) ,
(10.15)
where q =

f (x, x) dÎ¼(x), and the matrix JNâˆ’1 has the same functional form as
JN with x replaced by Ëœx = (x1, x2, . . . , xNâˆ’1). A friendly proof can be found in [1].

10.2 Integrating Inwards
71
This is a quite spectacular result, which is commonly referred to as Dyson-Gaudin
integration lemma.4 First of all, note that the 2 Ã— 2 result (10.14) is in agreement
with the general statement. Second, comparing (10.11) and (10.15), we see that this
lemma actually allows to integrate det(KN(x j, xk)) (essentially, the jpdf) over the
last variable xN, producing as a result a determinant of a smaller kernel matrix

det

KN(xi, x j)

1â‰¤i, jâ‰¤N dxN = det

KN(xi, x j)

1â‰¤i, jâ‰¤Nâˆ’1 ,
(10.16)
where we have used q =

dxKN(x, x) = N (immediate from the deï¬nition of the
kernel (10.7)). Basically, the reproducing property carries over from the kernel to
the determinant of the kernel!
Therefore, we can iterate the process N âˆ’k times, killing one integral at a time
and reducing the dimension of the determinant by one, with a remarkable domino
effect

. . .

det

KN (xi, x j)

1â‰¤i, jâ‰¤N dxk+1 Â· Â· Â· dxN = (N âˆ’k)! det

KN (xi, x j)

1â‰¤i, jâ‰¤k .
(10.17)
In particular, setting k = 0 we can normalize the jpdf (10.1) as
1 =

dxÏ(x) =
1
ZN (Nâˆ’1
j=0 a j)2

dx det (KN (xi, xk)) â‡’ZN
â›
â
Nâˆ’1

j=0
a j
â
â 
2
= N!,
(10.18)
so that the two-point marginal Ï(x1, x2)
Ï(x1, x2) =

N

j=3
dx jÏ(x1, . . . , xN ) =
1
N(N âˆ’1) det
 KN (x1, x1) KN (x1, x2)
KN (x2, x1) KN (x2, x2)

,
(10.19)
(where one uses (N âˆ’2)!/N! = 1/[N(N âˆ’1)]), while the one-point marginal (the
average spectral density ) is simply
Ï(x1) =

dx2 Â· Â· Â· dxNÏ(x1, . . . , xN) = 1
N KN(x1, x1) .
(10.20)
And the problem is solved not just for the one-point marginal, but for any k-
point correlation functionâ€”once the kernel is built out of suitable polynomials,
orthonormal with respect to the weight V (x). The fact that all such functions can be
4The most accurate reference seems however to be [3].

72
10
Finite N
expressed in terms of determinants is usually referred to as determinantal structure
of the unitarily invariant ensembles.
For modern extensions of the â€œintegrate-outâ€ lemma and applications, have a look
at [2, 4].
10.3
Do It Yourself
Let us apply the general formalism to the GUE case, for which the orthonormal
polynomials are Ï€ j(x) = Hj(x/
âˆš
2)/
âˆš
2Ï€2 j j!, where Hj(x) are Hermite poly-
nomials. Then, we obtain immediately the spectral density at ï¬nite N as
Ï(x) =
1
N
âˆš
2Ï€
eâˆ’x2/2
Nâˆ’1

j=0
H 2
j (x/
âˆš
2)
2 j j!
.
(10.21)
In Fig.12.1 of Chap.12 we show a comparison between a numerically generated
histogram of GUE eigenvalues, and the corresponding theoretical result in (10.21).
Question 10.2 If I send N â†’âˆin (10.21), shouldnâ€™t I recover the semicircle?
I do not see how.
â–¶Yes, you should, and you will! The precise statement is
lim
Nâ†’âˆ
âˆš
2NÏ(z
âˆš
2N) = 1
Ï€

2 âˆ’z2 ,
for âˆ’
âˆš
2 < z <
âˆš
2 ,
(10.22)
which requires a bit of work on the asymptotics of Hermite polynomials. We
will give a ï¬‚avor of the steps you need just below.
10.4
Recovering the Semicircle
First, one injects the so called Christoffel-Darboux formula [5] into the game, a
quite spectacular relation that hugely simpliï¬es sums of orthogonal polynomials.
Specialized to the Hermite polynomials, it reads
n

k=0
Hk(x)Hk(y)
k!2k
=
1
n!2n+1
Hn(y)Hn+1(x) âˆ’Hn(x)Hn+1(y)
x âˆ’y
.
(10.23)

10.4 Recovering the Semicircle
73
With an eye towards (10.21), with a few manipulations and taking the limit x â†’y,
we obtain the relation
Nâˆ’1

k=0
Ï€2
k (x) =
âˆš
N

Ï€Nâˆ’1(x)Ï€â€²
N(x) âˆ’Ï€N(x)Ï€â€²
Nâˆ’1(x)

,
(10.24)
where the orthonormal polynomials with respect to the Gaussian weight were deï¬ned
in (10.9).
After huge simpliï¬cations, the GUE spectral density for ï¬nite Nâ€”suitably
rescaledâ€”can be rewritten in the form
âˆš
2NÏ(z
âˆš
2N) =
2eâˆ’Nz2
âˆš
Ï€ N2N 
(N)

N H2
Nâˆ’1(z
âˆš
N) âˆ’(N âˆ’1)HN (z
âˆš
N)HNâˆ’2(z
âˆš
N)

.
(10.25)
We should now analyze (10.25) in the limit N â†’âˆfor z âˆ¼O(1). To do so, we
need to use the following asymptotic formula for Hermite polynomials in the bulk5
HN+m(X
âˆš
2N) =
 2
Ï€
1/4 2m/2+N/2Nm/2âˆ’1/4(N!)1/2eN X2
(1 âˆ’X2)1/4
gm,N (X)

1 + O
 1
N

,
(10.26)
valid for âˆ’1 < X < 1, m âˆ¼O(1) and gm,N(x) given by the following expression
gm,N(x) = cos

Nx

1 âˆ’x2 + (N + 1/2) arcsin(x) âˆ’NÏ€/2 âˆ’m arccos(x)

.
(10.27)
We can now apply this asymptotic expansion to (10.25), with m = 0, âˆ’1, âˆ’2 as
needed, after the identiï¬cation X = z/
âˆš
2.
The two terms H 2
Nâˆ’1 and HN Ã— HNâˆ’2 produce the same N-dependent prefactor
(2/Ï€)1/22Nâˆ’1N âˆ’3/2(N!)eNz2
(1âˆ’z2/2)1/2
(check it!), and after simpliï¬cations we get to
âˆš
2NÏ(z
âˆš
2N) âˆ¼
2
NÏ€
âˆš
2 âˆ’z2

N cos2(Î± + Ï†) âˆ’(N âˆ’1) cos(Î±) cos(Î± + 2Ï†)

,
(10.28)
where the cosine terms come from gm,N(x) in (10.27). Here
5What does in the bulk mean? The point is that Hermite polynomials (and other classical orthogonal
polynomials) have two different asymptotics, according to the way their argument and parameter
scale with N. This in turns corresponds to different regimes, namely different locations x where the
spectrum is looked at, and different zooming resolutions.

74
10
Finite N
Î± = N(z/
âˆš
2)

1 âˆ’z2/2 + (N + 1/2) arcsin(z/
âˆš
2) âˆ’NÏ€/2 ,
(10.29)
Ï† = âˆ’arccos(z/
âˆš
2) .
(10.30)
Keeping only the leading âˆN terms in the square bracket, and using the identity
cos2(Î± + Ï†) âˆ’cos(Î±) cos(Î± + 2Ï†) = sin2(Ï†), we ï¬nally get
âˆš
2NÏ(z
âˆš
2N) âˆ¼
2
Ï€
âˆš
2 âˆ’z2 sin2(âˆ’arccos(z/
âˆš
2)) = 1
Ï€

2 âˆ’z2 ,
(10.31)
as expected.
References
1. Y.V. Fyodorov, Introduction to the Random Matrix Theory: Gaussian Unitary Ensemble and
Beyond (2004), https://arxiv.org/pdf/math-ph/0412017.pdf
2. E. Kanzieper, G. Akemann, Phys. Rev. Lett. 95, 230201 (2005); G. Akemann, E. Kanzieper, J.
Stat. Phys. 129, 1159 (2007)
3. G. Mahoux, M.L. Mehta, J. Phys. I France 1, 1093 (1991)
4. G. Akemann, L. Shifrin, J. Phys. A : Math. Gen. 40, F785 (2007)
5. W. Van Assche, Asymptotics for orthogonal polynomials (Springer, 2006)

Chapter 11
Meet AndrÃ©ief
In this Chapter, we present a couple of very useful integral identities involving the
Vandermonde determinant, and one cute application.
11.1
Some Integrals Involving Determinants
We start with the AndrÃ©ief identityâ€”also called sometimes the Gram or Heine
identity [1]. It states that a certain multiple integral involving the product of two
determinants can be written as the determinant of a matrix whose entries are single
integrals.
Confused? Letâ€™s have a closer look.
We are given two sets of N functions, { fk(x)} and {gk(x)}. We also have an
integration measure Î¼(x). We then have

N

j=1
dÎ¼(x j) det( f j(xk)) det(g j(xk)) = N! det

dÎ¼(x) f j(x)gk(x)

.
(11.1)
This can be proved by just expanding the left hand side as a double sum over
permutations, performing the integrals and then folding the result back into a single
sum. Try to prove it yourselfâ€”for example, right now.
If you think about it for a second, this identity seems too good to be true. On the
left hand side, you have, say, a 20-fold integral of a truly nasty object, and on the
right hand side a 20 Ã— 20 determinant, which can be easily handled by any scientiï¬c
softwareâ€”when not explicitly computable in closed form!
Â© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_11
75

76
11
Meet AndrÃ©ief
This identity is especially useful for unitary invariant ensembles (Î² = 2), because
there you can write the square of the Vandermonde determinant as 
j<k(x j âˆ’xk)2 =
det(xkâˆ’1
j
) det(xkâˆ’1
j
). For example, the partition function of the GUE can be written
as a determinant
ZN,Î² =

(âˆ’âˆ,âˆ)N
N

j=1
dx jeâˆ’1
2 x2
j 
j<k
(x j âˆ’xk)2 = N! det
 âˆ
âˆ’âˆ
dx eâˆ’1
2 x2x j+kâˆ’2

= N! det

2
1
2 ( j+kâˆ’3) 
(âˆ’1) j+k + 1


1
2( j + k âˆ’1)

,
(11.2)
which can also be evaluated in closed form as a Selberg-like integral [2].
A nice feature of the ï¬nal determinantâ€”and this happens for all Î² = 2
calculationsâ€”is that it is of the form det(Mi+ j), i.e. it is a Hankel determinant
(the matrix M is constant along the skew-diagonals). This happens because the two
determinants in the integrand on the left hand side are equal, and this produces a
factor (x jâˆ’1)(xkâˆ’1) on the right hand side.
There are two other identities that are similar in spirit to the AndrÃ©ief identity (the
de Brujin identities [3]). They read as follows:

x1â‰¤x2â‰¤...â‰¤xN
dÎ¼(x) det[Ï†i(x j)] = Pf
	
sign(x âˆ’y)Ï†i(x)Ï† j(y)dÎ¼(x)dÎ¼(y)

,
(11.3)
where i and j run from 1 to N, and

dÎ¼(x) det

Ï†i(x j) Ïˆi(x j)

= (2N)!Pf
	
dÎ¼(x)(Ï†i(x)Ïˆ j(x) âˆ’Ï† j(x)Ïˆi(x))

,
(11.4)
where i and j run from 1 to 2N.
In both the equations above, Pf denotes a Pfafï¬an. Just like the determinant can
be written as a sum over permutations, a Pfafï¬an is written as a sum over pairings.
Given a set S with an even number of elements, {1, ..., 2n}, a pairing of S is a
collection of n pairs of elements from S. For instance, the set {1, 2, 3, 4} has three
possible pairings: {{1, 2}, {3, 4}}, {{1, 3}, {2, 4}}, and {{1, 4}, {2, 3}}.
We can realize pairings as permutations acting of the trivial pairing {{1, 2}, {3, 4}}.
The previous pairings then correspond to the identity permutation, the transposition
(23) and the cycle (243).
In terms of these permutations, we have
Pf(A) =

P
s(P)
n
j=1
AP(2 jâˆ’1),P(2 j) ,
(11.5)
where s(P) is the signature of the permutation and A is a even-dimensional skew-
symmetric matrix.

11.1 Some Integrals Involving Determinants
77
For example, take n = 2 and A the following 4 Ã— 4 matrix
A =
â›
âœâœâ
0
A12
A13
A14
âˆ’A12
0
A23
A24
âˆ’A13 âˆ’A23
0
A34
âˆ’A14 âˆ’A24 âˆ’A34
0
â
âŸâŸâ .
(11.6)
We have Pf(A) = A12 A34 âˆ’A13 A24 + A14 A23 (compare with the pairings listed
above for the set {1, 2, 3, 4}). Note also that Pf(A) = âˆšdet(A).
We will encounter Pfafï¬ans again in Chap.12.
11.2
Do It Yourself
Let us see a simple example where the AndrÃ©ief formula turns a nasty problem into
a doable one.
Question: what is the probability that a 9 Ã— 9 GUE matrix has N+ = 7 positive
eigenvalues? From ï¬rst principles, we have in general
PN(N+ = n) =

dx1 Â· Â· Â· dxNÏ(x1, . . . , xN)Î´

n âˆ’
N

i=1
Î¸(xi)

,
(11.7)
where Î¸(x) is the Heaviside step function, =1 if x > 0 and 0 otherwise.
Note that the delta function in (11.7) is more correctly a Kronecker delta
Î´n,N
i=1 Î¸(xi). We can introduce the generating function
Ï•N (z) =
N

n=0
PN (N+ = n)zn =
1
ZN,Î²=2
 âˆ
âˆ’âˆ
N

j=1
dx jeâˆ’1
2
N
i=1 x2
i +(ln z) N
i=1 Î¸(xi ) 
j<k
(x j âˆ’xk)2 .
(11.8)
This multiple integral seems hopelessly complicated. But spotting that

j<k(x j âˆ’xk)2 = det(x jâˆ’1
i
) det(x jâˆ’1
i
), we can use the AndrÃ©ief formula to write
Ï•N(z) =
det
 âˆ
âˆ’âˆdx eâˆ’1
2 x2+(ln z)Î¸(x)xi+ jâˆ’2
det
 âˆ
âˆ’âˆdx eâˆ’1
2 x2xi+ jâˆ’2

= det

(âˆ’1)i+ j + z

ci+ j

det

(âˆ’1)i+ j + 1

ci+ j
 ,
(11.9)
where ck = 2
kâˆ’3
2 
 kâˆ’1
2

. We have used AndrÃ©ief also to express ZN,Î²=2 as a deter-
minant, and erased a common N! factor.
Evaluating the integrals, we got a ratio of Hankel determinants, which can easily
be evaluated exactly with a symbolic software.

78
11
Meet AndrÃ©ief
Note that Ï•N(1) = 1, as it should by normalization of PN(N+ = n) (see (11.8)).
The probabilities PN(N+ = n) can then be reconstructed by differentiation
PN(N+ = n) = 1
n!âˆ‚(n)
z Ï•N(z)

zâ†’0 .
(11.10)
Carrying out this program, we may ï¬nd that for a 9 Ã— 9 GUE matrix,
PN=9(N+ = 7) = 161229045760 âˆ’20942589825Ï€2 âˆ’9172989000Ï€3 + 3386880000Ï€4
48168960000Ï€4
â‰ƒ5.67686 Ã— 10âˆ’6 .
(11.11)
Note that the evaluation is exact, and can be extended to many other values of
n, N. Of course, it would be desirable to have an exact and explicit formula for these
probabilities at arbitrary n, N â€“ see [4].
For a numerical check of (11.10), see [â™ Andreief_check.m].
11.3
To Know More...
1. There is an interesting connection between Hankel determinants, the so-called
Toda equation on a semi-inï¬nite lattice, and PainlevÃ© functions. Deï¬ne
Ï„n = det(ai+ jâˆ’2)i, j=1,...,n ,
(11.12)
with â€œinitial conditionsâ€ Ï„âˆ’1 = 0, Ï„0 = 1 and Ï„1 = a0. Imagine that the entries
ak of this Hankel matrix are functions of x (and so is Ï„n, for any ï¬xed n). If the
ak satisfy the following relation, ak = aâ€²
kâˆ’1 (where â€² denotes differentiation with
respect to x), then the following hierarchy of equations holds
Ï„ â€²â€²
n Ï„n âˆ’(Ï„ â€²
n)2 = Ï„n+1Ï„nâˆ’1 .
(11.13)
In [â™ Toda.m] we test this property for n = 3.
Quite amazingly, the same Toda lattice equation is obeyed by so-called Ï„-
functions, which arise in the Hamiltonian formulation of the six PainlevÃ© equa-
tions (PI-PVI), other fundamental objects in the theory of nonlinear integrable
systems [5].
These deep connections between AndrÃ©ief evaluations, Hankel determinants,
Toda lattice and PainlevÃ© functions are at the root of quite spectacular results
(see e.g. [6, 7]).

References
79
References
1. C. AndrÃ©ief, Mem. de la Soc. Sci. de Bordeaux 2, 1 (1883)
2. P.J. Forrester, S.O. Warnaar, Bull. Amer. Math. Soc. (N.S.) 45, 489 (2008)
3. N.G. de Bruijn, J. Indian Math. Soc. 19, 133 (1955)
4. N.S. Witte, P.J. Forrester, Random Matrices: Theory Appl. 01, 1250010 (2012)
5. P.J. Forrester, N.S. Witte, Commun. Math. Phys. 219, 357 (2001)
6. E. Kanzieper, Phys. Rev. Lett. 89, 250201 (2002)
7. E. Kanzieper, Constr. Approx. 41, 615 (2015)

Chapter 12
Finite N Is Not Finished
In this short Chapter, we compute in the quickest way the spectral density for the
GOE (Î² = 1) and GSE (Î² = 4). The symmetry classes beyond the Unitary have a
reputation for being â€œunfriendlyâ€. We do not aim at giving the most general treatment
of correlation functions for such cases. The goal of this Chapter is just to provide a
smooth and gentle appetizer, allowing you to tackle the nastier bits with your back
covered.
12.1
Î² = 1
Let us assume N is even for simplicity. Indices i, j run from 1 to N, while k, â„“run
from 0 to N âˆ’1.
Suppose the jpdf of eigenvalues is given by
Ï(x1, . . . , xN) = 1
Z|Î”N(x)|
N

i=1
w(xi) .
(12.1)
For w(x) = exp(âˆ’x2/2), we recover the jpdf for the GOE.
Letâ€™s compute the normalization factor, a.k.a. the partition function,
Z =

dx|Î”N(x)|
N

i=1
w(xi) = |Ë†aN|

dx| det(R jâˆ’1(xi)w(xi))| ,
(12.2)
where Rk(x) = akxk + Â· Â· Â· is a family of polynomials through which the Vander-
monde materializes, and
Ë†aN =
Nâˆ’1

k=0
ak
âˆ’1
.
(12.3)
Â© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_12
81

82
12
Finite N Is Not Finished
To get rid of the absolute value, we restrict integration to the domain where the
variables are ordered:
Z = N!|Ë†aN|

âˆ’âˆ<x1<x2<Â·Â·Â·<âˆ
dx det(R jâˆ’1(xi)w(xi)) .
(12.4)
We may now use the de Brujin identity (11.3) to get
Z = N!|Ë†aN|Pf(Ai, j) ,
(12.5)
where
Ai, j =
 âˆ
âˆ’âˆ
dx
 âˆ
âˆ’âˆ
dyRiâˆ’1(y)R jâˆ’1(x)w(x)w(y)sign(x âˆ’y) ,
(12.6)
and Pf denotes the Pfafï¬an of the skew-symmetric matrix Ai j.
Let us now stop for a second to check on a 2Ã—2 example that, indeed, the expres-
sions in (12.4) and (12.5) coincide. Starting from the integral in (12.4), specialized
to a 2 Ã— 2 case, we have
 +âˆ
âˆ’âˆ
dy
 y
âˆ’âˆ
dx R0(x)R1(y)w(x)w(y) âˆ’
 +âˆ
âˆ’âˆ
dy
 y
âˆ’âˆ
dx R0(y)R1(x)w(x)w(y)
=
 +âˆ
âˆ’âˆ
dx
 x
âˆ’âˆ
dyR0(y)R1(x)w(x)w(y) âˆ’
 +âˆ
âˆ’âˆ
dy
 y
âˆ’âˆ
dx R0(y)R1(x)w(x)w(y) ,
(12.7)
where we have simply renamed the variables x â†’y and y â†’x in the ï¬rst integrals.
If we now expand the Pfafï¬an in Eq.(12.5) we obtain
Pf(Ai, j) =
 +âˆ
âˆ’âˆ
dy
 +âˆ
y
dx R0(y)R1(x)w(x)w(y) âˆ’
 +âˆ
âˆ’âˆ
dy
 y
âˆ’âˆ
dx R0(y)R1(x)w(x)w(y)
=
 +âˆ
âˆ’âˆ
dx
 x
âˆ’âˆ
dyR0(y)R1(x)w(x)w(y) âˆ’
 +âˆ
âˆ’âˆ
dy
 y
âˆ’âˆ
dx R0(y)R1(x)w(x)w(y) ,
(12.8)
where in the ï¬rst integral we have simply rewritten the integration domain âˆ’âˆ<
y < x < âˆ. The above expression coincides with (12.7).
Now, stare at (12.6) for a few seconds. To simplify the notation slightly, we may
deï¬ne the following skew-symmetric inner product
âŸ¨f, gâŸ©1 = 1
2
 âˆ
âˆ’âˆ
dx
 âˆ
âˆ’âˆ
f (y)g(x)w(x)w(y)sign(x âˆ’y)dy ,
(12.9)
so that we can write Ai, j
= 2âŸ¨Riâˆ’1, R jâˆ’1âŸ©1. Note that in general âŸ¨f, gâŸ©1 =
âˆ’âŸ¨g, f âŸ©1â€”we wouldnâ€™t call it skew-symmetric otherwise, would we?

12.1 Î² = 1
83
Now, in complete analogy with what we did for Î² = 2â€”identifying speciï¬c
polynomials, orthonormal with respect to the given weightâ€”we may choose the
polynomials R that â€œbehave nicelyâ€ with respect to this inner product. The nice
properties we require are: evens and odds are orthogonal among themselves,
âŸ¨R2k, R2â„“âŸ©1 = âŸ¨R2k+1, R2â„“+1âŸ©1 = 0 ,
(12.10)
and evens are orthogonal to odds unless they are adjacent,
âŸ¨R2k, R2â„“+1âŸ©1 = âˆ’âŸ¨R2â„“+1, R2kâŸ©1 = Î´kâ„“.
(12.11)
With this particular choice, the Râ€™s are called skew-orthogonal polynomials. The
matrix A in (12.6) acquires a simple form,
A =
â›
âœâœâœâœâœâ
0
2
âˆ’2 0
0
2
âˆ’2 0
...
â
âŸâŸâŸâŸâŸâ 
,
(12.12)
and the expression for Z drastically simpliï¬es: the determinant of A becomes simply
2N, hence its Pfafï¬an becomes 2N/2, and all the information about the speciï¬c weight
function w(x) is contained in Ë†aN. As a consequence, from (12.5) we get for the
partition function in (12.2) Z = N!|Ë†aN|2N/2.
Let us now generalize this calculation slightly. Consider the quantity
Z[ f ] =|Ë†aN|

dx| det(R jâˆ’1(xi)w(xi) f (xi))|
=Z[ f = 1]

dx Ï(x1, . . . , xN)
N

i=1
f (xi) ,
(12.13)
where we introduced an arbitrary function f (x) in the game, such that the integral
is convergent. Note that Z[ f = 1] coincides with Z.
From this new partition function, we can recover the density of eigenvalues for
ï¬nite N
Ï(x) =

dx2dx3 Â· Â· Â· dxNÏ(x, x2, . . . , xN)
(12.14)
by means of a functional derivative. This is the operator
Î´
Î´f , which satisï¬es all the
properties of a derivative, plus the condition
Î´
Î´f (x) f (y) = Î´(y âˆ’x) .
(12.15)

84
12
Finite N Is Not Finished
Then,
Î´
Î´f (x)Z[ f ]

f =1
= Z[ f = 1]

dxÏ(x1, . . . , xN)
N

i=1
Î´(xiâˆ’x) = N Z[ f = 1]Ï(x).
(12.16)
Following a calculation perfectly analogous to the previous one, we arrive at
Z[ f ] = N!|Ë†aN|Pf(Ai j[ f ]) ,
(12.17)
where
Ai j[ f ] =
 âˆ
âˆ’âˆ
dx
 âˆ
âˆ’âˆ
dyRiâˆ’1(y)R jâˆ’1(x)w(x)w(y) f (x) f (y)sign(x âˆ’y) .
(12.18)
Computing the functional derivative, and recalling the deï¬nition of Pfafï¬an in
Eq.(11.5), we have
Î´
Î´f (x)Z[ f ]

f =1
= N!|Ë†aN|

P
s(P)

Î´
Î´f (x)
N/2

k=1
AP(2kâˆ’1),P(2k)[ f ]

f =1
. (12.19)
When we apply the product rule for the derivative, and set f = 1, for each term
in the sum over permutations we get

Î´
Î´f (x) AP(1),P(2)[ f ]

f =1
AP(3),P(4)[1] Â· Â· Â· AP(Nâˆ’1),P(N)[1] + . . .
+ AP(1),P(2)[1]

Î´
Î´f (x) AP(3),P(4)[ f ]

f =1
Â· Â· Â· AP(Nâˆ’1),P(N)[1] + . . .
+ AP(1),P(2)[1] Â· Â· Â·

Î´
Î´f (x) AP(Nâˆ’1),P(N)[ f ]

f =1
.
(12.20)
The orthogonality relations (12.10), (12.11) imply that the products in the above
expressions are different from zero only when P is the identity permutation, i.e.
when P(2 j âˆ’1) and P(2 j) are adjacent numbers for each matrix element in the
product.
Hence, the sum in (12.19) reduces to the expression in (12.20) where P( j) =
j, âˆ€j. Each element AP(2 jâˆ’1),P(2 j) yields a factor 2, from the matrix A in
(12.12) so that each product in (12.20) reduces to an expression of the type
2N/2âˆ’1 
Î´
Î´f (x) A2kâˆ’1,2k[ f ]

f =1. Comparing (12.16) and (12.19), we eventually ï¬nd
that
Ï(x) = N!|Ë†aN|2N/2âˆ’1
NZ[ f = 1]
N/2

k=1

Î´
Î´f (x) A2kâˆ’1,2k[ f ]

f =1
.
(12.21)

12.1 Î² = 1
85
Making the result of the functional differentiation of (12.18) explicit, and rear-
ranging indices, we ï¬nally obtain
Ï(x) =
1
2N
N/2âˆ’1

k=0
w(x)[R2k(x)Î¦2k+1(x) âˆ’R2k+1(x)Î¦2k(x)] ,
(12.22)
where
Î¦k(x) =
 âˆ
âˆ’âˆ
dyRk(y)w(y)sign(x âˆ’y) .
(12.23)
For the Gaussian case, it can be shown [1, 2] that we can choose
R2k(x) =
âˆš
2
Ï€
1
4 2k(2k)!!
H2k(x) ,
(12.24)
R2k+1(x) =
âˆš
2
Ï€
1
4 2k+2(2k âˆ’1)!!

âˆ’H2k+1(x) + 4kH2kâˆ’1(x)

,
(12.25)
where the Hk(x) are Hermite polynomials. This gives, for example,
R0(x) =
âˆš
2
Ï€
1
4
,
R1(x) = âˆ’
âˆš
2x
2Ï€
1
4
,
R2(x) =
âˆš
2(2x2 âˆ’1)
2Ï€
1
4
,
R3(x) = âˆ’
âˆš
2x(2x2 âˆ’5)
2Ï€
1
4
.
(12.26)
Since the leading coefï¬cient of Hk(x) is 2k, we have
Ë†aN =
(âˆ’1)
N
2 2
N(Nâˆ’2)
4
Ï€
N
4 N/2âˆ’1
k=0
(2k)!
,
(12.27)
even though this quantity has completely dropped out from the ï¬nal expression for
the density (12.22).
-10
-5
0
5
10
x
0
0.04
0.08
0.12
0.16
Ï(x)
-10
-5
0
5
10
x
0
0.02
0.04
0.06
0.08
0.1
0.12
Ï(x)
-10
-5
0
5
10
x
0
0.02
0.04
0.06
0.08
0.1
Ï(x)
Fig. 12.1 Comparison between numerically generated eigenvalue histograms of 50000 matrices
of size N = 8 belonging to the Gaussian ensembles (GOE on the left, GUE in the middle, and GSE
on the right) and the corresponding theoretical densities

86
12
Finite N Is Not Finished
For a numerical check of (12.22), see Fig.12.1 below, which was obtained with
the code [â™ Gaussian_finite_density_check.m].
12.2
Î² = 4
Now
Ï(x1, . . . , xN) = 1
Z|Î”N(x)|4
N

i=1
w(xi) .
(12.28)
Start by writing |Î”N(x)|4 as a determinant of size 2N, in which two columns
depend on each variable. This is
|Î”N(x)|4 = det[xk
i
kxkâˆ’1
i
] ,
(12.29)
where 1 â‰¤i â‰¤N and 0 â‰¤k â‰¤2N âˆ’1. For instance, for N = 2 we have
(x2 âˆ’x1)4 = det
â›
âœâœâ
1
0
1
0
x1
1
x2
1
x2
1 2x1 x2
2 2x2
x3
1 3x2
1 x3
2 3x2
2
â
âŸâŸâ ,
(12.30)
which can be veriï¬ed if you have 10min to spare.
We can change xk
i by any family of polynomials Qk(xi) = bkxk
i +Â· Â· Â· that produce
the Vandermonde, and kxkâˆ’1
i
by its derivative Qâ€²
k(xi). So
|Î”N(x)|4 = |Ë†bN| det[Q jâˆ’1(xi)
Qâ€²
jâˆ’1(xi)] ,
(12.31)
where 1 â‰¤j â‰¤2N and
Ë†bN =
Nâˆ’1

k=0
b2
k
âˆ’1
.
(12.32)
In this case, Eq. (12.17) gets modiï¬ed as
Z[a] = (2N)!|Ë†bN|Pf(Bi, j[a]) ,
(12.33)
where
Bi, j[a] =
 âˆ
âˆ’âˆ
dx[Qiâˆ’1(x)Qâ€²
jâˆ’1(x) âˆ’Qâ€²
iâˆ’1(x)Q jâˆ’1(x)]w(x)a(x) .
(12.34)
We have used here the second De Brujin identity (11.4).
We may consider the above integral as another skew-symmetric inner product

12.2 Î² = 4
87
âŸ¨f, gâŸ©4 = 1
2
 âˆ
âˆ’âˆ
dx[ f (x)gâ€²(x) âˆ’f â€²(x)g(x)]w(x) ,
(12.35)
and we may choose the polynomials Q to be skew-orthogonal with relation to this:
evens and odds are orthogonal among themselves,
âŸ¨Q2k, Q2â„“âŸ©4 = âŸ¨Q2k+1, Q2â„“+1âŸ©4 = 0 ,
(12.36)
and evens are orthogonal to odds unless they are adjacent,
âŸ¨Q2k, Q2â„“+1âŸ©4 = âˆ’âŸ¨Q2â„“+1, Q2kâŸ©4 = Î´kâ„“.
(12.37)
Computing the functional derivative as before, we have
Ï(x) =
1
2N
Nâˆ’1

k=0
w(x)[Q2k(x)Qâ€²
2k+1(x) âˆ’Q2k+1(x)Qâ€²
2k(x)] .
(12.38)
In the Gaussian case, we can choose
Q2k =
âˆš
2
Ï€
1
4 2k(2k)!!

4kQ2kâˆ’2(x) + H2k(x
âˆš
2)

,
Q2k+1(x) =
âˆš
2
Ï€
1
4 2k+1(2k + 1)!!
H2k+1(x
âˆš
2) .
(12.39)
This gives, for example,
Q0 =
âˆš
2
Ï€
1
4 ,
Q1(x) = 2x
Ï€
1
4 ,
Q2(x) =
âˆš
2(4x2 + 1)
2Ï€
1
4
,
Q3(x) = 2x(4x2 âˆ’3)
3Ï€
1
4
.
(12.40)
Also, we have
Ë†bN = 2âŒŠNâˆ’1
2 âŒ‹2N(Nâˆ’1)
Ï€
N
2 Nâˆ’1
k=0 (k!!)2 .
(12.41)
Again, for a numerical check of (12.38), see Fig.12.1, which was obtained with
the code [â™ Gaussian_finite_density_check.m].
References
1. S. Ghosh, A. Pandey, Phys. Rev. E 65, 046221 (2002)
2. M.L. Mehta, Random Matrices (Academic Press, 1967)

Chapter 13
Classical Ensembles: Wishart-Laguerre
In this Chapter, we present one of the â€œclassicalâ€ examples of rotationally invariant
models: the Wishart-Laguerre (WL) ensemble.
13.1
Wishart-Laguerre Ensemble
Historically, one of the earliest appearances of a random matrix ensemble1 occurred
in 1928, when the Scottish mathematician John Wishart published a paper on mul-
tivariate data analysis in the journal Biometrika [3].
Wishart matrices are square N Ã— N matrices W with correlated entries. They are
constructed as 2 W = H H â€ , where H is a N Ã— M matrix (M â‰¥N) ï¬lled with i.i.d.
Gaussian entries.3 These entries may be real, complex or quaternion (we shall use
again the Dyson index Î² = 1, 2, 4 for the three cases, respectively), and â€  stands
for the transpose or hermitian conjugate of the matrix H. For example, for a 2 Ã— 3
complex matrix H
W =
 x11 + iy11 x12 + iy12 x13 + iy13
x21 + iy21 x22 + iy22 x23 + iy23
 â›
â
x11 âˆ’iy11 x21 âˆ’iy21
x12 âˆ’iy12 x22 âˆ’iy22
x13 âˆ’iy13 x23 âˆ’iy23
â
â .
(13.1)
Work out the matrix product, and convince yourself that W is hermitian, therefore
has real eigenvalues.
1In Mathematics, however, the 1897 work by Hurwitz on the volume form of a general unitary
matrix is of historical signiï¬cance [1, 2].
2Sometimes you ï¬nd a normalized version of it, with a 1/M factor in front.
3The notation W(N, M) is also used.
Â© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_13
89

90
13
Classical Ensembles: Wishart-Laguerre
TheWishartensembleisalsoreferredtoasâ€œLaguerreâ€,sinceitsspectralproperties
involve Laguerre polynomials, and also â€œchiralâ€ in the context of applications to
Quantum Chromodynamics (QCD) [4]. They are often called LOE, LUE and LSE,
for Î² = 1, 2, 4, respectively.
While the Gaussian eigenvalues can in principle be anywhere on the real axis,
Wishartmatriceshave N non-negativeeigenvalues,{x1, x2, . . . , xN}.Indeed,Wishart
matrices W are positive semideï¬nite. This means that (e.g. for Î² = 2) uâ‹†Wu â‰¥0 for
all nonzero column vectors u of N complex numbers. The proof is not hard, have a
go at it!
Question 13.1 What is the jpdf of the entries of WL ensemble W?
â–¶With some effort (see below), it can be computed as
Ï[W] âˆeâˆ’1
2 TrW(det W)
Î²
2 (Mâˆ’N+1)âˆ’1 ,
(13.2)
from which you immediately see that (i) the entries are correlated4 (the deter-
minant easily kills any hope of factorizing this jpdf), and (ii) the model is rota-
tionally invariant.5
From (13.2),thejpdfofeigenvalues canbewrittendownimmediately(justexpress
everything in terms of the eigenvalues and append a Vandermonde at the end)
Ï(x1, . . . , xN) =
1
Z(L)
N,Î²
eâˆ’1
2
N
i=1 xi
N
	
i=1
xÎ±Î²/2
i
	
j<k
|x j âˆ’xk|Î²,
(13.3)
where Î± = (1+M âˆ’N)âˆ’2/Î² and the normalization constant Z(L)
N,Î² can be computed
again using modiï¬cations of the Selberg integral6 [6]. As for the Gaussian ensembles,
one may sometimes ï¬nd in the literature an extra factor Î² in the exponential.
The conï¬ning potential for the Wishart-Laguerre ensemble is thus V (x) =
1
2 x âˆ’Î±
2 ln x, and this clearly motivates the use of (associated) Laguerre polynomials
L(Î±)
n (x), which are orthogonal with respect to this precise weight (after a simple
rescaling),

 âˆ
0
dx xÎ±eâˆ’x L(Î±)
n (x)L(Î±)
m (x) = Î“ (n + Î± + 1)
n!
Î´m,n .
(13.4)
4Unless for speciï¬c combinations of Î², M, N for which the determinant disappears.
5The jpdf (13.2) is not in contrast with Weylâ€™s lemma (see Eq.(3.8)). The determinant of a N Ã— N
matrix W can be indeed written as a function of the traces of the ï¬rst N powers of W (see [5]).
6Note that while for Wishart matrices M âˆ’N is a non-negative integer and Î² = 1, 2 or 4, the jpdf
in (13.3) is well deï¬ned for any Î² > 0 and any Î± > âˆ’2/Î² (this last condition is necessary to ensure
that the jpdf is normalizable). When these parameters take continuous values, this jpdf deï¬nes the
so-called Î²-Laguerre ensemble.

13.1 Wishart-Laguerre Ensemble
91
Question 13.2 What happens if I take M < N?
â–¶This situation deï¬nes the so-called Anti-Wishart ensemble ËœW. In this case, one
can show that N âˆ’M eigenvalues are exactly 0. The jpdf is similar to (13.3), but
some of the matrix elements of ËœW are non-random and deterministically related
to the ï¬rst M rows of ËœW [7].
The code [â™ Wishart_check.m] produces instances of Wishart matrices for
different Î²s, as well as normalized histograms of their eigenvalues. You can start
having a look at it now, but please come back to it after reading the next chapter.
Question 13.3 What is the limiting spectral density of the WL ensembles for
N â†’âˆ?
â–¶It is called the MarË‡cenko-Pastur density [8], which is superimposed to the
histograms produced with the code above in Fig.14.1 of the next chapter. We
are going to derive it using the resolvent method very shortly.
13.2
Jpdf of Entries: Matrix Deltas...
The calculation of the jpdf of entries (13.2) proceeds through a few simple steps. Set
for simplicity Î² = 2 (hermitian matrices). We can formally write
Ï[W] =

d HÏ[H]Î´

W âˆ’H H â€ 
.
(13.5)
As usual, the measure d H means that we are integrating over the 2N M degrees of
freedom (dof) 7 of H: each entry of the N Ã— M matrix H is a complex number, so it is
parametrized by two real numbers.8 Therefore, d H = N
i=1
M
j=1 dRe[Hi j]dIm[Hi j].
The matrix delta Î´(W âˆ’H H â€ ) enforces the constraint that a certain matrix W
must be equal to another matrix H H â€ . We do have an integral representation for
the scalar delta function, which does the same job for real numbers. It should then
be easy to work out the corresponding integral representation for the delta function
7With â€œdegrees of freedomâ€ we mean the independent real parameters that are necessary to deï¬ne
a matrix. For example, a hermitian matrix has N 2 degrees of freedom â€“ the N real entries on the
diagonal, and the real and imaginary parts of the entries in the upper triangle.
8Note, in particular, that for N = M the square matrix H is not hermitian, and has 2N 2 â€œdegrees
of freedomâ€ (dof) instead of N 2.

92
13
Classical Ensembles: Wishart-Laguerre
of, say, a N Ã— N hermitian matrix K â€“ after all, it will just be the product of scalar
deltas, one for each of the real dof.
Î´(K) =
N
	
i=1
Î´(Kii)
N
	
i=1
	
j>i
Î´(K (R)
i j )Î´(K (I)
i j ) =

 dT11
2Ï€
Â· Â· Â· dTN N
2Ï€
exp
â§
â¨
â©i
N

i=1
Tii Kii
â«
â¬
â­

N
	
i=1
	
j>i
dT (R)
i j
2Ï€
dT (I)
i j
2Ï€
ei N
i=1

j>i [T (R)
i j
K (R)
i j
+T (I)
i j
K (I)
i j ] ,
(13.6)
where we have introduced a set of N(N + 1)/2 parameters {T }, one for each delta.
Arranging the parameters {T } into a hermitian matrix, try to show that the ugly
expression in (13.6) can be recast in the more elegant form
Î´(K) =
1
2NÏ€ N 2

dT eiTr[T K] .
(13.7)
We can now perform the multiple integral in (13.5), with Gaussian distributed dof
of H
Ï[H] â‰¡Ï(H(R)
11 , H(I)
11 , . . . , H(R)
N M, H(I)
N M) =
N
	
i=1
M
	
j=1
 1
2Ï€ exp

âˆ’1
2 H(R)2
i j
âˆ’1
2 H(I)2
i j

=
 1
2Ï€
N M
eâˆ’1
2 Tr(H Hâ€ ) ,
(13.8)
where (R) and (I) denote the real and imaginary part of each of the N M entries of H.
Combining (13.5), (13.7) and (13.8) we have
Ï[W] =
1
2NÏ€ N 2
 1
2Ï€
N M 
dT

d H eâˆ’1
2 Tr(H H â€ )+iTrT (Wâˆ’H H â€ ) .
(13.9)
Dividing all the dof of the hermitian matrix T by 1/2 (i.e. changing variables
T â†’T/2), we obtain
Ï[W] =
1
2NÏ€ N 2
 1
2Ï€
N M 1
2
N 2 
dT

d H eâˆ’1
2 Tr(H H â€ )+ i
2 TrT(Wâˆ’H H â€ ) .
(13.10)
13.3
...and Matrix Integrals
Next, we use the following identity for N Ã— N hermitian matrices T

13.3 ...and Matrix Integrals
93
[det(Î¼1 âˆ’T )]âˆ’M =
1
(4Ï€i)M N

M
	
k=1
d2sk exp

i
2Î¼
M

k=1
sâ€ 
ksk âˆ’i
2
M

k=1
sâ€ 
kT sk

,
(13.11)
where 1 is the N Ã—N identity matrix, sk are k = 1, . . . , M complex (column) vectors,
so that d2sk = N
i=1 dsk,idsâ‹†
k,i and Î¼ is such that Im[Î¼] > 0.
Question 13.4 Any hint on how to prove it?
â–¶Just write T = U â€ 	U, with U the unitary matrix diagonalizing T , and
	 the diagonal matrix of eigenvalues Î»i. Then make the change of variables
Usk â†’Ëœsk, which is unitary and thus has Jacobian equal to 1. The resulting
integral factorizes as

M
	
k=1
d2Ëœsk â†’
 N
	
â„“=1
2

dx dy e
i
2 (xâˆ’iy)(Î¼âˆ’Î»â„“)(x+iy)
M
,
(13.12)
where x, y are real and imaginary part of the â„“th entry of sk, and the factor of 2
is the Jacobian of the change of variables {Ëœsk,i, Ëœsâ‹†
k,i} â†’{x, y}. The integral in
{x, y} yields 2Ï€i/(Î¼ âˆ’Î»â„“), from which the claim is immediate.
We can now perform the H integral in (13.10). How? Just imagine that the kth
vector sk is constructed as sk = (H1k, . . . , HNk)T , i.e. it is basically the kth column
of the rectangular matrix H.
Hence, note the identity âˆ’(1/2)Tr(H H â€ ) = (i/2)Î¼ M
k=1 sâ€ 
ksk, with Î¼ = i.
Finally, we have to calculate the Jacobian of the change of variables {H (R)
ik , H (I)
ik } â†’
{sk,i, sâ‹†
k,i}. For each entry of H, we have

sk,i
= H (R)
ik
+ iH (I)
ik
sâ‹†
k,i
= H (R)
ik
âˆ’iH (I)
ik .
(13.13)
The Jacobian from s â†’H is
â›
âœâœâ
âˆ‚sk,i
âˆ‚H (R)
ik
âˆ‚sk,i
âˆ‚H (I)
ik
âˆ‚sâ‹†
k,i
âˆ‚H (R)
ik
âˆ‚sâ‹†
k,i
âˆ‚H (I)
ik
â
âŸâŸâ =
1 i
1 âˆ’i

= âˆ’2i .
(13.14)
Thus, the Jacobian from H â†’s (the one we need) is (in absolute value) equal to
1/2 for each entry. In total, (1/2)N M.
Therefore, using (13.11)

94
13
Classical Ensembles: Wishart-Laguerre
Ï[W] =
1
2N Ï€ N2
 1
2Ï€
N M 1
2
N2+N M
(4Ï€i)M N

dT e
i
2 Tr(T W) [det (i1 âˆ’T )]âˆ’M .
(13.15)
We now need another matrix integral, with the pompous name â€œIngham-Siegel
integral of second typeâ€ [9], whose general formula reads (see Appendix A in [10])
JN,M(Q, Î¼) =

dT eiTr(T Q)[det(T âˆ’Î¼1)]âˆ’M = CM,N(det Q)Mâˆ’NeiÎ¼TrQ ,
(13.16)
with CM,N = 2NÏ€ N(N+1)/2iN M/ M
j=Mâˆ’N+1 Î“ ( j), and the matrix Q is hermitian
and positive deï¬nite, while T is just hermitian. Both are N Ã— N. We also require
Im(Î¼) > 0 to ensure convergence, and M â‰¥N.
To use this integral (13.16), we need to multiply back again all the degrees of
freedom of the matrix T by 2, and pull out a factor (âˆ’2) from the determinant,
resulting in
Ï[W] = 2âˆ’N(1+M)Ï€âˆ’N 2iâˆ’M N

dT eiTr(T W)

det

T âˆ’i
21
âˆ’M
,
(13.17)
which can be evaluated using (13.16) as
Ï[W] = 2âˆ’N(1+M)Ï€âˆ’N 2iâˆ’M N Ã— 2NÏ€ N(N+1)/2iN M
M
j=Mâˆ’N+1 Î“ ( j)
(det W)Mâˆ’Neâˆ’(1/2)TrW
=
1
2N MÏ€
N(Nâˆ’1)
2
M
j=Mâˆ’N+1 Î“ ( j)
(det W)Mâˆ’Neâˆ’(1/2)TrW ,
(13.18)
i.e. the jpdf of the entries of Wishart matrices for Î² = 2, with the correct normaliza-
tion9 (note that all the imaginary factors have correctly disappeared). Well done!
13.4
To Know More...
1. The spectral densities of the Wishart-Laguerre ensemble for ï¬nite N and Î² =
1, 2, 4 have been given explicitly in [12], together with numerical checks.
2. The large-N behavior of the spectral density and two-point function for the
Wishart-Laguerre ensemble is determined by the asymptotics of Laguerre
polynomials (in complete analogy with the Gaussian case). These are explicitly
given in [13].
3. Non-hermitian analogues of the Wishart-Laguerre ensemble can also be deï¬ned
(see [14] for a nice review).
9A reliable source for such normalizations is [11].

13.4 To Know More...
95
4. Readers interested in the diagrammatic approach to ï¬‚uctuations in the Wishart
ensemble should have a look at [15].
5. For a nice review on usefulness of Wishart-Laguerre ensemble in physics, see
[16]. For speciï¬c applications to QCD, see [4, 17].
References
1. A. Hurwitz, Nachr. Ges. Wiss GÃ¶ttingen 71 (1897)
2. P. Diaconis, P.J. Forrester, Random Matrices: Theory Appl. 06, 1730001 (2017)
3. J. Wishart, Biometrika 20A, 32 (1928)
4. J.J.M. Verbaarschot, Applications of Random Matrix Theory to QCD, in The Oxford Handbook
of Random Matrix Theory, eds. by G. Akemann, J. Baik, and P. Di Francesco (Oxford University
Press, Oxford, 2011)
5. F. Kleefeld, M. Dillig, Trace evaluation of matrix determinants and inversion of 4Ã—4 matrices
in terms of Dirac covariants (1998), https://arxiv.org/pdf/hep-ph/9806415.pdf
6. P.J. Forrester, S.O. Warnaar, Bull. Amer. Math. Soc. (N.S.) 45, 489 (2008)
7. R.A. Janik, M.A. Nowak, J. Phys. A Math. Gen. 36, 3629 (2003)
8. V.A. MarË‡cenko, L.A. Pastur, Math. USSR-Sb 1, 457 (1967)
9. Y.V. Fyodorov, Nucl. Phys. B 621, 643 (2002)
10. E. Kanzieper, N. Singh, J. Math. Phys. 51, 103510 (2010)
11. A. Edelman, Eigenvalues and Condition Numbers of Random Matrices, MIT Ph.D. Thesis
(1989)
12. G. Livan, P. Vivo, Acta Phys. Pol. B 42, 1081 (2011)
13. P.J. Forrester, N.E. Frankel, T.M. Garoni, J. Math. Phys. 47, 023301 (2006)
14. G. Akemann, Acta Phys. Pol. B 42, 0901 (2011)
15. J. Jurkiewicz, G. Åukaszewski, M. Nowak, Acta Phys. Pol. B 39, 799 (2008)
16. S.N. Majumdar, Extreme eigenvalues of wishart matrices: application to entangled bipartite
system, in The Oxford Handbook of Random Matrix Theory, eds. by G. Akemann, J. Baik, and
P. Di Francesco (Oxford University Press, Oxford, 2011)
17. K. Splittorff, J.J.M. Verbaarschot, Lessons from Random Matrix Theory for QCD at Finite
Density (2008), https://arxiv.org/pdf/0809.4503.pdf

Chapter 14
Meet MarË‡cenko and Pastur
In this Chapter, we investigate the average spectral density for the Wishart-Laguerre
ensemble.
14.1
The MarË‡cenko-Pastur Density
The average density of eigenvalues has the following scaling form for N, M â†’âˆ
(such that c = N/M â‰¤1 is kept ï¬xed)
Ï(x) â†’
1
Î²N ÏMP
 x
Î²N

,
(14.1)
where the MarË‡cenko-Pastur scaling function (the analogue of the semicircle ÏSC(x)
in Eq. (3.6) for the Gaussian ensemble) is independent of Î² and given by [1]
ÏMP(y) =
1
2Ï€y

(y âˆ’Î¶âˆ’)(Î¶+ âˆ’y) ,
(14.2)
for x âˆˆ[Î¶âˆ’, Î¶+]. The edge-points Î¶Â± are given by Î¶âˆ’= (1 âˆ’câˆ’1/2)2 and Î¶+ =
(1 + câˆ’1/2)2.
This scaling function ÏMP(y) has a compact support on the positive semi-axis for
c < 1 (with two soft edges), but becomes singular at the origin if c â†’1 (and the
origin becomes a hard edge). This means that Wishart matrices constructed from
square matrices H exhibit an accumulation of eigenvalues very close to zero.
It is worth stressing that the typical scale of an eigenvalue is âˆ¼O(N) in the WL
case, as opposed to the scale âˆ¼O(
âˆš
N) for the Gaussian ensemble.
Â© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_14
97

98
14
Meet MarË‡cenko and Pastur
14.2
Do It Yourself: The Resolvent Method
Let us now derive the MarË‡cenko-Pastur density using the resolvent (or Stieltjes
transform) method. The partition function (normalization constant) for the Wishart-
Laguerre ensemble reads (after a rescaling xi â†’Î²Nxi)
Z(L)
N,Î² âˆ
 âˆ
0
N

j=1
dx j eâˆ’Î²N
2
N
i=1 xi
N

i=1
xÎ±Î²/2
i

j<k
|x j âˆ’xk|Î² =
 âˆ
0
N

j=1
dx j eâˆ’Î²NV[x] ,
(14.3)
with
V[x] = 1
2

i
xi +
	2/Î² âˆ’1
2N
âˆ’M
2N + 1
2

 
i
ln xi âˆ’1
2N

iÌ¸= j
ln |xi âˆ’x j| . (14.4)
As in Chap.8, the xi are now of O(1) for large N. We can again perform the saddle
point evaluation of the N-fold integral (14.3), but this time there is an additional
subtlety which, if overlooked, leads straight to a nonsensical answer.
The subtlety is that the minimization of the exponent should be carried out within
the set of positive x. In other words, on top of the saddle-point equation, there is an
inequality constraint to satisfy as well, xi > 0
âˆ€i.
One way to handle this constraint is to introduce a penalty function âˆ’Î¼ 
i ln(xi)
in the â€œactionâ€ V[x], with a Lagrange multiplier Î¼. Since âˆ’ln(t) â†’âˆfor t â†’0, it
actsasifeachparticlefeltanextraâ€œinï¬nitewallâ€-typeofrepulsionwhileapproaching
the origin, and thus helps conï¬ning the eigenvalues on the positive semi axis. The
extra wall is then â€œgentlyâ€ removed (Î¼ â†’0) at the end of the calculation.
The saddle-point equations now read for anyi (and for N â‰«1 and N/M = c â‰¤1)
1
2 +
1
2 âˆ’1
2c âˆ’Î¼
 1
xi
= 1
N

jÌ¸=i
1
xi âˆ’x j
.
(14.5)
Multiplying (14.5) by
1
N(zâˆ’xi) and summing over i, we get in analogy with
Eq. (8.18)
1
2G N(z) +
1
2 âˆ’1
2c âˆ’Î¼
 1
N

i
1
xi(z âˆ’xi) = 1
2G2
N(z) + 1
2N Gâ€²
N(z) .
(14.6)
The second term can be expressed in terms of G N(z) using
1
N

i
1
xi(z âˆ’xi) =
1
zN

i
1
xi
+
1
z âˆ’xi

= K + G N(z)
z
,
(14.7)

14.2 Do It Yourself: The Resolvent Method
99
and taking the average G(av)
âˆ(z) = âŸ¨G N(z)âŸ©in the limit N â†’âˆ, we obtain
1
2G(av)
âˆ(z) +
1
2 âˆ’1
2c âˆ’Î¼
 K + G(av)
âˆ(z)
z
= 1
2G(av)2
âˆ
(z) .
(14.8)
Here K is a constant that we assume ï¬nite (by derivation, we have K
=

dxÏ(x)/x).
Note that, had we not included the penalty function parametrized by Î¼ from the
beginning, we would have landed for c = 1 on the equation 1
2G(av)
âˆ(z) = 1
2G(av)2
âˆ
(z),
from which no sensible spectral density could be extracted! This is because the
Wishart eigenvalues cannot equilibrate on the entire real line under a potential
V (x) = x (which is not conï¬ning for x â†’âˆ’âˆ).
It is convenient to set Î³ = (1 âˆ’c)/c > 0. Solving now the quadratic equation
(14.8) for Î¼ â†’0, we get
G(av)
âˆ(z) = 1
2

Â±

Î³ 2 âˆ’4Î³ K z + z2 âˆ’2Î³ z
z
âˆ’Î³
z + 1

.
(14.9)
Setting now z = x âˆ’iÎµ, multiplying up and down by x + iÎµ and using the real
and imaginary part of the square root p and q as in (8.3), we obtain
1
Ï€ ImG(av)
âˆ(x âˆ’iÎµ) =
âˆ’Îµx Â± qx
2Ï€(x2 + Îµ2)
Îµâ†’0+
âˆ’â†’
âˆš(x âˆ’xâˆ’(Î³, K))(x+(Î³, K) âˆ’x)
2Ï€x
,
(14.10)
where it is understood that the (Â±) sign in (14.9) is to be chosen differently in
different x-intervals, in analogy with the Gaussian case. Of course, the right hand
side of (14.10) is only valid for x such that the square root exists. The constants
xÂ±(Î³, K) = Î³

âˆ’2
âˆš
K 2 + K + 2K + 1

.
We now have to ï¬x the constant K by requiring normalization of Ï(x). Using the
integral (for b > a)
 b
a
dx
âˆš(x âˆ’a)(b âˆ’x)
2Ï€x
= 1
4

âˆ’2
âˆš
ab + a + b

,
(14.11)
all we have to do is to assign a â†xâˆ’(Î³, K) and b â†x+(Î³, K), and to solve
1
4

âˆ’2
âˆš
ab + a + b

= 1 for K. This gives K = 1/Î³ .
And for this value of K, the edge points become xÂ±(Î³, 1/Î³ ) â†’(1 Â± 1/âˆšc)2,
which means that we have recovered the MP law (14.2) using the resolvent method.
Congratulations!

100
14
Meet MarË‡cenko and Pastur
0
2
4
6
8
10
12
14
16
x
0
0.1
0.2
0.3
0.4
0.5
Ï(x)
c = 1/2
c = 1/8
Î² = 1
Î² = 2
Î² = 4
Î² = 1
Î² = 2
Î² = 4
Fig. 14.1 Comparison between the MarË‡cenko-Pastur density for two different values of the rect-
angularity ratio c and the corresponding histograms obtained from the numerical diagonalization
of random Wishart matrices (for all possible values of Î²). All histograms are obtained from 5000
Wishart matrices of size N = 100
You can now fully enjoy Fig.14.1, where we show a comparison between the
MarË‡cenko-Pastur density and the histograms obtained by numerical diagonalization
of WL random matrices for different Î²s.
Question 14.1 Wait a second...In the derivation, we said that we had to assume
K ï¬nite and equal to K =

dxÏ(x)/x (because the constant K arises as the
average âŸ¨1
N

i
1
xi âŸ©). Shouldnâ€™t we check that this is consistent with the ï¬nal
result?
â–¶Yes, we should! The integral

dxÏ(x)/x amounts to computing the following
 b
a
dx
âˆš(x âˆ’a)(b âˆ’x)
2Ï€x2
= âˆ’2
âˆš
ab + a + b
4
âˆš
ab
,
(14.12)
and setting a â†(1 âˆ’1/âˆšc)2 and b â†(1 + 1/âˆšc)2. This gives c/(1 âˆ’c),
which is precisely equal to K = 1/Î³ . Bingo!

14.2 Do It Yourself: The Resolvent Method
101
Question 14.2 What if I wanted to use the Coulomb gas technique to derive the
MarË‡cenko-Pastur law?
â–¶The partition function (normalization constant) for the WL ensemble, after
the rescaling xi â†’xiÎ², reads
Z(L)
N,Î² = C(L)
N,Î²

(0,âˆ)N
N

j=1
dx j eâˆ’Î²V[x] ,
(14.13)
where the energy is given this time by V[x] =
1
2

i xi âˆ’Î±
2

i ln xi âˆ’
1
2

iÌ¸= j ln |xi âˆ’x j|.
In the WL case, the gas is in equilibrium under the competing effect of a
linear+logarithmic conï¬ning potential, and the 2D electrostatic repulsion.
Following the same procedure as in Chap.4 (but with the rescaling n(x) â†’
(1/N)n(x/N)), we obtain for the energy functional V[n(x)] = N 2 Ë†V, with
Ë†V[n(x)] =

dx v(x) n(x) âˆ’1
2

dxdxâ€²n(x)n(xâ€²) ln |x âˆ’xâ€²| ,
(14.14)
with v(x) = x/2 âˆ’1
2(1/c âˆ’1) ln x.
The singular integral equation for the equilibrium density readily follows
Pr

dxâ€² nâ‹†(xâ€²)
x âˆ’xâ€² = 1
2 âˆ’1
2
1
c âˆ’1
 1
x .
(14.15)
Try to apply Tricomiâ€™s formula (5.16) â€“ assuming a single-support solution on
[a, b] - and then determine a, b in such a way that the free energy is minimized.
You will discover that nâ‹†(x) = ÏMP(x) as it should.
14.3
Correlations in the Real World and a Quick Example:
Financial Correlations
A huge number of scientiï¬c disciplines, ranging from Physics to Economics, often
need to deal with statistical systems described by a large number of degrees of free-
dom. Thus, understanding and describing the collective behavior of a large numbers
of random variables is one of the most fundamental issues in multivariate Statistics.
More often than not, the problem can be addressed in terms of correlations.
Suppose we are interested in understanding the correlation structure of a system
described in terms of N random variables {x1, . . . , xN}, drawn from a â€“ potentially

102
14
Meet MarË‡cenko and Pastur
unknown, but not changing in time â€“ jpdf p(x). In order to do so, one of the most
obvious operations to perform is to collect, if possible, as many â€œexperimental obser-
vationsâ€ of such variables. Such observations can then be used to compute empirical
time averages of quantities expressed in terms of the random variables. So, let us
assume we have collected M observations â€“ say, equally spaced in time â€“ for each
variable. Quite straightforwardly, one can collect all these numbers in a N Ã— M
matrix X whose entries are xt
i (i = 1, . . . , N, t = 1, . . . , M).
Assuming all variables xt
i are adjusted in order that their sample mean1 is zero
and their sample variance is 1, then the quantity
ci j = 1
M
M

t=1
xt
i xt
j
(14.16)
yields the well known Pearson estimator for the correlation between variables xi and
x j. This is an estimator of the true (or population) correlation2 Ëœci j, which would
be measured exactly for M â†’âˆ, i.e. as more and more observations are added to
the data. However, real life practice always entails working with ï¬nite-sized datasets
(i.e. with ï¬nite M), which introduces some degree of measurement error.
The estimators for each pair of variables in the system can be collected into a
single N Ã— N matrix C = X X T /M, known as the sample correlation matrix of the
data in X, whose entries are given by Eq. (14.16). These amount to N(N âˆ’1)/2
real numbers (diagonal entries are equal to one), which for a large system represent
a whopping amount of information to process. So, what should we make of all
this? Well, a reasonable ï¬rst step could be to compare the empirical correlation
matrix of the system we are interested in with the prediction of a suitably deï¬ned
null hypothesis. In the ï¬rst instance, we could for example look for a null model
describing uncorrelated Gaussian random data and see how our empirical data differ
from it.
By any chance, do we know a random matrix ensemble from which we can draw
this kind of random correlation matrices? Well, of course we do! It is precisely the
Wishart-Laguerre ensemble. As we discussed, the density of eigenvalues is well
known for this ensemble, and it is given by the MarË‡cenko-Pastur law (14.2). This
means that a zero-th order assessment of the statistical signiï¬cance of the correlations
in a large system can be obtained from the comparison of the empirical eigenvalue
spectrum of its correlation matrix with the MarË‡cenko-Pastur law for a system with
the same rectangularity ratio N/M.
A prime example of the procedure outlined above is the analysis of ï¬nancial
correlations. Suppose you want to invest your money in N stocks by forming an
investment portfolio. As the old saying goes, â€œdonâ€™t put your eggs in one basketâ€,
which in ï¬nancial terms translates into â€œdonâ€™t invest all your money in a portfolio
of highly correlated stocksâ€ â€“ not the most effective punchline, admittedly. Hence,
1The sample mean is Â¯xi = (1/M) M
t=1 xt
i , not to be confused with the true mean âŸ¨xiâŸ©p(x), which
is a property of the jpdf p(x).
2The true correlation Ëœci j is a property of the jpdf p(x) of the random variables {x1, . . . , xN}.

14.3 Correlations in the Real World and a Quick Example: Financial Correlations
103
distinguishing signal from noise within ï¬nancial correlation matrices is of paramount
importance to build a well diversiï¬ed portfolio, where the possible losses due to the
adverse movement of a group of stocks can be offset by other groups of stocks.
When an empirical ï¬nancial correlation matrix is diagonalized, one usually ï¬nds
that several eigenvalues are much larger than the expected upper bound of the
MarË‡cenko-Pastur law. The information contained in the associated eigenvectors typ-
ically shows that these are due to the co-movements of groups of highly correlated
stocks belonging to well deï¬ned market sectors (e.g. pharmaceutical, ï¬nancial, etc.).
This kind of random matrix approach to ï¬nancial correlations was initiated in [3, 4]
and since then a considerable number of papers has been devoted to it (see [5] for a
recent account).
References
1. V.A. MarË‡cenko, L.A. Pastur, Math. USSR-Sb 1, 457 (1967)
2. J. Wishart, Biometrika 20A, 32 (1928)
3. L. Laloux, P. Cizeau, J.-P. Bouchaud, M. Potters, Phys. Rev. Lett. 83, 1467 (1999)
4. V. Plerou, P. Gopikrishnan, B. Rosenow, L.A.N. Amaral, H.E. Stanley, Phys. Rev. Lett. 83, 1471
(1999)
5. J. Bun, J.-P. Bouchaud, M. Potters, Phys. Rep. 666, 1 (2017)

Chapter 15
Replicas...
In this Chapter, we add one more powerful tool to our arsenal. The Edwards-Jones
formula, in conjunction with the celebrated replica trick.
15.1
Meet Edwards and Jones
The Edwards-Jones formula [1] allows to write down a formal expression for the
average spectral density Ï(x) of a completely generic ensemble of real symmetric
random matrices H, taking as a starting point just the jpdf of the entries in the upper
triangle, Ï[H].
The formula reads
Ï(x) = âˆ’2
Ï€ N lim
Îµâ†’0+ Im âˆ‚
âˆ‚x

Log Z(x)

,
(15.1)
where
Z(x) =

RN d y exp

âˆ’i
2 yT (xÎµ1 âˆ’H) y

,
(15.2)
where xÎµ = x âˆ’iÎµ.
The average âŸ¨Â·âŸ©is taken with respect to Ï[H], i.e. âŸ¨Â·âŸ©=

d H11 Â· Â· Â· d HN NÏ[H](Â·).
This formula is remarkable: it allows to compute the spectral densityâ€”the
marginal of the jpdf of the eigenvaluesâ€”without knowing the jpdf of eigenvalues!
Only the information about the entries is required as input.
While the formula (15.1) is in principle valid for any ï¬nite N, in practice the
calculations can be carried out until the end only in the limit N â†’âˆ, where several
simpliï¬cations take place.
Â© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_15
105

106
15
Replicas...
15.2
The Proof
The proof is not complicatedâ€”even though there are several subtleties. Recall from
Chap. 2 how the average spectral density is deï¬ned Ï(x) =

1
N
N
i=1 Î´(x âˆ’xi)

.
Recall also the Sokhotski-Plemelj identity: as Îµ â†’0+,
1
x Â± iÎµ â†’Pr
	1
x

âˆ“iÏ€Î´(x) .
(15.3)
This equation provides an interesting identity for the delta function, which we
already used in Chap.8. We can therefore write
Ï(x) =
1
Ï€ N lim
Îµâ†’0+ Im

N

i=1
1
x âˆ’iÎµ âˆ’xi

= âˆ’1
Ï€ N lim
Îµâ†’0+ Im

N

i=1
1
xi + iÎµ âˆ’x

,
(15.4)
where Im stands for the imaginary part, and we changed a sign for later convenience.
Next, we write the denominator in the sum as the derivative of a logarithm. But
the denominator is a complex number: and the logarithms of complex numbers are
nasty beasts1. Anyway, we can choose the principal branch of the logarithmâ€”and
denote it by Logâ€”to write
Ï(x) =
1
Ï€ N lim
Îµâ†’0+ Im âˆ‚
âˆ‚x

N

i=1
Log(xi + iÎµ âˆ’x)

.
(15.5)
Next, we use the identity
Z(x) = (2Ï€)N/2 exp

âˆ’1
2
N

i=1
Log(xi + iÎµ âˆ’x) + i NÏ€
4

,
(15.6)
where Z(x) is given by the multiple integral in (15.2). You can check this identity
with the code [â™ Zmultiple.m]
Now, compare the last two equations. Clearly, the ï¬nal formula would be easily
established if we could replace N
i=1 Log(xi + iÎµ âˆ’x) in (15.5) with something
related to Z(x), using (15.6).
To extract N
i=1 Log(xi + iÎµ âˆ’x) from (15.6), we should take the logarithm on
both sides. There is a small glitch, though, due to another mind-boggling feature of
complex logarithms. Namely, Log(exp(z))may not just be equal to z, for z âˆˆC!2
1For example, Log(z1z2) may not be equal to Log(z1) + Log(z2)!
2For instance, if z = 0.2 âˆ’4.4i, then Log(exp(z)) = 0.2 + 1.88319i.

15.2 The Proof
107
However, we can still write
N

i=1
Log(xi + iÎµ âˆ’x) = âˆ’2Log Z(x) + terms that are killed by âˆ‚
âˆ‚x .
(15.7)
Inserting (15.7) into (15.5), we establish the ï¬nal formula (15.1).
15.3
Averaging the Logarithm
The Edwards-Jones formula (15.1) thus requires computing

Log Z(x)

, where the
average is taken over several realizations of the matrix H.
This means that we should compute

Log Z(x)

=

d H11 Â· Â· Â· d HN NÏ[H]Log

RN d y exp

âˆ’i
2 yT (xÎµ1 âˆ’H) y

,
(15.8)
which is very annoying: the logarithm is right in the way!
We would really need to exchange the order of integrals to perform the average
over H before the average over yâ€”otherwise we would be running the Edwards-
Jones formula backwards and gain nothing!
There are two strategies to circumvent this obstacle, each with their own subtleties.
To know more about the replica method and its applications to spin glass theory see
[2, 3].
15.4
Quenched versus Annealed
Calling the quantity in (15.2) Z(x) is intentional: we wish to interpret it as the
partition function of an associated stat-mech model in the canonical ensemble. The
logarithm of Z will then be the free energy of this model.
Looking again at the multiple integral deï¬ning Z(x), Z(x) =

RN d y exp

âˆ’i
2 yT
(xÎµ1 âˆ’H) y

, we see that it encodes two different â€˜levelsâ€™ of randomness: (i)
the random matrix Hâ€”the so called disorderâ€”and (ii) the dynamical variables
y, which morally3 follow a Gibbs-Boltzmann distribution P(y1, . . . , yN) =
1
Z(x)
exp(âˆ’H(y; H, x)).
Computing now

Log Z(x)

â€”as we shouldâ€”assumes that the two levels of ran-
domness are unfolding on different timescales: ï¬rst, the dynamical variables y need
to equilibrate according to the Gibbs-Boltzmann distribution for a ï¬xed instance of
3â€œMorallyâ€, since the â€œHamiltonianâ€ H is actually complex, so P(y1, . . . , yN) is not a proper
distribution.

108
15
Replicas...
the random matrix Hâ€”and only afterwards the free energy is averaged over the
disorder (different realizations of H).
For these reasons, the disorder is called quenched4: it is there, but it acts slowly.
It only kicks in after the yâ€™s have thermalized.
Computing a quenched disorder average is difï¬cult, but can be attemptedâ€”in the
limit N â†’âˆâ€”usingthesocalledreplicatrick,whichgetsridofthelogarithminside
the integral in (15.8) and allows the integrations over H and y to be interchanged.
More on this later.
A second strategyâ€”which simpliï¬es the calculations considerablyâ€”is to cheat a
bit and treat the disorder as annealed instead.
This means that the associated stat-mech model is described in terms of the
joint set of dynamical variables {y, H}, leading to a partition function Z(ann)(x) =

d Hd y(Â· Â· Â· ).
The dynamical variables y are no longer integrated over at ï¬xed value of the
disorder H, but rather H and y ï¬‚uctuate and thermalize together. A questionable but
widespread way to describe in words an annealed average is: instead of computing
the quenched average âŸ¨Log Z(x)âŸ©â€”as we shouldâ€”move the average inside the
logarithm5, LogâŸ¨Z(x)âŸ©.
Clearly, this slick maneuver forces the logarithm out of the integrals, and allows
for a much quickerâ€”even though not entirely justiï¬ableâ€”computation.
In the following section, we present the annealed calculation to obtain the semi-
circle law for the GOE.6
References
1. S.F. Edwards, R.C. Jones, J. Phys. A: Math. Gen. 9, 1595 (1976)
2. T. Castellani, A. Cavagna, J. Stat. Mech. P05012 (2005)
3. F. Zamponi, Mean ï¬eld theory of spin glasses (2014), https://arXiv.org/pdf/1008.4844.pdf
4Quenched adj. made less severe or intense; subdued or overcome; allayed; squelched.
5For the annealed average, we should more properly write Log Z(ann)(x)â€”with no further average
over H.
6This is only for training purposes. There is no need to use Edwards-Jones when the jpdf of
eigenvalues is known!

Chapter 16
Replicas for GOE
In this Chapter, we apply the Edwards-Jones formula to compute the average spectral
density of the GOE ensemble.
16.1
Wignerâ€™s Semicircle for GOE: Annealed Calculation
The jpdf of entries in the upper triangle of a GOE is
Ï[H] =
N

i=1

exp

âˆ’N H 2
ii/2

/

2Ï€/N
 
i< j

exp

âˆ’N H 2
i j

/

Ï€/N

,
(16.1)
where we have already rescaled the unit variance by a factor 1/N. This has the net
effect of rescaling the eigenvalues by 1/
âˆš
N (why?), so the corresponding spectral
density will have edges between âˆ’
âˆš
2 and
âˆš
2â€“ not growing with N.
For the annealed calculation, we need to compute
Z(ann)(x) =

RN d y
 
iâ‰¤j
d Hi jÏ[H] exp
	
âˆ’i
2 yT (xÏµ1 âˆ’H) y

.
(16.2)
Separating diagonal and off-diagonal elements, and using the notation âŸ¨(Â·)âŸ©=
 
iâ‰¤j d Hi jÏ[H](Â·), we can write
Z(ann)(x) âˆ

RN d y exp
â¡
â£âˆ’i
2 xÏµ
N

i=1
y2
i
â¤
â¦

exp
â¡
â£i
2
N

i=1
Hii y2
i
â¤
â¦
 
exp
â¡
â£i
N

i< j
Hi j yi y j
â¤
â¦

, (16.3)
where we neglect some overall constant terms.
Â© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_16
109

110
16
Replicas for GOE
Expanding ez â‰ˆ1 + z + z2/2 + . . . and using the fact that the entries of H are
independent with âŸ¨Hi jâŸ©= 0 and âŸ¨H 2
i jâŸ©= 1/(N(2 âˆ’Î´i j)), we can write

exp

i
2
N

i=1
Hii y2
i

=
N

i=1

1 + i
2 Hii y2
i âˆ’1
8 H2
ii y4
i + . . .

=
N

i=1

1 âˆ’
1
8N y4
i + . . .

,
(16.4)

exp
â¡
â£i
N

i< j
Hi j yi y j
â¤
â¦

=

i< j

1 + iHi j yi y j âˆ’1
2 H2
i j y2
i y2
j + . . .

=

i< j

1 âˆ’
1
4N y2
i y2
j + . . .

. (16.5)
Re-exponentiating, we can write

exp

i
2
N

i=1
Hii y2
i
 
exp
â¡
â£i
N

i< j
Hi j yi y j
â¤
â¦

â‰ƒexp
â¡
â£âˆ’1
8N
N

i, j=1
y2
i y2
j
â¤
â¦= exp
â¡
â£âˆ’1
8N
 N

i=1
y2
i
2â¤
â¦.
(16.6)
Introducing a Gaussian identity
 âˆ
âˆ’âˆ
dq exp

âˆ’Î±q2 + iÎ³q

âˆexp

âˆ’Î³2/4Î±

(16.7)
with Î³ = N
i=1 y2
i and Î± = 2N yields
Z(ann)(x) âˆ
 âˆ
âˆ’âˆ
dq eâˆ’2Nq2 
RN d y exp

âˆ’i
2 xÏµ
N

i=1
y2
i + iq
N

i=1
y2
i

=
 âˆ
âˆ’âˆ
dq eâˆ’2Nq2 	
R
dy exp
	
âˆ’1
2Ïµy2 âˆ’i
1
2 x âˆ’q

y2


N
, (16.8)
where the y-integral is convergent as Ïµ > 0. Writing X N = exp

NLogX

, we have
Z(ann)(x) âˆ
 âˆ
âˆ’âˆ
dq exp
â¡
â¢â¢â¢â£âˆ’N

2q2 âˆ’1
2Log

2Ï€
Ïµ + i(x âˆ’2q)

 
!"
#
Ï•x(q)
â¤
â¥â¥â¥â¦.
(16.9)
This integral lends itself to a nice Laplaceâ€™s approximation, from which
Z(ann)(x) â‰ˆexp(âˆ’NÏ•x(qâ‹†)) .
(16.10)

16.1 Wignerâ€™s Semicircle for GOE: Annealed Calculation
111
The stationary point qâ‹†is computed as
Ï•â€²
x(qâ‹†) = 0 â‡’4qâ‹†+
1
2qâ‹†âˆ’xÏµ
= 0 â‡’qâ‹†= 1
4

xÏµ Â±
%
x2Ïµ âˆ’2

,
(16.11)
where again xÏµ = x âˆ’iÏµ.
Applying now the Edwards-Jones formulaâ€”in the annealed version and for
N â†’âˆ
Ï(x) = âˆ’2
Ï€N lim
Îµâ†’0+ Im âˆ‚
âˆ‚x Log Z(ann)(x) â‰ˆâˆ’2
Ï€N lim
Îµâ†’0+ Im âˆ‚
âˆ‚x

âˆ’NÏ•x(qâ‹†)

. (16.12)
Using now the chain rule
âˆ‚
âˆ‚x Ï•x(qâ‹†) = qâ‹†â€² âˆ‚
âˆ‚q Ï•x(q)
&&&
q=qâ‹†
 
!"
#
=0
+âˆ‚xÏ•x(q)
&&&
q=qâ‹†=
1
2xÏµ âˆ’4qâ‹†,
(16.13)
and substituting qâ‹†with (16.11), we obtain
Ï(x) = 2
Ï€ lim
Îµâ†’0+ Im
1
xÏµ âˆ“

x2Ïµ âˆ’2
= 1
Ï€ lim
Îµâ†’0+ Im
	
xÏµ Â±
%
x2Ïµ âˆ’2

,
(16.14)
after rationalizing the denominator.
Next, we use again the following short lemma. If âˆša + ib = p + qi, then
p =
1
âˆš
2
%
a2 + b2 + a ,
q = sign b
âˆš
2
%
a2 + b2 âˆ’a .
(16.15)
Using this with a = x2 âˆ’Ïµ2 âˆ’2 and b = âˆ’2Ïµx, and choosing the sign in order
to get a physical solution, we obtain
Ï(x) = 1
Ï€
1
âˆš
2

|x2 âˆ’2| âˆ’x2 + 2 ,
(16.16)
which is indeed zero outside [âˆ’
âˆš
2,
âˆš
2] and equal to Wignerâ€™s semicircle Ï(x) =
1
Ï€
âˆš
2 âˆ’x2 inside, as it should.
In the next section, we embark in the tougher task of using Edwards-Jones in
the correct (quenched) version (without shortcuts). This will require the use of the
celebrated replica trick.

112
16
Replicas for GOE
16.2
Wignerâ€™s Semicircle: Quenched Calculation
We use now Edwards-Jones in the full-ï¬‚edged form
Ï(x) = âˆ’2
Ï€N lim
Ïµâ†’0+ Im âˆ‚
âˆ‚x
'
Log

RN d y exp
	
âˆ’i
2 yT (xÏµ1 âˆ’H) y

 (
,
(16.17)
where the average âŸ¨Â·âŸ©is taken again with respect to Ï[H], i.e. âŸ¨Â·âŸ©=

d H11 Â· Â· Â· d HN N
Ï[H](Â·) and xÏµ = x âˆ’iÏµ.
Recall that we cannot perform the y-integral before taking the average over H,
otherwise we would be running the Edwards-Jones formula backwards! On the other
hand, we cannot exchange the two integrations as they stand, due to the logarithm
standing right in the middle. How to proceed then?
Using the replica identity in the form
âŸ¨Log Z(x)âŸ©= lim
nâ†’0
1
n LogâŸ¨Z(x)nâŸ©,
(16.18)
we replicate the y-integral n (integer) times, and we blindly hope that the analytical
continuation to n in the vicinity of zero makes sense. The formalism and notation
we shall use in the following are similar to those introduced ï¬rst in [1].
Using again
Ï[H] =
N

i=1

exp

âˆ’N H 2
ii/2

/

2Ï€/N
 
i< j

exp

âˆ’N H 2
i j

/

Ï€/N

,
(16.19)
we want to compute the replicated partition function
âŸ¨Z(x)nâŸ©=
 â›
â
iâ‰¤j
d Hi j
â
â 
N

i=1
eâˆ’N H 2
ii/2
âˆš2Ï€/N

i< j
eâˆ’N H 2
i j
âˆšÏ€/N Ã—
Ã—

RNn
 n
a=1
d ya

exp
â¡
â£âˆ’i
2
N

i, j=1
n

a=1
yia

xÏµÎ´i j âˆ’Hi j

y ja
â¤
â¦.
(16.20)
Now that the innermost integral has been â€œreplicatedâ€ n-times, we can exchange
the order of integrations to get
âŸ¨Z(x)nâŸ©=

RNn
 n

a=1
d ya

eâˆ’i xÏµ
2
N
i=1
n
a=1 y2
ia
  N

i=1
d Hii
âˆš2Ï€/N

eâˆ’N N
i=1 H2
ii /2+ i
2
N
i=1 Hii
n
a=1 y2
ia Ã—
Ã—
 â›
â
i< j
d Hi j
âˆšÏ€/N
â
â eâˆ’N N
i< j H2
i j +i 
i< j
n
a=1 yia Hi j y ja .
(16.21)

16.2 Wignerâ€™s Semicircle: Quenched Calculation
113
Neglecting constants, we can perform the two multiple Gaussian integrals involv-
ing H using (16.7) repeatedly, with Î± = N/2 (or N) and Î³ = (1/2) n
a=1 y2
ia (or
Î³ = n
a=1 yiay ja) to get
âŸ¨Z(x)nâŸ©=

RNn
 n
a=1
d ya

exp
â¡
â£âˆ’i xÏµ
2
N

i=1
n

a=1
y2
ia âˆ’1
8N
N

i=1

a
y2
ia
2
âˆ’1
4N

i< j
 n

a=1
yiay ja
2â¤
â¦,
(16.22)
which can be more compactly rewritten as
âŸ¨Z(x)nâŸ©=

RNn
 n

a=1
d ya

exp
â¡
â£âˆ’i xÏµ
2
N

i=1
n

a=1
y2
ia âˆ’1
8N
N

i, j=1
 n

a=1
yia y ja
2â¤
â¦.
(16.23)
In order to proceed further, we introduce the following normalized density
Î¼(âˆ’â†’y ) = 1
N
N

i=1
n
a=1
Î´(ya âˆ’yia) ,
(16.24)
where the n-dimensional vector âˆ’â†’y = (y1, . . . , yn).
You can now check by direct substitution that the second term in the exponential
in (16.23) can be rewritten as
âˆ’1
8N
N

i, j=1
 n

a=1
yiay ja
2
= âˆ’N
8

dâˆ’â†’y dâˆ’â†’
w Î¼(âˆ’â†’y )Î¼(âˆ’â†’
w )
 n

a=1
yawa
2
,
(16.25)
where dâˆ’â†’y = n
a=1 dya.
We can enforce the deï¬nition (16.24) using the following functional-integral rep-
resentation of the identity
1 =

DÎ¼D Ë†Î¼ exp
â¡
â£âˆ’i

dâˆ’â†’y Ë†Î¼(âˆ’â†’y )
â›
âNÎ¼(âˆ’â†’y ) âˆ’

i

a
Î´(ya âˆ’yia)
â
â 
â¤
â¦,
(16.26)
which leads to

114
16
Replicas for GOE
âŸ¨Z(x)nâŸ©=

DÎ¼D Ë†Î¼ exp
â¡
â£âˆ’iN

dâˆ’â†’y Î¼(âˆ’â†’y )Ë†Î¼(âˆ’â†’y ) âˆ’N
8

dâˆ’â†’y dâˆ’â†’
w Î¼(âˆ’â†’y ) Î¼(âˆ’â†’
w )
 n

a=1
yawa
2â¤
â¦Ã—
Ã—

RNn
 n

a=1
d ya

exp

âˆ’i xÏµ
2
N

i=1
n

a=1
y2
ia + i

i

dâˆ’â†’y Ë†Î¼(âˆ’â†’y )

a
Î´(ya âˆ’yia)

.
(16.27)
In the above equations, DÎ¼D Ë†Î¼ denotes again functional integration, which was
already used in Chap.4. If you want to know more on this, see [2].
The multiple integral

RNn
n
a=1 d ya

(Â· Â· Â· ) is just a collection of N-identical
copies of a single integral, hence

RNn
 n
a=1
d ya

exp

âˆ’i xÏµ
2
N

i=1
n

a=1
y2
ia + i

i

dâˆ’â†’y Ë†Î¼(âˆ’â†’y )

a
Î´(ya âˆ’yia)

=
-
Rn dâˆ’â†’y exp

âˆ’i xÏµ
2
n

a=1
y2
a + i

dâˆ’â†’y Ë†Î¼(âˆ’â†’y )

a
Î´(ya âˆ’y1a)
.N
=
-
Rn dâˆ’â†’y exp

âˆ’i xÏµ
2
n

a=1
y2
a + i Ë†Î¼(âˆ’â†’y )
.N
,
(16.28)
where in the last line we used the n delta functions to kill the multiple integral.
Exponentiating the last line of (16.28), we can eventually write
âŸ¨Z(x)nâŸ©=

DÎ¼D Ë†Î¼ exp
/
NSn[Î¼, Ë†Î¼; x]
0
,
(16.29)
where the action is given by
Sn[Î¼, Ë†Î¼; x] = âˆ’i

dâˆ’â†’y Î¼(âˆ’â†’y ) Ë†Î¼(âˆ’â†’y ) âˆ’1
8

dâˆ’â†’y dâˆ’â†’
w Î¼(âˆ’â†’y )Î¼(âˆ’â†’
w )
 n

a=1
yawa
2
+ Log

Rn dâˆ’â†’y exp

âˆ’i xÏµ
2
n

a=1
y2
a + i Ë†Î¼(âˆ’â†’y )

.
(16.30)
The expression (16.29) lends itself to a nice saddle-point evaluation for N â†’âˆ.
The only catch is that in doing so we would reverse the right order of limits: instead
of taking n â†’0 ï¬rst, and N â†’âˆafterwards, we are going to do the opposite! This
procedure is not mathematically justiï¬ed, but we will proceed as if it were.
16.2.1
Critical Points
Finding the critical points of this action yields the two equations

16.2 Wignerâ€™s Semicircle: Quenched Calculation
115
Î´S
Î´Î¼ = 0 â‡’âˆ’i Ë†Î¼â‹†(âˆ’â†’y ) = 1
4

dâˆ’â†’
w Î¼â‹†(âˆ’â†’
w )
 n

a=1
yawa
2
,
(16.31)
Î´S
Î´ Ë†Î¼ = 0 â‡’Î¼â‹†(âˆ’â†’y ) =
exp

âˆ’i xÏµ
2
n
a=1 y2
a + i Ë†Î¼â‹†(âˆ’â†’y )


Rn dâˆ’â†’y â€² exp

âˆ’i xÏµ
2
n
a=1 yâ€²2
a + i Ë†Î¼â‹†(âˆ’â†’y â€²)
 .
(16.32)
Inserting (16.32) into (16.31), we get
âˆ’i Ë†Î¼â‹†(âˆ’â†’y ) =
1
4

dâˆ’â†’
w exp

âˆ’i xÏµ
2
n
a=1 w2
a + i Ë†Î¼â‹†(âˆ’â†’
w )
 âˆ’â†’y Â· âˆ’â†’
w
2

dâˆ’â†’
w exp

âˆ’i xÏµ
2
n
a=1 w2a + i Ë†Î¼â‹†(âˆ’â†’
w )

,
(16.33)
where both integrals on the r.h.s. run over Rn.
In order to proceed, we have to make assumptions on the behavior of Î¼â‹†and Ë†Î¼â‹†
upon permutation of replica indices. There is a good body of researchâ€”although
not yet a formal proofâ€”pointing to the exactness of the replica-symmetric high-
temperature solution, i.e. the one preserving permutation-symmetry among replicas,
and rotational symmetry in the space of replicas.
This simply means that we should look for a solution of (16.31) and (16.32) in
the form Î¼â‹†(âˆ’â†’y ) = Î¼â‹†(y), with y = |âˆ’â†’y |, and similarly for Ë†Î¼â‹†.
Introducing n-dimensional spherical coordinates, we can rewrite (16.33) under
the replica-symmetric assumption as
âˆ’iË†Î¼â‹†(y) =
y2
4
 âˆ
0
dÏ‰ Ï‰nâˆ’1 exp[âˆ’i
2 xÏµÏ‰2 + iË†Î¼â‹†(Ï‰)]Ï‰2  Ï€
0 dÏ† (sin Ï†)nâˆ’2 (cos Ï†)2
 âˆ
0
dÏ‰ Ï‰nâˆ’1 exp[âˆ’i
2 xÏµÏ‰2 + iË†Î¼â‹†(Ï‰)]
 Ï€
0 dÏ† (sin Ï†)nâˆ’2
,
(16.34)
where Ï† is taken as the angle between âˆ’â†’y and âˆ’â†’
w , and the other angular integrals
cancel out between numerator and denominator.
Performing the remaining angular integrals, and after an integration by parts in
the denominator, we get
i Ë†Î¼â‹†(y) =
(n/2)n
2(1 + n/2)
y2
4
 âˆ
0 dÏ‰ Ï‰n+1G(Ï‰)
 âˆ
0 dÏ‰ Ï‰nGâ€²(Ï‰) ,
(16.35)
where G(Ï‰) := exp[âˆ’i
2 xÏµÏ‰2 + i Ë†Î¼â‹†(Ï‰)]. In the replica limit n â†’0, we obtain
i Ë†Î¼â‹†(y) = y2
4
 âˆ
0 dÏ‰ Ï‰G(Ï‰)
 âˆ
0 dÏ‰ Gâ€²(Ï‰) = C(x)y2 ,
(16.36)
where C(x) can be determined self-consistently using

116
16
Replicas for GOE
 âˆ
0
dÏ‰ Ï‰G(Ï‰)
 âˆ
0
dÏ‰ Gâ€²(Ï‰) =
 âˆ
0
dÏ‰ Ï‰ exp

âˆ’i
2 xÏµÏ‰2 + C(x)Ï‰2
 âˆ
0
dÏ‰ exp

âˆ’i
2 xÏµÏ‰2 + C(x)Ï‰2

2Ï‰

âˆ’i
2 xÏµ + C(x)
 =
1
2

âˆ’i
2 xÏµ + C(x)
 ,
(16.37)
so that
C(x) =
1
8

âˆ’i
2 xÏµ + C(x)
 â‡’C(x) = 1
4

ixÏµ Â±
%
2 âˆ’x2Ïµ

.
(16.38)
16.2.2
One Step Back: Summarize and Continue
Let us now pause for a second and recap what we are doing. We started from the
Edwards-Jones identity
Ï(x) = âˆ’2
Ï€N lim
Îµâ†’0+ Im âˆ‚
âˆ‚x
'
Log Z(x)
(
,
(16.39)
where
Z(x) =

RN d y exp
	
âˆ’i
2 yT (xÏµ1 âˆ’H) y

,
(16.40)
and xÏµ = x âˆ’iÏµ.
The average of the logarithm is performed by using the replica identity
âŸ¨Log Z(x)âŸ©= lim
nâ†’0
1
n LogâŸ¨Z(x)nâŸ©,
(16.41)
which in turn (for large N) can be approximated via a saddle-point evaluation from
(16.29) as
âŸ¨Z(x)nâŸ©=

DÎ¼D Ë†Î¼ exp
/
NSn[Î¼, Ë†Î¼; x]
0
âˆ¼exp

NSn[Î¼â‹†, Ë†Î¼â‹†; x]

.
(16.42)
Combining (16.39), (16.41) and (16.42), we obtain
Ï(x) = âˆ’2
Ï€
lim
Ïµâ†’0+ Im lim
nâ†’0
1
n
âˆ‚
âˆ‚x Sn[Î¼â‹†, Ë†Î¼â‹†; x] .
(16.43)
The derivative with respect to x only acts over the last term in the action (16.30),
because x appears explicitly (not through Î¼â‹†or Ë†Î¼â‹†) only there, and the action is sta-
tionary at the saddle point. Taking the derivative and writing the integral in spherical
n-dimensional coordinates, we obtain

16.2 Wignerâ€™s Semicircle: Quenched Calculation
117
Ï(x) = âˆ’2
Ï€
lim
Ïµâ†’0+ Im lim
nâ†’0
1
n
âˆ’i
2
 âˆ
0 dy yn+1 exp

âˆ’i xÏµ
2 y2 + C(x)y2
 âˆ
0 dy ynâˆ’1 exp

âˆ’i xÏµ
2 y2 + C(x)y2
.
(16.44)
Performing the integrals and simplifying
Ï(x) = 1
Ï€ lim
Ïµâ†’0+ Re
1
âˆ’2C(x) + ixÏµ
.
(16.45)
Recalling that C(x) = 1
4
1
ixÏµ Â±

2 âˆ’x2Ïµ
2
and xÏµ = x âˆ’iÏµ, we can ï¬rst extract
the real and imaginary part of C(x) using the lemma in (16.15). Therefore we can
write
C(x) = PÏµ(x) + iQÏµ(x) ,
(16.46)
with
PÏµ(x) =
1
âˆš
2
%
2 âˆ’x2 + Ïµ2 +

(2 âˆ’x2 + Ïµ2)2 + (2Ïµx)2
(16.47)
QÏµ(x) = sign(2Ïµx)
âˆš
2
%
(2 âˆ’x2 + Ïµ2)2 + (2Ïµx)2 âˆ’(2 âˆ’x2 + Ïµ2) .
(16.48)
Hence
Re
1
âˆ’2C(x) + ixÏµ
=
âˆ’2PÏµ(x)
4PÏµ(x)2 + (x âˆ’2QÏµ(x))2 .
(16.49)
In the limit Ïµ â†’0+ and for âˆ’
âˆš
2 < x <
âˆš
2, PÏµ and QÏµ converge to
P0(x) = Â±
âˆš
2 âˆ’x2
4
(16.50)
Q0(x) = x
4 ,
(16.51)
from which
Ï(x) = 1
Ï€

2 âˆ’x2 ,
(16.52)
i.e. Wignerâ€™s semicircle law as expected.
References
1. G.J. Rodgers, A.J. Bray, Phys. Rev. B 37, 3557 (1988)
2. R. MacKenzie, Path Integral Methods and applications (2000), https://arxiv.org/abs/quant-ph/
0004090

Chapter 17
Born to Be Free
We have so far dealt with the spectral properties of individual random matrix ensem-
bles. You may have been wondering (or not) what happens when you sum or multiply
random matrices belonging to different ensembles. In this Chapter we present an
overview of the rather complicated tool you will need to tackle this problem: free
probability theory [1, 2].
17.1
Things About Probability You Probably
Already Know
Two random variables X1 and X2, with pdfs Ï1 and Ï2, are said to be statistically
independent when the combined random variable (X1, X2) has a factorized jpdf of
the form
Ï1,2(x1, x2) = Ï1(x1)Ï2(x2) .
(17.1)
Statistical independence means that averages factorize as well (âŸ¨X1X2âŸ©=
âŸ¨X1âŸ©âŸ¨X2âŸ©), which in turn means that their covariance is zero, and is key to ï¬nding
the distribution of the sum of random variables. Let us consider a random variable
X with pdf Ï(x). Its characteristic function Ï•(t) is deï¬ned as
Ï•(t) = âŸ¨eit XâŸ©=

dx Ï(x) eitx ,
(17.2)
i.e. it is the Fourier transform of its pdf.
You should easily realize that the factorized jpdf in Eq.(17.1) implies that char-
acteristic functions are multiplicative upon the addition of statistically indepen-
dent random variables, i.e. Ï•1,2(t1, t2) = Ï•1(t1)Ï•2(t2). Even more simply, we can
Â© The Author(s) 2018
G. Livan et al., Introduction to Random Matrices, SpringerBriefs
in Mathematical Physics, https://doi.org/10.1007/978-3-319-70885-0_17
119

120
17
Born to Be Free
introduce the logarithm of the characteristic function h(t) = log Ï•(t), the so called
cumulant generating function, which is obviously additive upon the addition of ran-
dom variables:
h1,2(t1, t2) = h1(t1) + h2(t2) .
(17.3)
Therefore, the problem of ï¬nding the pdf of the sum of two independent random
variables X1 and X2 reduces to a simple â€œalgorithmâ€: compute the characteristic
functions of X1 and X2 from their pdfs, form the the cumulant generating function of
the sum X1+X2 via the additive law (17.3), compute the corresponding characteristic
function via exponentiation, and eventually compute the pdf of the sum X1 + X2 via
inverse Fourier transform.
17.2
Freeness
So, is there a generalization of statistical independence that will allow us to compute
the eigenvalue spectrum of sums of random matrices? At ï¬rst it might be tempting
to guess that the statistical independence of two scalar random variables could be
straightforwardly generalized to the case of two random matrices X1 and X2 by
merely requiring the mutual independence of all entries. Unfortunately, this is not the
case,asindependententriesarenotenoughtodestroyallpossibleangularcorrelations
between the eigenbases of two matrices.
Thepropertythat generalizes statistical independencetorandommatrices is that of
freeness. The theory of free probability was initiated a few years ago by the pioneering
works by Voiculescu and Speicher as an abstract approach to Von Neumann algebras,
and only later it was shown to have a concrete realization in terms of random matrices.
Here is how freeness works. Let us consider two N Ã— N random matrices X1 and
X2, and let us introduce the following operator
Ï„(X) = lim
Nâ†’âˆ
1
N Tr(X) .
(17.4)
Thetwomatrices X1 and X2 aresaidtobefreeifforallintegersn1, m1, n2, m2, . . . â‰¥
1 we have
Ï„

Xn1
1 âˆ’Ï„

Xn1
1
 
Xm1
2 âˆ’Ï„

Xm1
2
 
Xn2
1 âˆ’Ï„

Xn2
1
 
Xm2
2
âˆ’Ï„

Xm2
2

. . .

=
Ï„

Xn1
2 âˆ’Ï„

Xn1
2
 
Xm1
1 âˆ’Ï„

Xm1
1
 
Xn2
2 âˆ’Ï„

Xn2
2
 
Xm2
1
âˆ’Ï„

Xm2
1

. . .

=
0 .
(17.5)
Not so straightforward, is it?

17.2 Freeness
121
It might help to put the above deï¬nition into words. Two random matrices are free
if the traces of all non-commutative products of matrix polynomials, whose traces
are zero, are zero. Still not very intuitive, right? Well, unfortunately it does not get
much better than that, but some intuition can be gained by exploring some concrete
examples of the above deï¬nition. For example, Eq.(17.5) reduces to Ï„(X1X2) =
Ï„(X1)Ï„(X2) when n1 = m1, and it reduces to Ï„(X2
1 X2
2) = Ï„(X2
1)Ï„(X2
2) when n1 =
m1 = 2. As you should quickly realize, these equations generalize the moment
factorization rules for statistically independent variables, and you can verify that all
such relations for higher order moments can be obtained from Eq.(17.5).
However, the interesting part comes into play when we explore cases in which
matrix non-commutativity kicks in. For example, you can easily work out the fol-
lowing result from (17.5) for n1 = n2 = m1 = m2 = 1:
Ï„(X1X2X1X2) = Ï„ 2(X1)Ï„(X2
2) + Ï„(X2
1)Ï„ 2(X2) âˆ’Ï„(X2
1)Ï„(X2
2) .
(17.6)
This result has no counterpart in â€œconventionalâ€ probability theory. Hopefully,
this will convince you that freeness essentially represents a generalization of moment
factorization.
17.3
Free Addition
Let us now put freeness to work.
Suppose we want to compute the average spectral density of the sum of large (i.e.
N â†’âˆ) random matrices belonging to two different ensembles.
The ï¬rst ingredient we need is our old friend the resolvent, which we introduced
in Chap.8. Now, given the resolvent G(av)
âˆ(z) of a given ensemble, let us introduce
its functional inverse B(z):
G(av)
âˆ(B(z)) = B

G(av)
âˆ(z)

= z .
(17.7)
The above function is known as the Blue function [3]. In case you are wondering:
yes, it is called Blue because it is the inverse of the Greenâ€™s function.
The last ingredient we need is the so called R-transform. Blue functions usually
display a singular behavior at the origin, and the R-transform is just deï¬ned as a
Blue function minus its singular part:
R(z) = B(z) âˆ’1
z .
(17.8)
We are all set now. Let us consider random matrices X1 and X2 belonging to
ensembles characterized by resolvents Gav
âˆ,1(z) and Gav
âˆ,2(z), respectively. Let us
form, through Eqs. (17.7) and (17.8), the corresponding R-transforms R1 and R2.

122
17
Born to Be Free
The R-transform of the sum X = X1 + X2 is then simply given by the sum of the
two R-transforms:
R(z) = R1(z) + R2(z) .
(17.9)
The above addition rule is the free counterpart of (17.3) for the moment generating
functions of statistically independent random variables. Just like in that case, this rule
provides a simple addition â€œalgorithmâ€ for free random matrices, whose ï¬rst part
has been outlined above. Once the R-transform of the sum has been computed, the
corresponding Blue function and resolvent can be obtained through Eqs.(17.8) and
(17.7). Once that has been done, the eigenvalue density can be derived from the
resolvent via Eq.(8.8).
17.4
Do It Yourself
Enough with theory now: let us see free calculus at work on a concrete example.
All we need is the spectral density of large hermitian random matrix ensembles.
So, how about the eigenvalue density of the free sum of some of our usual suspects?
For example, let us consider a mixture of a GOE matrix H and a Wishart matrix W
S = pH + (1 âˆ’p)W ,
(17.10)
where p âˆˆ[0, 1].
For both ensembles we already have computed the resolvents (Eqs.(8.20) and
(14.9) with K = 1/Î±). The functional inverse of those functions yield the Blue func-
tions via Eq.(17.7), and the R-transforms are immediately obtained via Eq.(17.8).
Please verify that they are given by the following functions for the GOE and Wishart
ensembles respectively:
RGOE(z) = z
2
RW(z) = Î± + 1
1 âˆ’z .
(17.11)
Using the R-transformâ€™s scaling property RcH(z) = cRH(cz) (see the box below),
we can adapt the addition rule (17.9) to the present problem as follows:
RS(z) = p RGOE(pz) + (1 âˆ’p)RW ((1 âˆ’p)z) .
(17.12)
Plugging the functions in (17.11) into the equation above gives
RS(z) = p2
2 z + (1 âˆ’p)(1 âˆ’Î±)
1 âˆ’(1 âˆ’p)z
,
(17.13)

17.4 Do It Yourself
123
-1
-0.5
0
0.5
1
1.5
2
2.5
3
3.5
4
x
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Ï(x)
p = 0.3
p = 0.3
p = 0.5
p = 0.5
p = 0.7
p = 0.7
Fig. 17.1 Numerical check of the density obtained for the free addition of GOE and Wishart
random matrices from the solution of Eq.(17.14). The examples shown refer to different values of
the parameter p that quantiï¬es the relative weight between the two ensembles (see Eq.(17.10))
and the corresponding resolvent is obtained as BS(GS(z)) = z, where BS(z) =
RS(z) + 1/z is the Blue function. The equation for the resolvent reads
z = p2
s GS(z) +
(1 âˆ’p)(1 âˆ’Î±)
1 âˆ’(1 âˆ’p)GS(z) +
1
GS(z) .
(17.14)
This is a third degree equation yielding, in general, one real solution and two
complex conjugate ones for a given ï¬xed z. The relationship between the eigenvalue
density and the resolvent is the one in Eq. (8.8), and that informs us that we will need
to select the solution with a positive imaginary part. All this is done, and numerically
veriï¬ed, in the code [â™ GOE_Wishart_Sum.m]. An example of the output that
can be obtained is shown in Fig.17.1.
Our goal in this Chapter was just to provide you with a short overview of the
powerful tools free probability has to offer. There are plenty of papers out there if
youâ€™d like to know more. For example, you might have a look at the nice review
article in [4], which also details some of the many applications that free random
matrices have in quantitative ï¬nance.

124
17
Born to Be Free
Question 17.1 Where does the scaling property of the R-transform come from?
â–¶It is inherited from the scaling properties of our good old friend the resolvent.
Indeed, multiplying a matrix H by a constant c rescales the eigenvalues by
the same factor c. Hence, from Eqs.(8.4) and (8.5) it is easy to prove that
the two corresponding resolvents are related to each other through this simple
relationship: G(av)
âˆ,cH = G(av)
âˆ,H(z/c)/c. We can then write the equation for the
Blue function BcH
z = G(av)
âˆ,cH(BcH(z)) = 1
c G(av)
âˆ,H
1
c BcH(z)

,
(17.15)
which shows that BcH(z) = cBH(cz). We then have the following for the cor-
responding R functions:
cRH(cz) = cBH(cz) âˆ’1
z = BcH(z) âˆ’1
z = RcH(z) .
(17.16)
Question 17.2 We know that the sum of two Gaussian scalar random variables is
again Gaussian distributed. Is there an equivalent statement for the free addition
of Gaussian random matrices?
â–¶Given the tools provided in this Chapter you should be able to show that
the semicircle distribution is stable under free addition, i.e. if you free sum M
matrices each having the semicircle as spectral density, you still end up with a
matrix whose spectral density is a semicircle.
References
1. D.V. Voiculescu, J. Oper. Theory 18, 223 (1987)
2. A. Nica, R. Speicher, Duke Math. J. 92, 553 (1998)
3. A. Zee, Nucl. Phys. B 474, 726 (1996)
4. Z. Burda, A. Jarosz, M.A. Nowak, J. Jurkiewicz, G. Papp, I. Zahed, Quant. Fin. 11, 1103 (2011)


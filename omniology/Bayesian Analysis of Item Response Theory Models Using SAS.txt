support.sas.com/bookstore
Bayesian Analysis
of Item Response Theory
Models Using SAS
®
Clement A. Stone and Xiaowen Zhu

The correct bibliographic citation for this manual is as follows: Stone, Clement A., and Xiaowen Zhu. 2015. Bayesian 
Analysis of Item Response Theory Models Using SAS®. Cary, NC: SAS Institute Inc. 
Bayesian Analysis of Item Response Theory Models Using SAS® 
Copyright © 2015, SAS Institute Inc., Cary, NC, USA 
ISBN 978-1-62959-650-1 (Hardcopy) 
ISBN 978-1-62959-679-2 (EPUB)   
ISBN 978-1-62959-680-8 (MOBI) 
ISBN 978-1-62959-678-5 (PDF) 
All rights reserved. Produced in the United States of America.  
For a hard-copy book: No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in 
any form or by any means, electronic, mechanical, photocopying, or otherwise, without the prior written permission of 
the publisher, SAS Institute Inc. 
For a web download or e-book: Your use of this publication shall be governed by the terms established by the vendor 
at the time you acquire this publication. 
The scanning, uploading, and distribution of this book via the Internet or any other means without the permission of the 
publisher is illegal and punishable by law. Please purchase only authorized electronic editions and do not participate in 
or encourage electronic piracy of copyrighted materials. Your support of others’ rights is appreciated. 
U.S. Government License Rights; Restricted Rights: The Software and its documentation is commercial computer 
software developed at private expense and is provided with RESTRICTED RIGHTS to the United States Government. 
Use, duplication or disclosure of the Software by the United States Government is subject to the license terms of this 
Agreement pursuant to, as applicable, FAR 12.212, DFAR 227.7202-1(a), DFAR 227.7202-3(a) and DFAR 227.7202-4 
and, to the extent required under U.S. federal law, the minimum restricted rights as set out in FAR 52.227-19 (DEC 
2007). If FAR 52.227-19 is applicable, this provision serves as notice under clause (c) thereof and no other notice is 
required to be affixed to the Software or documentation. The Government's rights in Software and documentation shall 
be only those set forth in this Agreement. 
SAS Institute Inc., SAS Campus Drive, Cary, North Carolina 27513-2414. 
March 2015 
SAS® and all other SAS Institute Inc. product or service names are registered trademarks or trademarks of SAS Institute 
Inc. in the USA and other countries. ® indicates USA registration. 
Other brand and product names are trademarks of their respective companies. 

 
 
 
Contents 
 
About this Book ...............................................................................................ix 
About the Authors ......................................................................................... xiii 
Acknowledgments .......................................................................................... xv 
Chapter 1: Item Response Theory ................................................................... 1 
Introduction ........................................................................................................................................... 1 
Overview of IRT Models and Their Application .................................................................................. 2 
Visualization of IRT Models ........................................................................................................... 3 
Organization of the Chapter .......................................................................................................... 4 
Unidimensional IRT Models for Dichotomously Scored Responses ............................................... 5 
Unidimensional IRT Models for Polytomously Scored Responses ................................................. 5 
The Graded Response Model ........................................................................................................ 6 
Muraki’s Rating Scale Model ........................................................................................................ 8 
The Partial Credit Model and Extensions ..................................................................................... 8 
The Nominal Response Model .................................................................................................... 11 
Other Testing Effects in Unidimensional IRT Models ..................................................................... 12 
Locally Dependent Item Sets (Testlets) ..................................................................................... 13 
Rater Effects ................................................................................................................................. 13 
IRT Models for Multidimensional Response Data ........................................................................... 14 
Differential Item Functioning and Mixture IRT Models ................................................................... 16 
Hierarchical Models—Multilevel and Random Effects IRT Models ............................................... 17 
Evaluation of IRT Model Applications ............................................................................................... 19 
Dimensionality .............................................................................................................................. 19 
Local Independence ..................................................................................................................... 19 
Form of the IRT Model ................................................................................................................. 20 
Speededness ................................................................................................................................. 20 
Model Fit ........................................................................................................................................ 20 
IRT Model Parameter Estimation ...................................................................................................... 22 
Chapter 2: Bayesian Analysis ........................................................................ 25 
Introduction ......................................................................................................................................... 25 
Elements of Statistical Models and Inference ................................................................................. 25 
Frequentist and Bayesian Approaches to Statistical Inference ..................................................... 26 
Use of Bayesian Analysis to Estimate a Proportion—An Example ................................................ 28 
Choosing a Prior Distribution ...................................................................................................... 29 
Computing the Posterior and Drawing Inferences ................................................................... 30 
Comparing Bayesian and Frequentist Estimates for Proportions .......................................... 30 

iv  
Bayesian Estimation and the MCMC Method .................................................................................. 31 
SAS Code for Implementing a Metropolis Sampler to Estimate a Proportion ....................... 32 
MCMC and the Gibbs Sampler .................................................................................................... 35 
Burn-In and Convergence .................................................................................................................. 35 
Informative and Uninformative Prior Distributions .......................................................................... 38 
Model Comparisons ............................................................................................................................ 39 
Deviance Information Criterion ................................................................................................... 39 
Bayes Factor ................................................................................................................................. 40 
Model Fit and Posterior Predictive Model Checks .......................................................................... 41 
Chapter 3: Bayesian Estimation of IRT Models Using PROC MCMC ............... 43 
Introduction ......................................................................................................................................... 43 
Summary of PROC MCMC Statements for IRT Model Estimation ................................................ 44 
MCMC Sampling Algorithms ....................................................................................................... 45 
Built-In Distributions .................................................................................................................... 46 
Density Plots ................................................................................................................................. 47 
Arbitrary Distributions .................................................................................................................. 48 
PROC MCMC Template for IRT Model Estimation .......................................................................... 49 
How PROC MCMC Processes the SAS Data Set ...................................................................... 51 
More on the RANDOM Statement ............................................................................................... 51 
Model Identification ...................................................................................................................... 52 
Example Use of the Template for the 1-Parameter IRT Model ................................................ 52 
Example Output from Estimating the 1P IRT Model ................................................................. 54 
Strategies to Improve Convergence of the Monte Carlo Chain ..................................................... 59 
Chain Thinning .............................................................................................................................. 59 
Parameter Blocking ...................................................................................................................... 61 
Model Re-parameterizing ............................................................................................................ 64 
Analysis of Multiple Chains—Gelman-Rubin Test for Convergence ............................................. 67 
Informative, Uninformative, and Hierarchical Priors ....................................................................... 70 
Informative Priors ......................................................................................................................... 70 
Uninformative Priors .................................................................................................................... 70 
Hierarchical Priors ........................................................................................................................ 71 
Effect of Different Priors .............................................................................................................. 72 
Treatment of Item Parameters as Random Effects ......................................................................... 73 
Model Fit and Model Comparisons ................................................................................................... 76 
Comparison with WinBUGS ............................................................................................................... 76 
The OUTPOST SAS Data Set ............................................................................................................. 77 
Autocall Macros for Post-Processing PROC MCMC Data Sets .................................................... 78 
Preliminary Item Analyses .................................................................................................................. 79 
Chapter 4: Bayesian Estimation of Unidimensional IRT Models for 
Dichotomously Scored Items ......................................................................... 81 
Introduction ......................................................................................................................................... 81 
The 1-Parameter IRT or Rasch Model .............................................................................................. 82 
Use of a Normal Ogive Link Function ............................................................................................... 85 
The 2-Parameter IRT Model ............................................................................................................... 85 

v 
 
A 2-Parameter IRT Model with a Hierarchical Prior ........................................................................ 91 
The 3-Parameter IRT Model ............................................................................................................... 94 
Comparison of Results Based on MML Estimation....................................................................... 105 
Display of Item Response Functions .............................................................................................. 107 
Adding Elements to the Graph: Uncertainty in Item Parameters .......................................... 109 
Adding Elements to the Graph: Observed Results ................................................................. 111 
Chapter 5: Bayesian Estimation of Unidimensional IRT Models for 
Polytomously Scored Items ......................................................................... 113 
Introduction ....................................................................................................................................... 113 
The Graded Response Model .......................................................................................................... 114 
Program Template for the GR Model ....................................................................................... 114 
Options for Specifying Prior Distributions for the Threshold and Intercept Parameters ... 116 
Estimation of the GR Model by Using Method 1 ..................................................................... 118 
Comparison of Separate and Joint Prior Specifications ........................................................ 120 
Output from Estimating the GR Model (Method 1 Prior Specification) ................................. 121 
Comparison of the Posterior Densities for the Three Prior Specifications .......................... 127 
Computation of Transformations by Post-Processing Posterior Results ............................ 128 
Specification of the Likelihood Model with Use of the Table Function ................................ 129 
Estimation of the One-Parameter Graded Response Model ................................................. 129 
Muraki’s Rating Scale Model ........................................................................................................... 131 
Estimating the RS-GR Model .................................................................................................... 131 
Output from Estimating the RS-GR Model .............................................................................. 132 
The Nominal Response Model ......................................................................................................... 136 
Estimating the NR Model ........................................................................................................... 136 
Output from Estimating the NR Model ..................................................................................... 137 
The Generalized Partial Credit Model ............................................................................................. 143 
Estimating the GPC Model ........................................................................................................ 144 
Output from Estimating the GPC Model .................................................................................. 146 
Comparison of Results Based on MML Estimation....................................................................... 149 
Graphs of Item Category Response Functions .............................................................................. 151 
Graphs of Test Information Functions ............................................................................................ 153 
Chapter 6: IRT Model Extensions ................................................................. 155 
Introduction ....................................................................................................................................... 155 
The Bifactor IRT Model ..................................................................................................................... 155 
Description of the Model ........................................................................................................... 156 
Estimation of the Model in PROC MCMC ................................................................................ 156 
Output from PROC MCMC ........................................................................................................ 158 
Other Multidimensional IRT Models ......................................................................................... 163 
The Problem of “Label Switching” ............................................................................................ 165 
The Testlet IRT Model ....................................................................................................................... 166 
Description of the Model ........................................................................................................... 166 
Estimation of the Model ............................................................................................................. 166 
Output from PROC MCMC ........................................................................................................ 167 
Hierarchical Models—Multilevel IRT Models ................................................................................. 172 

vi  
Description of the Multilevel IRT Model ................................................................................... 172 
Estimation of the Model ............................................................................................................. 173 
Output from PROC MCMC ........................................................................................................ 175 
Differential Item Functioning—Multiple Group and Mixture IRT Models .................................... 180 
Multiple Group Models for Detecting DIF ................................................................................ 180 
Mixture IRT Models .................................................................................................................... 182 
Chapter 7: Bayesian Comparison of IRT Models .......................................... 189 
Introduction ....................................................................................................................................... 189 
Bayesian Model Comparison Indices ............................................................................................. 190 
Deviance Information Criterion ................................................................................................. 190 
Conditional Predictive Ordinate ................................................................................................ 190 
Computing Model Comparison Statistics in PROC MCMC ................................................... 191 
Example 1: Comparing Models for Dichotomously Scored Items (LSAT Data) ......................... 193 
DIC Results.................................................................................................................................. 193 
CPO Results ................................................................................................................................ 194 
Example 2: Comparing GR and RS-GR Models for Polytomously Scored Items (DASH Item 
Responses) ........................................................................................................................................ 196 
DIC Results.................................................................................................................................. 196 
CPO Results ................................................................................................................................ 197 
Example 3: Comparing a Unidimensional IRT Model and a Bifactor IRT Model ........................ 200 
DIC Results.................................................................................................................................. 200 
CPO Results ................................................................................................................................ 201 
Chapter 8: Bayesian Model-Checking for IRT Models .................................. 205 
Introduction ....................................................................................................................................... 205 
Different Model-Fit Statistics ........................................................................................................... 206 
Test-Level Fit .............................................................................................................................. 206 
Item-Level Fit .............................................................................................................................. 206 
Person Fit .................................................................................................................................... 207 
Posterior Predictive Model Checking ............................................................................................. 207 
The PPMC Method ..................................................................................................................... 207 
The Posterior Predictive Distribution ....................................................................................... 208 
Discrepancy Measures ..................................................................................................................... 212 
Test-Level Measures .................................................................................................................. 213 
Item-Level Measures .................................................................................................................. 213 
Pairwise Measures ..................................................................................................................... 214 
Person-Fit Measures .................................................................................................................. 214 
Evaluation of Model Fit .............................................................................................................. 215 
Example PPMC Applications ........................................................................................................... 216 
Example 1: Observed and Predicted Test Score Distributions—LSAT Data ........................ 216 
Example 2: Observed and Predicted Item-Test Score Correlations ..................................... 220 
Example 3: Item Fit Plots and Yen’s Q1 Measure ................................................................... 223 
Example 4: Observed and Predicted Odds Ratio Measure .................................................... 231 
Example 5: Observed and Predicted Yen’s Q3 Measure ........................................................ 236 
Example 6: Observed and Predicted Person-Fit Statistic ...................................................... 239 

vii 
 
Example 7: Use of PPMC to Compare Models ........................................................................ 243 
References .................................................................................................. 249 
Index ........................................................................................................... 257 
 
 

viii  
 

About This Book 
Purpose 
Assessment has played and continues to play an integral role in society and in a culture of inquiry. 
Accordingly, numerous instruments have been developed over the years to measure many different 
characteristics of individuals in the educational, psychological, health, social, and behavioral sciences. 
These instruments have been used to measure the status or level of different characteristics in individuals, 
as well as to capture differences in these characteristics across individuals.  
To support the development of instruments and measure characteristics in individuals, test theories have 
been used to describe how inferences, predictions, or estimates of a particular characteristic, trait, or ability 
of a person may be made from responses to items. Test theories such as classical test theory (CTT) and 
item response theory (IRT) provide models for explaining test performance in relation to variables that are 
assumed to influence behavior. They provide methods for selecting items, evaluating tests or scales, 
obtaining scores, and quantifying sources of errors in the measurement process. In the early decades of the 
21st century, psychometricians have favored IRT, as opposed to CTT, in scale development and assessment 
applications. 
IRT models consist of a family of mathematical models that predict item performance by using parameters 
that characterize both the items in an instrument and the respondents. Although numerous methods for 
estimating the parameters of IRT models exist, interest in estimating the parameters using Bayesian 
methods has grown tremendously. In part, this growth is due to the appeal of the Bayesian paradigm among 
psychometricians and statisticians, as well as to the advantages of these methods with small sample sizes, 
more complex or highly parameterized models (such as multidimensional IRT models), and interest in 
simultaneous estimation of item and person parameters. In contrast to traditional approaches for estimating 
model parameters, a Bayesian paradigm considers model parameters to be random variables and uses Bayes 
theorem to obtain distributions for the model parameters. 
Recently, routines have become available in the SAS system software to implement general Bayesian 
analyses (PROC MCMC). Use of the SAS system for Bayesian analysis of IRT models has several 
significant advantages over other available programs: (1) It is commonly used by researchers across 
disciplines; (2) it provides a robust programming language that extends the capability of the program—in 
particular, the capability for model checking; and (3) it shows increased performance and efficiency 
through the use of parallel processing. The purpose of this book is to illustrate Bayesian estimation and 
evaluation of a variety of IRT models that are of interest to psychometricians, scale developers, and 
practitioners responsible for implementing assessment programs.  
Is This Book for You? 
This book is designed for psychometricians, scale developers, and individuals interested in applications of 
Bayesian methods and model checking of IRT models for the development and analysis of assessment data. 
In addition, you may find this book useful if you are interested in applications with small sample sizes, 
applications requiring more complex or highly parameterized models (such as multidimensional IRT 
models), and simultaneous estimation of item and person parameters. 

x   Bayesian Analysis of Item Response Theory Models Using SAS 
 
Prerequisites 
Although introductory material related to IRT and Bayesian analysis is included in the book, some prior 
knowledge about these topics is necessary for you to better understand and implement the procedures in 
your own assessment applications. For detail and more technical information, see the references provided.  
Scope of This Book 
This book is example-driven, with sections presenting code for estimating particular models and results that 
illustrate convergence diagnostics and inferences for parameters, as well as results that can be used 
specifically by scale developers—for example, plotting item response functions. 
Chapters 1 and 2 provide introductory treatments of IRT and Bayesian analysis. These chapters are not 
meant to provide detailed information for individuals interested in learning about these topics. Rather, these 
chapters provide supporting documentation for the subsequent chapters that describe Bayesian estimation 
of different IRT models with the use of SAS PROC MCMC. Chapter 1, on IRT, motivates the use of the 
different models and provides the model formulations that will be used in subsequent chapters. Chapter 2, 
on Bayesian analysis, provides supporting documentation for terms and concepts integral to the use of 
PROC MCMC and the subsequent examples.  
Chapter 3 presents an overview of the PROC MCMC syntax commands and a template for the estimation 
of IRT models, but focuses on syntax that is most relevant to the estimation of IRT models. As a result, 
there is some duplication of this material with other SAS documentation for Bayesian analysis and the use 
of PROC MCMC. However, presentation of PROC MCMC in this book provides a self-contained guide to 
the use of PROC MCMC and Bayesian analysis of IRT models. 
Chapters 4 to 6 provide detailed presentations of the syntax used to estimate IRT models for different types 
of assessment applications. In these chapters, the IRT models and PROC MCMC syntax to estimate the 
models are discussed. Examples then illustrate the generation of convergence diagnostics, use of different 
prior distributions, and summary statistics from the posterior distributions for parameters. In some cases, 
item responses from data sets that are commonly analyzed in the literature are used. In other cases, item 
responses are simulated so that direct comparisons between estimated parameters and population 
parameters can be made. The comparison between estimated parameters and parameters used to obtain 
simulated item responses facilitates the evaluation of using PROC MCMC to estimate the models. In 
addition, comparisons of model parameter estimates derived from using PROC MCMC with other available 
software are also presented.  
Specifically, Chapter 4 illustrates estimation of IRT models for dichotomously scored items, and Chapter 5 
illustrates estimation of models for polytomously scored items. For both of these chapters, the models that 
are discussed assume that a single person parameter determines the performance of respondents 
(unidimensional IRT models). 
Chapter 6 illustrates extensions of IRT models to more highly parameterized models, including  
multidimensional IRT models, models that account for any dependence between sets of items (testlet IRT 
models), multilevel IRT models, and applications to differential item functioning (DIF). Multidimensional 
IRT models are considered when more than one person parameter is assumed. Testlet IRT models may be 
used for testing applications in which individuals respond to a set of items based on a single or common 
stimulus (testlet). Multilevel or hierarchical models may be estimated in testing applications in which there 
exists a nested structure of item responses within clusters of persons or within clusters of items. Finally, 
models for evaluating differential item functioning and the use of mixture IRT models are considered to 
examine whether items function differently across subpopulations of examinees (for example, masters 
versus nonmasters or males versus females). In the case of mixture IRT models, these models assume that 
the population of examinees consists of qualitatively different subpopulations or latent classes that are 
unknown. These models provide a mechanism for clustering examinees to better understand any 
heterogeneity in item responses.  

About this Book   xi 
 
Although the focus is often on estimating IRT model parameters, it is equally important to evaluate the fit 
of a particular IRT model to the item responses. When a model doesn’t fit the data, the validity of any 
inferences for the model parameter estimates is threatened. The SAS system and PROC MCMC affords 
advantages over other available software for Bayesian analysis of IRT models. For evaluating model-data-
fit, in particular, SAS provides a robust programming language and built-in statistical and graphical tools. 
The programming language and the tools expand the capability of the program beyond estimating the 
model to computing many different types of statistics for comparing competing models and evaluating 
model fit.  
The topics of model comparison and model checking in a Bayesian analysis are discussed in Chapters 7 and 
8 of this book. Chapter 7 considers methods for comparing competing models so that a preferred model can 
be identified. Methods based on information criteria indices and the Bayes Factor are considered. Chapter 8 
begins with a discussion of item and person fit in the context of IRT models and a description of a popular 
model checking tool in Bayesian analysis—posterior predictive model checking (PPMC). Then possible 
discrepancy statistics for a PPMC analysis of IRT models are discussed, followed by examples illustrating 
the use of SAS procedures and programming commands to perform a PPMC analysis. The use of output in 
the form of tables and graphs is also included to illustrate the capabilities of a PPMC analysis in SAS, as 
well as the different types of output that can be produced to report a PPMC analysis. 
About the Examples 
Software Used to Develop the Book’s Content 
PROC MCMC statements and examples using PROC MCMC in this book are based on SAS 9.4 and 
SAS/STAT 13.2.  
Example Programming Code and Data 
Examples are provided to estimate IRT models for dichotomously and polytomously scored items, as well 
as more complex IRT models, such as multidimensional and hierarchical models. 
You can access the example programs and data sets for this book by linking to its author page at 
http://support.sas.com/publishing/authors/stone.html. Select the name of the author. Then look for the cover 
thumbnail of this book, and select Example Code and Data to display the SAS programs that are included 
in this book. 
For an alphabetical listing of all books for which example code and data are available, see 
http://support.sas.com/bookcode. Select a title to display the book’s example code. 
If you cannot attain access to the code through the website, send email to saspress@sas.com.  
Output and Graphics 
All output was derived from programs using PROC MCMC or from other SAS programs. All graphics 
were obtained with the use of SAS Graphics Template Language with default SAS Output Delivery System 
settings. 
 
 

xii   Bayesian Analysis of Item Response Theory Models Using SAS 
 
Additional Help 
Although this book provides examples of many of the IRT analyses used by psychometricians, questions 
specific to your aims and issues may arise. To fully support you, SAS Institute and SAS Press offer you the 
following help resources: 
Ɣ 
For questions about topics covered in this book, contact the author through SAS Press: 
ƕ 
Send questions by email to saspress@sas.com. Include the book title in your correspondence. 
ƕ 
Submit feedback on the author’s page at http://support.sas.com/author_feedback. 
Ɣ 
For questions about topics in or beyond the scope of this book, post queries to the relevant SAS 
Support Communities at https://communities.sas.com/welcome.  
Ɣ 
SAS Institute maintains a comprehensive website with up-to-date information. One page that is 
particularly useful to both the novice and the seasoned SAS user is its Knowledge Base. Search for 
relevant notes in the Samples and SAS Notes section of the Knowledge Base at 
http://support.sas.com/resources.  
Ɣ 
Registered SAS users or their organizations can access SAS Customer Support at 
http://support.sas.com. Here you can pose specific questions to SAS Customer Support. Under 
Support, click Submit a Problem. You will need to provide an email address to which replies can 
be sent, identify your organization, and provide a customer site number or license information. 
This information can be found in your SAS logs. 
Keep in Touch 
We look forward to hearing from you. We invite questions, comments, and concerns. If you want to contact 
us about a specific book, please include the book title in your correspondence. 
Contact the Author through SAS Press 
Ɣ 
By email: saspress@sas.com 
 
Via the web: http://support.sas.com/author_feedback 
Purchase SAS Books 
For a complete list of books available through SAS, visit sas.com/store/books. 
Ɣ 
Phone: 1-800-727-0025 
 
E-mail: sasbook@sas.com 
Subscribe to the SAS Training and Book Report 
Receive up-to-date information about SAS training, certification, and publications via email by subscribing 
to the SAS Training & Book Report monthly eNewsletter. Read the archives and subscribe today at 
http://support.sas.com/community/newsletters/training! 
Publish with SAS 
SAS is recruiting authors! Are you interested in writing a book? Visit http://support.sas.com/saspress for 
more information. 

About These Authors 
Clement A. Stone is a professor in the Research Methodology program 
at the University of Pittsburgh School of Education. He is an expert in 
psychometrics, including educational and psychological instrument 
development and validation, item response theory (IRT) models and 
applications, and Bayesian analysis of IRT models. Also an expert in 
SAS software, he has used SAS extensively in research utilizing 
simulation methods, and he instructs graduate students in the use of 
SAS. He applies IRT in the development and validation of educational, 
psychological, and behavioral assessments, including a research focus 
on educational achievement, critical thinking, psychosocial stress, 
communication outcomes, risk factors for addiction, and physical disability. He has 
published numerous articles, and he coauthored a chapter on the development and 
analysis of performance assessments in Educational Measuremen. In addition to 
publishing in and reviewing for numerous prominent journals, he has served on the 
editorial boards for the Journal of Educational Measurement, Applied Measurement in 
Education, Educational and Psychological Measurement, and the American Educational 
Research Journal. Stone holds a Ph.D. in research methods, measurement, and statistics 
from the University of Arizona.  
Xiaowen Zhu is an associate professor in the Department of Sociology 
and a research fellow in the Institute for Empirical Social Science 
Research, Xi’an Jiaotong University, China. Formerly, she was a 
psychometrician for the Data Recognition Corporation in the United 
States, working on large-scale state educational assessments. She has a 
strong theoretical background, operational experience, and research 
experience in psychometrics and applied statistics. She is especially 
adept at Bayesian analysis of IRT models and has published journal 
articles on this subject. In addition, she has used SAS throughout her research career for 
general statistical analysis and simulation studies. Zhu holds a Ph.D. in research 
methodology from the University of Pittsburgh. 
Learn more about these authors by visiting their author pages, where you can download 
free book excerpts, access example code and data, read the latest reviews, get updates, 
and more: 
http://support.sas.com/publishing/authors/stone.html 
http://support.sas.com/publishing/authors/zhu.html   

xiv 
 
 

Acknowledgments 
The authors would like to acknowledge the assistance received from Fang K. Chen, Senior Manager of 
Advanced Analytics, Research and Development, at SAS Institute. He was extremely generous with this 
time and knowledge about Bayesian methods and the SAS MCMC procedure as examples in this book 
were developed. They would also like to acknowledge the other reviewers of this book, whose comments 
helped provide clarity and depth to the discussion. 
Clement A. Stone dedicates this book to his daughter Alexandra. 

xvi 
 
 

Chapter 1: Item Response Theory 
Introduction ........................................................................................................... 1
Overview of IRT Models and Their Application ...................................................... 2
Visualization of IRT Models ................................................................................................. 3
Organization of the Chapter ............................................................................................... 4
Unidimensional IRT Models for Dichotomously Scored Responses ....................... 5
Unidimensional IRT Models for Polytomously Scored Responses .......................... 5
The Graded Response Model ............................................................................................. 6
Muraki’s Rating Scale Model .............................................................................................. 8
The Partial Credit Model and Extensions .......................................................................... 8
The Nominal Response Model .......................................................................................... 11
Other Testing Effects in Unidimensional IRT Models .......................................... 12
Locally Dependent Item Sets (Testlets) ........................................................................... 13
Rater Effects ...................................................................................................................... 13
IRT Models for Multidimensional Response Data ................................................ 14
Differential Item Functioning and Mixture IRT Models ........................................ 16
Hierarchical Models—Multilevel and Random Effects IRT Models ....................... 17
Evaluation of IRT Model Applications .................................................................. 19
Dimensionality .................................................................................................................... 19
Local Independence .......................................................................................................... 19
Form of the IRT Model ....................................................................................................... 20
Speededness ...................................................................................................................... 20
Model Fit ............................................................................................................................. 20
IRT Model Parameter Estimation ......................................................................... 22
Introduction 
Item response theory (IRT) is one of several test theories that may be applied to assessment data to describe 
how estimates, inferences,  and predictions of a particular characteristic, trait, or ability of a person may be 
made from responses to test items (Lord & Novick, 1969). Test theories provide models for explaining test 
performance in relation to variables that are presumed to influence item response. Furthermore, they 
provide methods for obtaining scores and for quantifying errors in measurement.  
In the past, methods based on classical test theory (CTT) have dominated applications of test theories to 
assessments. CTT is an intuitive model that relates observed scores to true scores plus errors in 
measurement. Useful results from applications of CTT to assessment responses include estimation of item 
characteristics (for example, item difficulty), observed score reliability, and quantifying error in 
measurement (standard error of measurement – SEM). However, these and other results depend on the 
group of examinees from which they are derived. 
IRT addresses shortcomings of CTT and also solves practical problems related to test development (see, for 
example, Embretson & Reise, 2000; Hambleton & Swaminathan, 1985). Compared to CTT, which models 
responses at the test level, IRT models focus attention on the item level by modeling or predicting 

2    Bayesian Analysis of Item Response Theory Models Using SAS 
 
performance for individual items. If the model is appropriate, properties for items are independent of the 
specific group that is assessed (person-free measurement), and traits or abilities being measured are 
independent of the specific items that are administered (item-free measurement). Because IRT models 
focus on items and the independent contribution of items to test results, IRT provides a useful framework 
for test development that includes methods for selecting items appropriate to a particular testing 
application, equating multiple forms so that trait or ability estimates are comparable, and constructing item 
banks that may be used for computer adaptive testing applications. Santor and Ramsay (1998) further 
discuss that IRT methods afford a better estimate of an individual’s true level on the latent trait being 
measured than does a summated score of item responses used in a CTT approach. 
This chapter provides a description of different types of IRT models that can be used in different testing 
applications. The descriptions are brief by design and are intended to serve only as supporting 
documentation for the examples of Bayesian analyses of IRT models that are presented in subsequent 
chapters. For more general and complete treatments of different IRT models, see, for example, Embretson 
and Reise (2000); van der Linden and Hambleton (1997); and Yen and Fitzpatrick (2006).  
Rather than review this chapter in its entirety, you may wish to combine a review of specific sections of 
this chapter with the PROC MCMC code for a Bayesian analysis in other chapters. For example, Chapter 3 
describes the use of PROC MCMC, including syntax and options, but only in the context of simple 
unidimensional IRT models for binary responses. Thus, information relevant to unidimensional IRT models 
for binary responses is limited to the first two sections of this chapter. For readers focusing on models for 
polytomously scored responses, by contrast, Chapter 5 may be combined with the section on 
unidimensional models for polytomously scored responses in Chapter 1. 
Overview of IRT Models and Their Application 
IRT models involve a class of mathematical models that may be used to predict persons’ responses to each 
test item by using characteristics of the persons and characteristics of the items. The person characteristics 
reflect one or more latent constructs being measured by an instrument (test, survey, or scale), and a 
person’s particular level or status on each construct being measured is assumed to affect test performance. 
These characteristics could reflect many different types of traits for individuals, such as verbal ability in 
educational assessment, a psychological trait such as depression, or quality-of-life status in health 
assessments. A broad construct domain can be further conceptualized in terms of subdomains (for example, 
mental and physical quality of life), and each subdomain can represent a separate person characteristic that 
is being measured and modeled with the use of IRT. 
Underlying the measurement of each construct are the observable indicators for the construct—that is, the 
items. Because characteristics of items are also assumed to affect test performance, IRT models also 
include parameters that reflect item characteristics. For example, items are often targeted at specific levels 
of a trait and this target level of the trait is a characteristic of items. In the context of educational 
assessment, items may be targeted or designed to measure performance for individuals with high ability 
(that is, more difficult items). Therefore, the targeted level of the trait for an item reflects an item 
characteristic that is modeled within IRT models. 
To predict or model test item responses, IRT models consist of one or more latent person parameters and 
one or more item parameters (see, for example, Embretson & Reise, 2000; Hambleton & Swaminathan, 
1985; Reeve, 2002; Yen & Fitzpatrick, 2006). These parameters, in conjunction with a mathematical 
function, such as a normal ogive or logistic function, may be used to model the probability of a response uj 
to item j. This response could be, for example, correct or incorrect, or endorsement of a particular response 
option. A general expression for the class of IRT models is as follows: 
୨൫୨ൌ୨ȁ૑୨ǡ ી൯
where T is a vector of person characteristics reflecting different traits being measured, and Zj is a vector of 
item parameters for item j characterizing features of the item that are assumed to affect performance. 

Chapter 1: Item Response Theory    3
 
Notably, the response function is for an individual person, but a subscript for the person is excluded for 
convenience in this equation and in subsequent equations generally. 
Visualization of IRT Models 
The IRT model for a dichotomously scored item (0, 1; Incorrect, Correct; No, Yes) is typically displayed 
using an item response curve or item characteristic curve (ICC). Figure 1.1 illustrates an ICC for a 
dichotomously scored item, where Pj (Uj = 1 | Zj, T) is the item response function with one latent ability, or 
trait, assumed to determine item performance. In the figure, the x-axis corresponds to the latent trait or 
ability (T) being measured, and the y-axis corresponds to the expected probability of responding correctly 
or endorsing the item. As can be seen, the mathematical function reflects a monotonic increasing 
relationship between response probability and the status or level of the ability or trait being measured. 
Figure 1.1: Example ICC for a Dichotomously Scored Item (0, 1) 
 
 
In the ICC, the set of item parameters (Zj,) for the specific IRT model determines the shape of the function. 
A slope parameter for the ICC is used to describe how rapidly the probability of a response changes 
(sensitivity) as a function of changes in the latent trait T. It therefore also reflects the degree to which the 
item is related to the underlying ability, or trait, being measured. Higher slope parameters correspond to 
items that are more sensitive to changes in T and they provide more information for estimating T.  
A second parameter, the threshold parameter, is on the same scale as the latent trait Tand describes where 
the response function is centered on the T scale or the location of the ICC. Conceptually, threshold 
parameters reflect the level on the latent trait (T) continuum where one response becomes more likely than 
a different response, as well as reflecting the level of the trait where more information about T is available. 
For example, in Figure 1.1, the probability of a correct response (1) is greater than an incorrect response (0) 
in the region of T!.5. 
A lower asymptote parameter may also be included to model the probability of the response for examinees 
with low T. In Figure 1.1, very low-ability examinees (T.5) have a .20 probability of answering the 

4    Bayesian Analysis of Item Response Theory Models Using SAS 
 
item correctly. In an educational assessment context, these three parameters (slope, threshold, lower 
asymptote) are often referred to as discrimination, difficulty, and guessing parameters. While these three 
parameters have dominated the discussion of item parameters in IRT models for dichotomously scored 
items, other parameters have been discussed (for example, an upper asymptote parameter). 
Although item parameters across items may be compared either numerically or by comparing ICCs for 
different items, the focus is primarily on threshold or difficulty parameters when comparing items. Because 
items and the latent trait (T are on the same scale, threshold parameters may be compared with particular T 
values or compared across items. For example, comparing a threshold value with a T value indicates how 
much increase in T is required to achieve a particular level of expected performance or an expected 
response on an item. Also, the Tvalue for a particular person provides information about the types of items 
that the person would likely answer correctly or the types of items a person would likely endorse a 
particular response option (Yes, as opposed to No). For example, items with threshold values below a 
person’s Tvalue are more likely to be answered correctly or endorsed.  
Finally, a comparison of threshold values across different items indicates the relative difficulty of items in 
the case of dichotomously scored items. ICCs for multiple items may also be displayed in the same graph. 
ICCs that require higher or lower levels of the ability or trait being measured to achieve similar 
performance would be centered to the right (for higher levels) or left (for lower levels) of the ICC for the 
item in Figure 1.1.  
Note that for polytomously scored items—items with more than two response options, such as Likert-type 
scales—additional threshold parameters are used to describe where the response functions for each 
response option are centered on the T scale. Thus, for a polytomously scored item, item response functions 
for each response category or option are plotted, and knowing the Tvalue for a particular person provides 
information about the response option or category that the person would likely endorse or the rating that a 
person would likely receive. 
Organization of the Chapter 
The remainder of this chapter describes various types of IRT models that can be applied to different types 
of assessments. These include IRT models for achievement and ability tests that use items that are 
dichotomously scored (correct, incorrect), as well as models for constructed response items or performance 
assessments where raters typically score examinees’ responses by using ordinal response scales with two or 
more response categories (Lane & Stone, 2006). Developers of measures in the health and behavioral 
sciences (for example, personality tests, quality of life measures) can also take advantage of IRT models 
(Embretson & Reise, 2000; Gibbons, Immekus, & Bock, 2007; Immekus, Gibbons, & Rush, 2007; 
McHorney, 1997; Reeve, Hays, Chang, & Perfetto, 2007). These types of measures may also consist of 
dichotomously scored items (Yes, as opposed to No items) or polytomously scored items such as rating or 
Likert-scaled items.  
In addition to characterizing IRT models in terms of the types of items that can be modeled, IRT models 
can also be characterized in terms of the number of traits being measured: unidimensional IRT models 
measure one underlying trait and multidimensional IRT models measure multiple underlying traits. 
Extensions of IRT models to other testing applications include the following: 
Ɣ
tests where raters score responses  
Ɣ
scales comprised of sets of items with common stimuli (testlets) 
Ɣ
populations of examinees that include qualitatively different subpopulations 
•
applications in which item responses are nested within clusters of persons  
Finally, evaluating the validity of IRT model applications and estimating IRT model parameters are 
discussed. 

Chapter 1: Item Response Theory    5
 
Unidimensional IRT Models for Dichotomously Scored Responses 
IRT models for dichotomously scored responses were originally developed for educational assessment 
applications where responses are scored as Correct as opposed to Incorrect. However, these models can 
also be used for responses that are scored in any dichotomy such as Yes as opposed to No. For any of these 
types of applications, Hambleton and Swaminathan (1985) describe three IRT models that are generally 
used: 1-, 2-, and 3-Parameter models. In the general form for these three models, the probability that an 
examinee receives a response (1=Correct as opposed to 0=Incorrect) to the jth item is commonly modeled 
by a logistic function: 
୨ሺɅሻൌ୨൫୨ൌͳȁ૑୨ǡ Ʌ൯ൌȲሺ୨ሻൌ୨൅ሺͳ െ୨ሻ݁௓ೕ
݁௓ೕ 
where <(zj) is the cumulative logistic function and zj = Daj (T í bj ) is a logistic deviate; D is a scaling 
constant; T references the person parameter (for example, examinee ability); and Zj references the vector of 
possible item parameters for item j in the model: aj is the slope parameter for item j, bj is a threshold 
parameter for item j, and cj is a lower asymptote parameter for item j.  
A normal ogive function, )(zj), or any other function that models a cumulative density function, may also 
be used (Yen & Fitzpatrick, 2006). For )(zj), zj is called a normal deviate and is defined as above for the 
logistic link function (zj = Daj (T  bj )). However, because the logistic function is more mathematically 
tractable, the logistic function has received more attention in IRT applications. Note that the two link 
functions for IRT models, <(zj) and )(zj), produce essentially equivalent results if the scaling constant (D) 
is 1.702 for <(zj). Use of D = 1.702 is optional and is used in some IRT model parameterizations; D = 1.0 
is used in other IRT model parameterizations.  
The three models for dichotomously scored items are differentiated by the item parameters that are 
included in the model. The 3-Parameter (3P) model includes all three item parameters: aj, bj, and cj—see 
Figure 1.1 for an example of an ICC for this model. The 2-Parameter (2P) model does not include cj and is 
appropriate for applications where the lower asymptote is assumed to be 0 or guessing is assumed to be 
negligible. Finally, the 1-Parameter (1P) model also does not include cj and estimates a common slope 
parameter across items. That is, the logistic deviate is zj = Da(T  bj ). In essence, this model adds an 
assumption that items are related equally to the underlying trait or ability being measured. These three 
models reflect hierarchical or nested models, and you can use model comparison methods to compare them 
directly to determine a preferred model. Methods for comparing the models are discussed in Chapter 7.  
The Rasch (1960) model for dichotomously scored items has also received much attention. IRT models 
were developed to model item performance and focus on model fit. The Rasch model was developed to 
capture measurement principles inherent to instruments more broadly, such as those used in the physical 
sciences. Although developed independently of IRT models, the Rasch model is equivalent to a 1P logistic 
model with Da = 1 in the logistic deviate. Rasch-based or 1P models have several desirable properties 
when compared with 2P and 3P models. For example, interpreting person-item interactions is independent 
of the level of T for Rasch or 1P models. In other words, any comparisons of persons’ T levels are 
independent of the items used in the comparisons. Rasch or 1P models may also be preferred for practical 
reasons such as reduction of the number of estimated parameters. For a useful summary of Rasch model 
properties, see, for example, Embretson and Hershberger (1999). Because Rasch and 1P IRT models are 
equivalent, these models will be referred to as 1P models in subsequent chapters. 
Unidimensional IRT Models for Polytomously Scored Responses  
To extend the capability of educational assessments and to measure other characteristics of individuals, 
numerous instruments have been developed that use response scales and items that are scored 
polytomously. Examples of assessments in education include items that allow for partial credit, constructed 
response and performance items, and multiple-choice items for which each response option is scaled. 
Examples in the health, social, and behavioral sciences include a myriad of assessments that involve rating 
scales and Likert-type items. The appeal of IRT applications to polytomously scored items is to enable 

6    Bayesian Analysis of Item Response Theory Models Using SAS 
 
better understanding of the psychometric properties of items and to obtain more information about the 
characteristic being measured. In some assessment applications, this additional information translates into 
more diagnostic information about respondents. 
In tandem with the development of IRT models for dichotomously scored items, different types of IRT 
models have been proposed for applications with polytomously scored items. In all the models considered 
in this book, threshold parameters, similar to those used in models for dichotomously scored items, are used 
to define boundaries between adjacent response categories and are used to describe the transition in 
response from one category to another. Because the thresholds reflect boundaries between adjacent 
response categories, the number of threshold parameters is equal to one less than the number of response 
categories. 
The Graded Response Model 
Samejima’s (1969; 1996) graded response (GR) model is a commonly used model for assessment 
applications with ordinal response scales such as rating or Likert scaled items. The response process 
underlying the model assumes that all response categories or options are considered by persons and that the 
selected response option is a function of the trait being measured and all prior response categories in the 
scale. 
In the GR model, the cumulative probability that a person selects or receives a category score or rating k or 
higher for item j, ܲ௝௞
כ ሺɅሻ is modeled by the logistic deviate zjk = Daj (T  bjk), where D is the scaling 
constant; T references the person trait or ability being measured; aj is the slope (discrimination) parameter 
for item j; and bjk is a threshold parameter for item j and category k (k = 0, 1, 2… mj): 
ܲ௝௞
כ ሺߠሻൌ
݁௓ೕೖ
1+݁௓ೕೖ 
Figure 1.2 presents the cumulative probability functions, ܲ௝௞
כ ሺɅሻ, also called “operating characteristic 
curves,” for an item with five response categories. Under the GR model for this item, there are four 
category threshold parameters (bj1 = 1.5, bj2 = 0.5, bj3 = 0.5, and bj4 =1.5). Each parameter indicates the 
point on the T scale at which the curve is steepest, and an individual has a .5 probability of responding in 
category k or higher. If T exceeds bjk, the probability that the response is in category k or higher is greater 
than .5. For example, for individuals with T= 1.5, the probability is .5 that individuals will endorse 
response Category 1 or higher. Because the response functions reflect successive cumulative probabilities 
of responding in category k or higher, the bjk in the GR model are necessarily ordered bj1 < bj2 < … < bjm. 
Note that an operating characteristic curve for the lowest category (0) is not plotted, because the function 
would be equal to 1 for all T.  
For the GR model, one slope parameter is estimated for each item leading to the parallel operating 
characteristic curves shown in Figure 1.2. Under the further assumption that all items have a common slope 
parameter, the GR model reduces to a one-parameter GR model with the logistic deviate zjk = Da (T bjk). 
Therefore, the one-parameter GR model is a restricted version of the GR model with a common slope 
parameter across items. Finally, as discussed by Embretson & Reise (2000), the GR model for an item 
reflects a series of 2P logistic models for dichotomously scored items (k < 1 versus k > 1, k < 2 versus k > 
2). Thus, if there are only two categories, the model is equivalent to the 2P logistic response model for 
dichotomously scored items. 

Chapter 1: Item Response Theory    7
 
Figure 1.2: ࡼ࢐࢑
כ ሺࣂሻ for a 5-response Category Item under the GR Model 
 
The probability of a particular response in category k for a person on item j, ܲ௝௞ሺɅሻ, can be computed by  
differences between the cumulative probabilities for two adjacent categories: ܲ௝௞
כ ሺɅሻ and ܲ௝ሺ௞ାଵሻ
כ
ሺɅሻ, given 
ܲ௝଴
כ ሺߠሻൌͳ and ܲ௝ሺ௠ೕାଵሻ
כ
ሺߠሻൌͲ. Accordingly, Thissen and Steinberg (1986) described the GR model and 
its derivatives as “difference models.”  
Figure 1.3 presents the item category response curves, ܲ௝௞ሺɅሻ, derived from the difference between 
cumulative probabilities for adjacent categories for the item in Figure 1.2. For example, the item 
characteristic curve for Category 2 is derived by subtracting ܲ௝ଶ
כ ሺɅሻ and ܲ௝ଷ
כ ሺɅሻ. Note that for any specific 
values of the latent trait, T, the sum of the probabilities across all response categories equals one, and an 
expected score for a person with a particular T value on the item can be estimated by the sum of each score 
response times the probability associated with score response or σሾ݇ൈܲ௝௞ሺɅሻሿ over the m(j+1) response 
categories. 

8    Bayesian Analysis of Item Response Theory Models Using SAS 
 
Figure 1.3: ࡼ࢐࢑ሺࣂሻ for a 5-Response Category Item under the GR Model 
 
Muraki’s Rating Scale Model 
Another restricted case of the GR model is Muraki’s (1990) rating scale (RS) model. In this model, the 
threshold parameters (bjk) in the GR model are decomposed into two terms: (1) a location parameter (bj) for 
each item that can be compared with other items, and (2) one set of category threshold parameters (ck) that 
applies to all items; or zjk = Daj (T  (bj  ck )). The RS model is a restricted version of the GR model since 
category threshold parameters are assumed to be equal across all items in the RS model, whereas they are 
free to vary across items in the GR model. As a result, the number of parameters in the RS model is 
reduced significantly as compared with the GR model. Another advantage of this parameterization is that 
the bj parameter locates the item on the ability or trait continuum being measured and thus reflects the level 
of Tat which optimal measurement is attained.  
A model with a common set of category threshold parameters may be more consistent with responses to 
rating scales in questionnaires where a set of items share the same response scale. However, Lane and 
Stone (2006) pointed out that the RS model may also be appropriate for performance assessments. When a 
general rubric is used as the basis for developing specific item rubrics, the response scales and the 
differences between score levels may be the same across the set of items.  
The Partial Credit Model and Extensions  
Another class of models that may be used with polytomously scored item responses and ordinal scales is 
based on the Rasch model. The partial credit (PC) model (Masters, 1982) was originally developed to 
model responses to items that were designed to measure steps in performance. In the PC model, each 
transition from one category to the next is modeled by a dichotomous model where the logistic deviate is zjk 
= (T  Gjk); and Gjk are item category boundary parameters, also called “item step difficulty” parameters, 
governing the transition from category kí1 to category k in item j. As for the GR model, the number of 
category boundary parameters is equal to one less than the number of response options.  

Chapter 1: Item Response Theory    9
 
The item category boundary parameters may be interpreted as the point on the Tscale at which a score 
response becomes more likely than the preceding score response. Therefore, the parameters reflect the 
relative difficulty associated with the transition from one score category to the next adjacent score category. 
Because the boundary parameters only reference the previous step or response category, there is no 
requirement that the transitions from one score response to another be ordered in difficulty despite the 
requirement that the response scale be ordinal. For example, the transition from a score of 2 to a score of 3 
may be easier than the transition from a score of 0 to a score of 1. Finally, because no slope parameter is 
included in the model, all items are assumed to be equally related to the underlying trait being measured. 
A general expression for the PC model defines the probability of responding at a particular score category x
(or the number of completed steps ) to item j, given a particular level of T, as the following: 
୨୶ሺɅሻൌ
σ
୞ౠౡ
౮
ౡసబ
σ
σ
୞ౠౡ
౞
ౡసబ
୫ౠ
୦ୀ଴
 
for 
k = 0, 1, 2, x,… mj 
where zjk = (T  Gjk) and includes the following constraint: σ
ݖ௝௞
଴
௞ୀ଴
ൌͲ or ݁σ
௭ೕೖ
బ
ౡసబ
ൌͳ. In contrast to the 
GR model, the PC model may be used to directly derive the probability of the score response. This is 
because the numerator of the function is an exponential function of the difference between a person’s level 
of T and each category boundary (Gjk) up to a specific response (number of steps completed) divided by a 
summation of exponential functions of T and Gjk across all possible score responses. Accordingly, Thissen 
and Steinberg (1986) described these models and their derivatives as divide-by-total models. 
Figure 1.4 presents item category response curves for a sample 5-category item with four changes or steps 
in performance under the PC model (level 0 f level 1, …, level 3 f level 4). Under the PC model, the 
item step difficulty parameters, Gjk, reflect the point on the T continuum where adjacent response 
probabilities (k and k1) are equal, or the point on the T continuum where adjacent item category response 
curves intersect. This relationship is in contrast to the GR model, in which the threshold parameters reflect 
the point on the T continuum where the characteristic curve is steepest. The Gjk for the item in the figure 
reflect an ordered set: 2, .5, .5, and 2. As can be seen when T = Gjk, an individual has an equal probability 
of responding in the two score categories. For example, at T = 2, an individual has the same probability of 
endorsing response Category 0 and 1. Because the Gjk for the item in the figure are relatively widely spaced 
from one another and ordered, each response option would likely be endorsed by a number of persons, 
although the numbers in response categories 0 and 4 would be smaller because there are fewer persons in 
the tails of the T distribution. Thus, the item in Figure 1.4 reflects a well functioning item. 
As discussed, the Gjk need not be ordered, as in the GR model, because transitions to different score levels 
may differ in difficulty. For example, Figure 1.5 presents an item under the PC model where the set of Gjk 
parameters are now unordered: 2, .5, .5, and 2. From the Gjk, it is much easier now to transition from 
Category 2 to Category 3, and fewer individuals would be expected to have a response of 2. When 
comparing the figures, you find that the probability of responding in Category 2 in Figure 1.5 is small in 
comparison to the same probability in Figure 1.4. As is typical with cases where the Gjk parameters are not 
ordered for an item, at least one response category will not be most probable at some point along the T 
continuum. Such items may not be functioning as intended and may indicate categories that can be 
combined. 

10    Bayesian Analysis of Item Response Theory Models Using SAS 
 
Figure 1.4: ࡼ࢐࢑ሺࣂሻ for a 5-Category Item under the PC Model — Ordered Gjk 
 
Figure 1.5: ࡼ࢐࢑ሺࣂሻ for a 5-Category Item under the PC Model — Unordered Gjk 
 

Chapter 1: Item Response Theory    11
 
The generalized partial credit (GPC) model (Muraki, 1992) is a straightforward extension of the PC model. 
In this model, a slope parameter and scaling constant are added to the logistic deviate, zjk = Daj (T  Gjk), to 
relax the assumption that items are related equally to the underlying trait being measured. As for the 
category threshold parameters in the GR model, the item step difficulty parameters, Gjk, may be 
decomposed into a location parameter (Ej) for each item and category threshold parameters (Wk or Wjk); for 
example, Gjk = Ej – Wk. The values of Wk are not necessarily ordered sequentially, and each Wk reflects the 
difficulty of category k, relative to other categories, when compared to the location parameter Ej. This 
parameterization is equivalent to the Rating Scale (RS) model described by Andrich (1978) and Masters 
and Wright (1984). 
The Nominal Response Model 
The nominal response (NR) model was originally developed for multiple choice tests and capturing 
information about the distracters (Bock, 1972). In these types of applications as well as other applications 
with nominal scales, the response options are not necessarily ordered. Thissen (1993) discusses different 
applications of the NR model, such as items with multiple response options in personality and attitude 
assessments. The NR model can also be used with Likert or rating scales that include filters such as “Don’t 
Know.” Finally, Thissen, Cai, and Bock (2010) discuss that the nominal model could be used not only for 
items with nominal scales, but also used to check on the ordering of response scale options and used to 
model sets of item responses that have been combined. 
Thissen and Steinberg (1986) illustrated how all the divide-by-total models (for example, PC and GPC) are 
special cases of the more general nominal response (NR) model. In the NR model, the propensity to 
endorse option k of item j is given by the following:  
ܲ௝௞ሺɅሻൌ
௘೥ೕೖ
σ
௘೥ೕೣ
೘ೕ
ೣసబ
  
where zjk = ajkT + cjk, ajk is the option discrimination parameter, and cjk is called the option extremity 
parameter. As for other IRT models, the slope parameter for an item category, ajk, reflects the strength and 
direction of the relationship between an item category and T, and the intercept parameter for an item 
category, cjk, reflects the tendency to endorse option k at T=0. In order for the model to be identified, 6ajk 
and 6cjk across k are constrained to equal 0. Alternatively, the ajk and cjk for the first response category for 
each item can be constrained to equal 0. 
Figure 1.6 presents ܲ௝௞ሺɅሻ for an item, given the NR model, in which the response scale reflects a 5-
category rating scale (1 = not at all … 5 = very much). In this example, vectors of parameters for ajk are (0, 
2.03, 3.48, 5.0, 6.29) and for cjk are (0, 1.91, 1.45, .14, 3.43). In this parameterization, aj1 and cj1 are 
constrained to equal 0 to identify the model. You can see that, in this case, higher response categories 
become more likely with increased T and ajk > aj(k-1), indicating that the linear regression of z on T reflects 
an increasing probability of selecting option k+1 over k as T increases.  
To express the parameters in a form similar to other IRT models, a location parameter, bjk, can be derived 
from a function of the ajk and cjk parameters (bjk = cjk /ajk). Differing from interpretations of the other 
discussed models, however, the interpretation of these parameters and the locations of the item category 
response functions depend on all the response category parameters (Baker & Kim, 2004). For example, the 
bjk parameters do not reflect locations on the T continuum, where item category response functions 
intersect. Alternatively, De Ayala (2009) discusses that bjk can be defined as functions of the ajk and cjk for 
adjacent categories, to reflect where item category response functions for adjacent categories intersect on 
the T continuum. An intersection location for adjacent categories is defined by bjk = (cjk  cjk+1) / 
(ajk+1ajk). For example, the value of T where the adjacent response categories for Category 2 and 
Category 3 intersect, is computed by bj2 = (cj2  cj3) / (aj3aj2) = .32.  
 
 

12    Bayesian Analysis of Item Response Theory Models Using SAS 
 
Figure 1.6: ࡼ࢐࢑ሺࣂሻ for a 5-Category Item under the NR Model  
 
Other Testing Effects in Unidimensional IRT Models 
In practice, individuals may respond to a set of items (testlet) based on a single or common stimulus. For 
example, four items constructed for each reading passage in a reading comprehension test would form four 
separate testlets. Assessment applications using polytomously scored items (for example, science 
performance assessments) may also consist of one or more testlets (Lane & Stone, 2006). For these types of 
testing applications, the assumption of local independence is likely to be violated. The responses to the 
items within a testlet would be more highly related than predicted by the overall latent ability or trait for the 
entire test. 
In other assessment applications, individuals may respond to items that are rated (for example, performance 
assessments), and an essential component to the assessments involve the use of scoring rubrics and raters. 
Englehard (2002) refers to this type of assessment as “rater-mediated” because performance assessments do 
not provide direct information about the constructs of interest but mediated information through 
interpretations by raters. Thus, the observed performance is dependent not only on the measured construct 
of interest (for example math ability) but on rater characteristics and behaviors such as rater severity, task 
difficulty, and the structure of the rating scale such as number of score levels. Thus, there is a confounding 
of rater and item effects inherent to item response. 
A major concern with rater-scored assessments is that raters bring a variety of potential biases to the rating 
process. Some examples that Englehard discusses include differential interpretation of the score scale by 
raters, bias in ratings from male as opposed to female responses, and bias in rater interpretations about the 
difficulty level of the tasks. Other researchers have classified rater errors in terms of halo effects, central 
tendency, restriction of range, and severity or leniency of raters (Saal, Downey, & Lahey, 1980). 

Chapter 1: Item Response Theory    13
 
Locally Dependent Item Sets (Testlets) 
For assessment applications that use testlets, one solution to the problem is to sum the item responses 
within each testlet into a single item and use an IRT model for polytomously scored items. However, this 
approach ignores information available from each item, and the approach can effectively be used only for 
dichotomously scored testlet items. Alternatively, item level information may be retained by adapting an 
IRT model to account for the dependence between items within testlets. For example, a modified GR model 
for testlets proposed by Wang, Bradlow, and Wainer (2002) can be estimated for applications that involve 
polytomously scored items. In this model, the probability that the ith examinee receives a category score k 
(k = 1, 2… mj) or higher on item j within testlet d(j) is defined as follows: 
ܲ௜௝௞
כ ሺɅሻൌ
݁஽௔ೕ൫஘೔ି௕ೕೖିஓ೔೏ሺೕሻ൯
ͳ ൅݁஽௔ೕ൫஘೔ି௕ೕೖିஓ೔೏ሺೕሻ൯ 
where Jid(j) is a random person-specific testlet effect added to the common GR model in order to model the 
interaction of a person with a testlet and the dependence between items within a testlet d(j). In this model, Ʌ௜ 
is typically assumed to have a N(0,1) distribution, and Jid(j) is assumed to be independent of Ʌ௜ and 
distributed as N(0, V2d(j)). The values of Jid(j) are assumed to be constant for an examinee over all items 
within a given testlet. Further, the testlet effect is assumed to vary across examinees subject to the 
constraint that σ ɀ௜ௗሺ௝ሻൌͲ
௜
. Because the variances of the testlet effects (V2d(j)) are testlet-specific, the 
testlet effects can vary across different testlets. As the variance or V2d(j) increases, the amount of local 
dependence increases; whereas when V2d(j) = 0, the items within the testlet can be treated as conditionally 
independent. Notably, Ip (2010) discusses that the item parameters for testlet models do not necessarily 
have the same interpretation as parameters from standard IRT models because the interpretations are 
conditional on the values of the person-specific testlet effects. 
Rater Effects 
One rater effect that has received much attention in the literature is rater severity/leniency, or the tendency 
to rate examinees at the low or high ends of the scoring rubric (Wolfe, 2004). Applications of measurement 
models to these types of assessments can incorporate rater effects and behaviors into the prediction of 
performance. A model described by Linacre (1989), the many-facet Rasch measurement model (MFRM), is 
an extended version of Andrich’s Rating Scale model that includes an additional parameter to quantify the 
severity for the rth rater (Or): 
ܲ௝௞௥ሺɅሻൌ
݁σ
൫஘ି௕ೕିதೕೖି஛ೝ൯
ೣ
ೖసబ
σ
݁σ
൫஘ି௕ೕିதೕೖି஛ೝ൯
೓
ೖసబ
௠
௛ୀ଴
 
for k = 0, 1, 2… mj for item j. In this model, the probability of a particular rating x to item j by rater r is a 
function of the ability of the examinee (T); the difficulty level for item j (bj); the difficulty of rating k 
relative to rating k1 for item j or the boundary between categories on the rating scale (Wjk); and the severity 
rating for rater r (Or). Furthermore, conditional independence is assumed across raters and items, and the 
severity of raters is accounted for when one estimates both item and person parameters. If the model is 
appropriate, then estimates of the person parameters are subsequently invariant to subsets of raters. 
Notably, other facets or dimensions of the assessment can also be modeled. For example, subcomponents 
or domains of the latent construct of interest, such as those associated with analytic scoring rubrics, could 
be included. Also, if gender and time facets are included in the model, interaction effects involving these 
facets and the rater facet could be examined to explore rater bias related to gender or “rater drift” over time. 
For more detail about the MFRM model and applications to performance assessments and indices to 
identify various rater errors, see Englehard (1997, 2002). 
One issue with the MFRM model involves assessment applications in which examinee responses are scored 
with the use of multiple raters. When multiple raters score the same response, the MFRM model cannot 
account for the possible dependency among rater response judgments. Research has found that proficiency 
estimates may have lower than expected standard errors (Bock, Brennan, & Muraki, 1999; Donoghue & 
Hombo, 2000; Patz, 1996; Wilson & Hoskens, 2001). Patz, Junker, Johnson, and Mariano (2002) discuss 

14    Bayesian Analysis of Item Response Theory Models Using SAS 
 
that, as the number of raters increases in the MFRM model, the model incorporates the information as 
added information about the ability being measured even though no additional items are administered. 
Patz, Junker, Johnson, and Mariano (2002) in their approach consider the hierarchical structure to 
assessments that are scored by multiple raters. In these testing applications, ratings are nested within item 
responses which are nested within person parameters. To apply IRT models in these types of applications, 
these researchers introduced a hierarchical rating (HR) model to account for this structure and the 
dependencies between rater judgments. This model, as well as others (for example, Verhelst & Verstralen, 
2001), introduces an additional latent variable that can be considered the “ideal rating” or an unbiased 
rating that is conditionally independent across items. Random errors such as lack of consistency and 
systematic errors such as rater tendencies are likely to cause raters to differ from this ideal rating. Estimates 
of this latent variable from a multiple-rating design can be considered a “consensus rating” among the rater 
judgments, and variability in the estimates reflects rater error or unreliability.  
Thus, in the HR model, the rater scores are indicators of examinee performance quality which in turn are 
indicators of the latent trait being measured (DeCarlo, Kim, & Johnson, 2011). The observed ratings are 
related to the ideal ratings (performance quality) using a latent class or signal detection theory model that 
estimates the conditional probability that a rater provides a rating at each score level given an ideal rating. 
This level of the model is used to quantify rater effects and rater precision. The ideal rating variables are 
random variables that depend on examinee ability and these in turn may be modeled using any item 
response function in another level of the model. For example, a K category PC model may be used, where 
[j corresponds to the ideal rating of item j for an examinee:  
ܲ௝௞௥൫ߦ௝൯ൌ
݁σ
൫ஞೕି௕ೕିఛೕೖି஛ೝ൯
ೣ
ೖసబ
σ
݁σ
൫ஞೕି௕ೕିதೕೖି஛ೝ൯
೓
ೖసబ
௠
௛ୀ଴
 
and T, bj, Wjk, and Or are defined as in the MFRM model. In addition, Patz et al. (2002) discussed how 
covariates introduced to predict rater behaviors can be incorporated into the HR model. For example, 
parameters could be modeled that examine the effects of rater and examinee background variables or item 
features. 
IRT Models for Multidimensional Response Data 
The models discussed to this point are appropriate for analyzing item responses that are assumed to be 
determined by a single person characteristic or latent trait being measured (unidimensional model). 
However, multidimensionality in item responses for assessments is often observed. Multidimensionality 
can be caused by various factors, such as designed test structure, unintended construct-irrelevant variance, 
and mixed item formats. For example, when an assessment instrument is designed to measure more than 
one ability or trait, the item responses would be expected to exhibit a multidimensional structure. 
Construct-irrelevant variance can manifest as multidimensionality when, for example, mathematics ability 
is being measured by constructed response items that require examinees to explain answers. Variability in 
performance of the items may be due to both the mathematics and communication abilities of examinees. 
Finally, including items with different item formats, such as multiple-choice and constructed response, can 
induce multidimensionality in item response data. 
There are different ways of classifying multidimensionality (Reckase, 1997a, 1997b). One classification 
defines multidimensionality as between-item or within-item multidimensionality. Between-item 
multidimensionality is assumed if only one of the ability/trait dimensions in the model determines the 
response to an item (that is, simple structure), whereas within-item multidimensionality is assumed when 
an item response is dependent on multiple dimensions simultaneously (that is, complex structure). In 
addition, different types of multidimensional IRT (MIRT) models have been proposed: compensatory, 
noncompensatory, and bi-factor models. 
In predicting item response, compensatory models assume that a low level or status in one dimension can 
be compensated for by higher levels in the other dimensions. Simple generalizations of the logistic model 

Chapter 1: Item Response Theory    15
 
for dichotomously scored items or simple generalizations of the GR model have been discussed for 
compensatory MIRT models. For example, in a multidimensional version of the 2P logistic model, all that 
changes is the logistic deviate for item j which is typically expressed in a slope intercept form: 
ܲ௝ሺɅሻൌ
݁௓ೕ
1+݁௓ೕ
where, zj = D6h ajhTh + dj, Th is the ability level for dimension h, ajh is the slope (discrimination) parameter 
for item j on dimension h, and dj is an intercept term for item j and related to item difficulty through a 
composite function of the slope and difficulty parameters for item j. In matrix notation, the logistic deviate 
can be expressed as zj = D ajT+ dj where a is a vector of slope parameters and T is a vector of person 
parameters on each of the h dimensions. The primary difference between this compensatory MIRT model 
and its unidimensional counterpart is that the response model involves a weighted linear combination of Th 
and ajh parameters. In other words, multiple dimensions are modeled each with a corresponding slope 
parameter to reflect the relationship between the item and each dimension.  
Analogous to item characteristic curves (ICCs) for dichotomous items (for example Figure 1.1), item 
response surfaces may be used to display the probability of response across two T dimensions. For an 
introduction to MIRT models, see Ackerman, Gierl, and Walker (2003). For other research offering a more 
technical treatment, see for example, Ackerman (1994, 1996), McDonald (1999), Muraki & Carlson (1995) 
and Reckase, (1997a, 1997b). 
Noncompensatory models express response probability as a multiplicative function of response 
probabilities associated with each dimension (Sympson, 1978). Thus, a threshold (difficulty) parameter is 
included for each dimension in addition to person and slope parameters. For example, extending the 2P 
logistic model to the multidimensional case, a noncompensatory MIRT model for a dichotomously scored 
item, j, may be expressed as follows:  
୨൫୨ൌͳȁ૑୨ǡ Ʌ൯ൌȲሺ୨ሻൌෑ݁௓ೕ
݁௓ೕ
ୌ
୦ୀଵ

where zj = ajh(Th  bjh ), ajh is the slope (discrimination) parameter of item j on dimension h, and bjh is the 
threshold parameter for item j on dimension h. As can be seen, this model multiplies the probability of a 
response on each dimension together to obtain an overall probability of a response. The approach is 
noncompensatory in nature in that successful performance is dependent on successful performance on each 
component. For example, a low probability of success on one dimension results in an overall low 
probability of success on the task even if the probability of success on other dimensions is high. 
Noncompensatory MIRT models have received far less attention in the literature possibly due to the 
number of parameters that require estimation. However, these models have been effectively used to blend 
IRT models and cognitive psychology as it relates to the measurement of skills and abilities. For example, 
in Embretson’s multicomponent model (Embretson, 1997), performance on a task is modeled as a 
multidimensional Rasch model with a profile of cognitive components or skills identified for each task. 
Each component has an associated ability level and difficulty level, and the product of each component 
model provides the overall probability of success on the item. Thus, these models have the potential to help 
researchers better understand what features make a task difficult and what features require instructional 
focus. Recently, Embretson (2015) extended these models to provide diagnostic measurement of abilities 
for subdomains (components) and skill or attributes within subdomains in broad heterogeneous tests.  
Notably, a broader class of diagnostic classification models, which have also been designed to evaluate the 
extent to which individuals possess a multidimensional profile of skills or attributes underlying 
performance on items, have also been developed and discussed in the literature (Rupp, Templin, & Henson, 
2010). These models have been characterized as both compensatory and noncompensatory IRT models.  
Finally, another MIRT model that has recently received attention is the bifactor model. In the bifactor 
model, item responses are determined or explained by both a general or dominant factor and secondary 
orthogonal factors (Gibbons & Hedeker, 1992). Thus, the bifactor model allows for measuring a general 

16    Bayesian Analysis of Item Response Theory Models Using SAS 
 
ability or trait while controlling for the variance that arises due to the measurement of other factors. This 
model has received a good deal of attention, not only in educational assessment, but more recently in health 
and behavioral assessment (Gibbons, Immekus, & Bock, 2007).  
In this model, slope parameters (or factor loadings) are estimated for each item on the general factor as well 
a single secondary factor. For example, in a 6-item test, slope parameters (aj1) for Items 16 are estimated 
for Factor 1, slope parameters (aj2) for Items 14 are estimated for Factor 2, and slope parameters (aj3) for 
Items 5-6 are estimated for Factor 3. In matrix notation this would be displayed as follows: 
ۉ
ۈۈۈ
ۇ
Ƚଵ଴
Ƚଵଵ
Ƚଶ଴
Ƚଶଵ
Ƚଷ଴
Ƚଷଵ
Ƚସ଴
Ƚସଵ
Ƚହ଴
Ƚହଶ
Ƚ଺଴
Ƚ଺ଶی
ۋۋۋ
ۊ
 
 
The bifactor model can be considered a constrained case of a compensatory MIRT model where the first 
factor is defined as a general factor, and slope parameters for subsets of items are constrained to be 0 on 
secondary factors. 
Differential Item Functioning and Mixture IRT Models 
When evaluating the validity of inferences for scores, you should evaluate sample homogeneity and 
whether items function differently across subpopulations of examinees (for example, masters versus non-
masters; males versus females). When there is differential item functioning (DIF), measurement of the 
underlying abilities or traits is not considered invariant across subpopulations. In the context of IRT 
models, measurement invariance does not hold, or DIF exists when the ICCs for an item across 
subpopulations differ significantly. In other words, the probability of a response is different across 
subgroups of examinees with the same status on the trait being measured. When DIF exists, separate IRT 
models should be estimated and reported for subpopulations of examinees.  
There are a variety of possible sources for DIF that can cause groups of examinees to interpret and respond 
to questions in different manners. Zumbo (2007) provides a historical perspective for the discussion of DIF 
as well as the future of DIF assessment. Sources of DIF that he discusses include characteristics of test 
items, such as item content and format, as well as contextual factors and testing contexts, such as testing 
environment and socioeconomic status. 
DIF can be examined with the use of a multiple-group IRT model. In this approach, a test for DIF would 
involve testing a model in which item parameters across groups are estimated or unconstrained, against a 
model in which item parameters are constrained to be equal across groups. If the unconstrained model is 
preferred, then DIF would be indicated. In addition, the mean level of the trait or ability being measured in 
each group could also be compared to determine whether overall differences exist between the groups in 
the abilities or traits that are being measured. 
In the case of a multiple-group approach, the purpose is to evaluate measurement invariance across defined 
subpopulations or known groups based on observed variables, such as gender, and all members of a 
subpopulation are assigned to a particular group (such as males). However, another purpose of a DIF 
analysis is to detect and characterize subgroups of examinees based on their performance. In this case, the 
source or sources of examinee item-response heterogeneity are unobserved, and the assignment of 
examinees to latent classes is a random variable that is inferred from the item responses. For example, 
latent classes may consist of predominantly one subpopulation (for example, males), or mixtures of 
subpopulations (for example, males who graduated HS), and membership in each latent class is unknown 
and to be estimated.  

Chapter 1: Item Response Theory    17
 
In the case where group or latent class membership is to be estimated, mixture IRT models may be used to 
cluster examinees into latent classes based on different response patterns (Bolt, Cohen & Wollack, 2001). 
Mixture IRT models assume that the population of examinees consists of qualitatively different 
subpopulations or latent classes. Mixture IRT models provide a mechanism for clustering examinees to 
better understand any heterogeneity in item responses. An interesting application in the area of health 
assessment is a study that examines sample heterogeneity in responses from patient-reported outcomes 
(Sawatzky, Ratner, Kopec, & Zumbo, 2012). For a useful comparison of methods for studying population 
heterogeneity, see Lubke and Muthén (2005). 
In the context of examining DIF in educational assessments, the latent classes typically reflect a 
demographic characteristic such as gender or a combination of demographic characteristics. However, Cho 
and Cohen (2010) discuss the use of mixture models in DIF analyses to identify several testing effects, 
including use of different response strategies by examinees. In the context of attitude surveys, the latent 
classes could reflect clusters of individuals with different levels of response latitude or different ranges in 
response options that individuals are willing to endorse (Lake, Withrow, Zickar, Wood, Dalal, & 
Bochininski, 2013). Finally, in the context of personality assessment, Maij-de Meij, Kelderman, and van 
del Flier (2008) discuss the use of mixture models to detect latent classes related to use of the response 
scale, including tendency to select extreme ratings, midpoints, and filters such as Don’t Know.  
In IRT mixture model applications, differential performance and identification of latent classes can be 
examined by extending any of the IRT models to include group-level item parameters for each group or 
latent class g or Pj (Ujg = ujg | g, Zjg, Tg). In these models, the assumed IRT model holds across the latent 
classes, but each latent class has a unique set of model parameters that are estimated in addition to a model 
parameter reflecting the probability of latent class membership. These models may also be considered 
multidimensional IRT models because two person characteristics (subgroup membership and the trait being 
measured) determine item response in the model.  
In addition, the structure of the latent trait being measured is defined by class-specific means and standard 
deviations (Pg, Vg). Therefore, mixture IRT models may be used to account for qualitative differences 
between examinees (Rost, 1990). Measurement invariance across latent classes can be further explored if 
you compare non-invariant or unconstrained models across classes with partially or fully invariant models. 
These comparisons, as well as determining the number of modeled latent classes, can be conducted using 
goodness-of-fit measures such as information-based indices (for example, AIC, BIC). For a detailed 
discussion of mixture models, see von Davier and Rost (2007). 
Hierarchical Models—Multilevel and Random Effects IRT Models  
In the IRT models discussed above, the models make an assumption of independence of item response 
functions conditional on the modeled latent traits (T) in order to define the likelihood function. However, in 
some testing applications that reflect a nested structure of item responses within clusters of persons, within 
clusters of items, or both, this assumption may not be reasonable. For example, in testing applications 
where students are clustered within schools, the responses of the students in a school (cluster) are likely to 
be more related than is accounted for by their latent traits alone. Persons can also come from clusters of 
neighborhoods, and the neighborhood context could influence responses to behavioral assessments. Finally, 
in two examples discussed already, the conditional-independence assumption may not be reasonable for 
testing applications that use testlets or multiple raters.  
In these types of testing applications, hierarchical models, also referred to as multilevel, random effects, or 
mixed effects models, can be used to model any dependence within clusters of persons or items. Different 
types of hierarchical models are motivated and discussed further below. Chapter 6 introduces the 
hierarchical IRT model and illustrates an application of this approach. For more detail about hierarchical 
IRT models, see Johnson, Sinharay, and Bradlow (2007) and Fox (2010). 
A multilevel modeling approach to many educational and psychological or behavioral assessments 
conforms to the natural structure of item responses. Any IRT model can be conceptualized as a multilevel 
model with item responses or observations nested within persons. In this case, the person parameters are 
treated as random components, and item parameters are treated as fixed components (Fox, 2010; Fox & 

18    Bayesian Analysis of Item Response Theory Models Using SAS 
 
Glas, 2001). However, persons are often nested within clusters such as classrooms or schools. In multilevel 
IRT models, an item response model is used to measure the latent traits or abilities of persons, and this 
model is integrated with structural models to explain variability in trait or ability estimates at different 
levels and clusters of persons (Fox, 2010).  
In multilevel IRT models, the Level 1 model is an IRT model or a measurement model expressing the 
probability of a response, conditional on the modeled item and person characteristics. At this observational 
level, measurement error is associated with the individual Ti for persons. The Level 2 model, or structural 
model, is used to describe the relationship between the latent variable measured at Level 1 (Ti) and any 
covariates, xq, that are introduced to explain variability or uncertainty in T for persons. At this level, the 
latent variables measured for persons are now dependent variables: Ti E0 +E1x1i + …+ Eqxqi + Hi with the 
random effects for persons Hi ~ N(0, VT). Additional structural models or levels can be included to reflect 
clusters of persons such as persons nested within classrooms or schools (Level 3). If persons are clustered 
in groups, such as schools, then an index for the G groups is included in the model. The Eqg coefficients 
(random regression effects) now become the dependent variables with cluster level covariates, ywi, 
introduced: Eqg J0 +J1y1g + …+ Jwywg + Pqg and Pqg ~ multivariate N(0, 7).  
For this three-level model, Fox (2010) expresses the joint response probability of the response data (u) for 
the combined measurement and structural models as a product of the Level 1 and 2 models across I 
examinees, and the Level 3 model across G groups or clusters: 
ܲ൫ݑ௜௚หɅ௜௚ǡ ૑ሻ 
ܲ൫Ʌ௜௚หݔ௜௚ǡ ઺ࢍǡ ߪఏ
ଶሻ 
ܲ൫Ⱦ௤௚หݕ௚ǡ ߛǡ ࢀሻ 
where P(uig | Tig, Z) is the Level 1 IRT model within persons, P(Tig | xig, Eg, VT) is the Level 2 model with a 
set of X explanatory variables for persons within clusters (xig), and P(Eqg | yg, J, 7) is the Level 3 model with 
a set of Y explanatory variables across clusters of persons (yg).  
One of the primary advantages of a multilevel modeling approach is that variables can be introduced to 
explain variability in parameters and model measurement error at the different levels of the multilevel 
model. For example, a person explanatory model could introduce variables at the person level. For 
example, parent’s educational attainment may explain variation in the ability level of examinees, or level of 
physical activity in persons may explain variation in the physical health status measured in persons. Both of 
these variables are person properties introduced to explain variability in the trait being measured at the 
person level (Natesan, Limbers, & Varni 2010).  
Explanatory variables can also be introduced at other levels of the model to examine between-group 
differences – for example, alignment of school curriculum to content standards at the school level (groups 
of persons). Fox (2010) discusses that the introduction of explanatory variables can also lead to more 
accurate parameter estimation (for example, using collateral information about examinees). Further, models 
with different structural components but the same measurement model can also be compared directly. Note 
that IRT models have also been discussed that combine mixture and multilevel approaches (Cho & Cohen, 
2010). These models may be used to identify latent groups of persons that exhibit common response 
performance and these latent groups in turn can be used to explain differences in item level performance. 
Hierarchical models can also be used to model random item effects where covariates can be introduced to 
explain variability in item parameters. For example, covariates reflecting task features or cognitive skill 
requirements for items could be useful for test construction (Embretson & Reise, 2000). An example 
discussed by Fox (2010) considers “cluster-specific item effects” that can be used to relax the assumption 
of measurement invariance in IRT models. In these models, random person and random item effects are 
assumed. They can be used to model variability in latent trait distributions across groups at the level of 
persons and model variability in item parameters across groups at the level of items. In the context of 

Chapter 1: Item Response Theory    19
 
dichotomously scored responses, Fox describes the model for J items as a function of person ability (Ti) and 
group-specific item parameters (ag bg): 
ܲ൫௜௝௚ൌͳหࢇ௚ǡ ࢈௚ǡ Ʌ௜ሻൌܨሺܽ௚Ʌ௜െܾ௚െɂ௕௝௚൅ɂ௔௝௚Ʌ௜ሻ 
where the error terms, Hbjg ~ N(0, Vbg), Hajg ~N(0, Vag), and F can be the normal ogive or logistic ogive 
functions. This model can be used for another type of analysis of DIF, where a model reflecting 
measurement invariance, or a model with no group-specific item parameters, is compared with a model 
reflecting group-specific item parameters or DIF.  
Evaluation of IRT Model Applications 
To evaluate the validity of an IRT application to an assessment, a number of assumptions underlying the 
use of IRT models require examination (Embretson & Reise, 2000; Hambleton & Swaminathan, 1985; Yen 
& Fitzpatrick, 2006). When these assumptions are not sufficiently met, the validity of any inferences from 
the IRT model application is compromised. 
Dimensionality 
IRT models predict examinee performance in relation to one or more underlying dimensions (traits or 
abilities). With unidimensional IRT models, one latent trait is assumed to determine an examinee’s 
performance; in other words, items are homogeneous in their measurement of the measured trait or ability. 
However, responses to an assessment will always be multidimensional in nature and reflect nuisance or 
construct-irrelevant variance (for example, context effects) in addition to the number of factors measured 
by design. Nuisance or minor factors reflect random multidimensionality, whereas the number of 
dimensions a test is designed to measure reflect fixed dimensionality (Wainer & Thissen, 1996). 
Multidimensional IRT models assume that multiple traits underlie performance and may be used when 
more than one latent trait determines an examinee’s performance.  
Assessing the number and nature of the dimensions underlying item responses is relevant both to evaluating 
the application of an IRT model and to understanding the construct(s) being measured. Reckase (1994) 
argued that the exploration of the dimensionality may also be affected by the purpose of the assessment. 
When the structure of the assessment is the focus, overestimating the dimensionality may be more desirable 
than underestimating the dimensionality. However, if estimation of the trait or ability is the focus, then the 
minimum dimensionality that provides the greatest information and most meaningful interpretation is the 
objective. If dimensionality is overestimated, more parameters are estimated, and in turn estimation error 
may increase. 
Factor analytic or cluster-based methods have been used to explore the dimensionality in test item 
responses because these methods attempt to explain the associations among a set of variables (items) in 
terms of a smaller number of factors (dimensions). If the objective is to evaluate whether unidimensional 
IRT models may be useful in a testing application, Stout (1987; 1990) describes a nonparametric test for 
“essential unidimensionality” of a data set. These methods evaluate the presence of a dominant factor and 
possible nuisance factors. Finally, MIRT models may be estimated and compared with unidimensional IRT 
models. For example, with compensatory MIRT models, the pattern of slope parameters is analogous to the 
pattern of factor loadings. Nested models with different numbers of ability parameters can be compared to 
assess dimensionality. For a general discussion of these approaches in the context of IRT applications, see, 
for example, Hambleton and Swaminathan (1985), Embretson and Reise (2000), and Yen and Fitzpatrick 
(2006).  
Local Independence 
A second assumption is local independence. That is, after accounting for the underlying trait being 
measured (or conditional on T, examinee responses to items are statistically independent. This assumption 
is essential to the computation of the likelihood function and the estimation of model parameters. Note that 
in a MIRT model context, local independence now refers to a coordinate space for the multiple traits being 
measured.  

20    Bayesian Analysis of Item Response Theory Models Using SAS 
 
Yen (1993) discusses a number of possible sources for local item dependence between items. Some sources 
reflect unintended dimensions that are measured or unmodeled person characteristics, external assistance or 
interference, fatigue, speededness, practice, item or response format, scoring rubrics, or differential item 
functioning among subpopulations of examinees. Other sources may reflect unmodeled rater characteristics 
(for example, halo or leniency effects), and item interactions that may include dependence from use of a 
common passage or stimulus for a set of items, context effects, items that are “chained” or organized into 
steps, and items that share the same scoring rubric.  
Several statistical tests have been proposed to detect the presence of local item dependence. Yen (1984; 
1993) described a statistic, Q3, which examines the difference between examinees’ observed and expected 
item performance under a given IRT model, and computes the correlation for these residual terms across 
examinees for pairs of items. In essence, the statistic examines the correlation between items after 
accounting for overall test performance or ability. A disadvantage of the Q3 statistic is that Q3 requires 
estimates of T for examinees in order to compute expected frequencies. Alternatively, Chen and Thissen 
(1997) discuss several other statistics that do not depend on the estimates of T. These statistics examine the 
independence of item pairs based on estimated marginal frequencies under the given IRT model.  
Yen (1993) also discusses some ways to manage local independence in the different stages of test 
development, administration, and analysis. More relevant to the purpose of this book is what can be done at 
the analysis stage. At this stage, a commonly used approach with dichotomously scored tests is to construct 
“testlets” or sets of items that combine the scores from each set of the dependent items into a single item 
(Wainer & Kiely, 1987). For example, with reading passages followed by five dichotomously scored items, 
testlets may be formed where each testlet for a passage would now have a score range of 0 (no correct 
answers) to 5 (all correct answers). The testlets are then scaled using IRT models for polytomously scored 
responses such as those discussed above (for example, the GR model). Another approach to managing 
locally dependent item responses is to model the dependency of items in IRT models directly. These 
models incorporate a parameter in the model that reflects the degree of dependence among sets of locally 
dependent items or testlets (see the section “Locally Dependent Item Sets (Testlets)” in this chapter for 
detail). 
Form of the IRT Model 
A third assumption concerns the form of the IRT model, or the function and model parameters used to 
describe the relationship between the trait being measured and the probability of a response for each item 
(item response function). Although most functions assume that the probability of a response is non-
decreasing with increasing levels of the trait being measured (monotonicity), different model parameters 
may be included to explain test response performance. For example, the nature of the assessment may 
dictate the item characteristics that are included in the model. However, model comparison methods may 
also be used to statistically compare models that include different item characteristics (for example, 1P as 
opposed to 2P IRT models). Chapter 7 discusses model comparison techniques that may be used in a 
Bayesian analysis to compare competing IRT models or models with differing sets of model parameters. 
Speededness 
Often included as a specific assumption is that a test is nonspeeded, or examinees theoretically have as 
much time as needed to respond. In essence, this assumption reflects an unmodeled person characteristic 
because individual differences in examinees’ test-taking behaviors may affect test response performance. 
Hambleton and Swaminathan (1985) discuss several strategies for evaluating the presence of speededness 
in a testing application.  
Model Fit 
The fit of a model, or the correspondence between model predictions and observed data, is generally 
regarded as an important property of model-based procedures. When a model does not fit the data, the 
validity of using results (for example, estimated T parameters) may be compromised. In general, you can 
evaluate IRT model applications by comparing observed results with model-based predictions—that is, by 
examining model-data fit.  

Chapter 1: Item Response Theory    21
 
There are two types of model fit that can be examined: (1) item fit that can be evaluated for sets of items 
(test level) or for individual items (item level) and (2) person fit. Lack of fit for items indicates that 
observed responses (across examinees) do not conform well to model-based expectations using the 
assumed IRT model and estimated item parameters. In other words, lack of fit for items indicates that the 
assumed IRT model has uncertain validity because it does not accurately model how examinees respond to 
the items. Person fit, on the other hand, refers to the validity of model parameter estimates for specific 
examinees. In other words, the assumed IRT model and model parameter estimates have uncertain validity 
for a specific examinee because they should not be used in combination with an examinee’s item responses 
(response pattern) to estimate his or her ability or trait. 
Because IRT models afford predictions of expected item response for each examinee, E[uj] = Pj(Z, T), and 
predictions of expected test score across items in a test for each examinee, 6E[uj] = 6Pj(Z, T), these 
predictions may be summarized across examinees and compared with observed results. For example, at the 
test level, the observed numbers of examinees with each possible total or summated score can be compared 
with model-based expectations (see, for example, Béguin & Glas, 2001; Orlando & Thissen, 2000); or if 
the number of response patterns is reasonably small, observed counts of individuals with each response 
pattern can be compared with expected counts under the model. 
For unidimensional IRT models, individual item fit has typically been assessed by comparing observed 
item performance for various subgroups of examinees with performance that is predicted under the chosen 
model. For each item, a goodness-of-fit test can be obtained as follows: 
1.
Estimate the IRT model parameters (Z, T).  
2.
Approximate the continuous distribution for T by a small number of discrete T subgroups (for 
example, 10 T subgroups).  
3.
Cross-classify examinees, using their T estimate and their score response, into a G x K table, 
where G corresponds to the number of T subgroups and K represents the number of response 
categories.  
4.
Construct an observed score response distribution for each T subgroup by pooling or tallying the 
number of examinees in each combination of T subgroup and score response categories. 
5.
Construct an expected score response distribution across score categories for each T subgroup 
using the IRT model, item parameter estimates, and a T level representing the discrete ability 
subgroups (for example, midpoint of subgroup). 
6.
Compare model-based predictions for response score distributions from Step 5 with observed 
score response distributions from Step 4, using residual-based statistics or goodness-of-fit 
statistics (for more detail, see Hambleton & Swaminathan, 1985; Yen & Fitzpatrick, 2006). 
Rather than focus on whether the predictions afforded by each item or sets of items under the model match 
the examinee population, person fit is concerned with whether a particular examinee’s response pattern 
conforms to predictions based on the model. In the case of dichotomously scored items, for example, an 
IRT model implies that examinees with low Twould be expected to respond correctly to easy items and 
incorrectly to more difficult items; whereas examinees with high Twould be expected to respond correctly 
to both easy and more difficult items. A response pattern that does not match the model is called aberrant 
and compromises inferences about an examinee’s estimated score (T). There are a number of possible 
sources for aberrant responses patterns, including, for example, cheating and careless responding in the 
case of educational assessments, and individuals engaged in response sets such as socially desirable 
response or acquiescence in the case of psychological and behavioral assessments. 
One common approach to assessing person fit is to compare the relative likelihood of an examinee’s 
observed response pattern (Ti), given the IRT model parameter estimates (Z,T), in a population of 
examinees with T Ti. For example, a lZ statistic that assumes a standard normal null distribution has been 
discussed (Draskow, Levine, & Williams, 1985) as well as other person fit statistics (see Karabatsos, 2003 
for a review of 36 different person fit statistics). 
In a Bayesian analysis, the posterior predictive model-checking (PPMC) method is a popular model-
checking tool and has proved useful for evaluating model fit with IRT models (see, for example, Béguin & 

22    Bayesian Analysis of Item Response Theory Models Using SAS 
 
Glas, 2001; Levy, Mislevy, & Sinharay, 2009; Sinharay, 2005, 2006; Sinharay, Johnson, & Stern, 2006; 
Zhu & Stone, 2011). A separate chapter on model checking or evaluating IRT model applications is 
included (Chapter 8) because model checking requires programming and methods beyond the estimation of 
the IRT models using PROC MCMC. In PPMC, statistics that measure a particular aspect of model fit (for 
example, total score or item associations) or other statistics used to evaluate IRT assumptions (for example, 
goodness-of-fit statistics, Q3), are identified and computed for the observed data. These statistics are also 
computed for data sets of item responses predicted by the estimated model to examine the relative 
consistency of the observed result in a distribution based on model predictions.  
The advantages of using PPMC for evaluating IRT model-fit are threefold: (1) PPMC takes into account 
uncertainty in parameter estimation by using posterior distributions for model parameters rather than point 
estimates; (2) it can be used for assessing fit of complex IRT models that can only be estimated using 
Bayesian methods; and (3) PPMC constructs null sampling distributions empirically from MCMC 
simulations rather than relying on analytically derived distributions. This latter advantage is particularly 
useful because many of the fit statistics and associated hypothesis tests that are currently used in practice 
are based on assumed null distributions (for example, standard normal or chi-squared) that are typically not 
well approximated (see, for example, Karabatsos, 2003; Stone, 2000). 
IRT Model Parameter Estimation 
The basis for estimating parameters of an IRT model is the log of the likelihood function expressing the 
probability of the response pattern, u, across J items with K categories for each ith examinee: 
ሺ܃ ൌܝȁ૑ǡ Ʌሻൌ෍෍୧୨୩
୏
୎
ሾ୨୩ሺɅ୧ሻሿ 
where vijk is an indicator variable (1 if examinee i responds in category k to item j; 0 otherwise). The 
maximum of the log likelihood function relative to the different parameters being estimated can then be 
computed across examinees to derive maximum likelihood (ML) estimates, or a marginal log likelihood 
function can be used where the T parameter, a nuisance parameter, is removed by integrating the likelihood 
function over the assumed T distribution for the population of examinees. The maximum of this marginal 
likelihood function relative to the different item parameters being estimated can then be computed to derive 
marginal maximum likelihood (MML) estimates for item parameters. Note that the log of the likelihood 
function is typically used because it is more mathematically tractable than the likelihood function, which 
uses products of item response probabilities. For an introductory treatment of estimation methods based on 
the likelihood function for IRT models, see Yen and Fitzpatrick (2006).  
As is well known, IRT models are not identified because the ability or trait parameters and item difficulty 
or threshold parameters are unobservable and on the same scale. Thus, model predictions are invariant “up 
to a linear transformation,” and the metric for either the T or b parameters must be defined (see, for 
example, Hambleton & Swaminathan, 1985; Yen & Fitzpatrick, 2006). Typically, one defines the metric 
for each T dimension by specifying the mean and standard deviation for the population Thdistributions (for 
example, Normal (0,1)). In other words, the latent traits or abilities are assumed to be random samples from 
standard normal distributions. However, you can also anchor the metric for the scale by fixing the mean at 
0 for item difficulty or threshold parameters and standard deviation for item difficulty or threshold 
parameters at 1. One advantage of a Bayesian paradigm is that specification of the prior distribution for T 
resolves the identification problem.  
With regard to estimating standard unidimensional IRT models, you can reasonably apply the models to 
large-scale assessments by using available software (for example, IRTPRO; Cai, Thissen, & du Toit, 2011). 
For assessments that are administered on a smaller scale, Reise and Yu (1990) recommend a sample of at 
least 500 examinees for the graded IRT model. Others have suggested that ratios of the sample size to the 
total number of item parameters or to the number of item categories are more important for accurate 
parameter estimation (Choi, Cook, & Dodd, 1997; De Ayala, 2009; DeMars, 2003). For example, De Ayala 
suggests a ratio of 10:1 as a general heuristic for the number of examinees relative to the total number of 

Chapter 1: Item Response Theory    23
 
item parameters (number of parameters per item × number of items) if the sample is normally distributed, 
and a ratio that is somewhat greater for nonnormal examinee distributions.  
Other researchers (for example, Hambleton & Swaminathan, 1985; Lord, 1980) have discussed problems 
with the ML-based estimation of item parameters. For example, slope parameter estimates near zero can be 
obtained in combination with very large absolute estimates of item threshold parameters; slope parameter 
estimates for some items may not converge and continue to increase with each iterative cycle in the 
estimation algorithm; and for items with low slope values and threshold parameters centered below the 
midpoint of the theta scale, isolating and estimating a lower asymptote parameter may prove difficult. 
Finally, ML-based estimation methods are more complicated to implement for more highly parameterized 
IRT models such as MIRT and mixture IRT models. 
Interest in estimating IRT models using Bayesian methods has grown tremendously during the beginning of 
the 21st century. In part, this growth is due to the appeal of the Bayesian paradigm among psychometricians 
and statisticians, as well as the advantages of these methods with small sample sizes, when parameter 
estimation is unstable, and for more complex or highly parameterized models. In Bayesian estimation 
methods, for example, constraints may be inherently imposed on the parameter space through prior 
distributions so that parameters do not drift out of bounds. For more complex IRT models, a Bayesian 
approach has been found particularly useful (see, for example, Béguin & Glas, 2001; Bolt & Lall 2003; 
Bradlow, Wainer, & Wang, 1999; Cho, Cohen, & Kim, 2013; DeMars, 2006; Li, Bolt, & Fu, 2006; Patz et 
al., 2002; Yao & Schwarz, 2006; Zhu & Stone, 2011; Zhu & Stone, 2012). Finally, with commonly used 
estimation methods for IRT models (for example, MML), person parameters are not estimated 
simultaneously with item parameters. Rather, person parameters are estimated by fixing item parameters at 
estimated values and estimating person parameters in a separate analysis. A Bayesian approach estimates 
person parameters simultaneously with item parameters, and in doing so incorporates or models any 
uncertainty in item parameter estimates when estimating point estimates and uncertainty in person 
parameter estimates. 
 
 

24    Bayesian Analysis of Item Response Theory Models Using SAS 
 
 

Chapter 2: Bayesian Analysis 
Introduction ......................................................................................................... 25 
Elements of Statistical Models and Inference ..................................................... 25 
Frequentist and Bayesian Approaches to Statistical Inference ........................... 26 
Use of Bayesian Analysis to Estimate a Proportion—An Example ........................ 28 
Choosing a Prior Distribution .............................................................................................. 29 
Computing the Posterior and Drawing Inferences ........................................................... 30 
Comparing Bayesian and Frequentist Estimates for Proportions .................................. 30 
Bayesian Estimation and the MCMC Method ....................................................... 31 
SAS Code for Implementing a Metropolis Sampler to Estimate a Proportion .............. 32 
MCMC and the Gibbs Sampler ........................................................................................... 35 
Burn-In and Convergence .................................................................................... 35 
Informative and Uninformative Prior Distributions .............................................. 38 
Model Comparisons ............................................................................................. 39 
Deviance Information Criterion ........................................................................................... 39 
Bayes Factor ......................................................................................................................... 40 
Model Fit and Posterior Predictive Model Checks ............................................... 41 
Introduction 
Interest in Bayesian analysis has grown tremendously in recent years. In part, this growth is due to the 
appeal of the Bayesian paradigm among psychometricians and statisticians, as well as to the advantages of 
these methods when analyzing more complex or highly parameterized models. In contrast to traditional 
approaches for estimating model parameters, a Bayesian paradigm considers model parameters to be 
random variables, and Bayes theorem is used to obtain probability distributions for model parameters. 
However, interest in Bayesian analysis has also grown because of the availability of software that may be 
used to implement general Bayesian estimation methods—for example, WinBUGS (Spiegelhalter, Thomas, 
Best, & Lunn 2003) and the SAS MCMC procedure. 
This chapter provides a basic introduction to Bayesian analysis. The purpose is to provide foundational 
material to support the discussion in subsequent chapters that describe the implementation of Bayesian 
analysis with the use of PROC MCMC, and to support the discussion of the programs and results related to 
estimating item response theory (IRT) models with PROC MCMC.  
Among the numerous sources for a more detailed and theoretical discussion of Bayesian analysis, see, for 
example, Congdon (2006); Fox (2010); Gelman, Carlin, Stern, Dunson, Vehtari, & Rubin (2014); Lunn, 
Jackson, Best, Thomas, and Speigelhalter (2013). Fox (2010) is also an excellent reference for a technical 
treatment of Bayesian analysis of IRT models. 
Elements of Statistical Models and Inference 
Statistical models are used to structure data or variables that are measured into formulized relationships 
between the variables. For example, factor analysis models allow for structuring a larger set of observed 
variables into a smaller set of factors so that the relationships can be more easily discussed and evaluated. 

26   Bayesian Analysis of Item Response Theory Models Using SAS 
 
 
IRT models formulize the relationships between observed item responses and latent or unobserved 
characteristics of items and persons.  
Statistical inference, on the other hand, is used to separate signal from noise and decide which model or 
formulized relationships are relevant (that is, signal). For example, is there evidence that two factors 
explain the relationships between the set of observed variables? Or can test item responses be adequately 
explained by the modeled item characteristic (item difficulty) and one person characteristic (ability being 
measured by the items)?  
The three basic goals of statistical inference are estimation, prediction, and model selection. With regard to 
estimation, the parameters of the statistical model (for example, item parameters in IRT models or factor 
loadings in factor models) are estimated, and uncertainty about each parameter estimate is quantified. 
Estimation enables you to better understand the nature and importance of the formulized relationships 
relative to the uncertainty in estimating the model parameters. Prediction involves using current beliefs 
about a model and the estimated parameters to predict other values—for example, values that were not 
observed for some subjects in the population or values for missing data. Based on the estimated model, 
predictions can also be made for future samples from the population. Finally, model comparison and 
selection involves choosing a preferred model among competing models, and determining whether a 
particular model reflects “overfitting” (noise is being modeled) or a particular model reflects “underfitting” 
(signal in the data is missed). 
Frequentist and Bayesian Approaches to Statistical Inference 
The frequentist approach to statistical inference or more classical statistical analysis relates observed data 
to unknown fixed parameters, obtains inferences about parameter estimates from the data, and bases 
probability interpretations for model parameters on a hypothetical sequence of random observations or 
experiments. In contrast to this classical paradigm, a Bayesian approach considers model parameters to be 
random variables and is used to obtain distributions for model parameters from which direct probability 
statements about parameters may be made.  
Bayesian analysis begins with an assessment about the uncertainty in a parameter (prior distribution) before 
data is collected. Using Bayes Theorem, distributions for the parameters given the data (posterior 
distribution) may be obtained by updating beliefs about how parameters are distributed (prior distribution) 
with the distribution of data given the parameters (likelihood function). Table 2.1 provides a summary of 
the comparison between Bayesian and frequentist paradigms. 
 
 
 

Chapter 2: Bayesian Analysis   27 
 
Table 2.1:  Comparison of a Bayesian and a Frequentist Approach to Statistical Inference 
Bayesian  
Frequentist  
Assumes that model parameters (such as mean or 
proportion) that generated data are unknown 
random variables. This assumption allows for 
making direct probability statements about the 
parameters. Data are assumed to be fixed but are 
assumed to be a random sample from a population 
for the purpose of making inferences from the data 
to the population. 
Assumes that sample of data is random but that 
parameters generating the data are unknown and 
fixed. Therefore, direct probability statements about 
parameters cannot be made from any one sample 
but can be made based on a hypothetical sequence 
of repeated random samples and experiments. 
Enters the experiment with some expectations or 
opinions about parameter distributions (prior 
beliefs), observes data, and combines the prior 
beliefs about parameters with the data to obtain 
updated or posterior distributions for parameters. 
All information about parameters is derived from 
the data. That is, sample data is only used to derive 
parameter estimates. 
Posterior distribution for parameters given the data 
provides parameter estimates and quantifies the 
uncertainty in parameters.  
In likelihood-based inference, the likelihood 
function for the model and the data are evaluated. 
First and second derivatives of the likelihood 
function are obtained for estimating parameters and 
quantifying uncertainty (standard errors for 
parameters). 
Hypothesis testing and probability interpretations 
for parameters are based on posterior distribution 
for parameters. Direct probability interpretations 
for parameters are possible; null hypothesis is 
invalidated if a value is unlikely in the posterior 
distribution for a parameter.  
Hypothesis testing and probability interpretations 
for parameters are based on relative frequency 
across hypothetical repeated random samples or 
theoretical sampling distributions for parameters; 
null hypothesis is invalidated if observed data or 
parameter estimates are unlikely in this sampling 
distribution. 
For a Bayesian analysis, the posterior distributions for the parameters provide all the information needed 
for statistical inference. Using Bayes theorem, the posterior distribution for a parameter θ, given the data 
(D) or pș_D), may be obtained as follows: 
݌(Ʌ|ܦ) = ݌(ܦ|Ʌ)݌(Ʌ)
݌(ܦ)
 
where p(D _șLVWKHOLNHOLKRRGIXQFWLRQSUREDELOLW\RIGDWDJLYHQSRVVLEOHYDOXHVRIș pșLVWKHSULRU
GLVWULEXWLRQIRUWKHSDUDPHWHUșRUSULRUEHOLHIDERXWșEHIRUHREWDLQLQJWKHGDWDand, p(D) is the marginal 
RUXQFRQGLWLRQDOSUREDELOLW\RIGDWDDFURVVDOOSRVVLEOHYDOXHVRUWKHSDUDPHWHUVSDFHIRUș. The numerator, 
p(D _ș pș, reflects an updated belief about the distULEXWLRQRIșEDVHGRQWKHGDWDThe computation of 
p(D) requires either summing over the parameter space in the case of discrete distributions, or integrating 
over the parameter space in the case of continuous random variables. Note, p(D) is a constant used to 
normalize or rescale p(D _șp(șVRWKDW the posterior distribution, p(ș_D), is a probability distribution.  
In the equation, p(ș_D) is a weighted average of knowledge about a parameter before observations are 
obtained (prior) and the most likely value given the observations (likelihood), and p(ș_D) describes the 
GHJUHHRIFHUWDLQW\RUXQFHUWDLQW\LQșDIWHUXSGDWLQJthe prior with the observed data. Note further that p(ș_
D) is proportional to p(D _șp(șbecause p(D) is a constant and does not change the shape of p(D _șp(ș 
In a Bayesian paradigm, all statistical inference is performed from posterior distributions for model 
parameters. For example, means of parameter posterior distributions may be used to provide point 
estimates for parameters, and standard deviations of parameter posterior distributions may be used to 
provide information about uncertainty or error in estimating parameters. In addition to the direct probability  
 
 
 
 

28   Bayesian Analysis of Item Response Theory Models Using SAS 
 
 
interpretations afforded by a Bayesian paradigm, other advantages when estimating statistical models (in 
particular, IRT models) include the following: 
Ɣ 
parameter estimates are typically more precise than Maximum Likelihood (ML) estimates. 
Ɣ 
methods accommodate perfect and imperfect response patterns. 
Ɣ 
ability or trait parameters are estimated simultaneously with item parameters. Therefore, 
uncertainty in item parameter estimates is propagated to ability or trait parameter estimates.  
Ɣ 
use of prior distributions may provide more stable estimation. That is, model parameter estimates 
do not “drift out of bounds” as may be the case, for example, with small samples or short tests. 
Ɣ 
estimation of more complex or highly parameterized models is more accessible (for example, 
parameters that reflect additional facets of an assessment). 
Use of Bayesian Analysis to Estimate a Proportion—An Example 
Consider a bag of golf balls in which N balls are drawn at random, and r white balls are observed. What is 
an estimate of the proportion of white balls (π) in the bag? In a Bayesian analysis, the posterior distribution 
for the parameter given the observed data, p(π | D) = p(D | π)  p(π) / p(D), is computed. The likelihood 
function, p(D | π), reflects the probability of observing r successes (white as opposed to nonwhite golf 
balls) in N independent trials (draws from the bag). Given success rate π, p(D | π) may be modeled using 
the binomial distribution: 
݌(ܦ|ߨ) = ܮ(ߨ|ݎ, ܰ) = ൬ܰ
ݎ൰ ߨ௥(1 െߨ)ேି௥ 
The prior distribution, p(π), or the prior distribution or belief about the distribution of π before collecting 
the data, also needs to be specified in the Bayesian paradigm. It is often useful to choose a “conjugate” 
prior or a prior that has the same functional form as the likelihood. The advantage of a conjugate prior is 
that the product of a conjugate prior and the likelihood yields a posterior with the same form as the prior. 
Furthermore, use of a conjugate prior precludes the need to compute p(D), which can be complicated.  
The Beta distribution is a conjugate prior distribution for the binominal distribution; therefore, a Beta prior 
with a binomial likelihood produces a Beta posterior. Continuing with the example, you see that a Beta 
prior with parameters a and b or Beta (a,b) has the following form: 
݌(ߨ; ܽ, ܾ) = ݇× ߨ௔ିଵ(1 െߨ)௕ିଵ 
for  
0 < π < 1 
where k is a normalizing constant with location and scale parameters, a and b. Combining the Beta prior 
with the binomial likelihood p(D | π), you obtain the posterior distribution for the parameter π as follows: 
݌(ߨ|ܦ) = [൬ܰ
ݎ൰ ߨ௥(1 െߨ)ேି௥][݇× ߨ௔ିଵ(1 െߨ)௕ିଵ] 
Because π does not depend on the constants in the equation, they can be ignored and p(π | D) can be re-
expressed as proportional to a function as follows: 
݌(ߨ|ܦ) ן ߨ௥(1 െߨ)ேି௥ߨ௔ିଵ(1 െߨ)௕ିଵ= ߨ௥ା௔ିଵ(1 െߨ)ேି௥ା௕ିଵ 
for  
0 < π < 1 
 

Chapter 2: Bayesian Analysis   29
 
where the posterior, p(π | D), has the same form as the prior and is a Beta(a + r, N − r + b) distribution. In 
p(π | D), the number of successes or white balls that are observed are added to the location parameter of the 
Beta prior (a) and the number of failures or nonwhite balls is added to the scale parameter of the Beta prior 
(b). Thus, the Beta(a,b) prior can be thought of as representing a previous experiment, or belief about a 
previous experiment, with a successes in a + b trials. 
Choosing a Prior Distribution 
The Beta distribution is a natural conjugate prior for estimating a proportion because, as for proportions, 
the densities for all Beta distributions are bounded between 0 and 1. The two parameters of the Beta 
distribution, a and b, determine the shape of the distribution. Therefore, there are many Beta(a,b) 
distributions to choose from to reflect different prior beliefs about the proportion parameter being 
estimated.  
Figure 2.1 illustrates three different Beta distributions. If there is a strong belief that the proportion is close 
to 1, then a Beta(1, .5) prior could be chosen (left graph). If there is no prior belief favoring a region of the 
parameter space, a uniform prior Beta(1,1) could be used (middle graph); or if there is a prior belief 
favoring the mid region of the parameter space, one of the many Beta distributions can be used that 
resembles a normal distribution (for example, Beta(4,4) – right graph). 
Figure 2.1: Various Beta Distributions for p(π) 
In these Beta distributions, constants were used to define the parameters of the prior distributions. These 
parameters, called hyperparameters, can themselves be parameters with associated priors (hyperpriors or 
hierarchical priors). Adding hyperpriors creates a hierarchical structure to the prior specifications and may 
be used to reduce the strength of priors or specify non-informative priors and let the data influence the 
hyperparameters. For example, a parameter, θ, may be assumed to be distributed as Normal(μθ, σθ), and the 
parameters μθ and σθ also have assumed distributions. Hierarchical prior structures will be discussed 
further in Chapter 3 and in the context of specific IRT models to be estimated (Chapters 4–6). 
 

30   Bayesian Analysis of Item Response Theory Models Using SAS 
 
 
Computing the Posterior and Drawing Inferences 
Continuing with the example, consider 10 balls are drawn at random from a bag of golf balls, and 4 white 
balls are observed. What is an estimate of the proportion of white balls (π) in the bag? 
Assuming a Beta(1,1) or uniform prior and the data r = 4 and N = 10, the posterior distribution p(D | π)  
follows a Beta(a + r, N í r + b) or Beta(5,7) distribution. Because inferences for a Bayesian analysis use 
only the posterior distribution, you can compute the mean to provide a point estimate for π (ߨො) and the 
standard deviation for p(π | D) as an estimate of the standard error for π (ܵܧగෝ). For any Beta distribution, 
the mean of Beta(a,b) = a / (a + b) and the variance of Beta(a,b) = (a × b) / [(a + b) 2 × (a + b + 1)]. Thus, 
for the above example and a Beta(5,7) posterior distribution: 
ߨො= 
ܽ
(ܽ+ ܾ) = 5
12 = .417 
ܵܧగෝ= ඥ(ܽ× ܾ)/[(ܽ+ ܾ)ଶ× (ܽ+ ܾ+ 1)] = ξ. 019 = .138 
In a Bayesian paradigm, you can also construct a probability interval (Bayesian or Credible Interval) using 
the posterior distribution. For example, if the posterior distribution approximates a normal distribution, you 
can construct a (1 - Į) credible interval by using Ɏෝ± ݖఈ/ଶܵܧ஠ෝ, where Ɏෝ is the point estimate of π and ܵܧగෝ is 
the standard deviation of the posterior distribution p(π | D). When the posterior distribution is not normal, 
you can construct a credible interval based on the percentiles for an empirically derived posterior 
distribution using Markov Chain Monte Carlo (MCMC) methods. For example, the 95% credible interval 
for π reflects the interval [P2.5, P97.5] in p(π | D).  
Probability statements about the parameter π can also be made from p(π | D). For example, the probability 
is .95 that π is between the P2.5 and P97.5 percentiles of p(π | D). While credible intervals appear similar to 
confidence intervals, the interpretations and inferences are not the same. Recall that in a Frequentist 
framework, direct probability statements or interpretations from confidence intervals cannot be made since 
the parameter π is fixed. 
You can also use the posterior distribution for hypothesis testing (Ho) by comparing a null or Ho value with 
the posterior distribution. For example, you can compute the probability that the proportion of white golf 
balls in the bag, π, is less than a Ho value of .3 or p(π  <  πo  _D = 4) given p(π | D) ~ Beta(5,7). If a 
normal density is assumed for p(π | D), p(π  <  .3) = (p(z.3 − .417)  / .138) = ~ .20. That is, the 
probability is approximately .20 that π  <  .3. Note that p(π  <  πo  _D) can also be computed directly 
from an empirically derived posterior distribution, p(π | D), by computing how many times πo = .3 is less 
than or equal to each sampled value in p(π | D). 
Finally, p(π | D) can be used for predicting values for a future sample from the same population. For 
example, what is the probability of drawing a white ball in a new draw from the bag (D*)? A predictive 
posterior distribution for a new value (1 = white ball) from a new draw (denoted p(D* | D)) can be 
computed as follows: 
݌(ܦכ = 1|ܦ) = න
݌(ܦכ = 1|Ɏ)݌(Ɏ|ܦ)݀(Ɏ)
ଵ
గୀ଴
 
                                                         = ׬
 Ɏ ݌(Ɏ|ܦ)݀(Ɏ)
ଵ
஠ୀ଴
= ܧ[݌(Ɏ|ܦ)] 
For this example, the prediction for a new draw would be the mean for p(π | D), or .417. Using the mean 
from p(π | D) makes sense as a prediction for a single subsequent draw. 
Comparing Bayesian and Frequentist Estimates for Proportions 
In comparison to a Bayesian analysis, the frequentist estimate or maximum likelihood estimate (MLE) for 
Ɏ is Ɏෝ = ݎܰ= .4
Τ
, and the variance estimate for  Ɏ = [ Ɏෝ (1 െɎෝ)] ܰ= .024
Τ
. Comparing the two 
 

Chapter 2: Bayesian Analysis   31 
 
paradigms for estimating the proportion of white balls in the bag, you find that the mean for the Bayesian 
estimate is pulled slightly to the mean of the prior or .5, given a Beta(1,1) prior, and the variance estimate 
for π under a Bayesian paradigm (.1382 = .019) is slightly less than the variance estimate for the MLE. The 
slight decrease in the variance estimate is due to the small amount of prior information brought to the 
analysis even in the case of a uniform prior. When considering the Beta prior as representing a previous 
experiment, or belief about a previous experiment, a = 1 success in a + b = 2 trials reflects information in 
the prior, albeit a small amount of data or information. 
Under a Bayesian paradigm, the posterior can be affected by the choice of the prior distribution for a 
parameter. For example, assuming a Beta(4,4) distribution for the prior, where values around π = .5 are 
more likely than values in the tails, the mean from p(π | D) is .44 and the variance from p(π | D) is .013. 
When considering the Beta prior as representing a previous experiment, or belief about a previous 
experiment, a = 4 successes in a + b = 8 trials reflects more influential data considering the current data is 
based on only 10 trials. Thus, when a more informative prior is used, the mean of p(π | D) is pulled more 
toward the mean of the prior (.5) and the variance of p(π | D) is smaller (.013 as opposed to .019).  
Although a criticism of the Bayesian paradigm is that prior beliefs about the parameters influence results, 
uninformative prior distributions that are relatively flat across a wide range of possible or reasonable values 
for parameters (local region of the parameter space) may be specified so that the sample data dominate the 
estimation process. Priors are also less relevant when data are numerous and the likelihood dominates the 
posterior or when data are precise (for example, temperature measurements). 
Bayesian Estimation and the MCMC Method  
In the previous example, the posterior distribution for the parameter, p(π | D), was derived analytically. 
However, for most experiments and statistical models, Bayesian analysis and computing posterior 
distributions involves complex integrals. Furthermore, as the number of model parameters increases, the set 
of integrals becomes more numerically intractable. For example, the joint posterior for parameters of the 2-
P IRT model is as follows: 
݌(Ʌ, ܽ, ܾ |ܦ) =
݌(ܦ |Ʌ, ܽ, ܾ) ݌(Ʌ, ܽ, ܾ)
׬ ׬ ׬ ݌(ܦ |Ʌ, ܽ, ܾ) ݌(Ʌ, ܽ, ܾ) ߲Ʌ ߲ܽ ߲ܾ 
For these and other high dimensional models, the problem is how to compute the denominator or marginal 
probability distribution p(D) in order to form a proper probability distribution. To solve this problem, you 
can use MCMC methods to generate a sample of parameter values from the joint posterior distributions for 
parameters. In doing so, you use simulations to replace the complex mathematical calculations. The 
availability of increased computing power has made Bayesian analysis via MCMC much more accessible.  
One simple but very general MCMC method is the Metropolis-Hastings algorithm (see for example, 
Gelman et al., 2014). This procedure implements a random walk through a parameter space to generate a 
sample of parameter values from the posterior distribution and is used for many models in PROC MCMC. 
The Monte Carlo component to the method refers to the simulation methods used to draw random samples; 
the Markov Chain component refers to the fact that the chain of sampled values depends on previous 
draws. For example, to draw samples from the target posterior distribution for p(π | D) in the above 
example on estimating a proportion, a simple implementation of the algorithm is as follows:         
1. Start with an arbitrary initial or starting value in the target distribution for p(π | D) or πcurrent. 
Propose a new position (πproposed) in the parameter space, using a proposal distribution. The 
proposal distribution is chosen to ensure that samples are drawn from all regions of the parameter 
space. Normal distributions centered at πcurrent are typically used.  
2. Compute P(πproposed) and P(πcurrent), where each evaluation of the posterior is proportional to p(D | 
π) p(π) and reflects the relative likelihood of the sampled values. 
 
 

32   Bayesian Analysis of Item Response Theory Models Using SAS 
 
 
3. Compute pmove = min[1, P(πproposed) / P(πcurrent)], which focuses on the ratio of two likelihood 
expressions. Normalizing the values to probabilities is not required. Note that if P(πproposed) > 
P(πcurrent), pmove = 1; otherwise, the ratio will be a value < 1. 
4. To determine whether the proposed move should be accepted, sample a RV from U[0,1]. If 0 < 
RV < pmove, accept move; otherwise, reject move, add current value to the sample, and sample 
another value. If accepted, the new sampled value is likely within the targeted range of the 
proposal distribution. Note that the choice of the proposal distribution is important. For example, 
if the proposal distribution is wide relative to the parameter space, some proposed moves will be 
far from πcurrent and pmove small and rarely accepted. Thus, the algorithm often needs to be “tuned” 
for optimal acceptance rates to ensure that the chain reaches all regions of the posterior 
distribution and traverses the posterior distribution quickly. For further discussion of tuning in 
PROC MCMC, see the PROC MCMC documentation. 
5. Repeat the process for many samples to approximate the joint posterior distribution for the model 
parameters or obtain an empirically derived p(D | π). 
It is often useful to implement the MCMC algorithm in a log scale. In a log scale, all probability values are 
derived from computing logs of probability density functions, and in Step 3, P(πproposed) / P(πcurrent) is 
replaced with the exponential function ݁௟௢௚௉(గ೛ೝ೚೛೚ೞ೐೏ )ି௟௢௚௉(గ೎ೠೝೝ೐೙೟ ). 
SAS Code for Implementing a Metropolis Sampler to Estimate a Proportion 
Continuing with the Bayesian estimation of a proportion example, Program 2.1 implements the simple 
Metropolis algorithm discussed above using a log scale. In this example, p(D | π) or likelihood = π r(1−π)N-r  
and the prior for π is a uniform density over the interval [0,1] or a Beta(1,1) prior. The proposal distribution 
is a normal distribution with the mean 0 and standard deviation .1, or N(0, .01). Thus, any proposed move is 
equal to πcurrent plus a random value from N(0, .01) distribution.  
 
 
 

Chapter 2: Bayesian Analysis   33 
 
Program 2.1: Simple Metropolis Algorithm for Estimating a Proportion 
%let burnin=1000;      /* samples to discard */ 
%let start=.5;         /* initial value for chain */ 
%let chain=4000;       /* length of chain after burn-in iterations */ 
 
/* read in observed data */ 
data obs_data;  
  array x{*} x1-x10; input x(*); 
  n=10;             /* data for likelihood - number of draws */ 
  r=sum(of x1-x10); /* data for likelihood - # white balls from draws */ 
datalines; 
1 1 1 1 0 0 0 0 0 0 
; 
data mcmc_data; 
  set obs_data; 
  call streaminit(1234);         
  retain sample (&start) accept (0); 
  i=0; output; 
  do i = 1 to %eval(&burnin + &chain); 
    current=sample; 
    proposed=rand('normal', current , .1);/* normal proposal distribution 
                                          centered around current value*/ 
    /* check for out of bounds values */ 
    if proposed < 0 then proposed = 0; 
    if proposed > 1 then proposed = 1; 
 
    /* compute loglikelihood for current & proposed values – binomial */ 
    current_like=logpdf('binomial',r,current,n); 
    proposed_like=logpdf('binomial',r,proposed,n); 
     
    /* compute log probabilities for current & proposed - beta prior */ 
    current_prior=logpdf('beta',current,1,1); 
    proposed_prior=logpdf('beta',proposed,1,1); 
 
    /* determine whether to accept or reject the proposed sample */ 
    p_accept = min(1,exp(proposed_like+proposed_prior –  
                        (current_like+current_prior))); 
    if rand('uniform') < p_accept then do; 
        sample = proposed; 
 
 accept = accept+1; 
    end; 
    output; 
  end; 
run; 
Figure 2.2 presents the initial samples from the SAS program that implements the Metropolis algorithm, 
starting from π = .5. The x-axis provides the scale for the possible values of the proportion (0 < π < 1). In 
Figure 2.2, you can see that the draws traverse the parameter space but are mostly centered on 0.4 after a 
relatively small number of sampled values (100) because of the influence of the data on the likelihood. 
Also, you can see that most proposed moves or proposed samples are small because the standard deviation 
equals .1 in the proposal distribution.  
Figure 2.3 presents a histogram that reflects the distribution of sample values for the chain after a burn-in 
period or after discarding 1,000 samples. From the figure, you can see that much of the entire region of the 
parameter space is sampled. Also, p(D | π) has a mean equal to 0.41 and a standard deviation equal to 
0.135, both of which closely approximate the derived values for this example. 
 
 

34 Bayesian Analysis of Item Response Theory Models Using SAS
 
 
Figure 2.2: Plot of First 100 Sampled Values for π
Figure 2.3: Histogram for the Chain of Sampled π Values 
 

Chapter 2: Bayesian Analysis   35 
 
MCMC and the Gibbs Sampler 
In a multidimensional parameter space, a special case of the Metropolis-Hastings algorithm, the Gibbs 
sampler, may be used to sample from the joint posterior distribution for the model parameters (see, for 
example, Gelman et al., 2014). This method draws values from independent conditional distributions that 
are in the limit equal to drawing from the joint posterior distribution. In this algorithm, each parameter or 
random variable from a vector of parameters is sampled, conditional on the most recent sampled values for 
the other parameters in the vector. Thus, the joint posterior is reduced to a set of lower or one-dimensional 
conditional distributions that can be sampled as shown earlier.  
For example, with a 2P IRT model, univariate conditional distributions would be specified (for example, bj 
for item j given the data and all other item and person parameters) and values drawn from each parameter’s 
conditional distribution, given that all other parameters are known. Based on arbitrary starting values at 
iteration t = 0, the draws at iteration (t +1) would be as follows: 
Ɣ 
for examinee i, draw și(t+1) from p(și _ ș<i(t+1),ș>i(t), aj(t),  bj(t), D) 
Ɣ 
for item j, draw aj(t+1) from p(aj _a<j(t+1),a>j(t), ș(t+1), bj(t), D)  
 
for item j, draw bj(t+1) from p(bj _a(t+1), ș(t+1), b<j(t+1), b>j(t), D) 
When the Markov Chain converges, (șt+1, at+1,bt+1) will be a sample from the stationary joint posterior or 
target posterior distribution, p(ș, a, b _D).  
Note that PROC MCMC determines the sampling algorithm based on the parameters and their prior 
distribution specifications. When appropriate, PROC MCMC uses conjugate sampling methods to draw 
conditional posterior samples; otherwise, a random walk Metropolis algorithm is used by default in PROC 
MCMC. Other sampling algorithms are available such as a slice sampler. For more detail, see Chapter 3 in 
this book. In the SAS Help, search for PROC MCMC. Or, in the SAS/STAT 13.2 User’s Guide (SAS, 
2014), see Chapter 61, “The MCMC Procedure,” or Chapter 7, “Introduction to Bayesian Analysis 
Procedures.”  
Burn-In and Convergence 
Because the MCMC method samples parameter values iteratively, you should determine whether the 
Markov chain has converged to the target posterior distribution. If the chain has not converged, then the 
simulated draws from this chain do not represent the parameter space or target posterior distribution of 
interest. Therefore, you should assess convergence of Markov chains before drawing any inferences from 
the parameter posterior distributions. In addition, discarding sampled values from the initial iterations of 
the Markov chain (burn-in phase) is customary, in order to minimize any influence of early sampled values 
on the target posterior. 
Numerous diagnostic tools are available to determine the extent to which the chain has converged in PROC 
MCMC. One popular tool involves trace plots of sampled parameter values across iterations in the chain. A 
chain that “mixes” well traverses the posterior space rapidly, reaches all regions of the relevant parameter 
space, and appears as a random plot of values around a mean value for the posterior distribution. Another 
commonly used tool is an autocorrelation plot that reflects the amount of serial dependence among sampled 
values for different lags; and, in turn, it reflects the efficiency of the MCMC algorithm. All Markov chains 
exhibit some degree of serial dependence. However, if the MCMC algorithm is mixing well, then the serial 
dependence should decrease to 0 rapidly. Thus, the trace and autocorrelation plots typically indicate the 
relative degree of poor mixing. Note that poor mixing and high autocorrelations do not necessarily lead to 
biased inference, but instead may simply indicate inefficiency in the sampling process. 
Output 2.1 illustrates desired trace and autocorrelation plots from PROC MCMC for a parameter in which 
the chain is “mixing” well. This well-behaved performance is in contrast to Output 2.2, which illustrates 
trace and autocorrelation plots for a parameter reflecting poor mixing behavior. Finally, Output 2.3 displays 
the sampled values for a burn-in period of 1,000 iterations. As you can see, a long burn-in period is 
indicated because very poor mixing and lack of convergence are present. Other example plots that reflect 
 
 

36 Bayesian Analysis of Item Response Theory Models Using SAS
 
 
various degrees of mixing behavior may be found in the SAS Help; search for “Bayesian Analysis.”  Or, in 
the SAS/STAT 13.2 User’s Guide (SAS, 2014), see Chapter 7, “Introduction to Bayesian Analysis 
Procedures.” 
Output 2.1: Trace and Autocorrelation Plots for a Chain That Is Mixing Well 

Chapter 2: Bayesian Analysis   37
 
Output 2.2: Trace and Autocorrelation Plots for a Chain with Poor Mixing 
 
Output 2.3: Trace Plot Illustrating Mixing in Early Iterations of the Markov Chain 
When poor mixing for parameters is observed, there are several possible remedies. These remedies include 
thinning or deleting samples by only saving every nth sampled value; re-parameterizing the model; and 
altering the MCMC algorithm by, for example, changing the sampling algorithm or altering the sets of 
 

38   Bayesian Analysis of Item Response Theory Models Using SAS 
 
 
parameters that are updated sequentially. For discussion and illustration of these strategies in the context of 
estimating IRT models with use of PROC MCMC, see Chapter 3. 
In addition to trace and autocorrelation plots, you can compute other commonly used diagnostic statistics 
with PROC MCMC:   
Ɣ 
Gelman-Rubin. This statistic uses parallel chains with different and dispersed initial values to 
evaluate whether different chains converge to the same posterior distribution. 
Ɣ 
Geweke. This test compares parameter means from early and later iterations of the Markov chain 
to evaluate convergence. 
Ɣ 
Heidelberger−−Welch (stationary test). This diagnostic procedure tests whether the Markov chain 
is a “weakly” stationary process using a Cramer-von Mises statistic based on the relative accuracy 
of parameter means for different lengths of the chain.  
Ɣ 
Heidelberger−Welch (half−width test). Using the portion of the chain that passed the stationary 
test, the half-width test evaluates whether the number of iterations is adequate to meet a specified 
accuracy for the mean estimate. 
Ɣ 
Raftery−Lewis. This procedure reports the number of samples needed to reach a specified 
accuracy in the percentiles for parameter estimates. 
 
Effective sample size (ESS). This statistic measures the variation in sampled values across 
iterations of the chain. 
For details about the computations of these tests and procedures, see the PROC MCMC documentation in 
in the SAS Help. Or, in the SAS/STAT 13.2 User’s Guide (SAS, 2014), see Chapter 7, “Introduction to 
Bayesian Analysis Procedures.” These diagnostics are also described further in the context of an example 
in Chapter 3. Note that all these diagnostic tools cannot be used as proof of convergence, but instead are to 
be used only to detect nonconvergence or inefficiency in the chain. Furthermore, the different convergence 
diagnostics are designed to check different aspects of convergence. Therefore, a good practice is to evaluate 
convergence by using multiple diagnostic tools rather than relying on one tool. 
Informative and Uninformative Prior Distributions 
As discussed, the prior and the specified distribution for the prior can play an important role in Bayesian 
analysis, although priors are also less relevant when data are numerous or when data are precise. When 
thinking about different types of prior distributions, you can broadly classify them into two classes: 
uninformative and informative.  
Uninformative priors, also referred to as flat, vague, or diffuse priors, are designed to provide minimal 
information about the parameter being estimated and to allow the data to dominate the analysis or 
determination of the posterior distribution. Uninformative priors assign approximately equal probability or 
likelihood to all possible values of a parameter in the parameter space. In the example on estimating a 
proportion, the Beta(1, 1) distribution reflected an uninformative prior because it is flat relative to the 
parameter space (see Figure 2.1).  
Informative priors, on the other hand, value external information about the parameter before collecting 
data, and base the analysis or determination of the posterior on this external information, as well as on the 
data that is collected. The external information used to specify an informative prior could come from 
previous analyses on similar data sets or be based on mathematical properties of the parameters in a 
statistical model. In the example on estimating a proportion, the Beta(3, .5) and Beta(4, 4) priors represent 
different types of informative priors or expectations about the plausible values of the parameter before data 
is collected. Because informative priors may affect the form of the posterior distribution, you should 
conduct a sensitivity analysis to evaluate the influence of different informative priors on posterior 
distributions for parameters. 
Although uninformative priors may be preferred because they may be deemed less “subjective,” 
uninformative priors can lead to improper posteriors or probability distributions that are not defined. For 
 

Chapter 2: Bayesian Analysis   39 
 
example, a uniform prior on the “real” line from –WRZLOOJHQHUDOO\UHSUHVHQWDQLPSURSHUSULRU. 
However, if an uninformative prior is a conjugate prior, as in the case of the Beta(1,1) prior with a binomial 
likelihood, the posterior is proper.  
In addition, a Jeffrey’s prior, which represents a locally uniform or uninformative prior over a subset of the 
parameter space, can be specified for a variety of statistical models, although the use of a Jeffrey’s prior 
does not guarantee a proper posterior. However, you can achieve a relatively uninformative prior by 
choosing a conjugate prior that is locally uniform over a region of the parameter space that reflects typical 
values for a parameter. For example, a Normal prior could be chosen that has a large variance for a 
parameter that typically ranges from −3 to +3, as is the case for item difficulty parameters in IRT models. 
In this case, a Normal(0,1000) prior, truncated over the range from −3 to +3, could be specified to achieve 
an essentially locally uniform prior. Rather than specify a truncated prior, an alternative may simply be 
“weakly” informative priors using conjugate prior distributions with large variance terms (Sheng, 2013). 
For further discussion prior distributions in the context of IRT models, see Chapter 3. For more background 
on informative and noninformative priors see, for example, Gelman et al. (2014), Gelman (2006), Goldstein 
(2006), Fox (2010), and Kass & Wasserman (1996).  
Model Comparisons 
In many applications, different models may be estimated that reflect competing formulizations of the 
relationships among variables under study, or reflect competing theoretical perspectives. Furthermore, a 
researcher using IRT to model item responses is interested in a parsimonious model—that is, a model with 
fewer estimated parameters that adequately reflects the data. Consequently, you must often choose among 
competing models to arrive at a preferred model, and model comparison procedures may be used for this 
purpose.  
Model comparison methods also enable you to determine whether a particular model reflects “overfitting” 
or a particular model reflects “underfitting.” The model comparison methods in a Bayesian paradigm fall 
within two classes: (1) Bayesian information-based criteria—the Deviance Information Criteria 
(Spiegelhalter, Best, Carlin, & van der Linde, 2002), and (2) indices based on Bayes factors. These classes 
of indices are discussed in the subsections to follow. Note that these indices can only be used to assess the 
relative fit between competing models and do not address the absolute fit of any one particular model or the 
degree to which any one particular model best approximates the data. Nonetheless, these methods have 
been found useful for comparing competing IRT models in testing applications (see for example, Zhu & 
Stone, 2012).  
Absolute model fit in a Bayesian paradigm is generally assessed by means of posterior predictive model 
checks (PPMC). For this approach, a model fits the data if it produces good predictions. Of course this 
method could also be used to compare different models by taking a predictive approach to model 
comparisons and selecting the model that provides the better predictions. PPMC analysis is described 
briefly below and considered in more detail in Chapter 8.  
Deviance Information Criterion 
The Deviance Information Criterion (DIC; Spiegelhalter et al., 2002) is a model comparison tool similar to 
the well-known likelihood-based information criterion: Akaike Information Criterion (AIC; Akaike, 1974) 
and the Bayesian Information Criterion (BIC; Schwarz, 1978). DIC can be applied to nested or non-nested 
models, as well as models that have non-independent identically distributed variables.  
 
 
 
 

40   Bayesian Analysis of Item Response Theory Models Using SAS 
 
 
Similar to other information criteria indices, the DIC index weights both model fit and model complexity in 
identifying a preferred model. DIC was developed because MCMC estimation uses prior information, and 
the actual number of parameters cannot be clearly identified as AIC and BIC indices. The DIC index is 
widely used with Bayesian estimation and is defined as follows: 
ܦܫܥ= ܦഥ(Ʌ) + ݌஽ 
and 
݌஽= ܦഥ(Ʌ) െ ܦ(Ʌത) 
where ܦഥ(Ʌ) is the mean of the deviance across iterations and measures how well the data fits the model 
using the likelihood function (í2log L(D | ș )) and the sampled parameter values at each iteration. And ݌஽ 
is used to correct for the bias toward higher-dimensional models. More specifically, ݌஽ reflects the 
“effective” number of model parameters and is computed by the difference between ܦഥ(Ʌ) and the deviance, 
based on the log likelihood function at the posterior mean of model parameter estimates, ܦ(Ʌത). As for other 
information indices, smaller values of DIC indicate better fit. Note that DIC should be used cautiously with 
missing data because the likelihood is not uniquely defined (Lunn et al., 2013). In addition, DIC assumes 
that the posterior mean is an accurate and appropriate estimate from the posterior distribution. If this is not 
the case, as when the posterior density reflects extreme skewness or bimodality, the DIC should probably 
not be used.  
Lunn et al. (2013) discuss “rough” guidelines for interpreting differences in DIC when you are selecting a 
preferred model: differences of more than 10 rule out the model with higher DIC; differences between 5 
and 10 reflect “substantial” differences in favor of the model with the smaller DIC; and, it may be 
misleading to select a preferred model for differences that are less than 5 between the models. 
Bayes Factor 
A common approach to selecting a preferred model from two competing models in a Bayesian analysis is to 
compute the Bayes factor (BF). The BF is defined as the posterior odds of Model 1 (M1) to Model 2 (M2) 
divided by the prior odds for M1 to M2, which reduces to the ratio of marginal likelihoods of the data across 
the parameter space for each model:  
ܤܨ= ܲ(ܯଵ|ܦ)/ܲ(ܯଵ)
ܲ(ܯଶ|ܦ)/ܲ(ܯଶ) = ܲ(ܦ|ܯଵ)
ܲ(ܦ|ܯଶ) 
A BF larger than 1 supports M1 over M2, and a value less than 1 supports M2. The relative magnitude of BF 
also can be used in evaluating the relative weight of evidence in favor of either model. The following 
guidelines for interpreting BF values when considering supporting evidence for M1 over M2 have been 
suggested (Raftery, 1996): between 1 and 3 reflects minimal support for M1; between 3 and 12 reflects 
moderate support for M1; between 12 and 150 reflects strong evidence for M1; and larger than 150 reflects 
very strong evidence for M1.  
However, computation of the Bayes factor has both practical and theoretical limitations, when for example, 
the prior is vague or you are estimating highly parameterized models. In order to overcome these problems, 
an alternative criterion called a Pseudo–Bayes factor (PsBF) has been proposed (Geisser & Eddy, 1979; 
Gelfand, Dey, & Chang, 1992). This criterion approximates the marginal densities in the Bayes Factor 
using conditional predictive ordinates (CPO). In the context of item response data, the PsBF index can be 
expressed as: 
ܲݏܤܨ= 
ς
(ܥܱܲ௥| ܯଵ)
ோ
௥ୀଵ
ς
(ܥܱܲ௥| ܯଶ)
ோ
௥ୀଵ
  
 
where R denotes the total number of item responses from all examinees. When you are comparing the 
models at the item level, R equals the number of examinees N. When you are comparing the models at the 
test level, R equals the number of the responses for all examinees to all items (that is, R = N × I where I is 
 

Chapter 2: Bayesian Analysis   41 
 
the total number of items). As for the BF index, a PsBF larger than 1 supports selection of M1, and a value 
less than 1 supports selection of M2. 
Alternatively, the CPO index for any one model can be defined, ܥܱܲெ= ς
(ܥܱܲ௥| ܯ)
ோ
௥ୀଵ
, and a model 
with a larger CPO value will be preferred. Because use of PsBF involves computing CPO for two models 
simultaneously, it is more convenient to compare models directly by using CPOM for each model. The CPO 
index will be used in this book. 
In the context of IRT estimation with MCMC methods, the CPO values are first estimated at the level of an 
individual item response, using the inverse likelihood of each observation for T draws after the burn-in 
period: 
ܥܱܲ௜௝= ൭1
ܶ ෍
1
݌൫ݕ௜௝ หɅ௧)
்
ଵ
൱
ିଵ
 
where T is the number of samples from the chain and p(yij |θt) is the likelihood of the response data for an 
examinee i and item j (yij) based on the sampled item and person parameter values at time t. An item-level 
CPO index for a model can be computed by the product of the values of CPOij across all N examinees:  
ܥܱܲ௝= ෑ
ܥܱܲ௜௝
ே
௜ୀଵ
  
A CPO index for the overall test can then be computed by the product of the item-level CPOj across all J 
items:  
ܥܱܲ= ෑ
ܥܱܲ௝
௃
௝ୀଵ
  
The logarithm of the CPO index is typically computed and the product terms in the above equations are 
replaced with summations over persons and over items. The preferred model is the model with the larger 
CPO value. Computing CPO and performing model comparisons is illustrated in Chapter 7. 
Model Fit and Posterior Predictive Model Checks 
The Posterior Predictive Model Checking (PPMC) method is a commonly used Bayesian model checking 
tool and has proved useful with IRT models (see for example, Levy, Mislevy, & Sinharay, 2009; Sinharay, 
2005, 2006; Sinharay, Johnson, & Stern, 2006; Zhu & Stone, 2011). PPMC involves simulating data under 
the assumed or estimated model and comparing features of simulated data against observed data. The 
comparison is made using discrepancy measures or statistics calculated on both the simulated and observed 
data (for example, residuals or total scores). The rationale underlying PPMC is that, if a model fits the data, 
then observed data should be similar to replicated or simulated data derived from the posterior predictive 
distributions, p(ܦ௥௘௣ | D). In turn, discrepancy measures calculated on both simulated (Drep) and observed 
data (D) should also be similar, and any systematic differences indicate potential misfit of the model 
(Gelman et al., 2014).  
More specifically, PPMC assesses the fit of a model by examining whether the observed data, D, appears 
extreme with respect to the posterior predictive distribution of replicated data from the model (Drep):   
݌(ܦ௥௘௣|ܦ) = න݌(ܦ௥௘௣, Ʌ | ܦ) ݀(Ʌ) = න݌(ܦ௥௘௣ | Ʌ) ݌(Ʌ |ܦ) ݀(Ʌ) 
For each iteration or set of sampled parameter values from the target joint posterior distribution, ݌(Ʌ |ܦ), 
you compute data sets of model-based predicted observations, Drep,1, …, Drep,R. These data sets represent 
draws from the predictive posterior distribution, ݌(ܦ௥௘௣|ܦ). Then, you compute a discrepancy statistic, T, 
 
 

42   Bayesian Analysis of Item Response Theory Models Using SAS 
 
 
on each Drep,r replication to produce a distribution of discrepancy measures under the null condition of 
model-data-fit. When a discrepancy measure depends only on the data, the discrepancy measure may be 
denoted as T(Drep) for replicated data and T(D) for observed data. When the discrepancy measure depends 
on the data and model parameters (θ), the discrepancy measure may be denoted as T(Drep, θ) for the 
replicated data and T(D, θ) for observed data. 
You can locate a discrepancy measure computed from the observed data in a distribution of the same 
discrepancy measure computed from each Drep,1, …, Drep,R (posterior predictive distribution) using 
graphical displays. For example, a histogram for the distribution of T(Drep) can be obtained with the 
location of T(D) also identified in the graph. In addition, you can compute posterior predictive probability 
values (PPP values). A PPP value reflects the relative occurrence of T(D), for example, in the reference 
distribution of the discrepancy measure T(Drep) for each replication from ݌(ܦ௥௘௣|ܦ), or the following: 
ܲܲܲ= ܲ(ܶ(ܦ௥௘௣)) ൒ܶ(ܦ)  
where a PPP value is the proportion of times across R replications that T(Drep) equals or exceeds T(D). 
Thus, PPP values reflect the relative similarity between the discrepancy measure for the predicted and 
observed data. PPP values near.5 indicate that there is no systematic difference between the realized and 
predictive discrepancies, and thus indicate adequate fit of a model. PPP values near 0 or 1 (for example, 
values < .05 or > .95) suggest, conversely, that the realized discrepancies are inconsistent with the posterior 
predictive discrepancy values, and thus indicate inadequate model-data-fit (Gelman et al., 2014).  
It should be noted that the PPP values should not be interpreted in the same way as traditional hypothesis-
testing p-values. Though both are defined as tail-area probabilities, PPP values are not necessarily 
distributed as expected under the null hypothesis and tend to be closer to.5 more often than would be 
expected. As a result, using PPP values in a traditional hypothesis testing framework would lead to a 
conservative test (see for example, Levy, 2006; Sinharay, 2005; Sinharay et al., 2006).  
PPMC has also been found useful in model comparisons when Bayesian estimation is used (see for 
example, Sinharay, 2005). The relative fit of a set of candidate models can be evaluated by comparing the 
numbers of extreme PPP values for the discrepancy measure(s) for the different models. A model with 
fewer extreme PPP values is considered to fit the data better than an alternative model with larger numbers 
of extreme PPP values. 
As you might imagine, selecting appropriate discrepancy measures is an important consideration in 
applications of the PPMC method, and they should be chosen to reflect sources of potential misfit. This 
method and the choice of discrepancy measures in the context of IRT models and different testing 
applications are discussed in more detail in Chapter 8. 
 

Chapter 3: Bayesian Estimation of IRT Models 
Using PROC MCMC 
Introduction ......................................................................................................... 43 
Summary of PROC MCMC Statements for IRT Model Estimation ......................... 44 
MCMC Sampling Algorithms ............................................................................................... 45 
Built-In Distributions ............................................................................................................ 46 
Density Plots ......................................................................................................................... 47 
Arbitrary Distributions .......................................................................................................... 48 
PROC MCMC Template for IRT Model Estimation ................................................ 49 
How PROC MCMC Processes the SAS Data Set.............................................................. 51 
More on the RANDOM Statement ...................................................................................... 51 
Model Identification ............................................................................................................. 52 
Example Use of the Template for the 1-Parameter IRT Model ....................................... 52 
Example Output from Estimating the 1P IRT Model......................................................... 54 
Strategies to Improve Convergence of the Monte Carlo Chain ............................ 59 
Chain Thinning ...................................................................................................................... 59 
Parameter Blocking.............................................................................................................. 61 
Model Re-parameterizing .................................................................................................... 64 
Analysis of Multiple Chains—Gelman-Rubin Test for Convergence ..................... 67 
Informative, Uninformative, and Hierarchical Priors ............................................ 70 
Informative Priors ................................................................................................................. 70 
Uninformative Priors ............................................................................................................ 70 
Hierarchical Priors ................................................................................................................ 71 
Effect of Different Priors ...................................................................................................... 72 
Treatment of Item Parameters as Random Effects .............................................. 73 
Model Fit and Model Comparisons ...................................................................... 76 
Comparison with WinBUGS .................................................................................. 76 
The OUTPOST SAS Data Set ................................................................................ 77 
Autocall Macros for Post-Processing PROC MCMC Data Sets ............................. 78 
Preliminary Item Analyses ................................................................................... 79 
Introduction  
Previously, individuals implementing Bayesian methods had to write their own programs using 
programming languages such as R or Matlab. However, Bayesian analysis using the Markov Chain Monte 
Carlo (MCMC) method has been greatly simplified through programs like SAS PROC MCMC and 
WinBUGS (Spiegelhalter, Thomas, Best, & Lunn, 2003). Similar to WinBUGS, specification of the model 
using PROC MCMC is straightforward. All that is necessary is to specify the model or likelihood function 
for the observed data, model parameters, and prior distributions for model parameters. However, PROC 
MCMC has several significant advantages over WinBUGS: (1) SAS is commonly used by individuals 
across disciplines; (2) SAS provides a robust programming language that extends the capability of the 

44  Bayesian Analysis of Item Response Theory Models Using SAS 
program, in particular, to model checking; and (3) PROC MCMC displays increased performance and 
efficiency through the use of parallel processing.  
This chapter provides an overview of the PROC MCMC commands and options that are most relevant to 
Bayesian estimation of IRT models. These commands and options are then illustrated through the 
estimation of the 1P IRT model. Other commands that may be used will be described when used in other 
examples of IRT models in subsequent chapters. For a complete list of commands and options, see the SAS 
Help (search for PROC MCMC). Or, in the SAS/STAT 13.2 User’s Guide (SAS, 2014), see Chapter 61, 
“The MCMC Procedure.”  
Also, Chen (2009, 2011, and 2013) provides other useful sources for learning more about PROC MCMC. 
Note that the description of PROC MCMC statements and subsequent examples using PROC MCMC in 
this book are based on versions SAS 9.4 and SAS/STAT 13.2. 
Summary of PROC MCMC Statements for IRT Model Estimation 
The MCMC method is designed to estimate a wide range of statistical models under a Bayesian paradigm. 
The MCMC method is a general procedure for simulating or sampling from posterior distributions and 
summarizing information from posterior distributions. (For an introduction to MCMC, see Chapter 2.) In 
the context of a Bayesian analysis of IRT models using PROC MCMC, the following statements and 
statement options are typically specified: 
 
PROC MCMC statement. This statement is used to call the MCMC procedure and specify 
input/output data sets, control options for the sampling algorithm, and the summary/diagnostic 
information to be computed and output. The following options for the PROC MCMC statement 
are available and illustrated in later examples: 
ƕ 
DATA = <SAS dataset>. This option specifies the SAS data set of item response data. 
ƕ 
OUTPOST = <SAS dataset>. This option specifies a data set for saving samples from the 
posterior distribution. 
ƕ 
NBI = <value í default is 1,000>. This option specifies number of burn-in samples. 
ƕ 
NMC = <value – default is 1,000>. This option specifies number of MC samples for the 
posterior distribution post burn-in. 
ƕ 
THIN = <thinning rate – default = 1>. This option is used to “thin” the chain. If n > 1, every 
nth sample is saved into OUTPOST. 
ƕ 
SEED = <value – default is 0>. This option specifies a seed value to start the sampling 
process. A value of 0 means that the seed value is determined by the computer clock, and, 
therefore, results cannot be replicated. 
ƕ 
NTHREADS = <value>. This option is used to specify number of threads available for parallel 
processing (depends on micro-processor – starting with SAS/STAT 13.1). Note that parallel 
processing speeds up the processing significantly. Also, for analyses using a RANDOM 
statement, results may vary slightly when NTHREADS=0 as opposed to when NTHREADS>0 
because different random number streams are used under parallel processing. 
ƕ 
MISSING = <ALLCASE, COMPLETECASE>. This option specifies how missing values are 
accommodated in PROC MCMC. If MISSING=ALLCASE, then any missing responses are 
treated as unknown random variables and sampled directly using the response model and 
MODEL specification as part of process. IF MISSING=COMPLETECASE, then cases with 
missing data are not used. Starting with SAS/STAT 12.1, PROC MCMC performs Bayesian 
augmentation by default for missing data. 
ƕ 
MONITOR=<list of parameters>. This option is used to specify a subset of parameters for 
which statistics or plots (posterior analyses) are computed. If no MONITOR option is 
specified, all parameters contained in PARMS statements are monitored and saved in 
OUTPOST. 
 

Chapter 3: Bayesian Estimation of IRT Models Using PROC MCMC   45 
ƕ 
DIAGNOSTICS=<NONE | ALL | keyword>. This option specifies options for the reporting of 
convergence diagnostics. By default, PROC MCMC computes the Geweke test, sample 
autocorrelations, effective sample sizes, and Monte Carlo errors. All the available diagnostic 
tools are described in Chapter 2 as well as in the auxiliary documentation for PROC MCMC. 
ƕ 
STATISTICS=<NONE | ALL | keyword>. This option specifies statistics to be computed from 
posterior distributions for model parameters listed in the MONITOR option. If no 
STATISTICS option is specified, the output includes the number of posterior samples, mean, 
standard deviation, and the 95% highest posterior density (HPD) intervals. Use of 
SUMMARY as a keyword produces posterior means, standard deviations, and percentile 
values for each variable (default, 25th, 50th, and 75th). 
ƕ 
PLOTS= <NONE | ALL | keyword>. This option specifies the plots to be generated for model 
parameters that are listed in the MONITOR option. Three types of plots can be requested: 
trace plots (TRACE), autocorrelation plots (AUTOCORR), and density plots (DENSITY) for 
each model parameter. Plots are displayed in panels unless the option UNPACK is specified—
see auxiliary documentation for detail. 
ƕ 
DIC. This option is used to indicate that the DIC and associated statistics be reported for 
model comparison purposes (see Chapter 7 for detail on model comparisons). 
ƕ 
For the more advanced user, various options are available for specifying tuning options, 
acceptance rates, and different proposal distributions (see auxiliary documentation). 
Ɣ 
PARMS <parameter name(s) initial value> statement. This statement specifies model 
parameters to be estimated and their initial values. 
Ɣ 
ARRAY <list of parameter names > statement. This statement specifies a set of parameters or 
variables that may be more easily referenced in the code for PROC MCMC. 
Ɣ 
PRIOR < parameter names ~ distribution> statement. This statement is used to specify prior 
distributions for parameters that are specified in PARMS and ARRAY statements. 
Ɣ 
RANDOM <name of random effect parameter(s) ~ distribution SUBJECT=<variable> 
statement. This statement specifies a single or array of random effects and their prior 
distributions. The SUBJECT= option is used to identify an indexing variable for the random 
effects. This variable should be part of the input data set, and the data set does not need to be 
sorted by the variable. However, if random effects are modeled for each observation, 
SUBJECT=_OBS_ may be used. Optional arguments in the RANDOM statement include 
INITIAL=<initial values> to specify initial values and MONITOR=<parameters> to output results 
such as summary statistics for random effects parameters.  
Ɣ 
MODEL <response variable(s) ~ distribution> statement. This statement is used in the 
specification of the log-likelihood function linking the observed data and the model parameters. 
Every PROC MCMC program must have at least one MODEL statement. 
Ɣ 
PREDDIST <OUTPRED= SAS dataset NSIM=value> statement. This statement is used to 
create a data set that contains random samples of the response variable(s) from the posterior 
predictive distribution. 
 
SAS programming commands used in DATA steps. These statements, for example, DO and IF, 
may also be used in the PROC MCMC command as needed. 
MCMC Sampling Algorithms 
With regard to the MCMC algorithms used to sample parameter values, PROC MCMC determines the 
sampling algorithm based on the parameters and their prior distribution specifications. When appropriate, 
PROC MCMC uses conjugate sampling methods to draw conditional posterior samples and, typically for 
missing values, direct sampling methods. Otherwise, a normal-kernel, random walk Metropolis algorithm is 
used for continuous variables or a discretized normal random walk Metropolis algorithm by default in 
PROC MCMC. Other sampling algorithms are available, such as a slice sampler or a t-distribution random 
walk Metropolis algorithm. To specify an alternative sampler for continuous parameters specified in 
PARMS statements, the /OPTION = <algorithm> is used. To specify an alternative algorithm for random 
effects parameters, the /ALGORITHM = <algorithm> is used with the RANDOM statement. In addition, 

46  Bayesian Analysis of Item Response Theory Models Using SAS 
you can specify a user-defined Gibbs sampling algorithm using the USD statement. See the SAS auxiliary 
documentation for PROC MCMC for detail.  
Built-In Distributions 
A number of standard and familiar distributions are available for use with PROC MCMC and the PRIOR 
and RANDOM statements. Tables 3.1 and 3.2 provide a summary of the distributions most relevant for 
estimating IRT models. In the tables, any labels in bracketed arguments are optional, whereas other labeled 
arguments are required. For example, the specification NORMAL(MEAN=0, SD=1) is equivalent to 
NORMAL(0, SD=1).  
Also, for distributions requiring arguments reflecting dispersion, either the standard deviation (SD=), 
variance (VAR=), or precision (PREC=), which is the inverse of the variance, can be specified. Also, all of 
the univariate distributions, with the exception of binary and uniform, can have optional LOWER= and 
UPPER= arguments. These options can be used to specify bounds on the distributions. For a complete list 
and description of all available distributions, consult the SAS auxiliary documentation for PROC MCMC. 
Note that the gamma and inverse gamma distributions are widely used in Bayesian analysis. Given a 
normal distribution, the gamma distribution is typically used as a conjugate prior for the precision 
parameter. The inverse-gamma distribution is typically used as a conjugate prior for the variance 
parameter. As an example of the relationship between the gamma and inverse gamma distributions, the 
specification for precision, τ ~ gamma(shape=0.01, iscale=0.01), is equivalent to a specification for the 
variance σ2 ~ igamma(shape=0.01, scale=0.01). Use of the gamma and inverse gamma distributions for 
variance parameters is commonplace. However, Gelman (2006) recommends the use of uniform or half t 
priors. This is discussed more in Chapter 6. 
 
 
 

Chapter 3: Bayesian Estimation of IRT Models Using PROC MCMC   47 
Table 3.1: Commonly Used Univariate Distributions with IRT Models 
Distribution 
Description 
BETA(<A=> a, <B=> b)  
Beta distribution with shape parameters a and b 
BINARY(<PROB=> p)  
Binary (Bernoulli) distribution with probability of 
success p  
BINOMIAL (<N=> n, <PROB=> p)  
Binomial distribution with count n and probability o
success p  
CHISQUARE(<V=> df) 
Chi-squared distribution with df degrees of 
freedom 
GAMMA(<SHAPE=> a, SCALE=λ | ISCALE=λ)  Gamma distribution with shape a and a scale or 
inverse-scale value λ 
IGAMMA(<SHAPE=> a, SCALE=λ | 
ISCALE=λ)  
Inverse-gamma distribution with shape a and a 
scale or inverse-scale value λ 
LOGNORMAL(<MEAN=> μ, SD= | VAR= | 
PREC=) 
Log-normal distribution with mean μ and a value 
for the standard deviation, variance, or precision.  
NORMAL(<MEAN=> μ, SD=  | VAR= | PREC=) 
Normal (Gaussian) distribution with mean μ and a 
value for standard deviation, variance, or precision  
T(<MEAN=> μ, SD=  | VAR= | PREC=, <V=> df) 
t distribution with df degrees of freedom, mean μ, 
and a value for standard deviation, variance, or 
precision  
TABLE(p) 
Categorical distribution with array p of 
probabilities for discrete categories 
UNIFORM(<A=> a, <B=> b) 
Uniform distribution with range a and b 
GENERAL(ll) 
User-defined function constructed using SAS 
programming statements for a single or multiple 
continuous parameters. ll is an expression for the 
log-likelihood of the distribution and is interpreted 
as the log of the joint distribution for the 
parameters. 
Table 3.2: Commonly Used Multivariate Distributions with IRT Models 
Distribution 
Description 
MVN(<MU=> μ, <COV=> Σ) 
Multivariate normal distribution with mean vector μ 
and covariance matrix Σ 
MVNAR(<MU=> μ,  SD=  | VAR=  | PREC= , 
<RHO=> ρ) 
Multivariate normal distribution with mean vector μ 
and covariance matrix Σ. The covariance matrix Σ is 
a multiple of the scale parameter and a matrix with a 
first-order autoregressive structure with parameter ρ. 
 
Density Plots 
As discussed, prior distributions are specified in the PRIOR statement for fixed-effect model parameters 
and in the RANDOM statement for random-effect model parameters. PROC MCMC can also be used to 
explore and plot probability density functions for different prior distributions. Program 3.1 presents the 
code for plotting different probability density functions. Although no data is required to sample directly 
from a distribution, PROC MCMC requires a DATA=<dataset> specification. Therefore, the first element 
in the program includes the creation of an empty data set. The PROC MCMC statement specifies the 
number of MC samples or iterations and density plots that are requested. Other program statements are 
used to specify the four parameters to be plotted and their initial values (PARMS statement); different prior 
distributions are specified for each of these parameters (PRIOR statements). The prior specifications also 
illustrate the use of lower and upper boundaries on the density functions. Every PROC MCMC program 
must have at least one MODEL statement, but because there is no response variable, a MODEL 
GENERAL(<constant>) statement is included to reflect a uniform distribution on the real line. Output from 
this program is presented in Output 3.1.  

48 Bayesian Analysis of Item Response Theory Models Using SAS
Program 3.1: PROC MCMC Code for Exploring Distributions 
data dummy; /* SAS dataset with no observations to use with proc mcmc */ 
proc mcmc data=dummy diag=none nmc=20000 plots=density seed=123; 
parms a1 1 a2 1 b 0 s2 1; 
prior a1 ~ lognormal(0, var=.5);  
prior a2 ~ normal(0, var=4, lower=0); 
prior b ~ normal(0, var=100, lower=-6, upper=6); 
prior s2 ~ gamma(shape=.01,iscale=.01); 
model general(0); 
run; 
Output 3.1: Output from Program 3.1 
 
Arbitrary Distributions 
In some cases, the distributions that you want to specify for model parameters may not follow the built-in 
distributions in SAS. In these cases, you can use the GENERAL(ll) function with SAS programming 
statements to specify any arbitrary distribution for a single continuous variable or for multiple continuous 
variables. The argument ll is an expression for the log-likelihood of the distribution and interpreted as the 
log of the joint distribution in the case of multiple variables. In the context of estimating IRT models, the 
GENERAL function can be used with the MODEL statement to construct an explicit log-likelihood 
function for the probability of a response. This function is useful for some models (for example, 
polytomous IRT models). It can also be used with the PRIOR statement to specify prior distributions for 

Chapter 3: Bayesian Estimation of IRT Models Using PROC MCMC 49
parameters that do not follow built-in distributions. Note that a DGENERAL(ll) function can be used for 
discrete variables.  
An example of using the GENERAL function for defining a prior distribution is presented below. The first 
three lines define the log of the joint distribution for two parameters, b1 and b2, where the lower boundary 
for b2 is defined by b1. The PRIOR b: ~ GENERAL(LPRIOR) statement specifies the defined joint 
distribution (LPRIOR) as the prior distribution for b1 and b2. This example is useful for defining a joint 
distribution for an ordered set of parameters (for example, b1 < b2) and is used in Chapter 5 for estimating 
the parameters of the graded IRT model. This example also illustrates use of log density functions with 
SAS programming statements (LPDFNORM function) and boundaries for density functions: LOWER (−6, 
B1) and UPPER (6). Output from executing these statements in a PROC MCMC program is presented in 
Output 3.2. For other examples involving the GENERAL and DGENERAL functions, see the auxiliary 
documentation for PROC MCMC. 
lprior=0; 
lprior=lprior+lpdfnorm(b1,0,2,-6,6); 
lprior=lprior+lpdfnorm(b2,0,2,b1,6); 
parms b1 -1 b2 -.5; 
prior b: ~ general(lprior);
Output 3.2: Output of Density Functions for an Ordered Set of Parameters (b1 < b2) 
 
PROC MCMC Template for IRT Model Estimation 
This section illustrates a general template for the PROC MCMC commands used to estimate IRT models. A 
template is presented for use with dichotomously scored items under the assumption of unidimensionality 
in the trait being measured. Later sections describe modifications to the general template; for example, 
these modifications may be used to address convergence problems. Subsequent chapters illustrate 
commands for extending this template to polytomously scored IRT models (Chapter 5) and other more 
complex models, such as multidimensional IRT models (Chapter 6).  
 
 

50  Bayesian Analysis of Item Response Theory Models Using SAS 
Recall from Chapter 1 that the general form of the IRT response model for dichotomously scored items, or 
the probability that an examinee receives a correct response (e.g., 1 versus 0, or Yes versus No) to the jth 
item, is commonly modeled as follows: 
ܲ௝൫X୨= 1| ૑୨, Ʌ൯= ܿ௝+ (1 െܿ௝)ܨ(ݖ௝) 
where F(zj) is commonly a logistic or normal deviate with zj = Daj (θ − bj); D is an arbitrary scaling 
constant used to define the metric for θ; θ references a single person parameters (for example, examinee 
ability); and ωωj references the vector of possible item parameters for item j in the model: aj is the slope 
parameter, bj is a threshold parameter, and cj is a lower asymptote parameter.  
A general program template for use with dichotomously scored items is presented below. To estimate a 
model, you can see that all aspects of the model need to be specified: the particular IRT model, parameter 
names, prior distributions for parameters, and the likelihood model that links the observed data and the 
model parameters. PARMS statements are used to define item parameters (aj, bj, cj) in the model ߑand 
RANDOM statements are used to define person parameters (θ) in the model ߒ. The particular IRT model is 
specified through a set of SAS programming statements that define the probability of a response (P[J]) for 
each item ߓ. MODEL statements are used to define the log-likelihood functions that link the observed data 
for each item to the IRT model being estimated ߔ. ARRAY statements are not necessary but serve to 
facilitate reference to item parameters in a DO loop for defining the particular IRT model for each item. 
Although not apparent by the structure of the template program, there is no required order for PROC 
MCMC statements (MODEL, PRIOR, PARMS, and RANDOM). However, with any programming 
statements that are included, any essential ordering of the statements (for example, DO loop) should be 
maintained. 
proc mcmc data=<sas dataset> outpost==<sas dataset> seed=<Value> 
nmc=<value> nbi=<value>  nthreads=<value> statistics=<list>  
diagnostics=<list>  plots=(<list>); 
 
/* define arrays to reference item parameters */ 
array p[number of expected response variables for items]; 
array a[number of slope parameters]; 
array b[number of threshold parameters]; 
array c[number of pseudo-guessing parameters]; 
 
/* define IRT model item parameters */ߑ 
parms a: <initial value>; /* ‘:’ is used to reference array of elements 
*/ 
parms b: <initial value>; /* if only one parameter, do not use the ‘:’ */ 
parms c: <initial value>; 
 
/* define prior distributions for item parameters */ 
prior a: ~ <distribution>; 
prior b: ~ <distribution>; 
prior c: ~ <distribution>; 
 
/* define IRT model person parameters and prior distribution */ߒ 
random theta ~ <distribution> subject=_obs_; 
 
/*set of programming statements defining likelihood function for each 
item; 
 correct response probability, pj, for each item (1,..., J) based on the 
 chosen IRT model*/      
 do j=1 to <number of items>; 
    p[j] = <IRT model defining response probability for each item>; ߓ 
 end; 
 
 
 
 

Chapter 3: Bayesian Estimation of IRT Models Using PROC MCMC   51 
/* response data modeled as bernoulli random variables */ 
model x1 ~ binary(p1); ߔ    
model x2 ~ binary(p2); 
…   
model x<J>  ~ binary(p<J>); /* J = number of items */ 
 
run; 
ߑ The item parameters are random variables but treated as fixed effects using PARMS statements where 
each set of item parameters (aj, bj, cj) share common population distributions. Treating item parameters 
as random effects is discussed in a later section in this chapter. 
ߒ The THETA parameter (θ) for each person is treated as a random effect where persons are assumed to 
come from a specified population. A cluster variable (SUBJECT=) is used to identify each random 
effect in the data set. For standard IRT models, each subject is a random effect. Therefore, 
SUBJECT=_OBS_ indicates that an observation-level random-effects model is assumed where each 
observation is a separate random effect. Hierarchical IRT models that consider clusters of subjects in 
the model are discussed in Chapter 6. 
ߓ Because the IRT model is defined for each item, different models can be specified for different items. 
For example, some items may have cj=0, whereas cj may be estimated for other items. Thus, the cj 
parameter could be excluded from the IRT model specification for items where cj=0. Defining different 
IRT models for different items also allows for mixed item formats where, for example, a subset of 
items are dichotomously scored and other items polytomously scored. 
ߔ Use of the MODEL statement to define the likelihood model compels certain data requirements. For 
example, because the BINARY distribution is specified, the data must be 0s and 1s with 1s 
representing correct responses. An alternative to using MODEL statements is discussed later and does 
not have this same data requirement. 
How PROC MCMC Processes the SAS Data Set 
PROC MCMC uses the prior distributions and modeled log-likelihood function to calculate the posterior 
distributions for parameters. By default, it is assumed that observations are independent. Under this 
assumption, PROC MCMC calculates the log of the posterior distribution as follows: 
log (p(Ʌ|࢟)) = log൫݌(Ʌ)൯+ ෍log (݂(ݕ௜|Ʌ))
௡
௜ୀଵ
 
where θ can be a multidimensional parameter vector and log(f(yi | θ)) is the log-likelihood function for a 
single observation i in the SAS data set with n observations. At each iteration of the sampling algorithm 
(for example, Metropolis algorithm), the programming and MODEL statements are executed for each 
observation, and the log-likelihood is accumulated across all observations in the SAS data set. After the last 
observation is evaluated, the log of the prior distribution is added to compute the log of the posterior 
distribution. Because SAS procedures process sequentially the observations in a data set, computations 
across the observations are not explicitly programmed as is done in other programs implementing MCMC 
algorithms (for example,WinBUGS). After the posterior distributions are obtained, various statistics, 
convergence diagnostics, and graphical displays can be computed from the posterior distributions, as 
detailed in the options for the PROC MCMC statement.  
If observations are not independent, the summation described above produces an incorrect log likelihood 
value. In the case of dependent observations, or f(yi | θ, yj) for L j, you can use the JOINTMODEL option 
in the PROC MCMC statement. For more detail about the JOINTMODEL option and how PROC MCMC 
works, see the auxiliary documentation for PROC MCMC as well as Chen (2009). 
More on the RANDOM Statement 
The RANDOM statement is used to specify random effect parameters that can also reflect cluster-specific 
characteristics. Several options to the RANDOM statement are required: RANDOM <random effect(s)> ~ 
<prior distribution> SUBJECT=<option>. The random effect that is specified can be a parameter or array 
of parameters. Prior distributions for the random effects must be specified. However, only a subset of the 

52  Bayesian Analysis of Item Response Theory Models Using SAS 
distributions supported for the PRIOR or MODEL statements can be used: beta, binary, gamma, igamma, 
normal, and multivariate normal distributions (see Table 3.1 for syntax). The SUBJECT=<variable> option 
is used to identify the variable in the SAS data set that defines cluster membership. This variable can be a 
numeric or character variable and does not need to be sorted. As mentioned above, if random effects are 
modeled for each observation, as for standard IRT models, SUBJECT=_OBS_ may be used and no variable 
defining cluster membership is required. 
Multiple RANDOM statements are specified to reflect different cluster-specific characteristics—for 
example, students grouped within teachers (a teacher-level effect) or students grouped within schools (a 
school-level effect). If these cluster-specific effects are significant, then the responses of students in a 
cluster-specific characteristic are likely to be more related than is accounted for by their latent traits alone. 
See Chapter 1 for more detail on modeling other cluster effects in testing applications and hierarchical IRT 
models. Chapter 6 presents examples for estimating models where cluster-specific characteristics may 
exist. 
The number of random effect parameters is determined by the number of unique values for the 
SUBJECT=<variable> specification. All random effect parameters that are defined by one RANDOM 
statement are assumed to be conditionally independent given the data and other parameters in the model. 
However, random effect parameters that are defined by different RANDOM statements are not assumed to 
be conditionally independent.  
Model Identification 
Model identification is an issue that arises with IRT models because the models are over-parameterized, 
and ability or trait parameters (θ) and item difficulty or threshold parameters (bj) are unobservable and on 
the same scale. Thus, the metric for either the θ  or bj parameters must be defined. One advantage of a 
Bayesian paradigm is that specification of priors with constant parameter values defines a metric for the 
model (see, for example, Fox, 2010). Typically, the prior distributions for all θ or person parameters are 
defined as N(0,1). In other words, the latent traits or abilities are assumed to be random samples from 
standard normal distributions. In the case of multiple person parameters, θ, a multivariate standard normal 
prior would typically be specified. However, it is also possible to have unconstrained parameters for priors 
and establish a metric within each iteration of the MC algorithm; for example, standardize the θ estimates 
at each iteration (see, for example, Fox , 2010). Notably, this option would add overhead to algorithm and 
increase computation requirements. 
Example Use of the Template for the 1-Parameter IRT Model  
The 1-Parameter (1P) model that is illustrated uses a logistic deviate for each item j with zj = Da (θ − bj), 
where θ reflects a single person parameter for each examinee; bj reflects a threshold or item difficulty 
parameter for item j; and a is a common slope parameter across all J items. The example in this section is 
based on data that is used for illustration in various IRT software applications and originally analyzed by 
Bock and Lieberman (1970). This data set consists of responses from 1000 examinees to J=5 multiple-
choice items from the Law School Admissions Test (LSAT). It is assumed that the SAS data set of item 
responses (LSAT) has been read into the SAS Work library. See the SAS Press authors’ web page for a 
program to create this SAS data set (Program_AppendixA.SAS).  
The PROC MCMC code for estimating a 1P model with the LSAT data set is presented in Program 3.2. 
Program 3.2: PROC MCMC Code for Estimating a 1P IRT Model 
proc mcmc data=lsat outpost=lsat_bayes seed=23 nmc=20000 nbi=5000  
   diagnostics=all plots=(trace autocorr);ߑ 
 
   array b[5]; ߒ 
   array p[5];  
                                       
   parms a 1; ߓ 
   parms b: 0;   
 
 

Chapter 3: Bayesian Estimation of IRT Models Using PROC MCMC   53 
   prior b: ~ normal(0, var=4); ߔ 
   prior a ~ lognormal(0, var=4); 
                                             
   random theta ~ normal(0, var=1) subject=_obs_; ߕ 
 
   do j=1 to 5;       
          p[j] = logistic(a*(theta - b[j])); ߖ  
   end; 
 
   model x1 ~ binary(p1);   
   model x2 ~ binary(p2);   
   model x3 ~ binary(p3);   
   model x4 ~ binary(p4);   
   model x5 ~ binary(p5); 
 
run; 
ߑ The item response data (X1-X5) are read from the SAS data set LSAT, the first 5,000 burn-in samples 
(NBI) are discarded, and 20,000 samples (NMC) from the posterior distribution for the model 
parameters (a, bj, and θn) are saved to the SAS data set LSAT_BAYES. Default statistics, all 
diagnostics, and selected plots are requested from the joint posterior distribution. 
ߒ The statement ARRAY B[5] indicates five elements, B1-B5, representing the item difficulty 
parameters for the five items (bj). The statement ARRAY P[5] indicates five elements, P1-P5, 
corresponding to the probabilities of correct responses on each of the five items. Notably, array 
statements may be used to reference sets of parameters more efficiently in DO loops. Consider the 
specification of the IRT response model for each item in the above command. Five separate lines of 
specifications, one for each item (P1-P5), could be specified, but the use of a DO loop simplifies the 
writing of the response model, especially for longer tests. 
ߓ PARMS statements are used to specify model parameters for fixed effects and their initial values. In 
this program, the PARMS B: 0 statement references an array of B parameters, with an initial value of 0 
for each element in this array. 
ߔ PRIOR statements specify prior distributions for parameters that are specified in PARMS statements. 
Information about prior distributions may be available from previous assessments, from the purpose of 
the assessment, or from known properties of IRT response functions. For example, the distribution of 
any bj parameters is generally bounded between −4 and 4 on the θ scale. This information can be 
considered when specifying the prior distributions for bj parameters. In addition, specification of prior 
distributions can consider whether an assessment serves as a broad measure of achievement or is 
designed to measure the trait or ability in a local region of the θ continuum.  
As discussed in Chapter 2, vague or uninformative priors are often defined to mitigate the influence of 
priors on the estimation process. For these types of priors, the probability distributions are relatively 
flat across the range of possible values for the parameter. In this example, the priors for the item 
parameters reflect identical, independent distributions and reflect priors for IRT item parameters 
frequently used in the literature. Normal, lognormal, and beta priors are typically specified for the b, a, 
and c parameters in dichotomous IRT models, respectively. A lognormal distribution restricts the 
distribution of a parameters to be greater than 0. However, some researchers have suggested using 
truncated normal distributions rather than a lognormal distribution since the lognormal parameters 
interact; changing one parameter (for example, mean or location parameter) will change both the mean 
and variance of the lognormal distribution (Curtis, 2010). Although not specified in this example 
because the model reflects a 1-Parameter model, beta priors are typically used when estimating c in 
order to restrict the distribution of c parameters between 0 and 1. Other distributions can be explored 
using Program 3.1 as a template. 

54  Bayesian Analysis of Item Response Theory Models Using SAS 
ߕ Each subject is a random effect, and the NORMAL(0, VAR=1) SUBJECT=_OBS_  specification with 
the RANDOM statement indicates that an observation-level random-effects model is assumed where 
each observation is a random effect with a standard normal distribution. 
ߖ P[J] = LOGISTIC(A*(THETA − B[J])) in the program defines the expected probability of a correct 
response to each item using the logistic function and the scaling constant D=1.0. Alternatively, P[J]  = 
1/(1+EXP(−A*(THETA − B[J]))) could be used instead of the built-in LOGISTIC function. To 
estimate the normal ogive model, P[J] = PROBNORM(A*(THETA − B[J])) can be used, where the 
PROBNORM function  returns a probability from the standard normal distribution (inverse of 
PROBIT function). Note that results based on using the LOGISTIC and PROBNORM functions will 
be essentially equivalent if the scaling constant D=1.702 with the LOGISTIC function or P[J] = 
LOGISTIC(1.702*A*(THETA − B[J])) and D=1.0 with the PROBNORM function. Also note that the 
CDF function can also be specified for either the logistic or normal ogive models (for example, P[J] = 
CDF(‘NORMAL’, THETA , B[J], 1/A)), but use of the CDF function has significant overhead and 
therefore is not recommended as it increases computational requirements. 
Alternatively, a set of programming commands can be used to define the log-likelihood function for 
responses. This log-likelihood expression can then be used with the GENERAL option in the MODEL 
statement to construct an explicit log-likelihood model as in the following code:  
llike = 0;  * used to accumulate the log likelihood across observations; 
do j=1 to 5; 
   prob = 1/(1+exp(-a*(theta - b[j])));   
   llike = llike + x[j] * log(prob) + (1 - x[j]) * log(1 - prob); 
end; 
model general(llike); 
This set of commands would replace the DO loop and series of MODEL statements in Program 3.2. Note 
that this expression for the response model and likelihood is more efficient than the original code in 
Program 3.2 and produces identical results, assuming that the same SEED value is used. Also, prior to the 
release of SAS/STAT 13.2, no built-in distribution could be used with the MODEL statement for non-
binary responses. Thus, this specification will be necessary for estimating IRT models for polytomously 
scored items if using SAS/STAT 13.1 or earlier versions (see Chapter 5). 
Example Output from Estimating the 1P IRT Model 
Output 3.3 presents the default summary output (the number of posterior samples, posterior means, 
standard deviations, and the highest posterior density (HPD) intervals) for the posterior samples from 
PROC MCMC (see Program 3.2). HPD intervals are similar to Bayesian credible intervals (equal tail 
intervals), but HPD intervals are typically not equal tail intervals. They reflect the smallest interval in 
which 100(1 − α)% of the distribution lies (SAS, 2014, Chapter 7). Note that the mean of the posterior item 
distributions, or point estimates for item parameters, closely resemble the MML results produced using 
IRTPRO to estimate a 1P model with the LSAT data (Cai, Thissen, and du Toit, 2011): a = .75, b1 = −3.59, 
b2 = −1.33, b3 = −.31, b4 = −1.73, b5 = −2.62.  
Samples for random effects parameters (that is, 1000 examinee θ values for each iteration) are by default 
not included in the posterior summary output, although all sampled values are saved to the OUTPOST data 
set. The MONITOR option in the RANDOM statement can be used to output posterior summary statistics 
 

Chapter 3: Bayesian Estimation of IRT Models Using PROC MCMC 55
for random effect parameters, or the OUTPOST data set can be analyzed with the use of PROC MEANS to 
obtain posterior summary statistics for the THETA parameters. 
Output 3.3: Posterior Summary Output from PROC MCMC for the LSAT Data 
Because the MCMC method samples parameter values iteratively, determining whether the Markov Chain 
has converged to the target posterior distribution is important. If the chain does not converge, then the 
samples from this chain would not represent the target parameter space or posterior distribution of interest. 
Therefore, you must assess convergence of Markov chains before making any inferences. For a complete 
description of the diagnostics available in PROC MCMC, see Chapter 2.  
Trace or history plots (sampled values across iterations) and autocorrelation plots are typically examined. 
Output 3.4 presents these diagnostic plots for the posterior distributions. With regard to the trace plots, you 
can see that the sampled values are “mixing” reasonably well across iterations for all item parameters (a, b1 
− b5), indicating that the values were traversing in a random pattern over a plausible support region of the 
parameter space. Plots exhibiting a trend (for example, increasing over time) or values that do not traverse 
the entire parameter space quickly would reflect poor mixing (for examples of trace plots exhibiting poor 
mixing, see Chapter 2). 
Autocorrelation plots, on the other hand, are used to evaluate the level of dependence of sampled values 
with a focus on higher lags because dependence at lower lags is expected in a Markov chain. In Output 3.4, 
you can see that trace plots for all item parameters (a, b1 − b5) reflect good mixing, but the autocorrelation 
plots indicate a possible problem with dependence for a number of the parameters except parameter b3. 

56 Bayesian Analysis of Item Response Theory Models Using SAS
Output 3.4: Diagnostic Plots from the Posterior Distribution of Parameters 
 
 
 

Chapter 3: Bayesian Estimation of IRT Models Using PROC MCMC 57
Outputs 3.5 to 3.9 provide other diagnostic information available from PROC MCMC. Some of these tests 
are used to evaluate whether a sufficient number of Monte Carlo parameter values have been sampled in 
order to achieve desired precision (Monte Carlo Standard Error, Raftery-Lewis); other tests are used to 
evaluate convergence of the chain similar to trace plots (Effective Sample Size statistics, Geweke, 
Heidelberger-Welch).  
Output 3.5: Monte Carlo Standard Errors for the 1P Model 
 
The error in posterior estimation can be attributed not only to the standard error of the point estimates as 
reflected by the posterior sample standard deviation, but also to sampling error, referred to as Monte Carlo 
standard error (MCSE). Usually, the simulation should be run until the MC error for each parameter of 
interest is less than about 5% of the sample standard deviation (Spiegelhalter et al., 2003). The MC error 
can always be reduced by lengthening the chain. Results in Output 3.5 indicate that this criterion 
(MCSE/SD < .05) was not attained for 5 of the 6 parameters, indicating more samples may be needed. 
Output 3.6: Geweke Tests for the 1P Model 
 

58 Bayesian Analysis of Item Response Theory Models Using SAS
This statistic tests whether mean posterior estimates have converged by comparing means based on early 
versus later iterations of the chain. Large |z| values indicate lack of convergence. From Output 3.6, five 
item parameters failed the test, again indicating a lack of convergence of the chain. 
Output 3.7: Raftery-Lewis Diagnostic Test for the 1P Model 
 
This test evaluates the accuracy of the estimated percentiles in the summary output. Specifically, the test 
computes a minimum number of iterations to be discarded (BURN-IN samples); the TOTAL number of 
iterations needed to achieve a desired precision in the percentiles (tolerance determined by the level of an 
accuracy parameter); and a MINIMUM number of samples needed to achieve the desired accuracy, 
assuming that the samples are independent. Finally, the test provides a DEPENDENCE FACTOR, which 
measures the departure from independence sampling (TOTAL / MINIMUM). Values closer to 1 indicate a 
lack of dependence; values greater than 1 indicate greater dependence and could indicate that a longer 
Markov chain is required. Based on the results in Output 3.7, significant dependence and more samples are 
indicated. This is consistent with the other results discussed above. 
Output 3.8: Heildelberger-Welch Test for the 1P Model 
 
This test consists of two parts: a stationary test and a half-width test. The stationary test evaluates whether 
the chain is a “weakly” stationary process; the half-width test evaluates whether the part of the chain that is 
determined stationary is adequate to estimate the mean values accurately. PROC MCMC reports whether 
these tests pass or fail. As you can see in Output 3.8, all six item parameters passed both tests, indicating a 

Chapter 3: Bayesian Estimation of IRT Models Using PROC MCMC 59
stationary process for the chain and that the means of the posterior distributions for parameters could be 
estimated accurately. 
Output 3.9: Effective Sample Size Test for the 1P Model 
Effective sample size (ESS) also measures variation in sampled values across iterations and therefore 
reflects relative mixing in the chain. ESS is calculated by first computing the estimated iteration 
(AUTOCORRELATION TIME) where autocorrelations are close to 0. ESS is then equal to NMC / 
(AUTOCORRELATION TIME) where NMC equals the Monte Carlo sample size. In turn, EFFICIENCY 
of the chain is equal to ESS / NMC. Values of ESS that are close to NMC indicate better mixing, whereas 
large discrepancies between ESS and NMC indicate poor mixing. Results in Output 3.9 illustrate ESS 
values that are much lower than the 20000 (NMC), indicating poor mixing that is no doubt due to the high 
observed dependence or high autocorrelations between sampled values. 
The Gelman-Rubin test is another common technique that can be used to evaluate convergence to the target 
distribution. This test requires the use of multiple chains each initiated by different initial or starting values. 
The test compares the between- and within-chain variations in sampled values. This test cannot be 
requested as part of the DIAGNOSTIC options; rather, PROC MCMC must be executed multiple times for 
each chain and initial value. The commands necessary to obtain results for the Gelman-Rubin test are 
described in a later section in this chapter. For a technical discussion and references for the diagnostic 
statistics that are computed in PROC MCMC, see the auxiliary documentation for PROC MCMC. 
Strategies to Improve Convergence of the Monte Carlo Chain 
Obtaining chains that quickly traverse the support region for the joint posterior distribution (mix well) and 
chains that exhibit low dependence among sampled values (low autocorrelations) can be problematic with 
MCMC sampling. Poor mixing is a problem that may indicate a misspecified or over-parameterized model. 
High dependence or high autocorrelations among sampled values typically reflects low efficiency in the 
sampling algorithm, although there is often no impact on the posterior summary statistics for parameters. 
When poor mixing or high dependence is observed in the chains, there are several strategies that can be 
used to improve convergence. 
Chain Thinning 
One way to reduce dependence is to thin the chain by taking every nth sampled value. Thinning the chain is 
specified with the use of the THIN=<nth value> option in the PROC MCMC statement. This method can be 
useful for reducing autocorrelation, but it comes at a cost of inefficiency because sampled values are 
discarded. For example, Output 3.10 presents trace autocorrelation plots for the example with THIN=4 
added to the PROC MCMC command. As can be seen, thinning reduced the autocorrelation significantly 

60 Bayesian Analysis of Item Response Theory Models Using SAS
when compared to the original model (see Output 3.3). However, there are essentially no differences in the 
posterior summaries. 
Output 3.10: Trace and Autocorrelation Plots after Thinning (THIN = 4) 

Chapter 3: Bayesian Estimation of IRT Models Using PROC MCMC   61 
Parameter Blocking 
One strategy when you have poor mixing is to alter the way parameters are sampled or sequentially updated 
by the strategic use of PARMS statements. Each PARMS statement can be used to define a single 
parameter or a set or block of parameters. The Metropolis algorithm used in PROC MCMC updates the set 
of parameters in each PARMS statement simultaneously. In the above PROC MCMC model, there are two 
PARMS statements, one for the single a parameter and one for a block of bj parameters. When parameters 
exhibit strong posterior dependence, improved mixing can result from defining a block of parameters in a 
single PARMS statement. For example, if a and bj parameters were expected to be correlated or exhibit 
strong posterior dependence, the two PARMS statements can be combined into one PARMS statement with 
a single block of parameters (a, b1, … , b5) or PARMS A 1 B: 0. Given this specification, the Metropolis 
algorithm updates or samples all the item parameters (a and bj) jointly. Any correlation between parameters 
is more easily accounted for when jointly updating parameters. 
However, blocking a large number of parameters should be used cautiously because “good mixing” may be 
difficult to achieve when simultaneously sampling a large number of parameters (See the SAS Help for 
PROC MCMC, subtopic “Blocking of Parameters”). With a large number of parameters, fewer proposed 
values are accepted with the Metropolis algorithm, and convergence may be slow. It is recommended that 
PARMS statements be specified that reflect small sets of correlated or dependent parameters and a blocking 
strategy be adopted that falls between blocking all parameters in a single PARMS statement and using 
separate PARMS statements for each parameter.  
In order to compare these two strategies, Outputs 3.11 and 3.12 present trace and autocorrelation plots 
when separate PARMS statements for each parameter are used—that is, 6 PARMS statements—and plots 
based on use of a single PARMS statement—that is, the PARMS statement PARMS A 1 B: 0. The results 
can be compared against the use of two PARMS statements (PARMS A 1; PARMS B: 0) in the original 1P 
example (see Output 3.3).  
Although it is difficult to observe in Output 3.11, use of separate PARMS statements marginally reduced 
the observed dependence among sampled values when compared with the use of two PARMS statements. 
The autocorrelations ranged from .06 to .47 for lag 50 for the original program (Program 3.2); when six 
separate PARMS statements were used, the autocorrelations for the same lag ranged from .02 to .40. 
However, a disadvantage with the use of sampling each parameter individually is that the computational 
cost increases. In the present analysis, the computation time was approximately doubled. In Output 3.12, 
you can also see modest improvement from the use of a single block of parameters. Here, the 
autocorrelations range from .06 to .27 for lag 50. 

62 Bayesian Analysis of Item Response Theory Models Using SAS
Output 3.11: Trace and Autocorrelation Plots with Separate PARMS Statements for Each Parameter 

Chapter 3: Bayesian Estimation of IRT Models Using PROC MCMC 63
Output 3.12: Trace and Autocorrelation Plots after Blocking All Parameters (PARMS A1 B: 0) 
 

64  Bayesian Analysis of Item Response Theory Models Using SAS 
Model Re-parameterizing 
Finally, another strategy for addressing poor mixing is to re-parameterize the model and reduce dependence 
among the parameters. For example, the probability function in the 1P program (Program 3.2) can be re-
expressed as an intercept model with P[J] = LOGISTIC(A*THETA - D[J]), where D[J] is an intercept term 
and a function of the a and bj parameters or bj = dj / a. In the model, the computation, B[J] = D[J] / A, can 
be included in the DO LOOP of the program and the bj parameters monitored by including a 
MONITOR=(A B D) in the PROC MCMC statement. The example would be modified as presented in 
Program 3.3. 
Program 3.3: Re-parameterization of the 1P IRT Model 
proc mcmc data=lsat outpost=lsat_bayes seed=23 nbi=5000 nmc=20000   
   monitor=(a b d) diagnostics=all plots=(trace autocorr); 
   array b[5];  
   array d[5];  
   array p[5];  
   parms a 1; 
   parms d: 0;   
   prior d: ~ normal(0, var=4);  
   prior a ~ lognormal(0, var=.25);  
   random theta ~ normal(0, var=1) subject=_obs_;  
   do j=1 to 5;  
           p[j] = logistic(a*theta-d[j]); 
           b[j] = d[j] / a; 
   end; 
   model x1 ~ binary(p1);   
   model x2 ~ binary(p2);   
   model x3 ~ binary(p3);   
   model x4 ~ binary(p4);   
   model x5 ~ binary(p5);      
run; 
Selected results from using this re-parameterization of the IRT model are presented in Outputs 3.13 through 
3.15. Output 3.13 provides a matrix of scatterplots that reflects the correlations among the item parameters 
and illustrates the impact of the transformation on the level of dependence among the parameters. In the 
output, two blocked areas in the lower triangle of the matrix of scatterplots are defined. The upper block 
(red shape) describes the relationships between the original a and bj parameters, whereas the lower block 
(blue shape) portrays the relationships between the dj parameters and the original parameters (a, bj). You 
can see from the upper block that there are significant relationships among a number of the a and bj 
parameters—indicated by more linear patterns in the scatterplots. When compared with the lower block 
involving relationships with the dj parameters, you can see that most of the relationships appear very low 
except for one combination (d3 , b3). Thus, while re-parameterization does not remove the original 
dependence among the a and bj parameters, it does remove dependence among the parameters in the re-
parameterized model. 
Posterior summary statistics are presented in Output 3.14, and diagnostic plots for the a and bj parameters 
are presented in Output 3.15. As can be seen, this parameterization produces essentially the same posterior 
summary results but improves mixing and reduces autocorrelation by reducing substantially the 
dependence among the sampled a and bj parameters across iterations.  
Note that the dj parameters can be excluded from the MONITOR command because they lack the 
interpretability of the bj parameters. Finally, as discussed already, despite differences in the autocorrelation 
results across all the different strategies, essentially no impact was observed on the posterior summary 
statistics for parameters. 
 
 
 

Chapter 3: Bayesian Estimation of IRT Models Using PROC MCMC 65
Output 3.13: Parameter Correlations for the Re-parameterized Model 
 
Output 3.14: Posterior Summary Output for the Re-parameterized Model 
 
 

66 Bayesian Analysis of Item Response Theory Models Using SAS
Output 3.15: Diagnostic Plots for the Re-parameterized Model 
 

Chapter 3: Bayesian Estimation of IRT Models Using PROC MCMC   67 
Analysis of Multiple Chains—Gelman-Rubin Test for Convergence 
The Gelman-Rubin diagnostics are based on analysis of multiple MCMC chains, each initiated with 
different starting values for parameters. To obtain these diagnostics, the model must first be estimated 
multiple times with different initial values for parameters. The next step is to compare the variance of 
sampled values within each chain and between chains for each of the identified parameters. The commands 
to perform this task use an autocall macro GELMAN that computes the necessary variance components. In 
addition to the variance estimates, trace plots for the multiple chains can also be obtained. For a complete 
description of the calculations, see the auxiliary documentation for PROC MCMC.   
An adaptation of the program code in the PROC MCMC documentation is provided for the 1P model 
example in Program 3.4 and consists primarily of three macro calls: %GMCMC, %GELMAN, and 
%TRACE. Note that in the GMCMC macro, most output is suppressed in the PROC MCMC statement 
because the focus is on the Gelman-Rubin test. The number of burn-in cycles is 0 to trace values from the 
start of the chains. Finally, the option INIT=REINIT is important because this option resets the initial 
values, after the tuning phase, with the different values that are specified (for example, A_INIT1, A_INIT2, 
A_INIT3). The result of executing the macro GMCMC is three data sets (OUT1, OUT2, and OUT3) that 
include sampled values from the three different chains.  
The autocall macro GELMAN is a built-in macro that computes and outputs the necessary variances from 
the multiple chains that are produced by the macro GMCMC. By default, the number of chains is three. The 
macro TRACE overlays plots of the sampled values across the iterations for each chain. 
Program 3.4: Program for Obtaining the Gelman-Rubin Statistics 
%let nchain = 3;        * number of parallel chains; 
%let a_init1 = .1;      * set of 3 initial values for ”a” parameter; 
%let a_init2 = 1; 
%let a_init3 = 6; 
%let b_init1 = 4;       * set of 3 initial values for all ”b” parameters; 
%let b_init2 = 0; 
%let b_init3 = -4; 
%let nparm=6;           * number of parameters to test for convergence; 
%let nsim=5000;         * number of mc samples; 
%let var= a b1 b2 b3 b4 b5;   * specification of parameters to test; 
 
%macro gmcmc; * defines a macro to loop proc mcmc command for &nchain 
chains; 
%do i=1 %to &nchain; 
proc mcmc data=lsat outpost=out&i seed=&i nbi=0 nmc=&nsim init=reinit      
   monitor=(a b) statistics=(summary) diagnostics=none plots=none; 
   array b[5];  
   array p[5];                        
   parms a &&a_init&i;                
   parms b: &&b_init&i;               
   prior b: ~ normal(0, var=4);       
   prior a ~ lognormal(0, var=.25);   
   random theta ~ normal(0, var=1) subject=_obs_;  
   do j=1 to 5;                      
      p[j] = logistic(a*(theta-b[j])); 
   end; 
   model x1 ~ binary(p1);   
   model x2 ~ binary(p2);   
   model x3 ~ binary(p3);   
   model x4 ~ binary(p4);   
   model x5 ~ binary(p5);      
%end; 
%mend; 
 
ods listing close;   %gmcmc;  ods listing;  
 
 
 

68 Bayesian Analysis of Item Response Theory Models Using SAS
data all; * combine the posterior samples from the multiple chains; 
set out1(in=in1) out2(in=in2) out3(in=in3); 
if in1 then chain=1; 
if in2 then chain=2; 
if in3 then chain=3; 
run; 
/* Compute the Gelman-Rubin variance components and print the results */ 
%gelman(all, &nparm, &var, &nsim); 
data gelmanrubin(label='gelman-rubin diagnostics'); 
merge _gelman_parms _gelman_ests; 
run; 
proc print data=gelmanrubin; 
run; 
/* plot trace plots for the three Markov chains. */ 
%macro trace; 
%do i = 1 %to &nparm; 
proc sgplot data=all cycleattrs; 
series x=iteration y=%scan(&var, &i) / group=chain; 
run; 
%end; 
%mend; 
%trace; 
Partial results from computing the Gelman-Rubin test for the 1P model example are presented in Outputs 
3.16, 3.17, and 3.18. The Gelman-Rubin statistics are presented in Output 3.16. You can see large 
deviations between the two variance components (between- and within-chain variance), indicating possible 
non-convergence. However, the two variance components can be combined into a weighted estimate that 
reflects the expected variance if all chains have reached the same target distribution. This weighted 
estimate can then be compared to the within-chain variance component. If all chains are sampling from the 
same target distribution, the ratio of these two components (labeled Estimate) should be close to 1. Based 
on this weighted estimate in the output, you can see that all chains were sampling from the same target 
distribution.  
From the trace plots for a subset of item parameters (a, b1) in Outputs 3.17 and 3.18, the three chains all 
converged rather quickly to the same region of the parameter space despite different initial values. Trace 
plots were similar for the other bj parameters. Thus, in this example, results do not appear to be dependent 
on particular starting values for the MC sampling algorithm. 
Output 3.16: Gelman-Rubin Statistics from the 1P Model   

Chapter 3: Bayesian Estimation of IRT Models Using PROC MCMC 69
Output 3.17: Trace Plots for the Three Chains for the a Parameter   
Output 3.18: Trace Plots for the Three Chains for the b1 Parameter 

70  Bayesian Analysis of Item Response Theory Models Using SAS 
Informative, Uninformative, and Hierarchical Priors   
The Bayesian paradigm begins with an assessment about the uncertainty of model parameters (prior 
distributions) before data are collected. Specification of the prior distributions can be based on previous 
research, expert opinion, or based on the properties of parameters in the models being estimated. In the case 
of IRT model applications, it is typical to base prior specifications on the properties of the model 
parameters. For example, slope parameters, which reflect the relationship between the item and the 
underlying trait being measured, should be positive and rarely less than .5 or greater than 4. Also, threshold 
parameters are rarely less than −4 or greater than 4. In addition, specification of prior distributions can 
consider whether an assessment serves as a broad measure of achievement or is designed to measure the 
trait or ability in a local region of the θ continuum. Recently, power priors have been discussed that allow 
for historical data, such as previous item parameter and trait parameter estimates, to be combined with the 
current item response data in the MCMC parameter estimation process (Matteucci & Veldkamp, 2014). 
As discussed in Chapter 2, the prior is combined with the data through the likelihood function to obtain 
posterior distributions for model parameters. However, because the prior contributes additional information 
in a Bayesian analysis, different types of prior specifications can influence the analysis. 
Informative Priors 
An informative prior is a prior that dominates the likelihood. Informative priors place more value on the 
external information about a parameter in the Bayesian analysis. Generally speaking, the variance of the 
prior distribution determines the level of information provided by the prior. If the variance of a prior 
distribution is small, the prior will have more influence on the posterior distribution for the parameter. For 
example, the prior for thresholds parameters in an IRT model may be defined as Normal(0, 4). This may be 
considered a reasonable informative prior since threshold parameters are typically between −4 and 4. 
However, the primary effect of an informative prior is to “shrink” the distribution of parameter values 
toward the mean of the prior distribution. Thus, there will be a bias in the posterior toward the mean of the 
prior. Therefore, informative priors should be used with caution in practice. One advantage of more 
informative priors and shrinking values to the mean of the prior, is that the parameter space that is sampled 
can be constrained so that out-of-bounds or very uncommon values are not sampled. 
Uninformative Priors 
Uninformative priors are often used to reduce the influence of the prior distribution on the shape of the 
posterior distribution. Such priors can be defined by specifying large variance terms for prior distributions. 
For example, in the 1P IRT model considered above, the prior distributions for the bj parameters can be 
changed to Normal with mean=0 and variance =100 or Normal(0,100). These priors may be considered 
uninformative or weakly informative because the densities are relatively flat and therefore reflect similar 
likelihoods over a reasonable range of values for the parameters. For the bj parameters, for example, the 
middle 50% of the values from this distribution fall between approximately −6 and 6.  
Using these prior distributions for the bj parameters, the 1P model was re-estimated and summary statistics 
for the analysis are presented in Output 3.19. From the output, you can see that the summary results are 
similar to results based on Normal(0, 4) priors for the bj parameters in Program 3.2 (see Output 3.3), with 
slightly higher standard deviations than those for the more informative priors. 
 

Chapter 3: Bayesian Estimation of IRT Models Using PROC MCMC 71
Output 3.19: Posterior Summary Output from PROC MCMC with Uninformative Priors 
 
Hierarchical Priors 
In the above examples, constants were used to define the parameters of the prior distributions. These 
parameters, called hyperparameters, can themselves be parameters with associated priors (hyperpriors or 
hierarchical priors). This creates a hierarchical structure to the prior specifications that allows for, in the 
context of IRT models, “borrowing information across observational units (individuals or items), thereby 
making the results more robust” (Johnson, Sinharay, & Bradlow, 2007, p. 589). Hierarchical priors may 
also be used to reduce the strength of priors, perform a noninformative analysis, and let the data influence 
hyperparameters.  
For example, in the 1P IRT model, we may want to consider the bj parameters to be exchangeable and 
assign a normal prior to the bj parameters with unknown mean and variance (Kim & Bolt, 2007). Priors are 
then specified for these parameters—for example, a local or restricted uniform prior for the mean and an 
inverse gamma distribution for the variance parameter. Note that the inverse gamma distribution is 
typically specified in this context because it is a conjugate prior to the normal distribution, although 
Gelman (2006) has recommended uniform or half t priors for variance parameters.   
The commands for this hyperprior model are presented in Program 3.5. You can see that in this program, 
the prior for the bj parameters are normal distributions with hyperparameters mean = MUB and variance = 
VARB. Hyperpriors for these hyperparameters both reflect noninformative distributions: uniform 
distribution for MUB ranging from −6 to 6 and an inverse gamma distribution for VARB with parameters 
equal to .01. The parameters for the inverse gamma distribution are typically equal, although this is not 
required. These hyperpriors draw on the data to estimate the hyperparameters of the normal prior for the bj 
parameters. Thus, the amount of “shrinkage” to the mean of the prior is determined by the data. A 
cautionary note is that hyperpriors can become influential with small data sets where little information is 
available to estimate the hyperparameters precisely (Gelman, Carlin, Stern, Dunson, Vehtari, & Rubin, 
2014).  
Program 3.5: Posterior Summary Output from PROC MCMC with Hierarchical Priors 
proc mcmc data=lsat outpost=lsat_bayes seed=23 nbi=5000 nmc=20000 
  diagnostics=all plots=(trace autocorr); 
array b[5];                                                            
array p[5];                                                            
parms a 1; 
parms b: 0; 
parms mub 0 varb 4;  
prior b: ~ normal(mub, var=varb); 
prior mub ~ uniform(-6,6); 
prior varb ~ igamma(.01, scale=.01);  
prior a ~ lognormal(0, var=4);                                         

72 Bayesian Analysis of Item Response Theory Models Using SAS
random theta ~ normal(0, var=1) subject=_obs_;  
do j=1 to 5;                      
p[j] = logistic(a*(theta-b[j])); 
end; 
model x1 ~ binary(p1);   
model x2 ~ binary(p2);   
model x3 ~ binary(p3);   
model x4 ~ binary(p4);   
model x5 ~ binary(p5);      
run; 
Posterior summary results for the 1P IRT model with hierarchical priors for the bj parameters are presented 
in Output 3.20. From the output, you can see that the posterior statistics are also similar to the results based 
on both the uninformative and informative priors. Summary statistics are also provided for the mean and 
variance of the hyperparameters for the prior distribution of the bj parameters. Here, the mean for MUB is 
í1.95 which approximately equals the average of the five posterior means for the separate bj parameters. 
The mean for VARB is 3.21 indicating significant variability among the bj parameters. However, the 
standard deviations for these parameters reflect some degree of imprecision, particularly for the VARB 
parameter. This may not be surprising given that the data for estimating the mean and variance of the bj
parameters comes from five items. 
Output 3.20: Posterior Summary Output from PROC MCMC with Hierarchical Priors 
 
Other examples of hierarchical priors in this book include hierarchical priors on the aj and cj parameters in 
dichotomously scored IRT models (Chapter 4), as well as use of hierarchical priors with a testlet IRT 
model and multilevel IRT models (Chapter 6).  
Effect of Different Priors 
Because the prior contributes additional information in a Bayesian analysis, different prior specifications 
can influence the posterior estimates. Output 3.21 compares the posterior densities for one parameter (b2) in 
the 1P model under different prior specifications:  
Ɣ 
Typical prior⎯b2 ~ Normal(0,4) 
Ɣ
Uninformative prior⎯b2 ~Normal(0,100)  
 
Hierarchical prior⎯b2 ~Normal(mu=μb, var=σ2b) where μb ~ uniform(−6,6) and σ2b ~ 
igamma(.01,.01)    

Chapter 3: Bayesian Estimation of IRT Models Using PROC MCMC 73
Of the three priors compared in the figure, the Normal(0,4) prior provides the most information. As you can 
see, the posterior density is pulled more to the mean of the prior(mean=0), and the dispersion is lower. The 
hierarchical and uninformative priors appear to cause substantial overlap in the posterior densities for the 
parameter, indicating that both prior specifications are somewhat uninformative. However, it appears that 
the posterior density using a hierarchical prior is located between the densities based on uninformative and 
informative priors. This may indicate that the effect of a hyperprior structure on a posterior distribution is 
somewhat weaker than the effect from an informative prior, but somewhat stronger than the effect from an 
uninformative prior.  
Output 3.21: Posterior Densities for Parameter b2 under Different Priors 
Treatment of Item Parameters as Random Effects 
In addition to modeling the person parameters as random effects, you can also model the item parameters 
as random effects. As discussed by Fox (2010), this approach does not necessarily require a “sampling 
argument” and may improve parameter estimation by reducing the dimensionality space for the parameters. 
Furthermore, as discussed in Chapter 1, a class of hierarchical models can be used to model random item 
effects where a random error term is added at the item level and covariates. For example, task features or 
cognitive skill requirements can be introduced to explain variability in item parameters. In this section, 
specifying items as random effects is illustrated in order to compare with the estimation of the item 
parameters with the fixed effects specification in the above example. 
In order to estimate a model with random item and person effects, the data set must be consistent with the 
hierarchical structure to be modeled. In typical item response data sets, items are nested within persons. 
Program 3.6 may be used to restructure the data necessary to estimate a random item effects IRT model for 
the bj parameters. The DATA step in the program creates a SAS data set with 5 observations for each 
person (indexed by the variable OBS). Thus, there is a total of N × I observations in the data set, where N 
equals the number of persons and I the number of items. Each observation for each person will contain one 
item response (u) that corresponds to one particular item (indexed by the variable ITEM). 

74 Bayesian Analysis of Item Response Theory Models Using SAS
In order to specify random item effects for the bj parameters, a RANDOM statement is used for the item 
parameters instead of a PARMS statement in Program 3.6. No random item effects are modeled for the a 
parameter because only one slope parameter is estimated in the 1P IRT model. The RANDOM statement 
specifies identical and independent priors (normally distributed with mean = 0 and variance = 4) for each of 
the bj parameters. In addition, a MONITOR option is used so that results (for example, statistics and plots) 
are output for the random bj parameters. Finally, because there is one response variable for each 
observation in the data set, there is one statement expressing the probability of the response and one 
MODEL statement. The implication is that analyzing a hierarchical or multilevel data set will require more 
processing time because many more observations must be processed—for this example, N × I observations 
for random item effects versus of N observations for fixed item effects.  
Program 3.6: Program to Restructure the Data and Estimate a Random Item Effects IRT Model 
data lsat_ml; 
set lsat; 
array x{*} x1-x5; 
retain obs (0); 
obs=obs+1; 
do item=1 to 5; 
   u=x[item]; 
   output; 
end; 
keep obs item u; 
run; 
proc mcmc data=lsat_ml outpost=lsat_bayes_ml seed=23 nmc=20000 nbi=5000  
diagnostics=all plots=(trace autocorr); 
parms a 1; 
prior a ~ lognormal(0, var=4); 
random b ~ normal(0, var=4) subject=item monitor=(b);   
random theta ~ normal(0, var=1) subject=obs;  
p = logistic(a*(theta - b)); 
model u ~ binary(p); 
run; 
Posterior summary results for this model are presented in Output 3.22, and diagnostic plots are presented in 
Output 3.23. In comparison with the original model with fixed item effects, the posterior summary statistics 
and trace/autocorrelation plots are similar. In examining the autocorrelations at lag 50, a marginal 
improvement was noted for the random item effects model with correlations ranging from .03 to .44; the 
corresponding autocorrelations for the fixed effects model ranged from .06 to .47. 
Output 3.22: Posterior Summary Output for the Random Item Effects (bj Parameters) Model 

Chapter 3: Bayesian Estimation of IRT Models Using PROC MCMC 75
Output 3.23: Trace and Autocorrelation Plots for the Random Item Effects (bj  Parameters) Model 

76  Bayesian Analysis of Item Response Theory Models Using SAS 
Model Fit and Model Comparisons 
The fit of a model, or the correspondence between model predictions and observed data, is generally 
regarded as an important property of model-based procedures. When a model does not fit the data, the 
validity of using results (e.g., estimated θ parameters) may be compromised. Furthermore, in many IRT 
applications, different and competing models may be estimated, and it is important to choose between 
competing models to arrive at a preferred and parsimonious model.  
As discussed in Chapters 1 and 2, the posterior predictive model checking (PPMC) method is a popular 
Bayesian model-checking tool and has proved useful for evaluating model fit with IRT models. In PPMC, 
results based on the observed item responses are compared with results based on predicted item responses 
using the estimated IRT model (posterior predictive distribution). This comparison allows for examining 
the relative consistency between observed and model-based predicted item responses. In addition, in a 
Bayesian paradigm direct model comparison methods fall within two classes: (1) the Deviance Information 
Criteria (DIC); and (2) indices based on the Bayes factor. SAS software includes methods and tools for 
evaluating both model fit and model comparisons. These methods are discussed in separate chapters of this 
book (Chapters 7 and 8) and illustrated through several examples.  
Comparison with WinBUGS 
WinBUGS (Spiegelhalter, Thomas, Best, & Lunn, 2003) is a popular program that also facilitates Bayesian 
analysis by eliminating the need for directly programming the MCMC algorithm. For both PROC MCMC 
and WinBUGS, the user specifies the likelihood model and defines prior distributions for parameters. The 
following WinBUGS program replicates the same 1P IRT model illustrated in Program 3.2. 
model  
{ 
   for (j in 1:n)  { 
       for (k in 1:t)  { 
           logit(p[j,k]) <- a*(theta[j] - b[k]) 
           y[j,k] ~ dbern (p[j,k]) 
           } 
       theta[j] ~dnorm(0,1 ) 
       } 
   # priors for item paramters  
     for (k in 1:t) { 
          b[k] ~ dnorm(0,.25)  
     } 
     a ~ dlnorm(0, 4) 
} 
# initial values 
list(b=c(0,0,0,0,0), a=1, theta=c(0,0,0,0,…,0,0,0,0)) 
 
# data 
list(n=1000, t=5, y=structure(.data=c( 
0,0,0,0,0, 
0,0,0,0,0, 
0,0,0,0,0, 
… 
1,1,1,1,1, 
1,1,1,1,1, 
1,1,1,1,1), 
.dim = c(1000,5))) 
As can be seen, the structure of the code is very similar between the two programs. One difference is that 
processing over examinees is explicit in the outer FOR loop in the WinBUGS code, but in SAS PROC 
MCMC, the processing over examinees is implicit by the processing of a SAS data set. One advantage to 
using SAS is that a SAS data set can be used directly and may or may not include variables that are 
excluded from the analysis. When you are using WinBUGS, the data must have a specific format, one of 
 

Chapter 3: Bayesian Estimation of IRT Models Using PROC MCMC   77 
which is illustrated in the above code. And no unanalyzed variables can be included in the formatted data. 
Additionally, pre-processing of the SAS data set can be accomplished using the robust programming 
language available within SAS. Finally, specification of the prior distributions and initial values can be a 
bit more cumbersome in WinBUGS than the corresponding specifications in PROC MCMC. 
As for convergence diagnostics, the two programs both contain a number of tools and statistics for 
evaluating convergence. However, SAS does include some diagnostics that are available only when post-
processing the posterior distribution data sets from WinBUGS. In addition, both programs include graphic 
tools although some graphic tools for SAS are available by processing the posterior distribution with SAS 
graphics procedures. The graphic tools in WinBUGS are available within the user interface. Finally, the 
more significant advantage to PROC MCMC in comparison with WinBUGS is in model evaluation 
(PPMC) and in processing speed, especially through parallel processing using the NTHREADS option in 
the PROC MCMC statement.  
As will be illustrated and discussed in later chapters on model comparisons and evaluating model fit using 
PPMC, SAS has a more robust programming language, whereas the programming language available in 
WinBUGS can be difficult to use for some analyses. 
The OUTPOST SAS Data Set 
As mentioned in Chapter 2, one advantage of a Bayesian analysis of IRT models is that item and person 
parameter estimates are available simultaneously. Thus, any uncertainty in item parameter estimates is 
accounted for in the uncertainty in person parameter estimates. In PROC MCMC, the OUTPOST data set 
contains all monitored parameters, as well as parameters specified through the RANDOM statement for all 
NMC iterations.  
Because this data set is a standard SAS data set, this data set can be post-processed with the use of any SAS 
commands. For example, point estimates and standard errors for the θ parameters from estimating the 1P 
model can be obtained using PROC MEANS. The means for the θ parameters as well as the standard 
deviations for each θ parameter, or empirical standard errors, can be computed and merged with the data 
file of item responses as illustrated in Program 3.7. 
Program 3.7: Program to Merge Estimates of θ and SEθ with Original Item Responses 
/* obtain summary statistics from the posterior distributions */ 
proc means data=lsat_bayes; 
     var theta_1-theta_1000; 
     output out=means  mean=mean1-mean1000  std=sd1-sd1000; 
run; 
 
/* merge the summary statistics with the original data */ 
data lsat; 
   set lsat; 
   if _n_=1 then set means; 
   array th{1000} mean1-mean1000; 
   array sd{1000} sd1-sd1000; 
   retain obs (0) mean1--sd1000; 
   obs=obs+1; 
   theta=th[obs]; 
   se=sd[obs]; 
   keep x1-x5 total theta se; 
run; 
A subset of this data set is presented in Output 3.24—observations 1−5 and 995−1000. You can see that the 
point estimates for ability (θ) range from −1.92 for a raw score of 0 (none correct) to .65 for a raw score of 
5 (all correct). Standard errors for the ability estimates or standard deviations for the posterior distributions 
for the θ parameters range from approximately .77 to .87. Note that although a 1P model was estimated, 
point estimates for the ability parameters are not equivalent for equal raw scores. This is because each 

78 Bayesian Analysis of Item Response Theory Models Using SAS
parameter is sampled separately in the MCMC algorithm even with identical prior distributions. As 
expected, however, point estimates for equivalent raw score totals are essentially equivalent. 
Output 3.24: Item Responses and Theta Point Estimates / Standard Errors 
 
Autocall Macros for Post-Processing PROC MCMC Data Sets 
All of the plots and tables available in PROC MCMC can be regenerated with the use of autocall macros or 
macros available in SAS libraries. For PROC MCMC, Table 3.3 details the available autocall macros. For 
all macros, the data set of posterior samples and variable list are required. Optional parameters can be used 
to suppress printing (PRINT=NO) and save results to a SAS data set (OUT=<dataset>). For example, to 
regenerate the diagnostic plots from the 1P analysis where OUTPOST=LSAT_BAYES, use the following 
command: 
%TADPLOT(DATA=LSAT_BAYES, VAR=A B1-B5); 
Notably convergence diagnostics can also be produced using the OUTPUT data set and JMP software 
(Zink, 2014). 
 
 

Chapter 3: Bayesian Estimation of IRT Models Using PROC MCMC   79 
Table 3.3: Available Autocall Macros for PROC MCMC 
Autocall Macro Name 
Description 
%ESS  
Effective sample sizes  
%GEWEKE 
Geweke diagnostic  
%HEIDELfnMacroOpt  
Heidelberger-Welch diagnostic  
%MCSE  
Monte Carlo standard errors  
%RAFTERY  
Raftery diagnostic  
%POSTACF  
Autocorrelation plots  
%POSTCOR  
Correlation matrix  
%POSTCOV  
Covariance matrix  
%POSTINT  
Equal-tail and HPD intervals  
%POSTSUM  
Summary statistics  
%TADPLOT 
Trace, autocorrelation, and kernel density plots 
%CATER 
Bar plot of 95% intervals for multiple 
 
Preliminary Item Analyses 
Performing a preliminary item analysis before estimating an IRT model is often useful. Such an analysis 
can identify any problems with items or response scales—for example, items that have very low 
discrimination, or items with very low variability. Item responses for these types of items can result in 
instability in the estimation process both for Bayesian and ML-based methods. Simple item statistics based 
on classical test theory can be computed with the use of PROC CORR and the ALPHA option: 
proc corr data=<SAS Dataset> alpha; 
     var <Item Response Variables>; 
An example of the output for the LSAT data set with the item response variables X1−X5 is presented in 
Output 3.25. The simple statistics display the mean response; a proportion correct or item difficulty index 
for dichotomously scored items; and the standard deviation in the responses for items. Cronbach’s 
Coefficient Alpha, a measure of internal consistency reliability or the level of response consistency across a 
set of items, is also presented for both the raw and standardized responses (rescaled with unit variances). 
Finally, the correlations of items with a total score based on the remaining items (corrected item to total 
score correlation) is also provided. Here the total score serves as a proxy for the latent construct being 
measured.  
With regard to identifying potential problems with items, unexpected means, extreme means (for example, 
YDOXHV§0YDOXHV§), or items with very low standard deviations should be examined (for example, 
wording problems, score scale problems). Frequency distributions for item score responses can also be 
examined by means of PROC FREQ to ensure a range of responses to the items exists (individuals 
responding at all score levels). Low values of alpha indicate low consistency in responses across items; 
therefore, items should be examined prior to further analysis.  
In the output, you can see that alpha is low, but such a low value would be expected because of the small 
number of items. Item to total score correlations can be used to identify any items that are contributing to 
low reliability (that is, low alpha). Items with very low correlations between an item and the total score 
reflect items exhibiting very low discrimination. Items with corrected item to total score correlations that 
are negative or §0 should be examined because these values indicate response inconsistency or items with 
negative or very low discriminatory power. Correlations that are greater than .3 are desirable. The range 
and variability in the item to total score correlations also indicate how items vary in discrimination power. 
In the case of the LSAT responses, the correlations are low, but again this is probably due to the small 

80 Bayesian Analysis of Item Response Theory Models Using SAS
number of items; and the correlations are similar, suggesting that the items are exhibiting approximately 
equal ability to discriminate examinees. For more information on item analysis based on classical test 
theory, see Crocker and Algina (1986). 
Output 3.25: Classical Test Theory Item Statistics for the LSAT Data Set 
Finally, factor analytic methods can be useful for gaining insight into the structure and dimensionality that 
underlie the item responses. In the context of estimating IRT models, factor analytic methods can be useful 
for identifying the number of possible person parameters (θ) that should be included in the model. Factor 
analytic methods can also be useful for identifying competing models with differing numbers of person 
parameters that should be estimated and compared. This issue relates to the dimensionality assumption 
underlying IRT models that was discussed in Chapter 1. For more information on this assumption and the 
use of factor analytic methods see Chapter 1. 
LSAT Data Source: With kind permission from Springer Science+Business Media, Psychometrika, 
Fitting a response model for n dichotomously scored items, volume 35, 1970, page 188, Bock, R. D., and 
Lieberman, M., Section 6, Table 1: LSAT data, © 1970 Springer Link. 

 
 
Chapter 4: Bayesian Estimation of Unidimensional 
IRT Models for Dichotomously Scored Items 
Introduction ......................................................................................................... 81
The 1-Parameter IRT or Rasch Model .................................................................. 82
Use of a Normal Ogive Link Function ................................................................... 85
The 2-Parameter IRT Model ................................................................................ 85
A 2-Parameter IRT Model with a Hierarchical Prior ............................................. 91
The 3-Parameter IRT Model ................................................................................ 94
Comparison of Results Based on MML Estimation ............................................ 105
Display of Item Response Functions ................................................................. 107
Adding Elements to the Graph: Uncertainty in Item Parameters ................................ 109
Adding Elements to the Graph: Observed Results ....................................................... 111
 
Introduction  
Three IRT models that may be used with dichotomously scored items under the assumption of 
unidimensionality are the 1-, 2-, and 3-Parameter models (see, for example, Hambleton & Swaminathan, 
1985). In the general form for these three models, the probability that an examinee receives a correct 
response (1 as opposed to 0; Yes, as opposed to No) to the jth item is commonly modeled with a logistic 
function: 
୨ሺɅሻൌ୨൫୨ൌͳȁ૑୨ǡ Ʌ൯ൌȲሺ୨ሻൌ୨൅ሺͳ െ୨ሻ
௘ೋೕ
௘ೋೕ  
where <(zj) is the cumulative logistic function, zj = Daj (T   bj ) is a logistic deviate; D is a scaling 
constant; T references the single person parameter or examinee characteristic; and Zj references a vector of 
possible item parameters in the model for item j: aj is the slope parameter, bj is the threshold parameter, and 
cj is the lower asymptote parameter.   
The person parameter reflects a single construct or scale that corresponds to the characteristic or trait being 
measured by the items. This trait could reflect many different types of characteristics such as ability in 
educational assessments, a psychological trait such as depression, or quality of life in health assessments.  
The only requirement for using this model is that the construct reflects a single dimension or scale and that 
the items that reflect observable indictors of the construct are dichotomously scored. As will be described 
in subsequent sections, the three models for dichotomously scored items are differentiated only by the item 
parameters or item characteristics that are included the model. For more discussion of these models, see 
Chapter 1. 
The examples in this chapter are based on the same data set used in Chapter 3 to describe the general 
template for estimating IRT models using PROC MCMC. This data set comprises responses of 1000 
examinees to 5 multiple-choice items from the Law School Admission Test (LSAT) assessment. It is 
assumed that the SAS data set of item responses (LSAT) has been read into the SAS Work library (see 
Chapter 3 for detail).  In addition, all models in the PROC MCMC code below set D = 1 and reflect a re-
parameterization of the logistic deviate in order to reduce autocorrelation among parameter estimates (see 
Chapter 3 for a discussion). In the PROC MCMC commands, zj = aj (T bj) is re-parameterized into an 

82 Bayesian Analysis of Item Response Theory Models Using SAS 
 
intercept model where zj = aj T  dj and dj is a function of aj and bj or bj = dj / aj. The reader is referred to 
Chapter 3 for an overview on how to use PROC MCMC to estimate IRT models. 
The 1-Parameter IRT or Rasch Model 
In the 1-Parameter (1P) model, the logistic deviate zj = a (T bj) includes estimation of a person parameter 
(Tfor each examinee, one threshold parameter for each jth item (bj), and a common slope parameter for all 
items (a). In the context of educational assessments, the a parameter is referred to as an item discrimination 
parameter and the bj parameters are referred to as item difficulty parameters. By estimating a single slope 
parameter, this model assumes that all items have the same discriminatory power and are therefore related 
equally to the underlying trait or ability being measured. Note that the Rasch (1960) model is equivalent to 
a 1P logistic model with Da = 1 in the logistic deviate. As discussed in Chapter 1, Rasch or 1P models have 
several desirable properties, and may also be preferred for practical reasons (for example, to reduce the 
number of estimated parameters).  
The PROC MCMC code for estimating a unidimensional 1P IRT model is provided in Program 4.1. The 
commands in this program are similar to the commands illustrated in Chapter 3, with some exceptions. In 
addition to a logistic deviate reflecting an intercept model, the MONITOR option specifies that only the a 
and bj parameters from the joint posterior distribution are summarized. The bj parameters rather than the dj 
parameters are more interpretable and therefore of more interest. Secondly, somewhat more diffuse or 
uninformative priors are specified for the item parameters. Finally, the NTHREADS=8 option specifies the 
use of parallel processing in the SAS system. 
Program 4.1: PROC MCMC Code for the 1P Logistic IRT Model 
proc mcmc data=lsat outpost=lsat_bayes seed=23 nbi=5000 nmc=20000 
nthreads=8  monitor=(a b) diagnostics=all plots=(trace autocorr); 
   array b[5]; array d[5]; array p[5];
   parms a 1; parms d: 0;
   prior d: ~ normal(0, var=25);
   prior a ~ lognormal(0, var=4);
   random theta ~ normal(0, var=1) subject=_obs_;
   do j=1 to 5;
      p[j] = logistic(a*theta - d[j]); 
      b[j] = d[j] / a; 
   end; 
   model x1 ~ binary(p1);
   model x2 ~ binary(p2);
   model x3 ~ binary(p3);
   model x4 ~ binary(p4);
   model x5 ~ binary(p5);
run;
 
Outputs 4.1 and 4.2 provide the summary statistics from the posterior distributions for the item parameters 
and convergence diagnostic (trace and autocorrelation) plots. As can be seen, the chain appears to be 
mixing well and exhibits relatively low dependence. The means of the posterior distributions for the 
parameters may be used as point estimates for the parameters and the standard deviations reflect the 
uncertainty associated with parameter estimates. The point estimates for the bj parameters indicate that four 
of the five items reflect easy items (Items 1, 2, 4, and 5), with Items 1 and 5 being very easy items. For 
example, with T values ranging from 2 to 2 for most examinees, examinees with low ability, or T = 2, 

Chapter 4: Bayesian Estimation of Unidimensional IRT Models for Dichotomously Scored Items   83 
 
have a greater than .50 expected probability of answering Items 1 and 5 correctly (T= 2 is > b1 or b5).  
Recall that, for the 1P IRT model, the expected probability of a correct response is .5 when T = bj. 
Output 4.1: Posterior Summary Statistics for the Item Parameters—1P Model 
 
 
 

84 Bayesian Analysis of Item Response Theory Models Using SAS 
 
Output 4.2: Diagnostic Plots for Item Parameters—1P Model 
 
 

Chapter 4: Bayesian Estimation of Unidimensional IRT Models for Dichotomously Scored Items   85 
 
Use of a Normal Ogive Link Function 
Note that a normal ogive function, )(zj), which also models the cumulative probability of a variable, is a 
natural link function to use with IRT models. For )(zj), zj is called a normal deviate and is defined as for 
the logistic link function (zj = Daj (T  bj)). Because the logistic link function is more mathematically 
tractable in methods based on maximum likelihood procedures, the logistic function has received more 
attention in software used to estimate IRT models. However, in a Bayesian analysis, the intractability of 
)(zj) is not an issue. Nonetheless, many of the examples of Bayesian analyses of IRT models still use the 
logistic function in the likelihood expression.
To implement a model with a normal ogive link function, you replace the line of code, P[J] = 
LOGISTIC(A*THETA  D[J]), with P[J] = PROBNORM(A*THETA  D[J]). As an illustration, Output 
4.3 includes the posterior summary statistics for the 1P code in Program 4.1 with a normal ogive link 
function. As you can see, the bj parameters for the different link functions are essentially equivalent. As for 
the a parameter, the difference can be accounted for by the difference in the scaling constants (D) across 
the link functions. Although not specified, implicit in the )(zj) function is a scaling constant D = 1.702. 
With the logistic link function, the scaling constant is typically set to 1.702 or 1.0. When D = 1, the 
difference between the a parameters for the logistic and normal ogive link functions are functionally related 
(.4359 × 1.702 § .7546). Thus, to impose essential equivalence between the two link functions, D should be 
set to 1.702 when using the logistic link function.  
Output 4.3: Posterior Summary Statistics for the Item Parameters—1P Model 
 
The 2-Parameter IRT Model 
The 2-Parameter (2P) extends the 1P model by estimating a separate slope or item discrimination parameter 
for each item (aj); that is, the logistic deviate is zj = aj (T  íbj) or zj = aj T  ídj. By estimating separate 
slope parameters for each item, this model assumes that items have different discriminatory power or have 
different relationships with the underlying trait or ability being measured. The PROC MCMC commands 
for estimating the 2P model are provided in Program 4.2. Note that the commands are similar to the 
commands used to estimate a 1P model, except that now an array for the item discrimination parameters, aj, 
is defined (ARRAY A[5]), and the array elements (A[J]) are referenced in the LOGISTIC function. 
Other differences exist between the code for estimating 2P (Program 4.2) and the code for estimating the 
1P model (Program 4.1). To reduce autocorrelation among the sampled parameters in the 2P model, 
separate PARMS statements for each item to define separate blocks of parameters. Parameters in the same 
block are updated jointly in the sampling process, and any possible correlation between parameters in the 
same block is thereby taken into account (see Chapter 3 for more discussion on blocking of parameters). 

86 Bayesian Analysis of Item Response Theory Models Using SAS 
 
Because the item parameters are expected to be intercorrelated, higher degrees of autocorrelation are 
obtained if the following two PARMS statements are used: PARMS A: 1 and PARMS D: 0.  
Program 4.2 also illustrates the use of truncated prior distributions. For the 1P model (Program 4.1), vague 
or uninformative priors are specified: PRIOR D: ~ NORMAL(0, VAR=25) and PRIOR A: ~ 
LOGNORMAL(0, VAR=4). In Program 4.2, however, truncated priors are specified for the 2P model to set 
reasonable bounds on the parameter space to avoid very extreme and unrealistic sampled values. It is highly 
unlikely that estimates for the aj and bj parameters with real data would fall outside the bounds specified in 
Program 4.2. For example, the proportion of individuals in the LSAT data who answer Item 1 correctly is 
.92; and the proportion who answer Item 5 correctly is .87. These two items would be classified as 
extremely easy, and the bj parameters for these two items are estimated at í3.65 and í2.80 under the 1P 
model with nontruncated prior distributions. Thus, these priors are only slightly more informative than the 
nontruncated priors. But they still reflect diffuse priors or a relatively flat likelihood over very reasonable 
bounds on the item parameter space.   
Notably, although bounds of í6 and 6 were imposed on the dj parameters, bounds were not directly 
imposed on the bj parameters given the parameterization of the model (logistic deviate zj = aj T  dj).  
Because the bj parameters are a function of the aj and bj parameters (bj = dj / aj), bj parameters could be 
sampled beyond the boundaries of 6 to 6 for some sampled aj parameter values, particularly small 
sampled values. Bounds on the bj parameters can be directly imposed by changing the logistic deviate to zj 
= aj (T í bj). Nonetheless, the bounds on the dj parameters impose indirectly reasonable bounds on the item 
parameter space for the bj parameters and avoid very extreme values for the bj parameters.  
Program 4.2: PROC MCMC Code for the 2P Logistic IRT Model 
proc mcmc data=lsat  outpost=lsat_bayes seed=23 nbi=5000 nmc=20000 
nthreads=8 monitor=(a b) diagnostics=all  plots=(trace autocorr); 
   array a[5]; array b[5]; array d[5]; array p[5]; 
   parms a1 1 d1 0; 
   parms a2 1 d2 0; 
   parms a3 1 d3 0; 
   parms a4 1 d4 0; 
   parms a5 1 d5 0; 
   prior d: ~ normal(0, var=25, lower= -6, upper = 6); 
   prior a: ~ lognormal(0, var=4, lower = .2, upper = 5); 
   random theta ~ normal(0, var=1) subject=_obs_; 
   do j=1 to 5; 
      p[j] = logistic(a[j]*theta - d[j]); 
      b[j] = d[j] /a[j]; 
   end; 
   model x1 ~ binary(p1); 
   model x2 ~ binary(p2); 
   model x3 ~ binary(p3); 
   model x4 ~ binary(p4); 
   model x5 ~ binary(p5); 
run;
 
Outputs 4.4 through 4.6 provide the summary statistics from the posterior distributions for the item 
parameters in the 2P IRT model and convergence diagnostic plots. As can be seen, the chain appears to be 
mixing well and exhibits relatively low dependence but more dependence than that observed for the 1P 
model. In examining the posterior means for the item parameters, the bj parameters are similar to the bj 
parameters from the 1P model. As for the aj parameters, there appears to be some variability across the 5 
items, although much of the variability is due to the larger a value for Item 3. However, one significant 
difference between the results for the two models is the increased uncertainty or standard deviations in the 

Chapter 4: Bayesian Estimation of Unidimensional IRT Models for Dichotomously Scored Items   87 
 
posterior distributions.  Standard deviations for the more extreme parameters (b1 and b5) are large even with 
the use of truncated diffuse prior distributions.   
Output 4.4: Posterior Summary Statistics for the Item Parameters—2P Model 
 
 
 

88 Bayesian Analysis of Item Response Theory Models Using SAS 
 
Output 4.5: Diagnostic Plots for Item Parameters—2P Model 
 
 

Chapter 4: Bayesian Estimation of Unidimensional IRT Models for Dichotomously Scored Items   89 
 
Output 4.6: Diagnostic Plots for Item Parameters—2P Model (continued) 
 
 

90 Bayesian Analysis of Item Response Theory Models Using SAS 
 
To further explore the use of truncated distributions for this example, consider an example of trace and 
autocorrelation plots based on nontruncated prior distributions (Output 4.7). For the nontruncated prior 
distributions, the most notable difference in the plots for the parameters based on truncated priors (see 
Output 4.5) is the trace plot for the b1 parameter for Item 1 (a very easy item). For this parameter, numerous 
very extreme sampled values were obtained. As discussed, such values are not observed with real data 
applications. Thus, it is reasonable to place bounds on the prior distributions to stabilize the sampling 
process. It is also interesting to note that the autocorrelation plots indicate slightly lower autocorrelations 
for the sampled parameter values based on nontruncated priors, as opposed to truncated priors.   
Output 4.8 presents the posterior summary statistics for the item parameters based on nontruncated priors.  
In comparison with the posterior summary statistics based on the truncated distributions (see Output 4.4), 
the means are similar, as are the standard deviations, except for the b1 and b5  parameters (Items 1 and 5). 
These parameters are inflated because of extreme values that are sampled. 
Because the truncated normal distribution reflects a relatively flat likelihood over the bounds of í6 and 6, a 
uniform distribution could also be proposed. In order to evaluate the use of a uniform prior over the same 
bounds, the prior, PRIOR D: ~ UNIFORM(í6, 6), was used in place of the truncated normal distribution 
for the bj parameters in Program 4.2. In contrast with the results based on the truncated normal prior 
distributions, several failed convergence diagnostic tests for sampled parameters were observed. Thus, 
priors based on truncated normal distributions with large variance specifications may be preferred over the 
use of uniform distributions. 
Output 4.7: Sample of Trace and Autocorrelation Plots Given NonTruncated Priors—2P Model 
 
 
 

Chapter 4: Bayesian Estimation of Unidimensional IRT Models for Dichotomously Scored Items   91 
 
Output 4.8: Posterior Summary Statistics, Given NonTruncated Priors—2P Model 
 
A 2-Parameter IRT Model with a Hierarchical Prior 
Because the means for the sampled aj parameter values (point estimates) from the 2P model are similar, 
you might reasonably consider a model that reflects a common population for the slope parameters using 
hierarchical prior specifications. In this example, the model is a hybrid 2P model in which the slope or aj 
parameters are considered exchangeable, and a model is estimated that “shrinks” the parameters to a 
common value as in a 1P model (Johnson & Albert, 1999). Thus, the belief is that the aj parameters are 
approximately equal, but you want to model expected variation across items. A hierarchical prior 
specification can be used to estimate a modified 2P IRT model, with hyperparameters reflecting the mean 
and variance for the identical prior distributions for the aj parameters.   
The commands to estimate the hybrid 2P model are provided in Program 4.3. You can see that two 
additional parameters, MU_A and VAR_A, are specified and used in the hierarchical prior specification for 
the aj parameters. In this model, a locally uniform prior is specified for the mean of the slope parameters 
(MU_A) using a truncated normal distribution with a large variance. An inverse gamma distribution is used 
as the prior for the variance of the slope parameters. Inverse gamma distributions are typically used for 
variance parameters. And, in this case, the shape and scale specifications for the inverse gamma 
distribution are similar to those used by Fox (2010) in a similar application. Other than the hierarchical 
prior specification for the aj parameters, the model is the same as the 2P model in Program 4.2.  
 
 

92 Bayesian Analysis of Item Response Theory Models Using SAS 
 
Program 4.3: PROC MCMC Commands for the 2P IRT Model with Hierarchical Priors 
proc mcmc data=lsat outpost=lsat_bayes seed=23 nbi=5000 nmc=20000
   nthreads = 8 monitor=(a b mu_a var_a); 
   array a[5]; array b[5]; array d[5]; array p[5]; 
   parms a1 1 d1 0; 
   parms a2 1 d2 0; 
   parms a3 1 d3 0; 
   parms a4 1 d4 0; 
   parms a5 1 d5 0; 
   parms mu_a 1 var_a 1; 
   prior d: ~ normal(0, var=25, lower = -6, upper = 6); 
   prior a: ~ normal(mu_a, var=var_a, lower=0); 
   prior mu_a ~ normal(1,var=100, lower=0); 
   prior var_a ~ igamma(1, scale=.1); 
   random theta ~ normal(0, var=1) subject=_obs_; 
   do j=1 to 5; 
      p[j] = logistic(a[j]*theta-d[j]); 
      b[j]=d[j]/a[j]; 
   end; 
   model x1 ~ binary(p1); 
   model x2 ~ binary(p2); 
   model x3 ~ binary(p3); 
   model x4 ~ binary(p4); 
   model x5 ~ binary(p5); 
run;
Output 4.9 presents the posterior summary statistics for the item parameters, as well as the hyperparameters 
(mean and variance) for the slope parameters. Outputs 4.10 and 4.11 present the diagnostic plots for only 
the hyperparameters because the other plots were similar to those for the standard 2P model in the previous 
section (see Outputs 4.5 – 4.6).   
Focusing on the hyperparameters in Output 4.9, the mean for the slope parameters (MU_A) is .74, and the 
variance for the slope parameters (VAR_A) is .10. In examining the slope parameters for the 2P model in 
Output 4.4, you can see that the slope parameters are relatively similar, varying from .61  .90. Thus, it 
would be expected that the variance hyperparameter (VAR_A) would be relatively small in the hybrid 2P 
model. In addition, the mean for the five slope parameters in the 2P model is ~.76, which is similar to the 
hyperparameter mean (MU_A) for the hybrid 2P model. It should also be noted that the slope parameters in 
the hybrid 2P model are more similar, ranging from .72  .84, in comparison with those from the 2P model.  
This is a typical result when you are using hierarchical priors. The hierarchical prior “shrinks” the 
parameters to a common value, in this case to the value of .75 from the 1P model. As a result, the standard 
deviations for the slope parameters in the hybrid 2P model are also lower than those in the standard 2P 
model. 
Finally, as Fox (2010) specified, a relatively informative prior for the variance parameter (VAR_A ~ 
IGAMMA(1, SCALE=.1)) was used  rather than a more uninformative prior such as VAR_A ~ 
IGAMMA(1, SCALE=1). When using hierarchical priors, the data are used to estimate the parameters—in 
this case the mean and variance for the set of aj parameters. However, when there is little data, as in this 
case with 5 items, the priors on the mean and variance parameters in particular can overwhelm the data. If a 
more uninformative prior had been used, it would increase the variance estimate substantially, which does 

Chapter 4: Bayesian Estimation of Unidimensional IRT Models for Dichotomously Scored Items   93 
 
not reflect the observed variability in the data. Thus, hierarchical prior specifications should be used 
cautiously when little data is available. 
Output 4.9: Posterior Summary Statistics for the Item Parameters—Hybrid 2P Model 
 
Output 4.10: Diagnostic Plots for the Hyperparameter mu_a—Hybrid 2P Model 
 

94 Bayesian Analysis of Item Response Theory Models Using SAS 
 
Output 4.11: Diagnostic Plots for the Hyperparameter var_a—Hybrid 2P Model 
 
The 3-Parameter IRT Model 
Although the 3-Parameter (3P) model is also used to model responses that are scored dichotomously, the 
model is specifically designed to model testing applications where the probability of a correct response is 
greater than 0 for examinees with low T. For example, consider the ICC in Figure 4.1. In the figure, the 
graph has a lower asymptote that is .20 for T < 2. The primary application for this model involves the use 
of multiple-choice tests where examinees can obtain a correct response by guessing. In contrast, the 1P and 
2P models reflect models in which the lower asymptote of the ICC approaches 0, indicating that examinees 

Chapter 4: Bayesian Estimation of Unidimensional IRT Models for Dichotomously Scored Items   95 
 
with low ability have little chance of correctly answering an item. As such, the lower asymptote parameter 
(cj) is often referred to as a guessing parameter.    
Figure 4.1: Example ICC for a Dichotomously Scored Item (0,1) 
 
This model is also illustrated with the LSAT data because this model would be expected to be consistent 
with the multiple-choice LSAT items. The 3P model includes aj, bj, and cj item parameters to reflect all 
aspects of the shape of the ICC in the figure: lower asymptote (cj), location of the ICC along x-axis or the T 
scale (bj), and slope of the ICC (aj). In this model, the cj parameter is incorporated externally in the logistic 
deviate zj = Daj (T   bj) as follows: 
ܲ௝ሺɅሻൌܲ௝൫ܷ௝ൌͳȁ૑୨ǡ Ʌ൯ൌܿ௝൅ሺͳ െܿ௝ሻ݁௓ೕ
݁௓ೕ 
Note that if cj = 0, then the model defaults to a 2P IRT model. 
The PROC MCMC code to estimate the 3P model extends the code for the 2P model by including the cj 
parameter in the likelihood model and associated prior distribution specifications. However, there are 
several complications to estimating the cj parameters. Data or responses must be available for examinees 
with low T and estimates of cj are more stable for items that are of average difficulty level or more difficult 
and discriminate well (see, for example, Hambleton & Swaminathan, 1985). Consistent with this first 
recommendation, Lord (1980) recommends estimating cj when bj    (2 / aj) > 3 or > 3.5 for samples 
where N > 2,000. However, there is also literature suggesting that cj should be fixed at 1/k where k 
references the number of response choices (Han, 2012). In the context of a Bayesian paradigm, this last 
recommendation suggests that more informative priors for the cj parameters may be useful with means 
equal to 1/k.   
Because problems in estimating the cj parameters may exist for the 3P model, it is often useful to examine 
empirical ICC plots before estimating the model. An empirical ICC uses the possible total scores as a proxy 
for T on the x-axis and plots the mean response for an item on the y-axis. Thus, this plot provides the 

96 Bayesian Analysis of Item Response Theory Models Using SAS 
 
proportion of examinees answering an item correctly at each discrete total score. If for low total scores, 
there appears to be a relatively constant proportion of examinees that approximates 1/k, then there is 
empirical support for the presence of guessing and support for estimating cj parameters. An alternative to 
examining empirical ICC plots is to examine a table of proportions of correct responses or response means 
for each possible total score group for each item by using a program such as Program 4.4. 
Program 4.4: SAS Commands to Obtain Mean Responses for Each Total Score by Item 
proc means data=lsat; 
     class total;  * total created using total=sum(of x1-x5); 
     var x1-x5; 
run;
Table 4.1 presents these proportions for the LSAT data where k = 5 for each item. In this table, the total 
possible score across the five items appears by column heading. The actual result, given as the proportion 
of examinees answering each item correctly for each discrete total possible score, appears by row.  In the 
last row of the table, the numbers of examinees with each total score across the five items are provided. 
From this table you can see that there may be insufficient numbers of low ability examinees to identify the 
presence of guessing, because the number of individuals with low total scores is limited: 3 for Total = 0 and 
20 for Total = 1. Ignoring the proportion correct values for individuals with Total = 0, it would appear that 
individuals are not actively guessing when responding to Items 24 (proportions approximate 0). Although 
the probability of correct answers to Items 1 and 5 are markedly greater than 0, these items also appear to 
be very easy, making it difficult to isolate a lower asymptote or the presence of guessing behavior. Thus, 
estimating the 3P model for the LSAT data could be challenging. 
Table 4.1: Proportion Correct for Each LSAT Test Item by Total Score 
Total Score Possible 
0
1
2
3
4
5
Test Item  
Proportion Correct for Each Total Score Possible 
1 
0 
0.50 
0.73 
0.89 
0.96 
1.0 
2 
0 
0.05 
0.28 
0.46 
0.78 
1.0 
3 
0 
0.05 
0.08 
0.27 
0.52 
1.0 
4 
0 
0.10 
0.33 
0.59 
0.83 
1.0 
5 
0 
0.30 
0.58 
0.79 
0.92 
1.0 
 
Number of Examinees With Each Total Score Possible 
  
3 
20 
85 
237 
357 
298 
 
The PROC MCMC commands used in the illustration to estimate the 3P IRT model are presented in 
Program 4.5. As can be seen, this code is similar to the specifications for the 2P model except for the added 
specifications required for the cj parameters. An initial value of .20 was specified for the cj parameters to 
reflect 5-choice multiple-choice items. In keeping with the suggestion of Han (2012), a more informative 
prior was used that had a mean equal to .20 but also allowed for some variability in the cj parameters 
(PRIOR C: ~ BETA(5,20)).  

Chapter 4: Bayesian Estimation of Unidimensional IRT Models for Dichotomously Scored Items   97 
 
The density of the prior for the cj parameters is plotted in Output 4.12. It should be noted that a more 
diffuse prior was also used to further explore the behavior of the cj parameters (PRIOR C: ~ UNIFORM(0, 
.5)) with a reasonable upper bound of .5. However, use of this diffuse prior resulted in poor mixing for 
some of the item parameters. This should not be surprising given the proportion of correct patterns in Table 
4.1 and the hypothesized difficulty in isolating a lower asymptote parameter. 
Notably, other researchers have suggested that the logit or log-odds for the cj parameters be estimated 
rather than bounded probabilities for the cj parameters (see, for example, Johnson, Sinharay, & Bradlow, 
2007). This reparameterization of the model involves a simple transformation for the cj parameters; when 
the logit(cj) are estimated, a Normal prior for the cj parameters may be used. 
Program 4.5: PROC MCMC Commands for the 3P Logistic IRT Model   
proc mcmc data=lsat outpost=lsat_bayes seed=23 nbi=5000 nmc=20000 
nthreads=8 monitor=(a b c) diagnostics=all plots=(trace autocorr); 
   array b[5]; array a[5]; array d[5]; array c[5]; array p[5]; 
   parms a1 1 d1 0 c1 .20;
   parms a2 1 d2 0 c2 .20; 
   parms a3 1 d3 0 c3 .20; 
   parms a4 1 d4 0 c4 .20; 
   parms a5 1 d5 0 c5 .20; 
   prior a: ~ lognormal(0, var=4, lower=.2, upper = 3);
   prior d: ~ normal(0, var=25, lower= -6, upper = 6); 
   prior c: ~ beta(5,20); 
   random theta ~ normal(0, var=1) subject=_obs_; 
   do k=1 to 5; 
          p[k] = c[k]+(1-c[k])*logistic(a[k]*theta - d[k]); 
          b[k] = d[k]/a[k]; 
   end; 
   model x1 ~ binary(p1); 
   model x2 ~ binary(p2); 
   model x3 ~ binary(p3); 
   model x4 ~ binary(p4); 
   model x5 ~ binary(p5); 
run;

98 Bayesian Analysis of Item Response Theory Models Using SAS 
 
Output 4.12: Density Plot for the Beta(5,20) Prior—c Parameter  
 
Output 4.13 provides the summary statistics from the posterior distributions for the item parameters, and 
Outputs 4.14 through 4.16 display trace and density plots for the item parameters. Trace plots were 
displayed to provide a comparison of mixing behavior for the common item parameters across the models 
(1P, 2P, and 3P IRT models). Although not provided in the diagnostic plots, the autocorrelation plots were 
similar for the aj and bj parameters; for the cj parameters, the autocorrelation plots displayed low 
dependence except for c3. From the trace plots in the output, you can see that the chains appear to be 
mixing well. Note that thinning the chain could be used to reduce the autocorrelation, but this has no effect 
on the posterior summary statistics.   
Finally, the density plots generally exhibit a degree of skewness in sampled parameter values. For example, 
sampled values for the a3 parameters were more positively skewed with more high values than the other aj 
parameters. The bj parameters for the easiest items (Items 1 and 5) exhibited the most negative skew, which 
is not surprising. With regard to the cj parameters, the distributions more closely resembled the shape of the 
prior (see Output 4.10), no doubt because of the relative strength of the prior. Notably, the mean of the 
posterior distribution is commonly used for posterior inference even when the density is not normally 
distributed. It is well known that the posterior mean minimizes the Expected Bayesian Loss or posterior 
risk in using the mean as the parameter estimate under a squared-error loss function. 

Chapter 4: Bayesian Estimation of Unidimensional IRT Models for Dichotomously Scored Items   99 
 
Output 4.13: Posterior Summary Statistics for Item Parameters—3P Model 
 
 
 

100 Bayesian Analysis of Item Response Theory Models Using SAS 
 
Output 4.14: Diagnostic Plots for Item Parameters—3P Model 
 
 

Chapter 4: Bayesian Estimation of Unidimensional IRT Models for Dichotomously Scored Items   101 
 
Output 4.15: Diagnostic Plots for Item Parameters—3P Model (continued) 
 
 

102 Bayesian Analysis of Item Response Theory Models Using SAS 
 
Output 4.16: Diagnostic Plots for Item Parameters—3P Model (continued) 
 
It is also possible to consider the suggestion that cj parameters should not be estimated for very easy items.  
Based on Lord’s criterion (1980), cj parameters for items 1, 4, and 5 should be fixed at 0 based on the aj 
and bj parameter estimates in Output 4.4. This type of partially constrained model can be estimated with the 
use of the BEGINNODATA and ENDNODATA statements to specify constants for parameters that do not 
need to be sampled. The BEGINNODATA and ENDNODATA statements define a set of statements that 
are executed only twice: at the first and the last observation of the data set. The BEGINNODATA and 
ENDNODATA statements are used for computations that are identical for every observation and serve to 
reduce unnecessary observation-level computations in the execution of PROC MCMC.  
The code for this model is provided in Program 4.6. Note that results, which are not reported, are not 
substantially different from those in Output 4.13. The only exceptions are for the specified constants (c=0) 
for Items 1, 4, and 5 and some minor differences for the other item parameter estimates. 
 
 

Chapter 4: Bayesian Estimation of Unidimensional IRT Models for Dichotomously Scored Items   103 
 
Program 4.6: PROC MCMC Commands for a Constrained 3P Model 
proc mcmc data=lsat outpost=lsat_bayes seed=23 nbi=5000 nmc=5000 
nthreads=8 monitor=(a b c) diagnostics=all plots=(trace autocorr); 
   array b[5]; array a[5]; array d[5]; array c[5]; array p[5]; 
   parms a1 1 d1 0; 
   parms a2 1 d2 0 c2 .20; 
   parms a3 1 d3 0 c3 .20; 
   parms a4 1 d4 0; 
   parms a5 1 d5 0; 
   beginnodata; 
   c1=0; 
   c4=0; 
   c5=0; 
   endnodata; 
   prior a: ~ lognormal(0, var=4, lower=.2, upper = 3);
   prior d: ~ normal(0, var=25, lower= -6, upper = 6); 
   prior c2 c3 ~ beta(5,20); 
   random theta ~ normal(0, var=1) subject=_obs_; 
   do k=1 to 5; 
      p[k] = c[k]+(1-c[k])*logistic(a[k]*theta - d[k]); 
      b[k] = d[k]/a[k]; 
   end; 
   model x1 ~ binary(p1); 
   model x2 ~ binary(p2); 
   model x3 ~ binary(p3); 
   model x4 ~ binary(p4); 
   model x5 ~ binary(p5); 
run;
Finally, a hierarchical prior on cj parameters may prove useful with the 3P model (Johnson, Sinharay, & 
Bradlow, 2007). As discussed in Chapter 3, hierarchical priors borrow information across persons or items 
to stabilize the estimation process. Program 4.7 presents an example hierarchical prior for the logit(cj), 
where logit(cj) ~ NORMAL(MU, SIGMA2) and MU ~ NORMAL(1.38, VAR=10) and SIGMA2 ~ 
CHISQUARE(1). The logit(cj) is used to change the metric of the cj parameters to an unbounded scale so 
that a normal prior could be specified with independent parameters (mean and variance parameters). The 
values of í1.38 used for initial values for the cj parameters and in the prior for MU correspond to the 
logit(.2 or 1/k). 
Output 4.17 presents the posterior summary statistics from estimating the model with a hierarchical prior 
for the cj parameters. You can see that the logit of the cj parameters are all ~ 3.5, which corresponds to a 
probability of ~ 0. The other parameters (aj and bj) are similar to those estimated for the 2P model (see 
Output 4.8 and 4.9). This model, therefore, would indicate that a 2P model is sufficient to model the LSAT 
item responses. Notably, the LSAT data set may not be an optimal candidate for a hierarchical prior 
specification on cj parameters, given that there are few items and the items are generally very easy. 
 
 

104 Bayesian Analysis of Item Response Theory Models Using SAS 
 
Program 4.7: PROC MCMC Commands for a Hierarchical Prior on the cj Parameters—3P Model 
proc mcmc data=lsat outpost=lsat_bayes_3p seed=23 nbi=5000 nmc=60000 
monitor=(a b c mu sigma2) nthreads=8 plots=(trace autocorr); 
   array b[5]; array a[5]; array d[5]; array c[5]; array p[5]; 
   parms a1 1 d1 0 c1 -1.38; 
   parms a2 1 d2 0 c2 -1.38; 
   parms a3 1 d3 0 c3 -1.38; 
   parms a4 1 d4 0 c4 -1.38; 
   parms a5 1 d5 0 c5 -1.38; 
   parms mu -1.4; 
   parms sigma2 1 / slice; 
   prior a: ~ lognormal(0, var=4, lower=.2, upper = 5);
   prior d: ~ normal(0, var=25, lower= -6, upper = 6); 
   prior c: ~ normal(mu,var=sigma2); 
   prior mu ~ normal(-1.38, var=10); 
   prior sigma2 ~ chisquare(1); 
   random theta ~ normal(0, var=1) subject=_obs_; 
   do k=1 to 5; 
      p[k] = logistic(c[k])+ logistic(-c[k])*logistic(a[k]*theta - d[k]); 
      b[k] = d[k]/a[k]; 
   end; 
   model x1 ~ binary(p1); 
   model x2 ~ binary(p2); 
   model x3 ~ binary(p3); 
   model x4 ~ binary(p4); 
   model x5 ~ binary(p5); 
run;

Chapter 4: Bayesian Estimation of Unidimensional IRT Models for Dichotomously Scored Items   105 
 
Output 4.17: Posterior Summary Statistics for the 3P Model—Hierarchical Prior on cj Parameters 
 
Comparison of Results Based on MML Estimation   
This section compares results between analyses based on maximum likelihood principles and the Bayesian 
analyses presented and discussed. When you are estimating IRT models with traditional maximum 
likelihood methods (ML), the most common and current method uses the marginal maximum likelihood 
(MML) methods discussed in Chapter 1. To estimate the three different IRT models with the LSAT data, 
the computer program IRTPRO (Cai, Thissen, & du Toit, 2011) was used. The results for the three models 
are presented in Table 4.2. Note that the SAS procedure, PROC IRT, can also be used to compare Bayesian 

106 Bayesian Analysis of Item Response Theory Models Using SAS 
 
and MML estimates for these three models. For the PROC IRT commands, see the SAS documentation 
(SAS, 2014, Chapter 53). 
Table 4.2: MML Item Parameter Estimates for the Three IRT Models 
 
1P Model  
 
2P Model  
 
3P Model  
Test Item  
a 
b 
c 
 
a 
b 
c 
 
a 
b 
c 
1 
0.76 
í3.61 
0 
 
0.83 
í3.36 
0 
 
0.82 
í3.39 
0.00 
2 
0.76 
í1.32 
0 
 
0.72 
í1.37 
0 
 
0.87 
í0.66 
0.23 
3 
0.76 
í0.32 
0 
 
0.89 
í0.28 
0 
 
0.88 
í0.28 
0.00 
4 
0.76 
í1.73 
0 
 
0.69 
í1.87 
0 
 
0.70 
í1.85 
0.00 
5 
0.76 
í2.78 
0 
 
0.66 
í3.12 
0 
 
0.66 
í3.11 
0.00 
 
A few comparisons deserve mention. First, the results for the 1P model are very similar (see Table 4.2 as 
compared with Output 4.1), whereas you will see some differences when comparing the results for the 2P 
model (see Table 4.2 and Output 4.4). While the aj parameter estimates are similar with some values 
slightly greater and some values slightly smaller under the Bayesian analysis, the bj parameter estimates 
from the Bayesian analysis appear to be more extreme or more negative. For example, estimates for b1 = -3.91 
and b5 = -3.75 from the Bayesian analysis, whereas the corresponding MML estimates were 3.36 and 3.12. 
It is possible that these differences were due to the large variance specified in the prior distribution for the 
bj parameters.   
However, more noteworthy differences are observed when the estimates for the 3P model are compared 
(Table 4.2 and Output 4.13). For the 3P model case using IRTPRO, an initial model was specified in which 
all cj parameters were estimated. For this model cj estimates for Items 1, 3, 4, and 5 were estimated at 0, but 
standard errors could not be estimated, resulting in a lack of overall convergence in the solution. A solution 
that converged normally was obtained with the use of a constrained 3P model in which all cj parameters 
were fixed at 0 except for c2. This is the 3P model reported in Table 4.2. Based on MML methods, the 
single estimated c2 was .23 and consistent with the hypothesized value of 1/k. In contrast, all estimated cj 
from the Bayesian analysis were ~.20 because of the prior that was used.   
Output 4.18 presents a scatterplot comparing the T estimates from a 1P model for IRTPRO (MAP Bayesian 
estimates) with the T estimates from PROC MCMC. Because there are 6 possible raw scores, there are 6 
discrete groups of MAP estimates. As you can see, the T estimates for the two analyses both ranged from 

Chapter 4: Bayesian Estimation of Unidimensional IRT Models for Dichotomously Scored Items   107 
 
approximately 2 to .6. The correlation between the theta estimates approximated 1 as would be expected 
given the scatterplot. 
Output 4.18: Scatterplot of Theta Estimates from MULTILOG and PROC MCMC 
 
The three models (1P, 2P, and 3P) that were estimated reflect competing IRT models for the item response 
data. Given competing models, it is important to evaluate or compare the competing models and choose a 
preferred model. In examining descriptively the item parameter estimates for the three models (Table 4.2, 
Output 4.1, Output 4.4, and Output 4.13), the more parsimonious 1P model may be preferred. In both the 
MML and Bayesian analyses, there is little variability in the aj parameter estimates. Furthermore, there is 
little empirical support for estimating cj parameters, even though the items are multiple-choice items. The 
observed numbers of proportion that are correct for low total scores (Table 4.1) are also not consistent with 
the presence of a lower asymptote parameter, except in the case of Items 1 and 5, which are very easy 
items. And, as noted by Lord (1980), cj parameters should not be estimated for very easy items.   
As discussed in Chapter 1, however, a Bayesian analysis also provides indices or criteria for comparing 
competing models statistically: (1) Deviance Information Criteria (DIC; Spiegelhalter, Best, Carlin, & van 
der Linden, 2002), and (2) Indices based on Bayes factors. Rather than consider these methods for this 
specific application, the comparison of the 1P, 2P, and 3P models with the LSAT data are considered in a 
separate chapter in this book on model comparison methods (Chapter 7). 
Display of Item Response Functions 
Plots of item ICCs or plots of expected item response across the latent trait (T) scale are also often 
displayed. These can be obtained by first saving means from the item posterior distributions (that is, item 
parameter point estimates), creating a data set of expected responses across theta for each item, and then 
plotting the expected responses. Program 4.8 provides a template for the set of commands to illustrate this 
process using PROC MEANS, a DATA step to create expected scores across theta, and generating plots 
using PROC TEMPLATE and PROC SGRENDER. The output from executing the program is displayed in 
Output 4.19. Note that the commands must be edited to include LAYOUT OVERLAY / … ENDLAYOUT 
sections for each item as in Program 4.8. (See the SAS Press authors' web page for this program.) Also, the 

108 Bayesian Analysis of Item Response Theory Models Using SAS 
 
commands are easily modified to graph ICCs for other models by simply changing the IRT model used to 
calculate expected scores (P[I]=LOGISTIC(A*(THETA-B[I]))). 
Program 4.8: Graphing ICCs for Dichotomous IRT models—1P Model 
%let nitems=5; 
%let nsubj=1000; 
/* save point estimates for item parameters*/ 
proc means data=lsat_bayes_1p noprint; 
     var a b1-b&nitems;  output out=means_1p  mean=; 
/* create dataset of expected item and test scores across theta */ 
data plotdata_1p; 
  set means_1p; 
  array b{*} b1-b&nitems; 
  array p{*} p1-p&nitems; 
  retain group (0); 
  do theta=-4 to 4 by .5; 
     group=group+1; 
     do i=1 to &nitems; 
        p[i]=logistic(a*(theta-b[i])); /* IRT model-change as required */ 
     end; 
     output; 
  end; 
run;
/* define a graph template for the icc plots using seriesplots */ 
proc template; 
define statgraph iccplots; 
    begingraph; 
      entrytitle "ICCs for the &nitems items – 1P Model"; 
      layout lattice / columns=3 rowgutter=5 columngutter=5; 
      layout overlay /
          xaxisopts=(label="Theta" linearopts=(viewmin=-4 viewmax=4))
          yaxisopts=(label="Item 1" linearopts=(viewmin=0  viewmax=1));
          seriesplot x=theta y=p1; 
      endlayout; 
 
    *...repeat LAYOUT OVERLAY statements for other items; 
      layout overlay /
          xaxisopts=(label="Theta" linearopts=(viewmin=-4 viewmax=4))
          yaxisopts=(label="Item 5" linearopts=(viewmin=0 viewmax=1));
          seriesplot x=theta y=p5; 
      endlayout; 
      endlayout; 
    endgraph; 
end;
/* produce the plots using the template */ 
proc sgrender data=plotdata_1p template=iccplots; 
run;

Chapter 4: Bayesian Estimation of Unidimensional IRT Models for Dichotomously Scored Items   109 
 
Output 4.19: ICC Plots for the Five LSAT Items 
 
Adding Elements to the Graph: Uncertainty in Item Parameters 
Other elements can also be superimposed onto the plots. For example, bands reflecting uncertainty around 
expected scores can be computed based on lower and upper percentile bounds from posterior distributions 
for expected scores. These bands reflect the variation in the expected response across iterations along the T 
scale. Program 4.9 provides the SAS code to add this element. 
You can see that in the program a macro MAKEGTLLIST is used to create the GTL commands (LAYOUT 
OVERLAY blocks) that are used to plot the ICCs and percentile bands for each item. Use of a macro 
facilitates extending the program to any number of items. An example of the LAYOUT OVERLAY block 
for Item 1 generated by the macro is the following:  
layout overlay /
  xaxisopts=(label="Theta" linearopts=(viewmin=-4 viewmax=4))
  yaxisopts=(label="Item 1" linearopts=(viewmin=0 viewmax=1)); 
  bandplot x=theta limitupper=ub1 limitlower=lb1; 
  seriesplot x=theta y=p1; 
endlayout;
 
 

110 Bayesian Analysis of Item Response Theory Models Using SAS 
 
Program 4.9: Adding Uncertainty Bands to ICC Plots for the Five LSAT Items 
%let nitems=5; 
%let nsubj=1000; 
%let model=1p; 
%let lb=p5;  /* lower bound for capturing uncertainty */ 
%let ub=p95; /* upper bound for capturing uncertainty */ 
/* calculate expected scores for each iteration */ 
data calc_prob; 
     set lsat_bayes_1p; 
     array p{*} p1-p&nitems; 
     array b{*} b1-b&nitems; 
     do theta=-4 to 4 by .5; /* interval width of .5 logit intervals */ 
     do i=1 to &nitems; 
       p[i]=logistic(a*(theta-b[i]));/* irt model – change as required */ 
     end; 
     tau=sum(of p1-p&nitems); 
     output; 
     keep a b1-b&nitems p1-p&nitems tau theta; 
     end; 
run;
/* Obtain summary statistics for expected scores across iterations at 
each discrete theta value */ 
proc sort data=calc_prob; by theta; run; 
proc means data=calc_prob noprint; 
     var p1-p&nitems; 
     by theta; 
     output out=stats_1p  mean= &lb=lb1-lb&nitems &ub=ub1-ub&nitems; 
data stats_1p; 
     set stats_1p; 
     group=_n_; 
run;
/*Create gtl commands for nitem icc plots - 2 plots overlaid:
  bandplot- plot upper and lower limits for expected scores
  seriesplot- plot expected score using item parameter point estimates */ 
%macro makegtllist; 
%do i=1 %to &nitems; 
        layout overlay /
          xaxisopts=(label="Theta" linearopts=(viewmin=-4 viewmax=4))
          yaxisopts=(label="Item &i" linearopts=(viewmin=0 viewmax=1)); 
          bandplot x=theta limitupper=ub&i limitlower=lb&i; 
          seriesplot x=theta y=p&i; 
        endlayout; 
%end;
%mend;
proc template; 
  define statgraph iccplots; 
    begingraph; 
      entrytitle ""; 
      layout lattice / columns=3 rowgutter=5 columngutter=5; 
      /*execute macro to include list of commands for set of items*/ 
 
   %makegtllist;  
      endlayout; 
    endgraph; 
  end; 
proc sgrender data=stats_1p template=iccplots; 
run;
The output from executing this program is presented in Output 4.20. You can see from the output that the 
level of uncertainty is not uniform over the T scale. As would be expected, uncertainty is greater at the low 

Chapter 4: Bayesian Estimation of Unidimensional IRT Models for Dichotomously Scored Items   111 
 
end of the T scale because there are so few examinees with low values of T in the LSAT data set. In 
addition, more uncertainty can be observed for Items 1 and 5, in particular, because the bj parameters for 
these items are more extreme and more variability in the sampled values for these parameters was found 
(see Output 4.1). 
Output 4.20: ICC Plots with Bands Reflecting Uncertainty in Expected Response for the Five LSAT 
Items 
 
Adding Elements to the Graph: Observed Results 
The proportion of examinees responding correctly to each item within discrete T groups can also be added 
to the ICCs to descriptively examine model-data-fit. Theta estimates for persons, computed from means 
from posterior T distributions for each person, can be used to “bin” examinees into discrete groups along 
the continuous T scale. Uncertainty around binning persons in each T groups can be expressed by using 
lower and upper percentile bounds from the posterior distributions for T. The observed proportion correct 
values at discrete T groups and expected uncertainty in T estimates within T groups can be plotted with 
model-based expectations. The commands used to add these elements to the ICC plots are included in 
Program 4.10. (See the SAS Press authors’ web page for this program.) The output from executing the 
commands is presented in Output 4.21.   
Across the 5 items, several of the observed proportion-correct values conform reasonably closely with 
model expectations or within expected uncertainty, particularly for 1 < T < 1 where the number of persons 
in a T group are more substantial. While this method is a limited approach to examining model-data-fit, 
Bayesian analysis provides more robust methods for evaluating model-data-fit using posterior predictive 
model checks (PPMC). These methods are discussed briefly in Chapter 2 and discussed in more detail in 
Chapter 8. Note that for both of these graphs (Output 4.20 and 4.21), the bands reflecting uncertainty are 
derived from values for the 5th and 95th percentiles from posterior distributions. In addition, uncertainty in 
both item and T parameters can be reflected in Output 4.21 by combining the methods in the Programs 4.9 
and 4.10. 
 
 

112 Bayesian Analysis of Item Response Theory Models Using SAS 
 
Output 4.21: ICC Plots with Bands Reflecting Uncertainty in T Estimates with T Subgroups 
 

Chapter 5: Bayesian Estimation of Unidimensional 
IRT Models for Polytomously Scored Items 
Introduction ....................................................................................................... 113 
The Graded Response Model ............................................................................. 114 
Program Template for the GR Model ............................................................................... 114 
Options for Specifying Prior Distributions for the Threshold and Intercept 
Parameters .......................................................................................................................... 116 
Estimation of the GR Model by Using Method 1 ............................................................. 118 
Comparison of Separate and Joint Prior Specifications ............................................... 120 
Output from Estimating the GR Model (Method 1 Prior Specification) ........................ 121 
Comparison of the Posterior Densities for the Three Prior Specifications ................. 127 
Computation of Transformations by Post-Processing Posterior Results ................... 128 
Specification of the Likelihood Model with Use of the Table Function........................ 129 
Estimation of the One-Parameter Graded Response Model ......................................... 129 
Muraki’s Rating Scale Model ............................................................................. 131 
Estimating the RS-GR Model ............................................................................................ 131 
Output from Estimating the RS-GR Model ...................................................................... 132 
The Nominal Response Model ........................................................................... 136 
Estimating the NR Model ................................................................................................... 136 
Output from Estimating the NR Model ............................................................................. 137 
The Generalized Partial Credit Model ................................................................ 143 
Estimating the GPC Model ................................................................................................ 144 
Output from Estimating the GPC Model .......................................................................... 146 
Comparison of Results Based on MML Estimation ............................................ 149 
Graphs of Item Category Response Functions ................................................... 151 
Graphs of Test Information Functions ............................................................... 153 
Introduction 
The item response theory (IRT) models discussed in Chapter 4 apply to dichotomously scored items. When 
items are polytomously scored, such as constructed-response items in educational assessments and rating or 
Likert-type response items in health, social, and behavioural measures, polytomous IRT models are 
required. These models similarly represent the relationship between a person’s level on the latent trait being 
measured and the probability of a particular response. But now there are more than two response 
categories. This chapter focuses on polytomous IRT models where items are also assumed to measure one 
underlying latent trait. 
There are various unidimensional polytomous IRT models. The most commonly used models include (1) 
the graded response (GR) model (Samejima, 1969; 1996); (2) a modified GR model (Muraki, 1990), also 
called Muraki’s rating scale (RS) model; (3) the partial credit (PC) model (Masters, 1982); (4) the 
generalized partial credit (GPC) model (Muraki, 1992); and (5) the nominal response (NR) model (Bock, 
1972).  

114  Bayesian Analysis of Item Response Theory Models Using SAS 
This chapter illustrates how to estimate a variety of IRT models for polytomous responses using PROC 
MCMC. Although the models are briefly described in each section, the reader is referred to Chapter 1 for 
more detail. The data used for the illustrations are a subset of cases and items from the DASH survey data 
(Stone & Irrgang, 2004) and comprise responses of 1000 examinees to 10 five-category items. The DASH 
survey was designed to measure physical functioning with items asking about the degree of difficulty, from 
1 (“No Difficulty”) to 5 (“Unable”), in performing different tasks such as opening a jar, writing, making the 
bed, and others. One latent trait was found to underlie the responses to this survey. In order for you to 
execute the code provided in this chapter, it is assumed that that the SAS data set of item responses 
(DASH_DATA) has been read into the SAS Work library. (See the SAS Press authors’ web page for this 
SAS data set.) 
The Graded Response Model 
Samejima’s (1969; 1996) graded response (GR) model is used to model the cumulative probability of 
responding in a particular response category or higher for graded or ordered response scales. Specifically, 
for an item j with mj response categories (1, 2,… mj), the cumulative probability that a person receives a 
category score k (k = 2, … mj) or higher (ܲ௝(௞ିଵ)
כ
(Ʌ)) is modelled by the logistic deviate ݖ௝(௞ିଵ) =
ܦܽ௝൫Ʌ െܾ௝(௞ିଵ)൯, where D is the scaling constant (1.7 or 1), Ʌ references the trait being measured for a 
person, ܽ௝ is the discrimination (slope) parameter for item j, and ܾ௝(௞ିଵ) is a between category threshold 
parameter (k−1 and k) for item j. Given the cumulative response probabilities, the probability of receiving a 
category score k on item j is defined as the difference between the cumulative probabilities for two adjacent 
categories, with two constraints  ܲ௝ଵ
כ (Ʌ) = 1  and  ܲ௝(௠ೕାଵ)
כ
(Ʌ) = 0. Note that within each item, the between 
category threshold parameters ܾ௝௞ are necessarily ordered in the GR model. Also, the response function is 
for an individual person, but the subscript for the person is excluded for convenience in this equation and 
subsequent equations.  
Program Template for the GR Model 
A preliminary template of PROC MCMC code for estimating a GR model using the DASH data is provided 
in Program 5.1. For each 5-category item, one slope parameter (ܽ௝) and four threshold parameters (ܾ௝ଵ, 
ܾ௝ଶ , ܾ௝ଷ , ܾ௝ସ) are estimated. Therefore, for the DASH data and GR model, a total of 10 slope parameters 
(ܽଵ~ܽଵ଴) and 40 threshold parameters (ܾଵ,ଵ~ܾଵ,ସ, …,  ܾଵ଴,ଵ~ܾଵ଴,ସ) are estimated. As for the IRT model 
specifications for dichotomously scored items in Chapter 4, the model is parameterized in a slope intercept 
form where the logisitic  ݖ௝௞= ܽ௝Ʌ െ݀௝௞, D = 1 and ܾ௝௞= ݀௝௞/ܽ௝. These bjk parameters are saved rather 
than the djk parameters using the MONITOR option for the PROC MCMC command because the bjk 
parameters are more directly interpretable.  
The code in the template also presents the set of commands necessary to define the likelihood model for 
IRT models given polytomously scored items. As discussed in Chapter 3, rather than program the response 
probability model and use a MODEL statement to specify the conditional distribution of the data given the 
parameters, you can program the log-likelihood function explicitly using SAS programming statements and 
use the GENERAL option with the MODEL statement to specify the log of the joint distribution for the 
observed item responses. If you have SAS/STAT 13.1 or earlier, this approach is necessary for IRT models 
with polytomously scored items because there is no general categorical distribution function available in 
the MODEL statement beyond the binary distribution specification. As will be shown in a later section, a 
general categorical distribution function is available, starting in SAS/STAT 13.2, that can be used with the 
MODEL statement for polytomously scored items. However, this function is less efficient than the code 
presented in Program 5.1. 
Notably, the program for estimating a GR model assumes that the response categories for each item in the 
data are ordered sequentially from 1 to mj, where mj equals the number of response categories for each item 
j. If the item data do not conform to this assumption, the item responses will need to be transformed prior to 
your using any of the programs in this chapter. 
 

Chapter 5: Bayesian Estimation of Unidimensional IRT Models for Polytomously Scored Items   115 
Program 5.1: Preliminary PROC MCMC Template for Estimating the GR Model for the DASH data 
proc mcmc data=dash_data outpost=dash_post seed=23 nbi=5000 nmc=20000 
nthreads=8 monitor=(a b1_ b2_ b3_ b4_) diagnostics=all plots=(trace 
autocorr); 
 
   array ix[10]; array a[10]; ߑ 
   array b1_[10]; array b2_[10]; array b3_[10]; array b4_[10];  
   array d1_[10]; array d2_[10]; array d3_[10]; array d4_[10];  
   array p_star[10,4]; array p[10,5]; 
    
/* Declare model parameters and initial values */ ޾ 
   parms a1 1  d1_1 -1  d2_1 -0.5  d3_1 0  d4_1 1;  
   parms a2 1  d1_2 -1  d2_2 -0.5  d3_2 0  d4_2 1;  
   parms a3 1  d1_3 -1  d2_3 -0.5  d3_3 0  d4_3 1;  
   parms a4 1  d1_4 -1  d2_4 -0.5  d3_4 0  d4_4 1;  
   parms a5 1  d1_5 -1  d2_5 -0.5  d3_5 0  d4_5 1;  
   parms a6 1  d1_6 -1  d2_6 -0.5  d3_6 0  d4_6 1;  
   parms a7 1  d1_7 -1  d2_7 -0.5  d3_7 0  d4_7 1;  
   parms a8 1  d1_8 -1  d2_8 -0.5  d3_8 0  d4_8 1;  
   parms a9 1  d1_9 -1  d2_9 -0.5  d3_9 0  d4_9 1;  
   parms a10 1 d1_10 -1  d2_10 -0.5  d3_10 0  d4_10 1;  
 
 /* Prior distributions for slope and intercept parameters */  
   prior a: ~ lognormal(0, var=16); ޿       
 
   prior d1_1 ~ normal(<distribution specifications>); ߀ 
   prior d2_1 ~ normal(<distribution specifications>); 
   prior d3_1 ~ normal(<distribution specifications>); 
   prior d4_1 ~ normal(<distribution specifications>); 
 
   [repeat set of prior commands for other item threshold parameter sets] 
  
 
 /* Prior distribution for person ability parameters */ 
   random theta ~ normal(0, var=1)  subject=_obs_; 
 
 /*Specify the log-likelihood function based on the GR model */ 
   llike=0; 
   do j=1 to 10;߁ 
     p_star[j,1]=logistic(a[j]*theta-d1_[j]);  
     p_star[j,2]=logistic(a[j]*theta-d2_[j]); 
     p_star[j,3]=logistic(a[j]*theta-d3_[j]); 
     p_star[j,4]=logistic(a[j]*theta-d4_[j]); 
      
     b1_[j]=d1_[j]/a[j];  
     b2_[j]=d2_[j]/a[j]; 
     b3_[j]=d3_[j]/a[j]; 
     b4_[j]=d4_[j]/a[j]; 
 
     p[j,1]=1-p_star[j,1] ; 
     do k=2 to 4; 
        p[j,k]=p_star[j,(k-1)] - p_star[j,k]; 
     end; 
     p[j,5]=p_star[j,4] ; 
 
     llike=llike+log(p[j,ix[j]]); 
  end; 
 
  model general(llike);߂  
 
run; 


 

116  Bayesian Analysis of Item Response Theory Models Using SAS 
ߑ   Numerous arrays are designed to facilitate reference to different parameters and variables in the model. 
For example, ARRAY b1_ [10] represents the parameters b1_1 to b1_10 or the set of first threshold 
parameters for the 10 items. Similarly, b2_ [10], b3_ [10], and b4_ [10] represent the second, third, and 
fourth sets of threshold parameters for the 10 items. Thus, the item parameters for Item 1 are defined 
by a1, b1_1, b2_1, b3_1, and b4_1 in the program. The arrays P_STAR and P define two dimensional 
arrays with rows indexing the items and columns indexing cumulative response probabilities 
(P_STAR) and category response probabilities (P). 
޾   In order to reduce autocorrelations in sampled parameter values, multiple PARMS statements are used 
to block sets of parameters for each item (see Chapter 3, the section “Parameter Blocking”). In this 
program template, initial values of 1 are given for slope parameters (aj), and the initial values of −1, 
−0.5, 0, and 1 are given for the first, second, third, and fourth intercepts ( ݀௝ଵ, ݀௝ଶ , ݀௝ଷ , ݀௝ସ). It should 
be noted that the initial values of intercepts for this example should also be ordered to reflect the order 
constraint for the thresholds in the GR model: ܾ௝ଵ< ܾ௝ଶ <  ܾ௝ଷ <  ܾ௝ସ. 
޿   The PRIOR statements provide the prior distributions for parameters specified in PARMS statements. 
As for the models in Chapter 4, the prior distributions for the slope parameters (aj) for each DASH 
item are assumed to follow a lognormal distribution with the mean of 0 and the variance of 16. In 
contrast to the priors in Chapter 4, no lower and upper bounds were necessary to define a reasonable 
set of bounds on sampled values. Since all item slopes follow the same distribution, one PRIOR 
statement is used with A: indicating the 10 slopes. 
߀   The prior distribution specifications for the threshold (bjk), or intercept parameters (djk) which are  
functions of the threshold parameters, must conform to the order constraint in the GR model. That is, 
ܾ௝ଵ< ܾ௝ଶ < … <  ܾ௝௞ or ݀௝ଵ< ݀௝ଶ < … <  ݀௝௞. Though the priors for these parameters also typically 
follow normal distributions, one PRIOR statement for all intercepts PRIOR D: ~ NORMAL 
(<distribution specifications>) cannot be used as for the dichotomous IRT models (see Chapter 4). 
Instead, for each item, you have to specify four threshold or intercept parameters in four separate 
PRIOR statements to reflect the order constraint. Thus, there are 40 prior statements for the threshold 
or intercept parameters in total. Three different options for specifying the prior distributions are 
discussed in the next section. 
߁   The DO loop is used to compute cumulative response category probabilities for each item 
(P_STAR[J,1] − P_STAR[J,4]), transform the intercept parameters (djk) to compute the threshold 
parameters (bjk), and use differences between cumulative response probabilities to compute the 
probability of responding in each of the j response categories (P[J,1] − P[J,5]). Note that the response 
probability for response Category 1 (P[J,1]) is the difference between the cumulative probabilities for 
response Category 1 (1) and for response Category 2 (P_STAR[J,1]). The response probability in 
response Category 2 (P[J,2])  is the difference between the cumulative probabilities for response 
Category 2 (P_STAR [J, 1]) and for response Category 3 (P_STAR [J, 2]). The probability associated 
with the last response category (P[J,5]) is equal to  cumulative probability (P_STAR [J, 4]). Lastly, if 
threshold parameters are included in the response probability model, rather than intercept parameters, 
the code calculating the threshold parameters as a function of the slope and intercept parameters would 
be deleted. 
߂   The MODEL statement specifies the log likelihood distribution. There is no need to specify the 
dependent variable in the MODEL statement since the log likelihood function includes the dependent 
variable (IX[J] array). 
Options for Specifying Prior Distributions for the Threshold and Intercept Parameters 
The literature contains different approaches to specifying the prior distributions given the order constraint 
for the threshold or intercept parameters for each item. Three different methods are discussed that reflect an 
ordered constraint between four parameters, analogous to the four threshold parameters for each item in the 
GR model. The code below is presented without reference to the item subscript j to simplify the 
presentation. 
 
 
 

Chapter 5: Bayesian Estimation of Unidimensional IRT Models for Polytomously Scored Items   117 
Method 1(L)—Use the sampled values for bk  or dk as lower bounds on the distributions for bk+1  or dk+1 : 
   parms d1 -1 d2 -.5 d3 .5 d4 1; 
   prior d1 ~ normal(0, var=16); 
   prior d2 ~ normal(0, var=16, lower= d1); 
   prior d3 ~ normal(0, var=16, lower= d2); 
   prior d4 ~ normal(0, var=16, lower= d3); 
Method 2 (L/U)—Use sampled values for bk  or dk as both lower and/or upper bounds on the distributions 
for bk+1  or dk+1 : 
   parms d1 -1 d2 -.5 d3 .5 d4 1; 
   prior d1 ~ normal(0, var=16, upper= d2); 
   prior d2 ~ normal(0, var=16, lower= d1, upper= d3); 
   prior d3 ~ normal(0, var=16, lower= d2, upper= d4); 
   prior d4 ~ normal(0, var=16, lower= d3);    
Method 3 (Sort)—Sort the set of thresholds and use the sorted parameter values in the response probability 
model (Curtis, 2010): 
   parms q1 -1 q2 -.5 q3 .5 q4 1; 
   prior q: ~ n(0, sd=4); 
   d1=q1;  
   d2=q2;  
   d3=q3;  
   d4=q4; 
   call sortn(d1, d2, d3, d4); 
The use of Method 3 in PROC MCMC is complicated by the fact that no transformations can be performed 
directly on model parameters specified in PARMS statements. Thus, a copy of the variables in the PARMS 
statements may be obtained, and these copied variables may be sorted. These variables are then used in the 
response probability model for the GR model to satisfy the order constraint. It should also be noted that a 
direct sampling method cannot be used with the code as presented above for Method 2. Rather, the above 
presentation is used to more easily describe the parameter constraints on the normal distributions. Instead, a 
joint density function must be specified. This is discussed in the next section. 
Output 5.1 presents the prior densities for the four threshold parameters (d1−d4) from the above three 
methods where Method 1 is labeled L for lower bounds, Method 2 is labeled L/U for lower or upper 
bounds, and Method 3 is labeled Sort. These graphs can be produced by modifying the code presented in 
Chapter 3 (Program 3.1), stacking the data sets of sampled values from the distributions, and overlaying 
graphs using DENSITYPLOT graphics statements. 
As you can see, all three prior densities are different despite the same mean and variance specifications for 
the normal distributions. While Methods 2 (L/U) and 3 (Sort) have similar locations, the spreads in the 
densities are quite different. In contrast, you can see that the locations of the densities from Method 1 (L) 
are very different from the locations of the densities for Methods 2 (L/U) and 3 (Sort). Given the 
differences between the prior densities for the three methods, it may be useful to evaluate the use of all 
three methods with the GR model. Use of all three prior distributions and the impact on the posterior 
densities for the threshold parameters is explored further in a later section. Finally, the observation that 
Methods 1 and 2 are different runs counter to the claim by Curtis (2010) that these approaches are 
essentially equivalent.  
 

118 Bayesian Analysis of Item Response Theory Models Using SAS
Output 5.1: Densities of the Four Parameters for the Different Prior Specifications  
 
Estimation of the GR Model by Using Method 1 
Program 5.2 provides code for estimating the GR model. The code in the program is based on the Method 1 
prior specifications (lower bound specifications) because this method is commonly used with the GR 
model. In these programs, changes were made to the template to make the program more concise. First, 
two-dimensional arrays, B[4, 10] and D[4, 10], are used to represent the 40 threshold and 40 intercept 
parameters across the 10 items. Second, a joint prior distribution for the intercept parameters is specified, 
and all intercept parameters are sampled from this distribution. This is described in more detail below. 
Finally, a BEGINNODATA−ENDNODATA block of statements are used. Any statements included within 
the block are executed only at the first and the last observation of the data set. Thus a 
BEGINNODATA−ENDNODATA block of statements reduces unnecessary observational-level 
computations, and is appropriate for computations that are identical across observations, such as 
transformation of parameters. 
Although not presented, Programs 5.3 and 5.4 contain the code for estimating the GR model using the other 
two methods and are included on the SAS Press authors’ web page for this book. As discussed above, given 
the differences observed for the different methods of prior specifications, a comparison of results based on 
the different methods is presented in a later section. 
 

Chapter 5: Bayesian Estimation of Unidimensional IRT Models for Polytomously Scored Items   119 
Program 5.2: PROC MCMC Code for Estimating the GR Model (Method 1 Prior Specification) 
proc mcmc data=dash_data outpost=dash_postGR seed=23 nbi=5000 nmc=20000 
nthreads=8 monitor=(a b) diagnostics=all plots=(trace density); 
   
   array ix[10];  array a[10]; 
   array b[4,10] b1_1-b1_10  b2_1-b2_10 b3_1-b3_10 b4_1-b4_10;  
   array d[4,10] d1_1-d1_10  d2_1-d2_10 d3_1-d3_10 d4_1-d4_10; 
   array p_star[10,4]; array p[10,5]; 
 
   beginnodata; 
   lprior=0; 
   do j=1 to 10; 
      lprior = lprior + lpdfnorm(d[1,j],0,5);ߑ 
      lprior = lprior + lpdfnorm(d[2,j],0,5,d[1,j]); 
      lprior = lprior + lpdfnorm(d[3,j],0,5,d[2,j]); 
      lprior = lprior + lpdfnorm(d[3,j],0,5,d[3,j]); 
   end; 
     
   do j=1 to 10; 
     do k=1 to 4; 
        b[k,j] = d[k,j]/a[j]; ߒ 
     end; 
   end; 
   endnodata; 
    
   parms a1 1  d1_1 -1  d2_1 -0.5  d3_1 0.5  d4_1 1; 
   parms a2 1  d1_2 -1  d2_2 -0.5  d3_2 0.5  d4_2 1; 
   parms a3 1  d1_3 -1  d2_3 -0.5  d3_3 0.5  d4_3 1; 
   parms a4 1  d1_4 -1  d2_4 -0.5  d3_4 0.5  d4_4 1; 
   parms a5 1  d1_5 -1  d2_5 -0.5  d3_5 0.5  d4_5 1; 
   parms a6 1  d1_6 -1  d2_6 -0.5  d3_6 0.5  d4_6 1; 
   parms a7 1  d1_7 -1  d2_7 -0.5  d3_7 0.5  d4_7 1; 
   parms a8 1  d1_8 -1  d2_8 -0.5  d3_8 0.5  d4_8 1; 
   parms a9 1  d1_9 -1  d2_9 -0.5  d3_9 0.5  d4_9 1; 
   parms a10 1  d1_10 -1  d2_10 -0.5  d3_10 0.5  d4_10 1; 
 
   prior a: ~lognormal(0, var=25);ߓ 
   prior d: ~general(lprior); ߔ 
   random theta ~ normal(0, var=1) subject=_obs_; 
    
   llike=0; 
   do j=1 to 10; 
     do k=1 to 4; 
        p_star[j,k]=logistic(a[j]*theta-d[k,j]); ߕ  
     end; 
     p[j,1]=1-p_star[j,1]; 
     do k=2 to 4; 
        p[j,k]=p_star[j,(k-1)]−p_star[j,k]; 
     end; 
     p[j,5]=p_star[j,4]; 
     llike = llike + log(p[j,ix[j]]); 
   end; 
   model general(llike); 
run; 
ߑ   The set of prior specifications for threshold or intercept parameters for each item may be more 
concisely specified with use of a joint density function. In PROC MCMC a joint density function must 
be specified in the log scale. Here, the LPDFNORM is a log-density function for the normal 
distribution. PROC MCMC has a number of internally defined log-density functions for univariate and 
multivariate distributions. These functions have the basic form of LPDFdist(<variable>, <distribution 
parameters>), where ‘dist’ is the name of the distribution, the variable argument references the random 
variable, and distribution arguments are used to specify distribution parameters. For a normal 
 

120  Bayesian Analysis of Item Response Theory Models Using SAS 
distribution, the form is LPDFNORM (variable, mu, SD, lower bounds, upper bounds). The 
specification LPDFNORM(D[2,J], 0, 5, D[1,J]) in the DO loop indicates that the second intercept for 
each item follows a normal distribution with a mean of 0, standard deviation of 5, and a lower bound 
equal to the sampled value for first intercept parameter. The statement LPRIOR=LPRIOR + 
LPDFNORM(<distribution specification> ) computes the sum of the log prior density values across all 
intercept parameters. Therefore, lprior defines the log of the joint prior distribution for the intercept 
parameters across the 10 items. Since these computations are the same across observations, the 
statements are embedded within a BEGINNODATA−ENDNODATA block. The equivalence between 
separate and joint prior specifications is illustrated in the next section. 
ߒ   The transformation function of B[J, K]=D[J, K] / A[J] is included in the BEGINNODATA and 
ENDNODATA block also because the computations are the same across observations. Results are the 
same, assuming the same SEED value, whether the transformations are included in a 
BEGINNODATA and ENDNODATA block or as specified in Program 5.1. 
ߓ   No bounds are used to restrict the parameter space for estimating the ܽ௝ parameters or the ݀௝௞ 
parameters other than bounds required to satisfy the order constraint. As you will see in the posterior 
summary output and trace plots, the sampled values did not include unreasonable values. 
ߔ   To declare the prior distributions for the intercept parameters (djk), one PRIOR statement is required: 
PRIOR D: ~ GENERAL (LPRIOR). The GENERAL function indicates that the intercepts are sampled 
from a joint prior distribution with the joint distribution lprior. Notably, any new distributions used 
with the GENERAL function, either in the PRIOR statement or in the MODEL statement, have to be 
specified on the logarithm scale. 
ߕ   The use of two-dimensional arrays simplifies the programming statements used to compute the 
cumulative probabilities. As shown in the program, one statement: P_STAR [J, K] = LOGISTIC 
(A[J]*THETA í D[K, J]) is used for computing all of the required cumulative probabilities. 
Comparison of Separate and Joint Prior Specifications 
Program 5.5 presents code to test the equivalence between separate and joint prior specifications for 
Method 1. Output 5.2 compares the densities for the two sets of parameters. As you can see, there is 
essentially complete overlap between the densities for the four parameters (d1− d4), indicating equivalence 
between the approaches. Any trivial differences are because direct sampling is used to sample values in the 
separate prior specifications, whereas the metropolis sampler is used to sample values in the joint prior 
specifications. 
Program 5.5: Comparing Separate and Joint Prior Specifications—Method 1 
proc mcmc data=a nmc=100000 seed=1 outpost=o1 plots=(density); 
   parm d1 -1 d2 -.5 d3 .5 d4 1; 
   prior d1 ~ n(0, sd=5); 
   prior d2 ~ n(0, sd=5, lower=d1); 
   prior d3 ~ n(0, sd=5, lower=d2); 
   prior d4 ~ n(0, sd=5, lower=d3); 
   model general(0); 
   run; 
  
proc mcmc data=a outpost=o2 seed=23 nbi=1000 nmc=100000 plots=(density); 
   parms d1 -1 d2 -.5 d3 .5 d4 1; 
   beginnodata; 
       lprior = lpdfnorm(d1,0,5); 
       lprior = lprior + lpdfnorm(d2,0,5,d1); 
 
lprior = lprior + lpdfnorm(d3,0,5,d2); 
 
lprior = lprior + lpdfnorm(d4,0,5,d3); 
   endnodata; 
   prior d: ~general(lprior); 
   model general(0); 
run; 
 

Chapter 5: Bayesian Estimation of Unidimensional IRT Models for Polytomously Scored Items 121
Output 5.2: Densities of the Four Parameters for Separate and Joint Prior Specifications 
 
Output from Estimating the GR Model (Method 1 Prior Specification)
Output 5.3 presents selected posterior summary results from executing Program 5.2. These results were 
obtained using the autocall macro %POSTSUM to select a subset of parameters (see Chapter 3 for a 
discussion of the autocall macros that can be used with PROC MCMC OUTPOST data sets). As a result, 
the summary statistics differ from the default output produced by PROC MCMC. All slope parameter 
results are displayed along with a subset of threshold parameters due to the large number of threshold 
parameters in the model (40). These particular items (1, 7, and 10) were selected because they reflect 
measurement of the trait at varying locations on the θ scale. Point estimates for all parameters are available 
in a later section that compares Bayesian and marginal maximum likelihood (MML) parameter estimates. 
As you can see in the output, the slope point estimates (means) range from 1.83 to 3.49 indicating that the 
DASH items are highly discriminating. The threshold estimates for the selected items reflect the trend for 
all the items and do not spread out over the entire latent trait scale. A majority of the thresholds estimates 
are larger than 0, indicating that the subset of DASH items is measuring the trait at the upper end of the 
trait scale. Given the order constraint for the threshold parameters, you can see that the four thresholds 
within each item are indeed ordered. In addition, the standard deviations (SDs) of the posterior distributions 
for the parameters are quite small, indicating that all the slope and threshold parameter estimates were 
estimated with relatively high precision. 

122 Bayesian Analysis of Item Response Theory Models Using SAS
Output 5.3: Selected Posterior Summary Results—GR Model 
Because of the volume of diagnostic plots that are available from this analysis, Outputs 5.4 to 5.12 present 
selected trace, autocorrelation, and density plots for item parameters in the GR model. These results were 
obtained using the autocall macro %TADPLOT to select the subset of parameters. You can see from the 
plots for the selected slope parameters (Outputs 5.4 – 5.6) that the parameters appear to be mixing well or 
traversing the parameter space quickly and have reasonably low autocorrelations. This is in contrast to the 
diagnostic plots for the selected threshold parameters (Outputs 5.7–5.12). You can see that the chains for 
the threshold parameters do not mix as well as the chains for the slope parameters. This results in 
somewhat higher autocorrelations and therefore higher dependence between sampled values. But as for 
other models, this higher dependence does not affect posterior summary results. 

Chapter 5: Bayesian Estimation of Unidimensional IRT Models for Polytomously Scored Items 123
Output 5.4: Selected Diagnostics Plots for Slope Parameters—GR Model (Item 1) 
 
Output 5.5: Selected Diagnostics Plots for Slope Parameters—GR Model (Item 7)
 

124 Bayesian Analysis of Item Response Theory Models Using SAS
Output 5.6: Selected Diagnostics Plots for Slope Parameters—GR Model (Item 10) 
 
Output 5.7: Selected Diagnostics Plots for Threshold Parameters—GR Model (Item 1) 
 

Chapter 5: Bayesian Estimation of Unidimensional IRT Models for Polytomously Scored Items 125
Output 5.8: Selected Diagnostics Plots for Threshold Parameters—GR Model (Item 1) 
 
Output 5.9: Selected Diagnostics Plots for Threshold Parameters—GR Model (Item 7) 
 

126 Bayesian Analysis of Item Response Theory Models Using SAS
Output 5.10: Selected Diagnostics Plots for Threshold Parameters—GR Model (Item 7) 
 
Output 5.11: Selected Diagnostics Plots for Threshold Parameters—GR Model (Item 10) 
 

Chapter 5: Bayesian Estimation of Unidimensional IRT Models for Polytomously Scored Items 127
Output 5.12: Selected Diagnostics Plots for Threshold Parameters—GR Model (Item 10) 
 
Comparison of the Posterior Densities for the Three Prior Specifications 
Given the differences observed in the prior densities for the threshold parameters (see Output 5.1), you can 
examine any subsequent impact on the posterior densities for the parameters. Output 5.13 presents the 
posterior densities for the threshold parameters values for Item 1 under the three different prior density 
specifications. You can see that very small differences are observed between the methods using lower 
bounds (Lower) and the method using sorted values (Sort). However, there are some modest differences for 
several of the threshold parameters under the method using lower and upper bounds on the distributions 
(Up/Low). In particular, for the thresholds b1, b2, and b3, the densities are shifted slightly higher than those 
based on the other two methods. In terms of comparing the first two moments of the distributions, the 
moments only differed in the hundredths place. So essentially, the point estimates for the parameters and 
parameter standard errors were essentially identical across the three methods.  
Thus, estimation of the GR model for the DASH data set was not sensitive to the method used to specify 
the prior distributions for the threshold parameters. This is likely due to the relatively large amount of data 
in the DASH data set which “overwhelmed” the priors. Of course, since real data is analysed, it is not clear 
which density is closer to true values. Therefore more exploration of these different prior specifications 
may be needed.  

128 Bayesian Analysis of Item Response Theory Models Using SAS
Output 5.13: Comparison of Posterior Densities for the Four Threshold Parameters (Item 1) 
 
Computation of Transformations by Post-Processing Posterior Results 
Any transformations of parameter estimates may be obtained by post-processing the data set of posterior 
results saved using the OUTPOST option on the PROC MCMC command. For example, rather than 
compute the threshold parameters (bjk) using the intercept parameter (djk) and slope parameters (ajk) in the 
PROC MCMC program (see program 5.2), the OUTPOST data set can be processed. Assuming the 
transformation is excluded from Program 5.2, the following DATA step and PROC MEANS command 
may be used to compute the values of bjk for each iteration, and compute point estimates for the bjk 
parameters from means of the transformed posterior samples for the bjk parameters. The ease in post-
processing of the posterior samples illustrates the flexibility of using PROC MCMC for Bayesian analyses. 
data dash_transformed; 
set dash_postGR; 
array a[10] a1-a10; 
array d[4,10] d1_1-d1_10 d2_1-d2_10 d3_1-d3_10 d4_1-d4_10; 
array b[4,10] b1_1-b1_10 b2_1-b2_10 b3_1-b3_10 b4_1-b4_10; 
do j=1 to 10; 
 do k=1 to 4; 
   b[k,j]=d[k,j]/a[j];   
 end; 
end; 
run; 
proc means data=dash_transformed; 
var a1-a10  d1_1-d1_10 d2_1-d2_10 d3_1-d3_10 d4_1-d4_10 b1_1-b1_10  
b2_1-b2_10 b3_1-b3_10 b4_1-b4_10 ; 
run; 

Chapter 5: Bayesian Estimation of Unidimensional IRT Models for Polytomously Scored Items   129 
Specification of the Likelihood Model with Use of the Table Function 
Beginning with SAS/STAT 13.2, the likelihood model for items with more than two response categories 
can be specified using the TABLE function. The TABLE function supports the specification of a 
categorical distribution with an array of probabilities for the discrete values. As an example, the code for 
using this function with a 5-category item (Item 1 from the DASH data set) is presented below.  
array p_star[4]; 
array p[5]; 
do k=1 to 4; 
   p_star[k]=logistic(a[&j]*theta-d[k,1]);   
end; 
p[1]=1-p_star[1]; 
do k=2 to 4; 
   p[k]=p_star[(k-1)]-p_star[k]; 
end; 
p[5]=p_star[4];      
model ix1 ~ table(p); 
The TABLE function requires a vector of probabilities. A two-dimensional array of probabilities, indexed 
by item and response option categories, cannot be specified. This makes the use of the TABLE function 
cumbersome when estimating IRT models because the code must be repeated for each item separately 
rather than including the code within a DO LOOP across the items. Thus, you may find it easier to program 
the log-likelihood function explicitly as in Program 5.2. However, the macro presented below can be used 
to replicate the above code for each of the items in your data set. 
array p_star[4]; 
array p[5]; 
%macro prob_gr; 
   %do j=1 %to 10; 
     do k=1 to 4; 
        p_star[k]=logistic(a[&j]*theta-d[k,&j]);   
     end; 
     p[1]=1-p_star[1]; 
     do k=2 to 4; 
        p[k]=p_star[(k-1)]-p_star[k]; 
     end; 
     p[5]=p_star[4];      
     model ix&j ~ table(p); 
   %end;  
%mend; 
%prob_gr; 
Estimation of the One-Parameter Graded Response Model 
For the GR model, one slope parameter and four threshold parameters are estimated for each item. Under 
the assumption that all items have a common slope parameter, the GR model reduces to a one-parameter 
GR model with the logistic deviate  ݖ௝௞= ܦܽ൫Ʌ െܾ௝௞൯ ݋ݎ ݖ௝௞= ܦ൫ܽɅ െ݀௝௞൯. Therefore, the one-
parameter GR model can be treated as a restricted case of the GR model. These two models can be 
compared using Bayesian model comparison methods (see Chapter 7) to determine which model is 
preferred in a particular testing application.  
The PROC MCMC commands used to estimate the one-parameter GR model are provided in Program 5.6. 
You can see that the commands are similar to the commands used to estimate a GR model in Program 5.2, 
except that the slope parameter (a) is a single variable reflecting a common slope parameter across items. 
Thus, a single variable is specified in a PARMS statement for the parameter, PARMS A 1, in the prior 
distribution for the slope parameter, PRIOR A ~ LOGNORMAL(0, VAR=25), and in the response 
probability model, P_STAR[J,K]=LOGISTIC(A*THETA-D[K,J]). 
No output is provided for this model. But it can be estimated and compared with the GR model to 
determine whether slope parameters for each item should be estimated. As it turns out, for the DASH data 
set, the point estimate (mean of posterior values) for the slope parameter a is 2.63, which is similar to the 
 

130  Bayesian Analysis of Item Response Theory Models Using SAS 
mean value of the slope estimates from the GR model. However, the ܽ௝௞ parameter estimates from the GR 
model varied a good deal (see Output 5.3) with a range from 1.82 to 3.51. This would suggest that a GR 
model would be more appropriate for the DASH item responses. The model comparison methods discussed 
in Chapter 7 would, however, be more useful in determining the appropriateness of the one-parameter GR 
model to the DASH data. As for the threshold bjk parameters, most of the values were similar to the values 
from the GR model.  
Program 5.6: PROC MCMC Code for Estimating the One-Parameter GR Model  
proc mcmc data=dash_data outpost=dash_post1PGR seed=23 nbi=5000 nmc=20000 
nthreads=8 monitor=(a b) diagnostics=all plots=(trace autocorr); 
 
   array ix[10];   
   array b[4,10] b1_1-b1_10  b2_1-b2_10 b3_1-b3_10 b4_1-b4_10; 
   array d[4,10] d1_1-d1_10  d2_1-d2_10 d3_1-d3_10 d4_1-d4_10; 
   array p_star[10,4]; array p[10,5]; 
 
   beginnodata; 
   lprior=0; 
   do j=1 to 10; 
     lprior = lprior + lpdfnorm(d[1,j],0,5); 
     do k=2 to 4; 
        lprior = lprior + lpdfnorm(d[k,j],0,5,d[k-1,j]); 
     end; 
   end; 
     
   do j=1 to 10; 
     do k=1 to 4; 
        b[k,j] = d[k,j]/a; 
     end; 
   end; 
   endnodata; 
 
   parms  a  1; 
   parms  d1_1 -1  d2_1 -0.5  d3_1 0  d4_1 1; 
   parms  d1_2 -1  d2_2 -0.5  d3_2 0  d4_2 1; 
   parms  d1_3 -1  d2_3 -0.5  d3_3 0  d4_3 1; 
   parms  d1_4 -1  d2_4 -0.5  d3_4 0  d4_4 1; 
   parms  d1_5 -1  d2_5 -0.5  d3_5 0  d4_5 1; 
   parms  d1_6 -1  d2_6 -0.5  d3_6 0  d4_6 1; 
   parms  d1_7 -1  d2_7 -0.5  d3_7 0  d4_7 1; 
   parms  d1_8 -1  d2_8 -0.5  d3_8 0  d4_8 1; 
   parms  d1_9 -1  d2_9 -0.5  d3_9 0  d4_9 1; 
   parms  d1_10 -1  d2_10 -0.5  d3_10 0  d4_10 1; 
 
   prior a ~lognormal(0, var=25); 
   prior d: ~general(lprior); 
   random theta ~ normal(0, var=1) subject=_obs_; 
 
   llike=0; 
   do j=1 to 10; 
     do k=1 to 4; 
        p_star[j,k]=logistic(a*theta-d[k,j]); 
     end; 
     p[j,1]=1-p_star[j,1]; 
     do k=2 to 4; 
        p[j,k]=p_star[j,(k-1)]-p_star[j,k]; 
     end; 
     p[j,5]=p_star[j,4]; 
     llike = llike + log(p[j,ix[j]]); 
   end; 
   model general(llike); 
  run; 
 

Chapter 5: Bayesian Estimation of Unidimensional IRT Models for Polytomously Scored Items   131 
Muraki’s Rating Scale Model 
Another restricted version of the GR model is Muraki’s (1990) rating scale graded (RS-GR) model. In this 
model, the logistic deviate is  ݖ௝௞= ܦܽ௝൫Ʌ െ(ܾ௝+ ܿ௞)൯. The threshold parameters (ܾ௝௞) in the GR model 
are decomposed into two terms: (1) a location parameter (ܾ௝) for each item, and (2) one set of category 
threshold parameters (ܿ௞) that applies to all items. To identify the model, σ ܿ௞ across the k response options 
for each item are constrained to be 0. Also note that the thresholds ܿ௞ are ordered similarly to the ܾ௝௞ 
parameters in the GR model. One advantage of this parameterization is that the ܾ௝ parameters can be used 
to directly compare where different items are measuring on the latent θ continuum. Another advantage is 
that the number of parameters in the model is reduced significantly as compared to the GR model. 
The RS model is a restricted version of the GR model because the category threshold parameters, ܿ௞, are 
assumed to be equal across all items in the model, whereas they are free to vary across items in the GR 
model. Thus, the RS-GR model can be compared with the GR model to determine whether one set of 
category threshold parameters adequately describes the item responses. This model comparison is 
illustrated with the DASH data set in Chapter 7.  
Estimating the RS-GR Model  
Program 5.7 includes the PROC MCMC code for estimating the RS-GR model with the DASH data. One 
slope parameter (ܽ௝) and one location parameter (ܾ௝) are estimated for each DASH item, and there is one 
set of four category threshold parameters (ܿଵ, ܿଶ, ܿଷ, ܿସ) for the five response categories: 1 (“No Difficulty”) 
to 5 (“Unable”). The category thresholds parameters (ܿ௝) are declared in a separate PARMS statement ߑ. 
Notably, only three thresholds ܿଶ, ܿଷ, and ܿସ are defined in the PARMS statement. The fourth threshold 
(ܿଵ)  is obtained using ܿଵ= െ(ܿଶ+ ܿଷ+ ܿସ). This computation is embedded in a BEGINNODATA − 
ENDNODATA block and is used to satisfy the model identification constraint σ ܿ௞= 0 ߒ.The lower 
value of ܿଷ is specified as ܿଶ, and the lower value of ܿସ is specified as ܿଷ to also satisfy the order constraint 
ߓ. The code used to compute the log-likelihood function is the same as for the GR model except for the 
response model ߔ. 
Note that the same strategy of decomposing the threshold parameter into location (ܾ௝) and category 
threshold parameters (ܿ௝ଵ, ܿ௝ଶ, ܿ௝ଷ, ܿ௝ସ) can be done for each item in the GR model. This is illustrated in 
Program 5.8, which can be found on the SAS Press authors’ web page. 
 
 
 

132  Bayesian Analysis of Item Response Theory Models Using SAS 
Program 5.7: PROC MCMC Code for Estimating the RS-GR Model 
proc mcmc data=dash_data outpost=dash_postRSGR seed=23 nbi=5000 nmc=20000 
nthreads=8 monitor=(a b c) diagnostics=all plots=(trace autocorr); 
   
   array ix[10];   
   array a[10]; array b[10]; array c[4]; 
   array p_star[10,4]; array p[10,5]; 
 
   parms a1 1  b1 0; 
   parms a2 1  b2 0; 
   parms a3 1  b3 0; 
   parms a4 1  b4 0; 
   parms a5 1  b5 0; 
   parms a6 1  b6 0; 
   parms a7 1  b7 0; 
   parms a8 1  b8 0; 
   parms a9 1  b9 0; 
   parms a10 1  b10 0; 
   parms c2 -2  c3 0  c4 6;ߑ 
 
   beginnodata; 
     c1=-(c2+c3+c4);ߒ 
   endnodata; 
 
   prior a: ~ lognormal(0, var=25); 
   prior b: ~ normal(0, var=25); 
   prior c2 ~ normal(0, var=25); 
   prior c3 ~ normal(0, var=25, lower=c2);ߓ 
   prior c4 ~ normal(0, var=25, lower=c3); 
 
   random theta ~ normal(0, var=1) subject=_obs_; 
 
   llike=0; 
   do j=1 to 10; 
      do k=1 to 4; 
         p_star[j,k]=logistic(a[j]*(theta-(b[j]+c[k])));ߔ 
      end; 
      p[j,1]=1-p_star[j,1]; 
      do k=2 to 4; 
         p[j,k]=p_star[j,(k-1)]-p_star[j,k]; 
      end; 
      p[j,5]=p_star[j,4]; 
      llike = llike + log(p[j,ix[j]]); 
   end; 
   model general(llike); 
 run; 
Output from Estimating the RS-GR Model  
Output 5.14 shows the summary statistics for item parameters. Compared to the results for the GR model 
(see Output 5.3), the item slope estimates based on two models are similar. For each item, the item location 
parameter in the RS model approximates the average of the four thresholds in the GR model. The four 
category threshold parameter estimates for all items are ordered as expected. Outputs 5.15 to 5.18 provide 
the trace and autocorrelation plots for selected item parameters. You can see that the chains are mixing very 
well for the slope parameters with low autocorrelations. The location and category threshold parameters, on 
the other hand, exhibit reasonable mixing but a moderate degree of autocorrelation. To reduce 
autocorrelations in other models that have been discussed, the models where ݖ௝௞= ܦܽ௝൫Ʌ െܾ௝௞൯ were 
reparameterized into ݖ௝௞= ܦܽ௝൫Ʌ െ݀௝௞൯ where ܾ௝௞= ݀௝௞/ܽ௝. However, the RS-GR model is more difficult 
to reparameterize into a slope intercept model since the ܿ௞ parameters are not a function of each item. 
 

Chapter 5: Bayesian Estimation of Unidimensional IRT Models for Polytomously Scored Items 133
Output 5.14: Posterior Summary Statistics for the Item Parameters—RS-GR Model 
 
 

134 Bayesian Analysis of Item Response Theory Models Using SAS
Output 5.15: Trace and Autocorrelation Plots for Item Parameters (a1, a2, a3)—RS-GR Model 
Output 5.16: Trace and Autocorrelation Plots for Item Parameters (a10, b1, b2)—RS-GR Model 

Chapter 5: Bayesian Estimation of Unidimensional IRT Models for Polytomously Scored Items 135
Output 5.17: Trace and Autocorrelation Plots for Item Parameters (b9, b10, c1)—RS-GR Model 
 
Output 5.18: Trace and Autocorrelation Plots for Item Parameters (c2, c3, c4)—RS-GR Model 

136  Bayesian Analysis of Item Response Theory Models Using SAS 
The Nominal Response Model 
Bock’s (1972) nominal response (NR) model is the most general model for polytomously scored items and 
is used to model item responses when response categories are not necessarily ordered along the trait (θ) 
scale. In the NR model, the probability of an examinee with trait level θ responding in the ݇௧௛ response 
option, for item j with ݉௝ response options (or categories), is given by the following:   
ܲ௝௞(Ʌ) =
݁௭ೕೖ
σ
݁௭ೕೖ
௠ೕ
௞ୀଵ
          
for 
݇= 1, … . , ݉௝, 
where ݖ௝௞ = ܽ௝௞Ʌ + ܿ௝௞ is a multivariate logit for the kth option in item j, ܽ௝௞ is the option discrimination 
parameter for item j and category ݇, and ܿ௝௞ is called the option extremity parameter for item j and category 
݇. As discussed in Chapter 1, to identify the model, the constraint σ ܽ௝௞= σ ܿ௝௞= 0 across k or the 
constraint ܽ௝ଵ= ܿ௝ଵ= 0 for each item is typically specified. 
Estimating the NR Model  
Program 5.9 includes the PROC MCMC code for estimating the NR model with the DASH data set. For 
each DASH item, there are five slope parameters (ܽ௝ଵ, ܽ௝ଶ, ܽ௝ଷ, ܽ௝ସ, ܽ௝ହ) and five intercept parameters 
(ܿ௝ଵ, ܿ௝ଶ, ܿ௝ଷ, ܿ௝ସ, ܿ௝ହ) corresponding to five response categories. However, for model identification purposes, 
the first slope and intercept for each item are constrained to be 0 in a BEGINNODATA−ENDNODATA 
block. Two-dimensional arrays are used for the item parameters where the first index corresponds to the 
item number and the second index corresponds to the category response number. The multivariate logits 
and response probabilities for the 5 options are captured in the Z[5] and P[5] arrays. The item parameters 
within each item are treated as a block, and their initial values are set to be 0. The priors for slopes and 
intercepts are declared as unbounded normal distributions with mean of 0 and variance of 25 ߑ. To obtain 
the likelihood of the data, the multivariate logits for the second to fifth response options (z2 to z5) are 
calculated first ޾. Note that the logit for the first option (z1) is 1 since its slope and intercept are both 0. In 
the next step, the response probabilities for 5 options are computed ޿. 
 
 
 

Chapter 5: Bayesian Estimation of Unidimensional IRT Models for Polytomously Scored Items   137 
Program 5.9: PROC MCMC Code for Estimating the NR Model 
proc mcmc data=dash_data outpost=dash_postNR seed=23 nbi=5000 nmc=20000 
monitor=(a c) diagnostics=all plots=all; 
 
  array a[10,5] a1_1-a1_5 a2_1-a2_5 a3_1-a3_5 a4_1-a4_5 a5_1-a5_5 
                a6_1-a6_5 a7_1-a7_5 a8_1-a8_5 a9_1-a9_5 a10_1-a10_5;  
  array c[10,5] c1_1-c1_5 c2_1-c2_5 c3_1-c3_5 c4_1-c4_5 c5_1-c5_5 
                c6_1-c6_5 c7_1-c7_5 c8_1-c8_5 c9_1-c9_5 c10_1-c10_5;  
  array ix[10]; array z[5]; array p[5]; 
  parms a1_2-a1_5 c1_2-c1_5 0; 
  parms a2_2-a2_5 c2_2-c2_5 0; 
  parms a3_2-a3_5 c3_2-c3_5 0;  
  parms a4_2-a4_5 c4_2-c4_5 0; 
  parms a5_2-a5_5 c5_2-c5_5 0; 
  parms a6_2-a6_5 c6_2-c6_5 0; 
  parms a7_2-a7_5 c7_2-c7_5 0; 
  parms a8_2-a8_5 c8_2-c8_5 0; 
  parms a9_2-a9_5 c9_2-c9_5 0; 
  parms a10_2-a10_5 c10_2-c10_5 0; 
    
  beginnodata; 
  a1_1=0;a2_1=0;a3_1=0;a4_1=0;a5_1=0;a6_1=0;a7_1=0;a8_1=0;a9_1=0;a10_1=0; 
  c1_1=0;c2_1=0;c3_1=0;c4_1=0;c5_1=0;c6_1=0;c7_1=0;c8_1=0;c9_1=0;c10_1=0; 
  endnodata; 
 
  prior c1_2-c1_5 c2_2-c2_5 c3_2-c3_5 c4_2-c4_5 c5_2-c5_5 
        c6_2-c6_5 c7_2-c7_5 c8_2-c8_5 c9_2-c9_5 c10_2-c10_5  
        ~ normal(0, var=25);ߑ   
  prior a1_2-a1_5 a2_2-a2_5 a3_2-a3_5 a4_2-a4_5 a5_2-a5_5 
        a6_2-a6_5 a7_2-a7_5 a8_2-a8_5 a9_2-a9_5 a10_2-a10_5  
        ~ normal(0, var=25); 
  random theta ~ normal(0, var=1) subject=_obs_; 
  llike=0; 
  do j=1 to 10; 
     do k=2 to 5;޾ 
        z[k]=exp(a[j,k]*theta + c[j,k]); 
     end; 
     do k=2 to 5;޿ 
        p[k]=z[k]/(1 + sum(of z2-z5)); 
     end; 
     p[1]=1 - sum(of p2-p5); * calc prob for category 1; 
     llike = llike + log(p[ix[j]]); 
  end; 
  model general(llike); 
  run;    
Output from Estimating the NR Model  
Output 5.19 shows the summary statistics for selected item parameters. Selected results for Items 1, 7, and 
10 are again reported as these items reflect items with varying item properties. The autocall macro for 
regenerating results, %POSTSUM, was used, and, as in previous tables, the results from this macro differ 
from the default summary statistics produced by PROC MCMC. Note that all item parameter estimates are 
available in the section comparing Bayesian and MML results.  
As you can see in the table, the ܽ௝௞ parameters across response categories for an item are ordered. Given 
that the ܽ௝௞ parameters reflect the slope of the linear regression of the response process (z) on θ for each 
response option k, increasing values of the ܽ௝௞ parameters across response categories reflect an increasing 
probability of selecting option k+1 over option k. This in turn indicates that the scale of response options is 
ordered. Therefore, an ordered scale is indicated for the DASH items as would be expected.  
 
 
 

138 Bayesian Analysis of Item Response Theory Models Using SAS
Output 5.19: Selected Posterior Summary Statistics for the Item Parameters—NR Model
 

Chapter 5: Bayesian Estimation of Unidimensional IRT Models for Polytomously Scored Items 139
Outputs 5.20 to 5.23 provide diagnostic plots for selected item parameters generated using the autocall 
macro %TADPLOT. These plots are typical for all the item parameters. As you can see, the chains for 
slopes and intercepts reflect chains that were mixing well and relatively low autocorrelations. The 
parameter densities reflect approximately normal distributions, although some of these distributions reflect 
rather large standard deviations.  
Output 5.20: Diagnostic Plots for Selected Item Parameters—NR Model 
 
 

140 Bayesian Analysis of Item Response Theory Models Using SAS
Output 5.21: Diagnostic Plots for Selected Item Parameters—NR Model 
Output 5.22: Diagnostic Plots for Selected Item Parameters—NR Model 

Chapter 5: Bayesian Estimation of Unidimensional IRT Models for Polytomously Scored Items 141
Output 5.23: Diagnostic Plots for Selected Item Parameters—NR Model 
 
The parameters can be tranformed to conform to Bock’s constraint, σ ܽ௝௞= σ ܿ௝௞= 0 across k, by 
centering the samples values. The program code that follows illustrates centering the sampled values that 
result from executing Program 5.9, and Output 5.24 presents output from executing the program code. In 
the program, centering of the values is obtained at each iteration in the posterior results by computing the 
sum of the sampled values for the ܽ௝௞ and ܾ௝௞ paramters for each item. The mean based on this sum (sum / 
5) is then used to center the values for ܽ௝௞ and ܾ௝௞ by taking the sampled value minus the mean. PROC 
MEANS can then be used to obtain point estimates for the centered values. As you can see in the output 
(Output 5.24), the σ ܽ௝௞= σ ܿ௝௞= 0 across k for each item. 
 
 

142  Bayesian Analysis of Item Response Theory Models Using SAS 
data dash_centered; 
   set dash_postNR; 
   array a[10,5] a1_1-a1_5 a2_1-a2_5 a3_1-a3_5 a4_1-a4_5 a5_1-a5_5 
                 a6_1-a6_5 a7_1-a7_5 a8_1-a8_5 a9_1-a9_5 a10_1-a10_5;  
   array c[10,5] c1_1-c1_5 c2_1-c2_5 c3_1-c3_5 c4_1-c4_5 c5_1-c5_5 
                 c6_1-c6_5 c7_1-c7_5 c8_1-c8_5 c9_1-c9_5 c10_1-c10_5;  
   array a_c[10,5] a_c1_1-a_c1_5 a_c2_1-a_c2_5 a_c3_1-a_c3_5  
              a_c4_1-a_c4_5 a_c5_1-a_c5_5 a_c6_1-a_c6_5 a_c7_1-a_c7_5 
              a_c8_1-a_c8_5 a_c9_1-a_c9_5 a_c10_1-a_c10_5;  
   array c_c[10,5] c_c1_1-c_c1_5 c_c2_1-c_c2_5 c_c3_1-c_c3_5  
              c_c4_1-c_c4_5 c_c5_1-c_c5_5 c_c6_1-c_c6_5 c_c7_1-c_c7_5  
              c_c8_1-c_c8_5 c_c9_1-c_c9_5 c_c10_1-c_c10_5; 
  
   keep a_c1_1--a_c10_5 c_c1_1--c_c10_5; 
   do j=1 to 10; 
      m_a=sum(of a[j,2], a[j,3], a[j,4], a[j,5]); 
      m_c=sum(of c[j,2], c[j,3], c[j,4], c[j,5]); 
      do k=1 to 5; 
         if k=1 then do; 
            a_c[j,k]=-m_a/5; 
            c_c[j,k]=-m_c/5; 
         end; 
         else do; 
            a_c[j,k]=a[j,k]-m_a/5; 
            c_c[j,k]=c[j,k]-m_c/5; 
         end; 
      end; 
   end; 
run; 
 

Chapter 5: Bayesian Estimation of Unidimensional IRT Models for Polytomously Scored Items 143
Output 5.24: Centered Item Parameters for Selected Items—NR Model 
The Generalized Partial Credit Model 
The generalized partial credit (GPC) model (Muraki, 1992) is also widely used to model polytomous item 
responses. In this model, for item j with ݉௝ response options (or categories), the probability of an examinee 
with trait level θ responding in a particular score category x is given by the following:   
ܲ௝௫(Ʌ) =
݁σ
௭ೕೖ
ೣ
ೖసబ
σ
݁σ
௭ೕೖ
೓
ೖసబ
௠ೕ
௛ୀଵ
          
for 
݇= 0, 1, 2, ݔ, … m୨ 
where ݖ௝௞= ܦܽ௝(Ʌ െɁ௝௞), ܽ௝ is the slope parameter, and Ɂ௝௞is called the step difficulty parameter for 
category k. Different from the GR model, the response probabilities are computed directly in one step, and 
the step difficult parameters are not necessarily ordered. The GPC model reduces to the partial credit (PC) 

144  Bayesian Analysis of Item Response Theory Models Using SAS 
model under the assumption that all items have equal slopes. Similar to the RS-GR model, some 
parameterizations of the GPC model decompose the parameter Ɂ௝௞ into a location parameter (ܾ௝) for each 
item and a set of category thresholds parameters (ɒ௝௞). Using this parameterization, the logistic deviate is 
ݖ௝௞= ܦܽ௝(Ʌ െܾ௝+ ɒ௝௞). To identify and estimate the parameters of the GPC model, the threshold 
parameter of the last category for each item is set to equal the negative of the sum of the category threshold 
parameters for the previous categories, that is, 
ɒ௝௠ೕ= െ෍ɒ௝௞
௠ೕିଵ
௞ୀଵ
 
This constrains the set of category thresholds parameters to equal 0 for each item. 
Estimating the GPC Model  
Program 5.10 includes the PROC MCMC code for estimating the GPC model, where ݖ௝௞= ܦܽ௝(ߠെܾ௝+
ɒ௝௞), using the DASH data. For each DASH item, one slope parameter (ܽ௝), one location parameter (ܾ௝), 
and four category threshold parameters (ɒ௝ଵ, ɒ௝ଶ, ɒ௝ଷ, ɒ௝ସ)  are defined corresponding to four steps (1ĺ2, 
2ĺ3, 3ĺ4, 4ĺ5).  
In the code, the TAU[4,10] matrix represents the 40 threshold parameters (4 category thresholds for each 
item), and the Z[5] array denotes the five logistic deviate z values for each item, ݖ௝௞= ܦܽ௝൫Ʌ െɁ௝௞+ ߬௝௞൯ 
with D=1.0. SUM_Z[K] is an array reflecting the sum of the logistic deviates for response categories. That 
is, the values σ
ݖ௝௞
௫
௞ୀ଴
 for ݔ= 1, 2, 3, 4, 5. NUMER[K] are an array that reflects the numerator for response 
categories in the GPC model, and P[5] is an array of response probabilities for five score categories.  
As for other models, the item parameters within each item are treated as a single block. Note that for the 
model identification, only the category threshold parameters for the first three steps are declared as 
parameters. The last threshold parameter within each item equals to the negative sum of the first three 
steps, and is computed in a BEGINNODATA−ENDNODATA block ߑ. The priors for the slope parameters 
(ܽ௝) are log-normal distributions, and the priors for both location and thresholds parameters are normal 
distributions. To calculate the likelihood of the data, the Z values are calculated first ߒfollowed by the 
SUM_Z values. Then the numerator ߓand denominator ߔof the GPC model are computed and used to 
obtain the response probabilities ߕ. 
 
 
 

Chapter 5: Bayesian Estimation of Unidimensional IRT Models for Polytomously Scored Items   145 
Program 5.10: PROC MCMC Code for Estimating the GPC Model 
proc mcmc data=dash_data outpost=dash_postGPC seed=23  nbi=5000 nmc=20000 
nthreads=8 monitor=(a b tau)  diagnostics=all plots=(trace autocorr); 
 
array ix[10]; array a[10]; array b[10]; 
array tau[4,10] tau1_1-tau1_10 tau2_1-tau2_10 tau3_1-tau3_10  
      tau4_1-tau4_10; 
array z[5] z1-z5; 
array sum_z[5] sum_z1-sum_z5;  
array numer[5] numer1-numer5;  
array p[5] p1-p5;  
 
parms a1 1  b1 0   tau1_1 0  tau2_1 0  tau3_1 0 ; 
parms a2 1  b2 0   tau1_2 0  tau2_2 0  tau3_2 0 ; 
parms a3 1  b3 0   tau1_3 0  tau2_3 0  tau3_3 0 ; 
parms a4 1  b4 0   tau1_4 0  tau2_4 0  tau3_4 0 ; 
parms a5 1  b5 0   tau1_5 0  tau2_5 0  tau3_5 0 ; 
parms a6 1  b6 0   tau1_6 0  tau2_6 0  tau3_6 0 ; 
parms a7 1  b7 0   tau1_7 0  tau2_7 0  tau3_7 0 ; 
parms a8 1  b8 0   tau1_8 0  tau2_8 0  tau3_8 0 ; 
parms a9 1  b9 0   tau1_9 0  tau2_9 0  tau3_9 0 ; 
parms a10 1 b10 0  tau1_10 0 tau2_10 0 tau3_10 0 ; 
 
beginnodata;ߑ 
  tau4_1=-(tau1_1+tau2_1+tau3_1); 
  tau4_2=-(tau1_2+tau2_2+tau3_2); 
  tau4_3=-(tau1_3+tau2_3+tau3_3); 
  tau4_4=-(tau1_4+tau2_4+tau3_4); 
  tau4_5=-(tau1_5+tau2_5+tau3_5); 
  tau4_6=-(tau1_6+tau2_6+tau3_6); 
  tau4_7=-(tau1_7+tau2_7+tau3_7); 
  tau4_8=-(tau1_8+tau2_8+tau3_8); 
  tau4_9=-(tau1_9+tau2_9+tau3_9); 
  tau4_10=-(tau1_10+tau2_10+tau3_10); 
endnodata; 
 
prior a: ~ lognormal(0, var=25); 
prior b: ~ normal(0, var=25); 
prior tau1_1-tau1_10 ~ normal(0, var=25); 
prior tau2_1-tau2_10 ~ normal(0, var=25); 
prior tau3_1-tau3_10 ~ normal(0, var=25); 
random theta ~ normal(0, var=1) subject=_obs_; 
 
llike=0; 
do j=1 to 10; 
   z[1]=0; 
   sum_z[1]=0; 
   numer[1]=1; 
   do k=2 to 5;ߒ 
      z[k]=a[j]*(theta - b[j] + tau[k-1,j]); 
      sum_z[k] = sum_z[k-1]+z[k]; 
      numer[k] = exp(sum_z[k]);ߓ 
   end; 
   denom = sum (of numer1-numer5);ߔ 
 
   do k=1 to 5; 
      p[k]=numer[k]/denom;ߕ 
   end; 
 
   llike=llike+log(p[ix[j]]); 
end; 
model general(llike); 
run;              
 

146 Bayesian Analysis of Item Response Theory Models Using SAS
Output from Estimating the GPC Model  
Output 5.25 shows the summary statistics for selected item parameters from the GPC model. As previously 
done, selected results were generated with use of the autocall macro %POSTSUM because there are a large 
number of item parameters. The slope estimates vary across items, which indicates that the GPC model 
may be more appropriate for the DASH item responses than the PC model (a common slope model). The 
locations are all greater than 0 except for the last item. The four threshold parameter estimates (means) 
within each item are not ordered as in the GR model. Note that the item parameters for all items are 
presented in the section comparing Bayesian and MML estimation methods. 
Output 5.25: Posterior Summary Statistics for Selected Item Parameters—GPC Model 
 

Chapter 5: Bayesian Estimation of Unidimensional IRT Models for Polytomously Scored Items 147
Outputs 5.26 to 5.28 illustrate the trace and autocorrelation plots for selected item parameters. The plots 
that are presented represent typical plots across the set of items. You can see that the chains for the slope 
parameters (ܽ௝) and the category thresholds parameters (ɒ௝௞) all reflect chains mixing well with low 
dependence among sampled values. This is in contrast with the location parameters (ܾ௝), which exhibit 
reasonably good mixing but also moderately high dependence among sampled values. 
Output 5.26: Trace and Autocorrelation Plots for Selected Item Slope Parameters—GPC Model 
 
 

148 Bayesian Analysis of Item Response Theory Models Using SAS
Output 5.27: Trace and Autocorrelation Plots for Selected Item Location Parameters—GPC Model 
 
Output 5.28: Trace and Autocorrelation Plots for Item Tau Parameters—GPC Model 
 

Chapter 5: Bayesian Estimation of Unidimensional IRT Models for Polytomously Scored Items   149 
Comparison of Results Based on MML Estimation 
To compare the Bayesian estimation results with traditional MML estimation results, we estimated 
different models with the DASH data using the computer program IRTPRO (Cai, Thissen, & du Toit, 
2011). Tables 5.1, 5.2, and 5.3 present the comparison of Bayesian and MML results for the GR, NR, and 
GPC models. Notably, the procedure PROC IRT (SAS, 2014, Chapter 53) could be used for some of these 
comparisons. But it could not be used universally for all comparisons in this book, so this SAS procedure 
was not used. 
Across the tables, you can see that the Bayesian and MML estimates are very similar for the different 
models. The one exception may be the comparison for the NR model (see Table 5.2), where the slope 
parameters are more similar than the intercept parameters between two estimation methods, and not 
unexpectedly, the differences for more extreme values tend to be larger.  
Table 5.1: Comparison of Bayesian and MML Item Parameter Estimates for the GR Model 
Estimation Method 
a 
b1 
b2 
b3 
b4 
Item 1 
 
 
 
 
 
Bayesian 
1.83 
0.50 
1.23 
2.17 
2.97 
MML 
1.82 
0.49 
1.20 
2.16 
2.95 
Item 2 
 
 
 
 
 
Bayesian 
2.07 
0.61 
1.23 
1.89 
2.65 
MML 
2.06 
0.60 
1.22 
1.88 
2.65 
Item 3 
 
 
 
 
 
Bayesian 
3.49 
0.14 
0.79 
1.65 
2.14 
MML 
3.49 
0.13 
0.78 
1.63 
2.12 
Item 4 
 
 
 
 
 
Bayesian 
2.50 
í 
0.45 
1.29 
2.13 
MML 
2.49 
í0.41 
0.44 
1.28 
2.12 
Item 5 
 
 
 
 
 
Bayesian 
2.79 
í 
í 
0.50 
1.08 
MML 
2.79 
í0.85 
í0.20 
0.49 
1.07 
Item 6 
 
 
 
 
 
Bayesian 
3.21 
í 
0.61 
1.36 
1.84 
MML 
3.19 
í0.10 
0.59 
1.35 
1.82 
Item 7 
 
 
 
 
 
Bayesian 
3.33 
í 
0.34 
1.18 
1.54 
MML 
3.32 
í0.40 
0.33 
1.17 
1.53 
Item 8 
 
 
 
 
 
Bayesian 
3.18 
í 
í 
0.60 
1.16 
MML 
3.18 
í0.78 
í0.11 
0.59 
1.15 
Item 9 
 
 
 
 
 
Bayesian 
2.63 
0.53 
1.11 
1.71 
2.08 
MML 
2.62 
0.51 
1.09 
1.70 
2.06 
Item 10 
 
 
 
 
 
Bayesian 
2.32 
í 
í 
0.06 
0.69 
MML 
2.33 
í1.59 
í0.74 
0.04 
0.68 
 
 
 

150  Bayesian Analysis of Item Response Theory Models Using SAS 
Table 5.2: Comparison of Bayesian and MML Item Parameter Estimates for the NR Model 
Estimation 
a1 
a2 
a3 
a4 
a5 
 
c1 
c2 
c3 
c4 
c5 
Item 1 
 
 
 
 
 
 
 
 
 
 
 
Bayesian 
0 
1.02 
2.31 
3.15 
4.64 
 
0 
í1.30 
í2.36 
í4.35 
í7.30 
MML 
0 
1.00 
2.30 
3.18 
4.68 
 
0 
Ѹ1.32 
Ѹ2.40 
Ѹ4.44 
Ѹ7.46 
Item 2 
 
 
 
 
 
 
 
 
 
 
 
Bayesian 
0 
1.35 
2.56 
3.32 
5.35 
 
0 
í1.61 
í2.83 
í4.26 
í8.02 
MML 
0 
1.33 
2.57 
3.36 
5.44 
 
0 
Ѹ1.62 
Ѹ2.89 
Ѹ4.35 
Ѹ8.27 
Item 3 
 
 
 
 
 
 
 
 
 
 
 
Bayesian 
0 
2.70 
5.55 
7.42 
9.16 
 
0 
í0.73 
í2.77 
 í 
í9.25 
MML 
0 
2.73 
5.74 
7.80 
9.61 
 
0 
Ѹ0.79 
Ѹ2.99 
Ѹ6.75 
Ѹ9.90 
Item 4 
 
 
 
 
 
 
 
 
 
 
 
Bayesian 
0 
1.64 
3.27 
4.59 
7.80 
 
0 
0.33 
í0.42 
í2.22 
í7.50 
MML 
0 
1.62 
3.22 
4.58 
8.00 
 
0 
0.29 
Ѹ0.47 
Ѹ2.33 
Ѹ7.97 
Item 5 
 
 
 
 
 
 
 
 
 
 
 
Bayesian 
0 
1.29 
3.33 
4.90 
6.88 
 
0 
0.79 
1.23 
0.30 
í1.21 
MML 
0 
1.24 
3.30 
4.85 
6.83 
 
0 
0.77 
1.16 
0.31 
Ѹ1.34 
Item 6 
 
 
 
 
 
 
 
 
 
 
 
Bayesian 
0 
2.52 
4.89 
6.40 
8.01 
 
0 
í0.13 
í1.50 
í3.98 
í6.21 
MML 
0 
2.52 
4.93 
6.48 
8.10 
 
0 
Ѹ0.18 
Ѹ1.62 
Ѹ4.17 
Ѹ6.47 
Item 7 
 
 
 
 
 
 
 
 
 
 
 
Bayesian 
0 
2.10 
4.59 
6.26 
9.54 
 
0 
0.55 
í0.24 
í2.91 
í6.46 
MML 
0 
2.09 
4.60 
6.32 
9.63 
 
0 
0.51 
Ѹ0.34 
Ѹ3.06 
Ѹ6.76 
Item 8 
 
 
 
 
 
 
 
 
 
 
 
Bayesian 
0 
1.70 
3.87 
5.52 
8.41 
 
0 
1.07 
1.31 
 0.15 
í2.45 
MML 
0 
1.69 
3.84 
5.47 
8.33 
 
0 
1.04 
1.24 
0.04 
Ѹ2.59 
Item 9 
 
 
 
 
 
 
 
 
 
 
 
Bayesian 
0 
2.23 
3.24 
4.57 
5.73 
 
0 
í1.70 
í2.86 
í5.25 
í6.65 
MML 
0 
2.23 
3.26 
4.59 
5.77 
 
0 
Ѹ1.74 
Ѹ2.94 
Ѹ5.43 
Ѹ6.80 
Item 10 
 
 
 
 
 
 
 
 
 
 
 
Bayesian 
0 
0.77 
2.02 
3.63 
5.38 
 
0 
1.39 
2.37 
2.13 
1.54 
MML 
0 
0.77 
2.00 
3.59 
5.33 
 
0 
1.38 
2.32 
2.05 
1.43 
 
Table 5.3: Comparison of Bayesian and MML Item Parameter Estimates for the GPC Model 
Estimation 
a 
b 
Ĳ1 
Ĳ2 
Ĳ3 
Ĳ4 
Item 1 
 
 
 
 


Bayesian 
1.04 
1.64 
0.36 
0.74 
Ѹ0.53 
Ѹ0.57 
MML 
1.09 
1.59 
0.36 
0.70 
Ѹ0.50 
Ѹ0.57 
Item 2 
 
 
 
 


Bayesian 
1.18 
1.55 
0.20 
0.53 
Ѹ0.13 
Ѹ0.60 
MML 
1.24 
1.50 
0.21 
0.50 
Ѹ0.13 
Ѹ0.58 
Item 3 
 
 
 
 


Bayesian 
2.46 
1.15 
0.84 
0.40 
Ѹ0.58 
Ѹ0.66 
MML 
2.56 
1.12 
0.82 
0.38 
Ѹ0.56 
Ѹ0.65 
Item 4 
 
 
 
 


Bayesian 
1.61 
0.88 
1.08 
0.41 
Ѹ0.48 
Ѹ1.01 
MML 
1.67 
0.86 
1.06 
0.39 
Ѹ0.45 
Ѹ0.99 
 

Chapter 5: Bayesian Estimation of Unidimensional IRT Models for Polytomously Scored Items   151 
Estimation 
a 
b 
Ĳ1 
Ĳ2 
Ĳ3 
Ĳ4 
Item 5 
 
 
 
 


Bayesian 
1.61 
0.15 
0.77 
0.36 
Ѹ0.47 
Ѹ0.66 
MML 
1.66 
0.14 
0.75 
0.35 
Ѹ0.45 
Ѹ0.65 
Item 6 
 
 
 
 


Bayesian 
2.09 
0.92 
0.82 
0.32 
Ѹ0.56 
Ѹ0.59 
MML 
2.17 
0.89 
0.80 
0.31 
Ѹ0.54 
Ѹ0.57 
Item 7 
 
 
 
 


Bayesian 
2.17 
0.67 
0.94 
0.32 
Ѹ0.81 
Ѹ0.45 
MML 
2.25 
0.65 
0.91 
0.31 
Ѹ0.78 
Ѹ0.44 
Item 8 
 
 
 
 


Bayesian 
1.94 
0.24 
0.88 
0.35 
Ѹ0.49 
Ѹ0.74 
MML 
2.01 
0.23 
0.86 
0.34 
Ѹ0.48 
Ѹ0.72 
Item 9 
 
 
 
 


Bayesian 
1.54 
1.29 
0.28 
0.30 
Ѹ0.53 
Ѹ0.06 
MML 
1.60 
1.24 
0.29 
0.30 
Ѹ0.52 
Ѹ0.07 
Item 10 
 

 
 


Bayesian 
1.33 
Ѹ0.43 
1.10 
0.28 
Ѹ0.66 
Ѹ0.73 
MML 
1.38 
Ѹ0.42 
1.08 
0.27 
Ѹ0.64 
Ѹ0.71 
 
Graphs of Item Category Response Functions 
Differing from the dichotomous IRT models, polytomous items have more than one response curves, one 
for each category. However, as illustrated in Chapter 4 to plot ICCs for binary response items, the category 
response curves can be drawn for each polytomous item. Program 5.11 provides the commands for drawing 
the item category response curves for the DASH items based on the GR model. The plots are shown in 
Output 5.29. 
In the program, point estimates for the item parameters are first obtained using PROC MEANS ߑ. These 
point estimates are saved in a data set and used to generate a data set of response category probabilities for 
each item ߒ. In this data set, the rows reflect discrete values of θ from −4 to 4 in increments of .1. Given 
this data set, a graphics template can be defined to plot the category curves using a SERIESPLOT 
command for each response category and for each item ߓ. A LAYOUT LATTICE command ߔis used to 
graph all the category plots in a single graph with common scales for the x-axes and y-axes. 
Program 5.11: Graphing Item Category Response Curves for Polytomous IRT models—GR Model 
%let nitems=10; /* number of items */ 
%let ncats=5;   /* number of response categories */ 
%let nthres=4;  /* number of threshold parameters */ 
 
/* save point estimates for item parameters*/ 
proc means data=dash_postout noprint;ߑ 
     var a1-a&nitems b1_1-b1_&nitems b2_1-b2_&nitems b3_1-b3_&nitems 
         b4_1-b4_&nitems;   
     output out=means_gr  mean=; 
run; 
 
/* create dataset of expected item response probabilities across theta */ 
data plotdata_gr;ߒ 
     set means_gr; 
     array a[&nitems] a1-a&nitems; 
     array b[&nthres,&nitems] b1_1-b1_&nitems  b2_1-b2_&nitems  
           b3_1-b3_&nitems b4_1-b4_&nitems; 
 

152  Bayesian Analysis of Item Response Theory Models Using SAS 
     array p_star[&nitems,&nthres]; 
     array p[&nitems,&ncats]; 
 
     retain group(0); 
     do theta=-4 to 4 by .1; 
        group=group+1; 
        do j=1 to &nitems; 
           do k=1 to 4; 
 
       p_star[j,k]=logistic(a[j]*(theta-b[k,j])); 
 
    end; 
           p[j,1]=1-p_star[j,1]; 
 
    do k=2 to 4; 
 
       p[j,k]=p_star[j,(k-1)]-p_star[j,k]; 
           end; 
           p[j,5]=p_star[j,4]; 
        end; 
     output; 
     end; 
run; 
 
/* define a graph template for category icc plots using seriesplots */ 
proc template; 
     define statgraph iccplots; 
     begingraph  / designwidth=630px designheight=840px; 
     entrytitle "Category ICCs for the &nitems items – GR Model"; 
     layout lattice / columns=3 rows=4 rowgutter=10 columngutter=10;ߔ 
     layout overlay /  
       xaxisopts=(label="Theta" linearopts=(viewmin=-4 viewmax=4))  
       yaxisopts=(label="Item 1" linearopts=(viewmin=0  viewmax=1));  
       seriesplot x=theta y=p1;ߓ 
       seriesplot x=theta y=p2; 
       seriesplot x=theta y=p3; 
       seriesplot x=theta y=p4; 
       seriesplot x=theta y=p5; 
       endlayout; 
 
       *Repeat for Item2~Item9; 
 
     layout overlay /  
       xaxisopts=(label="Theta" linearopts=(viewmin=-4 viewmax=4))  
       yaxisopts=(label="Item 10" linearopts=(viewmin=0  viewmax=1));  
       seriesplot x=theta y=p46; 
       seriesplot x=theta y=p47; 
       seriesplot x=theta y=p48; 
       seriesplot x=theta y=p49; 
       seriesplot x=theta y=p50; 
       endlayout; 
    endlayout; 
    endgraph; 
end; 
run; 
 
/* produce the plots using the template */ 
proc sgrender data=plotdata_gr template=iccplots; 
run; 
 

Chapter 5: Bayesian Estimation of Unidimensional IRT Models for Polytomously Scored Items 153
Output 5.29: Category ICC Plots for the 10 DASH Items 
 
Graphs of Test Information Functions 
A program to illustrate the graphing of test information functions for the GR model is also included on the 
SAS Press authors’ web page for this book (Program 5.12). Test information functions are computed by 
summing up the information about θ provided by each item. These functions are useful in the test 
development process for exploring the degree of precision of a test or differing sets of items across the θ 
scale (see, for example, Hambleton & Swamination 1985). 

154  Bayesian Analysis of Item Response Theory Models Using SAS 
 
 

Chapter 6: IRT Model Extensions 
 
Introduction ....................................................................................................... 155 
The Bifactor IRT Model ...................................................................................... 155 
Description of the Model ................................................................................................... 156 
Estimation of the Model in PROC MCMC ........................................................................ 156 
Output from PROC MCMC ................................................................................................ 158 
Other Multidimensional IRT Models ................................................................................. 163 
The Problem of “Label Switching” ................................................................................... 165 
The Testlet IRT Model ....................................................................................... 166 
Description of the Model ................................................................................................... 166 
Estimation of the Model ..................................................................................................... 166 
Output from PROC MCMC ................................................................................................ 167 
Hierarchical Models—Multilevel IRT Models ...................................................... 172 
Description of the Multilevel IRT Model ........................................................................... 172 
Estimation of the Model ..................................................................................................... 173 
Output from PROC MCMC ................................................................................................ 175 
Differential Item Functioning—Multiple Group and Mixture IRT Models ............ 180 
Multiple Group Models for Detecting DIF ........................................................................ 180 
Mixture IRT Models ............................................................................................................ 182 
 
Introduction 
The item response theory (IRT) models discussed to this point are appropriate for analyzing item responses 
that are assumed to be determined by a single person parameter or latent trait (that is, unidimensional 
models). However, there are a number of possible extensions to these models for other testing applications. 
For example, multidimensionality in item responses for assessments is often detected and can be caused by 
various factors, such as designed test structure, unintended construct-irrelevant variance, and mixed item 
formats. For example, when an assessment instrument is designed to measure more than one ability or trait, 
such as quality of life, the item responses would be expected to exhibit a multidimensional structure. This 
chapter discusses several extensions to unidimensional IRT models, including, multidimensional IRT 
(MIRT) models, models that account for dependence between sets of items (testlet models), hierarchical 
IRT models, and models that can be used to examine differential item functioning (DIF) and sample 
heterogeneity in item responses. Although not discussed in this chapter, programs that reflect random item 
effects models are included on the authors’ web page for this book (see the section “Treating Item 
Parameters as Random Effects” in Chapter 3 for a brief discussion of these types of models). One example 
is a random item effects model for detecting differential item functioning. 
The Bifactor IRT Model 
A MIRT model that has recently received attention is the bifactor IRT model. In the bifactor model, each 
item response is determined or explained by both a general factor and secondary or specific factors 
(Gibbons & Hedeker, 1992; Gibbons, Immekus, & Bock, 2007). The bifactor model allows for measuring a 
general ability or trait while controlling for the variance that arises from the measurement of other factors. 

156   Bayesian Analysis of Item Response Theory Models Using SAS 
 
The bifactor model is particularly useful in testing applications where the general factor is the primary trait 
of interest. Secondary factors, which may be conceptually more narrow in scope, are introduced to capture 
any residual item relationships beyond those accounted by the general factor.  
In some applications, these secondary factors can be nuisance factors and can arise from a variety of 
sources. For example, Demars (2013) discusses that the residual variance in the secondary factors could 
arise from the presence of subscales with common content even though subscale scores are not the focus; or 
from survey instruments where both positively and negatively worded items are used and the directionality 
of items forms the factors. After controlling for these nuisance factors, better estimates of the general traits 
being measured for persons are obtained (Gibbons et al., 2007).  
In other testing applications, the secondary factors, or group factors from the factor-analysis literature, may 
be specific factors of interest. For example, when the directionality of items is modeled as a nuisance factor 
(negative wording), you may be interested in comparing persons who score high on the negatively worded 
items as opposed to persons who score low on the negatively worded items after controlling for estimation 
on the general trait (Demars, 2013). For another example, consider the case of an achievement test that 
consists of a number of items from different content areas (geometry, algebra, and trigonometry). You may 
be interested in focusing on students who score low on algebra after controlling for mathematics 
achievement in general. 
Description of the Model 
In the bifactor model, slope parameters (or factor loadings) are estimated for each item on the general 
factor as well as on a single secondary factor. For example, given six items with two secondary factors 
(items 1−4 are assumed to relate to factor 2 and items 5−6 to factor 3), slope parameters for the general 
factor (first dimension or factor 1) would be estimated for the six items. Also, slope parameters for the 
second factor are estimated for items 1−4, and slope parameters for the third factor are estimated for items 
5−6. Orthogonality among the different dimensions is assumed in the bifactor model because the secondary 
dimensions reflect the specific relationships among different groups of items after controlling for the 
contribution of the general factor. For example, a two-parameter (2P) bifactor model for dichotomously 
scored items is defined as: 
ܲ௝(ી) =
௘ೋೕ
1+௘ೋೕ=
ଵ
1+௘షೋೕ ,  
where, zj = Σh ajhθh + dj, θh is the person’s ability/trait level on dimension h in the vector of traits (θθ); ajh is 
the discrimination (slope) parameter of item j on dimension h; and dj is an intercept parameter, which is 
related to item difficulty through a composite function of the slope and difficulty parameters for item j. In a 
multidimensional context, dj is interpreted as the log-odds of a correct response for persons with θ =0 
(Demars, 2013). 
A bifactor model for polytomously scored items can be defined as an extension to the GR model. In this 
bifactor model, the cumulative probability that a person selects or receives a category score or rating k or 
higher for item j,ܲ௝௞
כ (Ʌ) is modeled by the logistic deviate zjk = Σh ajhθh + djk, where djk is an intercept 
parameter for item j and category k, and θh and ajh have the same meaning as above. 
Estimation of the Model in PROC MCMC 
Program 6.1 presents the PROC MCMC code for estimating the bifactor model for a simulated application 
with 10 items and 1,000 examinees, where all ten items are located on a general factor, ; and the first five 
items are associated with one secondary factor and the second five items are associated with another 
secondary factor. The application also illustrates model estimation for a set of mixed item types where the 
first five items were dichotomously scored and the second five items were polytomously scored (4 response 
categories). The item responses were simulated with loadings on the general factor (aj1) fixed at 1.75; and 
 

Chapter 6: IRT Model Extensions   157 
loadings on the secondary factors (aj2 and aj3) were all fixed at 1.0. The intercept parameters (dj and djk) 
reflected items located across the θ scale. 
The program combines code from previous examples in Chapters 4 and 5 for dichotomously and 
polytomously scored items. One exception is the use of an array for the person parameters, THETA, 
because now three θ parameters are estimated ߑ. Use of multiple θ parameters requires specification of 
multiple random statements to define priors for these parameters ޾. As in the previous program, item 
parameters are specified with blocks for each item. In the program code defining the likelihood model, 
separate sections are used for the dichotomously scored items ޿and polytomously scored items ߀. You 
should note that the bifactor model in the program uses a model specification where zj = Σh ajhθh − dj for 
dichotomously scored items and zjk = Σh ajhθh − djk for polytomously scored items. Thus, the signs for the 
intercept parameter reflect the location of the parameters on the θ scale directly. Also note that a large 
number of iterations are used because the chain exhibits high autocorrelations and is, therefore, not 
efficient. Despite the large number of iterations, the computer time required was less than 15 minutes real 
time. 
Program 6.1: Estimating the 2P Bifactor Model 
proc mcmc data=bifactor_data outpost=bifactor_post seed=423 nbi=5000 
nmc=100000 nthreads=8 plots=(trace density); 
   
   array ix[10];  array a1_[10]; array a2_[5]; array a3_[5]; 
   array d_cr[3,5] d1_1-d1_5  d2_1-d2_5 d3_1-d3_5; 
   array d[5]; array theta[3];ߑ 
   array p[3]; 
   beginnodata; 
   lprior=0; 
   do j=1 to 5; 
     lprior = lprior + lpdfnorm(d_cr[1,j],0,4); 
     lprior = lprior + lpdfnorm(d_cr[2,j],0,4,d_cr[1,j]); 
     lprior = lprior + lpdfnorm(d_cr[3,j],0,4,d_cr[2,j]); 
   end; 
   endnodata; 
   parms a1_1 1 a2_1 1 d1 0; 
   parms a1_2 1 a2_2 1 d2 0; 
   parms a1_3 1 a2_3 1 d3 0; 
   parms a1_4 1 a2_4 1 d4 0; 
   parms a1_5 1 a2_5 1 d5 0; 
   parms a1_6 1  a3_1 1 d1_1 -1  d2_1 0  d3_1 1; 
   parms a1_7 1  a3_2 1 d1_2 -1  d2_2 0  d3_2 1; 
   parms a1_8 1  a3_3 1 d1_3 -1  d2_3 0  d3_3 1; 
   parms a1_9 1  a3_4 1 d1_4 -1  d2_4 0  d3_4 1; 
   parms a1_10 1  a3_5 1 d1_5 -1  d2_5 0  d3_5 1; 
 
   prior a1_1-a1_10 ~normal(0, var=4, lower=.2, upper=4); 
   prior a2_1-a2_5 a3_1-a3_5 ~normal(0, var=2, lower=0, upper=2); 
   prior d1-d5 ~ normal(0, var=4); 
   prior d1_1-d1_5  d2_1-d2_5 d3_1-d3_5 ~ general(lprior); 
   random theta1 ~ normal(0, var=1) subject=_obs_ ;ߒ 
   random theta2 ~ normal(0, var=1) subject=_obs_ ; 
   random theta3 ~ normal(0, var=1) subject=_obs_ ; 
 
 
 
 

158   Bayesian Analysis of Item Response Theory Models Using SAS 
 
   llike=0; 
   do j=1 to 5;ߓ 
      z = -(a1_[j]*theta[1] + a2_[j]*theta[2] - d[j]); 
      pr = 1 / (1 + exp(z)); 
      llike = llike + ix[j] * log(pr) + (1 - ix[j]) * log(1 - pr); 
   end; 
   do j=6 to 10;ߔ 
     do k=1 to 3; 
        p[k]=(-(a1_[j]*theta[1] + a3_[j-5]*theta[3] - d_cr[k,j-5])); 
     end; 
     if ix[j]=1 then prob=1-1/(1+exp(p[1]));  
     else if ix[j]=2 then prob=1/(1+exp(p[1]))- 1/(1+exp(p[2]));  
     else if ix[j]=3 then prob=1/(1+exp(p[2]))- 1/(1+exp(p[3]));  
     else prob=1/(1+exp(p[3]));  
     llike = llike + log(prob); 
   end; 
   model general(llike); 
run; 
Rather than specify separate RANDOM statements for the three θ parameters, it is also possible to use a 
multivariate random effects statement. An ARRAY MU[3] (0 0 0) and an ARRAY SIGMA[3,3] (1 0 0 0 1 
0 0 0 1) could be used in conjunction with a RANDOM THETA ~ MVN(MU, SIGMA) SUBJECT=_OBS_  
statement. However, it was found that the chains for the algorithm estimating the bifactor model mixed 
better when separate RANDOM statements were used. In an MVN prior, the random effects are sampled 
jointly. Separate random statements allow for conjugate sampling of each random effect separately, which 
may improve mixing over an n-dimensional, random-walk Metropolis sampling procedure. 
Output from PROC MCMC 
Outputs 6.1 and 6.2 present posterior summary statistics for the item parameters. In Output 6.1, the labels 
A1_1…A1_10 correspond to the 10 slope parameters for the general factor; A2_1…A2_5 correspond to 
the 5 slope parameters for the second factor; and A3_1…A3_5 correspond to the 5 slope parameters for the 
third factor. In Output 6.2, D1…D5 correspond to the intercept parameters for Items 1−5, and D1_1…D3_5 
correspond to the category intercept parameters for Items 6−10. Given the simulated values described 
above, you can see that the simulated slope parameters are reasonably well recovered except for a few 
parameters. As would be expected, a closer correspondence between the simulated intercept parameters and 
the estimated intercept parameters is observed. 
 
 
 

Chapter 6: IRT Model Extensions   159
Output 6.1: Posterior Summaries for Slope Parameters—Bifactor Model 

160 Bayesian Analysis of Item Response Theory Models Using SAS
 
Output 6.2: Posterior Summaries for Intercept Parameters—Bifactor Model 
 
Outputs 6.3 through 6.6 present trace and density plots for selected parameters and illustrate convergence in 
the chains and the distributions of sampled parameter values. Although not obvious from the trace plots 
given the large number of iterations, the autocorrelation statistics were high for the slope parameters and 
reasonably low for the intercept parameters. Thus, the chains for slope parameters were inefficient and 
traversed the parameter space slowly (see, for example, the trace plot for the parameter A3_4 in Output 
6.4). In addition, while the density plots for the slope parameters on the general factor (A1_1 – A1_10) and 
the intercept parameters approximated normal distributions, some of the density plots for slope parameters 
on secondary factors (for example, A2_5 in Output 6.3 and A3_4 in Output 6.4) deviated markedly from 
normal distributions. As discussed for other models, however, the mean of the posterior distribution is 
commonly used for posterior inference even when the density is not normally distributed. It is well known 
that the posterior mean minimizes the Expected Bayesian Loss or posterior risk in using the mean as the 
parameter estimate under a squared-error loss function. 

Chapter 6: IRT Model Extensions   161
Given the complexity of the bifactor model in terms of the number of model parameters, a simpler model of 
interest may be a unidimensional IRT model. Comparing these two competing models for the data 
considered here is discussed in Chapters 7 and 8. 
Output 6.3: Trace and Density Plots for Sampled Parameter Values—Bifactor Model 
 
 

162 Bayesian Analysis of Item Response Theory Models Using SAS
 
Output 6.4: Trace and Density Plots for Sampled Parameter Values—Bifactor Model 
 
Output 6.5: Trace and Density Plots for Sampled Parameter Values—Bifactor Model 
 
 
 

Chapter 6: IRT Model Extensions   163
Output 6.6: Trace and Density Plots for Sampled Parameter Values—Bifactor Model 
 
Other Multidimensional IRT Models 
Assessments are often multidimensional in nature because items are sampled from multiple domains of an 
overall construct (for example, quality of life). In cases where these subdomains or subscales may be of 
interest, Gibbons et al. (2007) caution against treating these subscales as secondary factors in an application 
of the bifactor model. These authors discuss that trait estimates from the secondary factors in the bifactor 
model underestimate subscale trait levels. When you are interested in reporting both general and specific 
factors from the bifactor model, Demars (2013) suggests reporting a composite score. For a discussion of 
composite scores, see, for example, Haberman (2008), Sinharay, Puhan, and Haberman (2011), and 
Feinberg and Wainer (2014). 
Alternatively, other MIRT models that were discussed in Chapter 1 may be considered to model any 
multidimensionality in item responses. If you are interested in subscale scores, a simple structure factor 
analysis model can be estimated. Whereas in the bifactor model, items are related (load) to two factors, the 
general and one secondary factor. Items are related to only one factor or dimension in a simple structure 
factor analysis model. For example, given six items with two factors, items 1−4 may be assumed to relate 
to factor 1 and items 5−6 may be assumed to relate to factor 2. 
In a simple structure factor model, correlations between the factors or dimensions that are modeled may or 
may not be assumed. If no correlations or an orthogonal model is assumed, the model is equivalent to 
estimating separate unidimensional IRT models for each subscale, and estimating separate unidimensional 
IRT models may be preferred to estimating the more complex MIRT model.  
Because subscales are often assumed to be correlated, a simple structure correlated factor model is typically 
of interest in MIRT applications. In addition to modeling the multidimensionality in item responses, there 
may be some modest efficiency in parameter estimation gained by estimating correlations among the 
factors. This gain in efficiency, however, is at the expense of factor interpretation because each trait 
contributes to the estimation of the other traits through the correlation (see, for example, DeMars, 2013).  
Program 6.2 presents the PROC MCMC code for estimating a two-dimensional, simple structure, 
correlated IRT model to the same bifactor data. As you can see, the structure of the program is similar to 

164   Bayesian Analysis of Item Response Theory Models Using SAS 
 
the structure of Program 6.1. However, one array of slope parameters is identified ߑ, θ parameters are 
defined in an array with a specified mean vector and the covariance matrix for the θ parameters ߒ Note 
that two θ parameters are estimated, reflecting a two-dimensional simple-structure model. The parameters 
of the covariance matrix for θ are also specified where the variance for each θ parameter is set to 1 and the 
covariance of two θs are set to the correlation parameter r ߓ. The prior for this correlation parameter is 
specified as a truncated normal distribution with the lower bound of 0 and the upper bound of 1 ߔ. A 
multivariate specification for the random effects (THETA) is now required with a mean vector and 
covariance matrix specified in the multivariate normal prior ߕ. The model likelihood is modified to reflect 
the simple structure model: examinee θ estimates on dimension 1 (THETA[1]) are based on Items 1−5, and 
θ estimates on dimension 2 are based on Items 6−10 (THETA[2])ߖߗ.     
Results are not presented since the item responses were simulated using the bifactor model. Generally 
speaking, however, slope parameters were largely greater than the 1.75 simulated value, and intercept 
parameters were closely approximated. Chains demonstrated good mixing behavior with autocorrelation 
statistics that were markedly smaller than those for the bifactor model. This is possibly due to the 
estimation of only one slope parameter for each item. The correlation between the factors was estimated at 
.79. 
Program 6.2: Estimating a Simple Structure 2-Factor Model 
proc mcmc data=bifactor_data outpost=2d_post seed=23 nmc=20000 nbi=5000; 
 
   array ix[10]; array a[10];ߑ 
   array d_cr[3,5] d1_1-d1_5  d2_1-d2_5 d3_1-d3_5; 
   array d[5]; array p[3]; 
   array theta[2]; array mu[2] (0 0); array cov[2,2];ߒ 
 
   parms a1 1 d1 0; 
   parms a2 1 d2 0; 
   parms a3 1 d3 0; 
   parms a4 1 d4 0; 
   parms a5 1 d5 0; 
   parms a6 1  d1_1 -1  d2_1 0  d3_1 1; 
   parms a7 1  d1_2 -1  d2_2 0  d3_2 1; 
   parms a8 1  d1_3 -1  d2_3 0  d3_3 1; 
   parms a9 1  d1_4 -1  d2_4 0  d3_4 1; 
   parms a10 1 d1_5 -1  d2_5 0  d3_5 1; 
   parms r; 
 
   beginnodata; 
   lprior=0; 
   do j=1 to 5; 
     lprior = lprior + lpdfnorm(d_cr[1,j],0,4); 
     lprior = lprior + lpdfnorm(d_cr[2,j],0,4,d_cr[1,j]); 
     lprior = lprior + lpdfnorm(d_cr[3,j],0,4,d_cr[2,j]); 
   end; 
   endnodata; 
 
   begincnst;ߓ 
   cov[1,1]=1; 
   cov[2,2]=1; 
   endcnst; 
 
   cov[1,2]=r; cov[2,1]=r; 
 
   prior a: ~normal(0, var=4, lower=.2, upper=4); 
   prior d1-d5 ~ normal(0, var=4); 
   prior d1_1-d1_5  d2_1-d2_5 d3_1-d3_5 ~ general(lprior); 
   prior r ~ normal(0, var=.5, lower=0, upper=1);ߔ 
   random theta ~ mvn(mu, cov) subject=_obs_; ߕ 
 
 
 
 

Chapter 6: IRT Model Extensions   165 
   llike = 0; 
   do j=1 to 5; 
      z = -(a[j]*theta[1] - d[j]);ߖ 
      pr = 1 / (1 + exp(z)); 
      llike = llike + ix[j] * log(pr) + (1 - ix[j]) * log(1 - pr); 
   end; 
    
   do j=6 to 10; 
      do k=1 to 3; 
         p[k]=(-(a[j]*theta[2] - d_cr[k,j-5]));ߗ 
      end; 
      if ix[j]=1 then prob=1-1/(1+exp(p[1]));  
      else if ix[j]=2 then prob=1/(1+exp(p[1]))- 1/(1+exp(p[2]));  
      else if ix[j]=3 then prob=1/(1+exp(p[2]))- 1/(1+exp(p[3]));  
      else prob=1/(1+exp(p[3]));  
      llike = llike + log(prob); 
   end; 
 
   model general(llike); 
run; 
The Problem of “Label Switching”  
Although discussed primarily in the context of Bayesian analyses of mixture models, label switching is a 
potential problem with IRT models like the bifactor model. In the context of mixture models, for example, 
Jasra, Holmes, & Stephens (2005, p. 2) state the following: 
If exchangeable priors are placed upon the parameters of a mixture model, then the resulting posterior 
distribution will be invariant to permutations in the labelling of the parameters. As a result, the 
marginal posterior distributions for the parameters will be identical for each mixture component. 
Therefore, during MCMC simulation, the sampler encounters the symmetries of the posterior 
distribution and the interpretation of the labels switch.  
In the bifactor model, where multiple slope parameters are estimated for each item, the likelihood surface 
may have multiple equivalent modes when the slope parameters are similar in size. When this happens, 
posterior summaries for parameters are invalid. It is well known that evidence of label switching in a 
Bayesian analysis may be observed in trace plots where the plots look disjointed or segmented with good 
mixing in distinctly different locations of the parameter space. In addition, multimodality in density plots 
may indicate label switching.  
If this problem occurs, constraints on parameters may need to be imposed. For example, in the bifactor 
model (Program 6.1), upper bound constraints on the secondary factors could be defined as below. In this 
code segment, an upper bound in the joint distribution for the slope parameters for the secondary factors is 
defined as the sampled value for slope parameter on the general factor. While these types of constraints 
may be useful in certain circumstances, it is recommended that an unconstrained model be estimated first, 
and trace/density plots be examined for any abnormal patterns. 
   beginnodata; 
   lpriora=0; 
   do kk=1 to 10; 
     if kk < 6 then do; 
     lpriora=lpriora + lpdfnorm(a1_[kk],0,4,0); 
     lpriora=lpriora + lpdfnorm(a2_[kk],0,2,0,a1_[kk]); 
     end; 
     else do; 
     lpriora=lpriora + lpdfnorm(a1_[kk],0,4,0); 
     lpriora=lpriora + lpdfnorm(a3_[kk-5],0,2,0,a1_[kk]); 
     end; 
   end; 
   endnodata; 
   prior a1_1-a1_10 a2_1-a2_5 a3_1-a3_5 ~ general(lpriora); 
 

166   Bayesian Analysis of Item Response Theory Models Using SAS 
 
The Testlet IRT Model 
As discussed in Chapter 1, there are some testing applications in which individuals may respond to sets of 
items (testlet) based on common stimuli. For example, reading comprehension passages with sets of items 
associated with each passage would form separate testlets. For these types of testing applications, the 
assumption of local independence is likely to be violated. The item responses to the items within a testlet 
would be more highly related than predicted by a unidimensional latent ability or trait based on all test item 
responses and standard IRT models (for example, 2P IRT model). For assessment applications that use 
testlets, the IRT models discussed in previous chapters may be modified to account for the dependence 
between items within testlets. For example, a modified GR model for testlets proposed by Wang, Bradlow 
and Wainer (2002) can be estimated for applications involving polytomously scored items.  
Description of the Model 
In the testlet IRT model, a random person-specific testlet effect is added to a unidimensional IRT model in 
order to model the interaction of a person with a testlet and therefore model the dependence between items 
within a testlet. For example, in adding this parameter to the GR model, the probability that the ith examinee 
receives a category score k (k = 1, 2… mj) or higher on item j within a testlet t(j) is defined as follows: 
ܲ௜௝௞(Ʌ) =
݁௔ೕ൫ఏି௕ೕೖିఊ೔೟(ೕ)൯
1 + ݁௔ೕ൫ఏି௕ೕೖିఊ೔೟(ೕ)൯ 
where γit(j) is a random person-specific testlet effect that models the dependence between items within a 
testlet t(j). In this model, θ is typically assumed to have an N(0,1) distribution, and γit(j) is assumed to be 
independent of θ and distributed as N(0, σ2t(j)). The values of γit(j) are assumed to be constant for an 
examinee over all items within a given testlet. Further, the testlet effect is assumed to vary across 
examinees subject to the constraint that σ ߛ௜௧(௝) = 0
௜
. Because the variances of the testlet effects (σ2t(j)) are 
testlet-specific, the testlet effects can vary across different testlets. As the variance or σ2t(j) increases, the 
amount of local dependence increases; whereas when σ2t(j) = 0, the items within the testlet can be treated as 
conditionally independent.  
Estimation of the Model 
Program 6.3 presents the PROC MCMC code for estimating the testlet model for a simulated application 
with 15 dichotomously scored items and 1000 examinees. Thus, in the above equation, the subscript for the 
response category (k) is deleted, and Pij(θ) reflects the probability that the ith examinee correctly answers 
item j. This model can be described as a modified 2P IRT model for testlets. 
In the simulated data set, sets of five items were simulated as separate testlets with varying levels of 
dependence modeled across the three testlets: γi1(j) = .5, γi2(j) = 1, and γi3(j) = 2. The item responses were 
simulated with fixed slope parameters for items within testlets but varied for items across testlets (1.4, 1.7, 
and 2.0), and item difficulty parameters (bj) reflected items that were located across the θ scale. 
The program uses code from previous examples in Chapter 4 for dichotomously scored items. Additional 
model parameters include array specifications for gamma parameters (γit(j)) and variance parameters (σ2t(j)) 
for the distributions of the gamma parameters ߑ. Note also that the variance parameters are sampled using 
a slice sampler. This  sampler tended to provide better mixing of variance parameters than the default 
conjugate sampling of the random effects parametersߒ. Rather than use inverse gamma priors for the 
variance parameters, uniform or half t priors are recommended by Gelman (2006). In this program, a half t 
prior is used ߓ. Priors for the multiple random effect parameters are specified using multiple random 
statements with variances for the gamma parameters estimated ߔ. The gamma parameter is included in the 
likelihood function for the testlet model, and an array T is part of the data set and used to indicate which 
items are associated with specific testlets ߕ. Similar to the bifactor model program, a large number of 
iterations are used because the chains for the slope and variances for gamma parameters exhibit a high 
degree of dependence and are therefore not efficient. 
 

Chapter 6: IRT Model Extensions   167 
You could specify a multivariate normal prior for the gamma parameters with the variance parameters in 
the main diagonal of the covariance matrix. However, as was found for the bifactor model, better mixing 
was observed when using separate RANDOM statements for each gamma parameter.  
Program 6.3: Estimating the Testlet Model 
proc mcmc data=testlet_data outpost=testlet_post seed=123 nbi=5000 
nmc=60000 nthreads=8 plots=(trace density); 
 
   array b[15]; array ix[15]; array t[15];  
   array p[15]; array a[15];  
   array vard[3]; array gamma[3];ߑ 
   parms a1 1 b1 0;  
   parms a2 1 b2 0;  
   parms a3 1 b3 0;  
   parms a4 1 b4 0;  
   parms a5 1 b5 0; 
   parms a6 1 b6 0 ; 
   parms a7 1 b7 0;  
   parms a8 1 b8 0;  
   parms a9 1 b9 0;  
   parms a10 1 b10 0;  
   parms a11 1 b11 0;  
   parms a12 1 b12 0;  
   parms a13 1 b13 0;  
   parms a14 1 b14 0;  
   parms a15 1 b15 0;        
   parms vard: .5 / slice;ߒ 
 
   prior b: ~ normal(0, var=4) ;   
   prior a: ~ normal(0, var=4,lower=0); 
   prior vard: ~ t(0, sd=3, 1, lower=0);ߓ 
 
   random gamma1 ~ normal(0, var=vard1) subject=_obs_; ߔ 
   random gamma2 ~ normal(0, var=vard2) subject=_obs_; 
   random gamma3 ~ normal(0, var=vard3) subject=_obs_; 
   random theta ~ normal(0, var=1) subject=_obs_;  
   do j=1 to 15;                                            
      p[j] = logistic(a[j]*(theta - b[j] - gamma[t[j]]));ߕ  
   end; 
   model ix1 ~ binary(p1);   
   model ix2 ~ binary(p2);   
   model ix3 ~ binary(p3);   
   model ix4 ~ binary(p4);   
   model ix5 ~ binary(p5);  
   model ix6 ~ binary(p6); 
   model ix7 ~ binary(p7); 
   model ix8 ~ binary(p8); 
   model ix9 ~ binary(p9); 
   model ix10 ~ binary(p10); 
   model ix11 ~ binary(p11); 
   model ix12 ~ binary(p12); 
   model ix13 ~ binary(p13); 
   model ix14 ~ binary(p14); 
   model ix15 ~ binary(p15);  
run;                          
Output from PROC MCMC 
Output 6.7 presents the posterior summary statistics for the variance terms of the gamma parameters. You 
can see that the means across sampled values recover reasonably well the modeled levels of dependence 
across the three testlets: γi1(j) = .5, γi2(j) = 1, and γi3(j) = 2. Although the posterior summary results for the 
item parameters are not presented, the slope parameters were also reasonably well recovered and the 
threshold parameters well recovered. Output 6.8 provides diagnostic plots for the three variance parameters 
 

168 Bayesian Analysis of Item Response Theory Models Using SAS
 
that were sampled (VARD1, VARD2, and VARD3). You can see that the chain mixed reasonably well, 
although it was slow in traversing the parameter space (high autocorrelation). 
Output 6.7: Posterior Summary Statistics for Modeled Dependence—Testlet Model 
Output 6.8: Diagnostic Plots for Variance Parameters—Testlet Model 
 
Outputs 6.9, 6.10, and 6.11 present trace and density plots for a subset of item parameters in the estimated 
testlet model. From the output, you can see that the chains are mixing reasonably well, and the density plots 
are all approximately normal. Similar plots were observed for the other item parameters. Although not 
presented, significant autocorrelation among sampled values was observed for many of the parameters of 
the testlet model. As for the bifactor model, this indicates that the chains were traversing the parameter 
space slowly, but the high autocorrelations do not impact posterior inference and posterior summary 
statistics. 
 
 

Chapter 6: IRT Model Extensions   169
Output 6.9: Trace and Density Plots for Item Parameters—Testlet Model 
  
Output 6.10: Trace and Density Plots for Item Parameters—Testlet Model 
 

170 Bayesian Analysis of Item Response Theory Models Using SAS
 
Output 6.11: Trace and Density Plots for Item Parameters—Testlet Model 
 
Alternatively, applications involving testlets may be estimated using a bifactor model (see for example, Li 
Bolt, & Fu, 2006; Demars, 2006). The bifactor model is consistent with testlet applications because it 
allows for estimating a general ability or trait after controlling for the variance that arises from nuisance 
factors. In the case of applications with testlets, the use of a common stimulus with subsets of items is a 
nuisance factor. In applications of the bifactor model to item responses involving testlets, slope parameters 
for the general factor are estimated for all items. Item slope parameters are also estimated for secondary 
factors that correspond to the sets of items within testlets. Slope parameter estimates that are high on 
secondary factors indicate the presence of strong local dependence.  
If a separate slope parameter is estimated for a secondary factor or testlet, a general testlet model is 
estimated (Li et al., 2006). Such a model may be preferred over the testlet IRT model discussed above. Li et 
al. argue that in the testlet IRT model, the primary trait (θ) and testlet traits (γ) are multiplied by a single 
slope parameter (ܽ௝൫ߠെܾ௝௞െߛ௧(௝)൯). Thus, items with high slope parameters will have corresponding 
high testlet parameters, which may or may not be a reasonable result or expectation for real testing 
applications.  
Program 6.4 provides the code for estimating a bifactor model with testlet applications. In this code, the 
same data set of simulated item responses is used. In the code that is presented, a general testlet model is 
estimated in which a single slope parameter for each secondary factor is estimated (A2_1 – A2_5, A3_1 – 
A3_5, and A4_1 – A4_5) ߑ. Note that no output from estimating this model is presented. 
 
 

Chapter 6: IRT Model Extensions   171 
Program 6.4: Bifactor Model for the General Testlet Model 
proc mcmc data=testlet_data outpost=testlet_bifactor_post seed=423 
nbi=5000 nmc=60000 nthreads=8 diagnostics=none plots=(trace density); 
   
   array ix[15]; array d[15]; array theta[4]; 
   array a1_[15]; array a2_[5]; array a3_[5]; array a4_[5];   
   parms a1_1 1 a2_1 1 d1 0;ߑ 
   parms a1_2 1 a2_2 1 d2 0; 
   parms a1_3 1 a2_3 1 d3 0; 
   parms a1_4 1 a2_4 1 d4 0; 
   parms a1_5 1 a2_5 1 d5 0; 
   parms a1_6 1  a3_1 1 d6 0; 
   parms a1_7 1  a3_2 1 d7 0; 
   parms a1_8 1  a3_3 1 d8 0; 
   parms a1_9 1  a3_4 1 d9 0; 
   parms a1_10 1  a3_5 1 d10 0; 
   parms a1_11 1  a4_1 1 d11 0; 
   parms a1_12 1  a4_2 1 d12 0; 
   parms a1_13 1  a4_3 1 d13 0; 
   parms a1_14 1  a4_4 1 d14 0; 
   parms a1_15 1  a4_5 1 d15 0; 
    
   prior a1_1-a1_15 ~ normal(0, var=4, lower=0); 
   prior a2_1-a2_5 a3_1-a3_5 a4_1-a4_5 ~ normal(0, var=4, lower=0); 
   prior d1-d15 ~ normal(0, var=4); 
   random theta1 ~ normal(0, var=1) subject=_obs_; 
   random theta2 ~ normal(0, var=1) subject=_obs_; 
   random theta3 ~ normal(0, var=1) subject=_obs_; 
   random theta4 ~ normal(0, var=1) subject=_obs_; 
 
   llike=0; 
   do j=1 to 5; 
      z = -(a1_[j]*theta[1] + a2_[j]*theta[2] - d[j]); 
      pr = 1 / (1 + exp(z)); 
      llike = llike + ix[j] * log(pr) + (1 - ix[j]) * log(1 - pr); 
   end; 
   do j=6 to 10; 
      z = -(a1_[j]*theta[1] + a3_[j-5]*theta[3] - d[j]); 
      pr = 1 / (1 + exp(z)); 
      llike = llike + ix[j] * log(pr) + (1 - ix[j]) * log(1 - pr); 
   end; 
   do j=11 to 15; 
      z = -(a1_[j]*theta[1] + a4_[j-10]*theta[4] - d[j]); 
      pr = 1 / (1 + exp(z)); 
      llike = llike + ix[j] * log(pr) + (1 - ix[j]) * log(1 - pr); 
   end; 
   model general(llike); 
run; 
 
 
 

172   Bayesian Analysis of Item Response Theory Models Using SAS 
 
As discussed by Demars (2006), the slope parameters in Program 6.4 (bifactor model) are not comparable 
with the gamma parameters in the testlet model. To estimate a model that compares with the testlet model 
more directly, you may decompose the single slope parameter for items within testlets on a secondary 
factor into ܽ௝×  ߙ௧(௝), where ܽ௝ is the item slope parameter on the general factor and ߙ௧(௝) is a parameter 
corresponding to ߛ௧(௝) for each testlet (scaled N(0, 1)). This model can be estimated by using the following 
statements. 
   array alpha[3];              * alpha parameter for each testlet; 
   parms alpha: .5;             * initial value for each alpha parameter; 
   prior alpha: ~ N(0, var=1);  * prior for each alpha parameter; 
   llike=0; 
   do j=1 to 5; 
      z = -(a1_[j]*theta[1] + a1_[j]*alpha[1]*theta[2] - d[j]); 
      pr = 1 / (1 + exp(z)); 
      llike = llike + ix[j] * log(pr) + (1 - ix[j]) * log(1 - pr); 
   end; 
   do j=6 to 10; 
      z = -(a1_[j]*theta[1] + a1_[j]*alpha[2]*theta[3] - d[j]); 
      pr = 1 / (1 + exp(z)); 
      llike = llike + ix[j] * log(pr) + (1 - ix[j]) * log(1 - pr); 
   end; 
   do j=11 to 15; 
      z = -(a1_[j]*theta[1] + a1_[j]*alpha[3]*theta[4] - d[j]); 
      pr = 1 / (1 + exp(z)); 
      llike = llike + ix[j] * log(pr) + (1 - ix[j]) * log(1 - pr); 
   end; 
   model general(llike); 
Hierarchical Models—Multilevel IRT Models   
Many IRT models make an assumption of independence of item response functions conditional on the 
modeled latent traits (θ) in order to define the likelihood function. However, in some testing applications, 
there exists a nested structure of item responses within clusters of persons, within clusters of items, or both, 
and in these applications this assumption may not be reasonable. For example, in testing applications where 
students are clustered within schools, the responses of the students in a school (cluster) are likely to be 
more related than is accounted for by their latent traits alone. Another example is a behavioral assessment 
application in which persons clustered within neighborhoods may be similarly related. 
For these types of testing applications, hierarchical models, also referred to as multilevel, random effects, or 
mixed effects models, can be used to model any dependence within clusters of persons or items. For more 
detail about hierarchical IRT models, see for example, Johnson, Sinharay, and Bradlow (2007) and Fox 
(2010). 
Description of the Multilevel IRT Model 
A multilevel IRT modeling approach conforms to the natural structure of item responses, persons, and 
clusters or groups of persons (Fox, 2010; Fox & Glas, 2001). In the most basic application of a multilevel 
model to item responses, the model assumes that I persons are clustered in G groups. In the model, the 
Level 1 model is an IRT model or measurement model expressing response probability conditional on item 
and person characteristics. At the next level, Level 2, a structural model may be used to describe the 
relationship between the latent variable measured at Level 1 (θi) and any person-level covariates (Xq) 
introduced to explain variability in θi across persons. At this level, the latent variables measured for persons 
are dependent variables: θi = β0 +  β1X1i + …+ βqXqi + εi with the random effects for persons εi ~ N(0, σ2θ).  
If persons are clustered in groups, such as schools, then an index for the G groups is included in the Level 2 
model, θig = β0g + β1gX1i + …+ βqgXqi + εig and εig ~ N(0, σ2θ), and a Level 3 model is added to explain 
variability in the Level 2 random effects. At this level, the βqg coefficients are dependent variables with 
 

Chapter 6: IRT Model Extensions   173 
group level covariates, ywi, introduced to explain variability: βqg = γq0 +  γ1y1g + …+ γwywg + μqg  and μqg  ~ 
multivariate N(0, Τ).  
You can see that an advantage of a multilevel modeling approach is that variables can be introduced to 
explain variability in parameters and model measurement error at the different levels of the multilevel 
model. Fox (2010) discusses that the introduction of explanatory variables can also lead to more accurate 
parameter estimation (for example, use of collateral information about examinees).  
In a multilevel IRT, the scale of the latent θ variable comprises several variance components (for example, 
between school variance and within person variance) that are estimated. Thus, to identify the model, 
different approaches can be taken. Fox (2010) fixes the mean of sampled θ values at 0 and the standard 
deviation of sampled θ values at 1 in each MCMC iteration. In an earlier paper, Fox and Glas (2001) 
identified the model through item parameter constraints—fixing one slope parameter for an item at 1 and 
one intercept parameter for an item at 0. In the example that follows, constraints on the item parameters are 
imposed to identify the model.  
Estimation of the Model 
The example that is provided is based on an analysis by Fox (2010). The data consists of item responses to 
15 dichotomously scored mathematic items from the Dutch Primary School Mathematics Test. The student 
group consisted of 2,156 8th grade students from 97 different schools. Several student level covariates were 
available to explain student level variability in math ability: socioeconomic status (SES), standardized 
scores on a nonverbal intelligence test (ISI), and gender (GENDER – males = 0, females = 1). A school-
level covariate (Participate) was used to differentiate among schools that regularly participated in a 
particular primary school program (Participate=1 or 0). Thus, a three-level model can be used to analyze 
this data. 
Program 6.5 presents the code for estimating the unconditional model. That is, a model with no covariates. 
This model is a modified version of the model used to estimate the 2P IRT. To identify the model, 
parameters for one slope parameter and one intercept parameter are fixed ߑ. Since these parameters were 
fixed at particular values, these parameters are not included in PARMS statements for the separate items. 
The particular parameters that were chosen were based on a preliminary analysis using a standard 2P IRT 
model. From the preliminary analysis, the slope parameter for Item 10 approximated 1, and the intercept 
parameter for Item 6 approximated 0. Of course, any items can be chosen, but choosing items where the 
slope parameter is ~1 and the intercept parameter is ~0 allows the other items to vary around items 
reflecting average difficulty and average discrimination. 
In the unconditional model, the student level θig parameters are modeled by a school intercept (β0g) and a 
within-school error term (εig), that is, θig = β0g +  εig. The school intercept is modeled by an overall ability 
(γ00) and a between school error term (μ0g), that is, β0g = γ00 + μ0g. The parameters in the program reflecting 
these components include overall math ability for schools (GAMMA0) ߒ; a between-school variance 
parameter (SCH_V); and a student level θ variance parameter (THETA_V). Note that the SCH_M variable 
in the program serves as the school-specific intercept variable β0g.  
A Normal prior is assumed for the overall mean. Similar to the program used to estimate the testlet IRT 
model, both of the variance parameters are sampled using the slice sampler ߓ with half t-distribution 
priors. Two random effects statements are included: one for the school level (Level 3) with the variable 
used to identify schools (SUBJECT=SCHOOL) ߔ; and one for the person level (Level 2) with the variable 
used to identify students (SUBJECT=_OBS_) ߕ. Priors for the random effects statements reflect the 
parameters in the Normal distributions assumed for the different random effects in the model (SCH_V and 
THETA_V). The remaining code in the program is the same code used to specify the likelihood model for 
the 2P IRT model (Level 1). 
 
 
 

174   Bayesian Analysis of Item Response Theory Models Using SAS 
 
Program 6.5: Unconditional Multilevel IRT Model for the Math Item Responses 
proc mcmc data=math_data outpost=math_post seed=23 nbi=5000 nmc=40000  
   plots=(trace autocorr) nthreads=8; 
 
   array b[15]; array d[15]; array a[15]; array p[15]; 
   parms a1 1  d1 0; 
   parms a2 1  d2 0; 
   parms a3 1  d3 0; 
   parms a4 1  d4 0; 
   parms a5 1  d5 0; 
   parms a6 1; 
   parms a7 1  d7 0; 
   parms a8 1  d8 0; 
   parms a9 1  d9 0; 
   parms d10 0; 
   parms a11 1  d11 0; 
   parms a12 1  d12 0; 
   parms a13 1  d13 0; 
   parms a14 1  d14 0; 
   parms a15 1  d15 0; 
   beginnodata; 
   a10=1; d6=0;ߑ 
   endnodata; 
   parms gamma0 0;ߒ 
   parms sch_v theta_v .5 / slice;ߓ 
    
   prior d1-d5 d7-d15 ~ normal(0, var=4); 
   prior a1-a9 a11-a15 ~ normal(0, var=4, lower=0); 
   prior gamma0 ~ normal(0,var=2);  
   prior sch_v theta_v ~ t(0,sd=2,1,lower=0); 
   random sch_m ~ normal(gamma0, var=sch_v) subject=school;ߔ 
   random theta ~ normal(sch_m, var=theta_v) subject=_obs_;ߕ 
 
   do j=1 to 15; 
      p[j] = logistic(a[j]*theta - d[j]); 
      b[j]=d[j]/a[j]; 
   end; 
   model ix1 ~ binary(p1); 
   model ix2 ~ binary(p2); 
   model ix3 ~ binary(p3); 
   model ix4 ~ binary(p4); 
   model ix5 ~ binary(p5); 
   model ix6 ~ binary(p6); 
   model ix7 ~ binary(p7); 
   model ix8 ~ binary(p8); 
   model ix9 ~ binary(p9); 
   model ix10 ~ binary(p10); 
   model ix11 ~ binary(p11); 
   model ix12 ~ binary(p12); 
   model ix13 ~ binary(p13); 
   model ix14 ~ binary(p14); 
   model ix15 ~ binary(p15); 
run; 
Program 6.6 presents the code for including covariates in the model to explain variability in θ at the student 
and school levels. This program adds to the unconditional model (Program 6.5) parameters in a linear 
regression model used to explain student level variability (BETA1, BETA2, and BETA3) for the ISI, SES, 
and GENDER variables, and parameters in a linear regression model used to explain school level 
variability in θ (GAMMA0 and GAMMA1) ߑ. Additional prior specifications are included for these 
parameters ߒ. Finally, the linear regression models are included in the random effects statements for the 
school level model ߓand the student level model ߔ through the variables SCHOOL_L and STUDENT_L. 
 

Chapter 6: IRT Model Extensions   175 
Note that the linear regression models can be included directly in the random effects statements as is 
illustrated in the program (Program 6.6) on the website. 
Program 6.6: Conditional Multilevel IRT Model for the Math Item Responses 
proc mcmc data=math_data outpost=mathcov_post seed=23 nbi=5000 nmc=40000  
   plots=(trace autocorr) nthreads=8; 
 
   array b[15]; array d[15]; array a[15]; array p[15]; 
   parms a1 1  d1 0; 
   parms a2 1  d2 0; 
   parms a3 1  d3 0; 
   parms a4 1  d4 0; 
   parms a5 1  d5 0; 
   parms a6 1; 
   parms a7 1  d7 0; 
   parms a8 1  d8 0; 
   parms a9 1  d9 0; 
   parms d10 0; 
   parms a11 1  d11 0; 
   parms a12 1  d12 0; 
   parms a13 1  d13 0; 
   parms a14 1  d14 0; 
   parms a15 1  d15 0; 
   beginnodata; 
   a10=1; d6=0; 
   endnodata; 
   parms sch_v theta_v .5 / slice; 
    
   prior d1-d5 d7-d15 ~ normal(0, var=4); 
   prior a1-a9 a11-a15 ~ normal(0, var=4, lower=0); 
   parms beta1 beta2 beta3 gamma0 gamma1 0;ߑ 
   prior sch_v theta_v ~ t(0,sd=2,1,lower=0); 
   prior beta: gamma: ~ normal(0,var=2);ߒ  
   school_l=gamma0 + gamma1*participate;ߓ 
   random sch_m ~ normal(school_l, var=sch_v) subject=school;
   student_l=sch_m + beta1*ses + beta2*isi + beta3*gender;ߔ 
   random theta ~ normal(student_l, var=theta_v) subject=_obs_; 
 
   do j=1 to 15; 
      p[j] = logistic(a[j]*theta - d[j]); b[j]=d[j]/a[j]; 
   end; 
   model ix1 ~ binary(p1); 
   model ix2 ~ binary(p2); 
   model ix3 ~ binary(p3); 
   model ix4 ~ binary(p4); 
   model ix5 ~ binary(p5); 
   model ix6 ~ binary(p6); 
   model ix7 ~ binary(p7); 
   model ix8 ~ binary(p8); 
   model ix9 ~ binary(p9); 
   model ix10 ~ binary(p10); 
   model ix11 ~ binary(p11); 
   model ix12 ~ binary(p12); 
   model ix13 ~ binary(p13); 
   model ix14 ~ binary(p14); 
   model ix15 ~ binary(p15); 
run; 
Output from PROC MCMC 
Output 6.12 presents a subset of results for the unconditional model: posterior summary statistics for the 
parameters of the distribution for θ at the school level and student level. You can compute from these 
values that the intraclass correlation is ~.27 or ~27% of the individual variance in θ is explained at the 
 

176 Bayesian Analysis of Item Response Theory Models Using SAS
 
school level (.276 / (.276 + .727)). Therefore, a relatively high proportion of the variance is explained by 
the grouping of students within schools. Notably, the mean values for the parameters are very similar to the 
values reported by Fox (2010), although the standard deviations for the sampled values are somewhat 
larger. 
Output 6.12: Posterior Summary Statistics for the Distribution Parameters—Unconditional Model 
Outputs 6.13, 6.14, and 6.15 present diagnostic plots (trace, autocorrelation, and density) for the overall 
mean ability and variance parameters in the unconditional model. All plots indicate convergence in the 
distributions. You can also see that the sampled values were mixing reasonably well, although there was 
high autocorrelation for the THETA_V parameter. This high autocorrelation caused the sampled values to 
traverse the parameter space slowly. Although some of the options in Chapter 3 could possibly be used to 
reduce the autocorrelation, the high autocorrelation does not impact posterior summaries for parameters 
(for example, means from posterior distributions). As for other models in this chapter, high autocorrelations 
simply indicate inefficiency in the chains or sampling process. The density plots for parameters reflect 
approximately normal distributions for the three parameters. 
Output 6.13: Diagnostic Plots for Gamma0—Unconditional Model 
 
 
 

Chapter 6: IRT Model Extensions   177
Output 6.14: Diagnostic Plots for SCH_V (School Level θθ Variance)—Unconditional Model 
 
Output 6.15: Diagnostic Plots for THETA_V (Student Level θ Variance)—Unconditional Model 

178 Bayesian Analysis of Item Response Theory Models Using SAS
 
Output 6.16 presents the posterior summary statistics for a subset of parameters in the conditional model: 
parameters of the θ distributions at the school (GAMMA0, SCH_V) and student levels (THETA_V), as 
well as the parameters for the covariates introduced to explain variability in θ at the school level 
(GAMMA1) and at the student level (BETA1, BETA2, BETA3). All covariates are significant based on the 
means and standard deviations. As discussed by Fox (2010), the school level covariate (PARTICIPATE) 
indicates that there was a significant and positive effect on math ability for schools that participated 
regularly in the examination program. At the student level, students with higher SES and ISI scores 
performed better on the math test, and gender also showed a significant effect with females outperforming 
boys. You can also see a reduction in the variance parameters (SCH_V and THETA_V) when comparing 
mean values in Outputs 6.12 and 6.16. This indicates a significant amount of the variability in school level 
math ability and student level math ability, was explained by the student and school level covariates. As for 
the unconditional model, the mean values for the covariate parameters are very similar to the values 
reported by Fox, but in contrast with the unconditional model, the standard deviations for the parameters 
are also similar. For more discussion of the results, the reader should consult Fox (2010). 
Output 6.16: Posterior Summary Statistics for the Distribution and Covariate Parameters—
Conditional Model 
Finally, Outputs 6.17 and 6.18 present diagnostic plots for the covariates introduced to explain school level 
and student level variability in θ. From the plots, you can see that the chains for the parameters reflected 
reasonable mixing behavior; that is, sampled values were traversing the parameter space reasonably 
quickly, although the chains for BETA0 manifested high autocorrelation. From the plots, you can conclude 
that the chains were converging to the target distribution. Note that plots for SCH_V and THETA_V were 
not included since they were similar to the plots for the unconditional model. 
 

Chapter 6: IRT Model Extensions   179
Output 6.17: Diagnostic Plots for School Level Covariates—Conditional Model 
Output 6.18: Diagnostic Plots for Individual Level Covariates—Conditional Model 
 

180   Bayesian Analysis of Item Response Theory Models Using SAS 
 
Differential Item Functioning—Multiple Group and Mixture IRT Models 
When evaluating the validity of inferences for scores, you should evaluate sample heterogeneity and 
whether items function differently across subpopulations of examinees. Heterogeneity in how groups of 
examinees interpret and respond to questions can result from a variety of possible sources (see, for 
example, Zumbo, 2007). In the context of IRT models, heterogeneity in response or differential item 
functioning (DIF) exists when the probability of a response is different across subgroups of examinees with 
the same status on the trait being measured. When DIF exists, separate IRT models should be estimated and 
reported for the subpopulations of examinees. 
A DIF analysis can evaluate measurement invariance across defined subpopulations or known groups based 
on observed variables, such as gender, and all members of a subpopulation are assigned to a particular 
group (such as males). For this analysis, a multiple-group analysis may be used to evaluate the presence of 
DIF. In this approach, two models are defined and compared for the subpopulations under study. One 
model defines unique model parameters for each subpopulation. To evaluate the presence of DIF, this 
model is compared with a model that constrains model parameters to be equal across subpopulations. 
A DIF analysis may also be used to detect and characterize subgroups of examinees from their 
performance. In this case, the source(s) of examinee item response heterogeneity is unobserved, and the 
assignment of examinees to latent classes is a random variable that is inferred from the item responses. For 
example, latent classes may consist of predominantly one subpopulation (for example, males) or mixtures 
of subpopulations (for example, males who graduated college). In the case where latent class membership 
is to be estimated, mixture IRT models may be used to cluster examinees into latent classes based on 
different examinee response patterns (Bolt, Cohen, & Wollack, 2001). Chapter 1 discusses other uses of 
mixture IRT models in testing applications. 
Multiple Group Models for Detecting DIF 
Any of the IRT models discussed can be used in a multiple-group paradigm to evaluate DIF across known 
subpopulations or groups (for example, male and female). In this approach, a model is first estimated in 
which unique sets of item parameters across G groups are estimated (unconstrained group model). Thus, 
the program would estimate item parameters or model item response probabilities for each subpopulation. 
A second model is estimated in which one set of item parameters are defined for all groups (constrained 
group model). The constrained and unconstrained model can be compared using procedures discussed in 
Chapter 7, and if the unconstrained model is preferred, then DIF would be indicated in the subpopulations.  
Program 6.7 presents a PROC MCMC program to estimate an unconstrained 1PL model using a modified 
version of the LSAT data. Program 6.8 presents a PROC MCMC program to estimate a constrained 1PL 
model. Subpopulations in the LSAT data were simulated by assigning each even observation to group 1 and 
each odd observation to group 2. Thus, no DIF would be expected under this simulated condition. Note that 
the group membership is saved in the variable GROUP in the simulated data set LSAT_G. The code for 
creating the LSAT_G data set is included in Program 6.7 on the authors’ web page for this book. 
You can see that in Program 6.7, for the unconstrained model, unique item parameters are defined for each 
of the two groups ߑand a single RANDOM statement is used to define the random effects (θ parameters) 
and prior distributions for the two groups ߒ. Because a single RANDOM statement is used with a single 
prior specification, the assumption is that the two subpopulations have the same distribution. This 
assumption is used to identify the model. 
In Program 6.8, for the constrained model, one set of item parameters are defined and used in the likelihood 
model for both groups ߑ. In this model, a single RANDOM statement is used to model the random effect θ 
parameters in the two groups or subpopulations, but now the means for the prior distributions for the 
groups are not assumed to be equivalent. This is accomplished by using an array of parameters for means of 
the group-level prior distributions in the random effects statement ߒ, where the parameter for the first 
group is fixed at 0 and a parameter (MU_G) is included to estimate the mean for the second group. As you 
can see in the program, the mean that is used for the prior distribution is determined by the group 
membership of each examinee. Estimating the mean for a group(s) allows for overall differences in the 
 

Chapter 6: IRT Model Extensions   181 
abilities or traits being measured between the two groups when estimating the item parameters. The fact 
that the item parameters are constrained to be equal across the groups serves to identify the model. 
Notably, in Program 6.8, two RANDOM statements cannot be specified with the option SUBJECT=_OBS_ 
for each group. This would produce twice as many random θ effect parameters because each RANDOM 
statement would produce 1000 samples of θ parameters at each iteration. However, two group variables 
(for example, GROUP1 and GROUP2) could be added to the data set where each variable contains the 
observation number if assigned to that group and 0 otherwise. For example, the even numbered 
observations were assigned to group 1. Therefore, values for the GROUP1 variable would equal the 
observation number if the observation was even numbered and 0 if the observation was odd numbered. 
These variables could then be used in two random statements as follows: RANDOM THETA1 ~ 
NORMAL(0, VAR=1) SUBJECT=GROUP1 ZERO=0 and RANDOM THETA2 ~ NORMAL(0, VAR=1) 
SUBJECT=GROUP2 ZERO=0. Here, the option ZERO=<’value’> is used to indicate that a random effect 
for the subject should not be sampled. While this strategy produces identical results as the strategy used in 
Program 6.8, it requires preprocessing the data set to create the required group variables. Further, the 
strategy used in Program 6.8 would appear to generalize more easily to more than two groups.  
Program 6.7: Unconstrained Multiple Group Model—LSAT Data Example 
proc mcmc data=lsat_g outpost=lsat_bayes_1p_unconstrained seed=23 
ntreads=8 nbi=5000 nmc=20000; 
   array b1[5]; array b2[5]; array x[5];   
   parms a1 a2 1; parms b1: b2: 0;  
   prior b1: b2: ~ normal(0, var=16); 
   prior a1 a2 ~ lognormal(0, var=9);  
   random theta ~ normal(0, var=1) subject=_obs_; ߒ 
   llike=0; 
   do j=1 to 5;                      
      if group=1 then do; ߑ 
         prob = logistic(a1*(theta-b1[j])); 
         llike = llike + x[j] * log(prob) + (1 - x[j]) * log(1 - prob); 
      end; 
      else do; 
         prob = logistic(a2*(theta-b2[j])); 
         llike = llike + x[j] * log(prob) + (1 - x[j]) * log(1 - prob); 
      end; 
   end;     
   model general(llike);    
run; 
Program 6.8: Constrained Multiple Group Model—LSAT Data Example 
proc mcmc data=lsat_g outpost=lsat_bayes_1p_constrained seed=23 nbi=5000 
nmc=20000 nthreads=8 plots=none diagnostics=none; 
   array b[5]; array x[5]; array mu[2] 0 mu_g;      
   parms a 1; parms b: 0;  
   prior b: ~ normal(0, var=16); 
   prior a ~ lognormal(0, var=9); 
   parms mu_g 0; 
   prior mu_g ~ normal(0, var=1);  
   random theta ~ normal(mu[group], var=1) subject=_obs_; ߒ 
   llike=0; 
   do j=1 to 5;                      
      prob = logistic(a*(theta-b[j]));ߑ 
      llike = llike + x[j] * log(prob) + (1 - x[j]) * log(1 - prob); 
   end; 
   model general(llike);    
run; 
Output 6.19 provides the posterior summary statistics for the parameters in the constrained group model. 
Although the output is not provided for the unconstrained group model, not unexpectedly, item parameter 
means for the unconstrained model were similar across the groups, which indicates measurement 
 

182 Bayesian Analysis of Item Response Theory Models Using SAS
 
invariance across the groups. If you compare the results in Output 6.19 with the results where no 
subpopulations are assumed (see Output 4.1 in Chapter 4), you will see that the means for the item 
parameters are very similar. In addition, MU2 was estimated to be approximately 0 in the constrained 
model in Output 6.19, indicating no differences in the means for the distributions of θ parameters that 
would be expected given the random process used to form the groups. 
Output 6.19: Posterior Summary Statistics for Parameters in the Constrained Multiple Group 
Model—LSAT Data Example 
 
Mixture IRT Models 
In IRT mixture model applications, differential performance and identification of latent classes can be 
examined by extending any of the IRT models to include group-level item parameters for each group or 
latent class g: Pj (Ujg = ujg | g, ωωjg, θg). In these models, the assumed IRT model holds across the latent 
classes, but each latent class has a unique set of model parameters that are estimated in addition to a model 
parameter reflecting the probability of latent class membership. Thus, mixture IRT models may be used to 
account for qualitative differences between examinees and examinesample heterogeneity (Rost, 1990). 
Program 6.9 presents the code to estimate a mixture model for a simulated data set. The data set was 
simulated using example 7.27 in Mplus (Muthén & Muthén, 2012), an example that illustrates the 
simulation and estimation of a mixture IRT model. The example program simulates 8 item responses and 2 
latent groups for 2000 observations (~1000 observations per group), where the slope and threshold 
parameters vary across groups. Mplus was also used to validate estimation of the mixture IRT using PROC 
MCMC. 
In Program 6.9, you can see that a 2P model is estimated with group- or class-specific item parameters. 
These parameters are referenced in the program using two-dimensional arrays indexed by the group or 
latent class assignment (2 levels) and the number of items (8) ߑ. In addition, a random effect (GROUP) is 
defined for each examinee to estimate the probability of being assigned in each latent class ߒ. The prior 
distribution for the group random effects is specified as a discrete distribution with parameter PI (TABLE 
distribution in PROC MCMC). The parameter PI is an array of size two corresponding to the probabilities 
of being assigned to the two latent classes. The prior distribution for this parameter is a Dirichlet 
distribution, a commonly used prior for a discrete distribution ߓ. A single RANDOM statement is used to 
define the θ random effects and prior distributions for the two groups ߔ. Because a single RANDOM 
statement is used, the assumption is that the two subpopulations have the same distribution. This 
assumption is used to identify the model. Lastly the response probability model reflects estimation of the 
item parameters for each of the two latent classes ߕ. 

Chapter 6: IRT Model Extensions   183 
Notably, a BINARY distribution could have been specified in the RANDOM statement because only two 
groups were modeled. However, using the TABLE distribution allows for generalizing the program to 
applications that involve more than two groups.  
Program 6.9: Mixture IRT Model—Simulated Item Responses 
proc mcmc data=mixture_data outpost=mixture_post seed=23 nbi=5000 
nmc=30000 monitor=(a b pi) nthreads=8 plots=(trace autocorr); 
 
   array b[2,8]; array a[2,8]; array d[2,8]; ߑ 
   array p[8]; array pi[2]; 
   parms a: 1; 
   parms d: 1; 
   parms pi;  
   array prob[2] (.5 .5); 
       
   prior d: ~ normal(0, var=4); 
   prior a: ~ normal(0, var=2, lower=0); 
   prior pi ~ dirich(prob); ߓ 
   random group ~ table(pi) subject=_obs_; ߒ 
   random theta ~ normal(0, var=1) subject=_obs_;ߔ 
 
   do j=1 to 8; 
      p[j] = logistic(a[group,j]*theta - d[group,j]);ߕ 
      b[group,j]=d[group,j]/a[group,j]; 
   end; 
   model ix1 ~ binary(p1); 
   model ix2 ~ binary(p2); 
   model ix3 ~ binary(p3); 
   model ix4 ~ binary(p4); 
   model ix5 ~ binary(p5); 
   model ix6 ~ binary(p6); 
   model ix7 ~ binary(p7); 
   model ix8 ~ binary(p8); 
run; 
Output 6.20 presents posterior summary results, using the autocall macro %POSTSUM, for a subset of the 
model parameters in the mixture IRT model. In this output, the results are presented for the slope and 
threshold parameters for Items 1−3, corresponding to the first three items for latent class 1, and the slope 
and threshold parameters for Items 9−11, corresponding to the first three items for latent class 2. In 
addition, the posterior summary results are presented for the latent class probability parameters (pi1−pi2). 
Note that the posterior summary results closely approximate an analysis of the same data set using Mplus. 
From the output, you can see that both the slope and threshold parameters differ markedly between the two 
latent classes as was designed in the simulation of the item responses. You can also see that the latent class 
probability parameters, pi1 and pi2, were estimated at approximately .5, which again reflected the number 
of simulated observations in each class. Finally, you can see that the standard deviations for the latent class 
parameters are very small, indicating a high degree of certainty in the latent class assignments. Examining 
the GROUP random effects for simulated examinees confirmed that nearly all of the examinees were 
assigned to a single latent class across the iterations. Further processing of the posterior data revealed that 
the average probabilities across examinees and iterations for each latent class exceeded .95.  
 

184 Bayesian Analysis of Item Response Theory Models Using SAS
 
Output 6.20: Posterior Summary Results for a Subset of Parameters for the Mixture IRT Model 
Outputs 6.21 through 6.26 present the diagnostic plots for the item parameters for one item from the set of 
items estimated for each latent class (a1, b1, a9, b9). In addition, diagnostic plots are presented for the latent 
class probability parameters (pi1−pi2). You can see that the item parameters are mixing reasonably well, 
although high autocorrelation is observed, while the trace plots for the latent class probability parameters 
indicate excellent mixing in the chains and low dependence among the sampled values. All parameters 
have probability densities that are approximately normal. 
Several other mixture models can be estimated. For example, models with different numbers of latent 
classes can be estimated and compared in order to determine the optimal number of latent classes (class 
enumeration). Model comparison methods can be used for this purpose (see Chapter 7 for a discussion of 
the methods). Also, the structure of the latent trait being measured can be defined by class-specific means 
and standard deviations (μg, σg). In the above program, the distributions for the classes are identical, so one 
RANDOM statement is used to specify the random effects in the model. However, models could be 
specified with parameters of the class-specific distributions estimated using the same coding scheme in 
Program 6.8. However, a strategy, similar to that used in the previous section of this chapter with 
hierarchical IRT models, would be required to identify the model. That is, constrain item parameters—fix 
one slope parameter for an item at 1 and one intercept parameter for an item at 0 for the set of item 
parameters associated with each latent class. 
Lastly, some caution should be exercised when estimating mixture models. Sawatzky, Ratner, Kopec, & 
Zumbo (2012) caution that sample heterogeneity can result from other factors beyond DIF. For example, 
sample heterogeneity could result from non-normality in the distributions of variables or from incorrectly 
specified models (multidimensionality in item responses exists). Also, the problem of “label switching,” 
discussed in the context of bifactors models earlier in this chapter, is a particular problem with mixture 
models (Jasra, et al., 2005). In the present analysis, the item responses for the different latent classes were 

Chapter 6: IRT Model Extensions   185
well defined. In the case when the latent classes are less well defined, label switching may occur. For more 
information about this issue, see the section “The Problem of ‘Label Switching’” in this chapter. 
Output 6.21: Diagnostic Plots for Item 1 (a1 Latent Class 1) for the Mixture IRT Model 
 
Output 6.22: Diagnostic Plots for Item 1 (a9 Latent Class 2) for the Mixture IRT Model
 

186 Bayesian Analysis of Item Response Theory Models Using SAS
 
Output 6.23: Diagnostic Plots for Item 1 (b1 Latent Class 1) for the Mixture IRT Model 
Output 6.24: Diagnostic Plots for Item 1 (b9 Latent Class 2) for the Mixture IRT Model 

Chapter 6: IRT Model Extensions   187
Output 6.25: Diagnostic Plots for PI1 (Probability of Latent Class 1) for the Mixture IRT Model 
 
Output 6.26: Diagnostic Plots for PI2 (Probability of Latent Class 2) for the Mixture IRT Model 
 
 
 

188   Bayesian Analysis of Item Response Theory Models Using SAS 
 
 
 

Chapter 7: Bayesian Comparison of IRT Models 
 
Introduction ....................................................................................................... 189 
Bayesian Model Comparison Indices ................................................................. 190 
Deviance Information Criterion ......................................................................................... 190 
Conditional Predictive Ordinate ........................................................................................ 190 
Computing Model Comparison Statistics in PROC MCMC ........................................... 191 
Example 1: Comparing Models for Dichotomously Scored Items (LSAT Data) ... 193 
DIC Results.......................................................................................................................... 193 
CPO Results ........................................................................................................................ 194 
Example 2: Comparing GR and RS-GR Models for Polytomously Scored Items 
(DASH Item Responses) ..................................................................................... 196 
DIC Results.......................................................................................................................... 196 
CPO Results ........................................................................................................................ 197 
Example 3: Comparing a Unidimensional IRT Model and a Bifactor IRT Model.. 200 
DIC Results ......................................................................................................................... 200 
CPO Results ........................................................................................................................ 201 
 
Introduction 
In many testing applications, different models may be estimated that reflect competing theoretical 
perspectives about the nature of the item responses and therefore competing model specifications. For 
example, the three dichotomously scored models discussed in Chapters 1 and 4 reflect different 
assumptions about the nature of the items (item characteristics), persons (person characteristics), and the 
prediction of item responses. Model comparison methods can be used to evaluate competing models and to 
decide which assumptions about the item and person characteristics that are modeled are tenable and which 
assumptions are not tenable.  
This chapter begins with a description of two indices that can be used with the MCMC method. This is 
followed by three examples that compare competing models from Chapters 4, 5, and 6. In the programs for 
each of these examples, a random sample from the joint posterior distribution for model parameters is used 
to compute some of the model comparison indices. Certainly all iterations from the posterior distribution 
can be processed, but because likelihood results for the model comparison indices are summarized across 
persons and items (using means), processing all iterations in the joint posterior distribution is not 
necessarily required. Thus, the programs illustrate how to draw a random sample from the posterior 
distribution using PROC SURVEYSELECT. Furthermore, you will see that the programs differ primarily 
with respect to the IRT model that is used to compute the likelihood of response data. Thus, it is 
straightforward to use any of the illustrated programs for computing the model comparison statistics in 
other model comparison applications. 

190   Bayesian Analysis of Item Response Theory Models Using SAS 
Bayesian Model Comparison Indices 
As discussed in Chapter 2, in a Bayesian paradigm, the model comparison indices are based on information 
criteria and on the Bayes factor. These classes of indices can be used to assess the relative fit between 
competing models but do not address the absolute fit of any one particular model or the degree to which 
any one particular model best approximates the data. Absolute model fit in a Bayesian paradigm is 
generally assessed by using posterior predictive model checks (PPMC). PPMC analysis is considered in 
detail in Chapter 8.  
Deviance Information Criterion 
The Deviance Information Criterion (DIC; Spiegelhalter et al., 2002) is a model comparison tool similar to 
the well-known likelihood-based information criteria: Akaike Information Criterion (AIC; Akaike, 1974) 
and the Bayesian Information Criterion (BIC; Schwarz, 1978). DIC can be used to compare nested or non-
nested models, as well as models that have non-independent-and-identically-distributed variables.  
Similar to the use of other information criteria indices in identifying a preferred model, the DIC index 
weights both model fit based on likelihood functions and model complexity or the number of model 
parameters. DIC was developed because MCMC estimation uses prior information, and the actual number 
of parameters cannot be clearly identified as for AIC and BIC indices. The DIC index is widely used with 
Bayesian estimation and is defined as follows: 
ܦܫܥ= ܦഥ(઼) + ݌஽ 
where ܦഥ(઼) is the posterior mean of the deviance across iterations and measures how well the data fits the 
model using the log likelihood function (−2log L(D | δδ) ) and the sampled IRT model parameter values (δ) 
at each iteration; and ݌ܦ is used to correct for model complexity or overfitting; ݌ܦ is computed by the 
difference between ܦഥ(઼) and the deviance based on the log likelihood function at the posterior mean of 
model parameter estimates, ܦ(Ɂത).  
When interpreting DIC values across competing models, smaller values of DIC indicate better relative fit. 
However, there is no strict hypothesis test that can be performed to compare DIC values. Instead, the 
following guidelines for interpreting differences in DIC for models have been discussed: differences of 
more than 10 suggest “important” differences; differences between 5 and 10 reflect “substantial” 
differences; and differences that are less than 5 indicate similar model fit (Lunn, Jackson, Best, Thomas, & 
Speigelhalter, 2013). A final cautionary note is that DIC should be used cautiously with missing data 
because the likelihood is not uniquely defined (Lunn et al., 2013). In addition, DIC assumes that the 
posterior mean is an accurate and appropriate estimate from the posterior distribution. If this is not the case, 
as when the posterior density reflects extreme skewness or bimodality, the DIC should probably not be 
used.  
Conditional Predictive Ordinate 
A common approach to selecting a preferred model from two competing models in a Bayesian analysis is to 
compute the Bayes factor (BF). The BF is defined as the posterior odds of Model 1 in relation to Model 2 
divided by the prior odds for Model 1 to Model 2. This ratio reduces to the ratio of marginal likelihoods of 
the data across the parameter space for each model.  
However, computation of the Bayes factor has both practical and theoretical limitations, including 
computation when the prior is vague and computation for highly parameterized models. In order to 
overcome these problems, alternative criteria called Pseudo–Bayes factors (PsBF) have been proposed as 
surrogates for BF (Geisser & Eddy, 1979; Gelfand, Dey & Chang, 1992; Kim & Bolt, 2007). PsBF indices 
approximate the marginal densities in the BF using conditional predictive ordinates (CPO). As discussed in 
Chapter 2, the CPO index for any one model can be defined as ܥܱܲெ= ς
(ܥܱܲ௥| ܯ)
ோ
௥ୀଵ
, and a model with 
larger CPO value is preferred.  
 

Chapter 7: Bayesian Comparison of IRT Models   191 
In the context of IRT estimation with MCMC methods, the CPO statistics are first estimated at the level of 
an individual item response, using the inverse likelihood of each observation for T samples in the joint 
posterior distribution: 
ܥܱܲ௜௝= ൭1
ܶ ෍
1
݌൫ݕ௜௝ ห઼௧)
்
ଵ
൱
ିଵ
 
where T is the number of samples from the chain, and p(yij |  δ t) is the likelihood of the response data for an 
examinee i and item j (yij) based on the sampled item and person parameter values at time t (δt). Given the 
likelihood values for each examinee, CPOij is computed by the inverse of the average of the inverse 
likelihoods across the T draws.  
An item-level CPO index for a model can be computed by summarizing the product of the values of CPOij 
across all N examinees:  
ܥܱܲ௝= ෑ
ܥܱܲ௜௝
ே
௜ୀଵ
  
A CPO index for the overall test can then be computed by summarizing the product of the item-level CPOj 
across all J items:  
ܥܱܲ= ෑ
ܥܱܲ௝
௃
௝ୀଵ
  
Note that the natural logarithm of the CPO index was used for comparing models in this chapter. Therefore, 
the product terms above become summations over persons and over items. The preferred model is the 
model with the larger CPO value.  
Computing Model Comparison Statistics in PROC MCMC 
To output the DIC statistics in PROC MCMC, the DIC keyword is added to the list of options on the PROC 
MCMC command (for example, DIAGNOSTICS=<list>, NBI=<value>, NMC, DIC). In the PROC 
MCMC output, several components to the DIC statistic are provided: (1) Dbar corresponds to the mean 
deviance measure across iterations and is based on sampled parameter values for each iteration (ܦഥ(઼)); (2) 
Dmean corresponds to the deviance measure based on the means from the parameter posterior distributions 
or the “best” single model (ܦ(઼෡)); (3) pd is the effective number of parameters (݌ܦ); and (4) DIC is the 
index that may be used to compare models.  
In order to compute the CPO index for the overall test and the item-level CPOj for each of the J items, a 
SAS program must be written that processes the OUTPOST data set from a Bayesian analysis using PROC 
MCMC. A template for the program is presented below so that you can see the link between the 
programming code and the formulae presented above. The first step in the process involves linking sampled 
values from one iteration in the joint posterior distribution for the model parameters (item and person 
parameters) to the observed data for each examinee ߑ. This allows for computing the likelihood of the item 
response data for each examinee using the sampled parameter values in each iteration (݌൫ݕ௜௝ ห઼௧))  ߒ and 
accumulating results across iterations (σ 1/ ݌൫ݕ௜௝ ห઼௧)
்
ଵ
) ߓ. The next steps summarize results across 
iterations to obtain the CPO at the level of the individual item response (CPOij) ߔ, and then summarize 
CPOij results across examinees for each item (item-level CPO index or CPOj) and across items (test-level 
CPO index or CPO)ߕ. 
 
 
 

192   Bayesian Analysis of Item Response Theory Models Using SAS 
/* STEP 1: Compute CPO(i,j) for each posterior sample in OUTPOST  
           Create a dataset with the observed data and results from a 
           single iteration of posterior results – sampled values */ 
 
data one_iteration;ߑ 
 
   <read in one iteration of R posterior samples from OUTPOST=dataset> 
   <read in observed data with N response patterns from DATA=dataset> 
   <attach observed data to the posterior sample so that each examinee  
           in the observed data has the sampled values of item and  
           person parameters> 
 
Data cpo_i_j; 
   Set one_iteration; 
   do j=1 to &nitems; ߒ 
      <compute inverse of log likelihood of response for each person and 
      for each item using posterior samples for the parameters – item and  
      person parameters> 
   end; 
run; 
 
/* Accumulate results from each iteration in OUTPOST to a base SAS  
   dataset – dataset of accumulated results has N x R observations */ 
 
proc append base=<accumulated cpo_i_j results> data=cpo_i_j;ߓ 
run; 
 
<Repeat Step 1 across R iterations in OUTPOST> 
 
 
/* STEP2: compute item-level CPOj and test-level CPO */ 
          Compute means of log likelihood statistics for each person –  
          summarizes the log likelihood statistic for each person across 
          posterior iterations */ 
 
proc means data=<accumulated cpo_ij results>;ߔ 
     class <variable defining person observation>; 
     var <variable list of log likelihood statistics for each item>; 
     output out=<output dataset of means> Mean=; 
run; 
 
/* compute CPO index for each item */ 
data <cpo_i_j dataset of CPO values for each item>; 
     set =<output dataset of means>; 
     do j=1 to &nitem; 
       <compute CPO index for each item> 
     end; 
run; 
 
 
/* STEP 3: Summarize the CPO index across persons for each item and 
across items */  
proc summary data=<cpo_i_j dataset of CPO values for each item> ߕ 
     var <var list of cpo statistics for each item>; 
     output out=<dataset of CPO indices> SUM=<var list>; 
run; 
 
/* save item-level and compute test-level CPO results in file */ 
data <dataset of CPO indices>; 
     set =<dataset of CPO indices>; 
     CPO_test=sum(of CPO_item1-CPO_item&nitem); 
run;  
 

Chapter 7: Bayesian Comparison of IRT Models   193 
Example 1: Comparing Models for Dichotomously Scored Items (LSAT 
Data) 
For dichotomously scored items, it is useful to compare the application of the 1P, 2P, and 3P models to 
item response data in order to evaluate the extent to which different model specifications and item 
characteristics capture the response patterns. In particular, we are interested in whether separate slope 
and/or guessing parameters are useful in describing the item responses. The estimation of the three models 
is illustrated using the LSAT data analyzed in Chapters 3 and 4.  The example in this section will compare 
the three models using the DIC and CPO indices to examine which model conforms better to the LSAT 
data. 
DIC Results 
Table 7.1 provides the DIC statistics and related components from PROC MCMC for each of the three 
models. Based on the DIC statistics and the above guidelines, the preferred model for the LSAT item 
responses would be the 2P IRT model because this model reflects the smallest DIC value, and the 
difference between the DIC values for the 2P model and the other two models is greater than 10.  
This conclusion is somewhat different from the discussion in Chapter 4 that compared descriptively the 
model parameter estimates across the three models (see section “Comparison of Results Based on MML 
Estimation”). Recall, however, that there was some modest variability in the slope parameter estimates in 
the 2P model that might also support selecting the 2P model as the preferred model (range in slope 
SDUDPHWHUVZDVí$VIRUWKH3PRGHOUHFDOOIURP&KDSWHUWKDWLWFRXOGEHDUJXHGWKDWRIWKH
5 guessing parameters (cj) could be fixed at 0.  This would suggest that a 2P model was more appropriate 
for these items. 
Table 7.1 DIC Results Comparing the 1P, 2P, and 3P Models—LSAT data 
DIC Results 
1P 
2P 
3P 
Dbar 
4,589.98 
4,568.66 
4,565.01 
Dmean 
4,288.84 
4,270.85 
4,238.37 
pd 
   301.16 
   297.82 
   326.64 
DIC 
4,891.16 
4,866.47 
4,891.65 
 
The components of the DIC statistic can also be examined. Given that the deviance measure underlying 
Dmean and Dbar FRUUHVSRQGVWRWKHVWDWLVWLFíîORJOLNHOLKRRGDFFXPXODWHGDFURVVREVHUYDWLRQVWKHVH
statistics can also be compared for nested models. Dbar measures how well on average the model describes 
the data, whereas Dmean measures how well the “best” model (based on point estimates for the model 
parameters) describes the data. For both of these statistics, smaller values are observed from the 1P to 3P 
models. This indicates improvement in relative fit across the models. Recall that for both of these statistics, 
however, the statistics do not account for model complexity as in the DIC statistic.  
In addition, the difference in Dmean for nested models is argued to be distributed as chi-square with df 
equal to the difference in the number of estimated parameters (Spiegelhalter et al., 2002). In comparing the 
1P and 2P models, the difference chi-square is 17.99 with 5 df, corresponding to the 5 slope parameters that 
are estimated in the 2P model. This difference chi-square is significant (p < .05), indicating that the 
additional slope parameters are contributing significantly to the fit of the model. However, while this 
comparison is consistent with the interpretation of the DIC values, comparison of the Dmean values for the 
2P versus 3P models, indicates that the added cj parameters also contribute significantly to model fit – 
difference chi-square = 32.48 with 5 df. Thus, it is not entirely clear whether the difference in the Dmean 
statistic for nested models can be used for model comparisons, at least with the LSAT data. 
Finally, an examination of the pd values, reflecting the number of effective parameters, represents a 
seemingly paradoxical result. As you can see, pd is lowest for the 2P model, a model that includes more 
parameters than the 1P model. Recall that, by definition, pd is the difference between the “best” model 
(Dmean) and the average model (Dbar) across iterations. If pd reflected the true difference in parameters 
between the 1P and 2P models (5), Dmean might be expected to be greater for the 2P model. Because 
 

194   Bayesian Analysis of Item Response Theory Models Using SAS 
Dmean may be lower than expected, the additional slope parameters may be providing more information 
for modelling the data. This may provide additional support for the use of a 2P model with the LSAT data.  
CPO Results 
Program 7.1 presents the SAS program used to obtain the CPO results for the 1P model. The program 
assumes the observed data (LSAT) and the samples from the posterior distribution (OUTPOST file from 
PROC MCMC) are available in the work library. Control parameters are specified ߑ and labels for the 
parameters in the response probability models are defined as follows: the slope parameter is labeled a1 to 
an, threshold parameters b1 to bn, and guessing parameters c1 to cn, where n references the number of items 
ߒ Data sets to be processed are established by saving the observed data (LSAT) in the SAS data set 
RESP_DATA, and saving a random subset (2000 samples) of posterior estimates from the OUTPOST data 
set in the SAS data set OUTPOST_SAMPLE ߓ. A macro CPO_DI is used to compute the item-level and 
test-level CPO values for dichotomous models following the three steps in the previous template ߔ. For 
each observation in OUTPOST_SAMPLE, the program specifies the necessary model parameters (sampled 
item and person parameters) in the DATA step DATA ONE ߕand the sampled values are used in the 
DATA step DATA CPO_IJ to compute the likelihood of the response data for each observation ߖ. These 
results are accumulated in the SAS data set ALLCPO_IJ. Item level and test level CPO indices are then 
computed from the results in ALLCPO_IJ ߗand the results are summarized and saved in the data set 
CPO_RESULT ߘ.  
The program specifies the three response probability models for the 1P, 2P, and 3P models ߙ. In order to 
run the program for the 2P or 3P model, you need to change the macro variable MODEL to the correct 
model, change the macro variable ITEMPARS to the correct variable list, and change any required file 
specifications. 
Program 7.1: Computing the CPO Statistics for the 1P Model—LSAT Data 
/* Control Settings for the Macro – USER DEFINED*/ 
 
%let model=1;          /* 1=1P 2=2P 3=3P */ ߑ 
%let nperson=1000;     /* number of examinees */ 
%let nitem=5;          /* number of items */ 
%let postsamples=2000; /* subset of OUTPOST iterations to use in CPO */ 
%let itempars=a b1-b5; /* item parameters in OUTPOST */ߒ 
 
/* Create SAS datasets to process – observed data and subset of 
iterations from OUTPOST dataset */ ߓ 
data resp_data; 
  set lsat; 
run; 
proc surveyselect data=lsat_bayes_1p method=SRS rep=1  
  sampsize=&postsamples seed=12345 out=outpost_sample; 
  id _all_; 
run; 
 
%macro CPO_di;ߔ  
 
/* STEP1: compute CPO(i,j)for each iteration of posterior distribution */ 
proc datasets; delete allCPO_ij; run; 
 
%do mc=1 %to &postsamples;  
/* Attach posterior results to each record of the observed dataset */ 
/* Repeat the process for each iteration of the posterior distribution */ 
data one; ߕ 
   if _n_ = 1 then set outpost_sample(firstobs=&mc obs=&mc); 
   set resp_data; 
   array theta_{*} theta_1-theta_&nperson; 
   person=_n_; 
   theta=theta_[person]; 
   keep person x1-x&nitem &itempars theta; 
run; 
 
 

Chapter 7: Bayesian Comparison of IRT Models   195 
data cpo_ij;ߖ 
   sample=&mc; 
   set one; 
   array x{&nitem} x1-x&nitem; 
   array a_{&nitem} a1-a&nitem; 
   array b{&nitem} b1-b&nitem; 
   array c{&nitem} c1-c&nitem; 
   array p{&nitem} p1-p&nitem; 
   array l{&nitem} l1-l&nitem; 
   array inl{&nitem} inl1-inl&nitem; 
   do j=1 to &nitem; 
      if &model = 1 then p[j]=logistic(a*(theta – b[j]));ߙ 
      else if &model = 2 then p[j]=logistic(a_[j]*(theta – b[j])); 
      else if &model = 3 then  
            p[j]=c[j]+(1-c[j])*logistic(a_[j]*(theta – b[j])); 
      l[j]= p[j]**x[j]*(1-p[j])**(1-x[j]); 
      inl[j]=1/l[j]; 
   end; 
   keep sample person inL1-inL&nitem; 
run; 
 
/* Accumulate results from each iteration of posterior distribution */ 
proc append base=allcpo_ij  data=cpo_ij; 
run; 
%end; 
 
/* STEP2: compute item-level CPOj and test-level CPO for each person */ ߗ 
proc sort data=allcpo_ij;  
  by person sample; 
run; 
proc means data=allcpo_ij nway noprint; 
  class person; 
  var inL1-inL&nitem; 
  output out=allcpo_ij1 Mean=; 
run; 
data allcpo_ij1; 
  set allcpo_ij1; 
  array inL{&nitem} inL1-inL&nitem; 
  array logcpo_ij{&nitem} logcpo_ij1-logcpo_ij&nitem; 
  do j=1 to &nitem; 
     logcpo_ij[j]=log(inL[j]**(-1)); /* scale as log likelihood result */ 
  end; 
  keep person logcpo_ij1-logcpo_ij&nitem; 
run; 
 
/* STEP 3: Summarize CPO results across persons and items */ߘ 
proc summary data=allcpo_ij1; 
   var logcpo_ij1-logcpo_ij&nitem; 
   output out=CPO_item SUM=CPO_item1-CPO_item&nitem; 
run; 
 
/* Save item-level and test-level CPO results in file: Work.CPO_result */ 
data CPO_result; 
  set CPO_item; 
  CPO_test=sum(of CPO_item1-CPO_item&nitem); 
run; 
 
%mend; 
 
/* Run CPO for 1P model – macro assumes datasets are in Work Library */ 
%CPO_di;  
 
 

196   Bayesian Analysis of Item Response Theory Models Using SAS 
Table 7.2 presents the item-level and test-level CPO results for the three IRT models based on a random 
sample of iterations from the joint posterior distribution for the model parameters. The test-level CPO 
results indicate that the 1P model is preferred for the LSAT item responses because the CPO value is 
largest for the 1P. By comparing the three item-level CPO values for each item, we can see that the 1P 
model is also the preferred model for each LSAT item.  
Table 7.2: CPO Indices for the Three IRT Models—LSAT Data 
CPO results 
Item 1 
Item2 
Item 3 
Item 4 
Item 5 
Test 
1P 
Ѹ263.9 
Ѹ593.0 
Ѹ674.5 
Ѹ538.6 
Ѹ380.2 
Ѹ2450.2 
2P 
Ѹ264.8 
Ѹ594.0 
Ѹ675.9 
Ѹ540.9 
Ѹ382.6 
Ѹ2458.2 
3P 
Ѹ265.3 
Ѹ594.5 
Ѹ676.4 
Ѹ540.9 
Ѹ382.4 
Ѹ2459.6 
 
Based on the DIC statistic and the CPO statistic, the preferred model differs with the LSAT data. The 
inconsistent performance of the different model-comparison indices (DIC as opposed to CPO statistics) has 
been found in the literature. In particular, DIC has been shown to prefer a more complex model which may 
be the situation with the LSAT data (2P over the 1P model). As argued above, the differences among the 
item slope parameter estimates in the 2P model were modest, indicating that the 1P model should be 
adequate for describing the LSAT data. Interestingly, using the model comparison procedures available in 
IRTPRO, the preferred model was the 1P model. Thus, for the LSAT data, the CPO indices seem to more 
consistent with an MML paradigm. 
Example 2: Comparing GR and RS-GR Models for Polytomously 
Scored Items (DASH Item Responses) 
For polytomously scored items, it may be useful to compare GR models that specify a common slope 
parameter across items as opposed to a model where separate slope parameters are estimated. However, for 
rating scale item applications, it may be of particular interest to compare a model that specifies separate 
response scales for items (GR) as opposed to a model that specifies a common response scale across items 
(RS-GR). This comparison determines the extent to which respondents are interpreting the scale and 
response options similarly across the items. Because the DASH data set reflects rating scaled items, 
comparing the GR and RS-GR models with the DASH data is used to illustrate model comparisons with 
polytomously scored items (see Chapter 5 for the estimation of polytomously scored models with the 
DASH data).  
DIC Results 
Table 7.3 provides the DIC statistics and related components from PROC MCMC for the GR and RS-GR 
models. Based on the DIC statistics and the above guidelines for comparing DIC statistics, the preferred 
model for the DASH item responses would be the GR model as opposed to the RS-GR model, because the 
GR model reflects the smaller DIC value and the difference between the DIC values is greater than 10. 
Notably, the GR model reflected a standard model specification with no decomposition of the threshold 
parameter (see Chapter 5 for detail). A GR model where the threshold parameter was decomposed into a 
location and category threshold parameters was also examined. This model yielded nearly identical DIC 
results (DIC = 17655.47) as the standard GR model. 
 
 
 

Chapter 7: Bayesian Comparison of IRT Models   197 
Table 7.3: DIC Results for the GR and RS-GR Models—DASH Data 
DIC Results 
GR 
RS-GR 
Dbar 
1,7654.34 
1,7776.52 
Dmean 
1,6670.47 
1,6811.84 
pd 
     983.88 
     959.88 
DIC 
1,8638.22 
1,8731.20 
Recall that Dbar measures how well on average the model describes the data, whereas Dmean measures 
how well the “best” model (based on point estimates for the model parameters) describes the data. For both 
of these statistics smaller values are observed for the GR model, which indicates improvement in relative fit 
over the RS-GR model. Comparison of the pd values, or effective number of model parameters, is 
consistent with a comparison of the two models in this example. The more complex model, the GR model 
in this case, has a substantially higher pd value since category threshold parameters are estimated 
separately for each item. 
CPO Results 
Program 7.2 presents the SAS program for computing the CPO indices for the GR model. The program 
assumes the observed data (DASH_SAMPLE) and the samples from the posterior distribution (OUTPOST 
file from PROC MCMC) are available in the work library. The control parameters have been changed at 
the beginning of the program to reflect changes in the number of items and the variable list for item 
parameters ߑ. The program defines labels for the parameters in the GR model as follows: slope parameters 
are labeled a1 to an, and threshold parameters b1 to bn ߒ. You can also see that a standard GR model is 
specified with no decomposition of the threshold parameters. This is consistent with the analyses deriving 
the DIC statistics. Data sets to be processed are established by saving the observed data (DASH_SAMPLE) 
in the SAS data set RESP_DATA, and a random subset (2000 samples) of posterior estimates from the 
OUTPOST data set in the SAS data set OUTPOST_SAMPLE ߓ. For each observation in 
OUTPOST_SAMPLE, the program specifies the necessary model parameters (sampled item and person 
parameters) in the DATA step DATA ONE ߔand the sampled values are used in the DATA step DATA 
CPO_IJ to compute the likelihood of the response data for each observation ߕ. These results are 
accumulated in the SAS data set ALLCPO_IJ. Item level and test level CPO indices are then computed 
from the results in ALLCPO_IJ ߖand the results are summarized and saved in the data set 
CPO_RESULT ߗ.  
The likelihood function reflects the response probabilities for the GR model ߘ. To compute the CPO 
statistics for the RS-GR model, you need to change the parameter specifications throughout the program 
and the likelihood model in the DATA step DATA CPO_IJ in Program 7.2.  
 
 
 

198   Bayesian Analysis of Item Response Theory Models Using SAS 
Program 7.2: Program to Calculate CPO Indices for the GR Model 
/ * Control Settings for the Macro */ 
%let nperson=1000; /* number of examinees */ߑ 
%let nitem=10;     /* number of items */ 
%let postsamples=2000; /* subset of OUTPOST iterations to use in CPO */ 
%let itempars=a1-a&nitem b1_1-b1_&nitem b2_1-b2_&nitem b3_1-b3_&nitem 
b4_1-b4_&nitem;ߒ 
 
/* Create SAS datasets to process – observed data and subset of 
iterations from OUTPOST dataset */ ߓ 
data resp_data; 
  Set dash_sample; 
run; 
proc surveyselect data= dash_bayes_gr method=SRS rep=1  
  sampsize=&postsamples seed=12345 out=outpost_sample; 
  id _all_; 
run; 
 
%macro CPO_gr;  
 
/* STEP1: compute CPO(i,j)for each iteration of posterior distribution */ 
proc datasets; delete allCPO_ij; run; 
 
%do mc=1 %to &postsamples; 
/* Attach posterior results to each record of the observed dataset */ 
/* Repeat the process for each iteration of the posterior distribution */ 
data one;ߔ 
  if _n_ = 1 then set outpost_sample(firstobs=&mc obs=&mc); 
  set resp_data; 
  array theta_{*} theta_1-theta_&nperson; 
  person=_n_; 
  theta=theta_[person]; 
  keep person i1-i&nitem &itempars theta; 
run; 
 
data cpo_ij;ߕ 
  sample=&mc; 
  set one; 
  array i{&nitem} i1-i&nitem; 
  array a{&nitem} a1-a&nitem; 
  array b[4,&nitem] b1_1-b1_&nitem  b2_1-b2_&nitem b3_1-b3_&nitem  
        b4_1-  b4_&nitem; 
  array p_star[&nitem,4];  
  array p[&nitem,5]; 
  array L{&nitem} L1-L&nitem; 
  array inL{&nitem} inL1-inL&nitem; 
 
  do j=1 to &nitem; 
    do k=1 to 4; 
       p_star[j,k]=logistic(a[j]*(theta-b[k,j]));ߘ 
    end; 
    p[j,1]=1-p_star[j,1]; 
    do k=2 to 4; 
       p[j,k]=p_star[j,(k-1)]-p_star[j,k]; 
    end; 
    p[j,5]=p_star[j,4]; 
    L[j]= p[j,i[j]]; 
    inL[j]=1/L[j]; 
  end; 
  keep sample person inL1-inL&nitem; 
run; 
 
 
 
 

Chapter 7: Bayesian Comparison of IRT Models   199 
/* Accumulate results from each iteration of posterior distribution */ 
proc append base=allcpo_ij  data=cpo_ij; 
run; 
%end; 
 
/* STEP2: compute item-level CPOj and test-level CPO for each person */ ߖ 
proc sort data=allcpo_ij; by person sample; run; 
proc means data=allcpo_ij nway noprint; 
  class person; 
  var inL1-inL&nitem; 
  output out=allcpo_ij1 Mean=; 
run; 
data allcpo_ij1;  
  set allcpo_ij1; 
  array inL{&nitem} inL1-inL&nitem; 
  array logcpo_ij{&nitem} logcpo_ij1-logcpo_ij&nitem; 
  do j=1 to &nitem; 
     logcpo_ij[j]=log(inL[j]**(-1)); 
  end; 
  keep person logcpo_ij1-logcpo_ij&nitem; 
run; 
 
/* STEP 3: Summarize CPO results across persons and items */ ߗ 
proc summary data=allcpo_ij1; 
  var logcpo_ij1-logcpo_ij&nitem; 
  output out=CPO_item SUM=CPO_item1-CPO_item&nitem; 
run; 
 
/* Save item-level and test-level CPO results in file: Work.CPO_result */ 
data CPO_result; 
  set CPO_item; 
  CPO_test=sum(of CPO_item1-CPO_item&nitem); 
run; 
 
%mend; 
 
/* Run CPO for GR model – macro assumes datasets are in Work Library */ 
%CPO_gr;   
 
Table 7.4 compares the item-level and test-level CPO values for the GR model and the RS-GR models. 
Based on the test-level CPO statistics for the two models, the GR model is preferred (greater CPO statistic) 
for the DASH item responses. Item-level results can be used to further explore the comparison to examine 
whether this conclusion applies to all items. As you can see, a common response scale across all the items 
appears to also be less useful in describing the item response data. Even in cases where the RS-GR model 
has a greater CPO value than the GR model (Items 3, 6, and 8), the differences are marginal except for 
perhaps one item (Item 8). Thus, it would appear a GR model, or a model with separate category threshold 
parameters across items, is preferred based on both the test- and item-level CPO statistics. 
 
 
 

200   Bayesian Analysis of Item Response Theory Models Using SAS 
Table 7.4: CPO Indices for the GR and RS-GR Models—DASH Data 
Item 
GR Model 
RS-GR Model 
1 
Ѹ840.2 
Ѹ845.0 
2 
Ѹ768.3 
Ѹ771.1 
3 
Ѹ743.1 
Ѹ741.2 
4 
Ѹ1027.1 
Ѹ1039.4 
5 
Ѹ1159.0 
Ѹ1159.9 
6 
Ѹ878.5 
  -877.8 
7 
Ѹ936.9 
Ѹ940.4 
8 
Ѹ1089.2 
Ѹ1082.2 
9 
Ѹ729.8 
Ѹ740.1 
10 
Ѹ1200.8 
Ѹ1207.4 
Total (Test) 
Ѹ9372.9 
Ѹ9404.5 
Example 3: Comparing a Unidimensional IRT Model and a Bifactor IRT 
Model 
As discussed in Chapters 1 and 6, a multidimensional IRT (MIRT) model that has recently received 
attention is the bifactor IRT model. In the bifactor model, each item response is determined or explained by 
both a general factor and secondary factors (Gibbons & Hedeker, 1992; Gibbons, Immekus, & Bock, 
2007). The bifactor model allows for measuring a general ability or trait while controlling for the variance 
that arises from the measurement of other factors. The bifactor model is particularly useful in testing 
applications where the general factor is the primary trait of interest. Secondary factors, which may be 
conceptually narrower in scope and may be considered nuisance factors, are introduced to capture any 
residual item relationships beyond those accounted for by the general factor.  
In the bifactor model, slope parameters (or factor loadings) are estimated for each item on the general 
factor as well as a single secondary factor. Because the bifactor model allows for measuring a general trait 
while controlling for the variance that arises due to the measurement of other traits (subdomains), a useful 
competing model is a unidimensional IRT model. If the variance associated with the secondary factors is 
relatively small, then a simpler unidimensional IRT model can be estimated. As discussed by Reckase 
(1994), when estimation of the trait or traits is the focus, “The dimensionality of interest is the minimum 
dimensionality that provides the greatest information provided by the item responses.” Comparison of these 
two competing models is illustrated below using the simulated item responses that were analyzed in 
Chapter 6. Recall that the item responses were simulated under the bifactor model. 
Notably, a similar comparison of models could be used to evaluate whether a testlet-based model is 
preferred over a standard IRT model. Recall from Chapters 1 and 6 that individuals may respond to sets of 
items (testlet) based on a single stimulus. For these applications, each secondary factor reflects a testlet in 
the bifactor model. Thus, for each item, one slope parameter is estimated for the general factor, and one 
slope parameter is estimated for a single testlet factor. Comparing a unidimensional model with a bifactor 
model in this context can also be used to evaluate the extent to which a testlet model is useful for 
describing item responses. 
DIC Results 
Table 7.5 provides the DIC statistics and related components from PROC MCMC for the bifactor and 
unidimensional (one-factor) IRT models. The results indicate that the DIC value for the bifactor model 
reflects the smaller value and the difference between the DIC values is greater than 10. Thus, the preferred 
model for the simulated item responses is the true or data-generating model, that is, the bifactor model.  
 

Chapter 7: Bayesian Comparison of IRT Models   201 
Table 7.5: DIC Results for the Bifactor and Unidimensional (One-Factor) IRT Models  
DIC Results 
Bifactor Model  
One-Factor Model 
Dbar 
13,981.86 
14,792.79 
Dmean 
12,787.34 
13,897.94 
pd 
  1,194.52 
   894.848 
DIC 
15,176.37 
15,687.63 
In addition, for the Dbar measure and Dmean measure, smaller values are observed for the bifactor model, 
which indicates improvement in relative fit over the estimated unidimensional model. As expected, the pd 
values (effective number of model parameters) for the bifactor model have a substantially higher pd value 
since the model is more complex and additional slope parameters are estimated for each item. 
CPO Results 
Program 7.3 presents the SAS program for computing the CPO indices for the bifactor model. The program 
assumes that the observed data (BIFACTOR_DATA) and the samples from the posterior distribution 
(OUTPOST file from PROC MCMC) are available in the work library. The control parameters have been 
changed at the beginning of the program to reflect changes in the number of items and the variable list for 
item parameters ߑ. The program defines labels for the slope and intercept parameters in the bifactor model 
ߒ. Data sets to be processed are established by saving the observed data (DASH_SAMPLE) in the SAS 
data set RESP_DATA, and a random subset (1000 samples) of posterior estimates from the OUTPOST 
data set in the SAS data set OUTPOST_SAMPLE ߓ. For each observation in OUTPOST_SAMPLE, the 
program specifies the necessary model parameters (sampled item and person parameters) in the DATA step 
DATA ONE ߔand the sampled values are used in the DATA step DATA CPO_IJ to compute the 
likelihood of the response data for each observation ߕ. These results are accumulated in the SAS data set 
ALLCPO_IJ. Item level and test level CPO indices are then computed from the results in ALLCPO_IJ ߖ
and the results summarized and saved in the data set CPO_RESULT ߗ. You should note that to compute 
the CPO statistics for the unidimensional model, you need to change the control parameters, change the 
model parameters in the DATA step DATA ONE, and change the model specifications in DATA step 
DATA CPO_IJ in Program 7.3. 
 
 
 

202   Bayesian Analysis of Item Response Theory Models Using SAS 
Program 7.3: Program to Calculate CPO Indices for the Bifactor Model 
/* Control parameters */ ߑ 
%let nperson=1000;   
%let nitem1=10;        /*control variable for total number of items */ 
%let nitem2=5;         /*number of items in Factor 2 */ 
%let nitem3=5;         /*number of items in Factor 3 */ 
%let postsamples=1000; /* number of samples to use in calculating CPO */ 
%let itempars=a1_1-a1_&nitem1 a2_1-a2_&nitem2 a3_1-a3_&nitem3 d1-d&nitem2  
              d1_1-d1_&nitem3 d2_1-d2_&nitem3 d3_1-d3_&nitem3;  
              /*item pars in Bifactor model*/ ߒ 
 
/* Create SAS datasets to process – observed data and subset of 
iterations from OUTPOST dataset */ ߓ 
data resp_data; 
  set bifactor_data;  
  keep ix1-ix10; 
run; 
proc surveyselect data=bifactor_post method=SRS rep=1  
  sampsize=&postsamples seed=12345 out=outpost_sample; 
  id _all_; 
run; 
 
%macro CPO_bifactor;  
 
/* STEP1: compute CPO(i,j) */ 
proc datasets; delete allCPO_ij; run; 
 
%do mc=1 %to &postsamples; /* compute CPO(i,j) for each sample */ 
/* Attach posterior results to each record of the observed dataset */ 
/* Repeat the process for each iteration of the posterior distribution */ 
 
data one;ߔ 
  if _n_ = 1 then set outpost_sample(firstobs=&mc obs=&mc); 
  set resp_data; 
  array theta1_{*} theta1_1-theta1_&nperson; 
  array theta2_{*} theta2_1-theta2_&nperson; 
  array theta3_{*} theta3_1-theta3_&nperson; 
  person=_n_; 
  theta1=theta1_[person]; 
  theta2=theta2_[person]; 
  theta3=theta3_[person]; 
  keep person ix1-ix&nitem1 &itempars theta1-theta3; 
run; 
 
data cpo_ij; ߕ 
  sample=&mc; 
  set one; 
  array ix{&nitem1} ix1-ix&nitem1; 
  array a1_{&nitem1} a1_1-a1_&nitem1; 
  array a2_{&nitem2} a2_1-a2_&nitem2; 
  array a3_{&nitem3} a3_1-a3_&nitem3; 
  array d{&nitem2} d1-d&nitem2; 
  array d_cr[3,&nitem3] d1_1-d1_&nitem3 d2_1-d2_&nitem3 d3_1-d3_&nitem3; 
  array theta{3}; 
  array p[3];  
  array L{&nitem1} L1-L&nitem1; 
  array inL{&nitem1} inL1-inL&nitem1; 
 
  do j=1 to &nitem2; 
     z = -(a1_[j]*theta[1] + a2_[j]*theta[2] – d[j]); 
     pr = 1 / (1 + exp(z)); 
     L[j] = pr**ix[j] * (1-pr)**(1 – ix[j]); 
     inL[j]=1/L[j]; 
  end; 
 

Chapter 7: Bayesian Comparison of IRT Models   203 
 
   do j=(&nitem2+1) to &nitem1; 
      do k=1 to 3; 
         p[k]= -(a1_[j]*theta[1] + a3_[j-5]*theta[3] – d_cr[k,j-5]); 
      end; 
      if ix[j]=1 then prob=1-1/(1+exp(p[1])); 
      else if ix[j]=2 then prob=1/(1+exp(p[1]))- 1/(1+exp(p[2])); 
      else if ix[j]=3 then prob=1/(1+exp(p[2]))- 1/(1+exp(p[3])); 
      else prob=1/(1+exp(p[3])); 
      L[j]=prob; 
      inL[j]=1/L[j]; 
   end; 
   keep sample person inL1-inL&nitem1; 
run; 
 
/* Accumulate results from each iteration of posterior distribution */ 
proc append base=allcpo_ij  data=cpo_ij; run; 
%end; 
 
/* STEP2: compute item-level CPOj and test-level CPO */ ߂ 
proc sort data=allcpo_ij; by person sample; run; 
proc means data=allcpo_ij nway noprint; 
  class person; 
  var inL1-inL&nitem1; 
  output out=allcpo_ij1 Mean=; 
run; 
data allcpo_ij1; 
  set allcpo_ij1; 
  array inL{&nitem1} inL1-inL&nitem1; 
  array logcpo_ij{&nitem1} logcpo_ij1-logcpo_ij&nitem1; 
  do j=1 to &nitem1; 
     logcpo_ij[j]=log(inL[j]**(-1)); 
  end; 
  keep person logcpo_ij1-logcpo_ij&nitem1; 
run; 
 
/* STEP 3: Summarize CPO results across persons and items */ ߗ 
proc summary data=allcpo_ij1; 
  var logcpo_ij1-logcpo_ij&nitem1; 
  output out=CPO_item SUM=CPO_item1-CPO_item&nitem1; 
run; 
 
/* Save item-level and test-level CPO results in file: Work.CPO_result */ 
data CPO_result; 
  set CPO_item; 
  CPO_test=sum(of CPO_item1-CPO_item&nitem1); 
run; 
%mend; 
 
/* Run CPO for Bifactor model – assumes datasets are in Work Library */ 
%CPO_bifactor;  
 
Table 7.6 compares the item- and test-level CPO values for the unidimensional and bifactor models. Based 
on the test-level CPO statistics for the two models, the bifactor model is preferred (greater CPO statistic) 
over a unidimensional model, which would be expected given that the item responses were simulated under 
a bifactor model. The item-level results are also consistent with the test-level conclusion that the bifactor 
model is the preferred model with the exception of item 9 where the two item-level statistics are 
approximately equal. 
 
 
 

204   Bayesian Analysis of Item Response Theory Models Using SAS 
Table 7.6: CPO Indices for Bifactor and Unidimensional (One-Factor) Models  
Item 
One-Factor Model 
Bifactor Model 
1 
í 
í 
2 
í 
í 
3 
í 
í 
4 
í 
í 
5 
í 
í 
6 
í 
í 
7 
í 
í 
8 
í 
í1,129 
9 
í 
í 
10 
í 
í1,016 
Total (Test) 
í 
í 
 
 

Chapter 8: Bayesian Model-Checking for IRT 
Models 
 
Introduction ....................................................................................................... 205 
Different Model-Fit Statistics ............................................................................ 206 
Test-Level Fit ...................................................................................................................... 206 
Item-Level Fit ...................................................................................................................... 206 
Person Fit ............................................................................................................................ 207 
Posterior Predictive Model Checking ................................................................ 207 
The PPMC Method ............................................................................................................. 207 
The Posterior Predictive Distribution ............................................................................... 208 
Discrepancy Measures ...................................................................................... 212 
Test-Level Measures .......................................................................................................... 213 
Item-Level Measures.......................................................................................................... 213 
Pairwise Measures ............................................................................................................. 214 
Person-Fit Measures .......................................................................................................... 214 
Evaluation of Model Fit ...................................................................................................... 215 
Example PPMC Applications .............................................................................. 216 
Example 1: Observed and Predicted Test Score Distributions—LSAT Data ............... 216 
Example 2: Observed and Predicted Item-Test Score Correlations ............................ 220 
Example 3: Item Fit Plots and Yen’s Q1 Measure ............................................................ 223 
Example 4: Observed and Predicted Odds Ratio Measure ........................................... 231 
Example 5: Observed and Predicted Yen’s Q3 Measure ................................................ 236 
Example 6: Observed and Predicted Person-Fit Statistic ............................................. 239 
Example 7: Use of PPMC to Compare Models ............................................................... 243 
 
Introduction 
Evaluating the validity of item response theory (IRT) model applications involves the collection of 
different types of evidence about different aspects of model fit. For example, the degree to which 
underlying IRT model assumptions, such as unidimensionality and local independence, hold in the item 
response data can be assessed. In addition, goodness-of-fit of IRT models or the degree to which observed 
responses correspond to model-based predictions can be evaluated. Lack of model fit indicates that the 
assumed IRT model does not accurately model how examinees respond to the items. Thus, goodness-of-fit 
reflects the absolute fit of a specific model, whereas the model comparison methods discussed in Chapter 7 
are concerned with the relative fit of models. Chapter 1 discusses a variety of methods that have been 
proposed for assessing different aspects of model fit in the context of classical applications of IRT models. 
This chapter focuses on the evaluation of goodness-of-fit for IRT models in Bayesian applications.  
In a Bayesian paradigm, the posterior predictive model checking (PPMC) method is a commonly used 
Bayesian analysis tool and has proved useful for evaluating model fit with IRT models (see, for example, 
Levy, Mislevy, & Sinharay, 2009; Sinharay, 2005, 2006; Sinharay, Johnson, & Stern, 2006; Zhu & Stone, 
2011). This chapter describes the PPMC method and uses a number of examples to illustrate how to 

206   Bayesian Analysis of Item Response Theory Models Using SAS 
 
 
evaluate different aspects of model-fit using SAS. SAS code used to perform a PPMC analysis is described 
in detail in this chapter. Results that are presented use Graphics Template Language (GTL) commands. 
Because the focus of this chapter is on obtaining posterior predictive model checking (PPMC) results, the 
commands used to generate the graphs in this chapter are not presented but are included in the programs on 
the authors’ webpage for this book. The reader interested in the graphics commands should consult SAS 
Graph Template Language documentation. 
Different Model-Fit Statistics 
Model fit statistics for IRT models are generally grouped into three types: test-fit statistics, item-fit 
statistics, and person-fit statistics. As described in Chapter 1, both test- and item-fit statistics are used for 
evaluating IRT model fit. The difference is that test-fit statistics assess the fit of all test items as a whole, 
whereas item-fit statistics evaluate the fit for each individual item. Lack of fit indicates that the assumed 
IRT model does not accurately model how examinees respond to the items. Person-fit statistics are used to 
assess the fit of models for a specific examinee. Lack of fit indicates that estimated model parameters for 
the selected IRT model do not capture the response pattern for a specific examinee. To evaluate the fit of an 
IRT model, you should consider all three types of fit statistics. 
When test-level model fit is compromised, another IRT model might be appropriate and suggested either by 
the PPMC analysis or by consideration of an alternative theoretical perspective about item and person 
characteristics used to model item performance. For example, a multidimensional IRT model may be 
suggested by a PPMC analysis of a unidimensional IRT model application to item responses (see Example 
7 in this chapter). Alternatively, lack of fit at the test level may be due to lack of fit for a particular subset 
of items. Thus, an evaluation of model fit at the item level may uncover the responsible items, and these 
items could either be deleted from the test or revised. Even when model fit is found at the test level, it may 
still be useful to perform a PPMC analysis at the item level to evaluate whether any items are not 
functioning as expected. Finally, although a lack of model fit may be found, some researchers have argued 
that the practical significance of any uncovered model misfit should be examined. For example, Sinharay 
and Haberman (2014) present several examples of assessing the practical significance of model misfit and 
discuss that valid inferences about the item responses can still be made under some conditions of model 
misfit. 
When IRT models are compromised at the person level but not at the test or item level, the issue is that the 
IRT model application is not appropriate for an individual person or persons. In this case, the response 
pattern for a person does not match the model, or is called aberrant, and person misfit compromises 
inferences about a person’s estimated score (θ). There are a number of possible sources for aberrant 
responses patterns, including, for example, cheating and careless responding in the case of educational 
assessments, and individuals engaged in response sets such as socially desirable response or acquiescence 
in the case of psychological and behavioral assessments. A small number of individuals with aberrant 
response patterns is not likely to affect model fit, but a large number of individuals with aberrant response 
patterns could also be a source of test- or item-level model misfit. 
Test-Level Fit  
A common way to evaluate model fit at the test level is to compare observed and model-predicted total test 
score distributions or the number of persons with each total test score. If the observed test score distribution 
corresponds closely to the predicted total test score distribution, there is no evidence of model misfit at the 
test level. In addition to this descriptive comparison of the two distributions, Béguin and Glas (2001) 
suggest using a Pearson chi-squared statistic (ɖ்
ଶ) to test the difference between the observed and expected 
test score distributions for T possible total scores.  
Item-Level Fit  
Even when an IRT model fits the data at the test level, some of the item responses may not function as 
expected under the specific IRT model (Embretson & Reise, 2000). Identifying these misfitting items can 
help test constructers isolate poor items in item pools and retain only items in the pool that fit a selected 
 

Chapter 8: Bayesian Model-Checking for IRT Models   207 
IRT model. Compared to overall test-level fit statistics, more statistical procedures have been proposed in 
the literature to assess the fit of IRT models at the item level.  
Many item-fit statistics compare response score distributions or the frequencies of persons responding in 
each category for an item (for example, 0 as opposed to 1 for dichotomous items). The general steps for 
these statistics were described in Chapter 1, and one statistic that has received a lot of attention in the 
literature is Yen’s Q1 statistic (Yen, 1981). The Q1 statistic is based on a cross-classification of response 
categories with discrete levels of the continuous θ variable. A chi-squared statistic can be formed to 
summarize the differences between observed and expected frequencies within each response category 
across the θ subgroups. The expected frequencies for each response category k at each discrete level of the 
θ subgroup for each item j are model predictions based on the IRT model, P୨൫U୨= k| ઼୨, Ʌ subgroup൯, 
where ઼j is the set of item parameter estimates for item j. 
Person Fit  
Person-fit statistics refer to statistical methods used to evaluate the fit of an examinee’s response pattern to 
a particular IRT model. Investigation of person fit may help researchers obtain additional information about 
the response behavior of a person and identify examinees with aberrant response patterns. There are a large 
number of statistics available for evaluating person fit. Meijer and Sijtsma (2001) provide an excellent 
review of person fit statistics, and Karabatsos (2003) discusses a simulation study comparing thirty-six 
different person-fit statistics.  
One of the more widely used and intuitive measures of person fit is a log-likelihood statistic ݈௭ (Drasgow, 
Levine, & Williams, 1985). The ݈௭ statistic is defined as a standardized version of the ݈଴ statistic, where ݈଴ 
is the log-likelihood for an examinee’s response pattern based on the fitted IRT model, the examinee’s 
responses, and model parameter estimates (item and person parameters í ઼୨, Ʌ).  
Posterior Predictive Model Checking 
PPMC involves simulating item responses (data) under the assumed or estimated model using posterior 
samples of model parameters. The simulated or replicated data are compared against the observed data 
using discrepancy measures or statistics calculated on both the simulated and observed data (for example, 
residual or total score). The rationale underlying PPMC is that, if a model fits the data, then the observed 
data, and any statistic computed on the observed data, should be similar to statistics computed on data 
replicated under the estimated IRT model. Any systematic differences between the statistics indicate 
potential misfit of the model (Gelman, Carlin, Stern, Dunson, Vehtari, & Rubin, 2014). 
The PPMC Method 
The PPMC method uses draws of sampled model parameters from the joint posterior distribution (p(δδ | D)) 
to simulate item responses under the model, where D corresponds to the data and δ corresponds to the set 
of model parameters that are sampled. For each draw or iteration in the posterior distribution, a data set of 
model-based predicted observations is simulated based on the model. Thus, for R iterations, there are R 
predicted response data sets Drep,1, …, Drep,R computed. Each data set represents a sample from the posterior 
predictive distribution p(Drep| D). A discrepancy statistic, T, is computed for each Drep,r replication to 
produce a distribution of discrepancy measures under the null condition of model-data-fit (T(Drep)).  
A discrepancy statistic is also computed on the observed data (T(D)), and any systematic difference 
between T(D) and the distribution of T(Drep) is used to evaluate model fit. Posterior predictive probability 
values (PPP values) are computed to quantify the relative occurrence of T(D) in the reference distribution 
of T(Drep) from the posterior predictive distribution:   
ܲܲܲ= ܲ൫ܶ(ܦ௥௘௣) ൒ܶ(ܦ)൯ 
where PPP is the proportion of times across R replications that T(Drep) equals or exceeds T(D). Thus, 
PPP values reflect the relative similarity between the discrepancy measure for the predicted and observed 
 

208   Bayesian Analysis of Item Response Theory Models Using SAS 
 
 
data. PPP values near.5 indicate that there are no systematic differences between the realized and 
predictive values, and thus indicate adequate fit of a model. PPP values near 0 or 1 (typically values <.05 
or >.95) suggest, conversely, that the realized statistic is inconsistent with the posterior predictive values 
and therefore reflects inadequate model-data-fit (Gelman et al., 2014). However, as discussed in Chapter 2, 
PPP values should not be interpreted in the same way as traditional hypothesis testing p-values.  
The relative occurrence of T(D) in the reference distribution of T(Drep) may also be evaluated using 
graphical displays. For example, a histogram reflecting the distribution of T(Drep) can be drawn with T(D) 
located in the histogram to indicate the relative frequency of the discrepancy statistic based on the observed 
data in the empirical distribution of discrepancy statistics, which are based on the replicated data.  
Although other methods for model comparisons were discussed in Chapter 7, PPMC has also been found 
useful for comparing models in a Bayesian analysis. The relative fit of a set of candidate models can be 
evaluated by comparing the numbers of extreme PPP values for the discrepancy measure(s) for the 
different models. A model with fewer extreme PPP values is considered to fit the data better than an 
alternative model with larger numbers of extreme PPP values (see, for example, Sinharay, 2005; Zhu & 
Stone, 2012). 
Clearly, selecting appropriate discrepancy measures is an important consideration in applications of the 
PPMC method, and they should be chosen to reflect sources of potential misfit. Several different 
discrepancy measures are discussed in later sections. In the above discussion, the discrepancy measure 
depends only on the data (T(Drep) for replicated data and T(D) for observed data). However, discrepancy 
measures can also be computed that depend on both the data and model parameters (δδ). In this case, the 
discrepancy measures are denoted as T(Drep, δ) for the replicated data and T(D, δ) for observed data. 
The Posterior Predictive Distribution  
There are two ways to obtain the posterior predictive distribution p(Drep| D). One way is to use the 
PREDDIST statement with the PROC MCMC command. The syntax with options relevant to analysis of 
IRT models is as follows: PREDDIST OUTPRED = <SAS dataset> NSIM=<value>, where the <SAS 
dataset> option specifies the data set to contain the simulated posterior predictive samples, and the 
<value> option for NSIM specifies the number of replications in the SAS data set. 
Program 8.1 illustrates how to obtain the posterior predictive distribution based on the LSAT data and the 
1P model. As shown in the program, the PREDDIST statement is simply added to the PROC MCMC 
program for estimating the 1P model (Program 4.1). The PREDDIST statement generates 500 samples 
from the posterior predictive distribution and stores them in the SAS data set LSAT_PRED. The predicted 
response variable names in the LSAT_PRED data set are based on the variable names in the LSAT data set 
(X1, X2, X3, X4, X5). For example, the LSAT data includes 5 response variables and 1000 observations. 
Thus, the LSAT_PRED data set includes 500 replications of item responses (NSIM=500) with each 
replication consisting of 5 item responses for 1000 examines or 5000 variables labeled as X1_1, X2_1, 
X3_1, X4_1, X5_1, … , X1_1000, X2_1000, X3_1000, X4_1000, and X5_1000.  
Program 8.1: Simulate Posterior Predictive Responses Based on 1PL Model—LSAT Data 
proc mcmc data=lsat outpost=lsat_bayes seed=23 nbi=5000 nmc=20000 
monitor=(a b) diagnostics=all plots=(trace autocorr); 
 
array b[5]; array d[5]; array p[5]; 
parms a 1; parms d: 0;              
prior d: ~ normal(0, var=25); 
prior a ~ lognormal(0, var=4); 
random theta ~ normal(0, var=1) subject=_obs_;  
do j=1 to 5;                      
     p[j] = logistic(a*theta - d[j]); 
     b[j] = d[j] / a; 
end; 
model x1 ~ binary(p1);   
model x2 ~ binary(p2);   
 

Chapter 8: Bayesian Model-Checking for IRT Models   209 
model x3 ~ binary(p3);   
model x4 ~ binary(p4);   
model x5 ~ binary(p5);      
preddist outpred=lsat_pred  nsim=500; 
run; 
When the PREDDIST statement is used, simulation of predicted responses is conducted during estimation 
of the model parameters. Because use of the PREDDIST statement requires additional computer time, it is 
recommended that the analysis and model specifications be finalized prior to including the PREDDIST 
statement in a final PROC MCMC run to estimate your model and generate posterior predictions.  
A second way to obtain the posterior predictive distribution is by post-processing the joint posterior 
distribution of sampled parameter values (OUTPOST data set). Program 8.2 presents SAS code for 
generating polytomous item responses based on the posterior estimates of the graded response (GR) model 
(see, for example, Program 5.2). The GR model was estimated based on the DASH data using PROC 
MCMC, and posterior samples of all model parameters were saved in the DASH_POSTGR data set. 
Sampled values for model parameters in this data set include 10 slope parameters (A1− A10), 40 threshold 
parameters (B1_1, …, B4_10),  and 1000 person trait estimates (THETA_1, …, THETA_1000). The 
program is annotated and described below. 
Program 8.2 creates a single SAS data set consisting of item responses for each replication (Drep,r) with the 
number of observations equal to the number of cases in the observed data set times the number of 
replications (1000 cases in the DASH data × NSIM). In Program 8.2, the number of observations in the 
posterior predictive distribution data, PRED_DIST_DASH, includes 1000 cases × 500 data sets. This 
format (Format 1) for the posterior predictive responses is generally easier to work with in a PPMC 
analysis and is used in later examples. 
 
 
 

210   Bayesian Analysis of Item Response Theory Models Using SAS 
 
 
Program 8.2: Simulate Posterior Predictive Responses Based on the Graded Response Model (DASH 
Data)—Posterior Predictive Output Data Set Format 1 
/* CONTROL VARIABLES */ ߑ 
%let ncat=5;           /*number of response categories*/ 
%let nperson=1000;     /*number of persons*/ 
%let nitem=10;         /*number of items*/ 
%let postsamples=500;  /*number of replicated datasets to be generated*/ 
%let seed=0; 
 
/* randomly sample from the postout data – comment if not used*/  
proc surveyselect data=dir.dash_postgr method=SRS rep=1 ߒ 
  sampsize=&postsamples seed=0 out=outpost_sample; 
  id _all_; 
run; 
 
/* create a dataset with the number of observations equal to (number of  
   postsamples) x (number of persons) */ 
data pred_dist_dash; 
  set outpost_sample; 
  array a{&nitem} a1-a&nitem; 
  array b[4,&nitem] b1_1-b1_&nitem  b2_1-b2_&nitem b3_1-b3_&nitem  
        b4_1-b4_&nitem; 
  array theta{&nperson} theta_1-theta_&nperson; 
  array x{&nitem} x1-x&nitem; 
  array z{*} z1-z&ncat; 
  seed=&seed;  
  retain sample (0); 
  sample=sample+1; 
  do i=1 to &nperson;ߓ 
     do j=1 to &nitem; 
        do k=1 to &ncat-1;ߔ  
           z[k]=(-(a[j]*(theta[i] - b[k,j]))); 
           z[k]=1/(1+exp(z[k])); 
        end; 
        r01=ranuni(seed); 
        if r01 >= z[1] then x[j]=1;ߕ 
        else if r01 >= z[2] and r01 < z[1] then x[j]=2; 
        else if r01 >= z[3] and r01 < z[2] then x[j]=3; 
        else if r01 >= z[4] and r01 < z[3] then x[j]=4; 
        else if r01 < z[4] then x[j]=5; 
        else x[j]=.; 
     end; 
     person=i; 
     output;ߖ 
  end; 
  keep sample person x1-x&nitem; 
run;   
ߑMacro variables specifying elements to the data set (number of items, number of categories per item, 
and number of persons) and number of desired replications (number of posterior predictive samples). 
ߒPROC SURVEYSELECT can be used to randomly select a sample of iterations in the posterior 
distribution to reduce the number of replications, if desired. Here 500 replications are specified primarily 
for convenience. Depending on the discrepancy measures that are used, more replications may be required. 
In addition, more replications may be required to increase the precision in computed PPP values. 
ߓFor each sample of model parameter values, responses for 1000 examinees to the 10 DASH items are 
generated and saved in the PRED_DIST_DASH data set. The steps for the generation are as follows: (1) 
Using the sampled item and person parameter values, compute the cumulative probability that a person 
selects or receives a category score k or higher for an item j using the logistic deviate zjk = aj (θ  − bjk)ߔ;
(2) Compare a random number from a uniform distribution U (0, 1) and compare this random number with 
 

Chapter 8: Bayesian Model-Checking for IRT Models   211 
the cumulative probabilities for each response category to simulate a response under the model ߕ; and (3) 
Output each replicated data set into the SAS data set PRED_DIST_DASH ߖ. 
To use the program for other models, simply change as required the control parameters ߑ, the item and 
person parameter specifications in various KEEP and ARRAY statements, and change the model 
specifications ߔ ߕ
Program 8.3 creates a data set with each replicated observation containing all the predicted item responses 
in one row of the posterior predictive distribution data set. This format (Format 2) is equivalent to the 
format provided by the PREDDIST statement and may be useful for some PPMC analyses. This program 
also illustrates generating predicted item responses for dichotomously scored items based on the LSAT data 
under the 1P model.  
Each row of the SAS output data set PRED_DIST_LSAT contains predicted item responses for each person 
in the variable list: X1_1, X2_1, X3_1, X4_1, X5_1, … , X1_1000, X2_1000, X3_1000, X4_1000, 
X5_1000. The number of observations in the data PRED_DIST_LSAT is 500, which reflects 500 
replications of data. In both programs a variable SAMPLE is used to index each replicated observation in 
the data set. You will note that Program 8.3 embeds the code for creating predicted item responses within a 
macro. Use of the macro allows for structuring the data set and labeling the variables in a way that is 
consistent with the PREDDIST statement in PROC MCMC. 
Though the PREDDIST statement provides a convenient way to simulate predicted samples, it works only 
with likelihood expressions that use standard distributions and the MODEL statement as in Program 8.1. 
The PREDDIST statement is not supported with model statements using GENERAL or DGENERAL 
functions as is the case when the likelihood model is explicitly programmed (for example, IRT models for 
polytomously scored responses in Chapter 5). For these cases, you have to post-process the posterior 
distribution for model parameters in order to obtain the predictive posterior distribution as in Programs 8.2 
and 8.3.
 
 
 

212   Bayesian Analysis of Item Response Theory Models Using SAS 
 
 
Program 8.3: Simulate Posterior Predictive Responses Based on the 1P Model (LSAT Data)—
Posterior Predictive Output Data Set Format 2
%let ncat=2;           /*number of response categories*/ 
%let nperson=1000;     /*number of persons*/ 
%let nitem=5;          /*number of items*/ 
%let postsamples=500;  /*number of replicated datasets to be generated*/ 
%let seed=23; 
 
/* randomly sample from the postout data – comment if not used*/  
proc surveyselect data=lsat_bayes method=SRS rep=1  
  sampsize=&postsamples seed=0 out=outpost_sample; 
  id _all_; 
run; 
 
/* macro creates a dataset with the number of observations equal to the 
   number of postsamples and structures the dataset as using PREDIST  
   command */ 
%macro gen_pred; 
data pred_dist_lsat; 
  set outpost_sample; 
  array b[&nitem] b1-b&nitem; 
  array theta{&nperson} theta_1-theta_&nperson; 
  array z{*} z1-z&ncat; 
  seed=&seed;  
  retain sample (0); 
  sample=sample+1; 
  %do i=1 %to &nperson; 
     %do j=1 %to &nitem; 
        %do k=1 %to &ncat-1;  
           z[&k]=(-(a*(theta[&i] - b[&j]))); 
           z[&k]=1/(1+exp(z[&k])); 
        %end; 
        r01=ranuni(seed); 
        if r01 >= z[1] then x&j._&i=0;  
        else x&j._&i=1; 
     %end; 
  %end; 
  keep sample x1_1--x&nitem._&nperson; 
run;   
%mend; 
%gen_pred; 
Discrepancy Measures 
In a PPMC analysis of model fit, a discrepancy measure or statistic is computed, which summarizes a 
particular aspect of the item responses. For example, the total score distribution, describing the frequency 
of persons in the data set with a total score equal to each possible total score, can be computed on the 
observed data to summarize responses to all the test items. This statistic is then compared to the same 
statistic computed on each of the replicated data sets simulated from the posterior distribution. The 
rationale underlying PPMC is that, if a model fits the data, then the discrepancy measure computed from 
the observed data should be similar to the measure computed from the replicated or simulated data.  
Choosing discrepancy measures, therefore, is an important issue in applications of the PPMC method. They 
should be chosen to reflect potential sources of misfit that are most relevant to a testing application. For 
example, when a unidimensional IRT model is used in a testing application where item responses may 
reflect a multidimensional structure, discrepancy measures should be chosen that are sensitive to this type 
of threat to model fit. It is recommended that a number of different discrepancy measures be used for 
evaluating different sources of possible model misfit for a particular testing application (Sinharay et al., 
2006).  
 

Chapter 8: Bayesian Model-Checking for IRT Models   213 
As discussed, discrepancy measures can be a function of the data, or discrepancy measures can depend on 
both the data and model parameters. This section reviews briefly some discrepancy measures that have 
proved effective for assessing model fit in Bayesian IRT model applications. Examples of using several of 
these discrepancy measures are presented and discussed in later sections. 
Test-Level Measures  
As discussed previously, the test score distribution (frequency counts for each possible total score or 
summed score across the test items) is commonly used for evaluating model fit at the test-level. The test 
score distribution has been found useful for evaluating model fit in some testing applications, but it has 
been found to be not effective for evaluating threats due to multidimensionality or local dependence (see, 
for example, Béguin & Glas, 2001; Sinharay et al., 2006; Zhu & Stone, 2011).  
While a descriptive comparison of the total test score distributions can be conducted, Béguin and Glas 
(2001) suggest using a Pearson chi-squared statistic (்߯
ଶ) to summarize the differences between the 
observed and expected frequencies of test scores: ்߯
ଶ= σ [ܰ௧െܧ(ܰ௧)]ଶ
்
௧
/ܧ(ܰ௧), where T is the maximum 
total test score, ܰ௧ is the observed number of examinees with total score t, and ܧ(ܰ௧) is the expected 
number of examinees with total score t based on the selected IRT model. For any particular total test score, 
t, ܧ(ܰ௧)can be calculated as: 
ܧ(ܰ௧) = ܰ෍න݌(ݕ|Ʌ)݃(Ʌ)݀Ʌ
࢟|௧
 
where N is the total number of examinees; ࢟|ݐ represents the set of all possible response patterns resulting 
in a score t; ݌(ݕ|Ʌ)is the probability of response pattern ࢟ given ability θ and the item parameter estimates; 
and ݃(Ʌ) is the assumed density of the θ distribution. Because the expected frequencies for each total score 
are based on the selected IRT model and sampled model parameter values from the joint posterior 
distribution, the ்߯
ଶ discrepancy measure is a function of the data and the model parameters. 
Item-Level Measures  
An intuitive measure for assessing the item fit is the item score distribution which represents the number of 
examinees responding to each response category for each item. Similar to the test score distribution, the 
difference between observed and posterior predictive item score distributions can be summarized using a 
chi-squared statistic but for each item: ɖ௝
ଶ= σ
ൣܱ௝௞െܧ௝௞൧
ଶ
௄
௞ୀଵ
/ܧ௝௞, where Ojk is the observed number of 
examinees and Ejk is the predicted number of examinees scoring in response category k on item j. Ejk can be 
calculated by summing the probabilities of responding to category k on item j across all N examinees: 
ܧ௝௞= σ
ൣ݌௜௝௞(ߠ௜)൧
ଶ
ே
௜ୀଵ
. This ɖ௝
ଶ measure has been found useful with the PPMC method (see for example, 
Levy et al., 2009; Zhu & Stone, 2011). Similar to the test-level chi-square measure, the ɖ௝
ଶ measure for 
items depends on both data and model parameters. 
The item-test score correlation is the correlation between examinees’ total test scores and their scores on a 
particular item (ݎ௝௧). Though this measure is not a classic fit statistic, it has been found effective for some 
Bayesian analyses of IRT models. For example, Sinharay et al. (2006) found this discrepancy measure was 
effective in detecting misfit of 1PL models to data that were generated from a 2PL or 3PL model, 
indicating the measure's sensitivity to violations of the equal discrimination assumption. Zhu and Stone 
(2011) showed that this measure is also useful for detecting local dependence. This discrepancy measure is 
only dependent on the data.  
Classic item-fit statistics discussed previously, such as Yen’s Q1 statistic, can also be used as discrepancy 
measures with PPMC in order to evaluate IRT model fit. These statistics extend the analysis of item score 
distributions by comparing the distributions at different levels of the measured trait (θ). For example, Yen’s 
Q1 statistic is a Pearson chi-square test statistic defined as: ɖ௝
ଶ= σ
σ
൫ܱ௝௚௞െܧ௝௚௞൯
ଶ/
௄
௞ୀଵ
ଵ଴
௚ୀଵ
ܧ௝௚௞where the 
Ojgk are the observed number of examinees responding to category k on Item j for θ subgroup g, and the Ejgk 
are predicted number of examinees based on the IRT model and the item parameter estimates. In Yen’s 
statistic, examinees are divided into 10 θ subgroups of approximately equal size after they are rank-ordered 
 

214   Bayesian Analysis of Item Response Theory Models Using SAS 
 
 
by their θ estimates. The expected frequency in a response category for a θ subgroup is the number of 
examinees in that subgroup times the probability of responding in that category for examinees at the mean 
θ in the subgroup. 
Recently, the performance of these different measures with the PPMC method has been examined (see, for 
example, Sinharay, 2006; Zhu & Stone, 2011). Under conditions considered in these studies, the item-fit 
statistics were found useful in detecting modeled misfit due to the violation in the form of the IRT model, 
but were found ineffective in detecting item misfit due to multidimensionality or local dependence. 
Pairwise Measures  
Traditionally, dimensionality and local independence assumptions underlying IRT models have been 
evaluated using pairwise test statistics reflecting associations between responses to item pairs. For example, 
local-dependence statistics such as Yen’s Q3 statistic and the odds ratio (OR) are examples of pairwise 
statistics.  
Yen’s Q3 statistic measures the correlations between pairs of items after accounting for the latent variable 
θ (Yen, 1991). For items j and j', Q3 is defined as the correlation of deviation scores across all examinees: 
ܳଷ௝௝ᇲ= ݎ൫݀௝, ݀௝ᇱ൯, where dj is the deviation between the observed and expected response on item j. Yen’s 
Q3 can be used with either dichotomous or polytomous items. Used as a discrepancy measure with PPMC, 
previous studies have found that the Q3 measure was more powerful than other pairwise measures in 
detecting multidimensionality and local dependence among item responses (for example, Levy et al., 2009; 
Sinharay et al., 2006; Zhu & Stone, 2011). 
Chen and Thissen (1997) used odds ratios (OR) to evaluate local dependence for dichotomous items. The 
OR for dichotomous item pairs (j and j*) are computed from 2× 2 tables by ݊଴଴݊ଵଵ݊଴ଵ݊ଵ଴
Τ
, where ݊௣௤ is 
the observed number of examinees having response p (0 or 1) on Item j and response q (0 or 1) on Item j*. 
If local independence is not met, the observed OR will be larger than what is expected under a 
unidimensional IRT model. The OR measure has been found to be effective for checking several aspects of 
model fit in the PPMC context. One advantage of using OR as a discrepancy measure as opposed to Yen’s 
Q3 statistic is that computation of OR is dependent only on the data, whereas computation of Q3 is 
dependent on both the data and model parameters. 
In contrast to dichotomously scored items, multiple odds ratios can be computed for polytomous items 
since the contingency table is R × C (R > 2 and C > 2). One approach to this issue is to compute global 
odds ratios (Agresti, 2002). For any two items, the R × C contingency table may be reduced to a 2 × 2 
contingency table by dichotomizing the response categories of each item. The global odds ratio is then 
defined as the cross-ratio of this pooled 2 × 2 table. For example, for items with 5 response categories 
(0−4), categories 3 and 4 may be treated as correct responses, and categories 0, 1, and 2 treated as incorrect 
responses. Thus, the 5 × 5 contingency table is reduced to a 2 × 2 table. Zhu & Stone (2011) found the 
global odds ratio was a useful discrepancy measure for evaluating model fit with the GR model. 
Person-Fit Measures  
All person-fit statistics proposed in the frequentist framework may be used with the PPMC method. The 
performance of several person-fit statistics with the PPMC method has been examined in previous studies 
and the results indicate that some fit statistics perform better than others in a Bayesian framework (see, for 
example, Glas & Meijer, 2003).  
One of the most popular person-fit indices is the ݈௭ statistic (Drasgow, Levine, & Williams, 1985), which is 
defined as ݈௭= (݈଴െܧ(݈଴))/ඥVar(݈଴) , where ݈଴ is the log-likelihood for an examinee’s response pattern 
and ܧ(݈଴) denotes the expected value of  ݈଴ and ܸܽݎ(݈଴) denotes the variance of ݈଴. For dichotomously 
scored items, 
݈଴= ෍൛ܻ௝log [ܲ௝(Ʌ)] + (1 െܻ௝)logൣ1 െܲ௝(Ʌ)൧ൟ
௃
௝ୀଵ
 
 

Chapter 8: Bayesian Model-Checking for IRT Models   215 
where ܻ௝ is the response to item j,  and ܲ௝(Ʌ) is the probability of a correct response for an examinee with 
ability level Ʌ. ܧ(݈଴) and ܸܽݎ(݈଴) are computed by the following: 
ܧ(݈଴) = ෍൛ܲ௝(Ʌ)log[ܲ௝(Ʌ)] + [1 െܲ௝(ߠ)]logൣ1 െܲ௝(Ʌ)൧ൟ
௃
௝ୀଵ
 
and  
ܸܽݎ(݈଴) = σ
ቊܲ௝(Ʌ)[1 െܲ௝(Ʌ)] ൤݈݋݃
௉ೕ(஘)
ଵି௉ೕ(஘)൨
ଶ
ቋ
௃
௝ୀଵ
  
Evaluation of Model Fit 
The discrepancy statistic computed from the observed data is compared to the same measure computed on 
each of the replicated data sets based on the parameter values from the joint posterior distribution. If a 
model fits the data, then the discrepancy measure computed from the observed data should be similar to the 
same measure computed from the replicated or simulated data. Thus, discrepancy measures play a similar 
role in Bayesian model-fit checking as test statistics play in classic hypothesis testing. 
As discussed, this comparison is often conducted using graphical displays. For example, a histogram for the 
distribution of T(Drep) can be drawn with T(D) located in the histogram. If T(D) is located near the middle 
area of the histogram,  there is evidence the model fits the data. Posterior predictive probability values 
(PPP values) can also be computed to quantify the relative occurrence of T(D) in the reference distribution 
of the predictive posterior distribution, T(Drep). PPP values near.5 indicate adequate fit of a model, whereas 
PPP values near 0 or 1 (typically PPP values <.05 or PPP values >.95) suggest inadequate model-data-fit 
(Gelman et al., 2014). Use of both graphical displays and PPP values to evaluate model fit are provided in 
the examples in this chapter.  
Compared with hypothesis testing in classical model-fit analyses, there is more freedom in choosing 
discrepancy measures in a Bayesian paradigm because the reference or null distribution of any measure can 
be determined through the PPMC method. For example, a ɖௗ௙
ଶ statistic, used in the frequentist paradigm to 
evaluate item fit, assumes a null chi-square distribution with a specified df. This assumption does not hold 
for many cases, but this violation of the assumed theoretical null distribution is not an issue in a Bayesian 
paradigm. Because discrepancy measures such as χ2 are computed on replications using sampled model 
parameters from the joint posterior distribution, an empirical null distribution for the discrepancy measure 
is derived directly using PPMC. Thus, no theoretical null distribution need be assumed to assess the 
likelihood of T(D) in the distribution of T(Drep). 
For example, consider the use of a chi-squared statistic as a discrepancy measure (ɖ்
ଶ) where expected 
values are based on sampled values for the model parameters (δ). An empirical sampling distribution for 
the measure may be obtained by computing the measure for each ݎ௧௛ iteration of posterior distribution 
(rep௥) using model parameter values for that iteration (ߜ௥). Thus, over R replications, an empirical 
distribution ɖ்
ଶ (repோ, ߜோ) is obtained. For example, if there are R = 1,000 posterior samples, an empirical 
sampling distribution under the null condition or model fit is based on 1,000 ்߯
ଶ (ݎ݁݌௥, ߜ௥) statistics:  
(ɖ்
ଶ (repଵ, ߜଵ), ɖ்
ଶ (repଶ, ߜଶ), … , ɖ்
ଶ (repଵ଴଴଴, Ɂଵ଴଴଴)) 
Using the observed data, the same measure (ɖ்
ଶ) may be computed using point estimates of model 
parameters or ɖ்
ଶ൫obs, Ɂ෠൯. However, uncertainty in the model parameter estimates may be incorporated into 
the computation of the discrepancy measure by computing the measure on the observed data using sampled 
model parameter values from each ݎ௧௛ iteration of posterior distribution (Ɂோ). This process will produce a 
distribution of ɖ்
ଶ (݋ܾݏோ, Ɂோ). By directly comparing each pairing of the observed discrepancy statistic 
்߯
ଶ (݋ܾݏ, Ɂ௥) and the predicted discrepancy statistic ்߯
ଶ (ݎ݁݌௥, Ɂ௥) across R replications, the likelihood of 
the observed measure can be quantified using PPP values as discussed above. PPP values close to .5 
indicate adequate model fit whereas PPP values near 1 or 0 suggest possible model misfit. 
 

216   Bayesian Analysis of Item Response Theory Models Using SAS 
 
 
Example PPMC Applications 
This section presents examples to illustrate implementation of PPMC methods using different discrepancy 
measures with IRT model applications. We continue to use the LSAT data for evaluating the fit of 
dichotomous IRT models, and the DASH data for the fit of polytomous models. It is assumed that the 
posterior predicted response data sets are already simulated before the programs in this section are run. 
Note that a subset of the  discrepancy measures discussed above are used due to the space limitations, but at 
least one of the measures in each type of measure is illustrated. As mentioned, the display of the results and 
PPP values use SAS Graphics Template Language (GTL) commands. Because the focus of this chapter is 
on conducting the PPMC analysis, the commands used to generate graphs in this chapter are not presented 
but are included in the programs on the authors’ web site for this book [INSERT URL]. The reader 
interested in the graphics commands should consult SAS GTL documentation.  
Example 1: Observed and Predicted Test Score Distributions—LSAT Data 
The overall fit of the 1P model to the LSAT data was examined using the total score distribution as the 
discrepancy measure. The total score distribution consists of the frequency of persons in the data for each 
possible total score or summed score across the test items. This analysis was conducted using Program 8.4, 
which includes the SAS code used for comparing the test score distributions for the observed and replicated 
responses. The 1P model was estimated, and NSIM=500 predicted response data sets were simulated from 
the posterior predictive distribution using the PREDIST statement and saved in the SAS data set 
LSAT_PRED. These two data sets are assumed to be in the work library.  
From Program 8.4, you can see that the total test score for each examinee in the observed data (LSAT data 
set) is computed first ߑand the frequencies of total test scores were obtained using the PROC FREQ 
procedure. The observed frequencies are saved in the data set OBS_DIST ߒ. The same discrepancy 
measure, total score, is then computed for each replicated data set to obtain the test score frequency 
distributions for predicted responses using the macro TS ߓThe summary results (frequencies) are saved 
in the SAS data set ALL_PREDDIST ߔ. Based on the 500 predicted frequencies in ALL_PREDDIST, 
PROC UNIVARIATE is used to compute the 5th, 10th, 50th, 90th, and 95th percentiles of the predicted 
frequencies for each test score value. These percentiles are saved in the SAS data set PRED_PCTS ߕ. 
To compare the observed and predicted test score frequency distributions, the frequency distribution for the 
observed data and the 5th, 50th, and 95th percentiles for the frequency distributions from the predicted 
responses can be obtained and merged with the test score statistics for the observed and predicted data ߖ. 
These results can be examined in the SAS data set OBS_PRED_TS or plotted using SERIESPLOT 
statements and SAS GTL commands.  
You should note that the format of the posterior predictive data set is the format produced by using the 
PREDDIST statement (Format 2) in PROC MCMC—that is, NSIM rows of replications (500) in which 
each row contains the replicated responses for all observations in the LSAT data set (1000). In order to use 
PROC FREQ with a BY statement, you must transform this data set to Format 1 so that the number of 
observations in the posterior predictive distribution data set is 1000×500. This task is accomplished with 
the macro TS in Program 8.4 ߓ. Alternatively, Program 8.2 could have been used to create the posterior 
predictive data set in Format 1 directly. 
 
 
 

Chapter 8: Bayesian Model-Checking for IRT Models   217 
Program 8.4: Comparing Observed and Predicted Test Score Distributions—1P Model 
/* CONTROL VARIABLES */ 
%let nperson=1000;     /*number of persons*/ 
%let nitem=5;          /*number of items*/ 
 
/*obtain the discrepancy measure in the observed dist*/ 
data ts_obs;ߑ 
  set lsat; 
  array x{&nitem}; 
  ts=sum(of x:); 
run; 
proc freq data=ts_obs;ߒ 
  tables ts / out=obs_dist; 
run; 
 
/* obtain the discrepancy measure in the replicated data */ 
%macro ts;ߓ 
data ts_pred; 
  set lsat_pred; 
  sample=_n_; 
  %do i=1 %to &nperson; 
     ts=0; 
     %do j=1 %to &nitem; 
     ts=ts+x&j._&i; 
 
 %end; 
 
 i=&i; 
     output; 
  %end;   
  keep ts sample i; 
run;  
%mend; 
%ts; 
 
/* compute frequency distributions for predicted discrepancy measures */ 
proc freq data=ts_pred noprint;ߔ 
  tables ts /out=all_preddist; by sample; 
run; 
/* compute percentiles for each total score */ 
proc sort data=all_preddist; by ts sample; run; 
proc univariate data=all_preddist noprint;ߕ 
  var count; by ts; 
  output out=pred_pcts pctlpre=p  pctlpts=5 10 50 90 95; 
run; 
 
/* Combine the observed and predictive total test score distributions */ 
data obs_pred_ts;ߖ 
  merge obs_dist pred_pcts; by ts; 
run; 
Output 8.1 shows results from Program 8.4 after using SAS GTL commands to plot the observed 
frequencies for the total score with the 5th, 50th, and, 95th percentiles for frequencies of total scores based on 
replicated data. As can be seen, the observed score frequency distribution (solid line) falls between 5% and 
95% posterior predictive distributions of total test scores, or falls within the 90% credible interval of test 
score distributions. The observed score distribution is also very close to the 50th posterior distribution 
(dashed line). 
From the output, the 1PL model appears to model the LSAT data well in terms of the total test distribution. 
There is no evidence of model misfit at the test level based on this discrepancy measure. This finding can 
be compared with the model comparison results for the LSAT data in Chapter 7. Recall from the section 
“Example 1: Comparing Models for Dichotomously Scored Items (LSAT Data) ” in  Chapter 7, the 1P, 2P, 
and 3P models were compared using item and test-level CPO statistics and the DIC statistic. The item and 
test-level CPO statistics indicated that the 1P model was preferred whereas the DIC results indicated the 2P 
 

218 Bayesian Analysis of Item Response Theory Models Using SAS
 
 
model was preferred. Results of the PPMC analysis, using the total score as a discrepancy measure, are 
more consistent with the preferred model suggested by the item- and test-level CPO statistics (1P model). 
Output 8.1: Comparing Observed and Predicted Test Score Distributions—1PL Model 
 
Output 8.1 compares observed and predicted distributions regarding the entire total score distribution. You 
can also compare the distributions using summary statistics such as mean, median, and standard deviation 
of the data. Program 8.5 extends the PPMC analysis to compare the observed mean and standard deviation 
of the total test scores with the same measures for the predicted values, again using the LSAT data and the 
1PL model. The discrepancy measures used here are the mean and standard deviation of total test scores, 
which are only dependent on the data. Note that this program is based on an example provided in the SAS 
documentation for PROC MCMC. 
As in Program 8.4, the first step in Program 8.5 is to compute the observed and predicted total test scores 
based on observed and predicted response data. You should note that the macro TS is different from the 
macro TS in Program 8.4 ߑ. Next, the mean and standard deviation of the 1000 observed test scores are 
obtained using the PROC MEANS procedure ߒ. The CALL SYMPUTX statements are used to save these 
values in the macro variables MEAN and SD for later use in graphical output. The means and standard 
deviations of the total test scores for each predicted response data set (1000 test scores) are then computed 
ߓ.Thus, there is 1 observed value mean and standard deviation for the observed data, and an empirical 
distribution of NSIM means and NSIM standard deviations for the replicated data.  
To obtain PPP values, these predicted means and standard deviations are each compared to the observed 
values ߔ. If a predicted MEAN (or SD) is larger than the observed MEAN (or SD), the count variable 
COUNT_MEAN (and COUNT_SD) is incremented by1. The PPP value equals the value of the count 
variable divided by the number of predicted response data sets (500 in this example). In other words, the 
PPP value for the mean measure is the proportion of predicted mean scores greater than the observed mean 
score. The means and standard deviations for the observed and replicated data as well as the PPP values 
may be printed ߕ. Alternatively, these results can be compared graphically, as in Output 8.2 using SAS 

Chapter 8: Bayesian Model-Checking for IRT Models   219 
GTL commands and HISTOGRAM statements. (See the GTL commands in Program 8.5 on the SAS Press 
authors’ web page.)  
Program 8.5: Comparing Observed and Predicted Test Score Summary Statistics—1PL Model 
/* CONTROL VARIABLES */ 
%let nperson=1000;     /*number of persons*/ 
%let nitem=5;          /*number of items*/ 
 
/*obtain test scores for observed dataset */ 
data ts_obs; 
  set lsat; 
  array x{&nitem}; ts=sum(of x:); 
run; 
 
/* obtain the discrepancy measure in the replicated data */ 
%macro ts;ߑ
data ts_pred; 
  set dir.lsat_pred; 
  array ts{&nperson} ts1-ts&nperson; 
  %do i=1 %to &nperson; 
     ts[&i]=0; 
     %do j=1 %to &nitem; 
         ts[&i]=ts[&i] + x&j._&i; 
     %end;  
  %end;  
  output; keep ts1-ts&nperson; 
run;  
%mend; 
%ts; 
 
/*compute mean & sd of test scores for the observed data */   
proc means data=ts_obs noprint;ߒ 
  var ts; output out=stat mean=mean stddev=sd; 
run; 
data _null_;  
  set stat; call symputx('mean', mean); call symputx('sd',sd); 
run; 
 
/*compute mean & sd of test scores on each predicted data and compare 
them with the observed mean and sd to get the p-values */   
data pred;ߓ 
   set ts_pred; 
   mean = mean(of ts:); sd = std(of ts:); 
run; 
data ppp; ߔ 
  set pred end=eof nobs=nobs; 
  count_mean + (mean>&mean); count_sd + (sd>&sd); 
  if eof then do; 
      ppp_mean = count_mean/nobs; call symputx('pmean',ppp_mean); 
      ppp_sd = count_sd/nobs; call symputx('psd',ppp_sd); 
  end; 
run; 
 
data summary;ߕ 
  length type $15.; 
  set stat stat2 ppp; 
  if _n_=1 then type='Obs Dist'; else if _n_=2 then type='Rep Dist'; 
  else if _n_=3 then type='PPP Value'; 
  label mean='Mean' sd='Mean of SDs' ppp_mean='PPP Mean' ppp_sd='PPP SD'; 
  drop _type_ _freq_; 
 
proc print data=summary; run; 
 

220 Bayesian Analysis of Item Response Theory Models Using SAS
 
 
Output 8.2 presents the histograms of predicted values and the locations of observed values (macro 
variables MEAN and SD). The observed values are located in the middle of the histograms, indicating that 
the observed means and standard deviations exhibit no systematic differences from the distributions of 
predicted means and standard deviations for the total score discrepancy measure. In addition, the PPP value 
is 0.47 for the mean measure and 0.42 for the standard deviation measure. Both are close to 0.50, which 
also suggests adequate model-data-fit regarding the total test score discrepancy measure. 
Output 8.2: Comparing Observed and Predicted Test Score Summary Statistics —1PL Model 
 
Example 2: Observed and Predicted Item-Test Score Correlations  
Previous research has shown that the item-test score correlation is an effective measure for evaluating 
violations of the equal slope or discrimination assumption. This is a reasonable supposition because the 
correlation reflects an item’s power to discriminate among examinees. Research has also found that this 
measure is useful for detecting local dependence because local dependence affects estimation of item 
discrimination, and item discrimination is related to item-test score correlations (Yen, 1993). Therefore, 
when equal discrimination or local dependence is a possible source for model misfit, the item-test score 
correlation may be a useful measure in a PPMC analysis of a Bayesian IRT model application.  
Program 8.6 compares the observed and predicted item-total test score correlations for each LSAT item. 
The data sets required for this program include the original LSAT data set and a data set of posterior 
predictive responses (PRED_DIST_LSAT) in Format 1. In this case, the posterior predictive responses 
were generated using a modified version of Program 8.2 rather than using the PREDDIST statement in the 
PROC MCMC program because PROC CORR was used to compute the correlations for each replicated 
data set of simulated responses. This data set consists of 500 data sets of replications where each replicated 
data set is 1000 observations of 5 item responses (500 replicated data sets × 1000 observations). 
In Program 8.6, rather than the total score being based on all items, including the one being evaluated, the 
total score is based on all items excluding the one being evaluated ߑ. The correlations of item scores with 
these corrected total test scores are computed for each of the five LSAT items (observed data) using PROC 
CORR, and only the correlations are saved in the SAS data set OBS_CORRS ߒ.The corrected total scores 
are computed for the predicted item responses, ߓand the correlations are computed for each of the 500 
simulated data sets (PRED_DIST_LSAT) ߔ The correlation results from PROC CORR are then saved in 
the SAS data set PRED_CORRS ߕ.  
To compare the correlations for the observed and predicted response data sets, the percentiles of 
correlations for the predicted data are computed ߖ and merged with the observed correlations ߗ. The 
observed correlations can be compared with the 5th, 50th, and 95th percentiles for correlations for the 
replicated data in the SAS data set OBS_PRED_CORR.  

Chapter 8: Bayesian Model-Checking for IRT Models   221 
Program 8.6: Comparing Observed and Predicted Item-Test Score Correlations—1P Model 
/* control parameters */ 
%let nitems=5; 
%let nperson=1000; 
%let npostsample=500; 
 
/* compute observed item-corrected total score correlation */ 
data ts_obs;ߑ 
  set lsat; 
  array x{&nitems}; array ts{&nitems}; 
  total=sum(of x:); 
  do i = 1 to &nitems; 
     ts[i]=total-x[i]; 
  end; 
run; 
proc corr data=ts_obs noprint outp=corrs; ߒ  
  var x1-x&nitems; with ts1-ts&nitems; 
data obs_corrs; 
  set corrs end=last; 
  retain c1-c&nitems; 
  if _TYPE_='CORR' and _NAME_='ts1' then c1=x1; 
  if _TYPE_='CORR' and _NAME_='ts2' then c2=x2; 
  if _TYPE_='CORR' and _NAME_='ts3' then c3=x3; 
  if _TYPE_='CORR' and _NAME_='ts4' then c4=x4; 
  if _TYPE_='CORR' and _NAME_='ts5' then c5=x5; 
  keep c1-c&nitems; 
  if last then output; 
run; 
 
/* compute the predicted item-total score correlations for each item */ 
data pred_dist; ߓ 
  set pred_dist_lsat; 
  array x{&nitems}; array ts{&nitems};  
  total=sum(of x:); 
  do i = 1 to &nitems; 
     ts[i]=total-x[i]; 
  end; 
run; 
 
proc corr data=pred_dist_lsat noprint outp=pred_corrs; ߔ 
  var x1-x&nitems; with ts1-ts&nitems; by sample; 
run; 
 
data pred_corrs;ߕ 
  set pred_corrs; 
  if _TYPE_='CORR' and _NAME_='ts1' then c1=x1; 
  if _TYPE_='CORR' and _NAME_='ts2' then c2=x2; 
  if _TYPE_='CORR' and _NAME_='ts3' then c3=x3; 
  if _TYPE_='CORR' and _NAME_='ts4' then c4=x4; 
  if _TYPE_='CORR' and _NAME_='ts5' then c5=x5; 
  keep c1-c&nitems sample; 
run; 
proc summary data=pred_corrs; 
  var c1-c&nitems; 
  by sample; 
  output out=pred_corrs sum=; 
run; 
 
 
 
 
 

222   Bayesian Analysis of Item Response Theory Models Using SAS 
 
 
/* obtain percentiles for predicted correlations */ 
data item_pred_corrs;ߖ 
  set pred_corrs; 
  array c{&nitems}; 
  do item=1 to &nitems; 
    pred_corr=c{item}; 
  output; 
  end; 
  keep sample item pred_corr; 
run; 
proc sort data=item_pred_corrs; by item sample; run; 
proc univariate data=item_pred_corrs noprint;  
  var pred_corr; by item; 
  output out=pred_pcts pctlpre=p  pctlpts=5 10 50 90 95; 
run; 
 
/* merge percentile data for predicted correlations with observed 
correlations */ 
proc transpose data=obs_corrs out=obscorrs; run; 
data obscorrs; ߗ 
  set obscorrs; 
  rename col1=obs_corr; 
  item=_N_; 
  drop _name_; 
run; 
data obs_pred_corr;  
  merge obscorrs  pred_pcts; by item; 
run; 
Alternatively, the 5th, 50th, and 95th percentiles for the predicted correlations may be graphed with the 
observed correlations using the SCATTERPLOT statement with SAS GTL commands. These commands 
are included in the program on the website for the book, and the output is presented in Output 8.3. In this 
graph, the observed item-corrected total score correlations, which correspond to 90% posterior predictive 
intervals, and the median posterior correlations are provided for each item. As you can see, the observed 
correlation (solid dot) for each item falls within the 90% interval and is also approximately equal to the 
median of the posterior predictive correlations. Results from these graphs indicate adequate model fit of the 
1P model (equal slope assumption) to the LSAT data for this discrepancy measure and are consistent with 
the PPMC analysis using the total score as a discrepancy measure. 
 

Chapter 8: Bayesian Model-Checking for IRT Models 223
Output 8.3: Comparing Observed and Predicted Item-Test Score Correlations—1P Model 
 
Example 3: Item Fit Plots and Yen’s Q1 Measure 
As discussed, classic item-fit statistics can be used with PPMC for evaluating the fit of an IRT model for 
each item. Item fit evaluation is used to judge how well an IRT model predicts the responses to a particular 
item or the degree of agreement between observed responses and model predictions for an item. There are 
basically two ways to assess item fit. One way is to use plots of residuals or differences between observed 
data and model-based predictions. For example, consider a dichotomously scored item and a set of discrete 
groups of examinees with similar trait estimates. An item fit plot may be drawn to demonstrate the 
differences between the observed proportions correct and model-based predictions of the proportion correct 
in each group. Another approach is to use formal item-fit statistics, such as Yen’s Q1 statistic. In this 
section, we illustrate how to use item fit plots and Yen’s Q1 measure with PPMC to assess the fit of the 1PL 
model to the LSAT item. For more detail on item fit statistics, the reader is referred to Chapter 1. 
Item Fit Plots of Proportion Correct Statistics 
Program 8.7 compares observed and predicted proportions of correct responses to each LSAT item for 
different θ groups using item fit plots. Examinees are divided into different groups based on their θ 
estimates or by a proxy for θ such as total test score. Here, we use total test scores to form groups. This is   
equivalent to using groups based on θ estimates under the 1P model. Since the LSAT data has 5 
dichotomously scored items, there are six possible values of total test scores: 0, 1, 2, 3, 4, and 5. Thus, the 
examinees will be grouped into 6 groups. In this program, the two data sets that are required include the 
LSAT data set of observed item responses and a posterior predictive response data set formatted with 
NSIM (500) data sets of 1000 observations each or 500 × 1000 observations (Format 1). While this data set 
could be created using Program 8.2, the posterior predictive response data set (LSAT_PRED) was created 
in Format 2 using the PREDDIST statement.
In the program, examinees’ total test scores (TS) in the observed response data (LSAT data set) are 
computed first, and the proportions of correct responses for each group are obtained using the MEANS 
procedure with the BY option. The observed proportions are saved in the data set OBS_PROPS ߑ.The 

224   Bayesian Analysis of Item Response Theory Models Using SAS 
 
 
same procedure is then used to compute predicted correct proportions for each of the 500 predicted data 
sets of simulated item responses by converting the LSAT_PRED data set and transforming it to a data set 
with 500 × 1000 observations (PRED_DIST data set) or Format 1 using the macro TS ߒ. 
Using the MEANS procedure, the predicted proportions on each item for the 6 groups (ts = 0, 1, …, 5) are 
computed based on the data sets of predicted response (500). All predicted proportions for each item across 
the NSIM samples are then saved in the data set PRED_PROPS ߓ. For some of the 500 predicted data sets, 
however, the predicted proportions are 0 for some total score groups. For example, if no one had a total 
score of 0, the MEANS procedure computed only the proportions for the other five groups. To account for 
these cases, a TS_EXP data set is created to include all possible total scores for each simulated sample ߔ. 
This data set is merged with the data PRED_PROPS in order to create 6 groups for all samples. The 
missing proportions for some groups are then replaced with 0s ߕ.
When you compare the observed and predicted proportions, percentiles for the proportions for the 
replicated data and for each item are computed ߖ and merged with the observed proportions into the data 
set ALL_PROPS ߗ. The observed proportions can be compared with the percentiles for the replicated data 
in this data set. Alternatively, the proportion correct responses for the 6 total score groups and for each item 
can be plotted using SERIESPLOT statements and SAS GTL program code (see Program 8.7 on the 
authors’ webpage for the complete program). 
Program 8.7: Item Fit Plots of Proportion Correct Statistics 
/* control parameters */ 
%let nperson=1000; 
%let nitem=5; 
%let npredsamples=500; 
%let ncat=2; 
 
/* compute observed correct response proportions for each group */ 
data obs_ts; 
  set lsat; 
  ts=sum(of x:); 
  person=_n_; 
run; 
proc sort data=obs_ts; by ts; run; 
proc means data=obs_ts noprint;ߑ 
  var x1 x2 x3 x4 x5; by ts; 
  output out=obs_props  mean=; 
run;  
/* compute predicted correct response proportions for each group */ 
%macro ts;ߒ 
data pred_dist; 
  set lsat_pred; 
  array x{*} x1-x&nitem; 
  sample=_n_; 
  %do i=1 %to &nperson; 
     ts=0; 
     %do j=1 %to &nitem; 
         ts=ts+x&j._&i; 
 
  x[&j}=x&j._&i; 
     %end; 
     i=&i; 
     output; 
  %end; 
  keep ts x1-x&nitem sample i; 
run; 
%mend; 
% ts; 
 
 
 
 

Chapter 8: Bayesian Model-Checking for IRT Models   225 
proc sort data=pred_dist; by sample ts; run; 
proc means data=pred_dist noprint; ߓ 
  var x1-x&nitem; by sample ts; 
  output out=pred_props  mean=; 
run; 
 
data ts_exp; ߔ 
  do sample=1 to &npredsamples; 
  do ts=0 to %eval(&nitem*(&ncat-1)); 
     output; 
  end; 
  end; 
run; 
data pred_props; ߕ 
  merge ts_exp(in=a) pred_props(in=b); by sample ts; 
  array x{&nitem}; 
  if _freq_=. then _freq_=0; 
  do j=1 to &nitem; 
     if x{j}=. then x{j}=0; 
  end; 
run; 
proc sort data=pred_props; by ts sample; run; 
 
/* compute percentiles of predicted correct proportions for each group */ 
%macro loopitems;  
%do j=1 %to &nitem; 
proc univariate data=pred_props noprint;ߖ 
    var x&j; by ts; 
    output out=pred_pcts&j  pctlpre=item&j pctlpts=5 50 95; 
run; 
%end; 
%mend; 
%loopitems; 
 
/* merge observed proportions with percentiles of predicted values */ 
data all_props;ߗ 
  merge obs_props pred_pcts1-pred_pcts&nitem; by ts; 
run; 
Output 8.4 illustrates the item fit plots that compare the observed proportions with the percentiles of 
proportions for the replicated data for the five LSAT items. The x-axis of the plot denotes the possible total 
score groups, and the y-axis denotes the proportions of correct response. The solid line connecting the 
observed proportions forms the observed item category curve (ICC) for each item, and the other three 
dashed lines connect the 5th, 50th, and 95th predicted proportions separately. These lines form three ICCs for 
the predicted or replicated data. The observed ICC for each item is within the 5th and 95th predicted ICCs 
and is also close to the median ICC, indicating adequate fit of the 1P model to each LSAT item. 
 

226 Bayesian Analysis of Item Response Theory Models Using SAS
 
 
Output 8.4: Item Fit Plots for the LSAT Items—1P Model 
 
Item Fit Results with Use of Yen’s Q1 Measure 
Item fit plots provide meaningful insights into the nature of item misfit and have been recommended by 
Hambleton, Swaminathan, and Rogers (1991). However, it is also useful to use item-fit statistics that 
compare statistically observed and expected values to evaluate item fit. This example illustrates how to use 
classic item-fit statistics with the PPMC. Yen’s Q1 is selected as a discrepancy measure to evaluate the fit 
of the 1P model to the LSAT item. The proportion correct discrepancy measure used in the previous 
example depends only on the data, the discrepancy measure based on Yen’s Q1 statistic depends on both 
the data and model parameters (δδ). Thus, the discrepancy measure is denoted as T(Drep, δ) for the replicated 
data and T(D, δ) for observed data. 
Program 8.8 computes Yen’s Q1 and PPP values for each item for the observed and predicted response 
data. This program uses the following SAS data sets: observed responses (LSAT), posterior samples of 
model parameters (500 draws in OUTPOST_SAMPLE_LSAT), and posterior predictive responses (500 
sets of simulated responses) in PRED_DIST_LSAT. PRED_DIST_LSAT is in Format 1 and contains 500 
data sets × 1000 simulated observations based on the set of posterior samples for model parameters indexed 
by sample (1 to NSIM) and persons (1 to N). This program assumes that these data sets have been created 
and are available in the WORK library. Note that it is essential that the predicted data be generated based 
on the data set of posterior samples in the SAS data set OUTPOST_SAMPLE_LSAT. This is required for 
any PPMC analysis where the discrepancy measure is based on both the data and model parameters. 
Program 8.3 can be used to select the posterior samples and compute the predicted item responses to ensure 
this constraint. 
The item parameter estimates and θ estimates are saved in two separate data sets: ITEMPARS and THETA
ߑ. Next, 10 equal-sized θ groups are created for each sample using PROC RANK with the BY option, and 
these groups are saved in the data set THETAGRP ߒ. The mean theta for each group is obtained using 
PROC UNIVARIATE ߓ with results (means and number of persons in each group) saved in the data set 
MEANOUT. This data set is restructured into a data set MEANS ߔwhich has, for each NSIM simulated 
data sets (500), mean theta values for each group (MEAN_G1−MEAN_G10) and sample size counts 
(NSUBG1–NSUBG10) for each group.  

Chapter 8: Bayesian Model-Checking for IRT Models   227 
The next step is to compute the observed number of examinees responding in each response category (0 
and 1 in this example) for each theta group. PROC FREQ is used to cross-tabulate the response (X= 0,1) by 
theta group (GROUP) for each item. A macro LOOPITEMS executes the FREQ procedure for all five 
items ߕ. Note that the SPARSE option was specified in order to include any observation with a zero 
frequency. Frequency counts for each item are merged into a single data set (OBS_ALLITEMS) and 
restructured into a data set OBSDIST that has NSIM rows (500) of frequency counts ߖ.  
This process is then repeated for the predicted item responses (PRED_DIST_LSAT). The data set that is 
processed must be organized as Format 1. So in this case, you need to restructure the posterior predictive 
data set using the RESTRUCTURE macro ߗ. The macro LOOPITEMS is used to execute the FREQ 
procedure on the predicted item responses with frequency counts processed and saved in the data set 
PREDIST ߘ. Each of these data sets includes variables reflecting frequency counts in the cross-tabulation 
of the 10 discrete theta groups with response categories 0 and 1 for each of 5 items (NITEM × NCAT × 
NGROUPS values). Each of these 100 values are denoted Ojgk in Yen’s Q1 formula, where j is the item, g 
refers to the group, and k is the response category (see the section “Item-Level Measures” for detail on this 
statistic). 
When you compute Ejgk in Yen’s Q1 formula, the item parameters and group mean thetas are merged with 
the observed and predicted frequencies ߙ Then, for each item, mean thetas for groups and the sampled 
item parameter values are used to compute model-based expections (proportions responding 0 or 1) for 
each theta group. Because the 1P model was being investigated, the 1P model was used to compute the 
probability of responding 0 and 1, and expected frequencies were computed using this probability times the 
number of examinees in each group. Given Ojgk and Ejgk, Yen’s Q1 measure for item j is equal to: 
σ
ቂ൫ܱ௝௚଴െܧ௝௚଴൯
ଶܧ௝௚଴
ൗ
+ ൫ܱ௝௚ଵെܧ௝௚ଵ൯
ଶܧ௝௚ଵ
ൗ
ቃ
ଵ଴
୥ୀଵ
 ߆ 
To compute the Q1 measures for the observed responses, you compute Ojgk using the observed frequencies 
in the variables of the array OBSFREQ{J,G,X}. To compute the Q1 measures for the replicated responses, 
you compute Ojgk using the predicted frequencies in the variables of the array PREDFREQ{J,G,X}. The 
Ejgk are equivalent when you compute Q1 for the observed and replicated responses. 
The observed and predicted Yen’s Q1 measures are saved in the variables Q1OBS1−Q1OBS5 and 
Q1PRED1−Q1PRED5 in the data set Q1. Each of 500 pairs of observed and predicted measures may be 
compared, and PROC MEANS may be used to compute PPP values or the proportion of times across R 
replications that T(Drep, δδ) equals or exceeds T(D, δ). These 500 pairs of observed and predicted Yen’s Q1 
measures can also be displayed in a scatterplot using SAS GTL commands as illustrated in the program on 
the author’s webpage for this book. 
 
 
 

228   Bayesian Analysis of Item Response Theory Models Using SAS 
 
 
Program 8.8: Evaluating Item Fit with Use of Yen’s Q1 Measure 
/*control variables*/ 
%let nperson=1000; 
%let nitem=5; 
%let npostsample=500; 
%let ngroups=10;  /*number of discrete theta groups for persons */ 
%let ncat=2; 
 
/*read in observed data and sampled posterior estimates and separate item 
and theta parameter samples in different datasets*/ 
data obs;   
  set lsat; 
  person=_n_; 
run;  
data outpost_sample_lsat;ߑ  
  set outpost_sample_lsat; 
  sample=_n_; 
run; 
data theta;  
  set outpost_sample_lsat;  
  array theta_{&nperson}; 
  do person=1 to &nperson; 
    theta=theta_[person]; 
  output; 
  end; 
  keep sample person theta; 
run; 
data itempars;  
  set  outpost_sample_lsat; 
  keep sample a b1-b&nitem; 
run; 
 
/* create equal-sized theta groups - add rank for each person */ 
data theta; 
  set theta; 
  theta1=theta; 
run; 
proc rank data=theta out=thetagrp (rename=(theta1=group)) 
groups=&ngroups; ߒ 
  var theta1; by sample; 
run; 
data thetagrp; /* rank for each theta within each dataset added */ 
  set thetagrp; 
  group=group+1; 
run; 
 
/* Collect statistics for each theta group – means and counts */ 
proc sort data=thetagrp; by sample group; 
proc univariate data=thetagrp noprint; ߓ 
  var theta; by sample group; 
  output out=meanout mean=mean n=nsubg; 
proc transpose data=meanout out=means1 prefix=mean_g; var mean;by sample; 
proc transpose data=meanout out=means2 prefix=nsubg; var nsubg;by sample; 
data means; ߔ/* dataset has mean theta and counts per group */ 
  merge means1(drop=_name_ _label_) means2 (drop=_name_ _label_); 
  by sample; 
run; 
 
 
 
 

Chapter 8: Bayesian Model-Checking for IRT Models   229 
/* observed number scoring in each response category for each group */ 
/* dataset has theta, rank, and observed item responses */ 
proc sort data=thetagrp; by person; 
data obsgrp;  
  merge thetagrp obs; by person;  
proc sort data=obsgrp; by sample; 
run; 
 
/* collect frequency counts for tables: group x response for each item */ 
%macro loopitems (resp=); ߕ 
%do j=1 %to &nitem; 
proc freq data=&resp noprint;  
  tables x&j*group/ nocum nopercent sparse out=item&j(rename=(x&j=x)); 
  by sample; 
run; 
data item&j; 
  set item&j; 
  item=&j; 
  drop percent; 
run; 
%end; 
%mend; 
%loopitems(resp=obsgrp); /* run macro for observed data */ 
 
data obs_allitems; /* dataset has frequency counts for all items */ 
  set item1-item&nitem; 
run; 
proc sort data=obs_allitems; by sample item group x; 
proc transpose data=obs_allitems  out=obsdist (drop=_name_ _label_)  
prefix=obsfreq; ߖ 
  var count; by sample; 
run; /* obsdist: nsim frequency counts (nitems x ncat x ngroups) */ 
 
/* Compute predicted frequency in each response category for each group, 
   Uncomment macro if restructuring predictive distribution required */ 
%macro restructure; ߗ 
data pred_dist; 
  set pred_dist_lsat; 
  array x{*} x1-x&nitem; 
  sample=_n_; 
  %do person=1 %to &nperson; 
     %do j=1 %to &nitem; 
 
  x[&j}=x&j._&person; 
     %end; 
     person=&person; 
     output; 
  %end; 
  keep x1-x&nitem sample person; 
run; 
%mend; 
%restructure; 
 
proc sort data=thetagrp; by sample person; 
proc sort data=pred_dist; by sample person; 
data predgrp; 
  merge thetagrp pred_dist; by sample person; 
run; 
 
 
 
 

230   Bayesian Analysis of Item Response Theory Models Using SAS 
 
 
%loopitems(resp=predgrp); /* run macro for predicted data */ ߘ 
data pred_allitems; /* dataset has frequency counts for all items */ 
  set item1-item&nitem; 
run; 
proc sort data=pred_allitems; by sample item group x; 
proc transpose data=pred_allitems  out=preddist (drop=_name_ _label_)  
prefix=predfreq;  
  var count; by sample; 
run; /* preddist: nsim frequency counts (nitems x ncat x ngrouops) */ 
 
/* compute Yen Q1 stat */ ߙ 
data obs_pred;  
  merge itempars means obsdist preddist; by sample; 
 
data q1; /* dataset with Q1 statistic for each item, obs and predicted */  
  set obs_pred; 
  array b{&nitem} b1-b&nitem; 
  array mean{&ngroups} mean_g1- mean_g&ngroups;  
  array nsubg{&ngroups} nsubg1- nsubg&ngroups;  
  array obsfreq{&nitem,&ngroups,&ncat}; 
  array predfreq{&nitem,&ngroups,&ncat}; 
  array exfreq{&ncat}; 
  array q1obs{&nitem} q1obs1-q1obs&nitem; 
  array q1pred{&nitem} q1pred1-q1pred&nitem; 
 
  do j=1 to &nitem; 
    chisq_obs=0; chisq_pred=0; 
  do g=1 to &ngroups; 
    do x=0 to &ncat-1;   
      if x=(&ncat-1) then p=1/(1+exp(-a*(mean[g]-b[j]))); 
      else if x=0 then p=1-1/(1+exp(-a*(mean[g]-b[j]))); 
      exfreq{x+1}=p*nsubg[g]; 
      chisq_obs = chisq_obs+(obsfreq{j,g,x+1}- 
                  exfreq{x+1})**2/exfreq{x+1}; ߆  
      chisq_pred = chisq_pred+(predfreq{j,g,x+1}- 
                   exfreq{x+1})**2/exfreq{x+1};  
      end; 
  end; 
  q1obs[j]=chisq_obs; 
  q1pred[j]=chisq_pred; 
  end; 
  keep sample q1obs1-q1obs&nitem q1pred1-q1pred&nitem; 
run; 
 
/* compare predicted and observed Q1 to obtain PPP values */ 
data PPP; 
  set q1; 
  array q1obs{&nitem} q1obs1-q1obs&nitem; 
  array q1pred{&nitem} q1pred1-q1pred&nitem; 
  array sign{&nitem}; 
  do j=1 to &nitem; 
    if q1pred{j}>=q1obs{j} then sign{j}=1;  
    else sign{j}=0; 
  end; 
  drop j; 
run; 
 
proc means data=PPP noprint; 
  var sign1-sign&nitem; 
  output out=PPP_Q1 mean=; 
run; 
Output 8.5 shows the scatterplots of the distributions of observed and predicted values of Yen’s Q1 measure 
for the five LSAT items. This scatterplot compares graphically the empirical distributions of the different 
 

Chapter 8: Bayesian Model-Checking for IRT Models 231
measures. The solid line is the 450 line that represents the case where the distributions for the Q1 measures 
for the observed and predicted responses are equivalent. You can see that the distributions are not 
equivalent, but the predicted values are not consistently larger or smaller than the observed values. This 
provides graphical evidence about the fit of the 1P for each LSAT item. If a majority of values based on the 
observed data are larger than the predicted values, the scatterplots would be located below the solid line. 
The mean PPP values are also shown in the graphs. These values range from 0.46 to 0.53, also indicating 
good fit between model predictions and the observed item responses. 
Output 8.5: Comparing Observed and Predicted Yen’s Q1 Measures—1P Model
Example 4: Observed and Predicted Odds Ratio Measure 
In order to check if the local independence assumption holds in a Bayesian IRT application, you can use the 
odds ratio (OR) or Yen’s Q3 statistics between pairs of items as discrepancy measures. The example in this 
section uses PPMC with the OR measure to evaluate if the LSAT data and the DASH data meet the 
assumption of local independence. Note that the OR measure is only a function of the data. For example, 
there are 5 items and 10 item pairs for the LSAT data. Each item pair has one observed OR value based on 
the observed data set, but has 500 predicted values based on the 500 predicted response data sets. Each of 
the 500 predicted OR values are compared with the observed value to see if there are any systematic 
differences.  
Program 8.9 provides the code for comparing the observed and predicted OR values and also computes the 
PPP value for each pair of LSAT items. The program assumes that the LSAT data and a data set of 
posterior predictions (LSAT_PRED) is available in the work library. The data set of posterior predictions is 
in the format defined by use of the PREDDIST statement with the PROC MCMC command (Format 2). 
A macro OR_OBS ߑis used to iteratively run PROC FREQ and obtain OR values for all 10 item pairs 
with the RELRISK option ߒ. The 10 observed OR values (OBS_OR1−OBS_OR10) are saved in the data 
set OBS_OR. The 500 predicted response data sets are then restructured into the data set PRED, which has 
5 response variables (YREP1−YREP5) and 500 data sets × 1000 observations ߓ. To compute the OR 
values for the predicted response data sets, run a macro OR_PRED to run PROC FREQ with the BY 
SAMPLE and to obtain the predicted OR values for the 500 predicted responses ߔ. The 500 predicted OR

232   Bayesian Analysis of Item Response Theory Models Using SAS 
 
 
values for each item pair are saved in the data PRED_OR, and the variables containing the OR values for 
10 pairs are denoted PRED_OR1 – PRED_OR10.  
To compute the PPP values, you first merge the observed and predicted OR values into the data set 
OBS_PRED_OR ߕ. PPP values, or the proportion of times across R replications that OR(Drep) equals or 
exceeds OR(D), are collected in an indicator variable COUNT ߖ. COUNT equals 1 when the predicted 
value is equal to or greater than the observed value. The PPP value for each item pair is the proportion of 
replicated data sets with COUNT =1 and may be obtained using PROC MEANS ߗ. The PPP values for all 
item pairs are saved in the variables PPP_OR1 – PPP_OR10 in the data set PPP_OR. They range from 0.14 
to 0.82 and indicate that the LSAT data does not violate the local independence assumption under the 1PL 
model. 
PPP values may also be displayed using pie plots. The code used to create pie plots is not included in the 
program displayed below but is included in the program on the authors’ webpage for the book. Output 8.6 
displays the pie plots for the 10 item pairs. There is one pie plot for each item pair, and the proportion of a 
circle that is filled with the black color corresponds to the magnitude of the PPP value. Pie plots can be 
useful for visualizing patterns among the PPP values for item pairs when there are a large number of item 
pairs—for  example, blocks of item pairs with values close to 0 and blocks of item pairs with values close 
to 1. 
Program 8.9: Comparing Observed and Predicted Item-Pair Odds Ratios—1PL Model 
/* control parameters */ 
%let nitems=5; 
%let nperson=1000; 
%let npostsample=500; 
%let nors=%eval(&nitems*(&nitems-1)/2); /* total number of odds ratios */ 
 
/*compute the ORs for the observed dataset*/ 
%macro or_obs;   ߑ 
proc datasets; delete or_allitems; run; 
%do i=1 %to (&nitems-1); 
%do j=(&i+1) %to &nitems; 
  proc freq data=lsat noprint; ߒ 
    tables x&i * x&j / relrisk; 
    output out=or_table OR; 
  proc append base=or_allitems(keep=_rror_) data=or_table force nowarn;  
  run; 
%end; 
%end; 
%mend; 
%or_obs; 
run; 
proc transpose data=or_allitems out=obs_or(drop=_name_ _label_)     
prefix=obs_or; 
run; 
 
/*compute the ORs for the predicted response datasets */ 
data pred; 
  set lsat_pred; 
  sample=_n_; 
run; 
data pred; ߓ 
  set pred;  
  array items{&nperson,&nitems} x1_1—x&nitems._&nperson; 
  array yrep{&nitems} yrep1-yrep&nitems; 
  do ii=1 to &nperson; 
  do jj=1 to &nitems; 
     yrep[jj]=items[ii,jj]; 
  end; 
  output;  
end; 
 

Chapter 8: Bayesian Model-Checking for IRT Models   233 
keep sample yrep1-yrep&nitems; 
run; 
 
%macro or_pred;ߔ 
proc datasets; delete or_allitems; run; 
%do i=1 %to (&nitems-1); 
%do j=(&i+1) %to &nitems; 
  proc freq data=pred noprint;   
  tables yrep&i * yrep&j  / relrisk;  
  output out=or_table OR; 
  by sample; 
proc append base=or_allitems(keep=_rror_ sample) data=or_table force 
nowarn;  
run; 
%end; 
%end; 
%mend; 
%or_pred; 
proc sort data=or_allitems; by sample; 
proc transpose data=or_allitems out=pred_or(drop=_name_ _label_) 
prefix=pred_or; 
  by sample; 
run; 
 
/*compute the posterior predictive p-values*/ 
data obs_pred_or;ߕ 
  set pred_or; 
  if _n_=1 then set obs_or; 
run; 
data ppmc; 
  set obs_pred_or end=lastobs; 
  array count{*} count1-count&nors; 
  array obs_or{*} obs_or1-obs_or&nors; 
  array pred_or{*} pred_or1-pred_or&nors; 
  do i=1 to &nors;ߖ 
     if pred_or[i]>=obs_or[i] then count[i]=1; else count[i]=0; 
  end; 
  keep count1-count&nors; 
run; 
proc means data=ppmc noprint;ߗ 
  var count1-count&nors; 
  output out=PPP_OR (drop=_type_  _freq_) mean=ppp_OR1-ppp_OR&nors; 
run; 
 

234 Bayesian Analysis of Item Response Theory Models Using SAS
 
 
Output 8.6: Pie Plots to Show the PPP values of Odds Ratio for Item Pairs—LSAT Data 
A PPMC analysis using odds ratios as the discrepancy measure was also conducted with the DASH data. 
Program 8.9 was modified, but is not presented (see Program 8.10 on the authors’ webpage for the book), 
for this analysis. However, to analyze the DASH item responses, you need to consider three different types 
of odds ratios in an R × C contingency table for polytomously scored items: local odds ratios, local-global 
odds ratios, and global odds ratios (Agresti, 2002). Local odds ratios are defined using cells in adjacent 
rows and adjacent columns. A problem with local odds ratios is that the number of local odds ratios will 
increase dramatically as the number of response categories for each item increases.  
One alternative way to measure the association in as R × C contingency table is to dichotomize one of the 
items according to a cut-point and compute local-global odds ratios. While this type of odds ratio reduces 
the number of odds ratios to be compared, it still requires examination of multiple odds ratios for a single 
item pair. Thus, a single odds ratio is often preferred in order to simplify or summarize the association in a 
R × C contingency table by dichotomizing the response categories of each item in an item pair. When you 
do so, a 2 × 2 contingency table is constructed for each item pair, and a single global odds ratio is 
computed. Use of global odds ratios with a PPMC analysis of polytomous IRT models has been found 
useful (Zhu & Stone, 2011), and these ratios are used in this example. 
Output 8.7 and Output 8.8 present the results using global OR values as a discrepancy measure with 
different cut-points (2 and 3). Recall that the DASH survey was designed to measure physical functioning 
with items asking about the degree of difficulty, from 1 (“No Difficulty”) to 5 (“Unable”), in performing 
different tasks such as opening a jar, writing, and making the bed. Two different cut-points are examined 
because there isn’t an obvious cut-point to use for dichotomizing the responses. You can see that the 
different cut-points generated different results; but, overall, results indicate that between 25% and 33% of 
the OR values demonstrate PPP values that were either < .05 or > .95. Thus, there is evidence that the local 
independence assumption may be compromised for the DASH items. It is possible that this local 
independence is due to the common context (physical functioning) under which the persons are responding 
to the items. Finally, note that this discrepancy measure is not addressing model-data-fit for individual 

Chapter 8: Bayesian Model-Checking for IRT Models 235
items in the same way afforded by Yen’s Q1 statistic; rather, it addresses an underlying assumption for IRT 
model applications. 
Output 8.7: Pie Plots to Show the PPP Values of ORs for Item Pairs—DASH Data (Cut-Point at 2) 
 
Output 8.8: Pie Plots to Show PPP values of Odd Ratios for Item Pairs—DASH Data (Cut-Point at 3) 
 

236   Bayesian Analysis of Item Response Theory Models Using SAS 
 
 
Example 5: Observed and Predicted Yen’s Q3 Measure 
In Example 4, the PPMC with the global OR measure indicates that a number of the DASH item pairs may 
violate the local independent assumption. In this section, we use Yen’s Q3 statistic as the discrepancy 
measure to see if the same conclusion can be drawn. Different from the OR measure, Yen’s Q3 measure is a 
function of both data and parameters. An advantage of using Yen’s Q3 statistic as a discrepancy measure is 
that you don’t need to worry about the construction of global OR values for polytomously scored items. 
Program 8.11 presents the code used to implement PPMC with Yen’s Q3 measure and the DASH data. The 
first step is to read in the 500 samples of posterior estimates (OUTPOST_SAMPLE_DASH) and compute 
the expected item responses for each person based on the GR model and each posterior sampled value ߑ. 
The expected item responses are saved in the data set EX_RESP. A macro Q3 is used to obtain the Q3 
values for each item pair ߒ. This macro requires two input files: one is the observed response file 
(RESPFILE=), and the other is the expected response file (EXRESPFILE=). The obtained Q3 values are 
saved in the data set defined by OUTFILE=.  
You can see that in the macro code, the observed response file and the expected response file are merged 
into one file. Then the residuals reflecting differences between observed and expected item responses are 
calculated. For each item pair, the Q3 value is the correlation between residuals obtained through PROC 
CORR ߓ. The Q3 values for all item pairs are saved in the data set &OUTFILE. 
To compute the observed Q3 values, the observed DASH data (DASH_DATA) is modified for use with the 
macro Q3. Executing the macro produces observed Q3 values for all 45 item pairs (saved in the data set 
OBS_Q3)ߔ. The variables for observed Q3 values are changed to Q3OBS1−Q3OBS45 using the 
RENAME statement. Similar procedures are applied to the predicted response data ߕ, and the Q3 values for 
all 45 item pairs of predicted item responses are saved in the data set PRED_Q3. 
The observed and predicted Q3 values are merged into the data set OBS_PRED_Q3 ߖand the PPP values 
for all item pairs are computed ߗand saved in the variables PPP1 – PPP45 in the data set PPP_Q3 ߘ. 
Examination of the PPP values in the data set PPP_Q3 indicates that more than half of the item pairs have 
the extreme values (values < .05 or values > .95). This is consistent with the PPMC analysis using OR 
values as the discrepancy measure, suggesting a possible violation of the local independence assumption 
for the DASH items.  
Program 8.11: Comparing Observed with Predicted Yen’s Q3 Measure—GR Model 
/* control parameters */ 
%let nitems=10; 
%let ncat=5; 
%let nperson=1000; 
%let npostsample=500; 
%let nq3=%eval(&nitems*(&nitems-1)/2); /* total number of q3 */ 
 
/*compute the expected item responses */ 
data outpost; 
  set outpost_sample_dash; 
  sample=_n_; 
run; 
data ex_resp (rename=(i=person)); ߑ 
  set outpost; 
  array a{&nitems} a1-a&nitems; 
  array b{4,&nitems} b1_1-b1_&nitems b2_1-b2_&nitems b3_1-b3_&nitems  
        b4_1-b4_&nitems; 
  array theta{&nperson} theta_1-theta_&nperson; 
  array ex{&nitems} ex1-ex&nitems; 
  do i=1 to &nperson; 
     do j=1 to &nitems; 
        exp=0; 
     do k=1 to &ncat; 
      if k=1 then  p=1-1/(1+exp(-a[j]*(theta[i]-b[1,j]))); 
      else if k=&ncat then p=1/(1+exp(-a[j]*(theta[i]-b[(&ncat-1),j]))); 
 

Chapter 8: Bayesian Model-Checking for IRT Models   237 
      else p=1/(1+exp(-a[j]*(theta[i]-b[k-1,j])))-1 / 
              (1+exp(-a[j]*(theta[i]-b[k,j]))); 
      exp=exp+k*p; 
     end; 
     ex{j}=exp; 
     end; 
     output; 
  end; 
  keep sample i ex1-ex&nitems; 
run; 
 
/* macro used to compute Yen's Q3 measure */  
%macro q3(respfile=,exrespfile=,outfile=);ߒ 
proc sort data=&respfile; by person; 
proc sort data=&exrespfile; by person; 
 
data resp_ex; 
  merge &respfile &exrespfile; by person; 
run; 
 
data resids; *obtain the residuals; 
  set resp_ex; 
  array x{&nitems}; 
  array ex{&nitems}; 
  array resid{&nitems}; 
  do j=1 to &nitems; 
     resid{j}=x{j}-ex{j}; 
  end; 
  keep sample person resid1-resid&nitems; 
run; 
 
proc sort data=resids; by sample; 
proc corr data=resids noprint outp=corrs; ߓ 
  var resid1-resid&nitems; with resid1-resid&nitems; by sample; 
data corrs; 
  set corrs; 
  where _type_='CORR'; 
  item=input(substr(_name_,6),2.0); 
data corrs; 
  set corrs; 
  array resid{&nitems}; 
  do j1=1 to (&nitems-1); 
     if item=j1 then do; 
     do j2=(j1+1) to &nitems; 
        c=resid{j2}; 
 
output; 
     end; 
     end; 
  end; 
  keep sample j1 j2 c; 
run; 
proc transpose data=corrs out=&outfile(drop=_name_) prefix=q3_; 
  var c; by sample; 
run; 
%mend; 
 
/* compute observed Yen's Q3 measure */  
data obs; 
  set dash_data; 
  person=_n_; 
  rename i1-i&nitems=x1-x&nitems; 
run; 
 
 
 
 

238   Bayesian Analysis of Item Response Theory Models Using SAS 
 
 
%q3(respfile=obs, exrespfile=ex_resp, outfile=obs_q3);ߔ 
 
data obs_q3; 
  set obs_q3; 
  rename q3_1-q3_&nq3=q3obs1-q3obs&nq3; 
run; 
 
/* compute predicted Yen's Q3 measure */ ߕ 
data pred; 
  set pred_dist_dash; 
run; 
data index; 
  do sample=1 to &npostsample; 
  do person=1 to &nperson; 
     output; 
  end; 
  end; 
run; 
data pred; 
  merge pred index; by sample; 
run; 
 
%q3(respfile=pred, exrespfile=ex_resp, outfile=pred_q3);  
 
data pred_q3; 
  set pred_q3; 
  rename q3_1-q3_&nq3=q3pred1-q3pred&nq3; 
run; 
 
/*compute the posterior predictive p-values for Q3 measure */ 
data obs_pred_q3; 
  merge obs_q3 pred_q3; by sample;ߖ 
run; 
data ppmc; 
  set obs_pred_q3 end=lastobs; 
  array count{*} count1-count&nq3; 
  array q3obs{*} q3obs1-q3obs&nq3; 
  array q3pred{*}q3pred1-q3pred&nq3; 
  do i=1 to &nq3; 
     if q3pred[i]>=q3obs[i] then count[i]=1; else count[i]=0;ߗ 
  end; 
  keep count1-count&nq3; 
run; 
proc means data=ppmc noprint;ߘ 
  var count1-count&nq3; 
  output out=PPP_q3 (drop=_type_  _freq_) mean=ppp1-ppp&nq3; 
run; 
PPP values can also be displayed using pie plots as in prior examples (Output 8.9). Note that circles for 
item pairs that are entirely white indicate a PPP value = 0, whereas circles that are entirely black indicate a 
PPP value = 1. You should note that more extreme PPP values were found with Yen’s Q3 measure for the 
DASH item responses than were found with the global OR measure. Because dichotomization of responses 
are required to use global OR values as discrepancy measures with polytomously scored items, use of 
Yen’s Q3 for checking the local independence may be preferred for these types of testing applications. 
However, one advantage of the OR measure as opposed to the Q3 measure is that it is dependent only on the 
data. 
 

Chapter 8: Bayesian Model-Checking for IRT Models 239
Output 8.9: Item Pair Pie Plots of PPP values for Yen’s Q3 Measure—DASH Data 
Example 6: Observed and Predicted Person-Fit Statistic 
This example illustrates the application of PPMC to evaluate person fit. The person-fit statistic lz (see the 
section Person-Fit Measures for detail) is used as the discrepancy measure and is a function of both the 
data and model parameters. Program 8.12 provides the SAS code to assess person fit for the LSAT data 
under the 1PL model. To facilitate the computation of lz values, you restructure the data set of posterior 
samples (OUTPOST_SAMPLE_LSAT) to include 500 data sets x 1000 observations ߑ. A macro LZ is 
used to compute lz values and save results in the data set defined by the macro variable &OUTFILE ߒ. The 
log-likelihood for each person’s response pattern, ݈଴, is calculated (LO). The expected value (ELO) and the 
variance (VLO) of lo are obtained and used to compute the ݈௭ measure. The IRT model used to calculate the 
log-likelihood values may be modified in this macro for other IRT model applications ߓ. 
To compute the lz values for the observed item responses, the observed data LSAT are merged with the 
restructured posterior samples of model parameters. The LZ macro is called to process the merged data 
OBS_PARS and save the observed lz values in the data set LZ_OBS ߔ. The lz values for the 1000 persons 
(observed data) are saved in the variables LZOBS1−LZOBS1000 for each of the 500 posterior samples for 
model parameters (item and person parameters).  
Similarly, the 500 sets of predicted responses are restructured ߕand merged with the restructured posterior 
samples of model parameters ߖ. The LZ macro is called now to compute the lz values for the predicted 
responses ߗ. The data set LZ_PRED includes 1000 variables LZPRED1−LZPRED1000 for each of the 500 
draws of posterior samples for model parameters (item and person parameters). Note that it is essential that 
the predicted data be based on the data set of posterior samples in the data set 
OUTPOST_SAMPLE_LSAT. As mentioned in other examples, this is required for any PPMC analysis 
where the discrepancy measure is based on both the data and model parameters. Program 8.3 can be used to 
select the posterior samples and compute the predicted item responses to ensure this match.  
To compare the observed and predicted lz values, merge the LZ_OBS and LZ_PRED data sets and obtain 
PPP values as in previous examples ߘ. The PPP values for each observation in the LSAT data set are 

240   Bayesian Analysis of Item Response Theory Models Using SAS 
 
 
saved in the data set PPP_LZ ߙ. The PPP values can be examined within this data set for all examinees. 
Extreme values (values < .05 or values > .95) and corresponding response patterns are obtained and saved 
in a separate SAS data set MISPATTERNS in order to identify the aberrant response patterns or patterns 
that are not expected under the 1PL model ߆.  
Among the 1000 persons, a small number of examinees have aberrant response patterns. These misfitting 
response patterns on the five LSAT items are 00100, 00101, 00110, 00111, 01101, 01110, and 11100. 
Recall from Chapter 4 that the five items are ordered by difficulty as follows: b3 > b2 > b4 > b5 > b1. That 
is, Item 3 is the most difficult item, and Item 1 is easiest under the 1PL model. Thus, an examinee 
responding correctly on Item 3 should also be able to respond correctly on easier items. The response 
pattern 00100 means that this person only answered the most difficult item correctly, but answered easier 
items incorrectly. This would be an unlikely response pattern under the model. 
Program 8.12: Comparing Observed and Predicted lz Measures—LSAT Data 
/* control parameters */ 
%let nitems=5; 
%let nperson=1000; 
%let npostsample=500; 
 
/* restructure the posterior samples */ 
data modelpars; 
  set outpost_sample_lsat; 
  sample=_n_; 
  keep sample a b1-b&nitems theta_1-theta_&nperson; 
run; 
data modelpars; ߑ 
  set modelpars; 
  array theta_{*} theta_1-theta_&nperson; 
  do person=1 to &nperson; 
     theta=theta_{person}; 
     output; 
  end; 
  keep a b1-b&nitems theta sample person; 
run; 
 
/*macro to compute the Lz*/ 
%macro lz(infile=, outfile=); ߒ 
data lz; 
  set &infile; 
  array b{*} b1-b&nitems; 
  array x{*} x1-x&nitems; 
  lo=0; elo=0; vlo=0; 
  do j=1 to &nitems; 
     z=a*(theta - b[j]); /* 1P IRT model used to compute likelihood */ ߓ 
     call logistic(z); 
     lo=lo+(x[j]*log(z) + (1-x[j])*log(1-z)); 
     elo=elo+(z*log(z) + (1-z)*log(1-z)); 
     vlo=vlo+(z*(1-z))*log(z/(1-z))**2; 
  end; 
  lz=(lo-elo)/sqrt(vlo); 
  keep sample person lz; 
run; 
proc sort data=lz; by sample; 
proc transpose data=lz out=&outfile prefix=lz; id person; by sample; run; 
%mend; 
 
 
 
 

Chapter 8: Bayesian Model-Checking for IRT Models   241 
/* compute the Lz values for the observed item response data */ 
data obs; 
  set lsat; 
  person=_n_; 
run; 
proc sort data=modelpars; by person; 
data obs_pars; 
  merge modelpars obs; by person; 
run; 
%lz(infile=obs_pars, outfile=lz_obs); ߔ 
data lz_obs; 
  set lz_obs; 
  rename lz1-lz&nperson=lzobs1-lzobs&nperson; drop _name_; 
run; 
 
/* compute the predicted Lz values for the simulated item responses */ 
data pred; 
  set pred_dist_lsat; 
  sample=_n_; 
run; 
data pred; ߕ 
  set pred;  
  array xx{&nperson,&nitems} x1_1--x5_&nperson; 
  array x{&nitems} x1-x&nitems; 
  do person=1 to &nperson; 
     do j=1 to &nitems; 
        x[j]=xx[person,j]; 
     end; 
  output;  
  end; 
  keep sample person x1-x&nitems; 
run; 
 
proc sort data=modelpars; by sample person; 
data pred_pars; merge modelpars pred; by sample person; run;ߖ 
 
%lz(infile=pred_pars, outfile=lz_pred); ߗ
data lz_pred; 
  set lz_pred; 
  rename lz1-lz&nperson=lzpred1-lzpred&nperson; drop _name_; 
run; 
 
/*compute the posterior predictive ppp values*/ 
data obs_pred_lz; merge lz_obs lz_pred; by sample; run; 
data ppmc; 
  set obs_pred_lz end=lastobs; 
  array count{*} count1-count&nperson; 
  array lzobs{*} lzobs1-lzobs&nperson; 
  array lzpred{*}lzpred1-lzpred&nperson; 
  do i=1 to &nperson; 
     if lzpred[i]>=lzobs[i] then count[i]=1; else count[i]=0;ߘ 
  end; 
  keep count1-count&nperson; 
run; 
proc means data=ppmc noprint; 
  var count1-count&nperson; 
  output out=PPP_lz (drop=_type_  _freq_) mean=ppp1-ppp&nperson; 
run; 
 
 
 

242   Bayesian Analysis of Item Response Theory Models Using SAS 
 
 
data ppp_lz; ߙ 
  set ppp_lz; 
  array ppp{&nperson}; 
  do person=1 to &nperson; 
     ppp_value=ppp{person}; 
     output; 
  end; 
  keep person ppp_value; 
run; 
 
/* find misfitting response patterns */  
data misfit;  
  set ppp_lz; 
  where ppp_value>0.95 or ppp_value<0.05; 
run; 
proc sort data=misfit; by person; run; 
data misfit; merge misfit (in=a) obs (in=b); by person; if a; run; 
data mispatterns;߆ 
  set misfit; 
  pattern=cat(of x1-x5); 
run; 
Scatterplots can also be plotted to illustrate the relationship between the lz values for observed and 
predicted responses. The code used to draw the scatterplot is similar to other examples and is included in 
the program on the authors’ webpage for the book. Note that only one scatterplot for one person exhibiting 
person fit (PPP value =.5) and one scatterplot for another person exhibiting person misfit (PPP value =.99) 
are presented to illustrate the process of generating the graphs.  
Output 8.10 displays the scatterplots of observed and predicted lz values for these two persons. The left plot 
is for the person exhibiting person fit (PPP value =.5). This person’s response pattern was 10011, 
indicating that this person correctly answered the three easier items (Items 1, 4, 5) and incorrectly answered 
the two harder items (Items 2 and 3). The data points are scattered around the diagonal line, suggesting that 
there is no systematic difference between the observed and predicted lz values.  
In contrast, the plot on the right is for a person exhibiting person misfit (PPP value =.99). This person’s 
response pattern was 00110, indicating that the easier items were answered incorrectly and the harder items 
correctly (PPP value is 0.99). The values in the scatterplot are also above the diagonal line indicating that 
the predicted lz values are systematically greater than the observed values. Both the PPP value and the 
scatterplot pattern indicate misfit of the 1P model for this person. 
 

Chapter 8: Bayesian Model-Checking for IRT Models 243
Output 8.10: Scatterplot of Observed and Predicted ࢒ࢠ Measures—LSAT data 
 
Example 7: Use of PPMC to Compare Models 
It is also possible to use PPMC to compare models and supplement the model comparison statistics 
discussed in Chapter 7. Because the item-total score correlation is related to the discrimination parameter or 
slope parameter in IRT models, this statistic is particularly useful as a discrepancy measure in a PPMC 
analysis comparing models that estimate a common slope parameter for all items in an IRT model. For 
example, we could compare the 1P and 2P models for dichotomously scored items using the PPMC method 
to help determine the significance of incorporating separate slope parameters in the model.  
The same approach could be used to compare a one-parameter GR model with the GR model. Program 8.6 
was modified slightly to perform a comparison of the one-parameter GR model and the GR model for the 
DASH data (Program 8.13 for the GR model is included on the authors’ webpage for the book). Results 
from the two PPMC analyses is provided in Outputs 8.11 and 8.12. You can see that in Output 8.11, the 
item-corrected total score correlation for the observed data falls at the boundary or outside the credible 
interval for the predicted correlations for a number of items under a one-parameter GR model. However, in 
Output 8.12, you can see that the item-corrected total score correlation for the observed data falls within the 

244 Bayesian Analysis of Item Response Theory Models Using SAS
 
 
credible interval for all items, indicating that the GR model, with separate slope parameters for each item, 
provides better model fit than the one-parameter GR model. 
Output 8.11: Comparing Observed and Predicted Item-Test Score Correlations—One-Parameter GR 
Model (DASH Data) 
 
Output 8.12: Comparing Observed and Predicted Item-Test Score Correlations—GR Model (DASH 
Data) 
 

Chapter 8: Bayesian Model-Checking for IRT Models 245
A second example involves another comparison between competing models with the DASH data. Recall 
that the DASH data consist of rating scaled items. For these types of applications, it may be of particular 
interest to compare a model that specifies separate response scales for items (GR) as opposed to a model 
that specifies a common response scale across items (RS-GR). This comparison determines the extent to 
which respondents are interpreting the scale and response options similarly across the items.  
In the model comparison results in Chapter 7, the GR model was found to be the preferred model based on 
both CPO statistics and DIC statistics. To compare these models using a PPMC analysis, Yen’s Q3 measure
was used as the discrepancy measure. Output 8.13 presents the pie plots of PPP values for the GR model, 
and Output 8.14 presents the pie plots of PPP values for the RS-GR model. You can see that the pattern in 
the PPP values (degree pie plots are dark or white) is nearly equivalent across the two graphs. Thus, with 
respect to Yen’s Q3 statistic as the discrepancy measure, the two models appear very similar. 
This finding of similarity between the two models runs counter to the example comparing the GR and RS-
GR models in Chapter 7. In that example, the GR model was found to better approximate the DASH item 
responses than the RS-GR model. This PPMC example illustrates that the choice of the discrepancy 
measure is important. As discussed, discrepancy measures should be chosen to reflect potential sources of 
misfit most relevant to the models under evaluation. In addition, multiple discrepancy measures should be 
used to evaluate models. Because Yen’s Q3 measure is more sensitive to departures of models that violate 
the assumption of local dependence, the measure may not be sensitive to the type of response scale 
assumed under different GR models (common and separate response scales). Thus, another measure, such 
as Yen’s Q1 statistic may prompt another interpretation about the comparison of the two models. This 
analysis is left to the reader as an exercise. 
Output 8.13: Item Pair Pie Plots of PPP values for Yen’s Q3 Measure—2P-GR Model 

246 Bayesian Analysis of Item Response Theory Models Using SAS
 
 
Output 8.14: Item Pair Pie Plots of PPP values for Yen’s Q3 Measure—RS-GR Model 
 
A final example uses PPMC results to compare competing models for the bifactor model described and 
estimated in Chapter 6. Recall that the bifactor model is a multidimensional IRT model. A competing 
model of interest is a unidimensional IRT model. You can modify Program 8.11 to perform these analyses 
by changing the file, parameter, and model specifications. Although the programs to perform this analysis 
are not presented, Program 8.14 for the analysis based on the bifactor model is included as an example on 
the authors’ webpage for this book. 
Outputs 8.15 and 8.16 present PPMC results using Yen’s Q3 statistic as the discrepancy measure to 
compare these two models. This discrepancy measure should be useful in this context because 
dimensionality and local independence assumptions are related. The same data used to illustrate the bifactor 
model in Chapter 6 are used here.  
In Output 8.15. you can see that many of the PPP values reflect extreme values (pie plots that are nearly all 
white or nearly all dark), suggesting that the unidimensional model does not fit the data. In addition, the 10 
items clearly fell into two clusters that matched the modeled factor structure: Items 1−5 formed one cluster, 
and Items 6−10 formed another cluster. This result suggests that a two-dimensional model might be needed 
to fit the data, and you can see in Output 8.16 that the PPP values are not extreme, suggesting the bifactor 
model fits the data.  
From these three examples, you can see that a PPMC analysis comparing models can supplement analyses 
using the DIC or CPO statistics described in Chapter 7. When you are reporting Bayesian analyses of IRT 
model applications, an advantage of a PPMC analysis is that it can be used to compare both the relative and 
absolute fit of different models. In contrast, the DIC and CPO model-comparison indices only consider the 
relative fit of different models. Moreover, a PPMC analysis can provide more diagnostic information that 
might suggest alternative models for a specific IRT application. 
 
 

Chapter 8: Bayesian Model-Checking for IRT Models 247
Output 8.15: Item Pair Pie Plots of PPP values for Yen’s Q3 Measure—Unidimensional IRT Model 
Output 8.16: Item Pair Pie Plots of PPP values for Yen’s Q3 Measure—Bifactor IRT Model 
 

248   Bayesian Analysis of Item Response Theory Models Using SAS 
 
 
 
 

References 
Ackerman, T. A. (1994). Using multidimensional item response theory to understand what items and tests 
are measuring. Applied Measurement in Education, 7, 255-278.
Ackerman, T. A. (1996). Graphical representation of mulitidimensional item response theory analyses. 
Applied Psychological Measurement, 20, 311-330.
Ackerman, T. A., Gierl, M. J., & Walker, C. M. (2003). An NCME instructional module on using 
multidimensional item response theory to evaluate educational and psychological tests. 
Educational Measurement: Issues and Practice, 22, 37-53.
Agresti, A. (2002). Categorical data analysis. Hoboken, NJ: John Wiley.
Akaike, H. (1974). A new look at the statistical model identification. IEEE Transactions on Automatic 
Control, 19, 716-723.
Andrich, D. (1978). A rating formulation for ordered response categories. Psychometrika, 43, 561-573.
Baker, F. B., & Kim, S. (2004). Item response theory: Parameter estimation techniques. New York: Marcel 
Dekker, Inc.
Béguin, A. A., & Glas, C. A. W. (2001). MCMC estimation and some model-fit analysis of 
multidimensional IRT models. Psychometrika, 66, 541–562.
Bock, R. D. (1972). Estimating item parameters and latent ability when responses are scored in two or more 
nominal categories. Psychometrika, 37, 29-51.
Bock, R. D., & Lieberman, M. (1970). Fitting a response model for n dichotomously scored items.
Psychometrika, 35, 179-197.
Bock, R. D., Brennan, R. L., & Muraki, E. (1999, April). The introduction of essay questions into the GRE: 
Toward a synthesis of item response theory and generalizability theory. Paper presented at the 
Annual Meeting of the American Educational Research Association, Montreal, Canada.
Bolt, D. M., & Lall, V. F. (2003). Estimation of compensatory and noncompensatory multidimensional
item response models using Markov chain Monte Carlo. Applied Psychological Measurement, 27,
395-414.
Bolt, D. M., Cohen, A. S., & Wollack, J. A. (2001). A mixture item response model for multiple-choice 
data. Journal of Educational and Behavioral Sciences, 26(4), 381-409.
Bradlow, E. T., Wainer, H., & Wang, X. (1999). A Bayesian random effects model for testlets. 
Psychometrika, 64(2), 153-168.
Cai, L., Thissen, D., & du Toit, S. H. C. (2011). IRTPRO for Windows [Computer software]. Lincolnwood, 
IL: Scientific Software International.
Chen, F. (2009). Bayesian modeling using the MCMC procedure. Proceedings of the SAS Global Forum 
2009.
Chen, F. (2011). The random statement and more: Moving on with PROC MCMC. Proceedings of the SAS 
Global Forum 2011.
Chen, F. (2013). Missing no more: Using the MCMC procedure to model missing data. Proceedings of the
SAS Global Forum 2013.
Chen, W. H., & Thissen, D. (1997). Local dependence indexes for item pairs using item response theory. 
Journal of Educational and Behavioral Statistics, 22, 265-289.
Choi, S. W., Cook, K. F., & Dodd, B. G. (1997). Parameter recovery for the partial credit model using 
MULTILOG. Journal of Outcome Measurement, 1(114-142).
Cho, S., & Cohen, A. S. (2010). A multilevel mixture IRT model with an application to DIF. Journal of 
Educational and Behavioral Statistics, 35, 336-370.

250 Bayesian Analysis of Item Response Theory Models Using SAS
Cho, S. J., Cohen, A. S., & Kim, S. H. (2013). Markov chain Monte Carlo estimation of a mixture item 
response theory model. Journal of Statistical Computation and Simulation, 83, 278-306.
Congdon, P. (2006). Bayesian Statistical Modelling. Chichester: Wiley.
Crocker, L., & Algina, J. Introduction to classical and modern test theory. New York: Holt, Rinehart, and 
Winston.
Curtis, S. M. (2010). BUGS code for item response theory. Journal of Statistical Software, 36, Code 
Snippet 1, 1-34.
De Ayala, R. J. (2009). The theory and practice of item response theory. New York: Guilford Press.
DeCarlo, L.T., Kim, Y., & Johnson, M. S. (2011). A hierarchical rater model for constructed response 
responses, with a signal detection rater model. Journal of Educational Measurement, 48, 333-356.
DeMars, C. E. (2003). Sample size and the recovery of nominal response model item parameters. Applied 
Psychological Measurement, 27, 275-288.
DeMars, C. E. (2006). Application of the bi-factor multidimensional item response theory model to testlet-
based tests. Journal of Educational Measurement, 43, 145-168.
DeMars, C. E. (2013). A tutorial on interpreting bifactor model scores. International Journal of Testing, 13, 
354-378. Retrieved from 
http://www.tandfonline.com/doi/abs/10.1080/15305058.2013.799067#.VEm1Elft8i8
Donoghue, J. R., & Hombo, C. M. (2000, April). A comparison of different model assumptions about rater 
effects. Paper presented at the Annual Meeting of the National Council on Measurement in 
Education, New Orleans, LA.
Drasgow, F., Levine, M. V., & Williams, E. A. (1985). Appropriateness measurement with polychotomous 
item response models and standardized indices. British Journal of Mathematical and Statistical 
Psychology, 38, 67-86.
Embretson, S. E. (1997). Multicomponent response models. In W. J. V. d. Linden & R. K. Hambleton 
(Eds.), Handbook of modern item response theory (pp. 305-321). New York: Springer Verlag.
Embretson, S. E. (2015). The multicomponent latent trait model for diagnosis: Applications to 
heterogeneous test domains. Applied Psychological Measurement, 39, 16-30.
Embreston, S. E., & Hershberger, S. L. (1999). The new rules of measurement. Mahwah, NJ: Lawrence 
Erlbaum Associates.
Embretson, S. E., & Reise, S. P. (2000). Item response theory for psychologists. Mahwah, NJ: Lawrence 
Erlbaum Associates.
Engelhard, G. (1997). Constructing rater and task banks for performance assessments. Journal of Outcome 
Measurement, 1, 19-33.
Engelhard, G. (2002). Monitoring raters in performance assessments. In G. Tindal & T. M. Haladyna 
(Eds.), Large-scale assessment programs for all students: Validity, technical adequacy, and 
implementation (pp. 261-287). Mahwah, NJ: Erlbaum.
Feinberg, R. A., & Wainer, H. (2014). A simple equation to predict a subscore’s value. Educational 
Measurement: Issues and Practice, 33, 5556.
Fox, J.-P. (2010). Bayesian item response modeling: Theory and applications. New York: Springer.
Fox, J.-P., & Glas, C. A. W. (2001). Bayesian estimation of a multilevel IRT model using Gibbs sampling. 
Psychometrika, 66, 269-286.
Gelman, A. (2006). Prior distributions for variance parameters in hierarchical models. Bayesian Analysis, 1,
515-533.
Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2014). Bayesian data 
analysis. New York: Chapman & Hall.
Geisser, S., & Eddy, W. F. (1979). A predictive approach to model selection. Journal of the American 
Statistical Association, 74, 153-160.

References 251
Gelfand, A. E., Dey, D. K., & Chang, H. (1992). Model determination using predictive distributions with 
implementation via sampling-based methods. In J. M. Bernardo, J. O. Berger, A. P. Dawid, & A. 
F. M. Smith (Eds.), Bayesian statistics (p. 147-167). Oxford: Oxford University Press.
Gibbons, R., & Hedeker, D. R. (1992). Full-information item bi-factor analysis. Psychometrika, 57, 423-
436.
Gibbons, R. D., Immekus, J. C., & Bock, R. D. (2007). The added value of multidimensional IRT models.
Retrieved from http://www.healthstats.org/articles/NCI_Didactic_Workbook.pdf
Glas, C. A. W., & Meijer, R. R. (2003). A Bayesian approach to person fit analysis in item response theory 
models. Applied Psychological Measurement, 27(3), 217-233.
Goldstein, M. (2006), “Subjective Bayesian Analysis: Principles and Practice,” Bayesian Analysis, 3, 403–
420. Retrieved from http://ba.stat.cmu.edu/journal/2006/vol01/issue03/goldstein.pdf
Haberman, S. J. (2008). When can subscores have value? Journal of Educational and Behavioral Statistics,
33, 204–229
Hambleton, R. K., & Swaminathan, H. (1985). Item response theory. Boston, MA: Kuwer-Nijhoff.
Hambleton, R. K., Swaminathan, H., & Rogers, H. J. (1991). Fundamentals of item response theory.
Newbury Park, CA: Sage Publications, Inc. 
Han, Kyung T. (2012). Fixing the c parameter in the three-parameter logistic model. Practical Assessment, 
Research & Evaluation, 17. Retrieved from http://pareonline.net/getvn.asp?v=17&n=1
Immekus, J. C., Gibbons, R. D., & Rush, J. A. (2007). Patient-reported outcomes measurement and 
computerized adaptive testing: An application of post-hoc simulation to a diagnostic screening 
instrument. In D. J. Weiss (Ed.). Proceedings of the 2007 GMAC Conference on Computerized 
Adaptive Testing.
Ip, E. H. S. (2001). Testing for local dependency in dichotomous and polytomous item response models. 
Psychometrika, 66, 109-132.
Jasra , A., Holmes, C. C., & Stephens, D. A. (2005). MCMC and the label switching problem in Bayesian 
mixture models. Statistical Science, 20, 50-67.
Johnson, V. A. & Albert, J. H. (1999). Ordinal data modeling. New York: Springer.
Johnson, M. S., Sinharay, S., & Bradlow, E. T. (2007). Hierarchical item response theory models. In C. R. 
Rao & S. Sinharay (Eds.), Handbook of statistics Volume 26 (pp. 643-661). Amsterdam, The 
Netherlands: Elsevier B.V.
Junker, B. W., Patz, R. J., and VanHoudnos, N. (In press). Markov Chain Monte Carlo for Item Response 
models. Invited chapter for W. J. van der Linden & R. K. Hambleton (Eds.), Handbook of Modern 
Item Response Theory. Boca Raton, FL: Chapman & Hall/CRC. Retrieved from 
http://ebookbrowsee.net/junker-patz-vanhoudnos-mcmc-volume-2-chapter-15-pdf-d429291231
Karabatsos, G. (2003). Comparing the aberrant response detection performance of thirty-six person-fit 
statistics. Applied Measurement in Education, 16, 277–298.
Kass, R. E., and Wasserman, L. (1996), “Formal Rules of Selecting Prior Distributions: A Review and 
Annotated Bibliography,” Journal of the American Statistical Association, 91, 343–370.
Kim, J., & Bolt, D. (2007). Estimating item response theory models using Markov Chain Monte Carlo 
methods. Educational Measurement: Issues and Practice, 26, 38-51.
Lake, C. J., Withrow, S., Zickar, M. J., Wood, N. L., Dalal, D. K., & Bochinski, J. (2013). Understanding 
the relation between attitude involvement and response latitude using item response theory.
Educational and Psychological Measurement, 73, 690-712.
Lane, S., & Stone, C.A. (2006). Performance Assessment. In R. L. Brennan (Ed.), Educational measurement
(4th ed.). Westport, CT: American Council on Education/Praeger.
Levy, R. (2006). Posterior predictive model checking for multidimensionality in item response theory and 
Bayesian networks. Unpublished dissertation. University of Maryland.
Levy, R., Mislevy, R. J., & Sinharay, S. (2009). Posterior predictive model checking for 
multidimensionality in item response theory. Applied Psychological Measurement, 33, 519-537.

252 Bayesian Analysis of Item Response Theory Models Using SAS
Li, Y., Bolt, D. M., & Fu, J. (2006). A comparison of alternative models for testlets. Applied Psychological 
Measurement, 30, 3-21. 
Linacre, J. D. (1989). Many-facet Rasch measurement. Chicago: MESA Press.
Lord, F. N. (1980). Applications of item response theory to practical testing problems. Hillsdale, NJ: 
Erlabaum.
Lord, F. N., & Novick, M. R. (1969). Statistical theories of mental test scores. Reading, MA: Addison 
Wesley.
Lubke, G. H. & Muthen, B. (2005). Investigating population heterogeniety with vactor mixture models.
Psychological Methods, 10, 20-30.
Lunn, D., Jackson, C., Best, N., Thomas, A., Spiegelhalter, D. (2013). The BUGS book: A practical 
introduction to Bayesian analysis. New York: CRC Press.
Maij-de Meij, A. M., Kelderman, H., & van der Flier, H. (2008). Fitting a mixture item response theory 
model to personality questionnaire data: Characterizing latent classes and investigating 
possibilities for improving prediction. Applied Psychological Measurement, 32, 611-631.
Masters, G. N. (1982). A Rasch model for partial credit scoring. Psychometrika, 47, 149-174.
Masters, G. N., & Wright, B. D. (1984). The essential process in a family of measurement models. 
Psychometrika, 49, 529-544.
Matteucci, M., & Veldkamp, B. P. (2013). Bayesian estimation of item response theory models with power
priors. In Brentari E., Carpita M., Advances in Latent Variables. Retrieved from 
http://meetings.sis-statistica.org/index.php/sis2013/ALV/paper/view/2662
McDonald, R. P. (1999). Test theory: A unified treatment. Mahwah, NJ: Lawrence Erlbaum.
McHorney, C. A. (1997). Generic health measurement: Past accomplishments and a measurement 
paradigm for the 21st century. Annuls of Internal Medicine, 127, 743-750.
Meijer, R. R., & Sijtsma, K. (2001). Methodology review: Evaluating person fit. Applied Psychological 
Measurement, 25, 107–135.
Muraki, E. (1990). Fitting a polytomous item response model to Likert-type data. Applied Psychological 
Measurement, 14, 59-71.
Muraki, E. (1992). A generalized partial credit model: Application of an EM algorithm. Applied 
Psychological Measurement, 16, 159-176.
Muraki, E., & Bock, D. (1994) PARSCLE 3.0 [Computer program]. Chicago: Scientific Software 
International, Inc.
Muraki, E., & Carlson, J. E. (1995). Full-information factor analysis for polytomous item responses. 
Applied Psychological Measurement, 19, 73-90.
Muthén, L. K., & Muthén, B. O. (2012). Mplus user’s guide. Seventh Edition. Los Angeles, CA: Muthén & 
Muthén.
Natesan, P., Limbers, C. A., & Varni, J. W. (2010). Bayesian estimation of graded response multilevel 
models using Gibbs sampling: Formulation and illustration. Educational and Psychological 
Measurement, 70, 420–439.
Orlando, M., & Thissen, D. (2000).  Likelihood-based item-fit indices for dichotomous item response 
theory models. Applied Psychological Measurement, 24, 50-64.
Patz, R. J. (1996). Markov Chain Monte Carlo methods for item response theory models with applications 
for the National Assessment of Educational Progress. Unpublished manuscript, Carnegie Mello 
University, Pittsburgh PA.
Patz, R. J., Junker, B. W., Johnson, M. S., & Mariano, L. T. (2002). The hierarchical rater model for rated 
test items and its application to large-scale educational assessment data. Journal of Educational 
and Behavioral Statistics, 27, 341-384.
Raftery, A. E. (1996). Approximate Bayes factors and accounting for model uncertainty in generalised 
linear models. Biometrika, 83, 251-266. 

References 253
Rasch, G. Probabilistic models for some intelligence and attainment tests. Copenhagen: Danish Institute 
for Educational Research. 1960.
Reckase, M. D. (1994). What is the "correct" dimensionality for a set of item response data? In D. Laveault, 
B. D. Zumbo, M. E. Gessaroli & M. W. Boss (Eds.), Modern theories of measurement: Problems 
and issues (pp. 87-92). Ottawa: University of Ottawa.
Reckase, M. D. (1997a). A linear logistic multidimensional model for dichotomous item response data. In 
W. J. van der Linden & R. K. Hambleton (Eds.), Handbook of modern item response theory (pp. 
271-286). New York: Springer-verlag.
Reckase, M. D. (1997b). The past and future of multidimensional item response theory. Applied 
Psychological Measurement, 21, 25-36.
Reeve, B. B. 2002. An introduction to modern measurement theory. National Cancer Institute. Retrieved
from http://appliedresearch.cancer.gov/archive/cognitive/immt.pdf.
Reeve, B. B., Hays, R. D., Chang, C. H., & Perfetto, E. M. (2007). Applying item response theory to 
enhance health outcomes assessment. Quality of Life Research, 16, 1-3.
Reise, S. P., & Yu, J. (1990). Parameter recovery in the graded response model using MULTILOG. Journal 
of Educational Measurement, 27, 133-144.
Rost, J. (1990). Rasch models in latent classes: An integration of two approaches to item analysis. Applied 
Psychological Measurement, 14, 271-282.
Rupp, A. A., Templin, J., & Henson, R. A. (2010). Diagnostic measurement: Theory, methods, and 
applications. New York, NY: Guilford Press.
Saal, F., Downey, R. & Lahey, M. (1980). Rating the Ratings: Assessing the psychometric quality of rating 
data. Psychological Bulletin, 88, 413-428.
Samejima, F. (1969). Estimation of latent ability using a response pattern of graded scores. Psychometrika 
Monograph (No. 17).
Samejima, F. (Ed.). (1996). The graded response model. In W. J. van der Linden and R. Hambleton (Eds.), 
Handbook of modern item response theory. New York: Springer.
Santor, D. A., & Ramsay, J. O. 1998. Progress in the Technology of Measurement: Applications of Item 
Response Models. Psychological Assessment, 10, 345-359.
SAS Institute Inc. (2014). SAS/STAT User’s Guide, Version 13.2. Chapter 7, Introduction to Bayesian 
Analysis. Cary, NC: SAS Institute Inc. Retrieved from
http://support.sas.com/documentation/onlinedoc/stat/132/introbayes.pdf
SAS Institute Inc. (2014). SAS/STAT User’s Guide, Version 13.2. Chapter 61, The MCMC Procedure.
Cary, NC: SAS Institute Inc. Retrieved from
http://support.sas.com/documentation/onlinedoc/stat/132/mcmc.pdf
SAS Institute Inc. (2014). SAS/STAT User’s Guide, Version 13.2. Chapter 53, The IRT Procedure. Cary, 
NC: SAS Institute Inc. Retrieved from 
http://support.sas.com/documentation/onlinedoc/stat/132/irt.pdf
Sawatzky, R., Ratner, P. A., Kopec, J. A., & Zumbo, B. D. (2012). Latent variable mixture models: A 
promising approach for the validation of patient reported outcomes. Quality of Life Research, 
21(4), 637-650. Retrieved from doi:10.1007/s11136-011-9976-6.
Schwarz, G. (1978). Estimating the dimension of a model. The Annals of Statistics, 6, 461-464.
Sheng, Y. (2013). An empirical investigation of Bayesian hierarchical modeling with unidimensional IRT 
models. Behaviormetrika, 40, 19-40.
Sinharay, S. (2005). Assessing fit of unidimensional item response theory models using a Bayesian 
approach. Journal of Educational Measurement, 42, 375-394.
Sinharay, S. (2006). Bayesian item fit analysis for unidimensional item response theory models. British 
Journal of Mathematical & Statistical Psychology, 59, 429-449.
Sinharay, S., & Haberman, S. J. (2014). How often is the misfit of item response theory models practically 
significant? Educational Measurement: Issues and Practice, 33, 2335.

254 Bayesian Analysis of Item Response Theory Models Using SAS
Sinharay, S., Johnson, M. S., & Stern, H. S. (2006). Posterior predictive assessment of item response theory 
models. Applied Psychological Measurement, 30, 298-321.
Sinharay, S., Puhan, G., & Haberman, S. J. (2011). An NCME instructional module on subscores.
Educational Measurement: Issues and Practice, 30, 29–40.
Spiegelhalter, D. J., Best, N., Carlin, B. P., & van der Linde, A. (2002). Bayesian measures of model 
complexity and fit. Journal of the Royal Statistical Society, 64, 583-640.
Spiegelhalter, D. J., Thomas, A., Best, N., & Lunn, D. (2003). WINBUGS Version 1.4 User’s manual.
Cambridge, UK: MRC Biostatistics Unit.
Stone, C. A. (2000). Monte-Carlo based null distribution for an alternative fit statistic. Journal of 
Educational Measurement, 37, 58-75.
Stone, C. A., & Irrgang, J. J. (June, 2004). Simulation of a computer adaptive test utilizing the disabilities 
of the arm, shoulder and hand (DASH) outcome measure. Presented at the DIA Conference 
“Advances in Health Outcomes Measurement”, Bethesda, MD.
Stout, W. F. (1987). A nonparametric approach for assessing latent trait dimensionality. Psychometrika, 52,
589-617.
Stout, W. F. (1990). A new item response theory modeling approach with applications to unidimenstional 
assessment and ability estimates. Psychometrika, 55, 293-326.
Sympson, J. B. (1978). A model for testing with multidimensional items. Paper presented at the 1977 
computerized Adaptive Testing Conference, Minneapolis: University of Minnesota, Department of 
Psychology.
Thissen, D. (1993). Repealing rules that no longer apply to psychological measurement. In N. Frederiksen, 
R. J. Mislevy & I. I. Bejar (Eds.), Test theory for a new generation of tests (pp. 79-97). Hillsdale, 
NJ: Lawrence Erlbaum Associates.
Thissen, D., Cai, L., & Bock, R. D. (2010). The nominal categories item response model. In M. L. Nering 
& R. Ostini (Eds.), Handbook of polytomous item response theory models: Development and 
applications (pp. 43–75). New York, NY: Taylor& Francis.
Thissen, D., & Steinberg, L. (1986). A taxonomy of item response models. Psychometrika, 51, 567-577.
van der Linden, W. J., & Hambleton, R. K. (Eds.) (1997). Handbook of modern item response theory. New 
York: Springer.
Verhelst, N. D., & Verstralen, H. H. F. M. (2001). An IRT model for multiple raters. In A. Boomsma, M. 
A. J. V. Duijn & T. A. B. Snijders (Eds.), Essays on item response theory (pp. 89-108). New 
York: Springer-Verlag.
Wainer, H., & Kiely, G. L. (1987). Item clusters and computerized adaptive testing: A case for testlets. 
Journal of Educational Measurement, 27, 1-14.
Wainer, H., & Thissen, D. (1996). How is reliability related to the quality of test scores? What is the effect 
of local dependence on reliability? Educational Measurement: Issues and Practice, 15, 22-29.
Wang, X., Bradlow, E. T., & Wainer, H. (2002). A general Bayesian model for testlets: Theory and 
applications. Applied Psychological Measurement, 26, 109-128.
Wilson, M., & Hoskens, M. (2001). The rater bundle model. Journal of Educational and Behavioral 
Statistics, 26, 283-306.
Wolfe, E. W. (2004). Identifying rater effects using latent trait models. Psychology Science, 46, 35-51.
von Davier, M., & Rost, J. (2007). Mixture distribution item response models. In C.R. Rao & S. Sinharay 
(Eds.), Handbook of Statistics Volume 26 (pp. 643-661). Amsterdam, The Netherlands: Elsevier 
B.V.
Yao, L., & Schwarz, R. D. (2006). A multidimensional partial credit model with associated item and test 
statistics: An application to mixed-format tests. Applied Psychological Measurement, 30(6), 469-
492.
Yen, W. M. (1981). Using simulation results to choose a latent trait model. Applied Psychological 
Measurement, 5, 245-262.

References 255
Yen, W. M. (1984). Effects of local item dependence on the fit and equating performance of the three-
parameter logistic model. Applied Psychological Measurement, 8, 125-145.
Yen, W.M. (1993). Scaling performance assessments: Strategies for managing local item dependence.
Journal of Educational Measurement, 30, 187-213.
Yen, W. M. & Fitzpatrick, A. R. (2006). Item response theory. In R. L. Brennan (Ed.), Educational 
measurement (4th ed.). Westport, CT: American Council on Education/Praeger. 
Zhu, X., & Stone, C.A. (2011). Assessing fit of unidimensional graded response models using Bayesian 
methods. Journal of Educational Measurement, 48, 81-97.
Zhu, X., & Stone, C.A. (2012). Bayesian comparison of alternative graded response models for 
performance assessment applications. Educational and Psychological Measurement, 72, 774-799.
Zink, R. (2014). MCMC diagnostics add-in. JMP User Community. Retrieved from 
https://community.jmp.com/docs/DOC-6103
Zumbo, B. D. (2007). Three generations of differential item functioning (DIF) analyses: Considering where 
it has been, where it is now, and where it is going. Language Assessment Quarterly, 4, 223-233.

256 Bayesian Analysis of Item Response Theory Models Using SAS

Index 
A 
aberrant  206 
Akaike Information Criterion (AIC)  39–40, 190 
ALPHA option  79–80 
analysis, of multiple chains  67–69 
See also Bayesian Analysis 
Andrich, D.  11, 13 
See also Rating Scale (RS) model 
arbitrary distributions  48–49 
ARRAY statement, MCMC procedure  45, 50, 53, 211 
autocall macros, for post-processing MCMC procedure 
data sets  78–79 
B 
Bayes factor (BF)  40–41, 190–191 
Bayesian Analysis 
See also specific topics 
about  25 
Bayesian approaches to statistical inference  26–28 
burn-in  35–38 
convergence  35–38 
elements of statistical models and inference  25–26 
estimating proportions with  28–31 
frequentist approaches to statistical inference  26–
28 
informative prior distributions  38–39 
MCMC method and Bayesian estimation  31–35 
model comparisons  39–41 
model fit and posterior predictive model checks  
41–42 
uninformative prior distributions  38–39 
Bayesian comparison, of IRT models  189–204 
Bayesian comparison model indices 
about  190 
comparing GR and RS-GR models for 
polytomously scored items  196–200 
comparing models for dichotomously scored items  
193–196 
comparing unidimensional IRT models with 
bifactor IRT models  200–204 
computing model comparison statistics in MCMC 
procedure  191–192 
Conditional Predictive Ordinate (CPO)  190–191 
Deviance Information Criterion (DIC)  190 
Bayesian estimates 
comparing frequentist estimates with  30–31 
of IRT models using MCMC procedure  43–80 
to statistical inference  26–28 
of unidimensional IRT models for dichotomously 
scored items  81–112 
of unidimensional IRT models for polytomously 
scored items  113–153 
Bayesian Information Criterion (BIC)  39–40, 190 
Bayesian model-checking, for IRT models  205–247 
BEGINNODATA statement  102 
Béguin, A.A.  213 
BETA distribution  28–29, 47 
between-item multidimensionality  14–15 
bifactor IRT model 
about  155–156 
compared with unidimensional IRT models  200–
204 
estimating in MCMC procedure  156–158 
"label switching"  165 
other multidimensional IRT models  163–165 
output from MCMC  158–163 
BINARY distribution  47 
BINOMIAL distribution  47 
Bock, R.D.  11, 52, 136 
Bradlow. E.T.  13, 166 
built-in distributions  46–47 
burn-in  35–38 
BY option, RANK procedure  226 
C 
Cai, L.  11 
CALL SYMPUTX statement  218 
chain thinning  59–60 
chains, analysis of multiple  67–69 
Chen, W.H.  20, 214 
CHISQUARE distribution  47 
chi-squared statistic  207 
Cho, S.  17 
classical test theory (CTT)  1–2 
Cohen, A.S.  17 
comparing 
GR and RS-GR models for polytomously scored 
items  196–200 
models for dichotomously scored items  193–196 
models using PPMC  243–247 
results based on MML estimation  105–107, 149–
151 
unidimensional IRT models and bifactor IRT 
models  200–204 
with WinBUGS  76–77 
computing 
drawing inferences  30 
model comparison statistics in MCMC procedure  
191–192 
posterior inferences  30 
Conditional Predictive Ordinate (CPO) 
about  40–41, 190–191 
results for comparing models for dichotomously 
scored items  194–196 
results for comparing models for polytomously 
scored items  197–200 
results for comparing unidimensional and bifactor 
IRT models  201–204 
 
 

258   Index 
 
convergence 
about  35–38 
Gelman-Rubin test for  67–69 
strategies to improve of Monte Carlo Chain  59–66 
CORR procedure  79–80, 220–223 
Cronbach's Coefficient Alpha  79 
D 
DASH survey  114 
DATA option, MCMC procedure  44 
Dbar  193 
De Ayala, R.J.  11, 22–23 
DeMars, C.E.  156, 163, 172 
density plots  47–48 
Deviance Information Criterion (DIC) 
about  39–40, 190 
results for comparing GR and RS-GR modes for 
polytomously scored items  196–197 
results for comparing models for dichotomously 
scored items  193–194 
results for comparing unidimensional and bifactor 
IRT models  200–201 
DGENERAL function  49 
DIAGNOSTICS option, MCMC procedure  45, 59 
DIC option, MCMC procedure  45 
dichotomously scored items 
Bayesian estimation of unidimensional IRT models 
for  81–112 
comparing models for  193–196 
unidimensional IRT models for  5 
“difference models”  7 
Differential Item Functioning (DIF) 
mixture IRT models and  16–17, 182–187 
multiple group models for detecting  180–182 
difficulty parameter  4 
diffuse priors 
See uninformative prior distributions 
dimensionality  19 
discrepancy measures 
about  212–213 
evaluating model fit  215 
item-level measures  213–214 
pairwise measures  214 
person-fit measures  214–215 
test-level measures  213 
discrimination parameter  4 
displaying item response functions  107–112 
distributions 
arbitrary  48–49 
BETA  28–29, 47 
BINARY  47 
BINOMIAL  47 
built-in  46–47 
CHISQUARE  47 
GAMMA  47 
GENERAL  47 
informative prior  38–39, 70 
MVN  47 
MVNAR  47 
nontruncated  90 
NORMAL  47 
T  47 
TABLE  47 
total score  216 
truncated  90 
UNIFORM  47 
uninformative prior  38–39, 70–71 
Dmean  193–194 
DO loop  50, 53, 64, 116, 129 
drawing inferences, computing  30 
E 
Effective Sample Size (ESS)  38, 59 
Embretson, S.E.  6, 15 
ENDNODATA statement  102 
Englehard, G.  12 
estimating 
bifactor IRT model in MCMC procedure  156–158 
Generalized Partial Credit (GPC) model  144–148 
hierarchical models  173–175 
Nominal Response (NR) model  136–143 
1-Parameter (1P) graded response model  129–130 
proportions with Bayesian Analysis  28–31 
Rating Scale (RS) model  131–135 
testlet IRT model  166–167 
evaluating 
IRT model applications  19–22 
model fit  215 
F 
factor model  163–164 
flat priors 
See uninformative prior distributions 
Fox, J.-P.  18–19, 73, 173, 178 
FREQ procedure  216, 227, 231 
frequentist estimates 
comparing Bayesian estimates with  30–31 
to statistical inference  26–28 
functions 
See specific functions 
G 
GAMMA distribution  47 
Gelman, A.  46, 71, 166 
Gelman-Rubin test 
about  38, 59 
for convergence  67–69 
GENERAL distribution  47 
GENERAL function  48, 120 
Generalized Partial Credit (GPC) model 
about  11, 113, 143–144 
estimating  144–148 
Gewecke statistics  38 
 

Index   259 
Gibbons, R.D.  163 
Gibbs sampler, MCMC method and  35 
Glas, C.A.W.  173, 213 
global odds ratios  214 
graded response (GR) model 
about  6–8, 113, 114 
comparing posterior densities for prior 
specifications  127–128 
comparing separate and joint prior specifications  
120–121 
comparing with RS-GR models for polytomously 
scored items  196–200 
computing transformation by post-processing 
posterior results  128 
estimation of using Method 1  118–120 
output from estimating  121–127 
program template for  114–116 
specifying prior distribution for threshold and 
intercept parameters  116–118 
Graphics Template Language (GTL)  206, 216 
graphs 
if item category response functions  151–153 
of test information functions  153 
guessing parameter  4 
H 
Hambleton, R.K.  20, 95, 226 
Han, Kyung T.  96 
Heidelberger-Welch test 
for 1P model  58 
about  38 
hierarchical models 
about  17–19 
multilevel IRT model  172–179 
hierarchical priors 
2-Parameter (2P) model with  91–94 
about  29, 70, 71–72 
hierarchical rating (HR) model  14 
HISTOGRAM statement  218–219 
hyperparameters  29 
hyperpriors  29 
hypothesis testing, compared with classical model-fit 
analyses  215 
I 
“ideal rating”  14 
IGAMMA distribution  47 
implementing Metropolis sampler  32–34 
informative prior distributions  38–39, 70 
intercept parameter, specifying prior distributions for  
116–118 
inverse gamma distribution  91 
IRT model extensions 
about  155 
bifactor model  155–165 
differential item functioning  180–187 
hierarchical models  172–179 
testlet  166–172 
IRT models 
See also specific types 
about  2–4 
Bayesian comparison of  189–204 
Bayesian estimation of using MCMC procedure  
43–80 
Bayesian model-checking for  205–247 
evaluation of applications  19–22 
form of  20 
MCMC procedure for estimation  49–59 
for multidimensional response data  14–16 
multilevel  17–19 
parameter estimation  22–23 
random effects  17–19 
testing effects in unidimensional  12–14 
unidimensional, for dichotomously scored 
responses  5 
unidimensional, for polytomously scored responses  
5–12 
visualization of  3–4 
IRT procedure  149 
IRTPRO computer program  149 
item analyses, preliminary  79–80 
item category response functions, graphs of  151–153 
item characteristic curves (ICCs)  15 
item fit plots 
of proportion correct statistics  223–226 
using Yen's Q1 measure  226–231 
item parameters 
treatment of as random effects  73–75 
uncertainty in  109–111 
item response functions, displaying  107–112 
item response theory (IRT)  1–4 
See also IRT models 
item-free measurement  2 
item-level fit  206–207 
item-level measures  213–214 
J 
Johnson, M.S.  14 
JOINTMODEL option, MCMC statement  51 
Junker, B.W.  14 
K 
Karabatsos, G.  207 
KEEP statement  211 
Kelderman, H.  17 
Kopec, J.A.  184 
L 
“label switching”  165, 184–185 
Lane, S.  8 
Lieberman, M.  52 
likelihood model, specifying using TABLE function  
129 
 

260   Index 
 
Linacre, J.D.  13 
local independence  19–20 
locally dependent item sets (testlets)  13 
LOGISTIC function  54, 85 
LOGNORMAL distribution  47 
LOOPITEMS macro  227 
Lord, F.N.  95, 102, 107 
lower asymptote parameter  3–4 
LPDFNORM function  119–120 
M 
Maij-de Meij, A.M.  17 
MAKEGTLLIST macro  109 
many-facet Rasch measurement model (MFRM)  13 
marginal maximum likelihood (MML) estimates  22, 
105–107, 149–151 
Mariano, L.T.  14 
Markov Chain Monte Carlo (MCMC) method  31–32, 
35, 43–44 
Masters, G.N.  11 
MCMC method 
Bayesian estimation and  31–35 
Gibbs sampler and  35 
MCMC procedure 
about  31–32, 35–38, 121–127 
autocall macros for data sets  78–79 
Bayesian estimation of IRT models using  43–80 
Bayesian estimation of unidimensional IRT models 
for dichotomously scored items  81–112 
computing model comparison statistics in  191–192 
estimating bifactor IRT model in  156–163 
Graded Response (GR) model and  114–116 
processing of SAS data set by  51 
sampling algorithms  45–46 
statements for IRT model estimation  44–49 
template for IRT model estimation  49–59 
MEANS procedure  54–55, 77–78, 107, 128, 141, 151–
153, 218, 223–224, 227, 232 
Meijer, R.R.  207 
Metropolis sampler, implementing  32–34 
Metropolis-Hastings algorithm  31–32, 35 
MISSING option, MCMC procedure  44 
mixed effects models 
See hierarchical models 
mixture IRT models  16–17, 182–187 
model fit 
about  20–22, 76 
evaluating  215 
posterior predictive model checks and  41–42 
model fit statistics 
about  206 
item-level fit  206–207 
person fit  207 
test-level fit  206 
model identification  52 
model re-parameterizing  64–66 
MODEL statement, MCMC procedure  45, 47–48, 50, 
51–52, 54, 114–116, 120 
model-predicted results, compared with observed 
results  206 
models 
comparing  39–41, 76 
item response theory (IRT)  2–4 
MONITOR command  64 
MONITOR option, MCMC procedure  44, 54–55, 74, 
82 
Monte Carlo Chain  31–32, 59–66 
Monte Carlo standard error (MCSE)  57 
multidimensional response data, IRT models for  14–16 
multilevel IRT model 
about  17–19, 172 
description of  172–173 
estimating  173–175 
output from MCMC procedure  175–179 
multilevel models 
See hierarchical models 
multiple group models, for detecting differential item 
functioning  180–182 
Muraki, E.  8, 143–144 
See also Rating Scale (RS) model 
MVN distribution  47 
MVNAR distribution  47 
N 
NBI option, MCMC procedure  44 
NMC option, MCMC procedure  44 
Nominal Response (NR) model 
about  11–12, 113, 136 
estimating  136–143 
nontruncated distribution  90 
normal deviate  85 
NORMAL distribution  47 
normal ogive function  85 
NTHREADS option, MCMC procedure  44, 82 
O 
observed and predicted item-test score correlations, as 
an example of PPMC application  220–223 
observed and predicted odds ratio measure, as an 
example of PPMC application  231–235 
observed and predicted person-fit statistic, as an 
example of PPMC application  239–243 
observed and predicted Yen's Q3 measure, as an 
example of PPMC application  236–239 
observed results  111–112, 206 
observed test score distributions, as an example PPMC 
application  216–220 
odds ratio (OR)  214 
1-Parameter (1P) model 
about  5 
Bayesian estimation of unidimensional IRT models  
82–84 
example output from estimating  54–59 
 

Index   261 
graded response, estimating  129–130 
Heidelberger-Welch test for  58 
template for  52–54 
option discrimination parameter  11 
option extremity parameter  11 
options 
See specific options 
OUTPOST option, MCMC procedure  44, 121, 128 
OUTPOST SAS data set  77–78 
output 
from estimating GPC model  146–148 
from estimating hierarchical models  175–179 
from estimating NR model  137–143 
from estimating RS-GR model  133–135 
from estimating testlet IRT model  167–172 
example from estimating 1-Parameter (1P) model  
54–59 
from MCMC procedure when estimating bifactor 
IRT model  158–163 
“overfitting”  26 
P 
pairwise measures  214 
parameter blocking  61–63 
parameter estimation, in IRT models  22–23 
PARMS statement, MCMC procedure  45, 47–48, 50, 
51, 53, 61–63, 74, 117, 131–132, 173 
Partial Credit (PC) model  8–11, 113 
Patz, R.J.  14 
Pearson chi-squared statistic  213 
person fit  207 
person-fit measures  214–215 
person-free measurement  2 
PLOTS option, MCMC procedure  45 
polytomously scored items 
about  4 
Bayesian estimation of unidimensional IRT models 
for  113–153 
comparing models for  196–200 
unidimensional IRT models for  5–12 
posterior densities, comparing for prior specifications  
127–128 
posterior inferences, computing  30 
posterior predictive distribution  208–212 
posterior predictive model-checking (PPMC) 
about  21–22, 39, 76, 205–206, 207 
examples of applications  216–247 
method  207–208 
model fit and  41–42 
posterior predictive distribution  208–212 
posterior predictive probability values (PPP values)  42 
%POSTSUM macro  121, 137, 146, 183 
PREDDIST statement, MCMC procedure  45, 208–209, 
211, 216, 220–223, 231 
predicted test score distributions, as an example PPMC 
application  216–220 
preliminary item analyses  79–80 
prior distributions 
choosing  29 
specifying for threshold and intercept parameters  
116–118 
prior specifications, comparing separate and joint  120–
121 
PRIOR statement, MCMC procedure  45, 46–47, 47–
48, 48–49, 51–52, 53, 116, 120 
priors, effect of different  72–73 
PROBNORM function  54 
procedures 
See specific procedures 
program template, for graded response (GR) model  
114–116 
proportion correct statistics, item fit plots and  223–226 
proportions 
comparing Bayesian and frequentist estimates for  
30–31 
estimating with Bayesian Analysis  28–31 
Pseudo-Bayes factor (PsBF)  40–41, 190–191 
R 
Raftery-Lewis statistic  38 
Ramsay, J.O.  2 
random effects, treatment of item parameters as  73–75 
random effects models  17–19 
See also hierarchical models 
RANDOM statement, MCMC procedure  45–46, 46–
47, 47–48, 50, 51–52, 54, 74, 77–78, 158, 
167, 180–181, 182–184 
RANK procedure  226 
Rasch, G.  5, 15, 82–84 
rater effects  13–14 
Rating Scale (RS) model  8, 11, 13, 113, 131–132 
Ratner, P.A.  184 
Reckase, M.D.  19, 200 
Reise, S.P.  6 
re-parameterizing models  64–66 
RESTRUCTURE macro  227 
Rogers, H.J.  226 
RS-GR model 
comparing with GR models for polytomously 
scored items  196–200 
output from estimating  133–135 
S 
Samejima, F.  6, 114 
See also Graded Response (GR) model 
sampling algorithms  45–46 
Santor, D.A.  2 
Sawatzky, R.  184 
SCATTERPLOT statement  222 
SEED option, MCMC procedure  44 
SERIESPLOT statement  216, 224 
SGRENDER procedure  107 
Sijtsma, K.  207 
Sinharay, S.  213 
 

262   Index 
 
slope parameter  3 
SPARSE option  227 
specifying likelihood model using TABLE function  
129 
speededness  20 
standard error of measurement (SEM)  1–2 
statements 
See specific statements 
statistical inference 
Bayesian approaches to  26–28 
elements of  25–26 
frequentist approaches to  26–28 
statistical models, elements of  25–26 
STATISTICS option, MCMC procedure  45 
Steinberg, L.  7, 11 
Stone, C.A.  8 
Stout, W.F.  19 
SUBJECT option  52 
SURVEYSELECT procedure  189, 210 
Swaminathan, H.  20, 95, 226 
T 
T distribution  47 
TABLE distribution  47 
TABLE function, specifying likelihood model using  
129 
%TADPLOT macro  122, 139 
TEMPLATE procedure  107 
templates, for 1-Parameter (1P) model  52–54 
test information functions, graphs of  153 
testing effects, in unidimensional IRT models  12–14 
testlet IRT model 
about  166 
estimating  166–167 
output from MCMC procedure  167–172 
testlets  13 
test-level fit  206 
test-level measures  213 
THETA parameter  51 
THIN option, MCMC procedure  44 
Thissen, D.  7, 11, 20, 214 
3-Parameter (3P) model 
about  5 
Bayesian estimation of unidimensional IRT models  
94–105 
threshold parameter  3, 116–118 
total score distribution  216 
truncated distribution  90 
2-Parameter (2P) model 
about  5 
Bayesian estimation of unidimensional IRT models  
85–91 
with hierarchical priors  91–94 
U 
“underfitting”  26 
unidimensional IRT models 
Bayesian estimation of for dichotomously scored 
items  81–112 
Bayesian estimation of for polytomously scored 
items  113–153 
compared with bifactor IRT models  200–204 
for dichotomously scored responses  5 
for polytomously scored responses  5–12 
testing effects in  12–14 
UNIFORM distribution  47 
uninformative prior distributions  38–39, 70–71 
V 
vague priors 
See uninformative prior distributions 
VARB parameter  72 
visualization, of IRT models  3–4 
W 
Wainer, H.  13, 166 
Wang, X.  13 
WinBUGS  43–44, 76–77 
within-item multidimensionality  14–15 
Wright, B.D.  11 
Y 
Yen, W.M.  20 
Yen's Q1 statistic  213, 226–231 
Yen's Q3 statistic  214 
Z 
Zumbo, B.D.  16, 184 
 

SAS and all other SAS Institute Inc. product or service names are registered trademarks or trademarks of SAS Institute Inc. in the USA and other countries. ® indicates USA registration. Other brand and product names are 
trademarks of their respective companies. © 2013 SAS Institute Inc. All rights reserved. S107969US.0613
Discover all that you need on your journey to knowledge and empowerment.
support.sas.com/bookstore
for additional books and resources.
Gain Greater Insight into Your  
SAS
® Software with SAS Books.



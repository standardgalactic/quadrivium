

Chapter 1
Introduction
The goal of computer vision is to extract useful information from images. This has
proved a surprisingly challenging task; it has occupied thousands of intelligent and cre-
ative minds over the last four decades, and despite this we are still far from being able to
build a general-purpose “seeing machine.”
Part of the problem is the complexity of visual data. Consider the image in Figure 1.1.
There are hundreds of objects in the scene. Almost none of these are presented in a “typ-
ical” pose. Almost all of them are partially occluded. For a computer vision algorithm,
it is not even easy to establish where one object ends and another begins. For example,
there is almost no change in the image intensity at the boundary between the sky and the
white building in the background. However, there is a pronounced change in intensity on
the back window of the SUV in the foreground, although there is no object boundary or
change in material here.
We might have grown despondent about our chances of developing useful computer
vision algorithms if it were not for one thing: we have concrete proof that vision is
possible because our own visual systems make light work of complex images such as
Figure 1.1. If I ask you to count the trees in this image or to draw a sketch of the street
layout, you can do this easily. You might even be able to pinpoint where this photo was
taken on a world map by extracting subtle visual clues such as the ethnicity of the people,
the types of cars and trees, and the weather.
So, computer vision is not impossible, but it is very challenging; perhaps this was not
appreciated at ﬁrst because what we perceive when we look at a scene is already highly
processed. For example, consider observing a lump of coal in bright sunlight and then
moving to a dim indoor environment and looking at a piece of white paper. The eye
will receive far more photons per unit area from the coal than from the paper, but we
nonetheless perceive the coal as black and the paper as white. The visual brain performs
many tricks of this kind, and unfortunately when we build vision algorithms, we don’t
have the beneﬁt of this preprocessing.
Nonetheless, there has been remarkable recent progress in our understanding of com-
puter vision, and the last decade has seen the ﬁrst large-scale deployments of consumer
computer vision technology. For example, most digital cameras now have embedded
algorithms for face detection, and at the time of writing the Microsoft Kinect (a periph-
eral that allows real-time tracking of the human body) holds the Guinness World Record

2
1 Introduction
Figure 1.1 A visual scene containing many objects, almost all of which are partially occluded.
The red circle indicates a part of the scene where there is almost no brightness change to indicate
the boundary between the sky and the building. The green circle indicates a region in which there
is a large intensity change but this is due to irrelevant lighting effects; there is no object boundary
or change in the object material here.
for being the fastest-selling consumer electronics device ever. The principles behind both
of these applications and many more are explained in this book.
There are a number of reasons for the rapid recent progress in computer vision. The
most obvious is that the processing power, memory, and storage capacity of computers
has vastly increased; before we disparage the progress of early computer vision pioneers,
we should pause to reﬂect that they would have needed specialized hardware to hold
even a single high-resolution image in memory. Another reason for the recent progress
in this area has been the increased use of machine learning. The last 20 years have
seen exciting developments in this parallel research ﬁeld, and these are now deployed
widely in vision applications. Not only has machine learning provided many useful
tools, it has also helped us understand existing algorithms and their connections in a new
light.
The future of computer vision is exciting. Our understanding grows by the day, and
it is likely that artiﬁcial vision will become increasingly prevalent in the next decade.
However, this is still a young discipline. Until recently, it would have been unthink-
able to even try to work with complex scenes such as that in Figure 1.1. As Szeliski
(2010) puts it, “It may be many years before computers can name and outline all of the
objects in a photograph with the same skill as a two year old child.” However, this
book provides a snapshot of what we have achieved and the principles behind these
achievements.

1
Introduction
3
Organization of the book
The structure of this book is illustrated in Figure 1.2. It is divided into six parts.
The ﬁrst part of the book contains background information on probability. All the
models in this book are expressed in terms of probability, which is a useful language for
describing computer vision applications. Readers with a rigorous background in engi-
neering mathematics will know much of this material already but should skim these
chapters to ensure they are familiar with the notation. Those readers who do not have
this background should read these chapters carefully. The ideas are relatively simple, but
they underpin everything else in the rest of the book. It may be frustrating to be forced
to read ﬁfty pages of mathematics before the ﬁrst mention of computer vision, but please
trust me when I tell you that this material will provide a solid foundation for everything
that follows.
The second part of the book discusses machine learning for machine vision. These
chapters teach the reader the core principles that underpin all of our methods to extract
useful information from images. We build statistical models that relate the image data to
the information that we wish to retrieve. After digesting this material, the reader should
understand how to build a model to solve almost any vision problem, although that model
may not yet be very practical.
The third part of the book introduces graphical models for computer vision. Graphi-
cal models provide a framework for simplifying the models that relate the image data to
the properties we wish to estimate. When both of these quantities are high-dimensional,
the statistical connections between them become impractically complex; we can still
deﬁne models that relate them, but we may not have the training data or computa-
tional power to make them useful. Graphical models provide a principled way to assert
sparseness in the statistical connections between the data and the world properties.
The fourth part of the book discusses image preprocessing. This is not necessary to
understand most of the models in the book, but that is not to say that it is unimportant.
The choice of preprocessing method is at least as critical as the choice of model in deter-
mining the ﬁnal performance of a computer vision system. Although image processing
is not the main topic of this book, this section provides a compact summary of the most
important and practical techniques.
The ﬁfth part of the book concerns geometric computer vision; it introduces the pro-
jective pinhole camera – a mathematical model that describes where a given point in the
3D world will be imaged in the pixel array of the camera. Associated with this model
are a set of techniques for ﬁnding the position of the camera relative to a scene and for
reconstructing 3D models of objects.
Finally, in the sixth part of the book, we present several families of vision models
that build on the principles established earlier in the book. These models address some
of the most central problems in computer vision including face recognition, tracking, and
object recognition.
The book concludes with several appendices.
There is a brief discussion of the
notational conventions used in the book, and compact summaries of linear algebra and
optimization techniques. Although this material is widely available elsewhere, it makes
the book more self-contained and is discussed in the same terminology as it is used in the
main text.
At the end of every chapter is a brief notes section. This provides details of the related
research literature. It is heavily weighted toward the most useful and recent papers and

4
1 Introduction
2. Introduction
to probability
Part 1:
Probability
Part 2:
Machine
learning
Part 5:
Geometry
Part 4:
Preprocessing
Part 3:
Connecting local models
Part 6:
Vision models
5. The normal
distribution
4. Fitting
probability
distributions
3. Common
probability
distributions
8. Regression
models
9. Classification
models
14. The pinhole
camera
15. Models for
transformations
16. Multiple
cameras
13. Preprocessing
methods
20. Models for
visual words
19. Temporal
models
18. Models
for style and
identity
17. Models
for shape
10.  Graphical
models
12. Models
for grids
11. Models for
chains and
trees
6. Learning and
inference in
vision
7. Modeling
complex
data densities
Figure 1.2
Chapter dependencies. The book is organized into six sections. The ﬁrst section is
a review of probability and is necessary for all subsequent chapters. The second part concerns
machine learning and inference. It describes both generative and discriminative models. The third
part concerns graphical models: visual representations of the probabilistic dependencies between
variables in large models. The fourth part describes preprocessing methods. The ﬁfth part concerns
geometry and transformations. Finally, the sixth part presents several other important families of
vision models.

1
Introduction
5
does not reﬂect an accurate historical description of each area. There are also a num-
ber of exercises for the reader at the end of each chapter. In some cases, important but
tedious derivations have been excised from the text and turned into problems to retain
the ﬂow of the main argument. Here, the solution will be posted on the main book
Web site (http://www.computervisionmodels.com). A series of applications are also pre-
sented at the end of each chapter (apart from Chapters 1–5 and Chapter 10, which contain
only theoretical material). Collectively, these represent a reasonable cross-section of the
important vision papers of the last decade.
Finally, pseudocode for over 70 of the algorithms discussed is available and can
be downloaded in a separate document from the associated Web site (http://www.
computervisionmodels.com). Throughout the text, the symbol [
] denotes that there is
pseudocode associated with this portion of the text. This pseudocode uses the same nota-
tion as the book and will make it easy to implement many of the models. I chose not to
include this in the main text because it would have decreased the readability. However, I
encourage all readers of this book to implement as many of the models as possible. Com-
puter vision is a practical engineering discipline, and you can learn a lot by experimenting
with real code.
Other books
I am aware that most people will not learn computer vision from this book alone, so here
is some advice about other books that complement this volume. To learn more about
machine learning and graphical models, I recommend ‘Pattern Recognition and Machine
Learning’ by Bishop (2006) as a good starting point. There are many books on prepro-
cessing, but my favorite is ‘Feature Extraction and Image Processing’ by Nixon and
Aguado (2008). The best source for information about geometrical computer vision is,
without a doubt, ‘Multiple View Geometry in Computer Vision’ by Hartley and Zisserman
(2004). Finally, for a much more comprehensive overview of the state of the art of com-
puter vision and its historical development, consider ‘Computer Vision: Algorithms and
Applications’ by Szeliski (2010).


Part I
Probability

The ﬁrst part of this book (Chapters 2–5) is devoted to a brief review of probability
and probability distributions. Almost all models for computer vision can be interpreted
in a probabilistic context, and in this book we will present all the material in this light.
The probabilistic interpretation may initially seem confusing, but it has a great advantage:
it provides a common notation that will be used throughout the book and will elucidate
relationships between different models that would otherwise remain opaque.
So why is probability a suitable language to describe computer vision problems? In
a camera, the three-dimensional world is projected onto the optical surface to form the
image: a two-dimensional set of measurements. Our goal is to take these measurements
and use them to establish the properties of the world that created them. However, there
are two problems. First, the measurement process is noisy; what we observe is not the
amount of light that fell on the sensor, but a noisy estimate of this quantity. We must
describe the noise in these data, and for this we use probability. Second, the relationship
between world and measurements is generally many to one: there may be many real-
world conﬁgurations that are compatible with the same measurements. The chance that
each of these possible worlds is present can also be described using probability.
The structure of Part I is as follows:
in Chapter 2, we introduce the basic
rules for manipulating probability distributions including the ideas of conditional and
marginal probability and Bayes’ rule. We also introduce more advanced ideas such as
independence and expectation.
In Chapter 3, we discuss the properties of eight speciﬁc probability distributions. We
divide these into two sets of four distributions each. The ﬁrst set will be used to describe
either the observed data or the state of the world. The second set of distributions model
the parameters of the ﬁrst set. In combination, they allow us to ﬁt a probability model
and provide information about how certain we are about the ﬁt.
In Chapter 4, we discuss methods for ﬁtting probability distributions to observed
data. We also discuss how to assess the probability of new data points under the ﬁt-
ted model and how to take account of uncertainty in the ﬁtted model when we do this.
Finally, in Chapter 5, we investigate the properties of the multivariate normal distribution
in detail. This distribution is ubiquitous in vision applications and has a number of useful
properties that are frequently exploited in machine vision.
Readers who are very familiar with probability models and the Bayesian philosophy
may wish to skip this part and move directly to Part II.

Chapter 2
Introduction to probability
In this chapter, we provide a compact review of probability theory. There are very few
ideas, and each is relatively simple when considered separately. However, they combine
to form a powerful language for describing uncertainty.
2.1
Random variables
A random variable x denotes a quantity that is uncertain. The variable may denote the
result of an experiment (e.g., ﬂipping a coin) or a real-world measurement of a ﬂuctuating
property (e.g., measuring the temperature). If we observe several instances {xi}I
i=1 then
it might take a different value on each occasion. However, some values may occur more
often than others. This information is captured by the probability distribution Pr(x) of
the random variable.
A random variable may be discrete or continuous.
A discrete variable takes values
from a predeﬁned set. This set may be ordered (the outcomes 1–6 of rolling a die) or
unordered (the outcomes “sunny,” “raining,” “snowing” upon observing the weather). It
may be ﬁnite (there are 52 possible outcomes of drawing a card randomly from a standard
pack) or inﬁnite (the number of people on the next train is theoretically unbounded). The
probability distribution of a discrete variable can be visualized as a histogram or a Hinton
diagram (Figure 2.1). Each outcome has a positive probability associated with it, and the
sum of the probabilities for all outcomes is always one.
Continuous random variables take values that are real numbers. These may be ﬁnite
(the time taken to ﬁnish a 2-hour exam is constrained to be greater than 0 hours and
less than 2 hours) or inﬁnite (the amount of time until the next bus arrives is unbounded
above). Inﬁnite continuous variables may be deﬁned on the whole real range or may be
bounded above or below (the 1D velocity of a vehicle may take any value, but the speed
is bounded below by 0). The probability distribution of a continuous variable can be
visualized by plotting the probability density function (pdf). The probability density for
an outcome represents the relative propensity of the random variable to take that value
(see Figure 2.2). It may take any positive value. However, the integral of the pdf always
sums to one.

10
2 Introduction to probability
Figure 2.1 Two different representations for discrete probabilities a) A bar graph representing
the probability that a biased six-sided die lands on each face. The height of the bar represents the
probability, so the sum of all heights is one. b) A Hinton diagram illustrating the probability of
observing different weather types in England. The area of the square represents the probability, so
the sum of all areas is one.
Figure 2.2 Continuous probability distri-
bution (probability density function or pdf
for short) for time taken to complete a
test. Note that the probability density can
exceed one, but the area under the curve
must always have unit area.
2.2
Joint probability
Consider two random variables, x and y. If we observe multiple paired instances of x
and y, then some combinations of the two outcomes occur more frequently than others.
This information is encompassed in the joint probability distribution of x and y, which
is written as Pr(x,y). The comma in Pr(x,y) can be read as the English word “and”
so Pr(x,y) is the probability of x and y. A joint probability distribution may relate
variables that are all discrete or all continuous, or it may relate discrete variables to con-
tinuous ones (see Figure 2.3). Regardless, the total probability of all outcomes (summing
over discrete variables and integrating over continuous ones) is always one.
In general, we will be interested in the joint probability distribution of more than
two variables. We will write Pr(x,y,z) to represent the joint probability distribution of
scalar variables x,y, and z. We may also write Pr(x) to represent the joint probability
of all of the elements of the multidimensional variable x = [x1,x2,...,xK]T . Finally,
we will write Pr(x,y) to represent the joint distribution of all of the elements from
multidimensional variables x and y.
2.3
Marginalization
We can recover the probability distribution of any single variable from a joint distribution
by summing (discrete case) or integrating (continuous case) over all the other variables
(Figure 2.4). For example, if x and y are both continuous and we know Pr(x,y), then

2.3
Marginalization
11
Figure 2.3 Joint probability distributions between variables x and y. a–c) The same joint pdf of
two continuous variables represented as a surface, contour plot, and image, respectively. d) Joint
distribution of two discrete variables represented as a 2D Hinton diagram. e) Joint distribution of
a continuous variable x and discrete variable y. f) Joint distribution of a discrete variable x and
continuous variable y.
we can recover the distributions Pr(x) and Pr(y) using the relations
Pr(x) =
Z
Pr(x,y) dy,
Pr(y) =
Z
Pr(x,y) dx.
(2.1)
The recovered distributions Pr(x) and Pr(y) are referred to as marginal distri-
butions, and the process of integrating/summing over the other variables is called
marginalization. Calculating the marginal distribution Pr(x) from the joint distribu-
tion Pr(x,y) by marginalizing over the variable y has a simple interpretation: we are
ﬁnding the probability distribution of x regardless of (or in the absence of information
about) the value of y.
In general, we can recover the joint probability of any subset of variables, by
marginalizing over all of the others. For example, given variables, w,x,y,z, where w
is discrete and z is continuous, we can recover Pr(x,y) using
Pr(x,y) =
X
w
Z
Pr(w,x,y,z) dz.
(2.2)

12
2 Introduction to probability
Figure 2.4 Joint and marginal probability distributions. The marginal probability Pr(x) is found
by summing over all values of y (discrete case) or integrating over y (continuous case) in the
joint distribution Pr(x,y). Similarly the marginal probability Pr(y) is found by summing or
integrating over x. Note that the plots for the marginal distributions have different scales from
those for the joint distribution (on the same scale, the marginals would look larger as they sum all
of the mass from one direction). a) Both x and y are continuous. b) Both x and y are discrete. c)
The random variable x is continuous and the variable y is discrete.
2.4
Conditional probability
The conditional probability of x given that y takes value y∗tells us the relative propensity
of the random variable x to take different outcomes given that the random variable y is
ﬁxed to value y∗. This conditional probability is written as Pr(x|y = y∗) . The vertical
line “|” can be read as the English word “given.”
The conditional probability Pr(x|y = y∗) can be recovered from the joint distribution
Pr(x,y). In particular, we examine the appropriate slice Pr(x,y = y∗) of the joint
distribution (Figure 2.5). The values in the slice tell us about the relative probability that
x takes various values having observed y = y∗, but they do not themselves form a valid
probability distribution; they cannot sum to one as they constitute only a small part of
the joint distribution which did itself sum to one. To calculate the conditional probability
distribution, we hence normalize by the total probability in the slice
Pr(x|y = y∗) =
Pr(x,y = y∗)
R
Pr(x,y = y∗)dx = Pr(x,y = y∗)
Pr(y = y∗) ,
(2.3)
where we have used the marginal probability relation (Equation 2.1) to simplify the
denominator. It is common to write the conditional probability relation without explicitly
deﬁning the value y = y∗to give the more compact notation
Pr(x|y) = Pr(x,y)
Pr(y) .
(2.4)
This relationship can be rearranged to give
Pr(x,y) = Pr(x|y)Pr(y),
(2.5)
and by symmetry we also have
Pr(x,y) = Pr(y|x)Pr(x).
(2.6)

2.5
Bayes’ rule
13
Figure 2.5 Conditional probability. Joint pdf of x and y and two conditional probability distri-
butions Pr(x|y = y1) and Pr(x|y = y2). These are formed by extracting the appropriate slice
from the joint pdf and normalizing so that the area is one. A similar operation can be performed
for discrete distributions.
When we have more than two variables, we may repeatedly take conditional prob-
abilities to divide up the joint probability distribution into a product of terms
Pr(w,x,y,z) = Pr(w,x,y|z)Pr(z)
= Pr(w,x|y,z)Pr(y|z)Pr(z)
= Pr(w|x,y,z)Pr(x|y,z)Pr(y|z)Pr(z).
(2.7)
2.5
Bayes’ rule
In Equations 2.5 and 2.6 we expressed the joint probability in two ways. We can combine
these formulations to ﬁnd a relationship between Pr(x|y) and Pr(y|x),
Pr(y|x)Pr(x) = Pr(x|y)Pr(y),
(2.8)
or, rearranging, we have
Pr(y|x) = Pr(x|y)Pr(y)
Pr(x)
= Pr(x|y)Pr(y)
R
Pr(x,y) dy
=
Pr(x|y)Pr(y)
R
Pr(x|y)Pr(y) dy ,
(2.9)
where in the second and third lines we have expanded the denominator using the deﬁni-
tions of marginal and conditional probability, respectively. These three equations are all
commonly referred to as Bayes’ rule.
Each term in Bayes’ rule has a name. The term Pr(y|x) on the left-hand side is the
posterior. It represents what we know about y given x. Conversely, the term Pr(y) is
the prior as it represents what is known about y before we consider x. The term Pr(x|y)
is the likelihood, and the denominator Pr(x) is the evidence.

14
2 Introduction to probability
Figure 2.6 Independence. a) Joint pdf of continuous independent variables x and y. The inde-
pendence of x and y means that every conditional distribution is the same: the value of y tells
us nothing about x and vice versa. Compare this to Figure 2.5, which illustrated variables that
were dependent. b) Joint distribution of discrete independent variables x and y. The conditional
distributions of x given y are all the same.
In computer vision, we often describe the relationship between variables x and y in
terms of the conditional probability Pr(x|y). However, we may be primarily interested
in the variable y, and in this situation Bayes’ rule is exploited to compute the probability
Pr(y|x).
2.6
Independence
If knowing the value of variable x tells us nothing about variable y (and vice versa), then
we say x and y are independent (Figure 2.6). Here, we can write
Pr(x|y) = Pr(x)
Pr(y|x) = Pr(y).
(2.10)
Substituting into Equation 2.5, we see that for independent variables the joint
probability Pr(x,y) is the product of the marginal probabilities Pr(x) and Pr(y),
Pr(x,y) = Pr(x|y)Pr(y) = Pr(x)Pr(y).
(2.11)
2.7
Expectation
Given a function f[•] that returns a value for each possible value x∗of the variable x and
a probability Pr(x = x∗) that each value of x occurs, we sometimes wish to calculate
the expected output of the function. If we drew a very large number of samples from the
probability distribution, calculated the function for each sample, and took the average of
these values, the result would be the expectation. More precisely, the expected value of a
function f[•] of a random variable x is deﬁned as
E[f[x]] =
X
x
f[x]Pr(x)
E[f[x]] =
Z
f[x]Pr(x) dx,
(2.12)

Discussion
15
Function f[•]
Expectation
x
mean, µx
xk
kth moment about zero
(x −µx)k
kth moment about the mean
(x −µx)2
variance
(x −µx)3
skew
(x −µx)4
kurtosis
(x −µx)(y −µy)
covariance of x and y
Table 2.1 Special cases of expectation. For some functions f(x), the expectation E[f(x)] is given
a special name. Here we use the notation µx to represent the mean with respect to random variable
x and µy the mean with respect to random variable y.
for the discrete and continuous cases, respectively. This idea generalizes to functions
f[•] of more than one random variable so that, for example,
E[f[x,y]] =
Z Z
f[x,y]Pr(x,y) dx dy.
(2.13)
For some choices of the function f[•], the expectation is given a special name (Table 2.1).
Such quantities are commonly used to summarize the properties of complex probability
distributions.
There are four rules for manipulating expectations, which can be easily proved from
the original deﬁnition (Equation 2.12):
1. The expected value of a constant κ with respect to the random variable x is just the
constant itself:
E[κ] = κ.
(2.14)
2. The expected value of a constant κ times a function f[x] of the random variable x
is κ times the expected value of the function:
E[κf[x]] = κE[f[x]].
(2.15)
3. The expected value of the sum of two functions of a random variable x is the sum
of the individual expected values of the functions:
E[f[x] + g[x]] = E[f[x]] + E[g[x]].
(2.16)
4. The expected value of the product of two functions f[x] and g[y] of random vari-
ables x and y is equal to the product of the individual expected values if the
variables x and y are independent:
E[f[x]g[y]] = E[f[x]]E[g[y]],
if x,y independent.
(2.17)
Discussion
The rules of probability are remarkably compact and simple. The concepts of marginal-
ization, joint and conditional probability, independence, and Bayes’ rule will underpin all
of the machine vision algorithms in this book. There is one remaining important concept
related to probability, which is conditional independence. We discuss this at length in
Chapter 10.

16
2 Introduction to probability
Notes
For a more formal discussion of probability, the reader is encouraged to investigate one of the
many books on this topic (e.g., Papoulis 1991). For a view of probability from a machine learning
perspective, consult the ﬁrst chapter of Bishop (2006).
Problems
2.1 Give a real-world example of a joint distribution Pr(x,y) where x is discrete and y is
continuous.
2.2 What remains if I marginalize a joint distribution Pr(v,w,x,y,z) over ﬁve variables with
respect to variables w and y? What remains if I marginalize the resulting distribution with respect
to v?
2.3 Show that the following relation is true:
Pr(w,x,y,z) = Pr(x,y)Pr(z|w,x,y)Pr(w|x,y).
2.4 In my pocket there are two coins. Coin 1 is unbiased, so the likelihood Pr(h = 1|c = 1) of
getting heads is 0.5 and the likelihood Pr(h = 0|c = 1) of getting tails is also 0.5. Coin 2 is biased,
so the likelihood Pr(h = 1|c = 2) of getting heads is 0.8 and the likelihood Pr(h = 0|c = 2) of
getting tails is 0.2. I reach into my pocket and draw one of the coins at random. There is an equal
prior probability I might have picked either coin. I ﬂip the coin and observe a head. Use Bayes’
rule to compute the posterior probability that I chose coin 2.
2.5 If variables x and y are independent and variables x and z are independent, does it follow that
variables y and z are independent?
2.6 Use Equation 2.3 to show that when x and y are independent, the marginal distribution Pr(x)
is the same as the conditional distribution Pr(x|y = y∗) for any y∗.
2.7 The joint probability Pr(w,x,y,z) over four variables factorizes as
Pr(w,x,y,z) = Pr(w)Pr(z|y)Pr(y|x,w)Pr(x).
Demonstrate that x is independent of w by showing that Pr(x,w) = Pr(x)Pr(w).
2.8
Consider a biased die where the probabilities of rolling sides {1,2,3,4,5,6} are
{1/12,1/12,1/12,1/12,1/6,1/2}, respectively. What is the expected value of the die? If I roll
the die twice, what is the expected value of the sum of the two rolls?
2.9 Prove the four relations for manipulating expectations:
E[κ] = κ,
E[κf[x]] = κE[f[x]],
E[f[x] + g[x]] = E[f[x]] + E[g[x]],
E[f[x]g[y]] = E[f[x]]E[g[y]],
if x,y independent.
For the last case, you will need to use the deﬁnition of independence (see Section 2.6).
2.10 Use the relations from Problem 2.9 to prove the following relationship between the second
moment around zero and the second moment about the mean (variance):
E

(x −µ)2
= E

x2
−E[x]E[x].

Chapter 3
Common probability
distributions
In Chapter 2 we introduced abstract rules for manipulating probabilities. To use these
rules we will need to deﬁne some probability distributions. The choice of distribution
Pr(x) will depend on the domain of the data x that we are modeling (Table 3.1).
Data Type
Domain
Distribution
univariate, discrete,
x ∈{0,1}
Bernoulli
binary
univariate, discrete,
x ∈{1,2,...,K}
categorical
multivalued
univariate, continuous,
x ∈R
univariate normal
unbounded
univariate, continuous,
x ∈[0,1]
beta
bounded
multivariate, continuous,
x ∈RK
multivariate normal
unbounded
multivariate, continuous,
x = [x1,x2,...,xK]T
Dirichlet
bounded, sums to one
xk ∈[0,1],PK
k=1 xk = 1
bivariate, continuous,
x = [x1,x2]
normal-scaled
x1 unbounded,
x1 ∈R
inverse gamma
x2 bounded below
x2 ∈R+
vector x and matrix X,
x ∈RK
normal
x unbounded,
X ∈RK×K
inverse Wishart
X square, positive deﬁnite
zT Xz > 0
∀z ∈RK
Table 3.1 Common probability distributions: the choice of distribution depends on the type/
domain of data to be modeled.
Probability distributions such as the categorical and normal distributions are obvi-
ously useful for modeling visual data.
However, the need for some of the other
distributions is not so obvious; for example, the Dirichlet distribution models K positive
numbers that sum to one. Visual data do not normally take this form.

18
3 Common probability distributions
Distribution
Domain
Parameters Modeled by
Bernoulli
x ∈{0,1}
beta
categorical
x ∈{1,2,...,K}
Dirichlet
univariate normal
x ∈R
normal inverse gamma
multivariate normal
x ∈Rk
normal inverse Wishart
Table 3.2 Common distributions used for modeling (left) and their associated domains (center).
For each of these distributions, there is a second associated distribution over the parameters (right).
The explanation is as follows: when we ﬁt probability models to data, we need to
know how uncertain we are about the ﬁt. This uncertainty is represented as a probability
distribution over the parameters of the ﬁtted model. So for each distribution used for
modeling, there is a second distribution over the associated parameters (Table 3.2). For
example, the Dirichlet is used to model the parameters of the categorical distribution.
In this context, the parameters of the Dirichlet would be known as hyperparameters.
More generally, the hyperparameters determine the shape of the distribution over the
parameters of the original distribution.
We will now work through the distributions in Table 3.2 before looking more closely
at the relationship between these pairs of distributions.
3.1
Bernoulli distribution
The Bernoulli distribution (Figure 3.1) is a discrete distribution that models binary trials:
it describes the situation where there are only two possible outcomes x ∈{0,1} which
are referred to as “failure” and “success.” In machine vision, the Bernoulli distribution
could be used to model the data. For example, it might describe the probability of a pixel
taking an intensity value of greater or less than 128. Alternatively, it could be used to
model the state of the world. For example, it might describe the probability that a face is
present or absent in the image.
The Bernoulli has a single parameter λ ∈[0,1] which deﬁnes the probability of
observing a success x = 1. The distribution is hence
Pr(x = 0) = 1 −λ
Pr(x = 1) = λ.
(3.1)
We can alternatively express this as
Pr(x) = λx(1 −λ)1−x,
(3.2)
and we will sometimes use the equivalent notation
Pr(x) = Bernx[λ].
(3.3)

3.3
Categorical distribution
19
Figure 3.1 Bernoulli distribution. The Bernoulli
distribution is a discrete distribution with two pos-
sible outcomes, x ∈{0,1} which are referred to as
failure and success, respectively. It is governed by
a single parameter λ that determines the probabil-
ity of success such that Pr(x = 0) = 1 −λ and
Pr(x = 1) = λ.
3.2
Beta distribution
The beta distribution (Figure 3.2) is a continuous distribution deﬁned on single parameter
λ where λ ∈[0,1]. As such it is suitable for representing uncertainty in the parameter λ
of the Bernoulli distribution.
Figure 3.2 Beta distribution. The beta distribution is deﬁned on [0,1] and has parameters (α,β)
whose relative values determine the expected value so E[λ] = α/(α + β) (numbers in parentheses
show the α,β for each curve). As the absolute values of (α,β) increase, the concentration around
E[λ] increases. a) E[λ] = 0.5 for each curve, concentration varies. b) E[λ] = 0.25. c) E[λ] = 0.75.
The beta distribution has two parameters α,β ∈[0,∞], which both take positive val-
ues and affect the shape of the curve as indicated in Figure 3.2. Mathematically, the beta
distribution has the form
Pr(λ) = Γ[α + β]
Γ[α]Γ[β]λα−1(1 −λ)β−1,
(3.4)
where Γ[•] is the gamma function.1 For short, we abbreviate this to
Pr(λ) = Betaλ[α,β].
(3.5)
3.3
Categorical distribution
The categorical distribution (Figure 3.3) is a discrete distribution that determines the
probability of observing one of K possible outcomes. Hence, the Bernoulli distribution
1The gamma function is deﬁned as Γ[z] =
R ∞
0
tz−1e−tdt and is closely related to factorials, so that for
positive integers Γ[z] = (z −1)! and Γ[z + 1] = zΓ[z].

20
3 Common probability distributions
Figure 3.3 The categorical distribution is
a discrete distribution with K possible out-
comes, x ∈{1,2,...,K} and K parameters
λ1,λ2,...,λK where λk ≥0 and P
k λk =
1. Each parameter represents the probabil-
ity of observing one of the outcomes, so that
the probability of observing x = k is given
by λk. When the number of possible out-
comes K is 2, the categorical reduces to the
Bernoulli distribution.
is a special case of the categorical distribution when there are only two outcomes. In
machine vision the intensity data at a pixel is usually quantized into discrete levels and
so can be modeled with a categorical distribution. The state of the world may also take
one of several discrete values. For example an image of a vehicle might be classiﬁed
into {car,motorbike,van,truck} and our uncertainty over this state could be described by
a categorical distribution.
The probabilities of observing the K outcomes are held in a K × 1 parameter vector
λ = [λ1,λ2,...,λK], where λk ∈[0,1] and PK
k=1 λk = 1. The categorical distribution
can be visualized as a normalized histogram with K bins and can be written as
Pr(x = k) = λk.
(3.6)
For short, we use the notation
Pr(x) = Catx [λ].
(3.7)
Alternatively, we can think of the data as taking values x ∈{e1,e2,...,eK} where
ek is the kth unit vector; all elements of ek are zero except the kth, which is one. Here
we can write
Pr(x = ek) =
K
Y
j=1
λxj
j = λk,
(3.8)
where xj is the jth element of x.
3.4
Dirichlet distribution
The Dirichlet distribution (Figure 3.4) is deﬁned over K continuous values λ1 ...λK
where λk ∈[0,1] and PK
k=1 λk = 1. Hence it is suitable for deﬁning a distribution over
the parameters of the categorical distribution.
In K dimensions the Dirichlet distribution has K parameters α1 ...αK each of
which can take any positive value. The relative values of the parameters determine the
expected values E[λ1]...E[λk]. The absolute values determine the concentration around
the expected value. We write
Pr(λ1...K) = Γ[PK
k=1 αk]
QK
k=1 Γ[αk]
K
Y
k=1
λαk−1
k
,
(3.9)

3.6
Normal-scaled inverse gamma distribution
21
Figure 3.4 The Dirichlet distribution in K dimensions is deﬁned on values λ1,λ2,...,λK such
that P
k λk = 1 and λk ∈[0,1] ∀k ∈{1...K}. a) For K=3, this corresponds to a triangular
section of the plane P
k λk = 1. In K dimensions, the Dirichlet is deﬁned by K positive parameters
α1...K. The ratio of the parameters determines the expected value for the distribution. The absolute
values determine the concentration: the distribution is highly peaked around the expected value at
high parameter values but pushed away from the expected value at low parameter values. b–e) Ratio
of parameters is equal, absolute values increase. f–i) Ratio of parameters favors α3 > α2 > α1,
absolute values increase.
or for short
Pr(λ1...K) = Dirλ1...K[α1...K].
(3.10)
Just as the Bernoulli distribution was a special case of the categorical distribution
with two possible outcomes, so the beta distribution is a special case of the Dirichlet
distribution where the dimensionality is two.
3.5
Univariate normal distribution
The univariate normal or Gaussian distribution (Figure 3.5) is deﬁned on continuous
values x ∈[−∞,∞]. In vision, it is common to ignore the fact that the intensity of a
pixel is quantized and model it with the continuous normal distribution. The world state
may also be described by the normal distribution. For example, the distance to an object
could be represented in this way.
The normal distribution has two parameters, the mean µ and the variance σ2. The
parameter µ can take any value and determines the position of the peak. The parameter
σ2 takes only positive values and determines the width of the distribution. The normal
distribution is deﬁned as
Pr(x) =
1
√
2πσ2 exp

−0.5(x −µ)2
σ2

,
(3.11)
and we will abbreviate this by writing
Pr(x) = Normx[µ,σ2].
(3.12)
3.6
Normal-scaled inverse gamma distribution
The normal-scaled inverse gamma distribution (Figure 3.6) is deﬁned over a pair of
continuous values µ,σ2, the ﬁrst of which can take any value and the second of which is

22
3 Common probability distributions
(0.0,1.0)
(-3.4,0.25)
(1.5,4.41)
Probability Density
–6
6
0
Figure 3.5 The univariate normal distri-
bution is deﬁned on x ∈R and has two
parameters {µ,σ2}.
The mean parameter
µ determines the expected value and the
variance σ2 determines the concentration
about the mean so that as σ2 increases, the
distribution becomes wider and ﬂatter.
σ2
μ
Figure 3.6 The normal-scaled inverse gamma distribution deﬁnes a probability distribution over
bivariate continuous values µ,σ2 where µ ∈[−∞,∞] and σ2 ∈[0,∞]. a) Distribution with
parameters [α,β,γ,δ] = [1,1,1,0]. b) Varying α. c) Varying β. d) Varying γ. e) Varying δ.
constrained to be positive. As such it can deﬁne a distribution over the mean and variance
parameters of the normal distribution.
The normal-scaled inverse gamma has four parameters α,β,γ,δ where α,β, and γ
are positive real numbers but δ can take any value. It has pdf:
Pr(µ,σ2) =
√γ
σ
√
2π
βα
Γ[α]
 1
σ2
α+1
exp

−2β + γ(δ −µ)2
2σ2

,
(3.13)
or for short
Pr(µ,σ2) = NormInvGamµ,σ2[α,β,γ,δ].
(3.14)
3.7
Multivariate normal distribution
The multivariate normal or Gaussian distribution models D-dimensional variables x
where each of the D elements x1 ...xD is continuous and lies in the range [−∞,+∞]
(Figure 3.7). As such the univariate normal distribution is a special case of the multivari-
ate normal where the number of elements D is one. In machine vision the multivariate
normal might model the joint distribution of the intensities of D pixels within a region
of the image. The state of the world might also be described by this distribution. For
example, the multivariate normal might describe the joint uncertainty in the 3D position
(x,y,z) of an object in the scene.

3.8
Normal inverse Wishart distribution
23
Figure 3.7 The multivariate normal dis-
tribution models D-dimensional variables
x = [x1 ...xD]T where each dimension xd
is continuous and real. It is deﬁned by a
D × 1 vector µ deﬁning the mean of the
distribution and a D × D covariance matrix
Σ which determines the shape.
The iso-
contours of the distribution are ellipsoids
where the center of the ellipsoid is deter-
mined by µ and the shape by Σ. This ﬁgure
depicts a bivariate distribution, where the
covariance is illustrated by drawing one of
these ellipsoids.
The multivariate normal distribution has two parameters: the mean µ and covari-
ance Σ. The mean µ is a D × 1 vector that describes the mean of the distribution. The
covariance Σ is a symmetric D × D positive deﬁnite matrix so that zT Σz is positive for
any real vector z. The probability density function has the following form
Pr(x) =
1
(2π)D/2|Σ|1/2 exp

−0.5(x −µ)T Σ−1(x −µ)

,
(3.15)
or for short
Pr(x) = Normx [µ,Σ].
(3.16)
The multivariate normal distribution will be used extensively throughout this book,
and we devote the whole of Chapter 5 to describing its properties.
3.8
Normal inverse Wishart distribution
The normal inverse Wishart distribution deﬁnes a distribution over a D ×1 vector µ and
a D × D positive deﬁnite matrix Σ. As such it is suitable for describing uncertainty in
the parameters of a multivariate normal distribution. The normal inverse Wishart has four
parameters α,Ψ,γ,δ, where α and γ are positive scalars, δ is a D × 1 vector and Ψ is a
positive deﬁnite D × D matrix
Pr(µ,Σ)
= γD/2|Ψ|α/2|Σ|−(α+D+2)/2 exp

−0.5
 Tr[ΨΣ−1] + γ(µ −δ)T Σ−1(µ −δ)γD/2
2αD/2(2π)D/2ΓD[α/2]
,
(3.17)

24
3 Common probability distributions
–
–
–
–
–
–
–
–
Figure 3.8 Sampling from 2D normal inverse Wishart distribution. a) Each sample consists of a
mean vector and covariance matrix, here visualized with 2D ellipses illustrating the iso-contour of
the associated Gaussian at a Mahalanobis distance of 2. b) Changing α modiﬁes the dispersion of
covariances observed. c) Changing Ψ modiﬁes the average covariance. d) Changing γ modiﬁes
the dispersion of mean vectors observed. e) Changing δ modiﬁes the average value of the mean
vectors.
where ΓD[•] is the multivariate gamma function and Tr[Ψ] returns the trace of the matrix
Ψ (see Appendix C.2.4). For short we will write
Pr(µ,Σ) = NorIWisµ,Σ [α,Ψ,γ,δ].
(3.18)
The mathematical form of the normal inverse Wishart distribution is rather opaque.
However, it is just a function that produces a positive value for any valid mean vector
µ and covariance matrix Σ, such that when we integrate over all possible values of µ
and Σ, the answer is one. It is hard to visualize the normal inverse Wishart, but easy to
draw samples and examine them: each sample is the mean and covariance of a normal
distribution (Figure 3.8).
3.9
Conjugacy
We have argued that the beta distribution can represent probabilities over the parame-
ters of the Bernoulli. Similarly the Dirichlet deﬁnes a distribution over the parameters
of the categorical, and there are analogous relationships between the normal-scaled
inverse gamma and univariate normal and the normal inverse Wishart and the multivariate
normal.
These pairs were carefully chosen because they have a special relationship: in each
case, the former distribution is conjugate to the latter: the beta is conjugate to the
Bernoulli and the Dirichlet is conjugate to the categorical and so on. When we multi-
ply a distribution with its conjugate, the result is proportional to a new distribution which
has the same form as the conjugate. For example
Bernx[λ] · Betaλ[α,β] = κ(x,α,β) · Betaλ
h
˜α, ˜β
i
,
(3.19)
where κ is a scaling factor that is constant with respect to the variable of interest, λ.
It is important to realize that this was not necessarily the case: if we had picked any

Problems
25
distribution other than the beta, then this product would not have retained the same form.
For this case, the relationship in Equation 3.19 is easy to prove
Bernx[λ] · Betaλ[α,β] = λx(1 −λ)1−x Γ[α + β]
Γ[α]Γ[β]λα−1(1 −λ)β−1
= Γ[α + β]
Γ[α]Γ[β]λx+α−1(1 −λ)1−x+β−1
= Γ[α + β]
Γ[α]Γ[β]
Γ[x + α]Γ[1 −x + β]
Γ[x + α + 1 −x + β] Betaλ[x + α,1 −x + β]
= κ(x,α,β) · Betaλ
h
˜α, ˜β
i
,
(3.20)
where in the third line we have both multiplied and divided by the constant associated
with Betaλ[˜α, ˜β].
The conjugate relationship is important because we take products of distributions
during both learning (ﬁtting distributions) and evaluating the model (assessing the prob-
ability of new data under the ﬁtted distribution). The conjugate relationship means that
these products can both be computed neatly in closed form.
Summary
We use probability distributions to describe both the world state and the image data. We
have presented four distributions (Bernoulli, categorical, univariate normal, multivariate
normal) that are suited to this purpose. We also presented four other distributions (beta,
Dirichlet, normal-scaled inverse gamma, normal inverse Wishart) that can be used to
describe the uncertainty in parameters of the ﬁrst; they can hence describe the uncertainty
in the ﬁtted model. These four pairs of distributions have a special relationship: each
distribution from the second set is conjugate to one from the ﬁrst set. As we shall see,
the conjugate relationship makes it easier to ﬁt these distributions to observed data and
evaluate new data under the ﬁtted model.
Notes
Throughout this book, I use rather esoteric terminology for discrete distributions.
I distinguish
between the binomial distribution (probability of getting M successes in N binary trials) and the
Bernoulli distribution (the binary trial itself or probability of getting a success or failure in one
trial) and talk exclusively about the latter distribution. I take a similar approach to discrete vari-
ables which can take K values. The multinomial distribution assigns a probability to observing
the values {1,2,...,K} with frequency {M1,M2,...,MK} given N trials. The categorical dis-
tribution is a special case of this with N = 1. Most other authors do not make this distinction and
would term this “multinomial” as well.
A more complete list of common probability distributions and details of their properties are given
in Appendix B of Bishop (2006). Further information about conjugacy can be found in Chapter 2
of Bishop (2006) or any textbook on Bayesian methods, such as that of Gelman et al. (2004). Much
more information about the normal distribution is provided in Chapter 5 of this book.
Problems
3.1 Consider a variable x which is Bernoulli distributed with parameter λ. Show that the mean
E[x] is λ and the variance E[(x −E[x])2] is λ(1 −λ).

26
3 Common probability distributions
3.2 Calculate an expression for the mode (position of the peak) of the beta distribution with
α,β > 1 in terms of the parameters α and β.
3.3 The mean and variance of the beta distribution are given by the expressions
E[λ] = µ =
α
α + β
E[(λ −µ)2] = σ2 =
αβ
(α + β)2(α + β + 1).
We may wish to choose the parameters α and β so that the distribution has a particular mean µ and
variance σ2. Derive suitable expressions for α and β in terms of µ and σ2.
3.4
All of the distributions in this chapter are members of the exponential family and can be
written in the form
Pr(x|θ) = a[x]exp[b[θ]c[x] −d[θ]],
where a[x] and c[x] are functions of the data and b[θ] and d[θ] are functions of the parameters.
Find the functions a[x],b[θ],c[x] and d[θ] that allow the beta distribution to be represented in the
generalized form of the exponential family.
3.5 Use integration by parts to prove that if
Γ[z] =
Z ∞
0
tz−1e−tdt,
then
Γ[z + 1] = zΓ[z].
3.6 Consider a restricted family of univariate normal distributions where the variance is always 1,
so that
Pr(x|µ) =
1
√
2π
exp

−0.5(x −µ)2
.
Show that a normal distribution over the parameter µ
Pr(µ) = Normµ[µp,σ2
p]
has a conjugate relationship to the restricted normal distribution.
3.7 For the normal distribution, ﬁnd the functions a[x],b[θ],c[x], and d[θ] that allow it to be
represented in the generalized form of the exponential family (see Problem 3.4).
3.8 Calculate an expression for the mode (position of the peak in µ,σ2 space) of the normal-scaled
inverse gamma distribution in terms of the parameters α,β,γ,δ.
3.9 Show that the more general form of the conjugate relation in which we multiply I Bernoulli
distributions by the conjugate beta prior is given by
I
Y
i=1
Bernxi[λ] · Betaλ[α,β] = κ · Betaλ[˜α, ˜β],
where
κ = Γ[α + β]Γ[α + Pxi]Γ[β + P(1 −xi)]
Γ[α + β + I]Γ[α]Γ[β]
˜α = α +
X
xi
˜β = β +
X
(1 −xi).

Problems
27
3.10 Prove the conjugate relation
I
Y
i=1
Catxi[λ1...K] · Dirλ1...K[α1...K] = κ · Dirλ1...K[˜α1...K],
where
˜κ =
Γ[PK
j=1 αj]
Γ[I + PK
j=1 αj]
.
QK
j=1 Γ[αj + Nj]
QK
j=1 Γ[αj]
˜α1...K = [α1 + N1,α2 + N2,...,αK + NK].
and Nk is the total number of times that the variable took the value k.
3.11 Show that the conjugate relation between the normal and normal inverse gamma is given by
I
Y
i=1
Normxi[µ,σ2] · NormInvGamµ,σ2[α,β,γ,δ] = κ · NormInvGamµ,σ2[˜α, ˜β, ˜γ, ˜δ],
where
κ =
1
(2π)I/2
√γβα
√˜γ ˜β ˜α
Γ[˜α]
Γ[α]
˜α = α + I/2
˜β =
P
i x2
i
2
+ β + γδ2
2
−(γδ + P
i xi)2
2(γ + I)
˜γ = γ + I
˜δ = γδ + P
i xi
γ + I
.
3.12 Show that the conjugate relationship between the multivariate normal and the normal inverse
Wishart is given by
I
Y
i=1
Normxi [µ,Σ] · NorIWisµ,Σ [α,Ψ,γ,δ] = κ · NorIWis
h
˜α, ˜Ψ, ˜γ,˜δ
i
,
where
κ =
1
πID/2
γD/2
˜γD/2
Ψα/2
˜Ψ
˜α/2
ΓD[˜α/2]
ΓD[α/2]
˜α = α + I
˜Ψ = Ψ + γδδT +
I
X
i=1
xixT
i −
1
(γ + I)
 
γδ +
I
X
i=1
xi
! 
γδ +
I
X
i=1
xi
!T
˜γ = γ + I
˜δ = γδ + PI
i=1 xi
γ + I
.
You may need to use the relation Tr

zzT A−1
= zT A−1z.

Chapter 4
Fitting probability models
This chapter concerns ﬁtting probability models to data {xi}I
i=1. This process is referred
to as learning because we learn about the parameters θ of the model.1 It also concerns
calculating the probability of a new datum x∗under the resulting model. This is known as
evaluating the predictive distribution. We consider three methods: maximum likelihood,
maximum a posteriori, and the Bayesian approach.
4.1
Maximum likelihood
As the name suggests, the maximum likelihood (ML) method ﬁnds the set of parameters
ˆθ under which the data {xi}I
i=1 are most likely. To calculate the likelihood function
Pr(xi|θ) at a single data point xi, we simply evaluate the probability density function at
xi. Assuming each data point was drawn independently from the distribution, the likeli-
hood function Pr(x1...I|θ) for a set of points is the product of the individual likelihoods.
Hence, the ML estimate of the parameters is
ˆθ = argmax
θ
[Pr(x1...I|θ)]
= argmax
θ
" IY
i=1
Pr(xi|θ)
#
,
(4.1)
where argmaxθ f[θ] returns the value of θ that maximizes the argument f[θ].
To evaluate the predictive distribution for a new data point x∗(compute the prob-
ability that x∗belongs to the ﬁtted model), we simply evaluate the probability density
function Pr(x∗|ˆθ) using the ML ﬁtted parameters ˆθ.
4.2
Maximum a posteriori
In maximum a posteriori (MAP) ﬁtting, we introduce prior information about the param-
eters θ. From previous experience we may know something about the possible parameter
values. For example, in a time-sequence the values of the parameters at time t tell us a
1Here we adopt the notation θ to represent a generic set of parameters when we have not speciﬁed the
particular probability model.

4.3
The Bayesian approach
29
lot about the possible values at time t + 1, and this information would be encoded in the
prior distribution.
As the name suggests, maximum a posteriori estimation maximizes the posterior
probability Pr(θ|x1...I) of the parameters
ˆθ = argmax
θ
[Pr(θ|x1...I)]
= argmax
θ
Pr(x1...I|θ)Pr(θ)
Pr(x1...I)

= argmax
θ
"QI
i=1 Pr(xi|θ)Pr(θ)
Pr(x1...I)
#
,
(4.2)
where we have used Bayes’ rule between the ﬁrst two lines and subsequently assumed
independence. In fact, we can discard the denominator as it is constant with respect to
the parameters and so does not affect the position of the maximum, and we get
ˆθ = argmax
θ
" IY
i=1
Pr(xi|θ)Pr(θ)
#
.
(4.3)
Comparing this to the maximum likelihood criterion (Equation 4.1), we see that it is
identical except for the additional prior term; maximum likelihood is a special case of
maximum a posteriori where the prior is uninformative.
The predictive density (probability of a new datum x∗under the ﬁtted model) is again
calculated by evaluating the pdf Pr(x∗|ˆθ) using the new parameters.
4.3
The Bayesian approach
In the Bayesian approach we stop trying to estimate single ﬁxed values (point estimates)
of the parameters θ and admit what is obvious; there may be many values of the parame-
ters that are compatible with the data. We compute a probability distribution Pr(θ|x1...I)
over the parameters θ based on data {xi}I
i=1 using Bayes’ rule so that
Pr(θ|x1...I) =
QI
i=1 Pr(xi|θ)Pr(θ)
Pr(x1...I)
.
(4.4)
Evaluating the predictive distribution is more difﬁcult for the Bayesian case since we
have not estimated a single model but have instead found a probability distribution over
possible models. Hence, we calculate
Pr(x∗|x1...I) =
Z
Pr(x∗|θ)Pr(θ|x1...I) dθ,
(4.5)
which can be interpreted as follows: the term Pr(x∗|θ) is the prediction for a given value
of θ. So, the integral can be thought of as a weighted sum of the predictions given by
different parameters θ, where the weighting is determined by the posterior probability
distribution Pr(θ|x1...I) over the parameters (representing our conﬁdence that different
parameter values are correct).
The predictive density calculations for the Bayesian, MAP, and ML cases can be
uniﬁed if we consider the ML and MAP estimates to be special probability distributions

30
4 Fitting probability models
over the parameters where all of the density is at ˆθ. More formally, we can consider them
as delta functions centered at ˆθ. A delta function δ[z] is a function that integrates to one,
and that returns zero everywhere except at z = 0. We can now write
Pr(x∗|x1...I) =
Z
Pr(x∗|θ)δ[θ −ˆθ] dθ
= Pr(x∗|ˆθ),
(4.6)
which is exactly the calculation we originally prescribed:
we simply evaluate the
probability of the data under the model with the estimated parameters.
4.4
Worked example 1: Univariate normal
To illustrate the above ideas, we will consider ﬁtting a univariate normal model to scalar
data {xi}I
i=1. Recall that the univariate normal model has pdf
Pr(x|µ,σ2) = Normx[µ,σ2] =
1
√
2πσ2 exp

−0.5(x −µ)2
σ2

,
(4.7)
and has two parameters, the mean µ and the variance σ2. Let us generate I independent
data points {xi}I
i=1 from a univariate normal with µ = 1 and σ2 = 1. Our goal is to
reestimate these parameters from the data.
4.4.1
Maximum likelihood estimation
The likelihood Pr(x1...I|µ,σ2) of the parameters {µ,σ2} for observed data {xi}I
i=1 is
4.1
computed by evaluating the pdf for each data point separately and taking the product:
Pr(x1...I|µ,σ2) =
IY
i=1
Pr(xi|µ,σ2)
– 
– 
– 
Figure 4.1 Maximum likelihood ﬁtting. The likelihood of the parameters for a single datapoint
is the height of the pdf evaluated at that point (blue vertical lines). The likelihood of a set of
independently sampled data is the product of the individual likelihoods. a) The likelihood for this
normal distribution is low because the large variance means the height of the pdf is low everywhere.
b) The likelihood for this normal distribution is even lower as the left-most datum is very unlikely
under the model. c) The maximum likelihood solution is the set of parameters for which the
likelihood is maximized.

4.4
Worked example 1: Univariate normal
31
Figure 4.2 The likelihood function for a
ﬁxed set of observed data is a function of the
mean µ and variance σ2 parameters. The plot
shows that there are many parameter settings
which might plausibly be responsible for the
ten data points from Figure 4.1. A sensible
choice for the “best” parameter setting is the
maximum likelihood solution (green cross),
which corresponds to the maximum of this
function.
=
IY
i=1
Normxi[µ,σ2]
=
1
(2πσ2)I/2 exp
"
−0.5
I
X
i=1
(xi −µ)2
σ2
#
.
(4.8)
Obviously, the likelihood for some sets of parameters {µ,σ2} will be higher than others
(Figure 4.1), and it is possible to visualize this as a 2D function of the mean µ and
variance σ2 (Figure 4.2). The maximum likelihood solution ˆµ, ˆσ will occur at the peak
of this surface so that
ˆµ, ˆσ2 = argmax
µ,σ2

Pr(x1...I|µ,σ2)

.
(4.9)
In principle we can maximize this by taking the derivative of Equation 4.8 with
respect to µ and σ2, equating the result to zero and solving. In practice, however, the
resulting equations are messy. To simplify things, we work instead with the logarithm
of this expression (the log likelihood, L). Since the logarithm is a monotonic function
(Figure 4.3), the position of the maximum in the transformed function remains the same.
Algebraically, the logarithm turns the product of the likelihoods of the individual data
points into a sum and so decouples the contribution of each. The ML parameters can
now be calculated as
ˆµ, ˆσ2 = argmax
µ,σ2
" I
X
i=1
log

Normxi[µ,σ2]

#
(4.10)
= argmax
µ,σ2
"
−0.5I log[2π] −0.5I logσ2 −0.5
I
X
i=1
(xi −µ)2
σ2
#
.
To maximize, we differentiate this log likelihood L with respect to µ and equate the result
to zero
∂L
∂µ =
I
X
i=1
(xi −µ)
σ2
=
PI
i=1 xi
σ2
−Iµ
σ2 = 0,
(4.11)

32
4 Fitting probability models
Figure 4.3 The logarithm is a monotonic
transformation. If one point is higher than
another, then it will also be higher after
transformation by the logarithmic function.
It follows that if we transform the surface
in Figure 4.2 through the logarithmic func-
tion, the maximum will remain in the same
position.
and rearranging, we see that
ˆµ =
PI
i=1 xi
I
.
(4.12)
By a similar process, the expression for the variance can be shown to be
ˆσ2 =
I
X
i=1
(xi −ˆµ)2
I
.
(4.13)
These expressions are hardly surprising, but the same idea can be used to estimate
parameters in other distributions where the results are less familiar.
Figure 4.1 shows a set of data points and three possible ﬁts to the data. The mean
of the maximum likelihood ﬁt is the mean of the data. The ML ﬁt is neither too narrow
(giving very low probabilities to the furthest data points from the mean) nor too wide
(resulting in a ﬂat distribution and giving low probability to all points).
Least squares ﬁtting
As an aside, we note that many texts discuss ﬁtting in terms of least squares. Consider
ﬁtting just the mean parameter µ of the normal distribution using maximum likelihood.
Manipulating the cost function so that
ˆµ = argmax
µ
"
−0.5I log[2π] −0.5I logσ2 −0.5
I
X
i=1
(xi −µ)2
σ2
#
= argmax
µ
"
−
I
X
i=1
(xi −µ)2
#
= argmin
µ
" I
X
i=1
(xi −µ)2
#
(4.14)
leads to a formulation where we minimize the sum of squares. In other words, least
squares ﬁtting is equivalent to ﬁtting the mean parameter of a normal distribution using
the maximum likelihood method.

4.4
Worked example 1: Univariate normal
33
4.4.2
Maximum a posteriori estimation
Returning to the main thread, we will now demonstrate maximum a posteriori ﬁtting of
4.2
the parameters of the normal distribution. The cost function becomes
ˆµ, ˆσ2 = argmax
µ,σ2
" IY
i=1
Pr(xi|µ,σ2)Pr(µ,σ2)
#
= argmax
µ,σ2
" IY
i=1
Normxi[µ,σ2]NormInvGamµ,σ2[α,β,γ,δ]
#
,
(4.15)
where we have chosen normal inverse gamma prior with parameters α,β,γ,δ (Figure 4.4)
as this is conjugate to the normal distribution. The expression for the prior is
Pr(µ,σ2) =
√γ
σ
√
2π
βα
Γ(α)
 1
σ2
α+1
exp

−2β + γ(δ −µ)2
2σ2

.
(4.16)
The posterior distribution is proportional to the product of the likelihood and the prior
(Figure 4.5), and has the highest density in regions that both agree with the data and were
a priori plausible.
Like the ML case, it is easier to maximize the logarithm of Equation 4.15:
ˆµ, ˆσ2 = argmax
µ,σ2
" I
X
i=1
log[Normxi

µ,σ2]

+log

NormInvGamµ,σ2[α,β,γ,δ]

#
.
(4.17)
Figure 4.4 Prior over normal parameters. a) A normal inverse gamma with α,β,γ = 1 and δ = 0
gives a broad prior distribution over univariate normal parameters. The magenta cross indicates
the peak of this prior distribution. The blue crosses are ﬁve samples randomly drawn from the
distribution. b) The peak and the samples can be visualized by plotting the normal distributions
that they represent.

34
4 Fitting probability models
Figure 4.5 MAP inference for normal parameters. a) The likelihood function is multiplied by b)
the prior probability to give a new function c) that is proportional to the posterior distribution. The
maximum a posteriori (MAP) solution (cyan cross) is found at the peak of the posterior distribution.
It lies between the maximum likelihood (ML) solution (green cross) and the maximum of the prior
distribution (MP, magenta cross).
To ﬁnd the MAP parameters, we substitute in the expressions, differentiate with
respect to µ and σ, equate to zero, and rearrange to give
ˆµ =
PI
i=1 xi + γδ
I + γ
and
ˆσ2 =
PI
i=1(xi −ˆµ)2 + 2β + γ(δ −ˆµ)2
I + 3 + 2α
.
(4.18)
The formula for the mean can be more easily understood if we write it as
ˆµ = Ix + γδ
I + γ .
(4.19)
This is a weighted sum of two terms. The ﬁrst term is the data mean x and is weighted
by the number of training examples I. The second term is δ, the value of µ favored by
the prior, and is weighted by γ.
This gives some insight into the behavior of the MAP estimate (Figure 4.6). With a
large amount of data, the ﬁrst term dominates, and the MAP estimate ˆµ is very close to
the data mean (and the ML estimate). With intermediate amounts of data, ˆµ is a weighted
sum of the prediction from the data and the prediction from the prior. With no data at
all, the estimate is completely governed by the prior. The hyperparameter (parameter of
the prior) γ controls the concentration of the prior with respect to µ and determines the
extent of its inﬂuence. Similar conclusions can be drawn about the MAP estimate of the
variance.
Where there is a single data point (Figure 4.6e–f), the data tells us nothing about
the variance and the maximum likelihood estimate ˆσ2 is actually zero; the best ﬁt is
an inﬁnitely thin and inﬁnitely tall normal distribution centered on the one data point.
This is unrealistic, not least because it accords the datum an inﬁnite likelihood. How-
ever, MAP estimation is still valid as the prior ensures that sensible parameter values are
chosen.

4.4
Worked example 1: Univariate normal
35
d
d
d
1
Figure 4.6 Maximum a posteriori estimation. a) Posterior distribution over parameters µ and σ2.
MAP solution (cyan cross) lies between ML (green cross) and the peak of the prior (purple cross).
b) Normal distributions corresponding to MAP solution, ML solution and peak of prior. c–d) With
fewer data points, the prior has a greater effect on the ﬁnal solution. e–f) With only one data
point, the maximum likelihood solution cannot be computed (you cannot calculate the variance of
a single point). However, the MAP solution can still be calculated.
4.4.3
The Bayesian approach
In the Bayesian approach, we calculate a posterior distribution Pr(µ,σ2|x1...I) over
4.3
possible parameter values using Bayes’ rule,
Pr(µ,σ2|x1...I) =
QI
i=1 Pr(xi|µ,σ2)Pr(µ,σ2)
Pr(x1...I)
=
QI
i=1 Normxi[µ,σ2]NormInvGamµ,σ2[α,β,γ,δ]
Pr(x1...I)
= κNormInvGamµ,σ2[˜α, ˜β,˜γ, ˜δ]
Pr(x1...I)
,
(4.20)
where we have used the conjugate relationship between likelihood and prior (Section 3.9)
and κ is the associated constant. The product of the normal likelihood and normal inverse
gamma prior creates a posterior over µ and σ2, which is a new normal inverse gamma
distribution and can be shown to have parameters
˜α = α + I/2,
˜γ = γ + I
˜δ = γδ + P
i xi
γ + I
˜β =
P
i x2
i
2
+ β + γδ2
2 −(γδ + P
i xi)2
2(γ + I)
.
(4.21)

36
4 Fitting probability models
Figure 4.7 Bayesian predictions. a) Posterior probability distribution over parameters. b) Sam-
ples from posterior probability distribution correspond to normal distributions. c) The predictive
distribution for the Bayesian case is the average of an inﬁnite set of samples. Alternately, we can
think of choosing the parameters from a uniform distribution and computing a weighted average
where the weights correspond to the posterior distribution.
Note that the posterior (left-hand side of Equation 4.20) must be a valid probabil-
ity distribution and sum to one, so the constant κ from the conjugate product and the
denominator from the right-hand side must exactly cancel to give
Pr(µ,σ2|x1...I) = NormInvGamµ,σ2[˜α, ˜β,˜γ, ˜δ].
(4.22)
Now we see the major advantage of using a conjugate prior: we are guaranteed a closed
form expression for the posterior distribution over the parameters.
This posterior distribution represents the relative plausibility of various parameter
settings µ and σ2 having created the data. At the peak of the distribution is the MAP
estimate, but there are many other plausible conﬁgurations (Figure 4.6).
When data are plentiful (Figure 4.6a), the parameters are well speciﬁed, and the
probability distribution is concentrated. In this case, placing all of the probability mass
at the MAP estimate is a good approximation to the posterior. However, when data are
scarce (Figure 4.6c), many possible parameters might have explained the data and the
posterior is broad. In this case approximation with a point mass is inadequate.
Predictive density
For the maximum likelihood and MAP estimates, we evaluate the predictive density
(probability that a new data point x∗belongs to the same model) by simply evaluat-
ing the normal pdf with the estimated parameters. For the Bayesian case, we compute a
weighted average of the predictions for each possible parameter set, where the weighting
is given by the posterior distribution over parameters (Figures 4.6a–c and 4.7),
Pr(x∗|x1...I) =
Z Z
Pr(x∗|µ,σ2)Pr(µ,σ2|x1...I) dµdσ
(4.23)
=
Z Z
Normx∗[µ,σ2]NormInvGamµ,σ2[˜α, ˜β,˜γ, ˜δ] dµdσ
=
Z Z
κ(x∗, ˜α, ˜β,˜γ, ˜δ)NormInvGamµ,σ2[˘α, ˘β,˘γ, ˘δ] dµdσ.
Here we have used the conjugate relation for a second time. The integral contains a
constant with respect to µ and σ2 multiplied by a probability distribution. Taking the

4.4
Worked example 1: Univariate normal
37
Figure 4.8 a–c) Predictive densities for MAP and Bayesian approaches with 50, 5, and 1 training
examples. As the training data decreases, the Bayesian prediction becomes less certain but the
MAP prediction is erroneously overconﬁdent. d–f) This effect is even more clear on a log scale.
constant outside the integral, we get
Pr(x∗|x1...I) = κ(x∗, ˜α, ˜β,˜γ, ˜δ)
Z Z
NormInvGamµ,σ2[˘α, ˘β,˘γ, ˘δ] dµdσ
= κ(x∗, ˜α, ˜β,˜γ, ˜δ),
(4.24)
which follows because the integral of a pdf is one. It can be shown that the constant is
given by
κ(x∗, ˜α, ˜β,˜γ, ˜δ) =
1
√
2π
√˜γ ˜β ˜α
√˘γ ˘β ˘α
Γ[˘α]
Γ[˜α],
(4.25)
where
˘α = ˜α + 1/2,
˘γ = ˜γ + 1
˘β = x∗2
2 + ˜β + ˜γ˜δ2
2 −(˜γ˜δ + x∗)2
2(˜γ + 1) .
(4.26)
Here, we see the second advantage of using the conjugate prior; it means that the integral
can be computed, and so we get a nice closed form expression for the predictive density.
Figure 4.8 shows the predictive distribution for the Bayesian and MAP cases, for
varying amounts of training data. With plenty of training data, they are quite similar but
as the amount of data decreases, the Bayesian predictive distribution has a signiﬁcantly
longer tail. This is typical of Bayesian solutions: they are more moderate (less certain)
in their predictions. In the MAP case, erroneously committing to a single estimate of µ
and σ2 causes overconﬁdence in our future predictions.

38
4 Fitting probability models
Figure 4.9 a) Categorical probability dis-
tribution over six discrete values with
parameters {λk}6
k=1 where P6
k=1 λk = 1.
This could be the relative probability of a
biased die landing on its six sides. b) Fif-
teen observations {xi}I
i=1 randomly sam-
pled from this distribution. We denote the
number of times category k was observed
by Nk so that here the total observations
P6
k=1 Nk = 15.
4.5
Worked example 2: Categorical distribution
As a second example, we consider discrete data {xi}I
i=1 where xi ∈{1,2,...,6}
(Figure 4.9). This could represent observed rolls of a die with unknown bias. We will
describe the data using a categorical distribution (normalized histogram) where
Pr(x = k|λ1...K) = λk.
(4.27)
For the ML and MAP techniques, we estimate the six parameters {λk}6
k=1. For the
Bayesian approach, we compute a probability distribution over the parameters.
4.5.1
Maximum Likelihood
To ﬁnd the maximum likelihood solution, we maximize the product of the likelihoods for
4.4
each individual data point with respect to the parameters λ1...6.
ˆλ1...6 = argmax
λ1...6
" IY
i=1
Pr(xi|λ1...6)
#
s.t.
X
k
λk = 1
= argmax
λ1...6
" IY
i=1
Catxi[λ1...6]
#
s.t.
X
k
λk = 1
= argmax
λ1...6
" 6
Y
k=1
λNk
k
#
s.t.
X
k
λk = 1,
(4.28)
where Nk is the total number of times we observed bin k in the training data. As before,
it is easier to maximize the log probability, and we use the criterion
L =
6
X
k=1
Nk log[λk] + ν
 6
X
k=1
λk −1
!
,
(4.29)
where the second term uses the Lagrange multiplier ν to enforce the constraint on the
parameters P6
k=1 λk = 1. We differentiate L with respect to λk and ν, set the derivatives
equal to zero, and solve for λk to obtain
ˆλk =
Nk
P6
m=1 Nm
.
(4.30)
In other words, λk is the proportion of times that we observed bin k.

4.5
Worked example 2: Categorical distribution
39
4.5.2
Maximum a posteriori
To ﬁnd the maximum a posteriori solution we need to deﬁne a prior. We choose the
4.5
Dirichlet distribution as it is conjugate to the categorical likelihood. This prior over the
six categorical parameters is hard to visualize but samples can be drawn and examined
(Figure 4.10a–e). The MAP solution is given by
ˆλ1...6 = argmax
λ1...6
" IY
i=1
Pr(xi|λ1...6)Pr(λ1...6)
#
= argmax
λ1...6
" IY
i=1
Catxi[λ1...6]Dirλ1...6[α1...6]
#
= argmax
λ1...6
" 6
Y
k=1
λNk
k
6
Y
k=1
λαk−1
#
= argmax
λ1...6
" 6
Y
k=1
λNk+αk−1
k
#
.
(4.31)
which is again subject to the constraint that P6
k=1 λk = 1. As in the maximum likelihood
case, this constraint is enforced using a Lagrange multiplier. The MAP estimate of the
parameters can be shown to be
ˆλk =
Nk + αk −1
P6
m=1(Nm + αm −1)
,
(4.32)
where Nk is the number of times that observation k occurred in the training data. Note
that if all the values αk are set to one, the prior is uniform and this expression reverts to
the maximum likelihood solution (Equation 4.30).
4.5.3
Bayesian Approach
In the Bayesian approach, we calculate a posterior over the parameters
4.6
Pr(λ1 ...λ6|x1...I) =
QI
i=1 Pr(xi|λ1...6)Pr(λ1...6)
Pr(x1...I)
=
QI
i=1 Catxi[λ1...6]Dirλ1...6[α1...6]
Pr(x1...I)
= κ(α1...6,x1...I)Dirλ1...6[˜α1...6]
Pr(x1...I)
= Dirλ1...6[˜α1...6],
(4.33)
where ˜αk = Nk + αk. We have again exploited the conjugate relationship to yield a
posterior distribution with the same form as the prior. The constant κ must again cancel
with the denominator to ensure a valid probability distribution on the left-hand side.
Samples from this distribution are shown in Figure 4.10f–j.
Predictive Density
For the ML and MAP estimates we compute the predictive density (probability that a
new data point x∗belongs to the same model) by simply evaluating the categorical pdf

40
4 Fitting probability models
Figure 4.10 a–e) Five samples drawn from Dirichlet prior with hyperparameters α1...6 = 1. This
deﬁnes a uniform prior, so each sample looks like a random unstructured probability distribution.
f–j) Five samples from Dirichlet posterior. The distribution favors histograms where bin three is
larger and bin four is small as suggested by the data.
with the estimated parameters. With the uniform prior (α1...6 = 1) the MAP and ML pre-
dictions are identical (Figure 4.11a) and both are exactly proportional to the frequencies
of the observed data.
For the Bayesian case, we compute a weighted average of the predictions for each
possible parameter set, where the weighting is given by the posterior distribution over
parameters so that
Pr(x∗|x1...I) =
Z
Pr(x∗|λ1...6)Pr(λ1...6|x1...I) dλ1...6
=
Z
Catx∗[λ1...6]Dirλ1...6[˜α1...6] dλ1...6
=
Z
κ(x∗, ˜α1...6)Dirλ1...6[˘α1...6] dλ1...6
= κ(x∗, ˜α1...6).
(4.34)
Here, we have again exploited the conjugate relationship to yield a constant multiplied
by a probability distribution and the integral is simply the constant as the integral of the
pdf is one. For this case, it can be shown that
Pr(x∗= k|x1...I) = κ(x∗, ˜α1...6) =
Nk + ˜αk
P6
j=1(Nj + ˜αj)
.
(4.35)
This is illustrated in Figure 4.11b. It is notable that once more the Bayesian predictive
density is less conﬁdent than the ML/MAP solutions. In particular, it does not allot zero
probability to observing x∗= 4 despite the fact that this value was never observed in the
training data. This is sensible; just because we have not drawn a 4 in 15 observations

Problems
41
Figure 4.11 Predictive distributions with
α1...6 = 1 for a) maximum likelihood /
maximum a posteriori approaches and b)
Bayesian approach.
The ML/MAP app-
roaches predict the same distribution that
exactly follows the data frequencies.
The
Bayesian approach predicts a more moder-
ate distribution and allots some probability
to the case x = 4 despite having seen no
training examples in this category.
does not imply that it is inconceivable that we will ever see one. We may have just been
unlucky. The Bayesian approach takes this into account and allots this category a small
amount of probability.
Summary
We presented three ways to ﬁt a probability distribution to data and to predict the proba-
bility of new points. Of the three methods discussed, the Bayesian approach is the most
desirable. Here it is not necessary to ﬁnd a point estimate of the (uncertain) parameters,
and so errors are not introduced because this point estimate is inaccurate.
However, the Bayesian approach is only tractable when we have a conjugate
prior, which makes it easy to calculate the posterior distribution over the parameters
Pr(θ|x1...I) and also to evaluate the integral in the predictive density. When this is not
the case, we will usually have to rely on maximum a posteriori estimates. Maximum like-
lihood estimates can be thought of as a special case of maximum a posteriori estimates
in which the prior is uninformative.
Notes
For more information about the Bayesian approach to ﬁtting distributions consult chapter 3 of
Gelman et al. (2004). More information about Bayesian model selection (Problem 4.6), including
an impassioned argument for its superiority as a method of hypothesis testing can be found in
Mackay (2003).
Problems
4.1 Show that the maximum likelihood solution for the variance σ2 of the normal distribution is
given by
σ2 =
I
X
i=1
(xi −ˆµ)2
I
.
4.2 Show that the MAP solution for the mean µ and variance σ2 of the normal distribution are
given by
ˆµ =
PI
i=1 xi + γδ
I + γ
and
ˆ
σ2 =
PI
i=1(xi −ˆµ)2 + 2β + γ(δ −ˆµ)2
I + 3 + 2α
,

42
4 Fitting probability models
when we use the conjugate normal-scaled inverse gamma prior
Pr(µ,σ2) =
√γ
σ
√
2π
βα
Γ(α)
 1
σ2
α+1
exp

−2β + γ(δ −µ)2
2σ2

.
4.3 Taking Equation 4.29 as a starting point, show that the maximum likelihood parameters for
the categorical distribution are given by
ˆλk =
Nk
P6
m=1 Nm
,
where Nk is the number of times that category K was observed in the training data.
4.4 Show that the MAP estimate for the parameters {λ}K
k=1 of the categorical distribution is
given by
ˆλk =
Nk + αk −1
P6
m=1(Nm + αm −1)
,
under the assumption of a Dirichlet prior with hyperparameters {αk}K
k=1. The terms Nk again
indicate the number of times that category k was observed in the training data.
4.5 The denominator of Bayes’ rule
Pr(x1...I) =
Z
I
Y
i=1
Pr(xi|θ)Pr(θ) dθ
is known as the evidence. It is a measure of how well the distribution ﬁts regardless of the particular
values of the parameters. Find an expression for the evidence term for (i) the normal distribution
and (ii) the categorical distribution assuming conjugate priors in each case.
4.6
The evidence term can be used to compare models.
Consider two sets of data S1 =
{0.1,−0.5,0.2,0.7} and S2 = {1.1,2.0,1.4,2.3}. Let us pose the question of whether these two
data sets came from the same normal distribution or from two different normal distributions.
Let model M1 denote the case where all of the data comes from the one normal distribution. The
evidence for this model is
Pr(S1 ∪S2|M1) =
Z
Y
i∈S1∪S2
Pr(xi|θ)Pr(θ) dθ,
where θ = {µ,σ2} contains the parameters of this normal distribution. Similarly, we will let M2
denote the case where the two sets of data belong to different normal distributions
Pr(S1 ∪S2|M2) =
Z
Y
i∈S1
Pr(xi|θ1)Pr(θ1) dθ1
Z
Y
i∈S2
Pr(xi|θ2)Pr(θ2) dθ2,
where θ1 = {µ1,σ2
1} and θ2 = {µ2,σ2
2}.
Now it is possible to compare the probability of the data under each of these two models using
Bayes’ rule
Pr(M1|S1 ∪S2) =
Pr(S1 ∪S2|M1)Pr(M1)
P2
n=1 Pr(S1 ∪S2|Mn)Pr(Mn)
Use this expression to compute the posterior probability that the two datasets came from the same
underlying normal distribution. You may assume normal-scaled inverse gamma priors over θ, θ1,
and θ2 with parameters α = 1,β = 1,γ = 1,δ = 0.

Problems
43
Note that this is (roughly) a Bayesian version of the two-sample t-test, but it is much neater – we get
a posterior probability distribution over the two hypotheses rather than the potentially misleading
p value of the t-test. The process of comparing evidence terms in this way is known as Bayesian
model selection or the evidence framework. It is rather clever in that two normal distributions ﬁtted
with maximum likelihood will always explain the data better than one; the additional parameters
simply make the model more ﬂexible. However because we have marginalized these parameters
away here, it is valid to compare these models in the Bayesian case.
4.7 In the Bernoulli distribution, the likelihood Pr(x1...I|λ) of the data {xi}I
i=1 given parameter
λ where xi ∈{0,1} is
Pr(x1...I|λ) =
I
Y
i=1
λxi(1 −λ)1−xi.
Find an expression for the maximum likelihood estimate of the parameter λ.
4.8 Find an expression for the MAP estimate of the Bernoulli parameter λ (see Problem 4.7)
assuming a beta distributed prior
Pr(λ) = Betaλ[α,β].
4.9 Now consider the Bayesian approach to ﬁtting Bernoulli data, using a beta distributed prior.
Find expressions for (i) the posterior probability distribution over the Bernoulli parameters given
observed data {xi}I
i=1 and (ii) the predictive distribution for new data x∗.
4.10 Staying with the Bernoulli distribution, consider observing data 0,0,0,0 from four trials.
Assuming a uniform beta prior (α = 1,β = 1), compute the predictive distribution using the (i)
maximum likelihood, (ii) maximum a posteriori, and (iii) Bayesian approaches. Comment on the
results.

Chapter 5
The normal distribution
The most common representation for uncertainty in machine vision is the multivariate
normal distribution. We devote this chapter to exploring its main properties, which will
be used extensively throughout the rest of the book.
Recall from Chapter 3 that the multivariate normal distribution has two parameters:
the mean µ and covariance Σ. The mean µ is a D×1 vector that describes the position
of the distribution. The covariance Σ is a symmetric D×D positive deﬁnite matrix
(implying that zT Σz is positive for any real vector z) and describes the shape of the
distribution. The probability density function is
Pr(x) =
1
(2π)D/2|Σ|1/2 exp

−0.5(x −µ)T Σ−1(x −µ)

,
(5.1)
or for short
Pr(x) = Normx [µ,Σ].
(5.2)
5.1
Types of covariance matrix
Covariance matrices in multivariate normals take three forms, termed spherical, diago-
nal, and full covariances. For the two-dimensional (bivariate) case, these are
Σspher =
σ2
0
0
σ2

Σdiag =
σ2
1
0
0
σ2
2

Σfull =
σ2
11
σ2
12
σ2
21
σ2
22

.
(5.3)
The spherical covariance matrix is a positive multiple of the identity matrix and so
has the same value on all of the diagonal elements and zeros elsewhere. In the diag-
onal covariance matrix, each value on the diagonal has a different positive value. The
full covariance matrix can have nonzero elements everywhere although the matrix is still
constrained to be symmetric and positive deﬁnite so for the 2D example, σ2
12 = σ2
21.
For the bivariate case (Figure 5.1), spherical covariances produce circular iso-density
contours. Diagonal covariances produce ellipsoidal iso-contours that are aligned with
the coordinate axes. Full covariances also produce ellipsoidal iso-density contours, but
these may now take an arbitrary orientation. More generally, in D dimensions, spherical
covariances produce iso-contours that are D-spheres, diagonal covariances produce iso-
contours that are D-dimensional ellipsoids aligned with the coordinate axes, and full
covariances produce iso-contour that are D-dimensional ellipsoids in general position.

5.2
Decomposition of covariance
45
Figure 5.1 Covariance matrices take three forms.
a–b) Spherical covariance matrices are
multiples of the identity. The variables are independent and the iso-probability surfaces are hyper-
spheres. c–d) Diagonal covariance matrices permit different nonzero entries on the diagonal, but
have zero entries elsewhere. The variables are independent, but scaled differently and the iso-
probability surfaces are hyperellipsoids (ellipses in 2D) whose principal axes are aligned to the
coordinate axes. e–f) Full covariance matrices are symmetric and positive deﬁnite. Variables are
dependent, and iso-probability surfaces are ellipsoids that are not aligned in any special way.
When the covariance is spherical or diagonal, the individual variables are indepen-
dent. For example, for the bivariate diagonal case with zero mean, we have
Pr(x1,x2) =
1
2π
p
|Σ|
exp

−0.5
 x1
x2

Σ−1

x1
x2

=
1
2πσ1σ2
exp

−0.5
 x1
x2
σ−2
1
0
0
σ−2
2
x1
x2

=
1
p
2πσ2
1
exp

−x2
1
2σ2
1

1
p
2πσ2
2
exp

−x2
2
2σ2
2

= Pr(x1)Pr(x2).
(5.4)
5.2
Decomposition of covariance
We can use the foregoing geometrical intuitions to decompose the full covariance matrix
Σfull. Given a normal distribution with mean zero and a full covariance matrix, we know
that the iso-contours take an ellipsoidal form with the major and minor axes at arbitrary
orientations.

46
5 The normal distribution
Figure 5.2 Decomposition of full covari-
ance.
For every bivariate normal distri-
bution in variables x1 and x2 with full
covariance matrix, there exists a coor-
dinate system with variables x′
1 and x′
2
where the covariance is diagonal: the ellip-
soidal iso-contours align with the coor-
dinate axes x′
1 and x′
2 in this canoni-
cal coordinate frame.
The two frames
of reference are related by the rotation
matrix R which maps (x′
1,x′
2) to (x1,x2).
From this it follows (see text) that any
covariance matrix Σ can be broken down
into the product RT Σ′
diagR of a rota-
tion matrix R and a diagonal covariance
matrix Σ′
diag.
Now consider viewing the distribution in a new coordinate frame where the axes that
are aligned with the axes of the normal (Figure 5.2): in this new frame of reference,
the covariance matrix Σ′
diag will be diagonal. We denote the data vector in the new
coordinate system by x′ = [x′
1,x′
2]T where the frames of reference are related by x′ =
Rx. We can write the probability distribution over x′ as
Pr(x′)=
1
(2π)D/2|Σ′
diag|1/2 exp
h
−0.5x′T Σ′−1
diagx′i
.
(5.5)
We now convert back to the original axes by substituting in x′ = Rx to get
Pr(x) =
1
(2π)D/2|Σ′
diag|1/2 exp
h
−0.5(Rx)T Σ′−1
diagRx
i
=
1
(2π)D/2|RT Σ′
diagR|1/2 exp

−0.5xT (RT Σ′
diagR)−1x

,
(5.6)
where we have used |RT Σ′R| = |RT |.|Σ′|.|R| = 1.|Σ′|.1 = |Σ′|. Equation 5.6 is a
multivariate normal with covariance
Σfull = RT Σ′
diagR.
(5.7)
We conclude that full covariance matrices are expressible as a product of this form
involving a rotation matrix R and a diagonal covariance matrix Σ′
diag. Having under-
stood this, it is possible to retrieve these elements from an arbitrary valid covariance
matrix Σfull by decomposing it in this way using the singular value decomposition.
The matrix R contains the principal directions of the ellipsoid in its columns. The
values on the diagonal of Σ′
diag encode the variance (and hence the width of the dis-
tribution) along each of these axes. Hence we can use the results of the singular value
decomposition to answer questions about which directions in space are most and least
certain.

5.4
Marginal distributions
47
Figure 5.3 Transformation of normal variables. a) If x has a multivariate normal pdf and we apply
a linear transformation to create new variable y = Ax + b, then b) the distribution of y is also
multivariate normal. The mean and covariance of y depend on the original mean and covariance
of x and the parameters A and b.
5.3
Linear transformations of variables
The form of the multivariate normal is preserved under linear transformations
y = Ax + b (Figure 5.3). If the original distribution was
Pr(x) = Normx [µ,Σ],
(5.8)
then the transformed variable y is distributed as
Pr(y) = Normy

Aµ + b,AΣAT 
.
(5.9)
This relationship provides a simple method to draw samples from a normal distribu-
tion with mean µ and covariance Σ. We ﬁrst draw a sample x from a standard normal
distribution (with mean µ = 0 and covariance Σ = I) and then apply the transformation
y = Σ1/2x + µ.
5.4
Marginal distributions
If we marginalize over any subset of random variables in a multivariate normal distribu-
tion, the remaining distribution is also normally distributed (Figure 5.4). If we partition
the original random variable into two parts x = [xT
1 ,xT
2 ]T so that
Pr(x) = Pr

x1
x2

= Normx

µ1
µ2

,

Σ11
ΣT
21
Σ21
Σ22

,
(5.10)
then
Pr(x1) = Normx1 [µ1,Σ11]
Pr(x2) = Normx2 [µ2,Σ22].
(5.11)
So, to ﬁnd the mean and covariance of the marginal distribution of a subset of variables,
we extract the relevant entries from the original mean and covariance.

48
5 The normal distribution
Figure 5.4 The marginal distribution of
any subset of variables in a normal distri-
bution is also normally distributed. In other
words, if we sum over the distribution in
any direction, the remaining quantity is also
normally distributed. To ﬁnd the mean and
the covariance of the new distribution, we
can simply extract the relevant entries from
the original mean and covariance matrix.
5.5
Conditional distributions
If the variable x is distributed as a multivariate normal, then the conditional distribution
of a subset of variables x1 given known values for the remaining variables x2 is also
distributed as a multivariate normal (Figure 5.5). If
Pr(x) = Pr
x1
x2

= Normx
µ1
µ2

,

Σ11
ΣT
21
Σ21
Σ22

,
(5.12)
then the conditional distributions are
Pr(x1|x2=x∗
2) = Normx1
h
µ1+ΣT
21Σ−1
22 (x∗
2 −µ2),Σ11−ΣT
21Σ−1
22 Σ21
i
(5.13)
Pr(x2|x1=x∗
1) = Normx2
h
µ2+Σ21Σ−1
11 (x∗
1−µ1),Σ22−Σ21Σ−1
11 ΣT
21
i
.
5.6
Product of two normals
The product of two normal distributions is proportional to a third normal distribution
(Figure 5.6). If the two original distributions have means a and b and covariances A and
B, respectively, then we ﬁnd that
Normx[a,A]Normx[b,B] =
(5.14)
κ · Normx
h A−1+B−1−1 (A−1a+B−1b),
 A−1+B−1−1i
,
where the constant κ is itself a normal distribution,
κ = Norma[b,A + B] = Normb[a,A + B].
(5.15)

5.6
Product of two normals
49
Figure 5.5 Conditional distributions of multivariate normal. a) If we take any multivariate normal
distribution, ﬁx a subset of the variables, and look at the distribution of the remaining variables, this
distribution will also take the form of a normal. The mean of this new normal depends on the values
that we ﬁxed the subset to, but the covariance is always the same. b) If the original multivariate
normal has spherical or diagonal covariance, both the mean and covariance of the resulting normal
distributions are the same, regardless of the value we conditioned on: these forms of covariance
matrix imply independence between the constituent variables.
Figure 5.6 The product of any two normals
N1 and N2 is proportional to a third normal
distribution, with a mean between the two
original means and a variance that is smaller
than either of the original distributions.
5.6.1
Self-conjugacy
The preceding property can be used to demonstrate that the normal distribution is
self-conjugate with respect to its mean µ. Consider taking a product of a normal dis-
tribution over data x and a second normal distribution over the mean vector µ of the ﬁrst
distribution. It is easy to show from Equation 5.14 that
Normx[µ,Σ]Normµ[µp,Σp] = Normµ[x,Σ]Normµ[µp,Σp]
= κ · Normµ[˜µ, ˜Σ],
(5.16)

50
5 The normal distribution
Figure 5.7 a) Consider a normal distribution in x whose variance σ2 is constant, but whose
mean is a linear function ay + b of a second variable y. b) This is mathematically equivalent to
a constant κ times a normal distribution in y whose variance σ′2 is constant and whose mean is a
linear function a′x + b′ of x.
which is the deﬁnition of conjugacy (see Section 3.9). The new parameters ˜µ and ˜Σ
are determined from Equation 5.14. This analysis assumes that the variance Σ is being
treated as a ﬁxed quantity. If we also treat this as uncertain, then we must use a normal
inverse Wishart prior.
5.7
Change of variable
Consider a normal distribution in variable x whose mean is a linear function Ay + b of
a second variable y. We can reexpress this in terms of a normal distribution in y, which
is a linear function A′x + b′ of x so that
Normx[Ay + b,Σ] = κ · Normy[A′x + b′,Σ′],
(5.17)
where κ is a constant and the new parameters are given by
Σ′ = (AT Σ−1A)−1
A′ = (AT Σ−1A)−1AT Σ−1
b′ = −(AT Σ−1A)−1AT Σ−1b.
(5.18)
This relationship is mathematically opaque, but it is easy to understand visually when
x and y are scalars (Figure 5.7). It is often used in the context of Bayes’ rule where our
goal is to move from Pr(x|y) to Pr(y|x).
Summary
In this chapter we have presented a number of properties of the multivariate normal distri-
bution. The most important of these relates to the marginal and conditional distributions:
when we marginalize or take the conditional distribution of a normal with respect to a
subset of variables, the result is another normal. These properties are exploited in many
vision algorithms.

Problems
51
Notes
The normal distribution has further interesting properties which are not discussed because they are
not relevant for this book. For example, the convolution of a normal distribution with a second
normal distribution produces a function that is proportional to a third normal, and the Fourier
transform of a normal proﬁle creates a normal proﬁle in frequency space. For a different treatment
of this topic the interested reader can consult chapter 2 of Bishop (2006).
Problems
5.1 Consider a multivariate normal distribution in variable x with mean µ and covariance Σ.
Show that if we make the linear transformation y = Ax + b, then the transformed variable y is
distributed as
Pr(y) = Normy
h
Aµ + b,AΣAT i
.
5.2 Show that we can convert a normal distribution with mean µ and covariance Σ to a new
distribution with mean 0 and covariance I using the linear transformation y = Ax + b where
A = Σ−1/2
b = −Σ−1/2µ.
This is known as the whitening transformation.
5.3 Show that for multivariate normal distribution
Pr(x) = Pr
x1
x2

= Normx
µ1
µ2

,

Σ11
ΣT
21
Σ21
Σ22

,
the marginal distribution in x1 is
Pr(x1) = Normx1 [µ1,Σ11].
Hint: Apply the transformation y = [I,0]x.
5.4 The Schur complement identity states that inverse of a matrix in terms of its subblocks is
A
B
C
D
−1
=

(A −BD−1C)−1
−(A −BD−1C)−1BD−1
−D−1C(A −BD−1C)−1
D−1 + D−1C(A −BD−1C)−1BD−1

.
Show that this relation is true.
5.5 Prove the conditional distribution property for the normal distribution: if
Pr(x) = Pr
x1
x2

= Normx
µ1
µ2

,

Σ11
ΣT
12
Σ12
Σ22

,
then
Pr(x1|x2) = Normx1

µ1 + ΣT
12Σ−1
22 (x2 −µ2),Σ11 −ΣT
12Σ−1
22 Σ12

.
Hint: Use Schur’s complement.
5.6 Use the conditional probability relation for the normal distribution to show that the conditional
distribution Pr(x1|x2 = k) is the same for all k when the covariance is diagonal and the variables
are independent (see Figure 5.5b).

52
5 The normal distribution
5.7 Show that
Normx[a,A]Normx[b,B] ∝Normx[(A−1 + B−1)−1(A−1a + B−1b),(A−1 + B−1)−1].
5.8 For the 1D case, show that when we take the product of the two normal distributions with
means µ1,µ2 and variances σ2
1,σ2
2, the new mean lies between the original two means and the new
variance is smaller than either of the original variances.
5.9 Show that the constant of proportionality κ in the product relation in Problem 5.7 is also a
normal distribution where
κ = Norma[b,A + B].
5.10 Prove the change of variable relation. Show that
Normx[Ay + b,Σ] = κ · Normy[A′x + b′,Σ′],
and derive expressions for κ, A′, b′, and Σ′. Hint: Write out the terms in the original exponential,
extract quadratic and linear terms in y, and complete the square.

Part II
Machine learning for
machine vision

In the second part of this book (chapters 6–9), we treat vision as a machine learning prob-
lem and disregard everything we know about the creation of the image. For example, we
will not exploit our understanding of perspective projection or light transport. Instead,
we treat vision as pattern recognition; we interpret new image data based on prior experi-
ence of images in which the contents were known. We divide this process into two parts:
in learning we model the relationship between the image data and the scene content. In
inference, we exploit this relationship to predict the contents of new images.
To abandon useful knowledge about image creation may seem odd, but the logic
is twofold. First, these same learning and inference techniques will also underpin our
algorithms when image formation is taken into account. Second, it is possible to achieve
a great deal with a pure learning approach to vision. For many tasks, knowledge of the
image formation process is genuinely unnecessary.
The structure of Part II is as follows. In Chapter 6 we present a taxonomy of models
that relate the measured image data and the actual scene content. In particular, we dis-
tinguish between generative models and discriminative models. For generative models,
we build a probability model of the data and parameterize it by the scene content. For
discriminative models, we build a probability model of the scene content and parameter-
ize it by the data. In the subsequent three chapters, we elaborate our discussion of these
models.
In Chapter 7 we consider generative models. In particular, we discuss how to use
hidden variables to construct complex probability densities over visual data. As exam-
ples, we consider mixtures of Gaussians, t-distributions, and factor analyzers. Together,
these three models allow us to build densities that are multi-modal, robust, and suitable
for modeling high dimensional data.
In Chapter 8 we consider regression models: we aim to estimate a continuous quan-
tity from continuous data. For example, we might want to predict the joint angles from
an image of the human body. We start with linear regression and move to more complex
nonlinear methods such as Gaussian process regression and relevance vector regression.
In Chapter 9 we consider classiﬁcation models: here we want to predict a discrete quan-
tity from continuous data. For example, we might want to assign a label to a region of
the image to indicate whether or not a face is present. We start with logistic regression
and work toward more sophisticated methods such as Gaussian process classiﬁcation,
boosting, and classiﬁcation trees.

Chapter 6
Learning and inference
in vision
At an abstract level, the goal of computer vision problems is to use the observed image
data to infer something about the world. For example, we might observe adjacent frames
of a video sequence and infer the camera motion, or we might observe a facial image and
infer the identity.
The aim of this chapter is to describe a mathematical framework for solving this type
of problem and to organize the resulting models into useful subgroups, which will be
explored in subsequent chapters.
6.1
Computer vision problems
In vision problems, we take visual data x and use them to infer the state of the world w.
The world state w may be continuous (the 3D pose of a body model) or discrete (the
presence or absence of a particular object). When the state is continuous, we call this
inference process regression. When the state is discrete, we call it classiﬁcation.
Unfortunately, the measurements x may be compatible with more than one world
state w. The measurement process is noisy, and there is inherent ambiguity in visual data:
a lump of coal viewed under bright light may produce the same luminance measurements
as white paper in dim light. Similarly, a small object seen close-up may produce the same
image as a larger object that is further away.
In the face of such ambiguity, the best that we can do is to return the posterior prob-
ability distribution Pr(w|x) over possible states w. This describes everything we know
about the state after observing the visual data. So, a more precise description of an
abstract vision problem is that we wish to take observations x and return the whole
posterior probability distribution Pr(w|x) over world states.
In practice, computing the posterior is not always tractable; we often have to settle
for returning the world state ˆw at the peak of the posterior (the maximum a posteriori
solution). Alternatively, we might draw samples from the posterior and use the collection
of samples as an approximation to the full distribution.

56
6 Learning and inference in vision
6.1.1
Components of the solution
To solve a vision problem of this kind, we need three components.
• We need a model that mathematically relates the visual data x and the world state
w. The model speciﬁes a family of possible relationships between x and w and
the particular relationship is determined by the model parameters θ.
• We need a learning algorithm that allows us to ﬁt the parameters θ using paired
training examples {xi,wi}, where we know both the measurements and the
underlying state.
• We need an inference algorithm that takes a new observation x and uses the model
to return the posterior Pr(w|x,θ) over the world state w. Alternately, it might
return the MAP solution or draw samples from the posterior.
The rest of this book is structured around these components: each chapter focusses on
one model or one family of models, and discusses the associated learning and inference
algorithms.
6.2
Types of model
The ﬁrst and most important component of the solution is the model. Every model
relating the data x to the world w falls into one of two categories. We either
1. Model the contingency of the world state on the data Pr(w|x) or
2. Model the contingency of the data on the world state Pr(x|w).
The ﬁrst type of model is termed discriminative. The second is termed generative; here,
we construct a probability model over the data and this can be used to generate (confab-
ulate) new observations. Let us consider these two types of models in turn and discuss
learning and inference in each.
6.2.1
Model contingency of world on data (discriminative)
To model Pr(w|x), we choose an appropriate form for the distribution Pr(w) over the
world state w and then make the distribution parameters a function of the data x. So if
the world state was continuous, we might model Pr(w) with a normal distribution and
make the mean µ a function of the data x.
The value that this function returns also depends on a set of parameters, θ. Since the
distribution over the state depends on both the data and these parameters, we write it as
Pr(w|x,θ) and refer to it as the posterior distribution.
The goal of the learning algorithm is to ﬁt the parameters θ using paired training
data {xi,wi}I
i=1. This can be done using the maximum likelihood (ML), maximum a
posteriori (MAP), or Bayesian approaches (Chapter 4).
The goal of inference is to ﬁnd a distribution over the possible world states w for a
new observation x. In this case, this is easy: we have already directly constructed an
expression for the posterior distribution Pr(w|x,θ), and we simply evaluate it with the
new data.

6.3
Example 1: Regression
57
6.2.2
Model contingency of data on world (generative)
To model Pr(x|w), we choose the form for the distribution Pr(x) over the data and
make the distribution parameters a function of the world state w. For example, if the data
were discrete and multivalued then we might use a categorical distribution and make the
parameter vector λ a function of the world state w.
The value that this function returns also depends on a set of parameters θ. Since
the distribution Pr(x) now depends on both the world state and these parameters, we
write it as Pr(x|w,θ) and refer to it as the likelihood. The goal of learning is to ﬁt the
parameters θ using paired training examples {xi,wi}I
i=1.
In inference, we aim to compute the posterior distribution Pr(w|x). To this end we
specify a prior Pr(w) over the world state and then use Bayes’ rule,
Pr(w|x) =
Pr(x|w)Pr(w)
R
Pr(x|w)Pr(w)dw.
(6.1)
Summary
We’ve seen that there are two distinct approaches to modeling the relationship between
the world state w and the data x, corresponding to modeling the posterior Pr(w|x) or
the likelihood Pr(x|w).
The two model types result in different approaches to inference. For the discrimina-
tive model, we describe the posterior Pr(w|x) directly and there is no need for further
work. For the generative model, we compute the posterior using Bayes’ rule. This
sometimes results in complex inference algorithms.
To make these ideas concrete, we now consider two toy examples. For each case,
we will investigate using both generative and discriminative models. At this stage, we
won’t present the details of the learning and inference algorithms; these are presented in
subsequent chapters anyway. The goal here is to introduce the main types of model used
in computer vision, in their most simple form.
6.3
Example 1: Regression
Consider the situation where we make a univariate continuous measurement x and use
this to predict a univariate continuous state w. For example, we might predict the distance
to a car in a road scene based on the number of pixels in its silhouette.
6.3.1
Model contingency of world on data (discriminative)
We deﬁne a probability distribution over the world state w and make its parameters con-
tingent on the data x. Since the world state is univariate and continuous, we chose
the univariate normal. We ﬁx the variance, σ2 and make the mean µ a linear function
φ0 + φ1x of the data. So we have
Pr(w|x,θ) = Normw

φ0 + φ1x,σ2
,
(6.2)

58
6 Learning and inference in vision
Figure 6.1 Regression by modeling the posterior Pr(w|x) (discriminative).
a) We model
the world state w as a normal. b) We make the normal parameters a function of the observa-
tions x: The mean is a linear function µ = φ0 + φ1x of the observations, and the variance σ2
is ﬁxed. The learning algorithm ﬁts the parameters θ = {φ0,φ1,σ2} to example training pairs
{xi,wi}I
i=1 (blue dots). In inference, we take a new observation x and compute the posterior
distribution Pr(w|x) over the state.
where θ = {φ0,φ1,σ2} are the unknown parameters of the model (Figure 6.1). This
model is referred to as linear regression.
The learning algorithm estimates the model parameters θ from paired training
examples {xi,wi}I
i=1. For example, in the MAP approach, we seek
ˆθ = argmax
θ
[Pr(θ|w1...I,x1...I)]
= argmax
θ
[Pr(w1...I|x1...I,θ)Pr(θ)]
= argmax
θ
" IY
i=1
Pr(wi|xi,θ)Pr(θ)
#
,
(6.3)
where we have assumed that the I training pairs {xi,wi}I
i=1 are independent, and deﬁned
a suitable prior Pr(θ).
We also need an inference algorithm that takes visual data x and returns the posterior
distribution Pr(w|x,θ). Here this is very simple: we simply evaluate Equation 6.2 using
the data x and the learned parameters ˆθ.
6.3.2
Model the contingency of data on world (generative)
In the generative formulation, we choose a probability distribution over the data x and
make its parameters contingent on the world state w. Since the data are univariate and
continuous, we will model the data as a normal distribution with ﬁxed variance, σ2
and a mean µ that is a linear function φ0 + φ1w of the world state (Figure 6.2) so that
Pr(x|w,θ) = Normx

φ0 + φ1w,σ2
.
(6.4)
We also need a prior Pr(w) over the world states, which might also be normal so
Pr(w) = Normw[µp,σ2
p].
(6.5)

6.3
Example 1: Regression
59
Figure 6.2 Regression by modeling likelihood Pr(x|w) (generative). a) We represent the data
x with a normal distribution. b) We make the normal parameters functions of the world state w.
Here the mean is a linear function µ = φ0 + φ1w of the world state and the variance σ2 is ﬁxed.
The learning algorithm ﬁts the parameters θ = {φ0,φ1,σ2} to example training pairs {xi,wi}I
i=1
(blue dots).
c) We also learn a prior distribution over the world state w (here modeled as a
normal distribution with parameters θp = {µp,σp}). In inference, we take a new datum x and
compute the posterior Pr(w|x) over the state. d) This can be done by computing the joint distri-
bution Pr(x,w) = Pr(x|w)Pr(w) (weighting each row of (b) by the appropriate value from the
prior) and e) normalizing the columns Pr(w|x) = Pr(x,w)/Pr(x). Together these operations
implement Bayes’ rule: Pr(w|x) = Pr(x|w)Pr(w)/Pr(x).
The learning algorithm ﬁts the parameters θ = {φ0,φ1,σ2} using paired training data
{xi,wi}I
i=1 and ﬁts the parameters θp = {µp,σ2
p} using the world states {wi}I
i=1. The
inference algorithm takes a new datum x and returns the posterior Pr(w|x) over the
world state w using Bayes’ rule
Pr(w|x) = Pr(x|w)Pr(w)
Pr(x)
= Pr(x,w)
Pr(x) .
(6.6)
In this case, the posterior can be computed in closed form and is again normally
distributed with ﬁxed variance and a mean that is proportional to the data x.

60
6 Learning and inference in vision
Discussion
We have presented two models that can be used to estimate the world state w from an
observed data example x, based on modeling the posterior Pr(w|x) and the likelihood
Pr(x|w), respectively.
The models were carefully chosen so that they predict exactly the same posterior
P(w|x) over the world state (compare Figures 6.1b, and 6.2e). This is only the case with
maximum likelihood learning: in the MAP approach we would have placed priors on the
parameters, and because each model is parameterized differently, they would in general
have different effects.
6.4
Example 2: Binary classiﬁcation
As a second example, we will consider the case where the observed measurement x is
univariate and continuous, but the world state w is discrete and can take one of two
values. For example, we might wish to classify a pixel as belonging to a skin or non-skin
region based on observing just the red channel.
6.4.1
Model contingency of world on data (discriminative)
We deﬁne a probability distribution over the world state w ∈{0,1} and make its param-
eters contingent on the data x. Since the world state is discrete and binary, we will use
a Bernoulli distribution. This has a single parameter λ, which determines the probability
of success so that Pr(w = 1) = λ.
We make λ a function of the data x, but in doing so we must ensure the constraint
0 ≤λ ≤1 is obeyed. To this end, we form linear function φ0 + φ1x of the data x, which
returns a value in the range [−∞∞]. We then pass the result through a function sig[•]
that maps [−∞∞] to [0 1], so that
Pr(w|x) = Bernw [sig[φ0 + φ1x]] = Bernw

1
1 + exp[−φ0 −φ1x]

.
(6.7)
This produces a sigmoidal dependence of the distribution parameter λ on the data x
(Figure 6.3). The function sig[•] is called the logistic sigmoid. This model is confusingly
termed logistic regression despite being used here for classiﬁcation.
In learning, we aim to ﬁt the parameters θ = {φ0,φ1} from paired training examples
{xi,wi}I
i=1. In inference, we simply substitute in the observed data value x into Equation
6.7 to retrieve the posterior distribution Pr(w|x) over the state.
6.4.2
Model contingency of data on world (generative)
We choose a probability distribution over the data x and make its parameters contingent
on the world state w. Since the data are univariate and continuous, we will choose a
univariate normal and allow the variance σ2 and the mean µ to be functions of the binary
world state w (Figure 6.4) so that the likelihood is
Pr(x|w,θ) = Normx

µw,σ2
w

.
(6.8)

6.4
Example 2: Binary classiﬁcation
61
Figure 6.3 Classiﬁcation by modeling posterior Pr(w|x) (discriminative).
a) We represent
the world state w as a Bernoulli distribution. We make the Bernoulli parameter λ a function
of the observations x. b) To this end, we form a linear function φ0 + φ1x of the observations.
c) The Bernoulli parameter λ = sig[φ0 + φ1x] is formed by passing the linear function through
the logistic sigmoid sig[•] to constrain the value to lie between 0 and 1, giving the characteristic
sigmoid shape (red curve). In learning we ﬁt parameters θ = {φ0,φ1} using example training
pairs {xi,wi}I
i=1. In inference, we take a new datum x and evaluate the posterior Pr(w|x) over
the state.
In practice, this means that we have one set of parameters {µ0,σ2
0} when the state of the
world is w = 0 and a different set {µ1,σ2
1} when the state is w = 1 so
Pr(x|w = 0) = Normx

µ0,σ2
0

Pr(x|w = 1) = Normx

µ1,σ2
1

.
(6.9)
These are referred to as class conditional density functions as they model the density of
the data for each class separately.
We also deﬁne a prior distribution Pr(w) over the world state,
Pr(w) = Bernw[λp],
(6.10)
where λp is the prior probability of observing the state w = 1.
In learning, we ﬁt the parameters θ = {µ0,σ2
0,µ1,σ2
1,λp} using paired training data
{xi,wi}I
i=1. In practice, this consists of ﬁtting the parameters µ0 and σ2
0 of the ﬁrst
class-conditional density function Pr(x|w = 0) from just the data x where the state w
was 0, and the parameters µ1 and σ2
1 of P(x|w = 1) from the data x where the state was
1. We learn the prior parameter λp from the training world states {wi}I
i=1.
The inference algorithm takes new datum x and returns the posterior distribution
Pr(w|x,θ) over the world state w using Bayes’ rule,
Pr(w|x) =
Pr(x|w)Pr(w)
P1
w=0 Pr(x|w)Pr(w)
.
(6.11)
This is very easy to compute; we evaluate the two class-conditional density functions,
weight each by the appropriate prior and normalize so that the two values sum to one.

62
6 Learning and inference in vision
Figure 6.4 Classiﬁcation by modeling the likelihood Pr(x|w) (generative). a) We choose a
normal distribution to represent the data x. b) We make the parameters {µ,σ2} of this normal a
function of the world state w. In practice, this means using one set of mean and variance parameters
when the world state w = 0 and another when w = 1. The learning algorithm ﬁts the parameters
θ = {µ0,µ1,σ2
0,σ2
1} to example training pairs {xi,wi}I
i=1 . c) We also model the prior probability
of the world state w with a Bernoulli distribution with parameter λp. d) In inference, we take a
new datum x and compute the posterior Pr(w|x) over the state using Bayes’ rule.
Discussion
For binary classiﬁcation, there is an asymmetry between the world state, which is dis-
crete, and the measurements, which are continuous. Consequently, the generative and
discriminative models look quite different, and the posteriors over the world state w as a
function of the data x have different shapes (compare Figure 6.3c with Figure 6.4d). For
the discriminative model, this function is by deﬁnition sigmoidal, but for the generative
case it has a more complex form that was implicitly deﬁned by the normal likelihoods.
In general, choosing to model Pr(w|x) or P(x|w) will affect the expressiveness of the
ﬁnal model.

6.5
Which type of model should we use?
63
6.5
Which type of model should we use?
We have established that there are two different types of model that relate the world
state and the observed data. But when should we use each type of model? There is no
deﬁnitive answer to this question, but some considerations are:
• Inference is generally simpler with discriminative models. They directly model
the conditional probability distribution of the world Pr(w|x) given the data. In
contrast, generative models calculate the posterior indirectly via Bayes’ rule, and
sometimes this requires a computationally expensive algorithm.
• Generative methods build probability models Pr(x|w) over the data, whereas dis-
criminative models just build a probability model Pr(w|x) over the world state.
The data (usually an image) are generally of much higher dimension than the world
state (some aspect of a scene), and modeling it is costly. Moreover, there may
be many aspects of the data which do not inﬂuence the state; we might devote
parameters to describing whether data conﬁguration 1 is more likely than data
conﬁguration 2 although they both imply the same world state (Figure 6.5).
• Modeling the likelihood Pr(x|w) mirrors the actual way that the data were cre-
ated; the state of the world did create the observed data through some physical
process (usually light being emitted from a source, interacting with the object and
being captured by a camera). If we wish to build information about the gener-
ation process into our model, this approach is desirable. For example, we can
account for phenomena such as perspective projection and occlusion. Using the
other approaches, it is harder to exploit this knowledge: essentially we have to
relearn these phenomena from the data.
• In some situations, some parts of the training or test data vector x may be missing.
Here, generative models are preferred. They model the joint distribution over all
of the data dimensions and can effectively interpolate the missing elements.
Figure 6.5 Generative vs. discriminative models. a) Generative approach: we separately model
the probability density function Pr(x|w) for each class. This may require a complex model with
many parameters. b) Posterior probability distribution Pr(w|x) computed via Bayes’ rule with
a uniform prior. Notice that the complicated structure of the individual class conditional density
functions is hardly reﬂected in the posterior: In this case, it might have been more efﬁcient to take
a discriminative approach and model this posterior directly.

64
6 Learning and inference in vision
• A fundamental property of the generative approach is that it allows incorporation
of expert knowledge in the form of a prior. It is harder to impose prior knowledge
in a principled way in discriminative models.
It is notable that generative models are more common in vision applications. Conse-
quently, most of the chapters in the rest of the book concern generative models.
6.6
Applications
The focus of this chapter, and indeed most of the chapters of this book, is on the mod-
els themselves and the learning and inference algorithms. From this point forward, we
will devote a section at the end of each chapter to discussing practical applications of the
relevant models in computer vision. For this chapter, only one of the models can actu-
ally be implemented based on the information presented so far. This is the generative
classiﬁcation model from Section 6.4.2. Consequently, we will focus the applications on
variations of this model and return to the other models in subsequent chapters.
6.6.1
Skin detection
The goal of skin-detection algorithms is to infer a label w ∈{0,1} denoting the presence
or absence of skin at a given pixel, based on the RGB measurements x = [xR,xG,xB]
at that pixel. This is a useful precursor to segmenting a face or hand, or it may be used
as the basis of a crude method for detecting prurient content in Web images. Taking a
generative approach, we describe the likelihoods as
Pr(x|w = k) = Normx[µk,Σk]
(6.12)
and the prior probability over states as
Pr(w) = Bernw[λ].
(6.13)
In the learning algorithm, we estimate the parameters µ0,µ1,Σ0,Σ1 from training
data pairs {wi,xi}I
i=1 where the pixels have been labeled by hand. In particular, we
learn µ0 and Σ0 from the subset of the training data where wi = 0 and µ1 and Σ1 from
the subset where wi = 1. The prior parameter is learned from the world states {wi}I
i=1
alone. In each case, this involves ﬁtting a probability distribution to data using one of the
techniques discussed in Chapter 4.
To classify a new data point x as skin or non-skin, we apply Bayes’ rule
Pr(w = 1|x) =
Pr(x|w = 1)Pr(w = 1)
P1
k=0 Pr(x|w = k)Pr(w = k)
,
(6.14)
and denote this pixel as skin if Pr(w = 1|x) > 0.5. Figure 6.6 shows the result of
applying this model at each pixel independently in the image. Note that the classiﬁcation
is not perfect: there is genuinely an overlap between the skin and non-skin distributions,
and this inevitably results in misclassiﬁed pixels. The results could be improved by
exploiting the fact that skin areas tend to be contiguous regions without small holes. To
do this, we must somehow connect together all of the per-pixel classiﬁers. This is the
topic of chapters 11 and 12.
We brieﬂy note that the RGB data are naturally discrete with xR,xG,xB ∈
{0,1,...,255}, and we could alternatively have based our skin detection model on

6.6
Applications
65
Figure 6.6 Skin detection. For each pixel we aim to infer a label w ∈{0,1} denoting the absence
or presence of skin based on the RGB triple x. Here we modeled the class conditional density func-
tions Pr(x|w) as normal distributions. a) Original image. b) Log likelihood (log of data assessed
under class-conditional density function) for non-skin. c) Log likelihood for skin. d) Posterior
probability of belonging to skin class. e) Thresholded posterior probability Pr(w|x) > 0.5 gives
estimate of w.
this assumption. For example, modeling the three color channels independently, the
likelihoods become
Pr(x|w = k) = CatxR[λR
k ]CatxG[λG
k ]CatxB[λB
k ].
(6.15)
We refer to the assumption that the elements of the data vector are independent as na¨ıve
Bayes. Of course, this assumption is not necessarily valid in the real world. To model the
joint probability distribution of the R,G, and B components properly, we might combine
them to form one variable with 2563 entries and model this with a single categorical dis-
tribution. Unfortunately, this means we must learn 2563 parameters for each categorical
distribution, and so it is more practical to quantize each channel to fewer levels (say 8)
before combining them together.
6.6.2
Background subtraction
A second application of the generative classiﬁcation model is for background subtraction.
Here, the goal is to infer a binary label wn ∈{0,1}, which indicates whether the nth pixel
in the image is part of a known background (w = 0) or whether a foreground object is
occluding it (w = 1). As for the skin detection model, this is based on its RGB pixel data
xn at that pixel.
It is usual to have training data {xin}I,N
i=1,n=1 that consists of a number of empty
scenes where all pixels are known to be background.
However, it is not typical to
have examples of the foreground objects which are highly variable in appearance. For
this reason, we model the class conditional distribution of the background as a normal
distribution
Pr(xn|w = 0) = Normxn[µn0,Σn0],
(6.16)
but model the foreground class as a uniform distribution
Pr(xn|w = 1) =

1/2553
0 < xR
n ,xG
n ,xB
n < 255
0
otherwise
,
(6.17)
and again model the prior as a Bernoulli variable.
To compute the posterior distribution we once more apply Bayes’ rule. Typical results
are shown in Figure 6.7, which illustrates a common problem with this method: shadows

66
6 Learning and inference in vision
Figure 6.7 Background subtraction. For each pixel we aim to infer a label w ∈{0,1} denoting
the absence or presence of a foreground object. a) We learn a class conditional density model
Pr(x|w) for the background from training examples of an empty scene. The foreground model
is treated as uniform.
b) For a new image, we then compute the posterior distribution using
Bayes’ rule. c) Posterior probability of being foreground Pr(w = 1|x). Images from CAVIAR
database.
Figure 6.8 Background subtraction in deforming scene. a) The foliage is blowing in the wind
in the training images. b) The distribution of RGB values at the pixel indicated by the circle in
(a) is now bimodal and not well described by a normal density function (red channel only shown).
Images from video by Terry Boult.
are often misclassiﬁed as foreground. A simple way to remedy this is to classify pixels
based on the hue alone.
In some situations we need a more complex distribution to describe the back-
ground. For example, consider an outdoor scene in which trees are blowing in the wind
(Figure 6.8). Certain pixels may have bimodal distributions where one part of the foliage
intermittently moves in front of another. It is clear that the unimodal normal distribution

Notes
67
cannot provide a good description of this data, and the resulting background segmenta-
tion result will be poor. We devote part of the next chapter to methods for describing
more complex probability distributions of this type.
Summary
In this chapter, we have provided an overview of how abstract vision problems can be
solved using machine learning techniques. We have illustrated these ideas with some
simple examples. We did not provide the implementation level details of the learning
and inference algorithms; these are presented in subsequent chapters.
Model Pr(w|x)
Model Pr(x|w)
Regression
Linear regression
Linear regression
x ∈[−∞,∞],w ∈[−∞,∞]
Classiﬁcation
Logistic regression
Probability density function
x ∈[−∞,∞],w ∈{0,1}
Table 6.1 Example models in this chapter. These can be categorized into those that are based on
modeling probability density functions, those that are based on linear regression, and those that are
based on logistic regression.
The examples in this chapter are summarized in Table 6.1, where it can be seen
that there are three distinct types of model.
First, there are those that depend on
building probability density functions (describing the class conditional density functions
Pr(x|w = k)). In the following chapter, we investigate building complex probability
density models. The second type of model is based on linear regression, and Chapter 8
investigates a family of related algorithms. Finally, the third type of model discussed in
this chapter was logistic regression. We will elaborate on the logistic regression model
in Chapter 9.
Notes
The goal of this chapter was to give a very compact view of learning and inference in vision.
Alternative views of this material which are not particularly aimed at vision can be found in Bishop
(2006) and Duda et al. (2001) and many other texts.
Skin Detection: Reviews of skin detection can be found in Kakumanu et al. (2007) and Vezhnevets
et al. (2003). Pixel-based skin-segmentation algorithms have been variously used as the basis
for face detection (Hsu et al. 2002), hand gesture analysis (Zhu et al. 2000) and ﬁltering of
pornographic images (Jones and Rehg 2002).
There are two main issues that affect the quality of the ﬁnal results: the representation of the
pixel color and the classiﬁcation algorithm.
With regard to the latter issue, various genera-
tive approaches have been investigated, including methods based on normal distributions (Hsu
et al. 2002), mixtures of normal distributions (Jones and Rehg 2002), and categorical distribu-
tions (Jones and Rehg 2002) as well as discriminative methods such as the multilayer perceptron
(Phung et al. 2005). There are several detailed empirical studies that compare the efﬁcacy of
the color representation and classiﬁcation algorithm (Phung et al. 2005; Brand and Mason 2000;
Schmugge et al. 2007).
Background Subtraction:
Reviews of background subtraction techniques can be found in
Piccardi (2004), Bouwmans et al. (2010), and Elgammal (2011).
Background subtraction is

68
6 Learning and inference in vision
a common ﬁrst step in many vision systems as it quickly identiﬁes regions of the image that
are of interest.
Generative classiﬁcation systems have been built based on normal distribu-
tions (Wren et al. 1997), mixtures of normal distributions (Stauffer and Grimson 1999), and
kernel density functions (Elgammal et al. 2000). Several systems (Friedman and Russell 1997;
Horprasert et al. 2000) have incorporated an explicit label in the model to identify shadows.
Most recent research in this area has addressed the maintenance of the background model in chang-
ing environments. Many systems such as that of Stauffer and Grimson (1999) are adaptive and can
incorporate new objects into the background model when the background changes. Other models
compensate for lighting changes by exploiting the fact that all of the background pixels change
together and describing this covariance with a subspace model (Oliver et al. 2000). It is also com-
mon now to abandon the per-pixel approach and to estimate the whole label ﬁeld simultaneously
using a technique based on Markov random ﬁelds (e.g., Sun et al. 2006).
Problems
6.1 Consider the following problems.
i Determining the gender of an image of a face.
ii Determining the pose of the human body given an image of the body.
iii Determining which suit a playing card belongs to based on an image of that card.
iv Determining whether two images of faces match (face veriﬁcation).
v Determining the 3D position of a point given the positions to which it projects in two cameras
at different positions in the world (stereo reconstruction).
For each case, try to describe the contents of the world state w and the data x. Is each discrete
or continuous? If discrete, then how many values can it take? Which are regression problems and
which are classiﬁcation problems?
6.2 Describe a classiﬁer that relates univariate discrete data x ∈{1...K} to a univariate discrete
world state w ∈{1...M} for both discriminative and generative model types.
6.3 Describe a regression model that relates univariate binary discrete data x ∈{0,1} to a univari-
ate continuous world state w ∈[−∞,∞]. Use a generative formulation in which Pr(x|w) and
Pr(w) are modeled.
6.4 Describe a discriminative regression model that relates a continuous world state w ∈[0,1]
to univariate continuous data x ∈[−∞,∞]. Hint: Base your classiﬁer on the beta distribution.
Ensure that the constraints on the parameters are obeyed.
6.5 Find expressions for the maximum likelihood estimates of the parameters in the discriminative
linear regression model (Section 6.3.1). In other words ﬁnd the parameters {φ0,φ1,σ2} that satisfy
ˆφ0, ˆφ1, ˆσ2 = argmax
φ0,φ1,σ2
" I
Y
i=1
Pr(wi|xi,φ0,φ1,σ2)
#
= argmax
φ0,φ1,σ2
"
I
X
i=1
log

Pr(wi|xi,φ0,φ1,σ2)

#
= argmax
φ0,φ1,σ2
"
I
X
i=1
log

Normw

φ0 + φ1x,σ2
#
,
where {wi,xi}I
i=1 are paired training examples.

Problems
69
6.6 Consider a regression model which models the joint probability between the world w and the
data x (Figure 6.9) as
Pr
wi
xi

= Norm[wi,xi]T
µw
µx

,
σ2
ww
σ2
xw
σ2
xw
σxx

.
(6.18)
0
10
0
10
Figure 6.9 Regression model for Problem
6.6. An alternative generative approach to
regression is to model the joint probability
distribution Pr(x,w), instead of modeling
the likelihood Pr(x|w) and the prior Pr(w)
separately. In this case, Pr(x,w) is mod-
elled as a joint normal distribution in the
two variables.
In inference, we compute
the posterior distribution Pr(w|x) over the
world state using Bayes’ rule Pr(w|x) =
Pr(x,w)/Pr(x).
Use the relation in Section 5.5 to compute the posterior distribution Pr(wi|xi). Show that it has
the form
Pr(wi|xi) = Normwi[φ0 + φ1x],
(6.19)
and compute expressions for φ0 and φ1 in terms of the training data {wi,xi}I
i=1 by substituting in
explicit maximum likelihood estimates of the parameters {µw,µx,σ2
ww,σ2
xw,σ2
xx}.
6.7
For a two-class problem, the decision boundary is the locus of world values w where the
posterior probability Pr(w = 1|x) is equal to 0.5. In other words, it represents the boundary
between regions that would be classiﬁed as w = 0 and w = 1. Consider the generative classiﬁer
from section 6.4.2. Show that with equal priors Pr(w = 0) = Pr(w = 1) = 0.5 points on the
decision boundary (the locus of points where Pr(x|w = 0) = Pr(x|w = 1)) obey a constraint of
the form
ax2 + bx + c = 0,
(6.20)
where and {a,b,c} are scalars. Does the shape of the decision boundary for the logistic regression
model from section 6.4.1 have the same form?
6.8 Consider a generative classiﬁcation model for 1D data with likelihood terms
Pr(xi|wi = 0) = Normxi

0,σ2
Pr(xi|wi = 1) = Normxi

0,1.5σ2
.
What is the decision boundary for this classiﬁer with equal priors Pr(w = 0) = Pr(w = 1) = 0.5?
Develop a discriminative classiﬁer that can produce the same decision boundary. Hint: Base your
classiﬁer on a quadratic rather than a linear function.
6.9 Consider a generative binary classiﬁer for multivariate data based on multivariate normal
likelihood terms
Pr(xi|wi = 0) = Normxi [µ0,Σ0]
Pr(xi|wi = 1) = Normxi [µ1,Σ1]

70
6 Learning and inference in vision
and a discriminative classiﬁer based on logistic regression for the same data
Pr(wi|xi) = Bernwi
h
sig[φ0 + φT xi]
i
.
where there is one entry in the gradient vector φ for each entry of xi.
How many parameters does each model have as a function of the dimensionality of xi? What are
the relative advantages and disadvantages of each model as the dimensionality increases?
6.10 One of the problems with the background subtraction method described is that it erroneously
classiﬁes shadows as foreground. Describe a model that could be used to classify pixels into three
categories (foreground, background, and shadow).

Chapter 7
Modeling complex data
densities
In the last chapter we showed that classiﬁcation with generative models is based on build-
ing simple probability models. In particular, we build class-conditional density functions
Pr(x|w = k) over the observed data x for each value of the world state w.
In Chapter 3 we introduced several probability distributions that could be used for this
purpose, but these were quite limited in scope. For example, it is not realistic to assume
that all of the complexities of visual data are well described by the normal distribution.
In this chapter, we show how to construct complex probability density functions from
elementary ones using the idea of a hidden variable.
As a representative problem we consider face detection; we observe a 60 × 60 RGB
image patch, and we would like to decide whether it contains a face or not. To this end,
we concatenate the RGB values to form the 10800 × 1 vector x. Our goal is to take the
vector x and return a label w ∈{0,1} indicating whether it contains background (w = 0)
or a face (w = 1). In a real face detection system, we would repeat this procedure for
every possible subwindow of an image (Figure 7.1).
We will start with a basic generative approach in which we describe the likelihood of
the data in the presence/absence of a face with a normal distribution. We will then extend
this model to address its weaknesses. We emphasize though that state-of-the-art face
detection algorithms are not based on generative methods such as these; they are usually
tackled using the discriminative methods of Chapter 9. This application was selected for
purely pedagogical reasons.
7.1
Normal classiﬁcation model
We will take a generative approach to face detection; we will model the probability of
the data x and parameterize this by the world state w. We will describe the data with a
multivariate normal distribution so that
Pr(x|w) = Normx[µw,Σw]
(7.1)

72
7 Modeling complex data densities
Figure 7.1 Face detection. Consider examining a small window of the image (here 60 × 60). We
concatenate the RGB values in the window to make a data vector x of dimension 10800 × 1. The
goal of face detection is to infer a label w ∈{0,1} indicating whether the window contains a) a
background region (w =0) or b) an aligned face (w =1). c–i) We repeat this operation at every
position and scale in the image by sweeping a ﬁxed size window through a stack of resized images,
estimating w at every point.
or treating the two possible values of the state w separately, we can explicitly write
Pr(x|w = 0) = Normx[µ0,Σ0]
Pr(x|w = 1) = Normx[µ1,Σ1].
(7.2)
These expressions are examples of class conditional density functions. They describe the
density of the data x conditional on the value of the world state w.
The goal of learning is to estimate the parameters θ = {µ0,Σ0,µ1,Σ1} from exam-
ple pairs of training data {xi,wi}I
i=1. Since parameters µ0 and Σ0 are concerned
exclusively with background regions (where w =0), we can learn them from the subset
of training data S0 that belonged to the background. For example, using the maximum
likelihood approach, we would seek
ˆµ0, ˆΣ0 = argmax
µ0,Σ0
" Y
i∈S0
Pr(xi|µ0,Σ0)
#
= argmax
µ0,Σ0
" Y
i∈S0
Normxi[µ0,Σ0]
#
.
(7.3)
Similarly, µ1 and Σ1 are concerned exclusively with faces (where w = 1) and can be
learned from the subset S1 of training data which contained faces. Figure 7.2 shows the
maximum likelihood estimates of the parameters where we have used the diagonal form
of the covariance matrix.
The goal of the inference algorithm is to take a new facial image x and assign a label
w to it. To this end, we deﬁne a prior over the values of the world state Pr(w) = Bernw[λ]
and apply Bayes’ rule
Pr(w = 1|x) =
Pr(x|w = 1)Pr(w = 1)
P1
k=0 Pr(x|w = k)Pr(w = k)
.
(7.4)
All of these terms are simple to compute, and so inference is very easy and will not be
discussed further in this chapter.

7.1
Normal classiﬁcation model
73
Figure 7.2 Class conditional density func-
tions for normal model with diagonal
covariance. Maximum likelihood ﬁts based
on 1000 training examples per class.
a)
Mean for background data µ0 (reshaped
from 10800 × 1 vector to 60 × 60 RGB
image). b) Reshaped square root of diago-
nal covariance for background data Σ0. c)
Mean for face data µ1. d) Covariance for
face data Σ1. The background model has
little structure: the mean is uniform, and
the variance is high everywhere. The mean
of the face model clearly captures class-
speciﬁc information. The covariance of the
face is larger at the edges of the image,
which usually contain hair or background.
7.1.1
Deﬁciencies of the multivariate normal model
Unfortunately, this model does not detect faces reliably. We will defer presenting experi-
mental results until Section 7.9.1, but for now please take it on trust that while this model
achieves above-chance performance, it doesn’t come close to producing a state-of-the-art
result. This is hardly surprising: the success of this classiﬁer hinges on ﬁtting the data
with a normal distribution. Unfortunately, this ﬁt is poor for three reasons (Figure 7.3).
• The normal distribution is unimodal; neither faces nor background regions are well
represented by a pdf with a single peak.
• The normal distribution is not robust; a single outlier can dramatically affect the
estimates of the mean and covariance.
• The normal distribution has too many parameters; here the data have D = 10800
dimensions. The full covariance matrix contains D(D + 1)/2 parameters. With
only 1000 training examples, we cannot even specify these parameters uniquely,
so we were forced to use the diagonal form.
We devote the rest of this chapter to tackling these problems. To make the density
multimodal, we introduce mixture models. To make the density robust, we replace the
normal with the t-distribution. To cope with parameter estimation in high dimensions,
we introduce subspace models.
The new models have much in common with each other. In each case, we introduce
a hidden or latent variable hi associated with each observed data point xi. The hidden
variable induces the more complex properties of the resulting pdf. Moreover, because the
structure of the models is similar, we can use a common approach to learn the parameters.
In the following section, we present an abstract discussion of how hidden variables
can be used to model complex pdfs. In Section 7.3, we discuss how to learn the parame-
ters of models with hidden variables. Then in Sections 7.4, 7.5, and 7.6, we will introduce
mixture models, t-distributions, and factor analysis, respectively.

74
7 Modeling complex data densities
Figure 7.3 a) Problems with the multivariate normal density. b) Normal models are unimodal, but
mixtures of Gaussians can model multimodal distributions. c) Normal distributions are not robust
to outliers, but t-distributions can cope with unusual observations. d) Normal models need many
parameters in high dimensions but subspace models reduce this requirement. e) These solutions
can be combined to form hybrid models addressing several of these problems at once.
7.2
Hidden variables
To model a complex probability density function over the variable x, we will introduce
a hidden or latent variable h, which may be discrete or continuous. We will discuss the
continuous formulation, but all of the important concepts transfer to the discrete case.
To exploit the hidden variables, we describe the ﬁnal density Pr(x) as the marginal-
ization of the joint density Pr(x,h) between x and h so that
Pr(x) =
Z
Pr(x,h) dh.
(7.5)
We now concentrate on describing the joint density Pr(x,h).
We can choose this
so that it is relatively simple to model but produces an expressive family of marginal
distributions Pr(x) when we integrate over h (see Figure 7.4).
Whatever form we choose for the joint distribution, it will have some parameters θ,
and so really we should write
Pr(x|θ) =
Z
Pr(x,h|θ) dh.
(7.6)
There are two possible approaches to ﬁtting the model to training data {xi}I
i=1 using
the maximum likelihood method. We could directly maximize the log likelihood of the
distribution Pr(x) from the left-hand side of Equation 7.6 so that
ˆθ = argmax
θ
" I
X
i=1
log[Pr(xi|θ)]
#
.
(7.7)
This formulation has the advantage that we don’t need to involve the hidden variables
at all. However, in the models that we will consider, it will not result in a neat closed

7.3
Expectation maximization
75
Figure 7.4 Using hidden variables to help
model complex densities.
One way to
model the density Pr(x) is to consider
the joint probability distribution Pr(x,h)
between the observed data x and a hid-
den variable h. The density Pr(x) can be
considered as the marginalization of (inte-
gral over) this distribution with respect to
the hidden variable h.
As we manipu-
late the parameters θ of this joint distri-
bution, the marginal distribution changes
and the agreement with the observed data
{xi}I
i=1 increases or decreases. Sometimes
it is easier to ﬁt the distribution in this
indirect way than to directly manipulate
Pr(x).
form solution. Of course, we could apply a brute force nonlinear optimization technique
(Appendix B), but there is an alternative approach: we use the expectation maximization
algorithm, which works directly with the right-hand side of Equation 7.6 and seeks
ˆθ = argmax
θ
" I
X
i=1
log
Z
Pr(xi,hi|θ) dhi
#
.
(7.8)
7.3
Expectation maximization
In this section, we will present a brief description of the expectation maximization (EM)
algorithm. The goal is to provide just enough information to use this technique for ﬁtting
models. We will return to a more detailed treatment in Section 7.8.
The EM algorithm is a general-purpose tool for ﬁtting parameters θ in models of the
form of Equation 7.6 where
ˆθ = argmax
θ
" I
X
i=1
log
Z
Pr(xi,hi|θ) dhi
#
.
(7.9)
The EM algorithm works by deﬁning a lower bound B[{qi(hi)},θ] on the log likelihood
in Equation 7.9 and iteratively increasing this bound. The lower bound is simply a func-
tion that is parameterized by θ and some other quantities and is guaranteed to always
return a value that is less than or equal to the log likelihood L[θ] for any given set of
parameters θ (Figure 7.5).
For the EM algorithm, the particular lower bound chosen is
B[{qi(hi)},θ] =
I
X
i=1
Z
qi(hi)log
Pr(xi,hi|θ)
qi(hi)

dhi
(7.10)
≤
I
X
i=1
log
Z
Pr(xi,hi|θ) dhi

.

76
7 Modeling complex data densities
Figure 7.5 Manipulating the lower bound. a) Consider the log likelihood L[θ] of the data {x}I
i=1
as a function of the model parameters θ (red curve). In maximum likelihood learning, our goal
is to ﬁnd the parameters θ that maximize this function. A lower bound on the log likelihood is
another function B[θ] of the parameters θ that is everywhere lower or equal to the log likelihood
(green curve). One way to improve the current estimate (blue dot) is to manipulate the parameters
so that B[θ] increases (pink dot). This is the goal of the maximization step of the EM algorithm.
b) The lower bound B[{qi(hi)},θ] also depends on a set of probability distributions {qi(hi)}I
i=1
over hidden variables {hi}. Manipulating these probability distributions changes the value that
the lower bound returns for every θ (e.g., green curve). So a second way to improve the current
estimate (pink dot) is to change the distributions in such a way that the curve increases for the
current parameters (blue dot). This is the goal of the expectation step of the EM algorithm.
It is not obvious that this inequality is true, making this a valid lower bound; take this on
trust for the moment and we will return to this in Section 7.8.
In addition to the parameters θ, the lower bound B[{qi(hi)},θ] also depends on a set
of I probability distributions {qi(hi)}I
i=1 over the hidden variables {hi}I
i=1. When we
vary these probability distributions, the value that the lower bound returns will change,
but it will always remain less than or equal to the log likelihood.
The EM algorithm manipulates both the parameters θ and the distributions
{qi(hi)}I
i=1 to increase the lower bound. It alternates between
• Updating the probability distributions {qi(hi)}I
i=1 to improve the bound in
Equation 7.10. This is called the expectation step or E-step and
• Updating the parameters θ to improve the bound in Equation 7.10. This is called
the maximization step or M-step.
In the E-step at iteration t + 1, we set each distribution qi(hi) to be the posterior distri-
butions Pr(hi|xi,θ) over that hidden variable given the associated data example and the
current parameters θ[t]. To compute these, we use Bayes’ rule
ˆqi(hi) = Pr(hi|xi,θ[t]) = Pr(xi|hi,θ[t])Pr(hi|θ[t])
Pr(xi)
.
(7.11)
It can be shown that this choice maximizes the bound as much as possible.
In the M-step, we directly maximize the bound (Equation 7.10) with respect to the
parameters θ. In practice, we can simplify the expression for the bound to eliminate
terms that do not depend on θ and this yields
ˆθ
[t+1] = argmax
θ
" I
X
i=1
Z
ˆqi(hi)log[Pr(xi,hi|θ)] dhi
#
.
(7.12)

7.4
Mixture of Gaussians
77
Each of these steps is guaranteed to improve the bound, and iterating them alternately is
guaranteed to ﬁnd at least a local maximum with respect to θ.
This is a practical description of the EM algorithm, but there is a lot missing: we have
not demonstrated that Equation 7.10 really is a bound on the log likelihood. We have not
shown that the posterior distribution Pr(hi|xi,θ[t]) is the optimal choice for qi(hi) in
the E-step (Equation 7.11), and we have not demonstrated that the cost function for the
M-step (Equation 7.12) improves the bound. For now we will assume that these things
are true and proceed with the main thrust of the chapter. We will return to these issues in
Section 7.8.
7.4
Mixture of Gaussians
The mixture of Gaussians (MoG) is a prototypical example of a model where learning
is suited to the EM algorithm. The data are described as a weighted sum of K normal
distributions
Pr(x|θ) =
K
X
k=1
λkNormx[µk,Σk],
(7.13)
where µ1...K and Σ1...K are the means and covariances of the normal distributions and
λ1...K are positive valued weights that sum to one. The mixtures of Gaussians model
describes complex multimodal probability densities by combining simpler constituent
distributions (Figure 7.6).
To learn the parameters θ={µk,Σk,λk}K
k=1 from training data {xi}I
i=1, we could
apply the straightforward maximum likelihood approach
ˆθ = argmax
θ
" I
X
i=1
log[Pr(xi|θ)]
#
= argmax
θ
" I
X
i=1
log
" K
X
k=1
λkNormxi[µk,Σk]
##
.
(7.14)
Unfortunately, if we take the derivative with respect to the parameters θ and equate the
resulting expression to zero, it is not possible to solve the resulting equations in closed
form. The sticking point is the summation inside the logarithm, which precludes a simple
Figure 7.6 Mixture of Gaussians model in
1D. A complex multimodal probability den-
sity function (black solid curve) is created by
taking a weighted sum or mixture of several
constituent normal distributions with differ-
ent means and variances (red, green, and
blue dashed curves). To ensure that the ﬁnal
distribution is a valid density, the weights
must be positive and sum to one.

78
7 Modeling complex data densities
Figure 7.7
Mixture of Gaussians as a marginalization. The mixture of Gaussians can also be
thought of in terms of a joint distribution Pr(x,h) between the observed variable x and a discrete
hidden variable h. To create the mixture density, we marginalize over h. The hidden variable has
a straightforward interpretation: it is the index of the constituent normal distribution.
solution. Of course, we could use a nonlinear optimization approach, but this would be
complex as we would have to maintain the constraints on the parameters; the weights λ
must sum to one and the covariances {Σk}K
k=1 must be positive deﬁnite. For a simpler
approach, we express the observed density as a marginalization and use the EM algorithm
to learn the parameters.
7.4.1
Mixture of Gaussians as a marginalization
The mixture of Gaussians model can be expressed as the marginalization of a joint proba-
bility distribution between the observed data x and a discrete hidden variable h that takes
values h ∈{1...K} (Figure 7.7). If we deﬁne
Pr(x|h,θ) = Normx[µh,Σh]
Pr(h|θ) = Cath[λ],
(7.15)
where λ = [λ1 ...λK] are the parameters of the categorical distribution, then we can
recover the original density using
Pr(x|θ) =
K
X
k=1
Pr(x,h = k|θ)
=
K
X
k=1
Pr(x|h = k,θ)Pr(h = k|θ)
=
K
X
k=1
λkNormx[µk,Σk].
(7.16)

7.4
Mixture of Gaussians
79
Interpreting the model in this way also provides a method to draw samples from a
mixture of Gaussians: we sample from the joint distribution Pr(x,h) and then discard
the hidden variable h to leave just a data sample x. To sample from the joint distribution
Pr(x,h), we ﬁrst sample h from the categorical prior Pr(h), then sample x from the
normal distribution Pr(x|h) associated with the value of h. Notice that the hidden vari-
able h has a clear interpretation in this procedure. It determines which of the constituent
normal distributions is responsible for the observed data point x.
7.4.2
Expectation maximization for ﬁtting mixture models
To learn the MoG parameters θ = {λk,µk,Σk}K
k=1 from training data {xi}I
i=1 we apply
7.1
the EM algorithm. Following the recipe of Section 7.3, we initialize the parameters
randomly and alternate between performing the E- and M-steps.
In the E-step, we maximize the bound with respect to the distributions qi(hi) by
ﬁnding the posterior probability distribution Pr(hi|xi) of each hidden variable hi given
the observation xi and the current parameter settings,
qi(hi) = Pr(hi = k|xi,θ[t]) =
Pr(xi|hi = k,θ[t])Pr(hi = k,θ[t])
PK
j=1 Pr(xi|hi = j,θ[t])Pr(hi = j,θ[t])
=
λkNormxi[µk,Σk]
PK
j=1 λjNormxi[µj,Σj]
= rik.
(7.17)
In other words we compute the probability Pr(hi = k|xi,θ[t]) that the kth normal dis-
tribution was responsible for the ith datapoint (Figure 7.8). We denote this responsibility
by rik for short.
In the M-step, we maximize the bound with respect to the model parameters
θ = {λk,µk,Σk}K
k=1 so that
ˆθ
[t+1] = argmax
θ
" I
X
i=1
K
X
k=1
ˆqi(hi = k)log[Pr(xi,hi = k|θ)]
#
= argmax
θ
" I
X
i=1
K
X
k=1
rik log[λkNormxi[µk,Σk]]
#
.
(7.18)
This maximization can be performed by taking the derivative of the expression with
respect to the parameters, equating the result to zero, and rearranging, taking care to
enforce the constraint P
k λk = 1 using Lagrange multipliers. The procedure results in
the update rules:
λ[t+1]
k
=
PI
i=1 rik
PK
j=1
PI
i=1 rij
(7.19)
µ[t+1]
k
=
PI
i=1 rikxi
PI
i=1 rik
Σ[t+1]
k
=
PI
i=1 rik(xi −µ[t+1]
k
)(xi −µ[t+1]
k
)T
PI
i=1 rik
.

80
7 Modeling complex data densities
Figure 7.8 E-step for ﬁtting the mixture of Gaussians model. For each of the I data points
x1...I, we calculate the posterior distribution Pr(hi|xi) over the hidden variable hi. The posterior
probability Pr(hi = k|xi) that hi takes value k can be understood as the responsibility of normal
distribution k for data point xi. For example, for data point x1 (magenta circle), component 1 (red
curve) is more than twice as likely to be responsible than component 2 (green curve). Note that in
the joint distribution (left), the size of the projected data point indicates the responsibility.
Figure 7.9 M-step for ﬁtting the mixture of Gaussians model. For the kth constituent Gaussian,
we update the parameters {λk,µk,Σk}. The ith data point xi contributes to these updates accord-
ing to the responsibility rik (indicated by size of point) assigned in the E-step; data points that are
more associated with the kth component have more effect on the parameters. Dashed and solid
lines represent ﬁt before and after the update, respectively.

7.4
Mixture of Gaussians
81
Figure 7.10
Fitting a mixture of two Gaussians to 2D data. a) Initial model. b) E-step. For
each data point the posterior probability that is was generated from each Gaussian is calculated
(indicated by color of point). c) M-step. The mean, variance and weight of each Gaussian is
updated based on these posterior probabilities. Ellipse shows Mahalanobis distance of two. Weight
(thickness) of ellipse indicates weight of Gaussian. d–t) Further E-step and M-step iterations.
These update rules can be easily understood (Figure 7.9): we update the weights {λk}K
k=1
according to the relative total responsibility of each component for the data points. We
update the cluster means {µk}K
k=1 by computing the weighted mean over the datapoints
where the weights are given by the responsibilities. If component k is mostly responsible
for data point xi, then this data point has a high weight and affects the update more. The
update rule for the covariances has a similar interpretation.

82
7 Modeling complex data densities
In practice the E- and M-steps are alternated until the bound on the data no longer
increases and the parameters no longer change. The alternating E-steps and M-steps for
a two-dimensional example are shown in Figure 7.10. Notice that the ﬁnal ﬁt identiﬁes
the two clusters in the data. The mixture of Gaussians is closely related to clustering
techniques such as the K-means algorithm (Section 13.4.4).
The EM approach to estimating mixture models has three attractive features.
1. Both steps of the algorithm can be computed in closed form without the need for
an optimization procedure.
2. The solution guarantees that the constraints on the parameters are respected: the
weighting parameters {λk}K
k=1 are guaranteed to be positive and sum to one,
and the covariance matrices {Σk}K
k=1 are guaranteed to be positive deﬁnite.
3. The method can cope with missing data. Imagine that some of the elements of
training example xi are missing. In the E-step, the remaining dimensions can
still be used to establish a distribution over the hidden variable h. In the M-step,
this datapoint would contribute only to the dimensions of {µk}K
k=1 and {Σk}K
k=1
where data were observed.
Figure 7.11 shows a mixture of ﬁve Gaussians that has been ﬁt to a 2D data set.
As for the basic multivariate normal model, it is possible to constrain the covariance
matrices to be spherical or diagonal. We can also constrain the covariances to be the
same for each component if desired. Figure 7.12 shows the mean vectors µk for a ten-
component model with diagonal covariances ﬁtted to the face data set. The clusters
represent different illumination conditions as well as changes in pose, expression, and
background color.
In ﬁtting mixtures of Gaussians, there are several things to consider. First, the EM
algorithm does not guarantee to ﬁnd a global solution to this non-convex optimization
problem. Figure 7.13 shows three different solutions that were computed by starting
the ﬁtting algorithm with different initial random values for the parameters θ. The best
we can do to circumvent this problem is to start ﬁtting in different places and take the
solution with the greatest log likelihood. Second, we must prespecify the number of
mixing components. Unfortunately, we cannot decide the number of components by
comparing the log likelihood; models with more parameters will inevitably describe the
data better. There are methods to tackle this problem, but they are beyond the scope of
this volume.
Finally, although we presented a maximum likelihood approach here, it is important
in practice to include priors over model parameters Pr(θ) to prevent the scenario where
one of the Gaussians becomes exclusively associated with a single datapoint. Without a
prior, the variance of this component becomes progressively smaller and the likelihood
increases without bound.
7.5
The t-distribution
The second signiﬁcant problem with using the normal distribution to describe visual
data is that it is not robust: the height of the normal pdf falls off very rapidly as we
move into the tails. The effect of this is that outliers (unusually extreme observations)

7.5
The t-distribution
83
Figure 7.11
Covariance of components in mixture models. a) Full covariances. b) Diagonal
covariances. c) Identical diagonal covariances.
Figure 7.12 Mixtures of Gaussians model for face data. a–j) Mean vectors µk for a mixture of
ten Gaussians ﬁtted to the face data set. The model has captured variation in the mean luminance
and chromaticity of the face and other factors such as the pose and background color. Numbers
indicate the weight λk of each component.
Figure 7.13 Local maxima. Repeated ﬁtting of mixture of Gaussians model with different starting
points results in different models as the ﬁt converges to different local maxima. The log likelihoods
are a) 98.76 b) 96.97 c) 94.35, respectively, indicating that (a) is the best ﬁt.

84
7 Modeling complex data densities
Figure 7.14 Motivation for t-distribution. a) The multivariate normal model ﬁt to data. b) Adding
a single outlier completely changes the ﬁt. c) With the multivariate t-distribution the outlier does
not have such a drastic effect.
Figure 7.15 The univariate t-distribution. a) As well as the mean µ and scaling parameter σ2, the
t-distribution has a parameter ν which is termed the degrees of freedom. As ν decreases, the tails
of the distribution become longer and the model becomes more robust. b) This is seen more clearly
on a log scale.
drastically affect the estimated parameters (Figure 7.14). The t-distribution is a closely
related distribution in which the length of the tails is parameterized.
The univariate t-distribution (Figure 7.15) has probability density function
Pr(x) = Studx

µ,σ2,ν

=
Γ
 ν+1
2

√
νπσ2Γ
 ν
2


1 + (x −µ)2
νσ2
−ν+1
2
,
(7.20)
where µ is the mean and σ2 is the scale parameter. The degrees of freedom ν ∈(0,∞]
controls the length of the tails: when ν is small there is considerable weight in the tails.
For example, with µ = 0 and σ2 = 1 a datapoint at x = −5 is roughly 104 = 10000 times
more likely under the t-distribution with ν = 1 than under the normal distribution. As ν
tends to inﬁnity, the distribution approximates a normal more and more closely and there
is less weight in the tails. The variance of the distribution is given by σν/(ν −2) for
ν > 2 and inﬁnite if 0 < ν ≤2.

7.5
The t-distribution
85
0
10
Figure 7.16 The gamma distribution is
deﬁned on positive real values and has two
parameters α, β.
The mean of the dis-
tribution is E[h] = α/β and the variance
is E[(h−E[h])2] = α/β2. The t-distribution
can be thought of as a weighted sum of
normal distributions with the same mean,
but covariances that depend inversely on the
gamma distribution.
The multivariate t-distribution has pdf
Pr(x) = Studx [µ,Σ,ν]
=
Γ
 ν+D
2

(νπ)D/2|Σ|1/2Γ
 ν
2


1 + (x −µ)T Σ−1(x −µ)
ν
−ν+D
2
,
(7.21)
where D is the dimensionality of the space, µ is a D × 1 mean vector, Σ is a D×
D positive deﬁnite scale matrix, and ν ∈[0,∞] is the degrees of freedom. As for the
multivariate normal distribution (Figure 5.1), the scale matrix can take full, diagonal or
spherical forms. The covariance of the distribution is given by Σν/(ν −2) for ν > 2 and
is inﬁnite if 0 ≤ν ≤2.
7.5.1
Student t-distribution as a marginalization
As for the mixtures of Gaussians, it is also possible to understand the t-distribution in
terms of hidden variables. We deﬁne
Pr(x|h) = Normx[µ,Σ/h]
Pr(h) = Gamh[ν/2,ν/2],
(7.22)
where h is a scalar hidden variable and Gam[α,β] is the gamma distribution with param-
eters α,β (Figure 7.16). The gamma distribution is a continuous probability distribution
deﬁned on the positive real axis with probability density function
Gamh[α,β] = βα
Γ[α] exp[−βh]hα−1,
(7.23)
where Γ[•] is the gamma function.
The t-distribution is the marginalization with respect to the hidden variable h of the
joint distribution between the data x and h (Figure 7.17),
Pr(x) =
Z
Pr(x,h)dh =
Z
Pr(x|h)Pr(h)dh
=
Z
Normx[µ,Σ/h]Gamh[ν/2,ν/2]dh
= Studx[µ,Σ,ν].
(7.24)

86
7 Modeling complex data densities
Figure 7.17 The t-distribution as a marginalization. a) The t-distribution has a similar form to
the normal distribution but longer tails. b) The t-distribution is the marginalization of the joint
distribution Pr(x,h) between the observed variable x and a hidden variable h. c) The prior dis-
tribution over the hidden variable h has a gamma distribution. d) The conditional distribution
Pr(x|h) is normal with a variance that depends on h. So the t-distribution can be considered as
an inﬁnite weighted sum of normal distributions with variances determined by the gamma prior
(equation 7.24).
This formulation also provides a method to generate data from the t-distribution. We
ﬁrst generate h from the gamma distribution and then generate x from the associated nor-
mal distribution Pr(x|h). Hence the hidden variable has a simple interpretation: it tells
us which one of the continuous family of underlying normal distributions was responsible
for this datapoint.
7.5.2
Expectation maximization for ﬁtting t-distributions
Since the pdf takes the form of a marginalization of the joint distribution with a
7.2
hidden variable (Equation 7.24), we can use the EM algorithm to learn the parameters
θ = {µ,Σ,ν} from a set of training data {xi}I
i=1.
In the E-step (Figure 7.18a–b), we maximize the bound with respect to the distribu-
tions qi(hi) by ﬁnding the posterior Pr(hi|xi,θ[t]) over each hidden variable hi given
associated observation xi and the current parameter settings. By Bayes’ rule, we get
qi(hi) = Pr(hi|xi,θ[t]) = Pr(xi|hi,θ[t])Pr(hi)
Pr(xi|θ[t])
(7.25)
= Normxi[µ,Σ/hi]Gamhi[ν/2,ν/2]
Pr(xi)
= Gamhi
ν + D
2
, (xi −µ)T Σ−1(xi −µ)
2
+ ν
2

,
where we have used the fact that the gamma distribution is conjugate to the scaling factor
for the normal variance. The E-step can be understood as follows: we are treating each
data point xi as if it were generated from one of the normals in the inﬁnite mixture where

7.5
The t-distribution
87
Figure 7.18 Expectation maximization for ﬁtting t-distributions.
a) Estimate of distribution
before update. b) In the E-step, we calculate the posterior distribution Pr(hi|xi) over the hid-
den variable hi for each data point xi. The color of each curve corresponds to that of the original
data point in (a). c) In the M-step, we use these distributions over h to update the estimate of the
parameters θ = {µ,σ2,ν}.
the hidden variable hi determines which normal. So, the E-step computes a distribution
over hi, which hence determines a distribution over which normal created the data.
We now compute the following expectations (section 2.7) with respect to the
distribution in Equation 7.25:
E[hi] =
(ν + D)
ν + (xi −µ)T Σ−1(xi −µ)
(7.26)
E[log[hi]] = Ψ
ν + D
2

−log
ν + (xi −µ)T Σ−1(xi −µ)
2

,
(7.27)
where Ψ[•]) is the digamma function. These expectations will be needed in the M-step.
In the M-step (Figure 7.18c) we maximize the bound with respect to the parameters
θ = {µ,Σ,ν} so that
ˆθ
[t+1] = argmax
θ
" I
X
i=1
Z
ˆqi(hi)log[Pr(xi,hi|θ)]dhi
#
= argmax
θ
" I
X
i=1
Z
ˆqi(hi)(log[Pr(xi|hi,θ)] + log[Pr(hi)])dhi
#
= argmax
θ
" I
X
i=1
Z
Pr(hi|xi,θ[t])(log[Pr(xi|hi,θ)] + log[Pr(hi)])dhi
#
= argmax
θ
" I
X
i=1
E [log[Pr(xi|hi,θ)]] + E [log[Pr(hi)]]
#
,
(7.28)
where the expectation is taken relative to the posterior distribution Pr(hi|xi,θ[t]). Sub-
stituting in the expressions for the normal likelihood Pr(xi|hi) and the gamma prior
Pr(hi), we ﬁnd that
E [log[Pr(xi|hi,θ)]] = DE[loghi]−Dlog2π−log|Σ|−(xi −µ)T Σ−1(xi −µ)E[hi]
2
E [log[Pr(hi)]] = ν
2 log
hν
2
i
−logΓ
hν
2
i
+
ν
2 −1

E[loghi]−ν
2E[hi].
(7.29)

88
7 Modeling complex data densities
To optimize µ and Σ, we take derivatives of Equation 7.28, set the resulting expressions
to zero and rearrange to yield update equations
µ[t+1] =
PI
i=1 E[hi]xi
PI
i=1 E[hi]
Σ[t+1] =
PI
i=1 E[hi](xi −µ[t+1])(xi −µ[t+1])T
PI
i=1 E[hi]
.
(7.30)
These update equations have an intuitive form: for the mean, we are computing a
weighted sum of the data. Outliers in the data set will tend to be explained best by the nor-
mal distributions in the inﬁnite mixture which have larger covariances: for these distribu-
tions h is small (h scales the normal covariance inversely). Consequently, E[h] is small,
and they are weighted less in the sum. The update for the Σ has a similar interpretation.
Unfortunately, there is no closed form solution for the degrees of freedom ν. We
hence perform a one-dimensional line search to maximize Equation 7.28 having sub-
stituted in the updated values of µ and Σ, or use one of the optimization techniques
described in Chapter 9.
When we ﬁt a t-distribution with a diagonal scale matrix Σ to the face data set, the
mean µ and scale matrix Σ (not shown) look visually similar to those for the normal
model (Figure 7.2). However, the model is not the same. The ﬁtted degrees of freedom
ν is 6.6. This low value indicates that the distribution has signiﬁcantly longer tails than
the normal model.
In conclusion, the multivariate t-distribution provides an improved description of data
with outliers (Figure 7.14). It has just one more parameter than the normal (the degrees
of freedom, ν), and subsumes the normal as a special case (where ν becomes very large).
However, this generality comes at a cost: there is no closed form solution for the maxi-
mum likelihood parameters and so we must resort to more complex approaches such as
the EM algorithm to ﬁt the distribution.
7.6
Factor analysis
We now address the ﬁnal problem with the normal distribution. Visual data are often very
high dimensional; in the face detection task, the data comes in the form of 60 × 60 RGB
images and is hence characterized as a 60×60×3 = 10800 dimensional vector. To model
this data with the full multivariate normal distribution, we require a covariance matrix of
dimensions 10800 × 10800: we would need a very large number of training examples
to get good estimates of all of these parameters in the absence of prior information.
Furthermore, to store the covariance matrix, we will need a large amount of memory, and
there remains the problem of inverting this large matrix when we evaluate the normal
likelihood (Equation 5.1).
Of course, we could just use the diagonal form of the covariance matrix, which con-
tains only 10800 parameters. However, this is too great a simpliﬁcation: we are assuming
that each dimension of the data is independent and for face images this is clearly not
true. For example, in the cheek region, the RGB values of neighboring pixels covary
very closely. A good model should capture this information.
Factor analysis provides a compromise in which the covariance matrix is structured so
that it contains fewer unknown parameters than the full matrix but more than the diagonal
form. One way to think about the covariance of a factor analyzer is that it models part

7.6
Factor analysis
89
Figure 7.19 Linear subspaces. a) A one-dimensional subspace (a line through the origin, O) is
embedded in a two-dimensional space. Any point x in the subspace can be reached by weighting
the single basis vector φ1 appropriately. b) A two-dimensional subspace (a plane through the
origin, O) is embedded in a three dimensional space. Any point x in the subspace can be reached
using a linear combination x = αφ1 + βφ2 of the two basis functions φ1,φ2 that describe the
subspace. In general, a K-dimensional subspace can be described using K basis functions.
of the high-dimensional space with a full model and mops up remaining variation with a
diagonal model.
More precisely, the factor analyzer describes a linear subspace with a full covariance
model. A linear subspace is a subset of a high-dimensional space that can be reached by
taking linear combinations (weighted sums) of a ﬁxed set of basis functions (Figure 7.19).
So, a line through the origin is a subspace in two dimensions as we can reach any point
on it by weighting a single-basis vector. A line through the origin is also a subspace in
three dimensions, but so is a plane through the origin: we can reach any point on the
plane by taking linear combinations of two basis vectors. In general, a D-dimensional
space contains subspaces of dimensions 1,2,...,D −1.
The probability density function of a factor analyzer is given by
Pr(x) = Normx[µ,ΦΦT + Σ],
(7.31)
where the covariance matrix ΦΦT + Σ contains a sum of two terms. The ﬁrst term
ΦΦT describes a full covariance model over the subspace: the K columns of the portrait1
rectangular matrix Φ = [φ1,φ2,...,φK] are termed factors. The factors are basis vectors
that determine the subspace modeled. When we ﬁt the model to data, the factors will span
the set of directions where the data covary the most. The second term Σ is a diagonal
matrix that accounts for all remaining variation.
Notice that this model has K ×D parameters to describe Φ and another D parameters
to describe the diagonal matrix Σ. If the number of factors K is much less than the
dimensionality of the data D, then this model has fewer parameters than a normal with
full covariance and hence can be learned from fewer training examples.
When Σ is a constant multiple of the identity matrix (i.e., models spherical covari-
ance) the model is called probabilistic principal component analysis. This simpler model
has slightly fewer parameters and can be ﬁt in closed form (i.e., without the need
for the EM algorithm), but otherwise it has no advantages over factor analysis (see
Section 17.5.1 for more details). We will hence restrict ourselves to a discussion of
the more general factor analysis model.
1That is, it is tall and thin as opposed to landscape, which would be short and wide.

90
7 Modeling complex data densities
7.6.1
Factor analysis as a marginalization
As for the mixtures of Gaussians and the t-distribution, it is possible to view the factor
analysis model as a marginalization of a joint distribution between the observed data x
and a K-dimensional hidden variable h. We deﬁne
Pr(x|h) = Normx[µ + Φh,Σ]
Pr(h) = Normh[0,I],
(7.32)
where I represents the identity matrix. It can be shown (but is not obvious) that
Pr(x) =
Z
Pr(x,h)dh =
Z
Pr(x|h)Pr(h) dh
=
Z
Normx[µ + Φh,Σ]Normh[0,I] dh
= Normx[µ,ΦΦT + Σ],
(7.33)
which was the original deﬁnition of the factor analyzer (Equation 7.31).
Expressing factor analysis as a marginalization reveals a simple method to draw sam-
ples from the distribution. We ﬁrst draw a hidden variable h from the normal prior. We
then draw the sample x from a normal distribution with mean µ + Φh and diagonal
covariance Σ (see Equation 7.32).
This leads us to a simple interpretation of the hidden variable h: each element hk
weights the associated basis function φk in the matrix Φ and so h deﬁnes a point on the
subspace (Figure 7.19). The ﬁnal density (Equation 7.31) is hence an inﬁnite weighted
sum of normal distributions with the same diagonal covariance Σ and means µ+Φh that
are distributed over the subspace. The relationship between mixture models and factor
analysis is explored further in Figure 7.20.
7.6.2
Expectation maximization for learning factor analyzers
Since the factor analyzer can be expressed as a marginalization of a joint distribution
7.3
between the observed data x and a hidden variable h (Equation 7.33), it is possible to
use the EM algorithm to learn the parameters θ = {µ,Φ,Σ}. Once more, we follow the
recipe described in Section 7.3.
In the E-step (Figure 7.21), we optimize the bound with respect to the distributions
qi(hi). To do this we compute the posterior probability distribution Pr(hi|xi) over each
hidden variable hi given the associated observed data xi and the current values of the
parameters θ[t]. To this end we apply Bayes’ rule:
ˆq(hi) = Pr(hi|xi,θ[t])
(7.34)
= Pr(xi|hi,θ[t])Pr(hi)
Pr(xi|θ[t])
= Normxi[µ + Φhi,Σ]Normhi[0,I]
Pr(xi|θ[t])
= Normhi[(ΦT Σ−1Φ + I)−1ΦT Σ−1(xi −µ),(ΦT Σ−1Φ + I)],
where we have made use of the change of variables relation (Section 5.7) and then the
fact that the product of two normals is proportional to a third normal (Section 5.6).

7.6
Factor analysis
91
++ +
+
Figure 7.20 Relationship between factor analysis and mixtures of Gaussians. a) Consider an
MoG model where each component has identical diagonal covariance Σ. We could describe varia-
tion in a particular direction φ by parameterizing the mean of each Gaussian as µi = µ + φhi. b)
Different values of the scalar hidden variable hi determine different positions along direction φ.
c) Now we replace the MoG with an inﬁnite sum (integral) over a continuous family of Gaus-
sians, each of which is determined by a certain value of h. d) If we choose the prior over the
hidden variable to be normal, this integral has a closed form solution and is a factor analyzer. e)
More generally we want to describe variance in a set of directions Φ = [φ1,φ2,...,φK] in a high
dimensional space. f) To this end we use a K-dimensional hidden variable h and an associated
normal prior Pr(h).

92
7 Modeling complex data densities
Figure
7.21 E-step for expectation maximization algorithm for factor analysis. a) Two-
dimensional case with one factor. We are given a data point x (purple cross). b) In the E-step
we seek a distribution over possible values of the associated hidden variable h. It can be shown
that this posterior distribution over h is itself normally distributed. c) Three-dimensional case with
two factors. Given a data point x (purple cross), we aim to ﬁnd a distribution (d) over possible
values of the associated hidden variable h. Once more this posterior is normally distributed.
The resulting constant of proportionality exactly cancels out with the term Pr(x) in
the denominator, ensuring that the result is a valid probability distribution.
The E-step computes a probability distribution over the possible causes h for the
observed data. This implicitly deﬁnes a probability distribution over the positions Φh on
the subspace that might have generated this example.
We extract the following expectations from the posterior distribution (Equation 7.34)
as they will be needed in the M-step:
E[hi] = (ΦT Σ−1Φ + I)−1ΦT Σ−1(xi −µ)
E[hihT
i ] = E

(hi −E[hi])(hi −E[hi])T 
+ E[hi]E[hi]T
= (ΦT Σ−1Φ + I)−1 + E[hi]E[hi]T .
(7.35)
In the M-step, we optimize the bound with respect to the parameters θ = {µ,Φ,Σ}
so that
ˆθ
[t+1] = argmax
θ
" I
X
i=1
Z
ˆqi(hi)log[Pr(x,hi,θ)] dhi
#

7.7
Combining models
93
= argmax
θ
" I
X
i=1
Z
ˆqi(hi)[log[Pr(x|hi,θ)] + log[Pr(hi)]] dhi
#
= argmax
θ
" I
X
i=1
Z
ˆqi(hi)log[Pr(x|hi,θ)] dhi
#
= argmax
θ
" I
X
i=1
E [logPr(x|hi,θ)]
#
,
(7.36)
where we have removed the term log[Pr(hi)] as it is not dependent on the variables θ.
The expectations E[•] are taken with respect to the relevant posterior distributions
ˆqi(hi) = Pr(hi|xi,θ[t]). The expression for log[Pr(xi|hi)] is given by
logPr(xi|hi) = −D log[2π] + log[|Σ|] + (xi−µ−Φhi)T Σ−1(xi−µ−Φhi)
2
. (7.37)
We optimize Equation 7.36 by taking derivatives with respect to the parameters
θ = {µ,Φ,Σ}, equating the resulting expressions to zero and rearranging to yield
ˆµ =
PI
i=1 xi
I
ˆΦ =
 I
X
i=1
(xi −ˆµ)E[hi]T
! I
X
i=1
E[hihT
i ]
!−1
ˆΣ = 1
I
I
X
i=1
diag
h
(xi −ˆµ)T (xi −ˆµ) −ˆΦE[hi]xT i
,
(7.38)
where the function diag[•] is the operation of setting all elements of the matrix argument
to zero except those on the diagonal.
Figure 7.22 shows the parameters of a factor analysis model ﬁtted to the face data
using ten iterations of the EM algorithm. The different factors encode different modes of
variation of the data set, which often have real-world interpretations such as changes in
pose or lighting.
In conclusion, the factor analyzer is an efﬁcient model for capturing the covariance in
high-dimensional data. It devotes one set of parameters Φ to describing the directions in
which the data are most correlated and a second set Σ describes the remaining variation.
7.7
Combining models
The mixture of Gaussians, t-distribution, and factor analysis models are constructed sim-
ilarly: each is a weighted sum or integral of a set of constituent normal distributions.
Mixture of Gaussian models consist of a weighted sum of K normal distributions with
different means and variances. The t-distribution consists of an inﬁnite weighted sum
of normal distributions with the same mean, but different covariances. Factor analysis
models consist of an inﬁnite weighted sum of normal distributions with different means,
but the same diagonal covariance.
In light of these similarities, it is perhaps unsurprising then that the models can be
easily combined. If we combine mixture models and factor analyzers, we get a mixture

94
7 Modeling complex data densities
Figure 7.22 Factor analyzer with ten factors (four shown) for face classes. a) Mean µ for face
model. b) Diagonal covariance component Σ for face model. To visualize the effect of the ﬁrst
factor φ1 we add (c) or subtract (d) a multiple of it from the mean: we are moving along one axis
of the 10D subspace that seems to encode mainly the mean intensity. Other factors (e–j) encode
changes in the hue and the pose of the face.
of factor analyzers (MoFA) model. This is a weighted sum of factor analyzers, each
of which has a different mean and allocates high probability density to a different sub-
space. Similarly, combining mixture models and t-distributions results in a mixture of
t-distributions or robust mixture model. Combining t-distributions and factor analyzers,
we can construct a robust subspace model, which models data that lie primarily in a sub-
space but is tolerant to outliers. Finally, combining all three models, we get a mixture
of robust subspace models. This has the combined beneﬁts of all three approaches (it is
multimodal and robust and makes efﬁcient use of parameters). The associated density
function is
Pr(x) =
K
X
k=1
λkStudx
h
µk,ΦkΦT
k + Σk,νk
i
,
(7.39)
where µk,Φk and Σk represent the mean, factors and diagonal covariance matrix belong-
ing to the kth component, λk represents the weighting of the kth component and νk
represents the degrees of freedom of the kth component. To learn this model, we would
use a series of interleaved expectation maximization algorithms.
7.8
Expectation maximization in detail
Throughout this chapter, we have employed the expectation maximization algorithm,
using the recipe from Section 7.3.
We now examine the EM algorithm in detail to
understand why this recipe works.
The EM algorithm is used to ﬁnd maximum likelihood or MAP estimates of model
parameters θ where the likelihood Pr(x|θ) of the data x can be written as
Pr(x|θ) =
X
k
Pr(x,h = k|θ) =
X
k
Pr(x|h = k,θ)Pr(h = k)
(7.40)
and
Pr(x|θ) =
Z
Pr(x,h|θ) dh =
Z
Pr(x|h,θ)Pr(h) dh
(7.41)

7.8
Expectation maximization in detail
95
for discrete and continuous hidden variables, respectively. In other words, the likelihood
Pr(x|θ) is a marginalization of a joint distribution over the data and the hidden variables.
We will work with the continuous case.
The EM algorithm relies on the idea of a lower bounding function (or lower bound),
B[θ] on the log likelihood. This is a function of the parameters θ that is always guaran-
teed to be equal to or lower than the log likelihood. The lower bound is carefully chosen
so that it is easy to maximize with respect to the parameters.
This lower bound is also parameterized by a set of probability distributions
{qi(hi)}I
i=1 over the hidden variables, so we write it as B[{qi(hi)},θ]. Different prob-
ability distributions qi(hi) predict different lower bounds B[{qi(hi)},θ] and hence
different functions of θ that lie everywhere below the true log likelihood (Figure 7.5b).
In the EM algorithm, we alternate between expectation steps (E-steps) and maximiza-
tion steps (M-steps) where
• in the E-step (Figure 7.23a) we ﬁx θ and ﬁnd the best lower bound B[{qi(hi)},θ]
with respect to the distributions qi(hi). In other words, at iteration t
q[t]
i [hi] = argmax
qi[hi]
h
B[{qi(hi)},θ[t−1]]
i
.
(7.42)
The best lower bound will be a function that is as high as possible at the current
parameter estimates θ. Since it must be everywhere equal to or lower than the
log likelihood, the highest possible value is the log likelihood itself. So the bound
touches the log-likelihood curve for the current parameters θ.
• in the M-step (Figure 7.23b), we ﬁx qi(hi) and ﬁnd the values of θ that maximize
this bounding function B[{qi(hi)},θ]. In other words, we compute
θ[t] = argmax
θ
h
B[{q[t]
i (hi)},θ]
i
.
(7.43)
Figure 7.23 E-step and M-step. a) In the E-step, we manipulate the distributions {qi(hi)} to
ﬁnd the best new lower bound given parameters θ. This optimal lower bound will touch the log
likelihood at the current parameter values θ (we cannot do better than this!). b) In the M-step, we
hold {qi(hi)} constant and optimize θ with respect to the new bound.

96
7 Modeling complex data densities
Figure 7.24 Expectation maximization alg-
orithm. We iterate the expectation and max-
imization steps by alternately changing the
distributions qi(hi) and the parameter θ so
that the bound increases. In the E-step, the
bound is maximized with respect to qi(hi)
for ﬁxed parameters θ: the new function with
respect to θ touches the true log likelihood at
θ. In the M-step, we ﬁnd the maximum of
this function. In this way we are guaranteed
to reach a local maximum in the likelihood
function.
By iterating these steps, the (local) maximum of the actual log likelihood is approached
(Figure 7.24). To complete our picture of the EM algorithm, we must
• Deﬁne B[{qi(hi)},θ[t−1]] and show that it always lies below the log likelihood,
• Show which probability distribution qi(hi) optimizes the bound in the E-step,
• Show how to optimize the bound with respect to θ in the M-step.
These three issues are tackled in Sections 7.8.1, 7.8.2 and 7.8.3, respectively.
7.8.1
Lower bound for EM algorithm
We deﬁne the lower bound B[{qi(hi)},θ] to be
B[{qi(hi)},θ] =
I
X
i=1
Z
qi(hi)log
Pr(xi,hi|θ)
qi(hi)

dhi
≤
I
X
i=1
log
Z
qi(hi)Pr(xi,hi|θ)
qi(hi)
dhi

=
I
X
i=1
log
Z
Pr(xi,hi|θ) dhi

,
(7.44)
where we have used Jensen’s inequality between the ﬁrst and second lines. This states
that because the logarithm is a concave function, we can write
Z
Pr(y)log[y]dy ≤log
Z
yPr(y)dy

(7.45)
or E[log[y]] ≤log(E[y]).
Figure 7.25 illustrates Jensen’s inequality for a discrete
variable.

7.8
Expectation maximization in detail
97
Figure 7.25 Jensen’s inequality for the logarithmic function (discrete case). a) Taking a weighted
average of examples E[y] and passing them through the log function. b) Passing the samples
through the log function and taking a weighted average E[log[y]]. The latter case always produces
a smaller value than the former (E[log[y]] ≤log(E[y])): higher valued examples are relatively
compressed by the concave log function.
7.8.2
The E-Step
In the E-step, we update the bound B[{qi(hi)},θ] with respect to the distributions qi(hi).
To see how to do this, we manipulate the expression for the bound as follows:
B[{qi(hi)},θ] =
I
X
i=1
Z
qi(hi)log
Pr(xi,hi|θ)
qi(hi)

dhi
=
I
X
i=1
Z
qi(hi)log
Pr(hi|xi,θ)Pr(xi|θ)
qi(hi)

dhi
=
I
X
i=1
Z
qi(hi)log[Pr(xi|θ)] dhi−
I
X
i=1
Z
qi(hi)log

qi(hi)
Pr(hi|xi,θ)

dhi
=
I
X
i=1
log[Pr(xi|θ)] −
I
X
i=1
Z
qi(hi)log

qi(hi)
Pr(hi|xi,θ)

dhi,
(7.46)
where the hidden variables from the ﬁrst term are integrated out between the last two
lines. The ﬁrst term in this expression is constant with respect to the distributions qi(hi),
and so to optimize the bound we must ﬁnd the distributions ˆqi(hi) that satisfy
ˆqi(hi) = argmax
qi(hi)

−
Z
qi(hi)log

qi(hi)
Pr(hi|xi,θ)

dhi

= argmax
qi(hi)
Z
qi(hi)log
Pr(hi|xi,θ)
qi(hi)

dhi

= argmin
qi(hi)

−
Z
qi(hi)log
Pr(hi|xi,θ)
qi(hi)

dhi

.
(7.47)
This expression is known as the Kullback-Leibler divergence between qi(hi) and
Pr(hi|xi,θ). It is a measure of distance between probability distributions. We can use

98
7 Modeling complex data densities
the inequality log[y] ≤y −1 (plot the functions to convince yourself of this!) to show
that this cost function (including the minus sign) is always positive,
Z
qi(hi)log
Pr(hi|xi,θ)
qi(hi)

dhi ≤
Z
qi(hi)
Pr(hi|xi,θ)
qi(hi)
−1

dhi
=
Z
Pr(hi|xi,θ) −qi(hi) dhi
= 1 −1 = 0,
(7.48)
implying that when we reintroduce the minus sign the cost function must be positive. So
the criteria in Equation 7.46 will be maximized when the Kullback-Leibler divergence is
zero. This value is reached when qi(hi)=Pr(hi|xi,θ) so that
Z
qi(hi)log
Pr(hi|xi,θ)
qi(hi)

dhi =
Z
Pr(hi|xi,θ)log
Pr(hi|xi,θ)
Pr(hi|xi,θ)

dhi
=
Z
Pr(hi|xi,θ)log[1] dhi = 0.
(7.49)
In other words, to maximize the bound with respect to qi(hi), we set this to be the
posterior distribution Pr(hi|xi,θ) over the hidden variables hi given the current set of
parameters. In practice, this is computed using Bayes’ rule,
Pr(hi|xi,θ) = Pr(xi|hi,θ)Pr(hi)
Pr(xi)
.
(7.50)
So the E-step consists of computing the posterior distribution for each hidden variable
using Bayes’ rule.
7.8.3
The M-Step
In the M-step, we maximize the bound with respect to the parameters θ so that
θ[t] = argmax
θ
h
B[{q[t]
i (hi)},θ]
i
= argmax
θ
" I
X
i=1
Z
q[t]
i (hi)log
"
Pr(xi,hi|θ)
q[t]
i (hi)
#
dhi
#

7.9
Applications
99
= argmax
θ
" I
X
i=1
Z
q[t]
i (hi)log[Pr(xi,hi|θ)] −q[t]
i (hi)log
h
q[t]
i (hi)
i
dhi
#
= argmax
θ
" I
X
i=1
Z
q[t]
i (hi)log[Pr(xi,hi|θ)] dhi
#
,
(7.51)
where we have omitted the second term as it does not depend on the parameters. If you
look back at the algorithms in this chapter, you will see that we have maximized exactly
this criterion.
7.9
Applications
The models in this chapter have many uses in computer vision.
We now present a
cross-section of applications. As an example of two-class classiﬁcation using mixtures
of Gaussians densities, we reconsider the face detection application that has been a run-
ning theme throughout this chapter. To illustrate multiclass classiﬁcation, we describe an
object recognition model based on t-distributions. We also describe a segmentation appli-
cation which is an example of unsupervised learning: we do not have labeled training data
to build our model.
To illustrate the use of the factor analyzer for classiﬁcation, we present a face recog-
nition example. To illustrate its use for regression, we consider the problem of changing a
face image from one pose to another. Finally, we highlight the fact that hidden variables
can take on real-world interpretations by considering a model that explains the weak
spatial alignment of digits.
7.9.1
Face detection
In face detection, we attempt to infer a discrete label w ∈{0,1} indicating whether a
face is present or not based on observed data x. We will describe the likelihood for each
world state with a mixtures of Gaussians model, where the covariances of the Gaussian
components are constrained to be diagonal so that
Pr(x|w = m) =
K
X
k=1
λmkNormx[µkm,Σkm],
(7.52)
where m indexes the world state and k indexes the component of the mixture distribution.
We will assume that we have no prior knowledge about whether the face is present
or not so that Pr(w = 0) = Pr(w = 1) = 0.5. We ﬁt the two likelihood terms using a
set of labeled training pairs {xi,wi}. In practice this means learning one mixtures of
Gaussians model for the non-faces based on the data where wi = 0 and a separate model
for the faces based on the data where w = 1.
For test data x∗, we compute the posterior probability over w using Bayes’ rule:
Pr(w∗= 1|x∗) =
Pr(x∗|w∗= 1)Pr(w∗= 1)
P1
k=0 Pr(x∗|w∗= k)Pr(w∗= k)
.
(7.53)
Table 7.1 shows percent correct classiﬁcation for 100 test examples The results are
based on models learned from 1000 training examples of each class.

100
7 Modeling complex data densities
Color
Grayscale
Equalized
Single Gaussian
76%
79%
80%
Mixture of 10 Gaussians
81%
85%
89%
Table 7.1 Percent correct classiﬁcation rates for two different models and three different types
of preprocessing. In each case, the data x∗was assigned to be a face if the posterior probability
Pr(w = 1|x∗) was greater than 0.5.
The ﬁrst column shows results where the data vector consists of the RGB values
with a 24 × 24 region (the running example in this chapter used 60 × 60 pixel regions,
but this is unnecessarily large). The results are compared to classiﬁcation based on a
single normal distribution. The subsequent columns of the table show results for systems
trained and tested with grayscale 24×24 pixel regions and grayscale 24×24 regions that
have been histogram equalized (Section 13.1.2).
There are two insights to be gleaned from these classiﬁcation results. First, the choice
of model does make a difference; the mixtures of Gaussians density always results in
better classiﬁcation performance than the single Gaussian model. Second, the choice of
preprocessing is also critical to the ﬁnal performance. This book concerns models for
vision, but it is important to understand that this is not the only thing that determines the
ﬁnal performance of real-world systems. A brief summary of preprocessing methods is
presented in Chapter 13.
The reader should not depart with the impression that this is a sensible approach to
face detection. Even the best performance of 89% is far below what would be required
in a real face detector: consider that in a single image we might classify patches at 10000
different positions and scales, so an 11% error rate will be unacceptable. Moreover, eval-
uating each patch under both class conditional density functions is too computationally
expensive to be practical. In practice, face detection would normally be achieved using
discriminative methods (see Chapter 9).
7.9.2
Object recognition
In object recognition, the goal is to assign a discrete world vector wi ∈{1,2,...,M} indi-
cating which of M categories is present based on observed data xi from the ith image.
To this end, Aeschliman et al. (2010) split each image into 100 10 × 10 pixel regions
arranged in a regular grid. The grayscale pixel data xij from the jth region of the ith
image were concatenated to form a 100 × 1 vector xij. They treated the regions inde-
pendently and described each with a t-distribution so that the class conditional density
functions were
Pr(xi|w = m) =
J
Y
j=1
Studxij[µjm,Σjm,νjm].
(7.54)
Figure 7.26 shows results based on training with 10 classes from the Amsterdam
library of images (Guesebroek et al. 2005). Each class consists of 72 images taken at
5-degree intervals around the object. The data were divided randomly into 36 test images
and 36 training images for each class. The prior probabilities of the classes were set to

7.9
Applications
101
Figure 7.26 Object recognition. a) The training database consists of a series of different views
of ten different objects. The goal is to learn a class-conditional density function for each object
and classify new examples using Bayes’ rule. b) Percent correct results for class conditional den-
sities based on the t-distribution (top row) and the normal distributions (bottom row). The robust
model performs better, especially on objects with specularities. Images from Amsterdam library
(Guesebroek et al. 2005).
uniform, and the posterior distribution Pr(wi|xi) was calculated using Bayes’ rule. A
test object was classiﬁed according to the class with the highest posterior probability.
The results show the superiority of the t-distribution – for almost every class; the
percent correct performance is better, and this is especially true for objects such as the
china pig where the specularities act as outliers. By adding just one more parameter per
patch, the performance increases from a mean of 51% to 68%.
7.9.3
Segmentation
The goal of segmentation is to assign a discrete label {wn}N
n=1 which takes one of K
values wn ∈{1,2,...,K} to each of the N pixels in the image so that regions that belong
to the same object are assigned the same label. The segmentation model depends on
observed data vectors {xn}N
n=1 at each of the N pixels that would typically include the
RGB pixel values, the (x,y) position of the pixel and other information characterizing
local texture.
We will frame this problem as unsupervised learning. In other words, we do not have
the luxury of having training images where the state of the world is known. We must
both learn the parameters θ and estimate the world states {wi}I
i=1 from the image data
{xn}N
n=1.
We will assume that the kth object is associated with a normal distribution with
parameters µk and Σk and prevalence λk so that
Pr(wn) = Catwn[λ]
Pr(xi|wi = k) = Normxi[µk,Σk].
(7.55)
Marginalizing out the world state w, we have
Pr(x1...N) =
N
Y
n=1
K
X
k=1
λkNormxn[µk,Σk].
(7.56)
To ﬁt this model, we ﬁnd the parameters θ = {λk,µk,Σk}K
k=1 using the EM algo-
rithm. To assign a class to each pixel, we then ﬁnd the value of the world state that has

102
7 Modeling complex data densities
Figure 7.27 Segmentation. a–c) Original images. d–f) Segmentation results based on a mixture
of ﬁve normal distributions. The pixels associated with the kth component are colored with the
mean RGB values of the pixels that are assigned to this value. g–i) Segmentation results based on a
mixture of K t-distributions. The segmentation here is less noisy than for the MoG model. Results
from Sﬁkas et al. (2007). c⃝IEEE 2007.
the highest posterior probability given the observed data
ˆwi = argmax
wi
[Pr(wi|xi)],
(7.57)
where the posterior is computed as in the E-step.
Figure 7.27 shows results from this model and a similar mixture model based on t-
distributions from Sﬁkas et al. (2007). The mixture models manage to partition the image
quite well into different regions. Unsurprisingly, the t-distribution results are rather less
noisy than those based on the normal distribution.
7.9.4
Frontal face recognition
The goal of face identiﬁcation (Figure 7.28) is to assign a label w ∈{1...M} indicating
which of M possible identities the face belongs to based on a data vector x. The model is
learned from labeled training data {xi,wi}I
i=1 where the identity is known. In a simple
system, the data vector might consist of the concatenated grayscale values from the face
image, which should be reasonably large (say 50 × 50 pixels) to ensure that the identity
is well represented.
Since the data are high-dimensional, a reasonable approach is to model each class
conditional density function with a factor analyzer
Pr(xi|wi = k) = Normxi[µk,ΦkΦT
k + Σk],
(7.58)

7.9
Applications
103
Figure 7.28 Face recognition. Our goal is
to take the RGB values of a facial image
x and assign a label w ∈{1...K} corre-
sponding to the identity. Since the data are
high-dimensional, we model the class con-
ditional density function Pr(x|w = k) for
each individual in the database as a factor
analyzer. To classify a new face, we apply
Bayes’ rule with suitable priors Pr(w) to
compute the posterior distribution Pr(w|x).
We
choose
the
label
ˆw = argmaxw
[Pr(w = k|x)] that maximizes the poste-
rior. This approach assumes that there are
sufﬁcient training examples to learn a factor
analyzer for each class.
where the parameters for the kth identity θk = µk,Φk,Σk can be learned from the sub-
set of data that belongs to that identity using the EM algorithm. We also assign priors
P(w = k) according to the prevalence of each identity in the database.
To perform recognition, we compute the posterior distribution Pr(w∗|x∗) for the new
data example x∗using Bayes’ rule. We assign the identity that maximizes this posterior
distribution.
This approach works well if there are sufﬁcient examples of each gallery individual
to learn a factor analyzer, and if the poses of all of the faces are similar. In the next
example, we develop a method to change the pose of faces, so that we can cope when the
poses of the faces differ.
7.9.5
Changing face pose (regression)
To change the pose of the face, we predict the RGB values w of the face in the new pose
given a face x in the old pose. This is different from the previous examples in that it is a
regression problem: the output w is a continuous multidimensional variable rather than
a class label.
We form a compound variable z = [xT wT ]T by concatenating the RGB values from
the two poses together. We now model the joint density as Pr(z) = Pr(x,w) with a
factor analyzer
Pr(x,w) = Pr(z) = Normz[µ,ΦΦT + Σ],
(7.59)
which we learn for paired training examples {xi,wi}I
i=1 where the identity is known to
be the same for each pair.
To ﬁnd the non-frontal face w∗corresponding to a new frontal face x∗we use
the approach of Section 6.3.2: the posterior over w∗is just the conditional distribu-
tion Pr(w|x). Since the joint distribution is normal, we can compute the posterior
distribution in closed form using equation 5.5. Using the notation
µ =
µx
µw

and ΦΦT + Σ =
ΦxΦT
x + Σx
ΦxΦT
w
ΦwΦT
x
ΦwΦT
w + Σw

,
(7.60)

104
7 Modeling complex data densities
a)
c)
b)
e)
d)
h)
f)
g)
i)
Figure 7.29 Regression example: we aim to predict quarter-left face image w from the frontal
image x. To this end, we take paired examples of frontal faces (a–b) and quarter-left faces (c–d) and
learn a joint probability model Pr(x,w) by concatenating the variables to form z = [xT ,wT ]T
and ﬁtting a factor analyzer to z. e) Since the factor analyzer has a normal density (Equation 7.31),
we can predict the conditional distribution Pr(w|x) of a quarter-left face given a frontal face
which will also be normal (see section 5.5). f–g) Two frontal faces. h–i) MAP predictions for
non-frontal faces (the mean of the normal distribution Pr(w|x)).
Figure 7.30 Modeling transformations with hidden variables. a) The original set of digit images
are only weakly aligned. b) The mean and standard deviation images are consequently blurred
out. The probability density model does not ﬁt well. c) Each possible value of a discrete hidden
variable represents a different transformation (here inverse transformations are shown). The red
square highlights the most likely choice of hidden variable after ten iterations. d) The inversely
transformed digits (based on most likely hidden variable). d) The new mean and standard deviation
images are more focused: the probability density function ﬁts better.
the posterior is given by
Pr(w∗|x∗) = Normw∗[µw + ΦwΦT
x (ΦxΦT
x + Σx)−1(x∗−µx),
(7.61)
ΦwΦT
w + Σw −ΦwΦT
x (ΦxΦT
x + Σx)−1ΦxΦT
w],
and the most probable w∗is at the mean of this distribution.
7.9.6
Transformations as hidden variables
Finally, we consider a model that is closely related to the mixture of Gaussians, but
where the hidden variables have a clear real-world interpretation. Consider the problem
of building a density function for a set of poorly aligned images of digits (Figure 7.30).
A simple normal distribution with diagonal covariance produces only a very poor
representation of the data because most of the variation is due to the poor alignment.
We construct a generative model that ﬁrst draws an aligned image x′ from a normal
distribution, and then translates it using one of a discrete set {transk[•]}K
k=1 of K possible

Notes
105
transformations to explain the poorly aligned image x. In mathematical terms, we have
Pr(x′) = Normx′[µ,Σ]
Pr(h) = Cath[λ]
x = transh[x′],
(7.62)
where h ∈{1,...,K} is a hidden variable that denotes which of the possible transforma-
tions warped this example.
This model can be learned using an EM-like procedure. In the E-step, we compute a
probability distribution Pr(hi|xi) over the hidden variables by applying all of the inverse
transformations to the each example and evaluating how likely the result is under the cur-
rent parameters µ and Σ. In the M-step, we update these parameters by taking weighted
sums of the inverse transformed images.
Summary
In this chapter we have introduced the idea of the hidden variable to induce structure in
density models. The main approach to learning models of this kind is the expectation
maximization algorithm. This is an iterative approach that is only guaranteed to ﬁnd a
local maximum. We have seen that although these models are more sophisticated than
the normal distribution, they are still not really good representations of the density of
high-dimensional visual data.
Notes
Expectation maximization: The EM algorithm was originally described by Dempster et al. (1977)
although the presentation in this chapter owes more to the perspective espoused in Neal and Hin-
ton (1999). A comprehensive summary of the EM algorithm and its extensions can be found in
McLachlan and Krishnan (2008).
Mixtures of Gaussians: The mixtures of Gaussians model is closely related to the K-means algo-
rithm (discussed in Section 13.4.4), which is a pure clustering algorithm, but does not have a
probabilistic interpretation. Mixture models are used extensively in computer vision. Common
applications include skin detection (e.g., Jones and Rehg 2002) and background subtraction (e.g.,
Stauffer and Grimson 1999).
t-distributions: General information about the t-distribution can be found in Kotz and Nadarajah
(2004). The EM algorithm for ﬁtting the t-distribution is given in Liu and Rubin (1995), and
other ﬁtting methods are discussed in Nadarajah and Kotz (2008) and Aeschliman et al. (2010).
Applications of t-distributions in vision include object recognition (Aeschliman et al. 2010) and
tracking (Loxam and Drummond 2008; Aeschliman et al. 2010), and they are also used as the
building blocks of sparse image priors (Roth and Black 2009).
Subspace models: The EM algorithm to learn the factor analyzer is due to Rubin and Thayer
(1982). Factor analysis is closely related to several other models including probabilistic prin-
cipal component analysis (Tipping and Bishop 1999), which is discussed in Section 17.5.1,
and principal component analysis, which is discussed in Section 13.4.2. Subspace models have
been extended to the nonlinear case by Lawrence (2004). This is discussed in detail in Section 17.8.
In Chapter 18, we present a series of models based on factor analysis which explicitly encode the
identity and style of the object.
Combining models: Ghahramani and Hinton (1996c) introduced the mixtures of factor analyz-
ers model. Peel and McLachlan (2000) present an algorithm for learning robust mixture models
(mixtures of t-distributions). Khan and Dellaert (2004) and Zhao and Jiang (2006) both present

106
7 Modeling complex data densities
subspace models based on the t-distribution. De Ridder and Franc (2003) combined mixture mod-
els, subspace models and t-distributions to create a distribution that is mutli-modal, robust and
oriented along a subspace.
Face detection, face recognition, object recognition: Models based on subspace distributions
were used in early methods for face and object recognition (e.g., Moghaddam and Pentland
1997; Murase and Nayar 1995). Modern face detection methods mainly rely on discriminative
methods (Chapter 9), and the current state of the art in object recognition relies on bag of words
approaches (chapter 20). Face recognition applications do not usually have the luxury of hav-
ing many examples of each individual and so cannot build a separate density for each. However,
modern approaches are still largely based on subspace methods (see Chapter 18).
Segmentation: Belongie et al. (1998) use a mixtures of Gaussians scheme similar to that described
to segment the image as part of a content-based image retrieval system. A modern approach to
segmentation based on the mixture of Gaussians model can be found in Ma et al. (2007). Sﬁkas
et al. (2007) compared the segmentation performance of mixtures of Gaussians and mixtures of
t-distributions.
Other uses for hidden variables: Frey and Jojic (1999a), (1999b) used hidden variables to model
unseen transformations applied to data from mixture models and subspace models, respectively.
Jojic and Frey (2001) used discrete hidden variables to represent the index of the layer in a multi-
layered model of a video. Jojic et al. (2003) presented a structured mixture model, where the mean
and variance parameters are represented as an image and the hidden variable indexes the starting
position of subpatch from this image.
Problems
7.1 Consider a computer vision system for machine inspection of oranges in which the goal is to
tell if the orange is ripe. For each image we separate the orange from the background and calculate
the average color of the pixels, which we describe as a 3 × 1 vector x. We are given training pairs
{xi,wi} of these vectors, each with an associated binary variable w ∈{0,1} that indicates that this
training example was unripe (w = 0) or ripe (w = 1). Describe how to build a generative classiﬁer
that could classify new examples x∗as being ripe or unripe.
7.2 It turns out that a small subset of the training labels wi in the previous example were wrong.
How could you modify your classiﬁer to cope with this situation?
7.3 Derive the M-step equations for the mixtures of Gaussians model (Equation 7.19).
7.4 Consider modeling some univariate continuous visual data x ∈[0,1] using a mixture of beta
distributions. Write down an equation for this model. Describe in words what will occur in (i) the
E-step and (ii) the M-step.
7.5 Prove that the student t-distribution over x is the marginalization with respect to h of the joint
distribution Pr(x,h) between x and a hidden variable h where
Studx[µ,σ2,ν] =
Z
Normx[µ,σ2/ν]Gamh[ν/2,ν/2]dh.
7.6 Show that the peak of the Gamma distribution Gamz[α,β] is at
ˆz = α −1
β
.

Problems
107
7.7 Show that the Gamma distribution is conjugate to the inverse scaling factor of the variance in
a normal distribution so that
Normxi[µ,Σ/hi]Gamhi[ν/2,ν/2] = κGamhi[α,β],
and ﬁnd the constant of proportionality κ and the new parameters α and β.
7.8 The model for factor analysis can be written as
xi = µ + Φhi + ϵi,
where hi is distributed normally with mean zero and identity covariance and ϵi is distributed
normally with mean zero and covariance Σ. Determine expressions for
1. E[xi],
2. E[(xi −E[xi])2].
7.9 Derive the E-step for factor analysis (Equation 7.34).
7.10 Derive the M-step for factor analysis (Equation 7.38).

Chapter 8
Regression models
This chapter concerns regression problems: the goal is to estimate a univariate world
state w based on observed measurements x. The discussion is limited to discrimina-
tive methods in which the distribution Pr(w|x) of the world state is directly modeled.
This contrasts with Chapter 7 where the focus was on generative models in which the
likelihood Pr(x|w) of the observations is modeled.
To motivate the regression problem, consider body pose estimation: here the goal is
to estimate the joint angles of a human body, based on an observed image of the person in
an unknown pose (Figure 8.1). Such an analysis could form the ﬁrst step toward activity
recognition.
We assume that the image has already been preprocessed and a low-dimensional
vector x that represents the shape of the contour has been extracted. Our goal is to
use this data vector to predict a second vector containing the joint angles for each of the
major body joints. In practice, we will estimate each joint angle separately; we can hence
concentrate our discussion on how to estimate a univariate quantity w from continuous
observed data x. We begin by assuming that the relation between the world and the
data is linear and that the uncertainty around this prediction is normally distributed with
constant variance. This is the linear regression model.
8.1
Linear regression
The goal of linear regression is to predict the posterior distribution Pr(w|x) over the
world state w based on observed data x. Since this is a discriminative model, we pro-
ceed by choosing a probability distribution over the world w and making the parameters
dependent on the data x. The world state w is univariate and continuous and so a suitable
distribution is the univariate normal. In linear regression (Figure 8.2), we make the mean
µ of this normal distribution a linear function φ0+φT xi of the data and treat the variance
σ2 as a constant so that
Pr(wi|xi,θ) = Normwi
h
φ0 + φT xi,σ2i
,
(8.1)
where θ = {φ0,φ,σ2} are the model parameters. The term φ0 can be interpreted as the
y-intercept of a hyperplane and the entries of φ = [φ1,φ2,...,φD]T are its gradients with
respect to each of the D data dimensions.

8.1
Linear regression
109
Figure 8.1 Body pose estimation.
a–b)
Human beings in unknown poses. c–d) The
silhouette is found by segmenting the image
and the contour extracted by tracing around
the edge of the silhouette.
A 100 dimen-
sional measurement vector x is extracted that
describes the contour shape based on the
shape context descriptor (see Section 13.3.5).
e–f) The goal is to estimate the vector w con-
taining the major joint angles of the body.
This is a regression problem as each element
of the world state w is continuous. Adapted
from Agarwal and Triggs (2006).
It is cumbersome to treat the y-intercept separately from the gradients, so we apply
a trick that allows us to simplify the subsequent notation. We attach a 1 to the start of
every data vector xi ←[1
xT
i ]T and attach the y-intercept φ0 to the start of the gradient
vector φ ←[φ0
φT ]T so that we can now equivalently write
Pr(wi|xi,θ) = Normwi
h
φT xi,σ2i
.
(8.2)
In fact, since each training data example is considered independent, we can write the
probability Pr(w|X) of the entire training set as a single normal distribution with a
diagonal covariance so that
Pr(w|X) = Normw[XT φ,σ2I],
(8.3)
where X = [x1,x2,...,xI] and w = [w1,w2,...,wI]T .
Inference for this model is very simple: for a new datum x∗we simply evaluate
Equation 8.2 to ﬁnd the posterior distribution Pr(w∗|x∗) over the world state w∗. Hence
we turn our main focus to learning.
8.1.1
Learning
The learning algorithm estimates the model parameters θ = {φ,σ2} from paired training
8.1
examples {xi,wi}I
i=1. In the maximum likelihood approach we seek
ˆθ = argmax
θ
[Pr(w|X,θ)]
= argmax
θ
[log[Pr(w|X,θ)]],
(8.4)
where as usual we have taken the logarithm of the criterion. The logarithm is a monotonic
transformation, and so it does not change the position of the maximum, but the resulting
cost function is easier to optimize. Substituting in we ﬁnd that
ˆφ, ˆσ2 = argmax
φ,σ2

−I log[2π]
2
−I log[σ2]
2
−(w −XT φ)T (w −XT φ)
2σ2

.
(8.5)

110
8 Regression models
Figure 8.2 Linear regression model with univariate data x. a) We choose a univariate normal
distribution over the world state w. b) The parameters of this distribution are now made to depend
on the data x: the mean µ is a linear function φ0 +φ1x of the data and the variance σ2 is constant.
The parameters φ0 and φ1 represent the intercept and slope of the linear function, respectively.
We now take the derivatives with respect to φ and σ2, equate the resulting expressions to
zero and solve to ﬁnd
ˆφ = (XXT )−1Xw
ˆσ2 = (w −XT φ)T (w −XT φ)
I
.
(8.6)
Figure 8.2b shows an example ﬁt with univariate data x. In this case, the model
describes the data reasonably well.
8.1.2
Problems with the linear regression model
There are three main limitations of the linear regression model.
• The predictions of the model are overconﬁdent; for example, small changes in
the estimated slope φ1 make increasingly large changes in the predictions as we
move further from the y-intercept φ0. However, this increasing uncertainty is not
reﬂected in the posterior distribution.
• We are limited to linear functions and usually there is no particular reason that the
visual data and world state should be linearly related.
• When the observed data x is high-dimensional, it may be that many elements of
this variable aren’t useful for predicting the state of the world, and so the resulting
model is unnecessarily complex.
We tackle each of these problems in turn. In the following section we address the over-
conﬁdence of the model by developing a Bayesian approach to the same problem. In
Section 8.3, we generalize this model to ﬁt nonlinear functions. In section 8.6, we intro-
duce a sparse version of the regression model where most of the weighting coefﬁcients
φ are encouraged to be zero. The relationships between the models in this chapter are
indicated in Figure 8.3.

8.2
Bayesian linear regression
111
f)
Figure 8.3 Family of regression models. There are several limitations to linear regression which
we deal with in subsequent sections. The linear regression model with maximum likelihood learn-
ing is overconﬁdent, and hence we develop a Bayesian version. It is unrealistic to always assume
a linear relationship between the data and the world and to this end, we introduce a nonlinear ver-
sion. The linear regression model has many parameters when the data dimension is high, and hence
we consider a sparse version of the model. The ideas of Bayesian estimation, nonlinear functions
and sparsity are variously combined to form the Gaussian process regression and relevance vector
regression model.
8.2
Bayesian linear regression
In the Bayesian approach, we compute a probability distribution over possible values of
8.2
the parameters φ (we will assume for now that σ2 is known, see Section 8.2.2). When
we evaluate the probability of new data, we take a weighted average of the predictions
induced by the different possible values.
Since the gradient vector φ is multivariate and continuous, we model the prior Pr(φ)
as normal with zero mean and spherical covariance,
Pr(φ) = Normφ[0,σ2
pI],
(8.7)
where σ2
p scales the prior covariance and I is the identity matrix. Typically σ2
p is set to a
large value to reﬂect the fact that our prior knowledge is weak.
Given paired training examples {xi,wi}I
i=1, the posterior distribution over the
parameters can be computed using Bayes’ rule
Pr(φ|X,w) = Pr(w|X,φ)Pr(φ)
Pr(w|X)
,
(8.8)
where, as before, the likelihood is given by
Pr(w|X,θ) = Normw

XT φ,σ2I

.
(8.9)

112
8 Regression models
b)
a)
Figure 8.4 Bayesian linear regression. a) Prior Pr(φ) over intercept φ0 and slope φ1 parameters.
This represents our knowledge about the parameters before we observe the data. b) Posterior dis-
tribution Pr(φ|X,w) over intercept and slope parameters. This represents our knowledge about
the parameters after observing the data from Figure 8.2b: we are considerably more certain but
there remain a range of possible parameter values.
The posterior distribution can be computed in closed form (using the relations in
Sections 5.7 and 5.6) and is given by the expression:
Pr(φ|X,w) = Normφ
 1
σ2 A−1Xw,A−1

,
(8.10)
where
A = 1
σ2 XXT + 1
σ2p
I.
(8.11)
Note that the posterior distribution Pr(φ|X,w) is always narrower than the prior distri-
bution Pr(φ) (Figure 8.4); the data provides information that reﬁnes our knowledge of
the parameter values.
We now turn to the problem of computing the predictive distribution over the world
state w∗for a new observed data vector x∗. We take an inﬁnite weighted sum (i.e.,
an integral) over the predictions Pr(w∗|x∗,φ) implied by each possible φ where the
weights are given by the posterior distribution Pr(φ|X,w).
Pr(w∗|x∗,X,w) =
Z
Pr(w∗|x∗,φ)Pr(φ|X,w)dφ
=
Z
Normw∗[φT x∗,σ2]Normφ
 1
σ2 A−1Xw,A−1

dφ
= Normw∗
 1
σ2 x∗T A−1Xw,x∗T A−1x∗+ σ2

.
(8.12)
To compute this, we reformulated the integrand using the relations from Sections 5.7
and 5.6 as the product of a normal distribution in φ and a constant with respect to φ. The

8.2
Bayesian linear regression
113
Figure 8.5 Bayesian linear regression.
a) In learning we compute the posterior distribution
Pr(φ|X,w) over the intercept and slope parameters: there is a family of parameter settings that
are compatible with the data. b) Three samples from the posterior, each of which corresponds to
a different regression line. c) To form the predictive distribution we take an inﬁnite weighted sum
(integral) of the predictions from all of the possible parameter settings, where the weight is given
by the posterior probability. The individual predictions vary more as we move from the centroid x
and this is reﬂected in the fact that the certainty is lower on either side of the plot.
integral of the normal distribution must be one, and so the ﬁnal result is just the constant.
This constant is itself a normal distribution in w∗.
This Bayesian formulation of linear regression (Figure 8.5) is less conﬁdent about
its predictions, and the conﬁdence decreases as the test data x∗departs from the mean
x of the observed data. This is because uncertainty in the gradient causes increasing
uncertainty in the predictions as we move further away from the bulk of the data. This
agrees with our intuitions: predictions ought to become less conﬁdent as we extrapolate
further from the data.
8.2.1
Practical concerns
To implement this model we must compute the D × D matrix inverse A−1 (Equa-
tion 8.12). If the dimension D of the original data is large, then it will be difﬁcult to
compute this inverse directly.
Fortunately, the structure of A is such that it can be inverted far more efﬁciently. We
exploit the Woodbury identity (see appendix C.8.4), to rewrite A−1 as
A−1 =
 1
σ2 XXT + 1
σ2p
ID
−1
= σ2
pID −σ2
pX

XT X + σ2
σ2p
II
−1
XT ,
(8.13)
where we have explicitly noted the dimensionality of each of the identity matrices I
as a subscript. This expression still includes an inversion, but now it is of size I × I
where I is the number of examples: if the number of data examples I is smaller than
the data dimensionality D, then this formulation is more practical. This formulation also
demonstrates clearly that the posterior covariance is less than the prior; the posterior
covariance is the prior covariance σ2
pI with a data-dependent term subtracted from it.

114
8 Regression models
Substituting the new expression for A−1 into Equation 8.12, we derive a new
expression for the predictive distribution,
Pr(w∗|x∗,X,w)
(8.14)
= Normw∗
"
σ2
p
σ2 x∗T Xw −σ2
p
σ2 x∗T X

XT X + σ2
σ2p
I
−1
XT Xw,
σ2
px∗T x∗−σ2
px∗T X

XT X + σ2
σ2p
I
−1
XT x∗+ σ2
#
.
It is notable that only inner products of the data vectors (e.g., in the terms XT x∗, or
XT X) are required to compute this expression.
We will exploit this fact when we
generalize these ideas to nonlinear regression (Section 8.3).
8.2.2
Fitting the variance
The previous analysis has concentrated exclusively on the slope parameters φ. In prin-
ciple, we could have taken a Bayesian approach to estimating the variance parameter σ2
as well. However, for simplicity we will compute a point estimate of σ2 using the max-
imum likelihood approach. To this end, we optimize the marginal likelihood, which is
the likelihood after marginalizing out φ and is given by
Pr(w|X,σ2) =
Z
Pr(w|X,φ,σ2)Pr(φ) dφ
=
Z
Normw[XT φ,σ2I]Normφ[0,σ2
pI] dφ
= Normw[0,σ2
pXT X + σ2I]
(8.15)
where the integral was solved using the same technique as that used in equation 8.12.
To estimate the variance, we maximize the log of this expression with respect to σ2.
Since the unknown is a scalar it is straightforward to optimize this function by simply
evaluating the function over a range of values and choosing the maximum. Alternatively,
we could use a general purpose nonlinear optimization technique (see Appendix B).
8.3
Nonlinear regression
It is unrealistic to assume that there is always a linear relationship between the world state
w and the input data x. In developing an approach to nonlinear regression, we would like
to retain the mathematical convenience of the linear model while extending the class of
functions that can be described.
Consequently, the approach that we describe is extremely simple: we ﬁrst pass each
data example through a nonlinear transformation
zi = f[xi],
(8.16)
to create a new data vector zi which is usually higher dimensional than the original data.
Then we proceed as before: we describe the mean of the posterior distribution Pr(wi|xi)
as a linear function φT zi of the transformed measurements so that
Pr(wi|xi) = Normwi[φT zi,σ2].
(8.17)

8.3
Nonlinear regression
115
For example, consider the case of 1D polynomial regression:
Pr(wi|xi) = Normwi[φ0 + φ1xi + φ2x2
i + φ3x3
i ,σ2].
(8.18)
This model can be considered as computing the nonlinear transformation
zi =


1
xi
x2
i
x3
i

,
(8.19)
and so it has the general form of Equation 8.17.
8.3.1
Maximum likelihood
To ﬁnd the maximum likelihood solution for the gradient vector, we ﬁrst combine all of
the transformed training data relations (Equation 8.17) into a single expression:
Pr(w|X) = Normw[ZT φ,σ2I].
(8.20)
The optimal weights can now be computed as
ˆφ = (ZZT )−1Zw
ˆσ2 = (w −ZT φ)T (w −ZT φ)
I
,
(8.21)
where the matrix Z contains the transformed vectors {zi}I
i=1 in its columns. These
equations were derived by replacing the original data term X by the transformed data Z
in the equivalent linear expressions (Equation 8.6). For a new observed data example x∗,
we compute the vector z∗and then evaluate Equation 8.17.
Figures 8.6 and 8.7 provide two more examples of this approach. In Figure 8.6, the
new vector z is computed by evaluating the data x under a set of radial basis functions:
zi =


1
exp

−(xi −α1)2/λ

exp

−(xi −α2)2/λ

exp

−(xi −α3)2/λ

exp

−(xi −α4)2/λ

exp

−(xi −α5)2/λ

exp

−(xi −α6)2/λ



.
(8.22)
The term radial basis functions can be used to denote any spherically symmetric function,
and here we have used the Gaussian. The parameters {αk}K
k=1 are the centers of the
functions, and λ is a scaling factor that determines their width. The functions themselves
are shown in Figure 8.6b. Because they are spatially localized, each one accounts for a
part of the original data space. We can approximate a function by taking weighted sums
φT z of these functions. For example, when they are weighted as in Figure 8.6c, they
create the function in Figure 8.6a.

116
8 Regression models
Figure 8.6 Nonlinear regression using radial basis functions. a) The relationship between the
data x and world w is clearly not linear. b) We compute a new seven-dimensional vector z by
evaluating the original observation x against each of six radial basis functions (Gaussians) and a
constant function (black line). c) The mean of the predictive distribution (red line in (a)) can be
formed by taking a linear sum φT z of these seven functions where the weights are as shown. The
weights are estimated by maximum likelihood estimation of the linear regression model using the
nonlinearly transformed data z instead of the original data x. d) The ﬁnal distribution Pr(w|x)
has a mean that is a sum of these functions and constant variance σ2.
In Figure 8.7 we compute a different nonlinear transformation and regress against the
same data. This time, the transformation is based on arc tangent functions so that
zi =


arctan[λxi −α1]
arctan[λxi −α2]
arctan[λxi −α3]
arctan[λxi −α4]
arctan[λxi −α5]
arctan[λxi −α6]
arctan[λxi −α7]


.
(8.23)
Here, the parameter λ controls the speed with which the function changes and
the parameters {αm}7
m=1 determine the horizontal offsets of the arc tangent
functions.
In this case, it is harder to understand the role of each weighted arc tangent func-
tion in the ﬁnal regression, but nonetheless they collectively approximate the function
well.

8.3
Nonlinear regression
117
Figure 8.7 Nonlinear regression using arc tangent functions. a) The relationship between the data
x and world w is not linear. b) We compute a new seven-dimensional vector z by evaluating the
original observation x against each of seven arc tangent functions. c) The mean of the predictive
distribution (red line in (a)) can be formed by taking a linear sum of these seven functions weighted
as shown. The optimal weights were established using the maximum likelihood approach. d) The
ﬁnal distribution Pr(w|x) has a mean that is a sum of these weighted functions and constant
variance.
8.3.2
Bayesian nonlinear regression
In the Bayesian solution, the weights φ of the nonlinear basis functions are treated as
uncertain: in learning we compute the posterior distribution over these weights. For
a new observation x∗, we compute the transformed vector z∗and compute an inﬁnite
weighted sum over the predictions due to the possible parameter values (Figure 8.8). The
new expression for the predictive distribution is
Pr(w∗|z∗,X,w)
(8.24)
= Normw
"
σ2
p
σ2 z∗T Zw −σ2
p
σ2 z∗T Z

ZT Z + σ2
σ2p
I
−1
ZT Zw,
σ2
pz∗T z∗−σ2
pz∗T Z

ZT Z + σ2
σ2p
I
−1
ZT z∗+ σ2
#
,

118
8 Regression models
Figure 8.8 Bayesian nonlinear regression using radial basis functions.
a) The relationship
between the data and measurements is nonlinear. b) As in Figure 8.6, the mean of the predic-
tive distribution is constructed as a weighted linear sum of radial basis functions. However, in the
Bayesian approach we compute the posterior distribution over the weights φ of these basis func-
tions. c) Different draws from this distribution of weight parameters result in different predictions.
d) The ﬁnal predictive distribution is formed from an inﬁnite weighted average of these predictions
where the weight is given by the posterior probability. The variance of the predictive distribution
depends on both the mutual agreement of these predictions and the uncertainty due to the noise
term σ2. The uncertainty is greatest in the region on the right where there is little data and so the
individual predictions vary widely.
where we have simply substituted the transformed vectors z for the original data x in
Equation 8.14. The prediction variance depends on both the uncertainty in φ and the
additive variance σ2. The Bayesian solution is less conﬁdent than the maximum likeli-
hood solution (compare Figures 8.8d and 8.7d), especially in regions where the data is
sparse.
To compute the additive variance σ2 we again optimize the marginal likelihood. The
expression for this can be found by substituting Z for X in Equation 8.15.
8.4
Kernels and the kernel trick
The Bayesian approach to nonlinear regression described in the previous section is
rarely used directly in practice: the ﬁnal expression for the predictive distribution

8.5
Gaussian process regression
119
(Equation 8.24) relies on computing inner products zT
i zj. However, when the trans-
formed space is high-dimensional, it may be costly to compute the vectors zi = f[xi] and
zj = f[xj] explicitly and then compute the inner product zT
i zj.
An alternative approach is to use kernel substitution in which we directly deﬁne a
single kernel function k[xi,xj] as a replacement for the operation f[xi]T f[xj]. For many
transformations f[•], it is more efﬁcient to evaluate the kernel function directly than to
transform the variables separately and then compute the dot product.
Taking this idea one step further, it is possible to choose a kernel function k[xi,xj]
with no knowledge of what transformation f[•] it corresponds to. When we use kernel
functions, we no longer explicitly compute the transformed vector z. One advantage of
this is we can deﬁne kernel functions that correspond to projecting the data into very
high-dimensional or even inﬁnite spaces. This is sometimes called the kernel trick.
Clearly, the kernel function must be carefully chosen so that it does in fact correspond
to computing some function z = f[x] for each data vector and taking the inner product
of the resulting values: for example, since zT
i zj = zT
j zi the kernel function must treat its
arguments symmetrically so that k[xi,xj] = k[xj,xi].
More precisely, Mercer’s theorem states that a kernel function is valid when the ker-
nel’s arguments are in a measurable space, and the kernel is positive semi-deﬁnite so
that
X
ij
k[xi,xj]aiaj ≥0
(8.25)
for any ﬁnite subset {xn}N
n=1 of vectors in the space and any real numbers {an}N
n=1.
Examples of valid kernels include
• Linear
k[xi,xj] = xT
i xj,
(8.26)
• Degree p polynomial
k[xi,xj] = (xT
i xj + 1)p,
(8.27)
• Radial basis function (RBF) or Gaussian
k[xi,xj] = exp

−0.5
(xi −xj)T (xi −xj)
λ2

.
(8.28)
The last of these is particularly interesting. It can be shown that this kernel function
corresponds to computing inﬁnite length vectors z and taking their dot product. The
entries of z correspond to evaluating a radial basis function (Figure 8.6b) at every possible
point in the space of x.
It is also possible to create new kernels by combining two or more existing ker-
nels. For example, sums and products of valid kernels are guaranteed to be positive
semideﬁnite and so are also valid kernels.
8.5
Gaussian process regression
We now replace the inner products zT
i zj in the nonlinear regression algorithm (Equa-
8.3
tion 8.24) with kernel functions.
The resulting model is termed Gaussian process

120
8 Regression models
Figure 8.9 Gaussian process regression using an RBF kernel. a) When the length scale parameter
λ is large, the function is too smooth. b) For small values of the length parameter the model
does not successfully interpolate between the examples. c) The regression using the maximum
likelihood length scale parameter is neither too smooth nor disjointed.
regression. The predictive distribution for a new datum x∗is
Pr(w∗|x∗,X,w)
(8.29)
= Normw∗
"
σ2
p
σ2 K[x∗,X]w −σ2
p
σ2 K[x∗,X]

K[X,X] + σ2
σ2p
I
−1
K[X,X]w,
σ2
pK[x∗,x∗] −σ2
pK[x∗,X]

K[X,X] + σ2
σ2p
I
−1
K[X,x∗] + σ2
#
.
where the notation K[X,X] represents a matrix of dot products where element (i,j) is
given by k[xi,xj].
Note that kernel functions may also contain parameters. For example, the RBF kernel
(Equation 8.28) takes the parameter λ, which determines the width of the underlying RBF
functions and hence the smoothness of the function (Figure 8.9). Kernel parameters such
as λ can be learned by maximizing the marginal likelihood
ˆλ = argmax
λ

Pr(w|X,σ2)

= argmax
λ
Z
Pr(w|X,φ,σ2)Pr(φ)dφ

= argmax
λ

Normw[0,σ2
pK[X,X] + σ2I]

.
(8.30)
This typically requires a nonlinear optimization procedure.
8.6
Sparse linear regression
We now turn our attention to the third potential disadvantage of linear regression. It
8.4
is often the case that only a small subset of the dimensions of x are useful for pre-
dicting w. However, without modiﬁcation, the linear regression algorithm will assign
nonzero values to the gradient φ in these directions. The goal of sparse linear regression
is to adapt the algorithm to ﬁnd a gradient vector φ where most of the entries are zero.
The resulting classiﬁer will be faster, since we no longer even have to make all of the

8.6
Sparse linear regression
121
Figure
8.10 A product of two 1D t-
distributions where each has small degrees
of freedom ν. This 2D distribution favors
sparseness (where one or both variables are
close to zero).
In higher dimensions, the
product of t-distributions encourages solu-
tions where most variables are set to zero.
Note that the product of 1D distributions is
not the same as a multivariate t-distribution
with a spherical covariance matrix, which
looks like a multivariate normal distribution
but with longer tails.
measurements. Furthermore, simpler models are preferable to complex ones; they cap-
ture the main trends in the data without overﬁtting to peculiarities of the training set and
generalize better to new test examples.
To encourage sparse solutions, we impose a penalty for every nonzero weighted
dimension. We replace the normal prior over the gradient parameters φ=[φ1,φ2,...,
φD]T with a product of one-dimensional t-distributions so that
Pr(φ) =
D
Y
d=1
Studφd [0,1,ν]
=
D
Y
d=1
Γ
  ν+1
2

√νπΓ
  ν
2


1 + φ2
d
ν
−(ν+1)/2
.
(8.31)
The product of univariate t-distributions has ridges of high probability along the coor-
dinate axes, which encourages sparseness (see Figure 8.10). We expect the ﬁnal solution
to be a trade-off between ﬁtting the training data accurately and the sparseness of φ (and
hence the number of training data dimensions that contribute to the solution).
Adopting the Bayesian approach, our aim is to compute the posterior distribution
Pr(φ|X,w,σ2) over the possible values of the gradient variable φ using this new prior
so that
Pr(φ|X,w,σ2) = Pr(w|X,φ,σ2)Pr(φ)
Pr(w|X,σ2)
(8.32)
Unfortunately, there is no simple closed form expression for the posterior on the left-hand
side. The prior is no longer normal, and the conjugacy relationship is lost.
To make progress, we reexpress each t-distribution as an inﬁnite weighted sum of
normal distributions where a hidden variable hd determines the variance (Section 7.5),

122
8 Regression models
so that
Pr(φ) =
D
Y
d=1
Z
Normφd[0,1/hd]Gamhd[ν/2,ν/2] dhd
=
Z
Normφ[0,H−1]
D
Y
d=1
Gamhd[ν/2,ν/2] dH,
(8.33)
where the matrix H contains the hidden variables {hd}D
d=1 on its diagonal and zeros
elsewhere. We now write out the expression for the marginal likelihood (likelihood after
integrating over the gradient parameters φ) as
Pr(w|X,σ2) ∝
Z
Pr(w,φ|X,σ2) dφ
=
Z
Pr(w|X,φ,σ2)Pr(φ) dφ
=
Z
Normw[XT φ,σ2I]
Z
Normφ[0,H−1]
D
Y
d=1
Gamhd[ν/2,ν/2] dHdφ
=
Z Z
Normw[XT φ,σ2I]Normφ[0,H−1]
D
Y
d=1
Gamhd[ν/2,ν/2] dHdφ
=
Z
Normw[0,XT H−1X + σ2I]
D
Y
d=1
Gamhd[ν/2,ν/2] dH,
(8.34)
where the integral over Φ was computed using the same technique as used in
Equation 8.12.
Unfortunately, we still cannot compute the remaining integral in closed form, so we
instead take the approach of maximizing over hidden variables to give an approximate
expression for the marginal likelihood
Pr(w|X,σ2) ≈max
H
"
Normw[0,XT H−1X + σ2I]
D
Y
d=1
Gamhd[ν/2,ν/2]
#
.
(8.35)
As long as the true distribution over the hidden variables is concentrated tightly
around the mode, this will be a reasonable approximation. When hd takes a large value,
the prior has a small variance (1/hd), and the associated coefﬁcient φd will be forced to
be close to zero: in effect, this means that the dth dimension of x does not contribute to
the solution and can be dropped from the equations.
The general approach to ﬁtting the model is now clear. There are two unknown
quantities – the variance σ2 and the hidden variables h and we alternately update each to
maximize the log marginal likelihood.1
1More details about how these (nonobvious) update equations were generated can be found in Tipping
(2001) and section 3.5 of Bishop (2006).

8.6
Sparse linear regression
123
• To update the hidden variables, we take the derivative of the log of this expression
with respect to H, equate the result to zero, and rearrange to get the iteration
hnew
d
= 1 −hdΣdd + ν
µ2
d + ν
,
(8.36)
where µd is the dth element of the mean µ of the posterior distribution over the
weights φ and Σdd is the dth element of the diagonal of the covariance Σ of the
posterior distribution over the weights (Equation 8.10) so that
µ = 1
σ2 A−1Xw
Σ = A−1,
(8.37)
and A is deﬁned as
A = 1
σ2 XXT + H.
(8.38)
• To update the variance, we take the derivative of the log of this expression with
respect to σ2, equate the result to zero, and simplify to get
(σ2)new =
1
D −P
d(1 −hdΣdd) (w −Xµ)T (w −Xµ)
(8.39)
Between each of these updates, the posterior mean µ and variance Σ should be
recalculated.
In practice, we choose a very small value for the degrees of freedom (ν < 10−3) to
encourage sparseness. We may also restrict the maximum possible values of the hidden
variables hi to ensure numerical stability.
At the end of the training, all dimensions of φ where the hidden variable hd is large
(say > 1000) are discarded. Figure 8.11 shows an example ﬁt to some two-dimensional
data. The sparse solution depends only on one of the two possible directions and so is
twice as efﬁcient.
In principle, a nonlinear version of this algorithm can be generated by transforming
the input data x to create the vector z = f[x]. However, if the transformed data z is very
high-dimensional, we will need correspondingly more hidden variables hd to cope with
these dimensions. Obviously, this idea will not transfer to kernel functions where the
dimensionality of the transformed data could be inﬁnite.
To resolve this problem, we will develop the relevance vector machine. This model
also imposes sparsity, but it does so in a way that makes the ﬁnal prediction depend
only on a sparse subset of the training data, rather than a sparse subset of the observed
dimensions. Before we can investigate this model, we must develop a version of linear
regression where there is one parameter per data example rather than one per observed
dimension. This model is known as dual linear regression.

124
8 Regression models
Figure 8.11 Sparse linear regression. a) Bayesian linear regression from two-dimensional data.
The background color represents the mean µw|x of the Gaussian prediction Pr(w|x) for w. The
variance of Pr(w|x) is not shown. The color of the datapoints indicates the training value w, so
for a perfect regression ﬁt this should match exactly the surrounding color. Here the elements of φ
take arbitrary values and so the gradient of the function points in an arbitrary direction. b) Sparse
linear regression. Here, the elements of φ are encouraged to be zero where they are not necessary
to explain the data. The algorithm has found a good ﬁt where the second element of φ is zero and
so there is no dependence on the vertical axis.
8.7
Dual linear regression
In the standard linear regression model the parameter vector φ contains D entries cor-
responding to each of the D dimensions of the (possibly transformed) input data. In the
dual formulation, we reparameterize the model in terms of a vector ψ which has I entries
where I is the number of training examples. This is more efﬁcient in situations where
we are training a model where the input data are high-dimensional, but the number of
examples is small (I < D), and leads to other interesting models such as relevance vector
regression.
8.7.1
Dual model
In the dual model, we retain the original linear dependence of the prediction w on the
input data x so that
Pr(wi|xi) = Normxi[φT xi,σ2].
(8.40)
However, we now represent the slope parameters φ as a weighted sum of the observed
data points so that
φ = Xψ,
(8.41)
where ψ is a I × 1 vector representing the weights (Figure 8.12). We term this the dual
parameterization. Notice that, if there are fewer data examples than data dimensions,
then there will be fewer unknowns in this model than in the standard formulation of
linear regression and hence learning and inference will be more efﬁcient. Note that the
term dual is heavily overloaded in computer science, and the reader should be careful not
to confuse this use with its other meanings.
If the data dimensionality D is less than the number of examples I, then we can
ﬁnd parameters ψ to represent any gradient vector φ. However, if D > I (often true in

8.7
Dual linear regression
125
Figure 8.12 Dual variables. Two-dimen-
sional training data {xi}I
i=1 and associated
world state {wi}I
i=1 (indicated by marker
color).
The linear regression parameter φ
determines the direction in this 2D space in
which w changes most quickly. We can alter-
nately represent the gradient direction as a
weighted sum of data examples. Here we
show the case φ = ψ1x1 + ψ2x2. In prac-
tical problems, the data dimensionality D is
greater than the number of examples I so we
take a weighted sum φ = Xψ of all the dat-
apoints. This is the dual parameterization.
vision where measurements can be high-dimensional), then the vector Xψ can only span
a subspace of the possible gradient vectors. However, this is not a problem: if there was
no variation in the data X in a given direction in space, then the gradient along that axis
should be zero anyway since we have no information about how the world state w varies
in this direction.
Making the substitution from Equation 8.41, the regression model becomes
Pr(wi|xi,θ) = Normxi[ψT XT xi,σ2],
(8.42)
or writing all of the data likelihoods in one term
Pr(w|X,θ) = Normw

XT Xψ,σ2I

,
(8.43)
where the parameters of the model are θ = {ψ,σ2}. We now consider how to learn this
model using both the maximum likelihood and Bayesian approaches.
Maximum likelihood solution
We apply the maximum likelihood method to estimate the parameters ψ in the dual
formulation. To this end, we maximize the logarithm of the likelihood (Equation 8.43)
8.5
with respect to ψ and σ2 so that
ˆψ, ˆσ2 = argmax
ψ,σ2

−I log[2π]
2
−I log[σ]
2
−(w −XT Xψ)T (w −XT Xψ)
2σ2

.
(8.44)
To maximize this expression, we take derivatives with respect to ψ and σ2, equate
the resulting expressions to zero, and solve to ﬁnd
ˆψ = (XT X)−1w
ˆσ2 = (w −XT Xψ)T (w −XT Xψ)
I
.
(8.45)

126
8 Regression models
This solution is actually the same as for the original linear regression model (Equation
8.6). For example, if we substitute in the deﬁnition φ = Xψ,
ˆφ = Xˆψ = X(XT X)−1w
= (XXT )−1XXT X(XT X)−1w
= (XXT )−1Xw,
(8.46)
which was the original maximum likelihood solution for φ.
Bayesian solution
We now explore the Bayesian approach to the dual regression model. As before, we treat
the dual parameters ψ as uncertain, assuming that the noise σ2 is known. Once again,
we will estimate this separately using maximum likelihood.
The goal of the Bayesian approach is to compute the posterior distribution
Pr(ψ|X,w) over possible values of the parameters ψ given the training data pairs
{xi,wi}I
i=1. We start by deﬁning a prior Pr(ψ) over the parameters. Since we have
no particular prior knowledge, we choose a normal distribution with a large spherical
covariance,
Pr(ψ) = Normψ[0,σ2
pI].
(8.47)
We use Bayes’ rule to compute the posterior distribution over the parameters
Pr(ψ|X,w,σ2) = Pr(X|w,ψ,σ2)Pr(ψ)
Pr(X|w,σ2)
,
(8.48)
which can be shown to yield the closed form expression
Pr(ψ|X,w,σ2) = Normψ
 1
σ2 A−1XT Xw,A−1

,
(8.49)
where
A = 1
σ2 XT XXT X + 1
σ2p
I.
(8.50)
To compute the predictive distribution Pr(w∗|x∗), we take an inﬁnite weighted sum
over the predictions of the model associated with each possible value of the parameters ψ,
Pr(w∗|x∗,X,w) =
Z
Pr(w∗|x∗,ψ)Pr(ψ|X,w) dψ
(8.51)
= Normw∗
 1
σ2 x∗T XA−1XT Xw∗,x∗T XA−1XT x∗+ σ2

.
To generalize the model to the nonlinear case, we replace the training data X =
8.6
[x1,x2,...,xI] with the transformed data Z = [z1,z2,...,zI] and the test data x∗with the
transformed test data z∗. Since the resulting expression depends only on inner products
of the form ZT Z and ZT z∗, it is directly amenable to kernelization.
As for the original regression model, the variance parameter σ2 can be estimated by
maximizing the log of the marginal likelihood which is given by
Pr(w|X,σ2) = Normw[0,σ2
pXT XXT X + σ2I].
(8.52)

8.8
Relevance vector regression
127
8.8
Relevance vector regression
Having developed the dual approach to linear regression, we are now in a position to
develop a model that depends only sparsely on the training data. To this end, we impose
8.7
a penalty for every nonzero weighted training example.
We achieve this by replac-
ing the normal prior over the dual parameters ψ with a product of one-dimensional
t-distributions so that
Pr(ψ) =
IY
i=1
Studψi [0,1,ν].
(8.53)
This model is known as relevance vector regression.
This situation is exactly analogous to the sparse linear regression model (Section 8.6)
except that now we are working with dual variables. As for the sparse model, it is not pos-
sible to marginalize over the variables ψ with the t-distributed prior. Our approach will
again be to approximate the t-distributions by maximizing with respect to their hidden
variables rather than marginalizing over them (Equation 8.35). By analogy with Section
8.6, the marginal likelihood becomes
Pr(w|X,σ2) ≈max
H
"
Normw[0,XT XH−1XT X + σ2I]
D
Y
d=1
Gamhd[ν/2,ν/2]
#
,
(8.54)
where the matrix H contains the hidden variables {hi}I
i=1 associated with the
t-distribution on its diagonal and zeros elsewhere. Notice that this expression is simi-
lar to Equation 8.52 except that instead of every datapoint having the same prior variance
σ2
p, they now have individual variances that are determined by the hidden variables that
form the elements of the diagonal matrix H.
In relevance vector regression, we alternately (i) optimize the marginal likelihood
with respect to the hidden variables and (ii) optimize the marginal likelihood with respect
to the variance parameter σ2 using
hnew
i
= 1 −hiΣii + ν
µ2
i + ν
(8.55)
and
(σ2)new =
1
I −P
i(1 −hiΣii)
 w −XT Xµ
T  w −XT Xµ

.
(8.56)
In between each step we update the mean µ and variance Σ of the posterior distribution
µ = 1
σ2 A−1XT Xw
Σ = A−1,
(8.57)
where A is deﬁned as
A = 1
σ2 XT XXT X + H.
(8.58)

128
8 Regression models
Figure 8.13 Relevance vector regression. A prior applying sparseness is applied to the dual
parameters. This means that the ﬁnal classiﬁer only depends on a subset of the datapoints (indicated
by the six larger points). The resulting regression function is considerably faster to evaluate and
tends to be simpler: this means it is less likely to overﬁt to random statistical ﬂuctuations in the
training data and generalizes better to new data.
At the end of the training, all data examples where the hidden variable hi is large
(say > 1000) are discarded as here the coefﬁcients ψi will be very small and contribute
almost nothing to the solution.
Since this algorithm depends only on inner products, a nonlinear version of this algo-
rithm can be generated by replacing the inner products with a kernel function k[xi,xj]. If
the kernel itself contains parameters, these may be also be manipulated to improve the log
marginal variance during the ﬁtting procedure. Figure 8.13 shows an example ﬁt using
the RBF kernel. The ﬁnal solution now only depends on six datapoints but nonetheless
still captures the important aspects of the data.
8.9
Regression to multivariate data
Throughout this chapter we have discussed predicting a scalar value wi from multivariate
data xi. In real-world situations such as the pose regression problem, the world states wi
are multivariate. It is trivial to extend the models in this chapter: we simply construct a
separate regressor for each dimension. The exception to this rule is the relevance vector
machine: here we might want to ensure that the sparse structure is common for each of
these models, so the efﬁciency gains are retained. To this end, we modify the model
so that a single set of hidden variables is shared across the model for each world state
dimension.
8.10
Applications
Regression methods are used less frequently in vision than classiﬁcation, but nonetheless
there are many useful applications. The majority of these involve estimating the posi-
tion or pose of objects, since the unknowns in such problems are naturally treated as
continuous.

8.10
Applications
129
8.10.1 Human body pose estimation
Agarwal and Triggs (2006) developed a system based on the relevance vector machine
to predict body pose w from silhouette data x. To encode the silhouette, they computed
a 60-dimensional shape context feature (Section 13.3.5) at each of 400-500 points on the
boundary of the object. To reduce the data dimensionality, they computed the similarity
of each shape context feature to each of 100 different prototypes. Finally, they formed
a 100-dimensional histogram containing the aggregated 100-dimensional similarities for
all of the boundary points. This histogram was used as the data vector x. The body pose
was encoded by the 3 joint angles of each of the 18 major body joints and the overall
azimuth (compass heading) of the body. The resulting 55-dimensional vector was used
as the world state w.
A relevance vector machine was trained using 2636 data vectors xi extracted from sil-
houettes that were rendered using the commercial program POSER from known motion
capture data wi. Using a radial basis function kernel, the relevance vector machine based
its solution on just 6% of these training examples. The body pose angles of test data could
be predicted to within an average of 6o error (Figure 8.14). They also demonstrated that
the system worked reasonably well on silhouettes from real images (Figure 8.1).
a)
b)
Figure 8.14 Body pose estimation results.
a) Silhouettes of walking avatar. b) Esti-
mated body pose based on silhouette using
a relevance vector machine. The RVM used
radial basis functions and constructed its
ﬁnal solution from just 156 of 2636 (6%) of
the training examples. It produced a mean
test error of 6.0o averaged over the three
joint angles for the 18 main body parts and
the overall compass direction of the model.
Adapted from Agarwal and Triggs (2006).
Silhouette information is by its nature ambiguous: it is very hard to tell which leg is
in front of the other based on a single silhouette. Agarwal and Triggs (2006) partially
circumvented this system by tracking the body pose wi through a video sequence. Essen-
tially, the ambiguity at a given frame is resolved by encouraging the estimated pose in
adjacent frames in the sequence to be similar: information from frames where the pose
vector is well deﬁned is propagated through the sequence to resolve ambiguities in other
parts (see Chapter 19).
However, the ambiguity of silhouette data is an argument for not using this type
of classiﬁer: the regression models in this chapter are designed to give a unimodal
normal output. To effectively classify single frames of data, we should use a regres-
sion method that produces a multimodal prediction that can effectively describe the
ambiguity.
8.10.2 Displacement experts
Regression models are also used to form displacement experts in tracking applications.
The goal is to take a region of the image x and return a set of numbers w that indicate the

130
8 Regression models
Figure 8.15 Tracking using displacement experts. The goal of the system is to predict a displace-
ment vector indicating the motion of the object based on the pixel data at its last known position. a)
The system is trained by perturbing the bounding box around the object to simulate the motion of
the object. b) The system successfully tracks a face, even in the presence c) of partial occlusions.
d) If the system is trained using gradient vectors rather than raw pixel values, it is also quite robust
to changes in illumination. Adapted from Williams et al. (2005). c⃝2005 IEEE.
change in position of an object relative to the window. The world state w might simply
contain the horizontal and vertical translation vectors or might contain parameters of a
more complex 2D transformation (see Chapter 15). For simplicity, we will describe the
former situation.
Training data are extracted as follows. A bounding box around the object of inter-
est (car, face, etc.) is identiﬁed in a number of frames. For each of these frames, the
bounding box is perturbed by a predetermined set of translation vectors, to simulate the
object moving in the opposite direction (Figure 8.15a). In this way, we associate a trans-
lation vector wi with each perturbation. The data from the perturbed bounding box are
extracted and resized to a standard shape, and histogram equalized (Section 13.1.2) to
induce a degree of invariance to illumination changes. The resulting values are then
concatenated to form the data vector xi.
Williams et al. (2005) describe a system of this kind in which the elements of w were
learned by a set of independent relevance vector machines. They initialize the position of
the object using a standard object detector (see Chapter 9). In the subsequent frame, they
compute a prediction for the displacement vector w using the relevance vector machines
on the data x from the original position. This prediction is combined in a Kalman ﬁlter-
like system (Chapter 19) that imposes prior knowledge about the continuity of the motion
to create a robust method for tracking known objects in scenes. Figures 8.15b–d show a
series of tracking results from this system.
Discussion
The goal of this chapter was to introduce discriminative approaches to regression. These
have niche applications in vision related to predicting the pose and position of objects.
However, the main reason for studying these models is that the concepts involved (spar-
sity, dual variables, kernelization) are all important for discriminative classiﬁcation
methods. These are very widely used but are rather more complex and are discussed
in the following chapter.

Problems
131
Notes
Regression methods: Rasmussen and Williams (2006) is a comprehensive resource on Gaussian
processes. The relevance vector machine was ﬁrst introduced by Tipping (2001). Several innova-
tions within the vision community have extended these models. Williams et al. (2006) presented a
semisupervised method for Gaussian process regression in which the world state w is only known
for a subset of examples. Ranganathan and Yang (2008) presented an efﬁcient algorithm for online
learning of Gaussian processes when the kernel matrix is sparse. Thayananthan et al. (2006)
developed a multivariate version of the relevance vector machine.
Applications: Applications of regression in vision include head pose estimation (Williams et al.
2006; Ranganathan and Yang 2008; Rae and Ritter 1998), body tracking (Williams et al. 2006;
Agarwal and Triggs 2006; Thayananthan et al. 2006), eye tracking (Williams et al. 2006), and
tracking of other objects (Williams et al. 2005; Ranganathan and Yang 2008).
Multimodal posterior: One of the drawbacks of using the methods in this chapter is that they
always produce a unimodal normally distributed posterior. For some problems (e.g., body pose
estimation), the posterior probability over the world state may be genuinely multimodal – there
is more than one interpretation of the data. One approach to this is to build many regressors
that relate small parts of the world state to the data (Thayananthan et al. 2006). Alternatively,
it is possible to use generative regression methods in which either the joint density is modeled
directly (Navaratnam et al. 2007) or the likelihood and prior are modeled separately (Urtasun
et al. 2006). In these methods, the posterior compute distribution over the world is multimodal.
However, the cost of this is that it is intractable to compute the posterior exactly, and so we must
rely on optimization techniques to ﬁnd its modes.
Problems
8.1 Consider a regression problem where the world state w is known to be positive. To cope
with this, we could construct a regression model in which the world state is modeled as a gamma
distribution. We could constrain both parameters α,β of the gamma distribution to be the same so
that α = β and make them a function of the data x. Describe a maximum likelihood approach to
ﬁtting this model.
8.2 Consider a robust regression problem based on the t-distribution rather than the normal distri-
bution. Deﬁne this model precisely in mathematical terms and sketch out a maximum likelihood
approach to ﬁtting the parameters.
8.3 Prove that the maximum likelihood solution for the gradient in the linear regression model is
ˆφ = (XXT )−1Xw.
8.4 For the Bayesian linear regression model (Section 8.2), show that the posterior distribution
over the parameters φ is given by
Pr(φ|X,w) = Normφ
 1
σ2 A−1Xw,A−1

,
where
A = 1
σ2 XXT + 1
σ2p
I.
8.5 For the Bayesian linear regression model (Section 8.2), show that the predictive distribution
for a new data example x∗is given by
Pr(w∗|x∗,X,w) = Normw∗
 1
σ2 x∗T A−1Xw,x∗T A−1x∗+ σ2

.

132
8 Regression models
8.6 Use the matrix inversion lemma (Appendix C.8.4) to show that
A−1 =
 1
σ2 XXT + 1
σ2p
ID
−1
= σ2
pID −σ2
pX

XT X + σ2
σ2p
II
−1
XT .
8.7 Compute the derivative of the marginal likelihood
Pr(w|X,σ2) = Normw[0,σ2
pXT X + σ2I],
with respect to the variance parameter σ2.
8.8 Compute a closed form expression for the approximated t-distribution used to impose
sparseness.
q(h) = max
h

Normφ[0,h−1]Gamh[ν/2,ν/2]

.
Plot this function for ν = 2. Plot the 2D function [h1,h2] = q(h1)q(h2) for ν = 2.
8.9 Describe maximum likelihood learning and inference algorithms for a nonlinear regression
model based on polynomials where
Pr(w|x) = Normw[φ0 + φ1x + φ2x2 + φ3x3,σ2].
8.10 I wish to learn a linear regression model in which I predict the world w from I examples of
D × 1 data x using the maximum likelihood method. If I > D, is it more efﬁcient to use the dual
parameterization or the original linear regression model?
8.11 Show that the maximum likelihood estimate for the parameters ψ in the dual linear regression
model (Section 8.7) is given by
ˆψ = (XT X)−1w.

Chapter 9
Classiﬁcation models
This chapter concerns discriminative models for classiﬁcation. The goal is to directly
model the posterior probability distribution Pr(w|x) over a discrete world state w ∈
{1,...K} given the continuous observed data vector x. Models for classiﬁcation are
very closely related to those for regression and the reader should be familiar with the
contents of Chapter 8 before proceeding.
To motivate the models in this chapter, we will consider gender classiﬁcation: here
we observe a 60 × 60 RGB image containing a face (Figure 9.1) and concatenate the
RGB values to form the 10800 × 1 vector x. Our goal is to take the vector x and return
the probability distribution Pr(w|x) over a label w ∈{0,1} indicating whether the face
is male (w = 0) or female (w = 1).
Gender classiﬁcation is a binary classiﬁcation task as there are only two possible
values of the world state. Throughout most of this chapter, we will restrict our discussion
to binary classiﬁcation. We discuss how to extend these models to cope with an arbitrary
number of classes in Section 9.9.
9.1
Logistic regression
We will start by considering logistic regression, which despite its name is a model that
can be applied to classiﬁcation.
Logistic regression (Figure 9.2) is a discriminative
model; we select a probability distribution over the world state w ∈{0,1} and make
its parameters contingent on the observed data x. Since the world state is binary, we will
describe it with a Bernoulli distribution, and we will make the single Bernoulli parameter
λ (indicating the probability that the world state takes the value w = 1) a function of the
measurements x.
In contrast to the regression model, we cannot simply make the parameter λ a linear
function φ0 + φT x of the measurements; a linear function can return any value, but
the parameter λ must lie between 0 and 1. Consequently, we ﬁrst compute the linear
function and then pass this through the logistic sigmoid function sig[•] that maps the
range [−∞,∞] to [0,1]. The ﬁnal model is hence
Pr(w|φ0,φ,x) = Bernw [sig[a]],
(9.1)
where a is termed the activation and is given by the linear function
a = φ0 + φT x.
(9.2)

134
9 Classiﬁcation models
Figure 9.1 Gender classiﬁcation. Consider
a 60×60 pixel image of a face. We concate-
nate the RGB values to make a 10800 × 1
data vector x.
The goal of gender clas-
siﬁcation is to use the data x to infer a
label w ∈{0,1} indicating whether the win-
dow contains a) a male or b) a female face.
This is challenging because the differences
are subtle and there is image variation due
to changes in pose, lighting, and expres-
sion.
Note that real systems would pre-
process the image before classiﬁcation by
registering the faces more closely and com-
pensating in some way for lighting variation
(see Chapter 13).
The logistic sigmoid function sig[•] is given by
sig[a] =
1
1 + exp[−a].
(9.3)
As the activation a tends to ∞this function tends to one. As a tends to −∞it tends to
zero. When a is zero, the logistic sigmoid function returns a value of one half.
For 1D data x, the overall effect of this transformation is to describe a sigmoid curve
relating x to λ (Figures 9.2c and 9.3a). The horizontal position of the sigmoid is deter-
mined by the place that the linear function a crosses zero (i.e., the x-intercept) and the
steepness of the sigmoid depends on the gradient φ1.
In more than one dimension, the relationship between x and λ is more complex
(Figure 9.3b). The predicted parameter λ has a sigmoid proﬁle in the direction of the
gradient vector φ but is constant in all perpendicular directions. This induces a linear
decision boundary. This is the set of positions in data space {x : Pr(w = 1|x) = 0.5}
where the posterior probability is 0.5; the decision boundary separates the region where
the world state w is more likely to be 0 from the region where it is more likely to be 1.
For logistic regression, the decision boundary takes the form of a hyperplane with the
normal vector in the direction of φ.
As for regression, we can simplify the notation by attaching the y-intercept φ0 to the
start of the parameter vector φ so that φ ←[φ0
φT ]T and attaching 1 to the start of the
data vector x so that x ←[1
xT ]T . After these changes, the activation is now a = φT x,
and the ﬁnal model becomes
Pr(w|φ,x) = Bernw

1
1 + exp[−φT x]

.
(9.4)
Notice that this is very similar to the linear regression model (Section 8.1) except for
the introduction of the nonlinear logistic sigmoid function sig[•] (explaining the unfor-
tunate name “logistic regression”). However, this small change has serious implications:

9.1
Logistic regression
135
Figure 9.2 Logistic regression. a) We represent the world state w as a Bernoulli distribution and
make the Bernoulli parameter λ a function of the observations x. b) We compute the activation a as
a linear sum a = φ0 +φ1x of the observations. c) The Bernoulli parameter λ is formed by passing
the activation through a logistic sigmoid function sig[•] to constrain the value to lie between 0
and 1, giving the characteristic sigmoid shape. In learning, we ﬁt parameters θ={φ0,φ1} using
training pairs {xi,wi}. In inference, we take a new datum x∗and evaluate the posterior Pr(w∗|x∗)
over the state.
maximum likelihood learning of the parameters φ is considerably harder than for linear
regression and to adopt the Bayesian approach we will be forced to make approximations.
9.1.1
Learning: maximum likelihood
In maximum likelihood learning, we consider ﬁtting the parameters φ using I paired
9.1
examples of training data {xi,wi}I
i=1 (Figure 9.3).
Assuming independence of the
training pairs we have
Pr(w|X,φ) =
IY
i=1
λwi(1 −λ)1−wi
(9.5)
=
IY
i=1

1
1 + exp[−φT xi]
wi  
exp[−φT xi]
1 + exp[−φT xi]
!1−wi
,
where X=[x1,x2,..., xI] is a matrix containing the measurements and w=[w1,
w2,...,wI]T is a vector containing all of the binary world states.
The maximum
likelihood method ﬁnds parameters φ, which maximize this expression.
As usual, however, it is simpler to maximize the logarithm L of this expression.
Since the logarithm is a monotonic transformation, it does not change the position of the
maximum with respect to φ. Applying the logarithm replaces the product with a sum so
that
L =
I
X
i=1
wi log

1
1 + exp[−φT xi]

+
I
X
i=1
(1 −wi)log
"
exp[−φT xi]
1 + exp[−φT xi]
#
.
(9.6)
The derivative of the log likelihood L with respect to the parameters φ is
∂L
∂φ = −
I
X
i=1

1
1 + exp[−φT xi]
−wi

xi = −
I
X
i=1
(sig[ai] −wi)xi.
(9.7)

136
9 Classiﬁcation models
Figure 9.3 Logistic regression model ﬁtted to two different data sets. a) One dimensional data.
Green points denote set of examples S0 where w = 0. Pink points denote S1 where w = 1. Note
that in this (and all future ﬁgures in this chapter), we have only plotted the probability Pr(w = 1|x)
(compare to Figure 9.2c). The probability Pr(w = 0|x) can be computed as 1 −Pr(w = 1|x). b)
Two-dimensional data. Here, the model has a sigmoid proﬁle in the direction of the gradient φ and
Pr(w = 1|x) is constant in the orthogonal directions. The decision boundary (cyan line) is linear.
Unfortunately, when we equate this expression to zero, there is no way to re-arrange
to get a closed form solution for the parameters φ. Instead, we must rely on a nonlinear
optimization technique to ﬁnd the maximum of this objective function. Optimization
techniques are discussed in detail in Appendix B. In brief, we start with an initial estimate
of the solution φ and iteratively improve it until no more progress can be made.
Here, we will apply the Newton method, in which we base the update of the param-
eters on the ﬁrst and second derivatives of the function at the current position so
that
φ[t] = φ[t−1] + α
∂2L
∂φ2
−1 ∂L
∂φ,
(9.8)
where φ[t] denotes the estimate of the parameters φ at iteration t and α determines how
much we change this estimate and is usually chosen by an explicit search at each iteration.
For the logistic regression model, the D ×1 vector of ﬁrst derivatives and the D ×D
matrix of second derivatives are given by
∂L
∂φ = −
I
X
i=1
(sig[ai] −wi)xi
∂2L
∂φ2 = −
I
X
i=1
sig[ai](1 −sig[ai])xixT
i .
(9.9)
These are known as the gradient vector and the Hessian matrix, respectively.1
1Note that these are the gradient and Hessian of the log likelihood that we aim to maximize. If this is
implemented using a nonlinear minimization algorithm, we should multiply the objective function, gradient
and Hessian by −1.

9.1
Logistic regression
137
Figure 9.4 Parameter estimation for logistic regression with 1D data. a) In maximum likelihood
learning, we seek the maximum of Pr(w|X,φ) with respect to φ. b) In practice, we instead
maximize log likelihood: notice that the peak is in the same place. Crosses show results of two
iterations of optimization using Newton’s method. c) The logistic sigmoid functions associated
with the parameters at each optimization step. As the log likelihood increases, the model ﬁts the
data more closely: the green points represent data where w = 0 and the purple points represent
data where w = 1 and so we expect the best-ﬁtting model to increase from left to right just like
curve 3.
The expression for the gradient vector has an intuitive explanation. The contribution
of each datapoint depends on the difference between the actual class wi and the pre-
dicted probability λ = sig[ai] of being in class 1; points that are classiﬁed incorrectly
contribute more to this expression and hence have more inﬂuence on the parameter val-
ues. Figure 9.4 shows maximum likelihood learning of the parameters φ for 1D data
using a series of Newton steps.
For general functions, the Newton method only ﬁnds local maxima. At the end of the
procedure, we cannot be certain that there is not a taller peak in the likelihood function
elsewhere. However, the log likelihood for logistic regression has a special property. It is
a concave function of the parameters φ. For concave functions there are never multiple
maxima and gradient-based approaches are guaranteed to ﬁnd the global maximum. It
is possible to establish whether a function is concave or not by examining the Hessian
matrix. If this is negative deﬁnite for all φ, then the function is concave. This is the case
for logistic regression as the Hessian (Equation 9.9) consists of a negative weighted sum
of outer products.2
9.1.2
Problems with the logistic regression model
The logistic regression model works well for simple data sets, but for more complex
visual data it will not generally sufﬁce. It is limited in the following ways.
1. It is overconﬁdent as it was learned using maximum likelihood.
2. It can only describe linear decision boundaries.
3. It is inefﬁcient and prone to overﬁtting in high dimensions.
In the remaining part of this chapter, we will extend this model to cope with these
problems (Figure 9.5).
2When we are concerned with minimizing functions, we equivalently consider whether the function is
convex and so only has a single local minimum. If the Hessian is positive deﬁnite everywhere then the function
is convex.

138
9 Classiﬁcation models
Incremental 
learning
Logistic regression
Bayesian
formulation
Project data
through
nonlinearity  
Problem 1
Problem 2
Problem 3
Overconfident
Linear
Computational cost
Nonlinear logistic
regression
Bayesian logistic 
regression
b)
c)
Boosting
Gaussian process 
classification
d)
Classification trees 
g)
f)
Gating functions  
Dual
formulation
a)
e)
Relevance vector
classification  
Sparsity
Figure 9.5 Family of classiﬁcation models. a) In the remaining part of the chapter, we will
address several of the limitations of logistic regression for binary classiﬁcation. b) The logis-
tic regression model with maximum likelihood learning is overconﬁdent, and hence we develop
a Bayesian version. c) It is unrealistic to always assume a linear relationship between the data
and the world, and to this end we introduce a nonlinear version. d) Combining the Bayesian
and nonlinear versions of regression leads to Gaussian process classiﬁcation.
e) The logistic
regression model also has many parameters and may require considerable resources to learn
when the data dimension is high, and so we develop relevance vector classiﬁcation that encour-
ages sparsity.
f) We can also build a sparse model by incrementally adding parameters in
a boosting scheme.
g) Finally, we consider a very fast classiﬁcation model based on a tree
structure.
9.2
Bayesian logistic regression
In the Bayesian approach, we learn a distribution Pr(φ|X,w) over the possible param-
eter values φ that are compatible with the training data. In inference, we observe a new
data example x∗and use this distribution to weight the predictions for the world state
w∗given by each possible estimate of φ. In linear regression (Section 8.2), there were
closed form expressions for both of these steps. However, the nonlinear function sig[•]
in logistic regression means that this is no longer the case. To get around this, we will
approximate both steps so that we retain neat closed form expressions and the algorithm
is tractable.
9.2.1
Learning
We start by deﬁning a prior over the parameters φ. Unfortunately, there is no conjugate
9.2
prior for the likelihood in the logistic regression model (Equation 9.1); this is why there
won’t be closed form expressions for the likelihood and predictive distribution. With
nothing else to guide us, a reasonable choice for the prior over the continuous parameters

9.2
Bayesian logistic regression
139
Figure 9.6 Laplace approximation. A probability density (blue curve) is approximated by a nor-
mal distribution (red curve). The mean of the normal (and hence the peak) is chosen to coincide
with the peak of the original pdf. The variance of the normal is chosen so that its second derivatives
at the mean match the second derivatives of the original pdf at the peak.
φ is a multivariate normal distribution with zero mean and a large spherical covariance
so that
Pr(φ) = Normφ[0,σ2
pI].
(9.10)
To compute the posterior probability distribution Pr(φ|X,w) over the parameters φ
given the training data pairs {xi,wi}, we apply Bayes’ rule,
Pr(φ|X,w) = Pr(w|X,φ)Pr(φ)
Pr(w|X)
,
(9.11)
where the likelihood and prior are given by Equations 9.5 and 9.10, respectively. Since
we are not using a conjugate prior, there is no simple closed form expression for this
posterior and so we are forced to make an approximation of some kind.
One possibility is to use the Laplace approximation (Figure 9.6) which is a general
method for approximating complex probability distributions. The goal is to approximate
the posterior distribution by a multivariate normal. We select the parameters of this
normal so that (i) the mean is at the peak of the posterior distribution (i.e., at the MAP
estimate) and (ii) the covariance is such that the second derivatives at the peak match the
second derivatives of the true posterior distribution at its peak.
Hence, to make the Laplace approximation, we ﬁrst ﬁnd the MAP estimate of the
parameters ˆφ, and to this end we use a nonlinear optimization technique such as Newton’s
method to maximize the criterion,
L =
I
X
i=1
log[Pr(wi|xi,φ)] + log[Pr(φ)].
(9.12)

140
9 Classiﬁcation models
Figure 9.7 Laplace approximation for logistic regression. a) The prior Pr(φ) over the param-
eters is a normal distribution with mean zero and a large spherical covariance. b) The posterior
distribution Pr(φ|X,w) represents the reﬁned state of our knowledge after observing the data.
Unfortunately, this posterior cannot be expressed in closed form. c) We approximate the true pos-
terior with a normal distribution q(φ) = Normφ[µ,Σ] whose mean is at the peak of the posterior
and whose covariance is chosen so that the second derivatives at the peak of the true posterior match
the second derivatives at the peak of the normal. This is termed the Laplace approximation.
Newton’s method needs the derivatives of the log posterior which are
∂L
∂φ = −
I
X
i=1
(sig[ai] −wi)xi −φ
σ2p
∂2L
∂φ2 = −
I
X
i=1
sig[ai](1 −sig[ai])xixT
i −1
σ2p
.
(9.13)
We then approximate the posterior by a multivariate normal so that
Pr(φ|X,w) ≈q(φ) = Normφ[µ,Σ],
(9.14)
where the mean µ is set to the MAP estimate ˆφ, and the covariance Σ is chosen so that
the second derivatives of the normal match those of the log posterior at the MAP estimate
(Figure 9.7) so that
µ = ˆφ
Σ = −
∂2L
∂φ2
−1
φ=ˆφ
.
(9.15)
9.2.2
Inference
In inference we aim to compute a posterior distribution Pr(w∗|x∗,X,w) over the world
state w∗given new observed data x∗. To this end, we compute an inﬁnite weighted sum
(i.e., an integral) of the predictions Pr(w∗|x∗,φ) given by each possible value of the

9.2
Bayesian logistic regression
141
parameters φ,
Pr(w∗|x∗,X,w) =
Z
Pr(w∗|x∗,φ)Pr(φ|X,w)dφ
≈
Z
Pr(w∗|x∗,φ)q(φ)dφ,
(9.16)
where the weights q(φ) are given by the approximated posterior distribution over the
parameters from the learning stage. Unfortunately, this integral cannot be computed in
closed form either, and so we must make a further approximation.
We ﬁrst note that the prediction Pr(w∗|x∗,φ) depends only on a linear projec-
tion a = φT x∗of the parameters (see Equation 9.4). Hence we could reexpress the
prediction as
Pr(w∗|x∗,X,w) ≈
Z
Pr(w∗|a)Pr(a)da.
(9.17)
The probability distribution Pr(a) can be computed using the transformation property
of the normal distribution (Section 5.3) and is given by
Pr(a) = Pr(φT x∗) = Norma[µT x∗,x∗T Σx]
(9.18)
= Norma[µa,σ2
a],
where we have denoted the mean and variance of the activation by µa and σ2
a, respec-
tively. The one-dimensional integration in Equation 9.17 can now be computed using
numerical integration over a, or we can approximate the result with a similar function
such as
Z
Pr(w∗|a)Norma[µa,σ2
a]da ≈
1
1 + exp[−µa/
p
1 + πσ2a/8]
.
(9.19)
It is not obvious by inspection that this function should approximate the integral well;
however, Figure 9.8 demonstrates that the approximation is quite accurate.
Figure 9.8 Approximation of activation integral (Equation 9.19). a) Actual result of integral as a
function of µa and σ2
a. b) The (nonobvious) approximation from Equation 9.19. c) The absolute
difference between the actual result and the approximation is very small over a range of reasonable
values.

142
9 Classiﬁcation models
Figure 9.9 Bayesian logistic regression predictions. a) The Bayesian prediction for the class w
is more moderate than the maximum likelihood prediction. b) In 2D the decision boundary in
the Bayesian case (blue line) is still linear but iso-probability contours at levels other than 0.5 are
curved (compare to maximum likelihood case in Figure 9.3b). Here too, the Bayesian solution
makes more moderate predictions than the maximum likelihood model.
Figure 9.9 compares the classiﬁcation predictions Pr(w∗|x∗) for the maximum like-
lihood and Bayesian approaches for logistic regression. The Bayesian approach makes
more moderate predictions for the ﬁnal class. This is particularly the case in regions of
data space that are far from the mean.
9.3
Nonlinear logistic regression
The logistic regression model described previously can only create linear decision bound-
aries between classes.
To create nonlinear decision boundaries, we adopt the same
approach as we did for regression (Section 8.3): we compute a nonlinear transformation
z = f[x] of the observed data and then build the logistic regression model substituting
the original data x for the transformed data z, so that
Pr(w = 1|x,φ) = Bernw
h
sig[φT z]
i
= Bernw
h
sig[φT f[x]]
i
.
(9.20)
The logic of this approach is that arbitrary nonlinear activations can be built as a linear
sum of nonlinear basis functions. Typical nonlinear transformations include
• Heaviside step functions of projections: zk = heaviside[αT
k x],
• Arc tangent functions of projections: zk = arctan[αT
k x], and
• Radial basis functions: zk = exp[−1
λ0 (x −αk)T (x −αk)],
where zk denotes the kth element of the transformed vector z and the function
heaviside[•] returns zero if its argument is less than zero and one otherwise. In the
ﬁrst two cases we have attached a 1 to the start of the observed data x where we
use projections αT x to avoid having a separate offset parameter.
Figures 9.10 and
9.11 show examples of nonlinear classiﬁcation using arc tangent functions for one and
two-dimensional data, respectively.

9.3
Nonlinear logistic regression
143
Figure 9.10 Nonlinear classiﬁcation in 1D using arc tangent transformation. We consider a
complex 1D data set (bottom of all panels) where the posterior Pr(w = 1|x) cannot easily be
described by a single sigmoid.
Green circles represent data xi where wi = 0.
Pink circles
represent data xi where wi = 1. a) The seven dimensional transformed data vectors z1 ...zI
are computed by evaluating each data example against seven predeﬁned arc tangent functions
zik = fk[xi] = arctan[α0k + α1kxi]. b) When we learn the parameters φ, we are learning weights
for these nonlinear arc tangent functions. The functions are shown after applying the maximum
likelihood weights ˆφ. c) The ﬁnal activation a = φT z is a weighted sum of the nonlinear func-
tions. d) The probability Pr(w = 1|x) is computed by passing the activation a through the logistic
sigmoid function.
Note that the basis functions also have parameters. For example, in the arc tangent
example, there are the projection directions {αk}K
k=1, each of which contains an offset
and a set of gradients. These can also be optimized during the ﬁtting procedure together
with the weights φ. We form a new vector of unknowns θ = [φT ,αT
1 ,αT
2 ,...,αT
K]T and
optimize the model with respect to all of these unknowns together. The gradient vector
and the Hessian matrix depend on the chosen transformation f[•] but can be computed
using the expressions
∂L
∂θ = −
I
X
i=1
(wi −sig[ai])∂ai
∂θ
∂2L
∂θ2 = −
I
X
i=1
sig[ai](sig[ai] −1)∂ai
∂θ
∂ai
∂θ
T
−(wi −sig[ai])∂2ai
∂θ2 ,
(9.21)

144
9 Classiﬁcation models
Figure 9.11 Nonlinear classiﬁcation in 2D using arc tangent transform. a) These data have been
successfully classiﬁed with nonlinear logistic regression. Note the nonlinear decision boundary
(cyan line).
To compute the posterior Pr(w = 1|x), we transform the data to a new two-
dimensional space z = f[x] where the elements of z are computed by evaluating x against the 1D
arc tangent functions in b) and c). The arc tangent activations are weighted (the ﬁrst by a negative
number) and summed and the result is put through the logistic sigmoid to compute Pr(w = 1|x).
where ai = φT f[xi]. These relations were established using the chain rules for deriva-
tives. Unfortunately, this joint optimization problem is generally not convex and will
be prone to terminating in local maxima. In the Bayesian case, it would be typical to
marginalize over the parameters φ but maximize over the function parameters.
9.4
Dual logistic regression
There is a potential problem with the logistic regression models as described earlier: in
9.3
the original linear model, there is one element of the gradient vector φ corresponding
to each dimension of the observed data x, and in the nonlinear extension there is one
element corresponding to each transformed data dimension z. If the relevant data x (or
z) is very high-dimensional, then the model will have a large number of parameters:
this will render the Newton update slow or even intractable. To solve this problem, we
switch to the dual representation. For simplicity, we will develop this model using the
original data x, but all of the ideas transfer directly to the nonlinear case where we use
transformed data z.
In the dual parameterization, we express the gradient parameters φ as a weighted
sum of the observed data (see Figure 8.12) so that
φ = Xψ,
(9.22)
where ψ is an I × 1 variable in which each element weights one of the data examples.
If the number of data points I is less than the dimensionality D of the data x, then the
number of parameters has been reduced.
The price that we pay for this reduction is that we can now only choose gradient
vectors φ that are in the space spanned by the data examples. However, the gradient
vector represents the direction in which the ﬁnal probability Pr(w = 1|x) changes fastest,
and this should not point in a direction in which there was no variation in the training data
anyway, so this is not a limitation.

9.4
Dual logistic regression
145
Substituting Equation 9.22 into the original logistic regression model leads to the
dual logistic regression model,
Pr(w|X,ψ) =
IY
i=1
Bernwi [sig[ai]] =
IY
i=1
Bernwi
h
sig[ψT XT xi]
i
.
(9.23)
The resulting learning and inference algorithms are very similar to those for the original
logistic regression model, and so we cover them only in brief:
• In the maximum likelihood method, we learn the parameters ψ by nonlinear opti-
mization of the log likelihood L = log[Pr(w|X,ψ)] using the Newton method.
This optimization requires the derivatives of the log likelihood, which are
∂L
∂ψ = −
I
X
i=1
(sig[ai] −wi)XT xi
∂2L
∂ψ2 = −
I
X
i=1
sig[ai](1 −sig[ai])XT xixT
i X.
(9.24)
• In the Bayesian approach, we use a normal prior over the parameters ψ,
Pr(ψ) = Normψ[0,σ2
pI].
(9.25)
The posterior distribution Pr(ψ|X,w) over the new parameters is found using
Bayes’ rule, and once more this cannot be written in closed form, so we apply the
Laplace approximation. We ﬁnd the MAP solution ˆψ using nonlinear optimization,
which requires the derivatives of the log posterior L = log[Pr(ψ|X,w)]:
∂L
∂ψ = −
I
X
i=1
(sig[ai] −wi)XT xi −ψ
σ2p
∂2L
∂ψ2 = −
I
X
i=1
sig[ai](1 −sig[ai])XT xixT
i X −1
σ2p
.
(9.26)
The posterior is now approximated by a multivariate normal so that
Pr(ψ|X,w) ≈q(ψ) = Normψ[µ,Σ],
(9.27)
where
µ = ˆψ
(9.28)
Σ = −
 ∂2L
∂ψ2
−1
ψ= ˆψ
.
In inference, we compute the distribution over the activation
Pr(a) = Pr(ψT XT x∗) = Norma[µa,σ2
a]
= Norma[µT XT x∗,x∗T XΣXT x∗]
(9.29)
and then approximate the predictive distribution using Equation 9.19.

146
9 Classiﬁcation models
Dual logistic regression gives identical results to the original logistic regression algorithm
for the maximum likelihood case and very similar results in the Bayesian situation (where
the difference results from the slightly different priors). However, the dual classiﬁcation
model is much faster to ﬁt in high dimensions as the parameters are fewer.
9.5
Kernel logistic regression
We motivated the dual model by the reduction in the number of parameters ψ in the
9.4
model when the data lies in a high-dimensional space. However, now that we have devel-
oped the model, a further advantage is easy to identify: both learning and inference in
the dual model rely only on inner products xT
i xj of that data. Equivalently, the nonlinear
version of this algorithm depends only on inner products zT
i zj of the transformed data
vectors. This means that the algorithm is suitable for kernelization (see Section 8.4).
The idea of kernelization is to deﬁne a kernel function k[•,•], which computes the
quantity
k[xi,xj] = zT
i zj,
(9.30)
where zi = f[xi] and zj = f[xj] are the nonlinear transformations of the two data vec-
tors. Replacing the inner products with the kernel function means that we do not have
to explicitly calculate the transformed vectors z, and hence they may be of very high,
or even inﬁnite dimensions. See Section 8.4 for a more detailed description of kernel
functions.
The kernel logistic regression model (compare to Equation 9.23) is hence
Pr(w|X,ψ) =
IY
i=1
Bernwi [sig[ai]] =
IY
i=1
Bernwi
h
sig[ψT K[X,xi]
i
,
(9.31)
where the notation K[X,x]i represents a column vector of dot products where element
k is given by k[xk,xi].
Figure 9.12 Kernel logistic regression using RBF kernel and maximum likelihood learning. a)
With a small length scale λ, the model does not interpolate much from the data examples. b) With
a reasonable length scale, the classiﬁer does a good job of modeling the posterior Pr(w = 1|x).
c) With a large length scale, the estimated posterior is very smooth and the model interpolates
conﬁdent decisions into regions such as the top-left where there is no data.

9.6
Relevance vector classiﬁcation
147
Figure 9.13 Kernel logistic regression with
RBF kernel in a Bayesian setting: we now
take account of our uncertainty in the dual
parameters ψ by approximating their poste-
rior distribution using Laplace’s method and
marginalizing them out of the model. This
produces a very similar result to the max-
imum likelihood case with the same length
scale (Figure 9.12b). However, as is typical
with Bayesian implementations, the conﬁ-
dence is (appropriately) somewhat lower.
For maximum likelihood learning, we simply optimize the log posterior probability
L with respect to the parameters, which requires the derivatives:
∂L
∂ψ = −
I
X
i=1
(sig[ai] −wi)K[X,xi]
∂2L
∂ψ2 = −
I
X
i=1
sig[ai](1 −sig[ai])K[X,xi]K[xi,X].
(9.32)
The Bayesian formulation of kernel logistic regression, which is sometimes known
as Gaussian process classiﬁcation, proceeds along similar lines; we follow the dual
formulation, replacing each the dot products between data examples with the kernel
function.
A very common example of a kernel function is the radial basis kernel in which the
nonlinear transformation and inner product operations are replaced by
k[xi,xj] = exp

−0.5
(xi −xj)T (xi −xj)
λ2

.
(9.33)
This is equivalent to computing transformed vectors zi and zj of inﬁnite length, where
each entry evaluates the data x against a radial basis function at a different position, and
then computing the inner product zT
i zj. Examples of the kernel logistic regression with
a radial basis kernel are shown in Figures 9.12 and 9.13.
9.6
Relevance vector classiﬁcation
The Bayesian version of the kernel logistic regression model is powerful, but compu-
tationally expensive as it requires us to compute dot products between the new data
example and the all of the training examples (in the kernel function in Equation 9.31).

148
9 Classiﬁcation models
It would be more efﬁcient if the model depended only sparsely on the training data. To
9.5
achieve this, we impose a penalty for every nonzero weighted training example. As in
the relevance regression model (Section 8.8), we replace the normal prior over the dual
parameters ψ (Equation 9.25) with a product of one-dimensional t-distributions so that
Pr(ψ) =
IY
i=1
Studψi [0,1,ν].
(9.34)
Applying the Bayesian approach to this model with respect to the parameters Ψ is known
as relevance vector classiﬁcation.
Following the argument of Section 8.6, we rewrite each student t-distribution as a
marginalization of a joint distribution Pr(ψi,hi)
Pr(ψ) =
IY
i=1
Z
Normψi

0, 1
hi

Gamhi
hν
2, ν
2
i
dhi
=
Z
Normψ[0,H−1]
D
Y
d=1
Gamhd[ν/2,ν/2] dH,
(9.35)
where the matrix H contains the hidden variables {hi}I
i=1 on its diagonal and zeros
elsewhere. Now we can write the model likelihood as
Pr(w|X)
(9.36)
=
Z
Pr(w|X,ψ)Pr(ψ) dψ
=
Z Z
IY
i=1
Bernwi
h
sig[ψT K[X,xi]
i
Normψ[0,H−1]
D
Y
d=1
Gamhd[ν/2,ν/2] dHdψ.
Now we make two approximations.
First, we use the Laplace approximation to
describe the ﬁrst two terms in this integral as a normal distribution with mean µ and
covariance Σ centered at the MAP parameters, and use the following result for the
integral over ψ:
Z
q(ψ) dψ ≈q(µ)
Z
exp

−1
2(ψ −µ)T Σ−1(ψ −µ)

dψ
= q(µ)(2π)D/2|Σ|1/2.
(9.37)
This yields the expression
Pr(w|X) ≈
(9.38)
Z
IY
i=1
(2π)I/2|Σ|0.5Bernwi

sig[µT K[X,xi]

Normµ

0,H−1
Gamhi
hν
2, ν
2
i
dH,
where the matrix H contains the hidden variables {hi}I
i=1 on the diagonal.

9.6
Relevance vector classiﬁcation
149
In the second approximation, we maximize over the hidden variables, rather than
integrate over them. This yields the expression:
Pr(w|X) ≈
(9.39)
max
H
" IY
i=1
(2π)I/2|Σ|0.5Bernwi

sig[µT K[X,xi]

Normµ

0,H−1
Gamhi
hν
2, ν
2
i#
.
To learn the model, we now alternate between updating the mean and variance µ and
Σ of the posterior distribution and updating the hidden variables {hi}. To update the
mean and variance parameters, we ﬁnd the solution ˆψ that maximizes
L =
I
X
i=1
log
h
Bernwi
h
sig[ψT K[X,xi]
ii
+ log

Normψ

0,H−1
(9.40)
using the derivatives
∂L
∂ψ = −
I
X
i=1
(sig[ai] −wi)K[X,xi] −Hψ
∂2L
∂ψ2 = −
I
X
i=1
sig[ai](1 −sig[ai])K[X,xi]K[xiX] −H,
(9.41)
and then set
µ = ˆψ
(9.42)
Σ = −
 ∂2L
∂ψ2
−1
ψ= ˆψ
.
To update the hidden variables hi we use the same expression as for relevance vector
regression:
hnew
i
= 1 −hiΣii + ν
µ2
i + ν
.
(9.43)
As this optimization proceeds, some of the hidden variables hi will become very
large. This means that the prior over the relevant parameter becomes very concentrated
around zero and that the associated datapoints contribute nothing to the ﬁnal solution.
These can be removed, leaving a kernelized classiﬁer that depends only sparsely on the
data and can hence be evaluated very efﬁciently.
In inference, we aim to compute the distribution over the world state w∗given a new
data example x∗. We take the familiar strategy of approximating the posterior distribution
over the activation as
Pr(a) = Pr(ψT K[X,x∗]) = Norma[µa,σ2
a]
(9.44)
= Norma[µT K[X,x],K[x∗,X]ΣK[X,x∗]],
and then approximate the predictive distribution using Equation 9.19.
An example of relevance vector classiﬁcation is shown in Figure 9.14, which shows
that the data set can be discriminated based on 6 of the original 40 datapoints. This
results in a considerable computational saving and the simpler solution guards against
overﬁtting of the training set.

150
9 Classiﬁcation models
Figure 9.14 Relevance vector regression
with RBF kernel. We place a prior over the
dual parameters ψ that encourages sparsity.
After learning, the posterior distribution over
most of the parameters is tightly centered
around zero and they can be dropped from
the model. Large points indicate data exam-
ples associated with nonzero dual parame-
ters. The solution here can be computed from
just 6 of the 40 datapoints but nonetheless
classiﬁes the data almost as well as the full
kernel approach (Figure 9.13).
9.7
Incremental ﬁtting and boosting
In the previous section, we developed the relevance vector classiﬁcation model in which
we applied a prior that encourages sparsity in the dual logistic regression parameters
ψ and hence encouraged the model to depend on only a subset of the training data. It
is similarly possible to develop a sparse logistic regression method by placing a prior
9.6
that encourages sparsity in the original parameters φ and hence encourages the classiﬁer
to depend only on a subset of the data dimensions. This is left as an exercise to the
reader.
In this section we will investigate a different approach to inducing sparsity; we will
add one parameter at a time to the model in a greedy fashion; in other words, we add
the parameter that improves the objective function most at each stage and then consider
this ﬁxed. As the most discriminative parts of the model are added ﬁrst, it is possible
to truncate this process after only a small fraction of the parameters are added and still
achieve good results. The remaining, unused parameters can be considered as having a
value of zero, and so this model also provides a sparse solution. We term this approach
incremental ﬁtting. We will work with the original formulation (so that the sparsity
is over the data dimensions), although these ideas can equally be adapted to the dual
case.
To describe the incremental ﬁtting procedure, let us work with the nonlinear formu-
lation of logistic regression (Section 9.3) where the probability of the class given the data
was described as
Pr(wi|xi) = Bernwi[sig[ai]],
(9.45)
where sig[•] is the logistic sigmoid function and the activation ai is given by
ai = φT zi = φT f[xi],
(9.46)
and f[•] is a nonlinear transformation that returns the transformed vector zi.
To simplify the subsequent description, we will now write the activation term in a
slightly different way so that the dot product is described explicitly as a weighted sum of

9.7
Incremental ﬁtting and boosting
151
individual nonlinear functions of the data
ai = φ0 +
K
X
k=1
φkf[xi,ξk].
(9.47)
Here f[•,•] is a ﬁxed nonlinear function that takes the data vector xi and some parame-
ters ξk and returns a scalar value. In other words, the kth entry of the transformed vector
z arises by passing the data x through the function with the kth parameters ξk. Example
functions f[•,•] might include:
• Arc tan functions, ξ = {α}
f[x,ξ] = arctan[αT x].
(9.48)
• Radial basis functions, ξ = {α,λ0}
f[x,ξ] = exp

−(x −α)T (x −α)
λ2
0

.
(9.49)
In incremental learning, we construct the activation term in Equation 9.47 piecewise.
At each stage we add a new term, leaving all of the previous terms unchanged except the
additive constant φ0. So, at the ﬁrst stage, we use the activation
ai = φ0 + φ1f[xi,ξ1]
(9.50)
and learn the parameters φ0,φ1, and ξ1 using the maximum likelihood approach. At the
second stage, we ﬁt the function
ai = φ0 + φ1f[xi,ξ1] + φ2f[xi,ξ2]
(9.51)
and learn the parameters φ0,φ2, and ξ2, while keeping the remaining parameters φ1 and
ξ1 constant. At the Kth stage, we ﬁt a model with activation
ai = φ0 +
K
X
k=1
φkf[xi,ξk]
(9.52)
and learn the parameters φ0,φK, and ξK, while keeping the remaining parameters
φ1 ...φk−1 and ξ1 ...ξk−1 constant.
At each stage, the learning is carried out using the maximum likelihood approach.
We use a nonlinear optimization procedure to maximize the log posterior probability L
with respect to the relevant parameters. The derivatives required by the optimization
procedure depend on the choice of nonlinear function but can be computed using the
chain rule relations (Equation 9.21).
This procedure is obviously suboptimal as we do not learn the parameters together
or even revisit early parameters once they have been set. However, it has three nice
properties.
1. It creates sparse models: the weights φk tend to decrease as we move through
the sequence, and each subsequent basis function tends to have less inﬂuence
on the model. Consequently, the series can be truncated to the desired length and
the associated performance is likely to remain good.

152
9 Classiﬁcation models
Figure 9.15 Incremental approach to ﬁtting nonlinear logistic regression model with RBF func-
tions. a) Before ﬁtting, the activation (and hence the posterior probability) is uniform. b) Posterior
probability after ﬁtting one function (center and scale of RBF shown in blue). c–e) After ﬁtting
two, three, and four RBFs. e) After ﬁtting ten RBFs. f) The data are now all classiﬁed correctly as
can be seen from the decision boundary (cyan line).
2. The previous logistic regression models have been suited to cases where either
the dimensionality D of the data is small (original formulation) or the number of
training examples I is small (dual formulation). However, it is quite possible that
neither of these things is true. A strong advantage of incremental ﬁtting is that it is
still practical when the data are high-dimensional and there are a large number of
training examples. During training, we do not need to hold all of the transformed
vectors z in memory at once: at the Kth stage, we need only the Kth dimension
of the transformed parameters zK = f[x,ξK] and the aggregate of the previous
contributions to activation term PK−1
k=1 φkf[xi,ξk].
3. Learning is relatively inexpensive because we only optimize a few parameters at
each stage.
Figure 9.15 illustrates the incremental approach to learning a 2D data set using radial
basis functions. Notice that even after only a few functions have been added to the
sequence, the classiﬁcation is substantially correct. Nonetheless, it is worth continuing
to train this model even after the training data are classiﬁed correctly. Usually the model
continues to improve and the classiﬁcation performance on test data will continue to
increase for some time.

9.8
Classiﬁcation trees
153
9.7.1
Boosting
There is a special case of the incremental approach to ﬁtting nonlinear logistic regression
9.7
that is commonly used in vision applications. Consider a logistic regression model based
on a sum of step functions
ai = φ0 +
K
X
k=1
φkheaviside[αT
k xi],
(9.53)
where the function heaviside[•] returns 0 if its argument is less than 0 and 1 otherwise.
As usual, we have attached a 1 to the start of the data x so that the parameters αk contain
both a direction [αk1,αk2,...,αKD] in the D-directional space (which determines the
direction of the step function) and an offset αk0 (that determines where the step occurs).
One way to think about the step functions is as weak classiﬁers; they return 0 or 1
depending on the value of xi so each classiﬁes the data. The model combines these weak
classiﬁers to compute a ﬁnal strong classiﬁer. Schemes for combining weak classiﬁers in
this way are generically known as boosting and this particular model is called logitboost.
Unfortunately, we cannot simply ﬁt this model using a gradient-based optimiza-
tion approach because the derivative of the heaviside step function with respect to the
parameters αk is not smooth. Consequently, it is usual to predeﬁne a large set of J
weak classiﬁers and assume that each parameter vector αk is taken from this set so that
αk ∈{α(1) ...α(J)}.
As before, we learn the logitboost model incrementally by adding one term at a time
to the activation (Equation 9.53). However, now we exhaustively search over the weak
classiﬁers {α(1) ...α(J)} and for each use nonlinear optimization to estimate the weights
φ0 and φk. We choose the combination {αk,φ0,φk} that improves the log likelihood
the most. This procedure may be made even more efﬁcient (but more approximate) by
choosing the weak classiﬁer based on the log likelihood after just a single Newton or
gradient descent step in the nonlinear optimization stage. When we have selected the
best weak classiﬁer αk, we can return and perform the full optimization over the offset
φ0 and weight φk.
Note that after each classiﬁer is added, the relative importance of each datapoint is
effectively changed: the datapoints contribute to the derivative according to how well
they are currently predicted (Equation 9.9). Consequently, the later weak classiﬁers
become more specialized to the more difﬁcult parts of the data set that are not well
classiﬁed by the early ones. Usually, these are close to the ﬁnal decision boundary.
Figure 9.16 shows several iterations of the boosting procedure. Because the model
is composed from step functions, the ﬁnal classiﬁcation boundary is irregular and does
not interpolate smoothly between the data examples. This is a potential disadvantage of
this approach. In general, a classiﬁer based on arc tangent functions (roughly smooth
step functions) will have superior generalization and can also be ﬁt using continuous
optimization. It could be argued that the step function is faster to evaluate, but even this
is illusory as more complex functions such as the arc tangent can be approximated with
look-up tables.
9.8
Classiﬁcation trees
In the nonlinear logistic regression model, we created complex decision boundaries using
an activation function that is a linear combination φT z of nonlinear functions z = f[x]

154
9 Classiﬁcation models
Figure 9.16 Boosting. a) We start with a uniform prediction Pr(w = 1|x) and b) incrementally
add a step function to the activation (green line indicates position of step). In this case the param-
eters of the step function were chosen greedily from a predetermined set containing 20 angles
each with 40 offsets. c)-e) As subsequent functions are added the overall classiﬁcation improves.
f) However, the ﬁnal decision surface (cyan line) is complex and does not interpolate smoothly
between regions of high conﬁdence.
of the data x. We now investigate an alternative method to induce complex decision
boundaries: we partition data space into distinct regions and apply a different classiﬁer
in each region.
The branching logistic regression model has activation,
ai = (1 −g[xi,ω])φT
0 xi + g[xi,ω]φT
1 xi.
(9.54)
The term g[•,•] is a gating function that returns a number between 0 and 1. If this
gating function returns 0, then the activation will be φ0xi, whereas if it returns 1, the
activation will be φ1xi. If the gating returns an intermediate value, then the activation
will be a weighted sum of these two components. The gating function itself depends on
the data xi and takes parameters ω. This model induces a complex nonlinear decision
boundary (Figure 9.17) where the two linear functions φ0xi and φ1xi are specialized
to different regions of the data space. In this context, they are sometimes referred to as
experts.
The gating function could take many forms, but an obvious possibility is to use a
second logistic regression model. In other words, we compute a linear function ωT xi of
the data that is passed through a logistic sigmoid so that
g[xi,ω] = sig[ωT xi].
(9.55)

9.8
Classiﬁcation trees
155
Figure 9.17 Branching logistic regression. a) This data set needs a nonlinear decision surface
(cyan line) to classify the data reasonably. b) This linear activation is an expert that is specialized
to describing the right-hand side of the data. c) This linear activation is an expert that describes
the left-hand side of the data. d) A gating function takes the data vector x and returns a number
between 0 and 1, which we will use to decide which expert contributes at each decision. e) The
ﬁnal activation consists of a weighted sum of the activation indicated by the two experts where
the weight comes from the gating function. f) The ﬁnal classiﬁer predictions Pr(w = 1|x) are
generated by passing this activation through the logistic sigmoid function.
To learn this model we maximize the log probability L = P
i log[Pr(wi|xi)] of the
training data pairs {xi,wi}I
i=1 with respect to all of the parameters θ = {φ0,φ1,ω}. As
usual this can be accomplished using a nonlinear optimization procedure. The parame-
ters can be estimated simultaneously or using a coordinate ascent approach in which we
update the three sets of parameters alternately.
We can extend this idea to create a hierarchical tree structure by nesting gating
functions (Figure 9.18). For example, consider the activation
ai = (1 −g[xi,ω])
h
φT
0 xi + (1 −g[xi,ω0])φT
00xi + g[xi,ω0]φT
01xi
i
(9.56)
+g[xi,ω]
h
φT
1 xi + (1 −g[xi,ω1])φT
10xi + g[xi,ω1]φT
11xi
i
.
This is an example of a classiﬁcation tree.
To learn the parameters θ = {φ0,φ1,φ00,φ01,φ10,φ11,ω,ω0,ω1}, we could take
an incremental approach. At the ﬁrst stage, we ﬁt the top part of the tree (Equation 9.54),
setting parameters ω,φ0,φ1. Then we ﬁt the left branch, setting parameters ω0,φ00,φ01
and subsequently the right branch, setting parameters ω1,φ10,φ11, and so on.

156
9 Classiﬁcation models
Figure 9.18 Logistic classiﬁcation tree. Data ﬂows from the root to the leaves. Each node is a gat-
ing function that weights the contributions of terms in the subbranches in the ﬁnal activation. The
gray region indicates variables that would be learned together in an incremental training approach.
The classiﬁcation tree has the potential advantage of speed. If each gating function
produces a binary output (like the heaviside step function), then each datapoint passes
down just one of the outgoing edges from each node and ends up at a single leaf. When
each branch in the tree is a linear operation (as in this example), these operations can
be aggregated to a single linear operation at each leaf. Since each datapoint receives
specialized processing, the tree need not usually be deep, and new data can be classiﬁed
very efﬁciently.
9.9
Multiclass logistic regression
Throughout this chapter, we have discussed binary classiﬁcation. We now discuss how
9.8
to extend these models to handle N >2 world states. One possibility is to build N one-
against-all binary classiﬁers each of which computes the probability that the nth class
is present as opposed to any of the other classes. The ﬁnal label is assigned according to
the one-against-all classiﬁer with the highest probability.
The one-against-all approach works in practice but is not very elegant. A more prin-
cipled way to cope with multiclass classiﬁcation problems is to describe the the posterior
Pr(w|x) as a categorical distribution, where the parameters λ = [λ1 ...λN] are functions
of the data x
Pr(w|x) = Catw[λ[x]],
(9.57)
where the parameters are in the range λn ∈[0,1] and sum to one, P
n λn = 1.
In
constructing the function λ[x], we must ensure that we obey these constraints.
As for the two class logistic regression case, we will base the model on linear func-
tions of the data x and pass these through a function that enforces the constraints. To this

9.9
Multiclass logistic regression
157
Figure 9.19 Multiclass logistic regression. a) We form one activation for each class based on
linear functions of the data. b) We pass these activations through the softmax function to create
the distribution Pr(w|x) which is shown here as a function of x. The softmax function takes the
three real-valued activations and returns three positive values that sum to one, ensuring that the
distribution Pr(w|x) is a valid probability distribution for all x.
end, we deﬁne N activations (one for each class),
an = φT
nx,
(9.58)
where {φn}N
n=1 are parameter vectors. We assume that as usual we have prepended a 1 to
each of the data vectors xi so that the ﬁrst entry of each parameter vector φn represents
an offset. The nth entry of the ﬁnal categorical distribution is now deﬁned by
λn = softmaxn[a1,a2 ...aN] =
exp[an]
PN
j=1 exp[aj]
.
(9.59)
The function softmax[•] takes the N activations {an}N
n=1, which can take any real
number, and maps them to the N parameters {λn}N
n=1 of the categorical distribution,
which are constrained to be positive and sum to one (Figure 9.19).
To learn the parameters θ = {φn}N
n=1 given training pairs (wi,xi) we optimize the
log likelihood of the training data
L =
I
X
i=1
log[Pr(wi|xi)].
(9.60)
As for the two-class case, there is no closed form expression for the maximum
likelihood parameters. However, this is a convex function, and the maximum can be
found using a nonlinear optimization technique such as the Newton method. These tech-
niques require the ﬁrst and second derivatives of the log likelihood with respect to the
parameters, which are given by
∂L
∂φn
= −
I
X
i=1
(yin −δ[wi −n])xi
∂2L
∂φmφn
= −
I
X
i=1
yim(δ[m −n] −yin)xixT
i ,
(9.61)

158
9 Classiﬁcation models
where we deﬁne the term
yin = Pr(wi = n|xi) = softmaxn[ai1,ai2 ...aiN].
(9.62)
It is possible to extend multiclass logistic regression in all of the ways that we
extended the two-class model. We can construct Bayesian, nonlinear, dual and kernelized
versions. It is possible to train incrementally and combine weak classiﬁers in a boosting
framework. Here, we will consider tree-structured models as these are very common in
modern vision applications.
9.10
Random trees, forests, and ferns
In Section 9.8 we introduced the idea of tree-structured classiﬁers, in which the process-
ing for each data example is different and becomes steadily more specialized. This idea
9.9
has recently become extremely popular for multiclass problems in the form of random
classiﬁcation trees.
As for the two-class case, the key idea is to construct a binary tree where at each
node, the data are evaluated to determine whether it will pass to the left or the right
branch. Unlike in Section 9.8, we will assume that each data point passes into just one
branch. In a random classiﬁcation tree, the data are evaluated against a function q[x] that
was randomly chosen from a predeﬁned family of possible functions. For example, this
might be the response of a randomly chosen ﬁlter. The data point proceeds one way in
the tree if the response of this function exceeds a threshold τ and the other way if not.
While the functions are chosen randomly, the threshold is carefully selected.
We select the threshold that maximizes the log-likelihood L of the data:
L =
I
X
i=1
(1 −heaviside[q[xi] −τ])log
h
Catwi
h
λ[l]ii
(9.63)
+ heaviside[q[xi] −τ]log
h
Catwi
h
λ[r]ii
.
Here the ﬁrst term represents the contribution of the data that passes down the left
branch, and the second term represents the contribution of the data that passes down
the right branch. In each case, the data are evaluated against a categorical distribution
with parameters λ[l] and λ[r], respectively. These parameters are set using maximum
likelihood:
λ[l]
k =
PI
i=1 δ[wi −k](1 −heaviside[q[xi] −τ])
PI
i=1(1 −heaviside[q[xi] −τ])
λ[r]
k =
PI
i=1 δ[wi −k](heaviside[q[xi] −τ])
PI
i=1(heaviside[q[xi] −τ])
.
(9.64)
The log likelihood is not a smooth function of the threshold τ, and so in practice we
maximize the log likelihood by empirically trying a number of different threshold values
and choosing the one that gives the best result.
We then perform this same procedure recursively; the data that pass to the left branch
have a new randomly chosen classiﬁer applied to them, and a new threshold that splits it
again is chosen. This can be done without recourse to the data in the right branch. When
we classify a new data example x∗, we pass it down the tree until it reaches one of the

9.11
Relation to non-probabilistic models
159
leaves. The posterior distribution Pr(w∗|x∗) over the world state w∗is set to Catw∗[λ]
where the parameters λ are the categorical parameters associated with this leaf during
the training process.
The random classiﬁcation tree is attractive because it is very fast to train – after
all, most of its parameters are chosen randomly. It can also be trained with very large
amounts of data as its complexity is linear in the number of data examples.
There are two important variations on this model:
1. A fern is a tree where the randomly chosen functions at each level of the tree
are constrained to be the same. In other words, the data that pass through the
left and right branches at any node are subsequently acted on by the same func-
tion (although the threshold level may optionally be different in each branch). In
practice, this means that every datapoint is acted on by the same sequence of func-
tions. This can make implementation extremely efﬁcient when we are evaluating
the classiﬁer repeatedly.
2. A random forest is a collection of random trees, each of which uses a different ran-
domly chosen set of functions. By averaging together the probabilities Pr(w∗|x∗)
predicted by these trees, a more robust classiﬁer is produced. One way to think
of this is as approximating the Bayesian approach; we are constructing the ﬁnal
answer by taking a weighted sum of the predictions suggested by different sets of
parameters.
9.11
Relation to non-probabilistic models
In this chapter, we have described a family of probabilistic algorithms for classiﬁcation.
Each is based on maximizing either the log Bernoulli probability of the training class
labels given the data (two-class case) or the log categorical probability of the training
class labels given the data (multiclass case).
However, it is more common in the computer vision literature to use non-probabilistic
classiﬁcation algorithms such as the multilayer perceptron, adaboost, or support vector
classiﬁcation. At their core, these algorithms optimize different objective functions and
so are neither directly equivalent to each other, nor to the models in this chapter.
We chose to describe the less common probabilistic algorithms because
• They have no serious disadvantages relative to non-probabilistic techniques,
• They naturally produce estimates of certainty,
• They are easily extensible to the multiclass case, whereas non-probabilistic
algorithms usually rely on one-against-all formulations, and
• They are more easily related to one another and to the rest of the book.
In short, it can reasonably be argued that the dominance of non-probabilistic
approaches to classiﬁcation is largely for historical reasons. We will now brieﬂy describe
the relationship between our models and common non-probabilistic approaches.
The multilayer perceptron (MLP) or neural network is very similar to our nonlinear
logistic regression model in the special case where the nonlinear transformation consists
of a set of sigmoid functions applied to linear projections of data (e.g., zk = arctan[αT
k x]).
In the MLP, learning is known as back propagation and the transformed variable z is
known as the hidden layer.

160
9 Classiﬁcation models
Adaboost is very closely related to the the logitboost model described in this chapter,
but adaboost is not probabilistic. Performance of the two algorithms is similar.
The support vector machine (SVM) is similar to relevance vector classiﬁcation; it is a
kernelized classiﬁer that depends sparsely on the data. It has the advantage that its objec-
tive function is convex, whereas the objective function in relevance vector classiﬁcation
is non-convex and only guarantees to converge to a local minimum. However, the SVM
has several disadvantages: it does not assign certainty to its class predictions, it is not
so easily extended to the multiclass case, it produces solutions that are less sparse than
relevance vector classiﬁcation, and it places more restrictions on the form of the kernel
function. In practice, classiﬁcation performance of the two models is again similar.
9.12
Applications
We now present a number of examples of the use of classiﬁcation in computer vision from
the research literature. In many of the examples, the method used was non-probabilistic
(e.g., adaboost), but is very closely related to the algorithms in this chapter, and one
would not expect the performance to differ signiﬁcantly if these were substituted.
9.12.1 Gender classiﬁcation
The algorithms in this chapter were motivated by the problem of gender detection in
unconstrained facial images. The goal is to assign a label w ∈{0,1} indicating whether
a small patch of an image x contains a male or a female face. Prince and Aghajanian
(2009) developed a system of this type. First, a bounding box around the face was
identiﬁed using a face detector (see next section). The data within this bounding box
were resized to 60 × 60, converted to grayscale and histogram equalized. The resulting
image was convolved with a bank of Gabor functions, and the ﬁltered images sampled at
regular intervals that were proportionate to the wavelength to create a ﬁnal feature vec-
tor of length 1064. Each dimension was whitened to have mean zero and unit standard
deviation. Chapter 13 contains information about these and other preprocessing methods.
A training database of 32,000 examples was used to learn a nonlinear logistic
regression model of the form
Pr(wi|xi) = Bernwi


1
1 + exp
h
−φ0 −PK
k=1 φkf[xi,ξk]
i

,
(9.65)
where the nonlinear functions f[•] were arc tangents of linear projections of the data
so that
f[xi,ξk] = arctan[ξT
k xi].
(9.66)
As usual the data were augmented by prepending a 1 so the projection vectors {ξk} were
of length D + 1. This model was learned using an incremental approach so that at each
stage the parameters φ0,φk and ξk were modiﬁed.
The system achieved 87.5 percent performance with K = 300 arc tangent functions
on a challenging real-world database that contained large variations in scale, pose, light-
ing, and expression similar to the faces in Figure 9.1. Human observers managed only
95 percent performance on the same database using the resized face region alone.

9.12
Applications
161
9.12.2 Face and pedestrian detection
Before we can determine the gender of a face, we must ﬁrst ﬁnd it. In face detection
(Figure 7.1), we assign a label w ∈{0,1} to a small region of the image x indicating
whether a face is present (w =1) or not (w =0). To ensure that the face is found, this
process is repeated at every position and scale in the image and consequently the classiﬁer
must be very fast.
Viola and Jones (2004) presented a face detection system based on adaboost (Figure
9.20). This is a non-probabilistic analogue of the boosting methods described in Section
9.7.1. The ﬁnal classiﬁcation is based on the sign of a sum of nonlinear functions of the
data
a = φ0 +
K
X
k=1
φkf[x,ξk],
(9.67)
where the nonlinear functions f[•] are heaviside step functions of projections of the data
(weak classiﬁers giving a response of zero or one for each possible data vector x) so that
f[x,ξk] = heaviside[ξT
k x].
(9.68)
As usual, the data vector x was prepended with a 1 to account for an offset.
The system was trained on 5,000 faces and 10,000 non-face regions, each of which
were represented as a 24 × 24 image patch. Since the model is not smooth (due to the
step function) gradient-based optimization is unsuitable, and so Viola and Jones (2004)
exhaustively searched through a very large number of predeﬁned projections ξk.
There were two aspects of the design that ensured that the system ran quickly.
Figure 9.20 Fast face detection using a boosting method (Viola and Jones 2004). a) Each weak
classiﬁer consists of the response of the image to a Haar ﬁlter, which is then passed through a step
function. b) The ﬁrst two weak classiﬁers learned in this implementation have clear interpretations:
The ﬁrst responds to the dark horizontal region belonging to the eyes, and the second responds to
the relative brightness of the bridge of the nose. c) The data passes through a cascade: most regions
can be quickly rejected after evaluating only a few weak classiﬁers as they look nothing like faces.
More ambiguous regions undergo further preprocessing. d) Example results. Adapted from Viola
and Jones (2004).

162
9 Classiﬁcation models
Figure 9.21 Boosting methods based on the thresholded responses of Haar functions have also
been used for pedestrian detection in video footage. a) To improve detection rates two subsequent
frames are used. The absolute difference between the frames is computed as is the difference when
one of the frames is offset in each of four directions. The set of potential weak classiﬁers consists
of Haar functions applied to all of these representations. b,c) Example results. Adapted from Viola
et al. (2005). c⃝2005 Springer.
1. The structure of the classiﬁer was exploited: training in boosting is incremental –
the “weak classiﬁers” (nonlinear functions of the data) are incrementally added
to create an increasingly sophisticated strong classiﬁer. Viola and Jones (2004)
exploited this structure when they ran the classiﬁer: they reject regions that are
very unlikely to be faces based on responses of the ﬁrst few weak classiﬁers and
only subject more ambiguous regions to further processing. This is known as a
cascade structure. During training, the later stages of the cascade are trained with
new negative examples that were not rejected by the earlier stages.
2. The projections ξk were carefully chosen so that they were very fast to evalu-
ate: they consisted of Haar-like ﬁlters (Section 13.1.3), which require only a few
operations to compute.
The ﬁnal system consisted of 4297 weak classiﬁers divided into a 32-stage cascade.
It found 91.1 percent of 507 frontal faces across 130 images, with a false positive rate of
less than 1 per frame, and processed images in fractions of a second.
Viola et al. (2005) developed a similar system for detecting pedestrians in video
sequences (Figure 9.21). The main modiﬁcation was to extend the set of weak classiﬁers
to encompass features that span more than one frame and hence select for the particular
temporal patterns associated with human motion. To this end their system used not only
the image data itself, but also the difference image between adjacent frames and similar
difference images when taken after offsetting the frames in each of four directions. The

9.12
Applications
163
ﬁnal system achieved an 80 percent detection rate with a false alarm rate of 1/400,000
which corresponds to one false positive for every two frames.
9.12.3 Semantic segmentation
The goal of semantic segmentation is to assign a label w ∈{1...M} to each pixel indi-
cating which of M objects is present, based on the local image data x. Shotton et al.
(2009) developed a system known as textonboost that was based on a non-probabilistic
boosting algorithm called jointboost (Torralba et al. 2007). The decision was based
on a one-against-all strategy in which M binary classiﬁers are computed based on the
weighted sums
am = φ0m +
K
X
k=1
φkmf[x,ξk],
(9.69)
where the nonlinear functions f[•] were once more based on heaviside step functions.
Note that the weighted sums associated with each object class share the same nonlin-
ear functions but weight them differently. After computing these series, the decision is
assigned based on the activation am that is the greatest.
Shotton et al. (2009) based the nonlinear functions on a texton representation of the
image: each pixel in the image is replaced by a discrete index indicating the “type” of tex-
ture present at that position (see Section 13.1.5). Each nonlinear function considers one
of these texton types and computes the number of times that it is found within a rectan-
gular area. This area has a ﬁxed spatial displacement from the pixel under consideration
(Figures 9.22c–f). If this displacement is zero, then the function provides evidence about
the pixel directly (e.g., it looks like grass). If the spatial displacement is larger, then the
function provides evidence of the local context (e.g., there is grass nearby, so this pixel
may belong to a cow).
For each nonlinear function, an offset is added to the texton count and the result is
passed through a step function. The system was learned incrementally by assessing each
of a set of a randomly chosen classiﬁers (deﬁned by the choice of texton, rectangular
region, and offset) and choosing the best at the current stage.
The full system also included a postprocessing step in which the result was reﬁned
using a conditional random ﬁeld model (see Chapter 12). It achieved 72.2 percent per-
formance on the challenging MRSC database that includes 21 diverse object classes
including wiry objects such as bicycles and objects with a large degree of variation such
as dogs.
9.12.4 Recovering surface layout
To recover the surface layout of a scene we assign a label w ∈{1,...3} to each pixel in
the image indicating whether the pixel contains a support object (e.g., ﬂoor), a vertical
object (e.g., building), or the sky. This decision is based on local image data x. Hoiem
et al. (2007) constructed a system of this type using a one-against-all principle. Each
of the three binary classiﬁers was based on logitboosted classiﬁcation trees; different
classiﬁcation trees are treated as weak classiﬁers, and the results are weighted together
to compute the ﬁnal probability.
Hoiem et al. (2007) worked with the intermediate representation of superpixels – an
oversegmentation of the scene into small homogeneous regions, which are assumed to
belong to the same object. Each superpixel was assigned a label w using the classiﬁer

164
9 Classiﬁcation models
Figure 9.22 Semantic image labeling using “TextonBoost.” a) Original image. b) Image con-
verted to textons – a discrete value at each pixel indicating the type of texture that is present. c)
The system was based on weak classiﬁers that count the number of textons of a certain type within
a rectangle that is offset from the current position (yellow cross). d) This provides both information
about the object itself (contains sheep-like textons) and nearby objects (near to grass-like textons).
e,f) Another example of a weak classiﬁer. g) Test image. h) Per-pixel classiﬁcation is not very
precise at the edges of objects and so i) a conditional random ﬁeld is used to improve the result.
j) Examples of results and ground truth. Adapted from Shotton et al. (2009). c⃝2009 Springer.
based on a data vector x, which contained location, appearance, texture, and perspective
information associated with the superpixel.
To mitigate against the possibility that the original superpixel segmentation was
wrong, multiple segmentations were computed and the results merged to provide a ﬁnal
per-pixel classiﬁcation (Figure 9.23). In the full system, regions that were classiﬁed as
vertical were subclassiﬁed into left-facing planar surfaces, frontoparallel planar surfaces,
or right-facing planar surfaces or nonplanar surfaces, which may be porous (e.g., trees)
or solid. The system was trained and tested on a data set consisting of images collected
from the Web including diverse environments (forests, cities, roads, etc.) and conditions
(snowy, sunny, cloudy, etc.). The data set was pruned to remove photos where the horizon
was not within the image.
The system correctly labeled 88.1 percent of pixels correctly with respect to the main
three classes and 61.5 percent correctly with respect to the subclasses of the vertical
surface. This algorithm was the basis of a remarkable system for creating a 3D model
from a single 2D photograph (Hoiem et al. 2005).
9.12.5 Identifying human parts
Shotton et al. (2011) describe a system that assigns a discrete label w ∈{1,...,31},
indicating which of 31 body parts is present at each pixel based on a depth image x. The
resulting distribution of labels is an intermediate representation in a system that proposes
a possible conﬁguration of the 3D joint positions in the Microsoft Kinect gaming system
(Figure 9.24).

Discussion
165
Figure 9.23 Recovering surface layout. The goal is to take an image and return a label indicating
whether the pixel is part of a support surface (green pixels) vertical surface (red pixels) or the
sky (blue pixels). Vertical surfaces were subclassiﬁed into planar objects at different orientations
(left arrows, upward arrows and right arrows denote left-facing, frontoparrallel and right-facing
surfaces) and nonplanar objects which can be porous (marked as “o”) or nonporous (marked as
“x”. The ﬁnal classiﬁcation was based on (i) location cues (which include position in the image
and position relative to the horizon), (ii) color cues, (iii) texture cues, and (iv) perspective cues
which were based on the statistics of line segments in the region. The ﬁgure shows example
classiﬁcations for each of these cues alone and when combined. Adapted from Hoiem et al. (2007).
c⃝2007 Springer.
The classiﬁcation was based on a forest of decision trees: the ﬁnal probability
Pr(w|x) is an average (i.e., a mixture) of the predictions from a number of different clas-
siﬁcation trees. The goal is to mitigate against biases introduced by the greedy method
with which a single tree is trained.
Within each tree, the decision about which branch a datapoint travels down is based
on the difference in measured depths at two points, each of which is spatially offset from
the current pixel. The offsets are inversely scaled by the distance to the pixel itself, which
ensures that they address the same relative positions on the body when the person moves
closer or further away to the depth camera.
The system was trained from a very large data set of 900,000 depth images, which
were synthesized based on motion capture data and consisted of three trees of depth 20.
Remarkably, the system is capable of assigning the correct label 59 percent of the time,
and this provides a very solid basis for the subsequent joint proposals.
Discussion
In this chapter we have considered classiﬁcation problems. We note that all of the ideas
that were applied to regression models in Chapter 8 are also applicable to classiﬁcation
problems. However, for classiﬁcation the model includes a nonlinear mapping between
the data x and the parameters of the distribution Pr(w|x) over the world w. This means

166
9 Classiﬁcation models
Figure 9.24 Identifying human parts. a) The goal of the system is to take a depth image x and
assign a discrete label w to each pixel indicating which of 31 possible body parts is present. These
depth labels are used to form proposals about the position of 3D joints. b) The classiﬁcation is
based on decision trees. At each point in the tree, the data are divided according to the rela-
tive depth at two points (red circles) offset relative to the current pixel (yellow crosses). In this
example, this difference is large in both cases, whereas in c) this difference is small – hence these
differences provide information about the pose. d,e) Two more examples of depth image, labeling
and hypothesized pose. Adapted from Shotton et al. (2011). c⃝2011 IEEE.
that we cannot ﬁnd the maximum likelihood solution in closed form (although the prob-
lem is still convex) and we cannot compute a full Bayesian solution without making
approximations.
Classiﬁcation techniques have many uses in machine vision. Notice though that these
models have no domain-speciﬁc information about the problem other than that provided
by the preprocessing of the data. This is both an advantage (they ﬁnd many applications)
and a disadvantage (they cannot take advantage of a priori information about the prob-
lem). In the remaining part of the book we will explore models that introduce increasing
amounts of domain-speciﬁc information to the problem.
Notes
Classiﬁcation in vision: Classiﬁcation techniques such as those discussed in this chapter have
been applied to many problems in vision including face detection (Viola and Jones 2004), surface
layout estimation creation (Hoiem et al. 2007), boundary detection (Doll´ar et al. 2006), keypoint
matching (Lepetit et al. 2005), body part classiﬁcation (Hoiem et al. 2007), semantic segmentation
(He et al. 2004), object recognition (Csurka et al. 2004), and gender classiﬁcation (Kumar et al.
2008).
Probabilistic classiﬁcation: More information about logistic regression can be found in Bishop
(2006) and many other statistics textbooks.
Kernel logistic regression (or Gaussian process
regression) was presented in Williams and Barber (1998) and more information can be found in

Notes
167
Rasmussen and Williams (2006). A sparse version of kernel logistic regression (relevance vector
classiﬁcation) was presented by Tipping (2001) and a sparse multiclass variant was developed by
Brishnapuram et al. (2005). Probabilistic interpretations of boosting were introduced by Friedman
et al. (2000). Random forests of multinomial regressors were introduced in Prinzie and Van den
Poel (2008).
Other classiﬁcation schemes: In this chapter, we have presented a family of probabilistic clas-
siﬁcation models based on logistic regression. There are other non-probabilistic techniques for
classiﬁcation, and these include single and multilayer perceptrons (Rosenblatt 1958; Rumelhart
et al. 1986), support vector machines (Vapnik 1995; Cristianini and Shawe-Taylor 2000), and
adaboost (Freund and Schapire 1995). A critical difference between these techniques is the under-
lying objective function. Logistic regression models optimize the log Bernoulli probability, but
the other models optimize different criteria, such as the hinge loss (support vector machines) or
exponential error (adaboost). It is difﬁcult to make general statements about the relative merits of
these approaches, but it is probably fair to say that (i) there is no major disadvantage to using the
probabilistic techniques in this chapter and (ii) the choice of classiﬁcation method is usually less
important in vision problems than the preprocessing of the data. Methods based on boosting and
classiﬁcation trees are particularly popular in vision because of their speed.
Boosting: Adaboost was introduced by Freund and Schapire (1995). Since then there have been a
large number of variations, most of which have been used in computer vision. These include dis-
crete adaboost (Freund and Schapire 1996), real adaboost (Schapire and Singer 1998), gentleboost
(Friedman et al. 2000), logitboost (Friedman et al. 2000), ﬂoatboost (Li et al. 2003), KLBoost
(Liu and Shum 2003), asymmetric boost (Viola and Jones 2002), and statboost (Pham and Cham
2007a). Boosting has also been applied to the multiclass case (Schapire and Singer 1998; Torralba
et al. 2007) and for regression (Friedman 1999). A review of boosting approaches can be found in
Meir and M¨atsch (2003).
Classiﬁcation trees: Classiﬁcation trees have a long history in computer vision, dating back to at
least Shepherd (1983). Modern interest was stimulated by Amit and Geman (1997) and Breiman
(2001) who investigated the use of random forests. Since this time classiﬁcation trees and forests
have been applied to keypoint matching (Lepetit et al. 2005), segmentation (Yin et al. 2007), human
pose detection (Rogez et al. 2006; Shotton et al. 2011), object detection (Bosch et al. 2007), image
classiﬁcation (Moosmann et al. 2006, 2008), deciding algorithm suitability (Mac Aodha et al.,
2010), detection occlusions (Humayun et al. 2011), and semantic image segmentation (Shotton
et al. 2009).
Gender classiﬁcation: Automatic determination of gender from a facial image has variously been
tackled with neural networks (Golomb et al. 1990), support vector machines (Moghaddam and
Yang 2002), linear discriminant analysis (Bekios-Calfa et al. 2011) and both adaboost (Baluja and
Rowley 2003), and logitboost (Prince and Aghajanian 2009). A review is provided by M¨akinen
and Raisamo (2008b) and quantative comparisons are presented in M¨akinen and Raisamo (2008a).
Representative examples of the state of the art can be found in Kumar et al. (2008) and Shan (2012).
Face detection: The application of boosting to face detection (Viola and Jones 2004) usurped
earlier techniques (e.g., Osuna et al. 1997; Schneiderman and Kanade 2000). Since then, many
boosting variants have been applied to the problem including ﬂoatboost (Li et al. 2002; Li and
Zhang 2004), gentleboost (Lienhart et al. 2003), realboost (Huang et al. 2007a; Wu et al. 2007),
asymboost (Pham and Cham 2007b; Viola and Jones 2002), and statboost (Pham and Cham 2007a).
A recent review of this area can be found in Zhang and Zhang (2010).
Semantic segmentation: The authors of the system described in the text (Shotton et al. 2008b)
subsequently presented a much faster system based on classiﬁcation trees (Shotton et al. 2009). A
recent comparison of quantitative performance can be found in Ranganathan (2009). Other work
has investigated the imposition of prior knowledge such as the copresence of object classes (He
et al. 2006) and likely spatial conﬁgurations of objects (He et al. 2004).

168
9 Classiﬁcation models
Problems
9.1 The logistic sigmoid function is deﬁned as
sig[a] =
1
1 + exp[−a].
Show that (i) sig[−∞] = 0, (ii) sig[0] = 0.5, sig[∞] = 1.
9.2 Show that the derivative of the log posterior probability for the logistic regression model
L =
I
X
i=1
wi log

1
1 + exp[−φT xi]

+
I
X
i=1
(1 −wi)log

exp[−φT xi]
1 + exp[−φT xi]

with respect to the parameters φ is given by
∂L
∂φ = −
I
X
i=1
(sig[ai] −wi)xi.
9.3 Show that the second derivatives of the log likelihood of the logistic regression model is given
by
∂2L
∂φ2 = −
I
X
i=1
sig[ai](1 −sig[ai])xixT
i .
9.4 Consider ﬁtting a logistic regression model to 1D data x where the two classes are perfectly
separable. For example, perhaps all the data x where the world state w=0 takes values less than 0
and all the data x where the world state is w =1 takes values greater than 1. Hence it is possible
to classify the training data perfectly. What will happen to the parameters of the model during
learning? How could you rectify this problem?
9.5 Compute the Laplace approximation to a beta distribution with parameters α = 1.0, β = 1.0.
9.6 Show that the Laplace approximation to a univariate normal distribution with mean µ and
variance σ2 is the normal distribution itself.
9.7 Devise a method to choose the scale parameter λ0 in the radial basis function in kernel logistic
regression (Equation 9.33).
9.8 A mixture of experts (Jordan and Jacobs 1994) divides space into different regions, each of
which receives specialized attention (Figure 9.25). For example, we could describe the data as a
mixture of logistic classiﬁers so that
Pr(wi|xi) =
K
X
k=1
λk[xi]Bernwi
h
sig[φT
k xi]
i
.
Each logistic classiﬁer is considered as an expert and the mixing weights decide the combination
of experts that are applied to the data. The mixing weights, which are positive and sum to one,
depend on the data x: for a two-component model, they could be based on a second logistic
regression model with activation ωT x. This model can be expressed as the marginalization of a
joint distribution between wi and a hidden variable hi so that
Pr(wi|xi) =
K
X
k=1
Pr(wi,hi = k|xi) =
K
X
k=1
Pr(wi|hi = k,xi)Pr(hi = k|xi),

Problems
169
Figure 9.25 Mixture of two experts model for 1D data. Pink circles indicate positive examples.
Green circles indicate negative examples. a) Two expert is specialized to model the left and right
sides of the data respectively. b) The mixing weights change as a function of the data. c) The ﬁnal
output of the model is mixture of the two constituent experts and ﬁts the data well.
where
Pr(wi|hi = k,xi) = Bernwi
h
sig[φT
k xi]
i
Pr(hi = k|xi) = Bernhi
h
sig[ωT xi]
i
.
How does this model differ from branching logistic regression (Section 9.8)? Devise a learning
algorithm for this model.
9.9 The softmax[•,•,...,•] function is deﬁned to return a multivariate quantity where the kth
element is given by
sk = softmaxk[a1,a2,...aK] =
exp[ak]
PK
j=1 exp[aj]
.
Show that 0 < sk < 1 and that PK
k=1 sk = 1.
9.10 Show that the ﬁrst derivative of the log-probability of the multiclass logistic regression model
is given by Equation 9.61.
9.11 The classiﬁers in this chapter have all been based on continuous data x. Devise a model
that can distinguish between M world states w ∈{1...M} based on a discrete observation x ∈
{1...K} and discuss potential learning algorithms.


Part III
Connecting local models

The models in chapters 6–9 describe the relationship between a set of measurements and
the world state. They work well when the measurements and the world state are both low
dimensional. However, there are many situations where this is not the case, and these
models are unsuitable.
For example, consider the semantic image labeling problem in which we wish to
assign a label that denotes the object class to each pixel in the image. For example, in
a road scene we might wish to label pixels as ‘road’, ‘sky’, ‘car’, ‘tree’, ‘building’ or
‘other’. For an image with N = 10000 pixels, this means we need to build a model relat-
ing the 10000 measured RGB triples to 610000 possible world states. None of the models
discussed so far can cope with this challenge: the number of parameters involved (and
hence the amount of training data and the computational requirements of the learning
and inference algorithms) is far beyond what current machines can handle.
One possible solution to this problem would be to build a set of independent local
models: for example, we could build models that relate each pixel label separately to the
nearby RGB data. However, this is not ideal as the image may be locally ambiguous. For
example, a small blue image patch might result from a variety of semantically different
classes: sky, water, a car door or a person’s clothing. In general, it is insufﬁcient to build
independent local models.
The solution to this problem is to build local models that are connected to one another.
Consider again the semantic labeling example: given the whole image, we can see that
when the image patch is blue and is found above trees and mountains and alongside
similar patches across the top of the image, then the correct class is probably sky. Hence,
to solve this problem, we still model the relationship between the label and its local
image region, but we also connect these models so that nearby elements can help to
disambiguate one another.
In chapter 10 we introduce the idea of conditional independence, which is a way of
characterizing redundancies in the model (i.e., the lack of direct dependence between
variables). We show how conditional independence relations can be visualized with
graphical models. We distinguish between directed and undirected graphical models.
In chapter 11, we discuss models in which the local units are combined together to form
chains or trees. In chapter 12, we extend this to the case where they have more general
connections.

Chapter 10
Graphical models
The previous chapters discussed models that relate the observed measurements to some
aspect of the world that we wish to estimate. In each case, this relationship depended on
a set of parameters and for each model we presented a learning algorithm that estimated
these parameters.
Unfortunately, the utility of these models is limited because every element of the
model depends on every other. For example, in generative models we model the joint
probability of the observations and the world state. In many problems both of these
quantities may be high-dimensional. Consequently, the number of parameters required
to characterize their joint density accurately is very large. Discriminative models suffer
from the same pathology: if every element of the world state depends on every element of
the data, a large number of parameters will be required to characterize this relationship.
In practice, the required amount of training data and the computational burden of learning
and inference reach impractical levels.
The solution to this problem is to reduce the dependencies between variables in the
model by identifying (or asserting) some degree of redundancy. To this end, we introduce
the idea of conditional independence, which is a way of characterizing these redundan-
cies. We then introduce graphical models which are graph-based representations of the
conditional independence relations. We discuss two different types of graphical models –
directed and undirected – and we consider the implications for learning, inference, and
drawing samples.
This chapter does not develop speciﬁc models or discuss vision applications. The
goal is to provide the theoretical background for the models in subsequent chapters. We
will illustrate the ideas with probability distributions where the constituent variables are
discrete; however, almost all of the ideas transfer directly to the continuous case.
10.1
Conditional independence
When we ﬁrst discussed probability distributions, we introduced the notion of indepen-
dence (Section 2.6). Two variables x1 and x2 are independent if their joint probability
distribution factorizes as Pr(x1,x2) = Pr(x1)Pr(x2). In layman’s terms, one variable
provides no information about the other if they are independent.
With more than two random variables, independence relations become more com-
plex. The variable x1 is said to be conditionally independent of variable x3 given variable

174
10 Graphical models
Figure 10.1 Conditional independence. a) Joint pdf of three discrete variables x1,x2,x3, which
take 4, 3, and 2 possible values, respectively. All 24 probability values sum to one. b) Marginaliz-
ing, we see that variables x1 and x2 are dependent; the conditional distribution of x1 is different
for different values of x2 (the elements in each row are not in the same proportions)and vice versa.
c) Variables x1 and x3 are also dependent. d) Variables x2 and x3 are also dependent. e–g) How-
ever, x1 and x3 are conditionally independent given x2. For ﬁxed x2, x1 tells us nothing more
about x3 and vice versa.
x2 when x1 and x3 are independent for ﬁxed x2 (Figure 10.1). In mathematical terms,
we have
Pr(x1|x2,x3) = Pr(x1|x2)
Pr(x3|x1,x2) = Pr(x3|x2).
(10.1)
Note that conditional independence relations are always symmetric; if x1 is conditionally
independent of x3 given x2, then it is also true that x3 is independent of x1 given x2.
Confusingly, the conditional independence of x1 and x3 given x2 does not mean that
x1 and x3 are themselves independent. It merely implies that if we know variable x2,
then x1 provides no further information about x3 and vice versa. One way that this can
occur is in a chain of events: if event x1 causes event x2 and x2 causes x3, then the
dependence of x3 on x1 might be entirely mediated by x2.
Now consider decomposing the joint probability distribution Pr(x1,x2,x3) into the
product of conditional probabilities. When x1 is independent of x3 given x2, we ﬁnd that
Pr(x1,x2,x3) = Pr(x3|x2,x1)Pr(x2|x1)Pr(x1)
= Pr(x3|x2)Pr(x2|x1)Pr(x1).
(10.2)
The conditional independence relation means that the probability distribution factorizes
in a certain way (and is hence redundant). This redundancy implies that we can describe
the distribution with fewer parameters and so working with models with large numbers
of variables becomes more tractable.
Throughout this chapter, we will explore the relationship between factorization of
the distribution and conditional independence relations. To this end, we will introduce
graphical models. These are graph-based representations that make both the factorization

10.2
Directed graphical models
175
Figure 10.2 Example 1. A directed graph-
ical model has one node per term in the
factorization of the joint probability distri-
bution. A node xn with no incoming con-
nections represents the term Pr(xn).
A
node xn with incoming connections xpa[n]
represents the term Pr(xn|xpa[n]).
Vari-
able xn is conditionally independent of all
of the others given its Markov blanket. This
comprises its parents, its children, and other
parents of its children.
For example, the
Markov blanket for variable x8 is indicated
by the shaded region.
and the conditional independence relations easy to establish. In this book we will con-
sider two different types of graphical model – directed and undirected graphical models –
each of which corresponds to a different type of factorization.
10.2
Directed graphical models
A directed graphical model or Bayesian network represents the factorization of the joint
probability distribution into a product of conditional distributions that take the form of a
directed acyclic graph (DAG) so that
Pr(x1...N) =
N
Y
n=1
Pr(xn|xpa[n]),
(10.3)
where {xn}N
n=1 represent the constituent variables of the joint distribution and the
function pa[n] returns the indices of variables that are parents of variable xn.
We can visualize the factorization as a directed graphical model (Figure 10.2)
by adding one node per random variable and drawing an arrow to each variable xn
from each of its parents xpa[n].
This directed graphical model should never con-
tain cycles.
If it does, then the original factorization was not a valid probability
distribution.
To retrieve the factorization from the graphical model, we introduce one factorization
term per variable in the graph. If variable xn is independent of all others (has no parents),
then we write Pr(xn). Otherwise, we write Pr(xn|xpa[n]) where the parents xpa[n]
consist of the set of variables with arrows that point to xn.
10.2.1 Example 1
The graphical model in Figure 10.2 represents the factorization
Pr(x1 ...x15) = Pr(x1)Pr(x2)Pr(x3)Pr(x4|x1,x2)Pr(x5|x2)Pr(x6)
×Pr(x7)Pr(x8|x4,x5)Pr(x9|x5,x6)Pr(x10|x7)Pr(x11|x7,x8)
×Pr(x12|x8)Pr(x13|x9)Pr(x14|x11)Pr(x15|x12).
(10.4)

176
10 Graphical models
The graphical model (or factorization) implies a set of independence and conditional
independence relations between the variables. Some statements about these relations
can be made based on a superﬁcial look at the graph. First, if there is no directed path
between two variables following the arrow directions and they have no common ances-
tors, then they are independent. So, variable x3 in Figure 10.2 is independent of all of
the other variables, and variables x1 and x2 are independent of each other. Variables
x4 and x5 are not independent as they share an ancestor. Second, any variable is condi-
tionally independent of all the other variables given its parents, children, and the other
parents of its children (its Markov blanket). So, for example, variable x8 in Figure 10.2
is conditionally independent of the remaining variables given those in the shaded area.
For vision applications, these rules are usually sufﬁcient to gain an understanding of
the main properties of a graphical model. However, occasionally we may wish to test
whether one arbitrary set of nodes is independent of another given a third. This is not
easily established by looking at the graph, but can be tested using the following criterion:
The variables in set A are conditionally independent of those in set B given set C if all
routes from A to B are blocked. A route is blocked at a node if (i) this node is in C and the
arrows meet head to tail or tail to tail or (ii) neither this node nor any of its descendants
are in C and the arrows meet head to head.
See Koller and Friedman (2009) for more details of why this is the case.
10.2.2 Example 2
Figure 10.3 tells us that
Pr(x1,x2,x3) = Pr(x1)Pr(x2|x1)Pr(x3|x2).
(10.5)
In other words, this is the graphical model corresponding to the distribution in
Figure 10.1.
If we condition on x2, the only route from x1 to x3 is blocked at x2 (the arrows meet
head to tail here) and so x1 must be conditionally independent of x3 given x2. We could
have reached the same conclusion by noticing that the Markov blanket for variable x1 is
just variable x2.
In this case, it is easy to prove this conditional independence relation algebraically.
Writing out the conditional probability of x1 given x2 and x3
Pr(x1|x2,x3) = Pr(x1,x2,x3)
Pr(x2,x3)
=
Pr(x1)Pr(x2|x1)Pr(x3|x2)
R
Pr(x1)Pr(x2|x1)Pr(x3|x2)dx1
=
Pr(x1)Pr(x2|x1)
R
Pr(x1)Pr(x2|x1)dx1
,
(10.6)
Figure 10.3 Example 2.
Directed graphical
model relating variables x1,x2,x3 from Figure
10.1. This model implies that the joint proba-
bility can be broken down as Pr(x1,x2,x3) =
Pr(x1)Pr(x2|x1)Pr(x3|x2).

10.2
Directed graphical models
177
Figure 10.4 Example 3. Graphical models for a) mixture of Gaussians b) t-distribution and c)
factor analysis. A node (black circle) represents a random variable. In a graphical model a bullet •
represents a variable whose value is considered to be ﬁxed. Each variable may be repeated many
times, and this is indicated by a plate (blue rectangle) where the number of copies is indicated in
the lower right corner. For example, in a) there are I data examples {xi}I
i=1 and I hidden variables
{hi}I
i=1. Similarly, there are K sets of parameters {µk,Σk}K
k=1, but just one weight vector λ.
we see that the ﬁnal expression does not depend on x3 and so we deduce that x1 is
conditionally independent of x3 given x2 as required.
Notice that the factorized distribution is more efﬁcient to represent than the full ver-
sion. The original distribution Pr(x1,x2,x3) (ﬁgure 10.1a) contains 4 × 3 × 2 = 24
entries. However, the terms Pr(x1), Pr(x2|x1), and Pr(x3|x2) contain 4, 4 × 3 = 12,
and 3 × 2 = 6 entries, respectively, giving a total of 22 entries. In this case, this is not a
dramatic reduction, but in more practical situations it would be. For example, if each vari-
able took ten possible values, the full joint distribution would have 10 × 10 × 10 = 1000
values, but the factorized distribution would have only 10+100+100 = 210 values. For
even larger systems, this can make a huge saving. One way to think about conditional
independence relations is to consider them as redundancies in the full joint probability
distribution.
10.2.3 Example 3
Finally, in Figure 10.4 we present graphical models for the mixture of Gaussians,
t-distribution and factor analysis models from Chapter 7. These depictions immediately
demonstrate that these models have very similar structures.
They also add several new features to the graphical representation. First, they include
multidimensional variables. Second, they include variables that are considered as ﬁxed
and these are marked by a bullet •. We condition on the ﬁxed variables, but do not deﬁne
a probability distribution over them. Figure 10.4c depicts the factorization Pr(hi,xi) =
Pr(hi)Pr(xi|hi,µ,Φ,Σ).
Finally, we have also used plate notation.
A plate is depicted as a rectangle with
a number in the corner. It indicates that the quantities inside the rectangle should be
repeated the given number of times. For example, in Figure 10.4c there are I copies
{xi,hi}I
i=1 of the variables x and h but only one set of parameters µ,Φ, and Σ.
10.2.4 Summary
To summarize, we can think about the structure of the joint probability distribution in
three ways. First, we can consider the way that the probability distribution factorizes.

178
10 Graphical models
Second, we can examine the directed graphical model. Third, we can think about the
conditional independence relations.
There is a one-to-one mapping between directed graphical models (acyclic directed
graphs of conditional probability relations) and factorizations. However, the relationship
between the graphical model (or factorization) and the conditional independence rela-
tions is more complicated. A directed graphical model (or its equivalent factorization)
determines a set of conditional independence relations. However, as we shall see later
in this chapter, there are some sets of conditional independence relations that cannot be
represented by directed graphical models.
10.3
Undirected graphical models
In this section we introduce a second family of graphical models. Undirected graphical
models represent probability distributions over variables {xn}N
n=1 that take the form of
a product of potential functions φ[x1...N] so that
Pr(x1...N) = 1
Z
C
Y
c=1
φc[x1...N],
(10.7)
where the potential function φc[x1...N] always returns a positive number.
Since the
probability increases when φc[x1...N] increases, each of these functions modulates the
tendency for the variables x1...N to take certain values. The probability is greatest where
all of the functions φ1...C return high values. However, it should be emphasized that
potential functions are not the same as conditional probabilities, and there is not usually
a clear way to map from one to the other.
The term Z is known as the partition function and normalizes the product of these
positive functions so that the total probability is one. In the discrete case, it would be
computed as
Z =
X
x1
X
x2
...
X
xN
C
Y
c=1
φc[x1...N].
(10.8)
For realistically sized systems, this sum will be intractable; we will not be able to com-
pute Z and hence will only be able to compute the overall probability up to an unknown
scale factor.
We can equivalently write Equation 10.7 as
Pr(x1...N) = 1
Z exp
"
−
C
X
c=1
ψc[x1...N]
#
,
(10.9)
where ψc[x1...N] = −log[φc[x1...N]].
When written in this form, the probability is
referred to as a Gibbs distribution. The terms ψc[x1...N] are functions that may return any
real number and can be thought of as representing a cost for every combination of labels
x1...N. As the cost increases, the probability decreases. The total cost PC
c=1 ψc[x1...N]
is sometimes known as the energy, and the process of ﬁtting the model (increasing the
probability) is hence sometimes termed energy minimization.

10.3
Undirected graphical models
179
When each potential function φ[•] (or alternatively each cost function ψ[•]) addresses
all of the variables x1...N, the undirected graphical model is known as a product of
experts. However, in computer vision it is more common for each potential function to
operate on a subset of the variables S ⊂{xn}N
n=1. These subsets are called cliques and
it is the choice of these cliques that determines the conditional independence relations.
Denoting the cth clique by Sc we can rewrite Equation 10.7 as
Pr(x1...N) = 1
Z
C
Y
c=1
φc[Sc].
(10.10)
In other words, the probability distribution is factorized into a product of terms, each of
which only depends on a subset of variables. In this situation, the model is sometimes
referred to as a Markov random ﬁeld.
To visualize the undirected graphical model, we draw one node per random variable.
Then, for every clique Sc we draw a connection from every member variable xi ∈Sc to
every other member variable.
Moving in the opposite direction, we can take a graphical model and establish the
underlying factorization using the following method. We add one term to the factoriza-
tion per maximal clique (see Figure 10.6). A maximal clique is a fully connected subset
of nodes (i.e., a subset where every node is connected to every other) where it is not
possible to add another node and remain fully connected.
It is much easier to establish the conditional independence relations from an undi-
rected graphical model than for directed graphical models. They can be found using the
following property:
One set of nodes is conditionally independent of another given a third if the third set
separates them (prevents a path from the ﬁrst node to the second).
It follows that a node is conditionally independent of all other nodes given its set of
immediate neighbors, and so these neighbors form the Markov blanket.
10.3.1 Example 1
Consider the graphical model in Figure 10.5. This represents the factorization
Pr(x1,x2,x3) = 1
Z φ1[x1,x2]φ2[x2,x3].
(10.11)
We can immediately see that variable x1 is conditionally independent of variable x3 given
x2 because x2 separates the other two variables: it blocks the path from x1 to x3. In this
Figure 10.5 Example 1. Undirected graphi-
cal model relating variables x1, x2, and x3.
This model implies that the joint probabil-
ity can be factorized as Pr(x1,x2,x3) =
1
Z φ1[x1,x2]φ2[x2,x3].

180
10 Graphical models
Figure 10.6 Example 2. Undirected graph-
ical model representing variables {xi}5
i=1.
The associated probability distribution fac-
torizes into a product of one potential func-
tion per maximal clique. The clique S45 =
{x4,x5} is a maximal clique as there is no
other node that we can add that connects to
every node in the clique. The clique S23 =
{x2,x3} is not a maximal clique as it is pos-
sible to add node x1, and all three nodes in
the new clique are connected to each other.
case, the conditional independence relation is easy to prove:
Pr(x1|x2,x3) = Pr(x1,x2,x3)
Pr(x2,x3)
=
1
Z φ1[x1,x2]φ2[x2,x3]
R 1
Z φ1[x1,x2]φ2[x2,x3]dx1
=
φ1[x1,x2]
R
φ1[x1,x2]dx1
.
(10.12)
The ﬁnal expression does not depend on x3 and so we conclude that x1 is conditionally
independent of x3 given x2.
10.3.2 Example 2
Consider the graphical model in Figure 10.6. There are four maximal cliques in this
graph, and so it represents the factorization
Pr(x1...5) = 1
Z φ1[x1,x2,x3]φ2[x2,x4]φ3[x3,x5]φ4[x4,x5].
(10.13)
We can deduce various conditional independence relations from the graphical repre-
sentation. For example, variable x1 is conditionally independent of variables x4 and x5
given x2 and x3, and variable x5 is independent of variables x1 and x2 given x3 and x4,
and so on.
Note also that the factorization
Pr(x1...5) = 1
Z (φ1[x1,x2]φ2[x2,x3]φ3[x1,x3])φ4[x2,x4]φ5[x3,x5]φ6[x4,x5].
(10.14)
creates the same graphical model: there is a many-to-one mapping from factorizations
to undirected graphical models (as opposed to the one-to-one mapping for directed
graphical models). When we compute a factorization from the graphical model based
on the maximal cliques we do so in a conservative way.
It is possible that there
are further redundancies which were not made explicit by the undirected graphical
model.

10.5
Graphical models in computer vision
181
Figure 10.7 Directed versus undirected graphical models. a) Directed graphical model with three
nodes. There is only one conditional independence relation implied by this model: the node x3
is the Markov blanket of node x2 (shaded area) and so x2 ⊥⊥x1|x3, where the notation ⊥⊥can
be read as “is independent of”. b) This undirected graphical model implies the same conditional
independence relation. c) Second directed graphical model. The relation x2 ⊥⊥x1|x3 is no longer
true, but x1 and x2 are independent if we don’t condition on x3 so we can write x2 ⊥⊥x1. There
is no undirected graphical model with three variables that has this pattern of independence and
conditional independence.
10.4
Comparing directed and undirected graphical models
In Sections 10.2 and 10.3 we have discussed directed and undirected graphical models,
respectively. Each graphical model represents a factorization of the probability distribu-
tion. We have presented methods to extract the conditional independence relations from
each type of graphical model. The purpose of this section is to argue that these represen-
tations are not equivalent. There are patterns of conditional independence that can be rep-
resented by directed graphical models but not undirected graphical models and vice versa.
Figures 10.7a–b show an undirected and directed graphical model that do represent
the same conditional independence relations. However, Figure 10.7c shows a directed
graphical model for which there is no equivalent undirected graphical model. There is
simply no way to induce the same pattern of independence and conditional independence
with an undirected graphical model.
Conversely, Figure 10.8a shows an undirected graphical model that induces a pattern
of conditional independence relations that cannot be replicated by any directed graph-
ical model. Figure 10.8b shows a directed graphical model that is close, but still not
equivalent; the Markov blanket of x2 is different in each model and so are its conditional
independence relations.
We conclude from this brief argument that directed and undirected graphical models
do not represent the same subset of independence and conditional independence relations,
and so we cannot eliminate one or the other from our consideration. In fact, there are
other patterns of conditional independence that cannot be represented by either type of
model. However, these will not be considered in this book. For further information
concerning the families of distributions that can be represented by different types of
graphical model, consult Barber (2012) or Koller and Friedman (2009).
10.5
Graphical models in computer vision
We will now introduce a number of common vision models and look at their associ-
ated graphical models. We will discuss each of these in detail in subsequent chapters.
However, it is instructive to see them together.

182
10 Graphical models
Figure 10.8 Directed versus undirected models. a) This undirected graphical model induces
two conditional independence relations. However, there is no equivalent directed graphical model
that produces the same pattern. b) This directed graphical model also induces two conditional
independence relations, but they are not the same. In both cases, the shaded region represents the
Markov blanket of variable x2.
Figure 10.9a shows the graphical model for a hidden Markov model or HMM. We
observe a sequence of measurements {xn}N
n=1 each of which tells us something about
the corresponding discrete world state {wn}N
n=1. Adjacent world states are connected
together so that the previous world state inﬂuences the current one and potentially
resolves situations where the measurements are ambiguous. A prototypical application
would be tracking sequences of sign language gestures (Figure 10.9b). There is informa-
tion at each frame about which gesture is present, but it may be ambiguous. However,
we can impose prior knowledge that certain signs are more likely to follow others using
the HMM and get an improved result.
Figure 10.9c represents a Markov tree. Again we observe a number of measurements,
each of which provides information about the associated discrete world state. However,
the world states are now connected in a tree structure. A prototypical application would
be human body ﬁtting (Figure 10.9d) where each unknown world state represents a body
part. The parts of the body naturally have a tree structure and so it makes sense to build
a model that exploits this.
Figure 10.9e illustrates the use of a Markov random ﬁeld or MRF as a prior. The
MRF here describes the world state as a grid of undirected connections. Each node might
correspond to a pixel. There is also a measurement variable associated with each world
state variable. These pairs are connected with directed links, so overall this is a mixed
model (partly directed and partly undirected). A prototypical application of an MRF
in vision would be for semantic labeling (Figure 10.9f). The measurements constitute
the RGB values at each position. The world state at each pixel is a discrete variable
that determines the class of object present (i.e., cow versus grass). The Markov random
ﬁeld prior ties together all of the individual classiﬁers to help yield a solution that makes
global sense.
Finally, Figure 10.9g shows the Kalman ﬁlter. This has the same graphical model
as the hidden Markov model but in this case the world state is continuous rather than
discrete. A prototypical application of the Kalman ﬁlter is for tracking objects through

10.5
Graphical models in computer vision
183
Figure 10.9 Commonly used graphical models in computer vision. a) Hidden Markov model. b)
One possible application of the HMM is interpreting sign language sequences. The choice of sign
at time n depends on the sign at time n −1. c) Markov tree. d) An example application is ﬁtting
a tree-structured body model e) Markov random ﬁeld prior with independent observations. f) The
MRF is often used as a prior distribution in semantic labeling tasks. Here the goal is to infer a
binary label at each pixel determining whether it belongs to the cow or the grass. g) Kalman ﬁlter.
An example application is tracking an object through a sequence. It has the same graphical model
as the HMM, but the unknown quantities are continuous as opposed to discrete.
a time sequence (Figure 10.9h). At each time, we might want to know the 2D position,
size, and orientation of the hand. However, in a given frame the measurements might be
poor: the frame may be blurred or the object may be temporarily occluded. By building a
model that connects the estimates from adjacent frames, we can increase the robustness to
these factors; earlier frames can resolve the uncertainty in the current ambiguous frame.

184
10 Graphical models
Notice that all of these graphical models have directed links from the world w to
the data x that indicate a relationship of the form Pr(x|w). Hence, they all construct a
probability distribution over the data and are generative models. We will also consider
discriminative models, but, historically speaking, generative models of this kind have
been more important. Each model is quite sparsely connected: each data variable x
connects only to one world state variable w, and each world state variable connects
to only a few others. The result of this is that there are many conditional independence
relations in the model. We will exploit these redundancies to develop efﬁcient algorithms
for learning and inference.
We will return to all of these models later in the book. We investigate the hidden
Markov model and the Markov tree in Chapter 11. We discuss the Markov random ﬁeld
in Chapter 12, and we will present the Kalman ﬁlter in Chapter 19. The remaining part
of this chapter answers two questions: (i) How can we perform inference when there are
a large number of unknown world variables? (ii) What are the implications of using a
directed graphical model versus an undirected one?
10.6
Inference in models with many unknowns
We will now consider inference in these models. Ideally, we would compute the full
posterior distribution Pr(w1...N|x1...N) using Bayes’ rule. However, the unknown world
states in the preceding models are generally much larger than previously considered in
this book and this makes inference challenging.
For example, consider the space of world states in the HMM example. If we are given
1000 frames of video and there are 500 common signs in the sign language, then there
are 5001000 possible states. It is clearly not practical to compute and store the posterior
probability associated with each. Even when the world states are continuous, computing
and storing the parameters of a high-dimensional probability model is still problematic.
Fortunately, there are alternative approaches to inference, which we now consider in turn.
10.6.1 Finding the MAP solution
One obvious possibility is to ﬁnd the maximum a posteriori solution:
ˆw1...N = argmax
w1...N
[Pr(w1...N|x1...N)]
= argmax
w1...N
[Pr(x1...N|w1...N)Pr(w1...N)].
This is still far from trivial. The number of world states is extremely large so we cannot
possibly explore every one and take the maximum. We must employ intelligent and
efﬁcient algorithms that exploit the redundancies in the distribution to ﬁnd the correct
solution where possible. However, as we shall see, for some models there is no known
polynomial algorithm to ﬁnd the MAP solution.
10.6.2 Finding the marginal posterior distribution
An alternative strategy is to ﬁnd the marginal posterior distributions:
Pr(wn|x1...N) =
Z Z
Pr(w1...N|x1...N)dw1...n−1 dwn+1...N.
(10.15)

10.6
Inference in models with many unknowns
185
Since each of these distributions is over a single label, it is not implausible to compute
and store each one separately. Obviously it is not possible to do this by directly com-
puting the (extremely large) joint distribution and marginalizing it directly. We must
use algorithms that exploit the conditional independence relations in the distribution to
efﬁciently compute the marginals.
10.6.3 Maximum marginals
If we want a single estimate of the world state, we could return the maximum values of
the marginal distributions, giving the criterion
ˆwn = argmax
wn
[Pr(wn|x1...N)].
(10.16)
This produces estimates of each world state that are individually most probable, but
which may not reﬂect the joint statistics. For example, world state wn = 4 might be
the most probable value for the nth world state and wm = 6 might be the most proba-
ble value for the mth world state, but it could be that the posterior probability for this
conﬁguration is zero: although the states are individually probable, they never co-occur
(Figure 10.10).
10.6.4 Sampling the posterior
For some models, it is intractable to compute either the MAP solution or the marginal
distributions. One possibility in this circumstance is to draw samples from the posterior
distribution. Methods based on sampling the posterior would fall under the more general
category of approximate inference as they do not normally return the true answer.
Figure 10.10 MAP solution versus max
marginals solution.
The main ﬁgure
shows
the
joint
posterior
distribution
Pr(w1,w2|x1,x2).
The
MAP
solu-
tion is at the peak of this distribution at
w1 = 2,w2 = 4 (highlighted in green). The
ﬁgure also shows the two marginal distribu-
tions Pr(w1|x1,x2) and Pr(w2|x1,x2).
The maximum marginals solution is com-
puted by individually ﬁnding the maximum
of each marginal distributions, which gives
the solution w1 = 4,w2 = 2 (highlighted
in red).
For this distribution, this is very
unrepresentative; although these labels are
individually likely, they rarely co-occur and
the joint posterior for this combination has
low probability.

186
10 Graphical models
Having drawn a number of samples from the posterior, we can approximate the pos-
terior probability distribution as a mixture of delta functions where there is one delta
function at each of the sample positions. Alternatively, we could make estimates of
marginal statistics such as the mean and variance based on the sampled values or select
the sample with the highest posterior probability as an estimate of the MAP state; this
latter approach has the advantage of being consistent with the full posterior distribution
(as opposed to maximum marginals which is not) even if we cannot be sure that we have
the correct answer.
An alternative way to compute a point estimate from a set of samples from the pos-
terior is to compute the empirical max-marginals. We estimate the marginal probability
distributions by looking at the marginal statistics of the samples. In other words, we con-
sider one variable wn at a time and look at the distribution of different values observed.
For a discrete distribution, this information is captured in a histogram. For a continuous
distribution, we could ﬁt a univariate model such as a normal distribution to these values
to summarize them.
10.7
Drawing samples
We have seen that some of the approaches to inference require us to draw samples from
the posterior distribution. We will now discuss how to do this for both directed and
undirected models and we will see that this is generally more straightforward in directed
models.
10.7.1 Sampling from directed graphical models
Directed graphical models take the form of directed acyclic graphs of conditional
probability relations that have the following algebraic form:
Pr(x1...N) =
IY
n=1
Pr(xn|xpa[n]).
(10.17)
It is relatively easy to sample from a directed graphical model using a technique
known as ancestral sampling. The idea is to sample each variable in the network in turn,
where the order is such that all parents of a node are sampled before the node itself. At
each node, we condition on the observed sample values of the parents.
The simplest way to understand this is with an example.
Consider the directed
graphical model in Figure 10.11 whose probability distribution factorizes as
Pr(x1,x2,x3,x4,x5) = Pr(x1)Pr(x2|x1)Pr(x3|x4,x2)Pr(x4|x2,x1)Pr(x5|x3).
(10.18)
To sample from this model we ﬁrst identify x1 as a node with no parents and draw
a sample from the distribution Pr(x1).
Let us say the observed sample at x1 took
value α1.
We now turn to the remaining nodes. Node x2 is the only node in the network where
all of the parents have been processed, and so we turn our attention here next. We draw

10.7
Drawing samples
187
Figure 10.11 Ancestral sampling.
We
work our way through the graph in an order
(red number) that guarantees that the par-
ents of every node are visited before the
node itself. At each step we draw a sample
conditioned on the values of the samples at
the parents. This is guaranteed to produce a
valid sample from the full joint distribution.
a sample from the distribution Pr(x2|x1 = α1) to yield a sample α2. We now see that
we are not yet ready to sample from x3 as not all of its parents have been sampled, but
we can sample x4 from the distribution Pr(x4|x1 = α1,x2 = α2) to yield the value α4.
Continuing this process we draw x3 from Pr(x3|x2 = α2,x4 = α4) and ﬁnally x5 from
Pr(x5|x3 = α3).
The resulting vector w∗= [α1,α2,α3,α4,α5] is guaranteed to be a valid sample
from the full joint distribution Pr(x1,x2,x3,x4,x5). An equivalent way to think about
this algorithm is to consider it as working through the terms in the factorized joint dis-
tribution (right-hand side of Equation 10.18) sampling from each in turn conditioned on
the previous values.
10.7.2 Sampling from undirected graphical models
Unfortunately, it is much harder to draw samples from undirected models except in cer-
10.1 tain special cases (e.g., where the variables are continuous and Gaussian or where the
graph structure takes the form of a tree). In general graphs, we cannot use ancestral sam-
pling because (i) there is no sense in which any variable is a parent to any other so we
don’t know which order to sample in and (ii) the terms φ[•] in the factorization are not
probability distributions anyway.
One way to generate samples from any complex high-dimensional probability dis-
tribution is to use a Markov chain Monte Carlo (MCMC) method.
The principle is to
generate a series (chain) of samples from the distribution, so that each sample depends
directly on the previous one (hence “Markov”). However, the generation of the sample
is not completely deterministic (hence “Monte Carlo”).
One of the simplest MCMC methods is Gibbs sampling, which proceeds as follows.
First, we randomly choose the initial state x[0] using any method. We generate the next
sample in the chain x[1] by updating the state at each dimension {xn}N
n=1 in turn (in any
order). To update the nth dimension xn we ﬁx the other N −1 dimensions and draw
from the conditional distribution Pr(xn|x1...N\n) where the set x1...N\n denotes all of
the N variables x1,x2 ...xN except xn. Having modiﬁed every dimension in this way,
we obtain the second sample in the chain. This idea is illustrated in Figure 10.12 for the
multivariate normal distribution.
When this procedure is repeated a very large number of times, so that the initial
conditions are forgotten, a sample from this sequence can be considered as a draw from
the distribution Pr(x1...N). Although this is not immediately obvious (and a proof is

188
10 Graphical models
Figure 10.12 Gibbs sampling. We generate a chain of samples by cycling through each dimen-
sion in turn and drawing a sample from the conditional distribution of that dimension given that
the others are ﬁxed. a) For this 2D multivariate normal distribution, we start at a random posi-
tion x[0]. We alternately draw samples from the conditional distribution of the ﬁrst dimension
keeping the second ﬁxed (horizontal changes) and the second dimension keeping the ﬁrst ﬁxed
(vertical changes). For the multivariate normal, these conditional distributions are themselves nor-
mal (Section 5.5). Each time we cycle through both of the dimensions, we create a new sample x[t].
b) Many samples generated using this method.
beyond the scope of this book), this procedure does clearly have some sensible properties:
since we are sampling from the conditional probability distribution at each pixel, we are
more likely to change the current value to one which has an overall higher probability.
However, the stochastic update rule provides the possibility of (infrequently) visiting less
probable regions of the space.
For undirected graphical models, the conditional distribution Pr(xn|x1...N\n) can be
quite efﬁcient to evaluate because of the conditional independence properties: variable
xn is conditionally independent of the rest of the nodes given its immediate neighbors,
and so computing this term only involves the immediate neighbors. However, over-
all this method is extremely inefﬁcient: it requires a large amount of computational
effort to generate even a single sample. Sampling from directed graphical models is far
easier.
10.8
Learning
In Section 10.7 we argued that sampling from directed graphical models is consid-
erably easier than sampling from undirected graphical models.
In this section, we
consider learning in each type of model and come to a similar conclusion.
Note
that we are not discussing the learning of the graph structure here; we are talking
about learning the parameters of the model itself.
For directed graphical mod-
els, these parameters would determine the conditional distributions Pr(xn|xpa[n])
and for undirected graphical models they would determine the potential functions
φc[x1...N].

10.8
Learning
189
10.8.1 Learning in directed graphical models
Any directed graphical model can be written in the factorized form
Pr(x1 ...xN) =
N
Y
n=1
Pr(xn|xpa[n],θ),
(10.19)
where the conditional probability relations form a directed acyclic graph, and θ denotes
the parameters of the model. For example, in the discrete distributions that we have
focused on in this chapter, an individual conditional model might be
Pr(x2|x1 = k) = Catx2[λk]
(10.20)
where the parameters here are {λk}K
k=1. In general, the parameters can be learned using
the maximum likelihood method by ﬁnding
ˆθ = argmax
θ
" IY
i=1
N
Y
n=1
Pr(xi,n|xi,pa[n],θ)
#
(10.21)
= argmax
θ
" I
X
i=1
N
X
n=1
log[Pr(xi,n|xi,pa[n],θ)]
#
,
(10.22)
where xi,n represents the nth dimension of the ith training example. This criterion
leads to simple learning algorithms, and often the maximum likelihood parameters can
be computed in closed form.
10.8.2 Learning in undirected graphical models
An undirected graphical model is written as
Pr(x) = 1
Z
C
Y
c=1
φc[x,θ],
(10.23)
where x = [x1,x2,...,xN] and we have assumed that the training samples are inde-
pendent. However, in this form we must constrain the parameters so that they ensure
that each φc[•] always returns a positive number.
A more practical approach is to
reparameterize the undirected graphical model in the form of the Gibbs distribution,
Pr(x) = 1
Z exp
"
−
C
X
c=1
ψc[x1...N,θ]
#
(10.24)
so that we do not have to worry about constraints on the parameters.
Given I training examples {xi}I
i=1, we aim to ﬁt parameters θ. Assuming that the
training examples are independent, the maximum likelihood solution is
ˆθ = argmax
θ
"
1
Z(θ)I exp
"
−
I
X
i=1
C
X
c=1
ψc(xi,θ)
##
= argmax
θ
"
−I log[Z(θ)] −
I
X
i=1
C
X
c=1
ψc(xi,θ)
#
,
(10.25)

190
10 Graphical models
where as usual we have taken the log to simplify the expression.
To maximize this expression we calculate the derivative of the log likelihood L with
respect to the parameters θ:
∂L
∂θ = −I ∂log[Z(θ)]
∂θ
−
I
X
i=1
C
X
c=1
∂ψc(xi,θ)
∂θ
(10.26)
= −I
∂log
hP
xi exp
h
−PC
c=1 ψc(xi,θ)
ii
∂θ
−
I
X
i=1
C
X
c=1
∂ψc(xi,θ)
∂θ
.
The second term is readily computable but the ﬁrst term involves an intractable sum
over all possible values of the variable x: we cannot compute the derivative with respect
to the parameters for reasonable-sized models and so learning is difﬁcult. Moreover,
we cannot evaluate the original probability expression (Equation 10.23) as this too con-
tains an intractable sum. Consequently, we can’t compute the derivative using ﬁnite
differences either.
In short, we can neither ﬁnd an algebraic solution nor use a straightforward opti-
mization technique as we cannot compute the gradient. The best that we can do is to
approximate the gradient.
Contrastive divergence
One possible solution to this problem is the contrastive divergence algorithm. This is a
10.2 method for approximating the gradient of the log likelihood with respect to parameters θ
for functions with the general form,
Pr(x) =
1
Z(θ)f[x,θ],
(10.27)
where Z(θ) = P
x f[x,θ] is the normalizing constant and the derivative of the log
likelihood is
∂log[Pr(x)]
∂θ
= −∂log[Z(θ)]
∂θ
+ ∂log[f[x,θ]]
∂θ
.
(10.28)
The main idea behind contrastive divergence follows from some algebraic manipulation
of the ﬁrst term:
∂log[Z(θ)]
∂θ
=
1
Z(θ)
∂Z(θ)
∂θ
=
1
Z(θ)
∂P
x f[x,θ]
∂θ
=
1
Z(θ)
X
x
∂f[x,θ]
∂θ
=
1
Z(θ)
X
x
f[x,θ]∂log[f[x,θ]]
∂θ
=
X
x
Pr(x)∂log[f[x,θ]]
∂θ
.
(10.29)

10.8
Learning
191
Figure 10.13 The contrastive divergence
algorithm changes the parameters so that
the unnormalized distribution increases at
the observed data points (blue crosses) but
decreases at sampled data points from the
model (green stars). These two components
counterbalance one another and ensure that
the likelihood increases. When the model
ﬁts the data, these two forces will can-
cel out, and the parameters will remain
constant.
where we have used the relation ∂logf[x]/∂x = (∂f[x]/∂x)/f[x] between the third and
fourth lines.
The ﬁnal term in Equation 10.29 is the expectation of the derivative of log[f[x,θ]].
We cannot compute this exactly, but we can approximate it by drawing J independent
samples x∗from the distribution to yield
∂log[Z(θ)]
∂θ
=
X
x
Pr(x)∂log[f[x,θ]]
∂θ
≈1
J
J
X
j=1
∂log[f[x∗
j,θ]]
∂θ
.
(10.30)
With I training examples {xi}I
i=1, the gradient of the log likelihood L is hence
∂L
∂θ ≈−I
J
J
X
j=1
∂log[f(x∗
j,θ)]
∂θ
+
I
X
i=1
∂log[f(xi,θ)]
∂θ
.
(10.31)
A visual explanation of this expression is presented in Figure 10.13. The gradient
points in a direction that (i) increases the logarithm of the unnormalized function at the
data points xi but (ii) decreases the same quantity in places where the model believes
the density is high (i.e., the samples x∗
j). When the model ﬁts the data, these two forces
cancel out, and the parameters will stop changing.
This algorithm requires us to draw samples x∗from the model at each iteration of the
optimization procedure in order to compute the gradient. Unfortunately, the only way to
draw samples from general undirected graphical models is to use costly Markov chain
Monte Carlo methods such as Gibbs sampling (Section 10.7.2), and this is impractically
time consuming. In practice it has been found that even approximate samples will do:
one method is to restart J = I samples at the data points at each iteration and do just a few
MCMC steps. Surprisingly, this works well even with a single step. A second approach
is to start with the samples from the previous iteration and perform a few MCMC steps
so that the samples are free to wander without restarting. This technique is known as
persistent contrastive divergence.

192
10 Graphical models
Figure 10.14 a) Graphical model for Problem 10.2. b) Graphical model for Problem 10.4
Discussion
In this chapter, we introduced directed and undirected graphical models. Each represents
a different type of factorization of the joint distribution. A graphical model implies a set
of independence and conditional independence relations. There are some sets that can
only be represented by directed graphical models, others that can only be represented by
undirected graphical models, some that can be represented by both, and some that cannot
be represented by either.
We presented a number of common vision models and examined their graphical mod-
els. Each had sparse connections and hence many conditional independence relations. In
subsequent chapters, we will exploit these redundancies to develop efﬁcient learning and
inference algorithms. Since the world state in these models is usually very high dimen-
sional, we discussed alternative forms of inference including maximum marginals and
sampling.
Finally, we looked at the implications of choosing directed or undirected graphical
models for sampling and for learning. We concluded that it is generally more straightfor-
ward to draw samples from directed graphical models. Moreover, it is also easier to learn
directed graphical models. The best-known learning algorithm for general undirected
graphical models requires us to draw samples, which is itself challenging.
Notes
Graphical models: For a readable introduction to graphical models, consult Jordan (2004) or
Bishop (2006). For a more comprehensive overview, I would recommend Barber (2012). For an
even more encyclopaedic resource, consult Koller and Friedman (2009).
Contrastive divergence: The contrastive divergence algorithm was introduced by Hinton (2002).
Further information about this technique can be found in Carreira-Perpi˜n´an. and Hinton (2005) and
Bengio and Delalleau (2009).
Problems
10.1 The joint probability model between variables {xi}7
i=1 factorizes as
Pr(x1,x2,x3,x4,x5,x6,x7) =
Pr(x1)Pr(x3)Pr(x7)Pr(x2|x1,x3)Pr(x5|x7,x2)Pr(x4|x2)Pr(x6|x5,x4).

Problems
193
Draw a directed graphical model relating these variables. Which variables form the Markov blanket
of variable x2?
10.2 Write out the factorization corresponding to the directed graphical model in Figure 10.14a.
10.3 An undirected graphical model has the form
Pr(x1 ...x6) = 1
Z φ1[x1,x2,x5]φ2[x2,x3,x4]φ3[x1x5]φ4[x5,x6].
Draw the undirected graphical model that corresponds to this factorization.
10.4 Write out the factorization corresponding to the undirected graphical model in Figure 10.14b.
10.5 Consider the undirected graphical model deﬁned over binary values {xi}4
i=1 ∈{0,1}
deﬁned by
Pr(x1,x2,x3,x4) = 1
Z φ(x1,x2)φ(x2,x3)φ(x3,x4)φ(x4,x1),
where the function φ is deﬁned by
φ(0,0) = 1
φ(1,1) = 2
φ(0,1) = 0.1
φ(1,0) = 0.1
Compute the probability of each of the 16 possible states of this system.
10.6 What is the Markov blanket for each of the variables in Figures 10.7 and 10.8?
10.7 Show that the stated patterns of independence and conditional independence in Figures 10.7
and 10.8 are true.
10.8
A factor graph is a third type of graphical model that depicts the factorization of a joint
probability. As usual it contains a single node per variable, but it also contains one node per factor
(usually indicated by a solid square). Each factor variable is connected to all of the variables that
are contained in the associated term in the factorization by undirected links. For example, the
factor node corresponding to the term Pr(x1|x2,x3) in a directed model would connect to all
three variables x1, x2, and x3. Similarly, the factor node corresponding to the term φ12[x1,x2]
in an undirected model would connect variables x1 and x2. Figure 10.15 shows two examples of
factor graphs.
Figure 10.15 Factor graphs contain one node (square) per factor in the joint pdf as well as one
node (circle) per variable. Each factor node is connected to all of the variables that belong to that
factor. This type of graphical model can distinguish between the undirected graphical models a)
Pr(x1,x2,x3)= 1
Z φ123[x1,x2,x3] and b) Pr(x1,x2,x3)= 1
Z φ12[x1,x2]φ23[x2,x3]φ13[x1,x3].

194
10 Graphical models
Draw the factor graphs corresponding to the graphical models in Figures 10.7 and 10.8. You must
ﬁrst establish the factorized joint distribution associated with each graph.
10.9 What is the Markov blanket of variable w2 in Figure 10.9c?
10.10 What is the Markov blanket of variable w8 in Figure 10.9e?

Chapter 11
Models for chains and trees
In this chapter, we model the relationship between a multidimensional set of measure-
ments {xn}N
n=1 and an associated multidimensional world state {wn}N
n=1. When N is
large, it is not practical to describe the full set of dependencies between all of these vari-
ables, as the number of model parameters will be too great. Instead, we construct models
where we only directly describe the probabilistic dependence between variables in small
neighborhoods. In particular, we will consider models in which the world variables
{wn}N
n=1 are structured as chains or trees.
We deﬁne a chain model to be one in which the world state variables {wn}N
n=1 are
connected to only the previous variable and the subsequent variable in the associated
graphical model (as in Figure 11.2). We deﬁne a tree model to be one in which the
world variables have more complex connections, but so there are no loops in the resulting
graphical model. Importantly, we disregard the directionality of the connections when we
assess whether a directed model is a tree. Hence, our deﬁnition of a tree differs from the
standard computer science usage.
We will also make the following assumptions:
• The world states wn are discrete.
• There is an observed data variable xn associated with each world state wn.
• The nth data variable xn is conditionally independent of all other data variables
and world states given the associated world state wn.
These assumptions are not critical for the development of the ideas in this chapter but
are typical for the type of computer vision applications that we consider. We will show
that both maximum a posteriori and maximum marginals inference are tractable for this
subclass of models, and we will discuss why this is not the case when the states are not
organized as a chain or a tree.
To motivate these models, consider the problem of gesture tracking. Here, the goal
is to automatically interpret sign language from a video sequence (Figure 11.1). We
observe N frames {xn}N
n=1 of a video sequence and wish to infer the N discrete variables
{wn}N
n=1 that encode which sign is present in each of the N frames. The data at time n
tells us something about the sign at time n but may be insufﬁcient to specify it accurately.
Consequently, we also model dependencies between adjacent world states: we know
that the signs are more likely to appear in some orders than others and we exploit this
knowledge to help disambiguate the sequence. Since we model probabilistic connections
only between adjacent states in the time series, this has the form of a chain model.

196
11 Models for chains and trees
Figure 11.1 Interpreting sign language. We observe a sequence of images of a person using sign
language. In each frame we extract a vector xn describing the shape and position of the hands.
The goal is to infer the sign wn that is present. Unfortunately, the visual data in a single frame
may be ambiguous. We improve matters by describing probabilistic connections between adjacent
states wn and wn−1. We impose knowledge about the likely sequence of signs, and this helps
disambiguate any individual frame. Images from Purdue RVL-SLLL ASL database (Wilbur and
Kak 2006).
11.1
Models for chains
In this section, we will describe both a directed and an undirected model for describing
chain structure and show that these two models are equivalent.
11.1.1 Directed model for chains
The directed model describes the joint probability of a set of continuous measurements
{xn}N
n=1 and a set of discrete world states {wn}N
n=1 with the graphical model shown
in Figure 11.2a. The tendency to observe the measurements xn given that state wn
takes value k is encoded in the likelihood Pr(xn|wn = k). The prior probability of the
ﬁrst state w1 is explicitly encoded in the discrete distribution Pr(w1), but for simplicity
we assume that this is uniform and omit it from most of the ensuing discussion. The
remaining states are each dependent on the previous one, and this information is captured
in the distribution Pr(wn|wn−1). This is sometimes termed the Markov assumption.
Hence, the overall joint probability is
Pr(x1...N,w1...N) =
 N
Y
n=1
Pr(xn|wn)
! N
Y
n=2
Pr(wn|wn−1)
!
.
(11.1)
This is known as a hidden Markov model (HMM). The world states {wn}N
n=1 in the
directed model have the form of a chain, and the overall model has the form of a tree. As
we shall see, these properties are critical to our ability to perform inference.
11.1.2 Undirected model for chains
The undirected model (see Section 10.3) describes the joint probability of the measure-
ments {xn}N
n=1 and the world states {wn}N
n=1 with the graphical model shown in Figure
11.2b. The tendency for the measurements and the data to take certain values is encoded
in the potential function φ[xn,wn]. This function always returns positive values and
returns larger values when the measurements and the world state are more compatible.
The tendency for adjacent states to take certain values is encoded in a second poten-
tial function ζ[wn,wn−1] which returns larger values when the adjacent states are more

11.1
Models for chains
197
Figure 11.2 Models for chains. a) Directed model. There is one observation variable xn for
each state variable wn and these are related by the conditional probability Pr(xn|wn) (downward
arrows). Each state wn is related to the previous one by the conditional probability Pr(wn|wn−1)
(horizontal arrows). b) Undirected model. Here, each observed variable xn is related to its associ-
ated state variable wn via the potential function φ[xn,wn] and the neighboring states are connected
via the potential function ζ[wn,wn−1].
compatible. Hence, the overall probability is
Pr(x1...N,w1...N) = 1
Z
 N
Y
n=1
φ[xn,wn]
! N
Y
n=2
ζ[wn,wn−1]
!
.
(11.2)
Once more the states form a chain and the overall model has the form of a tree; there are
no loops.
11.1.3 Equivalence of models
When we take a directed model and make the edges undirected, we usually create a
different model. However, comparing Equations 11.1 and 11.2 reveals that these two
models represent the same factorization of the joint probability density; in this special
case, the two models are equivalent. This equivalence becomes even more apparent if we
make the substitutions
Pr(xn|wn) = 1
zn
φ[xn,wn]
Pr(wn|wn−1) = 1
z′n
ζ[wn,wn−1],
(11.3)
where zn and z′
n are normalizing factors which form the partition function:
Z =
 N
Y
n=1
zn
! N
Y
n=2
z′
n
!
.
(11.4)
Since the directed and undirected versions of the chain model are equivalent, we will
continue our discussion in terms of the directed model alone.
11.1.4 Hidden Markov model for sign language application
We will now brieﬂy describe how this directed model relates to the sign language appli-
cation. We preprocess the video frame to create a vector xn that represents the shape of
the hands. For example, we might just extract a window of pixels around each hand and
concatenate their RGB pixel values.
We now model the likelihood Pr(xn|wn =k) of observing this measurement vector
given that the sign wn in this image takes value k. A very simple model might assume

198
11 Models for chains and trees
that the measurements have a normal distribution with parameters that are contingent on
which sign is present so that
Pr(xn|wn=k) = Normxn[µk,Σk].
(11.5)
We model the sign wn as a being categorically distributed, where the parameters depend
on the previous sign wn−1 so that
Pr(wn|wn−1=k) = Catwn[λk].
(11.6)
This hidden Markov model has parameters {µk,Σk,λk}K
k=1. For most of this chapter,
we will assume that these parameters are known, but we return brieﬂy to the issue of
learning in Section 11.6. We now turn our focus to inference in this type of model.
11.2
MAP inference for chains
Consider a chain with N unknown variables {wn}N
n=1, each of which can take K pos-
sible values. Here, there are KN possible states of the world. For real-world problems,
this means that there are far too many states to evaluate exhaustively; we can neither
compute the full posterior distribution nor search directly through all of the states to ﬁnd
the maximum a posteriori estimate.
Fortunately, the factorization of the joint probability distribution (the conditional
independence structure) can be exploited to ﬁnd more efﬁcient algorithms for MAP
inference than brute force search. The MAP solution is given by
ˆw1...N = argmax
w1...N
[Pr(w1...N|x1...N)]
= argmax
w1...N
[Pr(x1...N,w1...N)]
= argmin
w1...N
[−log[Pr(x1...N,w1...N)]],
(11.7)
where line 2 follows from Bayes’ rule. We have reformulated this as a minimization
problem in line 3.
Substituting in the expression for the log probability (Equation 11.1), we get
ˆw1...N = argmin
w1...N
"
−
N
X
n=1
log[Pr(xn|wn)] −
N
X
n=2
log[Pr(wn|wn−1)]
#
,
(11.8)
which has the general form
ˆw1...N = argmin
w1...N
" N
X
n=1
Un(wn) +
N
X
n=2
Pn(wn,wn−1)
#
,
(11.9)
where Un is a unary term and depends only on a single variable wn and Pn is a pairwise
term, depending on two variables wn and wn−1. In this instance, the unary and pairwise
terms can be deﬁned as
Un(wn) = −log[Pr(xn|wn)]
Pn(wn,wn−1) = −log[Pr(wn|wn−1)].
(11.10)
Any problem that has the form of Equation 11.9 can be solved in polynomial time
using the Viterbi algorithm which is an example of dynamic programming.

11.2
MAP inference for chains
199
Figure 11.3 Dynamic programming formulation. Each solution is equated with a particular path
from left-to-right through an acyclic directed graph. The N columns of the graph represent vari-
ables w1...N and the K rows represent possible states 1...K. The nodes and edges of the graph
have costs associated with the unary and pairwise terms, respectively. Any path from left to right
through the graph has a cost that is the sum of the costs at all of the nodes and edges that it passes
through. Optimizing the function is now equivalent to ﬁnding the path with the least cost.
11.2.1 Dynamic programming (Viterbi algorithm)
To optimize the cost function in Equation 11.9, we ﬁrst visualize the problem with a 2D
11.1 graph with vertices {Vn,k}N,K
n=1,k=1. The vertex Vn,k represents choosing the kth world
state at the nth variable (Figure 11.3). Vertex Vn,k is connected by a directed edge to
each of the vertices {Vn+1,k}K
k=1 at the next pixel position. Hence, the organization of
the graph is such that each valid horizontal path from left to right represents a possi-
ble solution to the problem; it corresponds to assigning one value k ∈[1...K] to each
variable wn.
We now attach the costs Un(wn =k) to the vertices Vn,k. We also attach the costs
Pn(wn = k,wn−1 = l) to the edges joining vertices Vn−1,l to Vn,k. We deﬁne the total
cost of a path from left to right as the sum of the costs of the edges and vertices that make
up the path. Now, every horizontal path represents a solution and the cost of that path is
the cost for that solution; we have reformulated the problem as ﬁnding the minimum cost
path from left to right across the graph.
Finding the minimum cost
The approach to ﬁnding the minimum cost path is simple. We work through the graph
from left to right, computing at each vertex the minimum possible cumulative cost Sn,k
to arrive at this point by any route. When we reach the right-hand side, we compare the
K values SN,• and choose the minimum. This is the lowest possible cost for traversing
the graph. We now retrace the route we took to reach this point, using information that
was cached during the forward pass.
The easiest way to understand this method is with a concrete example (Figures 11.4
and 11.5) and the reader is encouraged to scrutinize these ﬁgures before continuing.
A more formal description is as follows. Our goal is to assign the minimum possible
cumulative cost Sn,k for reaching vertex Vn,k. Starting at the left-hand side, we set the
ﬁrst column of vertices to the unary costs for the ﬁrst variable:
S1,k = U1(w1 = k).
(11.11)

200
11 Models for chains and trees
Figure 11.4 Dynamic programming. a) The unary cost Un(wn = k) is given by the number
above and to the right of each node. The pairwise costs Pn(wn,wn−1) are zero if wn = wn−1
(horizontal), two if |wn −wn−1| = 1, and ∞otherwise. This favors a solution that is mostly
constant but can also vary smoothly. For clarity, we have removed the edges with inﬁnite cost as
they cannot become part of the solution. We now work from left to right, computing the minimum
cost Sn,k for arriving at vertex Vn,k by any route. b) For vertices {V1,k}K
k=1, the minimum cost
is just the unary cost associated with that vertex. We have stored the values S1,1 ...S1,5 inside
the circle representing the respective vertex. c) To compute the minimum cost S2,1 at vertex
(n = 2,k = 1), we must consider two possible routes. The path could have traveled horizontally
from vertex (1,1) giving a total cost of 2.0 + 0.0 + 1.1 = 3.1, or it may have come diagonally
upward from vertex (1,2) with cost 0.8 + 2.0 + 1.1 = 3.9. Since the former route is cheaper, we
use this cost, store S2,1 = 3.1 at the vertex, and also remember the path used to get here. Now,
we repeat this procedure at vertex (2,2) where there are three possible routes from vertices (1,1),
(1,2) and (1,3). Here it turns out that the best route is from (1,2) and has total cumulative cost of
S2,2 = 5.6. Example continued in Figure 11.5.

11.2
MAP inference for chains
201
a) 
b) 
c) 
Figure 11.5 Dynamic programming worked example (continued from Figure 11.4). a) Having
updated the vertices at pixel n=2, we carry out the same procedure at pixel n=3, accumulating
at each vertex the minimum total cost to reach this point. b) We continue updating the minimum
cumulative costs Sn,k to arrive at pixel n in state k until we reach the right hand side. c) We
identify the minimum cost among the right-most vertices. In this case, it is vertex (6,4), which
has cost S6,4 = 17.4. This is the minimum possible cost for traversing the graph. By tracing back
the route that we used to arrive here (red arrows), we ﬁnd the world state at each pixel that was
responsible for this cost.
The cumulative total S2,k for the kth vertex in the second column should represent the
minimum possible cumulative cost to reach this point. To calculate this, we consider the
K possible predecessors and compute the cost for reaching this vertex by each possible
route. We set S2,k to the minimum of these values and store the route by which we

202
11 Models for chains and trees
reached this vertex so that
S2,k = U2(w2 = k) + min
l
[S1,l + P2(w2 = k,w1 = l)].
(11.12)
More generally, to calculate the cumulative totals Sn,k, we use the recursion
Sn,k = Un(wn = k) + min
l
[Sn−1,l + Pn(wn = k,wn−1 = l)],
(11.13)
and we also cache the route by which this minimum was achieved at each stage. When
we reach the right-hand side, we ﬁnd the value of the ﬁnal variable wn that minimizes
the total cost
ˆwN = argmin
k
[SN,k],
(11.14)
and set the remaining labels { ˆwn}N−1
n=1 according to the route that we followed to get to
this value.
This method exploits the factorization structure of the joint probability between the
observations and the states to make vast computational savings. The cost of this proce-
dure is O(NK2), as opposed to O(KN) for a brute force search through every possible
solution.
11.3
MAP inference for trees
To show how MAP inference works in tree-structured models, consider the model in
11.2 Figure 11.6. For this graph, the prior probability over the states factorizes as
Pr(w1...6) = Pr(w1)Pr(w3)Pr(w2|w1)Pr(w4|w3)Pr(w5|w2,w4)Pr(w6|w5),
(11.15)
Figure 11.6 Tree-based models.
As be-
fore, there is one observation xn for each
world state wn, and these are related by the
conditional probability Pr(xn|wn). How-
ever, disregarding the directionality of the
edges, the world states are now connected
as a tree.
Vertex w5 has two incoming
connections, which means that there is a
‘three-wise’ term Pr(w5|w2,w4) in the fac-
torization. The tree structure means it is pos-
sible to perform MAP and max-marginals
inference efﬁciently.

11.3
MAP inference for trees
203
and the world states have the structure of a tree (disregarding the directionality of the
edges).
Once more, we can exploit this factorization to compute the MAP solution efﬁciently.
Our goal is to ﬁnd
ˆw1...6 = argmax
w1...6
" 6
X
n=1
log[Pr(xn|wn)] + log[Pr(w1...6)]
#
.
(11.16)
By a similar process to that in Section 11.2, we can rewrite this as a minimization problem
with the following cost function:
ˆw1...6 = argmin
w1...6
" 6
X
n=1
Un(wn) + P2(w2,w1) + P4(w4,w3)
+P6(w6,w5) + T5(w5,w2,w4)
#
.
(11.17)
As before, we reformulate this cost function in terms of ﬁnding a route through a
graph (see Figure 11.7).
The unary costs Un are associated with each vertex.
The
pairwise costs Pm are associated with edges between pairs of adjacent vertices. The
three-wise cost T5 is associated with the combination of states at the point where the tree
branches. Our goal now is to ﬁnd the least cost path from all of the leaves simultaneously
to the root.
We work from the leaves to the root of the tree, at each stage computing Sn,k, the
cumulative cost for arriving at this vertex (see worked example in Figure 11.7). For the
ﬁrst four vertices, we proceed as in standard dynamic programming:
S1,k = U1(w1 = k)
S2,k = U2(w2 = k) + min
l
[S1,l + P2(w2 = k,w1 = l)]
S3,k = U3(w3 = k)
S4,k = U4(w4 = k) + min
l
[S3,1 + P4(w4 = k,w3 = l)].
(11.18)
When we come to the branch in the tree, we try to ﬁnd the best combination of routes
to reach the nodes for variable 5; we must now minimize over both variables to compute
the next term. In other words,
S5,k = U5(w5 = k) + min
l,m [S2,l + S4,m + T5(w5 = k,w2 = l,w4 = m)].
(11.19)
Finally, we compute the last terms as normal so that
S6,k = U6(w6 = k) + min
l
[S5,l + P6(w6 = k,w5 = l)].
(11.20)
Now we ﬁnd the world state associated with the minimum of this ﬁnal sum and trace
back the route that we came by as before, splitting the route appropriately at junctions in
the tree.
Dynamic programming in this tree has a greater computational complexity than
dynamic programming in a chain with the same number of variables as we must mini-
mize over two variables at the junction in the tree. The overall complexity is proportional
to KW , where W is the maximum number of variables over which we must minimize.

204
11 Models for chains and trees
Figure 11.7 Dynamic programming example for tree model in Figure 11.6. a) Table of three-wise
costs at vertex 5. This is a K×K×K table consisting of the costs associated with Pr(w5|w2,w4).
Pairwise costs are as for the example in Figure 11.4. b) Tree-structured model with unary and
pairwise costs attached. c) We work from the leaves, ﬁnding the minimal possible cost Sn,k to
reach vertex n in state k as in the original dynamic programming formulation. d) When we reach
the vertex above a branch (here, vertex 5), we ﬁnd the minimal possible cost, considering every
combination of the incoming states. e) We continue until we reach the root. There, we ﬁnd the
minimum overall cost and trace back, making sure to split at the junction according to which pair
of states was chosen.

11.4
Marginal posterior inference for chains
205
For directed models, W is equal to the largest number of incoming connections at
any vertex. For undirected models, W will be the size of the largest clique. It should be
noted that for undirected models, the critical property that allows dynamic programming
solutions is that the cliques themselves form a tree (see Figure 11.11).
11.4
Marginal posterior inference for chains
In Section 11.3, we demonstrated that it is possible to perform MAP inference in chain
models efﬁciently using dynamic programming. In this section, we will consider a differ-
ent form of inference: we will aim to calculate the marginal distribution Pr(wn|x1...N)
over each state variable wn separately.
Consider computing the marginal distribution over the variable wN. By Bayes’ rule
we have
Pr(wN|x1...N) = Pr(wN,x1...N)
Pr(x1...N)
∝Pr(wN,x1...N).
(11.21)
The right-hand side of this equation is computed by marginalizing over all of the other
state variables except wN so we have
Pr(wN,x1...N) ∝
X
w1
X
w2
...
X
wN−1
Pr(w1...N,x1...N)
(11.22)
∝
X
w1
X
w2
...
X
wN−1
 N
Y
n=1
Pr(xn|wn)
!
Pr(w1)
 N
Y
n=2
Pr(wn|wn−1)
!
.
Unfortunately, in its most basic form, this marginalization involves summing over N −1
dimensions of the N dimensional probability distribution. Since this discrete probabil-
ity distribution contains KN entries, computing this summation directly is not practical
for realistic-sized problems. To make progress, we must again exploit the structured
factorization of this distribution.
11.4.1 Computing one marginal distribution
We will ﬁrst discuss how to compute the marginal distribution Pr(wN|x1...N) for the
last variable in the chain wN. In the following section, we will exploit these ideas to
compute all of the marginal distributions Pr(wn|x1...N) simultaneously.
We observe that not every term in the product in Equation 11.22 is relevant to every
summation. We can rearrange the summation terms so that only the variables over which
they sum are to the right
Pr(wN|x1...N) ∝
(11.23)
Pr(xN|wN)
X
wN−1
...
X
w2
Pr(w3|w2)Pr(x2|w2)
X
w1
Pr(w2|w1)Pr(x1|w1)Pr(w1).
Then, we proceed from right to left, computing each summation in turn. This technique
is known as variable elimination. Let us denote the rightmost two terms as
f1[w1] = Pr(x1|w1)Pr(w1).
(11.24)

206
11 Models for chains and trees
Then we sum over w1 to compute the function
f2[w2] = Pr(x2|w2)
X
w1
Pr(w2|w1)f1[w1].
(11.25)
At the nth stage, we compute
fn[wn] = Pr(xn|wn)
X
wn−1
Pr(wn|wn−1)fn−1[wn−1],
(11.26)
and we repeat this process until we have computed the full expression. We then normalize
the result to ﬁnd the marginal posterior Pr(wN|x1...N) (Equation 11.21).
This solution consists of N −1 summations over K values; it is much more efﬁcient
to compute than explicitly computing all KN solutions and marginalizing over N −1
dimensions.
11.4.2 Forward-backward algorithm
In the previous section, we showed an algorithm that could compute the marginal pos-
terior distribution Pr(wN|x1...N) for the last world state wN. It is easy to adapt this
method to compute the marginal posterior Pr(wn|x1...N) over any other variable wn.
11.3 However, we usually want all of the marginal distributions, and it is inefﬁcient to com-
pute each separately as much of the effort is replicated. The goal of this section is to
develop a single procedure that computes the marginal posteriors for all of the vari-
ables simultaneously and efﬁciently using a technique known as the forward-backward
algorithm.
The principle is to decompose the marginal posterior into two terms
Pr(wn|x1...N) ∝Pr(wn,x1...N)
= Pr(wn,x1...n)Pr(xn+1...N|wn,x1...n)
= Pr(wn,x1...n)Pr(xn+1...N|wn),
(11.27)
where the relation between the second and third line is true because x1...n and xn+1...N
are conditionally independent given wn (as can be gleaned from Figure 11.2). We will
now focus on ﬁnding efﬁcient ways to calculate each of these two terms.
Forward recursion
Let us consider the ﬁrst term Pr(wn,x1...n). We can exploit the recursion
Pr(wn,x1...n)
=
X
wn−1
Pr(wn,wn−1,x1...n)
=
X
wn−1
Pr(wn,xn|wn−1,x1...n−1)Pr(wn−1,x1...n−1)
=
X
wn−1
Pr(xn|wn,wn−1,x1...n−1)Pr(wn|wn−1,x1...n−1)Pr(wn−1,x1...n−1)
=
X
wn−1
Pr(xn|wn)Pr(wn|wn−1)Pr(wn−1,x1...n−1),
(11.28)

11.4
Marginal posterior inference for chains
207
where we have again applied the conditional independence relations implied by the
graphical model between the last two lines.
The term Pr(wn,x1...n) is exactly the intermediate function fn[wn] that we calcu-
lated in the solution for the single marginal distribution in the previous section; we have
reproduced the recursion
fn[wn] = Pr(xn|wn)
X
wn−1
Pr(wn|wn−1)fn−1[wn−1],
(11.29)
but this time, we based the argument on conditional independence rather than the factor-
ization of the probability distribution. Using this recursion, we can efﬁciently compute
the ﬁrst term of Equation 11.27 for all n; in fact, we were already doing this in our
solution for the single marginal distribution Pr(wN|x1...N).
Backward recursion
Now consider the second term Pr(xn+1...N|wn) from Equation 11.27. Our goal is to
develop a recursive relation for this quantity so that we can compute it efﬁciently for all
n. This time the recursion works backwards from the end of the chain to the front, so our
goal is to establish an expression for Pr(xn...N|wn−1) in terms of Pr(xn+1...N|wn):
Pr(xn...N|wn−1)
=
X
wn
Pr(xn...N,wn|wn−1)
=
X
wn
Pr(xn...N|wn,wn−1)Pr(wn|wn−1)
=
X
wn
Pr(xn+1...N|xn,wn,wn−1)Pr(xn|wn,wn−1)Pr(wn|wn−1)
=
X
wn
Pr(xn+1...N|wn)Pr(xn|wn)Pr(wn|wn−1).
(11.30)
Here we have again applied the conditional independence relations implied by the graph-
ical model between the last two lines. Denoting the probability Pr(xn+1...N|wn) as
bn[wn], we see that we have the recursive relation
bn−1[wn−1] =
X
wn
Pr(xn|wn)Pr(wn|wn−1)bn[wn].
(11.31)
We can use this to compute the second term in Equation 11.27 efﬁciently for all n.
Forward-backward algorithm
We can now summarize the forward-backward algorithm to compute the marginal poste-
rior probability distribution for all n. First, we observe (Equation 11.27) that the marginal
distribution can be computed as
Pr(wn|x1...N) ∝Pr(wn,x1...n)Pr(xn+1...N|wn) = fn[wn]bn[wn].
(11.32)
We recursively compute the forward terms using the relation
fn[wn] = Pr(xn|wn)
X
wn−1
Pr(wn|wn−1)fn−1[wn−1],
(11.33)

208
11 Models for chains and trees
Figure 11.8 Factor graph for chain model. There is one node per variable (circles) and one
function node per term in the factorization (squares). Each function node connects to all of the
variables associated with this term.
where we set f1[w1] = Pr(x1|w1)Pr(w1). We recursively compute the backward terms
using the relation
bn−1[wn−1] =
X
wn
Pr(xn|wn)Pr(wn|wn−1)bn[wn],
(11.34)
where we set bN[wN] to the constant value 1/K.
Finally, to compute the nth marginal posterior distribution, we take the product of
the associated forward and backward terms and normalize.
11.4.3 Belief propagation
The forward-backward algorithm can be considered a special case of a more general
technique called belief propagation. Here, the intermediate functions f[•] and b[•] are
considered as messages that convey information about the variables. In this section, we
describe a version of belief propagation known as the sum-product algorithm. This does
not compute the marginal posteriors any faster than the forward-backward algorithm, but
it is much easier to see how to extend it to models based on trees.
The sum-product algorithm operates on a factor graph. A factor graph is a new type
of graphical model that makes the factorization of the joint probability more explicit.
It is very simple to convert directed and undirected graphical models to factor graphs.
As usual, we introduce one node per variable; for example variables w1, w2, and w3
all have a variable node associated with them. We also introduce one function node
per term in the factorized joint probability distribution; in a directed model this would
represent a conditional probability term such as Pr(w1|w2,w3), and in an undirected
model it would represent a potential function such as φ[w1,w2,w3]. We then connect
each function node to all of the variable nodes relevant to that term with undirected links.
So, in a directed model, a term like Pr(x1|w1,w2) would result in a function node that
connects to x1, w1, and w2. In an undirected model, a term like φ12(w1,w2) would result
in a function node that connects to w1 and w2. Figure 11.8 shows the factor graph for the
chain model.
Sum-product algorithm
The sum product algorithm proceeds in two phases: a forward pass and a backward
11.4 pass. The forward pass distributes evidence through the graph and the backward pass

11.4
Marginal posterior inference for chains
209
collates this evidence. Both the distribution and collation of evidence are accomplished
by passing messages from node to node in the factor graph. Every edge in the graph is
connected to exactly one variable node, and each message is deﬁned over the domain of
this variable. There are three types of messages:
1. A message mzp→gq from an unobserved variable zp to a function node gq is
given by
mzp→gq =
Y
r∈ne[p]\q
mgr→zp,
(11.35)
where ne[p] returns the set of the neighbors of zp in the graph and so the expression
ne[p] \ q denotes all of the neighbors except q. In other words, the message from a
variable to a function node is the pointwise product of all other incoming messages
to the variable; it is the combination of other beliefs.
2. A message mzp→gq from an observed variable zp = z∗
p to a function node gq is
given by
mzp→gq = δ[z∗
p].
(11.36)
In other words, the message from an observed node to a function conveys the
certain belief that this node took the observed value.
3. A message mgp→zq from a function node gp to a recipient variable zq is deﬁned as
mgp→zq =
X
ne[p]\q
gp[ne[p]]
Y
r∈ne[p]\q
mzr→gp.
(11.37)
This takes beliefs from all variables connected to the function except the recipient
variable and uses the function gp[•] to convert these to a belief about the recipient
variable.
In the forward phase, the message passing can proceed in any order, as long as the
outgoing message from any variable or function is not sent until all the other incoming
messages have arrived. In the backward pass, the messages are sent in the opposite order
to the forward pass.
Finally, the marginal distribution at node zp can be computed from a product of all
of the incoming messages from both the forward and reverse passes so that
Pr(zp) ∝
Y
r∈ne[p]
mgr→zp.
(11.38)
A proof that this algorithm is correct is beyond the scope of this book. However, to
make this at least partially convincing (and more concrete), we will work through these
rules for the case of the chain model (Figure 11.8), and we will show that exactly the
same computation occurs as for the forward-backward algorithm.
11.4.4 Sum-product algorithm for chain model
The factor graph for the chain solution annotated with messages is shown in Figure 11.9.
We will now describe the sum-product algorithm for the chain model.

210
11 Models for chains and trees
Figure 11.9 Sum product algorithm for chain model (forward pass). The sum-product algorithm
has two phases. In the forward phase, messages are passed through the graph in an order such
that a message cannot be sent until all incoming messages are received at the source node. So, the
message mw2→g23 cannot be sent until the messages mg2→w2 and mg1,2→w2 have been received.
Forward pass
We start by passing a message mx1→g1 from node x1 to the function node g1. Using
rule 2, this message is a delta function at the observed value x∗
1, so that
mx1→g1 = δ[x∗
1].
(11.39)
Now we pass a message from function g1 to node w1. Using rule 3, we have
mg1→w1 =
Z
Pr(x1|w1)δ[x∗
1]dx1 = Pr(x1=x∗
1|w1).
(11.40)
By rule 1, the message from node w1 to function g1,2 is simply the product of the
incoming nodes, and since there is only one incoming node, this is just
mw1→g12 = Pr(x1=x∗
1|w1).
(11.41)
By rule 3, the message from function g1,2 to node w2 is computed as
mg1,2→w2 =
X
w1
Pr(w2|w1)Pr(x1 = x∗
1|w1).
(11.42)
Continuing this process, the messages from x2 to g2 and g2 to w2 are
mx2→g2 = δ[x∗
2]
mg2→w2 = Pr(x2 = x∗
2|w2),
(11.43)
and the message from w2 to g2,3 is
mw2→g2,3 = Pr(x2 = x∗
2|w2)
X
w1
Pr(w2|w1)Pr(x1 = x∗
1|w1).
(11.44)
A clear pattern is emerging; the message from node wn to function gn,n+1 is equal
to the forward term from the forward-backward algorithm:
mwn→gn,n+1 = fn[wn] = Pr(wn,x1...n).
(11.45)
In other words, the sum-product algorithm is performing exactly the same computations
as the forward pass of the forward-backward algorithm.

11.5
Marginal posterior inference for trees
211
Backward pass
When we reach the end of the forward pass of the belief propagation, we initiate the
backward pass. There is no need to pass messages toward the observed variables xn
since we already know their values for certain. Hence, we concentrate on the horizontal
connections between the unobserved variables (i.e., along the spine of the model). The
message from node wN to function gN,N−1 is given by
mwN→gN,N−1 = Pr(xN = x∗
N|wN),
(11.46)
and the message from gN,N−1 to wN−1 is given by
mgN,N−1→wN−1 =
X
wN
Pr(wN|wN−1)Pr(xN = x∗
N|wN).
(11.47)
In general, we have
mgn,n−1→wn−1 =
X
wn
Pr(wn|wn−1)Pr(xn|wn)mgn+1,n→wn
= bn−1[wn−1],
(11.48)
which is exactly the backward recursion from the forward-backward algorithm.
Collating evidence
Finally, to compute the marginal probabilities, we use the relation
Pr(wn|x1...N) ∝
Y
m∈ne[n]
mgm→wn,
(11.49)
and for the general case, this consists of three terms:
Pr(wn|x1...N) ∝mgn−1,n→wnmgn→wnmgn,n+1→wn
= mwn→gn,n+1mgn,n+1→wn
= fn[wn]bn[wn],
(11.50)
where in the second line we have used the fact that the outgoing message from a variable
node is the product of the incoming messages. We conclude that the sum-product algo-
rithm computes the posterior marginals in exactly the same way as the forward backward
algorithm.
11.5
Marginal posterior inference for trees
To compute the marginals in tree-structured models we simply apply the sum product
algorithm to the new graph structure. The factor graph for the tree in Figure 11.6 is
shown in Figure 11.10. The only slight complication is that we must ensure that the ﬁrst
two incoming messages to the function relating variables w2, w4, and w5 have arrived
before sending the outgoing message. This is very similar to the order of operations in
the dynamic programming algorithm.
For undirected graphs, the key property is that the cliques, not the nodes, form a tree.
For example, there is clearly a loop in the undirected model in Figure 11.11a, but when
we convert this to a factor graph the structure is a tree (Figure 11.11b). For models with
only pairwise cliques, the cliques always form a tree if there are no loops in the original
graphical model.

212
11 Models for chains and trees
Figure 11.10 Factor graph corresponding
to tree model in Figure 11.6. There is one
function node connecting each world state
variable to its associated measurement and
these correspond to the terms Pr(xn|wn).
There
is
one
function
node
for
each
of the three pairwise terms Pr(w2|w1),
Pr(w4|w3) and Pr(w6|w5) and this is con-
nected to both contributing variables. The
function node corresponding to the three-
wise term Pr(w5|w2,w4) has three neigh-
bors, w5, w2, and w4.
11.6
Learning in chains and trees
So far, we have only discussed inference for these models. Here, we brieﬂy discuss
learning which can be done in a supervised or unsupervised context. In the supervised
case, we are given a training set of I matched sets of states {win}I,N
i=1,n=1 and data
{xin}I,N
i=1,n=1. In the unsupervised case, we only observe the data {xin}I,N
i=1,n=1.
Supervised learning for directed models is relatively simple. We ﬁrst isolate the part
of the model that we want to learn. For example, we might learn the parameters θ of
Pr(xn|wn,θ) from paired examples of xn and wn. We can then learn these parameters
in isolation using the ML, MAP, or Bayesian methods.
Unsupervised learning is more challenging; the states wn are treated as hidden vari-
ables and the EM algorithm is applied. In the E-step, we compute the posterior marginals
over the states using the forward backward algorithm.
In the M-step we use these
marginals to update the model parameters. For the hidden Markov model (the chain
model), this is known as the Baum-Welch algorithm.
As we saw in the previous chapter, learning in undirected models can be challenging;
we cannot generally compute the normalization constant Z and this in turn prevents us
from computing the derivative with respect to the parameters. However, for the special
case of tree and chain models, it is possible to compute Z efﬁciently and learning is
tractable. To see why this is the case, consider the undirected model from Figure 11.2b,
which we will treat here as representing the conditional distribution
Pr(w1...N|x1...N) = 1
Z
 N
Y
n=1
φ[xn,wn]
! N
Y
n=2
ζ[wn,wn−1]
!
,
(11.51)
since the data nodes {xn}N
n=1 are ﬁxed. This model is known as a 1D conditional random
ﬁeld. The unknown constant Z now has the form
Z =
X
w1
X
w2
...
X
wN
 N
Y
n=1
φ[xn,wn]
! N
Y
n=2
ζ[wn,wn−1]
!
.
(11.52)

11.7
Beyond chains and trees
213
Figure 11.11 Converting an undirected model to a factor graph. a) Undirected model. b) Corre-
sponding factor graph. There is one function node for each maximal clique (each clique which is
not a subset of another clique). Although there was clearly a loop in the original graph, there is no
loop in the factor graph and so the sum-product algorithm is still applicable.
a)
b)
Figure 11.12 Grid-based models. For many vision problems, the natural description is a grid-
based model. We observe a grid of pixel values {xn}N
n=1 and wish to infer an unknown world
state {wn}N
n=1 associated with each site. Each world state is connected to its neighbors. These
connections are usually applied to ensure a smooth or piecewise smooth solution. a) Directed grid
model. b) Undirected grid model (2D conditional random ﬁeld).
We have already seen that it is possible to compute this type of sum efﬁciently, using a
recursion (Equation 11.26). Hence, Z can be evaluated and maximum likelihood learning
can be performed in this model without the need for contrastive divergence.
11.7
Beyond chains and trees
Unfortunately, there are many models in computer vision that do not take the form of
a chain or a tree. Of particular importance are models that are structured to have one
unknown wn for each RGB pixel xn in the image. These models are naturally structured
as grids and the world states are each connected to their four pixel neighbors in the
graphical model (Figure 11.12). Stereo vision, segmentation, denoising, superresolution,
and many other vision problems can all be framed in this way.

214
11 Models for chains and trees
Figure
11.13 Dynamic programming
fails when there are undirected loops.
Here, we show a 2 × 2 image where
we have performed a na¨ıve forward pass
through the variables. On retracing the
route, we see that the two branches dis-
agree over which state the ﬁrst variable
took. For a coherent solution the cumula-
tive minimum costs at node 4, we should
have forced the two paths to have com-
mon ancestors.
With a large number
of ancestors this is too computationally
expensive to be practical.
We devote the whole of the next chapter to grid-based problems, but we will brieﬂy
take the time to examine why the methods developed in this chapter are not suitable.
Consider a simple model based on a 2 × 2 grid. Figure 11.13 illustrates why we can-
not blindly use dynamic programming to compute the MAP solution. To compute the
minimum cumulative cost Sn at each node, we might na¨ıvely proceed as normal:
S1,k = U1(w1 = k)
S2,k = U2(w2 = k) + min
l
[S1(w1 = l) + P2(w2 = k,w1 = l)]
S3,k = U3(w3 = k) + min
l
[S1(w1 = l) + P2(w3 = k,w1 = l)].
(11.53)
Now consider the fourth term. Unfortunately,
S4,k ̸= U4(wk = 4) + min
l,m [S2(w2 = l) + S3(w3 = m) + T(w4 = k,w2 = l,w3 = m)].
(11.54)
The reason for this is that the partial cumulative sums S2 and S3 at the two previous
vertices both rely on minimizing over the same variable w1. However, they did not nec-
essarily choose the same value at w1. If we were to trace back the paths we took, the two
routes back to vertex one might predict a different answer. To properly calculate the min-
imum cumulative cost at node S4,k, we would have to take account of all three ancestors:
the recursion is no longer valid, and the problem becomes intractable once more.
Similarly, we cannot perform belief propagation on this graph: the algorithm requires
us to send a message from a node only when all other incoming messages have been
received. However, the nodes w1,w2,w3, and w4 all simultaneously require messages
from one another and so this is not possible.
11.7.1 Inference in graphs with loops
Although the methods of this chapter are not suitable for models based on graphs with
loops, there are a number of ways to proceed:
1. Prune the graph. An obvious idea is to prune the graph by removing edges until
what is left has a tree structure (Figure 11.14). The choice of which edges to prune
will depend on the real-world problem.

11.7
Beyond chains and trees
215
Figure 11.14 Pruning graphs with
loops. One approach to dealing with
models with loops is simply to prune
the connections until the loops are
removed.
This graphical model is
the model from Figure 11.12a after
such a pruning process. Most of the
connections are retained but now the
remaining structure is a tree.
The
usual approach to pruning is to asso-
ciate a strength with each edge so
that weaker edges are more desirable.
Then we compute the minimum span-
ning tree based in these strengths and
discard any connections that do not
form part of the tree.
Figure 11.15 Combining variables.
a) This graphical model contains
loops. b) We form three compound
variables, each of which consists of
all of the variables in one of the orig-
inal columns.
These are now con-
nected by a chain structure. However,
the price we pay is that if there were
K states for each original variable,
the compound variables now have
K3 states.
2. Combine variables. A second approach is to combine variables together until
what remains has the structure of a chain or tree. For example, in Figure 11.15
we combine the variables w1, w2, and w3, to make a new variable w123 and the
variables w4,w5, and w6 to form w456. Continuing in this way, we form a model
that has a chain structure. If each of the original variables had K states, then the
compound variables will have K3 states, and so inference will be more expensive.
In general the merging of variables can be automated using the junction tree algo-
rithm. Unfortunately, this example illustrates why this approach will not work for
large grid models: we must merge together so many variables that the resulting
compound variable has too many states to work with.
3. Loopy belief propagation. Another idea is to simply apply belief propagation
regardless of the loops. All messages are initialized to uniform and then the mes-
sages are repeatedly passed in some order according to the normal rules. This
algorithm is not guaranteed to converge to the correct solution for the marginals

216
11 Models for chains and trees
(or indeed to converge at all) but in practice it produces useful results in many
situations.
4. Sampling approaches. For directed graphical models, it is usually easy to draw
samples from the posterior. These can then be aggregated to compute an empirical
estimate of the marginal distributions.
5. Other approaches: There are several other approaches for exact or approximate
inference in graphs, including tree reweighted message passing and graph cuts.
The latter is a particularly important class of algorithms, and we devote most of
Chapter 12 to describing it.
11.8
Applications
The models in this chapter are attractive because they permit exact MAP inference. They
have been applied to a number of problems in which there are assumed spatial or temporal
connections between parts of a model.
11.8.1 Gesture tracking
The goal of gesture tracking is to classify the position {wn}N
n=1 of the hands within
each of the N captured frames from a video sequence into a discrete set of possible
gestures, wn ∈{1,2,...,K} based on extracted data {xn}N
n=1 from those frames. Starner
et al. (1998) presented a wearable system for automatically interpreting sign language
gestures. A camera mounted in the user’s hat captured a top-down view of their hands
(Figure 11.16). The positions of the hands were identiﬁed by using a per-pixel skin
segmentation technique (see Section 6.6.1). The state of each hand was characterized in
terms of the position and shape of a bounding ellipse around the associated skin region.
The ﬁnal eight-dimensional data vector x concatenated these measurements from each
hand.
To describe the time sequences of these measurements, Starner et al. (1998) devel-
oped a hidden Markov model-based system in which the states wn each represented a
part of a sign language word. Each of these words was represented by a progression
through four values of the state variable w, representing the various stages in the associ-
ated gesture for that word. The progression through these states might last any number of
time steps (each state can be followed by itself so it can cycle indeﬁnitely) but must come
in the required order. They trained the system using 400 training sentences. They used a
dynamic programming method to estimate the most likely states w and achieved recog-
nition accuracy of 97.8 percent using a 40-word lexicon with a test set of 100 sentences.
They found that performance was further improved if they imposed knowledge about the
ﬁxed grammar of each phrase (pronoun, verb, noun, adjective, pronoun). Remarkably,
the system worked at a rate of 10 frames a second.
11.8.2 Stereo vision
In dense stereo vision, we are given two images of the same scene taken from slightly
different positions. For our purposes, we will assume that they have been preprocessed
so that for each pixel in image 1, the corresponding pixel is on the same scanline in image
2 (a process known as rectiﬁcation; see Chapter 16). The horizontal offset or disparity
between corresponding points depends on the depth. Our goal is to ﬁnd the discrete

11.8
Applications
217
Figure 11.16 Gesture tracking from Starner et al. (1998). A camera was mounted on a baseball
cap (inset) looking down at the users hands. The camera image (main ﬁgure) was used to track the
hands in a HMM-based system that could accurately classify a 40-word lexicon and worked in real
time. Each word was associated with four states in the HMM. The system was based on a compact
description of the hand position and orientation within each frame. Adapted from Starner et al.
(1998), c⃝1998 Springer.
disparity ﬁeld w given the observed images x(1) and x(2), from which the depth of each
pixel can be recovered (Figure 11.17).
We assume that the pixel in image 1 should closely resemble the pixel at the appro-
priate offset (disparity) in image 2 and any remaining small differences are treated as
noise so that
Pr(x(1)
m,n|wm,n = k) = Normx(1)
m,n
h
x(2)
m,n+k,σ2I
i
,
(11.55)
where wm,n is the disparity at pixel (m,n) of image 1, x(1)
m,n is the RGB vector from
pixel (m,n) of image 1 and x(2)
m,n is the RGB vector from pixel (m,n) of image 2.
Unfortunately, if we compute the maximum likelihood disparities wm,n at each pixel
separately, the result is extremely noisy (Figure 11.18a). As Figure 11.17 illustrates,
the choice of disparity is ambiguous in regions of the image where there are few visual
changes. In layman’s terms, if the nearby pixels are all similar, it is difﬁcult to establish
with certainty which corresponds to a given position in the other image. To resolve this
ambiguity, we introduce a prior Pr(w) that encourages piecewise smoothness in the
disparity map; we are exploiting the fact that we know that the scene mainly consists of
smooth surfaces, with occasional jumps in disparity at the edge of objects.
One possible approach (attributed originally to Ohta and Kanade 1985) to recovering
the disparity is to use an independent prior for each scanline so that
Pr(w) =
M
Y
m=1
Pr(wm),
(11.56)

218
11 Models for chains and trees
Figure 11.17 Dense stereo vision. a–b) Two images taken from slightly different positions. The
corresponding point for every pixel in the ﬁrst image is somewhere on the same scanline in the
second image. The horizontal offset is known as the disparity and is inversely related to depth.
c) Ground truth disparity map for this image. d) Close-up of part of ﬁrst image with two pixels
highlighted. e) Close-up of second image with potential corresponding pixels highlighted. f) RGB
values for red pixel (dashed lines) in ﬁrst image and as a function of the position in second image
(solid lines). At the correct disparity, there is very little difference between the RGB values in
the two images and so g) the likelihood that this disparity is correct is large. h–i) For the green
pixel (which is in a smoothly changing region of the image), there are many positions where the
RGB values in the second image are similar and hence many disparities have high likelihoods; the
solution is ambiguous.
where each scanline was organized into a chain model (Figure 11.2) so that
Pr(wm) = Pr(wm,1)
N
Y
n=2
Pr(wm,n|wm,n−1).
(11.57)
The distributions Pr(wm,n|wm,n−1) are chosen so that they allot a high probability when
adjacent disparities are the same, an intermediate probability when adjacent disparities
change by a single value, and a low probability if they take values that are more widely
separated. Hence, we encourage piecewise smoothness.
MAP inference can be performed within each scanline separately using the dynamic
programming approach and the results combined to form the full disparity ﬁeld w.
Whereas this deﬁnitely improves the ﬁdelity of the solution, it results in a characteristic
“streaky” result (Figure 11.18b). These artifacts are due to the (erroneous) assumption
that the scanlines are independent. To get an improved solution, we should smooth in the

11.8
Applications
219
a)
b)
c)
Figure 11.18 Dense stereo results. Recovered disparity maps for a) independent pixels model, b)
independent scanlines model, and c) tree-based model of Veksler (2005).
vertical direction as well, but the resulting grid-based model will contain loops, making
MAP inference problematic.
Veksler (2005) addressed this problem by pruning the full grid-based model until it
formed a tree. Each edge was characterized by a cost that increased if the associated
pixels were close to large changes in the image; at these positions, either there is texture
in the image (and so the disparity is relatively well deﬁned) or there is an edge between
two objects in the scene. In either case, there is no need to apply a smoothing prior here.
Hence, the minimum spanning tree tends to retain edges in regions where they are most
needed. The minimum spanning tree can be computed using a standard method such as
Prim’s algorithm (see Cormen et al. 2001).
The results of MAP inference using this model are shown in Figure 11.18c. The
solution is piecewise smooth in both directions and is clearly superior to either the inde-
pendent pixels model or the independent scanline approach. However, even this model is
an unnecessary approximation; we would ideally like the variables to be fully connected
in a grid structure, but this would obviously contain loops. In Chapter 12, we consider
models of this sort and revisit stereo vision.
11.8.3 Pictorial Structures
Pictorial structures are models for object classes that consist of a number of individual
parts that are connected together by spring-like connections. A typical example would be
a face model (Figure 11.19), which might consist of a nose, eyes, and mouth. The spring-
like connections encourage the relative positions of these features to take sensible values.
For example, the mouth is strongly encouraged to be below the nose. Pictorial structures
have a long history in computer vision but were revived in a modern form by Felzen-
szwalb and Huttenlocher (2005) who identiﬁed that if the connections between parts take
the form of an acyclic graph (a tree), then they can be ﬁt to images in polynomial time.
The goal of matching a pictorial structure to an image is to identify the positions
{wn}N
n=1 of the N parts based on data {xn} associated with each. For example, a sim-
ple system might assign a likelihood Pr(x|wn = k) that is a normal distribution over a
patch of the image at position k. The relative positions of the parts are encoded using
distributions of the form Pr(wn|wpa[n]). MAP inference in this system can be achieved
using a dynamic programming technique.
Figure 11.19 shows a pictorial structure for a face. This model is something of a
compromise in that it would be preferable if the features had more dense connections:

220
11 Models for chains and trees
Figure 11.19 Pictorial structure. This face
model consists of seven parts (red dots),
which are connected together in a tree-like
structure (red lines). The possible positions
of each part are indicated by the yellow
boxes. Although each part can take several
hundred pixel positions, the MAP positions
can be inferred efﬁciently by exploiting the
tree-structure of the graph using a dynamic
programming approach.
Localizing facial
features is a common element of many face
recognition pipelines.
for example the left eye provides information about the position of the right eye as well
as the nose. Nonetheless, this type of model can reliably ﬁnd features on frontal faces.
A second application is for ﬁtting articulated models such as human bodies (Figure
11.20). These naturally have the form of a tree and so the structure is determined by the
problem itself. Felzenszwalb and Huttenlocher (2005) developed a system of this sort in
which each state wn represented a part of the model (e.g., the right forearm). Each state
could take K possible values representing different possible positions and shapes of the
object part.
The image was preclassiﬁed into foreground and background using a background
subtraction technique. The likelihood Pr(xn|w = k) for a particular part position was
evaluated using this binary image.
In particular, the likelihood was chosen so that
it increased if the area within the rectangle was considered foreground and the area
surrounding it was considered background.
Unfortunately, MAP inference in this model is somewhat unreliable: a common fail-
ure mode is for more than one part of the body to become associated with the same part
of the binary image. This is technically possible as the limbs may occlude each other, but
it can also happen erroneously if one limb dominates and supports the rectangle model
signiﬁcantly more than the others. Felzenszwalb and Huttenlocher (2005) dealt with this
problem by drawing samples from the posterior distribution Pr(w1...N|x1...N) over the
positions of the parts of the model and using a more complex criterion to choose the most
promising sample.
11.8.4 Segmentation
In Section 7.9.3, we considered segmentation as the problem of labeling pixels according
to the object to which they belong. A different approach to segmentation is to infer the
position of a closed contour that delineates two objects. In inference, the goal is usually
to infer the positions of a set of points {wn} on the boundary of this contour based on
the image data x. As we update these points during an attempt to ﬁnd the MAP solution,
the contour moves across the image, and for this reason this type of model is referred to
as an active contour or snake model.

11.8
Applications
221
Figure 11.20 Pictorial structure for human body. a) Original image. b) After background sub-
traction. c–f) Four samples from the posterior distribution over part positions. Each part position
is represented by a rectangle of ﬁxed aspect ratio and characterized by its position, size, and angle.
Adapted from Felzenszwalb and Huttenlocher (2005). c⃝2005 Springer.
Figure 11.21 Segmentation using snakes. a) Two points are ﬁxed, but the remaining points can
take any position within their respective boxes. The posterior distribution favors positions that are
on image contours (due to the likelihood term) and positions that are close to other points (due
to the pairwise connections). b) Results of inference. c) Two other points are considered ﬁxed.
d) Result of inference. In this way, a closed contour in the image is identiﬁed. Adapted from
Felzenszwalb and Zabih (2011). c⃝2011 IEEE.
Figure 11.21 shows an example of ﬁtting this type of model. At each iteration, the
positions {wn} of all of the points except two are updated and can take any position
within small region surrounding their previous position. The likelihood of taking a par-
ticular value wn = k is high at positions in the image where the intensity changes rapidly
(i.e., the edges) and low in constant regions. In addition, neighboring points are con-
nected and have an attractive force: they are more likely to be close to one another. As
usual, inference can be carried out using the dynamic programming method. During
inference, the points tend to become closer together (due to their mutual attraction) but
get stuck on the edge of an object.
In the full system, this process is repeated, but with a different pair of adjacent points
chosen to be ﬁxed at each step. Hence, the dynamic programming is a component step

222
11 Models for chains and trees
of a larger inference problem. As the inference procedure continues, the contour moves
across the image and eventually ﬁxes onto the boundary of an object. For this reason,
these models are known as snakes or active contour models. They are considered in more
detail in Chapter 17.
Discussion
In this chapter, we have considered models based on acyclic graphs (chains and trees).
In Chapter 12, we will consider grid-based models that contain many loops. We will see
that MAP inference is only tractable in a few special cases. In contrast to this chapter,
we will also see a large difference between directed and undirected models.
Notes
Dynamic programming: Dynamic programming is used in many vision algorithms, includ-
ing those where there is not necessarily a clear probabilistic interpretation. It is an attractive
approach when it is applicable because of its speed, and some efforts have been made to
improve this further (Raphael 2001).
Interesting examples include image retargeting (Avidan
and Shamir 2007), contour completion (Sha’ashua and Ullman 1988), ﬁtting of deformable tem-
plates (Amit and Kong 1996; Coughlan et al. 2000), shape matching (Basri et al. 1998), the
computation of superpixels (Moore et al. 2008) and semantic labeling of scenes with tiered
structure (Felzenszwalb and Veksler 2010) as well as the applications described in this chapter.
Felzenszwalb and Zabih (2011) provide a recent review of dynamic programming and other graph
algorithms in computer vision.
Stereo vision: Dynamic programming was variously applied to stereo vision by Baker and Binford
(1981), Ohta and Kanade (1985) (who use a model based on edges) and Geiger et al. (1992) (who
used a model based on intensities). Birchﬁeld and Tomasi (1998) improved the speed by removing
unlikely search nodes from the dynamic programming solution and introduced a mechanism that
made depth discontinuities more likely where there was intensity variation. Torr and Criminisi
(2004) developed a system that integrated dynamic programming with known constraints such as
matched keypoints. Gong and Yang (2005) developed a dynamic programming algorithm that ran
on a graphics processing unit (GPU). Kim et al. (2005) introduced a method for identifying dispar-
ity candidates at each pixel using spatial ﬁlters and a two-pass method that performed optimization
both along and across the scanlines. Veksler (2005) used dynamic programming in a tree to solve
for the whole image at once, and this idea has subsequently been used in a method based on line
segments (Deng and Lin 2006). A recent quantitative comparison of dynamic programming algo-
rithms in computer vision can be found in Salmen et al. (2009). Alternative approaches to stereo
vision, which are not based on dynamic programming, are considered in Chapter 12.
Pictorial structures: Pictorial structures were originally introduced by Fischler and Erschlager
(1973) but recent interest was stimulated by the work of Felzenszwalb and Huttenlocher (2005)
who introduced efﬁcient methods of inference based on dynamic programming. There have been
a number of attempts to improve the appearance (likelihood) term of the model (Kumar et al.
2004; Eichner and Ferrari 2009; Andriluka et al. 2009; Felzenszwalb et al. 2010). Models that do
not conform to a tree structure have also been introduced (Kumar et al. 2004; Sigal and Black 2006;
Ren et al. 2005; Jiang and Martin 2008) and here alternative methods such as loopy propagation
must be used for inference. These more general structures are particularly important for addressing
problems associated with occlusions in human body models. Other authors have based their model
on a mixture of trees (Everingham et al. 2006; Felzenszwalb et al. 2010). In terms of applications,
Ramanan et al. (2008) have developed a notable system for tracking humans in video sequences
based on pictorial structures, Everingham et al. (2006) have developed a widely used system for

Problems
223
locating facial features, and Felzenszwalb et al. (2010) have presented a system that is used for
detecting more general objects.
Hidden Markov models: Hidden Markov models are essentially chain-based models that are
applied to quantities evolving in time. Good tutorials on the subject including details of how to
learn them in the unsupervised case can be found in Rabiner (1989) and Ghahramani (2001). Their
most common application in vision is for gesture recognition (Starner et al. 1998; Rigoll et al.
1998; and see Moni and Ali 2009 for a recent review), but they have also been used in other
contexts such as modeling interactions of pedestrians (Oliver et al. 2000). Some recent work (e.g.,
Bor Wang et al. 2006) uses a related discriminative model for tracking objects in time known as a
conditional random ﬁeld (see Chapter 12).
Snakes: The idea of a contour evolving over the surface of an image is due to Kass et al. (1987).
Both Amini et al. (1990) and Geiger et al. (1995) describe dynamic programming approaches to
this problem. These models are considered further in Chapter 17.
Belief propagation: The sum-product algorithm (Kschischang et al. 2001) is a development of
earlier work on belief propagation by Pearl (1988). The factor graph representation is due to Frey
et al. (1997). The use of belief propagation for ﬁnding marginal posteriors and MAP solutions in
graphs with loops has been investigated by Murphy et al. (1999) and Weiss and Freeman (2001),
respectively. Notable applications of loopy belief propagation in vision include stereo vision (Sun
et al. 2003) and superresolving images (Freeman et al. 2000). More information about belief
propagation can be found in machine learning textbooks such as Bishop (2006), Barber (2012),
and Koller and Friedman (2009).
Problems
11.1 Compute by hand the lowest possible cost for traversing the graph in Figure 11.22 using the
dynamic programming method.
11.2 MAP inference in chain models can also be performed by running Djikstra’s algorithm on
the graph in Figure 11.23, starting from the node on the left-hand side and terminating when we
ﬁrst reach the node on the right-hand side. If there are N variables, each of which takes K values,
what is the best and worst case complexity of the algorithm? Describe a situation where Djikstra’s
algorithm outperforms dynamic programming.
11.3 Consider the graphical model in Figure 11.24a.
Write out the cost function for MAP
estimation in the form of Equation 11.17.
Discuss the difference between your answer and
Equation 11.17.
Figure 11.22 Dynamic programming example for Problem 11.1.

224
11 Models for chains and trees
Figure 11.23 Graph construction for Problem 11.2. This is the same as the dynamic programming
graph (Figure 11.3) except that: (i) there are two extra nodes at the start and the end of the graph.
(ii) There are no vertex costs. (iii) The costs associated with the leftmost edges are U1(k) and the
costs associated with the rightmost edges are 0. The general edge cost for passing from label a and
node n to label b at node n + 1 is given by Pn,n+1(a,b) + Un+1(b).
Figure 11.24 a) Graphical model for Problem 11.3. b) Graphical model for Problem 11.10. The
unknown variables w3,w4,... in this model receive connections from the two preceding variables
and so the graph contains loops.
11.4 Compute the solution (minimum cost path) to the dynamic programming problem on the tree
in Figure 11.25 (which corresponds to the graphical model from ﬁgure 11.6).
11.5 MAP inference for the chain model can be expressed as
ˆwN = argmax
wN
"
max
w1
"
max
w2
"
... max
wN−1
" N
X
n=1
log[Pr(xn|wn)] +
N
X
n=2
log[Pr(wn|wn−1)]
#
...
###
.
Show that it is possible to compute this expression piecewise by moving the maximization terms
through the summation sequence in a manner similar to that described in Section 11.4.1.
11.6 Develop an algorithm that can compute the marginal distribution for an arbitrary variable wn
in a chain model.
11.7 Develop an algorithm that computes the joint marginal distribution of any two variables wm
and wn in a chain model.

Problems
225
Figure 11.25 Dynamic programming example for Problem 11.4.
Figure 11.26 Graphical models for Problem 11.9.
11.8 Consider the following two distributions over three variables x1,x2, and x3:
Pr(x1,x2,x3) = 1
Z1 φ12[x1,x2]φ23[x2,x3]φ31[x3,x1]
Pr(x1,x2,x3) = 1
Z2 φ123[x1,x2,x3].
Draw (i) an undirected model and (ii) a factor graph for each distribution. What do you conclude?
11.9 Convert each of the graphical models in Figure 11.26 into the form of a factor graph. Which
of the resulting factor graphs take the form of a chain?
11.10 Figure 11.24b shows a chain model in which each unknown variable w depends on its two
predecessors. Describe a dynamic programming approach to ﬁnding the MAP solution. (Hint: You
need to combine variables). If there are N variables in the chain and each takes K values, what is
the overall complexity of your algorithm?
11.11 In the stereo vision problem, the solution was very poor when the pixels are treated inde-
pendently (Figure 11.18a). Suggest some improvements to this method (while keeping the pixels
independent).

226
11 Models for chains and trees
11.12 Consider a variant on the segmentation application (Figure 11.21) in which we update all
of the contour positions at once. The graphical model for this problem is a loop (i.e., a chain where
there is also a edge between wN and w1). Devise an approach to ﬁnding the exact MAP solution
in this model. If there are N variables each of which can take K values, what is the complexity of
your algorithm?

Chapter 12
Models for grids
In Chapter 11, we discussed models that were structured as chains or trees. In this
chapter, we consider models that associate a label with each pixel of an image. Since
the unknown quantities are deﬁned on the pixel lattice, models deﬁned on a grid struc-
ture are appropriate. In particular, we will consider graphical models in which each
label has a direct probabilistic connection to each of its four neighbors.
Critically,
this means that there are loops in the underlying graphical model and so the dynamic
programming and belief propagation approaches of the previous chapter are no longer
applicable.
These grid models are predicated on the idea that the pixel provides only very
ambiguous information about the associated label. However, certain spatial conﬁgura-
tions of labels are known to be more common than others, and we aim to exploit this
knowledge to resolve the ambiguity.
In this chapter, we describe the relative pref-
erence for different conﬁgurations of labels with a pairwise Markov random ﬁeld or
MRF. As we shall see, maximum a posteriori inference for pairwise MRFs is tractable
in some circumstances using a family of approaches known collectively as graph
cuts.
To motivate the grid models, we introduce a representative application. In image
denoising we observe a corrupted image in which the intensities at a certain proportion of
pixels have been randomly changed to another value according to a uniform distribution
(Figure 12.1). Our goal is to recover the original clean image. We note two important
aspects of the problem.
1. Most of the pixels are uncorrupted, so the data usually tell us which intensity value
to pick.
2. The uncorrupted image is mainly smooth, with few changes between intensity
levels.
Consequently, our strategy will be to construct a generative model where the MAP solu-
tion is an image that is mostly the same as the noisy version, but is smoother. As part of
this solution, we need to deﬁne a probability distribution over images that favor smooth-
ness. In this chapter, we will use a discrete formulation of a Markov random ﬁeld to
fulﬁll this role.

228
12 Models for grids
Figure 12.1 Image denoising. a) Original binary image. b) Observed image created by randomly
ﬂipping the polarity of a ﬁxed proportion of pixels. Our goal is to recover the original image from
the corrupted one. c) Original grayscale image. d) Observed corrupted image is created by setting
a certain proportion of the pixels to values drawn from a uniform distribution. Once more, we aim
to recover the original image.
12.1
Markov random ﬁelds
A Markov random ﬁeld is formally determined by
• A set of sites S = {1...N}. These will correspond to the N pixel locations.
• A set of random variables {wn}N
n=1 associated with each of the sites.
• A set of neighbors {Nn}N
n=1 at each of the N sites.
To be a Markov random ﬁeld, the model must obey the Markov property:
Pr(wn|wS\n) = Pr(wn|wNn).
(12.1)
In other words, the model should be conditionally independent of all of the other
variables given its neighbors. This property should sound familiar: this is exactly how
conditional independence works in an undirected graphical model.
Consequently, we can consider a Markov random ﬁeld as an undirected model (Sec-
tion 10.3) that describes the joint probability of the variables as a product of potential
functions so that
Pr(w) = 1
Z
J
Y
j=1
φj[wCj],
(12.2)
where φj[•] is the jth potential function and always returns a nonnegative value. This
value depends on the state of the subset of variables Cj ⊂{1,...,N}. In this context,
this subset is known as a clique. The term Z is called the partition function and is a
normalizing constant that ensures that the result is a valid probability distribution.
Alternatively, we can rewrite the model as a Gibbs distribution:
Pr(w) = 1
Z exp

−
J
X
j=1
ψj[wCj]

,
(12.3)
where ψ[•] = −log[φ[•]] is known as a cost function and returns either positive or
negative values.

12.1
Markov random ﬁelds
229
Figure
12.2 Graphical model for worked
MRF example.
The variables form a 2 × 2
grid. This is an undirected model where each
link represents a potential function deﬁned over
the two variables that it connects. Each poten-
tial returns a positive number that indicates
the tendency of the two variables to take these
particular values.
12.1.1 Grid example
In a Markov random ﬁeld, each potential function φ[•] (or cost function ψ[•]) addresses
only a small subset of the variables. In this chapter, we will mainly be concerned with
pairwise Markov random ﬁelds in which the cliques (subsets) consist of only neighboring
pairs in a regular grid structure.
To see how the pairwise MRF can be used to encourage smoothness in an image,
consider the graphical model for a 2 × 2 image (Figure 12.2). Here, we have deﬁned
the probability Pr(w1...4) over the associated discrete states as a normalized product of
pairwise terms:
Pr(w) = 1
Z φ12(w1,w2)φ23(w2,w3)φ34(w3,w4)φ41(w4,w1),
(12.4)
where φmn(wm,wn) is a potential function that takes the two states wm and wn and
returns a positive number.
Let us consider the situation where the world state wn at each pixel is binary and so
takes a value of 0 or 1. The function φmn will now return four possible values depending
on which of the four conﬁgurations {00,01,10,11} of wm and wn is present. For sim-
plicity, we will assume that the functions φ12,φ23,φ34 and φ41 are identical and that for
each:
φmn(0,0) = 1.0
φmn(0,1) = 0.1
φmn(1,0) = 0.1
φmn(1,1) = 1.0.
(12.5)
Since there are only four binary states, we can calculate the constant Z explicitly by
computing the unnormalized probabilities for each of the 16 possible combinations and
taking the sum. The resulting probabilities for each of the 16 possible states are:
w1...4
Pr(w1...4)
w1...4
Pr(w1...4)
w1...4
Pr(w1...4)
w1...4
Pr(w1...4)
0000
0.47176
0100
0.00471
1000
0.00471
1100
0.00471
0001
0.00471
0101
0.00005
1001
0.00471
1101
0.00471
0010
0.00471
0110
0.00471
1010
0.00005
1110
0.00471
0011
0.00471
0111
0.00471
1011
0.00471
1111
0.47176
The potential functions in Equation 12.5 encourage smoothness: the functions φmn
return higher values when the neighbors take the same state and lower values when they
differ, and this is reﬂected in the resulting probabilities.

230
12 Models for grids
Figure 12.3 Samples from Markov random
ﬁeld prior. Four samples from the MRF prior
which were generated using a Gibbs sam-
pling procedure (see Section 10.7.2). Each
sample is a binary image that is smooth
almost everywhere; there are only very occa-
sional changes from black to white and vice-
versa. This prior encourages smooth solu-
tions (like the original image in the denoising
problems) and discourages isolated changes
in label (as are present in the noise).
We can visualize this by scaling this model up to a larger image-sized grid where there
is one node per pixel and drawing samples from the resulting probability distribution
(Figure 12.3). The resulting binary images are mostly smooth, with only occasional
changes between the two values.
It should be noted that for this more realistically sized model, we cannot compute the
normalizing constant Z by brute force as for the 2 × 2 case. For example, with 10000
pixels each taking a binary value, the normalizing constant is the sum of 210000 terms.
In general we will have to cope with only knowing the probabilities up to an unknown
scaling factor.
12.1.2 Image denoising with discrete pairwise MRFs
Now we will apply the pairwise Markov random ﬁeld model to the denoising task. Our
goal is to recover the original image pixel values from the observed noisy image.
More precisely, the observed image x = {x1,x2,...,xN} is assumed to consist of
discrete variables where the different possible values (labels) represent different intensi-
ties. Our goal is to recover the original uncorrupted image w = {w1,w2,...,wN}, which
also consists of discrete variables representing the intensity. We will initially restrict our
discussion to generative models and compute the posterior probability over the unknown
world state w using Bayes’ rule
Pr(w1...N|x1...N) =
QN
n=1 Pr(xn|wn)Pr(w1...N)
Pr(x1...N)
,
(12.6)
where we have assumed that the conditional probability Pr(x1...N|w1...N) factorizes into
a product of individual terms associated with each pixel. We will ﬁrst consider denoising
binary images in which the noise process ﬂips the pixel polarity with probability ρ so that
Pr(xn|wn = 0) = Bernxn[ρ]
Pr(xn|wn = 1) = Bernxn[1 −ρ].
(12.7)
We subsequently consider gray level denoising where the observed pixel is replaced with
probability ρ by a draw from a uniform distribution.

12.2
MAP inference for binary pairwise MRFs
231
Figure
12.4 Denoising model.
The
observed data xn at pixel n is conditionally
dependent on the associated world state wn
(red directed edges). Each world state wn
has undirected edges to its four-connected
neighbors (blue undirected edges).
This
is hence a mixed model: it contains both
directed and undirected elements. Together
the world states are connected in a Markov
random ﬁeld with cliques that consist
of neighboring pairs of variables.
For
example variable w5 contributes to cliques
C25,C45,C65,C85.
We now deﬁne a prior that encourages the labels wn to be smooth: we want them
to mostly agree with the observed image, but to discourage conﬁgurations with isolated
changes in label. To this end, we model the prior as a pairwise MRF. Each pair of
four-connected neighboring pixels contributes one clique so that
Pr(w1...N) = 1
Z exp

−
X
(m,n)∈C
ψ[wm,wn,θ]

,
(12.8)
where we have assumed that the clique costs ψ[•] are the same for every (wm,wn). The
parameters θ deﬁne the costs ψ[•] for each combination of neighboring pairwise values,
ψ[wm = j,wn = k,θ] = θjk,
(12.9)
so when the ﬁrst variable wm in the clique takes label j and the second variable wn takes
label k we pay a price of θjk. As before, we will choose these values so that there is a
small cost when neighboring labels are the same (so θ00 and θ11 are small) and a larger
one when the neighboring labels differ (so θ01 and θ10 are large). This has the effect of
encouraging solutions that are mostly smooth.
The associated graphical model is illustrated in Figure 12.4. It is a mixed model,
containing both directed and undirected links. The likelihood terms (equation 12.7) con-
tribute the red directed links between the observed data and the denoised image at each
pixel, and the MRF prior (Equation 12.8) contributes the blue grid that connects the
pixels together.
12.2
MAP inference for binary pairwise MRFs
To denoise the image, we estimate the variables {wn}N
n=1 using MAP inference; we
aim to ﬁnd the set of world states {wn}N
n=1 that maximizes the posterior probability

232
12 Models for grids
Pr(w1...N|x1...N) so that
ˆw1...N = argmax
w1...N
[Pr(w1...N|x1...N)]
= argmax
w1...N
" N
Y
n=1
Pr(xn|wn)Pr(w1...N)
#
= argmax
w1...N
" N
X
n=1
log[Pr(xn|wn)] + log[Pr(w1...N)]
#
,
(12.10)
where we have applied Bayes’ rule and transformed to the log domain. Because the prior
is an MRF with pairwise connections, we can express this as
ˆw1...N = argmax
w1...N


N
X
n=1
log[Pr(xn|wn)] −
X
(m,n)∈C
ψ[wm,wn,θ]


= argmin
w1...N


N
X
n=1
−log[Pr(xn|wn)] +
X
(m,n)∈C
ψ[wm,wn,θ]


= argmin
w1...N


N
X
n=1
Un(wn) +
X
(m,n)∈C
Pmn(wm,wn)

,
(12.11)
where Un(wn) denotes the unary term at pixel n. This is a cost for observing the data at
pixel n given state wn and is the negative log-likelihood term. Similarly, Pmn(wm,wn)
denotes the pairwise term. This is a cost for placing labels wm and wn at neighboring
locations m and n and is due to the clique costs ψ[wm,wn,θ] from the MRF prior. Note
that we have omitted the term −log[Z] from the MRF deﬁnition as it is constant with
respect to the states {wn}N
n=1 and hence does not affect the optimal solution.
The cost function in Equation 12.11 can be optimized using a set of techniques known
collectively as graph cuts. We will consider three cases:
• Binary MRFs (i.e., wi ∈{0,1}) where the costs for different combinations of adja-
cent labels are “submodular” (we will explain what this means later in the chapter).
Exact MAP inference is tractable here.
• Multilabel MRFs (i.e., wi ∈{1,2,...,K}) where the costs are “submodular.” Once
more, exact MAP inference is possible.
• Multilabel MRFs where the costs are more general.
Exact MAP inference is
intractable, but good approximate solutions can be found in some cases.
To solve these MAP inference tasks, we will translate them into the form of maximum
ﬂow (or max-ﬂow) problems. Max-ﬂow problems are well-studied, and exact polynomial
time algorithms exist. In the following section, we describe the max-ﬂow problem and its
solution. In subsequent parts of the chapter, we describe how to translate MAP inference
in Markov random ﬁelds into a max-ﬂow problem.

12.2
MAP inference for binary pairwise MRFs
233
Figure 12.5 Max-ﬂow problem: we are given a network of vertices connected by directed edges,
each of which has a nonnegative capacity cmn. There are two special vertices s and t termed the
source and sink, respectively. In the max-ﬂow problem, we seek to push as much ‘ﬂow’ from
source to sink while respecting the capacities of the edges.
12.2.1 Max-ﬂow/Min-cut
Consider a graph G = (V,E) with vertices V and directed edges E connecting them
(Figure 12.5). Each edge has a nonnegative capacity so that the edge between vertices
m and n has capacity cmn. Two of the vertices are treated as special and are termed the
source and the sink.
Consider transferring some quantity (“ﬂow”) through the network from the source to
the sink. The goal of the max-ﬂow algorithm is to compute the maximum amount of ﬂow
that can be transferred across the network without exceeding any of the edge capacities.
When the maximum possible ﬂow is being transferred – the so-called max-ﬂow solu-
tion – every path from source to sink must include a saturated edge (one where the
capacity is reached). If not, then we could push more ﬂow down this path, and so by
deﬁnition this is not the maximum ﬂow solution.
It follows that an alternate way to think about the problem is to consider the edges
that saturate. We deﬁne a cut on the graph to be a minimal set of edges that separate
the source from the sink. In other words, when these edges are removed, there is no
path from the source to the sink. More precisely, a cut partitions the vertices into two
groups: vertices that can be reached by some path from the source, but cannot reach
the sink, and vertices that cannot be reached from the source, but can reach the sink via
some path. For short, we will refer to a cut as “separating” the source from the sink.
Every cut is given an associated cost, which is the sum of the capacities of the excised
edges.
Since the saturated edges in the max-ﬂow solution separate the source from the sink,
they form a cut. In fact, this particular choice of cut has the minimum possible cost
and is referred to as the min-cut solution. Hence, the maximum ﬂow and minimum cut
problems can be considered interchangeably.

234
12 Models for grids
Figure 12.6 Augmenting paths algorithm for max-ﬂow. The numbers attached to the edges cor-
respond to current ﬂow/capacity. a) We choose any path from source to sink with spare capacity
and push as much ﬂow as possible along this path. The edge with the smallest capacity (here edge
6-t) saturates. b) We then choose another path where there is still spare capacity and push as much
ﬂow as possible. Now edge 6-5 saturates. c–e) We repeat this until there is no path from source to
sink that does not contain a saturated edge. The total ﬂow pushed is the maximum ﬂow. f) In the
min-cut problem, we seek a set of edges that separate the source from the sink and have minimal
total capacity. The min-cut (dashed line) consists of the saturated edges in the max-ﬂow problem.
In this example, the paths were chosen arbitrarily, but to ensure that this algorithm converges in the
general case, we should choose the remaining path with the greatest capacity at each step.
Augmenting paths algorithm for maximum ﬂow
There are many algorithms to compute the maximum ﬂow, and to describe them properly
is beyond the scope of this volume. However, for completeness, we present a sketch of
the augmenting paths algorithm (Figure 12.6).
Consider choosing any path from the source to the sink and pushing the maximum
possible amount of ﬂow along it. This ﬂow will be limited by the edge on that path that

12.2
MAP inference for binary pairwise MRFs
235
has the smallest capacity that will duly saturate. We remove this amount of ﬂow from
the capacities of all of the edges along the path, causing the saturated edge to have a new
capacity of zero. We repeat this procedure, ﬁnding a second path from source to sink,
pushing as much ﬂow as possible along it and updating the capacities. We continue this
process until there is no path from source to sink without at least one saturated edge. The
total ﬂow that we have transferred is the maximum ﬂow, and the saturated edges form
the minimum cut.
In the full algorithm, there are some extra complications: for example, if there is
already some ﬂow along edge i−j, it may be that there is a remaining path from source to
sink that includes the edge j−i. In this situation, we reduce the ﬂow in i−j before adding
ﬂow to j−i. The reader should consult a specialized text on graph-based algorithms for
more details.
If we choose the path with the greatest remaining capacity at each step, the algorithm
is guaranteed to converge and has complexity O(|E|2|V|) where |E| is the number of
edges and |V| the number of vertices in the graph. From now on we will assume that
the max-ﬂow/min-cut problem can be solved and concentrate on how to convert MAP
estimation problems with MRFs into this form.
12.2.2 MAP inference: binary variables
Recall that to ﬁnd the MAP solution we must ﬁnd
12.1
ˆw1...N = argmin
w1...N


N
X
n=1
Un(wn) +
X
(m,n)∈C
Pmn(wm,wn)

,
(12.12)
where Un(wn) denotes the unary term and Pmn(wm,wn) denotes the pairwise term.
For pedagogical reasons, we will ﬁrst consider cases where the unary terms are
positive and the pairwise terms have the following zero-diagonal form
Pmn(0,0) = 0
Pmn(1,0) = θ10
Pmn(0,1) = θ01
Pmn(1,1) = 0,
where θ01,θ10 > 0. We discuss the more general case later in this section.
The key idea will be to set up a directed graph G = {V,E} and attach weights to the
edges, so that the minimum cut on this graph corresponds to the maximum a posteriori
solution. In particular, we construct a graph with one vertex per pixel, and a pair of
directed edges between adjacent vertices in the pixel grid. In addition, there is a directed
edge from the source to every vertex and a directed edge from every vertex to the sink
(Figure 12.7).
Now consider a cut on the graph. In any cut we must either remove the edge that
connects the source to a pixel vertex, or the edge that connects the pixel vertex to the
sink, or both. If we do not do this, then there will still be a path from source to sink
and it is not a valid cut. For the minimum cut, we will never cut both (assuming the
general case where the two edges have different capacities) – this is unnecessary and will
inevitably incur a greater cost than cutting one or the other. We will label pixels where
the edge to the source was cut as wn = 0 and pixels where the edge to the sink was

236
12 Models for grids
Figure 12.7 Graph structure for ﬁnding MAP solution for a MRF with binary labels and pairwise
connections in a 3 × 3 image. There is one vertex per pixel, and neighbors in the pixel grid are
connected by reciprocal pairs of directed edges. Each pixel vertex receives a connection from the
source and sends a connection to the sink. To separate source from sink, the cut must include one
of these two edges for each vertex. The choice of which edge is cut will determine which of two
labels is assigned to the pixel.
cut as having label wn = 1. So each plausible minimum cut is associated with a pixel
labeling.
Our goal is now to assign capacities to the edges, so the cost of each cut matches
the cost of the associated labeling as prescribed in Equation 12.12. For simplicity, we
illustrate this with a 1D image with three pixels (Figure 12.8), but we stress that all the
ideas are also valid for 2D images and higher dimensional constructions.
We attach the unary costs Un(0) and Un(1) to the edges from the pixel to the source
and sink, respectively. If we cut the edge from the source to a given pixel (and hence
assign wn = 0), we pay the cost Un(0). Conversely, if we cut the edge from the pixel to
the sink (and hence assign wn = 1), we pay the cost Un(1).
We attach the pairwise costs Pmn(1,0) and Pmn(0,1) to the pairs of edges between
adjacent pixels. Now if one pixel is attached to the source and the other to the sink, we
pay either Pmn(0,1) = θ01 or Pmn(1,0) = θ10 as appropriate to separate source from
sink. The cuts corresponding to all eight possible conﬁgurations of the three-pixel model
and their costs are illustrated in Figure 12.9.
Any cut on the graph in which each pixel is either separated from the source or the
sink now has the appropriate cost from Equation 12.12. It follows that the minimum
cut on this graph will have the minimum cost and the associated labeling w1...N will
correspond to the maximum a posteriori solution.
General pairwise costs
Now let us consider how to use the more general pairwise costs:
Pmn(0,0) = θ00
Pmn(1,0) = θ10
Pmn(0,1) = θ01
Pmn(1,1) = θ11.
(12.13)

12.2
MAP inference for binary pairwise MRFs
237
t
Figure 12.8 Graph construction for binary
MRF with diagonal pairwise terms using
simple 1D example. After the cut, vertices
attached to the source are given label 1 and
vertices attached to the sink are given label
0. We hence attach the appropriate unary
costs to the links between the sink/source
and the pixel vertices. The pairwise costs
are attached to the horizontal links between
pixels as shown. This arrangement ensures
that the correct cost is paid for each of the
eight possible solutions (Figure 12.9).
To illustrate this, we use an even simpler graph with only two pixels (Figure 12.10).
Notice that we have added the pairwise cost Pab(0,0) to the edge s −b. We will have to
pay this cost appropriately in the conﬁguration where wa = 0 and wb = 0. Unfortunately,
we would also pay it in the case where wa = 1 and wb = 0. Hence, we subtract the same
cost from the edge a −b, which must also be cut in this solution. By a similar logic, we
add Pab(1,1) to the edge a −t and subtract it from edge a −b. In this way, we associate
the correct costs with each labeling.
Reparameterization
The preceding discussion assumed that the edge costs are all nonnegative and can be valid
12.2 capacities in the max-ﬂow problem. If they are not, then it is not possible to compute
the MAP solution. Unfortunately, it is often the case that they are negative; even if the
original unary and pairwise terms were positive, the edge a−b in Figure 12.10 with
cost Pab(1,0) −Pab(1,1) −Pab(0,0) could be negative. The solution to this problem is
reparameterization.
The goal of reparameterization is to modify the costs associated with the edges in the
graph in such a way that the MAP solution is not changed. In particular, we will adjust
the edge capacities so that every possible solution has a constant cost added to it. This
does not change which solution has the minimum cost, and so the MAP labeling will be
unchanged.
We consider two reparameterizations (Figure 12.11). First, consider adding a con-
stant cost α to the edge from a given pixel to the source and the edge from the same pixel
to the sink. Since any solution cuts exactly one of these edges, the overall cost of every
solution increases by α. We can use this to ensure that none of the edges connecting
the pixels to the source and sink have negative costs: we simply add a sufﬁciently large
positive value α to make them all nonnegative.

238
12 Models for grids
Figure 12.9
Eight possible solutions for the three-pixel example. When we set the costs as in
Figure 12.8, each solution has the appropriate cost. a) For example, the solution (a = 0, b = 0,
c = 0) requires us to cut edges s−a, s−b, s−c and pay the cost Ua(0)+Ub(0)+Uc(0). b) For
the solution (a = 0, b = 0, c = 1) we must cut edges s−a, s−b, c−t, and c−b (to prevent ﬂow
through the path s −c −b −t). This incurs a total cost of Ua(0) + Ub(0) + Uc(1) + Pbc(0,1). c)
Similarly, in this example with (a = 0,b = 1,c = 0), we pay the appropriate cost Ua(0)+Ub(1)+
Uc(0) + Pab(0,1) + Pbc(1,0). d–h) The other ﬁve possible conﬁgurations.
A more subtle type of reparameterization is illustrated in Figure 12.11c. By changing
the costs in this way, we increase the cost of each possible solution by β. For example,
in the assignment (wa =0,wb =1), we must cut the links s−a, b−a, and b−t giving a
total cost of Ua(0) + Ub(1) + Pab(0,1) + β.
Applying the reparameterization in Figure 12.11c to the general construction in
Figure 2.10, we must ensure that the capacities on edges between pixel nodes are
nonnegative so that
θ10 −θ11 −θ00 −β ≥0
(12.14)
θ01 + β ≥0.
(12.15)

12.3
MAP inference for multilabel pairwise MRFs
239
Figure 12.10 Graph structure for general
(i.e., non-diagonal) pairwise costs. Consider
the solution (a = 0,b = 0). We must break
the edges s−a and s−b giving a total cost
of Ua(0) + Ub(0) + Pab(0,0). For the solu-
tion (a = 1,b = 0), we must break the edges
a −t,a −b, and s −b giving a total cost
of Ua(1) + Ub(0) + Pab(1,0).
Similarly,
the cuts corresponding to the solutions (a =
0,b = 1) and (a = 1,b = 1) on this graph
have pairwise costs Pab(0,1) and Pab(1,1),
respectively.
Adding these equations together, we can eliminate β to get a single inequality
θ01 + θ10 −θ11 −θ00 ≥0.
(12.16)
If this condition holds, the problem is termed submodular, and the graph can be repa-
rameterized to have only nonnegative costs. It can then be solved in polynomial time
using the max-ﬂow algorithm. If the condition does not hold, then this approach cannot
be used, and in general the problem is NP hard. Fortunately, the former case is common
for vision problems; we generally favor smooth solutions where neighboring labels are
the same and hence the costs θ01,θ10 for labels differing are naturally greater than the
costs θ00,θ11 for the labels agreeing.
Figure 12.12 shows the MAP solutions to the binary denoising problem with an MRF
prior as we increase the strength of the cost for having adjacent labels that differ. Here we
have assumed that the costs for adjacent labels being different are the same (θ01 = θ10)
and that there is no cost when neighboring labels are the same (θ00,θ11 = 0); we are in the
“zero-diagonal” regimen. When the MRF costs are small, the solution is dominated by
the unary terms and the MAP solution looks like the noisy image. As the costs increase,
the solution ceases to tolerate isolated regions, and most of the noise is removed. When
the costs become larger, details such as the center of the “0” in “10” are lost and even-
tually nearby regions are connected together. With very high pairwise costs, the MAP
solution is a uniform ﬁeld of labels: the overall cost is dominated by the pairwise terms
from the MRF and the unary terms merely determine the polarity.
12.3
MAP inference for multilabel pairwise MRFs
We now investigate MAP inference using MRF priors with pairwise connections when
12.3 the world state wn at each pixel can take multiple labels {1,2,...,K}. To solve the
multilabel problem, we change the graph construction (Figure 12.13a). With K labels
and N pixels, we introduce (K+1)N vertices into the graph.
For each pixel, the K + 1 associated vertices are stacked. The top and bottom of the
stack are connected to the source and sink by edges with inﬁnite capacity. Between the

240
12 Models for grids
Figure 12.11 Reparameterization.
a) Original graph construction.
b) Reparameterization 1.
Adding a constant cost α to the connections from a pixel vertex to both the source and sink results
in a problem with the same MAP solution. Since we must cut either, but not both of these edges,
every solution increases in cost by α, and the minimum cost solution remains the same. c) Repa-
rameterization 2. Manipulating the edge capacities in this way results in a constant β being added
to every solution and so the choice of minimum cost solution is unaffected.
Figure 12.12 Denoising results. a) Observed noisy image. b–h) Maximum a posteriori solution
as we increase zero-diagonal pairwise costs. When the pairwise costs are low, the unary terms
dominate and the MAP solution is the same as the observed image. As the pairwise costs increase,
the image gets more and more smooth until eventually it becomes uniform.
K +1 vertices in the stack are K edges forming a path from source to sink. These edges
are associated with the K unary costs Un(1)...Un(K). To separate the source from the
sink, we must cut at least one of the K edges in this chain. We will interpret a cut at
the kth edge in this chain as indicating that the pixel takes label k and this incurs the
appropriate cost of Un(k).

12.3
MAP inference for multilabel pairwise MRFs
241
Figure 12.13 a) Graph setup for multilabel case for two pixels (a,b) and four labels (1,2,3,4).
There is a chain of ﬁve vertices associated with each pixel. The four vertical edges between these
vertices are assigned the unary costs for the four labels. The minimum cut must break this chain to
separate source from sink, and the label is assigned according to where the chain is broken. Vertical
constraint edges of inﬁnite capacity run between the four vertices in the opposite direction. There
are also diagonal edges between the ith vertex of pixel a and the jth vertex of pixel b with assigned
costs Cab(i,j) (see text). b) The vertical constraint edges prevent solutions like this example with
three pixels. Here, the chain of vertices associated with the central pixel is cut in more than one
place and so the labeling has no clear interpretation. However, for this to happen a constraint link
must be cut and hence this solution has an inﬁnite cost.
To ensure that only a single edge from the chain is part of the minimum cut (and hence
that each cut corresponds to one valid labeling), we add constraint edges. These are edges
of inﬁnite capacity that are strategically placed to prevent certain cuts occurring. In this
case, the constraint edges connect the vertices backwards along each chain. Any cut
that crosses the chain more than once must cut one of these edges and will never be the
minimum cut solution (Figure 12.13b).
In Figure 12.13a, there are also diagonal interpixel edges from the vertices associated
with pixel a to those associated with pixel b. These are assigned costs Cab(i,j), where i
indexes the vertex associated with pixel a and j indexes the vertex associated with pixel b.
We choose the edge costs to be
Cab(i,j) = Pab(i,j −1) + Pab(i −1,j) −Pab(i,j) −Pab(i −1,j −1),
(12.17)
where we deﬁne any superﬂuous pairwise costs associated with the nonexistent labels 0
or K + 1 to be zero, so that
Pab(i,0) = 0
Pab(i,K + 1) = 0
∀i ∈{0...K + 1}
Pab(0,j) = 0
Pab(K + 1,j) = 0
∀j ∈{0...K + 1}.
(12.18)

242
12 Models for grids
Figure 12.14 Example cuts for multilabel case. To separate the source and sink, we must cut all
of the links that pass from above the chosen label for pixel a to below the chosen label for pixel b.
a) Pixel a is set to label 1 and pixel b is set to label 3 meaning we must cut the links from vertex
a1 to nodes b4 and b5. b) Pixel a takes label 4 and pixel b takes label 4.
When label I is assigned to pixel a and label J to pixel b, we must cut all of the links
from vertices a1 ...aI to the vertices bJ+1 ...bK+1 to separate the source from the sink
(Figure 12.14). So, the total cost due to the interpixel edges for assigning label I to pixel
a and label J to pixel b is
I
X
i=1
K+1
X
j=J+1
Cab(i,j) =
I
X
i=1
K+1
X
j=J+1
Pab(i,j−1)+Pab(i−1,j)−Pab(i,j)−Pab(i−1,j−1)
= Pab(I,J)+Pab(0,J)−Pab(I,K + 1)−Pab(0,K + 1)
= Pab(I,J).
(12.19)
Adding the unary terms, the total cost is Ua(I) + Ub(J) + Pab(I,J) as required.
Once more, we have implicitly made the assumption that the costs associated with
edges are nonnegative. If the vertical (intrapixel) edge terms have negative costs, it is

12.3
MAP inference for multilabel pairwise MRFs
243
Figure 12.15 Reparameterization for multilabel graph cuts.
The original construction (a) is
equivalent to construction (b). The label at pixel b determines which edges that leave node a1
are cut. Hence, we can remove these edges and add the extra costs to the vertical links associated
with pixel b. Similarly, the costs of the edges passing into node b5 can be added to the vertical
edges associated with pixel a. If any of the resulting vertical edges associated with a pixel are
negative, we can add a constant α to each: since exactly one is broken, the total cost increases by
α, but the MAP solution remains the same.
possible to reparameterize the graph by adding a constant α to all of the unary terms.
Since the ﬁnal cost includes exactly one unary term per pixel, every possible solution
increases by α and the MAP solution is unaffected.
The diagonal interpixel edges are more problematic. It is possible to remove the edges
that leave node a1 and the edges that arrive at bK+1 by adding terms to the intrapixel
edges associated with the unary terms (Figure 12.15). These intrapixel edges can then be
reparameterized as described above if necessary. Unfortunately, we can neither remove
nor reparameterize the remaining interpixel edges so we require that
Cab(i,j) = Pab(i,j −1) + Pab(i −1,j) −Pab(i,j) −Pab(i −1,j −1) ≥0.
(12.20)
By mathematical induction, we get the more general result (Figure 12.16),
Pab(β,γ) + Pab(α,δ) −Pab(β,δ) −Pab(α,γ) ≥0,
(12.21)
where α,β,γ,δ are any four values of the state y such that β > α and δ > γ. This is the
multilabel generalization of the submodularity condition (Equation 12.16). An important
class of pairwise costs that are submodular are those that are convex in the absolute differ-
ence |wi −wj| between the labels at adjacent pixels (Figure 12.17a). Here, smoothness
is encouraged; the penalty becomes increasingly stringent as the jumps between labels
increase.

244
12 Models for grids
Figure 12.16 Submodularity constraint for
multilabel case.
Color at position (m,n)
indicates pairwise costs Pab(m,n). For all
edges in the graph to be positive, we require
that the pairwise terms obey Pab(β,γ) +
Pab(α,δ)−Pab(β,δ)−Pab(α,γ) ≥0 for all
α,β,γ,δ such that β > α and δ > γ. In other
words, for any four positions arranged in a
square conﬁguration as in the ﬁgure, the sum
of the two costs on the diagonal from top-left
to bottom-right must be less than the sum on
the off diagonal. If this condition holds, the
problem can be solved in polynomial time.
12.4
Multilabel MRFs with non-convex potentials
Unfortunately, convex potentials are not always appropriate. For example, in the denois-
ing task we might expect the image to be piecewise smooth: there are smooth regions
(corresponding to objects) followed by abrupt jumps (corresponding to the boundaries
between objects). A convex potential function cannot describe this situation because it
penalizes large jumps much more than smaller ones. The result is that the MAP solution
smooths over the sharp edges changing the label by several smaller amounts rather than
one large jump (Figure 12.18).
To solve this problem, we need to work with interactions that are non-convex in the
absolute label difference, such as the truncated quadratic function or the Potts model
(Figures 12.17b–c). These favor small changes in the label and penalize large changes
equally or nearly equally. This reﬂects the fact that the exact size of an abrupt jump
in label is relatively unimportant. Unfortunately, these pairwise costs do not satisfy the
submodularity constraint (Equation 12.21). Here, the MAP solution cannot in general
be found exactly with the method described previously, and the problem is NP-hard.
Fortunately, there are good approximate methods for optimizing such problems, one of
which is the alpha-expansion algorithm.
12.4.1 Inference: alpha-expansion
The alpha-expansion algorithm works by breaking the solution down into a series of
12.4 binary problems, each of which can be solved exactly. At each iteration, we choose one
label value α, and for each pixel we consider either retaining the current label, or switch-
ing it to α. The name “alpha-expansion” derives from the fact that the space occupied by
label α in the solution expands at each iteration (Figure 12.19). The process is iterated
until no choice of α causes any change. Each expansion move is guaranteed to lower
the overall objective function, although the ﬁnal result is not guaranteed to be the global
minimum.

12.4
Multilabel MRFs with non-convex potentials
245
Figure 12.17 Convex vs. non-convex potentials. The method for MAP inference for multival-
ued variables depends on whether the costs are a convex or non-convex function of the difference
in labels. a) Quadratic function (convex), Pmn(wm,wn) = κ(wm −wn)2. For convex functions,
it is possible to draw a chord between any two points on the function without intersecting the
function elsewhere (e.g., dotted blue line).
b) Truncated quadratic function (non-convex),
Pmn(wm,wn) = min(κ1,κ2(wm −wn)2).
c) Potts model (non-convex), Pmn(wm,wn) =
κ(1 −δ(wm −wn)).
Figure 12.18 Denoising results with convex (quadratic) pairwise costs. a) Noisy observed image.
b) Denoised image has artifacts where there are large intensity changes in the original image.
Convex costs imply that there is a lower cost for a number of small changes rather than a single
large one.
For the alpha-expansion algorithm to work, we require that the edge costs form a
metric. In other words, we require that
P(α,β) = 0 ⇔α = β
P(α,β) = P(β,α) ≥0
P(α,β) ≤P(α,γ) + P(γ,β).
(12.22)
These assumptions are reasonable for many applications in vision, and allow us to model
non-convex priors.

246
12 Models for grids
a) 
b) 
c) 
d) 
Figure 12.19 The alpha-expansion algo-
rithm breaks the problem down into a series
of binary subproblems.
At each step, we
choose a label α and we expand: for each
pixel we either leave the label as it is or
replace it with α.
This subproblem is
solved in such a way that it is guaran-
teed to decrease the multilabel cost func-
tion.
a) Initial labeling.
b) Orange label
is expanded:
each label stays the same
or becomes orange.
c) Yellow label is
expanded. d) Red label is expanded.
In the alpha-expansion graph construction (Figure 12.20), there is one vertex asso-
ciated with each pixel. Each of these vertices is connected to the source (representing
keeping the original label or α) and the sink (representing the label α). To separate
source from sink, we must cut one of these two edges at each pixel. The choice of edge
will determine whether we keep the original label or set it to α. Accordingly, we asso-
ciate the unary costs for each edge being set to α or its original label with the two links
from each pixel. If the pixel already has label α, then we set the cost of being set to α
to ∞.
The remaining structure of the graph is dynamic: it changes at each iteration depend-
ing on the choice of α and the current labels. There are four possible relationships
between adjacent pixels:
• Pixel i has label α and the pixel j has label α. Here, the ﬁnal conﬁguration is
inevitably α−α, and so the pairwise cost is zero and there is no need to add further
edges connecting nodes i and j in the graph. Pixels a and b in Figure 12.20 have
this relationship.
• The ﬁrst pixel has label α but the second pixel has a different label β. Here the ﬁnal
solution may be α−α with zero cost or α−β with pairwise cost Pij(α,β). Here
we add a single edge connecting pixel j to pixel i with pairwise cost Pij(α,β).
Pixels b and c in Figure 12.20 have this relationship.
• Both pixels i and j have the same label β. Here the ﬁnal solution may be α−α
with zero pairwise cost, β−β with zero pairwise cost, α−β with pairwise cost
Pij(α,β) or β−α with pairwise cost Pij(β,α). We add two edges between the
pixels representing the two nonzero pairwise costs. Pixels c and d in Figure 12.20
have this relationship.
• Pixel i has label β and pixel j has a second label γ. Here the ﬁnal solution may
be α−α with zero pairwise cost, β−γ with pairwise cost Pij(β,γ), β−α with
pairwise cost Pij(β,α), or α−γ with pairwise cost Pij(α,γ). We add a new
vertex k between vertices i and j and add the three nonzero pairwise costs to edges
k −α, i−k, and j −k, respectively. Pixels d and e in Figure 12.20 have this
relationship.
Three example cuts are shown in Figure 12.21.

12.5
Conditional random ﬁelds
247
Figure 12.20 Alpha-expansion graph setup. Each pixel node (a,b,c,d,e) is connected to the source
and the sink by edges with costs U•(α) and U•(α), respectively. In the minimum cut, exactly one
of these links will be cut. The nodes and vertices describing the relationship between neighboring
pixels depend on their current labels, which may be α−α as for pixels a and b, α−β as for pixels
b and c, β−β as for pixels c and d or β−γ as for pixels d and e. For the last case, an auxiliary
node k must be added to the graph.
Note that this construction critically relies on the triangle inequality (Equation 12.22).
For example, consider pixels d and e in Figure 12.21a. If the triangle inequality does not
hold so that Pde(β,γ) > Pde(β,α) + Pde(α,γ), then the wrong costs will be assigned;
rather than the link k−α, the two links d−k and e−k will both be cut, and the wrong cost
will be used. In practice, it is sometimes possible to ignore this constraint by truncating
the offending cost Pij(β,γ) and running the algorithm as normal. After the cut is done,
the true objective function (sum of the unary and pairwise costs) can be computed for the
new label map and the answer accepted if the cost has decreased.
It should be emphasized that although each step optimally updates the objective func-
tion with respect to expanding α, this algorithm is not guaranteed to converge to the
overall global minimum. However, it can be proven that the result is within a factor of
two of the minimum and often it behaves much better.
Figure 12.22 shows an example of multilabel denoising using the alpha-expansion
algorithm. On each iteration, one of the labels is chosen and expanded, and the appro-
priate region is denoised. Sometimes the label is not supported at all by the unary costs
and nothing happens. The algorithm terminates when no choice of α causes any further
change.
12.5
Conditional random ﬁelds
In the models presented in this chapter, the Markov random ﬁelds have described the
prior Pr(w) in a generative model of the image data. We could alternatively describe
the joint probability distribution Pr(w,x) with the undirected model
Pr(w,x) = 1
Z exp
"
−
X
c
ψC[w] −
X
d
ζ[w,x]
#
,
(12.23)

248
12 Models for grids
Figure 12.21 Alpha-expansion algorithm. a–c) Example cuts on this graph illustrate that the
appropriate unary and pairwise costs are always paid.

12.5
Conditional random ﬁelds
249
Figure 12.22 Alpha-expansion algorithm for denoising task. a) Observed noisy image. b) Label
1 (black) is expanded, removing noise from the hair. c-f) Subsequent iterations in which the labels
corresponding to the boots, trousers, skin and background are expanded, respectively.
where the functions ψ[•] encourage certain conﬁgurations of the label ﬁeld and the
functions ζ[•,•] encourage agreement between the data and the label ﬁeld.
If we
now condition on the data (i.e., assume that it is ﬁxed), then we can use the relation
Pr(w|x) ∝Pr(w,x) to write
Pr(w|x) = 1
Z2
exp
"
−
X
c
ψC[w] −
X
d
ζ[w,x]
#
,
(12.24)
where Z2 = Z Pr(x). This discriminative model is known as a conditional random ﬁeld
or CRF.
We can choose the functions ζ[•,•] so that they each determine the compatibility of
one label wn to its associated measurement xn. If the functions ψ[•] are used to encour-
age smoothness between neighboring labels, then the negative log posterior probability
will again be the sum of unary and pairwise terms. The maximum a posteriori labels ˆw

250
12 Models for grids
Figure 12.23 Graphical model for con-
ditional random ﬁeld (compare to Figure
12.4).
The posterior probability of the
labels w is a Markov random ﬁeld for ﬁxed
data x.
In this model, the two sets of
cliques relate (i) neighboring labels and (ii)
each label to its associated measurement.
Since this model only includes unary and
pairwise interactions between the labels,
the unknown labels {wn}N
n=1 can be opti-
mized using graph cut techniques.
can hence be found by minimizing a cost function of the form
ˆw = argmin
w1...N


N
X
n=1
Un(wn) +
X
(m,n)∈C
Pmn(wm,wn)

,
(12.25)
and the graphical model will be as in Figure 12.23. This cost function can be minimized
using the graph cuts techniques described throughout this chapter.
12.6
Higher order models
The models that we have discussed so far have only connected immediate neighbors.
However, these only allow us to model relatively simple statistical properties of the label
ﬁeld. One way to improve this situation is to consider each variable wn ∈{1...K} as
representing the index of a square patch of labels from a predeﬁned library. The pairwise
MRF now encodes the afﬁnity of neighboring patches for each other. Unfortunately, the
resulting costs are less likely to be submodular, or even obey the triangle inequality, and
the number K of patches in the library is usually very large, making graph cut algorithms
inefﬁcient.
A second approach to modeling more complex statistical properties of the label ﬁeld
is to increase the number of the connections. For the undirected models (CRF, MRF) this
would mean introducing larger cliques. For example, to model local texture, we might
connect all of the variables in every 5×5 region of the image. Unfortunately, inference
is hard in these models; optimizing the resulting complex cost functions is still an open
research topic.
12.7
Directed models for grids
The Markov random ﬁeld and conditional random ﬁeld models are attractive because we
can use graph-cuts approaches to search for the MAP solution. However, they have the
drawback that it is very hard to learn the parameters of the model because they are based
on undirected models. An obvious alternative is to use a similar directed model (Figure
12.24). Here, learning is relatively easy, but it turns out that MAP inference using graph
cuts is not generally possible.

12.8
Applications
251
Figure 12.24 Directed graphical model
for grid. Although this model appears sim-
ilar to the pairwise Markov random ﬁeld
model, it represents a different factorization
of the joint probability.
In particular the
factorization contains terms involving three
variables such as Pr(w5|w2,w4).
This
means that the resulting cost function for
MAP inference is no longer amenable to
exact solution using graph-cut methods. In
this case, an attractive alternative is to use
sampling based methods as it is easy to
generate samples from this directed model.
To see this, consider the cost function for MAP inference in this model,
ˆw1...N = argmax
w1...N
[log[Pr(x1...N|w1...N)] + log[Pr(w1...N)]]
(12.26)
= argmax
w1...N
" N
X
n=1
log[Pr(xn|wn)] +
N
X
n=1
log[Pr(wn|wpa[n])]
#
= argmin
w1...N
" N
X
n=1
−log[Pr(xn|wn)] −
N
X
n=1
log[Pr(wn|wpa[n])]
#
,
where we have multiplied the objective function by minus one and now seek the
minimum. This minimization problem now has the general form
ˆw1...N = argmin
w1...N
" N
X
n=1
Un(wn) +
N
X
n=1
Tn(wn,wpa1[n],wpa2[n])
#
,
(12.27)
where Un(wn) is called a unary term reﬂecting the fact that it only depends on a single
element wn of the label ﬁeld and Tn(wn,wpa1[n],wpa2[n]) is called a three-wise term
reﬂecting the fact in general the label at a pixel is conditioned on the two parents pa1[n]
and pa2[n] above and to the left of the current position.
Notice that this cost function is fundamentally different from the cost function for
MAP inference in a pairwise MRF (Equation 12.11): it includes three-wise terms and
there is no known polynomial algorithm to optimize this criterion. However, since this
model is a directed graphical model, it is easy to generate samples from this model, and
this can be exploited for approximate inference methods such as computing the empirical
max marginals.
12.8
Applications
The models and algorithms in this chapter are used in a large number of computer
vision applications, including stereo vision, motion estimation, background subtraction,

252
12 Models for grids
interactive segmentation, semantic segmentation, image editing, image denoising, image
superresolution, and building 3D models. Here, we review a few key examples. We
consider background subtraction, which is a simple application with binary labels
and interactive segmentation which uses binary labels in a system that simultaneously
estimates the parameters in the likelihood terms. Then we consider stereo, motion esti-
mation, and image editing, all of which are multilabel graph cut problems. We consider
superresolution which is a multilabel problem where the units are patches rather than
pixels and which there are so many labels that the alpha-expansion algorithm is not suit-
able. Finally, we consider drawing samples from directed grid models to generate novel
images.
12.8.1 Background subtraction
First, let us revisit the background subtraction algorithm that we ﬁrst encountered in
Section 6.6.2. In background subtraction, the goal is to associate a binary label {wn}N
n=1
with each of the N pixels in the image, indicating whether this pixel belongs to the
foreground or background based on the observed RGB data {xn}N
n=1 at each pixel. When
the pixel is background (wn = 0), the data are assumed to be generated from a normal
distribution with known mean µn and covariance Σn. When the pixel is foreground
(wn = 1), a uniform distribution over the data are assumed so that
Pr(xn|w = 0) = Normxn[µn,Σn]
Pr(xn|w = 1) = κ,
(12.28)
where κ is a constant.
In the original description, we assumed that the models at each pixel were indepen-
dent, and when we inferred the labels, the results were noisy (Figure 12.25b). We now
place a Markov random ﬁeld prior over the binary labels where the pairwise cliques are
organized as a grid (as in most of the models in this chapter) and where the potential
functions encourage smoothness. Figure 12.25 illustrates the results of performing infer-
ence in this model using the graph-cuts algorithm. There are now far fewer isolated
foreground regions and fewer holes in the foreground object. The model has still erro-
neously discovered the shadow; a more sophisticated model would be required to deal
with this problem.
Figure 12.25 Background subtraction revisited. a) Original image. b) MAP solution of back-
ground subtraction model with independent pixels. The solution contains noise. c) MAP solution
of background subtraction model with Markov random ﬁeld prior. This smoothed solution has
eliminated most of the noise.

12.8
Applications
253
Figure 12.26 Grab Cut. a) The user draws a bounding box around the object of interest. b) The
algorithm segments the foreground from the background by alternating between building color
models and segmenting the image. c–d) A second example. e–f) Failure mode. This algorithm
does not segment “wiry” objects well as the pairwise costs for tracing around all the boundaries
are prohibitive. Adapted from Rother et al. (2005). c⃝2005 ACM.
12.8.2 Interactive segmentation (GrabCut)
The goal of interactive segmentation is to cut out the foreground object in a photo, based
on some input from the user (Figure 12.26). More precisely, we aim to associate a binary
label {wn}N
n=1 to each of the N pixels in the image, indicating whether this pixel belongs
to the foreground or background, based on the observed RGB data {xn}N
n=1 at each pixel.
However, unlike background subtraction, we do not have any prior knowledge of either
the foreground or the background.
In the GrabCut system of Rother et al. (2005) the likelihoods of observing the back-
ground (w = 0) and background (w = 1) are each modeled as a mixture of K Gaussians
so that
Pr(xn|w = j) =
K
X
k=1
λjkNormxn[µjk,Σjk],
(12.29)
and the prior over the labels is modeled as a pairwise connected Markov random ﬁeld
with the potentials chosen to encourage smoothness.
In this application, the image may have a wide variety of content, and so there is no
suitable training data from which to learn the parameters {λjk,µjk,Σjk}2,K
j=1,k=1 of the
foreground and background color models. However, we note that (i) if we knew the color
models, we could perform the segmentation via MAP inference with the graph cuts algo-
rithm and (ii) if we knew the segmentation, then we could compute the foreground and
background color models based on the pixels assigned to each category. This observation
leads to an alternating approach to inference in this model, in which the segmentation and
parameters are computed in turn until the system converges.
In the Grabcut algorithm, the user draws a bounding box around the desired object to
be segmented. This effectively deﬁnes a rough segmentation (pixels within the box are

254
12 Models for grids
foreground and pixels outside are background) from which the system is initialized. If
the segmentation is not correct after the alternating optimization algorithm converges, the
user may “paint” regions of the image with a foreground or background brush, indicating
that these must belong to the appropriate class in the ﬁnal solution. In practice, this
means that the unary costs are set to ensure that these take the appropriate values, and the
alternating solution is run again from this point until convergence. Example results are
shown in Figure 12.26.
To improve the performance of this algorithm, it is possible to modify the MRF so
that the pairwise cost for changing from foreground to background label is less where
there is an edge in the image. This is referred to as using geodesic distance. From a
pure probabilistic viewpoint, this is somewhat dubious as the MRF prior should embody
what we know about the task before seeing the data, and hence cannot depend on the
image. However, this is largely a philosophical objection, and the method works well in
practice for a wide variety of objects. A notable failure mode is in segmenting “wiry”
objects such as trees. Here, the model is not prepared to pay the extensive pairwise
costs to cut exactly around the many edges of the object and so the segmentation is
poor.
12.8.3 Stereo vision
In stereo vision, the goal is to infer a discrete multivalued label {wn}N
n=1 representing
the disparity (horizontal shift) at each pixel in the image given the observed image data
{xn}N
n=1. More details about the likelihood terms in this problem can be found in Sec-
tion 11.8.2, where we described tree-based priors for the unknown disparities. A more
suitable approach is to use an MRF prior.
As for the denoising example, it is undesirable to use an MRF prior where the costs
are a convex function of the difference in neighboring labels. This results in a MAP solu-
tion where the edges of objects are smoothed. Hence, it is usual to use a non-convex
prior such as the Potts function, which embodies the idea that the scene consists of
smooth surfaces, with sudden jumps in depth between them where the size of the jump
is unimportant.
Boykov et al. (1999) used the alpha-expansion algorithm to perform approximate
inference in a model of this sort (Figure 12.27). The performance of this algorithm is
good, but errors are found where there is no true match in the other image (i.e., where
Figure 12.27 Stereo vision. a) One image of the original stereo pair. b) Disparity estimated using
the method of Boykov et al. (1999). c) Ground truth disparity. Blue pixels indicate regions which
are occluded in the second image and so do not have a valid match or disparity. The algorithm does
not take account of this fact and produces noisy estimates in these regions. Adapted from Boykov
et al. (1999).

12.8
Applications
255
the corresponding point is occluded by another object). Kolmogorov and Zabih (2001)
subsequently developed a bespoke graph for dealing with occlusions in stereo vision and
an alpha-expansion algorithm for optimizing the associated cost function. These methods
can also be applied to optical ﬂow in which we attempt to identify pixel correspondences
between adjacent frames in a video sequence. Unlike in stereo vision, there is no guar-
antee that these matches will be on the same scanline, but other than this, the problem is
very similar.
12.8.4 Rearranging Images
Markov random ﬁeld models can also be used for rearranging images; we are given an
original image I(1) and wish to create a new image I(2) by rearranging the pixels from
I(1) in some way. Depending on the application, we may wish to change the dimensions
of the original image (termed image retargeting), remove an object or move an object
from one place to another.
Pritch et al. (2009) constructed a model with variables w = {w1 ...wN} at each of
the N pixels of I(2). Each possible value of wn ∈{1...K} represents a 2D relative offset
to image I(1) that tells us which pixel from image I(1) will appear at the nth pixel of the
new image. The label map w is hence termed a shift map as it represents 2D shifts to
the original image. Each possible shift-map deﬁnes a different output image I(2) (Figure
12.28).
Pritch et al. (2009) model the shift-map w as an MRF with pairwise costs that encour-
age smoothness. The result of this is that only shift-maps that are piecewise constant have
high probability: in other words, new images, which consist of large chunks of the origi-
nal image that have been copied verbatim, are favored. They modify the pairwise costs so
that they are lower when adjacent labels encode offsets with similar surrounding regions.
This means that where the label does change, it does so in such a way that there is no
visible seam in the output image.
The remainder of the model depends on the application (Figure 12.29):
• To move an object, we specify unary costs in the new region that ensure that we
copy the desired object here. The remainder of the shifts are left free to vary but
a)
c)
d)
b)
Figure 12.28 Shift maps for image retargeting to reduce width. a) New image I(2) is created from
b) the original image I(1) by copying piecewise regions (ﬁve regions shown). c) These regions are
carefully chosen to produce a seamless result. d) The underlying representation is a shiftmap –
a label at each pixel of the new image that speciﬁes the 2D offset to the position in the original
image that will be copied from. An MRF encourages the labels to be piecewise constant, and hence
the result tends to consist of large chunks copied verbatim. Figure shows method of Pritch et al.
(2009).

256
12 Models for grids
Figure 12.29 Applications of shift maps. Shift maps can be used to a) take an object from the
original image b) move it to a new position and c) then ﬁll in the remaining pixels to produce a new
picture. d) They can also be used to remove an undesirable object e) speciﬁed by a mask from an
image by f) ﬁlling in the missing area. g–h) Finally they can be used to retarget an original image
to a smaller size, or i-j) to retarget an original image to a larger size. Results from method of Pritch
et al. (2009).
favor small offsets so that parts of the scene that are far from the change tend to be
unperturbed.
• To replace an area of the image, we specify unary costs so that the remainder of
the image must have a shift of zero (verbatim copying), and the shift in the missing
region must be such that it copies from outside the region.
• To retarget an image to larger width, we set the unary costs so that the left and
right edges of the new image are forced to have shifts that correspond to the left
and right of the original image. We also use the unary costs to specify that vertical
shifts must be small.
• To retarget an image to a smaller width (Figure 12.28), we additionally specify
that the horizontal offset can only increase as we move from left to right across the
image. This ensures that the new image does not contain replicated objects and
that their horizontal order remains constant.
In each case the best solution can be found using the alpha-expansion algorithm.
Since the pairwise terms do not form a metric here, it is necessary to truncate the offend-
ing costs (see Section 12.4.1). In practice there are many labels and so Pritch et al.
(2009) introduce a coarse-to-ﬁne scheme in which a low resolution version of the image

12.8
Applications
257
is initially synthesized and the result of this is used to guide further reﬁnements at higher
resolutions.
12.8.5 Superresolution
Image superresolution can also be framed as inference within a Markov random ﬁeld
model. Here, the basic unit of currency is an image patch rather than a pixel. For exam-
ple, consider dividing the original image into a regular grid of N low resolution 3 × 3
patches {xn}N
n=1. The goal is to infer a set of corresponding labels {wn}N
n=1 at each
position in the grid. Each label can take one of K values, each of which corresponds to a
different possible high resolution 7 × 7 patch. These patches are extracted from training
images.
The pairwise cost for placing high-resolution patches together is determined by the
agreement at the abutting edge. The unary cost for choosing a patch at a given position
depends on the agreement between the proposed high-resolution patch and the observed
low-resolution patch. This can be computed by downsampling the high-resolution patch
to 3 × 3 pixels and then using a normal noise model.
In principle we could perform inference in this model with a graph cut formula-
tion, but there are two problems. First, the resulting cost function is not submodu-
lar. Second, the number of possible high-resolution patches must be very large and
so the alpha-expansion algorithm (which chooses these in turn) would be extremely
inefﬁcient.
Freeman et al. (2000) used loopy belief propagation to perform approximate infer-
ence in a model similar to this. To make this relatively fast, they used only a subset of
J ≪K possible patches at each position where these were chosen so that they were the
J patches which agreed best with the observed data (and so had the lowest unary costs).
Although the results (Figure 12.30) are quite convincing, they are sadly far from the feats
demonstrated in modern TV crime drama.
12.8.6 Texture synthesis
The applications so far have all been based on performing inference in the undirected
Markov random ﬁeld model. We now consider the directed model. Inference is difﬁcult
Figure 12.30 Superresolution. a) The observed image, which is broken down into a regular grid
of low-resolution patches. b) We infer a regular grid of labels, each of which corresponds to a
high-resolution patch, and “quilt” these together to form the superresolved image. c) Ground truth.
Adapted from Freeman et al. (2000). c⃝2000 Springer.

258
12 Models for grids
Figure 12.31 Texture synthesis. a,b ) Original texture samples. c,d) Synthesized textures using
image quilting. Adapted from Efros and Freeman (2001).
in this model due to the presence of three-wise terms in the associated cost function (see
Section 12.7). However, generation from this model is relatively easy; since this is a
directed model, we can use an ancestral sampling technique to generate examples. One
possible application of this technique is for texture synthesis.
The goal of texture synthesis is to learn a generative model from a small patch of
texture such that when we draw samples from the model they look like extended exam-
ples of the same texture (Figure 12.31). The particular technique that we describe here
is known as image quilting and was originally described by Efros and Freeman (2001).
We will ﬁrst describe the algorithm as it was initially conceived, and then relate it to the
directed model for grids.
The ﬁrst step (see Figure 12.32) is to extract all possible patches of a given size from
the input texture to form a patch library. The synthesized image will consist of a regular
grid of these library patches such that each overlaps its neighbors by a few pixels. A new
texture is synthesized starting in the top-left of this grid and proceeding to the bottom-
right. At each position, a library patch is chosen such that it is visually consistent with
the patches that have previously been placed above and to the left.
For the top-left position, we randomly choose a patch from the library. We then
consider placing a second patch to the right of the ﬁrst patch, such that they overlap
by roughly 1/6 of their width. We search through the library for the J patches where
the squared RGB intensity difference in the overlapping region is smallest. We choose
one of these J patches randomly and place it into the image at the second position. We
continue in this way, synthesizing the top row of patches in the image. When we reach
the second row, we must consider the overlap with the patches to the left and above in
deciding whether a candidate library patch is suitable: we choose the J patches where
the total RGB difference between the overlapping portions of the candidate patch and
the previously chosen patches is minimal. This process continues until we reach the
bottom-right of the image.
In this way, we synthesize a new example of the texture (Figure 12.32a-f).
By
forcing the overlapping regions to be similar, we enforce visual consistency between
adjacent patches. By choosing randomly from the J best patches, we ensure that the
result is stochastic: if we always chose the most visually consistent patch, we would
replicate the original texture verbatim. At the end of this process, it is common to blend

12.8
Applications
259
Figure 12.32 Image quilting. a) Original texture sample. b) Library of all overlapping patches
from the original texture sample. c) The ﬁrst patch is chosen randomly from the library. d) The
second patch is chosen randomly from the k library patches that are most similar in the overlapping
region. e) In subsequent rows, patches are chosen so that the overlapping region agrees with the
previously placed patches to the left and above. f) This continues until we reach the bottom-right
of the image. g) The patches are then blended together to give the ﬁnal results.
the resulting patches together to remove remaining artifacts in the overlapping region
(Figure 12.32g).
Image quilting can be thought of as ancestral sampling from the directed model for
images (Figure 12.33). The observed data {xn}N
n=1 are the output patches, and the hid-
den labels {wn}N
n=1 represent the patch index. The labels are conditioned on their parents
with a probability distribution that allots a constant probability if the overlapping region
is one of the J closest and zero otherwise. The only real change is that the relationship
between label and observed data is now deterministic: a given label always produces
exactly the same output patch.
12.8.7 Synthesizing novel faces
Mohammed et al. (2009) presented a related technique to synthesize more complex
objects such as frontal faces (Figure 12.34), based on a large database of weakly aligned
training examples. Faces have a distinct spatial structure, and we must ensure that our
model enforces these constraints. To this end, we build a separate library of patches for
each position in the image. This ensures that the features have roughly the correct spatial
relations: the nose always appears in the center and the chin at the bottom.
In principle we could now apply a standard image quilting approach by synthesiz-
ing patches starting in the top-left and moving to the bottom-right. Unfortunately, the
resulting faces can drift in appearance (e.g., from male to female) as we move through

260
12 Models for grids
Figure 12.33 Image quilting as ancestral sampling from a graphical model. When we synthesize
images we are effectively ancestral sampling from a directed grid model where each hidden node
represents a patch index and each observed variable represents the patch data.
the image. To prevent this from happening, we condition the patch synthesis on a draw
from a factor analysis model (Section 7.6), which has been trained with frontal faces. A
sample from this model looks like a blurry, but globally coherent face. Now when we
choose potential patches, they must agree with both the previously placed patches to the
left and above, but also be similar to the appropriate part of the blurry sample from the
subspace model. The generated images from this model look like highly realistic human
faces.
In terms of probability, the labels {wn}N
n=1 in this model are conditioned not only
on their ancestors wpa but also on the hidden variable in the subspace model h (see
Figure 12.35). This hidden variable connects to every patch label {wn}N
n=1 and gives the
resulting image a greater visual coherence than the Markov connections of the patches
alone.
Discussion
Models for grids are ubiquitous in vision: they occur in almost all applications that
attempt to associate a label with each position in the image. Depending on the appli-
cation, this label may indicate the depth, object type, segmentation mask, or motion
at that pixel.
Unfortunately, most problems of this type are NP hard, and so we
must resort to efﬁcient approximate inference techniques such as the alpha-expansion
algorithm.
Notes
MRFs and CRFs: Markov random ﬁelds were ﬁrst investigated in computer vision by Geman and
Geman (1984), although much of the early work dealt with continuous variables rather than the

Notes
261
Figure 12.34 Synthesizing novel faces.
a) A sample is drawn from a subspace model (see
Chapter 7) that has been trained on facial images.
b) Texture synthesis now proceeds but
with two differences from before.
First, the choice of patch must now agree with the sam-
ple from the subspace model as well as the previously placed patches.
Second, the library
patches are now different at each position: in this way we ensure that a nose patch is always
chosen in the center and so on.
c) After completing the synthesis and blending together the
patches. d-f) Three more examples of synthesized faces. Adapted from Mohammed et al. (2009).
c⃝2009 ACM.
discrete case as discussed in this chapter. A good review can be found in Li (2010). Conditional
random ﬁelds were ﬁrst used in computer vision by Kumar and Hebert (2003). An overview can
be found in Sutton and McCallum (2011).
Applications: Grid-based models and graph cuts are used extensively in vision and graphics. A
partial list of applications includes stereo vision (Kolmogorov and Zabih 2001; Woodford et al.
2009), optical ﬂow (Kolmogorov and Zabih 2001), texture synthesis (Kwatra et al. 2003), photo-
montage (Agarwala et al. 2004), summarizing photo collections with collages (Rother et al. 2005,
2006), bilayer segmentation (Kolmogorov et al. 2006), interactive segmentation (Rother et al.
2004; Boykov et al. 2001), superresolution (Freeman et al. 2000), image retargeting (Pritch et al.
2009), denoising (Greig et al. 1989), oversegmentation (Moore et al. 2010; Veksler et al. 2010),
image colorization (Levin et al. 2004), segmantic segmentation (Shotton et al. 2009), multi-view
reconstruction (Kolmogorov and Zabih 2002; Vogiatzis et al. 2007), and matching image points
(Isack and Boykov 2012).
Graph cuts: The ﬁrst application of graph cuts to inference in an MRF is due to Greig et al.
(1989) who investigated binary denoising. However, it was not until the work of Boykov et al.
(2001) that this result was rediscovered and graph cuts became widely used. Ishikawa (2003) pre-
sented the exact solution for multilabel graph cuts with convex potentials, and this was generalized
by Schlesinger and Flach (2006).
The presentation in this chapter is a hybrid of these two
methods. Boykov et al. (2001) introduced the idea of optimizing non-convex multilabel ener-
gies via a series of binary problems. They proposed two algorithms of this kind: the alpha-beta

262
12 Models for grids
Figure 12.35 Graphical model for synthesizing novel faces. When we generate a new image we
are ancestral sampling from a directed grid model, where each variable w is conditioned on the
hidden variable h of the subspace model. Adapted from Mohammed et al. (2009) c⃝2009 ACM.
swap in which pairs of labels are exchanged for one another and the alpha-expansion algorithm.
They also proved that the alpha-expansion solution is guaranteed to be within a factor of two
of the true solution. In the same spirit, Lempitsky et al. (2010) and Kumar et al. (2011) have
proposed more complex “moves”. Tarlow et al. (2011) elucidates the conection between graph-
cut methods and max-product belief propagation.
For more detailed overviews of graph cut
methods, consult Boykov and Veksler (2006), Felzenszwalb and Zabih (2011), and Blake et al.
(2011).
Max-ﬂow: Graph-cut methods rely on algorithms for computing maximum ﬂow. The most com-
mon of these are the augmenting paths method of Ford and Fulkerson (1962) and the push-relabel
method of Goldberg and Tarjan (1988). Details of these and other approaches to the same prob-
lem can be found in any standard textbook on algorithms such as Cormen et al. (2001). The
most common technique in computer vision is a modiﬁed version of the augmented paths algo-
rithm due to Boykov and Kolmogorov (2004) that has been demonstrated to have very good
performance for vision problems. Kohli and Torr (2005), Juan and Boykov (2006), and Alahari
et al. (2008) have all investigated methods for improving the efﬁciency of graph-cuts by reusing
solutions to similar graph-cut problems (e.g., based on the solution to the previous frames in a
time-sequence).
Cost functions and optimization: Kolmogorov and Zabih (2004) provide a summary of the
cost functions that can be optimized using the basic graph-cuts max-ﬂow formulation with
binary variables.
Kolmogorov and Rother (2007) summarize graph-cut approaches to non-
submodular energies. Rother et al. (2007) and Komodakis et al. (2008) present algorithms that
can approximately optimize more general cost functions.
Constraint edges: Recent work has investigated bespoke graph constructions that make heavy use
of constraint edges (edges of inﬁnite strength) to ensure that the solution conforms to a certain

Notes
263
structure. For example, Delong and Boykov (2009) devised a method that forced certain labels to
surround others, and Moore et al. (2010) described a method that forces the label ﬁeld to conform
to a lattice. See also Felzenszwalb and Veksler (2010) for a related scheme based on dynamic
programming.
Higher-order cliques: All of the methods discussed in this chapter assume pairwise connections;
the cliques include only two discrete variables. However, to model more complex statistics of the
label ﬁeld, it is necessary to include more than two variables in the cliques, and these are known as
higher-order models. Roth and Black (2009) demonstrated good denoising and inpainting results
with a continuous MRF model of this kind, and Domke et al. (2008) demonstrated the efﬁcacy of
a directed model in which each variable was conditioned on a number of variables above and to
the right in the image. There has recently been considerable interest in developing algorithms for
MAP estimation in models with discrete variables and higher-order cliques (Ishikawa 2009; Kohli
et al. 2009a, 2009b; Rother et al. 2009).
Other approaches to MAP estimation: There are many other contemporary approaches to MAP
estimation in MRFs and CRFs.
These include loopy belief propagation (Weiss and Freeman
2001), quadratic pseudo-boolean optimization that is used in non-submodular cost functions
(Kolmogorov and Rother 2007), random walks (Grady 2006), and linear programming (LP) relax-
ations (Weiss et al. 2011) and various approaches to maximize the LP lower bound such as tree
reweighted message passing (Wainright et al. 2005; Kolmogorov 2006). An experimental com-
parison between different energy minimization methods for MRFs can be found in Szeliski et al.
(2008).
Texture synthesis: Texture synthesis was originally investigated as a continuous problem and
the focus was on modeling the joint statistics of the RGB values in a small patch (Heeger and
Bergen 1995; Portilla and Simoncelli 2000). Although texture synthesis as a continuous prob-
lem is still an active research area (e.g., Heess et al. 2009), these early methods were displaced
by methods that represented the texture in terms of discrete variables (either by quantizing the
RGB values, indexing patches or using a shift-map representation).
The resulting algorithms
(e.g., Efros and Leung 1999; Wei and Levoy 2000; Efros and Freeman 2001; Kwatra et al.
2003) were originally described as heuristic approaches to generating textures, but can also
be interpreted as exact or approximate ways to draw samples from directed or undirected grid
models.
Interactive segmentation: The use of graph cuts for interactive segmentation algorithms was
pioneered by Boykov and Jolly (2001). In early works (Boykov and Jolly 2001; Boykov and
Funka Lea 2006; Li et al. 2004) the user interacted with the image by placing marks indicating
foreground and background regions. Grab cut (Rother et al. 2004) allowed the user to draw a box
around the object in question. More recent systems (Liu et al. 2009) are fast enough to allow
the user to interactively “paint” the selection onto the images. Current interest in graph cut –
based segmentation is mainly focused on developing novel priors over the shape that improve
performance (e.g., Malcolm et al. 2007; Veksler 2008; Chittajallu et al. 2010; Freiman et al. 2010).
To this end, Kumar et al. (2005) introduced a method for imposing high-level knowledge about the
articulation of the object, Vicente et al. (2008) developed an algorithm that is suited for cutting out
elongated objects, and Lempitsky et al. (2008) used a prior based on a bounding box around the
object.
Stereo vision: Most state-of-the-art stereo vision algorithms rely on MRFs or CRFs and are solved
using either graph cuts (e.g., Kolmogorov and Zabih 2001) or belief propagation (e.g., Sun et al.
2003). Comparisons of these approaches can be found in Tappen and Freeman (2003) and Szeliski
et al. (2008). An active area of research in dense stereo vision is the formulation of the compatibil-
ity of the two images given a certain disparity offset (e.g., Bleyer and Chambon 2010; Hirschm¨uller
and Scharstein 2009), which is rarely based on single pixels in practice (see Yoon and Kweon
2006; Tombari et al. 2008).

264
12 Models for grids
Figure 12.36 Graph for Problem 12.2.
For more information about stereo vision, see the reviews by Scharstein and Szeliski (2002) and
Brown et al. (2003) or consult Szeliski (2010), which contains a good modern summary. Chapter
11 of this book summarizes dynamic programming approaches. Notable stereo implementations
include the region growing approach of Lhuillier and Quan (2002); the systems of Zitnick and
Kanade (2000) and Hirschm¨uller (2005), both of which are available online; and the extremely
efﬁcient GPU-based system of Sizintsev and Wildes (2010). For an up-to-date quantitative com-
parison of the latest stereo vision algorithms, consult the Middlebury stereo vision Web site
(http://vision.middlebury.edu/stereo/).
Problems
12.1 Consider a Markov random ﬁeld with the structure
Pr(x1,x2,x3,x4) = 1
Z φ[x1,x2]φ[x2,x3]φ[x3,x4]φ[x4,x1]
but where the variables x1,x2,x3, and x4 are continuous and the potentials are deﬁned as
φ[a,b] = exp

−(a −b)2
.
This is known as a Gaussian Markov random ﬁeld. Show that the joint probability is a normal
distribution, and ﬁnd the information matrix (inverse covariance matrix).
12.2 Compute the MAP solution to the three-pixel graph-cut problem in Figure 12.36 by (i) com-
puting the cost of all eight possible solutions explicitly and ﬁnding the one with the minimum cost
(ii) running the augmenting paths algorithm on this graph by hand and interpreting the minimum
cut.
12.3 Explicitly compute the costs associated with the four possible minimum cuts of the graph in
Figure 12.10.
12.4 Compute the cost for each the four possible cuts of the graph in Figure 12.11c.
12.5 Consider the graph construction in Figure 12.37a, which contains a number of constraint
edges of inﬁnite cost (capacity). There are 25 possible minimum cuts on this graph, each of which
corresponds to one possible labeling of the two pixels. Write out the cost for each labeling. Which
solutions have ﬁnite cost for this graph construction?

Problems
265
Figure 12.37 Alternative multilabel graph constructions. Each of these these graphs has extra
constraint links with inﬁnite weight. These have the effect of giving an inﬁnite cost to a subset of
the possible solutions.
12.6 Which of the possible minimum cuts of the graph in Figure 12.37b have a ﬁnite cost?
12.7 Conﬁrm that the costs of the cuts in Figure 12.14 are as claimed by explicitly performing the
summation over the relevant terms Cij.
12.8 Show that the Potts model (Figure 12.17c) is not submodular by providing a counterexample
to the required criterion:
Pab(β,γ) + Pab(α,δ) −Pab(β,δ) −Pab(α,γ) ≥0.
12.9 An alternative to the alpha-expansion algorithm is the alpha-beta swap. Here, a multilabel
MRF with non-convex potentials is optimized by repeatedly choosing pairs of labels α,β and
performing a binary graph cut that allows them to swap in such a way that the overall cost function
decreases. Devise a graph structure that can be used to perform this operation. Hint: consider
separate cases for neighboring labels (α,α), (β,β), (β,γ), (α,γ) and (γ,γ) where γ is a label
that is neither α nor β.


Part IV
Preprocessing

The main focus of this book is on statistical models for computer vision; the previous
chapters concern models that relate visual measurements x to the world w. However,
there has been little discussion of how the measurement vector x was created, and it
has often been implied that it contains concatenated RGB pixel values. In state-of-the-
art vision systems, the image pixel data are almost always preprocessed to form the
measurement vector.
We deﬁne preprocessing to be any transformation of the pixel data prior to build-
ing the model that relates the data to the world. Such transformations are often ad hoc
heuristics: their parameters are not learned from training data, but they are chosen based
on experience of what works well. The philosophy behind image preprocessing is easy
to understand; the image data may be contingent on many aspects of the real world that
do not pertain to the task at hand. For example, in an object detection task, the RGB
values will change depending on the camera gain, illumination, object pose and particu-
lar instance of the object. The goal of image preprocessing is to remove as much of this
unwanted variation as possible while retaining the aspects of the image that are critical
to the ﬁnal decision.
In a sense, the need for preprocessing represents a failure; we are admitting that
we cannot directly model the relationship between the RGB values and the world state.
Inevitably, we must pay a price for this. Although the variation due to extraneous fac-
tors is jettisoned, it is very probable that some of the task-related information is also
discarded. Fortunately, in these nascent years of computer vision, this rarely seems to be
the limiting factor that governs the overall performance.
We devote the single chapter in this section to discussing a variety of preprocessing
techniques. Although the treatment here is not extensive, it should be emphasized that
preprocessing is very important; in practice the choice of preprocessing technique can
inﬂuence the performance of vision systems at least as much as the choice of model.

Chapter 13
Image preprocessing and
feature extraction
This chapter provides a brief overview of modern preprocessing methods for computer
vision. In Section 13.1 we introduce methods in which we replace each pixel in the image
with a new value. Section 13.2 considers the problem of ﬁnding and characterizing edges,
corners and interest points in images. In Section 13.3 we discuss visual descriptors;
these are low-dimensional vectors that attempt to characterize the interesting aspects of
an image region in a compact way. Finally, in Section 13.4 we discuss methods for
dimensionality reduction.
13.1
Per-pixel transformations
We start our discussion of preprocessing with per-pixel operations: these methods return
a single value corresponding to each pixel of the input image. We denote the original
2D array of pixel data as P, where pij is the element at the ith of I rows and the jth
of J columns. The element pij is a scalar representing the grayscale intensity. Per-pixel
operations return a new 2D array X of the same size as P containing elements xij.
13.1.1 Whitening
The goal of whitening (Figure 13.1) is to provide invariance to ﬂuctuations in the mean
intensity level and contrast of the image. Such variation may arise because of a change in
ambient lighting intensity, the object reﬂectance, or the camera gain. To compensate for
these factors, the image is transformed so that the resulting pixel values have zero mean
and unit variance. To this end, we compute the mean µ and variance σ2 of the original
grayscale image P:
µ =
PI
i=1
PJ
j=1 pij
IJ
σ2 =
PI
i=1
PJ
j=1(pij −µ)2
IJ
.
(13.1)

270
13 Image preprocessing and feature extraction
Figure 13.1 Whitening and histogram equalization. a) A number of faces which have been cap-
tured with widely varying contrasts and mean levels. b) After whitening, the images have the same
mean and variance. c) After histogram equalization, all of the moments of the images are approxi-
mately the same. Both of these transformations reduce the amount of variation due to contrast and
intensity changes.
These statistics are used to transform each pixel value separately so that
xij = pij −µ
σ
.
(13.2)
For color images, this operation may be carried out by computing the statistics µ and σ2
from all three channels or by separately transforming each of the RGB channels based
on their own statistics.
Note that even this simple transformation has the potential to hamper subsequent
inference about the scene: depending on the task, the absolute intensities may or may not
contain critical information. Even the simplest preprocessing methods must be applied
with care.
13.1.2 Histogram equalization
The goal of histogram equalization (Figure 13.1c) is to modify the statistics of the inten-
sity values so that all of their moments take predeﬁned values. To this end, a nonlinear
transformation is applied that forces the distribution of pixel intensities to be ﬂat.
We ﬁrst compute the histogram of the original intensities h where the kth of K entries
is given by
hk =
I
X
i=1
J
X
j=1
δ[pij −k],
(13.3)
where the operation δ[•] returns one if the argument is zero and zero otherwise. We then
cumulatively sum this histogram and normalize by the total number of pixels to compute
the cumulative proportion c of pixels that are less than or equal to each intensity level:
ck =
Pk
l=1 hl
IJ
.
(13.4)
Finally, we use the cumulative histogram as a look up table to compute the transformed
value so that
xij = Kcpij.
(13.5)

13.1
Per-pixel transformations
271
Figure 13.2 Histogram equalization. The abscissa indicates the pixel intensity. The ordinate
indicates the proportion of intensities that were less than or equal to this value. This plot can be
used as a look-up table for histogram equalizing the intensities. For a given intensity value on the
abscissa, we choose the new intensity to be the maximum output intensity K times the value on the
ordinate. After this transformation, the intensities are equally distributed. In the example image,
many of the pixels are bright. Histogram equalization spreads these bright values out over a larger
intensity range, and so has the effect of increasing the contrast in the brighter regions.
For example, in Figure 13.2 the value 90 will be mapped to K × 0.29 where K is
the maximum intensity (usually 255). The result is a continuous number rather than a
discretized pixel intensity but is in the same range as the original data. The result can be
rounded to the nearest integer if subsequent processing demands.
13.1.3 Linear ﬁltering
After ﬁltering an image, the new pixel value xij consists of a weighted sum of the inten-
sities of pixels in the surrounding area of the original image P. The weights are stored in
a ﬁlter kernel F, which has entries fm,n, where m ∈{−M ...M} and n ∈{−N ...N}.
More formally, when we apply a ﬁlter, we convolve the P with the ﬁlter F, where
two-dimensional convolution is deﬁned as
xij =
M
X
m=−M
N
X
n=−N
pi−m,j−nfm,n.
(13.6)
Notice that by convention, the ﬁlter is ﬂipped in both directions so the top left of the ﬁlter
f−M,−N weights the pixel pi+M,j+N to the right and below the current point in P. Many
ﬁlters used in vision are symmetric in such a way that this ﬂipping makes no practical
difference.

272
13 Image preprocessing and feature extraction
Figure 13.3 Image blurring. a) Original image. b) Result of convolving with a Gaussian ﬁlter
(ﬁlter shown in bottom right of image). Each pixel in this image is a weighted sum of the sur-
rounding pixels in the original image, where the weights are given by the ﬁlter. The result is that
the image is slightly blurred. c–d) Convolving with a ﬁlter of increasing standard deviation causes
the resulting image to be increasingly blurred.
Without further modiﬁcation, this formulation will run into problems near the borders
of the image: it needs to access points that are outside the image. One way to deal with
this is to use zero padding in which it is assumed that the value of P is 0 outside the
deﬁned image region.
We now consider a number of common types of ﬁlter.
Gaussian (blurring) ﬁlter
To blur an image, we convolve it with a 2D Gaussian,
f(m,n) =
1
2πσ2 exp

−m2 + n2
2σ2

.
(13.7)
Each pixel in the resulting image is a weighted sum of the surrounding pixels, where the
weights depend on the Gaussian proﬁle: nearer pixels contribute relatively more to the
ﬁnal output. This process blurs the image, where the degree of blurring is dependent on
the standard deviation σ of the Gaussian ﬁlter (Figure 13.3). This is a simple method to
reduce noise in images taken at very low light levels.
First derivative ﬁlters and edge ﬁlters
A second use for image ﬁltering is to locate places in the image where the intensity
changes abruptly. Consider taking the ﬁrst derivative of the image along the rows. We
could approximate this operation by simply computing the difference between two offset
pixels along the row. This operation can be accomplished by ﬁltering with the operator
F = [−1 0 1]. This ﬁlter gives zero response when the image is ﬂat in the horizontal
direction: it is hence invariant to constant additive luminance changes. It gives a negative
response when the image values are increasing as we move in a horizontal direction and
a positive response when they are decreasing (recall that convolution ﬂips the ﬁlter by
180◦). As such, it is selective for edges in the image.
The response to the ﬁlter F = [−1 0 1] is noisy because of its limited spatial
extent. Consequently, slightly more sophisticated ﬁlters are used to ﬁnd edges in practice.
Examples include the Prewitt operators (Figures 13.4b–c)
Fx =


1
0
−1
1
0
−1
1
0
−1

,
Fy =


1
1
1
0
0
0
−1
−1
−1

,
(13.8)

13.1
Per-pixel transformations
273
and the Sobel operators
Fx =


1
0
−1
2
0
−2
1
0
−1

,
Fy =


1
2
1
0
0
0
−1
−2
−1

,
(13.9)
where in each case the ﬁlter Fx is a ﬁlter selective for edges in the horizontal direction,
and Fy is a ﬁlter selective for edges in the vertical direction.
Laplacian ﬁlters
The Laplacian ﬁlter is the discrete two-dimensional approximation to the Laplacian
operator ∇2 and is given by
F =


0
−1
0
−1
4
−1
0
−1
0

.
(13.10)
Applying the discretized ﬁlter F to an image results in a response of high magnitude
where the image is changing, regardless of the direction of that change (Figure 13.4d):
the response is zero in regions that are ﬂat and signiﬁcant where edges occur in the image.
It is hence invariant to constant additive changes in luminance and useful for identifying
interesting regions of the image.
Laplacian of Gaussian ﬁlters
In practice the Laplacian operator produces noisy results. A superior approach is to
ﬁrst smooth the image with a Gaussian ﬁlter and then apply the Laplacian. Due to the
associative property of convolution, we can equivalently convolve the Laplacian ﬁlter
by a Gaussian and apply the resulting Laplacian of Gaussian ﬁlter to the image (Figure
13.4e). This Laplacian of Gaussian has the advantage that it can be tuned to be selective
for changes at different scales, depending on the scale of the Gaussian component.
Diﬀerence of Gaussians
The Laplacian of Gaussian ﬁlter is very well approximated by the difference of Gaussians
ﬁlter (compare Figures 13.4e and 13.4f). As the name implies, this ﬁlter is created by tak-
ing the difference of two Gaussians at nearby scales. The same result can be achieved by
ﬁltering the image with the two Gaussians separately and taking the difference between
the results. Again, this ﬁlter responds strongly in regions of the image that are changing
at a predetermined scale.
Gabor ﬁlters
Gabor ﬁlters are selective for both scale and orientation. The 2D Gabor function is the
product of a 2D Gaussian with a 2D sinusoid. It is parameterized by the covariance of
the Gaussian and the phase φ, orientation ω, and wavelength λ of the sine wave. If the
Gaussian component is spherical, it is deﬁned by
fmn =
1
2πσ2 exp

−m2 + n2
2σ2

sin
2π(cos[ω]m + sin[ω]n)
λ
+ φ

,
(13.11)
where σ controls the scale of the spherical Gaussian. It is typical to make the wavelength
proportional to the scale σ of the Gaussian so a constant number of cycles is visible.

274
13 Image preprocessing and feature extraction
Figure 13.4 Image ﬁltering with ﬁrst- and second-derivative operators. a) Original image. b)
Convolving with the vertical Prewitt ﬁlter produces a response that is proportional to the size and
polarity of edges in the vertical direction. c) The horizontal Prewitt ﬁlter produces a response to
edges in the horizontal direction. d) The Laplacian ﬁlter gives a signiﬁcant response where the
image changes rapidly regardless of direction. e) The Laplacian of Gaussian ﬁlter produces similar
results but the output is smoothed and hence less noisy. f) The difference of Gaussians ﬁlter is a
common approximation to the Laplacian of Gaussian.
The Gabor ﬁlter is selective for elements within the image at a certain frequency
and orientation band and with a certain phase (Figure 13.5). It is invariant to constant
additive changes in luminance when the sinusoidal component is asymmetric relative to
the Gaussian. This is also nearly true for symmetric Gabor functions, as long as several
cycles of the sinusoid are visible. A response that is independent of phase can easily be

13.1
Per-pixel transformations
275
Figure 13.5 Filtering with Gabor functions. a) Original image. b) After ﬁltering with horizon-
tal asymmetric Gabor function at a large scale (ﬁlter shown bottom right). c) Result of ﬁltering
with horizontal symmetric Gabor function at a large scale. d) Response to diagonal Gabor ﬁlter
(responds to diagonal changes).
generated by squaring and summing the responses of two Gabor features with the same
frequency, orientation, and scale, but with phases that are π/2 radians apart. The resulting
quantity is termed the Gabor energy and is somewhat invariant to small displacements of
the image.
Filtering with Gabor functions is motivated by mammalian visual perception: this
is one of the ﬁrst processing operations applied to visual data in the brain.
More-
over, it is known from psychological studies that certain tasks (e.g., face detection)
are predominantly dependent on information at intermediate frequencies. This may be
because high-frequency ﬁlters see only a small image region and are hence noisy and
relatively uninformative, and low-frequency ﬁlters act over a large region and respond
disproportionately to slow changes due to lighting.
Haar-like ﬁlters
Haar-like ﬁlters consist of adjacent rectangular regions that are balanced so that the aver-
age ﬁlter value is zero, and they are invariant to constant luminance changes. Depending
on the conﬁguration of these regions, they may be similar to derivative or Gabor ﬁlters
(Figure 13.6).
However, Haar-like ﬁlters are noisier than the ﬁlters they approximate: they have
sharp edges between positive and negative regions and so moving by a single pixel near
an edge may change the response signiﬁcantly. This drawback is compensated for by the
relative speed with which Haar functions can be computed.
To compute Haar-like functions rapidly, we ﬁrst form the integral image (Figure
13.6g). This is an intermediate representation in which each pixel contains the sum of all
of the intensity values above and to the left of the current position. So the value in the
top-left corner is the original pixel value at that position and the value in the bottom-right
corner is the sum of all of the pixel values in the image. The values in the other parts of
the integral image are between these extremes.
Given the integral image I, it is possible to compute the sum of the intensities in
any rectangular region with just four operations, regardless of how large this region is.
Consider the region is deﬁned by the range [i0,i1] down the columns and [j0,j1] along
the rows. The sum S of the internal pixel intensities is
S = Ii1,j1 + Ii0,j0 −Ii1,j0 −Ii0,j1.
(13.12)
The logic behind this calculation is illustrated in Figure 13.6f–i.

276
13 Image preprocessing and feature extraction
Figure 13.6 Haar-like ﬁlters. a–d) Haar-like ﬁlters consist of rectangular regions. Convolution
with Haar-like ﬁlters can be done in constant time. e) To see why, consider the problem of ﬁltering
with this single rectangular region. f) We denote the sum of the pixel values in these four regions
as A,B,C, and D. Our goal is to compute D. g) The integral image has a value that is the sum
of the intensities of the pixels above and to the left of the current position. The integral image
at position (i1,j1) hence has value A + B + C + D. h) The integral image at (i1,j0) has value
A+C. i) The integral image at (i0,j1) has value A+B. j) The integral image at (i0,j0) has value
A. The sum of the pixels in region D can now be computed as Ii1,j1 + Ii0,j0 −Ii1,j0 −Ii0,j1 =
(A + B + C + D) + A −(A + C) −(A + B) = D. This requires just four operations regardless
of the size of the original square region.
Since Haar-like ﬁlters are composed of rectangular regions, they can be computed
using a similar trick. For a ﬁlter with two adjacent rectangular regions, six operations are
required. With three adjacent rectangular regions, eight operations are required. When
the ﬁlter dimensions M and N are large, this approach compares very favorably to a
na¨ıve implementation of conventional ﬁltering which requires O(MN) operations to
compute the ﬁlter response due to a M × N kernel. Haar ﬁlters are often used in real-
time applications such as face detection because of the speed with which they can be
computed.
13.1.4 Local binary patterns
The local binary patterns (LBP) operator returns a discrete value at each pixel that char-
acterizes the local texture in a way that is partially invariant to luminance changes. For
this reason, features based on local binary patterns are commonly used as a substrate for
face recognition algorithms.
The basic LBP operator compares the eight neighboring pixel intensities to the center
pixel intensity, assigning a 0 or a 1 to each neighbor depending on whether they are less
than or greater than the center value. These binary values are then concatenated in a
predetermined order and converted to a single decimal number that represents the “type”
of local image structure (Figure 13.7).
With further processing, the LBP operator can be made orientation invariant: the
binary representation is repeatedly subjected to bitwise shifts to create eight new binary
values and the minimum of these values is chosen. This reduces the number of possible
LBP values to 36. In practice, it has been found that the distribution over these 36 LBP
values is dominated by those that are relatively uniform. In other words, binary strings
where transitions are absent (e.g., 00000000, 11111111) or infrequent (e.g., 00001111,

13.1
Per-pixel transformations
277
Figure 13.7 Local binary patterns. a) The local binary pattern is computed by comparing the
central pixel to each of its eight neighbors. The binary value associated with each position is set to
one if that neighbor is greater than or equal to the central pixel. The eight binary values can be read
out and combined to make a single 8-bit number. b) Local binary patterns can be computed over
larger areas by comparing the current pixels to the (interpolated) image at positions on a circle.
This type of LBP is characterized by the number of samples P and the radius of the circle R.
00111111) occur most frequently. The number of texture classes can be further reduced
by aggregating all of the nonuniform LBPs into a single class. Now the local image
structure is categorized into nine LBP types (eight rotationally invariant uniform patterns
and one nonuniform class).
The LBP operator can be extended to use neighborhoods of different sizes: the central
pixel is compared to positions in a circular pattern (Figure 13.7b). In general, these
positions do not exactly coincide with the pixel grid, and the intensity at these positions
must be estimated using bilinear interpolation. This extended LBP operator can capture
texture at different scales in the image.
13.1.5 Texton maps
The term texton stems from the study of human perception and refers to a primitive
perceptual element of texture. In other words, it roughly occupies the role that a phoneme
takes in speech recognition. In a machine vision context a texton is a discrete variable
that designates which one of a ﬁnite number of possible texture classes is present in a
region surrounding the current pixel. A texton map is an image in which the texton is
computed at every pixel (Figure 13.8).
Texton assignment depends on training data. A bank of N ﬁlters is convolved with a
set of training images. The responses are concatenated to form one N ×1 vector for each
pixel position in each training image. These vectors are then clustered into K classes
using the K-means algorithm (Section 13.4.4). Textons are computed for a new image by
convolving it with the same ﬁlter bank. For each pixel, the texton is assigned by noting
which cluster mean is closest to the N ×1 ﬁlter output vector associated with the current
position.
The choice of ﬁlter bank seems to be relatively unimportant. One approach has been
to use Gaussians at scales σ,2σ, and 4σ to ﬁlter all three color channels, and derivatives
of Gaussians at scales 2σ and 4σ and Laplacians of Gaussians at scales σ,2σ,4σ, and 8σ
to ﬁlter the luminance (Figure 13.9a). In this way, both color and texture information is
captured.
It may be desirable to compute textons that are invariant to orientation.
One
way of achieving this is to choose rotationally invariant ﬁlters to form the ﬁlter bank
(Figure 13.9b). However, these have the undesirable property of not responding at all to
oriented structures in the image. The Maximum Response (MR8) ﬁlter bank is designed

278
13 Image preprocessing and feature extraction
Figure 13.8 Texton maps. In a texton map each pixel is replaced by the texton index. This index
characterizes the texture in the surrounding region. a) Original image. b) Associated texton map.
Note how similar regions are assigned the same texton index (indicated by color). c) Original
image. d) Associated texton map (using different ﬁlter bank from (b)). Texton maps are often used
in semantic image segmentation. Adapted from Shotton et al. (2009). c⃝2009 Springer.
Figure 13.9 Textons. The image is convolved with a ﬁlter bank to yield an N × 1 vector of
ﬁlter responses at each position. Possible choices for the ﬁlter bank include a) a combination of
Gaussians, derivatives of Gaussians, and Laplacians of Gaussians; b) rotationally invariant ﬁlters;
and c) the maximum response (MR8) database. d) In training, the N ×1 ﬁlter response vectors are
clustered using K-means. For new data, the texton index is assigned based on the nearest of these
clusters. Thus, the ﬁlter space is effectively partitioned into Voronoi regions.
to provide a rotationally invariant measure of local texture, which does not discard this
information. The MR8 ﬁlter bank (Figure 13.9c) consists of a Gaussian and a Laplacian
of Gaussian ﬁlter, an edge ﬁlter at three scales and a bar ﬁlter (a symmetric oriented ﬁl-
ter) at the same three scales. The edge and bar ﬁlter are replicated at six orientations at
each scale, giving a total of 38 ﬁlters. To induce rotational invariance only the maximum
ﬁlter response over orientation is used. Hence the ﬁnal vector of ﬁlter responses consists
of eight numbers, corresponding to the Gaussian and Laplacian ﬁlters (already invariant)
and the maximum responses over orientation of the edge and bar ﬁlters at each of the
three scales.

13.2
Edges, corners, and interest points
279
13.2
Edges, corners, and interest points
In this section, we consider methods that aim to identify informative parts of the image.
In edge detection the goal is to return a binary image where a nonzero value denotes
the presence of an edge in the image. Edge detectors optionally also return other infor-
mation such as the orientation and scale associated with the edge. Edge maps are a
highly compact representation of an image, and it has been shown that it is possible to
reconstruct an image very accurately with just information about the edges in the scene
(Figure 13.10).
Corners are positions in the image that contain rich visual information and can be
found reproducibly in different images of the same object (Figure 13.12). There are many
schemes to ﬁnd corners, but they all aim to identify points that are locally unique. Corner
detection algorithms were originally developed for geometric computer vision problems
such as wide baseline image matching; here we see the same scene from two different
angles and wish to identify which points correspond to which. In recent years, corners
have also been used in object recognition algorithms (where they are usually referred to
as interest points). The idea here is that the regions surrounding interest points contain
information about which object class is present.
13.2.1 Canny edge detector
To compute edges with the Canny edge detector (Figure 13.11), the image P is ﬁrst
blurred and then convolved with a pair of orthogonal derivative ﬁlters such as Prewitt
ﬁlters to create images H and V containing derivatives in the horizontal and vertical
directions, respectively. For pixel (i,j), the orientation θij and magnitude aij of the
gradient is computed using
θij = arctan[vij/hij]
aij =
q
h2
ij + v2
ij.
(13.13)
A simple approach would be to assign an edge to position (i,j) if the amplitude there
exceeds a critical value. This is termed thresholding. Unfortunately, it produces poor
results: the amplitude map takes high values on the edge, but also at adjacent positions.
The Canny edge detector eliminates these unwanted responses using a method known as
non-maximum suppression.
Figure 13.10 Reconstruction from edges. a) Original image. b) Edge map. Each edge pixel
has associated scale and orientation information as well as a record of the luminance levels at
either side. c) The image can be reconstructed almost perfectly from the edges and their associated
information. Adapted from Elder (1999). c⃝1999 Springer.

280
13 Image preprocessing and feature extraction
Figure 13.11 Canny edge detection.
a) Original image.
b) Result of vertical Prewitt ﬁlter.
c) Results of horizontal Prewitt ﬁlter. d) Quantized orientation map. e) Gradient amplitude map.
f) Amplitudes after non-maximum suppression. g) Thresholding at two levels: the white pixels are
above the higher threshold. The red pixels are above the lower threshold but below the higher one.
h) Final edge map after hysteresis thresholding contains all of the white pixels from (g) and those
red pixels that connect to them.
In non-maximum suppression the gradient orientation is quantized into one of four
angles {0o,45o,90o,135o}, where angles 180o apart are treated as equivalent. The pixels
associated with each angle are now treated separately. For each pixel, the amplitude is set
to zero if either of the neighboring two pixels perpendicular to the gradient have higher
values. For example, for a pixel where the gradient orientation is vertical (the image is
changing in the horizontal direction), the pixels to the left and right are examined and the
amplitude is set to zero if either of these are greater than the current value. In this way,
the gradients at the maximum of the edge amplitude proﬁle are retained, and those away
from this maximum are suppressed.
A binary edge map can now be computed by comparing the remaining nonzero ampli-
tudes to a ﬁxed threshold. However, for any given threshold there will be misses (places
where there are real edges, but their amplitude falls below the threshold) and false posi-
tives (pixels labeled as edges where none exist in the original image). To decrease these
undesirable phenomena, knowledge about the continuity of real-world edges is exploited.
Two thresholds are deﬁned. All of the pixels whose amplitude is above the higher thresh-
old are labeled as edges, and this threshold is chosen so that there are few false positives.
To try to decrease the number of misses, pixels that are above the lower amplitude thresh-
old and are connected to an existing edge pixel are also labeled as edges. By iterating
this last step, it is possible to trace along weaker parts of strong contours. This technique
is known as hysteresis thresholding.

13.2
Edges, corners, and interest points
281
Figure 13.12 Harris corner detector. a) Image with detected corners. The corner detection algo-
rithm is based on the image structure tensor that captures information about the distribution of
gradients around the point. b) In ﬂat regions, both singular values of the image structure tensor are
small. c) On edges, one is small and the other large. d) At corners, both are large indicating that
the image is changing quickly in both directions.
13.2.2 Harris corner detector
The Harris corner detector (Figure 13.12) considers the local gradients in the horizontal
and vertical directions around each point. The goal is to ﬁnd points in the image where
the image intensity is varying in both directions (a corner) rather than in one direction
(an edge) or neither (a ﬂat region). The Harris corner detector bases this decision on the
image structure tensor
Sij =
i+D
X
m=i−D
j+D
X
n=j−D
wmn

h2
mn
hmnvmn
hmnvmn
v2
mn

,
(13.14)
where Sij is the image structure tensor at position (i,j), which is computed over a
square region of size (2D+1) × (2D+1) around the current position. The term hmn
denotes the response of a horizontal derivative ﬁlter (such as the Sobel) at position (m,n)
and the term vmn denotes the response of a vertical derivative ﬁlter. The term wmn
is a weight that diminishes the contribution of positions that are far from the central
pixel (i,j).
To identify whether a corner is present, the Harris corner detector considers the sin-
gular values λ1,λ2 of the image structure tensor. If both singular values are small, then

282
13 Image preprocessing and feature extraction
the region around the point is smooth and this position is not chosen. If one singular
value is large but the other small, then the image is changing in one direction but not the
other, and point lies on or near an edge. However, if both singular values are large, then
this image is changing rapidly in both directions in this region and the position is deemed
to be a corner.
In fact the Harris detector does not directly compute the singular values, but evaluates
a criterion which accomplishes the same thing more efﬁciently:
cij = λ1λ2 −κ(λ2
1 + λ2
2) = det[Sij] −κ · trace[Sij],
(13.15)
where κ is a constant (values between 0.04 and 0.15 are sensible). If the value of cij is
greater than a predetermined threshold, then a corner may be assigned. There is usually
an additional non-maximum suppression stage similar to that in the Canny edge detector
to ensure that only peaks in the function cij are retained.
13.2.3 SIFT detector
The scale invariant feature transform (SIFT) detector is a second method for identifying
interest points. Unlike the Harris corner detector, it associates a scale and orientation to
each of the resulting interest points. To ﬁnd the interest points a number of operations
are performed in turn.
The intensity image is ﬁltered with a difference of Gaussian kernel at a series of K
increasingly coarse scales (Figure 13.13). Then the ﬁltered images are stacked to make
a 3D volume of size I × J × K, where I and J are the vertical and horizontal size of
the image. Extrema are identiﬁed within this volume: these are positions where the 26
3D voxel neighbors (from a 3×3×3 block) are either all greater than or all less than the
current value.
These extrema are localized to subvoxel accuracy, by applying a local quadratic
approximation and returning the position of the peak or trough. The quadratic approx-
imation is made by taking a Taylor expansion about the current point. This provides a
position estimate that has subpixel resolution and an estimate of the scale that is more
accurate than the resolution of the scale sampling. Finally, the image structure tensor Sij
(Equation 13.14) is computed at the location and scale of each point. Candidate points in
smooth regions and on edges are removed by considering the singular values of Sij as in
the Harris corner detector (Figure 13.14).
This procedure returns a set of interest points that are localized to subpixel accu-
racy and associated accurately with a particular scale. Finally, a unique orientation is
also assigned to each interest point. To this end, the amplitude and orientation of the
local gradients are computed (Equations 13.13) in a region surrounding the interest point
whose size is proportional to the identiﬁed scale. An orientation histogram is then com-
puted over this region with 36 bins covering all 360o of orientation. The contribution to
the histogram depends on the gradient amplitude and is weighted by a Gaussian proﬁle
centered at the location of the interest point, so that nearby regions contribute more. The
orientation of the interest point is assigned to be the peak of this histogram. If there is a
second peak within 80 percent of the maximum, we may choose to compute descriptors
at two orientations at this point. The ﬁnal detected points are hence associated with a
particular orientation and scale (Figure 13.15).

13.3
Descriptors
283
Figure 13.13 The SIFT detector. a) Original image. b–h) The image is ﬁltered with difference of
Gaussian kernels at a range of increasing scales. i) The resulting images are stacked to create a 3D
volume. Points that are local extrema in the ﬁltered image volume (i.e., are either greater than or
less than all 26 3D neighbors) are considered to be candidates for interest points.
a)
b)
c)
Figure 13.14 Reﬁnement of SIFT detector candidates. a) Positions of extrema in the ﬁltered
image volume (Figure 13.13i). Note that the scale is not shown. These are considered candi-
dates to be interest points. b) Remaining candidates after eliminating those in smooth regions.
c) Remaining candidate points after removing those on edges using the image structure tensor.
13.3
Descriptors
In this section, we consider descriptors. These are compact representations that sum-
marize the contents of an image region.
13.3.1 Histograms
The simplest approach to aggregating information over a large image region is to compute
a histogram of the responses in this area. For example, we might collate RGB pixel
intensities, ﬁlter responses, local binary patterns, or textons into a histogram depending
on the application. The histogram entries can be treated as discrete and modeled with a
categorical distribution, or treated as a continuous vector quantity.
For continuous quantities such as ﬁlter responses, the level of quantization is critical.
Quantizing the responses into many bins potentially allows ﬁne discrimination between
responses. However, if data are scarce, then many of these bins will be empty, and it is
harder to reliably determine the statistics of the descriptor. One approach is to use an
adaptive clustering method such as K-means (Section 13.4.4) to automatically determine
the bin sizes and shapes.

284
13 Image preprocessing and feature extraction
Figure 13.15 Results of SIFT detector.
Each ﬁnal interest point is indicated using
an arrow. The length of the arrow indicates
the scale with which the interest point is
identiﬁed and the angle of the arrow indi-
cates the associated orientation. Notice that
there are some positions in the image where
the orientation was not unique and here two
interest points are used, one associated with
each orientation. An example of this is on
the right shirt collar. Subsequent descrip-
tors that characterize the structure of the
image around the interest points are com-
puted relative to this scale and orientation
and hence inherit some invariance to these
factors.
Histogramming is a useful approach for tasks where spatial resolution is not
paramount: for example, to classify a large region of texture, it makes sense to pool
information. However, this approach is largely unsuitable for characterizing structured
objects: the spatial layout of the object is important for identiﬁcation. We now intro-
duce two representations for image regions that retain some spatial information but also
pool information locally and thus provide invariance to small displacements and warps
of the image. Both the SIFT descriptor (Section 13.3.2) and the HOG descriptor (Section
13.3.3) concatenate several histograms that were computed over spatially distinct blocks.
13.3.2 SIFT descriptors
The scale invariant feature transform descriptor (Figure 13.16) characterizes the image
region around a given point. It is usually used in conjunction with interest points that
were found using the SIFT detector. These interest points are associated with a particular
scale and rotation, and the SIFT descriptor would typically be computed over a square
region that is transformed by these values. The goal is to characterize the image region
in a way that is partially invariant to intensity and contrast changes and small geometric
deformations.
To compute the SIFT descriptor, we ﬁrst compute gradient orientation and ampli-
tude maps (Equation 13.13) as for the Canny edge detector over a 16 × 16 pixel region
around the interest point. The resulting orientation is quantized into eight bins spread
over the range 0o–360o. Then the 16×16 detector region is divided into a regular grid of
nonoverlapping 4 × 4 cells. Within each of these cells, an eight-dimensional histogram
of the image orientations is computed. Each contribution to the histogram is weighted
by the associated gradient amplitude and by distance so that positions further from the
interest point contribute less. The 4 × 4 = 16 histograms are concatenated to make a
single 128 × 8 vector, which is then normalized.
The descriptor is invariant to constant intensity changes as it is based on gradi-
ents. The ﬁnal normalization provides some invariance to contrast. Small deformations
do not affect the descriptor too much as it pools information within each cell. How-
ever, by keeping the information from each cell separate, some spatial information is
retained.

13.3
Descriptors
285
Figure 13.16 SIFT descriptor. a) Gradients are computed for every pixel within a region around
the interest point. b) This region is subdivided into cells. Information is pooled within these cells
to form an 8D histogram. These histograms are concatenated to provide a ﬁnal descriptor that
pools locally to provide invariance to small deformations but also retains some spatial information
about the image gradients. In this ﬁgure, information from an 8 × 8 pixel patch has been divided
to make a 2 × 2 grid of cells. In the original implementation of the SIFT detector, a 16 × 16 patch
was divided into at 4 × 4 grid of cells.
13.3.3 Histogram of oriented gradients
The Histogram of Oriented Gradients (HOG) descriptor attempts to construct a more
detailed characterization of the spatial structure with a small image window. It is a useful
preprocessing step for algorithms that detect objects with quasi-regular structure such
as pedestrians. Like the SIFT descriptor, the HOG descriptor consists of a collection of
normalized histograms computed over spatially offset patches; the result is a descriptor
that captures coarse spatial structure, but is invariant to small local deformations.
The process of computing a HOG descriptor suitable for pedestrian detection consists
of the following stages. First, the orientation and amplitude of the image gradients are
computed at every pixel in a 64 × 128 window using Equation 13.13. The orientation is
quantized into nine bins spread over the range 0o–180o. The 64 × 128 detector region
is divided into a regular grid of overlapping 6 × 6 cells. A 9D orientation histogram is
computed within each cell, where the contribution to the histogram is weighted by the
gradient amplitude and the distance from the center of the cell so that more central pixels
contribute more. For each 3 × 3 block of cells, the descriptors are concatenated and
normalized to form a block descriptor. All of the block descriptors are concatenated to
form the ﬁnal HOG descriptor.
The ﬁnal descriptor contains spatially pooled information about local gradients
(within each cell), but maintains some spatial resolution (as there are many cells). It
creates invariance to contrast polarity by only using the gradient magnitudes. It creates
invariance to local contrast strength by normalizing relative to each block. The HOG
descriptor is similar in spirit to the SIFT descriptor but is distinguished by being invari-
ant to contrast polarity, having a higher spatial resolution of computed histograms and
performing normalization more locally.
13.3.4 Bag of words descriptor
The descriptors discussed thus far have been intended to characterize small regions of
images. Often these regions have been connected to interest points. The bag of words

286
13 Image preprocessing and feature extraction
Figure 13.17 HOG descriptor. a) Original image. b) Gradient orientation, quantized into nine
bins from 0o to 180o. c) Gradient magnitude. d) Cell descriptors are 9D orientation histograms
that are computed within 6 × 6 pixel regions. e) Block descriptors are computed by concatenating
3 × 3 blocks of cell descriptors. The block descriptors are normalized. The ﬁnal HOG descriptor
consists of the concatenated block descriptors.
representation attempts to characterize a larger region or an entire image by summarizing
the statistics of the descriptors (e.g., SIFT) associated with all of the interest points in a
region.
Each observed descriptor is considered to be one of a ﬁnite vocabulary of possible
descriptors (termed visual words). Collectively, this vocabulary is known as a dictio-
nary. The bag of words descriptor is simply a histogram describing the frequency of
observing these words giving no regard to their position. To compute the dictionary,
interest points are found in a large number of images and the associated descriptor is
computed. These descriptors are clustered using K-means (Section 13.4.4). To compute
the bag of words representation, each descriptor is assigned to the nearest word in this
dictionary.
The bag of words representation is a remarkably good substrate for object recogni-
tion. This is somewhat surprising given that it surrenders any knowledge about the spatial
conﬁguration about the object. Of course, the drawback of approaches based on the bag
of words is that it is very hard to localize the object after we have identiﬁed its presence
or to decide how many instances are present using the same model.
13.3.5 Shape context descriptor
For certain vision tasks, the silhouette of the object contains much more information than
the RGB values themselves. Consider, for example, the problem of body pose estimation:
given an image of a human being, the goal is to estimate the 3D joint angles of the body.
Unfortunately, the RGB values of the image depend on the person’s clothing and are
relatively uninformative. In such situations, it is wiser to attempt to characterize the
shape of the object.
The shape context descriptor is a ﬁxed length vector that characterizes the object
contour. Essentially, it encodes the relative position of points on the contour. In common
with the SIFT and HOG descriptors, it pools information locally over space to provide a
representation that can capture the overall structure of the object but is not affected too
much by small spatial variations.
To compute the shape context descriptor (Figure 13.18), a discrete set of points is
sampled along the contour of the object. A ﬁxed length vector is associated with each

13.4
Dimensionality reduction
287
a)
b)
c)
d)
e)
Figure 13.18 Shape context descriptor. a) Object silhouette. b) Contour of silhouette. c) Points
are placed at equally spaced intervals around the silhouette. d) A log polar sampling array is
centered at each point. e) The shape of the object relative to this point is captured by the histogram
over the bins of the log polar array. The ﬁnal descriptor would consist of a concatenation of the
values from histograms from multiple points around the edge of the object.
point that characterizes the relative position of the other points. To this end, a log polar
sampling array is centered on the current point. A histogram is then computed where each
bin contains the number of the other points on the silhouette that fell into each bin of the
log-polar array. The choice of the log-polar scheme means that the descriptor is very
sensitive to local changes in the shape, but only captures the approximate conﬁguration
of distant parts.
The collection of histograms for all of the points on this image captures the shape.
However, to directly match to another shape, the point correspondence must be estab-
lished. It is possible to make this descriptor invariant to orientation by evaluating the
orientation of the contour at each point and rotating the log polar sampling scheme so
that it is aligned with this orientation.
13.4
Dimensionality reduction
It is often desirable to reduce the dimensionality of either the original or pre-processed
image data. If we can do this without losing too much information, then the resulting
models will require fewer parameters and be faster to learn and to use for inference.
Dimensionality reduction is possible because a given type of image data (e.g., RGB
values from face images) usually lie in a tiny subset of the possible data space; not all sets
of RGB values look like real images, and not all real images look like faces. We refer
to the subset of the space occupied by a given dataset as a manifold. Dimensionality
reduction can hence be thought of as a change of variables: we move from the original
coordinate system to the (reduced) coordinate system within the manifold.
Our goal is hence to ﬁnd a low-dimensional (or hidden) representation h, which can
approximately explain the data x, so that
x ≈f[h,θ],
(13.16)

288
13 Image preprocessing and feature extraction
Figure 13.19 Reduction to a single dimension. a) Original data and direction φ of maximum
variance. b) The data are projected onto φ to produce a one-dimensional representation. c) To
reconstruct the data, we remultiply by φ. Most of the original variation is retained. PCA extends
this model to project high-dimensional data onto the K orthogonal dimensions with the most
variance, to produce a K-dimensional representation.
where f[•,•] is a function that takes the hidden variable and a set of parameters θ. We
would like the lower-dimensional representation to capture all of the relevant variation
in the original data. Hence, one possible criterion for choosing the parameters is to
minimize the least squares reconstruction error so that
ˆθ, ˆh1...I = argmin
θ,h1...I
" I
X
i=1
(xi −f[hi,θ])T (xi −f[hi,θ])
#
,
(13.17)
where xi is the ith of I training examples. In other words, we aim to ﬁnd a set of low-
dimensional variables {hi}I
i=1 and a mapping from h to x so that it reconstructs the
original data as closely as possible in a least squares sense.
13.4.1 Approximation with a single number
Let us ﬁrst consider a very simple model in which we attempt to represent each observed
datum with a single number (Figure 13.19) so that
xi ≈φhi + µ,
(13.18)
where the parameter µ is the mean of the data is zero and the parameter φ is a basis
vector mapping the low-dimensional representation h back to the original data space x.
For simplicity, we will assume from now on that the mean of the xi ≈φhi. This can be
achieved by computing the empirical mean µ and subtracting it from every example xi.
The learning algorithm optimizes the criterion
ˆφ,ˆh1...I = argmin
φ,h1...I
[E] = argmin
φ,h1...I
" I
X
i=1
(xi −φhi)T (xi −φhi)
#
.
(13.19)
Careful consideration of the cost function (Equation 13.19) reveals an immediate prob-
lem: the solution is ambiguous as we can multiply the basis function φ by any constant
k and divide each of the hidden variables {hi}I
i=1 by the same number to yield exactly
the same cost. To resolve this problem, we force the vector φ to have unit length. This is

13.4
Dimensionality reduction
289
accomplished by adding in a Lagrange multiplier λ so that the cost function becomes
E =
I
X
i=1
(xi −φhi)T (xi −φhi) + λ(φT φ −1)
=
I
X
i=1
xT
i xi −2hiφT xi + h2
i + λ(φT φ −1).
(13.20)
To minimize the function, we ﬁrst take the derivative with respect to hi and then equate
the resulting expression to zero to yield
ˆhi = ˆφ
T xi.
(13.21)
In other words, to ﬁnd the reduced dimension representation hi we simply project the
observed data onto the vector φ.
We now take the derivative of Equation 13.20 with respect to φ, substitute in the
solution for hi, equate the result to zero, and rearrange to get
I
X
i=1
xixT
i ˆφ = λˆφ,
(13.22)
or in matrix form
XXT ˆφ = λˆφ,
(13.23)
where the matrix X = [x1,x2,...,xI] contains the data examples in its columns. This is
an eigenvalue problem. To ﬁnd the optimal vector, we compute the SVD ULVT = XXT
and choose the ﬁrst column of U.
The scatter matrix XXT is a constant multiple of the covariance matrix, and so this
has a simple geometric interpretation. The optimal vector φ to project onto corresponds
to the principal direction of the covariance ellipse. This makes intuitive sense; we retain
information from the direction in space where the data vary most.
13.4.2 Principal component analysis
Principal component analysis (PCA) generalizes the above model. Instead of ﬁnding a
scalar variable hi that represents the ith data example xi, we now seek a K-dimensional
vector hi. The relation between the hidden and observed spaces is
xi ≈Φhi,
(13.24)
where the matrix Φ = [φ1,φ2,...,φK] contains K basis functions or principal compo-
nents; the observed data are modeled as a weighted sum of the principal components,
where the kth dimension of hi weights the kth component.
The solution for the unknowns Φ and h1...I can now be written as
ˆ
Φ,h1...I = argmin
Φ,h1...I
[E] = argmin
Φ,h1...I
" I
X
i=1
(xi −Φhi)T (xi −Φhi)
#
.
(13.25)
Once more, the solution to this is nonunique as we can postmultiply Φ by any matrix
A and premultiply each hidden variable hi by the inverse A−1 and still get the same

290
13 Image preprocessing and feature extraction
cost. To (partially) resolve this problem, we add the extra constraint that ΦT Φ = I. In
other words, we force the principal components to be orthogonal and length one. This
gives a modiﬁed cost function of
E =
I
X
i=1
(xi −Φhi)T (xi −Φhi) + λ(ΦT Φ −I),
(13.26)
where λ is a Lagrange multiplier. We now minimize this expression with respect to
Φ,h1...I and λ. The expression for the hidden variables becomes
hi = ΦT xi,
(13.27)
The K principal components Φ = [φ1,φ2,...,φK] are now found by computing the
singular value decomposition ULVT = XXT and taking the ﬁrst K columns of U.
In other words, to reduce the dimensionality, we project the data xi onto a hyperplane
deﬁned by the K largest axes of the covariance ellipsoid.
This algorithm is very closely related to probabilistic principal component analysis
(Section 17.5.1). Probabilistic PCA additionally models the noise that accounts for the
inexact approximation in Equation 13.24.
Factor analysis (Section 7.6) is also very
similar, but constructs a more sophisticated model of this noise.
13.4.3 Dual principal component analysis
The method outlined in Section 13.4.2 requires us to compute the SVD of the scatter
13.1 matrix XXT . Unfortunately, if the data had dimension D, then this is a D×D matrix,
which may be very large. We can sidestep this problem by using dual variables. We
deﬁne Φ as a weighted sum of the original datapoints so that
Φ = XΨ,
(13.28)
where Ψ = [ψ1,ψ2,...,ψK] is a I × K matrix representing these weights. The
associated cost function now becomes
E =
I
X
i=1
(xi −XΨhi)T (xi −XΨhi) + λ(ΨT XT XΨ −I).
(13.29)
The solution for the hidden variables becomes
hi = ΨT XT xi = ΦT xi,
(13.30)
and the K dual principal components Ψ = [ψ1,ψ2 ...,ψK] are extracted from the
matrix U in the SVD ULVT = XT X. This is a smaller problem of size I × I and
so is more efﬁcient when the number of data examples I is less than the dimensionality
of the observed space D.
Notice that this algorithm does not require the original datapoints: it only requires
the inner products between them and so it is amenable to kernelization. This resulting
method is known as kernel PCA.

13.4
Dimensionality reduction
291
Figure 13.20 K-means algorithm for K = 3 clusters. a) We initialize the three prototype vectors
(crosses) to random positions. We alternately b) assign the data to the nearest prototype vector and
c) update the prototype vectors to be equal to the mean of the points assigned to them. d–i) We
repeat these steps until there is no further change.
13.4.4 The K-means algorithm
A second common approach to dimensionality reduction is to abandon a continuous
13.2 representation altogether and represent the each datapoint using one of a limited set of
prototype vectors. In this model, the data are approximated as
xi ≈µhi,
(13.31)
where hi ∈{1,2,...,K} is an index that identiﬁes which of the K prototype vectors
{µk}K
k=1 approximates the ith example.
To ﬁnd the assignment indices and the prototype vectors (Figure 13.20), we optimize
ˆµ1...K,ˆh1...I = argmin
µ,h
" I
X
i=1
 xi −µhi
T  xi −µhi

#
.
(13.32)

292
13 Image preprocessing and feature extraction
In the K-means algorithm, this cost function is minimized using an alternating
strategy in which we ﬁrst assign each datapoint to the nearest prototype
ˆhi = argmin
hi
h xi −µhi
T  xi −µhi
i
,
(13.33)
and then update the prototypes
ˆµk = argmin
µk
" I
X
i=1
h xi −µhi
T  xi −µhi
i#
=
PI
i=1 xiδ[hi −k]
PI
i=1 δ[hi −k]
,
(13.34)
where δ[•] is a function that returns one when its argument is zero and zero otherwise.
In other words, the new prototype ˆ
µk is simply the average of the datapoints that are
assigned to this cluster.
This algorithm is not guaranteed to converge to the global
minimum and so it requires sensible starting conditions.
The K-means algorithm is very closely related to the mixtures of Gaussians model
(Section 7.4). The main differences are that the mixtures of Gaussians model is proba-
bilistic and deﬁnes a density over the data space. It also assigns weights to the clusters
and describes their covariance.
Conclusion
Careful reading of the information in this chapter should convince you that there are cer-
tain recurring ideas in image preprocessing. To make a descriptor invariant to intensity
changes we ﬁlter the image and normalize the ﬁlter responses over the region. A unique
descriptor orientation and scale can be computed by maximizing over responses at differ-
ent orientations and scales. To create invariance to small spatial changes, local responses
are pooled. Despite the simplicity of these ideas, it is remarkable how much impact they
have on the performance of real systems.
Notes
Image Processing: There are numerous texts on image processing which contain far more infor-
mation than I could include in this chapter.
I would particularly recommend the books by
O’Gorman et al. (2008), Gonzalez and Woods (2002), Pratt (2007), and Nixon and Aguado (2008).
A comprehensive recent summary of local image features can be found in Li and Allinson (2008).
Edge and corner detection: The Canny edge detector was ﬁrst described in Canny (1986). Elder
(1999) investigated whether it was possible to reconstruct an image based on edge information
alone. Nowadays, it is common to use machine learning methods to identify object boundaries in
images (e.g., Doll´ar et al. 2006)
Early work in corner detection (interest point detection) includes that of Moravec (1983), F¨orstner
(1986), and the Harris corner detector (Harris and Stephens 1988), which we described in this
chapter. Other more recent efforts to identify stable points and regions include the SUSAN corner
detector (Smith and Brady 1997), a saliency-based descriptor (Kadir and Brady 2001), maximally
stable extremal regions (Matas et al. 2002), the SIFT detector (Lowe 2004), and the FAST detec-
tor (Rosten and Drummond 2006). There has been considerable recent interest in afﬁne invariant
interest point detection, which aims to ﬁnd features that are stable under afﬁne transformations of
the image (e.g., Schaffalitzky and Zisserman 2002; Mikolajczyk and Schmid 2002, 2004). Mikola-
jczyk et al. (2005) present a quantitative comparison of different afﬁne region detectors. A recent
review of this area can be found in Tuytelaars and Mikolajczyk (2007).

Problems
293
Image descriptors: For robust object recognition and image matching, it is crucial to characterize
the region around the detected interest point in a way that is compact and stable to changes in
the image. To this end, Lowe (2004) developed the SIFT descriptor, Dalal and Triggs (2005)
developed the HOG descriptor, and Forss´en and Lowe (2007) developed a descriptor for use with
maximally stable extremal regions. Bay et al. (2008) developed a very efﬁcient version of SIFT
features known as SURF. Mikolajczyk and Schmid (2005) present a quantitative comparison of
region descriptors. Recent work on image descriptors has applied machine learning techniques to
optimize their performance (Brown et al. 2011; Philbin et al. 2010).
More information about local binary patterns can be found in Ojala et al. (2002). More information
about the shape context descriptor can be found in Belongie et al. (2002).
Dimensionality reduction: Principal components analysis is a linear dimensionality reduction
method. However, there are also many nonlinear approaches that describe a manifold of images in
high dimensions with fewer parameters. Notable methods include kernel PCA (Sch¨olkopf et al.
1997), ISOMAP (Tenenbaum et al. 2000), local linear embedding (Roweis and Saul 2000), chart-
ing (Brand 2002), the Gaussian process latent variable model (Lawrence 2004), and Laplacian
eigenmaps (Belkin and Niyogi 2001). Recent reviews of dimensionality reduction can be found in
Burgess (2010) and De La Torre (2011).
Problems
13.1 Consider an eight-bit image in which the pixel values are evenly distributed in the range
0–127, with no pixels taking a value of 128 or larger. Draw the cumulative histogram for this
image (see Figure 13.2). What will the histogram of pixel intensities look like after applying
histogram equalization?
13.2 Consider a continuous image p[i,j] and a continuous ﬁlter f[n,m] In the continuous domain,
the operation f ⊗p of convolving an image with the ﬁlter is deﬁned as
f ⊗p =
Z ∞
−∞
Z ∞
−∞
p[i −m,n −j]f[n,m] dndm.
Now consider two ﬁlters f and g. Prove that convolving the image ﬁrst with f and then with g
has the same effect as convolving f with g and then convolving the image with the result. In other
words:
g ⊗(f ⊗p) = (g ⊗f) ⊗p.
Does this result extend to discrete images?
13.3 Describe the series of operations that would be required to compute the Haar ﬁlters in Figures
13.6a–d from an integral image. How many points from the integral image are needed to compute
each?
13.4 Consider a blurring ﬁlter where each pixel in an image is replaced by a weighted average of
local intensity values, but the the weights decrease if these intensity values differ markedly from
the central pixel. What effect would this bilateral ﬁlter have when applied to an image?
13.5 Deﬁne a 3 × 3 ﬁlter that is specialized to detecting luminance changes at a 45o angle and
gives a positive response where the image intensity increases from the bottom left to the bottom
right of the image.
13.6 Deﬁne a 3 × 3 ﬁlter that responds to the second derivative in the horizontal direction but
is invariant to the gradient and absolute intensity in the horizontal direction and invariant to all
changes in the vertical direction.
13.7 Why are most local binary patterns in a natural image typically uniform or near-uniform?

294
13 Image preprocessing and feature extraction
Figure 13.21 Clustering with the K-means
algorithm in the presence of outliers (Prob-
lem 13.9).
This data set contains two
clusters and a single outlier (the point on
the right-hand side).
The outlier causes
problems for the K-means algorithm when
K = 2 clusters are used due to the implicit
assumption that the clusters can be mod-
eled as normal distributions with spherical
covariance.
13.8 Give one example of a 2D data set where the mixtures of Gaussians model will succeed in
clustering the data, but the K-means algorithm will fail.
13.9 Consider the data in Figure 13.21. What do you expect to happen if we run the K-means
algorithm on this data set? Suggest a way to resolve this problem.
13.10 An alternative approach to clustering the data would be to ﬁnd modes (peaks) in the density
of the points. This potentially has the advantage of also automatically selecting the number of
clusters. Propose an algorithm to ﬁnd these modes.

Part V
Models for geometry

In Part V, we ﬁnally acknowledge the process by which real-world images are formed.
Light is emitted from one or more sources and travels through the scene, interacting with
the materials via physical processes such as reﬂection, refraction, and scattering. Some of
this light enters the camera and is measured. We have a very good understanding of this
forward model. Given known geometry, light sources, and material properties, computer
graphics techniques can simulate what will be seen by the camera very accurately.
The ultimate goal for a vision algorithm would be a complete reconstruction, in
which we aim to invert this forward model and estimate the light sources, materials,
and geometry from the image. Here, we aim to capture a structural description of the
world: we seek an understanding of where things are and to measure their optical proper-
ties, rather than a semantic understanding. Such a structural description can be exploited
to navigate around the environment or build 3D models for computer graphics.
Unfortunately, full visual reconstruction is very challenging. For one thing, the solu-
tion is nonunique. For example, if the light source intensity increases, but the object
reﬂectance decreases commensurately, the image will remain unchanged. Of course, we
could make the problem unique by imposing prior knowledge, but even then reconstruc-
tion remains difﬁcult; it is hard to effectively parameterize the scene, and the problem is
highly non-convex.
In this part of the book, we consider a family of models that approximate both the
3D scene and the observed image with sparse sets of visual primitives (points). The
forward model that maps the proxy representation of the world (3D points) to the proxy
representation of the image (2D points) is much simpler than the full light transport
model, and is called the projective pinhole camera. We investigate the properties of this
model in Chapter 14.
In Chapter 15, we consider the situation where the pinhole camera views a plane in
the world; there is now a one-to-one mapping between points on the plane and points
in the image, and we characterize this mapping with a family of 2D transformations.
In Chapter 16, we will further exploit the pinhole camera model to recover a sparse
geometric model of the scene.

Chapter 14
The pinhole camera
This chapter introduces the pinhole or projective camera. This is a purely geometric
model that describes the process whereby points in the world are projected into the image.
Clearly, the position in the image depends on the position in the world, and the pinhole
camera model captures this relationship.
To motivate this model, we will consider the problem of sparse stereo reconstruction
(Figure 14.1). We are given two images of a rigid object taken from different positions.
Let us assume that we can identify corresponding 2D features between the two images –
points that are projected versions of the same position in the 3D world. The goal now
is to establish this 3D position using the observed 2D feature points. The resulting 3D
information could be used by a robot to help it navigate through the scene, or to facilitate
object recognition.
14.1
The pinhole camera
In real life, a pinhole camera consists of a chamber1 with a small hole (the pinhole) in
the front (Figure 14.2). Rays from an object in the world pass through this hole to form
an inverted image on the back face of the box, or image plane. Our goal is to build a
mathematical model of this process.
It is slightly inconvenient that the image from the pinhole camera is upside-down.
Hence, we instead consider the virtual image that would result from placing the image
plane in front of the pinhole. Of course, it is not physically possible to build a camera
this way, but it is mathematically equivalent to the true pinhole model (except that the
image is the right way up) and it is easier to think about. From now on, we will always
draw the image plane in front of the pinhole.
Figure 14.3 illustrates the pinhole camera model and deﬁnes some terminology. The
pinhole itself (the point at which the rays converge) is called the optical center. We will
assume for now that the optical center is at the origin of the 3D world coordinate system,
in which points are represented as w = [u,v,w]T . The virtual image is created on the
image plane, which is displaced from the optical center along the w-axis or optical axis.
The point where the optical axis strikes the image plane is known as the principal point.
The distance between the principal point and the optical center (i.e., the distance between
the image plane and the pinhole) is known as the focal length.
1This is not an accidental choice of word. The term camera is derived from the Latin word for ‘chamber.’

298
14 The pinhole camera
Figure 14.1 Sparse stereo reconstruction. a,b) We are given two images of the same scene taken
from different positions, and a set of I pairs of points in these images that are known to correspond
to the same points in the world (e.g., the points connected by the red line are a corresponding pair).
c) Our goal is to establish the 3D position of each of the world points. Here, the depth is encoded
by color so that closer points are red and more distant points are blue.
Figure 14.2 The pinhole camera model. Rays from an object in the world pass through the
pinhole in the front of the camera and form an image on the back plane (the image plane). This
image is upside-down, so we can alternatively consider the virtual image that would have been
created if the image plane was in front of the pinhole. This is not physically possible, but it is more
convenient to work with.
The pinhole camera model is a generative model that describes the likelihood
Pr(x|w) of observing a feature at position x = [x,y]T in the image given that it is
the projection of a 3D point w = [u,v,w]T in the world. Although light transport is
essentially deterministic, we will nonetheless build a probability model; there is noise
in the sensor, and unmodeled factors in the feature detection process can also affect the
measured image position. However, for pedagogical reasons we will defer a discussion
of this uncertainty until later, and temporarily treat the imaging process as if it were
deterministic.
Our task then is to establish the position x = [x,y]T where the 3D point w = [u,v,w]T
is imaged. Considering Figure 14.3 it is clear how to do this. We connect a ray between
w and the optical center. The image position x can be found by observing where this
ray strikes the image plane. This process is called perspective projection. In the next few
sections, we will build a more precise mathematical model of this process. We will start
with a very simple camera model (the normalized camera) and build up to a full camera
parameterization.

14.1
The pinhole camera
299
Figure 14.3 Pin-hole camera model terminology. The optical center (pinhole) is placed at the
origin of the 3D world coordinate system (u,v,w) and the image plane (where the virtual image
is formed) is displaced along the w-axis, which is also known as the optical axis. The position
where the optical axis strikes the image plane is called the principal point. The distance between
the image plane and the optical center is called the focal length.
14.1.1 The normalized camera
In the normalized camera, the focal length is one, and it is assumed that the origin of
the 2D coordinate system (x,y) on the image plane is centered at the principal point.
Figure 14.4 shows a 2D slice of the geometry of this system (the u- and x-axes now
point upward out of the page and cannot be seen). By similar triangles, it can easily be
seen that the y-position in the image of the world point at w = [u,v,w]T is given by v/w.
More generally, in the normalized camera, a 3D point w = [u,v,w]T is projected into the
image at x = [x,y]T using the relations
x = u
w
y = v
w,
(14.1)
where x,y,u,v, and w are measured in the same real-world units (e.g., mm).
14.1.2 Focal length parameters
The normalized camera is unrealistic; for one thing, in a real camera, there is no particular
reason why the focal length should be one. Moreover, the ﬁnal position in the image is
measured in pixels, not physical distance, and so the model must take into account the
photoreceptor spacing. Both of these factors have the effect of changing the mapping
between points w = [u,v,w]T in the 3D world and their 2D positions x = [x,y]T in the

300
14 The pinhole camera
Figure 14.4 Normalized camera.
The
focal length is one, and the 2D image coor-
dinate system (x,y) is centered on the prin-
cipal point (only y-axis shown). By similar
triangles, the y position in the image of a
point at (u,v,w) is given by v/w. This cor-
responds to our intuition: as an object gets
more distant, its projection becomes closer
to the center of the image.
image plane by a constant scaling factor φ (Figure 14.5) so that
x = φu
w
y = φv
w .
(14.2)
To add a further complication, the spacing of the photoreceptors may differ in the x-
and y-directions, so the scaling may be different in each direction, giving the relations
x = φxu
w
y = φyv
w ,
(14.3)
where φx and φy are separate scaling factors for the x- and y-directions. These parame-
ters are known as the focal length parameters in the x- and y-directions, but this name is
somewhat misleading – they account for not just the distance between the optical center
and the principal point (the true focal length) but also the photoreceptor spacing.
14.1.3 Oﬀset and skew parameters
The model so far is still incomplete in that pixel position x = [0,0]T is at the principal
point (where the w-axis intersects the image plane). In most imaging systems, the pixel
position x = [0,0]T is at the top-left of the image rather than the center. To cope with
this, we add offset parameters δx and δy so that
x = φxu
w + δx
y = φyv
w + δy,
(14.4)
where δx and δy are the offsets in pixels from the top-left corner of the image to the
position where the w-axis strikes the image plane. Another way to think about this is that
the vector [δx,δy]T is the position of the principal point in pixels.
If the image plane is exactly centered on the w-axis, these offset parameters should be
half the image size: for a 640×480 VGA image δx and δy would be 320 and 240, respec-
tively. However, in practice it is difﬁcult and superﬂuous to manufacture cameras with

14.1
The pinhole camera
301
Figure 14.5 Focal length and photoreceptor spacing. a–b) Changing the distance between the
optical center and the image plane (the focal length) changes the relationship between the 3D world
point w = [u,v,w]T and the 2D image point x = [x,y]T . In particular, if we take the original focal
length (a) and halve it (b), the 2D image coordinate is also halved. The ﬁeld of view of the camera
is the total angular range that is imaged (usually different in the x- and y-directions). When the
focal length decreases, the ﬁeld of view increases. c–d) The position in the image x = [x,y]T
is usually measured in pixels. Hence, the position x depends on the density of the receptors on
the image plane. If we take the original photoreceptor density (c) and halve it (d), then the 2D
image coordinate is also halved. Hence, the photoreceptor spacing and focal length both change
the mapping from rays to pixels in the same way.
the imaging sensor perfectly centered, and so we treat the offset parameters as unknown
quantities.
We also introduce a skew term γ that moderates the projected position x as a function
of the height v in the world. This parameter has no clear physical interpretation but can
help explain the projection of points into the image in practice. The resulting camera
model is
x = φxu + γv
w
+ δx
y = φyv
w + δy.
(14.5)
14.1.4 Position and orientation of camera
Finally, we must account for the fact that the camera is not always conveniently centered
at the origin of the world coordinate system with the optical axis exactly aligned with the
w-axis. In general, we may want to deﬁne an arbitrary world coordinate system that may
be common to more than one camera. To this end, we express the world points w in the

302
14 The pinhole camera
coordinate system of the camera before they are passed through the projection model,
using the coordinate transformation:


u′
v′
w′

=


ω11
ω12
ω13
ω21
ω22
ω23
ω31
ω32
ω33




u
v
w

+


τx
τy
τz

,
(14.6)
or
w′ = Ωw + τ,
(14.7)
where w′ is the transformed point, Ωis a 3×3 rotation matrix, and τ is a 3×1 translation
vector.
14.1.5 Full pinhole camera model
We are now in a position to describe the full camera model, by combining Equations
14.5 and 14.6. A 3D point w = [u,v,w]T is projected to a 2D point x = [x,y]T by the
relations
x = φx(ω11u + ω12v + ω13w + τx) + γ(ω21u + ω22v + ω23w + τy)
ω31u + ω32v + ω33w + τz
+ δx
y = φy(ω21u + ω22v + ω23w + τy)
ω31u + ω32v + ω33w + τz
+ δy.
(14.8)
There are two sets of parameters in this model.
The intrinsic or camera parame-
ters {φx,φy,γ,δx,δy} describe the camera itself, and the extrinsic parameters {Ω,τ}
describe the position and orientation of the camera in the world. For reasons that will
become clear in Section 14.3.1, we will store the intrinsic parameters in the intrinsic
matrix Λ where
Λ =


φx
γ
δx
0
φy
δy
0
0
1

.
(14.9)
We can now abbreviate the full projection model (Equations 14.8) by just writing
x = pinhole[w,Λ,Ω,τ].
(14.10)
Finally, we must account for the fact that the estimated position of a feature in the
image may differ from our predictions. There are a number of reasons for this, including
noise in the sensor, sampling issues, and the fact that the detected position in the image
may change at different viewpoints. We model these factors with additive noise that is
normally distributed with a spherical covariance to give the ﬁnal relation
Pr(x|w,Λ,Ω,τ) = Normx

pinhole[w,Λ,Ω,τ],σ2I

,
(14.11)
where σ2 is the variance of the noise.
Note that the pinhole camera is a generative model. We are describing the likelihood
Pr(x|w,Λ,Ω,τ) of observing a 2D image point x given the 3D world point w and the
parameters {Λ,Ω,τ}.

14.1
The pinhole camera
303
Figure 14.6 Radial distortion. The pinhole model is only an approximation of the true imaging
process. One important deviation from this model is a 2D warping in which points deviate from
their expected positions by moving along radial lines from the center of the image by an amount
that depends on the distance from the center. This is known as radial distortion. a) An image
that suffers from radial distortion is easily spotted because lines that were straight in the world are
mapped to curves in the image (e.g., red dotted line). b) After applying the inverse radial distortion
model, straight lines in the world now correctly map to straight lines in the image. The distortion
caused the magenta point to move along the red radial line to the position of the yellow point.
14.1.6 Radial distortion
In the previous section, we introduced the pinhole camera model. However, it has prob-
ably not escaped your attention that real-world cameras are rarely based on the pinhole:
they have a lens (or possibly a system of several lenses) that collects light from a larger
area and refocuses it on the image plane. In practice, this leads to a number of deviations
from the pinhole model. For example, some parts of the image may be out of focus,
which essentially means that the assumption that a point in the world w maps to a single
point in the image x is no longer valid. There are more complex mathematical models
for cameras that deal effectively with this situation, but they are not discussed here.
However, there is one deviation from the pinhole model that must be addressed.
Radial distortion is a nonlinear warping of the image that depends on the distance from
the center of the image. In practice, this occurs when the ﬁeld of view of the lens system
is large. It can easily be detected in an image because straight lines in the world no longer
project to straight lines in the image (Figure 14.6).
Radial distortion is commonly modeled as a polynomial function of the distance r
from the center of the image. In the normalized camera, the ﬁnal image positions (x′,y′)
are expressed as functions of the original positions (x,y) by
x′ = x(1 + β1r2 + β2r4)
y′ = y(1 + β1r2 + β2r4),
(14.12)
where the parameters β1 and β2 control the degree of distortion. These relations describe
a family of possible distortions that approximate the true distortion closely for most
common lenses.
This distortion is implemented after perspective projection (division by w) but before
the effect of the intrinsic parameters (focal length, offset, etc.), so the warping is relative
to the optical axis and not the origin of the pixel coordinate system. We will not discuss
radial distortion further in this volume. However, it is important to realize that for accu-
rate results, all of the algorithms in this and Chapters 15 and 16 should account for radial

304
14 The pinhole camera
distortion. When the ﬁeld of view is large, it is particularly critical to incorporate this
into the pinhole camera model.
14.2
Three geometric problems
Now that we have described the pinhole camera model, we will consider three important
geometric problems. Each is an instance of learning or inference within this model. We
will ﬁrst describe the problems themselves, and then tackle them one by one later in the
chapter.
14.2.1 Problem 1: Learning extrinsic parameters
We aim to recover the position and orientation of the camera relative to a known scene.
This is sometimes known as the perspective-n-point (PnP) problem or the exterior orien-
tation problem. One common application is augmented reality, where we need to know
this relationship to render virtual objects that appear to be stable parts of the real scene.
The problem can be stated more formally as follows: we are given a known object,
with I distinct 3D points {wi}I
i=1, their corresponding projections in the image {xi}I
i=1,
and known intrinsic parameters Λ. Our goal is to estimate the rotation Ωand translation
τ that map points in the coordinate system of the object to points in the coordinate system
of the camera so that
ˆΩ, ˆτ = argmax
Ω,τ
" I
X
i=1
log[Pr(xi|wi,Λ,Ω,τ)]
#
.
(14.13)
This is a maximum likelihood learning problem, in which we aim to ﬁnd parameters
Ω,τ that make the predictions pinhole[wi,Λ,Ω,τ] of the model agree with the observed
2D points xi (Figure 14.7).
Figure 14.7 Problem 1 – Learning extrinsic parameters (exterior orientation problem). Given
points {wi}I
i=1 on a known object (blue lines), their positions {xi}I
i=1 in the image (circles on
image plane), and known intrinsic parameters Λ, ﬁnd the rotation Ωand translation τ relating
the camera and the object. a) When the rotation or translation are wrong, the image points pre-
dicted by the model (where the rays strike the image plane) do not agree well with the observed
points xi. b) When the rotation and translation are correct, they agree well and the likelihood
Pr(xi|w,Λ,Ω,τ) will be high.
14.2.2 Problem 2: Learning intrinsic parameters
We aim to estimate the intrinsic parameters Λ that relate the direction of rays through
the optical center to coordinates on the image plane. This estimation process is known

14.2
Three geometric problems
305
Figure 14.8 Problem 2 – Learning intrinsic parameters. Given a set of points {wi}I
i=1 on a
known object in the world (blue lines) and the 2D positions {x}I
i=1 of these points in an image,
ﬁnd the intrinsic parameters Λ. To do this, we must also simultaneously estimate the extrinsic
parameters Ω,τ. a) When the intrinsic or extrinsic parameters are wrong, the prediction of the
pinhole camera (where rays strike the image plane) will deviate signiﬁcantly from the observed 2D
points. b) When the intrinsic and extrinsic parameters are correct, the prediction of the model will
agree with the observed image.
as calibration. Knowledge of the intrinsic parameters is critical if we want to use the
camera to build 3D models of the world.
The calibration problem can be stated more formally as follows: given a known 3D
object, with I distinct 3D points {wi}I
i=1 and their corresponding projections in the
image {xi}I
i=1, estimate the intrinsic parameters:
ˆΛ = argmax
Λ
"
max
Ω,τ
" I
X
i=1
log[Pr(xi|wi,Λ,Ω,τ)]
##
.
(14.14)
Once more this is a maximum likelihood learning problem in which we aim to ﬁnd
parameters Λ,Ω,τ that make the predictions of the model pinhole[wi,Λ,Ω,τ] agree
with the observed 2D points xi (Figure 14.8). We do not particularly care about the
extrinsic parameters Ω,τ; ﬁnding these is just a means to the end of estimating the
intrinsic parameters Λ.
The calibration process requires a known 3D object, on which distinct points can be
identiﬁed, and their corresponding projections in the image found. A common approach
is to construct a bespoke 3D calibration target2 that achieves these goals (Figure 14.9).
14.2.3 Problem 3: Inferring 3D world points
We aim to estimate the 3D position of a point w in the scene, given its projections
{xj}J
j=1 in J ≥2 calibrated cameras. When J = 2, this is known as calibrated stereo
reconstruction. With J > 2 calibrated cameras, it is known to as multiview reconstruc-
tion. If we repeat this process for many points, the result is a sparse 3D point cloud.
This could be used to help an autonomous vehicle navigate through the environment or
to generate an image of the scene from a new viewpoint.
More formally, the multiview reconstruction problem can be stated as follows: given
J calibrated cameras in known positions (i.e., cameras with known Λ,Ω,τ) viewing
the same 3D point w and knowing the corresponding 2D projections {xj}J
j=1, in the J
2It should be noted that in practice calibration is more usually based on a number of views of a known 2D
planar object (see Section 15.4.2).

306
14 The pinhole camera
Figure
14.9 Camera calibration target.
One way to calibrate the camera (estimate its
intrinsic parameters) is to view a 3D object
(a camera calibration target) for which the
geometry is known.
The marks on the
surface are at known 3D positions in the
frame of reference of the object and are easy
to locate in the image using basic image-
processing techniques.
It is now possible
to ﬁnd the intrinsic and extrinsic parameters
that optimally map the known 3D positions
to their 2D projections in the image. Image
from Hartley and Zisserman (2004).
images establish the 3D position w of the point in the world:
ˆw = argmax
w


J
X
j=1
log[Pr(xj|w,Λj,Ωj,τ j)]

.
(14.15)
The form of this inference problem is similar to that of the preceding learning problems:
we perform an optimization in which we manipulate the variable of interest w until the
predictions pinhole[w,Λj,Ωj,τ j] of the pinhole camera models agree with the data xj
(Figure 14.10). For obvious reasons, the principle behind reconstruction is known as
triangulation.
14.2.4 Solving the problems
We have introduced three geometric problems, each of which took the form of a learning
or inference problem using the pinhole camera model. We formulated each in terms of
maximum likelihood estimation, and in each case this results in an optimization problem.
Unfortunately, none of the resulting objective functions can be optimized in closed
form; each solution requires the use of nonlinear optimization. In each case, it is critical
to have a good initial estimate of the unknown quantities to ensure that the optimization
process converges to the global maximum. In the remaining part of this chapter, we
develop algorithms that provide these initial estimates. The general approach is to choose
new objective functions that can be optimized in closed form, and where the solution is
close to the solution of the true problem.
14.3
Homogeneous coordinates
To get good initial estimates of the geometric quantities in the preceding optimization
problems, we play a simple trick: we change the representation of both the 2D image
points and 3D world points so that the projection equations become linear. After this
change, it is possible to ﬁnd solutions for the unknown quantities in closed form. How-
ever, it should be emphasized that these solutions do not directly address the original
optimization criteria: they minimize more abstract objective functions based on alge-
braic error whose solutions are not guaranteed to be the same as those for the original
problem. However, they are generally close enough to provide a good starting point for
a nonlinear optimization of the true cost function.

14.3
Homogeneous coordinates
307
Figure 14.10 Problem 3 – Inferring 3D world points. Given two cameras with known position
and orientation, and the projections x1 and x2 of the same 3D point in each image, the goal of
calibrated stereo reconstruction is to infer the 3D position w of the world point. a) When the
estimate of the world point (red circle) is wrong, the predictions of the pinhole camera model
(where rays strike the image plane) will deviate from the observed data (brown circles on image
plane). b) When the estimate of w is correct, the predictions of the model agree with the observed
data.
We convert the original Cartesian representation of the 2D image points x to a 3D
homogeneous coordinate ˜x so that
˜x = λ


x
y
1

,
(14.16)
where λ is an arbitrary scaling factor. This is a redundant representation in that any
scalar multiple λ represents the same 2D point. For example, the homogeneous vectors
˜x = [2,4,2]T and ˜x = [3,6,3]T both represent the Cartesian 2D point x = [1,2]T , where
scaling factors λ = 2 and λ = 3 have been used, respectively.
Converting between homogeneous and Cartesian coordinates is easy. To move to
homogeneous coordinates, we choose λ = 1 and simply append a 1 to the original 2D
Cartesian coordinate. To recover the Cartesian coordinates, we divide the ﬁrst two entries
of the homogeneous 3-vector by the third, so that if we observe the homogeneous vector
˜x = [˜x, ˜y, ˜z]T , then we can recover the Cartesian coordinate x = [x,y]T as
x = ˜x
˜z
y = ˜y
˜z .
(14.17)
Further insight into the relationship between the two representations is given in
Figure 14.11.
It is similarly possible to represent the 3D world point w as a homogenous 4D vector
˜w so that
˜w = λ


u
v
w
1

,
(14.18)
where λ is again an arbitrary scaling factor. Once more, the conversion from Cartesian to
homogeneous coordinates can be achieved by appending a 1 to the original 3D vector w.

308
14 The pinhole camera
Figure 14.11 Geometric interpretation of homogeneous coordinates. The different scalar multi-
ples λ of the homogeneous 3-vector ˜x deﬁne a ray through the origin of a coordinate space. The
corresponding 2D image point x can be found by considering the 2D point that this ray strikes on
the plane z = 1. An interesting side-effect of this representation is that it is possible to represent
points at inﬁnity (known as ideal points). For example, the homogeneous coordinate [0,1,0]T
deﬁnes a ray that is parallel to z = 1 and so never intersects the plane. It represents the point at
inﬁnity in direction [0,1]T .
The conversion from homogeneous to Cartesian coordinates is achieved by dividing each
of the ﬁrst three entries by the last.
14.3.1 Camera model in homogeneous coordinates
It is hard to see the point of converting the 2D image points to homogeneous 3-vectors
and converting the 3D world point to homogeneous 4-vectors until we reexamine the
pinhole projection equations,
x = φxu + γv
w
+ δx
y = φyv
w + δy,
(14.19)
where we have temporarily assumed that the world point w = [u,v,w]T is in the same
coordinate system as the camera.
In homogeneous coordinates, these relationships can be expressed as a set of linear
equations
λ


x
y
1

=


φx
γ
δx
0
0
φy
δy
0
0
0
1
0




u
v
w
1

.
(14.20)

14.4
Learning extrinsic parameters
309
To convince ourselves of this, let us write these relations explicitly:
λx = φxu + γv + δxw
λy = φyv + δyw
λ = w.
(14.21)
We solve for x and y by converting back to Cartesian coordinates: we divide the ﬁrst two
relations by the third to yield the original pinhole model (Equation 14.19).
Let us summarize what has happened: the original mapping from 3D Cartesian world
points to 2D Cartesian image points is nonlinear (due to the division by w). However, the
mapping from 4D homogeneous world points to 3D homogeneous image points is linear.
In the homogeneous representation, the nonlinear component of the projection process
(division by w) has been side-stepped: this operation still occurs, but it is in the ﬁnal
conversion back to 2D Cartesian coordinates, and thus does not trouble the homogeneous
camera equations.
To complete the model, we add the extrinsic parameters {Ω,τ} that relate the world
coordinate system and the camera coordinate system, so that
λ


x
y
1

=


φx
γ
δx
0
0
φy
δy
0
0
0
1
0




ω11
ω12
ω13
τx
ω21
ω22
ω23
τy
ω31
ω32
ω33
τz
0
0
0
1




u
v
w
1

,
(14.22)
or in matrix form
λ˜x =
Λ
0 Ω
τ
0T
1

˜w,
(14.23)
where 0 = [0,0,0]T . The same relations can be simpliﬁed to
λ˜x = Λ
Ω
τ ˜w.
(14.24)
In the next three sections, we revisit the three geometric problems introduced in Sec-
tion 14.2. In each case, we will use algorithms based on homogeneous coordinates to
compute good initial estimates of the variable of interest. These estimates can then be
improved using nonlinear optimization.
14.4
Learning extrinsic parameters
Given a known object, with I distinct 3D points {wi}I
i=1, their corresponding projec-
14.1 tions in the image {xi}I
i=1, and known intrinsic parameters Λ, estimate the geometric
relationship between the camera and the object determined by the rotation Ωand the
translation τ:
ˆΩ, ˆτ = argmax
Ω,τ
" I
X
i=1
log[Pr(xi|wi,Λ,Ω,τ)]
#
.
(14.25)
This is a non-convex problem, so we make progress by expressing it in homogeneous
coordinates. The relationship between the ith homogeneous world point ˜wi and the ith
corresponding homogeneous image point ˜xi is
λi


xi
yi
1

=


φx
γ
δx
0
φy
δy
0
0
1




ω11
ω12
ω13
τx
ω21
ω22
ω23
τy
ω31
ω32
ω33
τz




ui
vi
wi
1

.
(14.26)

310
14 The pinhole camera
We would like to discard the effect of the (known) intrinsic parameters Λ. To this end,
we premultiply both sides of the equation by the inverse of the intrinsic matrix Λ to yield
λi


x′
i
y′
i
1

=


ω11
ω12
ω13
τx
ω21
ω22
ω23
τy
ω31
ω32
ω33
τz




ui
vi
wi
1

.
(14.27)
The transformed coordinates ˜x′ = Λ−1˜x are known as normalized image coordinates:
they are the coordinates that would have resulted if we had used a normalized camera.
In effect, premultiplying by Λ−1 compensates for the idiosyncrasies of this particular
camera.
We now note that the last of these three equations allows us to solve for the constant
λi, so that
λi = ω31ui + ω32vi + ω33wi + τz,
(14.28)
and we can now substitute this back into the ﬁrst two equations to get the relations
(ω31ui + ω32vi + ω33wi + τz)x′
i
(ω31ui + ω32vi + ω33wi + τz)y′
i

=
ω11
ω12
ω13
τx
ω21
ω22
ω23
τy



ui
vi
wi
1

.
(14.29)
These are two linear equations with respect to the unknown quantities Ωand τ. We can
take the two equations provided by each of the I pairs of points in the world w and the
image x to form the system of equations


u1
v1
w1
1
0
0
0
0
−u1x′
1
−v1x′
1
−w1x′
1
−x′
1
0
0
0
0
u1
v1
w1
1
−u1y′
1
−v1y′
1
−w1y′
1
−y′
1
u2
v2
w2
1
0
0
0
0
−u2x′
2
−v2x′
2
−w2x′
2
−x′
2
0
0
0
0
u2
v2
w2
1
−u2y′
2
−v2y′
2
−w2y′
2
−y′
2
...
...
...
...
...
...
...
...
...
...
...
...
uI
vI
wI
1
0
0
0
0
−uIx′
I
−vIx′
I
−wIx′
I
−x′
I
0
0
0
0
uI
vI
wI
1
−uIy′
I
−vIy′
I
−wIy′
I
−y′
I




ω11
ω12
ω13
τx
ω21
ω22
ω23
τy
ω31
ω32
ω33
τz


= 0.
(14.30)
This problem is now in the standard form Ab = 0 of a minimum direction problem.
We seek the value of b that minimizes |Ab|2 subject to the constraint |b| = 1 (to avoid
the uninteresting solution b = 0). The solution can be found by computing the sin-
gular value decomposition A = ULVT and setting ˆb to be the last column of V (see
Appendix C.7.2).
The estimates of Ωand τ that we extract from b have had an arbitrary scale imposed
on them, and we must ﬁnd the correct scaling factor. This is possible because the rotation
Ωhas a predeﬁned scale (its rows and columns must all have norm one). In practice,
we ﬁrst ﬁnd the closest true rotation matrix to Ω, which also forces our estimate to be

14.5
Learning intrinsic parameters
311
a valid orthogonal matrix. This is an instance of the orthogonal Procrustes problem
(Appendix C.7.3). The solution is found by computing the singular value decomposition
Ω= ULVT and setting ˆΩ= UVT . Now, we rescale the translation τ. The scaling factor
can be estimated by taking the average ratio of the nine entries of our initial estimate of
Ωto the ﬁnal one, ˆΩso that
ˆτ =
3
X
m=1
3
X
n=1
ˆΩmn
Ωmn
τ.
(14.31)
Finally, we must check that the sign of τz is positive, indicating that the object is in
front of the camera. If this is not the case, then we multiply both ˆτ and ˆΩby minus 1.
This scrappy algorithm is typical of methods that use homogeneous coordinates. The
resulting estimates ˆτ and ˆΩcan be quite inaccurate in the presence of noise in the mea-
sured image positions. However, they usually sufﬁce as a reasonable starting point for
the subsequent nonlinear optimization of the true objective function (Equation 14.25) for
this problem. This optimization must be carried out while ensuring that Ωremains a
valid rotation matrix (see Appendix B.4).
Note that this algorithm requires a minimum of 11 equations to solve the minimum
direction problem. Since each point contributes two equations, this means we require
I = 6 points for a unique solution. However, there are only really six unknowns (rotation
and translation in 3D), so a minimal solution would require only I = 3 points. Minimal
solutions for this problem have been developed and are discussed in the notes at the end
of the chapter.
14.5
Learning intrinsic parameters
We now address the second problem. In camera calibration we attempt to learn the intrin-
14.2 sic parameters based on viewing a known object or calibration target. More precisely,
we are given a known object, with I distinct 3D points {wi}I
i=1 and their corresponding
2D projections in the image {xi}I
i=1, and aim to form maximum likelihood estimates of
the intrinsic parameters Λ,
ˆΛ = argmax
Λ
"
max
Ω,τ
" I
X
i=1
log[Pr(xi|wi,Λ,Ω,τ)]
##
.
(14.32)
A simple (but inefﬁcient) approach to this problem is to use a coordinate ascent method
in which we alternately
• Estimate the extrinsic parameters for ﬁxed intrinsic parameters (problem 1),
ˆΩ, ˆτ = argmax
Ω,τ
" I
X
i=1
log[Pr(xi|wi,Λ,Ω,τ)]
#
,
(14.33)
using the procedure described in Section 14.4, and then
• Estimate the intrinsic parameters for ﬁxed extrinsic parameters,
ˆΛ = argmax
Λ
" I
X
i=1
log[Pr(xi|wi,Λ,Ω,τ)]
#
.
(14.34)

312
14 The pinhole camera
By iterating these two steps, we will get closer and closer to the correct solution. Since
we already know how to solve the ﬁrst of these two subproblems, we now concentrate
on solving the second. Happily there is a closed form solution that does not even require
homogeneous coordinates.
Given known world points {wi}I
i=1, their projections {xi}I
i=1, and known extrinsic
parameters {Ω,τ}, our goal is now to compute the intrinsic matrix Λ, which contains the
intrinsic parameters {φx,φy,γ,δx,δy}. We will apply the maximum likelihood method
ˆΛ = argmax
Λ
" I
X
i=1
log

Normxi

pinhole[wi,Λ,Ω,τ],σ2I

#
(14.35)
= argmin
Λ
" I
X
i=1
(xi−pinhole[wi,Λ,Ω,τ])T (xi−pinhole[wi,Λ,Ω,τ])
#
,
which results in a least squares problem (see Section 4.4.1).
Now we note that the projection function pinhole[•,•,•,•] (Equation 14.8) is linear
with respect to the intrinsic parameters, and can be written as Aih where
Ai =
" ω11ui+ω12vi+ω13wi+τx
ω31ui+ω32vi+ω33wi+τz
ω21ui+ω22vi+ω23wi+τx
ω31ui+ω32vi+ω33wi+τz
1
0
0
0
0
0
ω21ui+ω22vi+ω23wi+τy
ω31ui+ω32vi+ω33wi+τz
1
#
(14.36)
and h = [φx,γ,δx,φy,δy]T . Consequently, the problem has the form
ˆh = argmin
h
" I
X
i=1
(Aih −xi)T (Aih −xi)
#
,
(14.37)
which we recognize as a least squares problem that can be solved in closed form
(Appendix C.7.1).
We have described this alternating approach for pedagogical reasons; it is simple to
understand and implement. However, we emphasize that this is not really a practical
method as the convergence will be very slow. A better approach would be to perform
a couple of iterations of this method and then optimize both the intrinsic and extrinsic
parameters simultaneously using a nonlinear optimization technique such as the Gauss-
Newton method (Appendix B.2.3) with the original criterion (Equation 14.32). This
optimization must be done while ensuring that the extrinsic parameter Ωremains a valid
rotation matrix (see Appendix B.4).
14.6
Inferring three-dimensional world points
Finally, we consider the multiview reconstruction problem. Given J calibrated cameras
14.3 in known positions (i.e., cameras with known Λ,Ω,τ), viewing the same 3D point w
and knowing the corresponding projections in the images {xj}J
j=1, establish the position
of the point in the world.
ˆw = argmax
w




J
X
j=1
log[Pr(xj|w,Λj,Ωj,τ j)]



.
(14.38)
This cannot be solved in closed form and so we move to homogeneous coordinates
where we can solve for a good initial estimate in closed form. The relationship between

14.6
Inferring three-dimensional world points
313
the homogeneous world point ˜w and the jth corresponding homogeneous image point
˜xj is
λj


xj
yj
1

=


φxj
γj
δxj
0
φyj
δyj
0
0
1




ω11j
ω12j
ω13j
τxj
ω21j
ω22j
ω23j
τyj
ω31j
ω32j
ω33j
τzj




u
v
w
1

,
(14.39)
where we have appended the index j to the intrinsic and extrinsic parameters to denote
the fact that they belong to the jth camera. Premultiplying both sides by the intrinsic
matrix Λ−1
j
to convert to normalized image coordinates gives
λj


x′
j
y′
j
1

=


ω11j
ω12j
ω13j
τxj
ω21j
ω22j
ω23j
τyj
ω31j
ω32j
ω33j
τzj




u
v
w
1

,
(14.40)
where x′
j and y′
j denote the normalized image coordinates in the jth camera.
We use the third equation to establish that λj = ω31ju + ω32jv + ω33jw + τzj.
Substituting into the ﬁrst two equations, we get
(ω31ju + ω32jv + ω33jw + τzj)x′
j
(ω31ju + ω32jv + ω33jw + τzj)y′
j

=
ω11j
ω12j
ω13j
τxj
ω21j
ω22j
ω23j
τyj



u
v
w
1

.
(14.41)
These equations can be rearranged to provide two linear constraints on the three unknown
quantities in w = [u,v,w]T :
ω31jx′
j −ω11j
ω32jx′
j −ω12j
ω33jx′
j −ω13j
ω31jy′
j −ω21j
ω32jy′
j −ω22j
ω33jy′
j −ω23j


u
v
w

=
τxj −τzjx′
j
τyj −τzjy′
j

.
(14.42)
With multiple cameras, we can build a larger system of equations and solve for w in a
least squares sense (Appendix C.7.1). This typically provides a good starting point for
the subsequent nonlinear optimization of the criterion in Equation 14.38.
This calibrated reconstruction algorithm is the basis for methods that construct 3D
models. However, there are several parts missing from the argument.
• The method requires us to have found the points {xj}J
j=1 that correspond to the
same world point w in each of the J images. This process is called correspondence
and is discussed in Chapters 15 and 16.
• The method requires the intrinsic and extrinsic parameters. Of course, these could
be computed from a calibration target using the method of Section 14.5. However,
it is still possible to perform reconstruction when the system is uncalibrated; this is
known as projective reconstruction as the result is ambiguous up to a 3D projective
transformation. Furthermore, if a single camera was used to take all of the images,
it is possible to estimate the single intrinsic matrix and extrinsic parameters from a
sequence, and reconstruct points in a scene up to a constant scaling factor. Chapter
16 presents an extended discussion of this method.

314
14 The pinhole camera
14.7
Applications
We discuss two applications for the techniques in this chapter. We consider a method to
construct 3D models based on projecting structured light onto the object, and a method
for generating novel views of an object based on an approximate model built from the
silhouettes of the object.
14.7.1 Depth from structured light
In Section 14.6 we showed how to compute the depth of a point given its position in two
or more calibrated cameras. However, we did not discuss how to ﬁnd matching points
in the two images. We defer a full answer to this question to Chapter 16, but here we
will develop a method that circumvents this problem. This method will be based on a
projector and a camera, rather than two cameras.
It is crucial to understand that the geometry of a projector is exactly the same as
that of a camera: the projector has an optical center and has a regular pixel array that
is analogous to the sensor in the camera. Each pixel in the projector corresponds to a
direction in space (a ray) through the optical center, and this relationship can be captured
by a set of intrinsic parameters. The major difference is that a projector sends outgoing
light along these rays, whereas the camera captures incoming light along them.
Consider a system that comprises a single camera and a projector that are displaced
relative to one another but that point at the same object (Figure 14.12). For simplicity,
we will assume that the system is calibrated (i.e., the intrinsic matrices and the relative
positions of the camera and projector are known). It is now easy to estimate the depth of
the scene: we illuminate the scene using the projector one pixel at a time, and ﬁnd the
corresponding pixel in the camera by observing which part of the image gets brighter.
We now have two corresponding points and can compute the depth using the method of
Section 14.6. In practice, this technique is very time consuming as a separate image must
be captured for each pixel in the projector. Scharstein and Szeliski (2003) used a more
practical technique using structured light in which a series of horizontal and vertical
stripe patterns is projected onto the scene that allow the mapping between pixels in the
projector and those in the camera to be computed.
Figure 14.12 Depth maps from structured light. a) A three-dimensional scene that we wish to
capture. b) The capture hardware consists of a projector and a camera, which both view the scene
from different positions. c) The projector is used to illuminate the scene, and the camera records
the pattern of illumination from its viewpoint. The resulting images contain information that can
be used to compute a 3D reconstruction. Adapted from Scharstein and Szeliski (2003).
c⃝2003
IEEE.

14.7
Applications
315
To understand how this works, consider a projector image in which the top half is light
and the bottom half is dark. We capture two images I1 and I2 of the scene corresponding
to when the projector shows this pattern and when it shows its inverse. We then take the
difference I1 −I2 between the images. Pixels in the camera image where the difference
is positive must now belong to the top half of the projector image, and pixels where the
difference is negative must belong to the bottom half of the image. We now illuminate
the image with a second pattern, which divides the images into four horizontally oriented
black and white stripes. By capturing the image illuminated by this second pattern and its
inverse, we can hence deduce whether each pixel is in the top or bottom half of the region
determined by the ﬁrst pattern. We continue in this way with successively ﬁner patterns,
reﬁning the estimated position at each pixel until we know it accurately. The whole
procedure is repeated with vertical striped patterns to estimate the horizontal position.
In practice, more sophisticated coding schemes are used as the sequence described in
the preceding section; for example, this sequence means that there is always a boundary
between black and white in the center of the projector image. It may be hard to establish
the correspondence for a camera pixel, which always straddles this boundary. One solu-
tion is to base the sequences on Gray codes which have a more complex structure and
avoid this problem (Figure 14.13). The estimated depth map of the scene in Figure 14.12
is shown in Figure 14.14.
Figure 14.13 Projector to camera correspondence with structured light patterns. a) To establish
the vertical position in the projector image, we present a sequence of horizontally striped patterns.
Each height in the projected image receives a unique sequence of black and white values, so we
can determine the height (e.g., red line) by measuring this sequence. b–c) Two examples of these
horizontally striped patterns. d–e) Two examples of vertically striped patterns that are part of
a sequence designed to estimate the horizontal position in the projector pattern. Adapted from
Scharstein and Szeliski (2003). c⃝2003 IEEE.

316
14 The pinhole camera
Figure 14.14 Recovered depth map for scene in Figure 14.12 using the structured light method.
Pixels marked as blue are places where the depth is uncertain: these include positions in the image
that were occluded with respect to the projector, so no light was cast onto them. Scharstein and
Szeliski (2003) also captured the scene with two cameras under normal illumination; they sub-
sequently used the depth map from the structured light as ground truth data for assessing stereo
vision algorithms. Adapted from Scharstein and Szeliski (2003). c⃝2003 IEEE.
14.7.2 Shape from silhouette
The preceding system computed a 3D model of a scene based on explicit correspondences
between a projector and a camera. We now consider an alternative method for computing
3D models that does not require explicit correspondence. As the name suggests, shape
from silhouette estimates the shape of an object based on its silhouette in a number of
images.
The principle is illustrated in Figure 14.15. Given a single camera, we know that an
object must lie somewhere within the bundle of rays that fall within its silhouette. Now
consider adding a second camera. We also know that the object must lie somewhere
within the bundle of rays corresponding to the silhouette in this image. Hence, we can
reﬁne our estimate of the shape to the 3D intersection of these two ray bundles. As we
add more cameras, the possible region of space that the object can lie in is reduced.
This procedure is attractive because the silhouettes can be computed robustly and
quickly using a background subtraction approach. However, there is also a downside;
even if we have an inﬁnite number of cameras viewing the object, some aspects of the
shape will not be present in the resulting 3D region. For example, the concave region
at the back of the seat of the chair in Figure 14.15a cannot be recovered as it is not
represented in the silhouettes. In general, the “best possible” shape estimate is known as
the visual hull.
We will now develop an algorithm based on shape from silhouette that can be used to
generate images of an object from novel poses. One application of this is for augmented
reality systems in which we wish to superimpose an object over a real image in such
a way that it looks like it is a stable part of the scene. This can be accomplished by
establishing the position and pose of the camera relative to the scene (Section 14.4) and
then generating a novel image of the object from the same viewpoint. Figure 14.17

14.7
Applications
317
Figure 14.15 Shape from silhouette. a) The goal is to recover information about the shape of the
object based on the silhouettes in a number of cameras. b) Consider a single camera viewing an
object (2D slice shown). We know that the object must lie somewhere within the shaded region
deﬁned by the silhouette. c) When we add a second camera, we know that the object must lie
within the intersection of the regions determined by the silhouettes (gray region). d–f) As we add
more cameras, the approximation to the true shape becomes closer and closer. Unfortunately, we
can never capture the concave region, no matter how many cameras we add.
Figure 14.16 Generating novel views. a–c) An actor is captured from 15 cameras in a green
screen studio. d-l) Novel views of the actor are now generated and superimposed on the scene. The
novel view is carefully chosen so that it matches the direction that the camera views the desktop
giving the impression that the actor is a stable part of the scene. Adapted from Prince et al. (2002).
c⃝2002 IEEE.

318
14 The pinhole camera
Figure 14.17 Novel view generation. A new image is generated one pixel at a time by testing a
sequence of points w1,w2,... along the ray through the pixel. Each point is tested to see if it is
within the visual hull by projecting it into the real images. If it lies within the silhouette in every
real image, then it is within the visual hull and the search stops. In this case, point wk is the ﬁrst
point on the surface. To establish the color for the pixel, this point is projected into a nearby real
image, and the color is copied. Adapted from Prince et al. (2002). c⃝2002 IEEE.
depicts an example application of this kind, in which the performance of an actor is
captured and rebroadcast as if he is standing on the desk.
Prince et al. (2002) described a method to generate a novel image from a virtual
camera with intrinsic matrix Λ and extrinsic parameters {Ω,τ}. They considered each
pixel in the virtual camera in turn, and computed the direction of the ray r passing through
this point. With respect to the virtual camera itself, this ray has a direction
r = Λ−1˜x,
(14.43)
where ˜x = [x,y,1]T is the position of the point in the image expressed in homogeneous
coordinates. With respect to the global coordinate system, a point w that is κ units along
the ray can be expressed as
w = τ + κΩr.
(14.44)
The depth of the object at this pixel is then computed by exploring along this direc-
tion; it is determined by an explicit search starting at the virtual camera projection center
and proceeding outward along the ray corresponding to the pixel center (Figure 14.17).
Each candidate 3D point along this ray is evaluated for potential occupancy. A candidate
point is unoccupied if its projection into any of the real images is marked as background.
When the ﬁrst point is found for which all of the projected positions are marked as fore-
ground, this is considered the depth with respect to the virtual object and the search
stops.
The 3D point where the ray through the current virtual pixel meets the visual hull is
now known. To establish the color of this pixel in the virtual image, the point is projected
into the closest real image, and the color is copied. In general, the projection will not be
exactly centered on a pixel, and to remedy this, the color value is estimated using bilinear
or bicubic interpolation.

Notes
319
This procedure can be accomplished very quickly; the system of Prince et al. (2002)
depicted in Figure 14.16 ran at interactive speeds, but the quality is limited to the extent
that the visual hull is a reasonable approximation of the true shape. If this approximation
is bad, the wrong depth will be estimated, the projections into the real images will be
inaccurate, and the wrong color will be sampled.
Discussion
In this chapter, we have introduced a model for a pinhole camera and discussed learning
and inference algorithms for this model. In the next chapter, we consider what happens
when this camera views a planar scene; here there is a one-to-one mapping between
points in the scene and points in the image. In Chapter 16, we return to the pinhole
camera model and consider reconstruction of a scene from multiple cameras in unknown
positions.
Notes
Camera geometry: There are detailed treatments of camera geometry in Hartley and Zisserman
(2004), Ma et al. (2004), and Faugeras et al. (2001). Aloimonos (1990) and Mundy and Zisserman
(1992) developed a hierarchy of camera models (see Problem 14.3). Tsai (1987) and Faugeras
(1993) both present algorithms for camera calibration from a 3D object. However, it is now more
usual to calibrate cameras from multiple images of a planar object (see Section 15.4) due to the
difﬁculties associated with accurately machining a 3D object. For a recent summary of camera
models and geometric computer vision, consult Sturm et al. (2011).
The projective pinhole camera discussed in this chapter is by no means the only camera model used
in computer vision; there are specialized models for the pushbroom camera (Hartley and Gupta
1994), ﬁsh-eye lenses (Devernay and Faugeras 2001; Claus and Fitzgibbon 2005), catadioptric
sensors (Geyer and Daniilidis 2001; Mˇıcuˇs´ık and Pajdla 2003; Claus and Fitzgibbon 2005), and
perspective cameras imaging through an interface into a medium (Treibitz et al. 2008).
Estimating extrinsic parameters: A large body of work addresses the PnP problem of estimat-
ing the geometric relation between the camera and a rigid object. Lepetit et al. (2009) present a
recent approach that has low complexity with respect to the number of points used and provides a
quantitative comparison with other approaches. Quan and Lan (1999) and Gao et al. (2003) present
minimal solutions based on three points.
Structured light: The structured light method discussed in this chapter is due to Scharstein and
Szeliski (2003), although the main goal of their paper was to generate ground truth for stereo vision
applications. The use of structured light has a long history in computer vision (e.g., Vuylsteke and
Oosterlinck 1990), with the main research issue being the choice of pattern to project (Salvi et al.
2004; Batlle et al. 1998; Horn and Kiryati 1999).
Shape from silhouette: The recovery of shape from multiple silhouettes of an object dates back to
at least Baumgart (1974). Laurentini (1994) introduced the concept of the visual hull and described
its properties. The shape from silhouette algorithm discussed in this chapter was by Prince et al.
(2002) and is closely related to earlier work by Matusik et al. (2000) but rather simpler to explain.
Recent work in this area has considered a probabilistic approach to pixel occupancy (Franco and
Boyer 2005), the application of human silhouette priors (Grauman et al. 2003), the use of tem-
poral sequences of silhouettes (Cheung et al. 2004), and approaches that characterize the intrinsic
projective features of the visual hull.
Human performance capture: Modern interest in human performance capture was stimulated by
Kanade et al. (1997). More recent work in this area includes that of Starck et al. (2009), Theobalt
et al. (2007), Vlasic et al. (2008), de Aguiar et al. (2008), and Ballan and Cortelazzo (2008).

320
14 The pinhole camera
Problems
14.1 A pinhole camera has a sensor that is 1 cm × 1 cm and a horizontal ﬁeld of view of 60o.
What is the distance between the optical center and the sensor? The same camera has a resolution
of 100 pixels in the horizontal direction and 200 pixels in the vertical direction (i.e., the pixels are
not square). What are the focal length parameters fx and fy from the intrinsic matrix?
14.2 We can use the pinhole camera model to understand a famous movie effect. Dolly zoom was
ﬁrst used in Alfred Hitchcock’s Vertigo. As the protagonist looks down a stairwell, it appears to
deform (Figure 14.18) in a strange way. The background seems to move away from the camera,
while the foreground remains at a constant position.
In terms of the camera model, two things occur simultaneously during the dolly zoom sequence:
the camera moves along the w-axis, and the focal distance of the camera changes. The distance
moved and the change of focal length are carefully chosen so that objects in a predeﬁned plane
Figure 14.18 Dolly zoom. a–c) Three frames from Vertigo in which the stairwell appears to dis-
tort. Nearby objects remain in roughly the same place whereas object further away systematically
move through the sequence. To see this, consider the red and green circles which are at the same
(x,y) position in each frame. The red circle remains on the near bannister, but the green circle
is on the ﬂoor of the stairwell in the ﬁrst image but halfway up the stairs in the last image. d) To
understand this effect consider a camera viewing a scene that consists of several green points at the
same depth and some other surfaces (colored lines). e) We move the camera along the w-axis but
simultaneously change the focal length so that the green points are imaged at the same position.
Under these changes, objects in the plane of the green points are static, but other parts of the scene
move and may even occlude one another.

Problems
321
Figure 14.19 Alternative camera models. a) Orthographic camera. Rays are parallel and orthogo-
nal to image plane. b) Weak perspective model. Points are projected orthogonally onto a reference
plane at distance zr from the camera and then pass to the image plane by perspective projection.
remain at the same position. However, objects out of this plane move relative to one another
(Figures 14.18d–e).
I want to capture two pictures of a scene at either end of a dolly zoom. Before the zoom, the camera
is at w = 0, the distance between the optical center and the image plane is 1 cm, and the image
plane is 1 cm × 1cm. After the zoom, the camera is at w = 100 cm. I want the plane at w = 500 cm
to be stable after the camera movement. What should the new distance between the optical center
and the image plane be?
14.3
Figure 14.19 shows two different camera models: the orthographic and weak perspective
cameras. For each camera, devise the relationship between the homogeneous world points and
homogeneous image points. You may assume that the world coordinate system and the camera
coordinate system coincide, so there is no need to introduce the extrinsic matrix.
14.4 A 2D line can be as expressed as ax + by + c = 0 or in homogeneous terms
l˜x = 0,
where l = [a,b,c]. Find the point where the homogeneous lines l1 and l2 join where:
1. l1 = [3,1,1], and l2 = [−1,0,1]
2. l1 = [1,0,1], and l2 = [3,0,1]
Hint: the 3×1 homogeneous point vector ˜x must satisfy both l1˜x = 0 and l2˜x = 0. In other words
it should be orthogonal to both l1 and l2.
14.5 Find the line joining the homogeneous points ˜x1 and ˜x2 where
˜x1 = [2,2,1]T , ˜x2 = [−2,−2,1]T .
14.6 A conic C is a geometric structure that can represent ellipses and circles in the 2D image.
The condition for a point to lie on a conic is given by

x
y
1



a
b
c
b
d
e
c
e
f




x
y
1

= 0,
or
˜xT C˜x = 0.
Describe an algorithm to estimate the parameters a,b,c,d,e,f given several points x1,x2,...,xn
that are known to lie on the conic. What is the minimum number of points that your algorithm
requires to be successful?

322
14 The pinhole camera
14.7 Devise a method to ﬁnd the intrinsic matrix of a projector using a camera and known
calibration object.
14.8 What is the minimum number of binary striped light patterns of the type illustrated in Figure
14.13 required to estimate the camera-projector correspondences for a projector image of size
H × W?
14.9 There is a potential problem with the shape from silhouette algorithm as described; the point
that we have found on the surface of the object may be occluded by another part of the object with
respect to the nearest camera. Consequently, when we copy the color, we will get the wrong value.
Propose a method to circumvent this problem.
14.10
In the augmented reality application (Figure 14.16), the realism might be enhanced if
the object had a shadow. Propose an algorithm that could establish whether a point on the desktop
(assumed planar) is shadowed by the object with respect to a point light source at a known position.

Chapter 15
Models for transformations
In this chapter, we consider a pinhole camera viewing a plane in the world. In these
circumstances, the camera equations simplify to reﬂect the fact that there is a one-to-one
mapping between points on this plane and points in the image.
Mappings between the plane and the image can be described using a family of 2D
geometric transformations. In this chapter, we characterize these transformations and
show how to estimate their parameters from data. We revisit the three geometric problems
from Chapter 14 for the special case of a planar scene.
To motivate the ideas of this chapter, consider an augmented reality application in
which we wish to superimpose 3D content onto a planar marker (Figure 15.1). To do
this, we must establish the rotation and translation of the plane relative to the camera.
We will do this in two stages. First, we will estimate the 2D transformation between
points on the marker and points in the image. Second, we will extract the rotation and
translation from the transformation parameters.
15.1
Two-dimensional transformation models
In this section, we consider a family of 2D transformations, starting with the simplest
and working toward the most general. We will motivate each by considering viewing a
planar scene under different viewing conditions.
15.1.1 Euclidean transformation model
Consider a calibrated camera viewing a fronto-parallel plane at known distance, D (i.e.,
a plane whose normal corresponds to the w-axis of the camera). This may seem like
a contrived situation, but it is exactly what happens in machine inspection applications:
an overhead camera views a conveyor belt and examines objects that contain little or no
depth variation.
We assume that a position on the plane can be described by a 3D position w =
[u,v,0]T , measured in real-world units such as millimeters. The w-coordinate represents
directions perpendicular to the plane, and is hence always zero. Consequently, we will
sometimes treat w = [u,v]T as a 2D coordinate.
Applying the pinhole camera model to this situation gives
λ˜x = Λ[Ω,τ] ˜w,
(15.1)

324
15 Models for transformations
Figure 15.1 Video see-through augmented reality with a planar scene. a) The user views the
world through a head mounted display with a camera attached to the front. The images from
the camera are analyzed and augmented in near-real time and displayed to the user. b) Here, the
world consists of a planar 2D marker. c) The marker corners are found by ﬁtting edges to its sides
and ﬁnding their intersections. d) The geometric transformation between the 2D positions of the
corners on the marker surface and the corresponding positions in the image is computed. This
transformation is analyzed to ﬁnd the rotation and translation of the camera relative to the marker.
This allows us to superimpose a 3D object as if it were rigidly attached to the surface of the image.
e–f) As the marker is manipulated the superimposed object changes pose appropriately.
where ˜x is the 2D observed image position represented as a homogeneous 3-vector and
˜w is the 3D point in the world represented as a homogeneous 4-vector. Writing this out
explicitly, we have
λ


x
y
1

=


φx
γ
δx
0
φy
δy
0
0
1




ω11
ω12
0
τx
ω21
ω22
0
τy
0
0
1
D




u
v
0
1


=


φx
γ
δx
0
φy
δy
0
0
1




ω11
ω12
τx
ω21
ω22
τy
0
0
D




u
v
1

,
(15.2)
where the 3D rotation matrix Ωtakes a special form with only four unknowns, reﬂecting
the fact that the plane is known to be fronto-parallel.
We can move the distance parameter D into the intrinsic matrix without changing the
last of these three equations and equivalently write
λ


x
y
1

=


φx
γ
δx
0
φy
δy
0
0
D




ω11
ω12
τx
ω21
ω22
τy
0
0
1




u
v
1

.
(15.3)

15.1
Two-dimensional transformation models
325
Figure 15.2 The 2D Euclidean transforma-
tion describes 2D rigid rotations and trans-
lations. The blue squares are all Euclidean
transformations of the original red square.
The transformation has three parameters:
the rotation angle and the translations in the
x- and y-directions. When a camera views
a fronto-parallel plane at a known distance,
the relation between the normalized cam-
era coordinates and the 2D positions on the
plane is a Euclidean transformation.
If we now eliminate the effect of this modiﬁed intrinsic matrix, by premultiplying both
left and right by its inverse we get
λ


x′
y′
1

=


ω11
ω12
τx
ω21
ω22
τy
0
0
1




u
v
1

,
(15.4)
where x′ and y′ are camera coordinates that are normalized with respect to this modiﬁed
intrinsic matrix.
The mapping in Equation 15.4 is known as a Euclidean transformation. It can be
equivalently written in Cartesian coordinates as
x′
y′

=
ω11
ω12
ω21
ω22
u
v

+
τx
τy

,
(15.5)
or for short we may write
x′ = euc[w,Ω,τ],
(15.6)
where x′ = [x′,y′]T contains the normalized camera coordinates and w = [u,v]T is the
real-world position on the plane.
The Euclidean transformation describes rigid rotations and translations in the plane
(Figure 15.2). Although this transformation appears to take six separate parameters, the
rotation matrix Ωcan be reexpressed in terms of the rotation angle θ,
ω11
ω12
ω21
ω22

=
 cos[θ]
sin[θ]
−sin[θ]
cos[θ]

,
(15.7)
and hence the actual number of parameters is three (the two offsets τx and τy, and the
rotation, θ).

326
15 Models for transformations
15.1.2 Similarity transformation model
Now consider a calibrated camera viewing a fronto-parallel plane at unknown distance D.
The relationship between image points x = [x,y]T and points w = [u,v,0]T on the plane
is once more given by Equation 15.2. Converting to normalized image coordinates by
premultiplying both sides by the inverse of the intrinsic matrix gives
λ


x′
y′
1

=


ω11
ω12
0
τx
ω21
ω22
0
τy
0
0
1
D




u
v
0
1

=


ω11
ω12
τx
ω21
ω22
τy
0
0
D




u
v
1

.
(15.8)
We now multiply each of these three equations by ρ = 1/D to get
ρλ


x′
y′
1

=


ρω11
ρω12
ρτx
ρω21
ρω22
ρτy
0
0
1




u
v
1

.
(15.9)
This is the homogeneous representation of the similarity transformation. However it
is usual to incorporate ρ into the constant λ on the left-hand side so that λ ←ρλ and into
the translation parameters so that τx ←ρτx and τy ←ρτy on the right-hand side to yield
λ


x′
y′
1

=


ρω11
ρω12
τx
ρω21
ρω22
τy
0
0
1




u
v
1

.
(15.10)
Converting to Cartesian coordinates, we have
x′
y′

=
ρω11
ρω12
ρω21
ρω22
u
v

+
τx
τy

,
(15.11)
or for short,
x′ = sim[w,Ω,τ,ρ].
(15.12)
Figure
15.3 The similarity transforma-
tion describes rotations, translations, and
isotropic scalings.
The blue quadrilater-
als are all similarity transformations of the
original red square.
The transformation
has four parameters:
the rotation angle,
the scaling and the translations in the x-
and y-directions. When a camera views a
fronto-parallel plane at unknown distance,
the relation between the normalized camera
coordinates and positions on the plane is a
similarity.

15.1
Two-dimensional transformation models
327
The similarity transformation is a Euclidean transformation with a scaling (Figure
15.3) and has four parameters: the rotation, the scaling, and two translations.
15.1.3 Aﬃne transformation model
We motivated each of the previous transformations by considering a camera viewing
a fronto-parallel plane. Ultimately, we wish to describe the relationship between image
points and points on a plane in general position. As an intermediate step, let us generalize
the transformations in Equations 15.4 and 15.10 to
λ


x′
y′
1

=


φ11
φ12
τx
φ21
φ22
τy
0
0
1




u
v
1

,
(15.13)
where φ11,φ12,φ21, and φ22 are now unconstrained and can take arbitrary values. This
is known as an afﬁne transformation. In Cartesian coordinates, we have
x′
y′

=
φ11
φ12
φ21
φ22
u
v

+
τx
τy,

(15.14)
or for short we might write
x′ = aff[w,Φ,τ].
(15.15)
Note that the camera calibration matrix Λ also has the form of an afﬁne transformation
(i.e., a 3×3 matrix with two zeros in the bottom row). The product of two afﬁne transfor-
mations is a third afﬁne transformation, so if Equation 15.15 is true, then there is also an
afﬁne transformation between points on the plane and the original (unnormalized) pixel
positions.
The afﬁne transformation encompasses both Euclidean and similarity transforma-
tions, but also includes shears (Figure 15.4). However, it is far from general, and a
notable restriction is that parallel lines are always mapped to other parallel lines. It has
six unknown parameters, each of which can take any value.
Figure
15.4 The afﬁne transformation
describes rotations, translations, scalings,
and shears. The blue quadrilaterals are all
afﬁne transformations of the original red
square.
The afﬁne transformation has six
parameters: the translations in the x- and
y-directions, and four parameters that deter-
mine the other effects. Notice that lines that
were originally parallel remain parallel after
the afﬁne transformation is applied, so in
each case the square becomes a parallelo-
gram.

328
15 Models for transformations
Figure 15.5 Approximating projection of a plane. a) A planar object viewed with a camera
with a narrow ﬁeld of view (long focal length) from a large distance. The depth variation within
the object is small compared to the distance from the camera to the plane. Here, perspective
distortion is small, and the relationship between points in the image and points on the surface is
well approximated by an afﬁne transformation. b) The same planar object viewed with a wide
ﬁeld of view (short focal length) from a short distance. The depth variation within the object is
comparable to the average distance from the camera to the plane. An afﬁne transformation cannot
describe this situation well. c) However, a projective transformation (homography) captures the
relationship between points on the surface and points in this image.
The question remains as to whether the afﬁne transformation really does provide a
good mapping between points on a plane and their positions in the image. This is indeed
the case when the depth variation of the plane as seen by the camera is small relative to
the mean distance from the camera. In practice, this occurs when the viewing angle is
not too oblique, the camera is distant, and the ﬁeld of view is small (Figure 15.5a). In
more general situations, the afﬁne transformation is not a good approximation. A simple
counterexample is the convergence of parallel train tracks in an image as they become
more distant. The afﬁne transformation cannot describe this situation as it can only map
parallel lines on the object to parallel lines in the image.
15.1.4 Projective transformation model
Finally, we investigate what really happens when a pinhole camera views a plane from
an arbitrary viewpoint. The relationship between a point w = [u,v,0]T on the plane and
the position x = [x,y]T to which it is projected is
λ


x
y
1

=


φx
γ
δx
0
φy
δy
0
0
1




ω11
ω12
ω13
τx
ω21
ω22
ω23
τy
ω31
ω32
ω33
τz




u
v
0
1


=


φx
γ
δx
0
φy
δy
0
0
1




ω11
ω12
τx
ω21
ω22
τy
ω31
ω32
τz




u
v
1

.
(15.16)
Combining the two 3 × 3 matrices by multiplying them together, the result is a
transformation with the general form
λ


x
y
1

=


φ11
φ12
φ13
φ21
φ22
φ23
φ31
φ32
φ33




u
v
1


(15.17)

15.1
Two-dimensional transformation models
329
Figure 15.6 The projective transformation
(also known as a collinearity or homogra-
phy) can map any four points in the plane
to any other four points. Rotations, transla-
tions, scalings, and shears and are all special
cases. The blue quadrilaterals are all pro-
jective transformations of the original red
square.
The projective transformation has
eight parameters. Lines that were parallel
are not constrained to remain parallel after
the projective transformation is applied.
and is variously known as a projective transformation, a collinearity, or a homography.
In Cartesian coordinates the homography is written as
x = φ11u + φ12v + φ13
φ31u + φ32v + φ33
y = φ21u + φ22v + φ23
φ31u + φ32v + φ33
,
(15.18)
or for short
x = hom[w,Φ].
(15.19)
The homography can map any four points in the plane to any other four points (Figure
15.6). It is a linear transformation in homogenous coordinates (Equation 15.17) but is
nonlinear in Cartesian coordinates (Equation 15.18). It subsumes the Euclidean, similar-
ity, and afﬁne transformations as special cases. It exactly describes the mapping between
the 2D coordinates of points on a plane in the real world and their positions in an image
of that plane (Figure 15.5c).
Although there are nine entries in the matrix Φ, the homography only contains eight
degrees of freedom; the entries are redundant with respect to scale. It is easy to see that
a constant rescaling of all nine values produces the same transformation, as the scaling
factor cancels out of the numerator and denominator in Equation 15.18. The properties
of the homography are discussed further in Section 15.5.1.
15.1.5 Adding uncertainty
The four geometric models presented in the preceding sections are deterministic. How-
ever, in a real system, the measured positions of features in the image are subject to
noise, and we need to incorporate this uncertainty into our models. In particular, we will
assume that the positions xi in the image are corrupted by normally distributed noise with

330
15 Models for transformations
Figure 15.7 Learning and inference for transformation models. a) Planar object surface (position
measured in cm). b) Image (position measured in pixels). In learning, we estimate the param-
eters of the mapping from points w on the object surface to image positions xi based on pairs
{xi,wi}I
i=1 of known correspondences. We can use this mapping to ﬁnd the position x∗in the
image to which a point w∗on the object surface will project. In inference, we reverse this process:
given a position x∗in the image, the goal is to establish the corresponding position w∗on the
object surface.
spherical covariance so that, for example, the likelihood for the homography becomes
Pr(x|w) = Normx

hom[w,Φ],σ2I

.
(15.20)
This is a generative model for the 2D image data x. It can be thought of as a simpliﬁed
version of the pinhole camera model that is specialized for viewing planar scenes; it is a
recipe that tells us how to ﬁnd the position in the image x corresponding to a point w on
the surface of the planar object in the world.
The learning and inference problems in this model (Figure 15.7) are
• Learning: we are given pairs of points {xi,wi}I
i=1 where xi is a position in the
image and wi is the corresponding position on the plane in the world. The goal is
to use these to establish the parameters θ of the transformation. For example, in
the case of the homography, the parameters θ would comprise the nine entries of
the matrix Φ.
• Inference: we are given a new point in the image x∗, and our goal is to ﬁnd the
position on the plane w∗that projected to it.
We consider these two problems in the following sections.
15.2
Learning in transformation models
We are given a set of I 2D positions wi = [ui,vi]T on the surface of the plane and the I
corresponding 2D image positions xi = [xi,yi]T in the image. We select a transformation
class of the form trans[wi,θ]. The goal of the learning algorithm is then to estimate the
parameters θ that best map the points wi to the image positions xi.

15.2
Learning in transformation models
331
Adopting the maximum likelihood approach, we have
ˆθ = argmax
θ
" IY
i=1
Normxi

trans[wi,θ],σ2I

#
= argmax
θ
" I
X
i=1
log

Normxi

trans[wi,θ],σ2I

#
,
(15.21)
where, as usual, we have taken the logarithm which is a monotonic transformation and
hence does not affect the position of the maximum. Substituting in the expression for the
normal distribution and simplifying, we get the least squares problem
ˆθ = argmin
θ
" I
X
i=1
(xi −trans[wi,θ])T (xi −trans[wi,θ)])
#
.
(15.22)
The solutions to this least squares problem for each of the four transformation types
are presented in Sections 15.2.1–15.2.4. The details differ in each case, but they have the
common approach of reducing the problem into a standard form for which the solution
is known. The algorithms are somewhat involved, and these sections can be skipped on
ﬁrst reading.
15.2.1 Learning Euclidean parameters
The Euclidean transformation is determined by a 2 × 2 rotation matrix Ωand a 2 ×
15.1 1 translation vector τ = [τx,τy]T (Equations 15.4 and 15.5). Each pair of matching
points {xi,wi} contributes two constraints to the solution (deriving from the x- and
y-coordinates). Since there are three underlying degrees of freedom, we will require at
least I = 2 pairs of points to get a unique estimate.
Our goal is to solve the problem
ˆΩ, ˆτ = argmin
Ω,τ
" I
X
i=1
(xi −euc[wi,Ω,τ])T (xi −euc[wi,Ω,τ])
#
= argmin
Ω,τ
" I
X
i=1
(xi −Ωwi −τ)T (xi −Ωwi −τ)
#
,
(15.23)
with the constraint that Ωis a rotation matrix so that ΩΩT = I and |Ω| = 1.
An expression for the translation vector can be found by taking the derivative of the
objective function with respect to τ, setting the result to zero and simplifying. The result
is the mean difference vector between the two sets of points after the rotation has been
applied
ˆτ =
PI
i=1 xi −Ωwi
I
= µx −Ωµw,
(15.24)
where µx is the mean of the points {xi} and µw is the mean of the points {wi}.
Substituting this result into the original criterion, we get
ˆΩ= argmin
Ω
" I
X
i=1
((xi −µx) −Ω(wi −µw))T ((xi −µx) −Ω(wi −µw))
#
. (15.25)

332
15 Models for transformations
Deﬁning matrices B = [x1 −µx,x2 −µx,...,xI −µx] and A = [w1 −µw,w2 −
µw,...,wI −µw], we can rewrite the objective function for the best rotation Ωas
ˆΩ= argmin
Ω
[|B −ΩA|F ]
subject to ΩΩT = I,|Ω| = 1,
(15.26)
where |•|F denotes the Frobenius norm. This is an example of an orthogonal Procrustes
problem. A closed form solution can be found by computing the SVD ULVT = BAT ,
and then choosing ˆΩ= VUT (see Appendix C.7.3).
15.2.2 Learning similarity parameters
The similarity transformation is determined by a 2 × 2 rotation matrix Ω, a 2 × 1 trans-
15.2 lation vector τ, and a scaling factor ρ (Equations 15.10 and 15.11). There are four
underlying degrees of freedom, so we will require at least I = 2 pairs of matching points
{xi,wi} to guarantee a unique solution.
The objective function for maximum likelihood ﬁtting of the parameters is
ˆΩ, ˆτ, ˆρ = argmin
Ω,τ,ρ
" I
X
i=1
(xi −sim[wi,Ω,τ,ρ])T (xi −sim[wi,Ω,τ,ρ])
#
= argmin
Ω,τ,ρ
" I
X
i=1
(xi −ρΩwi −τ)T (xi −ρΩwi −τ)
#
,
(15.27)
with the constraint that Ωis a rotation matrix so that ΩΩT = I and |Ω| = 1.
To optimize this criterion, we compute Ωexactly as for the Euclidean transformation.
The maximum likelihood solution for the scaling factor is given by
ˆρ =
PI
i=1(xi −µx)T ˆΩ(wi −µw)
PI
i=1(wi −µw)T (wi −µw)
,
(15.28)
and the translation can be found using
ˆτ =
PI
i=1(xi −ˆρ ˆΩwi)
I
.
(15.29)
15.2.3 Learning aﬃne parameters
The afﬁne transformation is parameterized by an unconstrained 2 × 2 matrix Φ and a
15.3 2 × 1 translation vector τ (Equations 15.13 and 15.14). There are six unknowns, and so
we need a minimum of I = 3 pairs of matching points {xi,wi} to guarantee a unique
solution. The learning problem can be stated as
ˆΦ, ˆτ = argmin
Φ,τ
" I
X
i=1
(xi −aff[wi,Φ,τ])T (xi −aff[wi,Φ,τ])
#
= argmin
Φ,τ
" I
X
i=1
(xi −Φwi −τ)T (xi −Φwi −τ)
#
.
(15.30)

15.2
Learning in transformation models
333
To solve this problem, observe that we can reexpress Φwi +τ as a linear function of
the unknown elements of Φ and τ
Φwi + τ =

ui
vi
1
0
0
0
0
0
0
ui
vi
1



φ11
φ12
τx
φ21
φ22
τy


= Aib,
(15.31)
where Ai is a 2 × 6 matrix based on the point wi, and b contains the unknown
parameters. The problem can now be written as
ˆb = argmin
b
" I
X
i=1
(xi −Aib)T (xi −Aib)
#
,
(15.32)
which is a linear least squares problem and can be solved easily (Appendix C.7.1).
15.2.4 Learning projective parameters
The projective transformation or homography is parameterized by a 3 × 3 matrix Φ
(Equations 15.17 and 15.18), which is ambiguous up to scale, giving a total of eight
degrees of freedom. Consequently, we need a minimum of I = 4 pairs of corresponding
15.4 points for a unique solution. This neatly matches our expectations: a homography can
map any four points in the plane to any other four points, and so it is reasonable that we
should need at least four pairs of points to determine it.
The learning problem can be stated as
ˆΦ = argmin
Φ
" I
X
i=1
(xi −hom[wi,Φ])T (xi −hom[wi,Φ])
#
(15.33)
= argmin
Φ
" I
X
i=1

xi −φ11ui + φ12vi + φ13
φ31ui + φ32vi + φ33
2
+

yi −φ21ui + φ22vi + φ23
φ31ui + φ32vi + φ33
2#
.
Unfortunately, there is no closed form solution to this nonlinear problem and to ﬁnd the
answer we must rely on gradient-based optimization techniques. Since there is a scale
ambiguity, this optimization would normally be carried out under the constraint that the
sum of the squares of the elements of Φ is one.
A successful optimization procedure depends on a good initial starting point, and for
this we use the direct linear transformation or DLT algorithm. The DLT algorithm uses
homogeneous coordinates where the homography is a linear transformation and ﬁnds a
closed form solution for the algebraic error. This is not the same as optimizing the true
objective function (Equation 15.33), but provides a result that is usually very close to the
true answer and can be used as a starting point for the nonlinear optimization of the true
criterion. In homogeneous coordinates we have
λ


xi
yi
1

=


φ11
φ12
φ13
φ21
φ22
φ23
φ31
φ32
φ33




ui
vi
1

.
(15.34)

334
15 Models for transformations
Each homogeneous coordinate can be considered as a direction in 3D space (Figure
14.11). So, this equation states that the left-hand side ˜xi represents the same direction in
space as the right-hand side, Φ ˜wi. If this is the case, their cross product must be zero, so
that
˜x × Φ ˜w = 0.
(15.35)
Writing this constraint in full gives the relations


y(φ31u + φ32v + φ33) −(φ21u + φ22v + φ23)
(φ11u + φ12v + φ13) −x(φ31u + φ32v + φ33)
x(φ21u + φ22v + φ23) −y(φ11u + φ12v + φ13)

= 0.
(15.36)
This appears to provide three linear constraints on the elements of Φ. However, only two
of these three equations are independent, so we discard the third. We now stack the ﬁrst
two constraints from each of the I pairs of points {xi,wi} to form the system


0
0
0
−u1
−v1
−1
y1u1
y1v1
y1
u1
v1
1
0
0
0
−x1u1
−x1v1
−x1
0
0
0
−u2
−v2
−1
y2u2
y2v2
y2
u2
v2
1
0
0
0
−x2u2
−x2v2
−x2
...
...
...
...
...
...
...
...
...
0
0
0
−uI
−vI
−1
yIuI
yIvI
yI
uI
vI
1
0
0
0
−xIuI
−xIvI
−xI




φ11
φ12
φ13
φ21
φ22
φ23
φ31
φ32
φ33


= 0,
(15.37)
which has the form Aφ = 0.
We solve this system of equations in a least squares sense with the constraint φT φ = 1
to prevent the trivial solution φ = 0. This is a standard problem (see Appendix C.7.2). To
ﬁnd the solution, we compute the SVD A = ULVT and choose φ to be the last column
of V. This is reshaped into a 3×3 matrix Φ and used as a starting point for the nonlinear
optimization of the true criterion (Equation 15.33).
15.3
Inference in transformation models
We have introduced four transformations (Euclidean, similarity, afﬁne, projective) that
15.5 relate positions w on a real-world plane to their projected positions x in the image and
discussed how to learn their parameters. In each case, the transformation took the form of
a generative model Pr(x|w). In this section, we consider how to infer the world position
w from the image position x.
For simplicity, we will take a maximum likelihood approach to this problem. For the
generic transformation trans[wi,θ], we seek
ˆw = argmax
w

log

Normx

trans[w,θ],σ2I

= argmin
w
h
(x −trans[w,θ])T (x −trans[w,θ)])
i
.
(15.38)
It is clear that this will be achieved when the image point and the predicted image point
agree exactly so that
x = trans[w,θ].
(15.39)

15.4
Three geometric problems for planes
335
We can ﬁnd the w = [u,v]T that makes this true by moving to homogeneous
coordinates. Each of the four transformations can be written in the form
λ


x
y
1

=


φ11
φ12
φ13
φ21
φ22
φ23
φ31
φ32
φ33




u
v
1

,
(15.40)
where the exact expressions are given by Equations 15.4, 15.10, 15.13, and 15.17.
To ﬁnd the position w = [u,v]T , we simply premultiply by the inverse of the
transformation matrix to yield
λ′


u
v
1

=


φ11
φ12
φ13
φ21
φ22
φ23
φ31
φ32
φ33


−1 

x
y
1

,
(15.41)
and then recover u and v by converting back to Cartesian coordinates.
15.4
Three geometric problems for planes
We have introduced transformation models that mapped from 2D coordinates on the
plane to 2D coordinates in the image, the most general of which was the homography.
Now we relate this model back to the full pinhole camera and revisit the three geometric
problems from Chapter 14 for the special case of a planar scene. We will show how to
• learn the extrinsic parameters (compute the geometric relationship between the
plane and the camera),
• learn the intrinsic parameters (calibrate from a plane), and
• infer the 3D coordinates of a point in the plane relative to the camera, given its
image position.
These three problems are illustrated in Figures 15.8, 15.9, and 15.11, respectively.
15.4.1 Problem 1: learning extrinsic parameters
Given I 3D points {wi}I
i=1 that lie on a plane so that wi = 0, their corresponding
15.6 projections in the image {xi}I
i=1 and known intrinsic matrix Λ, estimate the extrinsic
parameters
ˆΩ, ˆτ = argmax
Ω,τ
" I
X
i=1
log[Pr(xi|wi,Λ,Ω,τ)]
#
= argmax
Ω,τ
" I
X
i=1
log

Normxi[pinhole[wi,Λ,Ω,τ],σ2I]

#
,
(15.42)
where Ωis a 3 × 3 rotation matrix and τ is a 3 × 1 translation vector (Figure 15.8).
The extrinsic parameters transform points w = [u,v,0] on the plane into the coordinate
system of the camera. Unfortunately, this problem (still) cannot be solved in closed
form and requires nonlinear optimization. As usual, it is possible to get a good initial
estimate for the parameters using a closed form algebraic solution based on homogeneous
coordinates.

336
15 Models for transformations
Figure 15.8 Problem 1 – Learning extrinsic parameters. Given points {wi}I
i=1 on a plane, their
positions {x}I
i=1 in the image and intrinsic parameters Λ, ﬁnd the rotation Ωand translation τ
relating the camera and the plane. a) When the rotation and translation are wrong, the image points
predicted by the model (where the rays strike the image plane) do not agree well with the observed
points x. b) When the rotation and translation are correct, they agree well and the likelihood
Pr(x|w,Λ,Ω,τ) will be high.
From Equation 15.16, the relation between a homogeneous point on the plane and its
projection in the image is
λ


x
y
1

=λ′


φx
γ
δx
0
φy
δy
0
0
D




ω11
ω12
τx
ω21
ω22
τy
ω31
ω32
τz




u
v
1

=


φ11
φ12
φ13
φ21
φ22
φ23
φ31
φ32
φ33




u
v
1

. (15.43)
Our approach is to (i) calculate the homography Φ between the points w = [u,v]T on
the plane and the points x = [x,y]T in the image using the method of Section 15.2.4 and
then (ii) decompose this homography to recover the rotation matrix Ωand translation
vector τ.
As a ﬁrst step toward this decomposition, we eliminate the effect of the intrinsic
parameters by premultiplying the estimated homography by the inverse of the intrinsic
matrix Λ. This gives a new homography Φ′ = Λ−1Φ such that


φ′
11
φ′
12
φ′
13
φ′
21
φ′
22
φ′
23
φ′
31
φ′
32
φ′
33

= λ′


ω11
ω12
τx
ω21
ω22
τy
ω31
ω32
τz

.
(15.44)
To estimate the ﬁrst two columns of the rotation matrix Ω, we compute the SVD of
the ﬁrst two columns of Φ′


φ′
11
φ′
12
φ′
21
φ′
22
φ′
31
φ′
32

= ULVT ,
(15.45)
and then set


ω11
ω12
ω21
ω22
ω31
ω32

= U


1
0
0
1
0
0

VT .
(15.46)
These operations ﬁnd the closest valid ﬁrst two columns of a rotation matrix to the ﬁrst
two columns of Φ′ in a least squares sense. This approach is very closely related to the
solution to the orthogonal Procrustes problem (Appendix C.7.3).

15.4
Three geometric problems for planes
337
We then compute the last column [ω13,ω23,ω33]T of the rotation matrix by taking
the cross product of the ﬁrst two columns: this guarantees a vector that is also length one
and is perpendicular to the ﬁrst two columns, but the sign may still be wrong: we test
the determinant of the resulting rotation matrix Ω, and if it is −1, we multiply the last
column by −1.
We can now estimate the scaling factor λ′ by taking the average of the scaling factors
between these six elements
λ′ =
P3
m=1
P2
n=1 φ′
mn/ωmn
6
,
(15.47)
and this allows us to estimate the translation vector as τ = [φ′
13,φ′
23,φ′
33]T /λ′. The result
of this algorithm is a very good initial estimate of the extrinsic matrix [Ω,τ], which can
be improved by optimizing the correct objective function (Equation 15.42).
15.4.2 Problem 2: learning intrinsic parameters
In this section, we revisit the problem of learning the intrinsic parameters of the camera
15.7 (Figure 15.9). This is referred to as camera calibration. In Section 14.5, we developed a
method based on viewing a special 3D calibration target. In practice, it is hard to man-
ufacture a three-dimensional object with easy-to-ﬁnd visual features at precisely known
locations. However, it is easy to manufacture a 2D object of this kind. For example, it is
possible to print out a checkerboard pattern and attach this to a ﬂat surface (Figure 15.10).
Unfortunately, a single view of a 2D calibration object is not sufﬁcient to uniquely iden-
tify the intrinsic parameters. However, observing the same pattern from several different
viewpoints does sufﬁce.
Hence, the calibration problem can be reformulated as follows: given a planar object,
with I distinct 3D points {wi}I
i=1 on the surface and the corresponding projections in
J images {xij}I,J
i=1,j=1, establish the intrinsic parameters in the form of the intrinsic
Figure 15.9 Problem 2 – Learning intrinsic parameters. Given a set of points {wi}I
i=1 on a plane
in the world (blue line) and the 2D positions {xi}I
i=1 of these points in an image, ﬁnd the intrinsic
parameters Λ. To do this, we must also simultaneously estimate the extrinsic parameters Ω,τ.
a) When the intrinsic and extrinsic parameters are wrong, the prediction of the pinhole camera
(where rays strike the image plane) deviates signiﬁcantly from the known 2D points. b) When they
are correct, the prediction of the model will agree with the observed image. To make the solution
unique, the plane must be seen from several different angles.

338
15 Models for transformations
Figure 15.10 Calibration from a plane. It
is considerably easier to make a 2D calibra-
tion target than to machine an accurate 3D
object.
Consequently, calibration is usu-
ally based on viewing planes such as this
checkerboard. Unfortunately, a single view
of a plane is not sufﬁcient to uniquely deter-
mine the intrinsic parameters.
Therefore
cameras are usually calibrated using several
images of the same plane, where the plane
has a different pose relative to the camera
in each image.
matrix Λ:
ˆΛ = argmax
Λ


max
Ω1...J,τ 1...J


I
X
i=1
J
X
j=1
log[Pr(xij|wi,Λ,Ωj,τ j)]



.
(15.48)
A simple approach to this problem is to use a coordinate ascent technique in which
we alternately
• Estimate the J extrinsic matrices relating the object frame of reference to the
camera frame of reference in each of the J images,
ˆΩj, ˆτ j = argmax
Ωj,τ j
" I
X
i=1
log[Pr(xij|wi,Λ,Ωj,τ j)]
#
,
(15.49)
using the method of the previous section and then,
• Estimate the intrinsic parameters using a minor variation of the method described
in Section 14.5:
ˆΛ = argmax
Λ


I
X
i=1
J
X
j=1
log[Pr(xij|wi,Λ,Ωj,τ j)]

.
(15.50)
As for the original calibration method (Section 14.5), a few iterations of this pro-
cedure will yield a useful initial estimate of the intrinsic parameters and these can be
improved by directly optimizing the true objective function (Equation 15.48). It should
be noted that this method is quite inefﬁcient and is described for pedagogical reasons; it is
easy both to understand and to implement. A modern implementation would use a more
sophisticated technique to ﬁnd the intrinsic parameters (see Hartley and Zisserman 2004).
15.4.3 Problem 3: inferring 3D position relative to camera
Given a calibrated camera (i.e., camera with known Λ) that is known to be related to a
planar scene by extrinsic parameters Ω,τ, ﬁnd the 3D point w that is responsible for a
given 2D position x in the image.
When the scene is planar there is normally a one-to-one relationship between points
in the 3D world and points in the image. To compute the 3D point corresponding to a
point x, we initiallty infer the position w = [u,v,0]T on the plane. We exploit our knowl-
edge of the intrinsic and extrinsic parameters to compute the homography Φ mapping

15.5
Transformations between images
339
Figure 15.11 Inferring a 3D position rel-
ative to the camera.
When the object is
planar (blue line), there is a one-to-one
mapping between points x in the image and
points w on the plane.
If we know the
intrinsic matrix, and the rotation and trans-
lation of the plane relative to the camera,
we can infer the 3D position w from the
observed 2D image point x.
from points in the world to points in the image,
T =


φ11
φ12
φ13
φ21
φ22
φ23
φ31
φ32
φ33

=


φx
γ
δx
0
φy
δy
0
0
D




ω11
ω12
τx
ω21
ω22
τy
ω31
ω32
τz

.
(15.51)
We then infer the coordinates w = [u,v,0]T on the plane by inverting this transformation
˜w = T−1˜x.
(15.52)
Finally, we transfer the coordinates back to the frame of reference of the camera to give
w′ = Ωw + τ.
(15.53)
15.5
Transformations between images
So far, we have considered transformations between a plane in the world and its image
in the camera. We now consider two cameras viewing the same planar scene. The one-
to-one mapping from positions on the plane to the positions in the ﬁrst camera can be
described by a homography. Similarly, the one-to-one mapping from positions on the
plane to the positions in the second camera can be described by a second homography.
It follows that there is a one-to-one mapping from the position in the ﬁrst camera to the
position in the second camera. This can also be described by a homography. The same
logic follows for the other transformation types. Consequently, it is very common to ﬁnd
pairs of real-world images that are geometrically related by one of these transformations.
For example, the image of the photograph in Figure 15.5a is related by a homography to
that in 15.5b.
Let us denote the 3 × 3 matrix mapping points on the plane to points in the ﬁrst
image by T1. Similarly, we denote the 3 × 3 matrix mapping points on the plane to
points in the second image by T2. To map from image 1 to image 2, we ﬁrst apply the
transformation from image 1 to the plane itself. By the argument of the previous section,
this is T−1
1 . Then we apply the transformation from the plane to image 2, which is T2.
The mapping T3 from image 1 to image 2 is the concatenation of these operations and is
hence T3 = T2T−1
1
(Figure 15.12).
15.5.1 Geometric properties of the homography
We’ve seen that transformations between planes in the world and the image plane are
described by homographies, and so are transformations between multiple images of a
real-world plane. There is another important family of images where the images are

340
15 Models for transformations
Figure 15.12 Transformations between images. Two cameras view the same planar scene. The
relations between the 2D points on this plane and the two images are captured by the 3 × 3 trans-
formation matrices T1 and T2, respectively. It follows that the transformation from the ﬁrst image
to the points on the plane is given by T−1. We can compute the transformation T3 from the ﬁrst
image to the second image by transforming from the ﬁrst image to the plane and then transforming
from the plane to the second image, giving the ﬁnal result T3 = T2T−1
1 .
related to one another by the homography. Recall that the homography mapping point
x1 to point x2 is linear in homogeneous coordinates:
λ


x1
y1
1

=


φ11
φ12
φ13
φ21
φ22
φ23
φ31
φ32
φ33




x2
y2
1

.
(15.54)
The homogeneous coordinates represent 2D points as directions or rays in a 3D space
(Figure 14.11). When we apply a homography to a set of 2D points, we can think of this
as applying a linear transformation (rotation, scaling, and shearing) to a bundle of rays
in 3D. The positions where the transformed rays strike the plane at w = 1 determine the
ﬁnal 2D positions.
We could yield the same results by keeping the rays ﬁxed and applying the inverse
transformation to the plane so that it cuts the rays in a different way. Since any plane
can be mapped to any other plane by a linear transformation, it follows that the images
created by cutting a ray bundle with different planes are all related to one another by
homographies (Figure 15.13). In other words, the images seen by different cameras with
the pinhole in the same place are related by homographies. So, for example, if a camera
zooms (the focal length increases), then the images before and after the zoom are related
by a homography.
This relationship encompasses an important special case (Figure 15.14). If the cam-
era rotates but does not translate, then the image plane still intersects the same set of
rays. It follows that the projected points x1 before the rotation and the projected points
x2 after the rotation are related by a homography. It can be shown that the homography

15.5
Transformations between images
341
Figure 15.13 Geometric interpretation of homography. A ray bundle is formed by connecting
rays from an optical center to points on a real-world object (the cube). A set of planes cut the ray
bundle, forming a series of images of the cube. Each of these images is related to every other by a
homography.
Figure 15.14 Images under pure camera rotation. When the camera rotates but does not translate,
the bundle of rays remains the same, but is cut by a different plane. It follows that the two images
are related by a homography.
Φ mapping from image 1 to image 2 is given by
Φ = ΛΩ2Λ−1,
(15.55)
where Λ is the intrinsic matrix and Ω2 is the rotation matrix that maps the coordinate
system of the second camera to the ﬁrst. This relationship is exploited when we stitch
together images to form panoramas (Section 15.7.2).
In conclusion, the homography maps between
• Points on a plane in the world and their positions in an image,
• Points in two different images of the same plane, and
• Two images of a 3D object where the camera has rotated but not translated.

342
15 Models for transformations
15.5.2 Computing transformations between images
In the previous sections we have argued that it is common for two images to be related to
one another by a homography. If we denote the points in image 1 by xi and their corre-
sponding positions in image 2 as yi, then we could for example describe the mapping as
a homography
Pr(xi|yi) = Normxi

hom[yi,Φ],σ2I

.
(15.56)
This method ascribes all of the noise to the ﬁrst image and it should be noted that it
is not quite correct: we should really build a model that explains both sets of image data
with a set of hidden variables representing the original 3D points so that the estimated
point positions in each image are subject to noise. Nonetheless, the model in Equation
15.56 works well in practice. It is possible to learn the parameters using the technique of
Section 15.2.
15.6
Robust learning of transformations
We have discussed transformation models that can be used either to (i) map the positions
of points on a real-world plane to their projections in the image or (ii) map the positions
of points in one image to their corresponding positions in another. Until now, we have
assumed that we know a set of corresponding points from which to learn the parameters
of the transformation model. However, establishing these correspondences automatically
is a challenging task in itself.
Let us consider the case where we wish to compute a transformation between two
images. A simple method to establish correspondences would be to compute interest
points in each image and to characterize the region around each point using a region
descriptor such as the SIFT descriptor (Section 13.3.2). We could then greedily associate
points based on the similarity of the region descriptors (as in Figure 15.17c–d). Depend-
ing on the scene, this is likely to produce a set of matches that are 70−90% correct.
Unfortunately the remaining erroneous correspondences (which we will call outliers)
can severely hamper our ability to compute the transformation between the images. To
cope with this problem, we need robust learning methods.
15.6.1 RANSAC
Random sample consensus or RANSAC is a general method for ﬁtting models to data
where the data are corrupted by outliers. These outliers violate the assumptions of the
underlying probability model (usually a normal distribution) and can cause the estimated
parameters to deviate signiﬁcantly from their correct values.
For pedagogical reasons, we will describe RANSAC using the example of lin-
ear regression. We will subsequently return to the problem of learning parameters in
transformation models. The linear regression model is
Pr(y|x) = Normy[ax + b,σ2]
(15.57)
and was discussed at length in Section 8.1. Figure 15.15 shows an example of the pre-
dicted mean for y when we ﬁt the model to data with two outliers. The ﬁt has been
unduly inﬂuenced by the outliers and no longer describes the data well.
The goal of RANSAC is to identify which points are outliers and to eliminate them
from the ﬁnal ﬁt. This is a chicken and egg problem: if we had the ﬁnal ﬁt, then it would

15.6
Robust learning of transformations
343
Figure 15.15 Motivation for random sam-
ple consensus (RANSAC). The majority of
this data set (blue points) can be explained
well by a linear regression model, but there
are two outliers (green points).
Unfortu-
nately, if we ﬁt the linear regression model
to all of this data, the mean prediction (red
line) is dragged toward the outliers and no
longer describes the majority of the data
well. The RANSAC algorithm circumvents
this problem by establishing which data-
points are outliers and ﬁtting the model to
the remaining data.
be easy to identify the outliers (they are not well described by the model), and if we knew
which points were outliers, it would be easy to compute the ﬁnal ﬁt.
RANSAC works by repeatedly ﬁtting models based on random subsets of the data.
The hope is that sooner or later, there will be no outliers in the chosen subset, and so we
will ﬁt a good model. To enhance the probability of this happening, RANSAC chooses
subsets of the minimal size required to uniquely ﬁt the model. For example, in the case
of the line, it would choose subsets of size two.
Having chosen a minimal subset of datapoints and ﬁtted the model, RANSAC
assesses its quality. It does this by classifying points as inliers or outliers. This requires
some knowledge about the expected amount of variation around the true model. For lin-
ear regression this means we need some prior knowledge of the variance parameter σ2.
If a datapoint exceeds the expected variation (perhaps two standard deviations from the
mean), then it is classiﬁed as an outlier. Otherwise, it is an inlier. For each minimal
subset of data, we count the number of inliers.
We repeat this procedure a number of times: on each iteration we choose a random
minimal subset of points, ﬁt a model, and count the number of datapoints that agree (the
inliers). After a predetermined number of iterations, we then choose the model that had
the most inliers and reﬁt the model from these alone.
The complete RANSAC algorithm hence proceeds as follows (Figure 15.16):
1. Randomly choose a minimal subset of data.
2. Use this subset to estimate the parameters.
3. Compute the number of inliers for this model.
4. Repeat steps 1–3 a ﬁxed number of times.
5. Reestimate model using inliers from the best ﬁt.
If we know the degree to which our data are polluted with outliers, it is possible to
estimate the number of iterations that provide any given chance of ﬁnding the correct
answer.
Now let us return to the question of how to apply the model to ﬁtting geometric
15.8 transformations. We will use the example of a homography (Equation 15.56). Here we

344
15 Models for transformations
Figure 15.16 RANSAC procedure. a) We select a random minimal subset of points to ﬁt the line
(red points). We ﬁt the line to these points and count how many of the other points agree with
this solution (blue points). These are termed inliers. Here there are only three inliers. b,c) This
procedure is repeated with different minimal subsets of points. After a number of iterations we
choose the ﬁt that had the most inliers. We reﬁt the line using only the inliers from this ﬁt.
Figure 15.17 Fitting a homography with RANSAC. a,b) Original images of a textured plane. c,d)
162 strongest matches selected by greedy method. In each case the associated line travels from the
interest point to the position of its match in the other image. These matches are clearly polluted
by outliers. e,f) Model ﬁtted from 102 inliers identiﬁed by applying 100 iterations of RANSAC.
These matches form a coherent pattern as they are all described by a homography.
repeatedly choose subsets of hypothesized matches between the two images. The size of
the subset is chosen to be four as this is the minimum number of pairs of points that are
needed to uniquely identify the eight degrees of freedom of the homography.
For each subset, we count the number of inliers by evaluating Equation 15.56 and
selecting those where the likelihood exceeds some predetermined value. In practice, this
means measuring the distance between the points xi in the ﬁrst image and the mapped
position hom[y,Φ] of the points from the second image. After repeating this many times,
we identify the trial with the most inliers (Figure 15.17) and recompute the model from
these inliers alone.

15.6
Robust learning of transformations
345
Figure 15.18 Piecewise planarity. a) Although this scene is clearly not well described by a plane,
it is well described by a set of planes. b) Consequently, the mapping to this second image of the
same scene can be described by a set of homographies. Images from Oxford Colleges dataset.
15.6.2 Sequential RANSAC
Now let us look at a more challenging task. So far, we assumed that one set of points
15.9 could be mapped to another using a single transformation model. However, many scenes
containing man-made objects are piecewise planar (Figure 15.18). It follows that images
of such scenes are related by piecewise homographies. In this section, we will consider
methods for ﬁtting this type of model.
The ﬁrst approach is sequential RANSAC. The idea is simple: we ﬁt a single homog-
raphy to the scene using the RANSAC method. In principle this will correctly ﬁt a model
to one of the planes in the image and reject all the other points as outliers. We then
remove the points that belong to this plane (i.e., the inliers to the ﬁnal homography) and
repeat the procedure on the remaining points. Ideally, each iteration will identify a new
plane in the image.
Unfortunately, this method does not work well in practice (Figure 15.19) for two
reasons. First, the algorithm is greedy: any matches that are erroneously incorporated
into, or missed by one of the earlier models cannot be correctly assigned later. Second,
the model has no notion of spatial coherence, ignoring the intuition that nearby points
are more likely to belong to the same plane.
15.6.3 PEaRL
The Propose, Expand and Re-Learn or PEaRL algorithm solves both of these problems.
15.10 In the “propose” stage, K hypothetical models are generated where K is usually of the
order of several thousand. This can be achieved by using a RANSAC type procedure in
which minimal subsets of points are chosen, a model is ﬁtted, and the inliers are counted.
This is done repeatedly until we have several thousand models {Φk}K
k=1 that each have
a reasonable degree of support.
In the “expand” stage, we model the assignment of matched pairs to the proposed
models as a multilabel Markov random ﬁeld. We associate a label li ∈[1,2,...,K] to
each match {xi,yi} where the value of the label determines which one of the K models
is present at this point. The likelihood of each data pair {xi,yi} under the kth model is
given by
Pr(xi|yi,li = k) = Normxi

hom[yi,Φk],σ2I

.
(15.58)

346
15 Models for transformations
Figure 15.19 Sequential ﬁtting of homographies mapping between images in Figure 15.18 using
RANSAC. a) Running the RANSAC ﬁtting procedure once identiﬁes a set of points that all lie on
the same plane in the image. b) Running the RANSAC procedure again on the points that were
not explained by the ﬁrst plane identiﬁes a second set of points that lie on a different plane. c)
Third iteration. The plane discovered does not correspond to a real plane in the scene. d) Fourth
iteration. The model erroneously associates parts of the image which are quite separate. This
sequential approach fails for two reasons. First, it is greedy and cannot recover from earlier errors.
Second, it does not encourage spatial smoothness (nearby points should belong to the same model).
To incorporate spatial coherence, we choose a prior over the labels that encourages
neighbors to take similar values. In particular, we choose an MRF with a Potts model
potential
Pr(l) = 1
Z exp

−
X
i,j∈Np
wijδ[li −lj]

,
(15.59)
where Z is the constant partition function, wij is a weight associated with the pair of
matches i and j, and δ[•] is a function that returns 1 when the argument is zero and
returns 0 otherwise. The neighborhood Ni of each point must be chosen in advance. One
simple technique is to compute the K-nearest neighbors for each point in the ﬁrst image
and declare two points to be neighbors if either is in the other’s set of closest neighbors.
The weights wij are chosen so that points that are close in the image are more tightly
coupled than points that are distant.
The goal of the “expand” stage is then to infer the labels li and hence the asso-
ciation between models and datapoints.
This can be accomplished using the alpha
expansion algorithm (Section 12.4.1). Finally, having associated each datapoint with

15.7
Applications
347
Figure 15.20 Results of the PEaRL algorithm. It formulates the problem as inference in a multi-
label MRF. The MRF label associated with each matching pair denotes the index of one of a set of
possible models. Inferring these labels is alternated with reﬁning the parameters of the proposed
models. The colored points denote different labels in the ﬁnal solution. The algorithm has success-
fully identiﬁed many of the surfaces in the scene. Adapted from Isack and Boykov (2012). c⃝2012
Springer.
one of the models, we move to the “re-learn” stage. Here, we reestimate the param-
eters of each model based on the data that was associated with it. The “expand” and
“re-learn” stages are iterated until no further progress is made. At the end of this pro-
cess, we throw away any models that do not have sufﬁcient support in the ﬁnal solution
(Figure 15.20).
15.7
Applications
In this section we present two examples of the techniques in this chapter. First, we
discuss augmented reality in which we attempt to render an object onto a plane in the
scene. This application exploits the fact that there is a homography between the image
of the plane and the original object surface and uses the method of Section 15.4.1 to
decompose this homography to ﬁnd the relative position of the camera and the plane.
Second, we discuss creating visual panoramas. The method exploits the fact that mul-
tiple images taken from a camera that has rotated but not translated are all related by
homographies.
15.7.1 Augmented reality tracking
Figure 15.1 shows an example of augmented reality tracking. To accomplish this, the
following steps were taken. First, the four corners of the marker are found as in Figure
15.1c; the marker was designed so that the corners can be easily identiﬁed using a set
of image-processing operations. In brief, the image is thresholded, and then connected
dark regions are found. The pixels around the edge of each region are identiﬁed. Then
four 2D lines are ﬁt to the edge pixels using sequential RANSAC. Regions that are not

348
15 Models for transformations
well explained by four lines are discarded. The only remaining region in this case is the
square marker.
The positions where the ﬁtted lines intersect provides four points {xi}4
i=1 in the
image, and we know the corresponding positions {wi}4
i=1 in cm that occur on the sur-
face of the planar marker. It is assumed that the camera is calibrated, and so we know
the intrinsic matrix Λ. We can now compute the extrinsic parameters Ω,τ using the
algorithm from Section 15.4.1.
To render the graphical object, we ﬁrst set up a viewing frustum (the graphics equiv-
alent of the “camera”) so that it has the same ﬁeld of view as speciﬁed by the intrinsic
parameters. Then we render the model from the appropriate perspective using the extrin-
sic parameters as the modelview matrix.
The result is that the object appears to be
attached rigidly to the scene.
In this system, the points on the marker were found using a sequence of image-
processing operations. However, this method is rather outdated. It is now possible to
reliably identify natural features on an object so there is no need to use a marker with any
special characteristics. One way to do this is to match SIFT features between a reference
image of the object and the current scene. These interest point descriptors are invariant
to image scaling and rotation, and for textured objects it is usually possible to generate
a high percentage of correct matches. RANSAC is used to eliminate any mismatched
pairs.
However, SIFT features are themselves relatively slow to compute. Lepetit et al.
(2005) described a system that applies machine learning techniques to identify objects
at interactive speeds (Figure 15.21). They ﬁrst identify K stable keypoints (e.g., Harris
corners) on the object to be tracked. They then create a training set for each keypoint
by subjecting the region around it to a large number of random afﬁne transformations.
Finally, they train a multiclass classiﬁcation tree that takes a new keypoint and assigns it
to match one of the K original points. At each branch of the tree, the decision is made by
Figure 15.21 Robust tracking using keypoints. a) Lepetit et al. (2005) presented a system that
automatically tracked objects such as this book. b) In the learning stage, the regions around the
keypoints were subjected to a number of random afﬁne transformations. c) Keypoints in the image
were classiﬁed as belonging to a known keypoint on the object, using a tree-based classiﬁer that
compared the intensity at nearby points. Adapted from Lepetit et al. (2005). c⃝2005 IEEE.

15.7
Applications
349
Figure 15.22 Computing visual panoramas. a–c) Three images of the same scene where the
camera has rotated but not translated. Five matching points have been identiﬁed by hand between
each pair. d) A panorama can be created by mapping the ﬁrst and third images into the frame of
reference of the second image.
a pairwise intensity comparison. This system works very quickly and reliably matches
most of the feature points. Once more, RANSAC can be used to eliminate any erroneous
matches.
15.7.2 Visual panoramas
A second application of the ideas of this chapter is the computation of visual panoramas.
Recall that a set of pictures that are taken by rotating a camera about the optical center
are all related by homographies. Hence, if we take pictures of this kind that are partially
overlapping, it is possible to map them all into one large image. This process is known
as image mosaicing.
An example is shown in Figure 15.22. This was constructed by placing the second
(middle) photo into the center of a much larger empty image. Then 5 matches were
identiﬁed by hand between this expanded second image and the ﬁrst image and the
homography from the expanded second image to the ﬁrst image was computed. For
each empty pixel in the expanded second image we compute the position in the ﬁrst
image using this homography. If this falls within the image boundaries, we copy the
pixel value.
This process can be completely automated by ﬁnding features and ﬁtting a homogra-
phy to each pair of images using a robust technique such as RANSAC. In a real system,
the ﬁnal result is typically projected onto a cylinder and unrolled to give a more visually
pleasing result.

350
15 Models for transformations
Discussion
This chapter has presented a number of important ideas. First, we discussed a family of
transformations and how each can be related to a camera viewing the scene under special
conditions. These transformations are used widely within machine vision and we will
see them exploited in Chapters 17–19. Second, we discussed a more practical method
for camera calibration based on viewing a plane at a number of different orientations.
Finally, we have also presented RANSAC, which is a robust method to ﬁt models, even
in the presence of noisy observations.
Notes
Transformations: For more information concerning the hierarchy of 2D transformations, consult
Hartley and Zisserman (2004). The closed form solutions for the rotation and similarity transfor-
mations are special cases of Procrustes problems. Many more details about this type of problem
can be found in Gower and Dijksterhuis (2004). The direct linear transformation algorithm for
estimating the homography dates back to at least Sutherland (1963). Hartley and Zisserman (2004)
present a detailed discussion of different objective functions for estimating homographies. In this
chapter, we have discussed the estimation of transformations from point matches. However, it is
also possible to compute transformations from other geometric primitives. For example, methods
exist to compute the homography from matched lines (see Problem 15.3), a combination of points
and lines (Murino et al. 2002), or conics (Sugimoto 2000).
Robust estimation: The RANSAC algorithm is due to Fischler and Bolles (1981).
It has
spawned many variants, the most notable of which is MLESAC (Torr and Zisserman 2000)
which puts this ﬁtting method on a sound probabilistic footing (see Problem 15.13 for a related
model). Chum et al. (2005) and Frahm and Pollefeys (2006) present variants of RANSAC that
can cope with degenerate data (where the model is nonunique). Torr (1998) and Vincent and
Laganiere (2001) used RANSAC to estimate multiple geometric entities sequentially. Both Ragu-
ram et al. (2008) and Choi et al. (2009) present recent quantitative comparisons of variations of the
RANSAC algorithm. The PEaRL algorithm is due to Isack and Boykov (2012). In the orig-
inal paper, they also include an extra cost, which encourages parsimony (to describe the data
with as few models as possible). Other approaches to robust estimation include (i) the use of
long-tailed distributions such as the t-distribution (Section 7.5), (ii) M-estimators (Huber 2009),
which replace the least-squares criterion with another function that penalizes large deviations
less stringently, and (iii) the self-explanatory least median of squares regression (Rousseeuw
1984).
Augmented reality: Pose estimation methods for augmented reality initially relied on detecting
special patterns in the scene known as ﬁducial markers. Early examples used circular patterns
(e.g. Cho et al. 1998; State et al. 1996), but these were largely supplanted by square markers
(e.g., Rekimoto 1998; Kato et al. 2000; Kato and Billinghurst 1999; Koller et al. 1997). The
system described in the text is ARToolkit (Kato and Billinghurst 1999; Kato et al. 2000) and can
be downloaded from http://www.hitl.washington.edu/artoolkit/.
Other systems have used “natural image features.” For example, Harris (1992) estimated the pose
of an object using line segments. Simon et al. (2000) and Simon and Berger (2002) estimated
the pose of planes in the image using the results of a corner detector. More information about
computing the pose of a plane can be found in Sturm (2000).
More recent systems have used interest point detectors such as SIFT features, which are robust
to changes in illumination and pose (e.g., Skrypnyk and Lowe 2004). To increase the speed of
systems, features are matched using machine learning techniques (Lepetit and Fua 2006; ¨Ozuysal
et al. 2010), and current systems can now operate at interactive speeds on mobile hardware (Wagner

Problems
351
et al. 2008). A review of methods to estimate and track the pose of rigid objects can be found in
Lepetit and Fua (2005).
Calibration from a plane: Algorithms for calibration from several views of a plane can be found
in Sturm and Maybank (1999) and Zhang (2000). They are now used much more frequently
than calibration based on 3D objects, for the simple reason that accurate 3D objects are harder
to manufacture.
Image mosaics: The presented method for computing a panorama by creating a mosaic of images
is na¨ıve in a number of ways. First, it is more sensible to explicitly estimate the rotation matrix and
calibration parameters rather than the homography (Szeliski and Shum 1997; Shum and Szeliski
2000; Brown and Lowe 2007). Second, the method that we describe projects all of the images onto
a single plane, but this does not work well when the panorama is too wide as the images become
increasingly distorted. A more sensible approach is to project images onto a cylinder (Szeliski
1996; Chen 1995) which is then unrolled and displayed as an image. Third, the method to blend
together images is not discussed. This is particularly important if there are moving objects in the
image. A good review of these and other issues can be found in Szeliski (2006) and chapter 9 of
Szeliski (2010).
Problems
15.1 The 2D point x2 is created by a rotating point x1 using the rotation matrix Ω1 and then
translating it by the translation vector τ 1 so that
x2 = Ω1x1 + τ 1.
Find the parameters Ω2 and τ 2 of the inverse transformation
x1 = Ω2x2 + τ 2
in terms of the original parameters Ω1 and τ 1.
15.2 A 2D line can be as expressed as ax + by + c = 0 or in homogeneous terms
l˜x = 0,
where l = [a,b,c]. If points are transformed so that
˜x′ = T˜x,
what is the equation of the transformed line?
15.3 Using your solution from Problem 15.2, develop a linear algorithm for estimating a homog-
raphy based on a number of matched lines between the two images (i.e., the analogue of the DLT
algorithm for matched lines).
15.4 A conic (see Problem 14.6) is deﬁned by
˜xT C˜x = 0,
where C is a 3 × 3 matrix. If the points in the image undergo the transformation
˜x′ = T˜x,
then what is the equation of the transformed conic?

352
15 Models for transformations
15.5 All of the 2D transformations in this chapter (Euclidean, similarity, afﬁne, projective) have
3D equivalents. For each class write out the 4 × 4 matrix that describes the 3D transformation in
homogeneous coordinates. How many independent parameters does each model have?
15.6 Devise an algorithm to estimate a 3D afﬁne transformation based on two sets of matching 3D
points. What is the minimum number of points required to get a unique estimate of the parameters
of this model?
15.7 A 1D afﬁne transformation acts on 1D points x as x′ = ax + b. Show that the ratio of two
distances is invariant to a 1D afﬁne transformation so that
I = x1 −x2
x2 −x3 = x′
1 −x′
2
x′
2 −x′
3
.
15.8 A 1D projective transformation acts on 1D points x as x′ = (ax + b)/(cx + d). Show that
the cross-ratio of distances is invariant to a 1D projective transformation so that
I = (x3 −x1)(x4 −x2)
(x3 −x2)(x4 −x1) = (x′
3 −x′
1)(x′
4 −x′
2)
(x′
3 −x′
2)(x′
4 −x′
1).
It was proposed at one point to exploit this type of invariance to recognize planar objects under
different transformations (Rothwell et al. 1995). However, this is rather impractical as it assumes
that we can identify a number of the points on the object in a ﬁrst place.
15.9 Show that Equation 15.36 follows from Equation 15.35.
15.10 A camera with intrinsic matrix λ and extrinsic parameters Ω= I,τ = 0 takes an image and
then rotates to a new position Ω= Ω1,τ = 0 and takes a second image. Show that the homography
relating these two images is given by
Φ = ΛΩ1Λ−1.
15.11 Consider two images of the same scene taken from a camera that rotates, but does not
translate between taking the images. What is the minimum number of point matches required
to recover a 3D rotation between two images taken using a camera where the intrinsic matrix is
known?
15.12 Consider the problem of computing a homography from point matches that include outliers.
If 50 percent of the initial matches are correct, how many iterations of the RANSAC algorithm
would we expect to have to run in order to have a 95 percent chance of computing the correct
homography?
15.13 A different approach to ﬁtting transformations in the presence of outliers is to model the
uncertainty as a mixture of two Gaussians. The ﬁrst Gaussian models the image noise, and the sec-
ond Gaussian, which has a very large variance, accounts for the outliers. For example, for the afﬁne
transformation we would have
Pr(x|w) = λNormx

aff[w,Φ,τ],σ2I

+ (1 −λ)Normx

aff[w,Φ,τ],σ2
0I

+

Problems
353
where λ is the probability of being an inlier, σ2 is the image noise, and σ2
0 is the large variance
that accounts for the outliers. Sketch an approach to learning the parameters σ2,Φ,τ, and λ of
this model. You may assume that σ2
0 is ﬁxed. Identify a possible weakness of this model.
15.14 In the description of how to compute the panorama (Section 15.7.2), it is suggested that
we take each pixel in the central image and transform it into the other images and then copy the
color. What is wrong with the alternate strategy of taking each pixel from the other images and
transforming them into the central image?

Chapter 16
Multiple cameras
This chapter extends the discussion of the pinhole camera model. In Chapter 14 we
showed how to ﬁnd the 3D position of a point based on its projections into multiple
cameras. However, this approach was contingent on knowing the intrinsic and extrinsic
parameters of these cameras, and this information is often unknown.
In this chap-
ter we discuss methods for reconstruction in the absence of such information. Before
reading this chapter, readers should ensure that they are familiar with the mathematical
formulation of the pinhole camera (Section 14.1).
To motivate these methods, consider a single camera moving around a static object.
The goal is to build a 3D model from the images taken by the camera. To do this, we
will also need to simultaneously establish the properties of the camera and its position
in each frame. This problem is widely known as structure from motion, although this is
something of a misnomer as both “structure” and “motion” are recovered simultaneously.
The structure from motion problem can be stated formally as follows. We are given J
images of a rigid object that is characterized by I distinct 3D points {wi}I
i=1. The images
are taken with the same camera at a series of unknown positions. Given the projections
{xij}I,J
i=1,j=1 of the I points in the J images, establish the 3D positions {wi}I
i=1 of
the points in the world, the ﬁxed intrinsic parameters Λ, and the extrinsic parameters
{Ωj,τ j}J
j=1 for each image:
{ ˆwi}I
i=1,{ ˆΩj, ˆτ j}J
j=1, ˆΛ
(16.1)
= argmax
w,Ω,τ,Λ


I
X
i=1
J
X
j=1
log[Pr(xij|wi,Λ,Ωj,τ j)]


= argmax
w,Ω,τ,Λ


I
X
i=1
J
X
j=1
log

Normxij

pinhole[wi,Λ,Ωj,τ j],σ2I


.
Since this objective function is a based on the normal distribution we can reformulate it
as a least squares problem of the form
{ ˆwi}I
i=1,{ ˆΩj, ˆτ j}J
j=1, ˆΛ =
(16.2)
argmin
w,Ω,τ,Λ


I
X
i=1
J
X
j=1
(xij−pinhole[wi,Λ,Ωj,τ j])T(xij−pinhole[wi,Λ,Ωj,τ j])

,

16.1
Two-view geometry
355
in which the goal is to minimize the total squared distance between the observed image
points and those predicted by the model. This is known as the squared reprojection
error. Unfortunately, there is no simple closed form solution for this problem, and we
must ultimately rely on nonlinear optimization. However, to ensure that the optimization
converges, we need good initial estimates of the unknown parameters.
To simplify our discussion, we will concentrate ﬁrst on the case where we have J =2
views, and the intrinsic matrix Λ is known. We already saw how to estimate 3D points
given known camera positions in Section 14.6, so the unresolved problem is to get good
initial estimates of the extrinsic parameters. Surprisingly, it is possible to do this based on
just examining the positions of corresponding points without having to reconstruct their
3D world positions. To understand why, we must ﬁrst learn more about the geometry of
two views.
16.1
Two-view geometry
In this section, we show that there is a geometric relationship between corresponding
points in two images of the same scene. This relationship depends only on the intrinsic
parameters of the two cameras and their relative translation and rotation.
16.1.1 The epipolar constraint
Consider a single camera viewing a 3D point w in the world. We know that w must lie
somewhere on the ray that passes through the optical center and position x1 on the image
plane (Figure 16.1). However, from one camera alone, we cannot know how far along
this ray the point is.
Figure 16.1 Epipolar line. Consider point x1 in the ﬁrst image. The 3D point w that projected
to x1 must lie somewhere along the ray that passes from the optical center of camera 1 through
the position x1 in the image plane (dashed green line). However, we don’t know where along that
ray it lies (4 possibilities shown). It follows that x2, the projected position in camera 2, must lie
somewhere on the projection of this ray. The projection of this ray is a line in image 2 and is
referred to as an epipolar line.

356
16 Multiple cameras
Now consider a second camera viewing the same 3D world point. We know from the
ﬁrst camera that this point must lie along a particular ray in 3D space. It follows that
the projected position x2 of this point in the second image must lie somewhere along the
projection of this ray in the second image. The ray in 3D projects to a 2D line which is
known as an epipolar line.
This geometric relationship tells us something important: for any point in the ﬁrst
image, the corresponding point in the second image is constrained to lie on a line. This
is known as the epipolar constraint. The particular line that it is constrained to lie on
depends on the intrinsic parameters of the cameras and the extrinsic parameters (i.e., the
relative translation and rotation of the two cameras).
The epipolar constraint has two important practical implications.
1. Given the intrinsic and extrinsic parameters, we can ﬁnd point correspondences
relatively easily: for a given point in the ﬁrst image, we only need to perform a 1D
search along the epipolar line in the second image for the corresponding position.
2. The constraint on corresponding points is a function of the intrinsic and extrinsic
parameters; given the intrinsic parameters, we can use the observed pattern of
point correspondences to determine the extrinsic parameters and hence establish
the geometric relationship between the two cameras.
16.1.2 Epipoles
Now consider a number of points in the ﬁrst image.
Each is associated with a ray
in 3D space. Each ray projects to form an epipolar line in the second image. Since
all the rays converge at the optical center of the ﬁrst camera, the epipolar lines must
converge at a single point in the second image plane; this is the image in the second
camera of the optical center of the ﬁrst camera and is known as the epipole (Figure 16.2).
Figure 16.2 Epipoles. Consider several observed points {xi}I
i=1 in image 1. For each point, the
corresponding 3D world position wi lies on a different ray. Each ray projects to an epipolar line li
in image 2. Since the rays converge in 3D space at the optical center of camera 1, the epipolar lines
must also converge. The point where they converge is known as the epipole e2. It is the projection
of the optical center of camera 1 into camera 2. Similarly, the epipole e1 is the projection of the
optical center of camera 2 into camera 1.

16.2
The essential matrix
357
Figure 16.3 Epipolar lines and epipoles. a) When the camera movement is a pure translation
perpendicular to the optical axis (parallel to the image plane), the epipolar lines are parallel and the
epipole is at inﬁnity. b) When the camera movement is a pure translation along the optical axis,
the epipoles are in the center of the image and the epipolar lines form a radial pattern.
Similarly, points in image 2 induce epipolar lines in image 1, and these epipolar lines
converge at the epipole in image 1. This is the image in camera 1 of the optical center of
camera 2.
The epipoles are not necessarily within the observed images: the epipolar lines
may converge to a point outside the visible area. Two common cases are illustrated
in Figure 16.3. When the cameras are oriented in the same direction (i.e., no relative
rotation) and the displacement is perpendicular to their optical axes (Figure 16.3a), then
the epipolar lines are parallel and the epipoles (where they converge) are hence at inﬁn-
ity. When the cameras are oriented in the same direction and the displacement is parallel
to their optical axes (Figure 16.3b), then the epipoles are in the middle of the images,
and the epipolar lines form a radial pattern. These examples illustrate that the pattern
of epipolar lines provides information about the relative position and orientation of the
cameras.
16.2
The essential matrix
Now we will capture these geometric intuitions in the form of a mathematical model. For
simplicity, we will assume that the world coordinate system is centered on the ﬁrst cam-
era so that the extrinsic parameters (rotation and translation) of the ﬁrst camera are {I,0}.
The second camera may be in any general position {Ω,τ}. We will further assume that
the cameras are normalized so that Λ1 = Λ2 = I. In homogeneous coordinates, a 3D
point w is projected into the two cameras as
λ1˜x1 = [I,0] ˜w
(16.3)
λ2˜x2 = [Ω,τ] ˜w.

358
16 Multiple cameras
where ˜x1 is the observed position in the ﬁrst camera, ˜x2 is the observed position in the
second camera, and both are expressed in homogeneous coordinates.
Expanding the ﬁrst of these relations, we get
λ1


x1
y1
1

=


1
0
0
0
0
1
0
0
0
0
1
0




u
v
w
1

=


u
v
w

.
(16.4)
This simpliﬁes to
λ1˜x1 = w.
(16.5)
By a similar process, the projection in the second camera can be written as
λ2˜x2 = Ωw + τ.
(16.6)
Finally, substituting Equation 16.5 into Equation 16.6 yields
λ2˜x2 = λ1Ω˜x1 + τ.
(16.7)
This relationship represents a constraint between the possible positions of corresponding
points x1 and x2 in the two images. The constraint is parameterized by the rotation and
translation {Ω,τ} of the second camera relative to the ﬁrst.
We will now manipulate the relationship in Equation 16.7 into a form that can be
more easily related to the epipolar lines and the epipoles. We ﬁrst take the cross product
of both sides with the translation vector τ. This removes the last term as the cross product
of any vector with itself is zero. Now we have
λ2τ × ˜x2 = λ1τ × Ω˜x1.
(16.8)
Then we take the inner product of both sides with ˜x2. The left-hand side disappears since
τ × ˜x2 must be perpendicular to ˜x2, and so we have
˜xT
2 τ × Ω˜x1 = 0,
(16.9)
where we have also divided by the scaling factors λ1 and λ2. Finally, we note that
the cross product operation τ× can be expressed as multiplication by the rank 2 skew-
symmetric 3 × 3 matrix τ ×:
τ × =


0
−τz
τy
τz
0
−τx
−τy
τx
0

.
(16.10)
Hence Equation 16.9 has the form
˜xT
2 E˜x1 = 0,
(16.11)
where E = τ ×Ωis known as the essential matrix. Equation 16.11 is an elegant formu-
lation of the mathematical constraint between the positions of corresponding points x1
and x2 in two normalized cameras.

16.2
The essential matrix
359
16.2.1 Properties of the essential matrix
The 3×3 essential matrix captures the geometric relationship between the two cameras
and has rank 2 so that det[E] = 0. The ﬁrst two singular values of the essential matrix
are always identical, and the third is zero. It depends only on the rotation and translation
between the cameras, each of which has three parameters, and so one might think it would
have 6 degrees of freedom. However, it operates on homogeneous variables ˜x1 and ˜x2
and is hence ambiguous up to scale: multiplying all of the entries of the essential matrix
by any constant does not change its properties. For this reason, it is usually considered
as having 5 degrees of freedom.
Since there are fewer degrees of freedom than there are unknowns, the nine entries of
the matrix must obey a set of algebraic constraints. These can be expressed compactly as
2EET E −trace[EET ]E = 0.
(16.12)
These constraints are sometimes exploited in the computation of the essential matrix,
although in this volume we use a simpler method (Section 16.4).
The epipolar lines are easily retrieved from the essential matrix. The condition for a
point being on a line is ax + by + c = 0 or
a
b
c


x
y
1

= 0.
(16.13)
In homogeneous co-ordinates, this can be written as l˜x = 0 where l = [a,b,c] is a 1 × 3
vector representing the line.
Now consider the essential matrix relation
˜xT
2 E˜x1 = 0.
(16.14)
Since ˜xT
2 E is a 1 × 3 vector, this relationship has the form l1˜x1 =0. The line l1 = ˜xT
2 E
is the epipolar line in image 1 due to the point x2 in image 2. By a similar argument, we
can ﬁnd the epipolar line l2 in the second camera due to the point x1 in the ﬁrst camera.
The ﬁnal relations are
l1 = ˜xT
2 E
l2 = ˜xT
1 ET .
(16.15)
The epipoles can also be extracted from the essential matrix. Every epipolar line in
image 1 passes through the epipole ˜e1, so at the epipole ˜e1 we have ˜xT
2 E˜e1 = 0 for all
˜x2. This implies that ˜e1 must lie in the right null-space of E (see Appendix C.2.7). By a
similar argument, the epipole ˜e2 in the second image must lie in the left null space of E.
Hence, we have the relations
˜e1 = null[E]
˜e2 = null[ET ].
(16.16)
In practice, the epipoles can be retrieved by computing the singular value decomposition
E = ULVT of the essential matrix and setting ˜e1 to the last column of V and ˜e2 to the
last row of U.

360
16 Multiple cameras
16.2.2 Decomposition of essential matrix
We saw previously that the essential matrix is deﬁned as
16.1
E = τ ×Ω,
(16.17)
where Ωand τ are the rotation matrix and translation vector that map points in the
coordinate system of camera 2 to the coordinate system of camera 1, and τ × is a 3 × 3
matrix derived from the translation vector.
We will defer the question of how to compute the essential matrix from a set of cor-
responding points until Section 16.3. For now, we will concentrate on how to decompose
a given essential matrix E to recover this rotation Ωand translation τ. This is known as
the relative orientation problem.
In due course, we shall see that whereas we can compute the rotation exactly, it is
only possible to compute the translation up to an unknown scaling factor. This remaining
uncertainty reﬂects the geometric ambiguity of the system; from the images alone, we
cannot tell if these cameras are far apart and looking at a large distant object or close
together and looking at a small nearby object.
To decompose E, we deﬁne the matrix
W =


0
−1
0
1
0
0
0
0
1


(16.18)
and then take the singular value decomposition E = ULVT . We now choose
τ × = ULWUT
Ω= UW−1VT .
(16.19)
It is convention to set the magnitude of the translation vector τ that is recovered from the
matrix τ × to unity.
The preceding decomposition is not obvious, but it is easily checked that multplying
the derived expressions for τ × and Ωyields E = ULVT . This method assumes that
we started with a valid essential matrix where the ﬁrst two singular values are identical
and the third is zero. If this is not the case (due to noise), then we can substitute L′ =
diag[1,1,0] for L in the solution for τ×. For a detailed proof of this decomposition,
consult Hartley and Zisserman (2004).
This solution is only one of four possible combinations of Ωand τ that are com-
patible with E (Figure 16.4). This fourfold ambiguity is due to the fact that the pinhole
model cannot distinguish between objects that are behind the camera (and are not imaged
in real cameras) and those that are in front of the camera.
Part of the uncertainty is captured mathematically by our lack of knowledge of the
sign of the essential matrix (recall it is ambiguous up to scale) and hence the sign of
the recovered translation. Hence, we can generate a second solution by multiplying the
translation vector by −1. The other component of the uncertainty results from an ambi-
guity in the decomposition of the essential matrix; we can equivalently replace W for
W−1 in the decomposition procedure, and this leads to two more solutions.
Fortunately, we can resolve this ambiguity using a corresponding pair of points from
the two images. For each putative solution, we reconstruct the 3D position associated
with this pair (Section 14.6). For one of the four possible combinations of Ω,τ, the point

16.3
The fundamental matrix
361
Figure 16.4 Fourfold ambiguity of reconstruction from two pinhole cameras. The mathematical
model for the pinhole camera does not distinguish between points that are in front of and points
that are behind the camera. This leads to a fourfold ambiguity when we extract the rotation Ω
and translation τ relating the cameras from the essential matrix. a) Correct solution. Points are in
front of both cameras. b) Incorrect solution. The images are identical, but with this interpretation
the points are behind camera 2. c) Incorrect solution with points behind camera 1. d) Incorrect
solution with points behind both cameras.
will be in front of both cameras, and this is the correct solution. In each of the other
three cases, the point will be reconstructed behind one or both of the cameras (Figure
16.4). For a robust estimate, we would repeat this procedure with a number of corre-
sponding points and base our decision on the total number of votes for each of the four
interpretations.
16.3
The fundamental matrix
The derivation of the essential matrix in Section 16.2 used normalized cameras (where
Λ1 = Λ2 = I). The fundamental matrix plays the role of the essential matrix for cameras
with arbitrary intrinsic matrices Λ1 and Λ2. The general projection equations for the two

362
16 Multiple cameras
cameras are
λ1˜x1 = Λ1[I,0] ˜w
(16.20)
λ2˜x2 = Λ2[Ω,τ] ˜w,
and we can use similar manipulations to those presented in Section 16.2 to derive the
constraint
˜xT
2 Λ−T
2
EΛ−1
1 ˜x1 = 0,
(16.21)
or
˜xT
2 F˜x1 = 0,
(16.22)
where the 3×3 matrix F = Λ−T
2
EΛ−1
1
= Λ−T
2
τ ×ΩΛ−1
1
is termed the fundamental
matrix. Like the essential matrix, it also has rank two, but unlike the essential matrix it
has 7 degrees of freedom.
If we know the fundamental matrix F and the intrinsic matrices Λ1 and Λ2, it is
possible to recover the essential matrix E using the relation
E = ΛT
2 FΛ1,
(16.23)
and this can further be decomposed to ﬁnd the rotation and translation between the cam-
eras using the method of Section 16.2.2. It follows that for calibrated cameras, if we can
estimate the fundamental matrix, then we can ﬁnd the rotation and translation between
the cameras. Hence, we now turn our attention on how to compute the fundamental
matrix.
16.3.1 Estimation of the fundamental matrix
The fundamental matrix relation (Equation 16.22) is a constraint on the possible positions
of corresponding points in the ﬁrst and second images. This constraint is parameterized
by the nine entries of F. It follows that if we analyze a set of corresponding points, we
can observe how they are constrained, and from this we can deduce the entries of the
fundamental matrix F.
A suitable cost function for the fundamental matrix can be found by considering the
epipolar lines. Consider a pair of matching points {xi1,xi2} in images 1 and 2, respec-
tively. Each point induces an epipolar line in the other image: the point xi1 induces line
li2 in image 2 and the point xi2 induces the line li1 in image 1. When the fundamen-
tal matrix is correct, each point should lie exactly on the epipolar line induced by the
corresponding point in the other image (Figure 16.5). We hence minimize the squared
distance between every point and the epipolar line predicted by its match in the other
image so that
ˆF = argmin
F
" I
X
i=1

(dist[xi1,li1])2 + (dist[xi2,li2])2#
,
(16.24)
where the distance between a 2D point x = [x,y]T and a line l = [a,b,c] is
dist[x,l] = ax + by + c
√
a2 + b2 .
(16.25)

16.3
The fundamental matrix
363
Figure 16.5 Cost function for estimating fundamental matrix. The point xi1 in image 1 induces
the epipolar line li2 in image 2. When the fundamental matrix is correct, the matching point xi2
will be on this line. Similarly the point xi2 in image 2 induces the epipolar line li1 in image 1.
When the fundamental matrix is correct, the point xi1 will be on this line. The cost function is the
sum of the squares of the distances between these epipolar lines and points (yellow arrows). This
is termed symmetric epipolar distance.
Here too, it is not possible to ﬁnd the minimum of Equation 16.24 in closed form,
and we must rely on nonlinear optimization methods. It is possible to get a good starting
point for this optimization using the eight-point algorithm.
16.3.2 The eight-point algorithm
The eight-point algorithm converts the corresponding 2D points to homogeneous coor-
16.2
16.3
dinates and then solves for the fundamental matrix in closed form. It does not directly
optimize the cost function in Equation 16.24, but instead minimizes an algebraic error.
However, the solution to this problem is usually very close to the value that optimizes the
desired cost function.
In homogeneous coordinates, the relationship between the ith point xi1 = [xi1,yi1]T
in image 1 and the ith point xi2 = [xi2,yi2]T in image 2 is
xi2
yi2
1


f11
f12
f13
f21
f22
f23
f31
f32
f33




xi1
yi1
1

= 0,
(16.26)
where fpq represents one of the entries in the fundamental matrix. When we write this
constraint out in full, we get
xi2xi1f11 + xi2yi1f12 + xi2f13 + yi2xi1f21 + yi2yi1f22 + yi2f23
+ xi1f31 + yi1f32 + f33 = 0.
(16.27)
This can be expressed as an inner product
[xi2xi1,xi2yi1,xi2,yi2xi1,yi2yi1,yi2,xi1,yi1,1]f = 0,
(16.28)
where f = [f11,f12,f13,f21,f22,f23,f31,f32,f33]T is a vectorized version of the funda-
mental matrix, F.

364
16 Multiple cameras
This provides one linear constraint on the elements of F. Consequently, given I
matching points, we can stack these constraints to form the system
Af =


x12x11
x12y11
x12
y12x11
y12y11
y12
x11
y11
1
x22x21
x22y21
x22
y22x21
y22y21
y22
x21
y21
1
...
...
...
...
...
...
...
...
...
xI2xI1
xI2yI1
xI2
yI2xI1
yI2yI1
yI2
xI1
yI1
1

f = 0.
(16.29)
Since the elements of f are ambiguous up to scale, we solve this system with the
constraint that |f| = 1. This also avoids the trivial solution f = 0. This is a minimum
direction problem (see Appendix C.7.2). The solution can be found by taking the singular
value decomposition, A = ULVT and setting f to be the last column of V. The matrix
F is then formed by reshaping f to form a 3×3 matrix.
There are 8 degrees of freedom in the fundamental matrix (it is ambiguous with
respect to scale) and so we require a minimum of I = 8 pairs of points. For this reason,
this algorithm is called the eight-point algorithm.
In practice, there are several further concerns in implementing this algorithm:
• Since the data are noisy, the singularity constraint of the resulting fundamental
matrix will not be obeyed in general (i.e., the estimated matrix will be full rank,
not rank two). We reintroduce this constraint by taking the singular decomposition
of F, setting the last singular value to zero, and multiplying the terms back out.
This provides the closest singular matrix under a Frobenius norm.
• Equation 16.29 is badly scaled since some terms are on the order of pixels squared
(∼10000) and some are of the order ∼1. To improve the quality of the solution,
it is wise to prenormalize the data (see Hartley 1997). We transform the points in
image 1 as ˜x′
i1 = T1˜xi1 and the points in image 2 as ˜x′
i2 = T2˜xi2. The transfor-
mations T1 and T2 are chosen to map the mean of the points in their respective
image to zero, and to ensure that the variance in the x- and y-dimensions is one.
We then compute the matrix F′ from the transformed data using the eight-point
algorithm, and recover the original fundamental matrix as F = TT
2 F′T1.
• The algorithm will only work if the three-dimensional positions wi corresponding
to the eight pairs of points xi1,xi2 are in general position. For example, if they
all fall on a plane then the equations become degenerate, and we cannot get a
unique solution; here the relation between the points in the two images is given by
a homography (see Chapter 15). Similarly, in the case where there is no translation
(i.e., τ = 0), the relation between the two images is a homography, and there is no
unique solution for the fundamental matrix.
• In the subsequent nonlinear optimization, we must also ensure that the rank of F
is two. In order to do this, it is usual to reparameterize the fundamental matrix to
ensure that this will be the case.
16.4
Two-view reconstruction pipeline
We now put all of these ideas together and present a rudimentary pipeline for the recon-
struction of a static 3D scene based on two images taken from unknown positions, but
with cameras where we know the intrinsic parameters. We apply the following steps
(Figures 16.6 and 16.7).

16.4
Two-view reconstruction pipeline
365
Figure 16.6 Two-view reconstruction pipeline (steps 1–3). a–b) A pair of images of a static
scene, captured from two slightly different positions. c–d) SIFT features are computed in each
image. A region descriptor is calculated for each feature point that provides a low dimensional
characterization of the region around the point. Points in the left and right images are matched
using a greedy procedure; the pair of points for which the region descriptors are most similar in a
least squares sense are chosen ﬁrst. Then the pair of remaining points for which the descriptors are
most similar are chosen, and so on. This continues until the minimum squared distance between
the descriptors exceeds a threshold. e–f) Results of greedy matching procedure: the lines repre-
sent the offset to the matching point. Most matches are correct, but there are clearly also some
outliers.

366
16 Multiple cameras
Figure 16.7 Two-view reconstruction pipeline (steps 4–8). A fundamental matrix is ﬁtted using
a robust estimation procedure such as RANSAC. a–b) Eight matches with maximum agreement
from the rest of the data. For each feature, the epipolar line is plotted in the other image. In each
case, the matching point lies on or very near the epipolar line. The resulting fundamental matrix
can be decomposed to get estimates for the relative rotation and translation between the cameras.
c–d) Result of greedily matching original feature points taking into account the epipolar geometry.
Matches where the symmetric epipolar distance exceeds a threshold are rejected. e–f) Computed
w coordinate (depth) relative to ﬁrst camera for each feature. Red features are closer, and blue
features are further away. Almost all of the distances agree with our perceived understanding of
the scene.

16.4
Two-view reconstruction pipeline
367
1. Compute image features. We ﬁnd salient points in each image using an interest
point detector such as the SIFT detector (Section 13.2.3).
2. Compute feature descriptors. We characterize the region around each feature in
each image with a low-dimensional vector. One possibility would be to use the
SIFT descriptor (Section 13.3.2).
3. Find initial matches. We greedily match features between the two images. For
example, we might base this on the squared distance between their region descrip-
tors and stop this procedure when the squared distance exceeds a predeﬁned
threshold to minimize false matches. We might also reject points where the ratio
between the quality of the best and second best match in the other image is too
close to one (suggesting that alternative matches are plausible).
4. Compute fundamental matrix. We compute the fundamental matrix using the
eight-point algorithm. Since some matches are likely to be incorrect, we use a
robust estimation procedure such as RANSAC (Section 15.6).
5. Reﬁne matches. We again greedily match features, but this time we exploit our
knowledge of the epipolar geometry: if a putative match is not close to the induced
epipolar line, it is rejected. We recompute the fundamental matrix based on all of
the remaining point matches.
6. Estimate essential matrix. We estimate the essential matrix from the fundamental
matrix using Equation 16.23.
7. Decompose essential matrix. We extract estimates of the rotation and translation
between the cameras (i.e., the extrinsic parameters) by decomposing the essential
matrix (Section 16.2.2). This provides four possible solutions.
8. Estimate 3D points.
For each solution, we reconstruct the 3D position of
the points using the linear solution from Section 14.6. We retain the extrinsic
parameters where most of the reconstructed points are in front of both cameras.
After this procedure, we have a set of I points {xi1}I
i=1 in the ﬁrst image, a set of I
corresponding points {xi2}I
i=1 in the second image, and a good initial estimate of the 3D
world positions {wi}I
i=1 that were responsible for them. We also have initial estimates
of the extrinsic parameters {Ω,τ}. We now optimize the true cost function
ˆw1...I, ˆΩ, ˆτ = argmax
w,Ω,τ


I
X
i=1
2
X
j=1
log[Pr(xij|wi,Λj,Ω,τ)]


(16.30)
= argmax
w,Ω,τ
" I
X
i=1
log

Normxi1[pinhole[wi,Λ1,I,0],σ2I]

+
I
X
i=1
log

Normxi2[pinhole[wi,Λ2,Ω,τ],σ2I]

#
to reﬁne these estimates. In doing so, we must ensure to enforce the constraints that
|τ| = 1 and Ωis a valid rotation matrix (see Appendix B).
16.4.1 Minimal solutions
The pipeline described previously is rather na¨ıve; in practice the fundamental and
essential matrices can be estimated considerably more efﬁciently.

368
16 Multiple cameras
For example, the fundamental matrix contains seven degrees of freedom, and so it is
actually possible to solve for it using only seven pairs of points. Unsurprisingly, this is
known as the seven point algorithm. It is is more complex as it relies on the seven linear
constraints and the nonlinear constraint det[F] = 0. However, a robust solution can be
computed much more efﬁciently using RANSAC if only seven points are required.
Even if we use this seven-point algorithm, it is still inefﬁcient. When we know the
intrinsic parameters of the cameras, it is possible to compute the essential matrix (and
hence the relative orientation of the cameras) using a minimal ﬁve-point solution. This
is based on ﬁve linear constraints from observed corresponding points and the nonlin-
ear constraints relating the nine parameters of the essential matrix (Equation 16.12).
This method has the advantages of being much quicker to estimate in the context of a
RANSAC algorithm and being robust to non-general conﬁgurations of the scene points.
16.5
Rectiﬁcation
The preceding procedure provides a set of sparse matches between the two images. These
may sufﬁce for some tasks such as navigation, but if we wish to build an accurate model
of the scene, we need to estimate the depth at every point in the image. This is known as
dense stereo reconstruction.
Dense stereo reconstruction algorithms (see Sections 11.8.2 and 12.8.3) generally
assume that the corresponding point lies on the same horizontal scanline in the other
image. The goal of rectiﬁcation is to preprocess the image pair so that this is true. In
other words, we will transform the images so that each epipolar line is horizontal and so
that the epipolar lines associated with a point fall on the same scanline to that point in
the other image. We will describe two different approaches to this problem.
16.5.1 Planar rectiﬁcation
We note that the epipolar lines are naturally horizontal and aligned when the camera
16.4 motion is purely horizontal and both image planes are perpendicular to the w-axis (Figure
16.3a). The key idea of planar rectiﬁcation is to manipulate the two images to recreate
these viewing conditions. We apply homographies Φ1 and Φ2 to the two images so that
they cut their respective ray bundles in the desired way (Figure 16.8).
In fact there is an entire family of homographies that accomplish this goal. One
possible way to select a suitable pair is to ﬁrst work with image 2. We apply a series of
transformations Φ2 = T3T2T1, which collectively move the epipole e2 to a position at
inﬁnity [1,0,0]T .
We ﬁrst center the coordinate system on the principal point,
T1 =


1
0
−δx
0
1
−δy
0
0
1

.
(16.31)
Then we rotate the image about this center until the epipole lies on the x-axis,
T2 =


cos[−θ]
−sin[−θ]
0
sin[−θ]
cos[−θ]
0
0
0
1

,
(16.32)

16.5
Rectiﬁcation
369
Figure 16.8 Planar rectiﬁcation. Green quadrilaterals represent image planes of two cameras
viewing a 3D object (cube). The goal of planar rectiﬁcation is to transform each of these planes
so that the ﬁnal conﬁguration replicates Figure 16.3a. After this transformation (dotted lines), the
image planes are coplanar, and the translation between the cameras is parallel to this plane. Now
the epipolar lines are horizontal and aligned. Since the transformed planes are just different cuts
through the respective ray bundles, each transformation can be accomplished using a homography.
where θ = atan2[ey,ex] is the angle of the translated epipole e = [ex,ey]. Finally, we
translate the epipole to inﬁnity, using the transformation
T3 =


1
0
0
0
1
0
−1/ex
0
1

,
(16.33)
where ex is the x-coordinate of the epipole after the previous two transformations.
After these transformations, the epipole in the second image is at inﬁnity in the hor-
izontal direction. The epipolar lines in this image must converge at the epipole, and are
consequently parallel and horizontal as desired.
Now we consider the ﬁrst image. We cannot simply apply the same procedure as this
will not guarantee that the epipolar lines in the ﬁrst image will be aligned with those in
the ﬁrst. It transpires, however, that there is a family of possible transformations that
do make the epipolar lines of this image horizontal and aligned with those in the second
image. This family can (not obviously) be parameterized as
Φ1[α] = (I + e2αT )Φ2M,
(16.34)
where
e2 =[1,0,0]T
is
the
transformed
epipole
in
the
second
image,
and
α=[α1,α2,α3]T is a 3D vector that selects the particular transformation from the family.
The matrix M comes from the decomposition of the fundamental matrix into F = SM,

370
16 Multiple cameras
where S is skew symmetric (see below). A proof of the relation in Equation 16.34 can
be found in Hartley and Zisserman (2004).
A sensible criterion is to choose α so that it minimizes the disparity,
ˆα = argmax
α
" I
X
i=1
(hom[xi1,Φ1[α]]−hom[xi2,Φ2])T(hom[xi1,Φ1[α]] −hom[xi2,Φ2])
#
.
(16.35)
This criterion simpliﬁes to solving the least squares problem |Aα −b|2 where
A =


x′
11
y′
11
1
x′
21
y′
21
1
...
...
...
x′
I1
y′
I1
1

and b =


x′
12
x′
22
...
x′
I2

,
(16.36)
where the vectors x′
ij = [x′
ij,y′
ij]T are deﬁned by
x′
i1 = hom[xi1,Φ2M]
x′
i2 = hom[xi2,Φ2].
(16.37)
This least squares problem can be solved using the standard approach (Appendix
C.7.1). Figure 16.9 shows example rectiﬁed images. After these transformations, the
corresponding points are guaranteed to be on the same horizontal scanline, and dense
stereo reconstruction can proceed.
Figure 16.9 Planar rectiﬁcation. The images from Figures 16.6 and 16.7 have been rectiﬁed by
applying homographies. After rectiﬁcation, each point induces an epipolar line in the other image
that is horizontal and on the same scanline (compare to Figure 16.7a). This means that the match
is guaranteed to be on the same scanline. In this ﬁgure, the red dotted line is the superimposed
outline of the other image.

16.5
Rectiﬁcation
371
Decomposition of the fundamental matrix
The preceding algorithm requires the matrix M from the decomposition of the funda-
mental matrix as F = SM, where S is skew symmetric. A suitable way to do this is to
compute the singular value decomposition of the fundamental matrix F = ULVT . We
then deﬁne the matrices
L′ =


l11
0
0
0
l22
0
0
0
l11+l22
2

and W =


0
−1
0
1
0
0
0
0
1

,
(16.38)
where lii denotes the ith element from the diagonal of L. Finally, we choose
M = UWL′VT .
(16.39)
16.5.2 Polar rectiﬁcation
The planar rectiﬁcation method described in Section 16.5.1 is suitable when the epipole
is sufﬁciently far outside the image. Since the basis of this method is to map the epipoles
to inﬁnity, it cannot work when the epipole is inside the image and distorts the image a
great deal if it is close to the image. Under these circumstances, a polar rectiﬁcation is
preferred.
Polar rectiﬁcation applies a nonlinear warp to each image so that corresponding
points are mapped to the same scanline. Each new image is formed by resampling the
original images so that the ﬁrst new axis is the distance from the epipole and the sec-
ond new axis is the angle from the epipole (Figure 16.10). This approach can distort the
image signiﬁcantly but works for all camera conﬁgurations.
This method is conceptually simple but should be implemented with caution; when
the epipole lies within the camera image, it is important to ensure that the correct
half of the epipolar line is aligned with the appropriate part of the other image. The
reader is encouraged to consult the original description (Pollefeys et al. 1999b) before
implementing this algorithm.
16.5.3 After rectiﬁcation
After rectiﬁcation, the horizontal offset between every point in the ﬁrst image and its
corresponding point in the second image can be computed using a dense stereo algorithm
(see Sections 11.8.2 and 12.8.3). Typical results are shown in Figure 16.11. Each point
and its match are then warped back to their original positions (i.e., their image positions
are “un-rectiﬁed”). For each pair of 2D points, the depth can then be computed using the
algorithm of Section 14.6.
Finally, we may wish to view the model from a novel direction. For the two-view
case, a simple way to do this is to form a 3D triangular mesh and to texture this mesh
using the information from one or the other image. If we have computed dense matches
using a stereo-matching algorithm, then the mesh can be computed from the perspective
of one camera with two triangles per pixel and cut where there are sharp depth disconti-
nuities. If we have only a sparse correspondence between the two images, then it is usual
to triangulate the projections of the 3D points in one image using a technique such as
Delaunay triangulation to form the mesh. The textured mesh can now be viewed from a
novel direction using the standard computer graphics pipeline.

372
16 Multiple cameras
Distance from epipole, r
Angle from epipole, θ
Distance from epipole, r
Figure 16.10 Polar rectiﬁcation. When the epipole is inside one of the images, the planar rectiﬁ-
cation method is no longer suitable. An alternative in this situation is to perform a nonlinear warp
of each image, in which the two new dimensions correspond to the distance and angle from the
epipole, respectively. This is known as polar rectiﬁcation.
16.6
Multiview reconstruction
So far, we have considered reconstruction based on two views of a scene. Of course, it is
common to have more than two views. For example, we might want to build 3D models
using a video sequence taken by a single moving camera, or equivalently from a video
sequence from a static camera of a rigidly moving object (Figure 16.12). This problem
is often referred to as structure from motion or multiview reconstruction.
The problem is conceptually very similar to the two camera case.
Once again,
the solution ultimately relies on a nonlinear optimization in which we manipulate the
camera position and the three-dimensional points to minimize the squared reprojection
error (Equation 16.2), and hence maximize the likelihood of the model. However, the
multiview case does bring several new aspects to the problem.
First, if we have a number of frames all taken with the same camera, there are now
sufﬁcient constraints to estimate the intrinsic parameters as well. We initialize the cam-
era matrix with sensible values and add this to the ﬁnal optimization. This process is
known as auto-calibration. Second, matching points in video footage is easier because
the changes between adjacent frames tend to be small, and so the features can be explic-
itly tracked in two dimensions. However, it is usual for some points to be occluded in

16.6
Multiview reconstruction
373
Figure 16.11 Disparity. After rectiﬁcation, the horizontal offset at each point can be computed
using a dense stereo algorithm. Here we used the method of Sizintsev et al. (2010). Color indicates
the horizontal shift (disparity) between images. Black regions indicate places where the matching
was ambiguous or where the corresponding part of the scene was occluded. Given these horizontal
correspondences, we undo the rectiﬁcation to get a dense set of matching points with 2D offsets.
The 3D position can now be computed using the method from Section 14.6.
any given frame, so we must keep track of which points are present at which time (Figure
16.12f).
Third, there are now additional constraints on feature matching, which make it eas-
ier to eliminate outliers in the matching set of points. Consider a point that is matched
between three frames. The point in the third frame will be constrained to lie on an
epipolar line due to the ﬁrst frame and another epipolar line due to the second frame:
its position has to be at the intersection of the two lines and so is determined exactly.
Unfortunately, this method will not work when the two predicted epipolar lines are the
same as there is not a unique intersection. Consequently, the position in the third view is
computed in a different way in practice. Just as we derived the fundamental matrix rela-
tion (Equation 16.22) constraining the positions of matching points between two views,
it is possible to derive a closed-form relation that constrains the positions across three
images. The three-view analogue of the fundamental matrix is called the tri-focal tensor.
It can be used to predict the position of the point in a third image given its position in
the ﬁrst and second images even when the epipolar lines are parallel. There is also a
relation between four images, which is captured by the quadri-focal tensor, but there are
no further relations between points in J > 5 images.
Finally, there are new ways to get initial estimates of the unknown quantities. It
may not be practical to get the initial estimates of camera position by computing the
transformation between adjacent frames and chaining these through the sequence. The
translation between adjacent frames may be too small to reliably estimate the motion, and
errors accrue as we move through the sequence. Moreover, it is difﬁcult to maintain a
consistent estimate of the (ambiguous) scale throughout. To this end, methods have been
developed that simultaneously provide an initial estimate of all the camera positions and

374
16 Multiple cameras
Figure 16.12 Multiframe structure from motion. The goal is to construct a 3D model from a
continuous video stream of a moving camera viewing a static object, or a static camera viewing a
moving rigid object. a–d) Features are computed in each frame and tracked through the sequence.
e) Features in current frame and their history. f) In each new frame a number of new features is
identiﬁed, and these are tracked until they are occluded or the correspondence is lost. Here, the
white pixels indicate that a feature was present in a frame, and black pixels indicate that it was
absent.
3D points, some of which are based on factorization of a matrix containing all the (x,y)
positions of every point tracked throughout the sequence.
16.6.1 Bundle adjustment
After ﬁnding the initial estimates of the 3D positions (structure) and the camera positions
(motion), we must again resort to a large nonlinear optimization problem to ﬁne-tune
these parameters. With I tracked points over J frames, the problem is formulated as
ˆθ = argmax
θ


I
X
i=1
J
X
j=1
log[Pr(xij|wi,Λ,Ωj,τ j)]


(16.40)
= argmax
θ


I
X
i=1
J
X
j=1
log

Normxij[pinhole[wi,Λ,Ωj,τ j],σ2I]


,
where θ contains the unknown world points {wi}I
i=1, the intrinsic matrix Λ, and the
extrinsic parameters {Ωj,τ j}J
j=1. This optimization problem is known as Euclidean
bundle adjustment. As for the two-view case, it is necessary to constrain the overall scale
of the solution in some way.
One way to solve this optimization problem is to use an alternating approach. We
ﬁrst improve the log likelihood with respect to each of the extrinsic sets of parameters
{Ωj,τ j} (and possibly the intrinsic matrix Λ if unknown) and then update each 3D posi-
tion wi. This is known as resection-intersection. It seems attractive as it only involves
optimizing over a small subset of parameters at any one time. However, this type of coor-
dinate ascent is inefﬁcient: it cannot take advantage of the large gains that come from
varying all of the parameters at once.

16.6
Multiview reconstruction
375
To make progress, we note that the cost function is based on the normal distribution
and so can be rewritten in the least squares form
ˆθ = argmin
θ

zT z

,
(16.41)
where the vector z contains the squared differences between the observed feature posi-
tions xij and the positions pinhole[wi,Λ,Ωj,τ j] predicted by the model with the current
parameters:
z =


x11 −pinhole[w1,Λ,Ω1,τ 1]
x12 −pinhole[w1,Λ,Ω2,τ 2]
...
xIJ −pinhole[wI,Λ,ΩJ,τ J]

.
(16.42)
The Gauss-Newton method (Appendix B.2.3) is specialized to this type of problem
and updates the current estimate θ[t] of the parameters using
θ[t] = θ[t−1] + λ(JT J)−1 ∂f
∂θ ,
(16.43)
where J is the Jacobian matrix. The entry in the mth row and nth column of J consists
of the derivative of the mth element of z with respect to the nth element of the parameter
vector θ:
Jmn = ∂zm
∂θn
.
(16.44)
In a real structure from motion problem, there might be thousands of scene points,
each with three unknowns, and also thousands of camera positions, each with six
unknowns. At each stage of the optimization, we must invert JT J, which is a square
matrix whose dimension is the same as the number of unknowns. When the number of
unknowns is large, inverting this matrix becomes expensive.
However, it is possible to build a practical system by exploiting the sparse structure
of JT J. This sparsity results from the fact that every squared error term does not depend
on every unknown. There is one contributing error term per observed 2D point, and
this depends only on the associated 3D point, the intrinsic parameters, and the camera
position in that frame.
To exploit this structure, we order the elements of the Jacobian matrix as J =
[Jw,JΩ], where Jw contains the terms that relate to the unknown world points {wi}I
i=1
in turn, and JΩcontains the terms that relate to the unknown camera positions
{Ωj,τ j}J
j=1. For pedagogical reasons, we will assume that the intrinsic matrix Λ is
known here and hence has no entries in the Jacobian. We see that the matrix to be
inverted becomes
JT J =
JT
wJw
JT
wJΩ
JT
ΩJw
JT
ΩJΩ

.
(16.45)
We now note that the matrices in the top-left and bottom-right of this matrix are
block-diagonal (different world points do not interact with one another, and neither do
the parameters from different cameras). Hence, these two submatrices can be inverted
very efﬁciently. The Schur complement relation (Appendix C.8.2), allows us to exploit
this fact to reduce the complexity of the larger matrix inversion.

376
16 Multiple cameras
The preceding description is only a sketch of a real bundle adjustment algorithm; in
a real system, additional sparseness in JT J would be exploited, a more sophisticated
optimization method such as Levenberg-Marquardt would be employed, and a robust
cost function would be used to reduce the effect of outliers.
16.7
Applications
We ﬁrst describe a typical pipeline for recovering a 3D mesh model from a sequence
of video frames. We then discuss a system that can extract 3D information about a
scene from images gathered by a search engine on the Internet, and use this information
to help navigate through the set of photos. Finally, we discuss a multicamera system for
capturing 3D objects that uses a volumetric representation and exploits a Markov random
ﬁeld prior to get a smooth reconstruction.
16.7.1 3D reconstruction pipeline
Pollefeys and Van Gool (2002) present a complete pipeline for constructing 3D models
from a sequence of images taken from an uncalibrated hand-held camera (Figure 16.13).
In the ﬁrst stage, they compute a set of interest points (corners) in each image. When the
image data consist of individual still photos, these points are matched between images.
When the image data consist of continuous video, they are tracked between frames. In
either case, a sparse set of potential correspondences is obtained. The multiview relations
are estimated using a robust procedure, and these are then used to eliminate outliers from
the correspondence set.
To estimate the motion of the cameras, two images are chosen, and a projective recon-
struction is computed (i.e., a reconstruction that is ambiguous up to a 3D projective
transformation because the intrinsic parameters are not known). For each of the other
images in turn, the pose for the camera is determined relative to this reconstruction, and
the reconstruction is reﬁned. In this way, it is possible to incorporate views that have no
common features with the original two frames.
A subsequent bundle adjustment procedure minimizes the reprojection errors to get
more accurate estimates of the camera positions and the points in the 3D world. At this
stage, the reconstruction is still ambiguous up to a projective ambiguity and only now are
initial estimates of the intrinsic parameters computed using a specialized procedure (see
Pollefeys et al. 1999a). Finally, a full bundle adjustment method is applied; it simultane-
ously reﬁnes the estimates of the intrinsic parameters, camera positions, and 3D structure
of the scene.
Successive pairs of images are rectiﬁed, and a dense set of disparities are computed
using a multiresolution dynamic programming technique. Given these dense correspon-
dences, it is possible to compute an estimate of the 3D scene from the point of view
of both cameras.
A ﬁnal estimate of the 3D structure relative to a reference frame
is computed by fusing all of these independent estimates using a Kalman ﬁlter (see
Chapter 19).
For relatively simple scenes, a 3D mesh is computed by placing the vertices of the tri-
angles in 3D space according to the values found in the depth map of the reference frame.
The associated texture map can be retrieved from one or more of the original images. For
more complex scenes, a single reference frame may not sufﬁce and so several meshes are
computed from different reference frames and fused together. Figures 16.13g–h show

16.7
Applications
377
Figure 16.13 3D reconstruction pipeline. a–c) A 20-second video sequence of the camera pan-
ning around the Medusa carving was captured. Every 20th frame was used for the reconstruction,
three of which are shown here. d) Sparse reconstruction (points) and estimated camera posi-
tions (pyramids) after bundle adjustment procedure. e) Depth map after dense stereo matching.
f) Shaded 3D mesh model. g–h) Two views of textured 3D mesh model. Adapted from Pollefeys
and Van Gool (2002). c⃝2002 Wiley.
examples of the resulting textured 3D model of a Medusa head at the ancient site of
Sagalassos in Turkey. More details concerning this pipeline can be found in Pollefeys
et al. (2004).
16.7.2 Photo-tourism
Snavely et al. (2006) present a system for browsing a collection of images of an object
that were gathered from the Internet. A sparse 3D model of the object is created by
locating SIFT features in each image and ﬁnding a set of correspondences between pairs
of images by computing the fundamental matrix using the eight point algorithm with
RANSAC.
A bundle adjustment procedure is then applied to estimate the camera positions and
a sparse 3D model of the scene (Figure 16.14a). This optimization procedure starts with
only a single pair of images and gradually includes images based on their overlap with

378
16 Multiple cameras
Figure 16.14 Photo-tourism. a) A sparse 3D model of an object is computed from a set of
photographs retrieved from the Internet and the relative positions of the cameras (pyramids) are
estimated. b) This 3D model is used as the basis of an interface that provides novel ways to
explore the photo-collection by moving from image to image in 3D space. Adapted from Snavely
et al. (2006). c⃝2006 ACM.
the current reconstruction, “rebundling” at each stage. The intrinsic matrix of each cam-
era is also estimated in this step, but this is simpliﬁed by assuming that the center of
projection is coincident with the image center, that the skew is zero, and that the pixels
are square, leaving a single focal length parameter. This is initialized in the optimization
using information from the EXIF tags of the image when they are present. The bun-
dle adjustment procedure was lengthy; for the model of NotreDame, it took two weeks
to compute a model from 2635 photos of which 597 images were ultimately included.
However, more recent approaches to reconstruction from Internet photos such as that of
Frahm et al. (2010) are considerably faster.
This sparse 3D model of the scene is exploited to create a set of tools for navigating
around the set of photographs. For example, it is possible to
• Select a particular view based on a 3D rendering (as in Figure 16.14a),
• Find images of the object that are similar to the current view,
• Retrieve images of the object taken from the left or the right of the current position
(effectively pan around the object),
• Find images that are from a similar viewpoint but closer or further from the object
(zoom into/away from the object),
• Annotate objects and have these annotations transferred to other images.
This system was extended by Snavely et al. (2008) to allow more natural interaction
with the space of images. For example, in this system it is possible to pan smoothly
around objects by warping the original photos, so that they appear to deﬁne a smooth
path through space.
16.7.3 Volumetric graph cuts
The reconstruction pipeline described in Section 16.7.1 has the potential disadvantage
that it requires the merging of multiple meshes of the object computed from different
viewpoints. Vogiatzis et al. (2007) presented a system that uses a volumetric represen-
tation of depth to avoid this problem. In other words, the 3D space that we wish to

16.7
Applications
379
Figure 16.15 Volumetric graph cuts. a–c) Three of the original photos used to build the 3D
model. d–f) Renderings of the resulting model from similar viewpoints. Adapted from Vogiatzis
et al. (2007). c⃝2007 IEEE.
reconstruct is divided into a 3D grid, and each constituent element (voxel) is simply
labeled as being inside or outside the object. Hence, reconstruction can be viewed as a
binary segmentation of the 3D space.
The relative positions of the cameras are computed using a standard bundle adjust-
ment approach. However, the reconstruction problem is now formulated in terms of an
energy function consisting of two terms and optimized using graph cuts. First, there is
an occupation cost for labeling each voxel as either foreground or background. Second,
there is a discontinuity cost for lying at the boundary between the two partitions. We will
now examine each of these terms in more detail.
The cost of labeling a voxel as being within the object is set to a very high value if this
voxel does not project into the silhouette of the object in each image (i.e., it is not within
the visual hull). Conversely, it is assumed that concavities in the object do not extend
beyond a ﬁxed distance from this visual hull, so a very high cost is paid if voxels close
to the center of the visual hull are labeled as being outside the object. For the remaining
voxels, a data-independent cost is set that favors the voxel being part of the object and
produces a ballooning tendency that counters the shrinking bias of the graph cut solution,
which pays a cost at transitions between the object and the space around it.
The discontinuity cost for lying on the boundary of the object depends on the photo-
consistency of the voxel; a voxel is deemed photo-consistent if it projects to positions
with similar RGB values in all of the cameras from which it is visible. Of course, to
evaluate this, we must estimate the set of cameras in which this point is visible. One
approach to this is to approximate the shape of the object using the visual hull. However,
Vogiatzis et al. (2007) propose a more sophisticated method in which each camera votes
for the photo-consistency of the voxel based on its pattern of correlation with the other
images.
The ﬁnal optimization problem now takes the form of a sum of unary occupation costs
and pairwise terms that encourage the ﬁnal voxel label ﬁeld to be smooth. These pairwise

380
16 Multiple cameras
terms are modiﬁed by the discontinuity cost (an example of using geodesic distance in
graph cuts) so that the transition from foreground to background is more likely in regions
where the photo-consistency is high. Figure 16.15 shows an example of a volumetric 3D
model computed in this way.
Discussion
This chapter has not introduced any truly new models; rather we have explored the
ramiﬁcations of using multiple projective pinhole camera models simultaneously. It is
now possible to use these ideas to reconstruct 3D models from camera sequences of
rigid objects with well-behaved optical properties. However, 3D reconstruction in more
general cases remains an open research problem.
Notes
Multiview geometry: For more information about general issues in multiview geometry consult
the books by Faugeras et al. (2001), Hartley and Zisserman (2004), and Ma et al. (2004) and the
online tutorial by Pollefeys (2002). A summary of multiview relations was presented by Moons
(1998).
Essential and fundamental matrices: The essential matrix was described by Longuet-Higgins
(1981) and its properties were explored by Huang and Faugeras (1989), Horn (1990), and Maybank
(1998) among others. The fundamental matrix was discussed in Faugeras (1992), Faugeras et al.
(1992), and Hartley (1992), (1994). The eight-point algorithm for computing the essential matrix
is due to Longuet-Higgins (1981). Hartley (1997) described a method for rescaling in the eight-
point algorithm that improved its accuracy. Details of the seven-point algorithm for computing the
fundamental matrix can be found in Hartley and Zisserman (2004). Nist´er (2004) and Stew´enius
et al. (2006) describe methods for the relative orientation problem that work directly with ﬁve-point
correspondences between the cameras.
Rectiﬁcation: The planar rectiﬁcation algorithm described in the text is adapted from the descrip-
tion in Hartley and Zisserman (2004). Other variations on planar rectiﬁcation can be found in
Fusiello et al. (2000), Loop and Zhang (1999), and Ma et al. (2004). The polar rectiﬁcation
procedure is due to Pollefeys et al. (1999b).
Features and feature tracking: The algorithms in this chapter rely on the computation of dis-
tinctive points in the image. Typically, these are found using the Harris corner detector (Harris
and Stephens 1988) or the SIFT detector (Lowe 2004). A more detailed discussion of how these
points are computed can be found in Section 13.2. Methods for tracking points in smooth video
sequences (as opposed to matching them across views with a wide baseline) are discussed in Lucas
and Kanade (1981), Tomasi and Kanade (1991), and Shi and Tomasi (1994).
Reconstruction pipelines: Several authors have described pipelines for computing 3D structure
based on a set of images of a rigid object including Fitzgibbon and Zisserman (1998), Pollefeys
et al. (2004), Brown and Lowe (2005), and Agarwal et al. (2009). Newcombe and Davison (2010)
present a recent system that runs at interactive speeds. A summary of this area can be found in
Moons et al. (2009).
Factorization: Tomasi and Kanade (1992) developed an exact ML solution for the projection
matrices and 3D points in a set of images based on factorization. This solution assumes that the
projection process is afﬁne (a simpliﬁcation of the full pinhole model) and that every point is visible
in every image. Sturm and Triggs (1996) developed a similar method that could be used for the
full projective camera. Buchanan and Fitzgibbon (2005) discuss approaches to this problem when
the complete set of data is not available.

Problems
381
Bundle adjustment: Bundle adjustment is a complex topic, which is reviewed in Triggs et al.
(1999). More recently Engels et al. (2006) discuss a real-time bundle adjustment approach that
works with temporal subwindows from a video sequence. Recent approaches to bundle adjustment
have adopted a conjugate gradient optimization strategy (Byr¨od and ˚Astr¨om 2010; Agarwal et al.
2010). A public implementation of bundle adjustment has been made available by Lourakis and
Argyros (2009). A system which is cutting edge at the time of writing is described in Jeong et al.
(2010), and recent methods that use multicore processing have also been developed (Wu et al.
2011).
Multiview reconstruction: The stereo algorithms in Chapters 11 and 12 compute an estimate of
depth at each pixel of one or both of the input images. An alternative strategy is to use an image-
independent representation of shape. Examples of such representations include voxel occupancy
grids (Vogiatzis et al. 2007; Kutulakos and Seitz 2000), level sets (Faugeras and Keriven 1998; Pons
et al. 2007), and polygonal meshes (Fua and Leclerc 1995, Hern´andez and Schmitt 2004). Many
multiview reconstruction techniques also enforce the constraints imposed by the silhouettes (see
Section 14.7.2) on the ﬁnal solution (e.g., Sinha and Pollefeys 2005; Sinha et al. 2007; Kolev
and Cremers 2008). A review of multiview reconstruction techniques can be found in Seitz et al.
(2006).
Problems
16.1 Sketch the pattern of epipolar lines on the images in Figure 16.17a.
16.2 Show that the cross product relation can be written in terms of a matrix multiplication so that
a × b =


0
−a3
a2
a3
0
−a1
−a2
a1
0




b1
b2
b3

.
16.3 Consider ﬁgure 16.16. Write the direction of the three 3D vectors O1O2, O1w, and O2w
in terms of the observed image positions x1,x2 and the rotation Ωand translation τ between the
cameras. The scale of the vectors is unimportant.
The three vectors that you have found must be coplanar. The criterion for three 3D vectors a,b,c
being coplanar can be written as (a × b).c. Use this criterion to derive the essential matrix.
16.4 A clueless computer vision professor writes:
“The essential matrix is a 3×3 matrix that relates image coordinates between two images of the
same scene. It contains 8 independent degrees of freedom (it is ambiguous up to scale). It has rank
2. If we know the intrinsic matrices of the two cameras, we can use the essential matrix to recover
the rotation and translation between the cameras exactly.”
Edit this statement to make it factually correct.
16.5 The essential matrix relates points in two cameras so that
xT
2 Ex1 = 0
is given by
E =


0
0
10
0
0
0
−10
0
0

.
What is the epipolar line in image 2 corresponding to the point x1 = [1,−1,1]? What is the
epipolar line in image 2 corresponding to the points x1 = [−5,−2,1]? Determine the position of
the epipole in image 2. What can you say about the motion of the cameras?
16.6 Show that we can retrieve the essential matrix by multiplying together the expressions from
the decomposition (Equations 16.19) as E = τ ×Ω.

382
16 Multiple cameras
Figure 16.16 Figure for Problem 16.3.
16.7 Derive the fundamental matrix relation:
˜xT
2 Λ−T
2
EΛ−1
1 ˜x1 = 0.
16.8 I intend to compute the fundamental matrix using the eight-point algorithm. Unfortunately,
my data set is polluted by 30 percent outliers. How many iterations of the RANSAC algorithm will
I need to run to have a 99 percent probability of success (i.e., computing the fundamental matrix
from eight inliers at least once)? How many iterations will I need if I use an algorithm based on
seven points?
16.9 We are given the fundamental matrix F13 relating images 1 and 3 and the fundamental matrix
F23 relating images 2 and 3. I am now given corresponding points x1 and x2 in images 1 and 2,
respectively. Derive a formula for the position of the corresponding point in image 3.
16.10
Tomasi-Kanade factorization. In the orthographic camera (ﬁgure 14.19), the projection
process can be described as
x
y

=
φx
γ
δx
0
φy
δy


ω11
ω12
ω13
τx
ω21
ω22
ω23
τy
0
0
0
1




u
v
w
1


=
π11
π12
π13
π21
π22
π23


u
v
w

+
τ ′
x
τ ′
y

,
or in matrix form
x = Πw + τ ′.
Now consider a data matrix X containing the positions {xij}IJ
i,j=1 of J points as seen in I images
so that
X =


x11
x12
...
xiJ
x21
x22
...
x2J
...
...
...
...
xI1
xI2
...
xIJ,



Problems
383
Figure 16.17 a) Figure for Problem 16.1. c) Figure for Problem 16.11. Gray regions represent
nonzero entries in this portrait Jacobian matrix.
and where xij = [xij,yij]T .
(i) Show that the matrix X can be written in the form
X = PW + T
where P contains all of the I 3 × 2 projection matrices {Πi}I
i=1, W contains all of the J
3D world positions {wj}J
j=1 and T contains the translation vectors {τ ′
i}I
i=1.
(ii) Devise an algorithm to recover the matrices P,W and T from the measurements X. Is your
solution unique?
16.11 Consider a Jacobian that has a structure of nonzero entries as shown in Figure 16.17b. Draw
an equivalent image that shows the structure of the nonzero entries in the matrix JT J. Describe
how you would use the Schur complement relation to invert this matrix efﬁciently.


Part VI
Models for vision

In the ﬁnal part of this book, we discuss four families of models. There is very lit-
tle new theoretical material; these models are straight applications of the learning and
inference techniques introduced in the ﬁrst nine chapters. Nonetheless, this material
addresses some of the most important machine vision applications: shape modeling, face
recognition, tracking, and object recognition.
In Chapter 17 we discuss models that characterize the shape of objects. This is a
useful goal in itself as knowledge of shape can help localize or segment an object. Fur-
thermore, shape models can be used in combination with models for the RGB values to
provide a more accurate generative account of the observed data.
In Chapter 18 we investigate models that distinguish between the identities of objects
and the style in which they are observed; a prototypical example of this type of applica-
tion would be face recognition. Here the goal is to build a generative model of the data
that can separate critical information about identity from the irrelevant image changes
due to pose, expression and lighting.
In Chapter 19 we discuss a family of models for tracking visual objects through
time sequences. These are essentially graphical models based on chains such as those
discussed in Chapter 11. However, there are two main differences. First, we focus here
on the case where the unknown variable is continuous rather than discrete. Second, we
do not usually have the beneﬁt of observing the full sequence; we must make a decision
at each time based on information from only the past.
Finally, in Chapter 20 we consider models for object and scene recognition. An
important recent discovery is that good object recognition performance can be achieved
using a discrete representation where the image is characterized as an unstructured his-
togram of visual words. Hence, this chapter considers models where the observed data
are discrete.
It is notable that all of these families of models are generative; it has proven difﬁcult
to integrate complex knowledge about the structure of visual problems into discriminative
models.

Chapter 17
Models for shape
This chapter concerns models for 2D and 3D shape. The motivation for shape models
is twofold. First, we may wish to identify exactly which pixels in the scene belong to a
given object. One approach to this segmentation problem, is to model the outer contour
of the object (i.e., the shape) explicitly. Second, the shape may provide information
about the identity or other characteristics of the object: it can be used as an intermediate
representation for inferring higher-level properties.
Unfortunately, modeling the shape of an object is challenging; we must account for
deformations of the object, the possible absence of some parts of the object and even
changes in the object topology.
Furthermore, the object may be partially occluded,
making it difﬁcult to relate the shape model to the observed data.
One possible approach to establishing 2D object shape is to use a bottom-up
approach; here, a set of boundary fragments are identiﬁed using an edge detector (Sec-
tion 13.2.1) and the goal is to connect these fragments to form a coherent object contour.
Unfortunately, achieving this goal has proved surprisingly elusive. In practice, the edge
detector ﬁnds extraneous edge fragments that are not part of the object contour and misses
others that are part of the true contour. Hence it is difﬁcult to connect the edge fragments
in a way that correctly reconstructs the contour of an object.
The methods in this chapter adopt the top-down approach. Here, we impose prior
information about the object that constrains the possible contour shapes and hence
reduces the search space. We will investigate a range of different types of prior infor-
mation; in some models this will be very weak (e.g., the object boundary is smooth) and
in others it will be very strong (e.g., the object boundary is a 2D projection of a particular
3D shape).
To motivate these models, consider the problem of ﬁtting a 2D geometric model
of the spine to medical imaging data (Figure 17.1). Our goal is to characterize this
complex shape with only a few parameters, which could subsequently be used as a basis
for diagnosing medical problems. This is challenging because the local edge information
in the image is weak. However, in our favor we have very strong prior knowledge about
the possible shapes of the spine.

388
17 Models for shape
17.1
Shape and its representation
Before we can introduce concrete models for identifying shape in images, we should
deﬁne exactly what we mean by the word “shape.” A commonly used deﬁnition comes
from Kendall (1984) who states that shape “is all the geometrical information that
remains when location, scale and rotational effects are ﬁltered out from an object.” In
other words, the shape consists of whatever geometric information is invariant to a sim-
ilarity transformation. Depending on the situation, we may generalize this deﬁnition to
other transformation types such as Euclidean or afﬁne.
We will also need a way to represent the shape. One approach is to directly deﬁne
an algebraic expression that describes the contour. For example, a conic deﬁnes points
x = [x,y]T as lying on the contour if they obey
x
y
1


α
β
γ
β
δ
ϵ
γ
ϵ
ζ




x
y
1

= 0.
(17.1)
This family of shapes includes circles, ellipses, parabolas, and hyperbolas, where the
choice of shape depends on the parameters θ = {α,β,γ,δ,ϵ,ζ}.
Algebraic models are attractive in that they provide a closed form expression for the
contour, but their applicability is extremely limited; it is difﬁcult to deﬁne a mathematical
expression that describes a family of complex shapes such as the spine in Figure 17.1.
It is possible to model complex objects as superimposed collections of geometric prim-
itives like conics, but the resulting models are unwieldy and lose many of the desirable
properties of the closed form representation.
a)
b)
c)
d)
Figure 17.1 Fitting a spine model. a) The spine model is initialized at a ﬁxed position in the
image. b–d) The model then adapts to the image until it describes the data as well as possi-
ble. Models that “crawl” across the image in this way are known as active shape models. Figure
provided by Tim Cootes and Martin Roberts.

17.2
Snakes
389
Figure 17.2 Landmark points. Object shape can be represented with sets of landmark points.
a) Here the landmark points (red dots) deﬁne a single open contour that describes the shape of a
hand. b) In this example the landmark points are connected into sets of open and closed contours
that describe the regions of the face.
Most of the models in this chapter adopt a different approach; they deﬁne the shape
using a set of landmark points (Figure 17.2). One way to think of landmark points is as
a set of discrete samples from one or more underlying continuous contours. The con-
nectivity of the landmark points varies according to the model: they may be ordered and
hence represent a single continuous contour, ordered with wrapping, so they represent
a closed contour or may have a more complex organization so that they can represent a
collection of closed and open contours.
The contour can be reconstituted from the landmark points by interpolating between
them according to the connectivity. For example, in Figure 17.2, the contours have been
formed by connecting adjacent landmark points with straight lines. In more sophisticated
schemes, the landmark points may indirectly determine the position of a smooth curve
by acting as the control points of a spline model.
17.2
Snakes
As a base model, we will consider parametric contour models, which are sometimes also
referred to as active contour models or snakes. These provide only very weak a priori
geometric information; they assume that we know the topology of the contour (i.e., open
or closed) and that it is smooth but nothing else. Hence they are suitable for situations
where little is known about the contents of the image. We will consider a closed contour
deﬁned by a set of N 2D landmark points W = [w1,w2,...,wN], which are unknown.
Our goal is to ﬁnd the conﬁguration of landmark points that best explains the shape
of an object in the image. We construct a generative model for the landmark positions,
which is determined by a likelihood term (which indicates the agreement of this conﬁg-
uration with the image data) and a prior term (which encompasses our prior knowledge
about the frequency with which different conﬁgurations occur).
The likelihood Pr(x|W) of the RGB image data x given the landmark points W
should be high when the landmark points w lie on or close to edges in the image, and
low when they lie in ﬂat regions. One possibility for the likelihood is hence
Pr(x|W) ∝
N
Y
n=1
exp[sobel[x,wn]],
(17.2)

390
17 Models for shape
where the function sobel[x,w] returns the magnitude of the Sobel edge operator (i.e.,
the square root of the sum of the squared responses to the horizontal and vertical Sobel
ﬁlters – see Section 13.1.3) at 2D position w in the image.
This likelihood will be high when the landmark points are on a contour and low other-
wise, as required. However, it has a rather serious disadvantage in practice; in completely
ﬂat parts of the image, the Sobel edge operator returns a value of zero. Consequently,
if the landmark points lie in ﬂat parts of the image, there is no information about which
way to move them to improve the ﬁt (Figures 17.3a–b).
A better approach is to ﬁnd a set of discrete edges using the Canny edge detec-
tor (Section 13.2.1) and then compute a distance transform; each pixel is allotted a
value according to its squared distance from the nearest edge pixel. The likelihood now
becomes
Pr(x|W) ∝
N
Y
n=1
exp

−(dist[x,wn])2
,
(17.3)
where the function dist[x,w] returns the value of the distance transform at position w in
the image. Now the likelihood is large when the landmark points all fall close to edges in
the image (where the distance transform is low) and smoothly decreases as the distance
between the landmark points and the edges increases (Figures 17.3c–d). In addition
to being pragmatic, this approach also has an attractive interpretation; the “squared dis-
tance” objective function is equivalent to assuming that the measured position of the edge
is a noisy estimate of the true edge position and that the noise is additive and normally
distributed.
If we were to ﬁnd the landmark points W based on this criterion alone, then each
point would be separately attracted to a strong edge in the image, and the result would
probably not form a coherent shape; they might even all be attracted to the same position.
To avoid this problem and complete the model, we deﬁne a prior that favors smooth
contours with low curvature. There are various ways to do this, but one possibility is to
choose a prior with two terms
Pr(W) ∝
N
Y
n=1
exp[α space[w,n] + β curve[w,n]],
(17.4)
where the scalars α and β control the relative contribution of these terms.
The ﬁrst term encourages the spacing of the points around the contour to be even; the
function space[w,n] returns a high value if spacing between the nth contour point wn
and its neighbors is close to the average spacing between neighboring points along the
contour:
space[w,n] =
(17.5)
−
 PN
n=1
p
(wn −wn−1)T (wn −wn−1)
N
−
q
(wn −wn−1)T (wn −wn−1)
!2
,
where we assume that the contour is closed so that w0 = wN.
The second term in the prior, curve,[w,n] returns larger values when the curvature is
small and so encourages the contour to be smooth. It is deﬁned as
curve[w,n] = −(wn−1 −2wn + wn+1)T (wn−1 −2wn + wn+1),
(17.6)

17.2
Snakes
391
Figure 17.3 Likelihood for landmark points. a) Original image. b) Output of Sobel edge opera-
tor – one possible scheme is to assign the landmark points a high likelihood if the Sobel response is
strong at their position. This encourages the landmark points to lie on boundaries, in the image but
the response is ﬂat in regions away from the boundaries and this makes it difﬁcult to apply gradient
methods to ﬁt the model c) Results of applying Canny edge detector. d) Negative distance from
nearest Canny edge. This function is also high at boundaries in the image but varies smoothly in
regions away from the boundaries.
where again we assume that the contour is closed so w0=wN and wN+1=w1.
There are only two parameters in this model (the weighting terms, α and β). These
can be learned from training examples, but for simplicity, we will assume that they were
set by hand so there is no need for an explicit learning procedure.
17.2.1 Inference
In inference, we observe a new image x and try to ﬁt the points {wi} on the contour
so that they describe the image as well as possible. To this end, we use a maximum a
posteriori criterion
ˆ
W = argmax
W
[Pr(W|x)] = argmax
W
[Pr(x|W)Pr(W)]
= argmax
W
[log[Pr(x|W)] + log[Pr(W)]].
(17.7)
The optimum of this objective function cannot be found in closed form, and we must
apply a general nonlinear optimization procedure such as the Newton method (Appendix
B). The number of unknowns is twice the number of the landmark points as each point
has an x and y coordinate.
An example of this ﬁtting procedure is illustrated in Figure 17.4. As the minimization
proceeds, the contour crawls around the image, seeking the set of landmark points with

392
17 Models for shape
Figure 17.4 Snakes. The snake is deﬁned by a series of connected landmark points. a–f) As the
optimization proceeds and the posterior probability of these points increases, the snake contour
crawls across the image. The objective function is chosen so that the landmark points are attracted
to edges in the image, but also try to remain equidistant from one another and form a shape with
low curvature.
highest posterior probability. For this reason, this type of contour model is sometimes
known as a snake or active contour model.
The ﬁnal contour in Figure 17.4 ﬁts snugly around the outer boundary of the hedge-
hog.
However, it has not correctly delineated the nose region; this is an area of
high curvature and the scale of the nose is smaller than the distance between adjacent
landmark points. The model can be improved by using a likelihood that depends on
the image at positions along the contour between the landmark points. In this way,
we can develop a model that has few unknowns but that depends on the image in a
dense way.
A second problem with this model is that the optimization procedure can get stuck in
local optima. One possibility is to restart the optimization from a number of different ini-
tial conditions, and choose the ﬁnal solution with the highest probability. Alternatively,
it is possible to modify the prior to make inference more tractable. For example, the
spacing term can be redeﬁned as
space[w,n] = −

µs −
q
(wn −wn−1)T (wn −wn−1)
2
,
(17.8)
which encourages the spacing of the points to be close to a predeﬁned value µs and hence
encourages solutions at a certain scale. This small change makes inference in the model
much easier: the prior is now a 1D Markov random ﬁeld, with cliques that consist of
each element and its two neighbors (due to the term in the curve[•,•]). The problem
can now be discretized and solved efﬁciently using dynamic programming methods (see
Section 11.8.4).
17.2.2 Problems with the snake model
The simple snake model described previously has a number of limitations; it embodies
only weak information about the smoothness of the object boundary and as such:

17.3
Shape templates
393
Figure 17.5 Shape models. a) The snake model only assumes that the contour is smooth. b)
The template model assumes a priori knowledge of the object shape. c) Active shape models are a
compromise in which some information about the object class is known, but the model can adapt
to the particular image. d–f) We consider three extensions to the active shape models describe
3D shape, simultaneously model intensity variation, and describe more complex shape variation,
respectively. g) Finally, we investigate models in which a priori information about the structure of
an articulated object is provided.
• It is not useful where we know the object shape but not its position within the
image.
• It is not useful where we know the class of the object (e.g., a face) but not the
particular instance (e.g., whose face).
• The snake model is 2D and does not understand that some contours are created by
projecting a 3D surface through the camera model.
• It cannot model articulated objects such as the human body.
We remedy these various problems in the subsequent parts of this chapter (Figure
17.5). In Section 17.3, we investigate a model in which we know exactly the shape
we are looking for, and the only problem is to ﬁnd its position in the image. In Sections
17.4–17.8, we investigate models that describe the statistical variation in a class of objects
and can ﬁnd unseen examples of the same class. Finally, in Section 17.9 we discuss
articulated models.
17.3
Shape templates
We will now consider shape template models. These impose the strongest possible form
of geometric information; it is assumed that we know the shape exactly. So, whereas the
snake model started with a circular conﬁguration and adapted this to ﬁt the image, the

394
17 Models for shape
template model starts with the correct shape of the object and merely tries to identify its
position, scale, and orientation in the image. More generally, the problem is to determine
the parameters Ψ of the transformation that maps this shape onto the current image.
We will now develop a generative model that determines the likelihood of the
observed image data as a function of these transformation parameters. The underly-
ing representation of the shape is again a set W = {wn}N
n=1 of 2D landmark points,
which are now assumed known. However, to explain the observed data, the points must
be mapped into the image by a 2D transformation trans[w,Ψ], where Ψ contains the
parameters of the transformation model. For example, with a similarity transformation,
Ψ would consist of the rotation angle, scaling factor, and 2D translation vector.
As with the snake model, we choose the likelihood of image data x to be dependent
on the negative distance from the closest edge in the image
Pr(x|W,Ψ) ∝
N
Y
n=1
exp
h
−(dist[x,trans[wn,Ψ])2i
,
(17.9)
where the function dist[x,w] returns the distance transform of the image x at position w.
Again the likelihood is larger when the landmark points all fall in regions where the
distance transform is low (i.e., close to edges in the image).
17.3.1 Inference
The only unknown variables in the template model are the transformation parameters Ψ.
For simplicity, we will assume that we have no prior knowledge of these parameters and
adopt the maximum likelihood approach in which we maximize the log-likelihood L:
ˆΨ = argmax
Ψ
[L] = argmax
Ψ
[log[Pr(x|W,Ψ)]]
= argmax
Ψ
" N
X
n=1
−(dist[x,trans[wn,Ψ]])2
#
.
(17.10)
There is no closed form solution to this problem, so we must rely on nonlinear optimiza-
tion. To this end, we must take the derivative of the objective function with respect to the
unknowns, and for this we employ the chain rule:
∂L
∂Ψ = −
N
X
n=1
2
X
j=1
∂(dist[x,w′
n])2
∂w′
jn
∂w′
jn
∂Ψ ,
(17.11)
where w′
n = trans[wn,Ψ] is the transformed point, and w′
jn is the jth entry in this 2D
vector.
The ﬁrst term on the right-hand side of Equation 17.11 is easy to compute. The
derivative of the distance image can be approximated in each direction by evaluating
horizontal or vertical derivative ﬁlters (Section 13.1.3) at the current position w′
n. In
general this will not exactly fall on the center of a pixel and so the derivative values should
be interpolated from nearby pixels. The second term depends on the transformation in
question.
Figure 17.6 illustrates the ﬁtting procedure for a template model based on an afﬁne
transform. As the optimization proceeds, the contour crawls over the image as it tries to

17.3
Shape templates
395
Figure 17.6 Shape templates. Here the shape of the object is known and only the afﬁne trans-
formation relating the shape to the image is unknown. a) Original image. b) Results of applying
Canny edge detector. c) Distance transformed image. The intensity represents the distance to the
nearest edge. d) Fitting a shape template. The template is initialized with a randomly chosen afﬁne
transformation (blue curve). After optimization (green curve) the landmark points, which deﬁne
the curve, have moved toward positions with lower values in the distance image. In this case, the
ﬁtting procedure has converged to a local optimum, and the correct silhouette has not been iden-
tiﬁed. e) When we start the optimization from nearer the true optimum, it converges to the global
maximum. f) Final ﬁt of template.
ﬁnd a more optimal position. Unfortunately, there is no guarantee that the optimization
will converge to the true position. As for the snake model, one way to deal with this
problem is to restart the optimization from many different places and choose the solution
with the overall maximum log likelihood. Alternatively, we could initialize the template
position more intelligently; in this example the initial position might be based on the
output of a face detector. It is also possible to restrict the possible solutions by imposing
prior knowledge about the possible transformation parameters and using a maximum a
posteriori formulation.
17.3.2 Inference with iterative closest point algorithm
In Chapter 15 we saw that the transformation mapping one set of points to another can be
computed in closed form for several common families of transformations. However, the
template model cannot be ﬁt in closed form because we do not know which edge points
in the image correspond to each landmark point in the model.
This suggests a different approach to inference for this model. The iterative closest
point (ICP) algorithm alternately matches points in the image to the landmark points and
computes the best transformation. More precisely:
• Each landmark point wn is transformed into the image as w′
n = trans[wn,Ψ]
using the current parameters Ψ.
• Each transformed point w′
n is associated with the closest image point yn that lies
on an edge.

396
17 Models for shape
Figure 17.7 Iterative closest point algo-
rithm.
We associate each landmark point
(positions where the red normals join the
blue contour) with a single edge point in the
image. In this example, we search along the
normal direction to the contour (red lines).
There are usually several points identiﬁed
by an edge detector along each normal (cir-
cles). In each case we choose the closest – a
process known as data association. We com-
pute the transformation that best maps the
landmark points to these closest edge posi-
tions. This moves the contour and poten-
tially changes the closest points in the next
iteration.
• The transformation parameters Ψ that best map the landmark points {wn}N
n=1 to
the image points {yn}N
n=1 are computed in closed form.
This procedure is repeated until convergence. As the optimization proceeds, the choice
of closest points changes – a process known as data association – and so the computed
transformation parameters also evolve.
A variation of this approach considers matching the landmark points to edges in a
direction that is perpendicular to the contour (Figure 17.7). This means that the search
for the nearest edge point is now only in 1D, and this can also make the ﬁtting more
robust in some circumstances. This is most practical for smooth contour models where
the normals can be computed in closed form.
17.4
Statistical shape models
The template model is useful when we know exactly what the object is. Conversely, the
snake model is useful when we have very little prior information about the object. In this
section we describe a model that lies in between these two extremes. Statistical shape
models, active shape models, or point distribution models describe the variation within a
class of objects, and so can adapt to an individual shape from that class even if they have
not seen this speciﬁc example before.
As with the template and snake models, the shape is described in terms of the posi-
tions of the N landmark points {wn}N
n=1, and the likelihood of these points depends on
their proximity to edges in the image. For example, we might choose the likelihood of
the ith training image data xi to be
Pr(xi|wi) ∝
N
Y
n=1
exp

−(dist[xi,trans[win,Ψi]])2
,
(17.12)
where win is the nth landmark point in the ith training image and dist[•,•] is a function
that computes the distance to the nearest Canny edge in the image.

17.4
Statistical shape models
397
However, we now deﬁne a more sophisticated prior model over the landmark
positions, which are characterized as a compound vector wi = [wT
i1,wT
i2 ...,wT
iN]T con-
taining all of the x- and y-positions of the landmark points in the ith image. In particular,
we model the density Pr(wi) as a normal distribution so that
Pr(wi) = Normwi[µ,Σ],
(17.13)
where the mean µ captures the average shape and the covariance Σ captures how
different instances vary around this mean.
17.4.1 Learning
In learning, our goal is to estimate the parameters θ = {µ,Σ} based on training data.
Each training example consists of a set of landmark points that have been hand-annotated
on one of the training images.
Unfortunately, the training examples are not usually geometrically aligned when
we receive them.
In other words, we receive the transformed data examples w′
i =
[w
′T
i1 ,w
′T
i2 ,...,w
′T
iN]T , where
w′
in = trans[win,Ψi].
(17.14)
Before we can learn the parameters µ and Σ of the normal, we must align the examples
using the inverse of this transformation
win = trans[w′
in,Ψ−
i ],
(17.15)
where {Ψ−
i }I
i=1 are the parameters of the inverse transformations.
Alignment of training examples
The method for aligning the training examples is known as generalized Procrustes analy-
17.1 sis (Figure 17.8) and exploits the “chicken and egg” structure of the underlying problem;
if we knew the mean shape µ, then it would be easy to estimate the parameters {Ψ−
i }I
i=1
of the transformations that best map the observed points to this mean. Similarly, if we
knew these transformations, we could easily compute the mean shape by transforming
the observed points and taking the mean of the resulting shapes. Generalized Procrustes
analysis takes an alternating approach to this problem in which we repeatedly
1. Update the transformations using the criterion
ˆΨ
−
i = argmin
Ψ−
i
" N
X
n=1
trans[w′
in,Ψ−
i ] −µn
2
#
,
(17.16)
where µ = [µT
1 ,µT
2 ,...,µT
N]T . This can be achieved in closed form for common
transformation families such as the Euclidean, similarity, or afﬁne transformations
(see Section 15.2).
2. Update the mean template
ˆµ = argmin
µ
" N
X
n=1
trans[w′
in,Ψ−
i ] −µn
2
#
.
(17.17)

398
17 Models for shape
Figure 17.8 Generalized Procrustes analysis.
a–d) Four training shapes. e) Superimposing the
training shapes shows that they are not aligned well. f) The goal of generalized Procrustes analysis
is simultaneously to align all of the training shapes with respect to a chosen transformation family.
Here, the images are aligned with respect to a similarity transformation (gray region illustrates
mean shape). After this procedure, the remaining variation is described by a statistical shape
model.
Figure 17.9 Statistical model of face shape. Three samples drawn from normally distributed
model of landmark vectors w. Each generated 136-dimensional vector was reshaped to create a
68 × 2 matrix containing 68 (x,y) points, which are plotted in the ﬁgure. The samples look like
plausible examples of face shapes.
In practice, to optimize this criterion we inversely transform each set of points
using Equation 17.15 and take the average of the resulting shape vectors. It is
important to normalize the mean vector µ after this stage to deﬁne the absolute
scale uniquely.
Typically, we would initialize the mean vector µ to be one of the training examples and
iterate between these steps until there was no further improvement.
After convergence, we can ﬁt the statistical model
Pr(wi) = Normwi[µ,Σ].
(17.18)
We already know the mean µ. We can compute the covariance Σ using the maximum
likelihood approach from the aligned shapes {wi}I
i=1. Figure 17.9 visualizes a shape
model for the human face that was learned using this technique.

17.5
Subspace shape models
399
17.4.2 Inference
In inference, we ﬁt the model to a new image. The simplest way to do this is to take a
brute force optimization approach in which we estimate the unknown landmark points
w = {wn}N
n=1 and the parameters Ψ of the transformation model so that
ˆw = argmax
w
"
max
Ψ
" N
X
n=1
−(dist[xi,trans[wn,Ψ]])2 + log[Normw[µ,Σ]]
##
. (17.19)
One way to optimize this objective function is to alternate between estimating the
transformation parameters and the landmark points. For ﬁxed landmark points {wn}N
n=1,
we are effectively ﬁtting a shape template model, and we can use the methods of Sections
17.3.1 and 17.3.2 to ﬁnd the transformation parameters Ψ. For ﬁxed transformation
parameters, it is possible to estimate the landmark points by nonlinear optimization of
the objective function.
As with the template model, the statistical shape model “crawls” across the image
as the optimization discovers improved ways to make the model agree with the image.
However, unlike the template model, it can adapt its shape to match the idiosyncrasies of
the particular object in this image as in Figure 17.1.
Unfortunately this statistical shape model has some practical disadvantages due to
the number of variables involved. In the ﬁtting procedure, we must optimize over the
landmark points, themselves. If there are N landmark points, then there are 2N variables
over which to optimize. For the face model in Figure 15.9, there would be 136 variables,
and the resulting optimization is quite costly. Moreover, we will require a large number
of training examples to accurately estimate the covariance Σ of these variables.
Furthermore, it is not clear that all these parameters are needed; the classes of object
that are suited to this normally distributed model (hands, faces, spines, etc.) are quite
constrained in their shape variation and so many of the parameters of this full model are
merely describing noise in the training shape annotation. In the next section we describe
a related model which uses fewer unknown variables (and can be ﬁt more efﬁciently) and
fewer parameters (and so can be learned from less data).
17.5
Subspace shape models
The subspace shape model exploits the structure inherent in the covariance of the land-
mark points. In particular, it assumes that the shape vectors {wi}I
i=1 all lie very close
to a K-dimensional linear subspace (see Figure 7.19) and describes the shape vectors as
resulting from the process:
wi = µ + Φhi + ϵi,
(17.20)
where µ is the mean shape, Φ = [φ1,φ2,...,φK] is a portrait matrix containing K basis
functions {φk}K
k=1 that deﬁne the subspace in its columns, and ϵi is an additive normal
noise term with spherical covariance σ2I. The term hi is a K×1 hidden variable where
each element is responsible for weighting one of the basis functions. This can be seen
more clearly by rewriting Equation 17.20 as
wi = µ +
K
X
k=1
φkhik + ϵi,
(17.21)

400
17 Models for shape
Figure 17.10 Approximation of face by weighting basis functions. a) Original face. b) Approx-
imating original (gray) by mean face µ. c) Approximating original by mean face plus optimal
weighting hi1 of basis function φ1. d–f) Adding further weighted basis functions. As more terms
are added, the approximation becomes closer to the original. With only four basis functions, the
model can explain 78 percent of the variation of the original face around the mean.
where hik is the kth element of the vector hi.
The principle of the subspace model is to approximate the shape vector by the
deterministic part of this process so that
wi ≈µ +
K
X
k=1
φkhik,
(17.22)
and so now we can represent the 2N × 1 vector wi using only the K × 1 vector hi.
For constrained data sets such as the spine, hand, and face models, it is remark-
able how good this approximation can be, even when K is set to quite a small number.
For example, in Figure 17.10 the face is very well approximated by taking a weighted
sum of only K = 4 basis functions.
We will exploit this phenomenon when we ﬁt
the shape model; we now optimize over the weights of the basis functions rather
than the landmark points themselves, and this results in considerable computational
savings.
Of course, the approximation in Figure 17.10 only works because (i) we have selected
a set of basis functions Φ that are suitable for representing faces and (ii) we have chosen
the weights hi appropriately to describe this face. We will now take a closer look at the
model and how to ﬁnd these quantities.

17.5
Subspace shape models
401
17.5.1 Probabilistic principal component analysis
The particular subspace model that we will apply here is known as probabilistic principal
component analysis or PPCA for short. To deﬁne the model, we reexpress Equation 17.20
in probabilistic terms:
Pr(wi|hi,µ,Φ,σ2) = Normwi[µ + Φhi,σ2I],
(17.23)
where µ is a 2N × 1 mean vector, Φ is a 2N × K matrix containing K basis functions
in its columns, and σ2 controls the degree of additive noise. In the context of this model,
the basis functions are known as principal components. The K ×1 hidden variable hi
weights the basis functions and determines the ﬁnal positions on the subspace, before the
additive noise component is added.
To complete the model, we also deﬁne a prior over the hidden variable hi, and we
choose a spherical normal distribution for this:
Pr(hi) = Normhi[0,I].
(17.24)
By marginalizing the joint distribution Pr(wi,hi) with respect to the hidden variable
hi, we can retrieve the prior density Pr(wi), and this is given by
Pr(wi) =
Z
Pr(wi|hi)Pr(hi)dhi
=
Z
Normwi[µ + Φhi,σ2I]Normhi[0,I]dhi
= Normwi[µ,ΦΦT + σ2I].
(17.25)
This algebraic result is not obvious; however, it has a simple interpretation. The prior
over the landmark points wi is once more normally distributed, but now the covariance
is divided into two parts: the term ΦΦT , which explains the variation in the subspace
(due to shape changes), and the term σ2I, which explains any remaining variation in the
data (mainly noise in the training points).
17.5.2 Learning
The PPCA model is very closely related to factor analysis (Section 7.6). The only dif-
ference is that the noise term σ2I is spherical in the PPCA model, but has the diagonal
form in factor analysis. Surprisingly, this difference has important implications; it is pos-
sible to learn the PPCA model in closed form whereas the factor analysis model needs
an iterative strategy such as the EM algorithm.
In learning we are given a set of aligned training data {wi}I
i=1
where
17.2 wi =[wT
i1,wT
i2 ...,wT
iN]T is a vector containing all of the x and y positions of the land-
mark points in the ith example. We wish to estimate the parameters µ,Φ, and σ2 of the
PPCA model.
To this end, we ﬁrst set the mean parameter µ to be the mean of the training
examples wi:
µ =
PI
i=1 wi
I
.
(17.26)
We then form a matrix W = [w1 −µ,w2 −µ,...,wI −µ] containing the zero-centered
data and compute the singular value decomposition of WWT
WWT = UL2UT ,
(17.27)

402
17 Models for shape
where U is an orthogonal matrix and L2 is diagonal. For a model that explains D
dimensional data with K principal components, we compute the parameters using
ˆσ2 =
1
D −K
D
X
j=K+1
L2
jj
ˆΦ = UK(L2
K −ˆσ2I)1/2,
(17.28)
where UK denotes a truncation of U where we have retained only the ﬁrst K columns,
L2
K represents a truncation of L2 to retain only the ﬁrst K columns and K rows, and Ljj
is the jth element from the diagonal of L.
If the dimensionality D of the data is very high, then the eigenvalue decomposition
of the D × D matrix WWT will be computationally expensive. If the number of train-
ing examples I is less than the dimensionality D, then a more efﬁcient approach is to
compute the singular value decomposition of the I×I scatter matrix WT W
WT W = VL2VT
(17.29)
and then rearrange the SVD relation W = ULVT to compute U.
There are two things to notice about the estimated basis functions Φ:
1. The basis functions (principal components) in the columns of Φ are all orthogonal
to one another. This can be easily seen because the solution for Φ (Equation 17.28)
is the product of the truncated orthogonal matrix UK and the diagonal matrix
(L2
K −ˆσ2I)1/2.
2. The basis functions are ordered: the ﬁrst column of Φ represents the direction in
the space of w that contained the most variance, and each subsequent direction
explains less variation. This is a consequence of the SVD algorithm, which orders
the elements of L2 so that they decrease.
We could visualize the PPCA model by drawing samples from the marginal density
(Equation 17.25). However, the properties of the basis functions permit a more sys-
tematic way to examine the model. In Figure 17.11 we visualize the PPCA model by
manipulating the hidden variable hi and then illustrating the vector µ + Φhi. We can
choose hi to elucidate each basis function {φk}K
k=1 in turn. For example, by setting
hi = ±[1,0,0,0,...,0] we investigate the ﬁrst basis function.
Figure 17.11 shows that the principal components φk sometimes have surprisingly
clear interpretations. For example, the ﬁrst principal component clearly encodes the
opening and closing of the ﬁngers. A second example that visualizes the spine model
from Figure 17.1 is illustrated in Figure 17.12.
17.5.3 Inference
In inference, we ﬁt the shape to a new image by manipulating the weights h of the basis
functions Φ. A suitable objective function is
ˆh = argmax
h
"
max
Ψ
" N
X
n=1
 
−(dist[xi,trans[µn + Φnh,Ψ]])2
σ2
!
+ log[Normh[0,I]]
##
,
(17.30)

17.5
Subspace shape models
403
Figure 17.11 Principal components for hand model. a–b) Varying the ﬁrst principal component.
In panel (a) we have added a multiple λ of the ﬁrst principal component φ1 to the mean vector
µ. In panel (b) we have subtracted the same multiple of the ﬁrst principal component from the
mean. In each case the shaded area indicates the mean vector. The ﬁrst principal component has a
clear interpretation: it controls the opening and closing of the ﬁngers. Panels (c–d) and (e–f) show
similar manipulations of the second and third principal components, respectively.
where µn contains the two elements of µ that pertain to the nth point and Φn contains the
two rows of Φ that pertain to the nth point. There are a number of ways to optimize this
model, including a straightforward nonlinear optimization over the unknowns h and Ψ.
We will brieﬂy describe an iterative closest point approach. This consists of iteratively
repeating these steps:
• The current landmark points are computed as w = µ + Φh.
• Each landmark point is transformed into the image as w′
n = trans[wn,Ψ].
• Each transformed point w′
n is associated with the closest edge point yn in the
image.
• The transformation parameters Ψ that best map the original landmark points
{wn}N
n=1 to the edge points {yn}N
n=1 are computed.
• Each point is transformed again using the updated parameters Ψ.
• The closest edge points {yn}N
n=1 are found once more.
• The hidden variables h are updated (see below).
We repeat these steps until convergence. After optimization, the landmark points can be
recovered as w = µ + Φh.

404
17 Models for shape
Figure 17.12 Learned spine model. a) Mean spine shape. b–c) Manipulating ﬁrst principal
component. d–e) Manipulating second principal component. Figure provided by Tim Cootes.
In the last step of the iterative algorithm we must update the hidden variables. This
can be achieved using the objective function:
ˆh = argmax
h
" N
X
n=1
log[Pr(yn|h),Ψ] + log[Pr(h)]
#
(17.31)
= argmax
h
" N
X
n=1
−(yn −trans[µn + Φnh,Ψ])2 /σ2 −log[hT h]
#
,
where µn contains the two elements of µ associated with the nth landmark point and Φn
contains the two rows of Φ associated with the nth landmark point. If the transformation
is linear so that trans[wn,Ψ] can be written in the form Awn + b, then this update can
be computed in closed form and is given by
ˆh =
 
σ2I +
N
X
n=1
ΦT
nAT AΦn
!−1 N
X
n=1
AΦn(yn −Aµ −b).
(17.32)
Examples of performing inference in subspace shape models are illustrated in Figures
17.1 and 17.13. As the optimization proceeds, the shape model moves across the surface
of the image and adapts itself to the shape of the object. For this reason, these models
are often referred to as active shape models.
We note that there are many variations of this model and many strategies that help
ﬁtting the model robustly. One particularly weak aspect of the model is that it assumes
that each landmark point maps to a generic edge in the image; it does not even distin-
guish between the polarity or orientation of this edge, let alone take advantage of other
nearby image information that might help identify the correct position. A more sensible
approach is to build a generative model that describes the likelihood of the local image
data when the nth feature wn is present. A second important concept is coarse-to-ﬁne
ﬁtting, in which a coarse model is ﬁtted to a low-resolution image. The result is then
used as a starting point for a more detailed model at a higher resolution. In this way, it is
possible to increase the probability of converging to the true ﬁt without getting stuck in
a local minimum.

17.7
Statistical models for shape and appearance
405
Figure 17.13 Several iterations of ﬁtting a subspace shape model to a face image. After conver-
gence, both the global transformation of the model and the details of the shape are correct. When
a statistical shape model is ﬁt to an image in this way, it is referred to as an active shape model.
Figure provided by Tim Cootes.
17.6
Three-dimensional shape models
The subspace shape model can be easily extended to three dimensions. For 3D data, the
model works in almost exactly the same way as it did in 2D. However, the landmark
points w become 3×1 vectors determining the position within 3D space, and the global
transformation that maps these into the image must also be generalized to 3D. For exam-
ple, a 3D afﬁne transformation encompasses 3D translations, rotations, shearing, and
scaling in 3D and is determined by 12 parameters.
Finally, the likelihood must also be adapted for 3D. A trivial way to do this would
be to create the 3D analogue of an edge operator by taking the root mean square of three
derivative ﬁlters in the coordinate directions. We then construct an expression so that
the likelihood is high in regions where the 3D edge operator returns a high value. An
example of a 3D shape model is shown in Figure 17.14.
17.7
Statistical models for shape and appearance
In Chapter 7 we discussed the use of subspace models to describe the pixel intensities
of a class of images such as faces. However, it was concluded that these were very poor
models of this high-dimensional data. In this section, we consider models that describe
both the intensity of the pixels and the shape of the object simultaneously. Moreover, they
describe correlations between these aspects of the image: the shape tells us something
about the intensity values and vice versa (Figure 17.15). When we ﬁt these models to
new images, they deform and adapt to the shape and intensity of the image, and so they
are known as active appearance models.
As before, we characterize the shape with a vector of N landmark points w =
[wT
1 ,wT
2 ,...,wT
N]. However, we now also describe a model of the pixel intensity values
x where this vector contains the concatenated RGB data from the image. The full model
can best be described as a sequence of conditional probability statements:
Pr(hi) = Normhi[0,I]
Pr(wi|hi) = Normwi[µw + Φwhi,σ2
wI]
Pr(xi|wi,hi) = Normxi[warp[µx + Φxhi,wi,Ψi],σ2
xI].
(17.33)

406
17 Models for shape
Figure 17.14 Three-dimensional statistical shape model. a) The model describes regions of the
human brain. It is again deﬁned by a set of landmark points. These are visualized by triangulat-
ing them to form a surface. b–c) Changing the weighting of the ﬁrst principal component. d–e)
Changing weighting of second component. Adapted from Babalola et al. (2008).
Figure 17.15 Modeling both shape and texture. We learn a model in which we a) parameterize
shape using a subspace model (results of manipulating weights of shape basis functions shown)
and b) parameterize intensity values for ﬁxed shape using a different subspace model (results of
manipulating weights of texture basis functions shown). c) The subspace models are connected
in that the weightings of the basis functions (principal components) in each model are always the
same. In this way, correlations between shape and texture are described (results of manipulating
weights of basis functions for both shape and texture together shown). Adapted from Stegmann
(2002).

17.7
Statistical models for shape and appearance
407
This is quite a complex model, so we will break it down into its constituent parts. At the
core of the model is a hidden variable hi. This can be thought of as a low-dimensional
explanation that underpins both the shape and the pixel intensity values. In the ﬁrst
equation, we deﬁne a prior over this hidden variable.
In the second equation, the shape data w is created by weighting a set of basis func-
tions Φw by the hidden variable and adding a mean vector µw. The result is corrupted
with spherically distributed normal noise with covariance σ2I. This is exactly the same
as for the statistical shape model.
The third equation describes how the observed pixel values xi in the ith image depend
on the shape wi, the global transformation parameters Ψi and the hidden variable hi.
There are three stages in this process:
1. The intensity values are generated for the mean shape µw; pixel values are
described as a weighted sum µx + Φxhi of a second set of basis functions Φx
which is added to a mean intensity µx.
2. These generated intensity values are then warped to the ﬁnal desired shape; the
operation warp[µx + Φxhi,wi,Ψi] warps the resulting intensity image µx +
Φxhi to the desired shape based on the landmark points wi and the global
transformation parameters Ψi.
3. Finally, the observed data xi is corrupted by normally distributed noise with
spherical covariance σ2
xI.
Notice that both the intensity values and the shape depend on the same underlying
hidden variable hi. This means that the model can describe correlations in the shape
and appearance; for example, the texture might change to include teeth when the mouth
region expands. Figure 17.15 shows examples of manipulating the shape and texture
components of a face model and illustrates the correlation between the two. Notice that
the resulting images are much sharper than the factor analysis model for faces (Figure
7.22); by explicitly accounting for the shape component, we get a superior model of the
texture.
Warping images
A simple approach to warping the images is to triangulate the landmark points and then
use a piecewise afﬁne warp; each triangle is warped from the canonical position to the
desired ﬁnal position using a separate afﬁne transformation (Figure 17.16). The coor-
dinates of the canonical triangle vertices are held in µ. The coordinates of the triangle
vertices after the warp are held in trans[µw + Φwhi,Ψi].
17.7.1 Learning
In learning we are given a set of I images in which we know the transformed landmark
points {wi}I
i=1 and the associated warped and transformed pixel data {xi}I
i=1, and we
aim to learn the parameters {µw,Φw,σ2
w,µx,Φx,σ2
x}. The model is too complex to
learn directly; we take the approach of simplifying it by eliminating (i) the effect of the

408
17 Models for shape
a)
b)
c)
d)
Figure 17.16 Piecewise afﬁne warping. a) The texture is synthesized for a ﬁxed canonical shape.
b) This shape is then warped to create the ﬁnal image. c) One technique for warping the image is
to use a piecewise afﬁne transformation. We ﬁrst triangulate the landmark points. d) Then each
triangle undergoes a separate afﬁne transformation such that the three points that deﬁne it move to
their ﬁnal positions. Adapted from Stegmann (2002).
transformation on the landmark points and (ii) both the warp and the transformation on
the image data. Then we estimate the parameters in this simpliﬁed model.
To eliminate the effect of the transformations {Ψi}I
i=1 on the landmark points, we
perform generalized Procrustes analysis. To eliminate the effect of the transformation
and warps and the observed images, we now warp each training image to the average
shape µw = PI
i=1 wi/I using piecewise afﬁne transformations.
The result of these operations is to generate training data consisting of aligned sets of
landmark points {wi}I
i=1 representing the shape (similar to Figure 17.15a) and a set of
face images {xi}I
i=1 that all have the same shape (similar to Figure 17.15b). These data
are explained using the simpler model:
Pr(hi) = Normhi[0,I]
Pr(wi|hi) = Normwi[µw + Φwhi,σ2
wI]
Pr(xi|hi) = Normxi[µx + Φxhi,σ2
xI].
(17.34)

17.7
Statistical models for shape and appearance
409
To learn the parameters, we write the last two equations in the generative form
wi
xi

=
µw
µx

+
Φw
Φx

hi +
ϵwi
ϵxi

,
(17.35)
where ϵwi is a normally distributed noise term with spherical covariance σ2
wI and ϵxi is
a normally distributed noise term with spherical covariance σ2
xI.
We now observe that the system is very similar to the standard form of a PPCA model
or factor analyzer x′ = µ′ + Φ′h + ϵ′. Unlike PPCA, the noise term is structured and
contains two values (σ2
w and σ2
x). However, each dimension of the data does not have a
separate variance as for factor analysis.
Unfortunately, there is no closed form solution as there was for the PPCA model. This
model can be learned with a modiﬁed version of the EM algorithm for factor analysis (see
Section 7.6.2), where the update step for the variance terms σ2 and σ2
x differs from the
usual equation.
17.7.2 Inference
In inference we ﬁt the model to new data by ﬁnding the values of the hidden variable
h, which are responsible for both the shape and appearance of the image. This low-
dimensional representation of the object could then be used as an input to a second
algorithm that analyzes its characteristics. For example, in a face model it might be
used as the basis for discriminating gender.
Inference in this model can be simpliﬁed by assuming that the landmark points
lie exactly on the subspace so that we have the deterministic relation wi = µ + Φh.
This means that the likelihood of the observed data given the hidden variable h can be
expressed as
Pr(x|h) = Normx[warp[µx + Φxh,µw + Φwh,Ψ],σ2
xI].
(17.36)
We adopt a maximum likelihood procedure and note that this criterion is based on
the normal distribution so the result is a least squares cost function:
ˆh, ˆΨ = argmax
h,Ψ
[log[Pr(x|h)]]
(17.37)
= argmin
h,Ψ
 x−warp[µx+Φxh,µw+Φwh,Ψ]
T
 x−warp[µx+Φxh,µw+Φwh,Ψ])

.
Denoting the unknown quantities by θ = {h,Ψ}, we observe that this cost function
takes the general form of f[θ] = z[θ]T z[θ] and can hence be optimized using the Gauss-
Newton method (Appendix B.2.3). We initialize the unknown quantities to some sensible
values and then iteratively update these values using the relation
θ[t] = θ[t−1] + λ(JT J)−1 ∂f
∂θ ,
(17.38)
where J is the Jacobian matrix. The entry in the mth row and nth column of J consists
of the derivative of the mth element of z with respect to the nth element of the parameter
vector θ:
Jmn = ∂zm
∂θn
.
(17.39)

410
17 Models for shape
Figure 17.17 Fitting a statistical model of shape and appearance. a) Shape model at start of ﬁtting
process superimposed on the observed image. b) Shape and appearance model (synthesized image
x) at start of ﬁtting process. c–d) After several iterations. e–f) At the end of the ﬁtting procedure.
The synthesized face (f) looks very similar to the observed image in (a),(c), and (e). Such models
are known as active appearance models as they can be seen to adapt to the image. Figure provided
by Tim Cootes.
Figure 17.17 shows an example of a statistical shape and appearance model being ﬁt
to face data. The spine model in Figure 17.1 was also a model of this kind although only
the shape component was shown. As usual, the success of this ﬁtting procedure relies
on having a good starting point for the optimization process and a course-to-ﬁne strategy
can help the optimization converge.
17.8
Non-Gaussian statistical shape models
The statistical shape model discussed in Section 17.4 is effective for objects where the
shape variation is relatively constrained and is well described by the normally distributed
prior. However, in some situations a normal distribution will not sufﬁce, and we must
turn to more complex models. One possibility is to use a mixture of PPCAs, and this
is straightforward to implement. However, we will use this opportunity to introduce an
alternative model for describing non-Gaussian densities.
The Gaussian process latent variable model or GPLVM is a density model that can
model complex non-normal distributions. The GPLVM extends the PPCA model so
that the hidden variables hi are transformed through a ﬁxed nonlinearity before being
weighted by the basis functions Φ.

17.8
Non-Gaussian statistical shape models
411
17.8.1 PPCA as regression
To help understand the GPLVM, we will reconsider subspace models in terms of
regression. Consider the PPCA model, which can be expressed as
Pr(w|µ,Φ,σ2) =
Z
Pr(w,h|µ,Φ,σ2)dh
=
Z
Pr(w|h,µ,Φ,σ2)Pr(h)dh
=
Z
Normw[µ + Φh,σ2I]Normh[0,I]dh.
(17.40)
The ﬁrst term in the last line of this expression has a close relationship to linear
regression (Section 8.1); it is a model for predicting w given the variable h. Indeed if we
consider just the dth element wd of w then this term has the form
Pr(wd|h,µ,Φ,σ2) = Normwd[µd + φT
d•h,σ2],
(17.41)
where µd is the dth dimension of µ and φT
d• is the dth row of Φ; this is exactly the linear
regression model for wd against h.
This insight provides a new way of looking at the model. Figure 17.18 shows a 2D
data set {wi}I
i=1, which is explained by a set of 1D hidden variables {hi}I
i=1. Each
dimension of the 2D data w is created by a different regression model, but in each case
we are regressing against the common set of hidden variables {hi}I
i=1. So, the ﬁrst
dimension w1 of w is described as µ1 + φT
1•h and the second dimension w2 is modeled
as µ2 + φT
2•h. The common underlying hidden variable induces the correlation we see
in the distribution Pr(w).
Now let us consider how the overall density is formed.
For a ﬁxed value of h,
we get a prediction for w1 and a prediction for w2 each of which has additive normal
Figure 17.18 PPCA model as regression. a) We consider a 2D data set that is explained by a
PPCA model with a single hidden variable. The data are explained by a 2D normal distribution with
mean µ and covariance φφT + σ2I. b) One way to think of this PPCA model is that the 2D data
are explained by two underlying regression models. The ﬁrst data dimension w1 is formed from
a regression against h and c) the second data dimension w2 is formed from a different regression
against the same values h.

412
17 Models for shape
Figure 17.19 Gaussian process latent variable model as regression. a) We consider a 2D data
set that is explained by a GPLVM with a single variable. b) One way to consider this model is
that the 2D data are explained by two underlying regression models. The ﬁrst data dimension
w1 is formed from a Gaussian process regression against the hidden variable h. c) The second
data dimension w2 is formed from a different Gaussian process regression model against the same
values h.
noise with the same variance (Figures 17.18b–c). The result is a 2D spherical normal dis-
tribution in w. To create the density, we integrate over all possible values of h, weighted
by a the normal prior; the ﬁnal density is hence an inﬁnite weighted sum of the 2D spher-
ical normal distributions predicted by each value of h, and this happens to have the form
of the normal distribution with nonspherical covariance φφ2 + σ2I, which is seen in
Figure 17.18a.
17.8.2 Gaussian process latent variable model
This interpretation of PPCA provides an obvious approach to describing more com-
plex densities; we simply replace the linear regression model with a more sophisticated
nonlinear regression model. As the name suggests, the Gaussian process latent variable
model makes use of the Gaussian process regression model (see Section 8.5).
Figure 17.19 illustrates the GPLVM. Each dimension of the density in Figure 17.19a
once again results from a regression against a common underlying variable h. How-
ever, in this model, the two regression curves are nonlinear (Figures 17.19b–c) and this
accounts for the complexity of the original density.
There are two major practical changes in the GPLVM:
1. In Gaussian process regression, we marginalize over the regression parameters µ
and Φ so we do not have to estimate these in the learning procedure.
2. Conversely, it is no longer possible to marginalize over the hidden variables h in
closed form; we must estimate the hidden variables during the training procedure.
The inability to marginalize over the hidden variables also creates some difﬁculties
in evaluating the ﬁnal density.
We will now consider learning and inference in this model in turn.
Learning
In the original Gaussian process regression model (Section 8.5), we aimed to pre-
dict the univariate world states w = [w1,w2,...,wI]T from multivariate data X =

17.8
Non-Gaussian statistical shape models
413
[x1,x2,...,xI]. The parameter vector φ was marginalized out of the model, and the
noise parameter σ2 was found by maximizing the marginal likelihood:
ˆσ2 = argmax
σ2

Pr(w|X,σ2)

(17.42)
= argmax
σ2
Z
Pr(w|X,Φ,σ2)Pr(Φ)dΦ

= argmax
σ2

Normw[0,σ2
pK[X,X] + σ2I]

,
where σ2
p controls the prior variance of the parameter vector φ and K[•,•] is the chosen
kernel function.
In the GPLVM we have a similar situation. We aim to predict multivariate world
values W = [w1,w2,...,wI]T from multivariate hidden variables H = [h1,h2,...,hI].
Once more, we marginalize the basis functions Φ out of the model and maximize over
the noise parameters σ2. However, this time, we do not know the values of the hid-
den variables H that we are regressing against; these must be simultaneously estimated,
giving the objective function:
ˆH, ˆσ2 = argmax
H,σ2

Pr(W,H,σ2)

(17.43)
= argmax
H,σ2
Z
Pr(W|X,Φ,σ2)Pr(Φ)Pr(H)dΦ

= argmax
H,σ2
" D
Y
d=1
Normwd•[0,σ2
pK[H,H] + σ2I]
IY
i=1
Normhi[0,I]
#
,
where there is one term in the ﬁrst product for each of the D dimensions of the world and
one term in the second product for each of the training examples.
Unfortunately, there is no closed form solution to this optimization problem; to learn
the model we must use one of the general-purpose nonlinear optimization techniques
discussed in Appendix B. Sometimes, the kernel K[•,•] also contains parameters, and
these should be simultaneously optimized.
Inference
For a new value of the hidden variable h∗the distribution over the dth dimension of the
world w∗
d is given by the analogue of Equation 8.24:
Pr(w∗
d|h∗,H,W) =
(17.44)
Normw∗
d

σ2
p
σ2 K[h∗,H]wd• −σ2
p
σ2 K[h∗,H]
 
K[H,H] + σ2
p
σ2 I
!−1
K[H,H]wd•,
σ2
pK[h∗,h∗] −σ2
pK[h∗,H]
 
K[H,H] + σ2
p
σ2 I
!−1
K[H,h∗] + σ2

.
To sample from the model, we select the hidden variable h∗from the prior and then
predict a probability distribution over the landmarks w∗using this equation.
To assess the probability of the a new sample w∗, we should use the relation
Pr(w) =
D
Y
d=1
Z
Pr(w∗
d|h∗,H,W)Pr(h∗)dh∗.
(17.45)

414
17 Models for shape
Figure 17.20 Samples from a non-Gaussian face model based on a GPLVM. The samples repre-
sent more signiﬁcant distortions to the shape than could be realistically described by the original
statistical shape model. Adapted from Huang et al. (2011). c⃝2011 IEEE.
Unfortunately, this integral cannot be computed in closed form. One possibility is to
maximize over h∗rather than marginalize over it. Another possibility is to approxi-
mate the density Pr(h∗) by a set of delta functions at the positions of the training data
{hi})I
i=1 and we can then replace the integral with a sum over the individual predictions
from these examples.
Application to shape models
Figure 17.20 illustrates several examples of sampling from a shape model for a face that
is based on the GPLVM. This more sophisticated model can cope with modeling larger
variations in the shape than the original PPCA, which was based on a single normal
distribution.
17.9
Articulated models
Statistical shape models work well when the shape variation is relatively small. However,
there are other situations where we have much stronger a priori knowledge about the
object. For example, in a body model, we know that there are two arms and two legs and
that these are connected in a certain way to the main body. An articulated model param-
eterizes the model in terms of the joint angles and the overall transformation relating one
root component to the camera.
The core idea of an articulated model is that the transformations of the parts are
cumulative; the position of the foot depends on the position of the lower leg, which
depends on the position of the upper leg and so on. This is known as a kinematic chain.

17.10
Applications
415
To compute the global transformation of the foot relative to the camera, we chain the
transformations relating each of the body parts in the appropriate order.
There are many approaches to constructing articulated models. They may be two
dimensional (e.g., the pictorial structures discussed in Chapter 11) or exist in three dimen-
sions. We will consider a 3D hand model that is constructed from truncated quadrics.
The quadric is the 3D generalization of the conic (see Section 17.1) and can represent
cylinders, spheres, ellipsoids, a pair of planes, and other shapes in 3D. Points in 3D which
lie on the surface of the quadric satisfy the relation
x
y
z
1


ψ1
ψ2
ψ3
ψ4
ψ2
ψ5
ψ6
ψ7
ψ3
ψ6
ψ8
ψ9
ψ4
ψ7
ψ9
ψ10




x
y
z
1

= 0.
(17.46)
Figure 17.21 illustrates a hand model that was constructed from a set of 39 quadrics.
Some of these quadrics are truncated; to make a tube shape of ﬁnite length, a cylinder
or ellipsoid is clipped by a pair of planes in 3D so that only parts of the quadric that are
between the planes are retained. The pair of planes is represented by a second quadric,
so each part of the model is actually represented by two quadrics. This model has 27
degrees of freedom, 6 for the global hand position, 4 for the pose of each ﬁnger, and 5
for the pose of the thumb.
The quadric is a sensible choice because its projection through a pinhole camera
takes the form of a conic and can be computed in closed form. Typically, an ellipsoid
(represented by a quadric) would project down to an ellipse in the image (represented by
a conic), and we can ﬁnd a closed form expression for the parameters of this conic in
terms of the quadric parameters.
Given the camera position relative to the model, it is possible to project the collection
of quadrics that form the 3D model into the camera image. Self occlusion can be handled
neatly by testing the depth along each ray and not rendering the resulting conic if the
associated quadric lies behind another part of the model. This leads to a straightforward
method for ﬁtting the model to an image of the object (i.e., ﬁnding the joint angles and
overall pose relative to the camera). We simulate a set of contours for the model (as in
Figure 17.21d) and then evaluate an expression for the likelihood that increases when
these match the observed edges in the image. To ﬁt the model, we simply optimize this
cost function.
Unfortunately, this algorithm is prone to converging to local minima; it is hard to ﬁnd
a good starting point for the optimization. Moreover, the visual data may be genuinely
ambiguous in a particular image, and there may be more than one conﬁguration of the
object that is compatible with the observed image. The situation becomes more man-
ageable if we view the object from more than one camera (as in Figures 17.21e–f and
h–i) as much of the ambiguity is resolved. Fitting the model is also easier when we are
tracking the model through a series of frames; we can initialize the model ﬁtting at each
time based on the known position of the hand at the previous time. This kind of temporal
model is investigated in Chapter 19.
17.10 Applications
We will now look at two applications that extend the ideas of this chapter into 3D. First,
we will consider a face model that is essentially a 3D version of the active appearance

416
17 Models for shape
Figure 17.21 Articulated model for a human hand. a) A three-dimensional model for a human
hand is constructed from 39 truncated quadrics. b) Exploded view. c) The model has 27 degrees
of freedom that control the joint angles. d) It is easy to project this model into an image and ﬁnd
the outer and occluding contours, which can then be aligned with contours in an image. e–f) Two
views of a hand taken simultaneously from different cameras g) Estimated state of hand model.
h–j) Two more views and another estimate of the position. Adapted from Stenger et al. (2001a).
c⃝2001 IEEE.
model. Second, we will discuss a model for the human body that combines the ideas of
the articulated model and the subspace representation of shape.
17.10.1Three dimensional morphable models
Blanz and Vetter (1999) developed a statistical model of the 3D shape and appearance
of faces. Their model was based on 200 laser scans. Each face was represented by
approximately 70,000 3D vertices and an RGB texture map. The captured faces were
preprocessed so that the global 3D transformation between them was removed, and the
vertices were registered using a method based on optical ﬂow.
A statistical shape model was built where the 3D vertices now take on the role of the
landmark points. As with most of the statistical shape models in this chapter, this model
was based on a linear combination of basis functions (principal components). Similarly,
the texture maps were described as a linear combination of a set of basis images (principal
components). Figure 17.22 illustrates the mean face and the effect of varying the shape
and texture components independently.
The model, as described so far, is thus a 3D version of the model for shape and
appearance that we described in Section 17.7. However, in addition, Blanz and Vetter
(1999) model the rendering process using the Phong shading model which includes both
ambient and directional lighting effects.
To ﬁt this model to a photograph of a face, the square error between the observed
pixel intensities and those predicted from the model is minimized. The goal then is to

17.10
Applications
417
Figure 17.22 3D morphable model of a face. The model was trained from laser scans of 200
individuals and is represented as a set of 70,000 vertex positions and an associated texture map.
a) Mean face. b) As for the 2D subspace shape model, the ﬁnal shape is described as a linear
combination of basis shapes (principal components). In this model, however, these basis shapes are
three dimensional. The ﬁgure shows the effect of changing the weighting of these basis functions
while keeping the texture constant. c) The texture is also modeled as a linear combination of basis
shapes. The ﬁgure shows the effect of changing the texture while keeping the shape constant.
Adapted from Blanz and Vetter (2003). c⃝2003 IEEE.
manipulate the parameters of the model so that the rendered image matches the observed
image as closely as possible. These parameters include:
• The weightings of the basis functions that determine the shape,
• The weightings of the basis functions that determine the texture,
• The relative position of the camera and the object,
• The RGB intensities of the ambient and directed light, and
• The offsets and gains for each image RGB channel.
Other parameters such as the camera distance, light direction, and surface shininess were
ﬁxed by hand. In practice, the ﬁtting was accomplished using a nonlinear optimization
technique. Figure 17.23 illustrates the process of ﬁtting the model to a real image. After
convergence, the shape and texture closely replicate the original image. At the end of
this process, we have full knowledge of the shape and texture of the face. This can now
be viewed from different angles, relit and even have realistic shadows superimposed.
Blanz and Vetter (Blanz and Vetter 2003; Blanz et al. 2005) applied this model to face
recognition. In the simplest case, they described the ﬁtted face using a vector containing
the weighting functions for both the shape and texture. Two faces can be compared by
examining the distance between the vector associated with each. This method has the
advantage that the faces can be originally presented in very different lighting conditions
and with very different poses as neither of these factors is reﬂected in the ﬁnal repre-
sentation. However, the method is limited in practice by the model-ﬁtting procedure,
which does not always converge for real images that may suffer from complex lighting
conditions and partial occlusion.
Matthews et al. (2007) presented a simpliﬁed version of the same model; this was
still 3D but had a sparser mesh and did not include a reﬂectance model. However, they
describe an algorithm for ﬁtting this face model to video sequences that can run at more
than 60 frames per second. This permits real-time tracking of the pose and expression of

418
17 Models for shape
Figure 17.23 Fitting a 3D morphable model to a real image of Audrey Hepburn. The goal is to
ﬁnd the parameters of the 3D model that best describe the 2D face. Once we have done this, we can
manipulate the image. For example we could relight the face or view it from different angles. a)
Simulated image from model with initial parameters. b) Simulated image from model after ﬁtting
procedure. This closely replicates the original image in terms of both texture and shape. c) Images
from ﬁtted model generated under several different viewing conditions. Adapted from Blanz and
Vetter (1999). c⃝1999 ACM.
faces (Figure 17.24). This technique has been used to capture facial expressions for CGI
characters in movies and video games.
17.10.2Three-dimensional body model
Anguelov et al. (2005) presented a 3D body model that combines an articulated structure
with a subspace model. The articulated model describes the skeleton of the body, and the
subspace model describes the variation of the shape of the individual around this skeleton
(Figure 17.25).
At its core, this model is represented by a set of triangles that deﬁne the surface of the
body. The model is best explained in terms of generation in which each triangle under-
goes a series of transformations. First, the position is deformed in a way that depends on
the conﬁguration of the nearest joints in the body. The deformation is determined using a
regression model and creates subtle effects such as the deformation of muscles. Second,
the position of the triangle is deformed according to a PCA model which determines the
individuals characteristics (body shape, etc.). Finally, the triangle is warped in 3D space
depending on the position of the skeleton.
Two applications of this model are shown in Figure 17.26. First, the model can be
used to ﬁll in missing parts of partial scans of human beings. Many scanners cannot

17.10
Applications
419
a)
b)
Figure 17.24 Real-time facial tracking using a 3D active appearance model. a–b) Two examples
from tracking sequence. The pose of the face is indicated in the top left-hand corner. The superim-
posed mesh shown to the right of the face illustrates two different views of the shape component
of the model. Adapted from Matthews et al. (2007). c⃝2007 Springer.
Figure 17.25 3D body model. a) The ﬁnal position of the skin surface is regressed against the
nearest joint angles; this produces subtle effects such as the bulging of muscles. b) The ﬁnal
position of the skin surface also depends on a PCA model which describes between-individual
variation in body shape. Adapted from Anguelov et al. (2005). c⃝2005 ACM.
capture a full 3D model at once; they may only capture the front of the object and so
several scans must be combined to get a full model. This is problematic for moving
objects such as human beings. Even for models that can capture 360o of shape, there
are often missing or noisy parts of the data. Figure 17.26a shows an example of ﬁtting
the model to a partial scan; the position of the skeleton and the weighting of the PCA
components are adapted until the synthesized shape agrees with the partial scan. The
remaining part of the synthesized shape plausibly ﬁlls in the missing elements.
Figures 17.26b–d illustrate the use of the model for motion capture based anima-
tion. An actor in a motion capture studio has his body position tracked, and this body
position can be used to determine the position of the skeleton of the 3D model. The
regression model then adjusts the vertices of the skin model appropriately to model mus-
cle deformation while the PCA model allows the identity of the resulting model to vary.

420
17 Models for shape
Figure 17.26 Applications of 3D body model. a) Interpolation of partial scans. In each case, the
purple region represents the original scan and the green region is the interpolated part after ﬁtting
a 3D body model (viewed from two angles). b) Motion capture animation. An actor is tracked in a
conventional motion capture studio. c) This results in the known position of a number of markers
on the body. d) These markers are used to position the skeletal aspect of the model. The PCA
part of the model can control the shape of the ﬁnal body (two examples shown). Adapted from
Anguelov et al. (2005). c⃝2005 ACM.
This type of system can be used to generate character animations for video games and
movies.
Discussion
In this chapter, we have presented a number of models for describing the shape of visual
objects. These ideas relate closely to the subsequent chapters in this book. These shape
models are often tracked in video sequences and the machinery for this tracking is devel-
oped in Chapter 19. Several of the shape models have a subspace (principal component)
representation at their core. In Chapter 18 we investigate models that exploit this type of
representation for identity recognition.
Notes
Snakes and active contour models: Snakes were ﬁrst introduced by Kass et al. (1987). Various
modiﬁcations have been made to encourage them to converge to a sensible answer including the
addition of a ballooning term (Cohen 1991) and a new type of external force ﬁeld called gradient
vector ﬂow (Xu and Prince 1998). The original description treated the contour as a continuous
object, but subsequent work also considered it as discrete and used greedy algorithms or dynamic
programming methods for optimization (Amini et al. 1990; Williams and Shah 1992). Subsequent
work has investigated the use of prior information about object shape and led to active contour
models (Cootes et al. 1995). This is still an open research area (e.g., Bergtholdt et al. 2005; Freifeld
et al. 2010).
The contour models discussed in this chapter are known as parametric because the shape is explic-
itly represented. A summary of early work on parametric active contours can be found in Blake
and Isard (1998). A parallel strand of research investigates implicit or nonparametric contours in

Notes
421
which the contour is implicitly deﬁned by the level sets of a function deﬁned on the image domain
(Malladi et al. 1994; Caselles et al. 1997). There has also been considerable interest in applying
prior knowledge to these models (Leventon et al. 2000; Rousson and Paragios 2002).
Bottom-up models: This chapter has concerned top-down approaches to contour detection in
which a generative model for the object is speciﬁed that explains the observed edges in the image.
However, there has also been a recent surge of research progress into bottom-up approaches, in
which edge fragments are combined to form coherent shapes. For example, Opelt et al. (2006)
introduced the “Boundary fragment model” in which pairs of edge fragments vote for the position
of the centroid of the object and the object is detected by ﬁnding the position with the most sup-
port. Shotton et al. (2008a) present a similar model that incorporates scale invariance and searches
through local regions of the image to identify objects that form only a small part of a larger scene.
Pairwise constraints between features were introduced by Leordeanu et al. (2007). Other work
has investigated reconstructing pieces of the contour as combinations of local geometric primitives
such as line segments and ellipses (Chia et al. 2010).
Subspace models: The statistical models in this chapter are based on subspace models such as
probabilistic PCA (Tipping 2001) although in their original presentation they used regular (non-
probabilistic) PCA (see Section 13.4.2). The models could equally have been built using factor
analysis (Rubin and Thayer 1982). This has the disadvantage that it cannot be learned in closed
form but can cope with modeling the joint distribution of quantities that are expressed in different
units (e.g., shape and texture in active appearance models). Nonlinear generalizations of PCA
(Sch¨olkopf et al. 1998) and factor analysis (Lawrence 2005) have extended these statistical models
to the non-Gaussian case.
Active shape and appearance models: More details about active shape models can be found in
Cootes et al. (1995). More details about active appearance models can be found in Cootes et al.
(2001) and Stegmann (2002). Jones and Soatto (2005) presented an interesting extension to active
appearance models in which the objects was modeled as a number of superimposed layers. Recent
interest in active appearance models has focussed on improving the efﬁciency of ﬁtting algorithms
(Matthews and Baker 2004; Matthews et al. 2007; Amberg et al. 2009). They have been applied
to many tasks including face recognition, face pose estimation and expression recognition (Lanitis
et al. 1997), and lip reading (Matthews et al. 2002). Several authors have investigated nonlinear
approaches including systems based on mixture models (Cootes and Taylor 1997), kernel PCA
(Romdhani et al. 1999), and the GPLVM (Huang et al. 2011).
3D Morphable models: Morphable models for faces were ﬁrst introduced by Blanz and Vetter
(1999) and were subsequently applied to editing images and video (Blanz et al. 2003), for face
recognition (Blanz and Vetter 2003; Blanz et al. 2005), and for tracking 3D faces (Matthews et al.
2007). A related model has been developed for vehicles (Leotta and Mundy 2011).
Body Tracking: Generative models for tracking human bodies have been developed based on a
number of representations including cylinders (Hogg 1983), ellipsoids (Bregler and Malik 1998),
stick men (Mori et al. 2004), and meshes (Shakhnarovich et al. 2003). As well as 3D models,
attempts have also been made to ﬁt purely 2D models (e.g., Felzenszwalb and Huttenlocher 2005,
and see Rehg et al. 2003). Some research has focused on multicamera setups which help disam-
biguate the observed data (e.g., Gavrila and Davis 1996). Models for tracking the body in time
sequences have attracted a lot of attention (e.g., Deutscher et al. 2000; Sidenbladh et al. 2000 and
see Chapter 19). Recent work has attempted to leverage knowledge of the physics of the body
movement to improve the results (Brubaker et al. 2010). There are also a number of approaches
to body tracking based on regression (see Chapter 8). Reviews of human motion tracking can be
found in Forsyth et al. (2006), Moeslund et al. (2006), Poppe (2007), and Sigal et al. (2010).
Body models for graphics: The work of Anguelov et al. (2005), who combined the skeletal and
statistical shape models, was preceeded by Allen et al. (2003) and Seo and Magnenat-Thalmann

422
17 Models for shape
(2003), who also applied PCA to skeletal models for the human body, although they do not include
a component that models deformations due to muscle movement.
Hand models: Many authors have developed models for tracking hands including Rehg and
Kanade (1994), (1995), Heap and Hogg (1996), Stenger et al. (2001a), Wu et al. (2001), and Lu
et al. (2003). De La Gorce et al. (2008) present a very sophisticated model that describes texture,
shading, and self-occlusions of the hand.
Problems
17.1 A conic is deﬁned as the set of points where

x
y
1



α
β
γ
β
δ
ϵ
γ
ϵ
ζ




x
y
1

= 0,
or
˜xT C˜x = 0.
Use MATLAB to draw the 2D function ˜xT C˜x and identify the set of positions where this function
is zero for the following matrices:
C1 =


3
0
0
0
2
0
0
0
−1


C2 =


0
0
1
0
0
0
1
0
−2


C3 =


−1
0
0
0
0
1
0
1
0

.
17.2 Devise an efﬁcient algorithm to compute the distance transform. The algorithm should take a
binary image and return at each pixel the city block distance to the nearest nonzero element of the
original image. The city block distance d between pixels (x1,y1) and pixel (x2,y2) is deﬁned as
d = |x1 −x2| + |y1 −y2|.
17.3 Consider a prior that is based in the curvature term:
curve[w,n] = −(wn−1 −2wn + wn+1)T (wn−1 −2wn + wn+1).
If landmark point w1 = [100,100], and landmark point w3 is at position w3 = [200,300], what
position w2 will minimize the function curve[w,2]?
17.4 If the snake as described in Section 17.2 is initialized in an empty image, how would you
expect it to evolve during the ﬁtting procedure?
17.5 The spacing element of the snake prior (Equation 17.5) encourages all of the control points
of the snake to be the equidistant. An alternative approach is to give the snake a tendency to shrink
(so that it collapses around objects). Write out an alternative expression for the spacing term that
accomplishes this goal.
17.6 Devise a method to ﬁnd the “best” weight vector h given a new vector w and the parameters
{µ,Φ,σ2} of the PPCA model (see Figure 17.10).
17.7 Show that if the singular value decomposition of a matrix W can be written as W = ULVT ,
then it follows that
WWT = ULUT
WT W = VLVT .

Problems
423
17.8 Devise a method to learn the PPCA model using the EM algorithm, giving details of both the
E- and M-steps. Are you guaranteed to get the same answer as the method based on the SVD?
17.9 Show that the maximum a posteriori solution for the hidden weight variable h is as given in
Equation 17.32.
17.10 You are given a set of 100 male faces and 100 female faces. By hand you mark 50 landmark
points on each image. Describe how to use this data to develop a generative approach to gender
classiﬁcation based on shape alone. Describe both the training process and how you would infer
the gender for a new face that does not contain landmark points.
17.11 Imagine that we have learned a point distribution for the shape of the human face. Now we
see a new face where everything below the nose is occluded by a scarf. How could you exploit
the model to estimate both the positions of the landmark points in the top half of the face and the
landmark points in the (missing) bottom half of the face?
17.12 An alternative approach to building a nonlinear model of shape is to use a mixture model.
Describe an approach to training a statistical shape model based on the mixture of probabilistic
principal component analyzers. How would you ﬁt this model to a new image?
17.13 One way to warp one image to another is to implement a piecewise afﬁne warp. Assume
that we have a number of points in image 1 and their corresponding points in image 2. We ﬁrst
triangulate each set of points in the same way. We now represent the position x1 in image 1 as a
weighted sum of the three vertices of the triangle a1,b1,c1 that it lies in so that
x1 = αa1 + βb1 + γc1,
where the weights are constrained to be positive with α + β + γ = 1. These weights are known as
barycentric coordinates.
To ﬁnd the position in the second image, we then compute the position relative to the three vertices
a2,b2,c2 of the warped triangle so that
x2 = αa2 + βb2 + γc2.
How can we compute the weights α,β,γ? Devise a method to warp the whole image in this
manner.
17.14 Consider an ellipsoid in 3D space that is represented by the quadric
˜wT
 A
b
bT
c

˜w = 0,
where A is a 3 × 3 matrix, b is a 3 × 1 vector, and c is a scalar.
For a normalized camera we can write the world point ˜w in terms of the image point ˜x as ˜w =
[˜xT ,s]T where s is a scaling factor that determines the distance along the ray ˜x.
(i) Combine these conditions to produce a criterion that must be true for an image point ˜x to lie
within the projection of the conic.
(ii) The edge of the image of the conic is the locus of points for which there is a single solution
for the distance s. Outside the conic there is no real solution for s and inside it there are two
possible solutions corresponding to the front and back face of the quadric. Use this intuition
to derive an expression for the conic in terms of A,b and c. If the camera has intrinsic
matrix Λ, what would the new expression for the conic be?

Chapter 18
Models for style and identity
In this chapter we discuss a family of models that explain observed data in terms of
several underlying causes. These causes can be divided into three types: the identity of
the object, the style in which it is observed, and the remaining variation.
To motivate these models, consider face recognition. For a facial image, the identity
of the face (i.e., whose face it is) obviously inﬂuences the observed data. However, the
style in which the face is viewed is also important. The pose, expression, and illumi-
nation are all style elements that might be modeled. Unfortunately, many other things
also contribute to the ﬁnal observed data: the person may have applied cosmetics, put
on glasses, grown a beard, or dyed his or her hair.
These myriad contributory ele-
ments are usually too difﬁcult to model and are hence explained with a generic noise
term.
In face recognition tasks, our goal is to infer whether the identities of face images are
the same or different. For example, in face veriﬁcation, we aim to infer a binary variable
w ∈{0,1}, where w=0 indicates that the identities differ and w=1 indicates that they
are the same. This task is extremely challenging when there are large changes in pose,
illumination, or expression; the change in the image due to style may dwarf the change
due to identity (Figure 18.1).
The models in this chapter are generative, so the focus is on building separate density
models over the observed image data cases where the faces do and don’t have the same
identity. They are all subspace models and describe data as a linear combination of basis
vectors. We have previously encountered several models of this type, including factor
analysis (Section 7.6) and PPCA (Section 17.5.1). The models in this chapter are most
closely related to factor analysis, so we will start by reviewing this.
Factor analysis
Recall that the factor analysis model explained the ith data example xi as
xi = µ + Φhi + ϵi,
(18.1)
where µ is the overall mean of the data. The matrix Φ = [φ1,φ2,...,φK] contains
K factors in its columns. Each factor can be thought of as a basis vector in a high-
dimensional space, and so together they deﬁne a subspace. The K elements of the hidden
variable hi weight the K factors to explain the observed deviations of the data from the

18 Models for style and identity
425
Figure 18.1 Face recognition.
In face
recognition the goal is to draw inferences
about the identities of face images. This
is difﬁcult because the style in which the
picture was taken can have a more drastic
effect on the observed data than the identi-
ties themselves. For example, the images
in (a–b) are more similar to one another
by most measures than the images in (c–
d) because the style (pose) has changed in
the latter case. Nonetheless, the identities
in (a–b) are different but the identities in
(c–d) are the same. We must build models
that tease apart the contributions of identity
and style to make accurate inferences about
whether the identities match.
mean. Remaining differences that cannot be explained in this way are ascribed to additive
noise ϵi which is normally distributed with diagonal covariance Σ.
In probabilistic terms, we write
Pr(xi|hi) = Normxi[µ + Φhi,Σ]
Pr(hi) = Normhi[0,I],
(18.2)
where we have also deﬁned a suitable prior over the hidden variable hi.
Ancestral
sampling from this model is illustrated in Figure 18.2.
We can compute the likelihood of observing a new data example by marginalizing
over the hidden variable to get a ﬁnal probability model
Pr(xi) =
Z
Pr(xi,hi) dhi =
Z
Pr(xi|hi)Pr(hi) dhi
= Normxi[µ,ΦΦT + Σ].
(18.3)
To learn this model from training data {xi}I
i=1, we use the expectation maximization
algorithm. In the E-step, we compute the posterior distribution Pr(hi|xi) over each
hidden variable hi,
Pr(hi|xi) = Normhi[(ΦT Σ−1Φ + I)−1ΦT Σ−1(xi −µ),(ΦT Σ−1Φ + I)].
(18.4)
In the M-step we update the parameters as
ˆµ =
PI
i=1 xi
I
ˆΦ =
 I
X
i=1
(xi −ˆµ)E[hi]T
! I
X
i=1
E[hihT
i ]
!−1
ˆΣ = 1
I
I
X
i=1
diag
h
(xi −ˆµ)T (xi −ˆµ) −ˆΦE[hi]xT
i
i
,
(18.5)

426
18 Models for style and identity
Figure 18.2 Ancestral sampling from factor analyzer. In both this ﬁgure and other subsequent
ﬁgures in this chapter, we assume that the mean µ is zero. a) To generate from a factor analyzer
deﬁned over 2D data we ﬁrst choose the hidden variable hi from the normally distributed prior.
Here hi is a 1D variable and a small negative value is drawn. b) For each case we weight the factors
Φ by the hidden variable. This generates a point on the subspace (here a 1D subspace indicated
by black line). c) Then we add the noise term ϵi, which is normally distributed with covariance
Σ. Finally, we would add a mean term µ (not shown). d–f) This process is repeated many times.
f) The ﬁnal distribution of the data is a normal distribution that is oriented along the subspace.
Deviations from this subspace are due to the noise term. The ﬁnal covariance is ΦΦT + Σ.
Figure 18.3 Subspace model vs. subspace identity model. a) The subspace model generates
data that are roughly aligned along a subspace (here a 1D subspace deﬁned by the black line) as
illustrated in Figure 18.2. b) In the identity subspace model, the overall data distribution is the
same, but there is additional structure: points that belong to the same identity (same color) are
generated in the same region of space.
where the expectations E[hi] and E[hihT
i ] over the hidden variable are extracted from
the posterior distribution computed in the E-step. More details about factor analysis can
be found in Section 7.6.

18.1
Subspace identity model
427
Figure 18.4 Graphical models for sub-
space model and subspace identity model.
a) In the subspace model (factor analysis),
there is one data example xi per hidden
variable hi and some other parameters θ =
{µ,Φ,Σ} that describe the subspace. b) In
the subspace identity model, there are J data
examples xij per hidden variable hi, and all
of these J examples have the same identity.
18.1
Subspace identity model
The factor analysis model provides a good description of the intensity data in frontal face
images: they really do lie close to a linear subspace (see Figure 7.22). However, this
description of the data does not account for identity. For images that have the same style
(e.g., pose, lighting), we expect faces that have the same identity to lie in a similar part
of the space (Figure 18.3), but there is no mechanism to accomplish this in the original
model.
We now extend the factor analysis model to take account of data examples which are
known to have the same identity and show how to exploit this to make inferences about
the identity of new data examples. We adopt the notation xij to denote the jth of J
observed data examples from the ith of I identities (individuals). In real-world data sets,
it is unlikely that we will have exactly J examples for every individual, and the models
we present do not require this, but this assumption simpliﬁes the notation.
The generative explanation for the observed data xij is now
xij = µ + Φhi + ϵij,
(18.6)
where all of the terms have the same interpretations as before. The key difference is that
now all of the J data examples from the same individual are formed by taking the same
linear combination hi of the basis functions φ1 ...φK. However, a different noise term is
added for each data example, and this explains the differences between the J face images
of a given individual. We can write this in probabilistic form as
Pr(hi) = Normhi[0,I]
(18.7)
Pr(xij|hi) = Normxij[µ + Φhi,Σ],
where as before we have deﬁned a prior over the hidden variables. The graphical models
for both factor analysis and the subspace identity model are illustrated in Figure 18.4.
Figure 18.5 illustrates ancestral sampling from the subspace identity model; as desired
this produces data points that lie close together when the identity is the same.
One way to think of this is that we have decomposed the variance in the model into
two parts. The between-individual variation explains the differences between data due to
different identities and the within-individual variation explains the differences between
data examples due to all other factors. The data density for a single datapoint remains
Pr(xij) = Normxij[µ,ΦΦT + Σ].
(18.8)

428
18 Models for style and identity
Figure 18.5 Ancestral sampling from identity subspace model. a) To generate from this model
we ﬁrst choose the hidden variable hi from the normally distributed prior. Here hi is a 1D variable
and a small negative number is drawn. b) We weight the factors Φ by the hidden variable. This
generates a point on the subspace. c) Then we add different noise terms {ϵij}J
j=1 to create each
of the J examples {xij}J
j=1. In each case, the noise is normally distributed with covariance Σ.
Finally, we would add a mean term µ (not shown). d–f) This process is repeated several times.
f) The ﬁnal distribution of the data is a normal distribution with covariance ΦΦT + Σ. However,
it is structured so that points with the same hidden variable (identity) are close to one another.
However, the two components of the variance now have clear interpretations. The term
ΦΦT corresponds to the between-individual variation, and the term Σ is the within-
individual variation.
18.1.1 Learning
Before we consider how to use this model to draw inferences about identity in face recog-
18.1 nition tasks, we will brieﬂy discuss how to learn the parameters θ = {µ,Φ,Σ}. As for
the factor analysis model, we exploit the EM algorithm to iteratively increase a bound on
the log likelihood. In the E-step we compute the posterior probability distribution over
each of the hidden variables hi given all of the data xi• = {xij}J
j=1 associated with that
particular identity,
Pr(hi|xi•) =
QJ
j=1 Pr(xij|hi)Pr(hi)
R QJ
j=1 Pr(xij|hi)Pr(hi) dhi
(18.9)
= Normhi

(JΦT Σ−1Φ+I)−1ΦT Σ−1
J
X
j=1
(xij−µ),(JΦT Σ−1Φ+I)−1

.

18.1
Subspace identity model
429
Figure 18.6 Subspace identity model parameters. These parameters were learned from J = 3
images of I = 195 individuals from the XM2VTS data set. a) Estimated mean µ. b) Estimated
covariance, Σ. c–l) Four of 32 subspace directions explored by adding and subtracting multiples
of each dimension to the mean.
From this we extract the moments that will be needed in the M-step,
E[hi] = (JΦT Σ−1Φ + I)−1ΦT Σ−1
J
X
j=1
(xij −µ)
E[hihT
i ] = (JΦT Σ−1Φ + I)−1 + E[hi]E[hi]T .
(18.10)
In the M-step we update the parameters using the relations
ˆµ=
PI
i=1
PJ
j=1 xij
IJ
(18.11)
ˆΦ=


I
X
i=1
J
X
j=1
(xij −ˆµ)E[hi]T


 I
X
i=1
JE[hihT
i ]
!−1
Σ= 1
IJ
I
X
i=1
J
X
j=1
diag
h
(xij −ˆµ)T (xij −ˆµ) −ˆΦE[hi]xT
ij
i
,
which were generated by taking the derivative of the EM bound with respect to the rel-
evant quantities, equating the results to zero, and rearranging. We alternate the E- and
M-steps until the loglikelihood of the data no longer increases.
Figure 18.6 shows parameters learned from 70 × 70 pixel face images from the
XM2VTS database. A model with a K =32 dimensional hidden space was learned with
195 identities and 3 images per person. The subspace directions capture major changes
that correlate with identity. For example, ethnicity and gender are clearly represented.
The noise describes whatever remains. It is most prominent around high-contrast features
such as the eyes.
In Figure 18.7 we decompose pairs of matching images into their identity and noise
components. To accomplish this, we compute the MAP hidden variable ˆhi. The posterior
over h is normal (Equation 18.9) and so the MAP estimate is simply the mean of this

430
18 Models for style and identity
Figure 18.7 Fitting subspace identity model to new data. a–b) Original images xi1 and xi2.
These faces can be decomposed into the sum of c–d) an identity component and e–f) a within-
individual noise component. To decompose the image in this way, we computed the MAP estimate
ˆhi of the hidden variable and set the identity component to be µ + Φˆhi. The noise comprises
whatever cannot be explained by the identity. g–l) A second example.
normal. We can then visualize the identity component µ + Φ ˆhi, which is the same for
each image of the same individual and looks like a prototypical view of that person. We
can also visualize the within-individual noise ˆ
ϵij = xij −µ −Φ ˆhi, which explains how
each image of the same person differs.
18.1.2 Inference
We will now discuss how to exploit the model to make inferences about new faces that
were not part of the training data set. In face veriﬁcation problems, we observe two data
examples x1 and x2 and wish to infer the state of the world w ∈{0,1}, where w = 0
denotes the case where the data examples have different identities and w = 1 denotes the
case where the data examples have the same identity.
This is a generative model, and so we calculate the posterior Pr(w|x1,x2) over the
world state using Bayes’ rule
Pr(w = 1|x1,x2) =
Pr(x1,x2|w = 1)Pr(w = 1)
P1
n=0 Pr(x1,x2|w = n)Pr(w = n)
.
(18.12)
To compute this we need the prior probabilities Pr(w = 0) and Pr(w = 1) of the
data examples having different identities or the same identity. In the absence of any other
information, we might set these both to 0.5. We also need expressions for the likelihoods
Pr(x1,x2|w = 0) and Pr(x1,x2|w = 1).
We will ﬁrst consider the likelihood Pr(x1,x2|w = 0) when the two datapoints have
different identities. Here, each image is explained by a different hidden variable and so
the generative equation looks like
x1
x2

=
µ
µ

+
Φ
0
0
Φ
h1
h2

+
ϵ1
ϵ2

,
(18.13)
We note that this has the form of a factor analyzer:
x′ = µ′ + Φ′h′ + ϵ′.
(18.14)

18.1
Subspace identity model
431
We can reexpress this in probabilistic terms as
Pr(x′|h′) = Normx′[µ′ + Φ′h′,Σ′]
Pr(h′) = Normh′[0,I],
(18.15)
where Σ′ is deﬁned as
Σ′ =
Σ
0
0
Σ

.
(18.16)
We can now compute the likelihood Pr(x1,x2|w = 0) by writing the joint likelihood
of the compound variables x′ and h′ and marginalizing over h′, so that
Pr(x1,x2|w = 0) =
Z
Pr(x′|h′)Pr(h′) dh′
= Normx′[µ′,Φ′Φ′T + Σ′],
(18.17)
where we have used the standard factor analysis result for the integration.
For the case where the faces match (w = 1), we know that both data examples
must have been created from the same hidden variable.
To compute the likelihood
Pr(x1,x2|w = 1), we write the compound generative equation
x1
x2

=
µ
µ

+
Φ
Φ

h12 +
ϵ1
ϵ2

,
(18.18)
which we notice also has the form of a standard factor analyzer (Equation 18.14), and so
we can compute the likelihood using the same method.
One way to think about this process is that we are comparing the likelihood for two
different models of the data (Figure 18.8a). However, it should be noted that the model
that categorizes the faces as different (w = 0) has two variables (h1 and h2), whereas
the model that categorizes the faces as the same (w = 1) has only one (h12). One might
expect then that the model with more variables would always provide a superior expla-
nation of the data. In fact this does not happen here, because we marginalized these
variables out of the likelihoods, and so the ﬁnal expressions do not include these hidden
variables. This is an example of Bayesian model selection: it is valid to compare models
with different numbers of parameters as long as they are marginalized out of the ﬁnal
solution.
18.1.3 Inference in other recognition tasks
Face veriﬁcation is only one of several possible face recognition problems.
Others
include:
• Closed set identiﬁcation: Find which one of N gallery faces matches a given
probe face.
• Open set identiﬁcation: Choose one of N gallery faces that matches a probe, or
identify that there is no match in the gallery.
• Clustering: Given N faces, ﬁnd how many different people are present and which
face belongs to each person.
All of these models can be thought of in terms of model comparison (Figure 18.8). For
example, consider a clustering task in which we have three faces x1,x2,x3 and wish to

432
18 Models for style and identity
Figure 18.8 Inference as model comparison. a) Veriﬁcation task. Given two faces x1 and x2, we
must decide whether (i) they belong to different people and hence have separate hidden variables
h1,h2 or (ii) they belong to the same person and hence share a single hidden variable h12. These
two hypotheses are illustrated as the two graphical models. b) Open set identiﬁcation task. We
are given a library {xi}I
i=1 of faces that belong to different people and a probe face xp. In this
case (where I = 2), we must decide whether the probe matches (i) gallery face 1, (ii) gallery face
2, or (iii) none of the gallery faces. In closed set identiﬁcation, we simply omit the latter model.
c) Clustering task. Given three faces x1, x2, and x3, we must decide whether (i) they are all from
the same person, (ii) all from different people, or (iii–v) two of the three match.
know if (i) there are three different identities, or (ii) all of the images belong to the same
person, or (iii) two images belong to the same person and the third belongs to someone
different (distinguishing between the three different ways that this can happen). The
world can take ﬁve states w ∈{1,2,3,4,5} corresponding to these ﬁve situations, and
each is explained by a different compound generative equation. For example, if the ﬁrst
two images are the same person, but the third is different we would write


x1
x2
x3

=


µ
µ
µ

+


Φ
0
Φ
0
0
Φ



h12
h3

+


ϵ1
ϵ2
ϵ3

,
(18.19)
which again has the form of a factor analyzer, and so we can compute the likelihood using
the method described earlier. We compare the likelihood for this model to the likelihoods
for the other models using Bayes’ rule with suitable priors.
18.1.4 Limitations of identity subspace model
The subspace identity model has three main limitations (Figure 18.9).
1. The model of within-individual covariance (diagonal) is inadequate.
2. It is a linear model and cannot model non-Gaussian densities.
3. It cannot model large changes in style (e.g., frontal vs. proﬁle faces).
We tackle these problems by introducing probabilistic linear discriminant analy-
sis (Section 18.2), nonlinear identity models (Section 18.3), and multilinear models
(Sections 18.4–18.5), respectively.

18.2
Probabilistic linear discriminant analysis
433
Figure 18.9 Identity models. a) There are three limitations to the subspace identity model. b)
First, it has an impoverished model of the within-individual noise. To remedy this we develop
probabilistic linear discriminant analysis. c) Second, it is linear and can only describe the distribu-
tion of faces as a normal distribution. Hence, we develop nonlinear models based on mixtures and
kernels. d) Third, it does not work well when there are large style changes. To cope with this, we
introduce multilinear models.
18.2
Probabilistic linear discriminant analysis
The subspace identity model explains the data as the sum of a component due to the iden-
tity and an additive noise term. However, the noise term is rather simple: it describes the
within-individual variation as a normal distribution with diagonal covariance. The esti-
mated noise components that we visualized in Figure 18.7 contain considerable structure,
which suggests that modeling the within-individual variation at each pixel as independent
is insufﬁcient.
Probabilistic linear discriminant analysis (PLDA) uses a more sophisticated model
for the within-individual variation. This model adds a new term to the generative equation
that describes the within-individual variation as also lying on a subspace determined by
a second factor matrix Ψ. The jth image xij of the ith individual is now described as
xij = µ + Φhi + Ψsij + ϵ,
(18.20)
where sij is a hidden variable that represents the style of this face: it describes systematic
contributions to the image from uncontrolled viewing parameters. Notice that it differs
for each instance j, and so it tells us nothing about identity.
The columns of Φ describe the space of between-individual variation, and hi deter-
mines a point in this space. The columns of Ψ describe the space of within-individual

434
18 Models for style and identity
Figure 18.10 Ancestral sampling from PLDA model. a) We sample a hidden variable hi from
the identity prior and use this to weight the between-individual factors Φ. b) We sample J hidden
variables {sij}J
j=1 from the style prior and use these to weight the within-individual factors Ψ. c)
Finally, we add normal noise with diagonal covariance, Σ. d–f) This process is repeated for several
individuals. Notice that the clusters associated with each identity in (f) are now oriented (compare
to Figure 18.5f); we have constructed a more sophisticated model of within-individual variation.
variation, and sij determines a point in this space. A given face is now modeled as the
sum of a term µ + Φhi that derives from the identity of the individual, a term Ψsij that
models the style of this particular image, and a noise term ϵij that explains any remaining
variation (Figure 18.10).
Once again, we can write the model in probabilistic terms:
Pr(hi) = Normhi[0,I]
Pr(sij) = Normsij[0,I]
Pr(xij|hi,sij) = Normxij[µ + Φhi + Ψsij,Σ],
(18.21)
where now we have deﬁned priors over both hidden variables.
18.2.1 Learning
In the E-step, we collect together all the J observations {xij}J
j=1 associated with the
18.2 same identity to form the compound system


xi1
xi2
...
xiJ

=


µ
µ
...
µ

+


Φ
Ψ
0
...
0
Φ
0
Ψ
...
0
...
...
...
...
...
Φ
0
0
...
Ψ




hi
si1
si2
...
siJ


+


ϵi1
ϵi2
...
ϵiJ

,
(18.22)

18.2
Probabilistic linear discriminant analysis
435
Figure 18.11 PLDA model. a–c) As we move around in the between-individual subspace Φ
the images look like different people. d–f) As we move around in the within-individual subspace
Ψ the images look like the same person viewed in slightly different poses and under different
illuminations. The PLDA model has successfully separated out contributions that correlate with
identity form those that don’t. Adapted from Li et al. (2012). c⃝2012 IEEE.
which takes the form of the original subspace identity model x′
i = µ′+Φ′h′
i+ϵ′. We can
hence compute the joint posterior probability distribution over all of the hidden variables
in h′ using Equation 18.9.
In the M-step we write a compound generative equation for each image,
xij = µ +
Φ
Ψ
hi
sij

+ ϵij.
(18.23)
On noting that this has the form xij = µ + Φ′′h′′
ij + ϵij of the standard factor analysis
model, we can solve for the unknown parameters using Equations 18.5. The computa-
tions require the expectations E[h′′
ij] and E[h′′
ijh
′′T
ij ], and these can be extracted from
the posterior computed in the E-step.
Figure 18.11 shows parameters learned from J = 3 examples each of I = 195 peo-
ple from the XM2VTS database with 16 between-individual basis functions in Φ and
16 within-individual basis functions in Ψ. The ﬁgure demonstrates that the model has
distinguished these two components.
18.2.2 Inference
As for the subspace identity model, we perform inference by comparing the likelihoods
of models using Bayes’s rule. For example, in the veriﬁcation task we compare models

436
18 Models for style and identity
that explain the two data examples x1 and x2 as having either their own identities h1 and
h2 or sharing a single identity h12. When the identities are different (w=0), the data are
generated as
x1
x2

=
µ
µ

+
Φ
0
Ψ
0
0
Φ
0
Ψ



h1
h2
s1
s2

+
ϵ1
ϵ2

.
(18.24)
When the identities are the same (w=1), the data are generated as
x1
x2

=
µ
µ

+
Φ
Ψ
0
Φ
0
Ψ


h12
s1
s2

+
ϵ1
ϵ2

.
(18.25)
Both of these formulae have the same form x′ = µ′ +Φ′h′ +ϵ′ as the original factor
analysis model, and so the likelihood of the data x′ after marginalizing out the hidden
variables h′ is given by
Pr(x′) = Normx′[µ′,Φ′Φ′T + Σ′],
(18.26)
where the particular choice of Φ′ comes from Equations 18.24 or 18.25, respectively.
Other inference tasks concerning identity such as closed set recognition and clustering
can be formulated in a similar way; we associate one value of the discrete world variable
w = {1,...,K} with each possible conﬁguration of identities, construct a generative
model for each, and compare the likelihoods via Bayes’ rule.
Figure 18.12 compares closed set identiﬁcation performance for several models as a
function of the subspace size (for the PLDA models, the size of Ψ and Φ were always
the same). The results are not state of the art: the images were not properly preprocessed,
and anyway, this data set is considered relatively unchallenging. Nonetheless, the pattern
of results nicely demonstrates an important point The %-correct classiﬁcation improves
Figure 18.12 Face recognition results. The
models were trained using three 70 × 70
RGB images each from 195 people from
the XM2VTS database and tested using two
images each from 100 different people. A
gallery was formed from one image of each
of the test individuals.
For each of the
remaining 100 test images the system had to
identify the match in the gallery. Plots show
%-correct performance as a function of sub-
space dimensionality (number of columns
in Φ and Ψ).
Results show that as the
noise model becomes more complex (adding
within-individual subspace, using diagonal
rather than spherical additive noise) the
results improve systematically.

18.3
Nonlinear identity models
437
Figure 18.13 Mixture models. One way
to create more complex models is to use
a mixture of subspace identity models, or
a mixture of PLDAs.
A discrete vari-
able is associated with each datapoint that
indicates to which mixture component it
belongs. If two faces belong to the same
person, this must be the same; every
image of the same person is associated
with one mixture component. The within-
individual model may also vary between
components.
Consequently, the within-
individual variation may differ depending
on the identity of the face.
as we increases the model’s ability to describe within-individual noise: building more
complex models is worth the time and effort!
18.3
Nonlinear identity models
The models discussed so far describe the between-individual and within-individual vari-
ance by means of linear models and produce ﬁnal densities that are normally distributed.
However, there is no particular reason to believe that the distribution of faces is normal.
We now brieﬂy discuss two methods to generalize the preceding models to the nonlinear
case.
The ﬁrst approach is to note that since the identity subspace model and PLDA are
both valid probabilistic models, we can easily describe a more complex distribution in
terms of mixtures of these elements. For example, a mixture of PLDAs model (Figure
18.13) can be written as
Pr(ci) = Catci[λ]
Pr(hi) = Normhi[0,I]
Pr(sij) = Normsij[0,I]
Pr(xij|ci,hi,sij) = Normxij[µci + Φcihi + Ψcisij,Σci],
(18.27)
where ci ∈[1...C] is a hidden variable that determines to which of the c clusters the
data belong. Each cluster has different parameters, so the full model is nonlinear. To
learn this model, we embed the existing learning algorithm inside a second EM loop that
associates each identity with a cluster. In inference, we assume that faces must belong to
the same cluster if they match.
A second approach is based on the Gaussian process latent variable model (see Sec-
tion 17.8). The idea is to induce a complex density by passing the hidden variable
through a nonlinear function f[•] before using the result to weight the basis functions.
For example, the generalization of the subspace identity model to the nonlinear case can

438
18 Models for style and identity
be written as
Pr(hi) = Normhi[0,I]
Pr(xij|hi,µ,Φ,Σ) = Normxij[µ + Φf[hi],Σ].
(18.28)
Although this model is conceptually simple, it is harder to work with in practice: it is
no longer possible to marginalize over the hidden variables. However, the model is still
linear with respect to the factor matrix Φ, and it is possible to marginalize over this and
the mean µ, giving a likelihood term of the form
Pr(xij|hi,Σ) =
Z Z
Normxij[µ + Φf[hi],Σ]dµdΦ.
(18.29)
This model can be expressed in terms of inner products of the transformed hidden
variables f[h] and so is amenable to kernelization.
Unfortunately, because we can-
not marginalize over hi, it is no longer possible exactly to compare model likelihoods
directly in the inference stage. However, in practice there are ways to approximate this
process.
18.4
Asymmetric bilinear models
The models that we have discussed so far are sufﬁcient if the within-individual variation
is small. However, there are other situations where the style of the data may change
considerably. For example, consider the problem of face recognition when some of the
faces are frontal and others proﬁle. Unfortunately, any given frontal face has more in
common visually with other non-matching frontal faces than it does with a matching
proﬁle face.
Motivated by this problem, we now develop the asymmetric bilinear model for mod-
eling identity and style: as before, we treat the identity hi as continuous, but now we
treat the style s ∈{1...S} as discrete taking one of S possible values. For example, in
the cross-pose face recognition example, s = 0 might indicate a frontal face, and s = 1
might indicate a proﬁle face. The model is hence asymmetric as it treats identity and
style differently. The expression of the identity depends on the style category so that the
same identity may produce completely different data in different styles.
We adopt the notation xijs to denote the jth of J examples of the ith of I identities
in the sth of S styles. The data are generated as
xijs = µs + Φshi + ϵijs,
(18.30)
where µs is a mean vector associated with the sth style, Φs contains basis functions
associated with the sth style, and ϵijs is additive normal noise with a covariance Σs
that also depends on the style. When the noise covariances are spherical, this model is
a probabilistic form of canonical correlation analysis. When the noise is diagonal, it is
known as tied factor analysis. We will use the generic term asymmetric bilinear model
to cover both situations.
Equation 18.30 is easy to parse; for a given individual, the identity hi is constant.
The data are explained as a weighted linear sum of basis functions, where the weights

18.4
Asymmetric bilinear models
439
Figure 18.14 Asymmetric bilinear model with two styles. a) We draw a hidden variable hi from
the prior and use this to weight basis functions Φ1 (one basis function φ1 shown). The result is
added to the mean µ1. b) We use the same value of hi to weight a second set of basis functions Φ2
and add the result to µ2. c) We add normally distributed noise with a diagonal covariance Σs that
depends on the style. d–f) When we repeat this procedure, it produces one normal distribution per
style. The data within each style are structured so that nearby points have the same identity (color)
and identities that are close in one cluster are also close in the other cluster.
determine the identity. However, the basis functions (and other aspects of the model) are
now contingent on the style.
We can write the model in probabilistic terms as
Pr(s) = Cats[λ]
Pr(hi) = Normhi[0,I]
Pr(xijs|hi,s) = Normxijs[µs + Φshi,Σs],
(18.31)
where λ contains parameters that determine the probability of observing data in each
style. Figure 18.14 demonstrates ancestral sampling from this model. If we marginalize
over the identity parameter h and the style parameter s, the overall data distribution
(without regard to the structure of the style clusters) is a mixture of factor analyzers,
Pr(x) =
S
X
s=1
λsNormx[µs,ΦsΦT
s + Σs].
(18.32)
18.4.1 Learning
For simplicity, we will assume that the styles of each training example are known and so
18.3 it is also trivial to estimate the categorical parameters λ. As for the previous models in
this chapter, we employ the EM algorithm.

440
18 Models for style and identity
In the E-step, we compute a posterior distribution over the hidden variable hi that
represents the identity, using all of the training data for that individual regardless of the
style. Employing Bayes’ rule, we have
Pr(hi|xi••) =
QJ
j=1
QS
s=1 Pr(xijs|hi)Pr(hi)
R QJ
j=1
QS
s=1 Pr(xijs|hi)Pr(hi) dhi
,
(18.33)
where xi•• = {xijs}J,S
j,s=1 denotes all the data associated with the ith individual.
One way to compute this is to write a compound generative equation for xi••. For
example, with J = 2 images at each of S = 2 styles we would have


xi11
xi12
xi21
xi22

=


µ1
µ2
µ1
µ2

+


Φ1
Φ2
Φ1
Φ2

hi +


ϵ11
ϵ12
ϵ21
ϵ22

,
(18.34)
which has the same form as the identity subspace model, x′
ij = µ′ +Φ′hi +ϵ′
ij. We can
hence compute the posterior distribution using Equation 18.9 and extract the expected
values needed for the M-step using Equation 18.10.
In the M-step we update the parameters θs = {µs,Φs,Σs} for each style separately
using all of the relevant data. This gives the updates
ˆµs =
PI
i=1
PJ
j=1 xijs
IJ
ˆΦs =


I
X
i=1
J
X
j=1
(xijs −ˆµs)E[hi]T


 
J
I
X
i=1
E[hihT
i ]
!−1
ˆΣs = 1
IJ
I
X
i=1
J
X
j=1
diag
h
(xijs −ˆµs)T (xijs −ˆµs) −ˆΦsE[hi]xT
ijs
i
.
(18.35)
As usual, we iterate these two steps until the system converges and the log likelihood
ceases to improve. Figure 18.15 shows examples of the learned parameters for a data set
that includes faces at two poses.
18.4.2 Inference
There are a number of possible forms of inference in this model. These include:
1. Given x, infer the style s ∈{1,...,S}.
2. Given x, infer the parameterized identity h.
3. Given x1 and x2, infer whether they have the same identity or not.
4. Given x1 in style s1, translate the style to s2 to create ˆx2.
We will consider each in turn.

18.4
Asymmetric bilinear models
441
Figure 18.15 Learned parameters of asymmetric bilinear model with two styles (frontal and pro-
ﬁle faces). This model was learned from one 70 × 70 image of each style in 200 individuals from
the FERET data set. a–b) Mean vector for each style. c–d) Diagonal covariance for each style.
e–f) Varying ﬁrst basis function in each style (notation φks denotes kth basis function of sth
style). g–h) Varying second basis function in each style. i–l) Varying third basis function in each
style. Manipulating the two sets of basis functions in the same way produces images that look like
the same person, viewed in each of the styles. Adapted from Prince et al. (2008). c⃝2008 IEEE.
Inferring style
The likelihood of the data given style s but regardless of identity h is
Pr(x|s) =
Z
Pr(x|h,s)Pr(h) dh
=
Z
Normx[µs + Φsh,Σs]Pr(h) dh
= Normx[µs,ΦsΦT
s + Σs].
(18.36)
The posterior Pr(s|x) over style s can be computed by combining this likelihood with
the prior Pr(s) using Bayes’ rule. The prior for style s is given by
Pr(s) = Cats[λ].
(18.37)
Inferring identity
The likelihood of the data for a ﬁxed identity h but regardless of style s is
Pr(x|h) =
S
X
s=1
Pr(x|h,s)Pr(s)
=
S
X
s=1
Normx[µs + Φsh,Σs]λs.
(18.38)

442
18 Models for style and identity
The posterior over identity can now be combined with the prior Pr(h) using Bayes’ rule
and is given by
Pr(h|x) =
S
X
s=1
λsNormhi[(ΦT
s Σ−1
s Φs + I)−1ΦT
s Σ−1
s (xi −µs),(ΦT
s Σ−1
s Φs + I)].
(18.39)
Note that this posterior distribution is a mixture of Gaussians, with one component for
each possible style.
Identity matching
Given two examples x1,x2, compute the posterior probability that they have the same
identity, even though they may be viewed in different styles. We will initially assume the
styles are known and are s1 and s2, respectively. We ﬁrst build the compound model
x1
x2

=
µs1
µs2

+
Φs1
0
0
Φs2
h1
h2

+
ϵ1
ϵ2

,
(18.40)
which represents the case where the identities differ (w = 0). We compute the likelihood
by noting that this has the form x′ = µ′ + Φ′h′ + ϵ′ of the original factor analyzer, and
so we can write
Pr(x′|w = 0) = Normx′[µ′,Φ′Φ
′T + Σ′],
(18.41)
where Σ′ is a diagonal matrix containing the (diagonal) covariances of the elements of
ϵ′ (as in Equation 18.16).
Similarly, we can build a system for when the identities match (w = 1)

x1
x2

=

µs1
µs2

+

Φs1
Φs2

h12 +

ϵ1
ϵ2

(18.42)
and compute its likelihood Pr(x′|w = 1) in the same way. The posterior probability
Pr(w = 1|x′) can then be computed using Bayes’ rule.
If we do not know the styles, then each likelihood term will become a mixture of
Gaussians where each component has the form of Equation 18.41. There will be one
component for every one of the S2 combinations of the two styles. The mixing weights
will be given by the probability of observing that combination so that Pr(s1 = m,s2 =
n) = λmλn.
Style translation
Finally, let us consider style translation. Given observed data x in style s1 translate to the
18.4 style s2 while maintaining the same identity. A simple way to get a point estimate of the
translated styles is to ﬁrst estimate the identity variable h based on the observed image
xs1. To do this, we compute the posterior distribution over the hidden variable
Pr(h|x,s1) = Normhi[(ΦT
s1Σ−1
s1 Φs1+I)−1ΦT
s1Σ−1
s1 (xi−µs1),(ΦT
s1Σ−1
s1 Φs1+I)],
(18.43)

18.5
Symmetric bilinear and multilinear models
443
Figure 18.16 Style translation based on asymmetric bilinear model from Figure 18.15. a) Orig-
inal face in style 1 (frontal). b) Translated to style 2 (proﬁle). c–d) A second example. Adapted
from Prince et al. (2008). c⃝2008 IEEE.
and then set h to the MAP estimate
ˆhMAP = argmax
h
[Pr(h|x,s1)]
= (ΦT
s1Σ−1
s1 Φs1)−1ΦT
s1Σ−1
s1 (xi −µs1),
(18.44)
which is just the mean of this distribution.
We then generate the image in the second style as
xs2 = µs2 + Φs2 ˆhMAP ,
(18.45)
which is the original generative equation with the noise term omitted.
18.5
Symmetric bilinear and multilinear models
As the name suggests, symmetric bilinear models treat both style and identity equiva-
lently. Both are continuous variables, and the model is linear in each. To write these
models in a compact way, it is necessary to introduce tensor product notation. In this
notation (see Appendix C.3), the generative equation for the subspace identity model
(Equation 18.6) is written as
xij = µ + Φ×2 hi + ϵij,
(18.46)
where the notation Φ×2 hi means take the dot product of the second dimension of Φ
with hi. Since Φ was originally a 2D matrix, this returns a vector.
In the symmetric bilinear model, the generative equation for the jth example of the
ith identity in the kth style is given by
xijk = µ + Φ×2 hi×3 sk + ϵijk,
(18.47)
where hi is a 1D vector representing the identity, sk is a 1D vector representing the style,
and Φ is now a 3D tensor. In the expression Φ×2 hi×3 sk we take the dot product with
two of these three dimensions, leaving a column vector as desired.
In probabilistic form, we write
Pr(hi) = Normhi[0,I]
Pr(sk) = Normsk[0,I]
Pr(xijk|hi,sk) = Normxijk[µ + Φ×2 hi×3 sk,Σ].
(18.48)

444
18 Models for style and identity
Figure 18.17 Ancestral sampling from symmetric bilinear model, with 1D identity and 2D style.
a) In this model each style dimension consists of a subspace identity model with a 1D subspace.
b) For a given style vector s1, we weight these models to create a new subspace identity model.
c) We then generate from this by weighting the factor by the hidden variable h and d) adding noise
to generate different instances of this identity in this style. e) A different weighting induced by
the style vector s2 creates a different subspace model. f) Generation from the resulting subspace
identity model.
For a ﬁxed style vector sk this model is exactly a subspace identity model with hid-
den variable hi The choice of style determines the factors by weighting a set of basis
functions to create them. It is also possible to make the mean vector depend on the style
by using the model
xijk = µ ×2 sk + Φ×2 hi×3 sk + ϵijk,
(18.49)
where µ is now a matrix with basis functions in the columns that are weighted by the
style s. Ancestral sampling from this model is illustrated in Figure 18.17.
It is instructive to compare the asymmetric and symmetric bilinear models. In the
asymmetric bilinear model, there were a discrete number of styles each of which gener-
ated data that individually looked like a subspace identity model, but the model induced a
relationship between the position of an identity in one style cluster and in another. In the
symmetric bilinear model, there is a continuous family of styles that produces a continu-
ous family of subspace identity models. Again the model induces a relationship between
the position of an identity in each.
Up to this point, we have described the model as a subspace identity model for ﬁxed
style. The model is symmetric, and so it is possible to reverse the roles of the variables.
For a ﬁxed identity, the model looks like a subspace model where the basis functions are
weighted by the variable sk. In other words, the model is linear in both sets of hidden
variables when the other is ﬁxed. It is not, however, simultaneously linear in both h and s
together. These variables have a nonlinear interaction, and overall the model is nonlinear.

18.5
Symmetric bilinear and multilinear models
445
18.5.1 Learning
Unfortunately, is not possible to compute the likelihood of the bilinear model in closed
form; we cannot simultaneously marginalize over both sets of hidden variables and
compute
Pr(xijk|θ) =
Z Z
Pr(xijk,hi,sk|θ) dhidsk,
(18.50)
where θ = {µ,Φ,Σ} represents all of the unknown parameters. The usual approach
for learning models with hidden variables is to use the EM algorithm, but this is no
longer suitable because we cannot compute the joint posterior distribution over the hidden
variables Pr(hi,sk|{xijk}J
j=1) in closed form either.
For the special case of spherical additive noise Σ = σ2I, and complete data (where
we see J examples of each of the I individuals in each of K styles), it is possible to solve
for the parameters in closed form using a method similar to that used for PPCA (Section
17.5.1). This technique relies on the N-mode singular value decomposition, which is the
generalization of the SVD to higher dimensions.
For models with diagonal noise, we can approximate by maximizing over one of the
hidden variables rather than marginalizing over them. For example, if we maximize over
the style parameters so that
θ = argmax
θ


K
X
k=1
max
sk


I
X
i=1
J
X
j=1
log
Z
Pr(xijk,hi,sk|θ) dhi



,
(18.51)
then the remaining model is linear in the hidden variables hi. It would hence be possible
to apply an alternating approach in which we ﬁrst ﬁx the styles and learn the parameters
with the EM algorithm and then ﬁx the parameters and update the style parameters using
optimization.
18.5.2 Inference
Various forms of inference are possible, including all of those discussed for the asym-
metric bilinear model. We can, for example, make decisions about whether identities
match by comparing different compound models. It is not possible to marginalize over
both the identity and style variables in these models, and so we maximize over the style
variable in a similar manner to the learning procedure. Similarly, we can translate from
one style to another by estimating the identity variable h (and the current style variable s
if unknown) from the observed data. We then use the generative equation with a different
style vector s to simulate a new example in a different style.
The symmetric bilinear model has a continuous parameterization of style, and so it is
also possible to perform a new translation task: given an example whose identity we have
not previously seen and whose style we have not previously seen, we can translate either
its style or identity as required. We ﬁrst compute the current identity and style which can
be done using a nonlinear optimization approach,
ˆh,ˆs = argmax
h,s
[Pr(x|θ,h,s)Pr(h)Pr(s)].
(18.52)
Then we simulate new examples using the generative equation, modifying the style s or
identity h as required. An example of this is shown in Figure 18.18.

446
18 Models for style and identity
Figure 18.18 Translation of styles using symmetric bilinear model. a) We learn the model from
a set of images, where the styles (rows) and identities (columns) are known. Then we are given
a new image which has a previously unseen identity and style. b) The symmetric bilinear model
can estimate the identity parameters and simulate the image in new styles, or c) estimate the style
parameters and simulate new identities. In both cases, the simulated results are close to the ground
truth. Adapted from Tenenbaum and Freeman (2000). c⃝2000 MIT Press.
18.5.3 Multilinear models
The symmetric bilinear model can be extended to create multilinear or multifactor mod-
els. For example,we might describe our data as depending on three hidden variables, h,s
and t, so the generative equation becomes
xijkl = µ + Φ×2 hi×3 sk×4 tl + ϵijkl,
(18.53)
and now the tensor Φ becomes four-dimensional. As in the symmetric bilinear model,
it is not possible to marginalize over all of the hidden variables in closed form, and this
constrains the possible methods for learning and inference.
18.6
Applications
We have illustrated many of the models in this chapter with examples from face recogni-
tion. In this section, we will describe face recognition in more detail and talk about some
of the practicalities of building a recognition system. Subsequently, we will discuss an
application in which a visual texture is compactly represented as a multilinear model.
Finally, we will describe a nonlinear version of the multilinear model that can be used to
synthesize animation data.

18.6
Applications
447
18.6.1 Face recognition
To provide a more concrete idea of how well these algorithms work in practice, we will
discuss a recent application in detail. Li et al. (2012) present a recognition system based
on probabilistic linear discriminant analysis.
Eight keypoints on each face were identiﬁed, and the images were registered using
a piecewise afﬁne warp. The ﬁnal image size was 400 × 400. Feature vectors were
extracted from the area of the image around each keypoint. The feature vectors consisted
of image gradients at 8 orientations and three scales at points in a 6 × 6 grid centered on
the keypoint. A separate recognition model was built for each keypoint and these were
treated as independent in the ﬁnal recognition decision.
The system was trained using only the ﬁrst 195 individuals from the XM2VTS
database and signal and noise subspaces of size 64. In testing, the algorithm was pre-
sented with 80 images taken from the remaining 100 individuals in the database and
was required to cluster them into groups according to identity. There may be 80 images
of the same person or 80 images of different people or any permutation between these
extremes.
In principle it is possible to calculate the likelihood for each possible clustering of the
data. Unfortunately, in practice there are far too many possible conﬁgurations. Hence, Li
et al. (2012) adopted a greedy agglomerative strategy. They started with the hypothesis
that there are 80 different individuals. They considered merging all pairs of individuals
and chose the combination that increased the likelihood the most. They repeated this
process until the likelihood could not be improved. Example clustering results are illus-
trated in Figure 18.19 and are typical of state-of-the-art recognition algorithms; they cope
relatively easily with frontal faces under controlled lighting conditions.
However, for more natural images the same algorithms struggle even with more
sophisticated preprocessing. For example, Li et al. (2012) applied the PLDA model to
face veriﬁcation in the “Labeled Faces in the Wild” dataset (Huang et al. 2007b), which
Figure 18.19 Face clustering results from Li et al. (2012). The algorithm was presented with a
set of 80 faces consisting of with 4 pictures each of 20 people and clustered these almost perfectly;
it correctly found 19 of these 20 groups of images but erroneously divided the data from one
individual into two separate clusters. The algorithm works well for these frontal faces despite
changes in expression (e.g., cluster 3), changes in hairstyle (e.g., cluster 9) and the addition or
removal of glasses (e.g., cluster 4). Adapted from Li et al. (2012). c⃝2012 IEEE.

448
18 Models for style and identity
Figure 18.20 Tensor textures. a) The training set consists of renderings of a set of coins viewed
from several different directions and with several different lighting directions. b) A new texture
(bottom-left corner) is computed as a weighted sum of the learned basis functions stored in the 3D
tensor Φ. c) Several frames of a video sequence in which the texture is synthesized appropriately
from the model. Adapted from Vasilescu and Terzopoulos (2004). c⃝2003 ACM.
contains images of famous people collected from the Internet, and obtained an equal-
error rate of approximately 10 percent. This is typical of the state of the art at the time of
writing and is much worse than for faces captured under controlled conditions.
18.6.2 Modeling texture
The interaction of light with a surface can be described by the bi-directional reﬂectance
distribution function; essentially, this describes the outgoing light at each angle from
the surface given incoming light at a particular angle to the surface. The bi-directional
texture function (BTF) generalizes this model to also depend on the 2D position on the
surface of the object. If we know the BTF, then we know how a textured surface will
appear from every angle and under every lighting combination.
This function could be approximated by taking several thousand images of the sur-
face viewed from different angles and under different lighting conditions. However, the
resulting data are clearly highly redundant. Vasilescu and Terzopoulos (2004) described
the BTF using a multilinear model known as “TensorTextures.” It contained style factors
that represent the lighting and viewing directions (Figure 18.20a–b). Although both of
these quantities are naturally 2D, they represented them as vectors of size 21 and 37,
respectively, where each training example was transformed to one coordinate axis.
Figure 18.20c shows images generated from the TensorTextures model; the hidden
variables associated in each style were chosen by linearly interpolating between the hid-
den variables of nearby training examples, and the image was synthesized without the

18.6
Applications
449
addition of noise. It can be seen that the TensorTextures model has learned a compact
representation of the appearance variation under changes in viewpoint and illumination,
including complex effects due to self-occlusion, inter-reﬂection, and self-shadowing.
18.6.3 Animation synthesis
Wang et al. (2007) developed a multifactor model that depended nonlinearly on the iden-
tity and style components. Their approach was based on the Gaussian process latent
variable model; the style and identity factors were transformed through nonlinear func-
tions before weighting the tensor Φ. In this type of model, the likelihood might be
given by
Pr(xijk|Σ,hi,sk) =
Z Z
Normxijk[µ + Φ×2 f[hi]×3 g[sk],Σ] dΦdµ,
(18.54)
where f[•] and g[•] are nonlinear functions that transform the identity and style parame-
ters, respectively. In practice, the tensor Φ can be marginalized out of the ﬁnal likelihood
computation in closed form along with the overall mean µ. This model can be expressed
in terms of inner products of the identity and style parameters and can hence be ker-
nelized. It is known as the multifactor Gaussian process latent variable model or the
multifactor GPLVM.
Wang et al. (2007) used this model to describe human locomotion. A single pose
was described as an 89-dimensional vector, which consisted of 43 joint angles, the corre-
sponding 43 angular velocities and the global translational velocity. They built a model
consisting of three factors; the identity of the individual, the gait of locomotion (walk,
stride, or run), and the current state in the motion sequence. Each was represented as a
3D vector. They learned the model from human capture data using an RBF kernel.
Figure 18.21 shows the results of style translation in this model. The system can
predict realistic body poses in styles that have not been observed in the training data.
Figure 18.21 Multifactor GPLVM applied to animation synthesis. A three-factor model was
learned with factors for identity, style, and position in the gait sequence. The ﬁgures shows training
data (red boxes) and synthesized data from the learned model (green boxes). In each case, it
manages to simulate the style and identity well. Adapted from Wang et al. (2007).

450
18 Models for style and identity
Since the system is generative, it can also be used to synthesize novel motion sequences
for a given individual in which the gait varies over time.
Discussion
In this chapter, we have examined a number of models that describe image data as a
function of style and content variables. During training, these variables are forced to take
the same value for examples where we know the style or content are the same. We have
demonstrated a number of different forms of inference including identity recognition and
style translation.
Notes
Face recognition: For a readable introduction to face recognition consult Chellappa et al. (2010).
For more details, consult the review paper by Zhao et al. (2003) or the edited book by Li and Jain
(2005).
Subspace methods for face recognition: Turk and Pentland (2001) developed the eigenfaces
method in which the pixel data were reduced in dimension by linearly projecting it onto a subspace
corresponding to the principal components of the training data. The decision about whether two
faces matched or not was based on the distance between these low-dimensional representations.
This approach quickly supplanted earlier techniques that had been based on measuring the relative
distance between facial features (Brunelli and Poggio 1993).
The subsequent history of face recognition has been dominated by other subspace methods.
Researchers have variously investigated the choice of basis functions (e.g., Bartlett et al. 1998; Bel-
humeur et al. 1997; He et al. 2005; Cai et al. 2007), analogous nonlinear techniques (Yang 2002),
and the choice of distance metric (Perlibakas 2004). The relationship between different subspace
models is discussed in (Wang and Tang 2004b). A review of subspace methods (without particular
reference to face recognition) can be found in De La Torre (2011).
Linear discriminant analysis A notable subcategory of these subspace methods consists of
approaches based on linear discriminant analysis (LDA). The Fisherfaces algorithm (Belhumeur
et al. 1997) projected face data to a space where the ratio of between-individual variation to within-
individual variation was maximized. Fisherfaces is limited to directions in which at least some
within-individual variance has been observed (the small-sample problem). The null-space LDA
approach (Chen et al. 2000) exploited the signal in the remaining subspace. The Dual-Space LDA
approach (Wang and Tang 2004a) combined these two sources of information.
Probabilistic approaches: The identity models in this chapter are probabilistic reinterpretations
of earlier non-probabilistic techniques. For example, the subspace identity model is very similar
to the eigenfaces algorithm (Turk and Pentland 2001), and probabilistic LDA is very similar to the
Fisherfaces algorithm (Belhumeur et al. 1997). For more details about these probabilistic versions,
consult Li et al. (2012) and Ioffe (2006), who presented a slightly different probabilistic LDA
algorithm. There have also been many other probabilistic approaches to face recognition (Liu and
Wechsler 1998; Moghaddam et al. 2000; Wang and Tang 2003; Zhou and Chellappa 2004).
Alignment and pose changes:
An important part of most face recognition pipelines is to
accurately identify facial features so that either (i) the face image can be aligned to a ﬁxed tem-
plate or (ii) the separate parts of the face can be treated independently (Wiskott et al. 1997;
Moghaddam and Pentland 1997). Common methods to identify facial features include the use
of active shape models (Edwards et al. 1998) or pictorial structures (Everingham et al. 2006;
Li et al. 2010).
For larger pose changes, it may not be possible to warp the face accurately to a common template,
and explicit methods are required to compare the faces. These include ﬁtting 3D morphable models

Problems
451
to the images and then simulating a frontal image from a non-frontal one (Blanz et al. 2005),
predicting the face at one pose from another using statistical methods (Gross et al. 2002; Lucey
and Chen 2006) or using the tied factor analysis model discussed in this chapter (Prince et al.
2008). A review of face recognition across large pose changes can be found in Zhang and Gao
(2009).
Current work in face recognition: It is now considered that face recognition for frontal faces in
constant lighting and with no pose or expression changes is almost solved. Earlier databases that
have these characteristics (e.g., Messer et al. 1999; Phillips et al. 2000) have now been supplanted
by test databases containing more variation (Huang et al. 2007b).
Several recent trends have emerged in face recognition. These include a resurgence of interest
in discriminative models (e.g., Wolf et al. 2009; Taigman et al. 2009; Kumar et al. 2009), the
application of learning of metrics to discriminate identity (e.g., Nowak and Jurie 2007; Ferencz
et al. 2008; Guillaumin et al. 2009; Nguyen and Bai 2010), the use of sparse representations (e.g.,
Wright et al. 2009), and a strong interest in preprocessing techniques. In particular, many current
methods are based on Gabor features (e.g., Wang and Tang 2003), local binary patterns (Ojala
et al. 2002; Ahonen et al. 2004), three-patch local binary patterns (Wolf et al. 2009), or SIFT
features (Lowe 2004). Some of the most successful methods combine or select several different
preprocessing techniques (Li et al. 2012; Taigman et al. 2009; Pinto and Cox 2011).
Bilinear and multilinear models: Bilinear models were introduced to computer vision by Tenen-
baum and Freeman (2000), and multilinear models were explored by Vasilescu and Terzopoulos
(2002). Kernelized multilinear models were discussed by Li et al. (2005) and Wang et al. (2007).
An alternative approach to nonlinear multifactor models was presnented in Elgammal and Lee
(2004). The most common use of bilinear and multilinear models in computer vision has been for
face recognition in situations where the capture conditions vary (Grimes et al. 2003; Lee et al. 2005;
Cuzzolin 2006; Prince et al. 2008).
Problems
18.1 Prove that the posterior distribution over the hidden variable in the subspace identity model
is as given in Equation 18.9.
18.2 Show that the M-step updates for the subspace identity model are as given in Equation 18.11.
18.3 Develop a closed form solution for learning the parameters {µ,Φ,σ2} of a subspace identity
model where the noise is spherical:
Pr(xij) = Normxij[µ,ΦΦT + σ2I].
Hint: Assume you have exactly J = 2 examples of each of the I training images and base your
solution on probabilistic PCA.
18.4 In a face clustering problem, how many possible models of the data are there with 2, 3, 4, 10,
and 100 faces?
18.5 An alternative approach to face veriﬁcation using the identity subspace model is to compute
the probability of the observed data x1 and x2 under the models:
Pr(x1,x2|w = 0) = Pr(x1)Pr(x2)
Pr(x1,x2|w = 1) = Pr(x1)Pr(x2|x1).
Write down expressions for the marginal probability terms Pr(x1), Pr(x2) and the condi-
tional probability Pr(x2|x1). How could you use these expressions to compute the posterior
Pr(w|x1,x2) over the world state?

452
18 Models for style and identity
18.6 Propose a version of the subspace identity model that is robust to outliers in the training data.
18.7 Moghaddam et al. (2000) took a different probabilistic approach to face veriﬁcation. They
took the difference x∆= x2 −x1 and modeled the likelihoods of this vector Pr(x∆|w = 0) and
Pr(x∆|w = 1) when the two faces match or don’t. Propose expressions for these likelihoods and
discuss learning and inference in this model. Identify one possible disadvantage of this model.
18.8 Develop a model that combines the advantages of PLDA and the asymmetric bilinear model;
it should be able to model the within-individual covariance with a subspace but also be able to
compare data between disparate styles. Discuss learning and inference in your model.
18.9 In the asymmetric bilinear model, how would you infer whether the style of two examples is
the same or not, regardless of whether the images matched?

Chapter 19
Temporal models
This chapter concerns temporal models and tracking. The goal is to infer a sequence
of world states {wt}T
t=1 from a noisy sequence of measurements {xt}T
t=1. The world
states are not independent; each is modeled as being contingent on the previous one. We
exploit this statistical dependency to help estimate the state wt even when the associated
observation xt is partially or completely uninformative.
Since the states form a chain, the resulting models are similar to those in Chap-
ter 11. However, there are two major differences. First, in this chapter, we consider
models where the world state is continuous rather than discrete. Second, the models here
are designed for real-time applications; a judgment is made based on only the past and
present measurements and without knowledge of those in the future.
A prototypical use of these temporal models is for contour tracking. Consider a
parameterized model of an object contour (Figure 19.1). The goal is to track this con-
tour through a sequence of images so that it remains ﬁrmly attached to the object. A
good model should be able to cope with nonrigid deformations of the object, background
clutter, blurring, and occasional occlusion.
19.1
Temporal estimation framework
Each model in this chapter consists of two components.
• The measurement model describes the relationship between the measurements xt
and the state wt at time t. We treat this as generative, and model the likelihood
Pr(xt|wt). We assume that the data at time t depend only on the state at time
t and not those at any other time. In mathematical terms, we assume that xt is
conditionally independent of w1...t−1 given wt.
• The temporal model describes the relationship between states. Typically, we make
the Markov assumption: we assume that each state depends only upon its predeces-
sor. More formally, we assume that wt is conditionally independent of the states
w1...t−2 given its immediate predecessor wt−1, and just model the relationship
Pr(wt|wt−1).
Together, these assumptions lead to the graphical model shown in Figure 19.2.

454
19 Temporal models
Figure 19.1 Contour tracking. The goal is to track the contour (solid blue line) through the
sequence of images so that it remains ﬁrmly attached to the object (see Chapter 17 for more infor-
mation about contour models). The estimation of the contour parameters is based on a temporal
model relating nearby frames, and local measurements from the image (e.g., between the dashed
blue lines). Adapted from Blake and Isard (1998). c⃝1998 Springer.
19.1.1 Inference
In inference, the general problem is to compute the marginal posterior distribution
Pr(wt|x1...t) over the world state wt at time t, given all of the measurements x1...t
up until this time. At time t = 1, we have only observed a single measurement x1, so
our prediction is based entirely on this datum. To compute the posterior distribution
Pr(w1|x1), Bayes’ rule is applied:
Pr(w1|x1) =
Pr(x1|w1)Pr(w1)
R
Pr(x1|w1)Pr(w1) dw1
,
(19.1)
where the distribution Pr(w1) contains our prior knowledge about the initial state.
At time t = 2 we observe a second measurement x2. We now aim to compute the
posterior distribution of the state at time t=2 based on both x1 and x2. Again, we apply
Bayes’ rule,
Pr(w2|x1,x2) =
Pr(x2|w2)Pr(w2|x1)
R
Pr(x2|w2)Pr(w2|x1) dw2
.
(19.2)
Notice that the likelihood term Pr(x2|w2) is only dependent on the current measurement
x2 (according to the assumption stated earlier). The prior Pr(w2|x1) is now based on
what we have learned from the previous measurement; the possible values of the state at
this time depend on our knowledge of what happened at the previous time and how these
are affected by the temporal model.
Generalizing this procedure to time t, we have
Pr(wt|x1...t) =
Pr(xt|wt)Pr(wt|x1...t−1)
R
Pr(xt|wt)Pr(wt|x1...t−1) dwt
.
(19.3)
To evaluate this, we must compute Pr(wt|x1...t−1), which represents our prior knowl-
edge about wt before we look at the associated measurement xt. This prior depends
on our knowledge Pr(wt−1|x1...t−1) of the state at the previous time and the temporal
model Pr(wt|wt−1), and is computed recursively as
Pr(wt|x1...t−1) =
Z
Pr(wt|wt−1)Pr(wt−1|x1...t−1) dwt−1,
(19.4)

19.2
Kalman ﬁlter
455
Figure 19.2 Graphical model for Kalman
ﬁlter and other temporal models in this
chapter. This implies the following condi-
tional independence relation: the state wt
is conditionally independent of the states
w1...t−2 and the measurements x1...t−1
given the previous state wt−1.
which is known as the Chapman–Kolmogorov relation. The ﬁrst term in the integral
represents the prediction for the state at time t given a known state wt−1 at time t−1.
The second term represents the uncertainty about what the state actually was at time
t−1. The Chapman–Kolmogorov equation amalgamates these two pieces of information
to predict what will happen at time t.
Hence, inference consists of two alternating steps. In the prediction step, we compute
the prior Pr(wt|x1...t−1) using the Chapman–Kolmogorov relation (Equation 19.4). In
the measurement incorporation step, we combine this prior with the new information
from the measurement xt using Bayes’s rule (Equation 19.3).
19.1.2 Learning
The goal of learning is to estimate the parameters θ that determine the relationship
Pr(wt|wt−1) between adjacent states and the relationship Pr(xt|wt) between the state
and the data, based on several observed time sequences.
If we know the states for these sequences, then this can be achieved using the max-
imum likelihood method. If the states are unknown, then they can be treated as hidden
variables, and the model can be learned using the EM algorithm (Section 7.8). In the
E-step, we compute the posterior distribution over the states for each time sequence.
This is a process similar to the inference method described earlier, except that it also uses
data from later in the sequence (Section 19.2.6). In the M-step, we update the EM bound
with respect to θ.
The rest of this chapter focusses on inference in this type of temporal model. We will
ﬁrst consider the Kalman ﬁlter. Here the uncertainty over the world state is described
by a normal distribution,1 the relationship between the measurements and the world is
linear with additive normal noise, and the relationship between the state at adjacent times
is also linear with additive normal noise.
19.2
Kalman ﬁlter
To deﬁne the Kalman ﬁlter, we must specify the temporal and measurement models. The
temporal model relates the states at times t−1 and t and is given by
wt = µp + Ψwt−1 + ϵp,
(19.5)
1In the original formulation of the Kalman ﬁlter, it was only assumed that the noise was white; however,
if the distribution is normal, we can compute the exact marginal posterior distributions, so we will favor this
assumption.

456
19 Temporal models
where µp is a Dw×1 vector, which represents the mean change in the state, and Ψ is a
Dw×Dw matrix, which relates the mean of the state at time t to the state at time t−1.
This is known as the transition matrix. The term ϵp is a realization of the transition noise,
which is normally distributed with covariance Σp and determines how closely related the
states are at times t and t−1. Alternately, we can write this in probabilistic form:
Pr(wt|wt−1) = Normwt[µp + Ψwt−1,Σp].
(19.6)
The measurement model relates the data xt at time t to the state wt,
xt = µm + Φwt + ϵm,
(19.7)
where µm is a Dx ×1 mean vector and Φ is a Dx ×Dw matrix relating the Dx ×1
measurement vector to the Dw×1 state. The term ϵm is a realization of the measurement
noise that is normally distributed with covariance Σm. In probabilistic notation, we have
Pr(xt|wt) = Normxt[µm + Φwt,Σm].
(19.8)
Notice that the measurement equation is identical to the relation between the data and
the hidden variable in the factor analysis model (Section 7.6); here the state w replaces
the hidden variable h. In the context of the Kalman ﬁlter, the dimension Dw of the state
w is often larger than the dimension Dx of the measurements x, so Φ is a landscape
matrix, and the measurement noise Σm is not necessarily diagonal.
The form of both the temporal and measurement equations is the same: each is a
normal probability distribution where the mean is a linear function of another variable
and the variance is constant. This form has been carefully chosen because it ensures
that if the marginal posterior Pr(wt−1|x1...t−1) at time t−1 was normal, then so is the
marginal posterior Pr(wt|x1...t) at time t. Hence, the inference procedure consists of
a recursive updating of the means and variances of these normal distributions. We now
elaborate on this procedure.
19.2.1 Inference
In inference, the goal is to compute the posterior probability Pr(wt|x1...t) over the
state wt given all of the measurements x1...t so far. As before, we apply the predic-
tion and measurement-incorporation steps to recursively estimate Pr(wt|x1...t) from
Pr(wt−1|x1...t−1). The latter distribution is assumed to be normal with mean µt−1
and variance Σt−1.
In the prediction step, we compute the prior at time t using the Chapman–
Kolmogorov equation
Pr(wt|x1...t−1) =
Z
Pr(wt|wt−1)Pr(wt−1|x1...t−1) dwt−1
=
Z
Normwt[µp + Ψwt−1,Σp]Normwt−1[µt−1,Σt−1] dwt−1
= Normwt[µp + Ψµt−1,Σp + ΨΣt−1ΨT ]
= Normwt[µ+,Σ+],
(19.9)
where we have denoted the predicted mean and variance of the state by µ+ and Σ+. The
integral between lines 2 and 3 was solved by using Equations 5.17 and 5.14 to rewrite the

19.2
Kalman ﬁlter
457
integrand as proportional to a normal distribution in wt−1. Since the integral of any pdf
is one, the result is the constant of proportionality, which is itself a normal distribution
in wt.
In the measurement incorporation step, we apply Bayes’ rule,
Pr(wt|x1...t) = Pr(xt|wt)Pr(wt|x1...t−1)
Pr(x1...t)
(19.10)
= Normxt[µm + Φwt,Σm]Normwt[µ+,Σ+]
Pr(x1...t)
= Normwt

ΦT Σ−1
m Φ + Σ−1
+
−1 
ΦT Σ−1
m (xt−µm) + Σ−1
+ µ+

,

ΦT Σ−1
m Φ + Σ−1
+
−1
= Normwt[µt,Σt],
where we have used Equation 5.17 on the likelihood term and then combined the likeli-
hood and prior using Equation 5.14. The right-hand side is now proportional to a normal
distribution in wt, and the constant of proportionality must be one to ensure that the
posterior on the left-hand side is a valid distribution.
Notice that the posterior Pr(wt|x1...t) is normal with mean µt and covariance Σt.
We are in the same situation as at the start, so the procedure can be repeated for the next
time step.
It can be shown that the mean of the posterior is a weighted sum of the values pre-
dicted by the measurements and the prior knowledge, and the covariance is smaller than
either. However, this is not particularly obvious from the equations. In the following
section we rewrite the equation for the posterior in a form that makes these properties
more obvious.
19.2.2 Rewriting measurement incorporation step
The measurement incorporation step is rarely presented in the above form in practice.
One reason for this is that the equations for µt and Σt contain an inversion that is of size
Dw × Dw. If the world state is much higher dimensional than the observed data, then it
would be more efﬁcient to reformulate this as an inverse of size Dx × Dx. To this end,
we deﬁne the Kalman gain as
K = Σ+ΦT (Σm + ΦΣ+ΦT )−1.
(19.11)
We will use this to modify the expressions for µt and Σt from Equation 19.10.
Starting with µt, we apply the matrix inversion lemma (Appendix C.8.4):
(ΦT Σ−1
m Φ + Σ−1
+ )−1(ΦT Σ−1
m (xt −µm) + Σ−1
+ µ+)
= K(xt −µm) + (ΦT Σ−1
m Φ + Σ−1
+ )−1Σ−1
+ µ+
= K(xt −µm) + (Σ+ −Σ+ΦT (ΦΣ+ΦT + Σm)−1ΦΣ+)Σ−1
+ µ+
= K(xt −µm) + µ+ −Σ+ΦT (ΦΣ+ΦT + Σm)−1Φµ+
= K(xt −µm) + µ+ −KΦµ+
= µ+ + K
 xt −µm −Φµ+

.
(19.12)

458
19 Temporal models
Figure 19.3 Recursive inference in the Kalman ﬁlter. a) The posterior probability Pr(wt−1|
x1...t−1) of the state wt−1 given all of the measurements x1...t−1 up to that time takes the form of
a normal distribution (green ellipse). b) In the prediction step, we apply the Chapman–Kolmogorov
relation to estimate the prior Pr(wt|x1...t−1), which is also a normal distribution (cyan ellipse). c)
The measurement likelihood Pr(xt|wt) is proportional to a normal distribution (magenta ellipse).
d) To compute the posterior probability Pr(wt|x1...t), we apply Bayes’ rule by taking the product
of the prior from panel (b) and the likelihood from panel (c) and normalizing. This yields a new
normal distribution (yellow ellipse) and the procedure can begin anew.
The expression in brackets in the ﬁnal line is known as the innovation and is the
difference between the actual measurements xt and the predicted measurements µm +
Φµ+ based on the prior estimate of the state. It is easy to see why K is termed the
Kalman gain: it determines the amount that the measurements contribute to the new
estimate in each direction in state space. If the Kalman gain is small in a given direction,
then this implies that the measurements are unreliable relative to the prior and should not
inﬂuence the mean of the state too much. If the Kalman gain is large in a given direction,
then this suggests that the measurements are more reliable than the prior and should be
weighted more highly.

19.2
Kalman ﬁlter
459
We now return to the covariance term of Equation 19.10. Using the matrix inversion
lemma, we get
(ΦT ΣmΦ + Σ−1
+ )−1 = Σ+ −Σ+ΦT (ΦΣ+ΦT + Σm)−1ΦΣ+
= Σ+ −KΦΣ+
= (I −KΦ)Σ+,
(19.13)
which also has a clear interpretation: the posterior covariance is equal to the prior covari-
ance less a term that depends on the Kalman gain: we are always more certain about
the state after incorporating information due to the measurement, and the Kalman gain
modiﬁes how much more certain we are. When the measurements are more reliable, the
Kalman gain is high, and the covariance decreases more.
After these manipulations, we can rewrite Equation 19.10 as
Pr(wt|x1...t) = Normwt

µ+ + K(xt −µm −Φµ+),(I −KΦ)Σ+

.
(19.14)
19.2.3 Inference summary
Developing the inference equations was rather long-winded, so here we summarize the
inference process in the Kalman ﬁlter (see also Figure 19.3). We aim to compute the
marginal posterior probability Pr(wt|x1...t) based on a normally distributed estimate
of the marginal posterior probability Pr(wt−1|x1...t−1) at the previous time and a new
19.1 measurement xt. If the posterior probability at time t−1 has mean µt−1 and variance
Σt−1, then the Kalman ﬁlter updates take the form
State Prediction:
µ+ = µp + Ψµt−1
(19.15)
Covariance Prediction:
Σ+ = Σp + ΨΣt−1ΨT
State Update:
µt = µ+ + K(xt −µm −Φµ+)
Covariance Update:
Σt = (I −KΦ)Σ+,
where
K = Σ+ΦT (Σm + ΦΣ+ΦT )−1.
(19.16)
In the absence of prior information at time t=1, it is usual to initialize the prior mean
µ0 to any reasonable value and the covariance Σ0 to a large multiple of the identity
matrix representing our lack of knowledge.
19.2.4 Example 1
The Kalman ﬁlter update equations are not very intuitive. To help understand the prop-
erties of this model, we present two toy examples. In the ﬁrst case, we consider an
object that is approximately circling a central point in two dimensions. The state consists
of the two-dimensional position of the object. The actual set of states can be seen in
Figure 19.4a.
Let us assume that we do not have a good temporal model that describes this motion.
Instead, we will assume the simplest possible model.
The Brownian motion model
assumes that the state at time t+1 is similar to the state at time t:
Pr(wt|wt−1) = Normwt[wt−1,σ2
pI].
(19.17)

460
19 Temporal models
Figure 19.4 Kalman ﬁlter example 1. The true state (red dots) evolves in a 2D circle. The mea-
surements (magenta dots) consist of direct noisy observations of the state. a) Posterior probability
Pr(wt|xt) of state using measurements alone (uniform prior assumed). The mean of each dis-
tribution is centered on the associated datapoint, and the covariance depends on the measurement
noise. b) Posterior probabilities based on Kalman ﬁlter. Here the temporal equation describes
the state as a random perturbation of the previous state.
Although this is not a good model
(the circular movement is not accounted for), the posterior covariances (blue ellipses) are smaller
than without the Kalman ﬁlter (magenta ellipses in (a)) and the posterior means (blue dots) are
closer to the true states (red dots) than the estimates due to the measurements alone (magenta dots
in (a)).
This is a special case of the more general formulation in Equation 19.6. Notice that this
model has no insight into the fact that the object is rotating.
For ease of visualization, we will assume that the observations are just noisy
realizations of the true 2D state so that
Pr(xt|wt) = Normxt[wt,Σm],
(19.18)
where Σm is diagonal. This is a special case of the general formulation in 19.8.
In inference, the goal is to estimate the posterior probability over the state at each time
step given the observed sequence so far. Figure 19.4b shows the sequence of posterior
probabilities Pr(wt|x1...t) over the state after running the Kalman recursions. It is now
easy to understand why this is referred to as the Kalman ﬁlter: the MAP states (the peaks
of the marginal posteriors) are smoothed relative to estimates from the measurements
alone. Notice that the Kalman ﬁlter estimate has a lower covariance than the estimate
due to the measurements alone (Figure 19.4a). In this example, the inclusion of the
temporal model makes the estimates both more accurate and more certain, even though
the particular choice of temporal model is wrong.
19.2.5 Example 2
Figure 19.5 shows a second example where the setup is the same except that the obser-
vation equation differs at alternate time steps. At even time steps, we observe a noisy
estimate of just the ﬁrst dimension of the state w. At odd time steps, we observe only a

19.2
Kalman ﬁlter
461
Figure 19.5 Kalman ﬁlter example 2. As
for example 1, the true state (red points)
proceeds in an approximate circle.
We
don’t have access to this temporal model
and just assume that the new state is a per-
turbation of the previous state.
At even
time steps, the measurement is a noisy real-
ization of the ﬁrst dimension (horizontal
coordinate) of the state. At odd time steps,
it is a noisy realization of the second dimen-
sion (vertical coordinate). The Kalman ﬁl-
ter produces plausible estimates (blue and
cyan points and ellipses) even though a sin-
gle measurement is being used to determine
a 2D state at any time step.
noisy estimate of the second dimension. The measurement equations are
Pr(xt|wt) = Normxt
1
0
wt,σ2
m

,
for t = 1,3,5...
Pr(xt|wt) = Normxt
0
1
wt,σ2
m

,
for t = 2,4,6....
(19.19)
This is an example of a nonstationary model: the model changes over time.
We use the same set of ground truth 2D states as for example 1 and simulate the
associated measurements using Equation 19.19. The posterior probability over the state
was computed using the Kalman ﬁlter recursions with the relevant measurement matrix
Φ at each time step.
The results (Figure 19.5) show that the Kalman ﬁlter maintains a good estimate of the
2D state despite having only a 1D measurement at each time step. At odd time steps, the
variance in the ﬁrst dimension decreases (due to the information from the measurement),
but the variance in the second dimension increases (due to the uncertainty in the temporal
model). At even time steps, the opposite occurs.
This model may seem esoteric, but it is common in many imaging modalities for
different aspects of the measurements to arrive at different times. One option is to wait
until a full set of measurements is present before estimating the state, but this means that
many of them are out of date. Incorporation of each measurement when it arrives using
a Kalman ﬁlter is a superior approach.
19.2.6 Smoothing
The preceding inference procedure is designed for real-time applications where the esti-
mation of the state is based only on measurements from the past and present. However,
there may be occasions where we wish to infer the state using measurements that lie in
the future. In the parlance of Kalman ﬁlters, this is known as smoothing.
We consider two cases. The ﬁxed lag smoother is still intended for on-line estimation,
but it delays the decision about the state by a ﬁxed number of time steps. The ﬁxed
interval smoother assumes that we have observed all of the measurements in the entire
sequence before we make a judgment about the world state.

462
19 Temporal models
Fixed lag smoother
The ﬁxed lag smoother relies on a simple trick. To estimate the state delayed by τ time
steps, we augment the state vector to contain the delayed estimates of the previous τ
times. The time update equation now takes the form


wt
w[1]
t
w[2]
t
...
w[τ]
t


=


Ψ
0
...
0
0
I
0
...
0
0
0
I
...
0
0
...
...
...
...
...
0
0
...
I
0




wt−1
w[1]
t−1
w[2]
t−1
...
w[τ]
t−1


+


ϵp
0
0
...
0,


,
(19.20)
where the notation w[m]
t
refers to the state at time t −m based on measurements up to
time t, and w[τ]
t
is the quantity we wish to estimate. This state evolution equation clearly
ﬁts the general form of the Kalman ﬁlter (Equation 19.5). It can be parsed as follows:
in the ﬁrst equation of this system, the temporal model is applied to the current estimate
of the state. In the remaining equations, the delayed states are created by simply copying
the previous state.
The associated measurement equation is
xt =
Φ
0
0
...
0


wt
w[1]
t
w[2]
t
...
w[τ]
t ,


+ ϵm,
(19.21)
which ﬁts the form of the Kalman ﬁlter measurement model (Equation 19.7). The cur-
rent measurements are based on the current state, and the time-delayed versions play no
part. Applying the Kalman recursions with these equations computes not just the current
estimate of the state but also the time-delayed estimates.
Fixed interval smoother
The ﬁxed interval smoother consists of a backward set of recursions that estimate the
marginal posterior distributions Pr(wt|x1...T ) of the state at each time step, taking into
account all of the measurements x1...T . In these recursions, the marginal posterior distri-
bution Pr(wt|x1...T ) of the state at time t is updated. Based on this result, the marginal
19.2 posterior Pr(wt−1|x1...T ) at time t −1 is updated and so on. We denote the mean and
variance of the marginal posterior Pr(wt|x1...T ) at time t by µt|T and Σt|T , respectively,
and use
µt|T = µt + Ct(µt+1|T −µ+|t)
Σt|T = Σt + Ct(Σt+1|T −Σ+|t)CT
t ,
(19.22)
where µt and Σt are the estimates of the mean and covariance from the forward pass.
The notation µ+|t and Σ+|t denotes the mean and variance of the posterior distribution
Pr(wt|x1...t−1) of the state at time t based on the measurements up to time t−1 (i.e.,
what we denoted as µ+ and Σ+ during the forward Kalman ﬁlter recursions) and
Ct = Σt|tΨT Σ−1
+|t.
(19.23)

19.2
Kalman ﬁlter
463
Figure 19.6 Fixed lag Kalman smoothing.
The estimated state (light blue dots) and the
associated covariance (light blue ellipses)
were estimated using a lag of 1 time step:
each estimate is based on all of the data
in the past, the present observation and the
observation one step into the future.
The
resulting estimated states have a smaller
variance than for the standard Kalman ﬁl-
ter (Compare to Figure 19.4b) and are closer
to the true state (red dots).
In effect, the
estimate averages out the noise in the mea-
surements (dark blue dots). The cost of this
improvement is that there is a delay of one
time step.
It can be shown that the backward recursions correspond to the backward pass of sum-
product belief propagation in the Kalman ﬁlter graphical model.
19.2.7 Temporal and measurement models
The choice of temporal model in the Kalman ﬁlter is restricted to be linear and is dictated
by the matrix Ψ. Despite this limitation, it is possible to build a surprisingly versatile
family of models within this framework. In this section we review a few of the best-
known examples.
1. Brownian motion: The simplest model is Brownian motion Figure 19.7a in which
the state is operated upon by the identity matrix so that
wt = wt−1 + ϵp.
(19.24)
2. Geometric transformations: The family of linear ﬁlters includes geometric trans-
formations such as rotations, stretches, and shears. For example, choosing Ψ so
that ΨT Ψ = I and |Ψ| = 1 creates a rotation around the origin Figure 19.7b: this
is the true temporal model in Figures 19.4–19.6.
3. Velocities and accelerations: The Brownian motion model can be extended by
adding a constant velocity v to the motion model (Figure 19.7c) so that.
wt = v + wt−1 + ϵp.
(19.25)
To incorporate a changing velocity, we can enhance the state vector to include the
velocity term so that
wt
˙wt

=
I
I
0
I
wt−1
˙wt−1

+ ϵp.
(19.26)
This has the natural interpretation that the velocity term ˙w contributes an offset
to the state at each time. However, the velocity is itself uncertain and is related

464
19 Temporal models
a)
b)
c)
d)
Ψ
Figure 19.7 Temporal and measurement models. a) Brownian motion model. At each time step
the state is randomly perturbed from the previous position, so a drunken walk through state space
occurs. b) Constant velocity model. At each time step, a constant velocity is applied in addition
to the noise. c) Transformation model. At each time step, the state is rotated about the origin and
noise is added. d) Oscillatory measurements. These quasi-sinusoidal measurements are created
from a two-dimensional state that itself undergoes Brownian motion. The two elements of the state
control the sinusoidal and co-sinusoidal parts of the measurements.
by a Brownian motion model to the velocity at the previous time step. For the
measurement equation, we have
xt =
I
0wt
˙wt

+ ϵm.
(19.27)
In other words, the measurements at time t do not directly depend on the veloc-
ity term, only the state itself. This idea can be easily extended to include an
acceleration term as well.
4. Oscillatory data: Some data are naturally oscillatory.
To describe oscillatory
1D data, we could use a state containing a 2×1 state vector w using Brownian
motion as the temporal model. We then implement the nonstationary measurement

19.2
Kalman ﬁlter
465
equation
xt =
cos[2πωt]
sin[2πωt]
wt + ϵm.
(19.28)
For a ﬁxed state w, this model produces noisy sinusoidal data. As the state varies
due to the Brownian motion of the temporal model, the phase and amplitude of the
quasi-sinusoidal output will change (19.7d).
19.2.8 Problems with the Kalman ﬁlter
Although the Kalman ﬁlter is a ﬂexible tool, it has a number of shortcomings (Figure
19.8). Most notably,
• It requires the temporal and measurement equations to be linear, and
• It assumes that the marginal posterior is unimodal and can be well captured by
a mean and covariance; hence, it can only ever have one hypothesis about the
position of the object.
Figure 19.8 Temporal models for tracking. The Kalman ﬁlter has two main problems: ﬁrst, it
requires the temporal and measurement models to be linear. This problem is directly addressed
by the extended and unscented Kalman ﬁlters. Second, it represents the uncertainty with a normal
distribution that is uni-modal and so cannot maintain multiple hypotheses about the state. This
problem is tackled by particle ﬁlters.

466
19 Temporal models
In the following sections, we discuss models that address these problems.
The
extended Kalman ﬁlter and the unscented Kalman ﬁlter both allow nonlinear state update
and measurement equations. Particle ﬁltering abandons the use of the normal distribution
and describes the state as a complex multimodal distribution.
19.3
Extended Kalman ﬁlter
The extended Kalman ﬁlter (EKF) is designed to cope with more general temporal mod-
els, where the relationship between the states at time t is an arbitrary nonlinear function
f[•,•] of the state at the previous time step and a stochastic contribution ϵp
wt = f[wt−1,ϵp],
(19.29)
where the covariance of the noise term ϵp is Σp as before. Similarly, it can cope with a
nonlinear relationship g[•,•] between the state and the measurements
xt = g[wt,ϵm],
(19.30)
where the covariance of ϵm is Σm.
The extended Kalman ﬁlter works by taking linear approximations to the nonlinear
functions at the peak µt of the current estimate using the Taylor expansion. If the function
19.3 is not too nonlinear, then this approximation will adequately represent the function in the
region of the current estimate and we can proceed as usual. We deﬁne the Jacobian
matrices,
Ψ = ∂f[wt−1,ϵp]
∂wt−1

µt−1,0
Υp = ∂f[wt−1,ϵp]
∂ϵp

µt−1,0
Φ = ∂g[wt,ϵm]
∂wt

µ+,0
Υm = ∂g[wt,ϵm]
∂ϵm

µ+,0
,
(19.31)
where the notation |µ+,0 denotes that the derivative is computed at position w = µ+ and
ϵ = 0. Note that we have overloaded the meaning of Φ and Ψ here. Previously they rep-
resented the linear transformations between states at adjacent times, and between the state
and the measurements, respectively. Now they represent the local linear approximation
of the nonlinear functions relating these quantities.
The update equations for the extended Kalman ﬁlter are
State Prediction:
µ+ = f[µt−1,0]
Covariance Prediction:
Σ+ = ΨΣt−1ΨT + ΥpΣpΥT
p
State Update:
µt = µ+ + K(xt −g[µ+,0])
Covariance Update:
Σt = (I −KΦ)Σ+,
(19.32)
where
K = Σ+ΦT (ΥmΣmΥT
m + ΦΣ+ΦT )−1.
(19.33)

19.4
Unscented Kalman ﬁlter
467
In the context of ﬁxed interval smoothing, the results can be improved by perform-
ing several passes back and forth through the data, relinearizing around the previous
estimates of the state in each sweep. This is called the iterated extended Kalman ﬁlter.
19.4
In conclusion, the extended Kalman ﬁlter is conceptually simple, but only copes
with relatively benign nonlinearities. It is a heuristic solution to the nonlinear tracking
problem and may diverge from the true solution.
19.3.1 Example
Figure 19.9 shows a worked example of the extended Kalman ﬁlter. In this case, the time
update model is nonlinear, but the observation model is still linear:
wt = f[wt−1,ϵp]
xt = w + ϵm,
(19.34)
where ϵp is a normal noise term with covariance Σp, ϵm is a normal noise term with
covariance Σm, and the nonlinear function f[•,•] is given by
f[w,ϵp] =

w1
w1 sin[w1] + ϵp

.
(19.35)
The Jacobian matrices for this model are easily computed. The matrices Υp, Φ, and
Υm are all equal to the identity. The Jacobian matrix Ψ is
Ψ =

1
0
sin[w1] + w1 cos[w1]
0

.
(19.36)
The results of using this system for tracking are illustrated in Figure 19.9.
The
extended Kalman ﬁlter successfully tracks this nonlinear model giving results that are
both closer to the true state and more conﬁdent than estimates based on the individ-
ual measurements alone. The EKF works well here because the nonlinear function is
smooth and departs from linearity slowly relative to the time steps: consequently, the
linear approximation is reasonable.
19.4
Unscented Kalman ﬁlter
The extended Kalman ﬁlter is only reliable if the linear approximations to the func-
tions f[•,•] or g[•,•] describe them well in the region of the current position. Figure
19.10 shows a situation where the local properties of the temporal function are not
representative and so the EKF estimate of the covariance is inaccurate.
The unscented Kalman ﬁlter (UKF) is a derivative-free approach that partially cir-
19.5 cumvents this problem. It is suited to nonlinear models with additive normally distributed
noise so that the temporal and measurement equations are
wt = f[wt−1] + ϵp
xt = g[wt] + ϵm.
(19.37)
To understand how the UKF works, consider a nonlinear temporal model.
Given the mean and covariance of the normally distributed posterior distribution

468
19 Temporal models
Figure 19.9 The extended Kalman ﬁlter. a) The temporal model that transforms the state (red
dots) here is nonlinear. Each observed data point (magenta dots) is a noisy copy of the state
at that time. Based on the data alone, the posterior probability over the state is given by the
magenta ellipses. b) Estimated marginal posterior distributions using the extended Kalman ﬁlter:
the estimates are more certain and more accurate than using the measurements alone. c) Close-
up of shaded region from (b). The EKF makes a normal prediction (dark blue ellipse) based on
the previous state (light blue ellipse) and the linearized motion model. The measurement makes
a different prediction (magenta ellipse). d) The EKF combines these two predictions to get an
improved estimate (light blue ellipse).
Pr(wt−1|x1...t−1) over the state at time t−1, we wish to predict a normally distributed
prior Pr(wt|x1...t−1) over the state at time t. We proceed by
• Approximating the normally distributed posterior Pr(wt−1|x1...t−1) with a set of
point masses that are deterministically chosen so that they have the same mean
µt−1 and covariance Σt−1 as the original distribution,
• Passing each of the point masses through the nonlinear function, and then
• Setting the predicted distribution Pr(wt|x1...t−1) at time t to be a normal
distribution with the mean and covariance of the transformed point masses.

19.4
Unscented Kalman ﬁlter
469
Figure 19.10 Problems with the EKF. A
2D temporal function f[•,•] operates on
the state (red lines indicate gradient direc-
tion). In the prediction step, the previous
estimate (blue ellipse) should be passed
through this function (to create distorted
green ellipse) and noise added (not shown).
The EKF approximates this by passing the
mean through the function and updating the
covariance based on a linear approxima-
tion of the function at the previous state
estimate. Here, the function changes quite
nonlinearly, and so this approximation is
bad, and the predicted covariance (magenta
ellipse) is inaccurate.
This process is illustrated in Figure 19.11.
As for the extended Kalman ﬁlter, the predicted state Pr(wt|x1...t−1) in the UKF is a
normal distribution. However, this normal distribution is a provably better approximation
to the true distribution than that provided by the EKF. A similar approach is applied to
cope with a nonlinearity in the measurement equations. We will now consider each of
these steps in more detail.
19.4.1 State evolution
As for the standard Kalman ﬁlter, the goal of the state evolution step is to form a pre-
diction Pr(wt|x1...t−1) about the state at time t by applying the temporal model to the
posterior distribution Pr(wt−1|x1...t−1) at the previous time step. This posterior distri-
bution is normal with mean µt−1 and covariance Σt−1, and the prediction will also be
normal with mean µ+ and covariance Σ+.
We proceed as follows. We approximate the marginal posterior at the previous time
step by a weighted sum of 2Dw+1 delta functions, where Dw is the dimensionality of
the state, so that
Pr(wt−1|x1...t−1) = Normwt−1[µt−1,Σt−1]
≈
2Dw
X
j=0
ajδ[wt−1 −ˆw[j]],
(19.38)
where the weights {aj}2Dw
j=0 are positive and sum to one. In this context, the delta func-
tions are referred to as sigma points. The positions { ˆw[j]}2Dw
j=0 of the sigma points are
carefully chosen so that
µt−1 =
2Dw
X
j=0
aj ˆw[j]
Σt−1 =
2Dw
X
j=0
aj( ˆw[j] −µt−1)( ˆw[j] −µt−1)T .
(19.39)

470
19 Temporal models
Figure 19.11 Temporal update for unscented Kalman ﬁlter.
a) In the temporal update step,
the posterior distribution from the previous time step (which is a normal) is approximated by a
weighted set of point masses that collectively have the same mean and covariance as this normal.
Each of these is passed through the temporal model (dashed lines). A normal prediction is formed
by estimating the mean and the covariance of the transformed point masses. In a real system, the
variance would subsequently be inﬂated to account for additive uncertainty in the position. b) The
resulting prediction is quite close to the true prediction and for this case is much better than that of
the extended Kalman ﬁlter (see Figure 19.10).
One possible scheme is to choose sigma points
ˆw[0] = µt−1
ˆw[j] = µt−1 +
r
Dw
1 −a0
Σ1/2
t−1ej
for all j ∈{1...Dw}
ˆw[Dw+j] = µt−1 −
r
Dw
1 −a0
Σ1/2
t−1ej
for all j ∈{1...Dw},
(19.40)
where ej is the unit vector in the jth direction. The associated weights are chosen so that
a0 ∈[0,1] and
aj = 1 −a0
2Dw
,
(19.41)
for all aj. The choice of weight a0 of the ﬁrst sigma point determines how far the
remaining sigma points are from the mean.
We pass the sigma points through the nonlinearity, to create a new set of samples
ˆw[j]
+ = f[ ˆw[j]] that collectively form a prediction for the state. We then compute the
mean and the variance of the predicted distribution Pr(wt|x1...t−1) from the mean and
variance of the transformed points so that
µ+ =
2Dw
X
j=0
aj ˆw[j]
+
Σ+ =
2Dw
X
j=0
aj( ˆw[j]
+ −µ+)( ˆw[j]
+ −µ+)T + Σp,
(19.42)

19.4
Unscented Kalman ﬁlter
471
where we have added an extra term Σp to the predicted covariance to account for the
additive noise in the temporal model.
19.4.2 Measurement incorporation
The measurement incorporation process in the UKF uses a similar idea: we approximate
the predicted distribution Pr(wt|x1...t−1) as a set of delta functions or sigma points
Pr(wt|x1...t−1) = Normwt−1[µ+,Σ+]
≈
2Dw
X
j=0
ajδ[wt −ˆw[j]],
(19.43)
where we choose the centers of the sigma points and the weights so that
µ+ =
2Dw
X
j=0
aj ˆw[j]
Σ+ =
2Dw
X
j=0
aj( ˆw[j] −µ+)( ˆw[j] −µ+)T .
(19.44)
For example, we could use the scheme outlined in Equations 19.40 and 19.41.
Then the sigma points are passed through the measurement model ˆx[j] = g[ ˆw[j]] to
create a new set of points {ˆx[j]}2Dw
j=0 in the measurement space. We compute the mean
and covariance of these predicted measurements using the relations
µx =
2Dw
X
j=0
ajˆx[j]
Σx =
2Dw
X
j=0
aj(ˆx[j] −µx)(ˆx[j] −µx)T + Σm.
(19.45)
The measurement incorporation equations are now
µt = µ+ + K(xt −µx)
Σt = Σ+ −KΣxKT ,
(19.46)
where the Kalman gain K is now redeﬁned as
K =


2Dw
X
j=0
aj( ˆw[j] −µ+)T (ˆx[j] −µx)T

Σ−1
x .
(19.47)
As for the prediction step, it can be shown that the UKF approximation is better than
that of the EKF.

472
19 Temporal models
19.5
Particle ﬁltering
The extended and unscented Kalman ﬁlters can partially cope with nonlinear temporal
and measurement models. However, they both represent the uncertainty over the state as
a normal distribution. They are hence ill-equipped to deal with situations where the true
probability distribution over the state is multimodal. Figure 19.12 illustrates a temporal
model that maps nearby states into two distinct regions. In this situation, neither the EKF
nor the UKF sufﬁce: the EKF models only one of the resulting clusters and the UKF
Figure 19.12 The need for particle ﬁltering. a) Consider this new temporal update function
(change of state with time indicated by red pointers), which bifurcates around the horizontal mid-
line. b) With the EKF, the predicted distribution after the time update is toward the bottom, as
the initial mean was below the bifurcation. The linear approximation is not good here and so the
covariance estimate is inaccurate. c) In the UKF, one of the sigma points that approximates the
prior is above the midline and so it is moved upward (green dashed line), whereas the others are
below the midline and are moved downward (other dashed lines). The estimated covariance is very
large. d) We can get an idea of the true predicted distribution by sampling the posterior at the pre-
vious time step and passing the samples through the nonlinear temporal model. It is clear that the
resulting distribution is bimodal and can never be well approximated with a normal distribution.
The particle ﬁlter represents the distribution in terms of particles throughout the tracking process
and so it can describe multimodal distributions like this.

19.5
Particle ﬁltering
473
tries to model both with a single normal model, assigning a large probability to the empty
region between the clusters.
Particle ﬁltering resolves this problem by representing the probability density as a
set of particles in the state space. Each particle can be thought of as representing a
hypothesis about the possible state. When the state is tightly constrained by the data, all
of these particles will lie close to one another. In more ambiguous cases, they will be
widely distributed or clustered into groups of competing hypotheses.
The particles can be evolved through time or projected down to simulate measure-
ments regardless of how nonlinear the functions are. The latter exercise leads us to
another nice property of the particle ﬁlter: since the state is multimodal, so are the
predicted measurements. In turn, the measurement density may also be multimodal.
In vision systems, this means that the system copes much better with clutter in the
scene (Figure 19.14). As long as some of the predicted measurements agree with the
measurement density, the tracker should remain stable.
One of the simplest particle ﬁlter methods is the conditional density propagation or
condensation algorithm. The probability distribution Pr(wt−1|x1...t−1) is represented
19.6 by a weighted sum of J weighted particles:
Pr(wt−1|x1...t−1) =
J
X
j=1
ajδ[wt−1 −ˆw[j]
t−1],
(19.48)
where the weights are positive and sum to one. Each particle represents a hypothesis
about the state and the weight of the particle indicates our conﬁdence in that hypothesis.
Our goal is to compute the probability distribution Pr(wt|x1...t) at the next time step,
which will be represented in a similar fashion. As usual this process is divided into a
time evolution and a measurement incorporation step, which we now consider in turn.
19.5.1 Time evolution
In the time evolution step, we create J predictions ˆw[j]
+ for the time-evolved state. Each
is represented by an unweighted particle. We create each prediction by re-sampling so
that we
• choose an index n ∈{1...J} of the original weighted particles where the proba-
bility is according to the weights. In other words, we draw a sample from Catn[a]
and
• draw the sample ˆw[j]
+ from the temporal update distribution Pr(wt|wt−1 = ˆw[n]
t−1)
(Equation 19.37).
In this process, the ﬁnal unweighted particles ˆw[j]
+ are created from the original
weighted particles ˆw[j]
t−1 according to the weights a = [a1,a2,...,aJ]. Hence, the highest
weighted original particles may contribute repeatedly to the ﬁnal set, and the lowest
weighted ones may not contribute at all.
19.5.2 Measurement incorporation
In the measurement incorporation step we weight the new set of particles according to
how well they agree with the observed data. To this end, we

474
19 Temporal models
Figure 19.13 The condensation algorithm. a) The posterior at the previous step is represented as
a set of weighted particles. b) The particles are re-sampled according to their weights to produce
a new set of unweighted particles. c) These particles are passed through the nonlinear temporal
function. d) Noise is added according to the temporal model. e) The particles are passed through
the measurement model and compared to the measurement density. f) The particles are reweighted
according to their compatibility with the measurements, and the process can begin again.

19.5
Particle ﬁltering
475
Figure 19.14 Data association. a) Consider the problem of tracking this contour. The contour
from the previous frame is shown in cyan, and the problem is to incorporate the measurements in
this frame. These measurements take the form of edges along one-dimensional slices perpendicular
to the contour. In most cases, there are multiple possible edges that might be due to the contour. b)
In the Kalman ﬁlter, the measurement density is constrained to be normal: we are forced to choose
between the different possible hypotheses. A sensible way to do this is to select the one that is
closest to the predicted measurement. This is known as data association. c) In the particle ﬁlter,
there are multiple predicted measurements and it is not necessary that the measurement density is
normal. We can effectively take into account all of the possible measurements.
• Pass the particles through the measurement model ˆx[j]
+ = g[ ˆw[j]
+ ].
• Weight the particles according to their agreement with the observation density. For
example, with a Gaussian measurement model, we could use
aj ∝Pr(xt| ˆw[j]
+ ) = Normxt[ˆx[j]
+ ,Σm].
(19.49)
• Normalize the resulting weights {aj}J
j=1 to sum to one.
• Finally, set the new states ˆw[j]
t
to the predicted states ˆw[j]
+ and the new weights
to aj.
Figure 19.13 demonstrates the action of the particle ﬁlter. The ﬁlter copes elegantly
with the multimodal probability distribution. It is also suitable for situations where it is
not obvious which aspect of the data is the true measurement. This is known as the data
association problem (Figure 19.14).
The main disadvantage of particle ﬁlters is their cost: in high dimensions, a very
large number of particles may be required to get an accurate representation of the true
distribution over the state.
19.5.3 Extensions
There are many variations and extensions to the particle ﬁlter. In many schemes, the par-
ticles are not resampled on each iteration, but only occasionally so the main scheme

476
19 Temporal models
operates with weighted particles.
The resampling process can also be improved by
applying importance sampling. Here, the new samples are generated in concert with
the measurement process, so that particles that agree with the measurements are more
likely to be produced. This helps prevent the situation where none of the unweighted
samples after the prediction stage agree with the measurements.
The process of Rao-Blackwellization partitions the state into two subsets of variables.
The ﬁrst subset is tracked using a particle ﬁlter, but the other subset is conditioned on
the ﬁrst subset and evaluated analytically using a process more like the Kalman ﬁlter.
Careful choice of these subsets can result in a tracking algorithm that is both efﬁcient
and accurate.
19.6
Applications
Tracking algorithms can be used in combination with any model that is being reesti-
mated over a time sequence. For example, they are often used in combination with 3D
body models to track the pose of a person in video footage. In this section, we will
describe three example applications. First we will consider tracking the 3D position of a
pedestrian in a scene. Second, we will describe simultaneous localization and mapping
(SLAM) in which both a 3D representation of a scene and the pose of the camera viewing
it are estimated through a time sequence. Finally, we will consider contour tracking for
an object with complex motion against a cluttered background.
19.6.1 Pedestrian Tracking
Rosales and Sclaroff (1999) described a system to track pedestrians in 2D from a ﬁxed
static camera using the extended Kalman ﬁlter. For each frame, the measurements x
consisted of a 2D bounding box x = {x1,y1,x2,y2} around the pedestrian. This was
found by segmenting the pedestrian from the background using a background subtraction
model, and ﬁnding a connected region of foreground pixels.
The state of the world w was considered to be the 3D position of a bounding box of
constant depth {u1,v1,u2,v2,w} and the velocities associated with these ﬁve quantities.
The state update equation assumed ﬁrst-order Newtonian dynamics in 3D space. The
relationship between the state and measurements was


x1
y1
x2
y2

=


u1
v1
u2
v2


1
1 + w + ϵ,
(19.50)
which mimics the nonlinear action of the pinhole camera.
Results from the tracking procedure are illustrated in Figure 19.15.
The system
successfully tracks pedestrians and was extended to cope with occlusions (which are
predicted by the EKF based on the estimated trajectory of the objects). In the full sys-
tem, the estimated bounding boxes are used to warp the images of the pedestrians to a
ﬁxed size, and the resulting registered images were used as input to an action recognition
algorithm

19.6
Applications
477
Figure 19.15 Pedestrian tracking results. a–f) Six frames from a sequence in which a 3D bound-
ing plane of constant depth was used to track each person. Adapted from Rosales and Sclaroff
(1999). c⃝1999 IEEE.
19.6.2 Monocular SLAM
The goal of simultaneous localization and mapping or SLAM is to build a map of an
unknown environment based on measurements from a moving sensor (usually attached
to a robot) and to establish where the sensor is in the world at any given time. Hence,
the system must simultaneously estimate the structure of the world and its own position
and orientation within that model. SLAM was originally based on range sensors but in
the last decade it has become possible to build systems based on a monocular camera
that work in real time. In essence it is a real-time version of the sparse 3D reconstruction
algorithms discussed in Chapter 16.
Davison et al. (2007) presented one such system which was based on the extended
Kalman ﬁlter. Here the world state w contains
• The camera position (u,v,w),
• The orientation of the camera as represented by a 4D quaternion q,
• The velocity and angular velocity, and
• A set of 3D points {pk} where pk = [puk,pvk,pwk]. This set generally expands
during the sequence as more parts of the world are seen.
The state update equation modiﬁes the position and orientation according to their
respective velocities, and allows these velocities themselves to change. The observa-
tion equation maps each 3D point through a pinhole camera model to create a predicted
2D image position (i.e., similar to Equation 19.50). The actual measurements consist
of interest points in the image, which are uniquely identiﬁed and associated with the
predicted measurements by considering their surrounding region. The system is very
efﬁcient as it is only necessary to search the image in regions close to the 2D point
predicted by the measurement equation from the current state.
The full system is more complex, and includes special procedures for initializing
new feature points, modeling the local region of each point as a plane, selecting which

478
19 Temporal models
Figure 19.16 Monocular SLAM model of Davison et al. (2007). a) Current view of world with
visual features and their 3D covariance ellipse superimposed. Yellow points are features in the
model that were actually measured. Blue points are features in the model that were not detected
in this frame. Red points are new features that can potentially be incorporated into the model. b)
Model of world at the time this frame was captured, showing the position of the camera and the
position and uncertainty of visual features. c–f) Two more pairs of images and associated models
from later in the same time sequence. Notice that the uncertainty in the feature positions gradually
decreases over time. Adapted from Davison et al. (2007). c⃝2007 IEEE.
features to measure in this frame, and managing the resulting map by deleting extraneous
features to reduce the complexity. A series of frames from the system is illustrated in
Figure 19.16.
19.6.3 Tracking a contour through clutter
In Section 17.3 we discussed ﬁtting a template shape model to an image. It was noted
that this was a challenging problem as the template frequently fell into local minima
when the object was observed in clutter. In principle, this problem should be easier in a
temporal sequence; if we know the template position in the current frame, then we can
hypothesize quite accurately where it is in the next frame. Nonetheless, tracking methods
based on the Kalman ﬁlter still fail to maintain a lock on the object; sooner or later, part
of the contour becomes erroneously associated with the background and the tracking
fails.
Blake and Isard (1998) describe a system based on the condensation algorithm that
can cope with tracking a contour undergoing rapid motions against a cluttered back-
ground. The state of the system consists of the parameters of an afﬁne transformation
that maps the template into the scene and the measurements are similar to those illus-
trated in Figure 19.14. The state is represented as 100 weighted particles, each of which
represents a different hypothesis about the current position of the template. This sys-
tem was used to track the head of a child dancing in front of a cluttered background
(Figure 19.17).

Notes
479
Figure 19.17 Tracking a contour through clutter. a) The representation of uncertainty about the
contour consists of a set of weighted particles, each of which represents a hypothesis about the
current position. In this frame, the different contours represent different hypotheses, and their
weight indicates our relative belief that the system takes this state. b) The overall estimate of the
current position can be illustrated by taking a weighted mean of the hypotheses. c–f) Four frames
of sequence tracked by condensation algorithm showing the current estimate and the estimates
from previous frames. Adapted from Blake and Isard (1998). c⃝1998 Springer.
Discussion
In this chapter we have discussed a family of models that allow us to estimate a set of
continuous parameters throughout a time sequence by exploiting the temporal coherence
of the estimates. In principle these methods can be applied to any model in this book that
estimates a set of parameters from a single frame. These models have a close relationship
with the chain-based models discussed in Chapter 11. The latter models used discrete
parameters and generally estimated the state from the whole sequence rather than just
the observed data up until the current time.
Notes
Applications of tracking: Tracking models are used for a variety of tasks in vision including track-
ing pedestrians (Rosales and Sclaroff 1999; Beymer and Konolige 1999), contours (Terzopolous
and Szeliski 1992; Blake et al. 1993; 1995; Blake and Isard 1996; 1998), points (Broida and Chel-
lappa 1986), 3D hand models (Stenger et al. 2001b), and 3D body models (Wang et al. 2008). They
have also been used for activity recognition (Vaswani et al. 2003), estimating depth (Matthies et al.
1989), SLAM (Davison et al. 2007), and object recognition (Zhou et al. 2004). Reviews of tracking
methods and applications can be found in Blake (2006) and Yilmaz et al. (2006). Approaches to
tracking the human body are reviewed in Poppe (2007).
Tracking models: The Kalman ﬁlter was originally developed by Kalman (1960) and Kalman and
Bucy (1961). The unscented Kalman ﬁlter was developed by Julier and Uhlmann (1997). The
condensation algorithm is due to Blake and Isard (1996). For more information about the Kalman
ﬁlter and its variants, consult Maybeck (1990), Gelb (1974), and Jazwinski (1970). Roweis and

480
19 Temporal models
Ghahramani (1999) provide a unifying review of linear models, which provides information about
how the Kalman ﬁlter relates to other methods. Arulampalam et al. (2002) provide a detailed
review of the use of particle ﬁlters. A summary of tracking models and different approaches to
inference within them can be found in Minka (2002).
There are a large number of variants on the standard Kalman ﬁlter. Many of these involve switch-
ing between different state space models or propagating mixtures (Shumway and Stoffer 1991;
Ghahramani and Hinton 1996b; Murphy 1998; Chen and Liu 2000; Isard and Blake 1998). A
notable recent extension has been to develop a nonlinear tracking algorithm based on the GPLVM
(Wang et al. 2008).
One topic that has not been discussed in this chapter is how to learn the parameters of the tracking
models. In practice, it is not uncommon to set these parameters by hand. However, information
about learning in these temporal models can be found in Shumway and Stoffer (1982), Ghahramani
and Hinton (1996a), Roweis and Ghahramani (2001), and Oh et al. (2005).
Simultaneous localization and mapping: Simultaneous localization and mapping has its roots
in the robotics community who were concerned with the representation of spatial uncertainty by
vehicles exploring an environment (Durrant-Whyte 1988; Smith and Cheeseman 1987), although
the term SLAM was coined much later by Durrant-Whyte et al. (1996). Smith et al. (1990) had
the important insight that the errors in mapped positions were correlated due to uncertainty in the
camera position. The roots of vision-based SLAM are to be found in the pioneering work of Harris
and Pike (1987), Ayache (1991), and Beardsley et al. (1995).
SLAM systems are usually based on either the extended Kalman ﬁlter (Guivant and Nebot 2001;
Leonard and Feder 2000; Davison et al. 2007) or a Rao-Blackwellized particle ﬁlter (Montemerlo
et al. 2002; Montemerlo et al. 2003; Sim et al. 2005). However, it currently the subject of some
debate as to whether a tracking method of this type is necessary at all, or whether repeated bundle
adjustments on tactically chosen subsets of 3D points will sufﬁce (Strasdat et al. 2010).
Recent examples of efﬁcient visual SLAM systems can be found in Nist´er et al. (2004), Davison
et al. (2007), Klein and Murray (2007), Mei et al. (2009), and Newcombe et al. (2011), and most of
these include a bundle adjustment procedure into the algorithm. Current research issues in SLAM
include how to efﬁciently match features in the image with features in the current model (Handa
et al. 2010) and how to close loops (i.e., recognize that the robot has returned to a familiar place)
in a map (Newman and Ho 2005). A review of SLAM techniques can be found in Durrant-Whyte
and Bailey (2006) and Bailey and Durrant-Whyte (2006).
Problems
19.1 Prove the Chapman–Kolmogorov relation:
Pr(wt|x1...t−1) =
Z
Pr(wt|wt−1)Pr(wt−1|x1...t−1) dwt−1
=
Z
Normwt[µp + Ψwt−1,Σp]Normwt−1[µt−1,Σt−1] dwt−1
= Normwt[µp + Ψµt−1,Σp + ΨΣt−1ΨT ].

Problems
481
19.2 Derive the measurement incorporation step for the Kalman ﬁlter. In other words, show that
Pr(wt|x1...t) = Pr(xt|wt)Pr(wt|w1...t−1,x1...t)
Pr(x1...t)
= Normxt[µm + Φwt,Σm]Normwt[µ+,Σ+]
Pr(x1...t)
= Normwt

ΦT Σ−1
m Φ + Σ−1
+
−1 
ΦT Σ−1
m (xt−µm) + Σ−1
+ µ+

,

ΦT Σ−1
m Φ + Σ−1
+
−1
.
19.3 Consider a variation of the Kalman ﬁlter where the prior based on the previous time step is a
mixture of K Gaussians
Pr(wt|x1...t−1) =
K
X
k=1
Normwt[µ+k,Σ+k].
What will happen in the subsequent measurement incorporation step? What will happen in the next
time update step?
19.4 Consider a model where there are two possible temporal update equations represented by
state transition matrices Ψ1 and Ψ2 and the system periodically switches from one regime to the
other. Write a set of equations that describe this model and discuss a strategy for max-marginals
inference.
19.5 In a Kalman ﬁlter model, discuss how you would compute the joint posterior distribution
Pr(w1...T |x1...T ) over all of the unknown world states. What form will this posterior distribution
take? In the Kalman ﬁlter we choose to compute the max-marginals solution instead. Why is this?
19.6 Apply the sum-product algorithm (Section 11.4.3) to the Kalman ﬁlter model and show that
the result is equivalent to applying the Kalman ﬁlter recursions.
19.7 Prove the Kalman smoother recursions:
µt|T = µt + Ct(µt+1|T −µ+|t)
Σt|T = Σt + Ct(Σt+1|T −Σ+|t)CT
t ,
where
Ct = Σt|tΨT Σ−1
+|t.
Hint: It may help to examine the proof of the forward-backward algorithm for HMMs (Section
11.4.2).
19.8 Discuss how you would learn the parameters of the Kalman ﬁlter model given training
sequences consisting of (i) both the known world state and the observed data and (ii) just the
observed data alone.
19.9 In the unscented Kalman ﬁlter we represented a Gaussian with mean µ and covariance Σ
with a set of delta functions
ˆw[0] = µt−1
ˆw[j] = µt−1 +
r
Dw
1 −a0 Σ1/2
t−1ej
for all j ∈{1...Dw}
ˆw[Dw+j] = µt−1 −
r
Dw
1 −a0 Σ1/2
t−1ej
for all j ∈{1...Dw},

482
19 Temporal models
with associated weights
aj = 1 −a0
2Dw .
Show that the mean and covariance of these points are indeed µt−1 and Σt−1 so that
µt−1 =
2Dw
X
j=0
aj ˆw[j]
Σt−1 =
2Dw
X
j=0
aj( ˆw[j] −µt−1)( ˆw[j] −µt−1)T .
19.10 The extended Kalman ﬁlter requires the Jacobian matrix describing how small changes
in the data create small changes in the measurements.
Compute the Jacobian matrix for the
measurement model for the pedestrian-tracking application (Equation 19.50).

Chapter 20
Models for visual words
In most of the models in this book, the observed data are treated as continuous. Hence, for
generative models the data likelihood is usually based on the normal distribution. In this
chapter, we explore generative models that treat the observed data as discrete. The data
likelihoods are now based on the categorical distribution; they describe the probability of
observing the different possible values of the discrete variable.
As a motivating example for the models in this chapter, consider the problem of
scene classiﬁcation (Figure 20.1). We are given example training images of different
scene categories (e.g., ofﬁce, coastline, forest, mountain) and we are asked to learn a
model that can classify new examples. Studying the scenes in Figure 20.1 demonstrates
how challenging a problem this is. Different images of the same scene may have very
little in common with one another, yet we must somehow learn to identify them as the
same. In this chapter, we will also discuss object recognition, which has many of the
same characteristics; the appearance of an object such as a tree, bicycle, or chair can vary
dramatically from one image to another, and we must somehow capture this variation.
The key to modeling these complex scenes is to encode the image as a collection of
visual words, and use the frequencies with which these words occur as the substrate for
further calculations. We start this chapter by describing this transformation.
20.1
Images as collections of visual words
To encode an image in terms of visual words, we need ﬁrst to establish a dictionary. This
is computed from a large set of training images that are unlabeled, but known to contain
examples of all of the scenes or objects that will ultimately be classiﬁed. To compute the
dictionary, we take the following steps:
1. For every one of the I training images, select a set of Ji spatial locations. One
possibility is to identify interest points (Section 13.2) in the image. Alternately,
the image can be sampled in a regular grid.
2. Compute a descriptor at each spatial location in each image that characterizes the
surrounding region with a low dimensional vector. For example, we might compute
the SIFT descriptor (Section 13.3.2).
3. Cluster all of these descriptor vectors into K groups using a method such as the
K-means algorithm (Section 13.4.4).
4. The means of the K clusters are used as the K prototype vectors in the dictionary.

484
20 Models for visual words
Figure 20.1 Scene recognition. The goal of scene recognition is to assign a discrete category to
an image according to the type or content. In this case, the data includes images of a) street scenes,
b) the sea, and c) forests. Scene recognition is a useful precursor to object recognition; if we know
that the scene is a street, then the probability of a car being present is high, but the probability of
a boat being present is small. Unfortunately, scene recognition is quite a challenging task in itself.
Different examples from the same scene class may have very little in common visually.
Typically, several hundred thousand descriptors would be used to compute the dictionary,
which might consist of several hundred prototype words.
Having computed the dictionary, we are now in a position to take a new image and
convert it into a set of visual words. To compute the visual words, we take the following
steps:
1. Select a set of J spatial locations in the image using the same method as for the
dictionary.
2. Compute the descriptor at each of the J spatial locations.
3. Compare each descriptor to the set of K prototype descriptors in the dictionary
and ﬁnd the closest prototype (visual word).
4. Assign to this location a discrete index that corresponds to the index of the closest
word in the dictionary.
After computing the visual words, the data x from a single image consist of a set
x = {fj,xj,yj}J
j=1 of J word indices fj ∈{1...K} and their 2D image positions
(xj,yj). This is a highly compressed representation that nonetheless contains the critical
information about the image appearance and layout. In the remaining part of the chapter,
we will develop a series of generative models that attempt to explain the pattern of this
data when different objects or scenes are present.
20.2
Bag of words
One of the simplest possible representations for an image in terms of visual words is the
20.1 bag of words. Here we entirely discard the spatial information held in the word positions

20.2
Bag of words
485
Figure 20.2 Scene recognition using bags of words. a) A set of interest points is found in this
desert scene and a descriptor is calculated at each. These descriptors are compared to a dictionary
containing K prototypes and the index of the nearest prototype is chosen (red numbers). Here
K =9, but in real applications it might be several hundred. b) The scene-type “desert” implies a
certain distribution over the observed visual words. c) A second image containing a jungle scene,
and the associated visual words. d) The scene type “jungle” implies a different distribution over
the visual words. A new image can be classiﬁed as belonging to one scene type or another by
assessing the likelihood that the observed visual words were drawn from the “desert” or “jungle”
distribution.
(xj,yj) and just retain the word indices fj, so that the observed data are x = {fj}J
j=1.
The image is simply represented by the frequency with which each word appears. It is
assumed that different types of object or scene will tend to contain different words and
that this can be exploited to perform scene or object recognition (Figure 20.2).
More formally, the goal is to infer a discrete variable w ∈{1,2,...,N} indicating
which of N classes is present in this image. We take a generative approach and model
each of the N classes separately. Since the data {fj}J
j=1 are discrete, we describe its
probability with a categorical distribution and make the parameters λ of this distribution
a function of the discrete world state:
Pr(x|w = n) =
J
Y
j=1
Catfj[λn]
=
K
Y
k=1
λTk
kn,
(20.1)
where Tk is the total number of times that the kth word was observed, so that
Tk =
J
X
j=1
δ[fj −k].
(20.2)
We will now consider the learning and inference algorithms for this model.

486
20 Models for visual words
Figure 20.3 Object recognition using bags of words. Csurka et al. (2004) built a generative bag
of visual words model to distinguish between examples of a) books, b) bicycles, c) people, d)
buildings, e) cars, f) trees, and g) phones. Despite the wide variety of visual appearance within
each class, they achieved 72 percent correct classiﬁcation. By applying a discriminative approach
to the same problem, they managed to improve performance further.
20.2.1 Learning
In learning, our goal is to estimate the parameters {λn}N
n=1 based on labeled pairs
{xi,wi} of the observed data xi ={fij}Ji
j=1 and the world state wi. We note that the
nth parameter vector λn is used only when the world state wi = n. Hence, we can
learn each parameter vector separately; we learn the parameter λn from the subset Sn of
training images where wi = n.
Making use of the results in Section 4.5, we see that if we apply a Dirichlet prior
with uniform parameter α = [α,α,...,α], then the MAP estimate of the categorical
parameters is given by
ˆλnk =
P
i∈Sn Tik + α −1
PK
k=1(P
i∈Sn Tik + α −1)
,
(20.3)
where λnk is the kth entry in the categorical distribution for the nth class and Tik is the
total number of times that word k was observed in the ith training image.
20.2.2 Inference
To infer the world state, we apply Bayes’ rule:
Pr(w = n|x) =
Pr(x|w = n)Pr(w = n)
PN
n=1 Pr(x|w = n)Pr(w = n)
,
(20.4)
where we allocate suitable prior probabilities Pr(w = n) according to the relative
frequencies with which each world type is present.
Discussion
Despite discarding all spatial information, the bag of words model works remarkably
well for object recognition. For example, Csurka et al. (2004) achieved 72 percent correct
performance at classifying images of the seven classes found in Figure 20.3. It should

20.3
Latent Dirichlet allocation
487
Figure 20.4 Problems with bag of words model. a) The bag of words model is quite effective
for object and scene recognition, but it can be improved upon by b) modeling the cooccurence of
visual words (creating the latent Dirichlet allocation model). c) This model can be extended to
describe the relative positions of different parts of the object (creating a constellation model) and
d) extended again to describe the relative position of objects in the scene (creating a scene model).
be noted that it is possible to improve performance further by treating the vector z =
[T1,T2,...,TK]/P
k Tk of normalized word frequencies as continuous and subjecting it
to a kernelized discriminative classiﬁer (see Chapter 9). Regardless, we will continue to
investigate the (more theoretically interesting) generative approach.
20.2.3 Problems with the bag of words model
There are a number of drawbacks to the generative bag of words model:
• It assumes that the words are generated independently, within a given object class
although this is not necessarily true. The presence of a particular visual word tells
us about the likelihood of observing other words.
• It ignores spatial information; consequently, when applied to object recognition, it
cannot tell us where the object is in the image.
• It is unsuited to describing multiple objects in a single image.
We devote the remaining part of this chapter to building a series of generative models
that improve on these weaknesses (Figure 20.4).
20.3
Latent Dirichlet allocation
We will now develop an intermediate model known as latent Dirichlet allocation. This
20.2 model has limited utility for visual applications in its most basic form, but it underpins
more interesting models that are discussed subsequently.
There are two important differences between the bag of words and latent Dirich-
let allocation models. First, the bag of words model describes the relative frequency
of visual words in a single image, whereas latent Dirichlet allocation describes the
occurrence of visual words across a number of images. Second, the bag of words model

488
20 Models for visual words
assumes that each word in the image is generated completely independently; having
observed word fi1, we are none the wiser about word fi2. However, in the latent Dirichlet
allocation model, a hidden variable associated with each image induces a more complex
distribution over the word frequencies.
Latent Dirichlet allocation can be best understood by analogy to text documents.
Each document is considered as a certain mixture of topics. For example, this book might
contain the topics “machine learning”, “vision,” and “computer science” in proportions
of 0.3, 0.5, and 0.2, respectively. Each topic deﬁnes a probability distribution over words;
the words “image” and “pixel” might be more probable under the topic of vision, and the
words “algorithm” and “complexity” might be more probable under the topic of computer
science.
To generate a word, we ﬁrst choose a topic according to the topic probabilities for
the current document. Then we choose a word according to a distribution that depends
on the chosen topic. Notice how this model induces correlations between the probability
of observing different words. For example, if we see the word “image”, then this implies
that the topic “vision” has a signiﬁcant probability and hence observing the word “pixel”
becomes more likely.
Now let us convert these ideas back to the vision domain. The document becomes an
image, and the words become visual words. The topic does not have an absolutely clear
interpretation, but we will refer to it as a part. It is a cluster of visual words that tend to
co-occur in images. They may or may not be spatially close to one another in the image,
and they may or may not correspond to an actual “part” of an object (Figure 20.5).
Formally, the model represents the words in an image as a mixture of categorical dis-
tributions. The mixing weights depend on the image, but the parameters of the categorical
distributions are shared across all of the images:
Pr(pij) = Catpij[πi]
Pr(fij|pij) = Catfij[λpij],
(20.5)
Figure 20.5 Latent Dirichlet allocation. This model treats each word as belonging to one of M
different parts (here M = 2). a) The distribution over the words given that the part is 1 is described
by a categorical distribution. b) The distribution over the words given that the part is 2 is described
by a different categorical distribution. c) In each image, the tendency for the observed words to
belong to each part is different. In this case, part 1 is more likely than part 2 and so most of the
words belong to part 1 (are red as opposed to green).

20.3
Latent Dirichlet allocation
489
where i indexes the image and j indexes the word. The ﬁrst equation says that the
part label pij ∈{1,2,...,M} associated with the jth word in the ith image is drawn
from a categorical distribution with parameters πi that are unique to this image. The
second equation says that the actual choice of visual word fij is a categorical distribution
where the parameters λpij depend on the part. For short, we will refer to {πi}I
i=1 and
{λm}M
m=1 as the part probabilities and the word probabilities, respectively.
The ﬁnal density over the words comes from marginalizing over the part labels, which
are hidden variables, so that
Pr(fij) =
M
X
m=1
Pr(fij|pij = m)Pr(pij = m).
(20.6)
To complete the model, we deﬁne priors on the parameters {πi}I
i=1, {λm}M
m=1, where
I is the number of images and M is the total number of parts. In each case, we choose
the conjugate Dirichlet prior with a uniform parameter vector so that
Pr(πi) = Dirπi[α]
Pr(λm) = Dirλm[β],
(20.7)
where α = [α,α,...,α] and β = [β,β,...,β]. The associated graphical model is shown
in Figure 20.6.
Notice that latent Dirichlet allocation is a density model for the data in a set of images.
It does not involve a “world” term that we wish to infer. In the subsequent models, we
will reintroduce the world term and use the model for inference in visual problems. How-
ever, for now we will concentrate on how to learn the relatively simple latent Dirichlet
allocation model.
Figure 20.6 Graphical model for latent Dirichlet allocation. The likelihood of the jth word fij in
the ith image taking each of the K different values depends on which of M parts it belongs to, and
this is determined by the associated part label pij. The tendency of the part label to take different
values is different for each image and is determined by the parameters πi. The hyperparameters α
and β determine the Dirichlet priors over the part probabilities and word probabilities, respectively.

490
20 Models for visual words
20.3.1 Learning
In learning, the goal is to estimate the part probabilities {πi}I
i=1 for each of the I training
images and the word probabilities for each of the M parts {λm}M
m=1 based on a set of
training data {fij}I,Ji
i=1,j=1, where Ji denotes the number of words found in the ith image.
If we knew the values of the hidden part labels {pij}I,Ji
i=1,j=1, then it would be easy
to learn the unknown parameters.
Adopting the approach of Section 4.4, the exact
expressions would be
ˆπim =
P
j δ[pij −m] + α
P
j,m δ[pij −m] + Mα
ˆλmk =
P
i,j δ[pij −m]δ[fij −k] + β
P
i,j,k δ[pij −m]δ[fij −k] + Kβ .
(20.8)
Unfortunately, we do not know these part labels and so we cannot use this direct
technique. One possible approach would be to adopt the EM algorithm in which we
alternately compute the posterior distribution over the part labels and update the param-
eters. Unfortunately, this is also problematic; all of the part labels {pij}Ji
j=1 in the ith
image share a parent πi in the graphical model. This means we cannot treat them as
independent. In theory, we could compute their joint posterior distribution, but there
may be several hundred words per image, each of which takes several hundred values
and so this is not practical.
Hence, our strategy will be to:
• Write an expression for the posterior distribution over the part labels,
• Develop an MCMC method to draw samples from this distribution, and then
• use the samples to estimate the parameters.
These three steps are expanded upon in the next three sections.
Posterior distribution over part labels
The posterior distribution over the part labels p = {pij}I,Ji
i=1,j=1 results from applying
Bayes’ rule:
Pr(p|f) =
Pr(f|p)Pr(p)
P
f Pr(f|p)Pr(p),
(20.9)
where f = {fij}I,Ji
i=1,j=1 denotes the observed words.
The two terms in the numerator depend on the word probabilities {λm}M
m=1 and the
part probabilities {πi}I
i=1, respectively. However, since each of these quantities has a
conjugate prior, we can marginalize over them and remove them from the computation
entirely. Hence, the likelihood Pr(f|p) can be written as
Pr(f|p) =
Z
IY
i=1
Ji
Y
j=1
Pr(fij|pij,λ1...M)Pr(λ1...M) dλ1...M
(20.10)
=
Γ[Kβ]
Γ[β]K
M
M
Y
m=1
QK
k=1 Γ
hP
i,j δ[fij −k]δ[pij −m] + β
i
Γ
hP
i,j,k δ[fij −k]δ[pij −m] + Kβ
i ,

20.3
Latent Dirichlet allocation
491
and the prior can be written as
Pr(p) =
IY
i=1
Z
Ji
Y
j=1
Pr(pij|πi)Pr(πi) dπi
(20.11)
=
Γ[Mα]
Γ[α]M
I
IY
i=1
QM
m=1 Γ
hP
j δ[pij −m] + α
i
Γ
hP
j,m δ[pij −m] + Mα
i ,
where we exploited conjugate relations to help solve the integral (see Problem 3.10).
Unfortunately, we cannot compute the denominator of Equation 20.9 as this involves
summing over every possible assignment of the word labels f. Consequently, we can
only compute the posterior probability for part labels p up to an unknown scaling factor.
We encountered a similar situation before in the MRF labeling problem (Chapter 12). In
that case there was a polynomial time algorithm to ﬁnd the MAP estimate, but here that
is not possible; the cost function for this problem cannot be expressed as a sum of unary
and pairwise terms.
Drawing samples from posterior distribution
To make progress, we will use a Monte Carlo Markov chain method to generate a set
of samples {p[1]p[2],...,p[T ]} from the posterior distribution. More speciﬁcally, we
will use a Gibbs sampling approach (see Section 10.7.2) in which we update each part
label pij in turn. To do this, we compute the posterior probability of the current part label
assuming that all of the others are ﬁxed and then draw a sample from this distribution. We
repeat this for every part label to generate a new sample of p. This posterior probability of
a single part label assuming that the others are ﬁxed has M elements that are computed as
Pr(pij = m|p\ij,f) =
Pr(pij = m,p\ij,f)
PM
m=1 Pr(pij = m,p\ij,f)
,
(20.12)
where the notation p\ij denotes all of the elements of p except pij. To estimate this, we
must compute joint probabilities Pr(f,p) = Pr(f|p)Pr(p) using Equations 20.10 and
20.11. In practice, the resulting expression simpliﬁes considerably to
Pr(pij = m|p\ij,f)∝
 
P
a,b\i,j δ[fab −fij]δ[pab −m] + β
P
k
P
a,b\i,j δ[fab −k]δ[pab −m] + Kβ
!
(20.13)
×
 
P
b\j δ[pib −m] + α
P
m
P
b\j δ[pib −m] + Mα
!
,
where the notation P
a,b\i,j means sum over all values of {a,b} except i,j. Although
it looks rather complex, this expression has a simple interpretation. The ﬁrst term is the
probability of observing the word fij given that part pij = m. The second term is the
proportion of the time that part m is present in the current document.
To sample from the posterior distribution, we initialize the part labels {pij}I,Ji
i=1,j=1
and alternately update each part label in turn. After a reasonable burn in period (several
thousand iterations over all of the variables), the resulting samples can be assumed to be
drawn from the posterior. We then take a subset of samples from this chain, where each
is separated by a reasonable distance to ensure that their correlation is low.

492
20 Models for visual words
Figure 20.7 Single author–topic model. The single author–topic model is a variant of latent
Dirichlet allocation that includes a variable wi ∈{1...N} that represents which of N possible
objects is in the image. It is assumed that the part probabilities {πn}N
n=1 are contingent on the
particular choice of object. a) Image 1 contains a motorbike and this induces the part probabilities
shown in the bottom right hand corner. The parts are drawn from this probability distribution (color
of crosses) and the words (numbers) are drawn based on the parts chosen. b) A second image of
a motorbike induces the same part probabilities. c–d) These two images contain a different object
and hence have different part probabilities (bottom right).
Using samples to estimate parameters
Finally, we estimate the unknown parameters using the expressions
ˆπim =
P
t,j δ[p[t]
ij −m] + α
P
t,j,m δ[p[t]
ij −m] + Mα
ˆλmk =
P
t,i,j δ[p[t]
ij −m]δ[fij −k] + β
P
t,i,j,k δ[p[t]
ij −m]δ[fij −k] + Kβ
,
(20.14)
which are very similar to the original expressions (Equation 20.8) for estimating the
parameters given known part labels.
20.3.2 Unsupervised object discovery
The preceding model can been used to help analyze the structure of a set of images.
Consider ﬁtting this model to an unlabeled data set containing several images each of a
number of different object categories. After ﬁtting, each of the I images is modeled as a
mixture of parts, and we have an estimate of the mixture weights λi for each. We now

20.4
Single author–topic model
493
Figure 20.8 Graphical model for the single author–topic model. The likelihood of the jth word
in the ith image fij being categorized as one word or another depends on which of M parts it
belongs to, and this is determined by the associated part label pij. The tendency of the part label
to take different values is different for each object wi ∈{1...N} is determined by the parameters
πn, where it is assumed there is a single object in each image.
cluster the images according to the dominant part in this mixtures. For small data sets,
it has been shown that this method can separate out different object classes with a high
degree of accuracy; this model allows the discovery of object classes in unlabeled data
sets.
20.4
Single author–topic model
Latent Dirichlet allocation is simply a density model for images containing sets of dis-
crete words. We will now describe an extension to this model that assumes there is a
single object in each image, and the identity of this object is characterized by a label
wi ∈{1...N}. In the single author–topic model (Figure 20.7), we make the assumption
that each image of the same object contains the same part probabilities:
Pr(pij|wi = n) = Catpij[πn]
Pr(fij|pij) = Catfij[λpij].
(20.15)
To complete the model, we add Dirichlet priors to the unknown parameters {πn}N
n=1 and
{λm}M
m=1:
Pr(πn) = Dirπn[α]
Pr(λm) = Dirλm[β],
(20.16)
where α = [α,α,...,α] and β = [β,β,...,β]. For simplicity we will assume that the prior
over the object label w is uniform and not discuss this further. The associated graphical
model is illustrated in Figure 20.8.
Like the bag of words and latent Dirichlet allocation models, this model was origi-
nally used for describing text documents; it assumes that each document was written by
one author (each image contains one object), and this determines the relative frequency
of topics (of parts). It is a special case of the more general author–topic model, which
allows multiple authors for each document.

494
20 Models for visual words
20.4.1 Learning
Learning proceeds in much the same way as in latent Dirichlet allocation. We are given
a set of I images, each of which has a known object label wi ∈{1...N} and a set
of visual words {fij}Ji
j=1, where fij ∈{1...M}. It would be easy to estimate the part
probabilities for each object {πn}N
n=1 and the word probabilities for each part {λm}M
m=1
if we knew the hidden part labels pij associated with each word. As before we take the
approach of drawing samples from the posterior distribution over the part labels and
using these to estimate the unknown parameters. This posterior is computed via Bayes’
rule:
Pr(p|f,w) =
Pr(f|p)Pr(p|w)
P
f Pr(f|p)Pr(p|w),
(20.17)
where w = {wi}I
i=1 contains all of the object labels.
The likelihood term Pr(f|p) is the same as for latent Dirichlet allocation and is given
by Equation 20.10. The prior term becomes
Pr(p|w) =
Z
IY
i=1
Ji
Y
j=1
Pr(pij|wi,π1...N)Pr(π1...N) dπ1...N
(20.18)
=
Γ[Mα]
Γ[α]M
N
N
Y
n=1
QM
m=1 Γ
hP
i,j δ[pij −m]δ[wi −n] + α
i
Γ
hP
i,j,m δ[pij −m]δ[wi −n] + Mα
i .
As before, we cannot compute the denominator of Bayes’ rule as it involves an intractable
summation over all possible words. Hence, we use a Gibbs sampling method in which
we repeatedly draw samples p[1] ...p[T ] from each marginal posterior in turn using the
relation:
Pr(pij = m|p\ij,f,wi = n)∝
 
P
a,b\i,j δ[fab −fij]δ[pab −m] + β
P
k
P
a,b\i,j δ[fab −k]δ[pab −m] + Kβ
!
(20.19)
×
 
P
a,b\i,j δ[pab −m]δ[wi −n] + α
P
m
P
a,b\i,j δ[pab −m]δ[wi −n] + Mα
!
,
where the notation P
a,b\i,j denotes a sum over all valid values of a,b except for the com-
bination i,j. This expression has a simple interpretation. The ﬁrst term is the probability
of observing the word fij given that part pij = m. The second term is the proportion of
the time that part m is present for the nth object.

20.5
Constellation models
495
Finally, we estimate the unknown parameters using the relations:
ˆπnm =
P
t,i,j δ[p[t]
ij −m]δ[wi −n] + α
P
t,i,j,m δ[p[t]
ij −m]δ[wi −n] + Mα
ˆλmk =
P
t,i,j δ[p[t]
ij −m]δ[fij −k] + β
P
t,i,j,k δ[p[t]
ij −m]δ[fij −k] + Kβ
.
(20.20)
20.4.2 Inference
In inference, we compute the likelihood of new image data f = {fj}J
j=1 under each
possible object w ∈{1...N} using
Pr(f|w = n) =
J
Y
j=1
M
X
pj=1
Pr(pj|w = n)Pr(fj|pj)
=
J
Y
j=1
M
X
pj=1
Catpj[πn]Catfj[λpj].
(20.21)
We now deﬁne suitable priors Pr(w) over the possible objects and use Bayes’ rule
to compute the posterior distribution,
Pr(w = n|f) =
Pr(f|w = n)Pr(w = n)
PN
n=1 Pr(f|w = n)Pr(w = n)
.
(20.22)
20.5
Constellation models
The single author–topic model described in Section 20.4 is still a very weak description
of an object as it contains no spatial information. Constellation models are a general class
of model that describe objects in terms of a set of parts and their spatial relations. For
example, the pictorial structures model described in Section 11.8.3 can be considered a
constellation model. Here, we will develop a different type of constellation model that
extends the latent Dirichlet allocation model (Figure 20.9).
We assume that a part retains the same meaning as before; it is a cluster of
co-occurring words.
However, each part now induces a spatial distribution over its
associated words, which we will model with a 2D normal distribution so that
Pr(pij|wi = n) = Catpij[πn]
Pr(fij|pij = m) = Catfij[λm]
Pr(xij|pij = m) = Normxij[µm,Σm],
(20.23)
where xij = [xij,yij]T is the two-dimensional position of the jth word in the ith image.
As before, we also deﬁne Dirichlet priors over the unknown categorical parameters so
that
Pr(πn) = Dirπn[α]
Pr(λm) = Dirλm[β],
(20.24)

496
20 Models for visual words
Figure 20.9 Constellation model. In the
constellation model the object or scene
is again described as consisting of set of
different parts (colors).
A number of
words are associated with each part, and
the word probabilities depend on the part
label.
However, unlike in the previous
models, each part now has a particular
range of locations associated with it, which
are described as a normal distribution. In
this sense it conforms more closely to the
normal use of the English word “part.”
Figure 20.10 Constellation model.
In
addition to all of the other variables in the
single author–topic model (compare to Fig-
ure 20.8), the position xij of the jth word
in the ith image is also modeled.
This
position is contingent on which of the M
parts that the current word is assigned to
(determined by the variable pij). When the
word is assigned to the mth part, the posi-
tion xij is modeled as being drawn from a
normal distribution with mean and variance
µm,Σm.
where α = [α,α,...,α] and β = [β,β,...,β]. The associated graphical model is shown
in Figure 20.10.
This model extends latent Dirichlet allocation to allow it to represent the relative
positions of parts of an object or scene. For example, it might learn that words associated
with trees usually occur in the center of the image and that those associated with the sky
usually occur near the top of the image.
20.5.1 Learning
As for latent Dirichlet allocation, the model would be easy to learn if we knew the part
assignments p = {pij}I,Ji
i=1,j=1. By the same logic as before, we hence draw samples
from posterior distribution Pr(p|f,X,w) over the part assignments given the observed
word labels f = {fij}I,Ji
,j=1, their associated positions X = {xij}I,Ji
i=1,j=1, and the known
object labels w = {wi}I
i=1. The expression for the posterior is computed via Bayes’ rule
Pr(p|f,X,w) =
Pr(f,X|p)Pr(p|w)
P
p Pr(f,X|p)Pr(p|w),
(20.25)
and once again, the terms in the numerator can be computed, but the denominator con-
tains an intractable sum of exponentially many terms. This means that the posterior

20.5
Constellation models
497
cannot be computed in closed form, but we can still evaluate the posterior probability
for any particular assignment p up to an unknown scale factor. This is sufﬁcient to draw
samples from the distribution using Gibbs sampling.
The prior probability Pr(p|w) of the part assignments is the same as before and is
given in Equation 20.18. However, the likelihood term Pr(f,X|p) now has an additional
component due to the requirement for the word position xij to agree with the normal
distribution induced by the part:
Pr(f,X|p)
(20.26)
=
Z
IY
i=1
Ji
Y
j=1
Pr(fij|pij,λ1...M)Pr(λ1...M)Pr(xij|pij,µ1...M,Σ1...M) dλ1...M
=
Γ[Kβ]
Γ[β]K
M
M
Y
m=1
QK
k=1 Γ
hP
i,j δ[pij −m]δ[fij −k] + β
i
Γ
hP
i,j,k δ[pij −m]δ[fij −k] + Kβ
i Normxij[µpij,Σpij].
In Gibbs sampling, we choose one data example {fij,xij} and draw from the poste-
rior distribution assuming that all of the other parts are ﬁxed. An approximate1 expression
to compute this posterior is given by
Pr(pij = m|p\ij,f,xij,wi = n) ∝
(20.27)
 
P
a,b\i,j δ[fab −fij]δ[pab −m] + β
P
k
P
a,b\i,j δ[fab −k]δ[pab −m] + Kβ
!
Normxij[˜µm, ˜
Σm]
×
 
P
a,b\i,j δ[pab −m]δ[wi −n] + α
P
m
P
a,b\i,j δ[pab −m]δ[wi −n] + Mα
!
,
where the notation P
a,b\i,j denotes summation over all values of {a,b} except i,j. The
terms ˜µm and ˜Σm are the mean and covariance of all of the word positions associated
with the mth part ignoring the contribution of the current position xij.
At the end of the procedure, the probabilities are computed using the relations in
Equation 20.20 and the part locations as
ˆµm =
P
i,j,t xijδ[p[t]
ij −m]
P
i,j,t δ[p[t]
ij −m]
ˆΣm =
P
i,j,t(xij −µm)T (xij −µm)δ[p[t]
ij −m]
P
i,j,t δ[p[t]
ij −m]
.
(20.28)
Example learning results can be seen in Figure 20.11. Each part is a spatially local-
ized cluster of words, and these often correspond to real-world objects such as “legs” or
“wheels.” The parts are shared between the objects and so there is no need for a dif-
ferent set of parameters to learn the appearance of wheels for bicycles and wheels for
motorbikes.
1More properly, we should deﬁne a prior over the mean and variance of the parts and marginalize over these
parameters as well. This also avoids problems when no features are assigned to a certain part and hence the
mean and covariance cannot be computed.

498
20 Models for visual words
Figure 20.11 Sharing words in the constellation model. a) Sixteen images from training set (two
from each class). Yellow ellipses depict words identiﬁed in the image associated with one part
of the image (i.e., they are equivalent of the crosses in Figure 20.9). It is notable that the words
associated with this part mainly belong to the lower part of the faces of the animal images. b) The
mean µ and variance Σ of this object part. c,d) A second part seems to correspond to the legs of
animals in proﬁle. e,f) A third part contains many words associated with the wheels of objects.
Adapted from Sudderth et al. (2008). c⃝2008 IEEE.
20.5.2 Inference
In inference, we compute the likelihood of new image data {fj,xj}J
j=1 under each
possible object w ∈{1...N} using
Pr(f,X|w = n) =
J
Y
j=1
M
X
m=1
Pr(pj = m|w = n)Pr(fj|pj = m)Pr(xj|pj = m)
=
J
Y
j=1
M
X
pj=1
Catpj[πn]Catfj[λpj]Normxij[µpj,Σpj].
(20.29)
We now deﬁne suitable priors Pr(w) over the possible objects and use Bayes’ rule
to compute the posterior distribution
Pr(w = n|f,X) =
Pr(f,X|w = n)Pr(w = n)
PN
n=1 Pr(f,X|w = n)Pr(w = n)
.
(20.30)

20.6
Scene models
499
20.6
Scene models
One limitation of the constellation model is that it assumes that the image contains a sin-
gle object. However, real images generally contain a number of spatially offset objects.
Just as the object determined the probability of the different parts, so the scene deter-
mines the relative likelihood of observing different objects (Figure 20.12). For example,
an ofﬁce scene might include desks, computers, and chairs, but it is very unlikely to
include tigers or icebergs.
To this end we introduce a new set of variables that represent the choice of scene
{si}I
i=1 ∈{1...C}
Pr(wij|si = c) = Catwij[φc]
Pr(pij|wij = n) = Catpij[πwn]
Pr(fij|pij = m) = Catfij[λm]
Pr(xij|pij = m) = Normxij[µ(w)
n
+ µ(p)
m ,Σ(w)
n
+ Σ(p)
m ].
(20.31)
Each word has an object label {wij}I,JI
i=1,j=1, which denotes which of the L objects it
corresponds to. The scene label {si} determines the relative propensity for each object to
be present, and these probabilities are held in the categorical parameters {φc}C
c=1. Each
object type also has a position that is normally distributed with mean and covariance µ(w)
m
and Σ(w)
m . As before, each object deﬁnes a probability distribution over the M shared
parts where the part assignment is held in the label pij. Each part has a position that is
measured relative to the object position and has mean and covariance µ(p)
m , and Σ(p)
m ,
respectively.
We leave the details of the learning and inference algorithms as an exercise for the
reader; the principles are the same as for the constellation model; we generate a series of
samples from the posterior over the hidden variables wij and pij using Gibbs sampling
and update the mean and covariances based on the samples.
Figure 20.13 shows several examples of scenes that have been interpreted using a
scene model very similar to that described. In each case, the scene is parsed into a number
of objects that are likely to co-occur and are in a sensible relative spatial conﬁguration.
Figure 20.12 Scene model. Each image
consists of a single scene. A scene induces
a probability distribution over the presence
of different objects (different colors) such
as the monitor, piece of paper, and key-
board in this scene and their relative posi-
tions (thick ellipses). Each object is itself
composed of spatially separate parts (thin
ellipses). Each part has a number of words
associated with it (crosses, shown only for
one part for clarity).

500
20 Models for visual words
Figure 20.13 Scene recognition. a) Example street image. b) Results of scene parsing model.
Each ellipse represents one word (i.e., the equivalent of the crosses in Figure 20.12). The ellipse
color denotes the object label to which that word is assigned (part labels not shown). c) Results
of the bag of words model, which makes elementary mistakes such as putting car labels at the top
of the image as it has no spatial information. d) Another street scene. e) Interpretation with scene
model and f) bag of words model. g–o) Three more ofﬁce scenes parsed by the scene model and
bag of words models. Adapted from Sudderth et al. (2008). c⃝2008 Springer.
20.7
Applications
In this chapter we have described a series of generative models for visual words of
increasing complexity.
Although these models are interesting, it should be empha-
sized that many applications use only the basic bag of words approach combined with a
discriminative classiﬁer. We now describe two representative examples of such systems.
20.7.1 Video Google
Sivic and Zisserman (2003) presented a system based on the bag of words, which can
retrieve frames from a movie very efﬁciently based on a visual query; the user draws
a bounding box around the object of interest and the system returns other images that
contain the same object (Figure 20.14).
The system starts by identifying feature positions in each frame of the video. Unlike
conventional bag of words models, these feature positions are tracked through several
frames of video and rejected if this cannot be done. The averaged SIFT descriptor over
each track is used to represent the image contents in the neighborhood of the feature.
These descriptors are then clustered using K-means to create of the order of 6,000–10,000
possible visual words. Each feature is then assigned to one of these words based on the
distance to the nearest cluster. Finally, each image or region is characterized by a vector
containing the frequencies with which each visual word is found.
When the system receives a query, it compares the vector for the identiﬁed region to
those for each potential region in the remaining video stream and retrieves those that are
closest.

20.7
Applications
501
Figure 20.14 Video Google. a) The user identiﬁes part of one frame of a video by drawing
a bounding box around part of the scene. b–i) The system returns a ranked list of frames that
contain the same object and identiﬁes where it is in the image (white bounding boxes). The system
correctly identiﬁes the leopard-skin patterned hat in a variety of contexts despite changes in scale
and position. In (h) it mistakes the texture of the vegetation in the background for an instance of
the hat.
The implementation includes several features that make this process more reliable.
First, it discards the top 5 percent and bottom 10 percent of words according to their
frequency. This eliminates very common words that do not distinguish between frames
and words that are very rare and are hence inefﬁcient to search on. Second, it weights
the distance measure using the term-frequency inverse document frequency scheme: the
weight increases if the word is relatively rare in the database (the word is discriminative)
and if it is used relatively frequently in this region (it is particularly representative of the
region). Finally, the matches are considered more reliable if the spatial arrangement of
visual words is similar. The ﬁnal retrieved results are re-ranked based on their spatial
consistency with the query.
The ﬁnal system reliably returns plausible regions for a feature length movie in less
than 0.1 seconds using an inverted ﬁle structure to facilitate efﬁcient retrieval.
20.7.2 Action recognition
Laptev et al. (2008) applied a bag of words approach to action recognition in video
sequences. They used a space-time extension of the Harris corner detector to ﬁnd interest
points in the video frames and extracted descriptors at multiple scales around each point.
They eliminated detections at boundaries between shots.
To characterize the local motion and appearance, they computed histogram-based
descriptors of the space time volumes surrounding these feature points. These were either
based on the histogram of oriented gradients (HOG) descriptor (Section 13.3.3) or based

502
20 Models for visual words
Figure 20.15 Example images from the KTH database (Sch¨uldt et al. 2004). Images in (a–f)
each show three examples of the six categories of walking, jogging, running, boxing, waving, and
clapping, respectively. Using the bag of words approach of Laptev et al. (2008), these actions can
be classiﬁed with over 90 percent accuracy.
on histograms of local motion. They clustered a subset of 100,000 descriptors from the
training data using the K-means algorithm to create 4000 clusters and each feature in the
test and training data was represented by the index of the nearest cluster center.
They binned these quantized feature indices over a number of different space-time
windows. The ﬁnal decision about the action was based on a one-against-all binary
classiﬁer in which each action was separately considered and rated as being present or
absent. The kernelized binary classiﬁer combined information from the two different
feature types and the different space time windows.
Laptev et al. (2008) ﬁrst considered discriminating between six actions from the KTH
database (Sch¨uldt et al. 2004). This is a relatively simple dataset in which the camera is
static and the action occurs against a relatively empty background (Figure 20.15). They
discriminated between these classes with an average of 91.8 percent accuracy, with the
major confusion being between jogging and running.
They also considered a more complex database containing eight different actions
from movie sequences (Figure 20.16). It was notable that the performance here relied
more on the HOG descriptors than the motion information, suggesting that the local
context was providing considerable information (e.g., the action “get out of car” is more
likely when a car is present). For this database, the performance was much worse, but it
was signiﬁcantly better than chance; action recognition “in the wild” is an open problem
in computer vision research.
Discussion
The models in this chapter treat each image as a set of discrete features. The bag of fea-
tures model, latent Dirichlet allocation, and single author–topic models do not explicitly
describe the position of objects in the scene. Although they are effective for recognizing
objects, they cannot locate them in the image. The constellation model improves this
by allowing the parts of object to have spatial relations, and the scene model describes a
scene as a collection of displaced parts.

Notes
503
Figure 20.16 Action recognition in movie database of Laptev et al. (2008). a) Example true
positives (correct detections), b) true negatives (action correctly identiﬁed as being absent), c)
false positives (action classiﬁed as occurring but didn’t) d) false negatives (action classiﬁed as
not occurring but did). This type of real-world action classiﬁcation task is still considered very
challenging.
Notes
Bag of words models: Sivic and Zisserman (2003) introduced the term “visual words” and ﬁrst
made the connection with text retrieval. Csurka et al. (2004) applied the bag of words methodol-
ogy to object recognition. A number of other studies then exploited developments in the document
search community. For example, Sivic et al. (2005) exploited probabilistic latent semantic anal-
ysis (Hofmann 1999) and latent Dirichlet allocation (Blei et al. 2003) for unsupervised learning
of object classes. Sivic et al. (2008) extended this work to learn hierarchies of object classes. Li
and Perona (2005) constructed a model very similar to the original author–topic model (Rosen-
Zvi et al. 2004) for learning scene categories. Sudderth et al. (2005) and Sudderth et al. (2008)
extended the author–topic model to contain information about the spatial layout of objects. The
constellation and scene models presented in this chapter are somewhat simpliﬁed versions of
this work. They also extended these models to cope with varying numbers of objects and or
parts.
Applications of bag of words: Applications of the bag of words model include object recogni-
tion (Csurka et al. 2004), searching through video (Sivic and Zisserman 2003), scene recognition
(Li and Perona 2005), and action recognition (Sch¨uldt et al. 2004), and similar approaches
have been applied to texture classﬁication (Varma and Zisserman 2004) and labeling facial
attributes (Aghajanian
et al. 2009).
Recent progress in object recognition can be reviewed
by examining a recent summary of the PASCAL visual object classes challenge (Everingham
et al. 2010).
In the 2007 competition, bag of words approaches with no spatial information
at all were still common.
Several authors (Nist´er and Stew´enius 2006; Philbin et al. 2007;
Jegou et al. 2008) have now presented large-scale demonstrations of object instance recognition
based on bag of words, and this idea has been used in commercial applications such as “Google
Goggles.”
Bag of words variants: Although we have discussed mainly generative models for visual words
in this chapter, discriminative approaches generally yield somewhat better performance. Grauman
and Darrell (2005) introduced the pyramid match kernel, which maps unordered data in a high-
dimensional feature space into multiresolution histograms and computes a weighted histogram
intersection in this space. This effectively performs the clustering and feature comparison steps
simultaneously. This idea was extended to the spatial domain of the image itself by Lazebnik et al.
(2006).

504
20 Models for visual words
Improving the pipeline: Yang et al. (2007) and Zhang et al. (2007) provide quantitative com-
parisons showing how the various parts of the pipeline (e.g., the matching kernel, interest point
detector, clustering method) affect object recognition results.
The focus of recent research has moved on to addressing various weaknesses of the pipeline such
as the arbitrariness of the initial vector quantization step and the problem of regular patterns (Chum
et al. 2007; Philbin et al. 2007, 2010; J´egou et al. 2009; Mikulik et al. 2010; Makadia 2010). The
current trend is to increase the problem to realistic sizes and to this end new databases for object
recognition (Deng et al. 2010) and scene recognition have been released (Xiao et al. 2010).
Action recognition: There has been a progression in recent years from testing action recogni-
tion algorithms in specially captured databases where the subject can easily be separated from the
background (Sch¨uldt et al. 2004), to movie footage (Laptev et al. 2008), and ﬁnally to completely
unconstrained footage that may not be professionally shot and may have considerable camera
shake. As this progression has taken place, the dominant approach has gradually become to base
the system on visual words that capture the context of the scene as well as the action itself (Laptev
et al. 2008). A comparison between approaches based on visual words and those that used explicit
parts for action recognition in a still frame is presented by Delaitre et al. (2010). Recent work in
this area has addressed unsupervised learning of action categories (Niebles et al. 2008).
Problems
20.1 The bag of words method in this chapter uses a generative approach to model the frequencies
of the visual words. Develop a discriminative approach that models the probability of the object
class as a function of the word frequencies.
20.2 Prove the relations in Equation 20.8, which show how to learn the latent Dirichlet allocation
model in the case where we do know the part labels {pij}I,Ji
i=1,j=1.
20.3 Show that the likelihood and prior terms are given by Equations 20.10 and 20.11, respectively.
20.4 Li and Perona (2005) developed an alternative model to the single author–topic model in
which the hyperparameter α was different for each value of the object label w.
Modify the
graphical model for latent Dirichlet allocation to include this change.
20.5 Write out generative equations for the author–topic model in which multiple authors are
allowed for each document. Draw the associated graphical model.
20.6 In real objects, we might expect visual words f that are adjacent to one another to take the
same part label. How would you modify the author–topic model to encourage nearby part labels
to be the same. How would the Gibbs sampling procedure for drawing samples from the posterior
probability over parts be affected?
20.7 Draw a graphical model for the scene model described in Section 20.6.
20.8 All of the models in this chapter have dealt with classiﬁcation; we wish to infer a discrete
variable representing the state of the world based on discrete observed features {fj}. Develop a
generative model that can be used to infer a continuous variable based on discrete observed features
(i.e., a regression model that uses visual words).

Part VII
Appendices


Appendix A
Notation
This is a brief guide to the notational conventions used in this text.
Scalars, vectors, and matrices
We denote scalars by either small or capital letters a,A,α. We denote column vectors
by bold small letters a,φ. When we need a row vector we usually present this as the
transpose of a column vector aT ,φT .
We represent matrices by bold capital letters B,Φ. The ith row and jth column of
matrix A is written as aij. The jth column of matrix A is written as aj. When we need
to refer to the ith row of a matrix, we write this as ai• where the bullet • indicates that
we are considering all possible values of the column index.
We concatenate two D×1 column vectors horizontally as a = [b,c] to form the D×2
matrix A. We concatenate two D×1 column vectors vertically as a = [bT ,cT ]T to form
the 2D × 1 vector a. Although this notation is cumbersome, it allows us to represent
vertical concatenations within a single line of text.
Variables and parameters
We denote variables with Roman letters a,b.
The most common examples are the
observed data which is always denoted by x and the state of the world which is always
denoted by w. However, other hidden or latent variables are also represented by Roman
letters. We denote parameters of the model by Greek letters µ,Φ,σ2. These are distin-
guished from variables in that there is usually a single set of parameters that explains the
relation between many sets of variables.
Functions
We write functions as a name, followed by square brackets that contain the arguments of
the function. For example, log[x] returns the logarithm of the scalar variable x. Some-
times we will write a function with bullets • as arguments (e.g., atan2[•,•]) to focus the
interest on the function itself rather than the arguments.
When the function returns one or more vector or matrix arguments, it is written in
bold. For example, the function aff[x,Φ,τ] applies an afﬁne transformation to the 2D
point x with parameters Φ,τ and returns a new 2D vector output. When a function
returns multiple outputs, we write this in Matlab notation so [U,L,V] = svd[X] returns
the three parts U,L,V of the singular value decomposition of X.

508
A Notation
Some functions are used repeatedly throughout the text. These include:
• minx f[x], which returns the minimum possible value of the function f[x] as we
vary x over its entire valid range,
• argminx f[x], which returns the value of the argument x that minimizes f[x],
• maxx and argmaxx, which fulﬁll the same roles as minx and argminx but where
we are maximizing the function,
• diag[A], which returns a column vector containing the elements on the diagonal
of matrix A,
• δ[x] for continuous x, which is a Dirac delta function and has the key property
R
f[x]δ[x −x0]dx = f[x0],
• δ[x] for discrete x, which returns 1 when the argument x is 0 and returns 0
otherwise, and
• heaviside[x], which represents the Heaviside step function. It returns 0 when the
argument x < 0 and returns 1 otherwise.
Probability distributions
We write the probability of a random variable x as Pr(x). We write the joint probability
of two variables a,b as Pr(a,b) and the conditional probability of a given b as Pr(a|b).
Sometimes, we wish to specify the exact value b∗that a is conditioned upon and here we
write Pr(a|b = b∗). Occasionally, we denote that variables a and b are independent by
writing a ⊥⊥b. Similarly we indicate that a and b are conditionally independent given c
by writing a⊥⊥b|c.
Probability distributions are written in the style Pr(x|µ,Σ) = Normx[µ,Σ]. This
returns the value of the multivariate normal distribution for data x when the distribution
has mean µ and covariance Σ. In this way, we always distinguish the argument of the
distribution (here x) from the parameters (here µ,Σ).
Sets
We denote sets with calligraphic letters S. The notation S ⊂T indicates that S is a
subset of T . The notation x ∈S indicates that x is a member of the set S. The notation
A = B ∪C indicates that set A is the union of sets B and C. The notation A = B \ C
indicates that set A consists of all of the elements of B except those that are in C.
Often we write out a set explicitly in terms of the elements and for this we use curly
brackets so that A = {x,y,z} indicates that the set A contains x,y, and z and nothing
else. When a set is empty, we write A = {}. We use the notation {xi}I
i=1 as shorthand
to represent the set {x1,x2,...,xI}, and we may write the same set in the compact form
x1...I if it is part of an equation.

Appendix B
Optimization
Throughout this book, we have used iterative nonlinear optimization methods to ﬁnd the
maximum likelihood or MAP parameter estimates. We now provide more details about
these methods. It is impossible to do full justice to this topic in the space available;
many entire books have been written about nonlinear optimization. Our goal is merely
to provide a brief introduction to the main ideas.
B.1
Problem statement
Continuous nonlinear optimization techniques aim to ﬁnd the set of parameters ˆθ that
minimize a function f[•]. In other words, they try to compute
ˆθ = argmin
θ
[f[θ]],
(B.1)
where f[•] is termed a cost function or objective function.
Although optimization techniques are usually described in terms of minimizing a
function, most optimization problems in this book involve maximizing an objective func-
tion based on log probability. To turn a maximization problem into a minimization, we
multiply the objective function by minus one. In other words, instead of maximizing the
log probability, we minimize the negative log probability.
B.1.1
Convexity
The optimization techniques that we consider here are iterative: they start with an esti-
mate θ[0] and improve it by ﬁnding successive new estimates θ[1],θ[2],...,θ[∞] each of
which is better than the last until no more improvement can be made. The techniques are
purely local in the sense that the decision about where to move next is based on only the
properties of the function at the current position. Consequently, these techniques can-
not guarantee the correct solution: they may ﬁnd an estimate θ[∞] from which no local
change improves the cost. However, this does not mean there is not a better solution in
some distant part of the function that has not yet been explored (Figure B.1). In opti-
mization parlance, they can only ﬁnd local minima. One way to mitigate this problem is
to start the optimization from a number of different places and choose the ﬁnal solution
with the lowest cost.

510
B Optimization
Figure B.1 Local minima. Optimization
methods aim to ﬁnd the minimum of the
objective function f[θ] with respect to
parameters θ. Roughly, they work by start-
ing with an initial estimate θ[0] and moving
iteratively downhill until no more progress
can be made (ﬁnal position represented by
θ[∞]). Unfortunately, it is possible to ter-
minate in a local minimum. For example,
if we start at θ′[0] and move downhill, we
wind up in position θ′[∞].
Figure B.2 Convex functions. If the func-
tion is convex, then the global minimum
can be found. A function is convex if no
chord (line between two points on the func-
tion) intersects the function.
The ﬁgure
shows two example chords (blue dashed
lines).
The convexity of a function can
be established algebraically by considering
the matrix of second derivatives. If this is
positive deﬁnite for all values of θ, then the
function is convex.
In the special case where the function is convex, there will only be a single mini-
mum, and we are guaranteed to ﬁnd it with sufﬁcient iterations (Figure B.2). For a 1D
function, it is possible to establish the convexity by looking at the second derivative of
the function; if this is positive everywhere (i.e., the slope is continuously increasing),
then the function is convex and the global minimum can be found. The equivalent test in
higher dimensions is to examine the Hessian matrix (the matrix of second derivatives of
the cost function with respect to the parameters). If this is positive deﬁnite everywhere
(see Appendix C.2.6), then the function is convex and the global minimum will be found.
Some of the cost functions in this book are convex, but this is unusual; most optimization
problems found in vision do not have this convenient property.
B.1.2
Overview of approach
In general the parameters θ over which we search are multidimensional. For example,
when θ has two dimensions, we can think of the function as a two-dimensional surface
(Figure B.3). With this in mind, the principles behind the methods we will discuss are
simple. We alternately

B.2
Choosing a search direction
511
• Choose a search direction s based on the local properties of the function, and
• Search to ﬁnd the minimum along the chosen direction. In other words, we seek
the distance λ to move such that
ˆλ = argmin
λ
h
f[θ[t] + λs]
i
,
(B.2)
and then set θ[t+1] = θ[t] + ˆλs. This is termed a line search.
We now consider each of these stages in turn.
B.2
Choosing a search direction
We will describe two general methods for choosing a search direction (steepest descent
and Newton’s method) and one method which is specialized for least squares problems
(the Gauss-Newton method). Both methods rely on computing derivatives of the function
with respect to the parameters at the current position. To this end, we are relying on the
function being smooth so that the derivatives are well behaved.
For most models, it is easy to ﬁnd a closed form expression for the derivatives. If
this is not the case, then an alternative is to approximate them using ﬁnite differences.
For example, the ﬁrst derivative of f[•] with respect to the jth element of θ can be
approximated by
∂f
∂θj
≈f [θ + aej] −f [θ]
a
,
(B.3)
where a is a small number and ej is the unit vector in the jth direction. In principle as a
tends to zero, this estimate becomes more accurate. However, in practice the calculation
is limited by the ﬂoating point precision of the computer, so a must be chosen with care.
B.2.1
Steepest descent
An intuitive way to choose the search direction is to measure the gradient and select
the direction which moves us downhill fastest. We could move in this direction until
the function no longer decreases, then recompute the steepest direction and move again.
In this way, we gradually move toward a local minimum of the function (Figure B.3a).
The algorithm terminates when the gradient is zero and the second derivative is positive,
indicating that we are at the minimum point, and any further local changes would not
result in further improvement. This approach is termed steepest descent. More precisely,
we choose
θ[t+1] = θ[t] −λ ∂f
∂θ

θ[t] ,
(B.4)
where the derivative ∂f/∂θ is the gradient vector that points uphill and λ is the distance
moved downhill in the opposite direction −∂f/∂θ. The line search procedure (Section
B.3) selects the value of λ.
Steepest descent sounds like a good idea but can be very inefﬁcient in certain situ-
ations (Figure B.3b). For example, in a descending valley, it can oscillate ineffectually
from one side to the other rather than proceeding straight down the center: the method
approaches the bottom of the valley from one side, but overshoots because the val-
ley itself is descending, so the minimum along the search direction is not exactly in

512
B Optimization
Figure B.3 Optimization on a two-dimensional function (color represents height of function).
We wish to ﬁnd the parameters that minimize the function (green cross). Given an initial starting
point θ0 (blue cross), we choose a direction and then perform a local search to ﬁnd the optimal
point in that direction. a) One way to chose the direction is steepest descent: at each iteration, we
head in the direction where the function changes the fastest. b) When we initialize from a different
position, the steepest descent method takes many iterations to converge due to oscillatory behavior.
c) Close-up of oscillatory region (see main text). d) Setting the direction using Newton’s method
results in faster convergence. e) Newton’s method does not undergo oscillatory behavior when we
initialize from the second position.
the valley center (Figure B.3c). When we remeasure the gradient and perform a sec-
ond line search, we overshoot in the other direction. This is not an unusual situation:
it is guaranteed that the gradient at the new point will be perpendicular to the previ-
ous one, so the only way to avoid this oscillation is to hit the valley at exactly right
angles.
B.2.2
Newton’s method
Newton’s method is an improved approach that also exploits the second derivatives at
the current point: it considers both the gradient of the function and how that gradient is
changing.
To motivate the use of second derivatives, consider a one-dimensional function (Fig-
ure B.4). If the magnitude of the second derivative is low, then the gradient is changing
slowly. Consequently, it will probably take a while before it completely ﬂattens out and
becomes a minimum, and so it is safe to move a long distance. Conversely, if the mag-
nitude of the second derivative is high, then things are changing rapidly, and we should
move only a small distance.
Now consider the same argument in two dimensions. Imagine we are at a point
where the gradient is identical in both dimensions. For steepest descent, we would move
equally in both dimensions. However, if the magnitude of the second derivative in the

B.2
Choosing a search direction
513
Figure B.4 Use of second derivatives. The gradient at the red and blue points is the same, but
the magnitude of the second derivative is larger at the red point than the blue point: the gradient
is changing faster at the red point than the blue point. The distance we move should be moderated
by the second derivative: if the gradient is changing fast, then the minimum may be nearby and we
should move a small distance. If it is changing slowly, then it is safe to move further. Newton’s
method takes into account the second derivative: it uses a Taylor expansion to create a quadratic
approximation to the function and then moves toward the minimum.
ﬁrst direction is much greater than that in the second, we would nonetheless wish to move
further in the second direction.
To see how to exploit the second derivatives algebraically, consider a truncated Taylor
expansion around the current estimate θ[t]:
f[θ] ≈f[θ[t]] + (θ −θ[t])T ∂f
∂θ

θ[t] + 1
2(θ −θ[t])T ∂2f
∂θ2

θ[t] (θ −θ[t]),
(B.5)
where θ is a D × 1 variable, the ﬁrst derivative vector is of size D × 1, and the Hessian
matrix of second derivatives is D×D. To ﬁnd the local extrema, we now take derivatives
with respect to θ and set the result to zero
∂f
∂θ ≈∂f
∂θ

θ[t] + ∂2f
∂θ2

θ[t] (θ −θ[t]) = 0.
(B.6)
By rearranging this equation, we get an expression for the minimum ˆθ,
ˆθ = θ[t] −
∂2f
∂θ2
−1 ∂f
∂θ ,
(B.7)

514
B Optimization
where the derivatives are still taken at θ[t], but we have stopped writing this for clarity.
In practice we would implement Newton’s method as a series of iterations
θ[t+1] = θ[t] −λ
∂2f
∂θ2
−1 ∂f
∂θ ,
(B.8)
where the λ is the step size. This can be set to one, or we can ﬁnd the optimal value using
line search.
One interpretation of Newton’s method is that we have locally approximated the func-
tion as a quadratic. On each iteration, we move toward its extremum (or move exactly
to it if we ﬁx λ = 1). Note that we are assuming that we are close enough to the correct
solution that the nearby extremum is a minimum and not a saddle point or maximum. In
particular, if the Hessian is not positive deﬁnite, then a direction that is not downhill may
be chosen. In this sense Newton’s method is not as robust as steepest descent.
Subject to this limitation, Newton’s method converges in fewer iterations than steep-
est descent (Figure B.3d–e). However, it requires more computation per iteration as we
have to invert the D × D Hessian matrix at each step. Choosing this method usually
implies that we can write the Hessian in closed form; approximating the Hessian from
ﬁnite derivatives requires many function evaluations and so is potentially very costly.
B.2.3
Gauss-Newton method
Cost functions in computer vision often take the special form of a least squares problem
f[θ] =
I
X
i=1
(xi −g[wi,θ])T (xi −g[wi,θ]),
(B.9)
where g[•,•] is a function that transfers the variables {wi} into the space of the variables
{xi}, and is parameterized by θ. In other words, we seek the values of θ that most
closely map {wi} to {xi} in a least squares sense. This cost function is a special case of
the more general form f[θ] = zT z, where
z =


x1 −g[w1,θ]
x2 −g[w2,θ]|
...
xI −g[wI,θ]

.
(B.10)
The Gauss-Newton method is an optimization technique that is used to solve least
squares problems of the form
ˆθ = argmin[f[θ]]
where f[θ] = z[θ]T z[θ].
(B.11)
To minimize this objective function, we approximate the term z[θ] with a Taylor series
expansion around the current estimate θ[t] of the parameters:
z[θ] ≈z[θ[t]] + J(θ −θ[t]),
(B.12)
where J is the Jacobian matrix. The entry jmn at the mth row and the nth column of J
contains the derivative of the mth element of z with respect to the nth parameter so that
jmn = ∂zm
∂θn
.
(B.13)

B.3
Line search
515
Now we substitute the approximation for z[θ] into the original cost function f[θ] = zT z
to yield
f[θ] ≈(z[θ[t]] + J(θ −θ[t]))T (z[θ[t]] + J(θ −θ[t]))
(B.14)
= z[θ[t]]T z[θ[t]] + 2(θ −θ[t])T JT z[θ[t]] + (θ −θ[t])T JT J(θ −θ[t]).
Finally, we take derivatives of this expression with respect to the parameters θ and
equate to zero to get the relation
∂f
∂θ ≈2JT z[θ[t]] + 2JT J(θ −θ[t]) = 0.
(B.15)
Rearranging, we get the update rule:
θ = θ[t] −(JT J)−1JT z[θ[t]].
(B.16)
We can rewrite this by noting that
∂f
∂θ

θ[t] = ∂zT z
∂θ

θ[t] = 2JT z[θ[t]]
(B.17)
to give the ﬁnal Gauss-Newton update
θ[t+1] = θ[t] −λ(JT J)−1 ∂f
∂θ ,
(B.18)
where the derivative is taken at θ[t] and λ is the step size.
Comparing with the Newton update (Equation B.8), we see that we can consider this
update as approximating the Hessian matrix as H ≈JT J. It provides better results than
gradient descent without ever computing second derivatives. Moreover, the term JT J is
normally positive deﬁnite resulting in increased stability.
B.2.4
Other methods
There are numerous other methods for choosing the optimization direction. Many of
these involve approximating the Hessian in some way with the goal of either ensuring
that a downhill direction is always chosen or reducing the computational burden. For
example, if computation of the Hessian is prohibitive, a practical approach is to approx-
imate it with its own diagonal. This usually provides a better direction than steepest
descent.
Quasi-Newton methods such as the Broyden Fletcher Goldfarb Shanno (BFGS)
method approximate the Hessian with information gathered by analyzing successive
gradient vectors. The Levenberg-Marquardt algorithm interpolates between the Gauss-
Newton algorithm and steepest descent with the aim of producing a method that requires
few iterations and is also robust. Damped Newton and trust-region methods also attempt
to improve the robustness of Newton’s method.
The nonlinear conjugate gradient
algorithm is another valuable method when only ﬁrst derivatives are available.
B.3
Line search
Having chosen a sensible direction using steepest descent, Newton’s method or some
other approach, we must now decide how far to move: we need an efﬁcient method to

516
B Optimization
Figure B.5 Line search over region [a,d] using bracketing approach. Gray region indicates cur-
rent search region. a) We deﬁne two points b,c that are interior to the search region and evaluate
the function at these points. Here f[b] > f[c] so we eliminate the range [a,b]. b) We evaluate two
points [b,c] interior to the new range and compare their values. This time we ﬁnd that f[b] < f[c]
so we eliminate the range [c,d]. c) We repeat this process until the minimum is closely bracketed.
ﬁnd the minimum of the function in the chosen direction. Line search methods start by
determining the range over which to search for the minimum. This is usually guided by
the magnitude of the second derivative along the line that provides information about the
likely search range (see Figure B.4).
There are many heuristics to ﬁnd the minimum, but we will discuss only the direct
search method (Figure B.5). Consider searching over the region [a,d]. We compute the
function at two internal points b and c where a < b < c < d. If f[b] < f[c], we eliminate
the range [c,d] and search over the new region [a,c] at the next iteration. Conversely, if
f[b] > f[c], we eliminate the range [a,b] and search over a new region [b,d].
This method is applied iteratively until the minimum is closely bracketed. It is typi-
cally not worth exactly locating the minimum; the line search direction is rarely optimal
and so the minimum of the line search is usually far from the overall minimum of the
function. Once the remaining interval is sufﬁciently small, an estimate of the minimum
position can be computed by making a parabolic ﬁt to the three points that remain after
eliminating one region or the other and selecting the position of the minimum of this
parabola.
B.4
Reparameterization
Often in vision problems, we must ﬁnd the best parameters θ subject to one or more
constraints. Typical examples include optimizing variances σ2 that must be positive,
covariance matrices that must be positive deﬁnite, and matrices that represent geometric
rotations which must be orthogonal. The general topic of constrained optimization is
beyond the scope of this volume, but we brieﬂy describe a trick that can be used to
convert constrained optimization problems into unconstrained ones that can be solved
using the techniques already described.
The idea of reparameterization is to represent the parameters θ in terms of a new set
of parameters φ, which do not have any constraints on them, so that
θ = g[φ],
(B.19)
where g[•] is a carefully chosen function.

B.4
Reparameterization
517
Then we optimize with respect to the new unconstrained parameters φ. The objective
function becomes f[g[φ]] and the derivatives are computed using the chain rule so that
the ﬁrst derivative would be
∂f
∂φ =
K
X
k=1
∂f
∂θk
∂θk
∂φ .
(B.20)
where θk is the kth element of θ. This strategy is easier to understand with some concrete
examples.
Parameters that must be positive
When we optimize a variance parameter θ = σ2 we must ensure that the ﬁnal answer is
positive. To this end, we use the relation
θ = exp[φ],
(B.21)
and now optimize with respect to the new scalar parameter φ. Alternatively, we can use
the square relation:
θ = φ2,
(B.22)
and again optimize with respect to the parameters φ.
Parameters that must lie between 0 and 1
To ensure that a scalar parameter θ lies between zero and one, we use the logistic sigmoid
function:
θ =
1
1 + exp[−φ],
(B.23)
and optimize with respect to the new scalar parameter φ.
Parameters that must be positive and sum to one
To ensure that the elements of a K × 1 multivariable parameter θ sum to one and are all
positive we use the softmax function:
θk =
exp[φk]
PK
j=1 exp[φj]
,
(B.24)
and optimize with respect to the new K × 1 variable φ.
3D rotation matrices
A 3 × 3 rotation matrix contains three independent quantities spread throughout its nine
entries. A number of nonlinear constraints exist between the entries: the norm of each
column and row must be one, each column is perpendicular to the other columns, each
row is perpendicular to the other rows, and the determinant is one.
One way to enforce these constraints is to reparameterize the rotation matrix as a
quaternion and optimize with respect to this new representation. A quaternion q is a
4D quantity q = [q0,q1,q2,q3]. Mathematically speaking, they are a four-dimensional
extension of complex numbers, but the relevance for vision is that they can be used to

518
B Optimization
represent 3D rotations. We use the relation:
Θ =
1
q2
0 + q2
1 + q2
2 + q2
3


q2
0 + q2
1 −q2
2 −q2
3
2q1q2 −2q0q3
2q1q3 + 2q0q2
2q1q2 + 2q0q3
q2
0 −q2
1 + q2
2 −q2
3
2q2q3 −2q0q1
2q1q3 −2q0q2
2q2q3 + 2q0q1
q2
0 −q2
1 −q2
2 + q2
3

.
(B.25)
Although the quaternion contains four numbers, only the ratios of those numbers
are important (giving 3 degrees of freedom): each element of equation B.25 consists
of squared terms, which are normalized by the squared amplitude constant, and so and
constant that multiplies the elements of q is canceled out when we convert back to a
rotation matrix.
Now we optimize with respect to the quaternion q . The derivatives with respect to
the kth element of q can be computed as
∂f
∂qk
=
3
X
i=1
3
X
j=1
∂f
∂Θij
∂Θij
∂qk
.
(B.26)
The quaternion optimization is stable as long as we do not approach the singularity
at q = 0. One way to achieve this is to periodically renormalize the quaternion to length
1 during the optimization procedure.
Positive deﬁnite matrices
When we optimize over a K × K covariance matrix Θ = Σ, we must ensure that the
result is positive deﬁnite. A simple way to do this is to use the relation:
Θ = ΦΦT ,
(B.27)
where Φ is an arbitrary K × K matrix.

Appendix C
Linear algebra
C.1
Vectors
A vector is a geometric entity in D dimensional space that has both a direction and a
magnitude. It is represented by a D × 1 array of numbers. In this book, we write vectors
as bold, small, Roman or Greek letters (e.g., a,φ). The transpose aT of vector a is a
1 × D array of numbers where the order of the numbers is retained.
C.1.1
Dot product
The dot product or scalar product between two vectors a and b is deﬁned as
c = aT b =
D
X
d=1
adbd,
(C.1)
where aT is the transpose of a (i.e., a converted to a row vector) and the returned value c
is a scalar. Two vectors are said to be orthogonal if the dot product between them is zero.
C.1.2
Norm of a vector
The magnitude or norm of a vector is the square root of the sum of the square of the D
elements so that
norm[a] = |a| =
 D
X
d=1
a2
d
!1/2
= (aT a)1/2.
(C.2)
C.1.3
Cross product
The cross product or vector product is specialized to three dimensions. The operation
c = a × b is equivalent to the matrix multiplication (Section C.2.1):


c1
c2
c3

=


0
−a3
a2
a3
0
−a1
−a2
a1
0




b1
b2
b3

,
(C.3)
or for short
c = A×b,
(C.4)

520
C Linear algebra
where A× is the 3×3 matrix from Equation C.3 that implements the cross product.
It is easily shown that the result c of the cross product is orthogonal to both a and b.
In other words,
aT (a × b) = bT (a × b) = 0.
(C.5)
C.2
Matrices
Matrices are used extensively throughout the book and are written as bold, capital,
Roman or Greek letters (e.g., A,Φ). We categorize matrices as landscape (more columns
than rows), square (the same number of columns and rows), or portrait (more rows than
columns). They are always indexed by row ﬁrst and then column, so aij denotes the
element of matrix A at the ith row and the jth column.
A diagonal matrix is a square matrix with zeros everywhere except on the diagonal
(i.e., elements aii) where the elements may take any value. An important special case
of a diagonal matrix is the identity matrix I.
This has zeros everywhere except for the
diagonal, where all the elements are ones.
C.2.1
Matrix multiplication
To take the matrix product C = AB, we compute the elements of C as
cij =
K
X
k=1
aikbkj.
(C.6)
This can only be done when the number of columns in A equals the number of rows in
B. Matrix multiplication is associative so that A(BC) = (AB)C = ABC. However, it
is not commutative so that in general AB ̸= BA.
C.2.2
Transpose
The transpose of a matrix A is written as AT and is formed by reﬂecting it around the
principal diagonal, so that the kth column becomes the kth row and vice versa. If we
take the transpose of a matrix product AB, then we take the transpose of the original
matrices but reverse the order so that
(AB)T = BT AT .
(C.7)
C.2.3
Inverse
A square matrix A may or may not have an inverse A−1 such that A−1A = AA−1 = I.
If a matrix does not have an inverse, it is called singular.
Diagonal matrices are particularly easy to invert: the inverse is also a diagonal matrix,
with each diagonal value dii replaced by 1/dii. Hence, any diagonal matrix that has
nonzero values on the diagonal is invertible. It follows that the inverse of the identity
matrix is the identity matrix itself.
If we take the inverse of a matrix product AB, then we can equivalently take the
inverse of each matrix individually, and reverse the order of multiplication
(AB)−1 = B−1A−1.
(C.8)

C.2
Matrices
521
C.2.4
Determinant and trace
Every square matrix A has a scalar associated with it called the determinant and denoted
by |A| or det[A]. It is (loosely) related to the scaling applied by the matrix. Matrices
where the magnitude of the determinant is small tend to make vectors smaller upon mul-
tiplication. Matrices where the magnitude of the determinants is large tend to make them
larger. If a matrix is singular, the determinant will be zero and there will be at least one
direction in space that is mapped to the origin when the matrix is applied. For a diagonal
matrix, the determinant is the product of the diagonal values. It follows that the deter-
minant of the identity matrix is 1. Determinants of matrix expressions can be computed
using the following rules:
|AT | = |A|
(C.9)
|AB| = |A||B|
(C.10)
|A−1| = 1/|A|.
(C.11)
The trace of a matrix is a second number associated with a square matrix A. It is
the sum of the diagonal values (the matrix itself need not be diagonal). The traces of
compound terms are bound by the following rules:
tr[AT] = tr[A]
(C.12)
tr[AB] = tr[BA]
(C.13)
tr[A + B] = tr[A] + tr[B]
(C.14)
tr[ABC] = tr[BCA] = tr[CAB],
(C.15)
where in the last relation, the trace is invariant for cyclic permutations only, so that in
general tr[ABC] ̸= tr[BAC].
C.2.5
Orthogonal and rotation matrices
An important class of square matrix is the orthogonal matrix. Orthogonal matrices have
the following special properties:
1. Each column has norm one, and each row has norm one.
2. Each column is orthogonal to every other column, and each row is orthogonal to
every other row.
The inverse of an orthogonal matrix Ωis its own transpose, so ΩT Ω= Ω−1Ω= I;
orthogonal matrices are easy to invert! When this class of matrix premultiplies a vector,
the effect is to rotate it around the origin and possibly reﬂect it.
Rotation matrices are a subclass of orthogonal matrices that have the additional
property that the determinant is one. As the name suggests when this class of matrix
premultiplies a vector, the effect is to rotate it around the origin with no reﬂection.
C.2.6
Positive deﬁnite matrices
A D×D real symmetric matrix A is positive deﬁnite if xT Ax > 0 for all nonzero vectors
x. Every positive deﬁnite matrix is invertible and its inverse is also positive deﬁnite. The
determinant and trace of a symmetric positive deﬁnite matrix are always positive. The
covariance matrix Σ of a normal distribution is always positive deﬁnite.

522
C Linear algebra
C.2.7
Null space of a matrix
The right null space of a matrix A consists of the set of vectors x for which
Ax = 0.
(C.16)
Similarly, the left null space of a matrix A consists of the set of vectors x for which
xT A = 0T .
(C.17)
A square matrix only has a nontrivial null space (i.e., not just x = 0) if the matrix is
singular (non-invertible) and hence the determinant is zero.
C.3
Tensors
We will occasionally have need for D > 2 dimensional quantities that we shall refer to as
D-dimensional tensors. For our purposes a matrix can be thought of as the special case
of a two-dimensional tensor, and a vector as the special case of a 1D tensor.
The idea of taking matrix products generalizes to higher dimensions and is denoted
using the special notion ×n where n is the dimension over which we take the product.
For example, the lth element fl of the tensor product f = A×2 b×3 c is given by
fl =
X
m
X
n
Almnbmcn,
(C.18)
where l, m and n index the 3D tensor A, and b and c are vectors.
C.4
Linear transformations
When we premultiply a vector by a matrix this is called a linear transformation. Figure
C.1 shows the results of applying several different 2D linear transformations (randomly
chosen 2 × 2 matrices) to the 2D vectors that represent the points of the unit square. We
can deduce several things from this ﬁgure. First, the point (0,0) at the origin in always
mapped back onto itself. Second, collinear points remain collinear. Third, parallel lines
are always mapped to parallel lines. Viewed as a geometric transformation, premultipli-
cation by a matrix can account for shearing, scaling, reﬂection and rotation around the
origin.
A different perspective on linear transformations comes from applying different
transformations to points on the unit circle (Figure C.2). In each case, the circle is trans-
formed to an ellipse. The ellipse can be characterized by its major axis (most elongated
axis) and its minor axis (most compact axis), which are perpendicular to one another.
This tells us something interesting: in general, there is a special direction in space
(position on the original circle) that gets stretched the most (or compressed the least)
by the transformation. Likewise there is a second direction that gets stretched the least
or compressed the most.
C.5
Singular value decomposition
The singular value decomposition (SVD) is a factorization of a M × N matrix A such
that
A = ULVT ,
(C.19)

C.5
Singular value decomposition
523
Figure C.1 Effect of applying three linear transformations to a unit square. Dashed square is
before transformation. Solid square is after. The origin is always mapped to the origin. Colinear
points remain colinear. Parallel lines remain parallel. The linear transformation encompasses
shears, reﬂections, rotations, and scalings.
Figure C.2 Effect of applying three linear transformations to a circle. Dashed circle is before
transformation. Solid ellipse is after. After the transformation, the circle is mapped to an ellipse.
This demonstrates that there is one special direction that is expanded the most (becomes the major
axis of the ellipse), and one special direction that is expanded the least (becomes the minor axis of
the ellipse).
where U is a M × M orthogonal matrix, L is a M × N diagonal matrix, and V is a
N ×N orthogonal matrix. It is always possible to compute this factorization, although a
description of how to do so is beyond the scope of this book.
The best way to get the ﬂavor of the SVD is to consider some examples. First let us
consider a square matrix:
A1 =


0.183
0.307
0.261
−1.029
0.135
−0.941
0.949
0.515
−0.162

= ULVT
(C.20)
=


−0.204
−0.061
−0.977
0.832
−0.535
−0.140
−0.514
−0.842
0.160




1.590
0
0
0
0.856
0
0
0
0.303




−0.870
−0.302
0.389
−0.135
−0.613
−0.778
−0.474
0.729
−0.492

.
Notice that by convention the nonnegative values on the principal diagonal of L decrease
monotonically as we move from top-left to bottom-right. These are known as the singular
values.

524
C Linear algebra
Now consider the singular value decomposition of a portrait matrix:
A2 =


0.537
0.862
1.839
0.318
−2.258
−1.307

= ULVT
(C.21)
=


−0.263
0.698
0.665
−0.545
−0.676
0.493
0.795
−0.233
0.559




3.273
0
0
0.76
0
0


−0.898
−0.440
−0.440
0.898

.
For this rectangular matrix, the orthogonal matrices U and V are different sizes and the
diagonal matrix L is the same size as the original matrix. The singular values are still
found on the diagonal, but the number is determined by the smallest dimension. In other
words, if the original matrix was M × N then there will be min[M,N] singular values.
To further understand the SVD, let us consider a third example:
A3=

−0.147
0.357
−0.668
0.811

=

0.189
0.981
0.981
−0.189

1.068
0
0
0.335
−0.587
0.8091
0.809
0.587

.(C.22)
Figure C.3 illustrates the cumulative effect of the transformations in the decomposi-
tion A3 = ULVT . The matrix VT rotates and reﬂects the original points. The matrix L
scales the result differently along each dimension. In this case, it is stretched along the
ﬁrst dimension and shrunk along the second. Finally, the matrix U rotates the result.
Figure C.4 provides a second perspective on this process. Each pair of panels depicts
what happens when we modify a different part of the SVD but keep the remaining parts
the same. When we change V, the shape of the ﬁnal ellipse is the same, but the mapping
from original directions to points on the ellipse changes (observe the color change along
the major axis). When we modify the ﬁrst element of L, the length of the major axis
changes. When we change the other nonzero element of L, the length of the minor axis
changes. When we change the matrix U, the orientation of the ellipse changes.
C.5.1
Analyzing the singular values
We can learn a lot about a matrix by looking at the singular values. We saw in Section
C.4, that as we decrease the smallest singular value the minor axis of the ellipse becomes
progressively smaller. When it actually becomes zero, both sides of the unit circle col-
lapse into one another (as do points from circles of all radii). Now there is a many-to-one
mapping from the original points to the transformed ones and the matrix is no longer
invertible. In general, a matrix is only invertible if all of the singular values are nonzero.
The number of nonzero singular values is called the rank of the matrix. The ratio of
the smallest to the largest singular values is known as the condition number: it is roughly
a measure of how ‘invertible’ the matrix is. As it becomes close to zero, our ability to
invert the matrix decreases.
The singular values scale the different axes of the ellipse by different amounts (Figure
C.3c). Hence, the area of a unit circle is changed by a factor that is equal to the product
of the singular values. In fact, this scaling factor is the determinant (Section C.2.4).
When the matrix is singular, at least one of the singular values is zero and hence the
determinant is also zero. The right null space consists of all of the vectors that can be
reached by taking a weighted sum of those columns of V whose corresponding singular
values are zero. Similarly, the left null space consists of all of the vectors that can be

C.5
Singular value decomposition
525
Figure C.3 Cumulative effect of SVD components for matrix A3. a) Original object. b) Apply-
ing matrix VT rotates and reﬂects the object around the origin. c) Subsequently applying L
causes stretching/compression along the coordinate axes. d) Finally, applying matrix U rotates
and reﬂects this distorted structure.
reached by taking a weighted sum of those columns of U whose corresponding singular
values are zero.
Orthogonal matrices only rotate and reﬂect points, and rotation matrices just rotate
them. In either case, there is no change in area to the unit circle: all the singular values
are one for these matrices and the determinant is also one.
C.5.2
Inverse of a matrix
We can also see what happens when we invert a square matrix in terms of the singular
value decomposition. Using the rule (AB)−1 = B−1A−1, we have
A−1 = (ULVT )−1 = (VT )−1L−1U−1 = VL−1UT ,
(C.23)
where we have used the fact that U and V are orthogonal matrices so U−1 = UT and
V−1 = VT . The matrix L is diagonal so L−1 will also be diagonal with new nonzero
entries that are the reciprocal of the original values. This also shows that the matrix is not
invertible when any of the singular values are zero: we cannot take the reciprocal of 0.
Expressed in this way, the inverse has the opposite geometric effect to that of the
original matrix: if we consider the effect on the transformed ellipse in Figure C.3d, it
ﬁrst rotates by UT so its major and minor axis are aligned with the coordinate axes
(Figure C.3c). Then it scales these axes (using the elements of L−1), so that the ellipse

526
C Linear algebra
Figure C.4 Manipulating different parts of the SVD of A3. a–b) Changing matrix V does not
affect the ﬁnal ellipse, but changes which directions (colors) are mapped to the minor and major
axes. c–d) Changing the ﬁrst diagonal element of L changes the length of the major axis of the
ellipse. e–f) Changing the second diagonal element of L changes the length of the minor axis. g–h)
Changing U affects the ﬁnal orientation of the ellipse.

C.6
Matrix calculus
527
becomes a circle (Figure C.3b). Finally, it rotates the result by V to get back to the
original position (Figure C.3a).
C.6
Matrix calculus
We are often called upon to take derivatives of compound matrix expressions. The deriva-
tive of a function f[a] that takes a vector as its argument and returns a scalar is a vector b
with elements
bi = ∂f
∂ai
(C.24)
The derivative of a function f[A] that returns a scalar, with respect to an M × N
matrix A will be a M × N matrix B with elements
bij = ∂f
∂aij
.
(C.25)
The derivative of a function f[a] that returns a vector with respect to vector a is a matrix
B with elements
bij = ∂fi
∂aj
.
(C.26)
where fi is the ith element of the vector returned by the function f[a].
We now provide several commonly used results for reference.
1. Derivative of linear function:
∂xT a
∂x
= a
(C.27)
∂aT x
∂x
= a
(C.28)
∂aT Xb
∂X
= abT
(C.29)
∂aT XT b
∂X
= baT .
(C.30)
2. Derivative of quadratic function:
∂bT XT Xc
∂X
=X(bcT + cbT )
(C.31)
∂(Bx + b)T C(Dx + d)
∂x
=BT C(Dx + d) + DT CT (Bx + b)
(C.32)
∂xT Bx
∂x
=(B + BT )x
(C.33)
∂bT XT DXc
∂X
=DT XbcT + DXcbT
(C.34)
∂(Xb + c)T D(Xb + c)
∂X
=(D + DT )(Xb + c)bT .
(C.35)
3. Derivative of determinant:
∂det[Y]
∂x
= det[Y]tr

Y−1 ∂Y
∂x

,
(C.36)

528
C Linear algebra
which leads to the relation
∂det[Y]
∂Y
= det[Y]Y−T .
(C.37)
4. Derivative of log determinant:
∂log[det[Y]]
∂Y
= Y−T .
(C.38)
5. Derivative of inverse:
∂Y−1
∂x
= −Y−1 ∂Y
∂x Y−1.
(C.39)
6. Derivative of trace:
∂tr[F[X]]
∂X
=
∂F[X]
∂X
T
.
(C.40)
More information about matrix calculus can be found in Petersen et al. (2006).
C.7
Common problems
In this section, we discuss several standard linear algebra problems that are found
repeatedly in computer vision.
C.7.1
Least squares problems
Many inference and learning tasks in computer vision result in least squares problems.
The most frequent context is when we use maximum likelihood methods with the normal
distribution. The least squares problem may be formulated in a number of ways. We may
be asked to ﬁnd the vector x that solves the system
Ax = b
(C.41)
in a least squares sense. Alternatively, we may be given i of smaller sets of equations of
the form
Aix = bi,
(C.42)
and again asked to solve for x. In this latter case, we form the compound matrix A =
[AT
1 ,AT
2 ...AT
I ]T and compound vector b = [bT
1 ,bT
2 ...bT
I ]T , and the problem is the
same as in Equation C.41.
We may equivalently see the same problem in an explicit least squares form,
ˆx = argmin
x
h
(Ax −b)T (Ax −b)
i
.
(C.43)
Finally, we may be presented the problem as a sum of smaller terms
ˆx = argmin
x
" I
X
i=1
(Aix −bi)T (Aix −bi)
#
,
(C.44)
in which case we form compound matrices A and b, which changes the problem back to
that in Equation C.43.

C.7
Common problems
529
To make progress, we multiply out the terms in Equation C.43
ˆx = argmin
x
h
(Ax −b)T (Ax −b)
i
.
= argmin
x

xT AT Ax −bT Ax −xT AT b + bT b

= argmin
x

xT AT Ax −2xT AT b + bT b

,
(C.45)
where we have combined two terms in the last line by noting that they are both the same:
they are transposes of one another, but they are also scalars, so they equal their own
transpose. Now we take the derivative with respect to x and equate the result to zero to
give
2AT Ax −2AT b = 0,
(C.46)
which we can rearrange to give the standard least squares result
x = (AT A)−1AT b.
(C.47)
This result can only be computed if there are at least as many rows in A as there are
unknown values in x (i.e., if the matrix A is square or portrait). Otherwise, the matrix
AT A will be singular. For implementations in Matlab, it is better to make use of the
backslash operator ‘\’ rather than explicitly implement Equation C.47.
C.7.2
Principal direction/minimum direction
We deﬁne the principal and minimal directions as
ˆb = argmax
b
[Ab]
subject to |b| = 1
ˆb = argmin
b
[Ab]
subject to |b| = 1,
(C.48)
respectively. This problem has exactly the geometric form of Figure C.2. The constraint
that |b| = 1 means that b has to lie on the circle (or sphere or hypersphere in higher
dimensions). In the principal direction problem, we are hence seeking the direction that
is mapped to the major axis of the resulting ellipse/ellipsoid. In the minimum direction
problem, we seek the direction that is mapped to the minor axis of the ellipsoid.
We saw in Figure C.4 that it is the matrix V from the singular value decomposition
of A that controls which direction is mapped to the different axes of the ellipsoid. To
solve the principal direction problem, we hence compute the SVD, A = ULVT and set
b to be the ﬁrst column of V. To solve the minimum direction problem, we set b to be
the last column of V.
C.7.3
Orthogonal Procrustes problem
The orthogonal Procrustes problem is to ﬁnd the closest linear mapping Ωbetween one
set of vectors A and another B such that Ωis an orthogonal matrix. In layman’s terms,
we seek the best Euclidean rotation (possibly including mirroring) that maps points A to
points B.
ˆΩ= argmin
Ω
[|ΩA −B|F ],
(C.49)

530
C Linear algebra
where |•|F denotes the Frobenius norm of a matrix – the sum of the square of all of the
elements. To make progress, we recall that the trace of a matrix is the sum of its diagonal
entries and so |X|F = tr[XT X], which gives the new criterion
ˆΩ= argmin
Ω
h
tr[AT A] + tr[BT B] −2tr[AT ΩT B]
i
= argmax
Ω
h
tr[AT ΩT B]
i
= argmax
Ω
h
tr[ΩT BAT ]
i
,
(C.50)
where we have used relation C.15 between the last two lines. We now compute the SVD
BAT = ULVT to get the criterion
ˆΩ= argmax
Ω
h
tr[ΩT ULVT ]
i
= argmax
Ω
h
tr[VT ΩT UL]
i
(C.51)
and notice that
tr[VT ΩT UL] = tr[ZL] =
I
X
i=1
ziilii,
(C.52)
where we deﬁned Z = VT ΩT U and used the fact that the L is a diagonal matrix, so each
entry scales the diagonal of Z on multiplication.
We note that the matrix Z is orthogonal (it is the product of three orthogonal matri-
ces). Hence, every value on the diagonal of the orthogonal matrix Z must be less than
or equal to one (the norms of each column are exactly one), and so we maximize the
criterion in Equation C.52 by choosing Z = I when the diagonal values are equal to one.
To achieve this, we set ΩT = VUT so that the overall solution is
ˆΩ= UVT .
(C.53)
A special case of this problem is to ﬁnd the closest orthogonal matrix Ωto a given
square matrix B in a least squares sense. In other words, we seek to optimize
ˆΩ= argmin
Ω
[|Ω−B|F ].
(C.54)
This is clearly equivalent to optimizing the criterion in Equation C.49 but with A = I.
It follows that the solution can be found by computing the singular value decomposition
B = ULVT and setting Ω= UVT .
C.8
Tricks for inverting large matrices
Inversion of a D × D matrix has a complexity of O(D3). In practice, this means it is
difﬁcult to invert matrices whose dimension is larger than a few thousand. Fortunately,
matrices are often highly structured, and we can exploit that structure using a number of
tricks to speed up the process.

C.8
Tricks for inverting large matrices
531
C.8.1
Diagonal and block-diagonal matrices
Diagonal matrices can be inverted by forming a new diagonal matrix, where the values on
the diagonal are reciprocal of the original values. Block diagonal matrices are matrices
of the form
A =


A1
0
...
0
0
A2
...
0
...
...
...
...
0
0
...
AN

.
(C.55)
The inverse of a block-diagonal matrix can be computed by taking the inverse of each
block separately so that
A−1 =


A−1
1
0
...
0
0
A−1
2
...
0
...
...
...
...
0
0
...
A−1
N

.
(C.56)
C.8.2
Inversion relation #1: Schur complement identity
The inverse of a matrix with sub-blocks A, B, C, and D in the top-left, top-right, bottom-
left, and bottom-right positions, respectively, can easily be shown to be
A
B
C
D
−1
=

(A −BD−1C)−1
−(A −BD−1C)−1BD−1
−D−1C(A −BD−1C)−1
D−1 + D−1C(A −BD−1C)−1BD−1

(C.57)
by multiplying the original matrix with the right-hand side and showing that the result is
the identity matrix.
This result is extremely useful when the matrix D is diagonal or block-diagonal (Fig-
ure C.5). In this circumstance, D−1 is fast to compute, and the remaining inverse quantity
(A −BD−1C)−1 is much smaller and easier to invert than the original matrix. The
quantity A −BD−1C is known as the Schur complement.
Figure C.5 Inversion relation #1. Gray regions
indicate parts of matrix with nonzero values, white
regions represent zeros.
This relation is suited
to the case where the matrix can be divided into
four submatrices A,B,C,D, and the bottom right
block is easy to invert (e.g., diagonal, block diag-
onal or structured in another way that means that
inversion is efﬁcient). After applying this relation,
the remaining inverse is the size of submatrix A.

532
C Linear algebra
C.8.3
Inversion relation #2
Consider the d × d matrix A, the k × k matrix C, and the k × d matrix B where A and
C are symmetric, positive deﬁnite matrices. The following equality holds:
(A−1 + BT C−1B)−1BT C−1 = ABT (BABT + C)−1.
(C.58)
Proof:
BT C−1BABT + BT = BT + BT C−1BABT
BT C−1(BABT + C) = (A−1 + BT C−1B)ABT .
(C.59)
Taking the inverse of both sides we get
(A−1 + BT C−1B)−1BT C−1 = ABT (BABT + C)−1,
(C.60)
as required.
This relation is very useful when B is a landscape matrix with many more columns
C than rows R. On the left-hand side, the term we must invert is of size C × C, which
might be very costly. However, on the right-hand side, the inversion is only of size R×R,
which might be considerably more cost efﬁcient.
C.8.4
Inversion relation #3: Sherman–Morrison–Woodbury
Consider the d × d matrix A, the k × k matrix C, and the k × d matrix B where A and
C are symmetric, positive deﬁnite matrices. The following equality holds:
(A−1 + BT C−1B)−1 = A −ABT (BABT + C)−1BA.
(C.61)
This is sometimes known as the matrix inversion lemma.
Proof:
(A−1 + BT C−1B)−1 = (A−1 + BT C−1B)−1(I + BT C−1BA −BT C−1BA)
= (A−1 + BT C−1B)−1  (A−1 + BT C−1B)A −BT C−1BA

= A −(A−1 + BT C−1B)−1BT C−1BA.
(C.62)
Now, applying inversion relation #2 to the term in brackets:
(A−1 + BT C−1B)−1 = A −(A−1 + BT C−1B)−1BT C−1BA
= A −ABT (BABT + C)−1BA,
(C.63)
as required.
C.8.5
Matrix determinant lemma
The matrices that we need to invert are often the covariances in the normal distribu-
tion. When this is the case, we sometimes also need to compute the determinant of the
same matrix. Fortunately, there is a direct analogy of the matrix inversion lemma for
determinants.
Consider the d×d matrix A, the k×k matrix C and the k×d matrix B where A and
C are symmetric, positive deﬁnite covariance matrices. The following equality holds:
|A−1 + BT C−1B| = |I + BABT ||C|−1|A|−1.
(C.64)

Bibliography
Aeschliman, C., Park, J., & Kak, A. C. (2010) A novel parameter estimation algorithm for
the multivariate t-distribution and its application to computer vision. In European
Conference on Computer Vision, pp. 594–607. 100, 105
Agarwal, A., & Triggs, B. (2006) Recovering 3D human pose from monocular images. IEEE
Transactions on Pattern Analysis & Machine Intelligence 28 (1): 44–48. 109, 129,
131
Agarwal, S., Snavely, N., Seitz, S. M., & Szeliski, R. (2010) Bundle adjustment in the large.
In European Conference on Computer Vision, pp. 29–42. 381
Agarwal, S., Snavely, N., Simon, I., Seitz, S. M., & Szeliski, R. (2009) Building Rome in a
day. In IEEE International Conference on Computer Vision, pp. 72–79. 380
Agarwala, A., Dontcheva, M., Agrawala, M., Drucker, S. M., Colburn, A., Curless, B.,
Salesin, D., & Cohen, M. F.
(2004)
Interactive digital photomontage.
ACM
Transactions on Graphics 23 (3): 294–302. 261
Aghajanian, J., Warrell, J., Prince, S. J. D., Li, P., Rohn, J. L., & Baum, B. (2009) Patch-based
within-object classiﬁcation. In IEEE International Conference on Computer Vision,
pp. 1125–1132. 503
Ahonen, T., Hadid, A., & Pietik¨ainen, M. (2004) Face recognition with local binary patterns.
In European Conference on Computer Vision, pp. 469–481. 451
Alahari, K., Kohli, P., & Torr, P. H. S. (2008) Reduce, reuse & recycle: Efﬁciently solving
multi-label MRFs. In IEEE Computer Vision & Pattern Recognition. 262
Allen, B., Curless, B., & Popovic, Z. (2003) The space of human body shapes: reconstruc-
tion and parameterization from range scans.
ACM Transactions on Graphics 22
(3): 587–594. 421
Aloimonos, J. Y.
(1990)
Perspective approximations.
Image and Vision Computing 8
(3): 177–192. 319
Amberg, B., Blake, A., & Vetter, T. (2009) On compositional image alignment, with an
application to active appearance models.
In IEEE Computer Vision & Pattern
Recognition, pp. 1714–1721. 421
Amini, A., Weymouth, T., & Jain, R.
(1990)
Using dynamic programming for solving
variational problems in vision. IEEE Transactions on Pattern Analysis & Machine
Intelligence 12 (9): 855–867. 223, 420
Amit, Y., & Geman, D. (1997) Shape quantization and recognition with randomized trees.
Neural Computation 9 (7): 1545–1588. 167
Amit, Y., & Kong, A. (1996) Graphical templates for model registration. IEEE Transactions
on Pattern Analysis & Machine Intelligence 18 (3): 225–236. 222

534
Bibliography
Andriluka, M., Roth, S., & Schiele, B. (2009) Pictorial structures revisited: People detection
and articulated pose estimation. In IEEE Computer Vision & Pattern Recognition.
222
Anguelov, D., Srinivasan, P., Koller, D., Thrun, S., Rodgers, J., & Davis, J. (2005) SCAPE:
Shape completion and animation of people.
ACM Transactions on Graphics 24
(3): 408–416. 418, 419, 420, 421
Arulampalam, M., Maskell, S., Gordon, N., & Clapp, T. (2002) A tutorial on particle ﬁlters
for online nonlinear/non-Gaussian Bayesian tracking. IEEE Transactions on Signal
Processing 50 (2): 174–188. 480
Avidan, S., & Shamir, A. (2007) Seam carving for content-aware image resizing.
ACM
Transactions on Graphics 26 (3): 10. 222
Ayache, N.
(1991)
Artiﬁcial Vision for Mobile Robots: Stereo Vision and Multisensory
Perception. MIT Press. 480
Babalola, K., Cootes, T., Twining, C., Petrovic, V., & Taylor, C.
(2008)
3D brain seg-
mentation using active appearance models and local regressors. In Medical Image
Computing and Computer-Assisted Intervention 2008, ed. by D. Metaxas, L. Axel,
G. Fichtinger, & G. Sz´ekely, volume 5241 of Lecture Notes in Computer Science,
pp. 401–408. Springer. 406
Bailey, T., & Durrant-Whyte, H. (2006) Simultaneous localization and mapping (SLAM):
Part II. Robotics & Automation Magazine, IEEE 13 (3): 108–117. 480
Baker, H. H., & Binford, T. O. (1981) Depth from edge and intensity-based stereo.
In
International Joint Conference on Artiﬁcial Intelligence, pp. 631–636. 222
Ballan, L., & Cortelazzo, G. M. (2008) Marker-less motion capture of skinned models in
a four camera set-up using optical ﬂow and silhouettes.
In 3D Data Processing,
Visualization and Transmission. 319
Baluja, S., & Rowley, H. A. (2003) Boosting sex identiﬁcation performance. International
Journal of Computer Vision 71 (1): 111–119. 167
Barber, D. (2012) Bayesian Reasoning and Machine Learning. Cambridge University Press.
181, 192, 223
Bartlett, M. S., Lades, H. M., & Sejnowski, T. J. (1998) Independent component represen-
tations for face recognition. In Proceedings of the SPIE Symposium on Electronic
Imaging: Science and Technology: Conference on Human Vision and Electronic
Imaging III, pp. 528–539. 450
Basri, R., Costa, L., Geiger, D., & Jacobs, D. (1998) Determining the similarity of deformable
shapes. Vision Research 38 (15–16): 2365–2385. 222
Batlle, J., Mouaddib, E., & Salvi, J. (1998) Recent progress in coded structured light as a
technique to solve the correspondence problem: A survey. Pattern Recognition 31
(7): 963–982. 319
Baumgart, B. G. (1974) Geometric modeling for computer vision. Stanford University PhD
dissertation. 319
Bay, H., Ess, A., Tuytelaars, T., & Gool, L. J. V. (2008) Speeded-up robust features (SURF).
Computer Vision and Image Understanding 110 (3): 346–359. 293
Beardsley, P. A., Reid, I. D., Zisserman, A., & Murray, D. W. (1995) Active visual navigation
using non-metric structure. In IEEE International Conference on Computer Vision,
pp. 58–65. 480

Bibliography
535
Bekios-Calfa, J., Buenaposada, J. M., & Baumela, L. (2011) Revisiting linear discrimi-
nant techniques in gender recognition. IEEE Transactions on Pattern Analysis &
Machine Intelligence 33 (4): 858–864. 167
Belhumeur, P. N., Hespanha, J. P., & Kriegman, D. J. (1997) Eigenfaces vs. Fisherfaces:
Recognition using class speciﬁc linear projection.
IEEE Transactions on Pattern
Analysis & Machine Intelligence 19 (7): 711–720. 450
Belkin, M., & Niyogi, P. (2001) Laplacian eigenmaps and spectral techniques for embedding
and clustering. In Advances in Neural Information Processing Systems, pp. 585–
591. 293
Belongie, S., Carson, C., Greenspan, H., & Malik, J. (1998) Color- and texture-based image
segmentation using EM and its application to content based image retrieval.
In
IEEE International Conference on Computer Vision, pp. 675–682. 106
Belongie, S., Malik, J., & Puzicha, J. (2002) Shape matching and object recognition using
shape contexts. IEEE Transactions on Pattern Analysis & Machine Intelligence 24
(4): 509–522. 293
Bengio, Y., & Delalleau, O. (2009) Justifying and generalizing contrastive divergence. Neural
Computation 21 (6): 1601–1621. 192
Bergtholdt, M., Cremers, D., & Sch¨orr, C. (2005) Variational segmentation with shape priors.
In Handbook of Mathematical Models in Computer Vision, ed. by Y. C. N. Paragios
& O. Faugeras, 131–144. 420
Beymer, D., & Konolige, K. (1999) Real-time tracking of multiple people using continuous
detection. In IEEE Frame Rate Workshop. 479
Birchﬁeld, S., & Tomasi, C. (1998) Depth discontinuities by pixel-to-pixel stereo. In IEEE
International Conference on Computer Vision, pp. 1073–1080. 222
Bishop, C. M. (2006) Pattern Recognition and Machine Learning. Springer, 2nd edition. 5,
16, 25, 51, 67, 166, 192, 223
Blake, A. (2006) Visual tracking: a short research roadmap.
In Handbook of Mathemat-
ical Models in Computer Vision, ed. by N. Paragios & Y. C. and. O. Faugeras,
pp. 293–307. Springer. 479
Blake, A., Curwen, R. W., & Zisserman, A. (1993) A framework for spatiotemporal control
in the tracking of visual contours.
International Journal of Computer Vision 11
(2): 127–145. 262
Blake, A., & Isard, M. (1996) The CONDENSATION algorithm – conditional density prop-
agation and applications to visual tracking.
In Advances in Neural Information
Processing Systems, pp. 361–367. 479
Blake, A., & Isard, M. (1998) Active Contours. Springer. 420, 454, 478, 479
Blake, A., Isard, M., & Reynard, D. (1995) Learning to track the visual motion of contours.
Artiﬁcial Intelligence 78 (1–2): 179–212. 262
Blake, A., Kohli, P., & Rother, C., eds. (2011) Advances in Markov Random Fields for Vision
and Image Processing. MIT Press. 262
Blanz, V., Basso, C., Poggio, T., & Vetter, T. (2003) Reanimating faces in images and video.
Computer Graphics Forum 22 (3): 641–650. 421
Blanz, V., Grother, P., Phillips, P. J., & Vetter, T. (2005) Face recognition based on frontal
views generated from non-frontal images.
In IEEE Computer Vision & Pattern
Recognition, pp. 454–461. 417, 421, 451

536
Bibliography
Blanz, V., & Vetter, T.
(1999)
A morphable model for the synthesis of 3D faces.
In
SIGGRAPH, pp. 187–194. 416, 418, 421
Blanz, V., & Vetter, T. (2003) Face recognition based on ﬁtting a 3D morphable model. IEEE
Transactions on Pattern Analysis & Machine Intelligence 25 (9): 1063–1074. 417,
421
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003) Latent Dirichlet allocation. Journal of Machine
Learning Research 3: 993–1022. 503
Bleyer, M., & Chambon, S. (2010) Does color really help in dense stereo matching?
In
International Symposiumon 3D Data Processing, Visualization and Transmission.
263
Bor Wang, S., Quattoni, A., Morency, L. P., Demirdjian, D., & Darrell, T. (2006) Hidden
conditional random ﬁelds for gesture recognition.
In IEEE Computer Vision &
Pattern Recognition, pp. 1521–1527. 223
Bosch, A., Zisserman, A., & Munoz, X. (2007) Image classiﬁcation using random forests
and ferns. In IEEE International Conference on Computer Vision. 167
Bouwmans, T., Baf, F. E., & Vachon, B. (2010) Statistical background modeling for fore-
ground detection: A survey.
In Handbook of Pattern Recognition and Computer
Vision, ed. by C. H. Chen, L. F. Pau, & P. S. P. Wang, pp. 181–199. World Scientiﬁc
Publishing. 67
Boykov, Y., & Funka Lea, G. (2006) Graph cuts and efﬁcient N-D image segmentation.
International Journal of Computer Vision 70 (2): 109–131. 263
Boykov, Y., & Jolly, M.-P. (2001) Interactive graph cuts for optimal boundary and region
segmentation of objects in N-D images.
In IEEE International Conference on
Computer Vision, pp. 105–112. 263
Boykov, Y., & Kolmogorov, V. (2004) An experimental comparison of min-cut/max-ﬂow
algorithms for energy minimization in vision.
IEEE Transactions on Pattern
Analysis & Machine Intelligence 26 (9): 1124–1137. 262
Boykov, Y., & Veksler, O. (2006) Graph cuts in vision and graphics: Theories and appli-
cations.
In Handbook of Mathematical Models in Computer Vision, ed. by Y. C.
N. Paragios & O. Faugeras, pp. 79–96. Springer. 262
Boykov, Y., Veksler, O., & Zabih, R. (1999) Fast approximate energy minimization via graph
cuts. In IEEE International Conference on Computer Vision, pp. 377–384. 254
Boykov, Y., Veksler, O., & Zabih, R. (2001) Fast approximate energy minimization via
graph cuts.
IEEE Transactions on Pattern Analysis & Machine Intelligence 23
(11): 1222–1239. 261
Brand, J., & Mason, J.
(2000)
A comparative assessment of three approaches to
pixel-level skin-detection.
In International Conference on Pattern Recognition,
pp. 1056–1059. 67
Brand, M. (2002) Charting a manifold.
In Advances in Neural Information Processing
Systems, pp. 961–968. 293
Bregler, C., & Malik, J. (1998) Tracking people with twists and exponential maps. In IEEE
Computer Vision & Pattern Recognition, pp. 8–15. 421
Breiman, L. (2001) Random forests. Machine Learning 45: 5–32. 167
Brishnapuram, B., Figueiredo, M., Carin, L., & Hartemink, A. (2005) Sparse multinomial
logistic regression: Fast algorithms and generalization bounds. IEEE Transactions
on Pattern Analysis & Machine Intelligence 27 (6): 957–968. 167

Bibliography
537
Broida, T. J., & Chellappa, R. (1986) Estimation of object motion parameters from noisy
images. IEEE Transactions on Pattern Analysis & Machine Intelligence 8 (1): 90–
99. 479
Brown, M., Hua, G., & Winder, S. A. J. (2011) Discriminative learning of local image
descriptors.
IEEE Transactions on Pattern Analysis & Machine Intelligence 33
(1): 43–57. 293
Brown, M., & Lowe, D. G. (2005) Unsupervised 3D object recognition and reconstruction in
unordered datasets. In 3D Digital Imaging and Modeling, pp. 56–63. 380
Brown, M., & Lowe, D. G. (2007) Automatic panoramic image stitching using invariant
features. International Journal of Computer Vision 74 (1): 59–73. 351
Brown, M. Z., Burschka, D., & Hager, G. D. (2003) Advances in computational stereo. IEEE
Transactions on Pattern Analysis & Machine Intelligence 25 (8): 993–1008. 264
Brubaker, M. A., Fleet, D. J., & Hertzmann, A. (2010) Physics-based person tracking using
the anthropomorphic walker.
International Journal of Computer Vision 87 (1–
2): 140–155. 421
Brunelli, R., & Poggio, T.
(1993)
Face recognition: Features versus templates.
IEEE
Transactions on Pattern Analysis & Machine Intelligence 15 (10): 1042–1052. 450
Buchanan, A. M., & Fitzgibbon, A. W. (2005) Damped Newton algorithms for matrix fac-
torization with missing data.
In IEEE Computer Vision & Pattern Recognition,
pp. 316–322. 380
Burgess, C. J. C. (2010) Dimension reduction: a guided tour. Foundations and Trends in
Machine Learning 2 (4): 275–365. 293
Byr¨od, M., & ˚Astr¨om, K. (2010) Conjugate gradient bundle adjustment.
In European
Conference on Computer Vision, pp. 114–127. 381
Cai, D., He, X., Hu, Y., Han, J., & Huang, T. S. (2007) Learning a spatially smooth subspace
for face recognition. In IEEE Computer Vision & Pattern Recognition. 450
Canny, J. (1986) A computational approach to edge detection. IEEE Transactions on Pattern
Analysis & Machine Intelligence 8 (6): 679–698. 292
Carreira-Perpi˜n´an., M. ´A., & Hinton, G. E. (2005) On contrastive divergence learning. In
Artiﬁcial Intelligence and Statistics, volume 2005, p. 17. Citeseer. 192
Caselles, V., Kimmel, R., & Sapiro, G. (1997) Geodesic active contours.
International
Journal of Computer Vision 22 (1): 61–79. 421
Chellappa, R., Sinha, P., & Phillips, P. J. (2010) Face recognition by computers and humans.
IEEE Computer 43 (2): 46–55. 450
Chen, L.-F., Liao, H.-Y. M., Ko, M.-T., Lin, J.-C., & Yu, G.-J. (2000) A new LDA-based
face recognition system which can solve the small sample size problem.
Pattern
Recognition 33 (10): 1713–1726. 450
Chen, R., & Liu, J. S. (2000) Mixture Kalman ﬁlters. Journal of the Royal Statistical Society
B. 62 (3): 493–508. 480
Chen, S. E.
(1995)
QuickTime VR: An image-based approach to virtual environment
navigation. In SIGGRAPH, pp. 29–38. 351
Cheung, G. K. M., Baker, S., & Kanade, T. (2004) Shape-from-silhouette across time. Part I:
Theory and algorithms. International Journal of Computer Vision 62 (3): 221–247.
319
Chia, A. Y. S., Rahardja, S., Rajan, D., & Leung, M. K. H. (2010) Object recognition
by discriminative combinations of line segments and ellipses. In IEEE Computer
Vision & Pattern Recognition, pp. 2225–2232. 421

538
Bibliography
Chittajallu, D. R., Shah, S. K., & Kakadiaris, I. A. (2010) A shape-driven MRF model for
the segmentation of organs in medical images. In IEEE Computer Vision & Pattern
Recognition, pp. 3233–3240. 263
Cho, Y., Lee, J., & Neumann, U. (1998) A multi-ring color ﬁducial system and an intensity-
invariant detection method for scalable ﬁducial-tracking augmented reality.
In
International Workshop on Augmented Reality, pp. 147–165. 350
Choi, S., Kim, T., & Yu, W. (2009) Performance evaluation of RANSAC family. In British
Machine Vision Conference. BMVA Press, pp. 110–119. 350
Chum, O., Philbin, J., Sivic, J., Isard, M., & Zisserman, A. (2007) Total recall: Auto-
matic query expansion with a generative feature model for object retrieval. In IEEE
International Conference on Computer Vision, pp. 1–8. 504
Chum, O., Werner, T., & Matas, J. (2005) Two-view geometry estimation unaffected by a
dominant plane.
In IEEE Computer Vision & Pattern Recognition, pp. 772–779.
350
Claus, D., & Fitzgibbon, A. W. (2005) A rational function lens distortion model for general
cameras. In IEEE Computer Vision & Pattern Recognition, pp. 213–219. 319
Cohen, L. (1991) On active contour models and balloons. CGVIP: Image Understanding 53
(2): 211–218. 420
Cootes, T. F., Edwards, G. J., & Taylor, C. J. (2001) Active appearance models.
IEEE
Transactions on Pattern Analysis & Machine Intelligence 23 (6): 681–685. 421
Cootes, T. F., & Taylor, C. J. (1997) A mixture model for representing shape variation. In
British Machine Vision Conference. BMVG Press, pp. 110–119. 421
Cootes, T. F., Taylor, C. J., Cooper, D. H., & Graham, J. (1995) Active shape models – their
training and application. Computer Vision & Image Understanding 61 (1): 38–59.
420, 421
Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2001) Introduction to Algorithms.
MIT Press, 2nd edition. 219, 262
Coughlan, J., Yuille, A., English, C., & Snow, D. (2000) Efﬁcient deformable template
detection and localization without user interaction.
Computer Vision & Image
Understanding 78 (3): 303–319. 222
Cristianini, M., & Shawe-Taylor, J. (2000) An Introduction to Support Vector Machines.
Cambridge University Press. 167
Csurka, G., Dance, C., Fan, L., Williamowski, J., & Bray, C. (2004) Visual categorization
with bags of keypoints. In ECCV International Workshop on Statistical Learning in
Computer Vision. 166, 486, 503
Cuzzolin, F. (2006) Using bilinear models for view-invariant action and identity recognition.
In IEEE Computer Vision & Pattern Recognition, pp. 1701–1708. 451
Dalal, N., & Triggs, B. (2005) Histograms of oriented gradients for human detection.
In
IEEE Computer Vision & Pattern Recognition, pp. 886–893. 293
Davison, A. J., Reid, I. D., Molton, N., & Stasse, O. (2007) MonoSLAM: Real-time single
camera SLAM. IEEE Transactions on Pattern Analysis & Machine Intelligence 29
(6): 1052–1067. 477, 478, 479, 480
de Aguiar, E., Stoll, C., Theobalt, C., Ahmed, N., Seidel, H.-P., & Thrun, S. (2008) Perfor-
mance capture from sparse multi-view video. ACM Transactions on Graphics 27
(3): 98:1–98:10. 319

Bibliography
539
De La Gorce, M., Paragios, N., & Fleet, D. J. (2008) Model-based hand tracking with texture,
shading and self-occlusions. In IEEE Computer Vision & Pattern Recognition.
De La Torre, F.
(2011)
A least-squares framework for component analysis.
IEEE
Transactions on Pattern Analysis & Machine Intelligence. 293, 450
De Ridder, D., & Franc, V. (2003) Robust subspace mixture models using t-distributions. In
British Machine Vision Conference, pp. 319–328. 106
Delaitre, V., Laptev, I., & Sivic, J. (2010) Recognizing human actions in still images: A
study of bag-of-features and part-based representations. In British Machine Vision
Conference. 504
Delong, A., & Boykov, Y. (2009) Globally optimal segmentation of multi-region objects. In
IEEE International Conference on Computer Vision, pp. 285–292. 263
Dempster, A. P., Laird, M. N., & Rubin, D. B. (1977) Maximum likelihood from incomplete
data via the EM algorithm.
Journal of the Royal Statistical Society, Series B 39
(1): 1–38. 105
Deng, J., Berg, A., Li, K., & Fei-Fei, L. (2010) What does classifying more than 10,000
image categories tell us?
European Conference on Computer Vision, pp. 71–84.
504
Deng, Y., & Lin, X. (2006) A fast line segment based stereo algorithm using tree dynamic
programming. In European Conference on Computer Vision, pp. 201–212. 222
Deutscher, J., Blake, A., & Reid, I. D. (2000) Articulated body motion capture by annealed
particle ﬁltering. In IEEE Computer Vision & Pattern Recognition, pp. 2126–2133.
421
Devernay, F., & Faugeras, O. D. (2001) Straight lines have to be straight. Mach. Vis. Appl.
13 (1): 14–24. 319
Doll´ar, P., Tu, Z., & Belongie, S. (2006) Supervised learning of edges and object boundaries.
In IEEE Computer Vision & Pattern Recognition, pp. 1964–1971. 166, 292
Domke, J., Karapurkar, A., & Aloimonos, Y. (2008) Who killed the directed model? In IEEE
Computer Vision & Pattern Recognition. 263
Duda, R. O., Hart, P. E., & Stork, D. G. (2001) Pattern Classiﬁcation. John Wiley and Sons,
2nd edition. 67
Durrant-Whyte, H., & Bailey, T. (2006) Simultaneous localization and mapping (SLAM):
Part I. Robotics & Automation Magazine, IEEE 13 (2): 99–110. 480
Durrant-Whyte, H., Rye, D., & Nebot, E. (1996) Localisation of automatic guided vehicles.
In International Symposium on Robotics Research, pp. 613–625. 480
Durrant-Whyte, H. F. (1988) Uncertain geometry in robotics. IEEE Transactions on Robot
Automation 4 (1): 23–31. 480
Edwards, G. J., Taylor, C. J., & Cootes, T. F.
(1998)
Interpreting face images using
active appearance models.
In IEEE International Conference on Automatic Face
& Gesture Recognition, pp. 300–305. 450
Efros, A. A., & Freeman, W. T. (2001) Image quilting for texture synthesis and transfer. In
SIGGRAPH, pp. 341–346. 258, 263
Efros, A. A., & Leung, T. K. (1999) Texture synthesis by non-parametric sampling. In IEEE
International Conference on Computer Vision, pp. 1033–1038. 263
Eichner, M., & Ferrari, V. (2009) Better appearance models for pictorial structures. In British
Machine Vision Conference. 222

540
Bibliography
Elder, J. H. (1999) Are edges incomplete?
International Journal of Computer Vision 34
(2–3): 97–122. 279, 292
Elgammal, A. (2011) Figure-ground segmentation – Pixel-based. In Guide to Visual Analysis
of Humans: Looking at People, ed. by T. Moeslund, A. Hilton, Kr¨uger, & L. Sigal.
Springer. 67
Elgammal, A., Harwood, D., & Davis, L. (2000) Non-parametric model for background
subtraction. In European Conference on Computer Vision, pp. 751–767. 68
Elgammal, A. M., & Lee, C.-S. (2004) Separating style and content on a nonlinear manifold.
In IEEE Computer Vision & Pattern Recognition, pp. 478–485. 451
Engels, C., Stew´enius, H., & Nist´er, D. (2006) Bundle adjustment rules. Photogrammetric
Computer Vision . 381
Everingham, M., Sivic, J., & Zisserman, A. (2006) Hello! My name is Buffy – Automatic
naming of characters in TV video. In British Machine Vision Conference, pp. 889–
908. 222, 450
Everingham, M., Van Gool, L., Williams, C., Winn, J., & Zisserman, A. (2010) The PASCAL
visual object classes (VOC) challenge. International Journal of Computer Vision
88 (2): 303–338. 503
Faugeras, O. (1993) Three-Dimensional Computer Vision: A Geometric Viewpoint.
MIT
Press. 319
Faugeras, O., Luong, Q., & Papadopoulo, T. (2001) The Geometry of Multiple Images. MIT
PRESS. 319, 380
Faugeras, O. D. (1992) What can be seen in three dimensions with an uncalibrated stereo rig.
In European Conference on Computer Vision, pp. 563–578. 380
Faugeras, O. D., & Keriven, R. (1998) Variational principles, surface evolution, PDEs, level
set methods, and the stereo problem.
IEEE Transactions on Image Processing 7
(3): 336–344. 381
Faugeras, O. D., Luong, Q.-T., & Maybank, S. J.
(1992)
Camera self-calibration:
Theory and experiments.
In European Conference on Computer Vision,
pp. 321–334. 380
Felzenszwalb, P., & Zabih, R. (2011) Dynamic programming and graph algorithms in com-
puter vision.
IEEE Transactions on Pattern Analysis & Machine Intelligence 33
(4): 721–740. 221, 222, 262
Felzenszwalb, P. F., Girshick, R. B., McAllester, D. A., & Ramanan, D. (2010) Object
detection with discriminatively trained part-based models. IEEE Transactions on
Pattern Analysis & Machine Intelligence 32 (9): 1627–1645. 222, 223
Felzenszwalb, P. F., & Huttenlocher, D. P. (2005) Pictorial structures for object recognition.
International Journal of Computer Vision 61 (1): 55–79. 219, 220, 221, 222, 421
Felzenszwalb, P. F., & Veksler, O. (2010) Tiered scene labeling with dynamic programming.
In IEEE Computer Vision & Pattern Recognition. 222, 263
Ferencz, A., Learned-Miller, E. G., & Malik, J. (2008) Learning to locate informative features
for visual identiﬁcation. International Journal of Computer Vision 77 (1–3): 3–24.
451
Fischler, M., & Bolles, R. (1981) Random sample consensus: a paradigm for model ﬁtting
with application to image analysis and automated cartography. Communications of
the ACM 24 (6): 381–395. 350

Bibliography
541
Fischler, M. A., & Erschlager, R. A. (1973) The representation and matching of pictorial
structures. IEEE Transactions on Computers 22 (1): 67–92. 222
Fitzgibbon, A. W., & Zisserman, A. (1998) Automatic camera recovery for closed or open
image sequences. In European Conference on Computer Vision, pp. 311–326. 380
Ford, L., & Fulkerson, D. (1962) Flows in Networks. Princeton University Press. 262
Forss´en, P.-E., & Lowe, D. G.
(2007)
Shape descriptors for maximally stable extremal
regions. In IEEE International Conference on Computer Vision, pp. 1–8. 293
F¨orstner, W.
(1986)
A feature-based correspondence algorithm for image matching.
International Archives of Photogrammetry and Remote Sensing 26 (3): 150–166.
292
Forsyth, D. A., Arikan, O., Ikemoto, L., O’Brien, J., & Ramanan, D.
(2006)
Com-
putational studies of human motion:
Part 1, Tracking and motion synthesis.
Foundations and Trends in Computer Graphics and Computer Vision 1 (2/2):
77–254. 421
Frahm, J.-M., Georgel, P. F., Gallup, D., Johnson, T., Raguram, R., Wu, C., Jen, Y.-H., Dunn,
E.,
Clipp, B., & Lazebnik, S. (2010) Building Rome on a cloudless day.
In
European Conference on Computer Vision, pp. 368–381. 378
Frahm, J.-M., & Pollefeys, M. (2006) RANSAC for (quasi-)degenerate data (QDEGSAC).
In IEEE Computer Vision & Pattern Recognition, pp. 453–460. 350
Franco, J.-S., & Boyer, E.
(2005)
Fusion of multi-view silhouette cues using a
space occupancy grid.
In IEEE International Conference on Computer Vision,
pp. 1747–1753. 319
Freeman, W. T., Pasztor, E. C., & Carmichael, O. T. (2000) Learning low-level vision.
International Journal of Computer Vision 40: 25–47. 223, 257, 261
Freifeld, O., Weiss, A., Zufﬁ, S., & Black, M. J. (2010) Contour people: A parameter-
ized model of 2D articulated human shape.
In IEEE Computer Vision & Pattern
Recognition, pp. 639–646. 420
Freiman, M., Kronman, A., Esses, S. J., Joskowicz, L., & Sosna, J. (2010) Non-parametric
iterative model constraint graph min-cut for automatic kidney segmentation.
In
Medical Image Computing and Computer-Assisted Intervention, pp. 73–80. 263
Freund, Y., & Schapire, R.
(1996)
Experiments with a new boosting algorithm.
In
International Conference on Machine Learning, pp. 148–156. 167
Freund, Y., & Schapire, R. E. (1995) A decision-theoretic generalization of on-line learning
and an application to boosting. In Computational Learning Theory: Eurocolt ’95,
pp. 23–37. 167
Frey, B., Kschischang, F., Loeliger, H., & Wiberg, N. (1997) Factor graphs and algorithms.
In Allerton Conference on Communication, Control and Computing. 223
Frey, B. J., & Jojic, N. (1999a) Estimating mixture models of images and inferring spa-
tial transformations using the EM algorithm. In IEEE Computer Vision & Pattern
Recognition, pp. 416–422. 106
Frey, B. J., & Jojic, N. (1999b) Transformed component analysis: Joint estimation of spa-
tial transformations and image components. In IEEE Computer Vision & Pattern
Recognition, pp. 1190–1196. 106
Friedman, J., Hastie, T., & Tibshirani, R. (2000) Additive logistic regression: A statistical
view of boosting. Annals of Statistics 28 (2): 337–407. 167

542
Bibliography
Friedman, J. H.
(1999)
Greedy function approximation: A gradient boosting machine.
Technical report, Department of Statistics, Stanford University. 167
Friedman, N., & Russell, S. J. (1997) Image segmentation in video sequences: A probabilistic
approach. In Uncertainty in Artiﬁcial Intelligence, pp. 175–181. 68
Fua, P., & Leclerc, Y. G. (1995) Object-centered surface reconstruction: Combining multi-
image stereo and shading. International Journal of Computer Vision 16 (1): 35–56.
381
Fusiello, A., Trucco, E., & Verri, A. (2000) A compact algorithm for rectiﬁcation of stereo
pairs. Machine Vision and Applications 12 (1): 16–22. 380
Gao, X.-S., Hou, X., Tang, J., & Cheng, H.-F. (2003) Complete solution classiﬁcation for
the perspective-three-point problem.
IEEE Transactions on Pattern Analysis &
Machine Intelligence 25 (8): 930–943. 319
Gavrila, D., & Davis, L. S. (1996) 3-D model-based tracking of humans in action: A multi-
view approach. In IEEE Computer Vision & Pattern Recognition, pp. 73–80. 421
Geiger, B., Ladendorf, B., & Yuille, A. (1992) Occlusions and binocular stereo. In European
Conference on Computer Vision, pp. 425–433. 222
Geiger, D., Gupta, A., Costa, L. A., & Vlontzos, J. (1995) Dynamic-programming for detect-
ing, tracking and matching deformable contours.
IEEE Transactions on Pattern
Analysis & Machine Intelligence 17 (3): 294–302. 223
Gelb, A. (1974) Applied Optimal Estimation. MIT Press. 479
Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (2004) Bayesian Data Analysis.
Chapman and Hall / CRC. 25, 41
Geman, S., & Geman, D.
(1984)
Stochastic relaxation, Gibbs distributions, and the
Bayesian restoration of images. IEEE Transactions on Pattern Analysis & Machine
Intelligence 6 (6): 721–741. 260
Geyer, C., & Daniilidis, K. (2001) Catadioptric projective geometry. International Journal
of Computer Vision 45 (3): 223–243. 319
Ghahramani, Z. (2001) An introduction to hidden Markov models and Bayesian networks.
In Hidden Markov Models: Applications in Computer Vision, ed. by B. H. Juang,
pp. 9–42. World Scientiﬁc Publishing. 223
Ghahramani, Z., & Hinton, G. (1996a) Parameter estimation for linear dynamical systems.
Technical Report CRG–TR–96–2, Department of Computer Science, University of
Toronto. 480
Ghahramani, Z., & Hinton, G. (1996b) Switching state-space models.
Technical Report
CRG–TR–96–3, Department of Computer Science, University of Toronto. 480
Ghahramani, Z., & Hinton, G. E. (1996c) The EM algorithm for mixtures of factor analyzers.
Technical Report CRG–TR–96–1, University of Toronto. 105
Goldberg, A., & Tarjan, R. (1988) A new approach to the maximum ﬂow problem. Journal
of the Association for Computing Machinery 35 (4): 921–940. 262
Golomb, B. A., Lawrence, D. T., & Sejnowski, T. (1990) SEXNET: a neural network identi-
ﬁes sex from human faces. In Advances in Neural Information Processing Systems,
pp. 572–579. 167
Gong, M., & Yang, Y. H. (2005) Near real-time reliable stereo matching using programmable
graphics hardware. In IEEE Computer Vision & Pattern Recognition, pp. 924–931.
222

Bibliography
543
Gonzalez, R. C., & Woods, R. E. (2002) Digital Image Processing.
Prentice Hall, 2nd
edition. 292
Gower, J. C., & Dijksterhuis, G. B. (2004) Procrustes Problems. Oxford University Press.
350
Grady, L. (2006) Random walks for image segmentation.
IEEE Transactions on Pattern
Analysis & Machine Intelligence 28 (11): 1768–1783. 263
Grauman, K., & Darrell, T. (2005) The pyramid match kernel: Discriminative classiﬁcation
with sets of image features. In IEEE International Conference on Computer Vision,
pp. 1458–1465. 503
Grauman, K., Shakhnarovich, G., & Darrell, T. (2003) A Bayesian approach to image-
based visual hull reconstruction. In IEEE Computer Vision & Pattern Recognition,
pp. 187–194. 319
Greig, D. M., Porteous, B. T., & Seheult, A. H. (1989) Exact maximum a posteriori estimation
for binary images. Journal of the Royal Statistical Society. Series B 51 (2): 271–279.
261
Grimes, D. B., Shon, A. P., & Rao, R. P. N.
(2003)
Probabilistic bilinear models for
appearance-based vision. In IEEE International Conference on Computer Vision,
pp. 1478–1485. 451
Gross, R., Matthews, I., & Baker, S. (2002) Eigen light-ﬁelds and face recognition across
pose. In Automated Face and Gestured Recongnition, pp. 3–9. 451
Guesebroek, J. M., Bughouts, G. J., & Smeulders, A. W. M. (2005) The Amsterdam library
of object images. International Journal of Computer Vision 61 (1): 103–112. 100,
101
Guillaumin, M., Verbeek, J. J., & Schmid, C. (2009) Is that you? Metric learning approaches
for face identiﬁcation.
In IEEE International Conference on Computer Vision,
pp. 498–505. 451
Guivant, J. E., & Nebot, E. M. (2001) Optimization of the simultaneous localization and map-
building algorithm for real-time implementation. IEEE Transactions on Robotics
17 (3): 242–257. 480
Handa, A., Chli, M., Strasdat, H., & Davison, A. J. (2010) Scalable active matching. In IEEE
Computer Vision & Pattern Recognition, pp. 1546–1553. 480
Harris, C. (1992) Tracking with rigid objects.
In Active Vision, ed. by A. Blake & A. L.
Yuille, pp. 59–73. 350
Harris, C., & Stephens, M. J. (1988) A combined corner and edge detector. In Alvey Vision
Conference, pp. 147–152. 292, 380
Harris, C. G., & Pike, J. M. (1987) 3D positional integration from image sequences. In Alvey
Vision Conference, pp. 233–236. 480
Hartley, R. I. (1992) Estimation of relative camera positions for uncalibrated cameras. In
European Conference on Computer Vision, pp. 579–587. 380 380
Hartley, R. I. (1994) Projective reconstruction from line correspondence. In IEEE Computer
Vision & Pattern Recognition, pp. 579–587.
Hartley, R. I. (1997) In defense of the eight-point algorithm. IEEE Transactions on Pattern
Analysis & Machine Intelligence 19 (6): 580–593. 364, 380
Hartley, R. I., & Gupta, R. (1994) Linear pushbroom cameras. In European Conference on
Computer Vision, pp. 555–566. 319

544
Bibliography
Hartley, R. I., & Zisserman, A.
(2004)
Multiple View Geometry in Computer Vision.
Cambridge University Press, 2nd edition. 5, 306, 319, 338, 350, 360, 370, 380
He, X., Yan, S., Hu, Y., Niyogi, P., & Zhang, H. (2005) Face recognition using Lapla-
cianfaces.
IEEE Transactions on Pattern Analysis & Machine Intelligence 27
(3): 328–340. 450
He, X., Zemel, R. S., & Carreira-Perpi˜n´an, M. ´A. (2004) Multiscale conditional random ﬁelds
for image labeling. In IEEE Computer Vision & Pattern Recognition, pp. 695–702.
166, 167
He, X., Zemel, R. S., & Ray, D. (2006) Learning and incorporating top-down cues in image
segmentation. In European Conference on Computer Vision, pp. 338–351. 167
Heap, T., & Hogg, D. (1996) Towards 3D hand tracking using a deformable model. In IEEE
International Conference on Automatic Face & Gesture Recognition, pp. 140–145.
422
Heeger, D., & Bergen, J. (1995) Pyramid-based texture analysis/synthesis.
In Computer
Graphics and Interactive Techniques, pp. 229–238. ACM. 263
Heess, N., Williams, C. K. I., & Hinton, G. E. (2009) Learning generative texture models
with extended ﬁelds of experts. In British Machine Vision Conference. 263
Hern´andez, C., & Schmitt, F. (2004) Silhouette and stereo fusion for 3D object modeling.
Computer Vision & Image Understanding 96 (3): 367–392. 381
Hinton, G. E. (2002) Training products of experts by minimizing contrastive divergence.
Neural Computation 14 (8): 1771–1800. 192
Hirschm¨uller, H. (2005) Accurate and efﬁcient stereo processing by semi-global matching
and mutual information. In IEEE Computer Vision & Pattern Recognition, pp. 807–
814. 264
Hirschm¨uller, H., & Scharstein, D. (2009) Evaluation of stereo matching costs on images
with radiometric differences.
IEEE Transactions on Pattern Analysis & Machine
Intelligence 31 (9): 1582–1599. 263
Hofmann, T. (1999) Probabilistic latent semantic analysis.
In Uncertainty in Artiﬁcial
Intelligence, pp. 289–296. 503
Hogg, D. (1983) Model-based vision: a program to see a walking person. Image and Vision
Computing 1 (1): 5–20. 421
Hoiem, D., Efros, A., & Hebert, M. (2005) Automatic photo pop-up. ACM Transcations on
Graphics (SIGGRAPH) 24 (3): 577–584. 164
Hoiem, D., Efros, A. A., & Hebert, M. (2007) Recovering surface layout from an image.
International Journal of Computer Vision 75 (1): 151–172. 163, 165, 166
Horn, B. K. P. (1990) Relative orientation.
International Journal of Computer Vision 4
(1): 59–78. 380
Horn, E., & Kiryati, N. (1999) Toward optimal structured light patterns. Image and Vision
Computing 17 (2): 87–97. 319
Horprasert, T., Harwood, D., & Davis, L. S. (2000) A robust background subtraction and
shadow detection. In Asian Conference on Computer Vision, pp. 983–988. 68
Hsu, R. L., Abdel-Mottaleb, M., & Jain, A. K. (2002) Face detection in color images. IEEE
Transactions on Pattern Analysis & Machine Intelligence 24 (5): 696–707. 67
Huang, C., Ai, H., Li, Y., & Lao, S. (2007a) High-performance rotation invariant multi-view
face detection. IEEE Transactions on Pattern Analysis & Machine Intelligence 29
(4): 671–686. 167

Bibliography
545
Huang, G. B., Ramesh, M., Berg, T., & Learned-Miller, E. (2007b) Labeled faces in the wild:
A database for studying face recognition in unconstrained environments. Technical
Report Technical Report 07–49, University of Massachusetts, Amherst. 447, 451
Huang, T. S., & Faugeras, O. D. (1989) Some properties of the E matrix in two-view motion
estimation.
IEEE Transactions on Pattern Analysis & Machine Intelligence 11
(12): 1310–1312. 380
Huang, Y., Liu, Q., & Metaxas, D. N. (2011) A component-based framework for generalized
face alignment. IEEE Transactions on Systems, Man, and Cybernetics, Part B 41
(1): 287–298. 414, 421
Huber, P. J. (2009) Robust Statistics. John Wiley and Sons, 2nd edition. 350
Humayun, A., Oisin, M. A., & Brostow, G. J., (2011) Learning to Find Occlusion Regions,
In IEEE Computer Vision and Pattern Recognition.
Ioffe, S. (2006) Probabilistic linear discriminant analysis.
In European Conference on
Computer Vision, pp. 531–542. 450
Isack, H., & Boykov, Y. (2012) Energy-based geometric multi-model ﬁtting. International
Journal of Computer Vision 97 (2), pp. 123–147. 261, 347, 350
Isard, M., & Blake, A.
(1998)
A mixed-state CONDENSATION tracker with auto-
matic model-switching.
In IEEE International Conference on Computer Vision,
pp. 107–112. 480
Ishikawa, H. (2003) Exact optimization for Markov random ﬁelds with convex priors. IEEE
Transactions on Pattern Analysis & Machine Intelligence 25 (10): 1333–1336. 261
Ishikawa, H. (2009) Higher order clique reduction in binary graph cut. In IEEE Computer
Vision & Pattern Recognition. 263
Jazwinski, A. H. (1970) Stochastic Processes and Filtering Theory. Academic Press. 479
Jegou, H., Douze, M., & Schmid, C. (2008) Recent advances in large scale image search. In
Emerging Trends in Visual Computing, pp. 305–326. 503
J´egou, H., Douze, M., & Schmid, C. (2009) Packing bag-of-features. In IEEE International
Conference on Computer Vision, pp. 2357–2364. 504
Jeong, Y., Nist´er, D., Steedly, D., Szeliski, R., & Kweon, I.-S. (2010) Pushing the envelope
of modern methods for bundle adjustment.
In IEEE Computer Vision & Pattern
Recognition, pp. 1474–1481. 381
Jiang, H., & Martin, D. R. (2008) Global pose estimation using non-tree models. In IEEE
Computer Vision & Pattern Recognition. 222
Jojic, N., & Frey, B. J. (2001) Learning ﬂexible sprites in video layers. In IEEE Computer
Vision & Pattern Recognition, pp. 199–206. 106
Jojic, N., Frey, B. J., & Kannan, A. (2003) Epitomic analysis of appearance and shape. In
IEEE International Conference on Computer Vision, pp. 34–41. 106
Jones, E., & Soatto, S. (2005) Layered active appearance models.
In IEEE International
Conference on Computer Vision, pp. 1097–1102. 421
Jones, M. J., & Rehg, J. M. (2002) Statistical color models with application to skin detection.
International Journal of Computer Vision 46 (1): 81–96. 67, 105
Jordan, M. I. (2004) Graphical models. Statistical science 19 (1): 140–155. 192
Jordan, M. I., & Jacobs, R. A. (1994) Hierarchical mixtures of experts and the EM algorithm.
Neural Computation 6 (2): 181–214. 168
Juan, O., & Boykov, Y. (2006) Active graph cuts.
In IEEE Computer Vision & Pattern
Recognition, pp. 1023–1029. 262

546
Bibliography
Julier, S., & Uhlmann, J. (1997) A new extension of the Kalman ﬁlter to nonlinear systems. In
International Symposium on Aerospace/Defense Sensing, Simulation and Controls,
volume 3, p. 26. 479
Kadir, T., & Brady, M. (2001) Saliency, scale and image description. International Journal
of Computer Vision 45 (2): 83–105. 292
Kakumanu, P., Makrogiannis, S., & Bourbakis, N. G.
(2007)
A survey of skin-colour
modeling and detection methods. Pattern Recognition 40 (3): 1106–1122. 67
Kalman, R. E. (1960) A new approach to linear ﬁltering and prediction problems. Journal of
Basic Engineering 82 (1): 35–45. 479
Kalman, R. E., & Bucy, R. S. (1961) New results in linear ﬁltering and prediction theory.
Transactions of the American Society for Mechanical Engineering D 83 (1): 95–108.
479
Kanade, T., Rander, P., & Narayanan, P. J. (1997) Virtualized reality: Constructing virtual
worlds from real scenes. IEEE MultiMedia 4 (1): 34–47. 319
Kass, M., Witkin, A., & Terzopolous, D.
(1987)
Snakes:
Active contour models.
International Journal of Computer Vision 1 (4): 321–331. 223, 420
Kato, H., & Billinghurst, M. (1999) Marker tracking and HMD calibration for a video-based
augmented reality conferencing system. In International Workshop on Augmented
Reality, pp. 85–94. 350
Kato, H., Billinghurst, M.,
Poupyrev, I., Imamoto, K., & Tachibana, K. (2000) Virtual
object manipulation on a table-top AR environment. In International Symposium
on Augmented Reality, pp. 111–119. 350
Kendall, D. G. (1984) Shape manifolds, Procrustean metrics, and complex projective spaces.
Bulletin of the London Mathematical Society 16 (2): 81–121. 388
Khan, Z., & Dellaert, F. (2004) Robust generative subspace modelling: The subspace t-
distribution. Technical Report GIT–GVU–04–11, Georgia Institute of Technology.
105
Kim, J. C., Lee, K. M., Choi, B., & Lee, S. U. (2005) A dense stereo matching using two pass
dynamic programming with generalized control points. In IEEE Computer Vision
& Pattern Recognition, pp. 1075–1082. 222
Klein, G., & Murray, D. (2007) Parallel tracking and mapping for small AR workspaces.
In Proc. Sixth IEEE and ACM International Symposium on Mixed and Augmented
Reality (ISMAR’07). 480
Kohli, P., Kumar, M. P., & Torr, P. H. S. (2009a) P3 & beyond: Move making algorithms for
solving higher order functions. IEEE Transactions on Pattern Analysis & Machine
Intelligence 31 (9): 1645–1656. 263
Kohli, P., Ladicky, L., & Torr, P. H. S.
(2009b)
Robust higher order potentials for
enforcing label consistency.
International Journal of Computer Vision 82 (3):
302–324. 263
Kohli, P., & Torr, P. H. S. (2005) Efﬁciently solving dynamic Markov random ﬁelds using
graph cuts. In IEEE International Conference on Computer Vision, pp. 922–929.
262
Kolev, K., & Cremers, D. (2008) Integration of multiview stereo and silhouettes via con-
vex functionals on convex domains. In European Conference on Computer Vision,
pp. 752–765. 381
Koller, D., & Friedman, N. (2009) Probabilistic Graphical Models. MIT Press. 181, 192,
223

Bibliography
547
Koller, D., Klinker, G., Rose, E., Breen, D., Whitaker, R., & Tuceryan, M. (1997) Real-
time vision-based camera tracking for augmented reality applications.
In ACM
Symposium on Virtual Reality Software and Technology, pp. 87–94. 350
Kolmogorov, V.
(2006)
Convergent tree-reweighted message passing for energy mini-
mization.
IEEE Transactions on Pattern Analysis & Machine Intelligence 28
(10): 1568–1583. 263
Kolmogorov, V., Criminisi, A., Blake, A., Cross, G., & Rother, C. (2006) Probabilistic fusion
of stereo with color and contrast for bi-layer segmentation. IEEE Transactions on
Pattern Analysis & Machine Intelligence 28 (9): 1480–1492. 261
Kolmogorov, V., & Rother, C. (2007) Minimizing non-submodular graph functions with
graph-cuts – A review.
IEEE Transactions on Pattern Analysis & Machine
Intelligence 29 (7): 1274–1279. 262, 263
Kolmogorov, V., & Zabih, R. (2001) Computing visual correspondence with occlusions via
graph cuts. In IEEE International Conference on Computer Vision, pp. 508–515.
255, 261, 263
Kolmogorov, V., & Zabih, R. (2002) Multi-camera scene reconstruction via graph cuts. In
European Conference on Computer Vision, pp. 82–96. 261
Kolmogorov, V., & Zabih, R. (2004) What energy functions can be minimized via graph cuts?
IEEE Transactions on Pattern Analysis & Machine Intelligence 26 (2): 147–159.
262
Komodakis, N., Tziritas, G., & Paragios, N. (2008) Performance vs computational efﬁciency
for optimizing single and dynamic MRFs: Setting the state of the art with primal-
dual strategies. Computer Vision & Image Understanding 112 (1): 14–29. 262
Kotz, S., & Nadarajah, S.
(2004)
Multivariate t Distributions and Their Applications.
Cambridge University Press. 105
Kschischang, F. R., Frey, B., & Loeliger, H. A. (2001) Factor graphs and the sum-product
algorithm. IEEE Transactions on Information Theory 47 (2): 498–519. 223
Kumar, M. P., Torr, P., & Zisserman, A. (2004) Extending pictorial structures for object
recognition. In British Machine Vision Conference, pp. 789–798. 222
Kumar, M. P., Torr, P. H. S., & Zisserman, A. (2005) OBJ CUT. In IEEE Computer Vision
& Pattern Recognition, pp. 18–25. 263
Kumar, M. P., Veksler, O., & Torr, P. H. S. (2011) Improved moves for truncated convex
models. Journal of Machine Learning Research 12: 31–67. 262
Kumar, N., Belhumeur, P., & Nayar, S. K. (2008) Face tracer: A search engine for large
collections of images with faces.
In European Conference on Computer Vision.
166, 167
Kumar, N., Berg, A. C., Belhumeur, P. N., & Nayar, S. K. (2009) Attribute and simile
classiﬁers for face veriﬁcation.
In IEEE International Conference on Computer
Vision, pp. 365–372. 451
Kumar, S., & Hebert, M. (2003) Discriminative random ﬁelds: A discriminative framework
for contextual interaction in classiﬁcation.
In IEEE International Conference on
Computer Vision, pp. 1150–1159. 261
Kutulakos, K. N., & Seitz, S. M. (2000) A theory of shape by space carving. International
Journal of Computer Vision 38 (3): 199–218. 381
Kwatra, V., Sch¨odl, A., Essa, I., Turk, G., & Bobick, A. (2003) Graphcut textures: Image
and video synthesis using graph cuts. ACM Transactions on Graphics (SIGGRAPH
2003) 22 (3): 277–286. 261, 263

548
Bibliography
Lanitis, A., Taylor, C. J., & Cootes, T. F. (1997) Automatic interpretation and coding of face
images using ﬂexible models. IEEE Transactions on Pattern Analysis & Machine
Intelligence 19 (7): 743–756. 421
Laptev, I., Marszałek, M., Schmid, C., & Rozenfeld, B. (2008) Learning realistic human
actions from movies. In IEEE Computer Vision & Pattern Recognition. 501, 502,
503, 504
Laurentini, A. (1994) The visual hull concept for silhouette-based image understanding.
IEEE Transactions on Pattern Analysis & Machine Intelligence 16 (2): 150–162.
319
Lawrence, N. D. (2004) Probabilistic non-linear principal component analysis with Gaussian
process latent variable models. Technical Report CS–04–08, University of Shefﬁeld.
105, 293
Lawrence, N. D. (2005) Probabilistic non-linear principal component analysis with Gaussian
process latent variable models.
Journal of Machine Learning Research 6: 1783–
1816. 421
Lazebnik, S., Schmid, C., & Ponce, J. (2006) Beyond bags of features: Spatial pyramid
matching for recognizing natural scene categories.
In IEEE Computer Vision &
Pattern Recognition, pp. 2169–2178. 503
Lee, J., Moghaddam, B., Pﬁster, H., & Machiraju, R. (2005) A bilinear illumination model
for robust face recognition. In IEEE International Conference on Computer Vision,
pp. 1177–1184. 451
Lempitsky, V., Blake, A., & Rother, C. (2008) Image segmentation by branch-and-mincut.
In European Conference on Computer Vision, pp. 15–29. 263
Lempitsky, V., Rother, C., Roth, S., & Blake, A. (2010) Fusion moves for Markov random
ﬁeld optimization. IEEE Transactions on Pattern Analysis & Machine Intelligence
32 (8): 1392–1405. 262
Leonard, J. J., & Feder, H. J. S. (2000) A computational efﬁcient method for large-scale
concurrent mapping and localisation.
In International Symposium on Robotics
Research, pp. 169–176. 480
Leordeanu, M., Hebert, M., & Sukthankar, R. (2007) Beyond local appearance: Category
recognition from pairwise interactions of simple features. In IEEE Computer Vision
& Pattern Recognition. 421
Leotta, M. J., & Mundy, J. L. (2011) Vehicle surveillance with a generic, adaptive, 3D
vehicle model. IEEE Transactions on Pattern Analysis & Machine Intelligence 33
(7): 1457–1469. 421
Lepetit, V., & Fua, P. (2005) Monocular model-based 3D tracking of rigid objects: A survey.
Foundations and Trends in Computer Graphics and Vision 1 (1): 1–89. 351
Lepetit, V., & Fua, P.
(2006)
Keypoint recognition using randomized trees.
IEEE
Transactions on Pattern Analysis & Machine Intelligence 28 (9): 1465–1479. 350
Lepetit, V., Lagger, P., & Fua, P. (2005) Randomized trees for real-time keypoint recognition.
In IEEE Computer Vision & Pattern Recognition, pp. 775–781. 166, 167, 348
Lepetit, V., Moreno-Noguer, F., & Fua, P. (2009) EPnP: An accurate O(n) solution to the PnP
problem. International Journal of Computer Vision 81 (2): 155–166. 319
Leventon, M. E., Grimson, W. E. L., & Faugeras, O. D. (2000) Statistical shape inﬂuence
in geodesic active contours.
In IEEE Computer Vision & Pattern Recognition,
pp. 1316–1323. 421

Bibliography
549
Levin, A., Lischinski, D., & Weiss, Y.
(2004)
Colorization using optimization.
ACM
Transactions on Graphics 23 (3): 689–694. 261
Lhuillier, M., & Quan, L. (2002) Match propagation for image-based modeling and rendering.
IEEE Transactions on Pattern Analysis & Machine Intelligence 24 (8): 1140–1146.
264
Li, F.-F., & Perona, P. (2005) A Bayesian hierarchical model for learning natural scene
categories. In IEEE Computer Vision & Pattern Recognition, pp. 524–531. 503,
504
Li, J., & Allinson, N. M.
(2008)
A comprehensive review of current local features for
computer vision. Neurocomputing 71 (10–12): 1771–1787. 292
Li, P., Fu, Y., Mohammed, U., Elder, J., & Prince, S. J. D. (2012) Probabilistic models
for inference about identity.
IEEE Transactions on Pattern Analysis & Machine
Intelligence 34(1): 144–157. 435, 447, 450, 451
Li, P., Warrell, J., Aghajanian, J., & Prince, S. (2010) Context-based additive logistic model
for facial keypoint localization.
In British Machine Vision Conference, pp. 1–11.
450
Li, S. Z. (2010) Markov Random Field Modeling in Image Analysis. Springer, 3rd edition.
261
Li, S. Z., & Jain, A. K. eds. (2005) Handbook of Face Recognition. Springer. 450
Li, S. Z., & Zhang, Z. (2004) Floatboost learning and statistical face detection.
IEEE
Transactions on Pattern Analysis & Machine Intelligence 26 (9): 1112–1123. 167
Li, S. Z., Zhang, Z. Q., Shum, H. Y., & Zhang, H. J.
(2003)
Floatboost learn-
ing for classiﬁcation.
In Advances in Neural Information Processing Systems,
pp. 993–1000. 167
Li, S. Z., Zhuang, Z., Blake, A., Zhang, H., & Shum, H. (2002) Statistical learning of
multi-view face detection. In European Conference on Computer Vision, pp. 67–82.
167
Li, Y., Du, Y., & Lin, X. (2005) Kernel-based multifactor analysis for image synthesis and
recognition. In IEEE International Conference on Computer Vision, pp. 114–119.
451
Li, Y., Sun, J., Tang, C.-K., & Shum, H.-Y. (2004) Lazy snapping. ACM Transactions on
Graphics 23 (3): 303–308. 263
Lienhart, R., Kuranov, A., & Pisarevsky, V. (2003) Empirical analysis of detection cascades
of boosted classiﬁers for rapid object detection. In Deutsche Arbeitsgemeinschaft
f¨ur Mustererkennung, pp. 297–304. 167
Liu, C., & Rubin, D. B. (1995) ML estimation of the t distribution using EM and its extensions
ECM and ECME. Statistica Sinica 5 (1): 19–39. 105
Liu, C., & Shum, H. Y. (2003) Kullback-Leibler boosting.
In IEEE Computer Vision &
Pattern Recognition, pp. 407–411. 167
Liu, C., & Wechsler, H. (1998) Probabilistic reasoning models for face recognition. In IEEE
Computer Vision & Pattern Recognition, pp. 827–832. 450
Liu, J., Sun, J., & Shum, H.-Y. (2009) Paint selection. ACM Transactions on Graphics 28
(3): 69:1–68:7. 263
Longuet-Higgins, H. C. (1981) A computer algorithm for reconstructing a scene from two
projections. Nature 293: 133–135. 380
Loop, C. T., & Zhang, Z. (1999) Computing rectifying homographies for stereo vision. In
IEEE Computer Vision & Pattern Recognition, pp. 1125–1131. 380

550
Bibliography
Lourakis, M. I. A., & Argyros, A. A. (2009) SBA: A software package for generic sparse
bundle adjustment.
ACM Transactions on Mathematical Software 36(1): 2:2:30.
381
Lowe, D. G. (2004) Distinctive image features from scale-invariant keypoints. International
Journal of Computer Vision 60 (2): 91–110. 292, 293, 380, 451
Loxam, J., & Drummond, T. (2008) Student-t mixture ﬁlter for robust, real-time visual
tracking. In European Conference on Computer Vision, pp. 372–385. 105
Lu, S., Metaxas, D. N., Samaras, D., & Oliensis, J. (2003) Using multiple cues for hand
tracking and model reﬁnement. In IEEE Computer Vision & Pattern Recognition,
pp. 443–450. 422
Lucas, B. D., & Kanade, T. (1981) An iterative image registration technique with an appli-
cation to stereo vision. In International Joint Conference on Artiﬁcial Intelligence,
pp. 647–679. 380
Lucey, S., & Chen, T. (2006) Learning patch dependencies for improved pose mismatched
face veriﬁcation. In IEEE Computer Vision & Pattern Recognition, pp. 909–915.
451
Ma, Y., Derksen, H., Hong, W., & Wright, J. (2007) Segmentation of multivariate mixed data
via lossy data coding and compression. IEEE Transactions on Pattern Analysis &
Machine Intelligence 29 (9): 1546–1562. 106
Ma, Y., Soatto, S., & Koseck´a, J. (2004) An Invitation to 3-D Vision. Springer. 319, 380
Mac Aodha, O., Brostow, G. J. and Pollefeys, M. (2010) Segmenting video into classes of
algorithm suitability, In IEEE Computer Vision and Pattern Recognition. 167
Mackay, D. J. (2003) Information Theory, Learning and Inference Algorithms. Cambridge
University Press. 41
Makadia, A.
(2010)
Feature tracking for wide-baseline image retrieval.
In European
Conference on Computer Vision, pp. 310–323. 504
M¨akinen, E., & Raisamo, R. (2008a) Evaluation of gender classiﬁcation methods with auto-
matically detected and aligned faces.
IEEE Transactions on Pattern Analysis &
Machine Intelligence 30 (3): 541–547. 167
M¨akinen, E., & Raisamo, R. (2008b) An experimental comparison of gender classiﬁcation
methods. Pattern Recognition Methods 29 (10): 1544–1556. 167
Malcolm, J. G., Rathi, Y., & Tannenbaum, A. (2007) Graph cut segmentation with nonlinear
shape priors. In IEEE International Conference on Image Processing, pp. 365–368.
263
Malladi, R., Sethian, J. A., & Vemuri, B. C.
(1994)
Evolutionary fronts for topology-
independent shape modeling and recoveery. In European Conference on Computer
Vision, pp. 3–13. 421
Matas, J., Chum, O., Urban, M., & Pajdla, T. (2002) Robust wide baseline stereo from
maximally stable extremal regions. In British Machine Vision Conference, pp. 348–
393. 292
Matthews, I., & Baker, S. (2004) Active appearance models revisited. International Journal
of Computer Vision 60 (2): 135–164. 421
Matthews, I., Cootes, T. F., Bangham, J. A., Cox, S., & Harvey, R. (2002) Extraction of
visual features for lipreading. IEEE Transactions on Pattern Analysis & Machine
Intelligence 24 (2): 198–213. 421

Bibliography
551
Matthews, I., Xiao, J., & Baker, S. (2007) 2D vs. 3D deformable face models: Representa-
tional power, construction, and real-time ﬁtting. International Journal of Computer
Vision 75 (1): 93–113. 417, 419, 421
Matthies, L., Kanade, T., & Szeliski, R. (1989) Kalman ﬁlter-based algorithms for estimating
depth from image sequences. International Journal of Computer Vision 3 (3): 209–
238. 479
Matusik, W., Buehler, C., Raskar, R., Gortler, S. J., & McMillan, L. (2000) Image-based
visual hulls. In SIGGRAPH, pp. 369–374. 319
Maybank, S. J. (1998) Theory of Reconstruction from Image Motion. Springer-Verlag. 380
Maybeck, P. S. (1990) The Kalman ﬁlter: An introduction to concepts. In Autonomous Robot
Vehicles, ed. by I. J. Cox & G. T. Wilfong, pp. 194–204. Springer-Verlag. 479
McLachlan, G. J., & Krishnan, T. (2008) The EM Algorithm and Extensions. Wiley, 2nd
edition. 105
Mei, C., Sibley, G., Cummins, M., Newman, P., & Reid, I. (2009) A constant time efﬁcient
stereo SLAM system. In British Machine Vision Conference. 480
Meir, R., & M¨atsch, G. (2003) An introduction to boosting and leveraging. In Advanced
Lectures on Machine Learning, ed. by S. Mendelson & A. Smola, pp. 119–184.
Springer. 167
Messer, K., Matas, J., Kittler, J., Luettin, J., & Maitre, G. (1999) XM2VTS: The extended
M2VTS database.
In Conference on Audio and Video-based Biometric Personal
Veriﬁcation, pp. 72–77. 451
Mikolajczyk, K., & Schmid, C. (2002) An afﬁne invariant interest point detector. In European
Conference on Computer Vision, pp. 128–142. 292
Mikolajczyk, K., & Schmid, C. (2004) Scale & afﬁne invariant interest point detectors.
International Journal of Computer Vision 60 (1): 63–86. 292
Mikolajczyk, K., & Schmid, C. (2005) A performance evaluation of local descriptors. IEEE
Transactions on Pattern Analysis & Machine Intelligence 27 (10): 1615–1630. 293
Mikolajczyk, K., Tuytelaars, T., Schmid, C., Zisserman, A., Matas, J., Schaffalitzky, F., Kadir,
T., & Gool, L. J. V. (2005) A comparison of afﬁne region detectors. International
Journal of Computer Vision 65 (1–2): 43–72. 292
Mikul´ık, A., Perdoch, M., Chum, O., & Matas, J. (2010) Learning a ﬁne vocabulary.
In
European Conference on Computer Vision, pp. 1–14. 504
Minka, T. (2002) Bayesian inference in dynamic models: an overview.
Technical report,
Carnegie Mellon University. 480
Moeslund, T. B., Hilton, A., & Kr¨uger, V. (2006) A survey of advances in vision-based
human motion capture and analysis. Computer Vision & Image Understanding 104
(2–3): 90–126. 421
Moghaddam, B., Jebara, T., & Pentland, A. (2000) Bayesian face recognition.
Pattern
Recognition 33 (11): 1771–1782. 450, 452
Moghaddam, B., & Pentland, A. (1997) Probabilistic visual learning for object representation.
IEEE Transactions on Pattern Analysis & Machine Intelligence 19 (7): 696–710.
106, 450
Moghaddam, B., & Yang, M. H.
(2002)
Learning gender with support faces.
IEEE
Transactions on Pattern Analysis & Machine Intelligence 24 (5): 707–711. 167
Mohammed, U., Prince, S. J. D., & Kautz, J. (2009) Visio-lization: Generating novel facial
images. ACM Transactions on Graphics (SIGGRAPH) 28(3). 259, 261

552
Bibliography
Moni, M. A., & Ali, A. B. M. S. (2009) HMM based hand gesture recognition: A review on
techniques and approaches. In International Conference on Computer Science and
Information Technology, pp. 433–437. 223
Montemerlo, M., Thrun, S., Koller, D., & Wegbreit, B. (2002) FastSLAM: A factored
solution to the simultaneous localization and mapping problem. In Proceedings of
AAAI National Conference on Artiﬁcal Intelligence, pp. 593–598. 480
Montemerlo, M., Thrun, S., Koller, D., & Wegbreit, B. (2003) FastSLAM 2.0: An improved
particle ﬁltering algorithm for simultaneous localization and mapping that provably
converges.
In International Joint Conference on Artiﬁcal Intelligence, pp. 1151–
1156. 480
Moons, T. (1998) A guided tour through multiview relations.
In SMILE, ed. by R. Koch
& L. J. V. Gool, volume 1506 of Lecture Notes in Computer Science, pp. 304–346.
Springer. 380
Moons, T., Van Gool, L. J., & Vergauwen, M. (2009) 3D reconstruction from multiple
images: Part 1 – Principles.
Foundations and Trends in Computer Graphics and
Vision 4 (4): 287–404. 380
Moore, A. P., Prince, S. J. D., & Warrell, J. (2010) ”Lattice Cut” – constructing superpixels
using layer constraints. In IEEE Computer Vision & Pattern Recognition, pp. 2117–
2124. 261, 263
Moore, A. P., Prince, S. J. D., Warrell, J., Mohammed, U., & Jones, G. (2008) Superpixel
lattices. In IEEE Computer Vision & Pattern Recognition. 222
Moosmann, F., Nowak, E., & Jurie, F. (2008) Randomized clustering forests for image
classiﬁcation. IEEE Transactions on Pattern Analysis & Machine Intelligence 30
(9): 1632–1646. 167
Moosmann, F., Triggs, B., & Jurie, F. (2006) Fast discriminative visual codebooks using ran-
domized clustering forests. In Advances in Neural Information Processing Systems,
pp. 985–992. 167
Moravec, H. (1983) The Stanford cart and the CMU rover.
Proceedings of the IEEE 71
(7): 872–884. 292
Mori, G., Ren, X., Efros, A. A., & Malik, J. (2004) Recovering human body conﬁgurations:
Combining segmentation and recognition.
In IEEE Computer Vision & Pattern
Recognition, pp. 326–333. 421
Mundy, J., & Zisserman, A. (1992) Geometric Invariance in Computer Vision. MIT Press.
319
Murase, H., & Nayar, S. K. (1995) Visual learning and recognition of 3-d objects from
appearance. International Journal of Computer Vision 14 (1): 5–24. 106
Murino, V., Castellani, U., Etrari, E., & Fusiello, A. (2002) Registration of very time-distant
aerial images. In IEEE International Conference on Image Processing, pp. 989–992.
350
Murphy, K., Weiss, Y., & Jordan, M.
(1999)
Loopy belief propagation for approxi-
mate inference: An empirical study.
In Uncertainty in Artiﬁcial Intelligence,
pp. 467–475. 223
Murphy, K. P. (1998) Switching Kalman Filters. Technical report. Department of Computer
Science, University of California, Berkeley. 480
Mˇıcuˇs´ık, B., & Pajdla, T. (2003) Estimation of omnidirectional camera model from epipolar
geometry. In IEEE Computer Vision & Pattern Recognition, pp. 485–490. 319

Bibliography
553
Nadarajah, S., & Kotz, S. (2008) Estimation methods for the multivariate t distribution.
Acta Applicandae Mathematicae: An International Survey Journal on Applying
Mathematics and Mathematical Applications 102 (1): 99–118. 105
Navaratnam, R., Fitzgibbon, A. W., & Cippola, R. (2007) The joint manifold model for semi-
supervised multi-valued regression. In IEEE International Conference on Computer
Vision, pp. 1–8. 131
Neal, R., & Hinton, G. (1999) A view of the EM algorithm that justiﬁes incremental, sparse
and other variants.
In Learning in Graphical Models, ed. by M. I. Jordan. MIT
PRess. 105
Newcombe, R. A., & Davison, A. J. (2010) Live dense reconstruction with a single moving
camera. In IEEE Computer Vision & Pattern Recognition, pp. 1498–1505. 380
Newcombe, R. A., Lovegrove, S., & Davison, A. J. (2011) DTAM: Dense tracking and
mapping in real-time. In IEEE International Conference on Computer Vision. 480
Newman, P., & Ho, K. L. (2005) SLAM – Loop closing with visually salient features. In
IEEE International Conference on Robotics and Automation. 480
Nguyen, H. V., & Bai, L. (2010) Cosine similarity metric learning for face veriﬁcation. In
Asian Conference on Computer Vision, pp. 709–720. 451
Niebles, J. C., Wang, H., & 0002, Li, F-.F. (2008) Unsupervised learning of human action
categories using spatial-temporal words. International Journal of Computer Vision
79 (3): 299–318. 504
Nist´er, D.
(2004)
An efﬁcient solution to the ﬁve-point relative pose problem.
IEEE
Transactions on Pattern Analysis & Machine Intelligence 26 (6): 756–777. 380
Nist´er, D., Naroditsky, O., & Bergen, J. R. (2004) Visual odometry.
In IEEE Computer
Vision & Pattern Recognition, pp. 652–659. 480
Nist´er, D., & Stew´enius, H. (2006) Scalable recognition with a vocabulary tree. In IEEE
Computer Vision & Pattern Recognition, pp. 2161–2168. 503
Nixon, M., & Aguado, A. S. (2008) Feature Extraction and Image Processing. Academic
Press, 2nd edition. 5, 292
Nowak, E., & Jurie, F. (2007) Learning visual similarity measures for comparing never seen
objects. In IEEE Computer Vision & Pattern Recognition. 451
O’Gorman, L., Sammon, M. J., & Seul, M. (2008) Practical Algorithms for Image Analysis.
Cambridge University Press, 2nd edition. 292
Oh, S. M., Rehg, J. M., Balch, T. R., & Dellaert, F.
(2005)
Learning and inference in
parametric switching linear dynamical systems. In IEEE International Conference
on Computer Vision, pp. 1161–1168. 480
Ohta, Y., & Kanade, T. (1985) Stereo by intra- and inter-scanline search using dynamic
programming.
IEEE Transactions on Pattern Analysis & Machine Intelligence 7
(2): 139–154. 217, 222
Ojala, T., Pietik¨ainen, M., & M¨aenp¨a¨a, T. (2002) Multiresolution gray-scale and rotation
invariant texture classiﬁcation with local binary patterns.
IEEE Transactions on
Pattern Analysis & Machine Intelligence 24 (7): 971–987. 293, 451
Oliver, N., Rosario, B., & Pentland, A. (2000) A Bayesian computer vision system for
modeling human interactions. IEEE Transactions on Pattern Analysis & Machine
Intelligence 22 (8): 831–843. 68, 223
Opelt, A., Pinz, A., & Zisserman, A. (2006) A boundary-fragment-model for object detection.
In European Conference on Computer Vision, pp. 575–588. 421

554
Bibliography
Osuna, E., Freund, R., & Girosi, F. (1997) Training support vector machines: An application
to face detection. In IEEE Computer Vision & Pattern Recognition, pp. 746–751.
167
¨Ozuysal, M., Calonder, M., Lepetit, V., & Fua, P. (2010) Fast keypoint recognition using
random ferns. IEEE Transactions on Pattern Analysis & Machine Intelligence 32
(3): 448–461. 350
Papoulis, A. (1991) Probability, Random Variables and Stochastic Processes. McGraw Hill,
3rd edition. 16
Pearl, J. (1988) Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann. 223
Peel, D., & McLachlan, G.
(2000)
Robust mixture modelling using the t distribution.
Statistics and Computing 10 (4): 339–348. 105
Perlibakas, V.
(2004)
Distance measures for PCA-based face recognition.
Pattern
Recognition Letters 25 (6): 711–724. 450
Petersen, K. B., Pedersen, M. S., Larsen, J., Strimmer, K., Christiansen, L., Hansen, K., He,
L., Thibaut, L., Baro, M., Hattinger, S., Sima, V., & The, W. (2006) The matrix
cookbook. Technical University of Denmark. 528
Pham, M., & Cham, T. (2007a) Fast training and selection of Haar features using statistics
in boosting-based face detection. In IEEE International Conference on Computer
Vision. 167
Pham, M., & Cham, T. (2007b) Online learning asymmetric boosted classiﬁers for object
detection. In IEEE Computer Vision & Pattern Recognition. 167
Philbin, J., Chum, O., Isard, M., Sivic, J., & Zisserman, A. (2007) Object retrieval with
large vocabularies and fast spatial matching. In IEEE Computer Vision & Pattern
Recognition. 503
Philbin, J., Isard, M., Sivic, J., & Zisserman, A. (2010) Descriptor learning for efﬁcient
retrieval. In European Conference on Computer Vision, pp. 677–691. 293
Phillips, P. J., Moon, H., Rizvi, S. A., & Rauss, P. J. (2000) The FERET evaluation method-
ology for face-recognition algorithms.
IEEE Transactions on Pattern Analysis &
Machine Intelligence 22 (10): 1090–1104. 451
Phung, S., Bouzerdoum, A., & Chai, D. (2005) Skin segmentation using color pixel clas-
siﬁcation: Analysis and comparison.
IEEE Transactions on Pattern Analysis &
Machine Intelligence 27 (1): 147–154. 67
Piccardi, M. (2004) Background subtraction techniques: a review.
In IEEE Int. Conf.
Systems, Man and Cybernetics, pp. 3099–3105. 67
Pinto, N., & Cox, D. (2011) Beyond simple features: A large-scale feature search approach
to unconstrained face recognition. In IEEE International Conference on Automatic
Face & Gesture Recognition. 451
Pollefeys, M. (2002) Visual 3D modeling from images.
On-line tutorial: http://www.cs.
unc.edu/marc/tutorial . 380
Pollefeys, M., Koch, R., & Van Gool, L. J. (1999a) Self-calibration and metric reconstruction
inspite of varying and unknown intrinsic camera parameters. International Journal
of Computer Vision 32 (1): 7–25. 376
Pollefeys, M., Koch, R., & Van Gool, L. J. (1999b) A simple and efﬁcient rectiﬁcation
method for general motion. In IEEE International Conference on Computer Vision,
pp. 496–501. 371, 380

Bibliography
555
Pollefeys, M., & Van Gool, L. J. (2002) Visual modelling: From images to images. Journal
of Visualization and Computer Animation 13 (4): 199–209. 376, 377
Pollefeys, M., Van Gool, L. J., Vergauwen, M., Verbiest, F., Cornelis, K., Tops, J., & Koch,
R. (2004) Visual modeling with a hand-held camera.
International Journal of
Computer Vision 59 (3): 207–232. 377, 380
Pons, J.-P., Keriven, R., & Faugeras, O. D. (2007) Multi-view stereo reconstruction and scene
ﬂow estimation with a global image-based matching score. International Journal of
Computer Vision 72 (2): 179–193. 381
Poppe, R. (2007) Vision-based human motion analysis: An overview. Computer Vision and
Image Understanding 108 (1–2): 4–18. 421, 479
Portilla, J., & Simoncelli, E. (2000) A parametric texture model based on joint statistics of
complex wavelet coefﬁcients. International Journal of Computer Vision 40 (1): 49–
70. 263
Pratt, W. H. (2007) Digital Image Processing. Wiley Interscience, 3rd edition. 292
Prince, S., Cheok, A. D., Farbiz, F., Williamson, T., Johnson, N., Billinghurst, M., & Kato,
H. (2002) 3D live: Real time captured content for mixed reality. In International
Symposium on Mixed and Augmented Reality, pp. 317–324. 317, 318, 319
Prince, S. J. D., & Aghajanian, J. (2009) Gender classiﬁcation in uncontrolled settings using
additive logistic models. In IEEE International Conference on Image Processing,
pp. 2557–2560. 160, 167
Prince, S. J. D., Elder, J. H., Warrell, J., & Felisberti, F. M. (2008) Tied factor analysis
for face recognition across large pose differences.
IEEE Transactions on Pattern
Analysis & Machine Intelligence 30 (6): 970–984. 441, 443, 451
Prinzie, A., & Van den Poel, D. (2008) Random forests for multiclass classiﬁcation: Random
multinomial logit. Expert Systems with Applications 35 (3): 1721–1732. 167
Pritch, Y., Kav-Venaki, E., & Peleg, S. (2009) Shift-map image editing. In IEEE International
Conference on Computer Vision, pp. 151–158. 255, 256, 261
Quan, L., & Lan, Z.-D. (1999) Linear n-point camera pose determination. IEEE Transactions
on Pattern Analysis & Machine Intelligence 21 (8): 774–780. 319
Rabiner, L. (1989) A tutorial on hidden Markov models and selected applications in speech
recognition. Proceedings of the IEEE 77 (2): 257–286. 223
Rae, R., & Ritter, H. (1998) Recognition of human head orientation based on artiﬁcial neural
networks. IEEE Transactions on Neural Networks 9 (2): 257–265. 131
Raguram, R., Frahm, J.-M., & Pollefeys, M. (2008) A comparative analysis of RANSAC
techniques leading to adaptive real-time random sample consensus.
In European
Conference on Computer Vision, pp. 500–513. 350
Ramanan, D., Forsyth, D. A., & Zisserman, A. (2008) Tracking people by learning their
appearance.
IEEE Transactions on Pattern Analysis & Machine Intelligence 29
(1): 65–81. 222
Ranganathan, A. (2009) Semantic scene segmentation using random multinomial logit. In
British Machine Vision Conference. 167
Ranganathan, A., & Yang, M. (2008) Online sparse matrix Gaussian process regression and
vision applications.
In European Conference on Computer Vision, pp. 468–482.
131
Raphael, C. (2001) Course-to-ﬁne dynamic programming. IEEE Transactions on Pattern
Analysis & Machine Intelligence 23 (12): 1379–1390. 222

556
Bibliography
Rasmussen, C. E., & Williams, C. K. I. (2006) Gaussian Processes for Machine Learning.
MIT Press. 131, 167
Rehg, J., & Kanade, T.
(1994)
Visual tracking of high DOF articulated structures: an
application to human hand tracking. In European Conference on Computer Vision,
pp. 35–46. 422
Rehg, J. M., & Kanade, T. (1995) Model-based tracking of self-occluding articulated objects.
In IEEE International Conference on Computer Vision, pp. 612–617. 422
Rehg, J. M., Morris, D. D., & Kanade, T. (2003) Ambiguities in visual tracking of articulated
objects using two- and three-dimensional models. International Journal of Robotics
Research 22(6): 393–418. 421
Rekimoto, J. (1998) MATRIX: A realtime object identiﬁcation and registration method for
augmented reality. In Asia Paciﬁc Computer Human Interaction, pp. 63–69. 350
Ren, X., Berg, A. C., & Malik, J. (2005) Recovering human body conﬁgurations using
pairwise constraints between parts. In IEEE International Conference on Computer
Vision, pp. 824–831. 222
Rigoll, G., Kosmala, A., & Eickeler, S. (1998) High performance real-time gesture recogni-
tion using hidden Markov models. In International Workshop on Gesture and Sign
language in Human-Computer Interaction. 223
Rogez, G., Rihan, J., Ramalingam, S., Orrite, C., & Torr, P. (2006) Randomized trees for
human pose detection.
In Advances in Neural Information Processing Systems,
pp. 985–992. 167
Romdhani, S., Cong, S., & Psarrou, A. (1999) A multi-view non-linear active shape model
using kernel PCA. In British Machine Vision Conference. 421
Rosales, R., & Sclaroff, S.
(1999)
3D trajectory recovery for tracking multiple objects
and trajectory guided recognition of actions. In IEEE Computer Vision & Pattern
Recognition, pp. 2117–2123. 476, 477, 479
Rosen-Zvi, M., Grifﬁths, T. L., Steyvers, M., & Smyth, P. (2004) The author–topic model
for authors and documents. In Uncertainty in Artiﬁcial Intelligence, pp. 487–494.
503
Rosenblatt, F. (1958) The Perceptron: A probabilistic model for information storage and
organization in the brain. Psychological Review 65 (6): 386–408. 167
Rosten, E., & Drummond, T. (2006) Machine learning for high-speed corner detection. In
European Conference on Computer Vision, volume 1, pp. 430–443. 292
Roth, S., & Black, M. J. (2009) Fields of experts. International Journal of Computer Vision
82 (2): 205–229. 105, 263
Rother, C., Bordeaux, L., Hamadi, Y., & Blake, A. (2006) Autocollage. ACM Transactions
on Graphics 25 (3): 847–852.
Rother, C., Kohli, P., Feng, W., & Jia, J. (2009) Minimizing sparse higher order energy
functions of discrete variables. In IEEE Computer Vision & Pattern Recognition,
pp. 1382–1389.
Rother, C., Kolmogorov, V., & Blake, A. (2004) Grabcut – Interactive foreground extraction
using iterated graph cuts. ACM Transactions on Graphics (SIGGRAPH 2004) 23
(3): 309–314. 261, 263
Rother, C., Kolmogorov, V., Lempitsky, V. S., & Szummer, M. (2007) Optimizing binary
MRFs via extended roof duality. In IEEE Computer Vision & Pattern Recognition.
262

Bibliography
557
Rother, C., Kumar, S., Kolmogorov, V., & Blake, A. (2005) Digital tapestry.
In IEEE
Computer Vision & Pattern Recognition, pp. 589–586. 253
Rothwell, C. A., Zisserman, A., Forsyth, D. A., & Mundy, J. L. (1995) Planar object recog-
nition using projective shape representation.
International Journal of Computer
Vision 16 (1): 57–99. 352
Rousseeuw, P. J. (1984) Least median of squares regression.
Journal of the American
Statistical Association 79 (388): 871–880. 350
Rousson, M., & Paragios, N. (2002) Shape priors for level set representations. In European
Conference on Computer Vision, pp. 78–92. 421
Roweis, S., & Saul, L.
(2000)
Nonlinear dimensionality reduction by locally linear
embedding. Science 290 (5500): 2323–2326. 293
Roweis, S. T., & Ghahramani, Z. (1999) A unifying review of linear Gaussian models. Neural
Computation 11 (2): 305–345. 480
Roweis, S. T., & Ghahramani, Z. (2001) Learning nonlinear dynamical systems using the
expectation-maximization algorithm.
In Kalman Filtering and Neural Networks,
ed. by S. Haykin, pp. 175–220. Wiley. 480
Rubin, D., & Thayer, D. (1982) EM algorithms for ML factor analysis. Psychometrica 47
(1): 69–76. 105, 421
Rumelhart, D. E., Hinton, G. E., & Williams, R.
(1986)
Learning internal representa-
tions by error propagation.
In Parallel Distributed Processing: Explorations in
the Microstructure of Cognition. Volume 1: Foundations, ed. by D. Rumelhart,
J. McLelland, & The PDP Research Group, pp. 318–362. MIT Press. 167
Salmen, J., Schlipsing, M., Edelbrunner, J., Hegemann, S., & L¨uke, S. (2009) Real-time
stereo vision: Making more out of dynamic programming. In Proceedings of the
13th International Conference on Computer Analysis of Images and Patterns, CAIP
’09, pp. 1096–1103. Springer-Verlag. 222
Salvi, J., Pags, J., & Batlle, J. (2004) Pattern codiﬁcation strategies in structured light systems.
Pattern Recognition 37 (4): 827 – 849. 319
Schaffalitzky, F., & Zisserman, A. (2002) Multi-view matching for unordered image sets,
or “how do i organize my holiday snaps?”. In European Conference on Computer
Vision, pp. 414–431.
Schapire, R., & Singer, Y. (1998) Improved boosting algorithms using conﬁdence-rated
predictions. In Conference on Computational Learning Theory, pp. 80–91. 167
Scharstein, D., & Szeliski, R. (2002) A taxonomy and evaluation of dense two-frame stereo
correspondence algorithms. International Journal of Computer Vision 47 (1): 7–42.
264
Scharstein, D., & Szeliski, R. (2003) High-accuracy depth maps using structured light. In
IEEE Computer Vision & Pattern Recognition, pp. 194–202. 314, 315, 316, 319
Schlesinger, D., & Flach, B. (2006) Transforming an arbitrary minsum problem into a binary
one. Technical Report TUD–FI06–01, Dresden University of Technology. 261
Schmugge, S. J., Jayaram, S., Shin, M., & Tsap, L. (2007) Objective evaluation of approaches
to skin detection using roc analysis. Computer Vision & Image Understanding 108
(1–2): 41–51. 67
Schneiderman, H., & Kanade, T. (2000) A statistical method for 3D object detection applied
to faces and cards. In IEEE International Conference on Computer Vision, pp. 746–
751. 167

558
Bibliography
Sch¨olkopf, B., Smola, A. J., & M¨uller, K.-R. (1997) Kernel principal component analysis. In
International Conference on Artiﬁcial Neural Networks, pp. 583–588. 293
Sch¨olkopf, B., Smola, A. J., & M¨uller, K.-R. (1998) Nonlinear component analysis as a
kernel eigenvalue problem. Neural Computation 10 (5): 1299–1319. 421
Sch¨uldt, C., Laptev, I., & Caputo, B. (2004) Recognizing human actions: A local SVM
approach.
In International Conference on Pattern Recognition, pp. 32–36.
502,
503, 504
Seitz, S. M., Curless, B., Diebel, J., Scharstein, D., & Szeliski, R. (2006) A comparison
and evaluation of multi-view stereo reconstruction algorithms. In IEEE Computer
Vision & Pattern Recognition, pp. 519–528. 381
Seo, H., & Magnenat-Thalmann, N. (2003) An automatic modeling of human bodies from
sizing parameters. In ACM Symposium on Interactive 3D Graphics, pp. 19–26. 422
Sﬁkas, G., Nikou, C., & Galatsanos, N. (2007) Robust image segmentation with mixtures of
Student’s t-distributions. In IEEE International Conference on Image Processing,
pp. 273–276. 102, 106
Sha’ashua, A., & Ullman, S. (1988) Structural saliency: The detection of globally salient
structures using a locally connected network. In IEEE International Conference on
Computer Vision, pp. 321–327. 222
Shakhnarovich, G., Viola, P. A., & Darrell, T. (2003) Fast pose estimation with parameter-
sensitive hashing. In IEEE International Conference on Computer Vision, pp. 750–
759. 421
Shan, C. (2012) Learning local binary patterns for gender classiﬁcation on real-world face
images. Pattern Recognition Letters, 33 (4), pp. 431–437. 167
Shepherd, B. (1983) An appraisal of a decision tree approach to image classiﬁcation.
In
International Joint Conferences on Artiﬁcial Intelligence, pp. 473–475. 167
Shi, J., & Tomasi, C. (1994) Good features to track. In IEEE Computer Vision & Pattern
Recognition, pp. 311–326. 380
Shotton, J., Blake, A., & Cipolla, R. (2008a) Multiscale categorical object recognition using
contour fragments. IEEE Transactions on Pattern Analysis & Machine Intelligence
30 (7): 1270–1281. 421
Shotton, J., Fitzgibbon, A. W., Cook, M., Sharp, T., Finoccio, M., Moore, R., Kipman, A.,
& Blake, A. (2011) Real-time human pose recognition in parts from single depth
images. In IEEE Computer Vision & Pattern Recognition. 164, 166, 167
Shotton, J., Johnson, M., & Cipolla, R. (2008b) Semantic texton forests for image cate-
gorization and segmentation.
In IEEE Computer Vision & Pattern Recognition.
167
Shotton, J., Winn, J., Rother, C., & Criminisi, A. (2009) Textonboost for image understand-
ing: Multi-class object recognition and segmentation by jointly modeling texture,
layout and context. International Journal of Computer Vision 81 (1): 2–23. 163,
164, 167, 261, 278
Shum, H.-Y., & Szeliski, R. (2000) Construction of panoramic image mosaics with global
and local alignment.
International Journal of Computer Vision 36 (2): 101–130.
351
Shumway, R. H., & Stoffer, D. S. (1991) Dynamic linear models with switching. Journal of
the American Statistical Association 86 (415): 763–769. 480

Bibliography
559
Shumway, R. H., & Stoffer, D. S.
(1982)
An approach to time series smooth-
ing and forecasting using the EM algorithm.
J. Time Series Analysis 3 (4):
253–264. 480
Sidenbladh, H., Black, M. J., & Fleet, D. J. (2000) Stochastic tracking of 3D human ﬁgures
using 2D image motion. In European Conference on Computer Vision, pp. 702–718.
421
Sigal, L., Balan, A. O., & Black, M. J. (2010) HumanEva: Synchronized video and motion
capture dataset and baseline algorithm for evaluation of articulated human motion.
International Journal of Computer Vision 87 (1–2): 4–27. 421
Sigal, L., & Black, M. J.
(2006)
Measure locally, reason globally: Occlusion-sensitive
articulated pose estimation.
In IEEE Computer Vision & Pattern Recognition,
pp. 2041–2048. 222
Sim, R., Elinas, P., Grifﬁn, M., & Little, J. (2005) Vision-based SLAM using the Rao-
Blackwellised particle ﬁlter. In IJCAI Workshop on Reasoning with Uncertainty in
Robotics, pp. 9–16. 480
Simon, G., & Berger, M.-O. (2002) Pose estimation for planar structures. IEEE Computer
Graphics and Applications 22 (6): 46–53. 350
Simon, G., Fitzgibbon, A. W., & Zisserman, A. (2000) Markerless tracking using pla-
nar structures in the scene. In International Symposium on Mixed and Augmented
Reality, pp. 120–128. 350
Sinha, S. N., Mordohai, P., & Pollefeys, M. (2007) Multi-view stereo via graph cuts on
the dual of an adaptive tetrahedral mesh.
In IEEE International Conference on
Computer Vision, pp. 1–8. 381
Sinha, S. N., & Pollefeys, M. (2005) Multi-view reconstruction using photo-consistency and
exact silhouette constraints: A maximum-ﬂow formulation. In IEEE International
Conference on Computer Vision, pp. 349–356. 381
Sivic, J., Russell, B. C., Efros, A. A., Zisserman, A., & Freeman, W. T. (2005) Discover-
ing objects and their localization in images. In IEEE International Conference on
Computer Vision, pp. 370–377. 503
Sivic, J., Russell, B. C., Zisserman, A., Freeman, W. T., & Efros, A. A. (2008) Unsupervised
discovery of visual object class hierarchies.
In IEEE Computer Vision & Pattern
Recognition. 503
Sivic, J., & Zisserman, A. (2003) Video google: A text retrieval approach to object matching
in videos. In IEEE International Conference on Computer Vision, pp. 1470–1477.
500, 503
Sizintsev, M., Kuthirummal, S., Sawhney, H., Chaudhry, A., Samarasekera, S., & Kumar, R.
(2010) GPU accelerated realtime stereo for augmented reality.
In International
Symposium 3D Data Processing, Visualization and Transmission. 373
Sizintsev, M., & Wildes, R. P.
(2010)
Coarse-to-ﬁne stereo vision with accurate 3d
boundaries. Image and Vision Computing 28 (3): 352–366. 264
Skrypnyk, I., & Lowe, D. G. (2004) Scene modelling, recognition and tracking with invariant
image features.
In International Symposium on Mixed and Augmented Reality,
pp. 110–119. 350
Smith, R., & Cheeseman, P. (1987) On the representation of spatial uncertainty. International
Journal of Robotics Research 5 (4): 56–68. 480

560
Bibliography
Smith, R., Self, M., & Cheeseman, P. (1990) Estimating uncertain spatial relationships in
robotics. In Autonomous Robot Vehicles, ed. by I. J. Cox & G. T. Wilton, pp. 167–
193. Springer. 480
Smith, S. M., & Brady, J. M. (1997) Susan - A new approach to low level image processing.
International Journal of Computer Vision 23 (1): 45–78. 292
Snavely, N., Garg, R., Seitz, S. M., & Szeliski, R. (2008) Finding paths through the world’s
photos.
ACM Transactions on Graphics (Proceedings of SIGGRAPH 2008) 27
(3): 11–21. 378
Snavely, N., Seitz, S. M., & Szeliski, R. (2006) Photo tourism: Exploring photo collections
in 3D. In SIGGRAPH Conference Proceedings, pp. 835–846. ACM Press. 377, 378
Starck, J., Maki, A., Nobuhura, S., Hilton, A., & Mastuyama, T.
(2009)
The multiple-
camera 3-D production studio.
IEEE Transactions on Circuits and Systems for
Video Technology 19 (6): 856–869. 319
Starner, T., Weaver, J., & Pentland, A. (1998) A wearable computer based American sign
language recognizer.
In Assistive Technology and Artiﬁcial Intelligence, Lecture
Notes in Computer Science, volume 1458, pp. 84–96. 216, 217, 223
State, A., Hirota, G., Chen, D. T., Garrett, W. F., & Livingston, M. A. (1996) Superior aug-
mented reality registration by integrating landmark tracking and magnetic tracking.
In ACM SIGGRAPH, pp. 429–438. 350
Stauffer, C., & Grimson, E. (1999) Adaptive background classiﬁcation using time-based
co-occurences. In IEEE Computer Vision & Pattern Recognition, pp. 246–252. 68,
105
Stegmann, M. B. (2002) Analysis and segmentation of face images using point annotations
and linear subspace techniques.
Technical report, Informatics and Mathemati-
cal Modelling, Technical University of Denmark, DTU, Richard Petersens Plads,
Building 321, DK–2800 Kgs. Lyngby. 406, 408, 421
Stenger, B., Mendonc¸a, P. R. S., & Cipolla, R. (2001a) Model-based 3D tracking of an
articulated hand. In IEEE Computer Vision & Pattern Recognition, pp. 310–315.
416, 422
Stenger, B., Mendonc¸a, P. R. S., & Cipolla, R. (2001b) Model-based hand tracking using an
unscented Kalman ﬁlter. In British Machine Vision Conference. 479
Stew´enius, H., Engels, C., & Nist´er, D.
(2006)
Recent developments on direct relative
orientation. ISPRS Journal of Photogrammetry and Remote Sensing 60: 284–294.
380
Strasdat, H., Montiel, J. M. M., & Davison, A. J. (2010) Real-time monocular SLAM: Why
ﬁlter? In IEEE International Conference on Robotics and Automation, pp. 2657–
2664. 480
Sturm, P. F. (2000) Algorithms for plane-based pose estimation. In IEEE Computer Vision
& Pattern Recognition, pp. 1706–1711. 350
Sturm, P. F., & Maybank, S. J. (1999) On plane-based camera calibration: A general algo-
rithm, singularities, applications. In IEEE Computer Vision & Pattern Recognition,
pp. 1432–1437. 351
Sturm, P. F., Ramalingam, S., Tardif, J.-P., Gasparini, S., & Barreto, J. (2011) Camera
models and fundamental concepts used in geometric computer vision. Foundations
and Trends in Computer Graphics and Vision 6 (1-2): 1–183. 319

Bibliography
561
Sturm, P. F., & Triggs, B. (1996) A factorization based algorithm for multi-image projective
structure and motion. In European Conference on Computer Vision, pp. 709–720.
380
Sudderth, E. B., Torralba, A., Freeman, W. T., & Willsky, A. S. (2005) Learning Hierar-
chical Models of Scenes, Objects, and Parts, In IEEE International Conference on
Computer Vision, pp. 1331–1338.
Sudderth, E. B., Torralba, A., Freeman, W. T., & Willsky, A. S. (2008) Describing visual
scenes using transformed objects and parts.
International Journal of Computer
Vision 77 (1–3): 291–330. 498, 500
Sugimoto, A.
(2000)
A linear algorithm for computing the homography for conics in
correspondence.
Journal of Mathematical Imaging and Vision 13 (2): 115–130.
350
Sun, J., Zhang, W., Tang, X., & Shum, H. Y. (2006) Background cut. In European Conference
on Computer Vision, pp. 628–641. 68
Sun, J., Zheng, N., & Shum, H. Y. (2003) Stereo matching using belief propagation. IEEE
Transactions on Pattern Analysis & Machine Intelligence 25 (7): 787–800.
223,
263
Sutherland, I. E.
(1963)
Sketchpad: a man-machine graphical communications system.
Technical Report 296, MIT Lincoln Laboratories. 350
Sutton, C., & McCallum, A.
(2011)
An introduction to conditional random ﬁelds.
Foundations and Trends in Machine Learning. 261
Szeliski, R. (1996) Video mosaics for virtual environments. IEEE Computer Graphics and
Applications 16 (2): 22–30. 351
Szeliski, R. (2006) Image alignment and stitching: A tutorial. Foundations and Trends in
Computer Graphics and Vision 2 (1). 351
Szeliski, R. (2010) Computer vision: algorithms and applications. Springer. 2, 5, 264, 351
Szeliski, R., & Shum, H.-Y.
(1997)
Creating full view panoramic image mosaics and
environment maps. In ACM SIGGRAPH, pp. 251–258. 351
Szeliski, R., Zabih, R., Scharstein, D., Veksler, O., Kolmogorov, V., Agarwala, A., Tappen,
M., & Rother, C. (2008) A comparative study of energy minimization methods
for Markov random ﬁelds.
IEEE Transactions on Pattern Analysis & Machine
Intelligence 30 (6): 1068–1080. 263
Taigman, Y., Wolf, L., & Hassner, T. (2009) Multiple one-shots for utilizing class label
information. In British Machine Vision Conference. 451
Tappen, M. F., & Freeman, W. T. (2003) Comparison of graph cuts with belief propagation
for stereo, using identical MRF parameters. In IEEE International Conference on
Computer Vision, pp. 900–907. 263
Tarlow, D., Givoni, I., Zemel, R., & Frey, B. (2011) Graph cuts is a max-product algorithms.
In Uncertainty in Artiﬁcial Intelligence. 262
Tenenbaum, J., Silva, V., & Langford, J. (2000) A global geometric framework for nonlinear
dimensionality reduction. Science 290 (5500): 2319–2315. 293
Tenenbaum, J. B., & Freeman, W. T. (2000) Separating style and content with bilinear
models. Neural Computation 12 (6): 1247–1283. 446, 451
Terzopolous, D., & Szeliski, R. (1992) Tracking with Kalman snakes. In Active Vision, ed.
by A. Blake & A. Y. Yuile, pp. 3–29. MIT Press.

562
Bibliography
Thayananthan, A., Navatnam, R., Stenger, B., Torr, P., & Cipolla, R. (2006) Multivariate
relevance vector machines for tracking.
In European Conference on Computer
Vision, pp. 124–138. 131
Theobalt, C., Ahmed, N., Lensch, H. P. A., Magnor, M. A., & Seidel, H.-P. (2007) See-
ing people in different light-joint shape, motion, and reﬂectance capture.
IEEE
Transactions on Visualization and Computer Graphics 13 (4): 663–674. 319
Tipping, M., & Bishop, C. M. (1999) Probabilistic principal component analysis. Journal of
the Royal Statistical Society: Series B 61 (3): 611–622. 105
Tipping, M. E. (2001) Sparse Bayesian learning and the relevance vector machine. Journal
Machine Learning Research 1: 211–244. 131, 167, 421
Tomasi, C., & Kanade, T. (1991) Detection and tracking of point features. Technical Report
CMU–SC–91–132, Carnegie Mellon. 380
Tomasi, C., & Kanade, T. (1992) Shape and motion from image streams under orthography:
A factorization method. International Journal of Computer Vision 9 (2): 137–154.
380
Tombari, F., Mattoccia, S., di Stefano, L., & Addimanda, E. (2008) Classiﬁcation and eval-
uation of cost aggregation methods for stereo correspondence. In IEEE Computer
Vision & Pattern Recognition. 263
Torr, P.
(1998)
Geometric motion segmentation and model selection.
Philosophical
Transactions of the Royal Society A 356 (1740): 1321–1340. 350
Torr, P. H. S., & Criminisi, A. (2004) Dense stereo using pivoted dynamic programming.
Image and vision computing 22 (10): 795–806. 222
Torr, P. H. S., & Zisserman, A. (2000) MLESAC: A new robust estimator with application
to estimating image geometry.
Computer Vision & Image Understanding 78(1):
138–156. 350
Torralba, A., Murphy, K., & Freeman, W. T. (2007) Sharing visual features for multiclass
and multi-view object detection. IEEE Transactions on Pattern Analysis & Machine
Intelligence 29 (5): 854–869. 163, 167
Treibitz, T., Schechner, Y. Y., & Singh, H. (2008) Flat refractive geometry. In IEEE Computer
Vision & Pattern Recognition. 319
Triggs, B., McLauchlan, P. F., Hartley, R. I., & Fitzgibbon, A. W. (1999) Bundle adjustment
– A modern synthesis. In Workshop on Vision Algorithms, pp. 298–372. 381
Tsai, R. (1987) A versatile cameras calibration technique for high accuracy 3D machine
vision metrology using off-the-shelf TV cameras and lenses. Journal of Robotics
and Automation 3 (4): 323–344. 319
Turk, M., & Pentland, A. P. (2001) Face recognition using eigenfaces. In IEEE Computer
Vision & Pattern Recognition, pp. 586–591. 450
Tuytelaars, T., & Mikolajczyk, K.
(2007)
Local invariant feature detectors: A survey.
Foundations and Trends in Computer Graphics and Vision 3 (3): 177–280. 292
Urtasun, R., Fleet, D. J., & Fua, P.
(2006)
3D people tracking with Gaussian process
dynamical models. In IEEE Computer Vision & Pattern Recognition, pp. 238–245.
131
Vapnik, V. (1995) The Nature of Statistical Learning Theory. Springer Verlag. 167
Varma, M., & Zisserman, A. (2004) Unifying statistical texture classiﬁcation frameworks.
Image and Vision Computing 22 (14): 1175–1183. 503

Bibliography
563
Vasilescu, M. A. O., & Terzopoulos, D. (2002) Multilinear analysis of image ensembles:
Tensorfaces. In European Conference on Computer Vision, pp. 447–460. 451
Vasilescu, M. A. O., & Terzopoulos, D. (2004) Tensortextures: Multilinear image-based
rendering. ACM Transactions on Graphics 23 (3): 336–342. 448
Vaswani, N., Chowdhury, A. K. R., & Chellappa, R. (2003) Activity recognition using the
dynamics of the conﬁguration of interacting objects. In IEEE Computer Vision &
Pattern Recognition, pp. 633–642. 479
Veksler, O. (2005) Stereo correspondence by dynamic programming on a tree.
In IEEE
Computer Vision & Pattern Recognition, pp. 384–390. 219, 222
Veksler, O.
(2008)
Star shape prior for graph-cut image segmentation.
In European
Conference on Computer Vision, pp. 454–467. 263
Veksler, O., Boykov, Y., & Mehrani, P. (2010) Superpixels and supervoxels in an energy
optimization framework.
In European Conference on Computer Vision, pp. 211–
224. 261
Vezhnevets, V., Sazonov, V., & Andreeva, A. (2003) A survey on pixel-based skin color
detection techniques. In Graphicon, pp. 85–92. 67
Vicente, S., Kolmogorov, V., & Rother, C. (2008) Graph cut based image segmentation with
connectivity priors. In IEEE Computer Vision & Pattern Recognition, pp. 1–8. 263
Vincent, E., & Laganiere, R.
(2001)
Detecting planar homographies in an image pair.
In IEEE International Symposium on Image and Signal Processing Analysis,
pp. 182–187. 350
Viola, P., & Jones, M. (2002) Fast and robust classiﬁcation using asymmetric adaboost
and a detector cascade.
In Advances in Neural Information Processing Systems,
pp. 1311–1318. 167
Viola, P. A., & Jones, M. J. (2004) Robust real-time face detection. International Journal of
Computer Vision 57 (2): 137–154. 161, 162, 166, 167
Viola, P. A., Jones, M. J., & Snow, D. (2005) Detecting pedestrians using patterns of motion
and appearance. International Journal of Computer Vision 63 (2): 153–161. 162
Vlasic, D., Baran, I., Matusik, W., & Popovic, J. (2008) Articulated mesh animation from
multi-view silhouettes. ACM Transactions on Graphics 27 (3). 319
Vogiatzis, G., Esteban, C. H., Torr, P. H. S., & Cipolla, R. (2007) Multiview stereo via
volumetric graph-cuts and occlusion robust photo-consistency. IEEE Transactions
on Pattern Analysis & Machine Intelligence 29 (12): 2241–2246.
261, 378, 379,
381
Vuylsteke, P., & Oosterlinck, A. (1990) Range image acquisition with a single binary-encoded
light pattern.
IEEE Transactions on Pattern Analysis & Machine Intelligence 12
(2): 148–164. 319
Wagner, D., Reitmayr, G., Mulloni, A., Drummond, T., & Schmalstieg, D. (2008) Pose
tracking from natural features on mobile phones. In International Symposium on
Mixed and Augmented Reality, pp. 125–134. 351
Wainright, M., Jaakkola, T., & Willsky, A.
(2005)
MAP estimation via agreement on
trees: Message passing and linear programming. IEEE Transactions on Information
Theory 5 (11): 3697–3717. 263
Wang, J. M., Fleet, D. J., & Hertzmann, A. (2007) Multifactor Gaussian process models
for style-content separation.
In International Conference on Machine Learning,
pp. 975–982. 449, 451

564
Bibliography
Wang, J. M., Fleet, D. J., & Hertzmann, A. (2008) Gaussian process dynamical models for
human motion. IEEE Transactions on Pattern Analysis & Machine Intelligence 30
(2): 283–298. 479, 480
Wang, X., & Tang, X. (2003) Bayesian face recognition using Gabor features. In Proceed-
ings of the 2003 ACM SIGMM Workshop on Biometrics Methods and Applications,
pp. 70–73. ACM. 450, 451
Wang, X., & Tang, X. (2004a) Dual-space linear discriminant analysis for face recognition.
In IEEE Computer Vision & Pattern Recognition, pp. 564–569. 450
Wang, X., & Tang, X. (2004b) A uniﬁed framework for subspace face recognition. IEEE
Transactions on Pattern Analysis & Machine Intelligence 26 (9): 1222–1228. 450
Wei, L.-Y., & Levoy, M.
(2000)
Fast texture synthesis using tree-structured vector
quantization. In ACM SIGGRAPH, pp. 479–488. 263
Weiss, Y., & Freeman, W. (2001) On the optimality of solutions of the max-product belief
propagation algorithm in arbitrary graphs.
IEEE Transactions on Information
Theory 47 (2): 723–735. 223, 263
Weiss, Y., Yanover, C., & Meltzer, T. (2011) Linear programming and variants of belief
propagation. In Advances in Markov Random Fields, ed. by A. Blake, P. Kohli, &
C. Rother. MIT Press. 263
Wilbur, R. B., & Kak, A. C. (2006) Purdue RVL-SLLL American sign language database.
Technical Report TR–06–12, Purdue University, School of Electrical and Computer
Engineering. 196
Williams, C., & Barber, D. (1998) Bayesian classiﬁcation with Gaussian priors.
IEEE
Transactions on Pattern Analysis & Machine Intelligence 20 (2): 1342–1351. 166
Williams, D., & Shah, M.
(1992)
A fast algorithm for active contours and curvature
estimation. CVGIP: Image Understanding 55 (1): 14–26. 420
Williams, O. M. C., Blake, A., & Cipolla, R. (2005) Sparse Bayesian learning for efﬁ-
cient tracking. IEEE Transactions on Pattern Analysis & Machine Intelligence 27
(8): 1292–1304. 130, 131
Williams, O. M. C., Blake, A., & Cipolla, R. (2006) Sparse and semi-supervised visual
mapping with the S3P. In IEEE Computer Vision & Pattern Recognition, pp. 230–
237. 131
Wiskott, L., Fellous, J.-M., Kr¨uger, N., & von der Malsburg, C. (1997) Face recognition
by elastic bunch graph matching.
In IEEE International Conference on Image
Processing, pp. 129–132. 450
Wolf, L., Hassner, T., & Taigman, Y. (2009) The one-shot similarity kernel.
In IEEE
International Conference on Computer Vision, pp. 897–902. 451
Woodford, O., Torr, P. H. S., Reid, I., & Fitzgibbon, A. W. (2009) Global stereo reconstruc-
tion under second-order smoothness priors. IEEE Transactions on Pattern Analysis
& Machine Intelligence 31 (12): 2115–2128. 261
Wren, C. R., Aazarbayejani, A., Darrell, T., & Pentland, A. P. (1997) Pﬁnder: Real-time
tracking of the human body.
IEEE Transactions on Pattern Analysis & Machine
Intelligence 19 (7): 780–785. 68
Wright, J., Yang, A. Y., Ganesh, A., Sastry, S. S., & Ma, Y. (2009) Robust face recogni-
tion via sparse representation. IEEE Transactions on Pattern Analysis & Machine
Intelligence 31 (2): 210–227. 451

Bibliography
565
Wu, B., Ai, H., Huang, C., & Lao, S. (2007) Fast rotation invariant multi-view face detec-
tion based on real adaboost.
In IEEE Workshop on Automated Face and Gesture
Recognition, pp. 79–84. 167
Wu, C., Agarwal, S., Curless, B., & Seitz, S. (2011) Multicore bundle adjustment. In IEEE
Computer Vision & Pattern Recognition, pp. 3057–3064. 381
Wu, Y., Lin, J. Y., & Huang, T. S. (2001) Capturing natural hand articulation.
In IEEE
International Conference on Computer Vision, pp. 426–432. 422
Xiao, J., Hays, J., Ehinger, K. A., Oliva, A., & Torralba, A. (2010) Sun database: Large-
scale scene recognition from abbey to zoo.
In IEEE Computer Vision & Pattern
Recognition, pp. 3485–3492. 504
Xu, C., & Prince, J. L. (1998) Snakes, shapes, and gradient vector ﬂow. IEEE Transactions
on Image Processing 7 (3): 359–369. 420
Yang, J., Jiang, Y.-G., Hauptmann, A. G., & Ngo, C.-W. (2007) Evaluating bag-of-visual-
words representations in scene classiﬁcation. In Multimedia Information Retrieval,
pp. 197–206. 504
Yang, M.-H. (2002) Kernel eigenfaces vs. kernel ﬁsherfaces: Face recognition using ker-
nel methods.
In IEEE International Conference on Automatic Face & Gesture
Recognition, pp. 215–220. 450
Yilmaz, A., Javed, O., & Shah, M. (2006) Object tracking: A survey.
Acm Computing
Surveys (CSUR) 38 (4): 1–45. 479
Yin, P., Criminisi, A., Winn, J., & Essa, I. (2007) Tree based classiﬁers for bilayer video
segmentation. In IEEE Computer Vision & Pattern Recognition. 167
Yoon, K.-J., & Kweon, I.-S. (2006) Adaptive support-weight approach for correspondence
search. IEEE Transactions on Pattern Analysis & Machine Intelligence 28 (4): 650–
656. 263
Zhang, C., & Zhang, Z. (2010) A survey of recent advances in face detection. Technical
Report MSR–TR–2010–66, Microsoft Research, Redmond. 167
Zhang, J., Marszalek, M., Lazebnik, S., & Schmid, C. (2007) Local features and kernels for
classiﬁcation of texture and object categories: A comprehensive study. International
Journal of Computer Vision 73 (2): 213–238. 504
Zhang, X., & Gao, Y. (2009) Face recognition across pose: A review. Pattern Recognition
42 (11): 2876–2896. 451
Zhang, Z. (2000) A ﬂexible new technique for camera calibration. IEEE Transactions on
Pattern Analysis & Machine Intelligence 22 (11): 1330–1334. 351
Zhao, J., & Jiang, Q. (2006) Probabilistic PCA for t distributions.
Neurocomputing 69
(16–18): 2217–2226. 105
Zhao, W.-Y., Chellappa, R., Phillips, P. J., & Rosenfeld, A. (2003) Face recognition: A
literature survey. ACM Comput. Surv. 35 (4): 399–458. 450
Zhou, S., Chellappa, R., & Moghaddam, B.
(2004)
Visual tracking and recognition
using appearance-adaptive models in particle ﬁlters. IEEE Transactions on Image
Processing, 13 (11): 1491–1506. 479
Zhou,
S. K.,
& Chellappa,
R.
(2004)
Probabilistic identity characterization
for face recognition.
In IEEE Computer Vision & Pattern Recognition,
pp. 805–812. 450

566
Bibliography
Zhu, X., Yang, J., & Waibel, A.
(2000)
Segmenting hands of arbitrary colour.
In IEEE International Conference on Automatic Face & Gesture Recognition,
pp. 446–453. 67
Zitnick, C. L., & Kanade, T. (2000) A cooperative algorithm for stereo matching and occlu-
sion detection. IEEE Transactions on Pattern Analysis & Machine Intelligence 22
(7): 675–684. 264

Index
3D body model, 418–420
3D morphable model, 416–418, 421
3D reconstruction, 297, 305–306,
312–313, 380
from structured light, 314–315
pipeline, 376–377
volumetric graph cuts, 378–380
action recognition, 501–502, 504
activation, 133
active appearance model, 405–410, 421
active contour model, 389–393, 420
active shape model, 388, 396–405, 421
3D, 405
adaboost, 160, 161, 167, see also
boosting
afﬁne transformation, 327–328
learning, 332–333
alignment of shapes, 397–398
alpha-beta swap, 261, 265
alpha-expansion algorithm, 244–247,
261
ancestral sampling, 186
application
3D reconstruction, 297, 376–380
action recognition, 501–502, 504
animation synthesis, 449–450
augmented reality tracking, 324,
340, 347–350
background subtraction, 65, 67,
252
body pose estimation, 108, 129,
164–165, 220
body tracking, 421
changing face pose, 103
contour tracking, 453, 478
denoising, 227, 230–231, 239, 247
depth from structured light,
314–315
face detection, 71, 99–100, 106,
161–163, 167
face recognition, 102–103, 424,
430–432, 436, 447–448, 450
face synthesis, 259–260
ﬁnding facial features, 219, 223
ﬁtting 3D body model, 418–420
ﬁtting 3D shape model, 416–418
gender classiﬁcation, 133, 160,
167
gesture tracking, 195, 216
image retargeting, 255–257
interactive segmentation,
253–254, 263
multiview reconstruction,
378–381
object recognition, 100–101, 106,
483–501
panorama, 349, 351
pedestrian detection, 161–163
pedestrian tracking, 476
Photo-tourism, 377–378
scene recognition, 483, 499
segmentation, 101–102, 220–222,
389–393
semantic segmentation, 163, 167
shape from silhouette, 316–319
sign language interpretation, 195,
197, 216
skin detection, 64–65, 67
SLAM, 477–478, 480
stereo vision, 216–219, 222,
254–255, 263
super-resolution, 257
surface layout recovery, 163–164

568
Index
application (cont.)
TensorTextures, 448–449
texture synthesis, 257–259, 263
tracking head position, 129–130
Video Google, 500–501
approximate inference, 185
AR Toolkit, 350
argmax function, 508
argmin function, 508
articulated models, 414–416
pictorial structures, 220
asymmetric bilinear model, 438–443
augmented reality, 324, 340, 347–350
augmenting paths algorithm, 234–235,
262
author–topic model, 493–495, 503
auto-calibration, 372
back propagation, 159
background subtraction, 65, 67, 252
bag of words, 285–286, 484–487, 503
Baum-Welch algorithm, 212
Bayes’ Rule, 13
Bayesian approach to ﬁtting, 29–30
Bayesian belief propagation, 208–211,
223
loopy, 215
sum-product algorithm, 208–209
Bayesian linear regression, 111–114
Bayesian logistic regression, 138–142
Bayesian model selection, 43, 431
Bayesian network, 175–178
comparison to undirected, 181
learning, 189
sampling, 186–187
Bayesian nonlinear regression, 117
belief propagation, 208–211, 223
loopy, 215
sum-product algorithm, 208–209
Bernoulli distribution, 17–18
conjugate prior, 24, 26
relation to binomial, 25
beta distribution, 17, 19
between-individual variation, 427
BFGS, 515
bilateral ﬁlter, 293
bilinear model, 451
asymmetric, 438–443
symmetric, 443–445
binary classiﬁcation, 60–61, 133–156
binomial distribution, 25
bivariate distribution, 44
block diagonal matrix, 531
blurring, 272
body pose estimation, 108, 129,
164–165, 220
body tracking, 421
boosting, 167
adaboost, 161
jointboost, 163
logitboost, 153
bottom-up approach, 387
branching logistic regression, 153–155
Brownian motion, 463
Broyden Fletcher Goldfarb Shanno,
515
bundle adjustment, 374–376, 381
calibration
from 3D object, 304–305,
311–312
from a plane, 337–338, 351
calibration target, 311
3D, 305
planar, 337
camera
geometry, 319
orthographic, 321
other camera models, 319
othographic, 382
parameters, 302
pinhole, 297–304
in Cartesian coordinates, 302
in homogeneous coordinates,
308–309
projective, 297
weak perspective, 321
camera calibration
from 3D object, 304–305,
311–312
from a plane, 337–338, 351
Canny edge detector, 279–280,
390
canonical correlation analysis, 438
capacity, 233
cascade structured classiﬁer, 162

Index
569
categorical distribution, 17, 19–20
Bayesian ﬁtting, 39–41
conjugate prior, 24, 27
ﬁtting, 38–41
MAP ﬁtting, 39
ML ﬁtting, 38
relation to multinomial, 25
chain model, 195–202, 205–211, 453
directed, 196
learning, 212
MAP inference, 198
marginal posterior inference, 205
sum product algorithm in, 209
undirected, 196–197
changing face pose, 103
Chapman–Kolmogorov equation, 455
class conditional density function, 61,
72
classiﬁcation, 55, 133–160, 166
adaboost, 160
applications of, 160–166
Bayesian logistic regression,
138–142
binary, 60–61, 133–156
boosting, 153
cascade structure, 162
dual logistic regression, 144–146
fern, 159
gender, 160
kernel logistic regression,
146–147
logistic regression, 60, 133–136
multiclass, 156–158
multilayer perceptron, 159
nonlinear logistic regression, 142
non-probabilistic models,
159–160
one-against-all, 156
random classiﬁcation tree,
158–159
random forest, 159
relevance vector, 147–149
support vector machine, 160
tree, 153–156, 167
weak classiﬁer, 153
clique, 179, 229
maximal, 179
closed set face identiﬁcation, 431
clustering, 82, 291–292
coarse-to-ﬁne approach, 256,
404
collinearity, 329, see homography
color model, 64, 65, 253
combining variables, 215
condensation algorithm, 472–476
for tracking contour, 478
condition number, 524
conditional independence, 173
in a directed model, 176
in an undirected model, 179
conditional probability distribution, 12
of multivariate normal, 48
conditional random ﬁeld, 260
1D, 212
2D, 247–250
conic, 321, 351, 388
conjugacy, 24
Bernoulli/beta, 26
categorical/Dirichlet distribution,
27
normal/normal inverse Wishart, 27
normal/normal-scaled inverse
gamma, 27
self-conjugacy of normal, 49
conjugate gradient method, 515
constellation model, 495–499
constraint edge, 241, 262, 264
continuous random variable, 9
contour model, 389–393
contour tracking, 453, 478
contrastive divergence, 190–191
persistent, 191
convex function, 137, 510
convex potentials, 243, 244
corner detection, 279, 281–282, 292
Harris corner detector, 281–282
SIFT, 282
cost function, 509
covariance, 15
covariance matrix, 23, 44
diagonal, 44
full, 44
spherical, 44
CRF, 260
1D, 212
2D, 247–250

570
Index
cross product, 519
cross-ratio, 352
cut on a graph, 233
cost, 233
minimum, 233
damped Newton, 515
data association, 396, 475
decision boundary, 69, 134
Delaunay triangulation, 371
delta function, 508
denoising, 227, 230–231
binary, 239
multilabel, 245, 247
dense stereo vision, 216–219, 254–255,
371
depth from structured light, 314–315
derivative ﬁlter, 272
descriptor, 283–287, 293
bag of words, 285–287
histogram, 283–284
HOG, 285
SIFT, 284
determinant of matrix, 521
diagonal covariance matrix, 44
diagonal matrix, 520
inverting, 531
dictionary of visual words, 286, 483
difference of Gaussians, 273
digits
modeling, 104
dimensionality reduction, 287–293
dual PCA, 290
K-Means, 291–292
PCA, 289–290
direct linear transformation algorithm,
333–335
direct search method, 516
directed graphical model, 175–178
chain, 196
comparison to undirected, 181
establishing conditional
independence relations in,
176
for grids, 250–251
learning, 189
Markov blanket, 176
sampling, 186–187
Dirichlet distribution, 17, 20–21
discrete random variable, 9
discriminative model, 56
classiﬁcation, 133–160
regression, 108–131
disparity, 217, 254
displacement expert, 129–130
distance transform, 390
distribution
Bernoulli, 17–18
beta, 17, 19
binomial, 25
categorical, 17, 19–20
conjugate, 24
Dirichlet, 17, 20–21
gamma, 85
multinomial, 25
multivariate normal, 22–23, 44–50
normal, 17
normal inverse Wishart, 17, 23–24
normal-scaled inverse gamma, 17,
21–22
probability, 17–25
t-distribution, 82–88
univariate normal, 21
DLT algorithm, 333–335
dolly zoom, 320
domain of a random variable, 17
dot product, 519
dual
linear regression, 124–126
logistic regression, 144–146
parameterization, 124, 144
PCA, 290
dynamic programming, 198, 222
for stereo vision, 222
in a chain, 199–202
in a loop, 226
in a tree, 202–205
E-step, 95, 76, 97–98
edge detection, 279, 292
Canny, 279–280, 390
edge ﬁlter, 272
eight-point algorithm, 363–364
EKF, 466–467
EM algorithm, 75–77, 94–99
E-step, 76, 95, 97–98

Index
571
for factor analyzer, 90–93
for mixture of Gaussians, 79–82
for t distribution, 86–88
lower bound, 96
M-step, 76, 98–99
empirical max-marginals, 186
energy minimization, 178
epipolar constraint, 356
epipolar geometry, 355
epipolar line, 356
computing, 359
epipole, 356–357
computing, 359
essential matrix, 357–359, 380
decomposition, 360–361
properties, 359
estimating parameters, 28–41
Euclidean transformation, 323–325
learning, 331–332
evidence, 13, 42
framework, 43
expectation, 14–15
expectation maximization, 75–77,
94–99, 105
E-step, 76, 95, 97–98
for factor analyzer, 90–93
for mixture of Gaussians, 79–82
for t-distribution, 86–88
lower bound, 96
M-step, 76, 98–99
expectation step, 76, 95, 97–98
expert, 154
exponential family, 26
extended Kalman ﬁlter, 466–467
exterior orientation problem, 309, 319
3D scene, 304, 309–311
planar scene, 335–337
extrinsic parameters, 302
estimation, 319
learning
3D scene, 304, 309–311
planar scene, 335–337
face
clustering, 431, 447
detection, 71, 72, 99–100, 106,
161–163, 167
recognition, 102–103, 424, 436,
447–448, 450
across pose, 103
as model comparison, 430–432
closed set identiﬁcation, 431
open set identiﬁcation, 431
synthesis, 259–260
veriﬁcation, 424
face model
3D morphable, 416–418
facial features
aligning, 450
ﬁnding, 219, 223
factor analysis, 88, 105, 424
as a marginalization, 90
learning, 90–93
mixture of factor analyzers, 94
probability density function, 89
factor graph, 193, 208, 223
factorization, 374
of a probability distribution, 175,
178
Tomasi-Kanade, 380, 382
feature, 380
tracking, 380
feature descriptor, 283–287
bag of words, 285–287
histogram, 283–284
HOG, 285
SIFT, 284
feature detector, 279
Canny edge detector, 279–280
Harris corner detector, 281–282
SIFT detector, 282
fern, 159
ﬁeld of view, 301
ﬁlter, 271
bilateral, 293
derivative, 272
different of Gaussian, 273
edge, 272
Gabor, 273
Haar, 275
Laplacian, 273
Laplacian of Gaussian, 273
Prewitt, 272
Sobel, 273
ﬁtting probability models, 28–41
ﬁxed interval smoothing, 462–463
ﬁxed lag smoothing, 461–462

572
Index
ﬂow
optical, 255
through graph, 233
focal length, 297
parameter, 299
forest, 159, 165
forward-backward algorithm, 206–208
Frobenius norm, 530
frustum, 348
full covariance matrix, 44
fundamental matrix, 361–362,
380
decomposition, 371
estimation, 362–364
relation to essential matrix, 362
Gabor energy, 275
Gabor ﬁlter, 273
gallery face, 431
gamma distribution, 85
gamma function, 19
gating function, 154
Gauss-Newton method, 514–515
Gaussian distribution, see normal
distribution
Gaussian Markov random ﬁeld, 264
Gaussian process
classiﬁcation, 147
latent variable model, 410–414,
437
multifactor, 449–450
regression, 119, 131
gender classiﬁcation, 133, 160, 167
generalized Procrustes analysis,
397–398
generative model, 56, 57
comparison to discriminative
model, 63
geodesic distance, 254
geometric invariants, 352
geometric transformation model,
323–347
2D, 323–330
application, 347–349
learning, 330
gesture tracking, 195, 216
Gibbs distribution, 178, 228
Gibbs sampling, 187–188
GPLVM, 410–414, 437
multifactor, 449–450
GrabCut, 253–254
gradient vector, 136, 511
graph cuts, 231–247, 261
alpha-expansion, 244–247
applications of, 251–257
binary variables, 235–239
efﬁcient reuse of solution, 262
multilabel, 239–247
reparameterization, 237–239
volumetric, 378–380
graphical model, 173–192
applications in computer vision,
181
chain, 195, 453
directed, 175–178
learning, 189
sampling, 186–187
directed versus undirected, 181
factor graph, 193
grid-based, 213
plate notation, 177
tree, 195
undirected, 178
learning, 189–192
sampling, 187–188
Gray codes, 315
grid-based model, 213, 227–264
applications, 261
directed, 250–251
Haar ﬁlter, 162, 275
hand model, 415, 416, 422
Harris corner detector, 281–282
head position
tracking, 129–130
Heaviside step function, 142, 153, 508
Hessian matrix, 136, 510
hidden layer, 159
hidden Markov model, 182, 196, 197,
216, 223
hidden variable, 73, 74
representing transformations, 104
higher order cliques, 250, 263
Hinton diagram, 9
histogram equalization, 270
histogram of oriented gradients, 285,
293

Index
573
histogram, RGB, 283
HMM, 182, 196, 197, 223
HOG descriptor, 285, 293
homogeneous coordinates, 306
homography, 328–329
learning, 333–335
properties, 339–341
human part identiﬁcation, 164–165
human performance capture, 318, 319
human pose estimation, 220
hyperparameter, 18
hysteresis thresholding, 280
ICP, 395
ideal point, 308
identity, 424
identity/style model, 424
asymmetric bilinear, 438–443
multifactor GPLVM, 449–450
multilinear, 446
nonlinear, 437–438
PLDA, 433–437
subspace identity model, 427–432
symmetric bilinear, 443–445
identity matrix, 520
image denoising, 227, 230–231
binary, 239
multilabel, 247
image descriptor, 293
image plane, 297
image processing, 269–287, 292
image quilting, 257–259
image retargeting, 255–257
image structure tensor, 281
importance sampling, 476
incremental ﬁtting
of logistic regression, 150–152
independence, 14
conditional, 173
inference, 56
algorithm, 56
empirical max-marginals, 186
in graphical models with loops,
214
MAP solution, 184
marginal posterior distribution,
184
maximum marginals, 185
sampling from posterior, 185
innovation, 458
integral image, 275
intensity normalization, 269
interactive segmentation, 253–254,
263
interest point detection, 279, 292
Harris corner detector, 281–282
SIFT, 282
intersection of two lines, 321
intrinsic matrix, 302
intrinsic parameters, 302
learning
from 3D object, 304–305,
311–312
from a plane, 337–338
invariant
geometric, 352
inverse of a matrix, 520, 525–527
computing for large matrices, 530
Ishikawa construction, 261
iterated extended Kalman ﬁlter, 467
iterative closest point, 395
Jensen’s inequality, 96
joint probability, 10
jointboost, 163
junction tree algorithm, 215
K-means algorithm, 82, 291–292
Kalman ﬁlter, 182, 455–463
temporal and measurement
models, 463
derivation, 456
extended, 466–467
iterated extended, 467
recursions, 459
smoothing, 461–463
unscented, 467–471
Kalman gain, 457
Kalman smoothing, 461–463
kernel function, 118–120, 146
kernel logistic regression, 146–147
kernel PCA, 290, 293
kernel trick, 119
Kinect, 164
kinematic chain, 415
Kullback-Leibler divergence, 97

574
Index
landmark point, 389
landscape matrix, 520
Laplace approximation, 139, 140, 148
Laplacian ﬁlter, 273
Laplacian of Gaussian ﬁlter, 273
latent Dirichlet allocation, 487–492,
503
learning, 490–492
latent variable, 73, 74
LDA (latent Dirichlet allocation),
487–492, 503
learning, 490–492
LDA (linear discriminant analysis), 450
learning, 28, 56
Bayesian approach, 29–30
in chains and trees, 212
in directed models, 189
in undirected models, 189–192
least squares, 32
maximum a posteriori, 28
maximum likelihood, 28
learning algorithm, 56
least median of squares regression, 350
least squares, 32
solving least squares problems,
528
likelihood, 13
line, 321, 351
epipolar, 356
joining two points, 321
line search, 515
linear algebra, 519–532
common problems, 528–530
linear discriminant analysis, 450
linear regression, 108–110
Bayesian approach, 111–114
limitations of, 110
linear subspace, 89
linear transformation, 522
local binary pattern, 276, 293
local maximum/minimum, 137, 509
log likelihood, 31
logistic classiﬁcation tree, 153–156
logistic regression, 60, 133–136
Bayesian approach, 138–142
branching, 153–155
dual, 144–146
kernel, 146–147
multiclass, 156–158
nonlinear, 142
logistic sigmoid function, 133
logitboost, 153
loopy belief propagation, 215
applications, 223
M-estimator, 350
M-step, 76, 95, 98–99
magnitude of vector, 519
manifold, 287
MAP estimation, 28
marginal distribution, 10
of multivariate normal, 47
marginal posterior distribution, 184
marginalization, 10
Markov assumption, 196, 453
Markov blanket, 176, 179
in a directed model, 176
in an undirected model, 179
Markov chain Monte Carlo, 187
Markov network
learning, 189–192
sampling, 187–188
Markov random ﬁeld, 179, 182, 227,
228, 260
Gaussian, 264
applications, 251–257, 261
higher order, 250, 263
pairwise, 229
Markov tree, 182
matrix, 520
block diagonal, 531
calculus, 527–528
condition number, 524
determinant, 521
diagonal, 520
Frobenious norm, 530
identity, 520
inverse, 520, 525–527
inverting large, 530
landscape, 520
multiplication, 520
null space, 522, 525
orthogonal, 521
portrait, 520
positive deﬁnite, 521
rank, 524

Index
575
rotation, 521
singular, 520
square, 520
trace, 521
transpose, 520
matrix determinant lemma, 532
matrix inversion lemma, 113, 532
max ﬂow, 233
algorithms, 262
augmenting paths algorithm,
234–235
max function, 508
maximal clique, 179
maximization step, 76, 95, 98–99
maximum a posteriori estimation, 28
maximum likelihood estimation, 28
maximum marginals, 185
MCMC, 187
measurement incorporation step, 455
measurement model, 453
Mercer’s theorem, 119
min cut, 233
min function, 508
minimum direction problem, 310, 529
mixture model
mixture of experts, 168
mixture of factor analyzers, 94,
105
mixture of Gaussians, 77–82, 105
mixture of PLDAs, 437
mixture of t-distributions, 94, 105
robust, 94
ML estimation, 28
model, 56
discriminative, 56
generative, 56, 57
model comparison, 42
model selection, 431
moment, 14–15
about mean, 15
about zero, 15
MonoSLAM, 477–478
morphable model, 416–418
mosaic, 349, 351
MRF, 179, 182, 227, 228, 260
applications, 251–257, 261
Gaussian, 264
higher order, 250, 263
pairwise, 229
multiclass classiﬁcation, 156–158
multiclass logistic regression,
156–158
random classiﬁcation tree,
158–159
multifactor GPLVM, 449–450
multifactor model, 446
multilayer perceptron, 159, 167
multilinear model, 446, 451
multiview geometry, 380
multiview reconstruction, 305–306,
313, 372, 378–381
multinomial distribution, 25
multiple view geometry, 354
multivariate normal distribution, 17,
44–50
multiview reconstruction, 312
na¨ıve Bayes, 65
neural network, 159
Newton method, 137, 512–514
non-convex potentials, 244
nonlinear identity model, 437–438
nonlinear logistic regression, 142
nonlinear optimization, 509–518
BFGS, 515
conjugate gradient method, 515
Gauss-Newton method, 514–515
line search, 515
Newton method, 512–514
over positive deﬁnite matrices,
518
over rotation matrices, 517
quasi-Newton methods, 515
reparameterization, 516
steepest descent, 511–512
trust-region methods, 515
nonlinear regression, 114
Bayesian, 117
nonstationary model, 461
norm of vector, 519
normal distribution, 17, 44–50
Bayesian ﬁtting, 35
change of variable, 50
conditional distribution, 48
covariance decomposition, 45
MAP ﬁtting, 33

576
Index
normal distribution (cont.)
marginal distribution, 47
ML ﬁtting, 30
multivariate, 17, 22–23
product of normals, 48, 52
self-conjugacy, 49
transformation of variable, 47
univariate, 17, 21
normal inverse Wishart distribution, 17,
23–24
normal-scaled inverse gamma
distribution, 17, 21–22
normalized camera, 299
normalized image coordinates, 310
null space, 522, 525
object recognition, 100–101, 106,
483–501
unsupervised, 492
objective function, 509
offset parameter, 300
one-against-all classiﬁer, 156
open-set face identiﬁcation, 431
optical axis, 297
optical center, 297
optical ﬂow, 255
optimization, 509–518
BFGS, 515
conjugate gradient method, 515
Gauss-Newton method, 514–515
line search, 515
Newton method, 512–514
over positive deﬁnite matrix, 518
over rotation matrix, 517
quasi-Newton methods, 515
reparameterization, 516
steepest descent, 511–512
trust-region methods, 515
orthogonal matrix, 521
orthogonal Procrustes problem, 311,
332, 529
orthogonal vectors, 519
orthographic camera, 321, 382
outlier, 82, 342
pairwise MRF, 229
pairwise term, 198, 232
panorama, 349, 351
parametric contour model, 389–393
part of object, 488
particle ﬁltering, 472–476
partition function, 178
PCA, 289–290
dual, 290
kernel, 290, 293
probabilistic, 89, 401–402
PDF, 9
PEaRL algorithm, 345, 350
pedestrian detection, 161–163
pedestrian tracking, 476
per-pixel image processing, 269
persistent contrastive divergence, 191
perspective projection, 298
perspective-n-point problem, 304,
319
Phong shading model, 416
Photo-tourism, 377–378
photoreceptor spacing, 300
pictorial structure, 219, 222
pinhole, 297
pinhole camera, 297–304, 354
in Cartesian coordinates, 302
in homogeneous coordinates,
308–309
plate, 177
PLDA, 433–437
PnP problem, 304, 319
point distribution model, 396–405
point estimate, 29
point operator, 269
polar rectiﬁcation, 371
portrait matrix, 520
pose estimation, 350
positive deﬁnite matrix, 521
optimization over, 518
posterior distribution, 13
potential function, 178
potentials
convex, 243
non-convex, 244
Potts model, 244, 265
PPCA, 401–402
learning parameters, 401
prediction step, 455
predictive distribution, 28
preprocessing, 100, 269–292
Prewitt operators, 272

Index
577
principal component analysis, 289–290
dual PCA, 290
probabilistic, 89, 401–402
principal direction problem, 529
principal point, 297
prior, 13
probabilistic latent semantic analysis,
503
probabilistic linear discriminant
analysis, 433–437
probabilistic principal component
analysis, 89, 105, 401–402
learning parameters, 401
probability
conditional, 12
joint, 10
marginal, 10
probability density function, 9
probability distribution, 17–25
ﬁtting, 28–41
probe face, 431
Procrustes analysis
generalized, 397–398
Procrustes problem, 311, 529
product of experts, 179
projective camera, 297
projective pinhole camera, 297
projective reconstruction, 313
projective transformation, 328–329
ﬁtting, 333–335
properties, 339–341
propose, expand and relearn, 345,
350
prototype vector, 291
pruning graphical models, 214, 219
quadri-focal tensor, 373
quadric, 415
truncated, 415
Quasi-Newton methods, 515
quaternion, 517
radial basis function, 115, 151
radial distortion, 303
random classiﬁcation tree, 158–159
random forest, 159
random sample consensus, 342–344,
350
sequential, 345
random variable, 9
continuous, 9
discrete, 9
domain of, 17
rank of matrix, 524
RANSAC, 342–344, 350
sequential, 345
Rao-Blackwellization, 476
reconstruction, 297, 305–306, 312–313
from structured light, 314–315
multiview, 372, 381
projective, 313
two view, 364
reconstruction error, 288
reconstruction pipeline, 376–377,
380
rectiﬁcation, 216, 368, 380
planar, 368
polar, 371
region descriptor, 283–286
bag of words, 285–287
histogram, 283–284
HOG, 285
SIFT, 284
regression, 55, 108–131
Bayesian linear, 111–114
dual, 124–126
Gaussian process, 119, 131
linear, 58, 108–110
limitations of, 110
nonlinear, 114
nonlinear, Bayesian, 117
polynomial, 115
relevance vector, 127–128
sparse, 120
to multivariate data, 128
relative orientation, 360, 380
relevance vector
classiﬁcation, 147–149
regression, 127–128
reparameterization
for optimization, 516
in graph cuts, 237–239
multilabel case, 243
reprojection error, 355
resection-intersection, 374
responsibility, 79
robust density modeling, 82–88

578
Index
robust learning, 342, 350
PEaRL, 345
RANSAC, 342–344
sequential RANSAC, 345
robust mixture model, 94
robust subspace model, 94
rotation matrix, 521
optimization over, 517
rotation of camera, 340
sampling
ancestral, 186
directed models, 186–187
Gibbs, 187
undirected models, 187–188
sampling from posterior, 185
scalar product, 519
scale invariant feature transform, 282,
284
SCAPE, 418–420
scene model, 499
scene recognition, 483
Schur complement, 531
segmentation, 101–102, 106, 220, 387,
389–393
supervised, 253–254
semantic segmentation, 163, 167
sequential RANSAC, 345
seven point algorithm, 368
shape, 387
alignment, 397–398
deﬁnition, 388
statistical model, 396–405
shape and appearance models, 405–410
shape context descriptor, 129, 286–287
shape from silhouette, 316–319
shape model
3D, 405
articulated, 414–416
non-Gaussian, 410–414
subspace, 399
shape template, 393, 395
Sherman–Morrison–Woodbury
relation, 113, 532
shift map image editing, 255–257
SIFT, 348
descriptor, 284, 293
detector, 282
sign language interpretation, 195, 197,
216
silhouette
shape from, 316–319
similarity transformation, 326
learning, 332
simultaneous localization and mapping,
477–478, 480
single author–topic model, 493, 495
singular matrix, 520
singular value decomposition, 522–525
singular values, 523
skew (camera parameter), 301
skew (moment), 15
skin detection, 64–65, 67
SLAM, 477–478, 480
smoothing, 461–463
ﬁxed interval, 462–463
ﬁxed lag, 461–462
snake, 220–223, 389–393, 420
Sobel operator, 273
softmax function, 157
sparse classiﬁcation model, 147–150
sparse linear regression, 120
sparse stereo vision, 297
sparsity, 120, 127, 148, 150
spherical covariance matrix, 44
square matrix, 520
squared reprojection error, 355
statistical shape model, 396–405
steepest descent, 511–512
step function, 153
stereo reconstruction, 305–306,
312–313
stereo vision, 216–219, 222, 254–255,
263
dense, 216–219
dynamic programming, 222
graph cuts formulation, 254–255
sparse, 297
strong classiﬁer, 153
structure from motion, 354, 372
structured light, 314, 319
Student t-distribution, 82–88
style, 424
style / identity model, 424
asymmetric bilinear, 438–443
multifactor GPLVM, 449–450

Index
579
multilinear, 446
nonlinear, 437–438
PLDA, 433–437
subspace identity model, 427–432
symmetric bilinear, 443–445
style translation, 442
submodularity, 239, 243
multilabel case, 243
subspace, 89
subspace identity model, 427–432
subspace model, 88, 105, 399, 421, 424
bilinear asymmetric, 438–443
bilinear symmetric, 443–445
dual PCA, 290
factor analysis, 424
for face recognition, 450
multifactor GPLVM, 449–450
multilinear model, 446
PLDA, 433–437
principal component analysis,
289–290
subspace identity model, 427–432
subspace shape model, 399
sum-product algorithm, 208–209, 223
for chain model, 209
for tree model, 211
super-resolution, 257
superpixel, 163
supervised segmentation, 253–254
support vector machine, 160, 167
surface layout recovery, 163–164
SVD, 522–525
SVM, 160
symmetric bilinear model, 443–445
symmetric epipolar distance, 363
t-distribution, 82–88, 105
mixture of, 94
multivariate, 85
univariate, 84
t-test, 43
temporal model, 453–480
tensor, 522
multiplication, 522
TensorTextures, 448–449
texton, 163, 277
textonboost, 163
texture synthesis, 257–259, 263
tied factor analysis, 438
Tomasi-Kanade factorization, 374, 380,
382
top-down approach, 387
topic, 488
trace of matrix, 521
tracking, 453–480
pedestrian, 476
applications, 479
condensation algorithm, 472–476
displacement expert, 129–130
features, 380
for augmented reality, 347–349
head position, 129–130
particle ﬁltering, 472–476
through clutter, 478
transformation, 323–347, 350
2D, 323–330
afﬁne, 327–328
application, 347–349
between images, 339
Euclidean, 323–325
homography, 328–329
indexed by hidden variable,
104
inference, 334
inverting, 334
learning, 330
afﬁne, 332–333
Euclidean, 331–332
homography, 333–335
projective, 333–335
similarity, 332
linear, 522
projective, 328–329
robust learning, 342
similarity, 326
transpose, 520
tree model, 195
learning, 212
MAP inference, 202–205
marginal posterior inference,
211
tri-focal tensor, 373
triangulation, 306
truncating potentials, 247
trust-region methods, 515
two-view geometry, 355

580
Index
UKF, 467–471
unary term, 232, 251
undirected graphical model, 178
chain, 196–197
conditional independence
relations in, 179
learning, 189–192
Markov blanket, 179
sampling, 187–188
univariate normal distribution, 17
unscented Kalman ﬁlter, 467–471
unsupervised object discovery, 492
variable elimination, 205
variance, 15
vector, 519
norm, 519
product, 519
Vertigo, 320
Video Google, 500–501
virtual image, 297
visual hull, 316
visual word, 285, 483
Viterbi algorithm, 198–202
volumetric graph cuts, 378–380
weak classiﬁer, 153
weak perspective camera, 321
whitening, 269
whitening transformation, 51
within-individual variation, 427,
433
Woodbury inversion identity, 113,
532
word, 285, 483
world state, 55


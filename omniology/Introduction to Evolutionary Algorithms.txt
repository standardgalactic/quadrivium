
Decision Engineering 

Series Editor 
 
Professor Rajkumar Roy 
Department of Enterprise Integration 
School of Industrial and Manufacturing Science 
Cranfield University 
Cranfield 
Bedford 
MK43 0AL 
UK 
Other titles published in this series 
 
Cost Engineering in Practice 
John McIlwraith 
IPA ‚Äì Concepts and Applications in Engineering 
Jerzy Pokojski 
Strategic Decision Making 
Navneet Bhushan and Kanwal Rai 
Product Lifecycle Management 
John Stark 
From Product Description to Cost: A Practical Approach 
Volume 1: The Parametric Approach 
Pierre Foussier 
From Product Description to Cost: A Practical Approach 
Volume 2: Building a Specific Model 
Pierre Foussier 
Decision-Making in Engineering Design 
Yotaro Hatamura 
Composite Systems Decisions 
Mark Sh. Levin 
Intelligent Decision-making Support Systems 
Jatinder N.D. Gupta, Guisseppi A. Forgionne and Manuel Mora T. 
Knowledge Acquisition in Practice 
N.R. Milton 
Global Product: Strategy, Product Lifecycle Management and the Billion Customer Question 
John Stark 
Enabling a Simulation Capability in the Organisation 
Andrew Greasley 
Network Models and Optimization 
Mitsuo Gen, Runewei Cheng and Lin Lin 
Management of Uncertainty 
Gudela Grote 

Xinjie Yu ¬∑ Mitsuo Gen 
Introduction  
to Evolutionary Algorithms 
123

Xinjie Yu, PhD 
Department of Electrical Engineering 
Tsinghua University 
100084 Beijing 
China 
yuxj@tsinghua.edu.cn 
Mitsuo Gen, PhD 
Fuzzy Logic Systems Institute (FLSI)  
680-41 Kawazu 
820-0067 Iizuka 
Japan 
gen@flsi.or.jp 
ISSN 1619-5736 
ISBN 978-1-84996-128-8 
e-ISBN 978-1-84996-129-5 
DOI 10.1007/978-1-84996-129-5 
Springer London Dordrecht Heidelberg New York 
 
British Library Cataloguing in Publication Data 
A catalogue record for this book is available from the British Library 
 
Library of Congress Control Number: 2010929767 
 
¬© Springer-Verlag London Limited 2010 
 
Discipulus is a trademark of RML Technologies, 7606 S. Newland St., Littleton, CO 80128, USA, 
www.rmltech.com 
Mathematica¬Æ is a registered trademark of Wolfram Research, Inc., 100 Trade Center Drive, 
Champaign, IL 61820-7237, USA, www.wolfram.com 
MATLAB¬Æ is a registered trademark of The MathWorks, Inc., 3 Apple Hill Drive, Natick, MA, 
01760-2098 USA, www.mathworks.com 
 
Apart from any fair dealing for the purposes of research or private study, or criticism or review, as
permitted under the Copyright, Designs and Patents Act 1988, this publication may only be
reproduced, stored or transmitted, in any form or by any means, with the prior permission in writing of
the publishers, or in the case of reprographic reproduction in accordance with the terms of licences
issued by the Copyright Licensing Agency. Enquiries concerning reproduction outside those terms
should be sent to the publishers. 
The use of registered names, trademarks, etc. in this publication does not imply, even in the absence of
a specific statement, that such names are exempt from the relevant laws and regulations and therefore
free for general use. 
The publisher makes no representation, express or implied, with regard to the accuracy of the
information contained in this book and cannot accept any legal responsibility or liability for any errors
or omissions that may be made. 
 
Cover design: eStudioCalamar, Figueres/Berlin 
 
Printed on acid-free paper 
 
Springer is part of Springer Science+Business Media (www.springer.com) 

To our families, friends, and students

Preface
This is a textbook on evolutionary algorithms (EAs). In preparing the proposal and
the manuscript, the following questions were always kept in our minds.
‚àôIs this book convenient for teaching, studying, and self-study, i.e., have the con-
tents been arranged in a pedagogically sound way?
‚àôDoes this book introduce the state of the art of EAs?
‚àôDoes this book cover the contents as comprehensively as possible?
‚àôDoes this book contain speciÔ¨Åc programming codes so that the reader can use
them directly?
For the Ô¨Årst two questions, we would like to say yes. We want this textbook to be
intuitive because intuitive ideas, and not strict proofs, lead to the kind of innovation
we seek. We also want to build a bridge connecting the basics and the cutting edge
so that our students can reach the peak quickly yet with a solid grasp of the mate-
rial. Our answer to the remaining two questions is no. This textbook is neither an
encyclopedia nor a cookbook. We selected only interesting and important topics and
left the reader to implement the codes for the algorithms after we have deliberately
removed all understanding obstacles. The reason for the latter consideration lies in
the belief contained in the following saying: ‚ÄúTell me and I‚Äôll forget; show me and I
may remember; let me try, and I can understand.‚Äù
EAs have attracted considerable interest in recent years. To understand how ‚Äúhot‚Äù
the topic is, consider the number of papers indexed by the Science Citation Index
(SCI) every year. If the reader has the access to the SCI, he/she is encouraged to do
a search to verify the hotness of this topic.
The reason the topic is so hot is mainly because of its effects on various problems,
i.e., it really works. We will introduce various application examples to show how
simple ideas can be expanded to solve complex problems.
The procedure of designing or analyzing an algorithm for solving optimization
or learning problems is full of challenges, i.e., problems are difÔ¨Åcult. This is irrele-
vant. We will accompany and assist the reader when necessary. We will demonstrate
the basic ideas behind the scary equations and try our best to make things easy to
understand. Incidentally, this cumbersome procedure is also interesting. Before and
vii

viii
Preface
during the procedure of climbing Mount Fuji, we thought it would be torture. While
at the summit, we felt that all the sufferings were interesting.
The prerequisites of this book lie in two aspects. In the mathematics part, a basic
understanding of linear algebra, multivariable calculus, probability, and statistics is
necessary.1 The other demand is derived from a proÔ¨Åciency in programming. We
assume a Ô¨Årm grasp of at least one programming language, i.e., you can implement
an algorithm. Since these requirements are in the common curriculum of junior
undergraduate programs, senior undergraduate or graduate students should be able
to read this textbook without knowledge obstacles.
The pedagogical approaches used in this text can be summarized as follows:
‚àôBuilding the mansion from the bricks. We will always focus on the key elements
of an algorithm because later they might become your tools.
‚àôFrom speciÔ¨Åc to general. We will always discuss speciÔ¨Åc simple examples before
formal expressions.
‚àôFrom idea to implementation. We will always introduce the initial notions before
discussing the concrete contents.
‚àôExplaining the critical part but leaving questions unanswered deliberately. We
will leave some obstacles deliberately in the context to activate your thoughts.
R.W. Hamming2 gave his motto as the following statement: ‚ÄúThe purpose of
computing is insight, not numbers‚Äù in his famous book Numerical Methods for Sci-
entists and Engineers. Here we would like to use it again to represent our own atti-
tude toward EAs. As algorithm designers, we care more about the solution landscape
of the problem and the corresponding search ability of the algorithms,3 although we
do seek the optimal solution to the problem. From this perspective, there will be
less ‚Äúhow-to‚Äù in this textbook for speciÔ¨Åc instructions. On the contrary, there will
be plenty of ‚Äúwhy-to‚Äùs to explain the insights and the rationale of a given algorithm
so that one can generate one‚Äôs own cookbook according to these ‚Äúwhy-to‚Äùs. We
believe that‚Äôs what the reader wants!
Other teaching and self-study considerations include plenty of footnotes for men-
tioning easily ignored yet important points, ‚ÄúSuggestions for Further Reading‚Äù sec-
tions in most chapters for further study, exercises with various degrees of difÔ¨Åculty
to deepen one‚Äôs understanding and even promote one‚Äôs own ‚Äúsmall-scale‚Äù research.
The pedagogical relationship among the ten chapters of this textbook is illus-
trated by Fig. 0.1, where the dotted circles separate the ten chapters into three cat-
egories: the basics, optimization problems, and other branches. The solid lines are
the suggested pedagogical path for teaching or self-study. Chapters 1 to 3 are all
necessary for subsequent chapters. After that, readers can skip around depending on
personal interests. Dashed lines represent the improvements, e.g., an understanding
1 Knowledge of operations research, e.g., linear programming, nonlinear programming, and com-
binatorial optimization, might contribute to one‚Äôs understanding but not be necessary.
2 Father of the Hamming distance, which will be used often in this textbook.
3 These terms will be explained explicitly and used throughout the book. Here they can be taken
as the procedure to Ô¨Ånd the optimal solution.

Preface
ix
of Chap. 4 might contribute to, but not be a prerequisite of, understanding Chap. 6
and vice versa. We hope Fig. 0.1 helps the reader form a mental map for different
branches and applications of EAs.
























	
	


	

	
	 !	
" 	

#$

	%
&
	'
(	
)*	

%
%
*	%
	
		
	%(
	
(
%
%
+

		


,''
		
	(	-	
.
	$
	-	
.
	
.
	.
 
.
	/	

Fig. 0.1 The pedagogical relationship of the ten chapters in this textbook
This book is the result of the development of the course Evolutionary Computa-
tion and Its Applications given by the Ô¨Årst author at Tsinghua University since 2002
and the numerous intensive courses taught by the second author around the world.
Too many names require mention in the acknowledgments. But we wish to express
our appreciation for our students Ô¨Årst, especially those in the seminar Evolutionary
Algorithms 2010. Without your critical comments during the seminar, we would
have no appreciation of the reader‚Äôs position, which is perhaps the most distinctive
feature of this textbook. Many of old friends gave encourage and comments on the
book, including Runwei Cheng, Baoding Liu, Kap Hwan Kim, etc. The fast response
and the professional skill of Claire Protherough at Springer and Nadja Kroke at le-
tex impressed us deeply. The Ô¨Ånal, but most important, acknowledgement belongs
to Lin Wang and Eiko Gen because you are the backbones of our lives.
Beijing China,
Xinjie Yu
Kitakyushu Japan,
Mitsuo Gen
May 2010

Contents
Part I Evolutionary Algorithms
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.1
What Are Evolutionary Algorithms Used For? . . . . . . . . . . . . . . . . . .
3
1.2
What Are Evolutionary Algorithms?. . . . . . . . . . . . . . . . . . . . . . . . . . .
6
Suggestions for Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
2
Simple Evolutionary Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.1
Introductory Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.2
Simple Genetic Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.2.1
An Optimization Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.2.2
Representation and Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.2.3
Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.2.4
Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
2.2.5
Variation Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
2.2.6
Simple Genetic Algorithm Infrastructure. . . . . . . . . . . . . . . . . 22
2.3
Evolution Strategy and Evolutionary Programming . . . . . . . . . . . . . . 25
2.3.1
Evolution Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
2.3.2
Evolutionary Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
2.4
Direction-based Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
2.4.1
Deterministic Direction-based Search . . . . . . . . . . . . . . . . . . . 28
2.4.2
Random Direction-based Search. . . . . . . . . . . . . . . . . . . . . . . . 32
2.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
Suggestions for Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
Exercises and Potential Research Projects . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
3
Advanced Evolutionary Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
3.1
Problems We Face . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
3.2
Encoding and Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
xi

xii
Contents
3.2.1
Binary Code and Related Operators . . . . . . . . . . . . . . . . . . . . . 42
3.2.2
Real Code and Related Operators . . . . . . . . . . . . . . . . . . . . . . . 45
3.2.3
Other Topics on Code and Operators . . . . . . . . . . . . . . . . . . . . 62
3.3
Selection Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
3.3.1
Dilemmas for Selection Methods . . . . . . . . . . . . . . . . . . . . . . . 64
3.3.2
Proportional Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
3.3.3
Fitness Scaling and Transferral . . . . . . . . . . . . . . . . . . . . . . . . 68
3.3.4
Ranking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
3.3.5
Tournament Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
3.4
Replacement and Stop Criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
3.4.1
Replacement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
3.4.2
Stop Criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
3.5
Parameter Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
3.5.1
Strategy Parameter Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
3.5.2
Examples of Variation Operator Control . . . . . . . . . . . . . . . . . 86
3.5.3
Examples of popsize Control . . . . . . . . . . . . . . . . . . . . . . . . . . 96
3.6
Performance Evaluation of Evolutionary Algorithms . . . . . . . . . . . . . 101
3.6.1
General Discussion on Performance Evaluation . . . . . . . . . . . 101
3.6.2
Performance Evaluation and Comparison . . . . . . . . . . . . . . . . 105
3.7
Brief Introduction to Other Topics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
3.7.1
Coevolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
3.7.2
Memetic Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
3.7.3
Hyper-heuristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
3.7.4
Handling Uncertain Environments . . . . . . . . . . . . . . . . . . . . . . 121
3.8
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
Suggestions for Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
Exercises and Potential Research Projects . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
Part II Dealing with Complicated Problems
4
Constrained Optimization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
4.1.1
Constrained Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
4.1.2
Constrained Optimization Evolutionary Algorithms . . . . . . . 137
4.2
Feasibility Maintenance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
4.2.1
Genetic Algorithm for Numerical Optimization of
Constrained Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
4.2.2
Homomorphous Mappings . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
4.3
Penalty Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
4.3.1
Static Penalty Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
4.3.2
Dynamic Penalty Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
4.3.3
Adaptive Penalty Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
4.3.4
Self-adaptive Penalty Function . . . . . . . . . . . . . . . . . . . . . . . . . 150
4.4
Separation of Constraint Violation and Objective Value . . . . . . . . . . . 150

Contents
xiii
4.4.1
Constrained Optimization Evolutionary Algorithms Based
on Rank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
4.4.2
Simple Multimembered Evolution Strategy . . . . . . . . . . . . . . 155
4.4.3
Œ± Constrained Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
4.5
Performance Evaluation of Constrained Optimization
Evolutionary Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
4.5.1
Benchmark Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
4.5.2
Performance Indices. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
4.6
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
Suggestions for Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
Exercises and Potential Research Projects . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
5
Multimodal Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
5.1
Problems We Face . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
5.1.1
Multimodal Problems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
5.1.2
Niche, Species, and Speciation . . . . . . . . . . . . . . . . . . . . . . . . . 167
5.2
Sequential Niche . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
5.3
Fitness Sharing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
5.3.1
Standard Fitness Sharing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
5.3.2
Clearing Procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
5.3.3
Clustering for Speciation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
5.3.4
Dynamic Niche Sharing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
5.3.5
Coevolutionary Shared Niching . . . . . . . . . . . . . . . . . . . . . . . . 179
5.4
Crowding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
5.4.1
Deterministic Crowding. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
5.4.2
Restricted Tournament Selection . . . . . . . . . . . . . . . . . . . . . . . 181
5.4.3
Species Conserving Genetic Algorithm . . . . . . . . . . . . . . . . . . 182
5.5
Performance Indices for Multimodal Optimization . . . . . . . . . . . . . . . 183
5.6
Application Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
5.7
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
Suggestions for Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
Exercises and Potential Research Projects . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
6
Multiobjective Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
6.1.1
Problems We Face . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
6.1.2
Terminologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
6.1.3
Why Are Evolutionary Algorithms Good at Multiobjective
Optimization Problems? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
6.2
Preference-based Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
6.2.1
Weight Sum Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
6.2.2
Compromise Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200
6.2.3
Goal Programming Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201

xiv
Contents
6.3
Vector-evaluated Genetic Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 202
6.4
Considerations for Designing Multiobjective Evolutionary
Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
6.4.1
Quality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
6.4.2
Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206
6.5
Classical Multiobjective Evolutionary Algorithms . . . . . . . . . . . . . . . 209
6.5.1
Nondominated Sorting Genetic Algorithm II . . . . . . . . . . . . . 209
6.5.2
Strength Pareto Evolutionary Algorithm 2 and Pareto
Envelope-based Selection Algorithm . . . . . . . . . . . . . . . . . . . . 211
6.5.3
Pareto Archived Evolution Strategy . . . . . . . . . . . . . . . . . . . . . 215
6.5.4
Micro-GA for Multiobjective Optimization . . . . . . . . . . . . . . 216
6.6
Cutting Edges of Multiobjective Evolutionary Algorithms. . . . . . . . . 217
6.6.1
Expanding Single-objective Evolutionary Algorithms into
Multiobjective Optimization Problems . . . . . . . . . . . . . . . . . . 217
6.6.2
Archive Maintenance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
6.6.3
Rebirth from the Ashes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
6.7
Performance Evaluation of Multiobjective Evolutionary Algorithms 234
6.7.1
Benchmark Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234
6.7.2
Performance Indices. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
6.8
Objectives vs. Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
6.8.1
Handling Constraints in Multiobjective Optimization
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
6.8.2
Multiobjective Evolutionary Algorithms for Constraint
Handling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
6.9
Application Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
6.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
Suggestions for Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
Exercises and Potential Research Projects . . . . . . . . . . . . . . . . . . . . . . . . . . . 258
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
7
Combinatorial Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
7.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
7.1.1
Combinatorial Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
7.1.2
NP-complete and NP-hard Problems . . . . . . . . . . . . . . . . . . . . 266
7.1.3
Evolutionary Algorithms for Combinatorial Optimization . . 267
7.2
Knapsack Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270
7.2.1
Problem Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270
7.2.2
Evolutionary Algorithms for Knapsack Problem . . . . . . . . . . 271
7.3
Traveling Salesman Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276
7.3.1
Problem Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276
7.3.2
Heuristic Methods for Traveling Salesman Problem . . . . . . . 278
7.3.3
Evolutionary Algorithm Code Schemes for Traveling
Salesman Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281
7.3.4
Variation Operators for Permutation Code. . . . . . . . . . . . . . . . 285
7.4
Job-shop Scheduling Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299

Contents
xv
7.4.1
Problem Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
7.4.2
Heuristic Methods for Job-shop Scheduling . . . . . . . . . . . . . . 305
7.4.3
Evolutionary Algorithm Code Schemes for Job-shop
Scheduling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310
7.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318
Suggestions for Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319
Exercises and Potential Research Projects . . . . . . . . . . . . . . . . . . . . . . . . . . . 320
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
Part III Brief Introduction to Other Evolutionary Algorithms
8
Swarm Intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
8.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
8.2
Ant Colony Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329
8.2.1
Rationale Behind Ant Colony Optimization . . . . . . . . . . . . . . 329
8.2.2
Discrete Ant Colony Optimization . . . . . . . . . . . . . . . . . . . . . . 330
8.2.3
Continuous Ant Colony Optimization . . . . . . . . . . . . . . . . . . . 336
8.3
Particle Swarm Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339
8.3.1
Organic Particle Swarm Optimization . . . . . . . . . . . . . . . . . . . 340
8.3.2
Neighbor Structure and Related Extensions . . . . . . . . . . . . . . 342
8.3.3
Extensions from Organic Particle Swarm Optimization . . . . . 347
8.4
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348
Suggestions for Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349
Exercises and Potential Research Projects . . . . . . . . . . . . . . . . . . . . . . . . . . . 350
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351
9
ArtiÔ¨Åcial Immune Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355
9.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355
9.2
ArtiÔ¨Åcial Immune System Based on Clonal Selection. . . . . . . . . . . . . 357
9.2.1
Clonal Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357
9.2.2
Clonal Selection Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359
9.2.3
ArtiÔ¨Åcial Immune System for Multiobjective Optimization
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361
9.3
ArtiÔ¨Åcial Immune System Based on Immune Network . . . . . . . . . . . . 364
9.3.1
Immune Network Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364
9.3.2
Continuous Immune Network . . . . . . . . . . . . . . . . . . . . . . . . . . 366
9.3.3
Discrete Immune Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368
9.4
ArtiÔ¨Åcial Immune System Based on Negative Selection . . . . . . . . . . . 370
9.4.1
File Protection by Negative Selection . . . . . . . . . . . . . . . . . . . 371
9.4.2
Intrusion Detection by Negative Selection. . . . . . . . . . . . . . . . 373
9.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375
Suggestions for Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376
Exercises and Potential Research Projects . . . . . . . . . . . . . . . . . . . . . . . . . . . 377
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377

xvi
Contents
10
Genetic Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381
10.1 Introduction to Genetic Programming. . . . . . . . . . . . . . . . . . . . . . . . . . 381
10.1.1 The Difference Between Genetic Programming and
Genetic Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381
10.1.2 Genetic Programming for Curve Fitting . . . . . . . . . . . . . . . . . 382
10.2 Other Code Methods for Genetic Programming . . . . . . . . . . . . . . . . . 390
10.2.1 Gene Expression Programming . . . . . . . . . . . . . . . . . . . . . . . . 390
10.2.2 Grammatical Evolution for Solving Differential Equations . . 392
10.3 Example of Genetic Programming for Knowledge Discovery . . . . . . 395
10.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397
Suggestions for Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 398
Exercises and Potential Research Projects . . . . . . . . . . . . . . . . . . . . . . . . . . . 399
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 400
A
Benchmark Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 403
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 409
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411

Part I
Evolutionary Algorithms


Chapter 1
Introduction
Abstract Perhaps this is a required textbook for a course, perhaps you want to learn
about evolutionary algorithms (EAs), or perhaps you just pick up this book occa-
sionally. In this simple chapter, we will discuss the necessity, deÔ¨Ånition, original
idea, branches, and information resources of EAs. We hope it will command your
attention and stimulate you to read the other chapters.
1.1 What Are Evolutionary Algorithms Used For?
In the procedure of exploring the natural laws in science or fabricating useful prod-
ucts in engineering, we often face multiple problems. Two of them are the main
focus of this textbook, i.e., the optimization problem and the learning problem.
For optimization, we want to Ô¨Ånd the optimal solution within the variables‚Äô deÔ¨Å-
nition domain. Provided that you have a basic grasp of multivariable calculus, for a
differentiable scalar function f(x,y,z) in a 3-D Cartesian coordinate system, its gra-
dient is deÔ¨Åned as ‚àáf =
(
‚àÇf
‚àÇx i, ‚àÇf
‚àÇy j, ‚àÇf
‚àÇz k
)
, where i, j, and k are unit vectors for three
coordinates. The direction of ‚àáf is the fastest increasing direction of function f and
the norm of ‚àáf is the extent of the fastness. Thus optimization problems seem easy.
We can start from any initial point and use the gradient to guide the search until we
Ô¨Ånd the point whose gradient is near the zero vector.
Unfortunately this is only applicable for some types of problems. For combina-
torial optimization, there is no deÔ¨Ånition of differentiation. For multiobjective opti-
mization, we face a vector function f, and generally there is no point in the deÔ¨Ånition
domain whose multiple objective values are all better than those of other points. For
constrained optimization, searching along the gradient might converge to a solution
that violates some constraints. Even in differentiable unconstrained optimization,
gradients might not work. Let us take the Griewank function as an example [1].
The Griewank function can be scaled to n dimensions. Here we only discuss the
two-variable version as follows:
3

4
1 Introduction
min f (x,y) = x2 +y2
4000 ‚àícos(x)cos
( y
‚àö
2
)
+1
(1.1)
It is quite easy to understand that the global optimal solution is x = y = 0.1 We
can draw the mesh graph for Eq. 1.1 with the deÔ¨Ånition domain (‚àí30 ‚â§x,y ‚â§30),
illustrated by Fig. 1.1.
Fig. 1.1 The mesh graph of the 2-D Griewank function
This is bad! There are so many local optimal solutions (multimodal) that the
gradient method with a randomly generated initial solution will converge to one of
them with a large probability.
These four types of optimization problems, i.e., constrained optimization, mul-
timodal optimization, multiobjective optimization, and combinatorial optimization,
form the central core of optimization and thus the main topics in the Ô¨Åeld of op-
erations research (OR). Researchers in OR have suggested many techniques for
these problems. Among them, EAs have unique properties and have been attracting
increasing attention.2
For learning, we want the computer to have human ‚Äúintelligence‚Äù to some extent,
i.e., artiÔ¨Åcial intelligence(AI). A simple example is illustrated by the scatter graph
of the ‚Äúchainlink dataset‚Äù in Fig. 1.2 [2].
1 But computers do not know.
2 Chapters 4 ‚Äì 7 mainly discuss optimization problems.

1.1 What Are Evolutionary Algorithms Used For?
5
The chainlink dataset has 1000 data and each of them belongs to one of the two
groups. In Fig. 1.2, we use + to represent group 1 and ‚àòto represent group 2. As
human beings, we can differentiate these two groups easily even if they are all rep-
resented with the same symbol. If our algorithms can identify the two groups (with
or without knowledge of the group number) only with the 3-D coordinates of each
datum, i.e., elucidating which datum belongs to which group, we say the algorithm
has the ‚Äúintelligence‚Äù of clustering. This type of learning problem is called unsu-
pervised learning, i.e., no teaching process in the clustering process.














	





	





Fig. 1.2 The chainlink dataset
The chainlink dataset can also be used in another way. We randomly select 500
of them as the training set with their coordinates and group tags and use these 500
data to ‚Äútrain‚Äù our algorithm. After training, we use the other 500 data as the testing
set with only coordinates to test whether our algorithm can classify them correctly.
This type of learning problem is called supervised learning, i.e., a training process
takes place before the testing process. The aforementioned learning problem is also
called classiÔ¨Åcation because its results are a discrete class number. If the tags of
the data is not the class number but the real value, i.e., we consider the previous
coordinates as the input of a system and the tags as the output of the system, this
kind of leaning problem is called regression. Many learning algorithms have been
proposed by researchers in AI. Among them are EAs.3
3 Chapters 8 ‚Äì 10 mainly discuss learning problems.

6
1 Introduction
Optimization problems and learning problems are correlated with each other.
Sometimes the function that needs to be optimized is so complex that we cannot
afford to calculate the objective value for every solution. Then the learning algorithm
could be used to do Ô¨Åtness approximation, i.e., ‚Äúintelligently‚Äù generate a simpler
function using previously evaluated solutions to replace the original function. On
the other hand, for both clustering problems and regression problems, we could
suggest a function to represent the clustering performance or the regression error.
Then the goal of improving the cluster performance or decreasing the regression
error would be equal to an optimization problem.
These two types of problems come from OR and AI respectively, and could be
applied to many other science and engineering disciplines. The relationship between
OR, AI, EAs, and other application disciplines is illustrated by Fig. 1.3
Fig. 1.3 The relationship
between OR, AI, EAs, and
other application disciplines
















	'
	%
/
	





	
%
%
	
 %
%
0000

1.2 What Are Evolutionary Algorithms?
Many great inventions have been the result of bionics, i.e., the application of bio-
logical or natural principles to the study and design of human systems. We mimic
bats to invent radar, Ô¨Åsh to invent submarine, etc. The natural evolution of species
could be looked at as a process of learning how to adapt to the environment and
optimizing the Ô¨Åtness of the species. So we could mimic the viewpoint of modern
genetics, i.e., ‚Äúsurvival of the Ô¨Åttest‚Äù principle, in designing optimizing or learning
algorithms. Enter EAs.
EAs4 are algorithms that perform optimization or learning tasks with the ability
to evolve. They have three main characteristics:
4 EAs are also called evolutionary computation (EC) and evolutionary intelligence (EI).

1.2 What Are Evolutionary Algorithms?
7
‚àôPopulation-based. EAs maintain a group of solutions, called a population, to op-
timize or learn the problem in a parallel way. The population is a basic principle
of the evolutionary process.
‚àôFitness-oriented. Every solution in a population is called an individual. Every
individual has its gene representation, called its code, and performance evalua-
tion, called its Ô¨Åtness value. EAs prefer Ô¨Åtter individuals, which is the foundation
of the optimization and convergence of the algorithms.
‚àôVariation-driven. Individuals will undergo a number of variation operations to
mimic genetic gene changes, which is fundamental to searching the solution
space.
Since the 1960s, many algorithms with population-based, Ô¨Åtness-oriented, and
variation-driven properties have been proposed. The timeline of the various EAs
that will be introduced in this textbook is illustrated in Fig. 1.4.
&)
))
&
12 
%$'3
&
+
14
3
&
.15%3
&"
*163
&&
/1,
%3
&&
./17( 

	3
)))
		

1*
3
&&"
,1	
.
3
&#
15

3
&&
*.17-3
Fig. 1.4 Timeline of various EAs introduced in this textbook
Several correlated concepts need to be differentiated here.
Soft computing uses inexact solution methods for computationally hard tasks
such as the solution of NP-complete problems [3‚Äì7].5 It includes expert systems, ar-
tiÔ¨Åcial neural networks, fuzzy systems, and EAs conventionally, but more and more
people are including swarm intelligence in soft computing. Soft computing puts
more focus on ‚Äúcomputing.‚Äù
Computational intelligence is another, relatively new, related concept whose fo-
cus is more on ‚Äúintelligence‚Äù [8‚Äì13]. Its three basic elements are also artiÔ¨Åcial neu-
ral networks, fuzzy systems, and EAs. Some researchers would like to include rough
set, but others think artiÔ¨Åcial immune system should be included.
For hard problems, the exact algorithm that would guarantee the global optimal
solution would be found within an acceptable time frame might not exist. Thus
many heuristic algorithms have been proposed suggested by different researchers.
Heuristic algorithms can generate a solution of acceptable quality relatively quickly.
But no guarantee for optimality can be made and the time to generate solutions is
also long in the worst case. Another notable property of heuristic algorithms is that
they are problem dependent.
5 This concept will be introduced in Sect. 7.1.2.

8
1 Introduction
People use metaheuristics to represent those algorithms that can be widely used
and generate high-quality solutions in general, i.e., metaheuristics is supposed to
be at the higher level of heuristics [14‚Äì22]. Compared to the other two concepts
explained above, metaheuristics focuses on ‚Äúoptimization.‚Äù Its three elements are
EAs, tabu search, andsimulated annealing.
Figure 1.5 illustrates the number of papers indexed by the Science Citation Index
(SCI) on EAs.6 Perhaps several hundred papers per year is not impressive. But here
we only use ‚Äúbroad‚Äù terms to do search. In the following chapters, we will see the
results of speciÔ¨Åc algorithms. These data will be impressive.





	
	




	

	




	








 

	

 





Fig. 1.5 Number of papers indexed by SCI on EAs
Suggestions for Further Reading
As has been illustrated in Sect. 1.1, EAs are interdisciplinary areas. Thus any infor-
mation related to OR, AI, statistics, probability, etc. might relate to EAs. Here we
only recommend several directly related conferences, journals, and books.
The two largest annual conferences on EAs are Genetic and Evolutionary Com-
putation COnference (GECCO) and IEEE Congress on Evolutionary Computation
(CEC). Apart from these conferences, the International Conference on Parallel Prob-
lem Solving from Nature (PPSN) (biannual), the European Conference on Evolu-
tionary Computation, and various symposia organized by the IEEE Computational
Intelligence Society are all conferences with considerable inÔ¨Çuence.
The top two journals on EAs are IEEE Transactions on Evolutionary Compu-
tation published by IEEE and Evolutionary Computation published by MIT Press.
6 TS = (‚Äúevolutionary computation‚Äù OR ‚Äúevolutionary computing‚Äù OR ‚Äúevolutionary algorithms‚Äù
OR ‚Äúevolutionary intelligence‚Äù). The SCI index ‚ÄúTS‚Äù is for the search topic in the title, the key-
words, and the abstract.

References
9
Other major journals are IEEE Transactions on Systems, Man, and Cybernetics pub-
lished by IEEE; Evolutionary Intelligence, Genetic Programming and Evolvable
Machines, Journal of Heuristics, Soft Computing, Natural Computing published by
Springer; Computers & Industrial Engineering, Computers & Operations Research,
and European Journal of Operational Research published by Elsevier.
Since the birth of EAs, many textbooks have been written to represent the cutting
edge at that time. The notable books include those by Goldberg [23], B¬®ack [24],
Michalewicz [25, 26], Eiben [27], Haupt [28], Burke [29], Sivanandam [30], and
Sumathi [31], etc. The books by Mitchell [32], Fogel [33], and Spears [34] also
provide insights from various perspectives. At the application level, Gen‚Äôs three
books [35‚Äì37] and those by Ashlock [38] and Yu [39] provide a comprehensive
introduction.
References
1. Griewank AO (1981) Generalized descent for global optimization.
J Optim Theory Appl
34(1):11‚Äì39
2. Ultsch A (2005) Clustering with SOM: U*C.
In: Proceedings of the Workshop on Self-
Organizing Maps, 75‚Äì82
3. Aliev RA, Aliev R (2001) Soft computing and its applications. World ScientiÔ¨Åc, Singapore
4. Tettamanzi A, Tomassini M (2001) Soft computing: integrating evolutionary, neural, and fuzzy
systems. Springer, Berlin Heidelberg New York
5. Karray FO, Silva CWD (2004) Soft computing and intelligent systems design: theory, tools
and applications. Addison-Wesley, Reading, MA
6. Pratihar DK (2007) Soft computing. Alpha Science, Oxford, UK
7. Maimon OZ, Rokach L (2008) Soft computing for knowledge discovery and data mining.
Springer, Berlin Heidelberg New York
8. Konar A (2005) Computational intelligence: principles, techniques and applications. Springer,
Berlin Heidelberg New York
9. Andina D (2007) Computational intelligence. Springer, Berlin Heidelberg New York
10. Eberhart RC, Shi Y (2007) Computational intelligence: concepts to implementations. Morgan
Kaufmann, San Francisco
11. Engelbrecht AP (2007) Computational intelligence: an introduction, 2nd edn. Wiley, New
York
12. John F, Jain LC (2008) Computational intelligence: a compendium. Springer, Berlin Heidel-
berg New York
13. Rutkowski L (2008) Computational intelligence: methods and techniques. Springer, Berlin
Heidelberg New York
14. Resende M, de Sousa JP (2003) Metaheuristics: computer decision-making. Springer, Berlin
Heidelberg New York
15. Glover FW, Kochenberger GA (2003) Handbook of metaheuristics. Springer
16. Gandibleux X, Sevaux M, S¬®orensen K et al (2004) Metaheuristics for multiobjective optimi-
sation. Springer, Berlin Heidelberg New York
17. Rego C, Alidaee B (2005) Metaheuristic optimization via memory and evolution: tabu search
and scatter search. Springer, Berlin Heidelberg New York
18. Dr¬¥eo J, P¬¥etrowski A, Siarry P et al (2005) Metaheuristics for hard optimization: methods and
case studies. Springer, Berlin Heidelberg New York
19. Gonzalez TF (2007) Handbook of approximation algorithms and metaheuristics. Chapman
and Hall/CRC, Boca Raton, FL

10
1 Introduction
20. Siarry P, Michalewicz Z (2007) Advances in metaheuristics for hard optimization. Springer,
Berlin Heidelberg New York
21. Blum C, Aguilera MJB, Roli A et al (2008) Hybrid metaheuristics: an emerging approach to
optimization. Springer, Berlin Heidelberg New York
22. Talbi E (2009) Metaheuristics: from design to implementation. Wiley, New York
23. Goldberg DE (1989) Genetic algorithms in search, optimization, and machine learning.
Addison-Wesley, Boston, MA
24. B¬®ack T (1996) Evolutionary algorithms in theory and practice: evolution strategies, evolution-
ary programming, genetic algorithms. Oxford University Press, Oxford, UK
25. Michalewicz Z (1998) Genetic algorithms + data structures = evolution programs. Springer,
Berlin Heidelberg New York
26. Michalewicz Z, Fogel DB (2004) How to solve it: modern heuristics. Springer, Berlin Heidel-
berg New York
27. Eiben AE, Smith JE (2003) Introduction to evolutionary computing. Springer, Berlin Heidel-
berg New York
28. Haupt RL, Haupt SE, L R (2004) Practical genetic algorithms, 2nd edn. Wiley, New York
29. Burke EK, Kendall G (2006) Search methodologies: introductory tutorials in optimization and
decision support techniques. Springer, Berlin Heidelberg New York
30. Sivanandam SN, Deepa SN (2007) Introduction to genetic algorithms. Springer, Berlin Hei-
delberg New York
31. Sumathi S, Hamsapriya T, Surekha P (2008) Evolutionary intelligence: an introduction to
theory and applications with Matlab. Springer, Berlin Heidelberg New York
32. Mitchell M (1998) An introduction to genetic algorithms. MIT Press, Cambridge, MA
33. Fogel DB, Michalewicz Z (2001) An introduction to evolutionary computation. IEEE, Piscat-
away, NJ
34. Spears WM (2004) Evolutionary algorithms: the role of mutation and recombination.
Springer, Berlin Heidelberg New York
35. Gen M, Cheng R (1997) Genetic algorithms and engineering design. Wiley-Interscience, New
York
36. Gen M, Cheng R (1999) Genetic algorithms and engineering optimization.
Wiley-
Interscience, New York
37. Gen M, Cheng R, Lin L (2008) Network models and optimization: multiobjective genetic
algorithm approach. Springer, Berlin Heidelberg New York
38. Ashlock D (2006) Evolutionary computation for modeling and optimization. Springer, Berlin
Heidelberg New York
39. Yu T, Davis L, Baydar C et al (2008) Evolutionary computation in practice. Springer, Berlin
Heidelberg New York

Chapter 2
Simple Evolutionary Algorithms
Abstract In Chap. 1, we drew a graph for EAs with the statement that EAs are
interesting, useful, easy-to-understand, and hot research topics. Starting with Chap.
2, we will demonstrate EAs in a pedagogical way so that you can enjoy the journey
around EAs with active reading. We strongly encourage readers to implement their
basic EAs in this chapter in one programming environment and improve its search
ability through other chapters. Footnotes, exercises, and possible research projects
are of great value for an in-depth understanding of the essence of the algorithms.
2.1 Introductory Remarks
Before introducing some standard EAs,1 we need to standardize some terms used
throughout the book.
Classical EAs, including genetic algorithms (GAs), evolution strategy (ES), evo-
lutionary programming (EP), and genetic programming (GP),2 are all random-based
solution space searching metaheuristic algorithms. So the most important thing be-
fore discussing a concrete algorithm is how to generate and manipulate random
numbers in a programming environment.
We summarize the functions for handling random numbers in MATLAB‚ìá,
C/C++, and Java in Table 2.1. As you can see, MATLAB‚ìáhas advantages in gen-
erating random numbers in a Ô¨Çexible way. So we suggest using MATLAB‚ìáas the
programming environment while learning EAs.
Even though MATLAB‚ìáhas provided a Genetic Algorithm and Direct Search
Toolbox, we strongly suggest that readers make their own EA source code from
scratch if your purpose in reading this textbook is to really understand how EAs
work and maybe improve some famous EAs to some degree.
1 We sometimes call these standard evolutionary algorithms ‚Äúsimple‚Äù algorithms in the sense that
they are simple compared to the improvements introduced in Chap. 3. But we need to mention that
simple algorithms here do not mean weak performance.
2 We will discuss the Ô¨Årst three algorithms in this chapter and introduce GP in Chap. 10.
11

12
2 Simple Evolutionary Algorithms
Table 2.1 The most common functions related to random numbers


,	
 	
89:;
<==
>

?'
	
 	1)@3


1'	3
13<
24,AB
	0


4
	
 	1)@3



+	*

2
		 	$
	%







2	$
'	(




2	$
%	'	(
'

'

'


2	$

		%






















The density function of a uniform distribution random number in the range (0,1),
denoted as Œæ ‚àºU (0,1), is as follows:
p(Œæ) =
{
1
0 < Œæ < 1
0
otherwise
(2.1)
Every programming language provides a uniform distribution3 random func-
tion. After generating Œæ ‚àºU (0,1), there are many ways to convert it into other
distributions [1, 2]. The most important of these are normal distributions, denoted
Œ∑ ‚àºN
(
Œº,œÉ2)
, whose density function is as follows:
p(Œ∑) =
1
‚àö
2œÄœÉ e‚àí(Œ∑‚àíŒº)2
2œÉ2
(2.2)
where Œº is the expectance and œÉ > 0 is the standard deviation. It is easy to verify
that if Œæ ‚àºN (0,1), then Œ∑ = (Œæ √ó œÉ + Œº) ‚àºN
(
Œº,œÉ2)
. So N (0,1) is very critical
for generating normal distribution random numbers.
A permutation in the range [1,n] is often used in evolutionary combinatorial op-
timization. Thus one should be familiar with how to generate permutations and ran-
dom permutations.
Sometimes we need to generate an integer random number with uniform distri-
bution in the range [1,n], where n is an integer number. We can either combine the
round function and n ‚àórand() or select the Ô¨Årst cell in the random permutation in
the range [1,n].
In EAs, we often say that an operator (such as a crossover or mutation, discussed
later) needs to be carried out with probability p. How does one implement such
a simple statement in a given programming environment? We give the example in
MATLAB ‚ìáas follows.4 These are similar in other environments.
3 Also called a Gaussian distribution.
4 We strongly encourage readers to make it clear why this ‚ÄúIf‚Äù statement could represent that the
operator needs to be carried out with probability p.

2.1 Introductory Remarks
13
%Operator A is carried out with probability p
If rand < p
Operator A
End
where rand ‚àºU(0,1).
With respect to selection, there are two ways to carry it out. We often pick up
one solution randomly from current solutions and determine whether it could be se-
lected. We will discuss the selection criteria in detail in later sections of this chapter
and in Chap. 3. Here we would just like to determine how to handle the one being
picked up. If it is put back, regardless of whether it is selected or not, and has the
chance to be picked up again in the next time, we call this selection with replace-
ment. If it is discarded, regardless of whether it is selected or not, and will never be
picked up again, we call this selection without replacement.5
Another term that often appears in the EA literature is norm, which could be seen
as some kind of length measure of vectors. ‚à£‚à£u‚à£‚à£is vector u‚Äôs norm. Suppose u is a
real vector with n variables. The general p(‚â•1) norm for u is
‚à•u‚à•p = (‚à£u1‚à£p +‚ãÖ‚ãÖ‚ãÖ+‚à£un‚à£p)1/p
(2.3)
where
		u j
		 is the absolute value of uj. If p = 1, then Eq. 2.3 is 1-norm; it is the sum
of the absolute values of all cells.
‚à•u‚à•1 = (‚à£u1‚à£+‚ãÖ‚ãÖ‚ãÖ+‚à£un‚à£)
(2.4)
If p = 2, 2-norm, it is the Euclidean distance.
‚à•u‚à•2 =
‚àö
u2
1 +‚ãÖ‚ãÖ‚ãÖ+u2n
(2.5)
If p = ‚àû, ‚àû-norm.
‚à•u‚à•‚àû= max(‚à£u1‚à£,‚ãÖ‚ãÖ‚ãÖ,‚à£un‚à£)
(2.6)
The Ô¨Ånal concept is convex function and concave function. A function f is convex
if any two points x1 and x2 in the deÔ¨Ånition domain satisfy Eq. 2.7 for any 0 ‚â§r ‚â§1,
which is illustrated in Fig. 2.1.6
f (rx1 +(1‚àír)x2) ‚â§r f (x1)+(1‚àír) f (x2)
(2.7)
For points x1 and x2, rx1 + (1‚àír)x2 is their convex combination. Inverting the
inequality deÔ¨Ånes a concave function.
5 Sometimes we call this sampling with replacement or sampling without replacement.
6 To call a curve convex or concave according to its geometric shape depends on where it is facing.
In the deÔ¨Ånition, we look at the origin.

14
2 Simple Evolutionary Algorithms
Fig. 2.1 A convex function

















 






	 
 	 








 


	
	13
	13







	 
 


2.2 Simple Genetic Algorithm
2.2.1 An Optimization Problem
The basic ideas of GAs were introduced in Chap. 1. In this section, we‚Äôd like to
introduce the details of implementing GAs with an optimization algorithm. The
problem is formulated by Eq. 2.8 and illustrated by Fig. 2.2.
max f (x) = xsin(10œÄx)+2.0
s.t.
‚àí1 ‚â§x ‚â§2
(2.8)

  

  

  


  

  

  

  


	
Fig. 2.2 A numerical example of an SGA
The curve of the objective function in Fig. 2.2 constitutes the solution landscape
in which we are searching for the optimal solution. In this problem, it has many local
optima, and, based on Fig. 2.2, the problem may seem difÔ¨Åcult. We will demonstrate
the strength of simple genetic algorithm (SGA) on this problem by the discussion in
the following subsections.

2.2 Simple Genetic Algorithm
15
In SGA, we maintain many individuals in a population. The number of individ-
uals in the population is the population size (popsize). Each individual7 has two
properties: its location (chromosome8 composed of genes) and its quality (Ô¨Åtness
value). After obtaining the quality of all individuals, we use the selection process to
generate a mating pool. Individuals with higher quality should have a higher prob-
ability of being selected into the mating pool so that the good ones will have more
chances to breed and the bad ones will not be selected. Individuals in the mating
pools are called parents. Generally two parents might be selected randomly from
the mating pool to generate two offspring9 without replacement and every offspring
might undergo small changes to become a new individual. Then the newly generated
population replaces the old one and another generation starts.
The relationship between the concepts and the operators described above and the
principle of Darwinian natural selection theory is listed below.
‚àôSelection ‚áê‚áísurvival of the Ô¨Åttest
‚àôTwo parents generate two offspring ‚áê‚áícrossover or recombination
‚àôSmall changes in the location of the offspring ‚áê‚áímutation
A good individual has more chances to be selected into the mating pool so that it
has more chances to mate than low-quality individuals have. Then the information
contained in the good individual has more chances to be preserved and passed onto
next generation. Information exchange between two parents and small changes in
the offspring promote the search for better individuals. Combining these two factors,
the population will become more and more Ô¨Åt until the optimal or near optimal
solution has been found, if we are lucky. In this way, GAs could gain favor over
traditional gradient-based algorithms. We will discuss these operators step by step.
2.2.2 Representation and Evaluation
We can use a real number, in the range [‚àí1,2], to represent a solution in Eq. 2.8
directly. Many operators can handle real number representation. But we use the
binary code or binary representation here for two reasons. GAs were originally
proposed to be binary code to imitate the genetic encoding of natural organisms.
On the other hand, binary code is good for pedagogy. A binary chromosome is
necessary to represent a solution x in the scale [‚àí1,2]. The same holds for the binary
representation of real numbers in a computer.
In binary code, we cannot represent a real number completely correctly, so a
tradeoff is necessary. A tolerance needs to be deÔ¨Åned by the user, which means the
errors below the tolerance are extraneous. If we divide the deÔ¨Ånition domain into
7 An individual may be understood as an agent.
8 Sometimes a chromosome is called a genome. These two terms have different meanings in ge-
netics and biology, but we disregard them in EAs.
9 Sometimes they are called progeny or descendants.

16
2 Simple Evolutionary Algorithms
21 = 2 parts evenly and select the smallest number in the parts to represent any
number in the division, we can only represent ‚àí1 and 0.5 by 0 and 1 respectively.
22 = 4 divisions make the 00, 01, 10, and 11 represent ‚àí1, ‚àí0.25, 0.5, and 1.25,
respectively. The larger division number we select, the less error there is in repre-
senting a real number on binary code. Suppose we use 100 binary codes to represent
a real number in the range [‚àí1,2]; the maximum error is 3/2100 ‚âà2.37‚àí30, which
would be satisfactory for most users. In this way,10 we can represent a real number
with any accuracy requirements.
In this problem, we use l = 12 binary codes to represent one real number11 as
follows, which constitutes a chromosome to be evolved.
Fig. 2.3 Binary representa-
tion of a real number










 )  ) )   )  )  
 ) & # "       )
For 12 binary codes in the chromosome, every part is called a gene. A gene has
two properties: its value (sometime called allele), which is the number in the square,
and its location (sometimes called locus), which is the number above the square. For
the binary representation of a real number with l genes, its counterpart real number
is
x =
l‚àí1
‚àë
i=0
ai2i
2l
√ó
(
¬Øx‚àíx‚àí
)
+x‚àí
(2.9)
where ai is the allele of locus i, ¬Øx and x‚àíare the upper and lower bounds for the real
number, respectively. For the chromosome in Fig. 2.3, its counterpart real number
is x =
(
211 +29 +26 +25 +23 +21 +20/
212)
√ó(2+1)‚àí1 = 0.9534. The process
of Ô¨Ånding one way to represent a solution to the problem is called representation.
Figure 2.3 illustrates binary encoding and Eq. 2.9 represents binary decoding.
We can use both 101001101011 and 0.9534 to represent an individual (or a chro-
mosome); the former is called a genotype representation of an individual and the
lattera phenotype representation of an individual.
Every individual has two properties: its location and its quality. The location is
the chromosome we described above and the quality is its objective value, which
can be evaluated by Eq. 2.8. The objective value of our example individual is
f(x) = 0.9534√ósin(10œÄ √ó0.9534)+2.0 = 1.0520. In GAs, we often use the Ô¨Åtness
value to evaluate how much the individual Ô¨Åts the problem. So the function used to
calculate the Ô¨Åtness value is called the Ô¨Åtness function, which is exactly the same as
10 Even though this might weaken your faith in computers.
11 What is the maximum error for this representation? Why do we only use l = 12? Why not
l = 300?

2.2 Simple Genetic Algorithm
17
the objective function in Eq. 2.8.12 The process of obtaining a Ô¨Åtness value from a
chromosome is called evaluation.
2.2.3 Initialization
SGA operates on a group of individuals. Generally the algorithm designers need to
deÔ¨Åne how many individuals will be in the population, which is often represented by
a variable popsize.13 We need to generate popsize individuals to start the evolving
process. This procedure is called initialization.
Sometimes we know which part of the deÔ¨Ånition domain contains better solu-
tions. But in most situations, we do a blind search over the solution landscape.14
So we could just initialize the population randomly. Another reason for random ini-
tialization is that SGA is capable of a global search, which will be illustrated later.
In some complicated real-world problems, workable solutions may be in the local
optimal solution‚Äôs domain of attraction15 so that starting from this domain may not
be good for a global search. So why not depend on the global search ability of the
SGA?
There are many ways to initialize popsize individuals randomly. The simplest
way might be to generate every individual with uniform distribution. SpeciÔ¨Åcally,
for every gene in the chromosome, its allele is 1 with probability 0.5, and vice
versa.16
Another way to generate evenly distributed individuals in the deÔ¨Ånition domain
is to divide the domain into several grids. Initially, a grid is randomly selected and
a solution in that grid is in turn randomly selected. We need the encoding procedure
to transfer the phenotype to the genotype to get the chromosome.17 We count the
12 After reading the Sect. 2.2.4 below, consider why we could use the objective value as the Ô¨Åtness
value directly. What is the Ô¨Åtness function in other situations, i.e., with a negative objective value,
minimum optimization, etc.? Chap. 3 will discuss this interesting problem.
13 DeÔ¨Åning popsize without any a priori knowledge about the problem is a very hard job for
algorithm designers. So either we need some kind of trial-and-error adjustment or we adopt some
information from a current population and change popsize according to that information. We will
discuss the latter interesting idea in Chap. 3.
14 The results of other optimization algorithms, currently workable solutions, and uneven sampling
on the deÔ¨Ånition domain according to the preference of users are examples of nonblind initializa-
tion.
15 The domain of attraction means a subset of the deÔ¨Ånition area. For some search technologies,
starting at any point in the domain of attraction will converge to the optimum in that domain even
if it is a local optimum. Consider any x ‚àà[‚àí0.9,‚àí0.8] in Fig. 2.2; it will converge to the local
optimum f = 2.8 with any up-hill algorithm.
16 Readers unfamiliar with this should review Sect. 2.1.
17 In our binary representation example, readers may check for methods of transforming a real
number into its binary code in the computer basis textbook.

18
2 Simple Evolutionary Algorithms
number of individuals generated from a grid. The larger the number for a grid is, the
less opportunity the grid will have to generate new ones.18
After the initial chromosomes have been generated, their Ô¨Åtness values are cal-
culated using a decoding process (Eq. 2.9) and the objective function (Eq. 2.8). It
bears mentioning again that an individual is comprised of a chromosome and a Ô¨Åt-
ness value.
For the problem illustrated by Eq. 2.8 and Fig. 2.2, we set popsize = 10 and
obtain the ten randomly generated initial individuals, illustrated by Fig. 2.4.
















Fig. 2.4 Initial population of an SGA
The crosses in Fig. 2.4 are the initial individuals. As can be seen, the performance
of the initial population is not impressive at all.
2.2.4 Selection
After initialization, the SGA enters the main loop. It starts with a selection process,
which imitates natural selection by granting Ô¨Åtter individuals higher opportunity
to breed, and ends with two variation operators,19 crossover and mutation, which
imitate natural reproduction by exchanging genes of parents to generate offspring.
In programming, we need to open another memory to reserve the individuals
selected to breed. This memory is called the mating pool.
18 Consider how to implement this idea in your program.
19 These operators are also called reproductive operators.

2.2 Simple Genetic Algorithm
19
There are many ways to embody the idea of natural selection. We will discuss
one here and introduce others in Chap. 3. Individual i20 in the current population
has a Ô¨Åtness value fi . According to natural selection, Ô¨Åtter individuals have more
advantages in breeding. So we could deÔ¨Åne the relative Ô¨Åtness value of individual i
as follows:
pi =
fi
‚àëpopsize
i=1
fi
(2.10)
It is easy to verify that
popsize
‚àë
i=1
pi = 1. Thus the relative Ô¨Åtness value can also be seen
as the probability of being selected for individual i, i.e., pi could be seen as the
probability of being selected as one candidate in the mating pool for individual i.21
How can we implement this in a programming environment? We can do the se-
lection as in roulette way.22 The difference between real roulette and our selection
is that the holes of the roulette wheel in our selection have different sizes. The larger
pi is, the larger hole individual i has. So the ball could drop into the hole with higher
probability. The size of the hole for i is propositional to pi, which is illustrated by
Fig. 2.5. Instead of digging a hole on the wheel, we use sectors with different central
angles to represent individuals. The central angle of individual i is 2œÄpi. The thick
arrow in Fig. 2.5 represents the ball. We want the arrow to start rotating clockwise
and stop randomly with a central angle 2œÄ‚ãÖrand, where rand ‚àºU(0,1). If the arrow
stops at one sector, the corresponding individual is selected into the mating pool. It
is clear that Ô¨Åtter individuals have more chances to be selected.
How is the roulette wheel implemented in a programming environment? We
could simulate the rotation process by an accumulated process. After obtaining
rand, we know where the arrow will be. We memorize the individual we have al-
ready passed during the rotation process, accumulate the probabilities, and compare
it to rand. Whenever we Ô¨Ånd an individual that is satisÔ¨Åed that
k
‚àë
i=1
pi < rand <
k+1
‚àë
i=1
pi,
we know that the arrow stops in sector i23 and select individual i into the mating
pool. Then the arrow returns to the original location, just like in Fig. 2.5. We can do
the above procedure popsize times until there are popsize individuals in the mating
pool. This selection procedure is called roulette wheel selection (RWS).
In this way, some individuals in the population will be selected more than once
and some will never be selected. The probability of being selected for individual i
is its relative Ô¨Åtness. It is also necessary to mention that Ô¨Åtter individuals are not to
be selected by RWS if they are unlucky enough never to have the arrow stop at their
sector popsize times. We call this phenomenon selection bias. Many studies have
20 Sometimes we also use indi to represent individual i.
21 This sentence has the implicated meaning that we want to select the candidates in the mating
pool in a serial way. We will introduce a parallel way in Chap. 3.
22 Roulette is a gambling game in which a ball is dropped onto a wheel with numbered holes in it
while the wheel is spinning round.
23 How can we make such as statement?

20
2 Simple Evolutionary Algorithms












Fig. 2.5 Roulette wheel selection
been done to minimize the selection bias, and we will introduce some of them in
later chapters.
Another consideration about RWS is that the problem needs to be maximum and
all the objective values need to be greater than zero so that we can use objective
values as Ô¨Åtness values and take RWS as the selection process directly.24
2.2.5 Variation Operators
There are many variation operators to change information in individuals in the mat-
ing pool. If information exchange, i.e., gene exchange, is done between two or more
individuals25, this variation operator is called crossover or recombination. If the
genes of one individual changes on its own, this variation operator is called muta-
tion. We will introduce single-point crossover and bit-Ô¨Çip mutation here.
There are two ways to select two individuals in the mating pool to determine
whether or not to cross over them. One is to shufÔ¨Çe the mating pool randomly
and assign individuals 1 and 2 without replacement to be a crossover pair, 3 and
4 without replacement to be another pair, etc. The other is to generate a random
integer permutation, per, between [1, popsize]. per(i) = j means the ith element in
the permutation is the jth individual in the mating pool. Then we assign indper(1)
and indper(2) without replacement as the Ô¨Årst crossover pair, indper(3) and indper(4)
without replacement as the second crossover pair, etc.
24 Why do we need two such requirements for RWS to handle objective values directly?
25 We will give examples of multiparent crossover in Chap. 3.

2.2 Simple Genetic Algorithm
21
Generally, we will assign the probability of crossover pc, called the crossover
rate, to control the possibility of performing a crossover.26
For two individuals selected to cross over, we assign a point between 1 and l ‚àí1
randomly, where l is the length of the chromosome. This means generating a random
integer in the range [1,l ‚àí1]. The genes after the point are changed between parents
and the resulting chromosomes are offspring. We call this operator a single-point
crossover. Figure 2.6 illustrates this.
)))) ) )  ) ) 
)))  )  )  
 
C
) ) )  ) 
) )  ) )
 )  ) ) 
 )  ) 
.
	
/''
%
Fig. 2.6 Single-point crossover
As can be seen from Fig. 2.6, two new individuals are generated by crossover,
which is generally seen as the major exploration mechanism of SGA.
If two parents do not perform a crossover according to probability pc, their off-
spring are themselves.
Now we discuss mutation. There are also two ways to implement mutation. One
way is to open another memory with size popsize to store the results of crossover,
and mutation is carried out in that memory. The other way is to mutate the offspring
of crossover directly. We use the latter way.
For every gene in an individual, we mutate it with probability pm, called the
mutation rate.27 Provided gene j needs to be mutated, we make a bit-Ô¨Çip change
for gene j, i.e., 1 to 0 or 0 to 1. We call this operator a bit-Ô¨Çip mutation. Figure 2.7
illustrates the bit-Ô¨Çip mutation. The individual after mutation is called the mutant.
) ))))  
 
C
 )  ) ) )  )  )  

/''
%'

		

Fig. 2.7 Bit-Ô¨Çip mutation for gene j of the offspring
26 How do we implement the statement ‚ÄúIndividual i and individual j cross over with probability
pc‚Äù?
27 How do we implement the statement ‚ÄúGene j mutates with probability pm‚Äù?

22
2 Simple Evolutionary Algorithms
As can be seen from Fig. 2.7, small changes are introduced into a chromosome by
bit-Ô¨Çip mutation,28 which could be seen as one local search method and is generally
considered a minor exploring mechanism of SGA. If every gene of an offspring does
not mutate according to probability pm, the mutant is the offspring itself.
After the variation operators, a new population with popsize individuals is gen-
erated. We need to evaluate the Ô¨Åtness for every individual and then replace the old
population with the new one.29 The process of replacing the current population with
the new population is called replacement.
Although we call the results of crossover offspring and the results of mutation
mutants, offspring is also used to represent the results after performing all variable
operators.
2.2.6 Simple Genetic Algorithm Infrastructure
The selection, crossover, mutation, and replacement discussed above constitute one
generation or iteration of an SGA. Then the evolving process continues to the next
generation. As mentioned above, selection grants Ô¨Åtter individuals greater odds op-
portunity of propagating their high-quality genes, and crossover and mutation ex-
plore the solution landscape. We could have a rational expectation that the popula-
tion will become better and better. But when do we stop? Here we just assign the
maximum generation number maxgen.30 If the generation number exceeds maxgen,
the SGA stops and the individuals in the Ô¨Ånal population are the results.
So the infrastructure, solution process, of an SGA can be illustrated as follows:
Solution Process of SGA
Phase 1: Initialization.
Step 1.1: Assign the parameters for SGA, such as pc, pm, popsize,
maxgen, etc.
Step 1.2: Generate popsize uniformly distributed individuals ran-
domly to form the initial population and evaluate their Ô¨Åtness values. gen = 0.
Phase 2: Main loop. Repeat the following steps until gen > maxgen.
Step 2.1: Select popsize individuals from current population using
RWS to generate the mating pool.
28 How large a perturbation will the bit-Ô¨Çip mutation introduce in a chromosome? Does something
not seem right? We will discuss this question in Chap. 3.
29 It seems cruel and unreasonable because perhaps we want to keep the good ones in the current
population. How do we solve this problem? We will discuss it in Chap. 3. Sect. 2.3 also gives some
hints.
30 We will discuss other more Ô¨Çexible techniques to stop EAs in Chap. 3.

2.2 Simple Genetic Algorithm
23
Step 2.2: Repeat the following operations until a new population with
popsize individuals has been generated. Select two individuals from the mat-
ing pool randomly without replacement to perform single-point crossover
with probability pc, and perform bit-Ô¨Çip mutation for every gene of the off-
spring with probability pm. Then insert the mutant into the new population.
Step 2.3: Evaluate the Ô¨Åtness value for every new individual in the
new population.
Step 2.4: Replace the current population with the new population.
gen = gen+1.
Phase 3: Submitting the Ô¨Ånal popsize individuals as the results of the SGA.
For the problem described by Eq. 2.8 and Fig. 2.2, we use an SGA with popsize =
10, maxgen = 10, pc = 0.8, and pm = 0.0131 and obtain the results illustrated in Fig.
2.8. The cross in Fig. 2.8 represents the Ô¨Ånal individuals.
















Fig. 2.8 Final population of SGA
As can be seen from comparing Figs. 2.2 and 2.8, an SGA can Ô¨Ånd a point very
close to the global optimal solution in 10√ó10 = 100 samples over a solution land-
scape without any requirement of gradient information for such a not-so-easy prob-
lem, which could illustrate that SGA, with the help of selection, crossover, and mu-
tation, is an effective search method.
To demonstrate the evolving process of SGA, we can draw a graph with the
horizontal axis representing generation, gen, and the vertical axis representing the
best Ô¨Åtness values in one generation, fbest. For our implementation, the graph is
31 Why do we assign such a small pm?

24
2 Simple Evolutionary Algorithms
Fig. 2.9. The global optimal solution is f (1.85) = 3.85 and the SGA Ô¨Ånds 3.8 at
generation 9.


























	






	




	





Fig. 2.9 The evolving process of the best Ô¨Åtness value
As can be seen from Fig. 2.9, the randomly generated initial best Ô¨Åtness value
is close to 2.4 and it evolves to about 3.8 with only nine generations, which could
demonstrate the global search and local Ô¨Åne-tuning ability of the SGA. It is neces-
sary to mention two considerations for Fig. 2.9. The Ô¨Årst thing is that connecting
the best values of different generations with direct lines is meaningless because the
points on the lines do not have any meaning. But we‚Äôd like to use this form to em-
phasize that EAs are evolutionary processes, even though they are implemented in
a discrete environment. The second thing is that after implementing and running
an SGA one time, drawing the results as in Fig. 2.8 and the evolving process as in
Fig. 2.9 is far from thoroughly evaluating the global and local search ability of an
algorithm. We will discuss this important issue, statistical performance evaluation,
in Chap. 3.
Figure 2.10 illustrates the number of papers indexed by the SCI on GAs).32
From Fig. 2.10 we can say that GA is becoming more and popular in recent
days.
32 TS = (‚Äúgenetic algorithm‚Äù OR ‚Äúgenetic algorithms‚Äù). The SCI index ‚ÄúTS‚Äù is for the search topic
in the title, the keywords, and the abstract.

2.3 Evolution Strategy and Evolutionary Programming
25

		


	







		

	






	
	
 
 
 



	







Fig. 2.10 Number of papers on GAs indexed by SCI
2.3 Evolution Strategy and Evolutionary Programming
2.3.1 Evolution Strategy
In designing an ES, Rechenberg and Schwefel use real numbers to represent alleles
of genes and make normal distribution mutation the most important exploration
technique over a solution landscape.
Let us take the simple example described by Eq. 2.8 and Fig. 2.2 again to illus-
trate the procedure of ES.
Suppose we have Œº individuals in the current population. For every individual,33
its chromosome is x, which is a real number in the range [‚àí1,2]. We Ô¨Årst randomly
select two of them, x1 and x2, to do the crossover and generate one offspring x as
x = x1 +x2
2
(2.11)
This crossover operator is called an intermediate crossover.34 There are other
options for crossover operators in ES. Because crossover is considered a minor ex-
ploring tool in ES by Rechenberg and Schwefel, we do not discuss it here. Real code
crossover operators will be discussed in Chap. 3.
We complete the crossover Œª times to generate Œª offspring (Œª is often larger
than Œº35). For every offspring, we want to give a normal distribution disturbance on
every variable (the example has only one variable).
For a normal distribution N
(
Œæ,œÉ2)
with mean Œæ and standard deviation œÉ, it can
be generated by N
(
Œæ,œÉ2)
= Œæ + œÉN (0,1), where N (0,1) is a standard normally
distributed random number with mean 0 and standard deviation 1. In MATLAB‚ìá,
33 Without any preference for Ô¨Åtter individuals.
34 Why?
35 So in the previous crossover, we need to sample parents with replacement.

26
2 Simple Evolutionary Algorithms
the randn function can generate N (0,1) directly. For other programming environ-
ments, readers should refer to textbooks on probability for generating normally dis-
tributed random numbers from uniformly distributed random numbers [1].
In ES, we often want to make changes for every variable based on its current
value. So we only use œÉ to represent the scale of mutation in ES.36 For the ith
variable xi of one offspring, execute
x
‚Ä≤
i = xi +Ni
(
0,œÉ2)
= xi +œÉNi (0,1)
(2.12)
where x
‚Ä≤
i is the mutant of xi.37 By using Eq. 2.12 for every gene of every offspring,
we can get new individuals. This mutation operator is called a normal mutation.
Then their Ô¨Åtness values can be calculated using the objective function. The standard
deviation œÉ might be the same for all variables and it also might be different for
every variable, depending on the algorithm‚Äôs designer.
Then we combine Œº current individuals and Œª new individuals and then pick the
Œº best ones according to their Ô¨Åtness values to form new population. So the solution
process of ES can be illustrated as follows:
Solution Process of (Œº +Œª)-ES
Phase 1: Initialization.
Step 1.1: Assign the parameters for ES, such as Œª, Œº, and œÉ.
Step 1.2: Generate Œº uniformly distributed individuals randomly to
form the initial population and evaluate their Ô¨Åtness values. gen = 0.
Phase 2: Main loop. Repeat the following steps until gen > maxgen.
Step 2.1: Repeat the following operations until a new population with
Œª individuals has been generated. Randomly select two individuals with re-
placement to perform crossover, such as intermediate crossover (Eq. 2.11).
Then perform a mutation (Eq. 2.12) for every gene of the offspring. Then
insert the mutant into the new population.
Step 2.2: Calculate the Ô¨Åtness value for every new individual in the
new population.
Step 2.3: Combine Œº current and Œª new individuals and pick the Œº
best ones to form a new population. gen = gen+1.
Phase 3: Submitting the Ô¨Ånal Œº individuals as the results of ES.
ES differs considerably from SGA and we will leave it to the reader to discover the
differences. Here we need to point out that difference in the replacement procedure.
A new population will certainly replace the old one in SGA, but the new population
36 What is the logical relationship between this sentence and the previous one?
37 Why do Rechenberg and Schwefel choose a normal distribution instead of a uniform distribu-
tion? What is their initial design idea behind N(0,1)?

2.3 Evolution Strategy and Evolutionary Programming
27
is selected from the union of Œª new individuals and Œº current individuals in ES.
Obviously, the replacement mechanism in ES conserves the best individual. Apart
from replacement, the coolest part of ES is perhaps its self-adaptive control of stan-
dard deviation œÉ , i.e., coding œÉ into the chromosomes. This will be discussed in
Chap. 3.
In the above solution process, Œº current individuals and Œª new individuals are
combined and the Œº best ones form a new population. Thus we call it (Œº +Œª)-ES. In
another type, (Œº, Œª)-ES, Œº current individuals are used to generate Œª new individ-
uals and the Œº best ones among the Œª new individuals form the new population.38
There are also other types like (1+1)-ES, (1+Œª)-ES, and (1, Œª)-ES.
2.3.2 Evolutionary Programming
Fogel used EP to solve the learning problem and used a Ô¨Ånite state machine to
represent the chromosome, which causes some difÔ¨Åculties for solving optimization
problems with EP. In the 1990s, many researchers developed EP into an optimization
Ô¨Åeld and formed many types of EP. The most cited EP is listed as the following
solution process, where real numbers are used to represent variables.
Solution Process of One Type of EP Implementation
Phase 1: Initialization.
Step 1.1: Assign the parameters for EP, such as Œª, Œº, and œÉ.
Step 1.2: Generate Œº uniformly distributed individuals randomly to
form the initial population and evaluate their Ô¨Åtness values. gen = 0.
Phase 2: Main loop. Repeat the following steps until gen > maxgen.
Step 2.1: Repeat the following operations until a new population with
Œº individuals has been generated. Perform a mutation (Eq. 2.12) for every
gene of the individual to generate a new one.
Step 2.2: Calculate the Ô¨Åtness value for every new individual.
Step 2.3: Combine Œº current and Œº new individuals and pick the Œº
best ones to form a new population. gen = gen+1.
Phase 3: Submitting the Ô¨Ånal Œº individuals as the results of EP.
Thus the above listed implementation of EP can be regarded as a (Œº +Œº)-ES without
crossover.
38 SGA can be regarded as a (popsize, popsize)-ES if we only consider the replacement procedure.
Here we would like to raise an interesting but important question: Is (Œº + Œª)-ES always better
than (Œº, Œª)-ES in optimization? The answer is no! In Chap. 3, we will introduce the dilemma of
exploration and exploitation and discuss the famous No Free Lunch Theorem. This question may
be revisited after reading these materials.

28
2 Simple Evolutionary Algorithms
Figure 2.11 illustrates the number of papers indexed by the SCI on ES and EP.39
































           
<HDU
1XPEHURISDSHUV
Fig. 2.11 Number of papers indexed by SCI on ES and EP
From Fig. 2.11 we can say that ES and EP attract a similar level of attention.
2.4 Direction-based Search
All the aforesaid algorithms are random-based, i.e., generating initial points, ex-
ploring new points, selecting better points, etc. In this section, we will Ô¨Årst in-
troduce a deterministic search method, simplex search, that can explore and ex-
ploit the solution space without the requirement for gradient information. Then two
stochastic direction-based search methods, scatter search and differential evolution,
are introduced. All of these algorithms use a speciÔ¨Åc direction, unlike SGA or ES
discussed above, for generating new solutions. Thus we call them direction-based
search methods.
2.4.1 Deterministic Direction-based Search
Methods that do not require gradient information to perform a search and sequen-
tially explore the solution space are called direct search methods. There are many
effective direct search methods, such as simplex search, pattern search, etc. All
39 TS = (‚Äúevolution strategies‚Äù) and TS = (‚Äúevolutionary programming‚Äù). The SCI index ‚ÄúTS‚Äù is
for the search topic in the title, the keywords, and the abstract.

2.4 Direction-based Search
29
of them are based on the following philosophy. The methods maintain a group of
points. They utilize some sort of deterministic exploration methods to search the
objective space and almost always utilize a greedy method to update the maintained
points.
Some of them use direction information, which might be the improvement di-
rections, to search the objective space. Thus it might be very useful to embed these
directions into one‚Äôs evolutionary algorithm as either a local search method or an
exploration operator.
2.4.1.1 Simplex Search
Nelder and Mead introduced the most famous deterministic direction-based search
method, the simplex search, in 1965 [3]. Thus sometimes the simplex search is re-
ferred as the Nelder‚ÄìMead method. Do not confuse it with the simplex methods
used in linear programming. But these algorithms use the same concept of simplex,
which is a polytope with n + 1 vertices in n dimensions: a line segment in one di-
mension, a triangle in 2-D, a tetrahedron in 3-D space, and so forth.40
In multidimensional spaces, the subtraction of two vectors means a new vector
starting at one vector and ending at the other, like (x2 ‚àíx1) in Fig. 2.12. Vectors in
the space could be moved with their length and direction freely. So we often refer
to the subtraction of two vectors as a direction.
The addition of two vectors can be implemented in a triangular way, moving the
start of one vector to the end of the other to form an addition vector, like x3 +(x2 ‚àí
x1) in Fig. 2.12. We often refer to the addition of two vectors as a point.
The expression x3 + (x2 ‚àíx1) can be regarded as the destination of a moving
point that starts at x3 and has a length and direction of (x2 ‚àíx1).
Fig. 2.12 Graphical meaning
of the subtraction between
two vectors













C


)

=1C3
In a direct search, generally we cannot obtain the gradient information so that
the idea of a step in the negative gradient direction for a minimum problem is im-
practical. But if the objectives of a group of solutions are available, we then know
which one is the best one (suppose it is B). For any other solution (suppose it is
40 A simplex can be considered the simplest set of points to make an effective search.

30
2 Simple Evolutionary Algorithms
C), its improvement direction is unambiguously C ‚ÜíB.41 This intuitive idea is the
foundation of many powerful direction-based search methods and EAs.
Now let us discuss the simplex search in the 2-D minimum optimization situa-
tion. There are three maintained points that are the vertices of a triangle, each with
an objective value. Let us rank and name them by Eq. 2.13, which means the Ô¨Årst
point (B) will always be the best one, the second point (G) will always be the middle
one, and the third point (W) will always be the worst one.
f (xB) < f (xG) < f (xW)
(2.13)
Point B is the best one in the simplex and point W is the worst one, which means
moving from W to B is a good search direction. Also moving from W to G is a good
search direction. So why not combine these two considerations and move from W
toward the centroid, gravity center, of B and G for a further step?
The gravity center of B and G is M. We think the direction from W to M is the
optimizing direction.
xM = xG +xB
2
(2.14)
We start from point W and go in the good search direction (W‚ÜíM) and extend
a further step to point R, which satisÔ¨Åes
xR = xM +(xM ‚àíxW)
(2.15)
The search discussed above is called a reÔ¨Çection procedure. W is reÔ¨Çected with
respect to M, which means a moving point starts at M and has a length and direction
of (xM ‚àíxW). Figure 2.13 illustrates the situation. Now B, G, and R constitute the
new simplex.
Fig. 2.13 Simplex search











:
*
D

2



If f (xR) < f (xB), then the reÔ¨Çection improves the best points thus far and proves
our guess that the direction (W‚ÜíM) is good. So why not extend it more? This is
41 Perhaps this direction is not the best or the fastest direction, but it is a workable one because B
is better than C.

2.4 Direction-based Search
31
the expansion procedure, which means that we want to expand the simplex B-G-R
to B-G-E. That is, we search point E by
xE = xM +2(xM ‚àíxW)
(2.16)
If f (xE) < f (xB), then we are fully satisÔ¨Åed and make E, B, and G the current
simplex. If not, then the expansion procedure is not good enough, but we still have
good point R. So R, B, and G constitute the current simplex.
If f (xB) < f (xR) < f (xG), which means the reÔ¨Çection is acceptable. So B, R,
G constitutes the current simplex.
If f (xG) < f (xR), then the reÔ¨Çection does not Ô¨Ånd good points. But we still
believe that the direction (W‚ÜíM) is correct. The too-large step is the cause of the
failure. So we shorten the step and make a contraction procedure, which means we
want to contract the simplex B-G-R to B-G-C. That is, we search point C by
xC = xW +0.5(xM ‚àíxW)
(2.17)
If f (xC) < f (xW), then we accept the results and make B, G, and C the current
simplex.
If f (xW) < f (xC), this weakens our idea that the direction (W‚ÜíM) is correct.
We reject it and do a shrink procedure, which means we want to shrink the simplex
based on point B. Then B, M, and S constitute the new simplex.
xS = xW +xB
2
(2.18)
For every new simplex, we need to assign B, G, W according to their objec-
tive values. Then the simplex search repeats reÔ¨Çection, expansion, contraction, and
shrink again and again in a very efÔ¨Åcient and deterministic way. Vertices of the sim-
plex will move toward the optimal point (perhaps the local optimal solution) and the
simplex will become smaller and smaller. Stop criteria could be made based on the
time of function evaluation, the length of the edge, the improving rate of B, etc.
Dennis and Torczon modiÔ¨Åed the standard simplex search by a different reÔ¨Çec-
tion procedure [4]. Based on that, MATLAB‚ìácontains a direct search toolbox [5].
The simplex search is a group-based deterministic search method capable of ex-
ploring the objective space very fast, but sometimes becoming trapped in the local
optimal point. Thus many EAs use simplex as a local search method after mutation,
which can combine the global search ability of EAs and the local search ability of
the simplex search. In Chap. 3, we will discuss it again as part of memetic algo-
rithms.

32
2 Simple Evolutionary Algorithms
2.4.2 Random Direction-based Search
2.4.2.1 Scatter Search
Although the simplex search is effective, it is more like a technique than an algo-
rithm when facing real-world complex problems. Glover, Laguna, and Marti include
the elitism mechanism in the simplex search and suggest an ES-like algorithm: the
scatter search [6, 7].
The basic idea of the scatter search is the same as that of the simplex search.
Given a group of points, the algorithm somehow Ô¨Ånds new points, accepts the better
ones, and discards the worse ones.
The scatter search has four main steps, illustrated by Fig. 2.14. The reference set
(RS) contains b ‚Äúbest‚Äù solutions, b1 of which are good with respect to their objective
value (RefSet1) and b2 of which are good with respect to diversity (far away from
Re fSet1 points) (RefSet2) (b = b1 +b2).
The initialization procedure of a scatter search randomly generates solutions in
such a way that the more individuals are generated in one area, the less opportunity
this area will have to generate new ones.42 In this way, the initial solutions of the
scatter search can maintain maximum diversity. After the initialization procedure,
the scatter search makes use of the improvement procedure, the simplex search, to
make the initial solutions better. After that, RefSet1 is selected from the improve-
ment results according to the objective quality, and RefSet2 is selected from the
improvement results according to the smallest distance to RefSet1 of the remaining
improved individuals (the larger the better). Then the main loop starts. We use RS
to generate subsets. The solutions in the subsets are combined in various ways to
get Psize new solutions. Then the solutions are improved by some local search ap-
proaches (such as simplex search) to become better solutions. Finally, the improved
solutions will replace some solutions of RS if they are good with respect to objective
quality or diversity. The main loop is illustrated in Fig. 2.14.











2'

	
 	
%
	
	
 


:		

	
	
Fig. 2.14 Scatter search
There are four types of subsets to be generated in a scatter search:
1. All two-element subsets containing all pairwise combinations of the b reference
set solutions.
42 Readers are encouraged to reread the second method for the initialization of SGA.

2.4 Direction-based Search
33
2. Three-element subsets derived from two-element subsets by adding the best so-
lution not in this subset (measured by objective value).
3. Four-element subsets derived from three-element subsets by adding the best so-
lution not in this subset (measured by objective value).
4. Subsets containing the best i elements (measured by objective value), for i = 5 to
b.
In this way, subsets regard the objective value as the most important factor but
still contain the diversity factor.
There are many types of combinations for generating new solutions from subsets.
Let us give an example for a two-element subset: x1 and x2. We can Ô¨Årst Ô¨Ånd a
vector starting at x1 and pointing to x2 as d = x2‚àíx1
2
(the length of the vector is half
the distance between x1 and x2). Glover, Laguna, and Marti suggested three types
of re-combination: (1) xnew = x1 ‚àírd, (2) xnew = x1 +rd, and (3) xnew = x2 +rd,43
where r ‚àºU (0,1). Every subset can generate several new solutions according to the
composition of the subset.44
‚àôBoth x1 and x2 belong to RefSet1, which means that they are all good solutions.
Four new solutions are generated by types 1 and 3 once and type 2 twice.
‚àôOnly one of x1 and x2 belongs to RefSet1. Three new solutions are generated by
types 1, 2, and 3 once.
‚àôNeither x1 nor x2 belongs to RefSet1, which means that they are all uncrowded
solutions. Two new solutions are generated by type 2 once and type 1 or 3 once.
As has been stated, a simplex search is used by Glover, Laguna, and Marti to
improve the new solutions.
If an improved solution‚Äôs objective value is better than that of the worst one in
Re fSet1, it will replace the worst one without replacement (delete it from the set of
improved solutions). If an improved solution‚Äôs distance to the closest RS solutions
is larger than that of most crowded solutions in RefSet2, it will replace the most
crowded one without replacement.
If RS does not change in the updating procedure and the stop criterion has not
been satisÔ¨Åed, then the initialization procedure will be started to construct a new
Re fSet2.
Glover, Laguna, and Marti suggested that Psize = max(100,5‚àób). We can con-
sider the scatter search as a special kind of (b + Psize)-ES. But in the updating
(replacement) phase, the objective value is not the only criterion.
2.4.2.2 Differential Evolution
Differential evolution (DE) is also a kind of direction-based search, suggested by
Storn and Price in 1997 [8, 9]. DE also maintains a population with Np individ-
43 Readers may Ô¨Ånd out the geometric explanation for these formulae using Fig. 2.12.
44 What is the intuitive idea of Glover, Laguna, and Marti for generating new solutions? This is the
origin of the name ‚Äúscatter.‚Äù

34
2 Simple Evolutionary Algorithms
uals and has mutation, crossover operators, and a selection process. So DE could
be regarded as a real parameter coded version of GA.45 We suppose there are n
dimensions in the decision space so that individual i of DE can be expressed as
xi = (xi,1,xi,2,‚ãÖ‚ãÖ‚ãÖ,xi,n)
(2.19)
where xi,j ‚ààR, i = 1,2,‚ãÖ‚ãÖ‚ãÖ,Np, j = 1,2,‚ãÖ‚ãÖ‚ãÖ,n.
The main difference between DE, SGA, and ES lies in the mutation operator,
where the direction is used to make the perturbation from the current individual.
Let us start with individual i. Unlike the random step size of mutation along each
dimension of ES, DE uses the directional information from the current population.
The standard mutation operator of DE needs three randomly selected different indi-
viduals from the current population (r1 ‚àï= r2 ‚àï= r3 ‚àï= xi) for each individual to form
a simplex like triangle. For xi, its mutants vi is46
vi = r1 +F (r2 ‚àír3)
(2.20)
where F is a positive real number (seldom larger than one) that controls the strength
of the direction. Differential (r2 ‚àír3) forms a direction that is the origin of DE and
the reason it is a direction-based search. Equation 2.20 means the mutant of xi starts
at r1 and has direction and length of F (r2 ‚àír3).
After mutation, DE utilizes a uniform crossover operator47 to combine the in-
formation of the parents xi and vi into the offspring ui. We need to have at least
one variable of vi, so a random integer number jrand in the range [1,n] should be
generated before crossover. The offspring ui is generated by Eq. 2.21:
ui, j =
{
vi, j
rand ‚â§Cr or j = jrand
xi, j
otherwise
(2.21)
where Cr ‚àà[0,1] is a user-deÔ¨Åned parameter controlling the effect of crossover,
rand ‚àºU(0,1). Equation 2.21 means that the offspring ui has the possibility of Cr
to select variables from mutant vi and ensures at least the jrandth variable will be
picked from vi.
Then ui, called a trial vector, competes with xi, called a target vector, for survival
in the next generation in a steady way,48 which means
xi (t +1) =
{
ui (t)
f (ui (t)) ‚â§f (xi (t))
xi (t)
otherwise
(2.22)
Mutation, crossover, and selection will be carried out for Np individuals in one
generation. The initialization and the stop criteria might be the same with SGA.
45 So we generally do not differentiate ‚Äúindividuals‚Äù between ‚Äúpoints,‚Äù ‚Äúsolutions,‚Äù and ‚Äúvectors.‚Äù
46 What is the geometric explanation of Eq. 2.20?
47 Uniform crossover will be discussed in detail in Chap. 3.
48 DE treats every individual with mutation, crossover, and a replacement procedure, which is a
steady state EA, unlike the generational approach in SGA. These two terms will be discussed in
Chap. 3.

2.5 Summary
35
Even though standard DE utilizes a random direction to mutate individuals, the
direction may point to the promising area during the evolving process. Apart from
that, more effectively assigning r1 ‚àï= r2 ‚àï= r3 ‚àï= xi may promote evolution toward the
designated area, which will be discussed in later chapters. DE is a kind of simple
but powerful EA that has been attracting increasing research interests. Figure 2.15
illustrates the number of papers indexed by the SCI on DE.49 Mathematica‚ìáhas
already added DE to its numerical optimizer package.





















 




 



<HDU
1XPEHURISDSHUV
Fig. 2.15 Number of papers indexed by SCI on DE
All the algorithms introduced in this section may be regarded as EAs because
they all maintain a group of individuals and have some kind of selection scheme
and variation operators.
2.5 Summary
After providing the necessary mathematical and programming backgrounds in Sect.
2.1, we introduced SGA in detail using a not-so-easy problem to demonstrate the
strength of EAs, and we explained the implementation key points for programming
EAs. After that, Ô¨Åve algorithms, including ES, EP, simplex search, scatter search,
and DE, were introduced. We do not want readers to remember every step of these
algorithms. But the intuitive ideas of these algorithms are of utmost importance
because they might be the effective search techniques of your algorithms.
Observing Fig. 2.9 carefully, you might notice that SGA can improve the quality
of the best Ô¨Åtness value very fast, and there is a saturation like character thereafter.
It is almost always correct with respect to every EA that about 80% of its initial
improvements from randomly generated initial individuals are done in about 20%
49 TS = (‚Äúdifferential evolution‚Äù). The SCI index ‚ÄúTS‚Äù is for the search topic in the title, the
keywords, and the abstract.

36
2 Simple Evolutionary Algorithms
of its evolving process and about 20% of its Ô¨Ånal improvements are made in about
80% of its evolving process.50 In Chap. 3, we will introduce many ways to improve
the Ô¨Ånal search efÔ¨Åciency.
Good metaheuristics, such as GA, ES, EP, scatter search, DE, etc., need at least
three mechanisms to accomplish the search requirement satisfactorily.
‚àôGlobal search mechanism. Good algorithms need a global search mechanism to
Ô¨Ånd the domain of attraction of global optimum.
‚àôConvergence mechanism. Good algorithms need a convergence mechanism to
promote the evolution of individuals toward the best ones.
‚àôUp-hill mechanism for minimum optimization problems.51 Good algorithms
need an up-hill mechanism to accept no-so-good individuals so that the popula-
tion can escape the domain of attraction of the local optimum where the current
best individual resides.
For SGA, its global search mechanism is randomly generated initial individu-
als and crossover and mutation operators; its convergence mechanism is RWS; its
up-hill mechanism is also RWS and maintaining a group of individuals in the pop-
ulation. Readers are strongly suggested to Ô¨Ånd out these mechanisms for every EA
we introduce hereafter.
Before concluding this chapter, we need to mention two viewpoints. The prereq-
uisite for designing an effective algorithm for real-world problems is to grasp the
innovative elements of standard EAs. The only way to grasp, instead of memorizing
them, is to read actively. Apart from that, if an powerful algorithm takes advantage
of various effective search techniques introduced in this chapter and the later ones,
it is sometimes hard to call it a GA, ES, EP, DE, or other speciÔ¨Åc algorithm.
You need to grasp the programming details of SGA and understand the intuitive
ideas behind other introduced EAs.
Suggestions for Further Reading
This chapter deals with the basics of EAs, so the suggested reading list only includes
chapters of the textbooks.
For SGA, ES, and EP, we encourage interested readers to read Chaps. 2‚Äì5 of
Eiben and Smith‚Äôs textbook [10] and Chaps. 8‚Äì10 of B¬®ack et al.‚Äôs textbook [11].
Haupt and Haupt also give good introductions for binary GA in Chap. 2 of their
second version of the textbook published in 2004 [12].
For DE, we recommend Chap. 2 of Price et al.‚Äôs textbook [9] and Chap. 1 of
Feoktistov‚Äôs textbook [13].
We introduced several EAs in this chapter. Readers interested in their taxonomy
are encouraged to read paper by Cal¬¥egari et al. [14].
50 This is an example of the 80/20 rule.
51 Or down-hill mechanism for maximum optimization problem.

References
37
Exercises and Potential Research Projects
2.1. Implement an SGA from scratch in any programming environment to repeat
the solution process of the problem illustrated by Eq. 2.8. We insist on encouraging
readers to implement their SGA without the help of any source codes to promote
an in-depth understanding of EAs. MATLAB‚ìáis suggested for implementing the
SGA. Comparisons between different parameter settings are encouraged.
2.2. In RWS, is the selection with replacement or without replacement?
2.3. Why do we need to deÔ¨Åne the crossover rate pc and the mutation rate pm? What
will happen if pc = pc = 1 in an SGA?
2.4. Summarize the difference between SGA, ES, and EP in a table.
2.5. What are the meanings of (1+1)-ES, (1+Œª)-ES, and (1, Œª)-ES?
2.6. Find out the global search mechanism, the convergence mechanism, and the
up-hill mechanism of ES.
2.7. Find out the global search mechanism, the convergence mechanism, and the
up-hill mechanism of scatter search.
2.8. Why do we use the centroid of B and G as the reÔ¨Çection center in simplex
search? Is there any other method? For example, if we think B should have twice as
much inÔ¨Çuence on the reÔ¨Çection center as G, then what is the expression for M?
2.9. Read [4] and summarize its simplex search on a single sheet of paper.
References
1. Papoulis A, Pillai S (2002) Probability, random variables and stochastic processes, 4th edn.
McGraw Hill Higher Education
2. Ross S (2009) A Ô¨Årst course in probability, 8th edn. Prentice Hall
3. Nelder J, Mead R (1965) A simplex method for function minimization. Comput J 7(4):308‚Äì
313
4. Dennis JE, Jr, Torczon V (1991) Direct search methods on parallel machines. SIAM J Optim
1:448‚Äì474
5. MathWorks T (2009) Genetic algorithm and direct search toolbox 2 user‚Äôs guide. Tech. rep.,
The MathWorks, Natick, MA
6. Glover F, Laguna M, Marti R (2003) Advances in evolutionary computation: theory and ap-
plications, chap. Scatter search, 519‚Äì537. Springer, Berlin Heidelberg New York
7. Laguna M, Marti R (2003) Scatter search: methodology and implementations in C. Springer,
Berlin Heidelberg New York
8. Storn R, Price K (1997) Differential evolution: A simple and efÔ¨Åcient heuristic for global
optimization over continuous spaces. J Glob Optim 11(4):341‚Äì359
9. Price KV, Storn RM, Lampinen JA (2005) Differential evolution: a practical approach to global
optimization. Springer, Berlin Heidelberg New York

38
2 Simple Evolutionary Algorithms
10. Eiben AE, Smith JE (2003) Introduction to evolutionary computing. Springer, Berlin Heidel-
berg New York
11. B¬®ack T, Fogel D, Michalewicz Z (2000) Evolutionary computation 1: basic algorithms and
operators. Taylor & Francis, London, UK
12. Haupt RL, Haupt SE (2004) Practical genetic algorithms, 2nd edn. Wiley, New York
13. Feoktistov V (2006) Differential evolution: in search of solutions. Springer, Berlin Heidelberg
New York
14. Cal¬¥egari P, Coray G, Hertz A et al (1999) A taxonomy of evolutionary algorithms in combi-
natorial optimization. J Heurist 5(2):145‚Äì158

Chapter 3
Advanced Evolutionary Algorithms
Abstract The numerical example in Chap. 2 demonstrates that EAs are powerful yet
simple search methodologies. In this chapter, several critical topics will be discussed
in a clear way through which you can really claim that you know EAs and have the
courage to design your own EA for the problem you face. We hope it‚Äôs a productive
journey reading this chapter.
3.1 Problems We Face
In Chap. 2, we showed that randomly generated initial individuals efÔ¨Åciently evolve
with the help of a selection process and variation operators toward the global optimal
solution. But we need to ponder the simple EAs before utilizing them in real-world
problems. These problems might include:
‚àôWhy do we need binary code? The numerical example discussed in Chap. 2 is
only with one variable. What would happen if we needed to optimize a problem
with 50 variables, each of them needing to be discretized into 20 binary genes?
The overall length of one chromosome would be 1000! Sure, a human being has
more than ten thousand genes. But it takes more than one hundred thousand years
for us to evolve. It could be anticipated that the time for EAs to Ô¨Ånd the global
optimal solution will be extended with longer chromosomes due to the larger
solution space where EAs conduct their search. So the Ô¨Årst problem we face is
how to represent the solution of the problem so that EAs‚Äô process and operators
can handle the representation easily without the expense of too large a search
space.
‚àôWhy RWS? As was discussed in Chap. 2, RWS can grant better individuals
more chances to breed. But an alert reader might have noticed that the numerical
example was designed elaborately so that the objective value for every individual
is larger than zero, and the larger the objective value is, the Ô¨Åtter the individual
is. So the second problem we face is how to handle the problem with a minimum
39

40
3 Advanced Evolutionary Algorithms
requirement. Another situation needs to be considered is a maximum problem
whose objective values might fall below zero.
‚àôHow does one determine the key EA elements that will greatly inÔ¨Çuence the
result? There are many decision making problems in implementing an EA. So
the third problem we face is how to determine the proper popsize, termination
criteria, crossover probability pc, mutation probability pm, etc.
‚àôWhich algorithm is better? We have introduced GA, ES, EP, DE, simplex
search, and scatter search and will introduce many other EAs and their varia-
tions. Which one is most Ô¨Åt for your problem? So the fourth problem we face is
how to evaluate the performance of an EA fairly.
These four problems constitute the main part of Chap. 3 and the main concerns of
the EA community. The basic problem behind the Ô¨Årst three problems listed above
is how to balance a global search with a local search so precisely that we can Ô¨Ånd
the global attraction basin1 of the solution landscape quickly in the early stages and
focus on Ô¨Åne tuning in this basin efÔ¨Åciently in the late stages. In the EA community,
we often call this balance the tradeoff between exploration and exploitation.2
Another way to understand the balance could be the tradeoff between population
diversity and selective pressure. In the same way, we want to distribute the popula-
tion evenly on the solution landscape, i.e., keeping the population diversity, in the
early stage to promote exploration and converge the population to the global opti-
mal solution, i.e., adding selective pressure, in the late stage to promote exploitation.
This relationship could also be called the tradeoff between diversiÔ¨Åcation and inten-
siÔ¨Åcation.
We have mentioned so many problems to make decisions. You might feel un-
sure about EAs. Cheer up! We will show you in the following sections that the EA
community has considered these problems carefully. Even though we cannot say
these problems have been solved satisfactorily, many conclusions have been drawn
to guide the design of EAs to at least alleviate these problems. Besides, these con-
siderations are quite helpful for understanding EAs in a more profound way. Thus
we urge you to actively read this chapter.
3.2 Encoding and Operators
This section answers the Ô¨Årst question mentioned in Sect. 3.1. Before the main part,
there are two points we need to discuss.
Using EAs to solve optimization problems or learning problems might need three
maps ( Fig. 3.1). If EAs cannot handle the variables of the optimization or learning
problems directly, we need to design some kind of codes that could be handled by
1 An attraction basin is the set in the deÔ¨Ånition domain. Points in this set will Ô¨Ånally converge to
one point, perhaps a global or local optimal solution, with a speciÔ¨Åc search technique.
2 These two terms are also known as diversiÔ¨Åcation and intensiÔ¨Åcation, respectively. Blum and
Roli gave intensive discussions on these terms in 2003 [1].

3.2 Encoding and Operators
41
EAs‚Äô operators to represent the real solution. So we need a decoding operator to map
the codes3 in the operation domain onto the real solutions4 in the deÔ¨Ånition domain.
Sometimes the inverse map, the encoding operator, is necessary. After getting the
solution, we could use the objective function, which is given by the problem, to map
it onto the objective value. If the problem is not maximum or the objective values
are not all greater than zero, we need the third map, Ô¨Åtness transferral, to enable
RWS for the selection process.5
Fig. 3.1 Three maps between
operation domain, deÔ¨Åni-
tion domain, objective value
domain, and Ô¨Åtness value
domain



























/
	


,'	

/ !	


5	


,%
%
5	
	
'


/ !	
'	
The map from set A onto set B means that every point a ‚ààA could be mapped
onto one image b ‚ààB. There are three types of maps.
‚àôInjective. Different points in A will be mapped onto different images in B.
‚àôSurjective. For every point in b ‚ààB, there is at least one original image a ‚ààA
that is mapped onto b.
‚àôBijective. Points in A and B have a one-to-one map. The sufÔ¨Åcient and necessary
conditions for a bijective map are it is both injective map and surjective map.
If we need the encoding/decoding map, we want this map to be bijective because
it will neither enlarge the search space nor lose the possible global optimal solution.
But for a speciÔ¨Åc problem, sometimes it is extremely hard to design a bijective map.
If the map is injective but not surjective, we need to ensure that the optimal
solution in the deÔ¨Ånition domain has an original image. If not, there is a sys-
tem error. For the numerical experiment discussed in Chap. 2, we divide the def-
inition into 212 ‚âà4000 discrete points. So the maximum system error might be
3/4000/2 ‚âà0.00375. Whether or not this system error is acceptable depends on the
requirement of the users. We can decrease the system error of binary code for a real
3 Genotypes.
4 Phenotypes.
5 The second map is given by the problem and the third map will be discussed in Sect. 3.3.

42
3 Advanced Evolutionary Algorithms
number by enlarging the length of the chromosome. But here comes the tradeoff
between the solution precision and the search ability of EAs. In numerical experi-
ments, the SGA searches the operation domain 10√ó10 = 100 times and gets rather
good results. So we can roughly say that the SGA could do an efÔ¨Åcient search on the
problem with about (100/4000)√ó100 = 2.5% samplings on the operation domain.
If we expand the operation domain too much, such as to use 100 binary genes to
represent a solution, the SGA should not achieve the same result quality with the
same parameter, i.e., popsize = 10 and maxgen = 10.
On the other hand, if the map is surjective but not injective, that means the map is
multi-to-one, i.e., multiple chromosomes might corresponding to the same solution.
Then the genetic search on these chromosomes is a waste of time. In this situation,
we need a powerful search ability to explore the enlarged operation domain.
The second point we need to mention here is that the variation operators of EAs
depend on the codes. Different representations have different types of chromosomes.
Then different techniques are required to handle the different chromosomes. So we
discuss codes together with their related operators.
3.2.1 Binary Code and Related Operators
Consider a binary chromosome with length l. It is easy to see that the operation
domain contains 2l points.
If we use a binary code to illustrate a real number, some drawbacks need to be
considered. The Ô¨Årst one is the Hamming cliff. Let us consider a simple problem.
How do we mutate the binary code from (0 1 1 1) to (1 0 0 0)? On the one hand,
(0 1 1 1) in binary means the integer 7; and (1 0 0 0) in binary means the integer 8.
They are neighbors in the discrete integer domain. On the other hand, we need to be
lucky enough to perform four bit-Ô¨Çip mutations successively to mutate (0 1 1 1) to
(1 0 0 0). Generally, pm is rather small. So the probability of 7 becoming 8 is rather
small. It seems that there is a cliff between 7 and 8 for bit-Ô¨Çip mutation to climb.
The Hamming distance can illustrate the situation. The Hamming distance is the
number of positions for which the corresponding genes are different. The Hamming
distance between (0 1 1 1) and (1 0 0 0) is 4, which is the largest Hamming distance
in the Ô¨Åeld of 4 binary code.
Gray code is a way to solve the problem of the Hamming cliff. The binary codes
of integers from 0 to 7 are 000, 001, 010, 011, 100, 101, 110, 111, but their Gray
codes are 000, 001, 011, 010, 110, 111, 101, 100.6 Thus the Hamming distance
between every two neighbors is just one. So in this way, there is no Hamming cliff!
The second drawback is that the neighborhood of binary code is not the same as
that of real value. In binary code, we can deÔ¨Åne the neighborhood as the Hamming
distance. If the neighborhood between two binary chains is no greater than a prede-
Ô¨Åned threshold, such as 1, these two chains are neighbors. The Hamming distance
6 Interested readers may Ô¨Ånd out the mapping rule between binary code and Gray code in the
computer basis textbook.

3.2 Encoding and Operators
43
between chromosome (0 0 0) and chromosome (1 0 0) is one, which means that (0
0 0) and (1 0 0) are neighbors. Only one bit-Ô¨Çip mutation can change from (0 0 0)
to (1 0 0). But the phenotypes of these two individuals are 0 and 4, respectively,
which is rather far away in the decision space.7 So it is hard to control the mutation
strength.
The third drawback for binary code is the contradiction between the system error
and the search ability of EAs, which was discussed at the beginning of this section.
Considering all these drawbacks, we may come to the conclusion that binary
code for a real value variable is not a good choice. But there exist some problems
that are suitable for binary code.8 So we‚Äôd like to introduce the variation operators
for binary codes.
Multiple-point Crossover
Single-point crossover, which was discussed in Chap. 2, is a way to exchange infor-
mation between two parents. We can just expand single-point crossover to multiple-
point crossover. Let us take a two-point crossover as an example. For two indi-
viduals selected to perform a two-point crossover, we assign two different points
between 1 and l ‚àí1 randomly, where l is the length of the chromosome. The genes
between these two points are exchanged between parents and the resulting chromo-
somes are offspring. Figure 3.2 illustrates the two-point crossover.
) ))))) ) ) 
 )))) )  
 
C
) ) )  ) 
) )  )  
 )  ) ) 
 )  ) ) 
.
	
/''
%
Fig. 3.2 Two-point crossover
When considering multi-variable problems, we use one binary chain to represent
each variable, the length of which need not be the same, and connect these chains
directly to generate a genotypic chromosome for the individual. When two individ-
uals are selected to perform crossover, we need to make decisions on whether the
operator is over the whole binary chromosome or over the separate chains and ex-
press it clearly. If we do the former single-point crossover, we should express it as
a single-point crossover over the chromosome. If we do the former multiple-point
crossover, we should express it as a multiple-point crossover over the chromosome.
If we do the single-point crossover and multiple-point crossover separately on the
7 This will also happen in Gray code.
8 We will discuss one example of this problem in Chap. 7.

44
3 Advanced Evolutionary Algorithms
binary chains of variables, we should express them as a single-point crossover over
the variables and a multiple-point crossover over the variables, respectively.
Uniform Crossover
A uniform crossover exchanges information between parents in a different way. We
suppose two individuals, i.e., A and B, are selected to perform uniform crossover.
Each gene has a probability of 0.5 of inheriting the gene from A, otherwise it inherits
the gene from B.9 If we want to generate two offspring from two parents, then every
gene of the second offspring could be selected inversely to the corresponding gene
of the Ô¨Årst offspring, i.e., if the gene of the Ô¨Årst offspring inherits the gene from A,
then the corresponding gene of the second offspring inherits the gene from B and
vice versa. Suppose we generate a random vector (0.55 0.03 0.67 0.77 0.30 0.25
0.99 0.78 0.10 0.58 0.87 0.73). Figure 3.3 illustrates the uniform crossover.
))))) )  ) ) 
)) ) ) )  
)
) 
 )  

 )
 )
)
) 
) )

) 
) ) ) 
.
	
/''
%
Fig. 3.3 Uniform crossover
Uniform crossover can use multiple parents to generate multiple offspring. Read-
ers may speculate the way to implement it. Apart from that, in two-parents-one-
offspring uniform crossover, we can deÔ¨Åne a probability p for the offspring to in-
herit genes from the Ô¨Årst parent, and 1 ‚àíp for the other offspring to inherit genes
from the other parent. This type of uniform crossover is called parameterized uni-
form crossover. If p is determined by the relative weight between two parents, i.e.,
p =
fp1
fp1+ fp2 for maximum problems, where fp1 and fp2 are the Ô¨Åtness values of two
parents, this operator is called the fusion operator. The classical method discussed
in the above paragraph means p = 0.5, i.e., 0.5 uniform crossover, which is also
called discrete crossover.
The most frequently used binary code mutation operator is a bit-Ô¨Çip mutation,
which was discussed in Chap. 2.
9 How does one implement this statement in a programming environment?

3.2 Encoding and Operators
45
3.2.2 Real Code and Related Operators
As was discussed in Sect. 3.2.1, binary code for a real-value variable is not a good
choice. For the optimization problem min f(x) = f(x1,x2,‚ãÖ‚ãÖ‚ãÖ,xn), where xi ‚àà‚Ñù, it
is natural to express the chromosome as (x1,x2,‚ãÖ‚ãÖ‚ãÖ,xn). EAs based on real code are
called real-code EAs (RCEAs).
Consider a real number chromosome with length n. It is easy to see that the
operation domain contains ‚àûpoints. For a point x in ‚Ñùn space, its neighborhood
contains all the points whose Euclidean distance to x is no more than Œµ, where Œµ is
the predeÔ¨Åned threshold.
Although the single-point crossover, discussed in Sect. 2.2.5, and multiple-point
crossover and uniform crossover, discussed in Sect. 3.2.1, could also be used di-
rectly, we do not encourage readers to do this. The reason is left as an exercise.
Many researchers have discussed the rationale for designing a good real code
crossover operator. Kita et al. suggested two useful guidelines [2] and Deb and
Beyer expressed theirs in a similar way [3].
1. Preservation of statistics. The offspring generated by good crossover opera-
tors should preserve the statistics of their parents. Those statistics might include
mean, variance, and covariance.
2. Diversity of offspring. The offspring generated by good crossover operators
should have as much diversity as possible under the constraint of guideline 1.
The crossover operators for real code could be roughly classiÔ¨Åed into two cate-
gories according to whether the operator has preference or not.
3.2.2.1 Real Code Crossover Without Preference
Let us Ô¨Årst discuss an optimization problem with two variables to illustrate the con-
cept of nonseparable variables and separable variables.
Suppose our search method is in a greedy way. From any point in 2-D space, we
search four directions in the order +x, ‚àíx, +y, and ‚àíy with the same small step.
Any improvement of the Ô¨Åtness value along a direction moves the current point
along that direction to the next point. The greedy search method continues until it
cannot improve in all directions or it Ô¨Ånds a solution whose Ô¨Åtness value is within
the predeÔ¨Åned threshold.
The Ô¨Årst problem is min f = x2 +2y2, and the deÔ¨Ånition domain for both x and y
is [‚àí3.5,+3.5].10 The function is an ellipsoid function whose principal axes are the
x-axis and the y-axis. The second problem is min f = (3x2 + 3y2 ‚àí2xy)/2, and the
deÔ¨Ånition domain for both x and y is also [‚àí3.5,+3.5].11 The function is the same
10 The objection function could be written as f = f1(x)+ f2(y). We then say that this optimization
problem is with separable variables, or it is a decomposable problem [4].
11 The objection function could not be written as f = f1(x)+ f2(y). We then say that this optimiza-
tion problem is with nonseparable variables.

46
3 Advanced Evolutionary Algorithms
ellipsoid as the Ô¨Årst one, but we rotate it 45‚àòcounterclockwise so that its principal
axes are x = ¬±y. In this way, the two variables are correlated. The trajectories of
the search points started from (3,‚àí3) in both cases are illustrated on the contour
of the function in Fig. 3.4a and b. The step size is 0.002, and the stop criterion is
f(x) < 10‚àí9. In Fig. 3.4a, the greedy method needs 6000 steps to stop, while in Fig.
3.4b it needs 7500 steps to stop.










































(a)









































(b)
Fig. 3.4 Problems with separable and nonseparable variables: (a) Searching the separable vari-
ables, and (b) Searching the nonseparable variables
For our greedy method, problems with nonseparable variables are harder and
have a longer search time with the same initial point, step size, and stop criterion.

3.2 Encoding and Operators
47
This conclusion could be extended to other algorithms, i.e., generally problems with
nonseparable variables are harder.
To solve problems with nonseparable variables effectively, we need to design
powerful variation operators so that the correlation between variables could be han-
dled. Several operators discussed below have the power to deal with such problems.
Arithmetic Crossover
For two individuals x1 = (x1
1,x1
2,‚ãÖ‚ãÖ‚ãÖ,x1
n) and x2 = (x2
1,x2
2,‚ãÖ‚ãÖ‚ãÖ,x2
n), their arithmetic
crossover result, i.e., offspring y = (y1,y2,‚ãÖ‚ãÖ‚ãÖyn), could be expressed as
y = Œ±x1 +(1‚àíŒ±)x2
(3.1)
where Œ± ‚àºU(0,1). If we want to generate two offspring from two parents, simply
assign Œ≤ = (1‚àíŒ±) and use Œ≤ as the random number in Eq. 3.1.
Equation 3.1 is also called a whole arithmetic crossover because it uses one uni-
formly distributed random number for all variables. Apart from that, you can gener-
ate n independent uniformly distributed random numbers in (0,1), ùú∂= (Œ±1,‚ãÖ‚ãÖ‚ãÖ,Œ±n)
and do the following local arithmetic crossover:
y = ùú∂‚ãÖx1 +(1‚àíùú∂)‚ãÖx2
(3.2)
where ‚ãÖis for the dot product of two vectors.
The intermediate crossover discussed in Sect. 2.3 could also be regarded as a
special deterministic arithmetic crossover.
Blend Crossover
The standard arithmetic crossover is actually a linear interpolation of two individ-
uals, so it could only generate offspring on the line connecting two parents. So the
second guideline discussed above is hard to satisfy. Eshelman and Schaffer sug-
gested blend crossover (BLX) to expand the range of arithmetic crossover [5].
BLX is performed on the gene level. For gene x1
i and x2
i , suppose x1
i < x2
i without
loss of generality; their offspring gene is
yi = rand((x1
i ‚àíŒ±(x2
i ‚àíx1
i )),(x2
i +Œ±(x2
i ‚àíx1
i )))
(3.3)
where rand(a,b) is a function to generate a uniformly distributed random number
in the range (a,b),12 Œ± is a user-deÔ¨Åned parameter that controls the extent of the
expansion. So we often use BLX-Œ± to make things clear. Eshelman and Schaffer
reported that Œ± = 0.5 is a good choice for most situations. So the most frequently
used BLX is BLX-0.5.13
12 How does one implement it in a programming environment?
13 What is the geometric interpretation of BLX-0.5?

48
3 Advanced Evolutionary Algorithms
Simplex Crossover
The simplex crossover (SPX), proposed by Tsutsui et al. in 1999 [6], uniformly
select offspring in the simplex generated by parents. For an n-dimensional space, its
simplex is n+1 points in the space.14
For a 2-D space, its simplex is a triangle, its vertices are x1, x2, and x3 (Fig.
3.5). Their centroid is k = (x1 +x2 +x3)/3. We then expand the simplex along the
directions x1 ‚àík, x2 ‚àík, and x3 ‚àík, respectively, with a step size of Œµ to simulate
the extrapolation of BLX. So the new simplex is constituted by y1 = x1 +Œµ(x1 ‚àík),
y2 = x2+Œµ(x2‚àík), and y3 = x3+Œµ(x3‚àík).15 The simplex and its expansion results
are illustrated in Fig. 3.5. Then we need to sample uniformly in the triangle formed
by the new simplex to generate offspring.16 If more offspring are required, we can
execute the sampling procedure in the simplex again and again.
Fig. 3.5 Simplex crossover

















	
	
3.2.2.2 Real Code Crossover with Preference
The preference depends on the designer‚Äôs consideration. Some examples are prefer-
ence for points near the parents or a preference for points near the centroid of the
parents.
Simulated Binary Crossover
Deb and Agrawal researched the single-point crossover for binary number code and
found that the offspring of the single-point crossover have the same centroid as that
of the parents. They then suggested simulated binary crossover (SBX) to simulate
this property in a real code crossover [3, 7].
They deÔ¨Åned a term called a spread factor Œ≤i for real code gene i as follows:
14 We introduced the concept of simplex in Sect 2.4.
15 Readers unfamiliar with this are referred to read Sect. 2.4.
16 How to implement the sampling is left as an exercise.

3.2 Encoding and Operators
49
Œ≤i =
				
c1
i ‚àíc2
i
p1
i ‚àíp2
i
				
(3.4)
where c1
i and c2
i are gene i of two offspring, and p1
i and p2
i are gene i of two par-
ents. If Œ≤i < 1, the operator is called a contracting crossover. If Œ≤i > 1, the oper-
ator is called an expanding crossover. If Œ≤i = 1, the operator is called a stationary
crossover.
To simulate the sharing centroid property of single-point crossover for binary
number code, Deb and Agrawal want the offspring to be symmetric to the centroid
of the two parents.
They also want the real code crossover to have a high probability of being a
stationary crossover and a low probability of being an contracting crossover and an
expanding crossover for every variable. To implement this, they regarded Œ≤i as a
random number and suggested a probability density function for Œ≤i as follows:
p(Œ≤i) =
{
0.5(n+1)Œ≤ n
i ,
Œ≤i ‚©Ω1
0.5(n+1)/
Œ≤ n+2
i
,
Œ≤i > 1
(3.5)
where n is a control parameter. Figure 3.6 illustrate two conditions where n = 2 and
n = 5. According to Fig. 3.6 Œ≤i has a high probability of being 1. Larger n promotes
this preference further.




















	

Fig. 3.6 Probability density function for SBX
But how do we generate a random number with the probability density function
illustrated by Eq. 3.5? Interested readers are referred to relevant textbooks such
as [8] and [9]. Here we just use the inverse transformation to generate a random

50
3 Advanced Evolutionary Algorithms
number Œ≤i with density function Eq. 3.5 from uniformly distributed random number
ui ‚àºU(0,1).
The distribution function of Œ≤i is the integration of Eq. 3.5, i.e.,
F (Œ≤i) =
{
‚à´Œ≤i
0 0.5(n+1)xndx = 0.5Œ≤ n+1
i
,
Œ≤i ‚©Ω1
0.5+
‚à´Œ≤i
1 0.5(n+1)x‚àí(n+2)dx = 1‚àí0.5Œ≤ ‚àí(n+1)
i
,
Œ≤i > 1
(3.6)
Let ui = F‚àí1(Œ≤i) and solve Œ≤i as
Œ≤i =
{
(2ui)
1
n+1 ,
ui ‚©Ω0.5
(2(1‚àíui))‚àí
1
n+1 ,
ui > 0.5
(3.7)
Then for every ui ‚àºU(0,1), Œ≤i, which satisÔ¨Åes Eq. 3.5, could be generated with
Eq. 3.7.
According to Eq. 3.7, smaller ui17 means smaller spread factor Œ≤i, which makes
the crossover a contracting crossover, i.e., the offspring are close to the centroid;
larger ui18 means a larger spread factor Œ≤i, which makes the crossover an expanding
crossover, i.e., the offspring are far from the centroid. But according to Fig. 3.6,
SBX is likely to be stationary crossover with a high probability.
Larger Œ≤i means farther distance from the centroid and vice versa. So Eq. 3.8
could be carried out on every gene of the offspring.
c1
i = 0.5
(
p1
i + p2
i
)
+0.5Œ≤i
(
p1
i ‚àíp2
i
)
c2
i = 0.5
(
p1
i + p2
i
)
+0.5Œ≤i
(
p2
i ‚àíp1
i
)
(3.8)
After substituting Eq. 3.8 into Eq. 3.4 we can see that Eq. 3.8 is just the im-
plementation method of Eq. 3.4 after we get Œ≤i from Eq. 3.7. Because Œ≤i has a high
probability of being close to 1 according to Fig 3.6, the offspring of SBX have a high
probability of being close to the parents according to Eq. 3.8, which guarantees that
the offspring have the same centroid as that of the parents.
If the parents are far from each other, the offspring of SBX are far from each
other with a high probability because of the high probability of Œ≤i = 1, which could
promote the exploration in the early stage of EAs. If the parents are close, the off-
spring of SBX are close with a high probability, which could promote exploitation
in the late stage of EAs.
SBX is based on axes. So its search ability is limited when dealing with problems
with nonseparable variables. Deb et al. suggested parent-centric crossover (PCX)
with the property of multi-parent crossover to improve SBX [10].
17 Close to zero.
18 Close to one.

3.2 Encoding and Operators
51
Unimodal Normal Distribution Crossover
Ono et al. proposed unimodal normal distribution crossover with both the nonsepa-
rable variable searching ability and the preference for centroid [11‚Äì13]. They want
the offspring to be close to the centroid of the parents, which can also satisfy the
previously discussed guidelines. Apart from that, they introduced the variance di-
rection orthogonal to the direction of the two parents. Let us discuss a 2-D example,
illustrated in Fig. 3.7.
Fig. 3.7 Unimodal normal
distribution crossover





















	
	


We need to pick up three parents, i.e., p1, p2, and p3, to generate one offspring
o. First, the centroid of p1 and p2, i.e., k, is calculated with k = (p1 +p2)/2. Then
d = p2 ‚àíp1 is the vector and starts at p1 and ends at p2. D is used to illustrate the
distance from the third parent to vector d.19 Using d and any other n‚àí1 independent
vectors in ‚Ñùn space, we can generate n unit orthogonal bases using Gram‚ÄìSchmidt
orthonormalization.20 We use e1,‚ãÖ‚ãÖ‚ãÖ,en‚àí1 to illustrate other n ‚àí1 unit orthogonal
bases besides d/‚à£d‚à£. Then we can generate the offspring in the following way.
o = k+Œæd+D
n‚àí1
‚àë
i=1
Œ∑iei
(3.9)
where Œæ ‚àºN(0,œÉ2
Œæ ) and Œ∑i ‚àºNi(0,œÉ2
Œ∑). From Eq. 3.9 we can see that the offspring
starts from the centroid of two parents p1 and p2, i.e., k, and extends in the direction
d and length Œæ, and then extends in other directions orthogonal to d. Because Œæ and
Œ∑ are all zero mean normally distributed random numbers, the offspring is close to
the centroid of p1 and p2 with high probability. Thus the guidelines are satisÔ¨Åed.
The Ô¨Årst two parts in Eq. 3.9 are the primary search component, and the third part
is the secondary search component. Generally this will generate offspring within the
ellipsoid whose principal axis is d. Thus Kita et al. called it a unimodal normal dis-
tribution crossover (UNDX). They also suggested that œÉŒæ = 0.5 and œÉŒ∑ = 0.35/‚àön.
Apart from the Gram‚ÄìSchmidt orthonormalization for orthogonal bases, Kita
also suggested that independent random vectors orthogonal to d are also acceptable.
Let us discuss a simple example in 2-D space. Suppose p1 = (4,2) and p2 = (8,4),
then d = (4,2). We just need to generate a random number u1 ‚àºU(0,1), for exam-
ple 0.35, and Ô¨Ånd another number u2 that can satisfy (u1,u2)‚ãÖd = 0, where ‚ãÖis for
the dot product. In our example, u2 = ‚àí0.7. Then we just need to normalize (u1,u2)
to be e1 = (0.447,‚àí0.894) and use Eq. 3.9 to do UNDX in 2-D space.
19 Consider how to calculate D in ‚Ñùn space.
20 Any textbook on linear algebra will have a discussion of Gram‚ÄìSchmidt orthonormalization.

52
3 Advanced Evolutionary Algorithms
UNDX could search other directions apart from d, so it can handle the problem
with nonseparable variables effectively. Kita et al. suggested a multiparental exten-
sion of UNDX, i.e., UNDX-m, to further improve the search ability [2].
3.2.2.3 Real Code Mutation Without Explicit Direction
As mentioned in Chap. 2, mutation is the secondary search operator for GAs but a
primary search operator for ES and EP. So here we‚Äôd like to introduce the real code
mutation operator in detail.21
Uniform Mutation
For individual xi = (xi
1,xi
2,‚ãÖ‚ãÖ‚ãÖxi
n), let us suppose the deÔ¨Ånition domain for variable
j is (Lj,Uj). Then for gene j, uniform mutation is carried out using Eq. 3.10 with
probability pm:
xi
j = rand(L j,Uj)
(3.10)
where rand() ‚àºU(0,1). Uniform mutation is a rather coarse operator because it
will randomly sample the deÔ¨Ånition domain at any time, which is not good for con-
vergence in the late stages of EAs. Many researchers have suggested different ways
to limit the mutation extent.
Boundary Mutation
Similar to uniform mutation, for gene j of individual xi = (xi
1,xi
2,‚ãÖ‚ãÖ‚ãÖxi
n), boundary
mutation is deÔ¨Åned by Eq. 3.11 with probability pm:
xi
j =
{
L j
rand ‚â§0.5
Uj
rand > 0.5
(3.11)
where rand() ‚àºU(0,1) and (Lj,Uj) is the deÔ¨Ånition domain for variable j. Bound-
ary mutation is useful in constrained optimization when the feasible optimal solution
is on the boundary of the feasible domain.
Nonuniform Mutation
Considering the drawbacks of uniform mutation, we might want to mutate the indi-
vidual in a nonuniform way. Then two principles are considered:
21 Most of the techniques discussed in Chap. 3 could be used in your EA, no matter what you
would like to call it.

3.2 Encoding and Operators
53
1. The mutant should be sampled randomly.
2. The extent of mutation should decrease with the evolving process of EAs.
When talking about decreasing the search scale, we need to mention the very fa-
mous simulated annealing (SA) suggested by Kirkpatrick [14]. SA is a global search
algorithm based on a local search method. The reason for its global search ability is
that it can accept the worse result of the local search with speciÔ¨Åc probability.
SA uses local search to explore the solution landscape from current solution i.
The local search procedure might be different for various problems and encoding
methods. The local search will suggest a new solution j. The acceptance proba-
bility of j taking the place of i as the current solution for a minimum problem is
determined by
pt (i ‚Üíj) =
{
1,
f (j) ‚©Ωf (i)
exp
(
f(i)‚àíf( j)
t
)
, otherwise
(3.12)
where t is a control parameter called temperature. If j is no worse than i, j takes the
place of i with probability 1. Otherwise, the probability depends on how bad j is and
what the temperature is. We suppose Œî = f(i)‚àíf(j) < 0 and draw the acceptance
probability of two temperatures (Fig. 3.8).


	




	









	








Fig. 3.8 Acceptance probability of SA
It is easy to see that the worse j is, the lower the acceptance probability it has,
and the lower the temperature is, the lower the acceptance probability with the same
extent of badness.

54
3 Advanced Evolutionary Algorithms
The temperature of SA will be relatively high at the beginning and decrease
according to some annealing rules. The simplest way to anneal is t
‚Ä≤ = Œ±t, where
0 < Œ± < 1 controls the speed of annealing.22
We can adopt the concept of annealing in many aspects of EAs. If we want to
limit the extent of mutation with the evolving process of EA, we can regard genera-
tion g as temperature t and design the following nonuniform mutation:
x
‚Ä≤
j =
{
x j +Œî (g,Uj ‚àíxj),
rand ‚â•0.5
xj ‚àíŒî (g,x j ‚àíL j),
rand < 0.5
(3.13)
where x
‚Ä≤
j is the mutant of xj, Lj and Uj are its lower and upper bounds, respectively,
and Œî(g,y) is the mutation extension function whose variables are generation g
and a possible extension length for variable j. Variable j has a probability of 0.5
of changing toward its upper limit and vice versa. The deÔ¨Ånition of Œî(g,y) is as
follows:
Œî (g,y) = y
(
1‚àírand
(
1‚àíg/maxgen
)b)
(3.14)
where rand ‚àºU(0,1), b is a parameter to control the annealing speed, g is the gener-
ation number, and maxgen is the maximum generation number. Suppose rand = 0.5
and y = 1, we can draw the curve of Œî(g) as Fig. 3.9.23
As can be seen from Fig. 3.9, a larger b means quicker annealing. Also, a larger
g, i.e., later stage of EAs, means smaller perturbation.
Normal Mutation
We discussed in Sect. 2.3 normal mutation and we would like to rewrite it here for
convenience:
x
‚Ä≤
j = x j +œÉNj (0,1)
(3.15)
where œÉ is the standard deviation of a normal distribution. Normal mutations do not
have an annealing mechanism, but they can limit the extent of mutation by normal
distribution, i.e., the possibility of the mutant being in the range xi
j ‚àí3œÉ,xi
j +3œÉ is
more than 0.99.24
22 A real implementation of SA will consider a reheating process to increase the temperature
under some conditions in order to help SA escape from the current local optimal solution. In EA,
we could simulate the reheating process by the restart part of the population randomly to add
population diversity [15].
23 Here we Ô¨Åxed the random number at 0.5 to simplify the problem and represent the overall
annealing convergence effect of nonuniform mutation. While in real implementation, Œî (g,y) might
not be a monotonically decreasing function.
24 It is called the 3œÉ rule. Any probability textbook on probability will illustrate this point.

3.2 Encoding and Operators
55
Equation 3.15 only has one standard deviation for all variables, i.e., all the vari-
ables change with the same strength. The geometrical interpretation for Eq. 3.15 in
a 2-D situation is that its equal probability density contour lines are circles and the
radius is deÔ¨Åned by œÉ. So we call this type of mutation a uncorrelated mutation with
one step size. If we want to implement the hyperellipsoid effect for the mutation, dif-
ferent standard deviations for various genes are necessary, i.e., ùùà= (œÉ1,œÉ2,‚ãÖ‚ãÖ‚ãÖ,œÉn).
Then the uncorrelated mutation with n step sizes could be deÔ¨Åned as
x‚Ä≤
j = x j +œÉjNj (0,1)
(3.16)
The geometrical interpretation of Eq. 3.16 in a 2-D situation is that equal proba-
bility density contour lines are ellipses whose principal directions are deÔ¨Åned by x
and y and the corresponding equatorial radii are deÔ¨Åned by œÉx and œÉy, respectively.







	






















Fig. 3.9 Annealing process of nonuniform crossover
In Fig. 3.4 we show the necessity for the variation operator to search in more
directions than ¬±x and ¬±y. SPX, PCX, UNDX, and UNDX-m could fulÔ¨Ål this re-
quirement. But how does one mutate the individual in different directions?
We Ô¨Årst need to review the relationship between covariance and rotation in multi-
dimensional normal distribution. The density function of a normal distribution ran-
dom vector could be illustrated as follows:
p(x) =
1
(2œÄ)n/2(detB)1/2 exp
{
‚àí1
2(x‚àía)TB‚àí1(x‚àía)
}
(3.17)
where a is the expectance vector and B is the covariance matrix.
We just take the 2-D case as an example. If we generate two independent nor-
mally distributed random numbers x1 and x2 with standard variations of œÉ1 and œÉ2,

56
3 Advanced Evolutionary Algorithms
respectively, and zero mean, then B in Eq. 3.17 is
[
œÉ2
1 0
0 œÉ2
2
]
. The covariance be-
tween two numbers is zero because they are independent. We need to introduce a
theorem before continuing the discussion.
Theorem 3.1. Suppose the linear transform of random vector Œæ is Œ∑ = CŒæ and
Œæ ‚àºN(a,B), then Œ∑ ‚àºN(Ca,CBCT).
This theorem is a very important fact of multidimensional normal distribution.
Interested readers are referred to the relevant probability textbooks.
Now we consider the rotation transform, which is a linear transform. For a vector
Œæ to rotate counterclockwise at angle œï, the rotation transform can be written as
(
Œ∑1
Œ∑2
)
=
(
cosœï ‚àísinœï
sinœï
cosœï
)(
Œæ1
Œæ2
)
(3.18)
Then the CBCT can be illustrated as
CBCT =
(
cos2 œïœÉ2
1 +sin2 œïœÉ2
2 sinœï cosœï
(
œÉ2
1 ‚àíœÉ2
2
)
sinœï cosœï
(
œÉ2
1 ‚àíœÉ2
2
)
sin2 œïœÉ2
1 +cos2 œïœÉ2
2
)
(3.19)
We take the standard deviation of the new random vector as œÉ
‚Ä≤
1 and œÉ
‚Ä≤
2, respec-
tively, and solve œÉ1 and œÉ2 from Eq. 3.19 as
œÉ2
1 = ‚àíœÉ
‚Ä≤2
1 cos2 œï +œÉ
‚Ä≤2
2 sin2 œï
sin2 œï ‚àícos2 œï
,
œÉ2
2 = œÉ
‚Ä≤2
1 sin2 œï ‚àíœÉ
‚Ä≤2
2 cos2 œï
sin2 œï ‚àícos2 œï
(3.20)
Substituting œÉ2
1 and œÉ2
2 into Eq. 3.19, we can solve the covariance between two new
random numbers as
c12 = c21 = 1
2 tan(2œï)
(
œÉ
‚Ä≤2
1 ‚àíœÉ
‚Ä≤2
2
)
(3.21)
From Eq. 3.21 we arrive at the conclusion that rotating a 2-D zero-mean un-
correlated normal distribution vector could generate a covariance between the two
numbers, and the relationship between the rotating angle and the covariance is given
by Eq. 3.21. The largest covariance happens with the rotation angle 45‚àò.
The geometrical interpretation for Eq. 3.17, whose covariance is not zero in a
2-D situation, is that its equal probability density contour lines are ellipses whose
principal directions are deÔ¨Åned by the eigenvectors of the covariance matrix B, the
rotation angle is determined by Eq. 3.21, and the corresponding equatorial radii
are deÔ¨Åned by the inverse of the square root of the eigenvalues, i.e., œÉ
‚Ä≤
1 and œÉ
‚Ä≤
2,
respectively.
Up to now, we know that covariance in normal distribution means rotation op-
eration. But how do we generate a 2-D normal distribution random vector whose
covariance is not zero?
Again, we need to use Theorem 3.1. Suppose we are given a positive deÔ¨Ånite
covariance matrix B from which to generate a normal distribution number. We can

3.2 Encoding and Operators
57
Ô¨Årst do the Cholesky decomposition25 to generate a lower triangular matrix L that
satisÔ¨Åes LLT = B. Then we can generate n independent standard normal distribu-
tion random numbers Œæ = (Œæ1
‚ãÖ‚ãÖ‚ãÖ
Œæn)T, Œæi ‚àºN(0,1). The covariance matrix of
this vector is unit matrix E. The linear transformation Œ∑ = LŒæ generates normal
distribution vector Œ∑ whose covariance matrix is LELT = B, which is exactly what
we want.
We can do the following numerical experiment to verify the above procedure. In
MATLAB‚ìá, randn(m,n) is used to generate n normal distribution random vec-
tors, each of which contains m uncorrelated standard random numbers. cov(x) is
used to calculate the covariance matrix for many random vectors. Each row in ma-
trix x is a random vector. So we can generate 10000 two-dimension standard normal
distribution random vectors and determine their covariance matrix as follows:
old_matrix=randn(2,10000);
covariance_old=cov(old_matrix‚Äô);
MATLAB‚ìágets
[
1.0223
- 0.0067
- 0.0067 0.9962
]
, which shows that these 10000 random
vectors are uncorrelated standard normal distribution vectors.
Suppose we want to generate the 2-D random vector with covariance matrix as
[
5 2
2 1
]
; the following commands could do the Cholesky decomposition:
B=[5 2; 2 1];
cho=chol(B);
MATLAB‚ìágets
[
2.2361 0.8944
0
0.4472
]
. It is an upper triangular matrix. So we just
need to do the transposition and time it with old_matrix:
new_matrix=cho‚Äô*old_matrix;
covariance_new=cov(new_matrix‚Äô);
MATLAB‚ìágets
[
5.1366 2.0823
2.0823 1.0445
]
. It is obvious that the Cholesky decomposition-
based method could generate the normal distribution with a given covariance matrix.
Sometimes, It is hard to ensure that the user-generated covariance matrix, which
is used to mutate an individual, is a positive deÔ¨Ånite matrix. So we can use the
rotation transform to generate the positive deÔ¨Ånite covariance matrix easily. After
given the standard deviation for each gene, we can generate the covariance between
every two genes using Eq. 3.21 if we know the degree of rotation between every two
axes. In this way, we can generate a normal distribution deviation from the current
point with any angle and any strength. So it is called a correlated mutation.
We can illustrate the power of a correlated mutation with Fig. 3.10.
Suppose that in this 2-D situation, we only use an uncorrelated mutation with
standard deviation œÉx = 1 and œÉy = 1/3. According to the 3œÉ rule, the main part
25 Cholesky decomposition is discussed in all linear algebra textbooks.

58
3 Advanced Evolutionary Algorithms
























&
&







Fig. 3.10 The power of correlated mutation
of the possible sampling region is in the ellipse with a solid line in Fig. 3.10. If the
global optimal solution is the star in Fig. 3.10, then the above mutation has little
change to Ô¨Ånd it because of the 3œÉ rule. To achieve the goal, we need to enlarge
the standard deviation, e.g., œÉx = 3 and œÉy = 1. Then the main part of the possible
sampling region is in the ellipse with a dashed line in Fig. 3.10. But the ellipse with
the dashed line is rather large so that the possibility of sampling the star is still rather
slight. But if we could generate the covariance matrix considering covariance by
Eq. 3.21, the normal distribution, whose main part of the possible sampling region is
in the ellipse with a dotted line in Fig. 3.10, might Ô¨Ånd the star with high probability.
It is obvious that considering covariance might improve the search signiÔ¨Åcantly.
The correlated mutation with n variables could be implemented with the follow-
ing steps.
1. Assign standard deviation, œÉj, for each variable j and rotation angles, œïi j, for
every two variables i and j.26
2. Use Eq. 3.21 to calculate the covariance between every two variables i and j.
Thus the positive covariance matrix B is generated.
3. Use Cholesky decomposition to get the lower triangular matrix L from B.
4. Generate random vector Œæ = (Œæ1
‚ãÖ‚ãÖ‚ãÖ
Œæn)T with n independent standard nor-
mal distribution random numbers.
5. Generate the multivariate normal distribution random vectors with covariance
matrix B by LŒæ.
6. Generate the mutant of individual x as follows:
x
‚Ä≤ = x+LŒæ
(3.22)
Even through correlated mutation is powerful, it requires a lot of parameters pre-
deÔ¨Åned by users27 and thus consumes much computation cost. So whether or not
one uses correlated mutation depends on the tradeoff between performance require-
ments and computation ability.
26 ES and EP use a self-adaptive control method to encode these control parameters into chromo-
somes, which will be discussed in Sect. 3.5.1. Here we just assume they are user deÔ¨Åned control
parameters.
27 This feature promotes the self-adaptive control on them, which will be discussed in Sect. 3.5.1,
and the covariance matrix adaptation, which will be introduced in Sect. 3.5.2.6.

3.2 Encoding and Operators
59
Cauchy Mutation
Apart from normal distribution, Yao et al. used a Cauchy distribution to implement
the wider mutation scale in EP [16]. The density function of a Cauchy distribution
is as follows:
p(Œæ) = 1
œÄ
t
t2 +Œæ 2
(3.23)
where t > 0 is a scale parameter. The density functions of the standard normal dis-
tribution and Cauchy distribution (t = 1) are illustrated as follows:




















	
	


 !"
#"$%&!"
Fig. 3.11 The density functions of the standard normal distribution and Cauchy distribution (t = 1)
A Cauchy mutation differs little from a normal mutation. Here we use uncorre-
lated mutation with one step size as an example; other ways are all straightforward.
x
‚Ä≤
j = x j +œÉCj
(3.24)
where Cj is a Cauchy distribution random number generated by Eq. 3.23.
As can be seen from Fig. 3.11, the Cauchy mutation is very good at searching
in a large neighborhood. So Yao et al. called it a ‚Äúfast EP.‚Äù To further promote the
global search ability of mutation in EP, Lee and Yao suggested a mutation based on
L¬¥evy distribution [17].

60
3 Advanced Evolutionary Algorithms
Polynomial Mutation
Deb and Goyal suggested the polynomial mutation based on the polynomial distri-
bution as follows [18]:
p(Œæ) = 0.5(n+1)(1‚àí‚à£Œæ‚à£)n
(3.25)
where n is a control parameter and ‚àí1 ‚â§Œæ ‚â§+1. Figure 3.12 illustrates two con-
ditions where n = 2 and n = 5. According to Fig. 3.12, Œæ has a high probability of
being 0. Larger n promotes this preference further.


	





	









	

1	
3


E
E
Fig. 3.12 Probability density function of polynomial distribution
As discussed in Sect. 3.2.2.2, we can generate the polynomial distribution num-
ber Œæ using uniform distribution u ‚àºU(0,1) as follows:
Œæ =
{
(2u)
1
n+1 ‚àí1,
u < 0.5
1‚àí[2(1‚àíu)]
1
n+1 ,
u ‚â•0.5
(3.26)
The mutant of i‚Äôs variable is as follows:
x
‚Ä≤
j = xj +Œîmax √óŒæj
(3.27)
where Œæj is a polynomial distribution random number generated by Eq. 3.26, and
Œîmax is the maximum permissible perturbation of xj.

3.2 Encoding and Operators
61
3.2.2.4 Directional Mutation
We introduced several direction-based search methods in Sect 2.4. Here we will
discuss some other directional mutation methods for DE apart from Eq. 2.18. For
the sake of completeness, we rewrite it here. The mutant vi of individual xi is as
follows:
vi = r1 +F (r2 ‚àír3)
(3.28)
where r1 ‚àï= r2 ‚àï= r3 ‚àï= xi are randomly selected individuals from the current popu-
lation and generally 0 ‚â§F ‚â§1 controls the strength of the direction. Equation 3.28
uses one random start point and one random direction to generate the mutant, so it
would belong to the rand/1 category.
Other categories include:
best/1
vi = xbest +F (r1 ‚àír2)
(3.29)
current to best/1
vi = xi +K (xbest ‚àíxi)+F (r1 ‚àír2)
(3.30)
best/2
vi = xbest +K (r1 ‚àír2)+F (r3 ‚àír4)
(3.31)
rand/2
vi = r1 +K (r2 ‚àír3)+F (r4 ‚àír5)
(3.32)
current/2
vi = xi +K (r3 ‚àíxi)+F (r1 ‚àír2)
(3.33)
where F and K are positive real numbers that control the strength of the direction,
r1, r2, r3, r4, r5 are different randomly selected individuals from the current popula-
tion, and xbest is the current best individual. Equations 3.28‚Äì3.33 are different with
respect to how they learn from other individuals, so these ideas are called learning
strategies.
DE‚Äôs authors, Storn and Price, consider Eqs. 3.28‚ÄìEq. 3.33 mutation operators,
but they actually use several parents to generate an offspring. So many GA partici-
pants also consider them crossover operators.

62
3 Advanced Evolutionary Algorithms
3.2.3 Other Topics on Code and Operators
Whenever a new code and its related operators are deÔ¨Åned, we need to know the
following things.
‚àôHow large is the operation domain? This determines the search space for EAs.
‚àôIs the encoding and decoding map injective, surjective, and bijective? This de-
termines whether there are illegal solutions and the size of the search space of
EAs.28
‚àôHow is the distance on the operation domain deÔ¨Åned? It will be used by many
EA operators.
‚àôWhat is the deÔ¨Ånition of neighborhood in the operation domain? It will be used
in the possible local search.
‚àôIs there any effective local search technique available for the operation domain?
This will accelerate the convergence of EA.
There are many other codes for different problems and different EAs. Permuta-
tion code and its related operators will be discussed in Chap. 7. Tree code and its
related operators will be discussed in Chap. 10.
Another very interesting and quite useful topic in code is variable length. Some-
times variable length code is a direct and efÔ¨Åcient way to express the solution. But
a special decoding process and variation operators need to be designed elaborately.
A messy GA might be the Ô¨Årst variable length code, which was suggested by Gold-
berg et al. [19]. Then the SAGA, suggested by Harvey et al. [20], and the SVLC,
suggested by Hutt and Warwick [21], are all famous implement of variable length
codes.
Here we would like to discuss the effects of crossover and mutation for explo-
ration and exploitation in different stages of EAs. In the early stage, we need to ex-
plore the solution landscape, so the crossover is mainly responsible for exploration
because the initial individuals are generated randomly and crossing them over may
allow for a large-scale search for new solutions. In the late stage, the EA approaches
convergence. So individuals are similar. Crossover then changes into a Ô¨Åne-tuning
technique to exploit the attraction basin of an optimal solution.
On mutation, we Ô¨Årst assume that the strength of mutation remains unchanged.
Mutation has the role of exploring all through the evolving process if its strength
remains large, or always exploiting the current point if its strength remains small,
or changing from exploitation to exploration compared to crossover if its strength
remains moderate.29 If there is some kind of annealing technique used in mutation
that shrinks the search scale with the evolving process, mutation has an effect similar
to that of crossover.
28 If some points in the operation domain do not have images in the deÔ¨Ånition domain, these points
are illegal.
29 Under this condition, we say that crossover and mutation are complement.

3.2 Encoding and Operators
63
In this section, we have introduced many crossover operators that can generate
one offspring with many parents. Then the parent selection process should be with
replacement to ensure generating popsize offspring.
Some variation operators, such as SBX, PCX, UNDX, UNDX-m, and normal
mutation, might generate illegal solutions if there is a deÔ¨Ånition region for every
variable. In this case, special techniques should be adopted to ensure the legality of
offspring and mutant. The most simple way is to just assign the value as the lower
bound if it is smaller than the lower bound and vice versa. Another way is to reÔ¨Çect
it back in the following way:
xi =
‚éß
‚é®
‚é©
2Li ‚àíxi,
xi < Li
2Ui ‚àíxi,
xi > Ui
xi,
otherwise
(3.34)
where xi is variable i of individual x in real value code and Li and Ui are lower
and upper bounds for that variable, respectively. Other advanced techniques will be
discussed in Chap. 4.
It is quite obvious that operators performing multidirection search, such as PCX,
UNDX, UNDX-m, and normal mutation, have advantages in optimizing problems
with nonseparable variables. But the time complexity of these operators is higher
than that of those basic operators and they are relatively hard to implement. So
algorithm designers need to make tradeoff between search ability and search speed.
Another topic on the selection of parents is also helpful. SGA has no preference
on this issue, i.e., any pair could cross over with probability pc, which is known as
panmictic mating or random mating. Actually, we can adjust population diversity
using a mating restriction. If two similar individuals (genotype or phenotype) have
more chances to breed, this type is called positive assortative mating. By contrast,
negative assortative mating promotes crossover between dissimilar parents. Nega-
tive assortative mating could be implemented by comparing the distance between
two individuals with a predeÔ¨Åned threshold. If the distance is larger than the thresh-
old, crossover can happen and vice versa. It is quite obvious that negative assorta-
tive mating promotes exploration. Thus population diversity could be maintained in
this way. On the other hand, positive assortative mating promotes exploitation and
pushes the population toward convergence. We will discuss an adaptive way to con-
trol the intensity of negative assortative mating in Sect. 3.5 and a positive assortative
mating to Ô¨Ånd different optimal solutions simultaneously in Chap. 5.

64
3 Advanced Evolutionary Algorithms
3.3 Selection Methods
3.3.1 Dilemmas for Selection Methods
As was discussed in Chap. 2, we want the selection process to grant the better in-
dividuals more chances to breed. So selection is the main driving force for EAs to
converge toward a global optimal solution.
But for real optimization or learning problems, designing an ideal selection pro-
cess is not a trivial matter. We need to Ô¨Årst emphasize something that is detrimental
to the ideal selection and then discuss the solution for those dilemmas.
Genetic Drift
We consider a very simple algorithm that does not have any variation operators.
The algorithm has popsize individuals in the population, each of which has only one
binary gene, i.e., the allele is 0 or 1. The only operation in one generation is selection
and we do not consider the performance of every individual, i.e., everyone has the
same chance of being selected into the next generation. This is a trivial selection
without any selective pressure. We can use a uniformly distributed random integer j
in the range [1, popsize] to select the current individual j as one individual in the next
generation. This operation is repeated until popsize individuals have been selected.
Remember, this selection is a sampling with replacement. The initialization is done
in a random way, i.e., all individuals have a probability of 0.5 of being 1 and vice
versa.
We deÔ¨Åne number of 1 as a variable to illustrate how many individuals are
1. Of course, number of 0 = popsize ‚àínumber o f 1. So number of 1 is a ran-
dom integer, which is deÔ¨Åned between 0 and popsize. In the evolving process,
number of 1 might change with generation number gen, which makes the evolv-
ing process a discrete time process, and number of 1(gen + 1) is only determined
by number of 1(gen). So it is a Markov chain. There are popsize + 1 states for
number of 1, i.e., {0,1,‚ãÖ‚ãÖ‚ãÖ, popsize}. For number of 1(gen), we can get the prob-
ability changing from the current state to any state in the next generation. In this
way, we can constitute (popsize+1)√ó(popsize+1) state transition matrix30 C as
follows:
C =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
1
0
‚ãÖ‚ãÖ‚ãÖ
0
p1,0
p1,1
‚ãÖ‚ãÖ‚ãÖ
p1,popsize
...
...
...
...
ppopsize‚àí1,0 ppopsize‚àí1,1 ‚ãÖ‚ãÖ‚ãÖppopsize‚àí1,popsize
0
‚ãÖ‚ãÖ‚ãÖ
0
1
‚é§
‚é•‚é•‚é•‚é•‚é•‚é¶
(3.35)
30 Here we do not deduce explicitly how to generate a stage transition matrix for this selection
process. Interested readers should consult the relevant textbooks, such as [8] or [22].

3.3 Selection Methods
65
where pi,j means the probability changing from state i to state j, which must satisfy
popsize
‚àë
j=0
pi,j = 1.31 It is quite obvious that p0,0 = 1 and p(popsize,popsize) = 1 because
of our selection rule, i.e., state 0 and popsize are absorbing states. So for any ini-
tial state, it always has a probability to enter state 0 or popsize during this trivial
selection. Whenever it enters 0 or popsize, it will never come out. Using the conclu-
sions from the Markov chain, we can calculate the expectant absorbing generation,
i.e., takeover time. But here we just use numerical simulation to illustrate this phe-
nomenon.
We run the above algorithm until the population converges, i.e., individuals are all
1 or 0, count the generation number for the population to converge, and record which
state it converges to. We run the algorithm with different popsize and run 1000 times
for each popsize. Then we average the convergence generation and calculate the
probability of converging to state popsize32 and draw Fig. 3.13.








0#
0)
0)
0#"
)0
"0
0"
0&
)0"&
)0
)0#
)0#
)0#
)0)
)0
)0)
)
)
)
)
#)
))
)
)
)


)
)

)
"
))


%%
	
)0
)0"
)0#
)0&
)0
)0
)0

  	('
%%			

















Fig. 3.13 Genetic drift. The left vertical axis is the average convergence generation for different
popsize, the squares and the links between them correspond to this axis. The right vertical axis
is the probability of converging to state popsize, the triangles and the links between them corre-
spond to this axis
It is quite obvious from Fig. 3.13 that even without any selective pressure, the
algorithm will converge to one absorbing state with almost the same probability
after al relatively short time. This phenomenon is called genetic drift. If the genes
have Ô¨Ånite states and the population has Ô¨Ånite popsize, genetic drift will happen
and cause the algorithm to converge toward the wrong solution. The binary number
code, the integer number code, and the permutation code all suffer from genetic
31 What is the meaning of this constraint?
32 You can get the probability of converging to state 0 in this way.

66
3 Advanced Evolutionary Algorithms
drift. So we need to design a good selection process to push the population in the
right direction.
Premature
We need to push the population to converge to a global optimal solution with the
help of a selection process. But sometimes a ‚Äúsuper individual‚Äù that is far better than
other individuals in the current population, appears in the early stages of EAs. It is
actually a local optimal solution. Suppose the Ô¨Åtness value of ‚Äúsuper individual‚Äù a
is 200, the global optimal value is 205, and the mean Ô¨Åtness value of nine other nor-
mal individuals is 30. Then RWS will generate 10√ó200/(30√ó9+200) = 4.25 ‚âà4
copies of a if there is no selective bias,33 which means a will soon dominate the
whole population with the help of RWS. This phenomenon is called premature. To
prevent premature, we want the selection process to preserve the population diver-
sity as much as possible while giving good individuals more chances to breed.
Random Walk
The counterpart of premature is random walk. In the late stage of EAs, the popu-
lation approaches the global optimal solution, if we are lucky enough. All the in-
dividuals are of high quality, i.e., their Ô¨Åtness values are high and similar. Their
probabilities of being selected are similar. So in this situation, EAs sample the pop-
ulation randomly, which is like a random walk in the optimal basin. The random
walk weakens the Ô¨Åne-tuning ability of EAs in the late stage. So we want the selec-
tion process to exaggerate the difference in the late stage of EAs.
High Expense of Calculating the Fitness Value
The Ô¨Åtness function of the numerical experiment in Chap. 2 is very simple. In a real-
world problem, sometimes it is hard to get a speciÔ¨Åc number for every individual
to evaluate its Ô¨Åtness. If we want to compose music using EAs, we need to evalu-
ate the chromosome, which is a composition of music. Generally, human beings are
used to evaluating music subjectively. It is easy for a listener to tell which one he or
she prefers when comparing two pieces of music, but very hard to assign an exact
number to evaluate the music. This EA type is often referred to as an interactive EA.
Giving a speciÔ¨Åc Ô¨Åtness value for every individual is useful for most algorithms. But
it is hard in some conditions, e.g., the above mentioned music evaluation. Another
way to evaluate individuals is to rank them, which requires less information. The
least requirement for evaluating an individual is to indicate a preference in the com-
parison process, i.e., the individual is better than, worse than, or the same as another
33 We will discuss selective bias later.

3.3 Selection Methods
67
individual, which often happens in multiobjective optimization and interactive EAs.
The selection process should be able to deal with these requirements.
Fitness Transferral
This situation was discussed at the beginning of this chapter. We need to change
the objective value into a maximum problem and require all the Ô¨Åtness values to be
positive to enable RWS. Or we need some other selection processes that can deal
with minimum and negative objective values directly.
3.3.2 Proportional Selection
If one selection process can fulÔ¨Åll the requirement that the expected number of an
individual be proportional to its relative Ô¨Åtness value, i.e., probability of being se-
lected, we call it a proportional selection. RWS is a typical proportional selection.
We rewrite the probability of being selected pi for individual i, Eq. 2.10, as fol-
lows:
pi =
fi
‚àëpopsize
i=1
fi
(3.36)
where fi is the Ô¨Åtness value of individual i. The expected number of an individual
in proportional selection is as follows:
ni = pi √ó popsize =
‚åä
popsize√ó
fi
‚àëpopsize
i=1
fi
‚åã
(3.37)
where fi is the Ô¨Åtness value of individual i, popsize is the population size, and
‚åä‚åãis the round function, which might be toward negative inÔ¨Ånity or toward the
nearest integer. We deÔ¨Åne the selective error as the difference between ni and the
real number of individual i after the selection.
In RWS, all the results are randomly determined, which means every individual
might be lost whatever its relative Ô¨Åtness value is. So RWS has selection errors.
Then we could say that RWS suffers from selective bias.
How does one decrease the selective bias or even eliminate the selective error?
Many methods have been proposed, such as remainder stochastic sampling with
(or without) replacement. But the most often used one is stochastic universal sam-
pling (SUS) suggested by Baker [23].
Let us discuss SUS in a simple four-individual population, illustrated in Fig. 3.14.
RWS has one arrow. Each time the arrow rotates rand √ó2œÄ angles, where rand ‚àº
U(0,1). If the arrow stops at area j, we say that individual j is selected. So RWS is
carried out in a serial way.

68
3 Advanced Evolutionary Algorithms
Fig. 3.14 Stochastic universal
sampling

















Unlike RWS, SUS has popsize evenly distributed arrows. The angle between
them is 2œÄ/popsize. Each time the arrows rotate rand √ó (2œÄ/popsize) degrees,
where rand ‚àºU(0,1). If any arrow i stops at area j, we say that individual j is
selected by arrow i. So SUS is carried out in a parallel way.34
It is quite obvious from Fig. 3.14 that if the expected number of an individual
is greater than one but less than two, such as individual 4 in Fig. 3.14, it will be
selected at least once and perhaps twice. Individual 2 in Fig. 3.14 will be selected
at least twice and perhaps three times. In this way, SUS could do a proportional
selection without bias. The unbiased characteristic makes SUS the most popular
proportional selection.
3.3.3 Fitness Scaling and Transferral
In this subsection, we will Ô¨Årst try to solve the contradiction between premature and
random walk and then discuss the Ô¨Åtness transferral.
Linear Scaling
Scaling is a very useful way to change the shape of a curve. Let us suppose that the
curve represents the alteration of temperature changing with time. If the scale for the
vertical axis is between ‚àí10 and 150‚àòC but the real temperature changes between
15 and 25‚àòC, the curve then looks more like a Ô¨Çat line. We can change the scale
of the vertical axis, a procedure called scaling, to between 10 and 30‚àòC to clear
the temperature change. Mathematically speaking, the above scaling procedure is
y
‚Ä≤ = 0.125√óy+11.25, where y is the original temperature graduation and y
‚Ä≤ is the
scaled graduation.35 Even though it is an afÔ¨Åne transformation, not a strict linear
transformation,36 we often call it linear scaling.
34 The implementation of SUS in a programming environment is left as an exercise.
35 You can substitute numbers into the function to verify it.
36 Translational move is not a linear transformation.

3.3 Selection Methods
69
Before discussing linear scaling used in EAs, we need to point out that scaling
could be used in both the vertical and horizontal axis and many other nonlinear
scaling methods are very useful in engineering, such as Bode plot in control theory
and signal process.
As was introduced above, linear scaling change the Ô¨Åtness value of individuals
in the following way:
f
‚Ä≤
i = afi +b
(3.38)
where fi is the Ô¨Åtness value of individual i before scaling, f
‚Ä≤
i is the Ô¨Åtness value
of i after scaling, and a and b are control parameters embodying the designer‚Äôs
consideration.
There might be many implementation methods to determine a and b in Eq. 3.38.
One of them might consider the following two things:
1. The original average Ô¨Åtness value does not change its value after scaling.
2. The original maximum Ô¨Åtness value becomes twice as large as the original aver-
age Ô¨Åtness value.
Equation 3.39 could satisfy the above consideration.37
f
‚Ä≤
i =
favg
fmax ‚àífavg
fi + favg
(
fmax ‚àí2 favg
)
fmax ‚àífavg
(3.39)
where favg is the original average Ô¨Åtness value and fmax is the original maximum
Ô¨Åtness value. Then the proportional selection discussed in Sect. 3.3.2 could be used
to do the selection.
We can say that the average Ô¨Åtness value does not change because
f
‚Ä≤
avg =
popsize
‚àë
i=1
f
‚Ä≤
i /popsize = a
(popsize
‚àë
i=1
f/popsize
)
+b = a favg +b = favg
If there is a ‚Äúsuper individual‚Äù in the early stage, linear scaling in Eq. 3.39 could
force the expected number of the individual with the maximum Ô¨Åtness value to be
two, which diminishes its inÔ¨Çuence and promotes exploration in the early stage.
If individuals hold similar Ô¨Åtness values in the late stage, linear scaling in Eq.
3.39 could also force the expected number of the one with the maximum Ô¨Åtness
value to be two, which enhances its inÔ¨Çuence and promotes exploitation in the late
stage.
But Eq. 3.39 might cause f
‚Ä≤
min < 0, which is not acceptable for proportional se-
lection. If this happens, we just need to modify our considerations as follows:
1. The original average Ô¨Åtness value does not change its value after scaling.
2. The original minimum Ô¨Åtness value becomes zero after scaling.
37 Readers are encouraged to deduce the result. At least you should verify whether Eq. 3.39 satisÔ¨Åes
the above considerations or not.

70
3 Advanced Evolutionary Algorithms
Equation 3.40 could satisfy the above considerations. We need to point out here
that the individual with the minimum Ô¨Åtness value has no chance to survive in the
following proportional selection:
f
‚Ä≤
i =
favg
favg ‚àífmin
fi ‚àí
favg fmin
favg ‚àífmin
(3.40)
Fitness Transferral
If the problem has a minimum requirement, i.e., ‚Äúmin f(x),‚Äù and we want to use pro-
portional selection, the simplest way to deal with it is to time ‚àí1 with the function
and change the optimization to be ‚Äúmax‚àíf(x).‚Äù
But this simple method cannot guarantee that all ‚àíf(x) ‚â•0, which is required
by proportional selection. We can add a constant number c to all Ô¨Åtness values to
ensure that c‚àíf(x) ‚â•0.
The problem is that it‚Äôs hard to assign a proper c if we are doing a blind search
over the solution landscape. If c is too small, it cannot ensure that c‚àíf(x) ‚â•0; if c
is too large, the proportional selection will like a random walk again.
The pragmatic way to solve this problem is to set c = fmax, where fmax is the max-
imum Ô¨Åtness value in the current population. In this way, the minimum optimization
problem ‚Äúmin f(x)‚Äù could be changed to ‚Äúmax(fmax ‚àíf(x))‚Äù so that proportional
selection is usable. This process is called Ô¨Åtness transferral.38 In the following part
of this subsection, we assume that Ô¨Åtness transferral has been done, i.e., we are
facing maximum Ô¨Åtness values that are greater than zero.
The alert reader might have already noticed that the above Ô¨Åtness transferral pro-
cess could be regarded as one type of linear scaling method.39
Sigma Truncation
Sigma truncation is a linear scaling method considering the standard deviation of
the current population. It can be illustrated as follows:
f
‚Ä≤
i = fi ‚àí
(
favg ‚àíc√óœÉ
)
(3.41)
where œÉ is the standard deviation of the current population, c is an integer control
parameter in the range [1,5],40 and favg is the average Ô¨Åtness value of the current
population.
If the result of sigma truncation is less than zero, simply set it to zero.
38 Readers are encouraged to verify the correctness of this transferral by letting f(x) = fmax and
f(x) = fmin. This point veriÔ¨Åcation method is very useful in understanding equations in papers.
39 If the objective value for a minimum optimization min f(x) satisÔ¨Åes f(x) ‚àà[0,‚àû), we can use
another nonlinear scaling to change it into a maximum problem, i.e., max
1
1+ f(x).
40 The effect of c on the selective pressure is left as an exercise.

3.3 Selection Methods
71
In the early stage, favg is relatively small and œÉ is relatively large, which makes
the favg ‚àíc√óœÉ relatively small, sometimes even smaller than zero. This is equiva-
lent to subtracting a small positive number or even adding a positive number to the
current Ô¨Åtness value, which decreases the selective pressure and promotes explo-
ration.
In the late stage, favg is relatively large and œÉ is relatively small, which makes the
favg‚àíc√óœÉ relatively large. This is equivalent to subtracting a large positive number
to the current Ô¨Åtness value, which increases the selective pressure and promotes
exploitation.
Power Law Scaling
Power law scaling uses the power law to implement scaling. It can be illustrated as
follows:
f
‚Ä≤
i = ( fi)Œ±
(3.42)
where Œ± is a control parameter. The larger the Œ± is, the larger selective pressure is.
Boltzmann Scaling
Boltzmann scaling uses exponentiation to scale the Ô¨Åtness value and considers the
annealing effect.41 It can be illustrated as follows:
f
‚Ä≤
i = exp
( fi
t
)
(3.43)
where t is the temperature. In the early stage, the temperature is relatively high.
This means the results of the Boltzmann scaling are similar, which represents a
low selective pressure and promotes exploration. In the late stage, the temperature
is relatively low. This means the results of the Boltzmann scaling are signiÔ¨Åcantly
different, which represents a high selective pressure and promotes exploitation.
Suppose we have a population. The Ô¨Åtness values varies from 1 to 10. The mean
favg and the standard deviation œÉ are 5 and 3, respectively. c in sigma truncation is
1, Œ± in power scaling is 2, and temperature t in Boltzmann scaling is 1. We use Eqs.
3.40‚Äì3.43 to scale the Ô¨Åtness value and draw the transfer function in Fig. 3.15.
Readers are encouraged to analyze Fig. 3.15 to summarize further conclusions.
41 Turn to page 53 if you are not familiar with annealing.

72
3 Advanced Evolutionary Algorithms










	














Fig. 3.15 Fitness scaling. For the reason of clearness, only Ô¨Åtness values in the range [1,3] is
presented
3.3.4 Ranking
We might need ranking in two conditions. The Ô¨Årst is that sometimes we cannot get
the exact Ô¨Åtness value but we can get the rank. So we cannot use proportional se-
lection directly. The second reason is that we can use ranking to adjust the selective
pressure effectively, which will be illustrated later. Ranking is to rank the individuals
Ô¨Årst and use these ranks to grant the probability of being selected. Then proportional
selection can be used.
Linear Ranking
There are several methods of linear ranking. We will discuss two of them.
The Ô¨Årst method is to rank the individual in the following way. The best one has
rank 0 and the worst one has rank popsize ‚àí1. Then we can assign the probability
of being selected pi for individual i as follows:
pi =
Œ± +
ranki
popsize‚àí1 (Œ≤ ‚àíŒ±)
popsize
(3.44)
where Œ± and Œ≤ are parameters to control selective pressure and ranki is the rank of
individual i.
The rank of the best individual is 0, its expected number in the following propor-
tional selection is Œ±. The rank of the worst individual is popsize ‚àí1, its expected
number in the following proportional selection is Œ≤.

3.3 Selection Methods
73
We need to ensure that all the probabilities of being selected add up to one. So
we can deduce that Œ± and Œ≤ must satisfy Œ± +Œ≤ = 2.42 The largest number for Œ± is
2, which means the worst individual will not be selected. And we can also deduce
that the expected number of an individual with average Ô¨Åtness is 1. So the expected
number of the best individual is no more than twice that of the average one.
The second method is to rank individuals in a similar way. The best one has rank
1 and the worst one has rank popsize. Then we can assign the probability of being
selected pi for individual i as follows:
pi = q‚àí(ranki ‚àí1) r
(3.45)
where q is the parameter to control selective pressure, r is the parameter to ensure the
sum of the probability of being selected is one, and ranki is the rank of individual i.
We can deduce that q and r must satisfy q = r(popsize‚àí1)
2
+
1
popsize.
If r = 0, then q = 1/popsize. According to Eq. 3.45, this means that every in-
dividual has the same probability of being selected 1/popsize, which reÔ¨Çects the
minimum selective pressure.
If r =
2
popsize(popsize‚àí1), then q = 2/popsize. This means that the worst individual
will not be selected, which reÔ¨Çects the maximum selective pressure.
It is not difÔ¨Åcult to get the result that the probability of being selected of the
individuals in the current population decreases from q to (
2
popsize ‚àíq) linearly 43.
Nonlinear Ranking
Unlike linear ranking, the probability of being selected in nonlinear ranking is the
nonlinear function of the rank. There are also many implementations for nonlinear
ranking. We just discuss one example, which uses the density function of geometric
distribution to construct the probability of being selected:
pi = Œ± (1‚àíŒ±)popsize‚àíranki
(3.46)
where 0 < Œ± < 1 is the parameter to control selective pressure. If popsize is very
large, the sum of the probability is approximately one. The best individual has rank
popsize, which makes its probability of being selected Œ±. The worst individual has
rank 1, which makes its probability of being selected Œ±(1‚àíŒ±)popsize‚àí1 ‚âà0.
Generally, nonlinear ranking has stronger selective pressure than linear ranking,
which is like the relationship between linear scaling and nonlinear scaling.
42 Readers are encouraged to do the deduction.
43 Why?

74
3 Advanced Evolutionary Algorithms
3.3.5 Tournament Selection
The above discussed selection methods all consider global information, i.e., the rel-
ative Ô¨Åtness value or the rank. Sometimes only local information, i.e., which is the
best one in a small group, is available. Then tournament selection is quite useful.
To implement tournament selection, we just need to pick up k individuals ran-
domly with replacement and compare the Ô¨Åtness values of these k individuals, which
is the tournament. The best one wins the tournament and is selected into the mating
pool. Repeat the above process until popsize individuals have been selected.
The k is called the tournament size, which controls the selective pressure. If k = 1,
then this indicates a random sample on the population without any selective pres-
sure. But if k = popsize, then the mating pool will only contain the best individual
in the current population. The most frequently used tournament selection is binary
tournament selection, in which k = 2.
The main characteristics of tournament selection could be summarized as fol-
lows. These properties make tournament selection quite useful in some situations,
such as multiobjective optimization.
‚àôTournament selection only uses local information.
‚àôTournament selection is very easy to implement and its time complexity is small.
‚àôTournament selection can be easily implemented in a parallel environment.
But tournament selection also suffers from selection bias, which means that the
best one will not be selected if it is very unlucky and vice versa. To diminish the
selective error, Sokolov and Whitley suggested unbiased tournament selection [24]
and made improvements to it subsequently [25].
The idea of unbiased tournament selection is very simple. Let us discuss it with
a population with Ô¨Åve individuals. We suppose it is a minimum problem. Their Ô¨Åt-
ness values are {4.1,3.4,0.5,9.8,7.7}, and the sequence represents the ID of each
individual. If we want to implement the tournament selection with k = 3, so we just
need to generate three random permutations in the range [1,5]. Let us suppose they
are (13452), (54123), and (32541), respectively. Then we can do the tournament
selection as illustrated in Fig. 3.16.
For tournament size k, every individual will be compared with others k times in
unbiased tournament selection, which ensures that the best individual will have k
copies in the mating pool and the worst one will not be selected.
Goldberg added probability into tournament selection and suggested Boltzmann
tournament selection [26]. Suppose the larger Ô¨Åtness value means Ô¨Åtter. Then in
binary Boltzmann tournament selection, we just pick up randomly two individuals
i and j with replacement. The probability of i winning the tournament is illustrated
by Eq. 3.47.
pi =
1
1+exp
( f j‚àífi
t
)
(3.47)

3.4 Replacement and Stop Criteria
75
Fig. 3.16 Unbiased tourna-
ment selection












0 0 )0
&0#
"0"





















D





8
	
where t is for temperature, which will be decreased by an annealing process, and fi
and f j are Ô¨Åtness values of individual i and j, respectively.
Let us Ô¨Årst Ô¨Åx the temperature. If fi = f j, then pi = 0.5, which reÔ¨Çects a random
sampling. If fi > f j, which means i is better than j, then pi > 0.5, which reÔ¨Çects the
fact that i has a greater chance of winning the tournament. If fi < f j, which means
i is worse than j, then pi < 0.5, which reÔ¨Çects the fact that i has less chance of
winning the tournament.
With the evolving process, temperature t decreases. If the temperature is very
high, the differences in Ô¨Åtness values are negligible. The selection is just like a ran-
dom walk to promote exploration in the early stage. Lower temperature will magnify
the differences between individuals so that very small t will give the Ô¨Åtter individual
probability 1 of being selected.
We have introduced many techniques in Sects. 3.3.2‚Äì3.3.5 to cope with the
dilemmas discussed in Sect. 3.3.1. These methods can be summarized in Fig 3.17.
3.4 Replacement and Stop Criteria
3.4.1 Replacement
In Sect. 3.2, we introduced many variation operators in which the number of the
offspring might be different from that of the parents. We could discipline ourselves
to ensure that the number of the total offspring, Œª, is always the same as that of the
parents, Œº. We can also suggest some rules to determine which offspring can go to
the next generation under the condition that Œª > Œº, or the rules are to determine
which parents will be replaced by the offspring under the condition that Œª < Œº. We
often call the former circumstance survivor selection 44 and the latter circumstance
replacement. In this book, we do not differentiate these two terms unless necessary.
44 Then the selection process for generating the mating pool would be called parental selection
accordingly for differentiation purposes.

76
3 Advanced Evolutionary Algorithms






















5	
	
'


%
.

	
	

2F%
8
	
	
.
 
13+< !	
13%	 !	
13%


(
13$

	
13 !	
1C3$	
F
1C3$	
$

'

13G13
13G13@1C3
13G13
Fig. 3.17 Techniques to deal with dilemmas in Sect. 3.3.1 and adjust selective pressure
Another reason to consider replacement arises by comparing (Œº + Œª)-ES with
SGA, where (Œº +Œª)-ES has the mechanism to maintain the best solutions found up
till now.45 From the discussions below you will know that many different replace-
ment rules have dramatically different impacts on selective pressure. Some of them
will maintain population diversity effectively. So in a dynamic environment, where
the objective function changes with time, these rules take precedence [27]. We will
discuss this in Sect. 3.7.
The replacement mechanism of SGA is called nonoverlap, which means that the
new population and the old one will not overlap if parents and offspring are differ-
ent.46 We can deÔ¨Åne the extent of overlap in replacement by the term generation
gap (G), which means that GŒº individuals in the old population will be replaced by
new individuals. G = 1 means nonoverlap replacement and is equivalent to SGA if
Œº = Œª [28].
Then the general solution process of EA considering replacement can be illus-
trated as follows.
45 Œº best individuals are selected deterministically from (Œº + Œª) individuals or Œª individuals in
(Œº +Œª)-ES and (Œº,Œª)-ES, respectively. So we often call it a deterministic replacement.
46 Here we assume that crossover and mutation will change the old population. There is a small
probability that pc and pm will also provide some overlap.

3.4 Replacement and Stop Criteria
77
General Solution Process of EA
Phase 1: Initialization.
Step 1.1: Assign the parameters for EA, such as Œº, Œª, pc, pm, popsize,
stop criteria (such as maxgen), etc.
Step 1.2: Generate Œº uniformly distributed individuals randomly to
form the initial population and evaluate their Ô¨Åtness values. gen = 0.
Phase 2: Main loop. Repeat the following steps until stop criteria are sat-
isÔ¨Åed (such as gen > maxgen).
Step 2.1: Generate the mating pool using some selection process in-
troduced in Sect. 3.3.
Step 2.2: Generate Œª new individuals using variation operators intro-
duced in Sect. 3.2.
Step 2.3: Evaluate the Ô¨Åtness values of new individuals.
Step 2.4: Replace the GŒº old individuals with new individuals accord-
ing to replacement rules introduced in Sect. 3.4.1. gen = gen+1.
Phase 3: Submitting the Ô¨Ånal Œº individuals as the results of the EA.
If G = 1, the algorithm is called generational EA. By contrast, if G = 1/Œº or G =
2/Œº, and Œª = 1 or Œª = 2, which means that only one or two new individuals will
be generated and one or two old individuals will be replaced by the new ones in one
generation, then the algorithm is called incremental, or steady state.47 After one or
two new individuals replace the old one(s), we say this algorithm goes to the next
generation.
In steady state EA, the replacement rules can be generally divided into three
categories as follows:
‚àôFitness-based replacement. Worse individuals have a greater chance of being
replaced.
‚àôRandom-based replacement. The individuals being replaced are selected ran-
domly.
‚àôAge-based replacement. Older individuals have a greater chance of being re-
placed.
There are many rules for Ô¨Åtness-based replacement. The most straightforward
one is to replace the worst individual. We can also use a tournament selection from
the old population48 to determine the one being replaced.
In random-based replacement, we just pick up a random individual among the
old individuals. In age-based replacement, we need to deÔ¨Åne the age of an individ-
ual as the generation of its existence. The most often used age-based replacement
47 Most of the population will remain unchanged in incremental EA. It seems that the algorithm is
in a steady state, hence the name.
48 Here the unÔ¨Åtter one wins the tournament. So it is sometimes called a kill tournament.

78
3 Advanced Evolutionary Algorithms
rule is to replace the oldest individual, also known as the Ô¨Årst-in-Ô¨Årst-out (FIFO)
rule. The initial population has the same age, so we need to perform random-based
replacement until all the individuals have different ages.
Replacement has a strong inÔ¨Çuence on the selective pressure, which will sub-
sequently inÔ¨Çuence the search result. So we need some more guidelines for the
selective pressure introduced by replacement.
We analyze the problem as in genetic drift. Suppose there is only one global opti-
mal solution in Œº individuals. The parental selection is based on binary tournament
selection and there are no variation operators. We deÔ¨Åne the generation the popu-
lation consisting of only the global optimal solution as the takeover time. This is
also a Markov chain and the state is the number of global optimal solutions in the
population. Probabilities of state transfer can be analyzed and the takeover time can
be calculated mathematically [29, 30]. Also numerical experiments are available.
We just summarize the sequence of the selective pressure as follows:
Replace worst > Kill tournament > FIFO ‚âàReplace random
The replace worst rule ensures the improvement of the population if the new
offspring is not worse than the worst one in the old population. So takeover will
absolutely happen. The kill tournament rule has the tendency to keep the better one,
the loser of the kill tournament, so that it contains some selective pressure. The
FIFO rule and the replace random rule are similar on a statistical basis. They have
the weakest selective pressure.
There are many other techniques to adjust the replacement rule. Some of them
are listed as follows:
‚àôElitism. If the best one is selected to be replaced according to the replacement
rule, the replacement will not happen.49 So elitism increases the selective pres-
sure. Elitism can be combined with the kill tournament rule, the FIFO rule, and
the random rule.
‚àôConservative way. If one individual is selected to be replaced according to the
replacement rule, it will participate in the parental tournament selection. In this
way, if the best one is selected to be replaced, it will be in the offspring. So the
conservative technique keeps the best one in another way and thus increases the
selective pressure. A conservative FIFO rule and a conservative random rule are
often used.
‚àôCompetition between parents and offspring. The competition could be be-
tween the direct parents and offspring in a steady state EA, or between Œº parents
and Œª offspring. In this way, the best individuals will never be lost and thus in-
crease the selective pressure.
49 Elitism could also be used in generational EA, where the best solution up till now, best, is main-
tained in another memory place. After the new population replaces the old one in a generational
way, check whether best is in the population. If not, replace any one in the population with best.
The memory to store the elitist is called the archive.

3.4 Replacement and Stop Criteria
79
‚àôReplacement happens with probability. We can deÔ¨Åne a probability for re-
placement so that the one being selected according to the replacement rule will
have a chance to survive. This technique decreases the selective pressure.
Smith‚Äôs research in 2007 reveals that the sequence of the selective pressure of
these four techniques is in the follow way and these four techniques are weaker than
the replace worst rule [30].
elitism > conservative > p vs. o competition >> probabilistic replacement
Since we have introduced so many types of parental selection, survivor selection,
crossover operator, and mutation operator, a very straightforward question must be
asked by alert readers: How do I determine the most suitable technique for my prob-
lem? Unfortunately, the answer is we do not know! Parental selection and survivor
selection determine the selective pressure among popsize individuals. The crossover
operator and mutation operator determine the explorative ability. The convergence
force and the exploration force need to be adjusted elaborately to enable adequate
search over the solution landscape. We can only give the suggestion that you may
select the components of the algorithms more precisely when you know more about
the problem. For example, if we know that the solution landscape of the problem is
large, isolated, and multimodal according to previous experiences or trial sampling,
then highly explorative variation operators and techniques to maintain the popula-
tion diversity for a relatively long time is necessary.
Now we introduce two EA examples with replacement process.
Boltzmann Selection
Boltzmann selection can be implemented in both generational and an incremental
way. Here we discuss steady state Boltzmann selection for a maximum problem.
After two individuals, p1 and p2, have been selected randomly with replacement
from a population, crossover and mutation are used to generate two offspring, c1
and c2. c1 will compete with p1 and c2 will compete with p2 for survival in the
population.50 Offspring win the competition with the following probability:
pci =
1
1+exp
( fpi‚àífci
t
)
(3.48)
where fpi and fci are the Ô¨Åtness values of pi and ci, respectively (the larger, the
better), and t is the temperature, which will be decreased with the evolving process.
If pi is better than ci, the denominator will be greater than 2, which makes ci
have a lower probability of winning the competition, and vice versa. Higher temper-
ature tends to diminish the difference and lower temperature tends to magnify the
difference.
50 Here we just form the comparison pair randomly. Special concerns will be discussed in Chap. 5.

80
3 Advanced Evolutionary Algorithms
Species Adaptation Genetic Algorithm
Species adaptation genetic algorithm (SAGA), proposed by Harvey in 2001, is a
very simple yet effective search algorithm for large-scale operation domain search-
ing [31].51
In a large-scale operation domain, the code might have large redundancy, and
the landscape might contain ridges between different local optimal solutions. The
Ô¨Åtness values on the ridge is the same or similar,52 but the ridges provide the path
for searching for the global optimal solution.
For every generation, two individuals are randomly selected without replace-
ment. The Ô¨Åtter one is called W, i.e., winner, and the other one is called L, i.e.,
loser. The user needs to assign two probabilities, i.e., probability of copying the
gene from W to L pr and the probability of bit-Ô¨Çip mutation pm.53 L changes every
gene with probability p = pr + pm. If a change happens, then gene L j copies the
relevant gene Wj with probability pr or Ô¨Çips it with probability pm. Then W and L
are reinserted into the population. That is one generation in SAGA.
There is no crossover operator in SAGA. In the large-scale searching domain
SAGA faces,54 for a very long time, i.e., several thousand generations, its best Ô¨Åtness
remains unchanged until a sudden improvement happens. Harvey uses the concept
of ridge and neutral network to explain this phenomenon.
3.4.2 Stop Criteria
The selection, crossover, mutation, and replacement discussed above constitute one
generation in EA. Then the evolving process continues to the next generation. As
mentioned above, selection grants Ô¨Åtter individuals a higher opportunity of propa-
gating its high-quality genes, crossover and mutation explore the solution landscape,
and replacement might increase or decrease the population diversity with different
rules. We could have the rational expectation that the population will become better
and better.
When do we stop?
Evolution in nature will never stop. But we need to deÔ¨Åne some stop criteria, or
termination criteria, for the optimization or learning problems we face. Generally
speaking, deÔ¨Åning stop criteria is a hard task, like deÔ¨Åning popsize, because we
have no idea of the true performance of the EA on a speciÔ¨Åc problem. We only have
the individuals‚Äô chromosomes and Ô¨Åtness values in all generations, and even though
they are getting better, we do not know whether they are good enough.
There are three categories of stop criteria often used by EA participants.
51 The concept of species will be discussed in Chap. 5. Not knowing what a specie is will not
inÔ¨Çuence one‚Äôs understanding of SAGA.
52 So Harvey also called it a neutral network or neutral plateaus.
53 Harvey only uses SAGA in binary code. It could be expanded to other codes.
54 Whose size is about 21900 in the example.

3.4 Replacement and Stop Criteria
81
1. Stop with Ô¨Åtness value. If we know the Ô¨Åtness value of the optimal solution,
then we could deÔ¨Åne a threshold so that EA stops when the current best, or the
recorded best, Ô¨Åtness value is within the threshold of the optimal one. Another
way to perform a stop with Ô¨Åtness value is to verify the variance of the Ô¨Åtness
values in the current population. If most individuals in the current population are
similar,55 we call this situation convergence If the population is convergent, the
variance of the Ô¨Åtness values is small. Then a threshold could be deÔ¨Åned so that
EA stops when the variance in the current population is below the threshold. The
third way to stop the algorithm according to Ô¨Åtness value is to use the ratio of
maximum Ô¨Åtness value to minimum Ô¨Åtness value or the relative error between
them. The closer the ratio is to 1 (or the smaller the relative error is), the more
convergent the current population is. A threshold could then be used to stop the
algorithm.
2. Stop with Ô¨Åtness change. We could record the changes of the best Ô¨Åtness val-
ues for every generation. If the change is below the threshold of predeÔ¨Åned val-
ues, EA stops. The changes in the average Ô¨Åtness value could also be used. The
change rate is another option.
3. Stop with time. We could record the generation number. If it reaches the pre-
deÔ¨Åned maximum generation, expressed by maxgen, EA stops. We could also
record the number of objective function evaluation. If this number reaches the
predeÔ¨Åned value, EA stops. We prefer the latter way to maxgen for the following
two reasons. The most time-consuming step in real-world problem is often ob-
jective evaluation (which might include a decoding procedure). If the calculating
ability is limited or if there are requirements for online performance, a stop with
objective evaluation could reveal more information in evaluating the speed of
searching high-quality solutions. The second reason is that the generation might
be totally different with different algorithms.56 Then it is hard to compare them
using generation.
In a real implementation, several criteria could be combined to achieve a Ô¨Çexible
yet effective rule to terminate an EA.
55 The comparison might be carried out on an operation/deÔ¨Ånition domain or an objective/Ô¨Åtness
value domain. If the problem is unimodal and the three maps illustrated in Fig. 3.1 all maintain the
neighborhood relationship, then the domain on which the comparison is carried out is irrelevant.
If not, then special considerations are necessary. Even though the variances of chromosomes are
possible in practical use, we just discuss the variances of Ô¨Åtness values here.
56 Consider the generation of steady state GA and SGA.

82
3 Advanced Evolutionary Algorithms
3.5 Parameter Control
3.5.1 Strategy Parameter Setting
Thus far we have introduced many variation operators and selection techniques.
These options are double-edged swords. They give you the possibility of acquir-
ing better solutions with the assumption that you have adequate knowledge about
the solution landscape and the ability of these techniques. It basically violates the
original intention of designing a robust, universal, and easy-to-use optimization and
learning algorithm enlightened by Darwin‚Äôs natural selection principle. But that‚Äôs
the truth of the real world: no free lunch.57
Right now, we are actually facing two problems: Ô¨Ånding the optimal solution of
the original problem, referred to as the problem parameter, and Ô¨Ånding the optimal
operators and their optimal parameters, referred to as the strategy parameter. Due to
the nonlinear intrinsic characteristics of EAs, sometimes the latter problem is even
harder than the former one! That‚Äôs why there are so many papers discussing how to
optimize the performance of optimization algorithms.
We can roughly divide the elements affecting the optimization results into two
groups: local factors and global factors. Those options and parameters that will only
have individual-level effects are local factors, e.g., crossover and mutation operators
and their parameters, and those options and parameters that will have population-
level effects are global factors, e.g., selection type and parameters, population size,
stop criteria. Global factors have a stronger inÔ¨Çuence on the selective pressure and
population diversity than local factors. So they are harder to set. In this section, we
will introduce various techniques, from local factors to global factors, to set EAs
with the intention that these techniques will release us, at least to some extent, from
the later optimization problem mentioned above.
Eiben et al. presented a wonderful survey on strategy parameter setting [32]. We
will follow the deÔ¨Ånition of the terms in that paper.
There exist some theoretical analyses on the optimal strategy parameter. But
since EAs themselves are complicated systems and given the various intrinsic char-
acteristics of the problems, these analyses all make strong assumptions that are hard
to satisfy in real-world problems.
Much effort has been devoted to Ô¨Åne-tuning the strategy parameters for most
problems in a numerical way. The most famous one is De Jong‚Äôs experiments in his
Ph.D. thesis [33]. He suggested that popsize = 50, pc = 0.6, pm = 0.001, G = 1,58
and elitism are proper strategy parameters for his test functions.
Grefenstette used a meta-level GA to handle the strategy parameter optimization
problem [34]. Every individual of the meta-level GA is a set of strategy parameters
of lower-level GA by which a full optimization process on the real problem could
be carried out. The best objective value of the lower-level GA was then regarded as
the Ô¨Åtness value of the individual in the meta-level GA. The optimal solution of the
57 We will discuss this paradox in Sect. 3.6 again.
58 Generation gap.

3.5 Parameter Control
83
meta-level GA, i.e., the best strategy parameters for online (ofÔ¨Çine) performance,59
were popsize = 30(80), pc = 0.95(0.45), pm = 0.01(0.01), G = 1(0.9), and elitism.
As can be seen from the above two efforts,60 suggestions from different re-
searchers are achieved by different test functions and the performance criteria are
different, which put GA users in an embarrassing situation, i.e., which suggestion is
best suit for my problem?
So sometimes GA users need to do the parameter tuning discussed above them-
selves. Whether they use the trial-and-error method or utilize the meta-GA on their
problems, both of them are very time consuming because the possible value num-
ber of the strategy parameters might be large and the coupling between strategy
parameters might be tight.
To make things worse, you may have already realized that different problems
might require different optimal strategy parameters. Even though you have already
found the best parameters for your problem, you may feel uncomfortable if you
want to adjust the population diversity in the evolving process of the EA, which
means that the strategy parameters need to be changed on the Ô¨Çy. Eiben et al. gave
the taxonomy of strategy parameter setting as follows and thus divided on the Ô¨Çy
parameter control into the following three groups.61
















.
	
		%
C	C'(

'
.
	
	%
.
	
	

,	
	
	

'C	
	

	
	

Fig. 3.18 Taxonomy of strategy parameter setting
1. Deterministic control. Strategy parameters are adjusted with heuristic rules gen-
erally only depend on gen.
2. Adaptive control. Strategy parameters are adjusted with heuristic rules that de-
pend on feedback from the current or previous population.
3. Self-adaptive control. Strategy parameters are encoded into chromosomes and
optimized at the same time as problem parameters using the same EA.
59 We will mention these two performance criteria in Sect. 3.6. Here you can just consider them as
two sets of optimal strategy parameters for different requirements.
60 There are many other suggestions. One thing that needs to be mentioned here is we sometimes
set pm = 1/n in real code, where n is the number of variables, with the intention of changing a
variable by mutation in a blind environment.
61 Kita and Deb considered the self-adaptive ability of GAs to be the ability to generate offspring
according to the distribution of parents, which is a different concept from Eiben‚Äôs deÔ¨Ånition [3, 12].

84
3 Advanced Evolutionary Algorithms
In the following part of this subsection, we take a mutation parameter as an ex-
ample to illustrate these three groups.
Deterministic Control over Normal Mutation
The very straightforward consideration is that we want to explore the solution land-
scape in the early stage of EAs with high standard deviation normal mutation and
exploit the optimal solution basin in the late stage using low standard deviation nor-
mal mutation, i.e., œÉ needs to be decreased with gen. One simple way to implement
this idea is
œÉ (gen) = 1‚àí0.9
gen
maxgen
where œÉ(gen) is the strength parameter, i.e., standard deviation, at generation gen in
Eq. 3.15. Equations 3.13‚Äì3.48 discussed in the above sections are other determinis-
tic controls for parameters.
If our heuristic rule is correct for the problem, deterministic control is easily
deÔ¨Åned as the change tendency of the strategy parameter, but special considerations
on the implementation details, e.g., the speed and shape of the annealing curve, are
needed. Furthermore, the robustness of deterministic control on different problems
is generally weak because it has no idea of the online performance of the heuristic
rule.
Adaptive Control over Normal Mutation
In (1+1)-ES, a mutation is considered to be successful if its mutant is better than
the current individual. Rechenberg suggested the ‚Äú1/5 success rule‚Äù to adaptively
control œÉ [35]. The heuristic rule is that the ratio of the success mutation62 to all
mutations, ps, should be 1/5.63 If we want to adjust œÉ every œÑ generations, we just
need to count the success mutation rate in the recent œÑ generations and deÔ¨Åne the
following procedure.
if (genmodœÑ = 0) then
œÉ (gen) =
‚éß
‚é®
‚é©
œÉ (gen‚àíœÑ)/c,
ps > 1/5
œÉ (gen‚àíœÑ)c,
ps < 1/5
œÉ (gen‚àíœÑ),
ps = 1/5
(3.49)
62 The mutant is Ô¨Åtter.
63 We guess that Rechenberg‚Äôs rule came from the Pareto principle, also called the 80-20 rule,
which states that, for many events, roughly 80% of the effects come from 20% of the endeavor. We
will encounter Mr. Pareto again in Chap. 6.

3.5 Parameter Control
85
else
œÉ (gen) = œÉ (gen‚àí1)
end
where 0.871 ‚â§c ‚â§1 is a control parameter. If the success mutation rate is lower
than 1/5, œÉ decreases so as to limit the search scale near the current solution, and
vice versa.
Three questions need to be answered before designing an adaptive parameter
control scheme: (1) What is the feedback information and how does one collect
them? (2) What is the controlled parameter? (3) What is the heuristic rule whose
input is the feedback information and output is the controlled parameter? In the
above 1/5 rule, the success mutation rate in the recent œÑ generations is the feedback
information, œÉ is the controlled parameter, and Eq. 3.49 is the heuristic control rule.
To implement one type of heuristic control rule, there are many speciÔ¨Åc details to
consider, e.g., c in Eq. 3.49. So a comprehensive study on the robustness of the rule
and its parameters over different problems is necessary.
Self-adaptive Control over Normal Mutation
For the self-adaptive control of œÉ in Eq. 3.15, we just need to code it into the
chromosome as (x1,x2,‚ãÖ‚ãÖ‚ãÖ,xn,œÉ). B¬®ack further suggested that the mutation for œÉ
should use lognormal distribution instead of normal distribution [36]. If we get a
Œ∑ ‚àºN(Œº,œÉ2), then Œæ = eŒ∑ is a lognormal distribution random number. The density
function of lognormal distribution for Œæ > 0 is as follows:
p(Œæ) =
1
ŒæœÉ
‚àö
2œÄ exp
(
‚àí(ln(Œæ)‚àíŒº)2
2œÉ2
)
(3.50)
where Œº and œÉ are, respectively, the mean and the standard deviation of the vari-
able‚Äôs natural logarithm lnŒæ = Œ∑. We can draw the density function of standard
normal distribution and lognormal distribution (Œº = 0) in Fig. 3.19.
Compared to a normal distribution, with the same œÉ, a lognormal distribution
has a smaller variance. Apart from that, with smaller œÉ, a lognormal distribution
will more likely be 1. B¬®ack wanted small perturbation for the standard deviation of
normal distributions and used multiplication to mutate it as follows:
œÉ
‚Ä≤ = œÉeN(0,œÑ)
(3.51)
where œÑ is a user-deÔ¨Åned parameter. B¬®ack suggested that œÑ ‚àù1/‚àön, where n is the
number of variables, i.e., dimensions, of the problem. After œÉ gets its mutant, other
genes, i.e., variables, mutate according to the new œÉ
‚Ä≤. In this way, we Ô¨Årst get the

86
3 Advanced Evolutionary Algorithms
mutant of the stepsize and then use it to mutate the chromosome. This is called a
mutative step-size control (MSC).














	






'() * !"

+,
'() * !"


'() 
!"
Fig. 3.19 Lognormal distribution and normal distribution
Besides uncorrelated mutation with one step size, uncorrelated mutation with
n step sizes and correlated mutation could also be implemented in a similar self-
adaptive way.64
A self-adaptive parameter control uses EAs to optimize problem parameters and
strategy parameters simultaneously, which is a fascinating idea, but this increases
the requirement for the exploration and exploitation ability of EAs.
Another thing that needs to be mentioned is that not all parameters can be self-
adaptively controlled. Generally speaking, only those that have no direct inÔ¨Çuence
on the Ô¨Åtness value can be self-adaptively controlled. For example, if we want to
self-adaptively control parameter a in Eq. 3.38, the only possible result is that all
individuals will have the same a gene, its upper bound.
In the following subsection, we will introduce some recently published interest-
ing parameter control examples.
3.5.2 Examples of Variation Operator Control
3.5.2.1 Conditional Variation Operators
Wang and Okazaki suggested a conditional, not probabilistic, way to control varia-
tion operators in a deterministic way [39].
64 Interested readers are referred to [37]. In addition, Schwefel discusses the initialization and the
control parameter update of ES in detail [38].

3.5 Parameter Control
87
We introduced the concept of mating restriction on page 63. Positive assortative
mating promotes similar parents to cross over and negative assortative mating has
the reverse effect. Both cases need a distance threshold to determine whether parents
are permitted to mate or not.
After generating the mating pool and Ô¨Ånishing parent pair matching, the differ-
ence degree (di) of the ith pair of parents could be calculated as follows:
di = Nd
Ng
(3.52)
where Nd is the number of different genes between the two parent chromosomes
and Ng is the length of the chromosome. Wang and Okazaki gave the example with
binary code or integer code, but this idea could be expands to other codes with
modiÔ¨Åcation.
If di is higher than a predeÔ¨Åned threshold, i.e., setting difference degree Ds, which
means that two parents are far away, they crossover. After cross over, if the number
of offspring is smaller than popsize, those parents whose di < Ds will undergo muta-
tion until the new population has popsize individuals. This is a negative assortative
mating rule introduced in Sect. 3.2
Wang and Okazaki also control the setting difference degree Ds by an annealing
like process with the following rule:
Ds (gen+1) = ŒºDs (gen)
(3.53)
where 0 < Œº < 1 is the predeÔ¨Åned cooling ratio. With the evolving process, Ds is
getting smaller, which makes crossover easier. On the other hand, if the population is
getting convergent, then individuals are becoming similar and crossover is becoming
harder. These two facets constitute the balance between exploration and exploitation
of their EAs.
3.5.2.2 Adaptive Control on the Intensity of Assortative Mating
In 2008 Fernandes and Rosa proposed an adaptive way to control the threshold of
negative assortative mating [40]. Their GA was also implemented with binary code,
so the Hamming distance is used for distance metric.
In every generation, after generating the mating pool using a selection process
and forming the parent pairs randomly, if the Hamming distance in one pair is larger
than a threshold, this pair is allowed to cross over and it is a successful mating. If
not, it does not cross over, but each of the parents mutates with probability pm, and
it is a failed mating.
At the end of every generation, new individuals and old ones are combined to-
gether, and their Ô¨Åtter popsize will form the new population.65
65 It could be regarded as a (Œº +Œª)-ES.

88
3 Advanced Evolutionary Algorithms
Also at the end of every generation, the number of successful matings and the
number of failed matings are compared. If more matings failed, then threshold =
threshold ‚àí1 in order to promote crossover and vice versa.
The initial threshold0 = L ‚àí1, where L is the length of the chromosome. It is a
rather high threshold so that there will be little crossover and the algorithm has a
strong explorative effect by mutation at that time. If the population is approaching
convergence, the Hamming distances between parents are getting smaller, and the
threshold will decrease adaptively.66 If the threshold is too low so that the number
of successful matings is very large, then it will increase. The adaptive control of the
threshold gives the algorithm good balance.
3.5.2.3 Fuzzy Logic Controller for Pc and Pm
There are many fuzzy logic controllers (FLC) for Pc and Pm. All of them use a
fuzziÔ¨Åcation interface to change real feedback information into fuzzy numbers and
a defuzziÔ¨Åcation interface to change fuzzy numbers into real strategy parameters.
Here we just give examples on the heuristic control methods, i.e., fuzzy inference,
and refer the interested reader on fuzziÔ¨Åcation and defuzziÔ¨Åcation to [41].
Yun and Gen suggested an FLC for Pc and Pm [42]. They took the improve-
ment of the average Ô¨Åtness value in the successive two generations as the feedback
information, i.e., Œî favg (gen) = favg (gen) ‚àífavg (gen‚àí1) and Œî favg (gen‚àí1) =
favg (gen‚àí1) ‚àífavg (gen‚àí2), where favg (gen) is the average Ô¨Åtness value in gen-
eration gen.67 After fuzziÔ¨Åcation interface, Œî favg (gen) and Œî favg (gen‚àí1) were
mapped into two fuzzy numbers with value NR (negative larger), NL (negative
large), NM (negative medium), NS (negative small), ZE (zero), PS (positive small),
PM (positive medium), PL (positive large), PR (positive larger). Yun and Gen con-
sidered that successive improvement of a population meant that the current popula-
tion is evolving toward a promising area so that we need to enlarge pc and Pm, and
vice versa. The control strategy is illustrated by Table 3.1.
After fuzzy number Œî pc (gen+1) has been determined by the fuzzy rules, de-
fuzziÔ¨Åcation can be carried out to transform it into a real value. Then the following
equation could be used:68
pc (gen+1) = pc (gen)+Œî pc (gen+1)
(3.54)
Lin and Gen further improved the above FLC controller for strategy parameters
[43]. They deÔ¨Åned three parameters: (1) pc is the ordinary crossover probability,
(2) pm is the probability of using heuristic local search methods to improve the
current individual,69 and (3) pi is the probability of introducing randomly generated
66 Why?
67 Here the larger Ô¨Åtness means better.
68 pm can also be controlled in a similar way.
69 Although it is written as pm, this operator is a heuristic local search method instead of a proba-
bilistic mutation.

3.5 Parameter Control
89
Table 3.1 Fuzzy decision rule for Œî pc
Œî pc (gen+1)
Œî favg (gen‚àí1)
NR NL NM NS ZE PS PM PL PR
Œî favg (gen)
NR NR NL NL NM NM NS NS ZE ZE
NL NL NL NM NM NS NS ZE ZE PS
NM NL NM NM NS NS ZE ZE PS PS
NS NM NM NS NS ZE ZE PS PS PM
ZE NM NS NS ZE ZE PS PS PM PM
PS NS NS ZE ZE PS PS PM PM PL
PM NS ZE ZE PS
PS PM PM PL PL
PL ZE ZE PS
PS PM PM PL PL PR
PR ZE PS
PS PM PM PL PL PR PR
new individuals. These three operators will be carried out on the current population
and their results form the offspring population. Then the best popsize individuals
are selected from the union of these two populations.70 These three probabilities
should satisfy the constraint pc + pm + pi = 1 so that there is a tradeoff between
exploration, caused by crossover and random initialization, and exploitation, caused
by local search-based mutation. Then the FLC-based adaptive control rules for pc
and pm are as follows.
First the weighted sum of the improvement in the recent œÑ generations is calcu-
lated:
Œî f =
œÑ
‚àë
i=1
2œÑ‚àíiŒª (Œî favg (gen‚àíi)),
gen ‚â•œÑ
(3.55)
where œÑ is a user-predeÔ¨Åned parameter, Œª (x) = 1 if x ‚â•0 or Œª (x) = 0 if x < 0, and
Œî favg(gen‚àíi) = favg(gen‚àíi)‚àífavg(gen‚àíi‚àí1). According to Eq. 3.55, improve-
ments in the recent generations are paid more attention. Lin and Gen think that a
large weighted improvement means that the current population is in the global opti-
mal attraction basin so that we need to employ more heuristic local search methods
to improve the quality of the current population and limit crossover in order to avoid
wasting computation resources. Thus, an FLC is used to increase Œî pm and decrease
Œî pc with linear proportion to Œî f.
The above two methods have different considerations on control rules, which
might be utilized at different stages of EAs. In this direction, Zhang et al. suggested
a clustering-based adaptive control method for pc and pm [44].71
They Ô¨Årst cluster individuals into several groups using their chromosomes. After
clustering, every individual has its own cluster and we know the size of each cluster,
70 It can be regarded as a kind of (Œº +Œª)-EA.
71 Clustering is a method to classify data into groups. It is often used in optimization, pattern
recognition, and machine learning. The simplest clustering method might be K-means clustering,
which is utilized by Zhang et al. Interested readers are referred to [45‚Äì49].

90
3 Advanced Evolutionary Algorithms
i.e., the individual number belonging to that cluster, and denote GB as the size of the
cluster containing the best individual in the current population and GW as the size
of the cluster containing the worst individual in the current population. Then they
are normalized in the scale [0,1] as follows:
ÀÜGB = GB ‚àíGmin
Gmax ‚àíGmin
ÀÜGW = GW ‚àíGmin
Gmax ‚àíGmin
where Gmin and Gmax are the size of the smallest and the largest cluster, respectively.
Then the fuzziÔ¨Åcation procedure is carried out to map ÀÜGB and ÀÜGW into fuzzy
numbers PB (positive big) or PS (positive small) depending on how close they are
to 1 and 0, respectively.
Zhang et al. considered that there are four different stages in EAs‚Äô evolving pro-
cedure that need to be considered separately.
‚àôIf ÀÜGB is PB and ÀÜGW is PS, the population is in the mature stage, so we need to
decrease both pc and pm.
‚àôIf ÀÜGB = ÀÜGW and are all PB, the population is in the maturing stage. The global
optimal solution has not been found and we want the EA to explore the solution
space using large pc and maintain the good solution with small pm. So we need
to increase pc and decrease pm.
‚àôIf ÀÜGB = ÀÜGW and are all PS, the population is in the submaturing stage. The best
solution has not been surrounded by many individuals, so we need to increase
both pc and pm to accelerate the search.
‚àôIf ÀÜGB is PS and ÀÜGW is PB, the population is in a bad situation. The population
might be in the initial stage. There are many individuals surrounding the worst
points, so we need to decrease pc in order to decrease the chance of generating
similar worse individuals. At the same time, pm needs to be increased to do the
global exploration.
The above heuristic rules can be implemented with fuzzy numbers and then the
defuzziÔ¨Åcation procedure is used to get real Œî pc and Œî pm.
3.5.2.4 Adaptive Control on Mutation Types
We introduced on page 59 the idea that mutation with Cauchy distribution occurs
with a relatively larger variance to promote global search and normal distribution
occurs with a relatively smaller variance to promote local search. So Yao et al. sug-
gested a simple way to adaptively control the mutation type [16]. Every individual
undergoes two mutations, normal mutation and Cauchy mutation, and the better
mutant is the offspring.
Qin and Suganthan solved the problem of adaptive mutation type selection in DE
with a probability [50, 51]. When an individual xi requires mutation, it might be

3.5 Parameter Control
91
mutated using ‚Äúrand/1,‚Äù Eq. 3.28, with probability p1 and ‚Äúcurrent to best/1,‚Äù Eq.
3.30, with probability p2 = 1‚àíp1.
The initial value p1 for every individual is 0.5, i.e., both strategies have an equal
chance to produce mutants.
During the selection, Eq. 2.20, we count the successes of the trial vector and
denote the numbers as ns1 and ns2 according to its mutation type, i.e., ns1 is the
number of trial vectors that are mutated by ‚Äúrand/1,‚Äù successfully entering the next
generation, and vice versa. Also the number of failed trial vectors is counted and de-
noted as n f1 and nf2. ns1 +nf1 is the number of trial vectors generated by ‚Äúrand/1‚Äù
and ns2 +n f2 is the number of trial vectors generated by ‚Äúcurrent to best/1.‚Äù
These numbers are accumulated in several generations (Qin and Suganthan used
50), and p1 is adjusted according to
p1 =
ns1
(ns1+nf1)
ns1
(ns1+n f1) +
ns2
(ns2+nf2)
=
ns1 (ns2 +n f2)
ns1 (ns2 +nf2)+ns2 (ns1 +n f1)
(3.56)
where
ns1
(ns1+nf1) and
ns2
(ns2+nf2) are the success rates of ‚Äúrand/1‚Äù and ‚Äúcurrent to
best/1,‚Äù respectively. It is quite certain that higher success rates cause a larger pos-
sibility of being used later.
Then p2 = 1‚àíp1, rest ns1, ns2, nf1, nf2 to 0, and continue the iteration.
In this way, competition is carried out between two mutation types statistically
so that the proper type has more chances of producing mutants.
3.5.2.5 Evolutionary Gradient Search
Classical gradient methods provide fast and reliable search on a differentiable solu-
tion landscape. But sometimes the differentiation assumption on the solution land-
scape cannot be satisÔ¨Åed or the gradient-based search will be trapped in a local
optimal solution if we cannot provide a proper initial solution. Salomon suggested a
method, evolutionary gradient search (EGS), using EAs to construct gradient infor-
mation on a nondifferential landscape and later developed it for noisy environment
optimization [52, 53].
EGS uses self-adaptive control for mutation strength, i.e., the chromosome is
coded as x = (x1,x2,‚ãÖ‚ãÖ‚ãÖ,xn,œÉ).
EGS has the sense of (1,Œª)-ES. So it only works on one individual. From current
point x, EGS generates Œª new individuals t1,‚ãÖ‚ãÖ‚ãÖ,tŒª using normal mutation, Eq.
3.15, and calculates their Ô¨Åtness values as f(t1),‚ãÖ‚ãÖ‚ãÖ, f(tŒª). The estimated gradient
is as follows:
g =
Œª
‚àë
i=1
( f (ti)‚àíf (x))(ti ‚àíx)
(3.57)

92
3 Advanced Evolutionary Algorithms
(ti ‚àíx) is the direction from x toward ti. For maximum optimization problems, if
this is an improvement direction, f (ti)‚àíf (x) > 0, it contributes to the estimation
of the gradient. Larger improvements account for larger weights. By contrast, if
this is a worsen direction, f (ti)‚àíf (x) < 0, it also contributes to the estimation
of the gradient because f (x) ‚àíf (ti) is the improvement direction. For minimum
optimization, Eq. 3.57 is also the estimation of the gradient.72
Then the estimated gradient is normalized as follows:
e =
g
‚à•g‚à•
(3.58)
After acquiring the estimation for the gradient, EGS utilizes an adaptive tech-
nique to change the mutation strength as on page 90. For maximum optimization
problems,73 two trial points could be generated according to œÉ and estimated gra-
dient e, i.e., x+(œÉŒ∂)e and x+(œÉ/Œ∂)e, 74 and the one with the larger Ô¨Åtness value
determines the mutant of œÉ, i.e.,
œÉ
‚Ä≤ =
{
œÉŒ∂,
f (x+(œÉŒ∂)e) > f (x+(œÉ/Œ∂)e)
œÉ/Œ∂,
f (x+(œÉŒ∂)e) ‚â§f (x+(œÉ/Œ∂)e)
(3.59)
Then we get the new individual
x
‚Ä≤ = x+œÉ
‚Ä≤e
(3.60)
With the random properties of normal mutation, EGS could in the meantime
estimate the gradient with the ability to escape from the local optimal attraction
basin.
3.5.2.6 Covariance Matrix Adaptation
Salomon‚Äôs EGS uses EAs to construct gradient information that is used to direct the
search efÔ¨Åciently. Hansen and Ostermeier suggested covariance matrix adaptation
(CMA) ES in 2001 [54] to further accelerate the search efÔ¨Åciency. CMA-ES sup-
poses that the local solution space of the current point has a quadratic shape, i.e.,
the Taylor series of f(x) around an xk is as follows:
f (xk +Œîx) ‚âàf (xk)+‚àáf (xk)T Œîx+ 1
2ŒîxTH(xk)Œîx
(3.61)
where ‚àáf (xk) and H(xk) are the gradient and the Hessian matrix at xk, respec-
tively. There are many techniques for convex quadratic numerical optimization,
such as the Newton‚Äôs method, the conjugate gradient method, and the quasi-Newton
method [55]. Convex quadratic programming uses a positive deÔ¨Ånite Hessian ma-
72 Why?
73 We leave the minimum optimization problem as an exercise.
74 Salomon suggested that Œ∂ ‚âà1.8 is a good option.

3.5 Parameter Control
93
trix to adjust the search direction so that these algorithms converge very fast, i.e.,
n-dimensional quadratic programming could be Ô¨Ånished with at most n steps.
Real-world problems are often multimodal and isolated, so using quadratic pro-
gramming directly could only guarantee the local optimal solution.
Recall Fig. 3.4. We use it to demonstrate that problems with nonseparable vari-
ables are hard. PCX, UNDX, UNDX-m, and correlated normal mutation could pro-
vide a multi dimensional search, which increases the search efÔ¨Åciency. As men-
tioned above, in self-adaptive ES, the standard deviation and the covariance (or the
corresponding rotation angle) of multi dimensional normal distribution could be en-
coded into chromosome to be optimized by the algorithm.
In CMA-ES, the Œª new individuals generated with normal distribution are re-
garded as samplings on the solution space. The density function, Eq. 3.17, is a
quadratic function of x. If we could simplify the local area of the solution space
as a convex quadratic surface, the Œº best individuals among Œª might form a bet-
ter density function by which we could generate better individuals. We can use a
numerical experiment to illustrate the above idea.
Consider the problem with nonseparable variables illustrated by Fig. 3.4. Let us
suppose that (3,3) is the start point. We use the technique introduced in Sect. 3.2 to
generate Œª = 100 2-D normal distribution random points with a(g) = (3,3)T,B(g) =
[
1 0
0 1
]
, where g = 0. The individuals are illustrated in Fig. 3.20a.
We rank these 100 points according to their objective values in ascending order
and pick the Ô¨Årst Œº = 20 points x(g+1) to represent the hopeful search direction and
use them to construct the new B(g+1) as follows:
B(g+1) = 1
Œº
Œº
‚àë
i=1
(
x(g+1)
i
‚àía(g))(
x(g+1)
i
‚àía(g))T
(3.62)
where x(g+1)
i
is an individual i of generation (g+1), and a(g) is the mean of gener-
ation (g). The new covariance matrix is B(1) =
[
0.9870 0.6597
0.6597 1.4560
]
, which suggests
that they are strongly correlation between two variables.
Then the expectance of the new generation could be calculated as follows:
a(g+1) = 1
Œº
Œº
‚àë
i=1
x(g+1)
i
(3.63)
The new expectance is a(1) = (2.19,2.02). After that we could use the new ex-
pectance a(g+1) and the new covariance matrix B(g+1) to generate Œª = 100 new
individuals, illustrated by Fig. 3.20b. Then g = g+1.
Using the above procedure again and again, we can see that the estimated
density function could estimate the solution surface in the local area of the cur-
rent mean, cf. Fig. 3.20c‚Äìe, the covariance matrices are B(2) =
[
1.6171 1.5699
1.5699 2.0693
]
,
B(3) =
[
1.1142 0.7717
0.7717 0.6916
]
, B(4) =
[
0.0659 0.0092
0.0092 0.0388
]
, respectively. In this way, CMA

94
3 Advanced Evolutionary Algorithms






	







(a)



















(b)















(c)

















(d)
	















(e)
Fig. 3.20 A simple example of CMA: (a) initial distribution [a = (3,3)], (b) Ô¨Årst distribution [a =
(2.19,2.02)], (c) second distribution [a = (1.05,0.72)], (d) third distribution [a = (0.08,0.00)],
and (e) fourth distribution [a = (‚àí0.04,0.04)]

3.5 Parameter Control
95
can use the current best Œº individuals (feedback information) and Eqs. 3.62 and 3.63
(control rule) to adaptively change the density function for generating Œª new indi-
viduals (controlled parameter). From these iterative covariance matrices we can see
that CMA cannot only point the main search in the most hopeful direction but also
adjust the step size, i.e., variance, adaptively.75 The power of CMA can be illustrated
clearly by comparing Figs. 3.21 and 3.4b.76















































Fig. 3.21 Searching the nonseparable variables with CMA
Here we just illustrate the basic idea of CMA. The mutation step size is changed
adaptively in CMA, which could be regarded as a derandomized mutative step-size
control compared to the randomized œÉ control in Eq. 3.51. Hansen et al. also sug-
gested many other ways to iteratively control a and B. Interested readers are referred
to [54, 56].77
Another thing that needs to be mentioned is that CMA could be regarded as
an estimation of distribution algorithm (EDA), which considers the solution land-
scape as a probability density function space and uses the population to estimate that
probability distribution. EDAs are beyond the scope of this textbook, and interested
readers are referred to [57‚Äì60].
Those algorithms that use the previously found solutions in such a way that the
search will concentrate on the regions containing high-quality solutions and infor-
mation gathered during the search are denoted model-based algorithms.78 Zlochin
75 Readers are encouraged to compare the axis scales of Fig. 3.20b and e.
76 Readers are now encouraged to reread about normal mutation on page 54 to gain an in-depth
understanding of the geometrical interpretation of different covariance matrices B using Fig. 3.20.
77 The MATLAB‚ìásource code of CMA-ES can be downloaded at http://www.bionik.
tu-berlin.de/user/niko/cmatutorial.pdf.
78 The counterpart of model-based algorithm is instance-based algorithm, which generate new
solutions using only current solutions. Examples of instance-based algorithms are SGA, simulated

96
3 Advanced Evolutionary Algorithms
et al. give an excellent survey on model-based search for combinatorial optimiza-
tion [61].
3.5.2.7 Self-adaptive Crossover Probability Control in Differential Evolution
Many parameter control methods have been suggested for DE recently. Here we
mention just two different self-adaptive considerations based on their crossover
probability Cr.79
In DE, Cr determines the probability that the offspring will inherit genes from
the mutant, cf. Eq. 2.21. The self-adaptive control for Cr needs to code it into chro-
mosomes, i.e., (x1,x2,‚ãÖ‚ãÖ‚ãÖ,xn,Cr).
Qin and Suganthan considered that the crossover probability Cri of individual
i is initially generated with N(0.5,0.1) [50, 51]. The value remains unchanged in
Ô¨Åve generations. After that, a new Cr is generated with the same distribution, i.e.,
N(0.5,0.1). If a trial vector successfully enters the new generation, we record its Cr.
Then for every 25 generations, every Cr has been changed 25/5 = 5 times with
the same distribution, and the centroid of all recorded successful Cr is calculated as
Crm. Then Cr is generated with N(Crm,0.1) and the success record is reset to zero.
In this way, the better value of Cr could have more and more inÔ¨Çuence on the
whole population.
Brest et al. suggested another simple way to mutate Cr [62]. The mutation is
carried out as follows:
Cr
‚Ä≤ =
{
rand1,
rand2 < 0.1
Cr,
otherwise
(3.64)
where rand1 and rand2 are all U(0,1) numbers. Equation 3.64 means that Cr
changes to a new uniform distribution random number within (0,1) with probability
0.1 and remains unchanged otherwise.
3.5.3 Examples of popsize Control
As mentioned above, popsize has strong inÔ¨Çuences on the population level, so ade-
quate control on popsize might noticeably increase EAs‚Äô search. Many studies have
been done on this topic, and we will select some of them according to our interests.
annealing, etc. As discussed previously, sometimes it is hard to discriminate between model-based
algorithms and instance-based algorithms when the algorithm absorbs several powerful techniques
from both of them.
79 The papers cited below also have adaptive or self-adaptive control over other parameters. We
just chose Cr as an example.

3.5 Parameter Control
97
3.5.3.1 Saw-tooth-type Deterministic Control on popsize
Koumousis and Katsaras suggested a deterministic control on popsize [63]. A larger
popsize is good for exploration in the early stage, while a smaller popsize is good
for exploitation in the late stage. So Koumousis and Katsaras decreased the popsize
with gen.
Another aspect of popsize is that population reinitialization, with the help of
elitism, has been reported to have ascendancy in optimizing multimodal problems.
Koumousis and Katsaras combined the idea of variable population size and popula-
tion reinitialization. The main control rule for popsize is illustrated by Fig. 3.22.
Fig. 3.22 Saw-tooth deter-
ministic control on popsize













)












As can be seen from Fig. 3.22, the initial population is popsize+D. It decreases
linearly with gen to popsize‚àíD after T generations. Then reinitialization is carried
out with elitism to ensure that the best solution will never be lost. Detailed discus-
sions have been done on the sensitivity of
T
popsize and
D
popsize to different types of
benchmark problems. One result is that T is not sensitive to the performance of EAs
if T > 20. Another conclusion is that a larger D seems to have more advantages,
e.g.,
D
popsize ‚àà[0.90,0.98].
3.5.3.2 Adaptive Control on popsize Using an Age Concept
As far as we know, Arabas et al. were the earliest researchers focusing on the adap-
tive control of popsize using an age concept. They proposed a GA with varying
population size (GAVaPS) [64].
In GAVaPS, every individual has two extra properties: lifetime, which is propor-
tional to its relative Ô¨Åtness value, and age, which will be increased every generation.
Whenever an individual‚Äôs age is greater than its li fetime, it dies. Otherwise it will
remain in the population. In this way popsize becomes an observable parameter,
rather than a strategy parameter. The general solution process of GAVaPS could be
illustrated as follows:

98
3 Advanced Evolutionary Algorithms
General Solution Process of GAVaPS
Phase 1: Initialization.
Step 1.1: Assign the parameters for GAVaPS, such as reproduction ra-
tio œÅ, maximum and minimum lifetime MaxLT and MinLT, initial population
size popsize(0), and stop criteria.
Step 1.2: Generate popsize(0) uniformly distributed individuals ran-
domly to form the initial population. Evaluate the Ô¨Åtness value fi, calcu-
late lifetimei, and set agei = 0 for every individual i separately. gen = 0.
popsize(1) = popsize(0).
Phase 2: Main loop. Repeat the following steps until stop criteria are sat-
isÔ¨Åed.
Step 2.1: agei = agei +1 for every individual. gen = gen+1.
Step 2.2: Select œÅ √ó popsize(gen) individuals randomly, indepen-
dently of fi or agei, and generate new individuals using crossover and mu-
tation based on these individuals.
Step 2.3: Evaluate the Ô¨Åtness value for every new individual and cal-
culate its lifetime. The age of the new individual is 1.
Step 2.4: Combine the old individuals with the new individuals and
remove those whose agei is greater than their li fetimei.
Phase 3: Submitting the Ô¨Ånal individuals as the results of GAVaPS.
The population size of GAVaPS is
popsize(gen+1) = popsize(gen)+newpopsize(gen)‚àíD(gen)
(3.65)
where newpopsize(g) is the number of the new individuals and D(gen) is the num-
ber of the removed individuals in generation gen.
To assign lifetimei, Arabas et al. suggested three rules: proportional allocation,
linear allocation, and bilinear allocation for maximum problems as follows:
lifetimei = min
(
MinLT +Œ∑ fi
favg
,MaxLT
)
(3.66)
lifetimei = MinLT +2Œ∑
fi ‚àífmin(Abs)
fmax(Abs) ‚àífmin(Abs)
(3.67)
lifetimei =
{
MinLT +Œ∑
fi‚àífmin
favg‚àífmin ,
fi ‚â§favg
1
2 (MinLT +MaxLT)+Œ∑
fi‚àífavg
fmax‚àífavg ,
fi > favg
(3.68)
where MaxLT and MinLT are user-deÔ¨Åned parameters corresponding to the max-
imum and minimum lifetime of an individual, favg, fmax, and fmin are, respec-

3.5 Parameter Control
99
tively, the average, maximum, and minimum Ô¨Åtness values of the current population,
fmax(Abs) and fmin(Abs) are, respectively, the maximum and minimum Ô¨Åtness values
found so far, and Œ∑ = 1
2(MaxLT ‚àíMinLT). As can be seen from Eqs. 3.66‚Äì3.68, all
of these lifetime appointing schemes are with the intuitive idea that individuals with
better relative Ô¨Åtness values have longer lifetimes.
There is no obvious selection process in GAVaPS. The selective pressure is ad-
justed by agei and lifetimei. Better individuals have longer lifetimes, so that they
could have more chances to produce offspring.
Four parameters, popsize(0), MaxLT, MinLT, and œÅ, are necessary to eliminate
the static popsize in GAVaPS. Arabas et al. studied the sensitivity of these parame-
ters and suggested that œÅ = 0.4 might be the optimal value and found that popsize(0)
has little inÔ¨Çuence on the solution quality. They used MinLT = 1 and MaxLT = 7
in their numerical experiments. On the three lifetime allocation methods, their con-
clusion was that Eq. 3.67 had the best performance but with high computation cost,
Eq. 3.68 had the cheapest computation cost but lowest performance, and Eq. 3.66
was in the middle position.
The main drawback of GAVaPS is that, according to Eq. 3.65, in the worst case,
the population size will be doubled in the successive generations so that there might
be a population explosion. So B¬®ack et al. suggested an adaptive population size GA
(APGA) to handle this issue [65].
There are only three differences between APGA and GAVaPS:
1. APGA uses le fttimei instead of agei. The initial le fttimei = li fetimei. In each
generation, le fttimei = lefttimei ‚àí1.
2. APGA is a steady state GA. Only two new individuals will be generated and
inserted into a population. Those whose lefttimei = 0 will be removed.
3. APGA utilizes an elitism mechanism, i.e., the lefttime of the best individual in
the current population will not be decreased.
The population size of APGA is
popsize(gen+1) = popsize(gen)+2‚àíD(gen)
(3.69)
where D(gen) is the number of the removed individuals in generation gen. Accord-
ing to Eq. 3.69, APGA has a rather ‚Äústable‚Äù population size control mechanism.
Its maximum population size is 2MaxLT +1 and its approximate population size is
MinLT +MaxLT +1 [66].
In order to make popsize change but not change too much, Fernandes and Rosa
suggested the self-regulated population size EA (SRP-EA) [67]. Similar to the
mating restriction introduced on page 87, they utilized negative assortative mat-
ing and counted the number of successful matings and the number of failed mat-
ings. If more matings fail, threshold = threshold ‚àíDec. By contrast, threshold =
threshold +Inc.
Newly generated individuals are added to the population. SRP-EA uses the same
lifetimei allocation mechanism and agei adding mechanism as GAVaPS, but has its

100
3 Advanced Evolutionary Algorithms
probabilistic killing rule. All of the individuals, except the best one, die with the
probability
pi,die = 1‚àíli fetimei ‚àíagei
maxLT
(3.70)
Equation 3.70 means that better individuals, with larger li fetimei, and younger in-
dividuals, with smaller agei, have a smaller probability of dying.
The main mechanism to inhibit excessive population size in SRP-EA is that if the
number of new individuals is larger than that of dying individuals in this generation,
then threshold = threshold +Inc, which decreases the possibility of generating new
individuals in the next generation. So in SRP-EA, there is a self-regulated population
control mechanism.
Five parameters, popsize0, MaxLT, MinLT, Dec, and Inc, are necessary to elim-
inate the static popsize in SRP-EA. Fernandes and Rosa did the parameter sensitiv-
ity analysis on benchmark problems. They claimed that Inc = Dec ‚àà(1%L,10%L),
where L is the length of chromosome, are good options for SRP-EA.
3.5.3.3 Population Competition-based Adaptive Control on popsize
In 1999 Harik and Lobo considered the problem of popsize control as the competi-
tion between different population sizes and suggested a very interesting method for
controlling popsize [68]. They call the algorithms parameter-less GA.
Their rationale for population competition is that smaller population should have
more chance to exploit the solution to save computational cost. But if the average
Ô¨Åtness value of a smaller population is worse than that of a larger population, the
smaller population will be overtaken by the larger population and will be deleted.
Apart from that, if all the chromosomes are the same in one population, meaning it
is stuck in the local optimal solution, it will be deleted.
The mechanism to control the competition is very straightforward: a simple
counter of base 4.
In a 4-base counter, we get (0), (1), (2), (3), (10), (11), (12) , (13), (20), (21),
(22), (23), (30), (31), (32), (33), (100), (101), etc.
Every counter in the above sequence means a generation for one population.
We start with the Ô¨Årst population with popsize = 4, and the Ô¨Årst four counters in
the above sequence mean that we run this population four generations. The carry,
from (3) to (10), means the generation of the second population, which is twice the
size of the Ô¨Årst population, i.e., popsize = 8 in this example. The new population will
run for a generation and return the control to the Ô¨Årst population, i.e., the population
with four individuals will run for the following three generations. Then the change
in the high position, from (13) to (21), means a generation for the second population,
i.e., the population with eight individuals will run for one generation and then return
to the Ô¨Årst population.
To sum up, every carry into the higher position means an initialization of a new
population whose popsize is twice that of its previous population, every change in

3.6 Performance Evaluation of Evolutionary Algorithms
101
the high position means a generation for the larger population, and after these two
operations the Ô¨Årst population will continue to run until the next change in the high
position or carry. If one population has been deleted, the counter will be reset.
In this way, parameter-less GA has a mechanism of increase and decrease
popsize with very few predeÔ¨Åned parameters.80 If we are dealing with some prob-
lem and have no idea of the complexity of its solution landscape, i.e., have no idea
of the magnitude of popsize, a parameter-less GA might be a powerful adaptive way
to search.
Some interesting numerical comparisons have been done on the above popsize
control mechanisms [66, 69]. Lobo and Lima gave a very comprehensive survey on
this subject [70]. We adopt some guidelines discussed in the survey as follows:
‚àôNo upper bound for adaptive popsize control. We have no idea on the difÔ¨Åculty
of the real-world problem. SpeciÔ¨Åc popsize, selection methods, and variation
operators constitute the speciÔ¨Åc search ability of the algorithm over the solution
landscape. So setting the upper bound for popsize might not be a good option.
‚àôDo not forget the excuse of adaptive control. We want to alleviate the pressure
of strategy parameter tuning for EA users. So do not transfer the pressure of
strategy parameter tuning to the pressure of control parameter tuning.
‚àôConsider the scalability of the algorithm. Try to test how the algorithm scales
up with problems of different sizes.
‚àôCompare in both directions. Both the solution quality after the same objective
function evaluation time and the objective function evaluation time to reach the
same solution quality should be considered.
We have no content to introduce the population size control in DE. Interested
readers are referred to three papers published in 2006, 2008, and 2009, respectively
[71‚Äì73].
3.6 Performance Evaluation of Evolutionary Algorithms
3.6.1 General Discussion on Performance Evaluation
If you have read some research papers on EAs according to our suggestion or your
interests, you must have noticed that at least half the content of the papers is about
the performance evaluation of an algorithm or an operator. That means we need to
focus ourselves on evaluating the improvements we made fairly and statistically to
conÔ¨Årm that our initial idea, which sounds reasonable, is correct. But unfortunately,
few papers and textbooks discuss such an important issue.81
80 The initial popsize = 4 might be regarded as one parameter.
81 Eiben and Smith‚Äôs textbook has a chapter on it. We strongly encourage our readers to read [37].

102
3 Advanced Evolutionary Algorithms
3.6.1.1 No Free Lunch Theorem for Optimization
The Ô¨Årst thing you need to keep in mind about the performance evaluation of algo-
rithms is that the performance depends on problems.
Wolpert and Macready published a paper with a very strong title: ‚ÄúNo Free Lunch
Theorems for Optimization‚Äù [74]. The key contents of the paper can be quoted as
follows:
For both static and time dependent optimization problems, the average performance of any
pair of algorithms across all possible problems is identical.
That‚Äôs really a very interesting yet annoying statement. The interesting part is the
fact that the performance evaluation is on speciÔ¨Åc (benchmark) problems, while the
annoying part is that we do not know what the real-world problem we will face is
while researching EAs. So we generally have no idea on the real performance of
our deliberately designed algorithm.82 We can roughly illustrate the No Free Lunch
theorem with Fig. 3.23.

















	







 


2


*



 $	

 
'
	
.
 C	
%
	
Fig. 3.23 One kind of interpretation of No Free Lunch theorem
The horizontal axis is various types of problems and we suppose that they have
been elaborately arranged so that problems with similar properties have been put
82 There are many other agreements and disagreements on this paper and they may strengthen your
understanding of performance evaluation [75].

3.6 Performance Evaluation of Evolutionary Algorithms
103
closer, and the vertical axis is one performance criterion.83 We use one uniform
distribution density function and three normal distribution density functions with
different standard deviations to illustrate four algorithms. Several points need to be
mentioned about Fig. 3.23.
‚àôThe more we understand the problem, the more speciÔ¨Åc technique we could de-
sign for solving it, and the better performance it will have, but the less robust it
will be for other problems.
‚àôWe need to demonstrate that our algorithms are better than random search on the
problem we face.84
‚àôGeneral purpose EAs are reliable methods when you are doing a blind or near-
blind search in most cases.
‚àôIf problem information could be embedded into the encoding and decoding pro-
cess and into operators, together with a problem-dependent local search method,
the performance of the algorithm would be improved at the expense of lower
adaptability for other problems.
To sum up, the No Free Lunch theorem is the sword of Damocles when you want
to prove that your algorithm is an ideal one.
3.6.1.2 Considerations on Benchmark Problems
A set of benchmark problems is often used by EA researchers to do the performance
evaluation and comparison. But the alert reader must wonder why we need to do the
performance comparison if there exists the No Free Lunch theorem. We have four
answers to this question.
‚àôThe No Free Lunch theorem addresses all kinds of problems including many
random or artiÔ¨Åcially generated freak examples. But most of the real-world prob-
lems we face are some kind of ‚Äúnormal‚Äù problems, which prompts us to use other
‚Äúnormal‚Äù benchmark problems to test the performance of our algorithm on the
problem we are concerned with.
‚àôWe can use different types of benchmark problems and different performance
criteria to Ô¨Ånd out which algorithm (with which operator) is good at which prob-
lem type. In fact, we do not do a totally blind search on a given problem. We
know some properties about the problem through previous experience or the trial
calculation. Thus we can choose our weapons, if we know what they are good at
in advance, to handle it.
‚àôWe can use different types of benchmark problems to test the robustness, or the
sensitivity, of the algorithm, the operator, or the parameter we are researching so
that we can further improve it.
83 We will discuss several criteria later. Here we suppose that a larger performance criterion value
means better performance.
84 This is not a trivial requirement and we will show you how to Ô¨Ånd competitors for your algorithm
later.

104
3 Advanced Evolutionary Algorithms
‚àôGenerally, we know the optimal value of the benchmark problem or at least know
the most recently reported best value. There is great value in doing performance
evaluation and comparison.
We can roughly classify benchmark problems into three groups as follows:
1. Problems separately suggested by different researchers. These researchers have
different backgrounds and interests, so their suggestions cover a large scale of
horizontal axis in Fig. 3.23, but it is hard to decide which one to use for test.
Famous benchmark problems are introduced in [16, 17, 33, 36]. Starting in
2005, the IEEE Congress on Evolutionary Computation has given special ses-
sions on numerical optimization competitions, including real parameter opti-
mization (2005), constrained real parameter optimization (2006), multiobjective
optimization (2007), large-scale global optimization (2008), dynamic optimiza-
tion (2009), and multiobjective optimization with constraints (2009). Interested
readers may consult its Web site.85
2. Benchmark problem generators suggested by different researchers. We can gen-
erate problems rather easily using these generators. But generally these gener-
ators generate problems with similar properties so it is not good to put all your
eggs in one basket. Famous generators are introduced in [76‚Äì78]. Interested read-
ers might refer to the Repository of Test Problem Generators Web site.86
3. Real-world problems. To solve real-world problem is the real value of EAs. But
generally every detail of real-world problems canot be published because of com-
mercial, conÔ¨Ådential, too complicated excuses, so it is hard for other researchers
to replicate the numerical experiments and results, which is a basic requirement
for comparison.
Selecting the test suite from the above benchmark problems for your performance
evaluation depends on what you want to test. Generally, efÔ¨Åcacy, i.e., quality, efÔ¨Å-
ciency, i.e., speed, and reliability, i.e., success rate, are the three main considerations
that we will discuss later. The following guidelines were suggested by B¬®ack [36] and
enhanced by Eiben in [37]; we add some other considerations to help you form the
test suite.
1. The test suite should include a few unimodal functions to test efÔ¨Åciency.
2. The test suite should include several multimodal functions with a large number
of local optima to test efÔ¨Åcacy.87
3. The test suite should include functions with uncertain elements, such as noisy
and dynamic problems to test the robustness of the algorithms.
4. The test suite should contain scalable problems, whose dimension can be changed
easily, to test the scalability of the algorithm for large problems.
5. The test suite should contain problems with nonseparable variables to avoid a
tricky decomposition.88
85 http://www3.ntu.edu.sg/home/EPNSugan/
86 http://www.cs.uwyo.edu/Àúwspears/generators.html
87 One special type of multimodal problem is the so-called deceptive problem [79].
88 Rotation provides a way to introduce nonseparability [4].

3.6 Performance Evaluation of Evolutionary Algorithms
105
3.6.2 Performance Evaluation and Comparison
In 1995 Barr et al. published a very helpful survey on designing and reporting nu-
merical experiments with heuristic methods [80], which could also be used in EAs.
According to Huband et al.‚Äôs suggestion, the typical scenario of EA comparison is
as follows [81]:
1. Select the EAs to compare.
2. Form the proper test suite.
3. Select adequate performance indices to evaluate the algorithms.
4. Obtain results for all competitors, either from the published results or generate
the results by users and then generate the performance indices for each algorithm.
5. Compare the metrics statistically.
6. Draw conclusions.
The procedure of selecting competitors depends on the current state of your re-
search. If little algorithm research has been done on your model, then at least a
comparison between your contribution and the random search is necessary. If other
(meta-)heuristic algorithms have been used to solve the model, such as SGA, you
need to make them the baseline. If the problem you are researching is rather ‚Äúhot‚Äù
so that there are several ‚Äúclassical‚Äù algorithms, such as SR in constrained optimiza-
tion and NSGA-II in multiobjective optimization, you need to compare with them.
If your research is the direct improvement of one algorithm, the comparison should
be done between the new one and the old one.
In the above scenario, different test suites, performance indices, and stop criteria
might suggest different conclusions of the comparison.89 So doing fair performance
evaluation and comparison is rather complicated. We will discuss these three ele-
ments and their interactions in the following part of this section.
3.6.2.1 Performance Indices of Numerical Optimization
The performance indices (PIs)90 strongly depend on the application, i.e., what you
want from the algorithm. De Jong suggested that the online and ofÔ¨Çine performance
imitated the performance requirements in on-line and off-line situations, respec-
tively, separately for EAs [33]. Then in 2003, Eiben and Smith suggested three
indices that have in recent years been used most often [37]. In 2005, at a special
session on real parameter optimization of the IEEE Congress on Evolutionary Com-
putation, they were developed further [82].
EAs are random-based algorithms, so they might provide different results at dif-
ferent runs. In order to evaluate and compare them fairly, we need to run them many
times and take the results of all runs to generate a PI. There is no rule regarding how
89 Sometimes even controversial conclusions.
90 Other names include performance assessment, performance metric, performance indicator, and
performance measure.

106
3 Advanced Evolutionary Algorithms
many runs are sufÔ¨Åcient or necessary for an algorithm on a problem, which might be
limited by the computing environment, i.e., the objective function evaluation cost.
But we believe a larger number of runs could provide more objective performances
and the following statistical procedure requires large samples to draw conclusions.
So at least 20 to 30 runs on one problem for each algorithm are required. Different
runs should start with different initial populations, but we suggest that the different
EAs have the same initial populations in one run in order to make the comparison
fairer.91
We have introduced many techniques to dynamically control popsize and adap-
tively stop the computation. So for fair comparison, it is better to stop the algorithms
with either of the following two conditions.
1. The difference between the Ô¨Åtness value of the current best individual fbest and a
predeÔ¨Åned value f ‚àóis under a predeÔ¨Åned threshold Œµ. f ‚àóis the optimal value of
the benchmark problem or the most recently reported best value and Œµ depends
on user requirements. We want to know whether the algorithm can reach the
threshold or not. If so, we further want to know how fast the algorithm can reach
it.
2. The number of objective function evaluation (NOFE) has reached the predeÔ¨Åned
value MaxNOFE. We want to know the solution quality after MaxNOFE iterations
of the objective function.
Smaller Œµ and larger MaxNOFE mean more search effort. It is necessary to men-
tion that determining Œµ and MaxNOFE is not a trivial task because the convergence
speeds of different algorithms vary distinctly, which will be demonstrated later.
Generally speaking, there are two categories of PI: the overall PI, which de-
scribes the overall performance with a number, and the evolving PI, which describes
the evolving process with a series of numbers.
We can group the overall PI into efÔ¨Åcacy, efÔ¨Åciency, and reliability.
1. EfÔ¨Åcacy. We want to evaluate the quality of the results the algorithm provides
and do not care about the speed. The mean best Ô¨Åtness (MBF) is deÔ¨Åned as the
average of the best Ô¨Åtness in the last population over all runs. MBF could be
used in totally ofÔ¨Çine situations. Apart from the best Ô¨Åtness in the last population,
the best Ô¨Åtness values thus far could be used as a more absolute evaluation for
efÔ¨Åcacy.
2. Reliability. We want to know the extent to which the algorithm can provide ac-
ceptable results. Success rate (SR) is deÔ¨Åned as the percentage of runs termi-
nated with success. We deÔ¨Åne a successful run as the difference between the best
Ô¨Åtness value in the last generation fbest and a predeÔ¨Åned value f ‚àóunder a pre-
deÔ¨Åned threshold Œµ, i.e., the algorithm ends with stop criterion 1. SR, combined
with MBF, could be used in situations with time requirements, so we want to get
good results with limited runs. Low SR and high MBF might suggest that the
91 Fogel and Beyer suggested that the initialization should be biased against the global solution of
the benchmark problems, i.e., do not include the global solution in the initial generation domain of
the variables, so that we can test the true explorative ability of the algorithms [83].

3.6 Performance Evaluation of Evolutionary Algorithms
107
algorithm converges slowly, and high SR and low MBF might suggest that the
algorithm is basically reliable but might provide very bad results accidentally.
3. EfÔ¨Åciency. In general, we want to Ô¨Ånd the global optimal solution as soon as
possible. So the average number of evaluations to a solution (AES) is deÔ¨Åned as
the number of evaluations it takes on average for the successful runs to Ô¨Ånd an
optimum, i.e., stop with criterion 1. If an algorithm has no successful runs, its
AES is undeÔ¨Åned. AES can be used in online situations.
4. Reliability and efÔ¨Åciency. We want smaller AES and larger SR, so why not com-
bine them as smaller AES/SR. This criterion considers reliability and efÔ¨Åciency
simultaneously.
Each of the above PIs provides a number to represent the overall performance
regarding one aspect for an algorithm on a benchmark problem with many runs.
Sometimes we are also interested in considering the evolving process of the al-
gorithms. Thus many other generation-based evolving PI are necessary, which are
illustrated as follows:
1. Best-so-far (BSF). We record the best solution found by the algorithm thus far
for each generation in every run.92 Obviously the BSF index is monotonic. The
Ô¨Ånal BSF is the BSF when the algorithm satisÔ¨Åes the stop criteria.
2. Best-of-current-population (BCP). We record the best solution in each gener-
ation in every run. MBF is the average of Ô¨Ånal BCP or Ô¨Ånal BSF over multiple
runs.
3. Average-of-current-population (ACP). We record the average solution in each
generation in every run.
4. Worst-of-current-population (WCP). We record the worst solution in each
generation in every run.
The answer to the problem of which evolving PI to take depends on the appli-
cation environment. If we want only one best solution by the end of the run, BSF
might be the proper selection. But if all the popsize individuals will be used by the
end of the run, BCP, ACP, and WCP describe the performance of the algorithm from
various viewpoints.
3.6.2.2 Performance Description and Comparison of Evolutionary Algorithms
The overall PI might suggest a concise description for the performance, but the
evolving PI might provide more detailed information. The problem of the evolving
PI lies in the fact that after many runs with random initial populations, we might get
so many evolving PI numbers that sometimes we are at a loss to describe and com-
pare the performance of EAs using these data. In this part, we will show you three
92 The best solution so far might participate in the following evolving process of the algorithm or
not, depending on the requirement of various algorithms. If it involves the evolving process, the
algorithm has an elitism mechanism.

108
3 Advanced Evolutionary Algorithms
types of statistical description, i.e., statistical visualization, descriptive statistics, and
statistical inference, as the tools to draw conclusions about your algorithm.
Statistical Visualization
Statistical visualization uses graphs to describe and compare EAs, which is very
illustrative.
The box plot is the most useful way to graphically illustrate the performance of
EAs. Suppose we run an algorithm on a problem 100 times and get 100 numbers
for one PI, e.g., Ô¨Ånal BSF.93 We can rank these 100 numbers in ascending order.
The lower quartile is the 0.25√ó100+1 = 26th smallest BSF x.25, the median is the
0.50√ó100+1 = 51st smallest BSF x.50, and the higher quartile is the 0.75√ó100+
1 = 76th smallest BSF x.75. Then we could draw a box to include numbers between
x.75 and x.25 to illustrate the main part of the data94 and draw a line at x.50 to illustrate
the median,95 just like in Fig. 3.24. The interquartile range (IQR) is x.75 ‚àíx.25. Any
data that lie more than 1.5√óIQR lower than the lower quartile or 1.5√óIQR higher
than the higher quartile is considered an outlier. We draw two lines to indicate the
smallest number that is not a lower outlier and the largest number that is not a
higher outlier,96 denoted as whisker, and connect the whiskers with the box. Other
outliers are illustrated by ‚Äú+‚Äù. We can do the above procedure for every algorithm
and draw their box plots together. Many softwares provide a function to draw box
plots easily.97 An example of the box plots of four algorithms is illustrated in Fig.
3.24.
From the performance graph in Fig. 3.24 we can easily say that algorithm 4 is
the best one because its median BSF is large and its IQR is small, which can be
interpreted as meaning that the average performance of algorithm 4 is good, along
with its small variance. But there are difÔ¨Åculties in differentiating the performance
between algorithms 1 and 3. Statistical visualization is not strong enough to do this.
We will deal with it later.
We often want to represent the evolving process of many runs. So the conver-
gence graph illustrating the performance over NOFE is quite useful.
Suppose we need to compare two groups of results, e.g., 100 runs by algorithm
A and another 100 runs by algorithm B, with BSF,98 then a box plot is a very clear
and straightforward way to illustrate the convergence procedure. We can record BSF
93 Suppose we are dealing with the maximum problem, i.e., larger Ô¨Ånal BSF means better perfor-
mance.
94 The vertical location of the lower and upper horizontal lines of the box are the true numbers of
x.25 and x.75, respectively.
95 The vertical location of the line is the true number of x.50.
96 Here the numbers are the true numbers in the data set, not the number calculated by 1.5√óIQR.
97 MATLAB‚ìáhas a ‚Äúboxplot‚Äù function. There are other variations of box plot. Interested readers
are referred to [84].
98 In this example, the smaller the better.

3.6 Performance Evaluation of Evolutionary Algorithms
109
every 1000 NOFEs.99 We run every algorithm 100 times and record the 100 perfor-
mance values on each record time. Then we draw the convergence box plot graph
illustrated in Fig. 3.25.
1
2
3
4
1
0.5
0
0.5
1
1.5
MB F
Algorithms
BSF
Fig. 3.24 Performance graph using box plot for four algorithms





	








	





 !"#$ 
 !"#$
Fig. 3.25 Convergence graph using box plot for two algorithms
99 This means that we take 1000 NOFEs as a virtual common generation if we want to compare
two algorithms with popsize variation techniques. Here every 1000 NOFEs is called a record time.

110
3 Advanced Evolutionary Algorithms
We also calculate the mean of 100 runs at every record time for every algorithm
and connect them in Fig. 3.25, i.e., the squares are for algorithm A and the circles
are for algorithm B. Several useful conclusions can be drawn from Fig. 3.25, such
as algorithm B converges fast but the Ô¨Ånal quality is not as good as algorithm A, and
the variance in the Ô¨Ånal best solutions of algorithm A is small, which means that A
is more reliable, etc.
With the help of Fig. 3.25, we can further discuss drawing conclusions. The var-
ious stop criteria introduced above might inÔ¨Çuence the comparison results. For ex-
ample, if Œµ is very small or MaxNOFE is very large, we can say that algorithm A
is better than algorithm B. But with larger Œµ or smaller MaxNOFE, opposite conclu-
sions might be drawn. But that does not mean that you can cheat others by selecting
stop criteria suitable for your proposed algorithms. Our suggestion is to run the al-
gorithms with the smallest affordable Œµ or the largest affordable MaxNOFE, and then
compare them with various PIs in various record times and Ô¨Ånd out the advantages
and disadvantages, using the various statistical descriptions described above and
below, of your algorithm over others.
Descriptive Statistics
Graphs are easy to understand, but sometimes the difference between different al-
gorithms is small. Then we need speciÔ¨Åc numbers to describe and compare the per-
formance.
The most often used descriptive statistics are mean and variance (or standard
deviation). The mean of n data is
¬ØX = 1
n
n
‚àë
i=1
xi
where xi is datum i. The variance of n data is
S2 = 1
n
n
‚àë
i=1
(xi ‚àí¬ØX)2 = 1
n
n
‚àë
i=1
x2
i ‚àí¬ØX2
where xi is datum i and ¬ØX is the mean of these n data. The standard deviation is the
square root of variance. Then we calculate and tabulate the mean and the variance
of all PIs and compare them.
Statistical Inference
Sometimes descriptive statistics is also not strong enough to differentiate between
two results, in which case we need statistical inference. Statistical inference in-
cludes parameter estimation, hypothesis testing, and many other techniques. Here
we focus on hypothesis testing to verify whether the difference between two results
is statistically signiÔ¨Åcant.

3.6 Performance Evaluation of Evolutionary Algorithms
111
Let us take the results of algorithms 1‚Äì3 in Fig. 3.24 for example.
The mean BSF of algorithm 1 is ‚àí0.0764, that of algorithm 2 is 0.5750, and that
of algorithm 3 is 0.1429. We can see from Fig. 3.24 or the mean of these algorithms
that algorithm 2 is the best one. But how do we draw conclusions about algorithms
1 and 3? Does the difference come from the nature of these algorithms, i.e., the two
groups of results by algorithms 1 and 3 are two samples on two different density
functions, so the difference comes from different expectancies of density functions,
or from the sampling noises, i.e., the two groups of results by algorithms 1 and 3
are two samples on the same distribution functions so the difference comes from
sampling noises? We need to use hypothesis testing to verify whether there exists
statistical signiÔ¨Åcance or not.
The most often used hypothesis testing method in EA community to verify the
above hypothesis is the t test, which assumes that the two groups of results all obey
normal distribution and have same variance. Then there is a theorem.
Theorem 3.2. For two groups of normal distribution random numbers with the same
standard deviation, N(Œº1,œÉ2) and N(Œº2,œÉ2), we independently sample n1 points
from group 1 and n2 points from group 2. Suppose that their means are ¬ØX1 and ¬ØX2
and variances are S‚àó2
1 and S‚àó2
2 , respectively. Then
T = ( ¬ØX1 ‚àí¬ØX2)‚àí(Œº1 ‚àíŒº2)
‚àö
1
n1 ‚àí1
n2 S‚àó
(3.71)
obeys a t distribution with n1 +n2 ‚àí2 degrees of freedom, where
S‚àó=
‚àö
(n1 ‚àí1)S‚àó2
1 +(n2 ‚àí1)S‚àó2
2
n1 +n2 ‚àí2
The proof of the theorem can be found in any statistics textbook. The density
function of a t distribution with n = 1 degrees of freedom is illustrated in Fig. 3.26.
Higher n makes the t distribution approach the standard normal distribution.
After the above ‚Äútedious‚Äùpreparation, we can start the hypothesis testing. One
way to do it is to hypothesize that Œº1 = Œº2, i.e., the two groups of samples are from
the same normal distribution. This suggests a null hypothesis of H0 : Œº1 = Œº2.100 The
counterpart of this null hypothesis is an alternative hypothesis, i.e., H1 : Œº1 ‚àï= Œº2.
If our hypothesis is right, Œº1 = Œº2, according to Eq. 3.71, the following random
number T satisÔ¨Åes t distribution with n1 + n2 ‚àí2 degrees of freedom, i.e., T is a
sample on the t distribution function.
T =
¬ØX1 ‚àí¬ØX2
‚àö
1
n1 ‚àí1
n2
‚àö
(n1‚àí1)S‚àó2
1 +(n2‚àí1)S‚àó2
2
n1+n2‚àí2
(3.72)
100 There are other options, such as H0 : Œº1 < Œº2. We just introduce the simplest one here.

112
3 Advanced Evolutionary Algorithms
We can calculate the number T by Eq. 3.72 using the n1 + n2 numbers and we
want to verify whether T is a t distribution random number or not. If so, our hypoth-
esis H0 : Œº1 = Œº2 is approved, otherwise it is rejected.





















1
@
3

1
3
Fig. 3.26 t distribution density function (n = 1) and its upper Œ± percent number
But a t distribution number can have any value according to Fig. 3.26. So we
need to Ô¨Ånd out the scale in which the t distribution mainly resides. We can deÔ¨Åne
the upper Œ± percent number of t distribution with n degrees of freedom tŒ± (n) as
P{t (n) > tŒ± (n)} =
‚à´‚àû
tŒ±(n) p(n,t)dt = Œ±
(3.73)
where t (n) is a random number obeying a t distribution with n degrees of freedom
and p(n,t) is the density function of the t distribution with n degrees of freedom.
The upper percent number is also illustrated in Fig. 3.26.
From the deÔ¨Ånition of the upper Œ± percent number and the symmetry property
of the t distribution, we know that if we generate a t distribution random number, it
has probability 1‚àíŒ± of being within the interval (‚àítŒ±/2 (n),tŒ±/2 (n)).
Then we can predeÔ¨Åne a threshold Œ± to verify whether the calculated number T,
according to Eq. 3.72, is a t distributed number or not, i.e., T is within the interval
(‚àítŒ±/2 (n),tŒ±/2 (n)) or not, where n = n1+n2‚àí2. If so, we claim that the probability
of T being a t distribution random number is larger than 1‚àíŒ± and then accept H0,
else reject H0.
Actually Œ± determines the probability of regret when T is a t distribution number
but we claim it is not, i.e., the hypothesis is true but we claim that it is wrong. A
larger Œ± makes the same T harder to claim as a t distribution number, i.e., the ac-

3.6 Performance Evaluation of Evolutionary Algorithms
113
cepted T is more like to be a t distribution number.101 So we call Œ± the signiÔ¨Åcance
level.
In practice, we need to Ô¨Årst make the null hypothesis, e.g., Œº1 = Œº2, and assign
the signiÔ¨Åcance level Œ±.102 Then we can calculate T with two groups of numbers, n1
and n2, and Ô¨Ånd the Œ±/2 upper percent number in the table of t distribution with n1+
n2 ‚àí2 degrees of freedom.103 If ‚à£T‚à£‚â•tŒ±/2 (n1 +n2 ‚àí2), we reject H0 and claim that
these two groups of numbers are samples from two normal distribution functions
with different expectancies, i.e., the means of these two groups of numbers have a
statistical difference with signiÔ¨Åcance level Œ±. Otherwise, we accept H0 and claim
that these two groups of numbers are samples from the same normal distribution
function, i.e., the means of these two groups of numbers do not have a statistical
difference with signiÔ¨Åcance level Œ±.
Many softwares provide a function to do a t test.104 The results of a t test be-
tween the data of algorithms 1 and 2 is that the means of algorithms 1 and 2 have a
statistical difference with signiÔ¨Åcance level Œ± = 0.05. According to the t test result,
we can say that the means of algorithms 1 and 3 do not have a statistical difference
with signiÔ¨Åcance level Œ± = 0.05. In this way, we can statistically claim whether the
difference in mean between two groups of numbers is from their internal differences
or from the sampling noise.
The t test has the assumption that the two groups of results all obey the normal
distribution and have the same variance. There is a Mann‚ÄìWhitney test to do a
similar task without this assumption. Also the difference between two variations
could be veriÔ¨Åed using an F test.105
ANOVA and Orthogonal Experiment Design
The last, but not least, thing about performance evaluation is how to design numer-
ical experiments to verify the inÔ¨Çuence of one parameter and the interaction be-
tween parameters. This type of statistical inference is called ANalysis Of VAriance
(ANOVA).
Recall the taxonomy of parameter control discussed in Sect. 3.5; one might argue
that we do not need to do the parameter tuning because there are so many determin-
istic, adaptive, and self-adaptive control methods for strategy parameter control. But
if you look at these techniques closely, most of them have other control parameters
to ‚Äúintelligently‚Äù control the strategy parameters. So at least we need to prove that
these control parameters are robust, i.e., different values of these parameters have
less inÔ¨Çuence on the performance of EAs. This could also be transferred to a hy-
101 It also means the higher conÔ¨Ådence for the accepted H0 but higher possibility to reject H0 with
the same T.
102 The most commonly used signiÔ¨Åcance level is Œ± = 0.05.
103 Every textbook on statistics has such a table.
104 MATLAB‚ìáhas a ‚Äúttest2‚Äù function to do a t test.
105 MATLAB‚ìáhas a ‚Äúranksum‚Äù function to do a Mann‚ÄìWhitney test and a ‚Äúvartest2‚Äù function to
do an F test. Interested readers are referred to [85].

114
3 Advanced Evolutionary Algorithms
pothesis testing problem with H0: the mean value of results with different control
parameters do not have a statistical difference with signiÔ¨Åcance level Œ±.106
When there are two control parameters, we want to know not only their robust-
ness but also their interactions, which means we cannot tune these control parame-
ters one by one. ANOVA can also solve this problem.107
Under the condition that there are many control parameters, each having many
possible values, it will be very hard to test their robustness and interactions fully.
So this is where the experiment design comes in. We just introduce the orthogonal
experiment design based on Latin square; other methods can be found in [86, 87].
In experiment design, the control parameters are called factors. Each factor can
have different levels. Suppose there are r factors, each with s levels; the full ex-
periment number is sr. This number is sometimes too large to be implemented. So
people have already designed orthogonal experiment tables, such as L4(23), L8(27),
L16(215), L12(211), L9(34), and L27(313), to decrease the experiment number while
still testing these factors and levels fully and fairly. Ln(sr) means for r factors, each
with s levels, we can do n experiments to test their inÔ¨Çuence on the performance.
The L9(34) table, where the full experiment number is 34 = 81, is as follows:
Table 3.2 Orthogonal experiment table L9(34)
Experiment
Factor
1
2
3
4
1
1
1
1
1
2
1
2
2
2
3
1
3
3
3
4
2
1
2
3
5
2
2
3
1
6
2
3
1
2
7
3
1
3
2
8
3
2
1
3
9
3
3
2
1
The Ô¨Årst column in Table 3.2 represents the experiment number and every other
column represents one factor. The contents in the table represent the level of the
factor in one experiment. In nine experiments, every level of every factor was used
three times. Every level of every factor encounters every level of every other factor,
so that we can use ANOVA to analyze the interaction between them.108
106 MATLAB‚ìáhas a ‚Äúanova1‚Äù function to do this.
107 MATLAB‚ìáhas a ‚Äúanova2‚Äù function to do this.
108 MATLAB‚ìáhas a ‚Äúanovan‚Äù function to do this.

3.6 Performance Evaluation of Evolutionary Algorithms
115
Aggregate Score on All Test Problems
Previously we gave suggestions for fair comparisons between algorithms with one
benchmark problem. With different benchmark problems, conclusions might be dif-
ferent. How do we draw the aggregate conclusion on all test problems? Kennedy
has suggested a way [88].
Let us discuss Kennedy‚Äôs method with a simple example. Suppose we have tested
algorithms A, B, and C on two minimum benchmark problems, Each algorithm
optimizes each problem ten times, and we use the Ô¨Ånal BSF as the PI. Then we have
ten numbers of each algorithm on each problem (Table 3.3).
Table 3.3 The Ô¨Ånal BSF of three algorithms on two minimum problems





.
 
.
 
2

:


:


)0&
&#0&
)#0##
 #0&#
 #0)"
 0

)0
&0&
0##
 #0##
 "0)
 0&

)#0))#
)0"
&)0)
 0)
 #0&"
 #0&)

&#0")
)#0"&
""0"
 #0&)&
 )0)#
 0#

##0")
)0&
0)
 #0&
 0#
 0)##

&0)"
))0)))
))0#)
 #0)
 0"&"
 0
"
")0)&#
)0)
#0#"#
 0#)
 0""))
 0&
#
&&0"&
)0
&#0"
 "0&&
 "0#&
 0)
&
&0&
&0"
&0#)
 &0)#
 #0&)
 #0&)
)
#&0)#&)
"0&
0"#
 &0
 0"#"
 0#&



The bottleneck of the aggregate score on different problems lies in the different
magnitude of the results (Table 3.3).
To scale them into comparable magnitudes, we can Ô¨Årst calculate the mean and
the standard deviation of the total results for each problem (30 numbers for each
problem in this example), i.e., Œº1 = 98.6904, œÉ1 = 13.9389, Œº2 = ‚àí29.7313, and
œÉ2 = 6.1936.
Then, by BSF
‚Ä≤
1 = (BSF1 ‚àíŒº1)/œÉ1 and BSF
‚Ä≤
2 = (BSF2 ‚àíŒº2)/œÉ2 ,we can trans-
form all the data into zero mean and unit standard deviation,109 thereby solving the
problem of different magnitudes.
Now we can calculate the mean of each algorithm on each problem. For exam-
ple, for algorithm A, we get ŒºA1 = ‚àí0.2782 on problem 1 and ŒºA2 = 0.2365 on
problem 2. Since for the total 30 results of problem 1 (and problem 2) the trans-
formed BSF
‚Ä≤
1 are with zero mean and unit standard deviation, we can say that algo-
rithm A is good at problem 1 (because ‚àí0.2782 < 0) but bad at problem 2 (because
109 Here BSF
‚Ä≤
1 and BSF
‚Ä≤
2 represent the transformed Ô¨Ånal BSF for each datum.

116
3 Advanced Evolutionary Algorithms
0.2365 > 0). Then we can sum these two data up to represent the overall perfor-
mance of algorithm A as ŒºA = ŒºA1 + ŒºA2 = ‚àí0.0417. In the same way, we can get
that ŒºB = ‚àí0.4234 and ŒºC = 0.4650.110 Because all the benchmark problems have
a minimum objective, we can then come to the conclusion that algorithm B is the
best one among them on the two benchmark problems with respect to Ô¨Ånal BSF.
3.7 Brief Introduction to Other Topics
3.7.1 Coevolution
In biology, coevolution is the mutual evolutionary inÔ¨Çuence between two species.
There are many types of coevolutions, and the two most famous are cooperative and
competitive coevolution.
An example of cooperative coevolution is the relationship between bees and
Ô¨Çowers. Bees get food from Ô¨Çowers while Ô¨Çowers get propagation of their seeds
from bees. In optimization, when the dimension of the problem is very large, the
chromosomes are very long. The search ability of our algorithm might not be strong
enough to handle it. If we can decompose the full solution into different parts ade-
quately, effective searches over smaller landscapes are possible [89].
Let us just discuss a simple example: designing a device whose best structure is
not clear, not to mention the best parameters under that structure. A simple structure
might require a small number of parameters and vice versa. If we use one population
whose individual is represented as the most complicated structure possible with its
parameters, we will waste a lot of computing resources in searching the hopeless
region. So we would like to decouple the population into two subpopulations, one
to represent and code the structure and the other to represent and code the param-
eter. For every individual i in each subpopulation, we combine it with k randomly
selected individuals from another subpopulation to form k full solutions to the prob-
lem. The Ô¨Åtness value of individual i could be the average (or the best) Ô¨Åtness values
among those k full solutions‚Äô Ô¨Åtness values. Other operators of cooperative coevo-
lution are the same as standard EAs. In this way, simple structures only search small
regions and complicated structures are also able to Ô¨Ånd the optimal solution [90].
An example of competitive coevolution is the relationship between wolves and
rabbits. Wolves take rabbits as food. Their relationship is competitive, which prompts
both of them to develop speed and Ô¨Çexibility. While cooperative coevolution could
be regarded as a decomposer for a complicated optimization problem, competitive
coevolution could be regarded as a best objective value estimator for complicated
learning problems, especially in a competitive environment [91, 92].
Game playing is an example. Generally we do not know the best strategy, which
is what we need to learn, unless the game is too simple. So we cannot use the best
strategy, which could be seen as the objective function in the optimization problem,
110 ŒºA + ŒºB + ŒºC = 0.

3.7 Brief Introduction to Other Topics
117
to evaluate strategy i‚Äôs Ô¨Åtness value. So we can only randomly select k individuals,
representing other strategies, to compete with i. The Ô¨Åtness value of i is proportional
to its winning rate in k games. Competitive coevolution has a self-regulating selec-
tive pressure. In the early stage, most strategies are weak, so the bad ones have a
chance to survive. With the evolving process, strategies get smarter, so the selective
pressure increases.
3.7.2 Memetic Algorithms
The general consideration for EAs‚Äô performance on a solution landscape search is
that they have strong global search ability but are relatively weak at local Ô¨Åne tuning.
So why not combine the already existing effective local search (LS) methods, such
as conjugate gradient method in differentiable problems, with EAs so that they can
complement each other? Here‚Äôs where the term hybrid EA or genetic local search
comes in. In the EA literature, we call this combination memetic algorithm (MA).
The word ‚Äúmeme,‚Äù from biology, describes a type of behavior that is passed
from one member of a group to another, not in the genes but by other means such as
people copying it.111 Memes are like LS methods in MAs.
When designing or analyzing an MA, we suggest that the following six issues
need to be considered.
1. When?
On page 77, we illustrate the general solution process for EAs. An LS
can be used in the initialization phase to improve the initial individuals‚Äô quality.
But the most frequent situation is between steps 2.2 and 2.3, i.e., rewrite step 2.3
as follows:
Step 2.3: Select new individuals to undergo LS and evaluate the Ô¨Åtness
values for all individuals.
2. Who? Which new individuals will under go LS? All? The bad ones? The good
ones? The balance of computational costs between LS and the main EAs need to
be maintained elaborately.
3. What? What kind(s) of LS methods will be used in step 2.3? We want to improve
the quality of individuals using LS. So these methods are called hill climbers.112
For a speciÔ¨Åc problem, generally there exist many of these hill climbers with
different speeds and efÔ¨Åciencies but suffers from local optimum.113
4. How long? Do we need to wait until LS methods can no longer improve or until
we stop them with some heuristic rules? This issue is also related to the balance
of computational costs.
111 Oxford Advanced Learner‚Äôs Dictionary.
112 The maximum problem is considered here.
113 We need to mention that EA operators, such as crossover, can be used in LS [93].

118
3 Advanced Evolutionary Algorithms
5. How should individuals be treated after a local search? One way is to replace
one individual, both chromosome and Ô¨Åtness value, before LS with the one after
it. This is called Lamarckian learning.114 The other way is to only use the Ô¨Åtness
value after LS to represent the individual before it without the replacement of
chromosomes. This way uses the ability to learn to represent an individual. It is
called Baldwin learning.115 Many discussions on the comparison of these two
learning strategies have been made [94].
6. How does one replace?
How does one appropriately design step 2.4 of MAs?
This will strongly affect the tradeoff between selective pressure and population
diversity.
Due to the introduction of MAs, a reconsideration on the tradeoff between ex-
ploration and exploitation is necessary because individuals after LS methods will be
improved and thus have selective advantages later. The evolving process might be
accelerated with higher chances of premature. So there are many discussions on the
balance between LS and other elements of MAs. Some of them are listed as follows:
‚àôUsing steady state EAs whose replacement method has a smaller selective pres-
sure. Sect. 3.4.1 discussed this aspect.
‚àôUsing variation operators who can preserve population diversity. Sect. 3.2.2 dis-
cussed this.
‚àôUsing large popsize, variable popsize, and a reinitialization mechanism [15].
Sect. 3.5.3 discussed this.
‚àôUsing adaptive control on the probability of undergoing LS. You may consider
that individuals with higher Ô¨Åtness values represent the hopeful search direction
so that they should have a greater chance of undergoing LS. Likewise the less
crowded individuals represent the undeveloped area so that they have a greater
chance of undergoing LS. With different ideas, adaptive control rules could be
designed with techniques introduced in Sect. 3.5.2.
‚àôUsing clustering methods and only picking one individual in each cluster to un-
dergo LS so that the balance of computational costs between LS and the main
EAs can be maintained.
‚àôUsing negative assortative mating to maintain population diversity [93, 95]. Sect.
3.5.3 discussed this.
‚àôUsing self-adaptive control for LS methods, i.e., every individual has its own LS
methods [96].
‚àôUsing a niche technique, which will be introduced in Chap. 5, to preserve popu-
lation diversity.
‚àôUsing parallel/distribution environments to maintain diversity. In some paral-
lel/distribution implementations, different subpopulations will evolve in a par-
allel way so that they can exploit different areas of the solution landscape.
114 Lamarck was a French biologist whose idea that an organism can pass on characteristics that it
acquired during its lifetime to its offspring was widely accepted before Darwin‚Äôs natural selection.
115 Baldwin was an American psychologist with the idea that offspring would tend to have an in-
creased capacity for learning new skills rather than being conÔ¨Åned to genetically coded knowledge.

3.7 Brief Introduction to Other Topics
119
The research on MAs is an interdisciplinary Ô¨Åeld that has strong ties with OR
and machine learning. Springer Press launched a new journal, Memetic Computing,
in 2009. An example of MA, suggested by Chelouah and Siarry [97], is to com-
bine Nelder‚ÄìMead‚Äôs simplex search, which is introduced in Sect. 2.4.1, with EA.
Krasnogor and Smith suggested an intensive survey on the taxonomy of MAs and
discussed important issues for designing a competent MA [98].
Looking from EAs‚Äô perspective, LS techniques could also be considered one
type of operator apart from crossover and mutation. So sometimes EA participants
consider these operators a component of EAs and do not mention MAs.
3.7.3 Hyper-heuristics
In the above subsection, MA uses a hill climber with the intention of improving
mutants. So it is an improving mechanism for EAs.
Hyper-heuristics is a relative new concept, it is a mechanism to use different
heuristics, so the whole algorithm might have remarkable search ability in many
different instances of a problem or even many types of problems.
In order to expand the robustness of the algorithm, hyper-heuristics divides itself
into two levels: upper and lower. The lower level contains heuristics, including mu-
tational heuristics, whose intention is to introduce perturbation, and a hill climber,
whose intention is to improve the quality. Generally we suppose that these heuristics
are available and readily prepared.
The upper level contains the rule(s) to schedule the lower level heuristics using
feedback information from the lower level such as the improvement of the objective
value using some heuristics and the CPU time on it, etc. In this way, the upper
level is isolated from the speciÔ¨Åc problem, which makes hyper-heuristics a robust
problem-solver.
There are two problems to be answered when designing a hyper-heuristics: the
rules of scheduling lower level heuristics, i.e., heuristic selection, and the rules to
accept the result of the heuristics, i.e., move acceptance. Various implementations
have different technical details. We will introduce the idea suggested by Cowling et
al. [99], perhaps the Ô¨Årst one, and refer interested readers to [100‚Äì102].
After generating a solution using any method, we spend some time calling Œ∂
heuristics n times, i.e., (N1,‚ãÖ‚ãÖ‚ãÖ,Nn), in a random way in order to collect the initial
performances of these heuristics. This time is called the warm-up period.116
At the end of the warm-up period, we calculate three properties for every heuris-
tics as follows:
f1 (Nj) =
m
‚àë
i=1
Œ±i‚àí1 Ii (Nj)
Ti (Nj)
(3.74)
116 Generally we want n >> Œ∂ so that we can evaluate the performance of each heuristic and their
combinations as thoroughly as possible. But too long a warm-up time is impossible. In Cowling et
al.‚Äôs implementation, the warm-up period is 1/3 of the total run time.

120
3 Advanced Evolutionary Algorithms
where Ii (Nj) and Ti (Nj) are, respectively, the performance improvement and the
CPU time of heuristic j in the recent i‚Äôs call, m is the number of recent calls of
heuristic i, and 0 < Œ± < 1. Equation 3.74 means that we want greater performance
with smaller CPU time from heuristic j. We count the recent m calls for heuristics
and put more focus on the recent calls with weight Œ±i‚àí1. A larger f1 (Nj) means a
better heuristic j.
f2 (Nj,Nk) =
l
‚àë
i=1
Œ≤ i‚àí1 Ii (Nj,Nk)
Ti (Nj,Nk)
(3.75)
where Ii (Nj,Nk) and Ti (Nj,Nk) are, respectively, the performance improvement and
the CPU time of the calls in which heuristic k is called immediately after heuristic j
in the recent i‚Äôs call, l is the number of the recent calls in which heuristic k is imme-
diately after heuristic j, and 0 < Œ≤ < 1. We need to calculate f2 for every pair and
also put more focus on the recent calls. A larger f2 (Nj) means better combinations
of heuristics j and k.
f3 (Nj) = œÑ (Nj)
(3.76)
where œÑ (Nj) is the CPU time that has elapsed since heuristic j was last called.
Then the hyper-heuristics starts. We calculate the overall performance of heuris-
tic k as follows:
F (Nk) = Œ± f1 (Nk)+Œ≤ f2 (Nj,Nk)+Œ¥ f3 (Nk)
(3.77)
where Œ¥ is a control parameter and Nj is the last heuristic in the warm-up period.
Then RWS can be used to select one heuristic for the next call.117
After the selection, the winner, heuristic k, is called. Its Ô¨Åtness values is modiÔ¨Åed
according to the following equations.
f
‚Ä≤
1 (Nk) = I (Nk)
T (Nk) +Œ± f1 (Nk)
(3.78)
where I (Nk) and T (Nk) represent the performance improvement and CUP time of
this call, respectively.
f ‚Ä≤
2 (Nk) = I (Nj,Nk)
T (Nj,Nk) +Œ≤ f2 (Nk)
(3.79)
where heuristic j is heuristic k‚Äôs immediately preceding call.
We also count the time for every heuristic since its last call, i.e., f3 (Nk).
Then we can use Eq. 3.77 to get the Ô¨Åtness for all heuristics and use RWS to pick
another heuristic and continue the hyper-heuristics until the predeÔ¨Åned stop criteria
are satisÔ¨Åed.118
117 Cowling et al. suggested the technique to implement RWS when the overall performance of
some heuristics is smaller than zero.
118 According to Eq. 3.77, we know that larger f3 (Nk) means higher probability of being called in
the next run, which embodies the idea that even though a heuristic might be bad currently, it still

3.7 Brief Introduction to Other Topics
121
The results of sensitivity analysis based on numerical experiments done by Cowl-
ing et al. showed that hyper-heuristics is insensitive either to the value of the strategy
parameters, e.g., Œ±, Œ≤, Œ¥ etc., or to problem instance and problem types (so long as
the appropriate low-level heuristics are available).
EAs can certainly be used in the upper level of hyper-heuristics [103] and hyper-
heuristics can be embedded in the LS of MAs to make them more effective, i.e.,
meta-Lamarckian learning [104].
3.7.4 Handling Uncertain Environments
Up till now, we have been discussing optimization and learning problems in an ideal
environment, i.e., there is no sampling noise, no fabrication errors, and the problem
has been formulated in a not-so-difÔ¨Åcult explicit expression and will never change.
These assumptions would never be valid in a real-world situation. So the last, but
not least, topic in this chapter is the uncertain environment and how EAs can handle
it. Jin and Branke published a comprehensive survey in 2005, and we adopt their
taxonomy [105].
Noise
We consider the situation where the evaluation process of EAs suffers from additive
noise, i.e., we can only get the contaminated Ô¨Åtness values for individuals. Whenever
we need to sample from the real world, there will be various additive noises. Gen-
erally we assume that the additive noise is normally distributed. For individual x,
its real Ô¨Åtness value is f (x) and the contaminated Ô¨Åtness value is F (x) = f (x)+Œæ,
where Œæ ‚àºN(0,œÉ2). We need to work on F (x) and try to Ô¨Ånd the real optimal so-
lution for f (x). The essence behind all possible solutions is that the mean of the
contaminated Ô¨Åtness value is the real Ô¨Åtness value, i.e.,
‚à´+‚àû
‚àí‚àû[ f (x)+Œæ] p(Œæ)dŒæ = f (x)
(3.80)
where p(Œæ) is the density function of Œæ. So the simplest way is to make multiple
samplings for one solution and use the average contaminated Ô¨Åtness value as the
real Ô¨Åtness value, i.e.,
f (x) ‚âà1
n
n
‚àë
i=1
Fi (x)
(3.81)
has the chance to be called later because of its increasing f3. This property might be helpful in the
condition that different heuristics are adequate in different search stages.

122
3 Advanced Evolutionary Algorithms
The method of average sum could be regarded as a kind of Ô¨Åltering technique.
There exist many other advanced Ô¨Åltering techniques that can be utilized in treating
noise in EAs.
Robustness
Robustness refers to the fact that a variable suffers from additive noise. It is very
normal in manufacturing. Every element of a device contains tolerances, i.e., the
same exact global optimal solution x we got from EAs could never be implemented
with the same exact value in real conditions. We can only implement x + Œæ, where
Œæ ‚àºN(0,œÉ2).
Even though the global optimal solution is very good, if the Ô¨Åtness values close
to it decrease rapidly,119 we do not like it because there might be a low-quality
production due to the fabrication tolerance and ‚Äúsharp‚Äù nature around the global
optimal solution.
In this situation, we generally want EAs to Ô¨Ånd a good and robust solution, which
means that the Ô¨Åtness value of it is high and the Ô¨Åtness values close to it are also
high. This brings up another closely related term, sensitivity analysis. We want our
best solution to be insensitive to fabrication tolerance.
The basic solution for this problem is to add Œæ ‚àºN(0,œÉ2) artiÔ¨Åcially to x when
we are optimizing the problem. For every individual x, we use the average sum of
the Ô¨Åtness values of n samplings of x+Œæ as its Ô¨Åtness value:
f (x) ‚âà1
n
n
‚àë
i=1
f (x+Œæi)
(3.82)
Equations 3.81 and 3.82 look similar but are different in nature. The random
number in Eq. 3.81 is inevitable and we want to cancel its effects as far as possible
and Ô¨Ånd the global optimal solution. By contrast, the random number in Eq. 3.82 is
added artiÔ¨Åcially, and we want to Ô¨Ånd a robust good solution instead of the global
optimal solution.
Fitness Approximation
If the model of the optimization problem comes from mechanics, electromagnetics,
and thermodynamics, generally its Ô¨Åtness function cannot be expressed in an explicit
form due to the partial differential equations behind the model. It is just an example
of a real condition we often face, i.e., the Ô¨Åtness value is very ‚Äúexpensive‚Äù to evaluate
so that we need some approximation techniques to generate a simpliÔ¨Åed model and
use the value from the simpliÔ¨Åed model as the Ô¨Åtness value of the real model.
119 We consider the maximum problem here.

3.8 Summary
123
So there inevitably exist estimation errors. For solution x, suppose its real Ô¨Åtness
value is f(x); we will use F(x) = f(x)+Œµ(x) to do the optimization, where Œµ(x) is
the estimation error.120
The general consideration for the Ô¨Åtness approximation is to use better models
and adaptively modify the model parameters during the optimization so that we
can depend on the approximation model more and more and reduce the cost of
evaluation with acceptable errors. Many techniques have been suggested to handle
the Ô¨Åtness approximation, including the topics we just introduced in this section,
i.e., MA-based approximation [106] and coevolution-based approximation [107].
Time-varying Fitness Functions
Suppose you want to solve an optimization problem. The variable is the rotation an-
gles of a photoelectric conversion plate, which can transfer solar power into electric
power, and the objective is to maximize the generated electric power. For a spe-
ciÔ¨Åc time, e.g., 9:00 AM, there is an optimal rotation angle to make the plating face
east. But the bad thing is that the Sun moves, which means the optimal solution
changes and the optimal value also changes with time. Then the function is called a
time-varying Ô¨Åtness function and the problem is called a dynamic or nonstationary
problem.
We can certainly do the optimization every hour and get the optimal angle again.
The feasibility behind this technique is that the Sun moves slowly and we compute
fast so that we can afford the total reoptimization. In many other dynamic optimiza-
tion problems with more rigorous online requirements, we have no time to totally
reoptimize.
In these situations, we hope that the change in the Ô¨Åtness function is not so violent
that previous good solutions could be used to accelerate the optimization of the new
function.
So the perseverance of population diversity is the critical consideration in opti-
mizing dynamic functions. Many techniques discussed in Sect. 3.7.2 can be used.
Apart from that, we can use redundant encodings [108], diploid [109], or evolvabil-
ity [108].
3.8 Summary
This is a rather long, perhaps tough, chapter.121 Many of the key elements for solv-
ing optimization and learning problems have been discussed. Suppose the problem
you are facing is the raw material and the programming environment is the kitchen.
We hope these topics provide enough Ô¨Çavoring to cook your own gourmet meal.
120 We write the expression in this way only with the intention of emphasizing the existence of the
estimation errors. The real F(x) is calculated using the simpliÔ¨Åed estimated model.
121 But we do not want to make it tedious.

124
3 Advanced Evolutionary Algorithms
Different types of problems may require different adequate code scheme and
related variation operators. We only brieÔ¨Çy discuss the binary code, put more focus
on the real code, and will discuss permutation code and tree code in the following
chapters. In real-world problems, an efÔ¨Åcient code scheme, i.e., how to encode and
decode quickly in a not-so-large operation domain, always needs to be considered
as the Ô¨Årst priority.
Variation operators explore and exploit the solution landscape, together with the
selection process, constitute the search ability of an algorithm, and maintain the
tradeoff between selective pressure and population diversity, which might be the
key element after the code scheme has been determined.
For the above two reasons, Sects. 3.2‚Äì3.4 are basic requirements for EA design-
ers. As to the speciÔ¨Åc crossover, mutation, and selection method, we just provided
the Ô¨Çavoring and you can select them according to your problem requirement and
preference.
One type of problem, perhaps one instance of a certain problem, has its own
solution landscape property, and thus requires a different search ability in different
evolving stages. So a strategy parameter control, whether deterministic, adaptive, or
self-adaptive, should be considered as a practical requirement for a robust problem
solver. We hope that the techniques discussed in Sect. 3.5 will inspire you to develop
innovative ideas for your algorithm.
Then you need to demonstrate that your innovative ideas really work, so numer-
ical experiments are absolutely necessary.122 Due to their stochastic nature, EAs
may provide different results for different runs. So the experiment design and re-
sults analysis will directly inÔ¨Çuence the conclusions. We hope that readers can draw
rigorous conclusions using methods introduced in Sect. 3.6.
After reading this chapter, you should skillfully grasp several real code varia-
tion operators, understand the selective pressure of different parental selections and
survival selections, comprehend the advantages and disadvantages of various strat-
egy parameter control methods, and know how to compare different operators and
algorithms using numerical experiments.
In all, designing an EA is the art of tradeoff between selective pressure and pop-
ulation diversity.
Suggestions for Further Reading
Eiben and Smiths‚Äô textbook [37] and Haupt and Haupt‚Äôs textbooks [110] give clear
discussions on the basic contents of this chapter. More detailed information on re-
lated topics can be found in B¬®ack et al.‚Äôs textbooks [92, 111].
This book does not discuss the theoretical part of EAs, such as building block
hypothesis, schema theorem, Markov chain analysis, linkage, epistasis, deceptive
problems, etc. Interested readers are referred to [37, 112]. There is a biannual inter-
122 Unless in the rare case, you can prove the absolute advantage of one way over others.

3.8 Summary
125
national workshop on the foundations of genetic algorithms, and recent proceedings
are of great value for this topic [113, 114].
The real code crossover operators discussed in this chapter, such as SPX,SBX,
PCX, UNDX, and UNDX-m, do not consider the quality of parents. Nakanishi et
al. use biased probability distribution functions to direct the search with preference
on the best individual in the current population [115].
We have discussed many crossover operators. Each of them has a different search
ability on different problems. If we are dealing with a blind problem, i.e., we know
almost nothing about the solution space, it is an interesting idea to verify the synergy
effects of various crossover operators. Yoon and Moon discuss this issue in a paper
published in 2002 [116]. Even though there was still no selection rule for speciÔ¨Åc
crossover operators or a combination of operators, they observed strong synergy
effects.
We introduced rank as a way to scale Ô¨Åtness values and adjust selective pressure
in Sect. 3.3.4, but Cervantes and Stephens use ranks to adjust pm by granting Ô¨Åtter
individuals a smaller pm and to promote mating restriction by allowing individuals
with similar ranks to cross over [117].
If readers have a strong interest in adaptive and self-adaptive parameter con-
trol,123 a book edited by Lobo et al. in 2007 [118] and one written by Kramer in
2008 [119] are suggested.
The selection process has a very strong inÔ¨Çuence on the evolving process, so
designing an adequate self-adaptive selection scheme is a rather elaborate task. In-
terested readers are referred to a paper published in 2006 [120].
The orthogonal experiment design introduced in Sect. 3.6.2 is quite useful in
analyzing the sensitivity of control parameters. Leung et al. developed the idea of
orthogonal design into a crossover operation and initialization to search the solution
landscape in a statistically sound manner [121, 122].
Yuen and Chow used a binary space partitioning tree to generate and query all
the searched points in a solution space so that their algorithm will never revisit the
searched points, which improves the search efÔ¨Åciency signiÔ¨Åcantly [123].
Lobo and Goldberg suggested an empirical way to measure the difÔ¨Åculty of a
given benchmark problem [124]. Their paper is recommended to those with an in-
terest in evaluating problems, not algorithms.
We did not discuss two very interesting and useful topics ‚Äì evolvable hardware
and parallel evolutionary algorithm.
Generally we design an equipment, implement it, and use it. We do not know in
advance the possible change of equipment functions due to environmental changes
or the failure of its inner parts. A robust design and dynamic problem handling tech-
niques may relieve the pain. But we have more powerful tools ‚Äì evolvable hardware.
Evolvable hardware uses Ô¨Åeld-programmable gate array (FPGA) or other Ô¨Åeld-
programmable devices to realize an equipment. There are environmental change
monitors and inner part failure monitors in the equipment so that whenever the
change or failure happens, the EAs in the equipment can automatically solve the
123 We guess more than 50% readers have.

126
3 Advanced Evolutionary Algorithms
new optimal design problem and use the remnant gates in FPGA to realize the new
functions. Then the new part replaces the failed one. Interested readers are referred
to [125‚Äì127]; Springer Press publishes a related journal: Genetic Programming and
Evolvable Machines; and IEEE hosts an annual conference called the IEEE Work-
shop on Evolvable and Adaptive Hardware.
EAs use populations to implement optimization and learning, which means there
must be a lot of samplings on the solution landscape. So the ideas of using paral-
lel/distributed computing infrastructure to implement EAs and designing the EAs
suitable for parallel/distributed computing environment are quite attractive. Inter-
ested readers are referred to [128].
The must-read papers of this chapter are [7] for SBX, [12] for UNDX, [32] for
strategy parameter control,124 [64] for dynamic popsize control (but mostly for writ-
ing a short yet informative paper), [63, 93] for strategy parameter sensitivity anal-
ysis, [80] for performance evaluation, and [62] for an example of using hypothesis
testing to draw statistical conclusions.
Exercises and Potential Research Projects
3.1. Why are single-point crossover, multiple-point crossover, and uniform crossover
not suitable for real code? (Hint: consider the search ability of these crossovers on
real code space.)
3.2. Prove or verify that Eqs. 3.5, 3.23, 3.25, and 3.46 are probability density func-
tions.
3.3. Implement BLX-0.5 and simplex crossover in your programming environment.
3.4. Implement SBX and UNDX crossover in your programming environment, use
the benchmark problems in Appendix and the techniques introduced in Sect. 3.6 to
do a fair comparison and draw statistical conclusions according to your numerical
experiments.
3.5. How does one generate a positive deÔ¨Ånite covariance matrix using rotation
transform? Present your method and verify it by MATLAB‚ìáusing the method dis-
cussed on page 57.
3.6. Implement nonuniform mutation and one kind of self-adaptive control normal
mutation in your programming environment, and use the benchmark problems in
Appendix and the techniques introduced in Sect. 3.6 to do a fair comparison and
draw statistical conclusions according to your numerical experiments.
3.7. Find the maximum selective error of RWS and SUS.
124 This paper won the IEEE Transactions on Evolutionary Computation Outstanding Paper Award.

References
127
3.8. How do we do Ô¨Åtness transferral if we want to maximize a function with nega-
tive objective value?
3.9. Why do we say that the probability of being selected decreases from q to
2
popsize ‚àíq linearly in ranking by Eq. 3.45?
3.10. Implement SUS and binary tournament selection in your programming envi-
ronment.
3.11. Implement at least one method of Ô¨Åtness scaling and one method of ranking in
your programming environment, and use the benchmark problems in Appendix and
the techniques introduced in Sect. 3.6 to do a fair comparison and draw statistical
conclusions according to your numerical experiments.
3.12. Analyze parameter c‚Äôs inÔ¨Çuence on the selective pressure in sigma truncation.
3.13. Select and implement at least one method of deterministic, adaptive, and self-
adaptive control over pm or pc using the benchmark problems given in Appendix.
Comparisons should be made using statistical methods suggested in Sect. 3.6.
3.14. Why for minimum optimization is Eq. 3.57 also used to estimate the gradient?
Explain it with sentences similar to those in the textbook. Do we need to change the
trial point generation, Eq. 3.59, and Eq. 3.60 for a minimum optimization problem?
If so, how?
3.15. Start from any adaptive strategy parameter control method and try to use tech-
niques introduced in this chapter to improve it. Report your basic idea, your imple-
mentation details, your numerical experimental results on the benchmark problems
in Appendix, and the statistical conclusions using methods introduced in Sect. 3.6.
3.16. Different variables might have different inÔ¨Çuences on the objective, i.e., the
sensitivity of different variables might vary. Can you suggest a way to adaptively
Ô¨Ånd the most sensitive variables and focus the search on these variables? Report your
basic idea, your implementation details, your numerical experimental results on the
benchmark problems in Appendix, and the statistical conclusions using methods
introduced in Sect. 3.6.
References
1. Blum C, Roli A (2003) Metaheuristics in combinatorial optimization: overview and concep-
tual comparison. ACM Comput Surv 35(3):268‚Äì308
2. Kita H, Ono I, Kobayashi S (1999) Multi-parental extension of the unimodal normal distri-
bution crossover for real-coded genetic algorithms. In: Proceedings of the IEEE congress on
evolutionary computation, p 1588
3. Deb K, Beyer H (2001) Self-adaptive genetic algorithms with simulated binary crossover.
Evol Comput 9(2):197‚Äì221

128
3 Advanced Evolutionary Algorithms
4. Salomon R (1996) Reevaluating genetic algorithm performance under coordinate rotation of
benchmark functions. BIOSYSTEMS 39:263‚Äì278
5. Eshelman LJ, Schaffer JD (1993) Real-coded genetic algorithms and interval schemata. In:
Proceedings of foundation of genetic algorithms, pp 187‚Äì202
6. Tsutsui S, Yamamura M, Higuchi T (1999) Multi-parent recombination with simplex
crossover in real coded genetic algorithms. In: Proceedings of the genetic and evolution-
ary computation conference, pp 657‚Äì664
7. Deb K, Agrawal R (1995) Simulated binary crossover for continuous search space. Complex
Syst 9:115‚Äì148
8. Papoulis A, Pillai S (2002) Probability, random variables and stochastic processes, 4th edn.
McGraw-Hill Higher Education, New York
9. Ross S (2009) A Ô¨Årst course in probability, 8th edn. Prentice Hall, Englewood Cliffs, NJ
10. Deb K, Joshi D, Anand A (2002) Real-coded evolutionary algorithms with parent-centric
recombination. In: Proceedings of the IEEE congress on evolutionary computation, 1:61‚Äì66
11. Ono I, Tatsuzawa Y, Kobayashi S (1997) A real-coded genetic algorithm for function opti-
mization using unimodal normal distribution crossover. In: Proceedings of the 7th interna-
tional conference on genetic algorithms, pp 81‚Äì84
12. Kita H (2001) A comparison study of self-adaptation in evolution strategies and real-coded
genetic algorithms. Evol Comput 9(2):223‚Äì241
13. Ono I, Kita H, Kobayashi S (2003) A real-coded genetic algorithm using the unimodal nor-
mal distribution crossover. In: Ghosh A and Tsutsui S (eds) Advances in evolutionary com-
puting: theory and applications. Springer, Berlin Heidelberg New York, pp 213‚Äì237
14. Kirkpatrick S, Gelatt CD, Vecchi MP (1983) Optimization by simulated annealing. Science
220(4598):671‚Äì680
15. Fukunaga A (1998) Restart scheduling for genetic algorithms. In: Proceedings of the inter-
national conference on parallel problem solving from nature, pp 357‚Äì366
16. Yao X, Liu Y, Lin G (1999) Evolutionary programming made faster. IEEE Trans Evol Com-
put 3(2):82‚Äì102
17. Lee C, Yao X (2004) Evolutionary programming using mutations based on the levy proba-
bility distribution. IEEE Trans Evol Comput 8(1):1‚Äì13
18. Deb K, Goyal M (1996) A combined genetic adaptive search (GeneAS) for engineering
design. Comput Sci Informat 26:30‚Äì45
19. Goldberg D, Deb K, Korb B (1989) Messy genetic algorithms: motivation, analysis, and Ô¨Årst
results. Complex Syst 3:493‚Äì530
20. Harvey I, Harvey I (1992) The SAGA cross: the mechanics of recombination for species
with variable-length genotypes. In: Proceedings of the international conference on parallel
problem solving from nature, pp 269‚Äì278
21. Hutt B, Warwick K (2007) Synapsing variable-length crossover: meaningful crossover for
variable-length genomes. IEEE Trans Evol Comput 11(1):118‚Äì131
22. Ross SM (2006) Introduction to probability models, 9th edn. Elsevier, Singapore
23. Baker JE (1987) Reducing bias and inefÔ¨Åciency in the selection algorithm. In: Proceedings
of the 2nd international conference on genetic algorithms on genetic algorithms and their
application, pp 14‚Äì21
24. Sokolov A, Whitley D (2005) Unbiased tournament selection. In: Proceedings of the confer-
ence on genetic and evolutionary computation, pp 1131‚Äì1138
25. Sokolov A, Whitley D, Barreto AMS (2007) A note on the variance of rank-based selection
strategies for genetic algorithms and genetic programming. Genet Programm Evolv Mach
8(3):221‚Äì237
26. Goldberg D (1990) A note on Boltzmann tournament selection for genetic algorithms and
population-oriented simulated annealing. Complex Syst 4(4):445‚Äì460
27. Vavak F, Fogarty T (1996) Comparison of steady state and generational genetic algorithms
for use in nonstationary environments. In: Proceedings of the IEEE international conference
on evolutionary computation, pp 192‚Äì195
28. Jong KAD, De KA, Sarma J (1992) Generation gaps revisited. In: Proceedings of interna-
tional workshop on foundations of genetic algorithms, pp 19‚Äì28

References
129
29. Smith J, Vavak F (1998) Replacement strategies in steady state genetic algorithms: Static en-
vironments. In: Proceedings of international workshop on foundations of genetic algorithms,
pp 219‚Äì234
30. Smith J (2007) On replacement strategies in steady state evolutionary algorithms. Evol Com-
put 15(1):29‚Äì59
31. Harvey I (2001) ArtiÔ¨Åcial evolution: a continuing SAGA. In Gomi T (ed) Evolutionary
robotics: from intelligent robotics to artiÔ¨Åcial life. Springer, Berlin Heidelberg New York, pp
94‚Äì109
32. Eiben A, Hinterding R, Michalewicz Z (1999) Parameter control in evolutionary algorithms.
IEEE Trans Evol Comput 3(2):124‚Äì141
33. de Jong KAD (1975) An analysis of the behavior of a class of genetic adaptive systems.
Ph.D. thesis, University of Michigan
34. Grefenstette J (1986) Optimization of control parameters for genetic algorithms. IEEE Trans
Sys, Man Cybern 16(1):122‚Äì128
35. Rechenberg I (1973) Evolutionsstrategie: Optimierung technischer Systeme nach Prinzipien
der biologischen Evolution. Fromman-Hozlboog, Stuttgart
36. B¬®ack T (1996) Evolutionary algorithms in theory and practice: evolution strategies, evolu-
tionary programming, genetic algorithms. Oxford University Press, Oxford, UK
37. Eiben AE, Smith JE (2003) Introduction to evolutionary computing. Springer, Berlin Hei-
delberg New York
38. Schwefel H (1995) Evolution and optimum seeking. Wiley-Interscience, New York
39. Wang R, Okazaki K (2007) An improved genetic algorithm with conditional genetic opera-
tors and its application to set-covering problem. Soft Comput 11(7):687‚Äì694
40. Fernandes C, Rosa AC (2008) Self-adjusting the intensity of assortative mating in genetic
algorithms. Soft Comput 12(10):955‚Äì979
41. Zimmermann H (2001) Fuzzy set theory and its applications, 4th edn. Springer, Berlin Hei-
delberg New York
42. Yun Y, Gen M (2003) Adaptive hybrid genetic algorithm with fuzzy logic controller. In:
Verdegay J(ed) Fuzzy sets based heuristics for optimization. Springer, Berlin Heidelberg
New York, pp 251‚Äì263
43. Lin L, Gen M (2009) Auto-tuning strategy for evolutionary algorithms: balancing between
exploration and exploitation. Soft Comput 13(2):157‚Äì168
44. Zhang J, Chung HS, Lo W (2007) Clustering-based adaptive crossover and mutation proba-
bilities for genetic algorithms. IEEE Trans Evol Comput 11(3):326‚Äì335
45. Xu R, Wunsch D (2008) Clustering. Wiley-IEEE, New York
46. Xu R, Wunsch D (2005) Survey of clustering algorithms.
IEEE Trans Neural Netw
16(3):645‚Äì678
47. Duda RO, Hart PE, Stork DG (2000) Pattern classiÔ¨Åcation, 2nd edn. Wiley-Interscience,
New York
48. Bishop CM (2007) Pattern recognition and machine learning. Springer, Berlin Heidelberg
New York
49. Alpaydin E (2004) Introduction to machine learning. MIT Press, Cambridge, MA
50. Qin A, Suganthan P (2005) Self-adaptive differential evolution algorithm for numerical op-
timization. In: Proceedings of the congress on evolutionary computation, 2:1785‚Äì1791
51. Qin A, Huang V, Suganthan P (2009) Differential evolution algorithm with strategy adapta-
tion for global numerical optimization. IEEE Trans Evol Comput 13(2):398‚Äì417
52. Salomon R (1998) Evolutionary algorithms and gradient search: similarities and differences.
IEEE Trans Evol Comput 2(2):45‚Äì55
53. Arnold D, Salomon R (2007) Evolutionary gradient search revisited. IEEE Trans Evol Com-
put 11(4):480‚Äì495
54. Hansen N, Ostermeier A (2001) Completely derandomized self-adaptation in evolution
strategies. Evol Comput 9(2):159‚Äì195
55. Nocedal J, Wright S (2006) Numerical optimization, 2nd edn. Springer, Berlin Heidelberg
New York

130
3 Advanced Evolutionary Algorithms
56. Hansen N, Mller SD, Koumoutsakos P (2003) Reducing the time complexity of the deran-
domized evolution strategy with covariance matrix adaptation (CMA-ES). Evol Comput
11(1):1‚Äì18
57. Pelikan M (2005) Hierarchical Bayesian optimization algorithm: toward a new generation of
evolutionary algorithms. Springer, Berlin Heidelberg New York
58. Larranaga P, Lozano JA (2002) Estimation of distribution algorithms: a new tool for evolu-
tionary computation. Springer, Berlin Heidelberg New York
59. Lozano JA, Larraaga P, Inza I et al (2006) Towards a new evolutionary computation: advances
on estimation of distribution algorithms. Springer, Berlin Heidelberg New York
60. Kern S, M¬®uller SD, Hansen N et al (2004) Learning probability distributions in continuous
evolutionary algorithms - a comparative review. Nat Comput 3(3):355‚Äì356
61. Zlochin M, Birattari M, Meuleau N et al (2004) Model-based search for combinatorial opti-
mization: a critical survey. Ann Operat Res 131(1):373‚Äì395
62. Brest J, Greiner S, Boskovic B et al (2006) Self-Adapting control parameters in differential
evolution: a comparative study on numerical benchmark problems. IEEE Trans Evol Comput
10(6):646‚Äì657
63. Koumousis V, Katsaras C (2006) A saw-tooth genetic algorithm combining the effects of
variable population size and reinitialization to enhance performance. IEEE Trans Evol Com-
put 10(1):19‚Äì28
64. Arabas J, Michalewicz Z, Mulawka J (1994) GAVaPS-a genetic algorithm with varying pop-
ulation size.
In: Proceedings of the Ô¨Årst IEEE conference on evolutionary computation,
1:73‚Äì78
65. B¬®ack T, Eiben AE, van der Vaart NAL (2000) An empirical study on GAs ‚Äúwithout‚Äù parame-
ters. In: Proceedings of the international conference on parallel problem solving from nature,
pp 315‚Äì324
66. Lobo FG, Lima CF (2006) Revisiting evolutionary algorithms with on-the-Ô¨Çy population size
adjustment. In: Proceedings of the annual conference on genetic and evolutionary computa-
tion, pp 1241‚Äì1248
67. Fernandes C, Rosa A (2006) Self-regulated population size in evolutionary algorithms. In:
Proceedings of the international conference on parallel problem solving from nature, pp 920‚Äì
929
68. Harik GR, Lobo FG (1999) A parameter-less genetic algorithm.
In: Proceedings of the
genetic and evolutionary computation conference, pp 258‚Äì265
69. Eiben AE, Marchiori E, Valk VA (2004) Evolutionary algorithms with on-the-Ô¨Çy population
size adjustment. In: Proceedings of the international conference on parallel problem solving
from nature, pp 41‚Äì50
70. Lobo FG, Lima CF (2005) A review of adaptive population sizing schemes in genetic algo-
rithms. In: Proceedings of the 2005 workshops on genetic and evolutionary computation, pp
228‚Äì234
71. Teo J (2006) Exploring dynamic self-adaptive populations in differential evolution. Soft
Comput 10(8):673‚Äì686
72. Brest J, Maucec MS (2008) Population size reduction for the differential evolution algorithm.
Appl Intell 29(3):228‚Äì247
73. Teng N, Teo J, Hijazi M (2009) Self-adaptive population sizing for a tune-free differential
evolution. Soft Comput 13(7):709‚Äì724
74. Wolpert D, Macready W (1997) No free lunch theorems for optimization. IEEE Trans Evol
Comput 1(1):67‚Äì82
75. Whitley D, Watson J (2005) Complexity theory and the no free lunch theorem. In: Burke
EK and Kendall G (eds) Search methodologies. Springer, Berlin Heidelberg New York, pp
317‚Äì339
76. Spears WM (2004) Evolutionary algorithms: the role of mutation and recombination.
Springer, Berlin Heidelberg New York
77. Liang J, Suganthan P, Deb K (2005) Novel composition test functions for numerical global
optimization. In: Proceedings of the IEEE swarm intelligence symposium, pp 68‚Äì75

References
131
78. Gallagher M, Yuan B (2006) A general-purpose tunable landscape generator. IEEE Trans on
Evol Comput 10(5):590‚Äì603
79. Goldberg DE, Deb K, Horn J (1992) Massive multimodality, deception, and genetic algo-
rithms. Tech. rep., Illinois Genetic Algorithms Laboratory, UIUC
80. Barr R, Golden B, Kelly J et al (1995) Designing and reporting on computational experiments
with heuristic methods. J Heurist 1(1):9‚Äì32
81. Huband S, Hingston P, Barone L et al (2006) A review of multiobjective test problems and a
scalable test problem toolkit. IEEE Trans Evol Comput 10(5):477‚Äì506
82. Suganthan P, Hansen N, Liang J et al (2005) Problem deÔ¨Ånitions and evaluation criteria for
the CEC 2005 special session on real parameter optimization. Tech. rep., Nanyang Techno-
logical University and IIT Kanpur, Singapore
83. Fogel DB, Beyer H (1995) A note on the empirical evaluation of intermediate recombination.
Evol Comput 3(4):491‚Äì495
84. Mcgill R, Tukey J, Larsen W (1978) Variations of box plots. Am Statistic 32(1):12‚Äì16
85. Marques JP (2007) Applied statistics using SPSS, STATISTICA, MATLAB and R, 2nd edn.
Springer, Berlin Heidelberg New York
86. Montgomery DC (2004) Design and analysis of experiments, 6th edn. Wiley, New York
87. Box GEP, Hunter JS, Hunter WG (2005) Statistics for experimenters: design, innovation, and
discovery, 2nd edn. Wiley-Interscience, New York
88. Kennedy J (2003) Bare bones particle swarms. In: Proceedings of the 2003 IEEE swarm
intelligence symposium, pp 80‚Äì87
89. Potter M, Jong KD (1994) A cooperative coevolutionary approach to function optimization.
In: Proceedings of the international conference on parallel problem solving from nature, pp
249‚Äì257
90. Yu Y, Xinjie Y (2007) Cooperative coevolutionary genetic algorithm for digital IIR Ô¨Ålter
design. IEEE Trans Ind Electron 54(3):1311‚Äì1318
91. Angeline PJ, Pollack JB (1993) Competitive environments evolve better solutions for com-
plex tasks. In: Proceedings of the 5th international conference on genetic algorithms, pp
264‚Äì270
92. B¬®ack T, Fogel D, Michalewicz Z (2000) Evolutionary computation 2: advanced algorithms
and operations. Taylor & Francis, London, UK
93. Lozano M, Herrera F, Krasnogor N et al (2004) Real-coded memetic algorithms with
crossover hill-climbing. Evol Comput 12(3):273‚Äì302
94. Whitley LD, Gordon VS, Mathias KE (1994) Lamarckian evolution, the Baldwin effect and
function optimization. In: Proceedings of the international conference on parallel problem
solving from nature, pp 6‚Äì15
95. Garc≈Ça-Mart≈Çnez C, Lozano M (2008) Local search based on genetic algorithms. In: Rozen-
berg G (ed) Advances in metaheuristics for hard optimization. Springer, Berlin Heidelberg
New York, pp 199‚Äì221
96. Krasnogor N, Gustafson S (2004) A study on the use of self-generation in memetic algo-
rithms. Nat Comput 3(1):53‚Äì76
97. Chelouah R, Siarry P (2003) Genetic and Nelder‚ÄìMead algorithms hybridized for a more ac-
curate global optimization of continuous multiminima functions. Eur J Operat Res 148:335‚Äì
348
98. Krasnogor N, Smith J (2005) A tutorial for competent memetic algorithms: model, taxonomy,
and design issues. IEEE Trans Evol Comput 9(5):474‚Äì488
99. Cowling PI, Kendall G, Soubeiga E (2001) A hyperheuristic approach to scheduling a sales
summit. In: Selected papers from the third international conference on practice and theory
of automated timetabling, pp 176‚Äì190
100. Burke E, Kendall G, Newall J et al (2003) Hyper-heuristics: an emerging direction in mod-
ern search technology. In: Glover FW, Kochenberger GA (eds) Handbook of metaheuristics.
Springer, Berlin Heidelberg New York, pp 457‚Äì474
101. Ross P (2005) Hyper-heuristics. In: Burke EK and Kendall G (eds) Search methodologies.
Springer, Berlin Heidelberg New York, pp 529‚Äì556

132
3 Advanced Evolutionary Algorithms
102. Ozcan E, Bilgin B, Korkmaz EE (2008) A comprehensive analysis of hyper-heuristics. Intell
Data Anal 12(1):3‚Äì23
103. Cowling P, Kendall G, Han L (2002) An investigation of a hyperheuristic genetic algorithm
applied to a trainer scheduling problem. In: Proceedings of the IEEE congress on evolution-
ary computation, 2:1185‚Äì1190
104. Ong YS, Keane A (2004) Meta-Lamarckian learning in memetic algorithms. IEEE Trans
Evol Comput 8(2):99‚Äì110
105. Jin Y, Branke J (2005) Evolutionary optimization in uncertain environments-a survey. IEEE
Trans Evol Comput 9(3):303‚Äì317
106. Zhou Z, Ong YS, Lim MH et al (2007) Memetic algorithm using multi-surrogates for com-
putationally expensive optimization problems. Soft Comput 11(10):957‚Äì971
107. Schmidt MD, Lipson H (2008) Coevolution of Ô¨Åtness predictors. IEEE Trans Evol Comput
12(6):736‚Äì749
108. Choi S, Moon B (2008) Normalization for genetic algorithms with nonsynonymously redun-
dant encodings. IEEE Trans Evol Comput 12(5):604‚Äì616
109. Yang S (2006) On the design of diploid genetic algorithms for problem optimization in dy-
namic environments. In: IEEE congress on evolutionary computation, pp 1362‚Äì1369
110. Haupt RL, Haupt SE (2004) Practical genetic algorithms, 2nd edn. Wiley, New York
111. B¬®ack T, Fogel D, Michalewicz Z (2000) Evolutionary computation 1: basic algorithms and
operators. Taylor & Francis, London, UK
112. Goldberg DE (1989) Genetic algorithms in search, optimization, and machine learning.
Addison-Wesley, Reading, MA
113. Wright AH, Vose MD, Jong KAD et al (2005) Proceedings of 8th international workshop on
foundations of genetic algorithms. Springer, Berlin Heidelberg New York
114. Stephens CR, Toussaint M, Whitley D et al (2007) Proceedings of 9th international workshop
on foundations of genetic algorithms. Springer, Berlin Heidelberg New York
115. Nakanishi H, Kinjo H, Oshiro N et al (2007) Searching performance of a real-coded genetic
algorithm using biased probability distribution functions and mutation. Artif Life Robot
V11(1):37‚Äì41
116. Yoon H, Moon B (2002) An empirical study on the synergy of multiple crossover operators.
IEEE Trans Evol Comput 6(2):212‚Äì223
117. Cervantes J, Stephens C (2009) Limitations of existing mutation rate heuristics and how a
rank GA overcomes them. IEEE Trans Evol Comput 13(2):369‚Äì397
118. Lobo FG, Lima CF, Michalewicz Z (2007) Parameter setting in evolutionary algorithms.
Springer, Berlin Heidelberg New York
119. Kramer O (2008) Self-adaptive heuristics for evolutionary computation. Springer, Berlin
Heidelberg New York
120. Eiben AE, Schut MC, de Wilde AR (2006) Boosting genetic algorithms with self-adaptive
selection. In: Proceedings of IEEE congress on evolutionary computation, pp 477‚Äì482
121. Zhang Q, Leung Y (1999) An orthogonal genetic algorithm for multimedia multicast routing.
IEEE Trans Evol Comput 3(1):53‚Äì62
122. Leung Y, Wang Y (2001) An orthogonal genetic algorithm with quantization for global nu-
merical optimization. IEEE Trans Evol Comput 5(1):41‚Äì53
123. Yuen SY, Chow CK (2009) A genetic algorithm that adaptively mutates and never revisits.
IEEE Trans Evol Comput 13(2):454‚Äì472
124. Lobo F, Goldberg DE (2004) The parameter-less genetic algorithm in practice. Inf Sci 167(1-
4):217‚Äì232
125. Zebulum RS, Pacheco MA, Vellasco MMB (2001) Evolutionary electronics: automatic de-
sign of electronic circuits and systems by genetic algorithms. CRC, Boca Raton, FL
126. Gokhale M, Graham PS (2005) ReconÔ¨Ågurable computing: accelerating computation with
Ô¨Åeld-programmable gate arrays. Springer, Berlin Heidelberg New York
127. Greenwood GW, Tyrrell AM (2006) Introduction to evolvable hardware: a practical guide for
designing self-adaptive systems. Wiley-IEEE Press, New York
128. Nedjah N, Alba E, Mourelle LM (2006) Parallel evolutionary computations. Springer, Berlin
Heidelberg New York

Part II
Dealing with Complicated Problems


Chapter 4
Constrained Optimization
Abstract In previous chapters, we only searched the space deÔ¨Åned by variables‚Äô
upper and lower bounds. But real-world problems are always with constraints. One
important question that needs to be answered when applying EAs in constrained
optimization is how to evaluate a solution that violates some constraints. Generally,
we want the Ô¨Ånal results of our EAs to satisfy all the constraints. But discarding the
those that violate some constraints and generating new ones again is very inefÔ¨Åcient.
Several wonderful ideas for constraint handling will be discussed in this chapter.
4.1 Introduction
4.1.1 Constrained Optimization
The real world is constrained by various rules, so that the mathematical models
representing real-world optimization and learning problems will have various con-
straints. The general form of constrained optimization problems (COPs) can be il-
lustrated as follows:1
min f (x), x ‚àà‚Ñùn
s.t.
gi (x) ‚â§0,
i = 1,‚ãÖ‚ãÖ‚ãÖ,q
h j (x) = 0,
j = q+1,‚ãÖ‚ãÖ‚ãÖ,k
Ll ‚â§xl ‚â§Ul,
l = 1,‚ãÖ‚ãÖ‚ãÖ,n
(4.1)
where Ll and Ul are the lower and upper bounds of variable xl, respectively, which
forms the search space S.2 q inequality constraints (linear or nonlinear) and k ‚àí
q equality constraints (linear or nonlinear) need to be satisÔ¨Åed, which forms the
1 If objective or constraint function is nonlinear with x, it is also called nonlinear programming
(NLP).
2 Generally, these n inequalities are not regarded as constraints but form the deÔ¨Ånition domain.
135

136
4 Constrained Optimization
feasible region F. F ‚äÜS. If point x ‚ààF, we say x is feasible, else if point x ‚ààS‚àñF,
we say x is infeasible.3 If the purpose of OR is to Ô¨Ånd feasible solutions, i.e., the
objective is not considered, then such problems are called constrained satisfaction
problems (CSPs).
If a point x satisÔ¨Åes gi (x) = 0 for inequality constraint i, we say that constraint i
is active at point x. All equality constraints are considered to be active at feasible
region F.
Due to the possible complex form of constraints, the relationship between F and
S might be complicated. Figure 4.1 is one example.


























%

#!
#!!#&
'#!!#&
(#$)&#&
Fig. 4.1 Feasible region and search space
In Fig.4.1, F is a nonconvex disconnected set and the feasible optimal solution is
at the edge of the feasible region, which is rather far away from the optimal solution
without constraints. All of these characteristics of F add difÔ¨Åculties to COPs.
Due to the above facets and considering the No Free Lunch theorem, generally it
is impossible to develop a deterministic method for COPs that might be better than
an exhaustive search, i.e., a COP is an intractable model for OR methods.
Equality constraints h j (x) = 0 in Eq. 4.1 might be the most difÔ¨Åcult part of NLP
because they make F extremely small compared to S. So generally, for almost all
NLP solvers, we need to relax the equality constraints to inequality constraints as
follows:
		h j (x)
		 ‚â§Œ¥,
j = q+1,‚ãÖ‚ãÖ‚ãÖ,k
(4.2)
where Œ¥ is the tolerance value predeÔ¨Åned by users.4 In this way, we can transfer Eq.
4.1 into an NLP with k inequality constraints.
3 If x /‚ààS, we say x is illegal.
4 Œ¥ = 0.001 or Œ¥ = 0.0001 are commonly suggested Ô¨Åxed values. Techniques to control Œ¥ in the
evolving process will be introduced later.

4.1 Introduction
137
4.1.2 Constrained Optimization Evolutionary Algorithms
EAs can maintain a group of individuals, either feasible or infeasible, that could
promote evolution in different feasible regions, the Ô¨Ånding of new feasible regions,
and approaching a feasible optimal solution from different directions. So EAs have
some advantages in COPs especially for those whose feasible regions are discon-
nected or whose feasible optimal solution is at the edge of the feasible region, like
the example in Fig. 4.1.
EAs with special considerations for COPs are called constrained optimization
EAs (COEAs). Because we often want COEAs to have the above-mentioned explo-
ration abilities, population diversity needs to be preserved carefully.
The following points need to be emphasized when designing and analyzing a
COEA; the Ô¨Årst one is of most importance.
1. How does one compare a feasible solution with a larger objective value and an in-
feasible solution with a smaller objective value?5 This consideration often affects
the selection or replacement part of EAs.
2. Standard EAs are generally ill-suited for solving CSPs or COPs because standard
variation operators might generate infeasible offspring from feasible parents, i.e.,
these operators are ‚Äúblind‚Äù to constraints. So are there any special considerations
on the variation operators that can promote the search on the constrained solution
landscape effectively?
3. How does one control the tolerance Œ¥ in Eq. 4.2 for equality constraints?
4. How does one maintain the feasibility of individuals if variation operators gen-
erate illegal or infeasible offspring?
The taxonomy of COEAs is given in Fig 4.2.
If a COEA can always limit its search in feasible region F with special encoding
and decoding techniques, it belongs to the ‚Äúfeasibility maintenance‚Äù group. We will
introduce an example of real-code COEA in Sect. 4.2 and discuss other topics in
Chap. 7.
If users can indicate a preference between the constraint violation and objective
value, i.e., using predeÔ¨Åned or adaptive controlled weights to change COPs into opti-
mization problems without constraints, the COEA belongs to the ‚Äúpenalty function‚Äù
group. We will introduce several methods to set or control the weight parameters in
Sect. 4.3.6
COEAs generally treat objective and constraint violation separately, i.e., they
maintain elaborate balance between infeasible solutions with relatively small objec-
tive values and feasible solutions with relative large objective values, which belongs
to the ‚Äúseparation of constraint violation and objective value‚Äù group. We will intro-
duce three examples in Sect. 4.4.
5 In this chapter, we will stick to the minimum constrained problem unless otherwise speciÔ¨Åed.
6 In Chap. 3 we sometimes ignored the difference between objective value and Ô¨Åtness value because
many selection processes can handle minimum requirements directly. In this chapter, we will use
objective value to represent f (x) in Eq. 4.1 and Ô¨Åtness value to represent the weighted penalty
function value.

138
4 Constrained Optimization















Fig. 4.2 The taxonomy of COEAs
Also, we can treat k constraints as k objectives or the total constraint violation
as an objective, thus change the COPs into multiobjective problems and solve them
using multiobjective EAs, which belongs to the ‚ÄúMOEAs‚Äù group. Sect. 6.8.2 will
introduce constraint handling techniques developed from MOEAs.
Figure 4.3 illustrates the number of papers indexed by the SCI on COEAs.7. As
can be seen from Fig. 4.3, COEAs have attracted interest in recent years.
4.2 Feasibility Maintenance
4.2.1 Genetic Algorithm for Numerical Optimization of
Constrained Problems
In 1998 Michalewicz suggested GENOCOP (genetic algorithm for numerical op-
timization of constrained problems) as an example COEA of ‚Äúfeasibility mainte-
nance‚Äù [1]. GENOCOP can handle COPs with convex feasible region F. We can
illustrate GENOCOP by a simple linear constrained problem as follows:
7 TS = ((‚Äúconstrained optimization‚Äù) AND (‚Äúgenetic algorithm‚Äù OR ‚Äúgenetic algorithms‚Äù OR ‚Äúevo-
lutionary computation‚Äù OR ‚Äúevolutionary computing‚Äù OR ‚Äúevolutionary algorithms‚Äù OR ‚Äúevolu-
tionary intelligence‚Äù)). The SCI index ‚ÄúTS‚Äù is for the search topic in the title, the keywords, and
the abstract.

4.2 Feasibility Maintenance
139


	

	
	












	
	




 




	



 



Fig. 4.3 Number of papers indexed by SCI on COEAs
min f (x1,x2,x3,x4,x5,x6)
s.t.
2x1 +x2 +x3 = 6
x3 +x5 ‚àí3x6 = 10
x1 +4x4 = 3
x2 +x5 ‚â§120
‚àí40 ‚â§x1 ‚â§10,
50 ‚â§x2 ‚â§75
0 ‚â§x3 ‚â§10,
5 ‚â§x4 ‚â§15
0 ‚â§x5 ‚â§20,
‚àí5 ‚â§x6 ‚â§5
(4.3)
In Eq. 4.3, we can Ô¨Årst eliminate all the equality constraints as follows:
x1 = 3‚àí4x4
x2 = ‚àí10+8x4 +x5 ‚àí3x6
x3 = 10‚àíx5 +3x6
(4.4)
By substituting Eq. 4.4 into Eq. 4.3, we get the following NLP with only linear
inequality constraints, which constitute the convex F.
min g(x4,x5,x6) = f (3‚àí4x4,‚àí10+8x4 +x5 ‚àí3x6,10‚àíx5 +3x6,x4,x5,x6)
s.t.
‚àí10+8x4 +2x5 ‚àí3x6 ‚â§120
‚àí40 ‚â§3‚àí4x4 ‚â§10
50 ‚â§‚àí10+8x4 +x5 ‚àí3x6 ‚â§75
0 ‚â§10‚àíx5 +3x6 ‚â§10
5 ‚â§x4 ‚â§15,0 ‚â§x5 ‚â§20,‚àí5 ‚â§x6 ‚â§5
(4.5)

140
4 Constrained Optimization
Provided that we have popsize feasible individuals,8 whole arithmetic crossover,
introduced in Sect. 3.2.2, can maintain the feasibility of the offspring.
As for mutation, feasible individual (x4,x5,x6) might generate infeasible mutants
by the methods discussed in Sect. 3.2.2. Since Eq. 4.5 only has linear inequality
constraints, we can easily generate the possible mutation region for each variable.
Suppose we want to mutate (x4 = 10,x5 = 8,x6 = 2). All we need to do is Ô¨Åx two
variables and get the possible mutation region for the third variable. For example,
if we want to mutate gene x4, then x5 = 8 and x6 = 2 will not change. Substituting
x5 = 8 and x6 = 2 into the constraints of Eq. 4.5, we can get several linear inequality
constraints for x4. The possible mutation region for x4 is the intersection of the
solutions of these linear inequality equations. In this example, if x5 = 8 and x6 = 2,
then the mutation region for x4 is [7.25,10.375]; if x4 = 10 and x6 = 2, the mutation
region for x5 is [6,11]; and if x4 = 10 and x5 = 8, the mutation region for x8 is
[1,2.666].9 Then the techniques discussed in Sect. 3.2.2 can be used.
GENOCOP can only maintain the feasibility of individuals within convex F,
which limits its application. But the idea of eliminating linear equality constraints
is used by almost all COEA designers.
4.2.2 Homomorphous Mappings
In 1999 Koziel and Michalewicz further developed GENOCOP for nonconvex and
disconnected F by homomorphous mappings (HM) [2].
In abstract algebra, a homomorphism is a structure-preserving map between two
algebraic structures. If we can generate a bijective map between a point in the cube
of n-dimensional space [‚àí1,1]n and a point in the feasible region F, i.e., every point
in cube [‚àí1,1]n has an exact image in F and vice versa, and the images in F of the
close points in cube [‚àí1,1]n are close, this map an is isomorphism and cube [‚àí1,1]n
and F are isomorphic.
Then we can just evolve in the convex cube [‚àí1,1]n with real-code crossover and
mutation operators and maintain the feasibility of the population. To determine the
original image in the cube and its image in F, their relative distance to the origin are
the same, which is the structure being preserved in the map.
We will discuss HM in the 2-D situation. Higher dimensions will increase the
computational cost and complexity, but with the same idea. The Ô¨Årst example is the
HM between a rectangle in 2-D and the convex F in S as in Fig. 4.4.
For point A in Fig 4.4a, we can determine a radial line starting from the origin
and going toward A and denote this radial line as follows:
8 If the cardinality of F is small compared to S, generating popsize feasible individuals initially is
not a trivial task. Here we overlook this difÔ¨Åculty and just simplify the problem by discarding all
the randomly generated infeasible solutions until we get popsize feasible ones.
9 For the reason of simplicity, we use linear constraints. If the constraints are nonlinear but we
know that F is convex, mutation region for each variable could be found by numerical methods,
such as binary search.

4.2 Feasibility Maintenance
141









 
*
(a)
































 +
*+
(b)
Fig. 4.4 HM between the rectangle in 2-D and the convex F in S: (a) rectangle in 2-D, and (b)
convex F in S
y = y0t
(4.6)
where y0 is the coordinates of point A and t is a parameter. We can calculate the
distance between point A and the origin as L1. The radial line intersects with the
boundary of the rectangle one time at one point because of the convexity of the
rectangle. We can get the intersection point T, i.e., get the maximum value of t as
tmax =
1
max(‚à£y0
1‚à£,‚à£y0
2‚à£).10 So the distance between T and point A can be calculated as
L2. The structure being preserved by HM is
L1
L1+L2 .
For the points in F in Fig. 4.4b, we need to appoint a reference point r0 artiÔ¨Åcially
corresponding to the origin in Fig 4.4a. After that we can determine another radial
line starting from r0 and with the same direction of y0 as follows:
x = r0 +y0œÑ
(4.7)
where œÑ is a parameter. For convex F, we can Ô¨Ånd only one intersection, i.e., point
T
‚Ä≤, of the radial line and the boundary of F with numerical methods, i.e., Ô¨Ånd œÑmax.
So the distance between the intersection and the reference point can be calculated
as l1 +l2.
To keep the relative distance to the origin (the reference point), the image of
point A in Fig 4.4a, i.e., point A
‚Ä≤ in Fig 4.4b, should satisfy
L1
L1 +L2
=
l1
l1 +l2
(4.8)
So A
‚Ä≤ can be calculated using
x0 = r0 +y0 œÑmax
tmax
(4.9)
10 Why the maximum coordinate value determines the maximum t?

142
4 Constrained Optimization
where x0 is the coordinates of point A
‚Ä≤.
After we generate the initial individuals in the rectangular area of Fig 4.4a, vari-
ation operators that guarantees that the offspring will still be in the rectangle can
be used freely. The objective value for an individual can be calculated after we get
its real coordinates by Eq. 4.9. Then we can select according to the objective value.
Thus the whole EA can be implemented.
For a disconnected and nonconvex situation, such as Fig. 4.5b, we just use the
distance concept to illustrate the idea and neglect the calculation detail.




































 
*
(a)




























 +
*+
(b)
Fig. 4.5 HM between the rectangle in two-dimension and the nonconvex F in S: (a) rectangle in
2-D, and (b) nonconvex F in S
In nonconvex and disconnected F, we can calculate multiple intersections be-
tween the radial line and the feasible boundary by numerical methods.11 After cal-
culating all the intersections, we can determine the farthest intersection point T
‚Ä≤. To
keep the relative distance to the origin (the reference point) in Fig. 4.5b, point A
‚Ä≤
can be calculated by
L1
L1 +L2
=
l1 +l2 +l3
l1 +l2 +l3 +l4
(4.10)
where the deÔ¨Ånition of l1, l2, l3, and l4 can easily be understood with Fig. 4.5b.
Other considerations are the same as with convex F.
The advantages and characteristics of HM can be summarized as follows:
‚àôHM will always preserve the feasibility of a population, so there is no require-
ments for evaluating infeasible solutions.
‚àôAlthough users need to provide a reference point r0 in F before the map, theo-
retically any point in F could be selected and would not affect the map.12
11 This might be hard to do if F has a complex shape.
12 But different r0 might affect the optimization results.

4.3 Penalty Function
143
‚àôNo additional parameters need to be provided by users.
The disadvantages of HM can be listed as follows:
‚àôThe numerical calculation for Ô¨Ånding all the intersections between the radial line
and the boundary of F is hard.
‚àôBasically, HM cannot handle equality constraints.13
‚àôSometimes Ô¨Ånding the initial feasible solutions is not a trivial task.
‚àôA bad selection of r0 might yield bad results.
Another topic that needs to be mentioned is that sometimes the variation opera-
tors might generate illegal solutions, i.e., xl ‚â•Ul or Ll ‚â•xl in Eq. 4.1. Several meth-
ods can be used that are basically based on a repair idea. We need to ‚Äúrepair‚Äù the
current illegal solution so that it becomes feasible. The simplest way to repair xl ‚â•Ul
is to assign xl = Ul directly. If you know that a feasible solution is on the bound-
ary of F, this simple technique might work. Otherwise, we can reÔ¨Çect the illegal
variable by x
‚Ä≤
l = 2Ul ‚àíxl, where x
‚Ä≤
l is the repaired legal variable. This symmetrical
reÔ¨Çection method can also be controlled using an annealing idea. Similar methods
can be used with Ll ‚â•xl.14 Although we discuss illegal repairing approaches in this
section, they can also be used in other COEAs.
4.3 Penalty Function
The penalty function is the ordinary OR method for COPs and is the most often
used technique in COEAs. For the COP model illustrated by Eq. 4.1, we can deÔ¨Åne
the violation of constraint j, i.e., vj(x), as follows:15
vj(x) =
{
max
{
0,g j(x)
}
1 ‚â§j ‚â§q
		hj(x)
		
q+1 ‚â§j ‚â§k
(4.11)
If we know that the constraint violations are of the same order of magnitude or
we have already normalized them, the overall constraint violation can be generated
by either Eq. 4.12 or Eq. 4.13:16
v(x) =
k
‚àë
j=1
vj (x)
(4.12)
v(x) = max
j
(vj (x))
(4.13)
13 Why?
14 A similar discussion appears on page 63.
15 This is based on Eq. 4.1. Readers should Ô¨Ågure out the constraint violation after we have already
transformed the equality constraints into inequality ones.
16 What is the meaning of Eq. 4.13?

144
4 Constrained Optimization
Furthermore, for the overall constraint violation Eq. 4.12, we can assign weights,
denoted as penalty coefÔ¨Åcients, œâi, for each constraint violation to represent our
preference or implement the scaling. Thus the penalty function is formulated as
follows:
penalty(x) =
k
‚àë
j=1
œâjv j (x)
(4.14)
Then the Ô¨Åtness value for a solution x can be described as follows:
f
‚Ä≤ (x) = f (x)+penalty(x)
(4.15)
From Eq. 4.11 we know that the constraint violations are not less than zero, so
Eq. 4.15 increases the Ô¨Åtness values of the infeasible solutions according to the ex-
tent of their constraint violation and keep the Ô¨Åtness values of the feasible solutions
the same as their objective values.17
Equation 4.15 penalizes the infeasible solutions by adding their Ô¨Åtness values
using plus operators and do not utilize the number of constraints being violated.
Methods considering this information will be introduced later.
Thus we have already transferred the COP, Eq. 4.1, into an unconstrained opti-
mization problem, Eq. 4.15. All the techniques discussed in Chaps. 2 and 3 can be
used.
It is not difÔ¨Åcult to understand that the feasible optimal solution of Eq. 4.1 is the
global optimal solution of Eq. 4.15. So we can Ô¨Ånd the feasible optimal solution of
Eq. 4.1 by optimizing Eq. 4.15.
4.3.1 Static Penalty Function
For constraint j, if the penalty coefÔ¨Åcient œâj is predeÔ¨Åned by the user and does not
change during the evolving process, Eq. 4.14 is called a static penalty function.
The main advantage of the static penalty function is its simplicity of implemen-
tation. But this method suffers from drawbacks similar to those of parameter tuning,
which is discussed in Sect. 3.5.1, and Ô¨Åxed weights sum, which will be introduced
in Sect. 6.2.1.1. We can summarize the shortcomings of this method as follows:
‚àôBy assigning penalty coefÔ¨Åcient œâj with a preference, we tell the EAs which
constraint is more important or more difÔ¨Åcult or with a larger violation. But this
information is generally hard to acquire. Sometimes the difÔ¨Åculty of providing
adequate penalty coefÔ¨Åcients is severer than that of Ô¨Ånding a feasible optimal
solution to the original problem.
‚àôToo small penalty coefÔ¨Åcients, which might ultimately bring about infeasible
solutions, and too large penalty coefÔ¨Åcients, which might ultimately cause a local
17 We discuss minimum problems in this chapter. So smaller Ô¨Åtness values means Ô¨Åtter individuals.

4.3 Penalty Function
145
optimal solution, are both inappropriate. But we do not know how large is large
enough.
‚àôDifferent penalty coefÔ¨Åcient vectors might provide different solution landscapes
of Eq. 4.15, which might induce different treatment of EAs. But we do not know
the relationship between the penalty coefÔ¨Åcient vector and the shape of the solu-
tion landscape beforehand.
An extreme example of a static penalty function is the death penalty, which re-
jects infeasible solutions and reuses the variation operators and initialization process
to generate new individuals again. For simple COPs, the death penalty is easy but
requires more and more computational costs with the increase in the complexity of
constraints.
4.3.2 Dynamic Penalty Function
Similar to the deterministic parameter control discussed in Sect. 3.5.1, we can let the
penalty coefÔ¨Åcient increase with the evolving process. So in the early stages, a small
penalty might promote the exploration of both feasible and infeasible regions; and
in the late states, a large penalty might guide the population toward a feasible region.
There are many types of deterministic penalty coefÔ¨Åcient control schemes. We just
introduce one proposed by Joines and Houck [3]. The penalized Ô¨Åtness function can
be illustrated as follows:
f
‚Ä≤(x) = f(x)+(c√ógen)Œ±
k
‚àë
j=1
vŒ≤
j (x)
(4.16)
where Œ±, Œ≤, and c are control parameters predeÔ¨Åned by users, Œ± = Œ≤ = 2 and c =
0.5 are suggested values, and gen is the generation number that increases with the
evolving process. So Eq. 4.16 is called a dynamic static penalty.
Dynamic penalty functions, Eq. 4.16, require fewer parameters than static penalty
functions and can increase the selective pressure with the evolving process, which
is exactly what we want. But the problem is that the proper way of increasing the
selective pressure might be problem dependent. Also, if the global optimal solution
without constraints is far from the feasible optimal solution, Eq. 4.15 might take the
search in the wrong direction.
4.3.3 Adaptive Penalty Function
If we can gather information from the current population and use it in penalty coefÔ¨Å-
cient control, fewer parameters are necessary and the penalty function might thereby
acquire a problem-independent ability. This is an adaptive penalty function. We will
discuss several examples with the intent of spurring readers‚Äô creativity.

146
4 Constrained Optimization
Gen and Cheng suggested an adaptive multiplication penalty function for maxi-
mum optimization problem as follows [4]:
f
‚Ä≤ (x) = f (x)√ó
[
1‚àí1
k
k
‚àë
j=1
(
v j (x)
vmax
j
)Œ±]
(4.17)
where Œ± is the control parameter, vj (x) is the violation of constraint j deÔ¨Åned by Eq.
4.11, and vmax
j
is the maximum violation of constraint j in the current population,
i.e., vmax
j
= max
(
Œµ,max
x (v j (x))
)
, where Œµ is to avoid dividing by zero when all
individuals are feasible.
In Eq. 4.17, the objective value of an individual with a larger average constraint
violation will be decreased further. The strength of penalty depends on the current
maximum constraint violation. So we group it as an adaptive penalty function.
Hadj‚ÄìAlouane and Bean suggested another type of adaptive penalty function in
1997 [5]. It uses the same penalty coefÔ¨Åcient, Œ©, for all constraints in one genera-
tion, that is,
f
‚Ä≤ (x) = f (x)+Œ©v(x)
(4.18)
where v(x) is deÔ¨Åned by Eq. 4.12.
The penalty coefÔ¨Åcient Œ© changes according to whether the individual with the
smallest objective value is in F or not, i.e.,
Œ© (gen+1) =
‚éß
‚é®
‚é©
Œ©(gen)
Œ≤1
, if bi ‚ààF for all gen‚àíœÑ +1 ‚â§i ‚â§gen
Œ≤2Œ© (gen),
if bi /‚ààF for all gen‚àíœÑ +1 ‚â§i ‚â§gen
Œ© (gen),
otherwise
(4.19)
where Œ≤1 > Œ≤2 > 1 and œÑ are control parameters predeÔ¨Åned by users, gen is the
current generation number, and bi is the best individual, with the smallest objective
value, at generation i.
If in the previous successive œÑ generations the individuals with the smallest ob-
jective value are all feasible, we need to decrease the penalty coefÔ¨Åcient so as to
promote exploration in the infeasible region. If in the previous successive œÑ genera-
tions the individuals with the smallest objective value are all infeasible, we need to
increase the penalty coefÔ¨Åcient so as to push the population toward F. Otherwise,
the penalty coefÔ¨Åcient will be kept unchanged.
A similar idea was adopted by Hinterding for adaptive equality control handling,
Eq. 4.2, [6].18 For every œÑ generations, we would like to change the tolerance in the
following way.
18 Readers should review the ‚Äú1/5 success rule‚Äù in Sect. 3.5.1.

4.3 Penalty Function
147
if (genmodœÑ = 0) then
Œ¥ (gen) =
‚éß
‚é®
‚é©
Œ¥(gen‚àíœÑ)
c
,
p f > 25%
Œ¥ (gen‚àíœÑ)c,
p f < 15%
Œ¥ (gen‚àíœÑ),
15% ‚â§p f ‚â§25%
(4.20)
else
Œ¥ (gen) = Œ¥ (gen‚àí1)
end
where pf is the feasible percentage of the current population and c = 1.3 is the
suggested control parameter. If the current feasible percentage is large, i.e., p f >
25%, we need to decrease the tolerance Œ¥ so as to Ô¨Ånd a more precise solution. If
the current feasible percentage is small, i.e., p f < 15%, we need to increase the
tolerance Œ¥ to promote the exploration of F. Otherwise, Œ¥ will be kept unchanged.
After Œ¥ is adaptively changed, all individuals in the current generation need to be
re-evaluated with respect to their constraint violation.
4.3.3.1 Adaptive Segregational Constraint Handling Evolutionary Algorithm
Hamida and Schoenauer adopted and improved the above penalty coefÔ¨Åcient and
tolerance adaptive control method and suggested the adaptive segregational con-
straint handling evolutionary algorithm (ASCHEA) [7]. Since then ASCHEA has
gradually becoming one of the classical COEAs and most current suggested COEAs
need to compare their numerical results on benchmark problems with ASCHEA.
ASCHEA is a (Œº +Œª)-ES, which was introduced in Sect. 2.3.1. The main char-
acteristics of ASCHEA can be summarized as follows:
1. Adaptive penalty coefÔ¨Åcient control. ASCHEA is similar to Eq. 4.19, but treats
penalty coefÔ¨Åcients separately.
2. Dynamic and adaptive tolerance control. ASCHEA develops the idea of toler-
ance control, like Eq. 4.20, and considers the situation more delicately.
3. Segregational selection. ASCHEA has special considerations on parental selec-
tion with preference for feasible individuals.
4. Mating restriction. ASCHEA embodies the idea of exploring F in a negative
assortative mating way.
The penalty coefÔ¨Åcients for constraint j are adaptively controlled as follows:

148
4 Constrained Optimization
if
(
p f (j) > ptarget
)
œâj (g+1) = œâj (g)
fact
otherwise
œâj (g+1) = œâj (g) fact
(4.21)
where pf (j) is the feasible percentage of constraint j in the current population,
ptarget is the ideal feasible percentage, œâj (g) and œâj (g+1) are, respectively, the
penalty coefÔ¨Åcients of the current and next generation for constraint j, and fact > 1
is a user-deÔ¨Åned parameter.19 The analysis of Eq. 4.21 is similar to that of Eq. 4.19.
Hamida and Schoenauer suggested two kinds of tolerance control for equality
constraints: dynamic adjustment (DA) and adaptive adjustment (AA). DA is similar
to Eq. 4.20:
if
(
p f (j) > preduct
)
Œ¥j (g+1) = Œ¥j (g)
factŒ¥
otherwise
Œ¥j (g+1) = Œ¥j (g) factŒ¥
(4.22)
where p f (j) is the feasible percentage of equality constraint j in the current pop-
ulation, preduct is the ideal feasible percentage for equality constraints, Œ¥j (g) and
Œ¥j (g+1) are, respectively, the tolerances of the current and next generation for
equality constraint j, and factŒ¥ > 1 is a user-deÔ¨Åned parameter.20 The analysis of
Eq. 4.22 is similar to that of Eq. 4.20.
DA only considers the feasibility number for equality constraints in the current
population and neglects the extent of violation. So the reduction of Œ¥j might be too
fast, with large factŒ¥, so that it will contain no feasible individuals after Eq. 4.22.21
AA ensures that it will always retain pequality proportional the feasible individuals
for equality constraint j if its current feasible percentage p f (j) > preduct.22
To implement this, we need to reÔ¨Åne the relaxation of equality constraint, Eq. 4.2,
as follows:
Œ¥ ‚àí
j ‚â§h j (x) ‚â§Œ¥ +
j ,
j = q+1,‚ãÖ‚ãÖ‚ãÖ,k
(4.23)
where Œ¥ ‚àí
j and Œ¥ +
j are the negative and positive tolerance for equality constraint j,
respectively.
Then for equality constraint j, if its feasible percentage p f ( j) > preduct, then
we know not only the number of individuals on both sides of the relaxation but
also the extent of violation on equality constraint j for each of these individuals.
Suppose there are n+
F and n‚àí
F individuals on the positive and negative relaxation
sides of equality constraint j, respectively, we can rank them according to the extent
of violation on j in ascending order and then take the violation of the pequality √ón+
F th
and pequality √ón‚àí
F th individual as the tolerance in the next generation.23 A graphical
19 Hamida and Schoenauer suggested ptarget = 0.5 and fact = 1.1.
20 Hamida and Schoenauer suggested preduct = 0.6 and factŒ¥ = 1.01 for DA.
21 This explains the suggestion of such a smaller value, factŒ¥ = 1.01, by Hamida and Schoenauer.
22 DA and AA are both adaptive controls of the tolerance value. But the ‚Äúadaptive‚Äù represented by
the Ô¨Årst letter of ‚ÄúAA‚Äù is referred to as the adaptive control of factŒ¥ in DA.
23 Hamida and Schoenauer suggested preduct = 0.7 and pequality = 0.3.

4.3 Penalty Function
149
illustration for determining the positive tolerance in the next generation is in Fig.
4.6, where vi is the ith violation of the equality constraint in ascending order.
 

000000
000000

 
    










 

Fig. 4.6 Using the violation of pequality √ó n+
F th individual in the positive relaxation region to de-
termine the positive tolerance for constraint j in the next generation
If the COP has only one equality constraint, AA could always maintain a certain
number of feasible individuals. But as AA treats the equality constraints separately,
in the worst scenario, there might be no feasible individuals after AA in COPs with
multiple equality constraints.
The segregation selection in ASCHEA segregates the survivor selection process,
replacement, into two parts: feasibility-based selection and penalty-function-based
selection. First, pselect √ó Œº feasible individuals are selected from Œº + Œª individu-
als without replacement based on their objective values. If the number of feasible
individuals is less than pselect √ó Œº, just select all the feasible individuals without
replacement. Then a penalized Ô¨Åtness value is used for selection until a total of Œº
individuals are picked.24 In this way, apart from the penalty function, ASCHEA
further prefers feasible individuals.
Some COPs have their feasible optimal solutions on the boundary of feasible
region F, so in ASCHEA, if one feasible individual is selected to mate, its mating
counterpart must be an infeasible individual so as to promote the exploration of
the feasible boundary. But Hamida and Schoenauer also suggested that this mating
restriction could only be used when too few feasible individuals are in the current
population to allow the exploration of the feasible region.
These techniques ‚Äì adaptive control of the penalty coefÔ¨Åcients and the equality
tolerances, preference for feasible individuals, and mating restriction promoting the
exploration of feasible boundaries ‚Äì are well balanced in ASCHEA. Hamida and
Schoenauer used (100 + 300)-ES to solve benchmark problems and achieve satis-
factory results.
24 Hamida and Schoenauer suggested pselect = 0.3.

150
4 Constrained Optimization
4.3.4 Self-adaptive Penalty Function
Basically, self-adaptive control for penalty coefÔ¨Åcients is not possible for COEAs
with proportional selection.25 But Eiben et al. suggested an interesting idea to over-
come the difÔ¨Åculty in solving CSP [8].
The key point of their self-adaptive penalty function is tournament selection. We
take binary tournament selection as an example. For two randomly selected indi-
viduals with a self-adaptive penalty coefÔ¨Åcient vector, i.e., (x1,ùùé1) and (x2,ùùé2),26
we can Ô¨Ånd the maximum penalty coefÔ¨Åcient of constraint j in the two values
œâmax
j
= max(œâ1
j ,œâ2
j ). Then the penalty function for both individuals is as follows:
penalty(x) =
k
‚àë
j=1
œâmax
j
vj (x)
(4.24)
where vj (x) is the violation of constraint j deÔ¨Åned by Eq. 4.11.
In this way, the penalty coefÔ¨Åcient could not determine the Ô¨Åtness value of its
corresponding individual, which avoids the ‚Äúcheating‚Äù of the individual by making
its penalty coefÔ¨Åcients all zero.
4.4 Separation of Constraint Violation and Objective Value
The main concern in designing COEAs is how to handle infeasible individuals that
have small objective values.
We have discussed the Ô¨Årst two classes of COEAs using the taxonomy introduced
in Sect. 4.1.2. The Ô¨Årst class has a special encoding and decoding procedure and
variation operators to ensure that the search is only in F. In this way, the above
question has been evaded. The representative algorithm is HM, which is introduced
in Sect. 4.2.2.
The second class uses penalty coefÔ¨Åcients to combine the objective value and the
constraint violation and then change COPs into unconstrained problems. Depend-
ing on the assignment of the penalty coefÔ¨Åcients, the infeasible individuals with
small objective values might have advantages compared with feasible individuals
with large objective values. The representative algorithm is ASCHEA, which is in-
troduced in Sect. 4.3.3.1.
The third class, which might be the cutting edge of COEAs, separates the ob-
jective value and the constraint violation. In the parental selection or replacement
procedure, when comparisons are necessary between every two individuals, special
rules are suggested to maintain the balance between exploring F and utilizing the
information within the infeasible individuals.
25 What will happen if we encode the penalty coefÔ¨Åcients into chromosome directly?
26 (x1,ùùé1) = (x1
1,‚ãÖ‚ãÖ‚ãÖ,x1
n,œâ1
1,‚ãÖ‚ãÖ‚ãÖ,œâ1
k ) and (x2,ùùé2) = (x2
1,‚ãÖ‚ãÖ‚ãÖ,x2
n,œâ2
1,‚ãÖ‚ãÖ‚ãÖ,œâ2
k ).

4.4 Separation of Constraint Violation and Objective Value
151
The most straightforward comparison method was suggested by Deb in 2000 [9].
It can be illustrated simply with the following three rules.
‚àôBetween two feasible individuals, the one with the better objective value wins.
‚àôBetween one feasible individual and one infeasible individual, the feasible one
wins.
‚àôBetween two infeasible individuals, the one with the smaller overall constraint
violation, Eq. 4.12 or Eq. 4.13, wins.
An example of using the above rules was proposed by Kukkonen and Lampinen.
They embedded the above ideas into DE for COPs in 2006 [10].
4.4.1 Constrained Optimization Evolutionary Algorithms Based on
Rank
4.4.1.1 Stochastic Ranking
Surry and Radcliffe suggested the important idea of using a probability to determine
the method of comparing two individuals, by the objective value or the constraint
violation [11]. Runarsson and Yao developed this idea into the stochastic ranking
(SR) procedure and implemented it in (Œº,Œª)-ES [12]. SR is simple yet powerful, so
it has actually become one of the benchmark COEAs.27
Bubble sort is a simple ranking method for a list of numbers. It works by repeat-
edly passing through the list to be ranked, comparing two adjacent items at a time,
and swapping them if they are in the wrong order. This procedure is repeated until
no swaps are needed, which indicates that the list is sorted. The algorithm gets its
name from the way bubbles rise to the surface of water.
For example, we would like to rank the numbers (1.3,6.1,0.5,1.2,3.3) in as-
cending order using bubble sort. The ranking procedure is illustrated by Fig. 4.7.
In bubble sort, large elements at the beginning of the list do not pose a problem
as they are quickly swapped down, e.g., 6.1 in Fig. 4.7; but small elements toward
the end move to the beginning extremely slowly.28 Bubble sort has a worst-case
complexity of O(n2), where n is the number of items being sorted. So generally, it
is a rather slow ranking method. The reason Runarsson and Yao adopted bubble sort
might be its convenience in implementation.
In SR-based (Œº,Œª)-ES, SR is carried out after Œª new individuals have been gen-
erated by Œº parents.29
In SR, from the beginning to the end of the population, pairs of individuals are
compared and might be swapped using the rules discussed later until there is no
swap in one passing through the population. So it is a bubble-sort-like procedure.
27 The source code of SR can be downloaded at http://www3.hi.is/Àútpr/index.php?
page=software/sres/sres.
28 How many steps are required if 0.5 is at the end of the initial list?
29 Refer to Sect. 2.3.1 for details of ES.

152
4 Constrained Optimization
Fig. 4.7 Bubble sort example.
The curly brackets indicate
the pair being compared



















00)00 0
00)00 0
0)000 0
0)000 0
0)000 0
)0000 0
)0000 0
After SR, the Ô¨Årst Œº new individuals in the ranking is selected as the parents for
the next generation. The only parameter required by SR is po, i.e., the probability
of comparison using the objective value. The rules for comparing two individuals, j
and (j +1), in the bubble sort, illustrated by Fig. 4.7, is as follows:
if (v(j) = v(j +1) = 0) or (rand < po) then
if f(j) > f(j +1) then
swap(j, j +1)
end if
else
if v( j) > v( j +1) then
swap(j, j +1)
end if
end if
where v( j) and v(j + 1) are an overall constraint violation of individuals j and
(j + 1), respectively, rand ‚àºU(0,1), swap(j, j + 1) is the function exchange of
the location of individuals j and (j +1) in the list.
The rationale of the above SR procedure is that the comparison is carried out on
the objective value with probability po or the individuals being compared are both
feasible; the comparison is carried out on the overall constraint violation otherwise.
After SR, the feasible individual with a small objective value, the infeasible indi-
vidual with a small constraint violation, and the infeasible individual with a small
objective value all have the probability of being at the front of the Ô¨Ånal list, i.e., the
best part of the Œª new individuals.
If po = 0, the above procedure is just Deb‚Äôs comparison idea discussed in Sect.
4.4.30 If po = 1, the above procedure only considers the objective value. po should
be less than 0.5 to capture the bias in favor of feasible solutions.
30 Why?

4.4 Separation of Constraint Violation and Objective Value
153
Runarsson and Yao used po = 0.45 in (30,200)-ES to solve 13 benchmark COPs
and got excellent results. They further used the DE idea in the mutation procedure
of ES in 2005 [13], i.e., the Ô¨Årst Œº ‚àí1 new individuals are generated with a DE
mutation operator as follows:
x
‚Ä≤
i = xi +Œ≥ (x1 ‚àíxi+1)
(4.25)
where xi and x
‚Ä≤
i are the old and new individuals before and after the DE mutation
operator, respectively, x1 is the Ô¨Årst and best individual after SR, xi+1 is the next
worse individual xi, and Œ≥ is a parameter predeÔ¨Åned by users.31 Equation 4.25 means
that the mutant of individual i starts at xi and changes with the direction from the
less worse individual to the best individual, i.e., improving direction.
4.4.1.2 Dynamic Stochastic Ranking in Multimember Differential Evolution
In SR, Runarsson and Yao only use Ô¨Åxed po = 0.45 in all numerical experiments.
Zhang et al. suggested a dynamic way to linearly control the change in po to shift
the focus from both the feasible and infeasible region searches to gradually Ô¨Ånding
a feasible optimal solution [14]. They use multimember differential evolution to
solve COPs, where M new individuals are generated for each old individual and the
Ô¨Årst one after the dynamic stochastic ranking on these (M + 1) individuals is the
individual for the next generation.
The solution process of the multimember DE based on a dynamic stochastic rank-
ing can be illustrated as follows:
Multimember Differential Evolution Based on Dynamic Stochastic Ranking
Phase 1: Initialization.
Step 1.1: Assign the parameters for multimember DE, such as Np, F,
Cr, M, stop criteria (such as maxgen), etc. The deÔ¨Ånition of Np, F, Cr was
introduced in Sect. 2.4.2.2.
Step 1.2: Generate Np uniformly distributed individuals randomly to
form the initial population and evaluate their Ô¨Åtness values. gen = 0.
Phase 2: Main loop. Repeat the following steps until stop criteria are sat-
isÔ¨Åed (such as gen > maxgen).
Step 2.1: gen = gen + 1. Do the following substeps from the Ô¨Årst in-
dividual to the last one. i = 1
Substep 2.1.1: Get M mutants of individual i using Eq. 2.20 M
times with different r1 ‚àï= r2 ‚àï= r3 ‚àï= xi.
31 Runarsson and Yao suggested that Œ≥ = 0.85.

154
4 Constrained Optimization
Substep 2.1.2: Get M offspring of individual i using Eq. 2.21 M
times for M mutants. These M offspring constitute the offspring group for
individual i.
Substep 2.1.3: i = i+1. Goto substep 2.1.1 if i ‚â§Np, otherwise
goto step 2.2.
Step 2.2: Evaluate the objective values and the constraint violations
for every offspring in the offspring group for each individual.
Step 2.3: Dynamic control of po.
Step 2.4: For each individual and its M offspring, use SR procedure
to rank these (M +1) individuals and take the Ô¨Årst one as the individual in the
next generation. Thus the new population is generated.
Phase 3: Submitting the Ô¨Ånal Np individuals as the results of EA.
In the above solution process, the dynamic control of po in step 2.3 is as follows:
po = 0.45
(
1‚àí
gen
maxgen
)
(4.26)
where gen is the current generation number and maxgen is the maximum generation
number. In the early stage, a large po value promotes a search in both feasible and
infeasible regions, while in the late stage, a small po value focuses on the exploration
and exploitation in F.
Zhang et al. suggested that M = 5 and claimed that the multimember DE with
dynamic SR got ‚ÄúsigniÔ¨Åcantly better‚Äù results than current available methods.
4.4.1.3 Constrained Optimization Evolutionary Algorithm Based on Multiple
Ranks
SR considers the rank of objective value and constraint violation in a stochastic
way. Ho and Shimizu use three deterministic ranks to guide the search for a feasible
optimal solution using (Œº,Œª)-ES [15].
After Œª new individuals have been generated in ES, we can get three ranks con-
sidering three properties, i.e., objective value, overall constraint violation, and num-
ber of constraints violated, in ascending order. For individual i, we use Rf (i), Rs(i),
and Rv(i) to represent its order in these three ranks, respectively.
The order of individual i in any rank is equal to 1 + d , where d is the number
of individuals that are better than i in this property. For example, if there are Ô¨Åve
feasible individuals in the current population, the order of any individual with only
one constraint violated in the rank of Rv is 6. It is easy to understand that all the
feasible individuals have the order Rs = 1 and Rv = 1.
In this way, objective value, overall constraint violation, and the number of con-
straints violated are considered simultaneously and and scaled on the same magni-
tude.

4.4 Separation of Constraint Violation and Objective Value
155
Then the Ô¨Åtness value of the Œª new individuals can be calculated as follows:
if (All Œª individuals are infeasible) then
f
‚Ä≤ (x) = Rs (x)+Rv (x)
(4.27)
else
f
‚Ä≤ (x) = Rf (x)+Rs (x)+Rv (x)
(4.28)
end
where R f (x), Rs(x), and Rv(x) are orders of individual x in different properties and
f
‚Ä≤ (x) is the Ô¨Åtness value of individual x.
The above procedure requires no predeÔ¨Åned parameters, which makes the algo-
rithms easy to implement and robust for COPs.
Ho and Shimizu also suggested adaptive equality constraint tolerance handling
techniques. Interested readers are referred to [15].
4.4.2 Simple Multimembered Evolution Strategy
Since 2003, Mezura-Montes and Coello Coello have published a series of papers
applying (Œº + 1)-ES and (1 + Œª)-ES in COPs [16, 17]. These algorithms either
represent premature convergence in some benchmark COPs or lack the explorative
power to allow them to sample large solution landscapes. So they use (Œº + Œª)-ES
to solve COPs [18], denoted as simple multimembered evolution strategy (SMES).
As was discussed in the above sections, the crucial part of COEAs is the ability to
maintain the diversity of the feasible and infeasible individuals.32 Mezura-Montes
and Coello Coello suggested a novel diversity preservation mechanism for COEAs
in the replacement process of (Œº + Œª)-ES, whose solution process was introduced
in Sect. 2.3.1.
The main characteristics of SMES are as follows:
1. Diversity mechanism in replacement. In step 2.3 of (Œº +Œª)-ES, Œº individuals
for the next generation are selected from the union of Œº parents and Œª offspring
using Deb‚Äôs comparison rule introduced in Sect. 4.4 with probability 0.97. For the
other 0.03 probability, individuals for the next generation are selected as the ‚Äúbest
infeasible individuals‚Äù from Œº parents and Œª offspring with the same chance.33
32 We leave the comparison of these techniques in several classical COEAs as an exercise.
33 The ‚Äúbest infeasible individuals‚Äù are those with the smallest objective value and smallest con-
straint violation.

156
4 Constrained Optimization
2. Combined recombination. In step 2.1 of (Œº + Œª)-ES, crossover operators are
selected from discrete crossover, introduced in Sect. 3.2.1, and intermediate
crossover, introduced in Sect. 2.3.1, with 0.5 probability.
3. Smaller initial step size. Schwefel gave suggestions on the initial step size, œÉ,
for mutation in ES [19] and Runarsson and Yao used it directly in RS-based
(Œº,Œª)-ES [12]. Mezura-Montes and Coello Coello suggested using a smaller
initial step size, 40% of the suggested value, to promote a Ô¨Åner search.
4. Dynamic control of the tolerance. Mezura-Montes and Coello Coello adopt the
dynamic control of the tolerance in ASCHEA with factŒ¥ = 1.00195 in Eq. 4.22.
Mezura-Montes and Coello Coello compared the contribution of the Ô¨Årst three
characteristics listed above for the performance of SMES in benchmark problems
and found that combined recombination is the most important part in these three
improvements.
They also compare (100+300)-ES with a GA with popsize = 200 on benchmark
COPs and came to the conclusion that ES is good at COPs for its good search engine,
i.e., the self-adaptive mutation. Their numerical comparison, published in 2008, also
discuss this issue [20].
To summarize the above two factors, they claimed that the selection of the search
engine is more critical than the selection of the constraint handling mechanism for
COEAs.
4.4.3 Œ± Constrained Method
Takahama and Sakai suggested the concept of satisfaction level, which is similar to
the concept of membership function in fuzzy sets, to transform COPs into uncon-
strained problems without penalty coefÔ¨Åcients [21].
For inequality constraints gi (x) in Eq. 4.1, their satisfaction levels are deÔ¨Åned as
follows:
Œºgi (x) =
‚éß
‚é®
‚é©
1,
gi (x) ‚â§0
1‚àígi(x)
bi ,
0 ‚â§gi (x) ‚â§bi
0,
otherwise
(4.29)
where bi is the user-deÔ¨Åned threshold.
For equality constraints h j (x) in Eq. 4.1, their satisfaction levels are deÔ¨Åned as
follows:
Œºhj (x) =
{
1‚àí‚à£hj(x)‚à£
bj
,
		h j (x)
		 ‚©Ωb j
0,
otherwise
(4.30)
where b j is the user-deÔ¨Åned threshold.34
34 Takahama and Sakai suggested that bi = bj = 1000.

4.4 Separation of Constraint Violation and Objective Value
157
The satisfaction level for inequality and equality constraints are illustrated in
Fig. 4.8.35
 



 




)
(a)




























 



 




)


(b)
Fig. 4.8 Satisfaction level for inequality constraints and equality constraints: (a) inequality con-
straints, and (b) equality constraints
Then the overall satisfaction level can be deÔ¨Åned as follows:
Œº (x) = min
i,j
{
Œºgi (x),Œºh j (x)
}
(4.31)
Equation 4.31 means that the worst performance in k satisfaction levels deter-
mines the overall performance, which is similar to the idea of Eq. 4.13.
Before comparing two individuals, we need to deÔ¨Åne a level Œ±. If the overall
satisfaction levels of two individuals are both above that level or their overall sat-
isfaction levels are the same, we neglect their difference in constraint violation and
only compare their objective values. Otherwise, the one with the higher overall sat-
isfaction level wins the competition. This is the idea of Œ± level comparison. For a
given comparison pair, individuals 1 and 2, 1 is better than 2 at the Œ± level if the
following conditions are satisÔ¨Åed:
( f1,Œº1) ‚â§Œ± (f2,Œº2) ‚áî
‚éß
‚é®
‚é©
f1 ‚â§f2,
Œº1,Œº2 ‚â•Œ±
f1 ‚â§f2,
Œº1 = Œº2
Œº1 > Œº2,
otherwise
(4.32)
Œ± level comparison neglects the overall satisfaction level above Œ±, i.e., the differ-
ence in constraint violation below a certain threshold. In the case of Œ± = 1, Eq. 4.32
equals to Deb‚Äôs comparison rule introduced in Sect. 4.4, which is similar to the case
of po = 0 in SR. If Œ± = 0, the Eq. 4.32 only considers the objective value, which is
similar to the case of po = 1 in SR.
After applying the comparison rule between two individuals, the original COP,
Eq. 4.1 can be transformed into Eq. 4.33 with a given Œ± level:
(P‚â§Œ±)
min
‚â§Œ± f (x)
(4.33)
35 Both Fig. 4.8a and b can be regarded as the membership function of fuzzy sets. That is why we
say that satisfaction level is similar to a fuzzy set.

158
4 Constrained Optimization
where min‚â§Œ± denotes minimization based on an Œ± level comparison ‚â§Œ±.
Takahama and Sakai proved that if Eq. 4.1 has feasible optimal solution x‚àóand
problem Eq. 4.33 has optimal solution ÀÜxn with any {Œ±n} ‚àº[0,1], a strictly increasing
nonnegative sequence {Œ±n} will cause the sequence {ÀÜxn} to converge to x‚àó. So we
can optimize Eq. 4.33 with an Œ± increase scheme as follows:
Œ± (gen) = (1‚àíŒ≤)Œ± (gen‚àí1)+Œ≤
(4.34)
where 0 < Œ≤ < 1 is a control parameter. It is easy to get the general expression of
Œ± (gen) as follows:
Œ± (gen) = (1‚àíŒ≤)gen Œ± (0)+Œ≤
[
1+(1‚àíŒ≤)+‚ãÖ‚ãÖ‚ãÖ+(1‚àíŒ≤)gen‚àí1]
(4.35)
If gen ‚Üí‚àû, the above expression will converge to 1. Takahama and Sakai sug-
gested the pragmatic Œ± dynamic control method, which can be illustrated as follows:
if (0 < gen ‚â§maxgen
2
) and ((gen mod TŒ±) = 0) then
Œ± (gen) = (1‚àíŒ≤)Œ± (gen‚àí1)+Œ≤
else if (0 < gen ‚â§maxgen
2
) and ((gen mod TŒ±) ‚àï= 0) then
Œ± (gen) = Œ± (gen‚àí1)
else if gen > maxgen
2
Œ± = 1
end if
where TŒ± is a control parameter. The above procedure increases Œ± for every TŒ±
generations in the Ô¨Årst half of the total generations and keeps it constant at 1 in the
second half of the total generations.36 The initial Œ± is suggested as the average of the
best satisfaction level and the mean of all satisfaction levels in the initial population,
i.e.,
Œ± (0) = 1
2
‚éõ
‚éú
‚éú
‚éùmax
i
Œº
(
xi)
+
popsize
‚àë
i=1
Œº
(
xi)
popsize
‚éû
‚éü
‚éü
‚é†
(4.36)
After the above transformation, we can solve Eq. 4.33 with the Œ± increase scheme
discussed above. Finally, the optimal solution of Eq. 4.33 with Œ± = 1 will converge
to the feasible optimal solution of Eq. 4.1.
Takahama and Sakai further utilized multiple simplex methods with mutation on
the worst solution for solving COPs by the Œ± constrained method. For the imple-
mentation detail, readers are referred to [21] and Sect. 2.4.1.1.
36 Takahama and Sakai suggested that Œ≤ = 0.03 and TŒ± = 50.

4.5 Performance Evaluation of Constrained Optimization Evolutionary Algorithms
159
4.5 Performance Evaluation of Constrained Optimization
Evolutionary Algorithms
4.5.1 Benchmark Problems
Several factors might affect the difÔ¨Åculty of a COP. Such factors are listed as fol-
lows:
‚àôThe ratio of solution numbers in the feasible region F to that of the search space
S. The analytic expression of such a ratio might be difÔ¨Åcult. But we can randomly
generate solutions in S and determine whether they are feasible or not.37 If the
sampling points are large enough, such as 106, we can get a reliable ratio.
‚àôThe dimensionality of variables, i.e., n in Eq. 4.1. Large n will signiÔ¨Åcantly in-
crease the difÔ¨Åculty of COPs.
‚àôThe number of nonlinear equality constraints and its ratio to the total number
of constraints. Linear equality constraints can be eliminated by GENOCOP but
nonlinear equality constraints are difÔ¨Åcult even though they could be relaxed with
Eq. 4.2.
‚àôThe number of inequality constraints that are active in the feasible optimal solu-
tion. Being active means that the feasible optimal solution is at the boundary of
that inequality constraint that requires a search from both feasible and infeasible
sides.
The benchmark COPs can be classiÔ¨Åed roughly into the following three groups.
1. Problems suggested by different researchers. Koziel and Michalewicz suggested
12 benchmark COPs in 1999 [2]. Then Runarsson and Yao developed them into
13 benchmark COPs in 2000 [12]. These 13 benchmark COPs are actually the
standard for comparing and evaluating COEAs. In 2006, the IEEE Congress on
Evolutionary Computation held a special session on constrained real parameter
optimization. Twenty four benchmark COPs were suggested there.38
2. Problem generators suggested by different researchers. Michalewicz et al. sug-
gested a test-case generator (TCG) for COPs in 2000 [22]. Schmidt and Michalew-
icz developed it into TCG-2 in 2000 [23]. In 2003, Craenen et al. suggested a sys-
tematic way to generate random binary constraint satisfaction problems [24].39
3. Real-world problems. In 2004 Mezura-Montes suggested Ô¨Åve engineering con-
strained optimization design problems [25].
The most recently reported best feasible solutions or the analytical optimal fea-
sible solutions for the Ô¨Årst two groups are available.
37 Just like the ordinary way in the Monte Carlo method.
38 http://www3.ntu.edu.sg/home/EPNSugan/.
39 C++ classes for generating the test sets of binary constraint satisfaction problems can be down-
loaded from http://freshmeat.net/projects/randomcsp.

160
4 Constrained Optimization
4.5.2 Performance Indices
Several performance indices have been suggested by COEA researchers [12, 25,
26]. Here we simply assemble and reformulate them as follows.
For every benchmark COP whose feasible optimal solution is x‚àó, we need to run
the COEA on it many times, e.g., 25, and the maximum number of the objective
function evaluation (MaxNOFE) needs to be predeÔ¨Åned, e.g., 500000, for each run.
We deÔ¨Åne a feasible run as a run during which at least one feasible solution is found
within MaxNOFE40 and a successful run as a run during which a feasible solution x
is found satisfying ‚à£f(x) ‚àíf(x‚àó)‚à£< 0.0001. All of the following PIs are for each
benchmark COP.
1. Report the feasible rate as the number of feasible runs over the total runs (25).
The larger the better.
2. Report the average number of objective function evaluations (NOFE) for Ô¨Ånding
the Ô¨Årst feasible solution. The smaller the better.
3. Report the successful rate as the number of successful runs over the total runs
(25). The larger the better.
4. Report the average NOFE in successful runs. The smaller the better.
5. Report the success performance as ratio of the average number of the objec-
tive function evaluation (NOFE) in successful runs over the successful rate. The
smaller the better.
6. Report the best, median(mean), worst feasible solutions and the standard devia-
tion of the feasible solutions for the 25 runs.
4.6 Summary
Constrained optimization problems are realistic models coming from the real world.
OR has done much research on this topic. If the selection procedure and the varia-
tion operators are designed elaborately, EAs can evolve in different feasible regions,
Ô¨Ånd new feasible regions, and approach the feasible optimal solution from different
directions, which might be very useful for COPs whose feasible regions are discon-
nected or whose feasible optimal solution is at the edge of the feasible region.
GENOCOP and HM, discussed in Sect. 4.2, use special decoding approaches
and variation operators to limit the search within feasible region F. In the case of
convex F and a large ratio of solution numbers of F to that of S, this ‚Äúfeasibility
maintenance‚Äù algorithm might search effectively.
The penalty function method is an all-purpose method for COPs but the assign-
ment of the penalty coefÔ¨Åcients requires different techniques. Deterministic, adap-
tive, and self-adaptive controls for penalty coefÔ¨Åcients were discussed with many
examples in Sect. 4.3. Numerical experimental results illustrated that a ‚Äúproperly‚Äù
40 The tolerance for equality constraints is Œ¥ = 0.0001.

4.6 Summary
161
chosen penalty function might accelerate the search and contribute high-quality so-
lutions [27].
The main part of this chapter is Sect. 4.4. The essences of three famous algo-
rithms ‚Äì SR, SMES, and Œ± constrained method ‚Äì are introduced separately and
many variation algorithms derived from them are discussed. They all use uncer-
tainty to determine the winner of the competition between feasible and infeasible
individuals. It is promising to change, with time or with feedback, the uncertainty
that controls the comparison rules.
Several comments can be made from the introduction of this chapter as follows:
‚àôNone of the famous COEAs introduced in this chapter has a complicated con-
straint handling mechanism. So there might be large opportunities for combining
traditional constraint handling techniques with EAs.
‚àôThe majority of the prevailing COEAs use ES, which might be the result of ES‚Äôs
Ô¨Åne search ability using the self-adaptive control of the step size œÉ.
‚àôThe idea of directional search, e.g., scatter search, simplex search, and differen-
tial evolution, might be adopted by COEAs to improve their search ability.
After reading this chapter, you should understand the difÔ¨Åculty of COPs and
why COEAs might have an advantage over COPs, be familiar with the basic idea of
several state-of-the-art MOEAs, and know how to compare COEAs with standard
PIs.
In all, designing a COEA is the art of tradeoff between feasible and infeasible
individuals.
Suggestions for Further Reading
Gen and Cheng‚Äôs book [28], Eiben and Smith‚Äôs textbook [29], and B¬®ack et al.‚Äôs
book [30] all have special chapters on constrained optimization.
Michalewicz and Schoenauer published a survey in 1996 [31]. Eiben‚Äôs 2001 sur-
vey [32] and Coello Coello‚Äôs 2002 survey [33] are also the most consulted ones.
In the Ô¨Åeld of penalty function, there are many successful cases. Some guidelines
are given for designing penalty functions by Richardson et al. [34]. An intensive
runtime analysis is given by Zhou and He for a better understanding of penalty
functions [27]. A numerical comparison of Ô¨Åve penalty-based constraint handling
techniques was published by Miettinen et al. in 2003 [35]. Readers fascinated by
the idea of designing adaptive penalty functions are referred to three other examples
by Smith and Tate [36],41 Powell and Skolnick [38], and Farmani and Wright [39].
Readers interested in SMES are referred to Mezura-Montes‚Äôs Ph.D. thesis, pub-
lished in 2004 [25], and a numerical comparison of different ESs in solving COPs
published in 2008 [20].
41 It was used by Tasgetiren and Suganthan in 2006 [37].

162
4 Constrained Optimization
Paredis used competitive coevolution in 1994 [40], and Coello Coello used co-
operative coevolution [41] separately to solve COPs in the framework of COEAs.
The must-read papers of this chapter are [12] for SR, [21] for the Œ± constrained
method, [25] for a detailed discussion of ES-based COEAs, and [33] for a survey.
Exercises and Potential Research Projects
4.1. How can a whole arithmetic crossover maintain the feasibility of the offspring
in GENOCOP?
4.2. How can the location of r0 affect the optimization results of HM? Can you offer
some advice for selecting the reference point?
4.3. In HM search, all points in the convex cube [‚àí1,1]n are treated identically,
which means that all points in F are treated identically. Can you suggest a way to
give preference to the boundary points?
4.4. Implement the DA and AA of ASCHEA in your programming environment.
Considering the discussion about them in the textbook, can you suggest other im-
provements? Report your basic idea, your implementation details, your numerical
experimental results on the benchmark problems in Appendix, and the statistical
conclusions using methods introduced in Sect. 3.6.
4.5. Implement the stochastic ranking in your programming environment or learn
to use the source code provided by Runarsson and Yao. Study the sensitivity of
po in SR with benchmark COPs. Report your numerical experimental results on
the benchmark problems in Appendix and the statistical conclusions using methods
introduced in Sect. 3.6.
4.6. Can you adopt other ranking methods apart from the bubble sort in SR and im-
plement it within your own COEA? Report your basic idea, your implementation de-
tails, your numerical experimental results on the benchmark problems in Appendix,
and the statistical conclusions using methods introduced in Sect. 3.6.
4.7. Summarize and compare the feasible and infeasible individual diversity preser-
vation mechanisms in ASCHEA, SR, SMES, and the Œ± constrained method.
4.8. Summarize and compare the equality constraint handling mechanisms in GENO-
COP, HM, ASCHEA, SR, SMES, and the Œ± constrained method. Please refer to
original paper for details.
4.9. Mezura-Montes and Coello Coello claimed that the selection of the search en-
gine is more critical than the selection of the constraint handling mechanism for
COEAs [20]. Read this paper carefully and do your own numerical comparisons on
ES and GA for COPs. Report your numerical experimental results on the benchmark
problems in Appendix, and the statistical conclusions using methods introduced in
Sect. 3.6. What is your conclusion?

References
163
References
1. Michalewicz Z (1998) Genetic algorithms + data structures = evolution programs. Springer,
Berlin Heidelberg New York
2. Koziel S, Michalewicz Z (1999) Evolutionary algorithms, homomorphous mappings, and con-
strained parameter optimization. Evol Comput 7(1):19‚Äì44
3. Joines J, Houck C (1994) On the use of non-stationary penalty functions to solve nonlinear
constrained optimization problems with GA‚Äôs. In: Proceedings of the IEEE conference on
evolutionary computation, pp 579‚Äì584
4. Gen M, Cheng R (1996) A survey of penalty techniques in genetic algorithms. In: Proceedings
of the IEEE international conference on evolutionary computation, pp 804‚Äì809
5. Hadj-Alouane AB, Bean JC (1997) A genetic algorithm for the multiple-choice integer pro-
gram. Oper Res 45(1):92‚Äì101
6. Hinterding R (2001) Constrained parameter optimisation: equality constraints. In: Proceed-
ings of the IEEE congress on evolutionary computation, 1:687‚Äì692
7. Hamida SB, Schoenauer M (2002) ASCHEA: new results using adaptive segregational con-
straint handling. In: Proceedings of the IEEE congress on evolutionary computation, 1: 884‚Äì
889
8. Eiben A, Jansen B, Michalewicz Z et al (2000) Solving CSPs using self-adaptive constraint
weights: how to prevent EAs from cheating. In: Proceedings of genetic and evolutionary
computation conference, pp 128‚Äì134
9. Deb K (2000) An efÔ¨Åcient constraint handling method for genetic algorithms. Comput Meth-
ods Appl Mechan Eng 186:311‚Äì338
10. Kukkonen S, Lampinen J (2006) Constrained real-parameter optimization with generalized
differential evolution. In: Proceedings of the IEEE congress on evolutionary computation, pp
207‚Äì214
11. Surry PD, Radcliffe NJ (1997) The COMOGA method: constrained optimisation by multiob-
jective genetic algorithms. Control Cybern 26(3):391‚Äì412
12. Runarsson T, Yao X (2000) Stochastic ranking for constrained evolutionary optimization.
IEEE Trans Evol Comput 4(3):284‚Äì294
13. Runarsson T, Yao X (2005) Search biases in constrained evolutionary optimization. IEEE
Trans Sys Man Cybern, C 35(2):233‚Äì243
14. Zhang M, Luo W, Wang X (2008) Differential evolution with dynamic stochastic selection for
constrained optimization. Inf Sci 178(15):3043‚Äì3074
15. Ho PY, Shimizu K (2007) Evolutionary constrained optimization using an addition of ranking
method and a percentage-based tolerance value adjustment scheme. Inf Sci 177(14):2985‚Äì
3004
16. Mezura-Montes E, Coello Coello CA (2003) A simple evolution strategy to solve constrained
optimization problems. In: Proceedings of the genetic and evolutionary computation Confer-
ence, p 198
17. Mezura-Montes E, Coello Coello CA (2003) Adding a diversity mechanism to a simple evolu-
tion strategy to solve constrained optimization problems. In: Prceedings of the IEEE congress
on evolutionary computation, pp 6‚Äì13
18. Mezura-Montes E, Coello Coello CA (2005) A simple multimembered evolution strategy to
solve constrained optimization problems. IEEE Trans Evol Comput 9(1):1‚Äì17
19. Schwefel H (1995) Evolution and optimum seeking. Wiley-Interscience, New York
20. Mezura-Montes E, Coello Coello CA (2008) An empirical study about the usefulness of evo-
lution strategies to solve constrained optimization problems. Int J Gen Syst 37(4):443‚Äì473
21. Takahama T, Sakai S (2005) Constrained optimization by applying the Œ± constrained method
to the nonlinear simplex method with mutations. IEEE Trans Evol Comput 9(5):437‚Äì451
22. Michalewicz Z, Deb K, Schmidt M et al (2000) Test-case generator for nonlinear continuous
parameter optimization techniques. IEEE Trans Evol Comput 4(3):197‚Äì215
23. Schmidt M, Michalewicz Z (2000) Test-case generator TCG-2 for nonlinear parameter opti-
misation. In: Proceedings of the international conference on parallel problem solving from
nature, pp 539‚Äì548

164
4 Constrained Optimization
24. Craenen B, Eiben A, van Hemert J (2003) Comparing evolutionary algorithms on binary con-
straint satisfaction problems. IEEE Trans Evol Comput 7(5):424‚Äì444
25. Montes EM (2004) Alternative techniques to handle constraints in evolutionary optimization.
Ph.D. thesis, CINVESTAV-IPN
26. Liang JJ, Runarsson TP, Mezura-Montes E et al (2006) Problem deÔ¨Ånitions and evaluation
criteria for the cec 2006 special session on constrained real-parameter optimization. Tech.
rep., Nanyang Technological University, University of Iceland, CINVESTAV-IPN, France
T¬¥el¬¥ecom, Indian Institute of Technology, Singapore
27. Zhou Y, He J (2007) A runtime analysis of evolutionary algorithms for constrained optimiza-
tion problems. IEEE Trans Evol Comput 11(5):608‚Äì619
28. Gen M, Cheng R (1997) Genetic algorithms and engineering design. Wiley-Interscience, New
York
29. Eiben AE, Smith JE (2003) Introduction to evolutionary computing. Springer, Berlin Heidel-
berg New York
30. B¬®ack T, Fogel D, Michalewicz Z (2000) Evolutionary computation 2: advanced algorithms
and operations. Taylor & Francis, London, UK
31. Michalewicz Z, Schoenauer M (1996) Evolutionary algorithms for constrained parameter op-
timization problems. Evol Comput 4:1‚Äì32
32. Eiben AE (2001) Evolutionary algorithms and constraint satisfaction: deÔ¨Ånitions, survey,
methodology, and research directions. In Kallel L, Naudts B, Rogers A (eds) Theoretical as-
pects of evolutionary computing. Springer, Berlin Heidelberg New York, pp 13‚Äì30.
33. Coello Coello CA (2002) Theoretical and numerical constraint-handling techniques used with
evolutionary algorithms a survey of the state of the art. Comput Methods Appl Mechan Eng
191(11):1245‚Äì1287
34. Richardson JT, Palmer MR, Liepins GE et al (1989) Some guidelines for genetic algorithms
with penalty functions. In: Proceedings of the 3rd international conference on genetic algo-
rithms, pp 191‚Äì197
35. Miettinen K, M¬®akel¬®a MM, Toivanen J (2003) Numerical comparison of some penalty-based
constraint handling techniques in genetic algorithms. J Glob Optim 27(4):427‚Äì446
36. Smith AE, Tate DM (1993) Genetic optimization using a penalty function. In: Proceedings of
the 5th international conference on genetic algorithms, pp 499‚Äì505
37. Tasgetiren M, Suganthan P (2006) A multi-populated differential evolution algorithm for solv-
ing constrained optimization problem. In: Priceedings of the IEEE congress on evolutionary
computation, pp 33‚Äì40
38. Powell D, Skolnick MM (1993) Using genetic algorithms in engineering design optimization
with non-linear constraints. In: Proceedings of the 5th international conference on genetic
algorithms, pp 424‚Äì431
39. Farmani R, Wright J (2003) Self-adaptive Ô¨Åtness formulation for constrained optimization.
IEEE Trans Evol Comput 7(5):445‚Äì455
40. Paredis J (1994) Co-evolutionary constraint satisfaction. In: Proceedings of the international
conference on parallel problem solving from nature, pp 46‚Äì55
41. Coello Coello CA (2000) Use of a self-adaptive penalty approach for engineering optimization
problems. Comput Ind 41(2):113‚Äì127

Chapter 5
Multimodal Optimization
Abstract Sometimes you run a EA for a problem several times. The algorithm might
provide different solutions with similar qualities. You may feel uncomfortable with
this. We will show you in this chapter that there really exist problems with several
high-quality solutions and we want to Ô¨Ånd all of them in a single run of an EA.
Techniques in this chapter could also help to adjust the tradeoff between selective
pressure and population diversity, which is an eternal subject in designing and ana-
lyzing EAs.
5.1 Problems We Face
We introduced the concept of multimodality in Chap. 1, where it is used to describe
the situation where there are several local optimal solutions in a solution space and
we want the algorithm to Ô¨Ånd the only global optimal solution as soon as possible.
But here we start from a different viewpoint.
5.1.1 Multimodal Problems
In this chapter, without loss of generality, we will only discuss the unconstrained
maximum problem whose objective values are all above zero:1
max f (x) ‚â•0
x ‚àà‚Ñùn
(5.1)
1 Fitness transferral, which changes other problems into the required form, was discussed in Sect.
3.3.3.
165

166
5 Multimodal Optimization
The concept of multimodal problems has different meanings. The general ex-
planation has been stated above. Sometimes there are other situations that make us
concern with Ô¨Ånding multiple solutions.
‚àôThere are several global optimal solutions and we want to Ô¨Ånd them all, which is
illustrated by Fig. 5.1a.
‚àôThere are global optimal solutions and local optimal solutions (not too many).
We are interested in Ô¨Ånding all of them, which is illustrated by Fig 5.1b.
(a)






	










	





1
3
(b)
Fig. 5.1 Multimodal optimization examples: (a) example 1, and (b) example 2
If we use the EAs introduced in Chaps. 2 and 3 to solve the problems illustrated
by Fig. 5.1a directly, they might Ô¨Ånd multiple global solutions after some genera-

5.1 Problems We Face
167
tions but will lose most of them and converge all the individuals into one peak due
to genetic drift. So we need some special techniques.
The reason for Ô¨Ånding multiple global optimal solutions and other interested local
optimal solutions can be summarized as follows:
‚àôIn real-world applications, there will be some factors that are hard to model math-
ematically, such as manufacturing degree of difÔ¨Åculty, maintenance degree of
difÔ¨Åculty, reliability, etc. Finding multiple solutions with similar quality provides
the decision maker with multiple options to be further determined by other vague
factors.
‚àôMultiple solutions with similar quality is important for Ô¨Ånding a robust solution
and useful for the sensitivity analysis of a problem.
‚àôAs has been discussed, counteracting the effect of genetic drift requires an elab-
orate tradeoff between selective pressure and population diversity. Multimodal
EAs provide useful techniques for such a task, which might be used elsewhere,
such as multiobjective optimization.
‚àôIf the search ability of the algorithm cannot guarantee that the global optimal
solution will be found, then the ability to Ô¨Ånd multiple solutions of similarly
high quality increases the possibility of Ô¨Ånding the global optimal solution [1].
In all, multimodal EAs need to identify and maintain multiple optimal solutions
(global/local) within multimodal domains. Basically, this ability has little relation-
ship to variation operators, so we will only discuss techniques in steps 2.1 and 2.4
of the general solution process of EA introduced in Sect. 3.4.1.
5.1.2 Niche, Species, and Speciation
Analogy is a useful way to innovate. In multimodal EAs, researchers use the con-
cepts of niche, species, and speciation, which come from natural ecosystems, to
describe the feature of a problem and an algorithm.
Niche refers to the conditions of the environment within which a particular type
of living thing can live successfully.2 A species is a class of plants or animals whose
members have the same main characteristics and are able to breed with each other.3
Finally, speciation is the formation of new and distinct species in the course of
evolution 4.
We use niche as a metaphor for the partial solution landscape in which only
one peak resides, species as a metaphor for the subpopulation maintained in a spe-
ciÔ¨Åc peak/niche, and speciation as a metaphor for the formation process of such a
2 From the Oxford Advanced Learner‚Äôs Dictionary.
3 From the Collins Cobuild Dictionary.
4 From Concise Oxford English Dictionary.

168
5 Multimodal Optimization
species.5 EAs using the concepts of niche, species, and speciation to solve multi-
modal problems are called niche EAs.
The common characteristics of one species in multimodal EAs is that individuals
of this species reside in one niche (surrounding one peak), which is often interpreted
as their being close to each other. So we need a distance metric for individuals. The
Hamming distance for binary code and the Euclidean distance for real value code
are often used, and the latter is getting more popular.
In the latter sections of this chapter, we will ignore the difference between a niche
and the attraction basin of a peak.
Every niche has its center, which might be the peak or the centroid of the niche.
For reasons of simplicity, we assume every niche has a radius œÉi, which means
that if the distance of individual j to niche center i, d(i, j), is larger than œÉi, then
individual j does not belong to niche i. Furthermore, we sometimes consider that
the radii of all peaks are the same or assume that setting the same radii for all peaks
will not inÔ¨Çuence the identiÔ¨Åcation of multiple peaks. If so, we say that this problem
is distinguishable.
Figure 5.2 illustrates the number of papers indexed by the SCI on EA-based
multimodal optimization.6















	








 



 	


 




Fig. 5.2 Number of papers indexed by SCI on EA-based multimodal optimization
Even though the number is not impressive, the techniques suggested in niche and
speciation have a strong inÔ¨Çuence on preserving population diversity.
5 The concepts of niche, species, and speciation were used initially to adjust population diversity
and now often refer to multimodal EAs.
6 TS = ((‚Äúmultimodal optimization‚Äù) AND (‚Äúgenetic algorithm‚Äù OR ‚Äúgenetic algorithms‚Äù OR
‚Äúevolutionary computation‚Äù OR ‚Äúevolutionary computing‚Äù OR ‚Äúevolutionary algorithms‚Äù OR
‚Äúevolutionary intelligence‚Äù)). The SCI index ‚ÄúTS‚Äù is for the search topic in the title, the keywords,
and the abstract.

5.2 Sequential Niche
169
5.2 Sequential Niche
We will Ô¨Årst introduce a very interesting algorithm called a sequential niche, sug-
gested by Beasley et al. in 1993 [2]. It is not an EA but a multimodal optimizing
infrastructure.
In a sequential niche, after Ô¨Ånding a global optimal solution using any search
algorithm, such as EAs, we can modify the solution landscape so that the attraction
basin of the found peak will be punished to ensure that the next run of the search
algorithm will Ô¨Ånd another peak. Continuing the above process will Ô¨Ånd multiple
peaks sequentially.
The key to sequential niche is the modiÔ¨Åcation of the solution landscape. Gener-
ally, punishment for the attraction basin of the found peak(s) means decreasing the
objective value of these points, which can illustrated as follows:
G(x,r) =
{ d(x,r)
œÉ
,d (x,r) < œÉ
1
, otherwise
(5.2)
where œÉ is the peak radius, r is the location of the found peak, x is the location of
one point, and d(x,r) is the distance between them. Equation 5.2 means that points
closer to r have a smaller G(x,r) and G(r,r) = 0, which represents the strongest
punishment.
Then the modiÔ¨Åed solution landscape could be illustrated as
f
‚Ä≤ (x) = f (x)√ó
k
‚àè
i=1
G(x,ri)
(5.3)
where f(x) is the raw objective value of point x, f
‚Ä≤(x) is the modiÔ¨Åed objective
value, and k is the number of found peaks. Equation 5.3 means that after Ô¨Ånding a
peak, we decrease the objective value around it and then use the search algorithm
again to Ô¨Ånd another peak.
If we properly set œÉ = 0.1 for Fig. 5.1b after x = 0.1 has been found using Eq. 5.3,
the solution landscape is illustrated by Fig. 5.3a. Then the second run of the search
method might Ô¨Ånd x = 0.3.
But generally we have no idea what the real peak radius is. A larger-than-
necessary peak radius might diminish real peaks, which is illustrated by Fig. 5.3b
with œÉ = 0.2, and a smaller-than-necessary peak radius might generate fake peaks,
which is illustrated by Fig. 5.3c with œÉ = 0.01. Both conditions introduce difÔ¨Åcul-
ties into the following search on the modiÔ¨Åed solution landscape.
There are other ways to deÔ¨Åne the punishment function apart from Eq. 5.2, but
they all suffer from the sensitive parameter œÉ. Another drawback of sequential niche
is its sequential approach, i.e., one run for one peak, which is ineffective for real-
time applications.

170
5 Multimodal Optimization






	










	





H1
3
(a)






	










	



H1
3
(b)






	










	





H1
3
(c)
Fig. 5.3 Settings of peak radius in sequential niche for Fig. 5.1b: (a) œÉ = 0.1, (b) œÉ = 0.2, and (c)
œÉ = 0.01

5.3 Fitness Sharing
171
5.3 Fitness Sharing
5.3.1 Standard Fitness Sharing
The concept of Ô¨Åtness sharing was introduced by Goldberg and Richardson in 1987
[3]. They think of the height of a peak as the resources in the niche. Individuals
of the same species residing in that niche share the resources by decreasing their
Ô¨Åtness values according to the crowdedness of the species. Standard Ô¨Åtness sharing
is carried out before the parental selection process, i.e., by adding the following
steps to the general solution process of EA on page 77.
Standard Fitness Sharing
Step 2.01: Calculate the distance dij between every two individuals i
and j.
Step 2.02: Calculate the sharing function value between every two
individuals i and j as
sh(dij) =
{
1‚àí
( dij
œÉ
)Œ±
dij < œÉ
0
otherwise
(5.4)
where œÉ is the niche radius, Œ± is a control parameter and often set as Œ± = 1,
and sh() deÔ¨Ånes the similarity between i and j within the range [0,1]. Readers
are suggested to draw the sharing function when Œ± = 1 and analyze the reason
we name it as the similarity metric.
Step 2.03: Calculate the niche count for every individuals i as
mi =
popsize
‚àë
j=1
sh(dij)
(5.5)
Step 2.04: Calculate the shared Ô¨Åtness value for every individual i as
f
‚Ä≤
i = fi
mi
(5.6)
where fi is the raw Ô¨Åtness of i.
According to Eq. 5.6, Ô¨Åtness sharing can also be regarded as a kind of Ô¨Åtness scaling
technique, whose intention is also to adjust the selective pressure and the population
diversity.
Suppose the Ô¨Åve peaks have heights of 1, 0.9, 0.7, 0.48, and 0.26, respectively,
in Fig. 5.1b; there is an equilibrium point in the evolving process, i.e., exactly 26
individuals reside in the lowest peak, ..., and exactly 100 individuals reside in the
highest peak. The shared Ô¨Åtness value of every individual is exactly 1, which makes

172
5 Multimodal Optimization
them have the same probability of being selected and cancels the effect of genetic
drift. Thus this equilibrium is stable and these Ô¨Åve peaks could be maintained if
there were no selective bias and the variation operators did not break it.7
So a selection process with no selective bias, such as SUS, is often used in niche
EAs. To avoid variation operators breaking the equilibrium, positive assortative mat-
ing or mating restriction, introduced on page 63, is often used. In Goldberg and
Richardson‚Äôs design, only individuals within a distance of œÉ have the opportunity
to cross over. pm is often set to be very small (or even zero) with the same intention.
According to the above discussions, standard Ô¨Åtness sharing decreases the selec-
tive pressure and promotes population diversity, which is necessary for niche EAs.
By contrast, there are several situations in which we need to scale the raw Ô¨Åtness
value before Ô¨Åtness sharing to balance the selective pressure between scaling and
sharing.
‚àôThere are some global peaks and many local peaks with heights similar to that of
the global peaks. We only want to Ô¨Ånd these global peaks.
‚àôThere is a strong bias in the solution landscape [4]. Suppose in Fig. 5.1b we
add 10 to all objective values, which makes the function value change between
[10,11]. There are three individuals in the population. Two are at the global peak
with a height of 11 and one individual has the minimum value 10 and is far from
global optimal solutions. The shared Ô¨Åtness value for the two at the global peak
is 5.5 and 10 for the third one, which is an unreasonable scheme. If we can use
power law or exponential scaling to enlarge the difference, things will be better.8
What an amazing technique standard Ô¨Åtness sharing is! It provides the possibility
of identifying and maintaining multiple peaks in a single run of EAs, which makes
it one of the most often used techniques in niche EAs. But in-depth analysis for
standard Ô¨Åtness sharing reveals its drawbacks as follows:
1. The niche radius parameter œÉ, which is hard to estimate for real-world problems
and sensitive to the algorithm‚Äôs performance, is required.
2. The niche radii are assumed to be the same in the solution landscape, Eq. 5.4,
which is unrealistic for real-world problems. Empirical equations for estimating
œÉ under the above two assumptions are reported in [5], but these assumptions are
too strong for many applications.
3. Standard Ô¨Åtness sharing does not generate species explicitly. So peaks are found
implicitly. To make things worse, individuals might cross over with individuals
belonging to other species if we use the mating restriction discussed above,9
which might generate useless offspring, known as lethal offspring.
4. One must calculate the distance for O(popsize2) times to get the shared Ô¨Åtness
value, which is an expensive cost when we want to Ô¨Ånd many peaks with large
popsize.
7 Twenty-six individuals for the lowest peak is just an example. The number can be scaled.
8 Consider Œ± = 10 in Eq. 3.42.
9 Why?

5.3 Fitness Sharing
173
Many researchers have suggested various improvements for standard Ô¨Åtness shar-
ing.
5.3.2 Clearing Procedure
Petrowski suggested a clearing procedure in 1996 to overcome drawbacks 2‚Äì4 of
standard Ô¨Åtness sharing [6].10 The clearing procedure also requires œÉ but deÔ¨Ånes it
as the minimum niche radius in the solution landscape. The following steps consti-
tute a clearing procedure, which can be embedded in the general solution process of
EA on page 77.
Clearing Procedure
Step 2.01: Sort the population in descending order of the raw Ô¨Åtness
values.
Step 2.02: Assign the Ô¨Årst individual as the Ô¨Årst species center and
call it a winner.
Step 2.03: Do the following substeps from the second individual to
the last one.
Substep 2.03.1: Assign the current individual as the new species
center if its distance to all the assigned species centers is larger than œÉ, and
call it a winner.
Substep 2.03.2: Assign the current individual to one species if
the distance between the individual and the species center is less than œÉ and
the number of individuals in that species is no larger than Œ∫, and also call it
a winner. Numerical experiments show that Œ∫ is a nonsensitive parameter for
clearing procedure. Users sometimes set Œ∫ = 1.
Substep 2.03.3: Assign others as losers.
Step 2.04: The shared Ô¨Åtness values of winners are the same as their
raw Ô¨Åtness values but, zero for losers. Save winners to another memory.
...
Step 2.4: For every saved winner, a competition between it and its
closest neighbor in the current population is carried out and the winner will
survive.
A clearing procedure is like a winner-takes-all mechanism: winners will not be pun-
ished and losers will die.11 The selective pressure for a clearing procedure is higher
than that of standard Ô¨Åtness sharing. So Petrowski does not use a mating restriction
10 Lee et al. suggested a similar procedure, restricted competition selection, in 1999 [7].
11 If we use m = 1 for winners and m = ‚àûfor losers in Eq. 5.6, the clearing procedure could be
regarded as a kind of Ô¨Åtness sharing technique.

174
5 Multimodal Optimization
with the intention of promoting exploration. With the help of elitism, step 2.4 above,
a clearing procedure can identify and maintain multiple peaks effectively, provided
œÉ is set adequately.
5.3.3 Clustering for Speciation
Speciation is a process of grouping individuals according to their locations. So clus-
tering techniques are suitable for it. The Ô¨Årst study on this topic was published by
Yin and Germay in 1993 [8]. Many clustering techniques [9‚Äì13] and many applica-
tions using hierarchical clustering [14] and nonhierarchical clustering [15, 16] with
a predeÔ¨Åned peak number or niche radius to group individuals have been proposed
recently. Here we just use Yin and Germay‚Äôs original suggestion, adaptive k-means
clustering, to illustrate how to combine clustering and sharing.
A minimum distance between species centers œÉmin (coarsening parameter) and
a maximum distance between an individual and its species center œÉmax (reÔ¨Åning
parameter) are required by Yin and Germay‚Äôs algorithm. The following steps con-
stitute adaptive k-means clustering with Ô¨Åtness sharing, which could be embedded
in the general solution process of EA on page 77.
Adaptive k-means Clustering with Fitness Sharing
Step 2.01: Sort the population in descending order of the raw Ô¨Åtness
values.
Step 2.02: Generate a random integer k in the range [1, popsize].
Step 2.03: Put the Ô¨Årst k individuals into different species as the
species centers and ensure the distance between every two species centers
is larger than œÉmin. If not, merge the species and take the centroid of the new
species as the species center.
Step 2.04: Calculate the distances of other (popsize ‚àík) individuals
to all species centers. If the smallest distance of one individual is larger than
œÉmax, this individual forms a new species and is the center of that species.
Otherwise, the individual belongs to the nearest species. After every alloca-
tion, the species center needs to be recalculated as its centroid. We need to
ensure that the distance between every two species centers is larger than œÉmin.
If not, merge the species and take the centroid of the new species as the species
center.
Step 2.05: After all individuals have been allocated, Ô¨Åx the location
of all species centers, reallocate every individual to its closest species center,
and count the number of individuals in each species.
Step 2.06: Calculate the niche count as

5.3 Fitness Sharing
175
mi = nc ‚àínc √ó
( dic
2œÉmax
)Œ±
, xi ‚ààCc
(5.7)
where nc is the number of individuals in species c, dic is the distance between
individual i and its corresponding species center, Cc is the individual set in
species c, and Œ± is the control parameter, which is often set as 1.
Step 2.07: Calculate the shared Ô¨Åtness using Eq. 5.6.
Yin and Germay also implemented a mating restriction that only allows breeding
between members in the same species. For the control parameters, they suggested
that œÉmax ‚âà(2 ‚àº3) √ó œÉmin for their test problems, but a more delicate choice of
those parameters for generalized multimodal problems is required.
5.3.4 Dynamic Niche Sharing
Miller and Shaw suggested dynamic niche sharing (DNS) in 1996 [17, 18]. DNS
requires two parameters: the minimum peak radius œÉ and the peak number k. The
following steps constitute DNS, which can be embedded in the general solution
process of EA on page 77.
Dynamic Niche Sharing
Step 2.01: Sort the population in descending order of the raw Ô¨Åtness
values.
Step 2.02: Assign the Ô¨Årst individual as the Ô¨Årst species center and
form the Ô¨Årst species.
Step 2.03: Do the following substeps from the individual to the last
individual.
Substep 2.03.1: Form a new species and assign the current in-
dividual as the new species center if its distance to all the assigned species
centers is larger than œÉ and the formed species number is smaller than k.
Substep 2.03.2: Assign the current individual to the closest
species if its smallest distance to all the assigned species centers is smaller
than œÉ.
Substep 2.03.3: Assign the current individual as an independent
individual if its distance to all the appointed species centers is larger than œÉ
and the formed species number is larger than k.
Step 2.04: For those who belong to one species, its niche count is the
number of individuals belonging to that species, call the occupation number.
For independent individuals, use Eq. 5.5 to calculate its niche count.

176
5 Multimodal Optimization
Step 2.05: Calculate the shared Ô¨Åtness value for every individual using
Eq. 5.6.
In the above step 2.03, we can identify the peaks, as the species center, in every
generation dynamically, so this is sometimes referred as dynamic peak identiÔ¨Åcation
(DPI).
Miller and Shaw also suggested two mating restriction rules for selecting par-
ents. In dynamic line-breeding, the Ô¨Årst parent is selected through tournament se-
lection based on shared Ô¨Åtness values and the second parent is its closest dynamic
species center. Dynamic line-breeding always makes individuals belonging to the
same species mate. In dynamic inbreeding, the Ô¨Årst parent is the same as dynamic
line-breeding and the second parent is the one with the highest Ô¨Åtness value and
belongs to the same species as the Ô¨Årst parent within the MF, a user-deÔ¨Åned param-
eter, randomly selected individuals from the mating pool. If there are no individuals
belonging to the same species, the closest one in the mating pool is selected as the
second parent to decrease the possibility of lethal offspring.12 Dynamic inbreeding
relaxes dynamic line-breeding by enabling mating between different members and
species, which might not be the centers of the species.
DNS has the following characteristics.
‚àôIt has an explicit and clear speciation process, which makes further improvement
easy.
‚àôIt assigns the best individual as the peak center, which makes the speciation un-
symmetrical.
‚àôIt has stronger punishment power comparing to standard Ô¨Åtness sharing,13 which
makes it faster than standard Ô¨Åtness sharing.
‚àôIt requires two kinds of information on the solution landscape, which is harder to
provide comparing to standard Ô¨Åtness sharing.
Setting popsize and œÉ for Fitness Sharing
As was discussed above, œÉ is a sensitive and critical parameter for Ô¨Åtness sharing.
Cioppa et al. suggested a way to estimate popsize and œÉ [19]. Their method uses a
procedure, inspired by DPI above, to identify the niche number, ŒΩ(g, popsize,œÉ), in
generation g. Then the effects of different œÉ and popsize on the mean, Œº(popsize,œÉ),
and the standard deviation, Œ¥(popsize,œÉ), of ŒΩ(g, popsize,œÉ) over generations are
used to estimate the proper popsize and œÉ.
12 Line-breeding and inbreeding are genetic terms that describe the mating process of relatives
breeding with one another and related individuals breeding with each other, respectively, which
is the nature of positive assortative mating. The nature of negative assortative mating is called
outcrossing, which describes the mating process of two completely unrelated individuals.
13 Why?

5.3 Fitness Sharing
177
To begin with, they suggest a dynamic niche identiÔ¨Åcation process, which is car-
ried out after we get a niche count for every individual in standard Ô¨Åtness sharing, to
identify the niche maintained in the current generation with a predeÔ¨Åned parameter
œÉ, illustrated as follows.14
Dynamic Niche IdentiÔ¨Åcation
Phase 1: Sorting the population in descending order of the niche counts.
The niche count is calculated by Eq. 5.5. Cioppa et al. use the centroid rather
than its Ô¨Åttest individual to represent a niche.
Phase 2: Generating a dynamic niche set. Unmark all individuals, empty
the dynamic niche set, and do the following steps from the Ô¨Årst individual
i = 1 to the last individual i = popsize.
Step 2.1: If current individual i has not been marked, calculate its
distance to the following individuals j = (i + 1)‚Äìpopsize. If any following
individual j is close to i, i.e., d(i, j) < œÉ, and it is not marked, mark it as a
member of the niche represented by individual i and increase the number of
individuals in that niche by one.
Step 2.2: If the number of individuals in the niche represented by in-
dividual i is no less than two, set individual i as the niche center and species
master and insert it into the dynamic niche set.
Step 2.3: i = i+1 and go to step 2.1.
Phase 3: Submitting the dynamic niche set.
With the help of dynamic niche identiÔ¨Åcation, we can Ô¨Ånd the niche number
ŒΩ(g, popsize,œÉ) in every generation g of standard Ô¨Åtness sharing with parameters
popsize and œÉ. If we make T runs for a set of parameters popsize and œÉ, there is a
matrix with T rows and maxgen columns whose elements are the niche numbers in
every generation and run with parameters popsize and œÉ. Then ŒΩ(g, popsize,œÉ) can
be calculated by averaging the niche numbers in the T runs at generation g. Finally,
Œº(popsize,œÉ) and Œ¥(popsize,œÉ) can be calculated using ŒΩ(g, popsize,œÉ).
In order to estimate the proper popsize and œÉ for a problem, we need to analyze
the effect of different popsize and œÉ on Œº(popsize,œÉ) and Œ¥(popsize,œÉ). Cioppa
et al. provided a thorough analysis and their key results are listed as follows:
‚àôProvided that popsize has been Ô¨Åxed, with an increase of œÉ from underestima-
tion to proper estimation and overestimation, Œº decreases and Œ¥ has a minimum,
which is close to zero, at proper estimation, œÉ = œÉ‚àó, and will Ô¨Ånally drop to zero
with very large œÉ.
‚àôProvided that œÉ has been Ô¨Åxed, with an increase of popsize from underestimation
to proper estimation and overestimation, Œ¥(popsize,œÉ‚àó) will become increas-
ingly smaller.
14 Readers should compare dynamic peak identiÔ¨Åcation and dynamic niche identiÔ¨Åcation.

178
5 Multimodal Optimization
Now we can start running standard Ô¨Åtness sharing with œÉmin and popsizemin,
which are user-deÔ¨Åned parameters. We Ô¨Årst Ô¨Åx popsize = popsizemin and increase
œÉ from œÉmin to Ô¨Ånd the minimum of Œ¥, which is close to zero. We record œÉ‚àóand
popsize‚àóas the current œÉ and popsize, respectively, set the upper bound œÉmax as the
current œÉ, and increase popsize. Then we increase œÉ again from œÉmin to Ô¨Ånd the
minimum of Œ¥. The process will continue until we cannot Ô¨Ånd œÉ‚àóin [œÉmin,œÉmax].
Then we take the previous œÉ‚àóand popsize‚àóas the proper estimation for œÉ and
popsize. In this way, we can Ô¨Ånd the smallest œÉ‚àóand its corresponding popsize‚àó,
which is quite useful for multimodal problems with different but distinguishable
niche radii .
The above process could be regarded as a metasearch process that uses standard
Ô¨Åtness sharing as the local searcher. So this process can Ô¨Ånd the proper parame-
ters for standard Ô¨Åtness sharing at the cost of a rather time-consuming performance
evaluation for different popsize and œÉ.15
Dynamic Fitness Sharing
In 2007, Cioppa et al. suggested dynamic Ô¨Åtness sharing (DFS) to further improve
DNS with only œÉ [20].
They use a procedure called dynamic species identiÔ¨Åcation to identify the species
master in the current generation. Dynamic species identiÔ¨Åcation is quite similar
to dynamic niche identiÔ¨Åcation introduced above. The only difference is that the
sorting in Phase 1 is done according to the raw Ô¨Åtness instead of the niche count.
After dynamic species identiÔ¨Åcation process, we get a dynamic species set with ŒΩ
species, say S1,‚ãÖ‚ãÖ‚ãÖ,SŒΩ, the element of which contains at least two individuals.
So the population P could be divided into (ŒΩ + 1) groups. The (ŒΩ + 1)th group,
S‚àó, contains isolated individuals that do not belong to any species and do not contain
any other isolated individuals.
Then the niche count for individuals belonging to one species is calculated as
follows:
mi = ‚àë
j‚ààSk
sh(dij)
(5.8)
where individual i belongs to species Sk and sh() is deÔ¨Åned by Eq. 5.4. The niche
count for dynamic species identiÔ¨Åcation considers the contribution of distance and
only considers individuals within the same species, which makes it different from
DNS and standard Ô¨Åtness sharing. But the niche count for isolated individuals is set
at 1 so as to improve the chances for those individuals to generate a new species.
Then we can use Eq. 5.6 to calculate the shared Ô¨Åtness value and go to an EA
selection process. Elitism is also adopted in DFS to copy the species masters found
at each generation into the next population.
15 No free lunch again!

5.3 Fitness Sharing
179
With the help of the dynamic species identiÔ¨Åcation, the redesigned niche count
function, and elitism, Cioppa et al. claimed that DFS performs signiÔ¨Åcantly better
than the three other Ô¨Åtness sharing EAs.
5.3.5 Coevolutionary Shared Niching
In 1997, Goldberg and Wang incorporated Ô¨Åtness sharing into coevolution, which
was discussed in Sect. 3.7.1, and suggested coevolutionary shared niching (CSN)
[21].
There are two groups of individuals in CSN, i.e., businessmen and customers.
Businessmen are for locating the peaks and customers are ordinary individuals of
EAs. The number of businessmen is k, which is the same as, or a bit larger than, the
predeÔ¨Åned peak number, and the number of customers is popsize. The Ô¨Åtness value
of a customer is the shared Ô¨Åtness value calculated with Eq. 5.6, and the Ô¨Åtness value
of a businessman is the sum of the raw Ô¨Åtness values of the customers belonging to
this businessman.
CSN also needs œÉ to proceed. The following steps constitute CSN, which could
be embedded in the general solution process of EA on page 77.
One Generation of Coevolutionary Sharing Niching
Phase 1: Assigning every customer to its closest businessman.
Phase 2: Calculating the customer niche count, which is the number of
customers belonging to their corresponding businessman.
Phase 3: Calculating the customer shared Ô¨Åtness value using Eq. 5.6.
Phase 4: Generating a new population of customers with selection (by
shared Ô¨Åtness values), crossover, and mutation.
Phase 5: Updating the businessman group with the following mutation pro-
cess, called imprint. Genomic imprinting is a genetic phenomenon by which
certain genes are expressed depending upon whether they resided in a male or
female in the previous generation.
Step 5.1: For every businessman b, randomly select one customer c
from its current customer group. Calculate its Ô¨Åtness value as a businessman
and distance to other businessmen. If the new one has a higher Ô¨Åtness value
and its smallest distance to other businessmen is larger than œÉ, it replaces the
old businessman.
Step 5.2: If the mutant does not satisfy both of the above conditions,
pick another customer until the mutation has been carried out nlimit times.
When the stop criteria are satisÔ¨Åed, provide businessmen as the found peaks.

180
5 Multimodal Optimization
5.4 Crowding
Standard crowding was Ô¨Årst introduced by De Jong in 1975 as a way of adjusting
population diversity [22]. With the application of crowding to multimodal problems,
standard crowding has been seldom used.
Apart from Ô¨Åtness sharing, crowding has a different perspective. We think that the
solution landscape is crowded with individuals. So the newly generated individual
need to compete with the old ones in order to replace a worse old individual close
to it.
According to the above considerations, crowding is often implemented in the
steady state EA form and the main concern is with the replacement process.
5.4.1 Deterministic Crowding
Deterministic crowding (DC) was suggested by Mahfoud in 1995 [23]. It uses par-
ents to compete with offspring in a deterministic way and does not require any
predeÔ¨Åned parameters.
DC is carried out in the replacement process, i.e., replacing Phase 2 in the general
solution process of EA on page 77 with the following steps.
Deterministic Crowding
Phase 2: Main loop. Repeat the following steps until stop criteria are
satisÔ¨Åed.
Step 2.1: Randomly select two parents p1 and p2 with replacement
from the current population.
Step 2.2: Generate two offspring c1 and c2 using the variation opera-
tors introduced in Sect. 3.2.
Step 2.3: Evaluate the Ô¨Åtness value of offspring, f(c1) and f(c2), and
calculate their distances to parents, i.e., d(p1,c1), d(p1,c2), d(p2,c1), and
d(p2,c2).
Step 2.4: Identify a close competition pair. If [d (p1,c1)+d (p2,c2)] ‚â§
[d (p1,c2)+d (p2,c1)], the competition is between p1 ‚áîc1 and p2 ‚áîc2. Oth-
erwise, the competition is between p1 ‚áîc2 and p2 ‚áîc1.
Step 2.5: Determine the winner. Individuals with higher Ô¨Åtness values
win the competition and will stay in the population. Losers are discarded.
With the competition between parents and offspring, DC maintains the population
diversity effectively. Thus it increases the possibility of identifying and maintaining
multiple peaks.

5.4 Crowding
181
Different variation operators may contribute different locations of the offspring.
Thus it is possible that the offspring will be far from their parents. So genetic drift
might be possible in DC, which explains why sometimes it discovers niches very
fast but only maintains them for a few generations [24]. Gomez suggested an im-
provement for DC in 2004 [25]. His method makes only slight changes to the above
step 2.5. If the winner of the competition is offspring, it can replace the correspond-
ing parent only if the distance between them are smaller than a predeÔ¨Åned parameter
Œ¥. Otherwise, parents will stay in the population.
DC‚Äôs competition rule is deterministic, which promotes convergence but might
suffer from premature. Mengshoel suggested probabilistic crowding in 1999 to in-
troduce down-hill possibility in competition [26]. In the above step 2.5, a offspring
c competes with parent p. Offspring c has the probability p =
f(c)
f(c)+f(p) to replace
parent p. In 2008, Mengshoel and Goldberg further developed probability crowding
and suggested a framework of local tournament algorithms [27].
5.4.2 Restricted Tournament Selection
With the same objective of avoiding excessive distance between the competitors
of DC, Harik suggested restricted tournament selection (RTS) in 1995 [28]. The
difference between RTS and DC shows up in steps 2.3‚Äì2.5, which is illustrated as
follows:
Restricted Tournament Selection
Step 2.3: Evaluate the Ô¨Åtness values of offspring f(c1) and f(c2).
Randomly select w (windowsize) individuals from the current population with
replacement as the comparison set.
Step 2.4: Without loss of generality, suppose d1 and d2 are the closest
individuals in the comparison set to c1 and c2, respectively.
Step 2.5: Determine the winner. If f(c1) > f(d1), replace d1 with c1,
otherwise keep d1 and discard c1. The same operation is carried out between
d2 and c2.
It is necessary to mention that Gomez‚Äôs suggestion and RTS improve DC in Ô¨Ånding
and maintaining multiple peaks in a single run provided that parameters, Œ¥ and w,
have been predeÔ¨Åned correctly.
CedeÀúno suggested multiniche crowding (MNC) in 1995 with a similar idea to
RTS [29]. MNC has more control parameters and gives a better performance with
adequate parameters.

182
5 Multimodal Optimization
5.4.3 Species Conserving Genetic Algorithm
Li et al. suggested a species conserving genetic algorithm (SCGA) in 2002 [1].
SCGA uses œÉ to determine the peak in a niche, denoted as species seeds, which
is similar to the dynamic niche identiÔ¨Åcation and dynamic species identiÔ¨Åcation
discussed in Sect. 5.3.4. SCGA does not use specie seeds to calculate the niche
count and punish the overcrowded species, but saves them until the replacement
process to replace the close and worse new individuals.
The procedure for determining the species seeds is carried out before the selec-
tion process as follows:
Determining Species Seeds
Phase 1: Sorting the population in descending order of the raw Ô¨Åtness val-
ues.
Phase 2: Generating a species seed set (SSS). Empty SSS and do the fol-
lowing steps from the Ô¨Årst individual i = 1 to the last individual i = popsize.
Step 2.1: If the distances of the current individual i to all SSS elements
are larger than œÉ, insert it into SSS. i = i+1.
Step 2.2: If the smallest distances of the current individual i to all SSS
elements is smaller than œÉ, i = i+1.
Phase 3: Submitting SSS.
Then EAs execute selection (with raw Ô¨Åtness value), crossover, and mutation to
generate a new population. Step 2.4 of the general solution process of EA introduced
on page 77 needs to be changed as follows:
Elitism
Step 2.4: Elitism.
Substep 2.4.1: Assign every new individual to its closest species
seed in SSS if the distance between it and its closest species seed is smaller
than œÉ. There might be some new individuals that do not belong to any SSS
element and there might be some SSS elements that do not contain any new
individuals.
Substep 2.4.2: If each SSS element i contains new individuals,
select the worst one (with the smallest raw Ô¨Åtness value) j. If f(i) > f(j),
replace j with i.
Substep 2.4.3: If SSS element i contains no new individuals, se-
lect the worst individual j in the new individuals that do not belong to any
SSS element. Replace j with i.

5.5 Performance Indices for Multimodal Optimization
183
SCGA uses œÉ to Ô¨Ånd SSS and it uses elitism in the replacement process but does not
change the raw Ô¨Åtness, so we introduce it here as a crowding method. In 2003, Li et
al. published another paper where they compared SCGA with CSN [30], which is
introduced in Sect. 5.3.5.
5.5 Performance Indices for Multimodal Optimization
We want niche EAs to Ô¨Ånd multiple peaks in a single run. If such peaks have differ-
ent heights, special considerations are necessary in evaluating the performance of
niche EAs.
Suppose we know the real location and the height of the peaks of the benchmark
problem.16 After niche EAs stop, we can identify the peaks found by these EAs.17 If
the location and height of one peak found by a niche EA are within the threshold of
the real peak, we claim that this peak has been maintained by the niche EA. For real
peak x with height hx, we often deÔ¨Åne 0.9 √ó hx or 0.8 √ó hx as the height threshold
and d(x,y) < Œµ as the distance threshold, where Œµ is the user-allowed discrepancy.
The most often used performance indices for multimodal optimization are listed
as follows.
Effective Number of the Peaks Maintained
We run the niche EAs several times and use the average and the standard deviation
of the effective number of peaks maintained (ENPM) to illustrate the niche EAs‚Äô
ability. A larger ENPM value indicates a better ability to identify and maintain mul-
tiple peaks.
Maximum Peak Ratio
ENPM does not consider the inÔ¨Çuence of peak heights. Thus in 1996, Miller and
Shaw proposed the maximum peak ratio (MPR) [18].
Suppose a problem has k peaks whose heights are h1,‚ãÖ‚ãÖ‚ãÖ,hk. Also suppose that
the niche EA Ô¨Ånds m peaks whose heights are h
‚Ä≤
1,‚ãÖ‚ãÖ‚ãÖ,h
‚Ä≤
m.18 MPR is deÔ¨Åned as fol-
lows:
16 If the explicit form of the optimal solution is not available, the best results of the multiple runs
of different niche EAs might be used as the ‚Äúreal‚Äù location and height of the peaks.
17 Dynamic species identiÔ¨Åcation, introduced in Sect. 5.3.4, might be an option.
18 These peaks need to satisfy the follow conditions: they are within the radius of one real peak;
their heights are 80% or higher than that of the corresponding real peaks; they have the largest
Ô¨Åtness values in the species.

184
5 Multimodal Optimization
MPR =
m
‚àë
i=1
h
‚Ä≤
i
k
‚àë
j=1
h j
(5.9)
If no real peak has been found, the corresponding part in the numerator is zero.
As can be seen from Eq. 5.9, MPR grants higher peaks with more preference.
The largest value of MPR is 1, which means all the peaks have been identiÔ¨Åed and
maintained correctly. A larger MPR value means a better convergence to peaks.
Chi-square-like Performance Criterion
ENPM and MPR do not consider the distribution of individuals in the last genera-
tion.19 Deb and Goldberg suggested a Chi-square-like (CSL) performance criterion
in 1989 to deal with this [5].
Suppose, after a ‚Äúperfect‚Äù run of our niche EA, every individual at the end of the
evolution converges to one peak. We can consider a probabilistic event for peak i as:
an individual is on peak i. It is a Bernoulli experiment. It‚Äôs natural and straightfor-
ward to suppose that the probability pi of event A is
pi =
hi
k
‚àë
l=1
hl
(5.10)
where hl is the height of peak l and k is the relevant peak number.
Then we can deÔ¨Åne a Bernoulli distribution random number y j for individual j
in the Ô¨Ånal population. y j = 1 means individual j is on peak i with probability pi,
and yj = 0 means it is not on peak i with probability 1‚àípi. It is easy to determine
that the mean and the standard deviation of this Bernoulli distribution are pi and
pi(1‚àípi), respectively.
For the Ô¨Ånal population, there are popsize Bernoulli distribution random numbers
for peak i. According to the central limit theorem, if popsize is large enough,
y1 +‚ãÖ‚ãÖ‚ãÖ+ypopsize ‚àípopsize√ó pi
‚àö
popsize√ó pi √ó(1‚àípi)
‚àºN (0,1)
(5.11)
We can then deÔ¨Åne the number of individuals on peak i as xi = y1+‚ãÖ‚ãÖ‚ãÖ+ypopsize. And
by deÔ¨Åning Œºi = popsize√ó pi and œÉ2
i = popsize√ó pi √ó(1‚àípi), we can reformulate
Eq. 5.11 as follows:
xi ‚àíŒºi
œÉi
‚àºN (0,1)
(5.12)
19 We want to populate our individuals with the proportion to peaks‚Äô heights, which was discussed
in Sect. 5.3.1.

5.6 Application Example
185
So for k peaks in the solution landscape, we have k standard normally distributed
random numbers x1,‚ãÖ‚ãÖ‚ãÖ,xk. According to the deÔ¨Ånition of chi-square distribution,
k
‚àë
i=1
(xi ‚àíŒºi)2
œÉ2
i
‚àºX2 (k)
(5.13)
where X2(k) is the chi-square distribution with (k) degrees of freedom.
To further emphasize the Ô¨Ånal individuals that do not belong to any peak, Deb and
Goldberg consider them as the (k + 1)th group. It can be represented as a random
number xk+1 = popsize ‚àí
k
‚àë
i=1
xi whose mean and variance can be calculated easily
as Œºk+1 = 0 and œÉ2
k+1 =
k
‚àë
i=1
œÉ2
i , respectively.
So CSL can be illustrated as follows:
CSL =
0
1
1
‚é∑
k+1
‚àë
i=1
(xi ‚àíŒºi)2
œÉ2
i
(5.14)
where xi is the real number of individuals on peak i by the end of a niche EA, Œºi and
œÉi can be calculated from previous discussions with the known height of every peak
of the benchmark problem.
By comparing Eqs. 5.14 and 5.13 we can see that CSL borrows the form of chi-
square distribution and can be used to evaluate the distribution of a population, i.e.,
if the number of individuals on every peak equals the mean of that peak, the CSL
value is zero. A smaller CSL value means a better individual distribution.
5.6 Application Example
The distributed multipump Raman ampliÔ¨Åer (DMRA) can offer ultra-wide and ul-
traÔ¨Çat gain bandwidth, improve noise characteristics, and realize ultralong-haul sys-
tems reach. Additionally, DMRA can mitigate Ô¨Åber nonlinear effects compared to
erbium-doped Ô¨Åber ampliÔ¨Åers. So the design of the DMRA is quite attractive. In
2004, Liu and Lee suggested a clustering-based niche EA to provide multiple DM-
RAs with equal quality in a single run [31].
Liu and Lee‚Äôs algorithm needs the peak number k and smallest peak radius œÉ as
the control parameters. The solution process can be illustrated as follows:
Phase 1: Initialization.
Step 1.1: Assign the parameters for the niche EA, such as k, œÉ, and
other control parameters for SGA.
Step 1.2: Generate popsize uniformly distributed individuals ran-
domly to form the initial population and evaluate Ô¨Åtness values. gen = 0.

186
5 Multimodal Optimization
Phase 2: Main loop. Repeat the following steps until the stop criteria are
satisÔ¨Åed.
Step 2.1: Sort the population in descending order of the raw Ô¨Åtness
value.
Step 2.2: Find k species centers.
Substep 2.2.1: Assign the Ô¨Årst individual as the Ô¨Årst conÔ¨Årmed
species center, and mark it.
Substep 2.2.2: Select k individuals orderly from the population
that satisfy the following two conditions: the individuals are not marked; the
distances from the individual to all of the conÔ¨Årmed species centers are larger
than œÉ.
Substep 2.2.3: Calculate the sum of distances between each of k
individuals and all the conÔ¨Årmed species centers, assign the individual with
the largest sum of distances as the next conÔ¨Årmed species center, and mark it.
Substep 2.2.4: Repeat substeps 2.2.2 and 2.2.3 until k species
centers are conÔ¨Årmed.
Step 2.3: Allocate other individuals to their nearest species center.
Substep 2.3.1: For each unmarked individual, calculate its dis-
tances to all the conÔ¨Årmed species centers.
Substep 2.3.2: Select the shortest distance and allocate the indi-
viduals to their corresponding species.
Substep 2.3.3: Repeat substeps 2.3.1 and 2.3.2 until all popsize
individuals are allocated.
Step 2.4: Calculate the niche count for each individual, which is equal
to the individual number of the species it belongs to. Calculate its shared Ô¨Åt-
ness value using Eq. 5.6. Select M individuals to be saved as the elitism set
according to the shared Ô¨Åtness values.
Step 2.4: Selection (using shared Ô¨Åtness value), crossover, and muta-
tion are carried out to generate a new population. g = g+1.
Step 2.5: Replace M worst individuals (according to the raw Ô¨Åtness
value) of the new population with individuals in the elitism set.
Phase 3: Submitting the Ô¨Ånal k species centers in the Ô¨Ånal generation as
the results of the niche EA.
The optimal results show that the broadest bandwidth of a DMRA can be 100.08
nm with just Ô¨Åve pumps under the design conditions of an ON-OFF gain of 10 dB,
a relative gain Ô¨Çatness of 0.1, and a Ô¨Åber length of 80 km.
Liu and Lee assume that k = 6, i.e., they want to Ô¨Ånd approximately six peaks in
the solution landscape, and their niche EA provides four design schemes with the
broadest bandwidths being 100.08 nm and two design schemes with bandwidths of
94.76 nm and 92.28 nm, respectively.

5.7 Summary
187
5.7 Summary
Multimodal problems have real-world applications. Niche EAs can provide both
multiple high-quality solutions for multimodal problems and techniques for improv-
ing population diversity.
Fitness sharing downscales the Ô¨Åtness values of overcrowded individuals so as
to maintain an equilibrium in which individuals are distributed proportionally to the
peak heights. These techniques contain useful information for adjusting the selective
pressure in a selection process according to different requirements.
Crowding has a competition mechanism between the offspring and their close
parents (or other close individuals) within the framework of a steady state EA. These
techniques also contain useful information for adjusting the selective pressure in a
replacement process according to different requirements.20 Apart from the above
classiÔ¨Åcation, we can give the taxonomy for niche EAs in a broader view as illus-
trated in Fig. 5.4.
Niche EAs can be classiÔ¨Åed into three categories. Sequential niche tries to solve
multimodal problems by modifying the objective function in a serial way. Whole
population niche tries to improve population diversity in a global way, and sub-
population niche uses rarely communicated subpopulations to improve population
diversity in a local way.
Furthermore, we can classify Ô¨Åtness sharing according to the required informa-
tion before calculation. Standard Ô¨Åtness sharing, adaptive k-means clustering with
Ô¨Åtness sharing, and clearing procedure belong to the ‚Äúniche radii‚Äù group. The ‚Äúniche
number‚Äù group includes the algorithms reported in [8, 15]. DNS, DFS, and CSN be-
long to the ‚Äúniche radii and number‚Äù group. The ‚Äúblind‚Äù group does not require any
predeÔ¨Åned information. Interested readers are referred to [32, 33].
Deterministic crowding involves competition with parents but restricted tourna-
ment selection, multiniche crowding, and species conserving genetic algorithm have
competition with others.
In the island model, the population is divided into multiple subpopulations,
which evolve independently for a Ô¨Åxed number of generations. After several gener-
ations the procedure of migration arises, in which a portion of each subpopulation
migrates to other subpopulations [34].
The Tagging algorithm gives individuals different tags by which we can divide
them into several groups. Only parents with the same tags are able to breed. Spears
claimed that tagging was shown to be a rather efÔ¨Åcient implementation [35].
The Multinational model considers the population as the world of ‚Äúnations,‚Äù
‚Äúgovernments,‚Äù and ‚Äúpoliticians,‚Äù and also uses the migration of individuals and
the merging of subpopulations in the evolving process [36].
The effective understanding of this chapter means in-depth comprehension of re-
lated concepts, e.g., niche, species, and speciation, grasping the essence of Ô¨Åtness
sharing and crowding, and being able to adopt adequate selective pressure adjust-
20 Readers are referred to Sect. 3.4.1 for further information.

188
5 Multimodal Optimization














Fig. 5.4 The taxonomy of niche EAs
ment techniques to maintain population diversity, which might contribute to Ô¨Ånding
multiple high-quality solutions in a single run.
In all, designing an EA for multimodal optimization is the art of the tradeoff
between converging to peaks and distributing individuals in different peaks.
Suggestions for Further Reading
There are two intensive surveys and comparisons for niche EAs published in 1998
and 2006 respectively. Interested readers are referred to [37, 38].
Shir and B¬®ack use CMA to solve multimodal problems by ES and also self-
adaptively controlled peak radius œÉ. Readers interested in (derandomized) ES are
referred to Shir‚Äôs Ph.D. thesis, published in 2008 [32], and a paper published in
2009 [39].

5.7 Summary
189
In 2005, Wei and Zhao combined a clearing procedure (Sect. 5.3.2) and Nelder‚Äì
Mead‚Äôs simplex method (Sect. 2.4.1) to Ô¨Ånd the only global optimal solution [40].
Although Ô¨Ånding the only global optimal solution is not the task of this chapter,
we still suggest it as an application of the clearing procedure and want our readers
to understand that innovative ideas from other Ô¨Åelds may help you in solving your
problem, which is why we explain so many intuitive ideas in this book.
The must-read papers of this chapter are [37] for a general survey, [3] for stan-
dard Ô¨Åtness sharing, [23] for deterministic crowding, [18] for DNS, and [20] for an
elaborate way to estimate the key control parameters œÉ and popsize.
Exercises and Potential Research Projects
5.1. Why does standard Ô¨Åtness sharing technique not suffer from genetic drift after
all relevant peaks have already been found?
5.2. Sareni and Krahenbuhl have reported that tournament selection is not as good
as SUS in standard Ô¨Åtness sharing with a mating restriction [37]. Use the benchmark
problems in Appendix and PI in Sect. 5.5 to compare SUS and unbiased tournament
selection, introduced in Sect. 3.3.5. Use the techniques introduced in Sect. 3.6 to
do a fair comparison and draw statistical conclusions according to your numerical
experiments.
5.3. How many distance calculations are necessary for clearing, adaptive k-means
clustering with Ô¨Åtness sharing, DNS, and CSN discussed in Sect. 5.3?
5.4. Produce a table to compare the differences between various sharing techniques,
e.g., standard Ô¨Åtness sharing, clearing, adaptive k-means clustering with Ô¨Åtness shar-
ing, DNS, and CSN discussed in Sect. 5.3. The content of comparison might in-
clude the information about the solution landscape necessary to proceed, whether
or not there exists explicit speciation, how to obtain the niche count, whether the
species/niche center is the peak or the centroid, and the time of the distance calcu-
lation.
5.5. Implement standard Ô¨Åtness sharing and deterministic crowding in your pro-
gramming environment. Use the benchmark problems in Appendix and PI in Sect.
5.5 to compare these two classical niche EAs. Use the techniques introduced in
Sect. 3.6 to do a fair comparison and draw statistical conclusions according to your
numerical experiments.
5.6. Suggest a way to expand DE into multimodal optimization. Use the benchmark
problems in Appendix and PI in Sect. 5.5 to compare your novel algorithm with
standard Ô¨Åtness sharing. Use the techniques introduced in Sect. 3.6 to do a fair com-
parison and draw statistical conclusions according to your numerical experiments.
5.7. What is the difference between dynamic niche identiÔ¨Åcation, dynamic species
identiÔ¨Åcation, and determining the species seeds? The Ô¨Årst two procedures were
introduced in Sect. 5.3.4 and the last one in Sect. 5.4.3.

190
5 Multimodal Optimization
References
1. Li J, Balazs ME, Parks GT et al (2002) A species conserving genetic algorithm for multimodal
function optimization. Evol Comput 10(3):207‚Äì234
2. Beasley D, Bull D, Martin R (1993) A sequential niche technique for multimodal function
optimization. Evol Comput 1(2):101‚Äì125
3. Goldberg DE, Richardson J (1987) Genetic algorithms with sharing for multimodal function
optimization. In: Proceedings of the 2nd international conference on genetic algorithms and
their applications, pp 41‚Äì49
4. Ursem R (2001) When sharing fails. In: Proceedings of the IEEE congress on evolutionary
computation, pp 873‚Äì879
5. Deb K, Goldberg DE (1989) An investigation of niche and species formation in genetic func-
tion optimization. In: Proceedings of international conference on genetic algorithms, pp 42‚Äì50
6. Petrowski A (1996) A clearing procedure as a niching method for genetic algorithms. In:
Proceedings of the IEEE conference on evolutionary computation, pp 798‚Äì803
7. Lee C, Cho D, Jung H (1999) Niching genetic algorithm with restricted competition selection
for multimodal function optimization. IEEE Trans Magnet 35(3):1722‚Äì1725
8. Yin X, Germay N (1993) A fast genetic algorithm with sharing scheme using cluster analysis
methods in multimodal function optimization. In: Proceedings of international conference on
artiÔ¨Åcial neural nets and genetic algorithms, pp 450‚Äì457
9. Duda RO, Hart PE, Stork DG (2000) Pattern classiÔ¨Åcation, 2nd edn. Wiley-Interscience, New
York
10. Bishop CM (2007) Pattern recognition and machine learning. Springer, Berlin Heidelberg
New York
11. Xu R, Wunsch D (2008) Clustering. Wiley-IEEE, New York
12. Alpaydin E (2004) Introduction to machine learning. MIT Press, Cambridge, MA
13. Xu R, Wunsch D (2005) Survey of clustering algorithms. IEEE Trans Neural Netw 16(3):645‚Äì
678
14. Shan S, Xinjie Y (2006) Multi-peak function optimization using a hierarchical clustering
based genetic algorithm. In: Proceedings of the sixth international conference on intelligent
systems design and applications, pp 425‚Äì428
15. Yu X (2005) A novel clustering Ô¨Åtness sharing genetic algorithm. In: Proceedings of the
international conference on natural computation, pp 1072‚Äì1079
16. Ando S, Sakuma J, Kobayashi S (2005) Adaptive isolation model using data clustering for
multimodal function optimization. In: Proceedings of the ACM conference on genetic and
evolutionary computation, pp 1417‚Äì1424
17. Miller BL, Hansen N, Shaw MJ (1995) Genetic algorithms with dynamic niche sharing
for multimodal function optimization. Tech. Rep. 95010, University of Illinois at Urbana-
Champaign
18. Miller B, Shaw M (1996) Genetic algorithms with dynamic niche sharing for multimodal
function optimization. In: Proceedings of IEEE conference on evolutionary computation, pp
786‚Äì791
19. Cioppa AD, Stefano CD, Marcelli A (2004) On the role of population size and niche radius in
Ô¨Åtness sharing. IEEE Trans Evol Comput 8(6):580‚Äì592
20. Cioppa AD, Stefano CD, Marcelli A (2007) Where are the niches? dynamic Ô¨Åtness sharing.
IEEE Trans Evol Comput 11(4):453‚Äì465
21. Goldberg D, Wang L (1997) Adaptive niching via coevolutionary sharing. In: Miettinen K
(ed) Genetic algorithms and evolution strategies in engineering and computer Science. Wiley,
New York, pp 21‚Äì38
22. Jong KAD (1975) An analysis of the behavior of a class of genetic adaptive systems. Ph.D.
thesis, University of Michigan
23. Mahfoud S (1995) Niching methods for genetic algorithms. Ph.D. thesis, University of Illinois
at Urbana-Champaign

References
191
24. Mahfoud SW (1995) A comparison of parallel and sequential niching methods. In: Proceed-
ings of the international conference on genetic algorithms, pp 136‚Äì143
25. Gomez J (2004) Self adaptation of operator rates for multimodal optimization. In: Proceedings
of the IEEE conference on evolutionary computation, pp 1720‚Äì1726
26. Mengshoel OJ (1999) EfÔ¨Åcient Bayesian network inference: genetic algorithms, stochastic
local search, and abstraction. Ph.D. thesis, University of Illinois at Urbana-Champaign
27. Mengshoel OJ, Goldberg DE (2008) The crowding approach to niching in genetic algorithms.
Evol Comput 16(3):315‚Äì354
28. Harik G (1995) Finding multimodal solutions using restricted tournament selection. In: Pro-
ceedings of the international conference on genetic algorithms, pp 24‚Äì31
29. CedeÀúno W (1995) The multi-niche crowding genetic algorithm: analysis and applications.
Ph.D. thesis, Universiteit California at Davis
30. Li J, Balazs ME, Parks GT et al (2003) Erratum: a species conserving genetic algorithm for
multimodal function optimization. Evol Comput 11(1):107‚Äì109
31. Liu X, Lee B (2004) Optimal design of Ô¨Åber raman ampliÔ¨Åer based on hybrid genetic algo-
rithm. IEEE Photon Technol Lett 16(2):428‚Äì430
32. Shir OM (2008) Niching in derandomized evolution strategies and its applications in quantum
control. Ph.D. thesis, Leiden University
33. Yu X (2005) Fitness sharing genetic algorithm with self-adaptive annealing peaks radii control
method. In: Proceedings of the international conference on natural computation, pp 1064‚Äì
1071
34. Martin W, Lienig J, Cohoon J (1997) Island migration models: evolutionary algorithms based
on punctuated equilibria. In: B¬®ack T, Fogel DB, Michalewicz Z (eds) Handbook of evolution-
ary computation. Oxford University Press, Oxford, UK, C6.3:1‚Äì16
35. Spears WM (1994) Simple subpopulation schemes. In: Proceedings of the 3rd annual confer-
ence on evolutionary programming, pp 296‚Äì307
36. Ursem RK (1999) Multinational evolutionary algorithms.
In: Proceedings of the IEEE
congress on evolutionary computation, pp 1633‚Äì1640.
37. Sareni B, Krahenbuhl L (1998) Fitness sharing and niching methods revisited. IEEE Trans
Evol Comput 2(3):97‚Äì106
38. Singh G, Deb K (2006) Comparison of multi-modal optimization algorithms based on evolu-
tionary algorithms. In: Proceedings of the ACM annual conference on genetic and evolution-
ary computation, pp 1305‚Äì1312
39. Shir OM, B¬®ack T (2009) Niching with derandomized evolution strategies in artiÔ¨Åcial and real-
world landscapes. Nat Comput 8(1):171‚Äì196
40. Wei L, Zhao M (2005) A niche hybrid genetic algorithm for global optimization of continuous
multimodal functions. Appl Math Comput 160(3):649‚Äì661


Chapter 6
Multiobjective Optimization
Abstract EAs are developed to solve real-world problems, such as designing and
scheduling. In real conditions, there are many requirements to fulÔ¨Åll. In previous
chapters, we sometimes wanted to model them into constraints because it is hard
to compare two objectives simultaneously.1 Pareto gave us the idea of dominance,
so we can divide the relationship between two vectors into three categories: one is
better than the other, the converse is true, or they are incomparable or incommen-
surable. For problems with multiple objectives, there exist many ‚Äúgood‚Äù points that
are no worse than any other point in the objective space. EAs contain a group of
individuals. So if we can distribute the individuals evenly on these ‚Äúgood‚Äù points, it
will be very helpful for designers and decision makers. This chapter discusses how
to use EAs to solve such problems. It is a fascinating and hot research area. You will
experience the shining wisdom of other researchers, which will deepen considerably
your understanding of EAs.
6.1 Introduction
6.1.1 Problems We Face
As was introduced in Chap. 1, there are many realistic applications that call for
multiple objectives. Let us start with a Ô¨Åctitious product you are designing. Every
product has many features to describe how good it is. Among them, you mainly care
about failure rate and cost. As a designer, you want the cost to be as low as possible,
which makes your product more competitive in the harsh market. At the same time,
you must consider the failure rate seriously so as to diminish the return repair rate.
Unfortunately, these two considerations are in conÔ¨Çict. Improving the failure rate
means utilizing more reliability approaches, which will deÔ¨Ånitely increase the cost.
The dilemma can be illustrated by Fig. 6.1.
1 Consider how to compare two vectors in 2-D space.
193

194
6 Multiobjective Optimization
According to previous discussions, square points a1 and a2 are two incommensu-
rable designs, sometimes referred as tradeoff designs, and circle point a3 is worse
than square point a2.





	










	


#!
)



Fig. 6.1 Failure rate vs. cost of a Ô¨Åctitious product
Vilfredo Pareto, an Italian economist, gave the deÔ¨Ånitions of the relationships
between these designs, which will be discussed in the next subsection.
6.1.2 Terminologies
Let us Ô¨Årst formulate the problem and then give the deÔ¨Ånitions of the terms used in
this chapter.
Consider the following optimization problem:2
min {z1 = f1 (x),z2 = f2 (x),‚ãÖ‚ãÖ‚ãÖ,zm = fm (x)}
s.t.
gi (x) ‚©Ω0,
i = 1,‚ãÖ‚ãÖ‚ãÖ,q
h j (x) = 0,
j = q+1,‚ãÖ‚ãÖ‚ãÖ,k
x ‚àà‚Ñùn
(6.1)
where x is the decision variable, fi is objective i, gi is inequality constraint i, and h j
is equality constraint j.
2 Unless speciÔ¨Åcally noted, minimum objectives are discussed in this chapter.

6.1 Introduction
195
There are m objectives in Eq. 6.1, which makes the model a multiobjective opti-
mization problem (MOP).3
Two spaces need to be deÔ¨Åned. Decision space (or parameter space) is deÔ¨Åned
as follows:
S =
{
x ‚àà‚Ñùn 		gi (x) ‚â§0,
i = 1,‚ãÖ‚ãÖ‚ãÖ,q ‚à©h j (x) = 0,
j = q+1,‚ãÖ‚ãÖ‚ãÖ,k
}
(6.2)
where x ‚ààS is called a decision vector(or parameter vector).
Criterion space(or objective space, Ô¨Åtness space) is deÔ¨Åned by Eq. 6.3. z ‚ààZ is
called an objective vector.
Z = {z ‚àà‚Ñùm ‚à£z1 = f1 (x),z2 = f2 (x),‚ãÖ‚ãÖ‚ãÖ,zm = fm (x)}
(6.3)
The most important term given by Pareto is ‚Äúdominance.‚Äù Dominance can be
deÔ¨Åned in either objective space or parameter space.
In objective space, ‚Äúpoint i dominates point j‚Äù means
zi
l ‚â§zj
l ,
‚àÄl ‚àà{1,2,‚ãÖ‚ãÖ‚ãÖ,m}
zi
k < zj
k,
‚àÉk ‚àà{1,2,‚ãÖ‚ãÖ‚ãÖ,m}
(6.4)
where zi
l is the lth objective value of point i.
In parameter space, ‚Äúpoint i dominates point j‚Äù means
f
(
xi)
l ‚â§f
(
xj)
l ,
‚àÄl ‚àà{1,2,‚ãÖ‚ãÖ‚ãÖ,m}
f
(
xi)
k < f
(
x j)
k ,
‚àÉk ‚àà{1,2,‚ãÖ‚ãÖ‚ãÖ,m}
(6.5)
where f
(
xi)
l is objective l‚Äôs value of point i.
We use i ‚â∫j to demonstrate that point i dominates j.4 If zi
l = zj
l , ‚àÄl ‚àà{1,2,‚ãÖ‚ãÖ‚ãÖ,m},
we say i is equivalent to j, expressed as i = j. If i either dominates or is equivalent
to j, we say i covers j. If zi
l ‚â§z j
l , ‚àÄl ‚àà{1,2,‚ãÖ‚ãÖ‚ãÖ,m}, we say i weakly dominates
j, expressed as i ‚â∫j. If zi
l < z j
l , ‚àÄl ‚àà{1,2,‚ãÖ‚ãÖ‚ãÖ,m}, we say i strongly dominates j,
expressed as i ‚â∫‚â∫j.5
In the objective space, we are most caring for those points which are not dom-
inated by any other point. Here comes the deÔ¨Ånition of nondominated solution (or
Pareto optimal solution). A point z0 ‚ààZ in objective space is a nondominated solu-
tion if and only if there is not any point z ‚ààZ which dominates z0. In other words,
any improvement in one objective of a nondominated solution will cause deteriora-
3 Other terms include multicriteria optimization problem, vector optimization problem, multiat-
tribute optimization, and multicriteria decision making.
4 i ‚â∫j only means that i is ‚Äúbetter‚Äù than j. For maximum problems, Eqs. 6.4 and 6.5 have different
forms. But we still use i ‚â∫j.
5 We strongly encourage readers to determine the areas that are dominated, weakly dominated, and
strongly dominated by a2 in Fig. 6.1. Further, readers are urged to draw the covering relationship
between these three sets.

196
6 Multiobjective Optimization
tion in at least one other objective. All the nondominated points consist the Pareto
front,6 often expressed as PF*.
Analogously, a point x0 ‚ààS in parameter space is an efÔ¨Åcient solution (or non-
inferior solution) if and only if there is no point x ‚ààS that dominates x0. All the
effective solutions consist of the Pareto set,7 often expressed as P*. PF* is the map
of P*.
Here is a simple example. For the optimization model expressed by Eq. 6.6,
min z1 (x1,x2) = x1 ‚àí3x2
min z2 (x1,x2) = 3x1 +x2
s.t. g1 (x1,x2) = x1 +2x2 ‚àí2 ‚©Ω0
g2 (x1,x2) = 2x1 +x2 ‚àí2 ‚â§0
x1,x2 ‚â•0
(6.6)
Figure 6.2 illustrates the above deÔ¨Ånitions in which the wide line is the P* and PF*
of the model, respectively.












,-.	/.	0
,/	0
,/0
,-	/0
1)#2)


,/0
,/0
,.	/.	0
,/0


)
Fig. 6.2 Illustration of Pareto solution set and Pareto front
6.1.3 Why Are Evolutionary Algorithms Good at Multiobjective
Optimization Problems?
Apart from single objective optimization problems,8 MOPs generally do not have
a single optimal solution (cf. Fig. 6.2), which means there are many solutions with
incommensurable quality for designers and decision makers. This characteristic of
MOPs is a two-edged sword. In one respect, there might be some features that are
hard to mathematically model. So designers welcome multiple solutions with the
same good quality (or incommensurable quality) so that they can select from them
6 Other names include Pareto frontier and Pareto optimal front.
7 Other names include Pareto solution set and Pareto optimal set.
8 Here we neglect the multimodal problem discussed in Chap. 5.

6.1 Introduction
197
according to their subjective preferences. But on the other hand, special concerns
are necessary to handle the solutions, especially when the number of nondominated
solutions in ‚à£PF‚àó‚à£9 is of substantial size. Considering both aspects, we generally
welcome moderate information about the objective space. So an algorithm is con-
sidered to be efÔ¨Åcient if it provides an adequate number of nondominated solutions
in a single run.
EAs designed for MOPs are called multiobjective EAs (MOEAs), and this kind
of optimization is called evolutionary multiobjective optimization (EMO).
What is the most distinguishing feature of EAs? A group of individuals evolve
together to search the solution landscape in a parallel way. So if we can control
the diversity of the population by preventing all individuals from converging to one
point and encouraging them to reside in different points on the Pareto front, EAs
could provide designers with at most popsize nondominated solutions in a single
run. That would be fantastic!
Apart from the above reason, by generating many nondominated solutions and
researching the location of these solutions, we could gain insights into a problem‚Äôs
solution landscape, which could facilitate the design of the problem-speciÔ¨Åc op-
erators in MOEAs. As has been discussed before, problem-speciÔ¨Åc operators will
dramatically alleviate search difÔ¨Åculties.
Figure 6.3 illustrates the number of papers indexed by the SCI on MOEAs and
EMO.10 By watching the increased frequency of papers on the topic and comparing
to other SCI-indexed graphs in this textbook, it is easy to conclude that MOEAs
have conspicuously attracted research interests.





















 




 



<HDU
1XPEHURISDSHUV
Fig. 6.3 Number of papers indexed by SCI on MOEAs
Let us review the MOEAs that are of interest to us.
9 The size of PF*, i.e., how many points there are in the PF*.
10 TS = ((‚Äúmultiobjective optimization‚Äù OR ‚Äúmulti-objective optimization‚Äù) AND (‚Äúgenetic algo-
rithm‚Äù OR ‚Äúgenetic algorithms‚Äù OR ‚Äúevolutionary computation‚Äù OR ‚Äúevolutionary computing‚Äù
OR ‚Äúevolutionary algorithms‚Äù OR ‚Äúevolutionary intelligence‚Äù)). The SCI index ‚ÄúTS‚Äù is for the
search topic in the title, the keywords, and the abstract.

198
6 Multiobjective Optimization
6.2 Preference-based Approaches
Approaches to deal with multiple objectives can be divided roughly into two parts:
preference-based approaches and construction approaches. If some preference in-
formation is provided before optimization (such as which objective decision maker
cares more), preference-based approaches can embed the user requirement into
the algorithm and generate high-quality solutions quickly. But under blind circum-
stances, constructing approaches can provide multiple nondominated solutions for
users to make decisions.
Generally, preference-based approaches transform MOPs into single-objective
problems in different ways. So in this section, we only discuss the transformation
procedure. After that, single-objective problems can be optimized by any algorithm
(such as GA).
6.2.1 Weight Sum Method
If the decision maker can provide the relative importance for each objective, MOPs
will be easier. Let us consider the design problem of Fig. 6.1. If the designer thinks
the importance of the cost is double that of the failure rate, she can express this idea
mathematically in the following way:
Satisfaction = 2
3cost+ 1
3failure rate
(6.7)
In this way, if there exists an algorithm that can optimize Eq. 6.7, which is a
single-objective problem, then the design problem has been solved. This is the initial
idea of the weight sum method, and sometimes formulations like Eq. 6.7 are called
the aggregate Ô¨Åtness function.
6.2.1.1 Fixed Weight Sum Method
Let us Ô¨Årst introduce the weight sum method formally. If the designer can provide m
relative importance weights for m objectives, the MOP (Eq. 6.1) can be transformed
as
min
m
‚àë
i=1
œâi zi =
m
‚àë
i=1
œâi fi (x)
s.t.
gi (x) ‚â§0,
i = 1,‚ãÖ‚ãÖ‚ãÖ,q
h j (x) = 0,
j = q+1,‚ãÖ‚ãÖ‚ãÖ,k
x ‚àà‚Ñùn
(6.8)

6.2 Preference-based Approaches
199
where œâi ‚àà[0,1] is the weight of objective i assigned by the user in advance.11 That
is the model of the Ô¨Åxed weight sum method.
6.2.1.2 Random Weight Sum Method
But actually the Ô¨Åxed weight sum method requires too much from the decision
maker, who generally cannot provide the overall relative importance weight for all
objectives.12 In addition, only one nondominated solution can be found by success-
fully optimizing Eq. 6.8,13 which makes the second purpose of researching MOPs
hard to fulÔ¨Åll.
This is where the random weight sum method comes in. Suppose we have no
idea of any preference and we also want to use the weight sum method to explore
the Pareto front. We could do the following transformation:
œâk =
rk
m
‚àë
j=1
rj
, k = 1,2,‚ãÖ‚ãÖ‚ãÖ,m
(6.9)
where r j ‚àºU(0,1) is the jth uniformly distributed random number. The MOP could
be transformed into a single-objective problem by substituting Eq. 6.9 into Eq. 6.8.
6.2.1.3 Adaptive Weight Sum Method
Please stop for a few minutes before continuing with this subsection by considering
why we need other methods after the random weight sum method has been intro-
duced.
The main reason is scale. Consider this situation. There are two objectives. Ob-
jective 1 is for cost and its value is in the interval [103 ‚àº106]. Objective 2 is for the
failure rate and its value is in the interval [0.01 ‚àº0.85]. If we use Eqs. 6.8 and 6.9
to do the optimization, what is the expected result? The main focus is on objective
1 and there is little concern for objective 2, which makes the exploration process for
objective 2 incomplete.
How can we improve the exploration ability of the random weight sum method?
What about assigning the weight according to the objective function value interval?
How do we deÔ¨Åne the objective function value interval before the optimization pro-
cess? It is difÔ¨Åcult.14 But EAs are population-based algorithms that contain a group
11 The user of the Ô¨Åxed weight sum method needs to assign normalized m weights that satisfy
m
‚àë
i=1
œâi = 1. Readers are encouraged to think why there is an additional requirement.
12 Readers are urged to reread Sect. 3.5.1 for parameter tuning and Sect. 4.3 for the static penalty
function.
13 Why is the optimal solution of Eq. 6.8 an efÔ¨Åcient solution?
14 If we know the range for all objective values, we can normalize them to the range [0,1] and
assign weights on objectives for the Ô¨Åxed or random weight sum method.

200
6 Multiobjective Optimization
of individuals. The population of the current generation represents some kind of
sampling over the solution space. So we can get the approximate objective function
value interval according to the current population. This is where the deÔ¨Ånition of the
ideal point comes in.
For m dimensions of objective space, if we can Ô¨Ånd a point with the smallest
value in all dimensions, it is absolutely the only nondominated point. But these
ideal things always fail in real situations.15 Thus we can only deÔ¨Åne the virtual ideal
points. In a group of points, the virtual point z+ = (z+
1 ,z+
2 ,‚ãÖ‚ãÖ‚ãÖ,z+
m) constructed by
z+
k = min{ fk(x)‚à£x ‚ààS},k = 1,2,‚ãÖ‚ãÖ‚ãÖ,m is called the ideal point or positive ideal
point. Conversely, in a group of points, the virtual point z‚àí= (z‚àí
1 ,z‚àí
2 ,‚ãÖ‚ãÖ‚ãÖ,z‚àí
m)
constructed by z‚àí
k = max{ fk(x)‚à£x ‚ààS},k = 1,2,‚ãÖ‚ãÖ‚ãÖ,m is called the negative ideal
point.16
In every generation, there is one positive ideal point z+ and one negative ideal
point z‚àí. Considering the aforementioned scale problem, the following normalized
weight sum scheme is natural:
z =
m
‚àë
k=1
zk ‚àíz+
k
z‚àí
k ‚àíz+
k
(6.10)
where zk is the kth objective value of z. Using Eq. 6.10 as the optimization objective,
we obtain the adaptive weight sum method.
If there really exists a positive ideal point, its normalized objective is 0. Similarly,
the normalized objective for a negative ideal point is m. In this way, every individual
can be mapped into the interval [0,m]. The smaller z is, the better z is.
6.2.2 Compromise Method
Let us consider a new idea. What about evaluating how far an individual is from the
positive ideal point? This would be interesting.
To accomplish this, we need to deÔ¨Åne a regret function (r(z)) to describe how bad
the current individual is. The better (closer to the positive ideal point), the smaller
(regret function value).
r(z) =
33z‚àíz+33 =
‚àö
m
‚àë
k=1
(
zk ‚àíz+
k
)2
(6.11)
Then the designer can normalize the expression in the following way:
eval (z) = rmax ‚àír(z)+Œ≥
rmax ‚àírmin +Œ≥
(6.12)
15 If a MOP has a real ideal point, in the general meaning, the model for the MOP is not adequate
because there are no conÔ¨Çicts between objectives, which is the essence of MOP modeling.
16 Remember we deal with minimum problems in this chapter. So here positive means ‚Äúgood
direction‚Äù and negative means ‚Äúbad direction.‚Äù

6.2 Preference-based Approaches
201
where eval(z) is the Ô¨Åtness value of z, r(z) is the regret function value of z, rmax and
rmin are the maximum and minimum regret function values in the current population,
respectively, and Œ≥ is a small number to avoid the divide-by-zero error. Equation 6.12
is the Ô¨Åtness function of the compromise method. The Ô¨Åtness of the best individual
is 1, and that of the worst individual is close to 0, which changes the model into a
maximum problem.
Even though there seems to be no need for a preference to be provided by the
decision maker, readers are encouraged to consider why the adaptive weight sum
method and compromise method are classiÔ¨Åed as preference-based approaches.
6.2.3 Goal Programming Method
Here we consider another situation. The designer cannot provide the relative impor-
tance for every objective. But she can provide the goal for every objective, which
could transform the MOP into a single-objective problem in a different way.
Let us revisit the ‚Äúfailure rate vs. cost‚Äù problem. If the designer realizes she
needs to make a tradeoff between two objectives, she may think that it is good to
make the cost close to 4.0 (the unit is unrelated) and the failure rate close to 0.3,
i.e., she sets the ‚Äúgoal‚Äù for programming. Not the smaller the better, but the closer
to the designated goal, the better. In this way, the designer provides the preference
information too.
If the goal of the objective k is f ‚àó
k , we can deÔ¨Åne the positive and negative devi-
ations as follows:
Œ¥ +
k =
{
fk ‚àíf ‚àó
k , fk ‚©æf ‚àó
k
0,
fk < f ‚àó
k
(6.13)
Œ¥ ‚àí
k =
{
0,
fk ‚©æf ‚àó
k
f ‚àó
k ‚àífk, fk < f ‚àó
k
(6.14)
Then the MOP described by Eq. 6.1 can be transformed into the following single-
objective problem:17
min
m
‚àë
k=1
(
Œ¥ +
k +Œ¥ ‚àí
k
)
s.t. gi (x) ‚©Ω0,
i = 1,‚ãÖ‚ãÖ‚ãÖ,q
h j (x) = 0,
j = q+1,‚ãÖ‚ãÖ‚ãÖ,k
x ‚àà‚Ñùn
(6.15)
Optimizing Eq. 6.15 properly could result in locating a nondominated point in
Pareto front, which reÔ¨Çects the designer‚Äôs preference.
17 Proofs to the correction of the transformation can be found in speciÔ¨Åc books on goal program-
ming.

202
6 Multiobjective Optimization
6.3 Vector-evaluated Genetic Algorithm
The initial idea of applying EAs to MOP came from Schaffer‚Äôs vector-evaluated
genetic algorithm (VEGA) suggested in 1985 [1].
VEGA is straightforward: dividing the population into m subpopulations, each
of which evolves toward a single objective. The whole algorithm can be illustrated
by Fig. 6.4.
















''




		
*
	 


A

	
 	
'
 !	
*
	= 


Fig. 6.4 VEGA
VEGA seems amazing at Ô¨Årst glance. It evolves the objectives in a parallel way.
The selection process in Fig. 6.4 could be any of those introduced in Chaps. 2 and
3 because only one objective needs to be considered. The shufÔ¨Çe procedure is quite
necessary when crossover and mutation are carried out from individual 1 to popsize.
In this way, individuals from different subpopulations are to exchange information,
which promotes the exploration of the objective space.
Unfortunately, the result of VEGA is not good. Let us take a simple two-objective
problem as an example. The model is illustrated by Eq. 6.16:
min z1 = x2
min z2 = (x‚àí2)2
s.t. x ‚ààR
(6.16)
Running VEGA with the following parameters results in the Ô¨Ånal population
distribution in Fig. 6.5, maxgen = 500, popsize = 100, pc = 1, pm = 0.
Why did VEGA fail in such a simple MOP? Readers are encouraged to consider
this question deeply before jumping to the next section.
Although VEGA is not good at directly distributing individuals along PF*, it can
be utilized in other ways. The following sections will discuss this issue .

6.4 Considerations for Designing Multiobjective Evolutionary Algorithms
203
6.4 Considerations for Designing Multiobjective Evolutionary
Algorithms
A good MOEA for MOPs should at least satisfy the following three requirements.
1. Convergence. MOEAs should have a convergence mechanism so that they can
Ô¨Ånd the PF* as soon as possible. In single-objective problems, the search direc-
tion is clear. But in MOPs, a good algorithm should search the objective space in
a different way.
2. Distribution. MOEAs should try to distribute their individuals as evenly as
possible along PF* so as to provide more nondominated solutions. In single-
objective problems, the selective pressure and the genetic drift will ultimately
converge the population into a single optimal solution. But how can a MOEA
maintain the distribution?
3. Elitism. The importance of elitism was discussed in Chap. 3. In a MOP envi-
ronment, which individuals need to be archived? How many of them should be
archived? Is there an insert/replacement mechanism? All these questions need to
be pondered before designing and analyzing a MOEA.






	










	



z1
z2
Fig. 6.5 Final population of VEGA for model Eq. 6.16

204
6 Multiobjective Optimization
In Sect. 6.1, we showed that generally there are many nondominated points in the
objective space in MOPs. So the elitism mechanism for MOEAs should contain a set
of nondominated individuals found thus far, often called the archive in MOEAs. The
variation operators, i.e., crossover and mutation, of MOEAs are the same as single
optimization EAs. The main difference is the selection and replacement process for
both the population and the archive.
6.4.1 Quality
For the aforementioned reasons, different measurement approaches for convergence
should be designed for MOPs.
Let us review the selection process of SGA Ô¨Årst. If we do not transform the
multiple objective values into one, we cannot use proportional selection. But ranking
and tournament selection can also be used if we deÔ¨Åne the rank and the comparison
rule according to MOP characteristics. We can categorize those approaches roughly
into two groups.
6.4.1.1 Quality Measure Approaches Considering Global Information
Goldberg suggested a way to rank a population: the Pareto rank method [2], which
can be implemented easily.
In Pareto rank, all individuals need to be compared with others using a Pareto
dominance concept to determine the nondominated solutions in the current popula-
tion. Those individuals are given rank 1. Then remove rank 1 individuals from the
population and determine the nondominated solutions in the remaining individuals
and give them rank 2. Do the procedure until all individuals have been assigned
a rank number.18 Figure 6.6 illustrates the results of many different quality mea-
sure approaches introduced in this chapter using a two-objective problem. The Ô¨Årst
number represents the point‚Äôs Pareto rank.
Let us discuss the two points with Pareto rank 2 in Fig. 6.6. Do we want to
distinguish them further? The point above is dominated by four rank 1 individuals,
but the point below is only dominated by one rank 1 individual. Even though these
two points cannot be compared in the Pareto dominance concept, we could assign
different quality signs for them. This is the Fonseca and Fleming‚Äôs idea [3]: MOP
rank.
The implementation is not sophisticated. All individuals in the current popula-
tions should be compared with others using the Pareto dominance concept. So for
every individual, we know how many individuals in the current population dominate
it. The fewer, the better. Then we assign the nondominated individuals rank 1 and
18 How does one evaluate the Pareto dominance relationship between two individuals, how does
one get the nondominated solutions in the current population, and how does one get the rank for
every individual in a programming environment?

6.4 Considerations for Designing Multiobjective Evolutionary Algorithms
205

















IJ<<1)3KL

IJ<<1)3KL
IJ<<1)3KL
IJ<<1)3KL
IJ<)<1)3KL
IJ<)<13KL
IJ<<1)3KL
IJ<<1)3KL
IJ<<13KL
IJ<)<1"3KL
.
	
 /.

	
%	
2$'	
2
Fig. 6.6 Results of different rank-based quality measure approaches
other individuals‚Äô rank equals 1 plus the number of individuals who dominated it.
In Fig. 6.6, the number in { } illustrates the Fonseca and Fleming‚Äôs rank result.19
Many other quality measure approaches will be introduced in later sections. We
will refer to Fig. 6.6 again and again to augment your understanding of the quality
of individuals.
6.4.1.2 Quality Measure Approaches Considering Local Information
In Chap. 3, we introduced tournament selection. As the binary tournament selec-
tion, two individuals are randomly selected from the current population, the better
individual (according to the Ô¨Åtness value) wins the tournament and is taken as a
selection result. Then those two individuals are put back (with replacement). The
procedure continues until popsize individuals have been selected.
But in MOP, how do we compare the two randomly selected individuals? It
is quite easy to determine the tournament winner if one dominates another, but
what if neither dominates the other? More considerations are necessary. Horn et
al. suggested the Pareto competition method to do the selection in a tournament like
way [4].
Let us take a binary tournament selection of a Pareto competition as an exam-
ple. Instead of comparing the two selected individuals directly, the Pareto compe-
tition method evaluates each selected individual with a comparison set, which was
selected randomly from the current population with a priori size. There are two
situations:
19 Consider the situation where PF* is a straight line between (0,1) and (1,0) in a normalized two-
objective situation and the individuals are distributed evenly above the PF*. Is there any preference
in MOP rank? If so, which individuals have advantages?

206
6 Multiobjective Optimization
1. One is dominated by the comparison set and the other dominates the comparison
set. Obviously the latter defeats the former.
2. Both or neither of them is dominated by the comparison set. Then they are of the
same quality. The one who resides in the uncrowded space wins the competition.
As to the evaluation of crowdedness, there are many ways to do this. The niche
count described in Chap. 5 is one option, which was adopted by Horn. Situation 2 is
illustrated by Fig. 6.7. The circle represents the species surrounding the individual.
The more individuals there are of the comparison set in the circle, the more crowded
the competing individual is.20
Fig. 6.7 Quality measure
result by Horn et al.‚Äôs Pareto
competition method

















	
	%
	
6.4.2 Distribution
In the selection and the replacement process of MOEAs, the convergence toward
PF* is, of course, the Ô¨Årst priority. Then the distribution might be considered in the
following conditions.
‚àôSelect an individual from the archive, which contains the nondominated individ-
uals thus far. A less crowded nondominated individual should gain some advan-
tage.
‚àôSelect an individual from those belonging to the same rank, i.e., a Pareto domi-
nance attribute that will be discussed later, of the population. The least crowded
one in that rank is preferred.
‚àôUpdate the archive with a newly generated individual that is nondominated by
individuals in the archive. Because there is a content limitation for the archive,
the most crowded one in the archive might be replaced by newly generated non-
dominated individuals with less extent of crowdedness.
20 According to the discussion in Chap. 5, generally we need a niche radius parameter to get
the niche count, which requires more computation resources and limits the application of Pareto
competition.

6.4 Considerations for Designing Multiobjective Evolutionary Algorithms
207
As for the distribution of the population, the Pareto competition method provides
one possibility. In addition, there are also two ways to measure the distribution dur-
ing evolution.
6.4.2.1 Distribution Measure Approaches Using Individual Numbers
One way to measure the distribution (or the crowdedness) of an individual is to
divide the objective space into many hyperboxes or hypercubes and count the num-
ber of individuals in the same hyperbox. Corne et al. suggested this approach in
2000 to evaluate the extent of proximity in the archive [5]. In two-objective condi-
tion, the hyperbox is degenerated into a rectangle (Fig. 6.8). The numbers, called
by Corne and Knowles the squeeze factor, represent how many nondominated in-
dividuals in the archive are in one hyperbox. The smaller the number is, the more
uncrowded the hyperbox is. If we want to select one nondominated individual from
the archive, we can use the binary tournament selection. The one with the smallest
squeeze factor wins the tournament. If a newly generated individual is nondomi-
nated by the individuals in the archive, we can calculate the squeeze factor for the
new one. If it is smaller than the largest one in the archive, the new individual re-
places the most crowded one in the archive. In this way, the algorithm can ‚Äúsqueeze‚Äù
the overcrowded individuals from the archive.
Fig. 6.8 Distribution mea-
sure result by Corne and
Knowles‚Äôs hyperbox method



















 



Knowles and Corne suggested an adaptive way to control Œ¥1 and Œ¥2 [6]. It is also
necessary to mention that Œ¥1 and Œ¥2 need not be the same and unchanged.
6.4.2.2 Distribution Measure Approaches Using Distance
In 1996, Osyczka and Kundu use the smallest distance between one individual and
the archive to evaluate the extent of proximity of a new individual [7]. After getting
the nondominated solutions in the current population, if we need to evaluate the

208
6 Multiobjective Optimization
distribution of the new individuals during the evolving procedure, one simple way
is to calculate the smallest modiÔ¨Åed distance to the nondominated solutions (Eq.
6.17).21
d (x) = min
j
0
1
1
‚é∑
m
‚àë
k=1
(
f j
k ‚àíœïk (x)
f j
k
)2
(6.17)
where f j
k is the kth objective value of the nondominated solution j, and œïk (x) is the
kth objective value of individual x.22 The one with the larger d (x) is preferred.
In 2002, Deb et al. suggested a way to measure the distribution without any
parameters: crowding distance method [8].
When comparing the crowdedness in the same Pareto rank, we give the sequence
for the individuals in that rank in ascending order according to the Ô¨Åtness of objec-
tive k and let f [j]
k
represent the Ô¨Åtness value of individual j in the sequence. Then
the crowdedness of individual j in dimension k in that rank can be expressed by Eq.
6.18:
c[j]
k = f [j+1]
k
‚àíf [j‚àí1]
k
f max
k
‚àíf min
k
(6.18)
where f max
k
and f min
k
represent the maximum and minimum values in objective k,
respectively.23 Individual i in a Pareto rank has m values for m objectives according
to Eq. 6.18. So one can simply add them up to represent the overall crowdedness,
crowding distance, of this individual (Eq. 6.19).
ci =
m
‚àë
k=1
ci
k
(6.19)
where ci
k is calculated by Eq. 6.18 and ci is the crowding distance of individual i.
Figure 6.9 represents the two-objective condition. All the points in Fig. 6.9 are
in the same rank and we have already done the normalization so that the maximum
distance in both objectives is 1. The crowding distance of individual i (represented
as a square) is exactly half of the normalized perimeter of the rectangle surrounding
it. The smaller the value is, the more crowded it is to others.
Do you have any questions on the above discussion? Isn‚Äôt it an amazing idea?
Yes, but more careful considerations is needed. How about the edge points, i.e.,
the top left point and the bottom right point? There is only one individual larger
or smaller than the edge point in the speciÔ¨Åc objective. You may argue that we can
just assign the one normalized distance to represent its crowdedness. Yes, but on the
other hand, edge points are valuable in exploring the PF* because they have more
21 Why don‚Äôt they use the smallest distance to all the other individuals in the current population?
Why is it called modiÔ¨Åed?
22 Readers are encouraged to consider why there is an f j
k in the denominator.
23 Why is there a f max
k
‚àíf min
k
term in the denominator?

6.5 Classical Multiobjective Evolutionary Algorithms
209
Fig. 6.9 Distribution measure
result by Deb et al.‚Äôs crowding
distance method













 

 






 
 
 


chances to generate new nondominated solutions by crossover and mutation. So
edge points can represent the spread ability of the MOEA, which will be discussed
in later sections. Thus generally, nondominated edge points need to be preserved in
a special way. Deb et al. assigned the inÔ¨Ånite crowding distance for edge points.
Up to now, we have introduced all the necessary elements for designing an efÔ¨Å-
cient MOEA.24 Let us now analyze several famous MOEAs to see how these ele-
ments are combined.
6.5 Classical Multiobjective Evolutionary Algorithms
6.5.1 Nondominated Sorting Genetic Algorithm II
Srinivas and Deb suggested their nondominated sorting genetic algorithm (NSGA)
in 1994 by combining Goldberg‚Äôs Pareto rank method and Ô¨Åtness sharing. After
Pareto ranking, every individual in the same rank r gets the same dummy Ô¨Åtness
value fr ( f1 > f2 > ‚ãÖ‚ãÖ‚ãÖ) [9]. Then individuals share their Ô¨Åtness with others in the
same rank. In this way, separated lower rank (good quality in Pareto dominance)
individuals gain a selection advantage, which pushes NSGA toward PF* with good
distribution.
Deb et al. suggested NSGA-II in 2002 to improve its predecessor in three ways:
improving the Pareto ranking procedure so it had less time complexity, adding the
elitism mechanism, and eliminating the requirement for the niche radius niche by
the new crowding distance method [8]. We will not discuss the Ô¨Årst improvement
24 We need to mention that all the techniques discussed in Sect. 6.4 provide effective elements for
guiding the population of MOEAs toward PF* evenly. In Sect. 6.7, we will discuss the problem of
evaluating the results of a MOEA considering convergence and distribution. They are two different
topics.

210
6 Multiobjective Optimization
for page limitation.25 The crowding distance method was discussed in Sect. 6.4.2.
So we will focus on the elitism mechanism.
The importance of elitism in MOEA was discussed in Sect. 6.4. In NSGA-II,
the capacity of the archive is popsize. The solution process of one generation in
NSGA-II is as follows.26
One Generation of NSGA-II
Phase 1: Assigning Pareto rank and crowding distance.
Step 1.1: Combine the population P(t) (popsize) and the archive A(t)
(popsize) to get 2√ó popsize individuals.
Step 1.2: Assign each individual a Pareto rank.
Step 1.3: Calculate the crowding distance for each individual.
Phase 2: Generating the new archive A(t +1).
Step 2.1: Insert the individuals into A(t +1). The individuals in rank 1
should be inserted Ô¨Årst, then rank 2 ...If rank r cannot be fully inserted into
A(t +1), then insert individuals in descending order of the crowding distance
until A(t +1) is full with popsize individuals.
Phase 3: Generating the new population P(t +1).
Step 3.1: Select from A(t + 1) using binary tournament selection to
form a mating pool. If two individuals in A(t + 1) have different ranks, the
one with the lower rank wins the tournament. Or the one with the same rank
but larger crowding distance wins the tournament.
Step 3.2: Generate the new population P(t +1) by crossover and mu-
tation from the mating pool. In standard NSGA-II the crossover operator is
SBX and the mutation operator is polynomial mutation, which were intro-
duced in Sect. 3.2.2.2 and 3.2.2.3, respectively.
Figure 6.10 illustrates the evolving process in one generation of NSGA-II.27
Suppose individual j reside in PF* and far from others. From the solution process
and Fig. 6.10 above we can see that individual j will never be lost. It will always be
in rank 1 with a large crowding distance, which means it will absolutely reside in
A(t +1). How about the crossover and the mutation that will destroy it? It does not
matter because in the next generation, A(t + 2) will be selected based on the union
of A(t +1) and P(t +1) and individual j is in A(t +1). So it is necessary to mention
25 The Ô¨Årst improvement is concerned with how to Ô¨Ånd a Pareto rank for each individual efÔ¨Åciently.
We omit this part to attract readers to theoretical issues at Ô¨Årst and encourage them to read [8] for
this part before implementation.
26 The C language implementation of NSGA-II can be downloaded at: http://www.iitk.
ac.in/kangal/codes.shtml.
27 In the original paper, Deb et al. did not explicitly label the two groups of individuals as popula-
tion and archive. Here we denote them in this way for pedagogical reasons.

6.5 Classical Multiobjective Evolutionary Algorithms
211


!13


"13
2F
2F
2F
+
F


!1=3


"1=3
.
	
F
8
		



		
Fig. 6.10 The evolving process in one generation of NSGA-II
that NSGA-II can be regard as a kind of (popsize+ popsize)-ES with rapid Pareto
ranking and crowding distance, even though it is called a GA.28
The convergence, distribution, and elitism mechanisms in NSGA-II are Pareto
rank and tournament selection, the crowding distance, and the introduction of the
archive A, respectively. In this way, NSGA-II establishes an elaborate balance in the
three requirements and requires no additional parameters besides SGA.
6.5.2 Strength Pareto Evolutionary Algorithm 2 and Pareto
Envelope-based Selection Algorithm
6.5.2.1 Strength Pareto Evolutionary Algorithm 2
Zitzler et al. suggested a strength Pareto evolutionary algorithm (SPEA) in 1999
and improved it into SPEA2 in 2001 [10].
Let us suppose we want another efÔ¨Åcient MOEA starting from NSGA-II.29 Task-
ing another group of individuals, an archive, with storing the Pareto solution up till
now is very fascinating. But why does the size of the archive need to be the same
as that of the population? In addition, in NSGA-II, rank is considered twice, as is
crowding distance (steps 2.1 and 3.1). Can we shorten them into one?
Based on these two questions, the evolving process in one generation of SPEA2
can be illustrated by Fig. 6.11.30 A(t) represents the archive at generation t with size
¬ØN, and P(t) represents the population at generation t with size N.
28 We would like to remind readers again that sometimes an algorithm that combines many ad-
vanced searching techniques is hard to classify.
29 We need to mention that all these discussions are based on a pedagogical purpose, not the same
developing procedure as the real development of SPEA2.
30 A C language implementation of SPEA2 can be downloaded at: http://www.tik.ee.
ethz.ch/sop/pisa/.

212
6 Multiobjective Optimization

"13


!13


!1=3
4	

"1=3
	



		
Fig. 6.11 The evolving process in one generation of SPEA2
Two questions might arise in consideration of Fig. 6.11:
1. What is the mechanism of selection? How can we combine the convergence re-
quirement and the distribution requirement into one selection process?
2. What is the mechanism of archive updating? SpeciÔ¨Åcally, what if the number of
nondominated solutions in the union of A(t) and P(t) is not equal to ¬ØN?
Let us consider the Ô¨Årst question Ô¨Årst. In SPEA2, every individual has been as-
signed a number to describe its ‚Äústrength‚Äù as follows:
S(i) = ‚à£j‚à£( j ‚ààP+A)‚à©i ‚â∫j‚à£
(6.20)
where ‚à£‚à£is the function of cardinality, i.e., the number of elements in one set. Ac-
cording to Eq. 6.20, an individual‚Äôs strength is the number of individuals it domi-
nates in the union of A and P. The larger the number is, the stronger the individual
is. In Fig. 6.6, the number in // illustrates the strength of each individual.
But just comparing the strength of every individual will lead to selection bias.31
Zitzler et al. deÔ¨Åned another term, raw Ô¨Åtness, to describe how good an individual
is of convergence.32 An individual‚Äôs raw Ô¨Åtness is deÔ¨Åned as follows:
R(i) = ‚àëj‚ààP+A, j‚â∫i S(j)
(6.21)
31 Why?
32 ‚ÄúRaw‚Äù here means the Ô¨Åtness needs to be modiÔ¨Åed later. Another thing that needs to be men-
tioned is that here Ô¨Åtter does not mean larger. Only with proportional selection is a larger Ô¨Åtness
value to be the better one.

6.5 Classical Multiobjective Evolutionary Algorithms
213
If an individual i is the nondominated solution in the union of A and P, it needs
to be assigned the best raw Ô¨Åtness (such as zero).33 In Fig. 6.6, the number in ()
illustrates the raw Ô¨Åtness of each individual.
As regards distribution, in SPEA2, one must know the distances of every indi-
vidual to all other individuals. Then the distances of one individual are ranked in
ascending order. Zitzler et al. deÔ¨Åned the term density to describe the crowdedness
of individual i as follows:34
D(i) =
1
œÉk
i +2
(6.22)
where œÉk
i is the kth shortest distance to individual i, i.e., the kth number in the
distance rank.35 The farther an individual is away from others, the larger œÉk
i is,
and the smaller D(i) is. Zitzler et al. suggested that k =
‚àö
N + ¬ØN might be a good
parameter option.
Finally, every individual is granted Ô¨Åtness by Eq. 6.23, which constitutes the basis
of the binary tournament selection in Fig. 6.11.
F (i) = R(i)+D(i)
(6.23)
For nondominated individual i, R(i) = 0 and D(i) < 1. So the MOP has been
transformed into a single-objective minimum problem in this way.
Now we consider how to make decisions when the number of nondominated
solutions in the union of A(t) and P(t) is not equal to ¬ØN. Zitzler et al.‚Äôs answer was
straightforward. Let B(t) represent the nondominated set selected from the union of
A(t) and P(t) and ‚à£B(t)‚à£represent its cardinality. If ‚à£B‚à£< ¬ØN, we add more dominated
individuals to Ô¨Åll in the gap according to Ô¨Åtness described by Eq. 6.23 until ‚à£B‚à£= ¬ØN.
If ‚à£B‚à£> ¬ØN, we delete those individuals from B(t) with the smallest distance to others
(œÉ1
i ), then delete individuals with the smallest second distance to others (œÉ2
i ) . . . until
‚à£B‚à£= ¬ØN.
From Fig. 6.6 we can see that for dominated individuals, their raw Ô¨Åtness val-
ues are several, dozens, or hundreds, large enough to neglect their density, which is
less than 0.5. So SPEA2 directs the search toward the PF*. For nondominated indi-
viduals, their raw Ô¨Åtness values are all 0, so sparser individuals, those with smaller
densities, gain the advantage.
Suppose individual j resides in PF* and far from others. From Fig. 6.11 we can
see that individual j will never be lost. So the elitism mechanism in SPEA2 works
Ô¨Åne.
The convergence, distribution, and elitism mechanisms in SPEA2 are raw Ô¨Åtness
assignment, density, and the introduction of the archive A, respectively.
33 Consider the situation where PF* is a straight line between (0,1) and (1,0) in a normalized two-
objective situation and the individuals are distributed evenly above the PF*. Is there any preference
in strength and raw Ô¨Åtness? If so, which individuals have advantages?
34 ‚ÄúDensity‚Äù here describes the crowdedness of an individual. From its literal meaning, if we prefer
evenly distributed individuals, the density of the individual should always be small.
35 Try to understand why we do not directly use the smallest distance?

214
6 Multiobjective Optimization
Recently nearly every new published paper on MOEA design and improvement
has found it necessary to compare the suggested algorithm at least with NSGA-
II and SPEA2 on MOEA benchmark problems, which actually makes these two
algorithms the benchmark algorithms in MOEAs.
6.5.2.2 Pareto Envelope-Based Selection Algorithm
Corne et al. suggested a Pareto envelope-based selection algorithm (PESA) in
2000 [5]; a similar idea is illustrated in Fig. 6.12.36 Compared to SPEA2, PESA
has different considerations in answering the two questions raised on page 212.

"13

!1=3
4	

"1=3
	



		
2	
Fig. 6.12 The evolving process in one generation of PESA
Before answering the two questions, we need to distinguish the diversity mecha-
nisms of NSGA-II, SPEA2, and PESA. As was discussed above, NSGA-II uses the
crowding distance and SPEA2 the density function. In PESA, the hyperbox method
and squeeze factor concept are used (Fig. 6.8).
For the Ô¨Årst question, PESA uses binary tournament selection to generate new
population from the archive. The archive in PESA only contains the nondominated
solutions found thus far. They are not comparable with respect to objectives. So
the one with the smaller squeeze factor, i.e., the one residing in the less crowded
hyperbox, wins the tournament.
For the second question, the archive-updating mechanism, if a new individual is
nondominated in both the population and the archive and the archive is full, select
the individual in the archive with the largest squeeze factor to be replaced by the
new one.
36 We guess that the meaning of ‚Äúenvelope‚Äù in the name of the algorithm is that Corne et al.
consider the archive as an envelope to store the nondominated individuals found thus far, which is
different from SPEA2 and NSGA-II.

6.5 Classical Multiobjective Evolutionary Algorithms
215
In 2001, Corne et al. developed PESA by suggesting a region-based selection.
Then the algorithm was named PESA-II [11]. PESA-II differs only in the selection
mechanism. Every hyperbox has its own squeeze factor. The hyperbox with the
smallest squeeze factor will be selected Ô¨Årst and then a randomly chosen individual
is selected. Corne et al. claimed that region-based selection could ensure a good
distribution along PF*.
6.5.3 Pareto Archived Evolution Strategy
All the above-discussed algorithms are based on GAs, this subsection discusses ES.
The evolution strategy was introduced in Chap. 2. Here we brieÔ¨Çy review (1+1)-
ES. We only have a current individual (c(t)) and an updated individual (u(t)). c(t)
evolves (mutation) into u(t).37 If u(t) is better than c(t), it replaces c(t) and becomes
c(t +1). Else, c(t) remains unchanged and becomes c(t +1).
How can (1+1)-ES be developed into a MOP? First, there should be an archive
containing the nondominated individuals. And special considerations are necessary
in the replacement procedure. Knowles and Corne proposed a Pareto archived evo-
lution strategy (PAES) in 2000 to expand ES to solve MOPs [6].
In (1+1)-PAES, there are three groups: one current individual c(t), one updated
individual u(t), and one archive A containing all the nondominated individuals found
thus far. First, c(t) evolves (mutation) into u(t).
For u(t), there are two targets: try to be the current individual and try to be added
to the archive.
If u(t) is bad (is dominated by c(t) or A), it has failed and c(t) is used again to
generate a new updated individual.
For the good u(t) (not dominated by c(t) and A), if any of the following con-
ditions is satisÔ¨Åed, it enters the archive: (1) u(t) dominates c(t), (2) the archive is
not full, (3) there is at least one individual x in A which is dominated by u(t), or
(4) u(t) and A are nondominated but there is at least one individual x in A whose
crowdedness is larger than that of u(t). In the third condition, u(t) replaces any of
its dominated ones and in the fourth condition, it replace the most crowded one.
For the good u(t) (not dominated by c(t) and A), if any of the following con-
ditions is satisÔ¨Åed, it replaces c(t): (1) u(t) dominates c(t) or (2) the crowdedness
of c(t) in A is larger than that of u(t). The crowdedness evaluation in PAES is the
squeeze factor, introduced by Fig. 6.8.
In all, PAES ensures that the nondominated solutions residing in an uncrowded
location will survive.
Starting from (1 + 1)-PAES, Knowles and Corne also discussed (1 + Œª)-PAES
and (Œº +Œª)-PAES [6].
37 The evolution from c(t) to u(t) is discussed in Chap. 2.

216
6 Multiobjective Optimization
6.5.4 Micro-GA for Multiobjective Optimization
Goldberg suggested the idea of micro-GA in 1989 [12]. Micro-GA has a small pop-
ulation size (e.g., 3) and small maximum generation number (e.g., 4). By the end
of the micro-GA, the population reaches the status of nominal convergence, and all
individuals are identical or similar. Then the best individual in the Ô¨Ånal generation
is used as one of the initial individuals for the next micro-GA, and other initial indi-
viduals are generated randomly. Goldberg claimed that a micro-GA evolved in this
way might search the solution space more efÔ¨Åciently.
All the MOEAs discussed above need a global dominance relationship (rank or
at least the nondominated solutions in the population), which is very time consum-
ing. Coello Coello and Pulido developed the micro-GA into the Ô¨Åeld of MOP, the
micro-GA for multiobjective optimization, in 2001 [13]. The initial population needs
to be updated by the results of a micro-GA and the archive is necessary to store
the nondominated individuals. Coello Coello and Pulido divided the individuals for
initialization into two groups: Ir (replaceable potential initial individuals) and In
(nonreplaceable potential initial individuals) with a predeÔ¨Åned percentage. Ir will
be replaced by the good micro-GA results and In will remain unchanged since it
was randomly generated, which ensures diversity.
The solution process of micro-GA for multiobjective optimization is as follows:
One Meta-Generation of Micro-GA for Multiobjective Optimization
Phase 1: Initialization.
Step 1.1: Combine Ir and In to form I.
Step 1.2: Select randomly from I to form the initial population for a
micro-GA.
Phase 2: Evolving micro-GA until nominal convergence.
Step 2.1: Select by binary tournament selection based on the Pareto
dominance relationship.
Step 2.2: Use crossover and mutation to explore the objective space.
Step 2.3: Store one nondominated individual arbitrarily and copy it
intact to the following generation.
Phase 3: Updating the archive and Ir in a local way.
Step 3.1: Determine whether the best nondominated individuals from
the micro-GA (1 or 2) could be inserted into the archive. If the archive is
not full and the individual is not dominated by the archive, it is inserted. If
the archive is full and the individual in not dominated by the archive, the
individual replaces the one in the archive with a larger crowdedness than it.
Step 3.2: Determine whether the best nondominated individuals from
micro-GA (1 or 2) will be inserted into Ir. Compare the nondominated indi-
vidual with the randomly selected one from Ir by the Pareto dominance rela-
tionship. If the former dominates the latter, it will replace the latter.

6.6 Cutting Edges of Multiobjective Evolutionary Algorithms
217
Phase 4: Updating the archive and Ir in a global way.
Step 4.1: After every several generations, discard the individual in Ir
and use the archive to Ô¨Åll Ir.
During the evolving process, the micro-GA will start from points getting closer
and closes to PF*, which makes the micro-GA for multiobjective optimization very
efÔ¨Åcient.
The crowdedness evaluation in the micro-GA for multiobjective optimization is
the squeeze factor, introduced by Fig. 6.8.
Up to now, we have discussed several classical MOEAs that satisfy the require-
ments suggested in Sect. 6.4. They (1) contain a convergence mechanism that allows
the algorithm to approach PF*; (2) contain a distribution mechanism that allows in-
dividuals to reside in different places on PF*; (3) contain an elitism mechanism that
the uncrowded nondominated solutions will never be lost in the evolving process.
If you would like to design a new MOEA or improve the classical one, you need to
check whether these requirements are satisÔ¨Åed in a balanced way.
All of these algorithms were proposed at the turn of the century, which explains
the Ô¨Årst high slope in Fig. 6.3 (from 2001 to 2003). Many new algorithms were
proposed and comparisons were carried out. After 2004, research interests have
been spilt out many areas. How does one provide persuasive comparison results
for MOEAs? How does one adapt MOEAs to uncertain environments? How can
MOEAs be used to handle constraints? All these questions make researchers strive
for a better understanding of MOEAs. We will discuss these topics in the next part
of this chapter. Before that, we need to mention some of the latest developments in
the design of MOEAs.
6.6 Cutting Edges of Multiobjective Evolutionary Algorithms
In this section, several different research directions will be introduced to excite read-
ers‚Äô interest in developing new algorithms, improving current ones, and combining
other ideas with MOEAs. All of these techniques are quite useful in the Ô¨Åeld of al-
gorithm design and analysis. It is necessary to mention again that the selection of
the material is based on the authors‚Äô interests and the potential of the application.
6.6.1 Expanding Single-objective Evolutionary Algorithms into
Multiobjective Optimization Problems
In this subsection, we will introduce three examples of expanding algorithms for
single-objective problems, discussed in Chaps. 2 and 3, into MOPs.

218
6 Multiobjective Optimization
6.6.1.1 Embedding Differential Evolution into NSGA-II
In Sect. 2.4.2, we introduced differential evolution, in which directions are used
to direct the new individual generation. In standard DE, the direction is randomly
generated. During the evolving process, individuals will converge toward the global
optimal solution due to the selective pressure, so the randomly generated directions
among individuals have some means of improving the quality.
From another aspect, in MOP, we need to quickly make individuals converge
toward PF* and distribute them as evenly as possible. Why not use the information
of the convergence and distribution to guide the evolving process?
One way to expand DE into MOP is to embed DE into NSGA-II. Iorio and Li
proposed their embedding work in 2006 [14]. In Fig. 6.10, standard crossover and
mutation are used to generate P(t +1) from A(t +1). Iorio and Li use DE to substi-
tute the new-population-generating process.
Iorio and Li utilized the new-individual-generating formula ‚Äúcurrent/2‚Äù as fol-
lows:
vi = xi +K (r3 ‚àíxi)+F (r1 ‚àír2)
(6.24)
where xi is an individual in A(t +1) and vi is the new generated individual in P(t +
1). Classically, r1 ‚àï= r2 ‚àï= r3 ‚àï= xi are random samplings in A(t +1). Equation 6.24
means that the new individual starts at xi and has a direction and length combination
of K (r3 ‚àíxi) and F (r1 ‚àír2).
If we are concerned with convergence, we can assign r3 as any individual better
than xi on Pareto dominance, which means it has a lower rank, so that direction
(r3 ‚àíxi) might be a Pareto dominance improving direction. This is scheme 1.
If we are concerned with distribution, we can assign r1 and r2 to be with same
rank. Then (r1 ‚àír2) might be the disperse direction on the same rank. This is
scheme 2.
Or we can consider both aspects, i.e., assign r3 as any individual better than xi
on Pareto dominance and r1 and r2 to be with same rank. This is scheme 3.
According to the numerical experiments carried out by Iorio and Li, scheme 3 is
the best one and scheme 2 has similar performances. Scheme 1 is the worst one but
still better than the standard NSGA-II.38
It is necessary to point out that the embedding techniques introduced above can
also be used with other MOEAs.
6.6.1.2 Multiobjective Scatter Search
Scatter search was introduced in Chap. 2. We demonstrate that scatter search is a
kind of direction-based method that utilizes the subtraction of two solutions as the
perturbation direction in an evolution episode.
38 Could you give some intuitive explanation for the results?

6.6 Cutting Edges of Multiobjective Evolutionary Algorithms
219
The main loop of scatter search is illustrated by Fig. 2.14. Reference set (RS)
stores high-quality solutions (Re fSet1 for objective value and Re fSet2 for crowd-
edness). With a generation procedure, subsets are generated from RS. After that, a
combination procedure is carried out to form new solutions from subsets. Then the
new solutions experience the local search by the improvement procedure to become
better solutions. There are update rules to determine whether an improved solution
could enter an RS.
How does one expand scatter search into MOP?
According to the discussion above, at the very least we need an archive (apart
from RefSet1) to store the nondominated individuals found thus far. In addition,
several questions need to be answered:
‚àôWhat is the improvement direction in MOP scatter search? In single-objective
situations, it is easy: the lower the objective value, the better.
‚àôHow does one update RS? Again, multiple objective values become the focus.
‚àôHow does one maintain diversity in an archive? This is the same important ques-
tion that must be answered by any MOEA designer.
Nebro et al. successfully answered the above questions and proposed the Archive-
based hYbrid Scatter Search (AbYSS) algorithm in 2008 [15]. The main loop of
AbYSS is the same as that of the standard scatter search illustrated by Fig. 2.14. We
will just present the main differences.
The initialization of RefSet1 is the same as in standard scatter search. After the
random initialization and the improvement procedure, AbYSS adopts the Ô¨Åtness of
SPEA2 (Eq. 6.23) to determine those who will be in Re fSet1.
In the main loop, the improvement procedure is a simple mutation-based EA, like
(1+1)-ES. After mutation, the parent and the offspring compete according to Pareto
dominance. The winner takes the place. If they are nondominated to each other and
the mutant is not dominated by the archive (not the Re fSet1), the mutant will be
inserted into the archive and replace the parent. Otherwise, discard the mutant.
If the improved solution dominates solutions in RefSet1, then all dominated so-
lutions are deleted and the improved one is inserted. If the improved solution is
dominated by RefSet1, it is then considered whether it is to be inserted into Re fSet2
with the standard procedure (distance to the closest RS solutions).
The archive is updated by the end of every loop. If the archive is not full, then the
improved solutions, which are nondominated by the archive, will be inserted into
the archive. If the archive is full, Nebro et al. use the crowding distance (Eq. 6.19
and Fig. 6.9) to determine which overcrowded archive member should be replaced
by the improved one residing in the uncrowded area.
If RS does not change in the updating procedure and the stop criterion has not
been satisÔ¨Åed, then the initialization procedure in AbYSS includes some nondomi-
nated solutions in the archive based on the standard scatter search reinitialization.
In this way, scatter search has been successfully expanded into the Ô¨Åeld of MOPs.
Nebro et al. claimed that AbYSS obtains very competitive results compared with
NSGA-II and SPEA2.

220
6 Multiobjective Optimization
6.6.1.3 Cooperative Coevolutionary Multiobjective Optimization
The concept of coevolution was introduced in Sect. 3.7.1. Cooperative coevolution
divides the full solution into several parts and promotes the one that has good col-
laboration relationships with others. Competitive coevolution promotes the one that
has good winning percentage with others.
In 2006, Tan et al. proposed a cooperative coevolutionary MOEA [16]. The key
problem to be resolved in designing a coevolutionary algorithm is the solution divi-
sion and integration scheme. Other considerations related to MOP also need to be
elaborated.
When solving a MOP with n variables, Tan et al. divide the population into n
subpopulations, each of which contains individuals only representing one variable.
They use a binary code scheme for chromosomes.
To evaluate the Ô¨Åtness value for newly generated individual j in subpopulation i,
it needs to be combined with other n‚àí1 individuals sampled from other n‚àí1 sub-
populations. The simplest method is to randomly sample the subpopulations, com-
bine them into a full solution, and use its Ô¨Åtness values to represent individual j in
subpopulation i. To accelerate the convergence, Tan et al. suggested another method.
Every subpopulation has a representative individual with the highest Ô¨Åtness values
determined previously. The new individual combines with other n‚àí1 representative
individuals to form a full solution and get its Ô¨Åtness values. After these two meth-
ods, the new individual will have two sets of Ô¨Åtness values, each of which contains
m Ô¨Åtness values. The point on the objective space generated by the latter approach
will represent the new individual unless it is dominated by the point generated by
the former approach.
In Tan et al.‚Äôs cooperative coevolutionary MOEA, the archive is used to store the
full nondominated solutions found thus far and the niche-based diversity preserving
techniques are adopted. The solution process of Tan et al.‚Äôs cooperative coevolu-
tionary MOEA is as follows.
Cooperative Coevolutionary MOEA
Phase 1: Initialization.
Step 1.1: Assign the parameters for the cooperative coevolutionary
MOEA, such as chromosome length l, pc, pm, popsize for each subpopula-
tion, clone number c, stop criteria, et.
Step 1.2: Generate n subpopulations, each with popsize uniformly
distributed individuals, randomly with one variable to form the initial pop-
ulation and evaluate their Ô¨Åtness values in the random sampling way and de-
termine the represent individual for each subpopulation. Initialize the archive
as empty. gen = 0.
Phase 2: Main loop. Repeat the following steps until the stop criteria are
satisÔ¨Åed. In each loop, n subpopulations are evolved in a sequential way, i.e.,

6.6 Cutting Edges of Multiobjective Evolutionary Algorithms
221
subpopulation 1 is Ô¨Årst evolved and then subpopulation 2 until subpopulation
n. i = 1.
Step 2.1: Calculate the rank for each individual in subpopulation i.
The rank-assigning scheme is the same as the MOP rank introduced in Sect.
6.4.1, i.e., the rank for individual j is one plus the number of solutions in the
archive that dominate j.
Step 2.2: Normalize the objective space and calculate the niche count
for each individual in subpopulation i. Tan et al. suggested an adaptive way to
estimate niche radius œÉ in the evolving process [17].
Step 2.3: Generate a mating pool for subpopulation i with binary tour-
nament selection. The one with the lower rank wins the tournament. If the
competitors have same rank, the one with the smaller niche count wins the
tournament.
Step 2.4: Generate a new subpopulation using uniform crossover with
probability pc and bit-Ô¨Çip mutation with probability pm.
Step 2.5: Calculate the Ô¨Åtness values for all new individuals in sub-
population i using the two methods discussed above.
Step 2.6: Update the archive in a way like PESA and PAES. For each
individual, its full solution will enter the archive if it is not dominated by
the archive. Any solution in the archive dominated by it will be removed. If
the capacity of the archive has been reached, the one in the archive with the
largest niche count will be removed to make room for the new nondominated
solution.
Step 2.7: i = i+1. Go back to step 2.1 until i > n.
Step 2.8: After all subpopulations have been evolved in one gener-
ation, clone c copies of the solution in the archive with the smallest niche
count and put their components into corresponding subpopulations. This clone
operator will promote exploration and exploitation over uncrowded regions.
g = g+1.
Phase 3: Submitting the archive as the results of an cooperative coevolu-
tionary MOEA.
Tan et al. also implemented the above algorithm in a distributed environment [16],
and in 2009 they expanded the algorithm into a competitive-cooperative paradigm
to solve dynamic MOPs [18].
6.6.2 Archive Maintenance
The main difference between MOEAs and the algorithms introduced in Chap. 3 are
selection and archive maintenance. In Sect. 6.5 we used classical MOEAs mainly

222
6 Multiobjective Optimization
to introduce the selection methods that promote convergence and diversity, while
several advanced techniques about archive maintenance will be discussed here.
Generally we will predeÔ¨Åne a capacity limitation for an archive, i.e., the number
of nondominated solutions provided at the end of a MOEA.39 In the following part
of this subsection, we maintain the archive A(t) with a capacity limitation of N at
generation t. After a new individual c is generated by variation operators, we need
to determine A(t +1) using techniques introduced here.40
Another thing that needs to be mentioned is that these techniques are generally
algorithm independent, i.e., applicable to any MOEA with an archive.
6.6.2.1 Adaptive Archive Maintenance Based on Hyperbox
In Sect. 6.4.2, we introduced the hyperbox concept, suggested by Corne et al., and
mentioned that Knowles and Corne suggested an adaptive way to control the size
of hyperboxes. They further developed the adaptive control of the hyperbox and in
2003 suggested a technique to archive the evenly distributed nondominated solu-
tions found thus far [20].41
Let us review the hyperbox in Fig. 6.8. If the user predeÔ¨Ånes that the grid numbers
along z1 and z2 are 9 and 6, respectively, the we can adaptively redraw the hyperbox
according to the current nondominated solutions in the archive in Fig. 6.13.
Fig. 6.13 Adaptive squeeze
factor determination



























  








We can deÔ¨Åne the positive and negative ideal points in the archive as z+ and z‚àí,
respectively, i.e., triangles in Fig. 6.13. For the ith objective, with the predeÔ¨Åned
39 The reason for the limited archive capacity was discussed in Sect. 6.1.3. Interested readers are
referred to [19, 20] for more discussions.
40 Thus we will discuss steady state MOEAs in this subsection, but these techniques can also be
applied in generational MOEAs.
41 This paper is the winner of the IEEE Transactions on Evolutionary Computation Outstanding
Paper Award.

6.6 Cutting Edges of Multiobjective Evolutionary Algorithms
223
grid number divi, the lower bound, Li, and the upper bound of the adaptive grid, Ui,
are deÔ¨Åned as follows:
Li = z+
i ‚àíz‚àí
i ‚àíz+
i
2divi
Ui = z‚àí
i + z‚àí
i ‚àíz+
i
2divi
(6.25)
The second parts in the above equations make the edge points at the center of the
outer hyperboxes. Then we can get the coordinates for every hyperbox, determine
which nondominated solution belongs to which hyperbox, and calculate the squeeze
factor for each hyperbox.
Whenever we get a new individual c with variation operators, the archive main-
tenance method is carried out with the follow logic considerations.
1. If c is dominated by at least one solution in archive A(t), discard it.
2. If c dominates at least one solution in A(t), all the dominated solutions in A(t)
are removed and c is inserted into the archive.
3. If the capacity limitation is not reached, ‚à£A(t)‚à£< N, and c is nondominated with
all solutions in A(t), just insert the new one.
4. If ‚à£A(t)‚à£= N and c is nondominated with all solutions in A(t), c is accepted with
any of the following conditions: (1) it does not reside in the range of the current
hyperbox, i.e., it increase the extend of the grid in at least of one objective dimen-
sion; (2) it resides in a less crowded hyperbox than some other point in A(t), e.g.,
the circle in Fig. 6.13. If so, randomly select one solution in the hyperbox(es)
with the largest squeeze factor to be replaced by the new one.
After the archive maintenance procedure, the above adaptive procedure is carried
out on A(t +1) again to get the new squeeze factor for each hyperbox.
The above adaptive method maintains a good balance in convergence, diversity,
edge-point retention, and efÔ¨Åciency in the archive.
6.6.2.2 Archive Maintenance Based on Œµ-Dominance
In 2002, Laumanns et al. suggested a new deÔ¨Ånition for dominance, Œµ-dominance,
to solve the tradeoff between convergence and diversity [19].
They suggested two types of Œµ-dominance. In objective space, point i Œµ-dominates
point j, expressed as i ‚â∫Œµ j, means
(1‚àíŒµ)zi
l ‚â§zj
l ,
‚àÄl ‚àà{1,2,‚ãÖ‚ãÖ‚ãÖ,m}
(6.26)
where 0 < Œµ < 1. Or
zi
l ‚àíŒµ ‚â§z j
l ,
‚àÄl ‚àà{1,2,‚ãÖ‚ãÖ‚ãÖ,m}
(6.27)
where Œµ > 0 and zi
l is the lth objective value of point i.

224
6 Multiobjective Optimization
For a point i in objective space, i.e., the square in Fig. 6.14, its Œµ-dominant do-
mains according to deÔ¨Ånition Eq. 6.26 are the areas on top and to the right of the
dashed line (the lines are included). For a point j in objective space, i.e., the circle
in Fig. 6.14, its Œµ-dominant domains according to deÔ¨Ånition Eq. 6.27 are the areas
on top and to the right of the dotted line (the lines are included).
Fig. 6.14 Œµ-dominance sug-
gested by Laumanns et al.

















)
3
3








Œµ-dominance relaxes the concept of weak dominance by making a point dominate
more points to the extent of Œµ. The difference between the two deÔ¨Ånitions is whether
the relaxation is based on absolute value, Eq. 6.27, or relative value, Eq. 6.26.
Let Z be the objective space, a set PFŒµ ‚äÜZ is called an Œµ-approximate Pareto set
if any point g ‚ààZ is Œµ-dominated by at least one point f ‚ààPFŒµ. Those points that
belong to PFŒµ and are Pareto nondominated solutions of the objective constitute an
Œµ-Pareto set, denoted as PF‚àó
Œµ . The Œµ-approximate Pareto set and Œµ-Pareto set are
illustrated by Fig. 6.15.


)
(a)




















)
(b)
Fig. 6.15 (a) Œµ-approximate Pareto set, and (b) Œµ-Pareto set

6.6 Cutting Edges of Multiobjective Evolutionary Algorithms
225
In Fig. 6.15, we use Eq. 6.26 for the deÔ¨Ånition of Œµ-dominance. Thus the relax-
ation extent of all points is not the same. Figure 6.15a and b have the same PF*. As
can be seen from these two Ô¨Ågures, with seven solutions in PFŒµ and four solutions
in PF‚àó
Œµ , we can Œµ-dominate all the solutions in objective space, which gives us an
efÔ¨Åcient way to pursue convergence and diversity simultaneously.
Based on the above preparations, Laumanns et al. suggested two methods for
archive maintenance.
Whenever we get a new individual c with variation operators. The Ô¨Årst archive
maintenance method converging to an Œµ-approximate Pareto set is carried out with
the follow logic considerations.
1. If c is Œµ-dominated by at least one solution in archive A(t), discard it.
2. If c is not Œµ-dominated by any solution in archive A(t), insert c into A(t) and
remove solutions that are Pareto dominated by c.
Suppose we have scaled the value range for objective i into range [1,Ui], then the
cardinality of A(t) can be calculated with predeÔ¨Åned parameter Œµ, illustrated by Fig.
6.16. For the Œµ-dominance deÔ¨Åned by Eq. 6.26, the largest value of Œµ-dominant 1
is
1
1‚àíŒµ , the largest value of Œµ-dominant
1
1‚àíŒµ is
1
(1‚àíŒµ)2 , . . . Suppose that the Ô¨Ånal nith
point in the above sequence is Ui; we can get
1
(1‚àíŒµ)n
i = Ui. So with value range
[1,Ui] for objective i and Œµ-dominance deÔ¨Åned by Eq. 6.26, the number of grids on
objective i is ni = ‚åä‚àí
lgUi
lg(1‚àíŒµ)‚åã, where ‚åä‚åãis for rounding toward minus inÔ¨Ånity. The
number of hyperboxes in the whole value space is ‚àèm
i=1 ni. Thus the above archive
maintenance method will keep a limited number solutions after convergence, which
belong to an Œµ-approximate Pareto set.
Fig. 6.16 Grid numbers on
objective i with Œµ-dominance






















) 



 





 





 

Let us now discuss Fig. 6.15b in more detail. Every solution in an Œµ-Pareto set
has a representation point, i.e., the lower left corner of its Œµ-dominated area. We
can use the grid sequence to express the representation point. For the nondominated
point z = (z1,‚ãÖ‚ãÖ‚ãÖ,zm), in objective i, its representation point is the bith grid.
bi =
‚åä
‚àí
lgzi
lg(1‚àíŒµ)
‚åã
(6.28)
Then the representation point of solution z can be expressed as b = (b1,‚ãÖ‚ãÖ‚ãÖ,bm).
Thus PF* is occupied by several hyperboxes constructed by the Œµ-dominance con-
cept.
Then the second archive maintenance method converging to an Œµ-Pareto set is
carried out with the following logic considerations.
1. If the representation point of the newly generated individual c Pareto dominates
the representation point of at least one solution in archive A(t), add c and remove

226
6 Multiobjective Optimization
those solutions whose representation points are dominated by the representation
point of c.
2. If the representation point of c is not Pareto dominated by any representation
point of archive A(t) and c shares the hyperbox with the one point d in the
archive, then c can replace d only if it Pareto dominates d.
3. If the representation point of c is not Pareto dominated by any representation
point of archive A(t) and there is no solution in the archive sharing the same
hyperbox with c, add c.
4. Otherwise, discard c.
In this way, there will always be at most one nondominated solution in one hyper-
box, which ensures diversity and convergence simultaneously. For the same reason,
the above archive maintenance method will keep a limited number of solutions after
convergence that belong to the Œµ-Pareto set.
There have been extensive comparisons between NSGA-II and SPEA2 on dif-
ferent benchmark problems. One of the conclusions is that, comparing to NSGA-II,
‚ÄúSPEA produced a much better distribution at the expense of a large computational
effort‚Äù [21]. In order to improve the distribution of NSGA-II, in 2005 Deb et al.
introduced the steady state into NSGA-II and adopted the Œµ-dominance concept to
control the distribution [21].
Deb et al. used Eq. 6.27 for their Œµ-dominance deÔ¨Ånition, i.e., objection spaces
are divided into hyperboxes evenly.
If users can provide (Œµ1,‚ãÖ‚ãÖ‚ãÖ,Œµm) for all objectives, then they express the idea that
they are disinterested in the difference between Œµ j for the jth dimension. We can
divide the objective space into hyperboxes and calculate the index of the represen-
tation point by Eq. 6.29:
bi
j =
‚åä
zi
j ‚àíf min
j
Œµ j
‚åã
(6.29)
where ‚åä‚åãis for rounding toward minus inÔ¨Ånity, zi
j is the jth Ô¨Åtness value of the ith
individual, f min
j
is the jth minimum Ô¨Åtness value in the population, and bi
j is the
index of the grid start from the minimum Ô¨Åtness value along dimension j. In this
way, we can construct hyperboxes using (Œµ1,‚ãÖ‚ãÖ‚ãÖ,Œµm), as in Fig. 6.17. For a given
set of (Œµ1,‚ãÖ‚ãÖ‚ãÖ,Œµm) and an archive, hyperboxes are determined. We can use the in-
dices calculated by Eq. 6.29 to express the representation point bi =
[
bi
1,bi
2,‚ãÖ‚ãÖ‚ãÖ,bi
m
]
,
which resides in the lower left corner of the hyperbox containing zi. That is all the
individuals residing in the same hyperbox have the same representation point.
A steady state GA starts with a population including popsize individuals. The
selection, crossover, and mutation processes are all individual-based, which means
every time a new individual or two, not a new population, is generated, the process
will substitute an individual or two in the population if the new one is good enough.
The changes in the population are much less dramatic than those of the generational
GA. If the initialization process generates widely distributed individuals across the

6.6 Cutting Edges of Multiobjective Evolutionary Algorithms
227
Fig. 6.17 Hyperboxes formu-
lated by (Œµ1,‚ãÖ‚ãÖ‚ãÖ,Œµm) provided
by users












3



	


	




objective space, it is reasonable to believe that a steady state GA, which replaces
the close parents with offspring, could maintain diversity better than a generational
one.
Deb et al.‚Äôs algorithm combines the archive and the steady state GA together and
has the main loop step as Fig. 6.18.42
Fig. 6.18 Deb et al.‚Äôs steady
state MOEA












.	




)
.C	 
C	
*
	
.C	

C	
There are Ô¨Åve procedures in Fig. 6.18. The ‚Äúgenerate‚Äù procedure is the same as in
ordinary GAs. In the ‚Äúpop-select‚Äù procedure, binary tournament selection based on
Pareto dominance is carried out. The winner is selected. If two individuals cannot
dominate each other, randomly select one. Because all the individuals thus far in the
archive are nondominated solutions, just randomly pick one in the ‚Äúarchive-select‚Äù
procedure.
After individual c is generated by p and a, it is considered for insertion into both
the population and the archive.
The ‚Äúpop-update‚Äù procedure is simple: (a) if c dominates at least one individual
in the population, c replaces any of the dominated ones; (b) if c is dominated by at
least one individual, discard c; (c) otherwise, c replaces a random individual in the
population.
The ‚Äúarchive-update‚Äù procedure is similar to that of the above archive mainte-
nance method converging to an Œµ-Pareto set. The only difference is in the second
logic consideration. If c is the winner of the Pareto competition or c is closer to the
42 A C language implementation of this algorithm can be downloaded at: http://www.iitk.
ac.in/kangal/codes.shtml.

228
6 Multiobjective Optimization
representation point of the hyperbox, then c replaces d. The adoption of distance
comparison between nondominated solutions promotes convergence.
Deb et al. claimed that the suggested steady state MOEA is a good compromise
in terms of convergence, diversity, and computation time.
6.6.3 Rebirth from the Ashes
The main reason for introducing the weighted sum method in Sect. 6.2.1 and VEGA
in Sect. 6.3 was to show the historical development process of MOEAs to readers.
In addition, recent developments of MOEAs promoted the rebirth of these ancient
methods. Generally speaking, new ideas are the results of reforming, reusing, and
reuniting old ones.
6.6.3.1 Rebirth of VEGA
We have discussed in depth the tradeoff between convergence and distribution. Dif-
ferent algorithms employ different mechanisms to achieve the best tradeoff. Like
Grefenstette‚Äôs metaevolution discussed in Chap. 3, in 2003 Toffolo and Benini used
VEGA to search for two objectives: the best Pareto rank and the best distribution
under the scheme of ES [22]. Also in 2003, Lu and Yen implemented the same
idea under the scheme of GA [23] and developed it further into a variable popsize
MOEA [24].
The rank-density-based genetic algorithm (RDGA) suggested by Lu and Yen
utilizes VEGA (Fig. 6.4) to deal with the tradeoff between convergence and dis-
tribution. The main differences between RDGA and VEGA are Ô¨Åtness assignment,
crossover operator, and elitism.
RDGA transforms the m-objective problem into a two-objective problem. For
each individual, it has two Ô¨Åtness values: rank and density. With respect to both of
them, the smaller, the better.
The deÔ¨Ånition of rank in RDGA of an individual is different from all the afore-
mentioned ranks. The rank for every nondominated individual is 1. Suppose that
the dominated individual y is dominated by P individuals y1,y2,‚ãÖ‚ãÖ‚ãÖ,yP, whose rank
values are already known as rank(y1),‚ãÖ‚ãÖ‚ãÖ,rank(yP). Then y‚Äôs rank can be calculated
as follows:
rank(y) = 1+
P
‚àë
j=1
rank(y j)
(6.30)
Equation 6.30 calculates the rank for each individual from the standard Pareto
rank 1 to higher ranks.The entire procedure can be carried out in an automatic ac-
cumulated way, hence its name: automatic accumulated ranking strategy (AARS).

6.6 Cutting Edges of Multiobjective Evolutionary Algorithms
229
In Fig. 6.6, the number in [ ] illustrates the AARS rank result.43
The density in RDGA is like that of the squeeze factor, illustrated in Fig. 6.8, but
in another adaptive way apart from Eq. 6.25. Users need to assign the grid numbers
for each objective. Suppose for the ith objective there are Ki grids and the negative
and positive ideal values in the current population are z‚àí
i and z+
i , respectively, then
the width of the box in i‚Äôs objective is as follows: 44
di = z‚àí
i ‚àíz+
i
Ki
(6.31)
The negative and positive ideal points in the current population will change dur-
ing the evolving process, and then the hyperbox size will vary accordingly. With
Eq. 6.31 we can determine the coordinates for the center of every hyperbox, thus
every individual can be assigned to the closest hyperbox. After that, the numbers of
individuals belonging to each hyperbox can be counted easily.
Figure 6.19 illustrates the evolving process in one generation of RDGA.



















A
A
4
4
4
2F
,	(




		
4
4
4
4
4
4


4
4
4


?	
	
''
*
	
*
	#
Fig. 6.19 The evolving process in one generation of RDGA
RDGA uses the selection process in the two-objective VEGA, and half of the
population is selected according merely to rank and another half merely to density.
In the crossover procedure, RDGA employs a mating restriction rule to promot-
ing the exploration and exploitation toward PF*. Individuals after the selection pro-
cess reside in the hyperboxes. For each hyperbox, a Ô¨Åxed number of parents are
randomly selected to cross over with the best individual (lowest rank) in that hyper-
box and the neighboring hyperbox. The offspring can survive only if they dominate
their parents or are less crowded than their parents.
43 Consider the situation where PF* is a straight line between (0,1) and (1,0) in a normalized two-
objective situation and the individuals are distributed evenly above PF*. Is there any preference in
AARS? If so, which individuals have advantages?
44 Compare it with the techniques in Sect. 6.6.2.1.

230
6 Multiobjective Optimization
Because RDGA concerns diversity with a probability of 0.5, some bad individu-
als (dominated by parents but residing in the empty area) may survive, which is not
good. So a forbidden region is set up to avoid such a situation.
Another characteristic of RDGA is that individuals in the archive have a proba-
bility pe of being selected as a parent to undergo crossover.
pe = 1‚àí
(
‚à£P‚à£
‚à£P‚à£+‚à£A‚à£
)2
(6.32)
where ‚à£P‚à£is the size of the population and ‚à£A‚à£is the size of the archive. The larger
A is, the greater the opportunity of an elitist being selected for crossover.
After new individuals have been generated by crossover and mutation, they might
be added to the archive if they dominate some part of the archive.
Finally, it is necessary to mention that the drawback of VEGA‚Äôs lacking diversity
is not important in RDGA because we only want one edge point in the Pareto front
of the 2-D problem (rank and density): rank = 1 and VEGA is good at maintain-
ing edge points. In this way, Lu and Yen utilized the drawback of VEGA to effec-
tively maintain the balance between quality and diversity. RDGA is competitive with
NSGA-II, SPEA2, and PAES according to Lu and Yen‚Äôs numerical experiments.
6.6.3.2 Rebirth of Weight Sum Method
The weight sum method was introduced in Sect. 6.2.1. Its main advantage over
dominance-based MOEAs is fast convergence. As for convergence, Miettinen has
proven that the optimal solution of the normalized weight sum of the m objectives
min
m
‚àë
i=1
œâi fi (x) is always Pareto optimal if œâ1,‚ãÖ‚ãÖ‚ãÖ,œâm are all positive or the solution
is unique [25]. So we can safely use the weight sum method to Ô¨Ånd nondominated
solutions. But there are several drawbacks obstructing the application of the weight
sum method in MOPs:
1. If the objective values are disparately scaled, i.e., whose value scales have large
differences, special normalization techniques are necessary, which limits the ran-
dom weight sum method.
2. Not all points on PF* have corresponding weight vectors. Only the point on con-
vex PF* can Ô¨Ånd a weight vector with which the weight sum has a minimum
on this point. This phenomenon is illustrated in Fig. 6.20.45 So we need other
techniques to transform multiple objectives into one.
For the convex PF* in Fig. 6.20, all the points on it and in its upper-right area are
legal solutions. For dashed line Œ±z1 +(1‚àíŒ±)z2 = h, its slope is ‚àíŒ±
1‚àíŒ± ‚àà(‚àí‚àû,0)
if 0 < Œ± < 1. The convex PF* is drawn using the equation z2 ‚àí1 =
1
z1‚àí1. So every
point on PF* has a slope ‚àí
1
(z1‚àí1)2 ‚àà(‚àí‚àû,0). With a decrease in h, the dashed line
45 Readers are encouraged to draw the PF* with a concave or partial-convex-and-partial-concave
shape to study this statement.

6.6 Cutting Edges of Multiobjective Evolutionary Algorithms
231
descends with the same slope. At the point at which PF* has the same slope as the
dashed line. The dashed line is the tangent of PF*. The intersection point is the
minimum of the weight sum function and also the corresponding nondominated
MOP solution. According to the above discussion, any point on convex PF* will
Ô¨Ånd a corresponding minimum for the weight sum function.










	


	



 

!





#1C3$

56
7#)#&#&##3#&"$(#"$
Fig. 6.20 Convex PF* and weight sum method
3. Even for convex PF*, evenly distributed samples of the weight vector might not
correspond to evenly distributed solutions on PF*. Let us discuss the above exam-
ple and let
Œ±
1‚àíŒ± =
1
(z1‚àí1)2 . We use even samples of 0.1,0.2,‚ãÖ‚ãÖ‚ãÖ,0.9 on Œ± and get
the nondominated solutions of (4.00,1.33),(3.00,1.50),‚ãÖ‚ãÖ‚ãÖ,(1.33,4.00). The
distance between these nine points are 1.013,0.497,0.343,0.290,0.290,0.343,0.497,1.014,
respectively.
Many researchers have attempted to overcome the above obstacles [26, 27]. In
2007, Zhang and Li suggested a MOEA based on decomposition (MOEA/D) to give
the weight sum method a new life [28].
For the problem of scaling, Zhang and Li suggested using adaptive normalization
for each objective. We can Ô¨Ånd the positive ideal point and negative ideal point in
the current population. Then the normalized objective value is
zi
k‚àíz+
k
z‚àí
k ‚àíz+
k , where zi
k is the
kth objective value of individual i, z+
k and z‚àí
k are k‚Äôs values of positive and negative
ideal points, respectively. Thus, every objective of the current population is mapped
into the range [0,1].
The Tchebycheff approach and boundary intersection approach are used to han-
dle the second drawback.46 These approaches ensure that any point on PF* has a
46 These approaches, together with the weight sum method, are decomposition methods by which
a single-objective aggregation function is formed with multiple objectives. Then we can use single-

232
6 Multiobjective Optimization
corresponding weight vector regardless of the shape of PF*. In these approaches,
we also need weight vectors to aggregate multiple objectives into one, but not sim-
ply add them up. We will denote the aggregation function as AF(ùùé,z), where z is
the normalized vector containing m objective values and ùùéis the weight vector.
The initial idea of MOEA/D is that for two close weight vectors ùùéi and ùùéj, their
corresponding optimal solutions of the aggregation functions are close because the
aggregation functions are continuous with ùùé. So if we can generate many evenly
distributed weight vectors before optimization and Ô¨Ånd the closest neighbors for
each vector according to Euclidean distance, we can do a single-objective search
with different directions and use the information of the neighbors to help the search.
The solution process of MOEA/D is as follows.
Solution Process of MOEA/D
Phase 1: Initialization.
Step 1.1: Assign the parameters for MOEA/D, such as popsize, clos-
est neighbor number T, stop criteria, etc.
Step 1.2: Generate
popsize evenly distributed weight vectors
ùùé1,‚ãÖ‚ãÖ‚ãÖ,ùùépopsize, each having m components ùùéi = œâi
1,‚ãÖ‚ãÖ‚ãÖ,œâi
m.
Step 1.3: Calculate the Euclidean distances between any pair of
weight vectors and thus Ô¨Ånd T closest weight vectors for each one. Set
B(i) = i1,‚ãÖ‚ãÖ‚ãÖ,iT, where ùùéi1,‚ãÖ‚ãÖ‚ãÖ,ùùéiT are T closest neighbors of ùùéi. We need to
point out that i ‚ààB(i). The main driving force in MOEA/D are weight vectors,
which is determined and whose neighbors are found in the initialization.
Step 1.4: Generate popsize initial individuals x1,‚ãÖ‚ãÖ‚ãÖ,xpopsize ran-
domly and calculate their objective values. Calculate the positive and negative
ideal points according to the initial individuals and normalize their objective
values using techniques discussed above as z1,‚ãÖ‚ãÖ‚ãÖ,zpopsize. gen = 0.
Phase 2: Main loop. Repeat the following steps until the stop criteria are
satisÔ¨Åed. In each loop, popsize individuals are evolved in a sequential way
using weight vectors. i = 1.
Step 2.1: Randomly select two indices, denoted as k and l, from B(i).
They correspond to individuals xk and xl. Use variation operators to generate
a new solution y from xk and xl. Calculate its normalized objective values
vector as z
‚Ä≤.
Step 2.2: For each index j ‚ààB(i), if AF(ùùéj,z
‚Ä≤) ‚â§AF(ùùéj,zj), x j = y.
In this way, we can maintain the best solution found thus far for the aggre-
gation function generated by weight vector ùùéj and later use it to promote the
local search for ùùéj‚Äôs neighbors.
Step 2.3: i = i+1. Go back to step 2.1 until i > popsize.
objective optimization algorithms to solve the aggregation function. We neglect the implementation
details of these decomposition methods and refer interested readers to [26, 27].

6.6 Cutting Edges of Multiobjective Evolutionary Algorithms
233
Step 2.4: Update the positive and negative ideal points with the current
population. g = g+1.
Phase 3: Submitting the Ô¨Ånal popsize individuals as the results of
MOEA/D.
Zhang and Li also suggested using local search to guarantee that the offspring y
will be a legal and feasible solution and they utilized the archive to contain the
nondominated solutions found thus far. But these techniques are optional.
Apart from inheriting the fast convergence property of optimizing a single-
objective aggregation function, MOEA/D established a good balance between ex-
ploration and exploitation. The convergence mechanisms of MOEA/D are the de-
composition method, which ensures that the optimal solutions of aggregation func-
tions are nondominated solutions, and the individual updating method in step 2.2,
which ensures that xi is the best solution found thus far for ùùéi. The diversity mecha-
nism of MOEA/D are (1) evenly distributed weight vectors, which direct the search;
(2) the mating restriction in step 2.1, which limits the search to a small area; (3) a
decomposition method that can handle the concave and convex shapes and provide
evenly distributed solutions on PF*.
For Pareto-dominance-based MOEAs, their ability to Ô¨Ånd edge points is subject
to variation operators. But MOEA/D‚Äôs ability to Ô¨Ånd edge points could be adjusted
by changing weight vectors. In addition, the property that the continuous aggre-
gation function of ùùéwill have close optimal solutions with close weight vectors
might force the nondominated solutions to be distributed between edge points in an
approximately even way.47 These two factors make MOEA/D good at PF* explo-
ration, especially when the shape of PF* is complex.
MOEA/D utilizes popsize evenly distributed weight vectors to search for popsize
evenly distributed nondominated solutions on PF* and use neighbor information to
accelerate the search. The closest neighbor number T of one weight vector is a
critical control parameter. Too small T might limit the exploration ability but too
large T might contribute worse parents with a high probability and thus limit the
exploitation ability. Numerical experimental results done by Zhang and Li illustrate
that T is not sensitive to benchmark problems.
In 2009, Li and Zhang adopted the directional mutation of DE in MOEA/D,
together with other techniques, making MOEA/D-DE a more efÔ¨Åcient MOEA
[29].48
47 Even though evenly distributed weight vectors would not guarantee evenly distributed nondom-
inated solutions.
48 The source code of MOEA/D-DE can be downloaded at: http://cswww.essex.ac.uk/
staff/zhang/.

234
6 Multiobjective Optimization
6.7 Performance Evaluation of Multiobjective Evolutionary
Algorithms
The reason for a special section on performance evaluation besides the one in
Chap. 3 is that situations in MOPs are quite different from those in single-objective
problems. Every algorithm can maintain a group of nondominated individuals at the
end of the run. Sometimes the result from one algorithm fully dominates the other,
which is the simplest condition. But generally, some results from one algorithm
dominate some from another algorithm, and vice versa.
Another reason for the special consideration on the performance evaluation is
that we are interested in not only the convergence to PF* but also the distribution
of the individuals along PF*. Adequately evaluating convergence and distribution is
still an open problem in the Ô¨Åeld of MOEAs.
Benchmark problem design is also an interesting Ô¨Åeld because we want to con-
veniently generate problems with different shapes of PF*, different convergent dif-
Ô¨Åculties, different dimensions, etc.
6.7.1 Benchmark Problems
In 2006, Husband et al. gave an excellent survey for generating multiobjective test
problems [30]. They suggested that good benchmark MOPs should contain follow-
ing characteristics.
1. The Pareto solution set should not reside at the edge of the feasible domain.
We introduced in Chap. 4 the idea that truncating an infeasible individual to the
edge of the feasible domain is one way of handling constraint problems. So this
technique may gain advantages in optimizing benchmark problems that do not
satisfy characteristic 1.
2. The Pareto solution set should not reside in the center of the domain. The in-
termediate crossover introduced in Chap. 2 may have advantages in optimizing
benchmark problems that do not satisfy characteristic 2.
3. Benchmark MOPs should have a scalable number of variables so that the de-
signer and the analyzer can generate arbitrary dimensional MOPs.
4. Benchmark MOPs should have a scalable number of objectives.
5. The variables of benchmark MOPs should have deÔ¨Ånition domains of different
magnitudes. This characteristic tests the ability to change mutation strengths with
different variables or the normalization ability of the algorithm.
6. The magnitudes of different objectives in PF* should be different.
7. The PF* of the problem can be expressed in explicit expression.
Besides all these requirements, Husband et al. also suggested that the following
features need to be considered in designing benchmark MOPs.

6.7 Performance Evaluation of Multiobjective Evolutionary Algorithms
235
1. PF* geometry. The geometry of the PF* in benchmark MOPs might show dif-
ferent shapes such as convex, concave, linear, disconnected, and combination of
these shapes in one PF*.
2. Parameter dependency. If we can Ô¨Ånd the PF* by optimizing one variable after
another, the variables of these kinds of problem are separable and the problem is
decomposable.49 By contrast, nonseparable variables or parameter dependency
makes a more difÔ¨Åcult problem.
3. Bias. If the evenly distributed points in the Pareto optimal solutions in P* map
into the unevenly distributed points in PF*, we say this problem has bias. Real-
world problems might always have a bias.
4. Many-to-one mapping. If different points in Pareto optimal set P* map into
the same point in PF*, this problem is a many-to-one mapping. The extreme
condition is that PF* has a Ô¨Çat region, which means small perturbations of the
decision variables do not change the objective values. Problems with many-to-
one mapping, especially with a Ô¨Çat region, are difÔ¨Åcult for MOEAs.
5. Modality. We introduced modality in Chap. 5. An objective function with only
one optimum is unimodal, which is easy to solve. Multimodal in a multiobjec-
tive environment means that a point in a small deÔ¨Ånition region deÔ¨Ånition is
the efÔ¨Åcient solution but is dominated by real Pareto optimal solutions. This
point is called a local optimal solution in MOPs. Multimodal MOPs are difÔ¨Åcult
for MOEAs, which has been proved by Li and Zhang using numerical experi-
ments [29].
We need to mention that for generating a test suite of benchmark MOPs, it is
not good to select all the ‚ÄúdifÔ¨Åcult‚Äù problems. The purpose of designing MOEAs
is to solve real-world MOPs. Not all of them are extremely difÔ¨Åcult. So it is nec-
essary to make a tradeoff between different benchmark problems and real-world
problems. Also, according to the No Free Lunch theorem discussed in sect. 3.6.1,
the average performance of any pair of algorithms across all possible problems is
identical. So if an algorithm is quite good at solving ‚ÄúdifÔ¨Åcult‚Äù problems, it might
exhibit overtraining-like characteristics in artiÔ¨Åcial neural networks and might not
solve ‚Äúeasy‚Äù problems properly.
Different benchmark MOP suites have been suggested to emphasize different
aspects. We will list some of these problems as the benchmark MOPs in Appendix.
These problems are selected from the suites listed below.50
1. Van Veldhuizen summarized the multiobjective test problems before 1999 and
selected seven of them as the benchmark [31].
2. ZDT. In 1999, Deb suggested a way to construct multiobjective test problems
systematically [32]. In Deb‚Äôs method, there is a function h to control the shape of
PF*, a function g to test the MOEAs‚Äô ability to converge to PF*, and a function
f1 to test the MOEAs‚Äô ability to distribute the individuals along PF*. In 2000,
Zitzler et al. used Deb‚Äôs method to generate six benchmark MOPs [33].
49 Those two terms were discussed in Sect. 3.2.2.1.
50 Readers with an interest in constructing the benchmark problem are encouraged to read these
papers in the listed sequence.

236
6 Multiobjective Optimization
3. DTLZ. In 2001, Deb et al. developed ZDT to nine scalable benchmark problems
[34].
4. OKA. In 2004, Okabe et al. suggested another way to generate benchmark MOPs
with an arbitrary Pareto optimal set shape and PF* shape [35]. Apart from two
examples to illustrate the effectiveness of the method, Okabe et al. also intro-
duced a way to measure the convergence difÔ¨Åculty in OKA.
5. WFG. In 2005 and 2006, Huband et al. suggested a new scalable benchmark
MOP suite with nine problems that contain and consider the characteristics and
features discussed above [30, 36].
6. In 2006, Iorio and Li pointed out that rotation might introduce difÔ¨Åculties for
MOEAs and suggested four rotated benchmark MOP examples [37].
7. In 2006, Deb et al. addressed the importance of parameter dependencies for de-
signing MOP benchmark problems and developed their ZDT and DTLZ through
variable linkage [38].
8. In 2007 and 2009, the IEEE Congress on Evolutionary Computation held spe-
cial sessions on multiobjective optimization and multiobjective optimization
with constraints, respectively. The technical reports illustrate the corresponding
benchmark problems [39, 40].
9. In 2009, Li and Zhang provided a new way of generating MOP benchmark prob-
lems with arbitrary prescribed PF* shapes and gave nine examples [29].
6.7.2 Performance Indices
After determining which benchmark MOPs to optimize, we need to make a careful
decision on how to evaluate the performance of different MOEAs. Those criteria are
performance indices (PI).
As was discussed above, we want MOEAs converge to PF*, so the convergence
is often the Ô¨Årst thing taken into consideration. Convergence performance can be
further divided into two groups: how many individuals belong to PF* and what is
the overall evaluation of convergence to PF*.
One of the reasons for using MOEAs in MOPs is to distribute the Ô¨Ånal individuals
evenly along PF*. So users are also interested in the distribution. Apart from that,
spread performance evaluates MOEAs‚Äô ability to capture the edge points of PF*,
which is sometimes taken into consideration.
So we can give the taxonomy of PIs in Fig. 6.21.51
There might be several situations in which to evaluate an algorithm. First, we
know PF* of the benchmark MOPs and want to evaluate how good the algorithm
is while optimizing the benchmark MOPs. Second, PF* is not known a priori, but
we want to compare the solutions of many algorithms. So the nondominated solu-
51 In 2003, Okabe et al. suggested an excellent survey on PI [41] and Zitzler proposed a very
theoretical analysis on PI also in 2003 [42]. Readers interested in designing and analyzing PIs are
urged to read them.

6.7 Performance Evaluation of Multiobjective Evolutionary Algorithms
237














Fig. 6.21 PI taxonomy
tions in the union of these Ô¨Ånal solutions constitute a reference set (RS). 52 Results
from different algorithms are evaluated with a RS. The PI for these two situations
is called a unary index (input of the PI is one solution set). For an algorithm, all the
nondominated solutions in the Ô¨Ånal archive are deÔ¨Åned as set S.
The third situation is that we would just like to compare two results (S1 and
S2), or we want to compare many different results pair-by-pair, then we need the PI
to point out which one is better, sometimes how much better. The PI for the third
situation is called a binary index, whose inputs are two solution sets.
In the comparison of two algorithms, the number of objective evaluations for
each algorithm should be the same to ensure the same search endeavor over the
objective space and their archives should have the same capacity to ensure the same
ability to report the nondominated solutions. For algorithm 1, all the nondominated
solutions in the archive are called S1. S2 contains all the nondominated solutions
found by algorithm 2.
We also need to point out again that the selection of the PI to be introduced is
based on the importance and the authors‚Äô interests.
6.7.2.1 Cardinality-based Performance Indices
If we already know PF*, or RS, we can evaluate the quality by counting how many
individuals in S are in PF*.
In 1999, Van Veldhuizen suggested a unary PI called error ratio (ER) [31].
52 The acronym RS was also used by scatter search in Sects. 2.4.2.1 and 6.6.1.2. In this section,
RS always implies the nondominated solution union.

238
6 Multiobjective Optimization
ER(S) =
‚à£S‚à£
‚àë
i=1
ei
‚à£S‚à£
(6.33)
where ei =
{ 0
zi ‚ààPF*
1
zi /‚ààPF* , zi is solution i in S, and ‚à£S‚à£is the cardinality of S.53 ER
can be calculated based on RS too. ER is to evaluate how bad a result is. So the
smaller ER is, the better.
In 1999, Zitzler suggested a binary PI called coverage (C) [43].
C(S1, S2) =
			
[
s2 ‚ààS2
			‚àÉs1 ‚ààS1 : s1 ‚â∫
‚àís2
]			
‚à£S2‚à£
(6.34)
C(S1, S2) is the percent of the individuals in S2 who are weakly dominated by S1.
The larger C(S1, S2) is, the better S1 outperforms S2 in C. It is necessary to point
out that generally C(S1, S2)+C(S2, S1) ‚àï= 1.
There is one drawback to C(S1, S2). Let us take the solutions in Fig 6.22 as an
example,54 where S1 is represented by squares and S2 is represented by cycles. It
is clear that C(S1, S2) = C(S2, S1) = 0.5. But we may prefer S1 because it is lower
than S2 in most areas. How to demonstrate such difference?
Fig. 6.22 Two solution sets to
be compared
































6.7.2.2 Volume-based Performance Indices
In 1999, Zitzler suggested a unary PI called hypervolume (HV) [43].55 The hyper-
volume is the size of the space dominated by S. In calculating HV, we need to point
53 We will omit the explanation for such operators in the following content of this section.
54 We will discuss the objective space of 2-D minimal problem as examples in this subsection.
Most of these examples could be expanded to higher dimensions.
55 This PI was originally called S metric by Zitzler, where S is for ‚Äúsize of the dominated space.‚Äù
It is now often referred to as hypervolume or hyperarea.

6.7 Performance Evaluation of Multiobjective Evolutionary Algorithms
239
out the reference point to compute, illustrated by √ó in Fig. 6.22. Then the size of
the space dominated by S1 is the size enclosed by the dotted line, and that by S2 is
the size enclosed by the dashed line. The larger HV of a solution set is, the better
it is. For the example in Fig. 6.22, HV (S1) = 1 √ó 9 + 2 √ó 8 + 2 √ó 6 + 2 √ó 4 = 45
and HV (S2) = 2 √ó 10 + 2 √ó 9 + 1 √ó 2 + 1 √ó 1 = 41. So S1 is better than S2 in HV.
Knowles and Corne reported in 2003 that the selection of the reference point might
inÔ¨Çuence the relative relationship between two sets of nondominated solutions [20].
To make the difference more obvious, in 1999 Zitzler suggested another binary
PI called coverage difference (D) [43].
{
D(S1, S2) = HV (S1 +S2)‚àíHV (S2)
D(S2, S1) = HV (S1 +S2)‚àíHV (S1)
(6.35)
In Fig. 6.22, HV (S1 +S2) = 2 √ó 10 + 2 √ó 9 + 1 √ó 6 + 2 √ó 4 = 52. D(S1, S2) =
11 is area Œ± and D(S2, S1) = 7 is area Œ≤. The larger D(S1, S2) is, the better S1
outperforms S2 in D.
In 1999, Van Veldhuizen suggested a similar PI called hypervolume ratio (HR)
[31].
HR(S1,S2) = HV (S1)
HV (S2)
(6.36)
The larger HR(S1, S2) is, the better S1 outperforms S2 in HR.
Because hypervolume-based PI is widely used and the calculation of HV is com-
plicated in high-dimensional situations, many efÔ¨Åcient ways of calculating HV have
been suggested recently [20, 44, 45].
6.7.2.3 Distance-based Performance Indices
Distance-based PI evaluate the performance of the solutions according to the dis-
tance to PF*, or RS.
In 1999, Van Veldhuizen suggested a unary PI called generational distance (GD)
[31]. First we need to deÔ¨Åne the minimum distance from S to PF* as
di = min
p‚ààPF‚àó
{‚àö
m
‚àë
k=1
(
zi
k ‚àízp
k
)2
}
(6.37)
where zi
k is the kth objective value of the ith individual and di is its minimum Eu-
clidean distance to PF*. Other norms, i.e., distance measure method, could also be
used. GD is deÔ¨Åned as follows:
GD(S) =
(
‚à£S‚à£
‚àë
i=1
dq
i
)1/q
‚à£S‚à£
(6.38)

240
6 Multiobjective Optimization
where q is a parameter. If q = 1, GD is equal to Deb‚Äôs Œ≥ PI [8] and Zitzler‚Äôs M‚àó
1
PI [43]. The smaller GD for one solution set is, the better it is in approaching PF*.
Laumanns introduced the concept of Œµ-dominance, discussed in Sect. 6.6.2.2,
[19] and in 2003 Zitzler et al. developed a deÔ¨Ånition of Œµ-dominance for perfor-
mance evaluation [42].
If zi
l ‚â§Œµzj
l , ‚àÄl ‚àà{1,2,‚ãÖ‚ãÖ‚ãÖ,m} , we say i Œµ-dominates j, expressed as i ‚â∫Œµ j.56
Figure 6.23 illustrates the meaning of Zitzler‚Äôs Œµ-dominance in two-objective situa-
tions.57 We neglect the difference between dominance and strong dominance in the
following discussion, so as to neglect the vertical and horizontal line extended from
point zi. If Œµ = 1, Œµ-dominance is the same as Pareto dominance. For Œµ > 1, the area
dominated by zi is enlarged because zi is only required to dominate the enlarged
version of zi, i.e., Œµzi. For Œµ < 1, the area dominated by zi is shrunk. So generally
speaking, Œµ-dominance relaxes the area of Pareto dominance Œµ times.
Fig. 6.23 Œµ-dominance sug-
gested by Zitzler et al.











3
.
	
C

C@


Zitzler et al. suggested a binary Œµ-dominance-based PI as follows:
IŒµ (S1,S2) = inf
Œµ‚ààR
{
‚àÄz2 ‚ààS2, ‚àÉz1 ‚ààS1 : z1 ‚â∫Œµ z2}
(6.39)
Equation 6.39 means we need to to Ô¨Ånd the minimum Œµ so that for every solution
z2 in S2, there will always be at least one solution z1 in S1 that Œµ-dominates z2.
According to Fig. 6.23 and Eq. 6.39, if IŒµ (S1,S2) > 1, at least one solution in S2 is
Pareto dominated by at least one solution in S1. For S1, the smaller the IŒµ (S1,S2)
value is, the better; for S2, the larger the IŒµ (S1,S2) value is, the better.
Zitzler et al. also suggested a pragmatic way to get IŒµ (S1,S2). For any two solu-
tions z1 and z2 from S1 and S2, respectively, z1
i /z2
i calculates Œµ in the ith objective.
If max
1‚©Ωi‚©Ωm
z1
i
z2
i < 1, then z1 Pareto dominates z2. Otherwise, z1 is dominated by z2 or z1
and z2 are nondominated by each other. So we can compare every individual pair
from S1 and S2 by
56 This deÔ¨Ånition is not in the original form, i.e., Eq. 6.26 or Eq. 6.27.
57 Compare it with Fig. 6.14.

6.7 Performance Evaluation of Multiobjective Evolutionary Algorithms
241
Œµz1,z2 = max
1‚â§i‚â§m
z1
i
z2
i
, ‚àÄz1 ‚ààS1,‚àÄz2 ‚ààS2
(6.40)
Equation 6.40 calculates the extent of superiority of z1 over z2 in the worst objective.
We can use Eq. 6.40 to illustrate the relationship between z1 and z2. We need to
mention that Œµz1,z2 = 1 means z1 covers z2.58
Provided we know the relationship between every two individuals in S1 and S2,
Eq. 6.41 Ô¨Ånds the best individual in S1 for dominating z2. We can use Eq. 6.41 to
illustrate the quality of z2. If Œµz2 < 1, then there is at least one individual in S1 that
dominates z2.
Œµz2 = min
z1‚ààS1
Œµz1,z2, ‚àÄz2 ‚ààS2
(6.41)
Then we can get the quality of every individual in S2 using Eq. 6.41. If S1 totally
dominates S2, the Œµz2 of every individual in S2 should be less than 1. So we can use
the best individual (with the largest Œµz2) to illustrate the quality of S2 compared to
S1 using Œµ-dominance as follows:
IŒµ (S1,S2) = max
z2‚ààS2
Œµz2
(6.42)
For the example given by Fig. 6.24, IŒµ (S1,S2) = 0.875 and IŒµ (S2,S1) = 1.5,
which means S1 is better than S2 in Œµ-dominance-based PI.59
Fig. 6.24 Example of Œµ-
dominance-based PI






























	





To draw conclusions using an Œµ-dominance-based PI, we need to calculate both
IŒµ (S1,S2) and IŒµ (S2,S1) [46].
‚àôIf IŒµ (S1,S2) ‚â§1 and IŒµ (S2,S1) > 1 , S1 is better than S2 in Œµ-dominance-based
PI, and vice versa.
‚àôIf IŒµ (S1,S2) > 1 and IŒµ (S2,S1) > 1, then at least one solution in S2 dominates
at least one solution in S1 and vice versa or all the solutions in S1 and S2 are
nondominated by each other; therefore no conclusion can be drawn regarding
which set is better.
58 Either z1 equals z2 or z1 dominates z2. Why might Œµz1,z2 = 1 mean that z1 dominates z2?
59 We encourage readers to calculate these two PI values using Eqs. 6.40‚Äì6.42.

242
6 Multiobjective Optimization
‚àôIf IŒµ (S1,S2) = 1 and IŒµ (S2,S1) = 1, then S1 = S2.
‚àôFor any solution set S, IŒµ (PF‚àó,S) ‚â§1 and IŒµ (S,PF‚àó) ‚â•1.
6.7.2.4 Attainment Surface-based Performance Indices
In 1996, Fonseca and Fleming suggested a way to display nondominated solutions
in an archive after the termination of the algorithm [47], and in 2000 Knowles and
Corne made a clear description for comparing two algorithms using attainment sur-
face [6]. In a 2-D situation, instead of connecting the solutions with lines, Fonseca
and Fleming used a dashed/dotted line to enclose the dominated area of nondomi-
nated solutions, which resembles hypervolume illustrated in Fig. 6.22. They called
such a dashed/dotted line the attainment surface.
Generally we need to run a MOEA on a benchmark MOP several times (at least
20 times, 30 or more are recommended). How do we evaluate these runs? Averaging
is not a good idea in MOPs. Union is also not so good.
Fonseca and Fleming suggested a 50% attainment surface to illustrate the aver-
age performance of a MOEA in many runs. First, we draw the attainment surfaces
for every run. Figure 6.25 illustrates three runs. The solid square, the square with
the small hole, and the square with the large hole are four nondominated solutions
for each run. The dashed line, dotted line, and the dashed-and-doted line illustrate
different attainment surfaces.
Fig. 6.25 Determining the
50% attainment surface






































Three attainment surfaces divide the objective space into three parts. Area 1 is
dominated by all the nondominated solutions in all runs. Area 2 dominates all the
nondominated solutions in all runs. Area 3 includes areas that are dominated by
some runs and dominates other runs.
We can draw several auxiliary straight lines from the origin going in different
directions in the Ô¨Årst quadrant. Every auxiliary line intersects with all the attainment
surfaces. Starting from the origin toward the direction of the auxiliary line, the Ô¨Årst
met point is the best one and the last met point is the worst one. Then in these
intersections, we can determine which point is the median point, i.e., in the middle
of the performance rank.

6.7 Performance Evaluation of Multiobjective Evolutionary Algorithms
243
If we draw enough auxiliary lines, we will get all the median points and their
corresponding partial attainment surface lines. Connecting these partial attainment
surface lines together will generate the 50% attainment surface, which is the average
performance of a MOEA. The solid line in Fig. 6.25 is the 50% attainment surface
for the three runs of the algorithm.
On comparing two algorithms, each of them has many runs on a benchmark
MOEA. We draw all the attainment surfaces as in Fig. 6.26. Squares (solid or with
holes) represent the nondominated solutions generated by algorithm 1 in two runs.
Circles (solid or with holes) represent the nondominated solutions generated by al-
gorithm 2 in two runs. Similar to the 50% attainment surface, we draw many aux-
iliary straight lines from the origin in different directions in the Ô¨Årst quadrant. The
intersections of every auxiliary line can be used to make a statistical test to deter-
mine whether or not the intersections for one algorithm are better than those of the
others with statistical signiÔ¨Åcance.60 Besides a rigorous statistical hypothesis testing
procedure, we can do the comparison in an empirical way. Suppose both algorithms
1 and 2 run the problem s times. For the lth auxiliary line, we get 2s intersections
and we are only concerned with the Ô¨Årst s intersections starting from the origin, i.e.,
the best s intersections. In these s points, if 70% of them belong to algorithm 1, we
conclude that algorithm 1 wins in line l; if 70% of them belong to algorithm 2, we
conclude that algorithm 2 wins in line l.61 If neither of the algorithms wins, we say
that there is no signiÔ¨Åcant conclusion for this auxiliary line. For all L lines, algo-
rithm 1 wins a times and algorithm 2 wins b times. If a > b, we say that generally
algorithm 1 outperforms algorithm 2, and vice versa. It is necessary to point out
that a+b ‚àï= L. Using attainment-surface-based PI, we cannot only determine which
algorithm is better but also, perhaps more importantly, know which algorithm wins
in which region.
Fig. 6.26 Comparing results
from different algorithms
using attainment surface



































60 We introduced the statistical comparison in Chap. 3. Readers interested in drawing statistical
conclusions are referred to [48].
61 The larger the percentage,the clearer the statistical signiÔ¨Åcance.

244
6 Multiobjective Optimization
6.7.2.5 Distribution Performance Indices
We will introduce three distribution PI examples.
The Ô¨Årst one was suggested by Schott in 1995 [49]. Schott called it spacing (SP),
which is the standard deviation of the closest distances. Schott used the 1-norm,
instead of 2-norm (Euclidean distance), for evaluating the distance.
di =
min
sj‚ààS‚à©s j‚àï=si
m
‚àë
k=1
			zi
k ‚àízj
k
			
(6.43)
where di is the smallest distance from the ith individual in S. Then the average
smallest distance of all individuals is calculated as follows:
¬Ød =
‚à£S‚à£
‚àë
i=1
di
‚à£S‚à£
(6.44)
SP is the standard deviation of di.
SP(S) =
0
1
1
‚é∑
1
‚à£S‚à£‚àí1
‚à£S‚à£
‚àë
i=1
(
di ‚àí¬Ød
)2
(6.45)
A smaller SP might mean better distribution. But Okabe et al. pointed out that the
smallest distance can be used twice, which might lead to wrong conclusion [41].62
The second distribution PI, suggested by Zitzler as M‚àó
2 in 1999, is based on the
niche concept [43].
M‚àó
2 (S) =
1
‚à£S‚à£‚àí1
‚à£S‚à£
‚àë
i=1
		sj ‚ààS
		33si ‚àísj
33 > œÉ
		
(6.46)
where ‚à•‚à•is a means of distance, such as Euclidean distance, and œÉ is the radius
of the niche. For the ith solution in S, we Ô¨Årst Ô¨Ånd out how many individuals are
far from it and use it as a representative of sparsity. Then we calculate the average
isolated individual number to demonstrate the overall distribution. Larger M‚àó
2 means
better distribution.63
The third PI, suggested by Li and Zhang, considers convergence and distribution
by inverting the meaning of generational distance (IGD) as follows [29]. We Ô¨Årst
need to get PF* or RS and select N solutions evenly on PF* or RS as representations.
For each nondominated solution in PF* or RS, the closest individual in S and the
corresponding distance can be found as follows:
di = min
p‚ààS
{‚àö
m
‚àë
k=1
(
z‚àói
k ‚àízp
k
)2
}
(6.47)
62 Readers are encouraged to calculate the SP for (0,8), (1,7), (7,1), (8,0).
63 What is the largest value of M‚àó
2?

6.7 Performance Evaluation of Multiobjective Evolutionary Algorithms
245
where z‚àói
k is the kth objective value of the ith solution in PF* or RS. IGD is deÔ¨Åned
as follows:
IGD(S) =
N
‚àë
i=1
di
N
(6.48)
As can be seen from Eq. 6.48, in the early stage of evolving, IGD mainly eval-
uates the convergence ability of MOEAs. Later, while the archive or population is
approaching PF*, smaller IGD requires the individuals in S being distributed simi-
larly to the N selected solutions, which promotes the even distribution of the results.
The minimum of IGD is zero, which means that the MOEA found every represen-
tation of PF* or RS. So larger number of N is preferred.
6.7.2.6 Spread Performance Indices
Spread is used to evaluate the ability of the algorithm in extreme conditions (one
weight is 1 and the others are 0). In 1999, Zitzler suggested M‚àó
3 to evaluate the
spread [43].64
M‚àó
3 (S) =
‚àö
m
‚àë
i=1
max{‚à•ui ‚àívi‚à•‚à£u, v ‚ààS}
(6.49)
where ‚à•‚à•is a way of measuring distance. We Ô¨Årst Ô¨Ånd the largest distance in differ-
ent objectives and calculate their sum square. The larger M‚àó
3, the better the spread.
6.7.2.7 Distribution and Spread Performance Indices
In 2000, Deb et al. proposed a way to evaluate distribution and spread simulta-
neously for two-objective MOPs [8]. Let us take Fig. 6.27 as an example, where
squares are solutions of S and circles are edge points generated from PF* or RS.
Fig. 6.27 Example of distri-
bution and spread PI
















89:"#&
8!#&


	

95
95














64 In Sects. 6.4 and 6.5, we ask readers to consider the preferences for different rank schemes. Do
those that prefer the side (not the center) have advantages in Ô¨Ånding the edge points?

246
6 Multiobjective Optimization
We Ô¨Årst calculate the distance between consecutive solutions, i.e., d(1,2), d(2,3),
d(3,4) in Fig. 6.27, and denote it di.65 Then we calculate the average of the con-
secutive distances as ¬Ød =
‚à£S‚à£‚àí1
‚àë
i=1
di
‚à£S‚à£‚àí1 . The distribution and spread PI (Œî) are deÔ¨Åned as
follows:
Œî (S) = df +dl +‚àë
‚à£S‚à£‚àí1
i=1
		di ‚àí¬Ød
		
d f +dl +(‚à£S‚à£‚àí1) ¬Ød
(6.50)
where d f and dl represent the smallest distance between the edge points and the
boundary solutions of S, i.e., the distance between point 1 and EP1 is d f (= 0) and
the distance between point 4 and EP2 is dl (= d(4,EP2)) in Fig. 6.27. If d f = dl = 0,
S has a good spread property. Based on that, if solutions in S distribute evenly,
Œî (S) = 0. By contrast, if solutions in S are crowded into one small group, even
if they are distributed evenly, Œî (S) is getting larger.66 A smaller Œî means a better
distribution and spread.
In 2003, Leung and Wang proposed another way, U-measure, to evaluate distri-
bution and spread simultaneously [50]. They suggested an efÔ¨Åcient way to calculate
the smallest distance to others for individual i, i.e., di
1. The U-measure also requires
the smallest distance di
2 to the edge points, like d f and dl in Œî.
A good distribution and spread of S means that
{
di
1
}
are similar and
{
d j
2
}
ap-
proaches 0. Leung and Wang uses an innovative way to combine these two objec-
tives by generating a new set
{
d
‚Ä≤i}
with D = ‚à£S‚à£+ m individuals. For the smallest
distances between individuals
{
di
1
}
, d
‚Ä≤i = di
1; for the minimal distance to the edge
points, d
‚Ä≤ j = d j
2 +
‚à£S‚à£
‚àë
i=1
di
1
/
‚à£S‚à£. By this transformation, the requirements for distribu-
tion and spread become the requirement that
{
d
‚Ä≤i}
be similar.
Then Leung and Wang deÔ¨Åne the ideal distance, which is the average of
{
d
‚Ä≤i}
:
dideal =
D
‚àë
i=1
d
‚Ä≤i
D
(6.51)
If the nondominated individuals are evenly distributed and occupy the edge
points, d
‚Ä≤i/
dideal ‚Üí1 for all i = 1,‚ãÖ‚ãÖ‚ãÖ,D, then the U-measure calculates the dis-
crepancy among
{
d
‚Ä≤i}
:
U (S) = 1
D
D
‚àë
i=1
					
d
‚Ä≤i
dideal
‚àí1
					
(6.52)
65 Unlike Eq. 6.43.
66 ‚àë
‚à£S‚à£‚àí1
i=1
		di ‚àí¬Ød
		 = 0, but d f and dl are not zero.

6.8 Objectives vs. Constraints
247
The smaller U is, the better the distribution and spread.
By the end of this subsection, we need to mention that [42]:
‚àôNo single PI is able to account for all aspects of the quality of MOEAs.
‚àôWith unary indices, we can only say that algorithm 1 is no worse than algorithm
2 if the PI of algorithm 1 is better than that of algorithm 2.
Sect. 6.6.2 introduced several techniques to maintain an archive with limited ca-
pacity. In 2003, Knowles and Corne suggested an archive maintenance method using
performance index [20]. For any newly generated individual i, while the archive is
full, it can enter the archive if either of the two following conditions satisÔ¨Åed. (1)
It dominates any member of the archive. (2) PI(s) are improved by adding the new
indidivual and removing one member from the archive.
6.8 Objectives vs. Constraints
There are two problems in the relationship of objectives and constraints. How are
constraints handled in a MOP environment? Can constraints be regarded as objec-
tives and MOEAs used to solve single-objective constrained problems? We will
discuss these problems separately.
6.8.1 Handling Constraints in Multiobjective Optimization
Problems
Generally speaking, the methods discussed in Chap. 4 can be used to handle the
constraints in MOPs. But if we could implant the feasibility requirement into the
binary tournament selection, which is used in almost all MOEAs, then the constraint
handling problem might be solved in a more Ô¨Çexible way.
In NSGA-II [8], Deb et al. suggested an intuitive idea to modify the Pareto rank
procedure in Fig. 6.10 and many MOEAs have adopted their idea in implement-
ing the algorithm. We just need to make a small change in Pareto dominance by
turning it into constrained dominance.67 In objective space, if any of the following
conditions is true, point i constrained-dominates point j.
1. Point i is feasible and point j is infeasible.
2. Both of them are infeasible, but the overall constraint violation of point i is less
than that of point j.
3. Both of them are feasible; point i Pareto dominates point j.
After determining the constrained-dominance pairwise, the rank can be allocated
by Fig. 6.6. Then all the feasible solutions will have selective advantages both in
67 This idea comes from Deb‚Äôs constraint handling technique published in 2000 [51].

248
6 Multiobjective Optimization
the Pareto ranking procedure and in the tournament selection, which accelerates the
convergence toward the feasible region.
6.8.2 Multiobjective Evolutionary Algorithms for Constraint
Handling
In real-world problem modeling, designers need to satisfy many requirements. Some
of them are boundary requirements, which are often modeled as constraints, and
some of them are tendency requirements, which are often modeled as objectives.
But there exist some requirements that could be modeled as both objectives and
constraints. Let us take the cost of a product as an example. Of course, lower cost
means more competitive advantages in the market. But the designer knows that costs
cannot be reduced without any side effects, such as lower quality. Yes, we could
model the quality as another objective and make it a MOP. But sometimes quality
(and other factors) is hard to express with formulas of design variables. Why not
model the costs as a constraint to ensure that they are below the upper bound?
Another viewpoint is from MOEAs. We have discussed many effective algo-
rithms that can handle multiple objectives. Why not fully use the power of MOEAs
to make them handle the constrained satisfaction problems?
Considerations from these two sides promoted the Ô¨Åeld of applying the MOEAs
to solve the constrained optimization problem (COP).68 Fonseca and Fleming for-
mulated this idea formally in 1998 [52]. In 2002, Mezura-Montes and Coello Coello
provided a numerical comparison of multiobjective-based techniques to handle con-
straints [53] and in 2006 gave an excellent survey in this Ô¨Åeld [54].
If we want to use MOEAs to solve COPs, constraints need to be transformed
into objective(s). In Mezura-Montes and Coello Coello‚Äôs classiÔ¨Åcation, there are
two categories of transformation.
The Ô¨Årst category is to transform the COP into a two-objective problem. One
objective is the original objective and the other is the overall violation of the con-
straints. We want to minimize both of them simultaneously.
The second category is to transform the COP into a (k + 1)-objective problem,
where k is the number of constraints and every objective related to one constraint is
the violation of that constraint. We want to minimize all of them simultaneously.
Here we need to present the constrained optimization problem (COP) again for
convenience.
min f (x), x ‚àà‚Ñùn
s.t.
gi (x) ‚©Ω0,
i = 1,‚ãÖ‚ãÖ‚ãÖ,q
h j (x) = 0,
j = q+1,‚ãÖ‚ãÖ‚ãÖ,k
(6.53)
68 We are only interested in single-objective COP here.

6.8 Objectives vs. Constraints
249
If a point x satisÔ¨Åes gi (x) = 0 for inequality constraint i, we say that constraint i is
active at point x. All equality constraints are considered active at feasible region F.
Runarsson and Yao presented an intensive discussion in 2005 [55] on the above
ideas and concluded that a search bias toward the feasible region must be intro-
duced in optimizing the MOPs formulated by the procedure discussed above. Their
opinion is illustrated by Fig. 6.28.69 The Pareto front of the two-objective problem
has been illustrated. The global optimum must be feasible. So it must reside in the
line constraint ‚àíviolation = 0, which is the thick horizontal line in Fig. 6.28. And
the objective of the problem is to Ô¨Ånd as small an objective value as possible. So
the circle at the intersection of the feasible line and the Pareto front is the point we
want, which means we do not need the evenly distributed solutions at the end of the
algorithms, but one edge point on the PF*. That is the meaning of Runarsson and
Yao‚Äôs bias toward the feasible region.
Fig. 6.28 Using MOEAs to
solve constraint problems












%

& 
5 	
* 	
.
	'
	
)
6.8.2.1 Transforming Constrained Problems into (1+k)-objective Problems
For q inequality constraints, we can do the transformation in the following way:
fi (x) = max (0,gi (x) ‚©Ω0),
i = 1,‚ãÖ‚ãÖ‚ãÖ,q
(6.54)
For k ‚àíq equality constraints, we can deÔ¨Åne
fi (x) = ‚à£hi (x)‚à£,
i = q+1,‚ãÖ‚ãÖ‚ãÖ,k
(6.55)
Or we can Ô¨Årst transform equality constraints into inequality constraints:
‚à£hi (x)‚à£‚àíŒµ < 0,
i = q+1,‚ãÖ‚ãÖ‚ãÖ,k
(6.56)
where Œµ is a predeÔ¨Åned small tolerance. Then Eq. 6.54 can be used to transform the
above inequality constraint into an objective.
69 It seems that Fig. 6.28 only addresses the Ô¨Årst category, but the second one has the same conclu-
sion.

250
6 Multiobjective Optimization
After the procedure, we get an unconstrained MOP with k + 1 minimum objec-
tives.
As mentioned above, bias should be taken in account in treating these k +1 ob-
jectives, where the Ô¨Årst objective is the original objective.
In 2003, Angantyr et al. proposed a way to use Pareto rank to solve (k + 1)-
objective problems [56]. They Ô¨Årst rank the population according to the Ô¨Årst objec-
tive. A value rank1 (x j) is given for individual j. The smaller, the better.
Then Goldberg‚Äôs Pareto rank method (Fig. 6.6) is carried out on the remaining k
objectives. A value rank2 (xj) is given for individual j. The smaller, the better.
Angantyr et al. used the following equation to express their consideration for
exploration and exploitation in the constrained solution space:
œï (xj) = feasize
popsizerank1 (x j)+ popsize‚àífeasize
popsize
rank2 (x j)
(6.57)
where œï (xj) is the Ô¨Åtness value for individual x j, popsize is the number of indi-
viduals in the population, and feasize is the number of feasible individuals in the
population. Equation 6.57 can be discussed with the following considerations.
1. If there is no feasible individual in the population, œï (xj) = rank2 (x j), the search
is directed toward the feasible region.
2. If there are feasible and infeasible individuals in the population, the feasible ones
might have less selective advantage than the infeasible ones according to the
percentage of feasible individuals.
3. If the majority of the population is feasible individuals, the search is directed
toward the unconstrained optimum.
Considerations 2 and 3 might cause the oscillation in the evolving process if the
unconstrained optimum is infeasible. But Angantyr et al. claimed that the oscillation
was helpful for exploring the global feasible optimum.
In 2003, Aguirre et al. suggested another way to handle k + 1 objectives based
on PAES [57]. The algorithm is a (1 + 1)-ES, maintaining an archive A, and only
requires Pareto dominance. The solution process is as follows:
One Generation of the Algorithm Suggested by Aguirre et al.
Phase 1: Generating new individual c by mutating parent h on every vari-
able dimension i using œÉi. Unfamiliar readers are referred to Sects. 2.3.1 and
3.2.2.3.
Phase 2: Elitism. Maintain the best feasible solution found thus far. No
need to insert it in the evolving process.
Phase 3: Determining whether c could replace h and whether c could enter
A. The determining procedure was elaborated in Sect. 6.5.3.

6.8 Objectives vs. Constraints
251
Phase 4: Selecting randomly from A to reinitialize h for every g genera-
tions. Angantyr et al. suggest that g = 10.
Phase 5: Shrinking the search domain for every r generations. Angantyr et
al. suggest that r = 1 or 2.
Step 5.1: Select the top 15% from A. For every constraint, the infeasi-
ble individuals are eliminated until the remaining individuals of A is 15%A. If
there are more than 15%A feasible individuals in A, select the best 15%A and
discard others.
Step 5.2: Determine the upper bound ¬Øxi (t) and lower bound xi
‚àí(t) in
generation t for every variable i (i = 1,‚ãÖ‚ãÖ‚ãÖ,n) using the 15%A individuals.
Suppose the interval for variable i is widthi (t) = ¬Øxi (t)‚àíxi
‚àí(t).
Step 5.3: Shrink the variable interval using a factor 0 < Œ≤ < 1 by
widthi (t +1) = widthi (t)Œ≤ and generating ¬Øxi (t +1) and xi
‚àí(t +1) accord-
ingly.
Step 5.4: Calculate the standard deviation for generating new individ-
uals by œÉi (t +1) = widthi (t +1)
/‚àön , i = 1,‚ãÖ‚ãÖ‚ãÖ,n.
In this way, the algorithm shrinks the mutation interval for every r generations to
exploit the global optimal solution and maintains a balance between the feasible and
infeasible individuals.
6.8.2.2 Transforming Constrained Problems into Two-objective Problems
In 2006, Cai and Wang suggested a way to transform COPs into two-objective prob-
lems [58], one of them being the original objective and the other one the overall
constraint violation. Their main idea could be expressed as follows:
‚àôA good NLP problem solver needs to direct the search toward the feasible region
effectively. So information contained in the infeasible solutions is quite helpful,
especially when the global optimal solution is at the edge of a feasible region or
the proportion of feasible region to whole region is small. So they use an archive
A to store the ‚Äúbest‚Äù infeasible individuals (with small constraint violation) when
all the offspring are infeasible.
‚àôTo Ô¨Ånd the global optimal solution, the algorithm should employ effective ways
to generate, compare, and replace individuals. So Cai and Wang implemented
their algorithm under the general ES framework.
The algorithm contains two groups of individuals: P, which is equal to the pop-
ulation in GAs, and A, which stores the ‚Äúbest‚Äù infeasible individuals close to the
feasible region.
The solution process is as follows.

252
6 Multiobjective Optimization
One Generation of the Algorithms Suggested by Cai and Wang
Phase 1: Improving P.
Step 1.1: Select Œº individuals from P randomly to form the parents in
ES. Then delete the Œº individuals from P.
Step 1.2: Generate Œª individuals from Œº individuals using simplex
cross-over (SPX) to form the offspring in ES. SPX has been discussed in Sect
3.2.2.1.
Step 1.3: Determine the nondominated individuals in Œª offspring us-
ing Pareto dominance on the two-objective problem. Suppose there are m
‚Ä≤
nondominated offspring.
Step 1.4: For every nondominated offspring in m
‚Ä≤, try to Ô¨Ånd one in-
dividual in Œº parents to be replaced. If an offspring does not dominate any
parent, there is no replacement. If an offspring only dominates one parent, it
replaces the parent. If an offspring dominates more than one feasible parent,
it replaces the worst one. Otherwise (the offspring dominates some infeasible
parents), it replaces one randomly.
Step 1.5: Combine the parent set with P.
Phase 2: Updating A. If there is no feasible solution in the Œª offspring, Ô¨Ånd
the infeasible individual with the lowest degree of constraint violation and add
it to A.
Phase 3: Inserting individuals of A into P for every m generations. For
every m generations, randomly select n individuals from A and replace n ran-
domly selected individuals in P. Then empty A.
Phase 1 is a (Œº + (m
‚Ä≤,Œª))-ES. There is an alternative way, which is to randomly
select only one nondominated offspring to replace one parent. The second way is a
(Œº +(1,Œª))-ES.
Besides the above two ideas, Cai and Wang also analyzed the difÔ¨Åculty of equal-
ity constraints. If we do not transform them into inequality constraints (Eq. 6.56),
the ratio of feasible region to solution region is nearly zero, which means feasible
points are far fewer in number than infeasible points in the neighborhood of the
global optimal solution.
In the procedure above, if Œº parents do not contain any feasible points and the
nondominated offspring include feasible solutions but they do not dominate the
infeasible parents, the feasible solutions cannot enter P.70 To make things worse,
phase 3 in the above procedure will insert infeasible individuals for every m gen-
erations. So in the Ô¨Ånal stage of the evolution, the population is approaching the
global optimal solution, which resides in the line of equality constraints, and it will
gradually lose all the feasible solutions step by step.
Cai and Wang made a pragmatic yet effective change in the above procedure to
avoid such a tragedy. They deÔ¨Åne a condition: individuals in P are all infeasible and
70 Consider this statement with the help of Fig. 6.28.

6.9 Application Example
253
their objective values are similar, which means P is approaching the global optimal
solution but it is hard to contain the feasible solution due to equality constraints.
If the condition holds, replacement rules for m
‚Ä≤ nondominated offspring replacing
parents, i.e., step 1.4, are changed to one similar to Deb‚Äôs idea discussed in Sect.
6.8.1. For every nondominated offspring, randomly select one individual among
the parents and begin the competition. Feasible offspring will replace infeasible
parents. Infeasible offspring with smaller constraint violations will replace the worse
infeasible parents. Apart from that, if the condition holds, phases 2 and 3 will not
be carried out. In this way, a strong preference for feasible individuals is set up, no
more infeasible individuals will be inserted into P, and the tragedy discussed above
will be avoided.
The archive scheme directs the evolution toward the feasible region. The Pareto
dominance in the replacement procedure directs the evolution toward feasible solu-
tions with good objective values. SPX explores the solution space effectively. The
tragedy-avoiding mechanism handles the equality constraint in a Ô¨Çexible way. All
these factors make the algorithm suggested by Cai and Wang ‚Äúremarkably outper-
forms‚Äù state-of-the-art COEAs including the stochastic ranking.71
6.9 Application Example
There are so many real-world applications of MOEAs on MOPs. Here we just in-
troduce the one on the design of superconducting magnetic energy storage (SMES)
solenoids suggested by Zhao and Yu in 2008 [59].
The SMES system has an attractive potential in power systems and other areas. A
solenoid coil, one important conÔ¨Åguration of SMES, can be simply built and provide
high energy density per unit of the conductor.
Many factors need to be considered in the design of solenoid SMES.
1. Coil volume. The volume determines the occupation of area, which in turn de-
termines the installation and using cost.
2. Uniformity of magnetic Ô¨Åeld. In some applications (such as magnetic resonance
imaging, MRI), solenoids are required to provide a uniformly distributed mag-
netic Ô¨Åeld.
3. Energy requirement. The energy determines the requirement of the user.
4. Stray Ô¨Åeld. The SMES should be safe for people far enough away from it.
5. Superconductivity requirement. In order to maintain superconductivity, the
current density J and the magnetic Ô¨Åeld B must ensure that the working point
in the B‚àíJ plane lies below the critical quenching curve.
The design variables are the inner radius of the solenoid coil (Ri ), the outer radius
of the solenoid coil (Ro), the height of the solenoid coil (h), and the current density
fed into the solenoid coil (J) (Fig. 6.29).
71 Stochastic ranking was discussed in Chap. 4.

254
6 Multiobjective Optimization
Fig. 6.29 Solenoid coil











/LQHE
/LQHD
':
':
'
'

The volume can be calculated easily with Ri, Ro, and h.
The uniformity of the magnetic Ô¨Åeld can be calculated by
U = ‚à£Bm ‚àíB0‚à£
B0
(6.58)
where Bm represents the maximum of the magnetic Ô¨Çux density in the solenoid and
B0 is the magnetic Ô¨Çux density at the center of the solenoid. An evenly distributed
magnetic Ô¨Åeld means small U.
Figure 6.29 illustrates the calculation of the stray Ô¨Åeld. Lines a and b are bor-
derline for evaluating the stray Ô¨Åeld. m points are picked with the same space along
lines a and b. The average stray Ô¨Åeld is calculated using
B2
s =
m
‚àë
i=1
‚à£Bsi‚à£2
m
(6.59)
where Bsi is the leak magnetic intensity at point i.
The linear approximation of the real critical quenching curve can be considered
a quench condition and can be expressed as follows. It must not be violated at any
point in the coils.
‚à£JC‚à£= (‚àí6.4‚à£Bm‚à£+54.0)
MA/m2
(6.60)
We take factors 1 and 2 as objectives and 3 to 5 as constraints and get a two-
objective design model as

6.9 Application Example
255
min Vs
min U = ‚à£Bm ‚àíB0‚à£
B0
s.t ‚à£E ‚àíE0‚à£
E0
< Œµ
Bs < Bs0
J < œÉJc
(6.61)
where Vs is the solenoid volume, E is the energy stored in the solenoid, E0 is the
design target of energy, Œµ represents the acceptable tolerance of energy error, Bs0 is
the safe magnet stray Ô¨Åeld, and 0 < œÉ < 1 is the safe factor of the current density.
According to Zhao and Yu‚Äôs calculation, the design model is highly constrained,
which means that the ratio of the feasible region to the search space is very small.
So we adopt (Œº +Œª)-ES to solve the problem.
In the replacement stage of ES, we divide the (Œº +Œª) individuals into four groups
[feasible Pareto (FP) solutions, feasible dominated (FD) solutions, infeasible Pareto
(IP) solutions, and infeasible dominated (ID) solutions] and use binary tournament
selection with replacement to determine the Œº parents in the next generation.
A comparison of the group is summarized in Table 6.1.
Table 6.1 The regulation of the binary tournament selection




5.
5,
.
,
5.
	 	 5. $	 

%

$%	 	5.
	5.
	5.
5,
	5.
	 	 5, 

 	
5.
	 5, 
 .  ( 

	
	
	5,
.
	5.
	 . 
 5,  ( 

	
	
	 	 . $	 
	
		
	.
,
	5.
	5,
	.
4	
















With (Œº + Œª)-ES and comparison rules in Table 6.1, the feasible Pareto front is
found after 200 generations with Œº = 50. The real solenoid SMES design parameters
are determined by picking one from the nondominated solutions and improving it
manually.

256
6 Multiobjective Optimization
6.10 Summary
Basically there are two ways to handle MOPs. If users can provide some kind of
preference information, such as weights on objectives, goals of objectives, etc., the
preference-based approach discussed in Sect. 6.2 is quite efÔ¨Åcient.
On the other hand, if the designer and the decision maker need to pick one so-
lution from many nondominated solutions, MOEAs should try to converge toward
PF* and distribute the individuals evenly along PF*. In Sect. 6.4, we introduce how
to evaluate the quality and distribution of individuals during the evolution, which is
extremely important for designing and analyzing MOEAs.
Classical MOEAs such as NSGA-II and SPEA2 were discussed in depth in Sect.
6.5 and some interesting state-of-the-art considerations were introduced in Sect. 6.6.
Readers should learn and borrow the innovative ideas from those sections.
In a multiple-objective environment, comparisons between the results of two al-
gorithms are hard to make because we have multiple objectives to compare: conver-
gence, distribution, and spread. Many performance indices were introduced in Sect.
6.7.
Transforming constraints into objectives and using MOEAs to solve the trans-
formed MOP is a wonderful idea for treating single-objective constrained problems.
In Sect. 6.8, we give the classiÔ¨Åcation Ô¨Årst, and then discuss three inspiring algo-
rithms.
After reading this chapter, you should understand the terms used in MOEAs,
have a full understanding of at least one classical MOEA, know some improvement
techniques, know how to make unbiased comparison of algorithms, and improve
your constraint-handling skills.
In all, designing a MOEA is the art of tradeoff between converging toward PF*
and distributing evenly along PF*.
Suggestions for Further Reading
Deb published the Ô¨Årst MOEA monograph in 2001 [60]. Coello Coello et al. pub-
lished their monograph on MOEAs in 2002 and revised it for a second edition in
2007 [61]. The third monograph on MOEAs was by Knowles et al. in 2008 [62].
In these three books, topics are discussed in more detail and more areas covered.
Books on the application have been written by Coello Coello et al. [63] and Tan et
al. [64].
There are some comparison papers [33, 65‚Äì67] suitable for reading to learn both
which algorithm is better in numerical experiments and how to design, implement,
and summarize comparisons of MOEAs.
We introduce the treatment of two-objective problems in this chapter for peda-
gogical reasons. But real-world MOPs might contain multiple objectives. Numerical
experiments indicate that the performances of MOEAs do not scale well with respect
to the number of objectives. Interested readers are referred to [34, 68].

6.10 Summary
257
We have not introduced the fast nondominated sorting approach to NSGA-II be-
cause we wished to focus on the quality and diversity of solutions at Ô¨Årst for students
reading this textbook. We encourage readers to check [8] for this approach before
carrying out their implementation. Jensen suggested a faster approach to nondomi-
nated sorting in 2003 [69].
If readers have a greater interest in evaluating quality (rank) during evolution,
they are suggested to read the paper published by Pierro et al. in 2007 [70]. Another
paper by Chan et al., published in 2008, discusses how to design new operators that
explore and exploit more efÔ¨Åciently for PF* [46].
We only discussed archive maintenance techniques with limited capacity in Sect.
6.6.2. In 2003, Fieldsend et al. suggested an efÔ¨Åcient data structure, i.e., dominated
trees, to search, add, and delete elements in archive quickly [71]. With the help of
dominated trees, their algorithm does not need to limit the size of the archive, which
promotes search speed and quality prominently.
The concept of the orthogonal GA has been introduced in the Suggested Read-
ings of Chap. 3. In 2004, Zeng et al. expanded the orthogonal GA into the Ô¨Åeld
of MOPs [72], which can be regarded as another example of expanding single-
objective algorithms into MOPs.
In 2005 Knowles and Corne gave a good introduction to combine memetic al-
gorithms with MOEAs, i.e., utilizing local search to promote PF* search and even
distribution [73], in 2009 Ishibuchi et al. implemented a biased neighborhood to as-
sign larger probabilities to more promising neighbors [74], and in 2009 Goh et al.
published a book on this topic [75].
Those readers interested in the estimation of distribution algorithms, introduced
in Sect. 3.5.2.6, will Ô¨Ånd its application to MOPs in a paper published in 2008 [76].
We have not discussed the issue of MOEAs in an uncertain environment, which
is extremely important in real-world applications because noise is everywhere. For
handling noise in MOEAs, readers are referred to [77] and [78], published in 2002
and 2007 by Buche et al. and Goh et al., respectively. For designing robust MOEAs,
readers might be interested in reading [79] by Deb and Gupta and [80] by Paenke et
al., published in 2006 respectively. In 2006 Knowles published a paper to estimate
the solution landscape for MOPs [81] and this paper won the IEEE Transactions
on Evolutionary Computation Outstanding Paper Award. For dynamic problems,
objective values change with time, and two papers published by Farina et al. and
Goh and Tan in 2004 and 2009, respectively, are suitable for reading [18, 82].
The research on archive maintenance, PI, constraint handling, uncertain environ-
ments, expanding other algorithms into MOPs, and applications has contributed to
the rapid increase in the number of papers indexed by SCI from 2003 until now.
For those readers interested in other discussions on PI, we suggest three papers
published in 2001, 2003, and 2008, respectively [83‚Äì85].
Using MOEAs to deal with COPs has attracted more attention recently. Apart
from Cai and Wang‚Äôs elaborate suggestion, which is introduced in Sect. 6.8.2.2,
Venkatraman and Yen also proposed a two-objective optimization approach for
COPs in 2005 [86]. They Ô¨Årst force the population to evolve toward feasible re-
gion using a linear-rank-based approach by only considering the constraint viola-

258
6 Multiobjective Optimization
tion. After at least one feasible solution has been found, the algorithm proceeds to
the second phase, simultaneously optimizing the objective function and constraint
violation. In 2007 and 2008, Wang et al. utilized deterministic crowding72 to main-
tain diversity and iteratively select half of the nondominated solutions according to
the constraint violation to push individuals toward a feasible domain respectively in
two-objective optimization approach for COPs [87, 88].
The must-read papers for MOEAs are [8] for NSGA-II, [10] for SPEA2, [23]
for elaborately transforming MOPs into two-objective problems and utilizing the
drawback of VEGA elaborately, [30] for test problem design and analysis, [41] for
PIs, [54] for constraint handling, and [33] or [29] for comparison.
Exercises and Potential Research Projects
6.1. Implement at least one classical MOEA introduced in Sect. 6.5 and use at least
three MOPs in Appendix to test its performance. Techniques introduced in Sect.
3.6.2 are required for drawing conclusions.
6.2. Adopt at least one improving technique introduced in Sect. 6.6 to your algo-
rithm and do a comparison between the original one and the improved one using
several PIs introduced in Sect. 6.7. Techniques introduced in Sect. 3.6.2 are required
for drawing conclusions.
6.3. Consider a way to improve Deb‚Äôs crowding distance for diversity evaluation
without any parameter and use the benchmark problems and PIs to verify your idea.
Techniques introduced in Sect. 3.6.2 are required for drawing conclusions.
6.4. What is the convergence, distribution, and elitism mechanism of PESA and
PAES?
6.5. Summarize the essence of [46] on a single sheet of paper to illustrate its new
operator.
6.6. Summarize the essences of [70] on a single sheet of paper to illustrate its new
ranking scheme.
6.7. Summarize the essences of [24] on a single sheet of paper to illustrate its new
dynamic popsize control method.
6.8. Is it possible that IŒµ (S1,S2) < 1 and IŒµ (S2,S1) < 1 in Œµ-dominance discussed in
Sect. 6.7.2.3? Give examples to support your statement.
6.9. Summarize and review the edge-points-keeping methods introduced in this
chapter.
72 DC is introduced in Sect. 5.4.1.

References
259
6.10. Compare the archive maintenance technique discussed in Sects. 6.6.2.1 and
6.6.2.2. If possible, implement them in your programming environment and use the
benchmark problems and PIs to do numerical comparison. Techniques introduced
in Sect. 3.6.2 are required for drawing conclusions.
6.11. What‚Äôs the difference between MOEA/D introduced in Sect. 6.6.3.2 and run
popsize times of single-objective EAs, each optimizing one of the popsize single-
objective aggregation functions generated by popsize weight vectors?
6.12. Implement at least one method of applying MOEAs to COPs introduced in
Sect. 6.8.2 and use the benchmark problems in Appendix to test its performance.
Techniques introduced in Sect. 3.6.2 are required for drawing conclusions.
References
1. Schaffer JD (1985) Multiple objective optimization with vector evaluated genetic algorithms.
In: Proceedings of the 1st international conference on genetic algorithms, pp 93‚Äì100
2. Goldberg DE (1989) Genetic algorithms in search, optimization, and machine learning.
Addison-Wesley, Boston, MA
3. Fonseca C, Fleming P (1993) Genetic algorithms for multiobjective optimization: Formula-
tion, discussion and generalization. In: Proceedings of the Ô¨Åfth international conference on
genetic algorithms, pp 416‚Äì423
4. Horn J, Nafpliotis N, Goldberg D (1994) A niched pareto genetic algorithm for multiobjective
optimization. In: Proceedings of the genetic and evolutionary computation conference, pp
82‚Äì87
5. Corne DW, Knowles JD, Oates MJ (2000) The pareto envelope-based selection algorithm
for multiobjective optimization. In: Proceedings of the international conference on parallel
problem solving from nature, pp 839‚Äì848
6. Knowles JD, Corne DW (2000) Approximating the nondominated front using the pareto
archived evolution strategy. Evol Comput 8:149‚Äì172
7. Osyczka A, Kundu S (1996) A modiÔ¨Åed distance method for multicriteria optimization, using
genetic algorithms. Comput Ind Eng 30(4):871‚Äì882
8. Deb K, Pratap A, Agarwal S et al (2002) A fast and elitist multiobjective genetic algorithm:
NSGA-II. IEEE Trans Evol Comput 6(2):182‚Äì197
9. Srinivas N, Deb K (1994) Multiobjective optimization using nondominated sorting in genetic
algorithms. Evol Comput 2:221‚Äì248
10. Zitzler E, Laumanns M, Thiele L (2001) SPEA2: improving the strength pareto evolutionary
algorithm. Tech. rep. 103, ETH Zurich, Switzerland
11. Corne DW, Jerram NR, Knowles JD et al (2001) PESA-II: region-based selection in evolution-
ary multiobjective optimization. In: Proceedings of the genetic and evolutionary computation
conference, pp 283‚Äì290
12. Goldberg DE (1989) Sizing populations for serial and parallel genetic algorithms. In: Pro-
ceedings of the third international conference on Genetic algorithms, pp 70‚Äì79
13. Coello Coello CA, Pulido GT (2001) A micro-genetic algorithm for multiobjective optimiza-
tion. In: Proceedings of the genetic and evolutionary computation conference, pp 126‚Äì140
14. Iorio AW, Li X (2006) Incorporating directional information within a differential evolution
algorithm for multi-objective optimization. In: Proceedings of the ACM annual conference on
Genetic and evolutionary computation, pp 691‚Äì698
15. Nebro AJ, Luna F, Alba E et al (2008) AbYSS: adapting scatter search to multiobjective
optimization. IEEE Trans Evol Comput 12(4):439‚Äì457

260
6 Multiobjective Optimization
16. Tan K, Yang Y, Goh C (2006) A distributed cooperative coevolutionary algorithm for multi-
objective optimization. IEEE Trans Evol Comput 10(5):527‚Äì549
17. Tan KC, Khor EF, Lee TH et al (2003) An evolutionary algorithm with advanced goal and
priority speciÔ¨Åcation for multi-objective optimization. J Artif Intell Res 18:183‚Äì215
18. Goh C, Tan KC (2009) A competitive-cooperative coevolutionary paradigm for dynamic mul-
tiobjective optimization. IEEE Trans Evol Comput 13(1):103‚Äì127
19. Laumanns M, Thiele L, Deb K et al (2002) Combining convergence and diversity in evolu-
tionary multiobjective optimization. Evol Comput 10(3):263‚Äì282
20. Knowles J, Corne D (2003) Properties of an adaptive archiving algorithm for storing nondom-
inated vectors. IEEE Trans Evol Comput 7(2):100‚Äì116
21. Deb K, Mohan M, Mishra S (2005) Evaluating the Œµ-domination based multi-objective evo-
lutionary algorithm for a quick computation of Pareto-optimal solutions.
Evol Comput
13(4):501‚Äì525
22. Toffolo A, Benini E (2003) Genetic diversity as an objective in multi-objective evolutionary
algorithms. Evol Comput 11(2):151‚Äì167
23. Lu H, Yen G (2003) Rank-density-based multiobjective genetic algorithm and benchmark test
function study. IEEE Trans Evol Comput 7(4):325‚Äì343
24. Yen G, Lu H (2003) Dynamic multiobjective evolutionary algorithm: adaptive cell-based rank
and density estimation. IEEE Trans Evol Comput 7(3):253‚Äì274
25. Miettinen K (1999) Nonlinear multiobjective optimization. Kluwer, Holland
26. Ishibuchi H, Murata T (1998) A multi-objective genetic local search algorithm and its appli-
cation to Ô¨Çowshop scheduling. IEEE Trans Syst Man Cybern C Appl Rev 28(3):392‚Äì403
27. Jaszkiewicz A (2002) Genetic local search for multi-objective combinatorial optimization. Eur
J Oper Res 137(1):50‚Äì71
28. Zhang Q, Li H (2007) MOEA/D: a multiobjective evolutionary algorithm based on decompo-
sition. IEEE Trans Evol Comput 11(6):712‚Äì731
29. Li H, Zhang Q (2009) Multiobjective optimization problems with complicated pareto sets,
MOEA/D and NSGA-II. IEEE Trans Evol Comput 13(2):284‚Äì302
30. Huband S, Hingston P, Barone L et al (2006) A review of multiobjective test problems and a
scalable test problem toolkit. IEEE Trans Evol Comput 10(5):477‚Äì506
31. Van Veldhuizen DA (1999) Multiobjective evolutionary algorithms: classiÔ¨Åcations, analyses,
and new Innovations. Ph.D. thesis, Air Force Institute of Technology, OH
32. Deb K (1999) Multi-objective genetic algorithms: Problem difÔ¨Åculties and construction of test
problems. Evol Comput 7:205‚Äì230
33. Zitzler E, Deb K, Thiele L (2000) Comparison of multiobjective evolutionary algorithms:
empirical results. Evol Comput 8(2):173‚Äì195
34. Deb K, Thiele L, Laumanns M et al (2001) Scalable test problems for evolutionary multi-
objective optimization. Tech. rep. 2001001, Kanpur Genetic Algorithms Laboratory. Indian
Institute of Technology
35. Okabe T, Jin Y, Olhofer M et al (2004) On test functions for evolutionary multi-objective
optimization. In: Proceedings of the international conference on parallel problem solving
from nature, pp 792‚Äì802
36. Huband S, Barone L, While L et al (2005) A scalable multi-objective test problem toolkit.
In: Coello Coello CA, Aguirre AH, Zitzler E (eds) Evolutionary multi-criterion optimization
Springer, Berlin Heidelberg New York, pp 280‚Äì295
37. Iorio AW, Li X (2006) Rotated test problems for assessing the performance of multi-objective
optimization algorithms. In: Proceedings of the ACM annual conference on genetic and evo-
lutionary computation, pp 683‚Äì690
38. Deb K, Sinha A, Kukkonen S (2006) Multi-objective test problems, linkages, and evolutionary
methodologies. In: Proceedings of the 8th ACM annual conference on genetic and evolution-
ary computation, pp 1141‚Äì1148
39. Huang VL, Qin AK, Deb K et al (2007) Problem deÔ¨Ånitions for performance assessment
on multi-objective optimization algorithms. Tech. rep., Nanyang Technological University,
Indian Institute of Technology, Swiss Federal Institute of Technology, University Dortmund,
The University of Western Australia, Singapore

References
261
40. Zhang Q, Zhou A, Zhaoy S et al (2008) Multiobjective optimization test instances for the
cec 2009 special session and competition. Tech. rep. CES-487, University of Essex, Nanyang
Technological University, Clemson University, Singapore
41. Okabe T (2003) A critical survey of performance indices for multi-objective optimization. In:
Proceedings of the IEEE congress on evolutionary computation, pp 878‚Äì885
42. Zitzler E, Thiele L, Laumanns M et al (2003) Performance assessment of multiobjective opti-
mizers: an analysis and review. IEEE Trans Evol Comput 7(2):117‚Äì132
43. Zitzler E (1999) Evolutionary algorithms for multiobjective optimization: methods and appli-
cations. Ph.D. thesis, Swiss Federal Institute of Technology (ETH) Zurich, Switzerland
44. While L, Hingston P, Barone L et al (2006) A faster algorithm for calculating hypervolume.
IEEE Trans Evol Comput 10(1):29‚Äì38
45. Bradstreet L, While L, Barone L (2008) A fast incremental hypervolume algorithm. IEEE
Trans Evol Comput 12(6):714‚Äì723
46. Chan T, Man K, Tang K et al (2008) A jumping gene paradigm for evolutionary multiobjective
optimization. IEEE Trans Evol Comput 12(2):143‚Äì159
47. Fonseca CM, Fleming PJ (1996) On the performance assessment and comparison of stochastic
multiobjective optimizers. In: Proceedings of the international conference on parallel problem
solving from nature, pp 584‚Äì593
48. Conover WJ (1999) Practical nonparametric statistics, 3rd edn. Wiley, New York
49. Schott J (1995) Fault tolerant design using single and multicriteria genetic algorithm optimiza-
tion. Master thesis, MIT, MA
50. Leung Y, Wang Y (2003) U-measure: a quality measure for multiobjective programming.
IEEE Trans Syst Man Cybern A 33(3):337‚Äì343
51. Deb K (2000) An efÔ¨Åcient constraint handling method for genetic algorithms. Comput Meth-
ods Appl Mech Eng 186:311‚Äì338
52. Fonseca C, Fleming P (1998) Multiobjective optimization and multiple constraint handling
with evolutionary algorithms. I: a uniÔ¨Åed formulation.
IEEE Trans Syst Man Cybern A
28(1):26‚Äì37
53. Mezura-Montes E, Coello Coello CA (2002) A numerical comparison of some multiobjective-
based techniques to handle constraints in genetic algorithms. Tech. rep. EVOCINV-03-2002,
Evolutionary Computation Group at CINVESTAV-IPN, Mexico
54. Mezura-montes E, Coello Coello CA (2006) A survey of constraint-handling techniques based
on evolutionary multiobjective optimization. In: Proceedings of the PPSN workshop on mul-
tiobjective problem solving from nature
55. Runarsson T, Yao X (2005) Search biases in constrained evolutionary optimization. IEEE
Trans Syst Man Cybern C 35(2):233‚Äì243
56. Angantyr A, Andersson J, Aidanpaa J (2003) Constrained optimization based on a multiob-
jective evolutionary algorithm. In: Proceedings of the IEEE congress on evolutionary compu-
tation, 1560‚Äì1567
57. Aguirre AH, Rionda SB, Coello Coello CA et al (2004) Handling constraints using multiob-
jective optimization concepts. Int J Numer Methods Eng 59(15):1989‚Äì2017
58. Cai Z, Wang Y (2006) A multiobjective optimization-based evolutionary algorithm for con-
strained optimization. IEEE Trans Evol Comput 10(6):658‚Äì675
59. Yuan Z, Xinjie Y (2008) Pareto competition based evolution strategy for two-objective opti-
mization design of SMES solenoids. IEEE Trans Appl Superconduct 18(2):1513‚Äì1516
60. Deb K, Kalyanmoy D (2001) Multi-objective optimization using evolutionary algorithms. Wi-
ley, New York
61. Coello Coello CA, Lamont GB, Van Veldhuizen DA (2007) Evolutionary algorithms for solv-
ing multi-objective problems, 2nd edn. Springer, Berlin Heidelberg New York
62. Knowles J, Corne D, Deb K (2008) Multiobjective problem solving from nature: from con-
cepts to applications. Springer, Berlin Heidelberg New York
63. Coello Coello CA, Lamont GB, Coello CA (2004) Applications of multi-objective evolution-
ary algorithms. World ScientiÔ¨Åc, Singapore
64. Tan KC, Khor EF, Lee TH (2005) Multiobjective evolutionary algorithms and applications.
Springer, Berlin Heidelberg New York

262
6 Multiobjective Optimization
65. Zitzler E, Thiele L (1998) Multiobjective optimization using evolutionary algorithms - a com-
parative case study. In: Proceedings of the international conference on parallel problem solv-
ing from nature, pp 292‚Äì304
66. Zitzler E, Thiele L (1999) Multiobjective evolutionary algorithms: a comparative case study
and the strength pareto approach. IEEE Trans Evol Comput 3(4):257‚Äì271
67. Jaszkiewicz A (2002) On the performance of multiple-objective genetic local search on the
0/1 knapsack problem - a comparative experiment. IEEE Trans Evol Comput 6(4):402‚Äì412
68. Khare V, Yao X, Deb K (2002) Performance scaling of multi-objective evolutionary algo-
rithms. Tech. rep. 2002009, KanGAL, Indian Institute of Technology Kanpur, India
69. Jensen M (2003) Reducing the run-time complexity of multiobjective EAs: the NSGA-II and
other algorithms. IEEE Trans Evol Comput 7(5):503‚Äì515
70. Pierro F, Khu S, Savi¬¥c D (2007) An investigation on preference order ranking scheme for
multiobjective evolutionary optimization. IEEE Trans Evol Comput 11(1):17‚Äì45
71. Fieldsend J, Everson R, Singh S (2003) Using unconstrained elite archives for multiobjective
optimization. IEEE Trans Evol Comput 7(3):305‚Äì323
72. Zeng SY, Kang LS, Ding LX (2004) An orthogonal multi-objective evolutionary algorithm for
multi-objective optimization problems with constraints. Evol Comput 12(1):77‚Äì98
73. Knowles J, Corne D (2005) Memetic algorithms for multiobjective optimization: issues, meth-
ods and prospects. In: Hart WE, Krasnogor N, Smith JE (eds) Recent advances in memetic
algorithms. Springer, Berlin Heidelberg New York, pp 313‚Äì352
74. Ishibuchi H, Hitotsuyanagi Y, Tsukamoto N et al (2009) Use of biased neighborhood struc-
tures in multiobjective memetic algorithms. Soft Comput 13(8):795‚Äì810
75. Goh C, Ong Y, Tan KC (2009) Multi-objective memetic algorithms. Springer, Berlin Heidel-
berg New York
76. Zhang Q, Zhou A, Jin Y (2008) RM-MEDA: a regularity model-based multiobjective estima-
tion of distribution algorithm. IEEE Trans Evol Comput 12(1):41‚Äì63
77. Buche D, Stoll P, Dornberger R et al (2002) Multiobjective evolutionary algorithm for the
optimization of noisy combustion processes. IEEE Trans Syst Man Cybern C 32(4):460‚Äì473
78. Goh C, Tan K (2007) An investigation on noisy environments in evolutionary multiobjective
optimization. IEEE Trans Evol Comput 11(3):354‚Äì381
79. Deb K, Gupta H (2006) Introducing robustness in multi-objective optimization. Evol Comput
14(4):463‚Äì494
80. Paenke I, Branke J, Jin Y (2006) EfÔ¨Åcient search for robust solutions by means of evolutionary
algorithms and Ô¨Åtness approximation. IEEE Trans Evol Comput 10(4):405‚Äì420
81. Knowles J (2006) ParEGO: a hybrid algorithm with on-line landscape approximation for ex-
pensive multiobjective optimization problems. IEEE Trans Evol Comput 10(1):50‚Äì66
82. Farina M, Deb K, Amato P (2004) Dynamic multiobjective optimization problems: test cases,
approximations, and applications. IEEE Trans Evol Comput 8(5):425‚Äì442
83. Wu J, Azarm S (2001) Metrics for quality assessment of a multiobjective design optimization
solution set. J Mech Design 123(1):18‚Äì25
84. Farhang-Mehr A, Azarm S (2003) An information-theoretic entropy metric for assessing
multi-objective optimization solution set quality. J Mech Design 125(4):655‚Äì663
85. Lizarraga-Lizarraga G, Hernandez-Aguirre A, Botello-Rionda S (2008) G-Metric: an m-ary
quality indicator for the evaluation of non-dominated sets. In: Proceedings of the 10th ACM
annual conference on genetic and evolutionary computation, pp 665‚Äì672
86. Venkatraman S, Yen G (2005) A generic framework for constrained optimization using genetic
algorithms. IEEE Trans Evol Comput 9(4):424‚Äì435
87. Wang Y, Cai Z, Guo G et al (2007) Multiobjective optimization and hybrid evolutionary algo-
rithm to solve constrained optimization problems. IEEE Trans Syst Man Cyberne B: Cybern
37(3):560‚Äì575
88. Wang Y, Cai Z, Zhou Y et al (2008) An adaptive tradeoff model for constrained evolutionary
optimization. IEEE Trans Evol Comput 12(1):80‚Äì92

Chapter 7
Combinatorial Optimization
Abstract Previous chapters discuss parameter optimization, i.e., we need to Ô¨Ånd the
optimal values of variables so that the objective function has the maximum/minimum
value. Many real-world problems are not like this. We often need to select some el-
ements from a set or arrange the sequence of some events with constraints so that
the objective function has the maximum/minimum value. These problems belong
to combinatorial optimization. We will introduce three examples, explain their re-
spective properties, illustrate how EAs solve them, and summarize design-effective
algorithms for them.
7.1 Introduction
7.1.1 Combinatorial Optimization
Combinatorial optimization is optimization derived from discrete mathematics,1 es-
pecially combinatorics. So we often face the concepts of enumeration, combination,
and permutation. Suppose we work on a set S whose size is ‚à£S‚à£= n.
An enumeration of set S is an exact listing of all its element.
A combination is an unordered collection of distinct elements, usually of a pre-
scribed size k and taken from S. The number of possible combinations in such a
situation is given by Eq. 7.1:
Ck
n =
(n
k
)
=
n!
k!(n‚àík)!
(7.1)
where n! denotes the factorial n. For example, the number of combinations of two
elements from set S = {w,x,y,z} is C2
4 =
(4
2
)
= 6, which is exactly {w,x}, {w,y},
{w,z}, {x,y}, {x,z}, {y,z}.
1 Discrete mathematics is the branch of mathematics dealing with objects that only have distinct,
separate values, often characterized by integers.
263

264
7 Combinatorial Optimization
A permutation is a sequence containing k distinct elements from S. The number
of possible permutations in such a situation is given by Eq: 7.2.
Pk
n =
n!
(n‚àík)!
(7.2)
The number of permutations of two elements from set S = {w,x,y,z} is P2
4 = 12,
which is exactly {w ‚Üíx}, {x ‚Üíw}, {w ‚Üíy}, {y ‚Üíw}, {w ‚Üíz}, {z ‚Üíw}, {x ‚Üíy},
{y ‚Üíx}, {x ‚Üíz}, {z ‚Üíx}, {y ‚Üíz}, {z ‚Üíy}.
An optimization problem related to the optimal combination of the elements is
called a grouping problem [1]. Bin packing is an example of this. Suppose we have
n items. Item i has capacity ci. We also have a lot of bins with the same size B.
The problem is to determine the minimum number of bins so that every item can be
packed into one bin and the sum of capacities of the items in every bin is no more
than B. In the perspective of a grouping problem, what we need to do is a proper
grouping of n items that is subject to a size constraint and with a minimum number
of groups.
Optimization related to the optimal permutation of the elements is called a
scheduling problem [2]. Flow-shop scheduling is an example. Suppose we have n
jobs. Each job has to be processed on each of the m machines, i.e., from machine
1 to machine m in the sequence order. The operation time of job i on machine k is
pik. The problem is to determine the minimum total processing time by adjusting
the sequence of jobs. In the perspective of a scheduling problem, what we need to
do is to schedule n jobs so as to make the minimum total processing time.2
For each combinatorial optimization problem, there might be several ways to
represent its solutions. The straightforward expression of the solution of bin pack-
ing is n integers, and the deÔ¨Ånition domain of each integer is [1,k], where k is the
maximum allowed number of bins. It can also be expressed as a permutation of n in-
tegers. The permutation 3 ‚Üí5 ‚Üí2 ‚Üí1 ‚Üí4 means that we put items in this order
into one bin until its size has been exceeded, then put them into another bin, etc.
If the variables of the optimization are restricted to integers, we often call this
problem integer programming. Further, if the deÔ¨Ånition domain of the variables is
only {0,1}, it is a 0/1 programming.
Perhaps the most distinctive part of combinatorial optimization compared with
continuous optimization, introduced in Chaps. 4‚Äì6, lies in the absence of the search
direction due to the lack of a native neighborhood deÔ¨Ånition, which makes the solu-
tion landscape hard to search. In continuous optimization, we can deÔ¨Åne a threshold
Œµ > 0 and assign all the points within the range (x‚àíŒµ,x+Œµ) to the neighbors of x.
Then many types of random sampling techniques can be used to search the neigh-
bor. But in combinatorial optimization, the neighborhood deÔ¨Ånition is representa-
tion dependent, which will be illustrated more speciÔ¨Åcally with the problems in the
following part of this chapter.
2 For the sake of simplicity, here we use the First In First Out rule implicitly when there is more
than one job waiting to be processed on one machine. So we just discuss permutation Ô¨Çow-shop
scheduling. Interested readers are referred to [2].

7.1 Introduction
265
Equation 7.3 is an example of integer programming, which is illustrated by
Fig. 7.1.
max z = 20x1 +10x2
s.t. 5x1 +4x2 ‚â§24
2x1 +5x2 ‚â§13
x1, x2 ‚â•0, x1, x2 ‚ààN
(7.3)
Fig. 7.1 An example of inte-
ger programming



























4
4


4



In Fig. 7.1, two dotted lines and the axes limit the 12 feasible solutions, rep-
resented by the cross in the Ô¨Ågure, in which point (4,1) is the optimal solution,
represented by the square in the Ô¨Ågure.
Equation 7.3 is an integer linear programming. If we could ‚Äúrelax‚Äù the constraint
of integer, the problem would be so easy that we could solve it on the 2-D plane
introduced as follows. The solid line and the arrow in Fig. 7.1 represent the line h =
20x1 +10x2, where h is a constant and the increasing direction of h. The intersection
of the line and the feasible region is the optimal solution of the relaxed 2-D linear
programming, i.e., (4.8,0), which is represented by the circle in the Ô¨Ågure. But we
cannot get the optimal solution by simply rounding up or rounding down, which
generates infeasible solution (5,0) and nonoptimal solution (4,0), respectively. So
the idea of ‚Äúrelax‚Äù and ‚Äúround up/down‚Äù does not work.3
Another intuitive idea is to enumerate all the feasible solutions and Ô¨Ånd the op-
timum because there will always be a limited number of feasible solutions. Yes, it
is simple in Fig. 7.1, but let us consider Ô¨Çow-shop scheduling. The number of solu-
tions for n jobs is n!. If n = 50, which is not a large problem, n! ‚âà1064. Currently
the fastest supercomputer, IBM‚Äôs Sequoia, can calculate approx. 1016 times per sec-
ond. It will take Sequoia about 1048 s to Ô¨Ånish the enumeration, which is about 1040
years!!
3 But the result of the relaxed problem can help the search. See [3] for details.

266
7 Combinatorial Optimization
So ‚Äúcrude‚Äù enumeration does not work even for medium-sized combinatorial
optimization problems. We need extra insight to limit the search space. Before dis-
cussing the solving algorithms, we need to discuss something about the difÔ¨Åculty
extent of problems.
7.1.2 NP-complete and NP-hard Problems
In combinatorial optimization, we often need to solve the problem instance with
dimension n using some algorithm. It is quite natural to estimate the time of the
algorithm to solve the instance. This time is related to both n and the speciÔ¨Åc algo-
rithm. For Ô¨Çow-shop scheduling with n jobs, if we use permutation to represent a
solution and enumerate every permutation to Ô¨Ånd the optimal solution, this ‚Äúcrude‚Äù
algorithm will take n! steps to solve it.4 In order to compare the time complexities
of different algorithms on the same problem instance, we often use O(n!), where O
stands for ‚Äúorder,‚Äù meaning that the time complexity of the algorithm is on the order
of n!.
Another question related to the efÔ¨Åciency and efÔ¨Åcacy of the algorithm is how
hard the problem itself is, which leads to the concept of NP-complete and NP-hard.5
We need to Ô¨Årst transform the optimization problem into a decision problem. If
it can be solved by a deterministic Turing machine using an algorithm with poly-
nomial time complexity, i.e., O(nk), this problem belongs to the class of P, where
P stands for ‚Äúpolynomial time complexity‚Äù.6 Problems in class P have algorithms
with polynomial time complexity.
If the decision problem of a problem can be solved by a nondeterministic Turing
machine using an algorithm with polynomial time complexity, this problem belongs
to the class of NP, where NP stands for ‚Äúnondeterministic polynomial time com-
plexity.‚Äù It has been proved that P ‚äÜNP, but most computer scientists believe that
P ‚àï= NP, even though there is no prove of that. Then we need to classify further for
those problems belong to NP‚àíP.
If we can design an algorithm with polynomial time complexity to transform any
solution of the decision problem D1 of one problem to one solution of the decision
problem D2 of another problem, we denote D1 ‚àùD2 with the meaning that D2 is at
least as hard as D1. This polynomial transformation is sometime called reduction.
If decision problem C belongs to NP and there exist polynomial algorithms to
transform every problem in NP into C, we say C is NP-complete or C is a NP-
complete problem.7 If a known NP-complete problem can be reduced to an NP prob-
lem C, we could also say that C is NP-complete because of the transitive property
of polynomial transformation. That is to say, if one of the NP-complete problems
4 We often use the key step number to represent the time requirement of an algorithm.
5 It is the key part of computational complexity theory.
6 We do not have content to explain the detail of the above statement. Interested readers are referred
to [4].
7 It means that C is no easier than any other NP problem.

7.1 Introduction
267
can be solved by a polynomial algorithm, so can all of them. But up to now, any
polynomial algorithm for any NP-complete problem has yet to be suggested. That
is why many researchers think P ‚àï= NP.
If a problem H does not belong to NP but there exists an NP-complete problem
C that can be polynomially transformed into H, we say H belongs to the class of
NP-hard or H is a NP-hard problem with the meaning that H is at least as hard as
C. The relationship between these classes is illustrated by Fig. 7.2.
Fig. 7.2 The relationship
between P, NP, NP-complete,
and NP-hard































4.C	
4.C

.
4.
Problems discussed in this chapter and most of the practical combinatorial op-
timization problems are all NP-complete or NP-hard [4],8 which means that they
might be the hardest problems. Right now, no algorithm with polynomial time com-
plexity can guarantee that an optimal solution will be found.
7.1.3 Evolutionary Algorithms for Combinatorial Optimization
Algorithms for combinatorial optimization can be classiÔ¨Åed into the following three
categories. The chart of Fig. 7.3 illustrates these statements in which n is the dimen-
sion of the problem.
1. Exact algorithms. These algorithms guarantee the optimality of the results. But
they do not have polynomial time complexity for NP-complete problems. So they
suffer from the ‚Äúcurse of dimension‚Äù or the ‚Äúdimension explosion.‚Äù This category
contains dynamic programming, branch and bound, cutting plane method, etc.
2. Heuristic algorithms. These algorithms can either generate a not-so-bad solu-
tion fast, i.e., construction algorithm,9 or improve the current solution fast, i.e.,
local search algorithm.10 The quality of solutions provided by heuristic algo-
rithms is relatively low in the worst case. Their time complexity is low. Heuristic
algorithms are problem-dependent. So we will introduce several of them in the
following sections.
8 Crescenzi and Kann maintain a Web site containing a compendium of the NP-complete optimiza-
tion problems http://www.csc.kth.se/Àúviggo/problemlist/.
9 Sometimes called the construction method or generation method.
10 Sometimes called the local search method.

268
7 Combinatorial Optimization
3. EAs. These algorithms will achieve a near-optimal solution, especially after be-
ing combined with heuristic algorithms, in a relatively short time because their
time complexity is not directly linked with the dimension of the problem.






!#&;!#
6
	

+	





















(a)





#)!#
6
	

+	






(b)
Fig. 7.3 Solution quality and time complexity of different algorithms: (a) Solution quality, and (b)
Time complexity
As can be seen from Fig. 7.3a, exact algorithms have a constant and the highest
solution quality, followed by EAs and then heuristic algorithms. The solution quality
of EAs will decrease if we keep the control parameter, i.e., popsize, maxgen, etc.,
unchanged. Heuristic algorithms almost keep the same solution quality. In Fig. 7.3b,
the increased speed of exact algorithms is apparently faster than that of the others,
which makes the exact algorithms unusable for large (or even intermediate-size)
combinatorial optimization.
The following points need to be focused on when designing and analyzing an EA
for combinatorial optimization.

7.1 Introduction
269
‚àôThe decoding and repair mechanism of the EA. Because combinatorial optimiza-
tion problems are often highly constrained, special decoding techniques and re-
pair methods are the most common strategies. We will introduce permutation
code, integer code, random key code, and their corresponding decoding mecha-
nisms in the follow sections.
‚àôSpecial variation operators to generate decodable chromosomes. For permutation
code, ordinary crossover and mutation operators might generate illegal chromo-
somes. We will discuss this in detail in Sect. 7.3.
‚àôThe way of selecting and utilizing local search methods. The search space of
combinatorial optimization problems are rather huge, so we need memetic algo-
rithms to get high-quality solutions within a limited time. Different local search
methods will be introduced in the following sections and their combination with
EAs will be discussed.
The above properties separate the main topic of this chapter from that of Chaps.
4‚Äì6, in which the methods to maintain the proper population diversity in a selection
or replacement process are the main concern.
Figure 7.4 illustrates the number of papers indexed by the SCI on EAs-based
combinatorial optimization,11 which means this Ô¨Åeld is rather hot.


	





		
	


	






	







 




	



 



Fig. 7.4 Number of papers indexed by SCI on EAs-based combinatorial optimization
11 TS = ((‚Äúcombinatorial optimization‚Äù) AND (‚Äúgenetic algorithm‚Äù OR ‚Äúgenetic algorithms‚Äù
OR ‚Äúevolutionary computation‚Äù OR ‚Äúevolutionary computing‚Äù OR ‚Äúevolutionary algorithms‚Äù OR
‚Äúevolutionary intelligence‚Äù)). The SCI index ‚ÄúTS‚Äù is for the search topic in the title, the keywords,
and the abstract.

270
7 Combinatorial Optimization
7.2 Knapsack Problem
7.2.1 Problem Description
The knapsack problem considers a robber containing a knapsack with limited ca-
pacity in a department store with n items. The robber wants to maximize the proÔ¨Åt
while subject to the constraint of the knapsack‚Äôs capacity as follows:
f(x) = max
n
‚àë
i=1
pixi
(7.4)
s.t.
n
‚àë
i=1
wixi ‚â§W
xi ‚àà{0,1}, i = 1,2,‚ãÖ‚ãÖ‚ãÖ,n
(7.5)
where pi and wi are the proÔ¨Åt and weight of item i, respectively, and W is the capac-
ity of the knapsack. xi = 1 means item i is put into the knapsack and xi = 0 means
item i is not put into the knapsack.
The knapsack problem has diverse practical applications such as cargo loading,
project selection, assembly line balancing, etc.
The knapsack problem can be regarded as grouping items into two classes, those
being put into the knapsack and those being discarded. So it is a grouping problem.
There are other variations of the standard knapsack problem.
If there is more than one item i in a department store, i.e., xi ‚àà{0,1,‚ãÖ‚ãÖ‚ãÖ,bi},
where bi is the maximum number of item i, it is called a bounded knapsack problem.
If we consider the constraint of the knapsack capacity in more detail, i.e., k con-
straints such as the length, width, height, etc., and give the corresponding properties
for each item, i.e., (wi1,‚ãÖ‚ãÖ‚ãÖ,wik) for item i with k-dimensional properties, the con-
straint, Eq. 7.5, can be rewritten as follows:
s.t.
n
‚àë
i=1
wijxi ‚â§Wj
j = 1,‚ãÖ‚ãÖ‚ãÖ,k
xi ‚àà{0,1}, i = 1,2,‚ãÖ‚ãÖ‚ãÖ,n
(7.6)
The explanation of Eq. 7.6 is that the robber cannot take items too long, too wide,
or too high. We can also interpret Eq. 7.6 as a k robbers problem. Each robber has
one knapsack. These k knapsacks have different capacities Wj and there is more
than one item i in the department store. Whenever the robbers decide to take item i,
it is put into k knapsacks. They want to maximum their proÔ¨Åts with the constraints
of each knapsack. Combining Eqs. 7.4 and 7.6 forms a multidimensional knapsack
problem [3, 5].
If we consider the proÔ¨Åt of an item in more detail, i.e., m proÔ¨Åts such as price,
satisfaction level, commemorative meaning, etc., and give corresponding properties
for each item, i.e., (pi1,‚ãÖ‚ãÖ‚ãÖ, pim) for item i with m proÔ¨Åt properties, the objective,
Eq. 7.4 can be rewritten as follows:

7.2 Knapsack Problem
271
f j(x) = max
n
‚àë
i=1
pijxi
j = 1,‚ãÖ‚ãÖ‚ãÖ,m
(7.7)
The explanation of Eq. 7.7 is that the robbers not only want the money but also
expect some spiritual harvests. Combining Eqs. 7.7 and 7.5 forms a multiobjective
knapsack problem.
In 1999 Zitzler and Thiele use Eqs. 7.6 and 7.7 as a benchmark problem for
testing MOEAs and many researchers have worked on it since then [6‚Äì8].
In the following section, we will only discuss a model of Eqs. 7.4 and 7.5.
7.2.2 Evolutionary Algorithms for Knapsack Problem
IN 2005, Raidl and Gottlieb studied six representations of the multidimensional
knapsack problem with different variation operators [3]. They suggested that a good
EA for combinatorial optimization should have the following characteristics.
‚àôLocality. Small steps in the operation domain corresponds to small steps in the
deÔ¨Ånition domain, which promotes a meaningful local search around a solution.
‚àôHeritability. The offspring generated by a crossover operator should contain
meaningful features of their parents, which means that the merits of the previ-
ous search could be maintained.
‚àôHeuristic bias. The map from the operation domain to the deÔ¨Ånition domain is
better when it contains some heuristic which promote the intensive search of high
Ô¨Åtness value region.12
Before we discuss the genetic code for the knapsack problem, an intuitive heuris-
tic method for constructing solutions and repairing infeasible solutions needs to be
introduced. It can be regarded as both a construction method and a local search
method.
Let us begin the discussion with an example of Eqs. 7.4 and 7.5, in which W = 9
and n = 7. Other parameters are listed in Table 7.1.
Table 7.1 Example data for knapsack problem


	






"



#
&

"

(



"

&

<(

0"
0
0&
0
)0"#
)0"

12 Reconsider the effect of different r0 in Sect. 4.2.2.

272
7 Combinatorial Optimization
It is natural for a ‚Äúreasonable‚Äù robber to pick an item with a large proÔ¨Åt and
small weight. So we can use the proÔ¨Åt/weight ratio pi/wi for item i to represent this
preference, as represented in Table 7.1 [9].
While constructing a new solution, we just need to pick items in a ‚Äúgreedy‚Äù way,
i.e., pick the one with the highest value of pi/wi, then the second, ..., until the
knapsack capacity is not enough. In this way, we can generate the solution {1,2,7}
with a proÔ¨Åt of 14.13
While repairing the infeasible solutions, we can take the one with the smallest
value of pi/wi in the knapsack Ô¨Årst, then the second, ..., until the solution is a
feasible one.
We will introduce three code schemes in this section as follows:
‚àôBinary code
‚àôVariable-length code
‚àôPermutation code
Binary Code
The binary code for the knapsack problem is very straightforward; just use a locus
to represent the item and an allele to represent whether the item is selected (1) or
not (0). An example chromosome of the seven-item knapsack problem is illustrated
by Fig. 7.5.
Fig. 7.5 Binary code for
knapsack problem






)  )  )  )
      "
The upper numbers in Fig. 7.5 represent the order number of the items and the
lower numbers represent the selection of the items. But this code scheme might
generate infeasible solutions. We will introduce two methods for it.
It is not difÔ¨Åcult to understand that the optimal solution of the knapsack problem
must be at or near the boundary of the feasible region, i.e., the sum of weights of
the items in the knapsack should be the same as or close to the knapsack capacity.
Olsen suggested a way to penalize the solution over and under the capacity of the
knapsack [10].
Olsen Ô¨Årst deÔ¨Ånes the scale of the penalization as follows:
Œ¥ = min
{
W,
					
n
‚àë
i=1
wi ‚àíW
					
}
(7.8)
According to Eq. 7.8, the maximum value of Œ¥ is W. Then the penalty function is
deÔ¨Åned as follows:
13 But the optimal solution is {1,4} with a proÔ¨Åt of 15.

7.2 Knapsack Problem
273
p(x) = 1‚àí
				
n
‚àë
i=1
wixi ‚àíW
				
Œ¥
(7.9)
The graphical illustration of the above penalty function is Fig. 7.6. This penalty
function penalizes not only the infeasible solutions but also the feasible solutions
that are not at the boundary of the feasible region.14
Fig. 7.6 Penalty function for
knapsack problem

















































( 

 
 

)
)

 
 *
 
)
#
*
 
5 	
' 	
The Ô¨Åtness function is the time of the objective function and the penalty function
as follows:
fitness(x) = f (x) p(x)
(7.10)
Repair is another way to handle infeasible individuals. It is basic idea, using the
proÔ¨Åt/weight ratio, was introduced previously. Here we just discuss an example with
W = 90 and n = 7. Other parameters are listed in Table 7.2.
Table 7.2 Example of repair in knapsack problem



	






"

)
)
)
)

)
)
(
)
)
)
)
)
)
)
<(

0
)0

)0
)0



Individuals represented by Fig. 7.5 are infeasible. The sum of weights of items 2,
4, and 6 is 100, which is larger than W = 90. According to the proÔ¨Åt/weight ratios
listed in Table 7.2, we can delete item 6, which has the smallest proÔ¨Åt/weight ratio
in the three selected items and thus makes (0101000) a feasible solution.
Repair is often used to change infeasible solutions to feasible solutions. The
method to deal with solutions out of the deÔ¨Ånition domain, introduced at the end
of Sect. 4.2, is also a repair method. Repair might limit the population in a small
solution area, i.e., it has bias, which needs to be avoided by designers.
14 For solutions rather far from W, we can just assign its Ô¨Åtness value as zero.

274
7 Combinatorial Optimization
Variable-length Code
Hinterding suggested variable-length code for the knapsack problem [11]. The
length of the chromosome could change during the evolving process. The gene lo-
cus has no meaning and the allele of the gene means the order number of the item.
That is (1,6,5) means items 1, 5, and 6 are in the knapsack.
Variable-length code has special methods in maintaining the feasibility of indi-
viduals. In initialization, we can generate random n number permutations. We check
from the Ô¨Årst item in the permutation whether the constraint will be violated if it is
selected. If adding the current item does not violate the knapsack capacity, it is se-
lected. If not, we discard it and check the next item until the knapsack is full or
we reach the end of the permutation. One feasible individual can be generated by
one random permutation. Repeat this method until popsize feasible individuals are
generated.
In crossover, the following steps are necessary to keep the feasibility of the off-
spring, which is illustrated as an example in Fig. 7.7.15 The capacity of the knapsack
is W = 90 and n = 7. Other parameters are listed in Table 7.2.
1. Step 1: Select a random point in parent 1.
2. Step 2: Select a random fragment in parent 2.
3. Step 3: Insert the fragment of parent 2 into the point of parent 1 and delete the
duplicate items to get an offspring, which might be infeasible.
4. Step 4: Use the proÔ¨Åt/weight ratio to repair the offspring if it is infeasible.





















"






"




"



"
.
	
.
	
/''
%
Fig. 7.7 Example of crossover for variable-length code
In the example illustrated by Fig. 7.7, the result after step 3 is (2,5,7,6), which
is infeasible. According to the proÔ¨Åt/weight ratios in Table 7.2, we Ô¨Årst try to delete
item 5 but it does not work because (2,7,6) is still infeasible. So we try item 6, thus
making (2,5,7) feasible.
The mutation operator is simple. Randomly delete an item from the chromosome
and randomly add an item, that does not exist in the chromosome. Then use step 4
introduced above to repair the mutant, if necessary, to be a feasible individual.
15 It is a two-parents-one-offspring crossover operator.

7.2 Knapsack Problem
275
Permutation Code
Any permutation of n numbers could be interpreted as the sequence of picking items.
Let us revisit the example illustrated by Table 7.2. Here we assume that W = 100
and n = 7.
For individual (1,6,4,7,3,2,5), the decoding procedure is as follows.
We put item 1 into the knapsack Ô¨Årst. Item 6 is OK because 40 + 40 = 80 <
100 = W. Item 4 is picked because 40+40+10 = 90 < 100 = W. But item 7 is not
acceptable. We keep on searching until we meet item 5, which means that 40+40+
10+10 = 100 = W. So the decoding results for individual (1,6,4,7,3,2,5) is item
1, 6, 4, and 5.
In this way, every n permutation can be decoded as a feasible solution.
Ordinary crossover and mutation operators will destroy the permutation, so spe-
cial variation operators are the main consideration in permutation code, which will
be discussed in detail in Sect. 7.3.
Another thing we need to mention is that individuals (1,6,4,7,3,2,5), (1,4,6,7,3,2,
5), (6,4,1,7,3,2,5), ..., (6,4,1,7,3,5,2) will all be decoded as the same feasible solu-
tion with items 1, 6, 4, and 5, which means that the map between the operation do-
main and the deÔ¨Ånition domain is many-to-one. The many-to-one map might waste
the search ability of EAs and cause low efÔ¨Åciency.
Finally, let us compare the search space of the binary code and the permutation
code with examples. In the binary code of n items, the search space contains 2n
solutions. In the permutation code of n items, the search space contains n! solutions.
The numerical comparison of n from 1 to 10, which are rather small numbers, is
illustrated by Table 7.3
Table 7.3 Comparison of the search space of binary code and permutation code









"
#
&
)



#



#


)
M




)
")
))
))
##)
##))



It is quite obvious that the search space of the permutation code is far more than
that of the binary code.
But sometimes we need to make a tradeoff between feasibility preservation and
limiting the search scale. Combinatorial optimization problems might have hard
constraints so that the ordinary techniques introduced in Chap. 4 do not work; we
will introduce such examples in the following sections. But if we can design the
decoding process so that every permutation corresponds to a feasible solution, we
might take this code scheme even if it is a many-to-one map.

276
7 Combinatorial Optimization
7.3 Traveling Salesman Problem
7.3.1 Problem Description
The traveling salesman problem (TSP) is perhaps the most famous combinatorial
optimization problem and is used as a benchmark for many optimization methods.
Suppose there is a salesman who would like to start from his home city and visit
other cities for commodity promotion. He would like to visit all other cities once
and return to his home city. The problem is how to arrange the visiting sequence of
these cities so that the total distance/traveling expenses are minimized.
TSP has diverse practical applications such as the vehicle routing problem, very-
large-scale integration (VLSI) design, printed circuit board design, etc.
Is such a clear and seemingly easy problem worth so much attention? Yes! We
will look at why this is so.
By the language of graph theory, we can formulate TSP in a more formal way.
A graph G = (V,E) contains a set of vertex V and a set of edge E. An edge e ‚ààE
links two vertices v1,v2 ‚ààV and can be represented as e = (v1,v2). If the edge
e = (v1,v2) has no information of direction, G is an undirected graph. Otherwise,
i.e., e1 = (v1,v2) ‚àï= (v2,v1) = e2, G is a directed graph. The cardinality of V, i.e.,
‚à£V‚à£, is the order of graph G and the cardinality of E, i.e., ‚à£E‚à£, is the size of graph G.
The number of edges connected to a vertex is the degree of that vertex.
If two edges e = (v1,v2) and e
‚Ä≤ = (v1,v2) share the same vertices, i.e., there are
two edges linking vertices v1 and v2, we say e and e
‚Ä≤ are parallel. Graphs without
parallel edges are called simple graphs. A complete graph is a simple graph in which
every pair of distinct vertices is connected by an edge. Figure 7.8 is an example of
an undirected simple graph whose order and size are both 4.
Fig. 7.8 An example of graph
































For a graph G
‚Ä≤ = (V
‚Ä≤,E
‚Ä≤), if V
‚Ä≤ ‚äÜV and E
‚Ä≤ ‚äÜE, we say that G
‚Ä≤ is a subgraph of
G. ({v1,v3,v4},{e3,e4}) is a subgraph of the graph illustrated by Fig. 7.8.
A path in graph G is a subgraph of G with a start vertex and an end vertex. We
can ‚Äúwalk‚Äù along the edges linking the vertices16 from the start vertex to the end
vertex. If there exists at least a path for every two vertices in graph G, we call G a
connected graph. Otherwise, it is a disconnected graph. The example in Fig. 7.8 is
a connected graph.
16 These edges and vertices are all elements of the subgraph path.

7.3 Traveling Salesman Problem
277
A cycle or loop is a path such that the start vertex and the end vertex are the same.
A special case of a cycle is e = (v,v), i.e., there exist an edge e that starts at vertex
v and ends at vertex v. It is called a self-cycle. We generally do not consider graphs
with self-cycles.
A tree is a connected subgraph of G containing all the vertices and without any
cycle.
A path with no repeated vertices is called a simple path and a cycle with no
repeated vertices besides the start/end vertex is a simple cycle.
A Hamiltonian path is a simple path in an undirected graph that visits each vertex
exactly once. A Hamiltonian cycle is a simple cycle in an undirected graph that visits
each vertex exactly once and also returns to the start vertex.
If we regard the vertices as cities and let the edges represent the distance informa-
tion between cities, then TSP could be interpreted as needing to Ô¨Ånd a Hamiltonian
cycle in an undirected simple complete graph with the minimum distance.17 Such
an example is given by Fig. 7.9, in which thick lines form a solution. In TSP, we
often use city instead of convex, road instead of edge, and tour instead of cycle.
Fig. 7.9 An example of TSP



























TSP can be regarded as grouping all edges into two classes, those in the solution
and other remaining edges, with the constraint that those edges in the solution should
form a Hamiltonian cycle. So it is a grouping problem.
The above problem is a symmetric TSP because we suppose the distance from
vi to vj is the same as that of vj to vi, i.e., the graph is undirected. If the graph is
directed, we could represent some other considerations, such as the the road from
city i to city j is uphill so the traveling expenses are different. In this situation,
the problem is an asymmetric TSP. We could use distance matrix D with n √ó n
nonnegative elements dij to represent the distance information of TSP with n cities.
It easy to see that the distance matrices of symmetric TSP and asymmetric TSP are
symmetric and asymmetric, respectively.
If cities of TSP are in a metric space, i.e., the elements of the distance matrix
satisfy Eq. 7.11, we call this problem metric TSP. Otherwise, it is nonmetric TSP.
Further, if the elements of distance matrix dij is the Euclidean distance between city
i and city j, it is a Euclidean TSP. Furthermore, if the cities are in a 2-D Euclidean
space, it is called a 2-D Euclidean TSP.
17 If the graph for a real TSP is not complete, we can add edges with a very large distance to make
it complete.

278
7 Combinatorial Optimization
dij ‚â§dik +dk j
(7.11)
There is a TSPLIB that contains a library of sample instances for the TSPs and
their current best solutions maintained by Reinelt.18 The largest city number in
TSPLIB is 85,900. This problem came from a VLSI application that arose in Bell
Laboratories in the late 1980s and was solved by Applegate et al.19 Right now, they
are working on a TSP with 100,000 cities.
For reasons of simplicity, we will consider a symmetric Euclidean TSP in what
follows unless otherwise speciÔ¨Åed, but most of the techniques could be used in
asymmetric TSP.20
7.3.2 Heuristic Methods for Traveling Salesman Problem
As has been introduced above, construction methods are used for generating so-
lutions from scratch that can be used to generate initial individuals with relatively
higher quality, and local search methods are for improving current solutions that can
be used in MAs.
7.3.2.1 Construction Methods
Closest Neighbor Method
We can construct a solution for TSP by considering the closest city of the current
city sequentially. The speciÔ¨Åc steps are illustrated as follows.
Closest Neighbor Construction Method for Traveling Salesman Problem
Step 1: Start from any city and take it as the current city.
Step 2: Select the closest city to the current city that has not been visited.
Go there and make that city the current city.
Step 3: Repeat step 2 until all cities have been visited. Then return to the
Ô¨Årst city.
Suppose we start from city 1 of Fig. 7.9. The solution we get is illustrated by Fig.
7.10, in which the arrows represent the visiting sequence.
18 http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/.
19 http://www.tsp.gatech.edu/pla85900/index.html.
20 There are other representation methods for TSP, such as 0/1 programming. Interested readers
are referred to sects. 8‚Äì10 of [12].

7.3 Traveling Salesman Problem
279
The closest neighbor method is a deterministic construction. But we could gen-
erate different solutions by picking different initial cities.
Fig. 7.10 The result of the
closest neighbor construction
method for TSP






















	



Closest Insertion Method
Another way to construct a solution for TSP is to absorb a new city into the current
cycle each time with a minimum distance increase until we get the Hamiltonian
cycle for the problem. The speciÔ¨Åc steps are illustrated as follows.
Closest Insertion Construction Method for Traveling Salesman Problem
Step 1: Use the closest neighbor construction method, or other methods, to
construct a cycle of three cities. Cycle (6,5,1,6) is constructed in this way in
Fig. 7.11.
Step 2: Select the closest city, which has not been visited, to the current
cycle. Suppose there are m cities in the current cycle. For each of the other
cities, we calculate its distance to these m cities and pick the smallest value to
represent its distance to the current cycle. Then the city closest to the current
cycle is selected. In Fig. 7.11, city 4 is selected because it has the smallest
distance to the current cycle.
Step 3: Find the best way to absorb the selected city into the cycle with
the smallest distance increase. Suppose there are m edges in the current cycle;
try to break each edge and absorb the selected city into the cycle. There are
m ways to absorb it. We compare the distance of these m new cycles and Ô¨Ånd
the one with the smallest distance to be the current cycle. The distances of
cycles (6,5,4,1,6), (6,4,5,1,6), and (6,5,1,4,6) are evaluated to determine
the current cycle in Fig. 7.11.
Step 4: Repeat steps 2 and 3 until all the cities have been included in the
cycle.

280
7 Combinatorial Optimization
Fig. 7.11 The procedure of
the closest insertion construc-
tion method for TSP




















	



7.3.2.2 Local Search Methods
2-opt
2-opt was suggested by Croes in 1958 [13]. It eliminates two edges that do not share
the cities from the current solution and add edges to form a new solution. We may
try every two such edges and change the current solution to the best improved one
if it exists. So 2-opt is the abbreviation of ‚Äúlocal optimal by 2-edge perturbation.‚Äù
For the example given by Fig. 7.12a, edge ei between vertices vw and vx and edge
e j between vertices vy and vz are in the solution. The dashed lines are other edges
of the solution. After removing ei and e j, there is only one way to construct a new
solution, i.e., add edge ek between vertices vw and vz and edge el between vertices
vx and vy.21 That forms the solution represented by Fig. 7.12b.


(




(a)

























+

(




(b)
Fig. 7.12 An example of one step in 2-opt: (a) solution before edge perturbation, and (b) solution
after edge perturbation
21 Adding the edge between vertiecs vw and vy and the edge between vertices vx and vz will form
local cycles and thus generate illegal solutions.

7.3 Traveling Salesman Problem
281
3-opt and Lin‚ÄìKernighan Algorithm
3-opt was suggested by Lin for symmetrical TSPs in 1965 [14]. Similar to 2-opt,
3-opt tries to remove three edges that do not share cities and reconnect them as a
new solution. With one set of three edges, seven new solutions could be generated
in which three solutions are actually 2-opt results.22 We try every three such edges
and change the current solution to the best improved one if it exists. So 3-opt is the
abbreviation of ‚Äúlocal optimal by 3-edge perturbation.‚Äù
Generally, the local search results of 3-opt are better than those of 2-opt.
Starting from 3-opt, we can generalize it as k-opt. But what is the best k value
for our TSP? Lin and Kernighan developed 3-opt and suggested the so-called Lin‚Äì
Kernighan algorithm in 1973 [15]. The Lin‚ÄìKernighan algorithm has special con-
siderations in limiting the search space and adaptively determining k. In 1997 John-
son and McGeoch used modern data structures to develop the Lin‚ÄìKernighan algo-
rithm [16]. So sometimes it is called Lin‚ÄìKernighan‚ÄìJohnson algorithm.
Among these local search methods, the Lin‚ÄìKernighan algorithm is the clear
winner.23
7.3.3 Evolutionary Algorithm Code Schemes for Traveling
Salesman Problem
TSP is actually a touchstone for combinatorial optimization EAs. So there are sev-
eral code methods for TSP. Before discussing the speciÔ¨Åc code methods, we need to
determine the real size of the search space for an n-city TSP.
Theorem 7.1. For an n-city symmetric TSP, there are (n‚àí1)!
2
different solutions.
Proof. We can prove the theorem by mathematical induction. For the smallest TSP,
n = 3, it is easy to know that there are 1 = 2!
2 solutions. Suppose there are different
(n‚àí2)!
2
solutions for (n ‚àí1) cities. We can get the solution for an n-city TSP in the
following way. For every edge of one solution for (n‚àí1) cities, we break it and then
absorb city n into the cycle. This means that for each solution of an (n‚àí1)-city TSP,
we have (n ‚àí1) different ways to generate the tour for an n-city TSP. Because the
(n‚àí2)!
2
solutions for (n ‚àí1) cities are different, there are (n‚àí1)!
2
different solutions
for n cities.
22 The proof of such a statement is left as an exercise.
23 Neto provided C source code for the Lin‚ÄìKernighan algorithm at the Web site http://www.
cs.toronto.edu/Àúneto/research/lk/.

282
7 Combinatorial Optimization
7.3.3.1 Edge Code
For an n-vertex complete graph, it has n(n‚àí1)
2
edges.24 The initial idea is that we can
number them, and select n edges from them to form a Hamiltonian cycle, which is
edge code. An example edge code of an eight-city TSP is (14,28,18,23,27,7,6,16).
These eight edges form a Hamiltonian cycle.
But the main drawback of edge code is ensuring that these n edges form a Hamil-
tonian cycle and preventing the variation operators from breaking these Hamiltonian
cycles. These questions are very hard to answer, so edge code is rarely used.
The operation domain of edge code is the number of combinations of n edges in
n(n‚àí1)
2
, i.e., Cn
n(n‚àí1)
2
=
( n(n‚àí1)
2n
)
.
7.3.3.2 Binary Code
Since representing solutions of TSPs with edges directly is hard, we turn to so-
lutions with vertices, i.e., cities. One type is to use binary code to represent city
numbers. For an n-city TSP, we need ‚åàlog2 n‚åâbinary numbers to represent the city,
where ‚åà‚åâis a rounding-up function. An example binary code of a six-city TSP is
(000 010 001 100 011 101), which means tour 1‚àí3‚àí2‚àí5‚àí4‚àí6‚àí1.
The standard variation operators will destroy the feasibility of offspring or mu-
tants. So extra repair techniques are necessary.
Generally, every gene, e.g., city 3 (010) in the above example, has n possibilities.
So for an n-city tour, the operation domain of the binary code is nn.
7.3.3.3 Random Key Code
Random key code is quite useful in a hard constrained situation, which will be il-
lustrated more clearly in Sect. 7.4. For an n-city TSP, we could generate n uni-
formly distributed random numbers in the range (0,1) and then rank them from
small to large. The locus of the smallest random number is the Ô¨Årst city being
visited, and then the locus of the second smallest random number is the sec-
ond city being visited, etc. An example random key code of a six-city TSP is
(0.9501,0.2311,0.4568,0.4860,0.8913,0.7621), which corresponds to tour 2‚àí3‚àí
4‚àí6‚àí5‚àí1‚àí2.
The standard variation operators will always generate legal offspring. That is the
reason for random key code being widely used in hard constrained combinatorial op-
timization. But the problem is that a lot of information contained in the parents will
sometimes be lost by standard single-point crossover. For example, another chromo-
some is (0.4565,0.0185,0.8214,0.6154,0.4447,0.7919), which corresponds to tour
2‚àí5‚àí1‚àí4‚àí6‚àí3‚àí2. A single-point crossover of these two chromosomes at the
24 The proof of this statement is left as an exercise.

7.3 Traveling Salesman Problem
283
intermediate location will generate (0.9501,0.2311,0.4568,0.6154,0.4447,0.7919)
and (0.4565,0.0185,0.8214,0.4860,0.8913,0.7621), which corresponds to tour 2‚àí
5‚àí3‚àí4‚àí6‚àí1‚àí2 and 2‚àí1‚àí4‚àí6‚àí3‚àí5‚àí2. The roads in the parents are almost
lost in the offspring, i.e., the good information contained in the parent is lost. The
reason for the low efÔ¨Åciency of random key code is that the sequence of the city in
the tour is not determined solely by its own value in the chromosome but also by the
relative magnitude to others.
The operation domain for random key code is a real value in the range (0,1),
which contains inÔ¨Ånite numbers.
7.3.3.4 Path Code
Perhaps the most straightforward representation for TSP is path code, i.e., the se-
quence of cities being visited as a Hamiltonian path is used for representing a tour.
For an n-city TSP, we could use an n permutation to represent a tour. An example
path code of a six-city TSP is (2‚àí3‚àí6‚àí1‚àí4‚àí5), which is corresponding to tour
2‚àí3‚àí6‚àí1‚àí4‚àí5‚àí2.
Sure the standard variation operators will break the permutation. Researchers
have suggested a lot special techniques for generating legal offsprings and mutants
with path code, which will be elaborated in Sect. 7.3.4.
The operation domain of the edge code is n number permutation, i.e., Pn
n =
n!
(n‚àín)! = n!.
It‚Äôs necessary to mention that there are 2n redundancy in path representation [17].
Chromosomes (2 ‚àí3 ‚àí6 ‚àí1 ‚àí4 ‚àí5), (3 ‚àí6 ‚àí1 ‚àí4 ‚àí5 ‚àí2), (6 ‚àí1 ‚àí4 ‚àí5 ‚àí
2 ‚àí3), (1 ‚àí4 ‚àí5 ‚àí2 ‚àí3 ‚àí6), (4 ‚àí5 ‚àí2 ‚àí3 ‚àí6 ‚àí1), (5 ‚àí2 ‚àí3 ‚àí6 ‚àí1 ‚àí4),
(5 ‚àí4 ‚àí1 ‚àí6 ‚àí3 ‚àí2), (4 ‚àí1 ‚àí6 ‚àí3 ‚àí2 ‚àí5), (1 ‚àí6 ‚àí3 ‚àí2 ‚àí5 ‚àí4), (6 ‚àí3 ‚àí
2 ‚àí5 ‚àí4 ‚àí1), (3 ‚àí2 ‚àí5 ‚àí4 ‚àí1 ‚àí6), and (2 ‚àí5 ‚àí4 ‚àí1 ‚àí6 ‚àí3) all represent
the same tour. Representation redundancy might cause unnecessary search effort
between individuals corresponding to the same phenotype and thus waste the search
resource.
But we would like to remind the reader again that random key code and path
code are all tradeoffs between feasibility preserving and search scaling limiting.
7.3.3.5 Adjacent Code
Adjacent code and the following ordinal code were all suggested by Grefenstette et
al. [18]. Adjacent code is similar to path code, i.e., also an n permutation, in which
allele j in locus i means city j just follows city i in the tour, i.e., they are ‚Äúadjacent.‚Äù
An example adjacent code of a six-city TSP is (2 ‚àí3 ‚àí6 ‚àí1 ‚àí4 ‚àí5). We could
decode it in this way. Locus 1 has a value of 2, which means cities 1‚àí2 are in the
tour. Then we go to locus 2. It has a value of 3, which means cities 1‚àí2‚àí3 are in
the tour. Then locus 3 with a value of 6 means cities 1 ‚àí2 ‚àí3 ‚àí6 are in the tour.
Finally we can get the tour 1‚àí2‚àí3‚àí6‚àí5‚àí4‚àí1.

284
7 Combinatorial Optimization
The variation operators for path code can be used in adjacent code and the oper-
ation domain of the edge code is also n!.
The problem with adjacent code is that not all permutations can generate a legal
tour. For example, (2 ‚àí3 ‚àí1 ‚àí6 ‚àí4 ‚àí5) means two cycles, 1 ‚àí2 ‚àí3 ‚àí1 and
4 ‚àí6 ‚àí5 ‚àí4. So special considerations, e.g., repair, are necessary when adjacent
variation operators generate new individuals for adjacent code.
7.3.3.6 Ordinal Code
We can use a standard city order list for n-city TSP, such as 1 ‚àí2 ‚àí3 ‚àí4 ‚àí5 ‚àí6
for six cities. Ordinal code also uses an n permutation to represent a solution. We
let allele j in locus i represent that city i in the tour is the city has the jth number,
i.e., its ‚Äúordinal,‚Äù in the current city order list. After we pick a city, it is deleted from
the order list. So for ordinal code chromosome (3,2,4,2,2,1) in a six-city TSP, its
decoding process could be illustrated by Table 7.4.
Table 7.4 The decoding process of chromosome (3,2,4,2,2,1) in a six-city TSP


N


	

	


	$
%%


		


CCCCC
1	@@@@@3


CCCC
1@@@@@3
C

CCC
1@@@@@3
CC

CC
1@@@@@3
CCC

C
1@@@@@3
CCCC


1@@@@@3
CCCCCC





















According to the above decoding process, it is not difÔ¨Åcult to determine that the
constraint for allele j of locus i is that 1 ‚â§j ‚â§n‚àíi+1.
The standard variation operators will generate legal offspring. But the problem is
that the heritability of ordinal code is not good, similar to random key code, because
the sequence of the city in the tour is not only determined by its own value in the
chromosome.25
The Ô¨Årst gene has n possible values. The second gene has (n‚àí1) possible values,
etc. So the operation domain of the ordinal code is also the number of n permutation,
i.e., n!.
25 We leave this statement as an exercise.

7.3 Traveling Salesman Problem
285
7.3.3.7 Matrix Code
Apart from one-dimensional representation for TSP, several 2-D representations,
i.e., matrix code, have been suggested by researchers [19, 20]. Special decoding
mechanisms are necessary. These code schemes suffer from bad heritability or re-
quire a repair process to maintain the legality of the offspring. So we neglect the
introduction to them.
Before ending this subsection, we need to compare the search space of the op-
eration domain for different code schemes. The numerical comparison is listed in
Table 7.5.26
Table 7.5 The numerical comparison of the search space of the operation domain for different
code schemes for TSP


















"
#
&
)
2


 M

 




)
)
)
))
#)
%









 
 
%







 









))

)

) 
"
) 
&
) 
:
(

 
"



)

)
"
) 
#
) 
)
)

.	
/

!	
M
 


)
")
))

) 

) 

) 

As can be seen from Table 7.5, even though path code has 2n redundancy, its
search space is relatively small compared to other coding methods, which means
with the same search ability of EAs, path code has more chances to Ô¨Ånd good solu-
tions. That is why path code has become the most often used code scheme.27
7.3.4 Variation Operators for Permutation Code
In this subsection, we will focus on several variation operators for permutation code,
which might be the most often used code scheme apart from real code. Mutation
operators will be introduced Ô¨Årst for their simplicity.
26 Some of the large numbers are represented in the approximate form of scientiÔ¨Åc notation to save
space without loss of generality.
27 The shortcomings of adjacent code and ordinal code have been explained above.

286
7 Combinatorial Optimization
7.3.4.1 Mutation Operators
Picking one gene at random and changing its allele into that of another city will
deÔ¨Ånitely break a permutation. So at least we need to handle them pairwise.
Exchange Mutation
The exchange mutation (EM) was suggested by Banzhaf [21]. It is also called swap
mutation.
In EM, we can just pick two loci randomly and exchange their alleles. Thus
permutation is kept and perturbation is achieved. For example, we pick loci 3 and 6
in path code (1‚àí2‚àí3‚àí4‚àí5‚àí6‚àí7‚àí8‚àí9) for a nine-city TSP and get (1‚àí2‚àí
6‚àí4‚àí5‚àí3‚àí7‚àí8‚àí9) as in Fig. 7.13.


























	










	





Fig. 7.13 An example of exchange mutation for permutation code
Insertion Mutation
The insertion mutation (ISM) was suggested by Michalewicz [22].
In ISM, we can just pick one locus randomly and insert its allele into a random
place. For example, we pick locus 6 in path code (1‚àí2‚àí3‚àí4‚àí5‚àí6‚àí7‚àí8‚àí9)
and insert its allele into the place between loci 2 and 3 for a nine-city TSP and get
(1‚àí2‚àí6‚àí3‚àí4‚àí5‚àí7‚àí8‚àí9) as in Fig. 7.14.



















	












	



Fig. 7.14 An example of insertion mutation for permutation code
Displacement Mutation
The displacement mutation (DM) was also suggested by Michalewicz [22].
DM is just a segment form of ISM; we just pick one subtour randomly and insert
it into a random place. For example, we pick subtour 4 ‚àí5 ‚àí6 in path code (1 ‚àí
2‚àí3‚àí4‚àí5‚àí6‚àí7‚àí8‚àí9) and insert it into the place between loci 8 and 9 for a
nine-city TSP and get (1‚àí2‚àí3‚àí7‚àí8‚àí4‚àí5‚àí6‚àí9) as in Fig. 7.15.

7.3 Traveling Salesman Problem
287























	








	







Fig. 7.15 An example of displacement mutation for permutation code
Simple Inversion Mutation
The simple inversion mutation (SIM) was suggested by Grefenstette et al. [18].
In SIM, we can just pick one subtour randomly and inverse it. For example, we
pick subtour 4‚àí5‚àí6 in path code (1‚àí2‚àí3‚àí4‚àí5‚àí6‚àí7‚àí8‚àí9) and inverse it
and get (1‚àí2‚àí3‚àí6‚àí5‚àí4‚àí7‚àí8‚àí9) as in Fig. 7.16.


























	








	






Fig. 7.16 An example of simple inversion mutation for permutation code
The SIM in Fig. 7.16 is identical to the tour change in Fig. 7.17. SIM is the same
as a step of local search in 2-opt.28




















"
#
&






"
#
&
Fig. 7.17 The relationship between simple inversion mutation and a step of local search in 2-opt
Inversion Mutation
Fogel combined the characteristics of SIM and DM and suggested the inversion
mutation (IM) [23].
We just pick one subtour randomly and insert it inversely into a random place. For
example, we pick subtour 4‚àí5‚àí6 in path code (1‚àí2‚àí3‚àí4‚àí5‚àí6‚àí7‚àí8‚àí9)
28 It is quite clear by comparing Figs. 7.17 and 7.12.

288
7 Combinatorial Optimization
and insert its allele into the place between loci 8 and 9 for a nine-city TSP and get
(1‚àí2‚àí3‚àí7‚àí8‚àí6‚àí5‚àí4‚àí9) as in Fig. 7.18.
































	








	






Fig. 7.18 An example of inversion mutation for permutation code
7.3.4.2 Crossover Operators
Crossover operators, i.e., single-point crossover, multiple-point crossover, and uni-
form crossover, will break a permutation. So we need to consider the real link situ-
ation of cities in crossover for permutation code.
Partially Mapped Crossover
The partially mapped crossover (PMX) was suggested by Goldberg and Lingle [24].
It can be implemented with the following steps.
Partially Mapped Crossover for Permutation Code
Step 1: Select two random places in a chromosome, e.g., between loci 2
and 3 and between loci 6 and 7 in Fig. 7.19.
Step 2: Exchange the subtours formed by these two places and get the
temporary offspring 1‚Äô and 2‚Äô. They might break the permutation.
Step 3: Determine the partial maps in these two subtours. The partial map
is the allele pair at same locus of the subtours. Thus 1 ‚Üî6 ‚Üî3, 4 ‚Üî9, and
5 ‚Üî2 are partial maps.
Step 4: Use the partial maps to map the conÔ¨Çict allele to the legal allele.
For example, in the Ô¨Årst locus of offspring 1‚Äô, allele 1 is in conÔ¨Çict with the
allele in the exchanged subtour, which needs to be kept, so we try to map it to
6 according to the partial maps, but 6 is still in conÔ¨Çict. Then we should map
it again using partial maps to get the legal allele 3.
PMX keeps one subtour and changes others according to partial maps for main-
taining permutation. So only the city connection relationships in the subtour, e.g.,
3 ‚àí4 ‚àí5 ‚àí6 and 6 ‚àí9 ‚àí2 ‚àí1 in Fig. 7.19, and those do not in conÔ¨Çict with the
subtour, e.g., 7‚àí8 in Fig. 7.19, will be kept.

7.3 Traveling Salesman Problem
289




















	



















	




	








	



	





.
	
.
	
/''
%O
/''
%O
.
	



	






	









/''
%
/''
%
Fig. 7.19 An example of partially mapped crossover for permutation code
Order Crossover
The order crossover (OX) was suggested by Davis [25]. It can be implemented
with the following steps. For simplicity, we only describe the process of generating
offspring 1. Offspring 2 is left as an exercise.
Order Crossover for Permutation Code
Step 1: Select two random places in a chromosome, e.g., between loci 2
and 3 and between loci 6 and 7 in Fig 7.20. Mark the second place on parent 2.
Step 2: Generate partial offspring 1 by copying the subtour from parent 1.
Step 3: Mark the allele in parent 2 that already exists in the partial offspring
1.
Step 4: From the marked place in parent 2, sequentially pick the allele that
is not in partial offspring 1 and put it sequentially into the locus start from the
marked place.














	














	


.
	
.
	


	






/''
%
Fig. 7.20 An example of order crossover for permutation code

290
7 Combinatorial Optimization
OX not only keeps the exchanged subtour but also tries to keep the city connec-
tion after the marked point. Apart from that, OX keeps the precedence relationship
between cities, which might contribute to the search.
Position-base Crossover
The position-based crossover (PBX) was suggested by Syswerda [26]. It can be
implemented with the following steps. For simplicity, we only describe the process
of generating offspring 1. Offspring 2 is left as an exercise.
Position-based Crossover for Permutation Code
Step 1: Select several loci randomly from parent 1, e.g., loci 2, 5, 6, and 9
in parent 1 in Fig. 7.21.
Step 2: Generate partial offspring 1 by putting the selected alleles from
parent 1 into their corresponding loci.
Step 3: Mark the alleles in parent 2 that have been selected in parent 1.
Step 4: From the beginning of parent 2, sequentially select the allele that
has not been marked and put it into the vacancy of partial offspring 1 from the
beginning.














	













	


.
	
.
	







	

/''
%
Fig. 7.21 An example of position-based crossover for permutation code
PBX keeps the position of several alleles from one parent and the precedence
relationship from another parent. The position in the chromosome for TSP has little
meaning because of the redundancy of permutation code. So generally PBX only
keeps the precedence relationship of parents unless the selected loci are connected.
Syswerda also suggested another crossover operator, order-based crossover (OBX),
which is almost the same as PBX.

7.3 Traveling Salesman Problem
291
Cycle Crossover
The cycle crossover (CX) was suggested by Oliver et al. [27]. They use the concept
of virtual cycle to maintain a permutation and thus keep the position of parents in
a deterministic way. It can be implemented with the following steps. For simplicity,
we only describe the process of generating offspring 1. Offspring 2 is left as an
exercise.
Cycle Crossover for Permutation Code
Step 1: Generate the virtual cycle from the Ô¨Årst unused locus in parent 1.
Suppose we are at locus i with allele j in parent 1; there is an instant transfer
machine that can transfer us from locus i of parent 1 to that of parent 2, whose
allele is k. The instant transfer machine will then send us instantly back to
locus k in parent 1, whose allele is the next step of our virtual cycle. Let us
start the virtual cycle from city 1 in parent 1 in Fig 7.22. The instant transfer
machine will transfer us to locus 1 of parent 2 and send us to locus 5 of
parent 1 instantly. With this instant transfer machine, we can Ô¨Ånd the virtual
cycle formed by cities 1‚àí5‚àí2‚àí4‚àí9‚àí1.
Step 2: Copy the alleles in the virtual cycle in parent 1 to their correspond-
ing loci in offspring 1.
Step 3: Generate the virtual cycle from the Ô¨Årst unused locus in parent 2
with the procedure introduced above. 6 ‚àí3 is the virtual cycle generated in
Fig. 7.22.
Step 4: Copy the alleles in the virtual cycle in parent 2 to their correspond-
ing loci in offspring 1.
Step 5: Repeat the above steps until all the loci have been marked.















	











	





.
	
.
	



	





/''
%
Fig. 7.22 An example of cycle crossover for permutation code

292
7 Combinatorial Optimization
CX uses virtual cycle to maintain the permutation and keep the position of the
related alleles. For TSP, it means to keep the precedence relationship between the
related cities.
Edge Recombination Crossover
The edge recombination crossover (ERX) was suggested by Whitley et al. [28] and
we adopt the way of edge-3 in Eiben and Smith‚Äôs book to introduce the method [29].
The real information contained in the permutation code of TSP is the edge be-
tween two cities. So it is better to maintain the same edge in both parents. That is
the basic idea of ERX.
For a six-city TSP, suppose parent 1 is (1 ‚àí2 ‚àí3 ‚àí4 ‚àí5 ‚àí6) and parent 2 is
(4‚àí1‚àí2‚àí6‚àí3‚àí5). Then we can generate the edge table in Table 7.6. The edge
column in the table is the cities with edges on the corresponding city. A + means
that this edge is in both parents and thus should be kept in the offspring.
Table 7.6 Edge table for (1‚àí2‚àí3‚àí4‚àí5‚àí6) and (4‚àí1‚àí2‚àí6‚àí3‚àí5)

	(
%
	(
%

=@@

@@=

=@@

@=@

@@@

@@@















ERX starts by randomly selecting one city and then checks the edge table to
determine the next city according to the following rules.
‚àôIf the current city has a common edge, go to the city with the common edge.
‚àôOtherwise go to the city with the shortest edge list.29
‚àôIf there is more than one city with the shortest edge list, ties are split by randomly
going to one of them.
After determining the next city, update the edge table by deleting all the cities
already visited. Repeat the above procedure until all the cities are visited.
Suppose we start from city 1; we can use Table 7.7 to help in the construction of
offspring.
We can draw the parents and offspring of ERX in Fig. 7.23. Only the connections
between cities 1 and 3 are new because city 3 is the last choice in the above proce-
dure. So the edge information in parents is maintained by ERX, which has made it
the best crossover operator for a rather long time.
29 The reason for selecting the city with the shortest edge list is to decrease the possibility of in-
completion in the construction process. Readers are encouraged to verify this statement by deleting
this rule and Ô¨Ånishing Table 7.7 again.

7.3 Traveling Salesman Problem
293
Table 7.7 ERX table for (1‚àí2‚àí3‚àí4‚àí5‚àí6) and (4‚àí1‚àí2‚àí6‚àí3‚àí5)





%	 


	
	(

2
4+	
	(
.
		

1=@@3@1=@@3
1@@@3@1@@=3
1@=@3@1@@@3

@@

%

1C3
1@3
1@@@3@1@=3
1@=@3@1@@3

@

		
%	

1CC3
1@@3@1@=3
1@=@3@1@3

@ 
2

1CCC3
1@3@1@=3
1@=3

@

%

1CCCC3
13@13


9		(

1CCCCC3
































Fig. 7.23 Parents and offspring of ERX
Edge Assembly Crossover
Johnson30 published a paper in Nature magazine in 1987 calling for the combination
of problem-speciÔ¨Åc techniques in coding and variation schemes [30], which caused
the invention of the following crossover operators.
The edge assembly crossover (EAX), which was suggested by Nagata and
Kobayashi in 1997 [31], is such a crossover operator. EAX contains three steps.
1. Generate the ‚ÄúAB-cycles‚Äù by parents A and B.
2. Generate the ‚ÄúE-set‚Äù from ‚ÄúAB-cycles.‚Äù
3. Generate an offspring using parent A, ‚ÄúE-set,‚Äù and the greedy method.
For two parents A and B, illustrated by the solid line and the dashed line in
Fig. 7.24, we can start at any city and go along the road belonging to A and B in
turn until we come back to the original city, which forms a cycle. If in a cycle the
edge number belonging to A is the same as that belonging to B, we call it an AB-
cycle, i.e., a cycle formed by roads in A and B in turn with the same road number.
30 The coauthor of the Lin‚ÄìKernighan‚ÄìJohnson algorithm.

294
7 Combinatorial Optimization
Fig. 7.24 Parents A and B of
EAX















































"
#
&
After one AB-cycle has been determined, its roads are removed from Fig. 7.24.
Iteratively operating the above process will generate several AB-cycles, which is
illustrated in Fig. 7.25.
Fig. 7.25 AB-cycles in Fig.
7.24






























#
&
"




#
&







"
Then a deterministic and a random approach were suggested by Nagata and
Kobayashi to select elements from an AB-cycle set to form an E-set. We can just
simplify the process as selecting each AB-cycle into an E-set with probability 0.5.
Suppose in the four AB-cycles of Fig. 7.25 only the one in the upper right is selected
for the E-set.
After that, a temporary offspring is generated by copying parent A. Then every
road in the E-set is used to modify the temporary offspring. If it belongs to parent A
and appears in the temporary offspring, it is deleted from the temporary offspring.
If it belongs to parent B and does not appear in the temporary offspring, it is added
to the temporary offspring. This procedure will form several subtours, which is il-
lustrated by Fig. 7.26.
Then a greedy method is adopted to generate offspring that are short in length.
We start from the subtour with the smallest number of roads, such as the lower
left subtour in Fig. 7.26, and check which subtour to combine it with and how to
combine them in a greedy way. Suppose we are considering the edges (vq,vq+1)

7.3 Traveling Salesman Problem
295
Fig. 7.26 Subtours generated
by parent A and E-set


























"
#
&
and (vr,vr+1) in Fig. 7.27, where (vq,vq+1) is a road in the considered subtour and
(vr,vr+1) is a road in an other subtour.31
Fig. 7.27 Greedy method to
generate offspring


















 


 
The decrease of distance by combining these two subtours is as follows:
cut (q,r) = d
(
vq,vq+1
)
+d (vr,vr+1)
(7.12)
where d() is the length of the road. The increase of distance by combining these two
subtours is as follows:
link(q,r) = min
{
d (vq,vr)+d
(
vq+1,vr+1
)
,d (vq,vr+1)+d
(
vq+1,vr
)}
(7.13)
The meaning of Eq. 7.13 is to Ô¨Ånd the way with smallest distance increase. The Ô¨Årst
part, i.e., d (vq,vr)+d
(
vq+1,vr+1
)
, is illustrated by dotted lines in Fig. 7.27, and
the second part, i.e., d (vq,vr+1)+d
(
vq+1,vr
)
, is illustrated by dashed lines in Fig.
7.27.
We start from the subtour with the smallest number of roads and Ô¨Ånd the edge
(vq,vq+1) in it and the edge (vr,vr+1) in the other subtours to minimize the total
length increase expressed by Eq. 7.14. Then these two subtours are integrated:
min
q,r {link(q,r)‚àícut (q,r)}
(7.14)
Repeat the above integration procedure until we get a full tour, i.e., the offspring.
31 Here the deÔ¨Ånition of node vq, vq+1, vr, and vr+1 is for the simplicity of the following statement.

296
7 Combinatorial Optimization
EAX requires a lot of computational resources for determining the AB-cycle,
the E-set, and offspring. But it uses edges from parents as much as possible and
has greedy methods to do local searches in the crossover process, which makes it a
highly efÔ¨Åcient search operator.
Nagata and Kobayashi used a steady state EA with a parent-offspring competition
in the replacement process.
Distance Preserving Crossover
In 1995 Boese Ô¨Ånished a technical report for studying the solution landscape of
TSP [32]. The very important conclusion for TSPs is that the local optimal solutions
are close to each other and also close to the global optimal solution, which means
that the solution landscape roughly has a unimodal shape with small ripples. So
generating offspring that are close to parents is good for the search in a TSP solution
landscape.
The distance preserving crossover (DPX) was suggested by Freisleben and Merz
in 1996 [33].
The distance between two tours deÔ¨Åned by Freisleben and Merz is the number of
roads in which two tours differ, corresponding to Hamming distance in binary code.
For example, Fig. 7.28 lists two parents. Both of them connect 5 with 3 and 3 with
9. But in parent 1, 9 is connected to 1, which is different from parent 2. In this way,
we can Ô¨Ånd out that their distance is 4, corresponding to 9‚àí1, 7‚àí8, 4‚àí6, and 6‚àí2
in parent 1 and 1‚àí2, 9‚àí4, 8‚àí6, and 6‚àí7 in parent 2.












	









	




Fig. 7.28 Parents of DPX
Parent 1 is then subdivided by the different roads into several subtours, i.e., ‚à£5‚àí
3‚àí9‚à£1‚àí7‚à£8‚àí4‚à£6‚à£2‚à£.32
Then we can start from any city in parent 1, e.g., city 5, and get the offspring
with following rules.
‚àôIf the current city is connected with another city in the subtour, go to that city.
After Ô¨Ånishing a subtour, delete it from the list of current subtours, such as (5‚àí
3‚àí9) in Table 7.8.
‚àôOtherwise Ô¨Ånd out the cities that might be the edge cities of the other subtours,
according to the current subtour. Delete the cities that are connected to either
parent. City 1 is deleted because of 9‚àí1 in parent 1 and city 4 is deleted because
of 9‚àí4 in parent 2. Then use a greedy method to Ô¨Ånd the minimum distance in
32 The subtour number has strong ties with the distance. What is their relationship?

7.3 Traveling Salesman Problem
297
d(9,7), d(9,8), d(9,6), and d(9,2). Suppose city 6 is the closet city to city 9 in
the choices. Then delete subtour 6 from the list of current subtours.33
Table 7.8 DPX table for (5‚àí3‚àí9‚àí1‚àí7‚àí8‚àí4‚àí6‚àí2) and (1‚àí2‚àí5‚àí3‚àí9‚àí4‚àí8‚àí6‚àí7)








%	 


	
	(

2
4+	
	(
.
		

1=@@3@1=@@3
1@@@3@1@@=3
1@=@3@1@@@3

@@

%

1C3
1@3
1@@@3@1@=3
1@=@3@1@@3

@

		
%	

1CC3
1@@3@1@=3
1@=@3@1@3

@ 
2

1CCC3
1@3@1@=3
1@=3

@

%

1CCCC3
13@13


9		(

1CCCCC3


Table 7.8 illustrates the crossover process for generating offspring (5 ‚àí3 ‚àí9 ‚àí
6 ‚àí1 ‚àí7 ‚àí4 ‚àí8 ‚àí2). It is easy to verify that the distance from the offspring to
either of the parents is 4, which means that DPX could generate an offspring that is
equidistant to both of its parents.34
DPX uses the duplicated roads in the parents, preserves the distance to the par-
ents, and utilizes a greedy method to connect the subtours into a solution. According
to Johnson‚Äôs paper [30] and Boese‚Äôs report [32], these properties promote searchs in
the TSP solution landscape.
Freisleben and Merz also used the Lin‚ÄìKernighan algorithm for the offspring of
DPX. This further improves the algorithm‚Äôs search ability and makes it the bench-
mark of EA-based TSP algorithms.
Natural Crossover
Jung and Moon divided EAs for TSPs into two categories: pure EAs, which do not
utilize local search methods and thus have the requirement that variation operators
be able to generate good offspring, and hybrid EAs, which are actually MAs and
thus have the requirement that variation operators have strong search abilities to
provide the local search method with a promising initial solution.
33 In this way, the offspring can keep the same distance to both parents.
34 It is a little bit like UNDX in real code.

298
7 Combinatorial Optimization
Based on this idea, Jung and Moon suggested a natural expression and a natural
crossover (NX) for 2-D Euclidean TSPs in 2002 [34].
There is no code for a solution of 2-D Euclidean TSPs. Jung and Moon just used
a real path to represent solutions of 2-D Euclidean TSPs. In Fig. 7.29, solid lines
represent one solution and dashed lines represent another solution. They are parents
1 and 2 of NX, respectively.
Fig. 7.29 Parents of NX


































"
#
&
Then they use several simple curves, such as straight lines, triangles, rectangles,
and ellipses, to divide the area into several parts. Parts covered by an odd number of
curves become gray and those covered by an even number of curves remain white.
An example of such curves is illustrated in Fig. 7.30.
Fig. 7.30 Parts covered by
several curves































"
#
&
For a road in parent 1, if both of its endpoints are in the gray part, it is kept.
Otherwise it is discarded. In contrast, for a road in parent 2, if both of its endpoints
are in the white part, it is kept. Otherwise it is discarded. The procedure generates
several subtours, illustrated in Fig. 7.31.
A greedy strategy is taken to connect these subtours into a Hamiltonian cycle. A
possible result is illustrated in Fig. 7.32, in which the solid lines are roads in parents
and dashed lines are generated by a greedy method.

7.4 Job-shop Scheduling Problem
299
Fig. 7.31 Subtours generated
by NX




















"
#
&






"
#
&
Fig. 7.32 Offspring of NX

























"
#
&
As can be seen from the above process, NX keeps certain roads in the parents
while introducing a certain perturbation from parents. After that, Jung and Moon
utilize the Lin‚ÄìKernighan algorithm to improve the offspring. In Jung and Moon‚Äôs
philosophy, the perturbations provide promising initial solutions for local search
methods.
They implemented the algorithm in a steady state way and compared NX, DPX,
and EAX using benchmark TSPs from TSPLIB. According to their numerical ex-
periments, the result of NX + Lin‚ÄìKernighan algorithm is slightly better than DPX
+ Lin‚ÄìKernighan algorithm but with a slightly longer time.
7.4 Job-shop Scheduling Problem
Scheduling is the process of generating a schedule. A schedule is a sequence of
events subject to some predeÔ¨Åned constraints. The objective of scheduling is to
achieve some goals, such as the shortest time or the smallest expense to Ô¨Ånish a
job.
Let us suppose you want to have friends over for a meal. The menu has been
made and there are several dishes to serve. Each dish will require different kitchen
facilities with different sequences, e.g., dough should Ô¨Årst be made in the bread

300
7 Combinatorial Optimization
maker and then, after covering it with cheese, tomato, and sausage, etc., baked in
the oven to make pizza. You need to schedule the sequence of all events, e.g., the
sequence of preparing various dishes on the stove and in the oven and bread maker,
etc., so that your preparation time is minimized. What a mess!
7.4.1 Problem Description
In scholarly disciplines, these events that need to be scheduled are called operations.
The resources to process these operations are called machines. Dishes you want to
serve at the party are called jobs.
Without loss of generality, we suppose there are n jobs needing to be Ô¨Ånished,
each of which should be processed35 with m different machines in a job shop. Differ-
ent jobs might have different processing sequences on these machines.36 We use oi jk
to represent that job i‚Äôs operation j will be processed on machine k with processing
time pijk.
For operation oijk, oi,(j+1),h is its direct succeeding operation or direct successor,
oi,(l> j),h are its succeeding operations or successors, oi,(j‚àí1),h is its direct preced-
ing operation or direct predecessor, and oi,(l<j),h are its preceding operations or
predecessors, where h is another machine.
An example of such predeÔ¨Åned parameters is illustrated in Table 7.9. According
to Table 7.9, job 2 should Ô¨Årst be processed by machine 2 with time 2, then by
machine 1 with time 3, and Ô¨Ånally by machine 3 with time 3.37
Table 7.9 An example of job-shop scheduling with n = 4 jobs and m = 3 machines






+

/
	






> 



,
,
,
> 



,
,
,
> 



,
,
,
> 



,
,
,

The predeÔ¨Åned job processing sequence on different machines is called the prece-
dence relationship or precedence constraint. The processing time and precedence
relationship together are called technological requirements. The job-shop schedul-
ing problem (JSSP), which is the most famous scheduling problem, is to deter-
mine operations‚Äô processing sequences on each machine, subject to precedence con-
35 Each process is an operation.
36 It is different from Ô¨Çow-shop introduced in Sect. 7.1.1.
37 Here we omit the unit of processing time for simplicity‚Äôs sake.

7.4 Job-shop Scheduling Problem
301
straints and considering the processing time, so as to minimize the total processing
time, i.e., the makespan.38 The mathematical model of JSSP can be illustrated as
follows:
min tM = max
i, j,k
{
tijk
}
s.t. ti,j‚àí1,h + pijk ‚â§tijk,
‚àÄi, j,k,h
tijk ‚â•0,
‚àÄi, j,k
(7.15)
where tijk is the completion time of operation oijk and tM is the makespan. ti,j‚àí1,h +
pijk ‚â§tijk means that for every operation, its direct predecessor‚Äôs completion time
plus its processing time might be smaller that its completion time because it might
not be processed right after the completion of its direct predecessor.39
The solution space of JSSP is rather large. We can consider it in the following
way. Suppose that all the precedence relationships of n jobs are 1 ‚Üí‚ãÖ‚ãÖ‚ãÖ‚Üím, which
makes the JSSP a Ô¨Çow-shop scheduling problem.40 There are n! possible solutions
for the Ô¨Årst machine. The possible number of solutions for the second machine is
also n! because n jobs must be processed by machine 2 after machine 1. So in total
there are (n!)m possible solutions for this problem. This is a rather large number.41
The above discussion is a simpliÔ¨Åcation so that we can estimate the maximum
feasible solution space. Actually, the precedence constraints will limit the number
of feasible solutions but increase the difÔ¨Åculty. Generally, n permutations on each
machine might generate an infeasible solution. Let us discuss this point with a very
simple JSSP with n = m = 2. Job 1 needs to be processed by machine 2 Ô¨Årst and
then machine 1, but job 2 needs to be processed in the sequence 1 ‚Üí2. So the only
possible solution for this problem is to let machine 1 process job 2 Ô¨Årst and then
job 1 and let machine 2 process job 1 Ô¨Årst and then job 2. But permutation (1,2)
on machine 1 and permutation (2,1) on machine 2 are infeasible solutions. This
phenomenon is called deadlock.42
There are several ways to illustrate the solution of a JSSP. We will introduce two
of them and focus on the second one later.
Because of the precedence relationship, the initial idea to represent a JSSP and
its solution was to use a directed graph, as in Fig. 7.33. We have a virtual start point
and a virtual end point. Each solid arrow represents the precedence relationship of
one job. The number in the circle represents the machine number and the number
above the circle is the processing time of that operation. For example, job 1 needs
38 Makespan could be understood as the span, i.e., period, of making jobs. There are other objec-
tives. For simplicity‚Äôs sake, we only discuss makespan in this text. Interested readers are referred
to [2, 35].
39 Perhaps machine k, which needs to process job i, is processing other jobs at ti,j‚àí1,h.
40 Not a permutation Ô¨Çow-shop scheduling problem.
41 Do you remember the solution size of 50 jobs for a permutation Ô¨Çow-shop scheduling problem
and how long it would take to solve that problem using a ‚Äúcrude‚Äù enumeration method with the
fastest supercomputer? Try n = m = 15 for JSSP.
42 Any permutation code for problems with precedence constraints might face deadlock. Special
techniques, e.g., repair, are necessary for unlocking the solution. We do not discuss this in the text.

302
7 Combinatorial Optimization
to be processed by machine 1 with time 2 Ô¨Årst, then machine 3 with time 2, and
Ô¨Ånally machine 2 with time 12 according to the technological requirements given
by Table 7.9. The solution of the JSSP is to Ô¨Ånd m Hamiltonian directed paths,
each of which is within the vertices with the same machine number to represent the
processing sequence of jobs on one machine. We use a dashed line to represent the
Hamiltonian directed path on machine 1, i.e., job 1 Ô¨Årst, then job 3, then job 4, and
Ô¨Ånally job 2, a dotted line to represent the processing sequence on machine 2, and
a dashed-and-doted line to represent machine 3. These directed edges between the
same machines are called disjunctive arcs.

















































Fig. 7.33 An example directed graph for JSSP
Then from the start point to the end point we have many Hamiltonian directed
paths, including both processing sequences of the jobs and disjunctive arcs of the
machines. We deÔ¨Åne the length of a path as the sum of the processing times pass-
ing through in the path. The longest length in these Hamiltonian directed paths de-
termines the makespan.43 We use heavy lines in Fig. 7.33 to demonstrate it. The
makespan is 2+3+1+12 = 18. This path is called the critical path.
H. Gantt was an American mechanical engineer and management consultant.
In the 1910s, he suggested a type of bar chart to illustrate a project schedule. An
example Gantt chart for the JSSP of Table 7.9 is Fig. 7.34.
The horizontal axis of a Gantt chart is time and the vertical axis represents several
machines with the sequence of operations processed on them.44 For example, on
machine 2, job 2‚Äôs Ô¨Årst operation is processed Ô¨Årst, then job 4‚Äôs second operation
is processed. Job 3‚Äôs second operation is processed until time 5 because its direct
predecessor, o311, is Ô¨Ånished at time 5 by machine 1. Finally job 1‚Äôs last operation
is processed. We can also determine the critical path in Fig. 7.34 by Ô¨Ånding the path
43 Why?
44 Another way a Gantt chart can represent the solution is to use the vertical axis for jobs.

7.4 Job-shop Scheduling Problem
303












	


	
	
	

	
	

			
		
	






















Fig. 7.34 An example Gantt chart for JSSP
from time 0 to tM without idle time,45 provided that we can jump seamlessly among
machines. A heavy dotted line is used to represent this path in Fig. 7.34.
The alert reader might have noticed from Fig. 7.34 that we can actually generate
an inÔ¨Ånite number of feasible solutions for a JSSP because we can add artiÔ¨Åcially
the idle time between every two operations on one machine. This is true. But the
objective of our JSSP is to minimize tM, so we want to eliminate any useless idle
time. If we move an operation rectangle in the Gantt chart to the left and do not
change any operation‚Äôs sequence, this movement is called a local left-shift. If there
exists a local left-shift, we can decrease the makespan by moving this operation to
the left. A solution without a local left-shift is called a semiactive schedule. It is not
difÔ¨Åcult to understand that there is Ô¨Ånite number of semiactive schedules for a JSSP.
If we change the sequence of some operations on a machine so that one operation
is Ô¨Ånished earlier but others are not delayed, this movement is called a global left-
shift. A solution without a global left-shift is called an active schedule.46
If a machine is available, i.e., it is not processing any job, and a job is available,
i.e., its previous operations are Ô¨Ånished, but we do not process this job on this ma-
chine immediately, there is a delay for the job. A solution without a delay is called
a nondelay schedule.
These concepts are clearly illustrated by Fig. 7.35.
There is no local left-shift in Fig. 7.35. So it is semiactive. o222 can be global
left-shifted to start between time [2,3] without delay o122. So it is not active. At
time 2, o222 could be started but we delay it. So it is not nondelay.
It has been proved that the relationship between these solution sets and the pos-
sible optimal solution sets for JSSP can be illustrated by Fig. 7.36. The optimal
solution belongs to the active schedule set but might not belong to the nondelay
schedule set. So we generally want to search the active schedule set.
Sometimes we can expand a JSSP into a parallel environment, i.e., there are c1
identical machines with type 1 in work center 1, ‚ãÖ‚ãÖ‚ãÖ, cm identical machines with
type m in work center m. Every job still needs to be processed by m operations in
45 A machine is idle if it is not processing jobs.
46 To be clear, an active schedule means that for every operation in that schedule there is not enough
idle time on its machine before its current location, or there is enough idle time but it cannot be
scheduled at that time due to the precedence constraint.

304
7 Combinatorial Optimization
Fig. 7.35 A semiactive, not
active, not nondelay partial
schedule




	

	


































5 
	
	
4(
/	
Fig. 7.36 The relationship between semiactive, active, and nondelay schedules and the possible
optimal solution sets for JSSP
m work centers in a predeÔ¨Åned sequence. But it can be processed by any machine
in that work center. This expansion causes a Ô¨Çexible job-shop scheduling problem
(FJSSP).
Another expansion from JSSP assumes that every machine has same capability,
i.e., every machine can process every operation. We still want every job to be pro-
cessed by every machine but without a predeÔ¨Åned sequence. Then the problem is
to determine both the sequence of processing machines for every job and the se-
quence of processed jobs for every machine. This expansion causes an open-shop
scheduling problem (OSSP).
Like the knapsack problem and TSP, there are many benchmark JSSPs. Beasley
published a paper advocating the establishment of an OR-Library [36]. Since then,
he has maintained a Web site containing many of the famous combinatorial op-
timization problems, which includes scheduling, bin packing, knapsack, network
Ô¨Çow, shortest path, TSP, vehicle routing, etc., and their current best solutions.47
In 1993 Taillard proposed a method to randomly generate large scheduling prob-
lems [37].
47 http://people.brunel.ac.uk/Àúmastjjb/jeb/info.html.

7.4 Job-shop Scheduling Problem
305
In the remainder of this section, we will only discuss the basic JSSP using the
example given by Table 7.9.
7.4.2 Heuristic Methods for Job-shop Scheduling
JSSP is a rather famous and widely applicable combinatorial optimization problem.
In addition, its search space is extremely large. So there are many heuristic methods
for JSSP. We will introduce one construction method and one local search method.
7.4.2.1 Construction Methods
From Fig. 7.36 we know that the optimal solution of a JSSP is an active schedule.
Then why not limit our search to the Ô¨Åeld of active schedules? In 1960 GifÔ¨Çer and
Thompson proposed the concept of active schedules and suggested a way to gener-
ate active schedules [38]. Baker and Trietsch clearly restated the algorithm in their
famous textbook published in 2009 [35].
The GifÔ¨Çer‚ÄìThompson algorithm generates an active schedule (or all active
schedules) by selecting one operation at a time. For operation oijk, it is schedu-
lable if operations oi,(l<j),h have all been scheduled. These scheduled operations
are called a partial schedule. We start from an empty set of the partial schedule,
add one schedulable operation one time, until all n √ó m operations are scheduled.
Then the partial schedule becomes the active schedule. We use PS(œÑ) to represent
the partial schedule containing œÑ scheduled operations and SO(œÑ) to represent the
set of schedulable operations at stage œÑ. Because we schedule one operation at one
stage, PS and SO have the same variable œÑ. For every operation oi jk ‚ààSO(œÑ), s j
is used to represent its earliest starting time and f j is used to represent its earliest
completion time. s j is determined by the maximum value of the completion time of
the last scheduled operation on machine k and the completion time of oi,( j‚àí1),h. And
f j = sj + pijk. The GifÔ¨Çer‚ÄìThompson algorithm can be illustrated as follows.
GifÔ¨Çer‚ÄìThompson Algorithm for Generating an Active Schedule
Step 1: œÑ = 0. PS(œÑ) = /0. SO(œÑ) includes all operations without predeces-
sors.
Step 2: Determine f ‚àó= min
j‚ààSO(œÑ)
{
f j
}
and the machine M‚àóby which f ‚àócan
be realized. If more than one machine has f ‚àó, ties are split randomly.
Step 3: For the operations in SO(œÑ) that can be operated by machine M‚àó,
Ô¨Ånd those where sj < f ‚àó. Select one operation, e.g., operation j, from among
them and put in PS(œÑ) according to some rule.

306
7 Combinatorial Optimization
Step 4: Remove operation j from SO(œÑ). œÑ = œÑ + 1. Add operation j‚Äôs
successor to SO(œÑ).
Step 5: Repeat step 2 to step 4 until all the operations have been added to
PS.
In step 2 of the above algorithm, PS contains (œÑ ‚àí1) scheduled operations. Step 3
determines the œÑth operation in PS.
The criterion sj < f ‚àóin step 3 is very critical for generating an active schedule.
We can use a partial mixed Gantt chart to illustrate this situation in Fig. 7.37. Sup-
pose currently there are more than three schedulable operations in SO but f ‚àóand M‚àó
are determined by job 1. The previous Ô¨Ånished operation on M‚àóis illustrated by the
thick rectangle. There are three jobs with schedulable operations on M‚àó. The direct
predecessors of these schedulable operations are represented by solid rectangles and
the schedulable operations are represented by dotted rectangles. The earliest starting
times and completion times are marked.











,P
> 

	

	
	
P

	

> 
> 
Fig. 7.37 The reason why criterion sj < f ‚àóensures active schedules
As can be seen from Fig. 7.37, the operations of jobs 1 and 2 can be scheduled
in PS according to different rules. But if we schedule operation of job 3 in PS, later
we could get a global left-shift operation of job 1 so that the operation of job 1 can
be Ô¨Ånished earlier without affecting other operations. So the operation of job 3 is
forbidden. Criterion s j < f ‚àóis to forbid the possibility of a future global left-shift
so as to ensure that the generated schedule will be active.
We can generate all the active schedules of a JSSP if we add every possible
schedulable operation to PS in step 3 and maintain a tree data structure to record all
the partial schedules. Due to the complexity of JSSPs, even though the number of
active schedules is smaller than that of feasible schedules, we will not proceed in
this way. The branch and bound method will limit the search space in this tree data
structure but is not applicable to moderate-scale JSSPs as well.

7.4 Job-shop Scheduling Problem
307
There are many rules, known as priority dispatching rules, to determine which
schedulable operation is added to PS in step 3. We list some of them as follows
[39].48
‚àôSPT (shortest processing time). Select the operation with the shortest process-
ing time.
‚àôLPT (longest processing time). Select the operation with the longest processing
time.
‚àôMWR (most work remaining). Select the operation for the job with the most
total remaining processing time.
‚àôLWR (least work remaining). Select the operation for the job with the least
total remaining processing time.
‚àôMOR (most operations remaining). Select the operation for the job with the
most remaining operations.
‚àôLOR (least operations remaining). Select the operation for the job with the
least remaining operations.
‚àôFCFS (Ô¨Årst come Ô¨Årst served). Select the Ô¨Årst operation in the machine‚Äôs wait-
ing queue.
‚àôR (random). Select an operation at random.
It was suggested by Baker and Trietsch that the MWR rule often produces a good
makespan [35]. So we just use the MWR rule as an example to illustrate the generat-
ing procedure of JSSPs illustrated by Table 7.9. The procedure can be illustrated by
Table 7.10, where wr stands for the work remaining, i.e., total remaining processing
time of the job. We only illustrate the newly added operation in column ‚ÄúPS(œÑ)‚Äù
for simplicity. The resulting schedule can be illustrated by Fig. 7.38, in which the
critical path is illustrated by dotted lines.











	


	
	
	

	
	

			
		
	























Fig. 7.38 The active schedule generated by the GifÔ¨Çer‚ÄìThompson algorithm with the MWR rule
48 Panwalkar and Iskander summarized these rules in 1977 [40]; the rules were later developed by
Blackstone et al. in 1982 [41] and Dorndorf and Pesch in 1995 [42]. A numerical comparison of
these dispatching rules was carried out by Kim et al. in 2008 [43]. A survey on job-shop scheduling
by local search was presented by Vaessens et al. in 1996 [44].

308
7 Combinatorial Optimization
Table 7.10 The procedure for generating an active schedule using the GifÔ¨Çer‚ÄìThompson algo-
rithm with the MWR rule

-
.&1-3
	
	
P ,
P

(
".1-3



@





@





	

@


	



	

@


	




)


@

)




(

@

&
(







@





@





	

@


	



	

@


	




)




#
(







@





@





	

@


	



	

@


	




)




"
(







@





@





	

@


	



	

@


	










(







@





@





	

@


	



	

@

"
	










(







@





@





	

@


	



	

@

"
	







@




@







(

@

&
(

@


(
 


"


@





@





	

@

#
	



	

@

#
	







@






(

@


(




#


@


@




#
	
 @

"
	

@

#
	
 
#





@






(

@


(
 


&


@


@





	

@

"
	

@


	





#





(
 


)


@





	

@

"
	




#

 


(
 







"
	


"








(










	





"





(
 




The schedule in Fig. 7.38 is nondelay. By comparing it with the schedule in
Fig. 7.34 we can conÔ¨Årm that a nondelay schedule is not necessarily better than a
delay but active schedule.49
If we want to generate nondelay schedules, the only change in the above algo-
rithm is in steps 2 and 3.50
GifÔ¨Çer‚ÄìThompson Algorithm for Generating a Nondelay Schedule
49 The schedule in Fig. 7.34 is not nondelay because o132 could be started at time 4, but we delib-
erately delay it until time 6.
50 Why?

7.4 Job-shop Scheduling Problem
309
Step 2: Determine s‚àó= min
j‚ààSO(œÑ)
{
sj
}
and the machine M‚àóby which s‚àócould
be realized.
Step 3: For the operations in SO(œÑ) that should be operated by ma-
chine M‚àó, Ô¨Ånd those s j = s‚àó. Select one operation, e.g., operation j, from
among them and put in PS(œÑ) according to some rule.
Another famous construction method for JSSP is the shifting bottleneck procedure
suggested by Adams et al. in 1988 [45].
7.4.2.2 Local Search Method
As for local searches, we need to deÔ¨Åne neighbors Ô¨Årst.51 Here we introduce the
deÔ¨Ånition of neighbor according to the critical path [35].
As mentioned above, a critical path is a group of operations in a Gantt chart.
GifÔ¨Çer and Thompson proved that every active schedule has at least one critical
path [38]. The contiguous operations on the critical path is either belong to the same
job and are processed by different machines consecutively or belong to the same
machine and are processed without idle time. Operations in the latter situation form
blocks. For example, o212, o422, o132, and o322 form a block on machine 2 in Fig.
7.38.
The neighbor of the current schedule can be deÔ¨Åned as the result of interchanging
adjacent operations in the block of the critical path. This method is called adjacent
pairwise interchange.
If we interchange operations o212 and o422, the resulting schedule is illustrated
by Fig. 7.39.52














	


	
	
	

	
	

			
		
	
























Fig. 7.39 The Ô¨Årst neighbor of the schedule in Fig. 7.38
If we interchange operations o422 and o132, the resulting schedule is illustrated
by Fig. 7.40.
51 Recall the deÔ¨Ånition of neighbor in the knapsack problem and in the TSP.
52 Other operations might be changed accordingly due to precedence constraints.

310
7 Combinatorial Optimization












	
	
	

	
	

			
		
	
























	

Fig. 7.40 The second neighbor of the schedule in Fig. 7.38
If we interchange operations o132 and o322, the resulting schedule is illustrated
by Fig. 7.41.













	
	
	

	
	

			
		
	
























	

Fig. 7.41 The third neighbor of the schedule in Fig. 7.38
Thus the one-step local search by adjacent pairwise interchange Ô¨Ånds a better
solution, i.e., Fig. 7.41.
Every result of a local search has its critical path, illustrated in Figs. 7.39‚Äì7.41
with dotted lines. So we can continue the local search from them by adjacent pair-
wise interchange. Baker and Trietsch proved that we can Ô¨Ånd the optimal solution
of a JSSP by adjacent pairwise interchange on active schedules [35].
7.4.3 Evolutionary Algorithm Code Schemes for Job-shop
Scheduling
The code scheme for a JSSP can be roughly divided into two categories: direct
approaches and indirect approaches. Every chromosome of a direct approach deter-
mines a schedule itself, but every chromosome of an indirect approach determines
a way to generate a schedule.
We will use the example in Table 7.9 throughout this subsection to demonstrate
the code schemes.

7.4 Job-shop Scheduling Problem
311
7.4.3.1 Direct Approaches
Operation-based Code
Since a JSSP is basically the schedule of operations, it is quite natural to number
the n√óm operations from 1 to n√óm. Every permutation of these n√óm operations
represents a (perhaps infeasible) schedule. This is operation-based code. In 1994
Gen et al. proposed an alternative idea to always generate a feasible schedule [46]
(Fig. 7.42).
Fig. 7.42 A chromosome of
operation-based code


















1! @  ! @  ! @  ! 3
Every gene represents an operation. The loci of the chromosome determine the
sequence of the operations (from left to right) and the alleles of the chromosome are
the job number. For example, according to the precedence constraint of Table 7.9,
from left to right, the Ô¨Årst allele 3 represents operation o311, the second allele 3
represents operation o322, and the third allele 3 represents operation o333. The cor-
responding operation sequence is illustrated in Fig. 7.43.





































Fig. 7.43 Operation sequence of Fig. 7.42











	


	
	
	

	
	

			
		
	







	













Fig. 7.44 Gantt chart of operation-based chromosome in Fig. 7.42
After we get the whole operation sequence, the Gantt chart of the schedule can
be generated as Fig. 7.44.53 In generating the Gantt chart, we start an operation
whenever its predecessor has been Ô¨Ånished and the machine to process it is available
54.
53 Why we could always generate feasible operation sequence?
54 This procedure will generate a nondelay schedule.

312
7 Combinatorial Optimization
Job-based Code
Job-based code for JSSP was suggested by Holsapple et al. in 1993 [47]. It uses n
genes to represent n jobs. The loci of the chromosome determines the sequence of
scheduling jobs and the alleles of the chromosome are the job number. An example
of a job-based chromosome is illustrated in Fig. 7.45.
Fig. 7.45 A chromosome of
job-based code






















In generating the schedule, we Ô¨Årst simply arrange the operations of job 3 in
their earliest possible time, then jobs 1, 2, and 4 sequentially. The procedure can be
illustrated by Fig. 7.46. If there are enough empty time blocks in the early processing
stage, the operations of the later arranged job could be inserted into the empty time
block, such as operations o212, o413, and o422 in Fig. 7.46.
According to the procedure of decoding, we know that job-based code will gen-
erate an active schedule. But it is necessary to mention that not all active solutions
can be represented by job-based code, i.e., the code is not surjective, which might
lose the optimal solution. For example, both of the schedules represented by Figs.
7.33 and 7.41 cannot be represented by job-based code.
Random-key-based Code
We have already introduced the random-key-based code for TSPs in Sect. 7.3.3.3.
Random-key-based code for JSSPs was suggested by Bean in 1994 [48].
We use n √ó m random numbers to represent a schedule. The chromosome is di-
vided into m parts. The Ô¨Årst n genes are with integer part 1, ..., and the last n genes
are with integer part m (Fig. 7.47).
In each part, i.e., each machine, the loci of genes represent the job number. We
rank the alleles of that part in ascending order so as to determine the processing
sequences of jobs on a machine. For example, the Ô¨Årst four random numbers deter-
mine that the processing sequence on machine 1 are jobs 3, 1, 4, and 2. With these
job sequences on the machines, we can generate the scheduling illustrated in Fig.
7.48.
As can be seen from Fig. 7.48, random-key-based code might not guarantee gen-
eration of an active schedule, e.g., o322 can be global left-shifted.
Random-key-based code provides sequences of operations on each machine di-
rectly, which might conÔ¨Çict with precedence constraints, i.e., suffering deadlock. In
1995 Norman and Bean suggested a method to handle the conÔ¨Çict with precedence
constraints [49].

7.4 Job-shop Scheduling Problem
313












	
	
	
			

	



(a)
















	

	
	
	
			
	







	







	

(b)






















	


	
	
	

			
		
	







	









	

(c)






















	


	
	
	

	
	

			
		
	







	











	

(d)
Fig. 7.46 The decoding process of the job-based chromosome in Fig. 7.45: (a) Gantt chart after
scheduling job 3, (b) Gantt chart after scheduling job 1, (c) Gantt chart after scheduling job 2, and
(d) Gantt chart after scheduling job 4

314
7 Combinatorial Optimization


0& 0&" 0)" 0#& 0& 0" 0" 0" 0 0" 0"& 0


           
/
	N
,    @,   @,    

















Fig. 7.47 A chromosome of random-key-based code

















	


	
	
	

	
	

			
		
	







	














Fig. 7.48 Gantt chart of the random-key-based chromosome in Fig. 7.47
7.4.3.2 Indirect Approaches
Preference-list-based Code
In scheduling operations for JSSPs, sometimes the sequence is obvious, e.g., o212
and o413 of Table 7.9 should be processed at the beginning. But sometimes decisions
should be made by schedulers, such as o311 and o111. They all want to use machine
1 at the beginning. If we could provide some preference information for each ma-
chine on different jobs, these conÔ¨Çicts could be solved automatically. Preference-
list-based code for JSSP was suggested by Davis in 1985 [50] and developed by
Falkenauer and Croce in 1991 [51].
In preference-list-based code, we connect m subchromosomes to one chromo-
some. Each subchromosome is used to represent the preference of one machine for
n jobs. This is illustrated by Fig. 7.49. Machine 1 favors job 3 the most, then job
2, then job 1, and dislikes job 4 the most. This is the preference list of machine 1.
These rules are useful for solving conÔ¨Çicts while decoding a schedule.
To generate a schedule, we need to maintain a waiting queue for each machine.
Whenever a machine Ô¨Ånishes its current operation, it checks its waiting queue. If
there is only one operation, the machine processes it. If there is more than one
operation, the machine processes the preferred one according to its preference list.
If no operations are in the waiting queue, it keeps idle until an operation comes.
Figure 7.50 is the schedule generated by preference list in Fig. 7.49.

7.4 Job-shop Scheduling Problem
315















.
'
	'
,
.
'
	'
,
.
'
	'
,














Fig. 7.49 Chromosome of preference-list-based code


















	


	
	
	

	
	

			
		
	







	












	

Fig. 7.50 Gantt chart of the preference-list-based chromosome in Fig. 7.49
The decoding procedure is illustrated by Table 7.11. We record the completion
time for every selected operation so that we can check the waiting queue for the
machine at that time. It basically involves using the precedence constraint in Table
7.9 and solving conÔ¨Çicts using the preference list in Fig. 7.49.
1. At the beginning, operations o111 and o311 are in machine 1‚Äôs waiting queue,
operation o212 is in machine 2‚Äôs waiting queue, and operation o413 is in machine
3‚Äôs waiting queue. Machine 1 selects o311 due to its preference list.
2. Then at time 2, o422 is the only selection for machine 2.55
3. At time 3, o311 is Ô¨Ånished. The waiting queue for machine 1 contains o111 and
o221. Machine 1 selects o221 according to its preference list.
4. At time 4, o422 is Ô¨Ånished. o322 is the only operation waiting for machine 2.
5. At time 5, o322 is Ô¨Ånished. No operation is waiting for machine 2. But operation
o333 is now the only one waiting for machine 3.
6. At time 6, o221 is Ô¨Ånished. o111 and o431 are in machine 1‚Äôs waiting queue. Ma-
chine 1 picks o111 according to its preference rule.
7. At time 8, o111 is Ô¨Ånished. o431 is the only operation waiting for machine 1.
8. At time 10, o333 is Ô¨Ånished. o123 and o233 are in machine 3‚Äôs waiting queue.
Machine 3 picks o233 according to its preference rule.
9. At time 13, o233 is Ô¨Ånished. o123 is the only operation waiting for machine 2.
10. At time 15, o123 is Ô¨Ånished, which makes o132 wait for machine 2.
According to the above procedure, operations will be processed whenever it is
available. So preference-list-based code only generates nondelay schedules. In 1995
Croce et al. suggested a look-ahead simulation method to generate active schedules
by preference-list-based code [52].
55 o311 has not Ô¨Ånished, so o322 is not in machine 2‚Äôs waiting queue.

316
7 Combinatorial Optimization
Table 7.11 Decoding procedure of the chromosome in Fig. 7.49






D	%N
/
		 
4 
13	
	O		


, 

, 

, 

, 

, 

, 
)


@











13


13


13









13




@







13











13











1)3



@







1#3


#







13


)




@







13










13









1"3




Priority-rule-based Code
In Sect. 7.4.2.1 we introduced how to generate an active schedule using the GifÔ¨Çer‚Äì
Thompson algorithm. There we only used one priority dispatching rule, i.e., MWR,
as an example. It is clear that every priority dispatching rule can generate one ac-
tive schedule using the GifÔ¨Çer‚ÄìThompson algorithm. If we use different priority
dispatching rules in different stages, i.e., œÑ, of the GifÔ¨Çer‚ÄìThompson algorithm, dif-
ferent schedules might be generated. That is the idea behind priority-rule-based
code, which was suggested by Dorndorf and Pesch in 1995 [42].
The chromosome has n√óm genes. The loci of genes represent the stage number.
From left to right, the alleles of œÑ‚Äôs gene represent the rule number in stage œÑ of the
GifÔ¨Çer‚ÄìThompson algorithm.
Here we just use an example with four priority dispatching rules: 1 for SPT
(shortest processing time), 2 for LPT (longest processing time), 3 for MWR (most
work remaining), and 4 for LWR (least work remaining). One chromosome is il-
lustrated in Fig. 7.51. The decoding procedure can be illustrated by Table 7.12, in
which the ‚Äúrule‚Äù column represents the priority dispatching rule determined by the
corresponding gene and the ‚Äúcompare‚Äù column is the comparison basics according
to various rules. While œÑ = 2 and œÑ = 3, there are situations where sj = f ‚àó. These
operations cannot be scheduled.56
The Gantt chart of the schedule can be illustrated by Fig. 7.52.
Because most of the above code schemes are permutations or integers, we do
not discuss their variation operators. But the selection of variation operators for
TSPs and JSSPs might be different because the locus of chromosomes in JSSPs has
speciÔ¨Åc meanings but not in TSP. We introduce a lot of consideration in maintaining
56 Why?

7.4 Job-shop Scheduling Problem
317













1.8@9.8@D2@9D23











Fig. 7.51 Chromosome of priority-rule-based code
Table 7.12 Decoding procedure of the chromosome in Fig. 7.51

-
.&1-3
	
	
P
,
P

2

 ".1-3



@





@





	

@


	



	

@


	




)


@

)



.8




@










@





@





	

@


	



	

@


	







@

)



9D2
C





@





@





	

@


	



	

@


	




)


@





9.8
C





@





@





	

@


	



	

@


	








.8
C





@





@





	

@


	



	

@

"
	








D2
C





@





@





	

@


	



	

@

"
	







@




@





9D2


(

@

&
(

@


(
 


"


@








	

@

)
	


)
	


)


"


@

"



9.8



 @



 


#


@








	

@


	



	








@

)



.8




@



 


&


@


@ 





	

@


	

@


	





)



D2
C


)


@


@ 





	

@


	

@


	








@





9.8




@










@





	

@

&
	

&






9D2
C








	









.8
C









	
	
	

	
	

			
		
	
















	





	


	



Fig. 7.52 Gantt chart of the priority-rule-based chromosome in Fig. 7.51

318
7 Combinatorial Optimization
the connection of two successive genes. In JSSP, preserving the absolute location of
genes in parents is also valuable for inheriting the merits of parents.
7.5 Summary
Combinatorial optimization has strong ties with economics and industry, so it has
been intensively studied by mathematicians, economists, and industrial engineer-
ing researchers since the 1960s. Its solution space is generally extra large, which
makes it intractable for exact algorithms, such as the ‚Äúcrude‚Äù enumeration method.57
Heuristic algorithms are the only option before EAs.
In the beginning of applying EAs in combinatorial optimization, researchers
tended to consider that EAs might be a substitution of heuristic algorithms. But
comprehensive research reveals the fact that EAs should be considered complemen-
tary rather than competitive [16, 30, 53]. So MAs and hyper-heuristics, introduced
in Sects. 3.7.2 and 3.7.3 respectively, are of great importance for combinatorial op-
timization.
In the three main sections for the knapsack problem, TSP, and JSSP, we can
summarize the following features of EAs for combinatorial optimization.
‚àôThe code scheme for combinatorial optimization is Ô¨Çexible due to its hard con-
straints. The Ô¨Årst priority of designing a successful EA for combinatorial op-
timization is to suggest an effective code scheme that limits the search space
without losing the optimal solution and is easy to decode.
‚àôOwing to the Ô¨Çexible code schemes, the variation operators are code-dependent.
Good variation operators should (1) maintain the feasibility of offspring, (2)
maintain the successful information of parents, and (3) explore the solution space
effectively.
‚àôHeuristic algorithms are very important for good EAs, or MAs. Construction
algorithms can be used to generate the initial population and a local search algo-
rithm can be used to improve the offspring. The advantage of MAs is obviously
the fast convergence speed but it requires more involved decoding procedure and
objective evaluation in one generation and thus might weaken the search ability
of EAs when the total objective function evaluation number is the stop criterion.
So the balance between genetic search and local search is critical for effective
search [54].
‚àôProvided the application of local search methods, the variation operators need to
put more focus on the exploration because exploitation could be implemented by
a local search method.
After reading this chapter, you should understand the difÔ¨Åculty of combinatorial
optimization and the position of EAs in combinatorial optimization, know some
57 Other techniques, such as cutting plane, branch and bound, dynamic programming, etc., could
eliminate the search space and guarantee Ô¨Ånding the optimal solution. But they also suffer from
‚Äúcombinatorial explosion.‚Äù

7.5 Summary
319
code schemes for different problems, be acquainted with variation operators of
permutation codes, and understand the idea and implementation of some heuristic
methods for different problems.
In all, designing an EA for combinatorial optimization is the art of tradeoff be-
tween exploitation caused by local search methods and exploration caused by ge-
netic operators.
Suggestions for Further Reading
Surveys on the application of metaheuristics in combinatorial optimization are given
by Blum and Roli [55] and Gendreau and Potvin [56]. In 1999 Cal¬¥egari et al. sug-
gested a taxonomy of evolutionary algorithms in combinatorial optimization [57].
Bianchi et al. published a survey on metaheuristics for stochastic combinatorial op-
timization58 in 2009 [58].
There are so many combinatorial optimization problems with distinct character-
istics. Many real-world problems can be formulated as examples. We only introduce
several examples in this chapter. Interested readers are referred to the textbook on
combinatorial optimization [59].
There are several monographs on the three problems introduced in this chapter,
such as Kellerer et al.‚Äôs book on knapsack problems in 2004 [60], Applegate et
al.‚Äôs, Gutin and Punnen‚Äôs, and Lawler et al.‚Äôs books on TSP in 2007, 2007, and
1985, respectively [61‚Äì63], and Pinedo‚Äôs, Baker and Trietsch‚Äôs, and Chakraborty‚Äôs
books on scheduling and job-shop scheduling in 2008, 2009, and 2009, respectively
[2, 35, 64].
The topic of selecting proper memes, i.e., local search methods, in knapsack prob-
lems is critical for designing a successful algorithm. See two papers published in
2008 and 2009 [5, 8]. Laporte gave an intensive survey on the approximate algo-
rithms for TSP in 1992 [65]. In 2009 Hasan published a paper to discuss in detail
the procedure of MAs for JSSP [66]. In 2007 Tang et al. suggested a parallel MA
including a diversity-based static/dynamic adaptive strategy to solve the large-scale
NP-hard combinatorial problem, e.g., quadratic assignment problems (QAPs) [67].
The multiple traveling salesman problem (MTSP) seeks a partition of n cities
into m salesmen and arranges the tour for each salesman so that (1) the total distance
traveled by all salesmen is minimized and (2) the maximum distance traveled by any
salesman is minimized to balance the load. MTSP has many real-world applications.
Bektas gave a survey in 2006 [68] and Singh and Baghel suggested an improvement
in 2009 [69].
In 1999 LarraÀúnaga et al. published a comprehensive survey on the representations
and variation operators for TSP [70] and Michalewicz and Fogel gave an interest-
ing introduction on this topic in 2004 [71]. Poon and Carter‚Äôs paper on crossover
58 Stochastic combinatorial optimization means that part of the information about the problem data
is only available in the probability distribution form.

320
7 Combinatorial Optimization
operations might give the reader some fresh ideas [72]. In 2006 Raidl et al. sug-
gested a biased edge exchange mutation operator with a preference on short distance
roads [73].
JSSP might be the most intensively studied scheduling model in academia. Thus
there are many reviews and comparisons on this topic. An early survey was provided
by Blackstone et al. in 1982 [41]. In 1998 Jain and Meeran gave a comprehensive
survey on job-shop scheduling techniques [53]. Gen and Cheng gave clear and com-
prehensive introductions in their books published in 1997 and 2008 [39, 74].
Evolutionary scheduling was reviewed by Hart et al. in 2005 [75]. Two earlier
reviews speciÔ¨Åcally focused on representations and hybrids of EAs for job shop
were published by Cheng et al. in 1996 and 1999, respectively [76, 77]. In 2000
V¬¥azquez and Whitley compared several GAs and tabu search for static and dynamic
job-shop scheduling in 2000, respectively [78, 79].
Optimization in dynamic environments always attracts the interest of researchers.
In 2003 Jensen discussed the situation of rescheduling due to failures, e.g., machine
breakdowns, sickness of employees, deliveries getting delayed, etc., to Ô¨Ånd robust
and Ô¨Çexible schedules with a low makespan [80].
The must-read papers of this chapter are [81] for constructing effective MAs,
[3] for an intensive survey on knapsack EAs, [70] for code schemes and variation
operators for TSPs, and [76] for code schemes for JSSPs. If readers want to design
effective MAs for TSPs and JSSPs, two old papers are highly recommended: [15,
38].
Exercises and Potential Research Projects
7.1. Search the SCI for the number of papers on knapsack problem, TSP, and JSSP
to see how hot they are.
7.2. Prove that in 3-opt with one set of three edges which do not share the cities,
seven new solutions can be generated, in which three solutions are actually 2-opt
results.
7.3. What is the corresponding genetic variation operators in permutation code to
3-opt?
7.4. Could you provide another proof for Theorem 7.1 using path code?
7.5. Prove that an n-vertices complete graph has n(n‚àí1)
2
edges.
7.6. Give an example to illustrate that the heritability of ordinal code is not good.
7.7. Generate offspring 2 in Figs. 7.20‚Äì7.22.
7.8. Could MTSP, introduced on page 319, belong to grouping problems? Why?

References
321
7.9. What is the solution size of n = m = 15 JSSP? How long would it take to solve
this problem on your computer using the enumeration method?
7.10. Why does the longest length in Hamiltonian directed paths determine the
makespan in Fig. 7.33?
7.11. Is Fig. 7.34 a semiactive, active, or nondelay schedule? Why?
7.12. Draw a Gantt chart whose vertical axis is for jobs according to Fig. 7.34.
7.13. How can the modiÔ¨Åcation on page 308 generate a nondelay schedule?
7.14. Generate a nondelay schedule using the GifÔ¨Çer‚ÄìThompson algorithm for the
problem in Table 7.9.
7.15. Implement at least one code for a knapsack problem, design your variation op-
erators, and use the benchmark problem in Appendix and the techniques introduced
in Sect. 3.6 to draw statistical conclusions according to your numerical experiments.
7.16. Use path code for a TSP, implement at least two crossover operators and two
mutation operators, use the benchmark problem in Appendix and the techniques
introduced in Sect. 3.6 to do a fair comparison, and draw statistical conclusions
according to your numerical experiments.
7.17. Use the implementation of the above problem, combine it with one heuristic
method for TSPs, use the benchmark problem in Appendix and the techniques in-
troduced in Sect. 3.6 to do a fair comparison about your MA and the original EA,
and draw statistical conclusions according to your numerical experiments.
7.18. Implement at least one code for JSSPs, design your variation operators, and
use the benchmark problem in Appendix and the techniques introduced in Sect. 3.6
to draw statistical conclusions according to your numerical experiments.
7.19. Use the implementation of the above problem, combine it with one heuristic
method for JSSPs, use the benchmark problem in Appendix and the techniques in-
troduced in Sect. 3.6 to do a fair comparison about your MA and the original EA,
and draw statistical conclusions according to your numerical experiments.
References
1. Falkenauer E (1998) Genetic algorithms and grouping problems. Wiley, New York
2. Pinedo ML (2008) Scheduling: theory, algorithms, and systems, 3rd edn. Springer, Berlin
Heidelberg New York
3. Raidl GR, Gottlieb J (2005) Empirical analysis of locality, heritability and heuristic bias in
evolutionary algorithms: a case study for the multidimensional knapsack problem. Evol Com-
put 13(4):441‚Äì475
4. Garey MR, Johnson DS (1979) Computers and intractability: a guide to the theory of NP-
completeness. Freeman, San Francisco

322
7 Combinatorial Optimization
5. ¬®Ozcan E, Bas¬∏aran C (2009) A case study of memetic algorithms for constraint optimization.
Soft Comput 13(8):871‚Äì882
6. Zitzler E, Thiele L (1999) Multiobjective evolutionary algorithms: a comparative case study
and the strength pareto approach. IEEE Trans Evol Comput 3(4):257‚Äì271
7. Jaszkiewicz A (2002) On the performance of multiple-objective genetic local search on the
0/1 knapsack problem - a comparative experiment. IEEE Trans Evol Comput 6(4):402‚Äì412
8. ZinÔ¨Çou A, Gagn¬¥e C, Gravel M et al (2008) Pareto memetic algorithm for multiple objective
optimization with an industrial application. J Heurist 14(4):313‚Äì333
9. Gordon VS, Whitley LD (1993) Serial and parallel genetic algorithms as function optimizers.
In: Proceedings of the international conference on genetic algorithms, pp 177‚Äì183
10. Olsen A (1994) Penalty functions and the knapsack problem. In: Proceedings of the IEEE
genetic and evolutionary computation conference, pp 554‚Äì558
11. Hinterding R (1994) Mapping, order-independent genes and the knapsack problem. In: Pro-
ceedings of the IEEE genetic and evolutionary computation conference, pp 13‚Äì17.
12. Hadley G (1964) Nonlinear and dynamic programming,. Addison-Wesley, Boston, MA
13. Croes GA (1958) A method for solving traveling-salesman problems. Oper Res 6(6):791‚Äì812
14. Lin S (1965) Computer solutions of the traveling salesman problem. Bell Syst Tech J 44:2245‚Äì
2269
15. Lin S, Kernighan B (1973) An effective heuristic algorithm for the traveling-salesman prob-
lem. Oper Res 21(2):498‚Äì516
16. Johnson D, Mcgeoch L (1997) The traveling salesman problem: a case study in local opti-
mization. In: Aarts EHL and Lenstra JK (eds) Local search in combinatorial optimization
Wiley, New York, pp 215‚Äì310
17. Ronald S, Asenstorfer J, Vincent M (1995) Representational redundancy in evolutionary al-
gorithms. In: Proceedings of the IEEE genetic and evolutionary computation conference, pp
631‚Äì636
18. Grefenstette JJ, Gopal R, Rosmaita BJ et al (1985) Genetic algorithms for the traveling sales-
man problem. In: Proceedings of the 1st international conference on genetic algorithms, pp
160‚Äì168
19. Fox B, McMahon M (1991) Genetic operators for sequencing problems. In: Proceedings of
international workshop on foundations of genetic algorithms, pp 284‚Äì300
20. Seniw D (1991) A genetic algorithm for the traveling salesman problem. Master thesis, Uni-
versity of North Carolina at Charlotte
21. Banzhaf W (1990) The ‚Äúmolecular‚Äù traveling salesman. Biol Cybern 64(1):7‚Äì14
22. Michalewicz Z (1998) Genetic algorithms + data structures = evolution programs. Springer,
Berlin Heidelberg New York
23. Fogel D (1990) A parallel processing approach to amultiple traveling salesman problem using
evolutionary programming. In: Proceedings on the fourth annual parallel processing sympo-
sium, pp 318‚Äì326
24. Goldberg D, Lingle K (1985) Alleles, loci and the TSP. In: Proceedings of the Ô¨Årst interna-
tional conference on genetic algorithms and their applications, pp 154‚Äì159
25. Davis L (1991) Handbook of genetic algorithms. Van Nostrand Reinhold Company,New York
26. Syswerda G (1991) Schedule optimization using genetic algorithms. In: Davis L (ed) Hand-
book of genetic algorithms. Van Nostrand Reinhold Company, New York, pp 332‚Äì349
27. Oliver IM, Smith DJ, Holland JRC (1987) A study of permutation crossover operators on the
traveling salesman problem. In: Proceedings of the second international conference on genetic
algorithms and their application, pp 224‚Äì230
28. Whitley D, Starkweather T, Fuquay D (1989) Scheduling problems and traveling salesman: the
genetic edge recombination. In: Proceedings of the third international conference on genetic
algorithms, pp 133‚Äì140
29. Eiben AE, Smith JE (2003) Introduction to evolutionary computing. Springer, Berlin Heidel-
berg New York
30. Johnson D (1987) More approaches to the travelling salesman guide. Nature 330(6148):525

References
323
31. Nagata Y, Kobayashi S (1997) Edge assembly crossover: A high-power genetic algorithm for
the traveling salesman problem. In: Proceedings of the 7th international conference on genetic
algorithms, pp 450‚Äì457
32. Boese KD (1995) Cost versus distance in the traveling salesman problem. Tech. rep. TR-
950018, UCLA
33. Freisleben B, Merz P (1996) New genetic local search operators for the traveling salesman
problem. In: In: Proceedings of the international conference on parallel problem solving from
nature, pp 890‚Äì899
34. Jung S, Moon B (2002) Toward minimal restriction of genetic encoding and crossovers for the
two-dimensional euclidean TSP. IEEE Trans Evol Comput 6(6):557‚Äì565
35. Baker KR, Trietsch D (2009) Principles of sequencing and scheduling. Wiley, New York
36. Beasley J (1990) OR-Library: distributing test problems by electronic mail. J Oper Res Soc
41(11):1069‚Äì1072
37. Taillard E (1993) Benchmarks for basic scheduling problems. Eur J Oper Res 64(2):278‚Äì285
38. GifÔ¨Çer B, Thompson GL (1960) Algorithms for solving production-scheduling problems.
Oper Re 8(4):487‚Äì503
39. Gen M, Cheng R (1997) Genetic algorithms and engineering design. Wiley-Interscience, New
York
40. Panwalkar SS, Iskander W (1977) A survey of scheduling rules. Oper Res 25(1):45‚Äì61
41. Blackstone JH, Phillips DT, Hogg GL (1982) A state-of-the-art survey of dispatching rules for
manufacturing job shop operations. Inter J Product Res 20(1):27‚Äì45
42. Dorndorf U, Pesch E (1995) Evolution based learning in a job shop scheduling environment.
Comput Oper Res 22(1):25‚Äì40
43. Kim I, Watada J, Shigaki I (2008) A comparison of dispatching rules and genetic algorithms
for job shop schedules of standard hydraulic cylinders. Soft Comput 12(2):121‚Äì128
44. Vaessens RJM, Aarts EH, Lenstra JK (1996) Job shop scheduling by local search. INFORMS
J Comput 8:302‚Äì317
45. Adams J, Balas E, Zawack D (1988) The shifting bottleneck procedure for job shop schedul-
ing. Manage Sci 34(3):391‚Äì401
46. Gen M, Tsujimura Y, Kubota E (1994) Solving job-shop scheduling problem using genetic
algorithms. In: Proceedings of the 16th intertational conference on computer and industrial
engineering, pp 576‚Äì579
47. Holsapple C, Jacob V, Pakath R et al (1993) A genetics-based hybrid scheduler for generating
static schedules in Ô¨Çexible manufacturing contexts. IEEE Trans Syst Man Cybern 23(4):953‚Äì
972
48. Bean JC (1994) Genetic algorithms and random keys for sequencing and optimization. ORSA
J Comput 6(2):154‚Äì160
49. Norman B, Bean J (1995) Random keys genetic algorithm for job-shop scheduling. Tech. rep.,
University of Michigan
50. Davis L (1985) Job shop scheduling with genetic algorithms. In: Proceedings of the 1st inter-
national conference on genetic algorithms, pp 136‚Äì140
51. Falkenauer E, Bouffouix S (1991) A genetic algorithm for job shop. In: Proceedings of the
IEEE international conference on robotics and automation, pp824‚Äì829
52. Croce FD, Tadei R, Volta G (1995) A genetic algorithm for the job shop problem. Comput
Oper Res 22(1):15‚Äì24
53. Jain AS, Meeran S (1998) A state-of-the-art review of job-shop scheduling techniques. Tech.
rep., University of Dundee
54. Ishibuchi H, Yoshida T, Murata T (2003) Balance between genetic search and local search in
memetic algorithms for multiobjective permutation Ô¨Çowshop scheduling. IEEE Trans Evol
Comput 7(2):204‚Äì223
55. Blum C, Roli A (2003) Metaheuristics in combinatorial optimization: Overview and concep-
tual comparison. ACM Comput Surv 35(3):268‚Äì308
56. Gendreau M, Potvin J (2005) Metaheuristics in combinatorial optimization. Ann Oper Res
140(1):189‚Äì213

324
7 Combinatorial Optimization
57. Cal¬¥egari P, Coray G, Hertz A et al (1999) A taxonomy of evolutionary algorithms in combi-
natorial optimization. J Heurist 5(2):145‚Äì158
58. Bianchi L, Dorigo M, Gambardella L et al (2009) A survey on metaheuristics for stochastic
combinatorial optimization. Nat Comput 8(2):239‚Äì287
59. Papadimitriou CH, Steiglitz K (1998) Combinatorial optimization: algorithms and complexity.
Dover, New York
60. Kellerer H, Pferschy U, Pisinger D (2004) Knapsack problems. Springer, Berlin Heidelberg
New York
61. Applegate DL, Bixby RE, Chvatal V et al (2007) The traveling salesman problem: a compu-
tational study. Princeton University Press, Princeton, NJ
62. Gutin G, Punnen AP (2007) The traveling salesman problem and its variations. Springer,
Berlin Heidelberg New York
63. Lawler EL, Lenstra JK, Kan AHGR et al (1985) The traveling salesman problem: a guided
tour of combinatorial optimization. Wiley, New York
64. Chakraborty UK (2009) Computational intelligence in Ô¨Çow shop and hob shop scheduling.
Springer, Berlin Heidelberg New York
65. Laporte G (1992) The traveling salesman problem: an overview of exact and approximate
algorithms. Eur J Oper Res 59(2):231‚Äì247
66. Hasan S, Sarker R, Essam D et al (2009) Memetic algorithms for solving job-shop scheduling
problems. Memet Comput 1(1):69‚Äì83
67. Tang J, Lim MH, Ong YS (2007) Diversity-adaptive parallel memetic algorithm for solving
large scale combinatorial optimization problems. Soft Comput 11(9):873‚Äì888
68. Bektas T (2006) The multiple traveling salesman problem: an overview of formulations and
solution procedures. Omega 34(3):209‚Äì219
69. Singh A, Baghel A (2009) A new grouping genetic algorithm approach to the multiple travel-
ing salesperson problem. Soft Comput 13(1):95‚Äì101
70. Larranaga P, Kuijpers CMH, Murga RH et al (1999) Genetic algorithms for the travelling
salesman problem: A review of representations and operators. Artif Intell Rev 13:129‚Äì170
71. Michalewicz Z, Fogel DB (2004) How to solve it: modern heuristics. Springer, Berlin Heidel-
berg New York
72. Poon PW, Carter JN (1995) Genetic algorithm crossover operators for ordering applications.
Comput Oper Res 22(1):135‚Äì147
73. Raidl G, Koller G, Julstrom B (2006) Biased mutation operators for subgraph-selection prob-
lems. IEEE Trans Evol Comput 10(2):145‚Äì156
74. Gen M, Cheng R, Lin L (2008) Network models and optimization: multiobjective genetic
algorithm approach. Springer, Berlin Heidelberg New York
75. Hart E, Ross P, Corne D (2005) Evolutionary scheduling: a review. Genet Programm Evolv-
able Mach 6(2):191‚Äì220
76. Cheng R, Gen M, Tsujimura Y (1996) A tutorial survey of job-shop scheduling problems
using genetic algorithms. I: representation. Comput Ind Eng 30(4):983‚Äì997
77. Cheng R, Gen M, Tsujimura Y (1999) A tutorial survey of job-shop scheduling problems
using genetic algorithms. II: hybrid genetic search strategies. Comput Ind Eng 37(1-2):51‚Äì55
78. V¬¥azquez M, Whitley LD (2000) A comparison of genetic algorithms for the static job shop
scheduling problem. In: Proceedings of the international conference on parallel problem solv-
ing from nature, pp 303‚Äì312
79. V¬¥azquez M, Whitley LD (2000) A comparison of genetic algorithms for the dynamic job shop
scheduling problem. In: Proceedings of the genetic and evolutionary computation conference,
pp 1011‚Äì1018
80. Jensen M (2003) Generating robust and Ô¨Çexible job shop schedules using genetic algorithms.
IEEE Trans Evol Comput 7(3):275‚Äì288
81. Krasnogor N, Smith J (2005) A tutorial for competent memetic algorithms: model, taxonomy,
and design issues. IEEE Trans Evol Comput 9(5):474‚Äì488

Part III
Brief Introduction to Other Evolutionary
Algorithms


Chapter 8
Swarm Intelligence
Abstract We look at the natural selection process as a learning or optimizing pro-
cess and apply the survival of the Ô¨Åttest principle to designing the learning and
optimizing algorithm. Then many EAs, e.g., GA, ES, EP, DE, etc., are suggested
accordingly. There are other similar phenomena in nature. A swarm of ‚Äúlow-level‚Äù
(not smart) insects sometimes surprises us with their amazing behaviors, such as
foraging for food efÔ¨Åciently and constructing exquisite nests. We can also look at
the process of foraging for food and constructing nest as a learning or optimizing
process and learn to design corresponding algorithms. This swarm-level smart be-
havior generated by an agent-level, not smart property could enlighten us to suggest
more robust algorithms for more complex problems in an uncertain environment.
8.1 Introduction
Swarm means a large group. It is generally used to describe social insects or social
animals, e.g., ant colonies, bee colonies, Ô¨Åsh schools, and bird Ô¨Çocks. We are often
fascinated by the masterpiece of these ‚Äúnaive‚Äù small lives, such as sophisticated
nests and highly efÔ¨Åcient foraging behaviors. If you have ever considered how a
swarm of low-level creatures could display high-level performance, you might have
arrived at the following properties.
1. Every low-level creature can be regarded as an unsophisticated agent who acts
according to simple rules.
2. The swarm fulÔ¨Åls tasks without central control.
3. Unsophisticated agents can be self-organized for coordinated/collective behav-
iors, through which they display complex problem solving skills. We say that
intelligence emerges from these collective behaviors.
4. A collective behavior is implemented by local/indirect communications/ interac-
tions between agents. These indirect interactions might be the local changes of
the environment, which will affect the behaviors of the other creatures later.
327

328
8 Swarm Intelligence
The most spectacular part of swarm behaviors are that, through collective be-
havior, they represent 1 + 1 > 2 effects. We can make these swarms a metaphor
for implementing our learning and optimizing problems, i.e., we mimic the above
properties to some extent. The artiÔ¨Åcial system that embodies some of the above
properties of social creatures to solve learning and optimizing problems is called
swarm intelligence (SI). SI is not an algorithm or system but a category of algo-
rithms whose most famous examples are ant colony optimization (ACO) and parti-
cle swarm optimization (PSO).1
Compared with other EAs, SI‚Äôs advantage lies in the fact that the intelligence-
emergence process is self-organized, which means that SI is more suitable for prob-
lems in noisy and dynamic environments and for Ô¨Ånding robust solutions.
Since their birth, ACO and PSO have attracted more and more focus and become
the hottest algorithms in the EA Ô¨Åeld. Figure 8.1 illustrates the number of papers
indexed by the SCI on ACO and PSO 2.































           
<HDU
1XPEHURISDSHUV
$&2
362
Fig. 8.1 Number of papers indexed by SCI on ACO and PSO
1 They are the topics of Sects. 8.2 and 8.3, respectively.
2 TS = (‚Äúant colony optimization‚Äù) and TS = (‚Äúparticle swarm optimization‚Äù OR ‚Äúswarm intelli-
gence‚Äù). The SCI index ‚ÄúTS‚Äù is for the search topic in the title, the keywords, and the abstract.

8.2 Ant Colony Optimization
329
8.2 Ant Colony Optimization
8.2.1 Rationale Behind Ant Colony Optimization
The foraging behavior of ant colonies has been intensively studied and the bion-
ics corresponding to this behavior Ô¨Ånds various applications. The indirect com-
munication between ants is accomplished through pheromones. A pheromone is a
chemical substance that triggers a natural response in another member of the same
species. There are many types of pheromones deposited by ants, e.g., aggregation
pheromones, alarm pheromones, trail pheromones, etc. Here we only care about
trail pheromones, which are deposited by ants with food as they return to their nest;
they attract other ants and get them to go along the pheromone trail to Ô¨Ånd food.
We can illustrate the simpliÔ¨Åed foraging behavior with the help of pheromones.
Suppose there are only two ants with the same ability (speed and pheromone-
depositing ability) and they have both found food at the same time. On their way
to their nest, they deposit pheromones, which is illustrated by the dotted lines in
Fig. 8.2a. Figure 8.2a is a snapshot of the time when these two ants are at the inter-
section of the two paths. The left one is shorter than the right one. This is the Ô¨Årst
time they need to make decisions. Because there is no any information about which
path is shorter, they take one path with 50% probability. Suppose the triangle ant
takes the shorter path and the square ant takes the longer path. So the triangle ant
reaches the nest faster. After returning to its nest, the triangle ant selects the path ac-
cording to the concentration of pheromones. Figure 8.2b is a snapshot of the triangle
ant‚Äôs second selection. We suppose at that time that the square ant is still on its way
back home. So the triangle ant takes the shorter path with 100% probability. Figure
8.2c illustrates the situation where the triangle ant is taking the third selection with
food and the square ant is taking the second selection without food. They take the
paths with 50% probability because both paths have one pheromone trail. But as
time elapses, more pheromones will be deposited along the shorter path because the
ants taking this path will reach their nest faster. This situation will further stimulate
more ants to take this path. Figure 8.2d is such a situation. Both the triangle ant and
the square ant will take the shorter path with high probability.
In the above simple example, every ant, i.e., agent, only has two properties. The
Ô¨Årst one is to deposit a pheromone trail on the way back to the nest and the second
one is to select the path containing a high pheromone concentration with high prob-
ability. These simple properties, together with the fact that the trail pheromone is an
evaporable chemical substance, cause the ant colonies to emerge the intelligence for
Ô¨Ånding the shortest path.
In order to mimic the ants‚Äô foraging behavior, we need to consider two issues
before designing an algorithm.
‚àôThe probability model for selecting different options, i.e., the way to form a
solution.
‚àôThe way by which ants deposit pheromones and pheromones evaporate, i.e., the
way to modify the solution-forming method.

330
8 Swarm Intelligence








5
4	
(a)










5
4	
(b)










5
4	
(c)









5
4	
(d)
Fig. 8.2 SimpliÔ¨Åed example of ant foraging behavior: (a) Ô¨Årst selection, (b) second selection, (c)
third selection, and (d) latter selection
In the following section, we will focus on these two issues to introduce our algo-
rithm.
8.2.2 Discrete Ant Colony Optimization
The emerging shortest path Ô¨Ånding ability in ant colonies can be used directly in TSP
(Sect. 7.3). We will take TSP as the example problem in most of this subsection and
discuss JSSP, which is illustrated in Sect. 7.4, at the end.
We suppose that there are m ants to solve the TSP with n cities. At Ô¨Årst, ants are
located at different cities randomly. Every ant has n ‚àí1 options at Ô¨Årst for which
city to goto in the next step. They use the pheromone concentrations in the edge
connecting their current city and the n ‚àí1 cities to make selections probabilisti-
cally, i.e., transition probability. Then all the ants move to their second city at the
same pace. Now they probabilistically select one feasible city, among n ‚àí2 cities,
according to the pheromone concentrations. This procedure continues until all m
ants have visited n cities. Then they need to go back to their Ô¨Årst city to form the
solution of the TSP.
That is to say, we divide the process of forming the solution for the TSP with n
cities into n time steps, with each ant going to a feasible city in each time step. This
is different from the aforementioned example, where ants travel at the same speed.
The different path length of account for the different pheromone concentrations.
In the discretized model of TSP, all m ants Ô¨Ånish their Hamiltonian cycles at the
same time. So we need to modify the pheromone concentration artiÔ¨Åcially after the
ants return to their Ô¨Årst cities according to the length of their Hamiltonian cycles to
mimic the effect of the aforementioned example.
The general solution process of ACO for TSP could be illustrated as follows.

8.2 Ant Colony Optimization
331
ACO for TSP
Phase 1: Initialization. Assign the parameters for ACO, such as ant num-
ber m, initial pheromone concentration œÑ(0) for every link in the graph, local
information importance factor Œ≤, evaporation factor œÅ, stop criteria (such as
maxgen and stagnation ), etc. gen = 0. Stagnation is the condition that all m
ants generate the same Hamiltonian cycle. It‚Äôs the same meaning of the con-
vergence in GA.
Phase 2: Main loop. Repeat the following steps until the stop criteria are
satisÔ¨Åed.
Step 2.1: Generate a Hamiltonian cycle for each ant.
Substep 2.1.1: Place each ant in a randomly selected initial city.
Substep 2.1.2: For each ant, select the next feasible city to goto
according to the transition probabilities in the links connecting the current
city and the next feasible cities. After the movement, update the feasible city
set. Whenever an ant goes through a link, it might modify the pheromone
concentration of that link (optional).
Substep 2.1.3: Repeat substep 2.1.2 until all ants have visited all
n cities. Then let them go back to their respective initial cities. Thus form the
m Hamiltonian cycles and compute the lengths of the m Hamiltonian cycles.
Determine the best iteration solution and update the best solution so far. The
best iteration solution is the Hamiltonian cycle with the shortest length in the
current generation and the best solution so far is the Hamiltonian cycle with
the shortest length since the beginning of this run.
Step 2.2: Update the pheromone concentrations of the links according
to the performance of these m Hamiltonian cycles. gen = gen+1.
Phase 3: Submitting the best solution so far as a result of ACO for TSP.
The remaining part of this subsection will mainly discuss the steps 2.1.2 and 2.2.
8.2.2.1 Ant System for Traveling Salesman Problem
Dorigo et al. published the Ô¨Årst paper on ACO in 1996 [1]. Their approach is called
an ant system (AS).
In substep 2.1.2, the transition probability of the link (i, j) that connects city i
and city j for ant k in generation gen is as follows:
pk
i j (gen) =
‚éß
‚é®
‚é©
œÑij(gen)(Œ∑ij)
Œ≤
‚àël‚ààallowedk œÑil(gen)(Œ∑il)Œ≤
j ‚ààallowedk
0
otherwise
(8.1)

332
8 Swarm Intelligence
where œÑi j (gen) is the pheromone concentration, Œ∑i j = 1/dij is the visibility used
to model the local perception ability of the ants,3 Œ≤ is a user-deÔ¨Åned parameter to
adjust the relative importance of œÑij (gen) and Œ∑ij, and allowedk is the set containing
all the feasible cities of ant k currently. Equation 8.1 means that the ant will select
the feasible link with a high pheromone concentration and short distance with high
probability.
After m ants form the Hamiltonian cycles, they modify the pheromone concen-
tration of the links they passed by in step 2.2. For the pheromone concentration œÑi j
on the link between city i and city j, the update function is as follows:
œÑij (gen+1) = (1‚àíœÅ)œÑij (gen)+ŒîœÑi j
= (1‚àíœÅ)œÑij (gen)+
m
‚àë
k=1
ŒîœÑk
i j
(8.2)
where œÅ is the evaporation factor4 and ŒîœÑk
ij is the contribution of ant k to œÑi j as
follows:
ŒîœÑk
ij =
{
Q
Lk
link(i, j) in ant k
‚Ä≤s tour
0
otherwise
(8.3)
where Lk is the length of the Hamiltonian cycle formed by ant k and Q is a user-
deÔ¨Åned parameter affecting the contribution of the distance to the pheromone.
Smaller Lk contributes a larger increase to œÑ. Equations 8.2 and 8.3 mean that: (1)
the pheromone on every link evaporates with time, (2) ants only deposit pheromones
on the links they pass, and (3) those ants with the better results deposit more
pheromones.
Dorigo et al. used AS parameters m = n, Œ≤ = 5, œÅ = 0.5, and Q = 100 to suc-
cessfully solve the TSP with 30 cities.
8.2.2.2 Ant Colony System for Traveling Salesman Problem
To further improve the search ability of AS, Dorigo and Gambardella suggested the
ant colony system (ACS) in 1997 [2].
The main difference between ACS and AS lies in the following three points.
In substep 2.1.2, an ant selects the link with the largest value of œÑil (gen)(Œ∑il)Œ≤
in the feasible set with probability p0. That is to say, if a uniform random number
between (0,1) is smaller than p0, the ant will deterministically select the link with
the largest value of œÑil (gen)(Œ∑il)Œ≤. Otherwise, it still uses Eq. 8.1 to probabilistically
determine the next city to be visited.
3 That is to say, an ant knows both the local information regarding the distance between its current
city and any other feasible city, i.e., di j, and the history experience of the ant colony, i.e., œÑij (gen).
4 Thus (1‚àíœÅ) is the remaining percentage.

8.2 Ant Colony Optimization
333
In ACS, we have two kinds of pheromone updating rule. The global updating in
step 2.2 considers only the best solution so far as follows, which is different from
Eq. 8.2:
œÑij (gen+1) = (1‚àíŒ±)œÑij (gen)+Œ±ŒîœÑi j
(8.4)
where Œ± is the global evaporation factor and ŒîœÑij is deÔ¨Åned as follows:
ŒîœÑij =
{
1
Lgb
link(i, j) in the best tour so far
0
otherwise
(8.5)
where Lgb is the shortest length of the Hamiltonian cycle found thus far by all ants.
Equations 8.4 and 8.5 mean that only the best solution so far can contribute to its
corresponding pheromone concentration and the pheromone concentration in other
links will evaporate with time.
The third improvement of ACS over AS is the local updating rule in substep
2.1.2. Whenever an ant passes a link, it will modify the pheromone concentration in
that link as follows:
œÑij = (1‚àíœÅ)œÑij +œÅŒî
‚Ä≤œÑi j
(8.6)
where œÅ is the local evaporation factor and Œî
‚Ä≤œÑij could be deÔ¨Åned in two ways. The
simplest way is as follows:
Œî
‚Ä≤œÑij = œÑ0
(8.7)
where œÑ0 is a predeÔ¨Åned factor.5
Another way is to modify the pheromone in a Q-learning way6 as follows:
Œî
‚Ä≤œÑij = Œ≥
max
l‚ààallowedk( j)œÑ jl
(8.8)
where Œ≥ is a control parameter and allowedk (j) is the feasible set while ant k is at
city j. Equations. 8.6 and 8.8 mean that the contribution of ant k while passing link
(i, j) is determined by the maximum pheromone concentration on the feasible links
connecting city j.7 The ACS with Eq. 8.8 is also called ant-Q.
All the above three modiÔ¨Åcations have a similar intuitive idea, i.e., reinforce the
inÔ¨Çuence of the historically better solutions and thus accelerate the search speed.
5 Dorigo and Gambardella suggested that œÑ0 = (n‚ãÖLnn)‚àí1. n is the city number and Lnn is the
tour length generated by the closest neighbor construction heuristic, which is introduced in Sect.
7.3.2.1.
6
Q-learning is an implementation of reinforcement learning [3, 4]. Interested read-
ers can Ô¨Ånd a funny tutorial at http://people.revoledu.com/kardi/tutorial/
ReinforcementLearning/index.html.
7 Because the pheromone concentration embodies historical experiences during the evolving pro-
cess, using the maximum pheromone concentration on the feasible links connecting city j to mod-
ify the pheromone concentration on line (i, j) might further accelerate the evolving process.

334
8 Swarm Intelligence
With the help of the above methods, Dorigo and Gambardella used ACS param-
eters m = 10, Œ≤ = 2, p0 = 0.9, and Œ± = œÅ = 0.1 to successfully solve the TSP with
100 cities.
Apart from the above two implementations of ACO, there are several other vari-
ations, e.g., the rank-based ant system suggested by Bullnheimer et al. in 1999 [5],
the MAX-MIN ant system suggested by St¬®uzel and Hoos in 2000 [6], and the hyper-
cube framework suggested by Blum and Dorigo in 2004 and discussed by Birattari
et al. in 2007 [7, 8].
8.2.2.3 Ant System for Job-shop Scheduling Problem
Because an ant colony has an innate ability to Ô¨Ånd the shortest path between the nest
and food, it is quite natural to apply ACO to distance-related problems, e.g., TSP.
For applying ACO to other combinatorial problems, several modiÔ¨Åcations should be
made. Let us take JSSP, which was introduced in Sect. 7.4, for example.8
The two-machine and four-job JSSP example is illustrated by Table 8.1.
Table 8.1 The two-machine and four-job JSSP example
+

/
	




> 


,
,
> 


,
,
> 


,
,
> 


,
,
The Ô¨Årst thing for solving JSSP with ACO is to model the JSSP as a half-directed
graph, illustrated in Fig. 8.3.
There are nine nodes in this eight-operation JSSP. The 0 node in Fig. 8.3 repre-
sents the dummy starting point. Unlike Fig. 7.33, the number in the node does not
represent the machine index, but only the index of operations.
There are both directed edges and undirected edges in Fig. 8.3, so we call it
half-directed. The directed edge represents the precedence relationships in different
jobs, e.g., the directed edge between nodes 1 and 5 means that the Ô¨Årst operation,
i.e., node 1, should be processed before the second operation, i.e., node 5, in job 1.
The undirected edges between operations indicate the possible next state of the ants.
After that, we need special considerations for constructing a feasible solution
with ants. All m ants start from node 0. We will only introduce the solution-
generating procedure of one ant.
8 There are many different implementation methods for ACO on JSSPs; we will only introduce the
method of Dorigo et al. [1].

8.2 Ant Colony Optimization
335
Fig. 8.3 A half-directed
graph representing a JSSP
in Table 8.1
)





"

#
We need to maintain two sets about ant k. Gk is the set of all the unvisited nodes
and Sk represents the set of feasible nodes in the next step. Initially, for ant k, Gk =
{1,2,3,4,5,6,7,8} and Sk = {1,2,3,4}.
Then we need to know how to deÔ¨Åne Œ∑ij in Eq. 8.1. We could use several priority
dispatching rules, introduced on page 307, to deÔ¨Åne Œ∑i j. Here we just take that
Œ∑ij = 1/pj, where p j is the processing time of operation j. Thus the next feasible
operations with less processing time are preferred.
Suppose we take operation 2 as the Ô¨Årst step, then node 2 should be deleted
from Gk and its direct successor, if there is one, is added to Sk, which makes Gk =
{1,3,4,5,6,7,8} and Sk = {1,3,4,6}. We can continue this procedure until Gk = /0.
Thus a permutation of the eight operations is generated. For example, permutation
(2 ‚àí3 ‚àí7 ‚àí1 ‚àí6 ‚àí4 ‚àí8 ‚àí5) represents the processing sequence of job (3 ‚àí1 ‚àí
2‚àí4) for machine 1 and job (2‚àí3‚àí4‚àí1) for machine 2. The Gantt chart of this
solution is illustrated by Fig. 8.4.













	



	








	








Fig. 8.4 The Gantt chart of the solution (2‚àí3‚àí7‚àí1‚àí6‚àí4‚àí8‚àí5)
Then all the techniques introduced in AS and ACS could be used to optimize the
JSSP. To sum up, at least three problems need to be resolved before applying ACO
to a combinatorial optimization problem.
1. How does one use a graph to illustrate the problem?
2. How does one deÔ¨Åne the visibility Œ∑ij in the problem?
3. How does one generate a feasible solution for an ant?

336
8 Swarm Intelligence
8.2.3 Continuous Ant Colony Optimization
As has been discussed above, the innate ability of an ant colony to Ô¨Ånd the shortest
path can be easily applied to distance-related combinatorial optimization problems.
By representing other combinatorial optimization problems to graphically, we can
still use ACO. The basic idea is to use pheromone concentration information to
construct new solutions and then use these newly generated solutions to modify the
pheromone trails, as illustrated by Fig. 8.5a.












*
	$
	 (


?	

$	
$	
(a)






















*
	$
	 (
	
 	
?	
	
 	$	
$	
(b)
Fig. 8.5 Comparison of ACO for combinatorial and continuous optimization: (a) ACO for combi-
natorial optimization, and (b) ACO for continuous optimization
If we want to expand ACO into continuous optimization, i.e., topics introduced
in Chaps. 4‚Äì6, a very intuitive idea is to change the discrete distributed pheromone
on the edge into a continuous distributed probabilistic distribution function on the
solution landscape, as illustrated in Fig. 8.5b. The relationship of these two Ô¨Ågures
is like the probabilistic distribution of discrete random variables and the density
function of continuous random variables.
There are several implementations of continuous ACO; here we only introduce
the ACO‚Ñùsuggested by Socha and Dorigo in 2008 [9].
In ACO‚Ñù, we maintain an archive with k good solutions with n variables and
use them to generate normal distribution density functions, which are later used to
generate m new solutions by ants. Then the m newly generated solutions replace the
worst solutions in the archive. At Ô¨Årst, an ant is used to generate a variable value, just
like it is used to generate a step in TSP. For a problem with n variables, an ant needs
n steps to generate a solution, just like it needs n steps to generate a Hamiltonian
cycle in TSP. In ACO‚Ñù, every variable is generated by a normal distribution density
function, i.e., Eq. 2.2.
The general solution process of ACO‚Ñùfor continuous optimization can be illus-
trated as follows.

8.2 Ant Colony Optimization
337
ACO‚Ñùfor Continuous Optimization
Phase 1: Initialization.
Step 1.1: Assign the parameters for ACO, such as ant number m,
archive size k, weight factor q, deviation factor Œæ, and stop criteria (such as
maxgen), etc. gen = 0.
Step 1.2: Generate k initial solutions randomly and put them into the
archive. Calculate their objective values and rank them so that the best solution
has rank 1 and the worst solution has rank k. Calculate solution sl‚Äôs weight œâl
as follows:
œâl =
1
qk
‚àö
2œÄe
‚àí(l‚àí1)2
2q2k2
(8.9)
where l is the rank of solution sl and q is the user-deÔ¨Åned weight factor. We
can draw two sets of weights with q = 0.1 and q = 0.2 (k = 50) in Fig 8.6. As
can be seen from Fig. 8.6, smaller q favors a Ô¨Åtter solution.
















	









E)0
E)0
Fig. 8.6 Two sets of weights with different q
Then calculate the probability of being the expectation of the new solution as
follows:

338
8 Swarm Intelligence
pl =
œâl
k
‚àë
l=1
œâl
(8.10)
According to Eqs. 8.9 and 8.10, Ô¨Åtter solutions have more chances of being
selected.
Phase 2: Main loop. Repeat the following steps until the stop criteria are
satisÔ¨Åed.
Step 2.1: Generate a solution by an ant. i = 1
Substep 2.1.1: Determine the mean of variable i. Select one so-
lution in the archive with the probability illustrated by Eq. 8.10. Any propor-
tional selection methods introduced in Sect. 3.3.2 could be used. This solution
is denoted as s j. The mean of variable i, i.e., Œºi, is the value x j
i .
Substep 2.1.2: Determine the standard deviation of variable i as
follows:
œÉi = Œæ
k
‚àë
e=1
			xe
i ‚àíx j
i
			
k ‚àí1
(8.11)
where s j is the selected solution in substep 2.1.1 and Œæ is the user-deÔ¨Åned
deviation factor. It is easy to see that a larger value of Œæ means a lower con-
vergence speed of ACO‚Ñù, similar to the effect of a larger value of evaporation
factor œÅ in AS and ACS.
Substep 2.1.3: Generate variable i with normal distribution
U (Œºi,œÉi). i = i+1. The ant moves to another variable.
Substep 2.1.4: Repeat substeps 2.1.1‚Äì2.1.3 until i > n. Different
solutions in the archive might be selected for different variables.
Step 2.2: Go back to step 2.1 if the number of newly generated solu-
tion is less than m.
Step 2.3: Calculate the objective values of these m newly generated
solutions.
Step 2.4: Replace the m worst solutions in the archive with the newly
generated solutions. Recalculate the weight and the probability of being se-
lected for each archive member using Eqs. 8.9 and 8.10, respectively.
Step 2.4: Update the best solution so far if necessary. gen = gen+1.
Phase 3: Submitting the best solution so far as the result of ACO‚Ñùfor
continuous optimization.
ACO‚Ñùis quite similar to CMA and EDA, which were introduced in Sect. 3.5.2.6.
Socha and Dorigo compared ACO‚Ñùwith the most famous probabilistic model-based

8.3 Particle Swarm Optimization
339
algorithms [10] with k = 50, q = 10‚àí4, Œæ = 0.85, and m = 2 and came to the con-
clusion that ACO‚Ñùis a rather competitive continuous optimization algorithm.9
The innate ability to handle discrete variables in AS or ACS and the expanded
ability to handle continuous variables in ACO‚Ñùmake ACO a very promising method
for mixed programming.
8.3 Particle Swarm Optimization
In 1995, Kennedy and Eberhart suggested a novel optimization method called par-
ticle swarm optimization (PSO) [11]. The intuitive idea behind its name is that they
want to mimic the ability of a bird Ô¨Çock to Ô¨Çy synchronously, change directions
suddenly, scatter, and regroup. The reason for such sophisticated behaviors can be
summarized as the social sharing of information among these birds. Kennedy and
Eberhart use velocity to describe the movement of birds and so gave the name ‚Äúpar-
ticle‚Äù (in the sense of physics) to their method. They Ô¨Årst implemented the algorithm
with a nearest neighbor velocity matching mechanism to mimic the behavior of bird
Ô¨Çocks. But they later gave up this idea after obtaining numerical results and con-
sidered the more broader social information. Thus they also abandoned the name of
‚ÄúÔ¨Çock‚Äù and used ‚Äúswarm.‚Äù
Since the birth of PSO, there have been many variations extending in different
directions. Due to the content limitation of this text, we can only illustrate the basic
ideas and the interesting and landmark algorithms. The relationships between the
variations introduced in this section can be summarized by Fig. 8.7.10


	
	
4% 
	
	

5(
'

:
 


9
%
/
%
/
%Q

	


Fig. 8.7 The relationships between the PSO variations introduced in this section
9 ACO‚Ñùcannot handle variable correlation, introduced in Sect. 3.2.2 as problems with separable
and nonseparable variables. Socha and Dorigo suggested a pragmatic method to adaptively rotate
the axes using solutions in the archive [9].
10 The terms in the Ô¨Ågure will be discussed later.

340
8 Swarm Intelligence
8.3.1 Organic Particle Swarm Optimization
In Kennedy and Eberhart‚Äôs initial PSO, a particle i in n-dimensional real space
‚Äúknows‚Äù three things as follows:
1. Its current location xi = {xi
1,‚ãÖ‚ãÖ‚ãÖ,xi
n};
2. Its historically best location pi; The ‚Äúbest‚Äù is evaluated by the objective value;
3. The overall best location found thus far in swarm pg, i.e., the historically best
location in m particles.
We Ô¨Årst calculate the next velocity as follows:
vi
j (gen+1) = c1rand1
(
pi
j ‚àíxi
j (gen)
)
+c2rand2
(
pg
j ‚àíxi
j (gen)
)
(8.12)
where gen is the iteration number of PSO, rand1 and rand2 ‚àºU(0,1), c1 > 0 and
c2 > 0 are acceleration coefÔ¨Åcients controlling the inÔ¨Çuence of a particle‚Äôs historical
best location and the swarm‚Äôs historical best location on its next velocity, respec-
tively.11 The Ô¨Årst part of the right side of Eq. 8.12 is called the cognitive component
and the second part is called the social component.12 Then particle i moves to the
next location with time step 1 as follows:
xi
j (gen+1) = xi
j (gen)+vi
j (gen+1)
(8.13)
In 1999 Shi and Eberhart added the fourth thing a particle ‚Äúknows‚Äù [12]:
4. Its current velocity vi = vi
1,‚ãÖ‚ãÖ‚ãÖ,vi
n.
Then the velocity updating is developed as follows:
vi
j (gen+1) = w(gen)vi
j (gen)+c1rand1
(
pi
j ‚àíxi
j (gen)
)
+c2rand2
(
pg
j ‚àíxi
j (gen)
)
(8.14)
where 0 < w < 1 is the inertia coefÔ¨Åcient13 and the Ô¨Årst part of the right side of
Eq. 8.14 is called the momentum component.14 In this book we call Eqs. 8.14 and
8.13 the organic PSO.
The general solution process of the PSO can be illustrated as follows.
11 Kennedy and Eberhart suggested that c1 = c2 = 2.
12 The cognitive component is for self-cognition and the social component is for social learning.
13 Shi and Eberhart suggested that w decreases linearly from 0.9 at the beginning to 0.4 by the
end of the algorithm, i.e., w(gen) = 0.9 ‚àí(0.5‚àógen/maxgen), for static problems [12] and w =
0.5+rand/2 for dynamic problems [13].
14 To consider the movement of particles in a more realistic physical environment.

8.3 Particle Swarm Optimization
341
Particle Swarm Optimization
Phase 1: Initialization.
Step 1.1: Assign the parameters for PSO, such as particle m, acceler-
ation coefÔ¨Åcients c1 and c2, and stop criteria (such as maxgen), etc. gen = 0.
Step 1.2: Generate m initial locations and velocities of the particles.
Calculate their objective values and initialize the personal best locations and
the overall best location accordingly.
Phase 2: Main loop. Repeat the following steps until the stop criteria are
satisÔ¨Åed.
Step 2.1: i = 1.
Step 2.2: Determine the next location of particle i.
Substep 2.2.1: Determine the next velocity of particle i using Eq.
8.14 with all variables. Different rand1 and rang2 should be used for different
variables.
Substep 2.2.2: Determine the next location of particle i using Eq.
8.13 with all variables.
Step 2.3: Calculate the objective value for particle i. Update pi and pg
if necessary. i = i+1. If i ‚â§m, go back to step 2.2.
Step 2.4: Update w according to the deterministic control rule. gen =
gen+1.
Phase 3: Submitting pg as the result.
To make the expression more concise, we can use the pointwise vector multiplica-
tion operator {.‚àó} to rewrite Eqs. 8.13 and 8.14 in vector form as follows.15 We also
neglect (gen) and (gen+1) for clearer expressions:
vi = wvi +c1rand1.‚àó
(
pi ‚àíxi)
+c2rand2.‚àó
(
pg ‚àíxi)
(8.15)
xi = xi +vi
(8.16)
where rand1 and rand2 are n-dimensional random vectors whose components are
‚àºU(0,1). If all the elements of rand1 are the same, and likewise with rand2, Eqs.
8.15 and 8.16 become space vector operations, like DE introduced in Sects. 2.4.2.2
and 3.2.2.4.
According to Eq. 8.15, particle i will start from its current location and Ô¨Çy along
the tradeoff direction to its personal best solution so far and the overall best solution
so far in the swarm. When the swarm approaches convergence, i.e., pi and pg are
both close to their current location, it will automatically decrease the velocity and
thus decrease the step size according to Eq. 8.15. So we can say that PSO has the
ability to adaptively control the step size.
15 (1,2,3).‚àó(2,3,4) = (2,6,12)

342
8 Swarm Intelligence
Too large a velocity may be generated by Eq. 8.15, which means the particles
move too fast and the swarm blows up. Generally we can predeÔ¨Åne an upper lim-
itation vmax. If the absolute value speed in any dimension exceeds vmax, it will be
assigned as ¬±vmax considering its sign [14].
If the particle exceeds the deÔ¨Ånition domain in any dimension, then two methods
can be adopted. The naive one is to just pull it back on the boundary. Liang et al.
also suggested not to update its pi so that it will Ô¨Çy back later [15].
The most attractive characteristics of PSO are its easy implementation (only two
operations) and fast convergence speed (with the help of pi and pg). But sometimes
it suffers premature convergence due to too fast a convergence, especially in highly
multimodal problems. So many studies have been done to control particle swarm
diversity, which is the main topic of the following subsections.
8.3.2 Neighbor Structure and Related Extensions
The distinction of PSO lies in pi and pg. Some researchers thought that letting pg be
the best solution in the swarm thus far might be too strong for multimodal problems.
So they suggested a term lbest, corresponding to gbest. gbest is a concept used in
organic PSO, i.e., the (global) best solution in the swarm thus far and lbest is for
the (local) best solution in the neighbor thus far. Here the neighbor does not mean
the real geometrical closeness relationship of the particles, but is the user-deÔ¨Åned
neighborhood relations among particles.
Many types of neighbor structures have been suggested [16, 17]. The gbest
model can also be represented as an all structure, where all particles are the neigh-
bors of every other particle. The all structure is the fastest way of propagating infor-
mation because the newly found best solution in the swarm will be known by any
particle in the next time step.
The simplest neighbor structure might be the ring structure. The nine-particle
swarm with ring structure can be illustrated by Fig. 8.8. For example, only particles
2 and 9 are neighbors of particle 1.
Fig. 8.8 The ring neighbor
structure























"
#
&

8.3 Particle Swarm Optimization
343
Another typical structure is the von Neumann structure. Particles are placed in
2-D grids. Every particle has four neighbors. In order to make it so that the edge
particles in 2-D space still have four neighbors, additional links should be added
and the grids should be wrapped to expand the structure into 3-D space. A nine-
particle swarm with the von Neumann structure can be illustrated by Fig. 8.9. For
example, particles 2, 3, 4, 7 are neighbors of particle 1.
"
#
&










"
#
&
&
"



(a)





























#
&
"
(b)
Fig. 8.9 The 2-D and 3-D representations of the von Neumann neighbor structure: (a) 2-D, and
(b) 3-D
After deÔ¨Åning the neighborhood, we can modify the velocity equation in Eq. 8.15
as follows:
vi = wvi +c1rand1.‚àó
(
pi ‚àíxi)
+c2rand2.‚àó
(
pli ‚àíxi)
(8.17)
where pli is the best solution so far (lbest) in particle i‚Äôs neighbors.
According to Eq. 8.17, the slowest way of information propagation is the ring
structure. In Fig. 8.8, suppose particle 1 Ô¨Ånds the global optimal solution; it will
take four time steps to transfer this information to particles 5 and 6. But it might
be an effective way to preserve the particle swarm diversity. Hence it might have
a more powerful global search ability in highly multimodal space compared to the
all neighbor structure. the von Neumann structure is in the middle of these two
extremes.
We can also assign rand randomly selected particles as the neighbors of particle
i, which is called random structure.
Selecting the proper neighbor structure for various problems might be a tough
job.

344
8 Swarm Intelligence
8.3.2.1 Constriction Particle Swarm
In Sect. 8.3.1, we introduced the vmax method to limit particle speed. In 2002 Clerc
and Kennedy introduced the constriction coefÔ¨Åcient in order to cancel the require-
ment of setting vmax artiÔ¨Åcially [18].
The velocity equation considering the constriction coefÔ¨Åcient can be illustrated
as follows:
vi = œá
(
vi +c1rand1.‚àó
(
pi ‚àíxi)
+c2rand2.‚àó
(
pli ‚àíxi))
(8.18)
where œá =
2
			2‚àíœï‚àí‚àö
œï2‚àí4œï
			 is the constriction coefÔ¨Åcient and œï = c1 +c2. Clerc and
Kennedy proved that if œï > 4, Eq. 8.18 could limit the particle coefÔ¨Åcient without
vmax.
The suggested values for the control parameter are c1 = c2 = 2.05 , œï = 4.10, and
thus œá = 0.7298. Comparing Eq. 8.18 to Eq. 8.17, we could say that the constriction
particle swarm is a special kind of organic particle swarm considering the neighbor
structure. Clerc and Kennedy called the constriction particle swarm a canonical
particle swarm.
In 2002 Kennedy and Mendes compared several neighbor structures using the
canonical particle swarm and came to the conclusion that the von Neumann struc-
ture is the best structure for it [17]. In 2000 Eberhart and Shi also compared the
canonical particle swarm and the organic particle swarm and suggested that the
best approach is to utilize the constriction coefÔ¨Åcient along with the velocity limit
vmax,i = xmax,i, where xmax,i is the deÔ¨Ånition length in i‚Äôs dimension and vmax,i is the
absolute maximum velocity in i‚Äôs dimension [19].
8.3.2.2 Fully Informed Particle Swarm
Both the canonical particle swarm and the organic particle swarm with a neighbor
structure only consider the best other particle in the neighborhood. This characteris-
tic might limit the search of PSO and therefore cause premature. In 2004 Mendes et
al. suggested the fully informed particle swarm (FIPS) to consider the contribution
of all neighbors [20].
We can deÔ¨Åne ùùã1 = c1rand1, ùùã2 = c1rand2, and ùùã= ùùã1 + ùùã2. Then Eq. 8.18
could be rewritten as follows:
vi = œá
(
vi +ùùã1.‚àó
(
pi ‚àíxi)
+ùùã2.‚àó
(
pli ‚àíxi))
= œá
(
vi +ùùã.‚àó
((
ùùã1.‚àópi +ùùã2.‚àópli
)
./ùùã‚àíxi))
(8.19)

8.3 Particle Swarm Optimization
345
where ./ is the pointwise division operator.16 Let pm =
(
ùùã1.‚àópi +ùùã2.‚àópli)
./ùùã.
Then we can further simplify the expression of the canonical particle swarm as
follows:
vi = œá
(
vi +ùùã
(
pm ‚àíxi))
(8.20)
Equation 8.20 means that only the weight sum of the personal best location and
the best neighbor location affects the velocity of particle i. FIPS considers all the
best neighbor locations in the following way.
Suppose particle i‚Äôs neighbor set is Ni and its neighbor number is ‚à£Ni‚à£. For k‚Äôs
neighbor in Ni, its personal best location is pk, its acceleration random vector is
ùùãk ‚àºU
(
0, œïmax
‚à£Ni‚à£
)
, where œïmax is 4.10 according to Clerc and Kennedy‚Äôs suggestion
regarding the constriction coefÔ¨Åcient, and U() is the vector uniform random func-
tion. Then the velocity updating equation in FIPS can be illustrated as follows:17
vi = œá
(
vi + ‚àë
k‚ààNi
ùùãk.‚àó
((
‚àë
k‚ààNi
ùùãk.‚àópk
)
./
(
‚àë
k‚ààNi
ùùãk
)
‚àíxi
))
(8.21)
We can deÔ¨Åne ùùã= ‚àë
k‚ààNi
ùùãk = U(0,œïmax) to simplify Eq. 8.21 as follows:
vi = œá
‚éõ
‚éú
‚éùvi +ùùã.‚àó
‚éõ
‚éú
‚éù
(
‚àë
k‚ààNi
ùùã.‚àópk
‚à£Ni‚à£
)
./ùùã‚àí
‚àë
k‚ààNi
xi
‚à£Ni‚à£
‚éû
‚éü
‚é†
‚éû
‚éü
‚é†
(8.22)
Then the Ô¨Ånal velocity updating rule in FIPS can be simpliÔ¨Åed as follows:
vi = œá
‚éõ
‚éú
‚éùvi +
‚éõ
‚éú
‚éù
‚àë
k‚ààNi
œï.‚àó
(
pk ‚àíxi)
‚à£Ni‚à£
‚éû
‚éü
‚é†
‚éû
‚éü
‚é†
(8.23)
In Eq. 8.23, all of particle i‚Äôs neighbors‚Äô personal best solutions will contribute
to its velocity.18
In canonical particle swarm or organic particle swarm with a neighbor structure,
the larger neighbor number means a faster convergence, but in FIPS this conclusion
will be reversed.19
In 2006 Kennedy and Mendes compared the organic particle swarm with the
neighbor structure and FIPS considering many types of structures [21]. They found
that the organic particle swarm with a neighbor structure is less sensitive to the
16 (3,4,9)./(2,2,3) = (1.5,2,3)
17 Following Eq. 8.19 but considering the weights of the ‚à£Ni‚à£neighbors.
18 It is necessary to note here that Ni in Eq. 8.19 does not contain particle i itself, i.e., Eq. 8.23 does
not consider particle i‚Äôs personal best location [21].
19 Why?

346
8 Swarm Intelligence
neighbor structure than FIPS. But with a proper neighbor structure,20 FIPS can gen-
erate better solutions.
8.3.2.3 Bare Bones Particle Swarm
In 2003 Kennedy revisited the canonical particle swarm and suggested a back-to-
nature idea by eliminating the velocity updating rule, i.e., Eq. 8.18 [22].
Kennedy summarized the essence of PSO as the ‚Äúsampling points on the prob-
lem space and using discovered knowledge to guide exploration‚Äù.21 Then he did
a very interesting yet simple numerical experiment. Suppose we are facing a one-
dimensional search space, pi ‚â°‚àí10 and pli ‚â°10 in Eq. 8.18. We only have one
particle with v(0) = 1 and x(0) = 0. Its Ô¨Çying trajectory can be simpliÔ¨Åed as fol-
lows:
v
‚Ä≤ = 0.7298(v+2.05‚àórand1 ‚àó(10‚àíx)+2.05‚àórand2 ‚àó(‚àí10‚àíx))
x
‚Ä≤ = x+v
‚Ä≤
(8.24)
where v
‚Ä≤ and x
‚Ä≤ are simpliÔ¨Åed versions of v(gen + 1) and x(gen + 1), respectively,
and v and x are simpliÔ¨Åed versions of v(gen) and x(gen), respectively. We record the
location the particle has visited in the 100000 iteration steps and draw the histogram
of the location in the scale of (‚àí40,40) as in Fig. 8.10.22 The horizontal axis is the
small region of the particle location and the vertical axis is the amount of time the
particle visited that region.
As can be seen from Fig. 8.10, the distribution presents a very clear bell shape.
That means the most often visited region is determined by pi and pli. It seems that
the particle Ô¨Çies in a normal distribution way with a mean Œº = pi+pli
2
and the stan-
dard deviation related to the distance between pi and pli.
So Kennedy discarded the velocity part and used a normal distribution to generate
the new locations as follows:
xi (gen+1) ‚àºN
(pi +pli
2
,
			pi ‚àípli
			
)
(8.25)
where N(ùùÅ,œÉ) is the uncorrelated normal distribution vector with mean ùùÅand the
same standard deviation œÉ and ‚à£‚à£is the norm function (distance).
At Ô¨Årst glance, Eq. 8.25 has no sense of birds Ô¨Çying. Kennedy argued that the
main characteristics of PSO lie in the mutual inÔ¨Çuence between particles, the self
memory of the best location so far, and the neighbor structure. Equation 8.25 dis-
cards all the previous considerations about the velocity (related to ‚ÄúÔ¨Çy‚Äù) but main-
20 The average degree of the graph is 3 and the vertices in the graph are clustered naturally.
21 Do these words invoke the continuous ACO‚Ñùand CMA?
22 All the locations outside of the scale are neglected.

8.3 Particle Swarm Optimization
347
tains the above-discussed properties and embodies the essence of ‚Äúsampling and
modifying the distribution.‚Äù So he named the algorithm the bare bones particle
swarm.


















	

#
Fig. 8.10 Histogram of a simple example of the canonical particle swarm
Then Kennedy further introduced the interaction probability IP. Particles modify
their location by Eq. 8.25 with probability IP.23 Otherwise, xi(gen+1) = pi.
Kennedy compared the bare bones particle swarm with the canonical particle
and FIPS using benchmark problems and different neighbor structures and found
that the best algorithm is the bare bones particle swarm with IP = 0.5 and a random
neighbor.
8.3.3 Extensions from Organic Particle Swarm Optimization
There are also many extensions of organic PSO, illustrated by Eq. 8.14, by intro-
ducing diversity in various ways. Here we introduce the comprehensive learning
particle swarm optimization (CLPSO) suggested by Liang et al. in 2006 [15].
Liang et al. deÔ¨Åned the learning probability pi
c for particle i to describe its in-
tention to learn from other particles. The velocity updating rule for dimension j of
particle i is as follows:
vi
j (gen+1) = w(gen)vi
j (gen)+c‚àórand ‚àó
(
pli
j ‚àíxi
j (gen)
)
(8.26)
23 The suggested value of IP is 0.5.

348
8 Swarm Intelligence
where pli
j is called the exemplar of particle i. Other parameters in Eq. 8.26 have the
same meaning in Eq. 8.14.
For variable j in particle i, we use tournament selection with probability pi
c to
pick its exemplar.24 In binary tournament selection, we compare the objective val-
ues of two randomly selected particles from the swarm excluding the current particle
and make the Ô¨Åtter one the exemplar.25 Otherwise (with probability 1‚àípi
c), its ex-
emplar is itself.
The exemplar will remain unchanged in Eq. 8.26 until particle i has not been
improved in the successive m generations.26
The learning probability pi
c determines the frequency of a particle to learn from
other particles. To further improve the swarm diversity, Liang et al. suggested that
different learning probabilities should be assigned as follows:
pi
c = 0.05+0.45‚àó
exp
(
10(i‚àí1)
m‚àí1
)
‚àí1
exp(10)‚àí1
(8.27)
where m is the swarm number and i is the order number of the particles (from 1
to m). As can be seen from Eq. 8.27, various pi
c could be generated in the range
(0.05,0.5).27
By comparing Eq. 8.26 with Eq. 8.23, we can come to the conclusion that CLPSO
introduces diversity into the swarm by probabilistically selecting the particle to
learn, which is different from how it is done in FIPS, i.e., all the neighbor parti-
cles will have inÔ¨Çuences.
Liang compared CLPSO with nine PSO algorithms, including the above intro-
duced organic particle swarm, canonical particle swarm, and FIPS, and came to
the conclusion that CLPSO achieves the best results on most complex multimodal
problems but is relatively weak for unimodal and simple multimodal problems.
8.4 Summary
Swarm intelligence has a lot of branches and we only brieÔ¨Çy introduced the two
most important of them, i.e., ACO and PSO.
Ant colonies can Ô¨Ånd the shortest length between food and the nest with the
help of pheromones. So ACO is basically designed for shortest-length problems
on a graph. Many techniques are suggested to adjust the pheromone concentration
properly. In order to solve other combinatorial problems, we need to transfer the
original problem to a graph and deÔ¨Åne the visibility and pheromone concentration
accordingly. By the idea of sampling according to distribution and modifying the
24 Different variables in particle i might select different particles as its exemplar.
25 We could also say that CLPSO uses the ‚Äúall‚Äù neighbor structure.
26 Liang et al. suggested that m = 7 based on empirical numerical experiments on benchmark
problems.
27 In CLPSO, pi
c remains unchanged during the evolving process.

8.4 Summary
349
distribution according to the result of the sampling, ACO can be expanded into
continuous optimization and related to the estimation of distribution algorithm.
Bird Ô¨Çocks also emerge intelligence in foraging behavior. To mimic the coordi-
nated Ô¨Çying property, PSO is recommended. PSO is basically designed for continu-
ous optimization using a particle‚Äôs historically best solution so far and its neighbors‚Äô
historically best solution(s) so far. PSO is fast because it always uses the best infor-
mation so far but suffers from premature thereby. So many variations have been
proposed to introduce swarm diversity by the neighbor structure, by utilizing neigh-
bor information, etc.
To some extent, ACO and PSO do not like to be EAs because they do not have
a selection procedure and they do not use crossover operators to exchange informa-
tion. But, according to the criteria given in Sect. 1.2, they are both population-based,
Ô¨Åtness-oriented, and variation-driven. So we can still regard them as EAs.
After reading this chapter, you should be familiar with the procedure of applying
ACO to combinatorial optimization and PSO to continuous optimization, under-
stand the rationale of suggesting ACO and PSO behind the speciÔ¨Åc implementation
steps, and learn some techniques for controlling parameters and adjusting the diver-
sity.
Suggestions for Further Reading
On the topic of SI, there is a journal, Swarm Intelligence published by Springer since
2007, a biannual conference on SI, the International Conference on Ant Colony Op-
timization and Swarm Intelligence, whose proceedings are published by Springer,
and an annual symposium, the IEEE Swarm Intelligence Symposium.
There are many recently published books about SI. Readers are encouraged to
read Bonabeau et al.‚Äôs book published in 1999 [23], Weiss‚Äôs book published in 2000
[24], Eberhart et al.‚Äôs book published in 2001 [25], Engelbrecht‚Äôs book published in
2006 [26], Abraham et al.‚Äôs book published in 2006 [27], and Blum and Merkle‚Äôs
book published in 2008 [28].
Montes de Oca maintains a Web site on ACO containing plenty of information.28
Two books about ACO are suggested for further reading, i.e., Dorigo and St¬®utzle‚Äôs
book [29] and Solnon‚Äôs book [30]. The paper published by Dorigo et al. in 1999
established the framework of ACO [31]. A tutorial for beginners on ACO was given
by Dorigo et al. in 2006 [32].
At the theory level of ACO, in 2000 Dorigo et al. discuss the stigmergy, i.e., in-
direct coordination between agents, in ACO [33]; in 2005 Zecchin et al. discussed
the parameter tuning guidelines for ACO [34]; and in 2005 Blum and Dorigo intro-
duced the concept of a competition-balanced system to overcome the second-order
deception in ACO [35].
28 http://www.aco-metaheuristic.org/

350
8 Swarm Intelligence
At the application level, in 2002 Solnon used ACO to solve constrained opti-
mization [36] and in 2009 Leguizam¬¥on and Coello Coello discussed the same topic
in another way [37]. In 2002 Merkle used ACO in resource-constrained project
scheduling [38]. In 2008 Martens et al. made ACO applicable to supervised learn-
ing problems, i.e., classiÔ¨Åcation [39]. In 2006 Nezamabadi-pour et al. applied ACO
to edge detection [40] and in 2008 Lim utilized ACO for path planning in sparse
graphs [41].
A Web site Particle Swarm Central contains a lot of the latest information on
PSO.29 The books suggested on PSO included Clerc‚Äôs book from 2006 [42], Poli
et al.‚Äôs book from 2008 [43], Kishk‚Äôs book from 2008 [44], and Parsopoulos and
Vrahatis‚Äôs book from 2009 [45]. The very intensive and up-to-date surveys on PSO
given by Banks et al. in 2007 and 2008 are highly recommended [46, 47].
In 2004 Ratnaweera et al. suggested deterministic control mechanisms for vari-
ous PSO control parameters [48]. In 2004 van den Bergh and Engelbrecht applied
cooperative approach to PSO [49]. In 2006 Kadirkamanathan et al. discussed the
stability analysis of PSO in a random environment [50]. Blackwell and Branke‚Äôs
paper and Janson and Middendorf‚Äôs paper, both from 2006, discuss PSO in noisy
and dynamic environments [51, 52]. Two comparison papers about PSO, DE, CMA,
and GA are recommended, i.e., Langdon and Poli‚Äôs paper from 2007 [53] and Vrugt
et al.‚Äôs paper from 2009 [54].
In 2006 Liang and Suganthan applied PSO to constrained optimization [55]. Par-
sopoulos and Vrahatis‚Äôs 2004 paper and Parrott and Li‚Äôs 2006 paper discuss mul-
timodal optimization [56, 57]. In 2004 Coello Coello et al. addressed MOP with
PSO [58]. Ho et al. also discussed this topic in 2006 [59]. A comprehensive sur-
vey on PSO-based MOP solvers was given by Reyes-Sierra and Coello Coello in
2006 [60].
Many successful applications of PSO have been proposed. Some of them are
recommended according to our interests: Hastings et al.‚Äôs paper on computer graph-
ics and animation from 2009 [61], O‚ÄôNeill and Brabazon‚Äôs paper on unsupervised
learning from 2008 [62], del Valle et al.‚Äôs paper on power systems from 2008 [63],
Li et al.‚Äôs paper on gene selection from 2008 [64], and Rahimi-Vahed et al.‚Äôs paper
on assembly line sequencing from 2007 [65].
The must-read papers of this chapter are [1] for AS, [2] for ACS, [9] for contin-
uous ACO, [11] for the initial idea of PSO, [21] for FIPS and its related neighbor
structure, and [15] for introducing diversity into a swarm.
Exercises and Potential Research Projects
8.1. Summarize the essences of [36] and [37] on a single sheet of paper separately
and compare them using the benchmark problems in Appendix, PI in Sect. 5.5, and
29 http://www.particleswarm.info

References
351
the techniques introduced in Sect. 3.6 to do a fair comparison and draw statistical
conclusions according to your numerical experiments.
8.2. Implement ACO‚Ñùand compare it to the EA you Ô¨Ånished in Chap. 3 using the
benchmark problems in Appendix, PI in Sect. 5.5, and the techniques introduced in
Sect. 3.6 to do a fair comparison and draw statistical conclusions according to your
numerical experiments.
8.3. Implement AS or ACS for TSP or JSSP and compare it to the EA you Ô¨Ån-
ished in Chap. 7 using the benchmark problems in Appendix, PI in Sect. 5.5, and
the techniques introduced in Sect. 3.6 to do a fair comparison and draw statistical
conclusions according to your numerical experiments.
8.4. Draw the illustrative vector graph in 2-D real space for Eqs. 8.15 and 8.16
provided that all the elements of rand1 are the same, and likewise for rand2.
8.5. Why in canonical particle swarm or organic particle swarm with a neighbor
structure does a larger neighbor number mean faster convergence but in FIPS the
opposite is true?
8.6. Implement organic particle and CLPSO in your programming environment, use
the benchmark problems in Appendix and the techniques introduced in Sect. 3.6 to
do a fair comparison and draw statistical conclusions according to your numerical
experiments.
8.7. Implement ACO‚Ñùand CLPSO, use the benchmark problems in Appendix and
the techniques introduced in Sect. 3.6 to do a fair comparison and draw statistical
conclusions according to your numerical experiments.
8.8. Try to improve CLPSO with techniques introduced in Chap. 3, use the bench-
mark problems in Appendix and the techniques introduced in Sect. 3.6 to do a fair
comparison and draw statistical conclusions according to your numerical experi-
ments.
8.9. Summarize the global search mechanism, convergence mechanism, and uphill
mechanism for minimum optimization problems of ACO and PSO.
References
1. Dorigo M, Maniezzo V, Colorni A (1996) Ant system: optimization by a colony of cooperating
agents. IEEE Trans Syst Man Cybern B Cybern 26(1):29‚Äì41
2. Dorigo M, Gambardella L (1997) Ant colony system: a cooperative learning approach to the
traveling salesman problem. IEEE Trans Evol Comput 1(1):53‚Äì66
3. Watkins C (1989) Learning from delayed rewards. Ph.D. thesis, University of Cambridge, UK
4. Kaelbling LP, Littman M, Moore A (1996) Reinforcement learning: a survey. J Artif Intell
Res 4:237‚Äì285

352
8 Swarm Intelligence
5. Bullnheimer B, Hartl RF, Strau√ü C (1999) A new rank based version of the ant system - a
computational study. Central Eur J Oper Res and Econ 7:25‚Äì38
6. St¬®utzle T, Hoos HH (2000) MAX-MIN ant system. Future Gener Comput Syst 16(8):889‚Äì914
7. Blum C, Dorigo M (2004) The hyper-cube framework for ant colony optimization. IEEE
Trans Syst Man Cybern B Cybern 34(2):1161‚Äì1172
8. Birattari M, Pellegrini P, Dorigo M (2007) On the invariance of ant colony optimization. IEEE
Trans Evol Comput 11(6):732‚Äì742
9. Socha K, Dorigo M (2008) Ant colony optimization for continuous domains. Eur J Oper Res
185(3):1155‚Äì1173
10. Kern S, M¬®uller SD, Hansen N et al (2004) Learning probability distributions in continuous
evolutionary algorithms - a comparative review. Nat Comput 3(3):355‚Äì356
11. Kennedy J, Eberhart R (1995) Particle swarm optimization.
In: Proceedings of the IEEE
international conference on neural networks, pp 1942‚Äì1948
12. Shi Y, Eberhart R (1999) Empirical study of particle swarm optimization. In: Proceedings of
the IEEE congress on evolutionary computation, pp 1945‚Äì1950
13. Eberhart R, Shi Y (2001) Tracking and optimizing dynamic systems with particle swarms. In:
Proceedings of the IEEE congress on evolutionary computation, pp 94‚Äì100
14. Eberhart RC, Dobbins R, Simpson PK (1996) Computational intelligence PC tools. Morgan
Kaufmann, San Francisco
15. Liang J, Qin A, Suganthan P et al (2006) Comprehensive learning particle swarm optimizer
for global optimization of multimodal functions. IEEE Trans Evol Comput 10(3):281‚Äì295
16. Kennedy J (1999) Small worlds and mega-minds: effects of neighborhood topology on particle
swarm performance. In: Proceedings of the IEEE congress on evolutionary computation, pp
1931‚Äì1938
17. Kennedy J, Mendes R (2002) Population structure and particle swarm performance. In: Pro-
ceedings of the IEEE congress on evolutionary computation, pp 1671‚Äì1676
18. Clerc M, Kennedy J (2002) The particle swarm - explosion, stability, and convergence in a
multidimensional complex space. IEEE Trans Evol Comput 6(1):58‚Äì73
19. Eberhart R, Shi Y (2000) Comparing inertia weights and constriction factors in particle swarm
optimization. In: Proceedings of the IEEE congress on evolutionary computation, pp 84‚Äì88
20. Mendes R, Kennedy J, Neves J (2004) The fully informed particle swarm: simpler, maybe
better. IEEE Trans Evol Comput 8(3):204‚Äì210
21. Kennedy J, Mendes R (2006) Neighborhood topologies in fully informed and best-of-
neighborhood particle swarms. IEEE Trans Syst Man Cybern C Appl Rev 36(4):515‚Äì519
22. Kennedy J (2003) Bare bones particle swarms. In: Proceedings of the IEEE swarm intelligence
symposium, pp 80‚Äì87
23. Bonabeau E, Dorigo M, Theraulaz G (1999) Swarm intelligence: from natural to artiÔ¨Åcial
systems. Oxford University Press, Oxford, UK
24. Weiss G (2000) Multiagent systems: a modern approach to distributed artiÔ¨Åcial intelligence.
MIT Press, Cambridge, MA
25. Eberhart RC, Shi Y, Kennedy J (2001) Swarm intelligence. Morgan Kaufmann, San Francisco
26. Engelbrecht AP (2006) Fundamentals of computational swarm intelligence. Wiley, New York
27. Abraham A, Grosan C, Ramos V (2006) Swarm intelligence in data mining. Springer, Berlin
Heidelberg New York
28. Blum C, Merkle D (2008) Swarm intelligence: introduction and applications. Springer, Berlin
Heidelberg New York
29. Dorigo M, St¬®utzle T (2004) Ant colony optimization. MIT Press, Cambridge, MA
30. Solnon C (2010) Ant colony optimization and constraint programming. Wiley-ISTE, New
York
31. Dorigo M, Caro GD, Gambardella LM (1999) Ant algorithms for discrete optimization. Artif
Life 5(2):137‚Äì172
32. Dorigo M, Birattari M, Stutzle T (2006) Ant colony optimization. IEEE Comput Intell Mag
1(4):28‚Äì39
33. Dorigo M, Bonabeau E, Theraulaz G (2000) Ant algorithms and stigmergy. Future Gener
Comput Syst 16(9):851‚Äì871

References
353
34. Zecchin A, Simpson A, Maier H et al (2005) Parametric study for an ant algorithm applied to
water distribution system optimization. IEEE Trans Evol Comput 9(2):175‚Äì191
35. Blum C, Dorigo M (2005) Search bias in ant colony optimization: on the role of competition-
balanced systems. IEEE Trans Evol Comput 9(2):159‚Äì174
36. Solnon C (2002) Ants can solve constraint satisfaction problems. IEEE Trans Evol Comput
6(4):347‚Äì357
37. Leguizam¬¥on G, Coello C (2009) Boundary search for constrained numerical optimization
problems with an algorithm inspired by the ant colony metaphor. IEEE Trans Evol Comput
13(2):350‚Äì368
38. Merkle D, Middendorf M, Schmeck H (2002) Ant colony optimization for resource-
constrained project scheduling. IEEE Trans Evol Comput 6(4):333‚Äì346
39. Martens D, Backer MD, Haesen R et al (2007) ClassiÔ¨Åcation with ant colony optimization.
IEEE Trans Evol Comput 11(5):651‚Äì665
40. Nezamabadi-pour H, Saryazdi S, Rashedi E (2006) Edge detection using ant algorithms. Soft
Comput 10(7):623‚Äì628
41. Lim KK, Ong Y, Lim MH et al (2008) Hybrid ant colony algorithms for path planning in
sparse graphs. Soft Comput 12(10):981‚Äì994
42. Clerc M (2006) Particle swarm optimization. ISTE Publishing Company, London, UK
43. Poli R, Kennedy J, Blackwell T et al (2008) Particle swarms: the second decade. J Artif Evol
Appl 2008:1‚Äì3
44. Mikki S, Kishk A (2008) Particle swarm optimizaton: a physics-based approach. Morgan and
Claypool, San Rafael, CA
45. Parsopoulos KE, Vrahatis MN (2009) Particle swarm optimization and intelligence: advances
and applications. Information Science, LinkHershey, PA
46. Banks A, Vincent J, Anyakoha C (2007) A review of particle swarm optimization. I: back-
ground and development. Nat Comput 6(4):467‚Äì484
47. Banks A, Vincent J, Anyakoha C (2008) A review of particle swarm optimization. II: hybridi-
sation, combinatorial, multicriteria and constrained optimization, and indicative applications.
Nat Comput 7(1):109‚Äì124
48. Ratnaweera A, Halgamuge S, Watson H (2004) Self-organizing hierarchical particle swarm
optimizer with time-varying acceleration coefÔ¨Åcients. IEEE Trans Evol Comput 8(3):240‚Äì
255
49. van den Bergh F, Engelbrecht A (2004) A cooperative approach to particle swarm optimiza-
tion. IEEE Trans Evol Comput 8(3):225‚Äì239
50. Kadirkamanathan V, Selvarajah K, Fleming P (2006) Stability analysis of the particle dynam-
ics in particle swarm optimizer. IEEE Trans Evol Comput 10(3):245‚Äì255
51. Blackwell T, Branke J (2006) Multiswarms, exclusion, and anti-convergence in dynamic en-
vironments. IEEE Trans Evol Comput 10(4):459‚Äì472
52. Janson S, Middendorf M (2006) A hierarchical particle swarm optimizer for noisy and dy-
namic environments. Genet Programm Evolvable Mach 7(4):329‚Äì354
53. Langdon W, Poli R (2007) Evolving problems to learn about particle swarm optimizers and
other search algorithms. IEEE Trans Evol Comput 11(5):561‚Äì578
54. Vrugt J, Robinson B, Hyman J (2009) Self-adaptive multimethod search for global optimiza-
tion in Real-Parameter spaces. IEEE Trans Evol Comput 13(2):243‚Äì259
55. Liang J, Suganthan P (2006) Dynamic multi-swarm particle swarm optimizer with a novel
constraint-handling mechanism. In: Proceedings of the IEEE congress on evolutionary com-
putation, pp 9‚Äì16
56. Parsopoulos K, Vrahatis M (2004) On the computation of all global minimizers through par-
ticle swarm optimization. IEEE Trans Evol Comput 8(3):211‚Äì224
57. Parrott D, Li X (2006) Locating and tracking multiple dynamic optima by a particle swarm
model using speciation. IEEE Trans Evol Comput 10(4):440‚Äì458
58. Coello C, Pulido G, Lechuga M (2004) Handling multiple objectives with particle swarm
optimization. IEEE Trans Evol Comput 8(3):256‚Äì279

354
8 Swarm Intelligence
59. Ho S, Ku W, Jou J et al (2006) Intelligent particle swarm optimization in multi-objective
problems, In: Ng WK, Kitsuregawa M, Li J et al (eds) Advances in knowledge discovery and
data mining. Springer, Berlin Heidelberg New York, pp 790‚Äì800
60. Reyes-Sierra M, Coello Coello CA (2006) Multi-objective particle swarm optimizers: A sur-
vey of the-state-of-the-art. Tech. rep., CINVESTAV-IPN, Mexico
61. Hastings E, Guha R, Stanley K (2009) Interactive evolution of particle systems for computer
graphics and animation. IEEE Trans Evol Comput 13(2):418‚Äì432
62. O‚ÄôNeill M, Brabazon A (2008) Self-organising swarm (SOSwarm). Soft Comput 12:1073‚Äì
1080
63. del Valle Y, Venayagamoorthy G, Mohagheghi S et al (2008) Particle swarm optimization: Ba-
sic concepts, variants and applications in power systems. IEEE Trans Evol Comput 12(2):171‚Äì
195
64. Li S, Wu X, Tan M (2008) Gene selection using hybrid particle swarm optimization and ge-
netic algorithm. Soft Comput 12(11):1039‚Äì1048
65. Rahimi-Vahed AR, Mirghorbani SM, Rabbani M (2007) A new particle swarm algorithm for
a multi-objective mixed-model assembly line sequencing problem. Soft Comput 11(10):997‚Äì
1012

Chapter 9
ArtiÔ¨Åcial Immune Systems
Abstract In previous chapters, we introduced GAs, which mimic the natural evolv-
ing process, and ACO and PSO, which mimic the feeding behavior of an ant colony
and the bird Ô¨Çock, respectively. Scientists in the EA Ô¨Åeld are used to taking these bi-
ological or physiological ‚Äúintelligent‚Äù phenomena as metaphors to excite the imagi-
nation for generating algorithms for learning and optimizing problems. In this chap-
ter, we will introduce a concept and a procedure borrowed from the human (verte-
brate) immune system and focus on how to apply these ideas to implementing our
algorithms for various applications.
9.1 Introduction
The human immune system is such a complicated system that not all cause-and-
effect connections have been revealed. The basic function for the immune system
is to identify and destroy foreign substances, denoted as pathogens or invaders, in-
cluding viruses, bacteria, fungi, and parasites. Self-malfunction cells, such as cancer
cells, are also targets of the immune system.
These pathogenic substances are recognized by special structure molecules, such
as polypeptide or nucleotide segments, on their surfaces. These pathogenic sub-
stances are known as antigen (Ag).
The human body has two sets of immune systems: the innate immune system and
the adaptive (acquired) immune system. The former is for general defense and has
no preference for any speciÔ¨Åc antigen and the latter responds to speciÔ¨Åc antigens.1
We will focus on the acquired immune system in this chapter.
We can regard the cells in our immune system as soldiers with various functions,
e.g., scouters, logistics soldiers, advisors, and attackers. The two most important
soldiers of them are T cells (with three types: TC, TH, TS) and B cells. Both are
1 The adaptive immune system could adapt itself to various antigens and this adaptation is acquired
after birth.
355

356
9 ArtiÔ¨Åcial Immune Systems
generated by the bone marrow. They are lymphatic cells, which are the white blood
cells, circulating in the blood and lymphatic vessels.
Some pathogenic substances in the human body will be swallowed by phago-
cytes. Then their antigens are represented on these cells.2 Cytotoxic T (TC) cells
that match the antigen are then proliferated and differentiated to kill the pathogenic
substances. This procedure is called cell-mediated immunity. Matched B cells are
also proliferated and differentiated, and then secrete antibodies (Ab),3 into body
Ô¨Çuids. Helper T cells (TH) can help the procedure of B cell proliferation and dif-
ferentiation. These antibodies bind with antigens to mark a tag. Then lymphocytes,
phagocytic cells, and the complement system are responsible for eliminating the
marked antigenic substances. This procedure is called humoral immunity. After the
invader are eliminated, suppressor T (TS) Cells are activated to suppress the stimu-
lated B cells.4 Some of the stimulated B cells and T cells in both humoral immunity
and cell-mediated immunity can exist inside the immune system for a certain period
and be prepared for the second invasion by a similar substance. The procedure from
the intrusion to the elimination of the pathogen is called the immune response.5
In the following three sections, we will illustrate three main ways of mimicking
human immune systems, resulting in an artiÔ¨Åcial immune system (AIS).
According to de Castro and Timmis‚Äô suggestion, the AIS framework contains
three basic elements, corresponding to three questions that need to be answered
before designing an AIS [3].
‚àôRepresentation. How does one represent antigens and antibodies in an AIS,
which is equivalent to the code scheme in a GA?
‚àôEvaluation. How does evaluate the binding effects of antibodies on antigens and
the interactions between antibodies, which is equivalent to the Ô¨Åtness evaluation
in GAs?
‚àôImmune principle. What are the immune principles applied in AIS to direct
learning and optimization, which is equivalent to the selection process and vari-
ation operators in GAs?6
We need to note here that AIS has by no means been an attempt to directly model
all of the processes occurring within human (vertebrate) immune system. Instead,
however, certain disciplines have been simpliÔ¨Åed and modeled in order to replicate
some of the mechanisms of immune response that might best be suited to manipu-
lating data, learning general patterns, or optimizing problems.
2 So these cells are called antigen-presenting cells (APCs).
3 Sometimes an antibody is also called a receptor, sensor, or detector.
4 TC cells are also stimulated and suppressed by TH cells and TS cells.
5 For more information on immunology knowledge, please refer to Kindt et al.‚Äôs book [1] and
Murphy et al.‚Äôs book [2].
6 In the subsequent sections, we will introduce three principles, i.e., clonal selection, immune
network, and negative selection.

9.2 ArtiÔ¨Åcial Immune System Based on Clonal Selection
357
Figure 9.1 illustrates the number of papers indexed by the SCI on AIS,7 which
demonstrates that AIS attracts a lot of attention every year.












	



	






 




	



 



Fig. 9.1 Number of papers indexed by SCI on AIS
9.2 ArtiÔ¨Åcial Immune System Based on Clonal Selection
9.2.1 Clonal Selection
We will only consider humoral immunity in this section. Every antigen has several
characteristic protein segments, denoted as epitopes, which is illustrated as differ-
ent geometrical shapes on the surface of the antigen in Fig. 9.2. This characteristic
makes it possible for different B cells to recognize an antigen.
Every B cell has many receptors (antibodies), whose magnitude is about 105, on
its surface with the same protein segment, i.e., one B cell only has one type of re-
ceptor. The part responsible for binding with the epitope of the antigen is called a
paratope. Receptors also have a part, also called an epitope, that is possibly recog-
nized and bound by other receptors.8
The human body has about 107 to 108 kinds of receptors, each of which has
different match effects on antigens. The evaluation of the match is called an afÔ¨Ånity.9
As can be seen in Fig. 9.2, B cell1 has a better match effect than B cell2, i.e., the
afÔ¨Ånity measure of B cell1 is higher than that of B cell2.
7 TS = (‚ÄúartiÔ¨Åcial immune system‚Äù OR ‚ÄúartiÔ¨Åcial immune systems‚Äù). The SCI index ‚ÄúTS‚Äù is for
the search topic in the title, the keywords, and the abstract.
8 For simplicity, we neglect the Y-shaped protein structure of the receptor and suppose that every
receptor has only one paratope and one epitope.
9 According to Fig. 9.2, this match is a complementary form. For example, if we could use binary
code to represent one antigen as (0010), then the receptor (1101) is the one with the highest afÔ¨Ånity.

358
9 ArtiÔ¨Åcial Immune Systems













%
:
	
2	

.
	
	
:
'(
Fig. 9.2 Illustration of the binding of receptors to an antigen
The clone selection procedure can be roughly illustrated as follows:
1. The invader enters the human body and is swallowed by phagocytes.
2. The antigen of the invader is represented on the surface of the APC and then
activates B cells.
3. B cells, with the help of TH cells, proliferate (clone) according to to their afÔ¨Ån-
ity measure, i.e., the clone number of one B cell is proportional to its afÔ¨Ånity
measure. The concentration of the Ô¨Åtter B cells increases faster.
4. The clones differentiate (mutate) according to their origin‚Äôs afÔ¨Ånity measure, i.e.,
the clones of the Ô¨Åtter origin will suffer less mutation. This procedure is called
somatic hypermutation.10
5. The mutants are plasma cells, which is responsible for secreting a lot of spe-
ciÔ¨Åc antibodies. The antibodies will break away from the plasma cells, become
free antibodies, and circulate in the body Ô¨Çuid. These B cells, and their secreted
antibodies, constitute the repertoire.
6. antibodies mark antigens (thus mark the antigenic substances).
7. Lymphocytes, phagocytic cells, and the complement system eliminate the marked
antigenic substances.
8. Some of the mutants with a high afÔ¨Ånity measure will be memorized in the body.
9. The above procedure is called a primary immune response. After that, if a simi-
lar antigenic substance enters the human body again, the memory B cells will be
quickly activated at a high stimulation level so that the human body can effec-
tively respond to the recognized or similar pathogen. This procedure is called a
secondary immune response.
Every memory B cell can monitor some types of antigens, so immunologists have
suggested that a Ô¨Ånite number of antibodies could protect us from inÔ¨Ånite antigens.
If we could imagine that memory B cells have different shapes corresponding to
their ability to eliminate different antigens, The B cells construct the shape space
that encompass (could respond to) all the possible antigens.
10 The name comes from the fact that the mutation happened inside of the body and the mutation
rate is higher than that of ordinary mutation.

9.2 ArtiÔ¨Åcial Immune System Based on Clonal Selection
359
9.2.2 Clonal Selection Algorithm
De Castro and Von Zuben utilized some parts of the above clonal selection principle
and proposed the clonal selection algorithm (CSA) in 2002 [4]. CSA can be used in
both unsupervised learning and optimization problems. We will only introduce the
optimization version of CSA here.11
Before illustrating the CSA, it is better to explain the terms used in AIS and
compare them to those of GA, as in Table 9.1.
Table 9.1 Comparison of the corresponding terms in AIS and GA







8



%	
*
	%
/ !	'	<
.		
%	 
%-<
8
%	
	 (
1	3
''	(

5	
2
	

.	
	(
		
		

	
	
	

$


/	-%<
%


(
	
2%
	':C
	
	( (	
%
$

As shown in Table 9.1, the antigen could be an objective function, patterns need-
ing to be recognized, or training data for optimizing, unsupervised learning, and
supervised learning problems.12
Let us consider the maximum problem g(x). De Castro and Von Zuben use binary
code to represent antibodies, i.e., l = 22 binary genes for one variable. The solution
process of the optimization version, clonal selection algorithm (CLONALG), can
be illustrated as follows:
11 Interested readers are referred to [4] for learning problems.
12 These terms were introduced in Sect. 1.1.

360
9 ArtiÔ¨Åcial Immune Systems
Optimization Version of the Clonal Selection Algorithm (CLONALG)
Phase 1: Initialization.
Step 1.1: Assign the parameters for CLONALG, such as repertoire
popsize, stop criteria (such as maxgen), clone factor Œ≤, etc.
Step 1.2: Generate popsize uniformly distributed antibodies randomly
to form the initial repertoire and evaluate their afÔ¨Ånity measures, i.e., Ô¨Åtness
values. gen = 0.
Phase 2: Main loop. Repeat the following steps until the stop criteria are
satisÔ¨Åed (such as gen > maxgen).
Step 2.1: Every antibody generates round (Œ≤ ‚ãÖpopsize) identical
clones. Here round( ) is the round function toward the nearest integer. Thus
make the repertoire size after cloning, Nc, as follows. It is necessary to men-
tion that optimization version of CLONALG does not represent the afÔ¨Ånity
measure dependent characteristics but the learning version of CLONALG
does.
Nc =
popsize
‚àë
i=1
round (Œ≤ ‚ãÖpopsize)+ popsize
(9.1)
Step 2.2: The clones perform somatic hypermutation, i.e., the muta-
tion extent of the Ô¨Åtter clone is smaller, while the popsize original antibodies
are kept unchanged. This step is also called an afÔ¨Ånity maturation process.
Step 2.3: Determine the afÔ¨Ånity measures of the mutants.
Step 2.4: Select the popsize antibodies with the highest afÔ¨Ånity mea-
sures among Nc antibodies and discard the others. gen = gen+1.
Phase 3: Submit the Ô¨Ånal popsize antibodies as the results of CLONALG.
Compared to the solution process of GA on page 22, we can summarize the follow-
ing characteristics of CLONALG.
‚àôCLONALG does not have a crossover operator.
‚àôThe clone step is the most distinctive property of CLONALG.
‚àôThere is no parameter pm to control the probability of mutation for every gene,
like in GA. Instead, every clone suffers from mutation to a different extent pro-
portional to its afÔ¨Ånity measure.
‚àôCLONALG has an elitism mechanism, i.e., popsize original antibodies are kept
unchanged.
According to the above analysis, CLONALG might be roughly regarded as a
parallel version of (1+round (Œ≤ ‚ãÖpopsize))-ES with adaptive mutation control. De
Castro and Von Zuben applied CLONALG to multimodal optimization and TSP and
got satisfactory results.

9.2 ArtiÔ¨Åcial Immune System Based on Clonal Selection
361
9.2.3 ArtiÔ¨Åcial Immune System for Multiobjective Optimization
Problems
In 2005 Coello Coello and Cort¬¥es developed a multiobjective immune system algo-
rithm (MISA) based on a clonal selection principle [5]. For convenience, we rewrite
the constrained MOP model as follows:
min {z1 = f1 (x),z2 = f2 (x),‚ãÖ‚ãÖ‚ãÖ,zm = fm (x)}
s.t.
gi (x) ‚©Ω0,
i = 1,‚ãÖ‚ãÖ‚ãÖ,q
h j (x) = 0,
j = q+1,‚ãÖ‚ãÖ‚ãÖ,k
x ‚àà‚Ñùn
(9.2)
Coello Coello and Cort¬¥es used binary strings to represent antibodies and also
adopted the archive to store the nondominated solutions found thus far. The non-
dominated antibodies found thus far is stored in the hyperboxes illustrated in Sect.
6.6.2.1. We know the squeeze factor of each hyperbox.
In order to coordinate the multiple objectives and constraints, Coello Coello and
Cort¬¥es assign preferences in the current repertoire by appointing feasible Pareto (FP)
antibodies as rank 1, feasible dominated (FD) antibodies as rank 2, infeasible Pareto
(IP) antibodies as rank 3, and infeasible dominated (ID) as rank 4.13
The solution process of MISA can be illustrated as follows:
Multiobjective Immune System Algorithm
Phase 1: Initialization.
Step 1.1: Assign the parameters for MISA, such as repertoire size
popsize, archive size asize, stop criteria (such as maxgen), grid number divi
for variable i in the archive, binary length l for each variable, etc.
Step 1.2: Generate popsize uniformly distributed antibodies randomly
to form the initial repertoire and evaluate their afÔ¨Ånity measures and feasibil-
ities. gen = 0.
Step 1.3: The initial archive is empty.
Phase 2: Main loop. Repeat the following steps until the stop criteria are
satisÔ¨Åed (such as gen > maxgen).
Step 2.1: Assign the rank in the current repertoire. Pareto dominance
is determined only among the same class, i.e., a feasible antibody is only
compared with other feasible antibodies for Pareto dominance.
Step 2.2: Select the ‚Äúbest‚Äù antibodies. If the FP antibodies number
is more than 5% popsize, just select all rank 1 antibodies. Otherwise select
some rank 2 antibodies that are dominated by less other feasible antibodies
until the ‚Äúbest‚Äù antibodies‚Äô size reaches 5% popsize. If the number of all FP
13 Similar to the idea in Sect. 6.9.

362
9 ArtiÔ¨Åcial Immune Systems
and FD antibodies is still less than 5% popsize, select some rank 3 antibodies
with a lesser constraint violation. If they still do not reach 5% popsize, rank
4 antibodies, which are dominated by less other antibodies, are selected. The
result of this step is at least 5% popsize of the relatively ‚Äúbest‚Äù individuals in
the current repertoire.
Step 2.3: Update the archive using the ‚Äúbest‚Äù antibodies.
Substep 2.3.1: Determine the Pareto dominance relationship be-
tween the archive members and the ‚Äúbest‚Äù antibodies. The dominated antibod-
ies cannot enter the archive.
Substep 2.3.2: If the archive is not full, insert all the nondomi-
nated ‚Äúbest‚Äù antibodies.
Substep 2.3.3: Otherwise, only the ‚Äúbest‚Äù nondominated antibod-
ies, which do not belong to the most crowded hyperbox, are allowed to enter
the archive. Whenever an antibody is inserted into the archive, randomly elim-
inate a member in the most crowded hyperbox.
Substep 2.3.4: Adaptively maintain the hyperbox using tech-
niques discussed in Sect. 6.6.2.1. Calculate the average squeeze factor of the
archive after all the ‚Äúbest‚Äù antibodies have been inserted or rejected.
Step 2.4: Clone the ‚Äúbest‚Äù antibodies with the help of the archive.
Substep 2.4.1: The total estimated clone number is 6√ó popsize.
The clones are initially evenly distributed among these ‚Äúbest‚Äù antibodies.
Substep 2.4.2: If the archive is not full, determine the Euclidean
distances between these ‚Äúbest‚Äù antibodies and get the average distance of ev-
ery ‚Äúbest‚Äù antibody and the overall average distance among them. If the av-
erage distance of one antibody is smaller than the overall average distance,
which means it belongs to the more crowded region, its clone number is re-
duced by half. Conversely, if the average distance of one antibody is larger
than the overall average distance, which means it belongs to the less crowded
region, its clone number is increased by 50%. Those whose average distance
is equal to the overall average distance are kept unchanged.
Substep 2.4.3: If the archive is full, there are three possible cases.
Substep 2.4.3.1: If the ‚Äúbest‚Äù antibody is rejected in sub-
steps 2.3.1 and 2.3.2 above, its clone number is zero.
Substep 2.4.3.2: If the ‚Äúbest‚Äù antibody belongs to the less
crowded hyperbox, i.e., its squeeze factor is smaller than the average squeeze
factor, double its clone number.
Substep 2.4.3.3: If the ‚Äúbest‚Äù antibody belongs to the more
crowded hyperbox, i.e., its squeeze factor is larger than the average squeeze
factor, its clone number is reduced by half.
Step 2.5: Perform a uniform somatic hypermutation for the clones. FP
antibodies will suffer from n bit-Ô¨Çip mutations, where n is the number of vari-
ables in Eq. 9.2. FD antibodies will suffer from n + 1 bit-Ô¨Çip mutations. The
number of mutations for IP and ID antibodies is n+2 and n+3, respectively.

9.2 ArtiÔ¨Åcial Immune System Based on Clonal Selection
363
Step 2.6: Perform nonuniform mutation for other ‚Äúnot-so-good‚Äù an-
tibodies. Here ‚Äúnonuniform‚Äù has the different meaning to the one introduced
in Sect. 3.2.2.3. Coello Coello and Cort¬¥es designed a deterministic control for
the number of bit-Ô¨Çip mutations in an antibody, denoted as nm, as follows:
nm = 0.6L+
gen
maxgen
(1
L ‚àí0.6L
)
where L = nl is the total binary chain length.
Step 2.7: Combine the mutants of steps 2.5 and 2.6. Rank them and
select popsize antibodies using techniques introduced in Step 2.2.
Phase 3: Submit the Ô¨Ånal popsize antibodies as the results of MISA.
If we can understand rank as the afÔ¨Ånity measure in AIS, MISA has a tendency
toward proportional proliferation and differentiation in steps 2.2‚Äì2.6. Figure 9.3
illustrates the evolving process in one generation of MISA.


















!13


"13
5.
5,
.
2F
,

R/	
S




		
6(
C
		


		


		
2F

5.

5,

.
,


"1=3
?	
*
R:	S
Fig. 9.3 The evolving process in one generation of MISA
It is not difÔ¨Åcult to determine that MISA satisÔ¨Åes the three requirements (con-
vergence, distribution, and elitism) in Sect. 6.4. So it possesses the basics of a good
MOEA. Coello Coello and Cort¬¥es compared MISA with NSGA-II, PAES, and mi-
croGA14 using performance indices ER, SP, and GD.15 The comparison results sug-
gested that MISA is ‚Äúa viable alternative‚Äù to these MOEAs.
14 Introduced in Sects. 6.5.1, 6.5.3, and 6.5.4, respectively.
15 Introduced in Sect. 6.7.2.

364
9 ArtiÔ¨Åcial Immune Systems
9.3 ArtiÔ¨Åcial Immune System Based on Immune Network
9.3.1 Immune Network Theory
In Sect. 9.2, we introduced the basic theory about the protective function of the hu-
man immune system and its metaphors for AIS. This and the next section will dis-
cuss the self-regulation of the immune system so that lymphatic cells can remember
the invading antigens and avoid destroying the self cells.
After one type of antigen enters the human body, it causes an immune response
and is eliminated by antibodies. Some of the matching B cells with a high afÔ¨Ån-
ity measure will go into a dormant state, waiting for the possibility of a second
response. In a real immune system, antigens are sometimes remembered for long
periods, in some cases equal to the lifespan of the human. why such a long time
memory? Do the memory B cells have extra long life without being stimulated?
Does bone marrow have the ability to generate the same memory B cells again and
again? The idiotypic network theory,16 suggested by Jerne in 1974 [6], gives us one
way to explain this phenomenon.
As has been illustrated roughly in Fig. 9.2, every receptor, with its epitope, on B
cell can also be matched by other receptors, with their paratopes. Without any anti-
gen, those recognizing other B cells might be stimulated, proliferated, and differenti-
ated, and those recognized by other B cells might be suppressed or even eliminated.
Farmer et al. simpliÔ¨Åed Jerne‚Äôs theory in 1986 [7] and Perelson further developed
this theory in 1989 [8]. Since then, the immune network theory has became one of
the most important principles in AIS.17
Suppose every antibody type has only one type of epitope and one type of id-
iotope, and these protein segments can be encoded by binary strings. We use le for
the length of epitope, lp for the length of paratope, and s for the threshold for stim-
ulating proliferation and differentiation. Then we can compare the complementary
matching situation for every two antibodies, i.e., the extent of antibody j matching
antibody i, as follows:
mi, j = ‚àë
k
G
(
‚àë
n
ei (n+k)‚àßp j (n)‚àís+1
)
(9.3)
where n is for the comparing gene, k is for the left-shifting step, p j(n) is the nth gene
of antibody j‚Äôs paratope, ei(n+k) is the (n+k)th gene of antibody i‚Äôs epitope, and
‚àßis the result of complementary matching.18 G(x) = x for x > 0 and G(x) = 0 other-
wise. We can illustrate the simple example of le = lp = 6 and s = 3 in Fig. 9.4. With
16 Different antibody types may have epitopes in common. An epitope that is unique to a given
antibody type is called an idiotope.
17 The immune network theory is controversial among immunologists. However, the lack of accep-
tance of the immune network hypothesis by immunologists has not stopped AIS researchers from
adapting it for their own use. The reason for this, we guess, lies in the fact that, as engineers and
scientists, we like formulas, differential equations, and equilibrium points.
18 1‚àß0 = 1, 0‚àß1 = 1, 1‚àß1 = 0, and 0‚àß0 = 0.

9.3 ArtiÔ¨Åcial Immune System Based on Immune Network
365
these parameters and considering the deÔ¨Ånition of G(x), we can easily determine
that a possible value of k is {‚àí3,‚àí2,‚àí1,0,1,2,3}.19
















)    ) 

  ) ) ) 

  ) ) ) 

  ) ) ) 

  ) ) ) 

  ) ) ) 

  ) ) ) 

 

)
+
/
 
 

)
+
/
 
 


+
/
 
 
)

+
/
 
 

)
+
/
 
 

)
+
/
 




  ) ) ) 

 

)
+
/
 
Fig. 9.4 Illustration of the antibody-antibody matching in an immune network. The total extent of
antibody j matching antibody i is mij = 4
Larger mij means that antibody j matches antibody i more and antibody j will
be stimulated and antibody i will be suppressed. Farmer et al. suggested the follow-
ing differential equation, i.e., the state function of state variable xi, to illustrate the
dynamic process among antibodies and antigens.
dxi
dt = c
(
N
‚àë
j=1
m jixix j ‚àík1
N
‚àë
j=1
mijxixj +
n
‚àë
j=1
m jixiyj
)
‚àík2xi
(9.4)
where xi is the concentration of antibody i, y j is the concentration of antigen j. k1,
k2, and c are all positive control parameters. According to Eq. 9.4, the number of
antibody i will increase if it matches more other antibodies, is matched by fewer
other antibodies, and matches more antigens.
Equation 9.4 suggests an explanation for the long life of memory B cells. If,
under some conditions, the right part of Eq. 9.4 equals zero, the number of antibody
i will be kept unchanged. That means antibody i will continue to exist in the dynamic
stimulation and suppression environment.
Since the introduction of the immune network theory to AIS, many variants have
been suggested and currently no one has been able to unify them into one frame-
work. Some of them utilize both the clone selection principle and the immune net-
work theory. In the following part of this section, we will introduce one type of
continuous immune network and one type of discrete immune network. The general
form of the dynamic process in the immune network can be illustrated as follows:
19 k < ‚àí3 or k > 3 will not provide G() > 0.

366
9 ArtiÔ¨Åcial Immune Systems
Œîxi = (Ab i generated)+(Ab i stimulated)‚àí(Ab i suppressed)‚àí(Ab i naturally died)
(9.5)
9.3.2 Continuous Immune Network
In this subsection, we will introduce an interesting research Ô¨Åeld: collaborative Ô¨Ål-
tering (CF) and how the immune network might contribute to CF.
CF is about giving predictions or recommendations according to other similar
behaviors. For example, suppose that you are an Internet-based store owner. You
have a lot of customers‚Äô proÔ¨Åles. If you could use one customer‚Äôs historical con-
sumption habits and those of others, who share similar tastes, to give appropriate
purchase recommendations, your sales volume might increase signiÔ¨Åcantly. These
techniques are CF.
In 2002 Cayzer and Aickelin suggested a movie score prediction and recommen-
dation system based on the continuous immune network [9, 10]. Suppose we need
to predict viewer u‚Äôs voting score for L unseen movies with the help of u‚Äôs previous
voting scores for other movies and N other viewers‚Äô voting scores for these movies.
The most common way to do CF is through a correlation coefÔ¨Åcient among dif-
ferent viewers. Every viewer‚Äôs voting scores for M total movies can be encoded as
in Fig. 9.5.












 
 


,
5 5 5
5,
9

Fig. 9.5 The code for movie recommendation system
where scorei might be {0,0.2,0.4,0.6,0.8,1.0,‚àí}. A larger voting score value
means more favorable for a given movie. ‚àíis for unseen movies.
Then the similarity measure, i.e., correlation coefÔ¨Åcient, of viewer u and v can be
calculated as follows:
ruv =
n
‚àë
i=1
(ui ‚àí¬Øu)(vi ‚àí¬Øv)
‚àön
‚àë
i=1
(ui ‚àí¬Øu)2 n
‚àë
i=1
(vi ‚àí¬Øv)2
(9.6)
where n is the number of movies both u and v have voted on, ui and vi are the
voting scores of u and v for movie i, respectively, and ¬Øu and ¬Øv are the average voting
scores of u and v for these n movies. The correlation coefÔ¨Åcient between u and v
deÔ¨Ånes their extent of linear correlation, i.e., the extent to which u and v are linearly

9.3 ArtiÔ¨Åcial Immune System Based on Immune Network
367
correlated. The largest value r = 1 means that u and v share the very same taste in
movies. The smallest value r = ‚àí1 means that u and v have complete opposite tastes
in movies. If r = 0, then u and v have no common interests in movies. Obviously,
in the N other viewers, we should prefer those whose similarity measures with u are
large (absolute value).
After we get all the similarity measures for the N other viewers, the k ones with
the highest absolute similarity measures are selected as the neighbors of u to predict
the voting scores of the L unseen movies for u.20 And then the predicted voting
score of u for movie l is as follows:
pl = ¬Øu+
‚àë
v‚ààneighbor
ruv (vl ‚àí¬Øv)
‚àë
v‚ààneighbor
ruv
(9.7)
Equation 9.7 is a weighted average of k neighbors. The one with a larger cor-
relation coefÔ¨Åcient has more inÔ¨Çuence on the deviation from the average. In this
way, the opinions of those with opposite tastes in movies with u can also be taken
into consideration. After we get the L predicted voting scores for u, we can give
recommendations for u from higher predicted voting scores.
In Cayzer and Aickelin‚Äô immune network, there is one antigen, i.e., u, and N
antibodies, i.e., other viewers. After getting all the similarity measures for u by Eq.
9.6, we can use an immune network to generate neighbors.
We deÔ¨Åne the concentration of antibody i and the antigen as xi and y, respectively.
Then the state function for antibody i in the immune network can be modiÔ¨Åed, from
Eq. 9.4, as follows:
dxi
dt = (Ag stimulation)‚àí(Ab suppression)‚àí(death rate)
= k1mixiy‚àík2
N
N
‚àë
j=1
mijxixj ‚àík3xi
(9.8)
where mij = ‚à£rij‚à£, mi = ‚à£rui‚à£. k1, k2, k3 are positive control factors representing stim-
ulation, suppression, and death rate, respectively.21
N
‚àë
j=1
mi jxixj/N is the average ab-
solute similarity measure of antibody i. The larger, the more crowded. Equation 9.8
considers the suppression effect between antibodies, i.e., viewers with higher simi-
larity measures to u and lower average absolute similarity measures are preferred.
The initial concentration of antibodies is set at 10. Then they evolve according
to N differential equations. If the concentration value of one antibody is smaller
than a predeÔ¨Åned threshold, it is discarded from the immune network. The evolving
process stops until the size of the immune network does not change for the succes-
20 For movie l, we only select k highest absolute similarity measures among those who have seen l.
21 Cayzer and Aickelin Ô¨Åxed k3 = 0.1 and determined the best value for k1 = 0.5 and k2 = 0.3 with
numerical experiments.

368
9 ArtiÔ¨Åcial Immune Systems
sive ten iterations, i.e., the immune network converges. Then these antibodies whose
concentration values are larger than a threshold are selected as the neighbors of u.
We can imagine that u is a point in 3-D space and the neighbors found by the
immune network might be distributed evenly within the ball surrounding u. The
suppression part of Eq. 9.8 requires that these neighbors not be too close to each
other, and so may contribute a more accurate prediction and recommendation.
The prediction equation could also consider the inÔ¨Çuence of the concentration
value as follows, i.e., an antibody with a larger concentration value has more right
to vote.
pl = ¬Øu+
‚àë
v‚ààneighbor
xvruv (vl ‚àí¬Øv)
‚àë
v‚ààneighbor
xvruv
(9.9)
where xv is the concentration value of neighbor v when the immune network stops
evolving.
By comparing the immune-network-based CF and traditional CF, Cayzer and
Aickelin concluded that these two methods have similar prediction accuracies, while
the former has a better average accuracy of recommendations.
9.3.3 Discrete Immune Network
In this subsection, we will introduce an example of a discrete immune network for
unsupervised learning (clustering). We talked about clustering several times in pre-
vious chapters. The most common methods are k-means clustering and hierarchi-
cal clustering [11‚Äì15]. These techniques classify data into groups according to the
original data, which make the pattern characteristics in clusters hard to reveal. Also,
users are generally required to assign the cluster number as a predeÔ¨Åned parameter
for these approaches.
Resource Limited ArtiÔ¨Åcial Immune System
In 2000 Timmis et al. proposed an artiÔ¨Åcial immune network for data analysis [16]
and then, in 2001, developed it into the resource limited artiÔ¨Åcial immune system
(RLAIS) [17]. Other more recent reports are still available [18, 19]. These methods
vary in their description. We will only introduce the RLAIS here [17], which uses
real code representation.
In RLAIS, antibodies are not encoded and evolved directly. The entity to be
evolved is the artiÔ¨Åcial recognition ball (ARB). ARBs form the immune network.
Each ARB can represent a certain number of antibodies according to its stimula-
tion level. Higher stimulated ARBs can acquire more resources, i.e., antibodies. But
the total resource is limited, which means the requirement of some ARBs cannot

9.3 ArtiÔ¨Åcial Immune System Based on Immune Network
369
be fulÔ¨Ålled. This situation causes lower stimulated ARBs to be removed from the
repertoire. The concept of ARBs skilfully solves the problem of data compression,
i.e., using a small number of ARBs to represent a large number of antigens, i.e., data
needing to be clustered.
Before clustering, the data are normalized so that the Euclidean distances be-
tween every two points are within the scale [0,1]. The solution process of RLAIS
can be illustrated as follows:
Resource Limited ArtiÔ¨Åcial Immune System
Phase 1: Initialization.
Step 1.1: Assign the parameters for RLAIS, such as total resource
number mb, clone and resource distribution parameter k, stop criteria (such as
maxgen), mutation probability pm, network afÔ¨Ånity threshold NAT, etc.
Step 1.2: Randomly copy several distinct data into the immune net-
work as initial ARBs to form the initial repertoire. Calculate their Euclidean
distances. If the distance between two ARBs is smaller than NAT, a link is set
up between them.
Phase 2: Main loop. Repeatedly train the immune network with one da-
tum taken from the original data set, which are denoted as antigens, until the
stop criteria are satisÔ¨Åed (such as gen > maxgen or the size, connection, and
location of ARBs have not changed for several generations). Every original
datum, except the ones taken as the initial ARBs, could either be used once or
more than once, depending on users‚Äô intention.
Step 2.1: Calculate the stimulation level for ARB j with the following
consideration:
sl j =
a
‚àë
k=1
(1‚àídAg (k, j))+
n
‚àë
i=1
(1‚àíd (i, j))‚àí
n
‚àë
i=1
d (i, j)
(9.10)
where a is the number of antigens ARB j has been exposed to, dAg (k, j) is
the Euclidean distance between antigen k and ARB j, n is the number of links
to ARB j, and d (i, j) is the Euclidean distance between linked ARB i and
ARB j. The Ô¨Årst, second, and third parts of Eq. 9.10 represent, respectively,
the primary stimulation, caused by antigen, second stimulation, caused by
other ARBs, and suppression, caused by other ARBs. Equation 9.10 suggests
that the ARBs that are close to an antigen and are closely linked get a high
stimulation level.
Step 2.2: Clone every ARB to identical k √ó sl copies, where sl is its
stimulation level. This step represents the proportional clone property of the
clonal selection.
Step 2.3: Every clone undergoes mutation with probability pm. Mu-
tants are incorporated into the immune network as ARBs if their distances to

370
9 ArtiÔ¨Åcial Immune Systems
the closest ARB are less than NAT. Then a link is set up between the mutant
and its closest neighbor ARB. Those who do not mutate are discarded because
the immune network already has this pattern.
Step 2.4: Recalculate the stimulation level for every ARB using
Eq. 9.10.
Step 2.5: Distribute the resources, i.e., antibodies, according to the
stimulation level. ARB j can apply for k √ó sl2
j resources. But we only have
mb resources for distribution. A pragmatic way to solve this is to fulÔ¨Åll the
requirement of the ARB with the highest stimulation level, then the second
highest stimulation level ...until all the resources are distributed, i.e., the one
with high stimulation level has more power to vote. This step is very critical
for RLAIS because limited resources is used to restrict the number of the ARB
and encourage those ARBs that are naturally closely existed in the AIS. Those
ARBs without resource are discarded.
Phase 3: The ARBs with links represent a cluster. Then we can easily
Ô¨Ånd out how many clusters are in the original data set and use the Euclidean
distance between the data and the ARBs to determine which datum belongs
to which cluster.
Among these control parameters, NAT is a very critical factor affecting the cluster-
ing results.22 Timmis and Neal test RLAIS with ‚ÄúFisher iris data set‚Äù to verify its
capability.23
The last thing that needs to be mentioned is that the original complementary
match, introduced by Eq. 9.3 and Fig. 9.4, is implemented inversely in some arti-
Ô¨Åcial immune networks. Equation 9.3 means roughly that a high stimulation level
equals a large Hamming distance between two binary chains. But in Eq. 9.6, a high
stimulation level, i.e., large mij, means that two viewers have the same/opposite
taste in movies, i.e., they are close to each other. In Eq. 9.10, the stimulation level
between ARBs, i.e., (1‚àíd (i, j)), is also inverse to distance.
9.4 ArtiÔ¨Åcial Immune System Based on Negative Selection
T cells are a type of lymphatic cells that have the most important role in cell-
mediated immunity. As was introduced in Sect. 9.1, TC cells that match the anti-
gen are then proliferated and differentiated to kill the pathogenic substances. Anti-
22 Small NAT values might correspond to large cluster numbers
23 Asuncion and Newman maintain a Web site ‚ÄúMachine Learning Repository‚Äù containing hun-
dreds of machine learning data sets at the University of California at Irvine (http://archive.
ics.uci.edu/ml/). It might be the most cited benchmark problem source in the Ô¨Åeld of
data mining, machine learning, or pattern recognition. Ultsch also provides a fundamental clus-
tering problems suite via http://www.uni-marburg.de/fb12/datenbionik/data?
language_sync=1.

9.4 ArtiÔ¨Åcial Immune System Based on Negative Selection
371
gens are also proteins with gene structures. How does one differentiate human self
proteins from the pathogenic nonself proteins for T cells? The negative selection
principle gives an explanation.
After being generated by bone marrow, T cells need to undergo the maturation
process in the thymus gland; where newly generated T cells meet various kinds
(about 105) of self proteins. Those whose receptors complementarily match the self
proteins are discarded. Other ‚Äúmatured‚Äù T cells are allowed to enter the lymphatic
system. Then any matching between the receptors of T cells and the proteins will
be recognized as foreign substances and thus stimulate an immune response. This
maturation process is called negative selection (NS).
If we consider computer Ô¨Åles and normal computer network communications as
self cells, and changed Ô¨Åles and abnormal communications as nonself cells. It is
straightforward to use the NS principle in Ô¨Åle protection and intrusion detection,
which are the topics in the following two subsections.24
9.4.1 File Protection by Negative Selection
In 1994 Forrest et al. used NS in Ô¨Åle protection events, caused by viruses or other
accidents [20]. They gave a framework for applying NS to computer security that is
still used by related researchers.
The protection requires two steps: training, similar to the maturation process in-
side the thymus gland, and protecting, similar to the immune response inside the
body. These two steps can be illustrated in Fig. 9.6.
Let us take a simple descriptive example to illustrate these steps. Suppose we
want to protect a very simple binary coded Ô¨Åle (1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0). We
divide this 16-bit Ô¨Åle into 4 segments artiÔ¨Åcially, i.e., (1 0 1 1), (1 0 0 1), (0 0 1 1),
and (0 0 0 0). These segments contain the self sting set (S).
We then randomly generate 4-bit strings to match S. As for the concept of
‚Äúmatch,‚Äù we need to deÔ¨Åne a threshold. If r contiguous bits are the same for the two
strings being compared, we call them ‚Äúmatched‚Äù.25 For example, (1 0 1 1) matches
(1 1 1 1) for r = 2 but does not match for r ‚â•3.
Due to the simplicity of the example, we can enumerate all 4-bit strings for the
training step, illustrated in Fig. 9.6a, and Ô¨Ånally get the detector set (R) as (0 1 1 0)
and (1 1 1 0) for r = 2. This R is like the matured T cells in the human immune
system, which cannot match self proteins (S).
Then we enter the protecting step where the protected self strings are regularly
compared to R. Whenever a match is detected, that means that R has found a modi-
Ô¨Åcation to the protected Ô¨Åle. Then an alarm can be generated for users.
24 It should be mentioned here that NS-based Ô¨Åle protection and intrusion detection is an alternative
solution for the problem, not a substitute for others.
25 We need to mention that the original complementary match in the human body is implemented
inversely here.

372
9 ArtiÔ¨Åcial Immune Systems
















'	
%1.3
2!	
	
*
	


	
%1')3
,		

	1'3
T
4
(a)


















	
T
4
,		

	1'3
.
		
	
%1.3
4

(b)
Fig. 9.6 Steps of NS-based Ô¨Åle protection: (a) training, and (b) protecting
We can examine the effectiveness of NS-based Ô¨Åle protection by single bit-Ô¨Çip
mutations on S. In all 16 possible mutations, our R could match 6.26
Perhaps the reader is disappointed at the protection results of the example be-
cause we can only monitor 37.5% of Ô¨Åle changes. We can explain it in several ways.
Firstly, bit-Ô¨Çip mutation is a very strict condition for Ô¨Åle protection. Generally, Ô¨Åle
changes, due to a virus or other reasons, will cause a larger number of bit changes,
which will alleviate the problem. Another reason lies in the fact that the Ô¨Åle size is
to small. Forrest et al. tested a Ô¨Åle with 512 bytes. They cut it into 128 self 4-byte
strings S and got 46 4-byte strings in R using the training steps in Fig. 9.6a. Then
they generated random changes in S, over 1000 trials, and the 46 detectors could
recognize 84.3% of them (r = 8). Apart from that, viruses generally attach their
codes at the end of a Ô¨Åle, which makes NS-based Ô¨Åle protection easier. Forrest et al.
tested a virus that modiÔ¨Åed the Ô¨Årst 5 bytes of a Ô¨Åle and appended 300 bytes at the
end of the Ô¨Åle, with a small COM Ô¨Åle with 2620 bytes. The NS-based system could
detect the virus with probability 98% using ten detectors (r = 10).
26 The matching detail is left as an exercise.

9.4 ArtiÔ¨Åcial Immune System Based on Negative Selection
373
9.4.2 Intrusion Detection by Negative Selection
Intrusion detection in a computer network is an extremely hot yet difÔ¨Åcult problem
because there are many types of intrusions and many parameters to monitor. In 2002
Dasgupta and Gonzalez utilized NS-based GA to detect abnormal communications
[21]. Due to space constraints, we only introduce the essence of their method and
referr interested readers to the original paper.
Suppose we want to detect the intrusion with two parameters: number of bytes
per second (S1) and number of ICMP packets per second (S2).27 We need samples
of normal communication and can illustrate these self samples on the 2-D space of
S1-S2, like the points in Fig. 9.7, where we have already normalized the data in the
scale [0,1]. The dotted line forms the self region we considered as normal situations,
i.e., like the self cells in the human body.























2
2
2
2
2
2
2"
2
#
2&
2
)


)


Fig. 9.7 illustrative example of NS-based intrusion detection
What we need to do is generate a set of rules to express abnormal network com-
munications. One such rule can be illustrated as follows:
IF S1 is within [0,1] AND S2 is within [0.8,1], THEN nonself.
With these rules, we can monitor the network, and whenever a rule is activated,
the algorithm will notify the administrator for further conÔ¨Årmation. One possible
rule set for intrusion detection is illustrated by the dashed line forming rectangles in
Fig. 9.7.
As can be seen from Fig. 9.7, due to the irregularity of the self data set, some
of these rules might contain self samples, i.e., diagnose the normal communication
as abnormal, and some of the nonself regions are not covered, i.e., no response to
abnormal communications. And some rules might have duplicated covering regions.
27 The speciÔ¨Åc meanings of these two parameters are irrelevant to the following discussion.

374
9 ArtiÔ¨Åcial Immune Systems
So Dasgupta and Gonzalez Ô¨Årst transformed the intrusion detection problem into
a detection rule generation problem and then transformed the latter problem into an
optimization problem.
We actually have several objectives. Firstly, we want fewer rules. This require-
ment is straightforward and intuitive for fast response and small storage require-
ments. So we want a rule to cover as large an area as possible. Also, we want a less
duplicated area between rules for the same reason. The last, but not least, objective
is that we want the normal samples to be covered by the rules as little as possible.
How do we effectively solve the multiple-objective problem?
Dasgupta and Gonzalez skillfully use the sequential niche, introduced in Sect.
5.2, to address the second requirement and combine the Ô¨Årst and the third ones.
Actually, we only need to evolve the ‚ÄúIF‚Äù part of a rule because all the ‚ÄúTHEN‚Äù
parts are ‚Äúnonself.‚Äù So the representation scheme is clear. For the above example
rule, its chromosome could be (0,1,0.8,1).28
The Ô¨Årst objective can be illustrated as follows:
volume(R) =
n
‚àè
i=1
(highi ‚àílowi)
(9.11)
where R is a rule, i.e., a chromosome, n is the dimension of the space, i.e., n = 2
in our example, and highi and lowi are the higher and lower bounds of dimension i,
respectively. Then highi ‚àílowi is the (hyper-)rectangle‚Äôs edge length in dimension
i. So Eq. 9.11 determines the (hyper-)area of the (hyper-)rectangle, i.e., the covering
area of that rule. Surely the larger the better.
The third objective can be illustrated as follows:
error number(R) =
{
xi ‚ààS
		xi ‚ààR
}
(9.12)
where xi is the ith normal sample for training, S is the normal communication set,
and R is a rule. Equation 9.12 counts the number of normal samples covered by the
rule.29 The smaller the better.
The raw Ô¨Åtness of individual R can be illustrated as follows:
raw fitness(R) = volume(R)‚àíC ‚ãÖerror number(R)
(9.13)
where C is a user-deÔ¨Åned parameter for evaluating the penalizing effects of covering
the normal samples. The larger raw fitness, the better.
Then we run the GA for the Ô¨Årst time. It will generate a rule, just like rule 1 in
Fig. 9.7. Then we run the GA again and again to Ô¨Ånd other rules. How does one
avoid Ô¨Ånding the same rule again?
In a sequential niche, we need to modify the objective function after every run of
the GA. Dasgupta and Gonzalez suggested the following modiÔ¨Åcation:
28 The Ô¨Årst two alleles are the lower and higher bounds of the dimension 1, and likewise the next
two alleles for dimension 2.
29 This is where the NS principle is applied. Those rules that cover more normal data will be
discarded; similarly those T cells that match more self proteins will be discarded.

9.5 Summary
375
fitness(R) = raw fitness(R)‚àí
‚àë
Rj‚ààrule set
volume
(
R‚à©R j)
(9.14)
where Rj is the previously found jth rule and volume
(
R‚à©R j)
is the common
(hyper-)area shared by rule R and rule R j. This simple yet powerful expression will
achieve the second objective.30
From the second run of the GA, we use Eq. 9.14 as the Ô¨Åtness objective until
the stop criteria, such as the maximum number of rules or maximum number of GA
runs to generate a rule, are satisÔ¨Åed.
Dasgupta and Gonzalez compared their NS-based intrusion detection with the
positive-characterization-based method31 and found that they have a similar accu-
racy but the former requires much less memory.
For the benchmark problem in intrusion detection, MIT‚Äôs Lincoln Laboratory,
in collaboration with the Defense Advanced Research Projects Agency and the Air
Force Research Laboratory, maintains a Web site of several intrusion detection data
sets. 32
9.5 Summary
AIS is a system, not an algorithm, so it has broader content than other chapters. We‚Äôd
like to repeat that it is not our concern to reproduce immune phenomena by AIS, but
to show that immune concepts can be used to develop powerful computational tools
for data processing and optimization.
To some extent, we can regard the immune response process in our body as the
fast version of a natural evolving process. The Ô¨Åtter lymphatic cells with similarity
measures close to those of nonself cells and different from those of self cells will be
proliferated and differentiated.
Clonal selection might be the most often used immune principle in AIS. It can be
utilized in both optimizing and learning problems. We only focus on optimization
in Sect. 9.2 owing to space constraints.
Immune networks are fascinating, even though the concept is disputed among
immunologists. Thus there are many application variations of this principle to un-
supervised learning.
NS is simple and has thoughts directly applicable to discrimination problems that
are not limited to Ô¨Åle protection and intrusion detections discussed in Sect. 9.4. Any
situation with the requirement of discriminating normal from abnormal, i.e., self
from nonself, might utilize these concepts.
30 Here you might understand why in this textbook we introduce so many different types of con-
siderations for one kind of problem.
31 If the shortest distance of a real-time sample to the trained normal samples is less than the
predeÔ¨Åned threshold, it is an abnormal communication.
32
http://www.ll.mit.edu/mission/communications/ist/corpora/
ideval/index.html

376
9 ArtiÔ¨Åcial Immune Systems
After reading this chapter, you should be familiar with applications of ideas about
the human immune response process to AIS design, understand the intuitive consid-
erations of clonal selection for optimization, immune network for clustering, and
NS for nonself recognition, and enjoy the various successful applications of these
simple ideas.
Suggestions for Further Reading
Since the introduction of AIS, many researchers have published monographs and
textbooks on the topic. Interested readers are referred to Dasgupta‚Äôs book from 1998
[22], de Castro and Timmis‚Äôs book from 2002 [3], Tarakanov et al.‚Äôs book from
2003 [23], and Ishida‚Äôs book in 2004 [24]. A tutorial on AIS was given by Dasgupta
in 2006 [25].
There is an annual conference on AIS, the International Conference on ArtiÔ¨Å-
cial Immune Systems, and its proceedings are published by Springer. Aitkin et al.
maintain a Web site containing plenty of information on AIS.33
In 2005 Garrett gave the criteria ‚Äúdistinctiveness‚Äù and ‚Äúeffectiveness‚Äù for evaluat-
ing AIS and surveyed the ordinary AIS models [26]. Timmis and colleagues recently
published several review papers [27‚Äì29].
The AIS introduced in this chapter only contains mutation operators. Bersini
incorporated chemical crossover operator into AIS in 2002 [30].
We only introduced the optimization version of CLONALG in Sect. 9.2.2. Those
interested in exploratory data analysis are referred to the papers published by Wu
and Fyfe in 2008 [31] and Do et al. in 2009 [32], in addition to [4]. In 2008 Lau et
al. provided a parallel immune optimization algorithm [33].
In 1997, entropy-based concentration was suggested and applied by Chun et al.
to AIS to further improve its optimization ability [34]. CLONALG has been suc-
cessfully applied in protein structure prediction, e.g., work done by Anile et al. and
Cutello in 2007 [35, 36], and in the optimal computation of Ô¨Ånite Ô¨Åeld exponentia-
tion, e.g., work done by Cruz-Cort¬¥es et al. in 2008 [37].
Immune-network-based learning is an attractive research Ô¨Åeld. Apart from the
two examples introduced in Sect. 9.3, in 2001 de Castro and Zuben also included the
stimulation and suppression idea among antibodies into CLONALG and suggested
the artiÔ¨Åcial immune network for data analysis (aiNet) [38]. Like CLONALG, aiNet
also has a learning version for clustering [39] and an optimization version for mul-
timodal optimization [40, 41]. In 2004 a branch RLAIS, introduced in Sect. 9.3.3,
for supervised learning was published by Watkins and Timmis [42]. For further dis-
cussions on immune networks, readers can refer to Ishida‚Äôs previous work on active
noise control and diagnosis from 1996 and 1997, respectively [43, 44].
For those interested in intrusion detection, we suggest two surveys published by
Aickelin et al. in 2004 [45] and by Kim et al. in 2007 [46]. Apart from the theory
33 http://www.artificial-immune-systems.org/.

References
377
introduced in Sect. 9.4.2, Hofmeyr and Forrest, Harmer et al., and Kim and Bentley
used negative selection principle in 2000, 2002, and 2002, respectively [47‚Äì49]. In
2003 Aickelin et al. applied danger theory in intrusion detection [50]. Esponda et
al. provided a formal framework for positive and negative detection schemes, which
is quite useful for intrusion detection [51].
As to the NS principle, Dasgupta and Forrest gave an industrial application ex-
ample in 1999 [52]. In 2007 Ji and Dasgupta published an intensive survey [53].
We only introduce three immune methods and their applications in learning and
optimization problems. In 2008 Lay and Bate used innate immune techniques for
improving the reliability of real-time embedded systems [54]. In 2009 Berg used
adaptive cellular immunity for designing AIS [55].
The must-read papers of this chapter are [28] for general introduction, [4] for
clonal selection, [7] for immune networks, and [20] for NS.
Exercises and Potential Research Projects
9.1. Summarize the essence of aiNet [38] on a single sheet of paper and compare it
with (Œº +Œª)-ES, introduced in Sect. 2.3.1, for optimization.
9.2. Compare qualitatively aiNet, [38], and RLAIS, introduced in Sect. 9.3.3, for
unsupervised learning, e.g., clustering.
9.3. Using qualitative and quantitative comparisons for multimodal optimization
among standard Ô¨Åtness sharing algorithms, introduced in Sect. 5.3.1, CLONALG,
introduced in Sect. 9.2.2, and aiNet [40, 41], use the benchmark problems in [4], PI
in Sect. 5.5, and the techniques introduced in Sect. 3.6 to do a fair comparison and
draw statistical conclusions according to your numerical experiments.
9.4. Summarize the global search mechanism, convergence mechanism, and uphill
mechanism for minimum optimization problems in CLONALG.
9.5. List the six matches in the example of bit-Ô¨Çip mutation in Sect. 9.4.1.
9.6. Summarize [48] on a single sheet of paper and compare its methods with those
of [21], introduced in Sect. 9.4.2.
References
1. Kindt TJ, Osborne BA, Goldsby RA (2006) Kuby immunology, 6th edn. Freeman, New York
2. Murphy KM, Travers P, Walport M (2007) Immunobiology: the immune system, 7th edn.
Garland Science, London, UK
3. de Castro LN, Timmis J (2002) ArtiÔ¨Åcial immune systems: a new computational intelligence
approach. Springer, Berlin Heidelberg New York

378
9 ArtiÔ¨Åcial Immune Systems
4. de Castro L, Zuben FV (2002) Learning and optimization using the clonal selection principle.
IEEE Trans Evol Comput 6(3):239‚Äì251
5. Coello Coello CA, Cort¬¥es NC (2005) Solving multiobjective optimization problems using an
artiÔ¨Åcial immune system. Genet Programm Evolvable Mach 6(2):163‚Äì190
6. Jerne NK (1974) Towards a network theory of the immune system. Ann Immunol 125C(1‚Äì
2):373‚Äì389
7. Farmer JD, Packard NH, Perelson AS (1986) The immune system, adaptation, and machine
learning. Phys D 2(1‚Äì3):187‚Äì204
8. Perelson A (1989) Immune network theory. Immunol Rev 110(1):5‚Äì36
9. Cayzer S, Aickelin U (2002) A recommender system based on the immune network.
In:
Proceedings of the congress on evolutionary computation, pp 807‚Äì813
10. Cayzer S, Aickelin U (2005) A recommender system based on idiotypic artiÔ¨Åcial immune
networks. J Math Modell Algorithms 4(2):181‚Äì198
11. Duda RO, Hart PE, Stork DG (2000) Pattern classiÔ¨Åcation, 2nd edn. Wiley-Interscience, New
York
12. Alpaydin E (2004) Introduction to machine learning. MIT Press, Cambridge, MA
13. Bishop CM (2007) Pattern recognition and machine learning. Springer, Berlin Heidelberg
New York
14. Xu R, Wunsch D (2008) Clustering. Wiley-IEEE, New York
15. Xu R, Wunsch D (2005) Survey of clustering algorithms. IEEE Trans Neural Netw 16(3):645‚Äì
678
16. Timmis J, Neal M, Hunt J (2000) An artiÔ¨Åcial immune system for data analysis. Biosystems
55(1‚Äì3):143‚Äì150
17. Timmis J, Neal M (2001) A resource limited artiÔ¨Åcial immune system for data analysis.
Knowledge-Based Syst 14(3-4):121‚Äì130
18. Neal M (2002) An artiÔ¨Åcial immune system for continuous analysis of time-varying data. In:
Proceedings of the 1st international conference on artiÔ¨Åcial immune systems, pp 76‚Äì85
19. Neal M (2003) Meta-stable memory in an artiÔ¨Åcial immune network. In: Proceedings of the
2nd international conference on artiÔ¨Åcial immune systems, pp 168‚Äì180
20. Forrest S, Perelson A, Allen L et al (1994) Self-nonself discrimination in a computer. In:
Proceedings of the IEEE computer society symposium on research in security and privacy, pp
202‚Äì212
21. Dasgupta D, Gonzalez F (2002) An immunity-based technique to characterize intrusions in
computer networks. IEEE Trans Evol Comput 6(3):281‚Äì291
22. Dasgupta D (1998) ArtiÔ¨Åcial immune systems and their applications. Springer, Berlin Heidel-
berg New York
23. Tarakanov AO, Skormin VA, Sokolova SP (2003) Immunocomputing: principles and applica-
tions. Springer, Berlin Heidelberg New York
24. Ishida Y (2004) Immunity-based systems. Springer, Berlin Heidelberg New York
25. Dasgupta D (2006) Advances in artiÔ¨Åcial immune systems. IEEE Comput Intell Mag 1(4):40‚Äì
49
26. Garrett SM (2005) How do we evaluate artiÔ¨Åcial immune systems? Evol Comput 13(2):145‚Äì
177
27. Freitas A, Timmis J (2007) Revisiting the foundations of artiÔ¨Åcial immune systems for data
mining. IEEE Trans Evol Comput 11(4):521‚Äì540
28. Timmis J (2007) ArtiÔ¨Åcial immune systems‚Äîtoday and tomorrow. Nat Comput 6(1):1‚Äì18
29. Timmis J, Andrews P, Owens N et al (2008) An interdisciplinary perspective on artiÔ¨Åcial
immune systems. Evol Intell 1(1):5‚Äì26
30. Bersini H (2002) The immune and the chemical crossover.
IEEE Trans Evol Comput
6(3):306‚Äì313
31. Wu Y, Fyfe C (2008) Exploratory data analysis with artiÔ¨Åcial immune systems. Evol Intell
1(2):159‚Äì169
32. Do TD, Hui SC, Fong A (2009) Associative classiÔ¨Åcation with artiÔ¨Åcial immune system. IEEE
Trans Evol Comput 13(2):217‚Äì228

References
379
33. Lau H, Tsang W (2008) A parallel immune optimization algorithm for numeric function opti-
mization. Evol Intell 1(3):171‚Äì185
34. Chun J, Lim J, Jung H (1997) Optimal design of synchronous motor with parameter correction
using immune algorithm. In: Proceedings of the IEEE international electric machines and
drives conference, pp 610‚Äì615
35. Anile AM, Cutello V, Narzisi G et al (2007) Determination of protein structure and dynamics
combining immune algorithms and pattern search methods. Nat Comput 6(1):55‚Äì72
36. Cutello V, Nicosia G, Pavone M et al (2007) An immune algorithm for protein structure pre-
diction on lattice models. IEEE Trans Evol Comput 11(1):101‚Äì117
37. Cruz-Cort¬¥es N, Rodriuez-Henriquez F, Coello CC (2008) An artiÔ¨Åcial immune system heuris-
tic for generating short addition chains. IEEE Trans Evol Comput 12(1):1‚Äì24
38. de Castro LN, Zuben FJV (2002) aiNet: an artiÔ¨Åcial immune network for data analysis. Abbass
HA, Sarker RA, Newton cs (eds) Data mining: a heuristic approach. Idea Group Publishing,
Hershey, PA, pp 231‚Äì260
39. de Casto LN, Zuben FV (2000) An evolutionary immune network for data clustering. In:
Proceedings of the sixth Brazilian symposium on neural networks, pp 84‚Äì89
40. de Castro L, Timmis J (2002) An artiÔ¨Åcial immune network for multimodal function optimiza-
tion. In: Proceedings of the IEEE congress on evolutionary computation, pp 699‚Äì704
41. Timmis J, Edmonds C (2004) A comment on Opt-AiNET: an immune network algorithm for
optimisation. In: Proceedings of the genetic and evolutionary computation conference, pp
308‚Äì317
42. Watkins A, Timmis J, Boggess L (2004) ArtiÔ¨Åcial immune recognition system (AIRS): an
immune-inspired supervised learning algorithm. Genet Programm Evolvable Mach 5(3):291‚Äì
317
43. Ishida Y, Adachi N (1996) Active noise control by an immune algorithm: adaptation in im-
mune system as an evolution. In: Proceedings of the IEEE international conference on evolu-
tionary computation, pp 150‚Äì153
44. Ishida Y (1997) Active diagnosis by self-organization: an approach by the immune network
metaphor.
In: Proceedings of international joint conferences on artiÔ¨Åcial intelligence, pp
1084‚Äì1091
45. Aickelin U, Greensmith J, Twycross J (2004) Immune system approaches to intrusion de-
tection - a review. In: Proceedings of the 3rd international conference on artiÔ¨Åcial immune
systems, pp 316‚Äì329
46. Kim J, Bentley PJ, Aickelin U et al (2007) Immune system approaches to intrusion detection
‚Äî a review. Nat Comput 6(4):413‚Äì466
47. Hofmeyr SA, Forrest S (2000) Architecture for an artiÔ¨Åcial immune system. Evol Comput
8(4):443‚Äì473
48. Harmer P, Williams P, Gunsch G et al (2002) An artiÔ¨Åcial immune system architecture for
computer security applications. IEEE Trans Evol Comput 6(3):252‚Äì280
49. Kim J, Bentley PJ (2002) Towards an artiÔ¨Åcial immune system for network intrusion detection:
An investigation of dynamic clonal selection. In: Proceedings of the IEEE world congress on
computational intelligence, pp 1244‚Äì1252
50. Aickelin U, Bentley P, Cayzer S et al (2003) Danger theory: the link between AIS and IDS?
In: Proceedings of the 2nd international conference on artiÔ¨Åcial immune systems, pp 147‚Äì155
51. Esponda F, Forrest S, Helman P (2004) A formal framework for positive and negative detection
schemes. IEEE Trans Syst Man Cybern B Cybern 34(1):357‚Äì373
52. Dasgupta D, Forrest S (1999) ArtiÔ¨Åcial immune systems in industrial applications. In: Pro-
ceedings of the second international conference on intelligent processing and manufacturing
of materials, pp 257‚Äì267
53. Ji Z, Dasgupta D (2007) Revisiting negative selection algorithms. Evol Comput 15(2):223‚Äì
251
54. Lay N, Bate I (2008) Improving the reliability of real-time embedded systems using innate
immune techniques. Evol Intell 1(2):113‚Äì132
55. van den Berg HA (2009) Design principles of adaptive cellular immunity for artiÔ¨Åcial immune
systems. Soft Comput 13(11):1073‚Äì1080


Chapter 10
Genetic Programming
Abstract Genetic programming is a very famous branch of EAs. The departure
point of genetic programming is to automatically generate functional programs in
the computer, whose elementary form could be an algebraic expression, logic ex-
pression, or a small program fragment. This idea can be expanded to generate ar-
tiÔ¨Åcial intelligence by computer. The reason for a separate chapter, instead of inte-
grating it into Chap. 2, lies in its special representation, evaluation, and variation
methods caused by the requirement of representing structure in the chromosome.
10.1 Introduction to Genetic Programming
Genetic programming (GP) was suggested by Koza in 1992 as a tool for computers
to solve problems automatically [1]. The intuitive idea of GP is to generate computer
programs automatically by genetic operations.
10.1.1 The Difference Between Genetic Programming and Genetic
Algorithms
In GAs, we are generally facing an optimization problem.1 What we need to do
is to Ô¨Ånd the optimal variable values so that their objective function has the maxi-
mum/minimum value. This kind of optimization problem is called parameter opti-
mization, i.e., to Ô¨Ånd the optimal problem parameters of the problem.
But sometimes problems are more complicated. We face not only parameter op-
timization but also structural optimization. In designing an artiÔ¨Åcial neural network,
we need to determine not only the thresholds for each nerve cell but also the num-
ber of the internal level nerve cells. In the example of designing a digital Ô¨Ålter, we
1 We often transform a learning problem into an optimization problem in GAs.
381

382
10 Genetic Programming
are interested in both Ô¨Ånding the optimal parameters of the Ô¨Ålter and determining
the order of the Ô¨Ålter. Generally such designing problems have an underlying re-
quirement for structure optimization. But traditional GAs cannot handle structure
optimization, i.e., GAs cannot express the structure of a solution in chromosomes.
The above discussion comes from the operations research community, i.e., math-
ematics point of view. Let us discuss the difference between GP and GAs from the
viewpoint of artiÔ¨Åcial intelligence, i.e., computer science point of view. There are
a lot of data. Some of them are the input of the system, others are outputs. The re-
quirement is to set up a nonlinear relationship2 between the input and the output
so that this relationship can map correctly the input data to the output data. If so,
we say the relationship has some ‚Äúintelligence.‚Äù In the future, if new data come,
we can hope that this intelligent relationship will suggest the correct output, that is
a forecast, classiÔ¨Åcation, pattern recognition, clustering, etc. This kind of nonlin-
ear relationship setup problem is called a learning problem, and the research Ô¨Åeld
is called data mining or knowledge discovery. Although we can transform learning
problems into optimization problems and use GAs to solve them, GP can handle
them directly.
After understanding the above two demands for GP, we need to point out several
things for learning GP using the basics of GAs, provided that you have grasped the
previous chapters.3
‚àôHow does one represent structure in GP, i.e., what is the meaning of genes and
chromosomes in GP?
‚àôHow does one generate randomly distributed individuals in the initialization pro-
cedure?
‚àôHow does one evaluate the Ô¨Åtness value of an individual in the environment of a
learning problem?
‚àôHow does one design variation operators for GP chromosomes?
In the next subsection, we will use a curve Ô¨Åtting problem to illustrate the basic
concepts of GP.
Since its inception, GP has been a hot research Ô¨Åeld. Figure 10.1 illustrates the
number of papers indexed by the SCI on GP.4
10.1.2 Genetic Programming for Curve Fitting
Suppose we are given a set of numbers as in Table 10.1, i.e., there are 17 observation
data with the input and the output of the system. What we need to do is to set up
the nonlinear relationship between the input and the output so that next time we can
2 This relationship can be regarded as a machine, algorithm, or even a brain.
3 The solution process of GP is the same as the one introduced on page 77.
4 TS = (‚Äúgenetic programming‚Äù). The SCI index ‚ÄúTS‚Äù is for the search topic in the title, the key-
words, and the abstract.

10.1 Introduction to Genetic Programming
383


	
	


	

	
	













 

	

 





Fig. 10.1 Number of papers indexed by SCI on GP
feed back a ‚Äúbest Ô¨Åt‚Äù output with an input between, or out of, the range [1,17].5 This
problem is called curve Ô¨Åtting or symbolic regression or system identiÔ¨Åcation.6
Table 10.1 An example of curve Ô¨Åtting


	






/		
0#
0
0#"
0
0""
0")
	
"
#
&
)


/		
0)
0##
0
0#
0"
0&#
	




"

/		
0)
0"
0
0"
0






















The nonlinear relationship between the input and the output can be written as a
nonlinear function between the input variable x and the output variable y. Actually,
these observation data are generated with the function y = f(x) = x0.5 +sin(x), i.e.,
we need to identify the form and the parameter of function f.7 The ideal function
and its sampling points are illustrated by Fig. 10.2.
In the following part of this subsection, we will introduce the four points men-
tioned in Sect. 10.1.1 and omit the other details of GP.
5 The so-called ‚Äúbest Ô¨Åt‚Äù can be understood as either exactly the same value or as close as possible.
The former situation is called interpolation and the latter one is called approximation. If we want
to forecast the value of the output outside the given range, this problem is called extrapolation.
6 The Ô¨Årst term comes from mathematics, the second one from computer science, and the last one
from control theory.
7 Here we regard ‚Äúcomputer program‚Äù as an algebraic function expression. Other ‚Äúprograms,‚Äù e.g.,
logic expressions, will be introduced in the following sections.

384
10 Genetic Programming
10.1.2.1 Syntax Tree Code
The key part of GP is the ability to represent a variable, e.g., x, and algebraic op-
erators, e.g., +, ‚àí, √ó, and √∑, in genes. We can use a tree to represent an alge-
braic expression. For example, the ideal function to be identiÔ¨Åed, e.g., y = f(x) =
x0.5 +sin(x), can be expressed by the tree in Fig. 10.3.8



	




	














	
1
3
Fig. 10.2 The function and its sampling points for generating the data set of Table 10.1
Fig. 10.3 Syntax tree repre-
sentation for ideal function




















<
=
4

#&

The tree in Fig. 10.3 is called a syntax tree. Some programming languages, e.g.,
Lisp, can represent the expression in a preÔ¨Åx notation way, so that the syntax tree
in Fig. 10.3 can be written as (+(ÀÜ(x 0.5) sin(x))), i.e., there is a bijective
map between the syntax tree and its preÔ¨Åx expression. The real chromosome in the
8 It is an upside down tree.

10.1 Introduction to Genetic Programming
385
memory of the computer is (+(ÀÜ(x 0.5) sin(x))), which can be understood
as Fig. 10.4.9
Fig. 10.4 The representation
of a GP chromosome using
the form of GAs
























=
U

 )0  
A tree is a special form of graph. In a tree, every circle is called a node. The lines
between nodes are called edges or links. The uppermost node is called the root. If
the node has only one link but is not the root, it is called a leaf. Other nodes besides
the root and leaves are called internal nodes.
As can be seen from Fig. 10.3, if we want to use a tree to represent an algebraic
expression, internal nodes can be functions, variables, and constants, but leaves can
only be variables and constants.10
10.1.2.2 Initialization
As can be seen from Fig. 10.4, the genes of GP could be variables, constants, func-
tions, or something else. But they cannot be connected as you wish. The chromo-
somes need to have the legal corresponding syntax tree, i.e., they should have mean-
ings. So randomly generating genes will likely produce illegal individuals.
In order to randomly generate a syntax tree from scratch, we need to know all the
available functions, variables, and constants. This set is called a primitive set, which
is predeÔ¨Åned by users. In determining the primitive set, we need to make sure that
the real solution of the problem could be expressed by the elements of the primitive
set.11
Suppose we have deÔ¨Åned the function set as sin, cos, tan, 1
x, +, ‚àí, √ó, √∑, ÀÜ, square
root, ln, rand(). Some of them require one variable while others require two.12 The
variable set is only x provided that we know in advance that this is a one-input and
one-output system. The constant set is œÄ and e.
9 But for simplicity, in the following part of this section, we will only discuss the syntax tree.
10 In GP, these variables and constants are called terminals, which means that the expression ter-
minates here.
11 In mathematics, given a set of n + 1 distinct data of input and output, we can prove that there
exists an norder polynomial that goes exactly through these points, i.e., interpolation. So if we
only want to interpolate these given data, normal functions to generate polynomials are enough.
But sometimes we want more than that, i.e., we want the forecast output to be as close as possible
to the real output but abandon the requirement of the exact right output at the given data points,
i.e., approximation. Then Fourier approximation and Chebyshev polynomial approximation are
possible options.
12 The rand() function does not require parameters and return a uniformly distributed random
number in the range (0,1). In the following part, for simplicity, we suppose that rand() can gener-
ate uniform random numbers between (a,b), where a and b are userdeÔ¨Åned parameters. There are
other functions that require three inputs, e.g., IF function in logic expression.

386
10 Genetic Programming
We can start the initialization procedure from the root. The root of an individual
should be a function. After that, we know the variable number of that function, and
then could randomly select elements from the primitive set. For every initialized
node, if it‚Äôs a function, we need to generate its variables (might be any element in the
primitive set). If a generated node is a terminal, no follow-up nodes are necessary.
For example, in this way, we generated 4 individuals: ‚àí4.3+1.21x, 0.667e0.071x,
0.127 + 1.77ln(x), and 1.242x0.76.13 The syntax trees of them could be illustrated
by Fig. 10.5.


<
-4	
4

6























(a)


6
4


4


6
























(b)






<
4
4
!&
6























(c)




6
4
=

4





















(d)
Fig. 10.5 Syntax tree of the four individuals after initialization: (a) individual 1, (b) individual 2,
(c) individual 3, and (d) individual 4
The alert readers might have noticed that the above procedure could generate
individuals with a lot of nodes if we always select function for node, which is not
realistic for pragmatic applications. So special considerations should be taken to
limit the node number of the initial individuals. We can deÔ¨Åne the depth of a node
as the number of links from the root to reach that node and the depth of the tree as
the maximum depth of its leaves. The depths of the four trees in Fig. 10.5 are 2, 3,
3, and 2, respectively. One pragmatic way to limit the node number is to predeÔ¨Åne
the maximum depth of the tree, denoted as D. Apart from that, the size of a tree is
deÔ¨Åned as its node number. The size of the four trees in Fig. 10.5 are 5, 6, 6, and 5,
respectively. So another way is to deÔ¨Åne the maximum size.
13 Suppose that popsize = 4 and the real numbers in these individuals are generated by rand().

10.1 Introduction to Genetic Programming
387
In order to increase the diversity of the initial individuals, Koza suggested three
methods as follows:
1. Full. All leaves in the individual have the same depth D.
2. Grow. The depth of the individuals should be no more than D.
3. Ramped half-and-half. The depth of the initial individuals are randomly dis-
tributed between [2,D]. For example, D = 5, which means that 0.25 √ó popsize
individuals are generated with maximum depth 2, ..., 0.25√ó popsize individuals
are generated with maximum depth 5.14 In each maximum depth, the individual
is generated by a full method or grow method with the same probability 0.5.
10.1.2.3 Evaluation
The initial four individuals and the original data are drawn in Fig. 10.6.

















	
1
3


/
%	
C0=0
)0")0)"+
)0"=0""13
0+)0"
Fig. 10.6 The initial four individuals and the original data
As can be seen from Fig. 10.6, individuals 2 and 3 are closer to the original data
compared to individuals 1 and 4. How does one evaluate the closeness quantita-
tively? As an option, we can use the sum of the absolute errors on the sampling
points.
14 Hence the meaning of ramped. Suppose we rank the new individuals with their depths. Then
there is a ramp shape of the individuals‚Äô depths.

388
10 Genetic Programming
We sample 17 points in these individuals, i.e., curves, at x = 1, ..., x = 17, cal-
culate their absolute errors to the original data at these points, and sum them up
to represent the distance of these curves to the original data and thus assign these
errors as their Ô¨Åtness values, i.e., Eq. 10.1
fitnessi =
17
‚àë
k=1
‚à£y(k)‚àíindi (k)‚à£
(10.1)
where indi and fitnessi are the curve value and the Ô¨Åtness value of individual i,
respectively, and y(k) is the original data at sampling point k. The smaller the error
the Ô¨Åtter the individual. By this method, we can calculate the errors of these curves
as 87.5140, 26.4728, 17.5729, and 60.9095, respectively, which corroborates the
above qualitative guess for their Ô¨Åtness.
Here is another obvious difference between GP and GAs. In GAs, the Ô¨Åtness
value of an individual can be obtained by only one run of the objective function. But
here, we need to evaluate the individual function many times to get the sum of the
absolute error, i.e., its Ô¨Åtness value. These evaluations are called Ô¨Åtness cases.
It is necessary to mention that although the constants in these four individu-
als are generated by a rand() function in the initialization procedure, their values
remain unchanged in the following genetic evolving processes, i.e., evaluation, se-
lection, and variation operators. So these constants are called ephemeral random
constants.15
After we get the Ô¨Åtness values of the initial population, selection can be processed
with the techniques discussed in Chap. 3.
10.1.2.4 Variation Operators
The crossover operator and the mutation operator of GP are carried out with a tree
representation. We only give a limited number of examples of the variation operators
here.
Subtree Crossover
If two individuals need to perform crossover according to genetic rules, we can
randomly pick one node in each individual and exchange their subtree. Thus two
new individuals can be generated.
For example, individuals 1 and 4 need to perform crossover and the x node and
the ÀÜ node are selected, respectively. Figure 10.7 illustrates one offspring.16
15 They are random numbers for a very short time, i.e., initialization, and then become constants.
16 The other one is left as an exercise.

10.1 Introduction to Genetic Programming
389



<
-4	
4

6
6
4
=

4
6
4

=



















Fig. 10.7 Subtree crossover to generate one offspring
Point Mutation
If one individual requires mutation. We randomly pick one node and generate ran-
domly another possible node to replace it. Then a mutant is generated.
For example, individual 4 needs to mutate and the ÀÜ node is randomly selected.
Because the ÀÜ function requires two variables, we need to select a function with two
variables randomly from the primitive set, e.g., ‚àó. Figure 10.8 illustrates the mutant.
6
4
=

4
6
4
6

4
4
=

















Fig. 10.8 Point mutation to generate mutant
Note: there should be some ‚Äúsafeguard‚Äù rules to ensure that the offspring and the
mutant are still legal individuals. The violations might be divide-by-zero, negative
variable in logarithmic function, etc.

390
10 Genetic Programming
10.2 Other Code Methods for Genetic Programming
The basic code method based on Lisp‚Äôs preÔ¨Åx notation language has the following
shortcomings.
‚àôChromosomes have variable length, which creates some inconveniences in de-
signing variation operators.
‚àôIt is relatively hard to directly determine the crossover and mutation nodes ac-
cording to a preÔ¨Åx expression. We need to transform it into syntax tree Ô¨Årst.
‚àôSimple crossover and mutation, e.g., single-point crossover and bit-Ô¨Çip mutation,
might generate illegal offspring.
Several methods with Ô¨Åxed chromosome length have been proposed since the
inception of GP that can undergo simple and standard variation operators. We just
give two examples here.
10.2.1 Gene Expression Programming
Gene expression programming (GEP) was proposed by Ferreira in 2001 [2, 3]. The
essence of GEP is to divide a chromosome into two parts: head and tail. The head
has h genes, which are composed of functions and terminals, and the tail has t genes,
which are only composed of terminals. h is the maximum number of internal nodes
provided by the user. Then t is determined by the following equation:
t = h(n‚àí1)+1
(10.2)
where n is the maximum number of variables required by functions. Let us prove
Eq. 10.2 by mathematical induction. For simplicity, we suppose that all the functions
require n variables. Then if h = 1, i.e., only one root in an internal node set, the
leaf number is n = 1 √ó (n ‚àí1) + 1 = n, which is equal to Eq. 10.2. Suppose the
extreme condition that all the internal nodes are h functions and the leaf number is
t = h(n‚àí1)+1. The h+1 internal nodes means one leaf needs to be changed into
a function with n variables. So the current leaf number is t = h(n‚àí1)+1‚àí1+n =
(h+1)(n‚àí1)+1,17 which is Eq. 10.2 in the situation of (h+1).
All the above discussions make the assumption that the internal nodes are all
functions with n variables. So Eq. 10.2 gives the maximum length of the tail, which
might not be used up completely in the decoding procedure.
Let us suppose that the function set includes square root, √ó, √∑, +, and ‚àí. The
maximum number of variables of these functions is two. The terminal set includes
variables a and b. After we set h = 10, we can get that the length of the chromosome
is h+t = 10+11 = 21. One chromosome is illustrated by Fig. 10.9.
where Q is for square root. We use italic bold fonts in Fig. 10.9 to represent the tail.
Others genes belong to the head.
17 ‚àí1 means that one internal node is deleted and +n means that n leaves are added.

10.2 Other Code Methods for Genetic Programming
391
Fig. 10.9 The chromosome
of GEP





) "#&)     " # & )
=V C < 
P=<V
        
As can be see from Fig. 10.9, genes in the head can be any element in the primi-
tive set but genes in the tail can only contain elements in the terminal set.
The decoding procedure is very straightforward. We start from the Ô¨Årst gene
in the chromosome. It is a +, which means that the root of the syntax tree is +.
The + function requires two variables, so the second and the third genes, i.e., Q
and ‚àí, represent the second level in the syntax tree. The square root only requires
one variable, while ‚àírequires two variables. So the fourth to the sixth genes are
arranged in the third level of the syntax tree from left to right. Execute the above
procedure until all the nodes are terminals. Then we can get the syntax tree in Fig.
10.10. 18
Fig. 10.10 The syntax tree of
the chromosome in Fig. 10.9











V
=

<

P
V

<
=






According to the deÔ¨Ånition of the head and the tail and the decoding procedure,
this code scheme will always represent legal syntax trees if we stick to the rule that
the tail can only contain terminals.19 So ordinary crossover and mutation can be
used freely.
GEP uses Ô¨Åxed-length chromosomes with ordinary variation operators, which
improves the shortcomings mentioned at the beginning of this subsection. Ferreira
discussed other topics, e.g., the assignment of h, powerful variation operators, and
the use of several subchromosomes in a chromosome to further improve the search
ability of GEP. Interested readers are referred to [2, 3]. In 2003 Zhou et al. published
a paper to apply GEP in classiÔ¨Åcation [4]. In 2008 Karakasis combined GEP with
AIS to get more accurate classiÔ¨Åcation rules [5]. Readers interested in GEP might
refer to these papers as examples of its applications.
18 This decoding procedure is called gene expression. baaab in the tail is not used in the decoding
procedure. So this code method has redundancy.
19 Readers are encouraged to randomly change a gene in Fig. 10.9 and decode it to a syntax tree.

392
10 Genetic Programming
10.2.2 Grammatical Evolution for Solving Differential Equations
In computer science, the Backus‚ÄìNaur form (BNF) is a syntax to illustrate a pro-
gramming language. One sentence of the program source can be written in the form
of BNF as follows:
<symbol> ::= __expression__
where <symbol> can be regarded as the expression needing to be determined here
and __expression__ contains expressions, operators, functions, and digits, de-
noted as <expr>, <op>, <func>, and <digit>, respectively. It is quite like the
primitive set discussed above. One example of the possible __expression__ to
indicate algebraic expressions with one variable is illustrated in Table 10.2. Here we
just list the set for algebraic expression for simplicity. A possible __expression__
for other programming language is also available. There are four types of rules in Ta-
ble 10.2, i.e., <expr>, <op>, <func>, and <digit>, in which the rule <expr>
can be recursively called.
Table 10.2 An example of the possible rules

2

+

W
2

+

W
X+
Y
E X+
YXYX+
Y
)
X%	Y
E
)
)


1X+
YXYX+
Y3







X'Y1X+
Y3







X%	Y













XY
E =
)






C







P



"
"


<



#
#
X'Y
E 
)


&
&










+














In 2001 O‚ÄôNeill et al. suggested a way to transfer a series of integer numbers
into BNF so as to represent a programming language and use genetic evolution to
automatically generate programs [6, 7]. They call this method grammatical evolu-
tion. Let us take Table 10.2 as a basic set and illustrate the decoding process for the
chromosome illustrated in Fig. 10.11.

10.2 Other Code Methods for Genetic Programming
393
Fig. 10.11 An example chro-
mosome of grammatical evo-
lution





)  " & # #     "
The genes of the chromosome are all integer numbers. We can denote the allele as
V. The Ô¨Årst expression must be <expr>. Then we can start the decoding procedure
from the leftmost gene, i.e., 10, using the following equation:
Rule = V mod NR
(10.3)
where NR is the current number of rules in the leftmost undetermined expression,
i.e., NR = 5 in Table 10.2 for rule <expr>, and Rule is the selected rule num-
ber. For example, the Ô¨Årst gene in Fig. 10.11 is 10. Using Eq. 10.3, we know
that allele 10 means Rule = 10 mod 5 = 0, i.e., rule <expr> <op> <expr>,
which is used to replace the current leftmost undetermined expression <expr>.
Then the leftmost undetermined expression is <expr>, which needs the second
gene 11 to decode. 11 mod 5 = 1, which means we need to replace <expr> with
(<expr> <op> <expr>). It is the expression (<expr> <op> <expr>)
<op> <expr>. Continue the above procedure until there is no undetermined ex-
pression. The decoding procedure can be illustrated as in Table 10.3.
Table 10.3 The decoding procedure of the chromosome in Fig. 10.11
+



	
%
/
	
X+
Y
)
)E)
X+
YXYX+
Y

E
1X+
YXYX+
Y3XYX+
Y

E
1X'Y1X+
Y3XYX+
Y3XYX+
Y
"
"E
11X+
Y3XYX+
Y3XYX+
Y
&
&E
113XYX+
Y3XYX+
Y
#
#E)
113=X+
Y3XYX+
Y
#
#E
113=X%	Y3XYX+
Y

)E
113=3XYX+
Y

E
113=3PX+
Y

E
113=3PX%	Y

)E
113=3P


As can be seen from Table 10.3, sometimes we do not use all 12 genes to get a
full expression, i.e., the Ô¨Ånal gene 7 is useless. If more than 12 genes are needed
to determine the clear expression, Tsoulos and Lagaris further suggested starting
over from the Ô¨Årst gene, i.e., to wrap [8]. Then the Ô¨Åxed-length chromosome could

394
10 Genetic Programming
be used to indicate the algebraic expressions that facilitate the common variation
operators.20
In 2006 Tsoulos and Lagaris used grammatical evolution to Ô¨Ånd an analytical
closed form solution for differential equations [8].21
An n-order ordinary differential equation (ODE) can be illustrated as follows:
f
(
x,y,y(1),‚ãÖ‚ãÖ‚ãÖ,y(n))
= 0,
x ‚àà[a,b]
(10.4)
where y(n) is the n-order derivation of y over x and [a,b] is the deÔ¨Ånition domain
of variable x. To determine function y(x), we still need n boundary conditions as
follows:
gk
(
x,y,y(1),‚ãÖ‚ãÖ‚ãÖ,y(n))
‚à£x=a or b = 0,
k = 1,‚ãÖ‚ãÖ‚ãÖ,n
(10.5)
If x = a, Eq. 10.5 is called the initial value condition; if x = b, Eq. 10.5 is called the
Ô¨Ånal value condition. What we need to do for solving ODE is to Ô¨Ånd the function
y(x) that satisÔ¨Åes Eqs. 10.4 and 10.5.
We can use an integer value chromosome, e.g., Fig. 10.11, to represent an alge-
braic expression, which might be the solution to the ODE problem. To determine the
Ô¨Åtness value of the chromosome i, we should Ô¨Årst decode it as algebraic expression
Mi in the way illustrated in Table 10.3.22 Then we can use some algebraic methods
to get its deviations M(1)
i
, . .., M(n)
i
[9]. After that, we can evaluate the closeness of
Mi to the real y. The evaluation procedure is similar to that discussed in the previous
section. We sample N points between [a,b] and take the following expression as the
sum square of the function errors in the N sampling points:
E (Mi) =
N
‚àë
j=1
f 2 (
x j,Mi (x j),M(1)
i
(xj),‚ãÖ‚ãÖ‚ãÖ,M(n)
i
(x j)
)
(10.6)
Then for n boundary conditions, we can also get their sum square errors as fol-
lows:
P(Mi) = Œª
n
‚àë
k=1
g2
k
(
x,Mi (x),M(1)
i
(x),‚ãÖ‚ãÖ‚ãÖ,M(n)
i
(x)
)
‚à£x=a or b
(10.7)
where Œª is a user-deÔ¨Åned parameter to scale these two errors on same magnitude
level.
Finally, the Ô¨Åtness value of individual i is as follows:23
20 Sometimes many wrappings still cannot parse a legal expression. So Tsoulos and Lagaris sug-
gested allowing at most two wrappings. After that, if we still cannot get a clear expression, this
individual is discarded.
21 Their paper discusses high order ODEs, a system of Ô¨Årst-order ODE, and elliptic partial differ-
ential equation. Here we just introduce the Ô¨Årst case.
22 Here we regard ‚Äúcomputer program‚Äù as the algebraic function expression that satisÔ¨Åes the dif-
ferential equation and boundary conditions.
23 The smaller the Ô¨Åtter.

10.3 Example of Genetic Programming for Knowledge Discovery
395
fitnessi = E(Mi)+P(Mi)
(10.8)
Thus, all the technical obstacles are removed for genetic evolving, and we can
select, cross over, mutate, and replace individuals until the stop criteria are satisÔ¨Åed.
After that, the best analytical closed form solution for Eqs. 10.4 and 10.5 can be
suggested.
10.3 Example of Genetic Programming for Knowledge Discovery
In the previous two sections we only discussed how to use GP to generate algebraic
expressions so as to implement the requirement of curve Ô¨Åtting or differential func-
tion solving. GP has been used to evolve other ‚Äúcomputer programs‚Äù for various
tasks. Here we will introduce the work published by Bojarczuk et al. in 2000 on
applying GP to aid the diagnosis of chest-pain [10].24
Chest pain is a symptom related to several diseases. There are 165 predicting
attributes related to chest pain. Every predicting attribute for the patient has a logic
value of TRUE or FALSE. For example, ‚Äúpain irradiates to the upper right region of
the abdomen‚Äù is a predicting attribute. If the patient has such a symptom, his logic
value for that attribute is TRUE, and conversely, if the patient does not have such a
symptom, then his logic value for that attribute is FALSE.
There are 12 diseases related to chest pain, e.g., angina, esophageal pain, etc.
The knowledge discovery problem here is to determine the disease given the 165
logic values for the predicting attributes of a patient. It can be expressed as the logic
expression ‚ÄúIF - THEN.‚Äù One of these logic expressions can be written as follows:25
IF (starting factor is emotion = TRUE) AND ((the pain lasts no more than seconds = TRUE)
OR ((the pain begins gradually = TRUE) AND (the pain irradiates towards the upper left
limb = TRUE))) THEN disease is stable angina.
In the above diagnostic logic expression, ‚Äústarting factor is emotion,‚Äù ‚Äúthe pain
lasts no more than seconds,‚Äù etc. are all predicting attributes, and ‚Äústable angina‚Äù is
one diagnosed disease.
This is a classiÔ¨Åcation problem. We need to classify one case, with 165 predicting
attributes, into one of the 12 diseases. Bojarczuk et al. divide the problem into 12
subproblems. Subproblem i answers one question: could this case be diagnosed as
disease i?26
So what we need to do is to evolve 12 logic expressions of the ‚ÄúIF‚Äù part in the
above ‚ÄúIF - THEN‚Äù rule for 12 diseases. The ‚ÄúTHEN‚Äù part is not necessary because
24 Here we regard ‚Äúcomputer program‚Äù as a logic expression that can generate correct decisions
for chest-pain-related diseases.
25 This rule is only for illustration. Readers should not diagnose themselves without consulting a
doctor.
26 Another way to illustrate their technique is to transform a multiclass classiÔ¨Åcation problem into
multiple two-class problems.

396
10 Genetic Programming
for disease i, we will only evolve the diagnostic rule for it. Any case that satisÔ¨Åes
the logic expression will be diagnosed as disease i.27 In the following part, we will
only discuss the evolving procedure of logic rule i.
Bojarczuk et al. have 138 diagnosed chest-pain-related disease cases, each with
165 attributes. They use 90 cases to train rule i and 48 cases to test the generalization
performance of that rule.28 They further divide the 90 training cases into two groups:
those with disease i and those without disease i.
Then they use GP to evolve the logic expressions. The function set contains only
three elements: ‚ÄúAND,‚Äù ‚ÄúOR,‚Äù and ‚ÄúNOT.‚Äù The terminal set contains the symbol of
the 165 attributes, expressed as a, b, etc. Each symbol can have one of two values:
‚ÄúTRUE‚Äù or ‚ÄúFALSE.‚Äù One syntax tree example of the rule for disease i is illustrated
by Fig. 10.12.
Fig. 10.12 An example syn-
tax tree for disease i
















4,
/2
/2

4/8

/2
 
0
4/8

The initialization and variation procedures are all similar techniques introduced
in Sect. 10.1.2. Here we just focus on the evaluation of the individual.
For every training case, we know its 165 attributes and whether it has disease i
or not. For an individual, illustrated by Fig.10.12, we can deduce whether these
165 attributes will cause disease i or not. So there are four situations illustrated as
follows.29
‚àôTrue positive. The rule predicts that the case has disease i and the case really has
it. We use tp to represent the case numbers belonging to this situation.
‚àôFalse positive. The rule predicts that the case has disease i, but the case does
not actually have it. We use f p to represent the case numbers belonging to this
situation.
‚àôTrue negative. The rule predicts that the case does not have disease i, and in fact
the case does not have it. We use tn to represent the case numbers belonging to
this situation.
27 The 12 subproblems are simpler than the original classiÔ¨Åcation problem. But there exist some
cases that satisfy more than one rule, which would not happen in the original problem.
28 Generalization ability is the key issue of machine learning algorithms. Interested readers are
referred to a survey published by Kushchu in 2002 [11].
29 ‚ÄúPositive‚Äù and ‚Äúnegative‚Äù concern the prediction of the rule and ‚Äútrue‚Äù and ‚Äúfalse‚Äù concern the
correctness of the prediction.

10.4 Summary
397
‚àôFalse negative. The rule predicts that the case does not have disease i, but the
case does have it. We use fn to represent the case numbers belonging to this
situation.
According to the above categories, tp+ fn is the number of cases that have dis-
ease i and f p +tn is the number of cases that do not actually have disease i. Then
the sensitivity (Se) of the correct prediction for those who actually have disease i is
deÔ¨Åned as follows:
Se =
tp
tp+ fn
(10.9)
And the speciÔ¨Åcity (Sp) of the correct prediction for those who do not actually have
disease i is deÔ¨Åned as follows:
Sp =
tn
tn+ f p
(10.10)
Sn and Sp concern the correctness of the prediction. The larger the better. Bojar-
czuk et al. also deÔ¨Åned the third objective, denoted simplicity (Sy), to limit the size
of the syntax tree as follows:
Sy = maxnodes‚àí0.5numnodes‚àí0.5
maxnodes‚àí1
(10.11)
where maxnodes is the maximum node number predeÔ¨Åned by the user and numnodes
is the node number of the current individual. According to Eq. 10.11, 0.5 ‚â§Sy ‚â§1;
the larger the better.
Then the Ô¨Åtness function of the syntax tree is deÔ¨Åned as follows:
fitness = Se√óSp√óSy
(10.12)
It is easy to understand that 0 ‚â§fitness ‚â§1 and that the larger the better.
Bojarczuk et al. use 90 cases to train the 12 logic expressions for 12 diseases and
then use 48 cases to test the rules. The test results illustrate that the rules evolved
by GP has better predicting results than those generated by another rule-induction
algorithm, C5.0.
10.4 Summary
Structure optimization or learning is a research Ô¨Åeld that has attracted a lot of atten-
tion. It has strong ties with other hot AI topics, e.g., machine learning, knowledge
discovery, and pattern recognition, etc. GP is one of the closest branches of EAs to
this Ô¨Åeld.
The solution process of GP is the same as other EAs. But it has special features
with respect to representation, evaluation, and variation. We introduced the basics

398
10 Genetic Programming
of GP with an example of curving Ô¨Åtting in Sect. 10.1.2, where tree code is illus-
trated and its relationship with Lisp‚Äôs preÔ¨Åx notation programming language are
introduced.
Apart from that, two other techniques for mapping between chromosomes and
syntax tree are discussed in Sect. 10.2. GEP uses a head-and-tail structure to main-
tain Ô¨Åxed-length chromosomes and the legality of the chromosomes. Grammatical
evolution uses integers and rule labels to Ô¨Ånd the corresponding expressions, func-
tions, or digits. In this way, grammatical evolution can also generate legal individu-
als with standard variation operators. Other related methods which be recommended
in ‚ÄúSuggested Readings‚Äù.
Several applications are discussed, including curving Ô¨Åtting, differential function
solving, and classiÔ¨Åcation, which is only the tip of the iceberg of the possible GP
applications. Other topics will be recommended in ‚ÄúSuggestions for Further Read-
ing.‚Äù
Due to the space constraints, we do not discuss the following several important
issues of GP.
‚àôThe details of the data structure of GP for storing, managing, decoding, and eval-
uating individuals, which is the essence of implementing GP.
‚àôSometimes, a crossover operator might generate too deep or too large syntax
trees, which introduces a too large search space and also limits the comprehensi-
bility of the results. This problem is called bloat. A paper published in 2006 by
Luke and Panait compared several bloat control methods for GP [12].
‚àôWe only introduce the simplest crossover operator and mutation operator. Like
the real code situation, there are many other types of variation operators, such
as the architecture-altering operator suggested by Koza [13] and transposition
operator suggested by Ferreira [2], with different effects on exploration and ex-
ploitation for tree code.
We give enough recommendations in ‚ÄúSuggestions for Further Reading‚Äù for in-
terested readers on these issues.
After reading this chapter, you should understand the reason for GP‚Äôs capability
of structure optimizing, be familiar with some of the special concepts in GP, e.g.,
syntax tree, primitive set, Ô¨Åtness cases, etc., and grasp at least one method of two-
way mapping between a chromosome and a syntax tree.
Suggestions for Further Reading
Many books on GP have been published since its inception. The most famous four-
volume set was published by Koza in 1992, 1994, 1999, and 2003 [1, 13‚Äì15]. A
more recent textbook on GP, with its JAVA source code, was published by Poli et al.
in 2008 [16].
Apart from the journals and conferences introduced in Chap. 1, GP still has
its own journal, Genetic Programming and Evolvable Machines, published by

10.4 Summary
399
Springer, and its own conference, the European Conference on Genetic Program-
ming, held every year.
As to Internet resources, Silva proposed a GP toolbox, GPLAB, in the MATLAB‚ìá
environment,30 RML Technologies, Inc. sells Discipulus commercial software,31
and Koza maintain a Web site containing proliÔ¨Åc information on GP.32
GP for differential equation solving is a fascinating research Ô¨Åeld. Interested
readers are encouraged to read a paper by Sobester et al. published in 2008 [17].
For those interested in applying GP to disease diagnosis, one paper published
by Zhang and Wong in 2008 might be insightful [18]. The paper on generating
classiÔ¨Åcation trees by GP, published by Kuo in 2007, might also be interesting [19].
In 2006, Folino et al. discussed the GP-based large-scale data classiÔ¨Åcation problem
[20].
Curve Ô¨Åtting, function solving, and knowledge discovery are three applications
discussed in this chapter. For more examples, works on combinational logic circuit
synthesis by Cheang et al. [21], passive Ô¨Ålter synthesis by Chang et al. [22], au-
tonomous language developing by Hong et al. [23], control system evolution for
sumo-Ô¨Åghting robots by Sharabi and Sipper [24], and time series forecasting for
dynamic environments by Wagner et al. [25] are highly recommended.
Apart from GEP and grammatical evolution, in 1999 Folino et al. proposed cel-
lular GP [26], Miller and Thomson proposed Cartesian GP in 2000 [27], and in 2006
Hoai et al. suggested using a different tree-based representation and local structural
modiÔ¨Åcation operators to further improve the search ability of GP [28].
Mating restrictions sometimes play an important role in genetic evolution. In
order to introduce mating restrictions into GP, a deÔ¨Ånition of the distance between
two syntax trees is necessary. In 2008 Gustafson and Vanneschi published a paper
to deal with this [29].
In 2008 Tay et al. implemented GP in a competitive coevolution environment,
introduced in Sect. 3.7.1 [30]. Pursuit and evasion strategies are discussed in detail
there.
Recall the estimation of distribution algorithm (EDA) discussed in Sect. 3.5.2.6.
In 2008 Hasegawa and Iba suggested a new program evolution algorithm employing
a Bayesian network for generating new individuals [31].
The must-read papers of this chapter are Chaps. 1‚Äì4 of [16] for a general intro-
duction, [2] for GEP, and [10] for an application example.
Exercises and Potential Research Projects
10.1. Generate another offspring of Fig. 10.7.
10.2. Decode the GEP chromosome in Fig. 10.13 into a syntax tree.
30 GPLAB can be downloaded at http://gplab.sourceforge.net/.
31 http://www.rmltech.com/
32 http://www.genetic-programming.org/

400
10 Genetic Programming
Fig. 10.13 The GEP chromo-
some for Exercise 10.2









)  "#&)       " # & )
=V C <
P<V
          
10.3. Find a chromosome that can be decoded as sin2(x2) by grammatical evolution
using Table 10.2.
References
1. Koza JR (1992) Genetic programming: on the programming of computers by means of natural
selection. MIT Press, Cambridge, MA
2. Ferreira C (2001) Gene expression programming a new adaptive algorithm for solving prob-
lems. Complex Syst 13(2):87‚Äì129
3. Ferreira C (2006) Gene expression programming: mathematical modeling by an artiÔ¨Åcial in-
telligence, 2nd edn. Springer, Berlin Heidelberg New York
4. Zhou C, Xiao W, Tirpak T et al (2003) Evolving accurate and compact classiÔ¨Åcation rules
with gene expression programming. IEEE Trans Evol Comput 7(6):519‚Äì531
5. Karakasis V, Stafylopatis A (2008) EfÔ¨Åcient evolution of accurate classiÔ¨Åcation rules using a
combination of gene expression programming and clonal selection. IEEE Trans Evol Comput
12(6):662‚Äì678
6. O‚ÄôNeill M, Ryan C (2001) Grammatical evolution. IEEE Trans Evol Comput 5(4):349‚Äì358
7. O‚ÄôNeill M, Ryan C (2003) Grammatical evolution: evolutionary automatic programming in
an arbitrary language. Springer, Berlin Heidelberg New York
8. Tsoulos I, Lagaris I (2006) Solving differential equations with genetic programming. Genet
Programm Evolvable Mach 7(1)
9. Griewank A (1989) On automatic differentiation. In: Iri M, Tanabe K (eds) Mathematical
programming: recent developments and applications. Kluwer Academic Publishers, Holland,
pp 83‚Äì108
10. Bojarczuk C, Lopes H, Freitas A (2000) Genetic programming for knowledge discovery in
chest-pain diagnosis. IEEE Eng Med Biol Mag 19(4):38‚Äì44
11. Kushchu I (2002) Genetic programming and evolutionary generalization. IEEE Trans Evol
Comput 6(5):431‚Äì442
12. Luke S, Panait L (2006) A comparison of bloat control methods for genetic programming.
Evol Comput 14(3):309‚Äì344
13. Koza JR (1994) Genetic programming II: automatic discovery of reusable programs. MIT
Press, Cambridge, MA
14. Koza JR, III FHB, Andre D et al (1999) Genetic programming III: Darwinian invention and
problem solving. Morgan Kaufmann, San Francisco
15. Koza JR, Keane MA, Streeter MJ et al (2003) Genetic programming IV: routine human-
competitive machine intelligence. Springer, Berlin Heidelberg New York
16. Poli R, Langdon WB, McPhee NF (2008) A Ô¨Åeld guide to genetic programming. Lulu Enter-
prises, Raleigh, NC
17. Sobester A, Nair P, Keane A (2008) Genetic programming approaches for solving elliptic
partial differential equations. IEEE Trans Evol Comput 12(4):469‚Äì478
18. Zhang M, Wong P (2008) Genetic programming for medical classiÔ¨Åcation: a program simpli-
Ô¨Åcation approach. Genet Programm Evolvable Mach 9(3):229‚Äì255
19. Kuo C, Hong T, Chen C (2007) Applying genetic programming technique in classiÔ¨Åcation
trees. Soft Comput 11(12):1165‚Äì1172
20. Folino G, Pizzuti C, Spezzano G (2006) GP ensembles for large-scale data classiÔ¨Åcation. IEEE
Trans Evol Comput 10(5):604‚Äì616

References
401
21. Cheang SM, Lee KH, Leung KS (2007) Applying genetic parallel programming to synthesize
combinational logic circuits. IEEE Trans Evol Comput 11(4):503‚Äì520
22. Chang S, Hou H, Su Y (2006) Automated passive Ô¨Ålter synthesis using a novel tree represen-
tation and genetic programming. IEEE Trans Evol Comput 10(1):93‚Äì100
23. Hong J, Lim S, Cho S (2007) Autonomous language development using dialogue-act tem-
plates and genetic programming. IEEE Trans Evol Comput 11(2):213‚Äì225
24. Sharabi S, Sipper M (2006) GP-sumo: using genetic programming to evolve sumobots. Genet
Programm Evolvable Mach 7(3):211‚Äì230
25. Wagner N, Michalewicz Z, Khouja M et al (2007) Time series forecasting for dynamic envi-
ronments: the DyFor genetic program model. IEEE Trans Evol Comput 11(4):433‚Äì452
26. Folino G, Pizzuti C, Spezzano G (1999) A cellular genetic programming approach to classi-
Ô¨Åcation. In: Proceedings of the genetic and evolutionary computation conference, pp 1015‚Äì
1020
27. Miller JF, Thomson P (2000) Cartesian genetic programming. In: Proceedings of the European
conference on genetic programming, pp 121‚Äì132
28. Hoai NX, McKay R, Essam D (2006) Representation and structural difÔ¨Åculty in genetic pro-
gramming. IEEE Trans Evol Comput 10(2):157‚Äì166
29. Gustafson S, Vanneschi L (2008) Crossover-based tree distance in genetic programming. IEEE
Trans Evol Comput 12(4):506‚Äì524
30. Tay J, Tng C, Chan C (2008) Environmental effects on the coevolution of pursuit and evasion
strategies. Genet Programm Evolvable Mach 9(1):5‚Äì37
31. Hasegawa Y, Iba H (2008) A Bayesian network approach to program generation. IEEE Trans
Evol Comput 12(6):750‚Äì764


Appendix A
Benchmark Problems
All trigonometric functions are calculated with radian.
Benchmark Problems for Chap. 2 and Chap. 3
A.1. Adopted from Spears‚Äôs problem [1]. The problem is in binary space with ten
variables. There is only one peak, Peak = (0011010101). The objective value of a
solution string is as follows:
f(string) = 1
10(10‚àíHamming(string,Peak))
(A.1)
where Hamming( ) is the function to calculate two binary strings‚Äô Hamming dis-
tance. The smaller the objective value is, the better the solution is.
A.2. The sphere model from Eiben and Smith‚Äôs book [2]. The problem is in real
space with n variables. It is a minimum problem whose global optimal value is 0.0
at x = 0. As an exercise, we set n = 5. The deÔ¨Ånition domain for xi is [‚àí5,5]:
f(x) =
n
‚àë
i=1
x2
i
(A.2)
A.3. Ackley‚Äôs function from B¬®ack‚Äôs book [3]. The problem is in real space with n
variables. This is a minimum problem whose global optimal value is 0.0 at x = 0.
As an exercise, we set n = 2. The deÔ¨Ånition domain for xi is [‚àí20,30]:
f(x) = ‚àí20exp
(
‚àí0.2
‚àö
1
n
n
‚àë
i=1
x2
i
)
‚àíexp
(
1
n
n
‚àë
i=1
cos(2œÄxi)
)
+20+e
(A.3)
where e ‚âà2.71828 is the Euler number.
403

404
A Benchmark Problems
A.4. From Yao et al.‚Äôs paper [4]. The problem is in real space with two variables. It
is a minimum problem whose global optimal value is ‚àí1.0316285. The deÔ¨Ånition
domain for both x1 and x2 is [‚àí5,5]:
f(x) = 4x2
1 ‚àí2.1x4
1 + 1
3x6
1 +x1x2 ‚àí4x2
2 +4x4
2
(A.4)
A.5. From Lozano et al.‚Äôs paper [5]. This is a frequency modulation sound parame-
ter identiÔ¨Åcation problem, i.e., to specify six parameters a1, œâ1, a2, œâ2, a3, and œâ3
of the frequency modulation sound model represented by
y(t) = a1 sin
( 2œÄ
100œâ1t +a2 sin
( 2œÄ
100œâ2t +a3 sin
( 2œÄ
100œâ3t
)))
The objective function is the summation of the square errors between the solution
and the real model as follows:
f (a1,œâ1,a2,œâ2,a3,œâ3) =
100
‚àë
t=0
(y(t)‚àíy0 (t))2
(A.5)
where the real model y0(t) is as follows:
y0 (t) = 1.0sin
( 2œÄ
1005.0t ‚àí1.5sin
( 2œÄ
1004.8t +2.0sin
( 2œÄ
1004.9t
)))
This is a minimum problem whose global optimal value is 0. The deÔ¨Ånition domain
for all variables is [‚àí6.4,6.35].
A.6. Schwefel‚Äôs Problem from Suganthan et al.‚Äôs paper [6]. The problem is in real
space with two variables. It is a minimum problem whose global optimal value is 0
at x1 = 1 and x2 = 3. The deÔ¨Ånition domain for both x1 and x2 is [‚àí20,20]:
f(x) = max{‚à£x1 +2x2 ‚àí7‚à£,‚à£2x1 +x2 ‚àí5‚à£}
(A.6)
Benchmark Problems for Chap. 4
All the benchmark problems for constrained optimization are selected from Koziel
and Michalewicz‚Äôs paper [7] and Runarsson and Yao‚Äôs paper [8].
A.7. This is a minimum problem with two real variables as follows. The deÔ¨Ånition
domains are 13 ‚â§x1 ‚â§100 and 0 ‚â§x2 ‚â§100:
f (x) = (x1 ‚àí10)3 +(x2 ‚àí20)3
s.t.
‚àí(x1 ‚àí5)2 ‚àí(x2 ‚àí5)2 +100 ‚©Ω0
(x1 ‚àí6)2 +(x2 ‚àí5)2 ‚àí82.81 ‚©Ω0
(A.7)

A Benchmark Problems
405
The optimum solution is x‚àó= (14.095,0.84296) with f(x‚àó) = ‚àí6961.81388. Both
constraints are active.
A.8. This is a minimum problem with Ô¨Åve real variables as follows. The deÔ¨Ånition
domains are 78 ‚â§x1 ‚â§102, 33 ‚â§x2 ‚â§45, and 27 ‚â§xi ‚â§45 for i = 3,4,5:
f (x) = 5.3578547x2
3 +0.8356891x1x5 +37.293239x1 ‚àí40792.141
s.t.
85.334407+0.0056858x2x5 +0.0006262x1x4 ‚àí0.0022053x3x5 ‚àí92 ‚©Ω0
‚àí85.334407‚àí0.0056858x2x5 ‚àí0.0006262x1x4 +0.0022053x3x5 ‚©Ω0
80.51249+0.0071317x2x5 +0.0029955x1x2 +0.0021813x2
3 ‚àí110 ‚©Ω0
‚àí80.51249‚àí0.0071317x2x5 ‚àí0.0029955x1x2 ‚àí0.0021813x2
3 +90 ‚©Ω0
9.300961+0.0047026x3x5 +0.0012547x1x3 +0.0019085x3x4 ‚àí25 ‚©Ω0
‚àí9.300961‚àí0.0047026x3x5 ‚àí0.0012547x1x3 ‚àí0.0019085x3x4 +20 ‚©Ω0
(A.8)
The optimum solution is x‚àó= (78,33,29.995256025682,45,36.775812905788)
with f(x‚àó) = ‚àí30665.539. Two constraints are active.
A.9. This is a minimum problem with four real variables as follows. The deÔ¨Ånition
domains are 0 ‚â§xi ‚â§1200 for i = 1,2 and ‚àí0.55 ‚â§xi ‚â§0.55 for i = 3,4:
f (x) = 3x1 +0.000001x3
1 +2x2 + 0.000002
3
x3
2
s.t.
‚àíx4 +x3 ‚àí0.55 ‚©Ω0
‚àíx3 +x4 ‚àí0.55 ‚©Ω0
1000sin(‚àíx3 ‚àí0.25)+1000sin(‚àíx4 ‚àí0.25)+894.8‚àíx1 = 0
1000sin(x3 ‚àí0.25)+1000sin(x3 ‚àíx4 ‚àí0.25)+894.8‚àíx2 = 0
1000sin(x4 ‚àí0.25)+1000sin(x4 ‚àíx3 ‚àí0.25)+1294.8 = 0
(A.9)
The best known solution is x‚àó= (679.9453,1026.067,0.1188764,‚àí0.3962336)
with f(x‚àó) = 5126.4981.
Benchmark Problems for Chap. 5
A.10. Adopted from Spears‚Äô problem [1]. The problem is in binary space with ten
variables. There are three peaks, Peak1 = (0011010101), Peak2 = (0010010111),
and Peak3 = (1011000100). The objective value of a solution string is as follows:
f (string) = 1
10 max
i=1,2,3{10‚àíHamming(string,Peaki)}
(A.10)
where Hamming( ) is the function to calculate two binary strings‚Äô Hamming dis-
tance. The smaller the objective value is, the better the solution is.

406
A Benchmark Problems
A.11. From Mahfoud‚Äôs thesis [9]. This is a one-variable maximum problem as fol-
lows. The deÔ¨Ånition domain for the variable is [0,1]. There are Ô¨Åve peaks at 0.080,
0.247, 0.451, 0.681, and 0.934 with height 1.000 and we are interested in Ô¨Ånding
them all:
f (x) = sin6 (
5œÄ
(
x0.75 ‚àí0.05
))
(A.11)
A.12. From Mahfoud‚Äôs thesis [9]. This is a one-variable maximum problem as fol-
lows. The deÔ¨Ånition domain for the variable is [0,1]. There are Ô¨Åve peaks at 0.080,
0.247, 0.451, 0.681, and 0.934 with heights 1.000, 0.948, 0.770, 0.503, and 0.250,
respectively separately. We are interested in Ô¨Ånding them all:
f (x) = e‚àí2(ln2)( x‚àí0.08
0.854 )
2
sin6 (
5œÄ
(
x0.75 ‚àí0.05
))
(A.12)
A.13. This is a two-variable maximum problem as follows. The deÔ¨Ånition domain
for both variable is [‚àí6,6]. There are four peaks, which are at (3.000,2.000),
(3.584,‚àí1.848), (‚àí3.779,‚àí3.283), and (‚àí2.805,3.131) with height 2500.000, and
we are interested in Ô¨Ånding them all:
f (x) = 2500‚àí
(
x2
1 +x2 ‚àí11
)2 ‚àí
(
x1 +x2
2 ‚àí7
)2
(A.13)
A.14. Massively deceptive problem from Goldberg‚Äôs report [10]. It is a maximum
problem with 30 binary variables deÔ¨Åned as follows. It has 32 global peaks and more
than one million local peaks. We are only interested in Ô¨Ånding the 32 global peaks
at (000000, 000000, 000000, 000000, 000000), (000000, 000000, 000000, 000000,
111111), ‚ãÖ‚ãÖ‚ãÖ,(111111, 111111, 111111, 111111, 111111) with height 5.0:
f (x0,‚ãÖ‚ãÖ‚ãÖ,x29) =
4
‚àë
i=0
u
(
5
‚àë
j=0
x6i+j
)
(A.14)
where ‚àÄxk ‚àà{0,1} ,k = 0,‚ãÖ‚ãÖ‚ãÖ,29 and the deÔ¨Ånition of function u(s) is as follows:
u(s) =
‚éß
Ô£¥
Ô£¥
‚é®
Ô£¥
Ô£¥
‚é©
1
s ‚àà{0,6}
0
s ‚àà{1,5}
0.360384
s ‚àà{2,4}
0.640576
s = 3
Benchmark Problems for Chap. 6
A.15. From Van Veldhuizen‚Äôs thesis [11]. The problem is in real space with one
variable. It has two minimum objectives. The deÔ¨Ånition domain for x is [‚àí105,105]:
f1 (x) = x2
f2 (x) = (x‚àí2)2
(A.15)

A Benchmark Problems
407
It has a convex PF*.
A.16. Adopted from Van Veldhuizen‚Äôs thesis [11]. The problem is in real space with
n variable. It has two minimum objectives. The deÔ¨Ånition domain for xi is [‚àí5,5].
As an exercise, we set n = 3:
f1 (x) =
n‚àí1
‚àë
i=1
(
‚àí10exp
(
‚àí0.2‚àó
‚àö
x2
i +x2
i+1
))
f2 (x) =
n
‚àë
i=1
(
‚à£xi‚à£0.8 +5sin(xi)3)
(A.16)
It has a disconnected PF*.
A.17. From Zitzler et al.‚Äôs ZDT set [12]. The problem is in real space with n vari-
ables. It has two minimum objectives. The deÔ¨Ånition domain for xi is [0,1]:
f1 (x) = x1
f2 (x) = g(x)
(
1‚àí
( f1 (x)
g(x)
)2)
g(x) = 1+
9
n‚àí1
n
‚àë
i=2
xi
(A.17)
where n = 30. Its P* is formed by g(x) = 1 with a concave PF*.
A.18. From Tanaka et al.‚Äôs problem [13]. The problem is in real space with two
variables. It has two minimum objectives. The deÔ¨Ånition domain for xi is [0,œÄ]:
f1 (x) = x1
f2 (x) = x2
s.t.
‚àíx2
1 ‚àíx2
2 +1+0.1cos
(
16arctan
(x2
x1
))
‚â§0
(x1 ‚àí0.5)2 +(x2 ‚àí0.5)2 ‚â§0.5
(A.18)
It is a constrained MOP with a disconnected PF*.
A.19. Okabe‚Äôs problem [14]. The problem is in real space with three variables. It
has two minimum objectives. The deÔ¨Ånition domain for x1 is [‚àíœÄ,œÄ] and x2,x3 is
[‚àí5,5]:
f1 = x1
f2 = 1‚àí1
4œÄ2 (x1 +œÄ)2 +‚à£x2 ‚àí5cos(x1)‚à£
1
3 +‚à£x3 ‚àí5sin(x1)‚à£
1
3
(A.19)
PF* is f2 = 1‚àí1
4œÄ2 (f1 +œÄ)2, f1 ‚àà[‚àíœÄ,œÄ] and P* is (x1,x2,x3) = (Œæ,5cos(Œæ),5sin(Œæ)),
Œæ ‚àà[‚àíœÄ,œÄ].

408
A Benchmark Problems
Benchmark Problems for Chap. 7
A.20. Solve the 100-item knapsack problem of Eqs. 7.4 and 7.5, where W = 1000
and item data are listed in Table A.1:
Table A.1 100-item knapsack problem

	

(
	

(
 	

(
	

(

"







"
"
&

#)

"
"
)

&

""

&

##
#
#
"




"#
"
"

")

&
"




"&
&


#

)





#)
)





)


&

#
)
#
"





"


#
&

#
&
#

#&
"
#


#


&

#



&


#
"

) "


"

)
&)
)
#
&

 ##



#

&
)
#


 "
#
"



#
&
#"
&"

 )
&
#





##

)
 

&
&


#

#&


 #
#
)
")




&)
#&

 


#



)
&
)
)
" )
)

#

"
#)

&
&

# "
)


&
#


&
#

& #
"

)
)
&

&
&
"

) 




")
"#

&
#)

 
"



"


&
)
&
 &
"
"
"&

"
"

&"

#
 "
&
#

"
"
#&

&#
"
#
 

&
#)
"
"
&#

&&


 "

)

"
"

"
))







A.21. Solve the 29-city Euclidean TSP.1 The cities‚Äô coordinate data are listed in
Table A.2:
1 This problem is from TSPLIB.

References
409
Table A.2 29-city TSP

	(


	(


	(


	(



)
")
&
"&)
)
"
)
&)

#)
"&)

)
)
)
")
)
#
)
#)

&)
)

)
)&)

#)
)
&
))
&)
"
)
)

")
))

")
))
)
&)
&)
#
)
&)

")
))

&")
)

#)
"")
&
)
&#)

))
)")

)
"))

&)
))



"
)
)

")
&))

#)
)



#
&)
)

#)
))

)
))




A.22. Solve the four-job, three-machine JSSP from Baker and Trietsch‚Äôs book [15].
The technological requirements are listed in Table A.3:
Table A.3 Four-job, three-machine JSSP








+

/
	






> 



,
,
,
> 



,
,
,
> 



,
,
,
> 



,
,
,



References
1. Spears WM (2004) Evolutionary algorithms: the role of mutation and recombination.
Springer, Berlin Heidelberg New York
2. Eiben AE, Smith JE (2003) Introduction to evolutionary computing. Springer, Berlin Heidel-
berg New York
3. B¬®ack T (1996) Evolutionary algorithms in theory and practice: evolution strategies, evolution-
ary programming, genetic algorithms. Oxford University Press, Oxford, UK
4. Yao X, Liu Y, Lin G (1999) Evolutionary programming made faster. IEEE Trans on Evol
Comput 3(2):82‚Äì102
5. Lozano M, Herrera F, Krasnogor N et al (2004) Real-coded memetic algorithms with
crossover hill-climbing. Evol Comput 12(3):273‚Äì302

410
A Benchmark Problems
6. Suganthan P, Hansen N, Liang J et al (2005) Problem deÔ¨Ånitions and evaluation criteria for the
CEC 2005 special session on real parameter optimization. Tech. rep., Nanyang Technological
University and IIT Kanpur, Singapore
7. Koziel S, Michalewicz Z (1999) Evolutionary algorithms, homomorphous mappings, and con-
strained parameter optimization. Evol Comput 7(1):19‚Äì44
8. Runarsson T, Yao X (2000) Stochastic ranking for constrained evolutionary optimization.
IEEE Trans on Evol Comput 4(3):284‚Äì294
9. Mahfoud S (1995) Niching methods for genetic algorithms. Ph.D. thesis, University of Illinois
at Urbana-Champaign
10. Goldberg DE, Goldberg DE, Deb K et al (1992) Massive multimodality, deception, and genetic
algorithms. Tech. rep., Illinois Genetic Algorithms Laboratory, UIUC
11. Veldhuizen V (1999) Multiobjective evolutionary algorithms: classiÔ¨Åcations, analyses, and
new innovations.
Ph.D. thesis, Air Force Institute of Technology School of Engineering,
Wright-Patterson AFB, OH
12. Zitzler E, Deb K, Thiele L (2000) Comparison of multiobjective evolutionary algorithms:
empirical results. Evol Comput 8(2):173‚Äì195
13. Tanaka M, Watanabe H, Furukawa Y et al (1995) GA-based decision support system for mul-
ticriteria optimization. In: Proceedings of the IEEE international conference on systems, man
and cybernetics, 2:1556‚Äì1561
14. Okabe T, Jin Y, Olhofer M et al (2004) On test functions for evolutionary multi-objective
optimization. In: Proceedings of the international conference on parallel problem solving
from nature, pp 792‚Äì802
15. Baker KR, Trietsch D (2009) Principles of sequencing and scheduling. Wiley, New York

Index
Symbols
(1+1)-ES
27
(1+Œª)-ES
27
(1, Œª)-ES
27
(Œº +Œª)-ES
26
(Œº, Œª)-ES
27
Œ± level comparison
157
Œµ-Pareto set
224
Œµ-approximate Pareto set
224
Œµ-dominance
223
Œµ-dominance-based PI
240
Œµ-dominate
240
‚àû-norm
13
k-means clustering
174
0/1 programming
264
1-norm
13
2-D Euclidean TSP
277
2-norm
13
A
AARS
228
AbYSS
219
Acceleration coefÔ¨Åcient
340
ACO
328
ACO‚Ñù
336
ACS
332
Active
136, 249
Active schedule
303
Adaptive control
83
Adaptive penalty function
145
Adaptive weight sum method
200
Adjacent code
283
Adjacent pairwise interchange
309
AES
106
AfÔ¨Ånity
357
AfÔ¨Ånity maturation process
360
Aggregate Ô¨Åtness function
198
AI
4
aiNet
376
AIS
356
Alternative hypothesis
111
Annealing
54
ANOVA
113
Ant colony optimization
328
Ant colony system
332
Ant system
331
Antibody
356
Antigen
355
Approximation
383
ARB
368
Archive
78, 204
Archive-based hybrid scatter search
219
Arithmetic crossover
47
ArtiÔ¨Åcial immune system
7, 356
ArtiÔ¨Åcial intelligence
4
ArtiÔ¨Åcial neural networks
7
ArtiÔ¨Åcial recognition ball
368
AS
331
ASCHEA
147
Assortative mating
63
Asymmetric TSP
277
Attainment surface
242
Attraction basin
40
Automatic accumulated ranking strategy
228
B
B cell
355
Backus‚ÄìNaur form
392
Baldwin learning
117
Bare bones particle swarm
347
Benchmark problem
102
411

412
Index
Best/1
61
Best/2
61
Bias
235
Bin packing
264
Binary code
15, 272, 282
Binary index
237
Binary representation
15
Binary tournament selection
73
Bit-Ô¨Çip mutation
21
Blend crossover
47
Bloat
398
Block
309
BLX
47
BNF
392
Boltzmann scaling
71
Boltzmann selection
79
Boltzmann tournament selection
74
Boundary mutation
52
Boundary requirement
248
Bounded knapsack problem
270
BSF
106
C
Canonical particle swarm
344
Chromosome
15
ClassiÔ¨Åcation
5
Clearing
173
Clonal selection algorithm
359
CLONALG
359
CLPSO
347
Clustering
5
Code
7
COEA
137
Coevolutionary shared niching
179
Cognitive component
340
Collaborative Ô¨Åltering
366
Combination
263
Combinatorial optimization
263
Competitive coevolution
116
Complete graph
276
Completion time
301
Comprehensive learning particle swarm
optimization
347
Compromise method
201
Computational intelligence
7
Concave function
13
Concentration
358
Connected graph
276
Conservative
78
Constrained dominance
247
Constrained optimization EA
137
Constrained optimization problem
135, 248
Constrained satisfaction problems
136
Constriction coefÔ¨Åcient
344
Construction algorithm
267
Construction approach
198
Construction method
267
Contraction
31
Convergence
80
Convergence graph
108
Convex combination
13
Convex function
13
Cooperative coevolution
115
COP
135, 248
Correlated mutation
57
Covariance matrix adaptation
92
Cover
195
Coverage
238
Coverage difference
239
Criterion space
195
Critical path
302
Crossover
20
Crossover rate
21
Crowding distance
208
CSA
359
CSN
179
CSPs
136
Current to best/1
61
Current/2
61
Curve Ô¨Åtting
383
CX
291
Cycle
277
Cycle crossover
291
D
DC
180
DE
33
Deadlock
301
Death penalty
145
Decision problem
266
Decision space
195
Decision vector
195
Decoding
16, 41
Decomposable
45, 235
DeÔ¨Ånition domain
41
Degree
276
Delay
303
Density
213
Depth
386
Derandomized mutative step-size control
94
Descendant
15
Descriptive statistics
109
Deterministic control
83
Deterministic crowding
180
Deterministic replacement
76
Deterministic Turing machine
266

Index
413
Differential evolution
33
Diploid
123
Direct approach
310
Direct preceding operation
300
Direct predecessor
300
Direct search methods
28
Direct succeeding operation
300
Direct successor
300
Directed graph
276
Direction-based search
28
Disconnected graph
276
Discrete crossover
44
Disjunctive arcs
302
Displacement mutation
286
Distance matrix
277
Distance preserving crossover
296
Distinguishable
168
DiversiÔ¨Åcation
40
DM
286
DNS
175
Domain of attraction
17
Dominance
195
DPI
176
DPX
296
DTLZ
236
Dynamic
122
Dynamic Ô¨Åtness sharing
178
Dynamic inbreeding
176
Dynamic line-breeding
176
Dynamic niche identiÔ¨Åcation
177
Dynamic niche sharing
175
Dynamic peak identiÔ¨Åcation
176
Dynamic species identiÔ¨Åcation
178
Dynamic static penalty
145
E
EAs
6
EAX
293
EC
6
Edge
276, 385
Edge assembly crossover
293
Edge code
282
Edge point
208
Edge recombination crossover
292
EfÔ¨Åcient solution
196
EI
6
Elitism
78
EM
286
EMO
197
Encoding
16, 41
ENPM
183
Enumeration
263
EP
27
Equivalent
195
Error ratio
237
ERX
292
ES
25
Estimation of distribution algorithm
94
Euclidean TSP
277
Evaluation
17
Evolution strategy
25
Evolutionary algorithm
6
Evolutionary computation
6
Evolutionary gradient search
91
Evolutionary intelligence
6
Evolutionary multiobjective optimization
197
Evolutionary programming
27
Evolvability
123
Evolvable hardware
125
Evolving PI
106
Exchange mutation
286
Expansion
31
Expert systems
7
Exploitation
40
Exploration
40
Extrapolation
383
F
Factor
113
Failed mating
87
Feasible
136
Feasible rate
160
Feasible region
136
Feasible run
160
FIPS
344
Fitness approximation
122
Fitness cases
388
Fitness function
16
Fitness sharing
171
Fitness space
195
Fitness transferral
70
Fitness value
7, 15, 16
Fixed weight sum method
199
FJSSP
304
Flexible job-shop scheduling problem
304
Flow-shop scheduling
264
Fully informed particle swarm
344
Fusion operator
44
Fuzzy inference
87
Fuzzy logic controller
87
Fuzzy systems
7
G
Gene
15, 16

414
Index
Gene expression programming
390
Generation
15, 22
Generation gap
76
Generation method
267
Generational
77
Generational distance
239
Genetic drift
65
Genetic local search
116
Genetic programming
381
Genome
15
Genomic imprinting
179
Genotype
16
GEP
390
GifÔ¨Çer‚ÄìThompson algorithm
305
Global left-shift
303
Goal programming
201
GP
381
Grammatical evolution
392
Graph
276
Graph theory
276
Gray code
42
Grouping problem
264
H
Hamiltonian cycle
277
Hamiltonian path
277
Hamming cliff
42
Hamming distance
42
Head
390
Heuristic algorithms
7
Heuristic selection
119
Hill climber
117, 118
HM
140
Homomorphous mapping
140
Hybrid EA
116
Hyper-heuristics
118
Hyperarea
238
Hyperbox
207
Hypercube
207
Hypervolume
238
Hypervolume ratio
239
Hypothesis testing
111
I
Ideal point
200
Idiotypic network theory
364
Idle time
303
Illegal
62, 136, 143
IM
287
Immune network theory
364
Incremental
77
Indirect approach
310
Individual
7, 15
Inertia coefÔ¨Åcient
340
Infeasible
136
Initialization
17
Insertion mutation
286
Instance-based algorithm
95
Integer programming
264
IntensiÔ¨Åcation
40
Interactive EA
66
Intermediate crossover
25
Internal node
385
Interpolation
383
Intrusion detection
373
Inversion mutation
287
Island
187
ISM
286
Iteration
22
J
Job
300
Job-based code
311
Job-shop scheduling problem
300
JSSP
300
K
Knapsack problem
270
L
Lamarckian learning
117
Leaf
385
Learning problem
382
Learning strategies
61
Lethal
172
Level
113
Linear scaling
68
Link
385
Local left-shift
303
Local search
116
Local search algorithm
267
Local search method
267
Local tournament algorithms
181
Locus
16
Loop
277
M
MA
116
Machine
300
Makespan
301
Many-to-one mapping
235
Mating pool
15, 18

Index
415
Mating restriction
63
Matrix code
285
MBF
106
Memetic algorithm
116
Meta-Lamarckian Learning
120
Metaheuristics
8
Metric TSP
277
Micro-GA
216
Micro-GA for multiobjective optimization
216
MNC
181
Model-based algorithm
95
ModiÔ¨Åed distance
208
MOEAs
197
Momentum component
340
MOP
195
MOP rank
204
Move acceptance
119
MPR
183
MSC
85
MTSP
319
Multiattribute optimization
195
Multicriteria decision making
195
Multicriteria optimization problem
195
Multidimensional knapsack problem
270
Multimodal
235
Multimodal EAs
167
Multimodal problem
166
Multimodality
165
Multinational
187
Multiniche crowding
181
Multiobjective evolutionary algorithms
197
Multiobjective knapsack problem
271
Multiobjective optimization problem
195
Multiple traveling salesman problem
319
Multiple-point crossover
43
Mutant
21
Mutation
20
Mutation rate
21
Mutational heuristics
118
Mutative step-size control
85
N
Natural crossover
298
Natural expression
298
Negative assortative mating
63
Negative ideal point
200
Negative selection
371
Neighbor structure
342
Neutral network
79
Neutral plateaus
79
Niche
167
Niche count
171
Niche EAs
168
Niche radius
168
NLP
135
No Free Lunch theorem
101
Node
385
NOFE
105
Noise
121
Nominal convergence
216
Nondelay schedule
303
Nondeterministic Turing machine
266
Nondominated solution
195
Nondominated sorting genetic algorithm
209
Noninferior solution
196
Nonlinear programming
135
Nonmetric TSP
277
Nonoverlap
76
Nonseparable variables
45
Nonstationary
122
Nonuniform mutation
54
Norm
13
Normal mutation
26, 54
NP-complete
266
NP-complete problem
7, 266
NP-hard
267
NP-hard problem
267
NS
371
NSGA-II
209
Null hypothesis
111
NX
298
O
Objective space
195
Objective vector
195
OBX
290
Offspring
15
OKA
236
Open-shop scheduling problem
304
Operation
300
Operation domain
41
Operation-based code
310
Operations research
4
OR
4
Order
276
Order crossover
289
Order-based crossover
290
Ordinal code
284
Organic PSO
340
Orthogonal experiment design
113
OSSP
304
Outcrossing
176
Overall PI
106
OX
289

416
Index
P
PAES
215
Panmictic mating
63
Parallel
276
Parallel evolutionary algorithm
125
Parameter control
83
Parameter optimization
381
Parameter setting
82
Parameter tuning
82
Parameter vector
195
Parameter-less GA
99
Parameterized uniform crossover
44
Parent-centric crossover
50
Parental selection
76
Parents
15
Pareto archived evolution strategy
215
Pareto competition
205
Pareto envelope-based selection algorithm
214
Pareto front
196
Pareto frontier
196
Pareto optimal front
196
Pareto optimal set
196
Pareto optimal solution
195
Pareto rank method
204
Pareto set
196
Pareto solution set
196
Partial schedule
305
Partially mapped crossover
288
Particle swarm optimization
328, 339
Path
276
Path code
283
PBX
290
PCX
50
Penalty coefÔ¨Åcient
144
Penalty function
144
Performance assessment
105
Performance graph
108
Performance index
105
Performance indicator
105
Performance indices
160, 183, 236
Performance measure
105
Performance metric
105
Permutation
264
PESA
214
PESA-II
215
Phenotype
16
Pheromone
329
Pheromone concentration
329
PI
105, 236
PMX
288
Polynomial mutation
60
Polynomial transformation
266
Population
7, 15
Population diversity
40
Population size
15
Position-based crossover
290
Positive assortative mating
63
Positive ideal point
200
Power law scaling
70
Precedence constraint
300
Precedence relationship
300
Preceding operation
300
Predecessor
300
Preference-based approach
198
Preference-list-based code
314
Premature
66
Primitive set
385
Priority dispatching rule
306
Priority-rule-based code
315
Probabilistic crowding
181
Probability of being selected
19
Processing time
300
ProÔ¨Åt/weight ratio
272
Progeny
15
Proportional selection
67
PSO
328, 339
R
Rand/1
61
Rand/2
61
Random key code
282
Random mating
63
Random walk
66
Random weight sum method
199
Random-key-based code
313
Rank-density-based genetic algorithm
228
Ranking
72
Raw Ô¨Åtness
212
RCEA
45
RDGA
228
Real-code EA
45
Recombination
20
Redundant encodings
123
Reference point
141
Reference set
32, 237
ReÔ¨Çection
30
Regression
5
Regret function
200
Relative Ô¨Åtness value
19
Repertoire
358
Replacement
22, 76
Representation
16
Reproductive operators
18
Restart
54
Restricted tournament selection
181

Index
417
Ridge
79
RLAIS
368
Robustness
121
Root
385
Rough set
7
Roulette wheel selection
19
RTS
181
RWS
19
S
SAGA
79
Sampling with replacement
13
Sampling without replacement
13
Satisfaction level
156
SBX
48
Scaling
68
Scatter search
32
SCGA
182
Schedulable
305
Schedule
299
Scheduling
299
Scheduling problem
264
Search space
135
Selection bias
19
Selection process
18
Selection with replacement
13
Selection without replacement
13
Selective bias
67
Selective error
67
Selective pressure
40
Self-adaptive control
83
Self-cycle
277
Semiactive schedule
303
Sensitivity analysis
121
Separable variables
45, 235
Sequential niche
169
SGA
14
Shape space
358
Shared Ô¨Åtness value
171
Sharing function
171
Shifting bottleneck procedure
308
Shrink
31
SI
328
Sigma truncation
70
SigniÔ¨Åcance level
112
SIM
287
Simple genetic algorithm
14
Simple graph
276
Simple inversion mutation
287
Simplex crossover
48
Simplex search
29
Simulated annealing
8, 53
Simulated binary crossover
48
Single-point crossover
21
Size
276, 386
SMES
155
Social component
340
Soft computing
7
Solution landscape
14
Spacing
244
SPEA2
211
Species
167
Species adaptation genetic algorithm
79
Species conserving genetic algorithm
182
Spread factor
48
SPX
48
Squeeze factor
207
SR
106, 151
Stagnation
331
Static penalty function
144
Statistical inference
110
Statistical signiÔ¨Åcance
110
Statistical visualization
107
Steady state
77
Stigmergy
349
Stochastic ranking
151
Stochastic universal sampling
67
Stop criteria
80
Strategy parameter
81
Strength
212
Strength Pareto evolutionary algorithm
211
Strongly dominate
195
Subgraph
276
Succeeding operation
300
Success performance
160
Successful mating
87
Successful rate
160
Successful run
160
Supervised learning
5
Survivor selection
76
SUS
67
Swap mutation
286
Swarm intelligence
7, 328
Symbolic regression
383
Symmetric TSP
277
Syntax tree
384
System identiÔ¨Åcation
383
T
T cell
355
Tabu search
8
Tagging
187
Tail
390
Takeover
77
Takeover time
64
Target vector
34

418
Index
Technological requirements
300
Tendency requirement
248
Terminal
385
Termination criteria
80
Time complexity
266
Time-varying Ô¨Åtness function
122
Tournament selection
73
Tournament size
73
Transition probability
330
Traveling salesman problem
276
Tree
277
Trial vector
34
TSP
276
Two-point crossover
43
U
U-measure
246
Unary index
237
Unbiased tournament selection
74
Uncertain environment
120
Uncorrelated mutation with n step sizes
55
Uncorrelated mutation with one step size
55
Undirected graph
276
UNDX
51
Uniform crossover
44
Uniform mutation
52
Unimodal normal distribution crossover
51
Unsupervised learning
5
Upper Œ± percent number
111
V
Variable-length code
274
Variation operators
18, 20
Vector optimization problem
195
Vector-evaluated genetic algorithm
202
VEGA
202
Vertex
276
Violation of constraint
143
Visibility
332
W
Waiting queue
314
Weakly dominate
195
Weight sum method
198
WFG
236
Z
ZDT
235


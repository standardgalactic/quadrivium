
Google Certified Professional Cloud Database
Engineer
Practice Questions
First Edition

www.ipspecialist.net
 
 
Document Control
Proposal Name
:
Google 
Certified
Professional Cloud Database
Engineer– 
Practice
Questions
Document Edition
:
First Edition
Document 
Release
Date
:
26th January 2024

 
 
Copyright © 2024 IPSpecialist LTD.
Registered in England and Wales
Company Registration No: 10883539
Registration Office at: Office 32, 19-21 Crawford Street,
London W1H 1PJ, United Kingdom
www.ipspecialist.net
All rights reserved. No part of this book may be reproduced
or transmitted in any form or by any means, electronic or
mechanical, including photocopying, recording, or by any
information storage and retrieval system, without the
written permission from IPSpecialist LTD, except for the
inclusion of brief quotations in a review.
Feedback:
If you have any comments regarding the quality of this book,
or otherwise alter it to better suit your needs, you can
contact us through email at info@ipspecialist.net

Please make sure to include the book’s title and ISBN in
your message.

About IPSpecialist
IPSPECIALIST LTD. IS COMMITTED TO EXCELLENCE AND
DEDICATED TO YOUR SUCCESS.
 
Our philosophy is to treat our customers like family. We
want you to succeed, and we are willing to do everything
possible to help you make it happen. We have the proof to
back up our claims. We strive to accelerate billions of
careers with great courses, accessibility, and affordability.
We 
believe that continuous learning and 
knowledge
evolution are the most important things to keep re-skilling
and up-skilling the world.
Planning and creating a specific goal is where IPSpecialist
helps. We can create a career track that suits your visions as
well as develop the competencies you need to become a
professional Network Engineer. Based on the career track
you choose, we can also assist you with executing and
evaluating your proficiency level, as they are customized to
fit your specific goals.
We help you STAND OUT from the crowd through our
detailed IP training content packages.
 
Course Features:
Self-Paced Learning

Learn at your own pace and in your own time
Covers Complete Exam Blueprint
Prep-up for the exam with confidence
Case Study Based Learning
Relate the content with real-life scenarios 
Subscriptions that Suits You
Get more and pay less with IPS subscriptions
Career Advisory Services
Let the industry experts plan your career journey
Virtual Labs to test your skills
With IPS vRacks, you can evaluate your exam
preparations
Practice Questions
Practice questions to measure your preparation
standards
On Request Digital Certification
On request digital certification from IPSpecialist
LTD.

About the Authors:
This book has been compiled with the help of multiple
professional engineers. These 
engineers 
specialize 
in
different fields, e.g., Networking, Security, Cloud, Big Data,
IoT, etc. Each engineer develops content in their specialized
field that is compiled to form a comprehensive certification
guide.
About the Technical Reviewers:
Nouman Ahmed Khan
AWS/Azure/GCP-Architect, 
CCDE, 
CCIEx5 
(R&S, 
SP,
Security, DC, Wireless), CISSP, CISA, CISM, CRISC,
ISO27K-LA is a Solution Architect working with a global
telecommunication provider. He works with enterprises,
mega-projects, and service providers to help them select the
best-fit technology solutions. He also works as a consultant
to understand customer business processes and helps select
an appropriate technology strategy to support business
goals. He has more than eighteen years of experience
working with global clients. One of his notable experiences
was his tenure with a large managed security services
provider, where he was responsible for managing the
complete MSSP product portfolio. With his extensive
knowledge and expertise in various areas of technology,

including cloud computing, network infrastructure, security,
and risk management, Nouman has become a trusted
advisor for his clients.
Abubakar Saeed
Having started from the grassroots level as an engineer and 
contributed to the Introduction of internet in Pakistan and 
elsewhere, a professional journey of over twenty-nine years 
in 
various 
organizations, 
national 
and 
international. 
Experienced 
in 
leading 
businesses 
with 
a 
focus on 
Innovation and Transformation. 
He is also experienced in Managing, Consulting, Designing,
and implementing projects. Heading Operations, Solutions
Design, and Integration. Emphasizing on adhering to Project
timelines and delivering as per customer expectations,
advocate for adopting technology to simplify operations and
enhance efficiency.
Dr. Fahad Abdali
Dr. Fahad Abdali is a seasoned leader with extensive
experience in managing diverse businesses. With an
impressive twenty years track record, Dr. Abdali brings a
wealth of expertise to the table. Holding a bachelor’s degree
from the NED University of Engineers & Technology and
Ph.D. from the University of Karachi, he has consistently

demonstrated a deep commitment to academic excellence
and professional growth. Driven by a passion for innovation
and a keen understanding of industry dynamics, he has
successfully navigated complex challenges, driving growth
and fostering organizational success.
Mehwish Jawed
Mehwish Jawed is working as a Senior Research Analyst.
She holds a Master's and Bachelor of Engineering degree in
Telecommunication Engineering from NED University of
Engineering and Technology. She also worked under the
supervision of HEC Approved supervisor. She has more than
three published papers, including conference and journal
papers. She has a great knowledge of TWDM Passive Optical
Network (PON). She also worked as a Project Engineer,
Robotic Trainer in a private institute and has research skills
in 
communication 
networks. 
She 
has 
the 
technical
knowledge and industry-sounding information, which she
effectively utilizes when needed. She also has expertise in
cloud platforms, such as AWS, GCP, Oracle, and Microsoft
Azure.
Mohib Abdali
Mohib Abdali is a content specialist for IPSpecialist who has
worked on Cloud certification content such as GCP Cloud

Digital Leader, GCP Cloud Developer, and GCP Cloud
Architect. He has extensive technical knowledge of cloud
operations and has worked extensively with the GCP
console. As a content developer with a year of experience, he
brings a wealth of knowledge and expertise to the table.
With a fresh and open mind, he approaches each project
with a new perspective, ensuring that the content is
innovative, engaging, and impactful.

Free Resources:
For Free Resources: Please visit our website and register to
access 
your 
desired 
Resources 
Or 
contact 
us 
at:
info@ipspecialist.net
Career Report: This report is a step-by-step guide for a
novice who wants to develop his/her career in computer
networks. It answers the following queries:
What are the current scenarios and prospects?
Is this industry moving toward saturation, or are
new opportunities knocking at the door?
What will the monetary benefits be?
Why get certified?
How to plan, and when will I complete the
certifications if I start today?
Is there any career track I can follow to accomplish
the specialization level?
 
Furthermore, this guide provides a comprehensive career
path toward being a specialist in networking and highlights
the tracks needed to obtain certification.
IPS Personalized Technical Support for Customers:
Good customer service means helping customers efficiently
and in a friendly manner. It is essential to be able to handle
issues for customers and do your best to ensure they are

satisfied. Good service is one of the most important things
that can set our business apart from others.
Excellent customer service will attract more customers and
attain maximum customer retention.
IPS offers personalized TECH support to its customers to
provide better value for money. If you have any queries
related to technology and labs, you can ask our technical
team for assistance via Live Chat or Email.

Our Products
 
Study Guides
IPSpecialist Study Guides are the ideal guides to developing
the hands-on skills necessary to pass the exam. Our
workbooks cover the official exam blueprint and explain the
technology with real-life case study-based labs. The content
covered in each workbook consists of individually focused
technology topics presented in an easy-to-follow, goal-
oriented, step-by-step approach. Every scenario features
detailed breakdowns and thorough verifications to help you
completely understand the task and associated technology.
We extensively used mind maps in our workbooks to visually
explain the technology. Our workbooks have become a
widely used tool to learn and remember information
effectively.
Practice Questions
IP Specialists' Practice Questions are dedicatedly designed
from a certification exam perspective. The collection of these
questions from our Study Guides is prepared to keep the
exam blueprint in mind, covering all the important topics of
the blueprint. It is an ideal document to practice and revise
your certification.

Exam Cram
Our Exam Cram notes are a concise bundling of condensed
notes of the complete exam blueprint. It is an ideal and
handy document to help you remember the most important
technology concepts related to the certification exam.
Hands-on Labs
IPSpecialist Hands-on Labs are the fastest and easiest way
to learn real-world use cases. These labs are carefully
designed to prepare you for the certification exams and your
next job role. Whether you are starting to learn technology
or solving a real-world scenario, our labs will help you learn
the core concepts in no time.
IPSpecialist self-paced labs were designed by subject matter
experts and \provided an opportunity to use products in a
variety of pre-designed scenarios and common use cases,
giving you hands-on practice in a simulated environment to
help you gain confidence. You have the flexibility to choose
from topics and products about which you want to learn
more.

Companion Guide
Companion Guides are portable desk guides for the
IPSpecialist 
course 
materials 
that 
users 
(students,
professionals, and experts) can access at any time and from
any location. Companion Guides are intended to supplement
online course material by assisting users in concentrating on
key ideas and planning their study time for quizzes and
examinations.

Google Cloud Certifications
We will be discussing the general structure, basic stuff,
advanced stuff, and much more during the entire course.
These things make the foundation of Google Cloud for your
concept building and continuous learning. You are going to
be challenged by Google in its exam to demonstrate what
you have learned.
Logistics
In the exam, you have to answer 50 questions in 120
minutes, and its fee is USD 200 for each time you attempt
it. Now, this is not something you can do online. You must
either appear in this exam at an authorized testing center
or give it on-site at Google Cloud Next. All of the questions
in the exam will be in the form of Multiple Choice
Questions (MCQs) with at least four options in each
question. Also, some of the questions require multiple
responses; for example, you may select 2 out of 5 options
given in the questions.
If you fail the exam on the first attempt, you have to wait
for 14 days, re-pay the full exam fee, and then retake the
exam. If you are unable to pass the exam on the second
attempt, you have to wait for another 60 days for the third
attempt. If you fail the exam even for the third time, then
you have to wait for a whole year for another attempt.

One of the things you can do to check how prepared you
are for the exam is to take the official practice exam that
Google offers. In this exam, you have got around 20
questions. This official practice exam is free to take
because it is available online using a Google Forum-based
UI. This official practice exam is repeatable, and you can do
it as many times as you want.
For your exam registration, you need to create a Web
Assessor account with your email address. You can cancel
or reschedule your exam without having to pay any extra
fee before 72 hours of the exam. If you make changes
within 72 hours, it will cost some or all of your exam fees.
Remember to bring your IDs and your exam code which you
get at the time of the exam registration. During the exam,
you can see an exam count down timer visible on every
page, and you can flag any of the questions for later review.
The overview page shows flags and lets you jump to any
question. The responses can be changed until the exam
ends. Double-check your answers before moving on.
All 
GCP 
Certifications 
expire 
after 
two 
years. 
Re-
certification expires two years after the renewal exam date,
not the original date.
Quick Summary of the Exam
Wide variety of Google Cloud services and their
working. It focuses on IAM, Compute, Storage, and
a bit of network and data service is included.

Covers 
Cloud 
SDK 
commands 
and 
Console
operations, which are required for day-to-day
work. If you have not used GCP before, ensure that
you do a lot of labs, or else, you will be clueless
about some of the questions and commands
Tests are updated for the latest enhancements.
There is no reference to Google Container Engine,
and Google Kubernetes Engine also covers Cloud
Functions and Cloud Spanner
The list of topics is quite long, but some topics that
you need to cover are General Services, Billing,
Cloud 
SDK, 
Network 
Services, 
VPC, 
Load
Balancer, Identity Service, Cloud IAM, Compute
Services, GCE, Google AE, GKE, Storage Services,
Cloud 
Storage, 
Cloud 
SQL, 
Cloud 
Spanner,
BigQuery, Data Services, Cloud Operations Suite
(former name is Stackdriver), DevOps Services,
Deployment Manager and Cloud Launcher
Final Preparation
Let’s discuss some exam tips. You will get several questions
in exams on Kubernetes and what happens when you
deploy something on Kubernetes. You should be aware of
the best practices for working with Google Cloud. Google
has published valuable documents for the knowledge of
Best Practices for Enterprise Organizations. This course
from IPSpecialist covers the whole exam blueprint. It will
help you get familiar with IAM roles, describing the Cloud
IAM roles you grant to identities to access Cloud Platform
resources. You should spend some time playing with the
GCP Pricing Calculator. Several questions appear on how
the GCP Calculator works and understanding the pricing

structure of the different services. Several questions
appear on how it works. Review gcloud structure before
your exam. Another thing that comes up several times in
the exam is Cloud Audit Logging.
Before the exam, re-read this exam guide and make sure
you can do every task. Revise the key tips or exam tips
provided in the course outline. Revise the key official
documentation pages. Another important thing is to repeat
the course practice exams because they are designed to
identify your gaps and areas of weaknesses and research
targets that matter. This is how you will understand the
scenarios of the questions and suggest the best response.
Go through the practice questions multiple times to make
yourself ready for the exam.
You must identify and overcome the gaps by going through
this exam guide, attempting the practice exams, and
reading student reports about their exams. Fill those gaps
by going through our course, going through the official
documentation, and watching Google's videos.

Soaring
Throughout the course, you will learn a lot of stuff which
will prepare you for the exam. But this is not the end; this is
the beginning of your learning journey. You are going to
take what you will learn into its practical use, where you
build a real system and teach your peers about it. The
entire technology landscape has massively developed and
expanded in the last 20 years; the technology will improve
as development occurs.
About Google Cloud Certifications
Google Cloud Platform (GCP) Certifications are industry-
recognized credentials that validate your technical cloud
skills and expertise while assisting your career growth.
These are some of the most valuable IT certifications right
now since GCP has established an overwhelming lead in the
public cloud market. Despite several tough competitors
such as Microsoft Azure, AWS, and Rackspace, GCP is
gaining ground in the public cloud platform today with an
astounding collection of proprietary services that continues
to grow.
The two key reasons as to why GCP certifications are
prevailing in the current cloud-oriented job market:
There is a dire need for skilled cloud engineers,
developers, and architects – and the current

shortage of experts is expected to continue into the
near future.
The Google Cloud Certified assignment means you
have exhibited the important aptitudes to use
Google Cloud innovation in a manner that can
change organizations and definitively sway the
general population and clients they serve.
Types of Certification
Role-based Certification
Associate- Technical role-based certifications. No
pre-requisite.
Professional- Highest level technical role-based
certification. 
Relevant 
Associate 
certification
required.
About Google Certified – Professional Cloud Database
Engineer Exam

 
Exam Questions
Case study, short answer, repeated
answer, MCQs
Number of
Questions
50-60
Time to Complete
120 minutes
Exam Fee
200 USD
A database expert with two years of Google Cloud
experience and five years of total database and IT expertise
is known as a Professional Cloud Database Engineer.
Applications employ Google Cloud databases to store and
retrieve data, and the Professional Cloud Database
Engineer designs, builds, maintains, and troubleshoots
these databases. Scalable and affordable database solutions
should be easily translated from business and technical
needs by a professional cloud database engineer.
The Professional Cloud Database Engineer exam assesses
your ability to:
Design scalable and highly available cloud
database solutions
Migrate data solutions
Manage a solution that can span multiple
database solutions
Deploy 
scalable 
and 
highly 
available
databases in Google Cloud

Recommended Google Cloud Knowledge
Analyze relevant variables to perform database
capacity and usage planning.
Evaluate database high availability and disaster
recovery options given the requirements.
Determine how applications will connect to the
database
Evaluate appropriate database solutions on Google
Cloud.
Determine 
database 
connectivity 
and 
access
management considerations.
Configure database monitoring and troubleshooting
options.
Design database backup and recovery solutions.
Optimize database cost and performance in Google
Cloud.
Determine solutions to automate database tasks.
Design 
and 
implement 
data 
migration 
and
replication.
Apply concepts to implement highly scalable and
available databases in Google Cloud.
 
Domain
Percentage
Domain
1
Design and plan a
cloud solution
architecture
42%
Domain
2
Design for
security and
compliance
34%

Domain
3
Manage
implementations
of cloud
architecture
14%
Domain
4
Manage and
provision the
cloud solution
infrastructure
10%
 

PRACTICE QUESTIONS
1. A customer wants to use the IAAS offering to deploy a
SQL Server database instance with High Availability to
achieve a 99.99% Monthly Uptime SLA. How should your
infrastructure be designed to achieve the same results?
A. Create GCE VMs in multiple regions and configure native
synchronous database replication.
B. Create GCE VMs in multiple zones and configure native
synchronous database replication.
C. Use Cloud SQL rather than GCE VMs.
D. Use Managed Instance Groups to scale database servers
horizontally across multiple zones.
2. You have a regional three-node Cloud Spanner Instance
that is experiencing slowness and performance issues during
peak business hours. During peak hours, CPU utilization is
around 75%, according to monitoring data. How can you
help improve the existing solution's performance while
keeping costs manageable?
A. Include a fourth node in the configuration.
B. Switch to a multi-regional configuration.
C. Create an alert for the spanner's High priority CPU and
manually increase the number of nodes or processing units.
D. Set up Spanner Autoscaler.

3. You've set up a Bigtable cluster and want to configure
High 
Availability 
to 
meet 
the 
application's 
business
requirements. The 
solution 
should 
support 
automatic
failover. What should you do in order to meet the
requirements?
A. Set up Bigtable replication and a multi-cluster routing app
profile.
B. Set up Bigtable replication and a single-cluster routing
app profile.
C. Create a read-replica of Bigtable in another region.
D. Replicate data to BigQuery.
4. You have been tasked to redesign an application that
currently uses Cloud SQL for MySQL. The application
performance is very sluggish, and users are complaining
about slow query response times. You need to recommend a
database storage solution on Google Cloud that offers sub-
millisecond data access capabilities. Which service would
you recommend?
A. Redesign your application using Firestore and modify
your application to use Firestore SDKs.
B. Redesign your application using Bigtable and modify your
application to use Firestore SDKs.

C. Create a read replica for Cloud SQL and offload your
queries to the read replica endpoint.
D. Redesign your application using Memorystore and modify
your application to use an in-memory database.
5. You have been tasked with developing a mobile app that
will store data in a serverless NoSQL database. The
following requirements apply to the application:
Backend user activity is being recorded.
Keeping track of user preferences.
Receiving in-app notifications and updates.
Help users who have slow or choppy internet
connectivity.
Which database platform do you think is best for this
problem statement?
A. Native Firestore mode
B. SQL in the Cloud
C. Cloud Spanner 
D. Bigtable
6. You have monitoring set up for Cloud SQL storage, and
you are constantly alerted when you run out of space. You
want to automate the process of increasing storage space
whenever an alert is received. How would you go about
doing the same?

A. Data compression on Cloud SQL instances
B. Archive data to Cloud SQL to free up disk space. 
C. Enable automatic storage increases for Cloud SQL. 
D. Create a cloud function task to increase disk space and
trigger alert events.
7. In Google Cloud, you manage multiple Cloud SQL for
MySQL instances. You've been tasked with effectively
monitoring database performance in order to identify
connections 
with 
resource-intensive 
and 
long-running
queries. How can you achieve the same result?
A. Examine the MySQL error logs in Cloud Logging.
B. Enable Cloud SQL Query Insights 
C. Create a custom Cloud Monitoring dashboard to track
slow-running queries
D. Track the problematic connections using the default
Cloud SQL monitoring dashboard.
8. You intend to move your existing data center to GCP. You
intend to lift and shift your application workloads to Google
Compute Engine, as well as your Oracle databases, to GCP's
Bare Metal Solution (BMS). How should the GCP platform
be designed to ensure that application workloads running on
Google Compute Engine in different GCP projects can

communicate 
securely 
and 
methodically 
with 
Oracle
databases?
A. Create a Shared VPC with multiple service projects and
firewall rules to enable secure access to Oracle databases.
B. Set up a private and secure connection from your
applications to the Oracle database VPC using Serverless
VPC Access.
C. Use Private Service Connect to set up a private and
secure connection from your VPCs to Oracle databases.
D. Use GCP Traffic Director to set up connectivity between
Oracle databases and application workloads.
9. You have been tasked with migrating on-premises
PostgreSQL instances to Cloud SQL. You have chosen
Database Migration Service to handle the migration and
cutover tasks. You must first determine the prerequisites for
using Database Migration Service to configure continuous
replication to Cloud SQL. Which of the following must you
consider before launching the Database migration service
task? (Choose two)
A. Turn off all user accounts except the migration service
account.
B. Turn off all foreign key and primary key constraints on the
source database.

C. Make sure that every table has a primary key constraint.
D. Before launching the Data Migration Service task, restart
the database.
E. Add pglogical to the source database.
10. A large corporation is currently running an on-premises
MySQL database. They want to migrate to Google Cloud
with as little downtime as possible and to reduce the
operational 
overhead 
associated 
with 
managing 
the
database system. To meet the requirements, which service
offering and migration service should the customer use?
A. Create a Google Kubernetes Engine (GKE) cluster, run
MySQL in a containerized environment, and then import the
MySQL database dump file produced by the source
database.
B. Create a database backup of the source database using
the mysqldump utility, and then migrate the data to Cloud
SQL using the import functionality.
C. Install and configure the MySQL service on the Compute
Engine VM, and then import the MySQL database dump file
generated from the source database.
D. Make an external replica and use Cloud SQL as a target
to synchronize the data. Plan a maintenance window, switch
to Cloud SQL, and configure the application connection
string to use the Cloud SQL endpoint.

11. You intend to move the SQL Server to the Google Cloud
Platform. You are currently deciding between using Cloud
SQL and SQL Server on Google Compute Engine. Which of
the following factors may have an impact on your decision?
(Choose Three)
A. For user databases, the source SQL Server System
employs column-level encryption.
B. The source SQL Server System makes use of BI
Components such as SSRS and SSIS.
C. The origin SQL Server System connects to other data
sources via Linked Servers.
D. The origin Transparent Data Encryption (TDE) is enabled
on the SQL Server System.
12. A mission-critical application is running on Cloud SQL
for MySQL for a customer. The client wishes to implement a
fault-tolerant and highly available environment. You have
been tasked with ensuring that the application database can
withstand a region-wide outage with a primary region of
Mumbai and a backup region of Delhi. What should you do in
order to meet the requirements?
A. Deploy a Cloud SQL instance in Mumbai region zone A
and create a multiple-zone replica in Mumbai region zone B

for High Availability. Create a read replica in any of the
Delhi region's zones.
B. Deploy a Cloud SQL instance in Mumbai region zone A
and a multiple-zone replica in Delhi region zone B for High
Availability. Create a read replica in any of the Delhi region's
zones.
C. Deploy a Cloud SQL instance in Mumbai region zone A
and a multiple-zone replica in Delhi region zone C for High
Availability. Create a read-replica in any one of the Mumbai
zones.
D. For High Availability, deploy a Cloud SQL instance in
zone A of the Delhi region and create a multiple-zone replica
in zone C of the Mumbai region. Create a read replica in any
of the Delhi region's zones.
13. You are running a mission-critical application on a multi-
zone Cloud SQL for the PostgreSQL database. The primary
replica is located in Region 1's Zone A, and a read replica is
created in Region 1's Zone B. How can you distribute the
application load across the zones between the primary and
read replicas?
A. Configure a network load balancer for the primary and
read replicas of Cloud SQL.
B. Install PgBouncer and configure database connection
pooling between the primary and read replicas of Cloud

SQL.
C. Configure an HTTP(S) load balancer for the primary and
read replicas of Cloud SQL.
D. Connect the Cloud SQL primary and read replicas using
the Cloud SQL Auth proxy.
14. Your global web application that uses Cloud SQL as a
backend solution must be globally accessible and available
24 hours a day, seven days a week. For high availability
configuration, you must enable the Cloud SQL backend.
What are the prerequisites for enabling Cloud SQL High
Availability? (Choose Two)
A. Install an on-premises database with high availability
(HA).
B. Enable Cloud SQL manual backups.
C. Configure backups to write to Google Cloud Storage in a
custom bucket
D. Enable Cloud SQL point-in-time recovery
E. Plan automated backups for Cloud SQL.
15. A customer wants to migrate their existing Oracle
database to the Google Cloud Platform. Which service should
they use to run Oracle on GCP?
A. Google Cloud Spanner

B. Google Cloud SQL
C. Google Cloud Bigtable
D. Google Cloud Dataproc
16. How can you achieve automatic scaling of your database
instances on the Google Cloud Platform?
A. Use Google Cloud Bigtable
B. Use Google Cloud Dataproc
C. Use Google Cloud SQL
D. Use Google Cloud Spanner
17. Which service can be used to migrate an on-premises
MySQL database to the Google Cloud Platform?
A. Google Cloud Storage
B. Google Cloud SQL
C. Google Cloud Bigtable
D. Google Cloud Pub/Sub
18. Which Google Cloud service provides a fully managed,
highly available, and scalable NoSQL document database for
your web, mobile, and server applications?
A. Cloud Firestore

B. Cloud Spanner
C. Cloud Bigtable
D. Cloud Memorystore
19. You need to perform complex analytical queries on large
datasets in the Google Cloud Platform. Which service would
you choose?
A. BigQuery
B. Cloud SQL
C. Cloud Spanner
D. Cloud Datastore
20. Your application requires a fully managed relational
database service that is compatible with PostgreSQL. Which
service should you choose?
A. Cloud SQL
B. Cloud Spanner
C. Cloud Firestore
D. Cloud Datastore
21. You need to cache frequently accessed data to improve
the performance of your application. Which Google Cloud

service can you use?
A. Cloud Storage
B. Cloud Datastore
C. Memorystore for Redis
D. BigQuery
22. Your 
application 
requires 
a 
globally 
distributed,
horizontally scalable NoSQL database. Which service would
you choose?
A. Cloud Spanner
B. Cloud Firestore
C. Cloud Bigtable
D. Cloud Datastore
23. Which Google Cloud service provides a fully managed,
highly available, and scalable MySQL database?
A. Cloud Spanner
B. Cloud Bigtable
C. Cloud Datastore
D. Cloud SQL

24. You want to analyze streaming data in real-time and
generate insights. Which service should you choose?
A. Cloud Dataflow
B. Cloud Pub/Sub
C. BigQuery
D. Cloud Dataproc
25. Your application requires a horizontally scalable key-
value store database. Which Google Cloud service can you
use?
A. Cloud Spanner
B. Cloud Firestore
C. Cloud Datastore
D. Cloud Bigtable
26. You want to store and serve large binary objects, such as
images or videos, in Google Cloud. Which service should you
choose?
A. Cloud Storage
B. Cloud Datastore
C. Cloud Spanner
D. Cloud Firestore

27. You need a highly scalable, fully managed time-series
database for analyzing and visualizing time-series data.
Which service would you choose?
A. Cloud Spanner
B. Cloud Bigtable
C. Cloud Datastore
D. Cloud Monitoring
28. Your application requires a globally distributed, highly
available, and strongly consistent relational database. Which
Google Cloud service can you use?
A. Cloud Spanner
B. Cloud Firestore
C. Cloud Bigtable
D. Cloud SQL
29. You want to migrate your on-premises Oracle database
to Google Cloud. Which service would you use?
A. Cloud Spanner
B. Cloud Bigtable
C. Cloud Datastore

D. Cloud SQL for Oracle
30. You need to manage and orchestrate your data pipelines
for batch and streaming data processing. Which service can
you use?
A. Cloud Dataflow
B. Cloud Pub/Sub
C. Cloud Dataproc
D. Cloud Composer
31. Your 
application 
requires 
a 
globally 
distributed,
horizontally scalable NoSQL database with low-latency read
and write operations. Which service should you choose?
A. Cloud Spanner
B. Cloud Firestore
C. Cloud Bigtable
D. Cloud Datastore
32. Which Google Cloud service provides a fully managed,
serverless data integration and transformation service?
A. Cloud Dataflow
B. Cloud Pub/Sub

C. Cloud Dataproc
D. Cloud Data Fusion
33. You need a fully managed, in-memory data store for
caching frequently accessed data. Which service should you
choose?
A. Cloud Spanner
B. Cloud Firestore
C. Memorystore for Redis
D. Cloud Datastore
34. Your 
application 
requires 
a 
globally 
distributed,
horizontally scalable SQL database with high availability and
strong consistency. Which service would you choose?
A. Cloud Spanner
B. Cloud Firestore
C. Cloud Bigtable
D. Cloud SQL
35. You want to store structured data in a fully managed,
serverless database service that automatically scales with
your application's needs. Which Google Cloud service can
you use?

A. Cloud SQL
B. Cloud Spanner
C. Cloud Firestore
D. Cloud Datastore
36. You need a fully managed, serverless analytics platform
that can analyze large datasets using SQL queries. Which
Google Cloud service should you choose?
A. BigQuery
B. Cloud Pub/Sub
C. Cloud Dataflow
D. Cloud Dataproc
37. You are tasked with designing a highly available and
fault-tolerant database infrastructure for a global e-
commerce platform. The infrastructure needs to be able to
handle millions of transactions per day with minimal
downtime. Which approach should you take?
A. Deploy a multi-region Cloud Spanner database with
automatic sharding for scalability and high availability.
B. Deploy a multi-zone Cloud SQL database with read
replicas in different regions for fault tolerance.

C. Use Cloud Firestore for storing transactional data and
Cloud Bigtable for analytical purposes.
D. Deploy a managed PostgreSQL instance with automatic
backups and point-in-time recovery.
38. Your company operates a social media platform that
generates a large amount of user-generated content. You
need to design a database infrastructure that can handle
high write throughput and provide real-time analytics on the
data. Which Google Cloud service should you choose?
A. Cloud Bigtable
B. Cloud Spanner
C. Cloud Firestore
D. BigQuery
39. Your company runs a global e-commerce platform and
needs a database solution that can handle high read and
write throughput while ensuring strong consistency. Which
Google Cloud service should you choose?
A. Cloud Bigtable
B. Cloud Firestore
C. Cloud Spanner
D. Cloud SQL

40. You are designing a data warehousing solution for a
large retail company. The solution needs to support complex
analytics queries on massive amounts of data. Which Google
Cloud service should you choose?
A. BigQuery
B. Cloud Spanner
C. Cloud Datastore
D. Cloud Bigtable
41. Your company needs a scalable, fully managed database
solution for storing and retrieving JSON documents. Which
Google Cloud service should you choose?
A. Cloud Bigtable
B. Cloud Firestore
C. Cloud Spanner
D. Cloud SQL
42. You are designing a data processing pipeline that
requires real-time data streaming and processing. Which
Google Cloud service should you choose?
A. Cloud Pub/Sub
B. Cloud Dataflow

C. Cloud Bigtable
D. Cloud Dataproc
43. Your company operates a mobile gaming platform that
requires a highly scalable and low-latency database solution
for storing user profiles and game data. Which Google Cloud
service should you choose?
A. Cloud Firestore
B. Cloud Spanner
C. Cloud Datastore
D. Cloud Bigtable
44. You are building a recommendation system for an e-
commerce platform that requires real-time user profiling
and personalized recommendations. Which Google Cloud
service should you choose?
A. Cloud Firestore
B. Cloud Pub/Sub
C. Cloud Spanner
D. Cloud Dataflow
45. You need to design a database infrastructure that can
handle high read-throughput for a content delivery platform.

The database needs to be globally distributed to minimize
latency. Which Google Cloud service should you choose?
A. Cloud Bigtable
B. Cloud Spanner
C. Cloud Firestore
D. Cloud SQL
46. Your company operates a data-intensive application that
requires low-latency access to large amounts of structured
data. Which Google Cloud service should you choose?
A. Cloud Spanner
B. Cloud Bigtable
C. Cloud Firestore
D. Cloud SQL
47. A customer wishes to migrate their existing PostgreSQL
workloads to Google Cloud and achieve an SLA of 99.999%
uptime. The solution should support built-in high availability
and disaster recovery, as well as be simple to maintain.
What solution can you suggest that does not require any
changes to the existing application code that refers to
database-related statements?

A. Upgrade to Cloud SQL and set up HA and cross-region
replication for disaster recovery.
B. Migrate to Google Compute Engine and set up disaster
recovery replication across regions.
C. Migrate to Cloud SQL and set up external replicas in a
customer facility.
D. Configure Spanner to use the PostgreSQL interface and
use multi-regional configuration.
48. You intend to migrate your existing PostgreSQL
workloads to Google Cloud and are considering AlloyDB as
the target platform. You have been tasked with providing an
AlloyDB high-availability solution on Google Cloud. Which of
the following best meets the needs of the customer?
A. Set up AlloyDB to use an HA configuration with a standby
replica in a different Zone
B. Set up AlloyDB read pools
C. AlloyDB provides out-of-the-box support for regional high
availability. Nothing needs to be done
D. Set up cross-region replication to secondary clusters in
AlloyDB
49. You have been tasked with troubleshooting performance
issues with the PostgreSQL instance on Cloud SQL. Which

tools will you use to troubleshoot the problem?
A. Use cloud logging to investigate performance issues.
B. Analyze performance issues using a cloud monitoring
dashboard.
C. Analyze performance issues with Cloud Profiler.
D. Analyze performance issues with Cloud Debugger.
50. You have been tasked with configuring automated
backups for the Cloud Spanner database as part of the
business continuity plan. Backups should be scheduled to
run every 4 hours and be available for restoration in the
event of logical data corruption. How would you configure
the backups to meet the needs of the client?
A. Spanner supports automated backups, and schedules can
be customized with a cron expression. 
B. Change export settings to schedule backups every 4
hours.
C. Configure scheduled backups using the dataflow spanner
backup template.
D. Create a cloud function to create a database backup and
use a cloud scheduler cron-based schedule to trigger the
function every 4 hours.

51. You have a Cloud SQL for MySQL instance running in
the us-east-1 region that is configured for HA. When running
reports, offices in Europe and Asia are experiencing slow
response times. How can you improve read performance in
other locations for the reporting platform?
A. Set up two cross-region read replicas, one in Europe and
one in Asia, and update the reporting platform to connect to
the read replicas.
B. Place the Cloud SQL instance in front of a Cloud CDN.
C. Establish one read replica in the US-east region and
update the reporting platform to connect to the read
replicas.
D. Connect the reporting platform to the stand-by replica.
52. You are using the Google Cloud Spanner database to
power an application. The application is experiencing
request failures, and you discovered DEADLINE_EXCEEDED
errors after reviewing the database logs. What could be the
cause of the errors and timeouts? (Choose Three)
A. Spanner CPU load exceeds thresholds
B. Using the client libraries' default timeout settings for all
requests
C. Inadequate database schema design
D. Inadequate automatic backup settings

53. A customer has deployed a global application on Google
Cloud that uses a multi-regional Cloud Spanner instance as a
backend database. Application owners are complaining
about performance issues following the latest release. You
examined the monitoring dashboard and noticed increased
read latency trends following the release. What should you
check next to help you troubleshoot the problem?
A. Use a single region Spanner configuration
B. Fine-tune query parameters
C. Retrieve stats from SPANNER_SYS.READ_STATS* tables
using SQL statements
D. Retrieve 
stats 
from 
SPANNER_SYS.QUERY_STATS*
tables using SQL statements
54. Given a multi-regional Cloud Spanner instance with
increased read latency trends after a release, what SQL
query should you use to troubleshoot the problem?
A. SELECT * FROM SPANNER_SYS.READ_STATS*
B. SELECT * FROM SPANNER_SYS.QUERY_STATS*
C. SELECT * FROM SPANNER_SYS.INSTANCE_STATS*
D. SELECT * FROM SPANNER_SYS.INSTANCE_CONFIG*

55. What SQL query can you use to retrieve the count of
records in a table named "Orders"?
A. SELECT COUNT(1) FROM Orders
B. SELECT COUNT(OrderID) FROM Orders
C. SELECT COUNT(*) FROM Orders
D. SELECT COUNT(DISTINCT OrderID) FROM Orders
56. How can you retrieve the maximum value from a column
named "Salary" in a table named "Employees"?
A. SELECT MAX(Salary) FROM Employees
B. SELECT MIN(Salary) FROM Employees
C. SELECT TOP 1 Salary FROM Employees ORDER BY
Salary DESC
D. SELECT LAST(Salary) FROM Employees
57. What SQL query should you use to calculate the total
sales amount for each product in a table named "Sales"?
A. SELECT Product, MAX(Amount) FROM Sales GROUP BY
Product
B. SELECT Product, AVG(Amount) FROM Sales GROUP BY
Product
C. SELECT Product, COUNT(Amount) FROM Sales GROUP
BY Product

D. SELECT Product, SUM(Amount) FROM Sales GROUP BY
Product
58. How can you retrieve the first 10 records from a table
named "Customers"?
A. SELECT TOP 10 * FROM Customers
B. SELECT * FROM Customers LIMIT 10
C. SELECT * FROM Customers WHERE ROWNUM <= 10
D. SELECT * FROM Customers FETCH FIRST 10 ROWS
ONLY
59. What SQL query can you use to calculate the average
age of employees in a table named "Employees"?
A. SELECT AVG(Age) FROM Employees
B. SELECT SUM(Age) / COUNT(*) FROM Employees
C. SELECT AVG(DATEDIFF(CURRENT_DATE, BirthDate))
FROM Employees
D. 
SELECT 
AVG(YEAR(CURRENT_DATE) 
-
YEAR(BirthDate)) FROM Employees
60. How can you retrieve the records from a table named
"Products" where the price is greater than 100 and the
quantity is less than 50?

A. SELECT * FROM Products WHERE Price > 100 &&
Quantity < 50
B. SELECT * FROM Products WHERE Price > 100 OR
Quantity < 50
C. SELECT * FROM Products WHERE Price > 100 AND
Quantity < 50
D. SELECT * FROM Products WHERE Price > 100, Quantity
< 50
61. What SQL query should you use to retrieve the distinct
values from a column named "Category" in a table named
"Products"?
A. SELECT Category DISTINCT FROM Products
B. SELECT UNIQUE Category FROM Products
C. SELECT DISTINCT Category FROM Products
D. SELECT Category UNIQUE FROM Products
62. How can you retrieve the top 5 highest-priced products
from a table named "Products"?
A. SELECT * FROM Products ORDER BY Price DESC LIMIT
5
B. SELECT * FROM Products ORDER BY Price ASC LIMIT 5

C. SELECT * FROM Products WHERE ROWNUM <= 5
ORDER BY Price DESC
D. SELECT * FROM Products FETCH FIRST 5 ROWS ONLY
ORDER BY Price DESC
63. What SQL query can you use to calculate the total
revenue generated from sales in a table named "Orders"?
A. SELECT AVG(Price * Quantity) FROM Orders
B. SELECT SUM(Price * Quantity) FROM Orders
C. SELECT COUNT(*) * AVG(Price * Quantity) FROM Orders
D. SELECT MAX(Price * Quantity) FROM Orders
64. Your organization has made the decision to migrate and
consolidate its existing RDBMS workloads to Cloud Spanner.
You have been tasked with developing and carrying out the
migration strategy. One of the development team's action
items is to test existing applications using a spanner
database. The development team needs to test the
applications locally on their laptop because they haven't yet
set up a Cloud Spanner development instance. The
development team's budget has already been depleted, and
they require a cost-effective solution. What do you
recommend they do?

A. Recommend that the developers use Cloud Spanner
Emulator.
B. Create a new development project, provision a Spanner
instance with 100 processing units, and distribute the
credentials for connecting to Spanner to developers.
C. Launch a new development project and set up a cloud-
native environment for developers to collaborate.
D. Set up a new free tier account, a new development
project, and a cloud-native environment where developers
can collaborate.
65. Your company is undergoing digital transformation and
wishes to migrate its legacy databases to Cloud SQL. You
considered migrating your Oracle databases to Cloud SQL
after the initial assessment. How can you ensure that the
migration activities are completed with the least amount of
downtime and fiction?
A. Use Database Migration Service to migrate to Cloud SQL
for Oracle.
B. Use Database Migration Service to migrate to Oracle on
Google Compute Engine.
C. Use Datastream to migrate PostgreSQL to Oracle.
D. Use the import/export utility to migrate to Cloud SQL.

66. You intend to migrate your current MySQL workloads to
Cloud Spanner. Migration planning should include the
configuration of continuous data replication between the
source and target databases to minimize downtime. How do
you keep inserts, updates, and deletes synchronized and
consistent between source and target?
A. Make use of a database migration service.
B. Using CDC to stream changes from the source, third-party
applications such as Striim can subscribe to this stream and
apply the changes to the Spanner database.
C. Make use of the Spanner import/export utility.
D. Using CDC, stream changes from the source and
configure Spanner to subscribe to this stream directly.
67. A client wants to move their highly transactional Oracle
database from VMware Vcenter to Google Cloud. You must
provide an optimal solution for Google Cloud migration
without changing the existing architecture. What is the most
suitbale option?
A. Transfer the database to Cloud SQL.
B. Transfer the database to the Compute Engine
C. Move the database to the Kubernetes Engine.
D. Move the database to GCVE (Google Cloud Virtual
Machine Engine).

68. A large corporation uses AWS Cloud to host multiple
MySQL databases. They intend to use Google Cloud native
migration tooling to migrate their workloads to GCP. They
want to be able to execute and monitor multiple migrations
to Cloud SQL at the same time. They also want to ensure
that there is as little downtime as possible during the
cutover period. What would you suggest?
A. Perform the migration using third-party streaming
applications such as Striim.
B. Use CDC and write an application to synchronize changes
between the source and target databases. 
C. Migrate all databases to Google Cloud using the database
migration service.
D. Create a backup of the source database and import it into
Cloud SQL.
69. You are creating a Bare Metal environment on Google
Cloud to install and run Oracle workloads. When attempting
to install database patches, you notice that the host has no
internet connectivity. What should you do to connect to the
internet to download updates?
A. Attach a public IP address to the bare metal host. 
B. Configure firewall rules to allow internet traffic.

C. Install and configure the NAT Gateway on the Compute
Engine instance.
D. Install and set up Cloud NAT.
70. A government agency wishes to set up a Cloud SQL
instance. They are concerned about the compliance and data
residency requirements associated with running MySQL
workloads on Google Cloud. How do you ensure that data
storage and access requirements are met in accordance with
local laws? (Choose two)
A. Encrypt data in a region with local bodies using customer-
managed keys.
B. Encrypt data using VPC Service Controls and identity and
access management.
C. Transfer data to another GCP region
D. Use the default encryption key to encrypt data.
71. Your current application is built on a microservices
architecture, and all of its components share a single Cloud
SQL instance. Without performing a major refactor, how can
you optimize the existing application design to meet the
application growth requirements while also adhering to
Google's recommended practices?
A. Transfer your data to Cloud Spanner.

B. Transfer to Firestore
C. Increase the instance size 
D. Divide the data into multiple smaller Cloud SQL
instances, each with its own database
72. You have a Google Cloud SQL instance running with a
private IP address and SSL turned off. You have a custom
API running on a colocation server that requires access to
the Cloud SQL database. How can you ensure secure
database connectivity without modifying the Cloud SQL
instance?
A. Enable Cloud SQL Public IP and create a connection
string with Cloud SQL Public IP as the data source, as well
as a username and password, to connect to the database.
B. To connect to the database, create a connection string
with Cloud SQL Private IP as the data source and a
username and password.
C. Use Cloud SQL Proxy with a service account and the
CLoud SQL instance's private IP address.
D. Make use of Cloud SQL Proxy with the service account
and public IP address of the Cloud SQL instance.
73. You work as a database engineer for a company that is
expanding into the global market. Given the platform's

popularity and rapid growth, you want to leverage a scalable
and highly available solution on Google Cloud. The database,
which is ACID-compliant, currently stores transactional,
sales, and inventory data. The existing relational data must
be deployed on Google Cloud. Which platform should you
consider in order to meet the requirement?
A. Use a Firestore with a multi-region location configuration.
B. Use Bigtable in one region with one cluster and a replica
cluster in another.
C. Make use of the Spanner multi-regional instance.
D. Make use of MemoryStore with instances spread across
multiple regions.
74. You have multiple read replicas in your Cloud SQL for
MySQL 
deployment. 
Following 
a 
recent 
application
deployment, you've noticed replication lag between your
Primary replica and read replicas. What should you do to
solve the problem without causing any disruptions? (Choose
Two)
A. Delete the existing read replicas and recreate them.
B. Enlarge the Primary replica.
D. Set parallel replication flags.
C. Increase the size of read replicas.
E. Identify and resolve queries that are running slowly.

75. Your website collects clickstream data in real-time to
analyze customer behavior. You have two Bigtable clusters
in the US region, one in region-1 and one in region-2. You
intend to expand into the APAC region. How can you deploy
new clusters with high availability while adhering to Google
Best practices?
A. Deploy three clusters: one in US region-1, one in Europe
region-1, and one in Asia region-1, with a target CPU
utilization of 75%.
B. Deploy three clusters: one in US region-1, one in US
region-2, and one in US region-3, with a 60% CPU utilization
target.
C. Deploy four clusters: one in US region-1, one in US
region-2, one in Asia region-1, and one in Asia region 2.
Maintain a CPU utilization target of 60%.
D. Deploy four clusters: one in US region-1, one in US
region-2, one in Asia region-1, and one in Asia region 2, with
a target CPU utilization of 35%.
76. You've been given the task of redesigning an application
that currently uses Cloud SQL for MySQL. The application's
performance is extremely slow, and users have complained
about long query response times. You must recommend a

Google Cloud database storage solution with sub-millisecond
data access capabilities. Which service would you suggest?
A. Redesign your application with Firestore and modify it to
use Firestore SDKs.
B. Redesign your application with Bigtable and modify it to
use Firestore SDKs.
C. Create a Cloud SQL read replica and offload your queries
to the read replica endpoint.
D. Redesign your application with Memorystore and modify
it to use an in-memory database.
77. A company using BigQuery must be able to delete all
user account information upon request. How is this possible?
A. Use BigQuery-specific queries for the deletion of data.
B. Use DML statement to update and delete data upon user
request.
C. Both A and B
D. None of the above
78. A customer is migrating several MySQL databases from
their data center to Google Cloud SQL. You have been
tasked with ensuring that the scheduled migrations take
place with as little downtime as possible. What data
synchronization mechanism would you recommend to keep

on-premises data changes synchronized with Cloud SQL?
(Choose Two).
A. Make use of a database migration service.
B. Make use of a cross-region read replica.
C. Use external server replication to connect to Cloud SQL.
D. Make use of the import/export functionality.
E. Make use of a Cloud SQL read replica.
79. A gaming company is developing a mobile application
that will allow users to tailor their game experience based
on player information and preferences. The application
should support dynamic schema and allow users to edit their
online profile. Changes to the profile should be synchronized
with the BigQuery data set to drive analytics. How can you
achieve the same result?
A. Store player profiles in Bigtable with the player's
username as the key.
B. Store player profiles in Cloud SQL with the player's
username as the key.
C. Use Firestore in native mode as a document with a player
profile.
D. Use Firestore in Datastore mode as a document with a
player profile.

80. Your Cloud SQL instance recently experienced downtime
during business hours, resulting in multiple escalations.
Following an examination of the events, it was discovered
that the downtime was caused by maintenance activity. How
can you ensure that this does not result in future unplanned
outages?
A. Migrate to Spanner because it is a zero-maintenance
platform. 
B. Change the Cloud SQL instance's maintenance window to
after business hours. 
C. File a support ticket with Google Cloud about not
receiving maintenance notifications. 
D. Use Cloud scheduler to schedule maintenance windows in
the future.
81. You are in charge of implementing a backup strategy for
a non-critical Cloud SQL instance with a one-week RPO.
How can you make backups while keeping costs to a
minimum?
A. Perform one manual backup per day.
B. Disable automatic backups.
C. Enable automated backups as well as transaction log
backups.

D. Enable automated backups but disable transaction log
backups.
82. The development team hosted an application on an on-
premises virtual machine. The application will use an ODBC
driver to connect to Cloud SQL for SQL Server. The Cloud
SQL instance has a private IP address, and SSL is disabled.
You've been tasked with ensuring that the hosted application
can connect to the SQL Server instance without making any
changes to the configuration. How would you go about doing
the same?
A. As the public IP of the Cloud SQL instance, use a
connection string with SQL Login credentials and a data
source.
B. As the private IP of the Cloud SQL instance, use a
connection string with SQL Login credentials and a data
source.
C. For authentication, use Cloud SQL Auth Proxy with a
service account and connect using the Cloud SQL instance's
private IP address.
D. For authentication, use Cloud SQL Auth Proxy with a
service account and connect to the Cloud SQL instance's
public IP address.

83. A game developer intends to release a new mobile-only 
game that will be available for both IOS and Android users. 
The primary backend database for storing game data and 
player information is Cloud Spanner. The cloud costs 
exceeded the anticipated budget during the initial testing, 
but query performance is within the SLA. How do you 
ensure database availability and performance while keeping 
costs to a minimum?
A. Use interleaving to align rows
B. Use Query Optimizer to find the most cost-effective way to
execute a query
C. Reduce the number of Spanner nodes to save money
D. Use Autoscaler to automatically scale nodes based on
demand
84. You have been tasked with configuring a daily CSV
extract from Cloud SQL for MySQL. How will you
accomplish this while adhering to Google Cloud's best
practices?
A. Call the Cloud SQL export API from Cloud Composer.
B. Use Cloud Composer to run a select command and then
export the results in CSV format.
C. Use Cloud Scheduler to call the export API by triggering
the cloud function via a PubSub topic.

D. Activate the cloud function that calls the export API using
Cloud Scheduler.
85. You are creating an app that can handle millions of
requests per second with sub-second latency and store
terabytes of historical data? Which service should you go
with?
A. Make use of Cloud SQL with read replicas.
B. Combine a Spanner and an autoscaler.
C. Make use of Bigtable and add nodes as needed to meet
demand.
D. Make use of Memorystore and add nodes as needed to
meet demand.
86. On a Compute Engine instance, you have a SQL Server
database running. You have been tasked with recommending
a cost-effective and long-lasting backup storage solution for
this instance. What is the most suitable option?
A. Store the backups on Cloud Storage and set up lifecycle
rules to move the backups to low-cost storage classes as
they age.
B. Store the backups on SSD Persistent disks.
C. Keep backups on standard persistent disks.

D. Keep backups on Filestore.
E. Keep backups on local SSDs.
87. You intend to move your current SQL Server workloads
to Cloud SQL for SQL Server. You discover that the current
instance uses 30000 read and write IOPS during the
migration assessment. You've been tasked with determining
the best instance size for SQL Server on Cloud SQL. What
would you suggest to increase throughput and IOPS?
A. Use a standard machine with a 4 TB hard drive.
B. Use a high-memory machine with a 4 TB hard drive.
C. Make use of a high-memory machine with a 4 TB SSD.
D. Make use of a high-memory machine with a 500 GB SSD.
88. You are developing a mission-critical application that
must withstand disruptions caused by regional failures. The
application will store sensitive data and requires encryption
to be configured using the AES-256 encryption algorithm
with complete control over the encryption key storage. How
can you achieve the same result?
A. Use Cloud SQL with a customer-managed encryption key.
B. Use Cloud Spanner with the default encryption key 

C. 
Use 
Cloud 
Spanner 
with 
the 
customer-managed
encryption key
D. Take into account using Cloud SQL with the default
encryption key.
89. You are in charge of a massive MySQL database hosted
on Cloud SQL. Your colleague started an on-demand backup
during the previous shift. You have been assigned the task of
monitoring and reporting on the status of the on-demand
backup. How can you achieve the same result?
A. Examine the instance log
B. Launch Cloudshell and execute the gcloud sql operations
list command
C. Examine the audit logs
D. Check the status of the cloud monitoring dashboard
90. You have deployed a Cloud SQL instance using Cloud
SQL Proxy Auth. Your application is deployed on a Compute
Engine VM and needs to access Cloud SQL instance per
recommended 
identity 
and 
access 
management 
best
practices. How can you accomplish the same?
A. Create a separate MySQL user for the application.
B. Create a shared MySQL user for the application.

C. Use the default service account to access Cloud SQL.
D. Create a separate service account to use with the
compute engine VM with an application hosted on it.
91. You are working on containerizing your MySQL database
workloads on Google Cloud. Which service allows you to
deploy MySQL and configure high availability in a container-
based environment?
A. Use Cloud Run for deploying the mysql docker container.
B. Use container registry to deploy the mysql docker
container.
C. Use Cloud SQL with a mysql docker container to run the
workloads and configure for high availability.
D. Use the Google Kubernetes engine along with stateful
sets for configuring high availability.
92. You are troubleshooting a connection issue for a newly
deployed Cloud SQL instance and have discovered an Error
403: 
Access 
Not 
Configured 
error 
message 
while
investigating the Cloud SQL Auth proxy logs. What steps
should you take to resolve the issue?
A. Verify that the connection string contains the correct
instance name.

B. Verify that the service account has the appropriate
permissions.
C. Verify that the Cloud SQL Admin API is enabled.
D. Verify that Public IP for Cloud SQL is enabled.
93. Your team is in charge of managing mysql on Cloud SQL.
The database is mission-critical and must be accessible 24
hours a day, seven days a week. You must perform a
database backup on Cloud Storage while ensuring that the
backup can be performed with minimal operational overhead
or impact on database workloads. How can you achieve the
same result?
A. Make use of Cloud SQL serverless export.
B. Create a read replica and then take a backup from it.
C. Clone the Cloud SQL instance and backup using the
clone.
D. Perform a direct backup on the primary instance.
94. Users have expressed dissatisfaction with the slow
response 
time 
of 
a 
multi-region 
Cloud 
Spanner
configuration. Following your investigation, you discovered
that the CPU utilization on the Spanner instance is high. You
want to use best practices to solve the problem. What should
your first step be without making any application changes?

A. Increase the number of nodes or processing units.
B. Reduce the number of nodes or processing units.
C. Modify the schema and add new indexes.
D. Distribute data among multiple Spanner instances.
95. You are running a mission-critical Cloud Spanner
instance, and the most recent release has corrupted data in
the production environment. How can the environment be
recovered and restored? (Choose Two)
A. If there is significant corruption, restore from a previous
timestamp before the corruption.
B. Perform a stale read from a previous timestamp before
corruption and write the results back to the database if
there is significant corruption.
C. Import the database from a previous point in time if there
is significant corruption.
D. Perform a restore from a previous timestamp before
corruption for minor corruption.
E. If the corruption is minor, perform a stale read from a
previous timestamp before the corruption and write the
results back to the database.

96. You have been assigned the task of creating lower
environments for your Cloud Spanner instance. This will
assist the development and quality assurance teams in
successfully testing their application with production data.
How can you achieve the same result?
A. Run a dataflow job to export the database to avro format
and then import the database from avro format.
B. Run a dataflow job to export the database to csv format
and then import the database from csv format.
C. Use the cloud function to export a csv file and the data
flow to import the csv file into the database.
D. Use the cloud function to run an avro format export and
the data flow to import the database from avro format.
97. You have several Spanner databases on Google Cloud.
You work for a company that must adhere to strict
regulatory compliance, and as part of that, you must audit
and report on all changes and updates made to the
Production database. How can you achieve the same result?
A. Perform production rollouts using Replication. For
auditing, use cloud logging.
B. Perform production rollouts using backup and restore.
For auditing, use cloud logging.

C. To perform production rollouts, use import-export. For
auditing, use cloud logging.
D. Perform production rollouts with Liquibase.
98. You want to expand your Cloud SQL for MySQL
instances. You need to increase the number of CPUs to meet
demand and want to minimize downtime and effort while
performing the operation. How can you do the same?
A. To increase the instance size, use the gcloud sql instances
patch command.
B. Change the MySQL configuration flag to increase the
number of CPUs.
C. To increase the instance size, use the gcloud compute
instance update command.
D. Create a new instance with a larger instance size and
backup and restore the existing one.
99. Your Cloud SQL for SQL Server instance has slow
response times. After some investigation, the physical read
and write times appear to be longer than expected. While
checking the IOPS and disk configuration, it was discovered
that the instance is currently using a 500 GB HDD. What
steps are you planning to take to improve database
performance?

A. Change the Cloud SQL instance Storage to SSD storage.
B. Set up a read replica and direct all read traffic to it. 
C. Place a MemoryStore cache in front of Cloud SQL to
improve performance.
D. Export the data and then import it into a new instance
with SSD drives as storage.
100. Data corruption has occurred in a Cloud SQL instance
for MySQL that is configured with primary and standby
replicas. The database must be restored to the last known
good copy of the data. What should you do next?
A. Make a clone of a read replica.
B. Make a clone of the backup replica.
C. Copy last week's backup file using the import feature.
D. Use point-in-time recovery to restore before the time
corruption occurred.
101. You have a custom dashboard that reports on Cloud
Spanner and Cloud SQL database statistics. You want to use
Google's security best practices to configure proper
authentication and authorization mechanisms. What is the
most suitable option?

A. Make a user account with the necessary permissions and
use it to authenticate the custom dashboard app.
B. Authenticate the custom dashboard app using the default
service account.
C. Create a new service account and grant it the Cloud SQL
and Spanner Reader roles.
D. Create a new service account and assign it the
Administrator role on Cloud SQL and Spanner.
102. You intend to move your database to Cloud SQL. The
monitoring team expressed concern that the current custom
tooling used for monitoring is incompatible with Cloud SQL,
and they require a successful mechanism to read the
database error log and send an alert if certain error code
messages are found in the log. How will you accomplish
this?
A. Capture the error message using Cloud Monitoring and
send alerts via notification channels.
B. Capture the error message with Error Reporting and send
alerts via notification channels.
C. Configure a log-based alert using Cloud Logging and
appropriate filters. 
D. Use Cloud Function to scan the logs and send alerts using
a custom service.

103. You intend to set up a High-Performance SQL Server
instance on Google Cloud. You're thinking about using
Google Compute Engine for the VM-based setup on a
Windows-based system. What considerations should be made
regarding persistent disks when setting up a system?
(Choose two)
A. Make use of a local SSD for TempDB.
B. For database files, use an SSD persistent disk.
C. For database files, use a standard persistent disk.
D. Store database files in the cloud.
104. You are setting up a new application on App Engine
that will connect with Cloud SQL for MySQL instances in the
same project. What role is required for the App Engine to
connect to the MySQL database? Make sure to follow the
principle of least privilege while setting up access.
A. roles/cloudsql.client
B. roles/cloudsql.viewer
C. roles/editor
D. roles/cloudsql.editor

105. You are in charge of a massive MySQL database hosted
on Cloud SQL. Your colleague started an on-demand backup
during the previous shift. You've been assigned the task of
monitoring and reporting on high CPU utilization during
operational hours. How can you do the same thing more
efficiently?
A. Set up a log-based alert to send notifications when the
CPU reaches 80%.
B. Create a cloud function that monitors CPU utilization and
sends an alert via a custom service when it exceeds the 80%
threshold.
C. Set up a Cloud Monitoring alert to send notifications
when the CPU reaches 80%.
D. Use the Cloud monitoring dashboard to check the status
and notify you when it reaches 80%.
106. You have a Bigtable-powered application that is
expected to grow organically over the next month. You're
thinking about using auto-scaling with BigTable. What
factors influence the number of nodes in a BigTable cluster
with auto-scaling enabled? (Choose three)
A. Use of the CPU
B. Utilisation of storage
C. Max and Min the number of nodes

D. Memory usage 
E. Number of active connections
107. You have been tasked with creating a database solution
for a global travel booking platform that must store and
process a large amount of data from various sources, such as
customer bookings and airline schedules. For near-instant
booking confirmations, the platform must support high
availability, low latency, and real-time data processing.
Which of the following database solutions is best suited to
this scenario?
A. Cloud Spanner
B. Cloud Bigtable 
C. Cloud SQL for PostgreSQL
D. Cloud Firestore
108. You are a software developer working on a project that
involves integrating with Datastore, a GCP cloud-based
NoSQL database service. During development and testing,
you should simulate the Datastore service's behavior locally
to ensure smooth functionality and minimize reliance on the
actual cloud service. Which of the following tools is best for
simulating the database service in a local development
environment?

A. Datastore Emulator
B. SQLite
C. Apache Kafka
D. CloudSQL
109. You have a Very Large Database (VLDB) for MySQL
hosted on Cloud SQL. The 
database 
instance is 
a
development database, and you have been tasked with
lowering the instance's operational costs. What should you
do in this situation to reduce the cost of backups?
A. Set the automated backup retention period to 1 day(s).
B. Upgrade the backup storage tier to HDD.
C. Reduce the number of automated backups.
D. Move backups to a different region
110. Your company is migrating its MySQL database to
Cloud SQL with the goal of avoiding planned downtime
throughout December. Cost-effectiveness is also an issue.
Which course of action should you take?
A. Set up Cloud SQL maintenance settings to prevent any
scheduled maintenance activities in December 
B. Create a regional MySQL instance in Cloud SQL so that if
any downtime occurs, a standby instance can serve as the

primary instance during December 
C. Deploy MySQL read replicas across zones to ensure that if
any downtime occurs in December, the read replicas can
take over as the primary instance
D. Create a support ticket with Google Cloud to ensure that
no maintenance is performed on the MySQL instance in
December
111. You used Cloud SQL Insights to identify inefficient
queries as the primary DBA for a Cloud SQL for PostgreSQL
database that supports six enterprise applications in
production. What should you look for in order to identify the
application that is generating these inefficient queries while
adhering to Google's best practices?
A. Perform a controlled shutdown of each application before
restarting it.
B. Develop a specialized tool for analyzing the database's
query logs.
C. Create a custom tool that is specifically designed to
analyze application logs.
D. Use query tags to enable application-centric database
monitoring.

112. In a hybrid setup, you use Compute Engine on Google
Cloud in conjunction with your on-premises data center to
manage a group of MySQL databases. The goal is to create
replicas to scale read operations and distribute management
workload. What should you do next?
A. Set up replication with an external server 
B. Make use of the Data Migration Service
C. Use Cloud SQL for external MySQL replicas
D. Make use of the mysqldump and binary logs utilities
113. Which of the following actions should you take to
ensure a cost-effective backup and restore solution that
meets a 2-hour recovery time objective (RTO) and a 15-
minute recovery point objective (RPO) when migrating an
Oracle-based application to Google Cloud?
A. Move the Oracle databases to Google Cloud's Bare Metal
Solution for Oracle and back them up to tapes on-premises.
B. Migrate the Oracle databases to Google Cloud's Bare
Metal Solution for Oracle and use Actifio's Nearline Storage
class to store backup files on Cloud Storage.
C. Move the Oracle databases to Google Cloud's Bare Metal
Solution for Oracle and back them up to Cloud Storage using
the Standard Storage class.

D. Migrate the Oracle databases to Compute Engine and
store backups on tapes on-premises
114. Your company has PostgreSQL databases on-premises
and on Amazon Web Services (AWS), and you intend to
migrate multiple databases to Google Cloud SQL to cut costs
and downtime. The goal is to adhere to Google's best
practices and use native Google data migration tools while
closely monitoring migrations to ensure a successful cutover
strategy. What is the most suitable option?
A. Make use of Database Migration Service to move all
databases to Cloud SQL.
B. Use Database Migration Service for one-time migrations
and third-party or partner tools designed specifically for
change data capture (CDC) migrations.
C. Use data replication and CDC tools to make the migration
process easier.
D. Use a hybrid approach to data migration, utilizing both
Database Migration Service and partner tools.
115. You are developing a write-heavy application and have
discovered that write workloads perform well in a regional
Cloud Spanner instance but significantly slower in a multi-
regional instance. Your goal is to improve write workload

performance in the multi-regional instance. What course of
action should you take?
A. Move the majority of read and write workloads closer to
the default leader region.
B. Increase the tolerance for staleness to at least 15
seconds.
C. Increase the number of read-write replicas in the multi-
regional instance.
D. Maintain a total CPU utilization of less than 45% in each
region.
116. You have been tasked with performing a one-time data
migration from an active Cloud SQL for MySQL instance in
the us-central1 region to a new Cloud SQL for MySQL
instance 
in 
the 
us-east1 
region. 
To 
minimize 
the
performance impact on the currently running instance, you
should adhere to Google's recommended practices. What is
the most suitable option?
A. Create and run a Dataflow job that uses JdbcIO to copy
data from the source to the destination Cloud SQL instance.
B. Create two Datastream connection profiles and use them
to create a stream for replicating data from the source to the
destination Cloud SQL instances.

C. Using a temporary instance, generate a SQL dump file
from the source Cloud SQL instance and store it in Cloud
Storage. Then, in the new Cloud SQL instance, import the
SQL dump file.
D. Export the data from the source Cloud SQL instance to a
CSV file by running the SELECT...INTO OUTFILE SQL
statement. The CSV file should then be moved to a Cloud
Storage bucket and imported into the new Cloud SQL
instance.
117. Your application employs a microservices architecture
and is supported by a single large Cloud SQL instance. With
the current configuration, you are experiencing performance
issues as your application scales. Despite the fact that the
Cloud Monitoring dashboard shows normal CPU utilization,
you want to address and prevent these performance issues
while avoiding extensive refactoring. What course of action
should you take?
A. Change your database solution from Cloud SQL to Cloud
Spanner.
B. Increase the number of CPUs assigned to the existing
instance.
C. Increase the instance's storage capacity.
D. Switch to a smaller number of Cloud SQL instances.

118. You are looking into Cloud SQL for PostgreSQL as a
possible migration solution for your existing on-premises
PostgreSQL instances. Given the growing importance of
geography in relation to customer privacy around the world,
your solution must adhere to data residency requirements. It
should also include the ability to configure data storage
locations, control encryption key storage, and govern data
access. Which of the following courses of action should you
take?
A. Duplicate Cloud SQL databases across multiple regions.
B. Create a PostgreSQL database on Google Cloud using
Cloud SQL for data that does not meet data residency
requirements. Maintain on-premises storage for data that
must meet these specifications. Modify the applications so
that they can support both databases.
C. Restrict application data access to users in the same
region as the Google Cloud region hosting the PostgreSQL
database's Cloud SQL.
D. Use customer-managed encryption keys (CMEK), VPC
Service Controls, and Identity and Access Management
(IAM) policies.
119. To handle increased user traffic, your team recently
deployed an updated version of a popular application.
However, the production monitoring team notified you of a

persistent replication lag between the primary instance and
read replicas in your Cloud SQL for MySQL environment.
What should you do to address the replication lag?
A. Examine and optimize slow queries or enable parallel
replication flags
B. Stop all active queries and rebuild the read replicas
C. Upgrade the primary instance's disk capacity and
increase the number of virtual CPUs (vCPUs)
D. Modify the primary instance to allocate more memory
resources
120. Consider two 
tables in 
an 
ANSI-SQL-compliant
database with identical columns. How should you merge
these two tables while removing the duplicate rows from the
result?
A. Use nested WITH statements.
B. Use the JOIN operator
C. Use the UNION ALL operator
D. Use the UNION operator.
121. Consider that you want to deploy your app to a GKE
cluster. The app exposes an HTTP-based health check at
/healthz. Using this endpoint, you must determine whether
the traffic should be routed to the pod through a load

balancer. Which code should be included in the pod
configuration?
A.   livenessProbe:
httpGet:
      path: /healthz
      port: 80
B. readinessProbe:
httpGet:
      path: /healthz
      port: 80
C. loadbalancerhealthcheck:
httpGet:
      path: /healthz
      port: 80
D. healthcheck:
httpGet:
      path: /healthz
      port: 80
122. Consider that you are developing an image resizing API
hosted on GKE. Callers of the service exist within the same
cluster. You want your clients to be able to get the IP
address of the service. What should you do?

A. Define a GKE Service. Clients should use the name of the
record in Cloud DNS
B. Define a GKE Service. Clients can use the service name in
the URL to connect.
C. Define a GKE Endpoint. Clients can obtain the endpoint
name from a client container.
D. Define a GKE Endpoint. Clients must get the endpoint
name from Cloud DNS.
123. Consider that you are creating a web app that runs on a
compute engine instance and writes a file to any user's
Google Drive. What should you do if you want to configure
the app to authenticate to the Drive API?
A. 
OAuth 
Client 
ID 
that 
uses
https://www.googleapis.com/auth/drive.file 
scope 
to
obtain an access token.
B. OAuth Client ID with domain-wide authority.
C. 
App 
Engine 
service 
account 
and
https://www.googleapis.com/auth/drive.file scope.
D. App engine service account with delegated domain-wide
authority.
 
124. You are creating a GKE cluster and running this
command:

1. gcloud container clusters create large-cluster --num-
nodes 200
2. The command fails with an error:
3. Insufficient regional quota to satisfy request: resource
"CPUs": request requires '200.0' and is short '176.0'. The
project has a quota of '24.0' with '24.0' available
4. What should be done to resolve this issue?
A. Request for GKE quota in the console.
B. Request for GCE quota in the console.
C. Open a support case to request for GKE quota.
D. Rewrite new clusters with fewer cores.
125. Consider that your analytics system executes queries
against a BigQuery dataset. The SQL query is executed in
batch and passes the contents of the SQL file to the
BigQuery CLI. It then redirects the BigQuery CLI output to
another process. When the queries are executed, you get a
permission error. What should you do to resolve this issue?
A. Grant data viewer and job user roles.
B. Grant data editor and data viewer roles.
C. Create a new dataset and copy the source table into this
new dataset.
D. None of the above.
126. What is Cloud Spanner in Google Cloud Platform?

A. Relational database
B. NoSQL database
C. Globally distributed, horizontally scalable database
D. In-memory database
127. Which Google Cloud service is designed for large-scale
data warehousing and analytics?
A. Cloud Firestore
B. BigQuery
C. Cloud SQL
D. Cloud Spanner
128. What is Cloud SQL in Google Cloud Platform?
A. NoSQL database
B. In-memory database
C. Managed relational database service
D. Time-series database
129. Which Google Cloud service is optimal for real-time
applications, 
offering 
a 
low-latency, 
high-throughput
architecture?
A. Azure Cosmos DB

B. Cloud Firestore
C. Cloud Spanner
D. Cloud SQL 
130. What is the purpose of Cloud Firestore in Google Cloud
Platform?
A. Document-oriented database for web applications
B. Relational database for structured data
C. Time-series database for IoT applications
D. In-memory database for caching 
131. In Cloud SQL, which database engine is fully managed
by Google Cloud Platform?
A. MySQL
B. PostgreSQL
C. SQL Server
D. All of the above
132. What is the primary use case for Cloud Bigtable?
A. Online Transaction Processing (OLTP)
B. Real-time analytics and streaming data processing
C. Document storage and retrieval

D. Relational database queries
133. Which Google Cloud service is suitable for storing and
querying geospatial data?
A. Cloud Bigtable
B. Cloud SQL
C. Cloud Firestore
D. BigQuery
134. What is the purpose of Cloud Memorystore?
A. In-memory database for caching
B. NoSQL database for document storage
C. Time-series database for analytics
D. Relational database for structured data
135. What is Cloud Datastore in Google Cloud Platform?
A. Relational database
B. NoSQL database
C. Time-series database
D. In-memory database

136. Which database service is specifically engineered for the
storage and querying of time-series data, a common
requirement in IoT applications?
A. Cloud Bigtable
B. Cloud Spanner
C. AWS Timestream 
D. Cloud BigQuery
137. What is the primary advantage of using Cloud Storage
for storing large amounts of unstructured data?
A. Low-latency querying
B. Cost-effective storage
C. Real-time analytics
D. ACID-compliant transactions
138. What is Cloud Pub/Sub used for in Google Cloud
Platform?
A. Database backup and recovery
B. Real-time messaging and event-driven systems
C. Geospatial data storage
D. In-memory caching 

139. What is the primary advantage of using Database
Migration Service (DMS) for mid-sized to large-sized
database migrations?
A. Fine-grained control of the migration process
B. Support for very large databases
C. Minimizing downtime during migration
D. Native MySQL replication
140. In external source replication, what is the purpose of
replicating from an external source (ES) using native binary-
logged-based replication?
A. Fine-grained control of migration
B. Minimizing replication lag
C. Importing baseline snapshot
D. Cloud Storage Import
141. Which storage engine is recommended for user tables
migrating to Cloud SQL in MySQL to support features like
replication and point-in-time recovery?
A. MyISAM
B. InnoDB
C. ARCHIVE

D. MEMORY
142. Why is it advisable to convert all MyISAM user tables to
InnoDB before performing a migration to Cloud SQL?
A. MyISAM is not supported in Cloud SQL
B. InnoDB provides better compression
C. MyISAM tables can lead to stability and reliability issues
D. MyISAM lacks Support for transactions
143. What privilege is not allowed for user accounts in
MySQL for Cloud SQL?
A. SUPER
B. FILE
C. SHUTDOWN
D. ALL PRIVILEGES
144. How can user privileges affect migrations in Cloud SQL,
especially for objects with a DEFINER attribute?
A. They do not affect migrations
B. Only affect stored procedures.
C. Can lead to issues if not supported in Cloud SQL
D. Only affect user-defined functions.

145. What is used in Cloud SQL for MySQL to modify
database parameters instead of calling the SET GLOBAL
command?
A. Custom stored procedures
B. MySQL events
C. User-defined functions
D. Flags
146. What is the primary focus of the content in the "Design
scalable and highly available cloud database solutions"
section of the exam?
A. Database security
B. Data modeling
C. Scalability and high availability
D. Query optimization
147. In the context of cloud database solutions, what does
scalability refer to?
A. Data encryption
B. Performance optimization
C. Capacity to handle increased load

D. Data modeling techniques
148. Why is high availability important in cloud database
solutions?
A. To reduce costs
B. To improve query performance
C. To ensure continuous access to data
D. To simplify data modeling
149. What is a key consideration when designing a scalable
database solution for the cloud?
A. Single point of failure
B. Query complexity
C. Data redundancy
D. Encryption algorithms
150. Which cloud database feature is essential for handling
increased workload by adding or removing resources
dynamically?
A. Sharding
B. Replication
C. Indexing
D. Partitioning

151. What does "high availability" typically refer to in the
context of databases?
A. Fast query execution
B. Minimal data redundancy
C. Continuous and reliable access to data
D. Advanced data encryption
152. How does replication contribute to high availability in a
cloud database solution?
A. Improves query performance
B. Ensures data consistency
C. Facilitates data distribution
D. Provides data redundancy and fault tolerance
153. In the context of cloud database solutions, what does
the term "read scalability" refer to?
A. The ability to handle increased write operations
B. The ability to efficiently execute complex queries
C. The ability to distribute read operations across multiple
nodes
D. The ability to ensure high availability during write
operations

154. Why is avoiding a single point of failure important in
designing highly available cloud database solutions?
A. To simplify data management
B. To reduce data redundancy
C. To ensure fault tolerance
D. To optimize query execution
155. What concept involves breaking down a large database
into 
smaller, 
more 
manageable 
pieces 
for 
improved
performance and scalability?
A. Data encryption
B. Database normalization
C. Data partitioning
D. Database indexing
156. In the context of cloud databases, what does the term
"replication lag" refer to?
A. Time delay between query execution and result retrieval
B. Delay in data consistency across replicated instances
C. Performance degradation due to redundant data
D. Inefficient use of indexing

157. What is the role of sharding in the context of scalable
database solutions?
A. Enhancing data encryption
B. Improving query complexity
C. Distributing data across multiple servers
D. Optimizing storage efficiency
158. How does partitioning contribute to query optimization
in a scalable database solution?
A. Reducing data redundancy
B. Enhancing fault tolerance
C. Improving data distribution
D. Focusing queries on specific data subsets
159. What is a common challenge when implementing
sharding in a cloud database solution?
A. Increased data redundancy
B. Difficulty in data distribution
C. Managing a single point of failure
D. Ensuring consistency across shards

160. Why is high availability critical for cloud database
solutions, especially in mission-critical applications?
A. To reduce costs
B. To minimize data redundancy
C. To ensure continuous service availability
D. To simplify data modeling techniques
161. What is the primary goal of database integration?
A. Data encryption
B. Creating data redundancy
C. Managing multiple databases separately
D. Creating a single authoritative version for improved
information sharing
162. Why is integration important for modern businesses
collecting large volumes of data?
A. To increase data redundancy
B. To simplify data management
C. To get overwhelmed by data
D. To use information efficiently and make better decisions

163. What is the downside of consolidating multiple
databases into a single source?
A. Improved data consistency
B. Reduced service interruptions
C. Risk of data loss, corruption, or service interruption
D. Enhanced performance
164. In the context of integrating databases, what is "read
scalability"?
A. The ability to handle increased write operations
B. The ability to distribute read operations across multiple
nodes
C. The ability to create data redundancy
D. The ability to store data in different formats
165. What is a key challenge during integration related to
maintaining consistency and integrity?
A. Optimizing query performance
B. Transforming existing data to meet the new schema
C. Handling duplicate values and discrepancies
D. Defining relationships between entities

166. What is the alternative to consolidating databases,
allowing seamless data integration without migrating data?
A. Data redundancy
B. Querying multiple databases for one application
C. Data encryption
D. Creating a data warehouse
167. What is the primary challenge when integrating
databases with different schemas?
A. Managing relationships
B. Assessing new database requirements
C. Maintaining data consistency
D. Dealing with duplication and discrepancies
168. Why might integrating databases be unavoidable for
certain applications?
A. To increase data redundancy
B. To simplify data management processes
C. To meet the functional requirements of the application
D. To create multiple authoritative versions

169. What is the key consideration when consolidating
databases with a single schema?
A. Creating data redundancy
B. Assessing new database requirements
C. Transforming existing data to meet the new schema
D. Retaining consistency across similar attributes stored in
multiple databases
170. What is the role of integration in preventing duplication
and inconsistency issues?
A. Creating more databases
B. Consolidating databases into a single source
C. Handling data loss and corruption
D. Managing relationships between entities
171. What is data migration?
A. A process of combining data from multiple source systems
B. A process of transferring data between data storage
systems or formats
C. A process of changing data from one format to another
D. A process of creating a unified set of data for operational
and analytical uses

172. When might an organization undertake a data
migration project?
A. When consolidating websites
B. When installing software upgrades
C. When moving data during a company merger
D. All of the above
173. What is the primary goal of data migration?
A. Creating data redundancy
B. Increasing data encryption
C. Boosting productivity and reducing storage costs
D. Transforming data formats
174. What is a key challenge during data migration related
to maintaining data integrity?
A. Assessing new database requirements
B. Extracting and transforming data
C. Ensuring sustainable governance
D. Backing up data before migration

175. What is the difference between Big Bang and trickle
migrations?
A. Big bang migrations involve transferring data in phases,
while trickle migrations transfer all data within a set time
window.
B. Big bang migrations are more complex and need more
planning, while trickle migrations require the system to be
offline for the entire migration.
C. Big bang migrations have less Risk of losing data, while
trickle migrations involve both old and new systems running
simultaneously.
D. Big bang migrations are less costly, while trickle
migrations are quicker and less complex.
 

ANSWERS
1. Answer: B
Explanation:
Google Compute Engine (GCE) VMs: Use GCE
VMs to host your SQL Server database instances. By
deploying VMs in multiple zones, you ensure
redundancy and fault tolerance.
Native Database Replication: Configure native
SQL Server database replication in synchronous
mode. This replication setup ensures that data
changes made on the primary database are
immediately 
replicated 
to 
the 
secondary
database(s). Synchronous replication provides high
data consistency but may introduce some latency.
For further results, you can visit the given URL.
https://cloud.google.com/compute/sla
2. Answer: D
Explanation: Cloud Spanner provides an autoscaling
feature called Spanner Autoscaler, which automatically
adjusts the number of nodes based on workload demands.
This allows you to scale up or down dynamically to handle
peak business hours efficiently while maintaining optimal
performance during off-peak hours. Spanner Autoscaler
ensures that you have the necessary resources available
when needed, reducing the chances of slowness or
performance issues.
For further results, you can visit the given URL.

https://cloud.google.com/spanner/docs/autoscaling-
overview#configuration
3. Answer: A
Explanation: 
Bigtable 
Replication: 
Enable 
Bigtable
replication to replicate data across multiple clusters.
Replication ensures that data is available in multiple
locations, providing redundancy and fault tolerance. In the
event of a failure in the primary cluster, the data can be
automatically accessed from the replicated cluster(s).
Multi-cluster Routing App Profile: Configure a multi-cluster
routing app profile to direct requests to the appropriate
cluster based on availability and proximity. The app profile
will automatically handle failover by redirecting requests to
the replicated cluster in case of a failure in the primary
cluster. This ensures that the application can continue to
access the data without interruption.
For further results, you can visit the given URL.
https://cloud.google.com/bigtable/docs/replication-
overview4. Answer: D
Explanation: Google Cloud Memorystore provides a fully
managed in-memory database service. It supports Redis, an
in-memory data structure store that can deliver extremely
fast data access times in the sub-millisecond range. By
redesigning your application to use Memorystore, you can

take 
advantage 
of 
the 
in-memory 
capabilities 
and
significantly improve the performance of your application.
For further results, you can visit the given URL.
https://cloud.google.com/appengine/docs/legacy/standard/g
o111/using-memorystore
5. Answer: A
Explanation: Firestore is a serverless, NoSQL database
solution for storing, syncing, and querying data for your
mobile and web apps on a global scale. This meets all of the
question's requirements.
For further results, you can visit the given URL.
https://cloud.google.com/datastore/docs/firestore-or-
datastore
6. Answer: C
Explanation: Google Cloud SQL provides a feature called
"Automatic storage increases" that allows you to automate
the process of increasing storage space for your Cloud SQL
instances. When enabled, Cloud SQL will automatically add
additional storage capacity as needed when the storage
space runs out, based on predefined thresholds.
For further results, you can visit the given URL.
https://cloud.google.com/sql/docs/mysql/instance-
settings#:~:text=To%20set%20the%20limit%20when,beta%
20sql%20instances%20patch%20command.

7. Answer: B
Explanation: Google Cloud SQL offers a feature called
"Query Insights" that provides detailed performance metrics
and query analysis for your Cloud SQL for MySQL instances.
Enabling Query Insights allows you to gather information on
query performance, including execution time, latency, and
resource utilization.
For further results, you can visit the given URL.
https://cloud.google.com/sql/docs/postgres/using-query-
insights#:~:text=Click%20Save.-,To%20enable%20Query%2
0insights%20for%20a%20Cloud%20SQL%20instance%20by,
the%20ID%20of%20the%20instance.
8. Answer: A
Explanation: Shared VPC: Shared VPC (Virtual Private
Cloud) allows you to create a virtual network (VPC) that can
be shared across multiple GCP projects. By setting up a
Shared VPC, you can centrally manage network resources,
including subnets and firewall rules, and ensure consistent
network connectivity between different GCP projects.
Service Projects: In the Shared VPC model, you can have
multiple service projects that contain your application
workloads running on Google Compute Engine. These
service projects are connected to the Shared VPC network
and can communicate securely with each other.

Firewall Rules: Within the Shared VPC, you can define
firewall rules to allow secure access from the application
workloads to the Oracle databases hosted on GCP's Bare
Metal Solution. These firewall rules can restrict access
based on source IP ranges, ports, and protocols, ensuring
that only authorized traffic can reach the Oracle databases.
For further results, you can visit the given URL.
https://cloud.google.com/bare-metal/docs/bms-
planning#bms-networking
9. Answer: C and E
Explanation: Primary Key Constraint: It is essential to
ensure that every table in the source PostgreSQL database
has a primary key constraint defined. Cloud SQL requires
tables to have primary keys for replication purposes. If a
table does not have a primary key, it may not be supported
for continuous replication using Database Migration Service.
pglogical Extension: The pglogical extension needs to be
added to the source database to enable continuous
replication from the source PostgreSQL database to Cloud
SQL. pglogical is a PostgreSQL extension that provides
logical replication capabilities required for continuous
replication to Cloud SQL.
For further results, you can visit the given URL.
https://cloud.google.com/database-
migration/docs/postgresql-to-alloydb/configure-source-

database
10. Answer: D
Explanation:
External Replica: By setting up an external replica, you can
replicate data from the on-premises MySQL database to a
Cloud SQL instance. This ensures that data remains
synchronized between the on-premises database and the
Cloud SQL database during the migration process.
Cloud SQL as the Target: Use Cloud SQL as the target for
the external replica. Cloud SQL is a fully managed database
service 
provided 
by 
Google 
Cloud, 
which 
reduces
operational overhead by handling database management
tasks such as backups, patches, and scaling.
Plan a Maintenance Window: Schedule a maintenance
window to minimize downtime during the cutover process.
During this window, perform the final synchronization of
data from the on-premises database to the Cloud SQL
instance.
Switch to Cloud SQL: Once the data is synchronized and
the maintenance window is in effect, switch the application's
connection string to use the Cloud SQL endpoint. This
ensures that the application starts using the Cloud SQL
database instead of the on-premises database.
For further results, you can visit the given URL.

https://cloud.google.com/sql/docs/mysql/replication/configu
re-replication-from-external
11. Answer: B, C, and D
Explanation: The source SQL Server System makes use of
BI Components such as SSRS and SSIS.
This is a relevant factor because Cloud SQL, being a
managed service, may have limitations on the availability
and support for certain BI components like SQL Server
Reporting Services (SSRS) and SQL Server Integration
Services (SSIS). If your source SQL Server system heavily
relies on these BI components, running SQL Server on
Google Compute Engine would give you more control and
flexibility to utilize them.
The origin SQL Server System connects to other data
sources via Linked Servers.
The ability to connect to other data sources via Linked
Servers is an important consideration. Cloud SQL has
limitations on external connectivity and may not provide the
same level of flexibility and control when setting up and
managing Linked Servers. If your source SQL Server system
extensively uses Linked Servers, choosing SQL Server on
Google Compute Engine would offer more flexibility in
establishing and managing these connections.
The origin Transparent Data Encryption (TDE) is enabled on
the SQL Server System.

Transparent Data Encryption (TDE) is a significant factor to
consider. Cloud SQL offers built-in encryption at rest, but if
your source SQL Server system has TDE specifically
enabled, you may need to choose SQL Server on Google
Compute Engine. With Compute Engine, you have more
control over encryption features and can configure TDE as
per your requirements.
For further results, you can visit the given URL.
https://cloud.google.com/blog/topics/data-
warehousing/google-bigquery-and-sql-server-ssis-and-ssrs
12. Answer: B
Explanation: Deploying a Cloud SQL instance in zone A of
the Mumbai region ensures the primary database is located
in the desired region.
Creating a multiple-zone replica in zone B of the Delhi
region provides fault tolerance and high availability across
different zones within the backup region of Delhi. In the
event of a region-wide outage in Mumbai, the replica in
Delhi can take over the workload.
Creating a read replica in any one of the zones in the Delhi
region allows for scaling read operations and spreading the
load across multiple replicas within the backup region.
For further results, you can visit the given URL.
https://cloud.google.com/sql/docs/mysql/locations

13. Answer: B
Explanation: PgBouncer is a lightweight connection pooling
tool for PostgreSQL that sits between the application and
the database. It allows multiple application connections to
be handled by a smaller number of actual database
connections.
By configuring PgBouncer to connect to both the primary
and read replica of the Cloud SQL database, you can
distribute the application load across the zones.
PgBouncer will manage the pooling of connections and
direct queries to either the primary or read replica based on
the load distribution configuration.
This approach provides a scalable and efficient way to
distribute the application load and utilize both the primary
and read replica for handling the workload.
For further results, you can visit the given URL.
https://8grams.medium.com/how-to-setup-postgresql-
connection-pooling-using-pgbouncer-in-gke-google-
kubernetes-engine-and-9c3abedb94b
14. Answer: D and E
Explanation: Enable point-in-time recovery for Cloud SQL.
Enabling point-in-time recovery allows you to restore your
Cloud SQL instance to a specific point in time in case of data

loss or corruption. This ensures high availability and
minimizes data loss in the event of a failure.
Schedule automated backups for Cloud SQL.
Scheduling automated backups ensures that regular backups
of your Cloud SQL instance are taken automatically. This
allows you to restore the database to a recent state if
necessary and helps maintain high availability.
For further results, you can visit the given URL.
https://cloud.google.com/sql/docs/mysql/backup-
recovery/pitr#:~:text=Enable%20point-in-
time%20recovery,-
When%20you%20create&text=In%20the%20Google%20Clo
ud%20console%2C%20go,the%20Cloud%20SQL%20Instanc
es%20page.&text=for%20the%20instance%20you%20want,-
in-time%20recovery%20checkbox
15. Answer: B
Explanation:
To run Oracle on the Google Cloud Platform, the customer
should use Google Cloud SQL. Cloud SQL provides a
managed service for running various database engines,
including 
Oracle. 
It 
takes 
care 
of 
the 
underlying
infrastructure and offers high availability, scalability, and
security features specific to Oracle databases.
For further results, you can visit the given URL.

https://cloud.google.com/sql/docs
16. Answer: C
Explanation:
To achieve automatic scaling of your database instances on
the Google Cloud Platform, you can use Google Cloud SQL.
Cloud SQL offers automated scaling capabilities that adjust
the resources allocated to your database based on demand.
It allows you to scale up or down without manual
intervention, 
ensuring 
optimal 
performance 
and 
cost
efficiency.
For further results, you can visit the given URL.
https://cloud.google.com/sql/docs
17. Answer: B
Explanation:
To migrate an on-premises MySQL database to the Google
Cloud Platform, you can use Google Cloud SQL. Cloud SQL
provides migration tools and services that simplify the
process of migrating your MySQL database to the cloud. It
supports both online and offline migration methods, making
it easier to transition your database to the Google Cloud.
For further results, you can visit the given URL.
https://cloud.google.com/database-
migration/docs/postgres/quickstart
18. Answer: A

Explanation:
Cloud Firestore is a fully managed NoSQL document
database 
provided 
by 
Google 
Cloud. 
It 
offers 
high
availability, scalability, and automatic scaling, making it
suitable for web, mobile, and server applications.
For further results, you can visit the given URL.
https://cloud.google.com/firestore?hl=en
19. Answer: A
Explanation:
BigQuery is a fully managed, serverless data warehouse
service in Google Cloud. It is designed for running complex
analytical queries on large datasets and offers high
scalability and performance.
For further results, you can visit the given URL.
https://cloud.google.com/bigquery/docs/introduction
20. Answer: A
Explanation:
Cloud SQL is a fully managed relational database service
provided by Google Cloud. It supports various database
engines, including PostgreSQL, making it a suitable choice
for a compatible managed PostgreSQL database.
For further results, you can visit the given URL.
https://cloud.google.com/sql?hl=en

21. Answer: C
Explanation:
Memorystore for Redis is a fully managed in-memory data
store service provided by Google Cloud. It can be used to
cache frequently accessed data, improving application
performance.
For further results, you can visit the given URL.
https://cloud.google.com/memorystore/docs/redis
22. Answer: A
Explanation:
Cloud Spanner is a globally distributed, horizontally scalable
relational database service provided by Google Cloud. It
offers strong consistency and high scalability, making it
suitable for globally distributed applications.
For further results, you can visit the given URL.
https://cloud.google.com/spanner?hl=en
23. Answer: D
Explanation:
Cloud SQL is a fully managed MySQL database service
provided by Google Cloud. It offers high availability,
scalability, and automated backups, making it a convenient
choice for MySQL-based applications.
For further results, you can visit the given URL.

24. Answer: A
Explanation:
Cloud Dataflow is a fully managed service provided by
Google Cloud for analyzing streaming data in real-time. It
offers powerful data processing capabilities and supports
data transformations, aggregations, and analysis.
For further results, you can visit the given URL.
https://cloud.google.com/dataflow/docs
25. Answer: D
Explanation:
Cloud Bigtable is a horizontally scalable, high-performance
NoSQL database service provided by Google Cloud. It is
suitable for storing and retrieving large amounts of key-
value data with low latency.
For further results, you can visit the given URL.
https://cloud.google.com/bigtable?hl=en
26. Answer: A
Explanation:
Cloud Storage is a scalable object storage service provided
by Google Cloud. It is suitable for storing and serving large
binary objects like images or videos. It offers durability, high

availability, and easy integration with other Google Cloud
services.
For further results, you can visit the given URL.
https://cloud.google.com/storage/docs/introduction
27. Answer: D
Explanation:
Cloud Monitoring is a Google Cloud service that provides a
highly scalable, fully managed time-series database for
storing, analyzing, and visualizing time-series data. It is
specifically designed for monitoring and observability use
cases.
For further results, you can visit the given URL.
https://cloud.google.com/products/operations?hl=en
28. Answer: A
Explanation:
Cloud Spanner is a globally distributed, highly available, and
strongly consistent relational database service provided by
Google Cloud. It combines the benefits of relational
databases with horizontal scalability and global distribution.
For further results, you can visit the given URL.
https://cloud.google.com/spanner?hl=en
29. Answer: D
Explanation:

Cloud SQL for Oracle is a Google Cloud service specifically
designed for migrating and hosting Oracle databases. It
provides a fully managed, compatible, and scalable solution
for running Oracle databases in the cloud.
For further results, you can visit the given URL.
https://cloud.google.com/solutions/migrate-oracle-
workloads
30. Answer: D
Explanation:
Cloud Composer is a fully managed workflow orchestration
service provided by Google Cloud. It can be used to manage
and orchestrate data pipelines for batch and streaming data
processing. It supports integration with other Google Cloud
services and third-party tools.
For further results, you can visit the given URL.
https://cloud.google.com/composer/docs
31. Answer: C
Explanation:
Cloud Bigtable is a globally distributed, horizontally scalable
NoSQL database service provided by Google Cloud. It offers
low latency read and write operations, making it suitable for
high-performance and globally distributed applications.
For further results, you can visit the given URL.

https://cloud.google.com/bigtable?hl=en
32. Answer: D
Explanation:
Cloud Data Fusion is a fully managed, serverless data
integration and transformation service provided by Google
Cloud. It simplifies the process of building data pipelines and
allows for the integration of various data sources and
targets.
For further results, you can visit the given URL.
https://cloud.google.com/data-fusion/docs
33. Answer: C
Explanation:
Memorystore for Redis is a fully managed, in-memory data
store service provided by Google Cloud. It is suitable for
caching 
frequently 
accessed 
data 
and 
offers 
high
performance and scalability.
For further results, you can visit the given URL.
https://cloud.google.com/memorystore?hl=en
34. Answer: A
Explanation:
Cloud Spanner is a globally distributed, horizontally scalable
SQL database service provided by Google Cloud. It offers

high availability, strong consistency, and global scalability
for SQL-based applications.
For further results, you can visit the given URL.
https://cloud.google.com/spanner?hl=en
35. Answer: C
Explanation:
Cloud Firestore is a fully managed, serverless NoSQL
document database service provided by Google Cloud. It is
suitable for storing structured data and automatically scales
with the needs of your application.
For further results, you can visit the given URL.
https://cloud.google.com/firestore?hl=en
36. Answer: A
Explanation:
BigQuery is a fully managed, serverless analytics platform
provided by Google Cloud. It is designed for analyzing large
datasets using SQL queries and offers high scalability,
performance, and ease of use.
For further results, you can visit the given URL.
https://cloud.google.com/bigquery/docs/introduction

37. Answer: A
Explanation:
The best approach would be to deploy a multi-region Cloud
Spanner database to achieve high availability and fault
tolerance for a global e-commerce platform with millions of
transactions. Cloud Spanner offers automatic sharding for
scalability and high availability across multiple regions,
ensuring that the database can handle the load and remain
accessible even during regional outages.
For further results, you can visit the given URL.
https://cloud.google.com/spanner/docs
38. Answer: A
Explanation:
To handle high write throughput and provide real-time
analytics on user-generated content, Cloud Bigtable would
be the most suitable choice. Cloud Bigtable is a high-
performance NoSQL database designed for large-scale data
ingestion and real-time analytics. It can handle high write
throughput and provides low-latency access to the data.
For further results, you can visit the given URL.
https://cloud.google.com/bigtable/docs/overview
39. Answer: C

Explanation:
For a global e-commerce platform requiring high read and
write throughput with strong consistency, Cloud Spanner
would be the best choice. Cloud Spanner is a globally
distributed relational database service that offers high
scalability, strong consistency, and low latency. It can
handle high read and write throughput while maintaining
strong consistency across regions.
For further results, you can visit the given URL.
https://cloud.google.com/spanner?hl=en
40. Answer: A
Explanation:
To support complex analytics queries on massive amounts of
data, BigQuery is the most suitable choice. BigQuery is a
fully managed, serverless analytics platform that can handle
petabytes of data and execute complex queries with high
performance. It is designed specifically for data warehousing
and analytics use cases.
For further results, you can visit the given URL.
https://cloud.google.com/bigquery/docs/introduction
41. Answer: B

Explanation:
For storing and retrieving JSON documents in a scalable and
fully managed manner, Cloud Firestore is the best choice.
Cloud Firestore is a NoSQL document database that
supports storing and querying JSON documents. It provides
automatic scaling, real-time synchronization, and flexible
querying capabilities.
For further results, you can visit the given URL.
https://cloud.google.com/firestore?hl=en
42. Answer: B
Explanation:
For real-time data streaming and processing, Cloud Dataflow
is the most appropriate choice. Cloud Dataflow is a fully
managed data processing service that allows you to build
and execute data pipelines for both batch and real-time
processing. It supports real-time data streaming and
provides scalable, reliable, and fault-tolerant processing
capabilities.
For further results, you can visit the given URL.
https://cloud.google.com/dataflow?hl=en
43. Answer: D

Explanation:
For a highly scalable and low-latency database solution for
storing user profiles and game data, Cloud Bigtable would
be the best choice. Cloud Bigtable is a fully managed NoSQL
database designed for high scalability and low-latency
access to large amounts of data. It can handle massive
workloads with low latency, making it suitable for gaming
platforms.
For further results, you can visit the given URL.
https://cloud.google.com/bigtable?hl=en
44. Answer: A
Explanation:
For 
real-time 
user 
profiling 
and 
personalized
recommendations, Cloud Firestore is the most suitable
choice. Cloud Firestore is a NoSQL document database that
provides real-time synchronization and flexible querying
capabilities. It can be used to store and retrieve user profiles
and deliver personalized recommendations based on real-
time data.
For further results, you can visit the given URL.
https://cloud.google.com/firestore?hl=en
45. Answer: A

Explanation:
To handle high read throughput and minimize latency for a
globally 
distributed 
content 
delivery 
platform, 
Cloud
Bigtable is the most suitable choice. Cloud Bigtable is a
NoSQL database that provides low-latency access to large
amounts of data. It can be globally distributed to minimize
latency and handle high read-throughput.
For further results, you can visit the given URL.
https://cloud.google.com/bigtable?hl=en
46. Answer: A
Explanation:
For low-latency access to large amounts of structured data,
Cloud Spanner is the most suitable choice. Cloud Spanner is
a globally distributed relational database service that offers
low-latency access to structured data. It provides high
scalability, strong consistency, and ACID transactions for
data-intensive applications.
For further results, you can visit the given URL.
https://cloud.google.com/spanner?hl=en
47. Answer: D
Explanation:

Cloud Spanner is designed to provide an SLA of 99.999%
uptime with no maintenance overhead. Cloud Spanner also
supports the PostgreSQL dialect, allowing developers to
make the necessary transition to Spanner with minimal code
changes.
For further results, you can visit the given URL.
https://cloud.google.com/spanner/docs/postgresql-interface
48. Answer: C
Explanation:
AlloyDB provides out-of-the-box support for regional high
availability on Google Cloud, so no additional configuration
or setup is required to achieve high availability.
AlloyDB is a managed PostgreSQL-compatible database
service offered by Google Cloud. It is designed to provide
high availability by default. When using AlloyDB, the service
automatically handles the underlying infrastructure and
configuration necessary to ensure regional high availability.
With regional high availability, AlloyDB automatically
replicates data across multiple zones within a single region.
This replication allows for continuous data availability and
automatic failover in the event of an issue with the primary
instance. The failover process is managed by AlloyDB,
ensuring 
minimal 
downtime 
and 
maintaining 
data
consistency.
For further results, you can visit the given URL.

https://cloud.google.com/blog/products/databases/announci
ng-the-general-availability-of-alloydb-for-postgresql
49. Answer: C
Explanation:
To troubleshoot performance issues on a Cloud SQL for
PostgreSQL instance, the best tool to use is the Cloud
Monitoring 
dashboard. 
Cloud 
Monitoring 
provides
comprehensive monitoring capabilities for Google Cloud
resources, including Cloud SQL instances. It allows you to
monitor and analyze various performance metrics and
diagnostics to identify and troubleshoot performance issues.
While options A, C, and D (Cloud logging, Cloud Profiler, and
Cloud Debugger) are useful tools in their respective areas,
they are not specifically designed for troubleshooting
performance issues on a Cloud SQL for PostgreSQL
instance. Cloud logging helps with collecting and analyzing
logs, 
Cloud 
Profiler 
assists 
in 
profiling 
application
performance, and Cloud Debugger enables debugging of
code. However, for troubleshooting performance issues in a
Cloud SQL instance, the Cloud Monitoring dashboard is the
recommended tool.
For further results, you can visit the given URL.
https://codelabs.developers.google.com/codelabs/cloud-
profiler#0

50. Answer: D
Explanation:
Since Cloud Spanner does not have built-in support for
automated backups, you can create a custom solution using
Cloud Functions and Cloud Scheduler.
By combining Cloud Functions and Cloud Scheduler, you can
create a custom backup solution for Cloud Spanner that
meets the client's requirement of backups running every 4
hours. This approach allows you to automate the backup
process and have control over the backup schedule.
For further results, you can visit the given URL.
https://firebase.google.com/docs/functions/schedule-
functions
51. Answer: A
Explanation:
The best approach is to create cross-region read replicas in
the respective regions and update the reporting platform to
connect to these read replicas to optimize read performance
for the reporting platform in Europe and Asia when using a
Cloud SQL for MySQL instance in the us-east-1 region.
Option A suggests creating two cross-region read replicas,
one in the Europe region and one in the Asia region. By
creating read replicas in the regions closer to the offices
experiencing slow response times, you can distribute the

read workload and reduce latency. The reporting platform
can then be updated to connect to these read replicas,
enabling faster access to the data for reporting purposes.
For further results, you can visit the given URL.
https://cloud.google.com/blog/products/databases/introduci
ng-cross-region-replica-for-cloud-sql
52. Answer: A, B, and C
Explanation:
The possible reasons for the DEADLINE_EXCEEDED errors
and timeouts in an application backed by the Google Cloud
Spanner database are:
A. Spanner CPU load breaching thresholds: High CPU load
on the Spanner database can lead to increased request
processing time, resulting in request failures and timeouts.
B. Using default timeout settings for all requests provided in
the client libraries: If the default timeout settings are used
for all requests without considering the specific needs of the
application, it may lead to timeouts and request failures,
especially for long-running or resource-intensive operations.
C. Bad database schema design: A poorly designed database
schema that does not efficiently distribute data across
shards or partitions can result in uneven data distribution
and increased latency, leading to request failures and
timeouts.

For further results, you can visit the given URL.
https://cloud.google.com/blog/products/databases/troublesh
ooting-deadline-exceeded-errors-on-cloud-spanner
53. Answer: C
Explanation:
When facing performance issues with Cloud Spanner,
checking the read statistics can provide valuable insights
into 
the 
database's 
performance. 
The
SPANNER_SYS.READ_STATS* tables contain information
about read latencies, such as latency distribution by
percentile, latency count, and latency histogram.
For further results, you can visit the given URL.
https://cloud.google.com/spanner/docs/introspection/read-
statistics
54. Answer: A
Explanation:
By querying the SPANNER_SYS.READ_STATS* tables, you
can retrieve statistics related to read latencies and analyze
the performance of the Cloud Spanner instance.
For further results, you can visit the given URL.
https://cloud.google.com/spanner/docs/introspection/read-
statistics

55. Answer: C
Explanation:
The COUNT(*) function returns the total number of records
in the specified table.
For further results, you can visit the given URL.
https://cloud.google.com/bigquery/docs/reference/legacy-
sql
56. Answer: A
Explanation:
The MAX() function returns the maximum value from the
specified column.
For further results, you can visit the given URL.
https://support.google.com/docs/answer/3094013?hl=en-GB
57. Answer: D
Explanation:
By using the SUM() function and grouping the results by the
"Product" column, you can calculate the total sales amount
for each product.
For further results, you can visit the given URL.

https://cloud.google.com/bigquery/docs/reference/standard-
sql/aggregate_functions
58. Answer: B
Explanation:
The LIMIT clause is used to restrict the number of rows
returned by a query, allowing you to retrieve the first 10
records.
For further results, you can visit the given URL.
https://cloud.google.com/spanner/docs/reference/standard-
sql/query-syntax
59. Answer: A
Explanation:
The AVG() function calculates the average value of a
specified column, in this case, the "Age" column.
For further results, you can visit the given URL.
https://cloud.google.com/bigquery/docs/reference/legacy-
sql
60. Answer: C
Explanation:

The WHERE clause is used to specify conditions for selecting
records from a table, and the AND operator is used to
combine multiple conditions.
For further results, you can visit the given URL.
https://cloud.google.com/bigquery/docs/reference/standard-
sql/query-syntax
61. Answer: C
Explanation:
The DISTINCT keyword is used to eliminate duplicate values
and retrieve only distinct values from the specified column.
For further results, you can visit the given URL.
https://cloud.google.com/bigquery/docs/reference/standard-
sql/query-syntax
62. Answer: A
Explanation:
The ORDER BY clause is used to sort the records in either
ascending (ASC) or descending (DESC) order and the LIMIT
clause is used to restrict the number of rows returned.
For further results, you can visit the given URL.
https://cloud.google.com/bigquery/docs/reference/standard-
sql/query-syntax

63. Answer: B
Explanation:
By multiplying the "Price" and "Quantity" columns and
applying the SUM() function, you can calculate the total
revenue generated from sales.
For further results, you can visit the given URL.
https://cloud.google.com/application-integration/docs/try-
sample-integration-ecommerce
64. Answer: A
Explanation:
It is recommended to use the Cloud Spanner Emulator to
provide a cost-efficient solution for the development team to
test their applications locally on their laptops without
incurring additional expenses. The Cloud Spanner Emulator
allows developers to simulate a Cloud Spanner environment
locally without the need for a Cloud Spanner development
instance. This way, developers can perform testing and
development activities without incurring any additional
costs.
For further results, you can visit the given URL.

https://cloud.google.com/blog/topics/developers-
practitioners/deployment-models-cloud-spanner-emulator
65. Answer: C
Explanation:
To ensure minimal downtime and friction during the
migration of Oracle databases to Cloud SQL, you can use
Datastream. Datastream is a fully managed serverless data
integration and migration service offered by Google Cloud. It
supports migrating Oracle databases to Cloud SQL for
PostgreSQL, providing a seamless and efficient migration
process. Datastream handles the data replication, schema
conversion, and synchronization between the source Oracle
database and the target Cloud SQL for PostgreSQL instance.
By utilizing Datastream, you can minimize downtime and
ensure a smooth transition of your legacy Oracle databases
to Cloud SQL.
For further results, you can visit the given URL.
https://github.com/GoogleCloudPlatform/community/blob/m
aster/archived/migrate-oracle-postgres-using-
datastream/index.md
66. Answer: B
Explanation:
To maintain synchronization and consistency between the
source MySQL database and the target Cloud Spanner
database during migration, you can utilize Change Data

Capture (CDC). CDC captures and streams the changes
(inserts, updates, deletes) made in the source database and
allows applications to subscribe to this stream and apply the
changes to the target database.
For further results, you can visit the given URL.
https://cloud.google.com/blog/products/databases/track-
and-integrate-change-data-with-spanner-change-streams
67. Answer: D
Explanation:
Google Cloud VMware Engine (GCVE) provides a dedicated
VMware environment on Google Cloud, allowing you to run
your existing VMware-based workloads without making
significant changes to the architecture. By migrating the
highly transactional Oracle database to GCVE, you can
leverage the benefits of Google Cloud while maintaining
compatibility with your existing VMware environment.
With GCVE, you can replicate your VMware environment in
the cloud, including your VMware vCenter, ESXi hosts, and
virtual machines. This allows you to seamlessly migrate your
Oracle database on VMware vCenter to GCVE without
needing to refactor the application or modify the existing
architecture.
For further results, you can visit the given URL.
https://cloud.google.com/blog/products/databases/migrate-
databases-to-google-cloud-vmware-engine-gcve

68. Answer: C
Explanation:
Database Migration Service is a fully managed service
provided by Google Cloud that simplifies and automates the
migration of databases from various sources to Google
Cloud. It supports migrating MySQL databases from AWS
Cloud to Google Cloud SQL.
For further results, you can visit the given URL.
https://cloud.google.com/database-migration/docs
69. Answer: C
Explanation:
When setting up a Bare Metal environment for installing and
running Oracle workloads on Google Cloud, you can
configure a NAT (Network Address Translation) Gateway on
a Compute Engine instance. This allows the host to connect
to the internet and download updates.
A NAT Gateway acts as an intermediary between the private
network of the Bare Metal host and the internet. It
translates the private IP addresses of the host to a public IP
address, allowing the host to communicate with resources
on the internet.
For further results, you can visit the given URL.
https://cloud.google.com/nat/docs/set-up-manage-network-
address-translation

70. Answer: A and B
Explanation:
A. Use customer-managed keys to encrypt data in a region
where local bodies are located:
By utilizing customer-managed keys, the organization can
have control over the encryption keys used to encrypt their
data. This ensures that the data is encrypted and provides
additional security measures. Choosing a region where local
bodies 
are 
located 
helps 
address 
data 
residency
requirements.
B. Use VPC Service Controls and Identity and Access
Management (IAM):
VPC Service Controls allow organizations to define security
perimeters around their resources. By using VPC Service
Controls and Identity and Access Management (IAM), the
organization 
can 
establish 
granular 
access 
controls,
ensuring that only authorized individuals or services can
access the Cloud SQL instance. This helps in meeting
compliance requirements and protecting sensitive data.
For further results, you can visit the given URLs.
https://cloud.google.com/vpc-service-controls/docs/overview
71. Answer: C
Explanation:

The challenges of splitting a large monolithic instance into
smaller databases are not the same. Furthermore, expensive
queries from one function have no effect on the application's
other functions.
For further results, you can visit the given URL.
https://cloud.google.com/compute/docs/disks/resize-
persistent-
disk#:~:text=In%20the%20list%20of%20persistent,resize%
20up%20to%202%20TB.
72. Answer: C
Explanation:
To ensure secure connectivity to the Cloud SQL instance
from the colocation facility without making changes to the
Cloud SQL instance, you can use Cloud SQL Proxy with a
service account and the private IP address of the Cloud SQL
instance.
Enabling a public IP address for the Cloud SQL instance
(option A) would expose the database directly to the
internet, which may not be desirable from a security
standpoint.
Creating a connection string with the Cloud SQL private IP
address (option B) is a step in the right direction, but it
alone does not provide the necessary secure connectivity.

Cloud SQL Proxy (option C) acts as an intermediary between
your application and the Cloud SQL instance, providing
secure connections without requiring any changes to the
Cloud SQL instance itself. By using Cloud SQL Proxy with a
service account, you can establish a secure connection to the
Cloud SQL instance's private IP address.
For further results, you can visit the given URL.
https://cloud.google.com/sql/docs/mysql/connect-instance-
private-ip
73. Answer: C
Explanation:
The recommended platform is a Spanner multi-regional
instance to fulfill the requirement of deploying the existing
relational data on Google Cloud with scalability and high
availability.
Spanner is a globally distributed and horizontally scalable
relational database service offered by Google Cloud. It
provides ACID (Atomicity, Consistency, Isolation, Durability)
compliance, strong consistency, and automatic scaling. The
multi-regional instance in Spanner allows data to be
distributed across multiple regions, providing low-latency
access and high availability.
For further results, you can visit the given URL.

https://cloud.google.com/spanner/docs/instance-
configurations
74. Answer: D and E
Explanation:
The following steps can be taken to troubleshoot the
replication lag issue between the primary replica and read
replicas 
in 
Cloud 
SQL 
for 
MySQL 
without 
causing
disruptions:
D. Set parallel replication flags: By enabling parallel
replication, multiple threads can be used to apply changes
from the primary replica to the read replicas. This can help
improve the replication speed and reduce replication lag.
E. Find and fix slow-running queries: Slow-running queries
on the primary replica can cause replication lag as the read
replicas need to wait for the changes to be applied.
Identifying and optimizing slow-running queries can help
improve the overall performance and reduce replication lag.
For further results, you can visit the given URLs.
https://cloud.google.com/sql/docs/mysql/replication/manag
e-replicas
75. Answer: D
Explanation:

To deploy Bigtable clusters with high availability and adhere
to Google's best practices, option D suggests deploying four
clusters across the US and APAC regions.
By having clusters in both the US and APAC regions, you
ensure redundancy and high availability. One cluster in each
region 
allows 
for 
workload 
distribution 
and 
failover
capability in case of any regional issues.
Maintaining a target of 35% CPU utilization is recommended
to leave ample headroom for spikes in workload and
unexpected resource demands. It ensures that the clusters
have enough capacity to handle increased traffic and
processing requirements.
For further results, you can visit the given URL.
https://cloud.google.com/compute/docs/regions-zones
76. Answer: D
Explanation:
Memorystore is a fully managed in-memory data store
service provided by Google Cloud. It is based on the open-
source Redis in-memory database and offers sub-millisecond
data access capabilities. By redesigning your application to
use Memorystore and modifying it to use an in-memory
database, you can significantly improve query response
times and overall application performance.

In-memory databases store data in memory rather than on
disk, allowing for faster data access and lower latency. This
makes Memorystore an ideal choice when sub-millisecond
response times are required.
For further results, you can visit the given URL.
https://cloud.google.com/appengine/docs/legacy/standard/g
o111/using-memorystore
77. Answer: B
Explanation:
DML (Data Manipulation Language) is extremely efficient in
updating, inserting, and deleting data from BigQuery tables.
DELETE FROM TableName
WHERE condition;
A simple query containing a table name and a condition for
operation can be used to delete the desired information.
For further results, you can visit the given URL.
https://cloud.google.com/bigquery/docs/data-manipulation-
language
78. Answer: A and C
Explanation:

The following data synchronization mechanisms can be
utilized to keep the on-prem data changes in sync with Cloud
SQL during the migration process:
A. Use Database Migration Service: Google Cloud provides
the Database Migration Service, which allows for seamless
and efficient migration of databases from various sources,
including on-premises environments, to Cloud SQL. It
supports continuous data replication, ensuring that changes
made to the on-prem databases are synchronized with the
Cloud SQL databases in near real-time. This helps minimize
downtime during the migration process.
C. Use replication from an external server to Cloud SQL:
Another approach is to set up replication from the on-
premises MySQL server to Cloud SQL. This involves
configuring the on-premises MySQL server as the master
and the Cloud SQL instance as the replica. With this setup,
changes made on the on-premises database are replicated to
the Cloud SQL instance, ensuring data synchronization.
Once the migration is complete, the replication can be
stopped, and the Cloud SQL instance can serve as the
primary database.
For further results, you can visit the given URL.
https://cloud.google.com/database-migration?hl=en
79. Answer: C

Explanation:
Firestore in native mode provides a flexible and scalable
NoSQL document database. It allows you to store data in a
schema-less format, making it suitable for supporting
dynamic schemas. Each player profile can be stored as a
document within Firestore, with the player username as the
key or identifier.
Firestore also provides real-time synchronization, which
means that any changes made to the player profile
documents will be automatically synchronized across devices
and can be easily accessed for analytics purposes. You can
set up a data pipeline to export the Firestore data to
BigQuery, allowing you to perform advanced analytics on the
player profile data.
For further results, you can visit the given URL.
https://cloud.google.com/datastore/docs/firestore-or-
datastore
80. Answer: B
Explanation:
Update the maintenance window of the Cloud SQL instance
to after business hours.
By adjusting the maintenance window of your Cloud SQL
instance, you can schedule maintenance activities to occur

during a time when your application has lower traffic or
during non-business hours. This reduces the impact on your
users and minimizes the chances of unplanned outages
during critical periods.
For further results, you can visit the given URL.
https://cloud.google.com/sql/docs/mysql/maintenance
81. Answer: D
Explanation:
By 
enabling 
automated 
backups, 
Cloud 
SQL 
will
automatically create regular backups of your database
instance. These backups are taken at specific intervals and
are stored in a separate storage location, ensuring data
durability and providing a recovery option in case of any
issues.
Turning off transaction log backups is a suitable choice for a
non-critical instance with a longer RPO requirement.
Transaction log backups are generally used for point-in-time
recovery and are useful for critical databases where minimal
data loss is desired. However, in this case, with an RPO of 1
week, the need for transaction log backups might not be
essential.
For further results, you can visit the given URL.

https://cloud.google.com/sql/docs/mysql/backup-
recovery/backups
82. Answer: C
Explanation:
To allow the hosted application to access the SQL Server
instance on Cloud SQL without making any configuration
changes, you can use Cloud SQL Auth Proxy with a service
account for authentication and connect via the private IP
address of the Cloud SQL instance. Therefore, the correct
option is C.
For further results, you can visit the given URL.
https://cloud.google.com/sql/docs/mysql/sql-proxy
83. Answer: D
Explanation:
Cloud Spanner (Autoscaler) is an open-source tool for
automatically increasing or decreasing the number of nodes
or processing units in one or more Spanner instances based
on how their capacity is used. This will ensure availability
and performance while also lowering costs during periods of
low demand.
For further results, you can visit the given URL.

https://cloud.google.com/kubernetes-
engine/docs/concepts/cluster-autoscaler
84. Answer: C
Explanation:
To enable a daily extract in CSV format from Cloud SQL for
MySQL while following best practices, you can use Cloud
Scheduler in combination with a Cloud Function that calls
the Cloud SQL export API.
For further results, you can visit the given URL.
https://cloud.google.com/scheduler/docs/tut-pub-sub
85. Answer: C
Explanation:
Bigtable is a fully managed, highly scalable NoSQL database
service provided by Google Cloud. It is designed to handle
massive workloads, providing high throughput and low-
latency access to large volumes of data. Bigtable is suitable
for applications that require scalability, high performance,
and the ability to handle millions of requests per second with
sub-second latency.
By using Bigtable, you can add nodes to the cluster as
necessary 
to 
meet 
the 
increasing 
demand 
for your
application. 
This 
allows 
you 
to 
scale 
the 
database

horizontally and handle higher traffic loads. Bigtable
automatically handles the distribution of data across the
added nodes, ensuring consistent performance.
For further results, you can visit the given URL.
https://cloud.google.com/bigtable/docs/instances-clusters-
nodes
86. Answer: A
Explanation:
For a cost-effective and durable backup storage solution for
your SQL Server database running on a Compute Engine
instance, it is recommended to store the backups on Google
Cloud Storage. Cloud Storage offers durability, scalability,
and high availability for storing your backups.
By configuring lifecycle rules on Cloud Storage, you can
define policies to automatically transition the backups to
low-cost storage classes as they age. This allows you to
optimize costs by moving the backups to more cost-effective
storage tiers, such as Nearline Storage or Coldline Storage,
while still maintaining their durability.
For further results, you can visit the given URL.
https://cloud.google.com/storage/docs/lifecycle
87. Answer: C

Explanation:
According to Google's documentation, using 4TB or more
will result in higher throughput and IOPS. IOPS increases as
disk size increases.
For further results, you can visit the given URL.
https://cloud.google.com/sql/docs/sqlserver/best-
practices#admin
88. Answer: C
Explanation:
The Spanner Multi Region configuration protects against
regional failures, and the customer-managed key gives users
control over their encryption key.
For further results, you can visit the given URL.
https://cloud.google.com/spanner/docs/cmek
89. Answer: B
Explanation:
Use Cloud Shell to run the gcloud sql operations list
command.
Here are the steps to accomplish this:
Open Cloud Shell in the Google Cloud Console.

Run the following command: gcloud sql operations list --
instance=INSTANCE_ID --type=BACKUP
Replace INSTANCE_ID with the actual ID of your Cloud SQL
instance.
This command will list all the operations related to backups
for the specified instance. It will display information such as
the operation ID, start time, end time, status, and any error
messages associated with the backup operation.
For further results, you can visit the given URL.
https://cloud.google.com/sdk/gcloud/reference/sql/operatio
ns/list
90. Answer: D
Explanation:
To ensure recommended identity and access management
best practices when accessing a Cloud SQL instance from a
Compute Engine VM, you should choose option D: Create a
separate service account to use with the Compute Engine
VM where the application is hosted.
For further results, you can visit the given URL.
https://cloud.google.com/compute/docs/access/service-
accounts

91. Answer: D
Explanation:
The correct answer is D: For high availability, use the
Google Kubernetes Engine in conjunction with StatefulSets.
Google Kubernetes Engine (GKE) is a managed Kubernetes
service provided by Google Cloud. It allows you to deploy,
manage, 
and 
scale 
containerized 
applications 
using
Kubernetes. When working with containerized MySQL
database workloads and aiming for high availability, GKE
provides the necessary features and capabilities.
By deploying MySQL containers on GKE using StatefulSets,
you can ensure high availability and reliability. StatefulSets
are specifically designed for stateful applications like
databases and provide unique identities and stable network
addresses for each instance.
For further results, you can visit the given URL.
https://cloud.google.com/blog/products/containers-
kubernetes/best-practices-for-creating-a-highly-available-
gke-cluster
92. Answer: C
Explanation:
If you do not enable Cloud SQL Admin API, you will see
Error 403: Access Not Configured in your Cloud SQL Auth

proxy logs.
For further results, you can visit the given URL.
https://cloud.google.com/sql/docs/mysql/admin-api
93. Answer: C
Explanation:
Cloning a Cloud SQL instance allows you to create an exact
copy of your database instance, including all its data and
configuration. By using this method, you can create a clone
of your mission-critical database and perform backups from
the clone without impacting the primary instance.
For further results, you can visit the given URL.
https://cloud.google.com/sql/docs/mysql/clone-instance
94. Answer: A
Explanation:
To address the high CPU utilization issue in a multi-region
Cloud Spanner configuration without making any application
changes, the first course of action should be:
A. Increase the nodes or processing units.
By increasing the number of nodes or processing units in
Cloud Spanner, you can distribute the workload across more
resources, which can help alleviate the high CPU utilization.

This allows Spanner to handle a higher number of
concurrent queries and transactions, leading to improved
response times for users.
For further results, you can visit the given URL.
https://cloud.google.com/spanner/docs/compute-capacity
95. Answer: A and C
Explanation:
The following options can be considered to recover and
restore a mission-critical Cloud Spanner instance after data
corruption:
A. For significant corruption, perform a restore from a
previous timestamp before corruption:
This option involves restoring the database from a backup
taken before the corruption occurred. By restoring the
database to a known good state, the corrupted data can be
replaced with clean data from the backup.
C. For significant corruption, import the database from a
previous point in time:
This option involves importing the database from a previous
point-in-time backup. This allows you to replace the
corrupted data with a clean copy of the database.
For further results, you can visit the given URLs.
https://cloud.google.com/architecture/dr-scenarios-for-data

96. Answer: A
Explanation:
An entire database can be exported through the avro format.
For further results, you can visit the given URL.
https://cloud.google.com/spanner/docs/export
97. Answer: A
Explanation:
To meet audit reporting requirements, we'd need a way to
track changes. This can be addressed by employing a source
code control system (SCCS). Liquibase is an SCCS that can
be used with Spanner.
For further results, you can visit the given URL.
https://www.cloudskillsboost.google/focuses/10911?
parent=catalog
98. Answer: A
Explanation:
To scale up your Cloud SQL for MySQL instances by
increasing the number of CPUs while minimizing downtime
and effort, you can follow these steps:

A. Use the gcloud sql instances patch command to increase
the instance size.
Identify the current instance size and note down the
configuration details and settings.
Determine the desired number of CPUs for the scaled-up
instance.
Use the gcloud sql instances patch command with the --tier
flag to specify the desired instance size and the --project flag
to specify the project ID.
Example: gcloud sql instances patch INSTANCE_NAME --
tier=NEW_TIER --project=PROJECT_ID
Replace INSTANCE_NAME with the name of your Cloud SQL
instance and NEW_TIER with the desired tier that includes
the desired number of CPUs.
Confirm the operation when prompted.
For further results, you can visit the given URL.
https://cloud.google.com/sdk/gcloud/reference/sql/instance
s/patch
99. Answer: D
Explanation:
SSDs provide more IOPS as disk size increases and are
faster than HDD drives. Google Cloud also recommends

using SSD for databases. To migrate to SSD-based volumes,
you must first recreate the instance and then backup and
restore the database.
For further results, you can visit the given URL.
https://cloud.google.com/sql/docs/mysql/choosing-ssd-hdd
100. Answer: D
Explanation:
A high level of availability Setup enabled PITR (point-in-time
recovery), which can be used to restore the database to a
time before data corruption was detected.
For further results, you can visit the given URL.
https://cloud.google.com/spanner/docs/pitr
101. Answer: C
Explanation:
By creating a new service account and assigning the Reader
role, you provide the custom dashboard app with read
access to the databases while limiting its permissions to only
what is necessary for reporting statistics. The Reader role
allows the service account to retrieve data from Cloud SQL
and Cloud Spanner without granting unnecessary privileges.

Using a service account instead of a user account or the
default service account is preferable in this scenario because
it 
provides 
a 
separate 
identity 
specifically 
for 
the
application, making it easier to manage and control
permissions 
independently. 
Additionally, 
following 
the
principle of least privilege helps mitigate the risk of
unauthorized access or accidental data modification.
For further results, you can visit the given URL.
https://cloud.google.com/iam/docs/service-accounts-create
102. Answer: C
Explanation:
Cloud Logging is a comprehensive log management solution
provided by Google Cloud. It allows you to collect, store,
search, analyze, and monitor logs from various Google Cloud
services, including Cloud SQL.
For further results, you can visit the given URL.
https://cloud.google.com/logging/docs/alerting/log-based-
alerts
103. Answer: A and B
Explanation:
Local SSD for TempDB:

Ultra-fast IOPS:  Local SSDs offer extremely high
IOPS (Input/Output Operations Per Second), which
is essential for TempDB as it handles temporary
data 
and 
intermediate 
results 
during 
query
processing.
Low latency: The proximity of local SSDs to the VM
ensures 
minimal 
latency, 
further 
boosting
performance.
SSD Persistent Disk for Database Files:
Faster access times:  SSD persistent disks provide
significantly faster read and write speeds compared
to standard persistent disks.
Optimized for databases:  They are specifically
designed to handle the high I/O demands of
database workloads.
For further results, you can visit the given URL.
https://cloud.google.com/compute/docs/tutorials/creating-
high-performance-sql-server-instance
104. Answer: A
Explanation:
When setting up a new application on App Engine to connect
with Cloud SQL for MySQL instances in the same project,
the 
App 
Engine 
service 
account 
requires 
the
roles/cloudsql.client role. This role provides the necessary
permissions for the App Engine application to establish
connections and interact with the Cloud SQL instances while
following the principle of least privilege.

For further results, you can visit the given URL.
https://cloud.google.com/sql/docs/mysql/iam-roles
105. Answer: C
Explanation:
To efficiently monitor and report on high CPU utilization in a
very large database hosted on Cloud SQL for MySQL, you
can use Cloud Monitoring. Cloud Monitoring allows you to
set up alerts based on specific conditions and receive
notifications when those conditions are met. Here is why this
option is the correct choice:
C. Create a Cloud Monitoring alert to send notifications
when 
the 
CPU 
breaches 
the 
80% 
threshold: 
Cloud
Monitoring provides a comprehensive monitoring and
alerting solution for various Google Cloud services, including
Cloud SQL. By setting up a Cloud Monitoring alert, you can
define a condition that triggers when the CPU utilization
crosses the 80% threshold. Once the condition is met, Cloud
Monitoring can send notifications through various channels,
such as email, SMS, or PagerDuty. This allows you to
proactively 
monitor 
high 
CPU 
utilization 
and 
take
appropriate actions to address the issue promptly.
For further results, you can visit the given URL.
https://cloud.google.com/monitoring/alerts/using-alerting-ui

106. Answer: A, B, and C
Explanation:
Auto-scaling in Bigtable adjusts the number of nodes in the
cluster based on the workload and resource utilization. The
following attributes are considered:
A. CPU utilization: Auto-scaling monitors the CPU utilization
of the nodes. When the CPU utilization exceeds a certain
threshold, it can trigger scaling up by adding more nodes to
handle the increased workload. Conversely, if the CPU
utilization remains low, it can scale down by removing
unnecessary nodes.
B. Storage utilization: Bigtable also takes into account
storage utilization. If the storage usage exceeds a specified
threshold, it may indicate that the current cluster size is
inadequate to handle the data volume efficiently. In such
cases, auto-scaling can add more nodes to accommodate the
increased storage requirements.
C. Min and Max Number of Nodes: These parameters define
the boundaries within which auto-scaling can operate. The
minimum number of nodes specifies the lower limit,
ensuring that the cluster always has a minimum capacity.
The maximum number of nodes sets an upper limit to
prevent excessive scaling in case of unexpected spikes or to
control costs. Auto-scaling adjusts the number of nodes

within these limits based on the workload and utilization
metrics.
For further results, you can visit the given URL.
https://cloud.google.com/bigtable/docs/autoscaling
107. Answer: A
Explanation:
Cloud Spanner is a globally distributed, horizontally
scalable, and strongly consistent relational database service
that 
provides 
mission-critical 
applications 
with 
high
availability and low latency. It can store and process large
amounts of structured data with complex relationships, and
it supports ACID transactions. As a result, A Cloud Spanner
is the correct answer because it is the best fit for a global
travel booking platform that requires high availability, low
latency, real-time data processing, and ACID transaction
support. Cloud Bigtable is another option, but it may not
offer the same level of transactional consistency as Cloud
Spanner.
For further results, you can visit the given URL.
https://cloud.google.com/spanner/docs
108. Answer: A
Explanation:

The Datastore Emulator is a tool provided by Google Cloud
Platform (GCP) specifically designed for emulating the
behavior of the Cloud Datastore service in a local
development environment. It allows developers to test their
applications without relying on the actual cloud service,
reducing dependencies and facilitating smooth functionality
during development and testing.
For further results, you can visit the given URL.
https://cloud.google.com/datastore/docs/tools/datastore-
emulator
109. Answer: C
Explanation:
To lower the cost of backups for a Very Large Database
(VLDB) hosted on Cloud SQL for MySQL, you can reduce the
frequency of automated backups. By default, Cloud SQL
automatically performs automated backups to provide point-
in-time recovery for your database. However, reducing the
frequency of backups can help decrease the storage and
operational costs associated with backups.
For further results, you can visit the given URL.
https://cloud.google.com/sql/docs/mysql/backup-
recovery/backups

110. Answer: A
Explanation:
You can specify a maintenance window that excludes the
month 
of 
December 
by 
adjusting 
the 
Cloud 
SQL
maintenance settings. This 
ensures 
that 
no 
planned
maintenance activities that could result in downtime are
scheduled during that time. This method allows you to keep
running without interruptions while minimizing any potential
impact on your services. It is also a cost-effective solution
because it does not require any additional resources or
support tickets.
For further results, you can visit the given URL.
https://cloud.google.com/sql/docs/mysql/maintenance
111. Answer: D
Explanation:
It is recommended to implement query tags to determine the
application that is generating inefficient queries in a Cloud
SQL for PostgreSQL database. Query tags allow you to
associate queries with specific applications, providing
application-centric monitoring of the database.
For further results, you can visit the given URL.
https://cloud.google.com/blog/topics/developers-
practitioners/enable-query-tagging-sqlcommenter-

understand-application-impact-database-performance
112. Answer: C
Explanation:
The recommended approach is to leverage Cloud SQL for
MySQL external replicas to establish replicas in a hybrid
setup using Compute Engine and an on-premises data center
with MySQL databases.
For further results, you can visit the given URL.
https://cloud.google.com/sql/docs/mysql/replication/configu
re-replication-from-external
113. Answer: C
Explanation:
Migrating the Oracle databases to Google Cloud's Bare
Metal Solution for Oracle provides a suitable environment
for 
running 
Oracle-based 
applications 
on 
dedicated
hardware. To ensure a cost-effective backup and restore
solution with a 2-hour Recovery Time Objective (RTO) and a
15-minute Recovery Point Objective (RPO).
For further results, you can visit the given URL.
https://cloud.google.com/bare-metal/docs/bms-planning

114. Answer: D
Explanation:
To successfully migrate PostgreSQL databases from both on-
premises and AWS to Google Cloud SQL while minimizing
downtime and following Google's recommended practices, it
is advisable to adopt a combination approach that utilizes
both Database Migration Service (DMS) and partner tools.
For further results, you can visit the given URL.
https://cloud.google.com/solutions/database-migration
115. Answer: C
Explanation:
To improve the performance of write workloads in a multi-
regional Cloud Spanner instance, you should add additional
read-write replicas to the instance.
For further results, you can visit the given URL.
https://cloud.google.com/spanner/docs/instance-
configurations
116. Answer: C
Explanation:

Option C recommends creating an SQL dump file to migrate
the data. This method entails creating a temporary Cloud
SQL instance from which a SQL dump file containing data
from the running Cloud SQL instance in the us-central1
region can be generated. This dump file can then be saved to
the cloud. The performance impact on the running instance
is reduced by using a temporary instance and generating an
SQL dump file. The process of creating the dump file is
independent of the source instance and has no direct impact
on it. After creating and storing the SQL dump file in Cloud
Storage, you can import it into the new Cloud SQL instance
in the us-east1 region.
For further results, you can visit the given URL.
https://cloud.google.com/sql/docs/mysql/import-
export/import-export-sql
117. Answer: C
Explanation:
When encountering performance issues with a single large
Cloud SQL instance in a microservices architecture,
expanding the storage size allocated to the instance can help
address the problem.
For further results, you can visit the given URLs.

https://cloud.google.com/sql/docs/mysql/using-query-
insights
118. Answer: D
Explanation:
The correct answer is D. Use customer-managed encryption
keys (CMEK), VPC Service Controls, and Identity and Access
Management (IAM) policies.
When migrating existing on-premises PostgreSQL instances
to Cloud SQL for PostgreSQL while adhering to data
residency requirements and maintaining control over data
storage locations, encryption keys, and data access, you
should take the following course of action:
D. Use customer-managed encryption keys (CMEK), VPC
Service Controls, and Identity and Access Management
(IAM) policies.
For further results, you can visit the given URL.
https://cloud.google.com/kms/docs/cmek
119. Answer: A
Explanation:
When dealing with persistent replication lag between the
primary instance and read replicas in a Cloud SQL for

MySQL environment, analyzing and optimizing slow-running
queries or enabling parallel replication flags are the
recommended actions to address the issue.
For further results, you can visit the given URL.
https://cloud.google.com/sql/docs/mysql/replication/manag
e-replicas
120. Answer: D
Explanation: UNION operator removes the duplicate row,
while the UNION ALL does not do that.
SELECTcolumn_name(s) FROM table1
UNION
SELECT column_name(s) FROM table2;
For further details, you can visit the given URL.
https://www.techonthenet.com/sql/union_all.php
121. Answer: A
Explanation:
The readiness probe marks the pod running and starts
accepting traffic. After the pod runs, however, the liveness
probe is responsible for health checks for the pod's lifecycle.
For further results, you can visit the given URL.
https://cloud.google.com/run/docs/configuring/healthchecks

122. Answer: B (Define a GKE Service. Clients can use the
service name in the URL to connect)
Explanation:
The clients in the same cluster can use the service DNS
names.
For further details, you can visit the given URL.
https://cloud.google.com/kubernetes-
engine/docs/concepts/service-discovery
123. Answer: A
Explanation:
Auth tokens should be requested per user. Therefore, for
each user, a token is requested by the app, and the user
needs to authorize it.
For further details, you can visit the given URL.
https://developers.google.com/drive/api/guides/api-specific-
auth
124. Answer: B
Explanation:
The error refers to CPUs meaning VM instances. Therefore,
more VM instances must be added.

For further details, you can visit the given URL.
https://cloud.google.com/kubernetes-engine/docs/how-
to/node-upgrades-quota
125. Answer: A
Explanation:
Considering the principle of least privilege, you need to
grant data viewer, job user roles, and nothing more to fix
this issue. The data editor role in option B is not required.
For further results, you can visit the given URL.
https://cloud.google.com/iam/docs/roles-overview
126. Answer: C
Explanation: Cloud Spanner is a globally distributed,
horizontally scalable database service that combines the
benefits of both traditional relational databases and NoSQL
databases.
For further results, you can visit the given URL.
https://cloud.google.com/spanner/docs
127. Answer: B

Explanation: BigQuery is a fully managed, serverless data
warehouse designed for large-scale data analytics.
For further results, you can visit the given URL.
https://cloud.google.com/terms/services#:~:text=BigQuery
%3A%20BigQuery
128. Answer: C
Explanation: Cloud SQL is a fully managed relational
database service for MySQL, PostgreSQL, and SQL Server.
For further results, you can visit the given URL.
https://cloud.google.com/sql/docs/introduction
129. Answer: C
Explanation: Cloud Spanner is designed for globally
distributed, horizontally scalable, and strongly consistent
databases. 
It 
provides 
low-latency, 
high-throughput
architecture, making it suitable for real-time applications
that require globally consistent and high-performance data
access. While Cloud SQL is a fully-managed relational
database 
service 
and 
can 
be 
suitable 
for 
various
applications, it may not provide the same level of global
distribution and scalability as Cloud Spanner. Cloud
Firestore is a NoSQL document database primarily designed
for mobile and web applications, and Azure Cosmos DB is a

multi-model, globally distributed database service provided
by Microsoft Azure.
For further results, you can visit the given URL.
https://cloud.google.com/spanner?hl=en
130. Answer: A
Explanation: Cloud Firestore is a NoSQL document database
designed for web, mobile, and server applications.
For further results, you can visit the given URL.
https://firebase.google.com/products/firestore#:~:text=Clo
ud%20Firestore
131. Answer: D
Explanation: Cloud SQL fully manages MySQL, PostgreSQL,
and SQL Server database engines.
For further results, you can visit the given URL.
https://cloud.google.com/sql?hl=en
132. Answer: B
Explanation: Cloud Bigtable is designed for large-scale, real-
time analytics and streaming data processing.
For further results, you can visit the given URL.

https://cloud.google.com/bigtable/docs/overview#:~:text=B
igtable
133. Answer: D
Explanation: 
BigQuery 
is 
Google 
Cloud's 
managed,
serverless data warehouse that can process petabytes of
data in near real-time.  BigQuery GIS  ingests processes and
analyses geospatial data with the same syntax as in PostGIS
but in a Cloud environment.
For further results, you can visit the given URL.
https://ngis.com.au/Newsroom/Want-to-query-GIS-data-in-
Google-Cloud-You-need
134. Answer: A
Explanation: Cloud Memorystore is a fully managed in-
memory data store service built on the popular open-source
Redis.
For further results, you can visit the given URL.
https://cloud.google.com/memorystore?hl=en
135. Answer: B
Explanation: Datastore is a highly scalable NoSQL database
for your applications. Datastore 
automatically handles

sharding and replication, providing you with a highly
available and durable database that scales automatically to
handle your applications' load.
For further results, you can visit the given URL.
https://cloud.google.com/datastore#:~:text=Datastore
136. Answer: A
Explanation: Cloud Bigtable is a highly scalable, NoSQL
database 
service 
designed 
for 
large 
analytical 
and
operational workloads, including the storage and querying of
time-series data. It is well-suited for applications, like those in
the Internet of Things (IoT) domain, that require high-
throughput and low-latency access to large volumes of time-
series data. Cloud Spanner, while a powerful and globally
distributed 
database 
service, 
is 
more 
suitable 
for
transactional workloads. AWS Timestream is an Amazon Web
Services database service specifically designed for time-
series data. Cloud BigQuery is a fully-managed, serverless
data warehouse designed for analytics rather than real-time,
operational workloads like IoT time-series data storage and
querying.
For further results, you can visit the given URL.
https://cloud.google.com/bigtable?hl=en

137. Answer: B
Explanation: Cloud Storage provides cost-effective storage
for large amounts of unstructured data.
For further results, you can visit the given URL.
https://www.quora.com/What-are-the-benefits-and-
limitations-of-using-cloud-computing-for-data-storage-and-
processing
138. Answer: B
Explanation: Google Cloud Pub/Sub is a fully managed,
scalable, global, and secure messaging service that  allows
you to send and receive messages among applications and
services. 
For further results, you can visit the given URL.
https://cloud.google.com/mysql/migrating#:~:text=If%20mi
nimizing%20downtime%20is%20a,based%20replication%20i
s%20an%20option.
139. Answer: C
Explanation: DMS is recommended for mid-sized to large-
sized database migrations, especially when
For further results, you can visit the given URL.

https://cloud.google.com/mysql/migrating#:~:text=If%20mi
nimizing%20downtime%20is%20a,based%20replication%20i
s%20an%20option.
140. Answer: A
Explanation: Replicating from an external source using
native binary-logged replication allows for fine-grained
control of the migration process.
For further results, you can visit the given URL.
https://cloud.google.com/mysql/migrating#:~:text=If%20mi
nimizing%20downtime%20is%20a,based%20replication%20i
s%20an%20option.
141. Answer: B
Explanation: InnoDB is recommended for user tables
migrating to Cloud SQL to support features like replication
and point-in-time recovery
For further results, you can visit the given URL.
https://cloud.google.com/mysql/migrating#:~:text=If%20mi
nimizing%20downtime%20is%20a,based%20replication%20i
s%20an%20option.
142. Answer: C

Explanation: Imported MyISAM tables on Cloud SQL can
lead to stability and reliability issues during operations like
replication, point-in-time recovery, and failover.
For further results, you can visit the given URL.
https://cloud.google.com/mysql/migrating#:~:text=If%20mi
nimizing%20downtime%20is%20a,based%20replication%20i
s%20an%20option.
143. Answer: A
Explanation: SUPER privilege is not allowed for user
accounts in MySQL for Cloud SQL.
For further results, you can visit the given URL.
https://cloud.google.com/mysql/migrating#:~:text=If%20mi
nimizing%20downtime%20is%20a,based%20replication%20i
s%20an%20option.
144. Answer: C
Explanation: 
User 
privileges 
can 
affect 
migrations,
especially for objects with a DEFINER attribute, if the
privilege is not supported in Cloud SQL.
For further results, you can visit the given URL.
https://cloud.google.com/mysql/migrating#:~:text=If%20mi
nimizing%20downtime%20is%20a,based%20replication%20i

s%20an%20option.
145. Answer: D
Explanation: Cloud SQL for MySQL uses pre-defined flags
that can be modified via the UI Console, GCloud CLI, and
REST API to modify database parameters instead of calling
the SET GLOBAL command.
For further results, you can visit the given URL.
https://cloud.google.com/mysql/migrating#:~:text=If%20mi
nimizing%20downtime%20is%20a,based%20replication%20i
s%20an%20option.
146. Answer: C
Explanation: The content in this section primarily focuses
on designing database solutions that are scalable and highly
available.
For further results, you can visit the given URL.
https://medium.com/cloud-native-daily/how-to-design-and-
build-a-scalable-and-ha-system-196e56a35098
147. Answer: C
Explanation: Scalability in cloud database solutions refers
to the capacity to handle increased loads or growing

amounts of data and traffic.
For further results, you can visit the given URL.
https://medium.com/cloud-native-daily/how-to-design-and-
build-a-scalable-and-ha-system-196e56a35098
148. Answer: C
Explanation: 
High 
availability 
is 
crucial 
to 
ensure
continuous access to data, minimizing downtime and
providing a reliable service.
For further results, you can visit the given URL.
https://medium.com/cloud-native-daily/how-to-design-and-
build-a-scalable-and-ha-system-196e56a35098
149. Answer: A
Explanation: Avoiding a single point of failure is a key
consideration in designing scalable database solutions to
ensure reliability.
For further results, you can visit the given URL.
https://www.netapp.com/devops-solutions/what-is-
kubernetes-persistent-volumes
150. Answer: A

Explanation: Sharding allows for distributing data across
multiple servers, enabling dynamic addition or removal of
resources to handle increased workload.
For further results, you can visit the given URL.
https://medium.com/cloud-native-daily/how-to-design-and-
build-a-scalable-and-ha-system-196e56a35098
151. Answer: C
Explanation: High availability in databases refers to
providing continuous and reliable access to data, minimizing
downtime.
For further results, you can visit the given URL.
https://medium.com/cloud-native-daily/how-to-design-and-
build-a-scalable-and-ha-system-196e56a35098
152. Answer: D
Explanation: Replication creates copies of data, providing
redundancy and fault tolerance for high availability.
For further results, you can visit the given URL.
https://medium.com/cloud-native-daily/how-to-design-and-
build-a-scalable-and-ha-system-196e56a35098
153. Answer: C

Explanation: Read scalability refers to the capability of
distributing read operations across multiple nodes or
servers, enhancing the system's ability to handle increased
read demand efficiently.
For further results, you can visit the given URL.
https://medium.com/cloud-native-daily/how-to-design-and-
build-a-scalable-and-ha-system-196e56a35098
154. Answer: C
Explanation: Avoiding a single point of failure is crucial to
ensure fault tolerance and maintain system availability.
For further results, you can visit the given URL.
https://medium.com/cloud-native-daily/how-to-design-and-
build-a-scalable-and-ha-system-196e56a35098
155. Answer: C
Explanation: Data partitioning involves breaking down a
large database into smaller, more manageable pieces,
contributing to improved performance and scalability.
For further results, you can visit the given URL.
https://medium.com/cloud-native-daily/how-to-design-and-
build-a-scalable-and-ha-system-196e56a35098

156. Answer: B
Explanation: Replication lag refers to the delay in achieving
data consistency across replicated instances.
For further results, you can visit the given URL.
https://medium.com/cloud-native-daily/how-to-design-and-
build-a-scalable-and-ha-system-196e56a35098
157. Answer: C
Explanation: Sharding involves distributing data across
multiple servers, enhancing scalability and load-handling
capabilities.
For further results, you can visit the given URL.
https://medium.com/cloud-native-daily/how-to-design-and-
build-a-scalable-and-ha-system-196e56a35098
158. Answer: D
Explanation: Partitioning allows queries to focus on specific
data subsets, contributing to query optimization.
For further results, you can visit the given URL.
https://medium.com/cloud-native-daily/how-to-design-and-
build-a-scalable-and-ha-system-196e56a35098
159. Answer: D

Explanation: Ensuring consistency across shards is a
common challenge when implementing sharding in a cloud
database solution.
For further results, you can visit the given URL.
https://medium.com/cloud-native-daily/how-to-design-and-
build-a-scalable-and-ha-system-196e56a35098
160. Answer: C
Explanation: 
High 
availability 
is 
critical 
to 
ensure
continuous service availability, especially in mission-critical
applications where downtime is not acceptable.
For further results, you can visit the given URL.
https://medium.com/cloud-native-daily/how-to-design-and-
build-a-scalable-and-ha-system-196e56a35098
161. Answer: D
Explanation: Database integration aims to create a single
authoritative version by taking data from multiple sources
for better information sharing.
For further results, you can visit the given URL.
https://budibase.com/blog/data/how-to-integrate-multiple-
databases/

162. Answer: D
Explanation: Integration helps modern businesses use their
collected data efficiently, enabling faster and better
decision-making.
For further results, you can visit the given URL.
https://budibase.com/blog/data/how-to-integrate-multiple-
databases/
163. Answer: C
Explanation: Consolidating databases carries risks such as
data loss, corruption, and service interruptions.
For further results, you can visit the given URL.
https://budibase.com/blog/data/how-to-integrate-multiple-
databases/
164. Answer: B
Explanation: Read scalability involves distributing read
operations across multiple nodes for efficient handling of
increased read demand.
For further results, you can visit the given URL.
https://budibase.com/blog/data/how-to-integrate-multiple-
databases/

165. Answer: C
Explanation: Maintaining consistency and integrity involves
dealing with duplicate values and discrepancies between
databases.
For further results, you can visit the given URL.
https://budibase.com/blog/data/how-to-integrate-multiple-
databases/
166. Answer: B
Explanation: 
Querying 
multiple 
databases 
for 
one
application is an alternative that achieves data integration
without migrating data.
For further results, you can visit the given URL.
https://budibase.com/blog/data/how-to-integrate-multiple-
databases/
167. Answer: B
Explanation: Integrating databases with different schemas
requires assessing new database requirements for effective
integration.
For further results, you can visit the given URL.
https://budibase.com/blog/data/how-to-integrate-multiple-
databases/

168. Answer: C
Explanation: Integrating databases might be unavoidable to
meet the functional requirements of certain applications.
For further results, you can visit the given URL.
https://budibase.com/blog/data/how-to-integrate-multiple-
databases/
169. Answer: D
Explanation: When consolidating databases with a single
schema, retaining consistency across similar attributes is a
key consideration.
For further results, you can visit the given URL.
https://budibase.com/blog/data/how-to-integrate-multiple-
databases/
170. Answer: B
Explanation: Integration, by consolidating databases into a
single source, helps prevent duplication and inconsistency
issues by creating a single authoritative version.
For further results, you can visit the given URL.
https://budibase.com/blog/data/how-to-integrate-multiple-
databases/

171. Answer: B
Explanation: Data migration involves transferring data
between data storage systems, data formats, or computer
systems.
For further results, you can visit the given URL.
https://www.techtarget.com/searchstorage/definition/data-
migration
172. Answer: D
Explanation: An organization might undertake a data
migration 
project 
for 
various 
reasons, 
including
consolidating websites, installing software upgrades, and
moving data during a company merger.
For further results, you can visit the given URL.
https://www.techtarget.com/searchstorage/definition/data-
migration
173. Answer: C
Explanation: The primary goal of data migration is to boost
productivity and reduce storage costs by successfully and
securely transferring data to another application, storage
system, or cloud.

For further results, you can visit the given URL.
https://www.techtarget.com/searchstorage/definition/data-
migration
174. Answer: C
Explanation: Sustainable governance helps organizations
track and report on data quality during data migration,
ensuring the integrity of the data.
For further results, you can visit the given URL.
https://www.techtarget.com/searchstorage/definition/data-
migration
175. Answer: C
Explanation: Big bang migrations transfer all associated
data within a set time window and involve less risk of losing
data. In contrast, trickle migrations complete data migration
within phases with both old and new systems running
simultaneously.
For further results, you can visit the given URL.
https://www.techtarget.com/searchstorage/definition/data-
migration

ABOUT OUR PRODUCTS
Other products from IPSpecialist LTD regarding CSP
technology are:
AWS 
Certified 
Cloud 
Practitioner
Study guide
 
AWS 
Certified 
SysOps 
Admin 
-
Associate Study guide
AWS Certified Solution Architect -
Associate Study guide
 
AWS Certified Developer Associate
Study guide
AWS Certified Advanced Networking –
Specialty Study guide
AWS Certified Security – Specialty
Study guide
AWS Certified Big Data – Specialty
Study guide
Microsoft 
Certified: 
Azure

Fundamentals
 
Microsoft 
Certified: 
Azure
Administrator
 
 
Microsoft Certified: Azure Solution
Architect
 
Microsoft Certified: Azure DevOps
Engineer
 
 
Microsoft Certified: Azure Developer
Associate
Microsoft Certified: Azure Security
Engineer
Microsoft 
Certified: 
Azure 
Data
Fundamentals
Microsoft 
Certified: 
Azure 
AI
Fundamentals

Microsoft 
Certified: 
Azure 
Data
Engineer Associate
 
Microsoft 
Certified: 
Azure 
Data
Scientist
 
 
Microsoft Certified: Azure Network
Engineer
Oracle 
Certified: 
Foundations
Associate
 
 
Microsoft 
Certified: 
Security,
Compliance, 
and 
Identity
Fundamentals
 
 
Other 
Network 
& 
Security 
related 
products 
from
IPSpecialist LTD are:
CCNA Routing & Switching Study Guide
CCNA Security Second Edition Study Guide
CCNA Service Provider Study Guide

CCDA Study Guide
CCDP Study Guide
CCNP Security SCOR Study Guide
CCNP Enterprise ENCOR Study Guide
CCNP Service Provider SPCOR Study Guide
CCNP Security SVPN Study Guide
CCNP Enterprise ENARSI Study Guide
CCNP Service Provider SPRI Study Guide
CompTIA Network+ Study Guide
CompTIA Security+ Study Guide
Ethical Hacker Certification v11 First Edition Study
Guide
Ethical Hacker Certification v12 First Edition Study
Guide
Certified Blockchain Expert v2 Study Guide
Fortinet FortiOS Exam First Edition Study Guide
Palo Alto Certified Network Security Administrator
First Edition Study Guide
 
 


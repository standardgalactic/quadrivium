www.ebook3000.com

Enterprise Information Systems VII
www.ebook3000.com

Enterprise Information
Systems VII
edited by
Chin-Sheng Chen
Florida International University, 
Miami, FL, U.S.A.
Joaquim Filipe
Setúbal, Portugal
Isabel Seruca
Universidade Portucalense,
Porto, Portugal
and
José Cordeiro
Setúbal, Portugal
INSTICC/EST,
INSTICC/EST,
www.ebook3000.com

A C.I.P. Catalogue record for this book is available from the Library of Congress.
ISBN-10  1-4020-5323-1 (HB)
ISBN-13  978-1-4020-5323-8 (HB)
ISBN-10  1-4020-5347-9 (e-book)
ISBN-13  978-1-4020-5347-4 (e-book)
Published by Springer,
P.O. Box 17, 3300 AA Dordrecht, The Netherlands.
www.springer.com
Printed on acid-free paper
All Rights Reserved
© 2006 Springer 
No part of this work may be reproduced, stored in a retrieval system, or transmitted
in any form or by any means, electronic, mechanical, photocopying, microfilming, recording
or otherwise, without written permission from the Publisher, with the exception
of any material supplied specifically for the purpose of being entered
and executed on a computer system, for exclusive use by the purchaser of the work.
www.ebook3000.com

TABLE OF CONTENTS 
Preface.................................................................................................................................................. ix
Conference Committee......................................................................................................................... xi
EIS IMPLEMENTATION RESEARCH: AN ASSESSMENT AND SUGGESTIONS FOR 
THE FUTURE 
Henri Barki.............................................................................................................................................. 3
CHANGING THE WAY THE ENTERPRISE WORKS - OPERATIONAL 
TRANSFORMATIONS 
Thomas J. Greene.........................................................................................................................................................11
ENTERPRISE ONTOLOGY – UNDERSTANDING THE ESSENCE OF 
ORGANIZATIONAL OPERATION 
Jan L. G. Dietz...........................................................................................................................................................19
BUILDING SUCCESSFUL INTERORGANIZATIONAL SYSTEMS - IT AND 
CHANGE MANAGEMENT 
M. Lynne Markus .......................................................................................................................................................31
PART 1 – DATABASES AND INFORMATION SYSTEMS INTEGRATION 
THE HYBRID DIGITAL TREE - A NEW INDEXING TECHNIQUE FOR LARGE 
STRING DATABASES 
v
INVITED SPEAKERS 
Qiang Xue, Sakti Pramanik, Gang Qianand Qiang Zhu............................................................................................45
www.ebook3000.com

vi
MUSICAL RETRIEVAL IN P2P NETWORKS UNDER THE WARPING DISTANCE 
Ioannis Karydis, Alexandros Nanopoulos, Apostolos N. Papadopoulos and Yannis Manolopoulos ..............................53
CHANGE DETECTION AND MAINTENANCE OF AN XML WEB WAREHOUSE 
Ching-Ming Chao.........................................................................................................................................................61
CHOOSING GROUPWARE TOOLS AND ELICITATION TECHNIQUES 
ACCORDING TO STAKEHOLDERS' FEATURES 
Gabriela N. Aranda, Aurora Vizcaíno, Alejandra Cechich and Mario Piattini..........................................................69
ANALYTICAL AND EXPERIMENTAL EVALUATION OF STREAM-BASED JOIN 
Henry Kostowski and Kajal T. Claypool.......................................................................................................................77
PART 2 – ARTIFICIAL INTELLIGENCE AND DECISION SUPPORT SYSTEMS 
CONSTRUCTION OF DECISION TREES USING DATA CUBE 
Lixin Fu......................................................................................................................................................................87
AN APPLICATION OF NON-LINEAR PROGRAMMING TO TRAIN RECURRENT 
NEURAL NETWORKS IN TIME SERIES PREDICTION PROBLEMS 
M. P. Cuéllar, M. Delgado and M. C. Pegalajar .........................................................................................................95
INTELLIGENT SOLUTION EVALUATION BASED ON ALTERNATIVE USER 
PROFILES
Georgios Bardis, Georgios Miaoulis and Dimitri Plemenos..........................................................................................103
Ramón Alberto Carrasco, María Amparo Vila and José Galindo .............................................................................113
KNOWLEDGE MANAGEMENT IN NON-GOVERNMENTAL ORGANISATIONS:  
A PARTNERSHIP FOR THE FUTURE 
José Braga de Vasconcelos, Paulo Castro Seixas, Paulo Gens Lemos and Chris Kimble..............................................121
TOWARDS A CHANGE-BASED CHANCE DISCOVERY 
Zhiwen Wu and Ahmed Y. Tawfik...........................................................................................................................131
PART 3 – INFORMATION SYSTEMS ANALYSIS AND SPECIFICATION 
EARLY DETECTION OF COTS FUNCTIONAL SUITABILITY FOR AN E-PAYMENT 
CASE STUDY 
Alejandra Cechich and Mario Piattini........................................................................................................................141
PRESERVING THE CONTEXT OF INTERRUPTED BUSINESS PROCESS 
ACTIVITIES 
Sarita Bassil, Stefanie Rinderle, Rudolf Keller, Peter Kropf and Manfred Reichert ......................................................149
THE “RIGHT TO BE LET ALONE” AND PRIVATE INFORMATION 
Sabah S. Al-Fedaghi..................................................................................................................................................157
Table of Contents
USING dmFSQL FOR FINANCIAL CLUSTERING 
PERSPECTIVES ON PROCESS DOCUMENTATION - A CASE STUDY 
Jörg Becker, Christian Janiesch, Patrick Delfmann and Wolfgang Fuhr......................................................................167
www.ebook3000.com

vii
SUSTAINABLE DEVELOPMENT AND INVESTMENT IN INFORMATION 
TECHNOLOGIES - A SOCIO-ECONOMIC ANALYSIS 
Manuel João Pereira, Luís Valadares Tavares and Raquel Soares .............................................................................179
QUALITY OF SERVICE IN FLEXIBLE WORKFLOWS THROUGH PROCESS 
CONSTRAINTS 
Shazia Sadiq, Maria Orlowska, Joe Lin and Wasim Sadiq ......................................................................................187
REAL TIME DETECTION OF NOVEL ATTACKS BY MEANS OF DATA MINING 
TECHNIQUES 
Marcello Esposito, Claudio Mazzariello, Francesco Oliviero, Simon Pietro Romano and Carlo Sansone.....................197
PART 4 – SOFTWARE AGENTS AND INTERNET COMPUTING 
GENERIC FAULT-TOLERANT LAYER SUPPORTING PUBLISH/SUBSCRIBE 
MESSAGING IN MOBILE AGENT SYSTEMS 
Milovan Tosic and Arkady Zaslavsky........................................................................................................................207
BOOSTING ITEM FINDABILITY: BRIDGING THE SEMANTIC GAP BETWEEN 
SEARCH PHRASES AND ITEM INFORMATION 
Hasan Davulcu, Hung V. Nguyen and Viswanathan Ramachandran.......................................................................215
INTEGRATING AGENT TECHNOLOGIES INTO ENTERPRISE SYSTEMS USING 
WEB SERVICES 
Eduardo H. Ramírez and Ramón F. Brena...............................................................................................................223
PART 5 – HUMAN-COMPUTER INTERACTION 
OPENDPI: A TOOLKIT FOR DEVELOPING DOCUMENT-CENTERED 
ENVIRONMENTS 
Olivier Beaudoux and Michel Beaudouin-Lafon..........................................................................................................231
WHY ANTHROPOMORPHIC USER INTERFACE FEEDBACK CAN BE EFFECTIVE 
AND PREFERRED BY USERS 
Pietro Murano............................................................................................................................................................241
DISTANCE LEARNING BY INTELLIGENT TUTORING SYSTEM. 
AGENT-BASED ARCHITECTURE FOR USER-CENTRED ADAPTIVITY 
Antonio Fernández-Caballero, José Manuel Gascueña, Federico Botella and Enrique Lazcorreta...............................249
A CONTROLLED EXPERIMENT FOR MEASURING THE USABILITY OF WEBAPPS 
USING PATTERNS 
F. Javier García, María Lozano, Francisco Montero, Jose Antonio Gallud, Pascual González and  
Carlota Lorenzo.........................................................................................................................................................257
Table of Contents
AUTHOR INDEX................................................................................................................................................265
www.ebook3000.com

PREFACE
Systems (ICEIS 2005), held in Miami (USA) and organized by INSTICC (Institute for Systems and 
Technologies 
of 
Information, 
Communication 
and 
Control) 
in 
collaboration 
with 
FLORIDA 
INTERNATIONAL UNIVERSITY, who hosted the event.  
Following the route started in 1999, ICEIS has become a major point of contact between research 
scientists, engineers and practitioners on the area of business applications of information systems. This 
conference, which is now one of the largest annual conferences in the Information Systems area, has 
received an increased interest every year, especially from the international academic community. This 
year, five simultaneous tracks were held, covering different aspects related to enterprise computing, 
including: “Databases and Information Systems Integration”, “Artificial Intelligence and Decision Support Systems”,
“Information Systems Analysis and Specification”, “Software Agents and Internet Computing” and “Human-Computer 
Interaction”. The sections of this book reflect the conference tracks. 
ICEIS 2005 received 459 paper submissions from 41 different countries, in all continents. 89 papers 
were published and orally presented as full papers, i.e. completed work, 110 position papers reflecting 
work-in-progress were accepted for short presentation and another 90 for poster presentation. These 
numbers, leading to a “full-paper” acceptance ratio below 20%, show the intention of preserving a high 
quality forum.  
As usual in the ICEIS conference series, a number of invited talks, including keynote lectures and 
technical tutorials were also held. These special presentations made by internationally recognized experts 
have definitely increased the overall quality of the Conference and provided a deeper understanding of 
the Enterprise Information Systems field. Some of these contributions have been included in a special 
section of this book. 
The program for this conference required the dedicated effort of many people. Firstly, we must thank 
the authors, whose research and development efforts are recorded here. Secondly, we thank the 
members of the program committee and the additional reviewers for their diligence and expert 
reviewing. Thirdly, we thank the invited speakers for their invaluable contribution and for taking the 
time to synthesise and prepare their talks. Fourthly, we thank the workshop chairs whose collaboration 
with ICEIS was much appreciated. Finally, special thanks to all the members of the local organising 
committee, especially Ron Giachetti, whose collaboration was fundamental for the success of this 
conference. 
Chin-Sheng Chen, Florida International University, USA 
Joaquim Filipe, INSTICC / EST Setúbal, Portugal 
Isabel Seruca,  Universidade Portucalense, Portugal 
José Cordeiro, INSTICC / EST Setúbal, Portugal
ix
This book contains the best papers of the Seventh International Conference on Enterprise Information 
www.ebook3000.com

CONFERENCE COMMITTEE 
Conference co-Chairs 
Joaquim Filipe, INSTICC / EST Setúbal, Portugal 
Chin-Sheng Chen, Florida International University, USA 
Programme co-Chairs 
Isabel Seruca, Universidade Portucalense, Portugal 
José Cordeiro, INSTICC / EST Setúbal, Portugal 
Organising Committee 
Marina Carvalho, INSTICC, Portugal 
Bruno Encarnação, INSTICC, Portugal 
Ronald Giachetti, Florida International University, USA 
Vítor Pedrosa, INSTICC, Portugal 
Senior Programme Committee 
Luís Amaral, Portugal  
Peter B. Andersen, Denmark  
Ricardo Baeza-Yates, Chile
Jean Bézivin, France  
Enrique Bonsón, Spain  
João Alvaro Carvalho, Portugal  
Albert Cheng, USA
Miguel Delgado, Spain  
Jan Dietz, The Netherlands  
Frank Dignum, The Netherlands  
António Figueiredo, Portugal  
Göran Goldkuhl, Sweden  
Thomas Greene, USA  
Nuno Guimarães, Portugal  
Jeet Gupta, USA 
Jean-Paul Haton, France 
Alberto Laender, Brazil
Maurizio Lenzerini, Italy  
Michel Léonard, Switzerland  
Kecheng Liu, UK
Peri Loucopoulos, UK  
Paul Luker, UK  
Kalle Lyytinen, USA  
Yannis Manolopoulos, Greece  
José Legatheaux Martins, Portugal  
Masao Matsumoto, Japan  
James Odell, USA 
George Papadopoulos, Cyprus 
Luís Moniz Pereira, Portugal  
Alain Pirotte, Belgium  
Klaus Pohl, Germany 
Matthias Rauterberg, The Netherlands 
Colette Rolland, France
Abdel-Badeeh Salem, Egypt 
Bernadette Sharp, UK  
Alexander Smirnov, Russia
Ronald Stamper, The Netherlands
Reza Torkzadeh, USA 
Miguel Toro, Spain  
José Tribolet, Portugal
François Vernadat, Luxembourg
Frank Wang, UK 
Merrill Warkentin, USA
Hans Weigand, The Netherlands  
Roel Wieringa, The Netherlands
xi
www.ebook3000.com

xii
Programme Committee 
Jesus S. Aguilar-Ruiz, Spain
Patrick Albers, France  
Salah Al-Sharhan, Kuwait
Andreas Andreou, Cyprus  
Pedro Antunes, Portugal 
Joaquim Aparício, Portugal  
Juan Carlos Augusto, UK
Bart Baesens, UK 
Cecilia Baranauskas, Brazil  
Balbir Barn,UK 
Senén Barro, Spain
Remi Bastide, France  
Nadia Bellalem, France  
Peter Bernus, Australia  
Peter Bertok, Australia
Robert Biddle, Canada  
Oliver Bittel, Germany  
Fernando Boavida, Portugal  
Luis Borges Gouveia, Portugal  
Djamel Bouchaffra, USA 
Danielle Boulanger, France
Jean-Louis Boulanger, France  
José Braga de Vasconcelos, Portugal  
Miguel Calejo, Portugal
Coral Calero, Spain  
Luis M. Camarinha-Matos, Portugal
Jorge Cardoso, Portugal
Fernando Carvalho, Brazil
Jose Jesus Castro-Schez, Spain  
Luca Cernuzzi, Paraguay 
Maria Filomena de Castro Lopes, Portugal  
Elizabeth Chang, Australia
Laurent Chapelier, France  
Nian-Shing Chen, Australia 
William Cheng-Chung Chu, Taiwan  
Rodney Clarke, UK 
Chrisment Claude, France  
Francesco Colace, Italy  
Bernard Coulette, France
Sharon Cox, UK 
Mohamed Dahchour, Morocco  
Sergio de Cesare, UK
Andrea De Lucia, Italy  
Nuno de Magalhães Ribeiro, Portugal 
José Javier Dolado, Spain  
Jean-Christophe Dubois, France  
Schahram Dustdar, Austria  
Alan Eardley, UK 
David Emery, UK 
Jean-Max Estay, France  
João Faria, Portugal
Jesus Favela, USA  
Eduardo Fernández-Medina, Spain 
Edilson Ferneda, Brazil  
Paulo Ferreira, Portugal
Filomena Ferrucci, Italy  
Andrew Finegan, Australia
Andre Flory, France  
Donal Flynn, UK  
Ulrich Frank, Germany  
Ana Fred, Portugal
Lixin Fu, USA 
Juan Garbajosa, Spain
Marcela Genero, Spain  
Joseph Giampapa, USA
Raúl Giráldez, Spain 
Pascual González, Spain  
Robert Goodwin, Australia  
Silvia Gordillo, Argentina  
John Gordon, UK 
Feliz Gouveia, Portugal
Virginie Govaere, France 
Jan Gulliksen, Sweden
Rune Gustavsson, Sweden
Sissel Guttormsen Schär, Switzerland  
Lamia Hadrich Belguith, Tunisia 
Thorsten Hampel, Germany  
Michael Heng, Australia  
Francisco Herrera, Spain  
Colin Higgins, UK 
Peter Higgins, Australia
Erik Hollnagel, Sweden
Jun Hong, UK 
Nguyen Hong Quang, Viet Nam  
Jiankun Hu, Australia 
Kaiyin Huang, The Netherlands  
Conference Committee
www.ebook3000.com

xiii
Patrick C. K. Hung, Canada  
Hamid Jahankhani, UK  
Arturo Jaime, Spain  
Luis Jiménez Linares, Spain 
Luis Joyanes, Spain  
Nikos Karacapilidis, Greece
Dimitris Karagiannis, Austria  
Stamatis Karnouskos, Germany  
Hiroyuki Kawano, Japan
Nicolas Kemper Valverde, Mexico  
A. Rahman Khan, USA 
Manuel Kolp, Belgium  
John Krogstie, Norway 
Stan Kurkovsky, USA 
Yannick Lallement, Canada  
Chul-Hwan Lee, USA 
Carlos León de Mora, Spain
Hareton Leung, China
Therese Libourel, France
John Lim, Singapore  
Matti Linna, Finland  
Jan Ljungberg, Sweden  
Stephane Loiseau, France  
João Correia Lopes, Portugal
María Dolores Lozano, Spain  
Jianguo Lu, Canada  
Christopher Lueg, Australia  
Edmundo Madeira, Brazil  
Laurent Magnin, Canada
Sam Makki, USA 
Mirko Malekovic, Croatia  
Nuno Mamede, Portugal
João Mangueira Sobral, Brazil
Esperanza Marcos, Spain
Farhi Marir, UK 
Maria João Martins, Portugal
Herve Martin, France  
Johannes Mayer, Germany 
Andreas Meier, Switzerland  
Emilia Mendes, New Zealand 
Engelbert Mephu Nguifo, France 
Miguel Mira da Silva, Portugal  
Ghodrat Moghadampour, Finland  
Paula Morais, Portugal
Fernando Moreira, Portugal  
José Moreira, Portugal  
Hector Munoz-Avila, USA 
Mietek Muraszkiewicz, Poland 
Ana Neves, Portugal 
Jose Angel Olivas, Spain 
Luis Olsina Santos, Argentina
Peter Oriogun, UK
Marcin Paprzycki, USA  
José R. Paramá, Spain  
Oscar Pastor, Spain  
Maria Carmen Gramaje, Spain  
Gabriel Pereira Lopes, Portugal
Laurent Péridy, France
Antonio Pescapé, Italy  
Steef Peters, The Netherlands 
Paolo Petta, Austria 
José Adriano Pires, Portugal
Jacek Plodzien, Poland  
Geert Poels, Belgium 
Macario Polo, Spain
Bhanu Prasad, USA 
Ed Price, USA
Pedro Ramos, Portugal  
Ulrich Reimer, Switzerland  
Marinette Revenu, France
Simon Richir, France  
António Rito-Silva, Portugal
David Rivreau, France 
Pilar Rodriguez, Spain
Agostinho Rosa, Portugal  
Gustavo Rossi, Argentina
Narcyz Roztocki, USA  
Francisco Ruiz, Spain
Henryk Rybinski, Poland 
Henry Samier, France 
Manuel Santos, Portugal  
Daniel Schang, France  
Arno Scharl, Australia
Mareike Schoop, Germany  
Hanifa Shah, UK 
Jianhua Shao, UK  
Timothy K. Shih, Taiwan  
Charles Shoniregun, UK
Alberto Silva, Portugal
Maria João Ferreira, Portugal
Conference Committee

xiv
Janice Sipior, USA 
Hala Skaf-Molli, France  
Liz Sokolowski, UK 
Chantal Soule-Dupuy, France  
Chris Stary, Austria 
Vijayan Sugumaran, USA 
Lily Sun, UK
David Taniar, Australia
Sotirios Terzis, UK 
Philippe Thiran, The Netherlands  
Claudine Toffolon, France  
Robert Tolksdorf, Germany  
Ambrosio Toval, Spain  
Gulden Uchyigit, UK
Antonio Vallecillo, Spain  
Luminita Vasiu, UK 
Christine Verdier, France  
Maria-Amparo Vila, Spain  
HO Tuong Vinh, Viet Nam  
Aurora Vizcaino, Spain   
Hans Weghorn, Germany 
Gerhard Weiss, Germany  
Graham Winstanley, UK 
Wita Wojtkowski, USA 
Robert Wrembel, Poland
Baowen Xu, China
Haiping Xu, USA 
Hongji Yang, UK 
Yoneo Yano, Japan  
Kokou Yetongnon, France  
Liping Zhao, UK 
Shuigeng Zhou, China  
Lin ZongKai, China 
Invited Speakers 
Richard Soley, Object Management Group, Inc., USA 
Jan Dietz, Delft University of Technology, The Netherlands 
Thomas Greene, MIT, USA 
Rosalind W. Picard, MIT, USA 
Henri Barki, HEC Montreal, Canada 
Daniel Schwabe, Catholic University in Rio de Janeiro (PUC-Rio), Brazil 
M. Lynne Markus, Bentley University, USA 
Raghavan N. Srinivas, Sun Microsystems, USA 
Eduardo B. Fernandez, Florida Atlantic University, USA 
Conference Committee

Invited Speakers 

EIS IMPLEMENTATION RESEARCH: AN ASSESSMENT AND 
SUGGESTIONS FOR THE FUTURE 
Henri Barki 
Canada Research Chair in Information Technology Implementation and Management 
HEC Montréal, 3000 chemin de la Côte-Ste-Catherine 
Montréal, Québec, Canada H3T 2A7 
Email: henri.barki@hec.ca 
Keywords: 
Systems implementation, Acceptance and diffusion of innovations, Behavioral research. 
Abstract: 
The implementation of information systems in organizations is a long standing research topic that has 
preoccupied researchers ever since computer-based information systems started being used in businesses in 
the early 1960s. However, despite more than 40 years of research, the implementation phenomenon 
continues to present both practical and research difficulties. The present paper presents a broad overview 
and assessment of past and current research on implementation, and provides suggestions for future research 
that will help address some of the challenges implementation researchers currently face. 
1 INTRODUCTION 
The implementation of information systems in 
organizations is a long standing research topic that 
has preoccupied researchers ever since computer-
based information systems started being used in 
businesses in the early 1960s. The initial motivation 
for 
researching 
implementation 
phenomena 
stemmed from the need to address the practical 
difficulties that plagued most information system 
development projects that organizations were 
implementing at the time. While today’s information 
system projects are less about development 
conducted in-house, and more on configuring 
integrated systems purchased from a vendor and 
installed by a third party, they continue to suffer 
from similar difficulties such as budget and schedule 
overruns, and the frustration and lack of satisfaction 
on the part of the users. For example, according to 
Barki et al. (2005), ERP implementations are more 
than 200% late and more than 170% over budget, 
with 50% of ERP projects failing to achieve their 
expected benefits and delivering less than 60% of 
what is expected.  
The objective of the present paper is to provide a 
broad overview and assessment of past and current 
research on the implementation of information 
systems, and to provide suggestions for future 
research that will help address some of the 
challenges implementation researchers currently 
face. It is important to note that the present paper’s 
focus is on implementation research that adheres to 
what Hevner et al. (2004) refers to as the behavioral 
science paradigm. Research following this approach 
is typically driven by the practical concerns and 
difficulties encountered in practice. Its broad 
objective is the improvement of organizational 
efficiency and effectiveness via the introduction and 
use of IT-based solutions. As such, it views 
implementation broadly to include all stages of IT 
adoption, diffusion, and use stages, from initial idea 
to roll out and afterwards (Cooper and Zmud 1990; 
Markus and Tanis 2000; Swanson and Ramiller 
2004). It is concerned with the study of individual, 
organizational, technological, and societal factors 
that influence the introduction and use of IT-based 
solutions in organizations. As such, behavioral 
science implementation research is different from 
research that follows the design science paradigm 
(Hevner et al. 2004) whose focus is the creation of 
intellectual and computational tools.  
The next two sections of the paper provide a 
broad summary and assessment of behavioral 
implementation research of the last 40 years, 
grouped into two categories labeled the Early Years 
and the Middle Ages. This is followed by a 
discussion of the strengths and weaknesses of past 
research on implementation, as well as three 
3
C.-S. Chen et al. (Eds.), Enterprise Information Systems VII, 3–10.
© 2006 Springer.

suggestions for improving the current state of 
research on this topic. Finally, the paper presents a 
summary of a presently ongoing research project as 
an example of one way in which two of these 
suggestions were operationalized.  
2 BEHAVIORAL 
IMPLEMENTATION
RESEARCH: THE EARLY 
YEARS (1970S) 
The beginnings of behavioral IS implementation 
research can be traced to the 1960s and the early 
studies that investigated the implementation failures 
of operations research and management science 
(OR/MS) solutions in organizations (Schultz and 
Slevin 1975). The main impetus for this line of 
research was the fact that many OR/MS solutions 
proposed by OR/MS researchers and practitioners 
were not being adopted or used by managers. In an 
effort to explain why managers were not interested 
in adopting solutions that seemingly could help 
organizations operate more efficiently, researchers 
began to identify and investigate the factors that 
influenced outcome variables such as adoption and 
use of OR/MS recommendations (Schultz and Slevin 
1975). As most implementations of OR/MS 
solutions required the use of computers and entailed 
extensive programming, implementations of OR/MS 
solutions and computer-based information systems 
had many parallels and shared similar difficulties. 
What later came to be known as “factor studies” of 
IS implementations (Lucas 1981) were essentially an 
outgrowth 
of 
factor 
studies 
in 
OR/MS 
implementation. 
The 
theoretical 
foundations 
of 
many 
IS 
implementation factor studies of the late 1960s and 
early 1970s can be traced to Churchman and 
Schainblatt (1965) who identified a lack of 
understanding 
between 
managers 
and 
the 
implementers (or the researchers) as a root cause of 
many implementation problems, and to Ackoff 
(1967) who identified the assumptions and myths 
surrounding these implementations as a root cause of 
implementation failures. As a result, early IS 
implementation studies typically tried to identify the 
factors that significantly influenced managers’ 
reactions to the introduction of IS in organizations 
and investigate their impact on outcomes such as 
system use, satisfaction, and system quality. Largely 
based on case studies and questionnaire-based field 
studies, a key practical objective of these studies was 
to provide recommendations regarding how to 
manipulate 
different 
factors, 
such 
as 
user 
participation and user training, so as to achieve 
greater implementation success.  
While no comprehensive study has examined the 
legacy of the early years of IS implementation 
research, three factors identified in that era stand out 
in terms of the consensus that has existed over the 
years regarding their importance. These factors are, 
top management support (Ginzberg 1981; Lucas 
1981; Thong et al. 1996), user participation 
(Hartwick and Barki 2001; Ives and Olson 1984), 
and user training (Olfman and Pitsatorn 2000). 
While 
the 
general 
consensus 
regarding 
the 
significant 
influence 
these 
factors 
have 
on 
implementation success (however defined) has been 
relatively constant, even today these factors remain 
as disconnected elements with no meaningful 
theoretical linkages between them. In part as a 
response to the largely atheoretical nature of the 
implementation studies of the 1970s, beginning with 
the 1980s many researchers sought to study 
implementation phenomena by more strongly 
grounding their research in theory. However, given 
the dearth of theories in the IS field, and heeding the 
calls of senior researchers (Dickson et al. 1982), 
they borrowed established theoretical frameworks 
and models from reference disciplines. 
3 BEHAVIORAL 
IMPLEMENTATION
RESEARCH: THE MIDDLE 
AGES (1980 TO NOW) 
In the late 1970s and early 1980s, the focus of 
implementation research began to shift from the 
study of the development and installation of 
transaction processing applications in organizations 
to the study of issues that were faced with the 
introduction of different system types, such as 
management information systems, decision support 
systems, and expert systems in the 1980s. Later, in 
the 1990s, the objects of implementation studies 
became more recently created technologies such as 
Case tools, EDI, data warehousing, and MRP/ERP 
systems. Note that, while the IT types being studied 
did change over the years, the implementation 
problems experienced had not: high project costs 
and scheduling delays, low usage and satisfaction 
4
Henri Barki 

levels continued to plague most implementation 
projects (Kirsch 2000; Saga and Zmud 1994).  
To address these recurrent IS implementation 
concerns, and to base their inquiries on solid 
theoretical footing, researchers borrowed existing 
theories from related disciplines, and applied them 
to the IT implementation context. One such theory 
that 
has 
had 
considerable 
influence 
on 
implementation research is the Theory of Reasoned 
Action (Fishbein and Ajzen 1975) and its derivative, 
the Theory of Planned Behavior (Ajzen 1991). TRA 
and TPB were modified to better fit IT contexts via 
the Technology Acceptance Model (TAM) (Davis et 
al. 1989) and later via UTAUT (Venkatesh et al. 
2003), with TAM being studied by a large number 
of researchers (Lee et al. 2003). Other theories that 
have been borrowed include Social Cognitive 
Theory (Bandura 1977) which was employed to 
study the impact of self-efficacy on individual 
behaviors, Diffusion of Innovations (Rogers 1983) 
and Media Richness Theory (Daft et al. 1987) which 
helped investigate the characteristics of IT that 
influenced 
adoption 
decisions 
and 
behaviors, 
Information Processing Theory (Galbraith 1974; 
1977) and Control Theory (Ouchi 1979; Kirsch 
1996) which were used to explain implementation 
outcomes at the project and organizational levels. 
In addition to grounding their studies on stronger 
theoretical foundations, IS researchers also began to 
recognize 
and 
increasingly 
adopt 
different 
epistemologies and methodologies (Orlikowski and 
Baroudi 1991; Walsham 1995). As noted by Markus 
and Robey (1988), IS implementation phenomena 
can be examined with at least three different visions 
which they labeled technological imperative (a 
deterministic view of technology and its impacts), 
organizational imperative (a contingent view of 
technology whereby its organizational impacts 
depend on what humans do with it), and emergent 
(an interaction view according to which it is the 
mutual interaction of the technology and its context 
that leads to difficult to predict impacts). An 
increasing number of IS researchers thus began to 
study 
implementation 
phenomena 
via 
such 
alternative research paradigms, using both variance 
and process approaches. 
From a theoretical standpoint, the efforts of IS 
implementation researchers of the last 25 years has 
resulted in the development of several theoretical 
models. Investigating the antecedents of users’ 
technology acceptance and usage behaviors has 
attracted a lot of research effort (Agarwal 2000). 
Originally derived from the Theory of Reasoned 
Action (Fishbein and Ajzen 1975) and its later 
version, the Theory of Planned Behavior (Ajzen 
1991), these efforts have led to a variety of research 
models which have recently been integrated in 
UTAUT (Venkatesh et al. 2003), proposed as a 
synthesis of past research on user acceptance. 
According to this model, perceived usefulness, 
perceived ease of use, social influences, and 
facilitating conditions are four key constructs that 
influence individuals’ usage of technology with their 
respective effects being moderated by individual 
difference variables such as experience, gender, and 
age.
In 
addition 
to 
these 
integration 
efforts, 
researchers have also tried to bridge the gap between 
different research streams by integrating research on 
technology acceptance, which typically focused on 
usage 
behaviors, 
with 
research 
investigating 
antecedents of user attitudes and satisfaction 
(Wixom and Todd 2005). Other constructs have also 
been identified as important influences on users’ 
acceptance of technology including computer self-
efficacy (Compeau and Higgins 1995), cognitive 
absorption (Agarwal and Karahanna 2000), and trust 
(Gefen et al. 2003).  
 From a practical standpoint, the findings of 
earlier studies regarding the positive impact of user 
participation, top management support, and user 
training have also been generally confirmed (Larsen 
2003). In addition, the significant positive impacts of 
having a project champion (Beath 1991; Howell and 
Higgins 1990), of adequately managing conflicts 
(Barki and Hartwick 2001; Robey et al. 1993; Smith 
and McKeen 1992), and of appropriate management 
of project risks (Barki et al. 2001) have also been 
empirically shown. 
4 TAKING STOCK AND 
SUGGESTIONS FOR THE 
FUTURE 
First, it is important to note that the constructs and 
theoretical models that have emerged in the 
implementation studies of the last 40 years are, for 
the most part, independent on the nature of the IT 
artifact being examined. While some factors may 
have greater importance in certain contexts, many of 
them such as top management support, user training, 
conflict management, and risk management are 
applicable 
to 
many 
implementation 
contexts, 
ranging from the increasingly diminishing in-house 
5
An Assessment and Suggestions for the Future 

IS development environments to the increasingly 
more popular enterprise system implementations by 
consultants. Thus, future research on implementation 
stands to benefit more from focusing on theoretical 
relationships between constructs than trying to 
reinvent success factors of each new technology that 
emerges over time. 
Second, it can be noted that the behavioral IS 
implementation studies of the last 40 years form a 
research stream that today is characterized by strong 
theoretical foundations, methodological rigor as well 
as methodological multiplicity, and findings that 
provide some useful guidelines to practitioners. 
However, despite these advances, implementation 
problems still continue to occur frequently and at 
great cost, especially in the implementation of many 
enterprise systems (Barki et al. 2005). The 
continuing nature of technology implementation 
problems suggests that, while our knowledge and 
expertise in this domain may have increased over the 
years, there is still a lot that we do not know and that 
more research needs to be done. However, this 
research needs to adopt a new stance if it is to yield 
greater insight into technology implementation 
phenomena.  
Looking at the present state of our knowledge, 
and the types of research articles that are being 
published, one can not help but wonder whether our 
current 
approaches 
to 
studying 
technology 
implementations have reached their limits. In terms 
of research methodology, both the variance and 
process approaches of studying implementation 
phenomena appear to have reached a stage where 
they seem to be stagnating. For example, when 
TAM was introduced in 1989 it provided a fresh 
start for studying usage behaviors as a form of 
system acceptance. Since then, however, a large 
number of papers have been published to test, in 
different contexts or with different IT types, the 
original TAM model or slightly modified versions of 
it that basically added one or more constructs to the 
initial model. It is interesting to note that, after 
synthesizing 
past 
research 
efforts 
on 
user 
acceptance, the integrative UTAUT model proposed 
by Venkatesh et al. (2003) is not very different from 
Ajzen’s TPB. Thus, after 17 years of research on 
user acceptance, we seem to have essentially 
returned back to square one, which is not a very 
encouraging sign of progress.  
The introduction of Structuration Theory and 
interpretivism 
to 
IS 
implementation 
research 
(Orlikowski 1992; 1996; Orlikowski and Baroudi 
1991; Walsham 1995) provided a qualitative 
approach that enabled the study of implementation 
phenomena with greater realism and by taking all its 
complexities into account. The increased use of 
positivist case study methods (Dubé and Paré 2003) 
and other qualitative approaches have also enabled 
researchers to examine IS implementations in 
greater depth and detail (e.g., Beaudry and 
Pinsonneault 2005; Lapointe and Rivard 2005). 
However, the lessons learned from such process 
approaches, while interesting in their own right, 
have been generally difficult to mold into theories 
that can be subjected to empirical testing. This is 
especially difficult when most researchers who 
adopt process approaches shun the adoption of 
variance approaches in their research and stay 
largely 
within 
their 
own 
theoretical 
and 
epistemological silos. Note that many variance 
researchers are guilty of the same given their 
reluctance to conducting process studies. While 
conceptually 
and 
methodologically 
more 
challenging, efforts that combine both approaches 
are also likely to help IS implementation research to 
significantly advance from its current state.  
In 
essence, 
the 
new 
stance 
that 
future 
implementation research needs to adopt includes: 1) 
Better conceptualizations of key constructs in 
variance models so that they better reflect the 
realism and richness inherent in actual IS 
implementations. A potentially useful source for 
doing so is the data and findings that are currently 
available 
in 
past 
process 
research 
on 
implementation; 2) As IS implementations involve 
actions and outcomes at individual, project, and 
organizational levels, we need to construct multi-
level theories that span all three levels. For example, 
presently, most researchers theorize within a single 
level, with same-level constructs in their research 
models. By ignoring the significant constraints and 
influences that variables at higher levels have on 
lower level constructs, such single-level approaches 
to theory building leave out an important part of 
typical implementation contexts out of the research 
equation; 3) IS implementations occur over a 
relatively long period of time during which the 
relationships between the constructs of a research 
model may vary. For example, users’ satisfaction 
with a system a year following roll out and after all 
corrections and adjustments to the system have been 
made may be different from their satisfaction right 
after go live. More importantly, different constructs 
and different model relationships may be needed to 
explain what happens in the two time periods. Until 
now, many researchers have ignored such time-
dependent 
relationships 
between 
their 
model 
6
Henri Barki 

constructs, with many of them using a static variance 
model to explore relationships between constructs 
that exist or occur at different time periods of an 
implementation. Given the potential inaccuracy and 
confound 
threats 
of 
such 
approaches, 
IS 
implementation researchers need to explicitly take 
into account the timing aspects of their model 
constructs into their future modeling efforts.  
5 A RESEARCH PROGRAM FOR 
STUDYING A MULTI-STAGE 
MODEL OF AN EXPANDED 
CONCEPTUALIZATION OF 
SYSTEM USE
This section of the paper provides an outline of an 
ongoing research program that was undertaken to 
address some of the shortcomings discussed in the 
above paragraphs. The program focuses on the 
development of a broad conceptualization and 
assessment of the system use construct, and an 
examination of its antecedents at different time 
periods in IS implementations.  
Information system use is a key dependent 
variable 
when 
assessing 
various 
information 
technology 
development 
and 
implementation 
phenomena such as IT adoption, acceptance, and 
diffusion (Agarwal 2000; Brancheau and Wetherbe 
1990; DeLone and McLean 1992; Straub et al. 
1995). The construct of individual-level IS use is 
generally operationalized in terms of frequency, 
duration, or variety of system functions used, 
providing quantitative measures that are useful in 
testing complex multivariate relationships. However, 
this approach has important limitations including 
their failure to consider the multidimensional nature 
of IS use (Doll and Torkzadeh 1998), their lack of 
relevance in mandatory use contexts (DeLone and 
McLean 1992; 2003), their difficulty in specifying 
what is "… the ideal, or at least sufficient, level of 
use for a successful or effective IS?" (Szajna 1993, 
p. 148), the fact that "Simply measuring the amount 
of time a system is used does not properly capture 
the relationship between usage and the realization of 
expected results." (DeLone and McLean 2003, p. 
16), and the fact that they miss "[...] much of the 
richness 
present 
in 
organizational 
utilization 
contexts." (Lassila and Brancheau 1999, p. 65) 
These limitations indicate that "[…] prior measures 
of system usage […] are inadequate from a 
conceptual point of view." (Straub et al. 1995, p. 
1339), and that "The problem to date has been a too 
simplistic definition of this complex variable." 
(DeLone and McLean 2003, p. 16)  
Viewing individual-level IS use as a multi-
dimensional, behavioral construct can address some 
of these shortcomings. Indeed, recently it has been 
suggested that researchers adopt a comprehensive 
conceptualization of users’ post-adoptive behaviors 
by focusing on “…factors that influence users to 
continuously exploit and extend the functionality 
built into IT applications.” (Jasperson et al. 2005, p. 
525) 
Other 
researchers 
have 
suggested 
that 
"Technology acceptance outcomes need to be 
extended to more formally include the notions of 
adaptation, reinvention, and learning" (Agarwal 
2000, p.102) Similarly, "emergent use" during IT 
infusion has been defined as "[…] using the 
technology in order to accomplish work tasks that 
were not feasible or recognized prior to the 
application of the technology to the work system." 
(Saga and Zmud 1994, p. 80). This definition 
suggests that the concept of emergent use includes 
activities of adaptation and reinvention (Rice and 
Rogers 1980), as well as learning behaviors. These 
behaviors also constitute key behavioral components 
of the individual cognition model of the recently 
proposed 
conceptual 
model 
of 
post-adoptive 
behavior (Jasperson et al. 2005). 
Based on the above considerations, an initial 
longitudinal study was conducted using a grounded 
theory approach to examine the behaviors of twelve 
users (located in different departments of a large 
organization) and what they actually did in their 
daily routines over an 18-month period as they used 
a recently implemented ERP (Boffo 2005). Based on 
the findings of that study, users’ system usage 
behaviors were categorized into:  
a. Task oriented usage. This category of use 
behaviors includes users' interactions with an IS in 
the accomplishment of their organizational tasks. It 
includes active, hands-on employment of an IS as a 
tool 
that 
supports 
the 
performance 
of 
an 
organizational task, as well as the employment of an 
IS through one or more intermediaries.  
b. Adaptation oriented usage. This category 
includes all user behaviors directed at changing or 
modifying IT and how they will be deployed and 
used in an organization. An underlying theme of 
such behaviors is reinvention which reflects 
deliberate 
modification-oriented 
and 
creative 
activities which users of IT engage in (Johnson and 
Rice 1987; Nambisan et al. 1999; Orlikowski 1996; 
7
An Assessment and Suggestions for the Future 

Rice and Rogers 1980; Tornatzky et al. 1983; Tyre 
and Orlikowski 1994). These can be further 
categorized into (a) Technology adaptation: user 
behaviors that change an IT that has been 
implemented; (b) Operational adaptation: user 
behaviors that change the way in which an 
implemented IT operates; and (c) Organizational 
adaptation: user behaviors that change how an IT is 
used in the organization. 
c. Learning oriented usage. As they learn how to 
use a new technology, users interact with each other 
and exchange information in order to adapt to new 
ways of performing their tasks (Papa and Papa 1992; 
Papa and Tracy 1988). They also engage in self-
directed learning behaviors such as browsing or 
scanning a system (Vandenbosch and Higgins 1996). 
This type of use can be categorized into (a) 
Communication: interactions with other users or 
professionals 
for 
information 
exchange; 
(b) 
Independent 
exploration: 
information 
search 
behaviors independently undertaken by users to 
improve their knowledge and mastery of an IT that 
has been implemented.  
We think that the above categories of user 
behaviors provide a comprehensive and meaningful 
framework for conceptualizing the construct of 
individual-level IS use. By providing a more 
complete 
and 
richer 
representation 
of 
what 
individual users do in terms of system use activities 
and behaviors, this three-dimensional view of 
system use can also be useful in explaining the 
relationships between different facets of this usage 
and other important IT implementation constructs. 
For example, each facet of use can be separately 
examined with existing models of user acceptance in 
order to better explain and understand the 
antecedents of users’ adaptation and learning 
behaviors. To further explore this idea, we 
undertook a longitudinal study to test a two-stage 
UTAUT model with IS use conceptualized as the 
three-dimensional construct described above, and 
measured at two time periods: the first immediately 
after go live, and the second after system use 
becomes routinized. A key hypothesis of the study is 
that users’ task oriented use, adaptation, and learning 
behaviors in the first time period will influence their 
perceived ease of use, perceived usefulness, social 
norms, and perceived behavioral control (i.e., the 
antecedents of task oriented use in UTAUT) at time 
period two. These in turn are hypothesized to 
influence users’ task oriented use, adaptation, and 
learning behaviors in the second time period. 
As can be seen, the above research program 
attempts to address the shortcomings of past IS 
implementation research by (1) conceptualizing IS 
use with an approach that captures its inherent 
complexity and realism (a conceptualization made 
possible by combining process and variance research 
approaches), and (2) a variance model that reflects 
the time-dependent nature of the relationships 
between 
implementation 
constructs. 
While 
it 
presently does not address the multi-level aspects of 
IS implementations, efforts are also being made in 
that direction with the hope that the research 
program summarized here will provide a first step in 
the 
development 
of 
the 
next 
stage 
in IS 
implementation research.    
REFERENCES
Ackoff, R., 1967. Management Misinformation Systems. 
Management Science. 13(12), B147-B156. 
Agarwal, R., 2000. Individual Acceptance of Information 
Technologies. In R.W. Zmud (ed.), Framing the 
Domains of IT Management, Pinnaflex, Cincinnati, 
OH, 85-104. 
Agarwal, R. and E. Karahanna. 2000. Time Flies When 
You're Having Fun: Cognitive Absorption and Beliefs 
about Information Technology Usage. MIS Quarterly,
24(4), 665-694. 
Ajzen, I., 1991. The Theory of Planned Behavior. 
Organizational 
Behavior 
and 
Human 
Decision 
Processes. 50(2), 179-211.  
Bandura, A., 1977. Self-efficacy: Toward a Unifying 
Theory of Behavioral Change. Psychological Review.
84(2), 191-215. 
Barki, H., S. Oktamıú and A. Pinsonneault. 2005. 
Dimensions of ERP Implementations and Their Impact 
on 
Project 
Outcomes. 
Journal 
of 
Information 
Technology Management. 16(1), 1-9. 
 
Barki, H. and J. Hartwick. 2001. Interpersonal Conflict 
and 
Its 
Management 
in 
Information 
System 
Development. MIS Quarterly. 25(2), 195-228. 
Barki, H., S. Rivard, S. and J. Talbot. 2001. An Integrative 
Contingency 
Model 
of 
Software 
Project 
Risk 
Management. Journal of MIS. 17(4), 37-70. 
Beath, 
C.M., 
1991. 
Supporting 
the 
Information 
Technology Champion. MIS Quarterly. 15(3), 355-
372.
Beaudry, A. and A. Pinsonneault. 2005. Understanding 
User Responses to IT: A User Adaptation Coping Acts 
Model. MIS Quarterly. 29(3), 493-524. 
Boffo, C., 2005. L’évolution des pratiques individuelles 
d’utilisation d’un système ERP : Comment se fait 
l’appropriation d’un changement technologique. Ph.D. 
thesis, HEC Montréal. 
Brancheau, J.C. and J.C. Wetherbe. 1990. The Adoption 
of Spreadsheet Software: Testing Innovation Diffusion 
8
Henri Barki 

Theory in the Context of End-User Computing. 
Information Systems Research. 1(2), 115-144. 
Compeau, D.R., and C.A. Higgins. 1995. Computer Self-
efficacy: Development of a Measure and Initial Test. 
MIS Quarterly. 19(2), 189-212. 
Cooper, R.B. and R.W. Zmud. 1990. Information 
Technology 
Implementation 
Research: 
A 
Technological 
Diffusion 
Approach. 
Management 
Science. 36(2), 123-139. 
Churchman, C.W. and A.H. Schainblatt. 1967. The 
Researcher 
and 
the 
Manager: 
A 
Dialectic of Implementation. Management Science.
11(4), B69-87. 
Daft, R. L., R. H. Lengel and L. Trevino. 1987. Message 
Equivocality, 
Media 
Selection, 
and 
Manager 
Performance. MIS Quarterly. 11(3), 355-366. 
Davis, F.D., R.P. Bagozzi and P.R. Warshaw. 1989. User 
Acceptance of Computer Technology: A Comparison 
of Two Theoretical Models. Management Science.
35(8), 982-1003. 
DeLone, W.H. and E. McLean. 1992. Information Systems 
Success: The Quest for the Dependent Variable. 
Information Systems Research. 3(1), 60-95. 
DeLone, W.H. and E. McLean. 2003. The DeLone and 
McLean Model of Information Systems Success: A Ten 
Year Update. Journal of Management Information 
Systems. 19(4), 9-30. 
Dickson, G.W., I. Benbasat and W. King. 1982. The MIS 
Area: 
Problems, 
Challenges, 
and 
Opportunities. 
Database. 14(1), 7-13. 
Doll, W.J. and G. Torkzadeh. 1998. Developing a 
Multidimensional Measure of System-Use in an 
Organizational Context. Information & Management.
33, 171-185. 
Dubé, L. and G. Paré. 2003. Rigor in IS Positivist Case 
Research: 
Currents 
Practices, 
Trends, 
and 
Recommendations. MIS Quarterly. 27(4), 597-635. 
Fishbein, M. and I. Ajzen. 1975. Attitude, Intention and 
Behavior: An Introduction to Theory and Research,
Addison-Wesley, Reading, MA.. 
Galbraith, 
J.R., 
1974. 
Organizational 
Design: 
An 
Information Processing View. Interfaces. 4(3), 28-36. 
Galbraith, J. R., 1977. Organization Design, Addison-
Wesley. 
Gefen, D., E. Karahanna and D.W. Straub. 2003. Trust and 
TAM in Online Shopping: An Integrated Model. MIS 
Quarterly. 27(1), 51-90. 
Ginzberg, M.J., 1981. Key Recurrent Issues in the MIS 
Implementation Process. MIS Quarterly. 5(2), 47-59. 
Hartwick, J. and H. Barki. 2001. Communication as a 
Dimension of User Participation. IEEE Transactions on 
Professional Communication. 44(1), 21-36. 
 
Hevner, A.R., S.T. March, J. Park, and S. Ram. 2004. 
Design Science in Information Systems Research. MIS
Quarterly. 28(1), 75-106. 
Howell, J.M., and C.A. Higgins. 1990. Champions of 
Change: Identifying, Understanding, and Supporting 
Champions 
of 
Technological 
Innovations. 
Organizational Dynamics. 40-55. 
Ives, B. and M.H. Olson. 1984. User Involvement and MIS 
Success:  A Review of Research. Management Science.
30(5), 586-603. 
Jasperson, J., P. E. Carter and R.W. Zmud. 2005. A 
Comprehensive Conceptualization of Post-Adoptive 
Behaviors Associated with Information Technology 
Enabled Work Systems. MIS Quarterly. 29(3), 525-
557.
Johnson, 
B.M. 
and 
R.E. 
Rice. 
1987. 
Managing
Organizational Innovation: The Evolution From Word 
Processing to Office Information Systems, Columbia 
University Press, New York, NY. 
Kirsch, L.J., 1996. The Management of Complex Tasks in 
Organizations: Controlling the Systems Development 
Process. Organization Science. 7(1), 1-21. 
Kirsch, L.J., 2000. Software Project Management: An 
Integrated Perspective for an Emerging Paradigm. In 
Framing the Domains of IT Management, R.W. Zmud 
(ed.), Cincinnati, OH, Pinnaflex, 285-304. 
Lapointe, L. and S. Rivard. 2005. A Multilevel Model of 
Resistance to Information Technology Implementation. 
MIS Quarterly. 29(3), 461-491. 
Larsen, K.R.T., 2003. A Taxonomy of Antecedents of 
Information Systems Success: Variable Analysis 
Studies. Journal of Management Information Systems.
20(2), 169-246. 
Lassila, K.S. and J.C. Brancheau. 1999. Adoption and 
Utilization 
of 
Commercial 
Software 
Packages: 
Exploring Utilization Equilibria, Transitions, Triggers, 
and Tracks. Journal of Management Information 
Systems. 16(2), 63-90. 
Lee, Y., K.A. Kozar and K.R.T. Larsen. 2003. The 
Technology Acceptance Model: Past, Present, and the 
Future. Communications of the AIS. 12, 752-780. 
Lucas, H., 1981. Implementation: The Key to Successful 
Information Systems, Columbia University Press, New 
York. 
Markus, L.M. and D. Robey. 1988. Information Technology 
and Organizational Change: Causal Structure in Theory 
and Research. Management Science. 34(5), 583-598. 
Markus, L.M. et C. Tanis. 2000. The Enterprise System 
Experience—From Adoption to Success. In Framing 
the Domains of IT Management, R.W. Zmud (ed.), 
Pinnaflex, Cincinnati, OH, 173-207. 
Nambisan, S., R. Agarwal and M. Tanniru. 1999. 
Organizational Mechanisms for Enhancing User 
Innovation in Information Technology. MIS Quarterly.
23(3), 365-395. 
Olfman, L. and P. Pitsatorn. 2000. End-User Training 
Research: Status and Models for the Future. In 
Framing the Domains of IT Management, R.W. Zmud 
(ed.), Pinnaflex, Cincinnati, OH, 129-146. 
Orlikowski, W.J. and J.J. Baroudi. 1991. Studying 
Information Technology in Organizations: Research 
Approaches and Assumptions. Information Systems 
Research. 2(1), 1-28. 
Orlikowski, W.J., 1992. The Duality of Technology: 
Rethinking 
the 
Concept 
of 
Technology 
in 
Organizations. Organization Science. 3(3), 398-427. 
9
An Assessment and Suggestions for the Future 
www.ebook3000.com

Orlikowski, W.J., 1996. Improvising Organizational 
Transformation over Time: A Situated Change 
Perspective. Information Systems Research. 7(1), 63-
92.
Ouchi, W.G., 1979. A Conceptual Framework for the 
Design 
of 
Organizational 
Control 
Mechanisms. 
Management Science. 25(9), 833-848. 
Papa, W.H. and M.J. Papa. 1992. Communication Network 
Patterns and the Re-Invention of New Technology. 
Journal of Business Communication. 29(1), 41-61.
Papa, W.H. and K. Tracy. 1988. Communicative Indices of 
Employee with New Technology. Communication 
Research. 15, 524-544.
Rice, R.E. and E.M. Rogers. 1980. Reinvention in the 
Innovation Process. Knowledge: Creation, Diffusion, 
Utilization. 1(4), 499-514. 
Robey, D., L.A. Smith and L.R. Vijayasarathy. 1993. 
Perceptions of Conflict and Success in Information 
Systems 
Development 
Projects. 
Journal 
of 
Management Information Systems. 10(1), 123-139. 
Rogers, E.M., 2003. Diffusion of Innovations. Free Press, 
New York,  NY, 5th edition. 
Saga, V. and R.W. Zmud. 1984. The Nature and 
Determinants of IT Acceptance, Routinization and 
Infusion. In Diffusion, Transfer and Implementation of 
Information Technology, L. Levine (ed.), North 
Holland, New York, NY, 67-86. 
Schultz, R.L. and D.P. Slevin (eds.). 1975. Implementing 
Operations Research/Management Science, Elsevier, 
New York, NY. 
Smith, H.A. and J.D. McKeen. 1992. Computerization and 
Management: A Study of Conflict and Change. 
Information & Management. 22, 53-64. 
Straub, D.W., M. Limayem, and E. Karahanna-Evaristo. 
1995. Measuring System Usage: Implications for IS 
Theory Testing. Management Science. 41(8), 1328-
1342.
Swanson, E.B. and N.C. Ramiller. 2004. Innovating 
Mindfully 
with 
Information 
Technology. 
MIS
Quarterly. 28(4), 553-583. 
Szajna, B., 1993. Determining Information System Usage: 
Some 
Issues 
and 
Examples. 
Information 
& 
Management.  25, 147-154. 
Thong, J.Y.L., C.S. Yap and K.S. Raman. 1996. Top 
Management Support, External Expertise and IS 
Implementation in Small Businesses. Information 
Systems Research. 7(2), 248-267. 
Tornatzky, L.G., J.D. Eveland, M.G. Boylan, W.A. 
Hetzner, E.C. Johnson, D. Roitman, and J. Schneider. 
1983.
The Process of Technological Innovation: 
Reviewing 
the 
Literature, 
National 
Science 
Foundation, Washington, DC. 
Tyre, M. J. and W.J. Orlikowski. 1994. Windows of 
Opportunity: Temporal Patterns of Technological 
Adaptation in Organizations. Organization Science.
5(1), 98-118. 
Vandenbosch, B. and C.H. Higgins. 1996. Information 
Acquisition and Mental Models: An Investigation into 
the Relationship Between Behavior and Learning. 
Information Systems Research. 7(2), 198-214.  
Venkatesh, V., M.G. Morris, G.B. Davis and F.D. Davis. 
2003. User Acceptance of Information Technology: 
Toward a Unified View. MIS Quarterly. 27(3), 425-478. 
Walsham, G., 1995. The Emergence of Interpretivism in IS 
Research. Information Systems Research. 6(4), 376-394. 
Wixom, B.H. and P.A. Todd. 2005. A Theoretical 
Integration of User Satisfaction and Technology 
Acceptance. Information Systems Research. 16(1), 85-
102.
10
Henri Barki 

CHANGING THE WAY THE ENTERPRISE WORKS 
Operational Transformations 
Thomas J. Greene 
Computer Science & Artificial Intelligence Laboratory,(CSAIL) at MIT, 32 Vassar Street, Cambridge.MA, USA 
E-mail: tjg@csail.mit.edu 
Keywords: 
information revolution, enterprise, operational transformation. 
Abstract: 
The communication and information revolution is caused by a  fast changing sets of technologies that have 
already caused changes in the enterprise. Furthermore the expectations of the "customers" of the enterprise 
have changed because of personal use of the internet and web. Customers expect the Time of response for 
any transaction to be instantaneous. Managing the pace of change is today’s big enterprise problem. The 
technologies of computers, networks, software that enable very fast response are complex and themselves 
fast changing .To use the new versions of technologies requires both learning new skills and changing 
internal operational procedures. Operational Transformation is the next frontier of business advantage. 
Because of global competition in uncertain times, Any enterprise of any size must be configured to change; 
change the way they conduct business and change basic operations.  Failure to do this will mean losing to 
competitors who do change. These issues will be examined and a possible solution to the problem offered. 
1 INTRODUCTION 
We can all agree that this new millennium is already 
characterized by change both on a human level and a 
technology level.  The information explosion 
continues at a forever-increasing pace. In order for 
our enterprise systems to be successful in this ever-
changing world; we need to look at our systems in a 
different and more flexible manner.  In the following 
presentation, I would like, with you, to explore what 
the problems of this accelerated change may be; 
where we are now in our design of systems; and 
where we need to be in the near future. 
During this presentation we will consider the 
following: 
x
A 
short 
history 
of 
people 
and 
technology
x
Some 
abstractions 
for 
Enterprise 
Managers  
x
The Forces causing faster change 
x
Why your Models need both open 
design and event  monitors  
x
Why the enterprise must be nimble in 
accessing updates and Changes. 
I first spoke at an ICEIS conference in 1999 and 
at that time I spoke of the changes that were 
occurring, but even then I could not predict the 
intensity of those changes and the pace at which they 
would occur.  Let us begin by looking at a short 
history of people and technology. 
1.1
People Growth 
In the time of Julius Caesar (35 BCE) there were 
world.  Now in 2000 A.D. the population of the 
occur in a straight line or in an even distribution. 
It took until 1800 for the first billion people then 
from that time to the year 2000, the population has 
increase to the present population of 6 billion, a 
mere 200 years for the earths population to increase 
times 6.  When we look at the distribution of the 
population, we see that the US, Canada, and Western 
Europe have only 17% of the population.  Asia has 
61%, Latin America; 9%, and Africa; 13%. 
11
C.-S. Chen et al. (Eds.), Enterprise Information Systems VII, 11–17.
© 2006 Springer.
said to be a total of 150 million people living in the 
world is 6 billion people.  This increase did not 

Figure 1: Billions of people. 
At a critical time during this population boom, it 
was felt that this world would not be able to sustain 
this growth because of an inability to feed these 
numbers.  It was because of a scientific discovery 
that enabled a substantial increase in the amount of 
grains that could be grown, that this dire prediction 
did not become a reality.  Even with that we 
continue to live in a world were millions go hungry.  
In Sept 2002, an article appeared in the HBR on 
Serving the World’s Poor, Profitably. A novel model 
proposed by an enterprise to respond to the ever-
growing problem of world hunger, while continuing 
to consider the necessity of profitability of the 
enterprise.
1.2
Technology Growth 
Figure 2: Technology growth. 
Along with the population growth and change, we 
have also seen consistent change in the rate of 
growth of technology.  In 3000 BCE, we see the 
abacus being used for calculations.  A number of 
years ago they used to have a competition between 
someone using an abacus and someone using a 
calculator to ascertain the amount of time it took for 
numerical calculation.  The abacus always won. 
In the 1400’s we saw the invention of the 
Guttenberg Press enabling the masses access to the 
written word.  The first time a radio wave was used 
for communication was with the invention of the 
telegraph in 1837 and then the telephone in 1876.  In 
1948 the first transistor was manufactured. 
The WWW initially was very successful for 
moving technical text around the world, but it wasn’t 
until 1994, when graphic capability was established 
that the Web became the force in society that it is 
today.  In 2000, we had the successful draft of the 
human genome, which will enable great strides in 
the understanding of the structure and biology of 
humankind, which could contribute to the ability to 
cure some of the most difficult diseases in the world. 
As we can see then, the pace of change in 
population and technology has been extremely fast 
and continues to accelerate at an every increasing 
pace today and in the future. The amount of 
information available to us at this time is often 
overwhelming, so today’s enterprises must find 
ways to access and use this information and 
technology to promote growth. 
2
ABSTRACTIONS TO MANAGE 
COMPLEXITY
Let us now look at some abstractions for enterprise 
managers, which will assist in understanding the rate 
of technological change and respond.  
2.1
The Phase Change Metaphor 
When we look at information space, let us consider a 
water molecule as metaphor.  A water molecule can 
exist in different phases, and as it changes phases its 
world becomes very different.  Suppose this 
molecule is in its solid state.  His neighbours are 
constant and easily found.  When this molecule finds 
itself in a liquid stage, he finds that he still has 
neighbors, but they are dynamic and many other 
molecule are flowing by.  In the gaseous state, the 
molecule finds no neighbors, and other molecules 
are moving away in all directions.  That is where we 
find ourselves at the present time; in a gaseous state. 
We all now Moore’s law that states the rate of 
information doubles every two years.  Since the 
inception of the network, information through the 
network is doubling every 18 months.  The speed of 
travel of information has also evolved over time 
from the use of the spoken word; then the ability to 
12
Thomas J. Greene 

write and send letters; then the move to the use of 
the horse and then phone which allowed us to 
transmit information at the speed of sound (600mph)  
Now with the use of the internet, we send 
information by the speed of light (186,000mps). 
2.2
Change as Transforms 
Perhaps thinking of the transformation being caused 
by transforms. The abstract state can be any set of 
properties or objects, e.g. color, letters, people.   
Let us say that A=blue, or 50 lbs or 10 men and 
B=red, 20 lbs and 15 women.   
A process has occurred that changed A to B.  
This process may involve ideas, people, machines, 
or paintbrushes.  Call T.  That is B = Top A.  Some 
us may consider this as Hibert space or Linear 
Algebra.  However you define it; it is just a powerful 
picture. 
2.3
Business Process Management
Figure 3: Business Process Management. 
The processes we are concerned with in the business 
enterprise has its roots in the:
”
process managed real-
time enterprise”, that is you buy an item, add value, 
and sell it at a profit. Business Process Management 
takes the islands of knowledge,, data, business rules 
and applications that represent core enterprise 
activities and unites then into an information system 
that is accessable to ordinary business people to help 
them get work done. This is accomplished through a 
process-oriented architecture which begins with 
information services that moves to integration  and 
interaction services which become collaboration 
services.  These processes must be scalable in real 
time and agile. 
The future is in connecting across industries to 
provide 
new 
services 
from 
complementary 
enterprises.
Figure 4: Process Oriented architecture. 
3
FORCES CAUSING 
ENTERPRISE CHANGES 
What are some of the forces that are causing the 
acceleration of change. The big buzz word at the end 
of the last century and beginning of this century has 
been globalization.  We became distracted from this 
process of globalization by a number of events in the 
beginning of this millennium.  One was the famous 
dot.com “bubble”. Innovation and investment  
moved at very rapid pace and some companies did 
not have substancial products and customers..  Start 
up companies were popping up everywhere and a lot 
of people were making a lot of money.  A university 
administer was quoted as having said.” There will be 
no classes today, the faculty has all called in rich!”.  
Then the “bubble” burst and the energy needed to 
recover from that was deflected away from the 
development of globalization.  Another factor was 
“9/11” which brought the entire world to somewhat 
of a standstill in which the global economy and our 
concepts of being citizens of the world was truly 
shaken.  Following that, there was a breakdown of 
trust in large corporations resulting from the 
scandals in the operation of big business. 
However, 
at 
the 
same 
time, 
the 
interconnectedness that continued growing because 
of the network and the WWW enabled information 
to flow across all boundries of geography and 
language. 
13
Changing the Way the Enterprise Works 

3.1
Unconstrained Barriers 
Distance, Language, Time 
You were now able to innovate without emigrating.  
Distance is no longer an issue; language is no longer 
an issue, and design and innovation cant occur 
anywhere and be instantly available for use 
everywhere.  On the other side of that, any economic 
incident, legislation, strike , or medical discovery 
effect 
your 
Enterprise 
both 
positively 
and 
negatively. 
In recent times we can see how information 
flows freely and instantly around the Globe.   
We were witness to the last days of the Pope and 
were present outside the Sistine Chapel waiting for 
the results of the election.   
The relief effort during the Tsunami disaster was 
able to be immediately set in motion as we watched 
in real time a tragedy of epic proportion unfold.   
The events of 9/11 galvanized the world.   
We follow elections in Britain, Iraq, and 
Palestine, because they have global impact  
                       
Technology has made us into a global 
community and we must evolve our systems to 
reflect that reality. 
  Recently Thomas Friedman has published a 
book the states that Columbus was wrong…the 
world is not round, but the world, in respects to the 
information space at least,  is flat…and becoming 
flatter.
3.2
The Big Changes 
Let us look at some of the realities of today’s world 
that support that concept of a FLAT WORLD.    
China and India are now on-line.  Remember back to 
the percentages of world populations with Asia 
having 61% of the population.  These countries as 
they enter the cyber world in a big way, will require 
new systems and increased resources;  The concern 
of China over the growing economy of Taiwan will 
only grow.  Technology is going to continue to grow 
and change the face of the world as we know it, and 
enterprises will have to react in a proactive way or 
they will not survive. The world’s consumers will 
continue to look for maximum value in a cheaper, 
better product.  And they will expect it at an ever-
faster rate. 
Other factors that support the concept of the 
FLAT EARTH are becoming more and more 
evident.   
Language barriers are down.  Computer systems 
provide translation in multiple languages at the 
touch of a mouse.  
Figure 5: The Google translator. 
Translations of small and large amounts of text  
are easily done. The languages now offered at 
Google 
include 
Spanish, 
French, 
German, 
Portuguese, Italian, and Korean, Japanese and 
Chinese. Native speakers tell me that there is room 
for improvement in the quality of translations, but a 
quick peak into the document the scholar or curious 
layman can reveal whether the material is relevant 
and worth pursuing. This is a very different world 
and more languages are being added. Soon Russian 
will be available. 
When I was a graduate student in the 1960’s the 
translation time from Russian to English was done 
by the American Physical Society. The time delay 
was 6 months and many experiments were 
duplicated because the translation had not been 
available in time. Now a push of a button and the 
information is there, to first order. 
Figure 6: Some Sample Translations.
14
Thomas J. Greene 
anywhere on the Globe can directly and immediately 

3.3
Examples of Information Space 
Flatness
Information is easily assessable and can be 
excessive.  I saw an ad for a disk containing 10,000 
books for $9:00.  A whole library at my fingertips.  I 
had to have it.  I then found a disk that has 
instruction in 35 languages.  In both those examples, 
there is more information than I could  process in a 
lifetime. 
3.3.1 Everyone is a Journalist-BLOGGING 
The BLOG where people can tell the story of there 
lives and you can read it if so moved.  Recently 
BLOGGING directly effected the change in 
administration at the National    lab in Los Alamos.  
Employees were Blogging to each other regarding 
the practices of the head administrator.  The federal 
government read these blogs and the administrator 
was replaced.  Pretty powerful stuff. 
3.3.2 World Goods 
A rather strange example that I came upon recently 
was an article stating that statues of Our Lady of 
Guadalupe in Mexico were now being made in 
China.
We all know that new bad word, “Outsourcing”. 
Your ‘help’ desk may be anywhere on the globe, and 
your new payroll program may come from India. 
3.3.3 University Courses 
A University education is free and instantly 
accessible. Certification is still the issue , but the 
information is accessible MIT began putting it’s 
courses on line 5 years ago.  That provides everyone 
with being able to access the information and use it 
in any way they choose.  They will not receive any 
certification for that learning, but the material is 
there for self study by all.  Let us consider this again 
later
3.3.4 Personal Effects 
People can make direct phone calls from the 
Amazon River .On a more personal note, my energy 
bill has doubled in 12 months because of situations 
across the world over which I have no control., such 
immediately and directly connected to my daily life. 
4
OPEN INFORMATION 
The trend toward information openness is also a 
challenge in dealing with the level of available 
information.  
4.1
Open Courses at MIT OCW 
We have already mentioned the fact that MIT has 
made available Open Course Ware about  five years 
ago. At present four new universities are offering 
their own Open Course Ware. 
Figure 7: MIT’s Open Course Ware project.
The traffic on the MIT OSW website shows not 
only a frequency in the millions, but also a 
distribution of hits world-wide. 
4.2
Open Software – LINUX Plus 
More that 25 years ago the concept of Open Source 
Ware was being developed with the Emacs project.  
The term used to label the openness of the source 
ware was that academics held this material by 
“copyleft” instead of “copy right” and the material 
was held in the public domain.  This enabled other 
users to actually change the software to better 
implement it to their needs and the only restriction 
was that they must give away their improved 
version, and reference the original. 
4.3
DSpace - Open University 
Research
More recently there has been a move towards the 
concepts of “creative commons” and “science 
commons”.  Basically this would enable the 
Academy to keep the copyright on materials and 
keep them in the public domain.  This would enable 
15
Changing the Way the Enterprise Works 
global events as Wars, weather, elections, etc. are 

the public to use these materials for both creative 
and scientific endeavors.  In the past ten years the 
cost of professional journals has doubled.  In 
response to this, the concept of DSPACE has been 
developed.  The DSPACE  Federation  would 
coordinate planning, development, research and 
distribution of DSPACE  in an open source digital 
repository.  They also encourage digital archiving to 
provide open access to scientific literature.   
Figure 8: DSPACE.
5
THE ENTERPRISE -
CHALLENGED
I hope it has become evident to you as we went 
through these material that there are many 
challenges to the Enterprise to enable it to remain 
relevant and timely in new developments. Some of 
Extreme competition 
Globalization  
Rapidly changing technologies 
Forces beyond our control (world events are now 
directly coupled with your activities) 
The Enterprise response must be a flexible, 
nimble, continuous self-educating, new models. 
These 
models 
now 
need 
world 
monitoring 
capabilities and open design to be nimble to keep 
with up new developments and uncontrollable 
An article in The McKinsey Quarterly (23 May 
2005) described what the responses to these 
challenges must be: 
“Established companies should brace themselves 
for a future of extreme competition which may make 
the pressures of the 1980s and the 1990s look tame 
in comparison. Incumbents must understand how 
powerful forces are aggregating the once-distant 
product and geographic markets, enhancing market-
clearing efficiency, and increasing specialization in 
the supply chain.  They should respond by adopting  
a new approach strategy-one that combines speed, 
openness, flexibility, and forward focused thinking.   
Mature companies must learn to be young at 
heart. Boundless new opportunities await executives 
who recognize that days of slow change are over.” 
Again the responses must be a flexible, nimble, 
continuous self- educating, new models. These 
model now need  world monitoring capabilities for 
timely updates.  The enterprise  must create almost 
“real time” operational changes or they will cease to 
exist. 
Every local working enterprise then must be 
“global in thinking”.  An example of a 21st century 
value network is a project called GLORIAD.  Here, 
the 
network 
is 
the 
value-delivery 
system.  
GLORIAD is the first optical fiber network research 
and education network encircling the globe. 
5.1
Across the Generations 
The enterprise must not only integrate its efforts 
across the space of the globe, but also across the 
time of the ages in its workforce. The accelerating 
change effects we have seen above have had 
significant effects on the generations that are now in 
our work force.  In a recent article in the student 
paper TECH TALK (2 Feb 2005) results of a study 
were given. “The generations at work” found that 
there are four co-existing generations in the 
workplace today.  They divided the workforce into 
four groups: 
x
“Matures” born between (1909 and 
1945) 
– 
Matures 
are 
the 
silent 
generation.  They value sacrifice, 
commitment, and financial and social 
conservatism.  They remember the 
depression.  They are the establishment. 
x
“Boomers” born between (1946 and 
1964)-Boomers value themselves. They 
are competitive and anti-authority.  
They grew up with Vietnam, Watergate, 
and Woodstock. They have high 
expectations. They’re diplomatic, loyal 
and want validation. And they value 
privacy. 
16
Thomas J. Greene 
these challenges are: 
Changes. 

x
“Gen Xers” born between (1965 and 
1978). Gen Xers were the first latchkey 
kids. There entrepreneurial, pragmatic, 
straightforward.  They grew up with 
x
“Millennials” born from (1978 and 
onward)-The 
Millennials 
are 
nontraditionalists, optimistic, and very 
community 
centered. 
They 
are 
technologically adept and very busy, 
busy. They grew up with the OJ 
Simpson trial, Columbine, and 9/11. 
They are versatile and they write blogs 
about their lives.  
To the Enterprise this means that co-workers and 
customers 
may 
have 
fundamentally 
different
approaches to work, teamwork, privacy, respect for 
authority, and values., and customers are not a single 
collection of people. 
5.2
Globalization as a New 
Marketplace
When we look at the world population-income 
distribution, it shows us that a small percentage 
occupies the top of the pyramid with incomes of 
over 20, 000.  Most companies concentrate on doing 
business with the very tip of the pyramid and 
completely ignore the business potential at its base.  
What they lack in income, could be more than 
overcome by the sheer numbers-over 4 billion 
strong.
5.3
The Biggest Challenge 
Fifty percent of finding strategic advantages for the 
Enterprise is learning how to use our technologies.  
The other fifty percent is how fast the human 
component of the Enterprise can absorb change.  Not 
solving both issues can be a problem. 
In conclusion, “The World is Flat” in information 
space and the new enterprise must learn to live with 
it to be able to survive global competition. 
REFERENCES
Diamond J. (1997).  Guns, Germs and Steel: The Fates of 
Human Societies. New York, Norton,  
Friedman., Thomas L. (2000) The Lexus and the olive tree 
New York : Anchor Books,
Friedman., Thomas L. (2005) The world is flat : a brief 
history of the twenty-first century. : Waterville, Me 
Thorndike Press,
Barabasi, A-L (2002), LINKED: The
 
New Science of  
Networks, Perseus, Cambridge, MA 
Slywotzky, (2000) Adrian and  David Morrison How
digital is your business?
New York : Crown Business,  
Naisbitt., John, (1982) Megatrends : ten new directions 
transforming our lives  New York : Warner Books,  
Toffler, Alvin (1980) . The Third Wave / New York : 
Morrow, 1980. 
17
Changing the Way the Enterprise Works 
Fingar, Peter and Howard Smith (2003) , Business Process 
Management (BPM): The Third Wave 
 MIT s Open Courseware: http://ocw.mit.edu/index.html 
Google Translator: http://www.google.com/language_ 
tools?hl=en  
DSpace at MIT: http://libraries.mit.edu/dspace-mit/World 
population 
Growth: 
http://www.prb.org/content/ 
navigationMenu/PRB/Educators/Human_Population/ 
population_Growth/Population_Growth.htm  
Technology Timeline: http://www.efn.org/~peace/ past/ 
spiral/  
’
AIDS, MTV, PCs, divorce. 

ENTERPRISE ONTOLOGY – UNDERSTANDING THE ESSENCE 
OF ORGANIZATIONAL OPERATION 
Jan L. G. Dietz 
Delft University of Technology, PO Box 5031, 2600 GA Delft, The Netherlands 
j.l.g.dietz@tudelft.nl 
Abstract: 
Enterprise ontology, as defined in this paper, is the essential knowledge of the construction and the 
operation of the organization of an enterprise, completely independent of the ways in which they are 
realized and implemented. A methodology is presented for producing the ontology of an enterprise. It has a 
sound theoretical foundation, which is summarized to the extent needed for understanding the methodology. 
A small example enterprise serves to illustrate how the methodology works, and to demonstrate the practical 
usefulness of the notion of enterprise ontology. The motivation for the reported research is the apparent 
need to make societal institutions, both companies and government agencies, transparent to its customers, as 
well as to its employees and managers. 
1 INTRODUCTION 
Managing an enterprise, but also getting services 
from it as a client or collaborating with it as partner 
in a network, is nowadays far more complicated than 
it was in the past. The problems in current 
enterprises, of any kind, are well investigated and 
well documented. They are all about complexity, 
and complexity can only be mastered if one dispose 
of a comprehensive theory about the kind of things 
whose complexity one wants to master, and of 
appropriate analysis methods and techniques, based 
on that theory. The knowledge that one acquires at 
management or business schools does not suffice 
anymore. Even the gifted entrepreneur or manager 
cannot succeed anymore without a basic, systematic, 
and integral understanding of how enterprises work. 
In order to cope with the current problems and the 
future challenges, a conceptual model of the 
enterprise is needed that is coherent, comprehensive, 
consistent, concise, and essential. By coherent we 
mean that the distinguished aspect models constitute 
a logical and truly integral whole. By comprehensive
we mean that all relevant issues are covered, that the 
whole is complete. By consistent we mean that the 
aspect models are free from contradictions or 
irregularities. By concise we mean that no 
superfluous matters are contained in it, that the 
whole is compact and succinct. The most important 
property, however, is that this model is essential,
that it concerns only the essence of the enterprise, 
independent from all realization and implementation 
issues. We call such a conceptual model the 
ontology or the ontological model of the enterprise. 
The original Greek word from which the English 
word “ontology” stems, means study or knowledge 
of what is or exists. In its modern use, ontology has 
preserved this original meaning, but it has also a 
definite practical goal. It serves to provide a basis 
for the common understanding of some area of 
interest among a community of people who may not 
know each other at all, and who may have very 
different cultural backgrounds. There are various 
definitions of the modern notion of ontology getting 
around. A widely adopted definition is the one in 
(Gruber, 1995): an ontology is a formal, explicit 
specification of a shared conceptualization. We will 
call this notion of ontology, like those in (Bunge , 
1977), (Gómez-Pérez  et al., 2004), (Guarino, 1998), and 
(Meersman, 2001),
world ontology, because it 
basically describes the state space of a world or 
universe of discourse. Common examples of worlds 
are the world of traveling and the world of baking. 
The focus of a world ontology is on the core 
elements in such a world and their interrelationships. 
The notion of ontology as applied in this paper is the 
notion of system ontology (Bunge, 1979), (Dietz,
2006). Our goal is to understand the essence of the 
construction and operation of systems, more 
specifically, of enterprises. As will become clear, 
this notion of system ontology includes the notion of 
world ontology. 
19
C.-S. Chen et al. (Eds.), Enterprise Information Systems VII, 19–30.
© 2006 Springer.

A major motivation for this paper stems from our 
conviction that people are in need for transparency 
about the operation of the systems we daily work 
with, ranging from the domestic appliances to the 
big societal institutions. This need can only increase 
if one imagines a future life in a cyber culture (Bell 
and Kennedy, 2000). Let us give some examples to 
clarify the point. First, regarding technical devices, 
if you read the user manual of a video recorder or a 
computer, you become overloaded with irrelevant 
details. You mostly end up with a headache instead 
of any relevant understanding. And in case you 
persevere, there is a high chance that you will 
discover so many errors and omissions in the 
description that reading has become solving a 
puzzle. Second, have you ever phoned the help desk 
of a company or a government agency in order to get 
the service they claim to deliver? Mostly you end up 
not by having what you were looking for, but by 
being frustrated. Why? Because the operation of 
these institutions is completely opaque to you. You 
do not know what to believe and what not to believe; 
you are literally lost in cyberspace. And, in case you 
have succeeded in penetrating to the right place, 
there is a chance that the responsible person does not 
take on his or her responsibility and concludes your 
case by blaming the computer or any other thing that 
he or she uses as an aid. Most probably, he or she 
acts in this way not to hamper or frustrate you, but 
because the institution is also opaque to him or her. 
This situation should stop because it is in no 
one’s interest that it continue, as it has been in no 
one’s interest to have come this far. To the best of 
our knowledge, there has never anywhere been a 
plan to organize modern society in such a way that 
nobody is able to understand how it works. Imagine 
that it is possible for you to acquire the right amount 
of the right kind of knowledge of the operation of 
the company from which you bought something you 
want to complain about, or of the government 
agency from which you are trying to get a license 
but have not succeeded yet. In summary, imagine 
that the business processes of these enterprises have 
become transparent to you. It is the goal of this 
paper to offer a new understanding of enterprises, 
such that one is able to look through the distracting 
and confusing appearance of an enterprise right into 
its deep kernel, like an X-ray machine can let you 
look through the skin and the tissues of the body 
right into the skeleton. We will try to achieve this 
goal through a notion of ontology that includes the 
dynamic aspects of a system, and that at the same 
time does justice to the nature of enterprises. This 
nature is that enterprises are social systems, of which 
the operating principle consists of the ability of 
human beings to enter into and comply with 
commitments.
In summary, this paper introduces the notion of 
enterprise ontology, and demonstrates its practical 
usefulness by applying the DEMO1 methodology to 
an example case. In Sect. 2, the <-theory that 
underlies this methodology is briefly explained, to 
the extent that is needed for understanding the 
remainder of the paper. The example case is 
presented and analyzed, on the basis of the <-
theory, in Sect. 3. In Sect. 4, we develop the 
ontological model of the case, applying DEMO.
Sect. 5 contains the conclusions that can be drawn 
from the presented material. 
2 SUMMARY OF THE  
Ȍ-THEORY
There exist two different system notions, each with 
its own value, its own purpose, and its own type of 
model: the function-oriented or teleological and the 
construction-oriented or ontological system notion 
(Dietz, 2006), (Dietz, 1990). The teleological system
notion is about the function and the (external) 
behavior of a system. The corresponding type of 
model is the black-box model. Ideally, such a model 
is a (mathematical) relation between a set of input 
variables and a set of output variables, called the 
transfer function. Knowing the transfer function 
means knowing how the system responds to 
variations in the values of the input variables by 
changing the values of the output variables. 
Otherwise said, through manipulating the input 
variables, one is able to control the behavior. 
The ontological system notion is about the 
construction and operation of a system. The 
corresponding type of model is the white-box model,
which is a direct conceptualization of the ontological 
system definition presented below. The relationship 
with function and behavior is that the behavior is 
brought forward, and consequently explained, by the 
construction and the operation of a system; through 
this exhibiting of behavior, the function of the 
system is realized. These definitions are in 
accordance with the work of Gero et al. if one 
substitutes their use of “structure” by “construction 
and operation” (Dietz, 2006),
(Gero, 1990). The 
                                                          
1 DEMO is an acronym for “Design and Engineering 
Methodology for Organizations”. It has been applied 
successfully for over 15 years now. Visit www.demo.nl 
for more information. 
20
Jan L.G. Dietz 

ontological definition of a system, based on the one 
that is provided in (Bunge, 1979), is as follows. 
Something is a system if and only if it has the next 
properties: 
x Composition: a set of elements of some 
category 
(physical, 
biological, 
social, 
chemical etc.). 
x Environment: a set of elements of the same 
category. 
The 
composition 
and 
the 
environment are disjoint. 
x Production: the elements in the composition 
produce things (products or services) that are 
delivered to the elements in the environment. 
x Structure: a set of interaction bonds among 
the elements in the composition and between 
these and the elements in the environment. 
The teleological system notion is adequate for the 
purpose of using or controlling a system. It is 
therefore the dominant system concept in e.g. the 
social 
sciences, 
including 
the 
organizational 
sciences. For the purpose of building and changing a 
system, one needs to adopt the ontological system 
notion. It is therefore the dominant system notion in 
all engineering sciences. 
The ontological definition of an organization2 is 
that it is a system in the category of social systems. 
This means that the elements are social individuals, 
i.e. human beings in their ability of entering into and 
complying with commitments about the things that 
are produced in collaboration. The <-theory 
provides an explanation of the construction and the 
operation of organizations, regardless their particular 
kind or branch (like industry or government, or 
manufacturing or service). It is based on several 
axioms, of which the relevant ones for this paper are 
presented hereafter. 
2.1 The Operation Axiom 
An organization consists of actors (human beings 
fulfilling an actor role) who perform two kinds of 
acts. By performing production acts, the actors bring 
about the function of the organization. A production 
act (P-act for short) may be material (e.g. a 
manufacturing or transportation act) or immaterial 
(e.g. deciding, judging, diagnosing). By performing 
coordination acts (C-acts for short), actors enter into 
                                                          
2 By the business of an enterprise is meant its (external) 
function and behavior; by the organization of an 
enterprise is meant its (internal) construction and 
operation.
and comply with commitments. In doing so, they 
initiate and coordinate the execution of production 
acts. An actor role is defined as a particular, atomic 
‘amount’ of authority, viz. the authority needed to 
perform precisely one kind of production act. The 
result of successfully performing a P-act is a 
production fact or P-fact, also called production 
result. P-facts in the case Volley (see Sect. 3) are 
“membership M has started to exist” and “the fee for 
membership M is paid”. The variable M denotes an 
instance of membership. Examples of C-acts are 
requesting and promising a P-fact. 
C-
world
Actors
P-
world
COORDINATION
PRODUCTION
ACTOR ROLES
C-fact
P-fact
P-act
C-act
Figure 1: The white-box model of an organization. 
The result of successfully performing a C-act is a 
coordination fact or C-fact (e.g. the being requested 
of the production fact “membership #387 has started 
to exist”). Just as we distinguish between P-acts and 
C-acts, we also distinguish between two worlds in 
which these kinds of acts have effect: the production 
world or P-world and the coordination world or C-
world respectively (see Figure 1). At any moment, 
the C-world and the P-world are in a particular state, 
simply defined as a set of C-facts or P-facts 
respectively created up to that moment. When 
active, actors take the current state of the P-world 
and the C-world into account (indicated by the 
dotted arrows in Figure 1). C-facts serve as agenda 
for actors, which they constantly try to deal with. 
Otherwise said, actors interact by means of creating 
and dealing with C-facts. 
2.2 The Transaction Axiom 
P-acts and C-acts appear to occur in generic 
recurrent patterns, called transactions (Dietz, 2003),
(Dietz, 2006). The genericity of this pattern has 
turned out to be so omnipresent and persistent that 
we consider it to be a socionomic law. Our notion of 
transaction is to a large extent similar to the notion 
Conversation for Action in (Winograd, 1986) and to 
the notion of Workflow Loop in (Denning et al., 
1995). A transaction goes off in three phases: the 
order phase (O-phase), the execution phase (E-
phase), and the result phase (R-phase). It is carried 
through by two actors, who alternately perform acts. 
The actor who starts the transaction and eventually 
21
Enterprise Ontology – understanding the Essence of Organizational Operation 

completes it, is called the initiator or customer. The 
other one, who actually performs the production act, 
is called the executor or producer. The O-phase is a 
conversation that starts with a request by the 
customer and ends (if successfully) with a promise 
by the producer. The R-phase is a conversation that 
starts with a statement by the producer and ends (if 
successfully) with an acceptance by the customer. In 
between these two conversations there is the E-phase 
in which the producer performs the P-act. 
customer
producer
result
stated
result
accepted
result
promised
result
produced
request
desired
result
result
requested
promise
state
accept
O-phase
E-phase
R-phase
Figure 2: The basic pattern of a transaction. 
In Figure 2, we present the basic form of this 
transaction pattern. It shows that the bringing about 
of an original new, thus, ontological, production 
result (as an example: the delivery of a bouquet of 
flowers) starts with the requesting of this result by 
someone in the role of customer from someone in 
the role of producer.  The original new thing that is 
created by this act, as is the case for every 
coordination act, is a commitment. Carrying through 
a transaction is a “game” of entering into and 
complying with commitments. For example, if the 
customer issues the request, she commits herself to 
perform this act and, consequently to create the state 
“result requested”. She cannot, after having done 
this, say that she did not mean it, that she was only 
joking. Likewise, if the producer responds to the 
request with a promise, and, thus brings the 
transaction process to the state “result promised”, he 
is committed to this act. 
So, the process starts with the request for the 
bouquet by the customer, which brings the process 
to the state “result requested”, the result being the 
ownership by the customer of the desired bouquet. 
The producer responds to the state “result requested” 
by promising to bring about the desired result, which 
brings the process to the state “result promised”. 
This represents a to-do item for the producer: he has 
to comply with the promise by actually producing 
the bouquet of flowers, i.e., executing the production 
act. In the act of handing over the bouquet to the 
customer, he states that he has complied with his 
promise. The process now comes to the state “result 
stated”. The customer responds to this state by 
accepting the result as produced. This act completes 
the transaction pattern. 
The basic pattern must always be passed through 
for establishing a new P-fact. A few comments are 
in place however. First, performing a C-act does not 
necessarily mean that there is oral or written 
communication. Every (physical) act may count as a 
C-act. Second, C-acts may be performed tacitly, i.e. 
without any signs being produced. In particular the 
promise and the acceptance are often performed 
tacitly (according to the rule “no news is good 
news”). Third, next to the basic transaction pattern, 
as presented in Figure 2, two dissent patterns and 
four cancellations patterns are identified (Dietz, 
2003), (Dietz, 2006). Together with the standard 
pattern they constitute the complete transaction 
pattern. Every transaction process is some path 
through this complete pattern, and every business 
process in every organization is a connected 
collection of such transaction processes. This holds 
also for processes across organizations, like in 
supply chains and networks. This is why the 
transaction pattern must be taken as a socionomic 
law: people always and everywhere conduct 
business (of whatever kind) along this pattern. 
2.3 The Distinction Axiom 
Three human abilities play a significant role in 
performing C-acts. They are called forma, informa 
and performa respectively (Dietz, 2003). The forma
ability concerns being able to produce and perceive 
sentences. The informa ability concerns being able 
to formulate thoughts into sentences and to interpret 
sentences. The term ‘thought’ is used in the most 
general sense. It may be a fact, a wish, an emotion 
etc. The performa ability concerns being able to 
engage into commitments, either as performer or as 
addressee of a coordination act. This ability may be 
considered as the essential human ability for doing 
business (of any kind). 
Looked upon from the production side, the 
distinction levels may be understood as ‘glasses’ for 
viewing an organization (see Figure 3). Looking 
through the ontological glasses, one observes the 
business actors (B-actors), who perform P-acts that 
result in original (non-derivable) facts, and who 
directly contribute to the enterprise’s function. So, 
an ontological act is an act in which new original 
things are brought about. Deciding and judging are 
22
Jan L.G. Dietz 

typical ontological production acts. For example, the 
deciding about enrollment in Volley (see Fig. 6) is 
an ontological act. Ontological production acts and 
facts are collectively called B-things. Looking 
through the infological3 glasses, one observes 
intellectual actors (I-actors), who execute infological 
acts like deriving and computing knowledge about 
business facts. An infological production act is an 
act in which one is not concerned about the form 
but, instead, about the content of information only. 
Typical infological acts are calculating, and 
reasoning. 
As 
an 
example, 
calculating 
the 
membership fee (Fig. 6) is considered to be an 
infological act. Infological production acts and facts 
are collectively called I-things. 
B-things
B-organization
I-organization
D-organization
B-actor
I-actor
D-actor
I-things
D-things
Figure 3: Depiction of the distinction axiom. 
Looking through the datalogical glasses, one 
observes datalogical actors (D-actors), who execute 
datalogical acts like gathering, distributing, storing, 
and copying documents containing the knowledge 
mentioned above. So, a datalogical production act is 
an act in which one manipulates the form of 
information, commonly referred to as data, without 
being concerned about its content. For example, the 
act of recording an application for membership in 
the letter book (Fig. 5) is considered to be a 
datalogical act. Datalogical production acts and facts 
are collectively called D-things.
Recall that an actor is a person fulfilling an actor 
role. So, for example, a person may simultaneously 
fulfill a B-actor role, an I-actor role and a D-actor 
role: if e.g. the administrator of Volley (see Sect. 3) 
receives an application letter, he may perform some 
datalogical acts (like archiving the letter), he may 
need to perform some infological acts (like asking 
the aspirant member for missing information) and he 
will actually deal with the request by tacitly 
performing a promise. 
                                                          
3 The terms “infological” and “datalogical” are taken from 
Langefors (Langefors, 1977). The meanings we attach 
to them are similar to the original meanings. 
The distinction levels as exhibited in Figure 3 are 
an example of a layered nesting of systems (Bunge,
1979). Generally spoken, the system in some layer 
supports the system in the next higher layer. 
Conversely, the system in some layer uses the 
system in the next lower layer. So, the B-
organization uses the I- organization and the I- 
organization uses the D- organization. Conversely, 
the D- organization supports the I- organization and 
the I- organization supports the B- organization. If a 
system X supports a system Y, it means that the 
function of system X is expressed in terms of the 
construction and operation of system Y. For 
example, the actor in the B-system of the case 
Volley who registers new members, needs to know 
the age of a candidate member. This information can 
by definition only be asked for in the I- organization. 
In order to get the information, the subject who 
fulfills the B-actor role has to take the ‘shape’ of I-
actor and initiate an (infological) transaction 
resulting in the provision of the needed knowledge 
by the executor of this transaction (the I-actor who is 
the proprietor of this piece of knowledge). On his or 
her turn, this I-actor may not know the requested 
knowledge by heart and thus has to initiate, in the 
‘shape’ of D-actor, a (datalogical) transaction of 
which the executor is a D-actor who keeps record of 
the requested knowledge. A copy of the record (a 
document) is sent to the initiator who, in the shape 
of I-actor, is able to interpret the document and 
lastly, in the shape of B-actor, is able to take the 
appropriate action based on the acquired knowledge. 
What the layered nesting constitutes is an 
intrinsically solid integration of the three aspect 
organizations in the (complete) organization of an 
enterprise. The integration is solid because it builds 
on the inseparability of the human being, who 
possesses the forma, the informa, and the performa 
ability simultaneously. 
3 THE CASE VOLLEY AND ITS 
ANALYSIS 
We will explain the relevant notions, as presented in 
Sect. 2 on the basis of a small example enterprise, 
namely the activities within a tennis club regarding 
the registration of new members. The following 
exposition applies: 
One can become member of the Volley tennis club 
by sending a letter to the club by postal mail. In that 
letter one has to mention surname and first name, 
23
Enterprise Ontology – understanding the Essence of Organizational Operation 

birth date, sex, telephone number, and postal 
address (street, house number, zip code, and 
residence). Charles, the administrator of Volley, 
empties daily the mailbox and checks whether the 
information provided is complete. If not, he makes a 
telephone call to the sender in order to complete the 
data. If a letter is completed, Charles adds an 
incoming mail number and the date, records the 
letter in the letter book, and archives it. 
Every Wednesday evening, Charles takes the 
collected letters to Miranda, the Secretary of Volley. 
He also takes the member register with him. If 
Miranda decides that an applicant will become 
member of Volley, she stamps ‘new member’ on the 
letter and writes the date below it. This date is the 
commencement date of the membership. She then 
hands the letter to Charles in order to add the new 
member to the member register. This is a book with 
numbered lines. Each new member is entered on a 
new line. The line number is the member number, by 
which the new member is referenced in the 
administration. 
Next, Miranda calculates the membership fee that 
the new member has to pay for the remaining part of 
the calendar year. She finds the amount due for 
annual fees, as decided at the general meeting, on a 
piece of paper in the drawer of her desk. Then, she 
asks Charles to write down the amount in the 
member register. 
If Miranda does not allow an applicant to become 
a member (e.g., because he or she is too young or 
because the maximum number of members has been 
reached), Charles will send a letter in which he 
explains why the applicant cannot (yet) become a 
member of Volley. 
If all applications are processed, Charles takes 
the letters and the member register home and 
prepares an invoice to all new members for the 
payment of the first fee. He sends these invoices by 
postal mail. Payments have to be performed by bank 
transfers. 
As soon as a payment is received, Charles prints 
a membership card on which are mentioned the 
membership number, the commencement date, the 
name, the birth date, and the postal address. The 
card is sent to the new member by postal mail.
As the first step in revealing the ontological 
essence that lies hidden in this description, we 
produce a set of flow charts that show the described 
processes. So, we start with taking the process view. 
This is very common in practice, and very 
appropriate in this case too. The Flow Chart is an 
adequate technique for our purpose, although one 
may use the UML Activity Diagram (UML 2.0), the 
Petri Net (Jensen , 1997), or the EPC (Keller et al., 
1991) as well. What all these techniques basically 
show is the sequence of actions that are performed, 
the information that is used as input, and the 
information that is produced as output. Figures 5, 6, 
and 7 exhibit the flow charts of Volley. For the 
moment one must ignore the added texts in italics. 
Figure 4 contains the legend of the Flow Chart 
technique. The sausage-like shape in the flow charts 
presented is just a connector from one part of the 
flow chart to another. 
choice of paths
information set
action
flow direction
Figure 4: Legend of the Flow Chart. 
check info
in letter
[administrator]
Is the info
complete?
add IM nr.
and date
record application
in letter book
No
Yes
ask for
additional info
LETTER BOOK
archive
letter
archived
application
letter
begin
additional
info
[administrator]
[administrator]
[administrator]
[administrator]
receiving
application letter
[administrator]
T01/ reques t
(Note: the cor responding T01/p romis e
is performed tacitly)
infological
infological
datalogical
datalogical
datalogical
Figure 5: Flow Chart of Volley (part 1). 
24
Jan L.G. Dietz 

archived
application
letter
deciding about
enrollment
[secretary]
enrolled ?
stamp
‘new member’
and date
making
refusal letter
Yes
No
new
membership
enter new
membership
MEMBER
REGISTER
calculate
membership fee
GENERAL
MEETING
RESOLUTIONS
enter fee in
member register
end
A
refusal
letter
sending
refusal letter
[administrator]
[administrator]
[administrator]
[administrator]
[secretary]
[secretary]
MEMBER
REGISTER
T01/execute
datalogical
datalogical
datalogical
datalogical
T01/decline
infological
Figure 6: Flow Chart of Volley (part 2). 
A
make first
invoice
[administrator]
invoice for
new member
sending
invoice
B
MEMBER
REGISTER
B
enter payment in
member register
make
membership card
membership
card
sending of
membership card
end
[administrator]
[administrator]
[administrator]
[administrator]
MEMBER
REGISTER
receiving
copy of payment
[administrator]
datalogical
T02/ reques t
(Note: the cor respondin g
T02/p romise is performe d
tacitly)
T02/state
datalogical
datalogical
T01/state
(Note: the cor respondin g
T01/accept is performed
tacitly)
(Note: the cor respondin g
T02/accept is performed
tacitly)
Figure 7: Flow Chart of Volley (part 3). 
25
Enterprise Ontology – understanding the Essence of Organizational Operation 

The analysis of the case Volley is exhibited in 
Figures 5, 6, and 7, in italic texts. The operation 
axiom (see Sect. 2) tell us that the actions in the flow 
charts are either P-acts or C-acts. The distinction 
axiom adds to it that the P-acts can be classified as 
either datalogical, infological, or ontological. 
Furthermore, according to the transaction axiom, 
there appear to be two (ontological) transactions. 
One is about becoming member of the tennis club 
(let us call it T01), and the other is about paying the 
first membership fee (T02). Comparing the 
identified C-acts with the ones shown in Figure 2
reveals that the promise of T01 and the acceptance 
of T01 are missing. This is very often the case in 
practice. It does not mean that they are not 
performed (they must be for the successful carrying 
through of a transaction), but that they are performed 
tacitly. This means that an aspirant member may 
assume that Volley has promised to deal with his or 
her application if he or she does not get an explicit 
assertion of the opposite, i.e., the denial to do it. 
Likewise, Volley may assume, after having sent the 
membership card to the new member, that 
everything is okay, unless the member explicitly, 
and within a reasonable term, says that something is 
not right. A similar reasoning holds for the absence 
of the explicit performance of the promise of T02 
(by the new member) and the acceptance of T02 (by 
Volley). 
4 THE ONTOLOGICAL MODEL 
OF THE CASE VOLLEY 
The next step in arriving at the ontological model of 
Volley is to focus exclusively on the ontological 
production acts and coordination acts in the two 
identified transactions. In other words, we consider 
only the top triangle in Figure 3, the B-organization, 
containing the B-actors and the B-things. The 
ontological model of an enterprise is the model of its 
B-organization. In DEMO (Dietz, 2006),  this model 
consists of four aspect models, as depicted in Figure 
8. The aspect models will be explained along with 
developing the specific models for Volley. The most 
concise model is the Construction Model, and the 
most detailed one is the Action Model. It contains 
the 
operational 
rules 
or 
guidelines 
of 
the 
organization. The State Model specifies the state 
space and the transition space of the P-world, 
whereas the Process Model specifies the state space 
and the transition space of the C-world. Each of 
them elaborates a part of the Construction Model. 
Construction Model
Process Model
Action Model
State Model
SM
PM
AM
CM
Figure 8: The ontological aspect models. 
The basic steps in both transaction processes of 
Volley, as well as their relationships, are exhibited 
in the Process Model of Figure 9. T01/rq, T01/pm, 
T01/ex, T01/st, and T01/ac respectively stand for the 
request and the promise of T01, the execution of 
T01 (i.e., performing the P-act), and the statement 
and acceptance of T01. A similar reasoning holds for 
transaction T02. A C-act (small box) and its 
resulting C-fact (small disk) are put together in one 
combined symbol. Likewise, the P-act (small gray 
box) and the P-result (small gray diamond) are 
combined. A solid arrow is a causal link. For 
example, the being requested of T01 ‘causes’ the 
promise of T01, not as an inevitable cause-effect 
sequence, like a trigger and its effect, but as a 
possible act of actor role A01 in dealing with the 
being requested of T01. In dealing with T01/pm, two 
acts are performed: the execution of T01 and the 
request of T02. However, the actual performing of 
the execution of T01 has to wait for T02 to be 
accepted; this is the meaning of the dashed arrow 
from T02/ac to the execution symbol of T01. In this 
way, the two transactions are connected; more 
specifically, the T02 is enclosed in the T01. Because 
of the causal and conditional (waiting) relationships 
between T01 and T02, they constitute together a 
business process, the only business process in (the 
considered part of) Volley. Figure 9 shows the 
process structure of the business processes. The 
actual sequence of steps in a successful instance of 
this process is T01/rq, T01/pm, T02/rq, T02/pm, 
T02/ex, T02/st, T02/ac, T01/ex, T01/st, T01/ac. The 
only unsuccessful kind of instance that we have 
taken into account consists of T01/rq followed by 
T01/dc 
(decline). 
The 
asking 
for 
additional 
information by the administrator (see Fig. 5) is 
‘contained’ in the performing of T01/rq; every 
coordination act can ontologically succeed only if 
there is full consensus about its content. 
26
Jan L.G. Dietz 

T01
T01
rq
T01
pm
T01
ac
T01
st
T02
rq
T02
pm
T02
ac
T02
st
A01
CA01
membership
payment
membership
start
CA02
T01
dc
x
T02
Figure 9: Ontological Process Model of Volley. 
There are three distinct actor roles in Volley, 
denoted 
by 
A01, 
CA01, 
and 
CA02. 
The 
corresponding gray-lined rectangles enclose the acts 
for which each of them is authorized and 
responsible. So, for example, actor role CA01 is 
responsible for T01/rq and T01/ac. For every 
coordination step (represented by a disk in a box), 
there is an action rule that guides how the 
performing actor role should respond to the reached 
status. Below, we present all action rules for actor 
role A01. The collection of action rules for an 
enterprise is called its Action Model. The Action 
Model is another ontological aspect model of an 
enterprise, along with the Process Model (discussed 
above), the State Model, and the Construction Model 
(the latter two to be discussed shortly). 
Every action rule is enclosed by an on-no bracket 
pair. The on clause specifies the C-fact that is being 
dealt with. Conditional responses (choices) are 
represented by an if clause (enclosed in an if-fi
bracket pair). If there is more than one choice, the 
second and following ones are preceded by the 
symbol “¸”. Every choice consists of the condition, 
which is checked to be true, followed by the symbol 
“o”, followed by the action(s) to take. 
on requested T01(M) with member(new M) = P 
 
if age(P) < minimal_age or
 
#members(Volley)=maximum_number(curre
nt_year) o
decline T01(M) 
¸ age(P)  minimal_age and
 
 
#members 
(Volley) 
< 
maximum_number(current_year) o
promise T01(M) 
 
fi 
no
This action rule specifies what has to be done if 
actor role A01 has to deal with the being requested 
of a T01. The first thing to do is to generate an entity 
of type membership. The person who will be the 
member of this membership is indicated by P. 
Applications in which the current age of the member 
is less than the minimum age are declined. This will 
also happen if the current number of members of 
Volley equals the maximum number accepted. All 
other applications are responded to with a promise. 
on promised T01(M) 
request T02(M) with first_fee(M) 
no
This action rule specifies how to deal with the 
being promised of a T01, regarding a particular 
membership M. The response is requesting payment 
(T02) of the first fee for this membership. The first 
fee is a derived fact type for which the derivation 
rule is presented shortly. The other action rules are 
presented hereafter without further comment. 
on stated T02(M) 
 
if < payment is acceptible > o accept
T02(M)
¸ not < payment is acceptible > o reject
T02(M)
 
fi 
no
on accepted T02(M) 
 
execute T01 
state T01(M) 
no
The next aspect model of the complete 
ontological model of Volley is the State Model. It 
shows all information items (object classes and fact 
types) that occur in the Action Model. The State 
Model of Volley is exhibited in Figure 10. The 
diagramming technique is based on ORM (Halpin, 
2001). Fact types that are pure mathematical 
functions are not included in the diagram but listed 
as properties below the diagram. The diagram must 
be interpreted as follows. In the world of Volley 
there is an object class MEMBERSHIP and an 
object class PERSON. Instances of the last one are 
created outside the scope of Volley; therefore, the 
“roundangle” of PERSON is colored gray. Instances 
of MEMBERSHIP are created within Volley. With 
every such instance, an instance of the fact type “the
member of M is P” is associated; it states that a 
27
Enterprise Ontology – understanding the Essence of Organizational Operation 

particular person is the member in a particular 
membership. For every membership there is exactly 
one person as the member, however a person may be 
the member of more than one membership. 
There are two production fact types or result types of 
transactions, namely, PF01 and PF02. Producing a 
PF01 for a particular membership means that the 
membership then starts to exist. Producing a PF02 
means that the first fee for the membership has been 
paid. The diamond shape of these fact types 
emphasizes that they are the results of successfully 
performed transactions. 
PF02
PF01
membership M has been started
the first fee  for membership 
M  is paid
the member of M is P
P
M
MEMBER
SHIP
PERSON
property 
 
 
 
domain 
 
range
minimal_age  
 
 
VOLLEY 
 
NUMBER 
annual_fee  
 
 
VOLLEY 
 
EURO 
maximum_number  
 
VOLLEY 
 
NUMBER 
number_of members (*) 
 
VOLLEY 
 
NUMBER 
first_fee (*) 
 
 
 
MEMBERSHIP 
EURO 
date_of_birth  
 
 
PERSON 
 
DATE 
age (*) 
 
 
 
PERSON 
 
NUMBER
Figure 10: Ontological State Model of Volley. 
The object property list below the diagram shows 
four properties of Volley itself: the minimum age 
that a member must have, the annual fee that has to 
be paid, the maximum number of members that are 
allowed and the current number of members. 
Memberships have one property, namely, the first 
fee that has to be paid. Lastly, persons have two 
properties: the date of birth and the age. The state 
model contains only those object classes, fact types, 
and properties that occur in the Action Model. Some 
properties are marked by an asterisk (*); these are 
derived fact types. Their derivation rules are: 
number_of_members (Volley) = < number of 
persons for which there exists currently a 
membership > 
first_fee (M) = ((12 - Current_Month#)/12) * 
annual_fee(Current_Year) 
age(P) = < difference in years between birth year 
of P and current year > 
The last, and most comprehensive, aspect model 
is the Construction Model (Figure 11). The two 
transaction patterns in Figure 9 are, so to speak, 
‘compressed’ to only one symbol in Figure 11,
namely, the disk with a diamond in it. The actor 
roles involved are represented by a box. They are 
connected to the transaction symbol by means of a 
straight line. The producer has, in addition, a small 
black box on the junction of this line and the box 
shape. The gray-lined rectangle in the back 
represents the boundary of the enterprise under 
consideration. By convention, all environmental 
components are colored gray, and their codes start 
with a C for “composite”. So, the customer of 
transaction 
type 
T01 
is 
the 
environmental 
(composite) actor role CA01, the aspirant member 
role. Likewise, the producer of T02 is the 
environmental (composite) actor role CA02, the 
payer role. The distinction between these two roles 
illustrates that an ontological model is independent 
of the way in which it is or might be implemented: 
the person who wants to become member may fulfill 
both role CA01 and CA02, but it may also be the 
case that somebody else fulfills role CA02, and it 
may even be the case that that somebody else fulfills 
role CA01, being delegated by the actual aspirant 
member.  
CA01
A01
T01
T02
CA02
aspirant
member
payer
admitter
CPB02
membership
start
membership
payment
personal
data
CPB01
admission
rules
Figure 11: Ontological Construction Model of Volley. 
Assigning actor roles to persons is a matter of 
implementing 
the 
enterprise; 
one 
completely 
abstracts from it on the ontological level of 
understanding the enterprise. There is only one 
internal actor role, namely, A01. It is colored white 
because it is an elementary actor role. An elementary 
actor role is an atomic amount of authority and 
responsibility. It is producer of exactly one 
transaction type, and customer of zero, one, or more 
transaction types. In our case, A01 is the producer of 
T01 and the customer of T02. An actor role is 
fulfilled by a human being. If one rereads the 
description of Volley, then it appears that actor role 
A01 is fulfilled by both Miranda and Charles. This is 
a deviation from the ideal implementation of an 
actor role (being the assignment of an elementary 
actor role to one organizational function or person). 
28
Jan L.G. Dietz 

It has its price, namely, the need for a continuous 
tuning in by Miranda and Charles of the norms and 
values they apply. 
The transaction symbol actually has two 
interpretations. In addition to representing a 
transaction type of which instances are carried 
through in the enterprise, it is the combination of a 
coordination bank (the disk) and a production bank 
(the diamond). A coordination bank contains all 
coordination facts created; they allow one to monitor 
the progress of all transaction instances. A 
production bank contains all created production 
facts. Production bank PB01 (of T01) contains 
instances of the type MEMBERSHIP as well as 
instances of the fact type “the member of M is P” 
and instances of the fact type “membership M has 
been started”. Production bank PB02 (of T02) 
contains instances of the fact type “the first fee for 
membership M has been paid”. There are two 
composite production banks (CPB01 and CPB02). 
They contain facts that are needed by A01 but that 
are 
produced 
outside 
the 
enterprise 
under 
consideration. CPB01 contains instances of the 
property types “minimum age”, “annual fee”, and 
“maximum number”. CPB02 contains instances of 
the type PERSON as well as instances of the 
property type “date of birth”. The dashed lines 
indicate that actor role A01 (the only internal actor 
role) has access to the contents of these banks. As 
the producer of T01 and the customer of T02, it has 
also access to the production bank and the 
coordination bank of T01 and to those of T02. These 
access rights are completely independent on the way 
it is realized and implemented. For example, it does 
not say that actor A01 is informed by some other 
actor or that he or she has to be active himself or 
herself to get the information needed. In addition, it 
does not say by which technological means the 
information is acquired. 
5 CONCLUSIONS 
Let us assess what has been presented and discussed 
in the previous sections, and try to answer some 
emerging questions. What is exactly the difference 
between the ontological model of Volley, expressed 
in the four aspect models, and any other kind of 
model, like the flow chart? What are the benefits of 
the ontological model, and for what purposes? 
Without trying to be exhaustive, let us look at some 
striking 
matters 
and 
discuss 
their 
practical 
consequences. 
First, the ontological model of Volley is really 
and fully abstracted from the current way in which it 
operates. 
It 
does 
not 
contain 
organizational 
functions, like secretary and administrator and 
members, or references to persons (Miranda or 
Charles). It also does not contain any infological or 
datalogical things: no computing, no inquiring, no 
letter book, no register, etc. Moreover, it completely 
abstracts from the way in which the fulfillers of the 
distinguished actor roles communicate: no letters, no 
telephone calls, etc. These properties, of course, 
make the ontological model very stable. A new 
organizational structure will not change the 
ontology; nor will replacing letters by e-mails; nor 
will replacing the register book by a database, and so 
on.
Second, the ontological model of Volley shows 
things that have no explicit implementation, like the 
promise and the acceptance of T01. As you may tell 
from your own experience, these omissions are 
potential breakdowns in business processes as soon 
as changes are implemented, e.g., the replacement of 
an employee by somebody new (who does not know 
all the “no news is good news” rules that are in 
place). Put differently, the ontological model is the 
only right starting point for proposing, analyzing, 
and implementing such changes because there is 
only one thing that one has to see to while bringing 
about changes in an enterprise. It is that all parts of 
the ontological model are implemented, nothing 
more and nothing less. The ontological model also 
provides the right amount of (re)design and 
(re)engineering freedom. For example, the state 
model of Volley rightly does not contain the postal 
mail address of members. Such information is 
dependent on the particular communication channel 
one chooses, in this case the postal mail service. 
However, should we choose e-mail, we would need 
e-mail addresses instead. The ontological model 
only states in this respect that the (fulfillers of the) 
two actor roles must be able to communicate. 
Third, the four aspect models constitute a truly 
coherent set of models, based on a common 
theoretical foundation (the <-theory). Besides 
DEMO, 
there 
are 
hardly 
any 
competing 
methodologies for enterprise ontology because the 
models produced with them are not guaranteed to be 
implementation independent. In addition, the aspect 
models in most approaches do not constitute a 
complete set and are not intrinsically linked to each 
other. For example, one may add a data model  to a 
process model, but they cannot be linked together 
intrinsically because of the missing common 
29
Enterprise Ontology – understanding the Essence of Organizational Operation 

foundation; they can only be “talked” together and 
that is what often happens in practice. 
Fourth, the Process Model (Fig. 9) shows the 
structure of the (single) business process in Volley: 
transaction type T02 enclosed in transaction type 
T01. This structure is lost when using current 
process modeling approaches, such as the Flow 
Chart, the UML Activity Diagram, the Petri Net, and 
the Event Driven Process Chain. They can exhibit 
only the sequences of actions. This sequence 
however can easily be derived from a Process 
Model. For example, the sequence of steps in the 
Process Model of Volley is: 
T01/rq, T01/pm or T01/dc (which ends the 
process unsuccessfully), 
T02/rq, T02/pm, T02/ex, T02/st, T02/ac 
T01/ex, T01/st, T01/ac (which ends the process 
successfully) 
Let us make some final remarks regarding the 
(re)design and the (re)engineering of business 
processes. Clearly, (re)engineering is concerned 
with implementation. It is done for every ontological 
process step. Starting not from the ontological model 
but from an implementation dependent model, like 
the ones we mentioned, may become tedious but will 
normally not raise real problems.  (R)edesigning a 
business process, however, means removing or 
adding complete transactions. One can imagine that 
without insight into the transactional structure, such 
a redesign cannot possibly be done in a guaranteed 
proper and correct way. The only guarantee that can 
be given is that the new process is formally correct, 
as can be checked, for example, for Petri Nets. 
Obviously, that is not enough; it is also not the first 
thing to worry about. Basing the (re)design of 
business processes on one of the ‘low level’ 
modeling techniques, mentioned above, not only 
makes it a hard job, one also can easily forget things, 
namely all those coordination acts that are 
performed by non-verbal language acts or even 
tacitly.
REFERENCES
Bell, D., Kennedy, B.M., (eds): The Cybercultures 
Reader, Routledge (2000) 
Bunge, M.A.: Treatise on Basic Philosophy, vol. 3, The 
Furniture of the World, D. Reidel Publishing 
Company, Dordrecht, The Netherlands (1977) 
Bunge, M.A.: Treatise on Basic Philosophy, vol. 4, A 
World of Systems, D. Reidel Publishing Company, 
Dordrecht, The Netherlands (1979) 
Denning, P., Medina-Mora, R., Completing the loops. In: 
ORSA/TIMS Interfaces (1995) 25: 42-55 
Dietz, J.L.G.: The Atoms, Molecules and Fibers of 
Organizations.
Data and Knowledge Engineering
(2003) 47: 301–325 
Dietz, J.L.G., Generic recurrent patterns in business 
processes. In: Aalst, W. van der, Hofstede, A. ter, 
Weske, M. (eds.) Business Process Management,
Lecture Notes in Computer Science 2678, Springer-
Verlag (2003) 
Dietz, J.L.G.: Enterprise Ontology – Theory and 
Methodology, Springer-Verlag (2006) 
Dietz, J.L.G., Albani, A.: Basic notions regarding business 
processes and supporting information systems. In: 
Requirement Engineerin Journal (2005) 10: 175-183. 
Gero, J.S., Design prototypes: a knowledge representation 
schema for design. In: AI Magazine 11 (1990) 4: 26-
36
Gero, J.S., Kannengiesser, U.: The situated function-
behaviour-structure framework. In: Design Studies 25 
(2004) 4: 373-391 
Gómez-Pérez, A., Fernández-López, M., Corcho, O.: 
Ontological Engineering, 2nd edn, Springer-Verlag 
(2004)
Gruber T.: Towards Principles for the Design of 
Ontologies Used for Knowledge  Sharing. In: 
International Journal of Human-Computer Studies, 43 
Guarino N.: Formal Ontologies and Information Systems. 
In: Guarino N. (ed), Proc.  of FOIS’98, IOS Press 
(1998) 3–15 
Halpin, T.A.: Information Modeling and Relational 
Databases. Morgan Kaufmann, San Francisco (2001) 
Jensen, K, Coloured Petri Nets. Basic Concepts, Analysis 
Methods and Practical Use. Volume 1, Basic 
Concepts. In: Monographs in Theoretical Computer 
Science, Springer-Verlag (1997) 
Keller, G., Nüttgens, M., Scheer, A-W.: Semantische 
Prozessmodellierung 
auf 
der 
Grundlage 
„Ereignisgesteuerte 
Prozessketten 
(EPK)“. 
Veröffentlichung 
des 
Institut 
für 
Wirtschaftsinformatik, Paper 089, Saarbrücken (1991) 
Langefors, 
B.: 
Information 
System 
Theory. 
In: 
Information Systems 2, (1977) 207–219 
Meersman R.: Ontologies and Databases: More than a 
Fleeting Resemblance. In: d’Atri, A. & Missikoff, 
M.,(eds.), OES/SEO 2001 Rome Workshop. Luiss 
Publications (2001) 
UML 2.0 (www.uml.org) 
Winograd, T, Flores, F.: Understanding Computers and 
Cognition: A New Foundation for Design. Ablex, 
Norwood NJ (1986) 
30
Jan L.G. Dietz 
(1995) 5/6: 907–928

BUILDING SUCCESSFUL
INTERORGANIZATIONAL SYSTEMS1
IT and Change Management 
M. Lynne Markus 
Bentley College 
175 Forest Street 
Waltham, MA 02452 
mlmarkus@bentley.edu
Keywords: 
Interorganizational information systems (IOS), enterprise integration, enterprise systems (ERP systems), 
organizational change management, information technology (IT), electronic data interchange (EDI), vertical 
industry information systems standards (e.g., XML, RosettaNet), supply chains, small and medium sized 
enterprises (SMEs), business process transformation, collaboration, coordination, implementation. 
Abstract: 
As organizations look for new ways to obtain business value from their investments in enterprise systems, 
they are naturally seeking to integrate their systems externally with suppliers, customers, and other business 
partners by implementing what are often called interorganizational systems (IOS). Although many 
organizations have realized sizable benefits from IOS, it is clear that much more improvement can be 
achieved. The key to realizing additional benefits is to build IOS on the understanding that the benefits an 
organization receives from IOS depend, not only on its own implementation choices, but also on those of its 
business partners. This paper reviews the conditions promoting and inhibiting benefit realization from IOS, 
presents two examples of companies that successfully addressed those conditions, and examines why some 
organizations find it challenging to imitate these examples.  
1 GREAT POTENTIAL IOS 
BENEFITS STILL EXIST  
There is no doubt that many large companies have 
already 
received 
significant 
benefits 
from 
investments in IOS. Numerous case studies describe 
the 
strategic 
and 
operational 
benefits 
of 
interorganizational systems (IOS) and electronic 
data interchange (EDI) in companies such as 
McKesson (Clemons & Row, 1988), Baxter (Short 
& Venkatraman, 1992), Brun Passot (Jelassi & 
Figon, 1994), Chrysler (Mukhopadhyay, Kekre, & 
Kalathur, 1995), Japan Airlines (Chatfield & Bjorn-
Andersen, 1997), FedEx (Williams & Frolick, 
2001), Sabre (Christiaanse & Venkatraman, 2002), 
and Enterprise Rent-a-Car (Premkumar, Richardson, 
& Zmud, 2004). Survey research shows that IOS can 
benefit customers (Mukhopadhyay & Kekre, 2002) 
and suppliers (Lee, Clark, & Tam, 1999; Subramani, 
2004). In addition, customer-side IOS initiatives 
have been shown to lead to better financial 
performance, and supplier-side initiatives support 
IOS on the customer side (Barua, Konana, & 
Whinston, 2004).  
It is also clear that many companies have not 
achieved the full potential of IOS, because they 
generally have implemented IOS with only some of 
their partners, usually the largest ones, and even then 
only with a few partners and for relatively few 
different types of business transactions. For 
example, Segev, Porra, and Roldan (1997) noted that 
EDI has been adopted by only a tiny fraction of the 
world’s businesses. In particular, few small 
organizations have adopted EDI, usually only when 
required to do so by their partners (Iacovou, 
1 This research was funded in part by the Bentley College
Invision Project, Jane Fedorowicz, Principal Investigator
(http://www.bentleyinvision.org/). The work benefited
greatly from discussions with other Invision team
members: Jane Fedorowicz, Ulric J. Gelinas, Janis Gogan,
Amy Ray, Catherine Usoff, and Christine Williams.
Partial support for the research was also provided by the
Fulbright Foundation and Queen’s University Monieson
Centre for Knowledge-Based Enterprises.
31
C.-S. Chen et al. (Eds.), Enterprise Information Systems VII, 31–41.
© 2006 Springer.

Benbasat, & Dexter, 1995; Tuunainen, 1998). Small 
organizations generally do not benefit from using 
EDI, since 1) the costs are high, 2) they have few 
partners and transactions, so their benefits are low, 
3) they lack the knowledge as well as the resources 
to implement EDI, and 4) they often cannot reuse 
their EDI investment with other partners, because of 
the lack of standards (Chen & Williams, 1998; 
McGrath & More, 2001; Tuunainen, 1998).  
Although EDI-over-Internet is bound to increase the 
use of EDI, web-based EDI is far from an ideal 
solution and is thus unlikely to catalyze widespread 
adoption of IOS (Beck & Weitzel, 2005; Juul, 
Andersen, & Korzen-Bohr, 2004). 
Many analysts believe that the additional benefits to 
be gained from expanding the deployment of IOS 
are vast. A recent US National Institute of Standards 
and Technology (NIST) report documented huge 
opportunity costs associated with inadequate IOS in 
the automotive and electronics industries: “We 
estimate the total annual costs of inadequacies in 
supply chain infrastructures to be in excess of $5 
billion for the automotive industry, and almost $3.9 
billion for the electronics industry. These figures 
represent about 1.2% of the value of shipments in 
each industry.” (White & O'Connor, 2004, p. ES-1) 
The business benefits of well-designed IOS could 
exceed those of intraorganizational business process 
reengineering 
and 
enterprise 
systems
implementation (Champy, 2002). 
Furthermore, emerging technical and industry 
developments such as XML, web services, and 
standardization efforts like RosettaNet promise to 
reduce the costs of IOS, thereby enabling use of IOS 
to be expanding to new partners and transactions, 
enhancing IOS benefits. An increasing number of 
organizations are pursuing the goal of automating 
100% of their most important interorganizational 
transactions (e.g., forecasting, ordering, invoicing, 
and shipping notices), which would eventually 
require IOS adoption by 100% of their business 
partners possibly in several supply chain tiers. 
2 SUCCESS TODAY MEANS 
LEARNING YESTERDAY’S IOS 
LESSONS
As companies pursue the benefits of IOS expansion, 
they have much to learn from past experience with 
earlier technologies, such as EDI. Perhaps the most 
important lesson is that the benefits an organization 
achieves depend on how an IOS is implemented 
technically and organizationally, not just by the 
organization itself, but also by its partners.  
For example, IOS are believed to yield the greatest 
benefits when all interacting partners implement 
them in combination with business process changes 
(Clark & Stoddard, 1996; Lee et al., 1999) integrated 
data exchange (Truman, 2000) and dynamic process 
monitoring or exception reporting and knowledge 
management, advanced analysis, or collaboration 
capabilities (Christiaanse & Venkatraman, 2002; 
Leser, Alt, & Osterle, 2005). Often, however, the 
business partners of IOS initiators are not willing or 
able to make the necessary process changes or 
technology investments. Thus, initiators have to 
settle for partial integration with their partners. This 
substantially reduces the benefits initiators get from 
IOS (Riggins & Mukhopadhyay, 1994), because 
one-sided integration increases the likelihood of 
errors, delays, and incomplete information.  
Furthermore, because initiators’ business partners 
can vary significantly in their technological 
readiness for IOS, initiators seeking to lower their 
operating costs through increased automation often 
find themselves required to maintain multiple IOS 
channels and business processes for different groups 
of partners. For example, they might have enabled 
ERP-to-ERP system integration with some partners, 
EDI via valued added networks (VANs) with others, 
and web interfaces for their least sophisticated 
partners (Williams, Magee, & Suzuki, 1998). 
Although 
these 
initiators 
might 
still 
profit 
handsomely from IOS, they generally have much 
higher IOS maintenance costs (and therefore lower 
benefits) than they would have if they were able to 
maintain a single, standardized IOS channel and 
business process with many different partners. As an 
example of the high costs associated with 
heterogeneous IOS connections, (Williams et al., 
1998) reported that FedEx “has a separate 
department devoted to connecting with its many 
trading partners, developing translation maps, and 
making EDI transmissions ready for downloading 
into the many different mainframe and PC-based 
applications. There are also other departments that 
have technical resources that are responsible for 
determining the abilities of trading partners and 
coordinating the transition to EDI” (p. 50). This 
analysis suggests that achieving the full benefits of 
IOS requires expanding the use of IOS to as many 
32
M. Lynne Markus 

partners as possible, while minimizing the costs of 
maintaining multiple IOS channels and processes.
2.1 Shared Infrastructures 
What strategies are companies using to expand IOS 
use and benefits while keeping maintenance costs 
low? One approach is to participate in a standardized 
IOS infrastructure that is shared among the members 
of an industrial community. A private sector 
example is NEHEN (http:// www.nehen.net)2
1. The 
New England Healthcare EDI Network (NEHEN) 
was founded in 1998 to provide a collaborative 
environment within which hospitals and other 
providers can communicate with health insurers and 
health maintenance organizations in a standardized 
and cost-effective manner. Transactions supported 
range from patient eligibility requests requiring a 
real-time response to large batches of transactions 
containing remittance advices for payment for 
services rendered. NEHEN employs a peer-to-peer 
network with a server at each member site that 
translates messages from internal data formats into 
health care industry EDI standards for transmission 
via a gateway. No central data repository is 
maintained. Costs of participation are kept low 
through an “open source” policy that makes software 
interfaces developed by any member available to all 
others. Membership costs are more than offset for 
large members by the elimination of VAN 
transaction fees and multiple channels/processes for 
different partners. Smaller members benefit to a 
lesser extent owing to their smaller transaction 
volume and lower levels of automation. However, 
members of the NEHEN consortium are working to 
make the shared infrastructure even more attractive 
to smaller members by innovative pricing schemes 
and by encouraging software vendors to build 
NEHEN 
connectivity 
into 
packaged 
clinical 
information systems. Shared infrastructures can also 
be found in the public sector in the form of 
integration 
hubs 
such 
as 
Elemica
(http://www.elemica.com) in the chemicals industry. 
Once these shared arrangements become established, 
they have great potential to attract many of the 
members of an organizational community while 
minimizing members’ costs of maintaining multiple 
2 NEHEN is one of the Invision Project case studies 
(http://www.bentleyinvision.org/). Partial support for the 
research was also provided by the Fulbright Foundation 
and Queen’s University Monieson Centre for Knowledge-
Based Enterprises. 
channels and process, because if an organization 
connects once to the shared infrastructure, a large 
part of the investment required to do so can be 
reused across business partners. However, shared 
infrastructures are not likely to work in all 
organizational communities. They tend to succeed 
best where there is considerable overlap in the 
connections among community members (as is the 
case in the New England health care community and 
in the chemicals industry). Where there is little 
overlap, the value of the shared infrastructure is 
considerably lower. Furthermore, if members have 
many relationships with organizations outside the 
community, gateways to other shared infrastructures 
are required before the community infrastructure 
provides much value. 
2.2 Private Infrastructures 
Perhaps a greater barrier to the success of shared 
infrastructures lies in competitive concerns. Many 
organizations that invest heavily in IOS do not want 
to join forces with their competitors, because they 
believe, as Intel does, that they will gain more by 
pursuing a proprietary solution (Cartwright, Hahn-
Steichen, He, & Miller, 2005). However, initiators 
of private IOS arrangements face two difficult 
challenges 
that 
members 
of 
shared 
IOS 
infrastructures are less likely to face—partners are 
less likely to join private IOS and, if they do join, 
they are less likely to use private IOS in ways that 
benefit initiators. These points are elaborated below 
and summarized in Table 1, which covers research 
on the reasons that partners may not adopt, integrate 
with, and effectively use IOS. 
First, business partners are often less enthusiastic 
about participating in initiators’ private arrangement 
than they are about joining shared infrastructures. 
Partners 
have 
very 
different 
cost-benefit 
considerations than IOS initiators do (Beck, Weitzel, 
& Konig, 2002; Howard, Vidgen, & Powell, 2003; 
Mackay, 1996; Marcussen, 1996; Rao, Pegels, 
Salam, Hwang, & Seth, 1995; Sriram, Arunachalam, 
& Ivancevich, 2000; Young, Carr, & Rainer, 1999). 
Indeed, the near consensus in the literature that 
partners do not benefit from IOS motivated studies 
focusing primarily on partners’ benefits (Lee et al., 
1999; Subramani, 2004). Whereas the initiator 
expects its private IOS infrastructure to be useful for 
all of its partners and many of its transactions, its 
partners are not able to use that infrastructure for all 
of their partners and transactions. This means that 
the 
private 
infrastructure 
is 
an 
additional 
33
Building Successful Interorganizational Systems

channel/process for most of the partners and hence 
an added cost with benefits limited to transactions 
with the initiator. Furthermore, unless the initiator 
provides incentives such as a larger volume of 
business or better payment terms, many partners are 
unable to benefit much from using the IOS even with 
the initiator, owing to a small number of 
transactions, lack of technical expertise needed for 
integration with the IOS, lack of a high quality 
internal IS infrastructure, etc. As a result, private 
infrastructures often fail to achieve adoption by 
100% of the initiators’ partners, thereby limiting 
initiators’ benefits. 
A second challenge facing private IOS arrangements 
is that even partners adopting IOS often do so in 
ways that do not maximize initiators’ benefits. Large 
companies frequently make participation in private 
IOS a condition of doing business with them 
(Iacovou et al., 1995; Tuunainen, 1998; Webster, 
1995). 
Rather 
than 
jeoparde 
their 
customer 
relationships, business partners may comply with 
these demands in letter, but not in spirit: They “use” 
the IOS to the extent of making and receiving 
electronic messages, but they do no change their 
processes or integrate the IOS with their back-end 
systems, for reasons discussed above. Although 
there are many reported cases of large customers 
requiring IOS adoption, I have found no cases 
reporting large customers demanding full integration 
or process change. Indeed, the record indicates that 
most initiators give their partners choices about how 
to connect (Cartwright et al., 2005; Jelassi & Figon, 
1994; Leser et al., 2005; Williams & Frolick, 2001). 
For example, Srinivasan, Kekre, and Mukhopadhyay 
(1994) noted that "Chrysler has mandated its 
suppliers to have an electronic link, but has not 
mandated integration of EDI with internal systems at 
the supplier end" (p.1294). As a result, initiators of 
private IOS infrastructures often do not capture the 
full business value of using an IOS even with the 
limited number of partners that have adopted it.
Although the policy of mandating IOS use without 
mandating process changes and systems integration 
does not result in full business value for initiators, it 
avoids damaging the relationship between initiators 
and partners, and it increases partners’ willingness to 
adopt the IOS. Therefore, most initiators do not see 
mandating their partners to make process and 
systems changes as a viable option. Therefore, 
experts have advised initiators to improve their 
returns from IOS by actively encouraging or helping 
their business partners to implement the IOS more 
effectively (Gebauer & Buxmann, 2000; Riggins & 
Mukhopadhyay, 
1994, 
1999) 
by 
providing 
incentives, subsidies, technical assistance, and/or 
IOS design choices that keep partners’ costs 
minimal. Although, as discussed below, relatively 
few companies take this advice, some manage to do 
so while keeping their maintenance costs under 
control. The next section provides examples of two 
such companies. 
3 EXAMPLES OF SUCCESSFUL 
PRIVATE IOS 
INFRASTRUCTURES
Initiators of private IOS arrangements have to be 
very creative to succeed at getting most of their 
partners to adopt the IOS while doing so in a way 
that maximizes their own benefits and minimizes 
their costs.  Much can be learned from companies 
that are achieving these goals in today’s technology 
environment. Two notable examples are Intel and 
Net-Tech (a pseudonym). The descriptions below 
are interpretations adapted from Cartwright et al. 
(2005) (for Intel) and from Leser et al. (2005) (for 
Net-Tech).  
3.1 Intel—Building an IOS with 
Partners in Mind 
Intel, the world’s largest chip manufacturer and a 
leading manufacturer of computer networking and 
communications products, was a founding member 
of 
the 
RosettaNet 
consortium, 
a 
standards 
organization 
created 
to 
support 
collaborative 
commerce in the electronics industry. Since 2000, 
Intel aggressively redeveloped its IT architecture 
using RosettaNet standards to reduce costs through 
greater process automation and to create new 
business value through supply chain models 
involving outsourcing to third party logistics 
companies. Intel estimated the benefits from these 
changes to be $40M in 2004 alone.  
3.3.1 Change Vision and Approach 
Intel’s vision involved connecting all its business 
partners from every tier of its supply chain to its 
back-end ERP systems. Seamless integration would 
enable 
greater 
process 
automation 
and 
the 
development of new collaboration approaches that 
connect 
several 
tiers 
of 
the 
supply 
chain 
34
M. Lynne Markus 

simultaneously, 
rather 
than 
sequentially 
and 
bilaterally.
A key challenge Intel faced in realizing this vision 
was the heterogeneity of its own IT architecture:
Intel uses a variety of different ERP systems in 
different parts of its business. And each ERP system 
has its own infrastructure of operating system, 
databases, and interfaces. Intel realized, first, that 
modifying its own internal system to comply with 
RosettaNet standards would be an expensive and 
time-consuming 
undertaking. 
Secondly, 
Intel 
realized that if the company were to succeed in 
encouraging its business partners to integrate 
electronically with Intel, the company would have to 
insulate its partners from the complexity and 
dynamism of Intel’s enterprise IT architecture. 
Because of these considerations, Intel created a re-
usable middleware platform to support the passing 
of standardized RosettaNet messages between its 
backend systems and those of its business partners. 
This platform would also insulate Intel from its 
business partners’ changes in the technologies they 
use for connecting to Intel. 
A second key challenge that Intel faced was 
heterogeneity of the IT environments, technical 
skills, and financial resources of its business 
partners. All of the thousands of companies in 
Intel’s extended supply chain had the ability to 
perform some type of electronic transmission, but 
they 
varied 
considerably 
in 
their 
specific 
capabilities. Many used EDI; others used file 
transfer; others had even more limited capabilities. 
All would face significant conversion costs to adopt 
the RosettaNet standards on which Intel’s platform 
was built. Intel believed that partners adopting 
RosettaNet standards would benefit from the ability 
to use the same approach to connect to any other 
business partner or even to disparate internal 
systems. 
However, 
implementing 
RosettaNet 
standards requires time, money, and know-how—
something that not all partners might be able or 
willing to commit. Therefore, Intel decided not to 
mandate the use of RosettaNet standards, but rather 
to give its partners a range of options by which they 
could connect to its platform: 
x
RosettaNet
x
EDI
x
File transfer 
x
A suite of four foundational Web-based 
transactions (purchase orders, forecasts, 
invoices, 
and 
advanced 
shipment
notifications) for the least IT-sophisticated 
companies. 
Although Intel developed and operates the platform 
in-house, thereby creating a significant internal 
competency in systems integration, Intel relies on 
third party service providers to help its partners 
convert from EDI to RosettaNet and to provide file 
transfer capability. These outsourcing arrangements 
allowed 
Intel 
to 
standardize 
on 
a 
single 
technology—web services—thus reducing its costs 
of owning and operating the platform.  
3.3.2 Additional Challenges and Solutions 
Despite the considerable advantages afforded by 
RosettaNet standards, small and medium sized 
enterprises (SMEs) remain slow to adopt them, 
because they lack the knowledge, time, and 
resources required for implementation. Intel formed 
a program within RosettaNet to develop a special 
interface process for SMEs. This process can be 
incorporated into a variety of tools, including 
software packages marketed by vendors to SMEs. 
SMEs will be able to use these tools to connect to 
Intel and to any other partners that use RosettaNet 
standards, thus enabling SMEs to leverage their 
investments in electronic interconnection across 
multiple business partners. This industry-wide 
solution will also help other large companies, 
including Intel’s competitors, by enabling them to 
connect electronically faster and more effectively to 
a larger proportion of their business partners.  
3.3.3 The Bottom Line 
Intel has used its RosettaNet platform to automate 
more than 50 unique business transactions with over 
200 different business partners that collectively 
interact with Intel in over one thousand “touch 
points”. As noted above, Intel attributes more than 
$40M in savings annually to this IT investment, 
either directly through greater automation or 
indirectly through the creation of new supply chain 
business models supported by the platform. 
Some of these benefits can surely be attributed to 
Intel’s success (and ongoing efforts) in encouraging 
its partners to connect to the platform. Intel’s 
strategies of reducing partners’ costs of connection, 
of not mandating that they adopt new technology, 
and of employing RosettaNet standards that could be 
leveraged with partners other than Intel, have clearly 
contributed to partners’ willingness to integrate 
electronically with Intel. 
35
Building Successful Interorganizational Systems

3.4 Net-Tech—Engaging One 
Partner to Bring Other Partners 
Along
Net-Tech (a pseudonym) is a major networking 
solutions provider. Net-Tech orchestrates a complex, 
multi-tier supply chain consisting of manufacturing 
partners, to which it outsources production, and 
component providers, who supply the parts needed 
in the manufacture of Net-Tech’s products. 
Historically, Net-Tech communicated sales forecasts 
and purchase orders to manufacturing partners, who 
in turn exchanged forecasts and purchase orders with 
component providers. This process had two 
drawbacks. First, because of the sequential nature of 
the data flow, crucial information—about back-
ordered parts, for example—sometimes did not 
reach Net-Tech in a timely fashion. Second, because 
of 
the 
heterogeneous 
information 
processing 
environments of the supply chain partners, a variety 
of errors and “disconnects” occurred when the 
partners exchanged information.  
3.4.1 Change Vision and Approach  
Net-Tech’s vision for improving its supply chain 
effectiveness was an IT-enabled collaboration hub 
that would 1) connect Net-Tech simultaneously with 
all tiers of its supply chain and 2) standardize 
information 
exchange 
and 
allow 
for 
data 
aggregation, 
dynamic 
process 
monitoring 
for 
exception reporting, and enhanced performance 
analysis and collaboration capabilities. The desired 
capabilities could not be enabled by existing data 
exchange technologies such as EDI, and no 
commercially available solution had all the needed 
functionality. Net-Tech was not willing to consider a 
consortium-type arrangement that would be open to 
its competitors, because the company believed that 
better supply chain management would give it 
significant competitive benefits, and Net-Tech was 
large enough to afford a closed solution. 
Net-Tech therefore faced the choice of integrating 
and operating a platform in-house or relying on a 
third-party services provider. One consideration in 
the decision was speed and cost: a provider could 
leverage its expertise and technology across 
different client companies, while still providing Net-
Tech with a secure and closed solution. Another 
important consideration was the challenge of “on 
boarding” a large number of partner companies. 
Viacore, Net-Tech’s chosen solution provider 
estimated that “on boarding” represents 49% of the 
total cost of ownership of a collaboration hub such 
as Net-Tech’s (Viacore, n.d.).  
The costs of “on boarding” aside, several issues 
make using an external service provider an attractive 
option in multi-tier supply chains. Many of the 
intended users of Net-Tech’s hub were component 
suppliers, affiliated with Net-Tech only indirectly 
through 
their 
relationships 
with 
Net-Tech’s 
manufacturing partners. For Net-Tech to begin 
working directly with the suppliers might have 
raised trust issues for the manufacturers, the 
suppliers, or both. Second, if Net-Tech were to 
operate the hub, the company would be providing 
high-cost, high-value IT services to its partners. 
There would be understandable internal pressures 
for Net-Tech to recover the costs of the hub by 
charging its partners. Partners, however, could easily 
see Net-Tech’s charging for use of the hub as a 
conflict of interest. Such perceptions might well 
cause the effort to fail, especially since most 
researchers emphasize the importance of IT-enabled 
collaborations being beneficial to all concerned. If it 
were 
necessary 
to 
charge 
the 
partners 
for 
participation in the hub, partners might view charges 
from a third party provider as more acceptable than 
the same changes from their customer, always 
assuming partners believe that the benefits of 
participating in the hub outweigh the costs. 
3.4.2 Additional Challenges and Solutions 
Collaboration hubs become more valuable to their 
originators as those hubs handles a larger proportion 
of the originator’s business volume and partners. 
Not surprisingly, Net-Tech wanted to connect its 
over 2000 partners immediately, but at Viacore’s 
suggestion Net-Tech pursued a more gradual 
approach. 
After 
a 
three-month 
pilot 
project 
involving seven partners, the collaboration hub was 
declared operational. By fourth quarter 2003, 59 
partners (manufacturers and component suppliers), 
accounting for almost 60% of Net-Tech’s direct 
purchases, were connected to the hub. 
Because this collaboration hub is closed to all but 
Net-Tech’s partner companies, the partners’ cost-
benefit equation is different from Net-Tech’s. Many 
of Net-Tech’s partners do business with other 
companies that are not allowed to use the Net-Tech 
hub. Therefore, partners might view the potential 
benefits of participating in the hub as limited. Net-
Tech and Viacore increased acceptance and use of 
36
M. Lynne Markus 

the hub by reducing partners’ costs in several ways. 
First, the hub was designed around electronics 
industry IT standards—RosettaNet’s XML standards 
and PIPs (partner interface processes). Thus, 
partners that elected to integrate their back-end 
systems with the hub would be able to use the same 
standardized approach for connecting with their 
other partners. Second, Net-Tech did not require 
partners 
to 
convert 
their 
legacy 
back-end 
technology. In 2003, 50% of messages exchanged 
via the hub were flatfiles, 25% were non-standard 
XML files, and 25% were RosettaNet XML. One 
partner continued to use EDI; these files were 
automatically translated by the collaboration hub 
into the Net-Tech format. 
3.4.3 The Bottom Line 
Net-Tech did not do a formal ROI analysis, but 
company executives were confident that the hub 
paid for itself by reducing cycle time and 
disconnects. The benefits can be attributed in part to 
process improvement: as part of the implementation 
process, responsibilities for resolving errors and 
disconnects were carefully assigned to appropriate 
partners.  
In addition to technology and process change, a fair 
amount of the credit for this success can be 
attributed to choices that Net-Tech made about 1) 
using a third-party service provider for developing 
and operating the hub and “on boarding” partners 
and 2) reducing partners’ implementation costs by 
using industry standards and by not requiring legacy 
system conversion.  
4 SUCCESS MEANS FACING 
THE SPECIAL CHALLENGES 
OF IOS 
Generalizing from these examples, it seems clear 
that 
successful 
initiation 
of 
private 
IOS 
infrastructures requires careful attention to the needs 
of business partners. Researchers who have analyzed 
IOS successes and ways to increase IOS benefits 
have emphasized the importance of creating mutual 
benefits for participants (Allen, Colligan, Finnie, & 
Kern, 2000; Alt & Fleisch, 2000-2001; Alt, Fleisch, 
& Osterle, 2000; Damsgaard & Lyytinen, 1998; 
Gebauer & Buxmann, 2000; Premkumar et al., 2004; 
Riggins & Mukhopadhyay, 1994; Tillquist, King, & 
Woo, 2002). However, the many studies previously 
cited that show how few benefits accrue to partners 
suggest that win-win outcomes are rare. In addition, 
some research shows directly that many IOS 
infrastructures are designed with little or no partner 
consultation that could lead to better adaptation to 
partners’ needs and concerns (Cavaye, 1995; Gerst 
& Bunduchi, 2005). Indeed, the researchers of the 
previously cited NIST study had to adjust their 
planned research methodology, because they could 
not find electronics and automotive industry supply 
chain members that were collaborating to develop 
private IOS infrastructures: 
“Early in the project, we believed that it 
might 
be 
useful 
to 
interview 
supplier/customer pairs from each of the 
supply chains. Upon discovering that firms 
in these two industry sectors were not 
actually cooperating in developing supply 
chain 
information 
processes 
(except 
through consortia like RosettaNet and the 
Automotive Industry Action Group), we 
elected to convert our case study plan into 
profiles of individual firms.” (White & 
O'Connor, 2004, pp. 6-10-6-11, emphasis 
added) 
What barriers could be preventing IOS initiators 
from working with their partners to develop 
mutually beneficial collaboration infrastructures? 
Research suggests that four barriers are most 
significant: initiators’ legacy systems, initiators’ 
unwillingness to adopt process changes, initiators’ 
unwillingness to collaborate or share benefits, 
initiators’ lack of resources to provide assistance to 
partners. 
4.1 Legacy Information Technology  
Legacy systems are not just a barrier for partners, it 
is also a barrier for initiators. Legacy systems 
prevent partners from adopting or integrating with 
an initiator’s IOS. But the initiator’s legacy 
information technology can make it difficult or 
impossible for the initiator to design an IOS that 
partners would find beneficial or less expensive to 
adopt. IOS arrangements based on Internet and 
industry standards help partners by lowering their 
adoption costs and increasing their potential 
benefits, for example, through ability to reuse the 
technology with other partners (Beck et al., 2002; 
Gebauer & Buxmann, 2000). However, small 
partners often have a much easier time becoming 
standards-compliant than large companies, because 
vendors are likely to build these standards 
37
Building Successful Interorganizational Systems

(eventually) into their packaged enterprise systems, 
which many small companies use. By contrast, large 
initiators that may have grown through acquisition 
often 
have 
extremely 
heterogeneous 
IT 
environments and therefore face very protracted and 
expensive 
efforts 
to 
transform 
their 
IT 
infrastructures. 
Consequently, 
many 
initiators 
propose IOS arrangements that require their partners 
to adapt to the initiators’ legacy technology (Gerst & 
Bunduchi, 2005; Howard et al., 2003). For reasons 
discussed above, partners have little incentive to 
adopt these arrangements in ways that are most 
beneficial to initiators. 
4.2 Process Change 
A closely related issue is the need for process 
change. Process changes made by initiators, such as 
streamlined payment cycles, can provide partners 
with incentives to adopt and integrate with an IOS 
(Gebauer & Buxmann, 2000). However, initiators 
may be unable to make process changes (because of 
poor IT infrastructure) or they might be unwilling to 
do so (Webster, 1995) for reasons discussed below. 
Whatever the explanation, the failure of initiators to 
redesign their own internal processes can result in 
their offering IOS arrangements that partners avoid 
or under-use. 
4.3 Beliefs about Collaboration 
A third barrier lies in initiators’ beliefs and attitudes 
about collaboration. In some industries, relationships 
between buyers and suppliers have traditionally been 
adversarial (Bensaou & Anderson, 1999). More 
recent efforts by large customers to develop 
partnerships with their suppliers have to overcome 
decades of mutual mistrust. These embedded 
feelings and habits of thought can easily sour IOS 
arrangements. Initiators might believe that their 
customer role or market power entitles them to 
dictate terms and to reap the lions’ share of benefits 
from their IOS arrangements (Jap, 2001). Yet IOS 
arrangements that reflect those beliefs can actually 
hurt initiators by decreasing their partners’ adoption 
of and integration with the IOS (Riggins & 
Mukhopadhyay, 1994).  
4.4 Resources for Helping Partners  
A final major barrier lies in the resources required to 
help partners implement and integrate IOS. First, 
much of the activity involved in getting one’s 
partners on board is non-technical, consisting of 
awareness raising, documenting and changing 
business processes, and change management (Alt & 
Fleisch, 2000-2001; Alt et al., 2000; Corbett, 
Blackburn, & Van Wassenhove, 1999; El Sawy, 
2003). The skills required to perform these activities 
may not reside in the IS function, necessitating 
cross-functional coordination between IS and 
business functions in the initiating organization. 
Second, such activities do not “scale” (El Sawy, 
2003), that is, the initiator must redo these activities 
for each partner with little returns to learning. This 
means that the total cost of performing them can be 
very high. Indeed, an IT services company that 
specializes in creating and operating collaborative IT 
arrangements estimates that “on boarding” partners 
accounts for nearly half of the total cost (Viacore, 
n.d.). The cross-functional coordination and high 
cost of building IOS arrangements that meet 
partners’ needs ensure that these initiatives will 
attract executive scrutiny. Without active executive 
sponsorship, justifying heavy expenditures that will 
directly benefit partners and only indirectly benefit 
the 
initiator 
could 
prove 
difficult. 
(Such 
expenditures 
are 
less 
necessary 
in 
shared 
infrastructures and could be easier to justify in 
private IOS arrangements when bundled with 
technical activities and outsourced to a third part 
provider.) 
5 CONCLUSION 
In short, the benefits IOS initiators receive from 
their investments in IOS depend, not just on their 
own implementation decisions, but also those of 
their business partners. Initiators can gain the 
greatest benefits from IOS by expanding fully 
integrated IOS usage to the majority of their partners 
while 
minimizing 
the 
costs 
associated 
with 
maintaining multiple channels and processes. Doing 
so requires, at minimum, that IOS be designed with 
partners’ needs in mind and generally also requires 
considerable additional expenditures for partner “on 
boarding” and change management. Many IOS 
initiators 
face 
high 
internal 
barriers—legacy 
systems, inflexible processes, unwillingness to share 
benefits, and lack of resources such as executive 
sponsorship, change management, and specialist 
technical skills—that can prevent their developing 
private IOS arrangements that partners will adopt 
and use effectively. Nevertheless, the potential 
rewards for organizations that can overcome these 
barriers are great.  
38
M. Lynne Markus 

Table 1: Why Partners Might Not Adopt, Integrate With, 
or Properly Use an IOS. 
x
Cost of setup (e.g., software) and operations (e.g., 
VAN transaction fees) too high in absolute terms  
(Beck et al., 2002; Gebauer & Buxmann, 2000; 
McGrath & More, 2001; Salmi & Tuunainen, 2001) 
x
Few transactions, therefore limited benefits (Beck et 
al., 2002; Chen & Williams, 1998; Tuunainen, 1998; 
Williams & Frolick, 2001; Young et al., 1999) 
x
Few other partners with which to use the IOS, therefore 
limited benefits (Beck et al., 2002; Philip & Pedersen, 
1997; Tuunainen, 1998) 
x
Lack of standards or closed membership policy that 
prevent the IOS from being used with other partners, 
therefore limited benefits and/or additional costs to 
maintain multiple processes/channels for multiple 
partners (Chen & Williams, 1998; Damsgaard & 
Truex, 2000; Howard et al., 2003; Tuunainen, 1998; 
Williams & Frolick, 2001) 
x
Lack of internal enterprise systems with which to 
exchange data with IOS, therefore additional labor 
costs to use and limited benefits (Beck et al., 2002) 
x
Low quality or unintegrated internal enterprise 
systems, making integration with IOS difficult or 
impossible, therefore additional labor costs to use and 
limited benefits (McGrath & More, 2001; Truman, 
2000)
x
Lack of in-house knowledge and expertise for 
integration, therefore additional labor costs to use and 
limited benefits (Cartwright et al., 2005; Gebauer & 
Buxmann, 2000; Iacovou et al., 1995; McGrath & 
More, 2001; Williams & Frolick, 2001) 
x
Lack of staff training in how to use IOS (Sriram et al., 
2000)
x
Absence of process changes or other concessions by
initiators that would give partners a real incentive to 
use the IOS (Gebauer & Buxmann, 2000; Howard et 
al., 2003; Premkumar et al., 2004; Riggins & 
Mukhopadhyay, 1999; Tillquist et al., 2002; Webster, 
1995)
REFERENCES
Allen, D. K., Colligan, D., Finnie, A., & Kern, T. (2000). 
Trust, Power and Interorganizational Information 
Systems: The Case of the Electronic Trading 
Community TransLease. Information Systems Journal, 
10, 21-40. 
Alt, R., & Fleisch, E. (2000-2001). Business Networking 
Systems: 
Characteristics 
and 
Lessons 
Learned. 
International Journal of Electronic Commerce, 5(2),
7-27.
Alt, R., Fleisch, E., & Osterle, H. (2000). Journal of 
Electronic Commerce Research. 1, 2, 67-78. 
Barua, A., Konana, P., & Whinston, A. B. (2004). An 
Empirical Investigation of Net-Enabled Business 
Value. MIS Quarterly, 28(4), 558-620. 
Beck, R., & Weitzel, T. (2005). Some Economics of 
Vertical Standards: Integrating SMEs in EDI Supply 
Beck, R., Weitzel, T., & Konig, W. (2002). Promises and 
Pitfalls of SME Integration. Paper presented at the 
15th Bled Electronic Commerce Conference: e-
Reality: Constructing the e-Economy, Bled, Slovenia. 
Bensaou, M., & Anderson, E. (1999). Buyer-Supplier 
Relationships in Industrial Markets: When Do Buyers 
Risk Making Idiosyncratic Investments? Organization 
Science, 10(4), 460-481. 
Cartwright, J., Hahn-Steichen, J., He, J., & Miller, T. 
(2005). 
RosettaNet 
for 
Intel's 
Trading 
Entity 
Automation. Intel Technology Journal, 9(3), 239-246. 
Cavaye, 
A. L. 
M. 
(1995). 
Participation 
in 
the 
Development 
of 
Inter-Organizational 
Systems 
Involving Users outside the Organization. Journal of 
Information Technology, 10, 135-147. 
Champy, J. (2002). X-Engineering the Corporation: The 
Next Frontier of Business Performance. New York: 
Warner Business Books. 
Chatfield, A. T., & Bjorn-Andersen, N. (1997). The 
Impact of IOS-enabled Business Process Change on 
Business Outcomes: Transformation of the Value 
Chain of Japan Airlines. Journal of Management 
Information Systems, 14(1), 13-40. 
Chen, J.-C., & Williams, B. C. (1998). The Impact of 
Electronic 
Data 
Interchange 
(EDI) 
on 
SMEs: 
Summary of Eight British Case Studies. Journal of 
Small Business Management, 36(4), 68-72. 
Christiaanse, E., & Venkatraman, N. (2002). Beyond 
Sabre: An Empirical Test of Expertise Exploitation in 
Electronic Channels. MIS Quarterly, 26(1), 15-38. 
Clark, T., & Stoddard, D. B. (1996). Interoganizational 
Business Process Redesign: Merging Technological 
and Process Innovation. Journal of Management 
Information Systems, 13(2), 9-28. 
Clemons, E. K., & Row, M. C. (1988). McKesson Drug 
Company: A Case Study of Economost--A Strategic 
Information 
Systems. 
Journal 
of 
Management 
Information Systems, 5(1), 36-50. 
Corbett, C. J., Blackburn, J. D., & Van Wassenhove, L. N. 
(1999). Case Study Parnterships To Improve Supply 
Chains. Sloan Management Review (Summer), 71-82. 
Damsgaard, J., & Lyytinen, K. (1998). Contours of 
Diffusion of Electronic Data Interchange in Finland: 
Overcoming Technological Barriers and Collaborating 
To Make It Happen. The Journal of Strategic 
Information Systems, 7, 275-297. 
Damsgaard, J., & Truex, D. (2000). Binary Trading 
Relations and the Limits of EDI Standards: The 
Procrustean Bed of Standards. European Journal of 
Information Systems, 9(3), 142-158. 
El Sawy, O. A. (2003). Collaborative Integration in E-
Business through Private Trading Exchanges (PTXs). 
Information Systems and e-Business, 1, 119-137. 
Gebauer, J., & Buxmann, P. (2000). Assessing the Value 
of Interorganizational Systems To Support Business 
Transactions.
International Journal of Electronic 
Commerce.
39
Building Successful Interorganizational Systems
Chains. Electronic Markets, 15(4), 313-322. 

Gerst, M., & Bunduchi, R. (2005). Shaping IT 
Standardization in the Automotive Industry – The 
Role of Power in Driving Portal Standardization. 
Howard, M., Vidgen, R., & Powell, P. (2003). 
Overcoming Stakeholder Barriers in the Automotive 
Industry: Building to Order with Extra-Organizational 
Systems. Journal of Information Technology, 18, 27-
43.
Iacovou, C. L., Benbasat, I., & Dexter, A. S. (1995). 
Electronic Data Interchange and Small Organizations: 
Adoption and Impact of Technology. MIS Quarterly, 
19(4), 465-485. 
Jap, S. D. (2001). Pie-Sharing in Complex Collaboration 
Contexts. Journal of Marketing Research, 38(1), 86-
99.
Jelassi, T., & Figon, O. (1994). Competing Through EDI 
at Brun Passot: Achievements in France and 
Ambitions for the Single European Market. MIS
Quarterly, 18(4), 337-352. 
Juul, N. C., Andersen, K. V., & Korzen-Bohr, S. (2004). 
Challenging the Paradigms on Up-Stream B2B E-
Commerce? Paper presented at the 37th Hawaii 
International Conference on Systems Sciences. 
Lee, H. G., Clark, T., & Tam, K. Y. (1999). Research 
report. Can EDI benefit adopters? Information Systems 
Research, 10(2), 186-195. 
Leser, F., Alt, R., & Osterle, H. (2005). Implementing 
Collaborative Process Management--The Case of Net-
Tech. International Journal of Cases on Electronic 
Commerce, 1(4), 1-18. 
Mackay, D. (1996). Measuring Organizational Benefits of 
EDI Diffusion: A Case of the Australian Automotive 
Industry. 
International 
Journal 
of 
Physical 
Distribution & Logistics, 26(10), 60-78. 
Marcussen, C. (1996). The Effects of EDI on Industrial 
Buyer-Seller Relationships: A Network Perspective. 
International Journal of Purchasing and Materials 
Management (Summer), 20-26. 
McGrath, G. M., & More, E. (2001, January 3-6). Data
Integration Along the Healthcare Supply Chain: The 
Pharmaceutical Extranet Gateway Project. Paper 
presented at the The 34th Annual Hawaii International 
Conference on Systems Sciences, Maui, Hawaii. 
Mukhopadhyay, T., & Kekre, S. (2002). Strategic and 
Operational Benefits of Electronic Procurement in 
B2B Procurement Processes. Management Science, 
48(10), 1301-1313. 
Mukhopadhyay, T., Kekre, S., & Kalathur, S. (1995). 
Business Value of Information Technology: A Study 
of Electronic Data Interchange. MIS Quarterly, 19(2),
137-156.
Philip, G., & Pedersen, P. (1997). Inter-Organisational 
Information Systems: Are Organisations in Ireland 
Deriving Strategic Benefits from EDI? International 
Journal of Information Management, 17(5), 337-357. 
Premkumar, G. P., Richardson, V. J., & Zmud, R. W. 
(2004). Sustaining Competitive Advantage Through a 
Value Net: The Case of Enterprise Rent-A-Car. MIS
Quarterly Executive, 3(4), 189-199. 
Rao, H. R., Pegels, C. C., Salam, A. F., Hwang, K. T., & 
Seth, V. (1995). The Impact of EDI Implementation 
Commitment 
and 
Implementation 
Success 
on 
Competitive Advantage and Firm Performance. 
Information Systems Journal, 5(3), 185-202. 
Riggins, 
F. 
J., 
& 
Mukhopadhyay, 
T. 
(1994). 
Interdependent 
Benefits 
from 
Interorganizational 
Systems: 
Opportunities 
for 
Business 
Partner 
Reengineering. Journal of Management Information 
Systems, 11(2), 37-57. 
Riggins, F. J., & Mukhopadhyay, T. (1999). Overcoming 
EDI 
Adoption 
and 
Implementation 
Risks. 
International Journal of Electronic Commerce, 3(4),
103-123.
Salmi, H., & Tuunainen, V. K. (2001, June 25-26). 
Diffusion of Electronic Business in Networks--Case 
Autolinkki Teaching Case. Paper presented at the 
Fourteenth Bled Electronic Commerce Conference, 
Bled, Slovenia. 
Segev, A., Porra, J., & Roldan, M. (1997). Internet-based 
EDI strategy. Decision Support Systems, 21(3), 157-
170.
Short, J. E., & Venkatraman, N. (1992). Beyond Business 
Process Redesign: Redefining Baxter's Business 
Network. Sloan Manage Review, 34(1), 7-21. 
Srinivasan, K., Kekre, S., & Mukhopadhyay, T. (1994). 
Impact of Electronic Data Interchange Technology on 
JIT Shipments. Management Science, 40(10), 1291-
1304.
Sriram, R. S., Arunachalam, V., & Ivancevich, D. M. 
(2000). EDI Adoption and Implementation: An 
Examination of Perceived Operational and Strategic 
Benefits, and Controls. Journal of Information 
Systems, 14(1), 37-52. 
Subramani, M. (2004). How Do Suppliers Benefit From 
Information Technology Use in Supply Chain 
Relationships? MIS Quarterly, 28(1), 45-73. 
Tillquist, J., King, J. L., & Woo, C. (2002). A 
Representational Scheme for Analyzing Information 
Technology and Organizational Dependency. MIS 
Quarterly, 26(2), 91-118. 
Truman, G. E. (2000). Integration in Electronic Exchange 
Environments. Journal of Management Information 
Systems, 17(1), 209-244. 
Tuunainen, V. K. (1998). Opportunities of Effective 
Integration of EDI for Small Businesses in the 
Automotive Industry. Information & Management, 34,
361-375.
Viacore. (n.d.). About BusinessTone: FAQs. Retrieved 
10/23/2005, 
from 
the 
World 
Wide 
Web: 
http://www.viacore.net/about_businesstone/faq.php. 
Webster, J. (1995). Networks of Collaboration or 
Conflict? Electronic Data Interchange and Power in 
the Supply Chain. Journal of Strategic Information 
Systems, 4(1), 32-42. 
40
M. Lynne Markus 
Electronic Markets, 15(4), 335-343. 
www.ebook3000.com

Williams, L. R., Magee, G. D., & Suzuki, Y. (1998). A 
Multidimensional View of EDI: Testing the Value of 
EDI Participation to Firms. Journal of Business 
Logistics, 19(2), 73-87. 
Williams, M. L., & Frolick, M. N. (2001). The Evolution 
of EDI for Competitive Advantage: The FedEx Case. 
Information Systems Management (Spring), 47-53. 
Young, D., Carr, H. H., & Rainer, J., R. Kelly. (1999). 
Strategic 
Implications 
of 
Electronic 
Linkages. 
Information Systems Management, 16(1). 
White, W. J., & O'Connor, A. C. R., Brent R. (2004). 
Economic Impact of Inadequate Infrastructure for 
Supply Chain Integration, National Institute of 
Standards and Technology Planning Report 04-2.
Research Triangle Park: RTI International. 
41
Building Successful Interorganizational Systems

PART 1 
Databases and Information
Systems Integration

THE HYBRID DIGITAL TREE
A New Indexing Technique for Large String Databases
Qiang Xue and Sakti Pramanik
Department of Computer Science and Engineering
Michigan State University, East Lansing, MI 48824, USA
Email: {xueqiang,pramanik}@cse.msu.edu
Gang Qian
Department of Computer Science
University of Central Oklahoma, Edmond, OK 73034, USA
Email: gqian@ucok.edu
Qiang Zhu
Department of Computer and Information Science
The University of Michigan,Dearborn, MI 48128, USA
Email: qzhu@umich.edu
Keywords:
Hybrid Digital tree, indexing, string databases, preﬁx searches, substring searches.
Abstract:
There is an increasing demand for efﬁcient indexing techniques to support queries on large string databases.
In this paper, a hybrid RAM/disk-based index structure, called the Hybrid Digital tree (HD-tree), is proposed.
The HD-tree keeps internal nodes in the RAM to minimize the number of disk I/Os, while maintaining leaf
nodes on the disk to maximize the capability of the tree for indexing large databases. Experimental results
using real data have shown that the HD-tree outperformed the Preﬁx B-tree for preﬁx and substring searches.
In particular, for distinctive random queries in the experiments, the average number of disk I/Os was reduced
by a factor of two to three, while the running time was reduced in an order of magnitude.
1
INTRODUCTION
Electronic text (string) collections have increased dra-
matically over the last decade, from megabytes of dic-
tionaries, to gigabytes of genome sequences, to ter-
abytes of web documents. Many applications need
efﬁcient indexing methods to process complex string
queries (e.g., substring searches) on these large string
data sets. In the past few decades, various data struc-
tures have been proposed for string indexing. They
can be divided into two categories: RAM-based and
disk-based. The ﬁrst category includes digital-tree-
based (trie-based) structures, such as the Patricia trie
(Morrison, 1968), the sufﬁx tree (McCreight, 1976;
Weiner, 1973), the sufﬁx array (Manber and My-
ers, 1990), and the PAT tree (Gonnet et al., 1991).
The second category includes the extendible hashing
(Fagin et al., 1979), inverted ﬁles (Baeza-Yates and
Ribiero-Neto, 1999), the Preﬁx B-tree (Bayer and Un-
terauer, 1977), and the String B-tree (Ferragina and
Grossi, 1999).
RAM-based index structures reside in the main
memory (RAM) where string queries are performed.
Among these RAM-based structures, Patricia tries
and PAT/sufﬁx trees are particularly effective in han-
dling relatively small amount of string data; however,
as the database size increases, it is no longer feasi-
ble to keep the trie structure in the RAM. Moreover,
because of the unbalanced structure of tries, it is inef-
ﬁcient to store tries on disk, especially when indexes
are dynamically created (Clark and Munro, 1996; Fer-
ragina and Grossi, 1999). Therefore, we argue that
RAM-based index structures are not suitable for in-
dexing large string databases.
On the other hand, disk-based data structures can
be used for indexing large string databases. Among
these disk-based structures, hashing technology is ef-
ﬁcient for exact string matches and inverted ﬁles are
efﬁcient for keyword-based searches; however, they
are unsuitable for substring searches. The Preﬁx B-
tree is capable of indexing large and dynamic string
databases. The String B-tree (Ferragina and Grossi,
1999) uses the Patricia trie inside its internal nodes
to provide the same worst-case performance as the B-
tree (Bayer and McCreight, 1972). Since the String
B-tree stores indexed strings in a separate ﬁle, it re-
quires more disk accesses than the Preﬁx B-tree in
general case. These disk-based indexing techniques
45
C.-S. Chen et al. (Eds.), Enterprise Information Systems VII, 45–51.
© 2006 Springer.

require limited RAM to conduct string queries. To
utilize the large amount of available memory, they
rely on caching mechanisms that are usually not opti-
mized for individual data structure.
In this paper, we propose the Hybrid Digital tree
(HD-tree), a novel hybrid RAM/disk-based index
structure to support efﬁcient queries on very large
string databases.
The HD-tree keeps its internal
nodes, which are similar to those in digital trees, in
the RAM to minimize the number of disk I/Os for a
string query. Its leaf nodes, which hold the sufﬁxes of
the indexed strings, are kept on disk to maximize the
capability of the tree for indexing a large database.
It is known that traditional disk-based trees, such as
Preﬁx B-trees, may use the available RAM to keep
their internal nodes (i.e., caching), so that the num-
ber of disk I/Os may be reduced. However, the HD-
tree is different from this approach as follows: First,
an internal node of disk-based trees is a disk block,
which is usually several kilobytes in size, while an in-
ternal node of the HD-tree is a data structure (i.e., a
trie node), which is usually several bytes in size. Sec-
ond, the internal nodes of disk-based trees are stored
on disks and have to be read into the RAM whenever
is necessary, while all internal nodes of the HD-tree
are kept in the RAM, so that no disk I/Os are required
to access these internal nodes.
The internal nodes of a HD-tree are built on the
preﬁxes of indexed strings and are used to guide the
search to the leaf node(s) containing the query an-
swer(s). Unlike a traditional digital tree, the parent
of a leaf node in the HD-tree allows a set (“range”) of
multiple preﬁxes so that indexed strings with different
preﬁxes may share the same leaf node (disk block) to
improve disk utilization. Moreover, unlike the tradi-
tional concept of range, the above preﬁx “range” of
a node may not be “continuous”, so that strings with
a preﬁx within the traditional range may be stored in
a separate leaf node(s) to allow further improvements
in disk utilization.
We did extensive experiments to study the behavior
of the HD-tree under different RAM sizes for various
string queries. It was observed that for a given data-
base size, a small amount of RAM improved the per-
formance of the HD-tree signiﬁcantly; however, when
the RAM size was increased beyond a certain thresh-
old point, the gain in performance became less sig-
niﬁcant. We also conducted experiments to evaluate
the performance of the HD-tree by comparing to the
Preﬁx B-tree. The experimental results showed that
the HD-tree outperformed the Preﬁx B-tree given the
same amount of RAM.
The rest of this paper is organized as follows: the
structure and algorithms of the HD-tree are described
in Section 2; experimental results using Text RE-
trieval Conference (TREC) collections (Voorhees and
sions and future work are presented in Section 4.
2
THE HD-TREE
The HD-tree incorporates and extends some indexing
strategies of the digital tree and the B+-tree (Comer,
1979), taking advantages of their strengths in search
performance, compression capability, and disk uti-
lization. We ﬁrst introduce the notation and assump-
tions used in this paper. A string consists of a se-
ries of letters (symbols) chosen from an alphabet Σ of
size |Σ|. The letters and strings are assumed to have a
lexicographic order. Symbols from Σ are denoted by
lower-case letters (e.g., a, b, and c), while strings are
denoted by lower-case Greek letters (e.g., α, β, and
γ). ♯is a special auxiliary symbol such that ♯/∈Σ
and ♯< c for any c ∈Σ. Given a string α=a1...an of
length |α|=n, we call a1...ai a preﬁx, aj...an a sufﬁx,
and ai...aj a substring of α, where 1 ≤i ≤j ≤n.
Given a set Ωof letters, function MAX(Ω) yields the
greatest element in Ω. The database is considered as a
set of records with the form Υi=(κi, Λi), where κi is
a unique string and Λi is the descriptive information
of κi, such as statistic, offset, or a pointer to another
location where the information can be found. Since
the focus of this paper is on studying the issues of
string indexing, Λi is ignored in our discussion (i.e.,
not strictly distinguishing a record and a string). Fi-
nally, databases are assumed to be too large to utilize
a RAM-based index technique.
2.1
HD-tree Structure
DISKS
Multi−Group
Leaf node
Single−Group
Leaf node
Multi−Group
Leaf Pointer
Single−Group
Leaf Pointer
Internal
Pointer
8
14
a
b
e
9
5
a
b
b
b
c
c
b
e
c
d
3
15
17
18
13
11
10
7
6
1
2
16
4
12
Alphabet:
{a, b, c, d, e}
Internal Node
d
d
e
e
RAM
TheHD-treeisanunbalancedandorderedtree.An
internal node δ of the HD-tree contains a list of pairs
L(δ)={(a1, P1), ..., (am, Pm)}, where Pi is a pointer
to its child node; ai(1 ≤i ≤m) is a letter from
Σ, called the label of Pi; and a1 < ... < am, such
Harman, 1997) are discussed in Section 3; conclu-
46
Qiang Xue et al.
Figure 1: An HD-tree.

that the pointers are ordered according to their labels.
Leaf nodes, which are implemented as disk blocks,
contain the sufﬁxes of indexed strings. The id-string
of a tree node is the concatenation of the labels along
the path traversing from the root to the node. The
id-string of the root is empty. Note that an HD-tree
node can be uniquely identiﬁed by its id-string. Let
ID(δ) denote the id-string of a tree node δ. In Figure
1, ID(2)=a, ID(9)=bbe, and ID(15)=db. An HD-
tree must satisfy two basic properties that determine
the proper leaf node for the indexed strings.
PROPERTY 1 For each internal node δ in an HD-tree,
ID(δ) is a common preﬁx of all strings contained in
any leaf node in the sub-tree with δ as the root.
Property 1 is similar to that of a digital tree; however,
the id-string of a leaf node δ′ in an HD-tree repre-
sents one or more preﬁxes (preﬁx-set) that strings in
δ′ may have. Let PS(δ′) be the preﬁx-set of δ′. If
|PS(δ′)|=1, all strings in δ′ share the same preﬁx in
PS(δ′). We call such a leaf node a Single-Group Leaf
(SGL). If |PS(δ′)| > 1, leaf node δ′ contains sev-
eral groups of strings, where the strings in each group
share a preﬁx that is different from the preﬁx of an-
other group. We call such a leaf node a Multi-Group
Leaf (MGL). The reason for using SGL and MGL is
to improve the disk utilization. Otherwise, some large
groups of strings may hinder the grouping of small
groups. Note that, based on Property 1, all preﬁxes
in PS(δ′) are different only at their last letters. An
internal node in an HD-tree may have three types of
pointers: (1) Internal Pointer (IP) to an internal node,
(2) Single-Group Leaf Pointer (SGLP) to an SGL, and
(3) Multi-Group Leaf Pointer (MGLP) to an MGL.
A key range in a traditional index tree, such as the
B-tree, is continuous, where no key between the two
boundaries of the range can be excluded; however, the
preﬁx-set (i.e., the preﬁx “range”) in the HD-tree may
not be continuous because one or more preﬁxes be-
tween the two boundaries (minimum and maximum
preﬁxes) of the range may be excluded. The preﬁx-
set PS(δ′) for an SGL δ′ contains the unique pre-
ﬁx ID(δ′), i.e., PS(δ′)={ID(δ′)}. For example, in
Figure 1, node 11 is an SGL where PS(11)={bbcc};
that is, all strings in this node have the common pre-
ﬁx bbcc. It is the task of the tree-building algorithm to
determine which node is an SGL.
Unlike an SGL, where its preﬁx-set is directly pre-
sented by its id-string, the preﬁx-set of an MGL
needs to be derived as follows.
Let δ′ be an
MGL, and δ be the parent node of δ′ containing
the list L(δ)={(a1, P1, ... , (ak, Pk), ... , (am, Pm)},
where m > 0 and Pk is the pointer to δ′.
Let
β=ID(δ). The preﬁx-set of the MGL δ′ is deﬁned
as: PS(δ′)={βc | c ∈ΩPk}, where ΩPk is a set of
letters obtained through the following steps:
1) Ω′
Pk={ai | (ai, Pi) ∈L(δ), ai < ak,
Piis an MGLP };
2) if (Ω′
Pk is empty ) b′=♯; else b′ = MAX( Ω′
Pk);
3) ΩΣ={a | a ∈Σ, b′ < a≤ak};
4) Ω′′
Pk={aj | (aj, Pj) ∈L(δ), b′ < aj < ak,
Pj is an IP or SGLP};
5) ΩPk=ΩΣ −Ω′′
Pk.
For example, in Figure 1, PS(9)={bbd,bbe} and
PS(12)={bbca, bbcb, bbcd}.
PROPERTY 2 Each leaf node δ′ in an HD-tree keeps
all the indexed strings with a preﬁx in its preﬁx-set
PS(δ′).
Based on the previous discussion on the preﬁx-set,
Property 2 of the HD-tree guarantees that any string
is placed in one and only one leaf node of an HD-tree.
Although we may logically consider that each string
is kept in a leaf node, the entire string does not have to
be stored in the leaf node physically, since the preﬁx
of a string can be found along the path from the root
to a leaf node. Therefore, only the sufﬁx of a string is
stored in a leaf node.
2.2
Building the HD-Tree
To build an HD-tree, algorithms are needed for inser-
tion, deletion, and update. Due to the limitation of
space, only the insertion and its related issues are de-
scribed in this paper. Interested readers can refer to
(Xue et al., 2004) for detail algorithms.
2.2.1
Insertion Procedure
The insertion procedure is to insert a new string κ
into a given HD-tree where κ=k1...kn, ki ∈Σ, and
1 ≤i ≤n. Note that ♯is appended at the end of a
string to distinguish the string from any id-string in
the given HD-tree. Assume the root of an HD-tree
is at level 1. Given an internal node δ at level l, kl
is used to determine the next pointer to follow. The
insertion procedure ﬁrst follows internal pointers (kl
must equal to the label) down the tree as far as possi-
ble. It stops at an internal node δ which satisﬁes the
following: PS(δ)=k1...ki; and for any internal node
δj in the tree, if PS(δj)=k1...kj then j ≤i. The letter
ki+1 is then used to ﬁnd a qualiﬁed leaf node (a child
of δ) according to Property 2. If no leaf node is qual-
iﬁed, either the right-most MGL is chosen (if avail-
able) and its preﬁx-set is expanded, or a new MGL is
created. Finally, the sufﬁx string ki+1...kn is stored
in the selected leaf node δ′. If δ′ overﬂows after the
insertion, the overﬂow processing is invoked. For ex-
ample, in Figure 1, to insert a string bbab, δ is the
internal node 7, δ′ is the leaf node 8, and ab is stored
in node 8. In the same way, string bbcca is stored in
node 11 as ca, string bbcab is stored in node 12 as ab.
47
The Hybrid Digital Tree

2.2.2
Overﬂow Processing
In HD-trees, only sufﬁxes of the original strings are
stored in a leaf node. These sufﬁxes are called sufﬁx-
strings. A sufﬁx-group is a set of sufﬁx-strings whose
ﬁrst letters are the same (see Figure 2). If the over-
ﬂow leaf node δ′ (whose parent is δ1) is an SGL, a
new internal node δ2 is created, the ﬁrst letter of each
sufﬁx-string in δ′ is removed; δ2 becomes the child of
δ1; δ′ becomes the child of δ2 (i.e., the grandchild of
δ1). Consequently, the tree grows to another level. If
the overﬂow leaf node δ′ is an MGL, it is considered
for splitting.
caa cb cbce
dca dcc
eab edea eee
A Multi−Group Leaf
node 17
cbba cbcd
caa cbc cdd ce
cabd cac 
A Single−Group Leaf
node 11
group
a
2.2.3
Splitting
When an MGL is split, the sufﬁx-strings in δ′ must
be moved by one sufﬁx-group at a time. If the MGL
δ′ is split into two whenever it overﬂows (SSplit), the
disk utilization is shown to be very low. In order to
improve the disk utilization, two heuristics are used
(HD-Split): (1) if the size of a sufﬁx-group is greater
than a threshold T (we use 85% of the disk block size
in our experiments), an SGL containing this sufﬁx-
group is formed; (2) before an overﬂow node is split
or after an SGL is moved out of an overﬂow leaf node,
sufﬁx-groups may be moved to the qualiﬁed left or
right siblings to avoid creating a new leaf node.
2.2.4
Linked Disk Blocks
The HD-tree keeps track of the current available
RAM whenever adding or deleting an internal node.
If the RAM is available, the tree grows by creating
internal nodes through the overﬂow processing. Oth-
erwise, the tree stops creating new internal nodes.
Hence, if a leaf node overﬂows after inserting a string,
an extra disk block is linked to the original disk block
to accommodate the overﬂowing data. Consequently,
a search within the leaf node needs to access all linked
disk blocks. Using this approach, the HD-tree works
with any given size of RAM.
2.2.5
Queries
After an HD-tree is created, various queries can be ef-
ﬁciently processed using the tree. Given a database
containing strings κ1, ..., κn, an ExactSearch(α)
retrieves κi such that κi=α, 1
≤
i
≤
n; a
PrefixSearch(α) retrieve κi where α is a preﬁx
of κi; a SubstringSearch(α) retrieves κi where
α is a substring of κi.
Note that in the HD-
tree, ExactSearch(α) equals to PrefixSearch(α♯)
and SubstringSearch(α) is processed by perform-
ing PrefixSearch(α) among all sufﬁx strings of
κ1, ..., κn (Ferragina and Grossi, 1999).
3
EXPERIMENTAL RESULTS
We conducted extensive experiments to analyze the
behavior of the HD-tree and evaluate its performance.
The string databases were generated from TREC
(Voorhees and Harman, 1997). The HD-tree was im-
plemented using C++. Experiments were conducted
on a PC running Linux OS. The disk block size used
in our experiments was 4096 bytes.
Sample database WSJ1 was generated from the
TREC collection, Wall Street Journal 1991, by ﬁrst
removing tags and breaking the text into segments of
5MB each, then extracting unique preﬁxes of the suf-
ﬁx strings at non-space letters for every segment, and
keeping the ﬁrst 32 letters if the preﬁx string is longer
than 32. WSJ1 can be used for keyword-based docu-
ment searches (Baeza-Yates and Ribiero-Neto, 1999)
or substring searches (Gonnet et al., 1991) depending
on the starting boundaries (either words or letters) of
the sufﬁx strings. WSJ1 contained 15 million strings
and each string was associated with a four-byte inte-
ger as the descriptive information. The size of WSJ1
was 252MB.
Table 1: Split heuristics on disk utilization
DBSize(MB)
50
100
150
200
250
SSplit
45.7
44.8
44.6
44.5
44.1
HD-Split
65.1
63.5
63.1
62.7
62.6
Improvement
42.5
41.7
41.5
40.1
42.0
Databases: Samples from WSJ1, Table value: %
3.1
Split Heuristics
One set of experiments is to show the effectiveness
of the split heuristics for building an HD-tree. Table
1 shows the comparison of the disk utilization (using
one disk block for each leaf node) between the SSplit,
which is a B+tree-like approach, and the HD-Split
(see Section 2.2.3). Note that the HD-Split adopted
two heuristics to improve the disk utilization. One is
to distinguish the SGL from the MGL, which allows
the preﬁx range to be “non-continuous”. The other is
to move groups to left or right sibling to avoid a split,
48
Qiang Xue et al.
Figure 2: The SGL and the MGL.

which dynamically adjusts the preﬁx-set of an MGL.
It is shown that the HD-Split increases the disk uti-
lization by more than 40%, which indicates the effec-
tiveness of the grouping mechanism in the HD-Split.
0
0.1
0.2
0.3
0.4
RAM Size as the Percentage of the Database Size
0
5
10
15
20
25
30
Average Number of Links
DB 25MB
DB 50MB
DB 150MB
DB 250MB
DBs: Samples from WSJ1
Figure 3: The relationship between the ANL and the avail-
3.2
Query Performance
As described in Section 2.2.4, using linked disk
blocks, the HD-tree is scalable for any RAM size.
Figure 3 shows the relationship between the average
number of links (ANL) and the available RAM size
as the percentage of the database size (RAM/DB).
The ANL is the total number of linked disk blocks
divided by the number of linked leaf nodes. An ANL
value of zero means that each leaf node occupies one
disk block. It is shown that the ANL decreases as the
RAM/DB increases. Note that there exists a thresh-
old (where the curve becomes ﬂat) in the ﬁgure. The
threshold is almost invariant of database sizes.
0
0.1
0.2
0.3
0.4
0
5
10
15
Number of Disk I/Os per Query
DB 25MB
DB 50MB
DB 150MB
DB 250MB
DBs: Samples from WSJ1
Figure 4: The relationship between the number of I/Os and
When ANL is greater than zero (i.e., the linked disk
blocks are used), the query performance of the HD-
tree is shown to be closely related to the ANL. Curves
in Figures 4 and 5, where the number of I/Os rather
than ANL is used, are similar to those in Figure 3. The
phenomenon of a threshold can be explained by the
following: because of the logarithmic nature of the
tree (i.e., lower level contains less nodes), as the HD-
tree grows, adding the same amount of the RAM (i.e.,
adding certain number of leaf nodes) has less impact
on the selectivity of the tree (i.e., the total number of
leaf nodes). Therefore, when the available RAM is
limited compared to the databases size, it is important
to allocate enough RAM at the threshold point where
the RAM is most effectively utilized.
0
0.1
0.2
0.3
0.4
0.5
0
5
10
15
20
Number of Disk I/Os per Query
DB 50MB
DB 100MB
DB 150MB
DB 250MB
DBs: Samples from WSJ1
Figure 5: The relationship between the number of I/Os and
3.3
Comparisons
In this subsection, we evaluate the performance of the
HD-tree by comparing it with that of the Preﬁx B-
tree. The Preﬁx B-tree is widely adopted by data-
base systems and has been shown to be a practical
technique for indexing large string databases.
The
Preﬁx B-tree we used was implemented by the popu-
lar Berkeley DB (Sleepycat, 2004), which is an open
source database system. As a disk-based index struc-
ture, the Preﬁx B-tree does not use any memory, while
the HD-tree requires certain amount of RAM to keep
its internal nodes.
For a fair comparison, we pro-
vided the same amount of RAM used by the HD-tree
for the Preﬁx B-tree as a cache. The caching algo-
rithm is based on the popular LRU (least-recently-
tree and the Preﬁx B-tree using 1000 queries with dif-
ferent numbers of distinctive queries. This set of ex-
periments was designed to evaluate the effect of the
locality of the query results on the performance of the
The queries are gen-
erated as follows: (1) select a certain number of dis-
tinctive queries to form a query pool; (2) randomly
used) heuristic,whichis used by almost allcommercial
database systems because of its simplicity and effecti-
We ﬁrst compared the disk I/Os between the HD-
veness. The LRU algorithm keeps recently accessed
internal nodes in the RAM to reduce the number of
HD-tree and the Preﬁx B-tree.
disk I/Os.
49
The Hybrid Digital Tree
able RAM as the percentage of the database size.
RAM Size as the Percentage of the Database Size.
the available RAM when the answer size is ﬁxed.
the available RAM when the answer size changes.
RAM Size as the Percentage of the Database Size.

generate 1000 queries from the query pool. In one
extreme case, the 1000 queries are all the same. As
the number of distinctive queries increases, the level
of localities in the query results reduces. The other
extreme is when all 1000 queries are different.
1
10
100
1000
Number of Distinctive Queries
2500
5000
7500
10000
12500
15000
Total Number of Disk I/Os
HD-tree
Prefix B-tree
Figure 6: I/O comparison for different query localities; av-
As shown in Figure 6, the performance of the Pre-
ﬁx B-tree is better when the number of distinctive
queries is small. However, as the number of distinc-
tive queries increases, the performance of the Pre-
ﬁx B-tree deteriorates quickly. The two curves cross
between 10 and 20 distinct queries, where the HD-
tree starts to outperform the Preﬁx B-tree. For 1000
distinctive queries, the HD-tree is almost three times
better than the Preﬁx B-tree in term of the number
of disk I/Os. The results show that the performance
of the Preﬁx B-tree using the LRU caching mecha-
nism is very susceptible to the locality of the query
results. On the other hand, the HD-tree is quite robust
to different queries. We conclude that the HD-tree
performs better as queries become more different. In
the following I/O comparisons, we used 1000 random
distinctive queries.
0
500
1000
1500
2000
RAM Size (KB)
0
1
2
3
4
5
6
Number of Disk I/Os per Query
HD-tree
Prefix B-tree
Figure 7: I/O comparison for different RAM sizes; average
In Figures 7 and 8, we compare the performance of
the HD-tree and the Preﬁx B-tree for different RAM
sizes. In Figure 7, it is shown that the HD-tree not
0
5
10
15
20
25
RAM Size (Mb)
0
5
10
15
20
Number of Disk I/Os per Query
HD-tree
Prefix B-tree
Figure 8: I/O comparison for different RAM sizes; average
only reduces the number of I/Os, but also uses the
RAM more effectively than the caching mechanism
adopted by the Preﬁx B-tree.
For example, as the
RAM increases from 250KB to 1.6MB, the HD-tree
reduces more than 50% of I/Os, but the Preﬁx B-tree
only reduces less than 20% of I/Os. For the given
database WSJ1 (252MB) and 1.6MB of RAM, the
HD-tree reaches its optimal status where each leaf
node occupies only one disk block. In Figure 8, more
RAM to the HD-tree is served as a cache which is
the same as that of the Preﬁx B-tree. It is shown that
the HD-tree is continually better than the Preﬁx B-
tree when the RAM is largely available. In Figure 9,
we compare the number of I/Os for different query
lengths. It is shown that the HD-tree performs in-
creasingly better than the Preﬁx B-tree as the query
string length increases. Since the Preﬁx B-tree uses
the same amount of RAM as that of the HD-tree to
cache internal nodes, we conclude that the hybrid
RAM/disk-based index structure (e.g., the HD-tree)
is better than the disk-based structure combined with
caching (e.g., the Preﬁx B-tree plus LRU caching),
especially when queries are more distinctive.
2
3
4
5
6
7
8
9
Average Query String Length
1
10
100
1000
Number of Disk I/Os per Query
HD-tree
Prefix B-tree
Figure 9: I/O comparison for different query lengths; y-axis
50
Qiang Xue et al.
erage query length is 6.
query string length is 6.
query string length is 8.
is in Logarithmic scale.
Finally, we compared the HD-tree with the Preﬁx

1
10
100
1000
Number of Distinctive Queries
25
50
75
100
125
Total Running Time (seconds)
HD-tree
Prefix B-tree
Figure 10: Running time comparison; average query string
the RAM processing time and the I/O time.
The
experiments were conducted in the same computing
environment (a Linux PC with 512MB RAM and
1.8GHz Pentium 4 processor). Figure 10 shows the
running time of the HD-tree and the Preﬁx B-tree
for 1000 queries with different numbers of distinc-
tive queries. We notice that the actual running time of
the HD-tree is comparable to that of the Preﬁx B-tree
even when the 1000 queries are the same. The rea-
son is that with a large amount of RAM available, the
operating system provides LRU caching for the HD-
tree as well. The HD-tree is shown to be increasingly
faster than the Preﬁx B-tree as the number of distinc-
tive queries increases. For 1000 distinctive queries,
the HD-tree is more than one magnitude faster than
the Preﬁx B-tree.
4
CONCLUSION
There is an increasing demand for efﬁcient index-
ing techniques to support various types of queries
on large string databases.
Most existing string in-
dexing techniques are either RAM-based or disk-
based. RAM-based index structures are not suitable
for string matching queries on large databases when
only a limited amount of RAM is available. Disk-
based structures, on the other hand, can index large
databases but usually do not fully utilize the available
RAM.
The HD-tree is proposed as a novel hybrid
RAM/disk-based structure, taking advantage of the
strengths of both RAM-based and disk-based struc-
tures. The HD-tree not only scales well with the sizes
of the RAM and the database, but also is efﬁcient
for various types of queries. The experimental results
show that the HD-tree outperforms the Preﬁx B-tree
for preﬁx and substring searches. For random distinc-
tive queries, the number of disk I/Os is reduced by a
factor of two to three, while the running time is re-
duced in an order of magnitude. Therefore, we con-
such as the HD-tree is promising for supporting efﬁ-
cient searches in large string databases whose indexes
cannot ﬁt entirely in the RAM.
REFERENCES
Baeza-Yates, R. and Ribiero-Neto, B. (1999). Modern In-
formation Retrieval. Addison Wesley Longman Pub-
lishing Co. Inc.
Bayer, R. and McCreight, E. M. (1972). Organization and
maintenance of large ordered indexes. Acta Informat-
ica, 1(3):173–189.
Bayer, R. and Unterauer, K. (1977). Preﬁx b-trees. ACM
Trans. Database Syst., 2(1):11–26.
Clark, D. R. and Munro, J. I. (1996). Efﬁcient sufﬁx trees on
secondary storage. In Proceedings of the seventh an-
nual ACM-SIAM symposium on Discrete algorithms,
pages 383–391, Atlanta, Georgia, United States. Soci-
ety for Industrial and Applied Mathematics.
Comer, D. (1979). Ubiquitous b-tree. ACM Comput. Surv.,
11(2):121–137.
Fagin, R., Nievergelt, J., Pippenger, N., and Strong, H. R.
(1979). Extendible hashing a fast access method for
dynamic ﬁles. ACM Trans. Database Syst., 4(3):315–
344.
Ferragina, P. and Grossi, R. (1999). The string b-tree: A
new data structure for string search in external mem-
ory and its applications.
J. Assoc. Comput. Mach.,
46(2):236–280.
Gonnet, G. H., Baeza-Yates, R. A., and Snider, T. (1991).
Lexicographical indices for text: Inverted ﬁles vs. pat
trees.
Technical Report OED-91-01, University of
Waterloo.
Manber, U. and Myers, G. (1990). Sufﬁx arrays: a new
method for on-line string searches.
In Proceedings
of the ﬁrst annual ACM-SIAM symposium on Discrete
algorithms, pages 319–327. Society for Industrial and
Applied Mathematics.
McCreight, E. M. (1976). A space-economical sufﬁx tree
construction algorithm. J. ACM, 23(2):262–272.
Morrison, D. R. (1968). Patricia practical algorithm to re-
trieve information coded in alphanumeric. J. ACM,
15(4):514–534.
Sleepycat (2004). Berkeley db. http://www.sleepycat.com/.
Voorhees, E. M. and Harman, D. (1997). Overview of the
sixth text retrieval conference (trec-6). In Proceedings
of the Sixth Text REtrieval Conference, pages 1–24.
NIST Special Publication.
Weiner, P. (1973). Linear pattern matching algorithms. In
14th Annual Symposium on Switching and Automata
Theory, pages 1–11. IEEE.
Xue, Q., Pramanik, S., Qian, G., and Zhu, Q. (2004). The
hybrid ram/disk-based index structure. Technical re-
port, Department of CSE, Michigan State University.
clude that a hybrid RAM/disk-based index structure
51
The Hybrid Digital Tree
length is 6.
B-tree in terms of total running time including both

MUSICAL RETRIEVAL IN P2P NETWORKS UNDER THE
WARPING DISTANCE∗
Ioannis Karydis, Alexandros Nanopoulos, Apostolos N. Papadopoulos and Yannis Manolopoulos
Department of Informatics, Aristotle University, 54124 Thessaloniki, Greece
Email: {karydis,alex,apostol,manolopo}@delab.csd.auth.gr
Keywords:
Music Information Retrieval, Similarity Searching, Peer-to-peer Networks, Multimedia Databases.
Abstract:
Peer-to-peer (P2P) networks present the advantages of increased size of the overall database offered by a
the network nodes, fault-tolerance support to peer failure, and workload distribution. Music ﬁle storage and
exchange has long abandoned the traditional centralised server-client approach for the advantages of P2P
networks. In this paper, we examine the problem of searching for similar acoustic data over unstructured
decentralised P2P networks. As distance measure, we utilise the time warping. We propose a novel algorithm,
which efﬁciently retrieves similar audio data. The proposed algorithm takes advantage of the absence of
overhead in unstructured P2P networks and minimises the required trafﬁc for all operations with the use of
an intelligent sampling scheme. Detailed experimental results show the efﬁciency of the proposed algorithm
compared to an existing baseline algorithm.
1
INTRODUCTION
The increasing popularity of the availability of mu-
sic in computer ﬁles gives further impulse to the de-
velopment of digitised music databases as well as to
new methods for Music Information Retrieval (MIR)
in these collections. Although abundantly used, even
nowadays, the traditional metadata (title, composer,
performer, genre, date, etc.) of a music object give
rather minimal information about the actual content
of the music object itself. Their use aims solely in
avoiding including musical content in the query. On
the other hand, queries based on humming (using a
microphone) or on a small piece of musical ﬁle, are
a more natural approach to MIR. This type of queries
lies within the Content-Based MIR (CBMIR). In CB-
MIR, an actual music piece is required in order to
compare its content with the content of the music
pieces already available in the database.
As far as the type of the database is concerned,
music ﬁle storage and exchange has long abandoned
the traditional centralised server-client approach for
the advantages of the peer-to-peer networks (P2P).
Within the advantageous qualities of the P2P net-
works lies the increased size of the overall database
∗Research funded by the IRAKLITOS national program
(2003-2005) under the EPEAEK framework
offered by a P2P network, its fault tolerance support
to peer failure by other peers and the workload distri-
bution over a network of available CPUs, since CB-
MIR is computationally highly intensive. Nonethe-
less, the very advantages of the P2P network are the
same parameters that make P2P information retrieval
much more complex than the traditional search meth-
ods. That is, the lack of central repository for the doc-
uments to be retrieved, the large number of documents
available and the dynamic character of the network,
introduce an increased degree of difﬁculty in the re-
trieval process.
Among numerous classiﬁcations, P2P networks
can be classiﬁed based on the control over data lo-
cation and network topology in unstructured, loosely
structured and highly structured (Li and Wu, 2004).
Unstructured P2P networks follow no rule in where
data is stored while the network topology is arbi-
trary (e.g., Gnutella). The absence of structure al-
lows for resilience in dynamic environments (peer
join/leave) while no guaranties can be given on the
retrieval of existing documents. Moving towards in-
creased structure, both the probability of retrieving
existing documents and the overhead of handling peer
join-leave augment (e.g., Freenet and Chord). Addi-
tionally, P2P networks can also be classiﬁed accord-
ing to the number of central directories of document
53
C.-S. Chen et al. (Eds.), Enterprise Information Systems VII, 53–60.
© 2006 Springer.

locations in centralised (e.g., Napster), hybrid (e.g.,
Kazaa) and decentralised (e.g., Chord). Centralised
P2P networks are subject to the same drawbacks for
which the traditional server-client model was origi-
nally abandoned (network failures due to central peer
failure, impaired scalability, joining/leaving of peers
not easily handled, possible undesirable dominion of
controllers). For these reasons, we focus on decen-
tralised unstructured P2P networks, which overcome
the aforementioned drawbacks. The absence of struc-
ture was selected for the looseness of control over the
data location, that is each peer can share its own doc-
uments without hosting any documents of other peers
due to locality restrains.2
Music representation can primarily be separated in
two classes: the symbolic representation (MIDI for-
mat) and the acoustic representation (audio format -
wav, mp3).
The focus of this work is on acoustic
data, thus a musical piece can be considered as a time
series of signal intensity over time. To measure the
similarity of two musical pieces, we utilise the Dy-
namic Time Warping (DTW) method. The main ﬂex-
ibility of the DTW method is its capability to with-
stand distortion of the comparing series in the time
axis. Accordingly, it allows for two locally out of
phase time series that are nevertheless similar to align
in a non-linear manner. Since different performances
of the same musical piece may include locally differ-
entiated tempo, DTW seems a natural choice for this
problem (Large and Palmer, 2002). For this reason, it
has been recently proposed for shake of MIR in cen-
tralised environments (Zhu and Shasha, 2003; Maz-
zoni and Danneberg, 2001; Jang et al., 2001; Adams
et al., 2004).
In this paper, we focus on the problem of search-
ing, based on DTW, for similar acoustic data over un-
structured decentralised P2P networks. The technical
contributions of this paper are summarised as follows:
• The development of a novel algorithm that efﬁ-
ciently retrieves audio data similar to an audio
query in an decentralised unstructured P2P net-
work.
• The proposed algorithm takes advantage of the ab-
sence of overhead in unstructured P2P networks
and efﬁciently minimises the required trafﬁc for all
operations with the use of an intelligent sampling
scheme on the lower and upper bounds used. The
proposed algorithm has such a design that no false
negative results occur.
2We must notice that with the examined framework we
refer to applications that support content sharing for legal
subscribers (e.g., iTunes). Moreover, it is interesting to no-
tice that the proposed approach can be adopted as a means
of identiﬁcation of illegal sharing, by ﬁnding sites that share
unregistered content.
• The detailed experimental results which show the
efﬁciency of the proposed algorithm, and the per-
formance gains compared to an existing baseline
algorithm.
The rest of the paper is organised as follows. Sec-
tion 2 describes related work. Section 3 provides a
complete account of the algorithm proposed in this
paper. Subsequently, Section 4 presents and discusses
the experimentation and results obtained. Finally, the
paper is concluded in Section 5.
2
BACKGROUND AND RELATED
WORK
2.1
Searching Methods in
Unstructured P2P Networks
In this section we summarise a number of different
searching methods for decentralised unstructured P2P
networks.
Initially, we examine the Breadth-First
Search (BFS) algorithm. In the BFS, a query peer
Q propagates the query q to all its neighbor peers.
Each peer P receiving the q initially searches its lo-
cal repository for any documents matching q and then
passes on q to all its neighbors.
In case a P has
a match in its local repository then a QueryMatch
message is created containing information about the
match. The QueryMatch messages are then transmit-
ted back, using reversely the path q travelled, to Q.
Finally, since more than one QueryMatch messages
have been received by Q, it can select the peer with
best connectivity attributes for direct downloading of
the match. It is obvious that the BFS sacriﬁces perfor-
mance and network trafﬁc for simplicity and high-hit
rates. In order to reduce network trafﬁc, the TTL para-
meter is used (see Section 2). In a modiﬁed version of
this algorithm, the Random BFS (RBFS) (Kalogeraki
et al., 2002), the query peer Q propagates the query q
not to all but at a fraction of its neighbor peers.
In an attempt to rectify the inability of the RBFS to
select a path of the network leading to large network
segments, the >RES algorithm was developed (Yang
and Garcia-Molina, 2002). In this approach, a node Q
propagates the q to k neighboring peers, all of which
returned the most results during the last m queries,
with k and m being conﬁgurable parameters. >RES
can be characterised as quantitative than qualitative,
since it does not consider the content of the query.
With ISM (Kalogeraki et al., 2002), on the other
hand, for each query, a peer propagates the query q to
the peers that are more likely to reply the query based
on the following two parameters; a proﬁle mechanism
and a relevance rank. The proﬁle is is built and main-
tained by each peer for each of its neighboring peers.
The information included in this proﬁle consists of the
54
Ioannis Karydis et al.

t most recent queries with matches and their matches
as well as the number of matches the neighboring peer
reported. Obviously, the strong point of the ISM ap-
proach is in environments that show increased degree
of document locality.
2.2
MIR in P2P Networks
The ﬁeld of combined CBMIR and P2P networks is
deﬁnitely very young as the inaugural research pa-
per dates back in 2002 (Wang et al., 2002). Despite,
the limited number of works that exist, are presented
thereinafter.
In this ﬁrst attempt, the authors of (Wang et al.,
2002) present four P2P models for CBMIR. The four
models include all centralised, decentralised and hy-
brid categories. Accordingly, the authors of (Wang
et al., 2002) propose a retrieval acceleration algorithm
based on difference in pitch between two tones of mu-
sic and a result ﬁltering method relying on replication
removal techniques.
Additionally, the authors pro-
pose an architecture of a CBMIR P2P system, that
falls within the hybrid category of P2P systems.
Another research based on a hybrid conﬁguration
is presented in (Tzanetakis et al., 2004). Therein the
authors propose a system that utilises both manually
speciﬁed attributes (artist, album, title, etc.) and ex-
tracted features in order to describe the musical con-
tent of a piece. The underlying P2P network is a DHT-
based system. In such systems each node is assigned
with a region in a virtual address space, while each
shared document is associated with a value of this ad-
dress space. Thus, locating a document requires a key
lookup of the node responsible for the key.
The author in (Yang, 2003) proposed the utilisation
of the feature selection and extraction process that is
described in (Yang, 2002) for CBMIR in a decen-
tralised unstructured P2P system. The research con-
siders both a replicated database and a general P2P
scenario, while special attention is given on the con-
trol of the workload produced at queried peers during
query time. Each query is divided into two phases,
the ﬁrst of which includes only a subpart of the actual
query vectors, in order to distinguish high probability
response peers. Accordingly, a peer ranking occurs
and the full query vectors are sent to all peers. Given
that a peer has free CPU resources, it decides whether
to process a query or not based on the ranking that
the speciﬁc query received. It is obvious that this ap-
proach produces large network trafﬁc, since the full
query vectors are sent to all peers, instead of the most
promising.
2.3
DTW Background Information
The efﬁcient processing of similarity queries requires
the addressing of the following important issues:
• the deﬁnition of a meaningful distance measure
D(S, C) in order to express the similarity between
two time series objects S and C,
• the efﬁcient representation of time series data, and
• the application of an appropriate indexing scheme
in order to quickly discard database objects that can
not contribute to the ﬁnal answer.
One of the most fundamental research issues in
time series is the deﬁnition of meaningful measures
towards time series similarity expression. Given two
time series S and C the problem is to deﬁne a distance
measure D(S, C) which expresses the degree of sim-
ilarity between S and C. One of the most widely used
distance measures for time series is the Euclidean dis-
tance (L2 norm), which has the restriction that both
series must be of the same length. Given two time se-
ries S and C of length N, the Euclidean distance is
deﬁned as follows:
Deuclidean =




N

i=1
(Si −Ci)2
(1)
where Si, Ci are the value of S and C for the i-th time
instance.
The Euclidean distance has been widely
used as a similarity measure in time series literature
(Agrawal et al., 1993; Faloutsos et al., 1994; Chan
and Fu, 1999; Kontaki and Papadopoulos, 2004), due
to its simplicity.
Several alternative distance functions have been
proposed in order to allow translation, rotation and
scaling invariance. Consider for example the time se-
ries depicted in Figure 1. Note that although all time
series have the same shape, they will be considered
non-similar if the Euclidean distance is used to ex-
press similarity. Translation, rotation and scaling in-
variance are studied in (Agrawal et al., 1995; Yi et al.,
2000; Chan and Fu, 1999; Yi and Faloutsos, 2000).
Taking into consideration that the Euclidean dis-
tance does not always meet the application’s require-
ments, Dynamic Time Warping (DTW) has been pro-
posed as a more robust similarity measure.
DTW
can express similarity between two time series even
if they are out of phase in the time axis, or they
do not have the same length.
The DTW distance
DDT W (S, C) between time series S and C is es-
sentially a way to map S to C and vice-versa. This
process is also known as alignment of time series. If
S is of length N and C is of length M, then the dis-
tance DDT W can be evaluated by using the following
method:
1. An N × M matrix is constructed, where the cell
in the i-th row and the j-th column contains the
distance d(Si, Cj) = (Si −Cj)2.
2. A warping path is deﬁned which is a contiguous
set of matrix cells that deﬁnes a mapping between
elements of S and elements of C.
55
Musical Retrieval in P2P Networks Under the Warping Distance

Although there are many warping paths that map
S to C, what is required is to determine the most
promising one, by trying to optimise the cumula-
tive distance γ(i, j) in each cell of the warping path.
Therefore, the following recurrence is deﬁned:
γ(i, j) = d(Si, Cj) + min{γ(i −1, j −1),
γ(i −1, j), γ(i, j −1)}
(2)
Figure 1 illustrates an example of two time series
aligned by means of the Euclidean distance (Figure
1(a)) and by DTW distance (Figure 1(b)). It is evident
that the two time series are similar but their phases are
different. However, their similarity can not be cap-
tured by the Euclidean distance.
(a) alignment using Euclidean distance
(b) alignment using DTW distance
Figure 1: Time series alignment with Euclidean and DTW
The most important disadvantage of the DTW
method is that it does not satisfy the triangular in-
equality, which is a desirable property for construct-
ing efﬁcient indexing schemes and pruning the search
space. Moreover, the calculation of DDT W (S, C) is
signiﬁcantly more CPU intensive than the calculation
of DEuclidean(S, C). Therefore, an interesting direc-
tion for performance improvement is the deﬁnition of
a lower bound, in order to take advantage of indexing
schemes and avoid the computation of DTW when
there is a guarantee that the two time series are not
similar. In this work we utilise the lower bound pro-
posed in (Keogh and Ratanamahatana, 2004) which is
termed LB Keogh and it is deﬁned as follows:
LB Keogh(S,C) =





N

i=1



(Ci −Ui)2,
if Ci > Ui
(Ci −Li)2,
if Ci < Li
0,
otherwise
(3)
where U and L is the upper and lower bound respec-
tively for the time series S. Essentially, for each i,
the upper bound guarantees that Ui ≥Si and the
lower bound guarantees that Li ≤Si. In (Keogh
and Ratanamahatana, 2004) it has been proven that
LB Keogh(S, C) ≤DDT W (S, C), and therefore
the distance measure LB Keogh(S, C) can be effec-
tively used for pruning, resulting in considerably less
number of DDW T computations.
3
PROPOSED METHOD
3.1
Overview
As explained, P2P searching algorithms are based on
the following scheme: the node that poses the query
examines its contents and ﬁnds documents that sat-
isfy the query. Then, it selects a subset of its peers and
propagates the query to them. Each peer in its turn ex-
amines its contents to ﬁnd qualifying documents, and
then propagates the query to a subset of its peers. To
avoid the involvement of a prohibitively large number
of nodes, the propagation of queries is restrained by a
MaxHop parameter, which determines the number of
peers a query should be forwarded. (The MaxHop pa-
rameter is equivalently called Time To Leave (TTL).)
In the context examined in this paper, each query
searches for similar music documents (i.e., query
by content and not by metadata, like title, artist,
etc.), where similarity is measured through DTW. To
speedup searching, we use lower bounds (LB) that
have been developed for DTW, the LB Keogh in par-
ticular (Keogh, 2002). As mentioned, LB Keogh is
based on a bounding envelope, which is deﬁned by
the U and L sequences (see Section 2.3). Therefore,
in this context, the information that is propagated be-
tween nodes comprises the U and L sequences. A
node that receives these sequences, computes the LB
value between its documents and the envelope. When
a LB value is smaller than the user-speciﬁed similar-
ity threshold, then the actual query sequence is prop-
agated to this node3 and the actual DTW distance is
computed between the query and the corresponding
document.
The queries we consider constitute music phrases,
that is, excerpts of the music documents that are a type
of units of music information4. This holds especially
in the context of query by humming, where users tend
to hum a piece that is (i) relatively short, (ii) well iden-
tiﬁed and separated within a song. The identiﬁcation
of phrases can be done following the methodology
presented in (Zhu and Shasha, 2003). In particular,
a transcription algorithm (Klapuri, 2004) can produce
the pitch information of the acoustic sequence. Time
intervals, corresponding to phrases in the pitch infor-
mation, are detected in between the time instances
that silence exists (the same time intervals produce
the phrases in the corresponding acoustic sequence).
In summary, we are interested in ﬁnding music doc-
uments that contain phrases similar to the query se-
3The query can be directly propagated from the node
that initially posed the query, since the currently visited
node always knows the address of this initial node.
4A minimum-length portion of the musical piece that is
meaningfully independent and complete within a piece of
music.
56
Ioannis Karydis et al.
distances.

quence. Similarity through DTW is suitable in this
context, since the properties of DTW help in alleviat-
ing errors that humming produces.
An important observation is that acoustic data tend
to be very large. Although queries are music phrases
(i.e., parts of the music sequences), the number of el-
ements in a phrase of even few seconds can be several
hundred thousands. The length of the U and L se-
quences is equal to the length of the query sequence.
This means that a straightforward approach, which di-
rectly propagates U and L sequences between nodes,
will result into an extremely large trafﬁc over the P2P
network. Moreover, when the length of the envelope’s
sequences are large, the computation of LB in each
node can become rather costly. This violates the need
of a P2P network to burden the participating nodes as
little as possible. Notice that the aforementioned re-
quirements are not present in other contexts, like the
searching of similar text documents over a P2P net-
work, where queries consist of up to some tenths of
terms.
We propose a two-fold scheme which signiﬁcantly
reduces the trafﬁc over the P2P network when query-
ing music documents by content. The scheme works
as follows:
• It reduces the length of the envelope’s sequence
by sampling them. However, plain sampling can
be ineffective, since it leads to underestimation of
LB. For this reason, we describe a novel sampling
method to reduce the length of the sequences with-
out signiﬁcantly affecting the computation of LB.
Additionally, we are interested in not introducing
false-negatives due to the use of sampling.
• It uses (whenever possible) a compact representa-
tion of the sampled sequences of the envelope. The
representation comprises a kind of compression for
the sequences, but it does not burden the nodes of
the P2P network with the cost of decompression.
If the latter is not undesirable, further compression
can be achieved through the use of existing meth-
ods. We do not explore this direction, since it does
not affect the relative performance of the proposed
scheme against the plain one that directly propa-
gates the envelope (i.e., the performance of both
methods will be equally improved).
In the following we describe the aforementioned
issues in more detail.
We have to notice that, for
simplicity, we use the BFS algorithm as a basis for
searching over the P2P network. The examination of
the proposed scheme in more advanced searching al-
gorithms (e.g., ISM, >RES) is actually a matter of
current research. In a larger version of this work, we
will include a comparison that considers such search-
ing algorithms as well.
3.2
Sampling and Representation
Let the considered phrase length be equal to N. The
length of each query Q, and therefore of its upper (U)
and lower (L) sequences, will also be equal to N. We
would like to sample U and L, so as to obtain two se-
quences U ′ and L′, each of length M ≪N. Initially,
we assume that uniform sampling is performed. In
this case, we simply select each time the (i × N/M)-
th element of U and L, where 1 ≤i ≤M. When we
compute the LB Keogh between the query sequence
Q and a data sequence, we consider each phrase C of
length N in Q. Each phrase has to be sampled in the
same way as U and L. This leads to a sampled phrase
C′. Therefore, we get a lower-bound measure LB′,
given as:
LB′
=





M

i=1



(C′
i −U ′
i)2,
if C′
i > U ′
i
(C′
i −L′
i)2,
if C′
i < L′
i
0,
otherwise
(4)
In the aforementioned equation, the third case (i.e.,
when L′
i ≤Ci ≤U ′
i) does not contribute in the com-
putation of LB′. The problem of uniform sampling is
that, as it selects elements without following any par-
ticular criterion, it tends to select many elements from
U and L that result to this third case. Therefore, LB′
may become a signiﬁcantly bad underestimation of
LB that would have been computed if sampling was
not used. The underestimation of the lower-bound
value will result to an increase in false-alarms, thus
incurring high trafﬁc.
To overcome this problem, we propose an alterna-
tive sampling method. We sample U and L separately.
Initially, we store the elements of U in ascending or-
der. In U ′ we select the ﬁrst M elements of this or-
dering. Respectively, we sort L in descending order
and we select the ﬁrst M elements in L′. The intu-
ition is that the selection of the smallest M values of
U, helps in increasing the number of occurrences of
the ﬁrst case (i.e., when C′
i > U ′
i), since the smallest
the value of U ′
i is, the more expected is to have a C′
i
larger than it. An analogous reasoning holds for the
sampling of L′.
It is easy to see the following:
Lemma 1 The sampling of U and L does not produce
any false negatives.
Proof. While computing LB′, due to sampling, the
ﬁrst and second cases of Equation 4 occur less times
than while computing LB (i.e., without sampling).
Therefore, LB′ ≤LB. Since LB ≤D (where D
the actual distance computed with DTW), we have:
LB′ ≤D. Thus, no false negatives are produced. □
The separate sampling of U and L presents the re-
quirement of having to store the positions from which
57
Musical Retrieval in P2P Networks Under the Warping Distance

elements are being selected in U ′ and L′.
If the
positions are stored explicitly, then this doubles the
amount of information kept (2M numbers for stor-
ing U ′ and L′ and additional 2M numbers for stor-
ing the positions of selected elements).
Since this
information is propagated during querying, trafﬁc is
increased. For this reason we propose an alternative
representation. To represent U ′, we use a bitmap of
length N (the phrase length). Each bit corresponds to
an element in U. If an element is selected in the sam-
ple U ′, then its bit is set to 1, otherwise it is set to 0.
Therefore, the combination of the bitmap and the M
values that are selected in U ′ are used to represent U ′.
The same is applied for L′. This representation is efﬁ-
cient: the space required for U ′ is M +⌈N/8⌉bytes.5
The plain representation requires 5M bytes (since it
requires only one integer, i.e., 4 bytes, to store the po-
sition of each selected element). Thus, the proposed
method is advantageous when N < 32M, i.e., for
sample larger than about 3% (our experiments show
that samples with size 10% are the best choice).
3.3
The Similarity Searching
Algorithm
As previously explained, the similarity searching al-
gorithm is on the basis of breadth-ﬁrst-search over the
nodes of the P2P network. The algorithm that uses the
proposed sampling and representation methods, is de-
noted as BFSS (breadth-ﬁrst-search with sampling).
The pseudo-code for BFSS is given in Figure 2. Each
time, the current node n is considered. A TTL value
denotes how many valid hops are remaining for n,
whereas Ts is the user-deﬁned similarity threshold. It
is assumed that sequences U ′ and L′ carry also the
associated bitmaps.
5Each element in an acoustic sequence is in the range
0-255, thus it requires one byte.
Procedure BFSS(Node n, int TTL, Sequence U ′,
Sequence L′, ﬂoat Ts)
begin
1.
foreach data sequence D in n
2.
foreach phrase C of D
3.
l = LB′(C, U ′, L′)
4.
if l < Ts
5.
get query sequence
6.
compute actual DTW distance, D,
between phrase C and query sequence
7.
if D ≤Ts
8.
include C in answer set
9.
if TTL > 0
10.
foreach peer p of n that has not been visited yet
11.
BFSS(p, TTL-1, U ′, L′, Ts)
end
4
EXPERIMENTAL RESULTS
The performance of the considered similarity search-
ing algorithms was compared through simulation.
The P2P network had 100 nodes and the average num-
ber of neighbors for each node was a random vari-
able with average value equal to 7 (this kind of topol-
ogy is called logarithmic). We used 500 real acoustic
sequences, which correspond to various pop songs.
Each song was sampled at 11 KHz and the average
duration was about 5 minutes. To represent the fact
that music songs (especially popular ones) are shared
among several nodes, we replicated each sequence.
The number of replications for each sequence was
randomly variable with average value equal to 10.
The evaluation metric is the average trafﬁc (mea-
sured in MB) that each query incurs. The parameters
we examine are: the sample size, query size (length
of query sequence), query range (the user-deﬁned
threshold for similarity), and TTL value (max allowed
number of hops).
In our ﬁrst experiment, we focused on BFSS
and compared the proposed sampling method (this
method is denoted as BFSS) against uniform sam-
pling (this method is denoted as BFSS-UNI). The re-
sults are depicted in Figure 3. Figure 3a illustrates the
relative trafﬁc between BFSS and BFSS-UNI (i.e., the
trafﬁc of the latter is normalised w.r.t. the trafﬁc of the
former) against the query range. As shown, BFSS-
UNI incurs about twice the trafﬁc that BFSS does. As
already explained, this is due to the fact that uniform
sampling produces a bad underestimation of the lower
bound value. This can be further understood when
examining the discrepancy, denoted as error, between
the bounds produced by BFSS and BFSS-UNI, and
the actual bound produced by LB Keogh. The relative
error between BFSS and BFSS-UNI (i.e., the latter is
normalised w.r.t. the former) is given in Figure 3b,
Evidently, the movement of the actual query se-
quence from the node that commenced the query to
the currently visited node, increases the trafﬁc (not
being sampled, the query sequence has rather large
length). For this reason, it is important not to have a
large number of false-alarms.
The algorithm that does not use sampling (denoted
as BFS) may produce less false-alarms. However, be-
tween each pair of peers it has to propagate U and L
sequences, with length equal to the one of the query
sequence. Therefore, it is clear that there is a trade-off
between the number of additional false-alarms pro-
duced due to sampling and the gains in trafﬁc from
propagating sampled (i.e., smaller) envelopes. This
trade-off is examined through the experimental results
in the following section.
58
Ioannis Karydis et al.
Figure 2: The BFSS algorithm.

against the query size. The error of BFSS-UNI ranges
between 1.3 times the error of BFSS (for smaller
queries) and 2.8 times (for medium sized queries).
1
1.3
1.6
1.9
2.2
2.5
2.8
316
548
707
837
949
query range
rel. traffic
BFSS-UNI
BFSS
(a)
1
1.3
1.6
1.9
2.2
2.5
2.8
20
40
60
80
100
150
200
500
query size (x1000)
rel. error
BFSS-UNI
BFSS
(b)
Figure 3: BFSS vs. BFSS-UNI (a) Relative trafﬁc (b) Rela-
We now move on to compare BFSS against BFS
(i.e., the method that does not use any sampling at
all). For BFSS we examined several sample sizes.
The results are depicted in Figure 4, whereas BFS has
a constant value, as it does not use sampling. In Fig-
ure 4a, TTL was set to 4, query size was 100,000, and
query range was set to 0 (i.e., exact match). As shown,
for very small samples (with 1,000 elements), BFS
performs better. This is expected, since the use of a
very small sample affects BFSS by resulting to a large
number of false alarms (due to bad underestimation of
lower bound values), which increase trafﬁc. However,
by increasing the sample size, BFSS becomes better
and clearly outperforms BFS. It is interesting to no-
tice that the best performance is for sample size equal
to 10,000 (i.e., 10% of the original query size). Fi-
nally, for large sample sizes, both methods converge
to the same trafﬁc. Analogous results are obtained for
the case where TTL is set to 5. It also worths noticing
that the trafﬁc of BFS is signiﬁcantly more increased
than the trafﬁc of BFSS does, compared to the case
when TTL was 4.
Next, we compared BFSS against BFS for varying
query size and query range. Figure 5a illustrates the
TTL = 4
0
5
10
15
20
25
1
5
10
25
100
sample size (x1000)
traffic (MB)
BFSS
BFS
(a)
TTL = 5
0
5
10
15
20
25
1
5
10
25
100
sample size (x1000)
traffic (MB)
BFSS
BFS
(b)
Figure 4: BFSS vs. BFS (a) Trafﬁc (in MB) when TTL=4
results for the former case. The size of sample for
BFSS was set each time to 10% of query size, TTL
was set to 4 and query range was set to 0. As shown,
BFSS clearly outperforms BFS in all cases, except
for rather small queries. Again, for very very small
queries, the resulting sample is very small and many
false-alarms are produced. Finally, Figure 5b depicts
the results for the latter case (varying query range).
Query size was set to 100,000, sample size was 25%,
and TTL was set to 5. BFSS clearly compares favor-
ably with BFS.
5
CONCLUSIONS
We have presented a novel algorithm, which efﬁ-
ciently retrieves similar audio data. The proposed al-
gorithm takes advantage of the absence of overhead
in unstructured P2P networks and minimises the re-
quired trafﬁc for all operations with the use of an
intelligent sampling scheme. Additionally, the algo-
rithm has such a design that no false negative results
occur. Detailed comparative evaluation to an already
existing algorithm showed signiﬁcantly reduction in
the trafﬁc produced by a query.
59
Musical Retrieval in P2P Networks Under the Warping Distance
tive error w.r.t. actual LB Keogh value.
(b) Trafﬁc (in MB) when TTL=5.

0
5
10
15
20
25
25
50
100
150
200
query size (x1000)
traffic (MB)
BFS
BFSS
(a)
10
15
20
25
30
35
141
200
245
283
316
query range
traffic (MB)
BFS
BFSS
(b)
Figure 5: BFSS vs. BFS w.r.t. (a) query size, (b) query
Further work is oriented towards the examination
of the proposed scheme in more advanced searching
algorithms (e.g., ISM, >RES).
REFERENCES
Adams, N. H., Bartsch, M. A., Shifrin, J. B., and Wake-
ﬁeld, G. H. (2004). Time series alignment for music
information retrieval. In Proceedings 5th ISMIR.
Agrawal, R., Faloutsos, C., and Swami, A. (1993). Efﬁcient
similarity search in sequence databases. In Proceed-
ings 4th FODO, pages 69–84.
Agrawal, R., Lin, K. I., Sawhney, H. S., and Swim, K.
(1995). Fast similarity search in the presence of noise,
scaling, and translation in time-series databases. In
Proceedings 21st VLDB.
Chan, K. and Fu, A. W. (1999). Efﬁcient time series match-
ing by wavelets. In Proceedings 15th ICDE, pages
126–133.
Faloutsos, C., Ranganathan, M., and Manolopoulos, Y.
(1994).
Fast subsequence matching in time-series
databases. In Proceedings ACM SIGMOD, pages 419–
429.
Jang, J. S. R., Lee, H. R., and Chen, J. C. (2001). Super
mbox: An efﬁcient/effective content-based music re-
trieval system. In Proceedings 9th ACM MM, pages
636–637.
Kalogeraki, V., Gunopulos, D., and Zeinalipour-Yazti, D.
(2002).
A local search mechanism for peer-to-peer
networks. In Proceedings 11th CIKM, pages 300–307.
Keogh, E. (2002). Exact indexing of dynamic time warping.
In Proceedings 28th VLDB, pages 406–417.
Keogh, E. and Ratanamahatana, A. N. (2004). Exact index-
ing of dynamic time warping. Knowledge and Infor-
mation Systems.
Klapuri, A. (2004). Automatic music transcription as we
know it today. Journal of New Music Research.
Kontaki, M. and Papadopoulos, A. N. (2004). Similarity
search in streaming time sequences. In Proceedings
16th SSDBM.
Large, E. and Palmer, C. (2002). Perceiving temporal regu-
larity in music. Cognitive Science, 26(1):1–37.
Li, X. and Wu, J. (2004). Searching techniques in peer-to-
peer networks. In Handbook of Theoretical and Algo-
rithmic Aspects of Ad Hoc, Sensor, and Peer-to-Peer
Networks. CRC Press.
Mazzoni, D. and Danneberg, R. B. (2001). Melody match-
ing directly from audio. In Proceedings 2nd ISMIR,
pages 17–18.
Tzanetakis, G., Gao, J., and Steenkiste, P. (2004). A scal-
able peer-to-peer system for music information re-
trieval. Computer Music Journal, 28(2):24–33.
Wang, C., Li, J., and Shi, S. (2002). A kind of content-based
music information retrieval method in a peer-to-peer
environment. In Proceedings 2nd ISMIR, pages 178–
186.
Yang, B. and Garcia-Molina, H. (2002). Improving search
in peer-to-peer networks. In Proceedings 10th ACM
MM, pages 5–15.
Yang, C. (2002). Efﬁcient acoustic index for music retrieval
with various degrees of similarity.
In Proceedings
10th ACM MM, pages 584–591.
Yang, C. (2003). Peer-to-peer architecture for content-based
music retrieval on acoustic data. In Proceedings 12th
WWW, pages 376–383.
Yi, B. K. and Faloutsos, C. (2000). Fast time sequence in-
dexing for arbitrary lp norms. In Proceedings 26th
VLDB.
Yi, B. K., Jagadish, H. V., and Faloutsos, C. (2000). Ef-
ﬁcient retrieval of similar time sequences under time
wrapping. In Proceedings 16th ICDE, pages 201–208.
Zhu, Y. and Shasha, D. (2003). Warping indexes with enve-
lope transforms for query by humming. In Proceed-
ings ACM SIGMOD, pages 181–192.
60
Ioannis Karydis et al.
range.

CHANGE DETECTION AND MAINTENANCE 
OF AN XML WEB WAREHOUSE 
Ching-Ming Chao 
Department of Computer and Information Science, Soochow University, Taipei, Taiwan, R.O.C. 
Email: chao@cis.scu.edu.tw 
Keywords: 
Web warehouse, XML, Change detection, Warehouse maintenance, Mobile agent. 
Abstract: 
The World Wide Web is a popular broadcast medium that contains a huge amount of information. The web 
warehouse is an efficient and effective means to facilitate utilization of information on the Web. XML has 
become the new standard for semi-structured data exchange over the Web. In this paper, therefore, we study 
the XML web warehouse and propose an approach to the problems of change detection and warehouse 
maintenance in an XML web warehouse system. This paper has three major contributions. First, we propose 
an object-oriented data model for XML web pages in the web warehouse as well as system architecture for 
change detection and warehouse maintenance. Second, we propose a change detection method based on 
mobile agent technology to actively detect changes of data sources of the web warehouse. Third, we propose 
an incremental and deferred maintenance method to maintain XML web pages in the web warehouse. We 
compared our approach with a rewriting approach to storage and maintenance of the XML web warehouse 
by experiments. Performance evaluation shows that our approach is more efficient than the rewriting ap-
proach in terms of the response time and storage space of the web warehouse. 
1 INTRODUCTION
The World Wide Web is a popular broadcast medium 
that contains a huge amount of information. Infor-
mation on the Web is important not only to individ-
ual users but also to business organizations, espe-
cially for decision-making purposes. Therefore, how 
to efficiently and effectively utilize web information 
is an important issue. Recently, the concept of web 
warehousing was proposed to address this issue (Ng, 
1998; Xyleme, 2001). The idea of web warehousing 
is to build a web warehouse, which materializes and 
manages useful information from the Web, so as to 
facilitate utilization of web information. A web 
warehouse is a repository of web pages extracted 
from remote web sites. It has specific data sources 
and is built with a specific theme or purpose that is 
of interest to a user community. Because XML has 
become the new standard for semi-structured data 
exchange over the Web, we study in this paper the 
web warehouse of XML data, which will be called 
the XML web warehouse thereafter. 
If changes occur at data sources, a web ware-
house should be aware of those changes that may 
affect the data in the web warehouse. How does a 
web warehouse detect such changes is called the 
change detection problem. In the traditional data 
warehouse environment, data sources are cooperated 
with the data warehouse and hence will send mes-
sages about their changes to the data warehouse 
(Labio, 1995). However, data sources of a web 
warehouse are autonomous web sites and will not 
actively notify the web warehouse of their changes. 
Previous work on change detection of web data que-
ries data sources for their changes (Bhowmick, 2000; 
Lim, 2001), which requires more message transmis-
sion and is inefficient. Therefore, how to efficiently 
detect changes of data sources is an important prob-
lem to a web warehouse. 
If changes occur at data sources, a web ware-
house may need to be maintained. Deciding whether 
and how to maintain a web warehouse is called the 
warehouse maintenance problem. The problem of 
warehouse maintenance is not a brand new problem 
and has been studied for traditional data warehouses. 
However, most of the previous work on this problem 
was focused on the relational data model and the 
traditional data warehouse environment (Agrawal, 
1997; Zhuge, 1995; Zhuge, 1996). A web warehouse 
can be differentiated from a traditional data ware-
house in terms of its data model and operating envi-
ronment, and hence requires a different maintenance 
method. Therefore, how to efficiently maintain a 
web warehouse is also an important problem to 
study. 
61
C.-S. Chen et al. (Eds.), Enterprise Information Systems VII, 61–68.
© 2006 Springer.

In this paper, we propose an approach to the 
problems of change detection and warehouse main-
tenance for an XML web warehouse system. First, 
we propose an object-oriented data model for XML 
web pages in the web warehouse as well as system 
architecture for change detection and warehouse 
maintenance. Then, we propose a change detection 
method based on mobile agent technology to ac-
tively detect changes of data sources of the web 
warehouse. Finally, we propose an incremental and 
deferred maintenance method to maintain XML web 
pages in the web warehouse. We have implemented 
an experimental prototype system for change detec-
tion and maintenance of an XML web warehouse 
and have compared our approach with a rewriting 
approach by experiments. Performance evaluation 
shows that our approach is efficient in terms of the 
response time and storage space of the web ware-
house. 
The remainder of this paper is organized as fol-
lows. In Section 2 we illustrate the data model of the 
web warehouse and the system architecture for 
change detection and warehouse maintenance. In 
Section 3 we present the change detection method 
and algorithm. In Section 4 we present the ware-
house maintenance method and algorithm. Section 5 
illustrates the experimental results and states our 
observations from the experimental results. Section 
6 concludes this paper and gives some directions for 
future research.
2 DATA MODEL AND SYSTEM 
ARCHITECTURE
2.1 Data Model 
Our web warehouse stores XML data from the Web. 
Hence, we propose a data model, called the XML 
Web Warehouse Data Model (XWWDM), for XML 
web pages in the web warehouse. Due to the hierar-
chical structure of an XML web page, we follow the 
Document Object Model (Apparao, 1998) to de-
compose an XML web page into a tree structure. 
Besides, the design of the XWWDM model is based 
on the OEM-like model (Chawathe, 1999) and con-
siders the characteristics of a web warehouse. First, 
a web warehouse is like a data warehouse in that it 
can store historical data. Therefore, the data model 
includes version information to keep track of the 
change of data. Second, data in a web warehouse are 
sourced from remote web sites. Therefore, the data 
model includes source information to identify the 
source of data. The XWWDM model is an ob-
ject-oriented model whose class definition is shown 
in Figure 1. 
class XML_Page {root: XML_Node, version: 
Version_Info, source: Source_Info};
class XML_Node
{content: Node_Content, version: Version_Info};
class Node_Content {label: string, value: string, p-node: 
XML_Node, child#: integer, s-action: char};
class Version_Info
{version#: integer, update-time: time};
class Source_Info
{url: string, title: string};
class Update
{content: Update_Content, source: Source_Info};
class Update_Content {label: string, value: string, 
p-node: XML_Node, detect-time: time, action: char};
An XML web page is represented as an object of 
the class XML_Page, which has three attributes root,
version, and source. The attribute root records the 
root node of the tree structure of the web page. The 
attributes version and source record the newest ver-
sion information and source information of the web 
page, respectively. Each node of a web page is rep-
resented as an object of the class XML_Node, which 
has two attributes content and version. The attribute 
content records the content, position, and source 
action of a node. The attribute version records the 
version 
information 
of 
a 
node. 
The 
class 
Node_Content has five attributes label,
value,
p-node, child#, and s-action. The attributes label and 
value record the tag label and data content of a node, 
respectively. The attributes p-node and child# record 
the parent node and child number under its parent, 
respectively. The attribute s-action records the 
source action causing the creation of a node, whose 
value is I (for insertion), D (for deletion), or M (for 
modification). The class Version_Info has two attrib-
utes version# and update-time, which record the 
version number and time of last update, respectively. 
The class Source_Info has two attributes url and title,
which record the URL and title of the source web 
page, respectively. 
We adopt a change-centric approach to storage 
of all versions of an XML web page. Only the first 
version is completely stored. For subsequent ver-
sions, only deltas are stored. As shown in Figure 2, 
all frames represent the same web page, in which 
each frame represents a specific version at time Ti. 
The first frame represents the first version, in which 
all nodes of a web page are stored. Other frames 
represent subsequent versions, in which only nodes 
that are changed are stored. The number and letter 
drawn by a node are the child number and source 
62
Ching-Ming Chao 
Figure 1: The class definition of the XWWDM model. 

action of the node. Different versions of a node have 
the same parent node and child number but different 
version numbers. The first version of a node has a 
value I in the attribute s-action. A value D in the last 
version of a node indicates that this node has been 
deleted. 
A source update is represented as an object of the 
class Update, which has two attributes content and
source. The attribute content records a variety of 
information about the update. The attribute source
records the identification information of the web 
page in which the update occurs. The class Up-
date_Content has five attributes label, value, p-node,
detect-time, and action. The attributes label, value,
and p-node have similar meaning as in the class 
Node_Content. The attributes detect-time and action
record the detection time and type of the update, 
respectively. The update type can be I (for insertion), 
D (for deletion) or M (for modification). Source up-
dates are detected by mobile agents and transmitted 
from data sources to the web warehouse, and are 
used for the purpose of maintaining the web ware-
house. 
2.2 System Architecture 
The system architecture for change detection and 
maintenance of the XML web warehouse is shown 
in Figure 3, which is divided into three layers: the 
storage layer, the system kernel layer, and the mobile 
agent layer. The primary components and their func-
tions in each of the three layers are presented below. 
2.2.1 Storage Layer 
This layer is where the web warehouse is stored. The 
web warehouse uses the XWWDM model to store 
web warehouse serves as a knowledge base of deci-
sion support systems, providing data for web mining 
and on-line analytical processing. 
2.2.2 System Kernel Layer 
This layer is the kernel of the part of the web ware-
house system for change detection and warehouse 
maintenance. It includes the Mobile Agent Monitor 
System, the Integrator, and the Update Storage. The 
Mobile Agent Monitor System further consists of 
two modules: the Management Module and the 
Message Module. The Management Module is re-
sponsible for producing, dispatching, and tracing 
mobile agents. The Message Module is responsible 
for sending messages to and receiving messages 
from mobile agents as well as placing received 
source updates into the Update Storage. The Inte-
grator is responsible for maintaining the web ware-
house according to source updates stored in the Up-
date Storage. The Update Storage is a temporary 
storage of source updates sent by mobile agents and 
provides the Integrator with required update infor-
mation for warehouse maintenance. 
2.2.3 Mobile Agent Layer 
This layer includes mobile agents that are operating 
in data sources of the web warehouse and are re-
sponsible for detecting and reporting changes of data 
sources. Each mobile agent includes three compo-
nents: the Detection Module, the Message Module, 
and the Register. The Detection Module is responsi-
ble for detecting changes of the data source. The 
Message Module is responsible for sending mes-
sages to and receiving messages from the Mobile 
Agent Monitor System. The Register is the storage 
for the mobile agent to store the last states of web 
pages and detected source updates. 
specific and historical XML data from the Web. The 
63
Change Detection and Maintenance of an Xml Web Warehouse 
Figure 2: All versions of an XML web page. 
Figure 3: System architecture. 

3 CHANGE DETECTION 
In this section, we address the issue of change detec-
tion of data sources of an XML web warehouse. Be-
cause a web warehouse system is operating in the 
Internet environment and its data sources are remote 
and autonomous web sites, it is better for it to ac-
tively detect changes of data sources so as to speed 
up data refreshment and reduce network traffic. Mo-
bile agents have the feature of cross platform as well 
as the abilities of active detection and rapid report-
ing. Therefore, we propose a change detection 
method based on mobile agent technology in order 
for the web warehouse system to fully and effi-
ciently obtain changes of data sources. The proce-
dure of our change detection method consists of the 
following four steps: 
1.
The Mobile Agent Monitor System produces 
and dispatches a mobile agent to each of the 
data sources. 
2.
When a mobile agent arrives at a data source, it 
first sends source information of the data source 
back to the Mobile Agent Monitor System. This 
source information allows the Mobile Agent 
Monitor System to trace the mobile agent. 
3.
Thereafter a mobile agent will actively and pe-
riodically detect changes of a data source using 
the Change Detection Algorithm. If changes are 
detected, the mobile agent sends detected 
source updates to the Mobile Agent Monitor 
System. 
4.
When the Mobile Agent Monitor System re-
ceives source updates from a data source, it 
stores these updates in the Update Storage for 
the purpose of warehouse maintenance. 
The Detection Module of a mobile agent uses the 
Change Detection Algorithm, the CD algorithm for 
short, shown in Figure 4 to detect the difference be-
tween the current state and the last detected state of 
an XML web page and send a collection of updates 
to the Mobile Agent Monitor System through the 
Message Module. Here we first give a brief over-
view of the CD algorithm. First, the algorithm 
fetches an XML web page from the web site and 
finds the last recorded state of the web page from the 
Register. Both states of the web page are decom-
posed into tree structures using an XML parser. Then, 
these two tree structures are compared level-by-level 
in a top-down fashion to detect their difference. 
During this process, a collection of updates repre-
senting the difference is accumulated. Finally, the 
current state of the web page is recorded in the Reg-
ister and the collection of updates is sent to the Mo-
bile Agent Monitor System. Now we explain the CD 
algorithm in detail. The algorithm executes the fol-
lowing steps in sequence: 
Algorithm Change Detection Algorithm 
¾ Fetch an XML web page WP. 
¾ Find the last recorded state LP of WP from the 
Register. 
¾ Initialize a collection of updates UC to be empty. 
¾ Use an XML parser to decompose WP and LP 
into tree structures. 
¾ For each level L of WP and LP do
c For each node WPN of level L of WP do
Find the node LPN of level L of LP such that 
LPN.content.label = WPN.content.label
If LPN is found then Mark WPN and LPN. 
If WPN.content.value z LPN.content.value then
 Create an update object U whose attributes are 
as follows: 
content.label m the tag label of WPN
content.value m the data content of WPN 
content.p-node m the parent node of WPN 
content.detect-time m current system time 
content.action m ‘M’ 
source.url m the URL of WP 
source.title m the title of WP 
 Put U into UC. 
d For each node WPN of level L of WP do
If WPN is not marked then
 Create an update object U whose attributes are 
as follows: 
content.label m the tag label of WPN
content.value m the content of WPN 
content.p-node m the parent node of WPN 
content.detect-time m current system time 
content.action m ‘I’ 
source.url m the URL of WP 
source.title m the title of WP 
 Put U into UC. 
e For each node LPN of level L of LP do
If LPN is not marked then
 Create an update object U whose attributes are 
as follows: 
content.label m the tag label of LPN 
content.value m the data content of LPN 
content.p-node m the parent node of LPN 
content.detect-time m current system time 
content.action m ‘D’
source.url m the URL of LP 
source.title m the title of LP 
 Put U into UC.  
¾ Record the state of WP in the Register. 
¾ Send UC to the Mobile Agent Monitor System. 
End Algorithm 
1.
Fetch an XML web page WP from the web site. 
WP is the current state of the web page whose 
difference from its last recorded state is to be 
detected.
2.
Find the last recorded state LP of WP from the 
64
Ching-Ming Chao 
Figure 4: The change detection algorithm. 

Register. Because the detected web page is as-
sumed to have been stored in the web ware-
house, it must have the last recorded state. 
3.
Initialize a collection of updates UC to be 
empty. UC represents the difference between 
WP and LP. 
4.
Decompose WP and LP into tree structures us-
ing an XML parser. 
5.
Compare each level of the tree structures of WP 
and LP in a top-down fashion in a loop. For 
each level L of WP and LP, there are three inner 
loops. In the first inner loop, for each node 
WPN of level L of WP, find the node LPN of 
level L of LP such that the tag label of LPN is 
equal to that of WPN. If such a node is found, 
mark WPN and LPN to indicate that this node 
exists in both states of the web page. If the data 
content of WPN is not equal to that of LPN, 
which indicates that WPN has been modified, 
create a modification update and put it into UC. 
In the second inner loop, for each node WPN of 
level L of WP, check if it is marked. If it is not 
marked, which indicates that this node has been 
inserted to the current state, create an insertion 
update and put it into UC. In the third inner 
loop, for each node LPN of level L of LP, check 
if it is marked. If it is not marked, which indi-
cates that this node has been deleted from the 
last state, create a deletion update and put it into 
UC.
6.
Record the state of WP in the Register. This 
state will become the last recorded state next 
time this web page is detected. 
7.
Send the accumulated collection of updates UC 
to the Mobile Agent Monitor System.
4 WAREHOUSE MAINTENANCE 
In this section, we address the issue of maintaining 
an XML web warehouse in response to changes of 
data sources. The way to warehouse maintenance 
somewhat depends on the way to storage of the 
warehouse as well as the way to change detection of 
data sources. Our web warehouse adopts a 
change-centric approach to storage of historical data 
of web pages. Our change detection method utilizes 
mobile agents to send actively and periodically 
source updates back to the web warehouse. Accord-
ingly, we propose an incremental and deferred 
maintenance method to maintain XML web pages in 
the web warehouse. Incremental maintenance of a 
web page means that only parts of the web page are 
updated according to its source updates. It is gener-
ally more efficient in terms of time and space than 
rewriting the whole web page. Deferred maintenance 
of a web page means that the web page is main-
tained only when it is accessed. It is generally more 
time efficient than immediate maintenance in which 
a web page is maintained immediately after a source 
update occurs. Besides, the user is still able to access 
reasonably up-to-date web pages in a relatively short 
time. The procedure of our warehouse maintenance 
method consists of the following four steps: 
1.
When the web warehouse receives a request 
from a user to access a web page, it notifies the 
Integrator to maintain the web page. 
2.
The Integrator checks the Update Storage to see 
if there are source updates relevant to the web 
page. The relevant source updates are those that 
have the same source location information (i.e., 
URL) as the web page. All updates made to the 
source web page of this web page are consid-
ered as relevant source updates. 
3.
If the web page has relevant source updates, 
which indicates that it is out of date, the Inte-
grator maintains it using the Web Warehouse 
Maintenance Algorithm. 
4.
The web warehouse returns the requested web 
page to the user. 
The Integrator uses the Web Warehouse Mainte-
nance Algorithm, the WWM algorithm for short, 
shown in Figure 5 to maintain an XML web page in 
the web warehouse according to the relevant source 
updates of the web page in the Update Storage. Here 
we first give a brief overview of the WWM algo-
rithm. First, the algorithm finds the XML web page 
to be maintained in the web warehouse. Then, it gets 
and removes the relevant source updates of the web 
page from the Update Storage. These relevant up-
dates are sorted by their detection time to reflect 
their order of occurrence. Finally, it maintains the 
web page for each relevant update in sequence. A 
new version of the web page is obtained. Now we 
explain the WWM algorithm in detail. The algorithm 
executes the following steps in sequence: 
1.
Find the XML web page to be maintained WP 
in the web warehouse. 
2.
Get and remove a collection of relevant source 
updates RU from the Update Storage. RU is the 
collection of relevant source updates of WP. 
3.
Sort the updates in RU by their detection time 
recorded in the attribute detect-time to reflect 
their order of occurrence. 
4.
Create a version object V whose attribute ver-
sion# is the current version number of WP plus 
one and whose attribute update-time is the cur-
rent system time. V records the newest version 
information and will be stored in the next ver-
sion of WP. 
5.
Change the newest version information of WP 
to V. 
65
Change Detection and Maintenance of an Xml Web Warehouse 

Algorithm Web Warehouse Maintenance Algorithm
¾ Find the XML web page to be maintained WP 
in the web warehouse. 
¾ Get and remove a collection of relevant source 
updates RU from the Update Storage. 
¾ Sort the updates in RU by the attribute detect-time.
¾ Create a version object V whose attributes are 
as follows: 
version# m WP.version.version# + 1 
update-time m current system time 
¾ WP.version m V 
¾ For each update U of RU do
c If U.content.action = ‘I’ then
 Find the maximum child# MN among all nodes 
whose parent is U.content.p-node
 Insert a node whose attributes are as follows: 
content.label m U.content.label
content.value mU.content.value
content.p-node mU.content.p-node
content.child# mMN + 1 
content.s-action m ‘I’ 
version m V
d If U.content.action = ‘D’ then
 Find the newest version of node N such that 
N.content.p-node = U.content.p-node and 
N.content.label = U.content.label
 Insert a node whose attributes are as follows: 
content.label mN.content.label
content.value mN.content.value
content.p-node mN.content.p-node
content.child# mN.content.child#
content.s-action m ‘D’ 
version m V
e If U.content.action = M then
 Find the newest version of node N such that 
N.content.p-node = U.content.p-node and 
N.content.label = U.content.label
 Insert a node whose attributes are as follows: 
content.label mN.content.label
content.value mU.content.value
content.p-node mN.content.p-node
content.child# mN.content.child#
content.s-action m ‘M’ 
version m V
End Algorithm 
6.
Maintain WP for each update U of RU in se-
quence in a loop. For each update U, check to 
see if U is an insertion, a deletion, or a modifi-
cation. If U is an insertion, insert a node whose 
parent node is U.content.p-node to WP. The at-
tribute child# of the inserted node is the current 
maximum child number among its siblings plus 
one. This inserted node represents that a node 
was inserted to the source web page of WP. If U 
is a deletion, find the newest version of the 
node in WP that corresponds to the node that 
was deleted from the source web page of WP 
and insert a node to WP. This inserted node 
represents that a node was deleted from the 
source web page of WP. If U is a modification, 
find the newest version of the node in WP that 
corresponds to the node that was modified in 
the source web page of WP and insert a node to 
WP. This inserted node represents that a node 
was modified in the source web page of WP 
and becomes the newest version of the node in 
WP. After every update of RU is processed, a 
new version of WP is obtained. 
5 PERFORMANCE EVALUATION 
We have implemented an experimental prototype 
system for change detection and maintenance of an 
XML web warehouse. The web warehouse is built 
on the ObjectStore object-oriented database man-
agement system and programs are written in the Java 
object-oriented programming language. Besides, we 
adopt the Aglets Software Development Kit (ASDK) 
and the Aglets Workbench both from IBM as the 
development tool and operating environment of mo-
bile agents, respectively. The hardware platform 
consists of several personal computers that commu-
nicate with the Internet. 
In the performance evaluation, we compare our 
approach with a rewriting approach to storage and 
maintenance of an XML web warehouse. In the re-
writing approach, every version of a web page is 
completely stored. In our approach, on the other 
hand, every version except the first one stores only 
its difference from the previous version. As in our 
approach, the rewriting approach uses the deferred 
strategy to maintain a web page. However, it adopts 
a different method for change detection and ware-
house maintenance. While receiving an access re-
quest for a web page, the web warehouse sends a 
request to the source web site of the web page for 
the current state of the web page. After maintaining 
the web page by creating a complete version of the 
current state, the web warehouse returns the re-
quested web page to the user. In our approach, on 
the other hand, the requested web page is maintained 
using updates that have already been sent back by 
the mobile agent and stored in the local Update 
Storage. We compare our approach with the rewrit-
ing approach in terms of two important performance 
criteria: the response time and the storage space. 
The response time is the elapsed time starting 
from the web warehouse receives an access request 
until it returns the requested web pages. From the 
point of view of the user of a web warehouse, the 
66
Ching-Ming Chao 
Figure 5: The warehouse maintenance algorithm. 

response time is the most important criterion to 
judge the performance of the system. We separately 
consider two factors that may affect the response 
time, the number of updates to a web page and the 
number of accessed web pages. We first compare the 
response time of two approaches for accessing a 
single web page with different numbers of updates to 
the web page. An experimental result of such a 
comparison is illustrated in Figure 6(a). In Figure 6 
and Figure 7, the abbreviations IM (standing for 
incremental maintenance) and RW (standing for re-
writing) represent our approach and the rewriting 
approach, respectively. From the experimental re-
sults of this comparison, we observe two phenomena. 
First, the response time of our approach increases 
with the number of updates to a web page. However, 
the response time of the rewriting approach does not 
depend on the number of updates. Second, the re-
sponse time of our approach is shorter than that of 
the rewriting approach in general, especially when 
the number of updates in smaller. 
(a)
(b)
We also compare the response time of two ap-
proaches for accessing multiple web pages. In this 
comparison, the number of updates to each web page 
is fixed and the numbers of updates to these web 
pages are small. An experimental result of such a 
comparison is illustrated in Figure 6(b). From the 
experimental results of this comparison, we observe 
two phenomena. First, the response time of both 
approaches increases with the number of accessed 
web pages. Second, no matter how many web pages 
are accessed, the response time of our approach is 
always shorter than that of the rewriting approach as 
long as the numbers of updates to web pages are 
small. 
The storage space is the size of the secondary 
storage required for storing the historical data of a 
web page in the web warehouse. We consider two 
factors that may affect the storage space, the size of 
a web page and the update percentage of a web page. 
We compare the storage space of two approaches in 
terms of three different sizes (large, median, and 
small) and three different update percentages (30%, 
50%, and 80%). An experimental result of such a 
comparison is illustrated in Figure 7, which shows 
the ratio of the storage space of our approach to the 
storage space of the rewriting approach. From the 
experimental results of this comparison, we observe 
three phenomena. First, the storage space of our ap-
proach is smaller than that of the rewriting approach 
in most of the situations. Second, the storage space 
of our approach increases with the update percentage 
of the web page. Third, our approach is more advan-
tageous to web pages of larger size. It tends to re-
quire more storage space than the rewriting approach 
as the size of the web page gets smaller and the up-
date percentage of the web page gets higher. 
6 CONCLUSION AND FUTURE 
WORK
In this paper, we proposed an approach to change 
6WRUDJH6SDFH








ODUJH
PHGLDQ
VPDOO
3DJH6L]H
Ratio of storage space
5:
,0
,0
,0
Accessing a single web page








2
5
10
20
40
Number of updates
5HVSRQVHWLPHP
IM
RW
Accessing multiple web pages










1XPEHURIZHESDJHV
5HVSRQVHWLPHP
IM
RW
67
Change Detection and Maintenance of an Xml Web Warehouse 
Figure 7: Comparison of the storage space. 
Figure 6: Comparison of the response time. 

web warehouse system. We compared our approach 
with a rewriting approach in terms of the response 
time and storage space of the web warehouse. In our 
approach, mobile agents are dispatched by the web 
warehouse to data sources and will actively and pe-
riodically detect and report changes of data sources 
back to the web warehouse. Beside, the web ware-
house is incrementally maintained in response to 
source updates that have already been stored in the 
local storage. These can dramatically reduce the 
number of messages transmitted between the web 
warehouse and data sources. Therefore, our ap-
proach is more efficient than the rewriting approach 
in terms of the response time. With regard to the 
storage space, our approach adopts a change-centric 
approach in which every version of a web page ex-
cept the first version stores only its difference from 
the previous version. This can dramatically reduce 
the size of the storage required for historical data of 
web pages in the web warehouse. Therefore, our 
approach is more efficient than the rewriting ap-
proach in terms of the storage space. 
In the future, we will improve our approach in an 
attempt to further increase its efficiency. First, with 
regard to the storage of all versions of an XML web 
page, we will consider storing the current version 
completely and only deltas for historical data. Sec-
ond, we will try to improve our warehouse mainte-
nance algorithm so as to handle large number of 
updates more efficiently. Besides, more comprehen-
sive experimentation will be performed to extract 
conclusive results. 
REFERENCES
Agrawal, D., El Abbadi, A., Singh, A., Yurek, T., 1997. 
Efficient view maintenance at data warehouses. In 
Proceedings of the 1997 ACM SIGMOD International 
Conference on Management of Data, pp. 417-427. 
Apparao, V., 1998. Document Object Model (DOM) Level 
1 Specification (Version 1.0). 
Bhowmick, S. S., Ng, W. K., Madria, S. K., Lim, E. P., 
2000. Detecting and representing relevant web deltas 
using web join. In Proceedings of the 20th IEEE In-
ternational Conference on Distributed Computing Sys-
tems, pp. 255-262. 
Chawathe, S. S., Abiteboul, S., Widom, J., 1999. Manag-
ing historical semistructured data. Theory and Practice 
of Object Systems, Vol. 5, No. 3, pp. 143-162. 
Labio, W., Garcia-Molina, H., 1995. Efficient snapshot 
differential algorithm for data warehousing. In Pro-
ceedings of the 22nd International Conference on Very 
Large Data Bases, pp. 63-74. 
Lim, S. J., Ng, Y. K., 2001. An automated change detec-
tion algorithm for HTML documents based on seman-
tic hierarchies. In Proceedings of the 17th IEEE Inter-
national Conference on Data Engineering, pp. 
303-312.
Ng, W. K., Lin, E. P., Huang, C. T., Bhowmick, S., Qin, F. 
Q., 1998. Web warehousing: an algebra for web in-
formation. In Proceedings of the 1998 IEEE Forum on 
Research and Technology Advances in Digital Librar-
ies, pp. 228-237. 
Xyleme, L., 2001. A dynamic warehouse for XML data of 
the web. IEEE Data Engineering Bulletin, Vol. 24, No. 
2, pp. 40-47. 
Zhuge, Y., Garcia-Molina, H., Hammer, J., Widom, J., 
1995. View maintenance in a warehousing environ-
ment. In Proceedings of the 1995 ACM SIGMOD In-
ternational Conference on Management of Data, pp. 
316-327.
Zhuge, Y., Garcia-Molina, H., Wiener, J. L., 1996. The 
Strobe algorithms for multi-source warehouse consis-
tency. In Proceedings of the 4th IEEE International 
Conference on Parallel and Distributed Information 
Systems, 146-157.
68
Ching-Ming Chao 
detection and warehouse maintenance for an XML 

CHOOSING GROUPWARE TOOLS AND
ELICITATION TECHNIQUES  
ACCORDING TO STAKEHOLDERS’ FEATURES 
Gabriela N. Aranda 
Departamento de Ciencias de la Computación, Universidad Nacional del Comahue 
Buenos Aires 1400, Neuquén, Argentina 
Email: garanda@uncoma.edu.ar 
Aurora Vizcaíno 
Departamento de Informática – Universidad de Castilla-La Mancha 
Paseo de la Universidad 4, Ciudad Real, España  
Email: Aurora.Vizcaino@uclm.es 
Alejandra Cechich 
Departamento de Ciencias de la Computación, Universidad Nacional del Comahue 
Buenos Aires 1400, Neuquén, Argentina 
Email: acechich@uncoma.edu.ar 
Mario Piattini 
Departamento de Informática – Universidad de Castilla-La Mancha 
Paseo de la Universidad 4, Ciudad Real, España  
Email: Mario.Piattini@uclm.es 
Keywords: 
Groupware, Distributed Cooperative Work, Distributed Requirements Elicitation, Cognitive Informatics.  
Abstract: 
The set of groupware tools used during a distributed development process is usually chosen by taking into 
account predetermined business politics, managers’ personal preferences, or people in charge of the project. 
However, perhaps the chosen groupware tools are not the most appropriate for all the group members and it 
is possible that some of them would not be completely comfortable with them.  
 
To avoid this situation we have built a model and its supporting prototype tool which, based on techniques 
from psychology, suggests an appropriate set of groupware tools and elicitation techniques according to 
stakeholders’ preferences.  
1
INTRODUCTION
Software 
development 
in 
scenarios 
where 
stakeholders are in many geographically distributed 
sites, seems to be more common every day. The 
multi-site development is a current matter of study 
and discussion, especially about people who are 
involved in those virtual teams. It is a fact that 
during a traditional requirement elicitation process, 
stakeholders must face many problems that have 
been detected and analysed for decades (Brooks, 
1987; Davis, 1993; Loucopoulos, 1995). When 
participants are distributed distance affects processes 
of communication, coordination and control and has 
consequences along all the software development 
process
(Damian, 
2004), specially during the 
requirement elicitation process which is critically 
based on communication between stakeholders 
(SWEBOK, 2004). In addition to barriers in 
communication, 
other 
obstacles 
appear, 
like 
problems in knowledge management, stakeholders’ 
cultural diversity and time differences between 
different sites (Damian, 2002). 
There are some areas of research that try to 
minimize the impact of these problems. One of them 
is the CSCW (Computer-Supported Cooperative 
Work), which takes into account both human 
69
C.-S. Chen et al. (Eds.), Enterprise Information Systems VII, 69–76.
© 2006 Springer.

behaviour and the technical support people need to 
work as a group in a more productive way. This 
technical support is called groupware and is one of 
the main subjects of our study. On the other hand, as 
another approach to solve same problems of 
distributed requirements elicitation, the use of 
Cognitive Informatics is increasing day by day.  
Cognitive Informatics (Chiew, 2003; Wang, 
2002) is a profound interdisciplinary research area 
that tackles the common root problems of modern 
informatics, computation, software engineering, 
artificial intelligence (AI), neural psychology, and 
cognitive science. One of the most interesting things 
found in cognitive informatics is that it embodies 
many science and engineering disciplines, such as 
informatics, computing, software engineering, and 
cognitive sciences, which share a common root 
problem: 
how 
natural 
intelligence 
processes 
information.
Considering that the quality of the requirements 
is influenced by the techniques employed during 
requirement elicitation (Hickey, 2003) and the role 
that groupware tools play when communicating in 
virtual teams (Damian, 2002), we aim at improving 
virtual teams performance by applying concepts 
from cognitive informatics. We are particularly 
interested in some techniques from the field of 
psychology, which are called Learning Style Models 
(LSM). LSM classify people according to the way in 
which they perceive and process information, and 
analyse 
relationships 
between 
students 
and 
instructors. Considering that during requirement 
elicitation a person acts like student and instructor 
alternatively; we propose using LSM as a base for 
improving the requirements elicitation process. In 
doing so, we propose choosing a set of groupware 
tools and elicitación techniques that support not only 
the communication itself but also the stakeholders’ 
preferences. 
With this in mind, in the following sections we 
present some basic concepts about groupware tools, 
and learning style models. In section four, we 
describe a model that supports stakeholders’ 
personal preferences in geographically distributed 
processes, and an automatic tool that uses the 
previous model. In section five we present some 
related works. Conclusions are addressed in the final 
section of the paper.   
2 CSCW AND GROUPWARE 
CSCW is an acronym that refers to research into 
experimental 
systems 
and 
the 
nature 
of 
organizations, 
while 
groupware 
focuses 
on 
technologies (Grudin, 1994).  
Generally speaking, groupware is software to 
enable communication between cooperating people 
who work on a common task. It may include 
different communication technologies, from simple 
plain-text chat to advanced videoconferencing 
(Gralla, 1996). To avoid ambiguities we will refer to 
every simple piece of communication technology as 
a groupware tool, and to the systems that combine 
them as groupware packages.  
The most common groupware tools used during 
multi-site developments are e-mails, newsgroups 
and mailing lists, electronic discussion or forums, 
electronic notice or bulletin boards, asynchronous 
and synchronous shared whiteboards, document 
sharing, 
chat, 
instant 
messaging, 
and 
videoconferencing (Damian, 2002; Gralla, 1996; 
Herlea, 1998). 
At first glance, groupware tools can be divided 
into synchronous and asynchronous; whether the 
users have to work at the same time or not (Ellis, 
1991). Synchronous tools are, for instance, chat and 
videoconferencing, while e-mails, forums, and 
document sharing are asynchronous. 
Some authors note the importance of using both 
types of tools in group work. Asynchronous 
collaboration is important because it allows team 
members to construct requirements individually and 
contribute to the collective activity of the group for a 
later discussion. This is significant when groups are 
distributed across time zones because of the 
difficulty in scheduling real time meetings. Also, 
real time collaboration and discussions seem to be 
necessary components of group requirements 
elicitation sessions, in such a way that, by means of 
synchronous tools, stakeholders have the chance of 
getting instant feedback (Herlea, 1998).  
A second classification of groupware tools can 
be made according to the way in which they show 
the information. Some of them are based primarily 
on images, figures, diagrams, etc., like shared 
whiteboards, videoconferencing; while others do it 
by predominantly using words, for instance, chat, e-
mails, newsgroups, mailing lists, forums, etc. 
3 LEARNING STYLE MODELS
A learning process involves two steps: reception and 
processing of information.  
During the first step, people receive external 
information –which is observable through senses– 
and internal information –which emerges from 
introspection–, then they select a part to process and 
ignore the rest. Processing involves memorization or 
reasoning (inductive or deductive), reflection or 
action, and introspection or interaction with others 
70
Gabriela N. Aranda et al. 

(Felder, 1996; Felder, 1988). 
Learning Style Models (LMS) classify people 
according to a set of behavioural characteristics 
pertaining to the ways they receive and process 
information and this classification is used to improve 
the way people learn a given task.  
These models have been discussed in the context 
of analysing relationships between instructors and 
students. We have tried to take advantage of this 
model and discussions by adapting their application 
to a virtual team that deals with a distributed 
elicitation process. To do so, we consider an analogy 
between stakeholders and roles in LSM since during 
the elicitation process everybody “learns” from 
others. In this way stakeholders play the role of 
student or instructor alternatively, depending on the 
moment or the task they are carrying out (Martin, 
2003).  
After analysing five LSM in (Martin, 2003) we 
found out that every item in the other models was 
included in the model proposed by Felder-Silverman 
(Felder, 1988), so that we may build a complete 
reference framework choosing this as a foundation. 
The Felder-Silverman (F-S) Model classifies 
people into four categories, each of them further 
decomposed into two subcategories as follows: 
Sensing/Intuitive; Visual/Verbal; Active/Reflective; 
Sequential/Global. 
Each 
subcategory 
has 
the 
following significant characteristics: 
Sensing people prefer learning facts. They like 
solving problems by well-established methods and 
dislike complications and surprises. Sensors tend to 
be patient with details and good at memorising facts 
and doing hands-on (laboratory) work. On the 
contrary, intuitive people often prefer discovering 
possibilities and relationships. They like innovation 
and dislike repetition. They tend to work faster and 
to be more innovative than sensors. Intuitors do not 
like work that involves a lot of memorisation and 
routine calculations.  
Visual people remember best what they see (such 
as pictures, diagrams, flow charts, time lines, films, 
and demonstrations). They prefer visually presented 
information. On the other hand, verbal people get 
more out of words, and written and spoken 
explanations. 
They 
prefer 
verbally 
presented 
information. 
Active people tend to retain and understand 
information by doing something active with it 
(discussing or applying it or explaining it to others). 
“Let’s try it out and see how it works” is an Active 
´s phrase.  In contrast, reflective people prefer to 
think about information quietly first. “Let’s think it 
through first” is the Reflective’s response. 
Sequential people tend to gain understanding in 
linear steps, with each step following logically from 
the previous one. They tend to follow logical 
stepwise paths in finding solutions. They may not 
fully understand the material but they can 
nevertheless do something with it (like solve 
homework problems or pass a test) since the pieces 
are logically connected. Contrarily, global people
tend to work in large jumps, absorbing material 
almost randomly without seeing connections, and 
then suddenly "getting it". They may be able to solve 
complex problems quickly or put things together in 
novel ways once they have grasped the big picture, 
but they may have difficulty explaining how they 
did it. 
Classification into the different categories is 
made by a multiple-choice test proposed by 
Soloman-Felder. As a result, each person gets a rank 
for each category that suggests his or her preference.  
People may fit into one category or the other 
depending on the circumstances: people may be 
“sometimes” active and “sometimes” reflective. The 
preference for one category may be strong,
moderate, or mild. Only when there is a strong 
preference, can people be catalogued as a member of 
a certain group.  
4 OUR PROPOSAL 
4.1 The Model 
Before proposing a methodology for supporting 
distributed elicitation we think it is necessary to 
determine the aspects that have to be considered and 
the way in which they relate to each other.  
With the aim of recommending a set of suitable 
groupware tools and elicitation techniques during a 
particular elicitation process, we have defined a 
model, which is depicted in Figure 1, and whose 
primary concepts and relationships are now 
described: 
x Virtual Team
Virtual team (Peters, 2003) virtual community 
(Geib, 2004), distributed group (Lloyd, 2002) are 
terms used to refer to a group of people who work 
together on a project. Their main characteristic is 
their distribution over many sites, and the use of 
information technology to communicate and 
coordinate efforts.  
 
In our model the common project or task which 
they carry out is the elicitation process, which is 
the process of “extract and inventory the 
requirements from a combination of human 
stakeholders” (SWEBOK, 2004).  
71
Choosing Groupware Tools and Elicitation Techniques According to Stakeholders’ Features 

x Stakeholder
A stakeholder is defined as “a person, such as an 
employee, […], who is involved with an 
organization, 
[…] 
and 
therefore 
has 
responsibilities towards it and an interest in its 
success” (Cambridge Dictionary, 2004).   
Typical stakeholders are users (those who will 
operate the system), customers (those who have 
commissioned the system), system developers, 
etc. (SWEBOK, 2004).  
 
Each person in a virtual team is supposed to play 
(at least) one Role during the elicitation process, 
and, as it is a person, he or she has some 
Personal Characteristics that tell us about his or 
her preferences when he/she perceives and 
process information. 
x Groupware Tools
As we have mentioned before, groupware is 
software to enable communication. According to 
the way in which they show the information, 
groupware tools have different Representation
Modes (based on figures or diagrams, or based on 
spoken 
or 
written 
words) 
and 
different 
Interaction Modes (for instance, synchronous or 
asynchronous). 
x Elicitation Techniques
Elicitation is fundamentally a human activity 
where communication plays a transcendental role 
(SWEBOK, 2004).  
 
Requirement 
engineers 
may 
face 
some 
difficulties, usually because users cannot clearly 
describe their tasks, or because they are not 
completely disposed to cooperate. 
 
The election of elicitation techniques plays a very 
important role in distributed teams. Since face-to-
face interaction is not possible, techniques have to 
be adapted to be used in combination with 
groupware. Some techniques that seem to be 
adaptable to the distributed elicitation process are 
question 
and 
answer 
methods, 
customer 
interviews, 
brainstorming, 
idea 
reduction, 
storyboards, prototyping, questionnaires, and use 
cases (Lloyd, 2002).  
 
Like groupware tools, elicitation techniques have 
different Representation Modes (based on 
images or based on words). 
Relationships between these concepts can be 
expressed generally as:  
x A
Virtual Team represents a group of 
Stakeholders that work cooperatively on a 
common task (which in our case is the Elicitation 
Process).
x Stakeholders play Roles that imply rights and 
responsibilities that have to do with their job.  
In our case the roles involved in the elicitation 
process are: users, clients, managers, analysts, 
project managers, etc. 
x Stakeholders communicate with each other using 
some Groupware Tools and build different 
models of a problem using a set of Elicitation 
Techniques.
Stakeholder
Role
Personal
Characteristic 
Groupware
Tool
Interaction
Mode
Representation 
Mode
Elicitation 
Technique 
1
2..n
1..n
1..n
1..n
1..n
1
1
1
plays
has
has
has
has
Virtual team
uses
uses
according to
according to
72
Gabriela N. Aranda et al. 
Figure 1: A model to support personal preferences in a virtual community. 

x Groupware Tools, as well as Elicitation 
Techniques, are supposed to be chosen according 
to the stakeholders’ Personal Characteristics, in 
order to make them feel comfortable and improve 
their performance.  
x Each Groupware Tool has a Representation 
Mode (verbal or visual) and an Interaction 
Mode (synchronous or asynchronous), which are 
important in deciding the suitability for a 
stakeholder’s personal preferences. 
x In a similar way, each Elicitation Technique has 
a predominant Representation Mode (verbal, 
visual, or a possible good combination of both) 
that we will take into account to suggest their use 
or non-use. 
4.2 Applying LSM to Choose 
Groupware Tools 
In order to support personal preferences, in (Martin, 
2003) we have proposed a classification of 
groupware tools focusing on Visual/Verbal and 
Active/Reflective categories of the F-S model. The 
classification is based on the description and the 
strategies suggested by Felder and Silverman for 
each subcategory. The results of such classifications 
are shown in Figure 2. The sign “++” is used to 
indicate those groupware tools which are more 
suitable for people with a strong preference for a 
given subcategory. The sign “+” indicates that a 
groupware tool would be mildly preferred by a 
stakeholder with those characteristics. Finally, the 
sign “-“ suggests that a particular groupware tool 
would be “not suitable” for that particular 
subcategory.
Also, we have proposed a way of choosing a set 
of groupware tools for a given group of 
stakeholders. To do so we suggest representing the 
information we know about each participant in a 
two-way matrix that collects their preferences for 
categories Visual/Verbal and Active/Reflective. By 
doing so, we can have a view of stakeholders’ 
preferences in general and, according to the quadrant 
that 
contains 
more 
instances, 
choose 
those 
groupware tools that adapt to most people in the 
group. Figure 3 shows an example of such a matrix. 
In (Aranda, 2004), we have presented a model 
based on fuzzy logic and fuzzy sets, which aims to 
obtain rules, given a set of representative examples, 
that tell us about the stakeholders’ preferences in 
their daily use of groupware tools.  
The model takes four inputs (X1, X2, X3, X4), 
which are the preferences for each category of the F-
S Model, and an output variable (Y) that is the 
preference for one of a given set of groupware tools.  
For each input variable we have defined a 
domain using the adverbs (and their correspondent 
abbreviations): Very (V), Moderate (M) and Slight 
(S). These adverbs correspond to strong, moderate
and mild, respectively, in the F-S model, but we 
have changed their names to avoid confusion with 
respect to the use of the first letter.  
For instance, the definition domain for the 
category 
Reflective-Active 
would 
be: 
Very 
reflective (VRe), Moderately reflective (MRe), 
Slightly reflective (SRe), Slightly active (SAc), 
Moderately active (MAc), Very active (VAc).  
Using a machine learning algorithm it is possible 
to obtain rules such as “Ro: if X1 is VAc and X3 is 
VVi then y is IM”, which is interpreted as: “If a 
user has a strong preference for the Active 
subcategory and a strong preference for the Visual 
subcategory, the tool that this person would prefer is 
Instant Messaging”
In a similar way it is possible to find a suitable 
set of elicitation techniques according to the 
preferences for each category of the F-S model. 
Visual
Verbal
Active
Reflective
Asynchronous 
Tools
E-mails 
+
++
-
++
Mailing lists, Newsgroups 
-
++
-
++
Asynch. shared whiteboards 
++
-
-
++
Forums
-
++
-
++
Synchronous 
Tools
Instant messaging 
+
++
++
-
Synch. shared whiteboards 
++
-
++
-
Chat
-
++
++
-
Videoconferencing 
++
++
++
-
73
Choosing Groupware Tools and Elicitation Techniques According to Stakeholders’ Features 
Figure 2: Classification of groupware tools according to category’ descriptions of the F-S model. 
www.ebook3000.com

4.3 A Tool to Automate the Selection 
Process 
As we have previously explained we aim to find a 
set of groupware tools and elicitation techniques that 
are suitable for a given group of stakeholders. By 
trying to do this in an automatic way we have 
designed a prototype tool. 
By means of our tool, stakeholders are asked to 
fill in a multiple-choice test so as to know their 
preferences. 
This 
information 
is 
maintained 
throughout the cooperative process.  
Once a group of stakeholders is defined and their 
preferences detected, our tool analyses them, using 
the sets of rules previously generated. As a result it 
returns the most suitable groupware tools and 
elicitation techniques for that group of people. 
The tool’s architecture has been designed 
basically on three layers:
x Lower Layer – Persistent Data 
It keeps the information concerning personal 
preferences of stakeholders, rules of suitability 
preferences-groupware 
tools 
and 
rules 
of 
suitability preferences-elicitation techniques. 
x Middle Layer – Application logic  
It contains those components that interact with 
the database and interface layers in order to find 
information and, by applying the appropriate 
algorithms, analyses it and produces a suitable 
answer.
x Upper Layer – User Interface 
It is the layer that contains all those components 
with which users of the tool interact. 
Figure 4 shows a screen of our prototype tool 
where three stakeholders (Mary, Tom and Pam) are 
interacting.
Information about their predominant personal 
characteristics is shown on the upper right hand side 
of the screen. On the bottom there are two lists of 
suggested groupware tools and elicitation techniques 
that would be most suitable for them.  
5 RELATED WORK 
Some 
related 
work 
concerning 
analysis 
of 
groupware tools and elicitation techniques in 
distributed teams is found in literature:   
In (Damian, 2002) a case study is described of a 
real multi-site organization that uses a mix of 
synchronous 
and 
asynchronous 
tools, 
like 
teleconferencing, 
a 
common 
repository 
of 
documents, email, and other Internet technologies. 
The 
authors 
collected 
data 
from 
inspecting 
documents, observed requirements meetings, and 
performed 
semi-structured 
interviews 
with 
stakeholders. As a conclusion, some of the points 
that stakeholders note as problems –which are 
especially interesting for us– are the lack of informal 
or face-to-face communication and the difficulty in 
sharing 
drawings 
on 
a 
whiteboard 
during 
spontaneous discussions. 
Pam 
Tom
Mary 
Asynchronous tools 
Synchronous tools 
E-mails
Forums
Chat
Videoconferencing 
Shared Whiteboards 
Videoconferencing 
E-mails
Instant Messaging 
Strong
Mod.
Mild
Mild
Strong
Mod
Strong
Mod
Mild
Mild
Strong
Mod
REFLECTIVE
ACTIVE
VISUAL
VERBAL
74
Gabriela N. Aranda et al. 
Figure 3: Choosing a set of groupware tools according to F-S Model categories.

Another example is reported in (Lloyd, 2002) 
and shows the results of an exploratory empirical 
study about effectiveness of requirement engineering 
in a distributed setting. Students from different 
graduate Software Engineering courses played the 
role of customers or engineers in separate groups. 
They used a previously selected set of groupware 
tools: audio-conferencing and chat for synchronous 
communication, and email for file sharing and 
asynchronous discussions. They could do just four 
planned audio-conferencing meetings, (no more than 
19 minutes each), while the use of other 
technologies was not restricted. They were able to 
use a wide set of requirement elicitation techniques. 
Participants playing the role of software engineers 
wrote a Software Requirements Specification (SRS) 
document using only the knowledge gained from 
remote collaboration with customers. After SRS 
documents were produced, a set of metrics was 
applied to assess document quality. They concluded 
that students who played the role of software 
engineers chose the techniques according to 
previous experience and instruction in the course. 
Data collected suggested that groups producing high 
quality SRS were those that had only used the 
synchronous tools and did not need to use email and 
asynchronous elicitation methods. 
Both case studies have interesting points for us. 
However, we think that different conclusions could 
be 
reached 
if 
aspects 
relative 
to 
personal 
characteristics had been applied. Why did students 
who wrote the highest quality SRS documents not 
need to use email to communicate with their 
customers?: It may be because their personal 
characteristics were suitable for synchronous tools, 
while those who needed email interaction needed 
more time to think and prepare questions or answers 
so that synchronous communication was not the best 
form for them; or it may be that they needed “to see” 
the words written, and audio-conferencing was not 
appropriate. With reference to the results obtained in 
(Damian, 2002), the need to use  a whiteboard to 
draw during discussions indicates people with a 
strong preference for visual tools.  
In (Carrizo Moreno, 2004), a survey of works 
where theories, empirical analysis and comparisons 
between different elicitation techniques is presented. 
It focuses on the fact that elicitation techniques are 
chosen without having a valid guide to select the 
best one.  
6 CONCLUSIONS
Today, 
many 
organisations 
have 
adopted 
a 
decentralised, 
team-based, 
distributed 
structure 
where members communicate through groupware 
tools. The selection of appropriate technology and 
elicitation techniques in such environments is a 
subject of study in current literature. 
By means of improving communication during 
the elicitation process, we think it is possible to 
improve the elicitation process itself. When 
stakeholders feel comfortable with the technology 
and methodologies they use, information gathered 
during elicitation is expected to be more accurate. 
75
Choosing Groupware Tools and Elicitation Techniques According to Stakeholders’ Features 
Figure 4: An interface that shows the suggestions for a particular group of stakeholders.

Stakeholders 
might 
feel 
more 
comfortable 
expressing their ideas and describing facts by using 
a tool closer to the way they perceive and reason 
about the world. 
In this paper, we have proposed a model and its 
supporting tool to relate stakeholders' learning 
preferences to communication tools and elicitation 
techniques – more suitable according to those 
preferences. However, an aspect that needs further 
discussion is the possibility of solving conflicts 
when stakeholders' preferences seem to be opposite. 
We are working on that restriction.  
Additionally, as a future work we are using this 
tool in academic and industrial environments, in 
order to evaluate how it behaves in real situations 
and analyse its effectiveness in virtual teams. 
REFERENCES
Aranda, G., Cechich, A., Vizcaíno, A. and Castro-Schez, 
J. J. (2004). Using fuzzy sets to analyse personal 
preferences 
on 
groupware 
tools. 
X 
Congreso 
Argentino de Ciencias de la Computación, CACIC 
2004, San Justo, Argentina, pp. 549-560. 
Brooks, F. P. (1987). No Silver Bullet: Essence and 
accidents of Software Engineering. IEEE Computer 
4(20), pp. 10-19. 
Cambridge Dictionary (2004). Cambridge Advanced 
Learner's Dictionary. Cambridge Dictionaries Online, 
<http://dictionary.cambridge.org/>. 
Cambridge
University Press 2004. 
Carrizo Moreno, D. (2004). Selección de Técnicas de 
Educción de Requisitos: Una Revisión Conjunta de la 
Ingeniería 
de 
Software 
y 
la 
Ingeniería 
del 
Conocimiento. IV Jornadas Iberoamericanas de 
Ingeniería 
del 
Software 
e 
Ingeniería 
del 
Conocimiento, JIISIC 2004, Madrid, Spain, pp. 159-
174.
Chiew, V. and Wang, Y. (2003). From Cognitive 
Psychology to Cognitive Informatics. Second IEEE 
International Conference on Cognitive Informatics, 
ICCI’03, London, UK, pp. 114-120. 
Damian, D., Lanubile, F., Hargreaves, E. and Chisan, J. 
(2004). Workshop Introduction. 3rd International 
Workshop on Global Software Development. Co-
located with ICSE 2004, Edinburgh, Scotland. 
Damian, D. and Zowghi, D. (2002). The impact of 
stakeholders geographical distribution on managing 
requirements in a multi-site organization. IEEE Joint 
International 
Conference 
on 
Requirements 
Engineering, RE'02, Essen, Germany, pp. 319-328. 
Davis, A. (1993). Software Requirements: Objects, 
Functions and States. New Jersey, Prentice Hall. 
Upper Saddle River. 
18-23.
Felder, R. and Silverman, L. (1988). Learning and 
Teaching 
Styles 
in 
Engineering 
Education. 
Engineering Education 78(7), pp. 674-681. 
Geib, M., Braun, C., Kolbe, L. and Brenner, W. (2004). 
Measuring 
the 
Utilization 
of 
Collaboration 
Technology 
for 
Knowledge 
Development 
and 
Exchange in Virtual Communities. 37th Hawaii 
International Conference on System Sciences, HICSS-
38, Big Island, Hawaii, pp. 1-10. 
Gralla, P. (1996). How Intranets Work. Emeryville, 
California, Ziff-Davis Press. 
Grudin, J. (1994). Computer-Supported Cooperative 
Work: History and Focus. IEEE Computer 27(5), pp. 
19-26.
Herlea, D. and Greenberg, S. (1998). Using a Groupware 
Space for Distributed Requirements Engineering. 7th 
IEEE Int'l Workshop on Coordinating Distributed 
Software Development Projects, Stanford, California, 
USA, pp. 57-62. 
Hickey, A. M. and Davis, A. M. (2003). Elicitation 
Technique Selection: How do experts do it? 
International Joint Conference on Requirements 
Engineering (RE03), Los Alamitos, California, IEEE 
Computer Society Press, pp. 169-178. 
Ellis, C.A.; Gibbs, S.J. and Rein, G.L. (1991). Groupware: 
Some Issues and Experiences. Comm. of ACM 34 (1), 
pp. 38-58. 
Lloyd, W., Rosson, M. B. and Arthur, J. (2002). 
Effectiveness of Elicitation Techniques in Distributed 
Requirements Engineering. 10th Anniversary IEEE 
Joint International Conference on Requirements 
Engineering, RE'02, Essen, Germany, pp. 311-318. 
Loucopoulos, P. and Karakostas, V. (1995). System 
Requirements Engineering. New York, NY, USA. Mc 
Graw-Hill. 
International 
series 
in 
Software 
Engineering.
Martin, A., Martinez, C., Martinez, N., Aranda, G. and 
Cechich, A. (2003). Classifying Groupware Tools to 
Improve 
Communication 
in 
Geographically 
Distributed Elicitation. IX Congreso Argentino de 
Ciencias de la Computación, CACIC 2003, La Plata, 
Argentina, pp. 942-953. 
Peters, L. (2003). The Virtual Environment: The “How-to” 
of Studying Collaboration and Performance of 
Geographically Dispersed Teams. Twelfth IEEE 
International Workshops on Enabling Technologies: 
Infrastructure 
for 
Collaborative 
Enterprises, 
WETICE’03, Linz, Austria, pp. 137-141. 
SWEBOK (2004). Guide to the Software Engineering 
Body 
of 
Knowledge. 
Software 
Engineering 
Coordinating Committee (IEEE-CS y ACM). 
Wang, Y. (2002). On Cognitive Informatics. First IEEE 
International Conference on Cognitive Informatics, 
ICCI'02, Calgary, Alberta, Canada, pp. 34-42. 
Felder, R. (1996). Matters of Styles. ASEE Prism 6(4), pp. 
76
Gabriela N. Aranda et al. 

ANALYTICAL AND EXPERIMENTAL EVALUATION OF
STREAM-BASED JOIN
Henry Kostowski
Department of Computer Science, University of Massachusetts - Lowell
Lowell, MA 01854
Email: hkostows@cs.uml.edu
Kajal T. Claypool
Department of Computer Science, University of Massachusetts - Lowell
Lowell, MA 01854
Email: kajal@cs.uml.edu
Keywords:
Data Streams, Continuous Queries, Join, Main Memory Joins.
Abstract:
Continuous queries over data streams have gained popularity as the breadth of possible applications, ranging
from network monitoring to online pattern discovery, have increased. Joining of streams is a fundamental issue
that must be resolved to enable complex queries over multiple streams. However, as streams can represent
potentially inﬁnite data, it is infeasible to have full join evaluations as is the case with traditional databases.
Joins in a stream environment are thus evaluated not over entire streams, but on speciﬁc windows deﬁned on
the streams. In this paper, we present windowed implementations of the traditional nested loops and hash join
algorithms. In our work we analytically and experimentally evaluate the performance of these algorithms for
different parameters. We ﬁnd that, in general, a hash join provides better performance. We also investigate
invalidation strategies to remove stale data from the window buffers, and propose an optimal strategy that
balances processing time versus buffer size.
1
INTRODUCTION
The proliferation of the Internet, the Web, and sensor
networks have fueled the development of applications
that treat data as a continuous stream, rather than as
a ﬁxed set. Telephone call records, stock and sports
tickers, streaming data from medical instruments, and
data feeds from sensors are examples of streaming
data.
As opposed to the traditional database view
where data is ﬁxed (passive) and the queries consid-
ered to be active, in these applications data is consid-
ered to be the active component, and the queries are
long-standing or continuous. Recently, a number of
systems have been proposed (Babu and Widom, 2001;
Motwani et al., 2003; Babcock et al., 2002; Carney
et al., 2002; Chen et al., 2000) to address this para-
digm shift from traditional database systems to now
meet the needs of query processing over streaming
data.
Many of the proposed data stream management
systems (DSMSs)(Babu and Widom, 2001; Motwani
et al., 2003; Babcock et al., 2002; Chen et al., 2000)
are based on relational databases, with suitable mod-
iﬁcations to handle streaming data.
Typically in a
DSMS, the streaming data is given by tuples that fol-
low a ﬁxed schema. Given this perspective, a stream
is similar to an inﬁnitely extended relational table in
the sense that the table is never closed and can al-
ways be extended in the future. Queries against such
streams are composed of stream operators that at-
tempt to preserve as much as possible the semantics of
the familiar relational operators. However, the poten-
tially inﬁnite nature of data streams poses difﬁculties
in both the deﬁnition of the semantics of such stream
operators as well as their implementation. One of the
key challenges is therefore to provide stream opera-
tors with well-deﬁned semantics that nevertheless do
not require inﬁnite storage and resources to compute
their results.
There are two basic approaches for providing se-
mantics for stream operators. The ﬁrst approach is
to compute summary information.
A data stream
is queried to produce a statistical summary of the
data seen so far. New data increments the statisti-
cal summary and can then be thrown away. The sec-
ond approach is to compute exact results while at the
same time limiting the memory resources required for
the computation.
This approach adds a time win-
dow to the deﬁnition of query operators so that only
data falling within the time window produces results.
When data falls outside the window it can be dis-
carded since by deﬁnition it cannot produce any new
77
C.-S. Chen et al. (Eds.), Enterprise Information Systems VII, 77–84.
© 2006 Springer.

results. However, to efﬁciently answer a query and to
provide scalability of user queries a query optimizer
must be able to analyze and compare the cost of the
different implementations of these windowed stream
operators. Towards that end, in this paper we provide
an analytical and experimental comparison of two im-
plementations of a windowed join operator.
The windowed join operator computes the join be-
tween two streams A and B, with windows windowA
and windowB respectively, using timestamps to limit
the possible matches.
A tuple from stream A,
tA, with timestamp time(tA), may join with an
earlier tuple from stream B, tB, with timestamp
time(tB), only if time(tA) −time(tB) <
windowB. The symmetric statement holds with tA
and tB interchanged. Based on this deﬁnition, the
window join is well-deﬁned provided the tuples are
processed in order of their timestamps. If a tuple’s
timestamp is given by time-of-arrival at the join input
then the total order requirement is fulﬁlled.
In this paper we examine the performance char-
acteristics of two implementations, namely the win-
dowed nested-loops and the windowed hash, of a win-
dowed join operator.
We use an evaluation metric
based on a rate-based cost model that can be used
by the query optimizer to determine the best plan
for evaluating a query over streaming data(Viglas and
Naughton, 2002). The cost model incorporates the
costs for the three fundamental steps that compose
each algorithm, namely scan, invalidation and inser-
tion. For a tuple tA arriving on stream A, scan is the
cost of scanning the stream B buffer for a match. In-
sertion is the cost of inserting the tuple tA into stream
A’s window buffer, and invalidation is the cost of re-
moving the expired tuples from the window buffers.
A tuple is considered to be expired if it’s time-stamp
falls outside the deﬁned window for its stream. For
each step we posit an average time per tuple cost and
combine this cost with the rates of the inputs to derive
a formula for the total processing cost. In this paper,
we derive the cost formulas for each of the algorithms
and compare them to the experimental measurements.
However, the cost model considers only the time
cost of the join algorithms, and not the memory re-
sources utilized by each of the algorithms. For scala-
bility of stream operators, where resource contention
can be a primary issue, such an analysis is essential.
In our work we consider four invalidation strategies,
namely invalidation of tuples at scan time, invalida-
tion of all buffers at once, and the invalidation of only
the buffers that are actually probed. For each of these
invalidation strategies we present an analytical model
to evaluate both their processing cost as well their
effect on memory utilization. We show experimen-
tally that the best choice is likely to be to invalidate a
bucket only when it is probed or scanned. The invali-
dation requirement of the join operators as discussed
Window
Buffer
A
Window
Buffer
B
Window Join
Operator
Input
Stream B
Input
Stream A
Output
Stream
here can be easily extended to other windowed stream
operators.
Roadmap.
The rest of the paper is organized as
follows. Section 2 presents an overview of the two
windowed join algorithms. In Sections 3 and 4 we
present our analysis and experimental results.
We
conclude in Section 6.
2
WINDOWED JOIN
ALGORITHMS
In this section, we present windowed versions of two
widely used join algorithms, namely the nested loops
and the hash join algorithms(Ullman and Widom,
1997). In the windowed version, timestamps are as-
signed to tuples as they arrive for processing on either
of the windowed join’s two inputs. Both windowed
algorithms, WNL (windowed nested loops) and WHJ
(windowed hash join), share a common structure as
shown in Figure 1. Here, there are two input streams.
Each input stream has a buffer that holds (at least)
one window’s worth of tuples. We term the stream
on which a tuple arrives the arrival stream, and we
call the other stream the opposite stream. Whenever
a tuple arrives on one stream, the opposite stream’s
window buffer is probed to check for matching join
attributes.
The tuple is then placed in the arrival
stream’s window buffer. Invalidation of stale tuples
in a window buffer can be performed at probe time,
when a tuple arrives on the opposite stream, or at
insertion time, when a tuple arrives on the arrival
stream.
Windowed Nested Loops Join.
In the WNL algo-
rithm, the window buffer of each stream is given by a
circular buffer structure ordered by arrival time stamp.
When a tuple tA arrives on one of the streams, say
stream A, the tuples in the opposite stream’s (stream
B’s) window buffer (B) are ﬁrst checked for expired
78
Henry Kostowski and Kajal T. Claypool
Figure 1: The Window Join Operator.

tuples. The remaining tuples in the window buffer B
are then scanned to match, based on the join attribute,
with the arriving tuple. The newly arrived tuple tA
is inserted at the head of arrival stream’s (stream A)
window buffer (A). Lastly, the expired tuples at the tail
of the window buffer A are removed. This last step is
optional from the point of view of the semantics of
the window join. That is, failure to remove these ex-
pired tuples will not produce erroneous results since
they will also be expired when a new tuple arrives to
be processed on the opposite stream (stream B). Note
that this property is solely due to the assumed correct
arrival ordering property of tuple timestamps. How-
ever, failure to expire these tuples will cost in memory
resources, which could be signiﬁcant depending on
the relative arrival rates of the two streams. Figure 2
outlines the windowed nested loops join algorithm.
WNL (Stream A, Stream B) {
event: (tuple arrives on stream A) do {
while (bottom tuple in buffer B expired) do
remove bottom tuple from buffer B;
for each (tuple in buffer B) do
if match then form join tuple and output;
insert (tuple) on head of buffer A;
//next step is optional
while (tail tuple in buffer A expired)
remove tail tuple from buffer A;
}
}
Windowed Hash Join.
For the Windowed Hash
join, the window buffer of each stream is divided into
hash buckets based on the join attributes. When a tu-
ple tA arrives on stream A, it is hashed to the corre-
sponding bucket of the opposite stream’s (stream B)
window buffer. Expired tuples in this bucket are re-
moved, and the bucket is then scanned for matches.
The arriving tuple tA is hashed into the correct hash
bucket of the arrival stream’s window buffer A and
(again optionally) the bucket is scanned for expired
tuples. The hash buckets are individually arranged
as circular buffers in order of time stamp to facili-
tate checking for expired tuples. Figure 3 outlines the
windowed hash join algorithm.
Invalidation Strategies.
Both join algorithms dis-
cussed in this paper must remove expired tuples from
the buffer about to be scanned before the scan phase
to ensure that the presence of stale tuples does not
produce erroneous results. This is the ﬁrst remove
step in the algorithms given in Figures 2 and 3. In
WHJ (Stream A, Stream B) {
event: (tuple arrives on stream A) do {
compute hash(tuple.joinAttributes)
retrieve handle to corres. hashbucket B;
//next steps use the circular buffer structure of
// each hash bucket
while (tail tuple in hash bucket B expired) do
remove tail tuple from hash bucket B;
for each (tuple in hashbucket B) do
if match then form join tuple and output;
retrieve handle to corres. hashbucket A
insert (tuple ) into hashbucket A
//next step is optional
while (tail tuple in hashbucket A expired)
remove tail tuple from hashbucket A;
}
}
addition to this required remove step, additional re-
move steps may be employed to delete expired tuples
as soon as possible in order to decrease buffer sizes.
For example, in the case of the windowed hash join
(nested loops can be considered a windowed hash join
with only one hash bucket), we can check and remove
expired tuples in the insertion buffer prior to the in-
sertion phase. This is indicated by the second remove
step in the algorithms given in Figure 2 and 3. Addi-
tionally, each remove step in the windowed hash join
algorithm may also scan all other buckets to remove
stale data before it can accumulate. Based on our im-
plementation using circular buffers, stale tuples can
only exist at the bottom of the circular buffer. Thus,
the cost of invalidation of a hash bucket is the product
of the tuple invalidation cost and the number of stale
tuples in the buffer. The added cost of scanning all
the buckets is the cost of scanning at least the bottom
tuple in each bucket.
3
ANALYTIC COST MODEL
In this section we present a cost model, based on the
average cost metric introduced by Viglas et al.(Viglas
and Naughton, 2002), to evaluate the total process-
ing costs of the two join algorithms described in Sec-
tion 2.
While Viglas et al.(Viglas and Naughton,
2002) focus on the output rate of a stream operator, an
important quantity for a query optimizer to calculate
in order to guarantee a certain result rate, we focus on
the scalability of the operators. Towards that end we
analyze the average cost per unit time of the join op-
erator. We also look at the effect of the invalidation
79
Analytical and Experimental Evaluation of Stream-Based Join
Figure 2: Windowed Nested Join Algorithm.
Figure 3: Windowed Hash Join Algorithm.

3.1
Total Processing Cost
In general, the average cost per unit time of the join
operator is given by the average cost of processing
one tuple on one input multiplied by the input rate.
For each operator the half-cost, that is, the theoreti-
cal cost to process one of the input streams, is com-
puted ﬁrst. Then the total cost, the cost to process
both input streams, given by the sum of the two half-
costs is calculated. We assume that the operators are
symmetric in their inputs to simplify the cost formu-
las. For the windowed nested loops join this total cost
Cost(WNL)total, is given as:
C(WNL)total
= 2 ∗Cost(WNL)half
where Cost(WNL)half = λA(Ct). Here, λA is the
rate of arrival of the tuples on stream A; and Ct is the
cost to handle one tuple and is given as:
Ct
=
λB
λA
Cinvalidate + TBλBCscan + Cinsert
Here λB is the rate of arrival of tuples on stream B,
Cinvalidate is the cost of invalidating tuples in the win-
dow buffer B of the stream B. Recall that invalidation
is the garbage collection of the expired tuples from the
buffers of the input streams. A tuple is considered to
be expired if it’s time-stamp falls outside the deﬁned
window for its stream. The term TB is the size in sec-
onds of the window on the stream B input, Cscan the
cost of scanning the tuples in the window buffer B for
a match, and Cinsert is the cost of inserting a tuple into
the window buffer A of the stream A. The factor λB
λA
gives the average number of buffer tuples that need
to be invalidated upon arrival of a tuple on stream A.
Thus, under the symmetry assumption (in particular
that the insertion, invalidation, and scan costs are the
same on both inputs), the total cost of computing a
join for tuples arriving on the two streams A and B is
as given in Equation 1.
C(WNL)total
=
(λA + λB)(Cinvalidate + Cinsert)
+λAλB(TA + TB)Cscan
(1)
Similarly, the total cost of evaluating the join of two
streams A and B using a windowed hash join algo-
rithm is given as the sum of its two half-costs. For
a windowed hash join, the half cost, C(WHJ)half is
given as:
C(WHJ)half
=
λA(λB
λA
Cinvalidate +
TBλB
NB
Cscan + Cinsert)
where NB is the number of hash buckets on stream
B’s side. The total cost for a windowed hash join is
given as:
C(WHJ)total
=
(λA + λB)(Cinvalidate + Cinsert)
+λAλB
NB (TA + TB)Cscan
(2)
Equation 1 and 2 show that both the algorithms,
windowed nested loops (WNL) and the windowed
hash join (WHJ), have the same overall functional de-
pendence on the input rates. In particular, holding
one of the input rates ﬁxed, the average cost per unit
time depends linearly on the input rate of the second
stream. This similarity is not unexpected when we
consider that the hash join algorithm with only one
bucket is nearly the same as the nested loops algo-
rithm. The most signiﬁcant difference between the
two algorithms is that the hash join algorithm needs
to scan only a fraction of the buffered tuples resid-
ing in one bucket, and therefore, should have supe-
rior performance when the number of buckets is large,
whereas the nested loops algorithm needs to scan the
entire buffer.
3.2
Invalidation Costs
A key aspect of the windowed join operator, and for
that matter any windowed operator, is the invalidation
of the tuples that fall outside the window. Eventually,
the stale data that falls outside the window must be
removed in order to preserve system resources and to
ensure correct results. In the steady state, the rate of
tuples falling outside the window should on average
be equal to the rate of new tuples entering the system.
Therefore, the invalidation strategies for windowed
algorithms differ only in when they invalidate and not
in the rate at which they invalidate. Approaches for
invalidating stale tuples range from greedy to lazy
strategies. Greedy strategies attempt to remove stale
tuples as soon as possible, whereas lazy strategies de-
lay removal until an appropriate time.
In the case of the windowed hash join algorithm
invalidation occurs at the arrival time of a tuple on
an input. The greedy approach is to invalidate every
bucket at this time, while the lazy approach is to in-
validate only the bucket about to be probed.
The
greedy approach ensures optimal memory usage since
no stale tuples are allowed to accumulate between ar-
rivals. The lazy approach allows stale tuples to build
up in a bucket until that bucket is probed. In gen-
eral, the average number of tuples that build up in a
bucket is the product of the time interval between in-
validations and the rate that tuples enter that bucket. If
invalidation occurs only at probe time, then the num-
ber of stale tuples that build up in buffer B is λB/λA
as the time between probes is the time between the
80
Henry Kostowski and Kajal T. Claypool
strategies on the use of memory resources.

arrivals on stream A, NB/λA, and the rate that tu-
ples enter the bucket λB/NB. Here NB is the num-
ber of hash buckets for stream B. Thus, signiﬁcant
accumulation of stale tuples occurs only if the rate
of the probe stream is small compared to the rate of
the arrival stream. An intermediate strategy is to in-
validate a bucket at both probe time and at insertion
time. Since the rate of either insertion or probe is the
sum of the individual rates, the average number of
stale tuples that accumulate between invalidations is
λB/(λA + λB), implying that large buildups of stale
tuples should not occur even if the rates of the streams
differ considerably.
The time cost of invalidation itself depends on the
algorithm. For our implementation of the windowed
hash join, the circular buffer structure of the hash
bucket results in efﬁcient invalidation as stale tuples
can accumulate only at the bottom of the buffer. Thus,
the total number of comparisons needed is 1 + Nstale
as at least one comparison is needed with the ﬁrst
non-stale tuple. The term Nstale denotes the number
of stale tuples. The higher time cost of the optimal
greedy invalidation strategy is the cost of performing
at least one comparison with a tuple in every bucket.
The trade-off between greedy and lazy invalidation is
thus one of space versus time.
4
EXPERIMENTAL EVALUATION
In this section, we present our experimental evalu-
ation of the two join algorithms, windowed nested
loops (WNL) join and the windowed hash (WHJ)
join.
4.1
Experimental Setup and
Methodology
The windowed nested loops join and hash join algo-
rithms were implemented in C++ and evaluated on
Dell 2.0 GHz Pentium 4 workstation with 512MB
of memory.
A batch of tuples was prepared and
processed en mass and the total processing time mea-
sured. The size of a batch is determined directly to
simulate the desired input rate. All tests were per-
formed for a batch of 60 seconds worth of tuples and a
time window of 30 seconds for both inputs. Only the
last 30 seconds worth of tuples were timed in order
to allow the buffers to reach steady-state conditions.
For the comparison measurements, the selectivity of
the join was set to 0.01 and the number of hash buck-
ets set to a corresponding value of 100. In addition
to the total processing cost, the effect of tuning the
hash bucket algorithm to match the selectivity to the
number of hash buckets was measured. Finally, the
effectiveness of invalidation strategies with respect to
memory and processing time were also measured.
4.2
Performance Measurements
Figure 4: Performance of the Windowed Nested Loop and
Windowed Hash Join with Respect to the Relative Rate of
Figure 4 shows the performance difference be-
tween WNL and WHJ for 100 buckets. The two order
of magnitude difference indicates that the cost to scan
buffers for matches is the dominating cost. The cost
formulas (Equations 1 and 2) show that this cost is in-
versely proportional to the number of buckets (100 in
this case), and the measurements bear this out. Fig-
ure 5 shows that the total processing time is inversely
proportional to number of buckets (everything else
held constant). Note that when the number of buck-
ets is greater than the number of distinct values of the
join attributes, there is no longer any advantage to in-
creasing the number of buckets. The graph shows the
ﬂattening occurs at 100 for selectivity 0.01 (implying
100 distinct join values) as expected. Note that this
ﬂattening out effect is not captured by the cost for-
mula given in Equation 2.
Figure 6 depicts total processing time of the win-
dowed hash join for different values of selectivity and
number of hash buckets, and also shows that ﬂatten-
ing occurs approximately when the number of buckets
equals the number of distinct hash values.
The effect of using extra invalidation in the hash
join algorithm was also measured. We measured the
processing cost and the memory utilization of four in-
validation strategies. The four strategies are described
as follows. Probe time invalidation is the minimal
invalidation required for the algorithm’s correctness,
and consists of removing stale tuples from the probed
hash bucket only whenever a tuple arrives on any in-
put. Probe and insert time invalidation removes stale
tuples from the probed and the insertion hash buckets,
in addition to the minimal invalidation of the probe
81
Analytical and Experimental Evaluation of Stream-Based Join
the Input Streams.

Figure 5: Performance of the Windowed Hash Join with
Figure 6: Performance of the Windowed Hash Join with Re-
spect to the Join Selectivity and the Number of Hash Buck-
time invalidation strategy. The next two invalidation
strategies, called probe all buckets and optimal, re-
move tuples from all hash buckets on the probe side
only and all hash buckets on both the probe and in-
sertion sides respectively. Figure 7 shows the total
processing time of the four strategies. We see that the
probe and insert invalidation strategy does as well as
minimal invalidation strategy, i.e., the probe time in-
validation strategy. Both these strategies substantially
outperform the two greedy strategies that invalidate
all buckets.
Finally, Figure 8 shows the memory costs of the
four invalidation strategies. In these experiments, the
stream on the probed side had a relative rate 100 times
the rate of the opposite arrival side stream, and the
average occupancy of the entire set of all the hash
buckets on the probed side was measured every 2 sec-
onds for 60 seconds. We note that the steady state
conditions were reached after 30 seconds, which is
ditionally, we note that, with the exception of probe
time invalidation, all the other strategies had approx-
imately the same average memory usage. In particu-
lar, the probe and insert time invalidation performed
as well as optimal invalidation strategy. Given the su-
perior processing time performance in Figure 7 and
its memory consumption, we conclude that the probe
and insert time invalidation provides the best choice
both for performance and memory consumption un-
der steady state conditions.
Figure 8:
Average Memory Occupancy of Hash Table
5
RELATED WORK
Recent years have seen a ﬂurry of activity in streams
and continuous queries.
We brieﬂy describe three
pure stream databases that are being implemented,
the Aurora system(Carney et al., 2002), the STREAM
system(Arasu et al., 2002) and the Gigascope sys-
tem(Cranor et al., 2002). The Aurora project(Carney
et al., 2002) proposes an architecture for a stream
database system based on a data ﬂow architecture.
The user of the Aurora system designs a query by
expected given a window size of 30 seconds. Ad-
82
Henry Kostowski and Kajal T. Claypool
Respect to the Number of Hash Buckets.
ets.
Figure 7: Processing Costs Of Four Invalidation Strategies.
Buffers For Four Invalidation Strategies.

manipulating a diagram of the query in a GUI. The
query is built from boxes that represent individual
operations on a stream.
The boxes are analogous
to the relational operators in a relational system and
the diagram of boxes analogous to a relational query
plan. However, there is no higher level declarative
query language analogous to SQL to compose these
queries. The operators themselves perform ﬁltering
operations (which require no buffering) and window-
ing operations (which require buffering). The notion
of a window is extended to include slides, latches, and
tumbles. Slide moves a window continuously down-
stream, tumble moves a window discontinuously so
that consecutive windows share no tuples, and latch
moves a window like a tumble but also keeps state
information between positions of the window. The
analysis presented in this paper can be easily extended
to slide, tumble and latch windows.
STREAM is a design for a stream database sys-
tem currently being constructed at Stanford Univer-
sity(Arasu et al., 2002).
The STREAM system is
designed to be a conservative extension of relational
database concepts. They provide an SQL-like query
language, CQL, with extensions for windowing and
other stream primitive operators, a semantics based
on mapping CQL to relational tables, and an imple-
mentation architecture based on a dataﬂow paradigm.
A CQL query is parsed into a query plan consisting
of a tree of stream operators. Synopses are general
data structures associated with an operator that main-
tain any state needed by an operator to compute cor-
rect results. The query plan can then be optimized
both statically at compile time and dynamically at run
time. Most of the reported optimization strategies at-
tempt to minimize total memory requirements. Our
analysis goes along the lines of the STREAM(Arasu
et al., 2002) work and the analyzes the memory re-
quirements for the different invalidation strategies a
key factor when dealing with windowed operators.
Gigascope is a network performance monitoring
tool that incorporates stream database ideas in its im-
plementation(Cranor et al., 2002). The kinds of com-
plex queries that users typically wish to make against
network data streams are difﬁcult or impossible to ex-
press in SQL. Ordering tuples from a data stream by
time stamp is not sufﬁcient since, for example, ses-
sion information may present a different order than
the time of arrival.
Therefore, the notion of order
in a stream needs to be extended. The implications
of this extension for stream database operators are
numerous. The most important is that buffering re-
quirements are increased since the determination of
when to discard stale data is no longer directly tied
to time. For example, in a windowed join operator,
if one stream stalls the other may need to have un-
bounded buffers while waiting for new data to arrive
on the stalled stream. Extending the deﬁnition of or-
der may help optimization, since there is more room
to play with in the implementation of an operator: dif-
ferent operator implementations may produce differ-
ent ordering properties in the output. Gigascope im-
plements some of these new ordering deﬁnitions into
its operators. The Minimum Memory join algorithm
presented in (Cranor et al., 2002) is similar to our win-
dowed nested loops join (WNL) algorithm presented
in Section 2, and as such our analysis of the invalida-
tion strategies can be applied.
work have presented brief discussions on non-
blocking, windowed versions of nested loops join and
symmetric hash join algorithms for the implementa-
tion of the windowed join operator. Although our ap-
proach is based on the cost model proposed by Viglas
et al.(Viglas and Naughton, 2002), our work differs in
three ways. First, our version of the windowed nested
loops join differs from the one presented in (Viglas
and Naughton, 2002) in its invalidation process. In
particular, the Viglas nested loops join(Viglas and
Naughton, 2002) does not invalidate the opposite win-
dow on arrival of a tuple and therefore can output join
tuples that are not strictly within the window. Sec-
ond, while Viglas et al.(Viglas and Naughton, 2002)
provide some discussion of a hash join, the algorithm
itself is not presented. We explicitly present a win-
dowed hash join in this paper.
Third, we directly
compare our results to those reported in (Viglas and
Naughton, 2002). A signiﬁcant difference between
the two results is the quadratic dependence on the in-
put rate as discussed in(Viglas and Naughton, 2002)
for the windowed hash join. We did not observe the
same quadratic dependence. Rather, our implementa-
tion provides a linear dependence to the input rate (in
the half-cost analysis) for our implementation of win-
dowed hash join. We further provide an analysis of
both the performance and the memory requirements
of the different invalidation strategies.
6
CONCLUSIONS AND FUTURE
WORK
In this paper we have presented analytical and ex-
perimental evaluation of two implementations of a
windowed join operator.
The results clearly show
that the windowed hash join is superior to windowed
nested loops join based on an average cost per unit
time. Both analysis and measurement yield this re-
sult. We also compared our results to those reported
in (Viglas and Naughton, 2002). A signiﬁcant differ-
ence between the two results was the quadratic de-
pendence on the input rate as discussed in(Viglas and
Naughton, 2002) for the windowed hash join. We did
not observe the same quadratic dependence. Rather
83
Analytical and Experimental Evaluation of Stream-Based Join
Viglas et al. (Viglas and Naughton, 2002) in their

our implementation uses circular buffer based hash-
buckets to provide linear performance. It should be
noted however that the circular buffer approach is ap-
plicable only when the tuples are ordered by arrival
time stamp. We also examined the cost of lazy and
greedy invalidation strategies for both windowed join
algorithms and examined both the processing cost as
well as the memory requirements for both invalida-
tion schemes. It should be noted that the analysis of
the invalidation strategies can be easily extended to
apply for any windowed operator.
In terms of continuing work, while in this paper
we have presented implementations of the windowed
join operator, we have found that this join operator is
not associative and it relies on strict order of arrival
time alone. In general we have found that precisely-
deﬁned stream operator algebra that can be used to
express well-deﬁned stream queries – much in the
same way that the relational algebra deﬁnes relational
queries – is not found in the stream database litera-
ture. Each system implements their own set of opera-
tors but does not provide a way to describe the compo-
sition of operators. The general problem of the com-
position of stream operators is difﬁcult. One would
like to have operators that would take stream inputs
with well-deﬁned properties and produce stream out-
puts with well-deﬁned properties in such a manner
that the resulting composition algebra is simple. That
these goals may conﬂict can be seen from a consider-
ation of the window join operator. If the output of the
window join operator is ordered by the time stamp
of one of its composite tuples, then neither choice
will lead to an associative window join operator. Per-
haps the closest well-deﬁned stream semantics and
query language are those proposed by the Stanford
STREAM project(Arasu et al., 2002). These seman-
tics depend upon mapping stream queries onto the
standard relational semantics.
However, no stream
operator deﬁnitions are given that may implement
the proposed stream semantics(Arasu et al., 2002).
We are now working on deﬁning a pure stream alge-
bra that addresses these problems and provides well-
deﬁned stream semantics.
REFERENCES
Arasu, A., Babu, S., and Widom, J. (2002).
An Ab-
stract Semantics and Concrete Language for Contin-
uous Queries over Streams and Relations. Technical
report, Stanford University.
Babcock, B., Babu, S., Datar, M., Motwani, R., and Widom,
J. (2002). Models and Issues in Data Stream Systems.
In Principles of Database Systems (PODS).
Babu, S. and Widom, J. (2001). Continuous Queries over
Data Streams. In Sigmod Record.
Carney, D., Cetintemel, U., Cherniack, M., Convey, C.,
Lee, S., Seidman, G., Stonebraker, M., Tatbul, N., and
Zdonik, S. (2002). Monitoring Streams - A New Class
of Data Management Applications. In Int. Conference
on Very Large Data Bases, pages 215–226.
Chen, J., DeWitt, D., Tian, F., and Wang, Y. (2000). Ni-
agaraCQ: A Scalable Continuous Query System for
Internet Databases. In SIGMOD, pages 379–390.
Cranor, C., Gao, Y., Johnson, T., Shkapenyuk, V., and
Spatscheck, O. (2002). Gigascope: High Performance
Network Monitoring with an SQL Interface. In SIG-
MOD, page 623.
Motwani, R., Widom, J., Arasu, A., Babcock, B., Babu, S.,
Datar, M., Manku, G., Olston, C., Rosenstein, J., and
Varma, R. (2003). Query Processing, Resource Man-
agement, and Approximation in a Data Stream Man-
agement System. In Conference on Innovative Data
Systems Research.
Ullman, J. and Widom, J. (1997). A First Course in Data-
base Systems. Prentice-Hall, Inc.
Viglas, S. and Naughton, J. (2002). Rate-based Query Opti-
mization for Streaming Information Sources. In SIG-
MOD, pages 37–48.
84
Henry Kostowski and Kajal T. Claypool

PART 2 
Artificial Intelligence and 
Decision Support Systems

CONSTRUCTION OF DECISION TREES USING DATA CUBE 
Lixin Fu 
383 Bryan Bldg., University of North Carolina at Greensboro, Greensboro, NC 27402-6170, USA 
Email:lfu@uncg.edu 
Keywords: 
Classification, Decision Trees, Data Cube. 
Abstract: 
Data classification is an important problem in data mining. The traditional classification algorithms based 
on decision trees have been widely used due to their fast model construction and good model 
understandability. However, the existing decision tree algorithms need to recursively partition dataset into 
subsets according to some splitting criteria i.e. they still have to repeatedly compute the records belonging 
to a node (called F-sets) and then compute the splits for the node. For large data sets, this requires multiple 
passes of original dataset and therefore is often infeasible in many applications. In this paper we present a 
new approach to constructing decision trees using pre-computed data cube. We use statistics trees to 
compute the data cube and then build a decision tree on top of it. Mining on aggregated data stored in data 
cube will be much more efficient than directly mining on flat data files or relational databases. Since data 
cube server is usually a required component in an analytical system for answering OLAP queries, we 
essentially provide “free” classification by eliminating the dominant I/O overhead of scanning the massive 
original data set. Our new algorithm generates trees of the same prediction accuracy as existing decision tree 
algorithms such as SPRINT and RainForest but improves performance significantly. In this paper we also 
give a system architecture that integrates DBMS, OLAP, and data mining seamlessly. 
1 INTRODUCTION 
Data classification is a process of building a model 
from available data called training data set and 
classifying the objects according to their attributes. 
It is a well-studied important problem (Han and 
Kamber 2001), and has many applications in 
insurance industry, tax and credit card fraud 
detection, medical diagnosis, etc.  
The existing decision tree algorithms need to 
recursively partition dataset into subsets physically 
according to some splitting criteria. For large data 
sets, building a decision tree this way requires 
multiple passes of the original dataset, therefore, is 
often infeasible in many applications. In this paper 
we present a new approach of constructing decision 
trees using pre-computed data cube.  
Our main contributions in this paper include: 
x
designing a new decision tree classifier 
built on data cube, and  
x
proposing an architecture that takes the 
advantages of above new algorithm and 
integrates DBMS, OLAP systems, and data 
mining systems seamlessly.  
The remaining of the paper is organized as 
follows. The next section gives a brief summary of 
the related work. In Sec. 3, statistics tree structures 
and related data cube computation algorithms are 
described as the foundation of later sections. An 
architecture that integrates DBMS, OLAP, and data 
mining functions is proposed in Sec. 4. Sec. 5 
describes our new cube-based decision tree 
classification algorithm called cubeDT. Evaluation 
of cubeDT is given in Sec. 6. Lastly, we summarize 
the paper, and discuss the directions of our related 
future work.  
2 BACKGROUND 
Decision trees have been widely used in data 
classification. As its precursor algorithm ID-3 
(Quilan 1986), algorithm C4.5 (Quilan 1993) 
generates a simple tree in a top-down fashion. Data 
are partitioned into subsets recursively according to 
best splitting criteria determined by highest 
information gain until the partitions contain samples 
of the same classes. For continuous attribute A, the 
values are sorted and the midpoint v between two 
values is considered as a possible split. The split 
87
C.-S. Chen et al. (Eds.), Enterprise Information Systems VII, 87–94.
© 2006 Springer.

form is A  v. For a categorical attribute, if its 
cardinality is small, all subsets of its domain can be 
candidate splits; otherwise, we can use a greedy 
strategy to create candidate splits.  
SLIQ (Mehta, Agrawal et al. 1996) and 
SPRINT (Shafer, Agrawal et al. 1996) are more 
recent decision-tree classifiers that address the 
scalability issues for large data sets. Both use Gini 
index as impurity function, presorting (for numerical 
attributes), 
and 
breadth-first-search 
to 
avoid 
resorting at each node. Both SLIQ and SPRINT are 
still multi-pass algorithms for large data sets due to 
the necessity of external sorting and out-of-memory 
structures such as attribute lists.  
Surajit et al. (Chaudhuri, Fayyad et al. 1999) 
give a scalable classifier over a SQL database 
backend. They develop a middleware that batches 
query executions and stages data into its memory or 
local files to improve performance. At its core is a 
data structure called count table or CC table, a four-
column table (attribute-name, attribute-value, class-
value, count). Gehrke et al. give a uniform 
framework algorithm RainForest based on AVC-
group (a data structure similar to CC tables but as 
independent work) for providing scalable versions of 
most decision tree classifiers without changing the 
quality of trees (Gehrke, Ramakrishnan et al. 1998). 
With usually much smaller sizes of CC tables or 
AVC-group than the original data or attribute lists in 
SPRINT, these two algorithms generally improve 
the mining performance. However, they together 
with all other classification algorithms (as far as we 
know) including SLIQ and SPRINT still need to 
physically access (sometimes in multiple scans) 
original data set to compute the best splits, and 
partition the data sets in the nodes according to the 
splitting criteria. Different from these algorithms, 
our cube-based decision tree construction does not 
compute and store the F-sets (all the records 
belonging to an internal node) to find best splits, nor 
does it partition the data set physically. Instead, we 
compute the splits through the data cubes, as shown 
in more detail in Sec. 5.  
The BOAT algorithm (Gehrke, Ganti et al. 
1999) constructs a decision tree and coarse split 
criteria from a large sample of original data using a 
statistical technology called bootstrapping. Other 
classification 
methods 
include 
Bayesian 
classification (Cheeseman and Stutz 1996), back 
propagation (Lu, Setiono et al. 1995),  association 
rule mining (Lent, Swami et al. 1997), k-Nearest 
neighbor classification (Duda and Hart 1973), etc. 
Recently, a statistics-based classifier is built on top 
of data cube (Fu 2003). 
Since cubeDT is built on top of the technologies 
of OLAP and data cube, the performance of cube 
computation has a direct influence on it. Next, we 
briefly introduce some of the cube systems and cube 
computation algorithms. To compute data cubes, 
various 
ROLAP 
(relational 
OLAP) 
systems, 
MOLAP (multidimensional OLAP) systems, and 
HOLAP (hybrid OLAP) systems are proposed 
(Chaudhuri and Dayal 1997). Materialized views 
and indexing are often used to speedup the 
evaluation of data cubes and OLAP queries.  
Materializing all the aggregate GROUP_BY 
views may incur excessive storage requirements and 
maintenance overhead for these views. A view 
selection algorithm proposed by Harinarayan et al. 
(Harinarayan, Rajaraman et al. 1996) uses a greedy 
strategy to choose a set of views over the lattice 
structure under the constraint of certain space or 
certain number of views to materialize. Agarwal et. 
al (Agarwal, Agrawal et al. 1996) overlap or 
pipeline the computation of the views so that the 
cost of the processing tree is minimized. For sparse 
data, Zhao et al. proposed the chunking method and 
sparse data structure for sparse chunks (Zhao, 
Deshpande et al. 1997).
For dimensions with small cardinalities, bitmap 
indexing is very effective (O'Neil 1987). It is 
suitable for ad-hoc OLAP queries and has good 
performance due to quick bitwise logical operations. 
However, it is inefficient for large domains, where 
encoded bitmap (Chan and Ioannidis 1998) or B-
trees (Comer 1979) can be used. Other work related 
to indexing includes variant indexes (O'Neil and 
Quass 1997), join indexes, etc. Beyer and 
Ramakrishnan develop BUC (bottom-up cubing) 
algorithm for cubing the group-bys that are above 
some threshold (Beyer and Ramakrishnan 1999). 
Johnson and Shasha (Johnson and Shasha 1997) 
propose cube trees and cube forests for cubing. In 
order to improve the performance of ROLAP 
algorithms, which often require multiple passes for 
large data sets, a multidimensional data structure 
called Statistics Tree (ST) (Fu and Hammer 2000) 
has been developed. The computation of data cubes 
that have arbitrary combination of different 
hierarchy levels is optimized in (Hammer and Fu 
2001). Other important recent work include Dwarf 
(Sismanis, Deligiannakis et al. 2002) and QC-trees 
(Lakshmanan, Pei et al. 2003).  
3 SPARSE STATISTICS TREES 
An ST tree is a multi-way and balanced tree with 
each level in the tree (except the leaf level) 
corresponding to an attribute. Leaf nodes contain the 
aggregates and are linked to facilitate the storage 
and retrieval. An internal node has one pointer for 
each domain value, and an additional “star” pointer 
Lixin Fu 
88

representing the entire attribute domain i.e. the 
special ALL value.  
ST trees are static structures. Once the number 
of dimensions and their cardinalities are given, the 
shape of the tree is set and will not change while 
inserting new records. The ST tree has exactly (V+1) 
pointers for an internal node, where V is the 
cardinality of the attribute corresponding to the level 
of the node. There is a serious problem of this static 
ST tree structure: when many dimensions have large 
cardinalities, the ST tree may not fit into memory, 
thus incurring too many I/O’s for insertions. To 
address this issue, we develop a new data structure 
called SST (sparse statistics trees) and related 
algorithm to evaluate data cubes. 
1
2
1
1
1
1
1
1
1
1
2
7
7
5
*
15
30
*
*
*
*
*
*
*
30
30
6
6
1
1
1
1
15
*
*
*
6
6
1
30
SST is very similar to ST but the pointers are 
labeled with attribute values instead of implied 
contiguous values. When a new record is inserted 
into SST, attribute values are checked along the 
paths with the existing entries in the nodes. If not 
matched, new entries will be added into the node 
and new subtrees are formed. Different from ST 
trees, where the internal nodes have pointers of 
contiguous indexes, an SST tree’s pointers have 
labels of corresponding attribute values not 
necessary contiguous.  Fig. 1 shows an SST tree 
after inserting first two records (5, 7, 30) and (2, 15, 
6). The paths accessed or newly created while 
inserting the second record are shown in dashed 
lines. 
If the number of records is large in the training 
data set, at some point during the insertion process, 
SST may not fit into memory any more. A cutting 
phase is then started, which deletes the sparse leaves 
and save them on disk for later retrieval. The leaves 
that are cut in a phase form a run. After all input 
records have been inserted, the runs are merged. The 
dense cubes are re-inserted into SST but the sparse 
cubes are stored on disks. While evaluating a cube 
query after SST initialization, we first check the in-
memory SST tree. Starting from the root, one can 
follow all the pointers corresponding to the 
constrained attribute values specified in the query 
for the dimension of that node, to the next level 
nodes. Recursively descending level by level, 
eventually we reach the leaves. All the values in the 
fall-off leaves are summed up as the final answer to 
the input query. Sparse leaves are retrieved from the 
merged run stored on disks. 
4 ARCHITECTURE 
Differently from transactional processing systems 
e.g. commercial DBMS, OLAP and data mining are 
mainly used for analytical purposes at the 
organizational level. “A data warehouse is a subject-
oriented, integrated, time-variant, and nonvolatile 
collection of data in support of management’s 
decision making process” (Inmon 1996). 
There are some advantages of deploying data 
analysis on top of data warehouses. Firstly, data is 
clean and consistent across the whole organization. 
Secondly, we can also use the existing infrastructure 
to manipulate and manage large amounts of data. 
Thirdly, the DBMS over a data warehouse can 
choose any interested subset of data to mine on, 
implementing an ad-hoc mining flexibility. OLAP 
and data mining algorithms can give “big picture” 
information and interesting patterns. OLAM (online 
analytical mining) system integrates OLAP with 
data 
mining 
and 
mining 
knowledge 
in 
multidimensional databases. A transaction-oriented 
commercial DBMS alone is, however, not up to 
efficient evaluation of complex ad-hoc OLAP 
queries and effective data mining because DBMS 
has different workloads and requirements. A natural 
solution is then to integrate three systems tightly. 
Fig. 2 is our proposed architecture for such an 
integrated system. 
The undirected lines represent bi-directional 
information flows. Users can submit SQL, CQL, and 
DMQL (data mining query language) queries 
through a common GUI API interface. The parser 
parses the user inputs and dispatches to the 
corresponding DBMS, OLAP, and OLAM engines if 
no syntactic errors are detected. Otherwise, the error 
messages are returned. Related metadata information 
is stored and will be used later by the data 
processing engines. The running results from the 
engines can be represented in various formats such 
as diagrams, tables, etc. through a visualizer. 
Construction of Decision Trees Using Data Cube 
89
Figure 1: SST tree example. 

Parser
OLAM
Visualizer
Database API / File System
Loader
OLAP
Metadata
ST trees
SQL
CQL
Error
DMQL
Result
GUI API
Figure 2: System architecture that integrates DBMS, 
In addition to mining directly on databases or 
files, the OLAM engine can also be built on top of 
OLAP engines, which is the main topic of this paper. 
The OLAP, or data cube server, instructs a loader to 
construct ST trees from databases or files so that 
later on the cube queries are evaluated using the 
initialized ST trees (or SST trees), which is 
significantly faster than using DBMS servers 
(Hammer and Fu 2001). After the ST tree is 
initialized, the data cubes can be extracted from the 
leaves to construct decision trees.  
5 CONSTRUCTION OF DECISION 
TREES USING DATA CUBE 
5.1 A General Template of Building 
Decision Trees 
In decision tree classification, one recursively 
partitions the training data set until the records in the 
sub-partitions are entirely or mostly from the same 
class. When the data cubes have been computed, in 
this section we will design a new decision tree 
algorithm which builds a tree from data cubes 
without accessing original training records any 
more.
The internal nodes in a decision tree are called 
splits, predicates to specify how to partition the 
records. The leaves contain class labels that the 
records satisfying the predicates along the root-to-
leaf paths are classified into. We consider binary 
decision trees though multi-way trees are also 
possible. The following is a general template for 
almost all decision tree classification algorithms: 
Partition (Dataset S) { 
If (all records in S are of the same 
class) then
return;
Compute 
the 
splits 
for 
each 
attribute;
 Choose the best split to
 partition S into S1 and S2;
Partition (S1);
Partition (S2);
}
An initial call of Partition (training dataset) will 
setup a binary decision tree for the training data set. 
Before the evaluation of the splits, the domain 
values of the training records are all converted into 
integers starting from 0. The conversions can be 
done during the scanning of original training data. 
5.2 Compute the Best Split for the 
Root
Given a training dataset with N records each of 
which has d predictor attributes and the classifying 
attribute B, suppose that they are classified into C 
known classes Lp, p = 0, 1, …, C-1.  We use gini-
index to compute the splits at the root of the decision 
tree as follows.   
|
|
|,
|
,
),
(
)
(
)
(
|
|
,
/
)
(
,
1
)
(
2
2
1
1
2
1
2
2
1
1
1
0
2
S
n
S
n
S
and
S
o
d
partitione
is
S
if
S
gini
n
n
S
gini
n
n
S
gini
S
n
n
j
B
count
p
S
in
j
class
of
frequency
the
is
p
where
p
S
gini
j
j
C
j
j
 
 

 
 
 
 

 
¦

 
int
A split for continuous attribute A is of form 
value(A)  v, where v is the upper bound of some 
interval of index k (k = 0, 1, …, V-1, where V is the 
total number of values for A). To simplify, let us just 
denote this as A  k. The following algorithm 
evaluates the best split for attribute A. 
1.
x[j]=0, for j =0, 1, …, C-1; 
CountSum = 0; 
2.
minGini = 1; minSplit = 0; 
3.
for i = 0 to V-1 do
4.
   countSumÅcountSum+count(A=i);
5.
  n1= countSum; n2 =n-countSum; 
6.
  squaredSumL, squaredSumH = 0; 
7.
for j = 0 to C-1 do
8.
       x[j]=x[j]+count(A=i;B = j);  
  y = count(B=j) – x[j]; 
9.
      sqSumL Å sqSumL+(x[j] /n1)2;
Lixin Fu 
90
OLAP, and OLAM.

10.       sqSumH Å sqSumH+(y /n2)2;
11.
endfor
12.   gini(S1)=1-sqSumL;
 gini(S2)=1- sqSumH; 
13.   gini(S)=n1gini(S1)/n+n2gini(S2)/n;
14.   if  gini(S) < minGini then
15.     MiniGini=gini(S);minSplit = i; 
16.
endif
17.
endfor
Lines 1 and 2 initialize temporary variables 
countSum and array x, and current minimal gini idex 
minGini and its split position miniSplit. Lines 3 
through 17 evaluate all possible splits A i (i=0, 1, 
…, V-1) and choose the best one. Each split 
partitions data set S into two subset S1 = {r in S | 
r[A]  i} and S2 = S-S1. Line 4 tries to simplify the 
computation of the size of S1 i.e. count (A  i) by 
prefix-sum computation. Similarly, array x[j] is used 
to compute count(A  i; B = j) for each class j (j =0, 
1, …, C-1) in lines 1 and 8. All these count
expressions are cube queries evaluated by the 
method in Sec. 3.
For categorical attributes, the splits are of form 
value(A)  T, where T is a subset of all the attribute 
values of A. Any such subset is a candidate split.  
n1 = count(value(A) T), and n2= n-n1
pj = count(value(A)  T; B = j) / n1
Knowing how to compute these variables, we can 
similarly compute the gini(S) for each split and 
choose the best one, as we did for continuous 
attributes. The final split for the root is then the split 
with the smallest gini index among all the best splits 
of the attributes. 
5.3 Partitioning and Computing 
Splits for Other Internal Nodes 
The best split computed above is stored in the root. 
All existing decision tree algorithms at this point 
partition the data set into subsets according to the 
predicates of the split. In contrast, cubeDT does not 
move data around. Instead, it just virtually partitions 
data by simply passing down the split predicates to 
its children without touching or querying the original 
data records any more at this phase. The removal of 
the expensive process of data partitioning greatly 
improves the classification performance. 
The computation of splits for an internal node 
other than the root is similar to the method in Sec. 
5.2 except that the split predicates along the path 
from the node to the root are concatenated as part of 
constraints in the cube query. For example, suppose 
a table containing customer information has three 
predictor attributes: age, income, and credit-report 
(values are poor, good, and excellent). The records 
are classified into two classes: buy or not buy 
computer. Suppose the best split of A turns out to be 
“age  30,” and now we are computing splits for the 
attribute income at node B. Notice that value n1 = 
count ( income    v; age  30), and n2 = count(age 
30) – n1.  Here, we do not actually partition data set 
by applying the predicate “age  30,” instead, we 
just form new cube queries to compute the splits for 
the node B. As before, these cube queries are 
evaluated through partial traversal of ST or SST 
trees.
At node C, n1 = count ( income    v; age > 30), 
and n2 = count(age > 30) – n1. All other variables are 
computed similarly for evaluating the splits. 
Suppose that after computation and comparison the 
best split at B is “income   $40, 000,” the diagram 
shown in Fig. 3. gives the initial steps of evaluating 
splits of nodes A, B, and C. 
Age <= 30
Income <= $40K
Yes
No
A
B
C
….
Figure 3: Example of computing splits of non-root internal 
6 SIMULATIONS 
To verify the effectiveness of cubeDT, we have 
conducted preliminary studies by comparing it with 
BUC (bottom-up cubing) (Beyer and Ramakrishnan 
1999) since the predominant time of cubeDT is 
spent on the construction of the SST. We compare 
with BUC because a family of algorithms such as 
BUC, BUC-BST, Condensed Cube, and QC-trees 
are all based on recursive partitioning and thus have 
similar I/O efficiency. All the experiments are done 
on a Dell PC Precision 330, which has a 1.7GHZ 
CPU, 256MB memory, and the Windows 2000 
operating 
system. 
All 
the 
algorithms 
are 
implemented in C++.  
Construction of Decision Trees Using Data Cube 
91
nodes.

0
5 0 0
10 0 0
15 0 0
2 0 0 0
Nu m be r of Re cords
Runtime (sec.)
S S T
2 2 .1
4 3
2 0 9 .8
4 2 0 .4 6
B UC
5 0 .9
117 .9
7 4 3 .7
17 6 5 .3
5 0 K
10 0 K
5 0 0 K
1M
We used uniformly distributed random data and 
set each of the five dimensions with a cardinality of 
10. The number of records is increased from 50, 000 
to 1,000,000 (data set sizes from 1megabytes to 20 
megabytes). The runtimes are shown in Fig. 4. SST 
is about 2-4 times faster than BUC.  The 
performance improvements we achieve increase 
quickly with an increase in the number of records. 
Note that the runtimes are the times for computing 
the data cubes.
We also investigate the behaviour of SST and 
BUC by varying the number of dimensions and 
using data of zipf distribution (factor is 2). We set 
the number of records is 100,000 and the cardinality 
of each dimension is fixed to 20. The number of 
dimensions increases from 4 to 8. Figure 5 shows 
the construction times. Clearly SST is scalable with 
respect to the number of dimensions. 
The query evaluation times are much faster than 
construction times. We measure the query times 
using total response times of 100 random queries. 
The queries are generated by first randomly 
choosing three dimensions where random numbers 
within the domains are selected as queried values. 
All other coordinates in the queries are star values. 
Since our SST can fit into memories in these 
experiments, queries can be evaluated without I/O’s. 
SST is one order faster than other BUC (Figure 6). 
0
2 0 0
4 0 0
6 0 0
8 0 0
10 0 0
12 0 0
14 0 0
16 0 0
18 0 0
2 0 0 0
Number of dimensions
Runtime (sec.)
SST
2 3 .8
3 8 .2
58 .6
8 6
12 2
B U C
6 1
13 8
3 2 7.5
76 0 .9
174 3
4
5
6
7
8
0
0 .5
1
1.5
2
2 .5
3
Nu m be r of dim e nsion s
Response time (sec.)
S S T
0 .0 1
0 .0 1
0 .0 1
0 .0 1
B U C
0 .8 11
0 .8 8 1
0 .8 8 1
2 .3 9 3
4
5
6
7
These experiments show that SST has a better 
performance, however, notice that cubeDT is 
general, i.e. the method of computing data cube is 
not restricted to our cubing algorithms using ST or 
SST trees. It can be on top of other data cube 
systems such as BUC as well.  
We also compare the performance of cubeDT, 
which includes both cube computation and decision 
tree generation phases, with that of RainForest 
algorithm. According to (Mehta, Agrawal et al. 
1996) and (Shafer, Agrawal et al. 1996), SLIQ 
produces accurate trees significantly smaller than the 
trees produced by IND-C4 (a predecessor of C4.5) 
but is almost one order faster than IND-Cart. 
SPRINT is faster and more scalable than SLIQ while 
producing exactly the same trees as SLIQ.  Previous 
experiments have also shown that RainForest in 
(Gehrke, Ramakrishnan et al. 1998) offers a 
performance improvement of a factor five over the 
previous fastest algorithm SPRINT.  So, we compare 
our cubeDT algorithm with RainForest. 
Among several implementations of RainForest 
such as RF-Read, RF-Write, RF-Hybrid, and RF-
Lixin Fu 
92
Figure 4: Varying number of records.
Figure 5: Varying number of dimensions. 
Figure 6: Query response times. 

Vertical, RF-Read is fastest, assuming that the AVC-
groups of all the nodes at one level of the decision 
tree can fit into memory. In this case, one can only 
need one scan of reading the input data to compute 
the AVC-groups at that level and compute the best 
splits from the AVC-groups. Even in this ideal case 
(hardly usable in real applications), RainForest 
needs at least h passes of original potentially large 
input data set, where h is the height of the decision 
tree. Other implementations need more read/write 
passes. In contrast, cubeDT requires one pass of 
input set to compute the cubes, after that, the 
decision tree can be built from the data cube without 
touching the input data any more. In this set of 
experiments, we use uniform data set containing 
four descriptive attributes, each of size 10, and the 
class attribute has five class values. We increase the 
number of records from half million to five millions. 
Figure 7 shows that cubeDT is faster, and more 
importantly, 
the 
performance 
gap 
becomes 
significantly wider when I/O times become 
dominant. The runtimes of cubeDT have already 
included the cube generation times, without which 
the decision tree construction using cube will be one 
order faster than RainForest. 
0
2000
4000
6000
8000
10000
12000
14000
Number of Records
Runtime (sec.)
cubeD T
1288
2100
9306
R ainF o rest
1328
2481
12037
500K
1M
5M
Since cubeDT uses the same formulas for 
computing the splits, it produces the same trees as 
SLIQ, SPRINT, and RainForest algorithms, that is, 
they have the same accuracy of classification. The 
accuracy issue is orthogonal to the performance 
issue here. However, cubeDT is significantly faster 
due to direct computation of splits from data cube 
without actually partitioning and storing the F-sets, 
especially when input data sets are so large that the 
I/O 
operations 
become 
the 
bottleneck 
of 
performance. 
7 CONCLUSIONS AND FUTURE 
WORK
In summary, in this paper we propose a new 
classifier that extracts some of the computed data 
cubes to setup decision trees for classification. Once 
the data cubes are computed by scanning the original 
data once and stored in statistics trees, they are ready 
to answer OLAP queries. The new classifiers 
provide additional “free” classification that may 
interest 
users. 
Through 
the 
combination 
of 
technologies from data cubing and classification 
based on decision trees, we pave the way of 
integrating data mining systems and data cube 
systems seamlessly. An architecture design of such 
an integrated system has been proposed. We will 
continue the research on the design of other efficient 
data mining algorithms on data cube in the future. 
REFERENCES
Agarwal, S., R. Agrawal, et al. (1996). On The 
Computation 
of 
Multidimensional 
Aggregates. 
Proceedings of the International Conference on Very 
Large Databases, Mumbai (Bomabi), India: 506-521. 
Beyer, K. and R. Ramakrishnan (1999). Bottom-Up 
Computation 
of 
Sparse 
and 
Iceberg 
CUBEs. 
Proceedings of the 1999 ACM SIGMOD International 
Conference on Management of Data (SIGMOD '99).
C. Faloutsos. Philadelphia, PA: 359-370.
Chan, C. Y. and Y. E. Ioannidis (1998). Bitmap Index 
Design and Evaluation. Proceedings of the 1998 ACM 
SIGMOD International Conference on Management of 
Data (SIGMOD '98), Seattle, WA: 355-366.
Chaudhuri, S. and U. Dayal (1997). "An Overview of Data 
Warehousing and OLAP Technology." SIGMOD
Record 26(1): 65-74. 
Chaudhuri, S., U. Fayyad, et al. (1999). Scalable 
Classification over SQL Databases. 15th International 
Conference on Data Engineering, March 23 - 26, 
1999,  Sydney, Australia: 470.
Cheeseman, 
P. 
and 
J. 
Stutz 
(1996). 
Bayesian 
Classification (AutoClass): Theory and Results. 
Advances in Knowledge Discovery and Data Mining.
R. Uthurusamy, AAAI/MIT Press: 153-180.
Comer, D. (1979). "The Ubiquitous Btree." ACM
Computing Surveys 11(2): 121-137. 
Duda, R. and P. Hart (1973). Pattern Classification and 
Scene Analysis. New York, John Wiley & Sons. 
Fu, L. (2003). Classification for Free. International
Conference on Internet Computing 2003 (IC'03) June 
23 - 26, 2003, Monte Carlo Resort, Las Vegas, 
Nevada, USA.
Construction of Decision Trees Using Data Cube 
93
Figure 7: Varying number of records.

Fu, L. and J. Hammer (2000). CUBIST: A New Algorithm 
For Improving the Performance of Ad-hoc OLAP 
Queries. ACM Third International Workshop on Data 
Warehousing and OLAP, Washington, D.C, USA, 
November: 72-79. 
Gehrke, J., V. Ganti, et al. (1999). BOAT - Optimistic 
Decision Tree Construction. Proc. 1999 Int. Conf. 
Management of Data (SIGMOD '99), Philadephia, 
PA, June 1999.: 169-180.
Gehrke, J., R. Ramakrishnan, et al. (1998). RainForest - A 
Framework for Fast Decision Tree Construction of 
Large Datasets. Proceedings of the 24th VLDB 
Conference (VLDB '98), New York, USA, 1998: 416-
427.
Hammer, J. and L. Fu (2001). Improving the Performance 
of OLAP Queries Using Families of Statistics Trees. 
3rd International Conference on Data Warehousing 
and Knowledge Discovery DaWaK 01, September, 
2001, Munich, Germany: 274-283. 
Han, J. and M. Kamber (2001). Data Mining: Concepts 
and Techniques, Morgan Kaufman Publishers. 
Harinarayan, 
V., 
A. 
Rajaraman, 
et 
al. 
(1996). 
"Implementing data cubes efficiently." SIGMOD
Record 25(2): 205-216. 
Inmon, W. H. (1996). Building the Data Warehouse. New 
York, John Wiley & Sons. 
Johnson, T. and D. Shasha (1997). "Some Approaches to 
Index Design for Cube Forests." Bulletin of the 
Technical Committee on Data Engineering, IEEE 
Computer Society 20(1): 27-35. 
Lakshmanan, L. V. S., J. Pei, et al. (2003). QC-Trees: An 
Efficient Summary Structure for Semantic OLAP. 
Proceedings of the 2003 ACM SIGMOD International 
Conference on Management of Data, San Diego, 
California, USA, June 9-12, 2003. A. Doan, ACM:
64-75.
Lent, B., A. Swami, et al. (1997). Clustering Association 
Rules. Proceedings of the Thirteenth International 
Conference on Database Engineering (ICDE '97), 
Birmingham, U.K.: 220-231. 
Lu, H., R. Setiono, et al. (1995). NeuroRule: A 
Connectionist Approach to Data Mining. VLDB'95, 
Proceedings of 21th International Conference on Very 
Large Data Bases, September 11-15, 1995, Zurich, 
Switzerland. S. Nishio, Morgan Kaufmann: 478-489. 
Mehta, M., R. Agrawal, et al. (1996). SLIQ: A Fast 
Scalable Classifier for Data Mining. Advances in 
Database Technology - EDBT'96, 5th International 
Conference on Extending Database Technology, 
Avignon, France, March 25-29, 1996, Proceedings. G. 
Gardarin, Springer. 1057: 18-32. 
O'Neil, 
P. 
(1987). 
Model 
204 
Architecture 
and 
Performance. Proc. of the 2nd International Workshop 
on High Performance Transaction Systems, Asilomar, 
CA: 40-59.
O'Neil, P. and D. Quass (1997). "Improved Query 
Performance with Variant Indexes." SIGMOD Record 
(ACM Special Interest Group on Management of 
Data) 26(2): 38-49. 
Quilan, J. R. (1986). Introduction of Decision Trees. 
Machine Learning. 1: 81-106.
Quilan, J. R. (1993). C4.5: Programs for Machine 
Learning, Morgan Kaufmann. 
Shafer, J., R. Agrawal, et al. (1996). SPRINT: A Scalable 
Parallel Classifier for Data Mining. VLDB'96,
Proceedings of 22th International Conference on Very 
Large Data Bases, September 3-6, 1996, Mumbai 
(Bombay), India. N. L. Sarda, Morgan Kaufmann:
544-555.
Sismanis, Y., A. Deligiannakis, et al. (2002). Dwarf: 
shrinking the PetaCube. Proceedings of the 2002 ACM 
SIGMOD international conference on Management of 
data (SIGMOD '02), Madison, Wisconsin: 464 - 475. 
Zhao, Y., P. M. Deshpande, et al. (1997). "An Array-
Based Algorithm for Simultaneous Multidimensional 
Aggregates." SIGMOD Record 26(2): 159-170. 
Lixin Fu 
94

AN APPLICATION OF NON-LINEAR PROGRAMMING TO 
TRAIN RECURRENT NEURAL NETWORKS IN TIME SERIES 
PREDICTION PROBLEMS 
M.P. Cuéllar, M. Delgado and M.C. Pegalajar 
Department of Computer Science and Artificial Intelligence. E.T.S. Ingeniería Informática. Univerity of Granada, C/. Pdta 
Daniel Saucedo Aranda, s.n.. Granada (Spain, Europe) 
Email: manupc@decsai.ugr.es, mdelgado@ugr.es, mcarmen@decsai.ugr.es
Keywords: 
Non-Linear Programming, Recurrent Neural Networks, Time Series Prediction. 
Abstract: 
Artificial Neural Networks are bioinspired mathematical models that have been widely used to solve many 
complex problems. However, the training of a Neural Network is a difficult task since the traditional 
training algorithms may get trapped into local solutions easily. This problem is greater in Recurrent Neural 
Networks, where the traditional training algorithms sometimes provide unsuitable solutions. Some 
evolutionary techniques have also been used to improve the training stage, and to overcome such local 
solutions, but they have the disadvantage that the time taken to train the network is high. The objective of 
this work is to show that the use of some non-linear programming techniques is a good choice to train a 
Neural Network, since they may provide suitable solutions quickly. In the experimental section, we apply 
the models proposed to train an Elman Recurrent Neural Network in real-life Time Series Prediction 
problems.
1 INTRODUCTION 
Artificial 
Neural 
Networks 
are 
bioinspired 
mathematical models that have been widely used to 
solve many complex problems, difficult to model 
because of their non-linearity, lack of information, 
or excessive difficulty. The most common neural 
network models used to solve these problems have 
mainly been feedforward networks (Haykin, 1999). 
There are many training algorithms for this kind of 
network and most are based on error propagation 
methods: for instance, backpropagation algorithms 
and similar ones. However, the disadvantage of 
these algorithms is that the search for the best 
solution is frequently trapped in a local optimum, 
therefore making it difficult for the network to work 
well.
In the case of Recurrent Neural Networks (D.P. 
Mandic et al., 2001), there are not as many training 
algorithms 
as 
for 
feedforward 
ones. 
These 
algorithms (M. Hüsken et al., 2003; R.J. Williams et 
al., 1989-1990) also share the same disadvantage as 
those used to train feedforward networks in that they 
get trapped in local optimal solutions very easily. In 
fact, This problem is greater in recurrent neural 
networks. Some evolutionary techniques have been 
proposed as a good choice to train these kind of 
networks (Blanco et al., 2001; M.P. Cuéllar et al., 
2004), because they can overcome the local optimal 
solutions, but they have the drawback that the 
training stage takes too much time. 
In this work, we propose some non-linear 
programming algorithms to train Recurrent Neural 
Networks. These algorithms are the BFGS (C. Zhu 
et al., 1997; R.H. Byrd et al., 1995) and the 
Levenberg-Marquardt minimization methods. Both 
algorithms are related to the Gauss-Newton method, 
but the implementations used in this work may be 
used to solve different kind of problems, as we will 
show in section 3.  
Related to Neural Network training, the BFGS 
method has been used to train feedforward networks, 
being hybridized with the evolutionary Scatter 
Search algorithm (R. Martí et al., 2002), in order to 
improve the solutions during the evolutionary 
process, obtaining suitable results in function 
approximation problems. On the other hand, the 
Levenberg-Marquardt (LM) (D.W. Marquardt, 1963; 
J.J More, 1977) method is a widely known algorithm 
used to optimize Radial Basis Function Networks 
(R. Zemomi, 2003). Some implementations have 
adapted the LM algorithm to train feedforward 
95
C.-S. Chen et al. (Eds.), Enterprise Information Systems VII, 95–102.
© 2006 Springer.

networks (M.T. Hagan et al., 1994). Other 
applications of both algorithms are ralated to solve 
minimization problems.  
In this work, we use the models proposed to train 
an Elman Recurrent Neural Network (ERNN) (D.P. 
Mandic et al., 2001) in real-life Time Series 
prediction problems. The reasons to choose an 
ERNN model are mainly related to the problem to 
solve:
Time Series are, basically, a data chaining, 
indexed in time. Such data chaining often has some 
temporal properties, in the sense that the value of the 
Time Series at the current time depends on some 
unknown past values. The objective of Time Series 
Prediction is to model the Time Series, and then to 
predict the future values with minimum error. Many 
approaches have been used to model a Time Series, 
and to the date, it is still a great problem. Traditional 
techniques, basically statistical methods, are very 
limited, since the models used are mainly linear 
regressions, and the temporal properties of the time 
series are often non-linear ones. In recent works, 
there are many approaches to model a Time Series 
with non-linear models, and Neural Networks play a 
great role in this field. The use of RBF Networks is 
very common in Time Series Prediction (R. Zemomi 
et al., 2003), but they have the disadvantage that the 
analysis of the temporal properties must be done 
manually, therefore increasing the number of 
experiments. Recurrent Neural Networks is a 
suitable approach to model a Time Series, since the 
recurrence allows the network to learn the temporal 
properties that the input data have, and the analysis 
of the temporal properties is done automatically. 
Considering some models of Recurrent Networks, 
the ERNN has obtained the best experimental results 
in our work. 
This work is structured as follows: Section 2 
introduces the Elman Recurrent Neural Neural 
Network. In section 3, the non-linear programming 
algorithms proposed are exposed. Section 4 shows 
the experimenta results obtained. Finally, section 5 
discusses the conclusions obtained. 
2 ELMAN RECURRENT NEURAL 
NETWORKS
An Elman recurrent neural network has three neuron 
layers: one is used for the input data, another is the 
hidden neuron layer, and the other is the output 
neuron layer. The network also has an additional 
neuron layer, called the state neuron layer. There are 
as many neurons in the state neuron layer as there 
are in the hidden layer. Recurrence is carried out 
from the hidden neurons to the state neurons so that 
the output of a hidden neuron at time t is also input 
to all hidden neurons at time t+1. This idea is 
illustrated in Figure 1. 
Figure 1 shows an example of an Elman recurrent 
neural network with two inputs, one output, and 
three hidden neurons. Therefore, the state neuron 
layer also has three neurons, corresponding to the 
values of the three hidden nodes at the previous 
time. The equations of the network dynamics are: 
(1) 
(2) 
(3) 
(4) 
where: 
x
YBk
B(t) is the output of neuron k in the output 
layer, at time t.
x
NetoutBk
B(t) is the output of neuron k in the 
output layer, when the activation function has 
not yet been applied.
x
NethBk
B(t) is the output of neuron k in the hidden 
layer, when the activation function has not yet 
been applied.
x
SBk
B(t) is the output of neuron k in the hidden 
layer.
x
H is the number of neurons in the hidden layer.
x
I is the number of neurons in the input layer.
x
SBk
B(t-1) is the state value, corresponding to the 
state neuron k, at time t. 
)
G(netout
(t)
Y
k
k
 
¦
¦
 
 


 
H
1
h
I
1
i
i
ji
h
jh
h
(t)
X
V
1)
(t
S
U
(t)
neth
(t))
f(neth
(t)
S
j
j
 
¦
 
 
H
0
j
j
kj
k
(t)
S
P
(t)
netout
Figure 1: Example of an Elman recurrent neural 
network with two intputs, three hidden neurons, and 
M.P. Cuéllar et al. 
96
one output. 

x
F(·) is the activation function of the neurons in 
the hidden layer.
x
G(·) is the activation function of the neurons in 
the output layer.
x
VBij
B is the weight of connection from neuron j in 
the input layer to neuron i in the hidden layer.
x
UBij
B is the weight of connection from neuron j in 
the state layer to neuron i in the hidden layer.
x
PBij
B is the weight of connection from neuron j in 
the hidden layer to neuron i in the output layer.
x
XBi
B(t) is the network input i at time t. 
The traditional gradient-based training algorithm 
used to train this kind of network is the 
Backpropagation Through Time (BPTT) algorithm. 
The main drawback of this algorithm, as shown in 
section 1, is that the method gets trapped into local 
optimal solutions very easily.  
In 
the 
following 
section, 
the 
non-linear 
programming algorithms proposed in this work, and 
its application to Recurrent Neural Network training, 
are introduced. 
3 NON-LINEAR PROGRAMMING 
ALGORITHMS
Before to explain the BFGS and the LM algorithms, 
an introduction to non-linear programming is shown. 
A non-linear program is a problem that can be 
considered as a minimization task with the following 
structure:
(5) 
The function F is called objective function, x is a 
vector of  n variables to be optimized. The functions 
gBi
B and hBj
B are called constraint equations. The 
constraint equations remark some conditions that 
must be fulfilled for a set of variables in x. If the 
value m=0, then there are no conditions or 
dependencies in the variables in x, and the problem 
is called unconstrained.
 
There are many reasons to choose the BFGS and 
the LM algorithms in this work. These are: 
-
They have been applied to optimize 
feedforward neural networks, obtaining good 
results. Therefore, it may be interesting to 
apply them to train recurrent networks, 
where the training stage is more difficult. 
-
The versions of the algorithms implemented 
in this work may be applied to problems with 
different characteristics, as shown below. 
Depending on the problem to solve, it may be 
classified, considering the number of training data, 
into 
underdetermined, 
determined, 
and 
overdetermined 
problems. 
A 
problem 
is 
underdetermined when there are less training 
examples than variables to optimize; it is determined
when there are as many training examples as 
variables to optimize, and overdetermined when 
there are more training examples than variables to 
optimize. The determined problems are not very 
common, so that in this work we only consider the 
underdetermined and the overdetermined problems. 
Furthermore, in some applications of Recurrent 
Neural Networks, it may be interesting to keep the 
weight values belonging not to , but to a valid 
interval in . An example of this situation is when 
the neural network must be implemented in 
hardware, and the number of bits to represent a 
weight is fixed. We can see this situation as a 
constrained situation, so that we can also classify the 
problems into constrained and unconstrained
problems. 
Attending to the previous classifications, the 
BFGS and the LM algorithms proposed in this work 
may be applied to a concrete kind of problems, as 
shown in tables 1 and 2. 
 
Unconstrain
ed problems 
Constrained 
problems 
Underdetermined
problems
NO 
NO 
Overdetermined 
problems 
YES 
NO 
 
 
 
Table 2: Problems solved by BFGS
Unconstrain
ed problems 
Constrained 
problems 
Underdetermined
problems
YES 
YES 
Overdetermined 
problems 
YES 
YES 
As we can see, the applications of BFGS also 
include the problems solved by LM. However, in the 
experimental section, we show that the solutions 
provided by LM are much better than the ones 
1
1
1
j
1
i
n
m
m
0
m
1,..m
m
j
0,
)
x
(
h
1..m
i
0,
)
x
(
g
:
to
subject
),
x
F(
minimize
t
t

 
t
 
 


x
Table 1: Problems solved by LM
An Application of Non-linear Programming to Train Recurrent Neural Networks 
97

provided 
by 
BFGS 
for 
overdetermined 
unconstrained problems. 
Below, subsections 3.1 and 3.2 explain the BFGS 
and the LM algorithms, and their application to 
Recurrent Neural Network training. 
3.1 The BFGS algorithm 
The BFGS algorithm was firstly proposed in 1970 
by Broyden, Fletcher, Goldfarb and Shanno. Since 
then, several approaches to improve the algorithm, 
and also to apply it to a wider set of problems, have 
been proposed. In this work, we use an adaptation of 
this algorithm, called limited-memory bound-
constrained/unconstrained BFGS algorithm (L-
BFGS-B) (R. H. Byrd et al., 1995). The L-BFGS-B 
algorithm solves a problem that is considered as 
shown in equation 5, where the constraint equations 
are basically bound constraints, it is said: 
hBj
B(x)= lBj
B dxBj
B duBj
B, j=1..n 
where lBj
B and uBj
B are the lower and upper bounds for 
the variable to optimize xBj
B, respectively. The main 
scheme of the algorithm is shown below: 
0.
At the beginning, a solution xBk
B, k=0, and the 
corresponding gradient for function F must 
be provided as input data. 
1.
If the search has converged, stop. 
2.
Find an initial local solution, by mean of 
calculating the Cauchy point. 
3.
Compute a search direction, called dBk
B.
4.
Perform a line search along dBk
B, subject to the 
bound constraints, in order to compute the 
step length OBk
B, and set xBk+1
B=xBk
B+OBk
B dBk
B.
5.
Compute F(xBk+1
B)
6.
Update the limited-memory matrices, if 
necessary. 
7.
Set k= k+1, and go to Step 1. 
Firstly, the algorithm computes an initial solution 
and a search direction, dBk
B. Then, the solution is 
modified according to dBk
B. The algorithm also apply a 
Quasi-Newton algorithm to approximate the Hessian 
matrix. In (C. Zhu et al., 1997; R.H. Byrd et al., 
1995), you can find an in-depth explanation about 
this algorithm.  
Now, the use of the L-BFGS-B algorithm to train 
an ERNN is explained. Firstly, an ERNN is 
considered as a non-linear program (equation 5). 
After that, we show how to calculate the gradient of 
the variables to be optimized. 
When training a neural network, the objective is 
to minimize the output error. One of the most 
common procedures used to do this, is to minimize 
the Mean Square Error (MSE) between the network 
output, and the desired output for the network. In the 
case of recurrent neural networks, the MSE must be 
minimized across the time, as equation 6 shows. 
(6)
where w is a vector containing the network weights, 
T is the number of training samples in the time, O is 
the number of network outputs, dBo
B(t) is the desired 
output for neuron o at time t, and YBo
B(t) is the 
network output provided by neuron o at time t (see 
equation 4). According to the notation introduced in 
Section 2, the vector w is structured as follows: 
Thus, equation 5 may be rewritten as follows: 
(7)
The area in brackets is optional, and it contains the 
bound constraints in constrained problems. 
 
The gradient value for a variable wBr
B, denoted qBr
B,
that must be provided to the algorithm, is calculated 
in the following way: 
 
Now, a vector Q, with the gradient values for the 
variables in vector w, may be defined as follows: 
¦
¦
 
 

 
 
O
1
o
2
o
o
T
1
t
(t))
d
(t)
(Y
t)
,
w
SE(
t)
,
w
SE(
T
1
)
w
MSE(
}
)
w
{MSE(
min
)
...P
..P
P
..P
P
...U
...U
..
U
..U
U
...V
V
..V
V
(V
w
OH
2H
21
1H
11
HH
2H
21
1H
11
HI
21
1I
12
11
 
»
¼
º
«
¬
ª
 
d
d


¦
 
n
j
u
w
l
w
j
j
j
..
1
,
:
to
subject
},
t)
,
w
SE(
T
1
{
minimize
n
T
1
t
r
r
w
)
w
MSE(
q
w
w
 
1..n
r
),
(q
Q
r
 
 
M.P. Cuéllar et al. 
98

 
The gradient value for each variable in w, using 
the notation from Section 2, is calculated below: 
Where 1didI, 1djdH, 1dodO. Thus, the gradient 
vector Q may be rewritten, following the same order 
defined for the vector w, as follows: 
You can find a Fortran free source code of the L-
BFGS-B 
algorithm 
in 
the 
web 
site 
http://www.ece.northwestern.edu/~nocedal/lbfgsb.ht
ml. In this work, we have translated it to C language, 
and also to adapt such source code in order to train 
ERNN.
3.2 The LM algorithm 
The basic LM algorithm was proposed by D.W. 
Marquardt in (D.W. Marquardt, 1963). Since then, 
there are many approaches to solve a non-linear 
program using the LM algorithm. 
The algorithm tries to fit n parameters xB1
B...xBn
B, in a 
non-linear problem. The initial assumption is that, 
sufficiently closed to the minimum of the function F
to be minimized, F may be approximated by a 
quadratic form: 
where h is a n-dimensional vector, and H is the 
Hessian matrix. If the approximation is good, then 
the optimal values for w may be calculated. 
Otherwise, the algorithm iterates using the steepest 
descent method (S. Haykin, 1999) in order to 
improve the current solution. The main scheme of 
the algorithm is structured as follows: 
0.
At the beginning, a solution xPk
P, k=0, and a 
step-length parameter O, must be provided as 
input data. 
1.
Compute F(xPk
P)
2.
Update the Hessian matrix, and calculate GxPk
P
3.
Compute F(xPk
P+GxPk
P).
4.
If  (F(xPk
P+GxPk
P) t F(xPk
P)), increase O by a factor 
of 10 
5.
If  (F(xPk
P+GxPk
P) < F(xPk
P)), then 
5.1. decrease O by a factor of 10 
5.2. set xPk+1
P= xPk
P+Gx
6.
Set k= k+1 
7.
If the algorithm has converged, stop. 
Otherwise, go to step 1. 
The implementation used in this work solves an 
overdetermined set of non-linear equations by mean 
of a modification of the LM algorithm. You can find 
a good explanation of the implementation of this 
algorithm in (J.J. More, 1977).  
Now, the training process for ERNN, using the 
LM algorithm, is explained. The training of an 
Elman 
Recurrent 
Neural 
Network 
may 
be 
considered as the fitting of set of m non-linear 
equations, being m the number of training samples: 
(8) 
 
Considering equation 8, m=T·O, where T is 
the number of training samples, and O is the number 
of network outputs; dBo
B(t) is the desired output for 
neuron o at time t, and YBo
B(t) is the network output 
provided by neuron o at time t.
 
Being considered an Elman Recurrent Neural 
Network as a set of m equations as shown in (8), the 
LM algorithm minimizes the sum of the squares for 
such m equations. Thus, equation 5 may be rewritten 
as follows: 
(9) 
 
Thus, equation 9 shows the function to be 
minimized, and the LM algorithm may be applied 
directly. 
 
There is an important advantage in the LM 
algorithm, being compared with BFGS. To explain 
it, we must take a look at equations 7 and 9. In (7), 
the function to minimize is the MSE across the time. 
Please note that the minimization is carried out for 
the sum of the errors in output values. On the other 
hand, equation (9) minimizes the errors for each 
¯
®
­
 
 

 
1..O
o
1..T
t
(t);
d
(t)
Y
t)
,
w
(
F
o
o
o
}
t))
,
w
(
(F
{
min
T
1
t
O
1
o
2
o
¦¦
 
 
x
H
x
2
1
x
h
Ȗ
F


|
¦
¦
¦
¦
 
 
 
 

 
 
w
w

 
w
w

 
w
w
O
1
o
o
o
oj
j
j
T
1
t
j
i
ji
T
1
t
j
k
jk
T
1
t
j
o
o
oj
(t))
d
(t)
(Y
P
(t))
(neth
f'
T
2
(t))
į(S
(t)))
(S
(t)(
X
V
t)
,
w
MSE(
(t)))
(S
1)
(t
S
U
t)
,
w
MSE(
(t)
(t))S
d
(t)
(Y
T
2
P
t)
,
w
MSE(
G
G
)
dP
t)
,
w
MSE(
..
dU
t)
,
w
MSE(
..
dV
t)
,
w
MSE(
(
Q
ok
jk
ji
w
w
w
 
An Application of Non-linear Programming to Train Recurrent Neural Networks 
99

output separately. This fact allows the LM algorithm 
to improve the minimization for each output 
individually, meanwhile the BFGS algorithm 
minimizes the global error for the whole set of 
outputs. Because of this, the LM algorithm may find 
better solutions as shown in the experimental 
section.  
4 EXPERIMENTAL RESULTS 
In this section, we show an example of the 
application of the model exposed in the previous 
section, to time series prediction problems. A time 
series is a sequence of values or observations, taken 
in time. The purpose of time series prediction is to 
predict the next values of such observations. In this 
work, we apply the model to two time series, taken 
from the web page www.economagic.com:
- Series1: ECB reference exchange rate, UK 
pound sterling-Euro, 215 pm (C.E.T.) UK Pound 
Sterling. Total values: 65, taken monthly from 1999 
to 2004-May. We predict the 5 last values, 
corresponding to the monthd of 2004. 
- Series2: Total Population of the U.S.; 
Thousands. Percentage variation from 1953 until 
2004-March. Total values: 615, taken monthly from 
1953. We predict the 3 last values, corresponding to 
the months of 2004. 
Figures 2-3 show the values of the time series. 
The parameters to be used in the algorithms are 
the following: 
-
Number of Input Neurons: 1
-
Number of Hidden Neurons: 8
-
Number of Output Neurons: 1
-
Stopping criteria: To reach 500 iterations 
-
Initial value for O: 0.0001
-
Weight interval (constrained BFGS): [-0.5, 
0.5] 
Considering the Network exposed in Section 2, 
the number of variables to be optimized is I·H + H·H 
+ O·H, where I is the number of input units, H is the 
number of hidden units, and O is the number of 
output units. Therefore, according to the previous 
specifications, the number of variables to optimize 
in each problem is 80. Thus, the problems to solve 
60 for Series1, and 613 for Series2, then Series1 is 
classified as an underdetermined problem (LM 
cannot be applied in this problem, in consequence), 
and Series2 as an overdetermined problem. 
We make 30 experiments for each algorithm in 
each Time Series. Tables 3 and 4 shows the results 
obtained. Column 1 means the algorithm. Column 2 
exposes the kind of solution (average, best or worse 
solution). Finally, columns 3, 4 and 5 shows the 
traning and test Mean Square Error of the solution, 
and the time taken to obtain it. The best solutions 
over the whole set of algorithms are underlined 
Algorithm
Solution 
Training 
MSE
Test MSE 
Time (in 
seconds)
Best 
6.7291e-04 8.0037e-04 
0.23 
Average 
8.0126e-04 9.6046e-04 
0.27 
BFGS
(Constrai-
ned) 
Worse 
1.3150e-03 1.5769e-03 
0.14 
Best
U9.4695e-05U
U1.0391e-04U
0.71 
Average 
6.0206
e-04 
6.9697e-04 
0.65 
BFGS(unc)
Worse 
4.9304e-02 5.8125e-02 
0.30 
Best 
1.6103e-04 1.7559e-04 
1.09 
Average 
1.2020e-03 1.5151e-03 
1.00 
Genetic
Algorithm 
Worse 
6.9654e-03 8.1305e-03 
0.97 
Best 
6.2092e-02 7.7774e-02 
1.12 
Average 
8.2314e-02 1.0431e-01 
1.03 
BPTT
Worse 
9.8027e-02 1.2536e-01 
0.95 
0
0,1
0,2
0,3
0,4
0,5
0,6
0,7
0,8
1
5
9 13 17 21 25 29 33 37 41 45 49 53 57 61 65
0
0,5
1
1,5
2
1
50
99
148 197 246 295 344 393 442 491 540 589
Table 3: Results for Series1
M.P. Cuéllar et al. 
100
Figure 2: Values of Series1 Time Series. 
Figure 3: Values of Series2 Time Series. 
may be classified (following the criteria exposed in 
section 3), into underdetermined, and overdeter- 
mined. As the number of training data are 

Algorithm 
Solution 
Training 
MSE
Test MSE 
Time (in 
seconds)
Best 
4.0093e-03 4.1564e-03 
1.31 
Average 
1.8302e-02 1.8788e-02 
1.25 
BFGS
constrained 
Worse 
4.8057e-02 4.9673e-02 
1.14 
Best 
1.2444e-04 1.2707e-04 
1.40 
Average 
1.0259e-03 1.0462e-03 
1.36 
BFGS
Unconstrai
-ned 
Worse 
5.3909e-02 5.4949e-02 
1.27 
Best
U8.5914e-05U
U8.8928e-05U
3.05 
Average 
2.2811e-03 2.3152e-03 
2.82 
LM
Worse 
1.3597e-02 1.4481e-02 
2.58 
Best 
3.6713e-04 3.8023e-04 
2.90 
Average 
4.7275e-03 4.8265e-03 
2.79 
Genetic
Algorithm 
Worse 
3.7482e-02 3.9969e-02 
2.76 
Best 
4.7391e-02 4.9600e-02 
1.41 
Average 
6.3998e-02 6.6973e-02 
1.36 
BPTT
Worse 
8.8101e-02 9.1760e-02 
1.33 
As we can see in tables 3-4, the non-linear 
programming algorithms obtain the best results. We 
also can see the difference in the constrained and the 
unconstrained BFGS: The unconstrained BFGS 
obtain better solutions, since the constrained can 
only take values in the bound intervals for the 
weights. For this reason, it is better to choose the 
unconstrained version than the unconstrained one, 
unless that bounds over the network weights are 
required. 
On the other hand, for the overdetermined 
problem, we can observe that the LM algorithm 
reach better solutions than BFGS, as we introduced 
in section 3.2. The LM algorithm can get more 
information about the problem because it tries to 
optimize each output value for each output neuron, 
since the BFGS only tries to minimize the output 
error for the whole set of output neurons (see 
equations 7 and 9). 
In tables 3-4, we also can see that the time taken 
to reach the solutions by the BFGS algorithm, is 
smaller than the time taken by the other algorithms, 
and it obtains suitable solutions. On the other hand, 
the LM algorithm spends more time than the rest of 
algorithms, but it can also obtain better solutions. 
Below, figures 4-5 show the adjustment and the 
prediction carried out by the best solutions in tables 
3 and 4 (the BFGS and the LM algorithm, 
respectively). Also, tables 5-6 expose the values of 
the real data and the prediction given by the 
solutions. Column 1 shows the real data, and 
Column 2 exposes the prediction given by the 
Network trained. 
REAL DATA 
PREDICTION 
0,69215 
0,702707 
0,67690 
0,690901 
0,67124 
0,674261 
0,66533 
0,668534 
0,67157 
0,672251 
REAL DATA 
PREDICTION 
0,975 
0,972407 
0,974 
0,969408 
0,973 
0,969410 
Now, a statistical t-test, with 0.05 of confidence 
level, is carried out in order to compare the BFGS 
and the LM algorithms. Tables 7-8 show the results 
of the t-test for the problems Series1 and Series2, 
respectively. We use (+) to mark the cells where the 
algorithm of Column x is better than the algorithm in 
Row y, (-) means that the algorithm of Column x is 
worse than the algorithm in Row y, and no sign 
means that there is no statistical difference between 
the models. 
0
0,1
0,2
0,3
0,4
0,5
0,6
0,7
0,8
1
7 13 19 25 31 37 43 49 55 61
Series1
Adjustment
Figure 4: Real data and Adjustment of the best solution 
Table 5: Values of Prediction for Series1
Table 6: Values of Prediction for Series2
0
0,5
1
1,5
2
1
73 145 217 289 361 433 505 577
Series2
Adjustment
Figure 5: Real data and adjustment of the best 
Table 4: Results for Series1
An Application of Non-linear Programming to Train Recurrent Neural Networks 
101
solution for Series 2. 
for Series 1. 

 
BFGS (Unconstrained) 
BFGS (Constrained) 
0.3118 
BFGS
(Unconstr.) 
0.4271 
LM 
0.3251 
0.3251 
 
BFGS 
(Constrained) 
BFGS
(Unconstr.) 
As shown in Tables 5-6, the statistical t-test has 
concluded that there is no statistical difference in the 
non-linear 
programming 
algorithms. 
However, 
tables 3-4 expose that the results using an algorithm 
may provide better results. What we recommend to 
solve a problem, is to make a set of experiments 
with each non-linear programming algorithm, and 
then choose the one that better results provide, in 
average. 
5 CONCLUSIONS 
In this work, we have introduced some non-linear 
programming algorithms to train Recurrent Neural 
Networks: the BFGS and the LM algorithms. After 
considering the training of an Elman Recurrent 
Neural Network as a non-linear programming 
problem, the models have been applied to some 
Time Series prediction problems in the experimental 
section, obtaining suitable results. The non-linear 
programming 
algorithms 
have 
improved 
the 
solutions provided by the traditional training 
algorithm for ERNN. They also have obtained better 
results than other recent techniques, such Genetic 
Algorithms, and those solutions have been reached 
in less time than the GA and the traditional 
algorithms. In addition, it also may be used when 
bound constraints are a requirement over the 
network weights, meanwhile this situation cannot be 
solved using traditional training algorithms. In 
conclusion, the use of non-linear programming 
techniques may be a good tool to be considered 
when training Recurrent Neural Networks. 
REFERENCES
Blanco, Delgado, Pegalajar. 2001. A Real-Coded genetic 
algorithm for training recurrent neural networks. 
Neural Networks, vol. 14, pp. 93-105. 
C. Zhu, R. H. Byrd and J. Nocedal. 1997. L-BFGS-B: 
Algorithm 778: L-BFGS-B, FORTRAN routines for 
large scale bound constrained optimization, ACM 
Transactions on Mathematical Software, Vol 23, Num. 
Cuéllar M.P., Delgado M., Pegalajar M.C.. 2004. A 
Comparative study of Evolutionary Algorithms for 
Training Elman Recurrent Neural Networks to predict 
the Autonomous Indebtedness. in Proc. ICEIS, Porto, 
Portugal, pp. 457-461. 
Danilo P. Mandic, Jonathon A. Chambers. 2001. 
Recurrent Neural Networks for Prediction. Wiley, 
John & Sons, Incorporated. 
D. W. Marquardt. 1963. An algorithm for least-squares 
estimation of nonlinear parameters, Journal of the 
Society for Industrialand Applied Mathematics, pp. 
Martin T. Hagan, Mohammed B. Menhaj. 1994. Training 
FeedForward networks with the Marquardt algorithm, 
IEEE transactions on Neural networks, vol 5, no. 6, 
pp. 989-993. 
Michael Hüsken, Peter Stagge. 2003. Recurrent Neural 
Networks 
for 
Time 
Series 
classification, 
Neurocomputing, vol. 50, pp. 223-235. 
More, J. J. 1977. The Levenberg-Marquardt algorithm: 
Implementation 
and 
theory. 
Lecture 
notes 
in 
mathematics, 
Edited 
by 
G. 
A. 
Watson, 
SpringerVerlag. 
R. H. Byrd, P. Lu and J. Nocedal. 1995. A Limited 
Memory 
Algorithm 
for 
Bound 
Constrained 
Optimization, SIAM Journal on Scientific and 
Statistical Computing , 16, 5, pp. 1190-1208. 
R. Martí, A. El-Fallahi. 2002. Multilayer Neural 
Networks: An experimental evaluation of on-line 
training methods. Computers and Operations Research 
31, pp. 1491-1513. 
Ryad Zemomi, Daniel Racaceanu, Nouredalime Zerhonn. 
2003. Recurrent Radial Basis fuction network for 
Time Seties prediction, Engineering appl. Of Artificial 
Intelligence, vol. 16, no. 5-6, pp. 453-463. 
Simon Haykin. 1999. Neural Networks (a Comprehensive 
foundation). Second Edition. Prentice Hall. 
Williams R.J., Peng J. 1990. An efficient Gradient-Based 
Algorithm for On-Line Training of Recurrent Network 
trajectories,” Neural Computation, vol. 2, pp. 491-501. 
Williams R.J., Zipser D. 1989. A learning algorithm for 
continually running fully recurrent neural networks, 
Neural Computation, vol. 1, pp. 270-280. 
Table 7: T-Test for the algorithms, in Series1
Table 8: T-Test for the algorithms, in Series2
M.P. Cuéllar et al. 
102
4, pp. 550-560. 
11431-441.

INTELLIGENT SOLUTION EVALUATION BASED ON 
ALTERNATIVE USER PROFILES 
Georgios Bardis 
Laboratoire Méthodes et Structures Informatiques – MSI, Faculté des Sciences, Université de Limoges 
83, rue d’Isle, 87060 Limoges cedex, France, Tel: (+33) 5 55 43 69 74, Fax: (+33) 5 55 43 69 77 
Email: gbardis@msi.unilim.fr 
Technological Education Institute of Athens, Department of Computer Science, 
Ag.Spyridonos St., 122 10 Egaleo, GREECE, Tel: (+30) 2 10 53 85 312, Fax: (+30) 2 10 59 10 975 
Email: gbardis@teiath.gr  
Georgios Miaoulis 
Technological Education Institute of Athens, Department of Computer Science, 
Ag.Spyridonos St., 122 10 Egaleo, GREECE, Tel: (+30) 2 10 53 85 312, Fax: (+30) 2 10 59 10 975 
Email: gmiaoul@teiath.gr 
Dimitri Plemenos 
Laboratoire Méthodes et Structures Informatiques – MSI, Faculté des Sciences, Université de Limoges 
83, rue d’Isle, 87060 Limoges cedex, France, Tel: (+33) 5 55 43 69 74, Fax: (+33) 5 55 43 69 77 
Email: plemenos@unilim.fr  
Keywords: 
Machine Learning, Multicriteria Decision Making, User Modeling. 
Abstract: 
The MultiCAD platform is a system that accepts the declarative description of a scene (e.g. a building) as 
input and generates the geometric descriptions that comply with the specific description. Its goal is to 
facilitate the transition from the intuitive hierarchical decomposition of the scene to its concrete geometric 
representation. The aim of the present work is to provide the existing system with an intelligent module that 
will capture, store and apply user preferences in order to eventually automate the task of solution selection. 
A combination of two components based on decision support and artificial intelligence methodologies 
respectively are currently being implemented. A method is also proposed for the fair and efficient 
comparison of the results.
1 INTRODUCTION 
The continuously increasing performance of modern 
computer hardware has made available software 
features that used to be prohibitive in terms of 
required time and complexity a few years ago. 
System developers are now not only willing but also 
able 
to 
design 
and 
implement 
powerful 
environments, rich in characteristics, capable of 
producing vast numbers of results in limited time. 
Nevertheless, the diversity of the user basis as well 
as the increased complexity and power of software 
systems call for intelligent features that will adapt 
the environment to each user’s characteristics. 
Personalized system behavior with respect to an 
individual user’s profile facilitates its use, increases 
both user and system efficiency and improves 
quality of the results.  
Adoption of user preferences for intelligent 
system response has been presented in numerous 
efforts in the area of hypermedia and the WWW, 
e.g. (Brusilovsky 01), (Chen 02), (Pazzani 97), 
(Soltysiak 98). Incorporation of user preferences in 
geometric representations is presented in (Essert-
Villard 00), where the user submits a set of constants 
together with a sketch from which the system 
extracts additional solution restrictions as well as in 
(Joan-Arinyo 03) where a genetic algorithm is 
periodically aided by the user to produce solutions 
closer to the latter’s preferences.  
On the other hand, the notion of multicriteria 
evaluation of building assemblies has also been 
103
C.-S. Chen et al. (Eds.), Enterprise Information Systems VII, 103–111.
© 2006 Springer.

presented in (Nassar 03), based on AHP calculated 
weights and a heuristic evaluation algorithm, but 
machine learning and user preferences have not been 
discussed.  
Our work proposes a component for intelligent 
solution evaluation according to user’s preferences 
in a declarative description environment. The 
proposed component combines multicriteria decision 
support and machine learning techniques for user 
modeling requiring only qualitative feedback on 
behalf of the user instead of exact geometric 
properties.
2 THE MULTICAD 
ENVIRONMENT
The MultiCAD system, presented in detail in 
(Miaoulis 02), was introduced as a platform 
supporting declarative object modeling (Plemenos 
95), thus assisting the transition from the intuitive to 
the geometric object representation. The described 
system is a complete design environment including 
modules for validation of the object description, 
storage and maintenance of the solutions produced, 
etc. 
Subsequent 
works 
have 
successfully 
implemented most of the described modules as well 
as additional ones that have evolved from this initial 
design – solution generation using CSP (Bonnefoi 
02) or genetic algorithms (Vassilas 02), concept 
modeling and ontology (Ravani 04), incorporation of 
architectural styles (Makris 03) and collaborative 
design (Golfinopoulos 04). Solution evaluation 
based on user preferences in this context was first 
introduced in (Plemenos 02) proposing a system 
based on the representation of each scene by a 
dedicated neural network. A new approach towards 
a user profile module was presented in (Bardis 04) 
describing two methods for user modeling and 
solution evaluation: a method based on the 
multicriteria nature of the problem and one relying 
on a neural network for the representation of each 
user’s preferences.  
The current work continues towards this 
direction by presenting the final design of the 
specific module – an approach incorporating 
multicriteria decision support and machine learning 
techniques – and the current stage of the 
implementation. 
Moreover, 
a 
set 
of 
criteria 
regarding alternative methods performance is 
introduced, that will serve as the basis for future 
testing and adjustment of the implemented module.  
3 THE INTELLIGENT USER 
PROFILE MODULE 
Figure 1 shows a typical session of the current 
implementation of MultiCAD where the declarative 
description of a scene appears together with the 
Georgios Bardis et al. 
104
Figure 1: A Typical MultiCAD Session.

visualization of one of the corresponding solutions.
The diagram in Figure 2 concentrates on the 
integration of the User Profile Module to the 
MultiCAD platform. 
This version of the system applies user profile 
information only after the geometric representations 
of the described objects have been generated, i.e. not 
during their generation. The current stage of our 
work focuses on the construction of those modules 
that are immediately connected to the user profile 
component of the system. 
Each declarative scene description may lead to a 
few thousands geometric representations complying 
with this description: the solutions. However, not all 
solutions are equally preferred by the user. Our 
intention is to eliminate those solutions not 
conforming to the user's preferences. Ideally, this 
has to happen with minimal or no user intervention. 
In particular, optimal user profile incorporation to 
the solution visualization process will maximize 
solution visualization throughput (SVT) (Bardis 04) 
and minimize the user intervention at later stages. In 
order to achieve this we have to resolve the 
following inter-connected problems: 
x
Solution representation and evaluation. 
x
User preferences modeling and representation. 
4 SOLUTION REPRESENTATION 
The geometric representation of each solution is 
translated to a set of attributes. This is a need that 
arises mainly from the fact that we have to reduce 
the complexity of the representation of each solution 
in order to be able to submit it as input to a neural 
network. In addition, this approach complies with 
the multicriteria decision methodologies (Vincke 
92), (Goodwin 98), in particular, their requirement 
to request the user’s evaluation through a limited set 
of object attributes instead of numerous geometric 
properties. We choose to observe a minimal set of 
attributes (Fribault 03), (Bardis 04). This set will be 
extended or revised in the future, since, for the 
moment, we care more for the development of a 
prototype covering all stages of the MultiCAD cycle 
(Miaoulis 02) instead of capturing all possible 
aspects of user preferences with respect to a building 
assembly. The attributes we have chosen are based 
on geometric characteristics of each solution and, 
therefore, can be easily extracted by its geometric 
representation.
In particular, the observed attributes for any 
solution Si are:  
BDi = Number of bedrooms 
BTi = Number of bathrooms 
NAi = Night-zone area 
DAi = Day-zone area 
NDSi = Night-zone / Day-zone separation 
SWBi = Existence of at least one south-western 
bedroom 
Therefore, each solution Si is represented by a 
vector of values: 
Si = ( BDi , BTi , NAi , DAi , NDSi , SWBi), 
for example 
Si = ( 2, 3, 52.4, 40.8, Partial, No). 
3 . So lu tio n
V isualisa tio n
5 . A ttrib ute  V alue s
E xtractio n
4. H um an
So lu tio n
E valuatio n
6. A u to m atic
So lutio n
Evaluation
U ser Pro file s
D a tabase
7 . In te llige nt 
U se r Pro file
U pda te
1 . Sce ne
D e scriptio n
D elarative
R e pre se ntatio n
2 . So lu tio n
G e ne ratio n
G eo m e tric
R ep re se ntatio n
0. U ser
Pro file
Initia lisatio n
3 . So lu tio n
V isualisa tio n
5 . A ttrib ute  V alue s
E xtractio n
4. H um an
So lu tio n
E valuatio n
6. A u to m atic
So lutio n
Evaluation
U ser Pro file s
D a tabase
7 . In te llige nt 
U se r Pro file
U pda te
1 . Sce ne
D e scriptio n
D elarative
R e pre se ntatio n
2 . So lu tio n
G e ne ratio n
G eo m e tric
R ep re se ntatio n
0. U ser
Pro file
Initia lisatio n
Intelligent Solution Evaluation Based on Alternative User Profiles 
105
Figure 2: User Profile Module - Block Diagram. 

The exact range of values for each observed 
attribute will be affected by the design of the ML 
component described in Section 6 as well as the 
solution generator used. Nevertheless, it is important 
to observe that solution generation of the existing 
system is not based on observed attributes. This 
implies that the number of generated solutions is not 
restricted by the range of values of the observed 
attributes. The observed attributes map each 
generated solution to a vector of values of restricted 
range. Thus, it may be the case that, at the present 
stage, two or more different solutions, i.e. of 
different geometric representations, are mapped to 
the same vector of observed attribute values.  
5 USER MODELING 
Figure 3 presents the database model that has been 
developed in order to store and maintain user profile 
information. Notice that only a few representative 
properties of entities and relationships appear in the 
ER graph. Apart from the entities directly connected 
with the user profile module, the database model 
also includes entities representing scene descriptions 
and geometric representations of the corresponding 
solutions, 
namely 
the 
DESCRIPTIONS 
and 
SOLUTIONS entities. Specialized databases have 
already been developed as part of other work, taking 
place in the context of the MultiCAD platform 
(Ravani 03). The database model proposed here is 
flexible enough to cooperate with these already 
existing structures and yet able to incorporate 
alternative implementations in case these become 
available in the future. 
Figure 2 offers insight regarding the choice of 
the specific entities and relationships appearing in 
the database model. In particular, 
USER. Example fields for the properties for the 
specific entity are the (unique) User Name, 
Password, First/Last Name, etc. 
PROJECT TYPE. A text name plus extra 
information connecting the specific project type with 
the corresponding description prototypes contained 
in 
alternative 
databases 
of 
the 
MultiCAD 
environment. 
DESCRIPTION. This entity represents the 
Prolog-like description of a scene. Importance 
represents the influence of the results of this session, 
i.e. the properties of the approved solutions during 
the specific this session, to the overall user profile 
for the specific project type.  
SOLUTION. Full geometric representation of 
each solution. 
U S ER
U S ER
D EC LAR ATIVE
D E SC R IPT IO N
D EC LAR ATIVE
D E SC R IPT IO N
S O LU TIO N
SO LU T IO N
ATT R IB U TE
ATT R IBU TE
PR O JEC T
TYPE
PR O JEC T
TYP E
SU BM ITS
SU BM ITS
YIE LD S
YIELD S
R EPR ESE NTED
BY
R EPR ESE NTED
BY
BE LO N G S
T O
BE LO N G S
T O
D S  PR O F ILE
D S PR O F ILE
W EIG H T
W E IG H T
D ATE
D ATE
IMP O R TAN CE
IMP O R T AN CE
V ALU E
V ALU E
G EN ER AT O R
G EN E R AT O R
M ID D LE
M ID D LE
1
N
1
N
N
N
M
1
N
G R AD ES
G R AD E S
1
M
K
N
IN TU IT IVE
IN T U IT IVE
AU TO M AT IC
AU TO M AT IC
M L PR O FILE
M L PR O FILE
N EU RA L
N ETW OR K
PA RAM E TER S
N E U RA L
N ETW OR K
PA RAM E TER S
M
N
K
U S ER
U S ER
D EC LAR ATIVE
D E SC R IPT IO N
D EC LAR ATIVE
D E SC R IPT IO N
S O LU TIO N
SO LU T IO N
ATT R IB U TE
ATT R IBU TE
PR O JEC T
TYPE
PR O JEC T
TYP E
SU BM ITS
SU BM ITS
YIE LD S
YIELD S
R EPR ESE NTED
BY
R EPR ESE NTED
BY
BE LO N G S
T O
BE LO N G S
T O
D S  PR O F ILE
D S PR O F ILE
W EIG H T
W E IG H T
D ATE
D ATE
IM P O R TAN CE
IM P O R T AN CE
V ALU E
V ALU E
G EN ER AT O R
G EN E R AT O R
M ID D LE
M ID D LE
1
N
1
N
N
N
M
1
N
G R AD ES
G R AD E S
1
M
K
N
IN TU IT IVE
IN T U IT IVE
AU TO M AT IC
AU TO M AT IC
M L PR O FILE
M L PR O FILE
N EU RA L
N ETW OR K
PA RAM E TER S
N E U RA L
N ETW OR K
PA RAM E TER S
M
N
K
Georgios Bardis et al. 
106
Figure 3: User Profile Module - Entity Relationship Model. 

ATTRIBUTE. Attributes used to map solution to 
a smaller space that will allow further processing 
with respect to user profiles. The fields include 
name, type (int, real, scalar – big, medium, small, 
max, min, etc.) 
NEURAL NETWORK PARAMETERS. The
values of all neural network construction variables. 
In addition to the aforementioned entities, a set 
of relationships will maintain the information about 
the active interconnection of the entities during user 
sessions. In particular, 
submits. Users submit scene descriptions for 
processing. Each scene is connected with a certain 
project type. 
yields. Each description, using one of the 
alternative solution generators that are available, 
results in a set of solutions complying with the 
description. 
is mapped to. Each solution is mapped to a set of 
values for the attributes we have chosen to observe.  
decision support profile. This relationship 
contains all information regarding the initial user 
profile as obtained by the Decision Support 
component. In particular, for each attribute of a 
specific project type, the user has already provided 
his/her personal interpretation of its importance for 
solution evaluation. This importance is represented 
by the corresponding weight. An additional personal 
parameter is that of the middle value for any 
attribute. The user is requested to suggest the middle 
value for all attributes, i.e. the actual value of the 
attribute that represents 50% performance of a 
solution with respect to the specific attribute. 
machine 
learning 
profile.
This 
is 
the 
relationship that interconnects the user, in the 
context of (any) specific project type, with the 
Machine Learning component, i.e. all values needed 
to fully describe the neural network used to 
represent the specific user’s dynamic profile. 
grades. Solutions are evaluated by the user 
through visual inspection, thus yielding the intuitive 
grade (approved/rejected or a number in case an 
alternative grading scheme is used). In addition, 
solutions are also evaluated based on the specific 
user’s profile for the specific project type. This 
information may be regenerated based on the 
contents of the database. Nevertheless, solution 
evaluation is a crucial and time-consuming task; 
hence, once the results are available they are stored 
in the database. 
6 SOLUTION EVALUATION 
Two alternative approaches are used for solution 
evaluation based on user preferences forming two 
independent components of the user profile module. 
Nevertheless, their concurrent operation and results 
affect the overall behavior of the system, as it will 
become apparent in the following section. 
Intelligent Solution Evaluation Based on Alternative User Profiles 
107
Figure 4: Decision Support Component - Weight Assignment using AHP. 

6.1 Decision Support Component 
Each user is requested to assign weights representing 
the importance of each one of the observed 
attributes. Two alternative methods for weight 
assignment are available to the user based on the 
corresponding stages of SMART (Goodwin 98) and 
AHP (Saaty 90) multicriteria decision algorithms 
respectively. An interesting discussion regarding 
dynamic weight assignment versus fixed weight 
values is presented in (Roberts 02). 
The weights are then used to produce a score for 
each generated solution using a fitness function. 
Alternative functions may be used in the future but 
we currently use the inner product of the user 
weights with the attribute values for each solution 
(Bardis 04). These scores are then used in order to 
sort the set of solutions as a list of descending user 
preference order: from the most to the least 
preferable solution. This Decision Support (DS) 
component has already been implemented and 
currently operates on solutions in the form of 
attribute vectors. An example execution of the DS 
component for user profile initialization is shown in 
Figures 4-5. Notice the inconsistency index in the 
AHP method, signifying discrepancies in the user’s 
answers, as well as the normalized weights 
appearing at the top row of the sample solution set 
evaluation of the SMART method. The user may 
rely on this component in order to obtain a set of 
automatically selected solutions based on the 
aforementioned weights. In this case solutions will 
be automatically visualized and presented to the user 
in descending preference order. However, the user 
may choose to manually select the preferred 
solutions thus contributing to the training of the 
neural network described in the next section. 
6.2 Machine Learning Component 
The ML component will be based on a neural 
network of six inputs – one for each observed 
attribute, at least one hidden layer and a single 
output, representing the approval/rejection of a 
solution by the user. Alternative structures of the 
network will be implemented and tested according to 
the criteria presented in the next section. This 
process may lead to the selection of more than one 
structures for systematic use as alternative ML 
components, similar to the use of two alternative 
methods for attribute weight assignment in the DS 
component. 
The user, having submitted a scene description, 
will evaluate the solutions that are generated and 
visualized for the specific scene. This set of 
approved/rejected solutions will serve as one of the 
training sets for the network(s). In particular, each 
example in a training set represents a correct input-
Georgios Bardis et al. 
108
Figure 5: Decision Support Component - Weight Assignment Using SMART. 

output mapping. In the present context, the input 
part, for any given solution, is comprised by the 
attribute values representing the specific solution. 
The output part simply contains the user’s approval 
or rejection for the specific solution. 
It is important to notice that the aforementioned 
process will be a completely transparent system task: 
the user will not have to submit any additional 
information with respect to the network(s) training 
and therefore, does not have to be aware of it. 
Automatic solution evaluation will be available from 
the very first session via the Decision Support 
component. 
In 
general, 
automatic 
solution
evaluation will be at the user’s discretion and the 
choice of the most appropriate component for this 
purpose will be based on the criteria presented in the 
next section. 
7 PERFORMANCE COMPARISON 
Either during the testing period or during the regular 
use of the system, at least two alternative methods  
will have to be compared with respect to their 
performance. In particular, solutions approved by 
each method will be compared to the solutions 
approved by the user. 
7.1 Performance Indices 
In order to be able to compare these methods we 
must 
provide 
the 
means 
to 
measure 
their 
performance. We concentrate on the solution 
selection stage that takes place after solution 
generation. Therefore, in the following, we will 
assume that solution generation has already taken 
place and the methods have been applied to the 
results. The application of each method to the 
solutions yields the corresponding subset of 
approved solutions. For simplicity, we mention only 
two methods in the following whereas, in practice, 
more alternatives – due to alternative weight 
assignment, alternative network adjustments, etc. – 
may be concurrently evaluated. 
In particular, let us define the following sets: 
G = The solutions generated based on the 
specific description of a scene. 
U = The preferred solutions, i.e. solutions in G 
that comply with the user preference. These 
solutions represent the user preference in the current 
context. Formally, U  G. In the following we may 
also refer to the members of U as approved
solutions.
G-U = The
discarded solutions, i.e. the 
generated solutions that are not preferred by the 
user.
M1 = solutions in G approved by Method 1. 
Formally, M1  G. 
M2 = solutions in G approved by Method 2. 
Formally, M2  G. 
|S| = the number of members of any set S. 
Therefore we may now define the hit rate of each 
method as: 
}
2
1
{
,
|
|
|
|
,
i
U
U
M
HR
i
i


 
,
i.e. the percentage of approved solutions 
captured by the specific method. 
The ratio of approved vs. total solutions selected 
by each method could also be used as measurement 
of their performance: 
}
2,1
{
,
|
|
|
|


 
i
M
U
M
PR
i
i
i
There are more than one ways to define a miss 
rate. We may define it as the percentage of discarded 
solutions that are selected by the method, expressed 
as:
}
2,1
{
,
|
|
|
|



 
i
U
G
U
M
MR
i
i
However, we expect that, only a small number of 
the generated solutions will fulfill the user 
preference. This is mainly due to time limitations 
posed by the requirement for human visual 
inspection. On the other hand, |G| greatly depends on 
the description and can vary significantly. Therefore, 
we need to relate the size of the error for each 
method with |U| instead of a quantity including |G|. 
Hence, we could alternatively define:
}
2,1
{
,
|
|
|
|


 
i
U
U
M
MMR
i
i
G
M2
M1
U
Intelligent Solution Evaluation Based on Alternative User Profiles 
109
Figure 6: Example Methods Performance.

and interpret a lower value as a better performance. 
Intuitively, this interpretation implies that a method 
should not select many discarded solutions when 
only a few preferred solutions exist. For example, 
when this rate is more than 1 the method gives more 
discarded solutions than the total number of 
approved solutions. Instead of 1, another value may 
be selected to reflect a specific performance 
threshold.  
The above are clarified in the example Venn 
diagram of Figure 6, representing a general case (i.e. 
no intersection is empty, no two sets are equal). For 
the sake of simplicity of the picture, the total number 
of solutions is rather small, i.e. |G| is only 35 
whereas this is generally not the case. Nevertheless, 
for the specific example, we have the following 
numbers: 
|G| = 35, |U| = 10, |M1| = 6, |M2| = 10, |M1  U| = 3, 
|M2  U| = 4, |M1 – U| = 3, |M2 – U| = 6 
Therefore for M1 we have: 
HR1 = 3/10, MR1 = 3/25, MMR1 = 3/10, PR1 = 3/6 
and for M2 we have: 
HR2 = 4/10, MR2 = 6/25, MMR2 = 6/10, PR2 = 4/10 
Table 1: Example Methods Performance Indices 
Method 
Hit Rate 
Performance 
Ratio
Miss Rate Modified
Miss Rate
Method 1 
3/10 = 30% 
3/6 = 50% 
3/25 = 12% 3/10 = 0.3 
Method 2 
4/10 = 40% 
4/10 = 40% 
6/25 = 24% 6/10 = 0.6 
Extreme 
Case 1 
1/10 = 10% 
1/1 = 100% 
0/25 = 0% 
0/10 = 0.0 
Extreme 
Case 2 
10/10 = 100% 10/35 = 28.6% 
25/25 = 
100%
25/10 = 2.5
M2 could be considered a worse (because of the 
higher miss rate) or a better (because of the higher 
hit rate) method than M1 depending on the 
interpretation of these numbers. Ideally, hit rate 
should be equal to 1, miss rate equal to 0 and 
performance ratio equal to 100%. Notice, however, 
that a method selecting only one preferred solution 
every time it is invoked (Extreme Case 1) would 
yield a performance ratio of 100% without 
necessarily representing an optimal method as 
shown by the low hit rate. On the other hand, simply 
selecting all produced solutions (Extreme Case 2) 
maximizes the hit rate but yields a low performance 
ratio. Extreme Case 1 appears to represent a method 
with acceptable performance whereas Extreme Case 
2 represents a trivial approach of no practical use. 
Therefore, a high Performance Ratio appears to be a 
necessary, although not sufficient, indication of an 
efficient method and it becomes apparent that we 
need a combination of these indices in order to 
accurately evaluate the performance of each method.  
7.2 Method Integration
The training process of the neural network will have 
to continue, with alternative scene descriptions, until 
the ML component is considered ready to support 
automatic solution evaluation. We may define this 
threshold of ML component based on the values of 
the performance indices we described above for the 
two alternative DS components as well as for the 
ML component itself. In the following we will 
assume that the user has initialized his/her profile 
giving answers to the DS components that 
reasonably represent his/her preferences.  
In particular, we will be able to rely on the 
results given by the network, and therefore adopt 
fully automated solution selection, as soon as the 
ML component performs consistently better than 
both of the DS alternatives. We can state that the 
ML component is mature, and therefore ready to 
take over automatic solution selection iff: 
PRML > PRDS1        PRML > PRDS2

HRML > HRDS1
    HRML > HRDS2 
MMRML < MMRDS1
    MMRML < MMRDS2
for an (adjustable) number of recent descriptions 
submitted by the user.  
The strictness of this set of conditions may be 
relaxed by omitting some of the inequalities. In any 
case, it is important to observe that, if the complete 
set of conditions is repeatedly true, this implies that 
the ML component will be capturing preferences 
better than the weight vectors submitted by the user 
himself/herself. In such a case, it will also be 
interesting to explore the possibility of capturing 
additional criteria that the user is not fully aware of, 
i.e.
sub-conscious criteria. This could become 
apparent through the examination of experimental 
results and users’ comments regarding system 
performance with respect to their preferences. 
8 FUTURE WORK 
This stage of our work will conclude with the 
detailed design and implementation the ML 
component. Subsequent performance comparison of 
the two components will lead to further refinement 
of their properties. Extending the solution evaluation 
to
grade 
assignment 
instead 
of 
plain 
approval/rejection will also be considered. In that 
case performance indices will have to be modified to 
also reflect the quality of the selected set of 
solutions.
Georgios Bardis et al. 
110

The next major stage of our work will focus on 
the enhancement of the user profile model with 
information originating from the connection between 
the declarative description and the corresponding 
approved solutions. Such an association will offer 
insight regarding the specific user’s interpretation of 
declarative properties and relations. Successful 
modeling of user preferences at the declarative as 
well as the geometric level will allow incorporation 
of user profile information to the process of solution 
generation, thus significantly improving system 
performance. 
ACKNOWLEDGMENTS
This study was co-funded by 75% from the 
European Union and 25% from the Greek 
Government under the framework of the Education 
and 
Initial 
Vocational 
Training 
Program 
– 
‘Archimedes’. 
REFERENCES
Bardis G., Miaoulis G., Plemenos D., 2004. An Intelligent 
User Profile Module for Solution Selection Support in 
the Context of the MultiCAD Project. 7e Infographie 
Interactive et Intelligence Artificielle (3IA), Limoges, 
France. 
Bonnefoi P.-F., Plemenos D., 2002. Constraint satisfaction 
techniques for declarative scene modeling by 
hierarchical decomposition, 3IA, Limoges, France 
Brusilovsky P., 2001. Adaptive Hypermedia, User 
Modeling And User-Adapted Interaction 11: 87-110. 
Chen C.C., Chen M.C., 2002. PVA: A Self-Adaptive 
Personal 
View 
Agent, 
Journal 
Of 
Intelligent 
Information Systems, 18:2/3, 173–194. 
Essert-Villard C., Schreck P., Dufourd J.-F. 2000. Sketch-
Based Pruning of a Solution Space within a Formal 
Geometric Constraint Solver, Artificial Intelligence 
124, 139-159. 
Fribault P., 2003. Modelisation Declarative d’Espaces 
Habitable, 
(in 
French), 
Doctoral 
dissertation, 
University of Limoges, France. 
Golfinopoulos V., Dragonas J., Miaoulis G., Plemenos D., 
2004. 
Declarative 
Design 
in 
Collaborative 
Environment, 7e 3IA, Limoges, France. 
Goodwin P., Wright G., 1998. Decision Analysis for 
Management Judgement, Second Edition, Wiley. 
Joan-Arinyo R., Luzon M.V., Soto A., 2003. Genetic 
Algorithms for Root Multiselection in Constructive 
Geometric 
Constraint 
Solving. 
Computers 
and 
Graphics 27, 51-60. 
Makris D., Ravani I., Miaoulis G., Skourlas C., Fribault 
P., Plemenos D., 2003. Towards a domain-specific 
knowledge 
intelligent 
information 
system 
for 
Computer-Aided 
Architectural 
Design, 
3IA 
conference, Limoges, France. 
Miaoulis G., 2002. Contribution à l'étude des Systèmes 
d'Information Multimédia et Intelligent dédiés à la 
Conception Déclarative Assistée par l'Ordinateur – Le 
projet MultiCAD (in French), Professorial dissertation, 
University of Limoges, France. 
Miaoulis G., Plemenos D., Skourlas C., 2000. MultiCAD 
Database: Toward a unified data and knowledge 
representation for database scene modeling, 3IA, 
Limoges, France. 
Nassar K., Thalet W., Beliveau Y., 2003. A Procedure for 
Multicriteria Selection of Building Assemblies, 
Automation in Construction 12, 543-560. 
Pazzani M., Billsus D., 1997. Learning And Revising User 
Profiles: The Identification Of Interesting Web Sites, 
Machine Learning 27, 313–331. 
Plemenos D., 1995. Declarative modeling by hierarchical 
decomposition. The actual state of the MultiFormes 
project, Communication in International Conference 
GraphiCon'95, St Petersburg, Russia. 
Plemenos D., Miaoulis G., Vassilas N., 2002. Machine 
learning for a General Purpose Declarative Scene 
Modeler. International Conference GraphiCon'2002, 
Nizhny Novgorod, Russia. 
Plemenos D., Tamine K., 1997. Increasing the efficiency 
of declarative modeling. Constraint evaluation for the 
hierarchical decomposition approach. International 
Conference WSCG’97, Plzen, Czech Republic. 
Ravani I., Makris D., Miaoulis G., Constantinides P., 
Petridis A., Plemenos D., 2003. Implementation of 
Architecture-oriented 
Knowledge 
Framework 
in 
MultiCAD Declarative Scene Modeling System, 1st
Balcan Conference in Informatics, Thessaloniki, 
Greece. 
Ravani J., Makris D., Miaoulis G., Plemenos D., 2004. 
Concept-Based Declarative Description Subsystem for 
Computer Aided Declarative Design (CADD). 7e 3IA,
Limoges, France. 
Roberts R., Goodwin P., 2002. Weight Approximations in 
Multi-attribute 
Decision 
Models, 
Journal 
of 
Multicriteria Decision Analysis 11: 291-303. 
Saaty, T.L., 1990. The Analytic Hierarchy Process, RWS 
Publications, Pittsburgh, USA. 
Soltysiak S.J., Crabtree I.B., 1998. Automatic learning of 
user profiles — towards the personalization of agent 
services, BT Technology Journal Vol. 16, No 3. 
Vassilas N., Miaoulis G., Chronopoulos D., Konstantinidis 
E., Ravani I., Makris D., Plemenos D, 2003. 
MultiCAD-GA: A System for the Design of 3D Forms 
Based on Genetic Algorithms and Human Evaluation, 
SETN 203-214, Thessaloniki, Greece. 
Vincke P., 1992. Multicriteria Decision-aid, Wiley (1992). 
Intelligent Solution Evaluation Based on Alternative User Profiles 
111

USING dmFSQL FOR FINANCIAL CLUSTERING 
Ramón Alberto Carrasco 
Dpto. de Lenguajes y Sistemas Informáticos, Universidad de Granada, Granada, Spain  
Email: rcarrasco@caja-granada.es 
María Amparo Vila 
Dpto. de Ciencias de la Computación e I.A., Universidad de Granada, Granada, Spain 
Email: vila@decsai.ugr.es
José Galindo 
Dpto. de Lenguajes y Ciencias de la Computación, Universidad de Málaga, Spain 
Email: ppgg@lcc.uma.es
Keywords: 
Clustering, Flexible Queries, Data Mining, Fuzzy SQL, Fuzzy Databases. 
Abstract: 
At present, we have a dmFSQL server available for Oracle© Databases, programmed in PL/SQL. This 
server allows us to query a Fuzzy or Classical Database with the dmFSQL (data mining Fuzzy SQL) 
language for any data type. The dmFSQL language is an extension of the SQL language, which permits us 
to write flexible (or fuzzy) conditions in our queries to a fuzzy or traditional database. In this paper, we 
propose the use of the dmFSQL language for fuzzy queries as one of the techniques of Data Mining, which 
can be used to obtain the clustering results in real time. This enables us to evaluate the process of extraction 
of information (Data Mining) at both a practical and a theoretical level. We present a new version of the 
prototype, called DAPHNE, for clustering witch use dmFSQL. We consider that this model satisfies the 
requirements of Data Mining systems (handling of different types of data, high-level language, efficiency, 
certainty, interactivity, etc) and this new level of personal configuration makes the system very useful and 
flexible.
1 INTRODUCTION
We can define Data Mining as the process of 
extraction of interesting information from the data in 
databases. 
According 
to 
(Frawley 
1991) 
a 
discovered knowledge (pattern) is interesting when 
it is novel, potentially useful and non-trivial to 
compute. A serie of new functionalities exist in Data 
Mining, which reaffirms that it is an independent 
area (Frawley 1991): 
- High-Level Language. This representation is 
desirable for discovered knowledge and for showing 
the results of the user's requests for information (e.g. 
queries). 
- Certainty. The discovered knowledge should 
accurately reflect the content of the database. The 
imperfectness (noise and exceptional data) should be 
expressed with measures of certainty. 
- Efficiency. 
The 
process 
of 
extraction 
of 
knowledge should be efficient, i.e. the running time 
should be predictable and acceptable when dealing 
with very large amounts of data. 
- Handling of Different Types of Data. There are 
different kinds of data and databases used in diverse 
applications (relational data, objects, hypertext, etc.) 
so it would be desirable that a Data Mining system 
would carry out its work in an effective way. 
- Interactive Mining Knowledge at Multiple 
Abstraction Levels. The interactive discovery of 
knowledge allows the user to refine a Data Mining 
request on line, dynamically change data focusing, 
progressively deepen a Data Mining process, and 
flexibly view the data and Data Mining results at 
multiple abstraction levels and from different angles. 
- Mining Information from Different Sources of 
Data. Currently the knowledge mining from 
different sources of formatted or unformatted data 
with diverse data semantic is perceived to be a 
difficult challenge. 
113
C.-S. Chen et al. (Eds.), Enterprise Information Systems VII, 113–119.
© 2006 Springer.

 
In this paper we discuss the implementation of 
two prototypes for Data Mining purposes: we have 
used a combination of DAPHNE which was initially 
designed for clustering on numeric data types 
(Carrasco, 1999) and dmFSQL which was designed 
for fuzzy (or flexible) queries (Galindo 1998, 
Galindo 1998b, Galindo 1999). At this point, we 
would like to point out that Data Mining is an 
autonomous and self-interesting field of research, in 
which techniques from other fields could be applied. 
Among these techniques are the use of dmFSQL 
(data mining Fuzzy SQL), which is a database query 
language which incorporates fuzzy logic. In 
particular, we use dmFSQL to solve, in real time, 
queries, which obtain objects (tuples) with similar 
characteristics, i.e. objects of a specific group 
through a process of clustering. Often, the clustering 
is carried out on a set of examples from the database 
and not on the entire database. We present some 
experimental results with this alternative solution in 
the context of a bank. This area needs a Data Mining 
system tailored to its needs, because this area 
manages very large databases and these data has a 
very concrete meaning. Thus, data must be treated 
according to this meaning. Finally, as conclusions 
we 
consider 
that 
this 
model 
satisfies 
the 
requirements of Data Mining systems [Chen 1996, 
Frawley 2001) (handling of different types of data, 
high-level 
language, 
efficiency, 
certainty, 
interactivity, etc.) and this new level of personal 
configuration makes the system very useful and 
flexible.
2 dmFSQL A LANGUAGE FOR 
FLEXIBLE QUERIES 
The dmFSQL language (Galindo 1998, Galindo 
1998b, Galindo 1999) extends the SQL language to 
allow flexible queries. We have extended the 
SELECT command to express flexible queries and, 
due to its complex format, we only show an abstract 
with the main extensions added to this command: 
-
Linguistic Labels: If an attribute is capable of 
undergoing fuzzy treatment then linguistic 
labels can be defined on it. These labels will be 
preceded with the symbol $ to distinguish them 
easily. They represent a concrete value of the 
attribute. dmFSQL works with any kind of 
attributes (see 2.1.1 section) therefore, by 
example, a label can have associated: a 
trapezoidal possibility (Figure 1), a scalar (if 
there is a similarity relationship defined 
between each two labels in the same domain), a 
text, a XML document, etc. 
-
Fuzzy Comparators: In addition to common 
comparators (=, >, etc.), dmFSQL includes 
fuzzy comparators in Table 1. There are some 
different kinds of fuzzy comparators. By 
example a fuzzy comparator is used to compare 
two trapezoidal possibility distributions A, B 
with A=$[DA,EA,JA,GA] B=$[DB,EB,JB,GB] (see 
Figure 1). In the same way as in SQL, fuzzy 
comparators can compare one column with one 
constant or two columns of the same type. More 
information can be found in (Galindo 1998b, 
Galindo 1999). These definitions can are based 
in fuzzy set theory, classical distance functions 
and other type of similarity functions. 
Table 1: Fuzzy Comparators for dmFSQL
Fuzzy Comparator (fcomp) 
for:
Possibility 
  Necessity 
Significance
FEQ 
NFEQ 
Fuzzy EQual 
FGT
FGEQ
NFGT
NFGEQ
Fuzzy Greater Than 
Fuzzy Greater or Equal 
FLT
FLEQ
NFLT
NFLEQ
Fuzzy Less Than 
Fuzzy Less or Equal 
MGT
MLT
NMGT
NMLT
Much Greater Than 
Much Less Than 
-
Fulfilment Thresholds J: For each simple 
condition a Fulfilment threshold may be 
established with 
the format 
<condition> 
THOLD J, indicating that the condition must be 
satisfied with a minimum degree J in [0,1] 
fulfilled.
-
CDEG(<attribute>) function: This function 
shows a column with the Fulfilment degree of 
the condition of the query for a specific 
attribute, which is expressed in brackets as the 
argument.  
-
Fuzzy Constants: We can use and store all of 
the fuzzy constants (which appear in Table 2) in 
dmFSQL. 
Ramón Alberto Carrasco et al. 
114
Figure 1: Trapezoidal possibility distributions: A, B.

Table 2: Fuzzy Constants of dmFSQL 
F. Constant 
Significance 
UNKOWN 
UNDEFINED 
NULL
Unknown value but the attribute is applicable 
The attribute is not applicable or it is meaningless 
Total ignorance: We know nothing about it 
A=$[DA,EA, JA,GA]
$label
[n, m] 
#n
Fuzzy trapezoid (DAdEAd JAdGA): See Figure 1 
Linguistic Label: It may be a trapezoid or a scalar 
(defined in dmFMB) 
Interval “Between n and m” (DA=EA=n and 
JA=GA=m) 
Fuzzy value “Approximately n” (EA=JA=n and n-
DA=GA=margin) 
2.1 Architecture of dmFSQL 
In this section, we shall describe the first prototype 
to be integrated in our approach. At present, we have 
a dmFSQL Server available for Oracle© Databases, 
mainly programmed in PL/SQL. The architecture of 
the Fuzzy Relational Database with the dmFSQL 
Server is made up by: 
1. Data: Traditional Database and data mining 
Fuzzy Meta-knowledge Base (dmFMB). 
2. dmFSQL Server. 
2.1.1 Data: Traditional Database and dmFMB 
The data can be classified in two categories: 
-
Traditional Database: They are data from our 
relations with a special format to store the fuzzy 
attribute values. The fuzzy attributes are 
classified by the system in 4 types: 
- 
Fuzzy Attributes Type 1: These attributes 
are totally crisp (traditional), but they have 
some linguistic trapezoidal labels defined on 
them, which allow us to make the query 
conditions for these attributes more flexible. 
Besides, we can use all constants in Table 2 in 
the 
query 
conditions 
with 
these 
fuzzy 
attributes.
- 
Fuzzy Attributes Type 2: These attributes 
admit crisp data as well as possibility 
distributions over an ordered underlying 
domain. With these attributes, we can store and 
use all the constants we see in Table 2. 
- 
Fuzzy Attributes Type 3: These attributes 
have not an ordered underlying domain. On 
these attributes, some labels are defined and on 
these labels, a similarity relation has yet to be 
defined. With these attributes, we can only use 
the fuzzy comparator FEQ, as they have no 
relation of order. Obviously, we cannot store or 
use the constants fuzzy trapezoid, interval and 
approximate value of Table 2. 
- 
Attributes Type 4: There are different kinds 
of data in a database used in diverse 
applications (relational data, objects, hypertext, 
XML, etc.) therefore, it would be desirable that 
a Data Mining system would carry out its work 
in an effective way. In order to manage these 
data we have defined these attributes. It is a 
generic type (fuzzy or crisp), which admits 
some fuzzy treatment. We permitted this 
attribute is formed by more than a column of 
the table (complex attributes). Therefore, with 
attributes Type 4 is possible to redefine the 
attributes Type 1, 2 and 3 using other 
representations 
(by 
example, 
alternative 
representation to the fuzzy trapezoid) or fuzzy 
comparators. With these attributes, we can 
store and use the constants linguistic label in 
Table 2. 
-
Data mining Fuzzy Meta-knowledge Base
(dmFMB): It stores information about the Fuzzy 
Relational Database in a relational format. It 
stores attributes which admit fuzzy treatment 
and it will store different information for each 
one of them, depending on their type: 
- 
Fuzzy Attributes Type 1: In order to use 
crisp attributes in flexible queries we will only 
have to declare them as being a fuzzy attribute 
Type
1 and store the following data in the dmFMB: 
Trapezoidal linguistic labels: Name of the label 
and DA, EA, JA and  GA values (as in Figure 1). 
Value for the margin of the approximate values 
(see Table 1). Minimum distance in order to 
consider two values very separated (used in 
comparators MGT/NMGT and MLT/NMLT). 
- 
Fuzzy Attributes Type 2: As well, as 
declare them as being a fuzzy attribute Type 2, 
these attributes have to store the same data in 
the
dmFMB as the fuzzy attributes Type 1. 
- 
Fuzzy Attributes Type 3: They store in the 
dmFMB their linguistic labels, the similarity 
degree 
amongst 
themselves 
and 
the 
compatibility between attributes of this type, 
i.e., the attributes that use the same labels and 
that can be compared amongst them. 
-
Attributes Type 4: The dmFMB stores 
information for the fuzzy treatment of the 
attributes Type 4:  
-
Fuzzy Comparison Functions: The 
user can define the functions of comparison 
(Table 1) for the treatment of each attribute 
of Type 4.  The format is: CDEG (A fcomp 
B) -> [0,1] with CDEG the compatibility 
degrees, A, B two attributes or linguistic 
labels Type 4 and fcomp any fuzzy 
comparator in Table 1. The user can 
115
Using dmFSQL for Financial Clustering 

associate each attribute functions already 
defined in the dmFMB. 
-
Representation Functions: The user 
can optionally define it to show the 
attributes in a more comprehensible way.  
Of course, the user can associate each 
attribute functions already defined in the 
dmFMB 
 - Linguistic labels: They represent a 
concrete value of the attribute. 
-
Complex attributes: We permitted this 
attribute is formed by more than a column 
of the table. Therefore, the dmFMB stores 
information on structure of the attributes 
Type 4. 
2.1.2 dmFSQL Server 
It has been programmed mainly in PL/SQL and it 
includes three kinds of functions for attributes Type 
1, Type 2 and Type 3: 
-
Translation Function: It carries out a lexical, 
syntactic and semantic analysis of the dmFSQL 
query. If errors, of any kind whatsoever, are found, it 
will generate a table with all the found errors. If 
there are no errors, the dmFSQL query is translated 
into a standard SQL sentence. The resulting SQL 
sentence includes reference to the following kinds of 
functions.
-
Representation Functions: These functions are 
used 
to 
show 
the 
fuzzy 
attributes 
in 
a 
comprehensible way for the user and not in the 
internally used format. 
-
Fuzzy Comparison Functions: They are utilized 
to compare the fuzzy values and to calculate the 
compatibility degrees (CDEG function). 
 
As we have seen, Translation and Representation 
Functions are included in the dmFMB for the 
attributes Type 4. 
3 USING dmFSQL TO 
CLUSTERING PROCESS 
In this section, we shall describe the integration of 
dmFSQL Server to the clustering process. This is a 
part of a project, which is currently under 
investigation with some Spanish banks. It deals with 
customer database segmentation, which allows 
differentiated 
treatment 
of 
customers 
(Direct 
Marketing).
 
Included in this project we have a prototype 
called DAPHNE (Carrasco, 1999). It is a generic 
tool for clustering focused on the financial 
environment. The prototype uses techniques, which 
come from diverse fields: hierarchical clustering, 
unsupervised learning based on fuzzy-set tools, 
statistical techniques, etc. In this paper, we show a 
new version of DAPHNE witch incorporate the 
dmFSQL 
Server 
to 
do 
effective 
clustering. 
Following we explain the full process. 
 
Operation of DAPHNE: In the first step, the 
relevant features of the customers for the clustering 
are chosen using the user's knowledge. For this 
selection, the user can use a method that we have 
developed for automatic selection of relevant 
characteristics based on genetic algorithms (Martín-
Bautista 1998). Therefore, the user inserts a new 
project for clustering in the meta-database of the 
prototype specifying the table or view with the 
source data (id_table_clustering) and the attributes, 
which DAPHNE will use for the clustering 
(col_clu1, col_clu2,…, col_clum). Theses attributes 
have to define in the dmFMB as Type 1, 2, 3 or 4 
specifying their characteristics. The user does not 
need to specify anything on the domains of the 
previously used attributes. It is important to note that 
they are not restriction: on the type of attributes to 
use for the clustering process (text, scalar, binary, 
numerical, etc) and the on possible uncertainty of the 
value of this attributes (unknown, undefined, null
and certain degree of belong). Besides the user 
specify the weight of each attributes in the clustering 
process (w_clu1, w_clu2,…, w_clum such that w_clur
ෛ [0,1] with r=1..m and verifying                   ) 
 
Subsequently the main processes of DAPHNE are 
explained, as well as its underlying theoretical base: 
1. Computing Ultrametric Distance Matrix (see 
Figure 2): This process attempts to obtain the 
population's ultrametric distance matrix. Since the 
results by Dunn, Zadeh y Bezdek (Delgado 1996) 
it has been well known that there is equivalence 
between 
hierarchical 
clustering, 
max-min 
transitive 
fuzzy 
relation, 
and 
ultrametric 
distances. Therefore, in the ultrametric matrix all 
the possible clustering that can be carried out on 
the population specified. The “dendogram" or 
“tree diagram" may be viewed as a diagrammatic 
representation of the results of a hierarchical 
clustering process which is carried out in terms of 
the distance matrix. This process contains the 
following treatments: 
¦
 
 
m
r
r
clu
w
1
1
_
Ramón Alberto Carrasco et al. 
116

- Computing population's normalized (in [0,1]) 
distance matrix (by example, the matrix D in 
Figure 2). For each pair of the population's 
individuals (i, j) the distance that separates both 
(dij) is obtained using dmFSQL as following: 
SELECT A1.ROW_ID AS i, A2.ROW_ID AS j,
1-(CDEG(A1.col_clu1)* wclu1 +…+  
    CDEG(A1. col_clum)* w_clum) AS dij,
FROM id_table_clustering A1,
             id_table_clustering A2
WHERE A1.ROW_ID < A2.ROW_ID 
AND (A1.col_clu1 fuzzy_ecomp1 A2.col_clu1 THOLD 0
           | A1.col_clu1 fuzzy_ecomp1 A2.col_clu1 THOLD 0
AND
              A2.col_clu1 fuzzy_ecomp1 A1.col_clu1 THOLD 0
           | A1.col_clu1 fuzzy_ecomp1 A2.col_clu1 THOLD 0
OR
              A2.col_clu1 fuzzy_ecomp1 A1.col_clu1 THOLD 0) 
     AND … AND 
             (A1.col_clum fuzzy_ecompm A2.col_clum THOLD 0
           | A1.col_clum fuzzy_ecompm A2.col_clum THOLD 0
AND
              A2.col_clum fuzzy_ecompm A1.col_clum THOLD 0
           | A1.col_clum fuzzy_ecompm A2.col_clum THOLD 0
OR
              A2.col_clum fuzzy_ecompm A1.col_clum THOLD 0); 
where 
fuzzy_ecompr 
is 
the 
fuzzy 
equal 
comparator (FEQ or NFEQ) chosen for the user 
for the fuzzy attribute col_clur. For each attribute 
col_clur the WHERE clausule has three optional 
forms (specified by | symbol): 
a) If fuzzy_ecompr is symmetric:  
    A1.col_clur fuzzy_ecompr A2.col_clur THOLD 0
b) Using a T-norm if fuzzy_ecompr is not 
symmetric:  
A1.col_clum fuzzy_ecompr A2.col_clur THOLD 0 
 
 
AND 
 
A2.col_clum fuzzy_ecompr A1.col_clur THOLD 0
c) Using a T-conorm if fuzzy_ecompr is not 
symmetric:  
A1.col_clum fuzzy_ecompr A2.col_clur THOLD 0 
 
 
OR 
 
A2.col_clum fuzzy_ecompr A1.col_clur THOLD 0
- Computing population's ultrametric distance 
matrix (by example, the matrix D’ in Figure 2). In 
the distance matrix, each of the three elements 
verifies the triangle inequality. The matrix is 
transformed so that each of the three elements of 
the ultrametric inequality is also verified. An 
algorithm based on the method of Benzécri 
(Benzécri, 1976) is used. For this purpose, we use a 
parallel algorithm using MPI (Quinn 2003). 
2. Computing possible ˞ -cuts: Since the ultrametric 
matrix is finite, it contains only a finite set of 
different values. Thus, for the hierarchical clustering 
or ultrametric matrix we can always determine 
unequivocally the set of all possible different ˞ -
cuts, that is, the set of all different equivalence 
relations associated with the matrix. In other words, 
every ˞ -cut implies a different partition or the 
population's clustering. By example, in the Figure 2 
the possible ˞ -cuts are 0.1, 0.15, 0.2, 0.5 and 0.6. 
3. Clustering: This process assigns each individual 
in the population to a certain cluster. In order to do 
so it is necessary to obtain a certain partition from 
the ultrametric matrix. Therefore, the problem 
consists of choosing an ˞ -cut among the possible 
˞ -cuts 
already 
obtained, 
according 
to 
the 
hypothesis that no previous information about the 
117
Figure 2: Computing a ultrametric distance matrix (dendograme) for six elements. 
Using dmFSQL for Financial Clustering 

structure of the data is available. The partition can 
be obtained in different ways according to the user's 
choice:
- Absolute good partition. We obtain the partition 
determined by the ˞ -cut 0.5 (Vila 1979). By 
example, in the Figure 2 the ˞ -cut 0.5 determines 
the classes {5, 6, 3} and {4, 2, 1}. 
- A good partition. We use an unsupervised 
learning procedure based on fuzzy-set tools. This 
procedure determines a good partition as the 
minimum value of a measure denned on the set of all 
possible  ˞ -cuts (Delgado 1996). 
- Partition that determines a certain number of 
groups. By means of a binary search algorithm on all 
possible  ˞ -cuts, we obtain the ˞ -cut which 
implies a number of groups which are closest to the 
user's request. 
4 EXPERIMENTAL RESULTS 
This system has been applied to some problems of 
the segmentation of bank customers in real life 
situations. Here we show a particular case of 
segmentation. The relevant attributes identified by 
the banking expert have been:  
-
Payroll (payroll): is a binary attribute that 
indicates if the client receives payroll through 
the financial company (value 1) or not (value 0). 
We decide define this attribute as Type 4 
specifying a FEQ comparator in the dmFMB 
based in the Sokal and Michener distance. 
-
Average account balance of the client in last 12 
moths (balance): it is obtained through an 
analytic study in the company data warehouse 
system. This is a crisp attribute but we decide 
define this as Type 1 in the dmFMB using the 
fuzzy constants value #n = 500 (approximately n,
see Table 2).  
-
Geographic area of clients (area): there are 
three areas in the study: Madrid, Barcelona 
(Spanish cities) and rest of World. Obviously, 
this is a scalar attribute (Type 3), therefore we 
define a similarity relationship for the FEQ 
comparator in the dmFMB (see Table 3). 
Table 3: Similarity relationship defined for area
area
Madrid 
Barcelona 
Rest of World 
Madrid 
1 
0.6 
0 
Barcelona 
 
1 
0 
Rest of  World 
 
1
 
Now we must specify the weight of each 
attributes in the clustering process in order to better 
focus the customers clustering according to the user 
criteria. The weights chosen are 0.4 for area and 
payroll and 0.2 for balance.
 
Finally, by means of a sample of a few tuples the 
system here proposed has obtained six clusters as the 
optimum number in the population (see Table 4). 
Table 4: Results of clustering: six clusters 
id_
client
area
pay
roll
balance
id_
cluster
93036 
Rest of World 
0 
-959 
1 
60932 
Rest of World 
0 
1 
1 
65940 
Rest of World 
0 
35 
1 
07788 
Madrid 
0 
10 
4 
87992 
Madrid 
0 
241 
4 
67476 
Madrid 
1 
1 
2 
44596 
Madrid 
1 
16 
2 
14160 
Madrid 
1 
141 
2 
11281 
Madrid 
1 
353 
2 
65532 
Madrid 
1 
631 
2 
74188 
Madrid 
1 
965 
2 
18096 
Barcelona 
0 
-36 
5 
45700 
Barcelona 
0 
0 
5 
21184 
Barcelona 
0 
5 
5 
10427 
Barcelona 
0 
9 
5 
49867 
Barcelona 
1 
0 
6 
01384 
Barcelona 
1 
7 
6 
50392 
Barcelona 
1 
1580 
3 
55689 
Barcelona 
1 
1831 
3 
87752 
Barcelona 
1 
1989 
3 
23952 
Barcelona 
1 
2011 
3 
5 CONCLUSIONS
dmFSQL Server has been extended to handling of 
different types of data (Carrasco 2002) and used as a 
useful tool for certain Data Mining process 
(Carrasco 1999, Carrasco 2001, Carrasco 2002) and 
other applications (Galindo 1999). Now we have 
applied dmFSQL for the clustering problem. Besides 
the specific requirements of the clustering problem, 
the prototype has been designed considering the 
above-mentioned desirable functionalities of Data 
Mining systems: 
- Handling of Different Types of Data: The 
possibility of combination any type of data for the 
clustering process is considered novel in the 
implementations of systems of this type. 
- Mining Information from Different Sources of 
Data: DAPHNE is very flexible when managing data 
of different DBMS. 
Ramón Alberto Carrasco et al. 
118

- Efficiency and Interactive Mining Knowledge: 
The prototype has been designed to be interactive 
with the user and to give the answer in real time in 
order to obtain the wanted population's partition. 
- Accuracy: The use of the classic method of 
Benzécri to obtain the hierarchy of parts has 
guaranteed the goodness of such a partition. In 
addition, the procedure to obtain a good partition 
based on fuzzy sets has given excellent results 
during the tests. 
- Friendly Interface: The interface of DAPHNE is 
graphic and completely user guided. Like-wise, the 
prototype includes a meta-database, in such a way 
that the management of a clustering project can 
become quick and easy for the user. 
 
Regarding future works: 
-
we will show a theoretical study of the 
properties of the new similarity functions 
incorporated in this work (combining fuzzy set 
theory, classical distance functions, etc.) and 
how imply the clustering process; 
-
we will specify an extension of dmFSQL 
language that includes clustering clausules; 
-
we will integrate DAPHNE functionalities into 
dmFSQL Server. 
REFERENCES
J.P. Benzécri et coll, 1976. L'analyse des données; Tomo 
I: 
La 
Taxinomie; 
Tomo 
II: 
L'analyse 
des 
correspondences. Paris, Dunod. 
R.A. Carrasco, J. Galindo, M.A. Vila, J.M. Medina, 1999. 
Clustering and Fuzzy Classification in a Financial 
Data Mining Environment.  3rd  International ICSC 
Symposium on Soft Computing, SOCO'99, pp. 713-
720, Genova (Italy), June 1999.  
R.A. Carrasco, J. Galindo, A. Vila, 2001. Using Artificial 
Neural Network to Define Fuzzy Comparators in 
FSQL with the Criterion of some Decision-Maker. In 
Bio-inspired applications of connectionism.-2001, eds. 
J. Mira and A. Prieto, Lecture Notes in Computer 
Science (LNCS) 2085, pp. 587-594. Ed. Springer-
Verlag, 2001, ISBN: 3-540-42237-4. 
R.A. Carrasco, M.A. Vila, J. Galindo, 2002. FSQL: a 
Flexible Query Language for Data Mining. In 
Enterprise Information Systems IV, eds. M. Piattini, J. 
Filipe and J. Braz, pp. 68-74. Ed. Kluwer Academic 
Publishers,  2002, ISBN: 1-4020-1086-9. 
M. Chen, J. Han, P.S. Yu, 1996. Data Mining: An 
overview from a Data Base  Perspective. IEEE 
Transac. On Knowledge and Data Engineering, Vol 8-
6 pp. 866-883. 
M. Delgado, A.F. Gómez-Skarmeta, A. Vila, 1996. On the 
Use of Hierarchical Clustering. In Fuzzy Modelling. 
International Journal of Approximate Reasoning, 14, 
pp. 237-257. 
W.J. Frawley, G. Piatetsky-Shapiro, C.J. Matheus, 1991. 
Knowledge Discovery in Databases: An Overview. In 
G. Piatetsky-Shapiro, W.J. Frawley eds. Knowledge 
Discovery in Databases pp. 1-31, The AAAI Press. 
J. Galindo, J.M. Medina, O. Pons, J.C. Cubero, 1998. A 
Server for Fuzzy SQL Queries. In Flexible Query 
Answering 
Systems, 
eds. 
T. 
Andreasen, 
H. 
Christiansen and H.L. Larsen, Lecture Notes in 
Artificial Intelligence (LNAI) 1495, pp. 164-174. Ed. 
Springer.
J. Galindo, J.M. Medina, A. Vila, O. Pons, 1998. Fuzzy 
Comparators for Flexible Queries to Databases. 
Iberoamerican Conference on Artificial Intelligence, 
IBERAMIA'98, pp. 29-41, Lisbon (Portugal), October 
1998.
J. Galindo, J.M. Medina, J.C. Cubero, O. Pons,  1999. 
Management of an Estate Agency Allowing Fuzzy 
Data and Flexible Queries. EUSFLAT-ESTYLF Joint 
Conference, pp. 485-488, Palma de Mallorca (Spain), 
September 1999. 
M.J. Martín-Bautista, M.A. Vila, 1998. Applying Genetic 
Algorithms to the Feature Selection Problem in 
Information Retrieval. In Flexible Query Answering 
Systems, eds. T. Andreasen, H. Christiansen and H.L. 
Larsen, Lecture Notes in Artificial Intelligence 
(LNAI) 1495, pp. 272-281. Ed. Springer. 
M.J. Quinn, 2003. Parallel Programming in C with MPI 
and OpenMP. McGraw-Hill. 
F.E. Petry, 1996. Fuzzy Databases: Principles and 
Application (with chapter contribution by Patrick 
Bosc). International Series in Intelligent Technologies.
Ed. H.-J. Zimmermann. Kluwer Academic Publishers 
(KAP).
M.A. Vila, 1979. Nota sobre el cálculo de particiones 
óptimas obtenidas a partir de una clasificación con 
jerárquica. Actas de la XI Reunión Nacional de I.O., 
Sevilla, España. 
119
Using dmFSQL for Financial Clustering 

KNOWLEDGE MANAGEMENT IN NON-GOVERNMENTAL 
ORGANISATIONS
A Partnership for the Future 
José Braga de Vasconcelos, Paulo Castro Seixas and Paulo Gens Lemos 
University Fernando Pessoa, Porto, Portugal 
Email: jvasco@ufp.pt,pseixas@ufp.pt, pglemos@ufp.pt 
Chris Kimble 
Department of Computer Science, University of York, York, UK 
Email: kimble@cs.york.ac.uk 
Keywords: 
Knowledge Management, Communities of Practice, Non-Governmental Organisations, Civil Society 
Organisations, Information Society, Humanitarian Aid, Social Development. 
Abstract: 
This paper explores Knowledge Management (KM) practices for use with portal technologies in Non-
Governmental Organizations (NGOs).  The aim is to help NGOs become true Civil Society Organizations 
(CSOs).  In order to deal with more donors (at the top) and more beneficiaries (at the bottom), NGOs 
working in Humanitarian Aid and Social Development increasingly need systems to manage the creation, 
accessing and deployment information: within the NGOs themselves, between different NGOs that work 
together and, ultimately, between NGOs and Society as a whole.  Put simply, NGOs are organizations that 
need an effective KM solution to tackle the problems that arise from both their local-global nature and from 
the difficulties of effective communication between and within NGOs and Civil Society.  To address these 
problems, the underlying objectives, entities, activities, workflow and processes of the NGO will be 
considered from a KM framework.  This paper presents the needs of a responsible, cooperative and 
participative NGO from a KM perspective, in order to promote the growth of Communities of Practice in 
local as well as in global network. 
1 INTRODUCTION 
Knowledge in an organization is the collection of 
expertise, 
experience 
and 
information 
that 
individuals and workgroups use during the execution 
of their tasks.  It is produced and stored by 
individual minds, or implicitly encoded and 
documented in organizational processes, services 
and systems.  Non-Governmental Organizations 
(NGOs) dealing with Humanitarian Aid and Social 
Development face a paradox concerning the 
production of knowledge: interventions require 
particular, even specialized expertise, and, at the 
same time, must be a participative undertaking in 
which each and every individual, group and 
organization must be involved, regardless of its own 
particular skills.  This technical paradox could be 
seen as an ideological one, where NGOs are viewed 
either “as voices of global civil society and 
democratic change”, or as agencies for carrying out 
“political agendas of foreign interests” (Tvedt, 2002: 
363).  This technical/ideological paradox reveals the 
problem of the participative-democratic versus 
specialized-directive approach to development.  
Nevertheless, more and more one cannot ignore the 
important role of the development NGOs in shaping 
national and global civil societies. 
The purpose of the applied research we present here 
is to pursue for a stronger network between NGOs 
and Civil Society in order to turn them into Civil 
Society Organizations (CSOs) in which a global 
network of citizens and institutions can interact and 
intervene in a local and global – a so-called glocal – 
way.
Supported by a University-NGOs network and built 
on existing NGOs workflows, this applied research 
aims to promote the application of KM practices 
across 
individuals, 
groups, 
institutions 
and 
121
C.-S. Chen et al. (Eds.), Enterprise Information Systems VII, 121–130.
© 2006 Springer.

communities in order to bridge citizens and 
development through a proper web tool KM for 
development turning NGOs into CSOs.  For this 
purpose, a web prototype system (section 4) is being 
developed in order to enhance knowledge sharing 
and reusing mechanisms for a selected set of NGOs, 
and their related networks, based in Portugal and 
Brazil.
The following section presents the main mission and 
objectives of a NGO, the third section introduces the 
KM research area and our KM approach in NGOs 
and the fourth section analyses KM shortfalls and 
related problems within and between NGOs.  
Finally, the fifth section presents a web-based KM 
prototype for the management of NGOs activities, 
concluding this paper with some final remarks and 
future directions for this research field. 
2 HUMANITARIAN AID AND 
NON-GOVERNMENTAL 
ORGANISATIONS 
In the history of humanitarian aid and social 
development, “aid to others” and “bilateral aid” 
were the predecessors of present partnerships or 
affiliations and even of development cooperation.  
At the beginning of 21st century, we must try to 
reflect on what humanitarian and emergency aid as 
well as social development means based on the 
rejection of all and any naïveté regarding goodwill, 
altruism and solidarity of western white man.  We 
must also reject the manipulation that hinders aid to 
the "Other" and aid among one’s own from 
becoming an extreme anthropological element, in 
other words, from becoming a part of the 
foundations of human sociability (Seixas, 2003).
It has become necessary to assume a global culture 
based on assumption of the right of all to have 
rights, so that the aid for others may be replaced by a 
binding contract of multilateral cooperation based on 
the certainty that the defence of the rights of any 
person in any part of the world is the defence of the 
rights of all human beings (Seixas, 2003). 
Bearing this in mind Knowledge Management (KM) 
through information society could represent a 
partnership for the future in order to promote a 
glocal counter-hegemonic continuous intervention.  
Although local, regional and world (as well as the 
several thematic) Forums have created a strong 
reflexive and intervention tool either in counter-
hegemonic “cosmopolitism”, either towards a 
“common heritage for humanity” (Santos, 2000) this 
revolution should make his way within and between 
organizations in a local-global continuum. 
Aid and development workers of NGOs build up a 
invaluable stock of local knowledge as are in 
permanent contact with local needs and aims, 
assessing them and building on them in their day-to-
day 
interventions. 
 
However, 
this 
priceless 
knowledge is often sidetracked and misplaced in the 
complexity 
of 
the 
communication 
hierarchy, 
forgotten over the length of field missions, dispersed 
due to the high level of turn-over associated with 
many NGOs or, as Winograd and Flores (1986) put 
it, simply ‘lost in the unfathomable depths of 
obviousness’. 
The recent incentive, even obligation, for local 
partnerships in development cooperation and for 
consortiums of NGOs is, obviously, a way of 
enhancing and promoting either a local knowledge 
network or an international knowledge network to 
improve development practices.  However, local 
partnerships are often just a presence and
international consortiums are simply ways of getting 
more money without creating this desired exchange 
of knowledge.  Thus, development urgently needs a 
civic infrastructure (from village to global scale) in 
which “glocal” knowledge exchange promotes a 
continuous sustained appropriation and use of 
knowledge in a more democratic way. 
The proposal that we present here sustains that KM 
and NGOs through the Information Society and 
could constitute a very relevant civic tool that would 
give back development to citizens, grass roots 
organizations and local communities without losing 
the training and specialization which are required in 
professionalized development work.  Information 
Society should be extensible in a planetary way 
enhancing 
shared 
knowledge 
and 
practices 
concerning concrete local development projects in 
which local and international NGOs, as well as other 
organizations, are involved.  This local web turns 
global 
as 
the 
information 
flows 
through 
organizational hierarchies, bridging up the problems 
as they are accessed not only by development 
workers but also, in a more relevant way, by local 
citizens and grass roots organizations.  KM through 
the Information Society in partnership with NGOs 
may therefore enhance a web for development and 
turning therefore, NGOs into CSOs. 
3 THE KNOWLEDGE 
MANAGEMENT APPROACH: 
From NGO to CSOs 
The KM approach views knowledge as the key asset 
of an organization and systematically develops 
activities to manage it efficiently.  The main 
objectives of Knowledge Management are to 
José Braga de Vasconcelos et al. 
122

promote 
knowledge 
growth, 
knowledge 
communication and knowledge preservation (Steels 
1993).  KM is a topic of great interest in the 
management and organizational sciences and it is 
argued that KM should be appropriately supported 
by enterprise information infrastructures (Wiig 
1993, Davenport & Prusak 2000). 
3.1 Knowledge Management Systems 
In many organisations, the knowledge used to solve 
problems, to direct actions and to make decisions, 
together with any lessons learnt, are lost in the 
'noise' 
of 
a 
turbulent 
business 
environment 
may be geographically distributed and stored in a 
variety of different representations, e.g. tacit 
knowledge 
in 
people 
minds 
and 
structured 
information in databases.  To be successful a KM 
initiative must address both the 'hard' knowledge in 
databases and the 'soft' knowledge in people's minds 
(Hildreth and Kimble, 2000).  A Knowledge 
Management 
System 
(KMS) 
addresses 
these 
problems by providing a mechanism to capture, 
retain and distribute knowledge assets within and 
between organizational agents (e.g., employees and 
information systems).  KMS generally deal with 
several phases of the KM life cycle (Abecker et al 
1998): identification, acquisition, development, 
dissemination, use and preservation of knowledge.  
This KM life cycle will be applied in this research 
work regarding with the NGO’s mission and 
objectives. 
Individuals and workgroups are the prime location 
where the knowledge assets of an organization are 
located.  KMS can easily deal with, for example, 
explicit (encoded) representations of organizational 
structures, and process descriptions, however this 
research work offers a KM approach to tackle 
specific problems concerning the activities of NGOs.  
This will involve the integration of another approach 
to KM: Communities of Practice (CoPs) 
3.2 Communities of Practice 
Communities of Practice are often described as an 
approach to KM that creates the proper environment 
for groups to come together to exchange existing 
knowledge and create new knowledge.  These 
groups 
have 
similar 
goals 
and 
a 
shared 
understanding of their activities (Brown and Gray 
1998, Greer et al. 2001); this often leads to CoPs 
becoming the basis for so-called "Knowledge 
Networks" (Hildreth and Kimble, 2004). 
The term Community of Practice (CoP) was coined 
in 1991 when Jean Lave and Etienne Wenger (Lave 
and Wenger, 1991).  Lave and Wenger saw the 
acquisition of knowledge as a social process in 
which people participated in communal learning at 
different levels depending on their authority in a 
group, i.e. whether they were a newcomer to the 
group or had been an active participant for some 
time.  The linking of CoPs to KM came in 1998, 
when Wenger (1998) published the results of a study 
of a large insurance company.  According to Wenger 
(2002), CoPs are groups of people who share 
common 
problems 
and 
interests, 
and 
their 
knowledge and expertise in specific areas is shared 
by interacting on an ongoing basis.  Over time, the 
main objective is the dynamic creation of a common 
body of knowledge, practices and approaches.  
These informal networks can act in several ways, 
such as resolving conflicting goals or disseminating 
best practices across communities. 
The development of Internet-based networking 
technologies, which can provide a convenient single 
platform for groups or networks of groups to form 
within 
larger 
organizations, 
have 
led 
to 
a 
proliferation of various forms of virtual groups and 
communities.  Subsequently, there has been much 
discussion about virtual CoPs (Kimble et al, 2001).  
These 
virtual 
CoPs depend 
on 
a 
common 
communication platform, and an organization to 
support this by providing both the communications 
infrastructure and the means to easily find and join 
the CoP (Lock Lee and Neff, 2004).  This concept of 
a CoP is applied in the web based prototype system 
presented in the following section. 
3.3 Knowledge Management 
Shortfalls
The underlying objective of this research is to 
develop mechanisms to minimize KM problems that 
happen across NGOs.  Both academic and corporate 
KM literature has identified a set of KM deficits that 
happen at the organizational and corporate level; this 
literature can also be adapted and applied to NGOs. 
Macintosh’s (1997) work on knowledge asset 
management identified a set of organizational 
impediments to more productivity and performance 
in knowledge-based companies were: 
“Highly paid workers spend much of their time 
looking for needed information”. 
“Essential know-how is available only in the heads 
of few employees”. 
“Valuable information is buried in piles of 
documents and data”. 
“Costly errors are repeated due to disregard of 
previous experiences”. 
123
Knowledge Management in Non-governmental Organisations 
(Vasconcelos et al., 2003). In addition, knowledge 

“Delays and suboptimal product quality result from 
insufficient flow of information”. 
Based in these statements, Dieng et al. (1998) 
elicited possible motivations to build a KMS based 
on Organizational Memories. 
x To avoid the loss of corporate expertise when a 
specialist leave the company; 
x To explore and reuse the experience acquired in 
past projects in order to avoid the repetition of 
previous mistakes; 
x To improve the information circulation and 
communication across the company; 
x To integrate the know-how from different 
sources in the company; 
x To ultimately to improve the process of 
individual and organizational learning. 
In what concerns NGOs, big business NGOs (so-
called “BINGOs”) already dealt with the “KM for 
development” problem and created their own 
internal devices to respond to those problems.  
Nevertheless, great number of small and medium 
size NGOs deal with constant constraints that are 
presented in detail below.  Our KM tool addresses, 
in particular, these issues. 
3.4 KM Constraints in NGOs 
Contextual Global and National information 
NGOs, either international or national, frequently 
become involved in missions without a deep 
background knowledge of the global/national/local 
problem and without skilled personal or proper 
accessible methodologies. 
Some specific problems are absence of just in time 
access to: 
a)
Specific information concerning the countries 
where NGOs will (or already have) a mission; 
b)
Skilled specialized personal in a particular 
country or linguistic grouping; 
c)
Manuals, checklists and other sources of 
managing knowledge needed to enhance the 
efficacy of project elaboration, formulation and 
implementation. 
Field-mission information 
NGOs projects and missions are often “go and run 
away” missions, that depend too much on the 
personalities that are send to the field, with scarce 
time too much focused on bureaucratic/operational 
work and with difficult beginnings and final periods 
to each of the mission field-workers.  Great lost of 
knowledge and capital is the usual balance. 
Some specific problems are: 
a)
Personalization of the information resulting in 
non accounted overheads moreover when field-
missions are 6 to 10 months length; 
b)
Loss 
of 
non 
bureaucratic/operational 
information (social and cultural) which isn’t 
refereed in standard reports; 
c)
Loss of information in mission team turnover. 
Communication between the field mission and 
the main office 
NGOs have a multi-level Knowledge Management 
which goes, in the field, from the Project manager to 
the Program manager / Country manager and in the 
main office, from the Project Director and Finances 
Director to the Direction Board.  Through this 
several local, national and international steps, too 
much information is lost. 
Some specific problems are that there might be 
a)
A strict hierarchical top to bottom decision-
making, 
which 
works 
against 
a 
more 
participative approach to decision making built 
through local knowledge; 
b)
The autonomy of field missions, which could 
lead, sometimes, to an information crisis in the 
management of the project or even an crisis in 
the NGO it self; 
c)
An absence of a communication platform 
accessible by levels of responsibilities for a just 
in time proper information flow and information 
register.
Communication between NGOs 
To often several NGOs, work in the same country, 
or even in the same region, without knowing about 
each other organizational purposes, projects and 
activities.
Some specific problems are an absence of access to: 
a)
A map of previous NGO interventions, if 
possible through purpose and projects; 
b)
Simple and direct access to communication 
forums or to create one which could invite and 
congregate 
NGOs 
professionals 
through 
purpose, project or activities a Community of 
Practice in the and in each field; 
c)
Best practices databases of projects selected by 
NGOs Direction Boards in order to have an 
identification card of each NGO as well as to 
publicise a problem-solving database for 
development. 
José Braga de Vasconcelos et al. 
124

Communication between NGOs and the 
beneficiaries
NGOs relations with beneficiaries of the projects are 
not so easy and well done as they should be and 
frequently a project is elaborated and formulated 
without sufficient participative enrolment by the 
community of beneficiaries.  In addition, in the 
implementation phase, authorities are often more 
aware of the project purposes and activities than the 
ones the project will direct supposedly benefit.  
Interface between NGOs and beneficiaries should be 
done probably moreover in a face-to-face basis, 
nevertheless, Knowledge Management through web 
interface tools could be an asset because besides and 
beyond 
information 
concerning 
the 
project, 
beneficiaries should have a continuous possibility of 
taking positions in relation to it. 
Some specific problems: 
a)
Inadequate, un-participative analysis of the 
needs felt by a particular population in which a 
NGO pinpoint an intervention; 
b)
Inadequate, un-participative representation of 
the beneficiary community in the formulation 
and implementation of the project 
c)
Required of a development concept as a citizen 
tool (an interactive day-to-day activity) through 
which everyone could have a word and 
participate in action. 
Communication between NGOs and the donors 
Although institutional donors usually have their own 
ways of controlling the uses of applied funds and the 
impacts of the projects, the non-organized social 
responsibility response of civil society through 
NGOs is much less informed in what concerns either 
the concrete development projects supported, or 
their real impact in the field.  NGOs are usually 
much more concerned with reports to institutions 
like World Bank or EU Offices than with Civil 
Society donors. 
Some specific problems: 
a)
Absence of a communication toll through which 
donors have direct link with the project in the 
field and with its impacts; 
b)
High 
levels 
of 
turn 
over 
in 
NGOs, 
personalization of the information in the field, 
as well as problems with storage of information 
often lead to difficulties either in internal 
supervision, either in external auditing by 
sponsors and donors; 
c)
Information coming from the field mission 
concerning the project is, to many times, sent 
only the country, mission or project manager 
without any kind of feed-back by the population 
who were impacted by the project. 
Communication between NGOs and Civil Society 
Each individual and each organization should, must 
and could be an active development actor and agent 
if only could have the proper knowledge capital 
(understood as social, symbolic and, therefore, 
economic capital) to make a difference.  This utopia 
could be built through a good communication 
network between Development NGOs and Civil 
Society in order to turn themselves into CSOs, Civil 
Society Organizations.  Therefore, we believe, KM 
through information society could be the tool to turn 
NGOs into CSOs, being this purpose the engine of 
the present project. 
Some specific problems: 
a)
Absence of a KMS to enrol individuals and 
organizations in order to a participative effort 
toward development, both in a local and in a 
global sense.  Such a systems should deploy of 
information concerning 1) basic continuously 
renewed 
geo-strategic 
and 
anthropological 
situation of countries in risk; 2) basic training 
towards citizen and organizational intervention; 
3) intervention protocols and counselling; 4) 
cultural online counselling and 5) psychological 
support.
b)
Absence of a development best practices 
database that could be used as an intervention 
guide not only to NGO experts but also to any 
citizen enrolled in a local development process 
or with global development concerns.  This 
guide could enhance not only the analysis, 
discussion 
and 
choosing 
of 
alternative 
development paths but also could be a way of a 
more adequate impact analysis of development 
projects by comparing the achieved goals in 
similar projects. 
c)
Absence of a Community of Practice of 
Knowledge Network that could cut across 
NGOs competence and competition but also that 
could bridge NGOs, donors and beneficiaries in 
a more coherent and focused way. 
4 GLOCAL COLLABORATION 
TOOL 
A practical result of this research work is a web-
based KM tool (or KM portal) that is currently under 
development.  This web tool is being developed to 
act as a KM portal for individuals, organizations and 
more specifically, for NGOs.  This Glocal 
Collaboration 
Tool 
aims 
to 
promote 
the 
125
Knowledge Management in Non-governmental Organisations 

democratization across different and heterogeneous 
communities. 
Bearing in mind the several NGO constraints 
presented 
and 
the 
difference 
between 
“soft 
knowledge” or “tacit knowledge” and “hard 
knowledge” or “explicit knowledge” (Hildreth and 
Kimble, 2001), the first one more centred in the 
knower and interactivity through “conversation”; the 
second one more centred in the storage of 
knowledge. 
 
Table 
1 
presents 
the 
portal 
functionalities as problem-solving devices to the 
constraints referred in the previous point. 
“Conversations” in CoPs are the most important 
device because of the multiple contexts (personal, 
social and cultural) that its interactivity includes.  
The goal of the research and development of our tool 
is to create a KM system based on “conversation 
analysis” which could help different users and 
preserve the context in which the knowledge is 
created.  CoPs can be seen as a glocal simulacrum of 
such places as corridors or water coolers where 
knowledge is exchanged in informal conversations.  
In many organisations, these conversations have 
been replaced by mobile phone calls in which the 
decisions of formal meetings are planned for and 
prepared.  The goal is to try to capture some 
information about both these calls and their context. 
4.1 Application Functionalities and 
Knowledge Resources 
The web Portal application (figure 1) has a specific 
registration area (for individuals and organizations) 
in order to access to other areas, both general and 
with specific interactive tools, such as a best-
practices database (DB), collaborative tools (forums) 
and knowledge (and glocal) libraries.  Other 
functionalities include personalisation mechanisms 
based 
on 
the 
registration 
profile 
of 
each 
individual/organization, and keyword-based and 
advanced search mechanisms. 
Table 1: Problem-solution analysis 
NGO constrains 
Portal functionalities: 
Tacit knowledge 
Portal functionalities: 
Explicit knowledge 
Contextual global and national information 
1. Intranet CoPs by project and 
thematic 
2. Extranet CoPs by project and 
thematic 
1. Lay-out plan with NGOs 
missions
2.Library 
3. Best Practices 
Field mission information 
1. Intranet CoPs by project and 
thematic 
2. Extranet CoPs by project and 
thematic 
1.Library 
Communication between field mission and main 
office
1. Intranet CoPs by project and 
thematic 
2. Extranet CoPs by project and 
thematic 
1. Lay-out plan with NGOs 
missions, 
2. Library 
3. Projects showcase 
Communication 
between 
NGOs 
and 
the 
beneficiaries 
1. Extranet CoPs by project and 
thematic 
1. Lay-out plan with NGOs 
missions, 
2. Library 
3. Projects showcase 
4. Best Practices 
Communication between NGOs and the donors 
1. Extranet CoPs by project and 
thematic 
1. Lay-out plan with NGOs 
missions, 
2. Library 
3. Projects showcase 
4. Best Practices 
Communication between NGOs and Civil 
Society 
1. Extranet CoPs by project and 
thematic 
1. Lay-out plan with NGOs 
missions, 
2. Library 
3. Projects showcase 
4. Best Practices 
José Braga de Vasconcelos et al. 
126

4.2 Best-Practices Database 
Based on the following attributes (table 1) and 
related registration area (figure 3), we are defining a 
best-practices database in order to reuse this 
information for future Civil Society Organizations 
(such as a NGO) projects.  The idea is the definition 
of a common, shared web space where CSOs, 
individuals and citizens representing different 
communities could upload their experiences, and 
related project’s best practices in a structured 
manner.  The best practice DB includes specific 
classification techniques based on the proposed 
taxonomy (figure 2) for development areas. 
Using 
the 
proposed 
collaboration 
tool, 
this 
taxonomy will have dynamic expansion based on the 
experiences of each CSO, and the inputs (and related 
uploads), which they include in the web platform.  
Trough this expansion and depending on particular 
requests (local knowledge), beyond the Internet 
platform, we will build specific Intranets to provide 
context-based data access for a better decision 
making in each CSO and related responsibility level. 
These attributes are used to create a database set of 
CSOs including their project experiences and best 
practices.  To create the initial profile, an individual 
or organization will register (figure 3) to create a 
personal (or organization) account that makes 
available the remaining functionalities. 
Results
 General guides to run effectively CSOs
- Protocols, Manuals, Standards 
- FAQs, Project rules, Proposals
 Best practices by world regions
- Continents
- Countries
- National regions
 Best practices by CSO
Best Practices by thematic area
 Conflict management and negotiation
 Healthcare
- Emergency
- Rehabilitation and development
- Public health and sanitation
- Nutrition
 Economics
- Emergency 
- Rehabilitation and development
 Social and Cultural
- Emergency
- Rehabilitation and development
Classification 
mechanisms
127
Figure 1: Web Collaboration Tool.
Figure 2: Best practices taxonomy.
Knowledge Management in Non-governmental Organisations 

Table 2: Data attributes for the best practices registration
Attribute 
Meaning 
Internal code (year/project) 
Application use for future data manipulation 
Project title 
Short project designation 
Start date 
Project start date 
Closing date 
Estimated closing date 
Activity area 
Short description of the underlying activities 
Applicant(s) 
The registered NGO, individual or organization 
Project cost  
An estimation of the project cost (if applicable) 
Objectives 
Overall objective and specific objectives 
Principal local partners 
The main project stakeholders 
Target groups 
Target communities, citizens, individuals  
Expected results 
Quantitative results; Qualitative results 
Activities 
Project timeline (different project stages) 
4.3 Application Architecture 
This prototype is based on a multi-tier architecture 
that allows a web-enabled personal computer (client) 
to interact with an application server connected to 
the Internet (figure 4).  This web infrastructure is 
crucial considering the purpose of this solution 
which means that clients are supposed to be any 
computers connected to the Internet anywhere near 
the area of intervention of the ONG. 
A multi-tier architecture environment provides data 
for clients and serves as an interface between clients 
and database servers (figure 5).  This architecture 
enables the use of an application server to validate 
the credentials of a client, such as a Web browser, to 
connect to a database server, and to perform the web 
requested operations. 
Application Server 
The application server provides access to the data 
for the client.  It serves as an interface between the 
client and one or more database servers, which 
provides an additional level of security.  The 
application server assumes the identity of the client 
when it is performing operations on the database 
server for that client.  The Application Server for the 
proposed Web-KM system is based on the Microsoft
(MS) .NET Framework with ASP.NET.  The .NET 
Framework is used for building and running all 
José Braga de Vasconcelos et al. 
128
Figure 3: Collaboration tool: registration area.
Figure 4: Web-enabled architecture.

kinds 
of 
software, 
including 
Web-based 
applications. 
 
This 
software 
platform 
uses 
components that facilitate integration by sharing 
data and functionality over a network through 
standard, 
platform-independent 
protocols.  
ASP.NET 
presents 
a 
fully 
object-oriented 
architecture that promotes the development of well 
structured and easy to maintain. 
Web Server 
Our system uses the MS Internet Information Server
(IIS) as the Web Server for the Glocal Collaboration 
Tool.  This Internet Information Service provides 
integrated, reliable, scalable, secure and manageable 
Web server capabilities over an intranet, the Internet 
or an extranet.  IIS is a tool for creating a strong 
communications platform of dynamic network 
applications.  Organizations of all sizes use IIS to 
host and manage Web pages on the Internet or on 
their intranets. 
Database Server 
A database server (engine) provides the data 
requested by an application server on behalf of a 
client.  The database server does all of the remaining 
query processing.  The database server can audit 
operations performed by the application server on 
behalf of individual clients as well as operations 
performed by the application server on its own 
behalf.  For example, a client operation can be a 
request for information to be displayed on the client, 
whereas an application server operation can be a 
request for a connection to the database server.  Our 
system uses the Microsoft SQL Server 2000 for the 
necessary database management system services.  
This database server is commonly used by 
governments and businesses for small to medium 
sized databases. 
Client
In a social viewpoint, the client could be an 
Individual or a CSO.  In a technological viewpoint, a 
client initiates a request for an operation to be 
performed on the database server.  The client can be 
a Web browser or other end-user process.  In a 
multi-tier architecture, the client connects to the 
database server through one or more application 
servers.  The client tier consists of a typical web-
enabled personal computer browser.  Applications 
are accessible by users running a browser on any 
operating system.  The Glocal Collaboration Tool is 
being developed to be used as a KM web-based 
application that could run anywhere in order to make 
it available to any CSO/Individual with a basic fixed 
or mobile Internet access.  Therefore, this research 
project and the proposed KM tool also intend to 
promote the democratization of the Information 
Society.  This could be achieved by providing and 
disseminating context-based CSOs information 
resources and collaborative services across different 
world regions and communities. 
5 CONCLUSIONS AND FUTURE 
WORK
At medium-term, the practical result of the proposed 
KM portal should enhance knowledge sharing and 
reuse tasks between and within CSOs and 
individuals.  The idea is to apply and promote 
collaborative tasks, knowledge (glocal) libraries and 
the dissemination of CSOs best practices across 
different communities.  Personalisation is a crucial 
factor to improve KM practices.  In this context, this 
research work and the related Glocal Collaboration 
Tool needs to investigate more personalisation 
mechanisms to incorporate into the KM tool.  These 
mechanisms 
should 
provide 
and 
distribute 
contextual information based on the specific profile 
(roles and responsibilities) of each registered 
organization.  The collection and classification of 
heterogeneous information resources into well-
structured CSO web pages (acting as individual CSO 
Intranets) is also a key success factor for an effective 
maintenance of this KM system. 
In order to enhance all these developments, the 
Glocal Collaboration Tool has to be tested in real 
organizational and communities’ environments.  
Access expectancy towards resources of this kind of 
Database engine
Application server 
Web server
- Business rules
- User interface
Client
CSO/Individual
Web browser
Database engine
Application server 
Web server
- Business rules
- User interface
Client
CSO/Individual
Web browser
129
Figure 5: Multi-tier architecture and services. 
Knowledge Management in Non-governmental Organisations 

tool will be our first challenge.  Nevertheless, we 
believe strong partnerships with boards of NGOs 
must be build in order to have a commitment of 
change in the NGO internal information and 
communication structure.  Relations between 
organizational intranets and extranets will also take 
some time and be the source of some problems.  The 
effectiveness of Communities of Practices, from a 
local project level to a global development area 
strategy level, however is the deepest challenge, as 
this calls for an organizational transformation of 
NGOs into more Glocal CSOs.  Pilot project, 
focused in the beginning on the lusophone world, 
will entail Portuguese and Brazilian NGOs and 
within a year we are hopping to have several NGOs 
involved turning this Collaboration Tool in a non-
stop glocal project in the construction of a new 
Glocal Civil Society. 
REFERENCES
Abecker, A., Bernardi, A., Hinkelmann, K., Kuhn, O. and 
Sintek, M., 1998. Towards a Technology for 
Organizational Memories, IEEE Intelligent Systems, 
Vol. 13, No. 3, May/June, pp. 30-34. 
Brown S. and Gray S., 1998. The People are the Company 
Fast 
Company 
[Online] 
Available:
http://www.fastcompany.com/online/01/people.html 
Davenport, T. and Prusak, L., 2000. Working Knowledge, 
Harvard Business School Press. 
Greer K., Bell D., Wang H., Bi,Y. and Guo G., 2001. An 
Application 
of 
Knowledge 
Management 
using 
Intelligent 
Agents, 
the 
Intelligent 
CONtent 
Management.
Hildreth, P. and Kimble, C., 2002. The duality of 
knowledge [Online] Information Research, 8(1), paper 
no. 142.  Available at http://InformationR.net/ir/8-
1/paper142.html [November 19, 2004]. 
Hildreth, P. and Kimble, C. 2004., Knowledge Networks: 
Innovation through Communities of Practice, Idea 
Group Publishing. Hershey (USA)/London (UK). 
Kimble, 
C. 
Hildreth, 
P 
and 
Wright. 
P., 
2001. 
"Communities of Practice: Going Virtual", Chapter 13 
in Knowledge Management and Business Model 
Innovation, 
Idea 
Group 
Publishing. 
Hershey 
Lave J. and Wenger E., 1991. Situated learning. 
Legitimate 
Peripheral 
Participation. 
Cambridge: 
Cambridge University Press. 
Lock Lee, L and Neff, M. 2004. How Information 
Technologies Can Help Build and Sustain an 
Organization's Communities of Practice: Spanning 
The Socio-Technical Divide?" Chapter 15 in Hildreth, 
P. and Kimble, C. (2004) "Knowledge Networks: 
Innovation through Communities of Practice", Idea 
Group Publishing, Hershey (USA)/London (UK), pp 
165 – 183. 
Macintosh, A., 1997. Knowledge asset management, 
AIring (20). 
Pearce, J., 2000. Development, NGOs, and Civil Society, 
Oxford, Oxfam GB. 
Santos, B., 2000. Globalização. Fatalidade ou Utopia?, 
Porto. Afrontamento. 
Seixas, P. 2003. Emergency and Humanitarian Aid. 
Revista ForumDC 
http://www.forumdc.net/forumdc/artigo.asp?cod_artig
o=145335
Seixas, P. forthcoming 2005 Antropologia e Intervenção 
Humanitária 
e 
para 
o 
Desenvolvimento. 
A 
Humanidade 
que 
o 
Humanitário 
construiu: 
Conceptualização e Acção, Perez, Xerardo Pereiro e al 
Antropologia Aplicada. Vila Real, UTAD. 
System 
(ICONS), 
Project 
ID: 
IST2001 
- 
II.1.2: 
Knowledge Management. 
Tvedt, T., 2002. Development NGOs: Actors in a Global 
Civil Society or in a New International Social 
System?”, 
Voluntas: 
International 
Journal 
of 
Voluntary and Nonprofit Organizations Vol. 13, No. 4. 
Vasconcelos, J, Kimble C and Rocha, Á., 2003.  
Organizational Memory Information Systems: An 
Example of a Group Memory System for the 
Management of Group Competencies, The Journal of 
Universal Computer Science, 9(12), pp. 1410 – 1427. 
Wenger E, McDermott R and Snyder W., 2002. 
Cultivating Communities of Practice: A Guide to 
Managing Knowledge, Harvard Business School 
Press, Boston, Massachusetts. 
Wenger E., 1998. Communities of Practice. Learning, 
Meaning and Identity. CUP. 
Winograd T. and Flores F., 1986 Understanding 
computers and cognition: a new foundation for design 
Norwood NJ: Ablex. 
José Braga de Vasconcelos et al. 
130
(USA)/London (UK), 2001. pp 220-234. 

TOWARDS A CHANGE-BASED CHANCE DISCOVERY 
Zhiwen Wu and Ahmed Y. Tawfik 
School of Computer Science, University of Windsor,401 Sunset Ave.,Windsor,Ontario N9B 3P4, Canada 
Email: wu1n@uwindsor.ca , atawfik@uwindsor.ca 
Keywords: 
Chance discovery, knowledge base, relevance, planning, ontology. 
Abstract: 
This paper argues that chances (risks or opportunities) can be discovered from our daily observations and 
background knowledge. A person can easily identify chances in a news article. In doing so, the person 
combines the new information in the article with some background knowledge. Hence, we develop a 
deductive system to discover relative chances of particular chance seekers. This paper proposes a chance 
discovery system that uses a general purpose knowledge base and specialised reasoning algorithms.  
1 INTRODUCTION
According to Ohsawa and McBurney (2003), a 
chance is a piece of information about an event or a 
situation with significant impact on decision-making 
of humans, agents, and robots. A ‘chance’ is also a 
suitable time or occasion to do something. A chance 
may be either positive –an opportunity or negative –
a risk. For example, predicting a looming earthquake 
represents a “chance discovery”. 
Many approaches have been applied to chance 
discovery.  Rare events may represent chances 
known to co-occur with important events, while the 
important events can be extracted using data mining 
techniques. KeyGraph, the application of this 
technique, was applied to various data, such as 
earthquake sequences, web pages, documents 
(Ohsawa et al., 1998; Ohsawa and Yachida, 1999; 
Ohsawa, 2003a; Ohsawa, 2003b). Tawfik (2004) 
proposes that chance discovery represents a dilemma 
for inductive reasoning. Induction assumes that 
current trends will carry into the future thus favoring 
temporal uniformity over change. However, current 
observations may lead to different possible futures in 
a branching time model. Finding a proper 
knowledge representation to represent all these 
possible futures is important. Otherwise some 
chances will be missed. Bayesian and game theoretic 
approaches are presented as viable chance discovery 
techniques. Abe  (2003a, 2003b) considers chances 
as unknown hypotheses. Therefore, a combination of 
abductive and analogical reasoning can be applied to 
generate such knowledge and chances can be 
discovered 
as 
an 
extension 
of 
hypothetical 
reasoning. McBurney and Parson (2003) present an 
argumentation-based 
framework 
for 
chance 
discovery in domains that have multi agents. Each 
agent has a partial view of the problem and may 
have insufficient knowledge to prove particular 
hypotheses individually. By defining locutions and 
rules for dialogues, new information and chances 
can be discovered in the course of a conversation. 
In this paper, we incorporate some new elements 
into the chance discovery process. These elements 
have implications to both the conception and 
discovery of chances and can be summarized as 
follows:
x
Chances 
are 
not 
necessarily 
unknown 
hypotheses. Many chances result from known 
events and rules. For example, applying for the 
right job at the right time represents a chance 
for an employment seeker as well as the 
employer. In this case, the goal is clear. 
However, chance discovery means that the 
employment seeker applies at the proper time 
and for the employer, it means to correctly 
project which applicant will be better for the 
job.  
x
Inherently, chance discovery has a temporal 
reasoning 
component. 
New 
risks 
and 
opportunities are typically associated with 
change. An invention, a new legislation, or a 
change in weather patterns may result in many 
chances. Incorporating chance discovery in a 
belief update process is fundamental to this 
work.   Chances are relative; someone’s trash 
may be another’s treasure. For example, finding 
a cure for a fatal disease represents more of a 
chance to an individual suffering from this 
condition or at risk to contact it.  
131
C.-S. Chen et al. (Eds.), Enterprise Information Systems VII, 131–138.
© 2006 Springer.

x
To discover chances and take advantage of 
them, a system which can perform deductive 
reasoning is needed.  
Therefore, we consider chance discovery as a 
process that tries to identify possibly important 
consequences of change with respect to a particular 
person or organization at a particular time. For this 
to happen, a logical reasoning system that 
continuously updates its knowledge base, including 
its private model of chance seekers (CS) is 
necessary. A chance discovery process may act as an 
advisor who asks relevant “what if” question in 
response to a change and present significant 
consequences much like seasoned parents advise 
their children. Such advice incorporates knowledge 
about the chance seekers, their capabilities, and 
preferences along with knowledge about the world 
and how it changes.   
In a word, to discover chances, we need three 
things: First, a knowledgeable KB which can infer 
and understand commonsense knowledge and that 
can incorporate a model of the chance seeker. 
Second, we need a source for information about 
change in the world. Third, we need a temporal 
projection system that would combine information 
about change with the background knowledge and 
that would assess the magnitude of the change with 
respect to the knowledge seeker.  Cyc knowledge 
base is supposed to become the world's largest and 
most complete general knowledge base and 
commonsense reasoning engine and therefore 
represents a good candidate as a source for 
background knowledge. Information about changes 
occurring in the world is usually documented in 
natural languages. For example, a newspaper can 
serve as a source for information about change. We 
need Nature Language Processing (NLP) tool to 
understand this newspaper. We assume that Cyc 
natural language module will be able to generate a 
working logic representation of new information in 
the newspaper. However, for the purpose of the 
present work, understanding news and converting it 
to Cyc representation has been done manually. This 
paper proposes an approach for assessing the 
implications of change to the chance seeker and 
bringing to the attention of the chance seeker 
significant risks or opportunities. 
The paper is organized as follows: Section 2 
establishes the notion that chance and change are 
tied together. Section 3 introduces Cyc knowledge 
base and its technology. Section 4 presents the 
chance discovery system based on Cyc. 
2 CHANCES IN CHANGES 
Chances and changes exist everywhere in our daily 
life. In general, changes are partially observable by a 
small subset of agents. Therefore, it is more likely to 
learn about changes happening in the world through 
others. For example, information about change could 
be deduced from conversations in chat rooms, 
newspapers, e-mail, news on the WWW, TV 
programs, new books and magazines, etc. In another 
word, change causing events occur daily around the 
world. The amount and rate of those events is very 
large. However, a relatively small portion of these 
changes represent risks or opportunities to any 
particular chance seeker.  
Initially, the system starts with a stable 
knowledge base KB. The knowledge base represents 
the set of widely held knowledge. As part of KB’s 
knowledge, each chance seeker maintains its own 
private knowledge that describes its current 
attributes. In addition to KB, each chance seeker 
also maintains its private goals and plans about how 
to achieve those goals. If chance seeker doesn’t 
maintain its goals, the system will use default goals 
that are widely accepted as common goals. For 
example, the system assumes that all people want to 
become more famous or richer, want their family 
members and relatives to be rich and healthy, etc. 
We assume that the chance seeker has already 
exploited the chances present in the current KB and 
that the current plans of chance seeker are the best 
according to current KB. However, current plans 
may only be able to achieve part of the goals. For 
example, the goal to own a house in Mars is 
unachieved by current knowledge.  
A goal of chance seeker can be represented by a 
set of sentences describing a future status of chance 
seeker’s attributes. For example, if chance seeker set 
up the goal to be a famous scientist, the system can 
judge the achievement of the goal by measuring 
chance seeker’s current attributes, such as education, 
occupation, published papers, social class, etc. The 
system maintains an attribute framework of chance 
seeker in KB. The attribute framework can be able 
to change if necessary. A goal can be considered as a 
future projection of current framework. On the other 
hand, a future set of attributes could satisfy many 
goals of chance seeker. Current plans of chance 
seeker project current set of attributes to the most 
achievable set of attributes.  
As new information B becomes available, an 
update operation is triggered. The update operation 
proceeds in two phases: a explanation phase and a 
projection phase. The explanation phase tries to 
revise current plans that may have been proven to be 
inaccurate by the occurrence of B. Similarly, the 
Zhiwen Wu and Ahmed Y. Tawfik 
132

followings: the occurrence of B enables another one 
of the goals of the chance seeker to become 
achievable, or better plans can come up after B.  In 
some cases, a particular piece of new information 
will result in both risks and opportunities.  
3 CYC KNOWLEDGE BASE FOR 
CHANCE DISCOVERY 
The Cyc knowledge base (KB) (OpenCyc.org, 2002) 
is a formal system that represents of a vast quantity 
of fundamental human knowledge: facts, rules of 
thumb, and heuristics for reasoning about objects 
and events of everyday life. The medium of 
representation is the formal language known as 
CycL. CycL is essentially an augmentation of first-
order predicate calculus (FOPC), with extensions to 
handle equality, default reasoning, skolemization, 
and some second-order features. For example: 
(#$forAll ?PERSON1 
(#$implies
(#$isa ?PERSON1 #$Person)
(#$thereExists ?PERSON2 
(#$and
(#$isa ?PERSON2 #$Person) 
(#$loves ?PERSON1 ?PERSON2))), 
in English, means  
“Everybody loves somebody.” 
In Cyc, a collection means a group or class. 
Collections have instances. Each instance represents 
an individual. For examples, 
(#$isa #$AbrahamLincoln, #$Person). 
(#$isa #$BillGates, #$Person). 
Abraham Lincoln and Bill Gates are individuals. 
Person is a collection. A collection could be an 
instance of another collection. For example, 
(#$genls #$Dog, #$Mammal),
means “Collection Dog is an instance collection 
of collection Mammal”.  
instance of Thing, which is the most general 
collection in Cyc KB.  Some individuals could be 
part of other individuals. For example, Microsoft is 
an individual. Joe works for Microsoft. Joe is part of 
Microsoft.  
Constants are the "vocabulary words" of the Cyc 
KB, standing for something or concept in the world 
that many people could know about. For example, 
#$isa, #$Person and #$BillGates are constants. 
The assertion is the fundamental unit of 
knowledge in the Cyc KB. Every assertion consists 
of:  
x
an expression in CycL language that makes 
some declarative statement about the world  
x
a truth value which indicates the assertion’s 
degree of truth. There are five possible truth 
values, including monotonically true, default 
true, unknown, default false and monotonically 
false.
x
A microtheory of which the assertion is part of a 
theory. Section 3.1 gives a detailed explanation 
of microtheories. 
x
A 
direction 
which 
determines 
whether 
inferences involving the assertion are done at 
assert time or at ask time. There are three 
possible 
values 
for 
direction: 
forward 
(inferences done at assert time), backward 
(inferences done at ask time), and code 
(assertion not used in regular inference). 
x
A justification which is the argument or set of 
arguments supporting the assertion's having a 
particular truth value.  
An assertion could be a rule or a Ground Atomic 
Formula (GAF). A rule is any CycL formula which 
begins with #$implies. A GAF is a CycL formula of 
the form, (predicate arg1 [arg2 ...argn]),  where the 
arguments are not variables.  
In Cyc, time is part of the upper ontology. It is a 
physical quantity. A temporal object such as an 
event, a process, or any physical object has a 
temporal extent. The time model is interval-based 
with suport for points. TimeInterval has dates, years, 
and so on, as its subcategories. An event is a set of 
assertions that describe a dynamic situation in which 
the state of the world changes. An event has non-
empty space and time components. It may also have 
performer, beneficiaries, or victims. A script in 
CycL is a type of complex event with temporally-
ordered sub-events. Applications can use script 
recognition – that allows them to identify a larger 
script from some stated events that are constituent 
parts of the script. Scripts can also be used for 
planning and for reading comprehension. 
Towards a Change-Based Chance Discovery 
133
projection phase, revises current plans to take into 
account the occurrence of B. A risk is detected if the 
occurrence of B results in a threat to the causal 
support for one of the plans of the chance seeker. An 
opportunity is detected if B satisfies one of the 
In other word, Dog is a specialization of 
Mammal. It can be said that every individual is an 

3.1 Microtheories 
A microtheory (Mt) is a bundle of assertions. The 
bundle of assertions may be grouped based on 
shared assumptions, common topic (geography, 
football, etc), or source (CIA world fact book 1997, 
USA today, etc). The assertions within a Mt must be 
mutually consistent. Assertions in different Mts may 
be inconsistent. For example, 
MT1: Mandela is President of South 
Africa
MT2: Mandela is a political prisoner 
Microtheories are a good way to cope with 
global inconsistence in the KB, providing a natural 
way to represent things like different points of 
views, or the change of scientific theories over time. 
Mts are one way of indexing all the assertions in 
Cyc KB.
There are two special Mts, one is #$BaseKB 
(always visible to all other Mts), the other one is 
#$EverythingPSC (all other Mts are visible to this 
Mt). #$EverythingPSC is a microtheory which has 
no logically consistent meaning but has a practical 
utility just because it is able to see the assertions in 
every microtheory. 
The Cyc KB is the repository of Cyc's 
knowledge. It consists of constants and assertions 
involving those constants. It could be regarded as a 
sea of assertions, see figure 1.  Form ontology point 
of view, the Cyc KB could also be thought of as 
#$ChemistryMt
#$OrganizationMt 
#$BiologyMt 
Figure 1: Cyc Knowledg
Zhiwen Wu and Ahmed Y. Tawfik 
134
e Base as a sea of Assertions.
Figure 2: Chance Discovery System.

made up of layers ordered by degree of generality. 
Cyc uses two rules of inference in theorem proving, 
modus ponens and modus tollens.  
Cyc-NL is the natural language processing 
system associated with the Cyc KB. It could 
translate natural language into CycL. Cyc-NL has 
three main components: a lexicon, a syntactic parser 
and a semantic interpreter. The lexicon along with a 
generative morphology component generates part-
of-speech assignments for words in a sentence. The 
syntactic parser uses a grammar to generate all valid 
parses for the sentence.  The semantic interpreter 
produces pure CycL equivalent for the input 
sentence. 
4 CHANCE DISCOVERY SYSTEM 
Figure 2 shows the framework of chance discovery 
system. Nature Language Processing (NLP) modules 
analyze daily news and generate new knowledge 
which is represented in logic. The new knowledge is 
then integrated into public Cyc KB servers. The 
private Cyc KB server owned by the chance seeker 
will connect to public KB servers and update its 
knowledge. On the other hand, the chance seeker 
updates its private attributes in the private Cyc KB. 
The knowledge about chance seeker can be regarded 
as a virtual chance seeker living in Cyc KB. A 
chance seeker sets up its goals or uses default goals 
in the Goals & Plans Module. New knowledge 
triggers the CD modules that  measure the relevance 
of the new knowledge to the chance seeker. The new 
knowledge is considered to be a chance candidate if 
the relevance score is above a certain threshold. By 
trying to revise current plans using the new 
knowledge, the magnitude of this chance candidate 
can be measured using a utility evaluation process. 
When the magnitude of the utility is above a 
specified threshold, a chance is detected. Finally, the 
system visualizes the chances to chance seeker, and 
revises current plans for future chance detections. 
4.1 The Relevance of New Knowledge 
New knowledge is relevant to the chance seeker if it 
has an immediate impact on the seeker’s attributes 
or on the achievability of the chance seeker’s goals.  
For example, the new knowledge that shows that the 
chance seeker inherited a fortune is relevant as it 
changes the seeker’s wealth attribute. The new 
information can affect the achievability of goals in 
three ways:  
x
making new goals achievable, 
x
making some previously achievable goals 
unattainable, or  
x
changing the cost or reward of achieving some 
goals. 
A goal is considered achievable if the system 
finds a plan to the goal from the current state. To 
impact the achievability of a plan, the new 
knowledge could affect the causal support for 
actions in the plan or the likelihood of success. 
Testing the relevance of new information to the 
chance seeker is desirable to filter out irrelevant 
information. Fully testing the relevance of new 
information with respect to its impact on the chance 
seeker’s 
attributes 
and 
plans 
could 
be 
computationally expensive. Therefore, we gradually 
apply a series of relevance tests with increasing 
computational cost. These tests are:  
x
testing if the new information is subsumed by 
existing knowledge, 
x
testing for temporal relevance, 
x
testing for spatial relevance, 
x
testing for impact on the chance seeker’s 
attributes, and
x
testing for impact on the chance seeker’s plans. 
To verify that the new information is actually 
new, and is not subsumed by knowledge already in 
the KB, we test if it is entailed by existing 
knowledge. For example, if the KB contains 
assertions indicating that Paul Martin is the leader of 
the Liberal Party, that the Liberals won the largest 
number of seats in the parliament and that the leader 
of the party that wins the most seats becomes the 
Prime Minister. It becomes redundant to add an 
assertion indicating that Paul Martin became the 
Prime Minister. Similarly, if KB contains a 
generalization 
of 
the 
new 
information, 
this 
information will be redundant. 
The relevance of information in a dynamic 
stochastic system degenerates gradually over time. 
The rate of degeneration of information relevance 
with respect to a rational decision maker depends on 
the probabilities of change as well as on the relative 
utilities (Tawfik and Khan, 2005).  Cyc supports a 
notion of possibility akin to probability. However, it 
is unlikely that the probabilistic knowledge in the 
KB will be specified fully to construct dynamic 
belief networks. Therefore, we rely on the 
intersection of the temporal extents associated with 
temporal object in the KB to verify the mutual 
relevance of temporal objects. Similarly, most 
spatial effects also weaken with distance. Therefore, 
it is fair to filter out new knowledge whose spatial or 
temporal effects lie outside the scope of interest.  
New knowledge could be divided into rules and 
events (facts). We consider that the chance seeker 
relies on a rule if chance seeker includes some 
actions that are causally supported by the 
consequences of the rule into its plan. The impact of 
the rule measures the role of the rule in reaching the 
Towards a Change-Based Chance Discovery 
135

goals. It could be regarded as the utility changes that 
are credited to the rule B. If S represents the state of 
chance seeker’s attributes, then impact is given by: 
impactB=V(SB)-V(S) 
To assess V(SB), we consider two cases:  In one 
case, V(SB) may already be stated clearly in the rule. 
For example, the time saving from taking a newly 
built high speed train to a certain destination will be 
clearly stated in the news. On the other hand, if 
V(SB) is unclear, we can deduce a reasonable 
hypothesis by combining the new rule and existing 
rules in background KB. This hypothesis will not go 
beyond the known knowledge. For example, if there 
is an assertion in KB stating that all the people in the 
same country speak the same language, then 
communicating with all Brazilians will be the utility 
of learning Portuguese for a chance seeker who 
wants to travel to Brazil. Note that this utility could 
be inaccurate since it is based on a hypothesis. In 
general, impactB may act as a greedy measure of 
progress towards the goals but does not guarantee 
reaching these goals. An exogenous rule may 
undermine actions in the other part of chance seeker.  
 When new knowledge is an event, to determine 
the value of an event, we have to take other factors 
into account. An event could be composed by a 
bundle of assertions describing its features, such as 
actions, locations, time, physical object involved,
etc. The impact of an event according a particular 
chance seeker is based on the following features:  
x
Importance of the entities involved in the event. 
To evaluate an event, we take the importance of 
those objects into account. For example, 
‘Microsoft’ may be considered to be a more 
important company than other small companies. 
However, a small company currently working 
with Microsoft may be important.  
x
The relationship between involved objects and 
chance seeker needs to be taken into account. 
For example, a company owned by family 
members may mean a lot to chance seeker 
though it’s a small company. For example, the 
chance seeker may work for this small business. 
Generally, 
close 
relatives, 
friends, 
and 
acquaintances are more important that strangers. 
According to the above:  
Where VE  is a value function that takes into 
account the importance/size of objects, the attributes 
involved and the relationships between objects and 
the 
chance 
seeker 
including 
spatio-temporal 
relationships. VE tries to guess the potential change 
in the chance seeker’s attributes.  
A negative impact indicates that the new 
knowledge is a potential threat. In the case of 
irrelevant new knowledge, the impact will be inside 
the range of [negative threshold, positive threshold]. 
The new knowledge will be integrated into KB for 
future reference. On the other hand, the new 
knowledge will be considered as a chance candidate 
if the impact is outside the range.   
4.2 The Magnitude of Chances 
Here, B is the set of new knowledge that passes 
the relevance tests, the system will try to revise 
current plans (CP) of the chance seeker using B. 
Partial Order Planning (POP) and SATplan 
algorithm (Russell and Norvig, 2002) can be used to 
generate new plans (NPB) by taking B into account. 
In our system, SHOP (Nau et al. 1999)   generates 
the plans for the chance seeker. SHOP is a domain-
independent automated-planning system. It is based 
on ordered task decomposition, which is a type of 
Hierarchical Task Network (HTN) planning. 
By adopting NPB instead of CP, the chance 
seeker may be able to achieve a different set of 
goals, or save less time and/or money while 
achieving the same goals. All these features can be 
reflected by a utility function mapping. The 
magnitude of B denoted by MB is represented as the 
utility difference between NPB and CP. 
There could be a gap between the goals of NPB
and the goals of CS. As describing in section 2, a set 
of goals can be represented by a future status of 
attributes important to the chance seeker. If we use a 
utility function (V) to map those attributes into real 
values and add them together, we can represent a 
notion of preference. The change in the utilities 
could be represented as:  
MB=VNPB -VCP
MB represents the difference between new plans 
and current plans. If MB in the range of [negative 
threshold, positive threshold], it means that NPB and 
CP are roughly the same. The magnitude of B is low. 
Whether B is a chance or not, there are the following 
possible cases: 

Short-term setback: When B has negative effect 
on chance seeker’s attribute and no threat to the 
current plans, B will be ignored. 

Potential risk: When B has negative effect on 
chance seeker, and threatens some of the current 
plans. However, repair plans can be found such 
that the new plans including the repair plans can 
achieve the same goal as before. This is 
considered a potential risk even though it is 
possible to repair the plans because if the 
))
,
(
),
(
(
CS
Object
relations
Objects
Size
V
impact
i
i
i
E
Event ¦
 
Zhiwen Wu and Ahmed Y. Tawfik 
136

chance seeker proceeds with the original plans 
the goals may not be reached.  

Risk: Repair plans cannot be found, NPB
achieve fewer goals than before. MB is out of 
range. The system will consider B is a risk.  

Short-term prosperity: When B has positive 
effect on chance seeker’s attribute, and no effect 
on the current plans. 

Exploitable efficiency: NPB can achieve the 
same goals as CP but in significantly shorter  
time or costs less. B is considered as a chance. 

Improved reliability: NPB can achieve the same 
goals as before for approximately the same cost 
but offer an alternative for some plan elements. 

Inefficient alternative: Exploiting B, NPB can 
achieve fewer goals than before or the same 
goals at a higher cost without threatning CP.  B 
is ignored. 

Opportunity: NPB can achieve more goals than 
before. MB is significant and positive and B is 
considered a chance. 

Short-term gain long-term risk: When B has 
positive effect on chance seeker, threatens some 
of the current plans and the plans cannot be 
repaired. 

Short-term loss long-term gain: B results in an 
immediate loss but enables longer term plans. 
Finally, if a chance is detected, NPB will be set 
as CP. 
4.3 Visualizing Chances 
When a chance is detected, visualizing chances is 
important as the last step of chance discovery. 
Sometimes chance seeker may not understand why 
chances returned by chance discovery system are 
chances. Visualization of chances could emphasize 
on the explanation and help chance seeker to realize 
chances.  
A detail visualization explanation including 
display of the future status of attributes of chance 
seeker, display of chance seeker’s current plans, etc, 
may be necessary. Kundu et al. (2002) present a 3-D 
visualization technique for hierarchical task network 
plans. Such visualizations will be useful for the 
chance seeker to understand the interactions between 
various elements in the plan.  
5 DISCUSSION & EVALUATIONS 
The evaluation of chance discovery (CD) systems 
could be based on precision, efficiency and chance 
management. As discussed in Section 1, many 
previous CD approaches regard chances as unknown 
hypothesises, focusing on techniques to derive 
common chances, i.e. chances for all people. Our 
approach focuses on knowledge management, 
finding chances in known knowledge (news, WWW, 
etc) for a particular chance seeker by the support of 
a large and rich knowledge base. In the 2005 
tsunami tragedy, scientists correctly detected the 
occurrence of the tsunami, but failed to warn the 
relevant people in South Asia in time to evacuate. 
Hence, chances are relative.  
KeyGraph, as introduced in Section 1, is a 
widely used technique in CD research. Matsumura 
and Ohsawa (2003) present a method to detect 
emerging topic (web page as chance) by applying 
KeyGraph on web pages. A “Human Genome 
project” example was presented. Its benefits include 
finding cures to conquer fatal illness. Two sets of 
web pages (CA and CB), each containing 500 web 
pages, were obtained by searching “human genome” 
in Google. CA was obtained on Nov 26, 2000. CB
was on Mar 11, 2001. In the output of KeyGraph,  
Celera (www.celera.com), a growing HG research 
website, was detected as a chance in CB because 
Celera co-occurred with the most important 
(foundation) websites in CB.  The set of foundation 
websites of CA and CB, such as NCBI (the National 
Centre for Biotechnology Information), etc, is 
almost the same. The following events about Celera 
were reported in the meantime: 
1.
The Human Genome Project team and Celera 
announced the completion of the draft sequence 
of the human genome in June, 2000.  
2.
Craig Venter, President and Chief Scientific 
Officer of Celera and Francis Collins, Director 
of the Human Genome Project, met President 
Bill Clinton and British Prime Minister Tony 
Blair for the progress of the human genome 
analysis.
3.
Papers about the completion were published in 
Nature and Science in 2001.  
For a researcher in medicine whose goals include 
finding a cure for genetic diseases, our CD system 
would report a chance after evaluating events 1&2 
and would propose new plans. The system may draw 
the researcher’s attention to the draft sequence as 
early as on Jun 27, 2000 because Clinton and Blair 
are very important individuals. The degree of 
relevance will be high. The magnitude of  “the draft 
sequence” will be high since it makes the 
researcher’s 
unattainable 
goals 
achievable. 
Therefore, our approach could discover chances fast. 
6 CONCLUSION
This paper describes a chance discovery system 
based on Cyc Knowledge base. The knowledge base 
Towards a Change-Based Chance Discovery 
137

works as a virtual reality. Cyc KB simulates the 
development of real society by continuously 
updating its knowledge. The new knowledge comes 
from newspaper, magazine, and WWW, etc. The 
chance discovery system searches chances in KB for 
on behalf of the virtual chance seekers. By assessing 
the relevance of new knowledge, the irrelevant 
knowledge to a chance seeker is ignored. Then 
chance in relevant knowledge is detected by 
considering its impact on the current plans and the 
possibility of new plans that are built based on the 
new knowledge.  
REFERENCES
Abe, A., 2003a.. Abduction and analogy in chance 
discovery. In Ohsawa, Y. and McBurney, P., editors, 
Chance Discovery, pages 231–247. Springer-Verlag 
Berlin Heidelberg. 
Abe, A., 2003b. The role of abduction in chance 
discovery. New Gen. Comput., 21(1):61–71.  
Kundu, K., Sessions, C.,,desJardins, M. and Rheingans, P. 
2002. Three-dimensional visualization of hierarchical 
task network plans, Proceedings of the Third 
International NASA Workshop on Planning and 
Scheduling for Space, Houston, Texas.  
McBurney, 
P. 
and 
Parsons, 
S., 
2003. 
Agent 
communications for chance discovery. In Ohsawa, Y. 
and McBurney, P., editors, Chance Discovery, pages 
133–146. Springer-Verlag Berlin Heidelberg. 
Nau, D.  Cao, Y.,  Lotem, A. and Muñoz-Avila, H. 1999. 
SHOP: Simple Hierarchical Ordered Planner. In 
Proceedings of the International Joint Conference on 
Ohsawa, Y., 2003a. Keygraph: Visualized structure among 
event clusters. In Ohsawa, Y. and McBurney, P., 
editors, Chance Discovery, pages 262–275. Springer-
Verlag Berlin Heidelberg. 
Ohsawa, Y., 2003b. Modeling the process of chance 
discovery. In Ohsawa, Y. and McBurney, P., editors, 
Chance Discovery, pages 2–15. Springer-Verlag 
Berlin Heidelberg. 
Ohsawa, Y., Benson, N. E., and Yachida, M. ,1998. 
Keygraph: Automatic indexing by co-occurrence 
graph based on building construction metaphor. 
Proceedings of the Advances in Digital Libraries 
Conference, pages 12–18. IEEE Computer Society. 
Ohsawa, Y. and McBurney, P., 2003. Preface. In 
Ohsawa,Y. and McBurney, P., editors, Chance 
Discovery. Springer-Verlag Berlin Heidelberg. 
Ohsawa, Y. and Yachida, M., 1999. Discover risky active 
faults 
by 
indexing 
an 
earthquake 
sequence. 
Proceedings of the Second International Conference 
on Discovery Science, pages 208–219. Springer-
Verlag.
OpenCyc.org,2002. 
OpenCyc 
documentation. 
http://www.openCyc.org/doc/. 
Parsons, S. and McBurney, P., 2003. Logics of 
argumentation for chance discovery. In Ohsawa, Y. 
and McBurney, P., editors, Chance Discovery, pages 
150–165. Springer-Verlag Berlin Heidelberg. 
Russell, S. and Norvig, P., 2002. Artificial Intelligence: A 
Modern Approach. Prentice Hall Series in Artificial 
Intelligence, 2nd edition. 
Tawfik, A. Y. 2004. Inductive reasoning and chance 
discovery. Minds and Machines, Volume 14 (Issue 
4):441– 451. 
Tawfik, A. Y. and Khan, S., 2005. The Degeneration of 
Relevance in Dynamic Decision Networks with Sparse 
Evidence, Applied Intelligence, to appear. 
Zhiwen Wu and Ahmed Y. Tawfik 
138
Artificial Intelligence (IJCAI-99), pp. 968- 973. 

PART 3 
Information Systems
Analysis and Specification 

EARLY DETECTION OF COTS FUNCTIONAL SUITABILITY 
FOR AN E-PAYMENT CASE STUDY 
Alejandra Cechich 
Departamento de Ciencias de la Computación, Universidad del Comahue,   Neuquén, Argentina 
Email: acechich@uncoma.edu.ar 
Mario Piattini 
Escuela Superior de Informática, Universidad de Castilla-La Mancha, Ciudad Real, España 
Email: Mario.Piattini@uclm.es 
Keywords: 
Component-Based System Assessment, COTS components, Software Quality. 
Abstract: 
The adoption of COTS-based development brings with it many challenges about the identification and 
finding of candidate components for reuse. Particularly, the first stage in the identification of COTS 
candidates is currently carried out dealing with unstructured information on the Web, which makes the 
evaluation process highly costing when applying complex evaluation criteria. To facilitate the process, in 
this paper we introduce an early measurement procedure for functional suitability of COTS candidates, and 
we illustrate the proposal by evaluating components for an e-payment case study.                                         
1 INTRODUCTION 
COTS-Based 
System 
Development 
is 
now 
recognised as the way forward in building software 
architectures 
that 
can 
operate 
in 
advanced 
distributed, intranet, and Internet environments. In 
essence, using components to build systems reduces 
complexity because composers do not need to know 
how a component works internally. They only need 
to know what the component is and the services it 
provides. 
Ideally, 
most 
of 
the 
application 
developer’s time is spent integrating components. 
Components become unwieldy when combined and 
re-combined in large-scale commercial applications. 
What are needed are ensembles of components that 
provide major chunks of application functionality 
that can be snapped together to create complete 
applications.   
COTS component filtering is to decide which 
components should be selected for more detailed 
evaluation. Decisions are driven by a variety of 
factors – foremost are several design constraints that 
help define the range of components. So a balance is 
struck, depending upon the level of abstraction, 
complexity of the component, goals and criteria, and 
so 
forth. 
Some 
methods 
include 
qualifying 
thresholds for filtering. For example, during the 
activity 
"Collect 
Measures" 
of 
the 
COTS 
Acquisition Process (Ochs et al., 2000), data 
according to a measurement plan are collected on a 
set of COTS software alternatives. Data are used in 
the filtering activity to eliminate those COTS 
alternatives that are unacceptable for use.  
Identification of COTS candidates is a complex 
activity itself. It implies not only dealing with an 
impressive number of possible candidates but also 
with unstructured information that requires a careful 
analysis. In this context, some proposals use 
description logics to develop an ontology for 
matching requested and provided components 
(Braga et al., 1999; Pahl, 2003); others suggest 
extending the identification stage with a learning 
phase, which provides support to the COTS 
component 
discovery 
process 
(Jaccheri 
and 
Torchiano, 2002). Some other approaches try to 
measure the semantic distance between required and 
offered functionality (Alexander and Blackburn, 
1999;  Jilani and Desharnais, 2001) but these 
measures usually need detailed information as input 
to the calculations.  
In addition to learning and classification issues, a 
filtering process is concerned with the pre-selection 
of candidates. It actually takes place by matching 
several properties of COTS components, including 
some inexact matching. Moreover, there are some 
cases where goals cannot be entirely satisfied 
without considerable product adaptation and other 
141
C.-S. Chen et al. (Eds.), Enterprise Information Systems VII, 141–148.
© 2006 Springer.

cases where these goals must be resigned to match 
product features (Alves and Filnkestain, 2002); 
(Cooper and Chung, 2002). 
As a possible improvement, the Six Sigma 
approach has been suggested selecting packaged 
software (Tayntor, 2002); however the evaluation 
mainly relies on the information provided by demos 
and additional documentation of the software. Then, 
the lack of measures makes this process perfectible.  
Along these lines, our approach based on Six-
Sigma precepts, focuses on fact-based decisions, 
teamwork, and measurement as a way of driving the 
identification and filtering process (Cechich and 
Piattini, 2004a; Cechich and Piattini 2004b).  
We refer to a component-based system as a 
system that uses at least one component in 
conjunction with other components, legacy systems, 
and other pieces of software – including COTS 
components – to satisfy user’s requirements. This 
concept is introduced to emphasize the fact that the 
output from the system satisfies the user’s 
requirements by using the functionality supplied by 
at least one COTS component. Particularly, we 
consider functional suitability as the main aspect to 
be measured; however, measures should be 
expressed in such a way that calculation is possible 
at early stages.
Our proposal aims at improving the filtering 
process by  performing three steps: (1) a 
“commitment” step, which produces a committed 
required specification of a component;  (2) a “pre-
filtering” step, in which COTS candidates are pre-
selected according to their functional suitability; and 
(3) a “filtering” step, in which architectural 
semantics adaptability produces an indicator of 
stability that serves as  a basis for the final candidate 
filtering.  In this paper, we particularly address the 
second step (“pre-filtering”), in which functional 
suitability measures are calculated and analysed. 
Metrics for COTS based systems are emerging 
from the academic and industrial field (Martín-Albo 
et al., 2003). However, many of these definitions do 
not provide any guideline or context of use, which 
makes metric’s usability dependable on subjective 
applications. Measures are not isolated calculations 
with different meanings; on the contrary, capability 
of measures is strongly related to the process of 
calculating and providing indicators based on the 
measures. Our approach intends to define a filtering 
process in which measures are included as a way of 
providing more specific values for comparison. At 
the same time, the process guides the calculation, so 
ambiguity is decreased. 
Among other relationships, resulting measures 
are related to the artefact to be measured. In our 
approach, the artefact is expressed as functionality 
required 
by 
a 
particular 
application, 
and 
functionality offered by COTS candidates. Generally 
speaking, both cases are subject to analysing 
information that is modelled and weighted by people 
– composers or integrators on one side, and 
component’s suppliers on the other. Different 
interpretations, perceptions, and judgements are then 
affected by the expressiveness of information. 
Nevertheless, our comparisons are abstract-level 
definitions, which allow us to customize the filtering 
process by instantiating the calculation procedure 
according to different contexts of use.  
Since information needed to compute the 
measures 
depends 
on 
how 
COTS 
suppliers 
document COTS component’s functionality (Bertoa 
et al., 2003), and how requirements are specified, in 
this paper we illustrate how metrics might be 
calculated by measuring functional suitability on  
COTS candidates for an E-payment case study.  
In section 2 we briefly introduce our compact 
suite of measures (Cechich and Piattini, 2004c) that 
should be used during the pre-filtering process. 
Then, section 3 shows how measures might be 
applied to our case and provides some discussion. A 
final section addresses conclusions and topics for 
further research. 
2 MEASURING FUNCTIONAL 
SUITABILITY  
In the previous section, we have emphasized the fact 
that a system should satisfy the user’s requirements 
by using the functionality supplied by at least one 
COTS component. Then, given a specification SC for 
an abstract component type C, a candidate 
component K to be a concrete instance of C must 
conform to the interface and behaviour specified by 
SC.  Mappings in SC, which represent the different 
required functionalities, are established between 
input and output domains.  We focus on 
incompatibilities derived from functional differences 
between the specification in terms of mappings of a 
component Ki (SKi) and the specification in terms of 
mappings of SC.
Our measures have been defined to detect 
domain 
compatibility 
as 
well 
as 
functional 
suitability. Let us briefly clarify this point: domain 
compatibility measures show that there are some 
candidate components able to provide some 
functionality. However, we cannot be certain of the 
amount of functionality that is actually provided – 
matching input data does not certify that output data 
match too. Therefore, even a component might be 
full domain compatible, there is still another set of 
measures to be applied in order to determine the 
functional suitability. 
Alejandra Cechich and Mario Piattini 
142

Let us illustrate the measurement procedure by 
using an  credit card payment system as an example. 
We suppose the existence of some scenarios 
describing the two main stages of the system – 
authorization and capture. Authorization is the 
process of checking the customer’s credit card. If the 
request is accepted, the customer’s card limit is 
reduced temporarily by the amount of the 
transaction. Capture is when the card is actually 
debited. 
Scenarios 
will 
provide 
an 
abstract 
specification of the mappings of SC that might be 
composed of: 
- Input domain:  
(AID) Auth_IData{#Card, Cardholder_Name, Exp-
Date};
(CID) Capture_Idata{Bank_Account, Amount}. 
- Output domain:  
(AOD)  Auth_Odata{ok-Auth};  
(COD)
 Capture_Odata{ok_Capture, DB_Update}. 
- Mapping: {AID o AOD};{CID o  COD} 
Suppose we pre-select two components to be 
evaluated, namely K1 and K2 respectively. A typical 
situation for inconsistency in the functional 
mappings between SK1, SK2 and SC is illustrated in 
Figure 1, where dashed lines indicate (required) 
mappings with respect to SC, and the solid lines are 
(offered) mappings with respect to SK1 (grey) and SK2
(black). Note that the input domain of the 
component K1 does not include all the values that the 
specification
SC 
requires, 
i.e. 
the 
capture 
functionality is not provided. Besides, the input 
domain of the component K2 includes more values 
than the required by SC, although the mapping 
satisfies the required functionality. We should also 
note that there is another functionality provided by 
K2, i.e. {Taxes o  Statistics}, which might inject 
harmful effects to the final composition. 
C
K1
K2
Table 1: Description of Functional Suitability measures 
Measure Id. 
Description
Component-Level 
CFC
Compatible 
Functionality 
The 
number 
of 
functional 
mappings provided by SK and 
required by SC in the scenario S
MFC
Missed Functionality 
The 
number 
of 
functional 
mappings required by SC and 
NOT provided by SK in the 
scenario S.
AFC
Added Functionality 
The 
number 
of 
functional 
mappings NOT required by SC
and provided by SK in the 
scenario S.
CCF
Component
Contribution
Percentage in which a component 
contributes to get the functionality 
required by SC in the scenario S.
Solution-Level 
SNCF 
Candidate Solution 
The number of components that 
contribute 
with 
compatible 
functionality 
to 
get 
the 
requirements of SC in the scenario 
S.
CFS
Compatible 
Functionality 
The 
number 
of 
functional 
mappings provided by SN and 
required by SC in the scenario S.
MFS
Missed Functionality 
The 
number 
of 
functional 
mappings required by SC in the 
scenario S and NOT provided by 
SN.
AFS
Added Functionality 
The 
number 
of 
functional 
mappings NOT required by SC in 
the scenario S and provided by 
SN.
SCF
Solution Contribution 
Percentage in which a solution 
contributes to get the functionality 
required by SC in the scenario S.
Our measures on functional suitability  have 
been 
classified 
into 
two 
different 
groups: 
component-level 
measures 
and 
solution-level 
measures. The first group of measures aims at 
detecting 
incompatibilities 
on 
a 
particular 
component K, which is a candidate to be analysed. 
However, it could be the case that we need to 
incorporate more than one component to satisfy the 
functionality required by the abstract specification 
SC. In this case, the second group of measures 
evaluates the functional suitability of all components 
that constitute the candidate solution.
Table 1 lists our suite of functional suitability 
measures. We refer the reader to (Cechich and 
Piattini, 2004c) for their formal definition. Solution-
level metrics are listed here for completeness 
reasons, since our case study only needs to apply 
• AID
• CID
dom SC
• Taxes
dom  SK1
dom SK2
• AOD
• COD
ran SC
• Statistics
ran  SK1
ran SK2
SKi(i)
SC(i)
SK2(i)
Early Detection of Cots Functional Suitability for an E-payment Case Study 
143
/S
Figure 1: Functional mappings of S
component-level measures;
i.e. combination of 
and
.
 S

components from the marketplace is not necessary to 
get the required functionality, therefore a solution-
level analysis is not required.  
3 MEASURING COTS 
CANDIDATES: A CASE STUDY 
Scenarios describing the two main stages of a credit 
card payment, as we introduced in the previous 
section, represent here a credit card (CCard) 
payment system, which provide an abstract 
specification of the input (AID, CID) and output 
domains (AOD, COD) of a component C, and their 
corresponding mappings.  
After a quick browse on the Web as a COTS 
repository, we chose COTS components catalogued 
by 
the 
ComponentSource 
organization 
(www.componentsource.org) as members of the 
“Credit Card Authorization” group. Following, we 
introduce some examples of our analysis.
Firstly, 
we 
chose 
one 
component 
–  
AcceptOnline  by Bahs Software – as a candidate to 
provide the required functionality. Properties of 
AcceptOnline are grouped into the following 
classes: merchant fields, transaction fields, and 
response fields. From those classes, we identify: 
x
transaction_type: This field identifies the type 
of 
transaction 
being 
submitted. 
Valid 
transaction types are: “CK” (System check), 
“AD” 
(Address 
Verification) 
“AS”
(Authorization), 
“ES” 
(Authorization 
and 
Deposit), “EV” (Authorization and Deposit with 
Address Verification), “AV” (Authorization 
with Address Verification), “DS” (Deposit), and 
“CR” (Credit). 
x
cc_number: The credit card number to which 
this transaction will be charged. 
x
cc_exp_month and cc_exp_year: The numeric 
month (01-12) and the year (formatted as either 
YY or CCYY) in which this credit card expires. 
x
billing phone: The shopper’s telephone number. 
x
grand total: The total amount of the transaction. 
x
merchant email: This is the Email address of the 
merchant. 
x
order type: This field determines which fields 
are used to validate the merchant and/or hosting 
merchant. 
x
transactionStatus: Transaction Status. Valid 
values are: G - Approved, D -Declined, C - 
Cancelled, T - Timeout waiting for host 
response, R – Received.  
Table 2: Required Fields by Transaction Type
Field
CK
AD
AS
ES
EV
AV
DS
CR
authorization
Y
billing_address1;
billing address2 
Y 
 
 Y 
Y 
billing_zip 
Y 
 
 Y 
Y 
billing_pone 
Y 
Y 
Y 
Y 
Y 
cc_number;  
cc_exp_month; 
cc_exp_year
Y
Y
Y
Y
Y
Y
Y
counter 
Y 
Y
Y
Y
Y
Y
debug
Y 
Y 
Y 
Y 
Y 
Y 
Y 
grand_total 
Y 
Y
Y
Y
Y
Y
merchant_email 
Y 
Y 
Y 
Y 
Y 
Y 
Y 
order_numer 
 
Y
Y
….
…
Methods of AcceptOnline are specified in terms 
of their main focus and required input. Particularly, 
the SendPacket method is used to send the 
transaction info to the ECHOOnline server, and 
required properties should be filled as shown in 
Table 2 (requirements for CR are partially listed). 
From the AcceptOnline (AOnline) description 
above, we might derive the following mappings 
related to our authorization (AS) and capture (DS) 
required functionality: 
– Input domain: 
 (AOnline.ASI) 
{billing_phone, 
cc_number, 
cc_exp_month, cc_exp_year, counter, debug, grand 
total, merchant_email}; 
(AOnline.DSI) 
{authorization, 
cc_number, 
cc_exp_month, 
cc_exp_year, 
counter, 
debug, 
grand_total, merchant email}. 
– Output domain: 
 (AOnline.ASO) {TransactionStatus};
 (AOnline-DSO) {TransactionStatus}. 
– Mapping: 
{AOnline.ASI o AOnline.ASO;  
AOnline.DSI o AOnline.DSO} . 
There 
are 
also 
other 
possible 
functional 
mappings as follows: 
{AOnline.ADI o AOnline.ADO;  
AOnline.EVI o AOnline.EVO; 
AOnline.AVI o AOnline.AVO;  
AOnline.CRI o AOnline.CRO},  
which 
represent 
address 
verification, 
authorization and deposit with address verification, 
and so forth. 
Alejandra Cechich and Mario Piattini 
144

For brevity reasons, we assume here that input 
domain compatibility measures have indicated that 
the AcceptOnline component is a candidate for 
further evaluation – after comparing AID, CID 
(from specification SC) to AOnline.ASI and 
AOnline.DSI. We should note that values of the 
input domain do not exactly match: billing_phone is 
used instead of cardholder_name to identify 
cardholders; and merchant_email is used for 
Bank_id. Similarly, ok_Auth, ok_Capture, and 
BD_Update might correspond to the different values 
of TransactionStatus. However, in all cases 
matching is possible since purpose is similar. Then, 
similarity is basically determined by analysing 
semantics of concepts with respect to their use. 
Now, computing measures from Table 1 
produces the following results: 
CFC = 2; MFC = 0; AFC = 4; and CCF = 1 
These results indicate that the AcceptOnline 
component has proved being 100% (CCF = 1) 
functionally suitable, and thus a candidate for further 
evaluation during the filtering process – for example 
by analysing size and complexity of adaptation. 
Measures also indicate that there are four added 
functions (AFC = 4), which deserve more careful 
examination. 
Let us analyse a second component from the 
same group, i.e. catalogued as a member of “Credit 
Card Authorization”. This time, we have chosen the 
Energy 
Credit 
Card 
component 
by 
Energy 
Programming as the candidate to provide the 
required functionality. 
The Energy Credit Card component provides two 
functions described as follows: 
1. Functionality “Extract_Card_Data”, which 
provides the ability to decode the magnetic data on 
the swipe card; and 
2. Functionality “Validate_Card_Details”, which 
provides the ability to validate keyed entry data from 
other systems. 
To accomplish both functionalities, input data is 
required as follows:
Input: 
{surname, initials, salutation, card_number, 
card_type, startDate, expiryDate, issue} 
Output: {error_number, error_text} 
As we easily can see, this second component 
does not provide the required functionality of our 
scenario. Although the component is classified as a 
member of the “Credit Card Authorization” group, 
functionalities show that only validation of credit 
card data is provided. Therefore, calculating 
measures from Table 1 would produce the following 
results:
CFC = 0; MFC = 2; AFC = 0; and CCF = 0 
These results indicate that the Energy Credit 
Card component is 0% (CCF = 0) functionally 
suitable, and we should not invest  more time and 
effort in more evaluation. However, note that 
functionalities provided by the Energy Credit Card 
component 
might 
be 
part 
of 
the 
required 
functionality associated to the “Authorization” 
scenario. To make this point explicit, if necessary, 
evaluators should expose the different functionalities 
through a more detailed description of the required 
scenario; hence calculation of partially satisfied 
functionality would be possible. In our example, 
“Authorization” could be expressed as “Credit Card 
Validation” and “Amount Authorization”. In this 
way,  calculating measures for the Energy Credit 
Card component would result in: 
CFC = 1; MFC = 2; AFC = 0; and CCF = 0.33 
These results  would indicate that the Energy 
Credit Card component might be a candidate to be 
combined along with other components to provide 
the required functionality (and not necessarily 
discharged). Of course, decisions on how detailed an 
scenario should be depend on requirements on a 
particular domain; i.e. components that do not 
provide the whole authorization procedure might not 
be useful in a particular case. We suppose here that 
balanced requirements among all stakeholders have 
been considered to provide the appropriated 
scenarios (Cechich and Piattini, 2004b). 
Now, let us consider a third component for our 
evaluation 
procedure: 
the 
PaymentCardAssist 
component by Aldebaran, that supports e-mail 
verification, event logging, data encryption, file 
compression, and payment card detail validation. 
The 
PaymentCard 
object 
within 
the 
DeveloperAssist Object Library validates payment 
card (credit, debit and charge card) information. The 
PaymentCard object does not provide authorization 
or clearing functionality, but rather provides a means 
to validate payment information entered by a site 
visitor, before pursuing a full authorization. After 
considering detailed data to be validated, we can  see 
that our measures will result as: 
CFC = 0; MFC = 2; AFC = 4; and CCF = 0; 
or  after considering a more detailed scenario, in 
which card data validation is made explicit, 
measures will result as: 
Early Detection of Cots Functional Suitability for an E-payment Case Study 
145

CFC = 1; MFC = 2; AFC = 4; and CCF = 0.33 
Finally, let us consider another component from 
the same group – the CCProcessing component by 
Bahs Software. It supports the authorization, 
settlement (capture) , and credit/refund operations. 
“Authorization” 
is 
divided 
 
into
“PerformAuthorization” 
and 
“AddToBatch”
operations, meanwhile “Capture” corresponds to the 
“PerformSettlement” 
operation. 
 
Transaction 
descriptions are presented as follows: 
x
“PURCHASE”: 
Standard 
purchase
transaction (In "card not present" mode); 
x
“PURCHASE_TRACK1”: 
Purchase
transaction in "card present" mode. Track1 
property should be set for such transaction 
type.
x
“VALIDATE_CARD”: Card authentication 
to determine only if a card has been reported 
lost or stolen. 
x
“REVERSE_AUTHORIZATION”: On-line 
Authorization Reversal. 
x
“REVERSE_SETTLEMENT”: 
Store 
& 
Forward Authorization Reversal.  
x
“CREDIT”: Credit/refund operation.  
By analysing input and output domains of 
CCProcessing, we have identified mappings that 
cover the functionalities described by our scenario. 
Considering “credit” and address validation (part of 
“validate card”) as additional functionality (reverse 
authorization and reverse settlement might be 
considered as part of a “Cancel” operation), 
measurement results might be expressed as: 
CFC = 2; MFC = 0; AFC = 2; and CCF = 1 
A similar treatment was applied to evaluate the 
other 
components 
in 
the 
group. 
From 
22 
components, we consider 12 for analysis since the 
other 10 components differ only in terms of their 
implementations, preserving the same functionality. 
Results of our calculations are shown in Table 3. 
Note that only four components provide the 
functionality required by our scenario. This fact 
would indicate that those components are pre-
selected for more evaluation, since they are 100% 
functionally suitable.  A special remark should be 
made on values assigned to the ComponentOne 
Studio Enterprise:  this component is a combination 
of 
four 
individual 
components 
that 
support 
reporting, charting, data manipulation, and user 
interface capabilities for .NET, ASP.NET, and 
ActiveX applications. As readers easily can see, this 
component essentially differs from the others in the 
group; however it is classified as a “Credit Card 
Authorization” 
component. 
For 
this 
reason, 
additional functionality (AFC) has not been scored. 
Table 3: Measurement results for components in the 
“Credit Card Authorization” category
Component 
CFC
MFC
AFC
CCF
AcceptOnline 
2 
0 
4 
1
CCProcessing 
2 
0 
2 
1
CCValidate 
0 
2 
0 
0 
CreditCardPack 
0 
2 
0 
0 
EnergyCreditCard 
0 
2 
0 
0 
IBiz 
2 
0
2
1
InaCardCheck 
0 
2 
0 
0 
IPWorks 
2 
0
1
1
LuhnCheck 
0 
2 
0 
0 
PaymentCardAssist 
0 
2 
4 
0 
SafeCard 
0 
2
0 
0 
ComponentOneStudio 
0 
2 
** 
0 
3.1 Discussion 
Scenarios have been widely used during design as a 
method to compare design alternatives and to 
express the particular instances of each quality 
attribute important to the customer of a system. 
Scenarios differ widely in breadth and scope, and its 
appropriate selection is not straightforward. Our use 
of scenarios is a brief description of some  
anticipated or desired use of a system. We 
emphasize the use of scenarios appropriated to all 
roles involving a system. The evaluator role is one 
widely considered but we also have roles for the 
system composer, the reuse architect, and others, 
depending on the domain.  
The process of choosing scenarios for analysis 
forces designers to consider the future uses of, and 
changes to, the system. It also forces to consider 
non-functional properties that should be properly 
measured during the COTS selection process. In 
some cases, this diversity of concerns produces fine-
grained  functionality described by scenarios, but 
coarse-grained functionality might be described as 
well.
As a consequence, our measures are affected by a 
particular scenario’s description since calculation 
refers to the number of functions – without further 
discussion about their particular specification. For 
example, in our CCard system, “validation with 
address” and “reverse authorization” could be 
considered as part of an ordinary credit card 
authorization process. Assuming that, scores for 
added functionality (AFC) would be decreased (only 
“credit” 
would 
be 
considered 
as 
added 
functionality). As another example, we could choose 
a more detailed description of the functionality and 
Alejandra Cechich and Mario Piattini 
146

decompose “Authorization” into “Credit Card 
Validation” and “Credit Card Authorization”. In this 
case, 
calculation 
of 
provided 
and 
missed 
functionality would be different and contribution 
(CCF) would show which components partially 
contribute to reach credit card authorization. 
Table 4 shows our measures considering the last 
two 
assumptions: 
(1) 
including 
validation 
with/without address and reverse authorization as 
part 
of 
the 
procedure, 
and 
(2) 
splitting 
“Authorization” into two processes – validation and 
authorization itself. By comparing scores from Table 
3 and Table 4 we illustrate the importance of 
standardizing 
the 
description 
of 
required 
functionality as well as providing a more formal 
definition of scenarios. 
Also, note that components providing all 
required functionality remain unchanged on both 
tables: only four components provide authorization 
and capture as required in our case (4 / 12 = 33%). It 
would indicate that searching a catalogue by 
category is not enough to find appropriated 
components. In our example, better categorizations 
would help distinguish credit card validation from 
authorization. Moreover, a better categorization 
would help avoid that a component that does not 
provide any functionality (accordingly to the 
category), like ComponentOneStudio, be catalogued 
as a member of any of those classes. 
Table 4: Measurement results after changing scenarios
Component 
CFC
MFC
AFC
CCF
AcceptOnline 
3 
0 
1 
1 
CCProcessing 
3 
0 
1 
1 
CCValidate 
1 
2 
0 
0.33 
CreditCardPack 
1 
2 
0 
0.33 
EnergyCreditCard 
1 
2 
0 
0.33 
 IBiz 
3 
0 
1 
1 
InaCardCheck 
1 
2 
0 
0.33 
IPWorks 
3 
0 
0 
1 
LuhnCheck 
1 
2 
0 
0.33 
PaymentCardAssist 
1 
2 
4 
0.33 
SafeCard 
1 
2 
0 
0.33 
ComponentOneStudio 
0 
3 
*** 
0 
Our measures indicate that four components are 
candidates to be accepted for more evaluation, i.e. 
the components are functionally suitable but there is 
some additional functionality that could inject 
harmful side effects into the final composition.  
Identifying and quantifying added functionality are 
subject to similar considerations – the number of 
functions essentially is a rough indicator that might 
be improved by weighting functionality; i.e. clearly 
the four functions added by the component 
PaymentCardAssist are different in scope and 
meaning from the other added functions. However, 
just counting functions would help decide on which  
components the analysis should start.  
Table 4 also shows that there are some candidates 
which 
are 
able 
to 
provide 
some 
required 
functionality – “credit card validation”. But making 
this functionality more visible not necessarily 
indicate the type of validation that actually is taking 
place, for example whether or not a MOD10/Luhn 
check digit validation is carried out. Our measures 
are just indicators of candidates for further 
evaluation, on which additional effort might be 
invested. Nevertheless,  our measures do not detect 
the best candidates at a first glance but a possible 
interesting set. A process guides calculations so 
ambiguity is decreased (Cechich and Piattini, 
2004a), but committed scenarios still depend on 
particular system’s requirements.  
Besides, there are another types of analysis the 
component should be exposed before being eligible 
as a solution – such as analysis of non-functional 
properties, analysis of vendor viability, and so forth 
(Ballurio et al., 2002). Our set of measures are only 
providing a way of identifying suitable components 
from a functional point of view. We might provide a 
more precise indicator when calculating the 
maintenance equilibrium value as introduced in 
(Abts, 2002): “Maximise the amount of functionality 
in your system provided by COTS components but 
using as few COTS components as possible”.
A final remark brings our attention into the 
necessity of balancing required and offered 
functionality during COTS-based developments. 
After analysing candidates, we might also change 
our 
expectations 
on 
finding 
appropriated 
components. In this case, we could potentially resign 
most of our expectations on a particular requirement 
letting offered services prevail.  For example, we 
could keep some of the alternative services resigning 
others whether COTS candidates are hard to find or 
adapt. An additional measure on modifiability  of 
goals (Cechich and Piattini, 2004b) would help 
detect the degree in which certain functionality can 
be changed when selecting COTS components. Of 
course, we could also decide not to select 
components at all, and build a solution from scratch. 
4 CONCLUSION 
We have briefly presented some measures for 
determining 
functional 
suitability 
of 
COTS 
candidates by applying the calculations on a case 
study. It showed how COTS information may be 
mapped onto our measurement model leading to an 
early value for decision making. 
Early Detection of Cots Functional Suitability for an E-payment Case Study 
147

However, differences in COTS component 
documentation 
make 
evaluation 
harder. 
Our 
application clearly remarks the importance of 
standardising COTS component documentation and 
analysing the diverse ways of structuring COTS 
component’s information to facilitate functional 
matching detection. However, successful matching 
also depends on how functional requirements are 
specified. 
Then, 
a 
formal 
procedure 
for 
identification of candidates should be defined to 
make the process cost-effectively.  
Constraints on the component’s use and 
constraints relative to a context might be also useful 
to be considered. These aspects would indicate that 
providing more complex classifications, such as 
taxonomies of components, would help catalogue 
them in a marketplace. Additionally,  more complex 
descriptions might be provided by using ontologies 
and contexts. Along these lines, our future work 
aims at defining some guidelines and hints on the 
searching and learning process of COTS component 
candidates. 
ACKNOWLEDGMENTS
This work was partially supported by the CyTED  
project VII-J-RITOS2, by the UNComa project 
04/E059, and by the MAS project supported by the 
Dirección General de Investigación of the Ministerio 
de Ciencia y Tecnología (TIC 2003-02737-C02-02). 
REFERENCES
Abts C. COTS-Based Systems (CBS) Functional density - 
A Heuristic for Better CBS Design, 2002. In 
Proceedings of the First International Conference on 
COTS-Based Software Systems, Springer Verlag 
Alexander R. and Blackburn M., 1999. Component 
Assessment Using Specification-Based Analysis and 
Testing. 
Technical 
Report 
SPC-98095-CMC,  
Software Productivity Consortium.
Alves C. and Filkenstein A., 2002. Challenges in COTS-
Decision Making: A Goal-Driven Requirements 
Engineering Perspective. In Proceedings of the 
Fourteenth International Conference on Software 
Engineering and Knowledge Engineering, SEKE’02. 
Ballurio K., Scalzo B., and Rose L, 2002. Risk Reduction 
in COTS Software Selection with BASIS. In 
Proceedings of the First International Conference on 
COTS-Based Software Systems, ICCBSS 2002, 
Springer-Verlag LNCS 2255 , pp. 31-43. 
Bertoa M., Troya J., and Vallecillo A., 2003. A Survey on 
the Quality Information Provided by Software 
Component Vendors. In Proceedings of the ECOOP 
QAOOSE Workshop.
Braga R., Mattoso M., and Werner C., 2001. The use of 
mediation and ontology for software component 
information retrieval. In Proceedings of the 2001 
Symposium on Software Reusability: putting software 
reuse in context, ACM Press, pp. 19-28. 
Cechich A. and  Piattini M., 2004a. Managing COTS 
Components using a Six Sigma-Based Process. In 
Proceedings of the 5th International Conference on 
Product Focused Software Process Improvement,
PROFES 2004, volume 2009 of LNCS, Springer-
Verlag, pp.556-567. 
Cechich A. and  Piattini M., 2004b. Balancing 
Stakeholder’s Preferences on Measuring COTS 
Component Functional Suitability. In Proceedings of 
the 6th International Conference on Enterprise 
Information Systems, ICEIS 2004, pp. 115-122. 
Cechich A. and  Piattini M., 2004c. On the Measurement 
of COTS Functional Suitability. In Proceedings of the 
3rd International Conference on COTS-based Software 
Systems, ICCBSS 2004, volume 2959 of LNCS, 
Springer-Verlag, pp. 31-40. 
Cooper K. and Chung L., 2002. A COTS-Aware 
Requirements 
Engineering 
and 
Architecting 
Approach: Defining System Level Agents, Goals, 
Requirements and Architecture, Technical Report 
UTDCS-20-02, Department of Computer Science, The 
University of Texas at Dallas.
Jaccheri L. and Torchiano M., 2002. A Software Process 
Model to Support Learning of COTS Products. 
Technical Report, IDI NTNU.
Jilani L. and Desharnais J., 2001. Defining and Applying 
Measures of Distance Between Specifications. IEEE 
Transactions on Software Engineering, 27(8):673—
703 . 
Martín-Albo J., Bertoa M., Calero C., Vallecillo A., 
Cechich A., and Piattini M., 2003. CQM: A Software 
Component Metric Classification Model. In Proc. of 
the 7th ECOOP Workshop QAOOSE 2003, pages 54-
60, Darmstadt, Germany. 
Ochs, D. Pfahl, G. Chrobok-Diening, and Nothhelfer-
Kolb, 2000. A Method for Efficient Measurement-
based COTS Assessment and Selection - Method 
Description and Evaluation Results. Technical Report 
IESE-055.00/ E, Fraunhofer Institut Experimentelles 
Software Engineering.
Pahl C., 2003. An Ontology for Software Component 
Matching. In Proceedings of the Sixth International 
Conference on Fundamental Approaches to Software 
Engineering, volume 2621 of LNCS, Springer-Verlag, 
pp. 6-21. 
Tayntor C., 2002. Six Sigma Software Development.
Auerbach Publications. 
Alejandra Cechich and Mario Piattini 
148
LNCS 2255, pages 1-9. 

PRESERVING THE CONTEXT OF
INTERRUPTED BUSINESS PROCESS ACTIVITIES
Sarita Bassil1, Stefanie Rinderle2, Rudolf Keller3, Peter Kropf4 and Manfred Reichert5
1DIRO, University of Montreal, C.P. 6128, succ. Centre-ville, Montreal, Quebec, H3C 3J7, Canada
bassil@iro.umontreal.ca
2DBIS, Faculty of Computer Science, University of Ulm, Germany, rinderle@informatik.uni-ulm.de
3Z¨uhlke Engineering AG, Schlieren, Switzerland, ruk@zuehlke.com
4Institute of Computer Science, University of Neuchatel, Switzerland, peter.kropf@unine.ch
5Information Systems Group, University of Twente, The Netherlands, m.u.reichert@cs.utwente.nl
Keywords:
Information systems, business processes, ﬂexibility, data analysis, B2B and B2C applications.
Abstract:
The capability to safely interrupt business process activities is an important requirement for advanced process-
aware information systems. Indeed, exceptions stemming from the application environment often appear
while one or more application-related process activities are running. Safely interrupting an activity consists
of preserving its context, i.e., saving the data associated with this activity. This is important since possible
solutions for an exceptional situation are often based on the current data context of the interrupted activity.
In this paper, a data classiﬁcation scheme based on data relevance and on data update frequency is proposed
and discussed with respect to two different real-world applications. Taking into account this classiﬁcation, a
correctness criterion for interrupting running activities while preserving their context is proposed and analyzed.
1
INTRODUCTION
To stay competitive in the market, companies must
be able to rapidly react to changing situations and to
align their business processes accordingly (Reichert
et al., 2003). In particular, e–business needs a pow-
erful infrastructure to isolate process logic from ap-
plication code (Gartner Group, 1999), and to deﬁne,
control, and monitor business processes.
Process–
Aware Information Systems (PAIS) offer a promising
perspective in this context (v.d. Aalst and van Hee,
2002). They aim to connect activities, i.e., pieces of
work to perform a task, in order to achieve a common
goal (Workﬂow Management Coalition, 1999).
However, today’s companies need to maintain a
satisfying level of agility. It appears that agile PAIS
are the ones that provide, among other things, an ap-
propriate and a competent way to cope with changing
situations and unexpected events. This, in turn, is of
particular importance for adequately supporting long-
running, distributed business processes.
From this perspective, transportation companies for
instance must adopt solutions where a close follow-up
of activities is possible such that a customer request
is well satisﬁed. An example of a transportation ac-
tivity is “move vehicle V from origin location O to
destination location D”. A close follow-up of this ac-
tivity can be achieved using GPS (Global Positioning
System) which enables to continuously calculate and
provide the position of a vehicle in movement.
Moreover, the occurrence of unexpected problems
during transportation cannot be avoided.
Indeed,
there is ample evidence that ﬂeet management at the
operational level (e.g., scheduling of transportation
activities) is highly dynamic in the sense that ongoing
transportation activity sequences require a high de-
gree of adaptation to deal with unexpected problems
(Bassil et al., 2003). As an example, technical prob-
lems of vehicles, trafﬁc jams or forced rerouting may
appear at any time while V is on the road between
O and D. This usually leads to the interruption of the
“move V from O to D” activity. In such a situation,
a dynamic adaptation of an already planned ﬂow of
activities for the satisfaction of a customer request is
needed. This adaptation should take into account the
current context of the interrupted activity. The new
transportation solution may propose to send a new ve-
hicle V’ to the current position of V or to change the
already planned route leading to D. In both cases, the
current position of V should be available such that an
appropriate new solution can be proposed.
In this paper, we focus on interrupted (business)
process activities that require context preservation. In
most cases, activity interruption is triggered by the ap-
pearance of unexpected events coming from the appli-
cation environment (i.e., semantic failures). Preserv-
ing the context of an interrupted activity consists of
saving data, which are produced by or associated with
this activity. This must be done at the right time, e.g.,
as soon as the data become available or relevant.
149
C.-S. Chen et al. (Eds.), Enterprise Information Systems VII, 149–156.
© 2006 Springer.

At this point, it is important to have a closer look
at the granularity of work unit descriptions. Usually,
a business process consists of a set of activities each
of them dealing with a logical task (e.g., preparing
a patient for a surgery). In addition, such a process
activity can be further subdivided into atomic steps
corresponding to basic working units (e.g., measuring
weight/temperature of a patient as atomic steps of ac-
tivity “prepare patient”) or to data provision services.
Basic working units are either directly coded within
application programs or worked on manually by peo-
ple.
Distinguishing between activities and atomic
steps is useful for the following reasons: Atomic steps
are not managed within worklists like activities are.
This contributes to better system performance since
the costs for managing and updating worklists de-
crease. Furthermore, this approach offers more ﬂex-
ibility to users (if desired) since they can choose the
order in which they want to work on atomic steps. The
distinction between activities and atomic steps ﬁnally
leads to the following basic considerations.
It is very important in this context to distinguish
between a continuous and a discrete data update by
activities. The “move V from O to D” activity intro-
duced above is an example of an activity continuously
updating the “V current position” data element by a
GPS system. An example of an activity discretely up-
dating data is even more obvious in process-oriented
applications. We may think about the activity ”ﬁll in
a form” with many sections, each one asking for in-
formation (i.e., data) related to a speciﬁc topic. The
information becomes relevant, and therefore may be
kept in the system, only after the completion of a spe-
ciﬁc section. Filling in a section could be seen as
working on a particular atomic step.
We highlight the fact that a process activity may
apply both updating kinds: it may discretely update
a particular data element d1 and continuously up-
date another data element d2.
Moreover, data ele-
ments may be discretely updated by a speciﬁc activity
n1 and be continuously updated by another activity
n2. As an example, activity ”monitor patient” in a
medical treatment process, may ask to measure twice
a day the “patient temperature” and to continuously
control the “patient heart electric signals”. On the
other hand, the ”patient temperature” may be contin-
uously controlled in case of high fever within activity
“monitor patient” while it may be measured twice a
day after operation within activity “aftercare”.
Data continuously or discretely updated by activ-
ities may be only relevant for the speciﬁcally stud-
ied application (e.g., the vehicle “current position” in
Fig. 3) or they may be relevant for process execution
as well; in the latter case, these data are consumed by
process activities and therefore have to be supplied by
preceding activities. At the occurence of exceptional
situations, it may appear that mandatory process rel-
evant data will not be available at the time an activ-
ity is invoked. Depending on the application context
and the kind of data, it may be possible to provide
the missing data by data provision services which are
to be executed before the task associated with the re-
spective activity is handled.
We distinguish between exclusive application data
and process relevant data. Note that exclusive ap-
plication data may become process relevant when a
failure occurs. In the transportation application, an
example of process relevant data would be the “con-
tainer temperature” (continuously) measured during a
“move V from O to D” activity and relevant for a “Re-
port to customer” activity within the same process.
Reporting on the container temperature would inform
the customer whether the transported goods (e.g.,
foods) were or were not continuously preserved un-
der the appropriate temperature. The “V current posi-
tion” is an example of exclusive application data since
it is relevant for the application, in particular for the
optimisation module of the application (Bassil et al.,
2004), but not for the business process management
system. If, however, a road trafﬁc problem occurs, the
“current position” of V may become relevant for the
process as well; i.e., the origin location O’ of a newly
proposed activity “move V from O’ to D” changing
the already planned route leading to D, would corre-
spond to “current position” of V.
Figure 1 shows a data classiﬁcation scheme in the
context of business processes. This classiﬁcation puts
the frequency of updating activity data and the rele-
vance of these data into relation. Within these two
dimensions, we respectively differentiate between:
• continuously and discretely updated data, and
• exclusive application and process relevant data.
Discrete
Continuous
Data
Relevance
Data Update
Frequency
Application data
continuously
updated
Application data
written by
atomic steps
Exclusive application
Process data
continuously
updated
Process data
written by
atomic steps
Process
Taking into account this classiﬁcation, and know-
ing that exceptions stemming from the application
environment cannot be avoided and generally appear
during activity performance, it would be a challenge
not to loose available data already produced by the ac-
tivity that will be inevitably interrupted or deleted. In
order to formally specify the correctness criterion for
Sarita Bassil et al.
150
Figure 1: Data Classiﬁcation Scheme.

interrupting running activities while preserving their
context, formal deﬁnitions of requisite foundation for
this speciﬁcation are indispensable.
The remainder of this paper is organized as follows:
In Section 2 we deﬁne such foundation; we also dis-
cuss two application processes (a medical process and
a transportation process) with respect to the provided
deﬁnitions. Then, Section 3 introduces a general cor-
rectness criterion ensuring a safe interruption of a run-
ning activity. Section 4 discusses related work and
Section 5 concludes the paper.
2
FORMAL FRAMEWORK
To be able to precisely deﬁne the different kinds of
data and update frequencies we need a formal process
meta model.
In this paper, we use the established
formalism of Well–Structured Marking Nets (WSM
Nets) (Rinderle et al., 2004b) and extend it for our
purposes. Informally, a WSM Net is a serial–parallel,
attributed process graph describing control and data
ﬂow of a business process. More precisely, different
node and edge types are provided for modeling con-
trol structures like sequences, branchings, and loops.
A simple example is depicted in Fig. 2. Here, the
upper two lanes show the control and data ﬂow of a
(simpliﬁed) medical treatment process. For example,
activities “admit patient”, “inform patient”, and ”pre-
pare patient” are arranged in sequence whereas activ-
ities ”monitor” and “operate” are executed in parallel.
”Weight” and “temperature” are examples of process
relevant data elements involved in a data ﬂow between
the activities “prepare patient” and “operate”.
As motivated in the introduction an activity can be
subdivided into a set of atomic steps. Going back to
Fig. 2, the lower two lanes show the atomic steps
assigned to the process activities as well as the data
ﬂow between these steps. For example, the atomic
steps “measure weight”, “measure temperature”, and
“wash patient” are assigned to activity “prepare pa-
tient”. “Provide weight” is an example of a data pro-
vision service assigned to activity “operate” as atomic
step. If an exceptional situation (e.g., failure at the
“measure weight” atomic step level) occurs this data
provision service will be invoked in order to supply
input data element “weight” of the activity “operate”
(and particularly of its atomic step “anesthetize”). We
deﬁne a partial order relation on the set of atomic
steps (incl. data provision services) assigned to a cer-
tain activity. The precedence relation depicts a micro
control ﬂow between elements of this set. Note that,
by contrast, a macro control ﬂow is deﬁned between
activities. We set up this relation by assigning nu-
meric labels to atomic steps, e.g., an atomic step with
numeric label “1” is considered as a predecessor of
all atomic steps with numeric label “2” or greater. By
default, all atomic steps have number “1”, i.e., they
can be worked on in parallel. In this case, the actor
which works on the respective activity is considered
as being the expert in choosing the best order. Data
provision services have number “0” since they must
be executed before other atomic steps assigned to the
same activity, in order to properly supply these atomic
steps with the required input data.
So far WSM Nets have not considered splitting ac-
tivities into atomic steps. Therefore we extend the
formal deﬁnition from (Rinderle et al., 2004b) by in-
cluding this additional level of granularity. In the fol-
lowing, S describes a process schema.
Deﬁnition 1 (Extended WSM Net) A tuple S = (N,
D, NT, CtrlE, DataE, ST, P, Asn, Aso, DataEextended)
is called an extended WSM Net if the following holds:
• N is a set of activities and D is a set of process data
elements
• NT: N →{StartFlow, EndFlow, Activity, AndSplit,
AndJoin, XorSplit, XorJoin, StartLoop, EndLoop}
To each activity NT assigns a respective node type.
• CtrlE ⊂N × N is a precedence relation setting out
the order between activities.
• DataE ⊆N × D × NAccessMode is a set of
data links between activities and data elements
(with NAccessMode = {read, write, continuous-
read, continuous-write})
• ST is the total set of atomic steps deﬁned for all
activities of the process (with P ⊆ST describing
the set of data provision services)
• Asn: ST →N assigns to each atomic step a respec-
tive activity.
• Aso: ST →N assigns to each atomic step a num-
ber indicating in which order the atomic steps of a
certain activity are to be executed. By default: If
s ∈P, Aso(s) = 0 holds; otherwise, Aso(s) = 1.
• DataEextended ⊆ST × D × STAccessMode is a
set of data links between atomic steps and data el-
ements (with STAccessMode = {read, write})
As can be seen in the example from Fig. 2, there
are atomic steps which produce data (e.g., “measure
weight”) and others which do not write any data el-
ement (e.g., “wash patient”).
In order to express
this fact, we logically extend the set DataE to set
DataEextended which comprises all read/write data
links between atomic steps and data elements.
In
particular, an intra-activity data dependency may be
deﬁned such that intermediate results of an activity
execution can be passed between subsequent atomic
steps st1 and st2 with Asn(st1) = Asn(st2); i.e.,
∃(st1, d, write), (st2, d, read) ∈DataEextended.
As an example (Fig. 2), consider the intra-activity
data ﬂow from “anesthetize” to “operate” via data ele-
ment “sensory perception degree”. In fact, the atomic
Preserving the Context of Interrupted Business Process Activities 
151

weight
temperature
electro
cardiogram
consent
Inform
Sign
Measure
weight
Measure
temp.
Wash
patient
Anesthetize
Operate
Provide
weight
1
1
1
1
1
0
1
2
Atomic 
steps
Data on 
macro 
level 
sensory perception
degree
Admit
patient
Inform
patient
Operate
Monitor
Prepare
patient
Aftercare
Control flow 
AndSplit 
AndJoin 
Process Schema S:
 : Control flow
 : Data flow
weight
temperature
consent
Data on 
micro 
level 
sensory perception
degree
step “operate” needs this data element to decide when
to begin surgery.
Based on Def.
1, process instances can be cre-
ated and executed. As discussed in (Rinderle et al.,
2004b), during runtime a process instance references
the WSM Net it was created from. Its current ex-
ecution state is reﬂected by model–inherent activity
markings. An activity which can be worked on is thus
labeled Activated. As soon as activity execution is
started the marking changes to Running. Finally, a
ﬁnished activity is marked as Completed and an ac-
tivity, which belongs to a non-selected, alternative ex-
ecution branch, is marked as Skipped.
Deﬁnition 2 (Process Instance on Extended WSM
Net) A process instance I on an extended WSM Net S
is deﬁned by a tuple (S, M S
extended, ValS) where:
• S = (N, D, NT, CtrlE, . . .) denotes the extended
WSM Net I was derived from
• M S
extended = (NSS, STSS) describes activity and
atomic step markings of I:
NSS : N →{NotActivated, Activated, Running,
Completed, Skipped}
STSS : ST →{NotActivated, Activated,
Running, Completed, Skipped}
• V alS denotes a function on D. It reﬂects for each
data element d ∈D either its current value or the
value Undeﬁned (if d has not been written yet).
Markings of activities and atomic steps are corre-
lated. When an activity becomes activated, related
atomic steps (with lowest number) become activated
as well. The atomic steps will then be carried out ac-
cording to the deﬁned micro control ﬂow. As soon
as one of them is executed, both the state of this
atomic step and of its corresponding activity change
to Running. An activity is marked as Completed af-
ter completion of all corresponding atomic steps. Fi-
nally, if an activity is skipped during process execu-
tion, all related atomic steps will be skipped as well.
As motivated in the introduction, it is important
to distinguish between data elements only relevant in
context of application and data elements relevant for
process progress as well. We can see whether a data
element is relevant for the process if there is an activ-
ity reading this data element.
Deﬁnition 3 (Data Relevance) Let S be an extended
WSM Net, let w ∈{write, continuous-write} and r ∈
{read, continuous-read}. Then we denote d ∈D as
• an exclusive application data element if
∃(n, d, w) ∈DataE =⇏ ∃(m, d, r) ∈DataE
• a process relevant data element if
∃(n, d, w) ∈DataE =⇒
∃m ∈Succ∗(S, n) ∪{n}: (m, d, r) ∈DataE
Succ∗(S, n) denotes all direct and indirect succes-
sors of activity n.
The Data Relevance dimension captures both data
elements that are produced by the process, but are
only consumed by the application, and data elements
that are produced and consumed by the process. In
our medical treatment process (cf. Fig. 2), data ele-
ments “weight” and “temperature” taken during the
“prepare patient” activity are examples of process
relevant data elements. They are of utmost impor-
tance for carrying out the subsequent
”
operate” ac-
tivity (e.g., to calculate the quantity of anesthesia that
has to be administered to the patient). By contrast,
“consent” is an exclusively application relevant data
element. As explained in Section 1, when a failure
occurs, an exclusive application data element may
become relevant for the process as well. A patient
who already consented upon a surgery accepts the
risks, and the “consent” data element may thus be
used in subsequent activities dealing with respective
problems. Turning now to the container transporta-
tion process, “current position” is an exclusive appli-
cation data element whereas “container temperature”
is a process relevant data element (cf. Fig. 3).
Sarita Bassil et al.
152
Figure 2: Medical Treatment Process.

Data on 
macro 
level 
Control flow 
current position
container temperature
Attach at
P
Move to
O
Load at
O
Move to
D
Unload
at D 
Move to P
Report to 
customer
…
Process Schema S:
  : Control flow
  : Data flow
We now deﬁne the notion of data update frequency.
Based on this notion we will be able to deﬁne a cri-
terion for safely interrupting running activities while
preserving their context.
Intuitively, for a discrete
data update by atomic steps there are certain peri-
ods of time between the single updates, whereas for
continuous data updates by activities the time slices
between the single updates converge to 0. For deﬁn-
ing the time slices between data updates, we need the
function stp : ST →R ∪{Undefined} which maps
each atomic step of ST either to a speciﬁc point in
time or to Undefined. In detail:
stp(st) :=

tst if ∃(st, d, write) ∈DataEextended
Undefined otherwise
whereby tst :=

completion time of st
∞by default
Note that the inﬁnite default value we assign to tst
is updated as soon as st is completed. Hence, the real
completion time of st is assigned to tst.
Deﬁnition 4 (Data Update Frequency) Let S be an
extended WSM Net, let w ∈{write, continuous-
write} ⊂NAccessMode, and let d ∈D, n ∈N with
(n, d, w) ∈DataE. Let further ST d
n be the set of
atomic steps associated with activity n and writing
data element d; i.e., ST d
n := {st |asn(st) = n,
∃(st, d, write) ∈DataEextended}.
Then we denote (d, n) as:
• A discrete data update of d by n if
∃(n, d, write) ∈DataE
In terms of atomic steps:
∀st ∈ST d
n: stp(st) = tst ̸= Undeﬁned
• A continuous data update of d by n if
∃(n, d, continuous-write) ∈DataE
In terms of atomic steps: ST d
n = ∅
In case an activity n continuously updates a data el-
ement d no atomic steps writing d are dissociated, i.e.,
there are no atomic steps associated with n that write
d; e.g., take the absence of atomic steps writing the
“current position”, the “container temperature”, and
the “electro cardiogram” in Figures 2 and 3. These
data elements are examples of data continuously up-
dated respectively by a GPS system, a thermometer,
and a cardiograph instrument.
On the other hand, the set of atomic steps discretely
writing a data element may be limited to only one
atomic step. The “consent”, the “weight”, and the
“temperature” are written once respectively by the
“sign”, the “measure weight” and the “measure tem-
perature” atomic steps (cf. Fig. 3).
Fig. 4 summarizes the classiﬁcation of the data in-
volved in the medical treatment and in the container
transportation process, taking into account the general
data classiﬁcation scheme presented in Fig. 1.
Discrete
Continuous
Data
Relevance
Data Update 
Frequency
Exclusive application
- Container current 
position
- Patient consent 
- Patient weight
- Patient temp.
- Sensory perception 
degree
- Container temp. 
- Patient electro
cardiogram
Process
Figure 4:
3
CORRECTNESS CRITERION
In order to correctly deal with exceptional situations,
it is crucial to know those points in time when running
activities can be safely interrupted. A running activity
is safely interrupted means that the context of this ac-
tivity is kept (1) such that all input data of subsequent
activities are correctly supplied, or (2) in order to ﬁnd
possible solutions for exceptional situations. We de-
note these certain points in time as safe points of the
respective activities.
The challenging question is how to determine the
safe point of an activity. In order to adequately answer
Preserving the Context of Interrupted Business Process Activities 
153
Figure 3: Container Transportation Process.
Container Transportation Processes.
Data Classiﬁcation for the Medical Treatment/

this question, our distinction between continuous and
discrete data update is helpful. As the following deﬁ-
nitions show, it is possible to precisely determine the
particular safe interrupt points for discrete and con-
tinuous data updates, i.e., those points in time when
the respective data are updated such that subsequent
activities reading these data are correctly supplied.
Deﬁnition 5 (Safe Interrupt Point for a Discrete
Data Update) Let (d, n) (n ∈N, d ∈D) be a dis-
crete data update of d by n, and let ST d
n be the set
of atomic steps associated with n and writing d. Let
further B := {stp(st), st ∈ST d
n | ̸ ∃p ∈P: Asn(p) =
n and (p, d, write) ∈DataEextended}. Then the safe
interrupt point td
safe of (d, n) corresponds to the max-
imum point in time any atomic step writes d (on con-
dition that d cannot be provided by a data provision
service). Formally:
td
safe :=

max(B)
:
B ̸= ∅
Undefined
:
otherwise
Informally, the safe interrupt point for a discrete
data update by atomic steps is that maximum point in
time when the last write access to the respective data
element has taken place.
Deﬁnition 6 (Safe Interrupt Point for a Continu-
ous Data Update) Let (d, n) (n ∈N, d ∈D) be a
continuous data update of d by n with a start updat-
ing time t1 and a ﬁnish updating time tk. The safe
interrupt point td
safe of (d, n) (t1 < td
safe < tk)
corresponds to the time when d becomes relevant
for subsequent activities.
This time is ﬁxed by the
user. If no safe interrupt point is ﬁxed by the user
td
safe := Undeﬁned holds.
Intuitively, for continuous data updates there is no
“natural” safe interrupt point. Therefore, we offer the
possibility to deﬁne a safe interrupt point by the user.
An example usage for such a user-deﬁned safe inter-
rupt point would be the “waiting time” in order to get
the right container temperature after attaching it to
the vehicle that shall power the refrigeration system
within the container.
In order to determine the safe point of an activity,
we have to consider that there might be several safe
interrupt points. One example is the activity “prepare
patient” which has two safe interrupt points belonging
to data elements “weight” and
”
temperature” (Fig. 2).
Deﬁnition 7 (Activity Safe Point) Let {d1, . . . , dk}
be the set of data elements (continously) written by
activity n ∈N (i.e., ∃(n, di, w) ∈DataE, i = 1,
. . . , k, w ∈{write, continuous−write}). Let further
td1
safe, . . . , tdk
safe be the related safe interrupt points.
Then we denote tsafe = max{td1
safe, . . . , tdk
safe} as the
safe point of n (if
tdi
safe = Undeﬁned ∀i = 1, . . . ,
k, tsafe is set to Undeﬁned as well). Thereby, tsafe
corresponds to the time when n can be safely inter-
rupted keeping its context. An activity n can be safely
interrupted if all input data of subsequent activities of
n are provided.
Using the notion of activity safe point we can state
a criterion based on which it is possible to decide
whether a running activity can be safely interrupted
or not.
Criterion 1 (Interrupting a Running Activity by
Keeping its Context) Let S be an extended WSM
Net, let I be an instance on S, and let w ∈{write,
continuous-write} ⊂NAccessMode. A node n ∈N
with NSS(n) = Running and safe point tsafe can be
safely interrupted at tinterrupt if one of the following
conditions holds:
• ̸ ∃(n, d, w) ∈DataE
• tsafe <= tinterrupt or tsafe = Undeﬁned
• ∀(n, d, w) ∈DataE, tinterrupt < td
safe:
d is an exclusive application data element
A running activity can be safely interrupted from a
process perspective if it either writes no data or if it
solely writes exclusive application data. If a running
activity writes process relevant data it can be safely
interrupted if it has an undeﬁned safe point or its safe
point has been already transgressed. Finally, if exclu-
sive application data become process relevant (e.g., if
an exception handling process makes use of the full
context of the interrupted activity), the last condition
of Criterion 1 may not be applicable.
In order to illustrate the deﬁned correctness
criterion, we consider the container transportation
process. Based on process schema S provided in Fig.
3, instance IS in Fig. 5 has been started. Taking into
account a deﬁned transportation network, each of the
activities’ locations in IS is captured by a coordinate
(x, y). E.g., the origin and the destination locations in
activity “move vehicle V from Montr´eal to Qu´ebec”
would respectively correspond to the coordinates
(1.5, 3.5) and (13, 8) within the transportation
network.
Suppose that a road trafﬁc problem
occurs at time tinterrupt = tn
begin + 75minutes
(elapsed time since departure) while V is on the
road between Montr´eal and Qu´ebec.
At this time,
suppose that the GPS system is indicating (7, 5.5)
for the current position of V. To avoid the trafﬁc
problem, an optimisation module may propose a new
transportation solution that consists of changing the
already planned route leading to Qu´ebec. The new
route includes a detour via another location, that is
Trois-Rivi`eres located at position (7, 7). However,
this new solution is only possible if V is close enough
to Trois-Rivi`eres, which means that the current
position of V is beyond (6, 5).
This correponds
to t”CurrentP osition”
safe
= tn
begin + 60minutes.
In
addition, suppose that the right container temperature
Sarita Bassil et al.
154

is reached 15minutes after ﬁnishing loading the
container and hence after the departure from the
origin location,
i.e.,
t
”
ContainerT emperature”
safe
=
tn
begin + 15minutes.
Taking into account Def.
7,
the
safe
point
of
activity
“move
vehicle
V
from
Montr´eal
to
Qu´ebec”
corresponds
to
max{t
”
CurrentP osition”
safe
, t
”
ContainerT emperature”
safe
}
< tinterrupt.
Hence, this activity can be safely
interrupted. The exclusive application data element
“current position” was used to generate the new
solution shown in Fig. 5. Following the road trafﬁc
problem, this data element becomes process relevant
as well: it is given as input to the inserted activity
“move vehicle V from current location to Trois-
Rivi`eres”.
Note that in this speciﬁc example, the
“container temperature” data element is not relevant
for the deﬁnition of the safe point, and hence it could
be ﬁxed to Undefined.
4
DISCUSSION
In this paper a “divide and conquer” approach is
adopted: An activity is divided into atomic steps so
that the interruption of this activity becomes possible
by preserving its context.
In (Sadiq et al., 2001; Mangan and Sadiq, 2002)
“pockets of ﬂexibility” are deﬁned. So called “con-
tainers” comprise different activities and constraints
posed on these activities (e.g., activity B always be-
fore activity C). These containers can be inserted into
certain regions within the process. If process execu-
tion reaches such a container the assigned user can
choose the order of working on the offered activities
by obeying the imposed constraints. This idea can
be compared to our approach of subdividing activi-
ties into atomic steps and posing an order relation on
them if necessary. However, both approaches use a
different level of granularity and focus on different
aims. The approach presented by (Sadiq et al., 2001)
provides more ﬂexibility regarding process modeling
whereas our approach uses atomic steps for being able
to preserve the data context in case of unexpected
events during runtime.
The two kinds of data addressed by the Data Rel-
evance dimension of our data classiﬁcation scheme
have already been discussed within the literature
(Workﬂow Management Coalition, 1999; v.d. Aalst
and van Hee, 2002).
In (Workﬂow Management
Coalition, 1999), a differentiation is made between
application data and process relevant data. It is ar-
gued that application data may become process rele-
vant if they are used by the workﬂow system to deter-
mine a state change. In this paper, we adopt the same
deﬁnitions and interpretations as provided in (Work-
ﬂow Management Coalition, 1999); furthermore, we
judiciously highlight the fact that exclusive applica-
tion data may become process relevant when a fail-
ure occurs.
In (v.d.
Aalst and van Hee, 2002), a
bigger variety of process data is featured: analysis
data, operational management data, historical data,
etc. It is stated that application data cannot be di-
rectly accessed by a workﬂow system but only in-
directly through instance attributes and applications
themselves. Hence, only the way of accessing appli-
cation data from a WfMS is discussed.
Preserving the Context of Interrupted Business Process Activities 
155
Figure 5: Container Transportation Scenario.
The inﬁnite completion time assigned as a default

value to an atomic step st may be more precisely pre-
dicted using, for instance, the forward/backward cal-
culation technique based on the duration of activities
as proposed in (Eder and Pichler, 2002; Eder et al.,
2003). This would allow estimating an activity safe
point (tsafe) as a speciﬁc point in time (instead of in-
ﬁnite) even before reaching this point.
Another interesting application of the presented re-
sults arises in the context of process schema evolution
(Rinderle et al., 2004a) i.e., process schema changes
and their propagation to running process instances.
One important challenge in this context is to ﬁnd cor-
rectness criteria in order to ensure correct process
instance migration after a process schema change.
According to the compliance criterion (Casati et al.,
1998; Rinderle et al., 2004a) it is forbidden to skip al-
ready running activities, i.e., the respective process in-
stances are considered as being non–compliant. How-
ever, if we transfer the concepts of safe interruption
of activities to the safe deletion of activities the num-
ber of process instances compliant with the changed
process schema can be increased.
5
SUMMARY AND OUTLOOK
We have proposed a framework to correctly
tions; i.e., interrupting running activities by preserv-
ing their data context, which is extremely important in
order to be able to provide adequate solutions in the
sequel. This work was motivated by the analysis of
data involved in the context of speciﬁc complex, yet
the container transportation application and the med-
ical application. Besides modeling logical work units
as process activities we have introduced another level
of granularity by deﬁning the atomic step concept.
The latter is of utmost importance to build up the ba-
sis for a two-dimensional data classiﬁcation scheme.
On the one hand, the deﬁnition of the data relevance
dimension, distinguishing between exclusive applica-
tion data and process relevant data, is considered at its
pure level within the safely interruption criterion con-
ditions statement. On the other hand, we dug deeper
regarding the data update frequency dimension by
deﬁning safe interrupt points for each of the discrete
and the continuous data update by activities. This has
led to the formal deﬁnition of the activity safe point
considered as the backbone for the safely interruption
criterion. Preserving this criterion, in turn, guarantees
that if an activity is safely interrupted all necessary
data is kept and can be used to ﬁgure out an adequate
solution for the respective exceptional situation.
As future work, we aim to study extended transac-
tional issues (e.g., semantic rollback) at both the mi-
cro ﬂow and the macro ﬂow level. In particular, this
must be done in a way that enables ﬂexible exception
handling procedures (incl. dynamic ﬂow changes).
Respective facilities are indispensable for realizing
adaptive enterprise applications.
REFERENCES
Bassil, S., Bourbeau, B., Keller, R., and Kropf, P. (2003).
A dynamic approach to multi-transfer container man-
agement. In Proc. Int’l Works. ODYSSEUS’03, Sicily.
Bassil, S., Keller, R., and Kropf, P. (2004). A workﬂow–
oriented system architecture for the management of
container transportation. In Proc. Int’l Conf. BPM’04,
pages 116–131, Potsdam.
Casati, F., Ceri, S., Pernici, B., and Pozzi, G. (1998). Work-
ﬂow evolution. DKE, 24(3):211–238.
Eder, J. and Pichler, H. (2002). Duration histograms for
workﬂow systems.
In Proc. Conf. EISIC’02, pages
25–27, Kanazawa, Japan.
Eder, J., Pichler, H., Gruber, W., and Ninaus, M. (2003).
Personal schedules for workﬂow systems.
In Proc.
Int’l Conf. BPM’03, pages 216–231, Eindhoven.
Gartner Group (1999).
Why e-business craves workﬂow
technology. Technical Report T-09-4929.
Mangan, P. and Sadiq, S. (2002). A constraint speciﬁcation
approach to building ﬂexible workﬂows. Journal of
Research and Practice in Inf Tech, 35(1):21–39.
Reichert, M., Dadam, P., and Bauer, T. (2003).
Dealing
with forward and backward jumps in workﬂow man-
agement systems. Int’l Journal SOSYM, 2(1):37–58.
Rinderle, S., Reichert, M., and Dadam, P. (2004a). Correct-
ness criteria for dynamic changes in workﬂow systems
– a survey. DKE, 50(1):9–34.
Rinderle, S., Reichert, M., and Dadam, P. (2004b). On deal-
ing with structural conﬂicts between process type and
instance changes. In Proc. Int’l Conf. BPM’04, pages
274–289, Potsdam.
Sadiq, S., Sadiq, W., and Orlowska, M. (2001). Pockets of
ﬂexibility in workﬂow speciﬁcations. In Proc. ER’01
Conf., pages 513–526, Yokohama.
v.d. Aalst, W. and van Hee, K. (2002). Workﬂow Manage-
ment. MIT Press.
Workﬂow Management Coalition (1999). Terminology &
glossary. Technical Report WFMC-TC-1011, WfMC.
address
the issue of safely interrupting running business
representative, process-oriented applications, namely
process activities in case of exceptional situa-
Sarita Bassil et al.
156

THE “RIGHT TO BE LET ALONE” AND PRIVATE 
INFORMATION
Sabah S. Al-Fedaghi 
Computer Engineering Department, Kuwait University, Kuwait 
Email: sabah@eng.kuniv.edu.kw 
Keywords: 
Privacy, right to be let alone, information, law. 
Abstract: 
The definition of privacy given by Warren and Brandeis as the “right to be let alone” is described as the 
most comprehensive of rights and the right most valued by civilized men. Nevertheless, the formulation of 
privacy as the right to be let alone has been criticized as “broad” and “vague” conception of privacy. In this 
paper we show that the concept of “right to let alone” is an extraordinary, multifaceted notion that coalesces 
practical and idealistic features of privacy. It embeds three types of privacy depending on their associated: 
active, passive and active/passive activities. Active privacy is “freedom-to” claim where the individual is an 
active agent when dealing with private affairs claiming he/she has the right to control the “extendibility of 
others’ involvement” in these affairs without interference. This is a right/contractual-based notion of 
privacy. Accordingly, Justice Rehnquist declaration of no privacy interest in a political rally refers to active 
privacy. Passive privacy is “freedom-from” notion where the individual is a passive agent when dealing 
with his/her private affairs and he/she has privacy not due control –as in active privacy– but through others 
being letting him/her alone. This privacy has duty/moral implications. In this sense Warren and Brandeis 
advocated that even truthful reporting leads to “a lowering of social standards and morality.” Active/passive 
privacy is when the individual is the actor and the one acted on. These three-netted interpretations of the 
“right to be alone” encompass most –if not all- definitions of privacy and give the concept narrowness and 
precision.
1 INTRODUCTION
The notion of privacy is becoming an important 
feature in all aspects of life of modern society. 
“[P]rivacy will be to the information economy of the 
next century what consumer protection and 
environmental concerns have been to the industrial 
society of the 20th century” (Gleick, 1996). New 
technologies worldwide have affected different 
aspects of dealing with private information in the 
areas of security, commerce, government, etc. The 
situation is described as “clash between privacy and 
technology” (DeCew, 2002). 
Several types of privacy have been distinguished 
in the literature including physical privacy and 
informational privacy (Floridi, 1998). Recent results 
have located ‘private information’ in true linguistic 
assertions about an identifiable individual. An 
ontological definition of private information   can   
be   developed    from    linguistic assertions in order 
to identify the basic units of private information. 
This definition will be briefly summarized next. The 
linguistic forms of information or linguistic 
assertions provide us with the basic privacy 
components. Simply, assertions about individuals 
are private assertions. Consequently, linguistic 
assertions are categorized according to the number 
of their referents as follows: 
(i) Zero assertion: An assertion that has no 
referent signifying a single individual.  
(ii) Atomic assertion: An assertion that has a 
single referent signifying a single individual.  
(iii) Compound assertion: An assertion that has 
several referents signifying individuals. 
In (i) and (ii), the referent refers to an individual 
(person), but not to a specific individual. He is shy,
Someone is in love are examples of atomic assertions 
because each has one (human) referent. They 
become private when “he” and “someone” are 
mapped to identifiable individual. On the other, hand 
Spare part ax123 is in store 5, is a zero assertion 
because it does not involve any individual (human). 
They are in love is a compound assertion because it 
has two referents.           
Linguistic assertions that are limited in their 
possible extension to human beings become 
157
C.-S. Chen et al. (Eds.), Enterprise Information Systems VII, 157–166.
© 2006 Springer.

‘private’ assertions when they are ‘coupled’ to 
specific individuals through
the mechanism of 
identification. For example, The Swedish Data Act 
“regards every single storage of any piece of 
information about a person as a personal information 
base” (Palme, 1998). If an assertion is true, then it is 
said to be information, otherwise it is said to be 
misinformation. Consequently, there are zero 
information, atomic information, and compound 
information according to the number of referents. 
Atomic information becomes private if it refers to an 
identifiable 
individual. 
Similarly, 
compound 
information becomes private if it refers to 
identifiable individuals. 
We identify the relationship between individuals 
and their own atomic private information through 
the notion of proprietorship. Proprietorship of 
private information is different from the concepts of 
possession, ownership, and copyrighting. Any 
atomic private information of an individual is 
proprietary private information of its proprietor. A 
proprietor of private information may or may not be 
its possessor and vice versa. Compound private 
information is proprietary information of its 
referents: all donors of pieces of atomic private 
information that are embedded in the compound 
private information. Atomic private information of 
an individual can be embedded in compound private 
information: a combination of pieces of atomic 
private information of several
individuals. The 
compound assertion is produced through compound 
activities or situations where individuals ‘discover’ 
each other. Two or more individuals have the same 
piece of compound private information because it 
embeds atomic private information from these 
individuals. But it is not possible that they have 
identical atomic private information because simply 
they have different identities. Atomic private 
information is the “source” of privacy. Compound 
private information is “private” because it embeds 
atomic private information. Also, the concept of 
proprietorship is applied to compound private 
information, 
which 
represents 
“sharing 
of 
proprietorship” but not necessarily shared possession 
or ‘knowing’. Some or all proprietors of compound 
private information may not “know” it.  
Compound private information is privacy-
reducible to a set of atomic assertions but it has 
more than that. It is a “bind” that does not only 
contain atomic assertions but also asserts something 
about its own assertions. Privacy-reducibility of 
compound information to atomic information means 
that “no known atomic information” of an individual 
implies “no known compound information” of that 
individual. Because if the compound information is 
known then its atomic assertions are known. 
Reducing a compound assertion to a set of atomic 
assertions refers to isolating the privacy aspects of 
the compound assertion. This means that, if we 
remove the atomic assertion concerning a certain 
individual, from the compound assertion then the 
remaining part will not be a privacy-related assertion 
with respect to the individual involved.  
Suppose we have the compound private 
information, John saw Mary’s uncle, Jim. The 
privacy-reducibility process produces the following 
three atomic private assertions:  
Assertion-1: John saw someone’s uncle. 
Assertion-2: Mary has an uncle. 
Assertion-3: Jim is an uncle of someone. 
Additionally, we can introduce the zero-
information meta-assertion: Assetion-1, assertion-2, 
and assertion-3 are assertions of one compound 
private information, from which it is possible to 
reconstruct the original compound assertion. The 
methodology of syntactical construction is not of 
central concern here. 
We have defined every piece of information that 
includes 
an 
identifiable 
person 
as 
private 
information. The private information can be 
sensitive, confidential, ordinary, trivial, etc. but all 
of these types are encompassed by the given 
definition: they refer to identifiable individuals. It 
seems that privacy “should come, in law as in life, 
too much less ... [than] all information about 
oneself” (Gerety, 1977). Here we can introduce the 
notion of ‘sensitive’ private information. However, 
while identifiably is a strict measure of what is 
private information, ‘sensitivity’ is a notion that is 
hard to pin down. It is “context dependent and thus 
global measures of sensitivity cannot be adopted” 
(Fule and Roddick, 2004). In this work we defer 
different levels of sentivity of private information to 
further study. 
Three categories can be applied to any type of 
activities and situations. For private information, we 
apply them to the act of processing private 
information. Suppose that we define each individual 
as a private information agent capable of processing 
his/her own private information. The individuals can 
process (e.g., control uses of) his/her private 
information in dealing with others (active role); can 
process his/her private information autonomously 
(passive-active role); or can be the source of private 
information processed by others (passive role).  
Accordingly, we categorize proprietary private 
information into two basic categories:  
Known: This is the set of atomic private
information that is known by others (in possession 
of others). In this category the individual can be an 
active processor of his/her information through, for 
example, releasing it to some but not others, selling 
it, controlling its uses, etc. Alternatively, the 
individual can be a passive agent where others 
Sabah S. Al-Fedaghi 
158

process his/her private information without his/her 
involvement. 
Not Known: This is the set of atomic private 
information that is only known by the proprietor and 
no one else. Here the individual can be an 
active/passive 
agent 
processing 
autonomously 
his/her own private information. Privacy intrusion 
can occur in this domain by such techniques as brain 
washing, 
forced 
propaganda, 
subliminal 
advertisements, etc. 
Next section introduces our new contribution in 
this paper. We review the notion of “right to be let 
alone” in preparation for generalizing private 
information to private affairs. We will use the 
resultant 
divisions 
of 
passive, 
active, 
and 
active/passive 
activities 
to 
develop 
three 
interpretations of the right to be let alone. 
2 THE RIGHT TO BE LET ALONE 
Samuel Warren and Louis Brandeis 1890 famous 
article (Warren and Brandeis, 1890) is described as 
“the most influential law review article ever 
published" (Etzioni, 1999). “[I]t spawned at least 
four common law tort actions to protect privacy; and 
it framed the discussion of privacy in the United 
States throughout the twentieth century” (Solove, 
2002). Warren and Brandeis described the notion of 
privacy as the right to be let alone. On the authority 
of Justice Louis Brandeis “the right to be let alone is 
the most comprehensive of rights and the right most 
valued by civilized men” (Brandeis, 1928).               
Originally, describing privacy as the right to be 
let alone came out of concern mainly with 
expanding 
communication 
and 
new 
media 
technologies in the United States in 19th century 
(e.g., instant photographs and the widely distributed 
tabloid press). Those technologies were invading 
"private and domestic life" through publishing idle 
gossip 
in 
sensational 
news 
(Ruiz, 
1997). 
Accordingly, it is claimed that the aim of privacy 
laws should be to protect “the privacy of private 
life” from unwanted publication of information 
about the private lives of individuals. According to 
Warren and Brandeis “... modern enterprise and 
invention have, through invasions upon his [man] 
privacy, subjected him to mental pain and distress, 
far greater than could be inflicted by mere bodily 
injury” (Warren and Brandeis, 1890). Warren and 
Brandeis noted that tort law did not typically protect 
privacy-based harm. While the law of defamation 
protected injuries to reputations, privacy-based harm 
is  “injury to the feelings” “that was difficult to 
translate into the tort law of their times, which 
focused more on tangible injuries” (Solove, 2002).
The formulation of privacy as the right to be let 
alone has been criticized for failing in providing a 
balanced view concerning other important notions, 
such as free speech, effective law enforcement 
(Solove, 2002). It is described as ‘too broad’ and 
“never define what privacy is” (Schoeman, 1984). 
“While the right to be let alone has often been 
invoked by judges and commentators, it still remains 
a rather broad and vague conception of privacy” 
(Solove, 2002). 
3 “TO BE LET ALONE” AS 
INFORMATIONAL PRIVACY  
In order to include all private affairs implied by the 
notion of “to be let alone” we propose three spheres 
of privacy as shown in figure 1. 
                          My Privacy                            
                                                                               
                                                                            
                                                                        
Active                    Passive         Active/Passive  
Figure 1: Categories of privacy-related activities of any 
The categories represent all privacy-related activities 
of the individual. We will concern here with “self” 
private affairs of the individual represented in My 
Privacy. “Self” here refers to proprietary private 
activities as in the informational context where 
atomic 
private 
information 
is 
proprietary 
information of the individual. Next we will explain 
this notion of “self” privacy-related activities in 
terms of “self” private information. 
4 ANATOMY OF PRIVATE 
INFORMATION  
Let p(V) be an atomic private assertion about the 
individual V. A private assertion ‘about’ V is an 
assertion whose subject (agent) is V or his/her 
‘things’. For example, the subject of the assertion 
John’s horse is brown is not John himself but his 
horse; however, the subject of John’s blood type is 
A, is John himself. This later type of assertions 
informs about the individuals themselves not about 
things in this world that are associated with them. 
The atomic private information John’s horse is 
brown is ‘infected’ by zero-information. Atomic 
The “Right to be Let Alone” and Private Information 
159
individual.

private information is said to be self-atomic private 
information (self-information) if its subject is its 
proprietor and only its proprietor. For example, 
John’s house is burning expresses two pieces of 
information: (a) John has a house, and (b) The house 
is burning. Assertion (a) is self-information because 
its subject is its proprietor. The assertion (b) is zero-
information because its subject is not a person but a 
house. The term ‘subject’ here means the ‘thing’ 
about which the information is communicated.  
Also, if the police inquired about my dog’s 
certificate or examined the soil of my land then these 
are ‘personally owned information’ not self private 
information about me. The only privacy-related 
information in these cases is that I own the dog or 
the land. However, if the inquiries are about my 
identity 
(e.g., 
the 
police 
asks 
to 
produce 
identification as an owner of the dog, alcohol level 
in my blood, my beliefs, etc.) then this information 
is in my self-private domain. So when we use terms 
such as ‘private affairs’ we mean “self affairs” that 
are proprietary of the individual not ‘personally 
owned affairs’ such as pieces of information (or 
affairs) about a company or a job, etc. that are in 
possession of or related to the individual.  
Furthermore, we concern ourselves here with 
information/affairs in My Privacy that includes the 
individual’s private information/affairs and not 
private information/affairs of others that are in 
his/her possession. This My Privacy can be divided 
according to the kinds of “freedom” available to the 
individual as follows: 
Active Privacy: This is the type of privacy 
where the individual is an active agent when dealing 
with his/her private affairs claiming he/she has the 
right to control the “extendibility of others’ 
involvement” in these affairs without interference. 
So “right to be let alone” means freedom “to” 
control private affairs that involve others. The 
difficulties here stem from conflict between the 
individual and others over who controls what private 
information about the individual and the extent of 
this control.
Passive Privacy: This is the type of privacy 
where the individual is a passive agent when dealing 
with his/her private affairs and he/she has privacy 
that is not due his control –as in active privacy– but 
through others being letting him/her alone. So  “right 
to be let alone” means freedom “from” being 
subjected to others’ activities with regard to private 
affairs. The difficulties here stem from the intrusion 
by others on what the individual consider his/her 
privacy arena.  
Active/Passive Privacy: This is the type of 
privacy where the individual is simultaneously 
active and passive agent when dealing with his/her 
private affairs. So “right to let alone” means freedom 
“in” doing whatever he/she wants with his/herself 
private affairs that do not involve others. The 
difficulties here stem from others preventing the 
individual from acting on his/herself. Consider the 
following examples: 
(a) The individual has an active role: presenting 
oneself in “false light”, giving his/her private 
information, filing an application for government 
benefits, flashing. 
(b) The individual has a passive role: unsolicited 
mail and unwanted phone calls, commercial 
exploitation of his/her private information, filing a 
compulsory government form, compulsory stripping 
and exposure.  
(c) The individual has active/passive role: 
tattooing oneself, burning own dairy, masturbation, 
suicide.         
5   ACTIVE PRIVACY: “TO BE LET 
ALONE” AS “FREEDOM TO” 
‘To be let alone’ can be interpreted as a type of 
freedom qualified, usually, with “to”. It is freedom 
“to associate”, “to distribute”, “to express”, “to 
possess”, “to receive information and ideas”, “to 
educate (children)”, “to reveal beliefs, thoughts, 
possessions”, “to communicate with others”, etc. 
The “material“ of this activity is one’s own private 
affairs such as his/her own private information; the 
practice of the activities is “expending” private 
affairs in the social exchange with others; and the 
target of the activities is to maximize social-life 
benefits with minimum expenditure of privacy. This 
active freedom -whenever used with the term 
“from”- reflects restricting individual’s activity as in 
freedom “from interference”, “from unwarranted 
governmental intrusion”, “from obstacles”, etc. 
Privacy here means “the ability to control” private 
affairs hence it is termed in the informational 
context as “control” over information. It also means 
“freedom of choice” in how to expend private 
affairs. 
With respect to private information active 
freedom is manifested in: 
 (a) The individual transfers his/her private 
information from Not Known to Known.
 (b) The individual controls the distribution of 
private information in possession of subsets of 
others in Known.
This type of privacy entangles directly with the 
troublesome concept of the limitation of ‘personal 
freedom’.  In its absolute sense, ‘I have the right to 
be let alone’ means ‘I have the right that: (a) others 
do not pay attention to my private affairs whatever 
these private affairs are, and (b) I control the limits 
Sabah S. Al-Fedaghi 
160

not refer to “control over when and by whom the 
various parts of us can be sensed by others” (Parker, 
1974) but to control what we ourselves put it under 
different levels of spotlight for others. In ordinary 
situations “various parts” of us that are sensed by 
others do not attract attention, but attention arises 
when we become active in some ‘unordinary’ way. 
On the other hand passivity in passive privacy does 
not embed any control notion.   
5.1 “Natural” State of Privacy 
There are reservations concerning the type of affairs 
and the control of limits in active privacy. First, the 
freedom in active privacy is at odd with the ‘natural’ 
state of privacy. The default case of privacy is that 
others ‘pay attention and simultaneously do not pay 
attention’ to people. Here privacy and non-privacy 
are inseparably intertwined since active freedom by 
its nature ‘attracts attention’. Attracting attention has 
many obvious facets and also non-obvious aspects. 
There is a minimum and ‘usual’ levels of attention 
arise from living in any community. Then come such 
aspects as becoming “quasi-public figures" or 
“participants, 
even 
though 
unwilling, 
in 
a 
newsworthy 
event”, 
“a 
subject 
in 
public 
documentation (e.g., official court records)”, etc. 
that attract attention by their very nature. Many legal 
cases have been understood according to this factor. 
For example, a case against publishing a photograph 
of a couple embracing in public is dismissed on the 
grounds that by embracing in public, the couple had 
voluntarily consented to be viewed by others 
(Prosser, 1984). According to Charles Fried, 
“Privacy is not simply an absence of information 
about us in the minds of others; rather it is the 
control we have over information about ourselves” 
(Fried, 1970). 
Hence 
active 
privacy/freedom 
where 
an 
individual tries “to control” the consequences of 
attention would results in more attention. For 
example in what is called “privacy-based harm”, the 
more 
the 
victim 
express 
dissatisfaction 
or 
resentment for his/her injury the more he/she is 
victimized. On the other hand there are activities (of 
passive privacy) that are clearly ‘non-attracting 
attention’ activities such as “using contraceptives in 
the 
privacy 
of 
marriage”, 
“making 
phone 
conversation”, etc. In these activities the individual 
is a passive agent who aims at freedom “from” 
intrusion in his/her privacy. 
fair” and exercising control over others’ dealings 
with own private affairs. Privacy in this case is "the 
right to exercise some measure of control over 
information about oneself" (Westin, 1967). “Control 
over information” means that “individuals have the 
right to decide "when, how, and to what extent 
information about them is communicated to others" 
(Westin, 1967). In this case privacy is the “right to 
decide how much knowledge of [a person’s] 
personal thought and feeling . . . private doings and 
affairs . . . the public at large shall have” (Godkin, 
1980).  Clearly our analysis generalizes this notion 
to the “right to control over others attention about 
one’s private activity.” This type of privacy is the 
so-called ‘limiting access to the Self’ conception of 
privacy, which embraces the demand for “freedom 
from government interference as well as from 
intrusions by the press and others” (Solove, 2002) 
without “constraining” private activities of the 
individual. Freedom here means exercising activities 
and controlling attentions about these activities. As 
we mentioned previously this activity is, by itself, 
naturally 
information-producing 
and 
attention-
attractive activity. One feels “free” when he/she 
does ‘naturally attention-attracting activity’ and be -
at the same time- in charge of the amount and type 
of the attention of others. He/she desires to be in 
control of how others pay notice to him/her. The 
meaning of freedom is ‘controlling others’ as much 
as controlling own activity. 
Privacy based on active privacy is the type of 
privacy that seems to conceal domination over 
others. “Right” in this context has its base in 
Thomas Hobbes’ right of nature. This type of 
privacy makes people players in the control game, 
free in selecting plays that they desire. Each person 
can cling to or display some or all of his/her private 
affairs. “It's a game for people who choose a form of 
existence impossible in the old world, maybe 
hermits at that, hiding in digitally equipped homes, 
visiting by telecam” (Gleick, 1996). This active 
freedom is ‘social’ freedom that involves using 
various types of measures of shielding (e.g., 
isolation, solitude, hiding, concealment, etc.) and 
various forms of exposing (straight publicity, 
leaking information, ... flashing). “Privacy” here 
may mean freedom to expose “everything” and 
controlling what part should be publicized. A flasher 
may object to announcing his/her name, a witness of 
5.2 Active Freedom 
Active freedom associated with active privacy refers 
to being an active entity in the social “private affairs 
The “Right to be Let Alone” and Private Information 
161
of others’ attention. This meaning of control does 

ironically over-security. It is an ‘aggressive privacy’ 
where people want “a right to remain anonymous, 
hiding their own numbers when placing telephone 
calls” and on the Internet, they “insist on a right to 
hide behind false names while engaging in verbal 
harassment or slander” (Gleick, 1996). And may 
boldly exhibit themselves and object if others misuse 
this exhibition. 
However a certain minimum level of this active 
privacy is necessary and may be naturally essential 
for any human being to live in a society. According 
to Frank Askin “the right to control information 
about oneself is an essential ingredient of a secure 
personality" (Smith, 2000). The society usually sets 
some limits on the minimal privacy and maximal 
publicity. For example, regulations that prevent 
flashing are measures to limit individual’s active 
privacy. The control-game of privacy involves not 
only individual, but also society and others as 
control-players (Allen-Castellitto, 1999).  
5.3 Active Privacy and Freedom
It is said that privacy is important in its aim of 
providing the environment of freedom to act. Thus, 
protecting privacy is a prerequisite for the exercise 
of freedom (Ruiz, 1997). This freedom is related to 
“the area within which the subject is or should be 
left to do or be what he is able to do or be without 
interference by other persons [or the State]” (Berlin, 
1969). The active privacy is an interpretation of “to 
be let alone” that coincides with one meaning of the 
notion of “liberty” which includes the condition of 
being able to act in any desired way without 
restraint; and power to do as one likes. Liberty 
implies the right and ability to control our own lives 
in terms of work, religion, beliefs, etc. It is claimed 
that the (constitutional) right to privacy more aptly 
described as a right to liberty (Thomson, 1975). 
We see that in active privacy, the notion of 
‘intrusion’ on privacy means conflict between the 
individual and others (e.g., government) on the 
limits of the freedom of action in private affairs. In 
the context of informational privacy this conflict is 
materialized in terms of: 
(a) Forcing the individual to disclose private 
information in Not Known.
(b) Limiting the individual control of transferring 
his/her private information from one set of others in 
Known to another.  
It is claimed that “[t]he theory’s [privacy as 
control over personal information] focus on 
information, ... , makes it too narrow a conception, 
for it excludes those aspects of privacy that are not 
informational, such as the right to make certain 
fundamental 
decisions 
about 
one’s 
body, 
reproduction, or rearing of one’s children” (Solove, 
2002). Our approach positions this notion of control-
over-information as one aspect of “activity-based” 
privacy. Additionally, active privacy is but one facet 
of the conceptualization of the right to be let alone.  
6 PASSIVE PRIVACY AS 
“FREEDOM FROM”
‘To be let alone’ can be interpreted as not being an 
object (subjected by others), which mandates duties 
on others not to make the individual an object (of 
their activities). Notice that Brandeis and Warren 
used the term “subjected” in referring to the 
individual when his/her privacy is invaded (Warren 
and Brandeis, 1890). This second notion of ‘to be let 
alone’ can be written as “the right of not-being an 
object’ or it is the duty of any ‘other’ to control 
his/her/its activities that subject an individual to 
intrusion on privacy. This right to privacy is 
“founded upon the claim that a man has the right to 
pass through this world, if he wills, without having 
his picture published, his business enterprises 
discussed, his successful experiments written up for 
the benefit of others, or his eccentricities commented 
upon either in handbills, circulars, catalogues, 
periodicals, or newspapers; and, necessarily, that the 
things which may not be written and published of 
him must not be spoken of him by his neighbors, 
whether the comment be favorable or otherwise. The 
theory that everyone has a right to privacy and that 
the same is a personal right growing out of the 
inviolability of the person. The right to one's person 
may be said to be a right of complete immunity, to 
be let alone” (See Ballentine Law Dictionary, 1969). 
In the context of informational privacy, passive 
freedom means that the individual is not subjected to 
the following: 
 (a) Someone transfers the individual’s private 
information from Not Known to Known.
 (b) Someone transfers the individual’s private 
information from one subset in Known to another. 
The distinction between active and passive forms 
of privacy has been recognized to a certain extent in 
the literature. According to Nockleby “This capacity 
to control information is a power; if you possess the 
power to control information about yourself we can 
call it a “right” of privacy.  If someone else 
possesses the power to control information about 
Sabah S. Al-Fedaghi 
162
only view privacy as “a type of ... seclusion” but 
also a type of exposure. It does not only involve 
feeling of insecurity, fear from risk or danger, safety, 
anxiety, etc. but also involves feeling of showiness, 
extravaganza, impressiveness, etc. that reflects 
a murder may volunteer for a newspaper interview 
but may object to publish anything about him/her in 
a crime magazine. This right to be let alone does not 

you, we can say that person has the power of 
control” (Nockleby, 2002). In our approach these 
types of privacy are variations of privacy as the right 
to be alone. We can notice that active privacy is a 
‘right-base’ privacy while passive privacy is a ‘duty-
based’ privacy. In the classical rights theory, 
positive rights are rights to “caring actions” from 
other people while negative rights, by contrast, are 
rights of non-interference. Negative rights are 
classified as (a) Active rights (or liberty rights) 
which are rights to do as one chooses (b) Passive 
rights which involve the right to let alone. We claim 
that the “to-be-let-alone” apparatus encompasses 
what is called negative rights. Also, passive privacy 
appears in different fashions. According to O’Brien 
(O’Brien, 1979) privacy “may be understood as 
fundamentally denoting an existential condition of 
limited access to an individual’s life experiences and 
engagements.” 
This 
statement 
has 
passivity 
ingredient. Similar feature of passivity appears when 
Justice Douglas wrote in 1967, “Those who wrote 
the Bill of Rights believed that every individual 
needs both to communicate with others and to keep 
his affairs to himself. That dual aspect of privacy 
means that the individual should have the freedom to 
select for himself the time and circumstances when 
he will share his secrets" (Smith, 2000). 
It can be noted that many dictionaries mention 
two senses of privacy: a type of freedom (from 
intrusion) and a quality of a state (e.g., seclusion). 
Our active and passive features of privacy offer 
clearer classification of the notion. Freedom also has 
two senses: active (unrestricted activity) and passive 
(the absence of outside pressure or constraint). 
It is interesting here to explore the notion of 
“passive freedom” further.  “Privacy is not identical 
with control over access to oneself, because not all 
privacy is chosen. Some privacy is accidental, 
compulsory, or even involuntary” (O’Brien, 1979). 
This passive freedom is usually qualified with the 
term “from” such as freedom “from intrusion”, 
“from unwarranted publicity”, “from using one's 
name or likeness”, “from unnecessary attacks on 
reputation”, etc. Sometimes it is stated in terms of 
“free of” as in the right to be free of door-to-door 
solicitors. Passivity here denotes being in a state in 
which the individual does not have immediate 
control over the resultant freedom as this freedom 
depends on how others respect their duties. When 
“to” is used with this passive freedom it refers to 
“others” doing some activities that affect the privacy 
of the individual. For example, others attempt “to 
control the moral content of a person's thought”, “to 
search or seizure private...”, etc. Clearly, in these 
situation the individual is a passive agent. Also “to” 
may be used in conjunction with a very general 
sense of freedom that includes active and passive 
freedom; for example, freedom “to enjoy a private 
life” means freedom “from” bodily restraint, and 
freedom “to” engage in any of the common 
occupations of life, etc. 
The interpretation of the “right to be let alone” as 
“freedom from” or passive privacy means “the right 
to not-being an object to (others’) activities’. Hence, 
“to be let alone” is sometimes rephrased as to be 
“left” alone where ‘left’ means untouched or hands 
off. This sense of freedom is in line with the Kantian 
notion of human beings as ends in themselves and 
the need to define and pursue one's own values free 
from the impingement of others. Also, Justice Louis 
Brandeis in his much quoted statement “[T]he right 
to be let alone is the most comprehensive of rights 
and the right most valued by civilized men” may 
refer largely to this type of freedom. This freedom 
has also a touch of the existentialist’s view of man 
as "a free being" that must be left alone to realize 
his/her true nature. Passive freedom has no control 
element; just one wants others not paying attention 
since he/she does not do any activities that attract 
attention. Passivity here refers to no extra-activity 
besides normal activities. This freedom means to be 
‘free to enjoy good life’ while ‘melting in the crowd’ 
and having immunity from the focused attention of 
others including the State. The assumption here is 
that if one doesn’t attract the attention of others then 
others will not subject him/her to their activities. 
There is no sense in “relinquishing privacy” in this 
case because “relinquishing” contradicts passivity. 
This type of freedom can be classified under what is 
called 
“inalienable 
rights” 
that 
cannot 
be 
surrendered. Brandeis and Warren used ‘natural law’ 
as a base for the right to privacy in this sense. It is a 
special type of the general Freedom advocated by 
many great men as the right that “consists in a 
Freedom from any superior Power on Earth, and not 
being under the Will or legislative Authority of Man, 
and having only the Law of Nature (or in other 
Words, of its MAKER) for his Rule” (Williams, 
1744). 
This passive freedom in private affairs is the one 
described as valuable to maintain intimacy. Love, 
friendship and trust are possible not in the 
controlling mode of one’s affairs (free in doing 
whatever one wants) but in the context of ‘living 
one’s affairs’ where one expects no others’ attention 
(free from subjection by others). It does not embed 
domination over others. An intimate relationship 
such as love grows naturally in the middle of the 
crowd, spontaneously, uncontrollable (by ones 
involved), and free. The lovers feel uninhibited 
(hiding it/ announcing it) because others are 
basically and non-negatively don’t care. Freedom 
here is not ‘control’ because the lovers are passive – 
they are not playing the control game: allowing or 
The “Right to be Let Alone” and Private Information 
163

preventing others, but they hide/announce their love 
as part of the love game.  
In passive privacy others may ‘know’ private 
affairs, yet they don’t ‘possess’ it. ‘Passive seeing’, 
in this privacy does not lead to ‘subjection’. That is, 
people who are being seen are not subjected to or 
immune to any consequences of this seeing. Thus we 
can understand Warren and Brandeis position when 
they advocated that even truthful reporting about 
private affairs was causing “a lowering of social 
standards and morality.”  
Passive freedom in this sense is ‘social’ freedom 
that does not involve seclusion, isolation, solitude, 
hiding, 
concealment, 
withdrawal, 
aloneness, 
alienation, and all other negative implications. It is 
also the privacy that involves feeling of security, 
freedom from risk or danger, safety, etc. This 
privacy-related security and freedom have been 
identified as an inalienable right to privacy, based on 
the Fourth Amendment to the United States 
Constitution: The right of the people to be secure in 
their persons, houses, papers, and effects, against 
unreasonable searches and seizures, shall not be 
violated... This refers to any ‘penetrations of 
individual privacy’.  
Also, the root of this privacy can be found to a 
great deal in the liberal notion of negative liberty. “I 
am normally said to be free to the degree to which 
no man or body of men interferes with my [passive] 
activity. Political liberty in this sense is simply the 
area within which a man can act unobstructed by 
others. If I am prevented by others from doing what 
I could otherwise do, I am to that degree unfree; and 
if this area is contracted by other men beyond a 
certain minimum, I can be described as being 
coerced, or, it may be, enslaved. Coercion is not, 
however, a term that covers every form of inability. 
If I say that I am unable to jump more than ten feet 
in the air, or cannot read because I am blind...it 
would be eccentric to say that I am to that degree 
enslaved or coerced. Coercion implies the deliberate 
interference of other human beings within the area in 
which I could otherwise [passively] act” (Berlin, 
1969).
7 PASSIVE/ACTIVE FREEDOM  
It remains the third categories in figure 1 where the 
individual is the actor and the one acted on. This 
type of activity appears in such expression as ““to 
control of his own person”, “to control own body”, 
“to remain silent”, “to develop one's intellect, 
interests, tastes, and personality”, “to keep his/her 
affairs to him/herself”,  “to care for one's health and 
person, to walk, stroll, or loaf”, “to sit on a park 
bench or to stroll in a city”, “to testify against 
oneself” (The Fifth Amendment), and “the right of 
every individual to the possession and control of his 
own person” (Union v Botsford, 1891). Justice 
Douglas linked some of these rights with “personal 
autonomy” as “rights protected by the First 
Amendment and ... they are absolute, permitting of 
no exceptions...” This active/passive privacy is a 
special type of autonomy. Its side of Autonomy 
includes 
decisions 
related 
to 
property-based 
activities that can be categorized under property 
rights. As mentioned previously, a property (e.g., a 
pet) can become a public attraction without any 
intrusion on the privacy of its owner. When the 
owner is implicated in the publicity then the issue 
moves to the active privacy domain. A self-based 
autonomy includes decisions about proprietary 
affairs. When I tattoo myself then I am the actor and 
the one acted on. I may use others to help in this 
tattooing action but they are only instruments in this 
operation since they don’t have to know me and I 
don’t have to know them. However when the tattoo 
attracts the attention of others then the issue moves 
to the active privacy domain. Similarly shopping is 
in the domain of active/passive privacy when I use 
cash. Shopping in this case is an autonomous and 
private operation. When I use credits then the issue 
moves to the active privacy domain. Also 
identifying oneself is an active/passive privacy right. 
When the individual volunteers his/her identification 
or does an activity that results in others requiring 
his/her identification then the issue moves to the 
active privacy domain.
8 NON-PRIVACY FREEDOM  
The types of freedom discussed above are privacy-
related types of freedom. They are different from 
non-privacy related types of freedom. For example, 
non-privacy related ‘freedom of speech/expression’ 
deals with non-private matters such as freedom of 
expression in politics, religion, social affairs, etc. 
Privacy related freedom of this type is a freedom 
where its subject matter is private information of 
individuals. This privacy related freedom of speech 
should not be considered as a special type of non-
privacy related freedom of speech. There has been 
no doubt regarding the social value of the later type 
of freedom as has frequently been the case in human 
history, while there have been questions about the 
social value of some or all of the former type of 
freedom. Treating them separately clarifies many of 
the issues related to privacy. 
The classical free speech claim is given by Mill, 
“there ought to exist the fullest liberty of professing 
Sabah S. Al-Fedaghi 
164

and discussing, as a matter of ethical conviction, any 
doctrine, however immoral it may be considered” 
(Mill, 1978). This freedom is restricted if it causes 
harms to others.  
Example: According to Mill, the claim that corn 
dealers are starvers of the poor is permissible free 
speech, however, it is not permissible to express the 
same view to an angry mob that has gathered outside 
the house of the dealer because this action is “a 
positive instigation to some mischievous act” (Mill 
1978). There is a dispute about what Mill had in 
mind; however, the example illustrates our notion of 
privacy-related freedom. Giving the speech outside a 
particular dealer can cause a direct harm to a specific 
identifiable person. Unique identification through 
the house of the dealer is clear. The cause of 
illegitimate personal harm is not the angry mob but 
the subjection of particular individual to a privacy 
related speech act. The first case, where the 
accusation is published in the press and may cause 
non-personal harm (e.g., financial loss to ALL 
dealers), is a non-privacy related freedom of 
expression, assuming that no individual dealer is 
identified in the article. 
Non-privacy freedom of speech can be related to 
privacy from another perspective. Preserving one's 
privacy may motivate anonymous publications 
concerning public issues (McIntyre v Ohio, 1995). 
9 CONCLUSION 
Using the general notion of private activities we 
developed three types of privacy dependent on the 
kind of activities of the individual: active, passive 
and active/passive private activities. According to 
Justice Rehnquist “Citizens attending a political 
rally have no privacy interest”  (Smith, 2000). 
According to our classifications, he means here 
active privacy. He also objected to count “arrests” as 
“a private occurrence” even when there is no 
“conclusive evidence of wrongdoing” (Smith, 2000).  
The assumption in this case is that the act is counted 
as an active private act where the individual did 
something, however minor it is, to motivate 
considered it as “a relevant factor by law 
enforcement 
authorities." 
The 
notions 
of 
“relevancy”, “a probable cause”, “a valid warrant”, 
etc. are obviously related to this issue in the context 
of active privacy not passive privacy. The passive 
privacy notion appears when the “arrest” was an 
absolutely ‘passive privacy event’ - say, the police 
made a mistake and arrested a citizen sleeping at 
his/her home; then the arrest may be treated 
differently. These explicit refinements of privacy as 
special cases of the right to be alone can clarify 
many 
privacy-related 
issues. 
Advocates 
of 
informational privacy can argued that police ought 
not release records of arrests in passive private 
events. 
Warren and Brandeis did ‘net’ the three 
ingredients of privacy in the concept of the right to 
be let alone. The individual is one of the “others” 
thus he/she has freedom as an individual and has 
duty as an entity in the society. Active privacy has 
‘right’ and contractual base while passive privacy 
has ‘duty’ and moral context. The later is an “ideal 
privacy” that requires that all others abide by a strict 
respect to privacy, thus it depends upon how others 
may discharge this duty.  Clearly this is unattainable 
in human society especially with respect to effective 
enforcement of this duty on all members. The 
individual may also not be receptive to this type of 
privacy. “Except for a few very eccentric, and 
probably dangerous, individuals, no one desires to 
be let entirely alone” (Johnson, 2003). Hence 
Warren and Brandeis did net it with active privacy 
where the individual is not completely a passive 
party with regard to his/her privacy. Thus the “right 
to be let alone” has a utilitarian touch that aims at 
maximizing benefits to the society and individuals. 
These active and passive interpretations of the 
right to be let alone have no right/duty correlativity. 
The right to be actively free correlates with the duty 
of non-interference by others; and the duty of others 
of not subjecting the individual to privacy intrusion 
correlates with the right of the individual to 
immunity in private affairs. So Warren and 
Brandeis’ “right to be let alone” integrates two rights 
and two duties. Beside these double facets of 
privacy, “to be let alone” encompasses the “real” 
right to “self”: active/passive privacy. This concept 
of “to be let alone” with its multi-rights and duties is 
a genius apparatus for describing the private realm 
of humans. 
REFERENCES
Anita Allen-Castellitto, 1999, Coercing Privacy, 40 Wm. 
& Mary Law. Review 723.
Berlin, Isaiah, 1969, Four Essays on Liberty, Oxford 
University Press, Oxford, at 121-122.  
Brandeis, Louis. 1928, in a dissenting opinion concerning 
the constitutionality of telephone wiretapping - 
Olmstead v. United States, 277 U.S. 438, 478. 
DeCew, J.  2002, Privacy, Section entitled “Privacy and 
Technology”, in the Stanford Encyclopaedia of 
Philosophy,
 
May
 
14. http://plato.stanford.edu/entries/
privacy/. 
Etzioni, A. 1999, The Limits of Privacy, Basic Books, 
New York. 
The “Right to be Let Alone” and Private Information 
165

Floridi, di Luciano. 1998, Information Ethics: On the 
Philosophical 
Foundation 
of 
Computer 
Ethics, 
ETHICOMP98 The Fourth International Conference 
on Ethical Issues of Information Technology,
http://www.wolfson.ox.ac.uk/~floridi/ie.html 
Fried, C. F. 1970, An Anatomy of Values, Harvard 
University Press, Cambridge. 
Fule, Peter and John Roddick 2004, Detecting Privacy and 
Ethical Sensitivity in Data Mining Results, Twenty-
Seventh Australasian Computer Science Conference
(ACSC2004), 
Dunedin, 
New 
Zealand. 
http://crpit.com/confpapers/CRPITV26Fule.pdf 
Gerety, Tom. 1977, Redefining Privacy, Harvard Civil 
Rights—Civil Liberties Law Review 12, no. 2: 236. 
Gleick, J. 1996. Behind Closed Doors; Big Brother Is Us,
New York Times, September 29: 130. 
Godkin, E. L. 1980, The Rights of the Citizen, IV—To 
His Own Reputation, Scribner’s Magazine, July-Dec. 
Johnson, Jeffery L. 2003, LEGAL PRIVACY AND THE 
ATTENTION OF OTHERS (Date of access). 
http://www.eou.edu/~jjohnson/pnwpsajudpol.htm 
McIntyre 
v. 
Ohio 
1995, 
115 
S. 
Ct. 
1511, 
http://caselaw.lp.findlaw.com/cgi-
bin/getcase.pl?court=us&vol=000&invol=u10296. 
Mill, John Stuart 1978, On Liberty, Hackett Pub. Press, 
Indianapolis. 
Nockleby, John T. 2002, Privacy:  Circa 2002.
http://cyber.law.harvard.edu/privacy/PrivacyCirca200
2.htm#_ftn1
O’Brien, David M. 1979, Privacy, Law, and Public Policy,
New York:  Praeger. 
analysis.html 
Prosser, William L. 1984, Privacy [A Legal Analysis], in 
Philosophical Dimensions of Privacy, Ferdinand 
David Schoeman (ed.). 
Parker, Richard B. 1974, A Definition of Privacy, 27 
Rutgers Law Review.
Ruiz, B. R. 1997, Privacy in Telecommunications, Kluwer 
Law International, The Hague/London/Boston. 
Schoeman, Ferdinand 1984, Privacy: Philosophical 
Dimensions of the Literature, in Philosophical 
Dimensions of Privacy, Ferdinand David Schoeman, 
(ed.). 
Smith, Robert Ellis. 2000, PRIVACY AND CURIOSITY 
FROM PLYMOUTH ROCK TO THE INTERNET, 
PRIVACY 
JOURNAL, 
Excerpt 
in: 
http://members.aol.com/RAHfan147/PrivacyBookExe
rpt.html
Solove, Daniel J. 2002, Conceptualizing Privacy, 
California Law Review, Vol. 90. 
Thomson, J. 1975, The Right to Privacy, Philosophy and 
Public Affairs 4: 295-314
Warren, Samuel D. and Louis D. Brandeis 1890, The 
Right to Privacy, Harvard Law Rev., Vol. IV, No. 5. 
http://www.lawrence.edu/fac/boardmaw/Privacy_bran
d_warr2.html. 
Westin, A. 1967, Privacy and Freedom, Athenaeum, New 
York (1967). 
Williams, Rev. Elisha, 1744, "A Seasonable Plea . . .", at 
http://www.lexrex.com/enlightened/AmericanIdeal/yar
dstick/pr3_quotes.html
Union Pacific Railway Co. v. Botsford, 1891, 141 US 250, 
11 S Ct 1000, 35 L Ed 734. 
Data Act. http://dsv.su.se/jpalme/society/data-act-
Palme, Joseph. 1998, Critical Review of the Swedish
Sabah S. Al-Fedaghi 
166

PERSPECTIVES ON PROCESS DOCUMENTATION 
A Case Study 
Jörg Becker, Christian Janiesch and Patrick Delfmann 
European Research Center for Information Systems, University of Münster, Leonardo-Campus 3, Münster, Germany 
Email: becker@ercis.de, christian.janiesch@ercis.de, patrick.delfmann@ercis.de 
Wolfgang Fuhr 
Bayer Business Services GmbH, Leverkusen, Germany 
Email: wolfgang.fuhr@bayerbbs.com 
Keywords: 
Process Documentation, Process Modeling, Configuration, Conceptual Modeling, Multi-Perspective Infor-
mation Modeling, Case Study. 
Abstract: 
The documentation of IT projects is of paramount importance for the lasting benefit of a project’s outcome. 
However, different forms of documentation are needed to comply with the diverse needs of users. In order 
to avoid the maintenance of numerous versions of the same documentation, an integrated method from the 
field of reference modeling creating perspectives on configurable models is presented and evaluated against 
a case in the field of health care. The proposal of a holistic to-be model for process documentation provides 
useful hints towards the need of presenting a model that relates to a specific user’s perspective. Moreover, it 
helps to evaluate the applicability of configurable, company-specific models concerning the relative operat-
ing efficiency. 
1 RESEARCH APPROACH AND 
CASE SCENARIO 
The documentation of IT projects, e. g. software 
development or process reengineering, is of para-
mount importance for the lasting benefit of a pro-
ject’s outcome. But different forms of documenta-
tion are needed to comply with the diverse needs of 
users who use information model for different pur-
poses. A system engineer for instance is in need of 
different information than a manager or a trainee. 
These problems are similar to those from the 
field of reference modeling where large overall 
process models have to be adapted to company-
specific contexts. To assist this model adjustment 
several model customization techniques have been 
discussed that allow the adaptation for various pur-
poses. However, the applicability to large company-
specific models has not been further investigated. 
Thus, these configuration techniques are discussed 
mainly in the field of reference modeling. Their 
applicability to company-specific models in terms of 
process documentation for process management 
purposes seems to be promising nonetheless. 
In the year 1996 the Bayer AG, a Germany-
based global enterprise in the fields of health care, 
nutrition, and innovative materials, settled on a 
comprehensive framework for a corporate-wide roll-
out of the standard software SAP R/3. In the run-up 
all major processes were analyzed and documented. 
This provided an ideal basis for a thorough ex-post 
analysis and allowed meaningful propositions for a 
future appearance of the process documentation that 
allow the utilization of the information for different 
purposes. The Bayer Business Services GmbH as-
sisted us greatly in the analysis of their documenta-
tion and provided us with any input requested. We 
thankfully acknowledge their support in this project. 
Thus, the research questions for the following 
are in detail: 
x
Which possibilities exist to allow a context-
oriented preparation of process documenta-
tions or process models? 
x
In which way can they be applied at the Bayer 
AG? 
x
Is the utilization of context-oriented process 
documentations beneficial for the Bayer AG? 
If so, does the concept sound promising for 
other companies as well? 
167
C.-S. Chen et al. (Eds.), Enterprise Information Systems VII, 167–177.
© 2006 Springer.

The structure of the paper mirrors this. At first, 
the idea of configurative modeling and its related 
work is elaborated upon. Then a comprehensive as-
is analysis is conducted whose findings result in the 
proposition of a holistic to-be model. Hereon the 
need for configurable models at the Bayer AG is 
identified as well as the application of possible oc-
currences is presented and discussed in detail. 
2 CONFIGURATIVE MODELING 
AND RELATED WORK 
2.1 Perspectives on Process Models 
According to the Total Quality Management (TQM) 
approach, the quality of a product is determined by 
its fitness-for-use for the consumer and his require-
ments (Ishikawa, 1985). When transferred to process 
models, their quality depends on the fitness-for-use 
concerning the requirements of particular users or 
user groups. User requirements result from their 
different 
perspectives 
on 
business 
processes 
(Rosemann et al., 2005), (Nissen et al., 1996), 
(Rosemann and Green, 2000). A perspective is de-
termined by the deliberate and specific use of a 
business process model, the organizational role of a 
user as well as individual preferences on the concep-
tual and representational design of business process 
models (Becker et al., 2002). See Figure 1 for an 
overview of specific purposes of process models. 
The more effective the process model meets the 
requirements of a particular perspective, the higher 
is its quality. Ideally, each identified perspective 
should be provided with a tailor-made version of a 
process model. This approach is called multi-
perspective process modeling (Darke and Shanks, 
1996), (Rosemann, 1998), (Becker et al., 2002). 
2.2 Related Work on Model  
Adaptation 
In order to enable multi-perspective information 
modeling, different approaches of model adaptation 
have been developed in the past. Some approaches 
focus on model transformation, as they are pro-
claimed by the Model Driven Architecture (MDA) 
(Soley and OMG Staff Strategy Group, 2000)). 
Implementations of model transformation mecha-
nisms can be found in the form of so-called Meta 
CASE Tools like the Generic Modeling Environ-
ment (GME) (Agrawal et al., 2002), (Ledeczi et al., 
2001) and Metaview (Findeisen, 1994). Model 
transformation aims at generating a destination 
model out of an original model, whereas the 
languages of both models can diverge extensively. 
Structural patterns are identified in the model of the 
initial modeling language via an algorithmic search 
and they are transformed into equivalent patterns of 
a model of the targeted modeling language. Trans-
formations are performed by using transformation 
rules that are defined for each combination of the 
original and destination language (Engels et al., 
1997). Model transformation approaches are charac-
terized by a high universality of the operators used 
for the definition of transformation rules (e. g. Cre-
                                                                        Cf. (Rosemann et al., 2005). 
Jörg Becker et al. 
168
Figure 1: Specific Purposes of Process Models. 

Link, Delete, Refer else Create, Create inside, Refer 
to) which makes high user competencies necessary. 
Transformations are usually employed in the 
software industry to adapt software to different op-
erating systems or computer platforms. In the field 
of process modeling it is not necessarily required to 
transform models. 
Other types of approaches focus on building 
views onto a model system. These views are then 
considered as perspectives which result from user 
requirements. Exemplary approaches of this type are 
the Semantic Object Model (SOM) (Ferstl and Sinz, 
1998), the Architecture of Integrated Information 
Systems (ARIS) (Scheer, 2000), the Zachman 
Framework (Zachman, 1987), the Open Systems 
Architecture for Computer Integrated Manufacturing 
(CIM-OSA) (ESPRIT Consortium AMICE, 1989), 
MEMO (Frank, 1994), and Viewpoints (Finkelstein 
et al., 1992). A common characteristic of these ap-
proaches is that the realization of multiple perspec-
tives is restricted to providing different modeling 
views which result in different model types. In the 
case of ARIS, these views are e. g. the data, the 
functional, the organizational and the process view 
which are represented by Entity-Relationship Mod-
els (ERM) (Chen, 1976), Function Trees, Organiza-
tional Charts, and Event-driven Process Chains 
(EPC) (Scheer, 2000). 
The approach which will be adopted here is 
based on the latter of the approaches mentioned, but 
provides extended configuration mechanisms instead 
that are not restricted to modeling views. 
2.3 Configuration Techniques for  
Information Models 
The most significant problem that results from a 
multiplicity of perspective specific, tailor-made 
models is the need to manage possible redundancies 
inside the model itself. This leads to increased mod-
eling and maintenance cost and the danger of incon-
sistencies within the model base. 
In order to enable an efficient multi-perspective 
process modeling, redundancies have to be over-
come. A modeling methodology which enables the 
user to avoid redundancies and to consider multiple 
perspectives within the model base is called configu-
rative process modeling (for the following refer to 
(Becker et al., 2004), (Becker et al., 2002)). The 
approach is based on the concept of model projec-
tion. A configurable information model that provides 
all relevant information for each perspective con-
tains constraints that determine to which perspective 
each model element belongs. By this means redun-
dancies are avoided and, simultaneously, multi-
configuration is performed, each element is hidden 
that does not belong to the selected perspective. This 
implies that the core modeling is conducted using 
the model base and as such can only be performed 
by modeling experts that are properly trained. Thus, 
the distributed modeling of the base model still 
causes problems since inconsistencies may occur. 
In order to reduce modeling complexity for the 
individual user, it makes sense to provide configura-
tion mechanisms with different effectiveness. First, 
coarse granular configuration mechanisms that oper-
ate on whole model sections, second, mechanisms 
that operate on single model elements should be 
provided. Hence, we distinguish configuration
mechanisms that are based on meta model projection 
or model projection respectively. Using meta model 
projection on the one hand, the user is enabled to 
create perspective-specific models that differ in the 
expressive power of the underlying modeling 
method (e. g. by hiding model elements of a specific 
object type). On the other hand, using model projec-
tion, particular model elements can be hidden (e. g. 
process branches that are of no relevance for the 
regarded perspective). Model projections are lan-
guage extensions in the sense of Domain-Specific 
Modeling (Nordstrom et al., 1999), which are par-
ticularly adapted to the requirements of multi-
perspective information modeling. 
3 THE BAYER CASE 
3.1 Initial Situation 
Since 1983 the Bayer AG used SAP R/2 systems in 
several parts of the company. In the year 1996 a 
comprehensive framework for a corporate-wide roll-
out of the ERP software SAP R/3 was resolved 
upon. In the run-up all major processes were ana-
lyzed and if necessary redesigned. The original idea 
was to conduct a complete redesign utilizing the 
concept of business reengineering (Hammer and 
Champy, 1993). However, the idea had to be ne-
glected later on for several reasons; mainly the size 
of the project in connection with established struc-
tures disallowed the necessary changes. See chapter 
3.2 for further factors that hindered the project’s 
development and documentation. 
Whenever possible, two perspectives were used 
in the process: one with a more managerial and one 
with a more technical focus. A large number of 
external consultants were part of the project teams. 
The documentation of the processes was con-
ducted with several plug-ins of a Lotus Notes 
groupware environment, the so-called Electronic 
perspective modeling is made possible. When a 
Perspectives on Process Documentation 
169
ate New, Replace, Same, Create Reference, Create 

Project Notebook (EPN). The documentation is 
structured in three perspectives: a function-oriented 
view, a process-oriented view and a technical view. 
The function view’s elements are detailed in Proc-
ess, Subprocess and Activity Profile, the process-
oriented view that spans multiple organizational 
units is arranged in Business Scenario Cluster 
(BSC), Business Scenario and Business Scenario 
Flow (BSF) (see Figure 2, all other perspectives are 
modeled similarly). The technical view which is of 
minor importance to the overall documentation is 
arranged in IT Solution Design Document, Configu-
ration Document and Activity Script as well as the 
Repository Objects which contain application source 
code. Connected to every node in every hierarchy 
level there is a document that is detailed in the level 
below. All three hierarchy levels as well as the three 
views have to be matched manually against each 
other whenever there is a change. 
Process View
Business Scenario Flow (BSF)
Business Scenario
Business Scenario Cluster (BSC)
Parallel to the reengineering project a quality 
management system was developed that documents 
process-design processes. It combines several in-
formation sources in one portal and its core idea is to 
describe those meta processes. Again, the arrange-
ment is function-oriented and the matching has to be 
conducted manually. 
The EPN and the quality management system, 
both have next to no support with information mod-
els at all; for the most part they only contain textual 
descriptions. Solely several PowerPoint slides exist 
that depict some aspects of the system in forms of 
processes and application dependence diagrams 
using informal models. The models only serve to 
present a minimal overview and – since they have 
not been developed with an integrated modeling tool 
– have to be matched manually against change in the 
three views to persist in a consistent state. 
Since the maintenance of this form of documen-
tation over time exceeds any sensible IT budget, a 
more integrated approach is favorable. In order to 
depict most of the scenarios attributes the Direct 
Business process was chosen since it is relevant as 
well for the applications system as organizational 
design. In addition to that, four variants of the proc-
ess exist that contain special functions for external 
customers and for internal use. The process is lo-
cated in the BSC Direktgeschäft (Direct Business) of 
the EPN. 
3.2 As-Is-Analysis of the Case Process 
As indicated above, the original intention of the 
modeling was to assist a process-oriented reorgani-
zation of the corporation focusing the software cus-
tomizing for an ERP implementation. Now, the main 
focus is a continuous process management to moni-
tor and improve the current processes. In addition to 
that, options for the utilization of the documentation 
for workflows and for training purposes is explored. 
In the near future, further use in the field of quality 
management is desirable. These different purposes 
also require different perspectives for the users in-
volved since a modeling expert does have other 
requirements than the process owner. 
The Direct Business process deals with the direct 
sales to customers or other Bayer AG companies. 
Two general variants exist: Intercompany and Ex-
ternal Customer each with one special case; i. e. for 
Intercompany the Bayer AG to Bayer Distribution 
Company via IDOC process and for external cus-
tomers the Direct Business via Letter of Credit (LC)
process. 
The process always starts with a customer order 
and ends with the managing of accounts receivable. 
If planning software is utilized for internal use, it 
generates the respective customer order at the deliv-
ering company. 
See Table 1 for the original process documenta-
tion of the BSF Direct Sales (External Customer).
The table is an excerpt of the respective BSF. Some 
columns of lesser interest are omitted: SAP Transac-
tion/ Script (Optional), Information Object Charac-
teristics (Optional), Processed by Partner (Op-
tional) as well as Remarks. The other content is still 
in its original format except for minor anonymiza-
tion of numbers and codes. The other three process 
variant descriptions are of similar build and not 
depicted here in detail. 
The analysis of the process was complicated in 
particular by the following factors: 
x
The storage in the processes in the EPN is 
not consistent. 
x
The sequence of process functions can only 
be determined when BSF and Business Sce-
nario, both are explored. 
x
The matching of documentation techniques 
does not only consist of documents from the 
three hierarchy levels but also their attached 
documents. 
x
The documentation language is not uniform 
on any level of documentation. 
Jörg Becker et al. 
170
Figure 2: Structure of the Process View. 

Consistency problems: All four variants of the 
process are attached to the same BSC but there are 
two names used simultaneously in different areas of 
the corporation: BSC Direct Business and BSC Di-
rect Sales/ Direktgeschäft. An automated synchroni-
zation of names or an integrated system could take 
care of this. 
Sequence problems: Apart from the tabular proc-
ess documentation in the BSF some more aggregated 
information in forms of PowerPoint slides is saved 
in the superordinated Business Scenario. The slides 
depict the sequence on a very rough granular scale in 
forms of column-oriented informal process models. 
They represent the only hints on the sequence of 
process functions, in particular whether they are 
executed alternatively or in a parallel fashion. 
Attached document problems: Not only the 
above mentioned PowerPoint slides but also Excel 
Documents that aggregate the content of the tables 
in the Lotus Notes documentation have to be 
matched manually (against all levels and views). 
This led to minor inconsistencies in the past that will 
aggravate in the future when more views are to be 
implemented. 
Language problems: The documentation is com-
posed in English and German interchangeably. This 
led to a non-uniform naming of hierarchy levels and 
processes as well as their functions. Textual descrip-
tions are partly composed in English and German, 
too. On the one hand English naming facilitates the 
identification of the SAP transactions involved. On 
the other hand this leads to naming inconsistencies 
and problems with the matching of abbreviations. In 
addition to that it disrupts the general train of read-
ing.
However, when criticizing the systems layout, 
the factors that led to the current state of operations 
have to be considered. They do not excuse the cur-
rent state but explain the development of the as-is 
system design. Time, cost and size of the project led 
to the decision that an integrated modeling tool was 
not commissioned. It was regarded as too big a cost 
factor and too time-consuming to train all involved 
personnel, especially since there was a considerable 
Table 1: BSF Direct Sales (External Customer) 
Decisive criteria for BS Flow: xxx 
Nr.
Short Description of Task 
Activity
Work-
field
Input/Output Information 
Objects (Optional) 
1 
Kundenauftrag anlegen 
Life
Science
O: Create/Change Order 
8 
--Order type 
--Sales organization 
--Distribution channel 
--Division
--Sold-to-party 
--Purchase Order no. 
--Material number 
--Quantity
2 
Auftragsfertigung 
Life
Science
BS Auftragsfertigung 
7 
 
3 
Lieferung anlegen 
Life
Science
MM: Process Delivery of Line 
Items 
8 
Versandstelle  
4 
Dispoliste drucken 
Life
Science
MM: Pack orders for shipment & 
Plan packing material 
8 
Shipping point and  
output type 
5 
Etikettendruck 
Life
Science
MM: Label Goods for Shipping 
8 
-- Delivery no. 
-- Delivery item 
-- Batch no.
6 
Warenausgang zur Lieferung 
Life 
Science
BS Auslieferung abwickeln 
6 
8
7 
Rechnung erstellen 
Support
O: Process invoice 
8 
 
8 
Legal Services 
Support
BSF Declaration to authorities 
und BSF Zollabwicklung 
8
12
9 
FI/CO 
Zahlungseingang 
Support
Subprocess FA Manage Account 
Receivables
9
Perspectives on Process Documentation 
171

amount of external consultants involved. After start-
ing the documentation in English the unequal educa-
tional background impeded a proper completion of 
the documentation in a foreign language. It was 
decided to continue in German in an easy-to-use 
Lotus Notes environment to at least maintain a cer-
tain set of documentation guidelines. However, these 
very same factors led to the fact the current system 
is neither able to provide proper perspectives for 
different uses nor is cost-efficient. It foremost pur-
pose at this point is to be a legal lifeline that com-
plies with German law for the documentation of 
health care company software. 
3.3 To-Be Model Proposition 
For the construction of a to-be model all four proc-
ess variants are integrated into one process with 
alternative paths. It is modeled with the EPC tech-
nique to show the benefits of a well structured 
graphical representation that allows the integration 
of further techniques like for instance the ERM. 
See Figure 3 for the complete to-be model 
proposition. The use of events is abstained from as 
far as possible to improve the readability of the 
model. This does not interfere with its semantics 
since events have not been documented in the first 
place. The omission does also apply for the follow-
ing figures. 
The integration of all four variants into one 
model allows an easier maintenance of the process 
documentation since no matching has to take place 
to adapt changes to other documents. However, the 
combination of four variants into one integrated 
model leads to a graphical representation with lots of 
XOR-connectors. This can be difficult to compre-
hend by the casual user, especially when he or she is 
not familiar with the configuration techniques at the 
connectors. Since they describe under which circum-
stances which process alternative has to be followed, 
it is essential to understand their working in order to 
execute a process variant. Exemplarily four XOR-
connectors have been equipped with configuration 
rules to allow process variant execution. Moreover, 
the enrichment of the model with further elements 
like organizational roles or application systems 
would most certainly not enhance the readability but 
have a contrary effect. That is why they have not 
been included in Figure 3. 
Therefore it is necessary to use perspectives or 
views on this integrated model so that it can be used 
even by the casual user to serve his purpose. These 
perspectives allow the enhancement of the model 
and the meta model with additional elements and 
omit non relevant process variants by a further 
model configuration. These mechanisms permit the 
use even by a casual user who has at least a minimal 
understanding of graphical process modeling. Since 
all configurations are solely projections of the inte-
grated original model, its consistency is not in ques-
tion. The definition of these configuration points 
however has to be done by a method expert since 
their complexity exceeds the casual user’s abilities. 
3.4 Configurable To-Be Models 
A configuration for the organizational design of the 
variant direct sales (external customer) is shown in 
Figure 4. 
Regarding model projection all non relevant 
process threads have been masked so that only rele-
vant functions are displayed. In this way, an easy-to-
read process model is displayed that only shows the 
desired variant. Apart from this model configuration 
a meta model projection was applied to integrate 
relevant further model types into the model, i. e. an 
Organizational Chart and a Technical Term Model. 
The utilization of their model elements assists the 
organizational design and enriches the model so that 
additional information becomes explicitly available. 
Exemplarily, elements of each modeling technique 
have been linked with a dotted line to show that it is 
indeed the same object. 
A corresponding configuration for the applica-
tion system design of the same variant is shown in 
Figure 5. 
In this case as well, the model is configured so 
that only the relevant process thread is displayed. 
Further model types that are employed via meta 
model projection are the ERM and an Application 
System Model. By using element types of both mod-
eling techniques the application system design can 
be assisted since the explanatory power of the model 
is increased by this enrichment, e. g. by displaying 
abundant SAP modules. For system engineering it is 
more relevant when conducting a process analysis 
which modules of an ERP system are linked to cer-
tain functions than the role of a person that executes 
them. As a next step the accordant SAP transaction 
and the program data of the technical documentation 
can be added to the model to let the system engineer 
go further into the documentation. Another interest-
ing option is the linking of process elements for the 
administration of test cases. The tests conducted can 
be directly associated with the corresponding func-
tion, thus allowing a simple but powerful manage-
ment of test cases. Through configuration mecha-
nisms critical test runs can be separated from routine 
tests. In this way testing for processes can be clus-
tered with the process model. In addition to that, 
results could be clustered as well to single out devia-
tions that exceed defined safety limits. 
Jörg Becker et al. 
172

Stock 
production
Sales AG to 
distribution 
(legacy)
Create/
change order
Made-to-order 
production 
Pack orders 
for shipment & 
plan packing 
material
Label goods 
for shipping
Goods issue 
to delivery
Process 
invoice
Declaration to 
authorities and 
customs
Manage 
accounts 
receivable
Book goods 
issue
Batch input to 
SAP BV
Dispatch 
notification via 
IDOC
Quote
Process 
proforma 
invoice
Process 
finance 
document
Adapt order to 
export 
requirements
Monitor order 
and financial 
document
Process 
predated 
invoice
Create export 
documents
Accounts 
receivable
done
Legal
Services
done
Batch input
to SAP BV 
done
Process 
delivery of
line items
Order via LC 
received
Customer
order
received
Order
received from 
distribution
XOR
XOR
XOR
XOR
XOR
XOR
XOR
XOR
XOR
XOR
XOR
XOR
Customer
order
received
Regular 
product is 
ordered
Product is 
ordered that is 
made-to-order
Product via LC 
is ordered that 
is made-to-
order
Product is 
ordered via LC
Product is not 
ordered via LC
Product is not 
ordered via 
IDOC
Product is 
ordered via 
IDOC
Product is not 
ordered via LC
Product is 
ordered via LC
Product is 
ordered via 
IDOC
Product is not 
ordered via LC
Product is 
ordered via LC
variant (external 
customer + external 
customer via LC + 
intercompany + 
intercompany via 
IDOC)
variant (external 
customer + external 
customer via LC + 
intercompany) AND 
product (made-to-
order)
variant (external 
customer via LC) 
AND product 
(made-to-order)
variant (external 
customer via LC) AND 
product (made-to-
order)
Perspectives on Process Documentation 
173
Figure 3: To-be Model Proposition. 

Create/ 
change order
Goods issue 
to delivery
Delivery of line 
items
Process 
invoice
Declaration to 
authorities and 
customs
Manage 
account 
receivables
Pack orders 
for shipment & 
plan packing 
material
Label goods 
for shipping
Distribution 
and Logistics 
(8)
Sales and 
Production 
Planning (7)
Distribution 
and Logistics 
(8)
Distribution 
and Logistics 
(8)
Local Systems 
(6)
Distribution 
and Logistics 
(8)
Local Systems 
(6)
Distribution 
and Logistics 
(8)
Transport 
Logistics (12)
Accounting (9)
Customer
FB
Order
FB
Invoice
FB
Customer
FB
Order
FB
Invoice
FB
Time
FB
is multiple characteristic of
is characteristic of
is characteristic of
Sales and 
Production 
Planning (7)
Distribution 
and Logistics 
(8)
Local Systems 
(6)
Transport 
Logistics (12)
Accounting (9)
Workfield local
Workfield 
regional
Human 
Resources 
(10)
Made-to-order 
production 
Workfields
Technical Terms
Account 
receivables
done
Legal
services
done
Customer order
received
XOR
XOR
Product is 
ordered that is 
made-to-order
Regular 
product is 
ordered
Distribution 
and Logistics 
(8)
Jörg Becker et al. 
174
Figure 4: Model Projection for Organizational Design. 

Create/
Change Order
Goods issue 
to delivery
Delivery of line 
items
Process 
Invoice
Customer
Order
Invoice
Made-to-order 
Production 
Declaration to 
authorities and 
customs
Manage 
account 
receivables
Pack orders 
for shipment & 
plan packing 
material
Label goods 
for shipping
FA   Financial Accounting
MM  Material Management
O     Order Management
SD   Sales & Distribution
Customer
Time
Order
(1,n)
(1,1)
(1,n)
(0,n)
SAP R/3
Data Model
SAP Modules
SAP MM
SAP FA
SAP SD
SAP O
SAP FA
100
SAP O
220-01
SAP MM
240-060
SAP MM
240-030
SAP MM
240-010
SAP MM
220-010
Invoice
(1,1)
Invoice-
Order-
Relation
Customer 
Order Received
Account 
receivables
done
Legal
services
done
XOR
XOR
Regular 
product is 
ordered
Product is 
ordered that is 
made-to-order
Perspectives on Process Documentation 
175
Figure 5: Model Projection for Application System Design. 

A projection of processes for communication sup-
port, like presentations or training uses, seemingly 
needs to provide colorful and pictographic, yet sim-
ple built element types – or rather just icons. The 
projection of the model is preferably directly usable 
with presentation software so that no further con-
verting has to be carried out. 
The design and layout might have to vary con-
siderably from the representation of the other two 
configurations discussed above since a communica-
tion model’s main contribution is to present a simple 
and understandable figure. The unambiguity of the 
process model and thus the complying with a given 
syntax is of minor importance as long as the simpli-
fication does not lead to inconsistencies in the un-
derstanding. Depending on the context, different 
graphical designs can and should be chosen to assist 
the specific purpose. In this way the original Bayer 
AG documentation models that are ordered in col-
umns, could even be recreated. The model projected 
can be generally classified as an informal representa-
tion of the model base. Its applicability is strictly 
limited to communication uses since the model does 
not allow the derivation of any formal specification. 
3.5 Case Evaluation 
This case study proves exemplarily that there is a 
multitude of practical circumstances which benefit 
from configurable information models and that they 
are virtually indispensable when an integrated ap-
proach is followed. 
In comparison to the textual descriptions of the 
original documentation the advantages of a graphical 
representation clearly show. Through the use of 
graphical elements the depiction of the real world is 
structured and intuitively accessible; textual descrip-
tion with a similar level of abstraction cannot com-
pete. Especially the initial training on the documen-
tation and the work with the processes, both become 
easier. Even though the process models discussed in 
the previous chapter do not contain all the informa-
tion of the textual description, it is quite obvious that 
a consistent representation proves to be beneficial. 
However, without further thoughts about the con-
figuration of models, no real benefit in terms of 
clarity can be achieved as Figure 3 shows. 
Regarding configurable models more achieve-
of models for different purposes can be made. This 
does apply to the example of the direct business 
process as well as to other contexts. For example, an 
adaptation of the granularity could allow the mana-
gerial as well as the technical process owner to work 
with the same model because only the relevant 
model elements and element types are displayed 
through recipient-specific model and meta model 
projection. The integration of other modeling tech-
niques like ERM or Organizational Charts provides 
an overview with additional information that was not 
possible before. 
It is not only of interest for the Bayer AG to re-
place the old system with such an integrated ap-
proach of perspectives on process documentation but 
also any new system could benefit therefrom. Criti-
cal success factors that have to be kept in mind 
though, are the initial costs for the procurement, roll-
out, model creation and pre-configuration, training, 
and the maintenance of such an integrated model. In 
contrast to this, the current maintenance costs to 
keep the documentation consistent have to taken into 
consideration as well as the benefit in productivity 
the users have when working with integrated and 
configurable models. In the ideal case – after defin-
ing his or her perspective – anyone can work with 
the system and it almost does explain itself for train-
ing purposes. However these advantages are hard to 
quantify since factors like non-productive time due 
to inconsistent, incorrect documentation in connec-
tion with prolonged retrieval time have to be in-
cluded in the calculation. Therefore, a decision has 
to be taken on both, the current and the intended 
purposes of utilization, the expected useful life, and 
the employee’s potential engagement of system use. 
4 CONLUSION AND OUTLOOK 
In this case study we show how the approach of 
configurative modeling from the domain of refer-
ence modeling can be applied to company-specific 
process documentation. 
The integration of different process variants into 
one holistic model does not prove to be beneficial at 
first since the sheer amount of elements and 
branches hinders the understanding of the model. 
However, superior clarity is achieved through the 
projection and configuration of the respective model 
and meta model. This adaptation of the model allows 
the application to various uses that can be roughly 
categorized into application system design, organ-
izational design, and communication design. 
In other respects, the economic evaluation stays 
on a relative level since only a thorough quantitative 
analysis can produce meaningful results that – how-
ever – have to be reviewed against qualitative fac-
tors that are to be explored yet. Questions to be ad-
dressed include but are not limited to sustainability 
metrics for information models, flexibility vs. ro-
bustness metrics and maintainability issues. All 
these consideration are, of course, to be made in 
combination with the model’s economic efficiency. 
ments, e.g. concerning the ease of use and usability 
Jörg Becker et al. 
176

During the last years Bayer has developed and 
improved its process engineering framework, having 
been investigated by this work, in its specification 
and documentation parts. Further on, Bayer has 
researched techniques in collaboration with scien-
tific institutes to proceed with the automation of the 
process development life cycle. The main topics are 
UML specification, UML-based automated testing, 
and tool-based process analysis. 
REFERENCES
Agrawal, A., Levendovszky, T., Sprinkle, J., Shi, F. and 
Karsai, G., 2002. Generative Programming via Graph 
Transformations in the Model-Driven Architecture. In 
Workshop on Generative Techniques in the Context of 
Model Driven Architecture (OOPSLA). Seattle, pp. 1-
11.
Becker, J., Delfmann, P., Dreiling, A., Knackstedt, R. and 
Kuropka, D., 2004. Configurative Process Modeling - 
Outlining an Approach to Increased Business Process 
Model Usability. In Information Resources Manage-
ment Association Conference (IRMA). New Orleans, 
pp. 615-619. 
Becker, J., Delfmann, P., Knackstedt, R. and Kuropka, D., 
2002. Konfigurative Referenzmodellierung. In Wis-
sensmanagement mit Referenzmodellen, (Eds, Becker, 
J. and Knackstedt, R.) Physica-Verlag, Heidelberg, pp. 
25-144.
Chen, P. P.-S., 1976. The Entity-Relationship Model. 
Toward a Unified View of Data. ACM Transactions 
on Database-Systems, 1, pp. 9-36. 
Darke, P. and Shanks, G., 1996. Stakeholder Viewpoints 
in Requirements Definition. Requirements Engineer-
ing, 1, pp. 88-105. 
Engels, G., Heckel, R., Taentzer, G. and Ehrig, H., 1997. 
A View-Oriented Approach to System Modelling 
Based on Graph Transformation. ACM SIGSOFT 
Software Engineering Notes, 22, pp. 327-343. 
ESPRIT Consortium AMICE, 1989. CIM-OSA. Open 
System Architecture for CIM, Springer-Verlag, Berlin 
et. al. 
Ferstl, O. K. and Sinz, E. J., 1998. SOM Modeling of 
Information Systems. In Handbook on Architectures of 
Information Systems, Vol. I (Eds, Bernus, P., Mertins, 
K. and Schmidt, G.) Springer-Verlag, pp. 339-358. 
Findeisen, P., 1994. The Metaview System, Alberta. 
Finkelstein, A., Kramer, J., Nuseibeh, B., Finkelstein, L. 
and Goedicke, M., 1992. Viewpoints: a framework for 
integrating multiple perspectives in system develop-
ment. International Journal of Software Engineering 
and Knowledge Engineering, 2, pp. 31-57. 
Frank, U., 1994. Multiperspektivische Unternehmensmod-
ellierung. Theoretischer Hintergrund und Entwurf 
einer 
objektorientierten 
Entwicklungsumgebung, 
Oldenbourg, München, Wien. 
Hammer, M. and Champy, J., 1993. Reengineering the 
Corporation. A Manifesto for Business Revolution, 
HarperBusiness, New York. 
Ishikawa, K., 1985. What is Total Quality Control? The 
Japanese Way, Prentice Hall, Englewood Cliffs. 
Ledeczi, A., Maroti, M., Bakay, A., Karsai, G., Garrett, J., 
Thomason, C., Nordstrom, G., Sprinkle, J. and Vol-
gyesi, P., 2001. The Generic Modeling Environment. 
In Workshop on Intelligent Signal Processing. Buda-
pest, pp. 19-25. 
Nissen, H. W., Jeusfeld, M., Jarke, M., Zemanek, G. V. 
and Huber, H., 1996. Managing Multiple Require-
ments Perspectives with Metamodels. IEEE Software,
13, pp. 37-48. 
Nordstrom, G., Sztipanovits, J., Karsai, G. and Ledeczi, 
A., 1999. Metamodeling - Rapid Design and Evolution 
of Domain-Specific Modeling Environments. In IEEE 
ECBS Conference. Nashville, pp. 68-74. 
Rosemann, M., 1998. Managing the Complexity of Multi-
perspective Information Models using the Guidelines 
of Modeling. In 3rd Australian Conference on Re-
quirements Engineering. Geelong, pp. 101-118. 
Rosemann, M. and Green, P., 2000. Integrating multi-
perspective views into ontological analysis. In 21st In-
ternational Conference on Information Systems. Bris-
bane, pp. 618-627. 
Rosemann, M., Schwegmann, A. and Delfmann, P., 2005. 
Preparation of Process Modeling. Appears in Process 
Management. A Guide for the Design of Business 
Processes, (Eds, Becker, J., Kugeler, M. and Rose-
mann, M.) Springer-Verlag, Berlin et al., 2nd Edition. 
Scheer, A.-W., 2000. ARIS - Business Process Modeling, 
Springer-Verlag, Berlin, 3rd Edition. 
Soley, R. and OMG Staff Strategy Group, 2000. Model 
Driven Architecture (White Paper), Object Manage-
ment Group, Needham. 
Zachman, J. A., 1987. A Framework for Information 
Systems Architecture. IBM Systems Journal, 26, pp. 
277-293.
Perspectives on Process Documentation 
177

SUSTAINABLE DEVELOPMENT AND INVESTMENT IN 
INFORMATION TECHNOLOGIES 
A Socio-Economic Analysis
Manuel João Pereira 
Instituto Nacional de Administração, Palácio dos Marqueses de Pombal, Oeiras, Portugal 
Email: mjp@ina.pt 
Luís Valadares Tavares 
Instituto Nacional de Administração, Palácio dos Marqueses de Pombal, Oeiras, Portugal 
Email: lvt@ina.pt 
Raquel Soares 
Universidade Católica Portuguesa, Palma de Cima, Lisboa, Portugal 
Email: raquel.soares@cepi.ucp.pt  
Keywords: 
Information technology, Sustainable Development, Economy. 
Abstract: 
The output of investments in Information Systems and Technologies (IST) has been a topic of debate among 
the IST research community. The “Productivity Paradox of IST Investments” sustains that the investment in 
IST does not increase productivity. Some researchers showed that developed countries have been having a 
rather stable and sometimes declining economic growth despite their efforts in Research and Development 
(R&D). Other researchers argue that there is sound evidence that investments in IST are having impacts on 
the productivity and competitiveness of countries. This paper analyses the relationship between IST and 
R&D investments and the global development of countries (not only productivity of countries) using 
economic, demographic and literacy independent variables that explain global development. The objective 
is to research whether R&D and IST investments are critical to the productivity and to global development 
of the countries. Working at a country level, the research used sixteen socio-economic variables during a 
period of five years (1995-1999). The research methodology included causal forecast, cluster analysis, 
factor analysis, discriminant analysis and regression analysis. The conclusion confirms the correlation 
between the Gross National Product (GNP) and R&D and IST investments. The variables illiteracy rate, life 
expectancy at birth, Software investment as percentage of GNP and number of patents per 1000 inhabitants 
can explain the development of a country.  
1 INTRODUCTION
Research on the relationship between technology 
and economic growth started long ago and has been 
studied by several authors. Arrow (1962), on the 
other hand, suggested that endogenizing the change 
in technology, the long-term economic growth 
depends of population growth. Uzawa (1965), 
Phelps (1966), Ackoff (1967), Conlisk (1967,1969) 
and Shell (1967) developed studies in the area of 
technological growth and development of new 
technologies. Castells (1997) showed that there is a 
relationship between the demographic position and 
the development of the country/area. More recently, 
Romer (1990), Grossmann (1991), Allen (1997), 
Pereira (2004) and Tavares (2002), all share the idea 
that persistent investment in new information 
technologies conducts to continuous economic 
growth. 
The debate on the productivity paradox of IST 
investments has several justifications. Jones (1995) 
showed that the number of researchers working in 
R&D (generally accepted as an indicator of the state 
of technology) in developed countries has increased 
substantially over the post-war period, while the 
economic growth has hardly changed. He tried to 
explain the contrast between the state of technology 
and the economic growth, holding that the 
movement of other variables, different from the state 
179
C.-S. Chen et al. (Eds.), Enterprise Information Systems VII, 179–186.
© 2006 Springer.

of technology, affected the economic growth 
permanently and suggested that continuous policy 
measures that probably should have permanent 
effects on economic growth do not have.  
Another well-known author of the productivity 
paradox of IST investments, Paul Strassmann 
(1997), indicates that productivity of a country or a 
company, must be the result of a good economic and 
financial strategy because economic figures are 
more important then the technical decisions when it 
comes to invest in IST. Only this way, this 
investment can be profitable and therefore contribute 
to productivity growth. The market pressure for 
higher productivity drives decision makers to big 
investments in IST, sometimes without an objective, 
quantitative knowledge of the markets and strategic 
positioning. Other authors used a micro-economic 
approach to study this question (Alpar and 
Kim,1997). 
Several 
authors 
and 
researchers 
indicate 
explanations to these findings:  
a)
R&D statistics do not show all efforts 
attributed to the technological progress 
(mainly the efforts from SME´s), Kraemer 
and Deadrick (1996); 
b)
In order to achieve full use of technologies 
there has to be both a change in the 
organisational 
structure 
and 
the 
development 
of 
complementary 
technologies (David, 1990); 
c)
Investments in IST have been directed to 
product differentiation and less to effective 
innovation, increasing costumers welfare 
but 
not 
economic 
growth 
(Soete, 
1996;Young, 1998); 
d)
Changes in the economy induces changes 
in the investment of companies and in 
consumer preferences (Kurdas,1994): when 
the economy grows both companies and 
consumers spend a lot on a wide variety of 
products and services. In the opposite 
scenario, real interest rates rises, consumers 
tend to spend on essential products while 
companies discard their risky efforts in 
R&D and IST and invest in the existing 
products. During this period, companies 
tend to choose the self-financing option 
instead of looking up for funding in the 
financial market (Kurdas, 1994). 
The new technologies are used to create more 
flexibility in the internal processes and empower the 
workers (Alpar and Kim, 1991; Baily and 
Gordon,1998; Laudon, 1974, 1986; Barua et al 
1995). 
The 
so-called 
Information 
and
Communication Technologies cannot be analysed 
separately but integrated with the surrounding 
environment, including the impact in the business 
areas and the relations between those areas (Young, 
1998). There are activities within the organisation 
that do not create value directly (administration, HR 
management, R&D, for instance) but are essential to 
the well functioning of the organisation since they 
support, 
complement 
and 
empower 
primary 
activities (Porter, 1985). 
Technology is traditionally used to transform 
existing activities improving the efficiency of the 
processes and the time of diffusion is probably not 
enough to get the real output generated by IST 
investments (David, 1990; Dewan, et al, 1992; 
Devarej, and Mohli, 2003; Wilcocks and Lester, 
1999).  
The type of methodology used to understand the 
impact of IST investments on organisations is 
sometimes not adequate (Allen, 1997; Brynjolfsson 
and Hitt, 1999; Barua et al, 1991; Mckeen et all, 
1997; Nissen et al, 1998; Orlikowski, 1996) this 
discussion is still an up-to-date research topic 
(Bauker and Kauffman, 2004). 
The main motivation of this research is to help to 
determine an answer to the following question: is the 
investment in IST and in R&D a relevant factor for 
the sustainable development of the countries? The 
next chapter will present the hypotheses and the 
methodology to understand this relationship. 
2 HYPOTHESIS 
The previous debate contributed to the generation of 
the following hypothesis of this research: 
H1: The investments in R&D and in IST are 
correlated with the global level of development of 
the countries. The confirmation of this hypothesis 
implies that higher investments in R&D and IST 
lead to a higher global development of a country. 
These hypotheses can help to find the answer to 
the following two objectives: 
x
Understand 
the 
relationship 
between 
technology, 
sustainable 
development 
and 
productivity of the countries. 
x
Study the impact of R&D effort on the global 
development and productivity of the countries. 
3 METHODOLOGY 
The methodology used in this research includes 
causal forecast, cluster analysis, factor analysis, 
discriminate analysis, regression analysis and 
descriptive statistics. The main steps of the 
methodology are: 
Manuel João Pereira et al. 
180

a) data 
gathering 
about 
socio-economic 
and 
technology variables of countries (OECD, 2002). 
The following countries are included in this analysis: 
Portugal, Greece, Spain, Italy, Korea, Ireland, 
Australia, Finland, Denmark, Holland, France, 
Belgium, Austria, Canada, Norway, Germany, 
Switzerland, Sweden, United Kingdom, Japan and 
USA. The period of analysis is five years (1995-
1999).  
b) identification of basic relationship between R&D 
investment and GDP of countries during a longer 
period of analysis (1981-1999). Correlation between 
both variables and identification of the time gap 
between R&D investment and GDP impact for a set 
of countries using causal forecast analysis.  
c) cluster analysis of global development of the 
countries. The following variables are included in 
the analysis for each country (OECD, 2002) and this 
choice was based in previous studies (Alpar and 
Kim, 1991; Baily and Gordon,1998; Brynjolfsson 
and Hitt, 1996; Pereira, 2004): GDP per hour 
worked (United States = 100); Life expectancy 
(years); GDP per capita (United States = 100); 
Personal computers per 1,000 inhabitants; Gross 
domestic expenditure in R&D; Software investment 
as percentage of GDP; Number of patents per 1000 
inhabitants; Share of high-technology investment as 
percentage 
of 
total 
venture 
capital 
of 
the 
communications sector; Electric power consumption 
(kwh per 
capita); 
Share 
of high-technology 
investment as percentage of total venture capital of 
information 
technology 
sector; 
Information 
exportation technology (percentage of manufactured 
exportation); 
Illiteracy 
rate; 
Share 
of 
high-
technology investment as percentage of total venture 
capital of health/biotechnology sector; Internet hosts 
per 1000 inhabitants; Internet users per 1000 
inhabitants; Telecommunications channels per 1000 
inhabitants. 
 
The analysis will identify clusters of countries 
with different levels of development based on the 
average values in a period of time of all variables in 
each cluster. The time frame of analysis is five 
years, although for some variables, due to the lack of 
data, the period is three years. 
d) Discriminant analysis to determine which are the 
characteristics that distinguish the members of one 
group from the members of the other group. 
Knowing the data of a country, we can predict to 
which cluster it belongs. 
 
The factorial analysis allows to transform a set of 
original correlated variables in a smaller number of 
hypothetical variables (Principal Components), not 
correlated between each other, without loosing 
significant information from the original variables. 
Each principal component derives from a linear 
combination of all original variables. 
e) Using variables that reflect the effort in IST and 
R&D, a regression analysis is designed to correlate 
them with the GNP. A basic analysis of these three 
variables is also performed keeping each cluster 
together, in order to determine if there is, in fact, a 
relevant difference of investment in IST, in R&D 
and of GDP between clusters of countries with 
different global development.  
4 ANALYSIS 
The following subchapters describe the application 
of the methodology presented in chapter 3.
4.1 R&D and GDP: Causal Forecast 
The first step is to understand the relationship 
between R&D and GDP across different countries. 
Correlation between investment in R&D and 
GDP in the same year for US, Japan, EU and OECD 
countries is strong and positive (0,97 for US, 0.99 
for Japan, 0.95 for the EU and 0.98 for the OECD 
countries as we can see in tables 1, 2, 3 and 4, 
leading us to believe that the investment in R&D 
depends on the immediate resources generated by 
the economy. 
 The causal forecast analysis of GDP using 
investment in R&D as dependable variable, allowed 
us to understand that not only R&D is highly 
influenced by the GDP of each year, but GDP itself 
is influenced by the investment in R&D in previous 
years, with different time gaps depending on the 
research efficiency and capacity of the economy to 
absorb innovation. 
In Japan the effects of R&D in the GDP appear 7 
years after the investments. In US, the return of the 
R&D investment happens after 13 years, in UE after 
10 years and in OECD after 11 years. In summary, 
Japan has a faster return on R&D then the EU, the 
OECD countries and US, in this order. 
4.2 Cluster Analysis 
The cluster analysis is the second step. A five-year 
average of the following statistics are used for this 
analyses: Electric power consumption, kwh per 
capita (A), GDP per capita, United States = 100 (B), 
Information exportation technology (percentage of 
manufactured exportation) (C), Illiteracy rate (D), 
Internet users per 1000 inhabitants (E), Life 
expectancy (F), Personal computers (per 1,000 
inhabitants) 
(G), 
Share 
of 
high-technology
Sustainable Development and Investment in Information Technologies 
181

Table 1: Casual Forecast – R&D; GDP – US                                    Table 2: Casual Forecast – R&D; RDP - Japan 
R&D Billions 
Dollars 
GDP
Provisional 
GDP
Correlation 
R&D Billions 
Dollars 
GDP 
Provisional 
GDP
Correlation
1981 
116 
4902
 
0 
0,9748 
 
1981 
33 
1537 
 
0 
0,9933 
1982 
121 
4796
 
1 
0,9677 
 
1982 
35 
1584 
 
1 
0,9527 
1983 
130 
4993
 
2 
0,9448 
 
1983 
38 
1619 
 
2 
0,8934 
1984 
142 
5358
 
3 
0,9094 
 
1984 
41 
1682 
 
3 
0,8569 
1985 
154 
5557
 
4 
0,8751 
 
1985 
45 
1758 
 
4 
0,8669 
1986 
159 
5745
 
5 
0,8493 
 
1986 
46 
1809 
 
5 
0,8971 
1987 
162 
5948
 
6 
0,8631 
 
1987 
49 
1882 
 
6 
0,9397 
1988 
166 
6185
 
7 
0,8880 
 
1988 
53 
1996 
1962 
7 
0,9462 
1989 
169 
6412
 
8 
0,9059 
 
1989 
58 
2095 
2039 
8 
0,9129 
1990 
173 
6518
 
9 
0,9294 
 
1990 
63 
2206 
2131 
9 
0,8673 
1991 
177 
6494
 
10 
0,9519 
 
1991 
64 
2286 
2222 
10 
0,8365 
1992 
177 
6679
 
11 
0,9627 
 
1992 
64 
2309 
2366 
11 
0,8840 
1993 
173 
6865
 
12 
0,9777 
 
1993 
62 
2316 
2391 
12 
0,8783 
1994 
173 
7147
7161 
13
0,9948 
 
1994 
61
2335 
2493 
13
0,7977 
1995 
184 
7348
7323 
14 
0,9901 
 
1995 
65 
2362 
2614 
14 
0,5949 
1996 
193 
7608
7590 
15 
0,9779 
 
1996 
85 
2986 
2772 
 
 
1997 
204 
7946
7973 
 
 
 
1997 
88 
3038 
2927 
 
 
1998 
215 
8282
8365 
EQM
2610 
 
1998 
90 
2961 
2978 
 
 
1999 
226 
8577
8494 
Intercept 51  
1999 
90 
2961 
2955 
 
Intercept 
2000 
 
 
8594 
 
3510,12 
 
2000 
 
 
2901 
 
912,42 
2001 
 
 
8719 
 
Slop 
 
2001 
 
 
2881 
 
Slop 
2002 
 
 
8830 
 
31,43 
 
2002 
 
 
3009 
 
32,05 
2003 
 
 
8939 
 
 
 
2003 
 
 
3621 
 
 
2004 
 
 
9062 
 
 
 
2004 
 
 
3735 
 
 
2005 
 
 
9072 
 
 
 
2005 
 
 
3797 
 
 
2006 
 
 
8947 
 
 
 
2006 
 
 
3797 
 
 
2007 
 
 
8946 
 
 
 
 
 
 
 
 
 
2008 
 
 
9283 
 
 
 
 
 
 
 
 
 
2009 
 
 
9583 
 
 
 
 
 
 
 
 
 
2010 
 
 
9928 
 
 
 
 
 
 
 
 
 
2011 
 
 
10278 
 
 
 
 
 
 
 
 
 
2012 
 
 
10626 
 
 
 
 
 
 
 
 
 
Data Source: OECD, Analysis by the authors 
 
Table 3: Casual Forecast, R&D, GDP – EU                                    Table 4: Casual Forecast, R&D, GDP - OCDE 
R&D Billions 
dollars 
GDP 
Provisional 
GDP
Correlation
R&D Billions 
Dollars 
GDP 
Provisional 
GDP
Correlation
1983 
97 
5558 
 
0 
0,9539 
1981 
261 
13236 
 
0 
0,9825 
1984 
101 
5693 
 
1 
0,9301 
1982 
273 
13239 
 
1 
0,9806 
1985 
109 
5851 
 
2 
0,8967 
1983 
287 
13621 
 
2 
0,9751 
1986 
114 
5991 
 
3 
0,8604 
1984 
309 
14240 
 
3 
0,9697 
1987 
119 
6184 
 
4 
0,8250 
1985 
336 
14751 
 
4 
0,9670 
1988 
124 
6433 
 
5 
0,8032 
1986 
347 
15223 
 
5 
0,9672 
1989 
129 
6675 
 
6 
0,8117 
1987 
360 
15785 
 
6 
0,9801 
1990 
133 
6788 
 
7 
0,8598 
1988 
374 
16468 
 
7 
0,9879 
1991 
130 
6839 
 
8 
0,9170 
1989 
390 
17087 
 
8 
0,9859 
1992 
130 
6885 
 
9 
0,9788 
1990 
403 
17531 
 
9 
0,9842 
1993 
129 
6888 
6875 
10
0,9903 
1991 
417 
18623 
 
10 
0,9915 
1994 
130 
7084 
7021 
11 
0,9827 
1992 
419 
19042 
18900 
11 
0,9959 
1995 
131 
7242 
7278 
12 
0,9853 
1993 
415 
19291 
19376 
12 
0,9910 
1996 
133 
7362 
7418 
13 
0,9924 
1994 
418 
19900 
19960 
13 
0,9782 
1997 
136 
7534 
7593 
14 
0,9898 
1995 
442 
20947 
20819 
14 
0,9783 
1998 
140 
7754 
7746 
 
 
1996 
462 
21593 
21905 
 
 
1999 
148 
7984 
7915 
 
 
1997 
482 
22332 
22333 
 
 
2000 
 
 
8028 
 
Intercept 
1998 
500 
22921 
22843 
 
Intercept 
2001 
 
 
7929 
 
3806,43 
1999 
519 
23506 
23397 
 
8530,64 
2002 
 
 
7935 
 
Slop 
2000 
 
 
24023 
 
Slop 
2003 
 
 
7915 
 
31,73 
2001 
 
 
24565 
 
39,77 
2004 
 
 
7920 
 
 
2002 
 
 
25120 
 
 
2005 
 
 
7966 
 
 
2003 
 
 
25190 
 
 
2006 
 
 
8034 
 
 
2004 
 
 
25025 
 
 
2007 
 
 
8109 
 
 
2005 
 
 
25150 
 
 
2008 
 
 
8259 
 
 
2006 
 
 
26107 
 
 
2009 
 
 
8493 
 
 
2007 
 
 
26907 
 
 
 
 
 
 
 
 
2008 
 
 
27713 
 
 
 
 
 
 
 
 
2009 
 
 
28401 
 
 
 
 
 
 
 
 
2010 
 
 
29189 
 
 
Data Source: OECD, Analysis by the authors  
Manuel João Pereira et al. 
182

Figure 1: Cluster analysis – Tree Diagram, Data Source: 
investment as percentage of total venture capital of 
the communications sector (H), Share of high-
technology investment as percentage of total venture 
capital of information technology sector (I), Share of 
high-technology investment as percentage of total 
venture capital for health/biotechnology sector(J), 
Internet 
hosts 
per 
1000 
inhabitants 
(K), 
Telecommunication channels per 1000 inhabitants 
(L), GDP per hour worked, United States = 100 (M), 
Number of patents per 1000 inhabitants (N), 
Software investment as percentage of GDP (O), 
Gross domestic expenditure in R&D (P). 
According to the cluster analysis using the 
countries mentioned in chapter 3, two clusters of 
countries emerged. Cluster 1 formed by Greece, 
Ireland, Korea, Portugal, Spain, with lower average 
levels on all indicators. Cluster 2 formed by 
Australia, Austria, Belgium, Canada, Denmark, 
Finland, France, Germany, Italy, Japan Netherlands, 
Norway, Sweden, Switzerland, United Kingdom and 
US with higher average levels in all indicators. 
4.3 Discriminant Analysis 
The objective of the discriminant analysis is to 
determine which are the characteristics that 
distinguish the members of one group from the 
members of the other. One or more classification 
functions (multivariable functions) are determined 
for each cluster, in order to maximise the difference 
between the groups. After the calculation of the 
discriminant functions we have to select the ones 
that are relevant (F value >4 and a p value <5%). 
The solution of the discriminant analysis (table 
5) showed that variables Illiteracy rate (D), Life 
expectancy 
(F),
Software 
investment 
as 
percentage of GDP (O) and Number of patents 
per 1000 inhabitants (N) are enough to classify 
countries as belonging to cluster 1 or to cluster 2. 
4.4 Factorial Analysis 
To determine which principal components are 
designed a combination of three conditions should 
be verified: a) to retain the first order factors until 
the eigenvalue has a abrupt fall b) to hold the 
components that explain a significant percentage of 
the total variance, usually above 70% c) and finally, 
to exclude the components that have an eigenvalue 
under one. The rotation of the principal components 
turns it easier to understand the dimension that each 
component 
represents. 
Four 
dimensions 
(components) were identified as table 7 shows. 
The analyses of the factor loadings (varimax 
normalized) showed the following (table 8): 
a)  the first dimension, explaining 43,98% of the 
development includes the variables: 
A) GDP per capita 
E) Internet users per 1000 inhabitants 
N) Software investment 
b)
the second dimension, explaining extra 
14,27% of the development includes the 
variables investment in IT venture capital 
(H,I). 
c)
the third dimension, explaining extra 
10,18% of the development includes the 
variables: 
F) Personal computers per 1000 
inhabitants 
H) Number of patents registed  
These 
variables 
explain 
68,43% 
of 
the 
development of the countries analysed. As we can 
see, the IST variables are relevant to the 
development of the countries. 
Sustainable Development and Investment in Information Technologies 
183
OECD, Analysis by the authors.
Figure 2: Cluster analysis – Plot of Means. 

Table 7: Eigenvalues of Principal Components 
 
Eigenvalue 
% Total Variance 
Cumulative Eigenvalue 
Cumulative % 
1 
7,0365 
43,9782 
7,0365 
43,9782 
2 
2,2828 
14,2675 
9,3193 
58,2457 
3 
1,6289 
10,1809 
10,9483 
68,4267 
4 
1,3282 
8,3012 
12,2765 
76,7279 
5 
1,0077 
6,2981 
13,2842 
83,0259 
Table 8: Factor Loadings (Varimax normalized) 
Principal components (Marked loadings are > 0,700000) 
 
Factor 1 
Factor 2 
Factor 3 
Factor 4 
A
0,776441 
0,313813 
0,226774 
0,217372 
B 
0,069716 
0,365983 
0,676391 
0,562182 
C 
-0,06284 
0,591424 
-0,07712 
0,283421 
D 
-0,08924 
-0,18102 
-0,44036 
-0,63511 
E
0,907184 
-0,06792 
0,097383 
0,083256 
F 
0,26646 
-0,09935 
0,830615 
0,016621 
G 
0,420495 
0,380441 
0,401267 
0,644132 
H 
0,212596 
0,790361 
0,240486 
-0,24583 
I 
0,169086 
0,880694 
-0,05586 
0,315324 
J 
0,118514 
0,211158 
-0,10157 
0,836123 
K 
0,587552 
0,387134 
0,208871 
0,46764 
L 
0,409986 
0,031861 
0,286064 
0,776305 
M 
0,027451 
0,152493 
0,797236 
0,277228 
N
0,808623 
-0,13354 
0,022294 
0,237529 
O 
0,277521 
0,082643 
0,410776 
0,779332 
P 
-0,27323 
0,614356 
0,280418 
0,20938 
Expl.Var 
3,09787 
2,79531 
2,6494 
3,733877 
Prp.Totl 
0,193617 
0,174707 
0,165587 
0,233367 
 
 
 
 
 
4.5 Regression Analysis
Analysing the GDP per 100 inhabitants, the 
investment in R&D per 100 inhabitants and the 
investment in software per 100 inhabitants in 1999, 
keeping each cluster together, conclusions of the 
cluster analysis are reinforced. 
The GDP per 100 inhabitants is higher among 
countries of cluster 1 then among countries of 
cluster 2. The same conclusion follows the analysis 
of the investment in R&D and in Software per 100 
inhabitants. 
The correlation between these three variables is 
high, as displayed in table 9 and shown in figure 6. 
The Investment in R&D, investment in Software, 
and the GDP are variables correlated. 
Figures 3, 4 and 5 :GDP per 100 inhabitants, Investment in R&D per 100 inhabitants and investment in software per 100 
Manuel João Pereira et al. 
184
inhabitants in US dollars, indexed to 1995. Data Source: OECD, Analysis by the authors.

Table 9: R-squared values 
R2 
R&D investment 
Software investment 
GDP 
0,756 
0,7742 
Figure 6: GDP, Investment in software and in R&D per 100 inhabitants in US dollars, 1999, indexed to 1995 (GDP per 100 
5 CONCLUSIONS AND FUTURE 
RESEARCH
The conclusion about the hypothesis formulated is 
the following: 
A stronger effort of investment in IST creates a 
higher sustained development of a country – is 
confirmed.
The temporal series analysis shows that there is a 
causal relationship between investment in R&D and 
productivity. The cluster analysis (figure2) shows 
that more developed countries have higher levels of 
investment 
in 
R&D 
and 
higher 
levels 
of 
productivity. The discriminant analysis shows that 
four variables are enough to classify countries
according 
to 
their 
maturity 
of 
sustained 
development. From these four variables, one is an 
indicator of IST (investment in software) and the 
other a R&D indicator (number of patents). 
GDP is positively and strongly correlated with 
the level of investment in R&D and the level of 
investment in Software. More developed countries 
also show better figures of these three variably. 
Finally, the return, of the financial effort in R&D is 
not the same for all countries, showing the research 
the Japan is the country that profits faster its 
investments (7 years). 
However, several developments can improve 
their work. Future research should increase the time 
dimension of the analysis. The methodology should 
be applied to a different set of time periods of seven 
years, ten and twenty years. The type of variables 
can also be argued. An important difficulty, already 
mentioned by previous researchers (Byrd and 
Marschall, 1997; Gurbaxani and Whang, 1991; Im et 
all 2001; Devaraj and Kohli, 2003; Pereira, 2004), 
was to select the significant socio-economic and 
technological variables. The use of variables 
describing in a even more robust way the sustainable 
development of a country, the productivity of a 
country and the state of the IST and R&D of a 
country, can complement future analysis.  
The contribution of this paper to the field is to 
confirm the importance of IST investments in the 
sustainable development of the countries. 
To summarise, this research concludes that IST 
and R&D variables should not be neglected by 
decision 
makers 
to 
achieve 
a 
sustainable 
development of a country.  
REFERENCES
Ackoff, R.L., (1967). “Management Misinformation 
Systems”, Management Science, 14, 4, B140-B156, 
December. 
Allen, D. (1997). “Where’s the Productivity Growth (from 
the information Technology Revolution)?”, Federal 
Reserve Bank of St. Louis Review, March/April, 
79,2,15-25. 
Alpar, P. and Kim M. A. (1991). “Microeconomic 
Approach to the Measurement of Information 
Technology 
Value”, 
Journal 
of 
Management 
Information Systems, 7, 2, 55-69. 
Arrow (1962). “The Economic Implications of Learning 
by Doing”. Economic Studies, 29. 
Bauker, R. D. and Kauffman, R. J. (2004), “The Evolution 
of Research on Information Systems: a fiftieth- year 
Sustainable Development and Investment in Information Technologies 
185
inhabitants presented in circles).
 

Survey of the Literature in Management Science”, 
Management Science, Vol. 50, nº 3. March 2004, pp. 
281-298.   
Baily, M.N. and Gordon, R.J. (1998). “The Productivity 
Slowdown, Measurement Issues and the Explosion of 
Computer Power”. In Brainard, W.C. and Perry, G.L. 
(eds) Brookings Papers on Economic Activity, The 
Brookings Institution, Washington, DC, 347-431. 
Barua, A., Kriebel, C. H., Mukhopadhyay, T., (1991). “An 
Economic 
Analysis 
of 
Strategic 
Information 
Technology Investments”, MIS Quarterly, September, 
pp. 313-332. 
Barua, A., Kriebel, C. H., Mukhopadhyay, T., (1995). 
“Information Technologies and Business Value: An 
Analytic and Empirical Investigation”, Information 
Systems Research 6(1), pp. 3-23. 
Brynjolfsson, E. e Hitt, L. (1996). “Productivity, Business 
Profitability and Consumer Surplus: Three different 
measures of Information Technology Value”, MIS 
Quarterly, 20, 2, 121-142, June. 
Brynjolfsson, E. e Hitt, L., (1999). “Paradox Lost? Firm 
level evidence on the returns to Information Systems 
Spending”. In Willcocks, L. e Lester S. “Beyond the 
IT Productivity Paradox”, Wiley, pp. 39-68. 
Byrd, T. and Marshall, T. (1997). “Relating Information 
Technology 
Investment 
to 
Organizational 
Performance”. Omega, 25, 1, 43-56. 
Castells, M. (1997), “The End of the Millennium, The 
Information Age: Economy, Society and Culture”, 
Vol. III, Cambridge, MA, Backwell. 
Conlisk (1969). “A Neoclassical Growth Model with 
Endogenously 
Positioned 
Technical 
Change 
Frontiers”. Economic Journal, nº 79. 
David (1990). “The Dynamo and the Computer”. 
American Economic Review, nº 80. 
Devaraj, S. and Kohli (2003). “Performance Impacts of 
Information Technology: Is actual usage the missing 
link?”, Management Science, 49, 11-273-289. 
Grossman (1991). “Innovation and Growth in the Global 
Economy”. MIT Press. 
Gurbaxani, V. and Whang, S., (1991). “The impact of 
Information Systems on Organisations and Markets”, 
Communications of the ACM, 34, nº1, January. 
Im, K.S., Dow, K.E. e Grover V. (2001), “Research 
Report: A Reexamination of IT Investment and the 
Market Value of the Firm – an event study 
methodology”, Information Systems Research, Vol. 
12, nº1, pp. 103-117. 
Jones (1995). “Time Series Tests of Endogenous Growth 
Models”. Quarterly Journal of Economics, 11; 27-32. 
Kraemer, K. and Dedrick, J. (1996). “IT and Economic 
Development: International Competitiveness”. In 
Dutton, W (ed.), Information and Communication 
Technologies. Oxford University Press, Oxford. 
Kurdas (1994), “Theories of Technological Change and 
Investment”. John Wiley. 
Laudon, K., (1974). “Computers and Bureaucratic 
Reform”. New York: John Willey. 
Laudon, K., (1986). “The Dossier Society: value choices 
in the design of National Information Systems”. New 
York: Columbia University Press. 
Mckeen, J., Smith, H. and Parent, M. (1997). “Assessing 
the Value of Information Technology: the leverage 
effect”. Proceedings of the Fifth European Conference 
on Information Systems, Cork, June 9-12th , 11; 77-89. 
Nissen, H.E., Klein, H.K. and Hirscheim; R. (1998), 
“Information 
Systems 
Research: 
Contemporary 
Approaches and Emergent Traditions”. Amsterdam: 
North-Holland.
OECD (2002), Technology and the Economy: Key 
Relationships.
Orlikowski, W. J. (1996). “Improvising Organizational 
Transformation over time: A Situated Change 
Perspective”. Information Systems Research, Volume 
7, Number 1, pp. 63-92. 
Pereira, M.J. (2004) “Impacts of information systems and 
technology on productivity and competitiveness of the 
Portuguese banking sector: an empirical study, 
International Transactions in Operational Research,
Vol. 11, nº 9, January 2004, 11; 43-63. 
Phelps (1966). “Models of Technical Progress and the 
Golden Rule of Research”. Review of Economic 
Studies, 33. 
Porter, M. (1985). Competitive advantage of nations. New 
York: Free Press. 
Romer (1990), “Endogenous Technological Change”. 
Journal of Political Economy, 98. 
Shell (1967). “A Model of Innovative Activity and Capital 
Accumulation”, Essays on the Theory of Optimal 
Economic Growth, Conference Proceedings, 11; 1-29. 
Soete (1996). “Uncertainty and Technological Change”, 
Technology and Growth Conference Proceedings, 11; 
27-39.
Strassman, P. (1997). “The Squaudered Computer”. 
Information Economics Press, New Canaan. 
Tavares, L. (2003), “Engineering and Technology: 2000 – 
2020; Foresight Challenges for Portugal”, Verbo. 
Uzawa (1965). “Optimum Technical Change in an 
Aggregate Model of Economic Growth”. International 
Economic Review ,6. 
Willcocks, L. and Lester S. (1999). “In Search of 
Information Technology Productivity: Assessment 
Issues”. In Willcocks, L. e Lester S. (eds.) “Beyond 
the IT Productivity Paradox”, Wiley, pp. 69-98. 
Young (1998). “Growth without Scale Effects”. Journal of 
Political Economy,106.
Manuel João Pereira et al. 
186

QUALITY OF SERVICE IN FLEXIBLE WORKFLOWS 
THROUGH PROCESS CONSTRAINTS 
Shazia Sadiq and Maria Orlowska 
School of Information Technology and Electrical Engineering 
The University of Queensland, St Lucia, Brisbane, Australia 
Email: {shazia, maria}@itee.uq.edu.au  
Joe Lin and Wasim Sadiq 
SAP Research Centre, Brisbane, Australia  
Email: jlin@itee.uq.edu.au; wasim.sadiq@sap.com
Keywords:  
Flexible Workflows, Workflow Modelling, Process Constraints. 
Abstract:  
Workflow technology has delivered effectively for a large class of business processes, providing the 
requisite control and monitoring functions. At the same time, this technology has been the target of much 
criticism due to its limited ability to cope with dynamically changing business conditions which require 
business processes to be adapted frequently, and/or its limited ability to model business processes which 
cannot be entirely predefined. Requirements indicate the need for generic solutions where a balance 
between process control and flexibility may be achieved. In this paper we present a framework that allows 
the workflow to execute on the basis of a partially specified model where the full specification of the model 
is made at runtime, and may be unique to each instance. This framework is based on the notion of process 
constraints. Where as process constraints may be specified for any aspect of the workflow, such as 
structural, temporal, etc. our focus in this paper is on a constraint which allows dynamic selection of 
activities for inclusion in a given instance. We call these cardinality constraints, and this paper will discuss 
their specification and validation requirements.  
1 INTRODUCTION
Process enforcement technologies have a dominant 
role in current enterprise systems development. It 
has been long established that automation of specific 
functions of enterprises will not provide the 
productivity gains for businesses unless support is 
provided for overall business process control and 
monitoring. Workflows have delivered effectively in 
this area for a class of business processes, but typical 
workflow systems have been under fire due to their 
lack of flexibility, i.e., their limited ability to adapt 
to changing business conditions. In the dynamic 
environment of e-business today, it is essential that 
technology supports the business to adapt to 
changing conditions. However, this flexibility 
cannot come at the price of process control, which 
remains an essential requirement of process 
enforcement technologies.
Providing a workable balance between flexibility 
and control is indeed a challenge, especially if 
generic solutions are to be offered. Clearly there are
parts of the process which need to be strictly
controlled through fully predefined models. There 
can also be parts of the same process for which some
level of flexibility must be offered, often because the 
process cannot be fully predefined due to lack of 
data at process design time. For example, in call 
centre responses, where customer inquiries and 
appropriate response cannot be completely pre-
defined, or in higher education, where study paths 
resulting from individual student preferences cannot
be entirely anticipated.  
In general, a process model needs to be capable 
of capturing multiple perspectives (Jablonki & 
Bussler, 1996), in order to fully capture the business 
process. There are a number of proposals both from 
research and academia, as well as from industry on 
the modelling environment (language) that allows 
these perspectives to be adequately described.
187
C.-S. Chen et al. (Eds.), Enterprise Information Systems VII, 187–195.
© 2006 Springer.

e.g. (Sadiq & Orlowska, 1999), (Casati et al 1995), 
(van der Aalst, 2003), although most focus on the 
control flow (what activities are performed and in 
what order).  
Basically these perspectives are intended to 
express the constraints under which the business 
process can be executed such that the targeted 
business goals can be effectively met. We see two 
fundamental classes of these constraints: 
Process level constraints: This constitutes the 
specification of what activities must be included 
within the process, and the flow dependencies within 
these activities including the control dependencies 
(such as sequence, alternative, parallel etc.) and 
temporal dependencies (such as relative deadlines).  
Activity level constraints: This constitutes the 
specification of various properties of the individual 
activities within the process, including activity 
resources (applications, roles and performers), data 
(produced and/or consumed), and time (duration and 
deadline constraints).  
In this paper, we focus on the flexible definition 
of process level constraints. We see the level of 
definition of these constraints along a continuum of 
specification There is the completely predefined 
model on one end, and the model with no 
predefinition on the other. Thus the former only has 
strong constraints (e.g. A and B are activities of a 
given process, and B must follow A), and the latter 
no constraints at all. The former extreme is too 
prescriptive and not conducive to dynamic business 
environments; and the latter extreme defeats the 
purpose 
of 
process 
enforcement, 
i.e. 
with 
insufficient constraints, the process goals may be 
compromised and quality of service for the process 
cannot be guaranteed. Finding the exact level of 
specificity along this continuum will mostly be 
domain dependent. However, technology support 
must be offered at a generic level. There is a need to 
provide a modelling environment wherein the level 
of specification can be chosen by the process 
designer such that the right balance between 
flexibility and control can be achieved.  
The work presented in this paper basically 
discusses flexible process definition for a particular 
class of constraints. In essence, a small number of 
constraints are specified at design time, but the 
process instances are allowed to follow a very large 
number of execution paths. As long as the given 
constraints are met, any execution path dynamically 
constructed at runtime is considered legal. This 
ensures flexible execution while maintaining a 
desired level of control through the specified 
constraints.
In the following sections, we first present the 
modelling framework which allows flexible process 
definition. We will then present the details of the 
constraint specification and validation. In the 
remaining 
sections, 
we 
will 
present 
some 
background related work to appropriately position 
this work, and finally a summary of this work and its 
potential extensions.  
2 MODELING FRAMEWORK 
The 
modelling 
framework 
required 
for 
the 
specification of process constraints is simple and has 
minimal impact on the underlying workflow 
management system. We assume that the underlying 
WFMS supports a typical graph-based process 
model and a state-based execution model. Such 
process models support typical constructs like 
sequence, fork, choice etc (Figure 1(a)), and activity 
execution is based on a finite state machine with 
typical states such as available, commenced, 
suspended, completed (Figure 1(b)). 
The workflow model (W) is defined through a 
directed graph consisting of nodes (N) and Flows 
(F). Flows show the control flow of the workflow. 
Thus W = <N, F> is a Directed Graph where N: 
Finite Set of Nodes, F: Flow Relation F  N & N. 
Nodes are classified into tasks (T) and coordinators 
(C), where  C  T, C  T = I.
Task nodes represent atomic manual / automated 
activities or sub processes that must be performed to 
satisfy the underlying business process objectives. 
Coordinator nodes allow us to build control flow 
structures to manage the coordination requirements. 
Basic modelling structures supported through these 
coordinators include Sequence, Exclusive Or-Split 
(Choice), Exclusive Or-Join (Merge), And-Split 
(Fork), And-Join (Synchronizer), and explicit begin 
and end coordinators.   
Shazia Sadiq et al. 
188
Figure 1: (a) Process Model. 
Different 
proposals 
offer 
different 
level 
of 
expressiveness in terms of these perspectives, see 

A process model will have several activities. An 
activity t  T is not a mere mode in the workflow 
graph, but has rich semantics which are defined 
through its properties, such as input and output data, 
temporal constraints, resources requirements etc.  
An
instance within the workflow graph 
represents a particular case of the process.  An 
instance type represents a set of instances that follow 
the same execution path within the workflow.  
Let i be an instance for W.  
 t  N, we define ActivityState(t, i) Æ {Initial, 
Available, Commenced, Completed, Suspened} 
We propose to extend the above environment 
with the following two functions: 
A 
design 
time 
function 
of 
constraint 
specification. To provide a facility to specify a pool 
of activities (including sub-processes) and associated 
constraints in addition to the core process model. 
These activities are allowed to be incorporated in the 
process at any time during execution, but under the 
given constraints. The core process defines the un-
negotiable part of the process, and the pool of 
activities and associate constraints define the 
flexible part – thus attempting to strike a balance 
between flexibility and control.  
We associate with every process W, these two 
additional elements of specification, namely the pool 
of activities given by P, and a set of constraints 
given by C. The definition of the flexible workflow 
Wf is thus given by <W, P, C>.  
A run time function of dynamic instance 
building.  To allow the execution path of given 
instance(s) to be adapted in accordance with the 
particular requirements for that instance which 
become known only at runtime. Thus the process for 
a given instance can be dynamically built based on 
runtime knowledge, but within the specified 
constraint set C.  
In order to provide explicit terminology, we call 
the instance specification prior to building, an open 
instance. The instance specification after building 
we call an instance template. Thus the instance 
template is a particular composition of the given 
activities within the flexible workflow Wf. The 
instance templates in turn have a schema-instance 
relationship with the underlying execution. In 
traditional terms, the instance template acts as the 
process model for the particular instance. Execution 
takes place with full enforcement of all coordination 
constraints as in a typical production workflow. 
However, template building is progressive. The 
process may be changed several times through the 
available 
pool 
of 
activities 
and 
associated 
constraints. As such the template remains open until 
the process has reached completion.  
The main feature of this approach is the 
utilization of the constraint set C. In previous work, 
we proposed the use of so called structural and 
containment constraints for flexible workflows 
(Sadiq et al, 2001), (Sadiq et al, 2004). The 
constraints belonging to the structural class impose 
restrictions on how activities can be composed in the 
templates. 
The 
constraints 
belonging 
to 
the 
containment class identify conditions under which 
combinations of activities can(not) be contained in 
the templates.  
For example serial is a type of structural 
constraint, where given activities must be executed 
serially, i.e. not concurrently. However the choice of 
order remains flexible and is determined by the user 
during the build. A practical example of a serial 
constraint can be found in healthcare. Pathologies 
and medical imaging labs need to schedule a large 
number of tests in different departments. A number 
of tests can be prescribed for a given patient e.g. 
blood test, X-Ray, ECG. These tests can be done in 
any order but only one at a time. A serial constraint 
on these activities will ensure this for a given patient 
or instance. 
In this paper, we introduce a new class of 
constraints for flexible workflows. We call these 
cardinality constraints. This new class is especially 
interesting, because it provides a new means of 
dealing with two well known challenges in 
workflow specification, namely n-out-of-m joins and 
implicit termination. In the sections below, we 
introduce the framework for the specification of 
cardinality constraints in flexible workflows. We 
will also present a means of validating the 
dynamically built instance (templates) against the 
specified constraints.  
Available
Commenced
Suspended
Completed
Initial
Available
Commenced
Suspended
Completed
Initial
Quality of Service in Flexible Workflows Through Process Constraints 
189
Figure 1: (b) Activity Execution Model. 

3 CARDINALITY CONSTRAINTS 
Cardinality constraints basically define the set of 
tasks that must be executed within the process, to 
guarantee that intended process goals will be met. In 
other words, which tasks must essentially be 
executed for a process to be considered complete.  
The completion of W is explicit due to the 
presence of an end coordinator and also since the 
tasks within an instance type are pre-determined. 
However, completion of Wf is not explicit, since the 
user may make different selections at run time from 
the available pool of activities.  
To further explain this, we define the function 
Complete (W, i) Æ {True, False}, where   
Complete (W, i) = True iff 
 t  T, ActivityState(t, i) = Completed | Initial 
 AND  t  T, ActivityState(t, i) = Completed 
Complete (Wf, i) = True iff 
Complete (W, i) = True  
AND  Pk  P, such that  
 t  Pk, ActivityState(t, i) = Completed 
 The interesting question is, how to define the 
set of tasks that constitute Pk. This requires 
consideration 
at 
both 
the 
conceptual 
and 
implementation level. As an example, consider the 
tertiary education domain.  
Today’s student communities are constantly 
changing, with more and more part time, mature age 
and international students with a wide variety of 
educational, professional and cultural backgrounds. 
These students have diverse learning needs and 
styles. Where as degree programs are generally well 
defined in terms of overall process constraints, it is 
difficult to judge the quality of specific choices 
made by students. Tertiary programs often offer a 
diverse 
collection 
of 
courses 
that 
allow 
specialisation on various aspects of a program. The 
wide variety of valid combinations of courses that 
satisfy a particular program’s requirement indicates 
a high degree of flexibility.  
The study of a simple program structure was 
conducted. The program consisted of nine courses of 
compulsory material and a further three courses of 
elective material which are selected from a schedule 
of 14 available electives. This was found to yield a 
total of some 364 instance types, when considering 
also the sequence in which the courses can be taken. 
A further illustration considers a less structured 
program, such as Arts or Science, where some 20 to 
30 courses are required from a schedule that can 
contain thousands of courses. A number of factors 
impact on the choices made by the students 
including changing areas of interest, changing 
workload requirements and changing program rules. 
The multiplicity of valid combinations of courses 
that can be undertaken, and ensuring that these 
satisfy the requirements of programs, particularly 
where these requirements have changed during the 
duration of the student’s enrolment constitute a 
complex problem.  
Although academic courses are not currently 
deployed as workflow tasks in the typical sense, the 
appropriateness of workflow modelling concepts has 
been demonstrated (Sadiq & Orlowska, 2002). 
Academic courses equate to process tasks, these 
courses are interdependent and the academic 
program represents a long duration business process. 
At the same time, there is an inherent flexibility in 
these processes, required for diverse student 
requirements, which makes their modelling in 
traditional prescriptive process definition languages 
very difficult.  
In the sections below, we will demonstrate how 
the use of cardinality constraints within a flexible 
workflow modelling framework provides an elegant 
means of capturing the requirements of such 
processes. Furthermore, the presented framework 
also provides a simple means of ensuring that the 
specified constraints are met for a given instance, 
thus providing the essential validation support.  
3.1 Specification
The specification of the task set Pk that satisfies the 
completion condition for Wf can be done in three 
ways: 
1.
Providing a static set of mandatory tasks which 
must all be performed in a given instance. In 
this case, the flexibility is found only in when 
these tasks will be executed, not which ones. 
We call this constraint include. Specification on 
include is rather straightforward and can be 
made as include: Pk
2.
Providing a set of tasks, together with a minimal 
cardinality for selection, that is at least n out of 
m tasks must be performed in a given instance. 
We call this constraint select. Specifying select 
is also simple and can be made by providing the 
set of tasks, together with an integer n, i.e. 
select: (P, n), where P is the available pool of 
activities for the flexible workflow. In this case 
Pk, is any subset of P where |Pk| = n 
3.
Providing a set of tasks, a minimal cardinality 
for selection, as well as prescribing some tasks 
as mandatory. Thus, at least n tasks must be 
preformed for a given instance, but this 
selection of n tasks must include the prescribed 
Shazia Sadiq et al. 
190

mandatory tasks. We call this constraint 
minselect.
Specifying
minselect 
requires 
further 
consideration, which we present below. 
We first introduce the notion of a family of set A,
as a collection of subsets of A. A notation to 
represent a family of set A is given by (A`, k; A) and 
is defined as follows: 
|A| = n 
A`A such that |A`| = m,  
Let k be such that m+k d n, 
(A`, k; A) = { A` B | B  2A\A`  and |B| = k} 
(A`, k; A) represent a collection of subsets of set 
A, such that each member of the collection is 
composed from A` and B. To illustrate further, we 
present the following simple example: 
Let A = {a, b, c, d} and family F = (A`, k; A)  
where A` = {a, b} and k = 1,  
then F = {{a, b, c}, {a, b, d}}  
There are  
¸¸
¹
·
¨¨
©
§
 m
n
k
 number of elemental  
subsets in the (A`, k; A) family. i.e. the cardinality 
of the family can be computed by |(A’, k; A)| = (n – 
m)!/k!(n – m – k)!. 
The notation of (A`, k; A) has the expressive 
power to represent a collection of subsets without 
listing every single one. Basically all members of 
(A`, k; A) family shares the common subset A` in 
the set, and the remaining subset is selected from the 
power set of the set difference A\A` where 
cardinality equals to k. Thus A’ represents the 
mandatory selection.
Since the list of all elements within the (A`, k; 
A) family may become very large, modelling the 
minselect constraint as (A`, k; A), provides an 
effective means of capturing a large number of 
choices effectively. Thus specification of this 
constraint can be given as minselect: (P`, k; P), 
where P is the available pool of activities for the 
flexible workflow.  
For example, the higher education degree 
program referred to earlier has 14 courses to select 
from (n = 14, i.e. |A| = n), 9 of which are 
compulsory courses (m = 9, i.e. |A’| = 9), and with a 
requirement to take at least 12 courses, students can 
choose any three, or at least three from the 
remaining courses, which indicates k = 3.  
3.2  Validation 
Once the flexible workflow has been defined, 
including the core process, pool of activities, and 
process constraints (which may include a number of 
structural, cardinality or other constraints), instances 
of the workflow may be created. Instance execution 
will take place as in typical workflow engines, until 
the time when a special purpose build function is 
invoked. This function basically allows the instance 
template to be modified. The next section will 
elaborate further on how the flexible workflow is 
managed by the WFMS.  
In this section we are interested in what happens, 
once the build function is invoked, and the instance 
template has been modified. Clearly the ability to 
modify the instance template on the fly provides the 
much desired flexibility. However, the question is, 
does the modification conform to the prescribed 
process constraints?  
Thus validating an instance template against a 
given set of constraints needs to be provided. In the 
context of cardinality constraints, this can be 
achieved as follows. 
In order to validate a dynamically built instance 
template, we have to ensure that all tasks in Pk are 
part of the node set of the newly defined instance 
template. This is required since the condition for 
completeness of an instance of Wf is dependent on 
the task set Pk. It can be observed that determining 
Pk in case of include and select constraints is a 
relatively straight forward procedure. In the case of 
minselect, Pk is defined as an element in the family 
of set P. That is, an instance i of Wf , for which a 
constraint of type minselect has been defined, can be 
guaranteed to complete satisfactorily under the 
following conditions:
Complete (Wf, i) = True iff 
Complete (W, i) = True  
AND  Pk  P, such that 
Pk  (P`, k; P) 
AND  t  Pk, ActivityState(t, i) = Completed 
A very important question to ask is: what 
happens if we want to specify several cardinality 
constraints for the same workflow? Could potential 
conflicts or redundancy arise within the constraint 
set itself. If so, it must be resolved at design time, 
that is before any instance templates are built under 
that constraint set.  
A number of relationships may exist between 
constraints. For example two minselect  constraints 
may be specified: 
minselect1: (P1’, k; P1) 
 minselect2: (P2’, k; P2) 
Quality of Service in Flexible Workflows Through Process Constraints 
191

where P1 and P2 are subsets of P, the given pool 
of activities for Wf.
How do we reason with the constraint set when 
P1 P2 I. The full scope of this reasoning is 
beyond the scope of this paper, however, (Lin & 
Orlowska, 2004), presents an investigation into 
dependencies between an arbitrary pair of (A’, k; A) 
families. Three relationships have been identified 
and analysed, namely Equivalent, Subsume and 
Imply. This reasoning provides the first step towards 
a complete analysis of the set of cardinality 
constraints, in particular minselect.
Another important question to be asked is, when 
is the instance template validated against prescribed 
process constraints? Clearly, this must be done prior 
to the instance resuming execution. In the next 
section, we will provide a detailed view of the 
procedure to manage the flexible workflow Wf.
3.3 Managing Wf
Below we explain the functions of the flexible 
workflow management system based on the 
concepts presented in this paper. The discussion is 
presented as a series of steps in the specification and 
deployment of an example process. Figure 2 
provides an overview diagram of these steps and 
associated functions of the flexible workflow engine. 
Process
Modeling Tool
Constraints
Validation
Engine
Process
Verification
Engine
Process
Enactment
Engine
Worklist
Manager
Process
Designer
Dynamic Instacne
Builder
Workitem
Performers
2,7
1
7
3
4
Applications /
Users Creating
Process Instances
5,6
5
6
6
8
6
Step 1: The definition of the (flexible) workflow 
model takes place. The core process, pool of 
activities and associated constraints are defined.  
Step 2: The process is verified for structural 
errors. The validation of the given constraint set may 
also takes place at this time.  
Step 3: The process definition created above is 
uploaded to the workflow engine. This process 
model is now ready for deployment.  
Step 4: For each case of the process model, the 
user or application would create an instance of the 
process model. On instantiation, the engine creates a 
copy of the process definition and stores it as an 
instance template. This process instance is now 
ready for execution. 
Step 5: The available process activities of the 
newly created instance are assigned to performers 
(workflow users) through work lists and activity 
execution takes place as usual, until the instance 
needs to be dynamically adapted to particular 
requirements arising at runtime.  
Step 6: The knowledge worker or expert user, 
shown as the dynamic instance builder, will invoke a 
special build function, and undertake the task of 
dynamically adapting the instance template with 
available pool of activities, while guided by the 
specified constraint set. This revises the instance 
template.   
The build function is thus the key feature of this 
approach which requires extension of the typical 
WFMS functionality to include this additional 
feature. Essentially the build function is the 
capability to load and revise instance templates for 
active instances.  
Step 7: The next step is to verify the new 
template, to ensure that it conforms to the 
correctness properties of the language as well as the 
given constraints.  
Step 8: On satisfactory verification results the 
newly defined (or revised) instance template 
resumes execution. Execution  will now continue as 
normal, until completion or until re-invocation of the 
build function, in which case steps 6-8 will be 
performed again.  
4 RELATED WORK 
There have been several works reported in research 
literature that aim towards providing the necessary 
support for flexible workflows. So much so, that the 
term 
flexible 
workflows 
has 
become 
rather 
overloaded. It can range from process evolution, to 
dealing with workflow exceptions, to flexible 
modelling frameworks. We position this work in the 
area of flexible modelling frameworks.  
Shazia Sadiq et al. 
192
Figure 2: Deployment of a Flexible Workflow. 

Where as there has been substantial work on the 
first two aspects, namely process evolution, see e.g. 
(Ellis, Keddara and Rozenberg, 1995), (Joeris and 
Herzog, 1998), (Kradolfer and Geppert, 1999), 
(Sadiq, Marjanovic and Orlowska, 2000). and 
exception handling, see e.g. (Reichert and Dadam, 
1998), (Casati and Pozzi, 1999). In the area of 
flexible workflow definition, the closest to our 
approach is the approach followed by rule-based 
workflows.  
(Knolmayer, Endl and Pfahrer, 2000), for 
example provides a rule based description of a 
business process and transforms it, by applying 
several refinement steps, to a set of structured rules 
which represent the business process at different 
levels of abstraction.. The underlying concept of 
developing a workflow specification from a set of 
rules describing the business processes is similar in 
principle to the work presented here. However the 
approach 
is 
primarily 
directed 
towards 
the 
development of coordinated processes that span 
enterprise boundaries by providing a layered 
approach that separates the transformation of 
business (sub-) processes and the derivation of 
workflow specifications and does not address the 
issue of catering for processes that cannot be 
completly predefined.  
Moving to the other end of our continuum for 
organisational processes that spans from highly 
specified and routine processes to highly unspecified 
and dynamic processes we acknowledge the 
significant work that has been performed in the 
coordination of collaboration intensive processes in 
the field of CSCW, see e.g. (Bogia and Kaplan, 
1995). The complete relaxation of coordination, to 
support ad-hoc processes is not conducive to the 
processes targeted by our work.  
However, structured ad-hoc workflows, where
patterns can be derived form the activities in the 
process as a result of underlying rules to achieve 
certain goals, have also been proposed (Han and 
Shim, 2000). This allows the workflow system to 
derive the workflow incrementally from workflow 
fragments and avoids the need to predefine the 
process prior to enactment. The completion of a 
structured ad-hoc workflow instance allows flows to 
be derived for other instances of workflows that 
share the same process rules. Although defining 
parts of a process incrementally rather than enacting 
on a predefining process is similar to the underlying 
assumption in our work. However the development 
of this concept to address the modeling of processes 
that cannot be eloquently predefined contains 
significant differences as a result of the rules being 
more explicit and consistent across instances. 
Rule based approaches that make use of 
inference mechanisms have also been proposed for 
flexible workflows, (Abrahams, Eyers, and Bacon, 
2002), (Kappel, Rausch-Schott, and Retschitzegger 
2000), (Zeng et al, 2002). For example (Zeng et al, 
2002) proposes PLM Flow, which provides a set of 
business inference rules designed to dynamically 
generate and execute workflow. The process 
definition in PLMflow is specified as business rule 
templates, which include backward-chain rules and 
forward-chain rules. PLM Flow is a task centric 
process model. The workflow schema is determined 
by inferring backward-chain and forward-chain tasks 
at runtime.  
Some researchers have also made use of agent 
technologies for flexible workflow definition e.g. 
ADEPT (Jennings et al, 2000), AgFlow (Zeng et al, 
2001), and RSA (Debenham, 1998). We present 
brief summaries below. 
ADEPT provides a method for designing agent-
oriented business process management system. It 
demonstrates how a real-world application can be 
conceived of as a multi-agent system.  
AgFlow is an agent-based workflow system built 
upon a distributed system. The system contains a 
workflow specification model and the agent-based 
workflow architecture. The process definition is 
specify by defining the set of tasks and the workflow 
process tuple. The control flow aspects can be 
reflected in the task specific ECA rule. 
RSA is an experimental distributed agent-based 
system based on a 3-layer Believe-Desire-Intension 
(BDI) architecture, hence the process definition is 
reflected by the conceptual architecture of the 
system as a whole.  
Inspite of substantial interest from research 
communities, our study shows that industry 
acceptance of rule based approaches has been low. 
Most commercial products continue to provide much 
more visual languages for workflow specification. 
Often some variant of Petri-nets, these languages 
have the dual advantage of intuitive representation 
as well as verifiability. A key distinguishing feature 
of our approach from typical rule based approaches 
is that the the core process as well as the instance 
template can still be visualized in a graphical 
language, as well as be supported by essential 
verification.  
5 CONCLUSIONS
Difficulties in dealing with change in workflow 
systems has been one of the major factors limiting 
the deployment of workflow technology. At the 
same time, it is apparent that change is an inherent 
characteristic of today’s business processes. In this 
paper we present an approach that recognizes the 
Quality of Service in Flexible Workflows Through Process Constraints 
193

presence of change, and attempts to integrate the 
process of defining a change into the workflow 
process itself. Our basic idea is to provide a 
powerful means of capturing the logic of highly 
flexible 
processes 
without 
compromising 
the 
simplicity 
and 
genericity 
of 
the 
workflow 
specification language. This we accomplish through 
process constraints in workflow specifications, 
which allow workflow processes to be tailored to 
individual instances at runtime. 
Process constraints can be defined for a number 
of aspects of workflow specification, including 
selection of activities, as demonstrated in this paper. 
In addition to selection, they can be defined for 
structural, resource allocation, as well as temporal 
constraints for and between workflow activities. One 
can observe that the design of an appropriate means 
to facilitate the specification of process constraints is 
an interesting and challenging issue.  
Another interesting and beneficial outcome of 
the above approach is that ad-hoc modifications can 
also be provided through essentially the same 
functionality. Ad-hoc modification means that any 
unexecuted part of the instance template may be 
modified at runtime. This is possible since the 
workflow engine provides the facility to modify 
instance templates even in the absence of process 
constraints.  However, it is important to point out 
that we advocate the approach using process 
constraints over ad-hoc modification because it 
provides greater control over allowable changes at 
runtime. 
The key feature of this approach is the ability to 
achieve a significantly large number of process 
models, from a relatively small number of 
constraints. Extensions to the constraint set may be 
envisaged, although it is arguable if such a complete 
and generic set can be found, and hence achieving 
flexibility still remains a matter of degree.  
REFERENCES
van Der Aalst, W. M. P., ter Hofstede, A. H. M., 
Kiepuszewski, B., Barros, A. P. Workflow Patterns, 
Distributed and Parallel Databases, vol.14 no.1, p.5-
51, July 2003. 
Abrahams, A., Eyers, D., and Bacon, J. An asynchronous 
rule-based approach for business process automation 
using obligations. ACM SIGPLAN workshop on Rule-
based programming,  2002. 
Casati, F., Ceri, S., Pernici, B., Pozzi, G. Conceptual 
Modeling of Workflows. Proceedings of the 14th 
International Conference on Object-Oriented and 
Entity-Relationship Modelling, vol. 1021 LNCS,  
pages: 341 – 354, Springer-Verlag, 1995. 
Casati, F., Pozzi, G. Modeling Exception Behaviors in 
Commercial 
Workflow 
Management 
Systems. 
Proceedings of the Fourth IFCIS International 
Conference on Cooperative Information Systems 
(CoopIS99). Edinburgh, Scotland. Sep 2-4, 1999. 
Debenham, J. Constructing an Intelligent Multi-agent 
Workflow System. Lecturer Notes in Computer 
Science: vol. 1502, Springer Verlag, 1998, pp. 119 - 
130.
Ellis, S., Keddara, K., Rozenberg, G.. Dynamic Changes 
within Workflow Systems. Proceedings of ACM 
Conference on Organizational Computing Systems
COOCS 95 (1995). 
Han, D. and Shim, J. Connector-oriented workflow system 
for the support of structured ad hoc workflow, 
Proceedings of the 33rd Hawaii International 
Conference on System Sciences. 2000 
Herrmann, T. Evolving workflows by user-driven 
coordination,
Proceedings of DCSCW, Munich, 
Germany, 102–114, September 2000 
Jablonski, S., Bussler, C. Workflow Management-
Modeling, 
Concepts, 
Architecture 
and 
Implementation, International Thomson Computer 
Press, 1996. 
Jennings, N. R., Faratin, P., T. Norman ,J., O'Brien, P., 
Odgers, B., and Alty, J. L. Implementing a Business 
Process Management System using ADEPT: a Real-
World Case Study. International Journal of Applied 
Artificial Intelligence, vol. 14, pp. 421--463, 2000 
Joeris, G., Herzog, O.. Managing Evolving Workflow 
Specifications. Proceedings of the third IFCIS 
International Conference on Cooperative Information 
Systems (CoopIS 98). NewYork, USA. Aug (1998). 
Kappel, G., Rausch-Schott, S., and Retschitzegger, W. A 
Framework for Workflow Management Systems 
Based on Objects, Rules and Roles. ACM Computing 
Surveys, vol. 32, pp. 27 - 27, 2000. 
Knolmayer, G., Endl R. and Pfahrer, M. Modeling 
processes and workflows by business rules, van der 
Aalst W. et al. (Eds.) Business Process Management,
LNCS 1806: 16–29. 2000 
Kradolfer, M., Geppert, A.. Dynamic Workflow Schema 
Evolution based on Workflow Type Versioning and 
Workflow Migration. Proceedings of the Fourth 
IFCIS International Conference on Cooperative 
Information 
Systems 
(CoopIS99). 
Edinburgh, 
Scotland. Sep 2-4, 1999. 
Lin, J., Orlowska, M. A new class of constraints for 
business process modelling. School of Information 
Technology 
and 
Electrical 
Engineering, 
The 
University of Queensland. Technical Report No.  453. 
Nov 2004.
Reichert, M., Dadam, P. ADEPTflex - Supporting 
Dynamic Changes of Workflow without loosing 
control. Journal of Intelligent Information Systems 
(JIIS), Special Issue on Workflow and Process 
Management 1998. 
Shazia Sadiq et al. 
194

Sadiq 
W., 
Orlowska, 
M. 
On 
capturing 
Process 
Requirements 
of 
Workflow 
Based 
Information 
Systems. Proceedings of the 3rd International 
Conference on Business Information Systems (BIS 
’99), Poznan, Poland. April 14-16, 1999.  
Sadiq, S., Marjanovic, O., Orlowska, M. Managing 
Change and Time in Dynamic Workflow Processes. 
The International Journal of Cooperative Information 
Systems. Vol 9, Nos 1&2. March-June 2000. 
Sadiq, S., Sadiq, W., Orlowska, M. Pockets of Flexibility 
in Workflow Specifications. 20th International 
Conference on Conceptual Modeling, ER’2001, 
Yokohama Japan, 2001. 
Sadiq, S., Sadiq, W., Orlowska, M. Workflow Driven e-
Learning – Beyond Collaborative Environments. 
Networked Learning in a Global Environment. 
Challenges and Solutions for Virtual Education.
Berlin, Germany May 1 - 4, 2002. 
Sadiq, S., Sadiq, W., Orlowska, M. Specification and 
Validation of Process Constraints for Flexible 
Workflows. Information Systems (To appear).  
Zeng, L., Flaxer, D., Chang, H., and Jeng, J.. PLMflow: 
Dynamic 
Business 
Process 
Composition 
and 
Execution by Rule Inference. 3rd VLDB Workshop on 
Technologies for E-Services (TES'02), HongKong 
P.R.China, 24-25 Aug 2002. 
Zeng, L., Ngu, A., Bentallah, B., and O'Dell, M. "An 
agent-based approach for supporting cross-enterprise 
workflows," presented at 12th-Australasian-Database-
Conference.-ADC-2001, 2001. 
Quality of Service in Flexible Workflows Through Process Constraints 
195

REAL TIME DETECTION OF NOVEL ATTACKS BY MEANS OF
DATA MINING TECHNIQUES ∗
Marcello Esposito, Claudio Mazzariello, Francesco Oliviero, Simon Pietro Romano and Carlo Sansone
Dipartimento di Informatica e Sistemistica – Universit`a degli Studi di Napoli “Federico II”
Via Claudio 21, 80125 Napoli (Italy)
Email: {mesposit,cmazzari,folivier,spromano,carlosan}@unina.it
Keywords:
Intrusion Detection, Trafﬁc Features.
Abstract:
Rule-based Intrusion Detection Systems (IDS) rely on a set of rules to discover attacks in network trafﬁc. Such
rules are usually hand-coded by a security administrator and statically detect one or few attack types: minor
modiﬁcations of an attack may result in detection failures. For that reason, signature based classiﬁcation is
not the best technique to detect novel or slightly modiﬁed attacks. In this paper we approach this problem by
extracting a set of features from network trafﬁc and computing rules which are able to classify such trafﬁc.
Such techniques are usually employed in off line analysis, as they are very slow and resource-consuming. We
want to assess the feasibility of a detection technique which combines the use of a common signature-based
intrusion detection system and the deployment of a data mining technique. We will introduce the problem,
describe the developed architecture and show some experimental results to demonstrate the usability of such
a system.
1
INTRODUCTION
Security is one of the main concerns in the devel-
opment of new technologies and services over the
Internet.
The most common and best known tools
used to ensure security of companies, campuses and,
more in general, of any network, are Firewalls and
Antiviruses. Though famous and well known, such
tools alone are not enough to protect a system from
malicious activities.
Basing one’s own site’s secu-
rity on the deployment of these instruments relies on
the idea that intrusion prevention will sufﬁce in ef-
ﬁcently assuring data availability, conﬁdentiality and
that they will sooner or later happen, despite the secu-
rity policy a network administrator deploys. Based on
such assumption, the researchers started to develop
instruments able to detect successful intrusions and,
in some cases, trace back the path leading to the at-
tack source. This is a more pessimistic, though much
more realistic way to look at the problem of network
security.
∗Research outlined in this paper is partially funded
by the Ministero dell’Istruzione, dell’Universit`a e della
Ricerca (MIUR) in the framework of the FIRB Project
“Middleware for advanced services over large-scale, wired-
wireless distributed systems (WEB-MINDS)”
2
RELATED WORK
This work has many liaisons with both intrusion de-
tection and data mining.
As to the ﬁrst research ﬁeld, intrusion detection is
the art of detecting inappropriate, incorrect or anom-
alous activity within a system, be it a single host or
a whole network.
An Intrusion Detection System
(IDS) analyzes a data source and, after preprocess-
ing the input, lets a detection engine decide, based
on a set of classiﬁcation criteria, whether the ana-
lyzed input instance is normal or anomalous, given a
suitable behavior model. Intrusion Detection Systems
can be grouped into three main categories: Network-
based Intrusion Detection Systems (N-IDS) (Vigna
and Kemmerer, 1999), Host-based Intrusion De-
tection Systems (H-IDS) (Andersson, 1995) (Tyson,
2000) and Stack-based Intrusion Detection Systems
(S-IDS) (Laing and Alderson, 2000). This classiﬁ-
cation depends on the information sources analyzed
to detect an intrusive activity.
An N-IDS analyzes
packets captured directly from the network. By set-
ting network cards in promiscuous mode, an IDS can
monitor trafﬁc in order to protect all of the hosts con-
nected to a speciﬁed network segment. On the other
hand, an H-IDS focuses on a single host’s activity:
the system protects such a host by directly analyzing
the audit trails or system logs produced by the host’s
197
C.-S. Chen et al. (Eds.), Enterprise Information Systems VII, 197–204.
© 2006 Springer.
integrity. Indeed, an interesting idea about intrusions is

operating system. Finally, S-IDS are hybrid systems,
which operate similarly to a N-IDS, but only analyze
packets concerning a single host of the network. They
monitor both inbound and outbound trafﬁc, follow-
ing each packet all the way up the TCP/IP protocol
stack, thus allowing the IDS to pull the packet out of
the stack even before any application or the operating
systems process it. The load each IDS must afford is
lower than the total trafﬁc on the network, thus keep-
ing the analysis overhead within reasonable bounds;
hypothetically, each host on the network could run a
S-IDS.
Intrusion Detection Systems can be roughly clas-
siﬁed (Figure 1) as belonging to two main groups as
well, depending on the detection technique employed:
anomaly detection and misuse detection (Bace, 2000).
Both such techniques rely on the existence of a reli-
able characterization of what is normal and what is
not, in a particular networking scenario.
More precisely, anomaly detection techniques base
their evaluations on a model of what is normal, and
classify as anomalous all the events that fall outside
such a model. Indeed, if an anomalous behavior is
recognized, this does not necessarily imply that an at-
tack activity has occurred: only few anomalies can be
actually classiﬁed as attempts to compromise the se-
curity of the system. Thus, a relatively serious prob-
lem exists with anomaly detection techniques which
generate a great amount of false alarms. On the other
side, the primary advantage of anomaly detection is
its intrinsic capability to discover novel attack types.
Numerous approaches exist which determine the vari-
ation of an observed behavior from a normal one. A
ﬁrst approach is based on statistical techniques. The
detector observes the activity of a subject (e.g. num-
ber of open ﬁles or TCP state transitions), and creates
a proﬁle representing its behavior. Every such proﬁle
is a set of “anomaly measures”. Statistical techniques
can then be used to extract a scalar measure represent-
ing the overall anomaly level of the current behavior.
The proﬁle measure is thus compared with a threshold
value to determine whether the examined behavior is
anomalous or not. A second approach, named predic-
tive pattern generation, is based on the assumption
that an attack is characterized by a speciﬁc sequence,
i.e. a pattern, of events. Hence, if a set of time-based
rules describing the temporal evolution of the user’s
normal activity exists, an anomalous behavior is de-
tected in case the observed sequence of events signif-
icantly differs from a normal pattern.
Misuse detection, also known as signature detec-
tion, is performed by classifying as attacks all the
events conforming to a model of anomalous behav-
ior. This technique is based on the assumption that an
intrusive activity is characterized by a signature, i.e. a
well-known pattern. Similarly to anomaly detection,
misuse detection can use either statistical techniques
or even a neural network approach to predict intru-
sions. Indeed, the rule-based approach is the most
used to detect an attack (SNORT2(Baker et al., 2004)
and Bro3(Paxson and Terney, 2004)). Intrusions are
coded by means of a set of rules: as soon as the ex-
amined event matches one of the rules, an attack is de-
tected. A drawback of this approach is that only well-
known intrusive activities can be detected, so that the
system is vulnerable to novel aggressions; sometimes,
few variations in an attack pattern may generate an in-
trusion that the IDS is not able to detect.
The main problem related to both anomaly and
misuse detection techniques resides in the encoded
models, which deﬁne normal or malicious behav-
iors. Although some recent open source IDS, such
as SNORT or Bro, provide mechanisms to write new
rules that extend the detection ability of the system,
such rules are usually hand-coded by a security ad-
ministrator, representing a weakness in the deﬁni-
tion of new normal or malicious behaviors. Recently,
many research groups have focused their attention on
the deﬁnition of systems able to automatically build a
set of models. Data mining techniques are frequently
applied to audit data in order to compute speciﬁc be-
havioral models (MADAM ID (Lee and Stolfo, 2000),
ADAM (Barbara et al., 2001)).
Coming to the second related research ﬁeld, we
recall that a data mining algorithm is referred to as
the process of extracting speciﬁc models from a great
amount of stored data (Fayyad et al., 1996). Machine
learning or pattern recognition processes are usually
exploited in order to realize this extraction (SLIP-
PER4 (Cohen and Singer, 1999)). These processes
may be considered as off-line processes. In fact, all
the techniques used to build intrusion detection mod-
els need a proper set of audit data.
The informa-
tion must be labelled as either “normal” or “attack”
2http://www.snort.org
3http://www.bro-ids.org
4http://www-2.cs.cmu.edu/∼wcohen/
slipper/
Marcello Esposito et al. 
198
Figure 1: Approaches to Intrusion Detection.

in order to deﬁne the suitable behavioral models that
represent these two different categories. Such audit
data are quite complicated to obtain.
The data set
used for The Third International Knowledge Discov-
ery and Data Mining Tools Competition, the 1999
KDD data5 (Lee and Stolfo, 2000)(Elkan, 2000), is
probably the most well-known example of this kind
of information, representing a processed version of
the DARPA Intrusion Detection Evaluation Program
database, collected and managed by the MIT Lincoln
Laboratory. The DARPA database contains tcpdump
data related to seven weeks of network trafﬁc gen-
erated over a military emulated LAN. KDD is ﬁlled
with ﬁve million connection records labelled as “nor-
mal” or “attack”.
3
RATIONALE AND
MOTIVATION
do not take into account the concern of real-time
processing of network trafﬁc. Though, an effective
IDS should be able to produce the analysis results in
time to react and possibly activate countermeasures
against malicious behaviors.
The ability to detect an intrusion as soon as it oc-
curs is mandatory for an IDS. The most common
types of attacks, e.g. denial of service, can be very
dangerous if they are not detected in time. Although
some IDS store audit data for later analysis, most
of them examine such data in real-time so that the
system can perform the actions necessary in order
to avoid serious problems. Commonly used N-IDS
typically analyze packets captured from the network,
ﬁnding in the current packet the signature of an at-
tack in-progress. However, malicious activity cannot
be detected by examining just a single packet: some
types of attacks generate in a certain time interval a
great amount of packets belonging to different ses-
sions. Hence an efﬁcient detection needs statistical
parameters taking into account the temporal relation
between sessions. As stated before, Stolfo et al. (Lee
and Stolfo, 2000) have deﬁned a set of connection
features which summarize the temporal and statisti-
cal relations of the connections with reference to each
other. These features have been used to create the
connection records contained in the KDD database.
Several data mining processes use these connection
features to extract suitable behavioral models.
Trafﬁc model deﬁnition based on an off-line analy-
sis does not consider the unavoidable problems of
real-time computation of connection features.
The
data mining process operates on a database, in which
5http://kdd.ics.uci.edu/
data can be organized in a suitable way in order to
compute the features. In real-time intrusion detec-
tion, instead, the incoming packets do not contain all
of the information needed to compute the connection
features, but an appropriate system has to be imple-
mented in order to compute relations among the ex-
isting connections. Moreover, off-line analysis does
not consider the problem of potential packet losses in
the IDS, which has to be taken into account in the case
of real time analysis.
Our research aims to develop a framework for real-
time intrusion detection.
The system we present
should be capable to effectively detect intrusions and
to operate under a variety of trafﬁc conditions, thus
providing an exploitable solution to the issue of real-
time analysis. Anomaly detection proves to be the
most suitable solution for our purpose, even though
such technique has the well known drawback related
to the relatively high number of false alarms raised.
Our intrusion detection system can be classiﬁed as
rule-based. Unfortunately the deﬁnition of a rule for
every attack is not an efﬁcient solution. On one hand,
this approach is not able to detect novel attack pat-
terns; on the other hand, the deﬁnition of new attacks
has a negative impact both on the computation load
and on the average time required to analyze every sin-
gle packet (hence, the related packet loss problem). In
order to overcome the above mentioned drawbacks,
by using a set of parameters derived by Stolfo’s con-
nection features — which cover a wide range of attack
types — it is possible to adopt different data min-
ing processes in order to characterize the attacks by
means of different sets of rules.
Summarizing the above considerations, with this
work we are interested in the analysis of real-time
intrusion detection.
To this purpose, we will ex-
ploit data mining techniques to design a novel intru-
sion detection framework. We will present an imple-
mentation of the framework and evaluate i ts perfor-
mance in a real network scenario, by focussing on two
main performance ﬁgures: packets processing time
and system resources needed to compute the connec-
tion features.
4
THE REFERENCE MODEL
In this section we present our framework for real-time
intrusion detection. The overall model is composed
of two parts: the former is the data mining process,
which extracts behavioral models from pre-elaborated
network trafﬁc, and consists of a database of labelled
connection features and a data mining algorithm; the
latter is a real-time intrusion detection system which
analyzes and classiﬁes network trafﬁc based on the
models inferred (Figure 2). In particular, we execute
Real Time Detection of Novel Attacks by Means of Data Mining Techniques
199
Strategies for non-punctual intrusion detection often

the off-line data mining process on a data set in or-
der to extract a set of rules; such a set is then used
in a real-time classiﬁcation process deployed by the
IDS that analyzes these pre-computed network data
and compares it with informations evaluated by real-
time network trafﬁc.
Data mining is part of a more complex KDD
(Knowledge Discovery in Databases) process consist-
ing of data analysis and discovery algorithms applied
to a database in order to extract high level informa-
tion — the patterns or the models — able to describe
a subset of the data. The models can be applied to un-
known data values in order to predict the right class to
which they belong. As we emphasized in the previous
section, such data mining processes operate on a set
of data which has been organized in a suitable fash-
ion (e.g. all the data are identiﬁed by a label which
explicitly speciﬁes the category they belong to).
In order to implement an efﬁcient classiﬁer, it is
important to deﬁne a suitable set of features to be ex-
tracted from the network trafﬁc contained in the data-
base. The greater the capability of the set of features
to discriminate among different categories, the better
the classiﬁer. There are three levels at which feature
sets may be deﬁned:
• The features may be referred to the single packet
captured from the network:
although this set is easy to compute, it is not able
to detect all the potential attack types.
• A set of features related to the entire session which
the packet belongs to may be deﬁned:
this is due to the fact that some intrusions may be
realized by means of a sequence of packets belong-
ing to either the same connection or different con-
nections.
• The computed set of features may perform a sta-
tistical analysis of the relation between the current
session and the other ones:
this is needed in order to capture intrusions which
affect the interrelation among different sessions.
To cope with the aforementioned requirements, we
have adopted a model descending from the one pro-
posed by Stolfo. We are interested in TCP, UDP and
ICMP trafﬁc. Therefore, a clear deﬁnition of the term
connection is necessary. For a TCP stream the con-
nection can be deﬁned, relying on the protocol speci-
ﬁcations, as the collection of messages exchanged be-
tween a client process and a server process. For UDP
and ICMP we considered each packet as a single, self-
contained connection.
The features deﬁned by Stolfo et al. can be clas-
siﬁed in tree main groups: intrinsic features, content
features, and trafﬁc features. Intrinsic features spec-
ify general information on the current session, like the
duration in seconds of the connection, the protocol
type, the port number (i.e. the service), the number
of bytes from the source to the destination, etc. (see
Table 1).
Table 1: Intrinsic Features
duration
connection duration (s)
protocol type
type of transport protocol
service
port number on the server side
src bytes
bytes from source to destination
dst bytes
bytes from destination to source
ﬂag
status of the connection
land
land attack
wrong fragment
number of wrong fragments
urgent
number of urgent packets
The content features are related to the semantic
content of connection payload: for example, they
specify the number of failed login attempts, or the
number of shell prompts (Table 2).
Table 2: Content Features
hot
number of hot indicators
failed logins
number of failed login attempts
logged in
successfully logged in
compromised
num compromised conditions
root shell
root shell is obtained
su
su root command attempted
ﬁle creations
number of ﬁle creations
shells
number of shell prompts
access ﬁles
number of ﬁle accesses
outbound cmds
outbound commands in ftp
hot login
the login belongs to the hot list
guest login
the login is a guest login
The trafﬁc features can be divided in two groups:
the same host and the same service features.
The
same host features examine all the connections in the
last two seconds to the same destination host as the
one involved in the current connection. We also focus
on the either the number of such connections, or the
rate of connections that have a “SYN” error. Instead,
the same service features examine all the connections
in the last two seconds to the same destination ser-
vice as the current one. These two feature sets are de-
Marcello Esposito et al. 
200
Figure 2: Reference Framework Model.

all the events which have occurred in a time interval of
two seconds (Table 3); some types of attacks, instead,
as the slow probing, may occur every few minutes.
Therefore these features might not be able to detect all
the attack types. To this aim a new set of trafﬁc fea-
tures, called host-based, has been deﬁned; same host
and same service trafﬁc features are also computed
over a window of one hundred connections rather that
over a time interval of two seconds. In our frame-
work we will only adopt intrinsic and trafﬁc features.
Our purpose is to implement a network-based intru-
sion detection system, and we deem the content fea-
tures more suitable for a host-based scenario. Thanks
to the access to the operating system’s audit trails or
system logs, an H-IDS is more efﬁcient in the analysis
of the execution of dangerous commands on a single
host.
The proposed real-time IDS architecture consists of
three components: a sniffer, a processor, and a classi-
ﬁer. The sniffer is the lowest component of the archi-
tecture; connected directly to the network infrastuc-
ture, this module captures all the packets on the wire.
Snifﬁng is made possible by setting the network card
in promiscuous mode. Usually the sniffer also trans-
lates raw packets into a human-readable format.
The processor component elaborates the packets
captured from the sniffer in order to extract the needed
set of features. The main issue of the features compu-
tation process is related to the need of keeping up-
to-date information about the current connection, as
well as the other active sessions. We have to keep in
memory a representation of the current network state
in order to evaluate the statistical relations among the
active connections. Data in memory have to be prop-
erly organized in order to reduce the features compu-
tation time.
The classiﬁer is the core of the architecture; this
component analyzes the current connection features
and classiﬁes them. Based on the misuse detection ap-
proach, the process of classiﬁcation uses a set of rules
extracted by data mining algorithms. The features are
compared against all the rules in the set; when the ex-
amined vector of features matches at least one rule, an
intrusive action is detected. As to the connection data
in the processor component, the rules may be orga-
nized in memory in a suitable way in order to reduce
the time of analysis.
5
REAL-TIME IDS
IMPLEMENTATION ISSUES
The implemented architecture addresses the main re-
quirements of a real-time detection system: monitor-
ing the network trafﬁc in order to extract a set of fea-
on the extracted features. Monitoring, in particular,
is the most challenging issue to face from the point
of view of a real-time analysis. In our architecture,
the monitoring system can be divided into two com-
ponents: the sniffer that captures trafﬁc from the net-
work, and the processor that computes both the in-
trinsic and the trafﬁc features. While in an off-line
analysis features computation is simpler, since all the
information about connections are stored in a data-
base, in a real time analysis statistic measures have to
be be computed every time a new packet is captured
from the network (DFP, 2004).
In order to extract features from the trafﬁc, an ef-
fective processor must ensure two requirements:
• it holds information about the state of the connec-
tion which the analyzed packet belongs to;
• it holds comprehensive information about the traf-
ﬁc ﬂows that have already been seen across the net-
work.
According to the deﬁnition proposed in the previ-
ous section, every packet can be considered as a sin-
gle unit that is inserted in a more complex structure,
namely the connection, and on which the features are
computed. While neither UDP nor ICMP trafﬁc re-
quires a heavy load of computation, TCP trafﬁc re-
quires to emulate the TCP state diagram on both the
client and the server sides and for every active con-
nection. In particular, when a new packet is captured,
the system retrieves information about the connection
to which such a packet belongs and updates the con-
nection state of both the client and the server based on
the TCP protocol speciﬁcations.
In order to compute the statistical relations, infor-
mation on the past TCP, UDP and ICMP ﬂows is re-
quired, including those connections which have been
closed. Trafﬁc features, in fact, are computed by an-
alyzing all the connections (either active or expired)
having similar characteristics — besides the destina-
tion IP address and/or the destination port — as the
current one. Every connection has to be kept in mem-
ory until it is not needed anymore for other computa-
tions.
Our architecture is implemented by means of the
open-source N-IDS Snort; we have used this system
as the base framework on top of which we have built
our components. Snort is a lightweight network IDS
created by Marty Roesch. Its architecture is made
up of four main blocks: a sniffer, a preprocessor en-
gine that pre-computes of captured packets, a rule-
based detection engine, and a set of user output tools.
Thanks to Snort’s modular design approach, it is pos-
sible to add new functionality to the system by means
of program plugins.
Moreover, Snort provides an
efﬁcient preprocessor plugin that reassembles TCP
streams and can thus be used to recover the TCP con-
tures from it, as well as behavior classiﬁcation based
nections status.
Real Time Detection of Novel Attacks by Means of Data Mining Techniques
201
ﬁned time-based trafﬁc features because they analyze

Table 3: Time-Based Trafﬁc Features
Same Host
count
number of connections to the same host
serror rate
% of connections with SYN errors
rerror rate
% of connections with REJ errors
same srv rate
% of connections to the same service
diff srv rate
% of connections to different services
Same Service
srv count
number of connections to the same service
srv serror rate
% of connections with SYN errors
srv rerror rate
% of connections with REJ errors
srv diff host rate
% of connections to different services
We have implemented a new preprocessor plugin
which computes the connection features. The main is-
sue we tackled has been the computation of the trafﬁc
features, which requires that a proper logical organi-
zation of the data is put into place in order to recover
information about the past network trafﬁc. Moreover,
to assure that the real-time requirement of the system
is met, a fast access to stored data is mandatory.
As to the data structures, we have adopted a binary
search tree. In the worse case this structure guarantees
a performance comparable to that achievable with a
linked list from the point of view of search time; per-
formance further improves in case the tree is a sta-
tic and well-balanced one. Unfortunately, our struc-
ture is not a static tree because the connections are
not known in advance; though, a self-adjusting binary
tree can be adopted in this case in order to balance a
dynamic tree.
We have used a Snort library of functions to man-
age the so-called Splay Trees. A Splay Tree is an ele-
gant self-organizing data structure created by Sleator
and Tarjan (Sleator and Tarjan, 1985): it actually is
an ordered binary tree, in which an item is moved
closer to the entry point — i. e. the tree root — when-
ever it is accessed, by means of a rotation of the item
with the parent node. This makes it faster to access
the most frequently used elements than the least fre-
quently used ones, without sacriﬁcing the efﬁciency
of operations such as insert and search.
With the above mentioned tree structure, we have
implemented two trees, a Same Host Tree and a Same
Service Tree to compute the same host and the same
service trafﬁc features, respectively. Every node in
the tree is identiﬁed by the destination IP address
in the ﬁrst tree, or by the destination service in the
second one.
In this way, we want to store in the
same node information about all the connections that
share the same characteristics. In order to compute
both the time-based and the host-based trafﬁc fea-
tures, for every node in the tree we have implemented
two linked lists, one for each set. The linked lists con-
tain information like source IP address and/or source
port for all the connections that have been identiﬁed
and that have the same destination IP address and/or
the same destination service (Figure 3). The elements
of the list, one for every connection, are ordered in
time: the ﬁrst element is the oldest one, the last is the
most recent.
When a new packet is captured from the network,
our preprocessor plugin ﬁrst analyzes the protocol
of the packet in order to identify the most appropri-
ate procedure to compute intrinsic features.
If the
packet belongs to either a UDP or an ICMP trafﬁc,
the information required to compute intrinsic features
is entirely contained in the packet. In case of TCP
trafﬁc, the procedure recovers the session which the
packet belongs to in order to determine some cru-
cial information, like the duration of the connection
or the number of bytes sent along both directions of
the stream, that cannot be directly inferred from the
packet. Then, the procedure analyzes the destination
IP address and the destination port to compute traf-
ﬁc features. Search operations are performed in both
trees: if no preexisting node is found, a new one is
created, and the trafﬁc features relative to the cur-
rent connection are initialized to zero. Otherwise, if
a node is already in the tree, the procedure analyzes
the two linked lists to compute the statistics for both
time-based and host-based trafﬁc features. Every el-
ement in the list is analyzed and the statistics are up-
dated. During this process the elements that do not
belong neither to a time interval of two seconds, nor
Marcello Esposito et al. 
202
Figure 3: Same-Host Tree Structure.

to a window of the latest one hundred connections are
pruned off.
6
TESTING THE APPROACH
In this section we evaluate the performance overhead
due to the operation of the IDS, pointing out the in-
crease in CPU utilization and memory consumption
with respect to the values observed while running
Snort without our plugins. Our purpose is to show
the affordability of real-time intrusion detection, by
means of techniques which are usually employed in
off-line analysis. We evaluate both CPU and mem-
ory overhead, as well as packet loss ratio. Such tests
are deployed in two scenarios: in the former case, we
build a testbed to emulate network trafﬁc in a con-
trolled environment; in the latter case, we analyze
trafﬁc ﬂowing across the local network at Genova Na-
tional Research Council (CNR). In this scenario, the
most important results concern packet loss analysis.
We show that the complexity increase due to the ap-
plication of our detection techniques does not affect
dramatically the percentage of lost packets. Thus we
demonstrate the affordability of intrusion detection by
means of such techniques. While working on the test-
bed, we consider the topology depicted in Figure 4.
In order to work in a totally controlled environ-
ment, we have to emulate the depicted scenario rather
than working in a real network environment; for that
purpose, we use another topology which just emulates
the one depicted above, as drawn in Figure 5.
Furthermore, we test the IDS using it on a real and
heavily loaded network, whose topology is drawn in
Figure 6. Such a test is useful to assess the limits
of applicability of our plugin, as well as to identify
directions for future improvements.
In table Table 4 we see the values of CPU overhead
due to the use of Snort alone, versus Snort plus our
plugins. The machine operating as IDS in the emu-
lated trafﬁc scenario is equipped with a 1GHz Pen-
tium III CPU and an amount of 256MB RAM, run-
ning Mandrake Linux 9.1 as operating system, kernel
version 2.4.19. In this case we can point out an almost
unperceptible increase in memory consumption (Ta-
ble 5). The doubling in CPU usage percentage, when
using the modiﬁed version of Snort with respect to
the case of Snort alone, is not such a negative result,
since overall CPU usage is still low and under rea-
sonable thresholds, also considering that we are using
general purpose, not dedicated, hardware.
Table 4: Average CPU Overhead
Snort-2.1.0
Snort + Plugins
Emulated Trafﬁc
0.12%
0.22%
CNR Trafﬁc
1.16%
2.42%
The extensive test on CNR network also shows a
slightly higher CPU usage for the modiﬁed version of
Snort, still within the limit of 8% overhead. The ma-
chine acting as IDS is equipped with a 2GHz Pentium
IV, 512MB RAM and RedHat Linux 8.0, using kernel
2.4.18.
Table 5: Memory Overhead
Snort-2.1.0
Snort + Plugins
Emulated Trafﬁc
1.69%
1.70%
CNR Trafﬁc
4.99%
9.46%
Real Time Detection of Novel Attacks by Means of Data Mining Techniques
203
Figure 6: CNR Network Topology.
Figure 4: Reference testbed.
Figure 5: A trafﬁc emulation scenario.

Once again it is worth pointing out that the results
of our measures must be looked at under the perspec-
Of course, the most interesting indication regards
the packet loss ratio. To attain the best results in in-
trusion detection, the main requirement is not to lose
any packets — no matter how much of the system
resources we use — if affordable with the available
hardware. Such result is sketched in Table 6. In the
test deployed using emulated trafﬁc, we notice an in-
crease of less than 10% in packet loss with respect to
the plain version of Snort, though the values are lower
than the ones obtained by testing the system on a real
network. This may be ascribed to the hardware used
in the two cases: the setup used in the latter scenario
is much more suitable than the one used in the former
case. In both cases, anyway, we observe a very low
increase in packet loss ratio, showing the feasibility
of such a technique.
Table 6: Packet Loss
Snort-2.1.0
Snort + Plugins
Emulated Trafﬁc
0.39%
0.42%
CNR Trafﬁc
0.14%
0.16%
7
CONCLUSIONS AND FUTURE
WORKS
This paper shows how it is possible to combine real-
time intrusion detection with data mining techniques,
while at the same time keeping the system overhead
under reasonable thresholds and containing the packet
loss ratio within certain boundaries. Future develop-
ment of this project will involve building rule sets and
evaluating their detection capabilities. We may test
rulesets computed with different algorithms which
make use of various techniques.
The work has also been published on Source-
Forge6, to hopefully receive feedback from users and
to communicate and cooperate with the Snort com-
munity.
ACKNOWLEDGEMENTS
We would like to thank Maurizio Aiello and the staff
at CNR laboratory in Genova, Italy, for their coopera-
tion and for providing us with part of the data as well
as the equipment used for the tests.
REFERENCES
(2004). Operation Experience with High-Volume Network
Intrusion Detection. ACM.
Andersson, D. (1995). Detecting usual program behavior
using the statistical component of the next-generation
intrusion detection expert system (nides). Technical
report, Computer Science Laboratory.
Bace, R. G. (2000). Intrusion Detection. Macmillan Tech-
nical Publishing.
Baker, A. R., Caswell, B., and Poor, M. (2004). Snort 2.1
Intrusion Detection - Second Edition. Syngress.
Barbara, D., Couto, J., Jajodia, S., Popyack, L., and Wu,
N. (2001). Adam: Detecting intrusion by data min-
ing. pages 11–16. IEEE. Workshop on Information
Assurance and Security.
Cohen, W. W. and Singer, Y. (1999). A simple, fast, and
effective rule learner.
Elkan, C. (2000). Results of the kdd99 classiﬁer learning. In
SIGKDD Explorations, volume 1, pages 63–64. ACM.
Fayyad, U., Piatetsky-Shapiro, G., and Smyth, P. (1996).
From data mining to knowledge discovery in data-
bases. AI Magazine, pages 37–52.
Laing, B. and Alderson, J. (2000).
How to guide - im-
plementing a network based intrusion detection sys-
tem. Technical report, Internet Security Systems, Sov-
ereign House, 57/59 Vaster Road, Reading.
Lee, W. and Stolfo, S. J. (2000). A framework for con-
structing features and models for intrusion detection
systems. ACM Transactions on Information and Sys-
tem Security (TISSEC), 3(4):227–261.
Paxson, V. and Terney, B. (2004). Bro reference manual.
Sleator, D. and Tarjan, R. (1985). Self Adjusting Binary
Search Trees. Journal of the ACM, 32(3).
Tyson, M. (2000). Derbi: Diagnosys explanation and re-
covery from computer break-ins. Technical report.
Vigna, G. and Kemmerer, R. (1999). Netstat: a network
based intrusion detection system.
Journal of Com-
puter Security, 7(1).
6http://sourceforge.net/projects/
s-predator
Marcello Esposito et al. 
204
Figure 7: CPU Usage - CNR Network.
tive of the employment of non-dedicated hardware.

PART 4 
Software Agents and
Internet Computing 

GENERIC FAULT-TOLERANT LAYER SUPPORTING 
PUBLISH/SUBSCRIBE MESSAGING
IN MOBILE AGENT SYSTEMS 
Milovan Tosic and Arkady Zaslavsky 
 School of Computer Science and Software Engineering, Monash University, 900 Dandenong Road 
Caulfield East, Victoria 3145, Australia 
Email: Milovan.Tosic@csse.monash.edu.au, Arkady.Zaslavsky@csse.monash.edu.au
Keywords: 
Reliability, Fault-tolerance, Agents, Multi-agent Systems, Publish/Subscribe Messaging, Web-services. 
Abstract: 
With the introduction of clustered messaging brokers and the fault-tolerant Mobile Connector, we can 
guarantee the exactly-once consumption of messages by agents. The context-aware messaging allowed us to 
decrease the messaging overhead which has to be present in any fault-tolerant solution. This paper proposes 
a complete fault-tolerant layer for multi-agent systems (EFTL) that does not restrict agent autonomy and 
mobility in any way. An application can choose if it wants EFTL support and that decision is based on 
support costs. A persistent publish/subscribe messaging model allows the creation of an external platform-
independent fault-tolerant layer. In order to support the multi-agent platforms of different vendors, a large 
part of the application logic is moved from those platforms to an application server. We present the EFTL 
system architecture, the algorithm of exactly-once message consumption and the system’s performance 
analysis. 
1 INTRODUCTION 
The use of web-services in many domains of 
distributed computing has proven its effectiveness. 
However, the research community has not paid a lot 
of attention to the application of web-services in 
domain-independent fault-tolerant support systems. 
Our 
External 
Fault-Tolerant 
Layer 
(EFTL) 
introduces a new dimension in this research area – 
moving the components of the fault-tolerant system 
out of the multi-agent platforms using standard 
tools, 
web-services 
and 
messaging 
brokers. 
Moreover, an application or an agent can choose if it 
wants EFTL support and that decision is based on 
support costs. 
The most important factors which can affect 
reliability of multi-agent systems are related to the 
reliability levels of their components. Multi-agent 
systems are comprised of different entities where the 
most important ones are the agents and agent hosts. 
In regard to the basic systems theory, the 
performance of the whole system and its outputs 
depends on the actions and performance of its 
entities. That means that the performance of the 
complete multi-agent system depends on the 
performance of its agents and hosts. On the other 
hand, not all the entities within a system have the 
same level of importance, so the failure of some 
entities might not cause the failure of the whole 
system. That is why fault-tolerant approaches can 
ignore some failures in order to lower the cost of a 
fault-tolerant solution.
Another category that can cause a failure of the 
system is communication. Knowing that the 
achievement of a goal usually depends on 
cooperation between the agents, we can conclude 
that any fault of a communication subsystem can 
produce the difference between the real and 
expected outcomes of a system. Agent migration 
between the hosts can be viewed as a special type of 
communication because many agent platforms use 
the same mechanisms for message and agent 
transfer. If an agent is lost during transmission from 
one host to another, then it is not an agent failure but 
a migration failure. 
The persistent publish/subscribe messaging 
model allows the creation of an external platform-
independent fault-tolerant support system. The most 
important part of any distributed fault-tolerant 
support system is its messaging subsystem. With the 
207
C.-S. Chen et al. (Eds.), Enterprise Information Systems VII, 207–214.
© 2006 Springer.

introduction of clustered messaging brokers and the 
fault-tolerant Mobile Connector, we can guarantee 
the exactly-once consumption of messages by 
agents. The Mobile Connector is a lightweight 
platform-independent component which does not 
restrict agent autonomy and mobility. 
This paper is organized as follows: firstly, we 
shall present related work from the area of multi-
agent system reliability. Then, we shall explain the 
reliability model which has been used in our 
research and describe the architecture of the External 
Fault-Tolerant Layer (EFTL) with focus on the 
Mobile Connector component. After that, we shall 
present a few scenarios in EFTL functioning and 
explain what needs to be done to develop an 
application that will be supported by EFTL. The last 
sections of this paper will present performance 
analysis of EFTL, the conclusions and motivations 
for future work. 
2 RELATED WORK 
A group of authors proposed checkpointing as a 
good procedure which saves agent states to a 
persistent storage medium at certain time intervals. 
Later, if an agent fails, its state can be reconstructed 
from the latest checkpoint (Dalmeijer et al, 1998). 
This approach depends on the reliability of the host 
because we have the so-called blocking problem 
when the host fails. The agents which have been 
saved at a particular host can be recovered only after 
the recovery of that host (Mohindra et al, 2000). The 
second approach that tries to ensure an agent’s 
reliability is replication. In this approach, there are 
groups of agents which exist as replicas of one 
agent, and can be chosen to act as the main agent in 
case of its failure. The number of agents is increased 
and they have to cooperate so the complexity of the 
system is also increased. In order to preserve the 
same view to the environment from all the members 
of the replica group, (Fedoruk, Deters, 2002) have 
proposed the concept of a group proxy, which is an 
agent acting as proxy through which all the 
interactions between the group and the environment 
have to pass. When the proxy agent approach is 
broadened with the primary agent concept, in 
(Taesoon et al, 2002) and (Zhigang, Binxing, 2000), 
then the primary agent is the only one which does all 
the computations until its failure. Then all the slaves 
vote in another primary agent from their group. 
Therefore, any slave agent can become a primary. 
In order to watch the execution of an agent from 
an external entity, (Eustace et al, 1994), (Patel, Garg, 
2004) and (Lyu, Wong, 2004) have proposed the 
usage of supervisor and executor agents. The 
supervisor agents watch the execution of the 
problem-solving agents and detect all the conditions 
which can lead to, or are, the failures, and react upon 
detected conditions. Hosts can also be used as the 
components of a fault-tolerant system (Dake, 2002). 
Basic services which are provided by the hosts can 
be extended by certain services which help the 
agents achieve a desirable level of reliability. 
Depending on the implementation of the fault-
tolerant system, it cannot cope with all kinds of 
failures. That is why some systems do not even try 
to recover from certain types of failures. In order to 
determine the feasibility of the recovery, (Grantner 
et al, 1997) proposed the usage of fuzzy logic. 
Moving on to the recovery of an agent host, if 
the state of the host has not been saved to a 
persistent storage medium, we can simply restart the 
host. Then, if a host is very important for the 
functioning of the whole agent platform, we can 
replicate it (Bellifemine et al, 2003). If our agents 
used the transaction-based approach which relied on 
the services provided by the host and not by an 
underlying application server or a database, then the 
host is the one which has to undo all the 
uncommitted actions after its restart (Patel, Garg, 
2004).
In order to deliver a message to an agent, we 
have to track the agent’s location to determine where 
to forward the message. The authors have proposed 
different solutions, such as the registration of the 
agent locations at some central entity (Moreau, 
2002) or the usage of the forwarding pointers 
principle (Zhou et al, 2003). Then, when we know 
the exact location of the agent, we have to deliver 
the message. Two main delivery principles have 
been specified in (Cao et al, 2002). In the “push” 
principle, we have to interfere with an agent’s 
autonomy and to constrain its mobility until we 
deliver the messages to it. In the “pull” principle, the 
agent is the one which decides when it wants to 
receive messages, and which messages it wants to 
receive. (Cao et al, 2004) have proposed the mailbox 
as a separate entity that is also mobile and moves to 
be at the same host as its agent or somewhere close 
to that host. 
The benefits of the publish/subscribe messaging 
model in mobile computing have been presented in 
(Padovitz et al, 2003). Their approach specifically 
concentrates on context-aware messaging, where an 
agent can subscribe to receive only the messages 
which satisfy its subscription filter. This solution 
leads us to a highly effective notification mechanism 
for the mobile agents. 
Another 
communication 
problem, 
the 
inaccessibility in the case of, for example, network 
fragmentation can be solved using the doubler 
Milovan Tosic and Arkady Zaslavsky 
208
agents, presented in (Pechoucek et al., 2003). 

Multicasting is the delivery of the same message to 
multiple receivers, and is often described by the “all 
or none” principle. Researchers usually used the 
two-phase commit protocol to solve this problem, as 
in (Macedo, Silva, 2002).  
3 RELIABILITY MODEL 
The reliability of multi-agent systems has to be 
measured differently from the reliability of other 
conventional distributed systems. Since almost all 
multi-agent systems share characteristics such as 
network fragmentation, component autonomy and 
mobility, then standard factors of reliability, like 
system availability, cannot be applied to them. 
Therefore, we have to find another reliability model 
able to describe the events which can cause multi-
agent system failures and allow us to evaluate our 
research achievements.  
As described in (Luy, Wong, 2004), reliability in 
multi-agent systems can be evaluated by measuring 
the reliability of each individual agent on a more 
general level. From the viewpoint of the whole 
system, each agent can either successfully complete 
its tasks or fail to do so. Therefore, the reliability of 
the whole system depends on the percentage of 
agents which managed to achieve their goals. 
 The same authors proposed that the agent tasks 
should be defined as scheduled round-trips in a 
network of agent hosts.  
In order to evaluate reliability, we can assume 
that the agents and agent hosts are prone to different 
types of failures. The agents can die unexpectedly or 
become non-responsive. A host can die and cause 
the failures of all the agents which resided on it at 
the moment of its death. Only the agent which 
managed to arrive at the final host and which has a 
state consistent with the states of all the other 
successful agents can be considered a successful 
finisher.  
4 DESIGN OF EFTL 
EFTL (External Fault-Tolerant Layer) is an 
application-independent fault-tolerant layer that 
provides multi-agent systems with extra reliability 
features. The system diagram is presented in Figure 
1. In order to support multi-agent platforms from 
different vendors, a large part of application logic is 
moved from those platforms to the application, web 
and messaging servers. The only platform-dependent 
components are the Reliable Agent Layer and the 
Platform Listener. They support only the basic 
operations which EFTL has to perform on an agent 
or agent platform. Those operations include the 
control of an agent’s life cycle and listening to the 
platform-wide events which are important from a 
reliability perspective.
The Platform Listener is not deployed at any 
agent host prior to EFTL execution time. The usage 
of a web-server allowed us to decouple the agent 
platform and the Platform Listener. It is installed by 
the Reliable Agent Layer only when EFTL decides 
that the listener functionality is needed. The Reliable 
Agent Layer downloads a Platform Listener class 
from a web-server and deploys it at an agent host.  
The costs of EFTL support can be expressed in 
monetary terms or system resources that have to be 
used for the functioning of EFTL. An application or 
an agent can decide whether those costs are 
acceptable in line with the additional reliability that 
EFTL provides.  
Our fault-tolerant solution employs a persistent 
publish/subscribe messaging model. It was the 
premise that allowed us to develop an almost 
completely 
external 
and 
platform-independent 
system. 
With 
the 
introduction 
of 
clustered 
messaging brokers and the fault-tolerant Mobile 
Connector, we can guarantee the exactly-once 
consumption of messages by the agents. 
4.1 The Mobile Connector 
After an agent registers with EFTL, it obtains the 
credentials needed to make subscriptions or to 
publish a message to a message topic. The Mobile 
Connector is a facility that allows agents to 
communicate independently to the changes in their 
Generic Fault-tolerant Layer Supporting Publish/subscribe Messaging in Mobile Agent Systems 
209
Figure 1: EFTL Architecture.

life cycles. It defines message selectors which can be 
used to allow context-aware messaging within a 
multi-agent platform. 
The Mobile Connector is used to subscribe 
and/or publish to a message topic. If a message-
receiving acknowledgement does not reach the 
message broker, due to link problems, then the 
message is resent. The agent would receive another 
copy of the same message. To enforce the exactly-
once property, every message published in EFTL is 
uniquely numbered. This allows the Mobile 
Connector to discard messages which have already 
been consumed. This process can be represented by 
the following pseudo-code: 
while(subscribed to a topic) 
begin
wait for next message; 
 receive message; 
read unique MsgID; 
if(MsgID <= ConsumedMsgID) 
begin
    discard the message; 
  end 
   else 
  begin 
    consume the message; 
    ConsumedMsgID = MsgID; 
  end 
end-while;
The actions of the Mobile Connector depend 
upon the changes in agent lifecycle and that is why 
the Reliable Agent Layer informs its Mobile 
Connector about each relevant change. Then, the 
Mobile Connector is able to perform all the 
operations that precede or follow events which can 
cause temporary disconnections from a message 
broker. At the moment of its creation, the Mobile 
Connector registers itself with the message broker, 
and creates both a publisher and a durable 
subscriber. Once it establishes the connection to the 
broker, this component makes sure that the link is 
reliable and if it detects a disconnection, it 
reconnects to the broker. 
To solve the problem of message loss during 
periods of disconnection from the message broker, 
the Mobile Connector only disconnects from the 
message broker prior to the next migration step, but 
its primary subscription stays valid at the broker so 
that all the missed messages are forwarded to the 
agent as soon as it reconnects to the broker. This 
way, the Mobile Connector does not affect an 
agent’s autonomy but guarantees message delivery. 
4.2 The EFTL Web-services 
Reliable agents use web-services for cost evaluation, 
contract signing, initial registrations with EFTL, 
deregistration from EFTL and redundant ways of 
communication if the standard publish/subscribe 
model fails. In order to register an agent and provide 
it with fault-tolerant support, web-services need 
information about the agent platform and that 
particular agent. 
Web-services act as a gateway from the Reliable 
Agent Layer to the other EFTL components - the 
Fault-Tolerant System Manager and the Messaging 
Broker Management Module. 
A multi-agent system can be comprised of 
reliable agents, which are supported by EFTL, and 
non-reliable agents. However, cooperation in the 
system is not constrained by the category to which 
the agents belong. EFTL controls only the agents 
with the Reliable Agent Layer, but listens to 
communication between all the agents within a 
platform. It can recover only reliable agents. The 
recovery of an agent might require the use of 
information about communication of that agent with 
all the other agents, irrespective of whether those 
agents were reliable or not. 
4.3 Negotiable EFTL Support 
Based on application domain, system goals and 
specific performance requests, developers can 
identify the agents which are critical to their 
mission. According to that information, the agents 
which have to be supported by EFTL and therefore 
provided with fault-tolerant support, need to have 
valid contracts with EFTL. A multi-agent system 
can be classified in one of three groups in regard to 
its demand for reliability: it can be either a high, 
medium or low demand system. If a system has a 
high demand for reliability, it probably has a mission 
critical application which wants EFTL support by all 
means and at any cost. Therefore, it will sign a 
contract with EFTL without any negotiation. If it has 
a medium demand for reliability, it needs to 
negotiate the costs with EFTL before it makes a 
decision about its support. If a system has a low 
demand for reliability, it will not use EFTL support. 
The negotiation of costs by a system which has 
medium demand for reliability can be done by each 
agent separately or by an external application.  
If an agent negotiates the costs with EFTL, it has 
to read an activation profile which defines the costs 
the agent is ready to accept. Those costs are sent to 
the Contractor web-service which compares them to 
the real costs dependant on the platform type and 
agent size. If the accepted costs are lower or equal to 
Milovan Tosic and Arkady Zaslavsky 
210

the real costs, the agent signs a contract for EFTL 
support.
If an external application negotiates the costs 
with EFTL, it has to forward information about the 
agent platform and agents which are going to be 
used, to the Contractor web-service. In order to 
facilitate the negotiation process, the application can 
use the EFTL Negotiator class which is distributed 
as part of the EFTL system. The Contractor web-
service returns the real costs so that the application 
can choose whether or not it wants EFTL support. If 
it decides to sign a contract with EFTL, that contract 
holds information about all the agents that are going 
to be supported. Therefore, the application has to 
forward the contract details to every agent in order 
to allow them to register with EFTL. 
4.4 Recovery Procedures 
The checkpointing procedures used in EFTL do not 
affect agent mobility. When an agent decides to 
move quickly to another host, its Reliable Agent 
Layer might not have time to save a local 
checkpoint. If the agent fails, EFTL will have to try 
to find an earlier checkpoint in order to recover that 
agent. This kind of checkpointing is developed in 
order to preserve agent autonomy and not restrict 
mobility. 
When the Platform Listener detects an event 
which might impair the functioning of the overall 
system, it notifies FTSM (Fault-Tolerant System 
Manager). Following the detection of the agent’s 
death, FTSM, which listens to the topic that the 
Platform Listener published the notification to, 
decides to recover the agent. It sends the recovery 
command to another reliable agent closest to the 
place where the recovery is going to take place. That 
reliable 
agent 
performs 
the 
whole 
recovery 
procedure. The dead agent is recovered from its last 
checkpoint if its host is alive and functioning. If the 
host is not alive, in order to prevent the problem of 
blocking while we wait for the host to recover, 
EFTL, using the web service which has access to the 
hosts registry, finds out which other hosts provide 
the same services as the failed host. Then, the agent 
is sent to the host which is most similar to the failed 
one. When the agent is recovered, EFTL resends all 
the messages which have been produced by other 
agents from the moment of its checkpoint save until 
the moment of its death. This way the state of the 
agent is driven to the point in which it is consistent 
with the states of all the other agents present in the 
system, regardless of whether they are reliable or 
not.
When EFTL detects that an agent’s life cycle has 
not changed during a longer period of time, it may 
decide to ping the agent using the publish/subscribe 
messaging subsystem. If the agent does not respond 
to the pings, EFTL concludes that it is non-
responsive, removes it from the multi-agent system 
and recovers it from the latest checkpoint. 
EFTL checks whether the recovery process is 
finished in a timely manner. An agent taking 
responsibility for the recovery of another agent 
might fail during that recovery process. Then EFTL 
tries to find another agent capable of doing that 
work. EFTL ceases the recovery if it cannot be 
finished within a predefined period of time. 
Moreover, EFTL will not be able to recover an agent 
which did not save any of its checkpoints. 
EFTL uses the web service to access hosts 
registry in case of resource unavailability. When an 
agent is blocked due to inaccessible resources, EFTL 
sends it to a host which has exactly the same or 
similar resources.  
In the case where network partitioning is 
detected, EFTL watches the agent actions and 
prevents them from running into situations which 
can cause their failures. When an agent (e.g. Agent 
A) is unable to move to a destination host, EFTL 
searches the system to find another agent (e.g. Agent 
B) of the same type which can move to the 
destination host. If such an agent is found, EFTL 
clones Agent B and sends it to the destination host. 
Then, using the publish/subscribe system and Java 
reflection mechanism, EFTL updates Agent B’s state 
with the state of Agent A which is removed from the 
system. 
5 APPLICATION DEVELOPMENT 
WITH EFTL 
The external part of EFTL has to be deployed to the 
application and web servers. It is distributed in the 
form of Java archive files, so the process of 
deployment is simple. Those files have to be copied 
to the deployment folders of the application and web 
servers. The next step is initialization of the 
messaging subsystem. If the messaging subsystem is 
a part of the application server, it is usually started at 
the same time as the server instance. If the 
messaging subsystem is a separate application, it has 
to be started and configured for proper use by EFTL. 
Configuration of each messaging system depends on 
what levels of reliability, fail-over and scalability are 
required within the messaging subsystem. 
The Reliable Agent Layer class has to be visible 
to a problem-solving agent class. The developer 
does not have to implement any new methods 
related to the special fault-tolerant features. If the 
Generic Fault-tolerant Layer Supporting Publish/subscribe Messaging in Mobile Agent Systems 
211

Reliable Agent Layer is implemented as a separate 
class, the public methods of the basic Agent class 
are defined as final. Then the problem-solving agent 
cannot implement those methods in its code. It can 
only implement the methods of the Reliable Agent 
Layer class which have similar names. For example, 
if the Agent’s method name was beforeMove, the 
Reliable 
Agent 
Layer’s 
name 
would 
be 
beforeReliableMove. If the agent platform’s license 
allows changing of its source code, the Reliable 
Agent Layer functionality can be embedded in the 
basic Agent class. Then, the developers could use 
the same method names as in the Agent class. 
Special property files have to be present at the 
host in which the reliable agents are being 
initialized. These configurable files are read by the 
Reliable Agent Layer. They provide the layer with 
the information on how to connect to the rest of 
EFTL. The developer or the system administrator 
has to edit the connection values in these files after 
the application, web and messaging servers have 
been configured. 
6 PERFORMANCE ANALYSIS 
Performance analysis of EFTL had to include two 
distinct categories which are applicable to any fault-
tolerant system: its reliability level and messaging 
overhead. 
Context-awareness 
of 
the 
EFTL 
messaging subsystem was designed with one 
objective in mind - to reduce messaging overhead 
between components of the fault-tolerant system. 
Tests were conducted in the JADE environment, 
using a fixed number of hosts distributed on 
different computers. Types of failures simulated in 
these tests were host and agent deaths. The host 
death rate was constant - one host failure per ten 
seconds. Every failed host was restarted after five 
seconds. The agent death rate was variable, and was 
a parameter of the simulation process. The choices 
of which hosts or agents should be killed for test 
purposes were random.  
All the tests included a number of mobile agents 
which had to complete their round-trips across the 
network of hosts. Their itineraries were dynamically 
determined at the time of start-up. They stayed five 
seconds at each of the hosts. The percentage of 
agents which succeeded in visiting all the hosts and 
returning to the place of their origin determined the 
level of system reliability. 
Our first experiment included a fixed number of 
mobile agents and a variable number of agent faults. 
The results of this experiment are shown in Figure 2. 
It can be concluded that EFTL greatly improves 
system reliability, even in the cases of high failure 
rates. If we compare the reliability of a multi-agent 
system with and without EFTL support, we can see 
that EFTL is capable of delivering a high reliability 
level to a system which would completely fail 
without its support.
Figure 2: System reliability with and without EFTL 
The following experiments calculated the 
messaging overhead which was generated by EFTL. 
In the first experiment calculating this overhead, we 
used the formula: 
   O – messaging overhead 
   n – number of reliable agents 
   m – number of published EFTL messages 
   Mij – size of the message j, published or received   
            by the agent i
   (equal to 0 if the agent i did not publish or receive   
     message j)
The experiments showed that there was no 
dependency between the agent failure rate and the 
messaging overhead. However, the overhead was 
related to the number of mobile agents presented in 
the test-bed system.  
Figure 3: Messaging overhead [size of published 
Milovan Tosic and Arkady Zaslavsky 
212
support.
messages].

As can be seen in Figure 3, messaging overhead 
slightly increases with the number of reliable agents 
in the system. However, this messaging overhead is 
so small that it can be compared to the overhead of 
the migration of one agent between two hosts.  
In addition, Figure 4 shows that the number of 
messages published in the EFTL internal messaging 
subsystem grows with the number of reliable agents 
present in a multi-agent system. The publishing and 
delivery of these messages is rapid because they are 
small in content size. This does not generate any 
notable operation slowdown on an agent level.  
Figure 4: Messaging overhead [number of published 
7 CONCLUSION AND FUTURE 
WORK
EFTL introduced a substantial amount of platform-
independence in the multi-agent fault-tolerant 
approaches. The idea to move as many components 
of the fault-tolerant system out of the agent 
platforms, using standard tools, allowed us to 
preserve agent autonomy and mobility. In addition 
to this, we proposed the use of the persistent 
publish/subscribe messaging model which employs 
context-aware message selection at the message 
brokers. This allowed us to decrease the messaging 
overhead which has to be present in any fault-
tolerant solution. Our modification of the model, 
with the usage of the Mobile Connector, guarantees 
the exactly-once consumption of messages. Since all 
the components of EFTL can inherit fault-tolerance 
and scalability of the application, web and 
messaging servers, we can claim that our approach 
offers an extra level of reliability and high 
availability. Moreover, EFTL introduced negotiable 
fault-tolerant support based on the costs. An 
application or an agent can choose if it wants an 
extra level of reliability accompanied by costs that 
can be expressed in monetary terms or the additional 
usage of system resources. 
Our future work will be focused on the 
development of different platform-independent 
checkpointing procedures with the use of Java 
reflection, and on the more adaptive mechanisms of 
EFTL usage and deployment in regard to its 
overheads. 
REFERENCES
Bellifemine, F.; Caire, G.; Trucco, T.; Rimassa, G., 2003. 
JADE administrator’s guide, TILAB S.p.A., Italy 
Cao, J.; Feng, X.; Lu, J.; Chan, H.; Das, S.K., 2002. 
Reliable message delivery for mobile agents: push or 
pull,
Parallel and Distributed Systems, 2002. 
Proceedings Ninth International Conference on, 314 – 
320.
Cao, J.; Zhang, L.; Yang, J.; Das, S.K., 2004. A reliable 
mobile agent communication protocol, Distributed 
Computing 
Systems, 
2004. 
Proceedings. 
24th 
International Conference on, 468 – 475. 
Dake, W.; Leguizamo, C.P.; Mori, K., 2002. Mobile agent 
fault tolerance in autonomous decentralized database 
systems, Autonomous Decentralized System, 2002. 
The 2nd International Workshop on, 192 – 199. 
Dalmeijer, M.; Rietjens, E.; Hammer, D.; Aerts, A.; 
Soede, 
M., 
1998. 
A 
reliable 
mobile 
agents 
architecture, Object-Oriented Real-Time Distributed 
Computing, 1998. (ISORC 98) Proceedings. 1998 
First International Symposium on, 64 – 72. 
Eustace, D.; Aylett, R.S.; Gray, J.O., 1994. Combining 
predictive and reactive control strategies in multi-
agent systems, Control, 1994. Control '94. Volume 2., 
International Conference on, 989 – 994. 
Fedoruk, A.; Deters, R., 2002. Improving fault-tolerance 
by replicating agents, Proceedings of the first 
international joint conference on Autonomous agents 
and multiagent systems: part 2, ACM Press New 
York, NY, USA, ISBN:1-58113-480-0, 737 – 744. 
Grantner, J.L.; Fodor, G.; Driankov, D., 1997. Using fuzzy 
logic for bounded recovery of autonomous agents, 
Fuzzy Information Processing Society, 1997. NAFIPS 
'97. 1997 Annual Meeting of the North American, 317 
– 322. 
Lyu, R. M.; Wong, Y. T., 2004. A progressive fault 
tolerant 
mechanism 
in 
mobile 
agent 
systems, 
Retrieved 
April 
25, 
2004, 
from 
http://www.cse.cuhk.edu.hk/~lyu/paper_pdf/ 
SCI2003.pdf
Macedo, A.; Silva, F., 2002. Coordination of mobile 
processes with mobile groups, Dependable Systems 
and Networks, 2002. Proceedings. International 
Conference on,  177 – 186. 
Mohindra, A.; Purakayastha, A.; Thati, P., 2000. 
Exploiting non-determinism for reliability of mobile 
agent systems, Dependable Systems and Networks, 
Messaging overhead
0
20
40
60
80
100
120
140
160
180
10
12
14
16
18
20
22
24
26
28
Nr. of agents
Nr. of messages
Generic Fault-tolerant Layer Supporting Publish/subscribe Messaging in Mobile Agent Systems 
213
messages].

DSN 2000. Proceedings International Conference 
on, 144 – 153. 
Moreau, L., 2002. A fault-tolerant directory service for 
mobile 
agents 
based 
on 
forwarding 
pointers, 
Proceedings of the 2002 ACM symposium on Applied 
computing, ACM Press New York, NY, USA, 
ISBN:1-58113-445-2, 93 – 100. 
Padovitz, A.; Zaslavsky, A.; Loke, S. W., 2003. 
Awareness and Agility for Autonomic Distributed 
Systems: 
Platform-Independent 
Publish-Subscribe 
Event-Based Communication for Mobile Agents, the 
1st International Workshop on Autonomic Computing 
Systems, DEXA 2003, Prague, Czech Republic 
Patel, R. B.; Garg, K., 2004. Fault-tolerant mobile agents 
computing on open networks, Retrieved April 18, 
2004, 
from 
http://www.caip.rutgers.edu
 
/~parashar/AAW-HiPC2003/patel-aaw-hipc-03.pdf 
Pechoucek, M.; Dobisek, M.; Lazansky, J.; Marik, V., 
2003. 
Inaccessibility 
in 
multi-agent 
systems, 
Intelligent Agent Technology, 2003. IAT 2003. 
IEEE/WIC International Conference on, 182 – 188.
Taesoon, P.; Ilsoo, B.; Hyunjoo, K.; Yeom, H.Y., 2002. 
The performance of checkpointing and replication 
schemes for fault tolerant mobile agent systems, 
Reliable Distributed Systems, 2002. Proceedings. 21st 
IEEE Symposium on, 256 – 261. 
Zhigang, W.; Binxing, F., 2000. Research on extensibility 
and reliability of agents in Web-based Computing 
Resource Publishing, High Performance Computing in 
the Asia-Pacific Region, 2000. Proceedings. The 
Fourth 
International 
Conference/Exhibition 
on, Volume: 1, 432 – 435. 
Zhou, J.; Jia, Z.; Chen, D., 2003. Designing reliable 
communication 
protocols 
for 
mobile 
agents, 
Distributed Computing Systems Workshops, 2003. 
Proceedings. 23rd International Conference on, 484 – 
487.
Milovan Tosic and Arkady Zaslavsky 
214

BOOSTING ITEM FINDABILITY: BRIDGING THE SEMANTIC 
GAP BETWEEN SEARCH PHRASES AND ITEM INFORMATION
Hasan Davulcu, Hung V. Nguyen and Viswanathan Ramachandran
Department of Computer Science and Engineering, Arizona State University Tempe, AZ 85287, USA 
Email: {hasandavulcu,hung,vishr}@asu.edu 
Keywords: 
E-commerce, Data Mining, Frequent Itemsets, Web Data, Information Retrieval, Information Extraction, 
Relevance Feedback.  
Abstract: 
Most search engines do their text query and retrieval based on keyword phrases. However, publishers 
cannot anticipate all possible ways in which users search for the items in their documents. In fact, many 
times, there may be no direct keyword match between a search phrase and descriptions of items that are 
perfect “hits” for the search. We present a highly automated solution to the problem of bridging the 
semantic gap between item information and search phrases. Our system can learn rule-based definitions that 
can be ascribed to search phrases with dynamic connotations by extracting structured item information 
from product catalogs and by utilizing a frequent itemset mining algorithm. We present experimental results 
for a realistic e-commerce domain. Also, we compare our rule-mining approach to vector-based relevance 
feedback retrieval techniques and show that our system yields definitions that are easier to validate and 
perform better.
1 INTRODUCTION 
Most search engines do their text query and retrieval 
using keywords. The average keyword query length 
is under three words (2.2 words (Crescenzi, 2000)). 
Recent research (Andrews, 2003) found that 40 
percent of companies rate their search tools as “not 
very useful” or “only somewhat useful.” Further, a 
review of 89 sites (Andrews, 2003) found that 75 
percent have keyword search engines that fail to 
retrieve important information and put results in 
order of relevance; 92 percent fail to provide guided 
search interfaces to help offset keyword deficiencies 
(Andrews, 2003), and seven out of 10 web shoppers 
were unable to find products using the search 
engine, even when the items were stocked and 
available.
The Defining Problem: Publishers cannot 
anticipate all possible ways in which users search for 
the items in their documents. In fact, many times, 
there may be no direct keyword match between a 
search phrase and descriptions of items that are 
perfect “hits” for the search. For example, if a 
shopper uses “motorcycle jacket” then, unless the 
publisher or search engine knows that every “leather 
jacket” is a “motorcycle jacket”, it cannot produce 
all matches for user’s search. Thus, for certain 
phrases, there is a semantic gap between the search 
phrase used and the way the corresponding matching 
items are described. A serious consequence of this 
gap is that it results in unsatisfied customers. Thus 
there is a critical need to boost item findability by 
bridging the semantic gap that exists between search 
phrases and item information.  Closing this gap has 
the strong potential to translate web search traffic 
into higher conversion rates and more satisfied 
customers. 
Issues in Bridging the Semantic Gap: We 
denote a search phrase to be a “target search 
phrase” if does not directly match certain relevant 
item descriptions.  The semantics of items matching 
such “target search phrases” is implicit in their 
descriptions. For phrases with fixed meanings i.e. 
their connotations do not change such as in “animal 
print comforter”, it is possible to close the gap by 
extracting their meaning with a thesaurus (Voorhees, 
1998) and relating it to product descriptions, such as 
“zebra print comforter” or “leopard print bedding” 
etc. Where they pose a more interesting challenge is 
when their meaning is subjective, driven by 
perceptions, and hence their connotations change 
over time as in the case of “fashionable handbag” 
and “luxury bedding”. The concept of a fashionable 
handbag is based on trends, which change over time, 
and 
correspondingly 
the 
attribute 
values 
characterizing such a bag also changes. Similarly, 
215
C.-S. Chen et al. (Eds.), Enterprise Information Systems VII, 215–222.
© 2006 Springer.

the concept of “luxury bedding” depends on the 
brands and designs available on the market that are 
considered as luxury and their attributes. Bridging 
the semantic gap therefore is in essence the problem 
of inferring the meaning of search phrases in all its 
nuances. 
Our Approach: In this paper we present an 
algorithm that (i) structures item information and (ii) 
uses a frequent itemset mining algorithm to learn the 
“target phrase” definitions.
2 RELATED WORKS  
In (Aholen, 1998), generalized episodes and episode 
rules are used for Descriptive Phrase Extraction. 
Episode rules are the modification of association 
rules and episode is the modification of frequent set.  
An episode is a collection of feature vectors with a 
partial order; authors claimed that their approach is 
useful in phrase mining in Finnish, a language that 
has the relaxed order of words in a sentence. In our 
previous work (Nguyen, 2003), we present a co-
occurrence clustering algorithm that identifies 
phrases that frequently co-occurs with the target 
phrase from the meta-tags of Web documents. 
However, in this paper we address a different 
problem; we attempt to mine the phrase definitions 
in terms of extracted item information, thus, the 
mined definitions can be utilized to connect “search 
phrases” to real items in all their nuances. 
     The frequent itemset mining problem is to 
discover a set of items shared among a large number 
of records in the database. There are two main 
search strategies to find the frequent items set. 
Apriori (Agrawal, 1994) and several other Apriori 
like algorithms adopt Breadth-First-Search model, 
while Eclat (Zaki, 2000) and FPGrowth (Han, 2000) 
are well known algorithms that employ Depth-First 
manner to search all frequent itemsets of a database. 
Our algorithm also searches for frequent itemsets in 
a Depth-First manner. But, unlike the lattice 
structure used in Eclat or the conditional frequent 
pattern tree used in FPGrowth, we propose the so 
called 2-frequent itemset graph and utilize heuristic 
syntheses to prune the search space in order to 
improve the performance. We plan to further 
optimize our algorithm and conduct detailed 
comparisons to the above algorithms. 
The relevance feedback (Salton, 1990) method 
can also be used to refine the original keyword 
phrase by using the document vectors (Baeza-Yates, 
1999) of the extracted relevant items as additional 
information. In Section 6, we present experimental 
results and show that the rules that our system 
learns, by utilizing the extracted relevant item 
information, are easier to validate and perform better 
than retrieval with the relevance feedback method. 
3 SYSTEM DESCRIPTION 
I. Item Name Structuring:  This component takes a 
product 
catalogue 
and 
extracts 
structured 
information for mining the phrase based and 
parametric definitions. Details are discussed in 
Section 4. 
II. Mining Search Phrase Definitions: In this 
phase, we divide the phrase definition mining 
problems into two sub problems (i) mining the 
parametric definitions from extracted attribute value 
pairs of items, and (ii) mining phrase based 
definitions from the long item descriptions. Details 
are discussed in Section 5. 
4 DATA LABELING 
This section presents the techniques for an e-
commerce domain, for the sake of providing 
examples. Our techniques can be customized for 
different domains.  The major tasks in this phase are 
structuring and labeling of extracted data. The 
readers are also referred to (Davulcu, 2003) for more 
information in details. 
4.1 Labeling and Structuring 
Extracted Data 
This section describes a technique to partition the 
short product item names into their various 
attributes. We achieve this by grouping and aligning 
the tokens in the item names such that the instances 
of the same attribute from multiple products fall 
under the same category indicating that they are of 
similar types.  
The motivation behind doing the partition is to 
organize data. By discovering attributes in product 
data and arranging the values in a table, one can 
build a search engine which can enable quicker and 
precise product searches in an efficient way.  
4.2 The Algorithm 
Before proceeding to the algorithm, it helps to 
identify item names as a sequence of tokens obtained 
when white-space is used as a delimiter. Since the 
sequences of tokens obtained from item names are 
Hasan Davulcu et al. 
216

all from a single web page and belong to the same 
category, they are likely to have a similar pattern. As 
mentioned before, our algorithm is designed to 
process collections of such item names without any 
labeling whatsoever. So it can be performed on the 
fly as and when data is extracted from the web sites. 
Following are the general properties of the data our 
algorithm can process: 
Super-Tokens: Any pair of tokens t1, t2 that always 
co-occur together and occur more than once belong 
to a multi token instance of a type.  
Context: All single tokens occurring between 
identical attribute types belong to the same type. 
This means that if two tokens t1 and t2 from distinct 
item names occur in between same types TL and TR
then they should be of the same type.  
Anchor Type: A token that uniquely occurs within 
all item names should belong to a unique type, 
which we call an Anchor Type.
Density: 
Attribute 
types 
should 
be 
densely 
populated. Meaning that, every type should occur 
within the majority of item names.  
Ordering: Pairwise ordering of all types should be 
consistent within a collection. 
Tokenization: The item names are tokenized by 
using white space characters as delimiters. Tokens 
are stemmed so using the Porter Stemmer (Porter, 
1980).  
Super Tokenization: The second step identifies 
multi-token attributes.  
Initialization of Types: To initialize, every item 
name is prefixed and suffixed with a Begin and an 
End token.  
Context Based Inference: This step aligns tokens 
from different item names under a single type. This 
step takes advantage of tokens repeating across 
descriptions and operates based on the first 
assumption, Context, that tokens within similar 
contexts have similar attribute types. 
If a token sequences tx,t, ty and t'x, t', t'y  exist in 
D such that tx, t'x Tp and ty, t'y Tq, then combine 
and replace the types of tokens t and t' with a new 
type Tn = Typeof(t) U Typeof(t') . 
Type Ordering: In this step, the set of inferred 
types T are sorted based on their ordering in the 
original item names. We utilize the Pairwise Offset 
Difference (POD) metric to compare different types.  
POD between types Ti and Tj is defined as: 
where fx is the token offset of x from the start of 
its item name and fy is the token offset of y. If this 
value is greater than zero, then the type Ti comes 
after type Tj in the sorted order. 
Due to space constraints, tokens have been 
aligned such that those from the same type are offset 
at the same column. The type numbers the tokens 
belong to are indicated at the top. 
___________________________________ 
Algorithm 1: Item Name Partition 
Type Merging: A careful observation shows that 
some of the neighbouring types are fillers for each 
other. Meaning that, they are not instantiated 
together for any item name. Such types are 
candidates for merging and are called merge 
compatible. Merging at this point is logical because 
of our assumption that the types are densely 
populated. 
Merge 
Concatenation: 
Finally, 
merge-
concatenation is performed to eliminate sparsely 
populated types. Sparsely populated types are those 
with a majority of missing values. By our 
assumption, collections of item names should have 
dense attributes. This implies that the tokens of a 
sparsely populated type should be concatenated with 
the values of one of the neighbouring types.  
4.3 Experimental Results 
To evaluate the algorithm, our DataRover system 
was used to crawl and extract list-of-products from 
the following five Web sites: www.officemax.com, 
(1)
Boosting Item Findability: Bridging the Semantic Gap Between Search Phrases and Item Information 
217

www.officedepot.com, 
www.acehardware.com,
www.homeclick.com and www.overstock.com.  
 Three metrics were used to measure the 
effectiveness of the algorithm. The first two evaluate 
the ability to identify fragments of the descriptions 
to the correct type and the last one indicates the 
correctness of the number of attributes. 
     Precision indicates how correctly type-value 
pairs are identified.  
     Recall This quantity indicates if every 
existing type-value pair is being identified.
     Attributes Error Rate indicates the error in 
the number of attributes described in the set of 
product names.
Table 1: Summary of Evaluation Measures for Different 
Web Sites for the Items Name Structuring Algorithm 
5 MINING THE DEFINITION OF A 
TARGET PHRASE 
In this section, we introduce the problem of mining 
definitions of a phrase from product data extracted 
from the matching Web pages. Using extraction 
techniques discussed in Section 4 we can retrieve 
tabular parametric attributes of matching products as 
well as their long descriptions. Next, we apply 
frequent itemset mining algorithms to learn the 
parametric definitions and phrase-based definitions 
of target phrases from the extracted product data.  
First, in Sections 5.1 thru 5.4 we introduce an 
algorithm that finds all frequent itemsets from a 
database. Section 5.5 discusses the problem of 
mining parametric definitions. In Section 5.6 textual 
definition 
mining 
is 
discussed. 
Since 
their 
introduction in 1994 by Agrawal et al. (Agrawal, 
1994), the frequent itemset and association rule 
mining problems have received a lot of attention 
among data mining research community. Over the 
last decade, many research papers (Han, 2001) have 
been published presenting new algorithms as well as 
improvements on existing algorithms to tackle the 
efficiency of frequent itemset mining problems. The 
frequent itemset mining problem is to discover a set 
of items shared among a large number of transaction 
instances in the database. For example, consider the 
product information database matching ‘trendy 
shoes’ that we extract from retail Web sites. Here, 
each instance represents the collection of product’s 
<attribute, value>pairs for attributes such as brand, 
price, style, gender, color and description. The 
discovered patterns would be the set of <attribute, 
value> pairs that most frequently co-occur in the 
database. These patterns define the parametric 
description of the target phrase ‘trendy shoe’. 
5.1 Boolean Representation of the 
Database 
The advantage of Boolean representation is that 
many logical operations such as superset, subset, set 
subtraction, OR, XOR, etc between any number of 
attribute vectors can be performed extremely fast. 
5.2 Constructing 2-frequent Itemsets 
Graph
The set of 2-frequent itemsets plays crucial role in 
finding all frequent itemsets. The main idea is that, 
from the observation that if {Ii….Ij} is a frequent 
itemset then all pairs of items in this set must also be 
a frequent itemset. Using this property of a frequent 
set, our algorithm will first create a graph that 
represents the 2-frequent itemsets among all items 
that satisfy the minimum support threshold.  
The the 2-frequent itemset graph is the directed 
graph G(V,E) which is constructed as follows: 
V = I; I is the set of items that satisfy the 
minimum support in database D.
E = {(vi,vj) | {i,j} is a 2-frequent itemset and i<j). 
We sort the frequent single items into 
lexicographical order and for a 2-frequent itemset, 
we construct a directed edge from the node (item) 
whose index is lower to the node whose index is 
higher. 
Hasan Davulcu et al. 
218

Example 1. 
                        
I1
I2
I3
I4
1 
1 
1 
0 
0 
1 
1 
1 
1 
0 
1 
0 
1 
1 
0 
0 
0 
0 
1 
1 
1 
1 
0 
0 
0 
1 
0 
1 
1 
0 
0 
1 
For this database, if minimum support G is set to 
25%, then the 2-frequent itemsets are I1I2, I2I3, I2I4,
I3I4. The 2-frequent itemsets graph would be as in 
Figure 1. 
5.3 Searching for Frequent Itemsets 
The algorithm iteratively starts from every node in 
the graph and recursively traverses depth-first to its 
descendants. At any step k (k>1), the algorithm will 
choose to go to a child node v of the current node so 
that the path from the beginning node to v forms a k-
frequent itemset. If so, the algorithm will continue 
expand to v’s children to search for (k+1)-frequent 
itemset and so on. There are several algorithms [8, 
16] that generate frequent itemsets in depth-first 
manner. A distinguishing feature of our algorithm is 
that it searches on the 2-frequent itemset graph. 
Finding all 2-frequent set takes O(n2) operations 
where n is the number of frequent single items. Our 
algorithm utilizes the following heuristics to guide 
the search. 
Heuristic 1: At step k, choose only children 
nodes of node vk-1 that have incoming degree greater 
than or equal to the number of visited nodes, 
counted from the beginning node. Incoming degree 
of a node v, denoted as deg(v) is the number of 
nodes that point to v. The meaning of this heuristic is 
that, if deg(v) is smaller than the number of visited 
nodes (nodes in the path) then there exists at least 
one node among the set of previously visited k-1 
nodes that does not point to v. In other words, there 
exists at least one node in the current path that does 
not form a 2-frequent itemset with v. Therefore the 
k-1 nodes in the path (visited nodes) and v cannot 
form a k-frequent itemset hence it is pruned out 
without candidate itemset generation. 
Heuristic 2: At step k, choose only children 
nodes of node vk-1 that have the set of incoming 
nodes that is a superset of the set of all k-1 nodes in 
the visited path. This heuristic, which is applied after 
Heuristic 1, ensures that all previously visited nodes 
in the current path, must point to the node in 
consideration. This is also a necessary precondition 
that each visited node forms a 2-frequent itemset 
with the node in consideration. 
Heuristic 1 is efficient since the 2-frequent 
itemset graph is already constructed and the degree 
of all nodes is stored before the search proceeds. 
Heuristic 2 superset testing operation can also be 
performed 
efficiently 
using 
the 
bit-vector 
representation. Consequently, by utilizing these 
heuristic estimates, we can prune a lot of nodes that 
cannot be added to the visited nodes to form a 
frequent itemset and eliminate a lot of candidate 
itemset generation. 
5.4 Mining Parametric Definition of 
Phrases
Note that, since we extract data from the Web by 
posing a search phrase query to a web search engine, 
all the instances in the data we get contain search 
phrase. Therefore, the association rule generation 
becomes simple by just putting the search phrase 
into the header of association rules and the body of 
rules is frequent itemsets. The support of obtained 
association rules equals to the support of frequent 
items set in their body since for a rule, the search 
phrase occurs in all instances that the frequent 
itemset (in the body of the rule) occurs. Next, we 
would like to utilize the extracted product 
information to mine parametric phrase definition 
rules made up from conjunctions of distinct 
<attribute, value> pairs, like: 
Trendy shoe ĸ
brand = Steve Madden,  
Color = black,  
material = leather  
 
  
5.5 Mining Textual Definitions of 
Target Phrases 
Another resource of rich phrase definitions is the 
long product descriptions of the matching products. 
In the Section 4, we have already described how we 
plan to collect long product descriptions from 
product Web pages that matches a given target 
search phrase. In this section we describe the 
proposed algorithm for mining phrase definitions 
Boosting Item Findability: Bridging the Semantic Gap Between Search Phrases and Item Information 
219
Figure 1: Database I and its 2-frequent item graph.

that can connect hidden phrases to product 
descriptions themselves. In order to generate 
candidate phrases first we perform part-of-speech 
(POS) tagging and noun and verb phrase chunking 
(Finch, 1997) on the long description to obtain a 
more structured textual description. Part-of-speech 
(POS) tagging and chunking the above description 
yields the following structure. In the next step, we 
utilize the noun phrases as transaction instances and 
mine frequently used phrases from all the noun 
phrases of all the product descriptions that we have 
collected from the Web documents.  
       ___________________________________ 
   Algorithm 2: Frequent Itemset Mining
  Next, we use the mined frequent phrases as 
items and create transaction instances by marking all 
of the frequently used phrases matching anywhere in 
the long description. This would yield transaction 
instances made-up from frequently used phrases 
matching the product descriptions. 
      Next we mine the frequent itemsets among 
instances corresponding to the long descriptions to 
find the phrase definitions. Note that, due to our way 
to construct the items, all items are combinations of 
single words; therefore, there are items that subsume 
other items. As a subsequence, there are a lot of 
redundant final resultant frequent itemsets. For 
example a long description might yield the following 
items: “suede”, “pump”, “suede pump”, “fashion”, 
“savvy”, 
“woman”, 
“fashion 
savvy”, 
“savvy 
woman”, “fashion savvy woman”. Hence, we only 
want to mine the frequent itemset “suede pump”, 
“fashion savvy woman” because these frequent 
itemsets subsume the former frequent itemsets.  
6 EXPERIMENTAL RESULTS 
The tables below show some of the definitions that 
were mined. It is a relatively easy task for a domain 
expert to inspect and evaluate the quality of such 
rule-based definitions.  
6.1 Comparison to Relevance 
Feedback Method 
In order to compare the performance of our 
definition miner to standard relevance feedback 
retrieval method we mined a large database of shoes 
(33,000 items) from a collection of online vendors. 
Next, we keyword queried the database with the 
target exemplary search phrase “trendy shoe”.From 
the 166 keyword matching shoes, we mined rule-
based phrase definitions for “trendy shoes” yielding 
rules such as fashionable sneaker, platform shoes 
etc. that were validated by a domain expert. These 
mined rules matched 3,653 additional shoes. 
Alternatively, we also computed the relevance 
feedback query vector using the above 166 matching 
shoes.  We also identified a similarity threshold by 
finding the maximal cosine theta, Ĭ, between the 
relevance feedback query vector and all of the 166 
shoe vectors. Retrieval using the relevance feedback 
vector with this threshold yields more than 29,000 
matches out of 33,000! The light colored bars in 
Figure 3 illustrates the histogram plot of the 29,293 
instances that falls into various similarity ranges. 
Similarly, the dark colored bars plots the similarity 
ranges of the 3,653 shoes that were retrieved by 
matching with our mined definitions. As can be seen 
from the distributions in the above chart, the items 
retrieved with our mined definitions have a very 
uniform similarity distribution (with around 300 of 
these being below the threshold), as opposed to 
having a skewed distribution towards the higher 
values of similarity. Since dark colored bars 
correspond to relevant “trendy shoes” matching our 
rules, which were validated by an expert, most of 
Hasan Davulcu et al. 
220

these items should have ranked towards the higher 
end of the similarity spectrum. However, relevance 
feedback measure failed to rank them as such; 
hence, it performed poorly for this task. 
6.2 Comparison to Relevance 
Feedback with LSI 
The plot of similarity ranges obtained by ranking the 
3,653 shoes, retrieved with our mined rules, using 
relevance feedback with and without latent semantic 
indexing (LSI) (Deerwester, 1990) technique is 
shown in Figure 2. The light colored dashed line 
represents the cosine theta threshold Ĭ for the 
relevance feedback ranking, similarly the dark 
colored dashed line represents the cosine theta 
threshold for the relevance feedback with LSI. The 
recall for relevance feedback is nearly 93%, 
however, since it matches 88% of a random 
collection of shoes, its precision is lower. On the 
other hand, even though the ranking of relevance 
feedback with LSI falls onto a higher similarity 
range, it appears to have a much lower recall (of 
25%) for this experiment with exemplary target 
phrase “trendy shoes”. 
7 CONCLUSIONS AND FUTURE 
WORK
Our initial experimental results for mining phrase 
definitions are promising according to our retail 
domain expert who is the Webmaster of an affiliate 
marketing web site. We plan to scale up our 
experiments to hundreds of product categories and 
thousands of phrases. Also, we would like to 
perform experiments to determine how precisely our 
algorithm learns the definitions of phrases that 
changes their meaning over time. 
 
Parametric Rules  
Support 
Brand = Jil Sander, material = leather, type = clutch Î fashion handbags 
4.25% 
Brand = Carla, design = mancini, material = leather Î fashion handbags 
2.4% 
Brand = Butterfly, design =beaded Î fashion handbags 
2.4% 
Brand = Sven, material = leather Î fashion handbags 
10.2% 
Design = beaded, color = pink Î fashion handbags 
2% 
Fashion
handbags
Design = beaded, color = blue, type = tote Î fashion handbags 
3.2% 
Design = Baffled box, material = cotton    Î luxury beddings 
5% 
Design = Waterford, material = linen    Î luxury beddings 
6% 
Material = silk    Î luxury beddings 
3% 
Luxury 
beddings
Design = Sussex, material = polyester    Î luxury beddings 
6% 
Design = All American, material = polyester Î sport beddings 
6% 
Design = All star, material = polyester Î sport beddings 
9% 
Design = Big and bold Î sport beddings 
17% 
Sport
beddings
Design = sports fan Î sport beddings 
45% 
 
Textual Rules  
Support 
Egyptian cotton mate-lass Î luxury beddings 
0.6% 
Silk, smooth, King set Î luxury beddings 
0.75% 
Piece ensemble Î luxury beddings 
0.75% 
American sport ensemble Î  sport beddings 
0.4% 
Paraphernalia sport Î  sport beddings 
0.6% 
fashionable sneaker Î trendy shoes 
7% 
Wedge edge Î trendy shoes 
5% 
Platform shoes Î trendy shoes 
6% 
Boosting Item Findability: Bridging the Semantic Gap Between Search Phrases and Item Information 
221

0
1000
2000
3000
4000
5000
6000
0
0.15
0.225
0.275
0.35
0.425
0.5
0.575
Sim ilarity Measurem ent
Num ber of Instances
Vector Space Relevance
Feedback
Definition Query
REFERENCES
R. Agrawal and R. Srikant. 1994, “Fast Algorithms for 
mining association rules”. In Proc. 20th Int. Conf. 
VLDB   pp. 487-499 
H. Aholen, O. Heinonen, M. Klemettinen, and A. I. 
Verkamo. 1998, “Applying Data Mining Techniques 
for
Descriptive Phrase Extraction in Digital Collections”. In 
Proceedings of ADL’98, Santa Barabara, USA 
W. Andrews. 2003 “Gartner Report: Visionaries Invade 
the 2003 Search Engine Magic Quadrant”, 
V. Crescenzi, G. Mecca, and P. Merialdo. 2001 
“Roadrunner: Towards automatic data extraction from 
large web sites”, In Proc. of the 2001 Intl. Conf. on 
Very Large Data Bases.
H. Davulcu, S. Vadrevu, S. Nagarajan, I.V. Ramakrishnan. 
2003, “OntoMiner: Bootstrapping and Populating 
Ontologies From Domain Specific Web Sites”, in 
IEEE Intelligent Systems, Volume 18, Number 5. 
Deerwester, S., Dumais, S. T., Landauer, T. K., Furnas, G. 
W. and Harshman, R. A. 1990, “Indexing. Latent 
semantic analysis”, journal of the Society for 
Information Science,  41(6),  pp. 391-407. 
Steve Finch and Andrei Mikheev. 1997, “A Workbench 
for Finding Structure in Texts”. Applied Natural 
Language Processing , Washington D.C. 
J. Han J.Pei, Y.Yin, and R. Mao. 2000, “Mining frequent 
pattern without candidate generation.” In Proceedings
of the ACM SIGMOD International Conference on 
Management of Data, volume 29(2) of SIGMOD 
Record, ACM Press. 
J. Han, and M. Kamber. 2001, “Data Mining: Concepts 
and Techniques”, Morgan Kaufmann Publishers. 
 Hung V. Nguyen, P. Velamuru, D. Kolippakkam, H. 
Davulcu, H. Liu, and M. Ates. 2003, “Mining "Hidden 
Phrase" Definitions from the Web”. APWeb, Xi'an, 
China, Springer-Velag, LNCS Vol 2642, pp. 156-165. 
M.F.Porter. 1980, “An algorithm for suffix stripping”, 
Program, 14 no. 3, pp. 130-137.  
G. Salton and C. Buckley. 1990, “Improving retrieval 
performance by relevance feedback”, journal of the 
American Society for Information Science, pp. 288—
297.
Ellen M. Voorhees. 1998, “Using WordNet for Text 
Retrieval”. In WordNet: An Electronic Lexical 
Database, Edited by Christiane Fellbaum, MIT Press. 
R. A. Baeza-Yates and Berthier A. Ribeiro-Neto. 1999, 
“Modern Information Retrieval”, ACM Press / 
Addison-Wesley. 
M.J. Zaki. 2000, “Scalable algorithms for association   
mining”. IEEE Transactions on Knowledge and Data 
Engineering, 12(3), pp. 372-390. 
0
200
400
600
800
1000
1200
1400
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Similarity
Number of document item
RF
LSI/RF
Figure 2: Similarity histogram for relevance feedback and 
Figure 3: Similarity histogram for rule-based and 
Hasan Davulcu et al. 
222
relevance feedback with LSI. 
relevance feedback based matches. 

INTEGRATING AGENT TECHNOLOGIES INTO ENTERPRISE
SYSTEMS USING WEB SERVICES
Eduardo H. Ram´ırez
Tecnol´ogico de Monterrey, Campus Monterrey
Eugenio Garza Sada 2501, Col. Tecnol´ogico, Monterrey, N.L. M´exico
eduardo.ramirez@itesm.mx
Ram´on F. Brena
Tecnol´ogico de Monterrey, Campus Monterrey
Eugenio Garza Sada 2501, Col. Tecnol´ogico, Monterrey, N.L.M´exico
rbrena@itesm.mx
Keywords:
Software agents, Web Services.
Abstract:
In this work we present a decoupled architectural approach that allows Software Agents to interoperate with
enterprise systems using Web Services. The solution leverages existing technologies and standards in order to
reduce the time-to-market and increase the adoption of agent-based applications. Insights on applications that
may be enhanced by the model are presented.
1
INTRODUCTION
Software Agents (Jennings and Wooldridge, 1996)
and Web Services (W3C, 2003) have become key re-
search areas for a growing number of organizations
and they are expected to bring a new generation
of complex distributed
systems (Jennings,
2000).
But even if Agent technology is little by lit-
tle ﬁnding its way into the mainstream, Web Ser-
vices have been adopted much more widely and
rapidly (Barry, 2003).
Several authors have pointed out some overlapping
between Agents and Web Services semantic capa-
bilities (Hunhs, 2002) (Preece and Decker, 2002), is-
sues regarding how they may be competing or com-
plementary technologies remain open (Petrie, 1996).
Because of that, research involving Agents and Web
Services is mainly focused on building improved se-
mantics (Dickinson and Wooldridge,2003) (Hendler,
2001) communication languages and interaction pro-
tocols (Labrou et al., 1999).
We assume that in order to impact real-world orga-
nizations, a greater emphasis should be made on inter-
operability between agent-based applications and en-
terprise information systems. Moreover, we believe
that the adoption of agent technologies will grow by
leveraging existing industry standards and technolo-
gies. Therefore the problem we address is an instance
of “the legacy software integration problem” (Nwana
andNdumu, 1999), (Genesereth and Ketchpel, 1994).
In this work we present a decoupled architec-
tural approach and design principles, called “Embed-
ded Web Services Architecture” (ESWA), that allows
agent-based applications to be integrated into enter-
prise application environments (Peng et al., 1998) us-
ing Web Services, thus allowing them to interoperate
with robust conventional systems such as:
• Web-applications and Portals.
• Enterprise Resource Planning (ERP)
• Manufacturing Execution Systems (MES)
• Workﬂow engines
we have found to be suitable for this approach
and the nature of Web Services that agents canprovide.
2
SOLUTION OVERVIEW
2.1
Architecture
223
C.-S. Chen et al. (Eds.), Enterprise Information Systems VII, 223–227.
© 2006 Springer.
	




	
	
Figure 1: Decoupled architecture top-level view.
software
“
“
Also, we discuss the kind of agent-based applicat-
ions

	

Agent container



	
	






	


Key :
	
	


	


	

(a) Component view

	
	


	

	

	
Key :





	 
Figure 2: EWSA decoupled architecture
An agent based application which exposes Web in-
terfaces requires the interoperability of Web compo-
nents and agents and their respective containers, as
they are built on different programming models each
following different sets of speciﬁcations. The relevant
components and containers are:
Web container Also called the “Servlet container,”
is the application that provides the execution envi-
ronment for the web components and implements
the Java Servlet API in conformity with the JSR-
154(Sun Microsystems, Inc., 2003) speciﬁcation.
Web containers are usually built within web servers
and provide network services related with HTTP
request processing.
Web component Servlets are the Java standard user-
deﬁned web components. JSR-154 deﬁnes them as
“A Java technology-based web component, man-
aged by a container that generates dynamic con-
tent”(Sun Microsystems, Inc., 2003). They follow
a synchronous processing model as they are de-
signed to handle the content of the HTTP requests.
The dynamic content delivered in the request may
be HTML for web pages or XML(W3C, 2000) for
Web Services.
Agent container The execution environment for the
agents provided by the agent platform in confor-
mity with FIPA(FIPA, 2002) speciﬁcations.
Web Service agent A Java thread that periodically
executes a set of behaviours containing the agent
tasks. For the purposes of this work we could say
that an agent is a “Web Service agent” if it receives
and processes requests formulated by a human user
or an application in collaboration with a Web com-
ponent. The requests may be synchronous or asyn-
chronous.
The development of our solution implies an in-
crease of internal cohesion inside agent-based appli-
cation components. A deeper analysis on which com-
ponents exist inside the application “black-box” is
shown in ﬁgure 2(a). The cohesion gain is achieved
by the means of an embedded Web container, that al-
lows the agent-based application to process HTTP pe-
titions in a reliable way. As the agent and web con-
tainers are both started on the same Java Virtual Ma-
chine operating system process, agents and web com-
ponents may communicate efﬁciently by sharing ob-
ject references in a “virtual channel”. The resulting
execution model is shown in ﬁgure 2(b).
2.2
Implementation
Among FIPA platforms, JADE(Bellifemine et al.,
1999) was selected because it is considered well
suited for large scale deployments mainly due to its
thread-per-agent programming model and the support
of “virtual channels” that allow agents to interact with
regular Java components(Rimassa, 2003).
In this particular implementation the “Launcher”
program, initializes and starts an instance of the JADE
platform besides an embedded version of the Tom-
cat Web Server (Jakarta Project - The Apache Soft-
ware Foundation, 2003). The mentioned “Registry”
is nothing but a data structure that holds references to
the running Service Agents, implemented as a Single-
ton (Gamma et al., 1995).
Eduardo H. Ramírez and Ramón F. Brena 
224
(b) Execution model.
used to determine the design strategy was the aim
to create a “black-box” in which agents can live and
perform complex tasks. The main architectural prin-
ciple consists of decoupling agent-based applications
through the exposure of Web Service interfaces. En-
terprise applications should not be aware that a ser-
vice is provided by agents if the system offers a stan-
dard SOAP endpoint as interface. Appearing to the
world as a conventional Web service or application.
As shown in ﬁgure 1, the underliying metaphor

Access to the agents source code is required as they
need to be recompiled to include the Web Service
capability which is encapsulated in a platform spe-
ciﬁc library. In JADE’s particular case, agents are en-
hanced with a custom behaviour class, that only re-
quires the addition of one line of code.
The architecture is applicable to FIPA platforms
other than Jade; however, it would be necessary
to port the framework components (Registry and
Launcher) using its particular libraries and program
interfaces and to add missing components provided
by the platform like virtual channels between objects
and agents.
2.3
Evaluation and comparison
Our proposal is not the ﬁrst solution that allow agents
to interoperate with web based components. In fact
such an architecture was deﬁned by developers(Berre
and Fourdrinoy, 2002) of the Jade platform and later
implemented on the WSAI Project (Whitestein Tech-
nologies, A.G., 2003) as a contribution to AgentCities
initiative (Dale et al., ).
The solution assumes the existence of two agent
containers, one
that we may call the
“main container” and one contained itself within the
web container. Each container is executed in a sepa-
rate JVM system process. WSAI introduces the con-
cept of “Gateway Agent” as an agent living in the
“web container”, responsible of translating HTTP re-
quests into ACL messages. The general gateway ar-
chitecture components are shown in ﬁgure 3(a).
One of the major drawbacks of the approach re-
sides in the existence of several processes that should
synchronize using remote method invocations even
if both of them are deployed on the same machine.
Additional complexity comes from the fact that it is
able to interoperate with any running FIPA-compliant
agent platform (even non Java-based ones) without
access to its source code.
We believe that in order to build enterprise-class
agent-based applications is not critical to provide
web-interoperability to an indeﬁnite number of FIPA
platforms, therefore, we trade-off this ﬂexibility in fa-
vor of good integration with the chosen agent plat-
form, even though the architectural principles remain
useful for them. As a result, our framework imple-
mentation is simple and provides good performance.
In a benchmark between WSAI and the EWSA de-
coupled architecture an important performance and
scalability gain was observed. A currency exchange
Web service is provided in the WSAI platform. The
service implementation is trivial as it only consist of
a simple mathematical conversion performed by an
agent. As shown in ﬁgure 4 we may notice that not
only EWSA’s response times are better, but that they
increase at a slower rate with respect to the number of
concurrent requests which leads to better scalability.
The performance gain in the embedded architecture
can be interpreted as an effect of the elimination of
network calls overhead between agents and web com-
ponents.
225
Figure 3: Gateway architecture.
Figure 4: Mean service time for concurrent requests.
Integrating Agent Technologies into Enterprise Systems Using Web Services
stand-alone,
JVM
JVM
Key :
(a) Component view
Key :
(b) Execution model

3
APPLICATIONS
JITIK
	




	




	
	
	


	


	



		
	
	
	



	
	

In a general we believe that the proposed integration
model is useful to allow gent-based applications to
provide knowledge-intensive services, such as:
• Search and automatic classiﬁcation
• User proﬁle inference
• Semantic-based content distribution
Web-enabled agent systems may serve in a variety
of domains. As presented in the JITIK case study,
they are well suited to support knowledge distribution
in collaborative environments.
3.1
Just-in-time Information and
Knowledge
The proposed model have been successfully imple-
mented in JITIK (for Just-in-Time information and
Knowledge) that may be deﬁned as a web-enabled
agent-based intelligent system capable of deliver
highly customized notiﬁcations to users in large dis-
tributed organizations(Brena et al., 2001). JITIK is
aimed to support collaboration within organizations
by delivering the right knowledge and information to
the adequate people just-in-time. JITIK was designed
to interoperate with enterprise systems in order to re-
trieve and distribute contents in a ﬂexible way.
The JITIK agent model is shown in ﬁgure 5. Per-
sonal Agents work in behalf of the members of the
organization. They ﬁlter and deliver useful content
according to user preferences.
Personal agents are
provided of information by the Site Agent who acts
as a broker between them and Service agents. For
the purposes of this work, the most relevant agents
of JITIK are the so called service agents which col-
lect and detect information and knowledge pieces that
are supposed to be relevant for someone in the or-
ganization. Examples of service agents are the Web
Service agents, which receives an process external re-
quests, as well as monitor agents which are continu-
ously monitoring sources of information and knowl-
edge (web pages, databases, etc.).
The ontology agent contains the knowledge about
the interest areas to the members of the organization
and about its structure(Brena and Ceballos, 2004).
That knowledge is hierarchically described in the
form of taxonomies, usually one for interest areas and
one describing the structure of the organization. For
example, in an academic institution, the interest areas
could be the science domains in which the institution
is specialized, and the organizational chart of the in-
stitution gives the structure of the organization.
3.2
JITIK Web Services
JITIK is an example of an agent-based application
able to provide knowledge intensive services which
may be grouped as follows:
Recommendation services A user’s proﬁle is repre-
sented by a set of points in the taxonomies, as each
user could have many interests and could be located
at different parts of the organizational structure. As
JITIK keeps track of user interests and preferences
it is able to recommend content to users on demand.
Recommended content may be used in Portals or
Web applications.
Content search and classiﬁcation One of the main
difﬁculties for web users is obtaining relevant in-
formation.
In normal conditions people waste a
lot of time searching documents on the web, most
of the times, because the users must examine the
documents in detail to determine if they are really
relevant for the search purposes. In the context of
JITIK, a service agent that searches the most rel-
evant documents on the web can be constructed.
The knowledge that guides the search is handled by
the ontology agent where the keywords with which
the search engine is invoked are deﬁned. The doc-
uments obtained by the search are qualiﬁed by a
fuzzy system and then the best ones are pushed to
the users.
Subscription services JITIK allows users to sub-
scribe to changes in speciﬁc areas.
Also, users
may customize the media and frequency of JITIK
notiﬁcations using using simple web-based inter-
faces. Rules may be deﬁned so as messages rela-
tive to certain topics are handled with higher pri-
orities. A rule may state that several alerts may
be sent to their cell-phone via SMS, and also de-
ﬁne that interest-area messages be sent in a weekly
summary via email. Organization managers may
set high-level distribution rules.
Content distribution services Enterprise
applica-
tions may deliver content to the system using
Eduardo H. Ramírez and Ramón F. Brena 
226
Figure 5: JITIK and enterprise systems interaction.

its semantic-based content distribution services.
When new content is received it is classiﬁed and
distributed to users who may be interested. Users
receive the notiﬁcations of new content as speciﬁed
by their own rules.
As shown above, the EWSA decoupled architec-
ture allows an agent-based application like JITIK to
provide enterprise communities with a number of
knowledge oriented Web Services, specially useful in
large organizations where performance and scalabil-
ity attributes become critical.
4
CONCLUSION
We have presented an architectural approach aimed
to allow integration of multi-agent systems as Web-
Services components. Besides its simplicity, the ad-
vantage of this approach is that it provides an efﬁcient
way of interoperating agent-based subsystems with
web-centric loosely-coupled systems. We think this
solution is a good compromise given the current sta-
tus of technology, and that it allows rapid integration
of modular systems conforming to open standards.
We presented experimental evidence to support our
claim of efﬁciency. We presented as well a case study,
which is the application of our architecture to the JI-
TIK system, a multi-agent system to deliver informa-
tion items to a distributed community of users.
In the near future we intend to test our architec-
ture in other real-world systems integrating agents
in a web-based framework. We are currently study-
ing methodological issues to guide the development
of hybrid agents-web systems, as current agent-
development methodologies need to be strongly en-
hanced in order to ﬁt our architecture.
REFERENCES
Barry, D. K. (2003). Web Services and Service-Oriented
Architectures: The Savvy Manager’s Guide. Morgan
Kaufmann.
Bellifemine, F., Poggi, A., and Rimassa, G. (1999). JADE
- A FIPA-compliant agent framework. In Proceedings
of PAAM’99, London.
Berre, D. L. and Fourdrinoy, O. (Jun 2002). Using JADE
with Java Server Pages. In JADE documentation.
Brena, R., Aguirre, J., and Trevi˜no, A. (2001).
Just-in-
Time Information and Knowledge: Agent technology
for KM Bussiness Process. In Proceedings of the 2001
IEEE Systems, Man, and Cybernetics Conference.
Brena, R. and Ceballos, H. (2004). A Hybrid Local-Global
Approach for Handling Ontologies in a Multiagent
System.
In Proceedings of the 2004 2nd Interna-
tional IEEE Conference Intelligent Systems. Varna,
Bulgaria.
Dale, J., Willmott, S., and Burg, B. Agentcities: Building a
global next-generation service environment.
Dickinson, I. and Wooldridge, M. (2003). Towards prac-
tical reasoning agents for the semantic web. In Pro-
ceedings of the second international joint conference
on Autonomous agents and multiagent systems, pages
827–834. ACM Press.
FIPA (2002). FIPA Abstract Architecture Speciﬁcation.
Gamma, E., Helm, R., Johnson, R., and Vlissides, J.
(1995).
Design Patterns:
Elements of Reusable
Object-Oriented Software. Addison-Wesley.
Genesereth, M. R. and Ketchpel, S. P. (1994).
Software
agents. Commun. ACM, 37(7):48–ff.
Hendler, J. (2001). Agents and the Semantic Web. IEEE
Intelligent Systems, 16(2).
Hunhs, M. (Jul-Ago 2002). Agents as Web Services. IEEE
Internet Computing, 6(4):93 –95.
Jakarta Project - The Apache Software Foundation (2003).
The Tomcat Web Server v. 4.1.
Jennings, N. and Wooldridge, M. (18 Jan. 1996). Software
agents. IEE Review, 42(1):17 –20.
Jennings, N. R. (2000). On agent-based software engineer-
ing. Artiﬁcial Intelligence, 177(2):277–296.
Labrou, Y., Finin, T., and Peng, Y. (March-April 1999).
Agent communication languages: the current land-
scape. IEEE Intelligent Systems, 14(2):45 –52.
Nwana, H. S. and Ndumu, D. T. (1999). A Perspective on
Software Agents Research. The Knowledge Engineer-
ing Review, 14(2):1–18.
Peng, Y., Finin, T., Labrou, Y., Chu, B., Long, J., Tolone,
W. J., and Boughannam, A. (1998).
A multi-agent
system for enterprise integration. In Proceedings of
the 3rd International Conference on the Practical Ap-
plications of Agents and Multi-Agent Systems (PAAM-
98), pages 155–169, London, UK.
Petrie, C. J. (Dic 1996). Agent-Based Engineering, the web
and intelligence. IEEE Expert, 11(6):24–29.
Preece, A. and Decker, S. (Ene-Feb 2002). Intelligent web
services. IEEE Intelligent Systems, 17(1).
Rimassa, G. (Jan 2003). Runtime Support for Distributed
Multi-Agent Systems. In Ph. D. Thesis, University of
Parma.
Sun Microsystems, Inc. (2003).
JSR-000154 Java(TM)
Servlet 2.4 Speciﬁcation (Final release).
W3C (2000).
Extensible Markup Language (XML) 1.0
(Second Edition).
W3C (Aug 2003). Web Services Glossary, Working Draft.
Whitestein Technologies, A.G. (2003). Web Services Agent
Integration Project.
227
Integrating Agent Technologies into Enterprise Systems Using Web Services

PART 5 
Human-Computer
Interaction

OPENDPI: A TOOLKIT FOR DEVELOPING
DOCUMENT-CENTERED ENVIRONMENTS
Olivier Beaudoux
ESEO, Computer Science Department & Laboratoire de Recherche en Informatique / INRIA Futurs*
4 rue Merlet de la Boulaye, 49009 Angers, France
olivier.beaudoux@eseo.fr
Michel Beaudouin-Lafon
Laboratoire de Recherche en Informatique / INRIA Futurs
Universit´e Paris-Sud, LRI - Bt 490, 91405 Orsay - France
mbl@lri.fr
Keywords:
Document centered systems, interaction model, active components, GUI toolkits.
Abstract:
Documents are ubiquitous in modern desktop environments, yet these environments are based on the notion of
application rather than document. As a result, editing a document often requires juggling with several applica-
tions to edit its different parts. This paper presents OpenDPI, an experimental user-interface toolkit designed
to create document-centered environments, therefore getting rid of the concept of application. OpenDPI relies
on the DPI (Document, Presentation, Instrument) model: documents are visualized through one or more pre-
sentations, and manipulated with interaction instruments. The implementation is based on a component model
that cleanly separates documents from their presentations and from the instruments that edit them. OpenDPI
supports advanced visualization and interaction techniques such as magic lenses and bimanual interaction.
Document sharing is also supported with single display groupware as well as remote shared editing. The pa-
per describes the component model and illustrates the use of the toolkit through concrete examples, including
multiple views and concurrent interaction.
1
INTRODUCTION
1.1
Documents versus Applications
The fact that the document is the main object of in-
terest within interactive workspaces had been identi-
ﬁed 25 years ago : “The document is the heart of the
world, and uniﬁes it” (Johnson et al., 1989). However,
today’s environments are mainly based on the appli-
cation concept where every application is dedicated
to a speciﬁc kind of data. Users are thus forced to
juggle between applications in order to edit a single
document. Software publishers react to this fact by
proposing three different and complementary strate-
gies:
1. Building small applications within larger ones –
For example, both Word and PowerPoint (which
are provided within a single software package) in-
clude two different small-applications dedicated to
vectorial drawings.
2. Open architecture based on plug-ins – For exam-
ple, many plug-ins are available for the PhotoShop
application, thus extending its initial functionalities
by following users’ needs.
3. Software suite with common interfaces – For exam-
ple, Photoshop, Illustrator, GoLive and InDesign
applications have a common look and feel to their
graphic interfaces and some common tools, thus re-
sulting in a more natural interaction since swapping
from one application to another is less visible.
These three approaches aim at positioning the docu-
ment in the heart of the interaction rather than the ap-
plication. However, they don’t reach their goals since
users still have to juggle between more than one appli-
cation and document in order to achieve a single task.
They try to make applications less visible by reducing
their gap, but they remain centered on the application
concept.
1.2
Interactions and Documents
The document concept often suggests the data that
documents can contain. However, it could be relevant
to specify the semantic of actions they can handle in
addition to the semantic of their data, such as Web
services which aim at specifying “interfaces” of of-
fered services (W3C, 2001). We have built our model
in this direction : it deﬁnes how documents can
be perceived by users and, above all, how users can
231
C.-S. Chen et al. (Eds.), Enterprise Information Systems VII, 231–239.
© 2006 Springer.

Figure
1:
Two
presentations
of
a
shape
bines a document model and an interaction model
inaunifyingway. In sodoing,weoffer agenericmecha-
nism which makes data independent from actions that
can be done on this data, and actions independent
from interactions that can induce these actions.
1.3
Structure of this Paper
The second section describes the core component
model of DPI. It explains how this component model
is used to deﬁne the three main components D, P, and
I, and how such components communicate with each
other. In the third section, we focus on the beneﬁts
of using a single and generic model for all interac-
tive components by illustrating the resulting simplic-
ity of their implementation. In the fourth section, we
present the replication point concept as a general con-
cept that we use to provide basic groupware capabili-
ties and a full alternate rendering engine. We compare
the DPI model with other complementary approaches
and works in the ﬁfth section. Finally, the implemen-
tation of the model in the OpenDPI toolkit and the
perspective of our work is discussed in the conclu-
sion.
2
THE CORE COMPONENT
MODEL
In the DPI (Document, Presentation, Instrument) con-
ceptual model, documents are visualized through one
or more presentations, and manipulated with inter-
action instruments (Beaudoux and Beaudouin-Lafon,
2001). The conceptual model deﬁnes the structures
of these three components and the way they commu-
nicate. In this paper, we focus on the generic compo-
nent model that was used to build the OpenDPI toolkit
that implements the DPI model. By instantiating this
component model, we allow the implementation of
the three D, P and I components.
In order to explain the core component model and
its instantiation for the D, P and I components, we
use a simple example that tackles the main aspects of
our model (ﬁgure 1)1. It consists of two presentations
of a single shape (such as a rectangle): the ﬁrst pre-
sentation displays the shape as a graphical object, and
the second presentation displays its properties within
textual ﬁelds.
2.1
Observable State
The state of a component is deﬁned by both its proper
state and its structural state. The proper state char-
acterizes the property values of the component. For
example, the shape deﬁnes the x, y, width and height
properties. Moreover, components can be themselves
composed in a hierarchical way, and they thus deﬁne
a structural state.
Depending on the context, the changes of compo-
nent states can interest other components. DPI com-
ponents are thus considered as observable instances in
the sense of the “observable / observer” design pattern
(Gamma et al., 1994). Each component class deﬁnes
a property observer interface as the means of observ-
ing its proper state. For example, the shape of our
example deﬁnes the following interface:
interface ShapeObserver
extends PropertyObserver {2
void newWidth(double width);
void newHeight(double height);
void newX(double x);
void newY(double y);
// etc.
}
In a complementary way, the structure observer inter-
face, common to all component classes, is the means
of observing the structural state of components. In
order to simplify representation of DPI components,
we extend the UML notation in a way that clearly dis-
plays observable and observer interfaces, and their re-
lation (see ﬁgures 2 and 3): a black point depicts the
observable interface of components, and a white point
(respectively a white point combined with the aggre-
gation symbol) depicts a property observer interface
(respectively the structure observer interface) imple-
mented by the observers.
2.2
Applying the Model to
Documents
Documents are structured as trees where each node
is a DPI component. The domain data part of a doc-
1All
given
examples
have
been
implemented
in
classes
mentioned
in
corresponding
ﬁgures.
They
can
be
tested
by
downloading
OpenDPI
at
http://www.eseo.fr/˜obeaudoux/opendpi
2In all this paper, we omit Java keywords public and pro-
tected for compactness.
Olivier Beaudoux and Michel Beaudouin-Lafon
232
(opendpi.scenarios.Presentations).
ter-) act on documents. We provide a model that
com
(in

Shape
GeometryData
ShapeForm
Text
4
1
has domain
data
1
has domain
data
Figure 2: Components involved in the two presentations of
ument contains components that are observed by as-
sociated presentations. In turn, embedded presenta-
tions also contain node components which are typi-
cally graphic in nature.
The synchronism between presentations and do-
main data within a single document is based on the
observation mechanism. Figure 2 shows how the rec-
tangle example is built:
• Domain data of the shape is deﬁned in the Geom-
etryData component class through the four proper-
ties xCenter, yCenter, width and height.
• The “Drawing” presentation contains one compo-
nent instance of the Shape class. The shape com-
ponent observes every change in the domain data
through the GeometryDataObserver interface in
order to update its own state consequently. More-
over, it observers its proper state that can vary
among user’s interactions through the above de-
ﬁned ShapeObserver interface in order to update
the domain data.
• The “Properties” presentation deﬁnes in the Shape-
Form component that allows the edition of the do-
main data through four Text components. The form
observes the domain data in order to refresh text
ﬁelds consequently, and the proper state of its text
ﬁelds in order to update the domain data.
2.3
Action Producer and Consumer
2.3.1
Specifying Actions
The observation concerns the changes of component
states and thus does not allow communication be-
tween components out of such changes. An action
allows the transmission of a state independent of any
component: it is handled by the system in order to
link actions, from the users to the documents.
The state of an action is deﬁned through a set of
properties. For example, the translate action simply
deﬁnes the dX and dY properties that describe a move-
ment on the x and y axis:
class Translate extends Action {
double dX;
double dY;
double getDX() {return dX;}
void setDX(double dX) { this.dX = dX; }
// same code for dY property...
}
An action class only deﬁnes such a state and does not
characterize any behavior related to the action. This
a consequence of the polymorphic nature of actions
(Beaudouin-Lafon and Mackay, 2000): a polymor-
phic action has an imprecise semantic deﬁned by the
action itself, and precise semantics deﬁned by objects
that can consume the action. As a consequence, an
action class does not deﬁne in what manner the action
is to be executed or cancelled: this manner is deﬁned
by consumers of actions.
2.3.2
Producer and Consumer Interfaces
The production of an action from a producer compo-
nent to a consumer component follows a cycle deﬁned
through the deﬁnition of both producer and consumer
interfaces.
The consumer interface of an action class A is im-
plemented by all classes of components that deﬁne
their ability to consume instances of A. It consists of
a set of four methods invoked by producers in the fol-
lowing order3:
1. The can-method carries out the feasibility test of
the action related to the current context.
2. Whenever the can-method has returned true, the be-
gin-method is invoked and starts the action.
3. The do-method is then invoked and represents the
main loop of the action consumption.
4. The end-method is ﬁnally invoked when the action
have to stop.
For example, the translate action deﬁnes the following
consumer interface:
interface TranslateConsumer{
extends Consumer {
boolean canTranslate(Producer p);
void beginTranslate(Producer prodpucer,);
void doTranslate(double dx, double dy);
void endTranslate(Producer p);
}
The producer interface of an action class A is imple-
mented by all classes of components that deﬁne their
ability to produce instances of A. It speciﬁes a min-
imal contract that producers must satisfy in order to
be able to produce the action. For example, the pick-
color action deﬁnes a producer interface so that the
consumer provides its picked-color to the producer:
interface PickColorProducer extends Producer {
void colorPicked(Color c);
}
3The consumer interface also deﬁned undo, redo and
echo methods that we do not describe in this paper.
A Toolkit for Developing Document-Centered Environments
233
the shape.

mal contract is empty. For example, the translate ac-
tion does not deﬁned a dedicated producer interface
since the translation does not need any speciﬁc con-
tract in order to be consumed.
In order to clarify the UML representation of DPI
der to display the producer and consumer interfaces
(see ﬁgure 3): a black square depicts the producer in-
terface that qualiﬁes the component which can pro-
duce the action, and a white square depicts the con-
sumer interface that qualiﬁes the component which
can consume the action.
2.3.3
Concurrency
When multiple actions are produced on a component
at the same time, the state of the component may be
modiﬁed concurrently.
In order to ensure data in-
tegrity, we have introduced the marking of compo-
nents.
The rule of marking is deﬁned as follows: an ac-
tion that modiﬁes a state of the consumer component
can be produced only if no mark has been set on its
state. As soon as consumers perform such a mark-
ing while they consume actions, the previous rule for-
bids the production of concurrent actions. This is car-
ried out by can-methods that return false whenever
concerned properties have been already marked. In
the following example, consuming a translation in-
duces the marking of both the x and y properties of
the shape thus forbidding any concurrent action (such
as another translation).
Moreover, the marking mechanism, shortly ex-
plained in this paper, is quite similar to ﬁne grain lock-
ing techniques such as in DistEdit application (Knis-
ter and Prakash, 1990). However, they have signif-
icant differences: locking aims at ensuring consis-
tency of data while marking aims at avoiding con-
current actions. As a consequence, a mark does not
need to explicitly and strictly forbid subsequent mod-
iﬁcation of the marked element (i.e. a set method can
be invoked on a marked property): it only forbits con-
current actions. In addition, a mark does not need to
be associated to a particular owner. These two points
make marking quite easy for programmers.
2.4
Applying the Model to
Instruments
Action chaining from users to documents is based
on the instrumental interaction (Beaudouin-Lafon,
2000). The physical part of instruments detects ges-
tural actions (or gestures) made by users on human in-
put devices, then the logical part transforms the ges-
Hand
Translate
Detector
translate
gesture
HidSensor
"X"
HidSensor
"button"
HidSensor
"Y"
Push
Detector
push
gesture
translate
push
rotate
scale
paint
Shape
Cursor
1
Physical part
Logical part
tures in intentional actions (or actions) that are pro-
duced by instruments and consumed by documents.
Figure 3 shows how the producer - consumer model
is used in combination with the observable - observer
model in order to build such a chaining. It illustrates
the production of the translate action by the “hand”
instrument and its consumption by the shape.
In order to be aware of the easiness of our approach,
we explain the ﬁgure by analyzing the pieces of code
for both the Hand and Shape components. The Hand
component is deﬁned as follows:
class Hand extends Tool implements
TranslateProducer,
TranslateGestureConsumer,
PushGestureConsumer
{
Translate translate;
Consumer shape;
void beginPushGesture(Producer p) {
shape = getPickedConsumer(translate);
if (shape != null)
translate.beginAction(shape);
}
void doTranslateGesture(
double dx, double dy) {
super.doTranslateGesture(dx, dy);
if (shape != null) {
translate.setDX(dx);
translate.setDY(dy);
translate.doAction();
}
}
void endPushGesture(Producer p) {
if (shape != null) {
translate.endAction();
shape = null;
}
}
}
The hand instrument is linked to the mouse and ob-
serves the proper states of the mouse x, y and but-
ton sensors during its construction (not shown in the
code). This observation is delegated to two gesture
Note that, however, most actions does not need to de-
components, we extend our component notation in or-
ﬁne an associated producer interface since their mini-
Olivier Beaudoux and Michel Beaudouin-Lafon
234
Figure 3: The “hand” instrument.

detector components: the translation detector pro-
duces a translate gesture whenever the x or y sen-
sor state changes, and the push gesture detector pro-
duces a push gesture whenever the button sensor
state changes. When the push gesture starts (begin-
PushGesture method), the hand does a picking that
consists of ﬁnding which component located under
the cursor can consume the translate action. In the ex-
ample, the picking returns the shape. When the hand
consumes the translate gesture (doTranslateGesture
method), it updates the location of its cursor and in-
vokes the execution of the action on the picked shape.
Finally, when the push gesture stops (endPushGesture
method), the hand terminates the action.
In turn, the Shape class involved in the consump-
tion phase is deﬁned as follows:
class Shape extends Component
implements TranslateConsumer
{
boolean canTranslate(Producer p) {
return !isMarked(”x”) && !isMarked(”y”);
}
void beginTranslate(Producer p) {
mark(”x”); mark(”y”);
}
void doTranslate(double dx, double dy) {
setLocation(getX() + dx, getY() + dy);
}
void endTranslate(Producer p) {
unmark(”x”); unmark(”y”);
}
}
When the shape consumes a translation, it ﬁrst mark
the modiﬁed properties: such a marking is checked
for by the producer before producing the translate ac-
tion. Then subsequent calls to the do-method are done
by the producer and the shape modiﬁes its x and y
properties accordingly. Finally, when the translate ac-
tion stops, the shape unmarks the previously marked
properties.
3
BENEFITS OF A GENERIC
MODEL
3.1
Direct Manipulation
The DPI component model allows the creation of
both direct and non-direct manipulation components.
This is a signiﬁcant difference between standard
GUI toolkits and OpenDPI: traditional GUI toolk-
its are based on the widget model which does not
allow the deﬁnition of direct manipulation com-
ponents such as paint-brush, or magnetic guide
(opendpi.scenarios.Magnetism).
Figure 4 displays two painting tools: a paint-brush
(a) and and color-toolglass (b, c) (Bier et al., 1993).
Shape
PaintBrush
pick-color
paint
pick-color
paint
(a) Paint-brush compo-
nent
(b) Color-toolglass
Hand
ColorToolglass
12
has buttons
click-through
paint
Shape
Palette
translate
has content
1
ColorGlassButton
pick-up
paint
pick-color
pick-color
(c) Color-toolglass component
Both these tools can produce the pick-color and paint
actions (on any shape component for example). Their
implementations follow strictly the same guidelines
as the ones previously explained for the translate ac-
tion example. The environment of the color-toolglass
is a little more complex but the code remains quite
simple:
1. The palette that contains the toolglass can be trans-
lated around the workspace. It can also be picked-
up by an instrument such as the hand instrument in
order to be used in a bimanual way (in the opposite
case, the toolglass behaves like a usual palette of
colors).
2. The toolglass contains 12 colored buttons that can
consume the click-through action. When the click-
through action is consumed, the button produces
in turn the paint action on the picked graphical
component located above the hand cursor (and thus
above the button).
As we can observe, each of the components involved
in this painting process are interactive components.
This allows the chaining of actions in many differ-
ent ways. For example, since the color glass-button
and the brush are graphics components, they can
be “painted”: by consuming the paint action, they
change their associated color. In the same way, their
color can be picked. Such combinations of painting
and color-picking can be done in many manners by
using any color toolglass or paint-brush. We there-
fore claim that using a common model for all inter-
active objects will help to discover such mixing and
enriching interaction capabilities.
A Toolkit for Developing Document-Centered Environments
235
Figure 4: Painting tools (opendpi.scenarios.Toolglass).

Figure 5:
3.2
Genericity of Actions
Tool instruments naturally inherit from the genericity
of actions: they can operate in many contexts. We
have illustrated this point through the translate action
example: the translation can be produced in a com-
mon way on many components and is thus generic.
Moreover, it may be interesting to override its default
behavior deﬁned in the Shape class.
For example,
when a UML component is translated, it may be use-
ful to translate a clone of the component rather than
the original component so that the time-consuming
computation of its linking is done when the interac-
tion ends. We have also experiment the genericity of
the paint action regarding image painting: when an
Image component consumes the paint action, it ap-
plies a ﬁltering effect so that the painting color is re-
inforced.
3.3
Adapting Actions
In some circumstances, a user may want to apply an
action to a component but does not have any instru-
ment that could produce such an action. Rather than
purchase a new instrument, the user may prefer to add
an adapter to an existing instrument. Figure 5 illus-
trates how a translation →rotation adapter works:
1. The adapter is added to an instrument that can pro-
duce the translate action, e.g. the hand instrument.
2. When the hand instrument produces the translate
action, the adapter transforms the consumed trans-
lation into a produced rotation. The transformation
is based on a simple mathematical operation that
sets the angle property value of the rotate action
proportionally to the dX property of the translate
action.
The following code well illustrates the easiness of its
implementation:
class TranslateToRotateAdapter
extends Component implements
TranslateConsumer, RotateProducer
{
Rotate rotate;
Consumer shape;
boolean canTranslate(Producer p) {
shape =
getPickedConsumer(rotate, getX(), getY());
if (shape != null) return shape.canRotate(p)
else return false;
}
void beginTranslate(Producer p) {
rotate.beginAction(shape);
}
void doTranslate(double dx, double dy) {
rotate.setAngle(dx);
rotate.doAction();
}
void endTranslate(Producer p) {
rotate.endAction();
}
}
After checking for the rotation feasibility, the adapter
just replicates the production cycle from the translate
action to the rotate action. The implementation of
adapters remains so simple that it may be automated
so that users can specify their own adapters (for ex-
ample by setting the mathematical operation such as
the one underlined in the previous code).
Moreover, adapters can be used in order to relax the
marking of components. Figure 5-a shows the Slid-
ingTranslateAdapter (labelled “wizzz”) that trans-
form any concurrent translate actions into sliding-
translate actions by ﬁltering the feasibility method on
the consumer.
3.4
Interoperability through Actions
The DPI component model allows the interoperabil-
ity of actions among applications. For example, the
pick-color action could be done on an application by
an instrument provided by another application. As a
result, a color can be picked from any DPI application
and used to paint an object in any other DPI applica-
tion.
Another well-known interaction that allows the in-
teroperability among applications is the drag’n drop.
However, such an interaction is not the easier one
that GUI toolkits implement. Figure 6 shows that,
from the DPI perspective, this interaction is not more
complex than others. It illustrates how a tabbed win-
dow may be dragged from a container window then
dropped into another container window:
Olivier Beaudoux and Michel Beaudouin-Lafon
236
Adapters (opendpi.scenarios.Adapters).
(a)
Two
adapters
(opendpi.scenarios.Adapters)
TranslateToRotate
Adapter
rotate
Graphics
Hand
translate
Shape
Text
(b) Translate-to-rotate adapter component

Figure
6:
Drag
&
drop
of
tabbed
windows
Figure
7:
A
local
replication
point
1. The instrument produces the drag-and-drop action
on the selected tabbed window. This action con-
sists in taking the window, translating it on the
workspace, and ﬁnally dropping it on a targeted
container window.
2. When the drag & drop ends, the tabbed window
produces in turn the dropTabbedWindow action to
the targeted container window. Note that, through-
out the dragging stage, the picking is used in con-
junction with the feasibility method in order to
check if the drop action can be produced.
4
REPLICATION POINT
A replication point is an abstract object that, when
attached to two or more (mostly graphical) contain-
ers C1..n, cross-replicates the initial contents of C1..n
and subsequently dispatches all user actions within a
container Ci to all the other containers Cj̸=i. Figure 7
displays a replication point associated to two contain-
ers, a clipping rectangle (on the left) and a clipping
circle (on the right), that share their content (a rectan-
gle, an ellipse ant a text). While the user translates the
ellipse within the right container, the replication point
replicates the translation to the left container. Since
the left container has enabled the echoing mode (see
next section), the translate action is played through an
echo that consists in tagging the ellipse with the name
of the action when the action begins, then playing a
translation animation when the actions ends.
4.1
Application to Groupware
The remote replication point is a replication point
which is identiﬁed by its unique IP group address. It
deﬁnes the way of synchronizing shared components
among sharing containers. This synchronization in-
duces a strong spatial coupling of sharing contain-
ers since they have exactly the same contents, and a
strong temporal coupling since these contents remain
identical at any time.
In order to extend this synchronization behavior,
we introduce the concept of a behavior point that
can be attached to a component contained in or equal
to a container attached to a replication point. Such
behavior points deﬁne extended behaviors by relax-
ing the temporal and/or spatial coupling, thus result-
ing in a ﬂexible coupling as deﬁned in (Dewan and
Choudhary, 1992).
For example, we have deﬁned
an asynchronous point that relaxes the temporal cou-
pling by allowing users to work asynchronously on
the component from which the asynchronous point is
attached. In the same way, we provide awareness be-
havior points that allow the insertion of speciﬁc infor-
mation in sharing containers though this information
is not necessarily the same for each sharing container.
Such behavior points thus relax the spatial coupling.
For example, the echoing point allows the perception
of a remote action produced inside the component
from which it is attached to through an echo of the ac-
tion (Beaudouin-Lafon and Karsenty, 1992) (see ﬁg-
ure 7), rather than through the original execution the
action. Such an echo often consists of an animation
that “summarizes” the result of the action, thus avoid-
ing the need to display all the disturbing details of
remote actions.
It is important to note that the implementation of re-
mote sharing point does not provide any concurrency
control yet. At this time, the remote replication point
works locally between two or more separated appli-
cation instances (opendpi.scenarios.RemoteSharing).
4.2
Application to Alternate
Rendering
Figure 8 illustrates the use of replication points for
magnifying glasses, magic lenses (Bier et al., 1993),
and radar views. The radar view implementation is
trivial: a local replication point is both attached to
the radar rectangle and to the ”window” layer of the
scene. The magnifying glass also uses a local repli-
cation both attached to the glass content and to the
main layer of the scene. The magic lens uses the same
technique but the replicated main layer is attached to
an outline renderer. The use of replication points for
both the magnifying glass and the magic lens may be
found unusual. However, it is motivated by the in-
A Toolkit for Developing Document-Centered Environments
237
(opendpi.scenarios.DnDPage).
(opendpi.scenarios.Sharing).

Figure
8:
Radar
view
and
(magic)
lenses
teraction consistency that OpenDPI guarantees: when
users interact above lenses, the interaction remains
consistent. In ﬁgure 8, the user moves the outlined
rectangle in a consistent way: he must point the rec-
tangle’s outline in order to translate it, which allows
the translation of the ellipse when pointing its masked
portion.
5
RELATED WORK
5.1
Document Centered Systems
OOE system extends the NextStep operating sys-
tem by allowing the edition of composite documents
(Backlund, 1997). It is based on the display Post-
Script capabilities of the NextStep: OOE applications
share a common display language so that areas of doc-
uments edited through an application can be displayed
without alteration by any another application. How-
ever, OOE only simpliﬁes the way of users swap be-
tween applications. OLE framework allows the edi-
tion of composite documents by deﬁning a communi-
cation protocol between applications (Brockschmidt,
1995). Consequently, it promotes the interoperability
between applications. However, it is based on the ap-
plication concept, forces to use a complex mechanism
of interoperability, and does not allow action interop-
erability such as needed in the pick-color example.
OpenDoc framework (Apple, 1994) is the closer ap-
proach to our model. It deﬁnes documents as a set of
hierarchically structured and typed parts. Each part
is associated with its own content model and interac-
tion model, can be viewed through a dedicated part
viewer, and can be edited within a dedicated part edi-
tor. However, the granularity of part editors and view-
ers are still high, there is no interoperability between
editors, and the interface changes from an editor to
another.
5.2
User Interface Models
The main common point between the MVC model
(Krasner and Pope, 1988) and our DPI model is the
separation and the synchronization of domain data
from their presentation.
Despite this fact, domain
data, presentation and instruments of DPI radically
differ respectively from the model, view and con-
troller of MVC. Firstly, the domain data is deﬁned
through a detailed model organized as a tree structure.
The model of MVC rather focuses on deﬁning prim-
itives (such as our property does) that can be viewed
and edited through widgets (view + controller pairs).
Secondly, the presentation is also deﬁned through a
detailed model based on scene-graphs while the view
of MVC does not provide any display model. Finally,
the instrument should deﬁnitively not be compared
with the controller of MVC. The instrument concept
deﬁnes an overall interaction model while MVC does
not address any since it only deals with widgets.
5.3
Document Model
DOM speciﬁcation (W3C, 2004) deﬁnes the docu-
ment as a tree where each node (called element) has a
state deﬁned by a set of attributes and/or child nodes.
This deﬁnition is thus compatible with the proper and
structural states of DPI components: each DPI com-
ponent is a node and the DPI properties might be con-
sidered as DOM attributes. However, the DOM model
does not address any aspects of action that can be per-
formed on documents and their elements and, as a
consequence, does not deﬁne any interaction model.
Moreover, the concurrent modiﬁcation of a document
is not taken into account.
5.4
Component Model
Using software components within workspaces is an
idea that appeared less than ten years ago. For ex-
ample, we can cite the COM architecture (Microsoft,
1995) and the JavaBeans component platform (Sun,
1997).
The implementation of the DPI model has
some similarities with the JavaBeans approach (e.g.
the deﬁnition of properties and the intensive use of in-
trospection). However, the JavaBeans model, like the
COM architecture, is truly generic and aims at build-
ing applications by assembling components. In our
approach, we focus on components that can substitute
applications: our goals and motivation thus radically
differ. Moreover, the DPI model deﬁnes a generic
component model that aims at being instantiated in a
document and an interaction model, while component
software architectures do no address such a problem.
Olivier Beaudoux and Michel Beaudouin-Lafon
238
(opendpi.scenarios.Rendering).

6
CONCLUSION AND
PERSPECTIVE
We have proposed in this article a component model
based on documents and instruments. It aims to sub-
stitute the application concept to the software compo-
nent concept at the workspace level in order to over-
come the problem induced by the intensive use of
widgets in today’s workspaces. The proposed model
goes into the opposite direction from widgets: DPI
components are open-boxes that respect an unifying
contract, while widgets are black-boxes that aim at
masking the internal complexity.
The overall DPI model has been implemented in
the OpenDPI java toolkit4. It uses Piccolo for dis-
playing graphics (Bederson et al., 2000). It provides
its own high level management of multiple human
input devices under Linux and a standard one under
other operating systems. The toolkit is made of about
250 classes and 18000 lines of code. We have imple-
mented a set of interactive components through sce-
nario, such as the ones presented in this paper, that
are often considered too complex to develop.
We
have noted that, by experimenting student projects,
the design and programming of new DPI components
is quite easy as soon as the DPI concepts are captured
(which was mainly done by analyzing sample codes).
Students have underlined the elegance of the ap-
proach and its unifying purpose.
DPI components
can be implemented without many programming ef-
forts in varied contexts such as bimanual interaction
or single display groupware, with the ability to guar-
antee the interaction consistency.
By deﬁning the
replication point concept, the OpenDPI toolkit pro-
vides an alternate rendering engine that can be used
to build interaction-consistent magic lenses, and gives
the foundation of the groupware facet of the model.
The next step of our work consists of validating the
DPI model by implementing a software suite dedi-
cated to speciﬁc tasks. Such a tool will allow our ap-
proach to be more precisely qualiﬁed, based on the in-
tensive use of interactive components without any ap-
plication (except the kernel of OpenDPI), and to dis-
cover potential new problems that this approach may
induce. We are also about to reﬁne the DPI model
by using well known standards. The document model
will be based on DOM and the presentation model
will be based on SVG (W3C, 2003). We will thus
extend DOM by specifying how the (inter)action and
collaboration aspects of DPI can be added on top of
it.
4http://www.eseo.fr/˜obeaudoux/opendpi
REFERENCES
Apple (1994). Opendoc technical summary. Technical doc-
umentation, Apple Computer Inc.
Backlund, B. E. (1997).
OOE: A compound document
framework. ACM SIGCHI Bulletin, 29(1):68–75.
Beaudouin-Lafon, M. (2000). Instrumental interaction: An
interaction model for designing post-wimp interfaces.
In Proc. CHI’00, pages 446–453. ACM Press.
Beaudouin-Lafon, M. and Karsenty, A. (1992).
Trans-
parency and awareness in a real-time groupware sys-
tem. In Proc. UIST’92, pages 171–180. ACM Press.
Beaudouin-Lafon, M. and Mackay, W. (2000). Reiﬁcation,
polymorphism and reuse: Three principles for design-
ing visual interfaces. In Proc. AVI’00, pages 102–109.
ACM Press.
Beaudoux, O. and Beaudouin-Lafon, M. (2001). DPI: A
conceptual model based on documents and interaction
instruments. In Proc. IHM-HCI’01, pages 247–263.
Springer Verlag.
Bederson, B. B., Meyer, J., and Good, L. (2000). Jazz: An
extensible zoomable user interface graphics toolkit in
java. In Proc. UIST’00, pages 171–180. ACM Press.
Bier, E. A., Stone, M. C., Pier, K., Buxton, W., and DeRose,
T. D. (1993). Toolglass and magic lenses: the see-
through interface. In Proc. of SIGGRAPH’93, pages
73–80. ACM Press.
Brockschmidt, K. (1995). Inside OLE, Second Edition. Mi-
crosoft Press.
Dewan, P. and Choudhary, R. (1992).
A high-level and
ﬂexible framework for implementing multiuser user
interfaces.
ACM Trans. on Information Systems,
10(4):345–380.
Gamma, E., Helm, R., Johnson, R., and Vlissides, J.
(1994).
Design Patterns:
Elements of Reusable
Object-Oriented Software. Addison-Wesley.
Johnson, J., Roberts, T. L., Verplank, W., Smith, D. C., Irby,
C., Beard, M., and Mackey, K. (1989). The Xerox
Star: A retrospective. IEEE Computer, 22(9):11–29.
Knister, M. J. and Prakash, A. (1990). DistEdit: a distrib-
uted toolkit for supporting multiple group editors. In
Proc. CSCW’90, pages 343–355. ACM Press.
Krasner, G. E. and Pope, S. T. (1988).
A cookbook for
using the Model-View-Controller user interface para-
digm in Smalltalk-80. Journal of Objet Oriented Pro-
gramming, pages 26–49.
Microsoft (1995). The component object model speciﬁca-
tion. Speciﬁcation Document.
Sun (1997).
JavaBeans API speciﬁcation.
Speciﬁcation
document.
W3C (2001). Web services description language (WSDL)
1.1. Technical report, Consortium W3C.
W3C (2003). Scalable vector graphics (SVG) 1.1 speciﬁca-
tion. Technical report, Consortium W3C.
W3C (2004). Document object model (DOM) level 3 core
speciﬁcation. Technical report, Consortium W3C.
A Toolkit for Developing Document-Centered Environments
239

WHY ANTHROPOMORPHIC USER INTERFACE FEEDBACK 
CAN BE EFFECTIVE AND PREFERRED BY USERS 
Pietro Murano 
University of Salford, School of Computing, Science and Engineering, Computer Science Research Centre, Interactive 
Systems and Media Laboratory, Gt. Manchester, M5 4WT, England, UK 
Email: p.murano@salford.ac.uk 
Keywords: 
User interface feedback, Anthropomorphism, Human behaviour. 
Abstract:  
This paper addresses and resolves an interesting question concerning the reason for anthropomorphic user 
interface feedback being more effective (in two of three contexts) and preferred by users compared to an 
equivalent non-anthropomorphic feedback. Firstly the paper will summarise the author’s three 
internationally published experiments and results. These will show statistically significant results indicating 
that in two of the three contexts anthropomorphic user interface feedback is more effective and preferred by 
users. Secondly some of the famous work by Reeves and Nass will be introduced. This basically shows that 
humans behave in a social manner towards computers through a user interface. Thirdly the reasons for the 
obtained results by the author are inextricably linked to the work of Reeves and Nass. It can be seen that the 
performance results and preferences are due to the subconscious social behaviour of humans towards 
computers through a user interface. The conclusions reported in this paper are of significance to user 
interface designers as they allow one to design interfaces which match more closely our human 
characteristics. These in turn would enhance the profits of a software house. 
1 INTRODUCTION 
User interface feedback in software systems is being 
improved as time passes and developers dedicate 
more time to the feedback and realise that feedback 
to the user is just as important as the rest of an 
application.  
In line with the goal of constant improvement 
and better understanding of user interface feedback 
this research has looked at the effectiveness and user 
approval of anthropomorphic feedback. This was 
compared to an equivalent non-anthropomorphic 
feedback.  
Anthropomorphism at the user interface usually 
involves assigning human characteristics or qualities 
or both to something which is not human, e.g. a 
talking dog or a cube with a face that can talk etc. A 
well known example is the Microsoft Office Paper 
Clip. It could also be the actual manifestation of a 
real human such as a video of a human (Bengtsson 
et al, 1999).  
This issue has been considered because there was 
a division between computer scientists where certain 
computer scientists are against (e.g. chapter by 
Shneiderman 
in 
((Bradshaw, 
1997) 
and 
(Shneiderman, 1992)) anthropomorphism at the user 
interface and others are in favour (e.g. Agarwal 
(1999), Cole et al. (1999), Dertouzos (1999), Guttag 
(1999), Koda and Maes (1996a), (1996b), Maes 
(1994) and Zue (1999)) of using anthropomorphism 
at the user interface. However there has not been 
concrete enough evidence to show which opinion 
may be correct. 
Experiments (summarised below and detailed in 
Murano (2001a), (2001b), (2002a), (2002b), (2003)) 
have been conducted where it has been shown with 
statistical significance that in certain contexts 
anthropomorphic user interface feedback is more 
effective and preferred by users. However these 
experiments concentrated on ‘what’ type of 
feedback was better (i.e. anthropomorphic or non-
anthropomorphic) and not on ‘why’ a particular type 
of feedback was better over the other. 
This issue of ‘why’ was raised as an interesting 
question 
at 
various 
international 
conferences 
attended by the author. Hence firstly this paper aims 
to address this question and provide an answer by 
means of the body of evidence produced by Reeves 
and Nass. It is believed by the author that no other 
researchers outside of Reeves and Nass’ influence 
have used and validated some of their results in such 
a detailed manner. Secondly, the experiments 
conducted and summarised below, are innovative in 
that while they follow the guidelines of Reeves and  
241
C.-S. Chen et al. (Eds.), Enterprise Information Systems VII, 241–248.
© 2006 Springer.

Table 1: Comparison of Video Vs Diagrams and text 
Comparison of Video Vs. 
Diagrams and Text
t-Observed
2.14
t-Critical (5%)
1.74
Table 2: Overall User Preferences 
Overall User Preferences
Mean 
Standard Deviation
Video
8.17
1.10
Diagrams and Text
7.11
2.17
Nass, this is the first time that the guidelines have 
been applied to a more realistic context. The 
experiments by Reeves and Nass were more 
artificial in nature. 
This is because despite many computers and 
applications being in homes and businesses, there 
are still many prospective users who are afraid of 
computers. These prospective users could become 
actual enthusiastic users, thus potentially increasing 
business profits for a software house. This could be 
achieved by the improvement of user interface 
feedback by using the findings of this paper. 
2 SUMMARY OF EXPERIMENTS 
In the next three sections below a brief summary is 
presented of the three experiments. Full details for 
repeatability can be found in Murano (2001a), 
(2001b), (2002a), (2002b), (2003). However for 
each of the three experiments within users’ designs 
were used. This meant that in each experiment all 
the subjects tried all tasks and had the opportunity to 
use all relevant kinds of feedback. Considerable 
efforts were made to maintain laboratory conditions 
constant for each subject. Also efforts were made to 
control possible confounding variables. 
2.1 Experiment One 
The first experiment Murano (2002a) was in the 
context of software for in-depth understanding. This 
was specifically English as a foreign language (EFL) 
pronunciation. The language group used was Italian 
native speakers who did not have ‘perfect’ English. 
Software was specifically designed to automatically 
handle user speech via an automatic speech 
recognition (ASR) engine. Further, in line with EFL 
literature by Kenworthy (1992) and Ur (1996) 
exercises were designed and incorporated as part of 
the software to test problem areas that Italian 
speakers have when pronouncing English.  
Anthropomorphic feedback in the form of a 
video of a real EFL tutor giving feedback was 
designed. This in effect was a set of dynamically 
loaded video clips which were activated based on 
the software’s decision concerning the potential 
error a user had done (if no errors were made no 
pronunciation corrections were made by the 
software). This type of feedback was compared 
against a non-anthropomorphic equivalent. In this 
case two-dimensional diagrams with guiding text 
were used. The diagrams were facial cross-sections 
aiming to assist a user in the positioning of their 
mouth and tongue etc. for the relevant pronunciation 
of a given exercise. This type of feedback was based 
on EFL principles found in Baker (1981) and Baker 
(1998). No feedback type was ever tied to the same 
exercise, i.e. feedback was randomly assigned to an 
exercise.
The results for 18 Italian users (with imperfect 
English pronunciation) taking part in a tightly 
controlled experiment, going through a series of 
exercises were statistically significant. Users were 
scored (scores used in hypothesis testing statistical 
analysis) according to the number of attempts they 
had to make to complete an exercise successfully.  
The 
statistical 
results 
suggested 
the 
anthropomorphic feedback to be more effective. 
Users were able to self-correct their pronunciation 
errors more effectively with the anthropomorphic 
feedback. The scores obtained were approximately 
normally distributed. These were then used in a t-
test. The results are in the Table 1. 
Furthermore it was clear that users preferred the 
anthropomorphic feedback. The actual scores 
obtained from the questionnaires using a Likert 
scale, where 1 was a negative response and 9 was a 
positive response, are detailed in Table 2. 
Hence it was concluded that the statistically 
significant results suggested the anthropomorphic  
Pietro Murano 
242

Table 3: Comparison of Video Vs Text 
Comparison of Video Vs Text
t-Observed
10.21
t-Critical (5%)
1.67
 Table 4: Overall User Preferences 
Overall User Preferences
Mean 
Standard Deviation
Video
7.53
1.40
Text
6.35
1.84
feedback to be more effective and preferred by 
users.  
2.2 Experiment Two 
The second experiment by Murano (2001a), 
(2001b), (2002b) was in the context of software for 
online systems usage. 
This was specifically concerned with the using of 
UNIX commands. This was an interesting area as 
typically novice users of UNIX commands can find 
it difficult to master the concepts of the command 
structure and to remember relevant commands in the 
first place. Software was designed to emulate a small 
session at the UNIX shell covering a sub-set of 
UNIX commands. As in the first session, an ASR 
engine was used which allowed the users to ‘query’ 
the system verbally. The users which were recruited 
for the experiment were complete novices to UNIX 
commands.  
In this experiment anthropomorphic feedback 
was 
compared 
with 
a 
non-anthropomorphic 
equivalent. In this case the anthropomorphic 
feedback consisted of dynamically loaded video 
clips of a person giving the command verbally for 
the current context the user was in. The feedback 
was prompted by the user requesting the feedback 
from the system (through the ASR engine). The non-
anthropomorphic feedback was a textual equivalent 
(based on the structure used in Gilly (1994)) 
appearing in a supplementary window next to the 
main X-Window. A small set of typical tasks a 
beginner might engage in, involving UNIX 
commands, were designed. Since the users had no 
knowledge of UNIX commands, they were obliged 
to make use of the feedback if they wished to 
complete the tasks. The two types of feedback were 
randomly assigned to the tasks so that one task was 
not tied to one type of feedback.  
The results for this tightly controlled experiment, 
which involved 55 users who were novices to UNIX 
commands, were statistically significant. The users 
were scored (scores used in hypothesis testing 
statistical analysis) as they attempted a set of tasks 
using UNIX commands. Scores were devised 
according to the number of errors, hesitations and 
completions/non-completions a user was able to 
carry out. Further, scores were obtained via a 
questionnaire for users’ opinions on the system 
feedback given them.  
The 
statistical 
results 
suggested 
the 
anthropomorphic feedback to be more effective. The 
scores obtained were approximately normally 
distributed. These were then used in a t-test. The 
results are in the Table 3. 
Furthermore it was clear that users preferred the 
anthropomorphic feedback. The actual scores from 
the questionnaires using a Likert scale, where 1 was 
a negative response and 9 was a positive response, 
are detailed in Table 4. 
Hence it was concluded that the statistically 
significant results suggested the anthropomorphic 
feedback to be more effective and preferred by 
users. The users were able to carry out the tasks 
more 
effectively 
with 
the 
anthropomorphic 
feedback, whilst indicating a preference for the 
anthropomorphic feedback.  
2.3 Experiment Three 
The third experiment by Murano (2003) was in the 
context of software for online factual delivery. 
Specifically the context for this area was direction 
finding. Software was developed to give directions 
to 
two 
different 
but 
equivalent 
locations 
(equivalence was concerned with approximately 
equal distances and difficulty), where the aim was 
for test subjects to physically find their way to the 
given locations. The subjects were to use the 
directions given to them by the system. Hence it was 
a prerequisite that the subjects should not have 
known where the locations were before taking part 
in the experiment (this was determined as part of a 
questionnaire).   
In this experiment anthropomorphic feedback 
was 
compared 
with 
a 
non-anthropomorphic 
equivalent. In this case the anthropomorphic 
feedback consisted of dynamically loaded video
Why Anthropomorphic User Interface Feedback can be Effective and Preferred by Users 
243

Table 5: F-test Results - Diagram Vs. Video 
Comparison of Diagram(Map) Vs Video
F-Observed
1.85
F-Critical (5%)
1.67
Table 6: Overall user preferences 
O verall U ser Preferen ces
M ean  
Stan dard D eviation
V ideo
6.42
1.68
D iagram  (M ap)
6.74
1.62
clips of a person giving directions to a location. This 
was 
compared 
with 
an 
equivalent 
non-
anthropomorphic feedback consisting of a map with 
guiding text based on the principles found in 
Southworth and Southworth (1982). One type of 
feedback was not tied to one particular location in 
the experiment. The feedback was rotated so that 
each location had either type of feedback at some 
point in the experiment.  
The results for this tightly controlled experiment, 
which 
involved 
53 
users, 
were 
statistically 
significant (for effectiveness in favour of the map). 
The users in the experiment were scored (scores 
used in hypothesis testing statistical analysis) 
according to the amount of mistakes they made (i.e. 
wrong turnings taken), visible hesitations and if the 
subjects actually reached the prescribed location. 
Users were also given a questionnaire which allowed 
them to express their opinions concerning the 
feedbacks. The results were statistically significant 
in favour of the map with guiding text feedback. The 
users overall performed the tasks of direction finding 
more effectively with the map and guiding text. 
The actual data collected was found to be 
approximately normally distributed and was used in 
an F-test. The results are in Table 5, suggesting the 
map to be more effective. 
The opinions of the users concerning their 
preferences were much less clear. The actual scores 
from the questionnaires using a Likert scale, where 1 
was a negative response and 9 was a positive 
response, are detailed in Table 6. 
As can be seen from the above table, the scores 
for the opinions (overall user preferences) showed 
the map to be only slightly better than the 
anthropomorphic feedback. Many of the users liked 
very much the idea of having ‘someone’ give them 
directions rather than the map. This resulted in them 
scoring the anthropomorphic feedback much higher 
than expected.  
2.4 Overall Discussion of 
Experiments
The experiments summarised in the last three 
sections 
show 
clearly 
that 
anthropomorphic 
feedback is generally liked by users in most 
situations. However the effectiveness of such 
feedback is dependant on the domain of concern. 
Hence certain domains appear to not be suited to 
anthropomorphic feedback, such as the domain for 
online factual delivery, particularly the direction 
finding context. This is also confirmed by the 
suggestion based on other research discussed in 
Dehn and van Mulken (2000). However as the third 
experiment showed, users still like seeing and 
interacting with anthropomorphic feedback even if it 
is not the best mode of feedback for them to achieve 
their tasks.
These results suggest the conclusion that it 
would be better for designers of feedback to include 
anthropomorphic feedback in the domains shown to 
be better suited to such a style. For the domains not 
suited to anthropomorphism it clearly needs stating 
that a suitable non-anthropomorphic feedback 
should be used instead. However based on what 
users like, it may be suitable to combine non-
anthropomorphic feedback with some form of 
anthropomorphic feedback. An example based on 
the third experiment described above is to have the 
map with guiding text (which was more effective), 
and to perhaps have a video or synthetic character of 
a person giving some ‘external’ (not the actual 
directions) information. ‘External’ information could 
simply be to introduce the user to study the map 
being presented to them. This would give the user 
the 
benefit 
of 
anthropomorphism 
and 
the 
effectiveness of the map with guiding text. This 
suggestion may seem simple. However many 
software packages have failed due to bad user 
interfaces and feedback. Sometimes the problems 
could have been resolved by fairly simple means. 
Hence this suggestion is in line with the idea that 
sometimes minor adjustments can dramatically 
improve the usability of a system.  
Pietro Murano 
244

3 THE WORK OF REEVES AND 
NASS APPLIED TO USER 
INTERFACE FEEDBACK 
As stated in the introduction, these issues really only 
deal with the ‘how’. This means that it has been 
discovered how we should give feedback in certain 
domains, i.e. certain domains are better suited to 
anthropomorphic feedback. However the issue of 
‘why’ has not been addressed by the experiments, 
i.e. 
why 
is 
it 
that 
in 
certain 
domains 
anthropomorphic feedback is more effective and 
preferred by users? The answer lies in us as humans.   
Reeves and Nass (1996) have for many years 
conducted research very compatible with the 
research summarised in this paper. In their book 
‘The Media Equation’ (1996), they have discussed 
empirical findings which give us the answer to the 
question posed in the previous paragraph. In this 
large body of research they have found that people 
in general (including computer scientists) tend to 
interact with a computer in a social manner and in a 
very similar manner to the way one interacts away 
from a computer, i.e. with other people etc. Also 
they have found that people apply the basic social 
rules of every day life to their interaction with 
computers. 
This 
is 
done 
automatically 
and 
intuitively by humans. In fact they do not even 
realise they are behaving in this manner.  
These points are crucial to the findings of the 
three 
experiments 
summarised 
above. 
The 
suggestion here is that the subjects concerned were 
subconsciously applying human social rules whilst 
interacting with the feedbacks and because one of 
the types of feedback (anthropomorphic) was more 
compatible with the applying of social rules, the 
results showed more effectiveness (in two of the 
experiments) and very importantly high user 
approval (in all three experiments).  
This suggestion does not explain why the results 
did not apply to one of the experiments (the 
direction finding experiment). One explanation is 
that whilst users would have been applying social 
rules in all circumstances, the direction finding 
experiment was much better suited to the map with 
guiding text feedback. However the issue of user 
approval for the anthropomorphic feedback in the 
direction finding experiment does support the 
findings that users will apply social rules whilst 
interacting with computers.  
This issue leads to other aspects of the research 
by Reeves and Nass – the unique characteristics of 
anthropomorphic feedback. There are various 
characteristics 
which 
easily 
occur 
in 
anthropomorphic feedback that are compatible with 
the subconscious use of social rules. It is these 
characteristics that account for the ‘why’ or the 
reason for anthropomorphic feedback being more 
effective in certain domains and being mostly 
preferred by users.  
One aspect concerns the fact that human-to-
human communication involves eye contact and it 
has been shown in Ekman (1973) and Ekman et al 
(1972) that if a person is looking at a ‘face’, about 
half of the time used in this activity is used to look at 
the eyes. Also if one matches modalities this usually 
incurs a better response, e.g. if one sends an email to 
a friend, usually the reply will be sent by email and 
not by a telephone call, as stated by Reeves and Nass 
(1996). Reeves and Nass (1996)argue that this 
human phenomenon could work at the user interface 
if one could overcome the obvious barriers to this, 
e.g. if an electronic voice issues advice/information 
to the user it would be better to have the system 
accept input verbally from the user, via an ASR 
engine. This is because communication modalities 
are being matched and are closer to the human-to-
human social rules.  
These two aspects are very important with 
respect to the anthropomorphic feedback used in the 
experiments. This is because as stated above the 
anthropomorphic feedback consisted of dynamically 
loaded video clips of a person. These clips showed 
the face of the person clearly and within the limits of 
the video one could see the eyes. This is very 
important based on the material of Ekman (1973) 
and Ekman et al (1972) because the users would 
have been subconsciously looking at the face and 
spending a good proportion of the time looking at 
the eyes of the person in the video clips (whilst 
listening to the help given). Furthermore the 
communication modalities were well matched as the 
user communicated with the system via the ASR 
engine and clearly the anthropomorphic feedback 
(video of human talking) was also communicating 
verbally.
Another aspect that should be considered is the 
way the experiments were conducted. Reeves and 
Nass argue that when one is testing a product 
presented by a computer the computer should not 
ask the user for evaluations. If the evaluations are to 
be done electronically it would be better to use a 
different computer. Alternatively these would be 
better conducted by some paper based means. 
However caution would still need to be deployed 
because if the person conducting the experiment was 
also the person helping the user in some way, then 
Reeves and Nass (1996) state that a subject may 
subconsciously look for the most diplomatic 
responses so as not to upset anyone. They suggest 
that the best way to overcome these problems is to 
test two ‘products’ against each other. In this way 
Why Anthropomorphic User Interface Feedback can be Effective and Preferred by Users 
245

the subjects do not feel obliged to respond in some 
socially acceptable manner.  
The experiments were conducted in the manner 
suggested by Reeves and Nass. In each case two 
types of feedback were the basis of each experiment, 
being tested against each other. The subjects were 
asked for their evaluations on these for usability etc. 
Furthermore the evaluations were carried out by the 
subjects not in an electronic manner, but away from 
the computer by means of carefully designed paper-
based questionnaires. Thus it is suggested that any 
bias on the evaluation concerning the applying of 
certain social rules should have been dramatically 
reduced if not eliminated completely.  
Another aspect requiring consideration is that 
Reeves and Nass (1996) and Reeves at al (1992) 
discovered that people tend to have similar reactions 
with a picture of a person as they do with a real 
person in front of them. When a person sees another 
person that is near them, the human subconscious 
result is that people will evaluate that person more 
intensely, pay more attention to them and remember 
them better. They found that these principles still 
applied if one looked at a picture of a person. This is 
important as it affects the way a person could view 
some anthropomorphic feedback, either of a person 
or some synthetic character.  
The experiments conducted by the author had as 
stated above anthropomorphic feedback consisting 
of video clips of a person. These were filmed in such 
a manner so as to follow the principles found by 
Reeves and Nass. The person in the clips was seen to 
be near to the person using the feedback. This was 
achieved simply by filming the person from not too 
far a distance. Also there were no large ‘open 
spaces’ around the person being filmed. This 
resulted in the person appearing quite close to the 
user viewing the feedback. This filming strategy 
would have resulted in users feeling a more intense 
evaluation of the feedback along with enhanced 
memory results and actually paying more attention 
to the feedback during the session. It is suggested 
that this would have resulted in the users basically 
performing their tasks better and also evaluating the 
feedback very positively.  
Reeves and Nass (1996) also discuss the effects 
of having ‘unnecessary peripheral motion’. They say 
that having this in an interaction leads the user to be 
distracted from their current attention giving activity 
to the ‘item’ moving at some other position in the 
screen or window. This clearly results in something 
being ignored from the primary interaction.  
This suggestion was put into practice for the 
feedbacks. Simply no ‘peripheral motion’ was used 
so that all the attention could be put onto the 
feedback and the help being given for achieving the 
tasks.
4 CONCLUSION 
The issues discussed above provide a reasonable 
explanation based on empirical findings concerning 
the reasons for the anthropomorphic feedback being 
more effective (in two of the three contexts tested) 
and liked by users in all cases.  
One aspect is that humans, whether they admit it 
or not, behave in a social manner with computers, 
applying various social rules as Reeves and Nass 
found. This has been crucial to the effects observed 
in the experiments summarised above.  
The next important aspect as discussed above is 
the fact that anthropomorphic feedback and the way 
it is presented to users can very naturally provide 
humans with the appropriate ‘cues’ for them to 
behave in a more social manner towards the 
feedback. This in turn results in better task 
completions and a higher satisfaction rate in certain 
contexts.  
This work has also shown the validity of the 
work by Reeves and Nass in this area as the findings 
of the reported experiments corroborate some of the 
findings by Reeves and Nass. This work also takes 
the work of Reeves and Nass further as this work 
has been conducted in a much more realistic set of 
contexts compared to the contexts used by Reeves 
and Nass.  
Concerning interface designers it is suggested 
that they should take seriously the use of 
anthropomorphic feedback, as the suggestion is that 
it leads to better more productive interactions and 
more usable interfaces. This is significant as more 
and more people who are not ‘professionals’ are 
using computer systems and their software. This in 
turn brings the requirement of developing better user 
interfaces by using results such as the ones discussed 
in this paper. This in turn could have a beneficial 
effect to the profits of a software house. Clearly if 
they can apply these principles they could 
potentially attract a new market, particularly 
composed of those who may be afraid of computers.  
However despite the potential for applying these 
findings immediately in a business context, there is 
more work required. One of the experiments showed 
that the anthropomorphic feedback was not as 
effective as the non-anthropomorphic equivalent. 
This leads to the requirement of investigating other 
areas of user interface feedback to try and find other 
domains not suited to this type of feedback. 
Ultimately, a taxonomy of possible domains and 
suitable types of feedback could be devised over 
time. If this was available based on empirical 
findings, user interface designers could be helped 
when they are faced with the many decisions they 
have to take when designing user interfaces.  
Pietro Murano 
246

A further area that would need investigating is 
feedback in a virtual reality setting and also the very 
different setting of performing two or more tasks at 
once. An example would be a car driver driving a 
car and trying to find directions to some location 
(currently this is done with navigation software and 
any future study should take into account issues such 
as cognitive load and divided attention etc.). 
Alternatively, investigating this type of feedback for 
pilots could lead to some interesting findings and 
perhaps a more comprehensive taxonomy of the kind 
suggested in the previous paragraph.  
ACKNOWLEDGMENTS 
The School of Computing, Science and Engineering 
at the University of Salford, Prof. Sunil Vadera, 
Prof. Tim Ritchings and Prof. Yacine Rezgui are 
thanked for their support.  
REFERENCES
Agarwal, 
A. (1999) 
Raw 
Computation. Scientific 
American., 281: 44-47. 
Baker, A. (1981) Ship or Sheep? An Intermediate 
Pronunciation Course, Cambridge University Press. 
Baker, A. (1998) Tree or Three? An Elementary 
Pronunciation Course, Cambridge University Press. 
Bengtsson, B., Burgoon, J.K. et al.( 1999) The Impact of 
Anthropomorphic 
Interfaces 
on 
Influence, 
Understanding and Credibility. Proc of the 32nd
Hawaii International Conference on System Sciences,.
IEEE
Bradshaw, J. M. (1997) Software Agents, AAAI Press, 
MIT Press 
Brennan, S.E and Ohaeri, J.O. (1994) Effects of Message 
Style on Users’ Attributions Toward Agents. CHI ’94 
Human Factors in Computing System 
Cole, R., D. W. Massaro, et al. (1999) New Tools for 
Interactive Speech and Language Training: Using 
Animated Conversational Agents in the Classrooms of 
Profoundly 
Deaf 
Children. 
Method 
and 
Tool 
Innovations for Speech Science Education,
Dehn, D. M. and van Mulken, S. (2000) The Impact of 
Animated Interface Agents: A Review of Empirical 
Research. International Journal of Human-Computer 
Studies 52: 1-22
Dertouzos, M. L. (1999) The Future of Computing. 
Scientific American., 281: 36-39.
Ekman, P. (ed.) (1973) Darwin and Facial Expression: A 
Century of Research in Review, Academic Press. New 
York. 
Ekman, P., Friesen, W.V., Ellsworth, P. (1972) Emotion in 
the Human Face: Guidelines for Research and an 
Integration of Findings, Pergamon Press. New York.  
Gilly, D. (1994) UNIX In a Nutshell, O’Reilly and 
Associates 
Guttag, J. V. (1999) Communications Chameleons 
Scientific American., 281: 42, 43.  
IBM (1998), IBM ViaVoice 98 User Guide, IBM, 
Kenworthy, J.(1992) Teaching English Pronunciation,
Longman.
Koda, T. and Maes, P. (1996a) Agents With Faces: The 
Effect of Personification. Proc of the 5th IEEE 
International Workshop on Robot and Human 
Communication,, IEEE.
Koda, T. and Maes, P. (1996b) Agents With Faces: The 
Effects of Personification of Agents. Proc of HCI ’96,
British HCI Group. 
Maes, P. (1994) Agents That Reduce Work and 
Information Overload. Communications of the ACM,
37(7): 31-40, 146.  
Murano, P. (2001a) A New Software Agent     'Learning' 
Algorithm.
People in Control An International
Conference on Human Interfaces in ControlRooms, 
Cockpits and Command Centres, IEE.  
Murano, 
P. 
(2001b) 
Mapping 
Human-Oriented 
Information to Software Agents For Online Systems 
Usage. People in Control An International Conference 
on Human Interfaces in Control Rooms, Cockpits and 
Command Centres, IEE.  
Murano, P. (2002a) Effectiveness of Mapping Human-
Oriented Information to Feedback From a Software 
Interface. 24th International Conference Information 
Technology Interfaces. 
Murano, 
P. 
(2002b) 
Anthropomorphic 
Vs 
Non-
Anthropomorphic Software Interface Feedback for 
Online Systems Usage. 7th European Research 
Consortium 
for 
Informatics 
and 
Mathematics 
(ERCIM) Workshop - 'User Interfaces for All' – 
Special Theme: 'Universal Access'.Paris,. Published in 
Lecture Notes in Computer Science (C) - Springer. 
Murano, 
P. 
(2003) 
Anthropomorphic 
Vs 
Non-
Anthropomorphic Software Interface Feedback for  
Online Factual Delivery. 7th International Conference on 
Information Visualisation (IV 2003) An International 
Conference on ComputerVisualisation and Graphics
Applications, (c) – IEEE.  
Nass, C., Steuer, J. et al. (1994) Computers are Social
Actors. CHI ’94 Human Factors in Computing 
Systems – ‘Celebrating Interdependence’, ACM. 
Reeves, B., Lombard, M. and Melwani, G. (1992) Faces 
on the Screen: Pictures or Natural Experience, 
International Communication Association.
Reeves, B. and Nass, C. (1996) The Media Equation How 
People Treat Computers, Television, and New Media 
Like Real People and Places, Cambridge University 
Press.  
Why Anthropomorphic User Interface Feedback can be Effective and Preferred by Users 
247

Shneiderman, B. (1992) Designing the User Interface – 
Strategies for Effective Human Computer Interaction,
Addison-Wesley. 
Southworth, M. and Southworth, S. (1982) Maps a Visual 
Survey and Design Guide. Little, Brown and Co.  
Ur, P. (1996) A Course in Language Teaching - Practice 
and Theory, Cambridge University Press 
Zue, V. (1999) Talking With Your Computer. Scientific 
American., 281: 40 
Pietro Murano 
248

DISTANCE LEARNING BY INTELLIGENT TUTORING SYSTEM
Agent-based architecture for user-centred adaptivity 
Antonio Fernández-Caballero and José Manuel Gascueña 
Computer Science Reseach Institute of Albacete, University of Castilla-La Mancha, Albacete, Spain 
Email: caballer@info-ab.uclm.es 
Federico Botella and Enrique Lazcorreta 
Operations Research Centre, University Miguel Hernandez of Elche, Elche, Spain 
Email: federico@umh.es,  enrique@umh.es 
Keywords: 
Intelligent Tutoring System, Agent system, Architecture, E-learning, E-teaching, Adaptivity. 
Abstract: 
Agent technology has been suggested by experts to be a promising approach to fully extend Intelligent 
Tutoring Systems (ITS). By using intelligent agents in an ITS architecture it is possible to obtain an 
individual tutoring system adaptive to the needs and characteristics of every student. The general 
architecture of the ITS proposed is formed by the three components that characterize an ITS – the Student 
Model, the Domain Model, and the Education Model. In the Student Model the knowledge that the system 
has about the student (profile and interaction with the system) is represented. In the Domain Model the 
knowledge about the contents to be taught is stored. Precisely, in this model four autonomous agents – the 
Preferences Agent, the Accounting Agent, the Exercises Agent and the Tests Agent - have been defined. 
Lastly, the Education Model provides the functionality that the teacher needs. Across this module, the 
teacher changes his preferences, gives reinforcement to the students, obtains statistics and consults the 
matter. 
1 INTRODUCTION 
Agent technology has been suggested by experts to 
be a promising approach to fully extend Intelligent 
Tutoring Systems (ITS). By using intelligent agents 
in an ITS architecture it is possible to obtain an 
individual tutoring system adapted to the needs and 
characteristics of every student (Frigo, Pozzebon & 
Bittencourt, 2004). In this article, an agent-based 
Intelligent Tutoring System architecture for user-
centred adpativity in e-learning/e-teaching of any 
matter is introduced. A detailed description of the 
agents which monitor the progress of the students 
and propose new tasks is also provided. The ITS 
proposed is not tied to any course in particular, 
being the only requisite that the course has to be 
divided into theory, exercises and tests.  
Many 
learning/teaching 
computer-based 
environments framed in the form of ITS use agent 
technology. For example, Cheikes has developed 
GIA (Generic Instructional Architecture), an agent-
based software infrastructure devoted to support 
rapid development of ITS applications (Cheikes, 
1995). Tang carried out the implementation of a 
multi-agent intelligent tutoring system for the 
learning of computer programming (Tang & Wu, 
2000). Capuano has described ABITS, a highly 
reusable Intelligent Tutoring Framework suitable to 
several knowledge domains (Capuano, Marsella & 
Salerno, 2000). A multi-agent system named 
MASPLANG developed for the adaptation of the so-
called teaching support units has been introduced 
(Peña, Marzo & de la Rosa, 2002). Hospers et al. 
have presented an agent-based ITS for nurse 
education (Hospers et al., 2003). And there are many 
more approaches in distance learning (e.g., Bello & 
Bringsjord, 2003; Mota, Oliveira & Mouta, 2004; 
Kinshuk et al., 2001; de Antonio et al., 2003; Dorça, 
Lopes & Fernández, 2003; Pesty & Webber, 2004; 
Baldoni, Baroglio & Patti, 2004). 
An ITS usually also incorporates pedagogical 
agents (animated characters) to do learning more 
attractive and effective. For example, there is Adele 
for medical education (Shaw et al., 1999), and 
AutoTutor for the students to learn the fundamentals 
of computer hardware, the operating system, and the 
Internet (Person & Graesser, 2000). SONIA is the 
animated agent incorporated in MASPLANG. The 
249
C.-S. Chen et al. (Eds.), Enterprise Information Systems VII, 249–256.
© 2006 Springer.

architecture that we introduce in this article does not 
incorporate at present any animated agent. 
The layout of the paper is as follows. In section 2 
a definition of ITS is provided as its most common 
features are introduced. In section 3 we define what 
an agent is. In section 4 the aims of our agent-based 
ITS are explained. From section 5 on, the ITS 
architecture is introduced. Lastly, some conclusions 
are provided.
2 DEFINITION OF AN ITS 
ITS are programs that possess a wide knowledge on 
a certain matter, and their intention is to transmit this 
knowledge to the students by means of an interactive 
individualized process, trying to emulate the form in 
which a tutor or human teacher would guide the 
student in his learning process (Millán, Agosta & 
Pérez, 1999).  
Thus, ITS for sure are systems of knowledge 
communication. They can be defined that way 
because the principal emphasis in the development 
of these systems is to provide them with access to 
the representation of the knowledge that the system 
tries to communicate to the student.  
In an ITS the emphasis is put in the knowledge 
(what) to being communicated to the student and not 
in the mechanism (how) of communication used to 
present the knowledge to the student. 
Generally speaking, ITS are characterized for 
incorporating three models corresponding to three 
knowledge levels (see figure 1). Firstly, there is a 
Domain Model where the Knowledge of the Domain 
is gathered, that is to say the knowledge of what has 
to be taught. A Student Model represents the 
Knowledge of the Student, that is to say all things 
the student knows on the domain. Finally, there is a 
Pedagogical Model where the Knowledge of the 
Instructional strategies is described; that is to say, 
how to teach the Domain Knowledge. 
3 DEFINITION OF AN AGENT 
There is no universally accepted definition for the 
term agent, but there are is a wide range of 
perspectives in function of the application domain, 
the author, and so on.  
Franklin and Graesser state: “An autonomous 
agent is a system situated within and a part of an 
environment that senses that environment and acts 
on it, over time, in pursuit of its own agenda and so 
as to effect what it senses in the future.” (Franklin & 
Graesser, 1996).  
Any agent,  in accordance with this definition, 
satisfies the four properties as indicated next: 
x autonomy: agents operate without the direct 
intervention of humans or others, and have 
some kind of control over their actions and 
internal state; 
x social ability: agents interact with other agents 
(and possibly humans) via some kind of 
agent-communication 
language; 
agents 
collaborate for the sake of performing tasks;
x reactivity: agents perceive their environment, 
(which may be the physical world, a user via a 
graphical user interface, a collection of other 
agents, the Internet, or perhaps all of these 
combined), and respond in a timely fashion to 
changes that occur in it; in order to respond 
effectively to changes, agents have to know at 
each instant their surrounding “world”;
x pro-activeness: agents do not simply act in 
response to their environment, they are able to 
exhibit goal-directed behaviour by taking the 
initiative. 
4 OBJECTIVES OF THE AGENT-
BASED ITS 
The ITS proposed in this paper creates an 
infrastructure for distance learning/teaching of a 
matter. In accordande with our experience, and in 
order to obtain good results, we propose to 
decompose the matter to be taught into theory, 
exercises and test questionnaires (see figure 2). The 
alumni study each topic of the matter reading theory 
first, then making exercises and finally answering to 
a test. The system will provide help the students 
whenever it will be felt necessary. 
Antonio Fernández-Caballero et al. 
250
Figure 1: Components of an ITS.

The first goal of the ITS proposed is that the 
alumni learn more and better, that is to say, the 
system has to be able to structure learning matter in 
such a way to facilitate learning as much as possible. 
One the most desirable characteristic to take into 
account in learning is the rhythm the student is able 
to learn. Thus, the ITS has to adapt the rhythm in 
which it introduces the concepts to the learning 
rhythm of each student (for instance, to show more 
or less exercises, to show more or less tests, etc.). 
Another aspect widely considered in learning theory 
is reinforcement by rewarding a correct answer and 
penalizing the errors (by means of messages, sounds, 
etc.).
The second goal in our environment is to 
enhance teaching in the same way as learning. One 
of the main problems a professor faces when 
teaching is that he does not know the skills of his 
alumni. Our proposal leads to conclusions that 
“teach how to teach”. Within this objective there is 
the need to make the matter more comprehensive for 
the overall alumni, but always keeping in mind the 
requisites given to the subject. 
5 ARCHITECTURE OF THE ITS 
The general architecture of our ITS (see figure 3) is 
formed by the three components that characterize an 
ITS, as explained before – the Student Model, the 
Domain Model, and the Education Model. In the 
Domain Model four agents have been added to 
provide the system of a user-centred adaptivity 
capacity.
In the Student Model the knowledge the system 
has about the student (profile and interaction with 
the system) is represented. The model is composed 
of three knowledge databases (KDBs). (1) The 
Personal Information KDB stores the necessary 
personal information of the student to control his 
access to the system. (2) The Profiles KDB stores 
the level as well as the presentation styles of the 
students. The students are assigned different levels 
depending on their learning rhythm. (3) The 
Learning KDB stores parameters such as the 
exercises and tests proposed so far to the students, 
the time spent on answering the questionnaires, the 
pages of theory visited and the scrolls performed on 
those pages, or the reinforcement material prepared 
by the Pedagogic Module.  
In the Domain Model the knowledge about the 
contents to be taught is stored. This model consists 
of four KDBs: (1) the Theory KDB incorporates the 
pages of theory that have been prepared for teaching  
the matter, (2) the Tests Questionnaire KDB stores 
the battery of test questions related to the matter, (3) 
the Exercises KDB stores the battery of exercises on 
the matter, and, (4) the Reinforcement KDB contains 
the information used by the Pedagogic Module to 
prepare the material to be shown when a student 
needs to be reinforced.  
Distance Learning by Intelligent Tutoring System 
251
Figure 2: Decomposition of the matter.

The Pedagogic Module provides the necessary 
mechanisms to efficiently present the matter to the 
student. This module is in charge of carrying out 
three tasks: (1) to provide the learning guidelines for 
the student (including any necessary reinforcement 
provided by the system), (2) to update statistics in 
the Domain Model of the exercises and tests 
presented, (3) to store into the Learning KDB 
important data such as the material prepared to 
reinforce the student who needs it, the responses 
given by the student to the exercises and tests 
proposed, as well as the scores that the student has 
gotten and the time that he has spent in reaching the 
aims.  
The Preferences Agent supervises the user 
preferred style of presentation (type and size of 
letter, colors, margins, and so on). When the user 
changes his style of presentation the Preferences 
Agent creates a personalized sheet of styles for the 
user and updates the user's interface in accordance 
with his new pleasures. The information that this 
agent gathers is stored in the Profiles KDB. The 
Accounting Agent observes the student interaction 
with the interface when the pupil accesses a page of 
theory. When the student changes to another page of 
theory, the Accounting Agent stores in the Learning 
KDB some valuable information (the name of the 
visited page, the time that the student has spent on it 
and the scrolls performed on it). The Exercises 
Agent takes charge of choosing the exercises that 
will be proposed to the student in the topic that he is 
currently studying. This agent stores the chosen 
exercises in the Learning KDB as well. In the same 
way, the Tests Agent is in charge of choosing the 
test questions that will compose a test questionnaire 
proposed to the student in the topic that he is 
studying at this moment. The test questions selected 
are also stored in the Learning KDB. The Exercises 
Agent and the Tests Agent do the selection when the 
student finishes the first visit to the first page of 
theory of every topic. We may highlight that the 
Exercises Agent and the Tests Agent are proactive 
because they carry out their tasks in parallel with the 
activity that the student performs. Indeed, the 
student is reading theory without realizing the work 
of both agents.
Lastly, the Education Model provides the 
functionality that the teacher of the system needs. 
Across this module the teacher changes his 
preferences, gives reinforcement to the students, 
obtains statistics and consults the matter. This model 
is in fact devoted to help the teacher to change the 
contents of the matter on the basis of the information 
obtained from the Student Model and the Domain 
Model.
Antonio Fernández-Caballero et al.
252
Figure 3: Architecture of the agent-based ITS system.

6 DESIGN OF THE AGENTS 
As it may be observed in figure 4, agents have been 
implemented as applets. 
6.1 Preferences Agent 
The Preferences Agent supervises the style of 
presentation that the user likes. The Preference 
Agent perceives the interaction of the student with 
the user interface and acts when he changes his 
tastes. The preference agent is continually running to 
know the student’s preferences at any time. 
The process that follows when the user decides 
to change his visual preferences is shown in figure 5 
as an activity diagram for activiy “Change 
preferences”. When the student decides to “Change 
preferences”, the Preferences Agent shows him a 
form with the preferences that he has selected up to 
this moment. This way the user can perform the 
changes when he considers that are appropriate.  
After having completed the form, the new 
selected preferences are updated and an example 
page is shown to the student with all the features of 
the new selected style of presentation. If the student 
does not like the page, he may continue changing his 
preferences. 
Distance Learning by Intelligent Tutoring System 
253
Figure 4: Agent class diagram.
Figure 5: Activity diagram for “Change preferences.”

6.2 Accounting Agent 
The Accounting Agent perceives the interaction 
between the student and the user interface and acts 
(gets information) when the student changes to 
another page of the ITS, scrolls up and/or down a 
page, performs an exercise or a test, and so on.  
Let us focus on the Accounting Agent when 
watching the interaction of the student with the 
interface in theory pages. Here, more concretely, the 
agent is in charge of watching the scroll that the 
student performs on a page of theory as well as the 
time that he has remained in that page. When the 
student leaves studying a page of theory, the 
Accounting Agent stores all parameters gathered 
during this time (scroll and time of permanence) in 
the database.  
In figure 6 the algorithm to detect the scroll that 
the student performs when he visits a page of theory 
is shown. Once the student has entered a theory 
page, he may advance in his reading or go back in 
the page. Whilst the student is advancing through the 
page, the value of  “Greatest advance” is being 
updated. Now, when he steps back the value of 
“Greatest backward” is updated. Notice how all 
steps are stored in the database as “Scroll History”. 
6.3 Exercises Agent 
The Exercises Agent is in charge of choosing the 
exercises that will be proposed to the student in the 
topic that he is currently studying. The Exercises 
Agent is autonomous as it controls its proper actions 
in some degree. The agent, by its own means (pro-
active), selects the set of exercises to be proposed in 
the subject studied by the student and adds to each 
exercise the links to the theory pages that explain the 
concepts (or topics) related to the exercise. When 
solicited, it sends the page containing the exercises 
to be proposed.  
Antonio Fernández-Caballero et al.
254
Figure 6: Activity diagram for “Detection of scroll.”

Figure 7: Exercises Agent state diagram
As it may be observed in figure 7, the Exercises 
Agent state diagram, when the student has just 
visited for the first time the first page of a topic, the 
Exercises Agent shows the selection of exercises 
that will be proposed to the student for the topic. If 
the student is a level-1 student (low level student), 
the agent selects the more basic exercises (state 
“Elaborate basic exercises”) and later on the more 
complex 
exercises 
(state 
“Elaborate 
complex 
exercises”). Now, if the student is a level-2 student 
(high level student), the agent is only allowed to 
select the complex exercises. Once the agent has 
selected the exercises it will remain inactive (in an 
“Idle” state) while the student does not go on to the 
following topic.  
6.4 Tests Agent 
Similarly, the Tests Agent is in charge of choosing 
the test questionnaires that will compose the test that 
will be proposed to the student in the topic that he is 
studying. The Tests Agent is also waiting until it is 
asked for tests questionnaires pages. The agent by its 
own means (pro-active) goes on designing a set of 
tests for the subject the student is engaged in. 
As you may observe in figure 8 – the Tests 
Agent state diagram -, the Tests Agent performs the 
selection of test questionnaires at the same time that 
the Exercises Agent performs the selection of 
exercises. 
Once 
it 
has 
selected 
the 
test 
questionnaires, the agent will remain inactive (“Idle” 
state), while the student does not go on to the next 
topic.
7 CONCLUSIONS  
In this paper we have proposed an architecture that 
considers the high diversity of users’ skills and 
preferences: a user-centred and adaptive interaction 
multi-agent system. Our model proposed has been 
applied to e-learning/e-teaching by taking advantage 
of the current state of the art of ITS. A way to insert 
user adaptivity into an ITS is by using agent 
technology. This is due to the characteristics that 
intelligent agents possess – autonomy, social ability, 
Distance Learning by Intelligent Tutoring System 
255
Figure 8: Tests Agent state diagram.
In this sense, in our distance learning system we 
have introduced a Student Model, a Domain Model, 
reactivity and pro-activity. I this article, we have 
introduced an agent-based ITS architecture that 
enables a better learning to the students and a better 
teaching to the professors. 
and an Education Model. In this latter model four 

agents – the Preferences Agent, the Accounting 
Agent, the Exercises Agent and the Tests Agent - 
have been proposed. To conclude, the multi-agent
 
system described in the paper gets data obtained 
from the profiles to adequate the contents shown to
 
the concrete student that accesses the distance
 
learning ITS. On the other hand, the multi-agent
 
system obtains measures that permit to get 
recommendations to enhance the course. This way, 
jointly e-learning and e-teaching are greatly 
enhanced.  
ACKNOWLEDGEMENTS
This work is supported in part by the Spanish Junta 
de Comunidades de Castilla-La Mancha PBC-03-
003 and the Spanish CICYT TIN2004-08000-C03-
01 grants. 
REFERENCES
Baldoni, M., Baroglio, C., Patti, V., 2004. Web-based
 
adaptive tutoring: an approach based on logic agents
 
and reasoning about actions. Artificial Intelligence
 
Review, 22(1):3-39.  
Bello, P., Bringsjord, S., 2003. HILBERT & PATRIC: 
Hybrid intelligent agent technology for teaching
 
context-independent 
reasoning. 
 
Educational
 
Technology & Society, 6(3):30-42. 
Capuano, N., Marsella, M., Salerno, S., 2000. ABITS: An
 
agent based Intelligent Tutoring System for distance
 
learning. Proceedings of the International Workshop 
on Adaptive and Intelligent Web-Based Education
 
Systems, ITS 2000. 
Cheikes, B.A., 1995. GIA: An agent-based architecture for
 
Intelligent Tutoring Systems. Proceedings of the
 
CIKM'95 
Workshop 
on 
Intelligent 
Information
 
Agents. 
de Antonio, A., Imbert, R., Ramirez, J., Mendez, G., 2003
. 
An agent-based architecture for the development of
 
intelligent virtual training environments. Second
 
International Conference on Multimedia and ICTs in
 
Education, m-ICTE 2003.  
Dorça, F.A., Lopes, C.R., Fernández. M.A., 2003. A 
multiagent architecture for distance education systems. 
Proceedings of the 3rd IEEE International Conference 
on Advanced Learning Technologies, ICALT’03, page 
368.
Franklin, S., Graesser, A., 1996. Is it an Agent, or Just a 
Intelligent Agents III, Agent Theories, Architectures, 
and Languages, ECAI '96 Workshop (ATAL), Lecture 
Notes in Computer Science 1193:21-35. 
Frigo, L.B., Pozzebon, E., Bittencourt, G., 2004. O papel 
dos 
agentes 
inteligentes 
nos 
sistemas 
tutores 
inteligentes. World Congress on Engineering and 
Technology Education, page 86. 
Hospers, M., Kroezen, E., Nijholt, A., op den Akker, 
H.J.A., Heylen, D., 2003. An agent-based intelligent 
tutoring system for nurse education. In Applications of 
Intelligent Agents in Health Care, J. Nealon and A. 
Moreno (eds), pages 143-159.  
Kinshuk, Han, B., Hong, H., Patel, A., 2001. Student 
adaptivity in TILE: A client-server approach. 
Proceedings of IEEE International Conference on 
Advanced Learning Technologies, ICALT2001, pages 
297-300.
Millán, E., Agosta, J.M, Pérez J.L., 1999. Application of 
bayesian networks to student modelling. Proceedings 
of 
PEG'99: 
Intelligent 
Computer 
and  
Communications Technology: Teaching & Learning 
for the 21st Century.   
Mota, D., Oliveira, E., Mouta, F., 2004. MyClass: A Web-
based system to support interactive learning in virtual 
environments. Workshop on Modelling Human 
Teaching. Tactics and Strategies.   
Peña, C.I., Marzo, J.L., de la Rosa, J.L., 2002. Intelligent 
agents in a teaching and learning environment on the 
Web. Proceedings of the 2nd IEEE International 
Conference on Advanced Learning Technologies, 
ICALT2002.
Person, N.K., Graesser, A.C., and the Tutoring Research 
Group, 2000. Designing AutoTutor to be an effective 
conversational 
partner. 
Fourth 
International 
Conference of the Learning Sciences, pages 246-253. 
Shaw, E., Ganeshan, R., Johnson, W., Millar, D., 1999. 
Building a case for agent-assisted learning as a 
catalyst for curriculum reform in medical education. 
Proceedings of the International Conference on 
Artificial Intelligence in Education, pages 509-516.  
Tang, T.Y., Wu, A., 2000. The implementation of a multi-
agent intelligent tutoring system for the learning of 
computer programming. Proceedings of 16th IFIP 
World Computer Congress-International Conference 
on 
Educational 
Uses 
of 
Communication 
and 
Information Technology, ICEUT 2000.  
Pesty, S., Webber, C., 2004. The Baghera multiagent 
learning environment: an educational community of 
artificial and human agents. Upgrade, Journal of 
CEPIS (Council of European Professional Informatics 
Societies), 4:40-44. 
Antonio Fernández-Caballero et al.
256
Program?: A Taxonomy for Autonomous Agents. 

A CONTROLLED EXPERIMENT FOR MEASURING THE 
USABILITY OF WEBAPPS USING PATTERNSi
F. Javier García, María Lozano, Francisco Montero, Jose Antonio Gallud and Pascual González 
Computer Science Department,University of Castilla-La Mancha, Albacete, Spain 
Email: fcoj.garcia3@alu.uclm.es, [mlozano, fmontero, jgallud, pgonzalez] @ info-ab.uclm.es 
Carlota Lorenzo 
Marketing Department, Universitity of Castilla-La Mancha, Albacete, Spain 
Email: carlota.lorenzo@uclm.es  
Keywords: 
Internet services, Dial-up networking. 
Abstract:  
Usability has become a critical quality factor of software systems in general, and especially important 
regarding Web-based applications. Measuring quality is the key to developing high-quality software, and it 
is widely recognised that quality assurance of software products must be assessed focusing on the early 
stages of the development process. This paper describes a controlled experiment carried out in order to 
corroborate whether the patterns associated to a quality model are closely related to the final Web 
application quality. The experiment is based on the definition of a quality model and the patterns associated 
to its quality criteria to prove that applications developed using these patterns improve its usability in 
comparison with other ones developed without using them. The results of this experiment demonstrate that 
the use of these patterns really improves the quality of the final Web application in a high degree. The 
experiment is formally based on the recommendations of the ISO 9126-4.  
1 INTRODUCTION 
Usability has become a critical quality factor of 
software systems in general, but with the increase 
important in web-based applications. Usability is
 a
 
key factor for users to decide whether or not a web 
application (WebApp) is satisfying.  
Thus, it is important to design WebApps with a 
basic level of quality in general and usability in 
particular, and also developing methods which allow 
metrics have been defined (Ivory, 2001), (ISO, 
2001), which can help us to measure the final 
usability or quality in use of a web application. 
Lots of efforts have been made in order to 
improve the quality of software systems, such as 
definition of quality metrics (Olsina, 1999), usability 
metrics (Ivory, 2001), usability evaluation methods 
(Nielsen, 93), (Constantine et al., 2000), etc. All 
these mechanisms allow us to check the usability of 
a software system but they have to be used on a final 
and running application. Nowadays, it is widely 
recognised that quality assurance of software 
products must be assessed focusing on the early 
stages of the development process. It is necessary to 
incorporate new mechanisms at the very beginning 
of the software process to produce high-quality 
software applications. In this sense, the use of design 
patterns in general has proved to be good for these 
purposes, especially interaction patterns regarding 
usability.  
Some interaction patterns have been proposed in 
the literature, but the novelty of our approach is to 
establish a clear association between a quality factor 
defined in a quality model and one or more concrete 
patterns in such a way that the use of that pattern in 
the construction of a WebApp makes it to assess the 
corresponding quality criteria.  
To validate these associations and the goodness 
of using interaction patterns to satisfy the 
corresponding 
quality 
factors, 
a 
controlled 
experiment has been carried out. 
This paper is organized as follows: firstly, we 
present a general idea about web quality, usability 
and patterns, to have a global vision of the frame 
work where this study is included and the related 
work. 
257
C.-S. Chen et al. (Eds.), Enterprise Information Systems VII, 257–264.
© 2006 Springer.
use of the Internet for everyday activities, it is espec-
ially 
for evaluating this quality. In this sense, different Web

Then we describe the experiment that we have 
carried out to prove the importance of using patterns 
and quality models to design quality WebApps, as 
well as the use of different metrics as a mechanism 
for evaluating usability. 
Finally, we finish the paper with interesting 
conclusions based on the experiment results and 
future research aspects are proposed. 
2 RELATED WORK  
Before describing the experiment carried out for this 
study is necessary to talk about web quality and web 
usability to establish the aim and the context of the 
experiment. 
Brajnik states  (Brajnik, 2002) that the quality in 
this context is a property of a website defined in 
terms of a group of attributes, like consistency of 
background colours or average download time.  ISO 
9126 defines web quality dividing it into more 
abstract terms, as effectiveness, efficiency and 
satisfaction. (ISO, 2001). 
Because of the variety and quantity of quality 
attributes proposed in the literature, it is necessary to 
develop a quality model that helps to know which 
attributes are important for the analysis, which one is 
more important than others, and which measurement 
methods have to be used to assess the attributes 
values. 
Usability is widely recognized as one of the most 
important attributes regarding Web quality, so we 
focus on usability to define a complete quality 
model (see figure 1) and define an accurate 
association between the final and more concrete 
criteria with one or more interaction pattern. The 
experiment aims to prove the goodness of the quality 
model and the interaction patterns associated.  
The model is centred specifically on usability 
criteria as regarding web environments this factor is 
more meaningful than others because of the new 
interactive features of internet. 
This quality model (Montero et al. 2003) centred 
on usability is based on usability features defined by 
ISO 9126, some ergonomic criteria and its sub-
criteria. These ergonomic criteria are implemented 
by means of patterns, as a way to materialize the 
abstract concept of a quality criterion with 
something implementable as it is a pattern.  
Pattern concept in computer science in this 
context is defined as a tuple of three elements: a 
problem, a context and a solution. Many patterns 
have been proposed for web development (Percel et 
al.,1999), (Tidwell, 2002), (Welie, 2003), (Van 
Duyne et al., 2002), (Rossi et al., 12), or (Montero et 
general terms and web quality model we base the 
experiment on, we continue this paper describing the 
case study about assessing the usability of an e-
commerce WebApp by using patterns and the 
complete description of the experiment. 
3 DESCRIPTION OF THE 
EXPERIMENT
The description of the experiment follows the 
structure and recommendations of ISO 9126-4. 
3.1 Goals 
The website that we use for the experiment is a 
fictitious on-line store named e-fashion in order to 
eliminate the effects of prior experience. This site is 
an e-commerce WebApp where you can buy men 
and women clothes.  
The content of an online shop is based on a 
homepage (Nielsen, 2001) which includes the same 
links and websites as other online apparel stores. 
We developed six different versions of this site 
according to different quality patterns we wanted to 
validate.
The main goal of the experiment is to prove that 
the association established between interaction 
patterns and the criteria defined on the proposed 
quality model (Montero et al. 2003) is correct (see 
figure 1), in such a way that the use of the patterns in 
the construction of a WebApp makes it to assess the 
corresponding quality criteria, and for this reason the 
WebApp versions that includes the recommended 
patterns for solving usability problems have more 
quality that the ones that do not include them. In this 
case we carried out the experiment choosing only 
one of the quality model criteria: “Guide”. The same 
method could be used with the rest of quality factors 
and patterns related. 
We had to evaluate the websites designed for the 
experiment using usability metrics to validate the 
hypotheses. 
F. Javier García et al. 
258
al., 2002b) can be cited. Once we have seen the 

3.2 Method 
To do the experiment for each feature of the criteria 
“Guide” represented in the quality model we based 
on, we had to design six versions of e-fashion. Thus, 
we had to choose six different groups of people, 
with the same experience on the use of internet and 
buying in electronic shops. 
Each group had to do four tasks, and then we had 
to measure them with usability metrics to reach the 
goals of the experiment. 
Each version of e-fashion was made using the 
patterns that are recommended according to the 
features of the quality criteria “Guide” of our quality 
model (Montero et al. 2003). We wanted to measure 
if the association established between the patterns 
and this features is correct and their use improves 
the usability of the WebApp generated.  
The sub-criteria defined for the criteria “Guide” 
are the following: 
-Prompting 
-Grouping
-Immediate feedback 
-Legibility 
The patterns, defined by D. Van Duyne (Van 
Duyne et al. 2002), associated to each sub-criteria 
are the following: 
- Prompting:
D3: Headlines and blurbs 
D9: Distinctive HTML titles 
G1: Featured products 
G2: Cross-selling and up-selling  
H6: Pop-up windows 
H8: Context-sensitive help  
K6: Location bread crumbs  
- Grouping:
B3: Hierarchical organization 
B4: Task-based organization 
B5: Alphabetical organization 
B6: Chronological organization 
B7: Popularity-based organization
D1: Page templates 
D7: Inverse-pyramid writing style 
F1: Quick-flow checkout  
G1: Featured products 
H7: Frequently asked questions 
J3: Organized search results 
K1: Navigation bar 
- Immediate feedback:
C2: Up-front value proposition 
D9: Distinctive HTML titles 
F3: Shopping cart   
F7: Order summary  
A Controlled Experiment for Measuring the Usability of Webapps Using Patterns 
259
Figure 1: Web Quality Model.

F8: Order confirmation and thank-you 
H6: Pop-up windows  
I3: Clear first reads  
K5: High-visibility action buttons  
K10: Obvious links 
K14: Page not found  
- Legibility:
D7: Inverse-pyramid writing style 
D9: Distinctive HTML titles 
I2: Above the fold 
I3: Clear first reads 
I4: Expanding width screen size 
I5: Fixed-width screen size 
Taking into account these associations, we 
implemented the six different websites of e-fashion: 
one version using all the patterns (e-fashion 3), 
another one using any of them (e-fashion 4), another 
one using only the patterns defined for the sub-
criteria Prompting (e-fashion 5), another one using 
only the patterns defined for the sub-criteria 
Grouping (e-fashion 6), another one using only the 
patterns defined for the sub-criteria Immediate 
Feedback (e-fashion 7), and the last one using only 
the patterns defined for the sub-criteria Legibility (e-
fashion 8). 
3.2.1 Participants 
The selection of participants was not easy as they 
should not be experts on using internet and buying 
through websites. 
Finally the participants selected to carry out the 
experiment were high school students between 16 
and 17 years. All of them had the same experience 
on using internet and no experience on buying in 
internet. There were only 2 users that had previously 
bought something in internet but just once. 
3.2.2 Context of Product Use in the 
Experiment
3.2.2.1 Tasks 
The proposed tasks that each user had to do on the 
experiment were designed according to the factor 
“Guide” that we wanted to evaluate and considering 
that the WebApps designed for the experiment were 
e-commerce sites. 
The tasks each user had to do were the 
following:
- Task 1: To buy two grey men jerseys. 
- Task 2: To write the price of a white 
woman shirt with black stripes 
- Task 3: To add to the shopping chart four 
pairs of uncovered woman shoes and a 
white man belt. 
- Task 4: To buy a brown woman skirt. 
After the execution of these tasks the users had to do 
the satisfaction test proposed in the experiment. 
With the results obtained using metrics we got 
conclusions about the use of patterns to create usable 
websites, as described afterwards. 
3.2.2.2 Context Used for the Experiment 
The evaluation was made on the Albacete high 
school called CEDES, in Spain, on June the 15th and 
16th of 2004. 
The participants were constantly observed by the 
person in charge of the experiment during the whole 
time.  
3.2.2.3 Participant’s Computing Environment 
All the participants used the same computer 
machines and the same internet connection. The 
computers used were Pentium MMX with 32 MB of 
RAM, with 15” monitors and a screen resolution of 
800x600. The operating system was Windows 2000. 
The internet connection was 150 kb/s ADSL, but 
shared by all the machines. This was determining in 
the development of the experiment, because the 
loading of the pages was very slow, because of the 
high quantity of images shown in the web. 
3.2.3 Experiment Design 
Six groups of participants were established, with a 
media of 12 users per group. Thus, the total amount 
did the proposed tasks in one of the versions of e-
fashion. Each user had to do the 4 proposed tasks 
and to fill the satisfaction test. 
3.2.3.1 Procedure 
When the participants arrived at the laboratories 
where the experiment was carried out, all of them 
were informed about the goals of the experiment at 
the same time. We told them that the experiment 
was made to measure the usability of the website e-
fashion where they had to do the proposed tasks to 
find out whether it met the needs of users as them. 
They were told to read each task (described on a link 
on the home page of e-fashion) and to make these 
tasks one by one. Finally they had to fill the 
satisfaction test available by a link on the home 
page. 
The experiment was about 50 minutes long for 
each one of the six groups. In this time the users had 
to be able to execute the four proposed tasks and the 
satisfaction test. 
The participants were given basic instructions 
describing the environment. The evaluator reset the 
state of the computers before the coming of each 
new users group, and gave them the appropriate and 
F. Javier García et al. 
260
of participants was 74 people. Each group of users 

convenient instructions. The participants also were 
informed that they could not be helped by the 
evaluator, because the web was enough to help the 
users to make the tasks properly. 
Any incident occurred during the experiment was 
solved by the evaluator in the most properly way. 
The evaluator finally asked the participants about 
the difficulties they had encountered to have a best 
vision about the results of the experiment. 
3.2.4 Metrics 
The metrics we used on the experiment were the 
following:
- Efficiency: We use as efficiency metric the 
“Task Time”, which is the time that each user spends 
completing each task. We did the mean time for all 
users on each task.
- Effectiveness: The effectiveness was measured 
using the metric “Task Completion” and “Error 
frequency”. With these metrics we can obtain the 
number of tasks that users did not completed and the 
number of errors the users had made on each task. 
In this case we considered as completed task the 
one where the user had done what we asked to do, 
independently of if he did it properly or not. 
As well as the number of completed and not 
completed tasks, we showed the results as a 
percentage to allow a simple analysis.   
One task is not completed for example if we 
asked to buy something and the user has added the 
products to the shopping chart but he has not paid 
for them. 
The results of the metric “Error Frequency”
allow us to evaluate the errors that each user has 
made. If one of the task was to buy a brown skirt, 
but the user have bought another issue, it will be 
considered that the task is completed but with errors. 
If the task consisted on taking note of the price 
of a certain shirt and this price was written from 
other shirt, then the task will be considered 
completed but with errors. 
Another possible way to have contemplated this 
metrics would have been to consider that the task 
with errors it is not completed. 
- Satisfaction: The satisfaction metric was 
measured using a satisfaction test, that was created 
based on some questions of the evaluation test 
SUSS, developed by Constantine (Constantine et al., 
1999) to measure key elements in the interfaces  
design as could be personal tastes, aesthetic, 
organization, understandability, and learning. 
Other questions of the test are based on SUMI 
test, accepted by expert evaluators and international 
organizations as ISO. 
Our test is about 20 questions that allow us to 
evaluate in a simple way the satisfaction of the users 
who have carried out the four tasks in the website. 
Each user had to answer each question of the test 
choosing an option between 1 and 5, like a Likert 
scale, except in the two first questions and the three 
final questions that were questions with a scale of 
three points.  
- Comprensibility and learning: Finally the 
metric learning was measured. This kind of metric 
measures how the user have learned to use the 
website.  To measure this metric we use a special 
question of the satisfaction test and we compared 
also the “Task Time” of task 1 and task 4, because 
these two tasks consisted on buying something. 
Thus, it was supposed that if the user had been able 
to learn, the Task Time for Task 4 would be inferior 
to Task Time for Task 1, as indeed occurred.  
3.2.5 Results 
Before comparing the data obtained by the metrics 
in the different versions of e-fashion we can say that 
the association established between the interaction 
patterns and the feature Guide in general is good and 
improves notably the quality of websites. 
Special case was e-fashion 5 because the server 
felt down for some minutes and some tasks could 
not be completed, and some Task Times were higher 
than expected, so the results of e-fashion 5 were 
taken carefully taking into account this unexpected 
situation.
3.2.5.1 Comparative of Medium Task Time 
Figure 2 shows a graphic where the medium times of 
task are represented for each version of e-fashion. 
We highlight the fact that it occurs something 
curious on the versions of e-fashion that use all the 
patterns of the criteria “Guide”, or some of them. 
These sites had more information to load, for 
example more quantity of images, than e-fashion 4, 
the “worst” version of all. For this reason the 
loading time of these pages was slower than the 
loading time on e-fashion 4. 
A Controlled Experiment for Measuring the Usability of Webapps Using Patterns 
261

0,00
5,00
10,00
15,00
20,00
25,00
1
2
3
4
Task
Time (min.)
e-fashion 3
e-fashion 4
e.fashion 5
e-fashion 6
e-fashion 7
e-fashion 8
This workload that made the loading slower was 
due to products images, for example on the home 
page that loads the new products of the month, and 
this could affect to the final time task. 
Despite this situation, on the Figure 2 we can see 
that in general, the users take more time to execute 
the tasks on e-fashion 4 than on e-fashion 3, as we 
expected. If we add the handicap that has been 
talked about, we have to take into account the times 
in function of the load of the page. In this case there 
is no doubt that e-fashion 4 is the worst of all 
versions of the sites, and e-fashion 3 is the best of 
the six versions because its task times are slower on 
3 of the 4 tasks. 
These results allow us to conclude that the 
association between the patterns with the criteria 
“Guide” is good in general and helps to improve the 
quality of websites. 
Each colour in Figure 2 represents each version 
of e-fashion that was used on the experiment. The 
X-axis shows the number of task and the Y-axis 
shows the time expressed in minutes. 
3.2.5.2 Comparative of Task Completion 
Figure 3 shows a graphic where the completion of 
tasks is compared on the six versions of e-fashion. 
We want to remind that when the experiment was 
made with e-fashion 5 the server felt down, and 
some tasks were not completed. So, this 23% of 
users that had not completed the tasks on e-fashion 5 
do not reflect the real usability of the web. 
If we analyse the rest of groups, we can see that 
on e-fashion 4 there was a 90% of uncompleted 
tasks, what shows that this web was the worst of the 
six versions, because in e-fashion 7 there were a 2 % 
of users that did not complete all the tasks, but it is a 
better result than e-fashion 4 and in the rest of the 
versions there was a 100% of task completed. 
3.2.5.3 Errors Frequency comparative 
Figure 4 shows which version of e-fashion had more 
error frequency on its tasks, and as we expected e-
fashion 4 was the worst site of the six. 
This result reinforces the hypothesis that it is 
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
e-fashion 3 e-fashion 4 e-fashion 5 e-fashion 6 e-fashion 7 e-fashion 8
F. Javier García et al. 
262
Figure 2: Medium Task Time Comparative. 
Figure 3: Task Completion Comparative. 

much better to design websites according to a 
quality model and using the ergonomic patterns 
identified.
3.2.5.4 Satisfaction comparative 
Now we show some of the results obtained from the 
satisfaction test bringing face to face the different 
versions of e-fashion showing some comparative 
graphics. 
We start with the affirmation “Working with this 
site is satisfactory”, because this question shows the 
general satisfaction of the users that have used the 
web application. In general, the satisfaction of the 
users was higher on e-fashion 3, because the most 
chosen option was number 5, which is the best of the 
different options. This shows that in general the 
satisfaction of the users that executed the tasks of the 
experiment was good.  
However, on e-fashion 4 the most chosen option 
was number 2, which is near to the worst option 
(Totally disagree), which indicates that in general, 
on e-fashion 4 the satisfaction was the worst. 
Another interesting question of the test was 
number 4: “The website is very attractive for me”,
because it shows the capacity of the website to 
attract users. In this case the results were also 
favourable to e-fashion 3. 
In general the results of e-fashion 3 were 
positives, if we have into account the percentages of 
the different versions. E-fashion 4 again was the 
worst of the six versions. 
The conclusions that we obtained about the rest 
of questions in the test were favourable to e-fashion 
3 too. This indicates again that using patterns for 
designing websites is good to improve their final 
quality.
3.2.5.5 Learning comparative 
In this case as we could see in the answers of 
question number 21 of the satisfaction test, in the 
notes taken by the evaluator while the experiment 
was carried out, and comparing the times obtained 
on the tasks where the users had to buy something 
(number 1 and number 4), the users proved to have 
understood the functionality of the website and have 
learned the basic use of it. Again this result was 
clearer on e-fashion 3, the version that was 
implemented using all the patterns defined for the 
criteria “Guide”. 
4 CONCLUSIONS AND FUTURE 
WORKS
Based on the interesting results obtained from the 
experiment, we can conclude that the use of 
ergonomic patterns to characterize quality criteria 
defined in a quality model produces better web 
applications with much more quality than others 
implemented without using the patterns. 
Moreover, the realization of the experiment has 
been useful to evaluate the metrics used on it. The 
metrics are a useful mechanism for evaluating the 
usability of websites. 
With the data obtained from the experiment we 
can conclude that the satisfaction metric is useful 
because it shows a good view about what users think 
about when they use the website on true conditions. 
However, we can also state that due to the subjective 
nature of this metric, and because we cannot assure 
that all the users say the truth when they fill a test, is 
important not to use only this metric to evaluate the 
quality of a website. In this sense, it is better to 
compare its results with the results obtained by using 
other metrics, as for instance, Task Time, Error 
case as the global results of the metrics used are 
0%
5%
10%
15%
20%
25%
30%
35%
40%
e-fashion
3
e-fashion
4
e-fashion
5
e-fashion
6
e-fashion
7
e-fashion
8
A Controlled Experiment for Measuring the Usability of Webapps Using Patterns 
263
Frequency and so on.
The validity of the experiment is clear in this 
Figure 4: Errors Frequency Comparative.

consistent one with the others, 
Another interesting conclusion is that “Task 
Time” and “Error Frequency” metrics are useful 
when we need to evaluate the quality of a website, 
because they show in an objective way the usability 
of the system when users execute tasks on it.  
However, in the experiment we can see that it is 
very important when we want to evaluate the results 
of the metric “Task Time” the loading time of the 
page, because the loading time can give a false view 
about the time users spend on doing the tasks. Thus, 
it is very important that all the users can carry out 
the tasks on computers with the same technical 
specifications. About the “learning” metric we must 
say that it is also a good metric because it gives a 
general idea about the capacity of the website to be 
learned and used by the user. 
So, applications easier to learn are more 
satisfactory for the user as we can conclude from the 
experiment results. The most important conclusion is 
that the use patterns to characterize the criteria 
defined in a quality model allow us to construct web 
applications with a higher degree of quality than 
others designed without any reference model nor 
patterns.
As a final remark, we can state that due to the 
complexity and difficulty of carrying out an 
experiment with real people it is much better to 
develop web quality models which allow developing 
websites with a basic level of usability. The design 
websites using a quality model and interaction 
patterns associated to the different criteria avoids 
making usability experiments because the final 
quality will be guaranteed. 
REFERENCES
Brajnik, G (2002). Quality models based on automatic 
webtesting. Automatically evaluating usability of Web 
Sites, Minneapolis, CHI April 2002. 
Constantine, L.L., Lockwood, L.A.D. (1999), Software for 
use, A practical guide to the models and methods of 
usage-centered design. ACM Press  
ISO/IEC 9126-1. (2001) Software Engineering – Product 
Quality- 
Part 
1: 
Quality 
Model, 
International 
Organization for Standardization, Geneva, 2001. 
ISO/IEC 9126-4. (2001) Software Engineering – Software 
Product Quality- Part 4: Quality in use metrics, 2001. 
Ivory, M.Y. (2001) An Empirical Foundation for 
Automated Web Interface Evaluation. Ph D. Thesis, 
Berkeley University, California. 
Montero, F., Lozano, M., González, P., Ramos, I. (2002) 
Designing Websites by Using Patterns. Second Latin 
American conference on Pattern Languages of 
Programming. SugarLoafPLoP02. Itaipava. Rio de 
Janeiro. Brasil. ISBN: 85-87837-07-9. pp. 209-224. 
Montero, F.; Lopez-Jaquero, V., Lozano, M.; González, P. 
(2003) A Quality Model For Testing the Usability of 
Web Sites. HCII´03. 
Nielsen, J. (1993). Usability Engineering. Morgan 
Kaufmann,
Nielsen, J. (2001). Homepage Usability: 55 websites 
deconstructed. New Riders, pp. 315 
Olsina, L. (1999) Metodología Cuantitativa para la 
Evaluación y Comparación de la Calidad de Sitios 
Web. PhD. Thesis, Universidad Nacional de La Plata, 
Argentina.
Perzel, K., Kane, D. (1999). Usability Patterns for 
Applications on the World Wide Web. PloP´99. 
Shneiderman, B (1998). Designing the User  Interface: 
Strategies for Effective Human-Computer Interaction. 
Addison-Wesley Publishers. 
Tidwell, J. (1999) Commond Ground: A pattern language 
for
 
HCI  design. http://www.mit.edu./~jtidwell/
Van Duyne, D.K.; Landay, J. A.; Hong, J. I.; (2002) The 
Design of Sites: Patterns, Principles, and Processes for 
Crafting a Customer-Centered Web Experience, 
Publisher: Addison Wesley, ISBN: 0-201-72149-X  
Welie, 
M. 
(2003) 
Interaction 
Design 
Patterns. 
http://www.welie.com/patterns.
                                                          
i This work is partially supported by the Spanish CICYT 
TIN2004-08000-C03-01 and PBC-03-003 grants.
F. Javier García et al. 
264
interaction_patterns.html

AUTHOR INDEX 
Al-Fedaghi, S.....................................157 
Aranda, G.............................................69 
Bardis, G. ...........................................103 
Barki, H..................................................3 
Bassil, S. ............................................149 
Beaudouin-Lafon, M..........................231 
Beaudoux, O. .....................................231 
Becker, J.............................................167 
Botella, F............................................249 
Brena, R. ............................................223 
Carrasco, R.........................................113 
Cechich, A. .................................. 69, 141 
Chao, C. ...............................................61 
Claypool, K..........................................77 
Cuéllar, M. ...........................................95 
Davulcu, H.........................................215 
Delfmann, P. ......................................167 
Delgado, M. .........................................95 
Dietz, J. ................................................19 
Esposito, M........................................197 
Fernández-Caballero, A.....................249 
Fu, L.....................................................87 
Fuhr, W..............................................167 
Galindo, J...........................................113 
Gallud, J.............................................257 
García, F.............................................257 
Gascueña, J. .......................................249 
González, P. .......................................257 
Greene, T. ............................................11 
Janiesch, C. ........................................167 
Karydis, I. ............................................53 
Keller, R.............................................149 
Kimble, C...........................................121 
Kostowski, H. ......................................77 
Kropf, P..............................................149 
Lazcorreta, E......................................249 
Lemos, P. ...........................................121 
Lin, J. .................................................187 
Lorenzo, C. ........................................257 
Lozano, M..........................................257 
Manolopoulos, Y. ................................53 
Markus, M............................................31 
Mazzariello, C....................................197 
Miaoulis, G. .......................................103 
Montero, F. ........................................257 
Murano, P...........................................241 
Nanopoulos, A......................................53 
Nguyen, H. .........................................215 
Oliviero, F. .........................................197 
Orlowska, M.......................................187 
Papadopoulos, A. .................................53 
Pegalajar, M. ........................................95 
Pereira, M...........................................179 
Piattini, M.....................................69, 141 
Plemenos, D. ......................................103 
Pramanik, S. .........................................45 
Qian, G.................................................45 
Ramachandran, V...............................215 
Ramírez, E..........................................223 
Reichert, M.........................................149 
Rinderle, S..........................................149 
Romano, S..........................................197 
Sadiq, S. .............................................187 
Sadiq, W.............................................187 
Sansone, C..........................................197 
Seixas, P.............................................121 
Soares, R. ...........................................179 
Tavares, L...........................................179 
Tawfik, A. ..........................................131 
Tosic, M. ............................................207 
Vasconcelos, J....................................121 
Vila, M. ..............................................113 
Vizcaíno, A. .........................................69 
Wu, Z..................................................131 
Xue, Q. .................................................45 
Zaslavsky, A.......................................207 
265


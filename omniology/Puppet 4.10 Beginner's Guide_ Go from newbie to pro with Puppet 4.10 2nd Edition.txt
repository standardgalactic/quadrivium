[ 1 ]

Puppet 4.10 Beginner's Guide
Second Edition
Go from newbie to pro with Puppet 4.10
John Arundel
BIRMINGHAM - MUMBAI

Puppet 4.10 Beginner's Guide
Second Edition
Copyright © 2017 Packt Publishing
All rights reserved. No part of this book may be reproduced, stored in a retrieval system, 
or transmitted in any form or by any means, without the prior written permission of the 
publisher, except in the case of brief quotations embedded in critical articles or reviews.
Every effort has been made in the preparation of this book to ensure the accuracy of the 
information presented. However, the information contained in this book is sold without 
warranty, either express or implied. Neither the author, nor Packt Publishing, and its dealers 
and distributors will be held liable for any damages caused or alleged to be caused directly or 
indirectly by this book.
Packt Publishing has endeavored to provide trademark information about all of the 
companies and products mentioned in this book by the appropriate use of capitals.  
However, Packt Publishing cannot guarantee the accuracy of this information.
First published: April 2013
Second edition: May 2017
Production reference: 1300517
Published by Packt Publishing Ltd.
Livery Place
35 Livery Street
Birmingham B3 2PB, UK.
ISBN 978-1-78712-400-4
www.packtpub.com

Credits
Author
John Arundel
Reviewer
Jo Rhett
Commissioning Editor
Kartikey Pandey
Acquisition Editor
Namrata Patil
Content Development Editor
Monika Sangwan
Technical Editor
Devesh Chugh
Copy Editor
Alpha Singh
Project Coordinator
Kinjal Bari
Proofreader
Safis Editing
Indexer
Tejal Daruwale Soni
Graphics
Kirk D'Penha
Production Coordinator
Arvindkumar Gupta
Cover Work
Arvindkumar Gupta

About the Author
John Arundel is a DevOps consultant, which means he helps people build world-
class web operations teams and infrastructure and has fun doing it. He was formerly a 
senior operations engineer at global telco Verizon, designing resilient, high-performance 
infrastructures for major corporations such as Ford, McDonald's, and Bank of America. He is 
now an independent consultant, working closely with selected clients to deliver web-scale 
performance and enterprise-grade resilience on a startup budget.
He likes writing books, especially about Puppet (Puppet 2.7 Cookbook and Puppet 3 
Cookbook are available from Packt as well). It seems that at least some people enjoy reading 
them or maybe they just like the pictures. He also provides training and coaching on Puppet 
and DevOps, which, it turns out, is far harder than simply doing the work himself.
Off the clock, he is a medal-winning competitive rifle and pistol shooter and a decidedly 
uncompetitive piano player. He lives in a small cottage in Cornwall, England and believes, like 
Cicero, that if you have a garden and a library, then you have everything you need. You may 
like to follow him on Twitter at @bitfield.

About the Reviewer
Jo Rhett is a DevOps architect with more than 25 years of experience conceptualizing  
and delivering large-scale Internet services. He creates automation and infrastructure  
to accelerate deployment and minimize outages.
Jo has been using, promoting, and enhancing configuration management systems for over 
20 years. He builds improvements and plugins for Puppet, Mcollective, Chef, Ansible, Docker, 
and many other DevOps tools.
Jo is the author of the following books:


Learning Puppet 4 by O'Reilly


Learning MCollective by O'Reilly


Instant Puppet 3 Starter by Packt Publishing
I'd like to thank the Puppet community for their never-ending inspiration 
and support.

www.PacktPub.com
eBooks, discount offers, and more
Did you know that Packt offers eBook versions of every book published, with PDF and ePub 
files available? You can upgrade to the eBook version at www.PacktPub.com and as a print 
book customer, you are entitled to a discount on the eBook copy. Get in touch with us at 
customercare@packtpub.com for more details.
At www.PacktPub.com, you can also read a collection of free technical articles, sign up for 
a range of free newsletters and receive exclusive discounts and offers on Packt books and 
eBooks.
https://www2.packtpub.com/books/subscription/packtlib
Do you need instant solutions to your IT questions? PacktLib is Packt's online digital book 
library. Here, you can search, access, and read Packt's entire library of books.
Why subscribe?


Fully searchable across every book published by Packt


Copy and paste, print, and bookmark content


On demand and accessible via a web browser

Customer Feedback
Thanks for purchasing this Packt book. At Packt, quality is at the heart of our editorial 
process. To help us improve, please leave us an honest review on this book's Amazon page at 
https://www.amazon.com/dp/1787124002.
If you'd like to join our team of regular reviewers, you can e-mail us at customerreviews@
packtpub.com. We award our regular reviewers with free eBooks and videos in exchange 
for their valuable feedback. Help us be relentless in improving our products!


[ i ]
Table of Contents
Preface	
xi
Chapter 1: Getting started with Puppet	
1
Why do we need Puppet anyway?	
2
Keeping configuration synchronized	
2
Repeating changes across many servers	
3
Self-updating documentation	
3
Version control and history	
4
Why not just write shell scripts?	
4
Why not just use containers?	
4
Why not just use serverless?	
5
Configuration management tools	
5
What is Puppet?	
5
Resources and attributes	
7
Puppet architectures	
7
Getting ready for Puppet	
7
Installing Git and downloading the repository	
8
Installing Virtualbox and Vagrant	
8
Running your Vagrant VM	
9
Alternative Vagrant VMs	
10
Adding Puppet to your path	
10
Troubleshooting Vagrant	
11
Summary	
11
Chapter 2: Creating your first manifests	
13
Hello, Puppet – your first Puppet manifest	
14
Understanding the code	
14
Modifying existing files	
16
Dry-running Puppet	
16
How Puppet applies the manifest	
17

Table of Contents
[ ii ]
Creating a file of your own	
18
Managing packages	
18
How Puppet applies the manifest	
19
Exercise	
19
Querying resources with puppet resource	
19
Services	
20
Getting help on resources with puppet describe	
20
The package-file-service pattern	
21
Notifying a linked resource	
22
Resource ordering with require	
22
Summary	
24
Chapter 3: Managing your Puppet code with Git	
25
What is version control?	
26
Tracking changes	
26
Sharing code	
27
Creating a Git repo	
27
Making your first commit	
28
How often should I commit?	
29
Branching	
30
Distributing Puppet manifests	
30
Creating a GitHub account and project	
31
Pushing your repo to GitHub	
32
Cloning the repo	
33
Fetching and applying changes automatically	
34
Writing a manifest to set up regular Puppet runs	
34
Applying the run-puppet manifest	
35
The run-puppet script	
36
Testing automatic Puppet runs	
36
Managing multiple nodes	
37
Summary	
38
Chapter 4: Understanding Puppet resources	
39
Files	
40
The path attribute	
40
Managing whole files	
40
Ownership	
42
Permissions	
43
Directories	
43
Trees of files	
43
Symbolic links	
44
Packages	
44
Uninstalling packages	
45

Table of Contents
[ iii ]
Installing specific versions	
45
Installing the latest version	
45
Installing Ruby gems	
46
Installing gems in Puppet's context	
46
Using ensure_packages	
47
Services	
48
The hasstatus attribute	
48
The pattern attribute	
49
The hasrestart and restart attributes	
49
Users	
51
Creating users	
51
The user resource	
52
The group resource	
53
Managing SSH keys	
53
Removing users	
54
Cron resources	
54
Attributes of the cron resource	
55
Randomizing cron jobs	
56
Removing cron jobs	
56
Exec resources	
57
Automating manual interaction	
57
Attributes of the exec resource	
57
The user attribute	
59
The onlyif and unless attributes	
59
The refreshonly attribute	
60
The logoutput attribute	
62
The timeout attribute	
62
How not to misuse exec resources	
62
Summary	
64
Chapter 5: Variables, expressions, and facts	
65
Introducing variables	
66
Using Booleans	
66
Interpolating variables in strings	
67
Creating arrays	
67
Declaring arrays of resources	
68
Understanding hashes	
69
Setting resource attributes from a hash	
70
Introducing expressions	
71
Meeting Puppet's comparison operators	
71
Introducing regular expressions	
72

Table of Contents
[ iv ]
Using conditional expressions	
72
Making decisions with if statements	
73
Choosing options with case statements	
73
Finding out facts	
74
Using the facts hash	
74
Running the facter command	
75
Accessing hashes of facts	
75
Referencing facts in expressions	
76
Using memory facts	
76
Discovering networking facts	
77
Providing external facts	
77
Creating executable facts	
79
Iterating over arrays	
80
Using the each function	
80
Iterating over hashes	
81
Summary	
82
Chapter 6: Managing data with Hiera	
83
Why Hiera?	
84
Data needs to be maintained	
84
Settings depend on servers	
84
Operating systems differ	
84
The Hiera way	
85
Setting up Hiera	
85
Adding Hiera data to your Puppet repo	
86
Troubleshooting Hiera	
87
Querying Hiera	
87
Typed lookups	
88
Writing Hiera data	
88
File header	
88
Single values	
89
Boolean values	
89
Arrays	
89
Hashes	
89
Interpolation	
90
The hierarchy	
91
Dealing with multiple values	
91
Merge behaviors	
91
Data sources based on facts	
92
What belongs in Hiera?	
93

Table of Contents
[ v ]
Creating resources with Hiera data	
94
Building resources from Hiera arrays	
94
Building resources from Hiera hashes	
95
The advantages of managing resources with Hiera data	
97
Managing secret data	
97
Setting up GnuPG	
98
Setting up hiera-eyaml-gpg	
99
Creating an encrypted secret	
100
How Hiera decrypts secrets	
101
Editing or adding encrypted secrets	
102
Distributing the decryption key	
103
Summary	
104
Chapter 7: Mastering modules	
105
Using Puppet Forge modules	
106
What is the Puppet Forge?	
106
Finding the module you need	
106
Using r10k	
107
Understanding the Puppetfile	
109
Managing dependencies with generate-puppetfile	
109
Using modules in your manifests	
110
Using puppetlabs/mysql	
110
Using puppetlabs/apache	
113
Using puppet/archive	
115
Exploring the standard library	
117
Safely installing packages with ensure_packages	
117
Modifying files in place with file_line	
118
Introducing some other useful functions	
119
The pry debugger	
121
Writing your own modules	
122
Creating a repo for your module	
122
Writing the module code	
123
Creating and validating the module metadata	
124
Tagging your module	
126
Installing your module	
126
Applying your module	
127
More complex modules	
127
Uploading modules to the Puppet Forge	
128
Summary	
129

Table of Contents
[ vi ]
Chapter 8: Classes, roles, and profiles	
131
Classes	
132
The class keyword	
132
Declaring parameters to classes	
133
Automatic parameter lookup from Hiera data	
135
Parameter data types	
137
Available data types	
137
Range parameters	
138
Content type parameters	
138
Flexible data types	
139
Defined resource types	
140
Node definitions, roles, and profiles	
141
Nodes	
142
Roles	
143
Profiles	
144
Summary	
147
Chapter 9: Managing files with templates	
149
What are templates?	
150
The dynamic data problem	
150
Puppet template syntax	
150
Using templates in your manifests	
151
Referencing template files	
151
Inline templates	
152
Template tags	
153
Computations in templates	
153
Conditional statements in templates	
154
Iteration in templates	
155
Iterating over Facter data	
155
Iterating over structured facts	
156
Iterating over Hiera data	
157
Working with templates	
158
Passing parameters to templates	
158
Validating template syntax	
160
Rendering templates on the command line	
160
Legacy ERB templates	
161
Summary	
162
Chapter 10: Controlling containers	
163
Understanding containers	
164
The deployment problem	
164
Options for deployment	
165

Table of Contents
[ vii ]
Introducing the container	
165
What Docker does for containers	
166
Deployment with Docker	
167
Building Docker containers	
167
The layered filesystem	
168
Managing containers with Puppet	
168
Managing Docker with Puppet	
169
Installing Docker	
169
Running a Docker container	
170
Stopping a container	
171
Running multiple instances of a container	
172
Managing Docker images	
173
Building images from Dockerfiles	
173
Managing Dockerfiles	
174
Building dynamic containers	
176
Configuring containers with templates	
176
Self-configuring containers	
177
Persistent storage for containers	
179
Host-mounted volumes	
179
Docker volumes	
180
Networking and orchestration	
182
Connecting containers	
182
Container orchestration	
185
What is orchestration?	
185
What orchestration tools are available?	
185
Running Puppet inside containers	
186
Are containers mini-VMs or single processes?	
186
Configuring containers with Puppet	
187
Containers need Puppet too	
187
Summary	
188
Chapter 11: Orchestrating cloud resources	
189
Introducing the cloud	
190
Automating cloud provisioning	
190
Using CloudFormation	
190
Using Terraform	
191
Using Puppet	
191
Setting up an Amazon AWS account	
191
Creating an AWS account	
192
Creating an IAM policy	
192
Creating an IAM user	
193
Storing your AWS credentials	
196

Table of Contents
[ viii ]
Getting ready to use puppetlabs/aws	
196
Creating a key pair	
196
Installing the puppetlabs/aws module	
198
Installing the AWS SDK gem	
198
Creating EC2 instances with Puppet	
199
Choosing an Amazon Machine Image (AMI)	
199
Creating the EC2 instance	
200
Accessing your EC2 instance	
201
VPCs, subnets, and security groups	
202
The ec2_securitygroup resource	
202
The ec2_instance resource	
203
Managing custom VPCs and subnets	
204
Creating an instance in a custom VPC	
204
The ec2_vpc resource	
205
The ec2_vpc_internet_gateway resource	
206
The ec2_vpc_routetable resource	
206
The ec2_vpc_subnet resource	
207
Other AWS resource types	
208
Provisioning AWS resources from Hiera data	
209
Iterating over Hiera data to create resources	
209
Cleaning up unused resources	
212
Summary	
212
Chapter 12: Putting it all together	
215
Getting the demo repo	
216
Copying the repo	
216
Understanding the demo repo	
217
The control repo	
217
Module management	
217
Nodes	
218
Roles	
219
Profiles	
219
Users and access control	
220
SSH configuration	
221
Sudoers configuration	
223
Time zone and clock synchronization	
224
Puppet configuration	
225
The bootstrap process	
228
Adapting the repo for your own use	
231
Configuring users	
231
Adding node definitions and role classes	
232

Table of Contents
[ ix ]
Modifying the bootstrap credentials	
232
Bootstrapping a new node	
232
Bootstrapping a Vagrant VM	
232
Bootstrapping physical or cloud nodes	
232
Using other distributions and providers	
233
Summary	
233
The beginning	
234
Index	
235


Preface
There are many bad ways to write a technical book. One simply rehashes the official 
documentation. Another walks the reader through a large and complex example, which 
doesn't necessarily do anything useful, except showing how clever the author is. Yet another 
exhaustively sets out every available feature of the technology, and every possible way you 
can use them, without much guidance as to which features you'll really use, or which are 
best avoided.
Like you, I read a lot of technical books as part of my job. I don't need a paraphrase of the 
documentation: I can read it online. I also don't want huge blocks of code for something that 
I don't need to do. And I certainly don't want an uncritical exposition of every single feature.
What I do want is for the author to give me a cogent and readable explanation of how the 
tool works, in enough detail that I can get started using it straight away, but not so much 
detail that I get bogged down. I want to learn about features in the order in which I'm 
likely to use them, and I want to be able to start building something that runs and delivers 
business value from the very first chapters. 
That's what you can expect from this book. Whether you're a developer, a system 
administrator, or merely Puppet-curious, you're going to learn Puppet skills you can put into 
practice right away. Without going into lots of theory or background detail, I'll show you 
how to install packages and config files, create users, set up scheduled jobs, provision cloud 
instances, build containers, and so on. Every example deals with something real and practical 
that you're likely to need in your work, and you'll see the complete Puppet code to make it 
happen, along with step-by-step instructions for what to type and what output you'll see. All 
the examples are available in a GitHub repo for you to download and adapt.

Preface
[ xii ]
After each exercise, I'll explain in detail what each line of code does and how it works, so that 
you can adapt it to your own purposes, and feel confident that you understand everything 
that's happened. By the end of the book, you will have all the skills you need to do real, 
useful, everyday work with Puppet, and there's a complete demo Puppet repository you can 
use to get your infrastructure up and running with minimum effort.
So let's get started.
What this book covers
Chapter 1, Getting started with Puppet, introduces Puppet and gets you up and running with 
the Vagrant virtual machine that accompanies this book.
Chapter 2, Creating your first manifests, shows you how Puppet works, and how to write 
code to manage packages, files, and services.
Chapter 3, Managing your Puppet code with Git, introduces the Git version control tool, 
shows you how to create a repository to store your code, and how to distribute it to your 
Puppet-managed nodes.
Chapter 4, Understanding Puppet resources, goes into more detail about the package, 
file, and service resources, as well as introducing resources to manage users, SSH keys, 
scheduled jobs, and commands.
Chapter 5, Variables, expressions, and facts, introduces Puppet's variables, data types, 
expressions, and conditional statements, shows you how to get data about the node using 
Facter, and how to create your own custom facts.
Chapter 6, Managing data with Hiera, explains Puppet's key-value database and how to use 
it to store and retrieve data, including secrets, and how to create Puppet resources from 
Hiera data.
Chapter 7, Mastering modules, teaches you how to install ready-to-use modules from the 
Puppet Forge using the r10k tool, introduces you to four key modules including the standard 
library, and shows you how to build your own modules.
Chapter 8, Classes, roles, and profiles, introduces you to classes and defined resource types, 
and shows you the best way to organize your Puppet code using roles and profiles.
Chapter 9, Managing files with templates, shows you how to build complex configuration 
files with dynamic data using Puppet's EPP template mechanism.
Chapter 10, Controlling containers, introduces Puppet's powerful new support for  
Docker containers, and shows you how to download, build, and run containers using 
 Puppet resources.

Preface
[ xiii ]
Chapter 11, Orchestrating cloud resources, explains how you can use Puppet to provision 
cloud servers on Amazon AWS, and introduces a fully-automated cloud infrastructure based 
on Hiera data.
Chapter 12, Putting it all together, takes you through a complete example Puppet 
infrastructure that you can download and modify for your own projects, using ideas  
from all the previous chapter.
What you need for this book
You'll need a reasonably modern computer system and access to the Internet. You won't 
need to be a UNIX expert or an experienced sysadmin; I'll assume you can install software, 
run commands, and edit files, but otherwise I'll explain everything you need as we go.
Who this book is for
The main audience for this book are those who are new to Puppet, including system 
administrators and developers who are looking to manage computer server systems for 
configuration management. No prior programming or system administration experience is 
assumed. However, if you have used Puppet before, you'll get a thorough grounding in all the 
latest features and modules, and I hope you'll still find plenty of new things to learn.
Conventions
In this book, you will find a number of styles of text that distinguish between different  
kinds of information. Here are some examples of these styles, and an explanation of  
their meaning.
Code words in text are shown as follows: "Puppet can manage files on a node using the  
file resource"
A block of code is set as follows:
file { '/tmp/hello.txt':
  ensure  => file,
  content => "hello, world\n",
}

Preface
[ xiv ]
When we wish to draw your attention to a particular part of a code block, the relevant lines 
or items are set in bold:
file { '/tmp/hello.txt':
  ensure  => file,
  content => "hello, world\n",
}
Any command-line input or output is written as follows:
sudo puppet apply /vagrant/examples/file_hello.pp
Notice: Compiled catalog for localhost in environment production in 
0.07 seconds
New terms and important words are shown in bold. Words that you see on the screen, in 
menus or dialog boxes for example, appear in the text like this: "In the AWS console, select 
VPC from the Services menu".
Warnings or important notes appear in a box like this.
Tips and tricks appear like this.
Reader feedback
Feedback from our readers is always welcome. Let us know what you think about this book—
what you liked or disliked. Reader feedback is important for us as it helps us develop titles 
that you will really get the most out of.
To send us general feedback, simply e-mail feedback@packtpub.com, and mention the 
book's title in the subject of your message.
If there is a topic that you have expertise in and you are interested in either writing or 
contributing to a book, see our author guide at www.packtpub.com/authors.
Customer support
Now that you are the proud owner of a Packt book, we have a number of things to help you 
to get the most from your purchase.

Preface
[ xv ]
Downloading the example code
You can download the example code files for this book from your account at http://
www.packtpub.com. If you purchased this book elsewhere, you can visit http://www.
packtpub.com/support and register to have the files e-mailed directly to you.
You can download the code files by following these steps:
1.	 Log in or register to our website using your e-mail address and password.
2.	 Hover the mouse pointer on the SUPPORT tab at the top.
3.	 Click on Code Downloads & Errata.
4.	 Enter the name of the book in the Search box.
5.	 Select the book for which you're looking to download the code files.
6.	 Choose from the drop-down menu where you purchased this book from.
7.	 Click on Code Download.
You can also download the code files by clicking on the Code Files button on the book's 
webpage at the Packt Publishing website. This page can be accessed by entering the book's 
name in the Search box. Please note that you need to be logged in to your Packt account.
Once the file is downloaded, please make sure that you unzip or extract the folder using the 
latest version of:


WinRAR / 7-Zip for Windows


Zipeg / iZip / UnRarX for Mac


7-Zip / PeaZip for Linux
The code bundle for the book is also hosted on GitHub at the following URLs:


https://github.com/bitfield/puppet-beginners-guide.git


https://github.com/bitfield/pbg-ntp.git


https://github.com/bitfield/control-repo
You can use the code bundle on GitHub from the Packt Publishing repository as well:
https://github.com/PacktPublishing/Puppet-Beginners-Guide-Second-
Edition
We also have other code bundles from our rich catalog of books and videos available at 
https://github.com/PacktPublishing/. Check them out!

Preface
[ xvi ]
Errata
Although we have taken every care to ensure the accuracy of our content, mistakes do 
happen. If you find a mistake in one of our books—maybe a mistake in the text or the code—
we would be grateful if you could report this to us. By doing so, you can save other readers 
from frustration and help us improve subsequent versions of this book. If you find any errata, 
please report them by visiting http://www.packtpub.com/submit-errata, selecting 
your book, clicking on the Errata Submission Form link, and entering the details of your 
errata. Once your errata are verified, your submission will be accepted and the errata will 
be uploaded to our website or added to any list of existing errata under the Errata section of 
that title.
To view the previously submitted errata, go to https://www.packtpub.com/books/
content/support and enter the name of the book in the search field. The required 
information will appear under the Errata section.
Piracy
Piracy of copyrighted material on the Internet is an ongoing problem across all media. 
At Packt, we take the protection of our copyright and licenses very seriously. If you come 
across any illegal copies of our works in any form on the Internet, please provide us with the 
location address or website name immediately so that we can pursue a remedy.
Please contact us at copyright@packtpub.com with a link to the suspected pirated material.
We appreciate your help in protecting our authors and our ability to bring you valuable content.
Questions
If you have a problem with any aspect of this book, you can contact us at  
questions@packtpub.com, and we will do our best to address the problem.

[ 1 ]
Getting started with Puppet
For a list of all the ways technology has failed to improve the quality of life, 
please press three.
	 	
	
	
	
	
	
	
       —Alice Kahn
In this chapter, you'll learn about some of the challenges of managing configuration on 
servers, some common solutions to these problems, and how automation tools such as 
Puppet can help. You'll also learn how to download the GitHub repository containing all the 
source code and examples in this book, how to set up your own Vagrant virtual machine to 
run the code, and how to download and install Puppet.
Whether you're a system administrator, a developer who needs to wrangle servers from time 
to time, or just someone who's annoyed at how long it takes to deploy a new app, you'll have 
come across the kind of problems Puppet is designed to solve.

Getting started with Puppet
[ 2 ]
Why do we need Puppet anyway?
Managing applications and services in production is hard work, and there are a lot of steps 
involved. To start with, you need some servers to serve the services. Luckily, these are readily 
available from your local cloud provider, at low, low prices. So, you've got a server with a 
base operating system installed on it, and you can log into it. So now what? Before you can 
deploy, you need to do a number of things:


Add user accounts and passwords


Configure security settings and privileges


Install all the packages needed to run the app


Customize the configuration files for each of these packages


Create databases and database user accounts; load some initial data


Configure the services that should be running


Deploy the app code and static assets


Restart any affected services


Configure the machine for monitoring
That's a lot to do—and the next server you build, you'll need to do the exact same things all 
over again. There's something not right about that. Shouldn't there be an easier solution to 
this problem?
Wouldn't it be nice if you could write an executable specification of how the server should be 
set up, and you could apply it to as many machines as you liked?
Keeping configuration synchronized
Setting up servers manually is tedious. Even if you're the kind of person who enjoys tedium, 
there's another problem to consider. What happens the next time you set up a server, a few 
weeks or months later? 
Your careful notes will no longer be up to date with reality. While you were on vacation, the 
developers installed a couple of new libraries which the app now depends on—I guess they 
forgot to tell you! They are under a lot of schedule pressure, of course. You could send out 
a sternly-worded e-mail demanding that people update the build document whenever they 
change something, and people might even comply with that. But even if they do update the 
documentation, no one actually tests the new build process from scratch, so when you come 
to do it, you'll find it doesn't work anymore. Turns out that if you just upgrade the database 
in place, it's fine, but if you install the new version on a bare server, it's not.

[ 3 ]
Also, since the build document was updated, a new version of a critical library was released 
upstream. Because you always install the latest version as part of the build, your new server 
is now subtly different to the old one. This will lead to subtle problems which will take you 
three days, or three bottles of whiskey, to debug.
By the time you have four or five servers, they're all a little different. Which is the 
authoritative one? Or are they all slightly wrong? The longer they're around, the more they 
will drift apart. You wouldn't run four or five different versions of your app code at once, so 
what's up with that? Why is it acceptable for the server configuration to be in a mess like this?
Wouldn't it be nice if the state of configuration on all your machines could be regularly 
checked and synchronized with a central, standard version?
Repeating changes across many servers
Humans just aren't good at accurately repeating complex tasks over and over; that's why we 
invented robots. It's easy to make mistakes, miss things out, or be interrupted and lose track 
of what you've done.
Changes happen all the time, and it becomes increasingly difficult to keep things up to date 
and in sync as your infrastructure grows. Again, when you make a change to your app code, 
you don't go and make that change manually with a text editor on each server. You change it 
once and roll it out everywhere. Isn't your firewall setup just as much a part of your code as 
your user model?
Wouldn't it be nice if you only had to make changes in one place, and they rolled out to your 
whole network automatically?
Self-updating documentation
In real life, we're too busy to stop every five minutes and document what we just did.  
As we've seen, that documentation is of limited use anyway, even if it's kept fanatically  
up to date.
The only reliable documentation, in fact, is the state of the servers themselves. You can look 
at a server to see how it's configured, but that only applies while you still have the machine. 
If something goes wrong and you can't access the machine or the data on it, your only option 
is to reconstruct the lost configuration from scratch.
Wouldn't it be nice if you had a clear, human-readable build procedure which was 
independent of your servers, and was guaranteed to be up to date, because the  
servers are actually built from it?

Getting started with Puppet
[ 4 ]
Version control and history
When you're making manual, ad hoc changes to systems, you can't roll them back to a point 
in time. It's hard to undo a whole series of changes; you don't have a way of keeping track of 
what you did and how things changed.
This is bad enough when there's just one of you. When you're working in a team it gets even 
worse, with everybody making independent changes and getting in each other's way.
When you have a problem, you need a way to know what changed and when, and who did it. 
And you also need to be able to set your configuration back to any previously stable state.
Wouldn't it be nice if you could go back in time?
Why not just write shell scripts?
Many people manage configuration with shell scripts, which is better than doing it manually, 
but not much. Some of the problems with shell scripts include the following:


Fragile and non-portable


Hard to maintain


Not easy to read as documentation


Very site-specific


Not a good programming language


Hard to apply changes to existing servers
Why not just use containers?
Containers! Is there any word more thrilling to the human soul? Many people feel as though 
containers are going to make configuration management problems just go away. This feeling 
rarely lasts beyond the first few hours of trying to containerize an app. Yes, containers 
make it easy to deploy and manage software, but where do containers come from? It 
turns out someone has to build and maintain them, and that means managing Dockerfiles, 
volumes, networks, clusters, image repositories, dependencies, and so on. In other words, 
configuration. There is an axiom of computer science called The Law of Conservation of 
Pain. If you save yourself pain in one place, it pops up again in another. Whatever cool new 
technology comes along, it won't solve all our problems; at best, it will replace them with 
refreshingly different problems. 

[ 5 ]
The truth is, container-based systems require even more configuration management. You 
need to configure the nodes which run the containers, build and update the container 
images based on a central policy, create and maintain the container network and clusters, 
and so on.
Why not just use serverless?
If containers are powered by magic pixies, serverless architectures are pure fairy dust. The 
promise is that you just push your app to the cloud, and the cloud takes care of deploying, 
scaling, load balancing, monitoring, and so forth. Like most things, the reality doesn't quite 
live up to the marketing. Unfortunately, serverless isn't actually serverless; it just means your 
business is running on servers you don't have direct control over, plus there are higher fixed 
costs because you're paying someone else to run them for you. Serverless can be a good way 
to get started, but it's not a long-term solution because ultimately, you need to own your 
own configuration.
Configuration management tools
Configuration management (CM) tools are the modern, sensible way to manage 
infrastructure as code. There are many such tools available, all of which operate more or 
less the same way—you specify your desired configuration state, using editable text files and 
a model of the system's resources, and the tool compares the current state of each node 
(the term we use for configuration-managed servers) with your desired state and makes any 
changes necessary to bring it in line.
As with most unimportant things, there is a great deal of discussion and argument on 
the Internet about which CM tool is the best. While there are significant differences in 
approaches and capabilities between different tools, don't let that obscure the fact that 
using a tool of any sort to manage configuration is much better than trying to do it by hand.
That said, while there are many CM tools available, Puppet is an excellent choice. No other 
tool is more powerful, more portable, or more widely-adopted. In this book, I'm going to 
show you what makes Puppet so good, and the things that only Puppet can do.
What is Puppet?
Puppet is two things: a language for expressing the desired state (how your nodes should be 
configured), and an engine which interprets code written in the Puppet language and applies 
it to nodes to bring about the desired state.

Getting started with Puppet
[ 6 ]
What does this language look like? It's not exactly a series of instructions, like a shell script or 
a Ruby program. It's more like a set of declarations about the way things should be. Consider 
the following example:
package { 'curl':
  ensure => installed,
}
In English, this code says—The curl package should be installed. When you apply this 
manifest (Puppet programs are called manifests), the tool will do the following:
1.	 Check the list of installed packages on the node to see if curl is already installed.
2.	 If it is, do nothing.
3.	 If not, install it.
Another example:
user { 'bridget':
  ensure => present,
}
This is Puppet language for the declaration The bridget user should be present (the 
keyword ensure means the desired state of the resource is....). Again, this results in Puppet 
checking for the existence of the bridget user on the node, and creating it if necessary. 
This is also a kind of documentation which expresses human-readable statements about the 
system in a formal way. The code expresses the author's desire that Bridget should always be 
present.
So you can see that the Puppet program—the Puppet manifest—for your configuration is a 
set of declarations about what things should exist, and how they should be configured. 
You don't give commands such as Do this, then do that. Rather, you describe how things 
should be, and let Puppet take care of making it happen. These are two quite different kinds 
of programming. One kind (so-called procedural style) is the traditional model used by 
languages like C, Python, shell, and so on. Puppet's is called declarative style, because you 
declare what the end result should be, rather than specify the steps to get there.
This means that you can apply the same Puppet manifest repeatedly to a node and the end 
result will be the same, no matter how many times you apply the manifest. It's better to 
think of Puppet manifests as a kind of specification, or declaration, rather than as a program 
in the traditional sense.

[ 7 ]
Resources and attributes
Puppet lets you describe configuration in terms of resources (types of things that can exist, 
such as users, files, or packages) and their attributes (appropriate properties for the type of 
resource, such as the home directory for a user, or the owner and permissions for a file). You 
don't have to get into the details of how resources are created and configured on different 
platforms. Puppet takes care of it.
The power of this approach is that a given manifest can be applied to different nodes, all 
running different operating systems, and the results will be the same everywhere. 
Puppet architectures
It's worth noting that there are two different ways to use Puppet. The first way, known as 
agent/master architecture, uses a special node dedicated to running Puppet, which all other 
nodes contact to get their configuration. 
The other way, known as stand-alone Puppet, does not need a special Puppet master node. 
Puppet runs on each individual node and does not need to contact a central location to get 
its configuration. Instead, you use Git, or any other way of copying files to the node, such as 
SFTP or rsync, to update the Puppet manifests on each node. 
Both stand-alone and agent/master architectures are officially supported by Puppet. It's 
your choice which one you prefer to use. In this book, I will cover only the stand-alone 
architecture, which is simpler and easier for most organizations, but almost everything in the 
book will work just the same whether you use agent/master or stand-alone Puppet.
To set up Puppet with an agent/master architecture, consult the 
official Puppet documentation.
Getting ready for Puppet
Although Puppet is inherently cross-platform and works with many different operating 
systems, for the purposes of this book I'm going to focus on just one operating system, the 
Ubuntu 16.04 LTS distribution of Linux, and the most recent version of Puppet, Puppet 
4. However, all the examples in the book should work on any recent operating system or 
Puppet version with only minor changes.

Getting started with Puppet
[ 8 ]
You will probably find that the best way to read this book is to follow along with the 
examples using a Linux machine of your own. It doesn't matter whether this is a physical 
server, desktop or laptop, cloud instance, or a virtual machine (VM). I'm going to use the 
popular Vagrant software to run a virtual machine on my own computer, and you can do the 
same. The public GitHub repository for this book contains a Vagrantfile which you can use to 
get up and running with Puppet in just a few steps.
Installing Git and downloading the repository
To get a copy of the repository that accompanies this book, follow these steps:
1.	 Browse to https://git-scm.com/downloads.
2.	 Download and install the right version of Git for your operating system.
3.	 Run the following command:
git clone https://github.com/bitfield/puppet-beginners- 
guide.git
Installing Virtualbox and Vagrant
If you already have a Linux machine or cloud server you'd like to use for working through the 
examples, skip this section and move on to Installing Puppet. If you'd like to use Virtualbox 
and Vagrant to run a local VM on your computer to use with the examples, follow these 
instructions:
1.	 Browse to https://www.virtualbox.org/.
2.	 Download and install the right version of Virtualbox for your operating system.
3.	 Browse to https://www.vagrantup.com/downloads.html.
4.	 Select the right version of Vagrant for your operating system: OS X, Windows,  
and so on.
5.	 Follow the instructions to install the software.

[ 9 ]
Running your Vagrant VM
Once you have installed Vagrant, you can start the Puppet Beginner's Guide virtual machine:
1.	 Run the following commands:
cd puppet-beginners-guide
vagrant plugin install vagrant-vbguest
vagrant up
2.	 Vagrant will begin downloading the base box, which is an Ubuntu 16.04 image pre-
installed with Puppet. This may take a while, but once the download is complete, 
the virtual machine will start.
3.	 Connect to the VM with the following command:
vagrant ssh
4.	 You now have a command line shell on the VM. To make sure you have the latest 
version of Puppet installed, run the following commands (answer y to any prompts):
curl https://apt.puppetlabs.com/DEB-GPG-KEY-puppet |sudo apt- 
key add
sudo apt-get update
sudo apt-get install -y puppetlabs-release-pc1
sudo apt-get install -y puppet-agent
5.	 Check that Puppet is installed and working (you may get a different version number, 
which is fine):
sudo /opt/puppetlabs/bin/puppet --version
4.10.1
If you're using Windows, you may need to install the PuTTY software to connect 
to your VM. There is some helpful advice about using Vagrant on Windows at 
http://tech.osteel.me/posts/2015/01/25/how-to-use-
vagrant-on-windows.html.

Getting started with Puppet
[ 10 ]
Alternative Vagrant VMs
The Vagrant configuration file (known as a Vagrantfile) in the example repository for this 
book specifies a particular Vagrant box, or downloadable machine image. If this box is no 
longer available, for whatever reason, you may see an error on running the vagrant up 
command, like the following:
The box 'puppetlabs/ubuntu-16.04-64-puppet' could not be found or 
could not be accessed in the remote catalog.
If you get this error, browse to the following URL to see the available Puppet Vagrant boxes:
https://atlas.hashicorp.com/puppetlabs/
Look for Ubuntu images ending in -puppet (these have Puppet pre-installed). For example, 
puppetlabs/ubuntu-16.04-64-puppet is an Ubuntu 16.04 image for 64-bit systems 
with Puppet pre-installed. Find the latest available such image and edit the Vagrantfile 
in the puppet-beginners-guide directory to change the config.vm.box setting 
appropriately:
  config.vm.box = "puppetlabs/ubuntu-16.04-64-puppet"
Then try running the vagrant up command again.
Adding Puppet to your path
We need to perform one more step which will make it easier for us to run Puppet on the 
node without having to specify the full path each time. Run the following command:
sudo visudo
An editor will start with the contents of the /etc/sudoers file. Look for the following line:
Defaults        secure_path="/usr/local/sbin:/usr/local/bin:/usr/
sbin:/usr/bin:/sbin:/bin"
Add :/opt/puppetlabs/puppet/bin to this set of paths, so that it reads:
Defaults        secure_path="/usr/local/sbin:/usr/local/bin:/usr/
sbin:/usr/bin:/sbin:/bin:/opt/puppetlabs/puppet/bin"

[ 11 ]
Save the file and exit the editor. The system is now setup to find the puppet executable 
without specifying the full path to it. To test this, run the following command:
sudo puppet —version
4.10.1
Troubleshooting Vagrant
If you have any problems running the VM, look for help on the Virtualbox or Vagrant websites. 
In particular, if you have an older machine, you may see a message like the following:
VT-x/AMD-V hardware acceleration is not available on your system. Your 
64-bit guest will fail to detect a 64-bit CPU and will not be able to 
boot.
Your computer may have a BIOS setting to enable 64-bit hardware virtualization (depending 
on the manufacturer, the trade name for this is either VT-x or AMD-V). Enabling this feature 
may fix the problem. If not, you can try the 32-bit version of the Vagrant box instead. Edit the 
file named Vagrantfile in the Git repository, and comment out the following line with a 
leading #:
config.vm.box = "puppetlabs/ubuntu-16.04-64-puppet"
Uncomment the following line by removing the leading # character:
# config.vm.box = "puppetlabs/ubuntu-16.04-32-puppet"
Now rerun the vagrant up command.
Summary
In this chapter, we looked at the various problems which configuration management tools 
can help solve, and how Puppet in particular, models aspects of system configuration. We 
checked out the Git repository of example code for this book, installed Virtualbox and 
Vagrant, started the Vagrant virtual machine, and ran Puppet for the first time.
In the next chapter, we'll write our first Puppet manifests, get some insight into the structure 
of Puppet resources and how they're applied, and learn about the package, file, and 
service resources.


[ 13 ]
Creating your first manifests
Beginnings are such delicate times.
	 	
	
	
	
	
	
—Frank Herbert, Dune
In this chapter, you'll learn how to write your first manifest with Puppet, and how to put 
Puppet to work configuring a server. You'll also understand how Puppet compiles and applies 
a manifest. You'll see how to use Puppet to manage the contents of files, how to install 
packages, and how to control services.

Creating your first manifests
[ 14 ]
Hello, Puppet – your first Puppet manifest
The first example program in any programming language, by tradition, prints hello, 
world. Although we can do that easily in Puppet, let's do something a little more ambitious, 
and have Puppet create a file on the server containing that text.
On your Vagrant box, run the following commands:
   cat /vagrant/examples/file_hello.pp
file { '/tmp/hello.txt':
  ensure  => file,
  content => "hello, world\n",
}
If you're not using the Vagrant box, copy the file examples/file_
hello.pp from the puppet-beginners-guide Git repo to the 
server you're using to try out Puppet.
Now apply it with the following command:
   sudo puppet apply /vagrant/examples/file_hello.pp
Notice: Compiled catalog for localhost in environment production in 
0.07 seconds
Notice: /Stage[main]/Main/File[/tmp/hello.txt]/ensure: defined content 
as '{md5}22c3683b094136c3398391ae71b20f04'
Notice: Applied catalog in 0.01 seconds
If you see the following error message:
sudo: puppet: command not found
Check that you followed the steps in the Adding Puppet to your path 
section in Chapter 1, Getting started with Puppet.
We can ignore the output from Puppet for the moment, but if all has gone well, we should 
be able to run the following command:
   cat /tmp/hello.txt
hello, world
Understanding the code
Let's look at that code in detail to see what's going on: 
file { '/tmp/hello.txt':

[ 15 ]
The word file begins a resource declaration for a file resource. Recall that a resource 
is some bit of configuration which you want Puppet to manage; for example, a file, user 
account, or package. A resource declaration looks like this:
RESOURCE_TYPE { TITLE:
  ATTRIBUTE => VALUE,
  ...
}
Resource declarations will make up almost all of your Puppet manifests, so it's important to 
understand exactly how they work. 
RESOURCE_TYPE indicates the type of resource you're declaring; in this case, it's a file.
The TITLE parameter is the name that Puppet uses to identify the resource internally. Every 
resource must have a unique title. With file resources, it's usual for this to be the full path 
to the file; in this case, /tmp/hello.
The remainder of this block of code is a list of attributes which describe how the resource 
should be configured. The attributes available depend on the type of resource; for a file, you 
can set attributes such as content, owner, group, and mode, but one attribute that every 
resource supports is ensure. 
Again, the possible values for the ensure attribute are specific to the type of resource. In 
this case, we use file to indicate that we want a regular file, as opposed to a directory or 
symlink:
  ensure  => file,
 Next, to put some text in the file, we specify the content attribute:
  content => "hello, world\n",
The content attribute sets the contents of a file to a string value you provide. Here, the 
contents of the file are declared to be hello, world followed by a newline character (in 
Puppet strings, we write the newline character as \n).
The content attribute specifies the entire content of the file; the string 
you provide will replace anything already in the file, rather than being 
appended to it.

Creating your first manifests
[ 16 ]
Modifying existing files
What happens if the file already exists when Puppet runs, and it contains something else? 
Will Puppet change it?
   sudo sh -c 'echo "goodbye, world" >/tmp/hello.txt'
   cat /tmp/hello.txt
goodbye, world
   sudo puppet apply /vagrant/examples/file_hello.pp
   cat /tmp/hello.txt
hello, world
The answer is yes. If any attribute of the file, including its contents, doesn't match the 
manifest, Puppet will change it so that it does.
This can lead to some surprising results if you manually edit a file managed by Puppet. If 
you make changes to a file without also changing the Puppet manifest to match, Puppet will 
overwrite the file the next time it runs, and your changes will be lost.
So it's a good idea to add a comment to the files which Puppet is managing; something such 
as the following:
# This file is managed by Puppet - any manual edits will be lost
Add this to Puppet's copy of the file when you first deploy it and it will remind you and 
others not to make manual changes.
Dry-running Puppet
Because you can't necessarily tell in advance what applying a Puppet manifest will change 
on the system, it's a good idea to do a dry run first. Adding the --noop flag to the puppet 
apply command will show you what Puppet would have done, without actually changing 
anything:
   sudo sh -c 'echo "goodbye, world" >/tmp/hello.txt'
   sudo puppet apply --noop /vagrant/examples/file_hello.pp
Notice: Compiled catalog for localhost in environment production in 
0.04 seconds
Notice: /Stage[main]/Main/File[/tmp/hello.txt]/content: current_value 
{md5}7678..., should be {md5}22c3... (noop)

[ 17 ]
Puppet decides whether or not a file resource needs updating based on its MD5 hash 
sum. In the previous example, Puppet reports that the current value of the hash sum for /
tmp/hello.txt is 7678... whereas according to the manifest it should be 22c3.... 
Accordingly, the file will be changed on the next Puppet run.
If you want to see what change Puppet would actually make to the file, you can use the 
--show_diff option:
   sudo puppet apply --noop --show_diff /vagrant/examples/file_hello.pp
Notice: Compiled catalog for localhost in environment production in 
0.04 seconds
Notice: /Stage[main]/Main/File[/tmp/hello.txt]/content:
--- /tmp/hello.txt      2017-02-13 02:27:13.186261355 -0800
+++ /tmp/puppet-file20170213-3671-2yynjt        2017-02-13 
02:30:26.561834755 -0800
@@ -1 +1 @@
-goodbye, world
+hello, world
These options are very useful when you want to make sure that your Puppet manifest 
will affect only the things you're expecting it to; or, sometimes, when you want to check if 
something has been changed outside Puppet without actually undoing the change.
How Puppet applies the manifest
Here's how your manifest is processed. First, Puppet reads the manifest and the list of 
resources it contains (in this case, there's just one resource), and compiles these into a 
catalog (an internal representation of the desired state of the node).
Puppet then works through the catalog, applying each resource in turn:
1.	 First, it checks if the resource exists on the server. If not, Puppet creates it. In the 
example, we've declared that the file /tmp/hello.txt should exist. The first time 
you run the sudo puppet apply command, this won't be the case, so Puppet will 
create the file for you.
2.	 Then, for each resource, it checks the value of each attribute in the catalog against 
what actually exists on the server. In our example, there's just one attribute, 
content. We've specified that the content of the file should be hello, world\n. 
If the file is empty, or contains something else, Puppet will overwrite the file with 
what the catalog says it should contain.
In this case, the file will be empty the first time you apply the catalog, so Puppet will 
write the string hello, world\n into it.
We'll go on to examine the file resource in much more detail in later chapters.

Creating your first manifests
[ 18 ]
Creating a file of your own
Create your own manifest file (you can name it anything you like, so long as the file  
extension is .pp). Use a file resource to create a file on the server with any contents  
you like. Apply the manifest with Puppet and check that the file is created and contains  
the text you specified.
Edit the file directly and change the contents, then re-apply Puppet and check that it changes 
the file back to what the manifest says it should contain. 
Managing packages
Another key resource type in Puppet is the package. A major part of configuring servers 
by hand involves installing packages, so we will also be using packages a lot in Puppet 
manifests. Although every operating system has its own package format, and different 
formats vary quite a lot in their capabilities, Puppet represents all these possibilities with 
a single package type. If you specify in your Puppet manifest that a given package should 
be installed, Puppet will use the appropriate package manager commands to install it on 
whatever platform it's running on.
As you've seen, all resource declarations in Puppet follow the following form:
RESOURCE_TYPE { TITLE:
  ATTRIBUTE => VALUE,
  ...
}
Package resources are no different. RESOURCE_TYPE is the package, and the only attribute 
you usually need to specify is ensure, and the only value it usually needs to take is 
installed:
package { 'cowsay':
  ensure => installed,
}
Try this example:
   sudo puppet apply /vagrant/examples/package.pp
Notice: Compiled catalog for localhost in environment production in 
0.52 seconds
Notice: /Stage[main]/Main/Package[cowsay]/ensure: created
Notice: Applied catalog in 29.53 seconds

[ 19 ]
How Puppet applies the manifest
The title of the package resource is cowsay, so Puppet knows that we're talking about a 
package named cowsay.
The ensure attribute governs the installation state of packages; unsurprisingly, installed 
tells Puppet that the package should be installed.
As we saw with the earlier example, Puppet processes this manifest by examining each 
resource in turn and checking its attributes on the server against those specified in the 
manifest. In this case, Puppet will look for the cowsay package to see if it's installed. It is 
not, but the manifest says it should be, so Puppet carries out all the necessary actions to 
make reality match the manifest, which here means installing the package.
It's still early on in the book, but you can already do a great deal with Puppet! 
If you can install packages and manage the contents of files, you can go a very 
long way toward setting up any kind of server configuration you might need. If 
you had to stop reading right here (which would be a shame, but we're all busy 
people), you would still be able to use Puppet to automate a large part of the 
configuration work you will encounter. But Puppet can do much more.
Exercise
Create a manifest which uses the package resource to install any software you find useful for 
managing servers. Some suggestions: tmux, sysdig, atop, htop, and dstat.
Querying resources with puppet resource
If you want to see what version of a package Puppet thinks you have installed, you can use 
the puppet resource tool:
sudo puppet resource package openssl
package { 'openssl':
  ensure => '1.0.2g-1ubuntu4.1',
}
puppet resource TYPE TITLE will output a Puppet manifest representing the current 
state of the named resource on the system. If you leave out TITLE, you'll get a manifest for 
all the resources of type TYPE. For example, if you run the puppet resource package 
command, you'll see Puppet code for all the packages installed on the system.

Creating your first manifests
[ 20 ]
puppet resource even has an interactive configuration feature. Try the following command:
puppet resource -e package openssl
If you run this, Puppet will generate a manifest for the current state of the 
resource, and open it in an editor. If you now make changes to the manifest 
and save it, Puppet will apply that manifest to make changes to the system. 
This is a fun little feature, but it would be rather time consuming to do all 
your configuration this way!
Services
The third most important Puppet resource type is the service, a long-running process  
which either does some continuous kind of work, or waits for requests and then acts on 
them. For example, on most systems, the sshd process runs all the time and listens for  
SSH login attempts.
Puppet models services with the service resource type. Service resources look like the 
following example (you can find this in service.pp in the /vagrant/examples directory. 
From now on, I'll just give the filename of each example, as they are all in the same 
directory):
service { 'sshd':
  ensure => running,
  enable => true,
}
The ensure parameter governs whether the service should be running or not. If its value is 
running then, as you might expect, Puppet will start the service if it is not running. If you 
set ensure to stopped, Puppet will stop the service if it is running.
Services may also be set to start when the system boots, using the enable parameter. If 
enable is set to true, the service will start at boot. If, on the other hand, enable is set to 
false, it will not. Generally speaking, unless there's a good reason not to, all services should 
be set to start at boot.
Getting help on resources with puppet describe
If you're struggling to remember all the different attributes of all the different resources, 
Puppet has a built-in help feature which will remind you. For example,  
try the following command:
sudo puppet describe service

[ 21 ]
Puppet will give you a description of the service resource, along with a complete list of 
attributes and allowed values. This works for all built-in resource types, as well as many 
provided by third-party modules. To see a list of all available resource types, run:
sudo puppet describe --list
The package-file-service pattern
It's very common for a given piece of software to require these three Puppet resource 
types—the package resource installs the software, the file resource deploys one or  
more configuration files required for the software, and the service resource runs the 
software itself.
Here's an example using the MySQL database server (package_file_service.pp):
package { 'mysql-server':
  ensure => installed,
  notify => Service['mysql'],
}
file { '/etc/mysql/mysql.cnf':
  source => '/vagrant/examples/files/mysql.cnf',
  notify => Service['mysql'],
}
service { 'mysql':
  ensure => running,
  enable => true,
}
The package resource makes sure the mysql-server package is installed.
The config file for MySQL is /etc/mysql/mysql.cnf, and we use a file resource to copy 
this file from the Puppet repo so that we can control the MySQL settings. 
Finally, the service resource ensures that the mysql service is running.

Creating your first manifests
[ 22 ]
Notifying a linked resource
You might have noticed a new attribute notify in the file resource in the previous 
example:
file { '/etc/mysql/mysql.cnf':
  source => '/vagrant/examples/files/mysql.cnf',
  notify => Service['mysql'],
}
What does this do? Imagine you've made a change to the mysql.cnf file and applied this 
change with Puppet. The updated file will be written to disk but, because the mysql service 
is already running, it has no way of knowing that its config file has changed. So your changes 
will not actually take effect until the service is restarted.
However, Puppet can do this for you, if you specify the notify attribute on the file 
resource. The value of notify is the resource to notify about the change, and what that 
involves depends on the type of resource that's being notified. When it's a service, the 
default action is to restart the service. (We'll find out about the other options in Chapter 4, 
Understanding Puppet resources.)
Usually, with the package-file-service pattern, the file notifies the service so whenever 
Puppet changes the contents of the file, it will restart the notified service to pick up the new 
configuration. If there are several files which affect the service, they should all notify the 
service, and Puppet is smart enough to only restart the service once, regardless of how 
many dependent resources are changed.
The name of the resource to notify is specified as the resource type, capitalized, followed by 
the resource title, quoted, within square brackets: Service['mysql'].
Resource ordering with require
In the package_file_service example, we declared three resources: the mysql-server 
package, the /etc/mysql/mysql.cnf file, and the mysql service. If you think about it, 
they need to be applied in that order. Without the mysql-server package installed, there 
will be no /etc/mysql directory to put the mysql.cnf file in. Without the package or the 
config file, the mysql service won't be able to run.
A perfectly reasonable question to ask is Does Puppet apply resources in the same order 
in which they're declared in the manifest? The answer is usually yes, unless you explicitly 
specify a different order, using the require attribute.

[ 23 ]
All resources support the require attribute, and its value is the name of another resource 
declared somewhere in the manifest, specified in the same way as when using notify. 
Here's the package-file-service example again, this time with the resource ordering specified 
explicitly using require (package_file_service_require.pp):
package { 'mysql-server':
  ensure => installed,
}
file { '/etc/mysql/mysql.cnf':
  source  => '/vagrant/examples/files/mysql.cnf',
  notify  => Service['mysql'],
  require => Package['mysql-server'],
}
service { 'mysql':
  ensure  => running,
  enable  => true,
  require => [Package['mysql-server'], File['/etc/mysql/mysql.cnf']],
}
You can see that the mysql.cnf resource requires the mysql-server package. The mysql 
service requires both the other resources, listed as an array within square brackets.
When resources are already in the right order, you don't need to use require, as Puppet 
will apply the resources in the order you declare them. However, it can be useful to specify 
an order explicitly, for the benefit of those reading the code, especially when there are lots 
of resources in a manifest file.
In older versions of Puppet, resources were applied in a more or less arbitrary order, so it 
was much more important to express dependencies using require. Nowadays, you won't 
need to use it very much, and you'll mostly come across it in legacy code.

Creating your first manifests
[ 24 ]
Summary
In this chapter, we've seen how a manifest is made up of Puppet resources. We've learned 
how to use Puppet's file resource to create and modify files, how to install packages 
using the package resource, and how to manage services with the service resource. 
We've looked at the common package-file-service pattern, and seen how to use the notify 
attribute on a resource to send a message to another resource that its configuration has 
been updated. We've covered the use of the require attribute to make dependencies 
between resources explicit, when necessary.
We've also learned to use puppet resource to inspect the current state of the system 
according to Puppet, and puppet describe to get command-line help on all Puppet 
resources. To check what Puppet would change on the system, without actually changing it, 
we've introduced the --noop and --show_diff options to puppet apply.
In the next chapter, we'll see how to use the version control tool Git to keep track of your 
manifests, we'll get an introduction to fundamental Git concepts such as the repo and the 
commit, and we'll learn how to distribute your code to each of the servers you're going to 
manage with Puppet.

[ 25 ]
Managing your Puppet  
code with Git
We define ourselves by our actions. With each decision, we tell ourselves and 
the world who we are.
	 	
	
	
	
	
	
	
—Bill Watterson
In this chapter, you'll learn how to use the Git version control system to manage your Puppet 
manifests. I'll also show you how to use Git to distribute the manifests to multiple nodes, so 
that you can start managing your whole network with Puppet.
3

Managing your Puppet code with Git
[ 26 ]
What is version control?
If you're already familiar with Git, you can save some reading by skipping ahead to the Creating 
a Git repo section. If not, here's a gentle introduction.
Even if you're the only person who works on a piece of source code (for example, Puppet 
manifests), it's still useful to be able to see what changes you made and when. For example, 
you might realize that you introduced a bug at some point in the past, and you need to 
examine exactly when a certain file was modified and exactly what the change was. A version 
control system lets you do that, by keeping a complete history of the changes you've made to a 
set of files over time.
Tracking changes
When you're working on code with others, you also need a way to communicate with the rest 
of the team about your changes. A version control tool such as Git not only tracks everyone's 
changes, but lets you record a commit message, explaining what you did and why. The 
following example illustrates some aspects of a good commit message:
Summarize changes in around 50 characters or less
More detailed explanatory text, if necessary. Wrap it to about 72
characters or so. In some contexts, the first line is treated as
the subject of the commit and the rest of the text as the body. 
The blank line separating the summary from the body is critical
(unless you omit the body entirely); various tools like `log`,
`shortlog`, and `rebase` can get confused if you run the two together.
Explain the problem that this commit is solving. Focus on why you
are making this change as opposed to how (the code explains that).
Are there side effects or other unintuitive consequences of this
change? Here's the place to explain them.
Further paragraphs come after blank lines.
 - Bullet points are okay, too
 - Typically a hyphen or asterisk is used for the bullet, preceded
   by a single space, with blank lines in between, but conventions
   vary here
If you use an issue tracker, put references to them at the bottom,
like this:
Resolves: #123
See also: #456, #789

[ 27 ]
This example is taken from Chris Beams's excellent blog post on How to 
write a Git commit message at
https://chris.beams.io/posts/git-commit/.
Of course, you won't often need such a long and detailed message; most 
of the time, a single line will suffice. However, it's better to give more 
information than less. 
Git also records when the change happened, who made it, what files were changed, added, or 
deleted, and which lines were added, altered, or removed. As you can imagine, if you're trying 
to track down a bug and you can see a complete history of changes to the code, that's a big 
help. It also means you can, if necessary, roll back the state of the code to any point in history 
and examine it.
You might think this introduces a lot of extra complication. In fact, it's very simple. Git stays 
out of your way until you need it, and all you have to do is write a commit message when you 
decide to record changes to a code.
Sharing code
A set of files under Git version control is called a repository, which is usually equivalent to a 
project. A Git repository (from now on, just repo) is also a great way to distribute your code to 
others, either privately or publicly, so that they can use it, modify it, contribute changes back to 
you, or develop it in a different direction for their own requirements. The public GitHub repo 
for this book, which we looked at in Chapter 1, Getting started with Puppet, is a good example 
of this. You'll be able to use this repo for working through examples throughout the book, 
but you can also use it for help and inspiration when building Puppet manifests for your own 
infrastructure.
Because Git is so important for managing Puppet code, it's a good idea to get familiar with 
it, and the only way to do that is to use it for real. So let's start a new Git repo we can use to 
experiment with.
Creating a Git repo
It's very easy to create a Git repo. Follow these steps:
1.	 Install Git on your Vagrant box with the following command:
sudo apt-get install git
2.	 Make a directory to hold your versioned files using the following commands:
cd
mkdir puppet

Managing your Puppet code with Git
[ 28 ]
3.	 Now run the following commands to turn the directory into a Git repo:
cd puppet
git init
Initialized empty Git repository in /home/vagrant/puppet/.git/
Making your first commit
You can change the files in your repo as much as you like, but Git will not know about the 
changes until you make what's called a commit. You can think of a commit as being like 
a snapshot of the repo at a particular moment, but it also stores information about what 
changed in the repo since the previous commit. Commits are stored forever, so you will always 
be able to roll back the repo to the state it was in at a certain commit, or show what files were 
changed in a past commit and compare them to the state of the repo at any other commit.
Let's make our first commit to the new repo: 
1.	 Because Git records not only the changes to the code but also who made them, it 
needs to know who you are. Set your identification details for Git (use your own 
name and e-mail address, unless you particularly prefer mine) using the following 
commands:
git config --global user.name "John Arundel"
git config --global user.email john@bitfieldconsulting.com
2.	 It's traditional for Git repos to have a README file, which explains what's in the 
repo and how to use it. For the moment, let's just create this file with a placeholder 
message: 
echo "Watch this space... coming soon!" >README.md
3.	 Run the following command:
git status
On branch master
Initial commit
Untracked files:
  (use "git add <file>..." to include in what will be 
  committed)
        README.md
nothing added to commit but untracked files present (use 
 "git add" to track)
4.	 Though we've added a new file to the repo, changes to it won't be tracked by Git 
unless we explicitly tell it to do so. We do this by using the git add command,  
as follows:
git add README.md

[ 29 ]
5.	 Git now knows about this file, and changes to it will be included in the next commit. 
We can check this by running git status again:
git status
On branch master
Initial commit
Changes to be committed:
  (use "git rm --cached <file>..." to unstage)
        new file:   README.md
6.	 The file is listed under Changes to be committed, so we can now actually make 
the commit:
git commit -m 'Add README file'
[master (root-commit) ee21595] Add README file
 1 file changed, 1 insertion(+)
 create mode 100644 README.md
7.	 You can always see the complete history of commits in a repo by using the git log 
command. Try it now to see the commit you just made:
git log
commit ee215951199158ef28dd78197d8fa9ff078b3579
Author: John Arundel <john@bitfieldconsulting.com>
Date:   Tue Aug 30 05:59:42 2016 -0700
    Add README file
How often should I commit?
A common practice is to commit when the code is in a consistent, working state, and have the 
commit include a set of related changes made for some particular purpose. So, for example, if 
you are working to fix bug number 75 in your issue-tracking system, you might make changes 
to quite a few separate files and then, once you're happy the work is complete, make a single 
commit with a message such as:
Make nginx restart more reliable (fixes issue #75)
On the other hand, if you are making a large number of complicated changes and you are not 
sure when you'll be done, it might be wise to make a few separate commits along the way, so 
that if necessary you can roll the code back to a previous state. Commits cost nothing, so when 
you feel a commit is needed, go ahead and make it.

Managing your Puppet code with Git
[ 30 ]
Branching
Git has a powerful feature called branching, which lets you create a parallel copy of the code 
(a branch) and make changes to it independently. At any time, you can choose to merge those 
changes back into the master branch. Or, if changes have been made to the master branch in 
the meantime, you can incorporate those into your working branch and carry on.
This is extremely useful when working with Puppet, because it means you can switch a single 
node to your branch while you're testing it and working on it. The changes you make won't be 
visible to nodes which aren't on your branch, so there's no danger of accidentally rolling out 
changes before you're ready.
Once you're done, you can merge your changes back into the master and have them roll out to 
all the nodes.
Similarly, two or more people can work independently on their own branches, exchanging 
individual commits with each other and with the master branch as they choose. This is a very 
flexible and useful way of working.
For more information about Git branching, and indeed about 
Git in general, I recommend the excellent book Pro Git by Scott 
Chacon and Ben Straub. The whole book is available free online at
https://git-scm.com/book/en/v2
Distributing Puppet manifests
So far in this book, we've only applied Puppet manifests to one node, using puppet apply 
with a local copy of the manifest. To manage several nodes at once, we need to distribute the 
Puppet manifests to each node so that they can be applied.
There are several ways to do this and, as we saw in Chapter 1, Getting started with Puppet, 
one approach is to use the agent/master architecture, where a central Puppet master server 
compiles your manifests and distributes the catalog (the desired node state) to all nodes.
Another way to use Puppet is to do without the master server altogether, and use Git to 
distribute manifests to client nodes, which then run the puppet apply command to update 
their configuration. This stand-alone Puppet architecture doesn't require a dedicated Puppet 
master server, and there's no single point of failure.

[ 31 ]
Both agent/master and stand-alone architectures are officially supported by Puppet, and it's 
possible to change from one to the other if you decide you need to. The examples in this book 
were developed with the stand-alone architecture, but will work just as well with agent/master 
if you prefer it. There is no difference in the Puppet manifest, language, or structure; the only 
difference is in the way the manifests are applied.
All you need for a stand-alone Puppet architecture is a Git server that each node can connect 
to and clone the repo. You can run your own Git server if you like or use a public Git hosting 
service such as GitHub. For ease of explanation, I'm going to use GitHub for this example setup.
In the following sections, we'll create a GitHub account, push our new Puppet repo to GitHub, 
and then set up our virtual machine to automatically pull any changes from the GitHub repo 
and apply them with Puppet.
Creating a GitHub account and project
The following are the steps to create a GitHub account. If you already have a GitHub account, 
or you're using another Git server, you can skip this section:
1.	 Browse to https://github.com/.
2.	 Enter the username you want to use, your e-mail address, and a password.
3.	 Choose the Unlimited public repositories for free plan.
4.	 GitHub will send you an e-mail to verify your e-mail address. When you get the e-mail, 
click on the verification link.
5.	 Select Start a project.
6.	 Enter a name for your repo (I suggest puppet, but it doesn't matter).
7.	 Free GitHub accounts can only create public repos, so select Public.
Be careful what information you put into a public Git repo because it 
can be read by anybody. Never put passwords, login credentials, private 
keys, or other confidential information into a repo like this, unless it is 
encrypted. We'll see how to encrypt secret information in your Puppet 
repo in Chapter 6, Managing data with Hiera.
8.	 Click Create repository.

Managing your Puppet code with Git
[ 32 ]
9.	 GitHub will show you a page of instructions about how to initialize or import code 
into your new repo. Look for the https URL which identifies your repo; it will be 
something like the following: https://github.com/pbgtest/puppet.git
Pushing your repo to GitHub
You're now ready to take the Git repo you created locally earlier in this chapter, and push it to 
GitHub so that you can share it with other nodes.
Follow these steps to push your repo to GitHub:
1.	 In your repo directory, run the following commands. After git remote add 
origin, specify the URL to your GitHub repo:
git remote add origin YOUR_REPO_URL
git push -u origin master

[ 33 ]
2.	 GitHub will prompt you for your username and password:
Username for 'https://github.com': pbgtest
Password for 'https://pbgtest@github.com':
Counting objects: 3, done.
Writing objects: 100% (3/3), 262 bytes | 0 bytes/s, done.
Total 3 (delta 0), reused 0 (delta 0)
To https://github.com/pbgtest/puppet.git
 * [new branch]      master -> master
Branch master set up to track remote branch master from 
 origin.
3.	 You can check that everything has worked properly by visiting the repo URL in your 
browser. It should look something like this:
Cloning the repo
In order to manage multiple nodes with Puppet, you will need a copy of the repo on each 
node. If you have a node you'd like to manage with Puppet, you can use it in this example. 
Otherwise, use the Vagrant box we've been working with in the previous chapters.
Run the following commands (replace the argument to git clone with the URL of your own 
GitHub repo, but don't lose production at the end):
cd /etc/puppetlabs/code/environments
sudo mv production production.sample
sudo git clone YOUR_REPO_URL production

Managing your Puppet code with Git
[ 34 ]
Cloning into 'production'...
remote: Counting objects: 3, done.
remote: Total 3 (delta 0), reused 3 (delta 0), pack-reused 0
Unpacking objects: 100% (3/3), done.
Checking connectivity... done.
How does this work? The standard place for Puppet manifests in a production environment is 
the /etc/puppetlabs/code/environments/production directory, so that's where our 
cloned repo needs to end up. However, the Puppet package installs some sample manifests in 
that directory, and Git will refuse to clone into a directory that already exists, so we move that 
directory out of the way with the mv production production.sample command. The 
git clone command then recreates that directory, but this time it contains our manifests 
from the repo.
Fetching and applying changes automatically
In a stand-alone Puppet architecture, each node needs to automatically fetch any changes  
from the Git repo at regular intervals, and apply them with Puppet. We can use a simple shell 
script for this, and there's one in the examples repo (/vagrant/examples/files/run-
puppet.sh):
#!/bin/bash
cd /etc/puppetlabs/code/environments/production && git pull
/opt/puppetlabs/bin/puppet apply manifests/
We will need to install this script on the node to be managed by Puppet, and create a cron job 
to run it regularly (I suggest every 15 minutes). Of course, we could do this work manually, but 
isn't this book partly about the advantages of automation? Very well, then let's practice what 
we're preaching.
Writing a manifest to set up regular Puppet runs
In this section, we'll create the necessary Puppet manifests to install the run-puppet script on 
a node and run it regularly from cron; follow these steps:
1.	 Run the following commands to create the required directories in your Puppet repo:
cd /home/vagrant/puppet
mkdir manifests files
2.	 Run the following command to copy the run-puppet script from the examples 
directory:
cp /vagrant/examples/files/run-puppet.sh files/

[ 35 ]
3.	 Run the following command to copy the run-puppet manifest from the examples 
directory:
cp /vagrant/examples/run-puppet.pp manifests/
4.	 Add and commit the files to Git with the following commands:
git add manifests files
git commit -m 'Add run-puppet script and cron job'
git push origin master
Your Git repo now contains everything you need to automatically pull and apply changes on 
your managed nodes. In the next section, we'll see how to set up this process on a node.
You might have noticed that every time you push to your GitHub 
repo, Git prompts you for your username and password. If you want 
to avoid this, you can associate an SSH key with your GitHub account. 
Once you've done this, you'll be able to push without having to re-
enter your credentials every time. For more information about using 
an SSH key with your GitHub account, see the article
https://help.github.com/articles/adding-a-new-
ssh-key-to-your-github-account/
Applying the run-puppet manifest
Having created and pushed the manifest necessary to set up automatic Puppet runs, we now 
need to pull and apply it on the target node.
In the cloned copy of your repo in /etc/puppetlabs/code/environments/production, 
run the following commands:
sudo git pull
sudo puppet apply manifests/
Notice: Compiled catalog for localhost in environment production in 
0.08 seconds
Notice: /Stage[main]/Main/File[/usr/local/bin/run-puppet]/ensure: 
defined content as '{md5}83a6903e69564bcecc8fd1a83b1a7beb'
Notice: /Stage[main]/Main/Cron[run-puppet]/ensure: created
Notice: Applied catalog in 0.07 seconds
You can see from Puppet's output that it has created the /usr/local/bin/run-puppet 
script and the run-puppet cron job. This will now run automatically every 15 minutes, pull 
any new changes from the Git repo, and apply the updated manifest.

Managing your Puppet code with Git
[ 36 ]
The run-puppet script
The run-puppet script does the following two things in order to automatically update the 
target node:
1.	 Pull any changes from the Git server (git pull).
2.	 Apply the manifest (puppet apply).
Our Puppet manifest in run-puppet.pp deploys this script to the target node using a 
file resource, and then sets up a cron job to run it every 15 minutes using a cron resource. 
We haven't met the cron resource before, but we will cover it in more detail in Chapter 4, 
Understanding Puppet resources. 
For now, just note that the cron resource has a name (run-puppet) which is just for the 
benefit of us humans, to remind us what it does, and it also has the command to run, and the 
hour and minute attributes to control when it runs. The value */15 tells cron to run the job 
every 15 minutes.
Testing automatic Puppet runs
To prove that the automatic Puppet run works, make a change to your manifest which 
creates a file (/tmp/hello.txt, for example). Commit and push this change to Git. Wait 15 
minutes, and check your target node. The file should be present. If not, something is broken. 
To troubleshoot the problem, try running the sudo run-puppet command manually. If 
this works, check that the cron job is correctly installed by running the sudo crontab -l 
command. It should look something like the following:
# HEADER: This file was autogenerated at 2017-04-05 01:46:03 -0700 by 
puppet.
# HEADER: While it can still be managed manually, it is definitely not 
recommended.
# HEADER: Note particularly that the comments starting with 'Puppet 
Name' should
# HEADER: not be deleted, as doing so could cause duplicate cron jobs.
# Puppet Name: run-puppet
*/15 * * * * /usr/local/bin/run-puppet

[ 37 ]
Managing multiple nodes
You now have a fully-automated stand-alone Puppet infrastructure. Any change that you check 
in to your Git repo will be automatically applied to all nodes under Puppet management. To 
add more nodes to your infrastructure, follow these steps for each new node:
1.	 Install Puppet (not necessary if you're using the Vagrant box).
2.	 Clone your Git repo (as described in the Cloning the repo section).
3.	 Apply the manifest (as described in the Applying the run-puppet  
manifest section).
You might be wondering how to tell Puppet to apply different manifests to different nodes. 
For example, you might be managing two nodes, one of which is a web server and the other a 
database server. Naturally, they will need different resources.
You can do this using the node keyword. By default, Puppet applies all resources in the 
manifest whenever it runs, but you can change this by putting resources inside a node 
definition. The following example shows how to do this (node_web1.pp):
node 'web1' {
  file { '/tmp/only_on_web1':
    content => "I'm only needed on the node named 'web1'!",
  }
}
Puppet determines whether or not resources in a node definition should be applied by 
checking the node's hostname. If the hostname matches the name following the node 
keyword, then the resources are applied. Otherwise, Puppet will skip them. In the previous 
example, if you apply this manifest on a node named web1, the file /tmp/only_on_web1 
will be created, but on a node with any other name, Puppet will ignore it. (You can try this 
for yourself by changing the hostname of the Vagrant box with the sudo hostname web1 
command, and then applying the manifest. Change it back again with the sudo hostname 
localhost command.)
We'll learn more about nodes and how to control the application of resources to different 
nodes in Chapter 8,  Classes, roles, and profiles, but for now, just know that you can use a  
node definition for each of your nodes to manage which resources are applied to them.

Managing your Puppet code with Git
[ 38 ]
Summary
In this chapter, we introduced the concepts of version control, and the essentials of Git in 
particular. We set up a new Git repo, created a GitHub account, pushed our code to it, and 
cloned it on a node. We wrote a shell script to automatically pull and apply changes from 
the GitHub repo on any node, and a Puppet manifest to install this script and run it regularly 
from cron. We also introduced the node definition, which controls whether or not a set of 
resources are applied to a particular node.
In the next chapter, we'll explore the power of Puppet resources, going into more detail 
about the Puppet file, package, and service resources we've already encountered, and 
introducing three more important resource types—user, cron, and exec.

[ 39 ]
Understanding  
Puppet resources
Perplexity is the beginning of knowledge.
—Khalil Gibran
We've already met three important types of Puppet resources: package, file, and 
service. In this chapter, we'll learn more about these, plus other important resource  
types for managing users, groups, SSH keys, cron jobs, and arbitrary commands.
4

Understanding Puppet resources
[ 40 ]
Files
We saw in Chapter 2, Creating your first manifests, that Puppet can manage files on a node 
using the file resource, and we looked at an example which sets the contents of a file to a 
particular string using the content attribute. Here it is again (file_hello.pp):
file { '/tmp/hello.txt':
  content => "hello, world\n",
}
The path attribute
We've seen that every Puppet resource has a title (a quoted string followed by a colon). In 
the file_hello example, the title of the file resource is '/tmp/hello.txt'. It's easy to 
guess that Puppet is going to use this value as the path of the created file. In fact, path is 
one of the attributes you can specify for a file, but if you don't specify it, Puppet will use the 
title of the resource as the value of path.
Managing whole files
While it's useful to be able to set the contents of a file to a short text string, most files we're 
likely to want to manage will be too large to include directly in our Puppet manifests. Ideally, 
we would put a copy of the file in the Puppet repo, and have Puppet simply copy it to the 
desired place in the filesystem. The source attribute does exactly that (file_source.pp):
file { '/etc/motd':
  source => '/vagrant/examples/files/motd.txt',
}
To try this example with your Vagrant box, run the following commands:
sudo puppet apply /vagrant/examples/file_source.pp
cat /etc/motd
The best software in the world only sucks. The worst software is 
significantly worse than that.
–Luke Kanies

[ 41 ]
(From now on, I won't give you explicit instructions on how to run the examples;  
just apply them in the same way using the sudo puppet apply command as  
shown here. All the examples in this book are in the examples directory of the  
GitHub repo, and I'll give you the name of the appropriate file for each example,  
such as file_source.pp.)
Why do we have to run the sudo puppet apply command instead of 
just the puppet apply command? Puppet has the permissions of the 
user who runs it, so if Puppet needs to modify a file owned by root, it 
must be run with root's permissions (which is what sudo does). You will 
usually run Puppet as root because it needs those permissions to do things 
like installing packages, modifying config files owned by root, and so on. 
Suppose you get an error like this:
sudo: puppet: command not found
In case you do, check that you followed the steps in the Adding Puppet to 
your path section in Chapter 1, Getting started with Puppet.
The value of the source attribute can be a path to a file on the node, as shown here, or an 
HTTP URL, as in the following example (file_http.pp):
file { '/tmp/README.md':
  source => 'https://raw.githubusercontent.com/puppetlabs/puppet/
master/README.md',
}
Although this is a handy feature, bear in mind that every time you add 
an external dependency like this to your Puppet manifest, you're adding 
a potential point of failure. Wherever you can, use a local copy of such a 
file instead of having Puppet fetch it remotely every time. This particularly 
applies to software which needs to be built from a tarball downloaded 
from a website. If possible, download the tarball and serve it from a local 
web server or file server. If this isn't practical, using a caching proxy server 
can help save time and bandwidth when you're building a large number of 
nodes.

Understanding Puppet resources
[ 42 ]
Ownership
On UNIX-like systems, files are associated with an owner, a group, and a set of permissions 
to read, write, or execute the file. Since we normally run Puppet with the permissions of the 
root user (via sudo), the files Puppet manages will be owned by that user:
ls -l /etc/motd
-rw-r--r-- 1 root root 109 Aug 31 04:03 /etc/motd
Often, this is just fine, but if we need the file to belong to another user (for example, if that 
user needs to be able to write to the file), we can express that by setting the owner attribute 
(file_owner.pp):
file { '/etc/owned_by_vagrant':
  ensure => present,
  owner  => 'vagrant',
}
After applying this manifest, run the following command:
ls -l /etc/owned_by_vagrant
-rw-r--r-- 1 vagrant root 0 Aug 31 04:48 /etc/owned_by_vagrant
You can see that Puppet has created the file and its owner has been set to vagrant. You can 
also set group ownership of the file using the group attribute (file_group.pp):
file { '/etc/owned_by_vagrant':
  ensure => present,
  owner  => 'vagrant',
  group  => 'vagrant',
}
ls -l /etc/owned_by_vagrant
-rw-r--r-- 1 vagrant vagrant 0 Aug 31 04:48 /etc/owned_by_vagrant
Note that this time we didn't specify either a content or source attribute for the file, but 
simply ensure => present. In this case, Puppet will create a file of zero size.

[ 43 ]
Permissions
Files on UNIX-like systems have an associated mode which determines access permissions 
for the file. It governs read, write, and execute permissions for the file's owner, any user 
in the file's group, and other users. Puppet supports setting permissions on files using the 
mode attribute. This takes an octal value (base 8, indicated by a leading 0 digit), with each 
digit representing a field of three binary bits—the permissions for owner, group, and other, 
respectively.
In the following example, we use the mode attribute to set a mode of 0644 (read and write 
for owner, read-only for group, read-only for other) on a file (file_mode.pp):
file { '/etc/owned_by_vagrant':
  ensure => present,
  owner  => 'vagrant',
  mode   => '0644',
}
This will be quite familiar to experienced system administrators, as the octal values for file 
permissions are exactly the same as those understood by the UNIX chmod command. For 
more information, run the man chmod command.
Directories
Creating or managing permissions on a directory is a common task, and Puppet uses the 
file resource to do this too. If the value of the ensure attribute is directory, the file  
will be a directory (file_directory.pp):
file { '/etc/config_dir':
  ensure => directory,
}
As with regular files, you can use the owner, group, and mode attributes to control access  
to directories.
Trees of files
We've already seen that Puppet can copy a single file to the node, but what about a whole 
directory of files, possibly including subdirectories (known as a file tree)? The recurse 
attribute will take care of this (file_tree.pp):
file { '/etc/config_dir':
  source  => '/vagrant/examples/files/config_dir',
  recurse => true,
}

Understanding Puppet resources
[ 44 ]
Run the following command:
ls /etc/config_dir/
1  2  3
When the recurse attribute is true, Puppet will copy all the files and directories (and their 
subdirectories) in the source directory (/vagrant/examples/files/config_dir in this 
example) to the target directory (/etc/config_dir).
If the target directory already exists and has files in it, Puppet will not 
interfere with them, but you can change this behavior using the purge 
attribute. If this is true, Puppet will delete any files and directories in the 
target directory which are not present in the source directory. Use this 
attribute with care.
Symbolic links
Another common requirement for managing files is to create or modify a symbolic link 
(known as a symlink for short). You can have Puppet do this by setting ensure => link on 
the file resource, and specifying the target attribute (file_symlink.pp):
file { '/etc/this_is_a_link':
  ensure => link,
  target => '/etc/motd',
}
Run the following command:
ls -l /etc/this_is_a_link
lrwxrwxrwx 1 root root 9 Aug 31 05:05 /etc/this_is_a_link -> /etc/motd
Packages
We've already seen how to install a package using the package resource, and this is all you 
need to do with most packages. However, the package resource has a few extra features 
which may be useful.

[ 45 ]
Uninstalling packages
The ensure attribute normally takes the value installed in order to install a package, but 
if you specify absent instead, Puppet will remove the package if it happens to be installed. 
Otherwise, it will take no action. The following example will remove the apparmor package 
if it's installed (package_remove.pp):
package { 'apparmor':
  ensure => absent,
}
By default, when Puppet removes packages, it leaves in place any files managed by the 
package. To purge all files associated with the package, use purged instead of absent.
Installing specific versions
If there are multiple versions of a package available to the system's package manager, 
specifying ensure => installed will cause Puppet to install the default version (usually 
the latest). But if you need a specific version, you can specify that version string as the value 
of ensure, and Puppet will install that version (package_version.pp):
package { 'openssl':
  ensure => '1.0.2g-1ubuntu4.1',
}
It's a good idea to specify an exact version whenever you manage packages with 
Puppet, so that all nodes get the same version of a given package. Otherwise, 
if you use ensure => installed, they will just get whatever version was 
current at the time they were built, leading to a situation where different nodes 
have different package versions.
When a newer version of the package is released, and you decide it's time to upgrade to it, 
you can update the version string specified in the Puppet manifest and Puppet will upgrade 
the package everywhere.
Installing the latest version
On the other hand, if you specify ensure => latest for a package, Puppet will make 
sure that the latest available version is installed every time the manifest is applied. When a 
new version of the package becomes available, it will be installed automatically on the next 
Puppet run.

Understanding Puppet resources
[ 46 ]
This is not generally what you want when using a package repository that's 
not under your control (for example, the main Ubuntu repository). It means 
that packages will be upgraded at unexpected times, which may break your 
application (or at least result in unplanned downtime). A better strategy is 
to tell Puppet to install a specific version which you know works, and test 
upgrades in a controlled environment before rolling them out to production.
If you maintain your own package repository, and control the release of new packages to 
it, ensure => latest can be a useful feature; Puppet will update a package as soon as 
you push a new version to the repo. If you are relying on upstream repositories, such as 
the Ubuntu repositories, it's better to manage the version number directly by specifying an 
explicit version as the value of ensure.
Installing Ruby gems
Although the package resource is most often used to install packages using the  
normal system package manager (in the case of Ubuntu, that's APT), it can install other 
kinds of packages as well. Library packages for the Ruby programming language are known 
as gems. Puppet can install Ruby gems for you using the provider => gem attribute 
(package_gem.pp):
package { 'ruby':
  ensure => installed,
}
package { 'bundler':
  ensure   => installed,
  provider => gem,
}
The bundler package is a Ruby gem, and therefore we have to specify provider => gem 
for this package so that Puppet doesn't think it's a standard system package and try to install 
it via APT. Since the gem provider is not available unless Ruby is installed, we install the ruby 
package first, and then the bundler gem.
Installing gems in Puppet's context
Puppet itself is written at least partly in Ruby, and makes use of several Ruby gems.  
To avoid any conflicts with the version of Ruby and gems which the node might need for 
other applications, Puppet packages its own version of Ruby and associated gems under 
the /opt/puppetlabs directory. This means you can install (or remove) whichever system 
version of Ruby you like, and Puppet will not be affected.

[ 47 ]
However, if you need to install a gem to extend Puppet's capabilities in some way, then doing 
it with a package resource and provider => gem won't work. That is, the gem will be 
installed, but only in the system Ruby context, and it won't be visible to Puppet.
Fortunately, the puppet_gem provider is available for exactly this purpose. When you use 
this provider, the gem will be installed in Puppet's context (and, naturally, won't be visible 
in the system context). The following example demonstrates how to use this provider 
(package_puppet_gem.pp):
package { 'puppet-lint':
  ensure   => installed,
  provider => puppet_gem,
}
To see the gems installed in Puppet's context, use Puppet's own version 
of the gem command, with the following path:
/opt/puppetlabs/puppet/bin/gem list
The puppet-lint tool, by the way, is a good thing to have installed. It will check your 
Puppet manifests for common style errors and make sure they comply with the official 
Puppet style guide. Try it now:
sudo puppet-lint /vagrant/examples/lint_test.pp
WARNING: indentation of => is not properly aligned (expected in column 
11, but found it in column 10) on line 2
In this example, the puppet-lint tool is warning you that the => arrows are not lined up 
vertically, which the style guide says they should be:
file { '/tmp/lint.txt':
  ensure => file,
  content => "puppet-lint is your friend\n",
}
When puppet-lint produces no output, the file is free of lint errors.
Using ensure_packages
To avoid potential package conflicts between different parts of your Puppet code, or 
between your code and third-party modules, the Puppet standard library provides a useful 
wrapper for the package resource, called ensure_packages(). We'll cover this in detail in 
Chapter 7, Mastering modules.

Understanding Puppet resources
[ 48 ]
Services
Although services are implemented in a number of varied and complicated ways at the 
operating system level, Puppet does a good job of abstracting away most of this with the 
service resource and exposing just the two attributes of services which you most commonly 
need to manage: whether they're running (ensure) and whether they start at boot time 
(enable). We covered the use of these in Chapter 2, Creating your first manifests, and most 
of the time, you won't need to know any more about service resources.
However, you'll occasionally encounter services which don't play well with Puppet, for a 
variety of reasons. Sometimes Puppet is unable to detect that the service is already running, 
and keeps trying to start it. Other times, Puppet may not be able to properly restart the 
service when a dependent resource changes. There are a few useful attributes for service 
resources which can help resolve these problems.
The hasstatus attribute
When a service resource has the attribute ensure => running, Puppet needs to be 
able to check whether the service is, in fact, running. The way it does this depends on the 
underlying operating system. On Ubuntu 16 and later, for example, it runs systemctl is-
active SERVICE. If the service is packaged to work with systemd, that should be just fine, 
but in many cases, particularly with older software, it may not respond properly.
If you find that Puppet keeps attempting to start the service on every Puppet run, even 
though the service is running, it may be that Puppet's default service status detection isn't 
working. In this case, you can specify the hasstatus => false attribute for the service 
(service_hasstatus.pp):
service { 'ntp':
  ensure    => running,
  enable    => true,
  hasstatus => false,
}
When hasstatus is false, Puppet knows not to try to check the service status using the 
default system service management command, and instead will look in the process table for 
a running process which matches the name of the service. If it finds one, it will infer that the 
service is running and take no further action.

[ 49 ]
The pattern attribute
Sometimes when using hasstatus => false, the service name as defined in Puppet 
doesn't actually appear in the process table, because the command that provides the service 
has a different name. If this is the case, you can tell Puppet exactly what to look for using the 
pattern attribute.
If hasstatus is false and pattern is specified, Puppet will search for the value of 
pattern in the process table to determine whether or not the service is running. To find  
the pattern you need, you can use the ps command to see the list of running processes:
ps ax
Find the process you're interested in and pick a string which will match only the name of 
that process. For example, if it's ntpd, you might specify the pattern attribute as ntpd 
(service_pattern.pp):
service { 'ntp':
  ensure    => running,
  enable    => true,
  hasstatus => false,
  pattern   => 'ntpd',
}
The hasrestart and restart attributes
When a service is notified (for example, if a file resource uses the notify attribute  
to tell the service that its config file has changed, a common pattern which we looked at in 
Chapter 2, Creating your first manifests), Puppet's default behavior is to stop the service, and 
then start it again. This usually works, but many services implement a restart command 
in their management scripts. If this is available, it's usually a good idea to use it—it may be 
faster or safer than stopping and starting the service. Some services take a while to shut 
down properly when stopped, for example, and Puppet may not wait long enough before 
trying to restart them, so you end up with the service not running at all.

Understanding Puppet resources
[ 50 ]
If you specify hasrestart => true for a service, then Puppet will try to send a restart 
command to it, using whatever service management command is appropriate for the current 
platform (systemctl, for example, on Ubuntu). The following example shows the use of 
hasrestart (service_hasrestart.pp):
service { 'ntp':
  ensure     => running,
  enable     => true,
  hasrestart => true,
}
To further complicate things, the default system service restart command may not work, 
or you may need to take certain special actions when the service is restarted (disabling 
monitoring notifications, for example). You can specify any restart command you like for 
the service using the restart attribute (service_custom_restart.pp):
service { 'ntp':
  ensure  => running,
  enable  => true,
  restart => '/bin/echo Restarting >>/tmp/debug.log && systemctl 
  restart ntp',
}
In this example, the restart command writes a message to a log file before restarting the 
service in the usual way; but it could, of course, do anything you need it to. Note that the 
restart command is only used when Puppet restarts the service (generally because it 
was notified by a change to some config file). It's not used when starting the service from a 
stopped state. If Puppet finds the service stopped, and thus needs to start it, it will use the 
normal system service start command.
In the extremely rare event that the service cannot be stopped or started by using the 
default service management command, Puppet also provides the stop and start attributes 
so that you can specify custom commands to stop and start the service, in just the same way 
as with the restart attribute. If you need to use either of these, though, it's probably safe 
to say that you're having a bad day.

[ 51 ]
Users
A user on UNIX-like systems does not necessarily correspond to a human person who logs 
in and types commands, although it sometimes does. A user is simply a named entity that 
can own files and run commands with certain permissions, and that may or may not have 
permission to read or modify other users' files. It's very common, for sound security reasons, 
to run each service on a system with its own user account. This simply means that the 
service runs with the identity and permissions of that user. 
For example, a web server will often run as the www-data user, which exists solely to 
own the files the web server needs to read and write. This limits the danger of a security 
breach via the web server, because the attacker would only have www-data's permissions, 
which are very limited, rather than root's, which can modify any aspect of the system. It 
is generally a bad idea to run services exposed to the public Internet as the root user. The 
service user should have only the minimum permissions it needs to operate the service.
Given this, an important part of system configuration involves creating and managing users, 
and Puppet's user resource provides a model for doing just that. Just as we saw with 
packages and services, the details of implementation and the commands used to manage 
users vary widely from one operating system to another, but Puppet provides an abstraction 
which hides those details behind a common set of attributes for users.
Creating users
The following example shows a typical user and group declaration in Puppet  
(user.pp):
group { 'devs':
  ensure => present,
  gid    => 3000,
}
user { 'hsing-hui':
  ensure => present,
  uid    => '3001',
  home   => '/home/hsing-hui',
  shell  => '/bin/bash',
  groups => ['devs'],
}

Understanding Puppet resources
[ 52 ]
The user resource
The title of the resource is the username (login name) of the user; in this example,  
hsing-hui. The ensure => present attribute says that the user should exist on  
the system.
The uid attribute needs a little more explanation. On UNIX-like systems, each user has an 
individual numerical ID, known as UID. The text name associated with the user is merely a 
convenience for those (mere humans, for example) who prefer strings to numbers. Access 
permissions are in fact based on the UID and not the username.
Why set the uid attribute? 
Often, when creating users manually, we don't specify a uid, so the 
system assigns one automatically. The problem with this is that if you 
create the same user (hsing-hui, for example) on three different 
nodes, you may end up with three different uids. This would be fine as 
long as you never shared files between nodes, or copied data from one 
place to another. But in fact, this happens all the time, so it's important 
to make sure that a given user's uid is the same across all nodes in your 
infrastructure. That's why we specify the uid attribute in the Puppet 
manifest.
The home attribute sets the user's home directory (this will be the current working directory 
when the user logs in, if she does log in, and also the default working directory for cron jobs 
that run as the user).
The shell attribute specifies the command line shell to run when the user logs in 
interactively. For humans, this will generally be a user shell such as /bin/bash or /bin/
sh. For service users, such as www-data, the shell should be set to /usr/sbin/nologin 
(on Ubuntu systems), which does not allow interactive access, and prints a message saying 
This account is currently not available. All users who do not need to log in 
interactively should have the nologin shell.
If the user needs to be a member of certain groups, you can pass the groups attribute an 
array of the group names (just devs in this example).
Although Puppet supports the password attribute for user resources, I don't advise you 
to use it. Service users don't need passwords, and interactive users should be logging in 
with SSH keys. In fact, you should configure SSH to disable password logins altogether (set 
PasswordAuthentication to no in sshd_config).

[ 53 ]
The group resource
The title of the resource is the name of the group (devs). You need not specify a gid 
attribute but, for the same reasons as the uid attribute, it's a good idea to do so. 
Managing SSH keys
I like to have as few interactive logins as possible on production nodes, because it reduces 
the attack surface. Fortunately, with configuration management, it should rarely be 
necessary to actually log in to a node. The most common reasons for needing an interactive 
login are for system maintenance and troubleshooting, and for deployment. In both cases, 
there should be a single account named for this specific purpose (for example, admin or 
deploy), and it should be configured with the SSH keys of any users or systems that need to 
log in to it.
Puppet provides the ssh_authorized_key resource to control the SSH keys associated 
with a user account. The following example shows how to use ssh_authorized_key  
to add an SSH key (mine, in this instance) to the vagrant user on our Vagrant VM  
(ssh_authorized_key.pp):
ssh_authorized_key { 'john@bitfieldconsulting.com':
  user => 'vagrant',
  type => 'ssh-rsa',
  key  => 'AAAAB3NzaC1yc2EAAAABIwAAAIEA3ATqENg+GWACa2
BzeqTdGnJhNoBer8x6pfWkzNzeM8Zx7/2Tf2pl7kHdbsiTXEUawq
zXZQtZzt/j3Oya+PZjcRpWNRzprSmd2UxEEPTqDw9LqY5S2B8og/
NyzWaIYPsKoatcgC7VgYHplcTbzEhGu8BsoEVBGYu3IRy5RkAcZik=',
}
The title of the resource is the SSH key comment, which reminds us who the key belongs to. 
The user attribute is the user account which this key should be authorized for. The type 
attribute identifies the SSH key type, usually ssh-rsa or ssh-dss. Finally, the key attribute 
sets the key itself.
When this manifest is applied, it adds the following to Vagrant's authorized_keys file:
ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAIEA3ATqENg+GWACa2BzeqTdGnJhNoBer8x6pf
WkzNzeM8Zx7/2Tf2pl7kHdbsiTXEUawqzXZQtZzt/j3Oya+PZjcRpWNRzprSmd2UxEEPT
qDw9LqY5S2B8og/NyzWaIYPsKoatcgC7VgYHplcTbzEhGu8BsoEVBGYu3IRy5RkAcZik= 
john@bitfieldconsulting.com
A user account can have multiple SSH keys associated with it, and anyone holding one of the 
corresponding private keys and its passphrase will be able to log in as that user.

Understanding Puppet resources
[ 54 ]
Removing users
If you need to have Puppet remove user accounts (for example, as part of an employee 
leaving process), it's not enough to simply remove the user resource from the Puppet 
manifest. Puppet will ignore any users on the system that it doesn't know about, and it 
certainly will not remove anything it finds on the system that isn't mentioned in the Puppet 
manifest; that would be extremely undesirable (almost everything would be removed). So 
we need to retain the user declaration for a while, but set the ensure attribute to absent 
(user_remove.pp):
user { 'godot':
  ensure => absent,
}
Once Puppet has run everywhere, you can remove the user resource if you like, but it does 
no harm to simply leave it in place. It's actually a good idea to do this, until you can verify 
manually that the user has been deleted from every affected system.
If you need to prevent a user logging in, but want to retain the account and 
any files owned by the user, for archival or compliance purposes, you can set 
their shell to /usr/sbin/nologin. You can also remove any ssh_
authorized_key resources associated with their account, and set the 
purge_ssh_keys attribute to true on the user resource. This will remove 
any authorized keys for the user that are not managed by Puppet.
Cron resources
Cron is the mechanism on UNIX-like systems which runs scheduled jobs, sometimes  
known as batch jobs, at specified times or intervals. For example, system housekeeping  
tasks such as log rotation, or checking for security updates, are run from cron. The details  
of what to run and when to run it are kept in a specially-formatted file called crontab  
(short for cron table).
Puppet provides the cron resource for managing scheduled jobs, and we saw an example of 
this in the run-puppet manifest we developed in Chapter 3, Managing your Puppet code 
with Git (run-puppet.pp):
cron { 'run-puppet':
  command => '/usr/local/bin/run-puppet',
  hour    => '*',
  minute  => '*/15',
}

[ 55 ]
The title run-puppet identifies the cron job (Puppet writes a comment to the crontab 
file containing this name, to distinguish it from other, manually-configured cron jobs). The 
command attribute is the command for the cron resource to run, and the hour and minute 
attributes specify the time (*/15 is cron syntax meaning every 15 minutes).
For more information about cron and the possible ways to 
specify the times of scheduled jobs, run the man 5 crontab 
command.
Attributes of the cron resource
The cron resource has a few other useful attributes which are shown in the following 
example (cron.pp):
cron { 'cron example':
  command     => '/bin/date +%F',
  user        => 'vagrant',
  environment => ['MAILTO=john@bitfieldconsulting.com', 'PATH=/bin'],
  hour        => '0',
  minute      => '0',
  weekday     => ['Saturday', 'Sunday'],
}
The user attribute specifies who should run the cron job (if none is specified, the job runs as 
root). If the environment attribute is given, it sets any environment variables the cron job 
might need. A common use for this is to e-mail any output from the cron job to a specified 
e-mail address, using the MAILTO variable.
As before, the hour and minute attributes set the time for the job to run, while you can 
use the weekday attribute to specify a particular day, or days, of the week. (The monthday 
attribute works the same way, and can take any range or array of values between 1-31 to 
specify the day of the month.)
One important point about cron scheduling is that the default value for any 
schedule attribute is *, which means all allowed values. For example, if you 
do not specify an hour attribute, the cron job will be scheduled with an 
hour of *, meaning that it will run every hour. This is generally not what you 
want. If you do want it to run every hour, specify hour => '*' in your 
manifest, but otherwise, specify the particular hour it should run at. The 
same goes for minute. Accidentally leaving out the minute attribute and 
having a job run 60 times an hour can have amusing consequences, to say 
the least.

Understanding Puppet resources
[ 56 ]
Randomizing cron jobs
If you run a cron job on many nodes, it's a good idea to make sure that the job doesn't  
run everywhere at the same time. Puppet provides a built-in function, fqdn_rand(),  
to help with this; it provides a random number up to a specified maximum value, which  
will be different on each node, because the random number generator is seeded with the 
node's hostname.
If you have several such jobs to run, you can also supply a further seed value to the fqdn_
rand() function, which can be any string, and which will ensure that the value is different 
for each job (fqdn_rand.pp):
cron { 'run daily backup':
  command => '/usr/local/bin/backup',
  minute  => '0',
  hour    => fqdn_rand(24, 'run daily backup'),
}
cron { 'run daily backup sync':
  command => '/usr/local/bin/backup_sync',
  minute  => '0',
  hour    => fqdn_rand(24, 'run daily backup sync'),
}
Because we gave a different string as the second argument to fqdn_rand for each cron job, 
it will return a different random value for each hour attribute.
The range of values returned by fqdn_rand() includes 0, but does not include the 
maximum value you specify. So in the previous example, the values for hour will be  
between 0 and 23, inclusive.
Removing cron jobs
Just as with user resources, or any type of resource, removing the resource declaration from 
your Puppet manifest does not remove the corresponding configuration from the node. In 
order to do that, you need to specify ensure => absent on the resource.

[ 57 ]
Exec resources
While the other resource types we've seen so far (file, package, service, user,  
ssh_authorized_key, and cron) have modeled some concrete piece of state on  
the node, such as a file, the exec resource is a little different. An exec allows you  
to run any arbitrary command on the node. This might create or modify the state,  
or it might not; anything you can run from the command line, you can run via an  
exec resource.
Automating manual interaction
The most common use for an exec resource is to simulate manual interaction on the 
command line. For example, some older software is not packaged for modern operating 
systems, and need to be compiled and installed from source, which requires you to run certain 
commands. The authors of some software have also not realized, or don't care, that users may 
be trying to install their product automatically, and have install scripts which prompt for user 
input. This can require the use of exec resources to work around the problem.
Attributes of the exec resource
The following example shows an exec resource for building and installing an imaginary piece 
of software (exec.pp):
exec { 'install-cat-picture-generator':
  cwd     => '/tmp/cat-picture-generator',
  command => '/tmp/cat-picture/generator/configure && /usr/bin/make 
install',
  creates => '/usr/local/bin/cat-picture-generator',
}
The title of the resource can be anything you like, though as usual with Puppet resources, it 
must be unique. I tend to name exec resources after the problem they're trying to solve, as 
in this example.
The cwd attribute sets the working directory where the command will be run (current 
working directory). When installing software, this is generally the software source directory.
The command attribute gives the command to run. This must be the full path to the 
command, but you can chain several commands together using the && shell operator. This 
executes the next command only if the previous one succeeded. So in the example, if the 
configure command completes successfully, Puppet will go on to run make install 
command; otherwise, it will stop with an error.

Understanding Puppet resources
[ 58 ]
If you apply this example, Puppet will give you an error like the following:
Error: /Stage[main]/Main/Exec[install-cat-picture-
generator]/returns: change from notrun to 0 failed: 
Could not find command '/tmp/cat-picture/generator/
configure'
That's expected, because the specified command does not in fact exist. In 
your own manifests, you may see this error if you give the wrong path to a 
command, or if the package that provides the command hasn't been installed 
yet.
The creates attribute specifies a file which should exist after the command has been run. 
If this file is present, Puppet will not run the command again. This is very useful because, 
without a creates attribute, an exec resource will run every time Puppet runs, which is 
generally not what you want. The creates attribute tells Puppet, in effect, Run the exec 
only if this file doesn't exist.
Let's see how this works, imagining that this exec is being run for the first time. We  
assume that the /tmp/cat-picture directory exists and contains the source of the  
cat-picture-generator application:
1.	 Puppet checks the creates attribute and sees that the /usr/local/bin/cat-
picture-generator file is not present, therefore the exec must be run. 
2.	 Puppet runs the /tmp/cat-picture-generator/configure && /usr/bin/
make install command. As a side-effect of this command, the /usr/local/
bin/cat-picture-generator file is created.
3.	 Next time Puppet runs, it again checks the creates attribute. This time, /usr/
local/bin/cat-picture-generator exists, so Puppet does nothing. 
This exec will never be applied again, so long as the file specified in the creates attribute 
exists. You can test this by deleting the file and applying Puppet again. The exec will be 
triggered, and the file recreated.
Make sure that your exec resources always include a creates attribute (or a 
similar control attribute such as onlyif or unless, which we'll look at later 
in this chapter). Without this, the exec command will be run every time Puppet 
runs, which is almost certainly not what you want.
Note that building and installing software from source is not a recommended practice for 
production systems. It's better to build the software on a dedicated build server (perhaps 
using Puppet code similar to this example), create a system package for it, and then use 
Puppet to install that package on the production nodes.

[ 59 ]
The user attribute
If you don't specify a user attribute for the exec resource, Puppet will run the command 
as the root user. This is often appropriate for installing system software or making changes 
to the system configuration. However, if you need the command to run as a particular user, 
specify the user attribute, as in the following example (exec_user.pp):
exec { 'say-hello':
  command => '/bin/echo Hello, this is `whoami` >/tmp/hello-vagrant.
txt',
  user    => 'vagrant',
  creates => '/tmp/hello-vagrant.txt',
}
This will run the specified command as the vagrant user. The whoami command  
returns the name of the user running it, so when you apply this manifest, the file  
/tmp/hello-vagrant.txt will be created with the following contents:
Hello, this is vagrant
As with the earlier example, the creates attribute prevents Puppet from running this 
command more than once.
The onlyif and unless attributes
Suppose you only want an exec resource to be applied under certain conditions. For 
example, a command which processes incoming data files only needs to run if there are data 
files waiting to be processed. In this case, it's no good adding a creates attribute; we want 
the existence of a certain file to trigger the exec, not prevent it.
The onlyif attribute is a good way to solve this problem. It specifies a command for Puppet 
to run, and the exit status from this command determines whether or not the exec will be 
applied. On UNIX-like systems, commands generally return an exit status of zero to indicate 
success, and a non-zero value for failure. The following example shows how to use onlyif in 
this way (exec_onlyif.pp):
exec { 'process-incoming-cat-pictures':
  command => '/usr/local/bin/cat-picture-generator --import /tmp/
incoming/*',
  onlyif  => '/bin/ls /tmp/incoming/*',
}
The exact command isn't important here, but let's assume it's something that we would only 
want to run if there are any files in the /tmp/incoming directory.

Understanding Puppet resources
[ 60 ]
The onlyif attribute specifies the check command which Puppet should run first, to 
determine whether or not the exec needs to be applied. If there is nothing in the /tmp/
incoming directory, then ls /tmp/incoming/* will return a non-zero exit status. Puppet 
interprets this as a failure, so does not apply the exec.
On the other hand, if there are files in the /tmp/incoming directory, the ls command 
will return a successful exit status. This tells Puppet that the exec must be applied, so it 
proceeds to run the /usr/local/bin/cat-picture-generator command (and we can 
assume this command deletes the incoming files after processing).
You can think of the onlyif attribute as telling Puppet to Run the exec only if this 
command succeeds.
The unless attribute is exactly the same as onlyif, but with the opposite sense. If 
you specify a command to the unless attribute, the exec will always be run unless the 
command returns a zero exit status. You can think of unless as telling Puppet to Run the 
exec unless this command succeeds.
When you run Puppet, if you see an exec running every time which shouldn't be, check 
whether it specifies a creates, unless, or onlyif attribute. If so, the creates attribute 
may be looking for the wrong file, or the unless or onlyif check command may not be 
returning what you expect. You can see what command is being run, and what output it 
generates, by running sudo puppet apply with the -d (debug) flag:
sudo puppet apply -d exec_onlyif.pp
Debug: Exec[process-incoming-cat-pictures](provider=posix): Executing 
check '/bin/ls /tmp/incoming/*'
Debug: Executing: '/bin/ls /tmp/incoming/*'
Debug: /Stage[main]/Main/Exec[process-incoming-cat- 
pictures]/onlyif: /tmp/incoming/foo
The refreshonly attribute
It's quite common to use exec resources for one-off commands, such as rebuilding a 
database, or setting a system tunable parameter. These generally only need to be triggered 
once, when a package is installed, or occasionally, when a config file is updated. If an 
exec needs to run only when some other Puppet resource is changed, we can use the 
refreshonly attribute to do this.

[ 61 ]
If refreshonly is true, the exec will never be applied unless another resource triggers  
it with notify. In the following example, Puppet manages the /etc/aliases file  
(which maps local usernames to e-mail addresses), and a change to this file triggers  
the execution of the newaliases command, which rebuilds the system alias database 
(exec_refreshonly.pp):
file { '/etc/aliases':
  content => 'root: john@bitfieldconsulting.com',
  notify  => Exec['newaliases'],
}
exec { 'newaliases':
  command     => '/usr/bin/newaliases',
  refreshonly => true,
}
When this manifest is applied for the first time, the /etc/aliases resource causes a 
change to the file's contents, so Puppet sends a notify message to the exec resource. This 
causes the newaliases command to be run. If you apply the manifest again, you will see 
that the aliases file is not changed, so the exec is not run.
While the refreshonly attribute is occasionally extremely useful, overuse 
of it can make your Puppet manifests hard to understand and debug, and it 
can also be rather fragile. Felix Frank makes this point in a blog post:
Friends don't let friends use refreshonly:
With the exec resource type considered the last ditch, its refreshonly 
parameter should be seen as especially outrageous. To make an exec 
resource fit into Puppet's model better, you should use [the creates, 
onlyif, or unless] parameters instead.
(http://ffrank.github.io/misc/2015/05/26/friends-
don't-let-friends-use-refreshonly/)
Note that you don't need to use the refreshonly attribute in order to make the exec 
notifiable by other resources. Any resource can notify an exec in order to make it run; 
however, if you don't want it to run unless it's notified, use refreshonly.
(By the way, if you actually want to manage mail aliases on a node, use Puppet's 
built-in mailalias resource. The previous example is just to demonstrate the use of 
refreshonly.)

Understanding Puppet resources
[ 62 ]
The logoutput attribute
When Puppet runs shell commands via an exec resource, the output is normally hidden 
from us. However, if the command doesn't seem to be working properly, it can be very  
useful to see what output it produced, as this usually tells us why it didn't work.
The logoutput attribute determines whether Puppet will log the output of the exec 
command along with the usual informative Puppet output. It can take three values: true, 
false, or on_failure.
If logoutput is set to on_failure (which is the default), Puppet will only log command 
output when the command fails (that is, returns a non-zero exit status). If you never want to 
see command output, set it to false.
Sometimes, however, the command returns a successful exit status, but does not appear 
to do anything. Setting logoutput to true will force Puppet to log the command output 
regardless of the exit status, which should help you figure out what's going on.
The timeout attribute
Sometimes commands can take a long time to run, or never terminate at all. By default, 
Puppet allows an exec command to run for 300 seconds, at which point Puppet terminates 
it if it has not finished. If you need to allow a little longer for the command to complete, you 
can use the timeout attribute to set this. The value is the maximum execution time for the 
command in seconds.
Setting a timeout value of 0 disables the automatic timeout altogether, and allows the 
command to run forever. This should be a last resort, as a command which blocks or hangs 
could stop Puppet's automatic runs altogether if no timeout is set. To find a suitable value for 
timeout, try running the command a few times, and choose a value which is perhaps twice 
as long as a typical run. This should avoid failures caused by slow network conditions, for 
example, but not block Puppet from running altogether.
How not to misuse exec resources
The exec resource can do anything to the system that you could do from the command line. 
As you can imagine, such a powerful tool can be misused. In theory, Puppet is a declarative 
language: the manifest specifies the way things should be, and it is up to Puppet to take the 
necessary actions to make them so. Manifests are therefore what computer scientists call 
idempotent: the system is always in the same state after the catalog is applied, and however 
many times you apply it, it will always be in that state.

[ 63 ]
The exec resource rather spoils this theoretical picture, by allowing Puppet manifests to 
have side-effects. Since your exec command can do anything, it could, for example, create 
a new 1GB file on disk with a random name, and since this will happen every time Puppet 
runs, you could rapidly run out of disk space. It's best to avoid commands with side-effects 
like this. In general, there's no way to know from within Puppet exactly what changes to a 
system were caused by an exec resource.
Commands run via exec are also sometimes used to bypass Puppet's existing resources.  
For example, if the user resource doesn't quite do what you want for some reason, you 
could create a user by running the adduser command directly from an exec. This is also a 
bad idea, since by doing this you lose the declarative and cross-platform nature of Puppet's 
built-in resources. The exec resources potentially change the state of the node in a way 
that's invisible to Puppet's catalog.
In general, if you need to manage a concrete aspect of system state which 
isn't supported by Puppet's built-in resource types, you should think about 
creating a custom resource type and provider to do what you want. This 
extends Puppet to add a new resource type, which you can then use to 
model the state of that resource in your manifests.
Creating custom types and providers is an advanced topic and not 
covered in this book, but if you want to know more, consult the Puppet 
documentation at
https://docs.puppet.com/guides/custom_types.html
You should also think twice before running complex commands via exec,  
especially commands which use loops or conditionals. It's a better idea to put  
any complicated logic in a shell script (or, even better, in a real programming language), 
which you can then deploy and run with Puppet (avoiding, as  
we've said, unnecessary side-effects).
As a matter of good Puppet style, every exec resource should have at least 
one of creates, onlyif, unless, or refreshonly specified, to stop 
it being applied on every Puppet run. If you find yourself using exec just to 
run a command every time Puppet runs, make it a cron job instead.

Understanding Puppet resources
[ 64 ]
Summary
We've explored Puppet's file resource in detail, covering file sources, ownership, 
permissions, directories, symbolic links, and file trees. We've learned how to manage 
packages by installing specific versions, or the latest version, and how to uninstall packages. 
We've covered Ruby gems, both in the system context and Puppet's internal context. Along 
the way, we met the very useful puppet-lint tool.
We have looked at service resources, including the hasstatus, pattern, hasrestart, 
restart, stop, and start attributes. We've learned how to create users and groups, 
manage home directories, shells, UIDs, and SSH authorized keys. We've seen how to 
schedule, manage, and remove cron jobs.
Finally, we've learned all about the powerful exec resource, including how to run arbitrary 
commands, and how to run commands only under certain conditions, or only if a specific 
file is not present. We've seen how to use the refreshonly attribute to trigger an exec 
when other resources are updated, and we've explored the useful logoutput and timeout 
attributes of exec resources.
In the next chapter, we'll find out how to represent data and variables in Puppet manifests, 
including strings, numbers, Booleans, arrays, and hashes. We'll learn how to use variables 
and conditional expressions to determine which resources are applied, and we'll also learn 
about Puppet's facts hash and how to use it to get information about the system.

[ 65 ]
Variables, expressions,  
and facts
It is impossible to begin to learn that which one thinks one already knows.
—Epictetus
In this chapter, you will learn about Puppet variables and data types, expressions, and 
conditional statements. You will also learn how Puppet manifests can get data about the 
node using Facter, find out which are the most important standard facts, and see how to 
create your own external facts. Finally, you will use Puppet's each function to iterate over 
arrays and hashes, including Facter data.
5

Variables, expressions, and facts
[ 66 ]
Introducing variables
A variable in Puppet is simply a way of giving a name to a particular value, which we can 
then use wherever we would use the literal value (variable_string.pp):
$php_package = 'php7.0-cli'
package { $php_package:
  ensure => installed,
}
The dollar sign ($) tells Puppet that what follows is a variable name. Variable names must 
begin with a lowercase letter or an underscore, though the rest of the name can contain 
uppercase letters or numbers as well.
A variable can contain different types of data—one such type is a String (like php7.0-cli), 
but Puppet variables can also contain Number values, or Boolean values (true or false). 
Here are a few examples (variable_simple.pp):
$my_name = 'Zaphod Beeblebrox'
$answer = 42
$scheduled_for_demolition = true
Using Booleans
Strings and numbers are straightforward, but Puppet also has a special data type to 
represent true or false values, which we call Boolean values, after the logician George 
Boole. We have already encountered some Boolean values in Puppet resource attributes 
(service.pp):
service { 'sshd':
  ensure => running,
  enable => true,
}
The only allowed values for Boolean variables are the literal values true and false, but 
Boolean variables can also hold the values of conditional expressions (expressions whose 
value is true or false), which we'll explore later in this chapter.

[ 67 ]
You might be wondering what type the value running is in the previous 
example. It's actually a string, but a special, unquoted kind of string called 
a bare word. Although it would be exactly the same to Puppet if you used a 
normal quoted string 'running' here, it' s considered good style to use 
bare words for attribute values which can only be one of a small number 
of words (for example, the ensure attribute on services can only take the 
values running or stopped). By contrast, true is not a bare word, but a 
Boolean value, and it is not interchangeable with the string 'true'. Always 
use the unquoted literal values true or false for Boolean values.
Interpolating variables in strings
It's no good being able to store something in a variable if you can't get it out again, and 
one of the most common ways to use a variable's value is to interpolate it in a string. 
When you do this, Puppet inserts the current value of the variable into the contents of the 
string, replacing the name of the variable. String interpolation looks like this (string_
interpolation.pp):
$my_name = 'John'
notice("Hello, ${my_name}! It's great to meet you!")
When you apply this manifest, the following output is printed:
Notice: Scope(Class[main]): Hello, John! It's great to meet you!
To interpolate (that is, to insert the value of) a variable in a string, prefix its name with a $ 
character and surround it with curly braces ({}). This tells Puppet to replace the variable's 
name with its value in the string.
We sneaked a new Puppet function, notice(), into the previous 
example. It has no effect on the system, but it prints out the value of 
its argument. This can be very useful for troubleshooting problems, 
or finding out what the value of a variable is at a given point in your 
manifest.
Creating arrays
A variable can also hold more than one value. An Array is an ordered sequence of values, 
each of which can be of any type. The following example creates an array of Integer values 
(variable_array.pp):
$heights = [193, 120, 181, 164, 172]
$first_height = $heights[0]

Variables, expressions, and facts
[ 68 ]
You can refer to any individual element of an array by giving its index number in square 
brackets, where the first element is index [0], the second is [1], and so on. (If you find this 
confusing, you're not alone, but it may help to think of the index as representing an offset 
from the beginning of the array. Naturally, then, the offset of the first element is 0.)
Declaring arrays of resources
You already know that in Puppet resource declarations, the title of the resource is usually 
a string, such as the path to a file, or the name of a package. You might well ask—what 
happens if you supply an array of strings as the title of a resource, instead of a single 
string? Does Puppet create multiple resources, one for each element in the array? Let's try 
an experiment where we do exactly that with an array of package names, and see what 
happens (resource_array.pp):
$dependencies = [
  'php7.0-cgi',
  'php7.0-cli',
  'php7.0-common',
  'php7.0-gd',
  'php7.0-json',
  'php7.0-mcrypt',
  'php7.0-mysql',
  'php7.0-soap',
]
package { $dependencies:
  ensure => installed,
}
If our intuition is right, applying the previous manifest should give us a package resource for 
each package listed in the $dependencies array, and each one should be installed. Here's 
what happens when the manifest is applied:
sudo apt-get update
sudo puppet apply /vagrant/examples/resource_array.pp
Notice: Compiled catalog for localhost in environment production in 
0.68 seconds
Notice: /Stage[main]/Main/Package[php7.0-cgi]/ensure: created
Notice: /Stage[main]/Main/Package[php7.0-cli]/ensure: created
Notice: /Stage[main]/Main/Package[php7.0-common]/ensure: created
Notice: /Stage[main]/Main/Package[php7.0-gd]/ensure: created
Notice: /Stage[main]/Main/Package[php7.0-json]/ensure: created
Notice: /Stage[main]/Main/Package[php7.0-mcrypt]/ensure: created

[ 69 ]
Notice: /Stage[main]/Main/Package[php7.0-mysql]/ensure: created
Notice: /Stage[main]/Main/Package[php7.0-soap]/ensure: created
Notice: Applied catalog in 56.98 seconds
Giving an array of strings as the title of a resource results in Puppet creating multiple 
resources, all identical except for the title. You can do this not just with packages, but also 
with files, users, or, in fact, any type of resource. We'll see some even more sophisticated 
ways of creating resources from data in Chapter 6, Managing data with Hiera.
Why did we run the sudo apt-get update command before applying 
the manifest? This is the Ubuntu command to update the system's local 
package catalogue from the upstream servers. It's always a good idea to run 
this before installing any package, to make sure you're installing the latest 
version. In your production Puppet code, of course, you can run this via an 
exec resource.
Understanding hashes
A Hash, also known as a dictionary in some programming languages, is like an array, but 
instead of just being a sequence of values, each value has a name (variable_hash.pp):
$heights = {
  'john'    => 193,
  'rabiah'  => 120,
  'abigail' => 181,
  'melina'  => 164,
  'sumiko'  => 172,
}
notice("John's height is ${heights['john']}cm.")
The name for each value is known as a key. In the example, the keys of this hash are john, 
rabiah, abigail, melina, and sumiko. To look up the value of a given key, you put the 
key in square brackets after the hash name: $heights['john'].

Variables, expressions, and facts
[ 70 ]
Puppet style note
Did you spot the trailing comma on the last hash key-value pair, and the 
last element of the array in the previous example? Although the comma 
isn't strictly required, it's good style to add one. The reason is that it's very 
common to want to add another item to an array or hash, and if your last 
item already has a trailing comma, you won't have to remember to add 
one when extending the list.
Setting resource attributes from a hash
You might have noticed that a hash looks a lot like the attributes of a resource; it's a  
one-to-one mapping between names and values. Wouldn't it be convenient if, when 
declaring resources, we could just specify a hash containing all the attributes and their 
values? As it happens, you can do just that (hash_attributes.pp):
$attributes = {
  'owner' => 'vagrant',
  'group' => 'vagrant',
  'mode'  => '0644',
}
file { '/tmp/test':
  ensure => present,
  *      => $attributes,
}
The * character, cheerfully named the attribute splat operator, tells Puppet to treat the 
specified hash as a list of attribute-value pairs to apply to the resource. This is exactly 
equivalent to specifying the same attributes directly, as in the following example:
file { '/tmp/test':
  ensure => present,
  owner  => 'vagrant',
  group  => 'vagrant',
  mode   => '0644',
}

[ 71 ]
Introducing expressions
Variables are not the only things in Puppet that have a value. Expressions also have a value. 
The simplest expressions are just literal values:
42
true
'Oh no, not again.'
You can combine numeric values with arithmetic operators, such as +, -, *, and /, to create 
arithmetic expressions, which have a numeric value, and you can use these to have Puppet 
do calculations (expression_numeric.pp):
$value = (17 * 8) + (12 / 4) - 1
notice($value)
The most useful expressions, though, are those which evaluate to true or false, known as 
Boolean expressions. The following is a set of examples of Boolean expressions, all of which 
evaluate to true (expression_boolean.pp):
notice(9 < 10)
notice(11 > 10)
notice(10 >= 10)
notice(10 <= 10)
notice('foo' == 'foo')
notice('foo' in 'foobar')
notice('foo' in ['foo', 'bar'])
notice('foo' in { 'foo' => 'bar' })
notice('foo' =~ /oo/)
notice('foo' =~ String)
notice(1 != 2)
Meeting Puppet's comparison operators
All the operators in the Boolean expressions shown in the previous example are known as 
comparison operators, because they compare two values. The result is either true or false. 
The following are the comparison operators that Puppet provides:


==, != (equal, not equal).


>, >=, <, <= (greater than, greater than or equal to, less than, less than or equal to).


A in B (A is a substring of B, A is an element of the array B, or A is a key of the  
hash B).


A =~ B (A is matched by the regular expression B, or A is a value of data type B. 
For example, the expression 'hello' =~ String' is true, because the value 
'hello' is of type String).

Variables, expressions, and facts
[ 72 ]
Introducing regular expressions
The =~ operator tries to match a given value against a regular expression. A regular 
expression (regular in the sense of constituting a pattern or a rule) is a special kind of 
expression which specifies a set of strings. For example, the regular expression /a+/ 
describes the set of all strings which contain one or more consecutive as: a, aa, aaa, and so 
on, as well as all strings which contain such a sequence among other characters. The slash 
characters // delimit a regular expression in Puppet.
When we say a regular expression matches a value, we mean the value is one of the set of 
strings specified by the regular expression. The regular expression /a+/ would match the 
string aaa, or the string Aaaaargh!, for example.
The following example shows some regular expressions which all match the string foo 
(regex.pp):
$candidate = 'foo'
notice($candidate =~ /foo/) # literal
notice($candidate =~ /f/)   # substring
notice($candidate =~ /f.*/) # f followed by zero or more characters
notice($candidate =~ /f.o/) # f, any character, o
notice($candidate =~ /fo+/) # f followed by one or more 'o's
notice($candidate =~ /[fgh]oo/) # f, g, or h followed by 'oo'
Regular expressions are a more-or-less standard language for expressing string 
patterns. It's a complicated and powerful language, which really deserves 
a book of its own (and there are several), but suffice it to say for now that 
Puppet's regular expression syntax is the same as that used in the Ruby 
language, and you can read more about it in the Ruby documentation at 
http://ruby-doc.org/core/Regexp.html.
Using conditional expressions
Boolean expressions, like those in the previous example, are useful because we can use 
them to make choices in the Puppet manifest. We can apply certain resources only if a given 
condition is met, or we can assign an attribute one value or another, depending on whether 
some expression is true. An expression used in this way is called a conditional expression.

[ 73 ]
Making decisions with if statements
The most common use of a conditional expression is in an if statement. The following 
example shows how to use if to decide whether to apply a resource (if.pp):
$install_perl = true
if $install_perl {
  package { 'perl':
    ensure => installed,
  }
} else {
  package { 'perl':
    ensure => absent,
  }
}
You can see that the value of the Boolean variable $install_perl governs whether  
or not the perl package is installed. If $install_perl is true, Puppet will apply the 
following resource:
  package { 'perl':
    ensure => installed,
  }
If, on the other hand, $install_perl is false, the resource applied will be:
  package { 'perl':
    ensure => absent,
  }
You can use the if statements to control the application of any number of resources, or, 
indeed, any part of your Puppet manifest. You can leave out the else clause if you like; in 
that case, when the value of the conditional expression is false, Puppet will do nothing.
Choosing options with case statements
The if statement allows you to take a yes/no decision based on the value of a Boolean 
expression. But if you need to make a choice among more than two options, you can  
use a case statement instead (case.pp):
$webserver = 'nginx'
case $webserver {
  'nginx': {

Variables, expressions, and facts
[ 74 ]
    notice("Looks like you're using Nginx! Good choice!")
  }
  'apache': {
    notice("Ah, you're an Apache fan, eh?")
  }
  'IIS': {
    notice('Well, somebody has to.')
  }
  default: {
    notice("I'm not sure which webserver you're using!")
  }
}
In a case statement, Puppet compares the value of the expression to each of the cases 
listed, in order. If it finds a match, the corresponding resources are applied. The special case 
called default always matches, and you can use it to make sure that Puppet does the right 
thing even if none of the other cases match.
Finding out facts
It's very common for Puppet manifests to need to know something about the system they're 
running on; for example, its hostname, IP address, or operating system version. Puppet's 
built-in mechanism for getting system information is called Facter, and each piece of 
information provided by Facter is known as a fact.
Using the facts hash
You can access Facter facts in your manifest using the facts hash. This is a Puppet variable 
called $facts which is available everywhere in the manifest, and to get a particular fact, you 
supply the name of the fact you want as the key (facts_hash.pp):
notice($facts['kernel'])
On the Vagrant box, or any Linux system, this will return the value Linux.
In older versions of Puppet, each fact was a distinct global variable, like this:
notice($::kernel)
You will still see this style of fact reference in some Puppet code, though it is now deprecated 
and will eventually stop working, so you should always use the $facts hash instead.

[ 75 ]
Running the facter command
You can also use the facter command to see the value of particular facts, or just to see 
what facts are available. For example, running facter os on the command line will show 
you the hash of available OS-related facts:
facter os
{
  architecture => "amd64",
  distro => {
    codename => "xenial",
    description => "Ubuntu 16.04 LTS",
    id => "Ubuntu",
    release => {
      full => "16.04",
      major => "16.04"
    }
  },
  family => "Debian",
  hardware => "x86_64",
  name => "Ubuntu",
  release => {
    full => "16.04",
    major => "16.04"
  },
  selinux => {
    enabled => false
  }
}
You can also use the puppet facts command to see what facts will be available to Puppet 
manifests. This will also include any custom facts defined by third-party Puppet modules (see 
Chapter 7, Mastering modules, for more information about this).
Accessing hashes of facts
As in the previous example, many facts actually return a hash of values, rather than a single 
value. The value of the $facts['os'] fact is a hash with the keys architecture, distro, 
family, hardware, name, release, and selinux. Some of those are also hashes; it's 
hashes all the way down!

Variables, expressions, and facts
[ 76 ]
As you know, to access a particular value in a hash, you specify the key name in square 
brackets. To access a value inside a hash inside a hash, you add another key name in square 
brackets after the first, as in the following example (facts_architecture.pp):
notice($facts['os']['architecture'])
You can keep on appending more keys to get more and more specific information  
(facts_distro_codename.pp):
notice($facts['os']['distro']['codename'])
Key fact 
The operating system major release is a very handy fact, and one you'll 
probably use often:
$facts['os']['release']['major']
Referencing facts in expressions
Just as with ordinary variables or values, you can use facts in expressions, including 
conditional expressions (fact_if.pp):
if $facts['os']['selinux']['enabled'] {
  notice('SELinux is enabled')
} else {
  notice('SELinux is disabled')
}
Although conditional expressions based on facts can be useful, an 
even better way of making decisions based on facts in your manifests 
is to use Hiera, which we'll cover in the next chapter. For example, if 
you find yourself writing an if or case statement which chooses 
different resources depending on the operating system version, 
consider using a Hiera query instead.
Using memory facts
Another useful set of facts is that relating to system memory. You can find out the total 
physical memory available and the amount of memory currently used, as well as the same 
figures for swap memory.

[ 77 ]
One common use for this is to configure applications dynamically based on the amount 
of system memory. For example, the MySQL parameter innodb_buffer_pool_size 
specifies the amount of memory allocated to database query cache and indexes, and it 
should generally be set as high as possible (as large a value as practical, leaving enough 
memory for other processes on the server to run without excessive paging, according to 
the documentation). So you might decide to set this to three-quarters of total memory (for 
example), using a fact and an arithmetic expression, as in the following snippet (fact_
memory.pp):
$buffer_pool = $facts['memory']['system']['total_bytes'] * 3/4
notice("innodb_buffer_pool_size=${buffer_pool}")
Key fact
The total system memory fact will help you calculate configuration 
parameters which vary as a fraction of memory.
$facts['memory']['system']['total_bytes']
Discovering networking facts
Most applications use the network, so you'll find Facter's network-related facts very useful 
for anything to do with network configuration. The most commonly-used facts are system 
hostname, fully qualified domain name (FQDN), and IP address (fact_networking.pp):
notice("My hostname is ${facts['hostname']}")
notice("My FQDN is ${facts['fqdn']}")
notice("My IP is ${facts['networking']['ip']}")
Key fact
The system hostname is something you'll need to refer to often in your 
manifests.
$facts['hostname']
Providing external facts
While the built-in facts available to Puppet provide a lot of important information, you can 
make the $facts hash even more useful by extending it with your own facts, known as 
external facts. For example, if nodes are located in different cloud providers, each of which 
requires a slightly different networking setup, you could create a custom fact called cloud to 
document this. You can then use this fact in manifests to make decisions.

Variables, expressions, and facts
[ 78 ]
Puppet looks for external facts in the /opt/puppetlabs/facter/facts.d directory. 
Try creating a file in that directory called facts.txt, with the following content (fact_
external.txt):
cloud=aws
A quick way to do this is to run the following command:
sudo cp /vagrant/examples/fact_external.txt /opt/puppetlabs/facter/
facts.d
The cloud fact is now available in your manifests. You can check that the fact is working by 
running the following command:
sudo facter cloud
aws
To use the fact in your manifest, query the $facts hash just as you would for a built-in fact 
(fact_cloud.pp):
case $facts['cloud'] {
  'aws': {
    notice('This is an AWS cloud server ')
  }
  'gcp': {
    notice('This is a Google cloud server')
  }
  default: {
    notice("I'm not sure which cloud I'm in!")
  }
}
You can put as many facts in a single text file as you like, or you can have each fact in a 
separate file; it doesn't make any difference. Puppet will read all the files in the facts.d 
directory and extract all the key=value pairs from each one.
Text files work well for simple facts (those that return a single value). If your external facts 
need to return structured data (arrays or hashes, for example), you can use a YAML or a JSON 
file instead to do this. We'll be learning more about YAML in the next chapter, but for now, if 
you need to build structured external facts, consult the Puppet documentation for details.
It's common to set up external facts like this at build time, perhaps as part of an automated 
bootstrap script (see Chapter 12, Putting it all together for more about the bootstrap 
process).

[ 79 ]
Creating executable facts
External facts are not limited to static text files. They can also be the output of scripts or 
programs. For example, you could write a script that calls a web service to get some data, 
and the result would be the value of the fact. These are known as executable facts.
Executable facts live in the same directory as other external facts (/opt/puppetlabs/
facter/facts.d), but they are distinguished by having the execute bit set on their files 
(recall that files on UNIX-like systems each have a set of bits indicating their read, write, 
and execute permissions) and also that they can't be named with .txt, .yaml, or .json 
extensions. Let's build an executable fact which simply returns the current date, as an 
example:
1.	 Run the following command to copy the executable fact example into the external 
fact directory:
sudo cp /vagrant/examples/date.sh /opt/puppetlabs/facter/facts.d
2.	 Set the execute bit on the file with the following command:
sudo chmod a+x /opt/puppetlabs/facter/facts.d/date.sh
3.	 Now test the fact:
sudo facter date
2017-04-12
Here is the script which generates this output (date.sh):
#!/bin/bash
echo "date=`date +%F`"
Note that the script has to output date= before the actual date value. This is because 
Facter expects executable facts to output a list of key=value pairs (just one such pair, in 
this case). key is the name of the fact (date), and value is whatever is returned by `date 
+%F` (the current date in the ISO 8601 format). By the way, you should use the ISO 8601 
format (YYYY-MM-DD) whenever you need to represent dates, because it's not only the 
international standard date format, but is also unambiguous and sorts alphabetically. 
As you can see, executable facts are quite powerful, because they can return any information 
which can be generated by a program (the program could make network requests or 
database queries, for example). However, you should use executable facts with care, as 
Puppet has to evaluate all external facts on the node every time it runs, which means 
running every script in /opt/puppetlabs/facter/facts.d.

Variables, expressions, and facts
[ 80 ]
If you don't need the information from an executable fact to be 
regenerated every time Puppet runs, consider running the script from a 
cron job at longer intervals and having it write output to a static text file 
in the facts directory instead.
Iterating over arrays
Iteration (doing something repeatedly) is a useful technique in your Puppet manifests to avoid 
lots of duplicated code. For example, consider the following manifest, which creates several 
files with identical properties (iteration_simple.pp):
file { '/usr/local/bin/task1':
  content => "echo I am task1\n",
  mode    => '0755',
}
file { '/usr/local/bin/task2':
  content => "echo I am task2\n",
  mode    => '0755',
}
file { '/usr/local/bin/task3':
  content => "echo I am task3\n",
  mode    => '0755',
}
You can see that each of these resources is identical except for the task numbers—task1, 
task2, and task3. Clearly this is a lot of typing, and should you later decide to change the 
properties of these scripts (for example, moving them to a different directory), you'll have to 
find and change each one in the manifest. For three resources this is already annoying, but 
for 30 or 100 resources it's completely impractical. We need a better solution.
Using the each function
Puppet provides the each function to help with just this kind of situation. The each function 
takes an array and applies a block of Puppet code to each element of the array. Here's the 
same example we saw previously, only this time we are using an array and the each function 
(iteration_each.pp):
$tasks = ['task1', 'task2', 'task3']
$tasks.each | $task | {
  file { "/usr/local/bin/${task}":

[ 81 ]
    content => "echo I am ${task}\n",
    mode    => '0755',
  }
}
Now this looks more like a computer program! We have a loop, created by the each 
function. The loop goes round and round, creating a new file resource for each element of 
the $tasks array. Let's look at a schematic version of an each loop:
ARRAY.each | ELEMENT | {
  BLOCK
}
The ARRAY variable can be any Puppet array variable or literal value (it could even be a call 
to Hiera that returns an array). In the previous example, we used $tasks as the array.
The ELEMENT variable is the name of the variable which will hold, each time round the loop, 
the value of the current element in the array. In the previous example, we decided to name 
this variable $task, although we could have called it anything.
The BLOCK variable is a section of Puppet code. This could consist of a function call or 
resource declarations, include statements or conditional statements—basically, anything 
which you can put in a Puppet manifest, you can also put inside a loop block. In the previous 
example, the only thing in the block was the file resource which creates /usr/local/
bin/$task.
Iterating over hashes
The each function works not only on arrays, but also on hashes. When iterating over a hash, 
the loop takes two ELEMENT parameters—the first is the hash key and the second is the 
value. The following example shows how to use the each function to iterate over a hash 
resulting from a Facter query (iteration_hash.pp):
$nics = $facts['networking']['interfaces']
$nics.each | String $interface, Hash $attributes | {
  notice("Interface ${interface} has IP ${attributes['ip']}")
}

Variables, expressions, and facts
[ 82 ]
The list of interfaces returned by $facts['networking']['interfaces'] is a hash 
where the key is the name of the interface (for example, lo for the local loopback interfaces) 
and the value is a hash of the interface's attributes (including the IP address, netmask, and so 
on). Applying the manifest in the previous example gives this result (on my Vagrant box):
sudo puppet apply /vagrant/examples/iteration_hash.pp
Notice: Scope(Class[main]): Interface enp0s3 has IP 10.0.2.15
Notice: Scope(Class[main]): Interface lo has IP 127.0.0.1
Summary
In this chapter, we've gained an understanding of how Puppet's variable and data type 
system works, including the basic data types: Strings, Numbers, Booleans, Arrays, and 
Hashes. We've seen how to interpolate variables in strings, and how to quickly create sets 
of similar resources using an array of resource names. We've learned how to set common 
attributes for resources using a hash of attribute-value pairs and the attribute splat operator.
We've seen how to use variables and values in expressions, including arithmetic expressions, 
and explored the range of Puppet's comparison operators to generate Boolean expressions. 
We've used conditional expressions to build if/else and case statements, and had a brief 
introduction to regular expressions.
We've learned how Puppet's Facter subsystem supplies information about the node via the 
facts hash, and how to use facts in our own manifests and in expressions. We've also pointed 
out some key facts, including the operating system release, the system memory capacity, and 
the system hostname. We've seen how to create custom external facts, such as a cloud fact, 
and how to dynamically generate fact information using executable facts.
Finally, we've learned about iteration in Puppet using the each function, and how to create 
multiple resources based on data from arrays or hashes, including Facter queries.
In the next chapter, we'll stay with the topic of data, and explore Puppet's powerful Hiera 
database. We'll see what problems Hiera solves, look at how to set up and query Hiera, how 
to write data sources, how to create Puppet resources directly from Hiera data, and also 
learn how to use Hiera encryption to manage secret data.

[ 83 ]
6
Managing data with Hiera
What you don't know can't hurt you.
                                                                                 —Edward S. Marshall
In this chapter, you will learn why it's useful to separate your data and code. You will see how 
to set up Puppet's built-in Hiera mechanism; how to use it to store and query configuration 
data, including encrypted secrets such as passwords; and how to use Hiera data to create 
Puppet resources.

Managing data with Hiera
[ 84 ]
Why Hiera?
What do we mean by configuration data? There will be lots of pieces of information in your 
manifests which we can regard as configuration data; for example, the values of all your 
resource attributes. Look at the following example:
package { 'puppet-agent':
  ensure => '1.10.1-1xenial',
}
The manifest declares that version '1.10.1-1xenial' of the puppet-agent package 
should be installed. But what happens when a new version of Puppet is released? When 
you want to upgrade to it, you'll have to find this code, possibly deep in multiple levels of 
directories, and edit it to change the desired version number.
Data needs to be maintained
Multiply this by all the packages managed throughout your manifest, and there's already a 
problem. Yet this is just one piece of data that needs to be maintained, while there are many 
more—the times of cron jobs, the email addresses for reports to be sent to, the URLs of 
files to fetch from the Web, the parameters for monitoring checks, the amount of memory 
to configure for the database server, and so on. If these values are embedded in the code in 
hundreds of manifest files, you're setting up trouble for the future.
How can you make sure your config data is easy to find and maintain?
Settings depend on servers
Mixing data with code makes it harder to find and edit that data. But there's another 
problem. What if you have two nodes to manage with Puppet, and there's a config value 
which needs to be different on each of them? For example, they both might have a cron job 
to run the backup, but the job needs to run at a different time on each node.
How can you use different values for different nodes without having lots of complicated logic 
in your manifest?
Operating systems differ
What if you have some nodes running on Ubuntu 16 and some on Ubuntu 18? As you'll 
know if you've ever had to upgrade the operating system on a node, things change from 
one version to the next. For example, the name of the database server package might have 
changed from mysql-server to mariadb-server.
How can you find the right value to use in your manifest depending on what operating 
system the node is running?

Chapter 6
[ 85 ]
The Hiera way
What we want is a kind of central database in Puppet where we can look up configuration 
settings. The data should be stored separately from the Puppet code to make it easy to find 
and edit values. It should be possible to look up values with a simple function call in Puppet 
code or templates. Furthermore, we need to be able to specify different values, depending 
on things such as the hostname of the node, the operating system, or potentially anything 
else. We would also like to be able to enforce a particular data type for values, such as String 
or Boolean. The database should do all this work for us, and just return the appropriate value 
to the manifest where it's needed.
Fortunately, Hiera does exactly this. Hiera lets you store your config data in simple text files 
(actually, YAML or JSON files, which use a popular structured text format), and it looks like 
the following example (hiera_sample.yaml):
---
  test: 'This is a test'
  consul_node: true
  apache_pkg: 'apache2'
  apache_worker_factor: 100
  apparmor_enabled: true
  ...
In your manifest, you query the database using the lookup() function, as in the following 
example (lookup.pp):
file { lookup('backup_path'):
  ensure => directory,
}
Setting up Hiera
Hiera needs to know one or two things before you can start using it, which are specified in 
the Hiera configuration file, usually named hiera.yaml (not to be confused with Hiera data 
files, which are also YAML files; we'll find about those later in this chapter). Each Puppet 
environment has its own local Hiera config file, located at the root of the environments 
directory (for example, for the production environment, the local Hiera config file would 
be /etc/puppetlabs/code/environments/production/hiera.yaml).
Hiera can also use a global config file located at /etc/puppetlabs/
puppet/hiera.yaml, which takes precedence over the per-environment 
file, but the Puppet documentation recommends you only use this config layer 
for certain exceptional purposes, such as temporary overrides; all your normal 
Hiera data and configuration should live in the environment layer.

Managing data with Hiera
[ 86 ]
The following example shows a minimal hiera.yaml file (hiera_minimal.config.yaml):
---
version: 5
defaults:
  datadir: data
  data_hash: yaml_data
hierarchy:
  - name: "Common defaults"
    path: "common.yaml"
The most important setting is datadir, in the defaults section. This tells Hiera in which 
directory to look for its data files. Conventionally, this is in a data subdirectory of the Puppet 
manifest directory, but you can change this if you need to.
Large organizations may find it useful to manage Hiera data files 
separately to Puppet code, perhaps in a separate Git repo (for 
example, you might want to give certain people permission to edit 
Hiera data but not Puppet manifests).
The hierarchy section is also interesting. This tells Hiera which files to read for its data. 
In the previous example, only Common defaults is defined, telling Hiera to look for data 
in a file called common.yaml. We'll see later in this chapter what else you can do with the 
hierarchy section.
Adding Hiera data to your Puppet repo
Let's add some example Hiera data to your Puppet repo and see what we can do with it. The 
following steps will take you through this process:
1.	 Run the following commands to install your per-environment Hiera config file:
cd /etc/puppetlabs/code/environments/production
sudo cp /vagrant/examples/hiera_minimal.config.yaml hiera.yaml
2.	 Create your data directory for the data files to live in:
sudo mkdir data

Chapter 6
[ 87 ]
3.	 Run the following command to install the example Hiera data file in your data 
directory (make sure you copy it exactly as shown; the file needs to end up named 
common.yaml):
sudo cp /vagrant/examples/hiera_sample.yaml data/common.yaml
4.	 You've configured Hiera and added some data, so let's test your setup by looking up 
the Hiera data within a Puppet manifest. Run the following command:
sudo puppet apply -e "notice(lookup('test'))"
Notice: Scope(Class[main]): This is a test
Troubleshooting Hiera
If you don't get the result This is a test, your Hiera setup is not working properly. If 
you see the warning Config file not found, using Hiera defaults, check the 
hiera.yaml file and make sure that it matches the contents of /vagrant/examples/
hiera_minimal.config.yaml.
If you see an error like the following, it generally indicates a problem with the Hiera data file 
syntax:
Error: Evaluation Error: Error while evaluating a Function Call, (/
etc/puppetlabs/code/environments/production/hiera.yaml): did not find 
expected key while parsing a block mapping at line 11 column 5  at 
line 1:8 on node localhost
If this is the case, check that your data/common.yaml file matches exactly the contents of  
/vagrant/examples/hiera_sample.yaml.
Querying Hiera
In Puppet manifests, you can use the lookup() function to query Hiera for the specified key 
(you can think of Hiera as a key-value database, where the keys are strings and values can be 
any type).
In general, you can use a call to lookup() anywhere in your Puppet manifests where 
you might otherwise use a literal value. The following code shows some examples of this 
(lookup2.pp):
$apache_pkg = lookup('apache_pkg')
unless lookup('apparmor_enabled') {
  exec { 'apt-get -y remove apparmor': }
}
notice("dns_allow_query enabled: ", lookup('dns_allow_query'))

Managing data with Hiera
[ 88 ]
Typed lookups
You can make your code more reliable and catch possible errors by specifying the required 
data type of the key you're looking up. For example, passing Boolean as the second 
argument to lookup() will cause Puppet to raise an error unless the retrieved value is of 
type Boolean (lookup_type.pp):
notice(lookup('apparmor_enabled', Boolean))
If you accidentally look up the wrong key, or mistype the value in the data file, you'll get an 
error like this:
Error: Evaluation Error: Error while evaluating a Function Call, Found 
value has wrong type, expects a Boolean value, got String at /vagrant/
examples/lookup_type.pp:1:8 on node localhost 
Writing Hiera data
As we've seen, Hiera data is stored in text files that are structured using the format called 
YAML (short for YAML Ain't Markup Language), which is a common way of organizing data. 
Here's another snippet from our sample Hiera data file (hiera_sample.yaml):
---
  ...
  syslog_server: '10.170.81.32'
  monitor_ips:
    - '10.179.203.46'
    - '212.100.235.160'
    - '10.181.120.77'
    - '94.236.56.148'
  cobbler_config:
    manage_dhcp: 1
    pxe_just_once: 1
There are actually three different kinds of Hiera data structures present: single values, 
arrays, and hashes. We'll examine these in detail in a moment.
File header
YAML files begin, like the previous example, with three dashes and a newline (---). This 
is part of the YAML format, not a Hiera feature; it's the syntax indicating the start of a new 
YAML document.

Chapter 6
[ 89 ]
Single values
Most Hiera data consists of a key associated with a single value, as in the previous example:
syslog_server: '10.170.81.32'
The value can be any legal Puppet value, such as a String, as in this case, or it can be an 
Integer:
apache_worker_factor: 100
Boolean values
You should specify Boolean values in Hiera as either true or false, without surrounding 
quotes. However, Hiera is fairly liberal in what it interprets as Boolean values—any of true, 
on, or yes (with or without quotes) is interpreted as a true value; and false, off, or no is 
interpreted as false. For clarity, though, stick to the following format:
consul_node: true
Arrays
Usefully, Hiera can also store an array of values associated with a single key:
monitor_ips:
  - '10.179.203.46'
  - '212.100.235.160'
  - '10.181.120.77'
  - '94.236.56.148'
The key (monitor_ips) is followed by a list of values, each on its own line, and preceded 
by a hyphen (-). When you call lookup('monitor_ips') in your code, the values will be 
returned as a Puppet array.
Hashes
As we saw in Chapter 5, Variables, expressions, and facts, a hash (also called a dictionary 
in some programming languages) is like an array where each value has an identifying name 
(called a key), as in the following example:
cobbler_config:
  manage_dhcp: 1
  pxe_just_once: 1
Each key-value pair in the hash is listed, indented, on its own line. The cobbler_config 
hash has two keys, manage_dhcp and pxe_just_once. The value associated with each of 
those keys is 1.

Managing data with Hiera
[ 90 ]
When you call lookup('cobbler_config') in a manifest, the data will be returned as 
a Puppet hash, and you can reference individual values in it using the normal Puppet hash 
syntax, as we saw in Chapter 5, Variables, expressions, and facts (lookup3.pp):
$cobbler_config = lookup('cobbler_config')
$manage_dhcp = $cobbler_config['manage_dhcp']
$pxe_just_once = $cobbler_config['pxe_just_once']
if $pxe_just_once {
  notice("pxe_just_once is enabled")
} else {
  notice("pxe_just_once is disabled")
}
Interpolation
Hiera data is not restricted to literal values; it can also include the value of Facter facts, as in 
the following example (from hiera_sample.yaml):
 backup_path: "/backup/%{facts.hostname}"
Anything within the %{} delimiters inside a quoted string is evaluated and interpolated by 
Hiera. Note the Hiera syntax for accessing the facts hash (or, indeed, any hash). In Puppet, 
you would look up a hash using the square bracket syntax:
$facts['hostname']
In Hiera, you use a dot character (.) to do the same thing:
facts.hostname
Helpfully, you can also interpolate Hiera data in Hiera data, by using the lookup() function 
as part of the value. This can save you repeating the same value many times and can make 
your data more readable, as in the following example (also from hiera_sample.yaml):
ips:
  home: '130.190.0.1'
  office1: '74.12.203.14'
  office2: '95.170.0.75'
firewall_allow_list:
  - "%{lookup('ips.home')}"
  - "%{lookup('ips.office1')}"
  - "%{lookup('ips.office2')}"
That is much more readable than simply listing a set of IP addresses with no indication of 
what they represent, and it prevents you accidentally introducing errors by updating a value 
in one place but not another. Use Hiera interpolation to make your data self-documenting.

Chapter 6
[ 91 ]
The hierarchy
So far, we've only used a single Hiera data source (common.yaml). Actually, you can have as 
many data sources as you like. Each usually corresponds to a YAML file, and they are listed in 
the hierarchy section of the hiera.yaml file, in priority order:
hierarchy:
  - name: "Common defaults"
    path: "common.yaml"
  - name: "Source 1"
    path: "source1.yaml"
  - name: "Source 2"
    path: "source2.yaml"
  - name: "Source 3"
    path: "source3.yaml"
It can be helpful to organize Hiera data into separate files; for example, all the configuration 
relating to web hosts could be stored in a webhost.yaml file. In general, though, you should 
keep as much data as possible in the common file, simply because it's easier to find and 
maintain data if it's in one place rather than scattered throughout several files.
Dealing with multiple values
You may be wondering what happens if the same key is listed in more than one Hiera data 
source. For example, imagine source1.yaml contains the following:
consul_node: false
And common.yaml contains this:
consul_node: true
What happens when you call lookup('consul_node') with this data? There are two 
different values for consul_node in two different files, so which one does Hiera return?
The answer is that Hiera searches data sources in the order they are listed in the hierarchy 
section. It returns the first value found, so if there are multiple values, only the value from 
the highest-priority data source will be returned (that's the hierarchy part).
Merge behaviors
We said in the previous section that if there is more than one value matching the specified 
key, the first matching data source takes priority over the others. This is the default behavior, 
and it's what you'll usually want. However, sometimes you want lookup() to return the 
union of all the matching values found throughout the hierarchy. Hiera allows you to specify 
which of these strategies it should use when multiple values match your lookup.

Managing data with Hiera
[ 92 ]
This is called a merge behavior, and you can specify which merge behavior you want as the 
third argument to lookup(), after the key and data type (lookup_merge.pp):
notice(lookup('firewall_allow_list', Array, 'unique'))
The default merge behavior is called first, and it returns only one value, the highest-
priority value. By contrast, the unique merge behavior returns all values found, as a 
flattened array, with duplicates removed (hence unique).
If you are looking up hash data, you can use the hash merge behavior to return a merged 
hash containing all the keys and values from all the matching hashes found. If Hiera finds two 
hash keys with the same name, only the highest-priority value will be returned. This is known 
as a shallow merge. If you want a deep merge (that is, one where all matching hashes will be 
recursively merged), use the deep merge behavior.
If this all sounds a bit complicated, don't worry. The default merge behavior is probably what 
you want most of the time, and if you should happen to need one of the other behaviors 
instead, you can read more about it in the Puppet documentation. 
Data sources based on facts
The hierarchy mechanism lets you set common default values for all situations (usually in 
common.yaml), but override them in specific circumstances. For example, you can set a  
data source in the hierarchy based on the value of a Puppet fact, such as the hostname 
(hiera_hostname.yaml):
hierarchy:
  - name: "Host-specific data"
    path: "nodes/%{facts.hostname}.yaml"
  - name: "Common defaults"
    path: "common.yaml"
Hiera will look up the value of the specified fact and search for a data file with that name 
in the nodes directory. In the previous example, if the node's hostname is web1, Hiera will 
look for the data file nodes/web1.yaml in the Hiera data directory. If this file exists and 
contains the specified Hiera key, the web1 node will receive that value for its lookup, while 
other nodes will get the default value from the common file. (Note that you can organize your 
Hiera data files in subdirectories under the main data directory.)

Chapter 6
[ 93 ]
Another useful fact to reference in the hierarchy is the operating system major version or 
codename. This is very useful when you need your manifest to work on more than one 
release of the operating system. If you have more than a handful of nodes, migrating to the 
latest OS release is usually a gradual process, upgrading one node at a time. If something 
has changed from one version to the next that affects your Puppet manifest, you can use the 
os.distro.codename fact to select the appropriate Hiera data, as in the following example 
(hiera_os_codename.yaml):
hierarchy:
  - name: "OS-specific data"
    path: "os/%{facts.os.distro.codename}.yaml"
  - name: "Common defaults"
    path: "common.yaml"
Alternatively, you can use the os.release.major fact (hiera_os_release.yaml):
hierarchy:
  - name: "OS-specific data"
    path: "os/%{facts.os.release.major}.yaml"
  - name: "Common defaults"
    path: "common.yaml"
For example, if your node is running Ubuntu 16.04 Xenial, Hiera will look for a data file 
named os/xenial.yaml (if you're using os.distro.codename) or os/16.04.yaml  
(if you're using os.release.major) in the Hiera data directory.
For more information about facts in Puppet, see Chapter 5, Variables, expressions, and facts.
What belongs in Hiera?
What data should you put in Hiera, and what should be in your Puppet manifests? A good 
rule of thumb about when to separate data and code is to ask yourself what might change 
in the future. For example, the exact version of a package is a good candidate for Hiera data, 
because it's quite likely you'll need to update it in the future.
Another characteristic of data that belongs in Hiera is that it's specific to your site or 
company. If you take your Puppet manifest and give it to someone else in another company 
or organization, and she has to modify any values in the code to make it work at her site, 
then those values should probably be in Hiera. This makes it much easier to share and reuse 
code—all you have to do is edit some values in Hiera.
If the same data is needed in more than one place in your manifests, it's a good idea for 
that data to be stored in Hiera as well. Otherwise, you have to either repeat the data, which 
makes it harder to maintain, or use a global variable, which is a bad style in any programming 
language, and especially so in Puppet.

Managing data with Hiera
[ 94 ]
If you'd have to change a data value when you apply your manifests on a different operating 
system, then that's also a candidate for Hiera data. As we've seen in this chapter, you can 
use the hierarchy to select the correct value based on facts such as the operating system or 
version.
Creating resources with Hiera data
When we started working with Puppet, we created resources directly in the manifest using 
literal attribute values. In this chapter, we've seen how to use Hiera data to fill in the title 
and attributes of resources in the manifest. We can now take this idea one step further and 
create resources directly from Hiera queries. The advantage of this method is that we can 
create any number of resources of any type, based purely on data.
Building resources from Hiera arrays
In Chapter 5, Variables, expressions, and facts, we learned how to use Puppet's each 
function to iterate over an array or hash, creating resources as we go. Let's apply this 
technique to some Hiera data. In our first example, we'll create some user resources  
from a Hiera array.
1.	 On your Puppet-managed node, run the following command:
cd /etc/puppetlabs/code/environments/production
sudo cp /vagrant/examples/hiera_users.yaml data/common.yaml
2.	 Next, run the following command:
sudo puppet apply /vagrant/examples/hiera_users.pp
Notice: /Stage[main]/Main/User[katy]/ensure: created
Notice: /Stage[main]/Main/User[lark]/ensure: created
Notice: /Stage[main]/Main/User[bridget]/ensure: created
Notice: /Stage[main]/Main/User[hsing-hui]/ensure: created
Notice: /Stage[main]/Main/User[charles]/ensure: created
Here's the data we're using (hiera_users.yaml):
users:
  - 'katy'
  - 'lark'
  - 'bridget'
  - 'hsing-hui'
  - 'charles'

Chapter 6
[ 95 ]
And here's the code which reads it and creates the corresponding user instances (hiera_
users.pp):
lookup('users').each | String $username | {
  user { $username:
    ensure => present,
  }
}
Combining Hiera data with resource iteration is a powerful idea. This short manifest can 
manage all the users in your infrastructure, without you ever having to edit the Puppet code 
to make changes. To add new users you need only edit the Hiera data.
Building resources from Hiera hashes
Of course, real life is never quite as simple as a programming language example. If you were 
really managing users with Hiera data in this way, you'd need to include more data than just 
their names; you'd need to be able to manage shells, UIDs, and so on, and you'd also need 
to able to remove the users if necessary. To do that, we need to add some structure to the 
Hiera data.
1.	 On your Puppet-managed node, run the following command:
cd /etc/puppetlabs/code/environments/production
sudo cp /vagrant/examples/hiera_users2.yaml data/common.yaml
2.	 Next, run the following command:
sudo puppet apply /vagrant/examples/hiera_users2.pp
Let's look at the code more closely to see how it works (hiera_users2.pp):
    lookup('users').each | String $username, Hash $attrs | {
      user { $username:
        * => $attrs,
      }
    }
The first difference from the previous example is that instead of $users being a simple 
array, it's a hash of hashes (hiera_users2.yaml):
users:
  'katy':
    ensure: present
    uid: 1900
    shell: '/bin/bash'
  'lark':

Managing data with Hiera
[ 96 ]
    ensure: present
    uid: 1901
    shell: '/bin/sh'
  'bridget':
    ensure: present
    uid: 1902
    shell: '/bin/bash'
  'hsing-hui':
    ensure: present
    uid: 1903
    shell: '/bin/sh'
  'charles':
    ensure: present
    uid: 1904
    shell: '/bin/bash'
The hash keys are the usernames, and each value is a hash of user attributes such as uid and 
shell.
When we call each on this hash, we specify two parameters to the loop instead of one:
| String $username, Hash $attrs |
As we saw in Chapter 5, Variables, expressions, and facts, when iterating over a hash, these 
two parameters receive the hash key and value, respectively.
Inside the loop, we create a user resource for each element of the hash:
user { $username:
  * => $attrs,
}
You may recall from the previous chapter that the * operator (the attribute splat operator) 
tells Puppet to treat $attrs as a hash of attribute-value pairs. So, the first time round 
the loop, with user katy, Puppet will create a user resource equivalent to the following 
manifest:
user { 'katy':
  ensure => present,
  uid    => 1900,
  shell  => '/bin/bash',
}
Every time we go round the loop with the next element of users, Puppet will create another 
user resource with the specified attributes.

Chapter 6
[ 97 ]
The advantages of managing resources with Hiera data
The previous example makes it easy to manage users across your network without having 
to edit Puppet code; if you want to remove a user, for example, you would simply change 
her ensure attribute in the Hiera data to absent. Although each of the users happens to 
have the same set of attributes specified, this isn't essential; you could add any attribute 
supported by the Puppet user resource to any user in the data. Also, if there's an attribute 
whose value is always the same for all users, you need not list it in the Hiera data for every 
user; you can add it as a literal attribute value of the user resource inside the loop, and thus 
every user will have it.
This makes it easier to add and update users on a routine basis, but there are other 
advantages too; for example, you could write a simple web application allowing HR staff to 
add or edit users using a browser interface, and it would only need to output a YAML file 
with the required data. This is much easier and more robust than trying to generate Puppet 
code automatically. Even better, you could pull user data from an LDAP or Active Directory 
server and put it into Hiera YAML format for input into this manifest.
This is a very powerful and flexible technique, and, of course, you can use it to manage any 
kind of Puppet resource—files, packages, Apache virtual hosts, MySQL databases—anything 
you can do with a resource, you can do with Hiera data and each. You can also use Hiera's 
override mechanism to create different sets of resources for different nodes, roles, or 
operating systems.
However, you shouldn't overuse this technique. Creating resources from Hiera data adds 
a layer of abstraction which makes it harder to understand the code for anyone trying to 
read or maintain it. With Hiera, it can also be difficult to work out from inspection exactly 
what data the node will get in a given set of circumstances. Keep your hierarchy as simple 
as possible, and reserve the data-driven-resources trick for situations where you have a 
large and variable number of resources which you need to update frequently. In Chapter 11, 
Orchestrating cloud resources, we'll see how to use the same technique to manage cloud 
instances, for example.
Managing secret data
Puppet often needs to know your secrets; for example, passwords, private keys, and 
other credentials need to be configured on the node, and Puppet must have access to this 
information. The problem is how to make sure that no one else does. If you are checking this 
data into a Git repo, it will be available to anybody who has access to the repo, and if it's a 
public GitHub repo, everybody in the world can see it. 

Managing data with Hiera
[ 98 ]
Clearly, it's essential to be able to encrypt secret data in such a way that Puppet can decrypt 
it on individual nodes where it's needed, but it's indecipherable to anybody who does 
not have the key. The popular GnuPG encryption tool is a good choice for this. It lets you 
encrypt data using a public key which can be distributed widely, but only someone with the 
corresponding private key can decrypt the information.
Hiera has a pluggable backend system, which allows it to support various different ways of 
storing data. One such backend is called hiera-eyaml-gpg, which allows Hiera to use a 
GnuPG-encrypted data store. Rather than encrypting a whole data file, hiera-eyaml-gpg 
lets you mix encrypted and plaintext data in the same YAML file. That way, someone who 
doesn't have the private key can still edit and update the plaintext values in Hiera data files, 
although the encrypted data values will be unreadable to them.
Setting up GnuPG
First, we'll need to install GnuPG and create a keypair for use with Hiera. The following 
instructions will help you do this:
1.	 Run the following command:
sudo apt-get install gnupg rng-tools
2.	 Once GnuPG is installed, run the following command to generate a new keypair:
gpg --gen-key
3.	 When prompted, select the RSA and RSA (default) key type:
Please select what kind of key you want:
   (1) RSA and RSA (default)
   (2) DSA and Elgamal
   (3) DSA (sign only)
   (4) RSA (sign only)
Your selection? 1
4.	 Select the 2048 bit key size:
RSA keys may be between 1024 and 4096 bits long.
What keysize do you want? (2048) 2048
5.	 Enter 0 for the key expiry time:
Key is valid for? (0) 0

Chapter 6
[ 99 ]
6.	 When prompted for a real name, email address, and comment for the key, enter 
whatever is appropriate for your site:
Real name: Puppet
Email address: puppet@cat-pictures.com
Comment:
You selected this USER-ID:
    "Puppet <puppet@cat-pictures.com>"
Change (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? o
7.	 When prompted for a passphrase, just hit Enter (the key can't have a passphrase, 
because Puppet won't be able to supply it).
8.	 It may take a few moments to generate the key, but once this is complete, GnuPG 
will print out the key fingerprint and details (yours will look different):
pub   2048R/40486112 2016-09-30
      Key fingerprint = 6758 6CEE D221 7AA0 8369  FF3A FEC1 0055 
4048 6112
uid                  Puppet <puppet@cat-pictures.com>
sub   2048R/472954EB 2016-09-30
This key is now stored in your GnuPG keyring, and Hiera will be able to use it to encrypt and 
decrypt your secret data on this node. We'll see later in the chapter how to distribute this 
key to other nodes managed by Puppet.
Setting up hiera-eyaml-gpg
We'll need to install a few dependencies on your node or Vagrant box, plus the hiera-
eyaml-gpg package itself, and configure Hiera to use it. The following steps will guide you 
through this process:
1.	 Run the following commands to install the required packages for the hiera-
eyaml-gpg backend:
sudo apt-get install ruby-dev
sudo /opt/puppetlabs/puppet/bin/gem install hiera-eyaml-gpg gpgme
2.	 Next, run the following command:
cd /etc/puppetlabs/code/environments/production
sudo cp /vagrant/examples/hiera_eyaml.config.yaml hiera.yaml

Managing data with Hiera
[ 100 ]
Here's what the new Hiera config file looks like (hiera_eyaml.config.yaml):
---
version: 5
defaults:
  datadir: data
  data_hash: yaml_data
hierarchy:
  - name: "Common defaults"
    path: "common.yaml"
  - name: "Per-datacenter secret data (encrypted)"
    lookup_key: eyaml_lookup_key
    path: "secret.eyaml"
    options:
      gpg_gnupghome: '/home/vagrant/.gnupg'
The second option in the hierarchy section tells Hiera where to look for secret data, 
and configures the hiera-eyaml-gpg backend to find its decryption key in the /home/
vagrant/.gnupg directory.
Creating an encrypted secret
With hiera-eyaml-gpg installed and configured, you're ready to add some secret data to 
your Hiera store.
1.	 Create a new, empty Hiera data file with the following commands:
cd /etc/puppetlabs/code/environments/production
sudo touch data/secret.eyaml
2.	 Run the following command to edit the data file using the eyaml editor (which 
automatically encrypts the data for you when you save it). Instead of puppet@cat-
pictures.com, use the email address that you entered when you created your 
GPG key.
sudo eyaml edit --gpg-always-trust --gpg-recipients=puppet@cat-
pictures.com data/secret.eyaml
3.	 If the system prompts you to select your default editor, choose the editor you prefer. 
If you're familiar with Vim, I recommend you choose that, but, otherwise, you will 
probably find nano the easiest option. (You should learn Vim, but that's a subject for 
another book.)

Chapter 6
[ 101 ]
4.	 Your selected editor will be started with the following text already inserted in to  
the file:
#| This is eyaml edit mode. This text (lines starting with #| at 
the top of the
#| file) will be removed when you save and exit.
#|  - To edit encrypted values, change the content of the 
DEC(<num>)::PKCS7[]!
#|    block (or DEC(<num>)::GPG[]!).
#|    WARNING: DO NOT change the number in the parentheses.
#|  - To add a new encrypted value copy and paste a new block from 
the
#|    appropriate example below. Note that:
#|     * the text to encrypt goes in the square brackets
#|     * ensure you include the exclamation mark when you copy and 
paste
#|     * you must not include a number when adding a new block
#|    e.g. DEC::PKCS7[]! -or- DEC::GPG[]!
5.	 Enter the following text below the commented message, exactly as shown, including 
the beginning three hyphens:
---
  test_secret: DEC::GPG[This is a test secret]!
6.	 Save the file and exit the editor.
7.	 Run the following command to test that Puppet can read and decrypt your secret:
sudo puppet apply -e "notice(lookup('test_secret'))"
Notice: Scope(Class[main]): This is a test secret
How Hiera decrypts secrets
To prove to yourself that the secret data is actually encrypted, run the following command to 
see what it looks like in the data file on disk:
cat data/secret.eyaml
---
  test_secret: ENC[GPG,hQEMA4+8DyxHKVTrAQf/QQPL4zD2kkU7T+FhaEdptu68RA
w2m2KAXGujjnQPXoONrbh1QjtzZiJBlhqOP+7JwvzejED0NXNMkmWTGfCrOBvQlZS0U9V
rgsyq5mACPHyeLqFbdeOjNEIR7gLP99aykAmbO2mRqfXvns+cZgaTUEPXOPyipY5Q6w6/
KeBEvekTIZ6ME9Oketj+1/zyDz4qWH+0nLwdD9L279d7hnokpts2tp+gpCUc0/qKsTXpdT
RPE2R0kg9Bl84OP3fFlTSTgcT+pS8Dfa1/ZzALfHmULcC3hckG9ZSR+0cd6MyJzucwiJC
reIfR/cDfqpsENNM6PNkTAHEHrAqPrSDXilg1KtJSAfZ9rS8KtRyhoSsk+XyrxIRH/S1Qg
1dgFb8VqJzWjFl6GBJZemy7z+xjoWHyznbABVwp0KXNGgn/0idxfhz1mTo2/49POFiVF4M
Bo/6/EEU4cw==]

Managing data with Hiera
[ 102 ]
Of course, the actual ciphertext will be different for you, since you're using a different 
encryption key. The point is, though, that the message is completely scrambled. GnuPG's 
encryption algorithms are extremely strong; even using every computer on Earth 
simultaneously, it would take (on average) many times the current age of the universe to 
unscramble data encrypted with a 2048-bit key. (Or, to put it a different way, the chances of 
decrypting the data within a reasonable time are many billions to one.)
When you reference a Hiera key such as test_secret in your manifest, what happens 
next? Hiera consults its list of data sources configured in hiera.yaml. The first one defined 
is common.yaml, so Hiera looks for the specified key in that file, but doesn't find it. The next 
source in the hierarchy is secret.eyaml, which does contain the key we're interested in 
(test_secret).
The result is this:
ENC[GPG,hQEMA4 … EEU4cw==]
ENC tells Hiera that this is an encrypted value, and GPG identifies which type of encryption 
is being used (hiera-eyaml supports several encryption methods, of which GPG is one). 
Hiera calls the GPG subsystem to process the encrypted data, and GPG searches the keyring 
to find the appropriate decryption key. Assuming it finds the key, GPG decrypts the data and 
passes the result back to Hiera, which returns it to Puppet, and the result is the plaintext:
This is a test secret
The beauty of the system is that all this complexity is hidden from you; all you have to do is 
call the function lookup('test_secret') in your manifest, and you get the answer.
Editing or adding encrypted secrets
If the secret data is stored in encrypted form, you might be wondering how to edit it when 
you want to change the secret value. Fortunately, hiera-eyaml-gpg provides a way to do 
this. Recall that, when you first entered the secret data, you used the following command:
sudo eyaml edit --gpg-always-trust --gpg-recipients=puppet@cat-
pictures.com data/secret.eyaml
This tells hiera-eyaml-gpg to open an editor with decrypted versions of all the secrets 
in the data file. If you run the same command again, you'll find that you're looking at your 
original plaintext (below some explanatory comments):
---
  test_secret: DEC(1)::GPG[This is a test secret]!

Chapter 6
[ 103 ]
You can edit the This is a test secret string (make sure to leave everything else 
exactly as it is, including the DEC::GPG[]! delimiters). When you save the file and close the 
editor, hiera-eyaml-gpg will re-encrypt the data for you, if it has changed.
Don't remove the (1) in parentheses after DEC; it tells hiera-eyaml-gpg that this is an 
existing secret, not a new one. As you add more secrets to this file, they will be identified 
with increasing numbers.
For convenience of editing, I suggest you make a shell script, called something like /usr/
local/bin/eyaml_edit, with the following contents (as before, substitute the gpg-
recipients email address with the one associated with your GPG key):
#!/bin/bash/opt/puppetlabs/puppet/bin/eyaml edit --gpg-always-
trust --gpg-recipients=puppet@cat-pictures.com /etc/puppetlabs/code/
environments/production/data/secret.eyaml
Now, whenever you need to edit your secret data, you can simply run the following command:
sudo eyaml_edit
To add a new secret, add a line like this:
new_secret: DEC::GPG[Somebody wake up Hicks]!
When you save and quit the editor, the newly encrypted secret will be stored in the data file.
Distributing the decryption key
Now that your Puppet manifests use encrypted Hiera data, you'll need to make sure that 
each node running Puppet has a copy of the decryption key. Export the key to a text file 
using the following command (use your key's email address, of course):
sudo sh -c 'gpg --export-secret-key -a puppet@cat-pictures.com >key.
txt'
Copy the key.txt file to any nodes which need the key, and run the following command to 
import it:
sudo gpg --import key.txt
sudo rm key.txt
Make sure that you delete all copies of the text file once you have imported the key. 

Managing data with Hiera
[ 104 ]
Important note
Because all Puppet nodes have a copy of the decryption key, this method only 
protects your secret data from someone who does not have access to the 
nodes. It is still considerably better than putting secret data in your manifests 
in plaintext, but it has the disadvantage that someone with access to a node 
can decrypt, modify, and re-encrypt the secret data. For improved security, 
you should use a secrets management system where the node does not 
have the key, and Puppet has read-only access to secrets. Some options here 
include Vault, from Hashicorp, and Summon, from Conjur.
Summary
In this chapter, we've outlined some of the problems with maintaining configuration data in 
Puppet manifests, and introduced Hiera as a powerful solution. We've seen how to configure 
Puppet to use the Hiera data store, and how to query Hiera keys in Puppet manifests using 
lookup().
We've looked at how to write Hiera data sources, including string, array, and hash data 
structures, and how to interpolate values into Hiera data, including Puppet facts, and other 
Hiera data. We've learned how Hiera's hierarchy works, and how to configure it using the 
hiera.yaml file.
We've seen how to configure our example Puppet infrastructure to use Hiera data, and 
proved the process by looking up a data value in a Puppet manifest. In case of problems, 
we've also looked at some common Hiera errors, and we've discussed rules of thumb about 
when to put data into Hiera.
We've explored using Hiera data to create resources, using an each loop over an array or 
hash. Finally, we've covered using encrypted data with Hiera, using the hiera-eyaml-gpg 
backend, and we've seen how to create a GnuPG key, use it to encrypt a secret value, and 
retrieve it again via Puppet. We've explored the process Hiera uses to find and decrypt secret 
data, developed a simple script to make it easy to edit encrypted data files, and outlined a 
basic way to distribute the decryption key to multiple nodes.
In the next chapter, we'll look at how to find and use public modules from Puppet Forge; 
how to use public modules to manage software, including Apache, MySQL, and archive files; 
how to use the r10k tool to deploy and manage third-party modules; and how to write and 
structure your own modules.

[ 105 ]
Mastering modules
There are no big problems, there are just a lot of little problems.
                                                                                 —Henry Ford
In this chapter, you'll learn about Puppet Forge, the public repository for Puppet modules, 
and you'll see how to install and use third-party modules from Puppet Forge, using the 
r10k module management tool. You'll see examples of how to use three important Forge 
modules: puppetlabs/apache, puppetlabs/mysql, and puppet/archive. You'll 
be introduced to some useful functions provided by puppetlabs/stdlib, the Puppet 
standard library. Finally, working through a complete example, you'll learn how to develop 
your own Puppet module from scratch, how to add appropriate metadata for your module, 
and how to upload it to Puppet Forge.
7

Mastering modules
[ 106 ]
Using Puppet Forge modules
Although you could write your own manifests for everything you want to manage, you can 
save yourself a lot of time and effort by using public Puppet modules wherever possible. A 
module in Puppet is a self-contained unit of shareable, reusable code, usually designed to 
manage one particular service or piece of software, such as the Apache web server.
What is the Puppet Forge?
The Puppet Forge is a public repository of Puppet modules, many of them officially 
supported and maintained by Puppet, and all of which you can download and use. You can 
browse the Forge at https://forge.puppet.com/.
One of the advantages of using a well-established tool such as Puppet is that there is a large 
number of mature public modules available, which cover the most common software you're 
likely to need. For example, here is a small selection of the things you can manage with 
public modules from Puppet Forge:


MySQL/PostgreSQL/SQL Server


Apache/Nginx


Java/Tomcat/PHP/Ruby/Rails


HAProxy


Amazon AWS


Docker


Jenkins


Elasticsearch/Redis/Cassandra


Git repos


Firewalls (via iptables)
Finding the module you need
The Puppet Forge home page has a search bar at the top. Type what you're looking for into 
this box, and the website will show you all the modules which match your search keywords. 
Often, there will be more than one result, so how do you decide which module to use?
The best choice is a Puppet Supported module, if one is available. These are officially 
supported and maintained by Puppet, and you can be confident that supported 
modules will work with a wide range of operating systems and Puppet versions. 
Supported modules are indicated by a yellow SUPPORTED flag in search results, or 
you can browse the list of all supported modules at https://forge.puppet.com/
modules?endorsements=supported.

Chapter 7
[ 107 ]
The next best option is a Puppet Approved module. While not officially supported, these 
modules are recommended by Puppet, and have been checked to make sure they follow 
best practices and meet certain quality standards. Approved modules are indicated by a 
green APPROVED flag in search results, or you can browse the list of all approved modules at 
https://forge.puppet.com/modules?endorsements=approved.
Assuming that a Puppet Supported or Approved module is not available, another useful way 
to choose modules is by looking at the number of downloads. Selecting the Most Downloads 
tab on the Puppet Forge search results page will sort the results by downloads, with the 
most popular modules first. The most-downloaded modules are not necessarily the best, of 
course, but they're usually a good place to start.
It's also worth checking the latest release date for modules. If the module you're looking at 
hasn't been updated in over a year, it may be better to go with a more actively-maintained 
module, if one is available. Clicking the Latest release tab will sort the search results by the 
most recently updated.
You can also filter search results by operating system support and Puppet version 
compatibility; this can be very useful for finding a module which works with your system.
Having chosen the module you want, it's time to add it to your Puppet infrastructure.
Using r10k
In the past, many people used to download Puppet Forge modules directly and check a copy 
of them into their code base, effectively forking the module repo (and some still do this). 
There are many drawbacks to this approach. One is that your code base becomes cluttered 
with code which is not yours, and this can make it difficult to search for the code you want. 
Another is that it's difficult to test your code with different versions of the public modules, 
without creating your own Git branches, re-downloading the modules, and so on. You also 
won't get future bug fixes and improvements from the Puppet Forge modules unless you 
manually update your copies. In many cases, you will need to make small changes or fixes 
to the modules to use them in your environment, and your version of the module will then 
diverge from the upstream version, storing up maintenance problems for the future.
A much better approach to module management, therefore, is to use the r10k tool, which 
eliminates these problems. Instead of directly downloading the modules you need and 
adding them to your code base, r10k installs your required modules on each Puppet-
managed node, using a special text file called Puppetfile. The r10k tool will manage the 
contents of your modules directory based entirely on the Puppetfile metadata. The module 
code is never checked into your code base, but always downloaded from the Puppet Forge 
when requested, so you can stay up to date with the latest releases if you want, or pin each 
module to a specified version which you know works with your manifest.

Mastering modules
[ 108 ]
The r10k tool is the de facto standard module manager for Puppet deployments, and we'll 
be using it to manage modules throughout the rest of this book.
In the following example, we'll use r10k to install the puppetlabs/stdlib module. Follow 
these steps:
1.	 Run the following commands to clear out your modules directory, if there's 
anything in it (make sure you have backed up anything here you want to keep):
cd /etc/puppetlabs/code/environments/production
sudo rm -rf modules/
2.	 Run the following command to install r10k:
sudo gem install r10k
3.	 Copy the example Puppetfile into place with the following command:
sudo cp /vagrant/examples/Puppetfile.stdlib Puppetfile
4.	 Run the following command to have r10k process the Puppetfile and install your 
requested modules:
sudo r10k puppetfile install --verbose
INFO     -> Updating module /etc/puppetlabs/code/environments/
production/modules/stdlib
5.	 Run the following command to create a .gitignore file, which will tell Git not to 
track any of the files in your modules directory (because they are now managed 
directly by r10k):
sudo sh -c 'echo modules >.gitignore'
As you'll see, r10k has downloaded the module into a subdirectory named modules in 
your Puppet code directory (/etc/puppetlabs/code/environments/production). All 
modules in this directory will be automatically loaded by Puppet and available for use in your 
manifests. Note that the next time you commit changes in your Git repo, you won't see the 
module listed in the results of git status, as anything in the modules directory is now 
excluded from version control by your .gitignore file. 
To test that the stdlib module is correctly installed, run the following command:
sudo puppet apply -e "notice(upcase('hello'))"
Notice: Scope(Class[main]): HELLO
The upcase function, which converts its string argument to uppercase, is part of the stdlib 
module. If this doesn't work, then stdlib has not been properly installed.

Chapter 7
[ 109 ]
Understanding the Puppetfile
The example Puppetfile for installing stdlib contains the following  
(Puppetfile.stdlib):
forge 'http:://forge.puppetlabs.com'
mod 'puppetlabs/stdlib', '4.17.0'
The forge statement specifies the repository where modules should be retrieved from.
The mod statement specifies the name of the module (puppetlabs/stdlib), and the 
specific version of the module to install (4.17.0). You can have multiple mod statements, 
one for each module you want to install, but in this example there's only one.
Managing dependencies with generate-puppetfile
r10k does not automatically manage dependencies between modules. For example, the 
puppetlabs/apache module depends on also having both puppetlabs/stdlib and 
puppetlabs/concat installed. r10k will not automatically install these for you unless you 
specify them, so you need to also include them in your Puppetfile.
However, you can use the generate-puppetfile tool to find out what dependencies you 
need, so that you can add them to your Puppetfile.
1.	 Run the following command to install the generate-puppetfile gem:
sudo gem install generate-puppetfile
2.	 Run the following command to generate the Puppetfile for a list of specified 
modules (list all the modules you need on the command line, separated by spaces):
generate-puppetfile puppetlabs/docker_platform
Installing modules. This may take a few minutes.
Your Puppetfile has been generated. Copy and paste between the 
markers:
==================================================================
=
forge 'http://forge.puppetlabs.com'
# Modules discovered by generate-puppetfile
mod 'garethr/docker', '5.3.0'
mod 'puppetlabs/apt', '3.0.0'
mod 'puppetlabs/docker_platform', '2.2.1'
mod 'puppetlabs/stdlib', '4.17.0'
mod 'stahnma/epel', '1.2.2'
==================================================================
=

Mastering modules
[ 110 ]
3.	 Run the following command to generate a list of updated versions and dependencies 
for an existing Puppetfile:
generate-puppetfile -p /vagrant/examples/Puppetfile.mysql
Installing modules. This may take a few minutes.
Your Puppetfile has been generated. Copy and paste between the 
markers:
==================================================================
=
forge 'http://forge.puppetlabs.com'
# Modules discovered by generate-puppetfile
mod 'puppet/staging', '2.2.0'
mod 'puppetlabs/mysql', '3.11.0'
mod 'puppetlabs/stdlib', '4.17.0'
==================================================================
=
This is an extremely useful tool both for finding dependencies you need to specify in your 
Puppetfile, and for keeping your Puppetfile up to date with the latest versions of all the 
modules you use.
Using modules in your manifests
Now that we know how to find and install public Puppet modules, let's see how to use them. 
We'll work through a few examples—using the puppetlabs/mysql module to set up a 
MySQL server and database, using the puppetlabs/apache module to set up an Apache 
website, and using puppet/archive to download and unpack a compressed archive. After 
you've tried out these examples, you should feel quite confident in your ability to find an 
appropriate Puppet module, add it to your Puppetfile, and deploy it with r10k.
Using puppetlabs/mysql
Follow these steps to install and use the puppetlabs/mysql module:
1.	 Run the following command to copy the appropriate Puppetfile from the examples 
directory:
cd /etc/puppetlabs/code/environments/production
sudo cp /vagrant/examples/Puppetfile.mysql Puppetfile

Chapter 7
[ 111 ]
2.	 Run the following command to install the module and its dependencies:
sudo r10k puppetfile install --verbose
INFO     -> Updating module /etc/puppetlabs/code/environments/
production/modules/mysql
INFO     -> Updating module /etc/puppetlabs/code/environments/
production/modules/staging
INFO     -> Updating module /etc/puppetlabs/code/environments/
production/modules/stdlib
3.	 Run the following command to apply the manifest:
sudo puppet apply /vagrant/examples/module_mysql.pp
Notice: Compiled catalog for localhost in environment production 
in 0.92 seconds
Notice: /Stage[main]/Mysql::Server::Config/File[mysql-config-
file]/ensure: defined content as '{md5}44e7aa974ab98260d7d013a2087
f1c77'
Notice: /Stage[main]/Mysql::Server::Install/Package[mysql-server]/
ensure: created
Notice: /Stage[main]/Mysql::Server::Root_password/Mysql_
user[root@localhost]/password_hash: password_hash changed '' to 
'*F4AF2E5D85456A908E0F552F0366375B06267295'
Notice: /Stage[main]/Mysql::Server::Root_password/File[/root/.
my.cnf]/ensure: defined content as '{md5}4d59f37fc8a385c9c50f8bb32
86b7c85'
Notice: /Stage[main]/Mysql::Client::Install/Package[mysql_client]/
ensure: created
Notice: /Stage[main]/Main/Mysql::Db[cat_pictures]/Mysql_
database[cat_pictures]/ensure: created
Notice: /Stage[main]/Main/Mysql::Db[cat_pictures]/Mysql_
user[greebo@localhost]/ensure: created
Notice: /Stage[main]/Main/Mysql::Db[cat_pictures]/Mysql_
grant[greebo@localhost/cat_pictures.*]/ensure: created
Let's take a look at the example manifest (module_mysql.pp):
# Install MySQL and set up an example database
class { 'mysql::server':
  root_password           => 'correct horse battery staple',
  remove_default_accounts => true,
}
mysql::db { 'cat_pictures':
  user     => 'greebo',
  password => 'tabby',
  host     => 'localhost',
  grant    => ['SELECT', 'UPDATE'],
}

Mastering modules
[ 112 ]
The first part installs the MySQL server itself. The class keyword is new to us (we'll find out 
more about it in Chapter 8, Classes, roles, and profiles), but you can think of this usage as 
being just like a resource declaration. We have the keyword class, followed by an opening 
curly brace, followed by the name of the required class, mysql::server. This tells Puppet 
to create an instance of the class mysql::server when this manifest is applied, just as a 
resource declaration tells Puppet to create an instance of a resource.
There follows a list of parameters, which are just like resource attributes. The 
mysql::server class takes a number of parameters, most of which we don't need for this 
example, but we do specify the root_password parameter. As you'd expect, this sets the 
password for the MySQL root user.
We also specify remove_default_accounts, which is a security feature. MySQL ships with 
various default user accounts for testing purposes, which should be turned off in production. 
This parameter disables these default accounts.
Although we've specified the password directly in the Puppet manifest for 
the purposes of this example, in your production manifests this should be a 
Hiera lookup (see Chapter 6, Managing data with Hiera) and the password 
should be encrypted in your data files. In general, all your configuration 
data should come from Hiera lookups.
Next comes a resource declaration:
mysql::db { 'cat_pictures':
  user     => 'greebo',
  password => 'tabby',
  host     => 'localhost',
  grant    => ['SELECT', 'UPDATE'],
}
As you can see, this looks just like the built-in resources we've used before, such as the file 
and package resources. In effect, the mysql module has added a new resource type to 
Puppet—mysql::db. This resource models a specific MySQL database—cat_pictures in 
our example.
The title of the resource is the name of the database; in this case, cat_pictures. There 
follows a list of attributes. The user, password, and host attributes specify that the user 
greebo should be allowed to connect to the database from localhost using the password 
tabby. The grant attribute specifies the MySQL privileges that the user should have—
SELECT and UPDATE on the database.

Chapter 7
[ 113 ]
When this manifest is applied, Puppet will create the cat_pictures database and set up 
the greebo user account to access it. This is a very common pattern for Puppet manifests 
which manage an application—usually, the application needs some sort of database to  
store its state, and user credentials to access it. The mysql module lets you configure  
this very easily.
So we can now see the general principles of using a Puppet Forge module:


First of all, we add the module and its dependencies to our Puppetfile and deploy it 
using r10k


Then, we add a class declaration to our manifest, which causes Puppet to install the 
software managed by the module (in this case, MySQL)


Finally, optionally, we add one or more resource declarations of a custom resource 
type defined by the module (in this case, a MySQL database)
Almost all Puppet modules work in a similar way, and in the next section we'll look at some 
key modules which you're likely to need in the course of managing servers with Puppet.
Using puppetlabs/apache
Most applications have a web interface of some kind, which usually requires a web server, 
and the venerable Apache remains a popular choice. The puppetlabs/apache module 
not only installs and configures Apache, but allows you to manage virtual hosts (individual 
websites, such as the frontend for your application).
Here's an example manifest which uses the apache module to create a simple virtual host 
serving an image file (module_apache.pp):
class { 'apache':
  default_vhost => false,
}
apache::vhost { 'cat-pictures.com':
  port          => '80',
  docroot       => '/var/www/cat-pictures',
  docroot_owner => 'www-data',
  docroot_group => 'www-data',
}
file { '/var/www/cat-pictures/index.html':
  content => "<img src='http://bitfieldconsulting.com/files/happycat.
jpg'>",
  owner   => 'www-data',
  group   => 'www-data',
}

Mastering modules
[ 114 ]
Follow these steps to apply the manifest:
1.	 Run the following commands to copy the example Puppetfile into place:
cd /etc/puppetlabs/code/environments/production
sudo cp /vagrant/examples/Puppetfile.apache Puppetfile
2.	 Run the following command to install the apache module:
sudo r10k puppetfile install —-verbose
3.	 Run the following command to apply the manifest:
sudo puppet apply /vagrant/examples/module_apache.pp
To test the new website, point your browser to the following link (for Vagrant users; if you're 
not using the Vagrant box, browse to port 80 on the server you're managing with Puppet):
http://localhost:8080/
You should see a picture of a happy cat:
Let's go through the manifest and see in detail how it works. It starts with the class 
declaration which actually installs Apache on the server (module_apache.pp):
class { 'apache':
  default_vhost => false,
}
There are many parameters you could set for the apache class, but in this example we  
only need to set default_vhost => false, which disables the default Apache 2 Test 
Page site.

Chapter 7
[ 115 ]
Next comes a resource declaration for an apache::vhost resource, which creates an 
Apache virtual host, or website:
apache::vhost { 'cat-pictures.com':
  port          => '80',
  docroot       => '/var/www/cat-pictures',
  docroot_owner => 'www-data',
  docroot_group => 'www-data',
}
The title of the resource is the domain name which the virtual host will respond to  
(cat-pictures.com). port tells Apache which port to listen on for requests. docroot 
identifies the pathname of the directory where Apache will find the website files on the 
server. Finally, the docroot_owner and docroot_group attributes specify the user and 
group which should own the docroot directory.
Finally, we create an index.html file to add some content to the website; in this case, an 
image of a happy cat:
file { '/var/www/cat-pictures/index.html':
  content => "<img src='http://bitfieldconsulting.com/files/happycat.
jpg'>",
  owner   => 'www-data',
  group   => 'www-data',
}
If you're using Vagrant, port 80 on the Vagrant box is mapped to port 8080 
on your local machine, so browsing to http://localhost:8080 is the 
equivalent of browsing directly to port 80 on the Vagrant box. If for some 
reason you need to change this port mapping, edit your Vagrantfile (in 
the Puppet Beginner's Guide repo) and look for the following line:
config.vm.network "forwarded_port", guest: 80, host: 
8080
Change these settings as required, and run the following command on your 
local machine in the PBG repo directory:
vagrant reload
Using puppet/archive
While installing software from packages is a common task, you'll also occasionally need to 
install software from archive files such as a tarball (a .tar.gz file) or ZIP file. The puppet/
archive module is a great help for this, as it provides an easy way to download archive files 
from the Internet, and it can also unpack them for you.

Mastering modules
[ 116 ]
In the following example, we'll use the puppet/archive module to download and unpack 
the latest version of the popular WordPress blogging software. Follow these steps to apply 
the manifest:
1.	 Run the following commands to copy the example Puppetfile into place:
cd /etc/puppetlabs/code/environments/production
sudo cp /vagrant/examples/Puppetfile.archive Puppetfile
2.	 Run the following command to install the archive module:
sudo r10k puppetfile install —-verbose
3.	 Run the following command to apply the manifest:
sudo puppet apply /vagrant/examples/module_archive.pp
Notice: Compiled catalog for localhost in environment production 
in 2.50 seconds
Notice: /Stage[main]/Main/Archive[/tmp/wordpress.tar.gz]/ensure: 
download archive from https://wordpress.org/latest.tar.gz to /tmp/
wordpress.tar.gz and extracted in /var/www with cleanup
Let's look at the archive resource in detail to see how it works (module_archive.pp):
archive { '/tmp/wordpress.tar.gz':
  ensure       => present,
  extract      => true,
  extract_path => '/var/www',
  source       => 'https://wordpress.org/latest.tar.gz',
  creates      => '/var/www/wordpress',
  cleanup      => true,
}


The title gives the path to the location where you want the archive file to be 
downloaded (/tmp/wordpress.tar.gz). Assuming you don't need to keep the 
archive file after it's been unpacked, it's usually a good idea to put it in /tmp.


The extract attribute determines whether or not Puppet should unpack the 
archive; this should usually be set to true.


The extract_path attribute specifies where to unpack the contents of the archive. 
In this case, it makes sense to extract it to a subdirectory of /var/www, but this will 
vary depending on the nature of the archive. If the archive file contains software 
which will be compiled and installed, for example, it may be a good idea to unpack it 
in /tmp, so that the files will be automatically cleaned up after the next reboot.


The source attribute tells Puppet where to download the archive from, usually (as 
in this example) a web URL.

Chapter 7
[ 117 ]


The creates attribute works exactly the same way as creates on an exec resource, 
which we looked at in Chapter 4, Understanding Puppet resources. It specifies a file 
which unpacking the archive will create. If this file exists, Puppet knows the archive 
has already been unpacked, so it does not need to unpack it again.


The cleanup attribute tells Puppet whether or not to delete the archive file once 
it has been unpacked. Usually, this will be set to true, unless you need to keep the 
archive around, or unless you don't need to unpack it in the first place.
Once the file has been deleted by cleanup, Puppet won't re-download 
the archive file, /tmp/wordpress.tar.gz, the next time you 
apply the manifest, even though it has ensure => present. 
The creates clause tells Puppet that the archive has already been 
downloaded and extracted.
Exploring the standard library
One of the oldest-established Puppet Forge modules is puppetlabs/stdlib, the official 
Puppet standard library. We looked at this briefly earlier in the chapter when we used it as 
an example of installing a module with r10k, but let's look more closely now and see what 
the standard library provides and where you might use it.
Rather than managing some specific software or file format, the standard library aims to 
provide a set of functions and resources which can be useful in any piece of Puppet code. 
Consequently, well-written Forge modules use the facilities of the standard library rather 
than implementing their own utility functions which do the same thing. You should do the 
same in your own Puppet code—when you need a particular piece of functionality, check the 
standard library first to see if it solves your problem, rather than implementing it yourself.
Safely installing packages with ensure_packages
As you know, you can install a package using the package resource, like this (package.pp):
package { 'cowsay':
  ensure => installed,
}
But what happens if you also install the same package in another class in a different part of 
your manifest? Puppet will refuse to run, with an error like this:
Error: Evaluation Error: Error while evaluating a Resource Statement, 
Duplicate declaration: Package[cowsay] is already declared in file /
vagrant/examples/package.pp:1; cannot redeclare at /vagrant/examples/
package.pp:4 at /vagrant/examples/package.pp:4:1 on node localhost

Mastering modules
[ 118 ]
If both of your classes really require the package, then you have a problem. You could create 
a class which simply declares the package, and then include that in both classes, but that is 
a lot of overhead for a single package. Worse, if the duplicate declaration is in a third-party 
module, it may not be possible, or advisable, to change that code.
What we need is a way to declare a package which will not cause a conflict if that package 
is also declared somewhere else. The standard library provides this facility in the ensure_
packages() function. Call ensure_packages() with an array of package names, and they 
will be installed if they are not already declared elsewhere (package_ensure.pp):
ensure_packages(['cowsay'])
If you need to pass additional attributes to the package resource, you can supply them 
in a hash as the second argument to ensure_packages(), like this (package_ensure_
params.pp):
ensure_packages(['rake'],
  { 'provider' => 'gem' })
Why is this better than using the package resource directly? When you declare the same 
package resource in more than one place, Puppet will give an error message and refuse 
to run. If the package is declared by ensure_packages(), however, Puppet will run 
successfully.
Since it provides a safe way to install packages without resource conflicts, you should always 
use ensure_packages() instead of the built-in package resource. It is certainly essential 
if you're writing modules for public release, but I recommend you use it in all your code. 
We'll use it to manage packages throughout the rest of this book.
Modifying files in place with file_line
Often when managing configuration with Puppet, we would like to change or add a particular 
line to a file, without incurring the overhead of managing the whole file with Puppet. 
Sometimes it may not be possible to manage the whole file in any case, as another Puppet 
class or another application may be managing it. We could write an exec resource to modify 
the file for us, but the standard library provides a resource type for exactly this purpose: 
file_line.
Here's an example of using the file_line resource to add a single line to a system config 
file (file_line.pp):
file_line { 'set ulimits':
  path => '/etc/security/limits.conf',
  line => 'www-data         -       nofile          32768',
}

Chapter 7
[ 119 ]
If there is a possibility that some other Puppet class or application may need to modify the 
target file, use file_line instead of managing the file directly. This ensures that your class 
won't conflict with any other attempts to control the file.
You can also use file_line to find and modify an existing line, using the match attribute 
(file_line_match.pp):
file_line { 'set root email alias':
  path  => '/etc/aliases',
  line  => 'root: john@bitfieldconsulting.com',
  match => '^root: ',
}
The value of match is a regular expression, and if Puppet finds a line in the file which 
matches this expression, it will replace it with the value of line. (If you need to potentially 
change multiple lines, set the multiple attribute to true, or Puppet will complain when 
more than one line matches the expression.)
You can also use file_line to delete a line in a file if it is present (file_line_absent.pp):
file_line { 'remove csh from valid shells':
  ensure            => absent,
  path              => '/etc/shells',
  match             => '^/bin/csh',
  match_for_absence => true,
}
When using ensure => absent, you also need to set the 
match_for_absence attribute to true if you want Puppet 
to actually delete matching lines.
Introducing some other useful functions
The grep() function will search an array for a regular expression and return all matching 
elements (grep.pp):
$values = ['foo', 'bar', 'baz']
notice(grep($values, 'ba.*'))
# Result: ['bar', 'baz']

Mastering modules
[ 120 ]
The member() and has_key() functions return true if a given value is in the specified 
array or hash, respectively (member_has_key.pp):
$values = ['foo', 'bar', 'baz']
notice(member($values, 'foo'))
# Result: true
$valuehash = { 'a' => 1, 'b' => 2, 'c' => 3 }
notice(has_key($valuehash, 'b'))
# Result: true
The empty() function returns true if its argument is an empty string, array, or hash 
(empty.pp):
notice(empty(''))
# Result: true
notice(empty([]))
# Result: true
notice(empty({}))
# Result: true
The join() function joins together the elements of a supplied array into a string, using a 
given separator character or string (join.pp):
$values = ['1', '2', '3']
notice(join($values, '... '))
# Result: '1... 2... 3'
The pick() function is a neat way to provide a default value when a variable happens to 
be empty. It takes any number of arguments, and returns the first argument which is not 
undefined or empty (pick.pp):
$remote_host = ''
notice(pick($remote_host, 'localhost'))
# Result: 'localhost'

Chapter 7
[ 121 ]
Sometimes you need to parse structured data in your Puppet code which comes from an 
outside source. If that data is in YAML format, you can use the loadyaml() function to read 
and parse it into a native Puppet data structure (loadyaml.pp):
$db_config = loadyaml('/vagrant/examples/files/database.yml')
notice($db_config['development']['database'])
# Result: 'dev_db'
The dirname() function is very useful if you have a string path to a file or directory and 
you want to reference its parent directory; for example, to declare it as a Puppet resource 
(dirname.pp):
$file = '/var/www/vhosts/mysite'
notice(dirname($file))
# Result: '/var/www/vhosts'
The pry debugger
When a Puppet manifest doesn't do quite what you expect, troubleshooting the problem can 
be difficult. Printing out the values of variables and data structures with notice() can help, 
as can running puppet apply -d to see detailed debug output, but if all else fails, you can 
use the standard library's pry() method to enter an interactive debugger session (pry.pp):
pry()
With the pry gem installed in Puppet's context, you can call pry() at any point in your code. 
When you apply the manifest, Puppet will start an interactive Pry shell at the point where 
the pry() function is called. You can then run the catalog command to inspect Puppet's 
catalog, which contains all the resources currently declared in your manifest:
sudo puppet apply /vagrant/examples/pry_install.pp
sudo puppet apply /vagrant/examples/pry.pp
...
[1] pry(#<Puppet::Parser::Scope>)> catalog
=> #<Puppet::Resource::Catalog:0x00000001bbcf78
...
 @resource_table={["Stage", "main"]=>Stage[main]{}, ["Class", 
"Settings"]=>Class[Settings]{}, ["Class", "main"]=>Class[main]{}},
 @resources=[["Stage", "main"], ["Class", "Settings"], ["Class", 
"main"]],
...
Once you've finished inspecting the catalog, type exit to quit the debugger and continue 
applying your Puppet manifest.

Mastering modules
[ 122 ]
Writing your own modules
As we've seen, a Puppet module is a way of grouping together a set of related  
code and resources which perform some particular task, such as managing the Apache  
web server, or dealing with archive files. But how do you actually create a module? In this 
section we'll develop a module of our own to manage the Network Time Protocol (NTP) 
service, familiar to most system administrators as the easiest way to keep server clocks 
synchronized with the Internet time standard. (Of course, it's not necessary to write your 
own module for this, because a perfectly good one exists on the Puppet Forge. But we'll do 
so anyway, for learning purposes.)
Creating a repo for your module
If we're going to use our new module alongside others that we've installed from Puppet 
Forge, then we should create a new Git repo just for our module. Then we can add its details 
to our Puppetfile and have r10k install it for us.
If you've already worked through Chapter 3, Managing your Puppet code with Git, you'll 
have created a GitHub account. If not, go to that chapter and follow the instructions in the 
Creating a GitHub account and project section before continuing.
1.	 Log into your GitHub account and click the Start a project button.
2.	 On the Create a new repository screen, enter a suitable name for your repo (I'm 
using pbg-ntp, for the Puppet Beginner's Guide NTP module).
3.	 Check the Initialize this repository with a README box.
4.	 Click Create repository.
5.	 GitHub will take you to the project page for the new repository. Click the Clone  
or download button. If you're using GitHub with an SSH key, as we discussed in 
Chapter 3, Managing your Puppet code with Git, copy the Clone with SSH link. 
Otherwise, click Use HTTPS and copy the Clone with HTTPS link.
6.	 On your own computer, or wherever you develop Puppet code, run the following 
command to clone the new repo (use the GitHub URL you copied in the previous 
step instead of this one):
git clone https://github.com/bitfield/pbg-ntp.git
When the clone operation completes successfully, you're ready to get started creating your 
new module.

Chapter 7
[ 123 ]
Writing the module code
As you'll see, if you look inside the Puppet Forge modules you've already installed, the 
modules have a standard directory structure. This is so that Puppet can automatically find 
the manifest files, templates, and other components within the module. Although complex 
modules have many subdirectories, the only ones we will be concerned with in this example 
are manifests and files. In this section, we'll create the necessary subdirectories, write 
the code to manage NTP, and add a config file which the code will install.
All the code and files are available in the GitHub repo for this module at
https://github.com/bitfield/pbg-ntp.
1.	 Run the following commands to create the manifests and files subdirectories:
cd pbg-ntp
mkdir manifests
mkdir files
2.	 Create the file manifests/init.pp with the following contents:
# Manage NTP
class ntp {
  ensure_packages(['ntp'])
  file { '/etc/ntp.conf':
    source  => 'puppet:///modules/ntp/ntp.conf',
    notify  => Service['ntp'],
    require => Package['ntp'],
  }
  service { 'ntp':
    ensure => running,
    enable => true,
  }
}
3.	 Create the file files/ntp.conf with the following contents:
driftfile /var/lib/ntp/ntp.drift
pool 0.ubuntu.pool.ntp.org iburst
pool 1.ubuntu.pool.ntp.org iburst
pool 2.ubuntu.pool.ntp.org iburst
pool 3.ubuntu.pool.ntp.org iburst

Mastering modules
[ 124 ]
pool ntp.ubuntu.com
restrict -4 default kod notrap nomodify nopeer noquery limited
restrict -6 default kod notrap nomodify nopeer noquery limited
restrict 127.0.0.1
restrict ::1
4.	 Run the following commands to add, commit, and push your changes to GitHub 
(you'll need to enter your GitHub username and password if you're not using an  
SSH key):
git add manifests/ files/
git commit -m 'Add module manifest and config file'
[master f45dc50] Add module manifest and config file
 2 files changed, 29 insertions(+)
 create mode 100644 files/ntp.conf
 create mode 100644 manifests/init.pp
git push origin master
Note that the source attribute for the ntp.conf file looks like the following:
puppet:///modules/ntp/ntp.conf
We haven't seen this kind of file source before, and it's generally only used within the 
module code. The puppet:// prefix indicates that the file comes from within the Puppet 
repo, and the path /modules/ntp tells Puppet to look within the ntp module for it. 
Although the ntp.conf file is actually in the directory modules/ntp/files, we don't 
need to specify the files part—that's assumed, because this is a file resource. (It's not just 
you—this confuses everybody.)
Rather than installing the ntp package via a package resource, we use ensure_
packages() from the standard library, as described earlier in this chapter.
Creating and validating the module metadata
Every Puppet module should have a file in its top-level directory named metadata.json, 
which contains helpful information about the module that can be used by module management 
tools, including Puppet Forge.
Create the file metadata.json with the following contents (use your own name and  
GitHub URLs):
{
  "name": "pbg-ntp",
  "version": "0.1.1",
  "author": "John Arundel",

Chapter 7
[ 125 ]
  "summary": "Example module to manage NTP",
  "license": "Apache-2.0",
  "source": "https://github.com/bitfield/pbg-ntp.git",
  "project_page": "https://github.com/bitfield/pbg-ntp",
  "tags": ["ntp"],
  "dependencies": [
    {"name":"puppetlabs/stdlib","version_requirement":">= 4.15.0 < 
5.0.0"}
  ],
  "operatingsystem_support": [
    {
      "operatingsystem": "Ubuntu",
      "operatingsystemrelease": [ "16.04" ]
    }
  ]
}
Most of these are fairly self-explanatory. The tags attribute is an array of strings which will 
help people find your module if it is listed on Puppet Forge, and it's usual to tag your module 
with the name of the software or service it manages (in this case, ntp).
If your module relies on other Puppet modules, which is very likely (for example, this 
module relies on puppetlabs/stdlib for the ensure_packages() function) you use the 
dependencies metadata to record this. You should list each module used by your module, 
along with the earliest and latest versions of that module which will work with your module. 
(If the currently-released version works, specify the next major release as the latest version. 
For example, if your module works with stdlib version 4.15.0 and that's the latest version 
available, specify 5.0.0 as the highest compatible version.)
Finally, the operatingsystem_support metadata lets you specify which operating 
systems and versions your module works with. This is very helpful for people searching 
for a Puppet module which will work with their operating system. If you know your 
module works with Ubuntu 16.04, as the example module does, you can list that in the 
operatingsystem_support section. The more operating systems your module can 
support, the better; so if possible, test your module on other operating systems and list them 
in the metadata once you know they work.
For full details on module metadata and how to use it, see the Puppet 
documentation at
https://docs.puppet.com/puppet/latest/
reference/modules_metadata.html.

Mastering modules
[ 126 ]
It's important to get the metadata for your module right, and there's a little tool that can 
help you with this, called metadata-json-lint.
1.	 Run the following commands to install metadata-json-lint and check your 
metadata:
sudo gem install metadata-json-lint
metadata-json-lint metadata.json
2.	 If metadata-json-lint produces no output, your metadata is valid and you 
can go on to the next steps. If you see error messages, fix the problem before 
continuing.
3.	 Run the following commands to add, commit, and push your metadata file to 
GitHub:
git add metadata.json
git commit -m 'Add metadata.json'
git push origin master
Tagging your module
Just like when you use third-party Puppet Forge modules, it's important to be able to specify 
in your Puppetfile the exact version of your module to be installed. You can do this by using 
Git tags to attach a version tag to a specific commit in your module repo. As you develop the 
module further and make new releases, you can add a new tag for each release.
For the first release of your module, which according to the metadata is version 0.1.1, run 
the following commands to create and push the Release tag:
git tag -a 0.1.1 -m 'Release 0.1.1'
git push origin 0.1.1
Installing your module
We can use r10k to install our new module, just as we did with the Puppet Forge modules, 
with one small difference. Since our module isn't on the Puppet Forge (yet), just specifying 
the name of the module in our Puppetfile isn't enough; we need to supply the Git URL, so 
that r10k can clone the module from GitHub.
1.	 Add the following mod statement to your Puppetfile (using your GitHub URL instead 
of mine):
mod 'pbg/ntp',
  :git => 'https://github.com/bitfield/pbg-ntp.git',
  :tag => '0.1.1'

Chapter 7
[ 127 ]
2.	 Because the module also requires puppetlabs/stdlib, add this mod  
statement too:
mod 'puppetlabs/stdlib', '4.15.0'
3.	 Now install the module in the normal way with r10k:
sudo r10k puppetfile install --verbose
r10k can install a module from any Git repo you have access to; all you have to do is add the 
:git and :tag parameters to the mod statement in your Puppetfile.
Applying your module
Now that you've created, uploaded, and installed your module, we can use it in a manifest.
1.	 In your main Puppet repo (we will now call this the control repo, as distinct from the 
various module repos you will eventually have, and we'll find out more about this 
in Chapter 12, Putting it all together), create the file manifests/ntp.pp with the 
following content:
include ntp
2.	 Run the following command to apply the manifest:
sudo puppet apply manifests/ntp.pp
If you're using the Vagrant box, or a recent version of Ubuntu, your server will likely already 
be running NTP, so the only change you'll see Puppet apply will be the ntp.conf file. 
Nonetheless, it confirms that your module works.
More complex modules
Of course, the module we've developed is a very trivial example. However, it demonstrates 
the essential requirements of a Puppet module. As you become a more advanced Puppet 
coder, you will be creating and maintaining much more complicated modules, similar to 
those you download and use from Puppet Forge.
Real-world modules often feature one or more of the following components:


Multiple manifest files and subdirectories


Parameters (which may be supplied directly or looked up from Hiera data)


Custom facts and custom resource types and providers


Example code showing how to use the module

Mastering modules
[ 128 ]


Specs and tests which developers can use to validate their changes


Dependencies on other modules (which must be declared in the module metadata)


Support for multiple operating systems
You can find more detailed information about modules and advanced features 
of modules in the Puppet documentation at
https://docs.puppet.com/puppet/latest/reference/
modules_fundamentals.html
Uploading modules to the Puppet Forge
It's very easy to upload a module to the Puppet Forge: all you need to do is sign up for 
an account, use the puppet module build command to create an archive file of your 
module, and upload it via the Puppet Forge website.
Before deciding to write a module in the first place, though, you should check whether 
there is already a module on the Puppet Forge which does what you need. There are over 
4,500 modules available at the time of writing, so it's quite likely that you'll be able to use an 
existing Puppet Forge module instead of writing your own. Contributing a new module when 
there is already one available just makes it more difficult for users to choose which module 
they should use. For example, there are currently 150 modules which manage the Nginx web 
server. Surely this is at least 149 too many. So only submit a new module if you've made sure 
that there are no similar modules already on the Puppet Forge.
If there is a module which covers the software you want to manage, but it doesn't support 
your operating system or version, consider improving this module instead of starting a new 
one. Contact the module author to see whether and how you can help improve their module 
and extend support to your operating system. Similarly, if you find bugs in a module, or want 
to make improvements to it, open an issue (if there is an issue tracker associated with the 
module), fork the GitHub repo (if it's versioned on GitHub), or contact the author to find out 
how you can help. The vast majority of Puppet Forge modules are written and maintained by 
volunteers, so your support and contributions benefit the entire Puppet community.
If you don't want to fork or contribute to an existing module, consider writing a small 
wrapper module which extends or overrides the existing module, rather than creating a new 
module from scratch.
If you do decide to write and publish your own module, use facilities from the standard 
library wherever possible, such as ensure_packages(). This will give your module the best 
chance of being compatible with other Forge modules.

Chapter 7
[ 129 ]
If you want to contribute more to the Puppet module community, 
consider joining the Vox Pupuli group, which maintains over 100 open 
source Puppet modules:
https://voxpupuli.org/
Summary
In this chapter, we've gained an understanding of Puppet modules, including an introduction 
to the Puppet Forge module repository. We've seen how to search for the modules we need, 
and how to evaluate the results, including Puppet Approved and Puppet Supported modules, 
operating system support, and download count.
We've looked at using the r10k tool to download and manage Puppet modules in your 
infrastructure, and how to specify the modules and versions you need in your Puppetfile.
Introducing the standard library for Puppet, we've covered the use of ensure_packages() 
to avoid package conflicts between modules, the file_line resource, which provides  
line-level editing for config files, and a host of useful functions for manipulating data,  
as well as looking at the Pry debugger.
To fully understand how modules work, we developed a simple module from scratch to 
manage the NTP service, hosted in its own Git repository and managed via a Puppetfile and 
r10k. We've also seen what a metadata module requires, and how to create it and validate 
it using metadata-json-lint.
Finally, we've looked at some of the features of more sophisticated modules, discussed 
uploading modules to the Puppet Forge, and outlined some considerations to bear in mind 
when you're deciding whether to start a new module, or extend and improve an existing one. 


[ 131 ]
Classes, roles, and profiles
Our life is frittered away by detail. Simplify, simplify!
                                                  —Henry David Thoreau
In this chapter, you will explore the details of Puppet classes and the distinction between 
defining a class and declaring a class, and also learn how to use parameters with classes 
and how to declare appropriate data types for your parameters. You'll learn how to create 
defined resource types, and how they differ from classes. You'll also see how to organize 
your Puppet code using the concepts of nodes, roles, and profiles.
8

Classes, roles, and profiles
[ 132 ]
Classes
We've come across classes a few times so far in this book, without really explaining them. 
Let's explore a little further now and see how to use this key Puppet language building block.
The class keyword
You may have noticed that in the code for our example NTP module in Chapter 7, Mastering 
modules (in the Writing the module code section), we used the class keyword:
class ntp {
  ...
}
If you're wondering what the class keyword does, the surprising answer is nothing at all. 
Nothing, that is, except inform Puppet that the resources it contains should be grouped 
together and given a name (ntp), and that these resources should not be applied yet.
You can then use this name elsewhere to tell Puppet to apply all the resources in the class 
together. We applied our example module by using the include keyword:
include ntp
The following example shows a class definition, which makes the class available to Puppet, 
but does not yet apply any of its contained resources:
class CLASS_NAME {
  ...
}
The following example shows a declaration of the CLASS_NAME class, which applies all the 
resources in that class (and the class must have already been defined):
include CLASS_NAME
The include keyword declares a class, but you can only use it when you are not supplying 
any parameters to the class. If you need to supply parameters, you have to use the class 
keyword to declare the class, as in the following example:
class { 'apache':
  default_vhost => false,
}

[ 133 ]
Note that this is a different way of using the class keyword. Previously, we used it to define 
a class (tell Puppet about it). Now we're using it to declare that class (have Puppet apply 
the resources it contains). You always need to use the class keyword to define a class, but 
you only need to use the class keyword to declare a class if you are supplying parameters 
directly to the class. If you don't need to supply parameters directly, you can use include 
instead to declare the class:
include apache
Declaring parameters to classes
If all a class does is group together related resources, that's still useful, but a class becomes 
much more powerful if we can use parameters. Parameters are just like resource attributes: 
they let you pass data to the class to change how it's applied.
The following example shows how to define a class that takes parameters. It's a simplified 
version of the ntp class we developed for our NTP module (ntp.pp):
# Manage NTP
class ntp (
  String $version = 'installed',
) {
  ensure_packages(['ntp'],
    {
      'ensure' => $version,
    }
  )
}
class { 'ntp':
  version => 'latest',
}
The important part to look at is in parentheses after the start of the class definition. This 
specifies the parameters that the class accepts:
String $version = 'installed',
String tells Puppet that we expect this value to be a String, and it will raise an error if we 
try to pass it anything else, such as an Integer. $version is the name of the parameter. 
Finally, the 'installed' part specifies a default value for the parameter. If someone 
declares this class without supplying the $version parameter, Puppet will fill it in 
automatically using this default value.

Classes, roles, and profiles
[ 134 ]
If you don't supply a default value for a parameter, that makes the parameter mandatory, so 
Puppet will not let you declare the class without supplying a value for that parameter.
When you declare this class, you do it in exactly the same way that we did previously with 
the Puppet Forge modules, using the class keyword and the name of the class followed by 
a list of parameters and values:
class { 'ntp':
  version => 'latest',
}
Classes can take more than one parameter, of course, and the following (contrived) example 
shows how to declare multiple parameters of various types (ntp2.pp):
# Manage NTP
class ntp2 (
  Boolean $start_at_boot,
  String[1] $version = 'installed',
  Enum['running', 'stopped'] $service_state = 'running',
) {
  ensure_packages(['ntp'],
    {
      'ensure' => $version,
    }
  )
  service { 'ntp':
    ensure => $service_state,
    enable => $start_at_boot,
  }
}
class { 'ntp2':
  start_at_boot => true,
  version       => latest,
  service_state => running,
}
Let's look closely at the parameter list:
  Boolean $start_at_boot,
  String[1] $version = 'installed',
  Enum['running', 'stopped'] $service_state = 'running',

[ 135 ]
The first parameter is of Boolean type and named $start_at_boot. There's no default 
value, so this parameter is mandatory. Mandatory parameters must be listed first, before any 
optional parameters.
We saw the $version parameter in the previous example, but now it's a String[1] instead 
of a String. What's the difference? String[1] is a string with at least one character. This 
means that you can't pass the empty string to such a parameter, for example. It's a good idea 
to specify a minimum length for String parameters, if appropriate, to catch the case where an 
empty string is accidentally passed to the class.
The final parameter, $service_state, is of a new type, Enum, which we haven't come 
across before. With an Enum parameter, we can specify exactly the list of allowed values it 
can take.
If your class expects a String parameter which can only take one of a handful of values,  
you can list them all in an Enum parameter declaration, and Puppet will not allow any  
value to be passed to that parameter unless it is in that list. In our example, if you try to 
declare the ntp2 class and pass the value bogus to the $service_state parameter,  
you'll get this error:
Error: Evaluation Error: Error while evaluating a Resource Statement, 
Class[Ntp2]: parameter 'service_state' expects a match for 
Enum['running', 'stopped'], got String at /vagrant/examples/ntp2.
pp:22:1 on node localhost
Just like any other parameter, an Enum parameter can take a default value, as it does in  
our example.
Puppet tip 
If you want to test by applying only a certain class and not your whole 
manifest, you can use the --tags parameter to puppet apply. For 
example, if you want Puppet to apply only the ntp class on the node, ignoring 
all others, run sudo papply --tags ntp.
Automatic parameter lookup from Hiera data
When we first encountered Puppet resources, such as files, packages, and users, we began 
by giving them literal attribute values in the manifest. Later, when we learned about Hiera 
data, we used the power of Hiera lookups to dynamically configure our resources based on 
facts, nodes, and other circumstances.

Classes, roles, and profiles
[ 136 ]
Let's take the same step now with classes, and see how to supply values for class parameters 
from Hiera data. In our first example ntp class, we defined a $version parameter:
class ntp (
  String $version = 'installed',
) {
In that example, when we declared the class, we specified a literal value for version in the 
manifest:
class { 'ntp':
  version => 'latest',
}
Instead of giving that value in the manifest, we can specify it via Hiera. If a key exists in Hiera 
named ntp::version, its value will be passed to the ntp class as the value of version. 
For example, suppose the Hiera data looks like the following:
ntp::version: 'latest'
In this case, Puppet will automatically find this value and pass it to the ntp class when it's 
declared.
In general, Puppet determines parameter values in the following order of priority  
(highest first):
1.	 Literal parameters specified in a class declaration.
2.	 Automatic parameter lookup from Hiera (the key must be named  
CLASS_NAME::PARAMETER_NAME).
3.	 Default values specified in a class definition.
Let's revisit our previous ntp2 example, this time supplying values for its parameters from 
Hiera data. The Hiera data file will contain the following:
ntp2::start_at_boot: true
ntp2::version: 'latest'
ntp2::service_state: 'running'
To declare this class on a node, we can now use the include keyword, because all the 
parameters will be looked up automatically:
include ntp2

[ 137 ]
This is exactly equivalent to declaring ntp2 with literal parameters, as we did in the previous 
example:
class { 'ntp2':
  start_at_boot => true,
  version       => latest,
  service_state => running,
}
However, because we are using Hiera data, we can use the hierarchy to supply different 
values for different nodes, different operating systems, or any other circumstances we 
choose.
Best practice
Use automatic parameter lookup from Hiera data wherever you can, to 
simplify your manifests and keep configuration data separate from code. 
Since automatic parameter lookup is built into Puppet, you can use it with 
all modules and classes, whether or not their documentation explicitly 
says that you can.
Parameter data types
You should always specify types for your class parameters, as it makes it easier to catch 
errors where the wrong parameters or values are being supplied to the class. If you're using 
a String parameter, for example, if possible, make it an Enum parameter with an exact list 
of the values your class accepts. If you can't restrict it to a set of allowed values, specify a 
minimum length with String[x]. (If you need to specify a maximum length too, the syntax 
is String[min, max].)
Available data types
So far in this chapter, we've encountered the data types String, Enum, and Boolean. Here 
are the others:


Integer (whole numbers)


Float (floating-point numbers, which have optional decimal fractions)


Numeric (matches either integers or floats)


Array


Hash


Regexp


Undef (matches a variable or parameter which hasn't been assigned a value)

Classes, roles, and profiles
[ 138 ]
There are also abstract data types, which are more general:


Optional (matches a value which may be undefined or not supplied)


Pattern (matches strings which conform to a specified regular expression)


Scalar (matches Numeric, String, Boolean, or Regexp values, but not Array, 
Hash, or Undef)


Data (matches Scalar values, but also Array, Hash, and Undef)


Collection (matches Array and Hash)


Variant (matches one of a specified list of data types)


Any (matches any data type)
In general, you should use as specific a data type as possible. For example, if you know that 
a parameter will always be an integer number, use Integer. If it needs to accept floating-
point values as well, use Numeric. If it could be a string as well as a number, use Scalar.
Range parameters
Most types can also accept parameters in square brackets, which make the type declaration 
more specific. For example, we've already seen that String can take a pair of parameters 
indicating the minimum and maximum length of the string.
Most types can take range parameters—Integer[0] matches any Integer greater than or 
equal to zero, while Float[1.0, 2.0] matches any Float between 1.0 and 2.0 inclusively.
If either range parameter is the special value default, the default minimum or maximum 
value for the type will be used. For example, Integer[default, 100] matches any 
Integer less than or equal to 100.
For arrays and hashes, the range parameters specify the minimum and maximum number 
of elements or keys; Array[16] specifies an array of no less than 16 elements, and 
Hash[5,5] specifies a hash containing exactly 5 key-value pairs.
Content type parameters
Types which represent a collection of values, such as Array and Hash (or their parent  
type, Collection), can also take a parameter indicating the type of values they contain.  
For example, Array[Integer] matches an array of Integer values.

[ 139 ]
If you declare a content type parameter to a collection, then all the values in that collection 
must match the declared type. If you don't specify a content type, the default is Data, which 
matches (almost) any type of value. The content type parameter can itself take parameters: 
Array[Integer[1]] declares an array of positive integers.
Hash takes two content type parameters, the first indicating the data type of its keys, the 
second the data type of its values. Hash[String, Integer] declares a hash whose keys 
are strings, each of which is associated with an integer value (this would match, for example, 
the hash {'eggs' => 61}).
You can specify both range and content type parameters at once—Array[String, 1, 
10] matches an array of between 1 and 10 strings. Hash[String, Hash, 1] specifies a 
hash with string keys and hash values containing at least one key-value pair.
Flexible data types
If you don't know exactly what type the values may be, you can use one of Puppet's more 
flexible abstract types, such as Variant, which specifies a list of allowed types. For example, 
Variant[String, Integer] allows its value to be either a String or an Integer.
Similarly, Array[Variant[Enum['true', 'false'], Boolean]] declares an array of 
values which can be either the String values 'true' or 'false', or the Boolean values 
true and false.
The Optional type is very useful when a value may be undefined. For example, 
Optional[String] specifies a String parameter which may or may not be passed to 
the class. Normally, if a parameter is declared without a default value, Puppet gives an error 
when it is not supplied. If it is declared Optional, however, it may be omitted, or set to 
undef (which is the special value indicating that the parameter has not been defined).
The Pattern type allows you to specify a regular expression. All strings matching that 
regular expression will be allowed values for the parameter. For example, Pattern[/a/] 
will match any string which contains the lowercase letter a. In fact, you can specify as many 
regular expressions as you like. Pattern[/a/, /[0-9]/] matches any string which 
contains the letter a, and also matches any string which contains a digit.

Classes, roles, and profiles
[ 140 ]
Defined resource types
Whereas a class lets you group together related resources, a defined resource type lets you 
create new kinds of resources, and declare as many instances of them as you like. A defined 
resource type definition looks a lot like a class (defined_resource_type.pp):
# Manage user and SSH key together
define user_with_key(
  String $key_type,
  String $key,
) {
  user { $title:
    ensure     => present,
    managehome => true,
  }
  file { "/home/${title}/.ssh":
    ensure => directory,
    owner  => $title,
    group  => $title,
    mode   => '0700',
  }
  ssh_authorized_key { $title:
    user => $title,
    type => $key_type,
    key  => $key,
  }
}
You can see that instead of the class keyword, we use the define keyword. This tells 
Puppet that we are creating a defined resource type instead of a class. The type is called 
user_with_key, and once it's defined, we can declare as many instances of it as we want, 
just like any other Puppet resource:
user_with_key { 'john':
  key_type => 'ssh-rsa',
  key      => 'AAAA...AcZik=',
}
When we do this, Puppet applies all the resources inside user_with_key—a user,  
a .ssh directory for that user, and an ssh_authorized_key for the user, containing  
the specified key.

[ 141 ]
Wait, we seem to be referring to a parameter called $title in the example 
code. Where does that come from? $title is a special parameter which 
is always available in classes and defined resource types, and its value is 
the title of this particular declaration of the class or type. In the example, 
that's john, because we gave the declaration of user_with_key the title 
john.
So what's the difference between defined resource types and classes? They look pretty much 
the same. They seem to act the same. Why would you use one rather than the other? The 
most important difference is that you can only have one declaration of a given class on a 
given node, whereas you can have as many different instances of a defined resource type as 
you like.
Recall our example ntp class, which installs and runs the NTP daemon. Usually, you would 
only want one NTP service per node. There's very little point in running two. So we declare 
the class once, which is all we need.
Contrast this with the user_with_key defined resource type. It's quite likely that you'll 
want more than one user_with_key on a given node, perhaps several. In this case, a 
defined resource type is the right choice.
Defined resource types are ideal in modules when you want to make a resource 
available to users of the module. For example, in the puppetlabs/apache module, the 
apache::vhost resource is a defined resource type, provided by the apache class. You can 
think of a defined resource type as being a wrapper for a collection of multiple resources.
Remember this rule of thumb when deciding whether to create a class or 
a defined resource type: if it's reasonable to have more than one instance 
on a given node, it should be a defined resource type, but if there will 
only ever be one instance, it should be a class.
Node definitions, roles, and profiles
In Chapter 3, Managing your Puppet code with Git, we introduced the node keyword and 
explained that you can use node definitions to control which resources are applied to a 
given node. It's time to look at node definitions in a little more detail, and explore more 
sophisticated ways to organize your manifests, including roles and profiles.

Classes, roles, and profiles
[ 142 ]
Nodes
As you know, when you apply a manifest to a node, by default Puppet applies all the 
resources declared in the manifest. When we have more than one node, we probably  
want different resources applied to different nodes, so how do we do that?
The node keyword introduces a node definition, and all the resources contained in the  
node definition will only be applied on nodes whose hostname matches the node name 
(node_app1.pp):
node 'app1' {
  file { '/tmp/only_on_app1':
    content => "I'm only needed on the node named 'app1'!",
  }
}
Node definitions should live in a file in the manifests directory. A node definition doesn't 
have to match exactly one node, although it often does. Puppet looks at the hostname of  
the node to decide whether or not it matches the node definition, and this could be either 
the short hostname (for example, app1) or the fully-qualified domain name (for example, 
app1.example.com).
If your node definition is named app1, then this would match any node whose fully-qualified 
domain name begins with app1, such as app1.example.com, app1.my_app.com,  
and so on.
Node names can also be a regular expression, as in the following example (node_regex.pp):
node /^app[\d+]$/ {
  include my_app
}
The regular expression ^app[\d+]$ matches a node name beginning with app followed 
by one or more digits, so this would match app1, app2, app3, app99, and so on. This is 
a common pattern to use when you have multiple nodes all serving the same role and 
all using the same Puppet configuration, and named in a numerical sequence. (Puppet's 
implementation of regular expressions is covered in more detail in Chapter 5, Variables, 
expressions, and facts).
In theory, then, you could just put your whole manifest in a single file inside a bunch of node 
definitions for your various nodes. While that would work, it's not good practice—for one 
thing, if nodes have resources in common, they would all be duplicated in different node 
definitions, and it also makes it a lot harder to read your manifest file and see what each 
node is doing.

[ 143 ]
A better way is to just have each node definition include the modules the node needs, as in 
the following example (node_modules.pp):
node 'app1' {
  include postgresql
  include apache
  include java
  include tomcat
  include my_app
}
This keeps the node definitions nice and short, and it's easier to see from the list of included 
modules what kind of role the node plays. In the previous example, the node is clearly an 
app server running a Java app named my_app served by Tomcat behind Apache, and backed 
by a PostgreSQL database. All the common or node-specific parameters for these modules 
can be supplied in Hiera, as described in the Automatic parameter lookup from Hiera data 
section earlier in this chapter.
We can do even better than this, however, and we'll see how in the next section.
Roles
To make it obvious that the node is an app server, why don't we create a class called 
role::app_server, which exists only to encapsulate the node's included modules? That 
class definition would look like this (role_app_server.pp):
# Be an app server
class role::app_server {
  include postgresql
  include apache
  include java
  include tomcat
  include my_app
}
We call this idea a role class. A role class could simply be a module in its own right, or to 
make it clear that this is a role class, we could organize it into a special role module. If you 
keep all your role classes in a single module, then they will all be named role::something, 
depending on the role they implement.

Classes, roles, and profiles
[ 144 ]
It's important to note that role classes are no different than any other 
class we use in Puppet; we call them role classes only to remind 
ourselves that they are for expressing the roles assigned to a particular 
node.
The node definition in our manifest is now simply:
node 'app1' {
  include role::app_server
}
Looking at the manifest file, it's now very easy to see what each node is for, and all app 
servers can now just include role::app_server.
Profiles
We've tidied up our manifest quite a bit, with the rule that node definitions should only 
include roles. This makes the manifest file more self-documenting, and our role classes 
are all neatly filed in the role module, each of them encapsulating all the functionalities 
required for that role. It's a big improvement. But can we do even better?
Let's look at a role class such as role::app_server. It contains lots of lines including 
modules, like the following:
  include tomcat
If all you need to do is include a module and have the parameters automatically looked up 
from Hiera data, then there's no problem. This is the kind of simple, encouraging, unrealistic 
example you'll see in product documentation or on a conference slide.
Real-life Puppet code is often more complicated, however, with logic and conditionals 
and special cases, and extra resources that need to be added, and so forth. We don't 
want to duplicate all this code when we use Tomcat as part of another role (for example, 
serving another Tomcat-based app). How can we neatly encapsulate it at the right level of 
abstraction and avoid duplication?
We could, of course, create a custom module for each app, which hides away all that messy 
support code. However, it's a big overhead to create a new module just for a few lines of 
code, so it seems like there should be a niche for a small layer of code which bridges the gap 
between roles and modules.

[ 145 ]
We call this a profile class. A profile encapsulates some specific piece of software or 
functionality which is required for a role. In our example, the app_server role requires 
several pieces of software—PostgreSQL, Tomcat, Apache, and so on. Each of these can now 
have its own profile.
Let's rewrite the app_server role to include profiles, instead of modules (role_app_
server_profiles.pp):
# Be an app server
class role::app_server {
  include profile::postgresql
  include profile::apache
  include profile::java
  include profile::tomcat
  include profile::my_app
}
What would be in these profile classes? The profile::tomcat class, for example,  
would set up the specific configuration of Tomcat required, along with any app-specific or 
site-specific resources required, such as firewall rules, logrotate config, file and directory 
permissions, and so on. The profile wraps the module, configures it, and provides everything 
the module does not, in order to support this particular application or site.
The profile::tomcat class might look something like the following example, adapted 
from a real production manifest (profile_tomcat.pp):
# Site-specific Tomcat configuration
class profile::tomcat {
  tomcat::install { '/usr/share/tomcat7':
    install_from_source => false,
    package_ensure      => present,
    package_name        => ['libtomcat7-java','tomcat7- 
   common','tomcat7'],
  }
  exec { 'reload-tomcat':
    command     => '/usr/sbin/service tomcat7 restart',
    refreshonly => true,
  }
  lookup('tomcat_allowed_ips', Array[String[7]]).each |String 
 $source_ip| {
    firewall { "100 Tomcat access from ${source_ip}":
      proto  => 'tcp',

Classes, roles, and profiles
[ 146 ]
      dport  => '8080',
      source => $source_ip,
      action => 'accept',
    }
  }
  file { '/usr/share/tomcat7/logs':
    ensure  => directory,
    owner   => 'tomcat7',
    require => Tomcat::Install['/usr/share/tomcat7'],
  }
  file { '/etc/logrotate.d/tomcat7':
    source => 'puppet:///site- 
    modules/profile/tomcat/tomcat7.logrotate',
  }
}
The exact contents of this class don't really matter here, but the point you should take away 
is that this kind of site-specific glue code, wrapping third-party modules and connecting 
them with particular applications, should live in a profile class.
In general, a profile class should include everything needed to make that particular software 
component or service work, including other profiles if necessary. For example, every profile 
which requires a specific configuration of Java should include that Java profile. You can 
include a profile from multiple other profiles without any conflicts.
Using profile classes in this way both makes your role classes neater, tidier, and easier to 
maintain, and also allows you to re-use the profiles for different roles. The app_server 
role includes these profiles, and other roles can include them as well. This way, our code is 
organized to reduce duplication and encourage re-use. The rule of thumb is, roles should 
only include profiles.
If you're still confused about the exact distinction between node definitions, roles, and 
profiles, don't worry, you're in good company. Let's try and define them as succinctly as 
possible:


Node definitions specify the resources that should be applied on a particular node. 
Node definitions should only include roles; ideally, only one role. (In real life, though, 
nodes often perform more than one role.) 


Roles identify a particular function for a node, such as being an app server or a 
database server. A role exists to document what a node is for. Roles should only 
include profiles, but they can include any number of profiles.

[ 147 ]


Profiles identify a particular piece of software or functionality which contributes 
to a role; for example, the tomcat profile is required for the app_server role. 
Profiles generally install and configure a specific software component or service, its 
associated business logic, and any other Puppet resources needed. Profiles are the 
glue layers which sit between roles and modules.
Summary
In this chapter, we've looked at a range of different ways of organizing your Puppet code. 
We've covered classes in detail, explaining how to use the class keyword to define a new 
class, use the include keyword to declare the class without parameters, and use the class 
keyword with a different syntax to declare the class with parameters.
Declaring parameters involves specifying the allowable data types for parameters, and we've 
had a brief overview of Puppet's data types, including scalars, collections, content types, 
range parameters, abstract types, and flexible types. We've seen how to separate your code 
from your configuration data by supplying module parameters in Hiera and using Puppet's 
automatic parameter lookup mechanism.
We've introduced the defined resource type, and explained the difference between defined 
resource types and classes, and when you would use one or the other.
We've looked at how to use node definitions to apply only certain resources to particular 
nodes, and how to use node names and regular expressions to match single or multiple 
nodes. We've introduced the idea of the role class, which encapsulates everything needed 
for a node to fulfil a particular role, such as an app server.
Finally, we've seen how to use profile classes to configure and support a particular software 
package or service, and how to compose several profile classes into a single role class. 
Between them, roles and profiles bridge the gap between node definitions (at the top level) 
and modules and configuration data (at the lowest level). We can summarize the rules by 
saying that node definitions should only include roles, and roles should only include profiles.
In the next chapter, we'll look at using Puppet to create files using templates, iteration, and 
Hiera data.


[ 149 ]
Managing files with templates
Simplicity does not precede complexity, but follows it.
                                                                                                                      —Alan Perlis
In this chapter, we'll learn about an important and powerful feature of Puppet—the 
template. We'll see how to use a simple template to interpolate the values of Puppet 
variables, facts, and Hiera data into a file, and we'll also introduce more complex templates 
using iteration and conditional statements to generate dynamic configuration files.
9

Managing files with templates
[ 150 ]
What are templates?
In previous chapters, we used Puppet to manage the contents of files on the node by various 
means, including setting the contents to a literal string using the content attribute and 
copying a file from a Puppet module using the source attribute. While these methods are 
very useful, they are limited in one respect; they can only use static text, rather than building 
the contents of the file dynamically, based on Puppet data.
The dynamic data problem
To see why this is a problem, consider a common Puppet file management task, such as 
a backup script. There are a number of site- and node-specific things the backup script 
needs to know—the local directories to backup, the destination to copy them to, and any 
credentials needed to access the backup storage. While we could insert these into the script 
as literal values, this is rather inflexible. We might have to maintain several versions of the 
script, each identical to the others except for a backup location, for example. This is clearly 
less than satisfactory.
Consider a configuration file for an application where some of the settings depend on 
specific information about the node—the available memory, perhaps. Obviously, we don't 
want to have to maintain multiple versions of an almost identical config file, each containing 
a suitable value for all the different sizes of memory we may come across. We have a way of 
obtaining that information directly in Puppet, as we saw in Chapter 5, Variables, expressions, 
and facts, and we also have a flexible, powerful database for configuration data, as we 
saw in Chapter 6, Managing data with Hiera. The question is how we can insert this data 
dynamically into text files.
Puppet template syntax
Puppet's template mechanism is one way to achieve this. A template is simply an  
ordinary text file, containing special placeholder markers that Puppet will replace with  
the relevant data values. The following example shows what these markers look like  
(aws_credentials.epp):
aws_access_key_id = <%= $aws_access_key %>
Everything outside the <%= and %> delimiters is literal text and will be rendered as-is  
by Puppet. 
The text inside the delimiters, however, is interpreted as a Puppet expression (in this 
case, just the variable $aws_access_key), which will be evaluated when the template is 
compiled, and the result will be interpolated into the text.

Chapter 9
[ 151 ]
For example, if the variable $aws_access_key has the value AKIAIAF7V6N2PTOIZVA2, 
then, when the template is processed by Puppet, the resulting output text will look like the 
following:
aws_access_key_id = AKIAIAF7V6N2PTOIZVA2
You can have as many of these delimited expressions (called tags) in the template as you like, 
and they will all be evaluated and interpolated when the template is used.
Puppet's template mechanism is called EPP (short for Embedded Puppet), and template files 
have the extension .epp.
Using templates in your manifests
Since the end result of a template is a file, you won't be surprised that we use Puppet's file 
resource to work with templates. In fact, we use an attribute of the file resource that 
you've seen before—the content attribute.
Referencing template files
Recall from Chapter 2, Creating your first manifests, that you can use the content attribute 
to set a file's contents to a literal string:
file { '/tmp/hello.txt':
  content => "hello, world\n",
}
And, of course, you can interpolate the value of Puppet expressions into that string:
file { "/usr/local/bin/${task}":
  content => "echo I am ${task}\n",
  mode    => '0755',
}
So far, so familiar, but we can take one further step and replace the literal string with a call to 
the epp() function (file_epp.pp):
file { '/usr/local/bin/backup':
  content => epp('backup.sh.epp'),
  mode    => '0755',
}
Puppet will compile the template file referenced by backup.sh.epp, replacing any tags 
with the value of their expressions, and the resulting text will be written to the file /usr/
local/bin/backup.

Managing files with templates
[ 152 ]
You can use the epp() function anywhere a string is expected, but it's most common to use 
it to manage a file, as shown in the example.
To reference a template file from within a module (for example, in our NTP module from 
Chapter 7, Mastering modules), put the file in the modules/ntp/templates/ directory 
and prefix the filename with ntp/, as in the following example:
file { '/etc/ntp.conf':
  content => epp('ntp/ntp.conf.epp'),
}
Remember 
Don't include templates/ as part of the path. Puppet knows it's a 
template, so it will automatically look in the templates/ directory of 
the named module.
Inline templates
Your template text need not be in a separate file; if it's a short template, you can put it in 
a literal string in your Puppet manifest and use the inline_epp() function to compile it 
(file_inline_epp.pp):
$web_root = '/var/www/vhosts'
$backup_dir = '/backups'
file { '/usr/local/bin/backup':
  content => inline_epp('rsync -a <%= $web_root %>/ <%= $backup_dir 
%>/'),
  mode    => '0755',
}
Note that we used a single-quoted string to specify the inline template text. If we'd used 
a double-quoted string, Puppet would have interpolated the values of $web_root and 
$backup_dir before processing the template, which is not what we want.
In general, though, it's better and more readable to use a separate template file for all but 
the simplest templates.

Chapter 9
[ 153 ]
Template tags
The tag we've been using in the examples so far in this chapter is known as an expression-
printing tag:
<%= $aws_access_key %>
Puppet expects the contents of this tag to have a value, which will then be inserted into the 
template in place of the tag.
A non-printing tag is very similar, but will not generate any output. It has no = sign in the 
opening delimiter:
<% notice("This has no effect on the template output") %>
You can also use a comment tag to add text, which will be removed when Puppet compiles 
the template:
<%# This is a comment, and it will not appear in the output of the 
template %>
Computations in templates
So far, we've simply interpolated the value of a variable into our template, but we can do 
more. Template tags can contain any valid Puppet expression.
It's very common for certain values in config files to be computed from other values, such 
as the amount of physical memory on the node. We saw an example of this in Chapter 5, 
Variables, expressions, and facts, where we computed a config value based on the value of 
$facts['memory']['system']['total_bytes'].
Naturally, whatever we can do in Puppet code, we can also do in a template, so here's the 
same computation in template form (template_compute.epp):
innodb_buffer_pool_size=<%= $facts['memory']['system']['total_bytes'] 
* 3/4 %>
The generated output (on my Vagrant box) is:
innodb_buffer_pool_size=384193536
You're not restricted to numerical computations; you can do anything a Puppet expression 
can do, including string manipulation, array and hash lookups, fact references, function calls, 
and so on.

Managing files with templates
[ 154 ]
Conditional statements in templates
You might not be very impressed with templates so far, given that you can already 
interpolate the values of Puppet expressions in strings, and hence files, without using a 
template. That said, templates allow you to interpolate data into much bigger files than 
would be practical or desirable to create with a literal string in your Puppet manifest.
Templates also allow you to do something else very useful—include or exclude sections of 
text based on the result of some Puppet conditional expression.
We've already met conditional statements in manifests in Chapter 5, Variables, expressions, 
and facts, where we used them to conditionally include sets of Puppet resources (if.pp):
if $install_perl {
  ...
} else {
  ...
}
Since the content of template tags is just Puppet code, you can use an if statement in a 
template too. Here's a similar example to the previous one, but this time controlling the 
inclusion of a block of configuration in a template (template_if.epp):
<% if $ssl_enabled { -%>
  ## SSL directives
SSLEngine on
SSLCertificateFile      "<%= $ssl_cert %>"
SSLCertificateKeyFile   "<%= $ssl_key %>"
  ...
<% } -%>
This looks a little more complicated, but it's actually exactly the same logic as in the previous 
example. We have an if statement that tests the value of a Boolean variable, $ssl_
enabled, and, depending on the result, the following block is either included or excluded.
You can see that the if statement and the closing } are enclosed in non-printing tags, so 
they generate no output themselves, and as Puppet compiles the template, it will execute 
the Puppet code within the tags and that will determine the output. If $ssl_enabled  
is true, the file generated by the template will contain the following (with the values of 
$ssl_cert and $ssl_key interpolated):
  ## SSL directives
SSLEngine on
SSLCertificateFile      "<%= $ssl_cert %>"
SSLCertificateKeyFile   "<%= $ssl_key %>"
  ...

Chapter 9
[ 155 ]
Otherwise, this part of the template will be omitted. This is a very useful way of conditionally 
including blocks in a configuration file.
Just as with the if statements in manifest files, you can also use else to include an 
alternative block instead, if the conditional statement is false.
The closing tags in the previous example had an extra leading hyphen: -%>. 
When you use this syntax, Puppet suppresses any trailing whitespace and 
line break after the tag. It's common to use this syntax with non-printing 
template tags, because otherwise you'd end up with empty lines in the 
output.
Iteration in templates
If we can generate parts of a file from Puppet expressions, and also include or exclude parts 
of the file depending on conditions, could we generate parts of the file with a Puppet loop? 
That is to say, can we iterate over an array or hash, generating template content for each 
element? Indeed we can. This very powerful mechanism enables us to generate files of 
arbitrary size, based on Puppet variables, or Hiera and Facter data.
Iterating over Facter data
Our first example generates part of the config file for an application that captures network 
packet data. To tell it which interfaces to listen on, we need to generate a list of all the live 
network interfaces on the node. 
How can we generate this output? We know Facter can give us a list of all the network 
interfaces available, with $facts['networking']['interfaces']. This is actually a 
hash, where the key is the name of the interface and the value is a hash of the interface's 
attributes, such as the IP address and netmask. 
You may recall from Chapter 5, Variables, expressions, and facts, that in order to iterate over 
a hash, we use a syntax like the following:
HASH.each | KEY, VALUE | {
  BLOCK
}

Managing files with templates
[ 156 ]
So let's apply this pattern to the Facter data and see what the output looks like (template_
iterate.epp):
<% $facts['networking']['interfaces'].each |String $interface, Hash 
$attrs| { -%>
interface <%= $interface %>;
<% } -%>
Each time round the loop, the values of $interface and $attrs will be set to the next 
key and value of the hash returned by $facts['networking']['interfaces']. As it 
happens, we will not be using the value of $attrs, but we still need to declare it as part of 
the loop syntax.
Each time round the loop, the value of $interface is set to the name of the next interface 
in the list, and a new output line such as the following is generated:
interface em1;
At the end of the loop, we have generated as many output lines as we have interfaces, which 
is the desired result. Here's the final output:
interface em1;
interface em2;
interface em3;
interface em4;
interface em5;
interface lo;
Iterating over structured facts
The next configuration data required for our application is a list of IP addresses associated 
with the node, which we can generate in a way similar to the previous example. 
We can use more or less the same Puppet code as in the previous example; only this time we 
will be using each interface's $attrs hash to get the IP address of the associated interface.
The following example shows how this works (template_iterate2.epp):
<% $facts['networking']['interfaces'].each |String $interface, Hash 
$attrs| { -%>
local_address<%= $attrs['bindings'][0]['address'] %>;
<% } -%>
The loop is the same as in the previous example, but this time each output line contains not 
the value of $interface but the value of $attrs['bindings'][0]['address'], which 
contains the IP address of each interface.

Chapter 9
[ 157 ]
Here's the final output:
local_address 10.170.81.11;
local_address 75.76.222.21;
local_address 204.152.248.213;
local_address 66.32.100.81;
local_address 189.183.255.6;
local_address 127.0.0.1;
Iterating over Hiera data
In Chapter 6, Managing data with Hiera, we used a Hiera array of users to generate Puppet 
resources for each user. Let's use the same Hiera data now to build a dynamic configuration 
file using iteration in a template.
The SSH daemon sshd can be configured to allow SSH access only by a list of named users 
(with the AllowUsers directive); indeed, it's good practice to do this.
Security tip
Most servers accessible from the public Internet regularly receive 
brute-force login attempts for random usernames, and dealing with 
these can use up a lot of resources. If sshd is configured to allow only 
specified users, it can quickly reject any users not in this list, without 
having to process the request further.
If our users are listed in Hiera, then it's easy to use a template to generate this AllowUsers 
list for the sshd_config file. 
Just as we did when generating Puppet user resources, we will make a call to lookup() to 
get the array of users and iterate over this using each. The following example shows what 
this looks like in the template (template_hiera.epp):
AllowUsers<% lookup('users').each | $user | { -%>
 <%= $user -%>
<% } %>
Note the leading space in the second line, which results in the usernames in the output 
being space-separated. Note also the use of the leading hyphen to the closing tag (-%>), 
which, as we saw earlier in the chapter, will suppress any trailing whitespace on the line.
Here's the result:
AllowUsers katy lark bridget hsing-hui charles

Managing files with templates
[ 158 ]
Working with templates
One potential problem with templates (since they can include Puppet code, variables, and 
Hiera data) is that it's not always clear from the Puppet manifest what variables the template 
is going to use. Conversely, it's not easy to see from the template code where any referenced 
variables are coming from. This can make it hard to maintain or update templates, and also 
to debug any problems caused by incorrect data being fed into the template.
Ideally, we would like to be able to specify in the Puppet code exactly what variables the 
template is going to receive, and this list would also appear in the template itself. For extra 
credit, we would like to be able to specify the data type of input variables, in just the same 
way as we do for classes and defined resource types (see Chapter 8, Classes, roles, and 
profiles for more about this).
The good news is that EPP templates allow you to declare the parameters you want passed 
to your template, along with the required data types, in exactly the same way as you can for 
classes. While it's not compulsory to declare parameters for your EPP templates, it's a very 
good idea to do so. With declared and typed parameters, you will be able to catch most data 
errors at the template compilation stage, which makes troubleshooting much easier.
Passing parameters to templates
To declare parameters for a template, list them between pipe characters (|) inside a non-
printing tag, as shown in the following example (template_params.epp):
<% | String[1] $aws_access_key,
     String[1] $aws_secret_key,
| -%>
aws_access_key_id = <%= $aws_access_key %>
aws_secret_access_key = <%= $aws_secret_key %>
When you declare parameters in a template, you must pass those parameters explicitly, in 
hash form, as the second argument to the epp() function call. The following example shows 
how to do this (epp_params.pp):
file { '/root/aws_credentials':
  content => epp('/vagrant/examples/template_params.epp',
    {
      'aws_access_key' => 'AKIAIAF7V6N2PTOIZVA2',
      'aws_secret_key' => '7IBpXjoYRVbJ/rCTVLaAMyud+i4co11lVt1Df1vt',
    }
  ),
}

Chapter 9
[ 159 ]
This form of the epp() function call takes two parameters—the path to the template file, 
and a hash containing all the required template parameters. The keys to the hash are the 
parameter names, and the values are the values. (These need not be literal values; they 
could be Hiera lookups, for example.)
It's very likely that you will be using Hiera data in templates. Also, although in our previous 
AllowUsers example we called lookup() directly from the template to look up the data, 
this isn't really the best way to do it. Now that we know how to declare and pass parameters 
to templates, we should do the same thing with Hiera data.
Here is an updated version of the AllowUsers example, where we do the Hiera lookup in 
the manifest as part of the epp() call. First, we need to declare a $users parameter in the 
template (template_hiera_params.epp):
<% | Array[String] $users | -%>
AllowUsers<% $users.each | $user | { -%>
 <%= $user -%>
<% } %>
Then, when we compile the template with epp(), we pass in the Hiera data by calling 
lookup() in the parameter's hash (epp_hiera.pp):
file { '/tmp/sshd_config_example':
  content => epp('/vagrant/examples/template_hiera_params.epp',
    {
      'users' => lookup('users'),
    }
  ),
}
If you have declared a parameter list in the template, you must pass it exactly those 
parameters in the epp() call, and no others. EPP templates declare parameters in the same 
way as classes do: parameters can be given default values, and any parameter without a 
default value is mandatory.
It's clear from the previous example that declaring parameters makes it much easier to see 
what information the template is going to use from the calling code, and we now have the 
benefit of automated checking of the parameters and their types.
Note, however, that even templates with a parameter list can still access any Puppet variable 
or fact in the template body; Puppet does not prevent the template from using variables that 
have not been declared as parameters, or getting data directly from Hiera. It should be clear 
by now, though, that bypassing the parameter checking machinery in this way is a bad idea.

Managing files with templates
[ 160 ]
Best practices 
Use EPP templates for dynamically-generated files, declare typed parameters 
in the template, and pass those parameters as a hash to the epp() function. 
To make your template code easier to understand and maintain, always pass 
data explicitly to the template. If the template needs to look up Hiera data, 
do the lookup in your Puppet manifest and have the template declare a 
parameter to receive the data.
Validating template syntax
We've seen in this chapter that templates can contain complex logic and iteration that can 
generate almost any output required. The downside of this power and flexibility is that it can 
be difficult to read and debug template code.
Fortunately, Puppet includes a tool to check and validate your templates on the command 
line: puppet epp validate. To use it, run the following command against your template 
file:
puppet epp validate /vagrant/examples/template_params.epp
If there is no output, the template is valid. If the template contains an error, you will see an 
error message, like the following:
Error: Syntax error at '%' at /vagrant/examples/template_params.
epp:3:4
Error: Errors while validating epp
Error: Try 'puppet help epp validate' for usage
Rendering templates on the command line
As any programmer knows, even programs with valid syntax don't necessarily produce the 
correct results. It can be very useful to see exactly what output the template is going to 
produce, and Puppet also provides a tool to do this: puppet epp render.
To use it, run the following command:
puppet epp render --values "{ 'aws_access_key' => 'foo', 'aws_secret_
key' => 'bar' }" /vagrant/examples/template_params.epp
aws_access_key_id = foo
aws_secret_access_key = bar
The --values argument allows you to pass in a hash of parameter-value pairs, just as you 
would when calling the epp() function in your Puppet manifest.

Chapter 9
[ 161 ]
Alternatively, you can use the --values_file argument to reference a Puppet manifest file 
containing the hash of parameters:
echo "{ 'aws_access_key' => 'foo', 'aws_secret_key' => 'bar' }" 
>params.pp
puppet epp render --values_fileparams.pp /vagrant/examples/template_
params.epp
aws_access_key_id = foo
aws_secret_access_key = bar
You can pass parameters both on the command line, with --values, and from a file with 
--values_file, simultaneously. Parameters given on the command line will take priority 
over those from the file:
puppet epp render --values_fileparams.pp --values "{ 'aws_access_key' 
=> 'override' }" /vagrant/examples/template_params.epp
aws_access_key_id = override
aws_secret_access_key = bar
You can also use puppet epp render to test inline template code, using the -e switch to 
pass in a literal template string:
puppet epp render --values "{ 'name' => 'Dave' }" -e 'Hello, <%= $name 
%>'
Hello, Dave
Just as when testing your manifests, you can also use puppet apply to test your templates 
directly, using a command similar to the following:
sudo puppet apply -e "file { '/tmp/result': content => epp('/vagrant/
examples/template_iterate.epp')}"
One advantage of this approach is that all Puppet variables, facts, and Hiera data will be 
available to your template.
Legacy ERB templates
You'll probably come across references to a different type of Puppet template in older 
code and documentation: the ERB template. ERB (short for Embedded Ruby) was the only 
template mechanism provided in Puppet up until version 3.5, when EPP support was added, 
and EPP has now replaced ERB as Puppet's default template format.
ERB template syntax looks quite similar to EPP. The following example is a snippet from an 
ERB template:
AllowUsers<%= @users.join(' ') %><%= scope['::vagrant'] == 'yes' ? 
',vagrant' : '' %>

Managing files with templates
[ 162 ]
The difference is that the template language inside the tags is Ruby and not Puppet. Early 
versions of Puppet were rather limited in language features (for example, there was no 
each function to iterate over variables), so it was common to use Ruby code embedded in 
templates to work around this.
This required some complicated plumbing to manage the interface between Puppet and 
Ruby; for example, accessing variables in non-local scope in ERB templates requires the use 
of the scope hash, as in the previous example. Similarly, in order to access Puppet functions 
such as strftime(), you have to call:
scope.call_function('strftime', ...)
ERB templates also do not support declared parameters or type checking. I recommend you 
use only EPP templates in your own code.
Summary
In this chapter, we've looked at one of the most powerful tools in Puppet's toolbox, the 
template file. We've examined the EPP tag syntax and seen the different kinds of tag 
available, including printing and non-printing tags.
We've learned that not only can you simply insert values from variables into templates, 
but also you can include or exclude whole blocks of text, depending on the value of Puppet 
expressions, or generate templates of arbitrary size by iterating over arrays and hashes.
We've looked at some real-life examples of dynamically generating config files from Facter 
and Hiera data, and seen how to declare typed parameters in the template file, and pass in 
values for those parameters when calling the epp() function in your Puppet manifest.
We've seen how to check the syntax of templates using puppet epp validate, and how 
to render the output of a template using puppet epp render, passing in canned values for 
the template parameters using --values and --values_file, or using puppet apply to 
render the template directly.
Finally, we've touched on legacy ERB templates: where they come from, how they compare 
against EPP templates, and why, although you may still encounter ERB templates in the wild, 
you should only use EPP in your own code.
In the next chapter, we'll explore the popular topic of containers and look at how to manage 
the Docker engine and Docker containers with Puppet, and deal with the vexed issue of how 
to manage configuration in containers.

[ 163 ]
10
Controlling containers
The inside of a computer is as dumb as hell but it goes like mad!
—Richard Feynman
In this chapter, we'll look at the emerging topic of containers and see how it relates to 
configuration management. We'll see how to use Puppet to manage the Docker daemon, 
as well as images and containers, and explore some different strategies for managing 
configuration within containers.

Controlling containers
[ 164 ]
Understanding containers
Although the technology behind containers is at least 30 years old, it's only in the last few 
years that containers have really taken off (to mix a metaphor). This is largely thanks to the 
rise of Docker, a software platform which makes it easier to create and manage containers.
The deployment problem
The problem that Docker solves is principally one of software deployment; that is, making 
it possible to install and run your software in a wide variety of environments with minimal 
effort. Let's take a typical PHP web application, for example. To run the application, you need 
at least the following to be present on a node:


PHP source code


PHP interpreter


PHP's associated dependencies and libraries


PHP modules required by your application 


Compiler and build tools for building native binaries for PHP modules


Web server (for example, Apache)


Module for serving PHP apps (for example, mod_php)


Config files for your application 


User to run the application


Directories for things such as log files, images, and uploaded data
How do you manage all of this stuff? You can use a system package format, such as RPM 
or DEB, which uses metadata to describe its dependencies in terms of other packages, and 
scripts which can do much of the system configuration required.
However, this packaging is specific to a particular version of a particular operating system, 
and a package intended for Ubuntu 18.04, for example, will not be installable on Ubuntu 
16.04, or on Red Hat Enterprise Linux. Maintaining multiple packages for several popular 
operating systems is a large workload on top of maintaining the application itself.

Chapter 10
[ 165 ]
Options for deployment
One way to address this problem is for the author to provide configuration management 
manifests for the software, such as a Puppet module or a Chef recipe to install the software. 
However, if the intended user of the software does not use a CM tool, or uses a different tool, 
then this is no help. Even if they use exactly the same version of the same tool on the same 
operating system, they may have problems integrating the third-party module with it, and the 
module itself will depend on other modules, and so on. It's certainly not a turnkey solution.
Another option is the omnibus package; a package which contains everything the software 
needs to run. An omnibus package for our example PHP application might contain the 
PHP binaries and all dependencies, plus anything else the application needs. These are 
necessarily quite large packages, however, and omnibus packages are still specific to a 
particular operating system and version, and involve a lot of maintenance effort. 
Most package managers do not provide an efficient binary update facility, so even the 
smallest update requires re-downloading the entire package. Some omnibus packages even 
include their own config management tool!
Yet another solution is to provide an entire virtual machine image, such as a Vagrant box 
(the Puppet Labs Vagrant box we've been using throughout the book is a good example). 
This contains not only the application, plus dependencies and configuration, but the entire 
operating system as well. This is a fairly portable solution, since any platform which can run 
the virtual machine host software (for example, Virtualbox or VMWare) can run the VM itself.
However, there is a performance penalty with VMs, and they also consume a lot of 
resources, such as memory and disk space, and the VM images themselves are large and 
unwieldy to move around a network.
While, in theory, you could deploy your application by building a VM image and pushing 
it to a production VM host, and some people do this, it's far from an efficient method of 
distribution.
Introducing the container
In recent years, many operating systems have added facilities for self-contained execution 
environments, more concisely called containers, in which programs can run natively on the 
CPU, but with very limited access to the rest of the machine. A container is like a security 
sandbox, where anything running inside it can access files and programs inside the container, 
but nothing else.

Controlling containers
[ 166 ]
This is similar in principle to a virtual machine, except that the underlying technology is quite 
different. Instead of running on a virtual processor, via a software emulation layer, programs 
in a container run directly on the underlying physical hardware. This makes containers a 
great deal more efficient than VMs. To put it another way, you need much less powerful 
hardware to run containers than you do for virtual machines of the same performance.
A single virtual machine consumes a large amount of its host's resources, which means that 
running more than one VM on the same host can be quite demanding. By contrast, running a 
process inside a container uses no more resources than running the same process natively.
Therefore, you can run a very large number of containers on a single host, and each is 
completely self-contained, and has no access to either the host or any other container 
(unless you specifically allow it). A container, at the kernel level, is really just a namespace. 
Processes running in that namespace cannot access anything outside it, and vice versa. All 
the containers on a machine use the host operating system's kernel, so although containers 
are portable across different Linux distributions, for example, a Linux container cannot run 
on a Windows host.
What Docker does for containers
So, if containers themselves are provided by the kernel, what is Docker for? It turns out that 
having an engine is not quite the same thing as having a car. The operating system kernel 
may provide the basic facilities for containerization, but you also need the following:


A specification for how to build containers


A standard file format for container images


A protocol for storing, versioning, organizing, and retrieving container images


Software to start, run, and manage containers


Drivers to allow network traffic to and from containers


Ways of communicating between containers


Facilities for getting data into containers
These need to be provided by additional software. There are, in fact, many software 
frontends which allow you to manage containers: Docker, OCID, CoreOS/rkt, Apache Mesos, 
LXD, VMware Photon, Windows Server Containers, and so on. However, Docker is by far the 
market leader, and currently the majority of containers in production are running under 
Docker (a 2016 survey put the proportion at over 90 percent).

Chapter 10
[ 167 ]
Deployment with Docker
The principle of deploying software with containers is very simple: the software, plus 
everything it needs to run, is inside the container image, which is like a package file, but is 
executable directly by the container runtime.
To run the software, all you need to do is execute a command like the following (if you have 
Docker installed, try it!):
docker run bitfield/hello
Hello, world
Docker will download the specified image from your configured registry (this could be the 
public registry, called Docker Hub, or your own private Docker registry) and execute it. There 
are thousands of Docker images available for you to use, and many software companies are 
increasingly using Docker images as their primary way to deploy products.
Building Docker containers
But where do these Docker images come from? Docker images are like an archive or a 
package file, containing the file and directory layout of all the files inside the container, 
including executable binaries, shared libraries, and config files. To create this image file,  
you use the docker build command.
The docker build command takes as input a special text file called a Dockerfile, which 
specifies what should be in the container. Usually, a new Docker image is based on an 
existing image, with a few modifications. For example, there is a Docker image for Ubuntu 
Linux, which contains a fully-installed operating system ready to run. 
Your a Dockerfile might specify that you use the Ubuntu Docker image as a starting point, 
and then install the package nginx. The resulting Docker container contains everything that 
was in the stock Ubuntu image, plus the nginx package. You can now upload this image to a 
registry and run it anywhere using docker run.
If you want to package your own software with Docker, you can choose a suitable base image 
(such as Ubuntu) and write a Dockerfile which installs your software onto that base image. 
When you build the container image with docker build, the result will be a container with 
your software inside it, which anyone can run using docker run. The only thing they need 
to install is Docker.
This makes Docker a great option, both for software vendors to package their products in an 
easily installable format, and for users to try out different software quickly to see if it meets 
their needs.

Controlling containers
[ 168 ]
The layered filesystem
The Docker filesystem has a feature called layering. Containers are built up in layers,  
so that if something changes, only the affected layer and those above it need to be rebuilt. 
This makes it much more efficient to update container images once they've been built  
and deployed.
For example, if you change one line of code in your app and rebuild the container, only the 
layer that contains your app needs to be rebuilt, along with any layers above it. The base 
image and the other layers below the affected layer remain the same and can be reused for 
the new container.
Managing containers with Puppet
There are a few things you need to be able to do to package and run software with Docker:


Install, configure, and manage the Docker service itself


Build your images


Rebuild images when the Dockerfile changes or a dependency is updated


Manage the running images, their data storage, and their configuration
Unless you want to make your images public, you will also need to host an image registry for 
your own images.
These sound like the kind of problems that configuration management tools can solve, and, 
luckily, we have a great configuration management tool available. Oddly enough, while most 
people recognize that traditional servers need to be built and managed automatically by a 
tool such as Puppet, the same does not seem to be true (yet) of containers.
The trouble is, it's so easy to make a simple container and run it that many people think 
configuration management for containers is overkill. That may be so when you're first trying 
out Docker and experimenting with simple containers, but when you're running complex, 
multi-container services in production, at scale, things get more complicated.
First, containerizing non-trivial applications is non-trivial. They need dependencies, 
configuration settings and data, and ways to communicate with other applications and 
services, and while Docker provides you with tools to do this it doesn't do the work for you.
Second, you need an infrastructure on which to build your containers, update them, store 
and retrieve the resulting images, and deploy and manage them in production. Configuration 
management for containers is very much like configuration management for traditional 
server-based applications, except that it's happening at a slightly higher level. 

Chapter 10
[ 169 ]
Containers are great, but they don't do away with the need for configuration management 
tools (remember the Law of Conservation of Pain from Chapter 1, Getting started with 
Puppet):
"If you save yourself pain in one place, it pops up again in another. Whatever cool 
new technology comes along, it won't solve all our problems; at best, it will replace 
them with refreshingly different problems."
Managing Docker with Puppet
Puppet can certainly install and manage the Docker service for you, just as it can any other 
software, but it can also do a lot more. It can download and run Docker images, build images 
from Dockerfiles, mount files and directories on the container, and manage Docker volumes 
and networks. We'll see how to do all these things in this chapter.
Installing Docker
Before we do anything else, we'll need to install Docker on our node (using Puppet, of 
course). The puppetlabs/docker_platform module is ideal for this. 
1.	 If you've already installed the r10k module management tool, as shown in  
Chapter 7, Mastering modules, you're all set. Otherwise, run the following  
command to install r10k:
sudo gem install r10k
2.	 Run the following commands to install the puppetlabs/docker_platform 
module:
cd /etc/puppetlabs/code/environments/production
sudo cp /vagrant/examples/Puppetfile.docker Puppetfile
sudo r10k puppetfile install --verbose
INFO     -> Updating module /etc/puppetlabs/code/environments/
production/modules/docker
INFO     -> Updating module /etc/puppetlabs/code/environments/
production/modules/apt
INFO     -> Updating module /etc/puppetlabs/code/environments/
production/modules/docker_platform
INFO     -> Updating module /etc/puppetlabs/code/environments/
production/modules/stdlib
INFO     -> Updating module /etc/puppetlabs/code/environments/
production/modules/epel

Controlling containers
[ 170 ]
3.	 Once the module is installed, all you need to do to install Docker on your node is to 
apply a manifest like the following (docker_install.pp):
include docker
4.	 Run the following command to apply the manifest:
sudo puppet apply /vagrant/examples/docker_install.pp
To check that Docker is installed, run the following command (you may see a different 
version number, but that's OK):
docker --version
Docker version 17.05.0-ce, build 89658be
Running a Docker container
In order to run a Docker container, we first of all have to download it from a Docker registry, 
which is a server that stores container images. The default registry is Docker Hub, the official 
public Docker registry.
To do this with Puppet, you can use the docker::image resource (docker_image.pp):
docker::image { 'bitfield/hello':
  ensure => 'latest',
}
As with the package resource, if you specify ensure => latest, Puppet will check the 
registry every time it runs and make sure you have the latest available version of the image.
To run the image you've just downloaded, add a docker::run resource to your manifest 
(docker_run.pp):
docker::run { 'hello':
  image   => 'bitfield/hello',
  command => '/bin/sh -c "while true; do echo Hello, world; sleep 1; 
done"',
}
Apply this manifest with the following command:
sudo puppet apply /vagrant/examples/docker_run.pp
The docker::run resource tells Docker to fetch the image bitfield/hello from the 
local image cache and run it with the specified command, which in this case just loops 
forever printing Hello, world. (I told you containers were useful.)

Chapter 10
[ 171 ]
The container is now running on your node, and you can check this with the following 
command:
sudo docker ps
CONTAINER ID        IMAGE               COMMAND                  
CREATED             STATUS              PORTS               NAMES
ba1f4aced778        bitfield/hello      "/bin/sh -c 'while tr"   4 
minutes ago       Up 4 minutes                            hello
The docker ps command shows all currently running containers (docker ps -a will show 
stopped containers too), with the following information:


The container ID—Docker's internal identifier for the container


The image name (bitfield/hello in our example)


The currently executing command in the container


The creation time


The current status


Any ports mapped by the container


The human-readable name of the container (which is the title we gave the 
docker::run resource in our manifest)
The container is running as a service, and we can check that with the following command:
systemctl status docker-hello
* docker-hello.service - Daemon for hello
   Loaded: loaded (/etc/systemd/system/docker-hello.service; enabled; 
   vendor preset: enabled)
   Active: active (running) since Tue 2017-05-16 04:07:23 PDT; 1min 
   4s ago
 Main PID: 24385 (docker)
   CGroup: /system.slice/docker-hello.service
           `-24385 /usr/bin/docker run --net bridge -m 0b --name 
   hello bitfield/hello...
...
Stopping a container
According to the Docker documentation, you can stop a container by running sudo docker 
stop NAME. However, if you try this, and then run sudo docker ps again, you'll see that 
the container is still running. What's that about?
The Puppet module assumes, by default, that you want to run all containers as services; that 
is, to configure systemd to keep the container running and to start it at boot time.

Controlling containers
[ 172 ]
Therefore, if you want to stop a container which is running as a service, you will need to 
do this with Puppet, by setting the ensure parameter on the docker::run resource to 
absent, as in the following example (docker_absent.pp):
docker::run { 'hello':
  ensure => absent,
  image  => 'bitfield/hello',
}
Alternatively, on the command line, you can use the systemctl command to stop  
the service:
sudo systemctl stop docker-hello
If you don't want your container to be managed as a service by systemd, 
specify the parameter restart => always to the docker::run 
resource. This tells Docker to restart the container automatically when it 
exits, so Puppet does not need to create a systemd service to manage it.
Running multiple instances of a container
Of course, the true power of automation is the ability to scale. We're not limited to running a 
single instance of a given container; Puppet will happily start as many as you like.
Each docker::run resource must have a unique name, as with any other Puppet resource, so 
you can create them in an each loop, as in the following example (docker_run_many.pp):
range(1,20).each | $instance | {
  docker::run { "hello-${instance}":
    image   => 'bitfield/hello',
    command => '/bin/sh -c "while true; do echo Hello, world; sleep 1; 
done"',
  }
}
The range() function comes from the stdlib module, and, as you might expect, 
range(1,20) returns the sequence of integers between 1 and 20, inclusive. We iterate 
over this sequence with the each function, and each time through the loop $instance is 
set to the next integer.

Chapter 10
[ 173 ]
The docker::run resource title includes the value of $instance on each iteration,  
so each container will be uniquely named—hello-1, hello-2, ... hello-20. I've chosen 
the number 20 at random, just as an example; you could compute the number of instances 
to run based on the resources available, for example, the number of system CPUs or 
available memory.
Managing Docker images
Of course, it's very useful to be able to download and run public images from Docker Hub 
or other registries, but to unlock the real power of Docker we need to be able to build and 
manage our own images too.
Building images from Dockerfiles
As we saw in the previous examples, if you don't already have the specified container image 
on your system, Puppet's docker::image resource will pull it from Docker Hub for you and 
save it locally. 
The docker::image resource is most useful, however, for actually building Docker images. 
This is usually done using a Dockerfile, so here is an example Dockerfile we can use to build 
an image (Dockerfile.hello):
FROM library/alpine:3.1
CMD /bin/sh -c "while true; do echo Hello, world; sleep 1; done"
LABEL org.label-schema.vendor="Bitfield Consulting" \
  org.label-schema.url="http://bitfieldconsulting.com" \
  org.label-schema.name="Hello World" \
  org.label-schema.version="1.0.0" \
  org.label-schema.vcs-url="github.com:bitfield/puppet-beginners-
guide.git" \
  org.label-schema.docker.schema-version="1.0"
The FROM statement tells Docker what base image to start from, of the many public images 
available. FROM scratch would start with a completely empty container. FROM library/
ubuntu would use the official Ubuntu Docker image.
Of course, one of the key advantages of containers is that they can be as small or as large as 
they need to be, so downloading a 188 MB image containing all of Ubuntu is unnecessary if 
you simply want to run /bin/echo.
Alpine is another Linux distribution designed to be as small and lightweight as possible, 
which makes it ideal for containers. The library/alpine image is only 5 MB, 40 times 
smaller than ubuntu; quite a saving.

Controlling containers
[ 174 ]
Dockerfiles can be fairly simple, as in the example, or quite complex. 
You can find out more about the Dockerfile format and commands 
from the Docker documentation at https://docs.docker.com/
engine/reference/builder/.
The following code shows how to create a Docker image from this file (docker_build_
hello.pp):
docker::image { 'pbg-hello':
  docker_file => '/vagrant/examples/Dockerfile.hello',
  ensure      => latest,
}
Once the docker::image resource has been applied, the resulting pbg-hello image will 
be available for you to run as a container (docker_run_hello.pp):
docker::run { 'pbg-hello':
  image => 'pbg-hello',
}
Managing Dockerfiles
When you run your own apps in containers, or third-party apps in your own containers, you 
can manage the associated Dockerfiles with Puppet. Here's an example of a simple Dockerfile 
which builds a container using Nginx to serve a web page with a friendly greeting message 
(Dockerfile.nginx):
FROM nginx:1.13.0-alpine
RUN echo "Hello, world" >/usr/share/nginx/html/index.html
LABEL org.label-schema.vendor="Bitfield Consulting" \
  org.label-schema.url="http://bitfieldconsulting.com" \
  org.label-schema.name="Nginx Hello World" \
  org.label-schema.version="1.0.0" \
  org.label-schema.vcs-url="github.com:bitfield/puppet-beginners-
guide.git" \
  org.label-schema.docker.schema-version="1.0"
Here's the Puppet manifest, which manages this Dockerfile and builds an image from it 
(docker_build_nginx.pp):
file { '/tmp/Dockerfile.nginx':
  source => '/vagrant/examples/Dockerfile.nginx',

Chapter 10
[ 175 ]
  notify => Docker::Image['pbg-nginx'],
}
docker::image { 'pbg-nginx':
  docker_file => '/tmp/Dockerfile.nginx',
  ensure      => latest,
}
Run the following command to apply this manifest:
sudo puppet apply /vagrant/examples/docker_build_nginx.pp
Whenever the contents of the Dockerfile change, applying this manifest will cause the 
image to be rebuilt.
For the purposes of this example, we are building and running the container 
on the same node. In practice, though, you should build your containers on a 
dedicated build node and upload the resulting images to the registry, so that 
your production nodes can download and run them.
Here's the manifest to run the container we just built (docker_run_nginx.pp): 
docker::run { 'pbg-nginx':
  image         => 'pbg-nginx:latest',
  ports         => ['80:80'],
  pull_on_start => true,
}
The pull_on_start attribute tells Puppet to always download 
the latest available version of the container when starting or 
restarting it.
Run the following commands to remove any already running web servers and apply  
this manifest:
sudo apt-get -y --purge remove apache2
sudo service docker restart
sudo puppet apply /vagrant/examples/docker_run_nginx.pp
You can check that the container is working by browsing the following URL on your  
local machine:
http://localhost:8080
You should see the text Hello, world.

Controlling containers
[ 176 ]
If you're using the Vagrant box, port 8080 on your local machine is 
automatically mapped to port 80 on the VM, which is then mapped by Docker 
to port 80 on the pbg-nginx container. If for some reason you need to change 
this port mapping, edit your Vagrantfile (in the Puppet Beginner's Guide 
repo) and look for the following line:
  config.vm.network "forwarded_port", guest: 80, host: 
  8080
Change these settings as required, and run the following command on your local 
machine in the PBG repo directory:
vagrant reload
If you're not using the Vagrant box, the container's port 80 will be exposed at 
your local port 80, so the URL will be simply:
http://localhost
Building dynamic containers
Although Dockerfiles are a fairly powerful and flexible way of building containers, they are 
only static text files, and very often you will need to pass information into the container to 
tell it what to do. We might call such containers, whose configuration is flexible and based on 
data available at build time, dynamic containers.
Configuring containers with templates
One way to configure containers dynamically is to use Puppet to manage the Dockerfile as an 
EPP template (see Chapter 9, Managing files with templates), and interpolate the required 
data (which could come from Hiera, Facter, or directly from Puppet code).
Let's upgrade our previous Hello, world example to have Nginx serve any arbitrary text 
string supplied by Puppet at build time.
Here's the manifest to generate the Dockerfile from a template (docker_template.pp):
file { '/tmp/Dockerfile.nginx':
  content => epp('/vagrant/examples/Dockerfile.nginx.epp',
    {
      'message' => 'Containers rule!'
    }
  ),
  notify => Docker::Image['pbg-nginx'],
}
docker::image { 'pbg-nginx':

Chapter 10
[ 177 ]
  docker_file => '/tmp/Dockerfile.nginx',
  ensure      => latest,
}
Apply this manifest with the following command:
sudo puppet apply /vagrant/examples/docker_template.pp
When you have applied the manifest and built the container, you will find that if you change 
the value of message and reapply, the container will be rebuilt with the updated text.
This is a powerful technique, and the key to applying the power of Puppet to 
containers. Since you can have Puppet put any arbitrary data into a Dockerfile, 
you can configure anything about the container and its build process: the base 
image, the list of packages to install, files and data that should be added to the 
container, and even the command entry point for the container.
Self-configuring containers
Let's take this idea even further and use Puppet to dynamically configure a container which 
can fetch its data from Git. Instead of serving static text supplied at build time, we will have 
the container itself check out a Git repo for the website.
Most of the code from the previous example remains unchanged, except for the Dockerfile 
resource (docker_website.pp):
file { '/tmp/Dockerfile.nginx':
  content => epp('/vagrant/examples/Dockerfile.website.epp',
    {
      'git_url' => 'https://github.com/bitfield/pbg-website.git'
    }
  ),
  notify  => Docker::Image['pbg-nginx'],
}
docker::image { 'pbg-nginx':
  docker_file => '/tmp/Dockerfile.nginx',
  ensure      => latest,
}

Controlling containers
[ 178 ]
The Dockerfile itself is a little more complicated because we need to install Git in the 
container and use it to check out the supplied Git repo (Dockerfile.website.epp):
<% | String $git_url | -%>
FROM nginx:1.13.0-alpine
RUN apk update \
  && apk add git \
  && cd /usr/share/nginx \
  && mv html html.orig \
  && git clone <%= $git_url %> html
LABEL org.label-schema.vendor="Bitfield Consulting" \
  org.label-schema.url="http://bitfieldconsulting.com" \
  org.label-schema.name="Nginx Git Website" \
  org.label-schema.version="1.0.0" \
  org.label-schema.vcs-url="github.com:bitfield/puppet-beginners-
guide.git" \
  org.label-schema.docker.schema-version="1.0"
When you apply this manifest, and browse to http://localhost:8080, you should see 
the following text:
Hello, world!
This is the demo website served by the examples in Chapter 10, 
'Controlling containers', from the Puppet Beginner's Guide.
Although we supplied the git_url parameter directly to the Dockerfile template, that data 
could, of course, come from anywhere, including Hiera. With this technique, you can build a 
container to serve any website, simply by changing the Git URL passed to it.
Using the iteration pattern we saw in the docker_run_many example earlier in this chapter, 
you could build a set of containers like this from an array of git_url values, each serving a 
different website. Now we're really starting to exploit the power of Docker-plus-Puppet.
Run the following command to stop the container before going on to the next example:
sudo docker stop pbg-nginx
There's one slight problem with this idea. Although it's good to have the container be able to 
serve content from a Git repo determined at build time, every time the container is started 
or restarted it will have to run the Git clone process again. This can take time and, if the repo 
or the network is unavailable for some reason, it can stop the container from working.
A better solution would be to serve the content from persistent storage, and we'll see how to 
do that in the next section.

Chapter 10
[ 179 ]
Persistent storage for containers
Containers are designed to be transient; they run for a while and then disappear. Anything 
inside the container disappears with it, including files and data created during the container's 
run. This isn't always what we want, of course. If you're running a database inside a 
container, for example, you usually want that data to persist when the container goes away.
There are two ways of persisting data in a container: the first is to mount a directory from 
the host machine inside the container, known as a host-mounted volume, and the second is 
to use what's called a Docker volume. We'll look at both of these in the following sections.
Host-mounted volumes
If you want a container to be able to access files on the host machine's filesystem (such as 
application code that you're working on and that you want to test, for example), the easiest 
way to do that is to mount a directory from the host on the container. The following example 
shows how to do this (docker_mount.pp):
docker::run { 'mount_test':
  image   => 'library/alpine',
  volumes => ['/tmp/container_data:/mnt/data'],
  command => '/bin/sh -c "echo Hello, world >/mnt/data/hello.txt"',
}
The volumes attribute specifies an array of volumes to attach to the container. If the volume 
is of the form HOST_PATH:CONTAINER_PATH, Docker will assume you want to mount the 
directory HOST_PATH on the container. The path inside the container will be CONTAINER_
PATH. Any files which already exist in the mounted directory will be accessible to the 
container, and anything the container writes to the directory will still be available once the 
container has stopped.
If you apply this example manifest, the container will mount the host machine's /tmp/
container_data/ directory (this will be created if it doesn't exist) as /mnt/data/ in the 
container.
The command attribute tells the container to write the string Hello, world to the file /
mnt/data/hello.txt.
Run the following command to apply this manifest:
sudo puppet apply /vagrant/examples/docker_mount.pp

Controlling containers
[ 180 ]
The container will start, write the data, and then exit. If all has gone well, you'll see that the 
file /tmp/container_data/hello.txt is now present and contains the data written by 
the container:
cat /tmp/container_data/hello.txt
Hello, world
Host-mounted volumes are very useful when a container needs to access or share data 
with applications running on the host machine. For example, you could use a host-mounted 
volume with a container which runs syntax checks, lint, or continuous integration tests on 
your source code directory.
However, containers using host-mounted volumes are not portable, and they rely on a 
specific directory being present on the host machine. You can't specify a host-mounted 
volume in a Dockerfile, so you can't publish a container which relies on one. While  
host-mounted volumes can be useful for testing and development, a better solution in 
production is to use Docker volumes.
Docker volumes
A more portable way of adding persistent storage to containers is to use a Docker volume. 
This is a persistent data object which lives in Docker's storage area, and can be attached to 
one or more containers.
The following example shows how to use docker::run to start a container with a Docker 
volume (docker_volume.pp):
docker::run { 'volume_test':
  image   => 'library/alpine',
  volumes => ['pbg-volume:/mnt/volume'],
  command => '/bin/sh -c "echo Hello from inside a Docker volume >/
mnt/volume/index.html"',
}
The volumes attribute is a little different from the previous example. It has 
the form VOLUME_NAME:CONTAINER_PATH, which tells Docker that this is 
not a host-mounted volume, but a Docker volume named VOLUME_NAME. If 
the value before the colon is a path, Docker assumes you want to mount that 
path from the host machine, but, otherwise, it assumes you want to mount a 
Docker volume with the specified name.
As in the previous example, the container's command argument writes a message to a file on 
the mounted volume.

Chapter 10
[ 181 ]
If you apply this manifest, once the container has exited, you can see that the volume is still 
present by running the following command:
sudo docker volume ls
DRIVER              VOLUME NAME
local               pbg-volume
A Docker volume is a good way to store data that you need to keep even when the container 
is not running (a database, for example). It's also a good way to make data available to 
containers without having to load it into each container every time it starts.
In the website example earlier in the chapter, instead of each container checking out its own 
copy of the Git repo, you could check out the repo into a Docker volume and then have each 
container mount this volume when it starts.
Let's test that idea with the following manifest (docker_volume2.pp):
docker::run { 'volume_test2':
  image   => 'nginx:alpine',
  volumes => ['pbg-volume:/usr/share/nginx/html'],
  ports   => ['80:80'],
}
This is the same nginx container we used earlier in the chapter, which serves whatever is in 
its /usr/share/nginx/html directory as a website.
The volumes attribute tells the container to mount the pbg-volume volume on /usr/
share/nginx/html.
Run the following commands to apply this manifest:
sudo docker stop pbg-nginx
sudo puppet apply /vagrant/examples/docker_volume2.pp
If everything works as we expect, we should be able to browse to the following URL on the 
local machine:
http://localhost:8080/
We should also then see the following text:
Hello from inside a Docker volume
This is a very powerful feature of containers. They can read, write, and modify data created 
by other containers, maintain persistent storage of their own, and share data with other 
running containers, all by using volumes.

Controlling containers
[ 182 ]
A common pattern for running applications in Docker is to use multiple, communicating 
containers, each providing a single specific service. For example, a web application might use 
an Nginx container to serve an application to users, while storing its session data in a MySQL 
container mounting a persistent volume. It could also use a linked Redis container as an in-
memory, key-value store.
Apart from sharing data via volumes, though, how do these containers actually communicate 
over the network? We'll see the answer to that in the next section.
Networking and orchestration
We started off the chapter by saying that containers are completely self-contained and 
have no access to each other, even if they're running on the same host. But, to run real 
applications, we need containers to communicate. Fortunately, there is a way to do  
this: the Docker network.
Connecting containers
A Docker network is like a private chat room for containers; all the containers inside  
the network can talk to each other, but they can't talk to containers outside it or in  
other networks, and vice versa. All you need to do is have Docker create a network,  
give it a name, and then you can start containers inside that network and they will  
be able to talk to each other.
Let's develop an example to try this out. Suppose we want to run the Redis database inside 
a container and send data to it from another container. This is a common pattern for many 
applications.
In our example, we're going to create a Docker network and start two containers inside it. 
The first container is a public Docker Hub image that will run the Redis database server. The 
second container will install the Redis client tool and write some data to the Redis server 
container. Then, to check it worked, we can try to read the data back from the server. 
Run the following command to apply the Docker network example manifest:
sudo puppet apply /vagrant/examples/docker_network.pp
If everything worked as it should, our Redis database should now contain a piece of data 
named message, containing a friendly greeting, proving that we've passed data from one 
container to another over the Docker network.

Chapter 10
[ 183 ]
Run the following command to connect to the client container and check that this is  
the case:
sudo docker exec -it pbg-redis redis-cli get message
"Hello, world"
So how does it all work? Let's take a look at the example manifest. First of all, we create the 
network for the two containers to run in, using the docker_network resource in Puppet 
(docker_network.pp):
docker_network { 'pbg-net':
  ensure => present,
}
Now, we run the Redis server container, using the public redis:alpine image:
docker::run { 'pbg-redis':
  image => 'redis:alpine',
  net   => 'pbg-net',
}
Did you note that we supplied the net attribute to the 
docker::run resource? This specifies the Docker network 
that the container should run in.
Next, we build a container which has the Redis client (redis-cli) installed, so that we can 
use it to write some data to the Redis container.
Here's the Dockerfile for the client container (Dockerfile.pbg-demo):
FROM nginx:1.13.0-alpine
RUN apk update \
  && apk add redis
LABEL org.label-schema.vendor="Bitfield Consulting" \
  org.label-schema.url="http://bitfieldconsulting.com" \
  org.label-schema.name="Redis Demo" \
  org.label-schema.version="1.0.0" \
  org.label-schema.vcs-url="github.com:bitfield/puppet-beginners-
guide.git" \
  org.label-schema.docker.schema-version="1.0"

Controlling containers
[ 184 ]
We build this container in the usual way using docker::image:
docker::image { 'pbg-demo':
  docker_file => '/vagrant/examples/Dockerfile.pbg-demo',
  ensure      => latest,
}
Finally, we run an instance of the client container with docker::run, passing in a command 
to redis-cli to write some data to the other container:
docker::run { 'pbg-demo':
  image   => 'pbg-demo',
  net     => 'pbg-net',
  command => '/bin/sh -c "redis-cli -h pbg-redis set message \"Hello, 
  world\""',
}
As you can see, this container also has the attribute net => 'pbg-net'. It will, therefore, 
run in the same Docker network as the pbg-redis container, and so the two containers will 
be able to talk to each other.
When the container starts, the command attribute calls redis-cli with the following 
command:
redis-cli -h pbg-redis set message "Hello, world"
The -h pbg-redis argument tells Redis to connect to the host pbg-redis.
How does using the pbg-redis name connect to the right container? When 
you start a container inside a network, Docker automatically configures DNS 
lookups within the container to find the other containers in the network by 
name. When you reference a container name (the title of the container's 
docker::run resource, which in our example is pbg-redis), Docker will 
route the network connection to the right place.
The command set message "Hello, world" creates a Redis key named message and 
gives it the value "Hello, world".
We now have all the necessary techniques to containerize a real application: using Puppet 
to manage multiple containers, built from dynamic data, pushed to a registry, updated on 
demand, communicating over the network, listening on ports to the outside world, and 
persisting and sharing data via volumes.

Chapter 10
[ 185 ]
Container orchestration
We've seen a number of ways to manage individual containers in this chapter, but the 
question of how to provision and manage containers at scale and across multiple hosts  
(what we call container orchestration) remains.
For example, if your app runs in a container, you probably won't be running just one instance 
of the container; you need to run multiple instances, and route and load-balance traffic to 
them. You also need to be able to distribute your containers across multiple hosts, so that 
the application is resilient against the failure of any individual container host.
What is orchestration?
When running containers across a distributed cluster, you also need to be able to deal with 
issues such as networking between containers and hosts, failover, health monitoring, rolling 
out updates, service discovery, and sharing configuration data between containers via a key-
value database.
Although container orchestration is a broad task, and different tools and frameworks focus 
on different aspects of it, the core requirements of orchestration include:


Scheduling: Running a container on the cluster and deciding which containers to run 
on which hosts to provide a given service


Cluster management: Monitoring and marshalling the activity of containers and 
hosts across the cluster, and adding or removing hosts


Service discovery: Giving containers the ability to find and connect to the services 
and data they need to operate
What orchestration tools are available?
Google's Kubernetes and Docker's Swarm are both designed to orchestrate containers. 
Another product, Apache Mesos, is a cluster management framework which can operate on 
different kinds of resources, including containers.
Most containers in production today are running under one of these three orchestration 
systems. Kubernetes has been around the longest and has the biggest user base,  
but Swarm, though a relatively new arrival, is part of the official Docker stack,  
so is being rapidly adopted.
Because all these products are necessarily rather complicated to set up and operate, there 
is also the option of Platform-as-a-Service (PaaS) orchestration; essentially, running your 
containers on a managed cloud platform. Google Container Engine (GKE) is Kubernetes as a 
service; Amazon's EC2 Container Service (ECS) is a proprietary, Kubernetes-like system.

Controlling containers
[ 186 ]
As yet, Puppet integration with container orchestrators is somewhat limited and at an early 
stage, though, given the popularity of containers, this is likely to advance rapidly. There is 
some elementary support for generating Kubernetes configuration from Puppet resources, 
and some for managing Amazon ECS resources, but it's fair to say that automating container 
orchestration at scale with Puppet is still in its infancy so far. Watch this space, however.
Running Puppet inside containers
If a container can contain a whole operating system, such as Ubuntu, you might be 
wondering: Can't I just run Puppet inside the container?
You can, and some people do take this approach to managing containers. It also has a 
number of advantages:


You can use your existing Puppet manifests or Forge modules; there is no need to 
write complex Dockerfiles


Puppet will keep the container continuously updated; there is no need to rebuild 
when something changes
Of course, there are a few disadvantages too:


Installing Puppet inflates the image size considerably and pulls in all sorts of 
dependencies


Running Puppet slows down the build process and also consumes resources in the 
running container
There are also some hybrid options, such as running Puppet in the container during the build 
stage, and then removing Puppet and its dependencies, plus any intermediate build artifacts, 
before saving the final image.
Puppet's image_build module is a promising new way of building containers directly from 
Puppet manifests, and I expect to see rapid progress in this space in the near future.
Are containers mini-VMs or single processes?
Which option you favor probably depends on your basic approach to containers. Do you see 
them as mini-virtual machines, not too different from the servers you're already managing? 
Or do you see them as transient, lightweight, single-process wrappers?
If you treat containers as mini-VMs, you'll probably want to run Puppet in your containers, in 
the same way as you do on your physical and virtual servers. On the other hand, if you think 
a container should just run a single process, it doesn't seem appropriate to run Puppet in it. 
With single-process containers, there's very little to configure.

Chapter 10
[ 187 ]
I can see arguments in favour of the mini-VM approach. For one thing, it makes it much 
easier to transition your existing applications and services to containers; instead of running 
them in a VM, you just move the whole thing (application, support services, and database) 
into a container, along with all your current management and monitoring tools. 
However, while this is a valid approach, it doesn't really make the most of the inherent 
advantages of containers: small image sizes, quick deployment, efficient rebuilding, and 
portability.
Configuring containers with Puppet
Personally, I'm a container minimalist: I think the container should contain only what it 
needs to do the job. Therefore, I prefer to use Puppet to manage, configure, and build my 
containers from the outside, rather than from the inside, and that's why I've used that 
approach in this chapter.
That means generating Dockerfiles from templates and Hiera data, as we've seen in the 
examples, as well as templating the config files which the container needs. You can have the 
Dockerfile copy these files into the container during the build, or mount individual files and 
directories from the host onto the container.
As we've seen, a good way to handle shared data is to have Puppet write it into a Docker 
volume or a file on the host, which is then mounted (usually read-only) by all running 
containers.
The advantage of this is that you don't need to rebuild all your containers following a config 
change. You can simply have Puppet write the changes to the config volume and trigger each 
container to reload its configuration using a docker::exec resource, which executes a 
specified command on a running container.
Containers need Puppet too
At the risk of laboring a point, containerization is not an alternative to using configuration 
management tools such as Puppet. In fact, the need for configuration management is even 
greater, because you not only have to build and configure the containers themselves, but 
also store, deploy, and run them, all of which requires infrastructure.
As usual, Puppet makes this sort of task easier, more pleasant, and—most importantly—
more scalable.

Controlling containers
[ 188 ]
Summary
In this chapter, we've examined some of the problems associated with software deployment, 
some of the options for solving them, and the advantages of the container solution. We've 
briefly introduced the basics of container technology and Docker in particular, and seen  
that containers are another kind of configuration management problem, which Puppet  
can help solve.
We've installed the docker_platform module, used it to set up Docker on our VM, 
and build and run simple Docker containers. We've seen how to automatically rebuild 
the container image when the underlying Dockerfile changes, and how to use Puppet to 
configure a Dockerfile dynamically at build time.
We've introduced the topic of persistent storage for containers, including host-mounted 
volumes and Docker volumes, and how to manage these with Puppet. We've set up a Docker 
network with two communicating containers exchanging data over network ports.
We've looked at the advantages and disadvantages of running Puppet inside containers, as 
opposed to using Puppet to configure and build containers from the outside, and suggested 
a hybrid strategy where Puppet manages configuration data on a volume attached to running 
containers.
Finally, we've covered some of the issues involved in container orchestration, and introduced 
some of the most popular platforms and frameworks available.
In the next chapter, we'll learn how to use Puppet to manage cloud computing resources, 
with an in-depth example developing a software-defined Amazon EC2 infrastructure.

[ 189 ]
11
Orchestrating cloud resources
Rest is not idleness, and to lie sometimes on the grass under trees on a 
summer's day, listening to the murmur of the water, or watching the clouds 
float across the sky, is by no means a waste of time.
—John Lubbock
In this chapter, you'll learn how to use the puppetlabs/aws module to create and manage 
Amazon AWS cloud instances, and associated resources such as subnets, security groups, and 
VPCs. You'll also learn how to build your entire cloud infrastructure directly from Hiera data.

Orchestrating cloud resources
[ 190 ]
Introducing the cloud
Before exploring the advantages of cloud computing, perhaps, we should define what it 
is. In the pre-cloud days, if you needed computing power, you bought an actual, physical 
computer. However, from the customer's point of view, we don't necessarily want a 
computer—we just want to compute. We would like to be able to buy as much or as little 
compute resource as we happen to need at a given time, without paying a large fixed cost for 
a dedicated computer.
Enter virtualization. A single physical server can provide a large number of virtual servers, 
each of which is (in theory) completely isolated from the others. The hosting provider builds 
a platform (consisting of many physical servers networked together), which provides, from 
the customer's point of view, a large intangible cloud of virtual compute resource (hence  
the term).
Automating cloud provisioning
Creating new cloud instances is cheaper and easier than buying physical hardware, but you 
still have choices to make: how much CPU or memory the instance has, how much disk 
space, what kind of disks (physical, solid-state, network-attached storage), what operating 
system should be installed, whether the instance has a public IP address, what firewall rules 
it should have, and so on.
If you've read this far in the book, you should now recognize this as a configuration 
management problem. You will also probably have some idea what I'm going to  
recommend in order to solve it, but first let's look at a few of the available options.
Using CloudFormation
CloudFormation is a template language specific to Amazon Web Services (AWS). It 
describes AWS resources in a declarative way, rather like Puppet resources. You upload your 
CloudFormation template to the AWS portal (or API), apply it, and AWS will create all the 
resources specified. The following example shows a snippet of CloudFormation code:
 "Resources" : {
    "EC2Instance" : {
      "Type" : "AWS::EC2::Instance",
      "Properties" : {
        "InstanceType" : { "Ref" : "InstanceType" },
        "SecurityGroups" : [ { "Ref" : "InstanceSecurityGroup" } ],
        "KeyName" : { "Ref" : "KeyName" },
        "ImageId" : { "Fn::FindInMap" : [ "AWSRegionArch2AMI", { "Ref" 
: "AWS::Region" },
                          { "Fn::FindInMap" : [ 

Chapter 11
[ 191 ]
"AWSInstanceType2Arch", { "Ref" : "InstanceType" }, "Arch" ] } ] }
      }
    },
Frankly, it's not much fun to program in. While it may technically be infrastructure as code, 
it's pretty basic. Nonetheless, it still represents an advance on manually setting up AWS 
infrastructure with a web browser.
Using Terraform
Terraform is a rather more sophisticated tool for provisioning cloud resources. It allows you 
to describe your resources in a declarative way, like CloudFormation, but at a slightly higher 
level of abstraction, which is not AWS-specific. The following example shows what Terraform 
code looks like:
resource "aws_instance" "web" {
  ami           = "${data.aws_ami.ubuntu.id}"
  instance_type = "t2.micro"
  tags {
    Name = "HelloWorld"
  }
}
Terraform is a promising technology, but it's fair to say it's at an early stage of development.
Using Puppet
Stand-alone tools for managing cloud infrastructure are fine, but if we're doing everything 
else with Puppet, it seems a shame to introduce a whole new tool just for that. So, could we 
use Puppet to manage the cloud resources instead?
Fortunately, Puppet provides an excellent Forge module (puppetlabs/aws) which does 
exactly this. In the remaining part of this chapter, we'll work through some examples of how 
to use puppetlabs/aws to manage AWS cloud resources.
Setting up an Amazon AWS account
If you already have an AWS account, skip to the next section. Otherwise, you can follow 
these instructions to set up a new account and get the credentials you need to start building 
an infrastructure with Puppet.

Orchestrating cloud resources
[ 192 ]
Creating an AWS account
Follow these steps to create a new AWS account:
1.	 Browse to the following URL:
https://aws.amazon.com/
2.	 Click Sign In to the Console.
3.	 Follow the instructions to create and verify your account.
To manage AWS resources using Puppet, we will create an additional AWS user account 
specifically for Puppet, using Amazon's IAM (short for Identity and Access Management) 
framework. We'll see how to do this in the following sections.
Creating an IAM policy
Before we create the user account for Puppet, we need to grant specific permissions for the 
things it needs to do, such as read and create EC2 instances. This involves creating an IAM 
policy, which is a set of named permissions you can associate with a user account.
IAM policies are expressed as a JSON-format document. There is a policy JSON file in the 
example repo, named /vagrant/examples/iam_policy.json. Open this file, copy its 
contents, and keep it ready to paste into your web browser.
Follow these steps to create the policy and associate it with the Puppet user:
1.	 In the AWS console, select Services | IAM.
2.	 Select Policies.
3.	 Click on Create Policy.
4.	 On the Create Policy screen, select Create Your Own Policy.
5.	 Enter Policy Name (for example, puppet).
6.	 In the Policy Document text box, paste the text you copied from the iam_policy.
json file.

Chapter 11
[ 193 ]
7.	 Click on Create Policy at the bottom of the page to save this:
Creating an IAM user
To create the Puppet IAM user and associate it with the policy, follow these steps:
1.	 Sign in to the AWS console.
2.	 Select Services | IAM | Users.
3.	 Click Add user.
4.	 Enter the username you want to use for this account (for example, puppet).

Orchestrating cloud resources
[ 194 ]
5.	 In the Access type section, select Programmatic access:
6.	 Click Next: Permissions.
7.	 Click Attach existing policies directly.

Chapter 11
[ 195 ]
8.	 Type puppet in the Policy Type search box and press Enter:
9.	 You should see the policy we created in the previous section, so check the box next 
to it and click Next: Review.
10.	 Check that the settings are correct and click Create user.
When you finish creating the IAM user and policy, you should see the Success screen,  
which lists your access credentials. Copy the access key ID and the secret access key  
(click Show to see the secret access key). You will need these credentials for the  
next steps (but keep them safe).

Orchestrating cloud resources
[ 196 ]
Storing your AWS credentials
Follow these steps to configure your VM for access to AWS with your newly-generated 
credentials:
1.	 On your Vagrant VM, run the following command to create the directory to hold 
your credentials file:
mkdir /home/vagrant/.aws
2.	 Create a file named /home/vagrant/.aws/credentials with the following 
contents (substitute your access key ID and secret access key values from the AWS 
console screen):
[default]
aws_access_key_id = AKIAINSZUVFYMBFDJCEQ
aws_secret_access_key = pghia0r5/GjU7WEQj2Hr7Yr+MFkf+mqQdsBk0BQr
Creating the file manually is fine for this example, but for production 
use, you should manage the credentials file with Puppet using 
encrypted Hiera data, as shown in the Managing secret data section of 
Chapter 6, Managing data with Hiera.
Getting ready to use puppetlabs/aws
In the following sections, we'll see how to generate an SSH key pair to connect to your EC2 
instances, and also install the puppetlabs/aws module with its dependencies.
Creating a key pair
You'll need an SSH key pair in order to connect to any EC2 instances you create. We will 
generate and download your key pair in this section:
1.	 In the AWS console, go to the EC2 section and select Key pairs in Network & 
Security in the left pane.

Chapter 11
[ 197 ]
2.	 Click the Create Key Pair button:
3.	 You will be prompted for the name of your key pair. Enter pbg for this example.
4.	 A file named pbg.pem will be automatically downloaded by your browser. Move 
this file to your ~/.ssh directory on your own computer (or copy it to the vagrant 
user's ~/.ssh directory on the Vagrant VM if you'd rather access your AWS 
instances from there).
5.	 Set the correct permissions on the key file with the following command:
chmod 600 ~/.ssh/pbg.pem

Orchestrating cloud resources
[ 198 ]
Installing the puppetlabs/aws module
Follow these steps to install the puppetlabs/aws module:
1.	 If you've already set up the r10k module management tool, as shown in Chapter 7, 
Mastering modules, you're all set. Otherwise, run the following command to install 
r10k:
sudo gem install r10k
2.	 Run the following commands to install the puppetlabs/aws module:
cd /etc/puppetlabs/code/environments/production
sudo cp /vagrant/examples/Puppetfile.aws Puppetfile
sudo r10k puppetfile install --verbose
INFO     -> Updating module /etc/puppetlabs/code/environments/
production/modules/aws
INFO     -> Updating module /etc/puppetlabs/code/environments/
production/modules/stdlib
Installing the AWS SDK gem
The puppetlabs/aws module requires a couple of gems, which we can install easily using 
Puppet with the following manifest (aws_sdk.pp):
ensure_packages([
  'aws-sdk-core',
  'retries'
],
  { provider => puppet_gem })
Notice the provider => puppet_gem in this example? You might 
remember from Chapter 4, Understanding Puppet resources, that puppet_gem 
installs a Ruby gem in Puppet's context (as opposed to the system Ruby context, 
which is completely separate). Gems which are required by Puppet modules 
need to be installed in this way, or Puppet won't be able to load them.
1.	 Apply the manifest with the following command:
sudo puppet apply /vagrant/examples/aws_sdk.pp
2.	 Create the /home/vagrant/.aws/config file with the following contents:
[default]
region=us-east-1

Chapter 11
[ 199 ]
Creating EC2 instances with Puppet
Although you can manage many different types of AWS resources with Puppet, the most 
important is the EC2 instance, or virtual server. In this section, we'll see how to create your 
first EC2 instance.
Choosing an Amazon Machine Image (AMI)
In order to run an EC2 instance, which is to say an AWS virtual machine, you need to choose 
which virtual machine to run out of the many thousands available. Each virtual machine 
snapshot is called an Amazon Machine Image (AMI) and has a unique ID. It's this ID which 
you will add to your Puppet manifest to tell it what kind of instance to start.
It doesn't matter much for the purposes of this example which AMI you choose, but we'll be 
using an official Ubuntu image. To find one, follow these steps:
1.	 Browse to the following URL:
https://cloud-images.ubuntu.com/locator/ec2/
2.	 In the Search: box, enter the following:
       us-east-1 xenial
3.	 You should see a list of Ubuntu Xenial AMIs in the us-east-1 region of various 
instance types, looking something like the following screenshot:
4.	 Find an AMI in the list whose Instance Type is ebs-ssd. In the preceding 
screenshot, the third AMI in the list (ami-26d6d131) is suitable.
5.	 The hexadecimal code in the AMI-ID column starting ami- is the AMI ID; make a 
note of this for later. Click the link to see the AWS instance type selection page, and 
check whether the AMI you've selected has a label saying Free tier eligible; these 
AMIs do not incur charges. If you start an instance of a non-free tier AMI, you will be 
charged for it.

Orchestrating cloud resources
[ 200 ]
Creating the EC2 instance
Now we have chosen a suitable AMI, we're ready to create an EC2 instance with Puppet.
Before we can do that, however, we need to make a couple of changes to the AWS settings, 
so follow these steps:
1.	 In the AWS console, select VPC from the Services menu.
2.	 Select Your VPCs in the left pane.
3.	 There will be only one VPC listed. Click the Name field and set its name to 
default-vpc.
4.	 Select Subnets in the left pane.
5.	 There will be several subnets listed, one for each availability zone. Find the one 
associated with the us-east-1a availability zone.
6.	 Click in the subnet's Name field and set the name to default-subnet.
Why do we have to set names for the VPC and subnet before running the 
example? The puppetlabs/aws module refers to resources by their 
name, which is an arbitrary string, rather than their ID, which is a long 
hexadecimal code like the AMI ID. Although AWS creates a default VPC and 
subnet for you automatically, it doesn't assign them a name, which means 
we can't refer to them in Puppet code until we've set names for them. It 
doesn't matter what the names actually are, so long as the name in your 
Puppet code is the same as the name assigned in the AWS control panel. 
We'll find out more about what VPCs and subnets do, and how to use 
them, later in the chapter.
7.	 On your Vagrant VM, run the following commands:
cd /etc/puppetlabs/code/environments/production
sudo cp /vagrant/examples/aws_instance.pp aws_instance.pp
8.	 Edit the file, and change YOUR_AMI_ID to the AMI ID you picked earlier (in our 
example, ami-26d6d131):
sudo vi aws_instance.pp
$ami = 'ami-26d6d131'
9.	 Save the file and run the following command:
sudo puppet apply aws_instance.pp

Chapter 11
[ 201 ]
10.	 You should see some output from Puppet like the following:
Notice: /Stage[main]/Main/Ec2_securitygroup[pbg-sg]/ensure: 
created
Notice: /Stage[main]/Main/Ec2_instance[pbg-demo]/ensure: changed 
absent to running
11.	 If you check the EC2 section of the AWS console, you should see that your new 
instance is Initializing and will soon be ready to use.
Accessing your EC2 instance
Once the status of the newly-launched instance has changed from Initializing to Running 
(you may need to click the refresh button on the AWS console), you can connect to it using 
SSH and the key file you downloaded earlier:
1.	 In the AWS console, look for the Public IP address of the instance and copy it.
2.	 From your own machine (or from the Vagrant VM, if you copied the pbg.pem file to 
it), run the following command (replacing YOUR_INSTANCE_IP with the public IP of 
the instance):
ssh -i ~/.ssh/pbg.pem -l ubuntu YOUR_INSTANCE_IP
The authenticity of host 'YOUR_INSTANCE_IP (YOUR_INSTANCE_IP)' 
can't be established.
ECDSA key fingerprint is SHA256:T/
pyWVJYWys2nyASJVHmDqOkQf8PbRGru3vwwKH71sk.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'YOUR_INSTANCE_IP' (ECDSA) to the list 
of known hosts.
Welcome to Ubuntu 16.04.2 LTS (GNU/Linux 4.4.0-1016-aws x86_64)
Now that you have SSH access to the instance, you can bootstrap it with 
Puppet in the same way as for physical nodes, or just install Puppet and 
Git manually and check out the manifest repo. (There's more about this 
process in Chapter 3, Managing your Puppet code with Git.)
Congratulations! You've just created your first EC2 instance with Puppet. In the next section, 
we'll look at the code and examine the resources in detail.

Orchestrating cloud resources
[ 202 ]
VPCs, subnets, and security groups
Let's go through the example manifest and see how it works. But first, we need to know 
something about AWS resources.
An EC2 instance lives inside a subnet, which is a self-contained virtual network. All instances 
within the subnet can communicate with each other. Subnets are partitions of a VPC (short 
for Virtual Private Cloud), which is a private internal network specific to your AWS account.
An instance also has a security group, which is a set of firewall rules governing network 
access to the instance.
When you create an AWS account, you get a default VPC, divided into subnets for each  
AWS availability zone (AZ). We are using the default VPC and one of the default subnets  
for the example instance, but since we also need a security group, we create that first in 
Puppet code.
The ec2_securitygroup resource
The first part of the example manifest creates the required ec2_securitygroup resource 
(aws_instance.pp):
ec2_securitygroup { 'pbg-sg':
  ensure      =>  present,
  description => 'PBG security group',
  region      => $region,
  vpc         => 'default-vpc',
  ingress     => [
    {
      description => 'SSH access from world',
      protocol    => 'tcp',
      port        => 22,
      cidr        => '0.0.0.0/0',
    },
    {
      description => 'Ping access from world',
      protocol    => 'icmp',
      cidr        => '0.0.0.0/0',
    },
  ],
}
First of all, an ec2_securitygroup has a title (pbg-sg), which we will use to refer to it 
from other resources (such as the ec2_instance resource). It also has a description, 
which is just to remind us what it's for.

Chapter 11
[ 203 ]
It is part of a region and a vpc, and has an array of ingress rules. These are your firewall 
rules. Each firewall port or protocol you want to allow needs a separate ingress rule.
Each ingress rule is a hash like the following:
{
  description => 'SSH access from world',
  protocol    => 'tcp',
  port        => 22,
  cidr        => '0.0.0.0/0',
}
The protocol attribute specifies the type of traffic (tcp, udp, and so on).
The port attribute is the port number to open (22 is the SSH port, which we'll need in order 
to log in to the instance).
Finally, the cidr key specifies the range of network addresses to allow access to 
(0.0.0.0/0 means all addresses).
The ec2_instance resource
The ec2_instance resource, as you'd expect, manages an individual EC2 instance. Here's 
the relevant section of the example manifest (aws_instance.pp):
ec2_instance { 'pbg-demo':
  ensure                      => present,
  region                      => $region,
  subnet                      => 'default-subnet',
  security_groups             => 'pbg-sg',
  image_id                    => $ami,
  instance_type               => 't1.micro',
  associate_public_ip_address => true,
  key_name                    => 'pbg',
}
First, ensure => present tells AWS that the instance should be running. (You can also 
use running as a synonym for present.) Setting ensure => absent will terminate and 
delete the instance (and any ephemeral storage attached to it).
EC2 instances can also be in a third state: stopped. Stopped instances preserve their storage 
and can be restarted. Because AWS bills by the instance-hour, you don't pay for instances 
that are stopped, so it's a good idea to stop any instances which don't need to be running 
right now.

Orchestrating cloud resources
[ 204 ]
The instance is part of a region and a subnet, and has one or more security_groups.
The image_id attribute tells AWS which AMI ID to use for the instance.
The instance_type attribute selects from AWS's large range of types, which more or less 
correspond to the computing power of the instance (different types vary in memory size and 
the number of virtual CPUs, and a few other factors).
As we're inside a private network, instances will not be reachable from the Internet unless 
we assign them a public IP address. Setting associate_public_ip_address to true 
enables this feature (you should set this to false unless the instance actually needs to 
expose a port to the Internet).
Finally, the instance has a key_name attribute, which tells AWS which SSH key we are  
going to use to access it. In this case, we're using the key we created earlier in the chapter, 
named pbg.
Before going on to the next example, terminate your instance to avoid using up 
your free hours. You can do this by selecting the instance in the AWS control 
panel and clicking Actions | Instance State | Terminate, or reapplying your 
Puppet manifest with the instance's ensure attribute set to absent.
Managing custom VPCs and subnets
In the previous example, we used the pre-existing default VPC and subnet to create our 
instance. That's fine for demonstration purposes, but in production, you'll want to use 
a dedicated VPC for your Puppet-managed resources to keep it separate from any other 
resources in your AWS account and from other Puppet-managed VPCs. You could, for 
example, have a staging VPC and a production VPC.
By default, a new VPC has no access to the Internet; we'll also need an Internet gateway 
(which routes Internet traffic to and from the VPC) and a route table (which tells a given 
subnet to send non-local traffic to the gateway). The puppetlabs/aws module provides 
Puppet resources to create and manage each of these entities.
Creating an instance in a custom VPC
In this section, we'll use a more sophisticated example manifest to create a new VPC and 
subnet, with an associated Internet gateway and route table, then add a security group and 
EC2 instance.

Chapter 11
[ 205 ]
Follow these steps to apply the manifest:
1.	 Run the following command:
cd /etc/puppetlabs/code/environments/production
sudo cp /vagrant/examples/aws_vpc.pp aws_vpc.pp
2.	 Edit the file and change YOUR_AMI_ID to the AMI ID you picked earlier (in our 
example, ami-26d6d131):
sudo vi aws_vpc.pp
$ami = 'ami-26d6d131'
3.	 Save the file and run the following command:
sudo puppet apply aws_vpc.pp
4.	 You should see some output from Puppet like the following:
Notice: /Stage[main]/Main/Ec2_vpc[pbg-vpc]/ensure: created
Notice: /Stage[main]/Main/Ec2_vpc_internet_gateway[pbg-igw]/
ensure: created
Notice: /Stage[main]/Main/Ec2_vpc_routetable[pbg-rt]/ensure: 
created
Notice: /Stage[main]/Main/Ec2_vpc_subnet[pbg-vpc-subnet]/ensure: 
created
Notice: /Stage[main]/Main/Ec2_securitygroup[pbg-vpc-sg]/ensure: 
created
Notice: /Stage[main]/Main/Ec2_instance[pbg-vpc-demo]/ensure: 
changed absent to running
5.	 If you check the EC2 section of the AWS console, you should see that your new 
instance is Initializing and will soon be ready to use.
The ec2_vpc resource
Let's look at the example manifest in detail. Here's the ec2_vpc resource (aws_vpc.pp):
ec2_vpc { 'pbg-vpc':
  ensure     => present,
  region     => $region,
  cidr_block => '10.99.0.0/16',
}
The VPC requires a region attribute and a cidr_block, which is the range of network 
addresses that the VPC will use. (Actually, this isn't required, as AWS will allocate you one at 
random if you don't specify it. We specify one here just for the demonstration.)

Orchestrating cloud resources
[ 206 ]
It doesn't matter what your network range actually is, as it's entirely internal. 
However, it's good practice to use one of the address ranges officially 
assigned to private networks, such as 10.x.y.z. To make it less likely that 
your range will conflict with any other assigned in your organization, pick a 
random number for x (we used 99 in the example).
The ec2_vpc_internet_gateway resource
We saw earlier that a VPC, by default, is not connected to the Internet. There are various 
ways to get Internet traffic into the VPC, including VPNs and Amazon Elastic Load Balancers, 
but for this example we'll use an ec2_vpc_internet_gateway resource, which looks like 
the following:
ec2_vpc_internet_gateway { 'pbg-igw':
  ensure => present,
  region => $region,
  vpc    => 'pbg-vpc',
}
The gateway has a title (pbg-igw) and it is associated with a particular region and vpc.
The ec2_vpc_routetable resource
Having provisioned an ec2_vpc_internet_gateway, we now need to set up a route table 
to determine which traffic to send to it. Here is the ec2_vpc_routetable resource from 
the example:
ec2_vpc_routetable { 'pbg-rt':
  ensure => present,
  region => $region,
  vpc    => 'pbg-vpc',
  routes => [
    {
      destination_cidr_block => '10.99.0.0/16',
      gateway                => 'local'
    },
    {
      destination_cidr_block => '0.0.0.0/0',
      gateway                => 'pbg-igw'
    },
  ],
}

Chapter 11
[ 207 ]
As usual, a route table has a title, region, and vpc. It also has an array of one or more 
routes.
A route is like a road sign for network packets. It says: if you're heading for this destination, 
take this junction. Each route in the array is a hash containing a destination_cidr_block 
and gateway key.
The first route in our example is for local traffic (destined for the 10.99.0.0/16 network, 
which is the network we assigned to our VPC):
{
  destination_cidr_block => '10.99.0.0/16',
  gateway                => 'local'
}
This tells traffic for the 10.99.0.0/16 network that it is local; that is, there's no need to use 
a gateway, because it's already on the desired network.
The second route is for all other traffic:
{
  destination_cidr_block => '0.0.0.0/0',
  gateway                => 'pbg-igw'
}
The network address 0.0.0.0/0 matches all possible network addresses (traffic for 
10.99.0.0/16 will already have been filtered out by the previous route, so we are left with 
all other traffic, which must be for the Internet). The designated gateway is pbg-igw, which 
is the ec2_vpc_internet_gateway we created earlier.
So, this route table equates to the following instructions for routing traffic:


Traffic for 10.99.0.0/16, stay on this network


For all other traffic, proceed to the pbg-igw gateway
These routes will suffice for a single VPC; if you have a more complicated network setup in 
AWS, you will need a more complicated route table, but the principles will be the same.
The ec2_vpc_subnet resource
A subnet, as we've seen, is a subdivision of the VPC network, which enables you to logically 
partition your VPC for different groups of resources. For example, you might have one 
subnet which is accessible from the Internet for public-facing nodes, and another for internal 
resources such as database or log servers.

Orchestrating cloud resources
[ 208 ]
In the example, we just have one subnet:
ec2_vpc_subnet { 'pbg-vpc-subnet':
  ensure            => present,
  vpc               => 'pbg-vpc',
  region            => $region,
  cidr_block        => '10.99.0.0/24',
  availability_zone => "${region}a",
  route_table       => 'pbg-rt',
}
It has a title, vpc, and region. Because it is a subdivision of the VPC network, it also needs 
a cidr_block specifying exactly which part of the network address space it occupies. This 
must be a subdivision of the network address you assigned to the containing VPC, as indeed 
it is in this example.
A subnet exists within an AWS availability zone (equivalent to a datacenter). These are 
named after their region; for example, the us-east-1 region has availability zones us-
east-1a, us-east-1b, and so on. This allows you to provision redundant resources 
in different availability zones, so that if one should fail, the other can take over. For this 
example, however, we're using just one availability zone, us-east-1a, which we pass to the 
availability_zone attribute.
By default, resources in a subnet can only communicate within the subnet. To allow traffic 
in and out of the subnet, we need to associate it with a route_table. Using the pbg-rt 
route table we created earlier, we can send Internet traffic via the pbg-igw gateway, and 
that's it.
The ec2_securitygroup and ec2_instance resources are more or less the same as in 
our earlier example, except for using the new subnet.
Other AWS resource types
Puppet is not limited to managing EC2 instances; the puppetlabs/aws module also 
supports ELB load balancers, Cloudwatch alarms, auto scaling groups, Elastic IPs, DHCP, 
VPNs, IAM users and policies, RDS databases, S3 storage buckets, SQS queues, Route 53 
DNS management, and the EC2 Container Service (ECS). Due to constraints of space, time, 
and energy, I have not provided examples for all of these, but you can consult the module's 
admirably comprehensive documentation available at this URL:
https://forge.puppet.com/puppetlabs/aws

Chapter 11
[ 209 ]
Provisioning AWS resources from Hiera data
There's nothing wrong with managing AWS resources directly in code as we've done in the 
previous examples, but we can do just a little bit better.
In Chapter 6, Managing data with Hiera, we saw how to create Puppet resources directly 
from Hiera data. In that example (Building resources from Hiera hashes), we stored all the 
users for our infrastructure in a Hiera hash called users, and then used the each keyword 
to iterate over that hash, creating a user resource for each user. Here's the example code 
again (hiera_users2.pp):
lookup('users').each | String $username, Hash $attrs | {
  user { $username:
    * => $attrs,
  }
}
The magic * character (the attribute splat operator) tells Puppet to use the contents of the 
$attrs hash as the attributes of the resource.
The advantage of describing resources as Hiera data is that when we come to add a new  
user or change the details for an existing user, we don't need to touch the Puppet code  
at all. Everything is defined in Hiera.
Iterating over Hiera data to create resources
Alert readers may be wondering, Couldn't we do the same thing with all these AWS 
resources? Can we just define everything in a Hiera hash and have Puppet iterate over it to 
create the resources?
Indeed we can. The manifest to create all these resources is surprisingly concise  
(aws_hiera.pp):
$aws_resources = lookup('aws_resources', Hash, 'hash')
$aws_resources.each | String $r_type, Hash $resources | {
  $resources.each | String $r_title, Hash $attrs | {
    Resource[$r_type] { $r_title:
      * => $attrs,
    }
  }
}

Orchestrating cloud resources
[ 210 ]
To apply the manifest, follow these steps:
1.	 Run the following commands to copy the Hiera data into the place on your Vagrant 
box or node:
cd /etc/puppetlabs/code/environments/production
sudo cp /vagrant/examples/hiera_minimal.config.yaml /etc/
puppetlabs/puppet/hiera.yaml
sudo cp /vagrant/examples/hiera_aws.yaml data/common.yaml
2.	 Edit the file and change YOUR_AMI_ID to the AMI ID you picked earlier (in our 
example, ami-26d6d131):
sudo vi data/common.yaml
ami: 'ami-26d6d131'
3.	 Save the file and run the following command:
sudo puppet apply /vagrant/examples/aws_hiera.pp
If you've already run the previous example and the AWS resources are still present, you'll see 
no output from Puppet, because the resources are exactly the same. (Remember, if the state 
of the system is already the same as the desired state expressed in the manifest, Puppet will 
do nothing.)
If you want to prove to yourself that the example manifest really works, delete the resources 
using the AWS control panel (or use Puppet to delete them by changing present to absent 
in the Hiera data) and reapply the manifest.
If you compare the manifest to that from the Hiera users example, you can see that instead 
of a single loop, it consists of two nested loops. The outer loop iterates over the contents of 
the $aws_resources hash:
$aws_resources = lookup('aws_resources', Hash, 'hash')
$aws_resources.each | String $r_type, Hash $resources | {
  ...
  }
}
Each key of the $aws_resources hash is the name of a Puppet resource type. Here's the 
first one (from hiera_aws.yaml):
 'ec2_vpc':
      ...

Chapter 11
[ 211 ]
So the first time around this loop, the value of $r_type will be ec2_vpc, and the value of 
$resources will be this hash:
'pbg-vpc':
  ensure: present
  region: "%{lookup('region')}"
  cidr_block: '10.99.0.0/16'
Now we enter the inner loop, which creates all the resources of the $r_type type:
$resources.each | String $r_title, Hash $attrs | {
  Resource[$r_type] { $r_title:
    * => $attrs,
  }
}
As it happens, there is only one ec2_vpc resource, so the first time around the inner loop 
the value of $r_title will be pbg-vpc, and the value of $attrs will be this hash:
ensure: present
region: "%{lookup('region')}"
cidr_block: '10.99.0.0/16'
So Puppet will create this resource:
ec2_vpc { 'pbg-vpc':
  ensure     => present,
  region     => 'us-east-1',
  cidr_block => '10.99.0.0/16',
}
This is identical to the ec2_vpc resource in the previous example, and as we go around the 
outer loop, we will create the other resources in the same way.
What's Resource[$r_type]? This is a bit of Puppet wizardry. The problem is that we need 
to declare a Puppet resource whose type we don't know yet; it will be supplied by the $r_
type variable. You might at first try using a syntax like the following:
$r_type = 'ec2_vpc'
$r_type { 'pbg-vpc':
  ...
}
Unfortunately, Puppet doesn't allow this syntax, but there is a way to get around the 
problem. The Resource abstract data type matches any resource type (you can read more 
about Puppet data types in Chapter 8, Classes, roles, and profiles).

Orchestrating cloud resources
[ 212 ]
We can make Resource more specific by including the actual resource type in square 
brackets: Resource['ec2_vpc']. This is valid syntax for declaring a resource.
So this is how we declare a resource whose type comes from a variable:
$r_type = 'ec2_vpc'
Resource[$r_type] { 'pbg-vpc':
  ...
}
Now that your AWS resources are described by Hiera data, it should be much easier to 
maintain and extend them as you use Puppet in production.
Cleaning up unused resources
To close down your EC2 instance, and thus avoid using up your free hours or being billed for 
the instance, edit your Hiera data to set ensure: absent on the ec2_instance resource:
'ec2_instance':
      'pbg-vpc-demo':
        ensure: absent
        region: "%{lookup('region')}"
        subnet: 'pbg-vpc-subnet'
        security_groups: 'pbg-vpc-sg'
        image_id: "%{lookup('ami')}"
        instance_type: 't1.micro'
        associate_public_ip_address: true
        key_name: 'pbg'
When you reapply the manifest, Puppet will stop the instance. You can leave the other 
resources in place, as they don't incur charges.
Summary
In this chapter, we introduced the basic idea of cloud computing, and looked at some options 
for managing cloud resources, including CloudFormation and Terraform, before meeting the 
puppetlabs/aws module.
We worked through the process of creating an AWS account, setting up an IAM user and 
policy, generating credentials and SSH keys, installing the AWS SDK gem, and choosing a 
suitable AMI.
Using Puppet, we created an EC2 instance and security group, and saw how to connect to 
the running instance with SSH. Furthermore, we created a whole VPC from scratch, complete 
with subnets, Internet gateway, route table, security group, and EC2 instance.

Chapter 11
[ 213 ]
Lastly, we saw how to build all these cloud resources directly from Hiera data, which is the 
most flexible and powerful way to describe Puppet resources.
In the next and final chapter we'll draw together ideas and techniques from all the previous 
chapters in this book, to create a complete, working example Puppet infrastructure, which 
you can use as a basis for your own.


[ 215 ]
12
Putting it all together
Manhood is patience. Mastery is nine times patience.
—Ursula K. Le Guin, 'A Wizard of Earthsea'
In this chapter, we will apply ideas from all the previous chapters to see what a complete, 
working Puppet infrastructure looks like, using a demonstration repo that illustrates all the 
principles explained in this book. You can use it as the basis of your own Puppet code base, 
adapting and expanding it as needed.

Putting it all together
[ 216 ]
Getting the demo repo
The demo repo is available on GitHub, and you can clone it, in the same way as for the 
example repo for this book, by running this command:
git clone https://github.com/bitfield/control-repo
The demo repo contains everything you'll need to manage nodes with Puppet:


User accounts and SSH keys


SSH and sudoers config


Time zone and NTP settings


Hiera data


Automatic Puppet update and apply scripts


Bootstrap script for new nodes
A Vagrantfile is also included, so you can try out the repo on a Vagrant VM.
Copying the repo
If you are going to use the demo repo as a basis for your own Puppet repo, you need to make 
a copy of it so that you can edit and maintain it yourself.
You can do this in two ways. One is to fork the repo to your own GitHub account. To do this, 
log in to GitHub and browse to the demo repo URL:
https://github.com/bitfield/control-repo.git
Look for the Fork button at the top right-hand side of the page and click it. This will create a 
new repo under your account, which contains all the code and history from the demo repo.
Alternatively, you can follow these steps:
1.	 Create a new repo in your GitHub account (name it puppet, or control-repo, or 
whatever you prefer).
2.	 Make a note of the repo URL.
3.	 Clone the demo repo to your personal machine:
git clone https://github.com/bitfield/control-repo
cd control-repo
4.	 Rename the original repository remote (so you can get updates in the future):
git remote rename origin upstream

Chapter 12
[ 217 ]
5.	 Add your new repo as the origin remote (using the URL for your repo that you 
noted earlier):
git remote add origin YOUR_GIT_URL
6.	 Push to the new remote:
git push origin production
Your repo now contains a complete copy of the demo repo, which you can edit and 
customize as you like.
As the original repo is updated in the future, you will be able to pull these changes into your 
own version. To get changes from upstream, run the following commands:
git fetch upstream
git rebase upstream/production
Understanding the demo repo
Now it's time to see how all the ideas from the previous chapters fit together. It should be 
helpful for you to see how a complete Puppet infrastructure works, and you can also use this 
repo as a basis for your own projects. We'll see how you can do that later in the chapter, but 
first, a word or two about the overall structure of the repo.
The control repo
A control repo is a Puppet code base that contains no modules, or only site-specific modules, 
and it's a good way to organize your Puppet code base.
In Chapter 7, Mastering modules, we learned about using the r10k tool to manage  
modules with a Puppetfile. The Puppetfile specifies the modules we use, with their exact 
versions and their sources (usually Puppet Forge, but they can also come from remote  
Git repos).
Therefore, our Puppet repo needs to contain only a Puppetfile, along with our node 
definitions, resource defaults, Hiera data, and the role and profile modules.
Module management
Because r10k expects to manage everything in the modules/ directory using the 
Puppetfile, our site-specific modules are kept in a separate directory, in the control  
repo named site-modules/.

Putting it all together
[ 218 ]
To enable this, we need to add the following setting to the environment.conf file:
modulepath = "modules:site-modules:$basemodulepath"
This adds site-modules/ to the list of places Puppet will look for modules.
As detailed in Chapter 7, Mastering modules, we will be using r10k and a Puppetfile to 
manage all third-party modules. Accordingly, there is no modules/ directory in the demo 
repo—r10k will create this when it installs the required modules.
Here's the Puppetfile with the list of modules we need for the initial repo. Of course, 
as you adapt the repo for your own needs, you'll be adding more modules to this list 
(Puppetfile):
forge "http://forge.puppetlabs.com"
# Modules from the Puppet Forge
mod 'puppetlabs/accounts', '1.1.0'
mod 'puppetlabs/ntp', '6.2.0'
mod 'puppetlabs/stdlib', '4.17.0'
mod 'saz/sudo', '4.1.0'
mod 'saz/timezone', '3.4.0'
mod 'stm/debconf', '2.0.0'
We'll see how these modules are used in the following sections.
Every so often, use the generate-puppetfile tool to automatically update your module 
versions and dependencies (refer to Chapter 7, Mastering modules for more about this). Run 
the following command in the repo directory:
generate-puppetfile -p Puppetfile
Copy and paste the output back into your Puppetfile, replacing the existing mod statements.
Nodes
As you know, Puppet looks for a node definition somewhere in the manifest to determine 
which classes and resources should be applied to the node. The node definitions for the 
demo repo are in the manifests/site.pp file.
The demo repo comes with a node definition for the demo node; you can change this to 
match the hostname of the node you want to manage. Here's the demo node definition 
(manifests/site.pp):
node 'demo' {
  include role::demo
}

Chapter 12
[ 219 ]
Roles
Role classes identify by name what the function of the node is, and define what profile classes 
should be included (refer to Chapter 8, Classes, roles, and profiles, for more about this).
Keeping your role classes in a role module is a common practice, and as this is a site-specific 
module, it's filed under site-modules/.
Here's the role::demo role manifest (site-modules/role/manifests/demo.pp):
# Be the demo node
class role::demo {
  include profile::common
}
Profiles
A profile class identifies by name some specific piece of software or functionality required for 
a role and declares the necessary resources to manage it (refer to Chapter 8, Classes, roles, 
and profiles, for a more detailed explanation of profiles).
However, there are some resources that are common to all nodes: our user accounts, for 
example, and a few others. It's logical to keep these in a profile called profile::common, 
which is included by all roles.
As you can see from the preceding figure, the role::demo class includes only 
profile::common. When you create your own roles, copy role::demo to use as a 
starting point, and include new profile classes to manage the things you need alongside 
profile::common.

Putting it all together
[ 220 ]
Here's our profile::common class (site-modules/profile/manifests/common.pp):
# Common profile for all nodes
class profile::common {
  include profile::ntp
  include profile::puppet
  include profile::ssh
  include profile::sudoers
  include profile::timezone
  include profile::users
}
We'll see what each of these profiles do in the following sections.
In the role::demo class, profiles are included in alphabetical order—
this can be helpful when you have many classes included, and can make 
it easier to see whether or not a given class is already in the list. Keep 
your included classes in alphabetical order.
Users and access control
The puppetlabs/accounts module provides a standard way to handle user accounts 
with the accounts::user class. Accordingly, we will use this to manage our users in the 
profile::users class.
If you prefer to manage user accounts directly in Puppet using the 
user and ssh_authorized_key resources, refer to Chapter 4, 
Understanding Puppet resources, for more information.
You could just list the required users as literal resources in your Puppet manifest, of course. 
But instead, let's take the data-driven approach described in Chapter 6, Managing data with 
Hiera, and define our users with Hiera data.
This is what the data structure looks like (data/common.yaml):
  users:
    'john':
      comment: 'John Arundel'
      uid: '1010'
      sshkeys:
        - 'ssh-rsa AAAA ...'
    'bridget':

Chapter 12
[ 221 ]
      comment: 'Bridget X. Zample'
      uid: '1011'
      sshkeys:
        - 'ssh-rsa AAAA ...'
Here's the code in the users profile to read the data and create the corresponding 
accounts::user resources (site-modules/profile/manifests/users.pp):
# Set up users
class profile::users {
  lookup('users', Hash, 'hash').each | String $username, Hash $attrs | 
{
    accounts::user { $username:
      * => $attrs,
    }
  }
}
As you can see, we fetch all the user data into a single $users hash with a call to lookup(). 
We then iterate over the hash, declaring an accounts::user resource for each user, whose 
attributes are loaded from the hash data.
Note that when using the accounts::user resource, the sshkeys attribute must contain 
an array of authorized SSH public keys for the user.
SSH configuration
Restricting SSH logins to a set of named users, using the AllowUsers directive in /etc/
ssh/sshd_config, is good security practice. We used a Puppet template to build this 
config file in Chapter 9, Managing files with templates. In that example, we got the list of 
allowed users from Hiera, and we will do the same here.
Here's the template for the sshd_config file (site-modules/profile/templates/
ssh/sshd_config.epp):
<%- | Array[String] $allow_users | -%>
# File is managed by Puppet
AcceptEnv LANG LC_*
ChallengeResponseAuthentication no
GSSAPIAuthentication no
PermitRootLogin no
PrintMotd no
Subsystem sftp internal-sftp

Putting it all together
[ 222 ]
AllowUsers <%= join($allow_users, ' ') %>
UseDNS no
UsePAM yes
X11Forwarding yes
We declare that the template takes an $allow_users parameter, which is an array of string 
values. Because the AllowUsers parameter in sshd_config expects a space-separated list 
of users, we call the join() function from the standard library to create this list from the 
Puppet array (refer to Chapter 7, Mastering modules, for more about this and other standard 
library functions).
Here's the relevant Hiera data (data/common.yaml):
  allow_users:
    - 'john'
    - 'bridget'
    - 'vagrant'
We could have just constructed the list from the $users hash, which contains 
all known users, but we don't necessarily want everyone on that list to be able to 
log in to every node. Conversely, we may need to allow logins for some accounts 
which are not managed by Puppet. An example is the vagrant account, which 
is required by Vagrant in order to manage the VM properly. If you're not using 
Vagrant boxes, you can remove the vagrant user from this list.
The code to read this Hiera data and populate the template is as follows (site-modules/
profile/manifests/ssh.pp):
# Manage sshd config
class profile::ssh {
  ensure_packages(['openssh-server'])
  file { '/etc/ssh/sshd_config':
    content => epp('profile/ssh/sshd_config.epp', {
      'allow_users' => lookup('allow_users', Array[String], 'unique'),
    }),
    notify  => Service['ssh'],
  }
  service { 'ssh':
    ensure => running,
    enable => true,
  }
}

Chapter 12
[ 223 ]
This is a package-file-service pattern, which you may remember from such chapters as 
Chapter 2, Creating your first manifests.
First, we install the openssh-server package (this is usually already installed, but it's good 
style to declare the package anyway, since we rely on it for what follows).
Next, we manage the /etc/ssh/sshd_config file with a template, which we populate 
using Hiera data from a call to lookup('allow_users'). This file notifies the ssh service 
whenever it changes.
Finally, we declare the ssh service and specify that it should be running and enabled at  
boot time.
Sudoers configuration
The sudo command is the standard Unix mechanism for controlling user privileges. It's 
usually used to allow normal users to run commands with the privileges of the root user.
Using sudo is preferable to allowing people to log in and run a shell 
as root, and sudo also audits and records which user ran which 
commands. You can also specify very fine-grained permissions, such as 
allowing a user to run only a certain command as root, but not others.
The most popular Forge module for managing the sudo permissions is saz/sudo, and 
that's what we'll use here. Here's the Hiera data listing the users with sudo access (data/
common.yaml):
  sudoers:
    - 'john'
    - 'bridget'
    - 'vagrant'
If you're not using Vagrant, you can remove the vagrant user from this list.
Here's the profile class that reads the data (site-modules/profile/manifests/
sudoers.pp):
# Manage user privileges
class profile::sudoers {
  sudo::conf { 'secure_path':
    content  => 'Defaults      secure_path="/usr/local/sbin:/usr/
local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/puppetlabs/puppet/bin"',
    priority => 0,
  }

Putting it all together
[ 224 ]
  $sudoers = lookup('sudoers', Array[String], 'unique', [])
  $sudoers.each | String $user | {
    sudo::conf { $user:
      content  => "${user} ALL=(ALL) NOPASSWD: ALL",
      priority => 10,
    }
  }
}
Adding the path to the Puppet binaries (/opt/puppetlabs/puppet/bin) to the  
secure_path variable of sudo is convenient for us, as you may recall we did in  
Chapter 1, Getting started with Puppet.
This allows us to run commands such as sudo puppet as a normal user. That's what this 
part of the manifest does:
  sudo::conf { 'secure_path':
    content  => 'Defaults      secure_path="/usr/local/sbin:/usr/
local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/puppetlabs/puppet/bin"',
    priority => 0,
  }
The sudo::conf resource, provided by the saz/sudo module, allows us to write arbitrary 
sudoers config as a string—in this case, setting the secure_path variable.
The remainder of the profile is concerned with configuring passwordless sudo privileges 
for each user named in the Hiera array sudoers. As usual, we get the array from Hiera and 
proceed to iterate over it with each, creating a sudo::conf resource for each named user.
Time zone and clock synchronization
There is a handy Forge module for managing server time zone: saz/timezone. Here's 
our timezone profile, which uses that module to set all nodes to UTC (site-modules/
profile/manifests/timezone.pp):
# Set the timezone for all nodes
class profile::timezone {
  class { 'timezone':
    timezone => 'Etc/UTC',
  }
}

Chapter 12
[ 225 ]
Similarly, we want to make sure that the clocks on all our nodes are synchronized, not 
only with each other but also with the global time standard as a whole. We will be using 
the puppetlabs/ntp module for this, and here is the relevant profile (site-modules/
profile/manifests/ntp.pp):
# Synchronize with NTP
class profile::ntp {
  include ::ntp
}
As it happens, there's no special configuration to do for NTP (though you could, if you 
wanted, specify a list of timeservers to contact, for example).
Puppet configuration
We need to configure a regular cron job that pulls any updates from the Git repo and runs 
Puppet to apply the updated manifest.
The profile::puppet class sets this up (site-modules/profile/manifests/
puppet.pp):
# Set up Puppet config and cron run
class profile::puppet {
  service { ['puppet', 'mcollective', 'pxp-agent']:
    ensure => stopped, # Puppet runs from cron
    enable => false,
  }
  cron { 'run-puppet':
    ensure  => present,
    command => '/usr/local/bin/run-puppet',
    minute  => '*/10',
    hour    => '*',
  }
  file { '/usr/local/bin/run-puppet':
    source => 'puppet:///modules/profile/puppet/run-puppet.sh',
    mode   => '0755',
  }
  file { '/usr/local/bin/papply':
    source => 'puppet:///modules/profile/puppet/papply.sh',
    mode   => '0755',
  }

Putting it all together
[ 226 ]
  file { '/tmp/puppet.lastrun':
    content => strftime('%F %T'),
    backup  => false,
  }
}
There are a fair number of resources in this profile, so let's look at each of them in turn.
First, we stop and disable some of the services started by the Puppet package, which we 
won't need:
  service { ['puppet', 'mcollective', 'pxp-agent']:
    ensure => stopped, # Puppet runs from cron
    enable => false,
  }
Next follows the cron job, which carries out the regular Git updates and Puppet runs. The 
run-puppet script looks like this (site-modules/profile/files/run-puppet.sh):
#!/bin/bash
cd /etc/puppetlabs/code/environments/production && git pull
/opt/puppetlabs/puppet/bin/r10k puppetfile install
/opt/puppetlabs/bin/puppet apply --environment production manifests/
Here's the cron resource that runs the script:
  cron { 'run-puppet':
    ensure  => present,
    command => '/usr/local/bin/run-puppet',
    minute  => '*/10',
    hour    => '*',
  }
The job is set to run every 10 minutes, but you can adjust this if you need to.
This looks very much like the run-puppet script you may recall from Chapter 3, Managing 
your Puppet code with Git. The only difference is the extra step to run r10k puppetfile 
install (in case you added any new external modules in the Puppetfile) and the 
addition of the --environment switch to puppet apply.

Chapter 12
[ 227 ]
The next resource in profile::puppet deploys a convenience script named papply, 
which saves you from having to type the whole puppet apply command manually (site-
modules/profile/files/papply.sh):
#!/bin/bash
environment=${PUPPET_ENV:-production}
/opt/puppetlabs/puppet/bin/r10k puppetfile install
/opt/puppetlabs/bin/puppet apply --environment ${environment} 
--strict=warning /etc/puppetlabs/code/environments/${environment}/
manifests/ $*
Just running papply from the command line will apply Puppet immediately, without pulling 
any Git changes.
If you want to test Puppet changes from a different environment (for example, if you have 
a staging branch checked out at /etc/puppetlabs/code/environments/staging), 
you can control this with the PUPPET_ENV variable, as follows:
PUPPET_ENV=staging papply
Note that papply passes its command-line arguments on to Puppet (with $*), so you can 
add any arguments supported by the puppet apply command:
papply --noop --show_diff
We also supply the --strict=warning flag to the puppet apply 
command, which will cause Puppet to alert you if any potentially problematic 
code is encountered (such as referencing a variable that has not yet been 
defined). If you want Puppet to be really strict, set --strict=error instead, 
which will prevent the manifest being applied until all such problems are fixed.
The final resource in the puppet profile creates a file named /tmp/puppet.lastrun:
  file { '/tmp/puppet.lastrun':
    content => strftime('%F %T'),
    backup  => false,
  }
Every time Puppet runs, this file will be updated with the current date and time (using the 
stdlib function strftime).
You can use this, for example, to check manually when the last Puppet run was, or 
automatically, using a monitoring check to make sure Puppet is up to date on all nodes.

Putting it all together
[ 228 ]
The bootstrap process
In order to prepare a new node for Puppet management using the demo repo, we need to 
do a number of things:


Install Puppet


Clone the Git repo


Run Puppet for the first time
In Chapter 3, Managing your Puppet code with Git, we performed these steps manually, but 
the demo repo automates this process (usually known as bootstrap). Here is the bootstrap 
script (scripts/bootstrap.sh):
#!/bin/bash
PUPPET_REPO=$1
HOSTNAME=$2
BRANCH=$3
if [ "$#" -ne 3 ]; then
  echo "Usage: $0 PUPPET_REPO HOSTNAME BRANCH"
  exit 1
fi
hostname ${HOSTNAME}
echo ${HOSTNAME} >/etc/hostname
source /etc/lsb-release
apt-key adv --fetch-keys http://apt.puppetlabs.com/DEB-GPG-KEY-puppet
wget http://apt.puppetlabs.com/puppetlabs-release-pc1-${DISTRIB_
CODENAME}.deb
dpkg -i puppetlabs-release-pc1-${DISTRIB_CODENAME}.deb
apt-get update
apt-get -y install git puppet-agent
cd /etc/puppetlabs/code/environments
mv production production.orig
git clone ${PUPPET_REPO} production
cd production
git checkout ${BRANCH}
/opt/puppetlabs/puppet/bin/gem install r10k --no-rdoc --no-ri
/opt/puppetlabs/puppet/bin/r10k puppetfile install --verbose
/opt/puppetlabs/bin/puppet apply --environment=production /etc/
puppetlabs/code/environments/production/manifests/
The bootstrap script expects to be run with three arguments (we'll see how this is done in a 
moment): PUPPET_REPO, the Git URL for the Puppet repo to clone; HOSTNAME, the desired 
hostname for the node; and BRANCH, the branch of the Puppet repo to use.

Chapter 12
[ 229 ]
First, the script sets the specified hostname:
hostname ${HOSTNAME}
echo ${HOSTNAME} >/etc/hostname
Next, it looks at the /etc/lsb-release file to find out the version of Ubuntu installed 
(this script is Ubuntu-specific, but you can easily modify it to work with a different Linux 
distribution if you need to). The appropriate Puppet Labs APT repository package is 
downloaded with wget and installed. Then the puppet-agent package is installed, along 
with git:
source /etc/lsb-release
apt-key adv --fetch-keys http://apt.puppetlabs.com/DEB-GPG-KEY-puppet
wget http://apt.puppetlabs.com/puppetlabs-release-pc1-${DISTRIB_
CODENAME}.deb
dpkg -i puppetlabs-release-pc1-${DISTRIB_CODENAME}.deb
apt-get update && apt-get -y install git puppet-agent
The next step in the bootstrap process is to clone the Git repo into the place where Puppet 
expects to find its manifests:
cd /etc/puppetlabs/code/environments
mv production production.orig
git clone ${PUPPET_REPO} production
cd production
git checkout ${BRANCH}
Next, we install r10k (in Puppet's gem context, using the Puppet-specific gem command) and 
run r10k puppetfile install to install all the required modules listed in the Puppetfile:
/opt/puppetlabs/puppet/bin/gem install r10k --no-rdoc --no-ri
/opt/puppetlabs/puppet/bin/r10k puppetfile install --verbose
Now we can run Puppet for the first time, which will configure everything else we need:
/opt/puppetlabs/bin/puppet apply --environment=production /etc/
puppetlabs/code/environments/production/manifests/
Of course, in order to run this script on the target node, we have to copy it there first. This 
step is performed by the puppify script (scripts/puppify):
#!/bin/bash
PUPPET_REPO=https://github.com/bitfield/control-repo.git
IDENTITY="-i /Users/john/.ssh/pbg.pem"
if [ "$#" -lt 2 ]; then
  cat <<USAGE
Usage: $0 TARGET HOSTNAME [BRANCH]

Putting it all together
[ 230 ]
Install Puppet on the node TARGET (IP address or DNS name) and run
the bootstrap process. Set the hostname to HOSTNAME, and optionally 
use
the control repo branch BRANCH.
USAGE
  exit 1
fi
TARGET=$1
HOSTNAME=${2}
BRANCH=${3:-production}
OPTIONS="-oStrictHostKeyChecking=no"
echo -n "Copying bootstrap script... "
scp ${IDENTITY} ${OPTIONS} $(dirname $0)/bootstrap.sh 
ubuntu@${TARGET}:/tmp
echo "done."
echo -n "Bootstrapping... "
ssh ${IDENTITY} ${OPTIONS} ubuntu@${TARGET} "sudo bash /tmp/bootstrap.
sh ${PUPPET_REPO} ${HOSTNAME} ${BRANCH}"
echo "done."
First, the script sets the URL of the Git repo to clone (you'll need to change this to your own 
URL when you adapt the demo repo for your own use):
PUPPET_REPO=https://github.com/bitfield/control-repo.git
Next, we specify the key file used to connect to the target node via SSH (again, modify this to 
use your own key):
IDENTITY="-i /Users/john/.ssh/pbg.pem"
After the usage message and the command-line arguments are processed, the script 
proceeds to copy the bootstrap.sh file to the target node:
scp ${IDENTITY} ${OPTIONS} $(dirname $0)/bootstrap.sh 
ubuntu@${TARGET}:/tmp
The final step is to run the bootstrap script on the node, passing it the required command-
line arguments:
ssh ${IDENTITY} ${OPTIONS} ubuntu@${TARGET} "sudo bash /tmp/bootstrap.
sh ${PUPPET_REPO} ${HOSTNAME} ${BRANCH}"

Chapter 12
[ 231 ]
Adapting the repo for your own use
You will need to change some of the data and settings in the demo repo to be able to 
use it yourself. To get you started, here is a table showing which files to change and what 
information you'll need to supply, with more detailed explanations in the following sections:
File
What to change
data/common.yaml
users: Users and SSH keys
allow_users: Users allowed to log in to nodes
sudoers: Users allowed to sudo
manifests/site.pp
Node definitions for the nodes you are managing
site-modules/role/manifests/
Role classes for your nodes (include 
profile::common in each one)
scripts/puppify
PUPPET_REPO: Git URL of your Puppet repo
IDENTITY: Path to the SSH key for initial bootstrap 
of nodes, if you need one
Configuring users
As we saw earlier in this chapter, the user accounts managed by Puppet are configured from 
Hiera data. Edit the data/common.yaml file, which looks like this:
  users:
    'john':
      comment: 'John Arundel'
      uid: '1010'
      sshkeys:
        - 'ssh-rsa AAAA... john@susie'
...
Replace the existing users with the user accounts you want to create on nodes (at first it 
may just be one account, for yourself). Add any SSH keys you want to use with them to the 
sshkeys array.
The list of allowed users on each node is controlled by the allow_users array. Replace the 
users listed there with your own users.
The list of users with sudo privileges is controlled by the sudoers array. Replace the users 
listed there with those of your own users who you want to have root privileges.

Putting it all together
[ 232 ]
Adding node definitions and role classes
The nodes recognized by Puppet are listed in the manifests/site.pp file. Edit this file and 
replace the demo node with the hostname or hostnames of the nodes you want to manage.
Include role classes suitable for these nodes (refer to Chapter 8, Classes, roles, and profiles 
for more information about this). To start with, you can just have your node include the 
role::demo class, which only includes the profile::common class. This will be enough 
to set your node up with your SSH account and key, and validate that the bootstrap process 
works properly. Later, you can start adding role classes to get actual work done.
Add your role classes to the site-modules/role/manifests/ directory, along the lines 
of role::demo. Don't forget to include the profile::common class in every role, as well 
as any special profiles you need to add for the role.
Modifying the bootstrap credentials
In the scripts/puppify file, edit the PUPPET_REPO setting to the URL of your own 
Git repo. If you need an SSH key to connect to the target node (for example, if you're 
using Amazon EC2, in which case you'll have a .pem file containing your key, which you 
downloaded from the AWS console), add its location to the IDENTITY variable.
Bootstrapping a new node
If you'd like to try out the demo repo on a Vagrant box, there is a suitable Vagrantfile 
included within the repo directory. (If you don't have Vagrant installed, follow the 
instructions in the Installing Virtualbox and Vagrant section of Chapter 1, Getting started 
with Puppet, first.)
Bootstrapping a Vagrant VM
Run the following commands in the repo directory to start your Vagrant VM:
vagrant plugin install vagrant-vbguest
vagrant up
Bootstrapping physical or cloud nodes
Alternatively, you can bootstrap a physical or cloud node using the repo. All you will need is 
the IP address or DNS name of the target node.

Chapter 12
[ 233 ]
Run the following command from the Puppet repo, replacing TARGET_SERVER with the 
address or name of the node, and HOSTNAME with the hostname that you want to set (for 
example, demo):
scripts/puppify TARGET_SERVER HOSTNAME
You will see some output related to copying the bootstrap script, installing the Puppet 
package, cloning the repo, installing the Forge modules, and running Puppet for the first 
time. Once this has completed, the node should be ready, and you can try logging in to it 
using your own SSH account.
Using other distributions and providers
The puppify and bootstrap scripts included with the demo repo will work for an  
Ubuntu node on Amazon EC2, but you can modify them to work with any Linux distribution 
or server provider.
For example, if you're using a Google Compute Engine instance, you can edit the puppify 
script to replace the ssh command with gcloud compute ssh. If you're using a Digital 
Ocean droplet, you can add your SSH key to the droplet when you provision it via the web 
interface, and you can modify the puppify script to log in as the root user instead of 
ubuntu.
If you're managing nodes on several different platforms, you may find it more convenient to 
use a customized puppify script for each one, naming them (for example) puppify_ec2, 
puppify_linode, and so on.
If you're not using Ubuntu or Debian, you may need to make some changes to the 
bootstrap.sh script. For example, if you're using Red Hat Linux or CentOS, you'll need to 
have the script install Puppet via yum instead of apt. Again, if you're managing nodes on 
multiple OS distributions, you may need to maintain a custom bootstrap script for each one.
Summary
In this chapter, we introduced the demo repo and saw how to download it. We explained the 
control repo pattern, and how it works with r10k and Puppetfile to manage third-party and 
local modules. You learned how to fork the repo and pull changes from upstream.
We looked at the example node definition, role, and profile classes; and how Puppet can use 
Hiera data to configure user accounts, SSH keys, allowed users, and sudoers privileges. We 
covered the use of Forge modules to manage time zone setting and NTP synchronization. 
Additionally, we explored the resources and scripts necessary to control automatic Puppet 
updates and runs.

Putting it all together
[ 234 ]
The demo repo contains bootstrap scripts to help you put a freshly-provisioned node under 
Puppet control, and we examined how these scripts work in detail.
Finally, you learned how to adapt the demo repo for your own site, and we outlined how to 
add your own users and access settings, and your own node definitions and role classes. We 
saw how to plug in your own information to the bootstrap scripts and how to use them to 
bootstrap a new node.
The beginning
I hope you enjoyed this book and have learned something useful from it; I certainly learned a 
lot from writing it. However, there's only so much you can learn from books. As Proust wrote, 
We don't receive wisdom; we must discover it for ourselves after a journey that no one can 
take for us or spare us.
It's good to have a friend point us in the right direction and come with us a little way for 
moral support, but then we need to walk on by ourselves. I hope that this book will be the 
beginning of your journey, not the end.
The world-famous classical guitarist John Williams was once asked how long it took him to 
learn to play the guitar. I'm still learning, he said.

[ 235 ]
Index
A
abstract data types  138
access control  220, 221
agent/master architecture  7, 30
alternative Vagrant VMs  10
Amazon AWS account
setting up  191
Amazon Machine Image (AMI)
selecting  199
AMD-V  11
Apache Mesos  185
arithmetic expressions  71
arrays
creating  67
declaring, of resources  68, 69
iterating over  80
associated mode  43
attribute splat operator  70, 209
attributes, Puppet  7
automatic parameter lookup
from Hiera data  135-137
automatic Puppet runs
testing  36
availability zone (AZ)  202
available data types  137
AWS
resource types  208
AWS account
creating  192
AWS credentials
storing  196
AWS resources
provisioning, from Hiera data  209
AWS SDK gem
installing  198
B
bare word  67
Boolean expressions  71
Booleans
using  66
bootstrap  228-230
bootstrapping
cloud nodes  232
distributions, using  233
node  232
physical nodes  232
providers, using  233
Vagrant VM  232
branching  30
C
case statements
options, selecting with  73, 74
catalog  30
classes
about  132
parameters, declaring to  133-135
class keyword  132, 133
clock
synchronization  225
cloud  190

[ 236 ]
CloudFormation
using  190, 191
cloud nodes
bootstrapping  232
cloud provisioning
automating  190
command line
templates, rendering on  160, 161
comment tag  153
commit
about  28
adding, to Git repo  28, 29
common practice  29
commit message  26
comparison operators  71
complex modules  127, 128
computations
in templates  153
conditional expressions
using  72
conditional statements
in templates  154, 155
configuration data  84
configuration management (CM) tools  5
configuration management manifests  165
configuration management problem  190
configuration synchronization
about  2, 3
changes, repeating across server  3
containers  4
documentation, self-updating  3
shell scripts, issues  4
version control  4
container orchestration  185
container orchestration, requisites
cluster management  185
scheduling  185
service discovery  185
containers
about  4, 164, 165, 166
configuring, with Puppet  187
configuring, with templates  176, 177
connecting  182, 184
managing, with Puppet  168, 169
mini-VMs, or single processes  186
multiple instances, running of  172, 173
Puppet, running inside  186
stopping  171
content type parameters  138, 139
control repo
about  217
URL  216
cron jobs
removing  56
cron resources
about  54, 55
attributes  55
randomizing  56
cron table  54
custom types
reference  63
custom VPC
instance, creating in  204
managing  204
D
decisions
making, with if statements  73
declarative style  6
deep merge  92
defined resource types  140, 141
demo repo
about  217
access control  220, 221
bootstrap credentials, modifying  232
clock synchronization  225
control repo  217
copying  216, 217
custom changes, adapting to  231
module management  217
node definitions, adding  232
nodes  217, 218
obtaining  216
profiles  219, 220
Puppet configuration  225-227
reference link  216
role classes, adding  232
roles  219
SSH configuration  221, 222
Sudoers, configuration  223, 224
time zone  224
users, configuring  231
users, managing  220, 221

[ 237 ]
deployment
options  165
deployment problem  164
dictionary  89
directory  43
dirname() function  121
Docker
containers  166
deployment with  167
installing  169, 170
layered filesystem  168
managing, with Puppet  169
Docker containers
building  167
running  170, 171
Dockerfile format
reference  174
Dockerfiles
images, building from  173
managing  174, 175
Docker images
managing  173
Docker network  182
Docker volume  179-182
dynamic containers
building  176
dynamic data problem  150
E
each function
using  80, 81
EC2 Container Service (ECS)  208
EC2 instance
accessing  201
creating  200, 201
ec2_instance resource  203, 204
EC2 instances
creating, with Puppet  199
ec2_securitygroup resource  202, 203
ec2_vpc_internet_gateway resource  206
ec2_vpc resource  205
ec2_vpc_routetable resource  206, 207
ec2_vpc_subnet resource  207
Embedded Ruby (ERB)  161
empty() function  120
encrypted secret
creating  100, 101
encrypted secrets
adding  102, 103
editing  102, 103
ensure_packages
packages, installing with  117, 118
using  47
EPP (Embedded Puppet)  151
ERB template  161
exec resource
about  57
attributes  57, 58
logoutput attribute  62
manual interaction, automating  57
onlyif attribute  59, 60
refreshonly attribute  60, 61
timeout attribute  62
unless attribute  59, 60
user attribute  59
executable facts
creating  79
expression-printing tag  153
expressions
about  71
facts, referencing in  76
external facts
providing  77, 78
F
fact
hash, accessing of  75
Facter  74
facter command
running  75
Facter data
iterating over  155, 156
facts
about  74
executable facts  79
external facts  77, 78
memory facts  76
networking facts  77
referencing, in expressions  76

[ 238 ]
facts hash
using  74
file_line
files, modifying with  118
files
about  40
modifying  16
modifying, with file_line  118
whole files, managing  40, 41
file tree  43
flexible data types  139
fully qualified domain name (FQDN)  77
G
gems
about  46
installing, in Puppet's context  46, 47
Git
installing  8
Git branching
reference  30
GitHub
repo, pushing to  32, 33
GitHub account
creating  31, 32
GitHub project
creating  31, 32
Git repo
commit, adding to  28, 29
creating  27
Git repository  27
GnuPG
setting up  98, 99
Google's Container Engine (GKE)  185
grep() function  119
group  42
group resource  53
H
hash
about  69
accessing, of fact  75
iterating over  81
resource attributes, setting from  70
has_key() function  120
hasrestart attribute  49
hasstatus attribute  48
Hiera
dealing, with multiple values  91
decryption key, distributing  103
need for  84, 85
querying  87
secrets, decrypting  101, 102
setting up  85, 86
troubleshooting  87
Hiera arrays
resources, building from  94, 95
Hiera data
adding, to Puppet repo  86, 87
AWS resources, provisioning from  209
iterating over  157
iterating over, to create resources  209-212
Hiera data, writing
about  88
arrays  89
Boolean values  89
file header  88
hashes  89
interpolation  90
single values  89
hiera-eyaml-gpg
setting up  99
Hiera hashes
resources, building from  95, 96
hierarchy
about  91
data sources, based on facts  92
merge behaviors  91
host-mounted volume  179
I
IAM policy
creating  192
IAM user
creating  193-195
idempotent  62
Identity and Access Management (IAM)  192
if statements
decisions, making with  73
images
building, from Dockerfiles  173
inline templates  152

[ 239 ]
instance
creating, in custom VPC  204
Internet gateway  204
iteration
about  80
in templates  155
J
join() function  120
K
key  69, 89
L
layering  168
legacy ERB templates  161, 162
linked resource
notifying  22
M
manifest file
creating  18
manifests
about  6
modules, using in  110
templates, using in  151
writing, to set up regular Puppet runs  34, 35
member() function  120
memory facts
using  76
merge behavior  91
module code
writing  123
module management  217
module metadata
creating  124, 126
reference  125
validating  124, 126
modules
about  106
applying  127
complex modules  127, 128
installing  126
reference  128
repo, creating for  122
tagging  126
uploading, to Puppet Forge  128
using, in manifests  110
writing  122
multiple instances
running, of containers  172, 173
multiple nodes
managing  37
N
networking facts
discovering  77
Network Time Protocol (NTP)  122
node definitions  142, 143
nodes  218
non-printing tag  153
O
omnibus package  165
options
selecting, with case statements  73, 74
orchestration
about  185
tools  185, 186
owner  42
ownership  42
P
package-file-service pattern  21
packages
about  44
ensure_packages, using  47
installing, with ensure_packages  117, 118
latest version, installing  45
managing  18
specific versions, installing  45
uninstalling  45
parameter data types
about  137
available data types  137
content type parameters  138, 139
flexible data types  139
range parameters  138

[ 240 ]
parameters
declaring, to classes  133-135
passing, to templates  158, 159
path
Puppet, adding to  10
path attribute  40
pattern attribute  49
permissions  42, 43
persistent storage, for containers
about  179
Docker volumes  180-182
host-mounted volumes  179, 180
physical nodes
bootstrapping  233
pick() function  120
Platform-as-a-Service (PaaS) orchestration  185
procedural style  6
profile class  145, 219, 220
profiles  144, 145, 146
pry debugger  121
Puppet
about  5, 7
adding, to path  10
attributes  7
configuration  225-227
containers, configuring with  187
containers, managing with  168, 169
Docker, managing with  169
dry-running  16, 17
EC2 instances, creating with  199
manifest, applying  17, 19
need for  2
resources  7
running, inside containers  186
template syntax  150, 151
using  191
Puppet Approved module
about  107
reference  107
Puppet architecture
changes, applying automatically  34
changes, fetching automatically  34
puppet/archive
using  115-117
puppet describe  20
Puppetfile  109
Puppet Forge
about  106
modules, uploading to  128
reference  106
Puppet Forge modules
finding  106
using  106
puppetlabs/apache
using  113-115
puppetlabs/aws
installing  198
SSH key pair, creating  196, 197
using  196
puppetlabs/mysql
installing  110
using  110-113
Puppet manifests
about  14
distributing  30
Puppet repo
Hiera data, adding to  86, 87
puppet resource
resources, querying with  19
Puppet Supported module
about  106
reference  106
R
r10k
using  107, 108
range parameters  138
registry  167
regular expressions
about  72
example  72
repo
cloning  33, 34
creating, for modules  122
pushing, to GitHub  32, 33
repository  27
require
resource, ordering with  22
resource attributes
setting, from hash  70
resource declaration  15

[ 241 ]
resource management, with Hiera data
advantages  97
resources
arrays, declaring of  68, 69
building, from Hiera arrays  94, 95
building, from Hiera hashes  95, 96
creating, with Hiera data  94
ordering, with require  22
querying, with puppet resource  19
resources, Puppet  7
restart attribute  49
role class  143
role classes  219
roles  143
route table  204
Ruby documentation
reference  72
Ruby gems
installing  46
run-puppet manifest
applying  35
run-puppet script  36
S
secret data
managing  97
security groups  202
self-configuring containers  177, 178
serverless architectures  5
services
about  20, 48
hasrestart attribute  49
hasstatus attribute  48
pattern attribute  49
restart attribute  49
shallow merge  92
shell scripts
issues  4
software deployment  164
SSH configuration  221, 222
SSH keys
managing  53
stand-alone Puppet  7
stand-alone Puppet architecture  30
standard library
exploring  117
static text  150
strings
variables, interpolating in  67
structured facts
iterating over  156, 157
subnet  202
Sudoers
configuration  223, 224
symbolic link  44
system hostname  77
system memory  76
T
tags  151
template files
referencing  151, 152
templates
about  150
computations  153
conditional statements  154, 155
containers, configuring with  176, 177
inline templates  152
parameters, passing to  158, 159
rendering, on command line  160, 161
using, in manifests  151
working with  158
template syntax
validating  160
template tags  153
Terraform
using  191
time zone  225
typed lookups  88
U
Ubuntu 16.04 LTS distribution  7
uid attribute  52
unused resources
cleaning up  212
user privileges  223
user resource  52
users
about  51
creating  51
managing  220, 221
removing  54

[ 242 ]
V
Vagrant
download link  8
installing  8
troubleshooting  11
Vagrantfile  10
Vagrant, on Windows
reference  9
Vagrant VM
bootstrapping  232
running  9
variables
about  66
interpolating, in strings  67
version control
about  26
changes, tracking  26, 27
code, sharing  27
Virtualbox
installing  8
reference  8
virtualization  190
virtual machine image  165
virtual machine (VM)  8
Virtual Private Cloud (VPC)  202
Vox Pupuli group
reference  129
VT-x  11



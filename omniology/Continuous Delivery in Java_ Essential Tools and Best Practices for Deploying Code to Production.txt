Daniel Bryant 

1. 1. Why Continuous Delivery?
1. Continuous Delivery Overview
2. Enabling Developers
1. Rapid Feedback Reduces Context Switching
2. Automatic, Repeatable and Reliable Releases
3. Codifying the “Definition of Done”
3. Requirements of Modern Java Architecture
1. Need for Business Speed and Stability
2. Rise of the API Economy
3. Opportunities and Costs of the Cloud
4. Modularity Redux: Embracing Small Services
5. Requirements, Architecture and Continuous Delivery
4. Evolution of Java Deployment Platforms
1. WARs and EARs: The Era of Application Server Dominance
2. Executable Fat JARs: Emergence of Twelve Factor Apps
3. Container Images: Increasing Portability (and Complexity)
4. Functions as-a-Service (FaaS): The Emergence of “Serverless”
5. Impact of Platforms on Continuous Delivery
5. DevOps, SRE, and Release Engineering
1. Development and Operations (DevOps)
2. Site Reliability Engineering (SRE)
3. Release Engineering
4. Shared Responsibility, Metrics and Observability
6. Exploring a Typical Build Pipeline
7. Sample Application
8. Summary

2. 2. Designing Architecture for Continuous Delivery
1. Fundamentals of Good Architecture
1. Loose Coupling
2. High Cohesion
3. Coupling, Cohesion and Continuous Delivery
2. Architecture for Business Agility
1. Bad Architecture Limits Business Velocity
2. Complexity and Cost of Change
3. Best Practices for API-Driven Applications
1. Build APIs “Outside-In”
2. Good APIs Assist Continuous Testing and Delivery
4. Deployment Platforms and Architecture
1. Designing Cloud-Native “Twelve Factor" Applications
2. Cultivating “Mechanical Sympathy”
3. Design and Continually Test for Failure
5. The Move Towards Small Services
1. Challenges for Delivering Monolithic Applications
2. Microservices: SOA Meets Domain-Driven Design
3. Functions, Lambdas, and Nanoservices
6. Architecture: “The Stuff That’s Hard to Change”
7. Summary 

Continuous Delivery in Java
Essential Tools and Best Practices for Deploying Code to Production
Daniel Bryant

Continuous Delivery in Java
by Daniel Bryant
Copyright © 2018 Daniel Bryant. All rights reserved.
Printed in the United States of America.
Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.
O’Reilly books may be purchased for educational, business, or sales promotional use. Online
editions are also available for most titles (http://oreilly.com/safari). For more information,
contact our corporate/institutional sales department: 800-998-9938 or corporate@oreilly.com .
Editor: Brian Foster
Production Editor: Nicholas Adams
Interior Designer: David Futato
Cover Designer: Karen Montgomery
Illustrator: Rebecca Demarest
March 2018: First Edition

Revision History for the Early Release
2017-11-01: First release
See http://oreilly.com/catalog/errata.csp?isbn=9781491986028 for release details.
The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Continuous Delivery in
Java, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.
While the publisher and the author have used good faith efforts to ensure that the information
and instructions contained in this work are accurate, the publisher and the author disclaim all
responsibility for errors or omissions, including without limitation responsibility for damages
resulting from the use of or reliance on this work. Use of the information and instructions
contained in this work is at your own risk. If any code samples or other technology this work
contains or describes is subject to open source licenses or the intellectual property rights of
others, it is your responsibility to ensure that your use thereof complies with such licenses and/or
rights.
978-1-491-98602-8
[LSI]

Chapter 1. Why Continuous Delivery?
In this chapter I introduce the core concepts of continuous delivery, and explore the benefits for
the developers, QA, operations and business teams. I will briefly look back in time and explore
how Java application architecture and deployment platforms have evolved, and discuss the
impact this has had on our ability to rapidly and safely delivery new software to production
environments. Finally I explore the human or “soft skills” side of continuous delivery that is
focusing on increasing shared responsibility for the creation and operation of software, such as
DevOps and SRE, before examining what a typical Java continuous delivery pipeline looks like. 
(1)

Continuous Delivery Overview
Continuous Delivery (CD) is fundamentally a set of practices and disciplines in which software
delivery teams produce valuable and robust software in short cycles. Care is taken to ensure that
functionality is added in small increments and that the software can be reliably released at any
time. This maximises the opportunity for rapid feedback and learning, both from a business and
technical perspective. In 2010, Jez Humble and Dave Farley published their seminal book
Continuous Delivery (Addison- Wesley), which collated their experiences of deploying software
delivery projects around the world, and this publication is still the go-to reference for CD. The
book contains a very valuable collection of techniques, methodologies, and advice from the
perspective of both technology and organisations. 
Much has changed in the world of software development and delivery over the past twenty years.
Business requirements and expectations have changed dramatically, with a focus of innovation,
speed and time-to-market. Architects and developers have reacted accordingly, and new
architectures have been designed to support these requirements. New deployment fabrics and
platforms have been created and have co-evolved alongside new methodologies like DevOps and
Release Engineering. Alongside these changes a best practice of creating a build pipeline has co-
evolved, through which any candidate change to the software being delivered is built, integrated,
tested, and validated before determining that it is ready for deployment to a production
environment. In this book I focus on accomplishing this task for modern Java-based applications,
enabling developers that are creating monoliths, microservices or serverless application to create
an effective build pipeline. 
(2)

Enabling Developers
An important questions to ask before undertaking any task with software development is ‘why?’.
Why, as a Java developer, should I invest my valuable time in embracing continuous delivery
and creating a build pipeline?
(3)

Rapid Feedback Reduces Context Switching
Feedback is vital when working with complex systems, and nearly all software applications are
complex adaptive systems. This is especially true of modern component-based software systems
that are deployed to the web, which are essentially distributed systems. A quick review of the IT
press publications over the past twenty years reveals that software development issues are often
only discovered when large (and costly) failures occur. Continual, rapid and high quality
feedback provides early opportunities to detect and correct errors. This allows the detection and
remediation of problems while they are smaller, cheaper and easier to fix. From a developer’s
point of view, one of the clear advantages of rapid feedback is the reduced cost in context
switching and attempting to remember what you were doing with a piece of code that contains a
bug. I don’t need to remind you that it is much easier to fix an issue that you were working on
five minutes ago, rather than one you were working on five months ago!
(4)

Automatic, Repeatable and Reliable Releases
The build pipeline must provide rapid feedback for the development team in order to be useful
within their daily work cycles, and the operation of the pipeline must be highly repeatable and
reliable. Accordingly, automation is used extensively, with the goal of 100% automation or as
close as you can realistically get to this. The following items should be automated:                       
                                    
Software compilation and code-quality static analysis.
Functional testing, including unit, component, integration, and end-to-end.
Provisioning of all environments, including the integration of logging, monitoring, and
alerting hooks.
Deployment of software artifacts to all environments, including production
Data store migrations.
System testing, including nonfunctional requirements like fault tolerance, performance,
and security.
Tracking and auditing of change history.
With the automation of the release process complete (and repeatable and reliable) we, as
developers and operators, have confidence in continually releasing new functionality without
causing breakages or regressions. Nothing destroys morale as quickly as have to rely on an
unreliable and flakey deployment process. This leads to fear in deploying code, which in turn
encourages teams to batch large amounts of functionality in “big bang” release, which ultimately
leads to even more problematic releases. This viscous feedback loop must be broken, and the
adoption of the continuous delivery of functionality in small batch sizes (ideally with single
piece flow) is a great approach to help encourage this.
(5)

Codifying the “Definition of Done”
The fast feedback and automation of the release process is useful for developers in and of itself.
However, another clear advantage of creating a build pipeline is that I can codify the ‘definition
of done’. When a software component successfully traverses a build pipeline then this should
unequivocally indicate that it is ready to go into production, provide the value planned, and
function within acceptable operational parameters that include availability, security and cost.
Historically, it has been difficult for teams to ensure a consistent definition of done, and this can
be a friction point between development and business teams within an organisation. As I will
show in later chapters of this book, the assertion of many functional and nonfunctional (cross
functional) properties can be codified within a modern Java build pipeline, including fault
tolerance, the absence of known security vulnerabilities, and basic performance/load
characteristics (which in turn can support the calculation of cost).
(6)

Requirements of Modern Java Architecture
Many Java developers have been practicing continuous integration and continuous delivery for
the past decade, and books like Java Power Tools (O’Reilly) by John Smart provided the
guidelines and frameworks to make this possible. Technologies have obviously changed within
the last ten years, and so have associated architectural styles. In particular, business teams within
organisations have increasingly demanded the IT teams to become more flexible and be capable
of rapidly responding to change in customer preferences and market conditions. The emergence
of dynamic and programmable compute resources and deployment platforms, combined with
teams and organisations exposing Application Programming Interfaces (APIs) as products has
resulted in the architectures that Java developers are creating to converge towards
component/service/function-based architectures. All of these factors have led to (and in turn,
have been driven by) the emergence of popular movements such as Agile, Lean, DevOps, Cloud
Computing, Programmable Infrastructure, microservices, and serverless or Function-as-as-
Service (FaaS).
(7)

Need for Business Speed and Stability
During his time as Cloud Architect at Netflix, Adrian Cockcroft talked a lot about about ‘time to
market’ being a competitive advantage, and in many modern markets ‘speed kills’. Uwe
Friedrichsen, CTO at codecentric, has also talked extensively about this trend beginning in the
1980s: globalisation, market saturation and the Internet led to highly competitive and dynamic
ecosystems. The markets became highly demand-driven and the new biggest challenge of the
companies was to adapt to the changing demands of the customers quickly enough. The key
driver changed from “cost-efficient scaling” to “responsiveness”. Over the same time period, the
move to public commodity infrastructure (“Cloud”) in combination with increasing transaction
value flowing through global computer systems has meant that new failure modes are being
discovered, and new attackers are emerging from the shadows. This has caused the need to
balance stability and security against the requirement for speed. Often this isn’t an easy balance
to maintain.
Continuous delivery is achieved when stability and speed can satisfy business demand.
Discontinuous delivery occurs when stability and speed are insufficient.
- Steve Smith (@AgileSteveSmith)
Accordingly, you now need to create architectures that support rapid, safe and stable change, and
continually ensure that you are meeting these requirements through automated testing and
validation.
(8)

Rise of the API Economy
Application Programming Interfaces (APIs) are at the core of the Internet and a modern
developer’s daily life - RESTful services are the de facto way to expose and consume third-party
online business services. However, as stated by Jennifer Riggins when attending the 2017
APIDays conference, what people might not realise is how much the API will be at the center of
the future technology, and part of every connected person’s daily life. APIs will continue to play
a central role in trends like chatbots and virtual assistants, the Internet of Things (IoT), mobile
services, and so much more. APIs are also being increasingly consumed as “shadow IT” by
departments that were traditionally less “tech-savvy” like marketing, sales, finance, and human
resources. “Mediated APIs” - APIs that act as bridges between new and old applications - are
becoming increasingly popular, as they provide adaptations and opportunities for innovation in
businesses that have considerable investment locked within legacy infrastructure. Gartner, the
US-based research and advisory firm, suggests that concepts such as the “API marketplace” and
the “API economy” are becoming increasingly important within the global economy. 
As the API marketplace becomes more sophisticated and widespread, the risks for failure and
security issues become more apparent. APIs have made technology more accessible than ever,
which means that Enterprise Architects, the traditional bastions of technology adoption, are no
longer the gatekeepers for technical decision-making. Accordingly, this empowers every
developer in an organisation to innovate, but at the same time can lead to unintended
consequences. It is essential to codify not only functional requirements for an API - for example
using Behaviour-Driven Design (BDD) and automated testing - but also non functional (or cross-
functional) requirements and Service Level Agreements (SLAs) related to security, performance
and expected cost. These must be continually tested and validated, as this has a direct impact in
the product being offered to customers.
(9)

Opportunities and Costs of the Cloud
It can be argued that the cloud computing revolution began when Amazon Web Services (AWS)
was officially launched in March 2006. Now the Cloud Computing market generates $200+
billion in revenue annually. Cloud computing technologies have brought many advantages - on-
demand hardware, rapid scalability and provisioning, and flexible pricing - but have also
provided many challenges for architects and developers. These include the requirements to
design for the ephemeral nature of cloud computing resources, the need to understand the
underlying characteristics of a cloud system (including ‘mechanical sympathy’ and fault
tolerance), and the requirement for an increase in operational and sysadmin knowledge (such as
operating systems, configuration management, and networking). Architects and developers
unfamiliar with cloud technologies must be able to experiment and implement continuous testing
with these deployment fabrics and platforms, and this must be done in a repeatable and reliable
way. Early testing within a build pipeline using application deployed on infrastructure and
platforms that are as like production as possible is essential to ensure assumptions on
performance, fault tolerance and security are valid.
(10)

Modularity Redux: Embracing Small Services
The combination of the need for speed from the business, the adoption of REST-like APIs, and
the emergence of cloud computing has provided new opportunities and challenges to software
architecture. Core topics in this space include the scaling both the organisation aspects of
developing software (e.g. Conway’s Law) and the technical aspects (e.g. modularisation), and
also the requirement to deploy and operate parts of the codebase independently of each other.
Much of this has been incorporated within the emerging architectural pattern known as the
“microservices”. This book will discuss the drivers and core concepts of microservices in
Chapter 2, and explore how this helps and hinders the implementation of CD. A further
introduction to microservices can be found in Christian Posta’s Microservices for Java
Developers (O’Reilly), and a more thorough treatment can be found in Sam Newman’s Building
Microservices (O’Reilly) and Amundsen et al’s Microservice Architecture (O’Reilly). At a high-
level the building of Java-based microservices impacts the implementation of CD in several
ways:
Multiple build pipelines (or branches within a single pipeline) must be created and
managed.
Deployment of multiple services to an environment now have to be orchestrated, managed,
and tracked.
Component testing may now have to mock, stub, or virtualize dependent services.
End-to-end testing must now orchestrate multiple services (and associated state) before
and after executing tests.
Process must be implemented to manage service version control (e.g., the enforcement of
only allowing the deployment of compatible interdependent services).
Monitoring, metrics, and application performance management (APM) tooling must be
adapted to handle multiple services.
Decomposing an existing monolithic application, or creating a new application that provides
functionality through a composite of microservices is a non-trivial task. Techniques such as
context mapping, from domain-driven design, can help developers (working alongside
stakeholders and the QA team) understand how application/business functionality should be
composed as a series of bounded contexts or focused services. Regardless of how applications
are composed, it is still vitally important that both individual components and the system as a
whole is continually being integrated and validated. The need for continuous delivery only
increases as more and more components are combined, as it becomes near-impossible to
manually reason about their combined interactions and functionality.
(11)

Requirements, Architecture and Continuous Delivery
Hopefully this exploration of the requirements of modern Java architecture has highlighted the
benefits - and some cases the essential need - of continuous delivery to ensure that software
systems provide the required functionality. The requirements and evolution of architectural styles
is just one part of the puzzle though, and at the same time new platforms have emerged that have
either codified several of the architectural best-practices or have attempted to help address some
of the same problems.
(12)

Evolution of Java Deployment Platforms
Java has an amazing history, and not many languages that are still relevant today can claim to
have been used for over twenty years. Obviously during this time the language has evolved itself,
partly to continually drive improvement and developer productivity, and partly to meet the
requirements imposed by new hardware and architectural practices. Because of this long history,
there are now a multitude of ways to deploy Java applications into production.
(13)

WARs and EARs: The Era of Application Server
Dominance
The native packaging format for Java is the Java Application Archive (JAR) file, which can
contain library code or a runnable artifact. The initial best-practice approach to deploying Java
Enterprise Edition (J2EE) applications was to package code into a series of JARs - often
consisting of modules that contain Enterprise Java Beans (EJB) class files and EJB deployment
descriptor - and these were further bundled up into another specific type of JAR with a defined
directory and structure and required metadata file. This was either a Web Application Archive
(WAR) - which consisted of Servlet class files, JSP Files and supporting files - or an Enterprise
Application Archive (EAR) file, which contained all the required mix of JAR and WAR files for
the deployment of a full J2EE application. This artifact was then deployed into a heavyweight
application server (commonly referred to at the time as a “container”) such as WebLogic,
WebSphere or JBoss EAP. These application servers offered container-managed enterprise
features such as logging, persistence, transaction management and security. 
Figure 1-1. The initial deployment of Java applications used WAR and EAR artifacts deployed into an application server that
defined access to external platform services via JNDI
Several lightweight application servers also emerged in response to changing developer and
operational requirements, such as Apache Tomcat, TomEE and Red Hat’s Wildfly. Classic Java
Enterprise applications and Service Oriented Architecture (SOA) were also typically supported at
runtime by the deployment of messaging middleware such as Enterprise Service Buses (ESBs)
and heavyweight Message Queue (MQ) technologies.
(14)

Executable Fat JARs: Emergence of Twelve Factor
Apps
With the emergence of the next generation of cloud-friendly service-based architectures and
introduction of open source and commercial Platform-as-a-Service (PaaS) platforms like Google
App Engine and Cloud Foundry, deploying Java applications using lightweight and embedded
application servers became popular. Technologies that emerged to support this included the in-
memory Jetty web server, and also later editions of Tomcat. Application frameworks such as
DropWizard and Spring boot soon began providing mechanism through Maven and Gradle to
package (for example, using Apache Shade) and embed these application servers into a single
deployable unit that can run as a standalone process - the executable “Fat JAR” was born. 
Figure 1-2. The second generation of Java application deployment utilised executable “Fat JARs”, and followed the principles
of the “Twelve Factor Application”, such as storing configuration within the environment.
The best practices for developing, deploying and operating this new generation of applications
was codified by the team at Heroku as the "Twelve Factor App“.
(15)

Container Images: Increasing Portability (and
Complexity)
Although Linux container technology had been around for quite some time, the creation of
Docker in March 2013 brought this technology to the masses. At the core of containers is Linux
technologies like cgroups, namespaces and a (pivot) root file system. If Fat JARs extended the
scope of traditional Java packaging and deployment mechanisms, containers have taken this to
the next level. Now, in addition to packaging your Java application as a Fat Jar, you must include
an operating system (OS) within your container image. 
(16)

Figure 1-3. Deploying Java Applications as Fat JARs running within their own namespaced container (or Pod) requires
developers to be responsible for packaging an OS within container images.
Due to the complexity and dynamic nature of running containers at scale, the resulting image is
typically run on a container orchestration and scheduling platform like Kubernetes, Docker
Swarm, or Amazon ECS.
(17)

Functions as-a-Service (FaaS): The Emergence
of “Serverless”
In November 2014 Amazon Web Services launched a preview of AWS Lambda at their global
re:Invent conference, held annually in Las Vegas. AWS Lambda lets developers run code
without provisioning or managing servers - this is commonly referred to as “severless” -
although as servers are still required to run the application the focus is typically on reducing the
operational burden of running and maintaining the application’s underlying runtime and
infrastructure. The development and billing model is also unique in that Lambdas are triggered
by external events - which can include a timer, a user request via an attached AWS API
Gateway, or an object being uploaded into the AWS S3 blobstore - and you only pay for the time
your function runs and the memory consumed. 
(18)

Figure 1-4. Deploying Java Applications via the Function-as-a-Service (FaaS) or serverless model. Code is packaged within a
JAR or ZIP, which is then deployed and managed via the underlying platform (that typically utilises containers)
In June 2015 AWS Lambda added support for Java, and you now return back to the deployment
requirement for a JAR or ZIP file containing Java code to be uploaded to the AWS Lambda
service.
(19)

Impact of Platforms on Continuous Delivery
Developers often ask if the packaging format of the application artifacts effect the
implementation of continuous delivery. My answer to this question, as with any truly interesting
question, is “it depends”. The answer is “yes”, because the packaging format clearly has an
impact on how an artifact is built, tested and executed: both from the moving parts involved, and
also the technological implementation of a build pipeline. However, the answer is also “no”,
because the core concepts, principles and assertions of continuously delivering a valid artifact
remain unchanged. Throughout this book I demonstrate core concepts at an abstract level, but
will also provide concrete examples for each of the three most relevant packaging styles: Fat
JARs, container images, and functions.
(20)

DevOps, SRE, and Release Engineering
Over the last tens years I have seen roles within software development evolve and change, with a
particular focus on shared responsibility. Let’s now explore the new approaches and philosophies
that have emerged, and attempt to understand how this has impacted continuous delivery and
vice versa.
(21)

Development and Operations (DevOps)
It can be argued that the compound of “Development” and “Operations”, DevOps, does not truly
capture the spirit of the associated movement or philosophy. Probably the term Businss-
Development-QA-Security-Operations (BizDevQaSecOps) captures the components better, but
is far too much of a mouthful. At the 2008 Agile Toronto conference, Andrew Shafer and Patrick
Debois introduced the term DevOps in their talk on "Agile Infrastructure“. From 2009, the term
has been steadily promoted and brought into more mainstream usage through a series of
“devopsdays”, which started in Belgium and has now spread globally. DevOps at its core is a
software development and delivery process that emphasises communication and collaboration
between product management, software development, operations professionals and close
alignment with business objectives. It supports this by automating and monitoring the process of
software integration, testing, deployment, and infrastructure changes by establishing a culture
and environment where building, testing, and releasing software can happen rapidly, frequently,
and more reliably. I’m sure many of you reading will think this sounds a lot like the principles of
continuous delivery - and you would be right! However, continuous delivery is really just one
tool in the DevOps toolbox. It is a core and very valuable tool, but to truly have success with
designing, implementing and operating a continuous delivery build pipeline, there typically
needs to be a certain level of buy-in throughout the organisation, and this is where the practices
associated with DevOps shine.
Figure 1-5. DevOps is a combination of Development and Operations (and more). Image taken from web.devopstopologies.com
Keen to Learn More About DevOps?
In this book I will focus on the technical implementation of continuous delivery, and as such if
you are interested to learn more about DevOps and the associated bigger picture, I recommend
reading "The Pheonix Project" and "The DevOps Handbook“. The "Lean Enterprise" and "Agile
IT Organisation Design" are also excellent references on the organisational and process changes
(22)

that can drive (and are some degree required) for continuous delivery.
(23)

Site Reliability Engineering (SRE)
The phrase "Site Reliability Engineering (SRE)" was made popular by the book of the same
name that was written by the SRE team at Google. In an interview with Niall Murphy and Ben
Treynor, both working within the engineering division at Google, they state that fundamentally
SRE is what happens when you ask a software engineer to design an operations function: “using
engineers with software expertise, and banking on the fact that these engineers are inherently
both predisposed to, and have the ability to, substitute automation for human labor." In general,
an SRE team is responsible for availability, latency, performance, efficiency, change
management, monitoring, emergency response, and capacity planning. However, a key
characteristic of SRE teams at Google is that each engineer should only be doing a maximum of
50% operations work - the rest of their time should be spent designing and building systems and
the supporting tooling. At Google this split between workloads is continually measured, and
reviewed once a quarter. SRE teams at Google are a precious resource, and development teams
typically have to create a case for SRE support on their projects; particularly in the early proof-
of-concept stage with a product. Google have institutionalised responses to providing SRE
support, with processes like the Production Readiness Review (PRR). The PRR helps to avoid
getting into a bad situation where the development teams are not incentivised to create
production-ready software with a low operational load by examining both the system and its
characteristics before taking it on, and also by having shared responsibility.
Figure 1-6. SRE and DevOps. Image taken from web.devopstopologies.com
The Google SRE team have also talked extensively about the way they monitor systems. A
classic approach to monitoring is to watch a value or a condition and when the monitoring
system observe something interesting, it sends an email. However, email is not the right
approach for this - if you are requiring a human to read the email and decide whether something
needs to be done, the Google team SRE believe you are making a mistake. Ideally a human never
interprets anything in the alerting domain. Interpretation is done by the software you write. You
(24)

just get notified when you need to take action. Accordingly, the SRE book states that there are
only three kinds of valid monitoring output.
There are alerts, which say a human must take action right now. Something that is
happening or about to happen, that a human needs to take action immediately to improve
the situation.
The second category is tickets. A human needs to take action, but not immediately. You
have maybe hours, typically, days, but some human action is required.
The third category is logging. No one ever needs to look at this information, but it is
available for diagnostic or forensic purposes. The expectation is that no one reads it.
This information is important, because as a developer I must implement appropriate logging and
metrics within our systems. This also must be tested as part of a continuous delivery pipeline.
(25)

Release Engineering
Release Engineering is a relatively new and fast-growing discipline of software engineering that
can be described as building and delivering software. Release engineers focus on building a
continuous delivery pipeline, and have expert understanding of source code management,
compilers, automated build tools, package managers, installation, and configuration
management. According to the Google SRE book, a release engineer’s skill set includes deep
knowledge of multiple domains: development, configuration management, test integration,
system administration, and customer support. The success of release engineering within an
organisation is highly correlated with the successful implementation of a build pipeline, and
typically consists of metrics focused on time taken for a code change to be deployed to
production, the number of open bugs, the percentage of successful releases, and the percentage of
releases that were abandoned or aborted after they began. Steve Smith has also talked extensively
in his book "Measuring Continuous Delivery" about the need to collect, analyse and take action
based on these metrics.
(26)

Shared Responsibility, Metrics and Observability
If you work within a team at a large enterprise company the concepts of DevOps, SRE and
Release Engineering may appear alien at first glance. A common push back from such teams is
that these approaches only work for the “unicorn” companies like Google, Facebook and
Amazon, but in reality these organisations are blazing a trail that many of us are now following
in. For example, Google was the first to embrace containerisation to facilitate rapid deployment
and flexible orchestration of services; Facebook promoted the use of a monorepo to store
code, and also released associated open source build tooling that is now used extensively; and
Amazon drove the acceptance of exposing internal service functionality only via well-defined
APIs. Although you should never cargo cult -- or blindly copy only the things or results you can
-- you can learn much from their approaches and processes. The key trends discussed in the
previous sections also have a direct impact on the implementation of continuous delivery:
Increasing shared responsibility across development, QA, and operations (and arguably the
entire organisation) is essential for the success adoption of continuous delivery
The definition, capture and analysis of software build, deployment and operation metrics
are vital to continuous delivery. They help the organisation understand where it currently
is and what success will look like, and assist in charting and monitoring the journey
towards this.
Automation is essential for reliably building, testing and deploying software.
With these trends in mind, let’s now explore a typical build pipeline.
(27)

Exploring a Typical Build Pipeline
The figure below demonstrates a typical continuous delivery build pipeline for a Java-based
monolithic application. The first step of the process of CD is continuous integration (CI). Code
that is created on a developer’s laptop is continually committed (integrated) into a shared version
control repository, and automatically begins its journey through the entire pipeline. 
(28)

Figure 1-7. A typical continuous delivery build pipeline 
The primary goal of the build pipeline is to prove that changes are production-ready. A code of
configuration modification can fail at any stage of the pipeline, and this change will accordingly
be rejected and not marked as ready for deployment to production. Initially, the software
application to which a code change is being applied is built and tested in isolation, and some
form of code quality analysis may (should) also be applied, perhaps using a tool like SonarQube.
Code that successfully passes the initial unit and component tests and the code-quality metrics
moves to the right in the pipeline, and is exercised within a larger integrated context. Ultimately,
code that has been fully validated emerges from the pipeline and is marked as ready for
deployment into production. Some organizations automatically deploy applications that have
successfully navigated the build pipeline and passed all quality checks—this is known as
continuous delivery. Once code has been deployed to production you should take care not to
forget about observability - monitoring, logging and alerting - for both the purposes of closing
the feedback loop for business and technical hypotheses, but also for potential debugging of
production issues.
(29)

Sample Application
(30)

Figure 1-8. Architecture of sample application, which continuously delivered throughout the chapters of this book
(31)

Summary
In this introductory chapter you have learned the core foundations of continuous delivery and
explored the associated principles and practices:
Continuous Delivery (CD) is fundamentally a set of practices and disciplines in which
software delivery teams produce valuable and robust software in short cycles.
For developers CD enables rapid feedback (reducing context switching); allows automatic,
repeatable and reliable software releases; and codifies the “definition of done”.
Modern software architecture must adapt to meet the changing requirements from the
business of speed and stability, and implementing an effective continuous delivery pipeline
is a core part of delivering and verifying this.
Java deployment packages and platforms have changed over the years, from WARs and
EARs deployed onto Application Servers, through to Fat (runnable) JARs deployed in the
cloud or PaaS, and ultimately to container images deployed into container orchestration or
serverless platforms. A continuous delivery pipeline must be built to support your specific
platform.
The focus on shared responsibility over the last ten years - through DevOps, SRE and
release engineering - has increased your responsibilities as a developer implementing
continuous delivery, and you must now implement continual testing and observability
within the software you write.
A CD build pipeline consists of local development, commit, build, code quality analysis,
packaging, QA and acceptance testing, nonfunctional (system quality attributes) testing,
deployment and observation.
Next I’ll show you how to design and implement software architecture for continuous delivery.
(32)

Chapter 2. Designing Architecture for
Continuous Delivery
Now that you have been introduced to the motivations for continuous delivery let’s look at the
technical foundations to enable this: software architecture. It could be argued that software based
on any architectural style can be continuously delivered, but according the State of DevOps
Report 2017 the strongest predictor of continuous delivery is loosely coupled architectures
(alongside loosely coupled teams). I’ll now explore how to create and cultivate an architecture
that supports continuously delivering Java applications.
(33)

Fundamentals of Good Architecture
The Software Engineering Institute (SEI) defines software architecture as “the set of structures
needed to reason about the system, which comprises software elements, relations among them,
and properties of both”. Although this may at first glance appear quite abstract, the mention of
“structures”, “elements” and “properties” are core to what the majority of software engineers
think of as architecture. Taking a slightly different perspective, it is quite possible that many of
us can relate to Martin Fowler’s definition that software architecture consists of the “things that
people perceive as hard to change”. Regardless of which definition you prefer, there are several
properties of a software system that are fundamental to creating fit-for-purpose architecture.
(34)

Loose Coupling
A loosely coupled system is one in which each of its components has, or makes use of, little or
no knowledge of the definitions of other separate components. The obvious advantage of this is
that components within a loosely coupled system can be replaced with alternative
implementations that provide the same functionality. Loose coupling within programming is
often interpreted as encapsulation, or information-hiding, versus non-encapsulation. Within the
Java programming language this can be seen in primarily two places: first with method
signatures that utilise interface types vs concrete class types - the former makes extending
applications much easier by loosely coupling and deferring the choice of concrete class until
runtime; and second via Plain Old Java Objects (POJO) getters and setters (accessors and
mutators) - by hiding and controlling the access to internal state I have much more control in
making changes to the internals of the class.
At the application or service level, loose coupling is typically achieved through well-defined and
flexible component interfaces, for example, using REST contracts (e.g. Pact or Spring Cloud
Contract) with JSON over HTTP/S; using an Interface Definition Language (IDL) such as gRPC,
Thrift or Avro; or messaging via RabbitMQ or Kafka. Examples of tight coupling would include
Java RMI, where domains objects are exchanged in the native Java serialization format.
(35)

High Cohesion
Cohesion refers to the degree to which the elements within a component belong together, and can
be thought of within programming as the measure of strength of the relationship between pieces
of functionality within a given module or class. Modules with high cohesion tend to be
preferable, because high cohesion is associated with several desirable traits of software including
robustness, reliability, reusability, and understandability. In contrast, low cohesion is associated
with undesirable traits such as being difficult to maintain, test, reuse, or even understand. A good
example in the Java language can be found within the java.util.concurrent package, which
contains classes that cohesively offer functions related to concurrency. The classic Java counter-
example is the java.util package itself, which contains functions that relate to concurrency,
collections, and a Scanner for reading text input - these functions are clearly not cohesive!
At the application and service level, the level of cohesion is often evident by the interface
exposed. For example, if a User service exposed functionality related only to working with
application Users, such as add new User, update contact email address, or promote the User’s
customer loyalty tier, this would be highly cohesive. A counter example would include a User
service that also offered functionality to add items to an e-commerce shopping basket, or a
payment API that also allowed stock information to be added to the system.
(36)

Coupling, Cohesion and Continuous Delivery
Applications with a loosely coupled and highly cohesive architecture are easier to continuously
deliver, and therefore you should strive to design and evolve systems with this in mind. A good
architecture facilitates CD through the following mechanisms:
Design - during the design phase of a new or evolving system, having clear and well-
defined interfaces specified throughout the system allows for loose coupling and high
cohesion. This in turns makes it easier to reason about the system. When given new
requirements for a specific area of functionality, a highly cohesive system immediately
directs you to where the work should take place, as an alternative to you having to trawl
through the code of several multi-functional modules with low cohesion. Loose coupling
allows you to change design details of an application (perhaps in order to reduce resource
consumption) with a significant reduction in concern that you will impact other
components within the overall system. 
Build, unit and integration test - a highly cohesive service or module facilitates
dependency management (and the associated testing) as the amount of functionality
offered is limited within scope. Unit testing, mocking and stubbing is also much easier in a
loosely coupled system, as you can simply swap configurable synthetic test doubles in for
the real thing when testing.
Component test - components that are highly cohesive lead to easy-to-understand test
suites, as the context needed by developers in order to grok the tests and assertions is
generally limited. Loose coupling of components allows external dependencies to be easily
simulated or virtualised as required.
End-to-end test - systems that are loosely coupled and highly cohesive are easier to
orchestrate when performing end-to-end tests. Highly coupled systems tend to share data
sources, which can make the curation of realistic test data fiendishly difficult. When the
inevitable issues do occur with end-to-end testing, a highly cohesive system will generally
be much easier to diagnose and debug, as functionality is logically grouped in relation to
theme.
Deployment - applications and services that are loosely coupled are generally easy to
deploy in a continuous fashion, due to the fact that each service has little or no knowledge
of others. Highly coupled services typically have to be deployed in lock-step (or
sequentially) due to the tight integration of functionality, which makes the process time-
consuming and error prone. Highly cohesive services typically minimise the number of
sub-systems that have to be deployed in order to release new functionality, and this results
in less artifacts being pushed down the pipeline, reducing resource consumption and
coordination.
Observability - a cohesive service is easy to observe and comprehend. Imagine if you have
a service that performs five unrelated tasks, and suddenly your production monitoring tool
alerts you to high CPU usage - it will be difficult to understand which functionality is
causing the issue. A highly coupled application is often difficult to diagnose when things
inevitably go wrong, as failures can cascade throughout the system, which in turn
obfuscate the underlying causes.
With the foundational guidance in place, let’s now take a look at designing applications the
provide business value using modern architectural themes and practices.
(37)

Architecture for Business Agility
If you have ever worked on a large-scale software system that is continually being driven by new
functional requirements, you will most likely at some point have bumped into a limiting factor
imposed by the system architecture. This is almost inevitable due to the increased focus in the
business world on short-term gains (versus long-term investment), and unforeseen changes in
both the business and the development of the technological landscape. Software architecture also
tends to “evolve” over time within many companies, with occasional refactoring sprints being
allocated to teams in order to prop up major issues, or worse case, little attention being paid until
a “big bang” re-write is forced. Continuous delivery can be used to monitor and enforce certain
architectural properties, but you have to understand the principles of how architecture relates to
business value, and then design applications and process accordingly.
(38)

Bad Architecture Limits Business Velocity
If a business has no well-defined architecture to its systems, it is much harder to properly assess
the cost of doing something in a well-defined timeframe. The ‘mess’ of your architecture creates
excessive costs and missed opportunities. This can have really bad competitive consequences, as
the overhead incurred can effectively be many multiples depending on the number of systems
and overall complexity. Often developers and architects find it difficult to convince non-
technical management of these issues, and although empathy must be developed by both sides,
one analogy that spans disciplines is building a house from a complete plan with the intended
usage versus just adding on rooms and floors as you go along and watching how many people
turn up.
The other hidden cost with a lack of architecture is that in inordinate amount of time is spent
patching systems rather than innovating. If more time is spent playing software bug ‘whack-a-
mole’ than actually doing new features – you know you have a rotten architecture. Good
software architectures encourage producing bug free software and actually guard against the
negative consequences of a bug. They ‘contain’ the error and through their structure provide
mechanisms to cheaply overcome any problems caused. Good architecture also encourages
greater innovation, as it is clearer what needs doing to support innovation on top of it – also good
architecture in itself can be a catalyst for innovation. You might see gaps or opportunities that
would have otherwise been hidden.
Although the continual monitoring and analysis of architectural qualities in relation to business
value is somewhat orthogonal to the implementation of continuous delivery, the creation of a
build pipeline can be an effective way to introduce the capture of relevant metrics. Tools such as
SonarQube can be woven into the build process and used to show cohesion, coupling and
complexity hotspots within the code and also report high-level complexity metrics such as
Cyclomatic Complexity -- the quantitative measure of the number of linearly independent paths
through a program’s source code -- and the Design Structure Quality Index (DSQI) -- an
architectural design metric used to evaluate a computer program’s design structure and the
efficiency of its modules. Additional tooling, such as Adam Tornhill’s Code Maat, can also mine
and analyse data from version-control systems, and show areas of the codebase that are regularly
churning. This can demonstrate to the business that an investment in improving the architecture
to reduce improve and facilitate understanding within these high-churn areas of the codebase can
provide a high return on investment.
(39)

Complexity and Cost of Change
A typical ‘evolved’ architecture usually consists of a mix of technologies, purely based on what
was to hand and the experiences of the engineers involved. This is where such a lack of structure
in the architecture hits a business hard, instead of having to consider just one technology when
making changes, you end with a forest of interconnecting technologies in which no one person or
team is the ‘domain expert’. Making any change is a risky undertaking with a lot of inherent cost.
Now compared to the business which has been very careful to keep its complexity under control,
its costs of change could be a fraction of that experienced by its ‘complex’ competitor – the end
result being the complex competitor cannot keep pace with its more technically lean rival – it’s a
slow and painful death by a thousand failed and delayed updates. In fact some businesses have
got management of their technical complexity and architecture fine-tuned to such a point that
they are able, with complete confidence, to ship live multiple updates each day.
A well-defined software architecture assists the management of complexity by showing and
tracking:
The real interdependencies between systems.
What system holds which data and when.
The overall technical complexity in terms of operating systems, frameworks, libraries and
programming languages used.
Many of these properties can be generated, monitored and enforced within a good continuous
delivery pipeline.
(40)

Best Practices for API-Driven Applications
All software applications expose APIs somewhere within the system - from the internal classes,
packages and modules, to the external software interface. Over the past ten years there has been
an increase as APIs being seen as software products themselves - just looks at the prevalence of
SaaS-based API offerings like Google Maps, Stripe payments, and the Auth0 authentication API.
From a programmer’s perspective, an API that is easy to work with must be highly cohesive and
loosely coupled, and the same applies for integrating API-based services into CD pipeline.
(41)

Build APIs “Outside-In”
A good API is typically designed outside-in, as this is the best way to meet user requirements
without overly-exposing internal implementation details. One of the challenges with classical
SOA was that APIs were often designed inside-out, which meant that the interface presented
‘leaked’ details on the internal entities and functions provided. This broke the principle of
encapsulating data, and in turn meant that services integrating with other services were highly
coupled as they relied on internal implementation details. 
Many teams attempt to define a service API upfront, but in reality the design process will be
iterative. A useful technique to enable this iterative approach is the behavior-driven development
(BDD) technique named “The Three Amigos,” where any requirement should be defined with at
least one developer, one QA specialist, and one project stakeholder present. The typical outputs
from this stage of the service design process include: a series of BDD-style acceptance tests that
asserts component (single microservice) level requirements, such as Cucumber Gherkin syntax
acceptance test scripts; and an API specification, such as a Swagger or RAML file, which the test
scripts will operate against. It is also recommended that each service has basic (happy path)
performance test scripts created (for example, using Gatling or JMeter) and also security tests
(for example, using bdd-security). These service-level component tests can then be run
continuously within the build pipeline, and will validate local microservice functional and
nonfunctional requirements. Additional internal resource API endpoints can be added to each
service, which can be used to manipulate internal state for test purposes or expose metrics.
(42)

Good APIs Assist Continuous Testing and Delivery
The benefits to the CD process of exposing application or service functionality via a well-defined
API include:
Easier automation of test fixture setup and teardown via internal resource endpoints (and
this limits or removes the need to manipulate state via file system or data store access).
Easier automation of specification tests (e.g., REST-assured). Triggering functionality
through a fragile UI is no longer required for every test.
API contracts can be validated automatically, potentially using techniques like consumer
contracts and consumer-driven contracts (e.g., Pact-JVM).
Dependent services that expose functionality through an API can be efficiently mocked
(e.g., WireMock), stubbed (e.g., stubby4j), or virtualized (e.g., Hoverfly).
Easier access to metrics and monitoring data via internal resource endpoints (e.g.,
Codahale Metrics or Spring Boot Actuator).
The popularity of APIs has increased exponentially over the past decade, but with good reason,
and embracing good architectural practices around this clearly makes implementing continuous
delivery much easier.
(43)

Deployment Platforms and Architecture
Fifteen years ago the deployment options for enterprise Java applications were relatively limited,
consisting of mostly heavyweight application servers that attempted to provide cross-cutting
platform concerns such as application lifecycle management, configuration, logging, and
transaction management. With the emergence of cloud computing from Amazon Web Services
(AWS), Google Cloud Platform (GCP) and Microsoft Azure; Platform-as-a-Service (PaaS), such
as Heroku, Google App Engine and Cloud Foundry; and Container-as-a-Service (CaaS) offerings
like Kubernetes, Mesos, Docker Swarm, there is now a lot more choice for Java developers. As
the underlying deployment fabrics and platforms have changed, so to have the associated
architectural best practices.
(44)

Designing Cloud-Native “Twelve
Factor" Applications
In early 2012, Platform-as-a-Service (PaaS) pioneer Heroku developed the "Twelve-Factor
App“, a series of rules and guidance for helping developers build cloud-ready PaaS applications
that:
Use declarative formats for setup automation, to minimize time and cost for new
developers joining the project
Have a clean contract with the underlying operating system, offering maximum portability
between execution environments
Are suitable for deployment on modern cloud platforms, minimizing the need for servers
and systems administration
Minimize divergence between development and production, enabling continuous
deployment for maximum agility
Can scale up without significant changes to tooling, architecture, or development practices
Let’s looks briefly at each of the factors now, and see how they map to continuously deploying
Java applications:
1. Codebase: One codebase tracked in revision control, many deploys
Each Java application (or service) should be tracked in a single, shared code repository, and
deployment configuration files, such as scripts, Dockerfiles and Jenkinsfiles, should be stored
alongside the application code.
2. Dependencies: Explicitly declare and isolate dependencies
Dependencies are commonly managed within Java applications using build tooling such as
Maven or Gradle, and OS-level dependencies should be clearly specified in the associated VM
image manifest, Dockerfile or serverless configuration files.
3. Config: Store config in the environment
The Twelve-Factor App guidelines suggest that configuration data should be injected into an
application via environment variables, although in practice many Java developers prefer to use
configuration files, and there can be security issues with exposing secrets via environment
variables. Storing non sensitive configuration data in a remote service like Spring Cloud Config
(backed by Git or Consul) and secrets in a service like HashiCorp’s Vault can be a good
compromise. 
4. Backing services: Treat backing services as attached resources (typically consumed over
the network)
Java developers are accustomed to treating data stores and middleware in this fashion, and in-
memory substitutes (e.g., HSQLDB, Apache Qpid, and Stubbed Cassandra) or service
virtualization (e.g., Hoverfly, and Wiremock) can be used for in-process component testing
within the build pipeline.
(45)

5. Build, release, run: Strictly separate build and run stages
For a compiled language such as Java, this guideline comes as no surprise (and with little choice
of implementation!). It is worth mentioning that the flexibility provided by VM and container
technology means that separate artifacts can be used to build, test, and run the application, each
configured as appropriate. For example, a deployment artifact can be created for build and test
with a full OS, JDK, and diagnostic tools; and an artifact can be built for running an application
in production with only a minimal OS and JRE. However, some may see this as an anti‐pattern,
as there should only be one “source of truth” artifact that is created, tested, and deployed within
the pipeline, and using multiple artifacts can lead to an impedance mismatch and configuration
drift between images.
6. Processes: Execute the app as one or more stateless processes
Building and running a Java application as a series of microservices can be made easier by using
VM images, container images or serverless functions (the concept of microservice is explained
later in this chapter).
7. Port binding: Export services via port binding
Java developers are used to exposing application services via ports e.g., running an application
on Jetty or Apache Tomcat.
8. Concurrency: Scale out via the process model
Traditional Java applications typically take the opposite approach to scaling, as the JVM runs as
a giant “uberprocess” that is often vertically scaled by adding more heap memory, or horizontally
scaled by cloning and load-balancing across multiple running instances. However, the
combination of decomposing Java applications into microservices and running these components
within VM, containers or serverless runtimes can enable this approach to scalability. Regardless
of the approach taken to implement scalability, this should be tested within the build pipeline.
9. Disposability: Maximize robustness with fast startup and graceful shutdown
This can require a mindset shift with developers who are used to creating a traditional long-
running Java application, where much of the expense of application configuration and
initialization was front-loaded in the JVM/application startup process. Modern, container-ready
applications should utilize more just-in-time (JIT) configuration, and ensure that best efforts are
taken to clean up resource and state during shutdown. 
10. Dev/prod parity: Keep development, staging, and production as similar as possible
The use of VM or container technology in combination with orchestration technologies like
VMware, Kubernetes, and Mesos can make this easier in comparison with traditional bare metal
deployments where the underlying hardware and OS configuration is often significantly different
than developer or test machines. As an application artifact moves through the build pipeline, it
should be exposed to more and more realistic environments e.g., unit testing can run in-memory
on a build box. However, end-to-end testing should be conducted in a production-like
environment.
11. Logs: Treat logs as event streams
(46)

Java has had a long and sometimes arduous relationship with logging frameworks, but modern
frameworks like Logback and Log4j 2 can be configured to stream to standard output or
streamed to disk.
12. Admin processes: Run admin/management tasks as one-off processes
The ability to create simple Java applications that can be run within a container or as a serverless
function allows administrative tasks to be run as one-off processes. However, these processes
must be tested within (or as part of) the build pipeline.
Exploring the “Fifteen Factor App”
For developers looking to develop a deeper understanding of architectural principles like those
mentioned here, I strongly recommend reading Kevin Hoffman’s Beyond the Twelve-Factor App
(O’Reilly).
The principles of the Twelve Factor App have hinted at designing systems that not only embrace
the properties of the underlying deployment fabric, but also actively exploit it. Closely related is
a topic known as “mechanical sympathy, and I’ll now explore this in more depth.
(47)

Cultivating “Mechanical Sympathy”
Martin Thompson and Dave Farley have talked about the concept of mechanical sympathy in
software development for several years. They were inspired by the Formula One racing driver
Jackie Stewart’s famous quote “You don’t have to be an engineer to be a racing driver, but you
do have to have mechanical sympathy”, meaning that understanding how a car works will make
you a better driver; and it has been argued that this is analogous to programmers understanding
how computer hardware works. You don’t necessarily need a degree in computer science or to be
a hardware engineer, but you do need to understand how hardware works and take that into
consideration when you design software. The days of architects sitting in ivory towers and
drawing UML diagrams is over. Architects and developers must continue to develop practical
and operational experience from working with the new technologies.
Using PaaS, CaaS and functions can fundamentally change the way your software interacts with
the hardware it is running on. In fact, many modern PaaS and function-based solutions use
container technology behind the scene in order to provide process isolation, and it is beneficial to
be aware of these changes:
Container technology can limit access to system resources, due to developer/operator
specification, or to resource contention.
In particular, watch out for the restriction of memory available to a JVM, and
remember that Java application memory requirements are not simply equal to heap
size. In reality, Java applications’ memory requirements include the sum of Xmx
heap size, PermGen/Metaspace, native memory thread requirements, and JVM
overhead.
Another source of potential issues is that containers typically share a single source of
entropy (/dev/random) on the host machine, and this can be quickly exhausted. This
manifests itself with Java applications unexpectedly stalling/ blocking during
cryptographic operations such as token generation on the initialization of security
functionality. It is often beneficial to use the JVM option -
Djava.security.egd=file:/dev/urandom, but be aware that this can have some security
implications.
Container technology can (incidentally) expose incorrect resource availability to the JVM
(e.g., the number of processor cores typically exposed to a JVM application is based on the
underlying host hardware properties, not the restrictions applied to a running container)
When running containerized deployment fabric, it is often the case that additional layers of
abstraction are applied over the operating system (e.g., orchestration framework, container
technology itself, and an additional OS).
Container orchestration and scheduling frameworks often stop, start, and move containers
(and applications) much more often compared to traditional deployment platforms.
The hardware fabric upon which containerized applications are run is typically more
ephemeral in nature (e.g., cloud computing).
Containerized applications can expose new security attack vectors that must be understood
and mitigated.
These changes to the properties of the deployment fabric should not be a surprise to developers,
as the use of many new technologies introduce some form of change (e.g., upgrading the JVM
version on which an application is running, deploying Java applications within an application
container, and running Java applications in the cloud). The vast majority of these potential issues
(48)

can be mitigated by augmenting the testing processes within the CD build pipeline.
(49)

Design and Continually Test for Failure
Cloud computing has provided amazing opportunities for developers -- a decade ago I could only
dream of the hardware I can now spin up at the touch of a button -- but the nature of this type of
infrastructure has also introduced new challenges. Due to the networked implementation,
commodity costing, and scale of modern cloud computing, performance issues and failures
within the platform are inevitable. The vast majority of interaction operation within a cloud-
based platform are going over the wire. For example, elastic block storage which can appear
local is typically provided by a SAN, and the performance characteristics are considerably
different. If you develop an application on your local development machine that consists of three
chatty services with intensive access to a database you can be sure that the network performance
of the localhost loopback adapter and direct access to an SSD-based block store will be markedly
different than the corresponding cloud operations. This can make or break a project. 
Watch for the Effects of “Burstable” Infrastructure
Many cloud vendors provide options to choose and provision infrastructure that has low-levels of
baseline CPU, network, and disk performance, but has “credits” to allow time-limited burstable
performance that is considerably over the baseline. An initial allowance of credits is given, and
credits can be accumulated when the infrastructure is being used below the baseline. This allows
for the creation of very cost-effective systems - the starting credit balance allows the
infrastructure to initialise rapidly, and if the system usage patterns truly are burstable then a
healthy level of credit can be maintained to burst as required. However, if the usage patterns of
an application is not burstable then the credit balance can soon become depleted, and the
corresponding application’s performance will drop, or potentially the application may fail.
Short development and test runs of an application deployed on to this type of infrastructure can
lead to the development team believing that longer-term performance will be better than it
actually is. Accordingly, the baseline performance of such infrastructure must be understood, and
simulations created within a build pipeline that test for system performance when running at the
baseline infrastructure performance.
Most cloud computing infrastructure is ephemeral in nature, and you are also exposed to failure
with much more regularity in comparison with on-premise hardware. Combine this with the fact
that many of us are designing inherently distributed systems, you must design systems that
tolerate services disappearing or being redeployed. When many developers think of testing this
type of failure the Netflix Simian Army and Chaos Monkeys jump to mind, however this type of
testing is typically conducted within production. When you are developing a CD build pipeline
you need to also implement a limited (but equally) valuable form of this type of chaos testing,
but provided in a more controlled and deterministic fashion.
Systems that are designed with loose coupling are typically easier to test, as you can isolate
components more readily, and high cohesion helps with the mental effort needed in order to
understand what is happening when fixing bugs. The key takeaway from this section of the book
is that a continuous delivery pipeline must allow deployment and testing on a realistic
production-like environment as soon as possible, and performance and failure scenarios must be
simulated and tested.
(50)

The Move Towards Small Services
It would appear that every-other software development article published today mentions
“microservices”, and so much so, that it is often difficult to remember that other architectural
styles do exist! Behind the popularity of this architecture there is, of course, many benefits of
decomposing large and complex applications into smaller interconnected services. However,
there are also several challenges.
(51)

Challenges for Delivering Monolithic Applications
Despite what the software development press may say, there is nothing inherently wrong with
designing and building a monolithic application. It is simply an architectural style, and as with
any architectural approach, there are tradeoffs. The increase in adoption and rise in popularity of
building service-based applications is primarily due to three constraints imposed with working
on a single monolithic application:
Scaling development on the codebase and system.
Scaling sub-systems within the application (independently, elastically and on-demand).
Isolating sub-systems for independent deployability.
Let’s now examine each of these issues in turn, and discuss how this impacts the implementation
of continuous delivery.
Scaling Development
When working with a monolithic application all of the developers have to “crowd around” the
same codebase. This can lead to developers having to develop an understanding of the entire
codebase in order to cultivate the appropriate domain context, and during implementation code
merge conflicts are almost inevitable, which leads to rework and lost time. If a monolithic
codebase is designed and implemented well -- for example embracing the principles of high
cohesion, loose coupling and modularity --- then this shouldn’t be a problem. However, the
reality with long-running systems is that they are incrementally evolved, and either by accident
or on-purpose, the modularity breaks down over time. Extracting modules from a monolithic
codebase and building these as independent sub-system services can lead to clearer domain
boundaries and interfaces, which in turn facilitates the ability of developers to understand the
context. The independent nature of these services also facilitates the distribution of labour over
the codebase.
Sub-system Scalability and Elasticity
An application that is run as a single process (or group of processes) has limited options for
scaling. Typically the only approach is to replicate the entire runnable application instance and
load-balance requests across the multiple instances. If you design an application as a series of
cohesive sub-systems that are loosely coupled, then you have many more options for scaling. A
sub-system that is under high-load can be scaled independently from the rest of the application.
Differing Change Cadence: Independent Deployability
An application that is designed as a single artifact has limited options for independent
deployability, and this can be a problem if functionality within the application requires differing
change cadence. At a basic level, every time a new piece of functionality is developed within the
codebase the entire application must be deployed. If releasing the application is resource-
intensive then on-demand resources may not be practical. Worse still is if the application is
highly coupled, as this means a change in a supposed isolated area of the codebase will require
intensive testing to ensure that no hidden dependencies have caused regressions. By dividing the
(52)

codebase into independently deployable modules or services you can schedule the release of
functionality independently.
(53)

Microservices: SOA Meets Domain-Driven Design
There are clear benefits of building small services that followed the Unix “single responsibility
principle”. If I design services and tools that do one thing and do it well, it is easy to compose
these systems to provide more complicated functionality, and it is also easier to deploy and
maintain such systems. Large organisations like Netflix, eBay and Amazon have also talked
publicly about how they are building smaller service-based architectures. The topic of Domain-
Driven Design (DDD) is frequently mentioned alongside discussions of microservices, and
although the founding work by Eric Evans in this space was published in 2003 "Domain-Driven
Design: Tackling Complexity in the Heart of Software“, the technique only gained traction when
supporting technologies and methodologies converged: i.e. the combination of the evolution of
architectural practices, the emergence of cloud platforms that allowed dynamic provisioning and
configuration, and the rise of the DevOps movement that encourage more collaboration
throughout the build and operation of software.
Exploring Microservices and DDD Further
The term "microservices" first emerged during a talk by James Lewis in 2012, "Micro Services:
Java the Unix Way“, and was also talked about by Fred George and Martin Fowler around a
similar time. Due to the scope of this book, I won’t cover the concept of microservices or DDD
in much depth; instead, an introduction to the topic can be found in Christian
Posta’s Microservices for Java Developers (O’Reilly), and a more thorough treatment can be
found in Sam Newman’s Building Microservices (O’Reilly) and Amundsen et al’s Microservice
Architecture (O’Reilly). A core concept of microservices revolves around creating services that
follow the single-responsibility principle and have one reason to change. This is closely related
to designing effective domain models, or “bounded contexts”, within DDD.
Building Java-based microservices impacts the implementation of CD in several ways:
Multiple build pipelines (or branches within a single pipeline) must be created and
managed.
Deployment of multiple services to an environment now have to be orchestrated, managed,
and tracked.
Component testing may now have to mock, stub, or virtualize dependent services.
End-to-end testing must now orchestrate multiple services (and associated state) before
and after executing tests.
Process must be implemented to manage service version control (e.g., the enforcement of
only allowing the deployment of compatible interdependent services).
Monitoring, metrics, and application performance management (APM) tooling must be
adapted to handle multiple services.
Decomposing an existing monolithic application, or creating a new application that provides
functionality through a composite of microservices is a non-trivial task. Techniques such as
context mapping, from Domain-Driven Design, can help developers (working alongside
stakeholders and the QA team) understand how application/business functionality should be
composed as a series of bounded contexts or focused services.
(54)

Functions, Lambdas, and Nanoservices
As stated by Mike Roberts on the Martin Fowler blog, there is no one clear view of what
“Serverless" is, and this is not helped by people talking about it in regards to two different but
overlapping areas:
Serverless was first used to describe applications that significantly or fully depend on 3rd
party applications or cloud services to manage server-side logic and state. These are
typically thick client applications (think single page web apps, or mobile apps) that use the
vast ecosystem of cloud accessible databases (like Parse, Firebase), authentication services
(Auth0, AWS Cognito), etc. These types of services have been previously described as
“Backend as a Service (BaaS)”.
Serverless can also mean applications where some amount of server-side logic is still
written by the application developer but unlike traditional architectures is run in stateless
compute containers that are event-triggered, ephemeral (may only last for one invocation),
and fully managed by a 3rd party. One way to think of this is “Functions as a service
(FaaS)" . 
In this book I will focus on the second type of serverless applications. The challenges of
continuously delivering serverless and FaaS application are much the same as with
microservices, although the added challenge of not getting access to the underlying platform can
provide additional challenges when testing nonfunctional requirements.
(55)

Architecture: “The Stuff That’s Hard to
Change”
Fundamentally architecture can be though of as the “stuff that is hard to change”. Getting a
software system’s architecture correct is a key enabler to facilitating continuous delivery.
Following the key principles of designing systems with loose coupling and high cohesion
facilitates testing and continuous deployment by allowing services to easily be understood and to
be worked with and validated in isolation before being assembled as part of the large systems.
Designing APIs outside-in (and with supporting internal APIs) also facilitate continuous testing
of functional and non functional requirements. Developers must now be aware of cloud, PaaS
and container runtimes, and the impact this have on continuous delivery - it can be a fundamental
benefit, allowing the dynamic provisioning of resources for testing; but it also changes the
characteristics of the underlying infrastructure fabric, and this must be continually tested, and
assumptions validated.
(56)

Summary
In this chapter you have learned about the considerable effect that architecture has on your ability
to continuously deliver a software system:
The fundamentals of creating an effective and maintainable architecture consist of
designing systems that have high cohesion and are loosely coupled.
High cohesion and loose coupling effect the entire CD process: at design time a cohesive
system is easier to reason about; when testing, a loosely coupled system allows the easy
substitution of mocks to isolate the functionality being verified; modules or services within
a loosely coupled system can be deployed in isolation; and a cohesive system is generally a
more observable and understandable system.
Bad or casually designed architecture limits both technical and business velocity, and will
reduce the effectiveness of a CD pipeline.
Designing effective APIs, which are built “outside-in”, assist with effective testing and CD
as they provide an interface for automation.
The architectural principles captured within Heroku’s “Twelve Factor Application” assist
with implementing systems that can be continuously delivered.
Cultivating mechanical sympathy, i.e. learning about the application platform and
deployment fabric, alongside designing for failure, are essential skills for a modern Java
developer.
There is a trend within software development to design systems consisting of small and
independently deployable (micro)services. Due to the high cohesiveness and loose
coupling, these systems lend themselves to being continuously delivered. These systems
also require continuous delivery to ensure both functional and nonfunctional system-level
requirements are met, and to avoid an explosion of complexity
Architecture is “the stuff that is hard to change”. Continuous delivery allows you to codify,
test and monitor core system quality attributes throughout the lifetime of a software
systems.
With a good understanding of the principles of architecture developed, in the next chapter you
will learn about how to effectively build and test Java applications that embody these properties.
(57)


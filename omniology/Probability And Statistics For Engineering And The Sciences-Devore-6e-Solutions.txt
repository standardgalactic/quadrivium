Chapter 1:  Overview and Descriptive Statistics 
CHAPTER 1 
 
Section 1.1 
 
1. 
 
a. Houston Chronicle, Des Moines Register, Chicago Tribune, Washington Post 
 
b. Capital One, Campbell Soup, Merrill Lynch, Pulitzer 
 
c. 
Bill Jasper, Kay Reinke, Helen Ford, David Menedez 
 
d. 1.78, 2.44, 3.5, 3.04 
 
 
2. 
 
a. 29.1 yd., 28.3 yd., 24.7 yd., 31.0 yd. 
 
b. 432, 196, 184, 321 
 
c. 
2.1, 4.0, 3.2, 6.3 
 
d. 0.07 g, 1.58 g, 7.1 g, 27.2 g 
 
 
3. 
 
a. In a sample of 100 VCRs, what are the chances that more than 20 need service while 
under warrantee?  What are the chances than none need service while still under 
warrantee? 
 
b. What proportion of all VCRs of this brand and model will need service within the 
warrantee period? 
 
 
 
1

Chapter 1:  Overview and Descriptive Statistics 
4. 
 
a. Concrete: All living U.S. Citizens, all mutual funds marketed in the U.S., all books 
published in 1980. 
 
Hypothetical:  All grade point averages for University of California undergraduates 
during the next academic year.  Page lengths for all books published during the next 
calendar year.  Batting averages for all major league players during the next baseball 
season. 
b. Concrete: Probability: In a sample of 5 mutual funds, what is the chance that all 5 have 
rates of return which exceeded 10% last year? 
Statistics:  
If previous year rates-of-return for 5 mutual funds were 9.6, 14.5, 8.3, 9.9 
and 10.2, can we conclude that the average rate for all funds was below 10%? 
Conceptual: Probability: In a sample of 10 books to be published next year, how likely is 
it that the average number of pages for the 10 is between 200 and 250? 
Statistics: 
If the sample average number of pages for 10 books is 227, can we be 
highly confident that the average for all books is between 200 and 245? 
 
 
5. 
 
a. No, the relevant conceptual population is all scores of all students who participate in the 
SI in conjunction with this particular statistics course. 
 
b. The advantage to randomly choosing students to participate in the two groups is that we 
are more likely to get a sample representative of the population at large.  If it were left to 
students to choose, there may be a division of abilities in the two groups which could 
unnecessarily affect the outcome of the experiment. 
 
c. 
If all students were put in the treatment group there would be no results with which to 
compare the treatments. 
 
 
6. 
One could take a simple random sample of students from all students in the California State 
University system and ask each student in the sample to report the distance form their 
hometown to campus.  Alternatively, the sample could be generated by taking a stratified 
random sample by taking a simple random sample from each of the 23 campuses and again 
asking each student in the sample to report the distance from their hometown to campus.  
Certain problems might arise with self reporting of distances, such as recording error or poor 
recall.  This study is enumerative because there exists a finite, identifiable population of 
objects from which to sample. 
 
 
7. 
One could generate a simple random sample of all single family homes in the city or a 
stratified random sample by taking a simple random sample from each of the 10 district 
neighborhoods.  From each of the homes in the sample the necessary variables would be 
collected.  This would be an enumerative study because there exists a finite, identifiable 
population of objects from which to sample. 
 
 
2

Chapter 1:  Overview and Descriptive Statistics 
8. 
 
a. Number observations equal 2 x 2 x 2 = 8 
 
b. This could be called an analytic study because the data would be collected on an existing 
process. There is no sampling frame. 
 
 
9. 
 
a. There could be several explanations for the variability of the measurements.  Among 
them could be measuring error, (due to mechanical or technical changes across 
measurements), recording error, differences in weather conditions at time of 
measurements, etc. 
 
b. This could be called an analytic study because there is no sampling frame. 
 
 
Section 1.2 
 
10. 
 
a. Minitab generates the following stem-and-leaf display of this data: 
 
 
 
 
 
 
5 9 
6 33588 
7 00234677889 
8 127 
9 077 
stem: ones 
10 7 
leaf: tenths 
11 368 
 
 
 
What constitutes large or small variation usually depends on the application at hand, but 
an often-used rule of thumb is: the variation tends to be large whenever the spread of the 
data (the difference between the largest and smallest observations) is large compared to a 
representative value. Here, 'large' means that the percentage is closer to 100% than it is to 
0%.  For this data, the spread is 11 - 5 = 6, which constitutes 6/8 = .75, or, 75%, of the 
typical data value of 8.  Most researchers would call this a large amount of variation. 
 
b. The data display is not perfectly symmetric around some middle/representative value.  
There tends to be some positive skewness in this data. 
 
c. 
In Chapter 1, outliers are data points that appear to be very different from the pack.  
Looking at the stem-and-leaf display in part (a), there appear to be no outliers in this data.  
(Chapter 2 gives a more precise definition of what constitutes an outlier). 
 
d. From the stem-and-leaf display in part (a), there are 4 values greater than 10.  Therefore, 
the proportion of data values that exceed 10 is 4/27 = .148, or, about 15%. 
 
 
3

Chapter 1:  Overview and Descriptive Statistics 
11. 
 
6l 034 
6h 667899 
7l 00122244 
7h  
Stem=Tens 
8l 001111122344 
Leaf=Ones 
8h 5557899 
9l 03 
9h 58 
 
This display brings out the gap in the data:   
There are no scores in the high 70's. 
 
 
12. 
One method of denoting the pairs of stems having equal values is to denote the first stem by 
L, for 'low', and the second stem by H, for 'high'.  Using this notation, the stem-and-leaf 
display would appear as follows: 
 
3L 1 
3H 56678 
4L 000112222234 
4H 5667888 
5L 144 
5H 58 
stem: tenths 
6L 2 
leaf: hundredths 
6H 6678 
7L 
7H 5 
 
The stem-and-leaf display on the previous page shows that .45 is a good representative value 
for the data.  In addition, the display is not symmetric and appears to be positively skewed.  
The spread of the data is .75 - .31 = .44, which is.44/.45 = .978, or about 98% of the typical 
value of .45. This constitutes a reasonably large amount of variation in the data.  The data 
value .75 is a possible outlier  
 
 
 
4

Chapter 1:  Overview and Descriptive Statistics 
13. 
 
a.  
 
 
 
 
12 
2 
Leaf  = ones 
12 445 
Stem = tens 
 
12 
6667777 
 
 
12 
889999 
 
 
13 
00011111111 
 
 
13 
2222222222333333333333333
 
 
13 
44444444444444444455555555555555555555 
13 
6666666666667777777777 
 
 
13 
888888888888999999 
 
 
14 
0000001111 
 
 
14 
2333333 
 
 
14 
444 
 
 
14 
77 
 
 
 
The observations are highly concentrated at 134 – 135, where the display suggests the 
typical value falls. 
 
b.  
 
122 124 126 128 130 132 134 136 138 140 142 144 146 148
0
10
20
30
40
strength
Frequency
 
 
The histogram is symmetric and unimodal, with the point of symmetry at approximately 
135. 
 
 
5

Chapter 1:  Overview and Descriptive Statistics 
14. 
 
a.  
2 
23 
 
stem units: 1.0 
3 
2344567789 
 
leaf units: .10 
4 
01356889 
 
 
5 
00001114455666789 
 
6 
0000122223344456667789999 
7 
00012233455555668 
 
8 
02233448 
 
 
9 
012233335666788 
 
10 
2344455688 
 
 
11 
2335999 
 
 
12 
37 
 
 
13 
8 
 
 
14 
36 
 
 
15 
0035 
 
 
16 
 
 
 
17 
 
 
 
18 
9 
 
 
 
   
b. A representative value could be the median, 7.0. 
 
c. 
The data appear to be highly concentrated, except for a few values on the positive side. 
 
d. No, the data is skewed to the right, or positively skewed. 
 
e. 
The value 18.9 appears to be an outlier, being more than two stem units from the previous 
value. 
 
 
15. 
 
Crunchy 
 
Creamy 
 2 2 
644 3 69 
77220 4 145 
6320 5 3666 
222 6 258 
55 7  
0 8  
 
Both sets of scores are reasonably spread out.  There appear to be no 
outliers.  The three highest scores are for the crunchy peanut butter, the 
three lowest for the creamy peanut butter. 
 
6

Chapter 1:  Overview and Descriptive Statistics 
16. 
 
a.  
beams 
 
cylinders 
9
5 
8 
88533
6 
16 
98877643200
7 
012488 
721
8 
13359 
770
9 
278 
7
10  
863
11 2 
12 6 
13  
14 1 
 
The data appears to be slightly skewed to the right, or positively skewed.  The value of 
14.1 appears to be an outlier.  Three out of the twenty, 3/20 or .15 of the observations 
exceed 10 Mpa. 
 
b. The majority of observations are between 5 and 9 Mpa  for both beams and cylinders, 
with the modal class in the 7 Mpa range.  The observations for cylinders are more 
variable, or spread out, and the maximum value of  the cylinder observations is higher. 
 
c. 
Dot Plot 
 
    . .  .  :..  : .: . . .   :         .        .         . 
          -+---------+---------+---------+---------+---------+-----
cylinder 
         6.0       7.5       9.0      10.5      12.0      13.5 
 
 
17. 
 
a.  
 
Number 
 
Nonconforming 
Frequency 
 RelativeFrequency(Freq/60)
  
0 
7 
      0.117 
 
1 
12 
      0.200 
 
2 
13 
      0.217 
 
3 
14 
      0.233 
 
4 
6 
      0.100 
 
5 
3 
      0.050 
 
6 
3 
      0.050 
 
7 
1 
      0.017 
 
8 
1 
      0.017
 
                doesn't add exactly to 1 because relative frequencies have been rounded 1.001 
 
b. The number of batches with at most 5 nonconforming items is 7+12+13+14+6+3 = 55, 
which is a proportion of 55/60 = .917.   The proportion of batches with (strictly) fewer 
than 5 nonconforming items is 52/60 = .867.  Notice that these proportions could also 
have been computed by using the relative frequencies: e.g., proportion of batches with 5 
or fewer nonconforming items = 1- (.05+.017+.017) = .916; proportion of batches with 
fewer than 5 nonconforming items = 1 - (.05+.05+.017+.017) = .866.  
 
7

Chapter 1:  Overview and Descriptive Statistics 
 
c. 
The following is a Minitab histogram of this data.  The center of the histogram is 
somewhere around 2 or 3 and it shows that there is some positive skewness in the data.  
Using the rule of thumb in Exercise 1, the histogram also shows that there is a lot of 
spread/variation in this data.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
8
7
6
5
4
3
2
1
0
.20
.10
.00
Number
Relative
Frequency
18. 
 
a.  
The following histogram was constructed using Minitab: 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
The most interesting feature of the histogram is the heavy positive skewness of the data. 
Note: One way to have Minitab automatically construct a histogram from grouped data 
such as this is to use Minitab's ability to enter multiple copies of the same number by 
typing, for example, 784(1) to enter 784 copies of the number 1.  The frequency data in 
this exercise was entered using the following Minitab commands: 
MTB > set c1 
DATA> 784(1) 204(2) 127(3) 50(4) 33(5) 28(6) 19(7) 19(8) 
DATA> 6(9) 7(10) 6(11) 7(12) 4(13) 4(14) 5(15) 3(16) 3(17) 
DATA> end  
18
16
14
12
10
8
6
4
2
0
800
700
600
500
400
300
200
100
0
Number of papers
Frequency
 
8

Chapter 1:  Overview and Descriptive Statistics 
 
b. From the frequency distribution (or from the histogram), the number of authors who 
published at least 5 papers is 33+28+19+…+5+3+3 = 144, so the proportion who 
published 5 or more papers is 144/1309 = .11, or 11%.  Similarly, by adding frequencies 
and dividing by n = 1309, the proportion who published 10 or more papers is 39/1309 =  
.0298, or about 3%.  The proportion who published more than 10 papers (i.e., 11 or more) 
is 32/1309 = .0245, or about 2.5%. 
 
c. 
No.  Strictly speaking, the class described by ' ≥15 ' has no upper boundary, so it is 
impossible to draw a rectangle above it having finite area (i.e., frequency). 
 
d. The category 15-17 does have a finite width of 2, so  the cumulated frequency of 11 can 
be plotted as a rectangle of height 6.5 over this interval.  The basic rule is to make the 
area of the bar equal to the class frequency, so area  = 11 = (width)(height) = 2(height) 
yields a height of 6.5. 
 
 
19. 
 
a. From this frequency distribution, the proportion of wafers that contained at least one 
particle is (100-1)/100 = .99, or 99%.  Note that it is much easier to subtract 1 (which is 
the number of wafers that contain 0 particles) from 100 than it would be to add all the 
frequencies for 1, 2, 3,… particles.  In a similar fashion, the proportion containing at least 
5 particles is (100 - 1-2-3-12-11)/100 = 71/100 = .71, or, 71%. 
 
b. The proportion containing between 5 and 10 particles is (15+18+10+12+4+5)/100 = 
64/100 = .64, or 64%.  The proportion that contain strictly between 5 and 10 (meaning 
strictly more than 5 and strictly less than 10) is (18+10+12+4)/100 = 44/100 = .44, or 
44%. 
 
c. 
The following histogram was constructed using Minitab.  The data was entered using the 
same technique mentioned in the answer to exercise 8(a).  The histogram is almost 
symmetric and unimodal; however, it has a few relative maxima (i.e., modes) and has a 
very slight positive skew.  
 
 
 
 
 
 
 
 
 
 
15
10
5
0
.20
.10
.00
Number of particles
Relative frequency
 
9

Chapter 1:  Overview and Descriptive Statistics 
 
20. 
 
a. The following stem-and-leaf display was constructed: 
 
0 123334555599 
 
1 00122234688 
stem: thousands 
2 1112344477 
leaf: hundreds 
 
3 0113338 
 
4 37 
 
5 23778 
 
 
A typical data value is somewhere in the low 2000's.  The display is almost unimodal (the 
stem at 5 would be considered a mode, the stem at 0 another) and has a positive skew. 
 
b. A histogram of this data, using classes of width 1000 centered at 0, 1000, 2000,  6000 is 
shown below.  The proportion of subdivisions with total length less than 2000 is 
(12+11)/47 = .489, or 48.9%.  Between 200 and 4000, the proportion is (7 + 2)/47 = .191, 
or 19.1%.   The histogram shows the same general shape as depicted by the stem-and-leaf 
in part (a). 
 
 
 
 
6000
5000
4000
3000
2000
1000
0
10
5
0
length
Frequency
 
10

Chapter 1:  Overview and Descriptive Statistics 
21. 
 
a. A histogram of the y data appears below.  From this histogram, the number of 
subdivisions having no cul-de-sacs (i.e., y = 0) is 17/47 = .362, or 36.2%.  The proportion 
having at least one cul-de-sac (y ≥ 1) is  (47-17)/47 = 30/47 = .638, or 63.8%.  Note that 
subtracting the number of cul-de-sacs with y = 0 from the total, 47, is an easy way to find 
the number of subdivisions with y ≥ 1. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
5
4
3
2
1
0
20
10
0
y
Frequency
b. A histogram of the z data appears below.  From this histogram, the number of 
subdivisions with at most 5 intersections (i.e., z ≤ 5) is 42/47 = .894, or 89.4%.  The 
proportion having fewer than 5 intersections (z < 5) is  39/47 = .830, or 83.0%. 
 
8
7
6
5
4
3
2
1
0
10
5
0
z
Frequency
 
11

Chapter 1:  Overview and Descriptive Statistics 
 
 
22. 
A very large percentage of the data values are greater than 0, which indicates that most, but 
not all, runners do slow down at the end of the race.   The histogram is also positively skewed, 
which means that some runners slow down a lot compared to the others.  A typical value for 
this data would be in the neighborhood of 200 seconds.  The proportion of the runners who 
ran the last 5 km faster than they did the first 5 km is very small, about 1% or so. 
 
 
 
23. 
 
a.  
 
 
 
 
900
800
700
600
500
400
300
200
100
0
30
20
10
0
brkstgth
Percent
 
The histogram is skewed right, with a majority of observations between 0 and 300 cycles.  
The class holding the most observations is between 100 and 200 cycles. 
 
12

Chapter 1:  Overview and Descriptive Statistics 
 
b.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
900
600
500
400
300
200
150
100
50
0
0.004
0.003
0.002
0.001
0.000
brkstgth
Density
c 
[proportion ≥ 100] = 1 – [proportion < 100] = 1 - .21 = .79 
 
 
24. 
 
 
 
 
 
 
6000
5800
5600
5400
5200
5000
4800
4600
4400
4200
4000
20
10
0
weldstrn
Percent
 
13

Chapter 1:  Overview and Descriptive Statistics 
25. 
Histogram of original data: 
 
 
 
 
 
Histogram of transformed data: 
 
The transformation creates a much more symmetric, mound-shaped histogram. 
 
80
70
60
50
40
30
20
10
15
10
5
0
IDT
Frequency
1.9
1.8
1.7
1.6
1.5
1.4
1.3
1.2
1.1
9
8
7
6
5
4
3
2
1
0
log(IDT)
Frequency
 
14

Chapter 1:  Overview and Descriptive Statistics 
26. 
 
a.  
 
 
Class Intervals 
Frequency 
Rel. Freq.
.15 -< .25 
8 
0.02192 
.25 -< .35 
14 
0.03836 
.35 -< .45 
28 
0.07671 
.45 -< .50 
24 
0.06575 
.50 -< .55 
39 
0.10685 
.55 -< .60 
51 
0.13973 
.60 -< .65 
106 
0.29041 
.65 -< .70 
84 
0.23014 
.70 -< .75 
11 
0.03014 
 
n=365 
1.00001 
 
 
0.75
0.70
0.65
0.60
0.55
0.50
0.45
0.35
0.25
0.15
6
5
4
3
2
1
0
clearness
Density
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
b. The proportion of days with a clearness index smaller than .35 is (
)
06
.
365
4
8
=
+
, or  6%. 
 
c. 
The proportion of days with a clearness index of at least .65 is (
)
26
.
365
11
84
=
+
, or 26%. 
 
 
15

Chapter 1:  Overview and Descriptive Statistics 
27. 
 
a. The endpoints of the class intervals overlap.  For example, the value 50 falls in both of the 
intervals ‘0 – 50’ and ’50 – 100’. 
 
b.  
Class Interval 
Frequency
Relative Frequency 
0 - <  50 
9 
0.18 
50 - < 100 
19 
0.38 
100 - < 150 
11 
0.22 
150 - < 200 
4 
0.08 
200 - < 250 
2 
0.04 
250 - < 300 
2 
0.04 
300 - < 350 
1 
0.02 
350 - < 400 
1 
0.02 
>= 400 
1 
0.02 
 
50 
1.00 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
The distribution is skewed to the right, or positively skewed.  There is a gap in the 
histogram, and what appears to be an outlier in the ‘500 – 550’ interval. 
600
550
500
450
400
350
300
250
200
150
100
50
0
20
10
0
lifetime
Frequency
 
 
16

Chapter 1:  Overview and Descriptive Statistics 
c. 
 
 
Class Interval 
Frequency
Relative Frequency 
2.25 - <  2.75 
2 
0.04 
2.75 - < 3.25 
2 
0.04 
3.25 - < 3.75 
3 
0.06 
3.75 - < 4.25 
8 
0.16 
4.25 - < 4.75 
18 
0.36 
4.75 - < 5.25 
10 
0.20 
5.25 - < 5.75 
4 
0.08 
5.75 - < 6.25 
3 
0.06 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
The distribution of the natural logs of the original data is much more symmetric than the 
original.   
6.25
5.75
5.25
4.75
4.25
3.75
3.25
2.75
2.25
20
10
0
ln lifetime
Frequency
 
d. The proportion of lifetime observations in this sample that are less than 100 is  .18 + .38 
= .56, and the proportion that is at least 200 is .04 + .04 + .02 + .02 + .02 = .14. 
 
 
28. 
There are seasonal trends with lows and highs 12 months apart. 
10
20
30
40
16
17
18
19
20
21
Index
radtn
 
 
 
 
 
17

Chapter 1:  Overview and Descriptive Statistics 
29. 
 
Complaint 
Frequency 
Relative Frequency 
B 
7 
0.1167 
C  
3 
0.0500 
F 
9 
0.1500 
J 
10 
0.1667 
M 
4 
0.0667 
N 
6 
0.1000 
O 
21 
0.3500 
 
60 
1.0000 
B
C
F
J
M
N
O
0
10
20
complaint
Count of complaint
 
30. 
 
 
1. 
incorrect component 
2. 
missing component 
3. 
  failed component 
4. 
insufficient solder 
5. 
excess solder 
 
 
1
2
3
4
5
0
100
200
prodprob
Count of prodprob
 
18

Chapter 1:  Overview and Descriptive Statistics 
31. 
 
 
 
 
Relative 
Cumulative Relative 
Class 
Frequency 
Frequency 
Frequency 
0.0 - under 4.0 
2 
2 
0.050 
4.0 - under 8.0 
14 
16 
0.400 
8.0 - under 12.0 
11 
27 
0.675 
12.0 - under 16.0 
8 
35 
0.875 
16.0 - under 20.0 
4 
39 
0.975 
20.0 - under 24.0 
0 
39 
0.975 
24.0 - under 28.0 
1 
40 
1.000 
 
 
 
32. 
 
a. The frequency distribution is: 
 
    
 
 
     
  
Relative  
 
 
      Relative 
  
 
    Class     
Frequency            
Class           Frequency
     0-< 150   
.193 
 
 
  900-<1050   .019 
 150-< 300    
.183 
 
 
1050-<1200   .029 
 300-< 450   
.251 
 
 
1200-<1350   .005 
 450-< 600   
.148 
 
 
1350-<1500   .004 
 600-< 750    
.097 
 
 
1500-<1650   .001 
 750-< 900   
.066 
 
 
1650-<1800   .002 
1800-<1950   .002 
 
 
 
The relative frequency distribution is almost unimodal and exhibits a large positive 
skew.  The typical middle value is somewhere between 400 and 450, although the 
skewness makes it difficult to pinpoint more exactly than this. 
 
b. The proportion of the fire loads less than 600 is .193+.183+.251+.148 = .775.  The 
proportion of loads that are at least 1200 is .005+.004+.001+.002+.002 = .014. 
 
c. 
The proportion of loads between 600 and 1200 is 1 - .775 - .014 = .211. 
 
 
 
 
19

Chapter 1:  Overview and Descriptive Statistics 
Section 1.3 
 
33. 
 
a. 
57
.
192
=
x
, 
189
~ =
x
. 
The mean is larger than the median, but they are still 
fairly close together.  
 
b. Changing the one value, 
71
.
189
=
x
, 
189
~ =
x
. 
The mean is lowered, the 
median stays the same. 
 
c. 
0.
191
=
tr
x
. 
07
.
14
1
=
 or 7% trimmed from each tail. 
 
d. For n = 13, Σx = (119.7692) x 13 = 1,557 
For n = 14, Σx = 1,557 + 159 = 1,716 
5714
.
122
14
1716 =
=
x
 or 122.6 
 
 
34. 
 
a. The sum of the n = 11 data points is 514.90, so x  = 514.90/11 = 46.81. 
 
b. The sample size (n = 11) is odd, so there will be a middle value.  Sorting from smallest to 
largest: 4.4   16.4   22.2   30.0   33.1   36.6   40.4   66.7   73.7   81.5   109.9.   The sixth 
value, 36.6 is the middle, or median, value.   The mean differs from the median because 
the largest sample observations are much further from the median than are the smallest 
values. 
 
c. 
Deleting the smallest (x = 4.4) and largest (x = 109.9) values, the sum of the remaining 9 
observations is 400.6.  The trimmed mean 
tr
x  is 400.6/9 = 44.51.  The trimming 
percentage is 100(1/11) ≈ 9.1%.  
tr
x  lies between the mean and median. 
 
35. 
 
a. The sample mean is x = (100.4/8) = 12.55. 
 
The sample size (n = 8) is even.  Therefore, the sample median is the average of the (n/2) 
and (n/2) + 1 values. By sorting the 8 values in order, from smallest to largest:  8.0   8.9   
11.0   12.0   13.0   14.5   15.0   18.0, the forth and fifth values are 12 and 13.  The sample 
median is (12.0 + 13.0)/2 = 12.5. 
 
The 12.5% trimmed mean requires that we first trim (.125)(n) or 1 value from the ends of 
the ordered data set.  Then we average the remaining 6 values. The 12.5% trimmed mean 
)
5.
12
(
tr
x
 is 74.4/6 = 12.4. 
 
All three measures of center are similar, indicating little skewness to the data set. 
 
b. The smallest value (8.0) could be increased to any number below 12.0 (a change of less 
than 4.0) without affecting the value of the sample median. 
 
20

Chapter 1:  Overview and Descriptive Statistics 
 
c. 
The values obtained in part (a) can be used directly.  For example, the sample mean of 
12.55 psi could be re-expressed as  
(12.55 psi) x
ksi
psi
ksi
70
.5
2.2
1
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
. 
 
36. 
 
a. A stem-and leaf display of this data appears below: 
 
32 55 
stem: ones 
33 49 
leaf: tenths 
34 
35 6699 
36 34469 
37 03345 
38 9 
39 2347 
40 23 
41 
42 4 
 
The display is reasonably symmetric, so the mean and median will be close. 
 
b. The sample mean is x  = 9638/26 = 370.7.  The sample median is  
x~   = (369+370)/2 = 369.50. 
 
c. 
The largest value (currently 424) could be increased by any amount.  Doing so will not 
change the fact that the middle two observations are 369 and 170, and hence, the median 
will not change.  However, the value x = 424 can not be changed to a number less than 
370 (a change of 424-370 = 54) since that will lower the values(s) of the two middle 
observations. 
 
d. Expressed in minutes, the mean is (370.7 sec)/(60 sec) = 6.18 min;  the median is 6.16 
min. 
 
 
37. 
01
.
12
=
x
, 
35
.
11
~ =
x
, 
46
.
11
)
10
(
=
tr
x
.  The median or the trimmed mean would be good 
choices because of the outlier 21.9. 
 
 
38. 
 
a. The reported values are (in increasing order) 110, 115, 120, 120, 125, 130, 130, 135, and 
140. Thus the median of the reported values is 125. 
 
b. 127.6 is reported as 130, so the median is now 130, a very substantial change. When there 
is rounding or grouping, the median can be highly sensitive to small change. 
 
 
21

Chapter 1:  Overview and Descriptive Statistics 
39. 
 
a. 
 so 
475
.
16
=
Σ lx
0297
.1
16
475
.
16
=
=
x
 
 
 
009
.1
2
)
011
.1
007
.1(
~
=
+
=
x
 
 
b. 1.394 can be decreased until it reaches 1.011(the largest of the 2 middle values) – i.e. by 
1.394 – 1.011 = .383,  If it is decreased by more than .383, the median will change. 
 
 
40. 
8.
60
~ =
x
 
3083
.
59
)
25
(
=
tr
x
 
3475
.
58
)
10
(
=
tr
x
 
54
.
58
=
x
 
All four measures of center have about the same value. 
 
 
41. 
 
a. 
70
.
10
7
=
 
 
b. 
70
.
=
x
= proportion of successes 
 
c. 
80
.
25 =
s
 so s = (0.80)(25) = 20 
total of 20 successes 
20 – 7 = 13 of the new cars would have to be successes 
 
 
42. 
 
a. 
c
x
n
nc
n
x
n
c
x
n
y
y
i
i
i
+
=
+
Σ
=
+
Σ
=
Σ
=
)
(
 
=
y~
the median of 
=
+
+
+
)
,...,
,
(
2
1
c
x
c
x
c
x
n
median of 
c
x
c
x
x
x
n
+
=
+
~
)
,...,
,
(
2
1
 
 
b. 
x
c
n
x
c
n
c
x
n
y
y
i
i
i
=
Σ
=
⋅
Σ
=
Σ
=
)
(
 
=
y~
)
,...,
,
(
2
1
n
cx
cx
cx
x
c
x
x
x
median
c
n
~
)
,...,
,
(
2
1
=
⋅
=
 
 
 
43. 
median = 
0.
68
2
)
79
57
(
=
+
, 20% trimmed mean = 66.2, 30% trimmed mean = 67.5. 
 
 
 
22

Chapter 1:  Overview and Descriptive Statistics 
Section 1.4 
 
44. 
 
a. range = 49.3 – 23.5 = 25.8 
 
b.  
 
 
 
 
    
ix
)
(
x
xi −
 
 
2)
(
x
xi −
 
 
 
2
ix
29.5 
-1.53
2.3409
870.25 
49.3 
18.27
333.7929
2430.49 
30.6 
-0.43
0.1849
936.36 
28.2 
-2.83
8.0089
795.24 
28.0 
-3.03
9.1809
784.00 
26.3 
-4.73
22.3729
691.69 
33.9 
2.87
8.2369
1149.21 
29.4 
-1.63
2.6569
864.36 
23.5 
-7.53
56.7009
552.25 
31.6 
0.57
0.3249
998.56 
3.
310
=
Σx
 
0
)
(
=
−
Σ
x
xi
 
801
.
443
)
(
2 =
−
Σ
x
xi
 
 
41
.
072
,
10
)
(
2 =
Σ
ix
 
03
.
31
=
x
 
 
3112
.
49
9
801
.
443
1
)
(
2
1
2
=
=
−
−
Σ
=
=
n
x
x
s
i
n
i
 
 
c. 
0222
.
7
2
=
=
s
s
 
 
d. 
3112
.
49
9
10
/
)
3.
310
(
41
.
072
,
10
1
/
)
(
2
2
2
2
=
−
=
−
Σ
−
Σ
=
n
n
x
x
s
 
 
 
 
45. 
 
a.  
x  =  ∑
i
i
n
x
1
 = 577.9/5 = 115.58.  Deviations from the mean:   
 
116.4 - 115.58 = .82, 115.9 - 115.58 = .32, 114.6 -115.58 = -.98,  
 
115.2 - 115.58 = -.38, and 115.8-115.58 = .22. 
 
b.        s2 = [(.82)2 + (.32)2 + (-.98)2 + (-.38)2 + (.22)2]/(5-1) = 1.928/4 =.482,  
 
so s = .694. 
 
c. 
  = 66,795.61,  so s
∑
i
ix
2
2  = 
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡
⎟
⎠
⎞
⎜
⎝
⎛
−
∑
∑
−
2
1
2
1
1
i
i
n
i
i
n
x
x
 =                      
[66,795.61 - (577.9)2 /5]/4 = 1.928/4 = .482. 
d. 
Subtracting 100 from all values gives 
58
.
15
=
x
, all deviations are the same as in 
part b, and the transformed variance is identical to that of part b. 
 
23

Chapter 1:  Overview and Descriptive Statistics 
46. 
 
a. 
x  =  ∑
i
i
n
x
1
 = 14438/5 = 2887.6.  The sorted data is: 2781   2856   2888   2900   3013, 
so the sample median is x~  = 2888. 
 
b. Subtracting a constant from each observation shifts the data, but does not change its 
sample variance (Exercise 16).  For example, by subtracting 2700 from each observation 
we get the values 81, 200, 313, 156, and 188, which are smaller (fewer digits) and easier 
to work with.  The sum of squares of this transformed data is 204210 and its sum is 938, 
so the computational formula for the variance gives s2  = [204210-(938)2/5]/(5-1) = 
7060.3. 
 
 
47. 
The sample mean, 
(
)
2.
116
162
,1
10
1
1
=
=
=
= ∑
x
x
n
x
i
. 
The sample standard deviation,  
(
)
(
)
75
.
25
9
10
162
,1
992
,
140
1
2
2
2
=
−
=
−
−
= ∑
∑
n
n
x
x
s
i
i
 
On average, we would expect a fracture strength of 116.2.  In general, the size of a typical 
deviation from the sample mean (116.2) is about 25.75.  Some observations may deviate from 
116.2 by more than this and some by less. 
 
 
48. 
Using the computational formula, s2 = 
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡
⎟
⎠
⎞
⎜
⎝
⎛
−
∑
∑
−
2
1
2
1
1
i
i
n
i
i
n
x
x
 =  
[3,587,566-(9638)2/26]/(26-1)  = 593.3415, so s = 24.36.   In general, the size of a typical 
deviation from the sample mean (370.7) is about 24.4.  Some observations may deviate from 
370.7 by a little more than this, some by less. 
 
 
49. 
 
a. 
, 
 
80
.
56
01
.3
...
75
.2
=
+
+
=
Σx
8040
.
197
)
01
.3
(
...
)
75
.2
(
2
2
2
=
+
+
=
Σx
 
b. 
,
5016
.
16
0252
.8
16
17
/
)
80
.
56
(
8040
.
197
2
2
=
=
−
=
s
 
708
.
=
s
 
 
 
24

Chapter 1:  Overview and Descriptive Statistics 
50. 
First, we need 
(
)
37
.
747
179
,
20
27
1
1
=
=
= ∑
ix
n
x
.  Then we need the sample standard 
deviation 
(
)
89
.
606
26
27
179
,
20
511
,
657
,
24
2
=
−
=
s
.  The maximum award should be 
16
.
1961
)
89
.
606
(
2
37
.
747
2
=
+
=
+ s
x
, or in dollar units, $1,961,160.  This is quite a 
bit less than the $3.5 million that was awarded originally. 
 
 
51. 
 
a. 
 and 
, so 
2563
=
Σx
501
,
368
2 =
Σx
 
766
.
1264
18
]
19
/
)
2563
(
501
,
368
[
2
2
=
−
=
s
 and 
564
.
35
=
s
 
 
b. If y = time in minutes, then y = cx where 
60
1
=
c
, so 
 
 
351
.
3600
766
.
1264
2
2
2
=
=
=
x
y
s
c
s
 and 
593
.
60
564
.
35
=
=
=
x
y
cs
s
 
 
 
52. 
Let d denote the fifth deviation.  Then 
0
3.1
0.1
9.
3.
=
+
+
+
+
d
 or 
, so 
.  One sample for which these are the deviations is 
0
5.3
=
+ d
5.3
−
=
d
,8.3
1 =
x
 
 
 
 
 (obtained by adding 3.5 to each deviation; adding any other 
number will produce a different sample with the desired property) 
,4.4
2 =
x
,5.4
3 =
x
,8.4
4 =
x
.0
5 =
x
 
 
53. 
 
a. lower half: 2.34 2.43 2.62 2.74 2.74 2.75 2.78 3.01 3.46 
 
      upper half: 3.46 3.56 3.65 3.85 3.88 3.93 4.21 4.33 4.52 
 
      Thus the lower fourth is 2.74 and the upper fourth is 3.88. 
 
b. 
 
14
.1
74
.2
88
.3
=
−
=
sf
 
c. 
 wouldn’t change, since increasing the two largest values does not affect the upper 
fourth. 
sf
 
d. By at most .40 (that is, to anything not exceeding 2.74), since then it will not change the 
lower fourth. 
 
e. 
Since n is now even, the lower half consists of the smallest 9 observations and the upper 
half consists of the largest 9.  With the lower fourth = 2.74 and the upper fourth = 3.93, 
. 
19
.1
=
sf
 
 
25

Chapter 1:  Overview and Descriptive Statistics 
54. 
 
a. The lower half of the data set:  4.4  16.4  22.2  30.0  33.1  36.6, whose median, and 
therefore, the lower quartile, is (
)
.1.
26
2
0.
30
2.
22
+
+
 
The top half of the data set:  36.6  40.4  66.7  73.7  81.5  109.9, whose median, and 
therefore, the upper quartile, is (
)
2.
70
2
7.
73
7.
66
=
+
. 
So, the IQR = (70.2 – 26.1) = 44.1 
 
b. 
 
 
A boxplot (created in Minitab) of this data appears below: 
100
50
0
sheer strength
 
There is a slight positive skew to the data.  The variation seems quite large.  There are no 
outliers. 
 
c. 
An observation would need to be further than 1.5(44.1) = 66.15 units below the lower 
quartile (
)
[
]
units
05
.
40
15
.
66
1.
26
−
=
−
 or above the upper quartile 
 to be classified as a mild outlier.  Notice that, in this 
case, an outlier on the lower side would not be possible since the sheer strength variable 
cannot have a negative value. 
(
)
[
units
35
.
136
15
.
66
2.
70
=
+
]
 
 
An extreme outlier would fall (3)44.1) = 132.3 or more units below the lower, or above 
the upper quartile.  Since the minimum and maximum observations in the data are 4.4 
and 109.9 respectively, we conclude that there are no outliers, of either type, in this data 
set. 
 
d. Not until the value x = 109.9 is lowered below 73.7 would there be any change in the 
value of the upper quartile.  That is, the value x = 109.9 could not be decreased by more 
than (109.9 – 73.7) = 36.2 units. 
 
 
26

Chapter 1:  Overview and Descriptive Statistics 
55. 
 
a. Lower half of the data set: 325   325   334   339   356   356   359   359   363   364   364   
366   369, whose median, and therefore the lower quartile, is 359 (the 7th observation in 
the sorted list).  
 
 The top half of the data is 370   373   373   374   375   389   392   393   394   397   402   
403   424, whose median, and therefore the upper quartile is 392.   So, the IQR = 392 - 
359 = 33. 
 
b. 1.5(IQR) = 1.5(33) = 49.5  and 3(IQR) = 3(33) = 99.  Observations that are further than 
49.5 below the lower quartile (i.e., 359-49.5 = 309.5 or less) or more than 49.5 units 
above the upper quartile (greater than 392+49.5 = 441.5) are classified as 'mild' outliers.  
'Extreme' outliers would fall 99 or more units below the lower, or above the upper, 
quartile.  Since the minimum and maximum observations in the data are 325 and 424, we 
conclude that there are no mild outliers in this data (and therefore, no 'extreme' outliers 
either).  
 
c. 
A boxplot (created by Minitab) of this data appears below.  There is a slight positive 
skew to the data, but it is not far from being symmetric.  The variation, however, seems 
large (the spread 424-325 = 99 is a large percentage of the median/typical value) 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
d. Not until the value x = 424 is lowered below the upper quartile value of 392 would there 
be any change in the value of the upper quartile.  That is, the value  x = 424 could not be 
decreased by more than 424-392 = 32 units. 
 
420
370
320
Escape time
 
27

Chapter 1:  Overview and Descriptive Statistics 
56. 
A boxplot (created in Minitab) of this data appears below. 
 
There is a slight positive skew to this data.  There is one extreme outler (x=511).  Even when 
removing the outlier, the variation is still moderately large. 
 
 
500
400
300
200
100
0
aluminum
57. 
 
a. 1.5(IQR) = 1.5(216.8-196.0) =  31.2  and 3(IQR) = 3(216.8-196.0) = 62.4. 
 
Mild outliers:       observations below 196-31.2 = 164.6 or above 216.8+31.2 = 248.  
Extreme outliers: observations below 196-62.4 = 133.6 or above 216.8+62.4 = 279.2.  Of 
the observations given, 125.8 is an extreme outlier and 250.2 is a mild outlier. 
 
b. A boxplot of this data appears below.  There is a bit of positive skew to the data but, 
except for the two outliers identified in part (a), the variation in the data is relatively 
small. 
 
x
120    140    160    180    200    220    240    260
*
*
 
 
 
58. 
The most noticeable feature of the comparative boxplots is that machine 2’s sample values 
have considerably more variation than does machine 1’s sample values.  However, a typical 
value, as measured by the median, seems to be about the same for the two machines.  The 
only outlier that exists is from machine 1. 
 
 
28

Chapter 1:  Overview and Descriptive Statistics 
59. 
 
a. ED:  median  = .4 (the 14th value in the sorted list of data).  The lower quartile (median of 
the lower half of the data, including the median, since n is odd) is  
 
 
( .1+.1 )/2 =  .1.  The upper quartile is (2.7+2.8)/2 = 2.75.  Therefore,  
 
 
IQR =  2.75 - .1 = 2.65. 
 
Non-ED: median = (1.5+1.7)/2 = 1.6.  The lower quartile (median of the lower 25 
observations) is .3;  the upper quartile (median of the upper half of the data) is 7.9.  
Therefore, IQR = 7.9 - .3 = 7.6. 
 
b. ED:  mild outliers are less than .1 - 1.5(2.65) =  -3.875 or greater than 2.75 + 1.5(2.65) = 
6.725.  Extreme outliers are less than .1 - 3(2.65) =  -7.85 or greater than 2.75 + 3(2.65) = 
10.7.  So, the two largest observations (11.7, 21.0) are extreme outliers and the next two 
largest values (8.9, 9.2) are mild outliers.  There are no outliers at the lower end of the 
data. 
 
Non-ED: mild outliers are less than .3 - 1.5(7.6) = -11.1 or greater than 7.9 + 1.5(7.6) = 
19.3.  Note that there are no mild outliers in the data, hence there can not be any extreme 
outliers either. 
 
c. 
A comparative boxplot appears below.  The outliers in the ED data are clearly visible.  
There is noticeable positive skewness in both samples;  the Non-Ed data has more 
variability then the Ed data; the typical values of the ED data tend to be smaller than 
those for the Non-ED data. 
 
 
 
 
 
 
 
 
 
 
20
10
0
Concentration (mg/L)
ED
Non-ED
 
29

Chapter 1:  Overview and Descriptive Statistics 
60. 
A comparative boxplot (created in Minitab) of this data appears below. 
 
8000
7000
6000
5000
test
cannister
burst strength
type
 
The burst strengths for the test nozzle closure welds are quite different from the burst 
strengths of the production canister nozzle welds. 
 
 
The test welds have much higher burst strengths and the burst strengths are much more 
variable. 
 
 
The production welds have more consistent burst strength and are consistently lower than the 
test welds.  The production welds data does contain 2 outliers. 
 
61. 
Outliers occur in the 6 a.m. data.  The distributions at the other times are fairly symmetric.  
Variability and the 'typical' values in the data increase a little at the 12 noon and 2 p.m. times.   
 
 
30

Chapter 1:  Overview and Descriptive Statistics 
Supplementary Exercises 
 
62. 
To somewhat simplify the algebra, begin by subtracting 76,000 from the original data.  This 
transformation will affect each date value and the mean.  It will not affect the standard 
deviation. 
 
831
,
048
,1
,
683
2
1
=
=
=
y
x
x
             
324
,3
)
831
)(
4
(
=
=
x
n
 so, 
324
,3
4
3
2
1
=
+
+
+
x
x
x
x
 
and 
593
,1
324
,3
4
1
3
2
=
−
−
=
+
x
x
x
x
 and 
(
)
2
3
593
,1
x
x
−
=
 
Next, 
(
)
(
)
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
−
=
=
∑
3
4
3324
180
2
2
2
2
ix
s
 
 
So, 
, 
 and 
 
444
,
859
,2
2 =
∑
ix
444
,
859
,2
2
4
2
3
2
2
2
1
=
+
+
+
x
x
x
x
651
,
294
,1
444
,
859
,2
2
4
2
1
2
3
2
2
=
+
−
=
+
x
x
x
x
 
By substituting 
 we obtain the equation 
. 
(
2
3
1593
x
x
−
=
)
(
)
0
651
,
294
,1
593
,1
2
2
2
2
=
−
−
+
x
x
0
499
,
621
593
,1
2
2
=
+
−
x
xx
 
Evaluating for 
we obtain 
2x
8635
.
682
2 =
x
 and 
1365
.
910
8635
.
682
593
,1
3
=
−
=
x
.  
Thus, 
. 
910
,
76
683
,
76
3
2
=
=
x
x
 
 
 
31

Chapter 1:  Overview and Descriptive Statistics 
63. 
Flow 
 
Lower 
Upper 
 
rate  
Median quartile quartile 
IQR 
1.5(IQR) 
3(IQR) 
 
 125 
3.1 
2.7 
 3.8 
1.1 
1.65 
 .3 
 
160 
4.4 
4.2 
4.9 
.7 
1.05 
 .1 
 
200 
3.8 
3.4 
4.6 
1.2 
1.80 
3.6 
 
 
There are no outliers in the three data sets.  However, as the comparative boxplot below 
shows, the three data sets differ with respect to their central values (the medians are different) 
and the data for flow rate 160 is somewhat less variable than the other data sets.  Flow rates 
125 and 200 also exhibit a small degree of positive skewness. 
 
5
4
3
200
160
125
Uniformity (%)
Flow rate
 
 
 
 
32

Chapter 1:  Overview and Descriptive Statistics 
64. 
 
 
6 
34 
stem=ones 
7 
17 
leaf=tenths 
8 
4589 
 
 
9 
1 
 
 
10 
12667789 
 
 
11 
122499 
 
 
12 
2 
 
 
13 
1 
 
 
6.
14
)
3.2
)(
5.1(
15
.
11
4.5
)
3.2
)(
5.1(
85
.8
3.2
27
7594
.1
6.
10
~
,
9556
.9
=
+
=
−
=
=
=
=
=
sf
n
s
x
x
lower fourth = 8.85, upper fourth = 11.15 
 
no outliers 
 
There are no outliers.  The distribution is skewed to the left. 
13
12
11
10
9
8
7
6
Radiation
 
 
 
 
33

Chapter 1:  Overview and Descriptive Statistics 
65. 
 
a. 
HC data: 
 = 2618.42  and 
∑
i
ix
2
∑
i
ix =  96.8,   
 
 
so s2 = [2618.42 - (96.8)2/4]/3 = 91.953  
 
 
and the sample standard deviation is  s = 9.59.   
 
 
 
CO data: ∑
 = 145645 and 
i
ix
2
∑
i
ix =735, so s2 = [145645 - (735)2/4]/3 = 
3529.583 and the sample standard deviation is  s = 59.41.  
 
b. 
 The mean of the HC data is 96.8/4 = 24.2; the mean of the CO data is 735/4 = 
183.75.  Therefore, the coefficient of variation of the HC data is 9.59/24.2 = .3963, 
or 39.63%.  The coefficient of variation of the CO data is 59.41/183.75 = .3233, or 
32.33%.  Thus, even though the CO data has a larger standard deviation than does 
the HC data, it actually exhibits less variability (in percentage terms) around its 
average than does the HC data. 
 
 
66. 
 
a. 
The histogram appears below.   A representative value for this data would be x = 90.  
The histogram is reasonably symmetric, unimodal, and somewhat bell-shaped.  The 
variation in the data is not small since the spread of the data (99-81 = 18) constitutes 
about 20% of the typical value of 90. 
 
99
97
95
93
91
89
87
85
83 
81 
.20 
.10 
0 
Fracture strength (MP a)
Relative frequency 
 
b. 
The proportion of the observations that are at least 85 is 1 - (6+7)/169 = .9231.  The 
proportion less than 95 is 1 - (22+13+3)/169 = .7751. 
 
c. 
x = 90 is the midpoint of the class 89-<91, which contains 43 observations (a relative 
frequency of 43/169 = .2544.  Therefore about half of this frequency, .1272, should 
be added to the relative frequencies for the classes to the left of x = 90.  That is, the 
approximate proportion of observations that are less than 90 is .0355 + .0414 + .1006 
+ .1775 + .1272 = .4822.   
 
 
34

Chapter 1:  Overview and Descriptive Statistics 
67. 
 
 
 
(
)
(
)
(
)
(
)
65
.
10
60
.
10
2
1
70
.
10
2
1
%
10
10
1
100
15
2
100
2
1
15
1
100
2
1
60
.
10
11
7.
13
6.
15
8.8
5.8
2.
163
%
15
2
100
70
.
10
13
6.
15
5.8
2.
163
%
15
1
100
2.
163
=
+
=
=
⎟
⎠
⎞
⎜
⎝
⎛
=
⎟
⎠
⎞
⎜
⎝
⎛
+
⎟
⎠
⎞
⎜
⎝
⎛
∴
=
−
−
−
−
=
⎟
⎠
⎞
⎜
⎝
⎛
=
−
−
=
⎟
⎠
⎞
⎜
⎝
⎛
=
∑
n
trimmedmea
n
trimmedmea
n
trimmedmea
xi
68. 
 
a. 
{
}
∑
∑
∑
∑
∑
∑
∑
∑
∑
=
=
⇒
=
⇒
=
−
⇒
=
−
⇒
=
−
⇒
=
−
−
=
−
=
−
.
0
0
0
)
(
0
)
(
2
)
(
)
(
2
2
x
n
x
c
x
nc
nc
x
c
x
c
x
c
x
c
x
dc
d
c
x
dc
d
i
i
i
i
i
i
i
i
 
 
b.       
(
)
(
)
∑
∑
−
−
.
2
2
µ
i
i
x
han
issmallert
x
x
 
 
 
69. 
 
a.  
(
)
(
)
(
)
(
)
(
)
.
1
1
1
)
(
1
.
2
2
2
2
2
2
2
2
x
i
i
i
i
y
i
i
i
s
a
n
x
x
a
n
x
a
ax
n
b
x
a
b
ax
n
y
y
s
b
x
a
n
b
x
a
n
b
ax
n
y
y
=
−
−
=
−
−
=
−
+
−
+
=
−
−
=
+
=
+
=
+
=
=
∑
∑
∑
∑
∑
∑
∑
 
 
b.  
(
)
(
)
872
.1
5044
.3
04
.1
5
9
14
.
189
32
3.
87
5
9
,
2
2
2
=
=
⎟
⎠
⎞
⎜
⎝
⎛
=
=
=
+
=
=
=
y
y
s
s
y
F
y
C
x
o
o
 
 
35

Chapter 1:  Overview and Descriptive Statistics 
70. 
 
a. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
There is a significant difference in the variability of the two samples.  The weight training 
produced much higher oxygen consumption, on average, than  the treadmill exercise, 
with the median consumptions being approximately 20 and 11 liters, respectively. 
 
Weight
Treadmill
25
20
15
10
5
0
Exercise Type
Oxygen Consumption
b. Subtracting the y from the x for each subject, the differences are 3.3, 9.1, 10.4, 9.1, 6.2, 
2.5, 2.2, 8.4, 8.7, 14.4, 2.5, -2.8, -0.4, 5.0, and 11.5. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
The majority of the differences are positive, which suggests that the weight training 
produced higher oxygen consumption for most subjects. The median difference is about 6 
liters. 
 
 
15
10
5
0
Difference
 
36

Chapter 1:  Overview and Descriptive Statistics 
71. 
 
a. The mean, median, and trimmed mean are virtually identical, which suggests symmetry.  
If there are outliers, they are balanced.  The range of values is only 25.5, but half of the 
values are between 132.95 and 138.25. 
 
b.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
The boxplot also displays the symmetry, and adds a visual of the outliers, two on the 
lower end, and one on the upper. 
 
 
150
140
130
120
strength
 
37

Chapter 1:  Overview and Descriptive Statistics 
72. 
A table of summary statistics, a stem and leaf display, and a comparative boxplot  are below.  
The healthy individuals have higher receptor binding measure on average than the individuals 
with PTSD.  There is also more variation in the healthy individuals’ values.  The distribution 
of values for the healthy is reasonably symmetric, while the distribution for the PTSD 
individuals is negatively skewed.  The box plot indicates that there are no outliers, and 
confirms the above comments regarding symmetry and skewness. 
 
 
PTSD 
Healthy 
Mean 
32.92 
52.23 
Median 
37 
51 
Std Dev 
9.93 
14.86 
Min 
10 
23 
Max 
46 
72 
 
 
 
 
1 
0 
stem = tens 
3
2 
058 
leaf = ones 
9
3 
1578899
 
7310
4 
26 
 
81
5 
 
 
9763
6 
 
 
2
7 
 
 
 
 
10
20
30
40
50
60
70
Healthy
PTSD
Receptor Binding
Individuals
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
38

Chapter 1:  Overview and Descriptive Statistics 
73. 
 
0.7 8 
stem=tenths 
0.8 11556 
leaf=hundredths 
0.9 2233335566 
1.0 0566 
96
.
,
855
.
93
.
~
,
0809
.
,
9255
.
=
=
=
=
=
h
upperfourt
h
lowerfourt
x
s
x
0.8
0.9
1.0
Cadence
 
The data appears to be a bit skewed toward smaller values (negatively skewed).  
There are no outliers.  The mean and the median are close in value.  
 
 
74. 
 
a. 
Mode = .93.  It occurs four times in the data set.  
 
b. 
The Modal Category is the one in which the most observations occur. 
 
 
 
39

Chapter 1:  Overview and Descriptive Statistics 
75. 
 
a. The median is the same (371) in each plot and all three data sets are very symmetric.  In 
addition, all three have the same minimum value (350) and same maximum value (392).  
Moreover, all three data sets have the same lower (364) and upper quartiles (378).  So, all 
three boxplots will be identical. 
 
b. A comparative dotplot is shown below.  These graphs show that there are differences in 
the variability of the three data sets.  They also show differences in the way the values are 
distributed in the three data sets. 
             .                        . 
             :         .              :::              .        :. 
          -----+---------+---------+---------+---------+---------+- Type 1       
 
             .    .     .    . .   ..  . . . .    .   .     .    . 
          -----+---------+---------+---------+---------+---------+- Type 2       
                                              . 
             .            . . :. .     .      :  .:              . 
          -----+---------+---------+---------+---------+---------+- Type 3       
           352.0     360.0     368.0     376.0     384.0     392.0 
 
c. 
The boxplot in (a) is not capable of detecting the differences among the data sets.  The 
primary reason is that boxplots give up some detail in describing data because they use 
only 5 summary numbers for comparing data sets.  Note:   The definition of lower and 
upper quartile used in this text is slightly different than the one used by some other 
authors (and software packages).   Technically speaking, the median of the lower half of 
the data is not really the first quartile, although it is generally very close.  Instead, the 
medians of the lower and upper halves of the data are often called the lower and upper 
hinges.   Our boxplots use the lower and upper hinges to define the spread of the middle 
50% of the data, but other authors sometimes use the actual quartiles for this purpose.  
The difference is usually very slight, usually unnoticeable, but not always.  For example 
in the data sets of this exercise, a comparative boxplot based on the actual quartiles (as 
computed by Minitab) is shown below.  The graph shows substantially the same type of 
information as those described in (a) except the graphs based on quartiles are able to 
detect the slight differences in variation between the three data sets. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
390
380
370
360
350
3
2
1
MPa
Type of wire
 
40

Chapter 1:  Overview and Descriptive Statistics 
76. 
The measures that are sensitive to outliers are:  the mean and the midrange.  The mean is 
sensitive because all values are used in computing it.  The midrange is sensitive because it 
uses only the most extreme values in its computation. 
The median, the trimmed mean, and the midhinge are not sensitive to outliers. 
 
The median is the most resistant to outliers because it uses only the middle value (or values) 
in its computation. 
 
The trimmed mean is somewhat resistant to outliers.  The larger the trimming percentage, the 
more resistant the trimmed mean becomes. 
 
The midhinge, which uses the quartiles, is reasonably resistant to outliers because both 
quartiles are resistant to outliers. 
 
 
 
77. 
 
a.  
0 2355566777888 
1 0000135555 
2 00257 
3 0033 
4 0057 
5 044 
6 
stem: ones 
7 05 
leaf: tenths 
8 8 
9 0 
10 3 
HI 22.0 24.5 
 
 
41

Chapter 1:  Overview and Descriptive Statistics 
b.  
Interval 
Frequency 
Rel. Freq. 
Density 
0 -< 2 
23 
.500 
.250 
2 -< 4 
9 
.196 
.098 
4 -< 6 
7 
.152 
.076 
6 -< 10 
4 
.087 
.022 
10 -< 20 
1 
.022 
.002 
20 -< 30 
2 
.043 
.004 
 
30
20
10
6
4
2
0
0.25
0.20
0.15
0.10
0.05
0.00
Repair Time
Density
 
 
 
78. 
 
a. Since the constant   x  is subtracted from each x value to obtain each y value, and 
addition or subtraction of a constant doesn’t affect variability,  
 and  
 
2
2
x
y
s
s =
x
y
s
s =
b. Let  c = 1/s, where s is the sample standard deviation of the x’s and also (by a ) of the y’s.  
Then sz = csy = (1/s)s = 1, and sz
2 = 1.  That is, the “standardized” quantities z1, … , zn 
have a sample variance and standard deviation of 1. 
 
 
42

Chapter 1:  Overview and Descriptive Statistics 
79. 
 
a. 
)1
(
]
[
,
1
1
1
1
1
1
1
+
+
=
+
=
+
=
+
+
+
=
+
+
=
∑
∑
n
x
x
n
x
so
x
x
n
x
x
x
n
n
n
n
n
n
i
n
i
n
i
i
 
 
b.   
{
}
2
1
2
2
1
2
2
1
2
2
1
2
1
2
2
1
1
1
2
1
1
2
1
2
1
)1
(
)1
(
)1
(
)1
(
)
(
+
+
+
+
=
+
+
=
+
=
+
+
+
−
+
+
−
=
+
−
+
+
−
=
+
−
=
−
=
∑
∑
∑
n
n
n
n
n
n
n
n
n
i
i
n
n
i
i
n
i
n
i
n
x
n
x
n
x
s
n
x
n
x
n
x
x
n
x
x
n
x
x
x
ns
 
 
 
When the expression for 
1
+
nx
 from a is substituted, the expression in braces simplifies to 
the following, as desired: 
)1
(
)
(
2
1
+
−
+
n
x
x
n
n
n
 
c. 
53
.
12
16
5.
200
16
8.
11
)
58
.
12
(
15
1
=
=
+
=
+
nx
 
( )
(
)
)
16
(
)
58
.
12
8.
11
(
512
.
15
14
)1
(
)
(
1
2
2
2
1
2
2
1
−
+
=
+
−
+
−
=
+
+
n
x
x
s
n
n
s
n
n
n
n
 
.  So the standard deviation 
238
.
038
.
245
.
=
+
=
532
.
238
.
1
=
=
+
ns
 
 
 
 
43

Chapter 1:  Overview and Descriptive Statistics 
80. 
 
a. 
 
 
45
35
25
15
5
0.06
0.05
0.04
0.03
0.02
0.01
0.00
length
Density
Bus Route Length
 
b. Proportion less than 
552
.
391
216
20
=
⎟
⎠
⎞
⎜
⎝
⎛
=
 
Proportion at least 
102
.
391
40
30
=
⎟
⎠
⎞
⎜
⎝
⎛
=
 
c. 
First compute (.90)(391 + 1) = 352.8.  Thus, the 90th percentile should be about the 352nd 
ordered value.  The 351st ordered value lies in the interval 28 - < 30.  The 352nd ordered 
value lies in the interval 30 - < 35.  There are 27 values in the interval 30 - < 35.  We do 
not know how these values are distributed, however, the smallest value (i.e., the 352nd 
value in the data set) cannot be smaller than 30.  So, the 90th percentile is roughly 30. 
d. First compute (.50)(391 + 1) = 196.  Thus the median (50th percentile) should be the 196 
ordered value.  The 174th ordered value lies in the interval  
16 -< 18.  The next 42 
observation lie in the interval 18 - < 20.  So, ordered observation 175 to 216 lie in the 
intervals 18 - < 20.  The 196th observation is about in the middle of these.  Thus, we 
would say, the median is roughly 19. 
 
81. 
Assuming that the histogram is unimodal, then there is evidence of positive skewness in the 
data since the median lies to the left of the mean (for a symmetric distribution, the mean and 
median would coincide).   For more evidence of skewness, compare the distances of the 5th 
and 95th percentiles from the median:  median - 5th percentile = 500 - 400 = 100  while 95th 
percentile -median = 720 - 500 = 220.   Thus, the largest 5% of the values (above the 95th 
percentile) are further from the median than are the lowest 5%.  The same skewness is evident 
when comparing the 10th and 90th percentiles to the median: median - 10th percentile = 500 - 
430 = 70  while 90th percentile -median = 640 - 500 = 140.   Finally, note that the largest 
value (925) is much further from the median (925-500 = 425) than is the smallest value (500 - 
220 = 280), again an indication of positive skewness. 
 
 
44

Chapter 1:  Overview and Descriptive Statistics 
82. 
 
a. There is some evidence of a cyclical pattern. 
10
5
60
50
40
Index
Temperature
b. 
.
,2.
48
23
.
48
)
7.
47
)(
9
(.
)
53
)(
1
(.
9.
1.
7.
47
)
47
)(
9
(.
)
54
)(
1
(.
9.
1.
2
3
3
1
2
2
etc
x
x
x
x
x
x
≈
=
+
=
+
=
=
+
=
+
=
 
 
t 
1.
.
=
α
for
xt
 
5.
.
=
α
for
xt
 
1 
47.0 
47.0 
2 
47.7 
50.5 
3 
48.2 
51.8 
4 
48.4 
50.9 
5 
48.2 
48.4 
6 
48.0 
47.2 
7 
47.9 
47.1 
8 
48.1 
48.6 
9 
48.4 
49.8 
10 
48.5 
49.9 
11 
48.3 
47.9 
12 
48.6 
50.0 
13 
48.8 
50.0 
14 
48.9 
50.0 
α= .1 gives a smoother series. 
 
c. 
1
)
1(
−
−
+
=
t
t
t
x
x
x
α
α
 
1
1
2
2
2
2
1
3
2
2
1
2
1
)
1(
)
1(
...
)
1(
)
1(
...
]
)
1(
[
)
1(
)
1(
]
)
1(
)[
1(
x
x
x
x
x
x
x
x
x
x
x
x
t
t
t
t
t
t
t
t
t
t
t
t
−
−
−
−
−
−
−
−
−
−
+
−
+
+
−
+
−
+
=
=
−
+
−
+
−
+
=
−
+
−
+
=
α
α
α
α
α
α
α
α
α
α
α
α
α
α
α
α
α
α
 
 
Thus, (x bar)t depends on xt and all previous values.  As k increases, the coefficient on xt-
k decreases (further back in time implies less weight). 
 
d. Not very sensitive, since (1-α)t-1 will be very small. 
 
45

Chapter 1:  Overview and Descriptive Statistics 
83. 
 
a. 
When there is perfect symmetry, the smallest observation y1 and the largest 
observation yn  will be equidistant from the median, so 
1y
x
x
yn
−
=
−
.  
 
Similarly, the second smallest and second largest will be equidistant from 
the median, so 
2
1
y
x
x
yn
−
=
−
−
 
 
 
and so on.  Thus, the first and second  numbers in each pair will be equal, so that 
each point in the plot will fall exactly on the 45 degree line.  When the data is 
positively skewed, yn will be much further from the median than is y1, so 
x
yn
~
−
 
will considerably exceed 
1
~
y
x −
 and the point 
)
~
,
~
(
1y
x
x
yn
−
−
  will fall 
considerably below the 45 degree line.  A similar comment aplies to other points in 
the plot. 
 
b. 
The first point in the plot is (2745.6 – 221.6, 221.6 0- 4.1) = (2524.0, 217.5).  The 
others are: (1476.2, 213.9), (1434.4, 204.1), ( 756.4, 190.2), ( 481.8, 188.9), ( 267.5, 
181.0), ( 208.4, 129.2), ( 112.5, 106.3), ( 81.2, 103.3), ( 53.1, 102.6), ( 53.1,  92.0), 
(33.4,  23.0), and (20.9, 20.9).  The first number in each of the first seven pairs 
greatly exceed the second number, so each point falls well below the 45 degree line.  
A substantial positive skew (stretched upper tail) is indicated. 
 
 
 
 
 
 
 
 
 
 
46

CHAPTER 2 
 
Section 2.1 
 
1. 
 
a. S = { 1324, 1342, 1423, 1432, 2314, 2341, 2413, 2431, 3124, 3142, 4123, 4132, 3214, 
3241, 4213, 4231 } 
 
b. Event A contains the outcomes where 1 is first in the list: 
A = { 1324, 1342, 1423, 1432 } 
 
c. 
Event B contains the outcomes where 2 is first or second: 
B = { 2314, 2341, 2413, 2431, 3214, 3241, 4213, 4231 } 
 
 
d. The compound event A∪B contains the outcomes in A or B or both: 
A∪B = {1324, 1342, 1423, 1432, 2314, 2341, 2413, 2431, 3214, 3241, 4213, 4231 } 
 
 
2. 
 
a. Event A = { RRR, LLL, SSS } 
 
b. Event B = { RLS, RSL, LRS, LSR, SRL, SLR } 
 
c. 
Event C = { RRL, RRS, RLR, RSR, LRR, SRR } 
 
d. Event D = { RRL, RRS, RLR, RSR, LRR, SRR, LLR, LLS, LRL, LSL, RLL, SLL, SSR, 
SSL, SRS, SLS, RSS, LSS } 
 
e. 
Event D′ contains outcomes where all cars go the same direction, or they all go different 
directions: 
D′ = { RRR, LLL, SSS, RLS, RSL, LRS, LSR, SRL, SLR } 
 
 
 
Because Event D totally encloses Event C, the compound event C∪D = D: 
C∪D = { RRL, RRS, RLR, RSR, LRR, SRR, LLR, LLS, LRL, LSL, RLL, SLL, SSR, 
SSL, SRS, SLS, RSS, LSS } 
 
Using similar reasoning, we see that the compound event C∩D = C: 
C∩D  = { RRL, RRS, RLR, RSR, LRR, SRR } 
 
47 

Chapter 2:  Probability 
3. 
 
a. Event A = { SSF, SFS, FSS } 
 
b. Event B = { SSS, SSF, SFS, FSS } 
 
c. 
For Event C, the system must have component 1 working ( S in the first position), then at 
least one  of the other two components must work (at least one S in the 2nd and 3rd 
positions:  Event C = { SSS, SSF, SFS } 
 
d. Event C′ = { SFF, FSS, FSF, FFS, FFF } 
Event A∪C = { SSS, SSF, SFS, FSS } 
Event A∩C = { SSF, SFS } 
Event B∪C = { SSS, SSF, SFS, FSS }  
 
Event B∩C = { SSS SSF, SFS } 
 
4. 
 
a.  
 
Home Mortgage Number 
Outcome 
1 
2 
3 
4 
1 
F 
F 
F 
F 
2 
F 
F 
F 
V 
3 
F 
F 
V 
F 
4 
F 
F 
V 
V 
5 
F 
V 
F 
F 
6 
F 
V 
F 
V 
7 
F 
V 
V 
F 
8 
F 
V 
V 
V 
9 
V 
F 
F 
F 
10 
V 
F 
F 
V 
11 
V 
F 
V 
F 
12 
V 
F 
V 
V 
13 
V 
V 
F 
F 
14 
V 
V 
F 
V 
15 
V 
V 
V 
F 
16 
V 
V 
V 
V 
 
b. Outcome numbers 2, 3, 5 ,9 
 
c. 
Outcome numbers 1, 16 
 
d. Outcome numbers 1, 2, 3, 5, 9 
 
e. 
In words, the UNION described is the event that either all of the mortgages are variable, 
or that at most all of them are variable: outcomes 1,2,3,5,9,16.  The INTERSECTION 
described is the event that all of the mortgages are fixed: outcome 1. 
 
f. 
The UNION described is the event that either exactly three are fixed, or that all four are 
the same:  outcomes 1, 2, 3, 5, 9, 16.  The INTERSECTION in words is the event that 
exactly three are fixed AND that all four are the same.  This cannot happen. (There are no 
outcomes in common) : b ∩ c = ∅. 
 
48

Chapter 2:  Probability 
 
 
5. 
 
a.  
 
 
Outcome 
 
Number 
Outcome 
1 
111 
2 
112 
3 
113 
4 
121 
5 
122 
6 
123 
7 
131 
8 
132 
9 
133 
10 
211 
11 
212 
12 
213 
13 
221 
14 
222 
15 
223 
16 
231 
17 
232 
18 
233 
19 
311 
20 
312 
21 
313 
22 
321 
23 
322 
24 
323 
25 
331 
26 
332 
27 
333 
 
b. Outcome Numbers 1, 14, 27 
 
c. 
Outcome Numbers 6, 8, 12, 16, 20, 22 
 
d. Outcome Numbers 1, 3, 7, 9, 19, 21, 25, 27 
 
 
49

Chapter 2:  Probability 
6. 
 
a.  
Outcome 
 
Number 
Outcome 
1 
123 
2 
124 
3 
125 
4 
213 
5 
214 
6 
215 
7 
13 
8 
14 
9 
15 
10 
23 
11 
24 
12 
25 
13 
3 
14 
4 
15 
5 
 
b. Outcomes  13, 14, 15 
 
c. 
Outcomes  3, 6,  9, 12, 15 
 
d. Outcomes  10, 11, 12, 13, 14, 15 
 
 
7. 
 
a. S = {BBBAAAA, BBABAAA, BBAABAA, BBAAABA, BBAAAAB, BABBAAA, 
BABABAA, BABAABA, BABAAAB, BAABBAA, BAABABA, BAABAAB, 
BAAABBA, BAAABAB, BAAAABB, ABBBAAA, ABBABAA, ABBAABA, 
ABBAAAB, ABABBAA, ABABABA, ABABAAB, ABAABBA, ABAABAB, 
ABAAABB, AABBBAA, AABBABA, AABBAAB, AABABBA, AABABAB, 
AABAABB, AAABBBA, AAABBAB, AAABABB, AAAABBB} 
 
b. {AAAABBB, AAABABB, AAABBAB, AABAABB, AABABAB} 
 
50

Chapter 2:  Probability 
8. 
 
a. A1 ∪ A2 ∪ A3  
 
b. A1 ∩ A2 ∩ A3 
 
c. 
A1 ∩ A2′ ∩ A3′ 
 
 
51

Chapter 2:  Probability 
d. (A1 ∩ A2′∩ A3 ′) ∪ (A1′ ∩ A2 ∩ A3 ′) ∪ (A1 ′∩ A2 ′∩ A3 ) 
 
 
e. 
A1 ∪ (A2 ∩ A3) 
 
 
 
 
52

Chapter 2:  Probability 
9. 
 
a. In the diagram on the left, the shaded area is (A∪B)′.  On the right, the shaded area is A′, 
the striped area is B′, and the intersection A′ ∩ B′ occurs where there is BOTH shading 
and stripes.  These two diagrams display the same area. 
 
b. In the diagram below, the shaded area represents (A∩B)′.  Using the diagram on the right 
above, the union of  A′ and  B′ is represented by the areas that have either shading or 
stripes or both.  Both of the diagrams display the same area. 
 
10. 
 
a. A = {Chev, Pont, Buick}, B = {Ford, Merc}, C = {Plym, Chrys} are three mutually 
exclusive events. 
 
b. No, let E = {Chev, Pont}, F = {Pont, Buick}, G = {Buick, Ford}.  These events are not 
mutually exclusive (e.g. E and F have an outcome in common), yet there is no outcome 
common to all three events. 
 
 
53

Chapter 2:  Probability 
Section 2.2 
 
11. 
 
a. .07 
 
b. .15 + .10 + .05 = .30 
 
c. 
Let event A = selected customer owns stocks.  Then the probability that a selected 
customer does not own a stock can be represented by  
P(A′) = 1 - P(A) = 1 – (.18 + .25) = 1 - .43 = .57.  This could also have been done easily 
by adding the probabilities of the funds that are not stocks. 
 
 
12. 
 
a. P(A ∪ B) = .50 + .40 - .25 = .65 
 
b. P(A ∪ B)′ = 1 - .65 = .35 
 
c. 
A ∩ B′ ; P(A ∩ B′) = P(A) – P(A ∩ B) = .50 - .25 = .25 
 
 
13. 
 
a. awarded either #1 or #2 (or both): 
P(A1 ∪ A2) = P(A1) + P(A2) - P(A1 ∩ A2) = .22 + .25 - .11 = .36 
 
b. awarded neither #1 or #2: 
 
P(A1′ ∩ A2′) = P[(A1 ∪ A2) ′] = 1 - P(A1 ∪ A2) = 1 - .36 = .64 
 
c. 
awarded at least one of #1, #2, #3: 
 
P(A1 ∪ A2 ∪ A3) = P(A1) + P(A2) + P(A3) - P(A1 ∩ A2) - P(A1 ∩ A3) –  
    P(A2 ∩ A3) + P(A1 ∩  A2 ∩  A3)  
= .22 +.25 + .28 - .11 -.05 - .07 + .01 = .53 
d. awarded none of the three projects: 
 
P( A1′ ∩  A2′ ∩  A3′ ) = 1 – P(awarded at least one) = 1 - .53 = .47. 
 
e. 
awarded #3 but neither #1 nor #2: 
 
P( A1′ ∩  A2′ ∩  A3 ) = P(A3) - P(A1 ∩ A3) – P(A2 ∩ A3)  
+ P(A1 ∩  A2 ∩  A3)  
 
 
 
 
      = .28 - .05 - .07+ .01      = .17 
 
 
 
 
 
 
54

Chapter 2:  Probability 
f. 
either (neither #1 nor #2) or #3:  
P[( A1′ ∩  A2′ ) ∪  A3 ] = P(shaded region) = P(awarded none) + P(A3)         
= .47 + .28 = .75 
 
Alternatively, answers to a – f can be obtained from probabilities on the accompanying 
Venn diagram 
 
 
 
 
 
55

Chapter 2:  Probability 
14. 
 
a. P(A ∪ B) = P(A) + P(B) - P(A ∩ B),  
so P(A ∩ B) = P(A) + P(B) - P(A ∪ B) 
 
       = .8 +.7 - .9 = .6 
 
b. P(shaded region) = P(A ∪ B) - P(A ∩ B) = .9 - .6 = .3 
 
Shaded region = event of interest = (A ∩ B′) ∪ (A′ ∩ B) 
 
 
15. 
 
 
a. Let event E be the event that at most one purchases an electric dryer.  Then E′ is the event 
that at least two purchase electric dryers. 
P(E′) = 1 – P(E) = 1 - .428 = .572 
 
b. Let event A be the event that all five purchase gas.  Let event B be the event that all five 
purchase electric.  All other possible outcomes are those in which at least one of each 
type is purchased.  Thus, the desired probability = 
 1 – P(A) – P(B) = 1 - .116 - .005 = .879 
 
 
16. 
 
a. There are six simple events, corresponding to the outcomes CDP, CPD, DCP, DPC, PCD, 
and PDC.  The probability assigned to each is 6
1 . 
 
b. P( C ranked first) = P( {CPD, CDP} ) = 
333
.
6
2
6
1
6
1
=
=
+
 
 
c. 
P( C ranked first and D last) = P({CPD}) = 6
1  
 
 
56

Chapter 2:  Probability 
17. 
 
a. The probabilities do not add to 1 because there are other software packages besides SPSS 
and SAS for which requests could be made. 
 
b. P(A′) = 1 – P(A) = 1 - .30 = .70 
 
c. 
P(A ∪ B) = P(A) + P(B) = .30 + .50 = .80  
(since A and B are mutually exclusive events) 
 
d. P(A′ ∩ B′) = P[(A ∪ B) ′] (De Morgan’s law) 
 
 
 = 1 - P(A ∪ B) 
 
 
    
 =1 - .80 = .20 
 
 
18. 
This situation requires the complement concept.  The only way for the desired event NOT to 
happen is if a 75 W bulb is selected first.  Let event A be that a 75 W bulb is selected first, 
and P(A) = 15
6 .  Then the desired event is event A′. 
So P(A′) = 1 – P(A) = 
60
.
1
15
9
15
6
=
=
−
 
 
 
19. 
Let event A be that the selected joint was found defective by inspector A. P(A) = 
000
,
10
724 .  Let 
event B be analogous for inspector B. P(B) = 
000
,
10
751 .  Compound event A∪B is the event that 
the selected joint was found defective by at least one of the two inspectors. P(A∪B) = 
000
,
10
1159 . 
 
a. The desired event is (A∪B)′, so we use the complement rule: 
P(A∪B)′ = 1 - P(A∪B) = 1 - 
000
,
10
1159  = 
000
,
10
8841  = .8841 
 
b. The desired event is B ∩ A′.   P(B ∩ A′) = P(B) - P(A ∩ B). 
P(A ∩ B) = P(A) + P(B) - P(A∪B), 
 
          = .0724 + .0751 - .1159 = .0316 
So P(B ∩ A′) = P(B) - P(A ∩ B) 
 
 
     = .0751 - .0316 = .0435 
 
 
20. 
Let S1, S2 and S3 represent the swing and night shifts, respectively.  Let C1 and C2 represent 
the unsafe conditions and unrelated to conditions, respectively. 
a. The simple events are {S1,C1}, {S1,C2}, {S2,C1}, {S2,C2},{S3,C1}, {S3,C2}. 
 
b. P({C1})= P({S1,C1},{S2,C1},{S3,C1})= .10 + .08 + .05 = .23 
 
c. 
P({S1}′) = 1 - P({S1,C1}, {S1,C2}) = 1 – ( .10 + .35) = .55 
 
 
 
57

Chapter 2:  Probability 
21. 
 
a. P({M,H}) = .10 
 
b. P(low auto) = P[{(L,N}, (L,L), (L,M), (L,H)}] = .04 + .06 + .05 + .03 = .18 Following a 
similar pattern, P(low homeowner’s) = .06 + .10 + .03 = .19 
 
c. 
P(same deductible for both) = P[{ LL, MM, HH }] = .06 + .20 + .15 = .41 
 
d. P(deductibles are different) = 1 – P(same deductibles) = 1 - .41 = .59 
 
e. 
P(at least one low deductible) = P[{ LN, LL, LM, LH, ML, HL }] 
      = .04 + .06 + .05 + .03 + .10 + .03 = .31 
 
f. 
P(neither low) = 1 – P(at least one low) = 1 - .31 = .69 
 
 
22. 
 
a. P(A1 ∩ A2) = P(A1) + P(A2) - P(A1 ∪ A2) = .4 + .5 - .6 = .3 
 
b. P(A1 ∩ A2′) = P(A1) - P(A1 ∩ A2) = .4 - .3 = .1 
 
c. 
P(exactly one) = P(A1 ∪ A2) - P(A1 ∩ A2) = .6 - .3 = .3 
 
 
23. 
Assume that the computers are numbered 1 – 6 as described.  Also assume that computers 1 
and 2 are the laptops.  Possible outcomes are (1,2) (1,3) (1,4) (1,5) (1,6) (2,3) (2,4) (2,5) (2,6) 
(3,4) (3,5) (3,6) (4,5) (4,6) and (5,6). 
 
a. P(both are laptops) = P[{ (1,2)}] = 15
1 =.067 
 
b. P(both are desktops) = P[{(3,4) (3,5) (3,6) (4,5) (4,6) (5,6)}] = 15
6 = .40 
 
c. 
P(at least one desktop) = 1 – P(no desktops) 
     = 1 – P(both are laptops) 
     = 1 – .067 = .933 
 
d. P(at least one of each type) =  1 – P(both are the same) 
  =  1 – P(both laptops) – P(both desktops) 
  =  1 - .067 - .40 = .533 
 
 
58

Chapter 2:  Probability 
24. 
Since A is contained in B, then B can be written as the union of A and  
(B ∩ A′), two mutually exclusive events.  (See diagram). 
From Axiom 3, P[A ∪ (B ∩ A′)] = P(A) + P(B ∩ A′). Substituting P(B), 
P(B) = P(A) + P(B ∩ A′) or P(B) - P(A) = P(B ∩ A′) .    From Axiom 1,  
P(B ∩ A′) ≥ 0, so P(B) ≥ P(A) or P(A) ≤ P(B).  For general events A and B, P(A ∩ B) ≤ P(A), 
and P(A ∪ B) ≥ P(A). 
 
 
25. 
P(A ∩ B) =  P(A) + P(B) - P(A∪B) = .65 
P(A ∩ C) = .55,  P(B ∩ C) = .60 
P(A ∩ B ∩ C) = P(A ∪ B ∪ C) – P(A) – P(B) – P(C)  
   + P(A ∩ B) + P(A ∩ C) + P(B ∩ C)  
 
 
            = .98 - .7 - .8 - .75 + .65 + .55 + .60 
 
 
            = .53 
 
a. P(A ∪ B ∪ C) = .98, as given. 
 
b. P(none selected) = 1 - P(A ∪ B ∪ C) = 1 - .98 = .02 
 
c. 
P(only automatic transmission selected) = .03 from the Venn Diagram 
 
d. P(exactly one of the three) = .03 + .08 + .13 = .24 
 
 
 
59

Chapter 2:  Probability 
26. 
 
a. P(A1′) = 1 – P(A1) = 1 - .12 = .88 
 
b. P(A1 ∩ A2 ) = P(A1) + P(A2) - P(A1 ∪ A2 ) = .12 + .07 - .13 = .06 
 
c. 
P(A1 ∩ A2 ∩ A3′) = P(A1 ∩ A2 ) -  P(A1 ∩ A2 ∩ A3 ) = .06 - .01 = .05 
 
d. P(at most two errors)  = 1 – P(all three types)  
= 1 - P(A1 ∩ A2 ∩ A3 )  
= 1 - .01 = .99 
 
 
27. 
Outcomes: 
(A,B) (A,C1) (A,C2) (A,F) (B,A) (B,C1) (B,C2) (B,F) 
(C1,A) (C1,B) (C1,C2) (C1,F) (C2,A) (C2,B) (C2,C1) (C2,F) 
(F,A) (F,B) (F,C1) (F,C2) 
a. P[(A,B) or (B,A)] = 
1.
10
1
20
2
=
=
 
 
b. P(at least one C) = 
7.
10
7
20
14
=
=
 
 
c. 
P(at least 15 years) = 1 – P(at most 14 years) 
= 1 – P[(3,6) or (6,3) or (3,7) or (7,3) or (3,10) or (10,3) or (6,7) or (7,6)] 
= 
6.
4.
1
1
20
8
=
−
=
−
 
 
 
28. 
There are 27 equally likely outcomes. 
a. P(all the same) = P[(1,1,1) or (2,2,2) or (3,3,3)] = 
9
1
27
3 =
 
 
b. P(at most 2 are assigned to the same station) = 1 – P(all 3 are the same) 
= 
9
8
27
24
27
3
1
=
=
−
 
 
c. 
P(all different) = [{(1,2,3) (1,3,2) (2,1,3) (2,3,1) (3,1,2) (3,2,1)}]  
= 
9
2
27
6 =
 
 
 
 
60

Chapter 2:  Probability 
Section 2.3 
 
29. 
 
a. (5)(4) = 20 (5 choices for president, 4 remain for vice president) 
 
b. (5)(4)(3) = 60 
c. 
10
!3!2
!5
2
5
=
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 (No ordering is implied in the choice) 
 
30. 
 
a. Because order is important, we’ll use P8,3 = 8(7)(6) = 336. 
 
b. Order doesn’t matter here, so we use C30,6 = 593,775. 
 
c. 
From each group we choose 2:  
 
160
,
83
2
12
2
10
2
8
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
•
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
•
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
d. The numerator comes from part c and the denominator from part b:  
14
.
775
,
593
160
,
83
=
 
e. 
We use the same denominator as in part d.  We can have all zinfandel, all merlot, or all 
cabernet, so  P(all same) = P(all z) + P(all m) + P(all c) = 
002
.
775
,
593
1162
6
30
6
12
6
10
6
8
=
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
 
 
31. 
 
a. (n1)(n2) = (9)(27) = 243 
 
b. (n1)(n2)(n3) = (9)(27)(15) = 3645, so such a policy could be carried out for 3645 
successive nights, or approximately 10 years, without repeating exactly the same 
program. 
 
61

Chapter 2:  Probability 
 
32. 
 
a. 5×4×3×4 = 240 
 
b. 1×1×3×4 = 12 
 
c. 
4×3×3×3 = 108 
 
d. # with at least on Sony = total # - # with no Sony = 240 – 108 = 132 
 
e. 
P(at least one Sony) = 
55
.
240
132 =
 
 
P(exactly one Sony) = P(only Sony is receiver) 
 
 
 
+ P(only Sony is CD player) 
 
 
 
+ P(only Sony is deck) 
 
 
 
413
.
240
99
240
36
36
27
240
1
3
3
4
240
3
3
1
4
240
3
3
3
1
=
=
+
+
=
×
×
×
+
×
×
×
+
×
×
×
=
 
 
 
33. 
 
a. 
130
,
53
!
20
!5
!
25
5
25
=
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
 
b. 
 
1190
1
17
4
8
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
•
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
c. 
P(exactly 4 have cracks) = 
022
.
130
,
53
1190
5
25
1
17
4
8
=
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
 
d. P(at least 4) = P(exactly 4) + P(exactly 5) 
= 
023
.
001
.
022
.
5
25
0
17
5
8
5
25
1
17
4
8
=
+
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
 
 
 
62

Chapter 2:  Probability 
34. 
 
a. 
 P(all from day shift) = 
.
760
,
38
6
20 =
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
0048
.
060
,
145
,8
760
,
38
6
45
0
25
6
20
=
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
 
b. P(all from same shift) =
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
6
45
0
35
6
10
6
45
0
30
6
15
6
45
0
25
6
20
 
      = .0048 + .0006 + .0000 = .0054 
 
c. 
P(at least two shifts represented) = 1 – P(all from same shift) 
 
 
 
 
 
  = 1 - .0054 = .9946 
 
d. Let A1 = day shift unrepresented, A2 = swing shift unrepresented, and A3 = graveyard 
shift unrepresented.  Then we wish P(A1 ∪ A2 ∪ A3). 
P(A1) = P(day unrepresented) = P(all from swing and graveyard) 
P(A1) = 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
6
45
6
25
,  
 
P(A2) = 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
6
45
6
30
,   
P(A3) = 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
6
45
6
35
, 
 
P(A1 ∩ A2) = P(all from graveyard) = 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
6
45
6
10
 
P(A1 ∩ A3) = 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
6
45
6
15
,  
P(A2 ∩ A3) = 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
6
45
6
20
, 
P(A1 ∩ A2 ∩ A3) = 0, 
So P(A1 ∪ A2 ∪ A3) = 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
6
45
6
25
+ 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
6
45
6
30
+ 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
6
45
6
35
- 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
6
45
6
10
- 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
6
45
6
15
- 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
6
45
6
20
 
 
 
 
= .2939 - .0054 = .2885 
 
 
 
63

Chapter 2:  Probability 
35. 
There are 10 possible outcomes -- 
 ways to select the positions for B’s votes:  BBAAA, 
BABAA, BAABA, BAAAB, ABBAA, ABABA, ABAAB, AABBA, AABAB, and AAABB.  
Only the last two have A ahead of B throughout the vote count.  Since the outcomes are 
equally likely, the desired probability is 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
2
5
20
.
10
2 =
. 
 
 
36. 
 
a. n1 = 3, n2 = 4, n3 = 5, so n1 × n2 × n3 = 60 runs 
 
b. n1 = 1, (just one temperature), n2 = 2, n3 = 5 implies that there are 10 such runs. 
 
 
37. 
There are 
⎟ways to select the 5 runs.  Each catalyst is used in 12 different runs, so the 
number of ways of selecting one run from each of these 5 groups is 12
⎟
⎠
⎞
⎜⎜
⎝
⎛
5
60
5.  Thus the desired 
probability is 
0456
.
5
60
125
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
. 
 
 
38. 
 
a. P(selecting 2 -  75 watt bulbs) = 
2967
.
455
9
15
3
15
1
9
2
6
=
⋅
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
 
b. P(all three are the same) = 
0747
.
455
20
10
4
3
15
3
6
3
5
3
4
=
+
+
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
 
c. 
2637
.
455
120
1
6
1
5
1
4
=
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
 
 
64

Chapter 2:  Probability 
d. To examine exactly one, a 75 watt bulb must be chosen first. (6 ways to accomplish this).  
To examine exactly two, we must choose another wattage first, then a 75 watt. ( 9 × 6 
ways).  Following the pattern, for exactly three, 9 × 8 × 6 ways; for four, 9 × 8 × 7 × 6; 
for five, 9 × 8 × 7 × 6 × 6.   
 
 P(examine at least 6 bulbs) = 1 – P(examine 5 or less)  
 
 
 
 
   = 1 – P( examine exactly 1 or 2 or 3 or 4 or 5) 
 
 
 
 
   = 1 – [P(one) + P(two) + … + P(five)]  
 
⎥⎦
⎤
⎢⎣
⎡
×
×
×
×
×
×
×
×
+
×
×
×
×
×
×
+
×
×
×
×
+
×
×
+
−
=
11
12
13
14
15
6
6
7
8
9
12
13
14
15
6
7
8
9
13
14
15
6
8
9
14
15
6
9
15
6
1
 
 
= 1 – [.4 + .2571 + .1582 + .0923 + .0503] 
= 1 - .9579 = .0421 
 
 
 
39. 
 
a. We want to choose all of the 5 cordless, and 5 of the 10 others, to be among the first 10 
serviced, so the desired probability is 
0839
.
3003
252
10
15
5
10
5
5
=
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
 
b. Isolating one group, say the cordless phones, we want the other two groups represented in 
the last 5 serviced.  So we choose 5 of the 10 others, except that we don’t want to include 
the outcomes where the last five are all the same.   
So we have 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
5
15
2
5
10
. But we have three groups of phones, so the desired probability is 
2498
.
3003
)
250
(
3
5
15
2
5
10
3
=
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎥
⎦
⎤
⎢
⎣
⎡
−
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⋅
. 
 
c. 
We want to choose 2 of the 5 cordless, 2 of the 5 cellular, and 2 of the corded phones: 
1998
.
5005
1000
6
15
2
5
2
5
2
5
=
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
 
 
 
65

Chapter 2:  Probability 
40. 
 
a. If the A’s are distinguishable from one another, and similarly for the B’s, C’s and D’s, 
then there are 12! Possible chain molecules.  Six of these are: 
A1A2A3B2C3C1D3C2D1D2B3B1, A1A3A2B2C3C1D3C2D1D2B3B1
A2A1A3B2C3C1D3C2D1D2B3B1, A2A3A1B2C3C1D3C2D1D2B3B1
A3A1A2B2C3C1D3C2D1D2B3B1, A3A2A1B2C3C1D3C2D1D2B3B1
These 6 (=3!) differ only with respect to ordering of the 3 A’s.  In general, groups of 6 
chain molecules can be created such that within each group only the ordering of the A’s 
is different.  When the A subscripts are suppressed, each group of 6 “collapses” into a 
single molecule (B’s, C’s and D’s are still distinguishable).  At this point there are 
!3
!
12 molecules.  Now suppressing subscripts on the B’s, C’s and D’s in turn gives 
ultimately 
600
,
369
4)!3
(
!
12 =
 chain molecules. 
 
b. Think of the group of 3 A’s as a single entity, and similarly for the B’s, C’s, and D’s.  
Then there are 4! Ways to order these entities, and thus 4! Molecules in which the A’s are 
contiguous, the B’s, C’s, and D’s are also.  Thus, P(all together) = 
00006494
.
600
.
369
!
4
=
. 
 
 
41. 
 
a. P(at least one F among 1st 3) = 1 – P(no F’s among 1st 3) 
= 1 - 
9286
.
0714
.
1
336
24
1
6
7
8
2
3
4
=
−
=
−
=
×
×
×
×
 
An alternative method to calculate P(no F’s among 1st 3) 
would be to choose none of the females and 3 of the 4 males, as follows: 
0714
.
56
4
3
8
3
4
0
4
=
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
, obviously producing the same result. 
 
b. P(all F’s among 1st 5) = 
0714
.
56
4
5
8
1
4
4
4
=
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
 
c. 
P(orderings are different) = 1 – P(orderings are the same for both semesters) 
= 1 – (# orderings such that the orders are the same each semester)/(total # of 
possible orderings for 2 semesters) 
99997520
.
)1
2
3
4
5
6
7
8
(
)
1
2
3
4
5
6
7
8
(
1
2
3
4
5
6
7
8
1
=
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
−
=
 
 
 
 
66

Chapter 2:  Probability 
42. 
Seats: 
 
 
P(J&P in 1&2)  
0667
.
15
1
1
2
3
4
5
6
1
2
3
4
1
2
=
=
×
×
×
×
×
×
×
×
×
×
=
 
 
P(J&P next to each other)  = P(J&P in 1&2) +  … + P(J&P in 5&6) 
 
 
 
 
= 
333
.
3
1
15
1
5
=
=
×
 
P(at least one H next to his W) = 1 – P( no H next to his W) 
We count the # of ways of no H next to his W as follows: 
# if orderings without a H-W pair in seats #1 and 3 and no H next to his W = 6* × 4 × 1* × 2# 
× 1 × 1  = 48 
*= pair, # =can’t put the mate of seat #2 here or else a H-W pair would be in #5 and 6. 
 
 
 
# of orderings without a H-W pair in seats #1 and 3, and no H next to his W = 6 × 4 × 2# × 2 × 
2 × 1 = 192 
#= can’t be mate of person in seat #1 or #2. 
So, # of seating arrangements with no H next to W = 48 + 192 = 240 
And P(no H next to his W) = 
3
1
1
2
3
4
5
6
240
=
×
×
×
×
×
=
, so 
P(at least one H next to his W) = 1 - 
3
2
3
1 =
 
 
 
 
43. 
# of 10 high straights = 4×4×4×4×4 ( 4 – 10’s, 4 – 9’s , etc) 
P(10 high straight) = 
000394
.
960
,
598
,2
1024
5
52
45
=
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
P(straight) = 
003940
.
5
52
4
10
5
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
×
 (Multiply by 10 because there are 10 different card 
values that could be high: Ace, King, etc.)  There are only 40 straight flushes (10 in each suit), 
so  
P(straight flush) = 
00001539
.
5
52
40 =
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
 
 
 
67

Chapter 2:  Probability 
44. 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
=
−
=
−
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
k
n
n
k
k
n
n
k
n
k
n
k
n
!
)!
(
!
)!
(!
!
 
 
The number of subsets of size k = the number of subsets of size n-k, because to each subset of 
size k there corresponds exactly one subset of size n-k (the n-k objects not in the subset of 
size k). 
 
 
Section 2.4 
 
45. 
 
a. P(A) =  .106 + .141 + .200 = .447, P(C) =.215 + .200 + .065 + .020 = .500  P(A ∩ C) = 
.200 
 
b. P(A|C) = 
400
.
500
.
200
.
)
(
)
(
=
=
∩
C
P
C
A
P
.  If we know that the individual came from ethnic 
group 3, the probability that he has type A blood is .40. P(C|A) = 
447
.
447
.
200
.
)
(
)
(
=
=
∩
A
P
C
A
P
.  If a person has type A blood, the probability that he is 
from ethnic group 3 is .447 
 
c. 
Define event D = {ethnic group 1 selected}.   We are asked for P(D|B′) = 
400
.
500
.
200
.
)
(
)
(
=
=
′
′
∩
B
P
B
D
P
.  P(D∩B′)=.082 + .106 + .004 = .192, P(B′) = 1 – P(B) = 
1 – [.008 + .018 + .065] = .909 
 
 
 
46. 
Let event A be that the individual is more than 6 feet tall.  Let event B be that the individual is 
a professional basketball player. Then  P (A⏐B) = the probability of the individual being more 
than 6 feet tall, knowing that the individual is a professional basketball player, and P (B⏐A) = 
the probability of the individual being a professional basketball player, knowing that the 
individual is more than 6 feet tall.   P (A⏐B) will be larger. Most professional BB players are 
tall, so the probability of an individual in that reduced sample space being more than 6 feet 
tall is very large.  The number of individuals that are pro BB players is small in relation to the 
# of males more than 6 feet tall. 
 
 
68

Chapter 2:  Probability 
47. 
 
 
a. P(B⏐A) = 
50
.
50
.
25
.
)
(
)
(
=
=
∩
A
P
B
A
P
 
 
b. P(B′⏐A) = 
50
.
50
.
25
.
)
(
)
(
=
=
′
∩
A
P
B
A
P
 
 
c. 
P(A⏐B) = 
6125
.
40
.
25
.
)
(
)
(
=
=
∩
B
P
B
A
P
 
 
d. P(A′⏐B) = 
3875
.
40
.
15
.
)
(
)
(
=
=
∩
′
B
P
B
A
P
 
e. 
P(A⏐A∪B) = 
7692
.
65
.
50
.
)
(
)]
(
[
=
=
∪
∪
∩
B
A
P
B
A
A
P
 
 
 
48. 
 
a. P(A2⏐A1) = 
50
.
12
.
06
.
)
(
)
(
1
2
1
=
=
∩
A
P
A
A
P
 
b. P(A1 ∩ A2 ∩ A3⏐A1) = 
0833
.
12
.
01
.
=
 
 
c. 
We want P[(exactly one) ⏐ (at least one)].   
P(at least one)  
= P(A1 ∪ A2 ∪ A3) 
 
 
 
= .12 + .07 + .05 - .06 - .03 - .02 + .01 = .14 
 
Also notice that the intersection of the two events is just the 1st event, since “exactly one” 
is totally contained in “at least one.”   
So P[(exactly one) ⏐ (at least one)]= 
3571
.
14
.
01
.
04
.
=
+
 
 
d. The pieces of this equation can be found in your answers to exercise 26 (section 2.2): 
833
.
06
.
05
.
)
(
)
(
)
|
(
2
1
3
2
1
2
1
3
=
=
∩
′
∩
∩
=
∩
′
A
A
P
A
A
A
P
A
A
A
P
 
 
69

Chapter 2:  Probability 
49. 
The first desired probability is P(both bulbs are 75 watt⏐at least one is  75 watt). 
P(at least one is  75 watt)   = 1 – P(none are 75 watt) 
 
 
 
 
= 1 - 
105
69
105
36
1
2
15
2
9
=
−
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
. 
Notice that P[(both are 75 watt)∩(at least one is 75 watt)]  
= P(both are 75 watt) =  
105
15
2
15
2
6
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
.  
So P(both bulbs are 75 watt⏐at least one is  75 watt) = 
2174
.
69
15
105
69
105
15
=
=
 
Second, we want P(same rating⏐ at least one NOT 75 watt). 
P(at least one NOT 75 watt) = 1 – P(both are 75 watt) 
 
 
 
 
 
= 1 - 
105
90
105
15 =
. 
Now, P[(same rating)∩(at least one not 75 watt)] = P(both 40 watt or both 60 watt). 
P(both 40 watt or both 60 watt) = 
105
16
2
15
2
5
2
4
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
Now, the desired conditional probability is 
1778
.
90
16
105
90
105
16
=
=
 
 
50. 
 
a. P(M ∩ LS ∩ PR) = .05, directly from the table of probabilities 
 
b. P(M ∩ Pr) = P(M,Pr,LS) + P(M,Pr,SS) = .05+.07=.12 
 
c. 
P(SS) = sum of 9 probabilities in SS table = 56, P(LS) = 1 = .56 = .44 
 
d. P(M) = .08+.07+.12+.10+.05+.07 = .49 
P(Pr) = .02+.07+.07+.02+.05+.02 = .25 
 
70

Chapter 2:  Probability 
 
e. 
P(M|SS ∩ Pl) = 
533
.
03
.
08
.
04
.
08
.
)
(
)
(
=
+
+
=
∩
∩
∩
Pl
SS
P
Pl
SS
M
P
 
 
f. 
P(SS|M ∩ Pl) = 
444
.
10
.
08
.
08
.
)
(
)
(
=
+
=
∩
∩
∩
Pl
M
P
Pl
M
SS
P
 
 
P(LS|M  Pl) = 1 - P(SS|M  Pl) = 1 - .444 = .556 
 
 
51. 
 
a. P(R from 1st ∩ R from 2nd ) = P(R from 2nd | R from 1st ) • P(R from 1st ) 
   = 
436
.
10
6
11
8
=
•
 
b. P(same numbers)  
= P(both selected balls are the same color) 
 
 
 
= P(both red) + P(both green) = 
581
.
10
4
11
4
436
.
=
•
+
 
 
 
 
52. 
Let A1 be the event that #1 fails and A2 be the event that #2 fails.  We assume that P(A1) = 
P(A2) = q and that P(A1 | A2) = P(A2 | A1) = r.  Then one approach is as follows:   
P(A1 ∩ A2) = P(A2 | A1) • P(A1) = rq = .01 
P(A1 ∪ A2) = P(A1 ∩ A2) + P(A1′∩  A2) + P(A1 ∩ A2′) = rq + 2(1-r)q = .07 
These two equations give 2q - .01 = .07, from which q = .04 and r = .25.  Alternatively, with t 
= P(A1′∩  A2) = P(A1 ∩ A2′) , t + .01 + t = .07, implying t = .03 and thus q = .04 without 
reference to conditional probability. 
 
 
53. 
P(B⏐A) = 
)
(
)
(
)
(
)
(
A
P
B
P
A
P
B
A
P
=
∩
 (since B is contained in A, A ∩ B = B) 
= 
0833
.
60
.
05
.
=
 
 
 
71

Chapter 2:  Probability 
54. 
P(A1) = .22, P(A2) = .25, P(A3) = .28, P(A1 ∩ A2) = .11, P(A1 ∩ A3) = .05, P(A2 ∩ A3) = .07,  
P(A1 ∩ A2 ∩ A3) = .01 
 
a. P(A2⏐A1) = 
50
.
22
.
11
.
)
(
)
(
1
2
1
=
=
∩
A
P
A
A
P
 
 
b. P(A2 ∩ A3⏐A1) = 
0455
.
22
.
01
.
)
(
)
(
1
3
2
1
=
=
∩
∩
A
P
A
A
A
P
 
 
c. 
)
(
)]
(
)
[(
)
(
)]
(
[
)
|
(
1
3
1
2
1
1
3
2
1
1
3
2
A
P
A
A
A
A
P
A
P
A
A
A
P
A
A
A
P
∩
∪
∩
=
∪
∩
=
∪
 
682
.
22
.
15
.
)
(
)
(
)
(
)
(
1
3
2
1
3
1
2
1
=
=
∩
∩
−
∩
+
∩
=
A
P
A
A
A
P
A
A
P
A
A
P
 
 
d. 
0189
.
53
.
01
.
)
(
)
(
)
|
(
3
2
1
3
2
1
3
2
1
3
2
1
=
=
∪
∪
∩
∩
=
∪
∪
∩
∩
A
A
A
P
A
A
A
P
A
A
A
A
A
A
P
 
 
This is the probability of being awarded all three projects given that at least one project 
was awarded. 
 
 
55. 
 
a. P(A  B) = P(B|A)•P(A) = 
0111
.
5
6
1
2
3
4
1
2
=
×
×
×
×
×
 
 
b. P(two other H’s next to their wives | J and M together in the middle)  
)
.
.
.
.
.
(
)]
.
.
(
)
.
.
(
)
.
.
[(
middle
the
in
J
M
or
M
J
P
H
W
or
W
H
and
J
M
or
M
J
and
H
W
or
W
H
P
−
−
−
−
−
−
−
−
 
numerator = 
!6
16
1
2
3
4
5
6
1
2
1
2
1
4
=
×
×
×
×
×
×
×
×
×
×
 
denominator = 
!6
48
1
2
3
4
5
6
1
2
1
2
3
4
=
×
×
×
×
×
×
×
×
×
×
 
so the desired probability = 
3
1
48
16 =
. 
 
 
72

Chapter 2:  Probability 
c. 
P(all H’s next to W’s | J & M together)  
= P(all H’s next to W’s – including J&M)/P(J&M together) 
=
2.
240
48
!6
1
2
3
4
1
2
5
!6
1
2
1
4
1
6
=
=
×
×
×
×
×
×
×
×
×
×
×
 
 
 
56. 
If P(B|A) > P(B), then P(B’|A) < P(B’). 
Proof by contradiction.  
Assume    P(B’|A) ≥ P(B’). 
Then 
1 – P(B|A) ≥ 1 – P(B). 
    - P(B|A) ≥  – P(B). 
      P(B|A) ≤ P(B). 
This contradicts the initial condition, therefore P(B’|A) < P(B’). 
 
 
57. 
1
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
|
(
)
|
(
=
=
∩
′
+
∩
=
∩
′
+
∩
=
′
+
B
P
B
P
B
P
B
A
P
B
A
P
B
P
B
A
P
B
P
B
A
P
B
A
P
B
A
P
 
 
58. 
)
(
)]
(
)
[(
)
(
)
)
[(
)
|
(
C
P
C
B
C
A
P
C
P
C
B
A
P
C
B
A
P
∩
∪
∩
=
∩
∪
=
∪
 
)
(
)
(
)
(
)
(
C
P
C
B
A
P
C
B
P
C
A
P
∩
∩
−
∩
+
∩
=
 
= P(A|C) + P(B|C) – P(A ∩ B | C) 
 
 
 
 
73

Chapter 2:  Probability 
59. 
 
)
|
(
)
(
)
(
12
.
3.
4.
1
1
A
B
P
A
P
B
A
P
•
=
∩
=
=
×
)
(
21
.
6.
35
.
2
B
A
P
∩
=
=
×
)
(
125
.
5.
25
.
3
B
A
P
∩
=
=
×
a.  P(A2 ∩ B) = .21 
 
b. P(B) = P(A1 ∩ B) + P(A2 ∩ B) + P(A3 ∩ B) = .455 
 
c. 
P(A1|B) = 
264
.
455
.
12
.
)
(
)
(
1
=
=
∩
B
P
B
A
P
 
P(A2|B) = 
462
.
455
.
21
.
=
, P(A3|B) = 1 - .264 - .462 = .274 
 
 
60. 
 
a. P(not disc | has loc) = 
 
067
.
42
.
03
.
03
.
)
.
(
)
.
.
(
=
+
=
∩
loc
has
P
loc
has
disc
not
P
 
 
b. P(disc | no loc) = 
509
.
55
.
28
.
)
.
(
)
.
(
=
=
∩
loc
no
P
loc
no
disc
P
 
 
 
74

Chapter 2:  Probability 
61. 
P(0 def in sample | 0 def in batch) = 1 
 
P(0 def in sample | 1 def in batch) = 
800
.
2
10
2
9
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
P(1 def in sample | 1 def in batch) = 
200
.
2
10
1
9
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
P(0 def in sample | 2 def in batch) = 
622
.
2
10
2
8
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
P(1 def in sample | 2 def in batch) = 
356
.
2
10
1
8
1
2
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
 
P(2 def in sample | 2 def in batch) = 
022
.
2
10
1
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
a. P(0 def in batch | 0 def in sample) = 
578
.
1244
.
24
.
5.
5.
=
+
+
 
P(1 def in batch | 0 def in sample) = 
278
.
1244
.
24
.
5.
24
.
=
+
+
 
P(2 def in batch | 0 def in sample) = 
144
.
1244
.
24
.
5.
1244
.
=
+
+
 
 
 
75

Chapter 2:  Probability 
b. P(0 def in batch | 1 def in sample) = 0 
P(1 def in batch | 1 def in sample) = 
457
.
0712
.
06
.
06
.
=
+
 
P(2 def in batch | 1 def in sample) = 
543
.
0712
.
06
.
0712
.
=
+
 
 
 
62. 
Using a tree diagram, B = basic, D = deluxe, W = warranty purchase, W’ = no warranty  
We want P(B|W) = 
2857
.
42
.
12
.
12
.
30
.
12
.
)
(
)
(
=
=
+
=
∩
W
P
W
B
P
 
 
 
)
(
12
.
3.
4.
W
B
P
∩
=
=
×
)
(
28
.
7.
4.
W
B
P
′
∩
=
=
×
)
(
30
.
5.
6.
W
D
P
∩
=
=
×
)
(
30
.
5.
6.
W
D
P
′
∩
=
=
×
 
76

Chapter 2:  Probability 
63. 
 
a.  
 
b. P(A ∩ B ∩ C) = .75 × .9 × .8 = .5400 
 
c. 
P(B ∩ C) = P(A ∩ B ∩ C) + P(A′ ∩ B ∩ C) 
          =.5400+.25×.8×.7 = .6800 
 
d. P(C) = P(A ∩ B ∩ C)+P(A′ ∩ B ∩ C) + P(A ∩ B′ ∩ C) + P(A′ ∩ B′ ∩ C) 
  = .54+.045+.14+.015 = .74 
e. 
P(A|B ∩ C) = 
7941
.
68
.
54
.
)
(
)
(
=
=
∩
∩
∩
C
B
P
C
B
A
P
 
 
 
 
77

Chapter 2:  Probability 
64. 
 
 
a. P(+) = .0588 
 
b. P(has d | +) = 
6735
.
0588
.
0396
.
=
 
 
c. 
P(doesn’t have d | - ) = 
9996
.
9412
.
9408
.
=
 
 
 
65. 
 
 
P(satis) = .51 
P(mean | satis) = 
3922
.
51
.
2.
=
 
P(median | satis) = .2941 
P(mode | satis) = .3137 
So Mean (and not Mode!) is the most likely author, while Median is least. 
 
 
78

Chapter 2:  Probability 
 
66. 
Define events A1, A2, and A3 as flying with airline 1, 2, and 3, respectively.  Events 0, 1, and 
2 are 0, 1, and 2 flights are late, respectively.  Event DC = the event that the flight to DC is 
late, and event LA = the event that the flight to LA is late. Creating a tree diagram as 
described in the hint, the probabilities of the second generation branches are calculated as 
follows:  For the A1 branch, P(0|A1) = P[DC′∩LA′] = P[DC′] ⋅ P[LA′] = (.7)(.9) = .63;                    
P(1|A1) = P[(DC′∩LA) ∪ (DC∩LA′)] = (.7)(.1) + (.3)(.9) = .07 + .27 = .34; P(2|A1) = 
P[DC∩LA] = P[DC] ⋅ P[LA] = (.3)(.1) = .03 
Follow a similar pattern for A2 and A3.   
 
From the law of total probability, we know that  
P(1) = P(A1∩1) + P(A2∩1) + P(A2∩1)  
= (from tree diagram below) .170 + .105 + .09 = .365. 
 
 
 
We wish to find P(A1|1),  P(A2|1),  and P(A2|1).    
 
 
 
 
P(A1/1) = 
466
.
365
.
170
.
)1(
)1
1
(
=
=
∩
P
A
P
;  
P(A2/1) = 
288
.
365
.
105
.
)1(
)1
2
(
=
=
∩
P
A
P
;  
P(A3/1) = 
247
.
365
.
090
.
)1(
)1
3
(
=
=
∩
P
A
P
;  
 
 
 
79

Chapter 2:  Probability 
67. 
 
a. P(U ∩ F ∩ Cr) = .1260 
 
b. P(Pr ∩ NF ∩ Cr) = .05 
 
c. 
P(Pr ∩ Cr) = .0625 + .05 = .1125 
 
d. P(F ∩ Cr) = .0840 + .1260 + .0625 = .2725 
 
e. 
P(Cr) = .5325 
 
f. 
P(PR | Cr) = 
2113
.
5325
.
1125
.
)
(
)
(Pr
=
=
∩
Cr
P
Cr
P
 
 
 
 
80

Chapter 2:  Probability 
Section 2.5 
 
68. 
Using the definition, two events A and B are independent if P(A|B) = P(A); 
P(A|B) = .6125; P(A) = .50;  .6125 ≠ .50, so A and B are dependent. 
Using the multiplication rule, the events are independent if  
P(A ∩ B)=P(A)• P(B); 
P(A ∩ B) = .25; P(A) • P(B) = (.5)(.4) = .2.  .25 ≠ .2, so A and B are dependent. 
 
 
69. 
 
a. Since the events are independent, then A′ and B′ are independent, too. (see paragraph 
below equation 2.7.  P(B′|A′) = .  P(B′) = 1 - .7 = .3 
 
b. P(A ∪ B)=P(A) + P(B) – P(A)⋅P(B) = .4 + .7 + (.4)(.7) = .82 
 
c. 
P(AB′| A∪ B) = 
146
.
82
.
12
.
)
(
)
(
)
(
))
(
(
=
=
∪
′
=
∪
∪
∩
′
B
A
P
B
A
P
B
A
P
B
A
B
A
P
 
 
 
70. 
P(A1 ∩ A2) = .11, P(A1) • P(A2) = .055.  A1 and A2 are not independent. 
P(A1 ∩ A3) = .05, P(A1) • P(A3) = .0616.  A1 and A3 are not independent. 
P(A2 ∩ A3) = .07, P(A1) • P(A3) = .07.  A2 and A3 are independent. 
 
 
71. 
P(A′ ∩ B) = P(B) – P(A ∩ B) = P(B) - P(A) • P(B) = [1 – P(A)] • P(B) = P(A′)• P(B). 
Alternatively, 
)
(
)
(
)
(
)
(
)
(
)
|
(
B
P
B
A
P
B
P
B
P
B
A
P
B
A
P
∩
−
=
∩
′
=
′
 
).
(
)
(
1
)
(
)
(
)
(
)
(
A
P
A
P
B
P
B
P
A
P
B
P
′
=
−
=
⋅
−
=
 
 
 
72. 
Using subscripts to differentiate between the selected individuals,  
P(O1 ∩ O2) = P(O1)•P(O2) = (.44)(.44) = .1936 
P(two individuals match) = P(A1∩A2)+P(B1∩B2) + P(AB1∩AB2) + P(O1∩O2)  
 
 
 
     = .422 + .102 + .042 + .442 = .3816 
 
 
73. 
Let event E be the event that an error was signaled incorrectly.  We want P(at least one 
signaled incorrectly) = P(E1 ∪ E2  ∪ …∪ E10) = 1 - P(E1′ ∩ E2′  ∩ …∩ E10′) .   P(E′) =1 - .05 
= .95.  For 10 independent points, P(E1′ ∩ E2′  ∩ …∩ E10′) = P(E1′ )P(E2′ )…P(E10′) so = P(E1 
∪ E2  ∪ …∪ E10) = 1 -  [.95]10 = .401.   Similarly, for 25 points, the desired probability is =1 -  
[P(E′)]25 =1 -  (.95)25=.723 
 
81

Chapter 2:  Probability 
 
74. 
P(no error on any particular question) = .9, so P(no error on any of the 10 questions) =(.9)10 = 
.3487.  Then P(at least one error) = 1 – (.9)10 = .6513.  For p replacing .1, the two probabilities 
are (1-p)n and 1 – (1-p)n. 
 
 
75. 
Let q denote the probability that a rivet is defective. 
 
a. P(seam need rework) = .20 = 1 – P(seam doesn’t need rework) 
= 1 – P(no rivets are defective) 
= 1 – P(1st isn’t def ∩ … ∩ 25th isn’t def) 
= 1 – (1 – q)25, so .80 = (1 – q)25, 1 – q = (.80)1/25, and thus q = 1 - 
.99111 = .00889.  
 
b. The desired condition is .10 = 1 – (1 – q)25, i.e. (1 – q)25 = .90, from which q = 1 - .99579 
= .00421.  
 
 
76. 
P(at least one opens) = 1 – P(none open) = 1 – (.05)5 = .99999969 
P(at least one fails to open) = 1 = P(all open) = 1 – (.95)5 = .2262 
 
 
77. 
Let A1 = older pump fails, A2 = newer pump fails, and x = P(A1 ∩ A2).  Then P(A1) = .10 + x, 
P(A2) = .05 + x, and x = P(A1 ∩ A2) = P(A1) •P(A2) = (.10 + x)( .05 + x) .  The resulting 
quadratic equation, x2 - .85x + .005 = 0, has roots x = .0059 and x = .8441.  Hopefully the 
smaller root is the actual probability of system failure. 
 
 
78. 
P(system works) = P( 1 – 2 works ∪ 3 – 4 works) 
= P( 1 – 2 works) + P( 3 – 4 works) - P( 1 – 2 works ∩ 3 – 4 works) 
= P(1 works ∪ 2 works) + P(3 works ∩ 4 works) – P( 1 – 2 ) • P(3 – 4) 
= ( .9+.9-.81) + (.9)(.9) – (.9+.9-.81)(.9)(.9) 
= .99 + .81 - .8019 = .9981 
 
 
 
82

Chapter 2:  Probability 
79. 
 
 
Using the hints, let P(Ai) = p, and x = p2, then P(system lifetime exceeds t0)   = p2 + p2 – p4 = 
2p2 – p4 = 2x – x2.  Now, set this equal to .99, or 2x – x2 = .99 ⇒ x2 – 2x + .99 = 0.  Use the 
quadratic formula to solve for x:  
1.
1
2
2.
2
2
)
99
)(.
4
(
4
2
±
=
±
=
−
±
=
 = .99 or 1.01  
Since the value we want is a probability, and has to be ≤ 1, we use the value of .99. 
 
 
80. 
Event A:  { (3,1)(3,2)(3,3)(3,4)(3,5)(3,6) }, P(A) = 6
1 ;   
Event B:  { (1,4)(2,4)(3,4)(4,4)(5,4)(6,4) }, P(B) = 6
1 ;  
Event C:  { (1,6)(2,5)(3,4)(4,3)(5,2)(6,1) }, P(C) = 6
1 ;     
Event A∩B: { (3,4) }; P(A∩B) = 36
1 ; 
Event A∩C: { (3,4) }; P(A∩C) = 36
1 ; 
Event B∩C: { (3,4) }; P(A∩C) = 36
1 ; 
Event A∩B∩C: { (3,4) }; P(A∩B∩C) = 36
1 ; 
P(A)⋅P(B)=  
36
1
6
1
6
1
=
⋅
=P(A∩B) 
P(A)⋅P(C)=  
36
1
6
1
6
1
=
⋅
=P(A∩C) 
P(B)⋅P(C)=  
36
1
6
1
6
1
=
⋅
=P(B∩C) 
The events are pairwise independent. 
P(A)⋅P(B) ⋅P(C)=  
36
1
216
1
6
1
6
1
6
1
≠
=
⋅
⋅
= P(A∩B∩C) 
The events are not mutually independent 
 
 
 
83

Chapter 2:  Probability 
81. 
P(both detect the defect) = 1 – P(at least one doesn’t) = 1 - .2 = .8 
 
a. P(1st detects ∩ 2nd doesn’t) = P(1st detects) – P(1st does ∩ 2nd does) 
  = .9 - .8 = .1 
Similarly, P(1st doesn’t ∩ 2nd does) = .1, so P(exactly one does)= .1+.1= .2 
 
b. P(neither detects a defect) = 1 – [P(both do) + P(exactly 1 does)] 
 = 1 – [.8+.2] = 0 
 
 
 
so P(all 3 escape) = (0)(0)(0) = 0. 
 
 
82. 
P(pass) = .70 
 
a. (.70)(.70)(.70) = .343 
 
b. 1 – P(all pass) = 1 - .343 = .657 
 
c. 
P(exactly one passes) = (.70)(.30)(.30) + (.30)(.70)(.30) + (.30)(.30)(.70) = .189 
 
d. P(# pass ≤ 1) = P(0 pass) + P(exactly one passes) = (.3)3 + .189 = .216 
 
e. 
P(3 pass | 1 or more pass) = 
353
.
973
.
343
.
)
.1
(
)
.3
(
)
.1
(
)
.1
.3
(
=
=
≥
=
≥
≥
∩
=
pass
P
pass
P
pass
P
pass
pass
P
 
 
 
83. 
 
a. Let D1 = detection on 1st fixation, D2 = detection on 2nd fixation. 
P(detection in at most 2 fixations) = P(D1) + P(D1′ ∩ D2) 
 
 
 
 
= P(D1) + P(D2 | D1′ )P(D1) 
 
 
 
 
= p + p(1 – p) = p(2 – p). 
 
b. Define D1, D2, … , Dn as in a.  Then P(at most n fixations) 
= P(D1) + P(D1′ ∩ D2) + P(D1′ ∩ D2′ ∩  D3) + …+  P(D1′ ∩ D2′ ∩ … ∩ Dn-1′ ∩  Dn)  
= p + p(1 – p) + p(1 – p)2 + … + p(1 – p)n-1
= p [ 1 + (1 – p) + (1 – p)2 + … + (1 – p)n-1] = 
n
n
p
p
p
p
)
1(
1
)
1(
1
)
1(
1
−
−
=
−
−
−
−
•
 
Alternatively, P(at most n fixations) = 1 – P(at least n+1 are req’d) 
 
 
 
 
= 1 – P(no detection in 1st n fixations) 
 
 
 
 
= 1 – P(D1′ ∩ D2′ ∩ … ∩ Dn′ ) 
 
 
 
 
= 1 – (1 – p)n
 
c. 
P(no detection in 3 fixations) = (1 – p)3 
 
 
84

Chapter 2:  Probability 
d. P(passes inspection) = P({not flawed} ∪ {flawed and passes}) 
= P(not flawed) + P(flawed and passes) 
= .9 + P(passes | flawed)• P(flawed) = .9+(1 – p)3(.1) 
 
e. 
P(flawed | passed) = 
3
3
)
1(1.
9.
)
1(1.
)
(
)
(
p
p
passed
P
passed
flawed
P
−
+
−
=
∩
 
 
For p = .5, P(flawed | passed) = 
0137
.
)
5
(.
1.
9.
)
5
(.
1.
3
3
=
+
 
 
 
84. 
 
a. P(A) = 
02
.
000
,
10
2000 =
, P(B) = P(A ∩ B) + P(A′ ∩  B) 
= P(B|A) P(A) + P(B|A′) P(A′) = 
2.
)
8
(.
9999
2000
)
2
(.
9999
1999
=
•
+
•
 
P(A ∩ B) = .039984; since P(A ∩ B) ≠ P(A)P(B), the events are not independent. 
 
b. P(A ∩ B) = .04.  Very little difference. Yes. 
 
c. 
P(A) = P(B) = .2, P(A)P(B) = .04, but P(A ∩ B) = P(B|A)P(A) = 
0222
.
10
2
9
1
=
⋅
, so the 
two numbers are quite different. 
In a, the sample size is small relative to the “population” size, while here it is not. 
 
 
85. 
P(system works) = P( 1 – 2 works ∩ 3 – 4 – 5 – 6 works ∩ 7 works) 
= P( 1 – 2 works) • P( 3 – 4 – 5 – 6 works) •P( 7 works) 
= (.99) (.9639) (.9) = .8588 
 
With the subsystem in figure 2.14 connected in parallel to this subsystem,  
P(system works) = .8588+.927 – (.8588)(.927) = .9897 
 
 
 
85

Chapter 2:  Probability 
86. 
 
a. For route #1, P(late) = P(stopped at 2 or 3 or 4 crossings) 
   =  1 – P(stopped at 0 or 1) = 1 – [.94 + 4(.9)3(.1)]  
   = .0523 
For route #2, P(late) = P(stopped at 1 or 2 crossings) 
 
 
 
 
   = 1 – P(stopped at none) = 1 - .81 = .19 
thus route #1 should be taken. 
 
b. P(4 crossing route | late) = 
)
(
)
sin
4
(
late
P
late
g
cros
P
∩
 
 
=
216
.
)
19
)(.
5
(.
)
0523
)(.
5
(.
)
0523
)(.
5
.(.
=
+
 
 
 
87. 
 
P(at most 1 is lost) = 1 – P(both lost) 
 
 
      = 1 –  π2
P(exactly 1 lost) = 2π(1 - π) 
P(exactly 1 | at most 1 ) = 
2
1
)
1(
2
)1
.
(
)1
(
π
π
π
−
−
=
most
at
P
exactly
P
 
 
π
1
)
1(
π
π
−
π
π
π
−
1
π
−
2
π
π
π )
1( −
2)
1(
π
−
π
−
1
 
86

Chapter 2:  Probability 
Supplementary Exercises 
 
88. 
 
a. 
 
1140
3
20 =
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
b. 
 
969
3
19 =
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
c. 
# having at least 1 of the 10 best =  1140 - # of crews having none of 10 best   = 1140 - 
- 120 = 1020 
1140
3
10 =
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
d. P(best will not work) = 
85
.
1140
969 =
 
 
 
89. 
 
a. P(line 1) = 
333
.
1500
500 =
;   
P(Crack) = 
(
)
(
)
(
)
444
.
1500
666
1500
600
40
.
400
44
.
500
50
.
=
=
+
+
 
 
b. P(Blemish | line 1) = .15 
 
c. 
P(Surface Defect) =
(
)
(
)
(
)
1500
172
1500
600
15
.
400
08
.
500
10
.
=
+
+
 
P(line 1 and Surface Defect) = 
(
)
1500
50
1500
500
10
.
=
 
So P(line 1 | Surface Defect) = 
291
.
1500
172
1500
50
=
=
 
 
90. 
 
a. The only way he will have one type of forms left is if they are all course substitution 
forms.  He must choose all 6 of the withdrawal forms to pass to a subordinate.  The 
desired probability is 
00476
.
6
10
6
6
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
 
b. He can start with the wd forms: W-C-W-C or with the cs forms: C-W-C-W: 
# of ways: 6 × 4 × 5 × 3 + 4 × 6 × 3 × 5 = 2(360) = 720; 
The total # ways to arrange the four forms: 10 × 9 × 8 × 7 = 5040. 
The desired probability is 720/5040 = .1429 
 
87

Chapter 2:  Probability 
91. 
P(A∪B) = P(A) + P(B) – P(A)P(B) 
   .626    =  P(A) + P(B) - .144 
 
So P(A) + P(B) = .770 and P(A)P(B) = .144.    
Let x = P(A) and y = P(B), then using the first equation, y = .77 – x, and substituting this into 
the second equation, we get x ( .77 – x ) = .144 or  
x2 - .77x + .144 = 0.  Use the quadratic formula to solve:  
32
.
2
13
.
77
.
2
)
144
)(.
4
(
77
.
77
.
2
=
±
=
−
±
 or .45   
So P(A) = .45 and  P(B) = .32 
 
 
92. 
 
a. (.8)(.8)(.8) = .512 
 
b.  
 
.512+.032+.023+.023 = .608 
 
c. 
P(1 sent | 1 received) = 
7835
.
5432
.
4256
.
1(
)
1
1(
=
=
∩
received
P
received
sent
P
 
 
 
88

Chapter 2:  Probability 
93. 
 
a. There are 5×4×3×2×1 = 120 possible orderings, so P(BCDEF) = 
0083
.
120
1 =
 
 
b. # orderings in which F is 3rd = 4×3×1*×2×1 = 24, ( * because F must be here), so         
P(F 3rd) = 
2.
120
24 =
 
 
c. 
P(F last) = 
2.
120
1
1
2
3
4
=
×
×
×
×
 
 
 
94. 
P(F hasn’t heard after 10 times) = P(not on #1 ∩ not on #2 ∩…∩  not on #10) 
= 
1074
.
5
4
10
=
⎟
⎠
⎞
⎜
⎝
⎛
 
 
 
95. 
When three experiments are performed, there are 3 different ways in which detection can 
occur on exactly 2 of the experiments: (i)  #1 and #2 and not #3  (ii) #1 and not #2 and #3; 
(iii) not#1 and #2 and #3.  If the impurity is present, the probability of exactly 2 detections in 
three (independent) experiments is (.8)(.8)(.2) + (.8)(.2)(.8) + (.2)(.8)(.8) = .384.  If the 
impurity is absent, the analogous probability is 3(.1)(.1)(.9) = .027.  Thus 
P(present | detected in exactly 2 out of 3) =  
)
2.
.
.
(det
)
2.
.
.
(det
exactly
in
ected
P
present
exactly
in
ected
P
∩
 
= 
905
.
)
6
)(.
027
(.
)
4
)(.
384
(.
)
4
)(.
384
(.
=
+
 
 
 
96. 
P(exactly 1 selects category #1 | all 3 are different) 
= 
)
.
.
(
)
.
.
1
#
.1.
(
different
are
all
P
different
are
all
selects
exactly
P
∩
 
Denominator = 
5556
.
9
5
6
6
6
4
5
6
=
=
×
×
×
×
 
Numerator = 3 P(contestant #1 selects category #1 and the other two select two different 
categories) 
= 
6
6
6
3
4
5
6
6
6
4
5
1
3
×
×
×
×
=
×
×
×
×
×
 
The desired probability is then 
5.
2
1
4
5
6
3
4
5
=
=
×
×
×
×
 
 
89

Chapter 2:  Probability 
97. 
 
 
a. P(pass inspection) = P(pass initially ∪ passes after recrimping) = P(pass initially) + P( 
fails initially ∩ goes to recrimping ∩ is corrected after recrimping)  
= .95 + (.05)(.80)(.60) (following path “bad-good-good” on tree diagram) 
= .974 
 
b. P(needed no recrimping | passed inspection) = 
)
.
(
)
.
(
inspection
passed
P
initially
passed
P
 
 
 
 
 
 
 
 
= 
9754
.
974
.
95
.
=
 
 
 
98. 
 
a. P(both + ) = P(carrier  ∩ both + ) + P(not a carrier ∩ both + ) 
=P(both +  | carrier) x P(carrier)  
+ P(both  +  | not a carrier) x P(not a carrier) 
= (.90)2(.01) + (.05)2(.99) = .01058 
P(both – ) = (.10)2(.01) + (.95)2(.99) = .89358 
P(tests agree) = .01058 + .89358 = .90416 
b. P(carrier | both + ve) = 
7656
.
01058
.
)
01
(.
)
90
(.
)
.
(
)
.
(
2
=
=
∩
positive
both
P
positive
both
carrier
P
 
 
 
99. 
Let A = 1st functions, B = 2nd functions, so P(B) = .9, P(A ∪ B) = .96, P(A ∩ B)=.75.  Thus, 
P(A ∪ B) = P(A) + P(B) - P(A ∩ B) = P(A) + .9 - .75 = .96, implying P(A) = .81. 
This gives P(B | A) = 
926
.
81
.
75
.
)
(
)
(
=
=
∩
A
P
A
B
P
 
 
 
100. 
P(E1 ∩ late) = P( late | E1 )P(E1) = (.02)(.40) = .008 
 
 
90

Chapter 2:  Probability 
101. 
 
a. The law of total probability gives 
P(late) = 
 
∑
=
⋅
3
1
)
(
)
|
(
i
i
i
E
P
E
late
P
     = (.02)(.40) + (.01)(.50) + (.05)(.10) = .018 
 
b. P(E1′ | on time) = 1 – P(E1 | on time) 
 
 
 
= 
601
.
982
.
)
4
)(.
98
(.
1
)
.
(
)
.
(
1
1
=
−
=
∩
−
time
on
P
time
on
E
P
 
 
 
102. 
Let B denote the event that a component needs rework.   Then  
P(B) = 
= (.05)(.50) + (.08)(.30) + (.10)(.20) = .069 
∑
=
⋅
3
1
)
(
)
|
(
i
i
i
A
P
A
B
P
 
Thus  
P(A1 | B) = 
362
.
069
.
)
50
)(.
05
(.
=
 
P(A2 | B) = 
348
.
069
.
)
30
)(.
08
(.
=
 
 
P(A3 | B) = 
290
.
069
.
)
20
)(.
10
(.
=
 
 
 
103. 
 
a. P(all different) = 
883
.
)
365
(
)
356
)...(
364
)(
365
(
10
=
 
P(at least two the same) = 1 - .883 = .117 
 
b. P(at least two the same) = .476 for k=22, and  = .507 for k=23 
 
c. 
P(at least two have the same SS number) = 1 – P(all different) 
 
 
 
 
 
 
= 
10
)
1000
(
)
991
)...(
999
)(
1000
(
1−
 
 
 
 
 
 
 
= 1 - .956 = .044 
Thus P(at least one “coincidence”) = P(BD coincidence ∪ SS coincidence)  
 
 
 
  = .117 + .044 – (.117)(.044) = .156 
 
 
 
91

Chapter 2:  Probability 
104. 
 
 
a. P(G | R1 < R2 < R3) = 
67
.
075
.
15
.
15
.
=
+
, P(B | R1 < R2 < R3) = .33, classify as granite. 
 
b. P(G | R1 < R3 < R2) = 
2941
.
2125
.
0625
.
=
 < .05, so  classify as basalt. 
P(G | R3 < R1 < R2) = 
0667
.
5625
.
0375
.
=
, so  classify as basalt. 
 
c. 
P(erroneous classif) = P(B classif as G) + P(G classif as B) 
= P(classif as G | B)P(B) + P(classif as B | G)P(G) 
= P(R1 < R2 < R3 | B)(.75) + P(R1 < R3 < R2 or R3 < R1 < R2 | G)(.25) 
= (.10)(.75) + (.25 + .15)(.25) = .175 
 
 
92

Chapter 2:  Probability 
d. For what values of p will P(G | R1<R2<R3) > .5, P(G | R1 < R3 < R2) > .5,  
P(G | R3 < R1 < R2) > .5? 
P(G | R1 < R2 < R3) = 
5.
5.
1.
6.
)
1(1.
6.
6.
>
+
=
−
+
p
p
p
p
p
 iff 
7
1
>
p
 
P(G | R1 < R3 < R2) = 
5.
)
1(
2.
25
.
25
.
>
−
+
p
p
p
 iff 
9
4
>
p
 
P(G | R3 < R1 < R2) = 
5.
)
1(
7.
15
.
15
.
>
−
+
p
p
p
 iff 
17
14
>
p
 (most restrictive) 
If 
17
14
>
p
 always classify as granite. 
 
 
105. 
P(detection by the end of the nth glimpse) = 1 – P(not detected in 1st n) 
= 1 – P(G1′ ∩ G2′ ∩ … ∩ Gn′ ) = 1 - P(G1′)P(G2′) … P(Gn′) 
= 1 – (1 – p1)(1 – p2) … (1 – pn) = 1 - 
 )
1(
1
i
n
i
p
−
=π
 
106. 
 
a. P(walks on 4th pitch) = P(1st 4 pitches are balls) = (.5)4 = .0625 
 
b. P(walks on 6th) = P(2 of the 1st 5 are strikes, #6 is a ball) 
= P(2 of the 1st 5 are strikes)P(#6 is a ball) 
= [10(.5)5](.5) = .15625 
 
c. 
P(Batter walks) = P(walks on 4th) + P(walks on 5th) + P(walks on 6th) 
 
 
 
= .0625 + .15625 + .15625 = .375 
d. P(first batter scores while no one is out) = P(first 4 batters walk) 
 
 
 
 
 
 
=(.375)4 = .0198 
 
107. 
 
a. P(all in correct room) = 
0417
.
24
1
1
2
3
4
1
=
=
×
×
×
 
 
b. The 9 outcomes which yield incorrect assignments are: 2143, 2341, 2413, 3142, 3412, 
3421, 4123, 4321, and 4312, so P(all incorrect) = 
375
.
24
9 =
 
 
 
93

Chapter 2:  Probability 
108. 
 
a. P(all full) = P(A ∩ B ∩ C) = (.6)(.5)(.4) = .12 
P(at least one isn’t full) = 1 –  P(all full) = 1 - .12 = .88 
 
b. P(only NY is full) = P(A ∩ B′  ∩ C′) = P(A)P(B′)P(C′) = .18 
Similarly, P(only Atlanta is full) = .12 and P(only LA is full) = .08 
So P(exactly one full) = .18 + .12 + .08 = .38 
 
 
109. 
Note: s = 0 means that the very first candidate interviewed is hired.  Each entry below is the 
candidate hired for the given policy and outcome. 
 
Outcome 
s=0 
s=1 
s=2 
s=3 
Outcome 
s=0 
s=1 
s=2 
s=3 
1234 
1 
4 
4 
4 
3124 
3 
1 
4 
4 
1243 
1 
3 
3 
3 
3142 
3 
1 
4 
2 
1324 
1 
4 
4 
4 
3214 
3 
2 
1 
4 
1342 
1 
2 
2 
2 
3241 
3 
2 
1 
1 
1423 
1 
3 
3 
3 
3412 
3 
1 
1 
2 
1432 
1 
2 
2 
2 
3421 
3 
2 
2 
1 
2134 
2 
1 
4 
4 
4123 
4 
1 
3 
3 
2143 
2 
1 
3 
3 
4132 
4 
1 
2 
2 
2314 
2 
1 
1 
4 
4213 
4 
2 
1 
3 
2341 
2 
1 
1 
1 
4231 
4 
2 
1 
1 
2413 
2 
1 
1 
3 
4312 
4 
3 
1 
2 
2431 
2 
1 
1 
1 
4321 
4 
3 
2 
1 
s 
0 
1 
2 
3 
P(hire#1) 
24
6  
24
11  
24
10  
24
6  
 
 
So s = 1 is best. 
 
110. 
P(at least one occurs) = 1 – P(none occur) 
= 1 – (1 – p1) (1 – p2) (1 – p3) (1 – p4) 
= p1p2(1 – p3) (1 – p4) + …+ (1 – p1) (1 – p2)p3p4  
+ (1 – p1) p2p3p4 + … + p1 p2p3(1 – p4) + p1p2p3p4 
 
111. 
P(A1) = P(draw slip 1 or 4) = ½; P(A2) = P(draw slip 2 or 4) = ½; 
P(A3) = P(draw slip 3 or 4) = ½; P(A1 ∩ A2) = P(draw slip 4) = ¼; 
P(A2 ∩ A3) = P(draw slip 4) = ¼;  P(A1 ∩ A3) = P(draw slip 4) = ¼ 
Hence P(A1 ∩ A2) = P(A1)P(A2) = ¼, P(A2 ∩ A3) = P(A2)P(A3) = ¼, 
 
P(A1 ∩ A3) = P(A1)P(A3) = ¼, thus there exists pairwise independence 
P(A1 ∩ A2 ∩ A3) = P(draw slip 4) = ¼  ≠ 1/8 = P(A1)p(A2)P(A3), so the events are not 
mutually independent. 
 
94

CHAPTER 3 
 
Section 3.1 
 
1. 
 
S: 
FFF 
SFF 
FSF 
FFS 
FSS 
SFS 
SSF 
SSS 
X: 
0 
1 
1 
1 
2 
2 
2 
3 
 
 
2. 
X = 1 if a randomly selected book is non-fiction and X = 0 otherwise 
X = 1 if a randomly selected executive is a female and X = 0 otherwise 
X = 1 if a randomly selected driver has automobile insurance and X = 0 otherwise 
 
 
3. 
M = the difference between the large and the smaller outcome with possible values 0, 1, 2, 3, 
4, or 5; W = 1 if the sum of the two resulting numbers is even and W = 0 otherwise, a 
Bernoulli random variable. 
 
 
4. 
In my perusal of a zip code directory, I found no 00000, nor did I find any zip codes with four 
zeros, a fact which was not obvious.  Thus possible X values are 2, 3, 4, 5 (and not 0 or 1).  X 
= 5 for the outcome 15213, X = 4 for the outcome 44074, and X = 3 for 94322. 
 
 
5. 
No.  In the experiment in which a coin is tossed repeatedly until a H results, let Y = 1 if the 
experiment terminates with at most 5 tosses and Y = 0 otherwise.  The sample space is 
infinite, yet Y has only two possible values. 
 
 
6. 
Possible X values are1, 2, 3, 4, … (all positive integers) 
 
Outcome: 
RL 
AL 
RAARL 
RRRRL 
AARRL 
X: 
2 
2 
5 
5 
5 
 
 
95 

Chapter 3:  Discrete Random Variables and Probability Distributions 
7. 
 
a. Possible values are   0, 1, 2, …, 12; discrete 
 
b. With N = # on the list, values are 0, 1, 2, … , N; discrete 
 
c. 
Possible values are 1, 2, 3, 4, … ; discrete 
 
d. { x: 0< x < ∞ } if we assume that a rattlesnake can be arbitrarily short or long; not 
discrete 
 
e. 
With c = amount earned per book sold, possible values are 0, c, 2c, 3c, … , 10,000c; 
discrete 
 
f. 
{ y: 0 < y < 14} since 0 is the smallest possible pH and 14 is the largest possible pH; not 
discrete 
 
g. With m and M denoting the minimum and maximum possible tension, respectively, 
possible values are { x: m < x < M }; not discrete 
 
h. Possible values are 3, 6, 9, 12, 15, … --   i.e. 3(1), 3(2), 3(3), 3(4), …giving a first 
element, etc,; discrete 
 
 
8. 
Y = 3 : SSS; 
 
Y = 4:  FSSS; 
 
Y = 5:  FFSSS, SFSSS; 
Y = 6: SSFSSS, SFFSSS, FSFSSS, FFFSSS; 
Y = 7: SSFFS, SFSFSSS, SFFFSSS, FSSFSSS, FSFFSSS, FFSFSSS, FFFFSSS 
 
 
9. 
 
a. Returns to 0 can occur only after an even number of tosses; possible S values are 2, 4, 6, 
8, …(i.e. 2(1), 2(2), 2(3), 2(4),…) an infinite sequence, so x is discrete. 
 
b. Now a return to 0 is possible after any number of tosses greater than 1, so possible values 
are 2, 3, 4, 5, … (1+1,1+2, 1+3, 1+4, …, an infinite sequence) and X is discrete 
 
10. 
 
a. T = total number of pumps in use at both stations.  Possible values: 0, 1, 2, 3, 4, 5, 6,  7, 
8, 9, 10 
 
b. X: -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6 
 
c. 
U: 0, 1, 2, 3, 4, 5, 6 
 
d. Z: 0, 1, 2 
 96

Chapter 3:  Discrete Random Variables and Probability Distributions 
Section 3.2 
 
11. 
 
a.  
x 
4 
6 
8 
 
P(x) 
.45 
.40 
.15 
 
 
b.  
 
8
7
6
5
4
.50
.40
.30
.20
.10
0
x
Relative
Frequency
c. 
P(x ≥ 6) = .40 + .15 = .55 
 
P(x > 6) = .15 
 
 
12. 
 
a. In order for the flight to accommodate all the ticketed passengers who show up, no more 
than 50 can show up.  We need y ≤ 50.    
P(y ≤ 50) = .05 + .10 + .12 + .14 + .25 + .17 = .83 
 
b. Using the information in a. above, P(y > 50) = 1 - P(y ≤ 50) = 1 - .83 = .17 
 
c. 
For you to get on the flight, at most 49 of the ticketed passengers must show up.  P(y ≤ 
49) = .05 + .10 + .12 + .14 + .25 = .66.  For the 3rd person on the standby list, at most 47 
of the ticketed passengers must show up.  P(y ≤ 44) = .05 + .10 + .12 = .27 
 
 
 97

Chapter 3:  Discrete Random Variables and Probability Distributions 
13. 
 
a. P(X ≤ 3) = p(0) + p(1) + p(2) + p(3) = .10+.15+.20+.25 = .70 
 
b. P(X < 3) = P(X ≤ 2) = p(0) + p(1) + p(2) = .45 
 
c. 
P(3 ≤ X) = p(3) + p(4) + p(5) + p(6) = .55 
 
d. P( 2 ≤X≤ 5) = p(2) + p(3) + p(4) + p(5) = .71 
 
e. 
The number of lines not in use is 6 – X , so 6 – X = 2 is equivalent to X = 4, 6 – X = 3 to 
X = 3, and 6 – X = 4 to X = 2.  Thus we desire P( 2 ≤X≤ 4) = p(2) + p(3) + p(4) = .65 
 
f. 
6 – X ≥ 4 if 6 – 4 ≥ X, i.e. 2 ≥ X, or X ≤ 2, and P(X ≤ 2) = .10+.15+.20 = .45 
 
 
14. 
 
a. 
 = K[1 + 2 + 3 + 4 + 5] = 15K = 1 
∑
=
5
1
)
(
y
y
p
15
1
=
⇒K
 
 
b. P(Y ≤ 3) = p(1) + p(2) + p(3) = 
4.
15
6 =
 
 
c. 
P( 2 ≤Y≤ 4) = p(2) + p(3) + p(4) = 
6.
15
9 =
 
 
d. 
1
50
55
]
25
16
9
4
1[
50
1
50
5
1
2
≠
=
+
+
+
+
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
∑
=
y
y
; No 
 
 
15. 
 
a. (1,2) (1,3) (1,4) (1,5) (2,3) (2,4) (2,5) (3,4) (3,5) (4,5) 
 
b. P(X = 0) = p(0) = P[{ (3,4) (3,5) (4,5)}] = 
3.
10
3 =
 
P(X = 2) = p(2) = P[{ (1,2) }] = 
1.
10
1 =
 
P(X = 1) = p(1) = 1 – [p(0) + p(2)] = .60, and p(x) = 0 if x ≠ 0, 1, 2 
 
c. 
F(0) = P(X ≤ 0) = P(X = 0) = .30 
F(1) = P(X ≤ 1) = P(X = 0 or 1) = .90 
F(2) = P(X ≤ 2) = 1 
 
The c.d.f. is  
 
 
F(x) = 
 
⎪
⎪
⎩
⎪⎪
⎨
⎧
1
90
.
30
.
0
x
x
x
x
≤
<
≤
<
≤
<
2
2
1
1
0
0
 
 
 98

Chapter 3:  Discrete Random Variables and Probability Distributions 
 
16. 
 
a.  
x 
Outcomes 
p(x) 
0 
FFFF 
(.7)4                =.2401 
1 
FFFS,FFSF,FSFF,SFFF 
4[(.7)3(.3)]       =.4116 
2 
FFSS,FSFS,SFFS,FSSF,SFSF,SSFF 
6[(.7)2(.3)2]     =.2646 
3 
FSSS, SFSS,SSFS,SSSF 
4[(.7)(.3)3]       =.0756 
4 
SSSS 
(.3)4                =.0081 
 
 
b.  
 
0
1
2
3
4
0
.10
.20
.30
.40
Insured
Frequency
Relative
c. 
p(x) is largest for X = 1 
 
d. P(X ≥ 2) = p(2) + p(3) + p(4) = .2646+.0756+.0081 = .3483 
 
This could also be done using the complement. 
 
 
17. 
 
a. P(2) = P(Y = 2) = P(1st 2 batteries are acceptable) 
 
        = P(AA) = (.9)(.9) = .81 
 
b. p(3) = P(Y = 3) = P(UAA or AUA) = (.1)(.9)2 + (.1)(.9)2 = 2[(.1)(.9)2] = .162 
 
c. 
The fifth battery must be an A, and one of the first four must also be an A.  Thus, p(5) = 
P(AUUUA or UAUUA or UUAUA or UUUAA) = 4[(.1)3(.9)2] = .00324 
 
d. P(Y = y) = p(y) = P(the yth is an A and so is exactly one of the first y – 1) 
 
 
=(y – 1)(.1)y-2(.9)2, y = 2,3,4,5,… 
 99

Chapter 3:  Discrete Random Variables and Probability Distributions 
18. 
 
a. p(1) = P(M = 1 ) = P[(1,1)] = 36
1  
 
p(2) = P(M = 2 ) = P[(1,2) or (2,1) or (2,2)] = 36
3  
 
p(3) = P(M = 3 ) = P[(1,3) or (2,3) or (3,1) or (3,2) or (3,3)] = 36
5  
 
Similarly, p(4) = 36
7 ,  p(5) = 36
9 , and p(6) = 36
11  
 
b. F(m) =  
0 for m < 1, 36
1  for 1 ≤ m < 2,  
 
 
F(m) = 
⎪
⎪
⎪
⎪
⎩
⎪
⎪
⎪
⎪
⎨
⎧
1
0
36
25
36
16
36
9
36
4
36
1
 
6
6
5
5
4
4
3
3
2
2
1
1
≥
<
≤
<
≤
<
≤
<
≤
<
≤
<
m
m
m
m
m
m
m
 
8
7
6
5
4
3
2
1
0
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
19. 
Let A denote the type O+ individual ( type O positive blood) and B, C, D, the other 3 
individuals.  Then p(1) – P(Y = 1) = P(A first) = 4
1  = .25 
p(2) = P(Y = 2) = P(B, C, or D first and A next) = 
25
.
4
1
3
1
4
3
=
=
⋅
 
p(4) = P(Y = 3) = P(A last) = 
25
.
4
1
2
1
3
2
4
3
=
=
⋅
⋅
 
So p(3) = 1 – (.25+.25+.25) = .25 
 
 
20. 
P(0) = P(Y = 0) = P(both arrive on Wed.) = (.3)(.3) = .09 
 
P(1) = P(Y = 1) = P[(W,Th)or(Th,W)or(Th,Th)]  
 
 
  = (.3)(.4) + (.4)(.3) + (.4)(.4) = .40 
 
P(2) = P(Y = 2) = P[(W,F)or(Th,F)or(F,W) or (F,Th) or (F,F)] = .32 
 
P(3) = 1 – [.09 + .40 + .32] = .19 
 
 
 
100

Chapter 3:  Discrete Random Variables and Probability Distributions 
21. 
The jumps in F(x) occur at x = 0, 1, 2, 3, 4, 5, and 6, so we first calculate F( ) at each of these 
values: 
 
 
F(0) = P(X ≤ 0) = P(X = 0) = .10 
 
 
F(1) = P(X ≤ 1) = p(0) + p(1) = .25 
 
 
F(2) = P(X ≤ 2) = p(0) + p(1) + p(2) = .45 
 
 
F(3) = .70, F(4) = .90, F(5) = .96, and F(6) = 1. 
 
The c.d.f. is  
 
 
F(x) = 
 
⎪
⎪
⎪
⎪
⎪
⎩
⎪⎪
⎪
⎪
⎪
⎨
⎧
00
.1
96
.
90
.
70
.
45
.
25
.
10
.
00
.
x
x
x
x
x
x
x
x
≤
<
≤
<
≤
<
≤
<
≤
<
≤
<
≤
<
6
6
5
5
4
4
3
3
2
2
1
1
0
0
 
 
Then P(X ≤ 3) = F(3) = .70, P(X < 3) = P(X ≤ 2) = F(2) = .45,  
P(3 ≤ X) = 1 – P(X ≤ 2) = 1 – F(2) = 1 - .45 = .55,  
and P(2 ≤ X ≤ 5) = F(5) – F(1) = .96 - .25 = .71 
 
 
22. 
 
a. P(X = 2) = .39 - .19 = .20 
 
b. P(X > 3) = 1 - .67 = .33 
 
c. 
P(2 ≤ X ≤ 5) = .92 - .19 = .78 
 
d. P(2 < X < 5) = .92 - .39 = .53 
 
 
23. 
 
a. Possible X values are those values at which F(x) jumps, and the probability of any 
particular value is the size of the jump at that value.  Thus we have: 
 
x 
1 
3 
4 
6 
12 
p(x) 
.30 
.10 
.05 
.15 
.40 
 
b. P(3 ≤ X ≤ 6) = F(6) – F(3-) = .60 - .30 = .30 
P(4 ≤ X) = 1 – P(X < 4) = 1 – F(4-) = 1 - .40 = .60 
 
 
24. 
P(0) = P(Y = 0) = P(B first) = p 
P(1) = P(Y = 1) = P(G first, then B) = P(GB) = (1 – p)p 
P(2) = P(Y = 2) = P(GGB) = (1 – p)2p 
Continuing, p(y) = P(Y=y) = P(y G’s and then a B) = (1 – p)yp for y = 0,1,2,3,… 
 
101

Chapter 3:  Discrete Random Variables and Probability Distributions 
25. 
 
a. Possible X values are 1, 2, 3, … 
 
P(1) = P(X = 1 ) = P(return home after just one visit) = 3
1  
 
P(2) = P(X = 2) = P(second visit and then return home) = 
3
1
3
2 ⋅
 
 
P(3) = P(X = 3) = P(three visits and then return home) = ( )
3
1
2
3
2
⋅
 
 
In general p(x) = ( )
( )
3
1
1
3
2
−
x
 for x = 1, 2, 3, … 
 
b. The number of straight line segments is Y = 1 + X (since the last segment traversed 
returns Alvie to O), so as in a, p(y) = ( )
( )
3
1
2
3
2
−
y
 for y =  2, 3, … 
 
c. 
Possible Z values are 0, 1, 2, 3 , … 
 
p(0) = P(male first and then home) =
6
1
3
1
2
1
=
⋅
, 
 
p(1) = P(exactly one visit to a female) = P(female 1st, then home) + P(F, M, home) + 
P(M, F, home) + P(M, F, M, home)  
 
= ( )( ) ( )( )( ) ( )( )( ) ( )( )( )( )
3
1
3
2
3
2
2
1
3
1
3
2
2
1
3
1
3
2
2
1
3
1
2
1
+
+
+
 
 
=( )(
)( ) ( )( )(
)( )
( )( )( ) ( )( )( )( )
3
1
3
5
3
2
2
1
3
1
3
5
2
1
3
1
3
2
3
2
2
1
3
1
3
2
2
1
1
1
+
=
+
+
+
 
 
where the first term corresponds to initially visiting a female and the second term 
corresponds to initially visiting a male.  Similarly,  
 
p(2) = ( )( ) ( )( ) ( )( ) ( )( )
3
1
3
5
2
3
2
2
1
3
1
3
5
2
3
2
2
1
+
.  In general, 
 
 
p(z) = ( )( )
( )( ) ( )( )
( )( )
( )( )
2
2
3
2
54
24
3
1
3
5
2
2
3
2
2
1
3
1
3
5
2
2
3
2
2
1
−
−
−
=
+
z
z
z
 for z = 1, 2, 3, … 
 
 
26. 
 
a. The sample space consists of all possible permutations of the four numbers 1, 2, 3, 4: 
 
 
 
 
outcome 
y value 
outcome 
y value 
outcome 
y value 
1234 
4 
2314 
1 
3412 
0 
1243 
2 
2341 
0 
3421 
0 
1324 
2 
2413 
0 
4132 
1 
1342 
1 
2431 
1 
4123 
0 
1423 
1 
3124 
1 
4213 
1 
1432 
2 
3142 
0 
4231 
2 
2134 
2 
3214 
2 
4312 
0 
2143 
0 
3241 
1 
4321 
0 
 
 
 
 
 
 
 
b. Thus p(0) = P(Y = 0) = 24
9 , p(1) = P(Y = 1) = 24
8 , p(2) = P(Y = 2) = 24
6 , 
p(3) = P(Y = 3) = 0, p(3) = P(Y = 3) = 24
1 . 
 
 
 
102

Chapter 3:  Discrete Random Variables and Probability Distributions 
27. 
If x1 < x2, F(x2) = P(X ≤ x2) = P( {X ≤ x1} ∪ { x1 < X ≤ x2})  
 
 
 
= P( X ≤ x1) + P( x1 < X ≤ x2 ) ≥ P( X ≤ x1) = F(x1). 
 
F(x1) = F(x2) when P( x1 < X ≤ x2 ) = 0. 
 
 
Section 3.3 
 
28. 
 
a. E (X) = ∑
 
=
⋅
4
0
)
(
x
x
p
x
   = (0)(.08) + (1)(.15) + (2)(.45) + (3)(.27) + (4)(.05) = 2.06 
 
b. V(X) = 
= (0 – 2.06)
∑
=
⋅
−
4
0
2
)
(
)
06
.2
(
x
x
p
x
2(.08) + …+ (4 – 2.06)2(.05) 
 
 
= .339488+.168540+.001620+.238572+.188180 = .9364 
 
c. 
σx= 
9677
.
9364
.
=
 
 
d. V(X) = 
= 5.1800 – 4.2436 = .9364 
2
4
0
2
)
06
.2
(
)
(
−
⎥⎦
⎤
⎢⎣
⎡
⋅
∑
=
x
x
p
x
 
 
29. 
 
a. E (Y) = ∑
= (0)(.60) + (1)(.25) + (2)(.10) + (3)(.05) = .60 
=
⋅
4
0
)
(
x
y
p
y
 
b. E (100Y2) = 
= (0)(.60) + (100)(.25)  
∑
=
⋅
4
0
2
)
(
100
x
y
p
y
 
 
 
 
+ (400)(.10) + (900)(.05) = 110 
 
 
30. 
E (Y) = .60; 
 
E (Y2) = 1.1 
V(Y) = E(Y2) – [E(Y)]2  = 1.1 – (.60)2 = .74 
σy= 
8602
.
74
.
=
 
E (Y) ± σy= .60 ± .8602 = (-.2602, 1.4602) or ( 0, 1). 
P(Y = 0) + P(Y =1) = .85 
 
 
 
103

Chapter 3:  Discrete Random Variables and Probability Distributions 
31. 
 
a. E (X) = (13.5)(.2) + (15.9)(.5) + (19.1)(.3) = 16.38, 
E (X2) = (13.5)2(.2) + (15.9)2(.5) + (19.1)2(.3) = 272.298, 
V(X) = 272.298 – (16.38)2 = 3.9936 
 
b. E (25X – 8.5) = 25 E (X) – 8.5 = (25)(16.38) – 8.5 = 401 
 
c. 
V(25X – 8.5) = V(25X) = (25)2V(X) = (625)(3.9936) = 2496 
 
d. E[h(X)] = E[X - .01X2] = E(X) - .01E(X2) = 16.38 – 2.72 = 13.66 
 
 
32. 
 
a. E(X2) = 
= (0
∑
=
⋅
1
0
2
)
(
x
x
p
x
2)((1 – p) + (12)(p) = (1)(p) = p 
 
b. V(X) = E(X2) – [E(X)]2  = p – p2 = p(1 – p) 
 
c. 
E(x79) = (079)(1 – p) + (179)(p) = p 
 
 
33. 
E(X) = 
∑
∑
∑
∞
=
∞
=
∞
=
=
⋅
=
⋅
1
2
1
3
1
1
)
(
x
x
x
x
c
x
c
x
x
p
x
, but it is a well-known result from the theory of 
infinite series that ∑
∞
=1
2
1
x
x
 < ∞, so E(X) is finite. 
 
 
34. 
Let h(X) denote the net revenue (sales revenue – order cost) as a function of X.  Then h3(X) 
and h4(X) are the net revenue for 3 and 4 copies purchased, respectively.  For x = 1 or 2 , 
h3(X) = 2x – 3, but at x = 3,4,5,6 the revenue plateaus. Following similar reasoning, h4(X) = 
2x – 4 for x=1,2,3, but plateaus at 4 for x = 4,5,6. 
 
x 
1 
2 
3 
4 
5 
6 
h3(x) 
-1 
1 
3 
3 
3 
3 
h4(x) 
-2 
0 
2 
4 
4 
4 
p(x) 
15
1  
15
2  
15
3  
15
4  
15
3  
15
2  
 
 
E[h3(X)] = 
= (-1)(
∑
=
⋅
6
1
3
)
(
)
(
x
x
p
x
h
15
1 ) + … + (3)( 15
2 ) = 2.4667 
Similarly, E[h4(X)] = 
= (-2)(
∑
=
⋅
6
1
4
)
(
)
(
x
x
p
x
h
15
1 ) + … + (4)( 15
2 ) = 2.6667 
Ordering 4 copies gives slightly higher revenue, on the average. 
 
 
104

Chapter 3:  Discrete Random Variables and Probability Distributions 
35. 
 
P(x) 
.8 
.1 
.08 
.02 
x 
0 
1,000 
5,000 
10,000 
H(x) 
0 
500 
4,500 
9,500 
 
E[h(X)] = 600.  Premium should be $100 plus expected value of damage minus deductible or 
$700. 
 
 
36. 
E(X) = 
∑
∑
=
=
+
=
⎥⎦
⎤
⎢⎣
⎡
+
=
⎟
⎠
⎞
⎜
⎝
⎛
=
⎟
⎠
⎞
⎜
⎝
⎛⋅
n
x
n
x
n
n
n
n
x
n
n
x
1
1
2
1
2
)1
(
1
1
1
 
 
E(X2) = 
∑
∑
=
=
+
+
=
⎥⎦
⎤
⎢⎣
⎡
+
+
=
⎟
⎠
⎞
⎜
⎝
⎛
=
⎟
⎠
⎞
⎜
⎝
⎛⋅
n
x
n
x
n
n
n
n
n
n
x
n
n
x
1
2
1
2
6
)1
2
)(
1
(
6
)1
2
)(
1
(
1
1
1
 
 
So V(X) = 
12
1
2
1
6
)1
2
)(
1
(
2
2
−
=
⎟
⎠
⎞
⎜
⎝
⎛
+
−
+
+
n
n
n
n
 
 
 
37. 
E[h(X)] = E
408
.
1
6
1
)
(
1
1
6
1
6
1
=
=
⋅⎟
⎠
⎞
⎜
⎝
⎛
=
⎟
⎠
⎞
⎜
⎝
⎛
∑
∑
=
=
x
x
x
x
p
x
X
, whereas 
286
.
5.3
1 =
, so you 
expect to win more if you gamble. 
 
 
38. 
E(X) = 
 = 2.3, E(X
∑
=
⋅
4
1
)
(
x
x
p
x
2) = 6.1, so V(X) = 6.1 – (2.3)2 = .81 
 
Each lot weighs 5 lbs, so weight left = 100 – 5x.   
Thus the expected weight left is 100 – 5E(X) = 88.5,  
and the variance of the weight left is  
V(100 – 5X) = V(-5X) = 25V(x) = 20.25. 
 
 
39. 
 
a. The line graph of the p.m.f. of –X is just the line graph of the p.m.f. of X reflected about 
zero, but both have the same degree of spread about their respective means, suggesting 
V(-X) = V(X). 
 
b. With a = -1, b = 0, V(aX + b) = V(-X) = a2V(X). 
 
 
 
40. 
V(aX + b) = 
∑
∑
+
−
+
=
⋅
+
−
+
x
x
x
p
b
a
b
aX
x
p
b
aX
E
b
aX
)
(
)]
(
[
)
(
)]
(
[
2
2
µ
 
 
).
(
)
(
]
[
)
(
)]
(
[
2
2
2
2
X
V
a
x
p
X
a
x
p
a
aX
x
x
=
−
=
−
=
∑
∑
µ
µ
 
 
105

Chapter 3:  Discrete Random Variables and Probability Distributions 
41. 
 
a. E[X(X-1)] = E(X2) – E(X),  
⇒E(X2) = E[X(X-1)] + E(X) = 32.5 
 
b. V(X) = 32.5 – (5)2 = 7.5 
 
c. 
V(X) = E[X(X-1)] + E(X) – [E(X)]2 
 
 
42. 
With a = 1 and b = c, E(X – c) = E(aX + b) =  aE(X) + b = E(X) – c.  When    c = µ,  E(X - µ) 
= E(X) - µ = µ - µ = 0, so the expected deviation from the mean is zero. 
 
 
43. 
 
a.  
 
k 
2 
3 
4 
5 
10 
2
1
k
 
.25 
.11 
.06 
.04 
.01 
 
 
b. 
, 
64
.2
)
(
6
0
=
⋅
= ∑
=
x
x
p
x
µ
,
37
.2
)
(
2
6
0
2
2
=
−
⎥⎦
⎤
⎢⎣
⎡
⋅
= ∑
=
µ
σ
x
x
p
x
54
.1
=
σ
 
Thus µ - 2σ = -.44, and µ + 2σ = 5.72,  
so P(|x-µ| ≥ 2σ) = P(X is lat least 2 s.d.’s from µ) 
 
 
        = P(x is either ≤-.44 or ≥ 5.72) = P(X = 6) = .04. 
 
Chebyshev’s bound of .025 is much too conservative.  For K = 3,4,5, and 10, P(|x-µ| ≥ 
kσ) = 0, here again pointing to the very conservative nature of the bound 
2
1
k
. 
 
 
c. 
µ = 0 and 
3
1
=
σ
, so P(|x-µ| ≥ 3σ) = P(| X | ≥ 1) 
 
 
= P(X = -1 or +1) = 
9
1
18
1
18
1
=
+
, identical to the upper bound. 
 
d. Let p(-1) = 
25
24
50
1
50
1
)
0
(
,
)1
(
,
=
=
+
p
p
. 
 
 
 
 
106

Chapter 3:  Discrete Random Variables and Probability Distributions 
Section 3.4 
 
44. 
 
a. b(3;8,.6) = 
 = (56)(.00221184) = .124 
5
3
)
4
(.
)
6
(.
3
8
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
b. b(5;8,.6) = 
 = (56)(.00497664) = .279 
3
5
)
4
(.
)
6
(.
5
8
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
c. 
P( 3 ≤ X ≤ 5) = b(3;8,.6) + b(4;8,.6) + b(5;8,.6) = .635 
 
d. P(1 ≤ X) = 1 – P(X = 0 ) = 1 - 
 = 1 – (.9)
12
0
)
9
(.
)1
(.
0
12
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
12 = .718 
 
 
45. 
 
a. B(4;10,.3) = .850 
 
b. b(4;10,.3) = B(4;10,.3) - B(3;10,.3) = .200 
 
c. 
b(6;10,.7) = B(6;10,.7) - B(5;10,.7) = .200 
 
d. P( 2 ≤ X ≤ 4) = B(4;10,.3) - B(1;10,.3) = .701 
 
e. 
P(2 < X) = 1 - P(X ≤ 1) = 1 - B(1;10,.3) = .851 
 
f. 
P(X ≤ 1) = B(1;10,.7) = .0000 
 
g. P(2 < X < 6) = P( 3 ≤ X ≤ 5) = B(5;10,.3) - B(2;10,.3) = .570 
 
 
46. 
X ~ Bin(25, .05) 
a. P(X ≤ 2) = B(2;25,.05) = .873 
 
b. P(X ≥ 5) = 1 - P(X ≤ 4) = 1 –  B(4;25,.05) = .1 - .993 = .007 
 
c. 
P( 1 ≤ X ≤ 4) = P(X ≤ 4) – P(X ≤ 0) = .993 - .277 = .716 
 
d. P(X = 0) = P(X ≤ 0) = .277 
 
e. 
E(X) = np = (25)(.05) = 1.25 
V(X) = np(1 – p) = (25)(.05)(.95) =1.1875 
σx = 1.0897 
 
 
 
107

Chapter 3:  Discrete Random Variables and Probability Distributions 
47. 
X ~ Bin(6, .10) 
a. P(X = 1) = 
= 
 
x
n
x
p
p
x
n
−
−
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
)
1(
)
(
3543
.
)
9
(.
)1
(.
1
6
5
1
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
b. P(X ≥ 2) = 1 – [P(X = 0) + P(X = 1)].   
From a , we know P(X = 1) = .3543, and P(X = 0) =
. 
5314
.
)
9
(.
)1
(.
0
6
6
0
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
Hence P(X ≥ 2) = 1 – [.3543 + .5314] = .1143 
 
c. 
Either 4 or 5 goblets must be selected 
i) 
Select 4 goblets with zero defects: P(X = 0) = 
. 
6561
.
)
9
(.
)1
(.
0
4
4
0
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
ii) Select 4 goblets, one of which has a defect, and the 5th is good: 
 
 
26244
.
9.
)
9
(.
)1
(.
1
4
3
1
=
×
⎥
⎦
⎤
⎢
⎣
⎡
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
So the desired probability is  .6561 + .26244 = .91854 
 
 
48. 
Let S = comes to a complete stop, so p = .25 , n = 20 
 
a. P(X ≤ 6) = B(6;20,.25) = .786 
 
b. P(X = 6) = b(6;20,.20) = B(6;20,.25) - B(5;20,.25) = .786 - .617 = .169 
 
c. 
P(X ≥ 6) = 1 – P(X ≤ 5) = 1 - B(5;20,.25) = 1 - .617 = .383 
 
d. E(X) = (20)(.25) = 5.  We expect 5 of the next 20 to stop. 
 
 
49. 
Let S = has at least one citation.  Then p = .4, n = 15 
 
a. If at least 10 have no citations (Failure), then at most 5 have had at least one (Success): 
 
P(X ≤ 5) = B(5;15,.40) = .403 
 
b. P(X ≤ 7) =  B(7;15,.40) =  .787  
 
c. 
P( 5 ≤ X ≤ 10) = P(X ≤ 10) – P(X ≤ 4) = .991 - .217 = .774 
 
 
 
108

Chapter 3:  Discrete Random Variables and Probability Distributions 
50. 
X ~ Bin(10, .60) 
a. P(X ≥ 6) = 1 – P(X ≤ 5) = 1 - B(5;20,.60) = 1 - .367 = .633 
 
b. E(X) = np = (10)(.6) = 6;  V(X) = np(1 – p) = (10)(.6)(.4) = 2.4; 
σx = 1.55 
E(X) ± σx = ( 4.45, 7.55 ). 
We desire P( 5 ≤ X ≤ 7) = P(X ≤ 7) – P(X ≤ 4) = .833 - .166 = .667 
 
c. 
P( 3 ≤ X ≤ 7) = P(X ≤ 7) – P(X ≤ 2) = .833 - .012 = .821 
 
 
51. 
Let S represent a telephone that is submitted for service while under warranty and must be 
replaced.  Then p = P(S) = P(replaced | submitted)⋅P(submitted) = (.40)(.20) = .08.  Thus X, 
the number among the company’s 10 phones that must be replaced, has a binomial 
distribution with n = 10, p = .08, so p(2) = P(X=2) = 
 
1478
.
)
92
(.
)
08
(.
2
10
8
2
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
 
52. 
X ∼ Bin (25, .02) 
a. P(X=1) = 25(.02)(.98)24 = .308 
 
b. P(X≥1) = 1 – P(X=0) = 1 – (.98)25 = 1 - .603 = .397 
 
c. 
P(X≥2) = 1 – P(X≤1) = 1 – [.308 + .397] 
 
d. 
5.
)
02
(.
25
=
=
x
;  
7.
49
.
)
98
)(.
02
(.
25
=
=
=
=
npq
σ
  
9.1
4.1
5.
2
=
+
=
+ σ
x
 So P(0 ≤ X ≤ 1.9 = P(X≤1) = .705 
 
e. 
03
.3
25
)3
(
5.
24
)
5.4
(
5.
=
+
 hours 
 
 
53. 
X = the number of flashlights that work.  
Let event B = {battery has acceptable voltage}.   
Then  P(flashlight works) = P(both batteries work) = P(B)P(B) = (.9)(.9) = .81  We must 
assume that the batteries’ voltage levels are independent. 
X∼ Bin (10, .81).   P(X≥9) = P(X=9) + P(X=10) 
 
(
) (
)
(
)
407
.
122
.
285
.
81
.
10
10
19
.
81
.
9
10
10
9
=
+
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
 
 
109

Chapter 3:  Discrete Random Variables and Probability Distributions 
54. 
Let p denote the actual proportion of defectives in the batch, and X denote the number of 
defectives in the sample. 
 
a. P(the batch is accepted) = P(X ≤ 2) = B(2;10,p) 
  
p 
.01 
.05 
.10 
.20 
.25 
P(accept) 
1.00 
.988 
.930 
.678 
.526 
 
 
b.  
 
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1.0
0.5
0.0
p
P(accept)
c. 
P(the batch is accepted) = P(X ≤ 1) = B(1;10,p)  
p 
.01 
.05 
.10 
.20 
.25 
P(accept) 
.996 
.914 
.736 
.376 
.244 
 
d. P(the batch is accepted) = P(X ≤ 2) = B(2;15,p)  
p 
.01 
.05 
.10 
.20 
.25 
P(accept) 
1.00 
.964 
.816 
.398 
.236 
 
 
e. 
We want a plan for which P(accept) is high for p ≤ .1 and low for p > .1 
The plan in d seems most satisfactory in these respects. 
 
 
 
110

Chapter 3:  Discrete Random Variables and Probability Distributions 
55. 
 
a. P(rejecting claim when p = .8) = B(15;25,.8) = .017 
 
b. P(not rejecting claim when p = .7) = P(X ≥ 16 when p = .7) 
= 1 - B(15;25,.7) = 1 - .189 = .811; for p = .6, this probability is 
= 1 - B(15;25,.6) = 1 - .575 = .425. 
 
c. 
The probability of rejecting the claim when p = .8 becomes B(14;25,.8) = .006, smaller 
than in a above.  However, the probabilities of b above increase to .902 and .586, 
respectively. 
 
 
56. 
h(x) = 1 ⋅ X + 2.25(25 – X) = 62.5 – 1.5X, so E(h(X)) = 62.5 – 1.5E(x) 
 
= 62.5 – 1.5np – 62.5 – (1.5)(25)(.6) = $40.00 
 
 
57. 
If topic A is chosen,  when n = 2, P(at least half received)  
= P(X ≥ 1) = 1 – P(X = 0) = 1 – (.1)2 = .99 
If B is chosen, when n = 4, P(at least half received)  
 
= P(X ≥ 2) = 1 – P(X ≤ 1) = 1 – (0.1)4 – 4(.1)3(.9) = .9963 
 
Thus topic B should be chosen.  
If p = .5, the probabilities are .75 for A and .6875 for B, so now A should be chosen. 
 
 
58. 
 
a. np(1 – p) = 0 if either p = 0  (whence every trial is a failure, so there is no variability in 
X) or if p = 1 (whence every trial is a success and again there is no variability in X) 
 
b. 
[
)
1(
p
np
dp
d
−
] = n[(1 – p) + p(-1)] = n[1 – 2p = 0   
⇒  
p = .5, which is easily 
seen to correspond to a maximum value of V(X). 
 
 
59. 
 
a. b(x; n, 1 – p) = 
= 
 = b(n-x; n, p) 
x
n
x p
p
x
n
−
−
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
)
(
)
1(
x
x
n
p
p
x
n
n
)
1(
)
(
−
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
−
 
Alternatively, P(x S’s when P(S) = 1 – p) = P(n-x F’s when P(F) = p), since the two 
events are identical), but the labels S and F are arbitrary so can be interchanged (if P(S) 
and P(F) are also interchanged), yielding P(n-x S’s when P(S) = 1 – p) as desired. 
 
b. B(x;n,1 – p)  = P(at most x S’s when P(S) = 1 – p)  
 
 
 
= P(at least n-x F’s when P(F) = p) 
 
 
 
= P(at least n-x S’s when P(S) = p)  
 
 
 
= 1 – P(at most n-x-1 S’s when P(S) = p) 
 
 
 
= 1 – B(n-x-1;n,p) 
 
c. 
Whenever p > .5, (1 – p) < .5 so probabilities involving X can be calculated using the 
results a and b in combination with tables giving probabilities only for p ≤ .5 
 
 
111

Chapter 3:  Discrete Random Variables and Probability Distributions 
60. 
Proof of E(X) = np: 
 
E(X)  
= 
x
n
x
n
x
n
x
x
n
x
p
p
x
n
x
n
x
p
p
x
n
x
−
=
=
−
−
−
⋅
=
−
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛⋅
∑
∑
)
1(
)!
(!
!
)
1(
1
0
 
 
= 
x
n
x
n
x
x
n
x
n
x
p
p
x
n
x
n
np
p
p
x
n
x
n
−
−
=
−
=
−
−
−
−
=
−
−
−
∑
∑
)
1(
)!
(
)!
1
(
)!
1
(
)
1(
)!
(
)!
1
(
!
1
1
1
 
 
= 
y
n
y
n
y
p
p
y
n
y
n
np
−
−
=
−
−
−
−
∑
1
0
)
1(
)!
1
(
)!
(
)!
1
(
 (y replaces x-1) 
 
= 
 
⎭
⎬
⎫
⎩
⎨
⎧
−
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
∑
−
=
−
−
1
0
1
)
1(
1
n
y
y
n
y
p
p
y
n
np
The expression in braces is the sum over all possible values y = 0, 1, 2,  … , n-1 of a binomial 
p.m.f. based on n-1 trials, so equals 1, leaving only np, as desired. 
 
 
61. 
 
a. Although there are three payment methods, we are only concerned with S = uses a debit 
card and F = does not use a debit card.  Thus we can use the binomial distribution.  So n 
= 100 and p = .5.  E(X) = np = 100(.5) = 50, and V(X) = 25. 
 
b. With S = doesn’t pay with cash, n = 100 and p = .7, E(X) = np = 100(.7) = 70, and V(X) 
= 21. 
 
 
62. 
 
a. Let X = the number with reservations who show, a binomial r.v. with n = 6 and p = .8.  
The desired probability is  
P(X = 5 or 6) = b(5;6,.8) + b(6;6,.8) = .3932 + .2621 = .6553 
 
b. Let h(X) = the number of available spaces.  Then 
When x is: 
0 
1 
2 
3 
4 
5 
6 
 
H(x) is: 
4 
3 
2 
1 
0 
0 
0 
 
E[h(X)] = 
 = 4(.000) + 3(.002) = 2(.015 + 3(.082) = .277 
∑
=
⋅
6
0
)
8
,.
6;
(
)
(
x
x
b
x
h
 
c. 
Possible X values are 0, 1, 2, 3, and 4.  X = 0 if there are 3 reservations and none show or 
…or 6 reservations and none show, so 
P(X = 0) = b(0;3,.8)(.1) + b(0;4,.8)(.2) + b(0;5,.8)(.3) + b(0;6,.8)(.4)  
 
= .0080(.1) + .0016(.2) + .0003(.3) + .0001(.4) = .0013 
P(X = 1) = b(1;3,.8)(.1) + … + b(1;6,.8)(.4) = .0172 
P(X = 2) = .0906, 
P(X = 3) = .2273, 
P(X = 4) = 1 – [ .0013 + … + .2273 ] = .6636 
 
112

Chapter 3:  Discrete Random Variables and Probability Distributions 
63. 
When p = .5, µ = 10 and σ = 2.236, so 2σ = 4.472 and 3σ = 6.708.   
 
The inequality |X – 10| ≥ 4.472 is satisfied if either X ≤ 5 or X ≥ 15, or P(|X - µ| ≥ 2σ) = P(X 
≤ 5 or X ≥ 15) = .021 + .021 = .042. 
 
 
In the case p = .75, µ = 15 and σ = 1.937, so 2σ = 3.874 and 3σ = 5.811. P(|X - 15| ≥ 3.874) = 
P(X ≤ 11 or X ≥ 19) = .041 + .024 = .065, whereas  P(|X - 15| ≥ 5.811) = P(X ≤ 9) = .004.  All 
these probabilities are considerably less than the upper bounds .25(for k = 2) and .11 (for k = 
3) given by Chebyshev. 
 
 
 
Section 3.5 
 
64. 
 
a. X ∼ Hypergeometric N=15, n=5, M=6 
 
b. P(X=2) = 
280
.
3003
840
5
15
3
9
2
6
=
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
  
P(X≤2) = P(X=0) + P(X=1) + P(X=2) 
573
.
3003
1722
3003
840
756
126
3003
840
5
15
4
9
1
6
5
15
5
9
=
=
+
+
=
+
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
=
 
P(X≥2) = 1 – P(X≤1) = 1 – [P(X=0) + P(X=1)] = 
706
.
3003
756
126
1
=
+
−
 
 
c. 
E(X) = 
2
15
6
5
=
⎟
⎠
⎞
⎜
⎝
⎛
; V(X) = 
857
.
15
6
1
15
6
5
14
5
15
=
⎟
⎠
⎞
⎜
⎝
⎛−
⋅⎟
⎠
⎞
⎜
⎝
⎛⋅
⋅⎟
⎠
⎞
⎜
⎝
⎛
−
;  
926
.
)
(
=
=
X
V
σ
 
 
 
 
113

Chapter 3:  Discrete Random Variables and Probability Distributions 
65. 
X∼h(x; 6, 12, 7) 
a. P(X=5) = 
114
.
924
105
6
12
1
5
5
7
=
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
 
b. P(X≤4) = 1 – P(X≥5) = 1 – [P(X=5) + P(X=6)] = 
879
.
121
.
1
924
7
105
1
6
12
6
7
6
12
1
5
5
7
1
=
−
=
+
−
=
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎣
⎡
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
 
 
c. 
E(X) = 
5.3
12
7
6
=
⎟
⎠
⎞
⎜
⎝
⎛
⋅
; 
( )( )( )( )
892
.
795
.
6
12
5
12
7
11
6
=
=
=
σ
                                
P(X > 3.5 + .892) = P(X > 4.392) = P(X≥5) = .121 (see part b) 
 
d. We can approximate the hypergeometric distribution with the binomial if the population 
size and the number of successes are large:  h(x;15,40,400) approaches b(x;15,.10).  So 
P(X≤5) ≈ B(5; 15, .10) from the binomial tables = .998 
 
 
66. 
 
a. P(X = 10) = h(10;15,30,50) = 
2070
.
15
50
5
20
10
30
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
b. P(X ≥ 10) = h(10;15,30,50) + h(11;15,30,50) + … + h(15;15,30,50) 
 
  = .2070+.1176+.0438+.0101+.0013+.0001 = .3799 
 
c. 
P(at least 10 from the same class) = P(at least 10 from second class [answer from b]) + 
P(at least 10 from first class).  But “at least 10 from 1st class” is the same as “at most 5 
from the second”  or P(X ≤ 5). 
 
  
P(X ≤ 5) = h(0;15,30,50) + h(1;15,30,50) + … + h(5;15,30,50) 
 
        = 11697+.002045+.000227+.000150+.000001+.000000 
 
        = .01412  
So the desired probability = P(x ≥ 10) + P(X ≤ 5) 
 
 
 
 
 = .3799 + .01412 = .39402 
 
 
114

Chapter 3:  Discrete Random Variables and Probability Distributions 
d. E(X) = 
9
50
30
15
=
⋅
=
⋅N
M
n
 
V(X) = 
( )
5714
.2
50
30
1
9
49
35
=
⎟
⎠
⎞
⎜
⎝
⎛−
⎟
⎠
⎞
⎜
⎝
⎛
 
σx = 1.6036 
 
e. 
Let Y = 15 – X.  Then E(Y) = 15 – E(X) = 15 – 9 = 6 
V(Y) = V(15 – X) – V(X) = 2.5714, so σY = 1.6036 
 
 
67. 
 
a. Possible values of X are 5, 6, 7, 8, 9, 10. (In order to have less than 5 of the granite, there 
would have to be more than 10 of the basaltic).  
 P(X = 5) = h(5; 15,10,20) = 
0163
.
15
20
10
10
5
10
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
.   
 
Following the same pattern for the other values, we arrive at the pmf, in table form 
below. 
x 
5 
6 
7 
8 
9 
10 
p(x) 
.0163 
.1354 
.3483 
.3483 
.1354 
.0163 
 
 
b. P(all 10 of one kind or the other) = P(X = 5) + P(X = 10) = .0163 + .0163 = .0326 
 
c. 
E(X) = 
5.7
20
10
15
=
⋅
=
⋅N
M
n
; V(X) = 
(
)
9868
.
20
10
1
5.7
19
5
=
⎟
⎠
⎞
⎜
⎝
⎛−
⎟
⎠
⎞
⎜
⎝
⎛
;                     
σx = .9934 
 
µ ± σ = 7.5 ± .9934 = (6.5066, 8.4934), so we want  
P(X = 7) + P(X = 8) = .3483 + .3483 = .6966 
 
 
68. 
 
a. h(x; 6,4,11) 
 
b. 
18
.2
11
4
6
=
⎟
⎠
⎞
⎜
⎝
⎛⋅
 
 
 
 
 
115

Chapter 3:  Discrete Random Variables and Probability Distributions 
69. 
 
a. h(x; 10,10,20)  (the successes here are the top 10 pairs, and a sample of 10 pairs is drawn 
from among the 20) 
 
b. Let X = the number among the top 5 who play E-W.  Then P(all of top 5 play the same 
direction) = P(X = 5) + P(X = 0) = h(5;10,5,20) +  h(5;10,5,20) 
 
 
= 
033
.
10
20
10
15
10
20
5
15
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
 
c. 
N = 2n; M = n; n = n 
h(x;n,n,2n) 
E(X) = 
n
n
n
n
2
1
2
=
⋅
; 
V(X) = 
⎟
⎠
⎞
⎜
⎝
⎛⋅
⋅⎟
⎠
⎞
⎜
⎝
⎛
−
=
⎟
⎠
⎞
⎜
⎝
⎛−
⋅
⋅⎟
⎠
⎞
⎜
⎝
⎛
−
=
⎟
⎠
⎞
⎜
⎝
⎛−
⋅
⋅
⋅⎟
⎠
⎞
⎜
⎝
⎛
−
−
2
1
2
1
2
2
1
2
1
2
2
1
2
1
2
2
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
 
 
 
70. 
 
a. h(x;10,15,50) 
 
b. When N is large relative to n, h(x;n,M,N) 
,
,
;
⎟
⎠
⎞
⎜
⎝
⎛
=
N
M
n
x
b
&
 
so h(x;10,150,500) 
 
(
)
3
,.
10
;x
b
=&
 
c. 
Using the hypergeometric model, E(X) = 
3
500
150
10
=
⎟
⎠
⎞
⎜
⎝
⎛⋅
 and  
V(X) = 
06
.2
)1.2
(
982
.
)
7
)(.
3
)(.
10
(
499
490
=
=
 
Using the binomial model, E(X) = (10)(.3) = 3, and  
V(X) = 10(.3)(.7) = 2.1 
 
 
 
116

Chapter 3:  Discrete Random Variables and Probability Distributions 
71. 
 
a. With S = a female child and F = a male child, let X = the number of F’s before the 2nd S.  
Then P(X = x) = nb(x;2, .5) 
 
b. P(exactly 4 children) = P(exactly 2 males) 
 
 
    = nb(2;2,.5) = (3)(.0625) = .188 
 
c. 
P(at most 4 children) = P(X ≤ 2)  
 
 
= ∑
= .25+2(.25)(.5) + 3(.0625) = .688 
=
2
0
)
5
,.
2;
(
x
x
nb
 
d. E(X) = 
2
5.
)
5
)(.
2
(
=
, so the expected number of children = E(X + 2)  
= E(X) + 2 = 4 
 
 
72. 
The only possible values of X are 3, 4, and 5. 
p(3) = P(X = 3) = P(first 3 are B’s or first 3 are G’s) = 2(.5)3 = .250 
p(4) = P(two among the 1st three are B’s and the 4th is a B) + P(two among the 1st three are 
G’s and the 4th is a G) = 
 
375
.
)
5
(.
2
3
2
4 =
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛⋅
p(5) = 1 – p(3) – p(4) = .375 
 
 
73. 
This is identical to an experiment in which a single family has children until exactly 6 females 
have been born( since p = .5 for each of the three families), so p(x) = nb(x;6,.5) and E(X) = 6 
( = 2+2+2, the sum of the expected number of males born to each one.) 
 
 
74. 
The interpretation of “roll” here is a pair of tosses of a single player’s die(two tosses by A or 
two by B). With S = doubles on a particular roll, p = 6
1 .  Furthermore, A and B are really 
identical (each die is fair), so we can equivalently imagine A rolling until 10 doubles appear.  
The P(x rolls) = P(9 doubles among the first x – 1 rolls and a double on the xth roll = 
10
10
9
10
6
1
6
5
9
1
6
1
6
1
6
5
9
1
⎟
⎠
⎞
⎜
⎝
⎛
⎟
⎠
⎞
⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
=
⎟
⎠
⎞
⎜
⎝
⎛⋅
⎟
⎠
⎞
⎜
⎝
⎛
⎟
⎠
⎞
⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
−
−
x
x
x
x
 
E(X) = 
50
)
5
(
10
)
(
10
)
1(
6
1
6
5
=
=
=
−
p
p
r
  V(X) = 
( )
300
)
6
)(
5
(
10
)
(
10
)
1(
2
6
1
6
5
2
=
=
=
−
p
p
r
 
 
 
 
 
117

Chapter 3:  Discrete Random Variables and Probability Distributions 
Section 3.6 
 
75. 
 
a. P(X ≤ 8) = F(8;5) = .932 
 
b. P(X = 8) = F(8;5) - F(7;5) = .065 
 
c. 
P(X ≥ 9) = 1 – P(X ≤ 8) = .068 
 
d. P(5 ≤ X ≤ 8) = F(8;5) – F(4;5) = .492 
 
e. 
P(5 < X < 8) = F(7;5) – F(5;5) = .867-.616=.251 
 
 
76. 
 
a. P(X ≤ 5) = F(5;8) = .191 
 
b. P(6 ≤ X ≤ 9) = F(9;8) – F(5;8) = .526 
 
c. 
P(X ≥ 10) = 1 – P(X ≤ 9) = .283 
 
d. E(X) = λ= 10,  σX = 
83
.2
=
λ
, so P(X > 12.83) = P(X ≥ 13) = 1 – P(X ≤ 12) =1 - 
.936 = .064 
 
77. 
 
a. P(X ≤ 10) = F(10;20) = .011 
 
b. P(X > 20) = 1 – F(20;20) = 1 - .559 = .441 
 
c. 
P(10 ≤ X ≤ 20) = F(20;20) – F(9;20) = .559 - .005 = .554 
 
P(10 < X < 20) = F(19;20) – F(10;20) = .470 - .011 = .459 
 
d. E(X) = λ= 20,  σX = 
472
.4
=
λ
 
P(µ - 2σ < X < µ + 2σ )  
=  P(20 – 8.944 < X < 20 + 8.944) 
 
 
 
 
= P(11.056 < X < 28.944)  
 
 
 
 
= P(X ≤ 28) - P(X ≤ 11) 
 
 
 
 
= F(28;20) - F(12;20)] 
 
 
 
 
= .966 - .021 = .945 
 
 
78. 
 
a. P(X = 1) = F(1;2) – F(0;2) = .982 - .819 = .163 
 
b. P(X ≥ 2) = 1 – P(X ≤ 1) = 1 – F(1;2) = 1 - .982 = .018 
 
c. 
P(1st doesn’t ∩ 2nd doesn’t) = P(1st doesn’t) ⋅ P(2nd doesn’t) 
 = (.819)(.819) = .671 
 
 
 
118

Chapter 3:  Discrete Random Variables and Probability Distributions 
79. 
200
1
=
p
; n = 1000; λ = np = 5 
a. P(5 ≤ X ≤ 8) = F(8;5) – F(4;5) = .492 
 
b. P(X ≥ 8) = 1 – P(X ≤ 7) = 1 - .867 = .133 
 
 
80. 
 
a. The experiment is binomial with n = 10,000 and p = .001,  
so µ = np = 10 and σ = 
161
.3
=
npq
. 
 
b. X has approximately a Poisson distribution with λ = 10,  
so P(X > 10) ≈ 1 – F(10;10) = 1 - .583 = .417 
 
c. 
P(X = 0) ≈ 0  
 
 
81. 
 
a. λ = 8 when t = 1, so P(X = 6) = F(6;8) – F(5;8) =.313 -  .191  = .122,  
 
P(X ≥ 6) = 1 - F(5;8) = .809, and P(X ≥ 10) = 1 - F(9;8) = .283 
 
b. t = 90 min = 1.5 hours, so λ = 12; thus the expected number of arrivals is 12 and the SD 
=
464
.3
12 =
 
 
c. 
t = 2.5 hours implies that λ = 20; in this case, P(X ≥ 20) = 1 – F(19;20) = .530 and P(X ≤ 
10) = F(10;20) = .011. 
 
 
82. 
 
a. P(X = 4) = F(4;5) – F(3;5) = .440 - .265 = .175 
 
b. P(X ≥ 4) = 1 - P(X ≤ 3) = 1 - .265 = .735 
 
c. 
Arrivals occur at the rate of 5 per hour, so for a 45 minute period the rate is  λ = (5)(.75) 
= 3.75, which is also the expected number of arrivals in a 45 minute period. 
 
 
83. 
 
a. For a two hour period the parameter of the distribution is  λt = (4)(2) = 8,  
so  P(X = 10) = F(10;8) – F(9;8) = .099. 
 
b. For a 30 minute period, λt = (4)(.5) = 2, so  P(X = 0) = F(0,2) = .135 
 
c. 
E(X) = λt = 2 
 
 
 
119

Chapter 3:  Discrete Random Variables and Probability Distributions 
84. 
Let X = the number of diodes on a board that fail. 
 
a. E(X) = np = (200)(.01) = 2, V(X) = npq = (200)(.01)(.99) = 1.98, σX = 1.407 
 
b. X has approximately a Poisson distribution with λ = np = 2,  
so P(X ≥ 4) = 1 – P(X ≤ 3) = 1 – F(3;2) = 1 - .857 = .143 
 
c. 
P(board works properly) = P(all diodes work) = P(X = 0) = F(0;2) = .135 
 
Let Y = the number among the five boards that work, a binomial r.v. with n = 5 and p = 
.135.  Then P(Y ≥ 4) = P(Y = 4 ) + P(Y = 5) = 
 = .00144 + .00004 = .00148 
0
5
4
)
865
(.
)
135
(.
5
5
)
865
(.
)
135
(.
4
5
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
 
85. 
α = 1/(mean time between occurrences) = 
2
5.
1 =
 
a. αt = (2)(2) = 4 
 
b. P(X > 5 ) 1 – P(X ≤ 5) = 1 - .785 = .215 
 
c. 
Solve for t , given α = 2: 
 
.1 = e-αt
 
ln(.1) = -αt 
 
t = 
15
.1
2
3026
.2
≈
years 
 
 
86. 
E(X) = 
λ
λ
λ
λ
λ
λ
λ
λ
λ
λ
λ
=
=
=
=
∑
∑
∑
∑
∞
=
−
∞
=
−
∞
=
−
∞
=
−
0
1
1
0
!
!
!
!
y
y
x
x
x
x
x
x
y
e
x
x
e
x
x
e
x
x
e
x
 
 
 
87. 
 
a. For a one-quarter acre plot, the parameter is (80)(.25) = 20,  
 
so P(X ≤ 16) = F(16;20) = .221 
 
b. The expected number of trees is λ⋅(area) = 80(85,000) = 6,800,000. 
 
c. 
The area of the circle is πr2 = .031416 sq. miles or 20.106 acres.  Thus X has a Poisson 
distribution with parameter 20.106 
 
 
120

Chapter 3:  Discrete Random Variables and Probability Distributions 
88. 
 
a. P(X = 10 and no violations) = P(no violations | X = 10) ⋅ P(X = 10) 
 
 
 
 
= (.5)10 ⋅ [F(10;10) – F(9;10)] 
 
 
 
 
= (.000977)(.125) = .000122 
 
b. P(y arrive and exactly 10 have no violations)  
= P(exactly 10 have no violations | y arrive) ⋅ P(y arrive)  
= P(10 successes in y trials when p = .5) ⋅ 
!
)
10
(
10
y
e
y
−
 
= 
)!
10
(!
10
)
5
(
!
)
10
(
)
5
(.
)
5
(.
10
10
10
10
10
−
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
−
−
y
e
y
e
y
y
y
y
 
 
c. 
P(exactly 10 without a violation) = ∑
∞
=
−
−
10
10
)!
10
(!
10
)
5
(
y
y
y
e
  
 
 
= 
∑
∞
=
−
−
−
⋅
10
10
10
10
)!
10
(
)
5
(
!
10
5
y
y
y
e
 = 
∑
∞
=
−
⋅
0
10
10
)!
(
)
5
(
!
10
5
u
u
u
e
 = 
5
10
10
!
10
5
e
e
⋅
⋅
−
 
 
 
= 
!
10
510
5 ⋅
−
e
 = p(10;5).   
 
In fact, generalizing this argument shows that the number of “no-violation” arrivals 
within the hour has a Poisson distribution with parameter 5; the 5 results from λp = 
10(.5). 
 
 
89. 
 
a. No events in (0, t+∆t) if and only if no events in (o, t) and no events in (t, t+∆t).  Thus, P0 
(t+∆t) = P0(t) ⋅P(no events in (t, t+∆t))  
= P0(t)[1 - λ ⋅ ∆t – o(∆t)] 
 
b. 
t
t
o
t
P
t
t
t
P
t
t
P
t
t
P
∆
∆
⋅
−
∆′
∆′
−
=
∆
−
∆
+
)
(
)
(
)
(
)
(
)
(
0
0
0
0
λ
 
c. 
[
t
e
dt
d
λ
−] = -λe-λt = -λP0(t) , as desired. 
d. 
!
)
(
!
)
(
!
)
(
1
k
t
e
k
k
t
e
k
t
e
dt
d
k
t
k
t
k
t
−
−
−
−
+
−
=
⎥⎦
⎤
⎢⎣
⎡
λ
λ
λ
λ
λ
λ
λ
λ
 
 
= 
=
−
+
−
−
−
−
)!
1
(
)
(
!
)
(
1
k
t
e
k
t
e
k
t
k
t
λ
λ
λ
λ
λ
λ
  -λPk(t) + λPk-1(t) as desired. 
 
 
 
 
121

Chapter 3:  Discrete Random Variables and Probability Distributions 
Supplementary Exercises 
 
90. 
Outcomes are(1,2,3)(1,2,4) (1,2,5) … (5,6,7); there are 35 such outcomes. Each having 
probability 35
1 .  The W values for these outcomes are 6 (=1+2+3), 7, 8, …, 18.  Since there is 
just one outcome with W value 6, p(6) = P(W = 6) = 35
1 .  Similarly, there are three outcomes 
with W value 9 [(1,2,6) (1,3,5) and 2,3,4)], so p(9) = 35
3 .  Continuing in this manner yields 
the following distribution: 
 
W 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
P(W) 
35
1  
35
1  
35
2  
35
3  
35
4  
35
4  
35
5  
35
4  
35
4  
35
3  
35
2  
35
1  
35
1  
Since the distribution is symmetric about 12, µ = 12, and 
 
∑
=
−
=
18
6
2
2
)
(
)
12
(
w
w
p
w
σ
 
= 35
1 [(6)2(1) + (5)2(1) + … + (5)2(1) + (6)2(1) = 8 
 
 
91. 
 
a. p(1) = P(exactly one suit) = P(all spades) + P(all hearts) + P(all diamonds)  
 
+ P(all clubs) = 4P(all spades) = 
00198
.
5
52
5
13
4
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⋅
 
p(2) = P(all hearts and spades with at least one of each) + …+ P(all diamonds and clubs 
with at least one of each)  
= 6 P(all hearts and spades with at least one of each) 
= 6 [ P( 1 h and 4 s) + P( 2 h and 3 s) + P( 3 h and 2 s) + P( 4 h and 1 s)] 
=
14592
.
960
,
598
,2
616
,
44
590
,
18
6
5
52
2
13
3
13
2
5
52
1
13
4
13
2
6
=
⎥⎦
⎤
⎢⎣
⎡
+
=
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎣
⎡
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⋅
+
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⋅
⋅
 
 
p(4) = 4P(2 spades, 1 h, 1 d, 1 c) = 
26375
.
5
52
)
13
)(
13
)(
13
(
2
13
4
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛⋅
 
 
p(3) = 1 – [p(1) + p(2) + p(4)] = .58835 
 
b. µ = 
 
,
114
.3
)
(
4
1
=
⋅
∑
=
x
x
p
x
(
)
636
.
,
405
.
114
.3
)
(
2
4
1
2
2
=
=
−
⎥⎦
⎤
⎢⎣
⎡
⋅
= ∑
=
σ
σ
x
x
p
x
 
 
 
122

Chapter 3:  Discrete Random Variables and Probability Distributions 
92. 
p(y) = P(Y = y) = P(y trials to achieve r S’s) = P(y-r F’s before rth S)  
= nb(y – r;r,p) =
, y = r, r+1, r+2, … 
r
y
r
p
p
r
y
−
−
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
−
)
1(
1
1
 
 
93. 
 
a. b(x;15,.75) 
 
b. P(X > 10) = 1 - B(9;15, .75) = 1 - .148 
 
c. 
B(10;15, .75) - B(5;15, .75) = .314 - .001 = .313 
 
d. µ = (15)(.75) = 11.75, σ2= (15)(.75)(.25) = 2.81 
 
e. 
Requests can all be met if and only if X ≤ 10, and 15 – X ≤ 8, i.e. if  7 ≤ X ≤ 10, so P(all 
requests met) = B(10; 15,.75) - B(6; 15,.75) = .310 
 
 
94. 
P( 6-v light works) = P(at least one 6-v battery works) = 1 – P(neither works)  
 
= 1 –(1 – p)2.   P(D light works) = P(at least 2 d batteries work) = 1 – P(at most 1 D battery 
works) = 1 – [(1 – p)4 + 4(1 – p)3].  The 6-v should be taken if  1 –(1 – p)2   
≥ 1 – [(1 – 
p)4 + 4(1 – p)3]. 
 
Simplifying, 
1 ≤ (1 – p)2 + 4p(1- p)  ⇒  0  ≤ 2p – 3p3  ⇒ p ≤ 3
2 . 
 
 
95. 
Let X ~ Bin(5, .9).  Then P(X ≥ 3) = 1 – P(X ≤ 2) = 1 – B(2;5,.9) = .991 
 
 
96. 
 
a. P(X ≥ 5) = 1 - B(4;25,.05) = .007 
 
b. P(X ≥ 5) = 1 - B(4;25,.10) = .098 
 
c. 
P(X ≥ 5) = 1 - B(4;25,.20) = .579 
 
d. All would decrease, which is bad if the % defective is large and good if the % is small. 
 
 
97. 
 
a. N = 500, p = .005, so np = 2.5 and b(x; 500, .005) =& p(x; 2.5), a Poisson p.m.f. 
 
b. P(X = 5) = p(5; 2.5) - p(4; 2.5) = .9580 - .8912 = .0668 
 
c. 
P(X ≥ 5) = 1 – p(4;2.5) = 1 - .8912 = .1088 
 
 
 
123

Chapter 3:  Discrete Random Variables and Probability Distributions 
98. 
X  ~ B(x; 25, p).   
a. B(18; 25, .5) – B(6; 25, .5) = .986 
 
b. B(18; 25, .8) – B(6; 25, .8) = .220 
 
c. 
With p = .5, P(rejecting the claim) = P(X ≤ 7) + P(X ≥ 18)  =  .022 + [1 - .978] = .022 + 
.022 = .044 
 
d. The claim will not be rejected when 8 ≤ X ≤ 17.   
With p=.6, P(8 ≤ X ≤ 17) = B(17;25,.6) – B(7;25,.6) = .846 - .001 = .845. 
With p=.8, P(8 ≤ X ≤ 17) = B(17;25,.8) – B(7;25,.8) = .109 - .000 = .109. 
 
e. 
We want P(rejecting the claim) ≤ .01.  Using the decision rule “reject if   X ≤ 6 or X ≥ 
19” gives the probability .014, which is too large.  We should use “reject if   X ≤ 5 or X ≥ 
20” which yields  P(rejecting the claim) = .002 + .002 = .004. 
 
 
99. 
Let Y denote the number of tests carried out.  For n = 3, possible Y values are 1 and 4.  P(Y = 
1) = P(no one has the disease) = (.9)3 = .729 and P(Y = 4) = .271, so E(Y) = (1)(.729) + 
(4)(.271) = 1.813, as contrasted with the 3 tests necessary without group testing. 
 
 
100. 
Regard any particular symbol being received as constituting a trial.  Then p = P(S) = 
P(symbol is sent correctly or is sent incorrectly and subsequently corrected) = 1 – p1 + p1p2.  
The block of n symbols gives a binomial experiment with n trials and p =  1 – p1 + p1p2. 
 
 
101. 
p(2) = P(X = 2) = P(S on #1 and S on #2) = p2 
p(3) = P(S on #3 and S on #2 and F on #1) = (1 – p)p2
p(4) = P(S on #4 and S on #3 and F on #2) = (1 – p)p2
p(5) = P(S on #5 and S on #4 and F on #3 and no 2 consecutive S’s on trials prior to #3) = [ 1 
– p(2) ](1 – p)p2
p(6) = P(S on #6 and S on #5 and F on #4 and no 2 consecutive S’s on trials prior to #4) = [ 1 
– p(2) – p(3)](1 – p)p2
In general, for x = 5, 6, 7, …:  p(x) = [ 1 – p(2) - … – p(x - 3)](1 – p)p2
For p = .9, 
x 
2 
3 
4 
5 
6 
7 
8 
p(x) 
.81 
.081 
.081 
.0154 
.0088 
.0023 
.0010 
 
So P(X ≤ 8) = p(2) + … + p(8) = .9995 
 
 
102. 
 
a. With X ~ Bin(25, .1),P(2 ≤ X ≤ 6) = B(6;25,.1 – B(1;25,.1) = .991 - .271 = 720 
 
b. E(X) = np = 25(.1) = 2.5, σX = 
50
.1
25
.2
)
9
)(.
1
(.
25
=
=
=
npq
 
 
c. 
P(X ≥ 7 when p = .1) = 1 – B(6;25,.1) = 1 - .991 = .009 
 
d. P(X ≤6 when p = .2) = B(6;25,.2) = = .780, which is quite large 
 
124

Chapter 3:  Discrete Random Variables and Probability Distributions 
103. 
 
a. Let event C = seed carries single spikelets, and event P = seed produces ears with single 
spikelets.  Then P( P ∩ C) = P(P | C) ⋅ P(C) = .29 (.40) = .116.  Let X = the number of 
seeds out of the 10 selected that meet the condition P ∩ C. Then X ~ Bin(10, .116).     
P(X = 5) = 
 
002857
.
)
884
(.
)
116
(.
5
10
5
5
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
b. For 1 seed, the event of interest is P  = seed produces ears with single spikelets.          
P(P) =  P( P ∩ C) + P( P ∩ C′) = .116 (from a) + P(P | C′) ⋅ P(C′)  
= .116 + (.26)(.40) = .272. 
Let Y = the number out of the 10 seeds that meet condition P.   
Then Y ~ Bin(10, .272), and P(Y = 5) = .0767. 
P(Y ≤ 5) = b(0;10,.272) + … + b(5;10,.272) = .041813 + … + .076719 = .97024 
 
 
104. 
With S = favored acquittal, the population size is N = 12, the number of population S’s is M = 
4, the sample size is n = 4, and the p.m.f. of the number of interviewed jurors who favor 
acquittal is the hypergeometric p.m.f. h(x;4,4,12). E(X) = 
33
.1
12
4
4
=
⎟
⎠
⎞
⎜
⎝
⎛⋅
 
 
105. 
 
a. P(X = 0) = F(0;2) 0.135 
 
b. Let S = an operator who receives no requests.  Then p = .135 and we wish P(4 S’s in 5 
trials) = b(4;5,..135) = 
 
00144
.
)
884
(.
)
135
(.
4
5
1
4
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
c. 
P(all receive x) = P(first receives x) ⋅ … ⋅ P(fifth receives x) = 
5
2
!
2
⎥⎦
⎤
⎢⎣
⎡
−
x
e
x
, and P(all 
receive the same number ) is the sum from x = 0 to ∞. 
 
 
106. 
P(at least one) = 1 – P(none) = 1 - 
!0
)
(
0
2
2
R
e
R
λπ
λπ
⋅
−
 = 1 - 
= .99 ⇒ 
= .01  
2
R
e λπ
−
2
R
e λπ
−
 
⇒ 
λπ
)
01
(.
1
2
n
R
−
=
 = .7329 ⇒ R = .8561 
 
 
107. 
The number sold is min (X, 5), so E[ min(x, 5)] = 
 
∑
∞
)
4;
(
)
5,
min(
x
p
x
= (0)p(0;4) + (1) p(1;4) + (2) p(2;4) + (3) p(3;4) + (4) p(4;4) + 
 
∑
∞
=5
)
4;
(
5
x
x
p
 = 1.735 + 5[1 – F(4;4)] = 3.59 
 
125

Chapter 3:  Discrete Random Variables and Probability Distributions 
108. 
 
a. P(X = x) = P(A wins in x games) + P(B wins in x games)  
= P(9 S’s in 1st x-1 ∩ S on the xth) + P(9 F’s in 1st x-1 ∩ F on the xth) 
= 
 + 
 
p
p
p
x
x 10
9
)
1(
9
1
−
−
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
)
1(
)
1(
9
1
10
9
p
p
p
x
x
−
−
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
−
= 
 
[
]
10
10
10
10
)
1(
)
1(
9
1
−
−
−
+
−
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
x
x
p
p
p
p
x
 
b. Possible values of X are now 10, 11, 12, …( all positive integers ≥ 10). Now  
 
P(X = x) = 
  for x = 10, … , 19,  
[
]
10
10
10
10
)
1(
)
1(
9
1
−
−
−
+
−
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
x
x
q
q
p
p
x
 
So P(X ≥ 20) = 1 – P(X < 20) and P(X < 20) = 
 
∑
=
=
19
10
)
(
x
x
X
P
 
 
109. 
 
a. No; probability of success is not the same for all tests 
 
b. There are four ways exactly three could have positive results.  Let D represent those with 
the disease and D′ represent those without the disease. 
 
Combination 
Probability 
D 
D′ 
 
0 
 
3 
⎥
⎦
⎤
⎢
⎣
⎡
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⋅
⎥
⎦
⎤
⎢
⎣
⎡
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
2
3
5
0
)1
(.
)
9
(.
3
5
)
8
(.
)
2
(.
0
5
=(.32768)(.0729) = .02389 
 
1 
2 
⎥
⎦
⎤
⎢
⎣
⎡
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⋅
⎥
⎦
⎤
⎢
⎣
⎡
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
3
4
1
)1
(.
2
)
9
(.
2
5
)
8
(.
)
2
(.
1
5
=(.4096)(.0081) = .00332 
 
2 
1 
⎥
⎦
⎤
⎢
⎣
⎡
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⋅
⎥
⎦
⎤
⎢
⎣
⎡
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
4
1
3
2
)1
(.
)
9
(.
1
5
)
8
(.
)
2
(.
2
5
=(.2048)(.00045) = .00009216 
 
3 
0 
⎥
⎦
⎤
⎢
⎣
⎡
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⋅
⎥
⎦
⎤
⎢
⎣
⎡
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
5
0
2
3
)1
(.
)
9
(.
0
5
)
8
(.
)
2
(.
3
5
=(.0512)(.00001) = .000000512 
 
Adding up the probabilities associated with the four combinations yields 0.0273. 
 
126

Chapter 3:  Discrete Random Variables and Probability Distributions 
110. 
k(r,x) = 
!
)
)...(
2
)(
1
(
x
x
r
x
r
x
r
x
−
+
−
+
−
+
 
With r = 2.5 and p = .3, p(4) = 
1068
.
)
7
(.
)
3
(.
!4
)
5.2
)(
5.3
)(
5.4
)(
5.5
(
4
5.2
=
 
Using k(r,0) = 1, P(X ≥ 1) = 1 – p(0) = 1 – (.3)2.5 = .9507 
 
 
111. 
 
a. p(x;λ,µ) = 
)
;
(
)
;
(
2
1
2
1
µ
λ
x
p
x
p
+
where both p(x;λ) and p(x; µ) are Poisson p.m.f.’s 
and thus ≥ 0, so p(x;λ,µ) ≥ 0.  Further, 
1
2
1
2
1
)
;
(
2
1
)
;
(
2
1
)
,
;
(
0
0
0
=
+
=
+
=
∑
∑
∑
∞
=
∞
=
∞
=
x
x
x
x
p
x
p
x
p
µ
λ
µ
λ
 
 
b. 
)
;
(
4.
)
;
(
6.
µ
λ
x
p
x
p
+
 
 
c. 
E(X) = 
)
;
(
2
1
)
;
(
2
1
)]
;
(
2
1
)
;
(
2
1
[
0
0
0
µ
λ
µ
λ
x
p
x
x
p
x
x
p
x
p
x
x
x
x
∑
∑
∑
∞
=
∞
=
∞
=
+
=
+
 
= 
2
2
1
2
1
µ
λ
µ
λ
+
=
+
 
 
d. E(X2) = 
)
(
2
1
)
(
2
1
)
;
(
2
1
)
;
(
2
1
2
2
0
2
0
2
µ
µ
λ
λ
µ
λ
+
+
+
=
+ ∑
∑
∞
=
∞
=
x
p
x
x
p
x
x
x
(since for a 
Poisson r.v., E(X2) = V(X) + [E(X)]2 = λ + λ2),  
so V(X) = [
]
2
2
2
2
1
2
2
2
2
µ
λ
µ
λ
µ
λ
µ
µ
λ
λ
+
+
⎟
⎠
⎞
⎜
⎝
⎛
−
=
⎥⎦
⎤
⎢⎣
⎡
+
−
+
+
+
 
 
 
112. 
 
a. 
1
)
1(
)1
(
)
(
)
,
;
(
)
,
;1
(
>
−
⋅
+
−
=
+
p
p
x
x
n
p
n
x
b
p
n
x
b
  if np – (1 – p) > x, from which the stated 
conclusion follows. 
 
b. 
1
)1
(
)
;
(
)
;1
(
>
+
=
+
x
x
p
x
p
λ
λ
λ
  if  x < λ - 1 , from which the stated conclusion follows.  If 
λ is an integer, then λ - 1 is a mode, but p(λ,λ) = p(1 - λ, λ) so λ is also a mode[p(x; λ)] 
achieves its maximum for both x = λ - 1 and x = λ. 
 
 
 
127

Chapter 3:  Discrete Random Variables and Probability Distributions 
113. 
P(X = j) = 
(arm on track i ∩ X = j) = 
(X = j | arm on i  ) ⋅ p
∑
=
10
1
i
P
∑
=
10
1
i
P
i 
 
 
= ∑
(next seek at I+j+1 or I-j-1) ⋅ p
=
10
1
i
P
i  = 
 
∑
=
−
−
+
+
+
10
1
1
1
)
(
i
i
j
i
j
i
p
P
p
 
 
where pk = 0 if k < 0 or k > 10. 
 
 
114. 
E(X) = 
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
−
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⋅
∑
=
n
x
n
N
x
n
M
N
x
M
x
0
∑
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
−
⋅
−
−
n
x
n
N
x
n
M
N
x
M
x
M
1
)!
(
)!
1
(
!
 
 
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
−
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
−
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
−
⋅
∑
=
n
x
n
N
x
n
M
N
x
M
N
M
n
1
1
1
1
1
∑
−
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
−
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
−
−
−
−
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
⋅
1
0
1
1
1
)1
(
1
1
n
y
n
N
y
n
M
N
y
M
N
M
n
 
 
N
M
n
N
M
n
y
h
N
M
n
n
y
⋅
=
−
−
−
⋅
∑
−
=
1
0
)1
,1
,1
;
(
 
 
 
115. 
Let A = {x: |x - µ| ≥ kσ}.  Then σ2 = 
.  But  
∑
∑
≥
−
A
A
x
p
k
x
p
x
)
(
)
(
)
(
)
(
2
2
σ
µ
∑
A
x
p
)
(
= P(X is in A) = P(|X - µ| ≥ kσ), so  σ2 ≥ k2σ2⋅ P(|X - µ| ≥ kσ), as desired. 
 
 
116. 
 
a. For [0,4],  λ = 
 = 123.44, whereas for [2,6], λ = 
 = 409.82 
∫
+
4
0
6.
2
dt
e
t
∫
+
6
2
6.
2
dt
e
t
 
b. λ = 
  = 9.9996 ≈ 10, so the desired probability is F(15, 10) = .951. 
∫
+
9907
.0
0
6.
2
dt
e
t
 
 
 
 
 
 
 
128

CHAPTER 4 
 
Section 4.1 
 
1. 
 
a. P(x ≤ 1) = 
]
25
.
)
(
1
0
2
4
1
1
0 2
1
1
=
=
= ∫
∫∞
−
x
xdx
dx
x
f
 
 
b. P(.5 ≤ X ≤ 1.5) = 
]
5.
5.1
5.
2
4
1
5.1
5.
2
1
=
=
∫
x
xdx
 
 
c. 
P(x > 1.5) = 
]
438
.
)
(
16
7
2
5.1
2
4
1
2
5.1
2
1
5.1
≈
=
=
= ∫
∫
∞
x
xdx
dx
x
f
 
 
 
2. 
F(x) = 10
1  for –5 ≤ x ≤ 5, and = 0 otherwise 
a. P(X < 0) = 
5.
0
5 10
1
=
∫−
dx
 
 
b. P(-2.5 < X < 2.5) = 
5.
5.2
5.2
10
1
=
∫−
dx
 
 
c. 
P(-2 ≤ X ≤ 3) = 
5.
3
2 10
1
=
∫−
dx
 
 
d. P( k < X < k + 4) = 
]
4.
]
)
4
[(
10
1
4
10
4
10
1
=
−
+
=
=
+
+∫
k
k
dx
k
k
x
k
k
 
 
3. 
 
a. Graph of  f(x) = .09375(4 – x2)  
 
3
2
1
0
-1
-2
-3
0.5
0.0
-0.5
x1
f(x1)
 
129

Chapter 4:  Continuous Random Variables and Probability Distributions 
b. P(X > 0) = 
5.
)
3
4
(
09375
.
)
4
(
09375
.
2
0
3
2
0
2
=
⎥⎦
⎤
−
=
−
∫
x
x
dx
x
 
 
c. 
P(-1 < X < 1) = 
 
6875
.
)
4
(
09375
.
1
1
2
=
−
∫−
dx
x
 
d. P(x < -.5 OR x > .5) = 1 – P(-.5 ≤ X ≤ .5) = 1 - 
 
∫−
−
5.
5.
2)
4
(
09375
.
dx
x
 
 
 
 
  = 1 - .3672 = .6328 
 
 
4. 
 
a. 
]
1
)1
(
0
)
;
(
0
2
/
0
2
/
2
2
2
2
2
=
−
−
=
−
=
=
∞
−
∞
−
∞
∞
−
∫
∫
θ
θ
θ
θ
x
x
e
dx
e
x
dx
x
f
 
 
b. P(X ≤ 200) = 
∫
∫
−
∞
−
=
200
0
2
/
2
200
2
2
)
;
(
dx
e
x
dx
x
f
x
θ
θ
θ
 
]
8647
.
1
1353
.
200
0
2
/
2
2
=
+
−
≈
−
=
−
θ
x
e
 
P(X  < 200) = P(X ≤ 200) ≈  .8647, since x is continuous. 
P(X ≥ 200) = 1 - P(X ≤ 200) ≈  .1353 
 
c. 
P(100 ≤ X ≤ 200) = 
 
=
∫
200
100
)
;
(
dx
x
f
θ
]
4712
.
200
100
000
,
20
/
2
≈
−
−x
e
 
d. For x > 0, P(X ≤ x) = 
=
∫∞
−
x
dy
y
f
)
;
(
θ
∫
−
x
y
dx
e
e
y
0
2
/
2
2
2
θ
]
2
2
2
2
2
/
0
2
/
1
θ
θ
x
x
y
e
e
−
−
−
=
−
=
 
 
 
5. 
 
a. 1 = 
( )]
( )
8
3
3
8
2
0
3
2
0
2
3
)
(
=
⇒
=
=
= ∫
∫
∞
∞
−
k
k
k
dx
kx
dx
x
f
x
 
 
b. P(0 ≤ X ≤ 1) = 
]
125
.
8
1
1
0
3
8
1
1
0
2
8
3
=
=
=
∫
x
dx
x
 
 
c. 
P(1 ≤ X ≤ 1.5) = 
]
( )
( )
2969
.
1
64
19
3
8
1
3
2
3
8
1
5.1
1
3
8
1
5.1
1
2
8
3
≈
=
−
=
=
∫
x
dx
x
 
 
d. P(X ≥ 1.5) = 1 - 
]
( )
[
]
5781
.
1
0
1
64
37
64
27
3
2
3
8
1
5.1
0
3
8
1
5.1
0
2
8
3
≈
=
−
=
−
−
=
=
∫
x
dx
x
 
 
130

Chapter 4:  Continuous Random Variables and Probability Distributions 
6. 
 
a.  
5
4
3
2
1
0
2
1
0
x
f(x)
b. 1 = 
∫
∫
−
=
⇒
=
−
=
−
−
1
1
2
4
2
2
4
3
3
4
]
1[
]
)
3
(
1[
k
du
u
k
dx
x
k
 
 
c. 
P(X > 3) = 
5.
]
)
3
(
1[
4
3
2
4
3
=
−
−
∫
dx
x
 by symmetry of the p.d.f 
 
d. 
(
)
367
.
128
47
]
)
(
1[
]
)
3
(
1[
4
/
1
4
/
1
2
4
3
4
/
13
4
/
11
2
4
3
4
13
4
11
≈
=
−
=
−
−
=
≤
≤
∫
∫
−
du
u
dx
x
X
P
 
 
e. 
P( |X-3| > .5) = 1 – P( |X-3| ≤ .5) = 1 – P( 2.5 ≤ X ≤ 3.5) 
 
 
 
 
 
= 1 - 
313
.
16
5
]
)
(
1[
5.
5.
2
4
3
≈
=
−
∫−
du
u
 
 
 
7. 
 
a. f(x) = 10
1  for 25 ≤ x ≤ 35 and = 0 otherwise 
 
b. P(X > 33) = 
2.
35
33 10
1
=
∫
dx
 
 
c. 
E(X) = 
30
20
35
25
2
35
25
10
1
=
⎥⎦
⎤
=
⋅
∫
x
dx
x
 
30 ± 2 is from 28 to 32 minutes:  
P(28 < X < 32) = 
]
4.
32
28
10
1
32
28 10
1
=
=
∫
x
dx
 
 
d. P( a ≤ x ≤ a+2) =
2.
2
10
1
=
∫
+
a
a
dx
, since the interval has length 2. 
 
 
131

Chapter 4:  Continuous Random Variables and Probability Distributions 
8. 
 
a.  
 
10
5
0
0.5
0.4
0.3
0.2
0.1
0.0
x
f(x)
b. 
dy
y
ydy
dy
y
f
)
(
)
(
10
5
25
1
5
2
5
0 25
1
∫
∫
∫
−
+
=
∞
∞
−
 = 
10
5
2
5
0
2
50
1
5
2
50
⎥⎦
⎤⎟
⎠
⎞
⎜
⎝
⎛
−
+
⎥⎦
⎤
y
y
y
 
 
= 
1
2
1
2
1
)
2
1
2
(
)
2
4
(
2
1
=
+
=
⎥⎦
⎤
⎢⎣
⎡
−
−
−
+
 
 
c. 
P(Y ≤ 3) = 
=
∫
ydy
3
0 25
1
18
.
50
9
50
5
0
2
≈
=
⎥⎦
⎤
y
 
 
d. P(Y ≤ 8) = 
92
.
25
23
)
(
8
5
25
1
5
2
5
0 25
1
≈
=
−
+
=
∫
∫
dy
y
ydy
 
 
e. 
P( 3 ≤ Y ≤ 8) = P(Y ≤ 8) - P(Y < 3) = 
74
.
50
37
50
9
50
46
=
=
−
 
 
f. 
P(Y < 2 or Y > 6) = 
4.
5
2
)
(
10
6
25
1
5
2
3
0 25
1
=
=
−
+
=
∫
∫
dy
y
ydy
 
 
 
9. 
 
a. P(X ≤ 6) = 
 (after u = x - .5) 
du
e
dx
e
u
x
∫
∫
−
−
−
=
=
5.5
0
15
.
6
5.
)
5
(
15
.
15
.
15
.
= 
 
]
562
.
1
825
.
5.5
0
15
.
≈
−
=
−
−
e
e
u
 
b. 1 - .562 = .438; .438 
 
c. 
P( 5 ≤ Y ≤ 6) = P(Y ≤ 6) - P(Y ≤ 5) ≈ .562 - .491 = .071 
 
132

Chapter 4:  Continuous Random Variables and Probability Distributions 
10. 
 
a.  
 
 
 
 
 
 
 
 
 
    θ 
b. 
1
1
)
,
;
(
1
=
=
⎥⎦
⎤⎟
⎠
⎞
⎜
⎝
⎛−
⋅
=
=
=
∞
∞
+
∞
∞
−
∫
∫
k
k
k
k
k
k
x
dx
x
k
dx
k
x
f
θ
θ
θ
θ
θ
θ
θ
 
 
c. 
P(X ≤ b) = 
k
b
k
k
b
k
k
b
x
dx
x
k
⎟
⎠
⎞
⎜
⎝
⎛
−
=
⎥⎦
⎤⎟
⎠
⎞
⎜
⎝
⎛−
⋅
=
∫
+
θ
θ
θ
θ
θ
1
1
1
 
 
d. P(a ≤ X ≤ b) = 
k
k
b
a
k
k
b
a
k
k
b
a
x
dx
x
k
⎟
⎠
⎞
⎜
⎝
⎛
−
⎟
⎠
⎞
⎜
⎝
⎛
=
⎥⎦
⎤⎟
⎠
⎞
⎜
⎝
⎛−
⋅
=
∫
+
θ
θ
θ
θ
1
1
 
 
Section 4.2 
 
11. 
 
a. P(X ≤ 1) = F(1) = 
25
.
4
1 =
 
 
b. P(.5 ≤ X ≤ 1) = F(1) – F(.5) = 
1875
.
16
3 =
 
 
c. 
P(X > .5) = 1 – P(X ≤ .5) = 1 – F(.5) = 
9375
.
16
15 =
 
 
d. .5 = 
414
.1
2
~
2
~
4
~
)
~
(
2
2
≈
=
⇒
=
⇒
=
µ
µ
µ
µ
F
 
 
e. 
f(x) = F′(x) = 2
x  for 0 ≤ x < 2, and = 0 otherwise 
f. 
E(X) = 
333
.1
6
8
6
2
1
2
1
)
(
2
0
3
2
0
2
2
0
≈
=
⎥⎦
⎤
=
=
⋅
=
⋅
∫
∫
∫
∞
∞
−
x
dx
x
xdx
x
dx
x
f
x
 
 
g. E(X2) = 
,2
8
2
1
2
1
)
(
2
0
4
2
0
3
2
0
2
2
=
⎥⎦
⎤
=
=
=
∫
∫
∫
∞
∞
−
x
dx
x
xdx
x
dx
x
f
x
  
So Var(X) = E(X2) – [E(X)]2 = 
( )
222
.
2
36
8
2
6
8
≈
=
−
, σx ≈ .471 
 
h. From g , E(X2) = 2 
 
 
133

Chapter 4:  Continuous Random Variables and Probability Distributions 
12. 
 
a. P(X < 0) = F(0) = .5 
 
b. P(-1 ≤ X ≤ 1) = F(1) – F(-1) = 
6875
.
16
11 =
 
 
c. P(X > .5) = 1 – P(X ≤ .5) = 1 – F(.5) = 1 - .6836 = .3164 
 
d. F(x) = F′(x) = 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
+
3
4
32
3
2
1
3
x
x
dx
d
= 
(
)
2
2
4
09375
.
3
3
4
32
3
0
x
x
−
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
+
 
 
e. 
( )
5.
~ =
µ
F
 by definition. F(0) = .5 from a above, which is as desired. 
 
 
13. 
 
a. 
3
3
1
)1
)(
3
(
0
1
3
3
1
1
1
1
4
=
⇒
=
⇒
−
−
=
⇒
−
−
=
⇒
=
∞
∞∫
k
k
k
k
dx
x
k
x
 
 
b. cdf:  F(x)= 
3
3
1
1
4
1
1
1
3
3
3
3
)
(
x
x
dy
y
dy
y
f
y
x
x
x
−
=
+
−
=
−
−
=
=
−
−
∞
−
∫
∫
.  So 
 
( )
1
1
,
1
,0
3
>
≤
⎩
⎨
⎧
−
=
−
x
x
x
x
F
 
c. 
P(x > 2) = 1 – F(2) = 1 –(
)
8
1
8
1
1
=
−
 or .125; 
(
) (
)
088
.
875
.
963
.
1
1
)
2
(
)3
(
)3
2
(
8
1
27
1
=
−
=
−
−
−
=
−
=
<
<
F
F
x
P
 
 
d. 
2
3
2
3
0
2
2
3
3
3
)
(
1
1
3
1
4
=
+
=
−
−
=
⎟
⎠
⎞
⎜
⎝
⎛
=
⎟
⎠
⎞
⎜
⎝
⎛
=
∫
∫
∞
∞
x
x
dx
x
dx
x
x
x
E
 
3
3
0
1
3
3
)
(
3
1
1
2
1
4
2
2
=
+
=
−
−
=
⎟
⎠
⎞
⎜
⎝
⎛
=
⎟
⎠
⎞
⎜
⎝
⎛
=
∫
∫
∞
∞
x
x
dx
x
dx
x
x
x
E
 
4
3
4
9
3
2
3
3
)]
(
[
)
(
)
(
2
2
2
=
−
=
⎟
⎠
⎞
⎜
⎝
⎛
−
=
−
=
x
E
x
E
x
V
 or .75  
866
.
)
(
4
3 =
=
=
x
V
σ
 
 
e. 
)
366
.2
(
)
366
.2
(
)
866
.
5.1
866
.
5.1(
F
x
P
x
P
=
<
=
+
<
<
−
9245
.
)
366
.2
(
1
3 =
−
=
−
 
 
 
 
134

Chapter 4:  Continuous Random Variables and Probability Distributions 
14. 
 
a. If X is uniformly distributed on the interval from A to B, then 
3
)
(
,
2
1
)
(
2
2
2
B
AB
A
X
E
B
A
dx
A
B
x
X
E
B
A
+
+
=
+
=
−
⋅
= ∫
 
V(X) = E(X2) – [E(X)]2 = (
)
2
2
A
B −
. 
With A = 7.5 and B = 20, E(X) = 13.75, V(X) = 13.02 
 
b. F(X) = 
⎪
⎩
⎪
⎨
⎧
−
1
5.
12
5.7
0
x
 
20
20
5.7
5.7
≥
<
≤
<
x
x
x
 
 
c. 
P(X ≤ 10) = F(10) = .200; P(10 ≤ X ≤ 15) = F(15) – F(10) = .4 
 
d. σ = 3.61, so µ ± σ = (10.14, 17.36)   
 
Thus, P(µ - σ ≤ X ≤ µ + σ) = F(17.36) – F(10.14) = .5776 
 
Similarly, P(µ - σ ≤ X ≤ µ + σ) = P(6.53 ≤ X ≤ 20.97) = 1 
 
15. 
 
a. F(X) = 0 for x ≤ 0, = 1 for x ≥ 1, and for 0 < X < 1,  
∫
∫
∫
−
=
−
=
=
∞
−
x
x
x
dy
y
y
dy
y
y
dy
y
f
X
F
0
9
8
0
8
)
(
90
)
1(
90
)
(
)
(
 
 
 
(
)]
10
9
0
10
10
1
9
9
1
9
10
90
x
x
y
y
x
−
=
−
  
 
0.0
0.5
1.0
0.0
0.5
1.0
x
F(x)
 
 
b. F(.5) = 10(.5)9 – 9(.5)10 ≈ .0107 
 
c. 
P(.25 ≤ X ≤ .5) = F(.5) – F(.25) ≈ .0107 – [10(.25)9 – 9(.25)10]  
 
 
 
 
 
≈ .0107 – .0000 ≈ .0107  
 
d. The 75th percentile is the value of x for which F(x) = .75  
⇒ .75 = 10(x)9 – 9(x)10  
⇒ x ≈ .9036 
 
 
135

Chapter 4:  Continuous Random Variables and Probability Distributions 
e. 
E(X) = 
 
∫
∫
∫
−
=
−
⋅
=
⋅
∞
∞
−
1
0
9
1
0
8
)
1(
90
)
1(
90
)
(
dx
x
x
dx
x
x
x
dx
x
f
x
]
8182
.
9
11
9
1
0
11
11
90
10
≈
=
−
=
x
x
 
 
E(X2) = 
 
∫
∫
∫
−
=
−
⋅
=
⋅
∞
∞
−
1
0
10
1
0
8
2
2
)
1(
90
)
1(
90
)
(
dx
x
x
dx
x
x
x
dx
x
f
x
 
 
]
6818
.
1
0
12
12
90
11
11
90
≈
−
=
x
x
 
 
 
V(X) ≈  .6818 – (.8182)2 = .0124, 
σx = .11134. 
 
f. 
µ ± σ = (.7068, .9295). Thus, P(µ - σ ≤ X ≤ µ + σ) = F(.9295) – F(.7068) 
 = .8465 - .1602 = .6863 
 
 
16. 
 
a. F(x) = 0 for x < 0 and F(x) = 1 for x > 2.  For 0 ≤ x ≤ 2,  
F(x) = 
]
3
8
1
0
3
8
1
0
2
8
3
x
y
dy
y
x
x
=
=
∫
 
2
1
0
1.0
0.5
0.0
x
F(x)
 
b. P(x ≤ .5) = F(.5) = ( )
64
1
3
2
1
8
1
=
 
 
c. 
P(.25 ≤ X ≤ .5) = F(.5) – F(.25)  
= 
( )
0137
.
512
7
3
4
1
8
1
64
1
≈
=
−
 
 
 
d. .75 = F(x) = 
3
8
1 x  ⇒  x3 = 6 ⇒ x ≈ 1.8171 
 
e. 
E(X) =  
(
)
(
)]
5.1
)
(
2
3
2
0
4
4
1
8
3
1
0
3
8
3
2
0
2
8
3
=
=
=
=
⋅
=
⋅
∫
∫
∫
∞
∞
−
x
dx
x
dx
x
x
dx
x
f
x
 
 
E(X2) =  
(
)
(
)]
4.2
5
5
12
2
0
5
1
8
3
1
0
4
8
3
2
0
2
8
3
=
=
=
=
⋅
∫
∫
x
dx
x
dx
x
x
 
 
V(X) = 
( )
15
.
20
3
2
2
3
5
12
=
=
−
 σx = .3873 
 
f. 
µ ± σ = (1.1127, 1.8873). Thus, P(µ - σ ≤ X ≤ µ + σ) = F(1.8873) – F(1.1127) = .8403 - 
.1722 = .6681 
 
 
 
136

Chapter 4:  Continuous Random Variables and Probability Distributions 
17. 
 
a. For 2 ≤ X ≤ 4, 
∫
∫
−
−
=
=
∞
−
x
x
dy
y
dy
y
f
X
F
2
2
4
3
]
)
3
(
1[
)
(
)
(
(let u = y-3) 
⎥⎦
⎤
⎢⎣
⎡
−
−
−
=
⎥⎦
⎤
⎢⎣
⎡−
=
−
=
−
−
−
−∫
3
)
3
(
3
7
4
3
3
4
3
]
1[
3
3
1
3
3
1
2
4
3
x
x
u
u
du
u
x
x
. Thus 
F(x) = 
⎪⎩
⎪⎨
⎧
−
−
−
1
]
)
3
(
7
3
[
0
3
4
1
x
x
 
4
4
2
2
>
≤
≤
<
x
x
x
 
 
b. By symmetry of f(x), 
3
~ =
µ
 
 
c. 
E(X) =  
∫
∫
−
−
+
=
−
−
⋅
1
1
2
4
3
4
2
2
4
3
)
1
)(
3
(
]
)
3
(
1[
dx
y
y
dx
x
x
 
 
 
 
3
4
4
3
4
2
3
4
3
1
1
4
3
2
=
⋅
=
⎥⎦
⎤
⎢⎣
⎡
−
−
+
=
−
y
y
y
y
 
 
V(X) = 
(
)
∫
∫
−
−
⋅
−
=
−
∞
∞
−
4
2
2
2
2
]
)
3
(
1[
3
4
3
)
(
)
(
dx
x
x
dx
x
f
x
µ
 
 
   =
2.
5
1
15
4
4
3
)
1(
4
3
1
1
2
2
=
=
⋅
=
−
∫−
dy
y
y
 
 
 
18. 
 
a. F(X) = 
A
B
A
x
−
−
 = p  ⇒ 
x = (100p)th percentile = A + (B - A)p 
 
b. 
(
)
2
1
2
1
2
1
1
)
(
2
2
2
B
A
A
B
A
B
x
A
B
dx
A
B
x
X
E
B
A
B
A
+
=
−
⋅
−
⋅
=
⎥⎦
⎤
⋅
−
=
−
⋅
= ∫
 
(
)
3
1
3
1
)
(
2
2
3
3
2
B
AB
A
A
B
A
B
X
E
+
+
=
−
⋅
−
⋅
=
  
 
(
)
(
) ,
12
2
3
)
(
2
2
2
2
A
B
B
A
B
AB
A
X
V
−
=
⎟
⎠
⎞
⎜
⎝
⎛
+
−
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
+
=
 
12
)
(
A
B
x
−
=
σ
 
 
c. 
)
)(
1
(
1
)
(
1
1
A
B
n
A
B
dx
A
B
x
X
E
n
n
B
A
n
n
−
+
−
=
−
⋅
=
+
+
∫
 
 
 
137

Chapter 4:  Continuous Random Variables and Probability Distributions 
19. 
 
a. P(X ≤ 1) = F(1) = .25[1 + ln(4)] ≈ .597 
 
b. P(1 ≤ X ≤ 3) = F(3) – F(1) ≈ .966 - .597 ≈ .369 
 
c. 
f(x) = F′(x) = .25 ln(4) - .25 ln(x) for o < x < 4 
 
 
20. 
 
a. For 0 ≤ y ≤ 5, F(y) = 
50
25
1
2
0
y
udu
y
=
∫
 
For  5 ≤ y ≤ 10, F(y) = 
 
∫
∫
∫
+
=
y
y
du
u
f
du
u
f
du
u
f
5
5
0
0
)
(
)
(
)
(
 
 
1
50
5
2
25
5
2
2
1
2
0
−
−
=
⎟
⎠
⎞
⎜
⎝
⎛
−
+
=
∫
y
y
du
u
y
 
10
5
0
1.0
0.5
0.0
x1
F(x1)
 
b. For 0 < p ≤ .5, p = F(yp) = 
(
)
2
/
1
2
50
50
p
y
y
p
p
=
⇒
 
 
For  .5 < p ≤ 1, p = 
)
1(
2
5
10
1
50
5
2
2
p
y
y
y
p
p
p
−
−
=
⇒
−
−
 
 
c. 
E(Y) = 5 by straightforward integration (or by symmetry of f(y)), and similarly V(Y)= 
1667
.4
12
50 =
.  For the waiting time X for a single bus,  
 
E(X) = 2.5 and V(X) = 12
25  
 
 
21. 
E(area) = E(πR2) = 
(
)
∫
∫
−
−
⎟
⎠
⎞
⎜
⎝
⎛
=
∞
∞
−
11
9
2
2
2
)
10
(
1
4
3
)
(
dr
r
r
dr
r
f
r
π
π
 
(
)
π
π
π
2
100
20
99
4
3
)
20
100
(
1
4
3
11
9
4
3
2
11
9
2
2
⋅
=
−
+
−
=
+
−
−
⎟
⎠
⎞
⎜
⎝
⎛
=
∫
∫
dr
r
r
r
dr
r
r
r
 
 
138

Chapter 4:  Continuous Random Variables and Probability Distributions 
22. 
 
a. For 1 ≤ x ≤ 2,  F(x) = 
,4
1
2
1
2
1
1
2
1
1
2
−
⎟
⎠
⎞
⎜
⎝
⎛
+
=
⎥
⎦
⎤
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛−
∫
x
x
y
y
dy
y
x
x
 so 
F(x) = 
(
)
⎪⎩
⎪⎨
⎧
−
+
1
4
2
0
1
x
x
 
2
2
1
1
>
≤
≤
<
x
x
x
 
 
b. 
p
x
x
p
p
=
−
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
4
1
2
⇒ 2xp
2 – (4 – p)xp + 2 = 0 ⇒ xp = 
]
8
4
[
2
4
1
p
p
p
+
+
+
  To 
find µ~ , set p = .5 ⇒ µ~  = 1.64 
 
c. 
E(X) = 
614
.1
)
ln(
2
2
1
2
1
1
2
2
1
2
2
1
2
1
2
=
⎥
⎦
⎤
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
=
⎟
⎠
⎞
⎜
⎝
⎛
−
=
⎟
⎠
⎞
⎜
⎝
⎛−
⋅
∫
∫
x
x
dx
x
x
dx
x
x
 
E(X2) = 
(
)
⇒
=
⎥
⎦
⎤
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
=
−
∫
3
8
3
2
1
2
2
1
3
2
1
2
x
x
dx
x
  Var(X) = .0626 
 
d. Amount left = max(1.5 – X, 0), so 
 
E(amount left) = 
061
.
1
1
)
5.1(
2
)
(
)
0,
5.1
max(
5.1
1
2
2
1
=
⎟
⎠
⎞
⎜
⎝
⎛−
−
=
−
∫
∫
dx
x
x
dx
x
f
x
 
 
 
23. 
With X = temperature in °C, temperature in °F = 
,
32
5
9
+
X
  so 
,
248
32
)
120
(
5
9
32
5
9
=
+
=
⎥⎦
⎤
⎢⎣
⎡
+
X
E
 
96
.
12
)
2
(
5
9
32
5
9
2
2
=
⋅
⎟
⎠
⎞
⎜
⎝
⎛
=
⎥⎦
⎤
⎢⎣
⎡
+
X
Var
, 
so σ = 3.6 
 
139

Chapter 4:  Continuous Random Variables and Probability Distributions 
24. 
 
a. E(X) = 
1
1
1
1
1
−
=
⎥⎦
⎤
+
−
=
=
⋅
∞
+
−
∞
∞
+
∫
∫
k
k
k
x
k
dx
x
k
dx
x
k
x
k
k
k
k
k
k
θ
θ
θ
θ
θ
θ
θ
 
 
b. E(X) = ∞ 
 
c. 
E(X2) = 
2
1
2
1
−
=
∫
∞
−
k
k
dx
x
k
k
k
θ
θ
θ
, so  
Var(X) = 
(
)(
)
2
2
2
2
1
2
1
2
−
−
=
⎟
⎠
⎞
⎜
⎝
⎛
−
−
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
k
k
k
k
k
k
k
θ
θ
θ
 
 
d. Var(x) = ∞, since E(X2) = ∞. 
 
e. 
E(Xn) = 
, which will be finite if n – (k+1) < -1, i.e. if n<k. 
∫
∞
+
−
θ
θ
dx
x
k
k
n
k
)
1
(
 
 
25. 
 
a. P(Y ≤ 1.8 µ~  + 32) = P(1.8X + 32 ≤ 1.8 µ~  + 32) = P( X ≤ µ~ ) = .5 
 
b. 90th for Y = 1.8η(.9) + 32 where η(.9) is the 90th percentile for X, since  
P(Y ≤ 1.8η(.9) + 32) = P(1.8X + 32 ≤ 1.8η(.9) + 32) 
 
= (X ≤ η(.9) ) = .9 as desired. 
 
c. 
The (100p)th percentile for Y is 1.8η(p) + 32, verified by substituting p for .9 in the 
argument of b.  When Y = aX + b, (i.e. a linear transformation of X), and the (100p)th 
percentile of the X distribution is η(p), then the corresponding (100p)th percentile of the 
Y distribution is a⋅η(p) + b. (same linear transformation applied to X’s percentile) 
 
 
 
140

Chapter 4:  Continuous Random Variables and Probability Distributions 
Section 4.3 
 
26. 
 
a. P(0 ≤ Z ≤ 2.17) = Φ(2.17) -   Φ(0) = .4850 
 
b. Φ(1) -   Φ(0) = .3413 
 
c. 
Φ(0) -   Φ(-2.50) = .4938 
 
d. Φ(2.50) -   Φ(-2.50) = .9876 
 
e. 
Φ(1.37) = .9147 
 
f. 
P( -1.75 < Z) + [1 – P(Z < -1.75)] = 1 - Φ(-1.75) = .9599 
 
g. Φ(2) - Φ(-1.50) = .9104 
 
h. Φ(2.50) -   Φ(1.37) = .0791 
 
i. 
1 - Φ(1.50)  = .0668 
 
j. 
P( |Z| ≤ 2.50 ) = P( -2.50 ≤ Z ≤ 2.50) = Φ(2.50) -   Φ(-2.50) = .9876 
 
 
27. 
 
a. .9838 is found in the 2.1 row and the .04 column of the standard normal table so c = 2.14. 
 
b. P(0 ≤ Z ≤ c) = .291 ⇒ Φ(c) = .7910 ⇒ c = .81 
 
c. 
P(c ≤ Z) = .121 ⇒  1 - P(c ≤ Z) = P(Z < c) = Φ(c) = 1 - .121 = .8790 ⇒    c = 1.17 
 
d. P(-c ≤ Z ≤ c) = Φ(c) - Φ(-c) = Φ(c) – (1 - Φ(c)) = 2Φ(c) – 1  
⇒ Φ(c) = .9920 ⇒ c = .97 
 
e. 
P( c ≤ | Z | ) = .016  ⇒  1 - .016 = .9840 = 1 – P(c ≤ | Z | ) = P( | Z |  < c ) 
 
= P(-c < Z < c) = Φ(c) - Φ(-c) = 2Φ(c) – 1 
 
⇒ Φ(c) = .9920   ⇒  c = 2.41 
 
 
 
141

Chapter 4:  Continuous Random Variables and Probability Distributions 
28. 
 
a. Φ(c) = .9100  ⇒  c ≈ 1.34  (.9099 is the entry in the 1.3 row, .04 column) 
 
b. 9th percentile = -91st percentile = -1.34 
 
c. 
Φ(c) = .7500  ⇒  c ≈ .675 since .7486 and .7517 are in the .67 and .68 entries, 
respectively. 
 
d. 25th = -75th = -.675 
 
e. 
Φ(c) = .06  ⇒  c ≈ .-1.555  (both .0594 and .0606 appear as the –1.56 and –1.55 entries, 
respectively). 
 
 
29. 
 
a. Area under  Z curve above z.0055 is .0055, which implies that  
Φ( z.0055) = 1 - .0055 = .9945, so z.0055  = 2.54 
 
b. Φ( z.09) = .9100  ⇒  z = 1.34 (since .9099 appears as the 1.34 entry). 
 
c. 
Φ( z.633) = area below z.633 = .3370 ⇒ z.633 ≈ -.42 
 
 
30. 
 
a. P(X ≤ 100) = 
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
10
80
100
z
P
= P(Z ≤ 2) = Φ(2.00) = .9772 
 
b. P(X ≤ 80) = 
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
10
80
80
z
P
= P(Z ≤ 0) = Φ(0.00) = .5 
 
c. 
P(65 ≤ X ≤ 100) = 
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
≤
−
10
80
100
10
80
65
z
P
= P(-1.50 ≤ Z ≤ 2)  
= Φ(2.00) - Φ(-1.50) = .9772 - .0668 = .9104 
 
d. P(70 ≤ X) = P(-1.00 ≤ Z) = 1 - Φ(-1.00) = .8413 
 
e. 
P(85 ≤ X ≤ 95) = P(.50 ≤ Z ≤ 1.50) = Φ(1.50) - Φ(.50) = .2417 
 
f. 
P( |X – 80 | ≤ 10) = P(-10 ≤ X - 80 ≤ 10) = P(70 ≤ X ≤ 90) 
 
 
 
 
P(-1.00 ≤ Z ≤ 1.00) = .6826 
 
 
142

Chapter 4:  Continuous Random Variables and Probability Distributions 
31. 
 
a. P( X  ≤ 18) = 
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
25
.1
15
18
z
P
= P(Z  ≤ 2.4) = Φ(2.4) = .9452 
 
b. P(10 ≤ X ≤ 12) = P(-4.00 ≤ Z ≤ -2.40) ≈ P(Z ≤ -2.40) =  Φ(-2.40) = .0082 
 
c. 
P( |X – 10| ≤ 2(1.25) ) = P(-2.50 ≤ X-15 ≤ 2.50) = P(12.5 ≤ X ≤ 17.5) 
P(-2.00 ≤ Z ≤ 2.00) = .9544 
 
 
32. 
 
a. P(X > .25) = P(Z > -.83) = 1 - .2033 = .7967 
 
b. P(X ≤ .10) = Φ(-3.33) = .0004 
 
c. 
We want the value of the distribution, c, that is the 95th percentile (5% of the values are 
higher).  The 95th percentile of the standard normal distribution = 1.645.  So c = .30 + 
(1.645)(.06) = .3987.  The largest 5% of all concentration values are above .3987 mg/cm3. 
 
 
33. 
 
a. P(X ≥ 10) = P(Z ≥ .43) = 1 - Φ(.43) = 1 - .6664 = .3336. 
P(X > 10) = P(X ≥ 10) = .3336, since for any continuous distribution, P(x = a) = 0. 
 
b. P(X > 20) = P(Z > 4) ≈ 0 
 
c. 
P(5 ≤ X ≤ 10) = P(-1.36 ≤ Z ≤ .43) = Φ(.43) - Φ(-1.36) = .6664 - .0869 = .5795 
 
d. P(8.8 – c ≤ X ≤ 8.8 + c) = .98, so 8.8 – c and 8.8 + c are at the 1st and the 99th percentile 
of the given distribution, respectively.  The 1st percentile of the standard normal 
distribution has the value –2.33, so  
8.8 – c = µ + (-2.33)σ = 8.8 – 2.33(2.8) ⇒ c = 2.33(2.8) = 6.524. 
 
e. 
From a, P(x > 10) = .3336.  Define event A as {diameter > 10}, then P(at least one Ai) = 
1 – P(no Ai) =
  
8028
.
1972
.
1
)
3336
.
1(
1
)
(
1
4
4
=
−
=
−
−
=
′
−
A
P
 
 
34. 
Let X denote the diameter of a randomly selected cork made by the first machine, and let Y be 
defined analogously for the second machine. 
P(2.9 ≤ X ≤ 3.1) = P(-1.00 ≤ Z ≤ 1.00) = .6826 
P(2.9 ≤ Y ≤ 3.1) = P(-7.00 ≤ Z ≤ 3.00) = .9987 
So the second machine wins handily. 
 
 
 
143

Chapter 4:  Continuous Random Variables and Probability Distributions 
35. 
 
a. µ + σ⋅(91st  percentile from std normal) = 30 + 5(1.34) = 36.7 
 
b. 30 + 5( -1.555) = 22.225 
 
c. 
µ = 3.000 µm; σ = 0.140.  We desire the 90th percentile: 30 + 1.28(0.14) = 3.179 
 
 
36. 
µ = 43; σ = 4.5 
a. P(X < 40) = 
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
5.4
43
40
z
P
= P(Z < -0.667) = .2514 
P(X > 60) = 
⎟
⎠
⎞
⎜
⎝
⎛
−
>
5.4
43
60
z
P
= P(Z > 3.778) ≈ 0 
 
b. 43 + (-0.67)(4.5) = 39.985 
 
 
37. 
P(damage) = P(X < 100) = 
⎟
⎠
⎞
⎜
⎝
⎛
−
<
300
200
100
z
P
= P(Z < -3.33) = .0004 
P(at least one among five is damaged)  
= 1 – P(none damaged)  
= 1 – (.9996)5 = 1 - .998 = .002 
 
 
38. 
From Table A.3, P(-1.96 ≤ Z ≤ 1.96) = .95.  Then P(µ - .1 ≤ X ≤ µ + .1) = 
⎟
⎠
⎞
⎜
⎝
⎛
<
<
−
σ
σ
1.
1.
z
P
implies that σ
1.
= 1.96, and thus that 
0510
.
96
.1
1.
=
=
σ
 
 
 
39. 
Since 1.28 is the 90th z percentile (z.1 = 1.28) and –1.645 is the 5th z percentile (z.05 = 1.645), 
the given information implies that µ + σ(1.28) = 10.256 and  µ + σ(-1.645) = 9.671, from 
which σ(-2.925) = -.585, σ = .2000, and µ = 10. 
 
40. 
 
a. P(µ - 1.5σ  ≤ X ≤ µ + 1.5σ) = P(-1.5 ≤ Z ≤ 1.5) = Φ(1.50) - Φ(-1.50) = .8664 
 
b. P( X < µ - 2.5σ or X > µ + 2.5σ) = 1 - P(µ - 2.5σ  ≤ X ≤ µ + 2.5σ) 
= 1 - P(-2.5 ≤ Z ≤ 2.5) = 1 - .9876 = .0124 
 
c. 
P(µ - 2σ  ≤ X ≤ µ - σ or µ + σ  ≤ X ≤ µ + 2σ) = P(within 2 sd’s) – P(within 1 sd) = P(µ - 
2σ  ≤ X ≤ µ + 2σ) - P(µ - σ  ≤ X ≤ µ + σ)  
= .9544 - .6826 = .2718 
 
 
 
144

Chapter 4:  Continuous Random Variables and Probability Distributions 
41. 
With µ = .500 inches, the acceptable range for the diameter is between .496 and .504 inches, 
so unacceptable bearings will have diameters smaller than .496 or larger than .504.  The new 
distribution has µ = .499 and σ =.002. P(x < .496 or x >.504) = 
(
)
(
5.2
5.1
002
.
499
.
504
.
002
.
499
.
496
.
>
+
−
<
=
⎟
⎠
⎞
⎜
⎝
⎛
−
>
+
⎟
⎠
⎞
⎜
⎝
⎛
−
<
z
P
z
P
z
P
z
P
(
)
(
)
073
.
0062
.
0068
.
5.2
1
)
5.1
(
)
=
+
=
Φ
−
+
−
Φ
, or 7.3% of the bearings will be 
unacceptable. 
 
 
42. 
 
a. P(67 ≤ X ≤ 75) = P(-1.00 ≤ Z ≤ 1.67) = .7938 
 
b. P(70 – c  ≤ X ≤ 70 + c) = 
9750
.
)
3
(
95
.
1
)
3
(
2
3
3
=
Φ
⇒
=
−
Φ
=
⎟
⎠
⎞
⎜
⎝
⎛
≤
≤
−
c
c
c
Z
c
P
 
88
.5
96
.1
3
=
⇒
=
c
c
 
 
c. 
10⋅P(a single one is acceptable) = 9.05 
 
d. p = P(X < 73.84) = P(Z < 1.28) = .9, so P(Y ≤ 8) = B(8;10,.9) = .264 
 
 
43. 
The stated condition implies that 99% of the area under the normal curve with µ = 10 and σ = 
2 is to the left of  c – 1, so c – 1 is the 99th percentile of the distribution.  Thus c – 1 = µ + 
σ(2.33) = 20.155, and c = 21.155. 
 
 
44. 
 
a. By symmetry, P(-1.72 ≤ Z ≤ -.55) = P(.55 ≤ Z ≤ 1.72) = Φ(1.72) - Φ(.55)  
 
b. P(-1.72 ≤ Z ≤ .55) = Φ(.55) - Φ(-1.72) = Φ(.55) – [1 - Φ(1.72)] 
No, symmetry of the Z curve about 0. 
 
 
45. 
X ∼N(3432, 482) 
a. 
(
)
(
)
18
.1
482
3432
4000
4000
>
=
⎟
⎠
⎞
⎜
⎝
⎛
−
>
=
>
z
P
Z
P
x
P
 
1190
.
8810
.
1
)
18
.1(
1
=
−
=
Φ
−
=
(
)
⎟
⎠
⎞
⎜
⎝
⎛
−
<
<
−
=
<
<
482
3432
4000
482
3432
3000
4000
3000
Z
P
x
P
(
)
(
)
6969
.
1841
.
8810
.
90
.
18
.1
=
−
=
−
Φ
−
Φ
=
 
 
b. 
(
)
⎟
⎠
⎞
⎜
⎝
⎛
−
>
+
⎟
⎠
⎞
⎜
⎝
⎛
−
<
=
>
<
482
3432
5000
482
3432
2000
5000
2000
Z
P
Z
P
orx
x
P
 
(
)
(
)
[
]
0021
.
0006
.
0015
.
25
.3
1
97
.2
=
+
=
Φ
−
+
−
Φ
=
 
 
145

Chapter 4:  Continuous Random Variables and Probability Distributions 
 
c. 
We will use the conversion 1 lb = 454 g, then 7 lbs = 3178 grams, and we wish to find 
(
)
7019
.
)
53
.
(
1
482
3432
3178
3178
=
−
Φ
−
=
⎟
⎠
⎞
⎜
⎝
⎛
−
>
=
>
Z
P
x
P
 
 
d. We need the top .0005 and the bottom .0005 of the distribution.  Using the Z table, both 
.9995 and .0005 have multiple z values, so we will use a middle value, ±3.295.   Then 
3432±(482)3.295 = 1844 and 5020, or the most extreme .1%  of all birth weights are less 
than 1844 g and more than 5020 g. 
 
e. 
Converting to lbs yields mean 7.5595 and s.d. 1.0608.  Then  
(
)
7019
.
)
53
.
(
1
0608
.1
5595
.7
7
7
=
−
Φ
−
=
⎟
⎠
⎞
⎜
⎝
⎛
−
>
=
>
Z
P
x
P
  This yields the same 
answer as in part c. 
 
 
46. 
We use a Normal approximation to the Binomial distribution:  X ∼ b(x;1000,.03) ≈ 
N(30,5.394)  
a. 
(
)
(
)
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
−
=
≤
−
=
≥
394
.5
30
5.
39
1
39
1
40
Z
P
x
P
x
P
 
0392
.
9608
.
1
)
76
.1(
1
=
−
=
Φ
−
=
 
 
b. 5% of 1000 = 50:  (
)
00
.1
)
80
.3
(
394
.5
30
5.
50
50
≈
Φ
=
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
=
≤
Z
P
x
P
 
 
 
47. 
P( |X - µ | ≥ σ ) = P( X ≤ µ - σ  or X ≥ µ + σ )  
= 1 – P(µ - σ ≤ X ≤ µ + σ) = 1 – P(-1 ≤ Z ≤ 1) = .3174 
 
Similarly, P( |X - µ | ≥ 2σ ) = 1 – P(-2 ≤ Z ≤ 2) = .0456 
 
 
And P( |X - µ | ≥ 3σ ) = 1 – P(-3 ≤ Z ≤ 3) = .0026 
 
 
48. 
 
a. P(20 - .5 ≤ X ≤ 30 + .5) = P(19.5 ≤ X ≤ 30.5) = P(-1.1 ≤ Z ≤ 1.1) = .7286 
 
b. P(at most 30) = P(X ≤ 30 + .5) = P(Z ≤ 1.1) = .8643. 
P(less than 30) = P(X < 30 - .5) = P(Z < .9) = .8159 
 
 
 
146

Chapter 4:  Continuous Random Variables and Probability Distributions 
49. 
P: 
.5 
.6 
.8 
µ: 
12.5 
15 
20 
σ: 
2.50 
2.45 
2.00 
a.  
P(15≤ X ≤20) 
P(14.5 ≤ normal ≤ 20.5) 
.5 
.212 
P(.80 ≤ Z ≤ 3.20)  = .2112  
.6 
.577 
P(-.20 ≤ Z ≤ 2.24)  = .5668 
.8 
.573 
P(-2.75 ≤ Z ≤ .25)  = .5957 
 
b.  
 
 
P(X ≤15) 
P(normal ≤ 15.5) 
.885 
P(Z ≤ 1.20)  = .8849 
.575 
P(Z ≤ .20)  = .5793 
.017 
P( Z ≤ -2.25)  = .0122 
 
 
 
 
 
 
 
c. 
 
 
P(20 ≤X) 
P(19.5 ≤ normal) 
.002 
.0026 
.029 
.0329 
.617 
.5987 
 
 
 
 
 
 
 
  
50. 
 P = .10; n = 200; np = 20, npq = 18 
a. P(X ≤ 30) = 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
+
Φ
18
20
5.
30
= Φ(2.47) = .9932 
 
b. P(X < 30) =P(X ≤ 29) = 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
+
Φ
18
20
5.
29
= Φ(2.24) = .9875 
 
c. 
P(15 ≤ X ≤ 25) = P(X ≤ 25) – P(X ≤ 14) = 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
+
Φ
−
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
+
Φ
18
20
5.
14
18
20
5.
25
 
Φ(1.30) - Φ(-1.30) = .9032 - .0968 = .8064 
 
 
51. 
 N = 500, p = .4, µ = 200,  σ   = 10.9545 
a. P(180 ≤ X ≤ 230) = P(179.5 ≤ normal ≤ 230.5) = P(-1.87 ≤ Z ≤ 2.78) = .9666 
 
b. P(X < 175) = P(X ≤ 174) = P(normal ≤ 174.5) = P(Z ≤ -2.33) = .0099 
 
 
147

Chapter 4:  Continuous Random Variables and Probability Distributions 
 
52. 
P(X ≤ µ + σ[(100p)th  percentile for std normal]) 
[ ]⎟
⎠
⎞
⎜
⎝
⎛
≤
−
...
σ
µ
X
P
= P(Z ≤ […]) = p as desired 
 
 
53. 
 
a. Fy(y) = P(Y ≤ y) = P(aX + b ≤ y) = 
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
a
b
y
X
P
)
(
 (for a > 0). 
Now differentiate with respect to y to obtain 
fy(y) = 
2
2
2
)]
(
[
2
1
2
1
)
(
b
a
y
a
y
e
a
y
F
+
−
−
=
′
µ
σ
σ
π
so Y is normal with mean aµ + b 
and variance a2σ2. 
 
b. Normal, mean 
239
32
)
115
(
5
9
=
+
, variance = 12.96 
 
 
54. 
 
a. P(Z ≥ 1) ≈ 
1587
.
165
703
562
351
83
exp
5.
=
⎟
⎠
⎞
⎜
⎝
⎛
+
+
+
⋅
 
 
b. P(Z > 3) ≈ 
0013
.
3333
.
399
2362
exp
5.
=
⎟
⎠
⎞
⎜
⎝
⎛−
⋅
 
 
c. 
P(Z > 4) ≈ 
0000317
.
75
.
340
3294
exp
5.
=
⎟
⎠
⎞
⎜
⎝
⎛−
⋅
, so 
P(-4 < Z < 4) ≈ 1 – 2(.0000317) = .999937 
 
d. P(Z > 5) ≈ 
00000029
.
6.
305
4392
exp
5.
=
⎟
⎠
⎞
⎜
⎝
⎛−
⋅
 
 
 
 
148

Chapter 4:  Continuous Random Variables and Probability Distributions 
Section 4.4 
 
55. 
 
a. Γ(6) = 5! = 120 
 
b. 
329
.1
4
3
2
1
2
1
2
3
2
1
2
3
2
5
≈
⎟
⎠
⎞
⎜
⎝
⎛
=
⎟
⎠
⎞
⎜
⎝
⎛
Γ
⋅
⋅
=
⎟
⎠
⎞
⎜
⎝
⎛
Γ
=
⎟
⎠
⎞
⎜
⎝
⎛
Γ
π
 
 
c. 
F(4;5) = .371 from row 4, column 5 of Table A.4 
 
d. F(5;4) = .735 
 
e. 
F(0;4) = P(X ≤ 0; α= 4) = 0 
 
 
56. 
 
a. P(X ≤ 5) = F(5;7) = .238 
 
b. P(X < 5) = P(X ≤ 5) = .238 
 
c. 
P(X > 8) = 1 – P(X < 8) = 1 – F(8;7) = .313 
 
d. P( 3 ≤ X ≤ 8 ) = F(8;7) – F(3;7) = .653 
 
e. 
P( 3 < X < 8 ) =.653 
 
f. 
P(X < 4 or X > 6) = 1 – P(4 ≤ X ≤ 6 ) = 1 – [F(6;7) – F(4;7)] = .713 
 
 
57. 
 
a. µ = 20,  σ2 = 80  ⇒ αβ = 20, αβ2 = 80  ⇒ β = 20
80 , α = 5 
 
b. P(X ≤ 24) = 
⎟
⎠
⎞
⎜
⎝
⎛
5;
4
24
F
= F(6;5) = .715 
 
c. 
P(20 ≤ X ≤ 40 ) = F(10;5) – F(5;5) = .411 
 
 
58. 
µ = 24,  σ2 = 144  ⇒ αβ = 24, αβ2 = 144 ⇒ β = 6, α = 4 
 
a. P(12 ≤ X ≤ 24 ) = F(4;4) – F(2;4) = .424 
 
b. P(X ≤ 24 ) = F(4;4) = .567, so while the mean is 24, the median is less than 24. (P(X ≤ 
µ~ ) = .5); This is a result of the positive skew of the gamma distribution. 
 
 
 
149

Chapter 4:  Continuous Random Variables and Probability Distributions 
c. 
We want a value of X for which F(X;4)=.99. In table A.4, we see F(10;4)=.990.  So with 
β = 6, the 99th percentile = 6(10)=60. 
d. We want a value of X for which  F(X;4)=.995.  In the table, F(11;4)=.995, so t = 
6(11)=66.  At 66 weeks, only .5% of all transistors would still be operating. 
 
 
59. 
 
a. E(X) = 
1
1 =
λ
 
 
b. 
1
1 =
= λ
σ
 
 
c. 
P(X ≤ 4 ) = 
 
982
.
1
1
4
)
4
)(
1(
=
−
=
−
−
−
e
e
 
d. P(2 ≤ X ≤ 5) = 
[
]
129
.
1
1
5
2
)
2
)(
1(
)
5
)(
1(
=
−
=
−
−
−
−
−
−
−
e
e
e
e
 
 
 
60. 
 
a. P(X ≤ 100 ) = 
 
7499
.
1
1
386
.1
)
01386
)(.
100
(
=
−
=
−
−
−
e
e
P(X ≤ 200 ) = 
 
9375
.
1
1
772
.2
)
01386
)(.
200
(
=
−
=
−
−
−
e
e
P(100 ≤ X ≤ 200) = P(X ≤ 200 ) - P(X ≤ 100 ) = .9375 - .7499 = .1876 
 
b. µ = 
15
.
72
01386
.
1
=
, σ = 72.15 
P(X > µ + 2σ) = P(X > 72.15 + 2(72.15)) = P(X > 216.45) = 
[
]
0498
.
1
1
9999
.2
)
01386
)(.
45
.
216
(
=
=
−
−
−
−
e
e
 
 
c. 
.5 = P(X ≤ µ~ ) ⇒ 
5.
5.
1
)
01386
)(.
~
(
)
01386
)(.
~
(
=
⇒
=
−
−
−
µ
µ
e
e
 
 
50
~
693
.
)
5
ln(.
)
01386
(.
~
=
⇒
=
=
−
µ
µ
 
 
 
61. 
Mean = 
000
,
25
1 =
λ
 implies λ = .00004 
a. P(X > 20,000) = 1 – P(X ≤ 20,000) = 1 – F(20,000; .00004) 
 
449
.
)
000
,
20
)(
00004
(.
=
=
−
e
P(X ≤ 30,000) = F(30,000; .00004) 
 
699
.
2.1
=
=
−
e
P(20,000 ≤ X ≤ 30,000) =  .699 - .551 = .148 
 
b. 
000
,
25
1 =
= λ
σ
, so P(X > µ + 2σ) = P( x > 75,000) =  
1 – F(75,000;.00004) = .05. 
Similarly, P(X > µ + 3σ) = P( x > 100,000) = .018 
 
 
 
150

Chapter 4:  Continuous Random Variables and Probability Distributions 
62. 
 
a. E(X) = αβ = 
;
1
λ
λ
n
n
=
 for λ = .5, n = 10, E(X) = 20 
 
b. P(X ≤ 30) = 
⎟
⎠
⎞
⎜
⎝
⎛
10
;
2
30
F
= F(15;10) = .930 
 
c. 
P(X ≤ t) = P(at least n events in time t) = P( Y ≥ n) when Y ∼ Poisson with parameter λt .  
Thus P(X ≤ t) = 1 – P( Y < n) = 1 – P( Y ≤ n – 1) 
(
) .
!
1
1
0∑
−
=
−
−
=
n
k
k
t
k
t
e
λ
λ
 
 
 
63. 
 
a. {X ≥ t} = A1 ∩ A2 ∩ A3 ∩ A4 ∩ A5  
 
b. P(X ≥ t) =P( A1 ) ⋅ P( A2 ) ⋅ P( A3 ) ⋅ P( A4 ) ⋅ P( A5 ) = (
)
t
t
e
e
05
.
5
−
−
=
λ
, so Fx(t) = P(X ≤ 
t) = 1 - 
, f
t
e
05
.−
x(t) = 
 for t ≥ 0.  Thus X also ha an exponential distribution , but 
with parameter λ = .05. 
t
e
05
.
05
.
−
 
c. 
By the same reasoning, P(X ≤ t) = 1 - 
, so X has an exponential distribution with 
parameter nλ. 
t
n
e
λ
−
 
 
64. 
With xp = (100p)th  percentile, p = F(xp) = 1 - 
, 
p
e
e
p
p
x
x
−
=
⇒
−
−
1
λ
λ
[
]
λ
λ
)
1
ln(
)
1
ln(
p
x
p
x
p
p
−
−
=
⇒
−
=
−
⇒
.  For p = .5, x.5 = 
λ
µ
693
.
~ =
. 
 
 
65. 
 
a. {X2 ≤ y} = {
}
y
X
y
≤
≤
−
 
 
b. P(X2 ≤ y) = ∫−
−
y
y
z
dz
e
2
/
2
2
1
π
.  Now differentiate with respect to y to obtain the chi-
squared p.d.f. with ν = 1. 
 
 
 
151

Chapter 4:  Continuous Random Variables and Probability Distributions 
Section 4.5 
 
66. 
 
a. E(X) = 
66
.2
2
1
2
1
3
2
1
1
3
=
⎟
⎠
⎞
⎜
⎝
⎛
Γ
⋅
⋅
=
⎟
⎠
⎞
⎜
⎝
⎛+
Γ
,  
Var(X) = 
(
)
926
.1
2
1
1
1
1
9
2
=
⎥⎦
⎤
⎢⎣
⎡
⎟
⎠
⎞
⎜
⎝
⎛+
Γ
−
+
Γ
 
 
b. P(X ≤  6) = 
 
982
.
1
1
1
4
)
3
/
6
(
)
/
6
(
2
=
−
=
−
=
−
−
−
−
e
e
e
α
β
 
c. 
P(1.5 ≤  X ≤ 6) = 
[
]
760
.
1
1
4
25
.
)
3
/
5.1
(
)
3
/
6
(
2
2
=
−
=
−
−
−
−
−
−
−
e
e
e
e
 
 
 
67. 
 
a. P(X ≤ 250) = F(250;2.5, 200) = 
 
8257
.
1
1
75
.1
)
200
/
250
(
5.2
≈
−
=
−
−
−
e
e
P(X < 250) = P(X ≤ 250) ≈ .8257 
P(X > 300) = 1 – F(300; 2.5, 200) = 
 
0636
.
5.2)
5.1(
=
−
e
 
b. P(100 ≤ X ≤ 250) = F(250;2.5, 200) - F(100;2.5, 200) ≈ .8257 - .162 = .6637 
 
c. 
The median µ~  is requested.  The equation F( µ~ ) = .5 reduces to  
.5 = 
, i.e., ln(.5) ≈ 
5.2)
200
/
~
(µ
−
e
5.2
200
~
⎟
⎠
⎞
⎜
⎝
⎛
−
µ
, so µ~  = (.6931).4(200) = 172.727. 
 
 
68. 
 
a. For x > 3.5, F(x) = P( X ≤ x) = P(X – 3.5 ≤ x – 3.5) = 1 - 
[
]
2
5.1
)
5.3
( −
−
x
e
 
 
b. E(X – 3.5) = 
⎟
⎠
⎞
⎜
⎝
⎛
Γ 2
3
5.1
= 1.329 so E(X) = 4.829 
Var(X) = Var(X – 3.5) = (
)
( )
483
.
2
3
2
5.1
2
2
=
⎥⎦
⎤
⎢⎣
⎡
⎟
⎠
⎞
⎜
⎝
⎛
Γ
−
Γ
 
 
c. 
P(X > 5) = 1 – P(X ≤ 5) = 
[
]
368
.
1
1
1
1
=
=
−
−
−
−
e
e
 
 
d. P(5 ≤ X ≤ 8) = 
[
]
3678
.
0001
.
3679
.
1
1
9
1
1
9
=
−
=
−
=
−
−
−
−
−
−
−
e
e
e
e
 
 
 
 
152

Chapter 4:  Continuous Random Variables and Probability Distributions 
69. 
(
)
∫
∞
−
−
⋅
=
0
1
dx
e
x
x
x
α
β
α
α
β
α
µ
 = (after y = 
α
β ⎟⎟
⎠
⎞
⎜⎜
⎝
⎛x
, dy = 
α
α
β
α
1
−
x
dx ) 
⎟
⎠
⎞
⎜
⎝
⎛+
Γ
⋅
=
∫
∞
−
α
β
β
α
1
1
0
1
dy
e
y
y
 by definition of the gamma function. 
 
 
70. 
 
a. 
( )
(
)2
3
/
1
~
5.
µ
µ
−
−
=
=
e
F
 ⇒ 
50
.2
~
2383
.6
)
5
ln(.
9
~
5.
2
9
/
=
⇒
=
−
=
⇒
=
−
µ
µ
µ
e
 
 
b. 
  
(
)
[
]
⇒
=
−
−
−
5.
1
2
5.1
/
5.3
~µ
e
(
)
2
5.3
~ −
µ
= -2.25 ln(.5) = 1.5596 ⇒ µ~  = 4.75 
 
c. 
P = F(xp) = 1 - 
(
)
α
β
p
x
e
−
⇒ (xp/β)α = -ln(1 – p) ⇒ xp = β[ -ln(1-p)]1/α 
 
d. The desired value of t is the 90th percentile (since 90% will not be refused and 10% will 
be).  From c, the 90th percentile of the distribution of X – 3.5 is 1.5[ -ln(.1)]1/2 = 2.27661, 
so t = 3.5 + 2.2761 = 5.7761 
 
 
71. 
X ∼ Weibull: α=20,β=100 
a. 
(
)
( )
(
)
930
.
070
.
1
1
1
,
20
,
20
100
105
=
−
=
−
=
−
=
−
−
e
e
x
F
x α
β
β
 
 
b. 
(
)
(
)
(
)
298
.
632
.
930
.
1
930
.
100
105
1
=
−
=
−
−
=
−
−
e
F
F
 
 
c. 
(
)
(
)
(
)
)
50
ln(.
50
.
1
50
.
20
100
20
100
20
100
=
−
⇒
=
⇒
−
=
−
−
x
x
x
e
e
 
(
)
18
.
98
)
50
ln(.
100
)
50
ln(.
100
20
20
=
⇒
=
−
⇒
=
⎟
⎠
⎞
⎜
⎝
⎛−
x
x
x
 
 
 
72. 
 
a. 
97
.
123
)
(
82
.4
2
2
=
=
=
⎟⎠
⎞
⎜⎝
⎛
+
e
e
X
E
σ
µ
 
(
)
(
) (
) (
)(
)
53
.
776
,
13
8964
.
34
.
367
,
15
1
)
(
8.
8.
)
5.4
(
2
2
=
=
−
⋅
=
−
+
e
e
X
V
  
373
.
117
=
σ
 
 
b. 
(
)
5517
.
13
.0
8.
5.4
)
100
ln(
)
100
(
=
Φ
=
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
=
≤
z
P
x
P
  
c. 
(
)
)
200
(
1587
.
8413
.
1
00
.1
1
8.
5.4
)
200
ln(
)
200
(
>
=
=
−
=
Φ
−
=
⎟
⎠
⎞
⎜
⎝
⎛
−
≥
=
≥
x
P
z
P
x
P
 
 
153

Chapter 4:  Continuous Random Variables and Probability Distributions 
73. 
 
a. E(X) = 
= 68.0335; V(X) = 
(
)
2
/
2.1
5.3
2
+
e
(
) (
)
(
)
(
)
168
.
14907
1
2
2
2.1
2.1
5.3
2
=
−
⋅
+
e
e
; 
σx = 122.0949 
 
b. P(50 ≤ X ≤ 250) = 
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
−
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
2.1
5.3
)
50
ln(
2.1
5.3
)
250
ln(
z
P
z
P
 
P(Z ≤ 1.68) – P(Z ≤ .34) = .9535 - .6331 = .3204. 
 
c. 
P(X ≤ 68.0335)  = 
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
2.1
5.3
)
0335
.
68
ln(
z
P
= P(Z ≤ .60) = .7257.  The lognormal 
distribution is not a symmetric distribution. 
 
 
74. 
 
a. .5 = F( µ~ ) = 
⎟
⎠
⎞
⎜
⎝
⎛
−
Φ
σ
µ
µ)
~
ln(
, (where µ~  refers to the lognormal distribution and µ and 
σ to the normal distribution).  Since the median of the standard normal distribution is 0, 
0
)
~
ln(
=
−
σ
µ
µ
, so ln( µ~ ) = µ  ⇒ µ~ =
.  For the power distribution, 
µ
e
µ~ =
 
12
.
33
5.3
=
e
 
b. 1 - α = Φ(zα) = P(Z ≤ zα) = 
)
)
(ln(
)
ln(
α
α
σ
µ
σ
µ
z
X
P
z
X
+
≤
=
⎟
⎠
⎞
⎜
⎝
⎛
≤
−
 
=
, so the 100(1 - α)th percentile is 
.  For the power distribution, 
the 95
)
(
α
σ
µ
z
e
X
P
+
≤
α
σ
µ
z
e
+
th percentile is 
 
41
.
238
474
.5
)
2.1
)(
645
.1
(
5.3
=
=
+
e
e
 
 
75. 
 
a. E(X) = 
; Var(X) = 
157
.
149
005
.5
2
/)
01
(.
5
=
=
+
e
e
(
)
594
.
223
1
01
.
)
01
(.
10
=
−
⋅
+
e
e
 
 
b. P(X > 125) = 1 – P(X ≤ 125) = 
(
)
9573
.
72
.1
1
1.
5
)
125
ln(
1
=
−
Φ
−
=
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
−
=
z
P
 
 
c. 
P(110 ≤ X ≤ 125) 
(
)
0414
.
0013
.
0427
.
1.
5
)
110
ln(
72
.1
=
−
=
⎟
⎠
⎞
⎜
⎝
⎛
−
Φ
−
−
Φ
=
 
 
d. µ~ =
  (continued) 
41
.
148
5 =
e
e. 
P(any particular one has X > 125) = .9573 ⇒ expected # = 10(.9573) = 9.573 
 
f. 
We wish the 5th percentile, which is 
 
90
.
125
)
1
)(.
645
.1
(
5
=
−
+
e
 
154

Chapter 4:  Continuous Random Variables and Probability Distributions 
76. 
 
a. E(X) = 
; Var(X) = 
024
.
10
2
/
9
9.1
2
=
+
e
(
)
395
.
125
1
81
.
)
81
(.
8.3
=
−
⋅
+
e
e
, σx = 11.20 
 
b. P(X ≤ 10) = P(ln(X) ≤ 2.3026) = P(Z ≤ .45) = .6736 
P(5 ≤ X ≤ 10)  
= P(1.6094 ≤ ln(X) ≤2.3026)  
= P(-.32 ≤ Z ≤ .45) = .6736 - .3745 = .2991 
 
 
77. 
The point of symmetry must be 2
1 , so we require that (
)
(
)
µ
µ
+
=
−
2
1
2
1
f
f
, i.e., 
(
)
(
)
(
)
(
)
1
2
1
1
2
1
1
2
1
1
2
1
−
−
−
−
−
+
=
+
−
β
α
β
α
µ
µ
µ
µ
, which in turn implies that α = β. 
 
 
78. 
 
a. E(X) = (
)
714
.
7
5
2
5
5
=
=
+
, V(X) = 
0255
.
)
8
)(
49
(
10
=
 
 
b. f(x) = 
( )
( ) ( )
(
)
(
)
5
4
4
30
1
2
5
7
x
x
x
x
−
=
−
⋅
⋅
Γ
Γ
Γ
 for 0 ≤ X ≤ 1, 
so P(X ≤ .2) = 
 
(
)
0016
.
30
2.
0
5
4
=
−
∫
dx
x
x
 
c. 
P(.2 ≤ X ≤ .4) = 
 
(
)
03936
.
30
4.
2.
5
4
=
−
∫
dx
x
x
 
d. E(1 – X) = 1 – E(X) = 1 - 
286
.
7
2
7
5
=
=
 
 
 
79. 
 
a. E(X) = 
(
)
( ) ( )
(
)
(
)
( ) ( )
(
)
∫
∫
−
−
−
−
Γ
Γ
+
Γ
=
−
Γ
Γ
+
Γ
⋅
1
0
1
1
0
1
1
1
1
dx
x
x
dx
x
x
x
β
α
β
α
β
α
β
α
β
α
β
α
 
(
)
( ) ( )
(
) ( )
(
)1
1
+
+
Γ
Γ
+
Γ
⋅
Γ
Γ
+
Γ
β
α
β
α
β
α
β
α
 = 
( )
( ) ( )
(
)
(
) (
)
β
α
α
β
α
β
α
β
α
β
α
α
α
+
=
+
Γ
+
+
Γ
⋅
Γ
Γ
Γ
 
 
b. E[(1 – X)m] = 
(
)
(
)
( ) ( )
(
)
∫
−
−
−
Γ
Γ
+
Γ
⋅
−
1
0
1
1 1
1
dx
x
x
x
m
β
α
β
α
β
α
 
(
)
( ) ( )
(
)
(
)
(
)
(
) ( )
β
β
α
β
β
α
β
α
β
α
β
α
Γ
+
+
Γ
+
Γ
⋅
+
Γ
=
−
Γ
Γ
+
Γ
=
∫
−
+
−
m
m
dx
x
x
m
1
0
1
1 1
 
For m = 1, E(1 – X) = 
β
α
β
+
. 
 
 
 
155

Chapter 4:  Continuous Random Variables and Probability Distributions 
80. 
 
a. E(Y) = 10 
β
α
α
+
=
=
⎟
⎠
⎞
⎜
⎝
⎛
⇒
2
1
20
Y
E
; Var(Y) = 
28
1
2800
100
20
7
100
=
=
⎟
⎠
⎞
⎜
⎝
⎛
⇒
Y
Var
 
(
) (
)
3
,3
1
2
=
=
⇒
+
+
+
β
α
β
α
β
α
αβ
, after some algebra. 
 
b. P(8 ≤ X ≤ 12) = 
⎟
⎠
⎞
⎜
⎝
⎛
−
⎟
⎠
⎞
⎜
⎝
⎛
3,3;
20
8
3,3;
20
12
F
F
= F(.6;3,3) – F(.4; 3,3).   
The standard density function here is  30y2(1 – y)2,  
so P(8 ≤ X ≤ 12) = 
. 
(
)
365
.
1
30
6.
4.
2
2
=
−
∫
dy
y
y
 
c. 
We expect it to snap at 10, so P( Y < 8 or Y > 12) = 1 - P(8 ≤ X ≤ 12)  
= 1 - .365 = .665. 
 
 
Section 4.6 
 
81. 
The given probability plot is quite linear, and thus it is quite plausible that the tension 
distribution is normal. 
 
 
82. 
The z percentiles and observations are as follows: 
 
percentile 
observation 
-1.645 
152.7 
-1.040 
172.0 
-0.670 
172.5 
-0.390 
173.3 
-0.130 
193.0 
0.130 
204.7 
0.390 
216.5 
0.670 
234.9 
1.040 
262.6 
1.645 
422.6 
 
 
-2
-1
0
1
2
200
300
400
z %ile
lifetime
 
The accompanying plot is quite straight except for the point corresponding to the largest 
observation.  This observation is clearly much larger than what would be expected in a normal 
random sample.  Because of this outlier, it would be inadvisable to analyze the data using any 
inferential method that depended on assuming a normal population distribution. 
 
 
 
 
 
 
156

Chapter 4:  Continuous Random Variables and Probability Distributions 
83. 
The z percentile values are as follows: -1.86, -1.32, -1.01, -0.78, -0.58, -0.40, -0.24,-0.08, 
0.08, 0.24, 0.40, 0.58, 0.78, 1.01, 1.30, and 1.86.  The accompanying probability plot is 
reasonably straight, and thus it would be reasonable to use estimating methods that assume a 
normal population distribution. 
 
 
2
1
0
-1
-2
1.8
1.3
0.8
z %ile
thickness
84. 
The Weibull plot uses ln(observations) and the z percentiles of the pi values given.  The 
accompanying probability plot appears sufficiently straight to lead us to agree with the 
argument that the distribution of fracture toughness in concrete specimens could well be 
modeled by a Weibull distribution. 
 
 
 
 
2
1
0
-1
-2
0.0
-0.1
-0.2
-0.3
-0.4
-0.5
-0.6
-0.7
-0.8
z %ile
ln(x)
 
157

Chapter 4:  Continuous Random Variables and Probability Distributions 
85. 
The (z percentile, observation) pairs are (-1.66, .736), (-1.32, .863),  
    (-1.01, .865), (-.78, 
.913), (-.58, .915), (-.40, .937), (-.24, .983), (-.08, 1.007), (.08, 1.011), (.24, 1.064), (.40, 
1.109), (.58, 1.132), (.78, 1.140), (1.01, 1.153), (1.32, 1.253), (1.86, 1.394). The 
accompanying probability plot is very straight, suggesting that an assumption of population 
normality is extremely plausible. 
 
 
 
2
1
0
-1
-2
1.4
1.3
1.2
1.1
1.0
0.9
0.8
0.7
z %ile
obsvn
86. 
 
a. The 10 largest z percentiles are 1.96, 1.44, 1.15, .93, .76, .60, .45, .32, .19 and .06; the 
remaining 10 are the negatives of these values.  The accompanying normal probability 
plot is reasonably straight.  An assumption of population distribution normality is 
plausible. 
 
-2
-1
0
1
2
0
100
200
300
400
500
z %ile
load life
 
 
 
 
158

Chapter 4:  Continuous Random Variables and Probability Distributions 
b. For a Weibull probability plot, the natural logs of the observations are plotted against 
extreme value percentiles; these percentiles are -3.68, -2.55, -2.01, -1.65, -1.37, -1.13, -
.93, -.76, -.59, -.44, -.30, -.16, -.02, .12, .26, .40, .56, .73, .95, and 1.31. The 
accompanying probability plot is roughly as straight as the one for checking normality (a 
plot of ln(x) versus the z percentiles, appropriate for checking the plausibility of a 
lognormal distribution, is also reasonably straight - any of 3 different families of 
population distributions seems plausible.) 
 
-4
-3
-2
-1
0
1
4
5
6
W %ile
ln(loadlife)
 
 
87. 
To check for plausibility of a lognormal population distribution for the rainfall data of 
Exercise 81 in Chapter 1, take the natural logs and construct a normal probability plot.  This 
plot and a normal probability plot for the original data appear below.  Clearly the log 
transformation gives quite a straight plot, so lognormality is plausible.  The curvature in the 
plot for the original data implies a positively skewed population distribution - like the 
lognormal distribution. 
-2
-1
0
1
2
0
1000
2000
3000
z %ile
rainfall
 
-2
-1
0
1
2
1
2
3
4
5
6
7
8
z %ile
ln(rainfall)
 
 
 
 
 
 
 
159

Chapter 4:  Continuous Random Variables and Probability Distributions 
88. 
  
a.  The plot of the original (untransformed) data appears somewhat curved.   
2
1
0
-1
-2
5
4
3
2
1
0
z %iles
precip
 
b. The square root transformation results in a very straight plot.  It is reasonable that this 
distribution is normally distributed. 
-2
-1
0
1
2
0.5
1.0
1.5
2.0
z %iles
sqrt
 
c. The cube root transformation also results in a very straight plot.  It is very reasonable that 
the distribution is normally distributed. 
-2
-1
0
1
2
0.6
1.1
1.6
z %iles
cubert
 
 
 
160

Chapter 4:  Continuous Random Variables and Probability Distributions 
89. 
The pattern in the plot (below, generated by Minitab) is quite linear. It 
is very plausible that strength is normally distributed. 
P-Value:   0.008
A-Squared: 1.065
Anderson-Darling Normality Test
N: 153
StDev: 4.54186
Average: 134.902
145
135
125
.999
.99
.95
.80
.50
.20
.05
.01
.001
Probability
strength
Normal Probability Plot
 
 
 
90. 
We use the data (table below) to create the desired plot. 
ordered absolute 
values (w's) 
probabilities
z 
values 
0.89 
0.525 
0.063 
1.15 
0.575 
0.19 
1.27 
0.625 
0.32 
1.44 
0.675 
0.454 
2.34 
0.725 
0.6 
3.78 
0.775 
0.755 
3.96 
0.825 
0.935 
12.38 
0.875 
1.15 
30.84 
0.925 
1.44 
43.4 
0.975 
1.96 
45
40
35
30
25
20
15
10
5
0
2
1
0
wi
z values
 
This half-normal plot reveals some extreme values, without which the distribution may appear 
to be normal.  
 
161

Chapter 4:  Continuous Random Variables and Probability Distributions 
91. 
The (100p)th percentile η(p) for the exponential distribution with λ = 1 satisfies F(η(p)) = 1 – 
exp[-η(p)] = p, i.e., η(p) = -ln(1 – p).  With n = 16, we need η(p) for p = 
16
5.
15
16
5.1
16
5
,...,
,
.  
These are .032, .398, .170, .247, .330, .421, .521, .633, .758, .901, 1.068, 1.269, 1.520, 1.856, 
2.367, 3.466.  this plot exhibits substantial curvature, casting doubt on the assumption of an 
exponential population distribution.  Because λ is a scale parameter (as is σ for the normal 
family), λ = 1 can be used to assess the plausibility of the entire exponential family. 
 
 
 
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
600
500
400
300
200
100
0
percentile
failtime
Supplementary Exercises 
 
 
92. 
 
a. P(10 ≤ X ≤ 20) = 
4.
25
10 =
 
 
b. P(X ≥ 10) = P(10 ≤ X ≤ 25) = 
6.
25
15 =
 
 
c. 
For  0 ≤ X ≤ 25, F(x) = 
25
25
1
0
x
dy
x
=
∫
.  F(x)=0 for x < 0 and = 1 for x > 25. 
 
d. E(X) = (
)
(
)
5.
12
2
25
0
2
=
+
=
+ B
A
; Var(X) = (
)
083
.
52
12
625
12
2
=
=
−A
B
 
 
 
 
162

Chapter 4:  Continuous Random Variables and Probability Distributions 
93. 
 
a. For  0 ≤ Y ≤ 25, F(y) = 
y
y
u
u
u
u
0
3
2
0
2
36
2
24
1
12
24
1
⎥
⎦
⎤
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
∫
.  Thus  
F(y) = 
⎪
⎪
⎩
⎪⎪
⎨
⎧
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
1
18
48
1
0
3
2
y
y
 
12
12
0
0
>
≤
≤
<
y
y
y
 
 
 
b. P(Y ≤ 4) = F(4) = .259, P(Y > 6) = 1 – F(6) = .5 
P(4 ≤ X ≤ 6) = F(6) – F(4) = .5 - .259 = .241 
 
c. 
E(Y) = 
6
48
3
24
1
12
1
24
1
12
0
4
3
12
0
2
=
⎥⎦
⎤
⎢⎣
⎡
−
=
⎟
⎠
⎞
⎜
⎝
⎛−
∫
y
y
dy
y
y
 
E(Y2) = 
2.
43
12
1
24
1
12
0
3
=
⎟
⎠
⎞
⎜
⎝
⎛−
∫
dy
y
y
, so V(Y) = 43.2 – 36 = 7.2 
 
d. P(Y < 4 or Y > 8) = 1 - P(4 ≤ X ≤ 8) = .518 
 
e. 
the shorter segment has length min(Y, 12 – Y) so 
E[min(Y, 12 – Y)] = 
 
∫
∫
⋅
−
=
⋅
−
6
0
12
0
)
(
)
12
,
min(
)
(
)
12
,
min(
dy
y
f
y
y
dy
y
f
y
y
∫
∫
∫
⋅
−
+
⋅
=
⋅
−
+
12
6
6
0
12
6
)
(
)
12
(
)
(
)
(
)
12
,
min(
dy
y
f
y
dy
y
f
y
dy
y
f
y
y
 = 
75
.3.
24
90 =
 
 
 
94. 
 
a. Clearly f(x) ≥ 0.  The c.d.f. is , for x > 0,  
(
)
(
)
(
)
2
0
2
0
3
4
16
1
4
32
2
1
4
32
)
(
)
(
+
−
=
⎥
⎦
⎤
+
⋅
−
=
+
=
=
∫
∫∞
−
x
y
dy
y
dy
y
f
x
F
x
x
x
 
( F(x) = 0 for x ≤ 0.) 
Since F(∞) = 
 f(x) is a legitimate pdf. 
,1
)
(
=
∫
∞
∞
−
dy
y
f
 
b. See above 
 
c. 
P(2 ≤ X ≤ 5) = F(5) – F(2) = 
247
.
36
16
1
81
16
1
=
⎟
⎠
⎞
⎜
⎝
⎛−
−
−
 
(continued) 
 
163

Chapter 4:  Continuous Random Variables and Probability Distributions 
d. 
(
)
(
)
dx
x
x
dx
x
x
dx
x
f
x
x
E
3
0
3
4
32
)
4
4
(
4
32
)
(
)
(
+
⋅
−
+
=
+
⋅
=
⋅
=
∫
∫
∫
∞
∞
∞
−
∞
∞
−
 
 
 
(
)
(
)
4
4
8
4
32
4
4
32
0
3
0
2
=
−
=
+
−
+
=
∫
∫
∞
∞
dx
x
dx
x
 
 
e. 
E(salvage value) = 
(
)
(
)
67
.
16
)
64
)(
3
(
3200
4
1
3200
4
32
4
100
0
4
0
3
=
=
+
=
+
⋅
+
=
∫
∫
∞
∞
dx
y
dx
y
x
 
 
 
95. 
 
a. By differentiation, 
 
f(x) = 
⎪
⎩
⎪
⎨
⎧
−
0
4
3
4
7
2
x
x
 
otherwise
y
x
3
7
1
1
0
≤
≤
<
≤
 
 
b. P(.5 ≤ X ≤ 2) = F(2) – F(.5) = 
( )
917
.
12
11
3
5.
2
4
3
4
7
2
3
7
2
1
1
3
=
=
−
⎟
⎠
⎞
⎜
⎝
⎛
⋅
−
⎟
⎠
⎞
⎜
⎝
⎛
−
−
 
 
c. 
E(X) = 
213
.1
108
131
4
3
4
7
3
7
1
1
0
2
=
=
⎟
⎠
⎞
⎜
⎝
⎛
−
⋅
+
⋅
∫
∫
dx
x
x
dx
x
x
 
 
 
96. 
µ = 40 V;  σ = 1.5 V 
a. P(39 < X < 42) = 
⎟
⎠
⎞
⎜
⎝
⎛
−
Φ
−
⎟
⎠
⎞
⎜
⎝
⎛
−
Φ
5.1
40
39
5.1
40
42
 
= Φ(1.33) - Φ(-.67) = .9082 - .2514 = .6568 
 
b. We desire the 85th percentile:  40 + (1.04)(1.5) = 41.56 
 
c. 
P(X > 42) = 1 – P(X ≤ 42) = 1  
⎟
⎠
⎞
⎜
⎝
⎛
−
Φ
−
5.1
40
42
= 1 - Φ(1.33) = .0918 
Let D represent the number of diodes out of 4 with voltage exceeding 42. 
P(D ≥ 1 ) = 1 – P(D = 0) = 
=1 - .6803 = .3197 
(
) (
4
0 9082
.
0918
.
0
4
1
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
)
 
 
164

Chapter 4:  Continuous Random Variables and Probability Distributions 
97. 
µ = 137.2 oz.;  σ = 1.6 oz 
a. P(X > 135) = 1 
⎟
⎠
⎞
⎜
⎝
⎛
−
Φ
−
6.1
2.
137
135
= 1 -  Φ(-1.38) = 1 - .0838 = .9162 
 
b. With Y = the number among ten that contain more than 135 oz,  
Y ~ Bin(10, .9162, so P(Y ≥ 8) = b(8; 10, .9162) + b(9; 10, .9162)  
+ b(10; 10, .9162) =.9549. 
 
c. 
µ = 137.2; 
33
.1
65
.1
2.
137
135
=
⇒
−
=
−
σ
σ
 
 
 
98. 
 
a. Let S = defective.  Then p = P(S) = .05; n = 250 ⇒ µ = np = 12.5, σ = 3.446.  The 
random variable X = the number of defectives in the batch of 250.  X ~ Binomial.  Since 
np = 12.5 ≥ 10, and nq = 237.5 ≥ 10, we can use the normal approximation. 
P(Xbin ≥ 25) ≈ 1 
(
)
0003
.
9997
.
1
48
.3
1
446
.3
5.
12
5.
24
=
−
=
Φ
−
=
⎟
⎠
⎞
⎜
⎝
⎛
−
Φ
−
 
 
b. P(Xbin = 10) ≈ P(Xnorm ≤ 10.5) - P(Xnorm ≤ 9.5) 
 = 
(
)
(
)
0888
.
1922
.
2810
.
87
.
58
.
=
−
=
−
Φ
−
−
Φ
 
 
 
99. 
 
a. P(X > 100) = 1 
(
)
3859
.
6141
.
1
29
.
1
14
96
100
=
−
=
Φ
−
=
⎟
⎠
⎞
⎜
⎝
⎛
−
Φ
−
 
 
b. P(50 < X < 80) = 
⎟
⎠
⎞
⎜
⎝
⎛
−
Φ
−
⎟
⎠
⎞
⎜
⎝
⎛
−
Φ
14
96
50
14
96
80
 
= Φ(-1.5) - Φ(-3.29) = .1271 - .0005 = .1266. 
 
c. 
a = 5th percentile = 96 + (-1.645)(14) = 72.97. 
 
b = 95th percentile = 96 + (1.645)(14) = 119.03.  The interval (72.97, 119.03) contains the 
central 90% of all grain sizes. 
 
 
165

Chapter 4:  Continuous Random Variables and Probability Distributions 
100. 
 
a. F(X) = 0 for x < 1 and = 1 for x > 3.  For 1 ≤ x ≤ 3, 
 
∫∞
−
=
x
dy
y
f
x
F
)
(
)
(
⎟
⎠
⎞
⎜
⎝
⎛−
=
⋅
+
=
∫
∫∞
−
x
dy
y
dy
x
1
1
51
.1
1
2
3
0
1
2
1
 
 
b. P(X ≤ 2.5) = F(2.5) = 1.5(1 - .4) = .9; P(1.5 ≤ x ≤ 2.5) =  
F(2.5) – F(1.5) = .4 
 
c. 
E(X) = 
]
648
.1
)
ln(
5.1
1
2
3
1
2
3
3
1
3
1
3
1
2
=
=
=
⋅
⋅
=
∫
∫
x
dx
x
dx
x
x
 
 
d. E(X2) = 
3
2
3
1
2
3
3
1
3
1
2
2
=
=
⋅
⋅
=
∫
∫
dx
dx
x
x
, so V(X) = E(X2) – [E(X)]2 = .284,  
σ =.553 
 
e. 
h(x) = 
 
 
⎪⎩
⎪⎨
⎧
−
1
5.1
0
x
3
5.2
5.2
5.1
5.1
1
≤
≤
≤
≤
≤
≤
x
x
x
 
so E[h(X)] = 
(
)
267
.
1
2
3
1
1
2
3
5.1
3
5.2
2
5.2
5.1
2
=
⋅
⋅
+
⋅
⋅
−
=
∫
∫
dx
x
dx
x
x
 
 
 
101. 
 
a.  
3
2
1
0
-1
-2
0.4
0.3
0.2
0.1
0.0
x
f(x)
 
b. F(x) = 0 for x < -1 or == 1 for x > 2.  For –1 ≤ x ≤ 2, 
(
)
27
11
3
4
9
1
4
9
1
)
(
3
1
2
+
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
=
−
= ∫−
x
x
dy
y
x
F
x
 
 
c. 
The median is 0 iff F(0) = .5.  Since F(0) = 27
11 , this is not the case.  Because 27
11 < .5, the 
median must be greater than 0. 
 
d. Y is a binomial r.v. with n = 10 and p = P(X > 1) = 1 – F(1) = 27
5  
 
 
 
166

Chapter 4:  Continuous Random Variables and Probability Distributions 
102. 
 
a. E(X) = 
λ
1  = 1.075, 
λ
σ
1
=
 = 1.075 
 
b. P(3.0 < X) = 1 – P(X ≤ 3.0) = 1 – F(3.0) = 3-.93(3.0) = .0614 
P(1.0 ≤ X ≤  3.0) = F(3.0) – F(1.0) = .333 
 
c. 
The 90th percentile is requested; denoting it by c, we have  
 
 
.9 = F(c) = 1 – e-(.93)c, whence c = 
476
.2
)
93
.
(
)1
ln(.
=
−
 
 
 
103. 
 
a. P(X ≤ 150) = 
368
.
)1
exp(
)]
0
exp(
exp[
90
)
150
150
(
exp
exp
=
−
=
−
=
⎥⎦
⎤
⎢⎣
⎡
⎟
⎠
⎞
⎜
⎝
⎛
−
−
−
, where 
exp(u) = eu.  P(X ≤ 300) = 
828
.
)]
6667
.1
exp(
exp[
=
−
−
,  
and P(150 ≤ X ≤  300) = .828 - .368 = .460. 
 
b. The desired value c is the 90th percentile, so c satisfies  
.9 = 
⎥⎦
⎤
⎢⎣
⎡
⎟
⎠
⎞
⎜
⎝
⎛
−
−
−
90
)
150
(
exp
exp
c
.  Taking the natural log of each side twice in succession 
yields ln[ ln(.9)] = 
90
)
150
( −
−c
, so c = 90(2.250367) + 150 = 352.53. 
 
c. 
f(x) = F′(X) = 
(
)
(
)
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
−
⋅
⎥
⎦
⎤
⎢
⎣
⎡
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
−
−
⋅
β
α
β
α
β
x
x
exp
exp
exp
1
 
 
d. We wish the value of x for which f(x) is a maximum; this is the same as the value of x for 
which ln[f(x)] is a maximum.  The equation of 
(
)
0
]
)
(
[ln
=
dx
x
f
d
 gives 
(
)
1
exp
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
−
β
α
x
, so 
(
)
0
=
−
−
β
α
x
, which implies that x = α.  Thus the mode is α. 
 
e. 
E(X) = .5772β + α = 201.95, whereas the mode is 150 and the median is  
–(90)ln[-ln(.5)] + 150 = 182.99.  The distribution is positively skewed. 
 
 
104. 
 
a. E(cX) = cE(X) = 
λ
c  
 
b. E[c(1 - .5eax)] = 
(
)
a
a
c
dx
e
e
c
x
ax
−
−
=
⋅
−
∫
∞
−
λ
λ
λ
λ
]
5
[.
5.
1
0
 
 
167

Chapter 4:  Continuous Random Variables and Probability Distributions 
105. 
 
a. From a graph of f(x; µ, σ) or by differentiation,  x* = µ. 
 
b. No; the density function has constant height for A ≤ X ≤ B. 
 
c. 
F(x;λ) is largest for x = 0 (the derivative at 0 does not exist since f is not continuous 
there) so x* = 0. 
 
d. 
(
)
(
)
( )
(
)
(
)
;
)
ln(
1
ln
ln
]
,
;
ln[
β
α
α
β
β
α
α
x
x
x
f
−
−
+
Γ
−
−
=
 
 
(
)
β
α
β
α
β
α
)1
(
*
1
1
]
,
;
ln[
−
=
=
⇒
−
−
=
x
x
x
x
f
dx
d
 
 
e. 
From d 
( )
.2
2
1
2
*
−
=
⎟
⎠
⎞
⎜
⎝
⎛
−
=
ν
ν
x
 
 
 
106. 
 
a. 
 1
5.
5.
1.
1.
)
(
0
2.
0
2.
=
+
=
+
=
∫
∫
∫
∞
−
∞
−
∞
∞
−
dx
e
dx
e
dx
x
f
x
x
 
-2
-1
0
1
2
0.00
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.10
x
fx
b. For x < 0, F(x) = 
x
x
y
e
dy
e
2.
2.
2
1
1.
=
∫∞
−
. 
For x ≥ 0, F(x) = 
x
x
y
e
dy
e
2.
0
2.
2
1
1
1.
2
1
−
−
−
=
+ ∫
. 
 
c. 
P(X < 0) = F(0) = 
5.
2
1 =
, P(X < 2) = F(2) = 1 - .5e-.4 = .665, 
P(-1 ≤ X ≤ 2) – F(2) – F(-1) = .256, 1 - (-2 ≤ X ≤ 2) = .670 
 
168

Chapter 4:  Continuous Random Variables and Probability Distributions 
107. 
 
a. Clearly f(x; λ1, λ2, p) ≥ 0 for all x, and 
 
∫
∞
∞
−
dx
p
x
f
)
,
,
;
(
2
1 λ
λ
=
  
(
)
[
]
(
)
∫
∫
∫
∞
∞
−
−
∞
−
−
−
+
=
−
+
0
0
2
1
0
2
1
2
1
2
1
1
1
dx
e
p
dx
e
p
dx
e
p
e
p
x
x
x
x
λ
λ
λ
λ
λ
λ
λ
λ
= p + (1 – p) = 1 
 
b. For x > 0, F(x; λ1, λ2, p) =
 
).
1
)(
1(
)
1(
)
,
,
;
(
2
1
0
2
1
x
x
x
e
p
e
p
dy
p
y
f
λ
λ
λ
λ
−
−
−
−
+
−
=
∫
 
c. 
E(X) = 
 
[
]
∫
∞
−
−
−
+
⋅
0
2
1
)
)
1(
)
2
1
dx
e
p
e
p
x
x
x
λ
λ
λ
λ
(
)
2
1
0
2
0
1
1
)
1(
2
1
λ
λ
λ
λ
λ
λ
p
p
dx
e
x
p
dx
e
x
p
x
x
−
+
=
−
+
=
∫
∫
∞
−
∞
−
 
 
d. E(X2) = 
(
)
2
2
2
1
1
2
2
λ
λ
p
p
−
+
, so Var(X) = 
(
)
2
2
2
1
1
2
2
λ
λ
p
p
−
+
(
)
2
2
1
1
⎥⎦
⎤
⎢⎣
⎡
−
+
−
λ
λ
p
p
 
 
e. 
For an exponential r.v., CV = 
1
1
1
=
λ
λ
.  For X hyperexponential,  
CV = 
(
)
(
)
2
1
2
2
1
2
2
2
1
1
1
1
2
2
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
−
⎥
⎦
⎤
⎢
⎣
⎡
−
+
−
+
λ
λ
λ
λ
p
p
p
p
= 
(
)
(
)
2
1
1
)
1(
)
1(
2
2
1
2
2
1
2
2
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡
−
−
+
−
+
λ
λ
λ
λ
p
p
p
p
 
 
= [2r – 1]1/2   where r = (
)
(
)
2
1
2
2
1
2
2
)
1(
)
1(
λ
λ
λ
λ
p
p
p
p
−
+
−
+
.  But straightforward algebra shows that r > 
1 provided 
2
1
λ
λ ≠
, so that CV > 1. 
 
f. 
λ
µ
n
=
, 
 
2
2
λ
σ
n
=
,  
so 
λ
σ
n
=
 
and CV = 
1
1 <
n
 if n > 1. 
 
 
 
169

Chapter 4:  Continuous Random Variables and Probability Distributions 
108. 
 
a. 
(
)
α
α
α
α
α
−
−
∞
−
=
⇒
−
⋅
=
= ∫
1
1
5
5
1
1
5
1
k
k
dx
x
k
 where we must have α > 1. 
 
b. For x ≥ 5, F(x) = 
1
1
1
1
5
5
1
1
5
1
5
−
−
−
−
⎟
⎠
⎞
⎜
⎝
⎛
−
=
⎥⎦
⎤
⎢⎣
⎡
−
=
∫
α
α
α
α
α
x
x
dy
y
k
x
. 
 
c. 
E(X) = 
(
)
2
5
2
5
1
5
−
⋅
=
⋅
=
⋅
−
∞
−
∞
∫
∫
α
α
α
α
k
dx
x
k
x
dx
x
k
x
, provided α > 2. 
 
d. 
(
)
(
)
1
5
5
1
5
5
5
5
ln
−
⎟
⎠
⎞
⎜
⎝
⎛
−
=
=
≤
=
⎟
⎠
⎞
⎜
⎝
⎛
≤
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
≤
⎟
⎠
⎞
⎜
⎝
⎛
α
y
y
y
y
e
e
F
e
X
P
e
X
P
y
X
P
 
(
)y
e
1
1
−
−
−
α
, the cdf of an exponential r.v. with parameter α - 1. 
 
 
109. 
 
a. A lognormal distribution, since 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
i
o
I
I
ln
 is a normal r.v. 
 
b. 
(
)
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
≤
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
>
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
>
=
>
2
ln
ln
1
2
ln
ln
2
2
i
o
i
o
i
o
i
o
I
I
P
I
I
P
I
I
P
I
I
P
 
(
)
1
14
.6
1
05
.
1
2
ln
1
=
−
Φ
−
=
⎟
⎠
⎞
⎜
⎝
⎛
−
Φ
−
 
 
c. 
,
72
.2
2
/
0025
.
1
=
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
e
I
I
E
i
o
 
(
)
0185
.
1
0025
.
0025
.
2
=
−
⋅
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
e
e
I
I
Var
i
o
 
 
 
 
170

Chapter 4:  Continuous Random Variables and Probability Distributions 
110. 
 
a.  
250
200
150
100
50
0
1.0
0.5
0.0
C1
C2
 
 
b. P(X > 175) = 1 – F(175; 9, 180) = 
(
)
4602
.
9
180
175
=
−
e
 
P(150 ≤ X ≤ 175) = F(175; 9, 180) - F(150; 9, 180)  
= .5398 - .1762 = .3636 
 
c. 
P(at least one) = 1 – P(none) = 1 – (1 - .3636)2 = .5950 
 
 
d. We want the 10th percentile:  .10 = F( x; 9, 180) = 
(
)
9
180
1
x
e
−
−
.  A small bit of algebra 
leads us to x = 140.178.  Thus 10% of all tensile strengths will be less than 140.178 MPa. 
 
 
111. 
F(y) = P(Y ≤ y) = P(σZ + µ ≤ y) = 
(
)
(
)
∫
−
∞
−
−
=
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
σ
µ
π
σ
µ
y
z dz
e
y
Z
P
2
2
1
2
1
.  Now 
differentiate with respect to y to obtain a normal pdf with parameters µ and σ. 
 
 
112. 
 
a. FY(y) = P(Y ≤ y) = P(60X ≤ y) =
.
;
60
60
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
=
⎟
⎠
⎞
⎜
⎝
⎛
≤
α
β
y
F
y
X
P
  Thus fY(y) 
=
(
)
( )
α
β
β
α
β
α
β
α
Γ
=
⋅⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
−
60
60
1
;
60
60
1
y
e
y
y
f
, which shows that Y has a gamma distribution 
with parameters α and 60β. 
 
b.  With c replacing 60 in  a, the same argument shows that cX has a gamma distribution 
with parameters α and cβ. 
 
 
 
171

Chapter 4:  Continuous Random Variables and Probability Distributions 
113. 
 
a. Y = -ln(X) ⇒ x = e-y = k(y), so k′(y) = -e-y.  Thus since f(x) = 1,  
g(y) = 1 ⋅ | -e-y | = e-y for 0 < y < ∞, so y has an exponential distribution with parameter λ 
= 1. 
 
b. y = σZ + µ ⇒ y = h(z) = σZ + µ ⇒ z = k(y) = (
)
σ
µ
−
y
 and k′(y) = 
σ
1 , from which the 
result follows easily. 
 
c. 
y = h(x) = cx ⇒ x = k(y) = 
c
y  and k′(y) = 
c
1 , from which the result follows easily. 
 
 
114. 
 
a. If we let 
2
=
α
 and 
σ
β
2
=
, then we can manipulate f(v) as follows: 
(
)
(
)
(
)2
2
2
2
2
2
1
2
/
1
2
2
2
/
2
2
/
2
2
2
2
2
)
(
β
ν
α
α
σ
ν
σ
ν
σ
ν
ν
β
α
ν
σ
ν
σ
σ
ν
ν
−
−
−
−
−
−
=
=
=
=
e
e
e
e
f
, 
which is in the Weibull family of distributions. 
 
b. 
( ) ∫
−
=
25
0
800
400
ν
ν
ν
ν
d
e
F
; cdf: (
)
800
2
2
1
1
2
,2;
v
e
e
F
−
−
=
−
=
⎟⎠
⎞
⎜⎝
⎛
−
σ
ν
σ
ν
, so 
(
)
542
.
458
.
1
1
2
,2;
25
800
625
=
−
=
−
=
−
e
F
 
 
 
115. 
 
a. Assuming independence, P(all 3 births occur on March 11) = (
)
00000002
.
3
365
1
=
 
 
b. (
)
0000073
.
)
365
(
3
365
1
=
 
 
c. 
Let X = deviation from due date.  X∼N(0, 19.88).  Then the baby due on March 15 was 4 
days early.  P(x = -4) ≈ P(-4.5 < x < -3.5) 
(
)
(
)
0196
.
4090
.
4286
.
237
.
18
.
88
.
19
5.4
88
.
19
5.3
=
−
=
−
Φ
−
−
Φ
=
⎟
⎠
⎞
⎜
⎝
⎛−
Φ
−
⎟
⎠
⎞
⎜
⎝
⎛−
Φ
=
.  
Similarly, the baby due on April 1 was 21 days early, and P(x = -21) 
≈
(
)
(
)
0114
.
1401
.
1515
.
08
.1
03
.1
88
.
19
5.
21
88
.
19
5.
20
=
−
=
−
Φ
−
−
Φ
=
⎟
⎠
⎞
⎜
⎝
⎛−
Φ
−
⎟
⎠
⎞
⎜
⎝
⎛−
Φ
. 
The baby due on April 4 was 24 days early, and P(x = -24) ≈ .0097 
 
Again, assuming independence, P( all 3 births occurred on March 11) = 
 
(
)(
)(
)
00002145
.
0097
.
0114
.
0196
.
=
 
d. To calculate the probability of the three births happening on any day, we could make 
similar calculations as in part c for each possible day, and then add the probabilities. 
 
 
172

Chapter 4:  Continuous Random Variables and Probability Distributions 
116. 
 
a. F(x) = 
 and F(x) = 
, so r(x) = 
x
e λ
λ
−
x
e λ
−
−
1
λ
λ
λ
λ
=
−
−
x
x
e
e
, a constant (independent of X);  
this is consistent with the memoryless property of the exponential distribution. 
 
b. r(x) = 
1
−
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
α
α
β
α
x
; for α > 1 this is increasing, while for α < 1 it is a decreasing function. 
 
c. 
ln(1 – F(x)) = 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
−
−
=
⇒
⎥⎦
⎤
⎢⎣
⎡−
−
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛−
−∫
β
α
β
α
β
α
2
2
2
1
)
(
2
1
x
x
e
x
F
x
x
dx
x
, 
f(x) = 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
−
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛−
β
α
β
α
2
2
1
x
x
e
x
  
 0 ≤ x ≤ β 
 
 
117. 
 
a. FX(x) = 
(
)
(
)
(
)
x
e
U
P
x
U
P
x
U
P
λ
λ
λ
−
≥
−
=
−
≥
−
=
⎟
⎠
⎞
⎜
⎝
⎛
≤
−
−
1
)
1
ln(
1
ln
1
 
(
)
x
x
e
e
U
P
λ
λ
−
−
−
=
−
≤
=
1
1
since FU(u) = u (U is uniform on [0, 1]).  Thus X has an 
exponential distribution with parameter λ. 
 
b. By taking successive random numbers u1, u2, u3, …and computing 
(
)
i
i
u
x
−
−
=
1
ln
10
1
, 
… we obtain a sequence of values generated from an exponential distribution with 
parameter λ = 10. 
 
 
118. 
 
a. E(g(X)) ≈ E[g(µ) + g′(µ)(X - µ)] = E(g(µ)) + g′(µ)⋅E(X - µ), but E(X) - µ = 0 and E(g(µ)) 
= g(µ) ( since g(µ) is constant), giving E(g(X)) ≈ g(µ). 
V(g(X)) ≈ V[g(µ) + g′(µ)(X - µ)] = V[g′(µ)(X - µ)] = (g′(µ))2⋅V(X - µ) = (g′(µ))2⋅V(X). 
 
b. 
2
)
(
,
)
(
I
v
I
g
I
v
I
g
−
=
′
=
, so (
)
20
)
(
v
v
I
g
E
I
R
=
≈
=
µ
µ
 
(
)
( )
800
20
,
)
(
2
)
(
2
2
v
v
I
V
v
I
g
V
I
I
g
I
=
⋅
≈
⋅
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛−
≈
σ
σ
µ
 
 
 
119. 
g(µ) + g′(µ)(X - µ) ≤ g(X) implies that E[g(µ) + g′(µ)(X - µ)] = E(g(µ)) = g(µ) ≤ E(g(X)), i.e. 
that g(E(X)) ≤ E(g(X)). 
 
 
 
173

Chapter 4:  Continuous Random Variables and Probability Distributions 
120. 
For  y > 0, 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
≤
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
≤
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
≤
=
≤
=
2
2
2
)
(
)
(
2
2
2
2
y
X
P
y
X
P
y
X
P
y
Y
P
y
F
β
β
β
.  Now 
take the cdf of X (Weibull), replace x by 
2
y
β
, and then differentiate with respect to y to 
obtain the desired result fY(y). 
 
 
 
 
 
 
 
 
 
 
174

CHAPTER 5 
 
Section 5.1 
 
1. 
 
a. P(X = 1, Y = 1) = p(1,1) = .20 
 
b. P(X ≤ 1 and Y ≤ 1) = p(0,0) + p(0,1) + p(1,0) + p(1,1) = .42 
 
c. 
At least one hose is in use at both islands.  P(X ≠ 0 and Y ≠ 0) = p(1,1) + p(1,2) + p(2,1) 
+ p(2,2) = .70 
 
d. By summing row probabilities, px(x) = .16, .34, .50 for x = 0, 1, 2, and by summing 
column probabilities, py(y) = .24, .38, .38 for y = 0, 1, 2.  P(X ≤ 1) = px(0) + px(1) = .50 
 
e. 
P(0,0) = .10, but px(0) ⋅  py(0) = (.16)(.24) = .0384 ≠ .10, so X and Y are not independent. 
 
 
2. 
 
a.  
 
 
 
 
y 
 
 
 
 
p(x,y) 
0 
1 
2 
3 
4 
 
 
0 
.30 
.05 
.025 
.025 
.10 
.5 
x 
1 
.18 
.03 
.015 
.015 
.06 
.3 
 
2 
.12 
.02 
.01 
.01 
.04 
.2 
 
 
.6 
.1 
.05 
.05 
.2 
 
 
b. P(X ≤ 1 and Y ≤ 1) = p(0,0) + p(0,1) + p(1,0) + p(1,1) = .56  
= (.8)(.7) = P(X ≤ 1) ⋅ P(Y ≤ 1) 
 
c. 
P( X + Y = 0) = P(X = 0 and Y = 0) = p(0,0) = .30 
 
d. P(X + Y ≤ 1) = p(0,0) + p(0,1) + p(1,0) = .53 
 
 
3. 
 
a. p(1,1) = .15, the entry in the 1st row and 1st column of the joint probability table. 
 
b. P( X1 = X2 ) = p(0,0) + p(1,1) + p(2,2) + p(3,3) = .08+.15+.10+.07 = .40 
 
c. 
A = { (x1, x2): x1 ≥ 2 + x2 } ∪  { (x1, x2): x2 ≥ 2 + x1 } 
P(A) = p(2,0) + p(3,0) +  p(4,0) + p(3,1) +  p(4,1) + p(4,2) + p(0,2) + p(0,3) + p(1,3) =.22 
 
d. P( exactly 4) = p(1,3) + p(2,2) + p(3,1) + p(4,0) = .17 
P(at least 4) = P(exactly 4) + p(4,1) + p(4,2) + p(4,3) + p(3,2) + p(3,3) + p(2,3)=.46 
175 

Chapter 5:  Joint Probability Distributions and Random Samples 
4. 
 
a. P1(0) = P(X1 = 0) = p(0,0) + p(0,1) + p(0,2) + p(0,3) = .19 
P1(1) = P(X1 = 1) = p(1,0) + p(1,1) + p(1,2) + p(1,3) = .30, etc. 
 
x1
0 
1 
2 
3 
4 
p1(x1) 
.19 
.30 
.25 
.14 
.12 
 
b. P2(0) = P(X2 = 0) = p(0,0) + p(1,0) + p(2,0) + p(3,0) + p(4,0) = .19, etc 
 
x2
0 
1 
2 
3 
p2(x2) 
.19 
.30 
.28 
.23 
 
c. 
p(4,0) = 0, yet p1(4) = .12 > 0 and p2(0) = .19 > 0 , so p(x1 , x2) ≠ p1(x1) ⋅ p2(x2) for every  
(x1 , x2), and the two variables are not independent. 
 
 
5. 
 
a. P(X = 3, Y = 3) = P(3 customers, each with 1 package)  
= P( each has 1 package | 3 customers) ⋅ P(3 customers) 
= (.6)3 ⋅ (.25) = .054 
 
b. P(X = 4, Y = 11) = P(total of 11 packages | 4 customers) ⋅ P(4  customers) 
 
Given that there are 4 customers, there are 4 different ways to have a total of 11 
packages: 3, 3, 3, 2 or 3, 3, 2, 3 or 3, 2, 3 ,3  or 2, 3, 3, 3.  Each way has probability 
(.1)3(.3), so p(4, 11) = 4(.1)3(.3)(.15) = .00018 
 
 
6. 
 
a. p(4,2) = P( Y = 2 | X = 4) ⋅ P(X = 4) = 
 
0518
.
)
15
(.
)
4
(.
)
6
(.
2
4
2
2
=
⋅
⎥
⎦
⎤
⎢
⎣
⎡
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
b. P(X = Y) = p(0,0) + p(1,1) + p(2,2) + p(3,3) + p(4,4) = .1+(.2)(.6) + (.3)(.6)2 + (.25)(.6)3 
+ (.15)(.6)4 = .4014 
 
 
176

Chapter 5:  Joint Probability Distributions and Random Samples 
c. 
p(x,y) = 0 unless y = 0, 1, …, x; x = 0, 1, 2, 3, 4.  For any such pair,  
p(x,y) = P(Y = y | X = x) ⋅ P(X = x) = 
 )
(
)
4
(.
)
6
(.
x
p
y
x
x
y
x
y
⋅
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
 
py(4) = p(y = 4) = p(x = 4, y = 4) = p(4,4) = (.6)4⋅(.15) = .0194 
py(3) = p(3,3) + p(4,3) =  
 
1058
.
)
15
)(.
4
(.
)
6
(.
3
4
)
25
(.
)
6
(.
3
3
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
py(2) = p(2,2) + p(3,2) + p(4,2) = 
 )
25
)(.
4
(.
)
6
(.
2
3
)
3
(.
)
6
(.
2
2
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
 
 
2678
.
)
15
(.
)
4
(.
)
6
(.
2
4
2
2
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
py(1) = p(1,1) + p(2,1) + p(3,1) + p(4,1) = 
 )
3
)(.
4
)(.
6
(.
1
2
)
2
)(.
6
(.
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
 
 
3590
.
)
15
(.
)
4
)(.
6
(.
1
4
)
25
(.
)
4
)(.
6
(.
1
3
3
2
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
py(0) = 1 – [.3590+.2678+.1058+.0194] = .2480 
 
 
7. 
 
a. p(1,1) = .030 
 
b. P(X ≤ 1 and Y ≤ 1 = p(0,0) + p(0,1) + p(1,0) + p(1,1) = .120 
 
c. 
P(X = 1) = p(1,0) + p(1,1) + p(1,2) = .100; P(Y = 1) = p(0,1) + … + p(5,1) = .300 
 
d. P(overflow) = P(X + 3Y > 5) = 1 – P(X + 3Y ≤ 5) = 1 – P[(X,Y)=(0,0) or …or (5,0) or 
(0,1) or (1,1) or (2,1)] = 1 - .620 = .380 
 
e. 
The marginal probabilities for X (row sums from the joint probability table) are px(0) = 
.05, px(1) = .10 , px(2) = .25,  px(3) = .30, px(4) = .20, px(5) = .10; those for Y (column 
sums) are py(0) = .5, py(1) = .3, py(2) = .2.  It is now easily verified that for every (x,y), 
p(x,y) = px(x) ⋅ py(y), so X and Y are independent. 
 
 
 
177

Chapter 5:  Joint Probability Distributions and Random Samples 
8. 
 
a. numerator = 
 
(
)(
)(
)
240
,
30
12
45
56
1
12
2
10
3
8
=
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
denominator = 
; p(3,2) = 
775
,
593
6
30 =
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
0509
.
775
,
593
240
,
30
=
 
 
b. p(x,y) = 
(
)
⎪⎪
⎪
⎩
⎪⎪
⎪
⎨
⎧
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
−
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
0
6
30
6
12
10
8
y
x
y
x
 
otherwise
y
x
that
such
egers
negative
non
are
y
x
6
0
_
_
int
_
_
,
≤
+
≤
−
 
 
 
9. 
 
a. 
 
∫∫
∫∫
+
=
=
∞
∞
−
∞
∞
−
30
20
30
20
2
2
)
(
)
,
(
1
dxdy
y
x
K
dxdy
y
x
f
∫
∫
∫∫
∫∫
+
=
+
=
30
20
2
30
20
2
30
20
30
20
2
30
20
30
20
2
10
10
dy
y
K
dx
x
K
dxdy
y
K
dydx
x
K
 
000
,
380
3
3
000
,
19
20
=
⇒
⎟
⎠
⎞
⎜
⎝
⎛⋅
=
K
K
 
 
b. P(X < 26 and Y < 26) = 
 
∫
∫∫
=
+
26
20
2
26
20
26
20
2
2
12
)
(
dx
x
K
dxdy
y
x
K
 
 
 
 
3024
.
304
,
38
4
26
20
3
=
=
K
Kx
 
 
 
c. 
 
 
 
 
 
 
 
 
 
 
I
II
2
+
= x
y
2
−
= x
y
20
20
30
30
III
 
 
P( | X – Y | ≤ 2 ) = 
 
∫∫
III
region
dxdy
y
x
f
)
,
(
 
 
 
 
 
∫∫
∫∫
−
−
II
I
dxdy
y
x
f
dxdy
y
x
f
)
,
(
)
,
(
1
 
 
 
 
 
∫∫
∫∫
−
+
−
−
30
22
2
20
28
20
30
2
)
,
(
)
,
(
1
x
x
dydx
y
x
f
dydx
y
x
f
 
 
 
 
= (after much algebra) .3593 
 
 
178

Chapter 5:  Joint Probability Distributions and Random Samples 
d. fx(x) = 
30
20
3
2
30
20
2
2
3
10
)
(
)
,
(
y
K
Kx
dy
y
x
K
dy
y
x
f
+
=
+
= ∫
∫
∞
∞
−
 
 
 
 
= 10Kx2 + .05,   
20 ≤ x ≤ 30 
 
e. 
fy(y) is obtained by substituting y for x in (d);  clearly f(x,y) ≠ fx(x) ⋅ fy(y), so X and Y are 
not independent. 
 
 
10. 
 
a. f(x,y) = 
 
⎩
⎨
⎧
0
1
otherwise
y
x
6
5,6
5
≤
≤
≤
≤
 
since fx(x) = 1, fy(y) = 1 for 5 ≤ x ≤ 6, 5 ≤ y ≤ 6 
 
b. P(5.25 ≤ X ≤ 5.75, 5.25 ≤ Y ≤ 5.75) = P(5.25 ≤ X ≤ 5.75) ⋅ P(5.25 ≤ Y ≤ 5.75) = (by 
independence) (.5)(.5) = .25 
 
c. 
 
 
 
 
 
 
 
 
 
 
P((X,Y) ∈ A) = ∫∫
 
A
dxdy
1
 
 
= area of A = 1 – (area of I + area of II ) 
 
 
= 
306
.
36
11
36
25
1
=
=
−
 
 
 
I
II
6
/1
+
= x
y
6
/1
−
= x
y
5
5
6
6
11. 
 
a. p(x,y) = 
!
!
y
e
x
e
y
x
µ
λ
µ
λ
−
−
⋅
 for x = 0, 1, 2, …; y = 0, 1, 2, … 
 
b. p(0,0)  + p(0,1) + p(1,0) = 
[
]
µ
λ
µ
λ
+
+
−
−
1
e
 
 
c. 
P( X+Y=m ) = 
∑
∑
=
=
−
−
=
−
=
−
=
=
m
k
k
m
k
m
k
k
m
k
e
k
m
Y
k
X
P
0
0
)!
(
!
)
,
(
µ
λ
µ
λ
 
!
)
(
!
)
(
0
)
(
m
e
k
m
m
e
m
m
k
k
m
k
µ
λ
µ
λ
µ
λ
µ
λ
+
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
−
=
−
+
−
∑
, so the total # of errors X+Y also has a 
Poisson distribution with parameter 
µ
λ +
. 
 
179

Chapter 5:  Joint Probability Distributions and Random Samples 
12. 
 
a. 
P(X> 3) = 
 
050
.
3
3
0
)
1
(
=
= ∫
∫∫
∞
−
∞
∞
+
−
dx
e
dydx
xe
x
y
x
 
b. 
The marginal pdf of X is 
 for 0 ≤ x; that of Y is 
x
y
x
e
dy
xe
−
∞
+
−
=
∫0
)
1
(
2
3
)
1(
)
1(
1
y
dx
xe
y
x
+
=
∫
∞
+
−
 for 0 ≤ y.  It is now clear that f(x,y) is not the product of  
the marginal pdf’s, so the two r.v’s are not independent. 
 
c. 
P( at least one exceeds 3) = 1 – P(X ≤ 3 and Y ≤ 3)  
=
 
∫∫
∫∫
−
−
+
−
−
=
−
3
0
3
0
3
0
3
0
)
1(
1
1
dy
e
xe
dydx
xe
xy
x
y
x
=
 
300
.
25
.
25
.
)
1(
1
12
3
3
0
3
=
−
+
=
−
−
−
−
−
−
∫
e
e
dx
e
e
x
x
 
 
13. 
 
a. f(x,y) = fx(x) ⋅ fy(y) = 
 
 
⎩
⎨
⎧
−
−
0
y
x
e
otherwise
y
x
0
,0
≥
≥
 
b. P(X ≤ 1 and Y ≤ 1) = P(X ≤ 1) ⋅ P(Y ≤ 1) = (1 – e-1) (1 – e-1) = .400 
 
c. 
P(X + Y ≤ 2) = 
[
]
∫
∫∫
−
−
−
−
−
−
−
=
2
0
)
2
(
2
0
2
0
1
dx
e
e
dx
dy
e
x
x
x
y
x
 
=
 
594
.
2
1
)
(
2
2
2
0
2
=
−
−
=
−
−
−
−
−
∫
e
e
dx
e
e x
 
d. P(X + Y ≤ 1) = 
,  
[
]
264
.
2
1
1
1
1
0
)
1(
=
−
=
−
−
−
−
−
∫
e
dx
e
e
x
x
so P( 1 ≤ X + Y ≤ 2 ) = P(X + Y ≤ 2) – P(X + Y ≤ 1) = .594 - .264 = .330 
 
 
14. 
 
a. P(X1 < t, X2 < t, … , X10 < t) = P(X1 < t) … P( X10 < t) = 
 
10
)
1(
t
e λ
−
−
 
b. If “success” = {fail before t}, then p = P(success) = 
,  
t
e λ
−
−
1
and P(k successes among 10 trials) = 
 
k
t
t
e
e
k
k
−
−
−
−
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
10
)
(
1
10
λ
λ
 
c. 
P(exactly 5 fail) = P( 5 of λ’s fail and other 5 don’t) + P(4 of λ’s fail,  µ fails, and other 5 
don’t) = 
 
(
)
(
)
(
) (
)
5
4
4
5
)
(
1
1
4
9
)
(
1
5
9
t
t
t
t
t
t
e
e
e
e
e
e
λ
µ
λ
µ
λ
λ
−
−
−
−
−
−
−
−
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
−
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
 
 
180

Chapter 5:  Joint Probability Distributions and Random Samples 
15. 
 
a. F(y) = P( Y ≤ y ) = P [(X1 ≤y) ∪ ((X2 ≤ y) ∩ (X3 ≤ y))] 
= P (X1 ≤ y) + P[(X2 ≤ y) ∩ (X3 ≤ y)] - P[(X1 ≤ y) ∩ (X2 ≤ y) ∩ (X3 ≤ y)] 
=  
 for y ≥ 0 
3
2
)
1(
)
1(
)
1(
y
y
y
e
e
e
λ
λ
λ
−
−
−
−
−
−
+
−
 
 
b. f(y) = F′(y) = 
(
)
(
)
y
y
y
y
y
e
e
e
e
e
λ
λ
λ
λ
λ
λ
λ
λ
−
−
−
−
−
−
−
−
+
2)
1(3
)
1(
2
 
 =  
  for y ≥ 0 
y
y
e
e
λ
λ
λ
λ
3
2
3
4
−
−
−
 
E(Y) = 
(
)
λ
λ
λ
λ
λ
λ
λ
3
2
3
1
2
1
2
3
4
0
3
2
=
−
⎟
⎠
⎞
⎜
⎝
⎛
=
−
⋅
∫
∞
−
−
dy
e
e
y
y
y
 
 
 
16. 
 
a. f(x1, x3) = 
 
(
)
∫
∫
−
−
∞
∞
−
−
=
3
1
1
0
2
3
2
1
2
3
2
1
1
)
,
,
(
x
x
dx
x
x
kx
dx
x
x
x
f
(
)(
2
3
1
3
1
1
1
72
x
x
x
x
−
−
−
)
x
x
x
x
−
+
−
  0 ≤ x1, 0 ≤ x3, x1 + x3 ≤ 1 
 
b. P(X1 + X3 ≤ .5) = 
 
∫∫
−
−
−
−
5.
0
5.
0
1
2
2
3
1
3
1
1
)
1
)(
1(
72
x
dx
dx
x
x
x
x
 
 
 
 
= (after much algebra) .53125 
 
c. 
 
(
)(
)
∫
∫
−
−
−
=
=
∞
∞
−
3
2
3
1
3
1
3
3
1
1
1
1
72
)
,
(
)
(
1
dx
x
x
x
x
dx
x
x
f
x
f x
 
 
 
18
 
0 ≤ x
5
1
3
1
2
1
1
6
36
48
1 ≤ 1 
 
 
17. 
 
a. 
within a circle of radius 
(
)
,
(
Y
X
P
)
∫∫
=
=
A
R
dxdy
y
x
f
A
P
)
,
(
)
(
2
 
25
.
4
1
.
.
1
2
2
=
=
=
=
∫∫
R
A
of
area
dxdy
R
A
π
π
 
 
b.  
 
 
 
 
 
 
 
 
 
 
 
π
π
1
2
2
,
2
2
2
2
=
=
⎟
⎠
⎞
⎜
⎝
⎛
≤
≤
−
≤
≤
−
R
R
R
Y
R
R
X
R
P
 
 
 
181

Chapter 5:  Joint Probability Distributions and Random Samples 
 
c. 
 
 
 
 
 
 
 
 
 
 
 
π
π
2
2
2
2
,
2
2
2
2
=
=
⎟
⎠
⎞
⎜
⎝
⎛
≤
≤
−
≤
≤
−
R
R
R
Y
R
R
X
R
P
 
 
d. 
( )
2
2
2
2
2
1
)
,
(
2
2
2
2
R
x
R
dy
R
dy
y
x
f
x
f
x
R
x
R
x
π
π
−
=
=
=
∫
∫
−
−
−
∞
∞
−
  for –R ≤ x ≤ R  and 
similarly for fY(y).   X and Y are not independent since e.g. fx(.9R) = fY(.9R) > 0, yet 
f(.9R, .9R) = 0 since (.9R, .9R) is outside the  circle of radius R. 
 
 
18. 
 
a. Py|X(y|1) results from dividing each entry in x = 1 row of the joint probability table by 
px(1) = .34: 
2353
.
34
.
08
.
)1
|
0
(
|
=
=
x
y
P
 
5882
.
34
.
20
.
)1
|
1(
|
=
=
x
y
P
 
1765
.
34
.
06
.
)1
|
2
(
|
=
=
x
y
P
 
 
b. Py|X(x|2) is requested; to obtain this divide each entry in the y = 2 row by  
px(2) = .50: 
 
y 
0 
1 
2 
Py|X(y|2) 
.12 
.28 
.60 
 
c. 
P( Y ≤ 1 | x = 2) = Py|X(0|2) + Py|X(1|2) = .12 + .28 = .40 
 
d. PX|Y(x|2) results from dividing each entry in the y = 2 column by py(2) = .38: 
 
 
 
 
x 
0 
1 
2 
Px|y(x|2) 
.0526 
.1579 
.7895 
 
 
182

Chapter 5:  Joint Probability Distributions and Random Samples 
19. 
 
a. 
05
.
10
)
(
)
(
)
,
(
)
|
(
2
2
2
|
+
+
=
=
kx
y
x
k
x
f
y
x
f
x
y
f
X
X
Y
 
 
20 ≤ y ≤ 30 
05
.
10
)
(
)
|
(
2
2
2
|
+
+
=
ky
y
x
k
y
x
f
Y
X
 
20 ≤ x ≤ 30  
⎟
⎠
⎞
⎜
⎝
⎛
=
000
,
380
3
k
 
 
b. P( Y ≥ 25 | X = 22 ) = 
 
∫
30
25
|
)
22
|
(
dy
y
f
X
Y
 
 
 
 
     = ∫
=
+
+
30
25
2
2
2
783
.
05
.
)
22
(
10
)
)
22
((
dy
k
y
k
 
P( Y ≥ 25 ) = 
 
75
.
)
05
.
10
(
)
(
30
25
2
30
25
=
+
= ∫
∫
dy
ky
dy
y
fY
 
c. 
E( Y | X=22 ) = 
dy
k
y
k
y
dy
y
f
y
X
Y
05
.
)
22
(
10
)
)
22
((
)
22
|
(
2
2
2
30
20
|
+
+
⋅
=
⋅
∫
∫
∞
∞
−
 
 
 
 
= 25.372912 
E( Y2 | X=22 ) = 
028640
.
652
05
.
)
22
(
10
)
)
22
((
2
2
2
30
20
2
=
+
+
⋅
∫
dy
k
y
k
y
 
V(Y| X = 22 ) = E( Y2 | X=22 ) – [E( Y | X=22 )]2 = 8.243976 
 
 
20. 
 
a. 
(
)
)
,
(
)
,
,
(
,
|
2
1
,
3
2
1
2
1
3
,
|
2
1
2
1
3
x
x
f
x
x
x
f
x
x
x
f
x
x
x
x
x
=
  where 
=
)
,
(
2
1
, 2
1
x
x
f
x
x
 the marginal joint pdf 
of (X1, X2) = 
 
3
3
2
1
)
,
,
(
dx
x
x
x
f
∫
∞
∞
−
 
b. 
(
)
)
(
)
,
,
(
|
,
1
3
2
1
1
3
2
|
,
1
1
3
2
x
f
x
x
x
f
x
x
x
f
x
x
x
x
=
  where 
 
∫∫
∞
∞
−
∞
∞
−
=
3
2
3
2
1
1
)
,
,
(
)
(
1
dx
dx
x
x
x
f
x
f x
 
 
21. 
For every x and y, fY|X(y|x) = fy(y), since then f(x,y) = fY|X(y|x)  ⋅ fX(x) = fY(y)  ⋅ fX(x), as  
required. 
 
 
 
183

Chapter 5:  Joint Probability Distributions and Random Samples 
Section 5.2 
 
22. 
 
a. E( X + Y ) = 
 )
02
)(.
0
0
(
)
,
(
)
(
+
=
+
∑∑
x
y
y
x
p
y
x
10
.
14
)
01
)(.
15
10
(
...
)
06
)(.
5
0
(
=
+
+
+
+
+
 
 
b. E[max (X,Y)] = ∑∑
 
⋅
+
x
y
y
x
p
y
x
)
,
(
)
max(
60
.9
)
01
)(.
15
(
...
)
06
)(.
5
(
)
02
)(.
0
(
=
+
+
+
=
 
 
 
23. 
E(X1 – X2) = 
=  
(
)
∑∑
=
=
⋅
−
4
0
3
0
2
1
2
1
1
2
)
,
(
x
x
x
x
p
x
x
(0 – 0)(.08) + (0 – 1)(.07) + … + (4 – 3)(.06) = .15 
 
(which also equals E(X1) – E(X2) = 1.70 – 1.55) 
 
 
24. 
Let h(X,Y) = # of individuals who handle the message. 
 
 
 
 
y 
 
 
 
 
 
h(x,y) 
1 
2 
3 
4 
5 
6 
 
1 
- 
2 
3 
4 
3 
2 
 
2 
2 
- 
2 
3 
4 
3 
x 
3 
3 
2 
- 
2 
3 
4 
 
4 
4 
3 
2 
- 
2 
3 
 
5 
3 
4 
3 
2 
- 
2 
 
6 
2 
3 
4 
3 
2 
- 
 
Since p(x,y) = 30
1  for each possible (x,y), E[h(X,Y)] = 
80
.2
)
,
(
30
84
30
1
=
=
⋅
∑∑
x
y
y
x
h
 
 
 
25. 
E(XY) = E(X) ⋅ E(Y) = L ⋅ L = L2 
 
 
26. 
Revenue = 3X + 10Y, so E (revenue) = E (3X + 10Y) 
 
4.
15
)
2,5
(
35
...
)
0,0
(
0
)
,
(
)
10
3
(
5
0
2
0
=
⋅
+
+
⋅
=
⋅
+
= ∑∑
=
=
p
p
y
x
p
y
x
x
y
 
 
184

Chapter 5:  Joint Probability Distributions and Random Samples 
27. 
E[h(X,Y)] = 
(
)
∫∫
∫∫
⋅
−
=
⋅
−
1
0
0
2
1
0
1
0
2
6
2
6
x
ydydx
x
y
x
ydxdy
x
y
x
 
 
(
)
3
1
6
12
12
1
0
5
1
0
0
2
2
3
=
=
−
∫
∫∫
dx
x
dydx
y
x
y
x
x
 
 
 
28. 
E(XY) = 
∑
∑∑
∑
∑∑
⋅
=
⋅
⋅
=
⋅
y
y
x
y
x
x
x
y
y
x
y
yp
x
xp
y
p
x
p
xy
y
x
p
xy
)
(
)
(
)
(
)
(
)
,
(
 
= E(X) ⋅ E(Y).  (replace Σ with ∫in the continuous case) 
 
 
29. 
Cov(X,Y) = 
75
2
−
 and 
5
2
=
=
y
x
µ
µ
.   E(X2) = 
 
∫
⋅
1
0
2
)
(
dx
x
f
x
x
5
1
60
12
)
1(
12
1
0
2
3
=
=
−
= ∫
dx
x
x
, so Var (X) = 
25
1
25
4
5
1
=
−
 
Similarly, Var(Y) = 25
1 , so 
667
.
75
50
25
1
25
1
75
2
,
−
=
−
=
⋅
=
−
Y
X
ρ
 
 
 
30. 
 
a. E(X) = 5.55, E(Y) = 8.55, E(XY) = (0)(.02) + (0)(.06) + … + (150)(.01) = 44.25, so 
Cov(X,Y) = 44.25 – (5.55)(8.55) = -3.20 
 
b. 
, so 
15
.
19
,
45
.
12
2
2
=
=
Y
X
σ
σ
207
.
)
15
.
19
)(
45
.
12
(
20
.3
,
−
=
−
=
Y
X
ρ
 
 
 
31. 
 
a. E(X) = 
[
]
)
(
329
.
25
05
.
10
)
(
30
20
2
30
20
Y
E
dx
Kx
x
dx
x
xf x
=
=
+
= ∫
∫
 
E(XY) = 
 
447
.
641
)
(
30
20
30
20
2
2
=
+
⋅
∫∫
dxdy
y
x
K
xy
111
.
)
329
.
25
(
447
.
641
)
,
(
2
−
=
−
=
⇒
Y
X
Cov
 
 
b. E(X2) = 
,  
[
]
)
(
8246
.
649
05
.
10
2
30
20
2
2
Y
E
dx
Kx
x
=
=
+
∫
so Var (X) = Var(Y) = 649.8246 – (25.329)2 = 8.2664 
0134
.
)
2664
.8
)(
2664
.8
(
111
.
−
=
−
=
⇒ρ
 
 
 
 
185

Chapter 5:  Joint Probability Distributions and Random Samples 
32. 
There is a difficulty here.  Existence of ρ requires that both X and Y have finite means and 
variances.  Yet since the marginal pdf of Y is (
)
2
1
1
y
−
 for y ≥ 0, 
(
)
(
)
(
)
(
)
(
)
∫
∫
∫
∫
∞
∞
∞
∞
+
−
+
=
+
−
+
=
+
=
0
2
0
0
2
0
2
1
1
1
1
1
1
1
1
)
(
dy
y
dy
y
dy
y
y
dy
y
y
y
E
, and the 
first integral is not finite.  Thus ρ itself is undefined. 
 
 
33. 
Since E(XY) = E(X) ⋅ E(Y), Cov(X,Y) = E(XY) – E(X) ⋅ E(Y) = E(X) ⋅ E(Y) - E(X) ⋅ E(Y) = 
0, and since Corr(X,Y) = 
y
x
Y
X
Cov
σ
σ
)
,
(
, then Corr(X,Y) = 0 
 
 
34. 
 
a. In the discrete case, Var[h(X,Y)] = E{[h(X,Y) – E(h(X,Y))]2} = 
∑∑
∑∑
−
=
−
x
y
x
y
Y
X
h
E
y
x
p
y
x
h
y
x
p
Y
X
h
E
y
x
h
2
2
2
))]
,
(
(
[
)]
,
(
)
,
(
[
)
,
(
))]
,
(
(
)
,
(
[
 with 
replacing 
in the continuous case. 
∫∫
∑∑
 
b. E[h(X,Y)] = E[max(X,Y)] = 9.60, and E[h2(X,Y)] = E[(max(X,Y))2] = (0)2(.02) 
+(5)2(.06) + …+ (15)2(.01) = 105.5, so Var[max(X,Y)] = 105.5 – (9.60)2 = 13.34 
 
 
35. 
 
a. Cov(aX + b, cY + d) = E[(aX + b)(cY + d)] – E(aX + b) ⋅ E(cY + d) 
= E[acXY + adX + bcY + bd] – (aE(X) + b)(cE(Y) + d) 
= acE(XY) – acE(X)E(Y) = acCov(X,Y) 
 
b. Corr(aX + b, cY + d) = 
)
(
)
(
|
|
|
|
)
,
(
)
(
)
(
)
,
(
Y
Var
X
Var
c
a
Y
X
acCov
d
cY
Var
b
aX
Var
d
cY
b
aX
Cov
⋅
⋅
=
+
+
+
+
 
= Corr(X,Y) when a and c have the same signs. 
 
c.   When a and c differ in sign, Corr(aX + b, cY + d) = -Corr(X,Y). 
 
 
36. 
Cov(X,Y) = Cov(X, aX+b) = E[X⋅(aX+b)] – E(X) ⋅E(aX+b) = a Var(X),  
so Corr(X,Y) = 
)
(
)
(
)
(
)
(
)
(
)
(
2
X
Var
a
X
Var
X
aVar
Y
Var
X
Var
X
aVar
⋅
=
⋅
= 1 if a > 0, and –1 if a < 0 
 
 
 
186

Chapter 5:  Joint Probability Distributions and Random Samples 
Section 5.3 
 
37. 
 
 
 
P(x1) 
.20 
.50 
.30 
P(x2) 
x2 | x1
25 
40 
65 
.20 
25 
.04 
.10 
.06 
.50 
40 
.10 
.25 
.15 
.30 
65 
.06 
.15 
.09 
 
a.  
x  
25 
32.5 
40 
45 
52.5 
65 
( )
x
p
 
.04 
.20 
.25 
.12 
.30 
.09 
 
 
 
( )
µ
=
=
+
+
+
=
5.
44
)
09
(.
65
...
)
20
(.
5.
32
)
04
)(.
25
(
x
E
 
 
b.  
 
s2
0 
112.5 
312.5 
800 
P(s2) 
.38 
.20 
.30 
.12 
 
 
 
E(s2) = 212.25 = σ2
 
 
38. 
 
a. 
 
T0
0 
1 
2 
3 
4 
P(T0) 
.04 
.20 
.37 
.30 
.09 
 
b. 
µ
µ
⋅
=
=
=
2
2.2
)
(
0
0
T
E
T
 
 
c. 
 
2
2
2
0
2
0
2
2
98
.
)
2.2
(
82
.5
)
(
)
(
0
σ
σ
⋅
=
=
−
=
−
=
T
E
T
E
T
 
 
 
187

Chapter 5:  Joint Probability Distributions and Random Samples 
39. 
 
x 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
x/n 
0 
.1 
.2 
.3 
.4 
.5 
.6 
.7 
.8 
.9 
1.0 
p(x/n) 
.000 
.000 
.000 
.001 
.005 
.027 
.088 
.201 
.302 
.269 
.107 
 
X is a binomial random variable with p = .8. 
 
 
40. 
 
a. Possible values of M are: 0, 5, 10.   M = 0 when all 3 envelopes contain 0 money, hence 
p(M = 0) = (.5)3 = .125.   M = 10 when there is a single envelope with $10, hence p(M = 
10) = 1 – p(no envelopes with $10) = 1 – (.8)3 = .488.   
p(M = 5) = 1 – [.125 + .488] = .387. 
 
M 
0 
5 
10 
p(M) 
.125 
.387 
.488 
 
An alternative solution would be to list all 27 possible combinations using a tree diagram 
and computing probabilities directly from the tree. 
 
b. The statistic of interest is M, the maximum of x1, x2, or x3, so that M = 0, 5, or 10.  The 
population distribution is a s follows: 
x 
0 
5 
10 
p(x) 
1/2 
3/10 
1/5 
 
Write a computer program to generate the digits 0 – 9 from a uniform distribution.  
Assign a value of 0 to the digits 0 – 4, a value of 5 to digits 5 – 7, and a value of 10 to 
digits 8 and 9.  Generate samples of increasing sizes, keeping the number of replications 
constant and compute M from each sample.  As n, the sample size, increases, p(M = 0) 
goes to zero, p(M = 10) goes to one.  Furthermore, p(M = 5) goes to zero, but at a slower 
rate than p(M = 0). 
 
 
 
188

Chapter 5:  Joint Probability Distributions and Random Samples 
41. 
 
Outcome 
1,1 
1,2 
1,3 
1,4 
2,1 
2,2 
2,3 
2,4 
 
Probability 
.16 
.12 
.08 
.04 
.12 
.09 
.06 
.03 
 
x  
1 
1.5 
2 
2.5 
1.5 
2 
2.5 
3 
 
r 
0 
1 
2 
3 
1 
0 
1 
2 
 
 
Outcome 
3,1 
3,2 
3,3 
3,4 
4,1 
4,2 
4,3 
4,4 
 
Probability 
.08 
.06 
.04 
.02 
.04 
.03 
.02 
.01 
 
x  
2 
2.5 
3 
3.5 
2.5 
3 
3.5 
4 
 
r 
2 
1 
0 
1 
3 
2 
1 
2 
 
a.  
x  
1 
1.5 
2 
2.5 
3 
3.5 
4 
( )
x
p
 
.16 
.24 
.25 
.20 
.10 
.04 
.01 
 
 
b. P(
)
5.2
≤
x
= .8 
 
c. 
 
r 
0 
1 
2 
3 
p(r) 
.30 
.40 
.22 
.08 
 
 
d. 
)
5.1
(
≤
X
P
= P(1,1,1,1) + P(2,1,1,1) + … + P(1,1,1,2) + P(1,1,2,2) + … + P(2,2,1,1) + 
P(3,1,1,1) + … + P(1,1,1,3)  
 
= (.4)4 + 4(.4)3(.3) + 6(.4)2(.3)2 + 4(.4)2(.2)2  = .2400 
 
 
42. 
 
a.  
x  
27.75 
28.0 
29.7 
29.95 
31.65 
31.9 
33.6 
( )
x
p
 
30
4  
30
2  
30
6  
30
4  
30
8  
30
4  
30
2  
 
b.  
x  
27.75 
31.65 
31.9 
( )
x
p
 
3
1  
3
1  
3
1  
 
 
c. 
all three values are the same:  30.4333 
 
 
 
189

Chapter 5:  Joint Probability Distributions and Random Samples 
43. 
The statistic of interest is the fourth spread, or the difference between the medians of the 
upper and lower halves of the data.  The population distribution is uniform with A = 8 and B 
= 10.   Use a computer to generate samples of sizes n = 5, 10, 20, and 30 from a uniform 
distribution with A = 8 and B = 10.  Keep the number of replications the same (say 500, for 
example).  For each sample, compute the upper and lower fourth, then compute the 
difference.  Plot the sampling distributions on separate histograms for n = 5, 10, 20, and 30. 
 
 
44. 
Use a computer to generate samples of sizes n = 5, 10, 20, and 30 from a Weibull distribution 
with parameters as given, keeping the number of replications the same, as in problem 43 
above.  For each sample, calculate the mean.  Below is a histogram, and a normal probability 
plot for the sampling distribution of x  for n = 5, both generated by Minitab.  This sampling 
distribution appears to be normal, so since larger sample sizes will produce distributions that 
are closer to normal, the others will also appear normal.  
 
 
45. 
Using Minitab to generate the necessary sampling distribution, we can see that as n increases, 
the distribution slowly moves toward normality.  However, even the sampling distribution for 
n = 50 is not yet approximately normal.  
n = 10 
 
n = 50 
0
10
20
30
40
50
60
70
80
90
0
10
20
30
40
50
60
70
80
90
Frequency
P-Value:   0.000
A-Squared: 7.406
Anderson-Darling Normality Test
85
75
65
55
45
35
25
15
5
.999
.99
.95
.80
.50
.20
.05
.01
.001
Probability
n=10
Normal Probability Plot
P-Value:   0.000
A-Squared: 4.428
Anderson-Darling Normality Test
60
50
40
30
20
.999
.99
.95
.80
.50
.20
.05
.01
.001
Probability
Normal Probability Plot
65
55
45
35
25
15
70
60
50
40
30
20
10
0
Frequency
 
190

Chapter 5:  Joint Probability Distributions and Random Samples 
Section 5.4 
 
46. 
µ = 12 cm 
σ = .04 cm 
a. n = 16  
cm
X
E
12
)
(
=
= µ
   
cm
n
x
x
01
.
4
04
.
=
=
= σ
σ
 
 
b. n = 64   
cm
X
E
12
)
(
=
= µ
   
cm
n
x
x
005
.
8
04
.
=
=
= σ
σ
 
 
c. 
X is more likely to be within .01 cm of the mean (12 cm) with the second, larger, 
sample.  This is due to the decreased variability of  X  with a larger sample size. 
 
 
47. 
µ = 12 cm 
σ = .04 cm 
a. n = 16   P( 11.99 ≤ X  ≤ 12.01) = 
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
≤
−
01
.
12
01
.
12
01
.
12
99
.
11
Z
P
 
 
 
 
 
 
= P(-1 ≤ Z ≤ 1) 
 
 
 
 
 
= Φ(1) - Φ(-1)  
 
 
 
 
 
=.8413 - .1587 
 
 
 
 
 
=.6826 
 
b. n = 25    P( X  > 12.01) = 
⎟
⎠
⎞
⎜
⎝
⎛
−
>
5
/
04
.
12
01
.
12
Z
P
= P( Z > 1.25) 
 
 
 
 
 
 
 
= 1 - Φ(1.25) 
 
 
 
 
 
 
 
= 1 - .8944 
 
 
 
 
 
 
 
=.1056 
 
48. 
 
a. 
50
=
= µ
µ X
, 
10
.
100
1
=
=
=
n
x
x
σ
σ
 
P( 49.75 ≤ X  ≤ 50.25) = 
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
≤
−
10
.
50
25
.
50
10
.
50
75
.
49
Z
P
 
 
 
 
 
 
= P(-2.5 ≤ Z ≤ 2.5) = .9876 
 
b. P( 49.75 ≤ X  ≤ 50.25) ≈ 
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
≤
−
10
.
8.
49
25
.
50
10
.
8.
49
75
.
49
Z
P
 
 
 
 
 
 
= P(-.5 ≤ Z ≤ 4.5) = .6915 
 
 
191

Chapter 5:  Joint Probability Distributions and Random Samples 
49. 
 
a. 11 P.M. – 6:50 P.M. = 250 minutes.  With T0 = X1 + … + X40 = total grading time, 
240
)
6
)(
40
(
0
=
=
= µ
µ
n
T
 and 
,
95
.
37
0
=
=
n
T
σ
σ
 so P( T0 ≤ 250) ≈ 
(
)
6026
.
26
.
95
.
37
240
250
=
≤
=
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
Z
P
Z
P
 
 
b. 
(
)
(
)
2981
.
53
.
95
.
37
240
260
260
0
=
>
=
⎟
⎠
⎞
⎜
⎝
⎛
−
>
=
>
Z
P
Z
P
T
P
 
 
 
50. 
µ = 10,000 psi 
 
σ = 500 psi 
a. n = 40 
P( 9,900 ≤ X  ≤ 10,200) ≈ 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
≤
≤
−
40
/
500
000
,
10
200
,
10
40
/
500
000
,
10
900
,9
Z
P
 
 
 
 
 
 
= P(-1.26 ≤ Z ≤ 2.53)  
 
 
 
 
 
= Φ(2.53) - Φ(-1.26)  
 
 
 
 
 
= .9943 - .1038 
= .8905 
b. According to the Rule of Thumb given in Section 5.4, n should be greater than 30 in 
order to apply the C.L.T., thus using the same procedure for n = 15 as was used for n =   
40 would not be appropriate. 
 
 
51. 
X ~ N(10,4).  For day 1, n = 5 
P( X  ≤ 11)=
8686
.
)
12
.1
(
5
/
2
10
11
=
≤
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
≤
Z
P
Z
P
 
 
 
 
 
For day 2, n = 6 
 
P( X  ≤ 11)=
8888
.
)
22
.1
(
6
/
2
10
11
=
≤
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
≤
Z
P
Z
P
 
 
For both days, 
 
P( X  ≤ 11)= (.8686)(.8888) = .7720 
 
 
52. 
X ~ N(10), n =4 
40
)
10
)(
4
(
0
=
=
= µ
µ
n
T
 and 
,2
)1
)(
2
(
0
=
=
=
n
T
σ
σ
  
We desire the 95th percentile:  40 + (1.645)(2) = 43.29 
 
 
 
192

Chapter 5:  Joint Probability Distributions and Random Samples 
53. 
µ = 50, σ = 1.2  
a. n = 9 
P( X ≥ 51) =
0062
.
9938
.
1
)
5.2
(
9
/
2.1
50
51
=
−
=
≥
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
≥
Z
P
Z
P
 
 
b. n = 40 
P( X ≥ 51) =
0
)
27
.5
(
40
/
2.1
50
51
≈
≥
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
≥
Z
P
Z
P
 
 
 
54. 
 
a. 
65
.2
=
= µ
µ X
, 
17
.
5
85
.
=
=
=
n
x
x
σ
σ
 
P( X  ≤ 3.00)=
9803
.
)
06
.2
(
17
.
65
.2
00
.3
=
≤
=
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
Z
P
Z
P
 
P(2.65 ≤ X  ≤ 3.00)=
4803
.
)
65
.2
(
)
00
.3
(
=
≤
−
≤
=
X
P
X
P
 
 
b. P( X  ≤ 3.00)=
99
.
/
85
.
65
.2
00
.3
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
≤
n
Z
P
 implies that 
,
33
.2
/
85
35
.
=
n
 from 
which n = 32.02.  Thus n = 33 will suffice. 
 
 
55. 
20
=
= np
µ
 
464
.3
=
=
npq
σ
 
a. P( 25 ≤ X ) ≈
0968
.
)
30
.1(
464
.3
20
5.
24
=
≤
=
⎟
⎠
⎞
⎜
⎝
⎛
≤
−
Z
P
Z
P
 
 
b. P( 15 ≤ X ≤ 25) ≈
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
≤
−
464
.3
20
5.
25
464
.3
20
5.
14
Z
P
 
8882
.
)
59
.1
59
.1
(
=
≤
≤
−
=
Z
P
 
 
56. 
 
a. With Y = # of tickets, Y has approximately a normal distribution with 
50
=
= λ
µ
, 
071
.7
=
=
λ
σ
, so P( 35 ≤ Y ≤ 70) ≈
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
≤
−
071
.7
50
5.
70
071
.7
50
5.
34
Z
P
 = P( -2.19 
≤ Z ≤ 2.90) = .9838 
 
b. Here 
250
=
µ
, 
, so P( 225 ≤ Y ≤ 275) ≈ 
811
.
15
,
250
2
=
=
σ
σ
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
≤
−
811
.
15
250
5.
275
811
.
15
250
5.
224
Z
P
 = P( -1.61 ≤ Z ≤ 1.61) = .8926 
 
 
193

Chapter 5:  Joint Probability Distributions and Random Samples 
57. 
E(X) = 100, Var(X) = 200, 
14
.
14
=
x
σ
, so P( X ≤ 125) ≈ 
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
14
.
14
100
125
Z
P
 
= P( Z ≤ 1.77) = .9616 
 
 
Section 5.5 
 
58. 
 
a. E( 27X1 + 125X2 + 512X3 ) = 27 E(X1) + 125 E(X2) + 512 E(X3)  
= 27(200) + 125(250) + 512(100) = 87,850 
V(27X1 + 125X2 + 512X3) = 272 V(X1) + 1252 V(X2) + 5122 V(X3) 
 
 
 
 
= 272 (10)2 + 1252 (12)2 + 5122 (8)2 = 19,100,116 
 
b. The expected value is still correct, but the variance is not because the covariances now 
also contribute to the variance. 
 
 
59. 
 
a. E( X1 + X2 + X3 ) = 180, V(X1 + X2 + X3) = 45, 
708
.6
3
2
1
=
+
+
x
x
x
σ
 
P(X1 + X2 + X3 ≤ 200) = 
9986
.
)
98
.2
(
708
.6
180
200
=
≤
=
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
Z
P
Z
P
 
P(150 ≤ X1 + X2 + X3 ≤ 200) = 
9986
.
)
98
.2
47
.4
(
≈
≤
≤
−
Z
P
 
 
 
b. 
60
=
= µ
µ X
, 
236
.2
3
15 =
=
=
n
x
x
σ
σ
 
9875
.
)
236
.2
(
236
.2
60
55
)
55
(
=
−
≥
=
⎟
⎠
⎞
⎜
⎝
⎛
−
≥
=
≥
Z
P
Z
P
X
P
 
(
)
6266
.
89
.
89
.
)
62
58
(
=
≤
≤
−
=
≤
≤
Z
P
X
P
 
 
c. 
E( X1 - .5X2 -.5X3 ) = 0;  
V( X1 - .5X2 -.5X3 ) = 
sd = 4.7434 
,5.
22
25
.
25
.
2
3
2
2
2
1
=
+
+
σ
σ
σ
P(-10 ≤ X1 - .5X2 -.5X3 ≤ 5) =
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
≤
−
−
7434
.4
0
5
7434
.4
0
10
Z
P
 
(
05
.1
11
.2
≤
≤
−
=
Z
P
)  =  .8531 - .0174 = .8357 
 
 
194

Chapter 5:  Joint Probability Distributions and Random Samples 
d. E( X1 + X2 + X3 ) = 150,  V(X1 + X2 + X3) = 36, 
6
3
2
1
=
+
+
x
x
x
σ
 
P(X1 + X2 + X3 ≤ 200) = 
9525
.
)
67
.1
(
6
150
160
=
≤
=
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
Z
P
Z
P
 
We want P( X1 + X2 ≥ 2X3), or written another way,  P( X1 + X2 - 2X3 ≥ 0)  
E( X1 + X2 - 2X3 ) = 40 + 50 – 2(60) = -30,   
V(X1 + X2 - 2X3) = 
36, sd = 8.832, so  
,
78
4
2
3
2
2
2
1
=
+
+
σ
σ
σ
P( X1 + X2 - 2X3 ≥ 0) = 
0003
.
)
40
.3
(
832
.8
)
30
(
0
=
≥
=
⎟
⎠
⎞
⎜
⎝
⎛
−
−
≥
Z
P
Z
P
 
 
 
60. 
Y is normally distributed with 
(
)
(
)
1
3
1
2
1
5
4
3
2
1
−
=
+
+
−
+
=
µ
µ
µ
µ
µ
µY
, and 
7795
.1
,
167
.3
9
1
9
1
9
1
4
1
4
1
2
5
2
4
2
3
2
2
2
1
2
=
=
+
+
+
+
=
Y
Y
σ
σ
σ
σ
σ
σ
σ
. 
Thus, (
)
2877
.
)
56
(.
7795
.1
)1
(
0
0
=
≤
=
⎟
⎠
⎞
⎜
⎝
⎛
≤
−
−
=
≤
Z
P
Z
P
Y
P
  and  
(
)
3686
.
)
12
.1
0
(
7795
.1
2
0
1
1
=
≤
≤
=
⎟
⎠
⎞
⎜
⎝
⎛
≤
≤
=
≤
≤
−
Z
P
Z
P
Y
P
 
 
 
61. 
 
a. The marginal pmf’s of X and Y are given in the solution to Exercise 7, from which E(X) 
= 2.8, E(Y) = .7, V(X) = 1.66, V(Y) = .61.  Thus E(X+Y) = E(X) + E(Y) = 3.5, V(X+Y) 
= V(X) + V(Y) = 2.27, and the standard deviation of X + Y is 1.51 
 
b. E(3X+10Y) = 3E(X) + 10E(Y) = 15.4, V(3X+10Y) = 9V(X) + 100V(Y) = 75.94, and the 
standard deviation of revenue is 8.71 
 
 
62. 
E( X1 + X2 + X3 ) = E( X1) + E(X2 ) + E(X3 ) = 15 + 30 + 20 = 65 min., 
 V(X1 + X2 + X3) = 12 + 22 + 1.52 = 7.25, 
6926
.2
25
.7
3
2
1
=
=
+
+
x
x
x
σ
 
 
Thus, P(X1 + X2 + X3 ≤ 60) = 
0314
.
)
86
.1
(
6926
.2
65
60
=
−
≤
=
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
Z
P
Z
P
 
 
 
63. 
 
a. E(X1) = 1.70, E(X2) = 1.55, E(X1X2) = 
33
.3
)
,
(
1
2
2
1
2
1
=
∑∑
x
x
x
x
p
x
x
, so Cov(X1,X2) = 
E(X1X2) - E(X1) E(X2) = 3.33 – 2.635 = .695 
 
b. V(X1 + X2) = V(X1) +  V(X2) + 2 Cov(X1,X2)  
= 1.59 + 1.0875 + 2(.695) = 4.0675 
 
 
195

Chapter 5:  Joint Probability Distributions and Random Samples 
64. 
Let X1, …, X5 denote morning times and X6, …, X10 denote evening times. 
a. E(X1 + …+ X10) = E(X1) + … + E(X10) = 5 E(X1) + 5 E(X6)  
= 5(4) + 5(5) = 45 
 
b. Var(X1 + …+ X10) = Var(X1) + … + Var(X10) = 5 Var(X1) + 5Var(X6) 
33
.
68
12
820
12
100
12
64
5
=
=
⎥⎦
⎤
⎢⎣
⎡
+
=
 
 
c. 
E(X1 – X6) = E(X1) - E(X6) = 4 – 5 = -1 
Var(X1 – X6) = Var(X1) + Var(X6) = 
67
.
13
12
164
12
100
12
64
=
=
+
 
 
d. E[(X1 + … + X5) – (X6 + … + X10)] = 5(4) – 5(5) = -5 
Var[(X1 + … + X5) – (X6 + … + X10)]  
= Var(X1 + … + X5) + Var(X6 + … + X10)] = 68.33 
 
 
65. 
µ = 5.00, σ = .2 
a. 
;0
)
(
=
−Y
X
E
 
0032
.
25
25
)
(
2
2
=
+
=
−
σ
σ
Y
X
V
, 
0566
.
=
−Y
X
σ
 
 
(
)
(
)
9232
.
77
.1
77
.1
1.
1.
=
≤
≤
−
≈
≤
−
≤
−
⇒
Z
P
Y
X
P
 (by the CLT) 
 
b. 
0022222
.
36
36
)
(
2
2
=
+
=
−
σ
σ
Y
X
V
, 
0471
.
=
−Y
X
σ
 
(
)
(
)
9660
.
12
.2
12
.2
1.
1.
=
≤
≤
−
≈
≤
−
≤
−
⇒
Z
P
Y
X
P
 
 
 
66. 
 
a. With M = 5X1 + 10X2, E(M) = 5(2) + 10(4) = 50,  
Var(M) = 52 (.5)2 + 102 (1)2 = 106.25, σM = 10.308. 
 
b. P( 75 < M ) = 
0075
.
)
43
.2
(
308
.
10
50
75
=
<
=
⎟
⎠
⎞
⎜
⎝
⎛
<
−
Z
P
Z
P
 
 
c. 
M = A1X1 + A2X2 with the AI’s and XI’s all independent, so  
E(M) = E(A1X1) + E(A2X2) = E(A1)E(X1) + E(A2)E(X2) = 50 
 
d. Var(M) = E(M2) – [E(M)]2.   Recall that for any r.v. Y,  
E(Y2) = Var(Y) + [E(Y)]2.  Thus, E(M2) = (
)
2
2
2
2
2
2
1
1
2
1
2
1
2
X
A
X
A
X
A
X
A
E
+
+
 
(
) (
)
(
) (
) (
) (
)
(
) (
)
2
2
2
2
2
2
1
1
2
1
2
1
2
X
E
A
E
X
E
A
E
X
E
A
E
X
E
A
E
+
+
=
 
(by independence) 
= (.25 + 25)(.25 + 4) + 2(5)(2)(10)(4) + (.25 + 100)(1 + 16) = 2611.5625, so Var(M) = 
2611.5625 – (50)2 = 111.5625 
 
 
196

Chapter 5:  Joint Probability Distributions and Random Samples 
e. 
E(M) = 50 still, but now 
 )
(
)
,
(
2
)
(
)
(
2
2
2
2
1
2
1
1
2
1
X
Var
a
X
X
Cov
a
a
X
Var
a
M
Var
+
+
=
 
= 6.25 + 2(5)(10)(-.25) + 100 = 81.25 
 
 
67. 
Letting X1, X2, and X3 denote the lengths of the three pieces, the total length is  
X1 + X2 - X3.   This has a normal distribution with mean value 20 + 15 – 1 = 34, variance 
.25+.16+.01 = .42, and standard deviation .6481.  Standardizing gives  
P(34.5  ≤ X1 + X2 - X3 ≤ 35) = P(.77 ≤ Z ≤ 1.54) = .1588 
 
 
68. 
Let X1 and X2 denote the (constant) speeds of the two planes. 
a. After two hours, the planes have traveled 2X1 km. and 2X2  km., respectively, so the  
second will not have caught the first if 2X1 + 10 > 2X2, i.e. if X2 – X1 < 5. X2 – X1 has a 
mean 500 – 520 = -20, variance 100 + 100 = 200, and standard deviation 14.14.  Thus, 
.
9616
.
)
77
.1
(
14
.
14
)
20
(
5
)
5
(
1
2
=
<
=
⎟
⎠
⎞
⎜
⎝
⎛
−
−
<
=
<
−
Z
P
Z
P
X
X
P
 
 
b. After two hours, #1 will be 10 + 2X1 km from where #2 started, whereas #2 will be 2X2 
from where it started.  Thus the separation distance will be al most 10 if  |2X2 – 10 – 2X1| 
≤ 10, i.e. –10 ≤ 2X2 – 10 – 2X1 ≤ 10,  
i.e. 0 ≤ X2 – X1 ≤ 10.  The corresponding probability is  
P(0 ≤ X2 – X1 ≤ 10) = P(1.41 ≤ Z ≤ 2.12) = .9830 - .9207 = .0623. 
 
 
69. 
 
a. E(X1 + X2 + X3) = 800 + 1000 + 600 = 2400. 
 
b. Assuming independence of X1, X2 , X3, Var(X1 + X2 + X3)  
= (16)2 + (25)2 + (18)2 = 12.05 
 
c. 
E(X1 + X2 + X3) =  2400 as before, but now Var(X1 + X2 + X3)  
= Var(X1) + Var(X2) + Var(X3) + 2Cov(X1,X2) + 2Cov(X1, X3) + 2Cov(X2, X3) = 1745, 
with sd = 41.77 
 
 
70. 
 
a. 
 so 
,5.
)
(
=
iY
E
4
)1
(
5.
)
(
)
(
1
1
+
=
=
⋅
=
∑
∑
=
=
n
n
i
Y
E
i
W
E
n
i
n
i
i
 
 
b. 
 so 
,
25
.
)
(
=
iY
Var
24
)1
2
)(
1
(
25
.
)
(
)
(
1
2
1
2
+
+
=
=
⋅
=
∑
∑
=
=
n
n
n
i
Y
Var
i
W
Var
n
i
n
i
i
 
 
 
 
197

Chapter 5:  Joint Probability Distributions and Random Samples 
71. 
 
a. 
 so 
,
72
2
2
1
1
12
0
2
2
1
1
W
X
a
X
a
xdx
W
X
a
X
a
M
+
+
=
+
+
=
∫
E(M) = (5)(2) + (10)(4) + (72)(1.5) = 158m 
( ) ( )
(
) ( )
(
) (
)
25
.
430
25
.
72
1
10
5.
5
2
2
2
2
2
2
2
=
+
+
=
M
σ
, 
74
.
20
=
M
σ
 
 
b. 
9788
.
)
03
.2
(
74
.
20
158
200
)
200
(
=
≤
=
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
=
≤
Z
P
Z
P
M
P
 
 
 
72. 
The total elapsed time between leaving and returning is To = X1 + X2 + X3 + X4, with 
 
, 
,
40
)
(
=
o
T
E
40
2 =
o
T
σ
477
.5
=
o
T
σ
. To  is normally distributed, and the desired value t 
is the 99th percentile of the lapsed time distribution added to 10 A.M.:  10:00 + 
[40+(5.477)(2.33)] = 10:52.76 
 
 
73. 
 
a. Both approximately normal by the C.L.T. 
 
b. The difference of two r.v.’s is just a special linear combination, and a linear combination 
of normal r.v’s has a normal distribution, so 
Y
X −
 has approximately a normal 
distribution with 
5
=
−Y
X
µ
 and 
621
.1
,
629
.2
35
6
40
8
2
2
2
=
=
+
=
−
−
Y
X
Y
X
σ
σ
 
 
c. 
(
)
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
≤
−
−
≈
≤
−
≤
−
6213
.1
5
1
6213
.1
5
1
1
1
Z
P
Y
X
P
&
0068
.
)
47
.2
70
.3
(
≈
−
≤
≤
−
=
Z
P
 
 
d. 
(
)
.
0010
.
)
08
.3
(
6213
.1
5
10
10
=
≥
=
⎟
⎠
⎞
⎜
⎝
⎛
−
≥
≈
≥
−
Z
P
Z
P
Y
X
P
&
  This probability is 
quite small, so such an occurrence is unlikely if 
5
2
1
=
−µ
µ
, and we would thus doubt 
this claim. 
 
 
74. 
X is approximately normal with 
35
)
7
)(.
50
(
1
=
=
µ
 and 
, as 
is Y with 
5.
10
)3
)(.
7
)(.
50
(
2
1
=
=
σ
30
2 =
µ
 and 
.  Thus 
12
2
2 =
σ
5
=
−Y
X
µ
 and 
, so  
5.
22
2
=
−Y
X
σ
(
)
4826
.
)
0
11
.2
(
74
.4
0
74
.4
10
5
5
=
≤
≤
−
=
⎟
⎠
⎞
⎜
⎝
⎛
≤
≤
−
≈
≤
−
≤
−
Z
P
Z
P
Y
X
p
 
 
 
198

Chapter 5:  Joint Probability Distributions and Random Samples 
Supplementary Exercises 
 
75. 
 
a. pX(x) is obtained by adding joint probabilities across the row labeled x, resulting in pX(x) 
= .2, .5, .3 for x = 12, 15, 20 respectively.  Similarly, from column sums py(y) = .1, .35, 
.55 for y = 12, 15, 20 respectively. 
 
b. P(X ≤ 15 and Y ≤ 15) = p(12,12) + p(12,15) + p(15,12) + p(15,15) = .25 
 
c. 
px(12) ⋅ py(12) = (.2)(.1) ≠ .05 = p(12,12), so X and Y are not independent. (Almost any 
other (x,y) pair yields the same conclusion). 
 
d. 
 (or =  E(X) + E(Y) = 33.35) 
35
.
33
)
,
(
)
(
)
(
=
+
=
+
∑∑
y
x
p
y
x
Y
X
E
 
e. 
85
.3
)
,
(
)
(
=
+
=
−
∑∑
y
x
p
y
x
Y
X
E
 
 
 
76. 
The roll-up procedure is not valid for the 75th percentile unless 
0
1 =
σ
 or 
0
2 =
σ
 or both 
1
σ  and 
0
2 =
σ
, as described below. 
Sum of percentiles: 
)
)(
(
)
(
)
(
2
1
2
1
2
2
1
1
σ
σ
µ
µ
σ
µ
σ
µ
+
+
+
=
+
+
+
Z
Z
Z
 
Percentile of sums: 
2
2
2
1
2
1
)
(
σ
σ
µ
µ
+
+
+
Z
 
These are equal when Z = 0 (i.e. for the median) or in the unusual case when 
2
2
2
1
2
1
σ
σ
σ
σ
+
=
+
, which happens when 
0
1 =
σ
 or 
0
2 =
σ
 or both 
1
σ  and 
0
2 =
σ
. 
 
77. 
 
 
 
 
 
 
 
 
a. 
 
∫∫
∫∫
∫∫
−
−
−
∞
∞
−
∞
∞
−
+
=
=
30
20
30
0
20
0
30
20
)
,
(
1
x
x
x
kxydydx
kxydydx
dxdy
y
x
f
250
,
81
3
3
250
,
81
=
⇒
⋅
=
k
k
 
 
30
=
+ y
x
20
=
+ y
x
b. 
⎪⎩
⎪⎨
⎧
+
−
=
−
=
=
∫
∫
−
−
−
)
30
450
(
)
10
250
(
)
(
3
2
1
2
30
0
2
30
20
x
x
x
k
kxydy
x
x
k
kxydy
x
f
x
x
x
X
 
30
20
20
0
≤
≤
≤
≤
x
x
 
 
and by symmetry fY(y) is obtained by substituting y for x in fX(x).  Since fX(25) > 0, and 
fY(25) > 0, but f(25, 25) = 0 , fX(x) ⋅ fY(y) ≠ f(x,y) for all x,y  so X and Y are not 
independent. 
 
199

Chapter 5:  Joint Probability Distributions and Random Samples 
 
c. 
 
∫∫
∫∫
−
−
−
+
=
≤
+
25
20
25
0
20
0
25
20
)
25
(
x
x
x
kxydydx
kxydydx
Y
X
P
 
 
 
 
355
.
24
625
,
230
250
,
81
3
=
⋅
=
 
 
d. 
(
)
{∫
−
⋅
=
+
=
+
20
0
2
10
250
2
)
(
)
(
)
(
dx
x
x
k
x
Y
E
X
E
Y
X
E
 
 
(
) }
∫
+
−
⋅
+
30
20
3
2
1
2
30
450
dx
x
x
x
k
x
    
969
.
25
)
67
.
666
,
351
(
2
=
= k
 
 
e. 
 
∫∫
∫∫
−
−
∞
∞
−
∞
∞
−
=
⋅
=
20
0
30
20
2
2
)
,
(
)
(
x
x
dydx
y
kx
dxdy
y
x
f
xy
XY
E
 
 
 
4103
.
136
3
000
,
250
,
33
3
30
20
30
0
2
2
=
⋅
=
+ ∫∫
−
k
dydx
y
kx
x
, so 
 
Cov(X,Y) = 136.4103 – (12.9845)2 = -32.19, and E(X2) = E(Y2) = 204.6154, so 
and 
0182
.
36
)
9845
.
12
(
6154
.
204
2
2
2
=
−
=
=
y
x
σ
σ
894
.
0182
.
36
19
.
32
−
=
−
=
ρ
 
 
f. 
Var (X + Y) = Var(X) + Var(Y) + 2Cov(X,Y) = 7.66 
 
 
78. 
FY(y) = P( max(X1, …, Xn) ≤ y) = P( X1 ≤ y, …, Xn ≤ y) = [P(X1 ≤ y)]n 
n
y
⎟
⎠
⎞
⎜
⎝
⎛
−
=
100
100
 for 
100 ≤ y ≤ 200.   
Thus fY(y) = 
(
1
100
100
−
−
n
n y
n
)
 for 100 ≤ y ≤ 200. 
(
)
(
)
∫
∫
−
−
+
=
−
⋅
=
100
0
1
200
100
1
100
100
100
100
)
(
du
u
u
n
dy
y
n
y
Y
E
n
n
n
n
 
100
1
1
2
1
100
100
100
100
100
0
⋅
+
+
=
+
+
=
+
=
∫
n
n
n
n
du
u
n
n
n
 
 
 
79. 
3400
2000
900
500
)
(
=
+
+
=
+
+
Z
Y
X
E
 
014
.
123
365
180
365
100
365
50
)
(
2
2
2
=
+
+
=
+
+
Z
Y
X
Var
, and the std dev = 11.09. 
1
)
0.9
(
)
3500
(
≈
≤
=
≤
+
+
Z
P
Z
Y
X
P
 
 
 
 
200

Chapter 5:  Joint Probability Distributions and Random Samples 
80. 
 
a. Let X1, …, X12 denote the weights for the business-class passengers and Y1, …, Y50 
denote the tourist-class weights.  Then T = total weight  
= X1 + … + X12 + Y1 + … + Y50 = X + Y 
E(X) = 12E(X1) = 12(30) = 360; V(X) = 12V(X1) = 12(36) = 432. 
E(Y) = 50E(Y1) = 50(40) = 2000; V(Y) = 50V(Y1) = 50(100) = 5000. 
Thus E(T) = E(X) + E(Y) = 360 + 2000 = 2360 
And V(T) = V(X) + V(Y) = 432 + 5000 = 5432, std dev = 73.7021 
 
b. 
(
)
9713
.
90
.1
7021
.
73
2360
2500
)
2500
(
=
≤
=
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
=
≤
Z
P
Z
P
T
P
 
 
 
81. 
 
a. E(N) ⋅ µ = (10)(40) = 400 minutes 
 
b. We expect 20 components to come in for repair during a 4 hour period,  
so E(N) ⋅ µ = (20)(3.5) = 70 
 
 
82. 
X ~ Bin ( 200, .45) and Y ~ Bin (300, .6).  Because both n’s are large, both X and Y are 
approximately normal, so X + Y is approximately normal with mean (200)(.45) + (300)(.6) = 
270, variance 200(.45)(.55) + 300(.6)(.4) = 121.40, and standard deviation 11.02.  Thus, P(X 
+ Y ≥ 250) 
(
)
9686
.
86
.1
02
.
11
270
5.
249
=
−
≥
=
⎟
⎠
⎞
⎜
⎝
⎛
−
≥
=
Z
P
Z
P
 
 
 
83. 
0.95 = 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
≤
≤
−
=
+
≤
≤
−
n
Z
n
P
X
P
/
01
.
02
.
/
01
.
02
.
)
02
.
02
.
(
&
µ
µ
 
= (
),
2.
2.
n
Z
n
P
≤
≤
−
 but (
)
95
.
96
.1
96
.1
=
≤
≤
−
Z
P
 so 
.
97
96
.1
2.
=
⇒
=
n
n
 The C.L.T. 
 
 
84. 
I have 192 oz.  The amount which  I would consume if there were no limit is To = X1 + …+ 
X14 where each XI is normally distributed with µ = 13 and σ = 2.  Thus To is normal with 
182
=
o
T
µ
 and 
483
.7
=
o
T
σ
, so P(To < 192) = P(Z < 1.34) = .9099. 
 
 
85. 
The expected value and standard deviation of volume are 87,850 and 4370.37, respectively, so 
9973
.
)
78
.2
(
37
.
4370
850
,
87
000
,
100
)
000
,
100
(
=
≤
=
⎟
⎠
⎞
⎜
⎝
⎛
−
≤
=
≤
Z
P
Z
P
volume
P
 
 
86. 
The student will not be late if X1 + X3 ≤ X2 , i.e. if X1 – X2 + X3 ≤ 0.  This linear combination 
has mean –2, variance 4.25, and standard deviation 2.06, so 
8340
.
)
97
.
(
06
.2
)
2
(
0
)
0
(
3
2
1
=
≤
=
⎟
⎠
⎞
⎜
⎝
⎛
−
−
≤
=
≤
+
−
Z
P
Z
P
X
X
X
P
 
 
201

Chapter 5:  Joint Probability Distributions and Random Samples 
 
 
87. 
 
a. 
. 
2
)
,
(
2
)
(
2
2
2
2
2
2
y
Y
X
x
y
x
a
a
Y
X
aCov
a
Y
aX
Var
σ
ρ
σ
σ
σ
σ
σ
+
+
=
+
+
=
+
Substituting  
X
Y
a
σ
σ
=
 yields 
(
)
0
1
2
2
2
2
2
2
≥
−
=
+
+
ρ
σ
σ
ρ
σ
σ
Y
Y
Y
Y
, so 
1
−
≥
ρ
 
 
b. Same argument as in a 
 
c. 
Suppose 
1
=
ρ
.  Then 
(
)
(
)
0
1
2
2
=
−
=
−
ρ
σ Y
Y
aX
Var
, which implies that 
 (a constant), so 
k
Y
aX
=
−
k
aX
Y
aX
−
=
−
, which is of the form 
. 
b
aX +
 
 
88. 
  To find the minimizing value of t, 
take the derivative with respect to t and equate it to 0: 
 
∫∫
⋅
−
+
=
−
+
1
0
1
0
2
2
.
)
,
(
)
(
)
(
dxdy
y
x
f
t
y
x
t
Y
X
E
t
dxdy
y
x
tf
y
x
f
t
y
x
=
⇒
=
−
−
+
=
∫∫
∫∫
1
0
1
0
1
0
1
0
)
,
(
0
)
,
(
)1
)(
(
2
0
 
, so the best prediction is the individual’s 
expected score ( = 1.167). 
)
(
)
,
(
)
(
1
0
1
0
Y
X
E
dxdy
y
x
f
y
x
+
=
⋅
+
= ∫∫
 
 
89. 
 
a. With Y = X1 + X2, 
( )
(
)
(
)
1
2
2
1
2
2
1
2
2
2
/
0
0
1
2
/
2
1
2
1
1
21
1
1
2
/
2
1
2
/
2
1
dx
dx
e
x
x
y
F
x
x
y
x
y
Y
⎭
⎬
⎫
⋅
Γ
⎩
⎨
⎧
⋅
Γ
=
+
−
−
−
−
∫∫
ν
ν
ν
ν
ν
ν
.  
But the inner integral can be shown to be equal to 
(
)
(
)
(
)
2
/
1
]
2
/
[
2
1
2
/
2
1
2
1
2
/)
(
2
1
y
e
y
−
−
+
+
+
Γ
ν
ν
ν
ν
ν
ν
, from which the result follows. 
 
b. By a, 
 is chi-squared with 
2
2
2
1
Z
Z
+
2
=
ν
, so (
)
2
3
2
2
2
1
Z
Z
Z
+
+
 is chi-squared with 
3
=
ν
, etc, until 
 9s chi-squared with
2
2
1
...
n
Z
Z
+
+
n
=
ν
 
 
c. 
σ
µ
−
i
X
 is standard normal, so 
2
⎥⎦
⎤
⎢⎣
⎡
−
σ
µ
i
X
is chi-squared with 
1
=
ν
, so the sum 
is chi-squared with 
n
=
ν
. 
 
 
 
202

Chapter 5:  Joint Probability Distributions and Random Samples 
90. 
 
a. Cov(X, Y + Z) = E[X(Y + Z)] – E(X) ⋅ E(Y + Z)  
= E(XY) + E(XZ) – E(X) ⋅ E(Y) – E(X)  ⋅  E(Z)  
= E(XY) – E(X)  ⋅ E(Y) + E(XZ) – E(X) ⋅ E(Z)  
= Cov(X,Y) + Cov(X,Z). 
 
b. Cov(X1 + X2 , Y1 +  Y2) = Cov(X1 , Y1) + Cov(X1 ,Y2) + Cov(X2 , Y1) + Cov(X2 ,Y2)  
(apply a twice) = 16. 
 
 
 
91. 
 
a. 
 and  
)
(
)
(
)
(
)
(
2
2
2
2
1
1
X
V
E
W
V
E
W
V
X
V
E
W
=
+
=
+
=
+
=
σ
σ
+
+
=
+
+
=
)
,
(
)
,
(
)
,
(
)
,
(
2
2
1
2
1
E
W
Cov
W
W
Cov
E
W
E
W
Cov
X
X
Cov
 
2
2
1
1
)
(
)
,
(
)
,
(
)
,
(
w
W
V
W
W
Cov
E
E
Cov
W
E
Cov
σ
=
=
=
+
.   
Thus, 
2
2
2
2
2
2
2
2
E
W
W
E
W
E
W
W
σ
σ
σ
σ
σ
σ
σ
σ
ρ
+
=
+
⋅
+
=
 
 
b. 
9999
.
0001
.
1
1
=
+
=
ρ
 
 
 
92. 
 
a. Cov(X,Y)  
= Cov(A+D, B+E) 
= Cov(A,B) + Cov(D,B) + Cov(A,E) + Cov(D,E)= Cov(A,B).       Thus 
2
2
2
2
)
,
(
)
,
(
E
B
D
A
B
A
Cov
Y
X
Corr
σ
σ
σ
σ
+
⋅
+
=
 
2
2
2
2
)
,
(
E
B
B
D
A
A
B
A
B
A
Cov
σ
σ
σ
σ
σ
σ
σ
σ
+
⋅
+
⋅
=
 
 
The first  factor in this expression is Corr(A,B), and (by the result of exercise 70a) the 
second and third factors are the square roots of Corr(X1, X2) and Corr(Y1, Y2), 
respectively.  Clearly, measurement error reduces the correlation, since both square-root 
factors are between 0 and 1. 
 
b. 
855
.
9025
.
8100
.
=
⋅
.  This is disturbing, because measurement error substantially 
reduces the correlation. 
 
 
 
203

Chapter 5:  Joint Probability Distributions and Random Samples 
93. 
[
]
26
120
)
,
,
,
(
)
(
20
1
15
1
10
1
4
3
2
1
=
+
+
=
=
µ
µ
µ
µ
h
Y
E
&
 
The partial derivatives of 
)
,
,
,
(
4
3
2
1
µ
µ
µ
µ
h
 with respect to x1, x2, x3, and x4 are 
,
2
1
4
x
x
−
 
,
2
2
4
x
x
−
 
,
2
3
4
x
x
−
 and 
3
2
1
1
1
1
x
x
x
+
+
, respectively.  Substituting x1 = 10, x2 = 15, x3 = 20, and 
x4 = 120 gives –1.2, -.5333, -.3000, and .2167, respectively, so V(Y) = (1)(-1.2)2 + (1)(-
.5333)2 + (1.5)(-.3000)2 + (4.0)(.2167)2 = 2.6783, and the approximate sd of y is 1.64. 
 
 
 
94. 
The four second order partials are 
,
2
3
1
4
x
x
,
2
3
2
4
x
x
,
2
3
3
4
x
x
and 0 respectively.  Substitution gives 
E(Y) = 26 + .1200 + .0356 + .0338 = 26.1894. 
 
 
204

CHAPTER 7 
 
Section 7.1 
 
1. 
 
a. 
81
.2
2 =
αz
 implies that 
(
)
0025
.
81
.2
1
2
=
Φ
−
=
α
, so 
005
.
=
α
 and the confidence 
level is 
(
)
%
5.
99
%
1
100
=
−α
. 
 
b. 
44
.1
2 =
αz
 for 
(
)
[
]
15
.
44
.1
1
2
=
Φ
−
=
α
, and 
(
)
%
85
%
1
100
=
−α
. 
 
c. 
99.7% implies that 
003
.
=
α
, 
0015
.
2 =
α
, and 
96
.2
0015
.
=
z
. (Look for cumulative 
area .9985 in the main body of table A.3, the Z table.) 
 
d. 75% implies 
25
.
=
α
, 
125
.
2 =
α
, and 
15
.1
125
.
=
z
. 
 
2. 
 
a. The sample mean is the center of the interval, so 
115
2
6.
115
4.
114
=
+
=
x
. 
 
b. The interval (114.4, 115.6) has the 90% confidence level.  The higher confidence level 
will produce a wider interval. 
 
 
3. 
 
a. A 90% confidence interval will be narrower (See 2b, above)  Also, the z critical value for 
a 90% confidence level is 1.645, smaller than the z of 1.96 for the 95% confidence level, 
thus producing a narrower interval. 
 
b. Not a correct statement.  Once and interval has been created from a sample, the mean µ  
is either enclosed by it, or not.  The 95% confidence is in the general procedure, for 
repeated sampling. 
 
c. 
Not a correct statement.  The interval is an estimate for the population mean, not a 
boundary for population values. 
 
d. Not a correct statement.  In theory, if the process were repeated an infinite number of 
times, 95% of the intervals would contain the population mean µ . 
 
219 

Chapter 7:  Statistical Intervals Based on a Single Sample 
4. 
 
a. 
( )
(
)
5.
59
,1.
57
18
.1
3.
58
25
3
96
.1
3.
58
=
±
=
±
 
 
b. 
( )
(
)
9.
58
,7.
57
59
.
3.
58
100
3
96
.1
3.
58
=
±
=
±
 
 
c. 
( )
(
)1.
59
,5.
57
77
.
3.
58
100
3
58
.2
3.
58
=
±
=
±
 
 
d. 82% confidence 
09
.
18
.
82
.
1
2 =
⇒
=
⇒
=
−
⇒
α
α
α
, so 
34
.1
09
.
2
=
= z
zα
 and 
the interval is 
( )
(
)
7.
58
,9.
57
100
3
34
.1
3.
58
=
±
. 
 
e. 
(
)
62
.
239
1
3
58
.2
2
2
=
⎥⎦
⎤
⎢⎣
⎡
=
n
 so n = 240. 
 
 
5. 
 
a. 
(
)(
)
=
±
=
±
33
.
85
.4
20
75
.
96
.1
85
.4
 (4.52, 5.18). 
 
b. 
33
.2
01
.
2
02
.
2
=
=
=
z
z
zα
, so the interval is 
(
)(
) =
±
16
75
.
33
.2
56
.4
 (4.12, 5.00). 
 
c. 
(
)(
)
02
.
54
40
.
75
.
96
.1
2
2
=
⎥⎦
⎤
⎢⎣
⎡
=
n
, so n = 55. 
 
d. 
(
)(
)
61
.
93
2.
75
.
58
.2
2
2
=
⎥⎦
⎤
⎢⎣
⎡
=
n
, so n = 94. 
 
 
6. 
 
a. 
(
)(
)
=
±
=
±
9.
32
8439
25
100
645
.1
8439
 (8406.1, 8471.9). 
 
b. 
04
.
08
.
92
.
1
2 =
⇒
=
⇒
=
−
α
α
α
 so 
75
.1
04
.
2
=
= z
zα
 
 
 
220

Chapter 7:  Statistical Intervals Based on a Single Sample 
7. 
If  
n
z
L
σ
α 2
2
=
 and we increase the sample size by a factor of 4, the new length is 
2
2
1
2
4
2
2
2
L
n
z
n
z
L
=
⎟
⎠
⎞
⎜
⎝
⎛
⎥⎦
⎤
⎢⎣
⎡
=
=
′
σ
σ
α
α
.  Thus halving the length requires n to be 
increased fourfold.  If  
, then 
n
n
25
=
′
5
L
L =
′
, so the length is decreased by a factor of 5. 
 
 
8. 
 
a. With probability 
α
−
1
, 
(
)
2
1
α
α
σ
µ
z
n
X
z
≤
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
≤
.  These inequalities can be 
manipulated exactly as was done in the text to isolate µ ;  the result is 
n
z
X
n
z
X
σ
µ
σ
α
α
1
2
+
≤
≤
−
, so a 
(
)%
1
100
α
−
 interval is 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
−
n
z
X
n
z
X
σ
σ
α
α
1
2
,
 
 
b. The usual 95% interval has length 
n
σ
92
.3
, while this interval will have length 
(
)
n
z
z
σ
α
α
2
1 +
.  With 
24
.2
0125
.
1
=
= z
zα
 and 
78
.1
0375
.
2
=
= z
zα
, the length is 
(
)
,
02
.4
78
.1
24
.2
n
n
σ
σ
=
+
 which is longer. 
 
 
9. 
 
a. 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
∞
−
,
645
.1
n
x
σ
.  From 5a, 
85
.4
=
x
, 
75
.
=
σ
, n = 20; 
5741
.4
20
75
.
645
.1
85
.4
=
−
, so the interval is (
)
∞
,
5741
.4
. 
 
b. 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
∞
−
,
n
z
x
σ
α
 
 
 
c. 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
∞
−
n
z
x
σ
α
,
; From 4a, 
3.
58
=
x
, 
0.3
=
σ
, n = 25; 
(
)
70
.
59
,
25
3
33
.2
3.
58
∞
−
=
+
 
 
221

Chapter 7:  Statistical Intervals Based on a Single Sample 
10. 
 
a. When n = 15, 
has a chi-squared distribution with 30 d.f.  From the 30 d.f. row 
of Table A.6, the critical values that capture lower and upper tail areas of .025 (and thus a 
central area of .95) are 16.791 and 46.979.  An argument parallel to that given in 
Example 7.5 gives 
∑
i
X
λ
2
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
∑
∑
791
.
16
2
,
979
.
46
2
i
i
x
x
 as a 95% C. I. for 
.
1
λ
µ =
  Since 
 the interval is (2.69, 7.53). 
2.
63
=
∑
ix
 
b. A 99% confidence level requires using critical values that capture area .005 in each tail of 
the chi-squared curve with 30 d.f.; these are 13.787 and 53.672, which  replace 16.791 
and 46.979 in a. 
 
c. 
( )
2
1
λ
=
X
V
 when X has an exponential distribution, so the standard deviation is λ
1 , 
the same as the mean.  Thus the interval of a is also a 95% C.I. for the standard deviation 
of the lifetime distribution.  
 
 
11. 
Y is a binomial r.v. with n = 1000 and p = .95, so E(Y) = np = 950, the expected number of 
intervals that capture µ , and 
892
.6
=
=
npq
Y
σ
.  Using the normal approximation to 
the binomial distribution, P(940 ≤ Y ≤ 960) = P(939.5 ≤ Ynormal ≤ 960.5) = P(-1.52 ≤ Z ≤ 1.52) 
= .9357 - .0643 = .8714. 
 
 
Section 7.2 
 
12. 
(
)
89
,.
73
.
08
.
81
.
110
34
.
58
.2
81
.
58
.2
=
±
=
±
=
±
n
s
x
 
 
 
13. 
 
a. 
(
)
066
.1,
990
.
038
.
028
.1
69
163
.
96
.1
028
.1
025
.
=
±
=
±
=
±
n
s
z
x
 
 
b. 
(
)(
)
(
)(
)
(
)
158
544
.
12
544
.
12
05
.
16
.
96
.1
2
16
.
96
.1
2
05
.
2 ≈
=
⇒
=
=
⇒
=
=
n
n
n
w
 
 
 
222

Chapter 7:  Statistical Intervals Based on a Single Sample 
14. 
 
a. 
(
66
.
89
,.
54
.
88
56
.
10
.
89
169
73
.3
96
.1
10
.
89
=
±
=
±
).  Yes, this is a very narrow 
interval.  It appears quite precise. 
 
b. 
(
)(
)
246
86
.
245
5.
16
.
96
.1
2
=
⇒
=
⎥⎦
⎤
⎢⎣
⎡
=
n
n
. 
 
 
15. 
 
a. 
, and 
84
.
=
αz
(
)
80
.
7995
.
84
.
≈
=
Φ
, so the confidence level is 80%. 
 
b. 
, and 
05
.2
=
αz
(
)
98
.
9798
.
05
.2
≈
=
Φ
, so the confidence level is 98%. 
 
c. 
, and 
67
.
=
αz
(
)
75
.
7486
.
67
.
≈
=
Φ
, so the confidence level is 75%. 
 
 
16. 
n = 46, 
1.
382
=
x
, s = 31.5;  The 95% upper confidence bound = 
74
.
389
64
.7
1.
382
46
5.
31
645
.1
1.
382
=
+
=
+
=
+
n
s
z
x
α
 
 
 
17. 
53
.
134
865
.
39
.
135
153
59
.4
33
.2
39
.
135
01
.
=
−
=
−
=
−
n
s
z
x
   With a confidence 
level of 99%, the true average ultimate tensile strength is between (134.53, ∞). 
 
 
18. 
90% lower bound: 
06
.4
75
30
.1
28
.1
25
.4
10
.
=
−
=
−
n
s
z
x
 
 
 
19. 
5646
.
356
201
ˆ
=
=
p
; We calculate a 95% confidence interval for the proportion of all dies 
that pass the probe: 
(
)
(
)
(
)(
)
(
)
(
)
(
)
(
)
615
,.
513
.
01079
.1
0518
.
5700
.
356
96
.1
1
356
4
96
.1
356
4354
.
5646
.
96
.1
356
2
96
.1
5646
.
2
2
2
2
=
±
=
+
+
±
+
 
 
 
223

Chapter 7:  Statistical Intervals Based on a Single Sample 
20. 
Because the sample size is so large, the simpler formula (7.11) for the confidence interval for 
p is sufficient. 
(
)(
)
(
)
163
,.
137
.
013
.
15
.
4722
85
.
15
.
58
.2
15
.
=
±
=
±
 
 
 
21. 
2468
.
539
133
ˆ
=
=
p
; the 95% lower confidence bound is: 
(
)
(
)
(
)(
)
(
)
(
)
(
)
218
.
005
.1
0307
.
2493
.
539
645
.1
1
539
4
645
.1
539
7532
.
2468
.
645
.1
539
2
645
.1
2468
.
2
2
2
2
=
−
=
+
+
−
+
 
 
 
22. 
; the 99% upper confidence bound is: 
072
.
ˆ =
p
(
)
(
)
(
)(
)
(
)
(
)
(
)
1043
.
0111
.1
0279
.
0776
.
487
33
.2
1
487
4
33
.2
487
928
.
072
.
33
.2
487
2
33
.2
072
.
2
2
2
2
=
+
=
+
+
+
+
 
 
 
23. 
 
a. 
6486
.
37
24
ˆ
=
=
p
; The 99% confidence interval for p is 
(
)
(
)
(
)(
)
(
)
(
)
(
)
(
)
814
,.
438
.
1799
.1
2216
.
7386
.
37
58
.2
1
37
4
58
.2
37
3514
.
6486
.
58
.2
37
2
58
.2
6486
.
2
2
2
2
=
±
=
+
+
±
+
 
b. 
(
) (
) (
) (
)
(
) (
)(
)
(
)
01
.
58
.2
01
.
01
.
25
.
25
.
58
.2
4
01
.
58
.2
25
.
58
.2
2
4
4
2
2
+
−
±
−
=
n
 
659
01
.
3282
.3
261636
.3
≈
±
=
 
 
 
24. 
n = 56, 
17
.8
=
x
, s = 1.42;  For a 95% C.I., 
96
.1
2 =
αz
.  The interval is   
(
542
.8,
798
.7
56
42
.1
96
.1
17
.8
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
±
).  We make no assumptions about the distribution if 
percentage elongation. 
 
224

Chapter 7:  Statistical Intervals Based on a Single Sample 
25. 
 
a. 
(
) (
) (
) (
)
(
) (
)(
)
(
)
381
01
.
96
.1
01
.
01
.
25
.
25
.
96
.1
4
01
.
96
.1
25
.
96
.1
2
4
4
2
2
≈
+
−
±
−
=
n
 
 
b. 
(
) (
) (
) (
)
(
) (
)(
)
(
)
339
01
.
96
.1
01
.
01
.
96
.1
4
01
.
96
.1
96
.1
2
4
3
2
3
1
3
2
3
1
4
2
3
2
3
1
2
≈
+
−
⋅
⋅
±
−
⋅
=
n
 
 
 
26. 
With  
λ
θ =
, 
X
=
θˆ
 and 
n
λ
σ θ =
ˆ
 so 
n
X
=
θ
σ ˆˆ
.  The large sample C.I. is then 
n
x
z
x
2
/
α
±
.  We calculate ∑
= 203
ix
, so 
06
.4
=
x
, and a 95% interval for λ  is 
(
)
62
.4,
50
.3
56
.
06
.4
50
06
.4
96
.1
06
.4
=
±
=
±
 
 
 
27. 
Note that the midpoint of the new interval is 
2
2
2
z
n
z
x
+
+
, which is roughly 
4
2
+
+
n
x
 with a 
confidence level of 95% and approximating 
2
96
.1
≈
.  The variance of this quantity is 
(
)
(
)
2
2
1
z
n
p
np
+
−
, or roughly (
)
4
1
+
−
n
p
p
.  Now replacing p  with 
4
2
+
+
n
x
, we have 
4
4
2
1
4
2
4
2
2
+
⎟
⎠
⎞
⎜
⎝
⎛
+
+
−
⎟
⎠
⎞
⎜
⎝
⎛
+
+
±
⎟
⎠
⎞
⎜
⎝
⎛
+
+
n
n
x
n
x
z
n
x
α
; For clarity, let 
 and 
, then 
2
*
+
= x
x
4
*
+
= n
n
*
*
*ˆ
n
x
p =
 and the formula reduces to 
*
*
*
*
ˆ
ˆ
ˆ
2
n
q
p
z
p
α
±
, the desired conclusion.  For 
further discussion, see the Agresti article. 
 
Section 7.3 
 
28. 
 
a. 1.341 
 
b. 1.753 
 
c. 
1.708 
 
d. 1.684 
 
e. 
2.704 
 
 
 
225

Chapter 7:  Statistical Intervals Based on a Single Sample 
29. 
 
a. 
 
228
.2
10
,
025
.
=
t
 
b. 
 
086
.2
20
,
025
.
=
t
 
c. 
 
845
.2
20
,
005
.
=
t
 
d. 
678
.2
50
,
005
.
=
t
 
 
e. 
485
.2
25
,
01
.
=
t
 
 
f. 
571
.2
5,
025
.
−
=
−t
 
 
30. 
 
a. 
 
228
.2
10
,
025
.
=
t
 
b. 
 
131
.2
15
,
025
.
=
t
 
c. 
 
947
.2
15
,
005
.
=
t
 
 
d. 
604
.4
4
,
005
.
=
t
 
 
e. 
492
.2
24
,
01
.
=
t
 
 
f. 
712
.2
37
,
005
.
≈
t
 
 
31. 
 
a. 
 
812
.1
10
,
05
=
t
 
b. 
 
753
.1
15
,
05
.
=
t
 
c. 
 
602
.2
15
,
01
.
=
t
 
d. 
747
.3
4
,
01
.
=
t
 
 
e. 
064
.2
24
,
025
.
=
≈t
 
 
f. 
429
.2
37
,
01
.
≈
t
 
 
32. 
d.f. = n – 1 = 7, so the critical value for a 95% C.I. is 
365
.2
7
,
025
.
=
t
.  The interval is 
(
)
(
)
8.
32
,6.
27
6.2
2.
30
8
1.3
365
.2
2.
30
=
±
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
±
. 
 
 
 
226

Chapter 7:  Statistical Intervals Based on a Single Sample 
33. 
 
a. The boxplot indicates a very slight positive skew, with no outliers.  The data appears to 
center near 438.  
 
 
420
430
440
450
460
470
polymer
b. Based on a normal probability plot, it is reasonable to assume the sample observations 
came from a normal distribution. 
 
c. 
With d.f. = n – 1 = 16, the critical value for a 95% C.I. is 
120
.2
16
,
025
.
=
t
, and the 
interval is 
(
)
(
)
08
.
446
,
51
.
430
785
.7
29
.
438
17
14
.
15
120
.2
29
.
438
=
±
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
±
.  
Since 440 is within the interval, 440 is a plausible value for the true mean.  450, however, 
is not, since it lies outside the interval. 
 
 
34. 
n = 14, 
48
.8
=
x
, s = .79; 
771
.1
13
,
05
.
=
t
 
a. A 95% lower confidence bound: 
11
.8
37
.
48
.8
14
79
.
771
.1
48
.8
=
−
=
⎟
⎠
⎞
⎜
⎝
⎛
−
.  With 
95% confidence, the value of the true mean proportional limit stress of all such joints lies 
in the interval (
.  If this interval is calculated for sample after sample, in the 
long run 95% of these intervals will include the true mean proportional limit stress of all 
such joints.  We must assume that the sample observations were taken from a normally 
distributed population. 
)
∞
,
11
.8
 
b. A 95% lower prediction bound: 
(
)
03
.7
45
.1
48
.8
14
1
1
79
.
771
.1
48
.8
=
−
=
+
−
.  If 
this bound is calculated for sample after sample, in the long run 95% of these bounds will 
provide a lower bound for the corresponding future values of the proportional limit stress 
of a single joint of this type. 
 
 
 
227

Chapter 7:  Statistical Intervals Based on a Single Sample 
35. 
n = 5, 
6.
2887
=
x
, s = .84.0; 
776
.2
4
,
025
.
=
t
 
a. A 95% C.I. for the mean: 
(
)
(
)
9.
2991
,3.
2783
5
84
776
.2
6.
2887
⇒
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
±
 
 
b. A 95% Prediction Interval: 
(
)
(
)1.
3143
,1.
2632
5
1
1
84
776
.2
6.
2887
⇒
+
±
.  The 
P.I. is considerably larger than the C.I., about 2.5 times larger. 
 
 
36. 
n = 26, 
69
.
370
=
x
, s = 24.36; 
708
.1
25
,
05
.
=
t
 
a. A 95% upper confidence bound: 
(
)
85
.
378
16
.8
69
.
370
26
36
.
24
708
.1
69
.
370
=
+
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
 
 
b. A 95% upper prediction bound: 
(
)
14
.
413
45
.
42
69
.
370
26
1
1
36
.
24
708
.1
69
.
370
=
+
=
+
+
 
 
c. 
Following a similar argument as that on p. 300 of the text, we need to find the variance of  
new
X
X −
:  (
)
( )
(
)
( )
(
)
(
)
28
27
2
1
X
X
V
X
V
X
V
X
V
X
X
V
new
new
+
+
=
+
=
−
 
( )
(
)
(
)
( )
(
)
(
)
28
4
1
27
4
1
28
2
1
27
2
1
X
V
X
V
X
V
X
V
X
V
X
V
+
+
=
+
+
=
 
⎟
⎠
⎞
⎜
⎝
⎛
+
=
+
+
=
n
n
1
2
1
4
1
4
1
2
2
2
2
σ
σ
σ
σ
.  We eventually arrive at 
~
1
2
1
n
new
s
X
X
T
+
−
=
t 
distribution with n – 1 d.f., so the new prediction interval is 
n
n
s
t
x
1
2
1
1
,2
/
+
⋅
±
−
α
.  For 
this situation, we have 
(
)
(
)
53
.
400
,
47
.
39
53
.
30
69
.
370
26
1
2
1
36
.
24
708
.1
69
.
370
=
±
=
+
±
 
 
 
37. 
 
a. A 95% C.I. : 
(
)
(
)
9634
,.
8876
.
0379
.
9255
.
0181
.
093
.2
9255
.
⇒
±
=
±
 
 
b. A 95% P.I. : 
(
)
(
)
0990
.1,
7520
.
1735
.
9255
.
1
0809
.
093
.2
9255
.
20
1
⇒
±
=
+
±
 
 
c. 
A tolerance interval is requested, with k = 99, confidence level 95%, and n = 20.  The 
tolerance critical value, from Table A.6, is 3.615.  The interval is 
(
)
(
)
2180
.1,
6330
.
0809
.
615
.3
9255
.
⇒
±
. 
 
 
 
228

Chapter 7:  Statistical Intervals Based on a Single Sample 
38. 
N = 25, 
0635
.
=
x
, s = .0065 
a. 95% P.I. : 
(
)
(
)
0772
,.
0498
.
0137
.
0635
.
1
0065
.
064
.2
0635
.
25
1
⇒
±
=
+
±
. 
 
b. 99% Tolerance Interval, with k = 95, critical value 2.972 (table A.6): 
(
)
(
)
0828
,.
0442
.
0065
.
972
.2
0635
.
⇒
±
. 
 
 
39. 
 
a.  
Average: 52.2308
StDev: 14.8557
N: 13
Anderson-Darling Normality Test
A-Squared: 0.360
P-Value:   0.392
30
50
70
.001
.01
.05
.20
.50
.80
.95
.99
.999
Probability
volume
Normal Probability Plot
 
 
Based on the above plot, generated by Minitab, it is plausible that the population 
distribution is normal. 
 
b. We require a tolerance interval. (from table A6, with 95% confidence, k = 95, and n=13, 
the tcv = 3.081.  
(
)
(
)
(
)
002
.
98
,
460
.6
771
.
45
231
.
52
856
.
14
081
.3
231
.
52
⇒
±
=
±
=
±
s
tcv
x
 
 
c. 
A prediction interval, with 
179
.2
12
,
025
.
=
t
:  
(
)
(
)
824
.
85
,
638
.
18
593
.
33
231
.
52
1
856
.
14
179
.2
231
.
52
13
1
⇒
±
=
+
±
 
 
 
 
229

Chapter 7:  Statistical Intervals Based on a Single Sample 
40. 
 
a. We need to assume the samples came from a normally distributed population. 
 
b. A Normal Probability plot, generated by Minitab: 
P-Value:   0.008
A-Squared: 1.065
Anderson-Darling Normality Test
N: 153
StDev: 4.54186
Average: 134.902
145
135
125
.999
.99
.95
.80
.50
.20
.05
.01
.001
Probability
strength
Normal Probability Plot
 
 
The very small p-value indicates that the population distribution from which this data was 
taken is most likely not normal. 
 
c. 
95% lower prediction bound: 
(
)
(
)
824
.
85
,
638
.
18
593
.
33
231
.
52
1
856
.
14
179
.2
231
.
52
13
1
⇒
±
=
+
±
 
 
 
41. 
The 20 d.f. row of Table A.5 shows that 1.725 captures upper tail area .05 and 1.325 captures 
uppertail area .10  The confidence level for each interval is 100(central area)%.  For the first 
interval, central area = 1 – sum of tail areas = 1 – (.25 + .05) = .70, and for the second and 
third intervals the central areas are 1 – (.20 + .10) = .70 and 1 – (.15 + .15) = 70.  Thus each 
interval has confidence level 70%.  The width of the first interval is 
(
)
n
s
n
s
2412
.
725
.1
687
.
=
+
, whereas the widths of the second and third intervals are 2.185 
and 2.128 respectively.  The third interval, with symmetrically placed critical values, is the 
shortest, so it should be used.  This will always be true for a t interval. 
 
 
 
230

Chapter 7:  Statistical Intervals Based on a Single Sample 
Section 7.2 
 
42. 
 
a. 
 (.1 column, 15 
d.f. row) 
307
.
22
2
15
,1.
=
χ
 
b. 
 
381
.
34
2
25
,1.
=
χ
 
c. 
 
313
.
44
2
25
,
01
.
=
χ
 
d. 
 
925
.
46
2
25
,
005
.
=
χ
 
e. 
 (from .99 
column, 25 d.f. row) 
523
.
11
2
25
,
99
.
=
χ
 
f. 
 
519
.
10
2
25
,
995
.
=
χ
 
43. 
 
a. 
 
307
.
18
2
10
,
05
.
=
χ
 
b. 
 
940
.3
2
10
,
95
.
=
χ
c. 
Since 
 and 
,
2
22
,
975
.
987
.
10
χ
=
2
22
,
025
.
78
.
36
χ
=
(
)
95
.
2
22
,
025
.
2
2
22
,
975
.
=
≤
≤
χ
χ
χ
P
. 
 
d. Since 
 and 
,
2
25
,
95
.
61
.
14
χ
=
2
25
,
05
.
65
.
37
χ
=
(
)
90
.
2
25
,
05
.
2
2
25
,
95
.
=
≤
≤
χ
χ
χ
P
. 
 
 
44. 
n – 1 = 8 , 
, 
, so the 95% interval for 
 is 
543
.
17
2
8,
025
.
=
χ
180
.2
2
8,
975
.
=
χ
2
σ
(
98
.
28
,
60
.3
180
.2
)
90
.7
(
8
,
543
.
17
)
90
.7
(
8
=
⎟
⎠
⎞
⎜
⎝
⎛
) .  The 95% interval for σ  is 
(
)
(
)
38
.5,
90
.1
98
.
28
,
60
.3
=
. 
 
 
45. 
n = 22 implies that d.f. = n – 1 = 21, so the .995 and .005 columns of Table A.7 give the 
necessary chi-squared critical values as 8.033 and 41.399. 
3.
1701
=
Σ
ix
 and 
, so 
.  The interval for 
 is 
35
.
097
,
132
2 =
Σ
ix
368
.
25
2 =
s
2
σ
(
)
(
)
(
317
.
66
,
868
.
12
033
.8
368
.
25
21
,
399
.
41
368
.
25
21
=
⎟
⎠
⎞
⎜
⎝
⎛
) and that for σ is (
)1.8,6.3
  Validity of 
this interval requires that fracture toughness be  (at least approximately) normally distributed. 
 
 
46. 
 
a. Using a normal probability plot, we ascertain that it is plausible that this sample was 
taken from a normal population distribution. 
 
b. With s = 1.579 , n = 15, and  
the 95% upper confidence bound for 
685
.
23
2
14
,
05
.
=
χ
σ  
is 
(
)
214
.1
685
.
23
579
.1
14
2
=
 
 
231

Chapter 7:  Statistical Intervals Based on a Single Sample 
Supplementary Exercises 
 
47. 
 
a. n = 48, 
079
.8
=
x
, s2 = 23.7017, and s = 4.868.   
A 95% C.I. for µ = the true average strength is  
(
)
456
.9,
702
.6
377
.1
079
.8
48
868
.4
96
.1
079
.8
96
.1
=
±
=
±
=
±
n
s
x
 
 
b. 
2708
.
48
13
ˆ
=
=
p
.  A 95% C.I. is 
(
)
(
)(
)
(
)
(
)
410
,.
166
.
0800
.1
1319
.
3108
.
48
96
.1
1
48
4
96
.1
48
7292
.
2708
.
96
.1
48
2
96
.1
2708
.
2
2
2
2
=
±
=
+
+
±
+
 
 
 
48. 
A 98% t C.I. requires 
896
.2
8,
01
.
1
,2
/
=
=
−
t
t
n
α
.  The interval is 
(
)
0.
195
,0.
181
0.7
0.
188
9
2.7
896
.2
0.
188
=
±
=
±
. 
 
49. 
 
a. There appears to be a slight positive skew in the middle half of the sample, but the lower 
whisker is much longer than the upper whisker.  The extent of variability is rather 
substantial, although there are no outliers.  
50
40
30
20
%porevolume
 
 
b. The pattern of points in a normal probability plot is reasonably linear, so, yes, normality 
is plausible. 
 
c. 
n = 18, 
66
.
38
=
x
,  s = 8.473, and 
586
.2
17
,
01
.
=
t
.  The 98% confidence interval is 
(
)
79
.
43
,
53
.
33
13
.5
66
.
38
18
473
.8
586
.2
66
.
38
=
±
=
±
. 
 
232

Chapter 7:  Statistical Intervals Based on a Single Sample 
 
50. 
=
x
 the middle of the interval = 
.
633
.
231
2
502
.
233
764
.
229
=
+
  To find s we use 
(
)
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
=
n
s
t
width
4
,
025
.
2
, and solve for s.  Here, n = 5, 
776
.2
4
,
025
.
=
t
, and width = upper 
limit – lower limit = 3.738. 
(
)
(
)
(
)
5055
.1
776
.2
2
738
.3
5
5
2776
2
738
.3
=
=
⇒
=
s
s
.  So for 
a 99% C.I., 
, and the interval is 
604
.4
4
,
005
.
=
t
(
)
733
.
234
,
533
.
228
100
.3
633
.
213
5
5055
.1
604
.4
633
.
231
=
±
=
±
. 
 
 
51. 
 
a. 
⇒
=
=
680
.
200
136
ˆp
 a 90% C.I. is  
(
)
(
)(
)
(
)
(
)
732
,.
624
.
01353
.1
0547
.
6868
.
200
645
.1
1
200
4
645
.1
200
320
.
680
.
645
.1
200
2
645
.1
680
.
2
2
2
2
=
±
=
+
+
±
+
 
b. 
(
) (
) (
) (
)
(
) (
)(
)
(
)
0025
.
645
.1
05
.
0025
.
25
.
25
.
645
.1
4
05
.
645
.1
25
.
645
.1
2
4
2
4
2
2
2
+
−
±
−
=
n
 
⇒
=
±
=
7.
1079
0025
.
3530
.1
3462
.1
 use n = 1080 
 
c. 
No, it gives a 95% upper bound. 
 
 
52. 
 
a. Assuming normality, 
753
.1
15
,
05
.
=
t
, do s 95% C.I. for µ  is  
(
)
230
,.
198
.
016
.
214
.
16
036
.
753
.1
214
.
=
±
=
±
 
 
b. A 90% upper bound for σ , with 
,  is 
341
.
1
2
15
,
10
.
=
χ
(
)
120
.
0145
.
341
.1
036
.
15
2
=
=
 
 
c. 
A 95% prediction interval, with 
131
.
2
15
,
025
.
=
t
, is 
(
)
(
)
2931
,.
1349
.
0791
.
214
.
1
036
.
131
.
2
214
.
16
1
=
±
=
+
±
. 
 
 
233

Chapter 7:  Statistical Intervals Based on a Single Sample 
53. 
With  
(
)
4
3
2
1
3
1
ˆ
X
X
X
X
−
+
+
=
θ
, 
(
)
(
)
4
3
2
1
9
1
2
ˆ
X
Var
X
X
X
Var
+
+
+
=
θ
σ
 = 
4
2
4
3
2
3
2
2
2
1
2
1
9
1
n
n
n
n
σ
σ
σ
σ
+
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
+
;  
θ
σ ˆ
ˆ  is obtained by replacing each 
 by 
 and taking the 
square root.  The large-sample interval for 
2
ˆ i
σ
2
is
θ  is then 
(
)
4
2
4
3
2
3
2
2
2
1
2
1
2
/
4
3
2
1
3
1
9
1
n
s
n
s
n
s
n
s
z
x
x
x
x
+
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
+
±
−
+
+
α
.  For the given data, 
, 
50
.
ˆ
−
=
θ
1718
.
ˆ ˆ =
θ
σ
, so the interval is 
(
)
(
)
16
.
,
84
.
1718
.
96
.1
50
.
−
−
=
±
−
. 
 
54. 
⇒
=
=
2.
55
11
ˆp
 a 90% C.I. is  
(
)
( )( )
(
)
(
)
2986
,.
1295
.
0492
.1
0887
.
2246
.
55
645
.1
1
55
4
645
.1
55
8.
2.
645
.1
55
2
645
.1
2.
2
2
2
2
=
±
=
+
+
±
+
. 
 
 
55. 
The specified condition is that the interval be length .2, so 
(
)( )
86
.
245
2.
8.
96
.1
2
2
=
⎥⎦
⎤
⎢⎣
⎡
=
n
, so 
n = 246 should be used. 
 
 
56. 
 
a. A normal probability plot lends support to the assumption that pulmonary compliance is 
normally distributed.  Note also that the lower and upper fourths are 192.3 and 228,1, so 
the fourth spread is 35.8, and the sample contains no outliers. 
 
b. 
, so the C.I. is 
131
.2
15
,
025
.
=
t
(
)
62
.
222
,
88
.
196
87
.
12
75
.
209
16
156
.
24
131
.2
75
.
209
=
±
=
±
. 
 
c. 
K = 95, n = 16, and the tolerance critical value is 2.903, so the 95% tolerance interval is 
(
)
(
)
875
.
279
,
625
.
139
125
.
70
75
.
209
156
.
24
903
.2
75
.
209
=
±
=
±
. 
 
 
57. 
Proceeding as in Example 7.5 with Tr replacing 
i
X
Σ
, the C.I. for λ
1  is 
⎟
⎟
⎠
⎞
⎜
⎜
⎝
⎛
−
2
2
,
2
2
,
1
2
2
2
,
2
r
r
r
r
t
t
α
α
χ
χ
 
where 
(
)
.
...
1
r
r
r
y
r
n
y
y
t
−
+
+
+
=
  In Example 6.7, n = 20, r = 10, and tr = 1115.  With 
d.f. = 20, the necessary critical values are 9.591 and 34.170, giving the interval (65.3, 232.5).  
This is obviously an extremely wide interval.  The censored experiment provides less 
information about λ
1  than would an uncensored experiment with n = 20. 
 
234

Chapter 7:  Statistical Intervals Based on a Single Sample 
58. 
 
a. 
)
~
)
max(
)
min(
,
~
(
1
))
max(
~
)
(min(
µ
µ
µ
<
<
−
=
≤
≤
i
i
i
i
X
or
X
P
X
X
P
 
)
~
)
(max(
))
min(
,
~
(
1
µ
µ
<
−
<
−
=
i
i
X
P
X
P
  
)
~
,...,
~
(
)
~
,...,
~
(
1
1
1
µ
µ
µ
µ
<
<
−
<
<
−
=
n
n
X
X
P
X
X
P
   
( )
( )
( )
1
5.
2
1
5.
5.
1
−
−
=
−
−
=
n
n
n
, from which the confidence interval follows. 
 
b. Since  
 and 
44
.1
)
min(
=
ix
,
54
.3
)
max(
=
ix
 the C.I. is (1.44, 3.54). 
 
c. 
)
~
(
)
,
~
(
1
)
~
(
)
1
(
)
2
(
)
1
(
)
2
(
µ
µ
µ
<
−
<
−
=
≤
≤
−
−
n
n
X
P
X
P
X
X
P
    
= 1 – P( at most one XI is below µ~ ) – P(at most one XI exceeds µ~ )  
( )
( ) ( )
( )
( )
( )
5.
5.
1
5.
5.
5.
1
5.
1
1
1
1
−
−
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
−
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
−
n
n
n
n
n
n
.   
(
)( )
(
)( )
1
5.
1
1
5.
1
2
1
−
+
−
=
+
−
=
n
n
n
n
 
 
 
Thus the confidence coefficient is 
(
)( )
1
5.
1
1
−
+
−
n
n
, or in another way, a 
(
)( )
(
)%
5.
1
1
100
1
−
+
−
n
n
confidence interval. 
 
 
59. 
 
a. 
(
)
(
)
](
)
(
)
α
α
α
α
α
α
α
−
=
−
−
=
=
−
−
−
∫
1
2
2
1
/
1
/
1
/
1
/
1
2
/
1
2
/
2
/
1
2
/
1
n
n
n
n
n
n
u
du
nu
.  From the probability 
statement, 
( )
(
)
(
)
(
)
i
i
X
X
n
n
max
1
1
max
1
1
2
2
α
α
θ
−
≤
≤
 with probability 
α
−
1
, so taking the 
reciprocal of each endpoint and interchanging gives the C.I. 
(
)
(
)
(
)
( )
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
n
n
i
i
X
X
1
1
2
2
max
,
1
max
α
α
 
for θ . 
 
b. 
1
)
max(
1
≤
≤
θ
α
i
X
n
 with probability 
α
−
1
, so 
(
)
n
i
X
1
1
max
1
α
θ
≤
≤
 with 
probability 
α
−
1
, which yields the interval 
(
)
(
)⎟
⎠
⎞
⎜
⎝
⎛
n
i
i
X
X
1
max
,
max
α
. 
 
c. 
It is easily verified that the interval of b is shorter – draw a graph of 
 and verify 
that the shortest interval which captures area 
( )
u
fU
α
−
1
 under the curve is the rightmost such 
interval, which leads to the C.I. of b.  With 
,
05
.
=
α
 n = 5, max(xI)=4.2; this yields (4.2, 
7.65). 
 
 
235

Chapter 7:  Statistical Intervals Based on a Single Sample 
60. 
The length of the interval is  (
)
n
s
z
z
γ
α
γ
−
+
, which is minimized when 
 is 
minimized, i.e. when 
γ
α
γ
−
+ z
z
(
)
(
)
γ
α
γ
+
−
Φ
+
−
Φ
−
−
1
1
1
1
 is minimized.  Taking γ
d
d  and 
equating to 0 yields 
(
)
(
)
γ
α
γ
+
−
Φ
=
−
Φ
1
1
1
1
  where 
( )
•
Φ
 is the standard normal p.d.f., 
whence 
2
α
γ =
. 
 
 
61. 
,2.
76
~ =
x
 the lower and upper fourths are 73.5 and 79.7, respectively, and 
  The 
robust interval is 
.2.6
=
sf
(
)
(
)
8.
78
,6.
73
6.2
2.
76
22
2.6
93
.1
2.
76
=
±
=
⎟
⎠
⎞
⎜
⎝
⎛
±
. 
 
33
.
77
=
x
, s = 5.037, and 
080
.2
21
,
025
.
=
t
, so the t interval is  
(
)
(
6.
79
,1.
75
23
.2
33
.
77
22
037
.5
080
.2
33
.
77
=
±
=
⎟
⎠
⎞
⎜
⎝
⎛
±
).  The t interval is centered at 
x , which is pulled out to the right of x~  by the single mild outlier 93.7; the interval widths 
are comparable. 
 
 
62. 
 
a. Since 
i
X
Σ
λ
2
 has a chi-squared distribution with 2n d.f. and the area under this chi-
squared curve to the right of  
 is .95, 
2
2
,
95
.
n
χ
(
)
95
.
2
2
2
,
95
.
=
Σ
<
i
n
X
P
λ
χ
.  This implies 
that 
i
n
X
Σ
2
2
2
,
95
.χ
 is a lower confidence bound for λ with confidence coefficient 95%.  Table 
A.7 gives the chi-squared critical value for 20 d.f. as 10.851, so the bound is 
(
)
0098
.
87
.
550
2
851
.
10
=
.  We can be 95% confident that λ  exceeds .0098. 
 
b. Arguing as in a, (
)
95
.
2
2
2
,
05
.
=
<
Σ
n
i
X
P
χ
λ
.  The following inequalities are equivalent 
to the one in parentheses: 
i
n
X
Σ
< 2
2
2
,
05
.χ
λ
   
i
n
X
t
t
Σ
−
<
−
⇒
2
2
2
,
05
.χ
λ
   
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡
Σ
−
<
⇒
−
i
n
t
X
t
e
2
exp
2
2
,
05
.χ
λ
.   
Replacing the 
 by 
 in the expression on the right hand side of the last inequality 
gives a 95% lower confidence bound for 
.  Substituting t = 100, 
 
and 
 gives .058 as the lower bound for the probability that time until 
breakdown exceeds 100 minutes. 
i
X
Σ
ix
Σ
t
e λ
−
410
.
31
2
20
,
05
.
=
χ
87
.
550
=
Σ
ix
 
236

